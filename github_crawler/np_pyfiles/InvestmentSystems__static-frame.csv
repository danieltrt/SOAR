file_path,api_count,code
setup.py,0,"b'# Always prefer setuptools over distutils\nfrom setuptools import setup\n# To use a consistent encoding\nfrom codecs import open\nfrom os import path\nimport typing as tp\n\n# https://packaging.python.org/distributing/\n# to deploy:\n# pip install wheel, twine\n# python setup.py sdist\n# python setup.py bdist_wheel\n# twine upload dist/*\n# rm -r build; rm -r dist; rm -r *.egg-info\n\n# in /static-frame-feedstock/recipe\n# update meta.yaml in feedstock: set version and tar sha256 for tar, commit and push\n# submit PR to conda-forge/static-frame-feedstock from fork\n# merge into conda forge feedstock after all checks pass\n\nROOT_DIR_FP = path.abspath(path.dirname(__file__))\n\ndef get_long_description() -> str:\n    with open(path.join(ROOT_DIR_FP, \'README.rst\'), encoding=\'utf-8\') as f:\n        msg = []\n        collect = False\n        start = -1\n        for i, line in enumerate(f):\n            if line.startswith(\'static-frame\'):\n                start = i + 2 # skip this line and the next\n            if i == start:\n                collect = True\n            if line.startswith(\'Installation\'):\n                collect = False\n            if collect:\n                msg.append(line)\n\n    return \'\'.join(msg).strip()\n\n\ndef get_version() -> str:\n    with open(path.join(ROOT_DIR_FP, \'static_frame\', \'__init__.py\'),\n            encoding=\'utf-8\') as f:\n        for l in f:\n            if l.startswith(\'__version__\'):\n                if \'#\' in l:\n                    l = l.split(\'#\')[0].strip()\n                return l.split(\'=\')[-1].strip()[1:-1]\n    raise ValueError(""__version__ not found!"")\n\n\ndef _get_requirements(file_name: str) -> tp.Iterator[str]:\n    with open(path.join(ROOT_DIR_FP, file_name)) as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                yield line\n\ndef get_install_requires() -> tp.Iterator[str]:\n    yield from _get_requirements(\'requirements.txt\')\n\ndef get_extras_require() -> tp.Dict[str, tp.List[str]]:\n    # For now, have only one group that installs all extras; in the future, can create specialized groups if necessary.\n    return {\'extras\': list(_get_requirements(\'requirements-extras.txt\'))}\n\nsetup(\n    name=\'static-frame\',\n    version=get_version(),\n    description=\'Immutable structures for one- and two-dimensional calculations with labelled axes\',\n    long_description=get_long_description(),\n    python_requires=\'>3.6.0\',\n    install_requires=list(get_install_requires()),\n    extras_require=get_extras_require(),\n    url=\'https://github.com/InvestmentSystems/static-frame\',\n    author=\'Christopher Ariza\',\n    license=\'MIT\',\n\n    # See https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[\n            \'Development Status :: 5 - Production/Stable\',\n            \'Intended Audience :: Developers\',\n            \'Topic :: Software Development\',\n            \'Topic :: Scientific/Engineering\',\n            \'Topic :: Scientific/Engineering :: Information Analysis\',\n            \'License :: OSI Approved :: MIT License\',\n            \'Operating System :: MacOS :: MacOS X\',\n            \'Operating System :: Microsoft :: Windows\',\n            \'Operating System :: POSIX\',\n            \'Programming Language :: Python :: 3.6\',\n            \'Programming Language :: Python :: 3.7\',\n            \'Programming Language :: Python :: 3.8\',\n            ],\n\n    keywords=\'staticframe pandas numpy immutable array\',\n    packages=[\n            \'static_frame\',\n            \'static_frame.core\',\n            \'static_frame.performance\',\n            ],\n    )'"
tasks.py,0,"b'import sys\nimport os\nimport typing as tp\n\nimport invoke\n\n#-------------------------------------------------------------------------------\n\n@invoke.task\ndef clean(context):\n    \'\'\'Clean doc and build artifacts\n    \'\'\'\n    context.run(\'rm -rf htmlcov\')\n    context.run(\'rm -rf doc/build\')\n    context.run(\'rm -rf build\')\n    context.run(\'rm -rf dist\')\n    context.run(\'rm -rf *.egg-info\')\n    context.run(\'rm -rf .mypy_cache\')\n    context.run(\'rm -rf .pytest_cache\')\n    context.run(\'rm -rf .hypothesis\')\n    context.run(\'rm -rf .ipynb_checkpoints\')\n\n\n\n@invoke.task()\ndef doc(context):\n    \'\'\'Build docs\n    \'\'\'\n    context.run(f\'{sys.executable} doc/doc_build.py\')\n\n\n@invoke.task\ndef performance(context):\n    \'\'\'Run performance tests.\n    \'\'\'\n    # NOTE: we do not get to see incremental output when running this\n    cmd = \'python static_frame/performance/main.py --performance ""*""\'\n    context.run(cmd)\n\n\n@invoke.task\ndef interface(context, container=None):\n    \'\'\'\n    Optionally select a container type to discover what API endpoints have examples.\n    \'\'\'\n    from static_frame.core.container import ContainerBase\n    import static_frame as sf\n\n\n    def subclasses(cls) -> tp.Iterator[tp.Type]:\n        if cls.__name__ not in (\'IndexBase\', \'IndexDatetime\'):\n            yield cls\n        for sub in cls.__subclasses__():\n            yield from subclasses(sub)\n\n    if not container:\n        def frames():\n            for cls in sorted(subclasses(ContainerBase),\n                    key=lambda cls: cls.__name__):\n                yield cls.interface.unset_index()\n        f = sf.Frame.from_concat(frames(), axis=0, index=sf.IndexAutoFactory)\n    else:\n        f = getattr(sf, container).interface\n\n    print(f.display_tall())\n\n@invoke.task\ndef example(context, container=None):\n    \'\'\'\n    Discover API members that have a code example.\n    \'\'\'\n    from static_frame.test.unit.test_doc import api_example_str\n    from static_frame.core.display_color import HexColor\n    from doc.source.conf import get_jinja_contexts\n\n    start_prefix = \'#start_\'\n    end_prefix = \'#end_\'\n\n    defined = set()\n    signature_start = \'\'\n    signature_end = \'\'\n\n    for line in api_example_str.split(\'\\n\'):\n        if line.startswith(start_prefix):\n            signature_start = line.replace(start_prefix, \'\').strip()\n        elif line.startswith(end_prefix):\n            signature_end = line.replace(end_prefix, \'\').strip()\n            if signature_start == signature_end:\n                defined.add(signature_start)\n                signature_start = \'\'\n                signature_end = \'\'\n            else:\n                raise RuntimeError(f\'mismatched: {signature_start}: {signature_end}\')\n\n    signatures = set()\n\n    # discover all signatures; if it is defined, print in a darker color\n    for name, cls, frame in get_jinja_contexts()[\'interface\']:\n        for signature, row in frame.iter_tuple_items(axis=1):\n            target = f\'{name}-{row.signature_no_args}\'\n            signatures.add(target) # accumulate all signatures\n            if container and name != container:\n                continue\n            if target in defined:\n                print(HexColor.format_terminal(0x505050, target))\n            else:\n                print(target)\n\n\n    for line in sorted(defined - signatures):\n        print(HexColor.format_terminal(0x00ccff, line))\n\n\n#-------------------------------------------------------------------------------\n\n@invoke.task\ndef test(context, unit=False, filename=None):\n    \'\'\'Run tests.\n    \'\'\'\n    if unit:\n        fp = \'static_frame/test/unit\'\n    else:\n        fp = \'static_frame/test\'\n\n    if filename:\n        fp = os.path.join(fp, filename)\n\n    cmd = f\'pytest -s --color no --disable-pytest-warnings --tb=native {fp}\'\n    print(cmd)\n    context.run(cmd)\n\n\n@invoke.task\ndef coverage(context):\n    \'\'\'\n    Perform code coverage, and open report HTML.\n    \'\'\'\n    cmd = \'pytest -s --color no --disable-pytest-warnings --cov=static_frame/core --cov-report html\'\n    print(cmd)\n    context.run(cmd)\n    import webbrowser\n    webbrowser.open(\'htmlcov/index.html\')\n\n\n@invoke.task\ndef mypy(context):\n    \'\'\'Run mypy static analysis.\n    \'\'\'\n    context.run(\'mypy --strict\')\n\n@invoke.task\ndef lint(context):\n    \'\'\'Run pylint static analysis.\n    \'\'\'\n    context.run(\'pylint static_frame\')\n\n@invoke.task(pre=(test, mypy, lint))\ndef integrate(context):\n    \'\'\'Perform all continuous integration.\n    \'\'\'\n\n#-------------------------------------------------------------------------------\n\n@invoke.task(pre=(clean,))\ndef build(context):\n    \'\'\'Build packages\n    \'\'\'\n    context.run(f\'{sys.executable} setup.py sdist bdist_wheel\')\n\n@invoke.task(pre=(build,), post=(clean,))\ndef release(context):\n    context.run(\'twine upload dist/*\')\n\n\n'"
doc/__init__.py,0,b''
doc/doc_build.py,0,"b""import os\nfrom sphinx.cmd.build import main\n\nif __name__ == '__main__':\n    doc_dir = os.path.abspath(os.path.dirname(__file__))\n    doctrees_dir = os.path.join(doc_dir, 'build', 'doctrees')\n    source_dir = os.path.join(doc_dir, 'source')\n    build_dir = os.path.join(doc_dir, 'build', 'html')\n\n    args = ['-E',\n            '-b',\n            'html',\n            '-d',\n            doctrees_dir,\n            source_dir,\n            build_dir]\n    status = main(args)\n\n    import webbrowser\n    webbrowser.open(os.path.join(build_dir, 'index.html'))\n"""
static_frame/__init__.py,0,"b'#pylint: disable=W0611\n# We import the names ""as"" themselves here (and here only) to tell linting tools\n# that they are explicitly being exported here (and not just unused).\n\nfrom static_frame.core.util import GetItemKeyType as GetItemKeyType\nfrom static_frame.core.util import GetItemKeyTypeCompound as GetItemKeyTypeCompound\nfrom static_frame.core.util import CallableOrMapping as CallableOrMapping\nfrom static_frame.core.util import KeyOrKeys as KeyOrKeys\nfrom static_frame.core.util import PathSpecifierOrFileLike as PathSpecifierOrFileLike\nfrom static_frame.core.util import DtypeSpecifier as DtypeSpecifier\nfrom static_frame.core.util import IndexSpecifier as IndexSpecifier\nfrom static_frame.core.util import IndexInitializer as IndexInitializer\n\nfrom static_frame.core.util import SeriesInitializer as SeriesInitializer\nfrom static_frame.core.util import FrameInitializer as FrameInitializer\nfrom static_frame.core.util import mloc as mloc\n\nfrom static_frame.core.node_selector import InterfaceGetItem as InterfaceGetItem\n\nfrom static_frame.core.node_iter import IterNodeApplyType as IterNodeApplyType\nfrom static_frame.core.node_iter import IterNodeType as IterNodeType\nfrom static_frame.core.node_iter import IterNodeDelegate as IterNodeDelegate\n# from static_frame.core.node_iter import IterNode as IterNode\n\nfrom static_frame.core.display import DisplayConfig as DisplayConfig\nfrom static_frame.core.display import DisplayConfigs as DisplayConfigs\nfrom static_frame.core.display import DisplayActive as DisplayActive\nfrom static_frame.core.display import Display as Display\nfrom static_frame.core.display import DisplayFormats as DisplayFormats\n\nfrom static_frame.core.type_blocks import TypeBlocks as TypeBlocks\nfrom static_frame.core.index import Index as Index\nfrom static_frame.core.index import IndexGO as IndexGO\n\nfrom static_frame.core.index_datetime import IndexYear as IndexYear\nfrom static_frame.core.index_datetime import IndexYearGO as IndexYearGO\n\nfrom static_frame.core.index_datetime import IndexYearMonth as IndexYearMonth\nfrom static_frame.core.index_datetime import IndexYearMonthGO as IndexYearMonthGO\n\nfrom static_frame.core.index_datetime import IndexDate as IndexDate\nfrom static_frame.core.index_datetime import IndexDateGO as IndexDateGO\n\nfrom static_frame.core.index_datetime import IndexMinute as IndexMinute\nfrom static_frame.core.index_datetime import IndexMinuteGO as IndexMinuteGO\n\nfrom static_frame.core.index_datetime import IndexHour as IndexHour\nfrom static_frame.core.index_datetime import IndexHourGO as IndexHourGO\n\nfrom static_frame.core.index_datetime import IndexSecond as IndexSecond\nfrom static_frame.core.index_datetime import IndexSecondGO as IndexSecondGO\n\nfrom static_frame.core.index_datetime import IndexMillisecond as IndexMillisecond\nfrom static_frame.core.index_datetime import IndexMillisecondGO as IndexMillisecondGO\n\nfrom static_frame.core.index_datetime import IndexMicrosecond as IndexMicrosecond\nfrom static_frame.core.index_datetime import IndexMicrosecondGO as IndexMicrosecondGO\n\nfrom static_frame.core.index_datetime import IndexNanosecond as IndexNanosecond\nfrom static_frame.core.index_datetime import IndexNanosecondGO as IndexNanosecondGO\n\nfrom static_frame.core.index import ILoc as ILoc\nfrom static_frame.core.hloc import HLoc as HLoc\n\nfrom static_frame.core.index_level import IndexLevel as IndexLevel\nfrom static_frame.core.index_level import IndexLevelGO as IndexLevelGO\nfrom static_frame.core.index_hierarchy import IndexHierarchy as IndexHierarchy\nfrom static_frame.core.index_hierarchy import IndexHierarchyGO as IndexHierarchyGO\n\nfrom static_frame.core.index_auto import IndexAutoFactory as IndexAutoFactory\nfrom static_frame.core.index_auto import IndexAutoInitializer as IndexAutoInitializer\nfrom static_frame.core.index_auto import IndexAutoFactoryType\n\nfrom static_frame.core.series import Series as Series\nfrom static_frame.core.series import SeriesAssign as SeriesAssign\n\nfrom static_frame.core.frame import Frame as Frame\nfrom static_frame.core.frame import FrameGO as FrameGO\nfrom static_frame.core.frame import FrameAssign as FrameAssign\n\nfrom static_frame.core.bus import Bus as Bus\nfrom static_frame.core.store_filter import StoreFilter as StoreFilter\nfrom static_frame.core.store import StoreConfigMap as StoreConfigMap\nfrom static_frame.core.store import StoreConfig as StoreConfig\n\nfrom static_frame.core.exception import ErrorInit\nfrom static_frame.core.exception import ErrorInitTypeBlocks\nfrom static_frame.core.exception import ErrorInitSeries\nfrom static_frame.core.exception import ErrorInitFrame\nfrom static_frame.core.exception import ErrorInitIndex\nfrom static_frame.core.exception import ErrorInitIndexLevel\nfrom static_frame.core.exception import ErrorInitBus\nfrom static_frame.core.exception import ErrorInitStore\nfrom static_frame.core.exception import ErrorInitStoreConfig\nfrom static_frame.core.exception import LocEmpty\nfrom static_frame.core.exception import LocInvalid\nfrom static_frame.core.exception import AxisInvalid\nfrom static_frame.core.exception import StoreFileMutation\n\nfrom static_frame.core.node_iter import IterNodeNoArg\nfrom static_frame.core.node_iter import IterNodeAxis\nfrom static_frame.core.node_iter import IterNodeGroup\nfrom static_frame.core.node_iter import IterNodeGroupAxis\nfrom static_frame.core.node_iter import IterNodeDepthLevel\nfrom static_frame.core.node_iter import IterNodeDepthLevelAxis\nfrom static_frame.core.node_iter import IterNodeWindow\n\n\nfrom static_frame.core.node_selector import InterfaceSelectDuo\nfrom static_frame.core.node_selector import InterfaceSelectTrio\nfrom static_frame.core.node_selector import InterfaceSelectQuartet\n\nfrom static_frame.core.node_selector import InterfaceAssignTrio\nfrom static_frame.core.node_selector import InterfaceAssignQuartet\nfrom static_frame.core.node_selector import InterfaceAsType\n\nfrom static_frame.core.node_str import InterfaceString\nfrom static_frame.core.node_dt import InterfaceDatetime\n\n\n__version__ = \'0.6.18\' # use -dev for new version in development\n\n'"
static_frame/__main__.py,0,"b""'''Drop into an interactive interpreter pre-loaded with sf, np, and (if installed) pd.\n\n$ python -m static_frame\n$ ipython -m static_frame\n'''\n\n\nfrom code import interact\nfrom sys import platform\nfrom sys import version\n\nimport numpy as np\n\nimport static_frame as sf\nfrom static_frame.core.display_color import HexColor\n\nimports = {'np': np, 'sf': sf}\n\ntry: # Import pandas, if it's installed:\n    import pandas as pd\nexcept ImportError: #pragma: no cover\n    pass #pragma: no cover\nelse:\n    imports['pd'] = pd\n\ncommands = sorted(\n        f'import {package.__name__} as {name} # {package.__version__}'\n        for name, package in imports.items()\n        )\n\ntry: # This lets us play nicely with IPython:\n    from builtins import __IPYTHON__  #type: ignore\n    from IPython import embed\n    from IPython import get_ipython\n\nexcept ImportError:\n    is_ipython = False\nelse:\n    is_ipython = __IPYTHON__\n\n\nif __name__ == '__main__':\n\n    if is_ipython:\n        ipython = get_ipython()\n\n        print() # Spacer.\n        for command in commands:\n            ipython.auto_rewrite_input(command)\n        print() # Spacer.\n\n        embed(user_ns=imports, colors='neutral')\n\n    else:\n        banner_head = f'Python {version} on {platform}\\n'\n        banner_body = '\\n'.join(f'>>> {command}' for command in commands)\n        interact(banner=(banner_head + HexColor.format_terminal(0x505050, banner_body)),\n                local=imports,\n                exitmsg=''\n                )\n"""
doc/animate/animator.py,1,"b'import typing as tp\nimport time\nimport sys\nimport traceback\nimport random\nimport argparse\nimport subprocess\n\nfrom static_frame.core.display_color import HexColor\n\n\nclass Line:\n    pass\n\nPAUSE_SHORT = Line()\nPAUSE_LONG = Line()\nPAUSE_FINAL = Line()\n\nclass Comment(Line):\n    def __init__(self, message: str, color: int = 0xaaaaaa) -> None:\n        self.message = message\n        self.color = color\n\n    def __iter__(self) -> tp.Iterator[str]:\n        return HexColor.format_terminal(self.color, self.message).__iter__()\n\nLineIter = tp.Iterator[tp.Union[Line, str]]\n\n#-------------------------------------------------------------------------------\nclass LineGen:\n    CMD_PREFIX = \'\'\n\n    @staticmethod\n    def lines() -> LineIter:\n        raise NotImplementedError()\n\nclass DisplayConfig(LineGen):\n\n    @staticmethod\n    def lines() -> LineIter:\n        yield \'import numpy as np\'\n        yield \'import pandas as pd\'\n        yield \'import static_frame as sf\'\n        yield PAUSE_SHORT\n\n\nclass LowMemoryOps(LineGen):\n    CMD_PREFIX = \'prlimit --as=800000000\' # shown to cause expected memory error\n\n    @staticmethod\n    def lines() -> LineIter:\n        # return lines of code to execute\n\n        yield Comment(""# This example demonstrates one of the many benefits of StaticFrame\'s use of immutable data by simulating a low-memory environment with prlimit. Let us start by importing numpy, pandas, and static_frame"")\n        yield PAUSE_SHORT\n\n        yield \'import numpy as np\'\n        yield \'import pandas as pd\'\n        yield \'import static_frame as sf\'\n        yield PAUSE_SHORT\n\n        yield Comment(\'# We will create a large 2D array of integers and a tuple of column labels.\')\n        yield PAUSE_SHORT\n\n        yield \'a1 = np.arange(10_000_000).reshape(1_000_000, 10)\'\n        yield ""columns = tuple(\'abcdefghij\')""\n        yield PAUSE_SHORT\n\n        yield Comment(\'# Next, we create a Pandas DataFrame using that array.\')\n        yield PAUSE_SHORT\n\n\n        yield \'df1 = pd.DataFrame(a1, columns=columns)\'\n        yield \'df1.shape\'\n        yield PAUSE_LONG\n\n\n        yield Comment(\'# Pandas cannot rename the DataFrame without defensively copying the data, which in this low-memory environment causes a MemoryError.\')\n        yield PAUSE_SHORT\n\n        yield \'df1.rename(columns=lambda x: x.upper())\'\n        yield PAUSE_LONG\n\n\n        yield Comment(\'# Similarly, concatenating the DataFrame with itself results in a MemoryError.\')\n        yield PAUSE_SHORT\n\n        yield \'pd.concat((df1, df1), axis=1, ignore_index=True)\'\n        yield PAUSE_LONG\n\n\n        yield Comment(\'# To reuse the same array in StaticFrame, we can make it immutable.\')\n        yield PAUSE_SHORT\n\n        yield \'a1.flags.writeable = False\'\n        yield PAUSE_SHORT\n\n        yield \'f1 = sf.Frame(a1, columns=columns)\'\n        yield \'f1.shape\'\n        yield PAUSE_SHORT\n\n\n        yield Comment(\'# As StaticFrame is built on immutable arrays, we can relabel the Frame without a MemoryError, as underlying data does not need to be copied.\')\n        yield PAUSE_SHORT\n\n        yield \'f2 = f1.relabel(columns=lambda x: x.upper())\'\n        yield \'f2.shape\'\n        yield \'f2.columns\'\n        yield PAUSE_LONG\n\n\n        yield Comment(\'# Similarly, while Pandas runs out of memory, StaticFrame can successfully concatenate the Frame.\')\n        yield PAUSE_SHORT\n\n        yield \'f3 = sf.Frame.from_concat((f1, f2), axis=1)\'\n        yield \'f3.columns.values\'\n        yield \'f3.shape\'\n        yield PAUSE_LONG\n\n#------------------------------------------------------------------------\\------\nclass Animator:\n\n    PROMPT = HexColor.format_terminal(\'lightgrey\', \'>>> \')\n    CHAR_INTERVAL = 0.04 #0.07\n    CHAR_JITTER = [x * .01 for x in range(6)] + [0.08, .12]\n\n    @classmethod\n    def print_char(cls, char: str) -> None:\n        print(char, end=\'\')\n        sys.stdout.flush()\n        time.sleep(cls.CHAR_INTERVAL + random.choice(cls.CHAR_JITTER))\n\n    @classmethod\n    def pause(cls, interval: float) -> None:\n        print(cls.PROMPT, end=\'\')\n        sys.stdout.flush()\n        time.sleep(interval)\n        print() # newline\n        sys.stdout.flush()\n\n\n    @classmethod\n    def main(cls, func: tp.Callable[[], LineIter]) -> None:\n\n        for line in func():\n            if line is PAUSE_SHORT:\n                cls.pause(0.5)\n                continue\n            if line is PAUSE_LONG:\n                cls.pause(2)\n                continue\n            if line is PAUSE_FINAL:\n                cls.pause(5)\n                continue\n\n            assert isinstance(line, (Comment, str))\n\n            print(cls.PROMPT, end=\'\')\n            for char in line:\n                cls.print_char(char)\n            cls.print_char(\'\\n\') # get new line\n\n            if isinstance(line, Comment):\n                continue\n\n            try:\n                post = eval(line)\n                if post is not None:\n                    print(post)\n            except SyntaxError:\n                exec(line)\n            except MemoryError as e:\n                traceback.print_exc(limit=-3)\n\n\ndef get_arg_parser() -> argparse.ArgumentParser:\n    p = argparse.ArgumentParser(\n            description=\'Terminal animator\',\n            formatter_class=argparse.RawDescriptionHelpFormatter,\n            )\n    p.add_argument(\'--animate\',\n            help=\'Name of function to display the animation.\',\n            )\n    p.add_argument(\'--record\',\n            help=\'Name of function to record.\',\n            )\n    return p\n\n\nif __name__ == \'__main__\':\n\n    options = get_arg_parser().parse_args()\n    line_gen = {cls.__name__: cls for cls in (LowMemoryOps, DisplayConfig)}\n\n    if options.animate:\n        cls = line_gen[options.animate]\n        Animator.main(cls.lines)\n\n    elif options.record:\n        cls = line_gen[options.record]\n\n        if cls.CMD_PREFIX:\n            command = f""{cls.CMD_PREFIX} python3 doc/animate/animator.py --animate {options.record}""\n        else:\n            command = f""python3 doc/animate/animator.py --animate {options.record}""\n\n        cmd = [\'termtosvg\',\n            \'--template\',\n            \'window_frame\',\n            \'-g\', \'90x20\',\n            \'--command\',\n            command,\n            \'/tmp/term.svg\',\n            ]\n        subprocess.run(cmd)\n'"
doc/source/__init__.py,0,b''
doc/source/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# function-pipe documentation build configuration file, created by\n# sphinx-quickstart on Fri Jan  6 16:49:22 2017.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\nimport datetime\nimport io\nimport inspect\nimport typing as tp\n\nimport static_frame as sf\n\nfrom static_frame.core.container import _UFUNC_UNARY_OPERATORS\nfrom static_frame.core.container import _UFUNC_BINARY_OPERATORS\nfrom static_frame.core.container import UFUNC_AXIS_SKIPNA\nfrom static_frame.core.util import AnyCallable\n\nfrom static_frame.performance import core\nfrom static_frame.performance.perf_test import PerfTest\nfrom static_frame.core.interface import InterfaceSummary\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n\ndef get_jinja_contexts() -> tp.Dict[str, tp.List[tp.Tuple[str, str]]]:\n\n    post = {}\n\n    performance_cls = []\n    for name in dir(core):\n        obj = getattr(core, name)\n        if inspect.isclass(obj) and issubclass(obj, PerfTest):\n            performance_cls.append(obj.__name__)\n\n    post[\'performance_cls\'] = performance_cls\n\n    def get_func_doc(cls: type, func_iter: tp.Iterable[str]) -> tp.List[tp.Tuple[str, str]]:\n        return [(f, getattr(cls, f).__doc__) for f in sorted(func_iter)]\n\n    for cls in (sf.Index, sf.Series, sf.Frame):\n        label = cls.__name__\n        post[label + \'_operator_unary\'] = get_func_doc(cls,\n                _UFUNC_UNARY_OPERATORS)\n        post[label + \'_operator_binary\'] = get_func_doc(cls,\n                _UFUNC_BINARY_OPERATORS)\n\n    for cls in (sf.Index, sf.Series, sf.Frame):\n        label = cls.__name__\n        post[label + \'_ufunc_axis\'] = sorted(UFUNC_AXIS_SKIPNA.keys())\n\n\n    post[\'interface\'] = []\n    for target in (\n            sf.Series,\n            sf.Frame,\n            sf.FrameGO,\n            sf.Bus,\n            sf.Index,\n            sf.IndexGO,\n            sf.IndexHierarchy,\n            sf.IndexHierarchyGO,\n            sf.IndexYear,\n            sf.IndexYearGO,\n            sf.IndexYearMonth,\n            sf.IndexYearMonthGO,\n            sf.IndexDate,\n            sf.IndexDateGO,\n            sf.IndexMinute,\n            sf.IndexMinuteGO,\n            sf.IndexHour,\n            sf.IndexHourGO,\n            sf.IndexSecond,\n            sf.IndexSecondGO,\n            sf.IndexMillisecond,\n            sf.IndexMillisecondGO,\n            sf.IndexMicrosecond,\n            sf.IndexMicrosecondGO,\n            sf.IndexNanosecond,\n            sf.IndexNanosecondGO,\n            ):\n        post[\'interface\'].append((\n                target.__name__,\n                target,\n                InterfaceSummary.to_frame(target, #type: ignore\n                        minimized=False,\n                        max_args=99, # +inf, but keep as int\n                        )\n                ))\n    return post\n\njinja_contexts = {\'ctx\': get_jinja_contexts()}\n\n\n# -- General configuration -----------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named \'sphinx.ext.*\') or your custom ones.\nextensions = [\n        \'sphinx.ext.autodoc\',\n        \'sphinx.ext.viewcode\',\n        \'sphinx.ext.graphviz\',\n        \'sphinx.ext.inheritance_diagram\',\n        \'sphinxcontrib.napoleon\',\n        \'sphinxcontrib.jinja\',\n        ]\n\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix of source filenames.\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'StaticFrame\'\ncopyright = \'%s, Christopher Ariza\' % datetime.datetime.now().year\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'.\'.join(sf.__version__.split(\'.\')[:2])\n# The full version, including alpha/beta/rc tags.\nrelease = sf.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns: tp.List[str] = []\n\nadd_module_names = False\n# The reST default role (used for this markup: `text`) to use for all documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\nadd_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\nadd_module_names = False\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n\n# -- Options for HTML output ---------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\non_rtd = os.environ.get(\'READTHEDOCS\') == \'True\'\nif on_rtd:\n    html_theme = \'default\'\nelse:\n    import sphinx_rtd_theme\n    html_theme = ""sphinx_rtd_theme""\n    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\nhtml_favicon = ""../images/favicon.ico""\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\n# html_static_path = [\'_static\']\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\nhtml_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'static-frame\'\n\n\n# -- Options for LaTeX output --------------------------------------------------\n\nlatex_elements: tp.Dict[str, str] = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\n# latex_documents = []\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output --------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages: tp.List[tp.Tuple[str, str, str, str, str]] = []\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output ------------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents: tp.List[tp.Tuple[str, str, str, str, str, str, str]] = []\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n'"
static_frame/core/__init__.py,0,b''
static_frame/core/array_go.py,6,"b""\nimport typing as tp\n\nimport numpy as np\n\nfrom static_frame.core.util import immutable_filter\n\n\nclass ArrayGO:\n    '''\n    A grow only, one-dimensional, object type array, specifically for usage in IndexHierarchy IndexLevel objects.\n    '''\n\n    _array: tp.Optional[np.ndarray]\n    _array_mutable: tp.Optional[tp.List[tp.Any]]\n\n    __slots__ = (\n            '_dtype',\n            '_array',\n            '_array_mutable',\n            '_recache',\n            )\n\n    # NOTE: this can be implemented with one array, where we overallocate for growth, then grow as needed, or with an array and list. Since most instaces will not need to grow (only edge nodes), overall efficiency might be greater with a list\n\n    def __init__(self,\n            iterable: tp.Union[np.ndarray, tp.List[object]],\n            *,\n            own_iterable: bool = False) -> None:\n        '''\n        Args:\n            own_iterable: flag iterable as ownable by this instance.\n        '''\n\n        self._dtype = object # only object arrays are supported\n\n        if isinstance(iterable, np.ndarray):\n            if own_iterable:\n                self._array = iterable\n                self._array.flags.writeable = False\n            else:\n                self._array = immutable_filter(iterable)\n            if self._array.dtype != self._dtype:\n                raise NotImplementedError('only object arrays are supported')\n            self._recache = False\n            self._array_mutable = None\n        else: # assume it is a list or listable\n            self._array = None\n            self._recache = True\n            # always call list to get new object, or realize a generator\n            if own_iterable:\n                self._array_mutable = iterable\n            else:\n                self._array_mutable = list(iterable)\n\n    def _update_array_cache(self) -> None:\n        if self._array_mutable is not None:\n            if self._array is not None:\n                len_base = len(self._array)\n                array = np.empty(\n                        len_base + len(self._array_mutable),\n                        self._dtype)\n                array[:len_base] = self._array\n                array[len_base:] = self._array_mutable\n                array.flags.writeable = False\n                self._array = array\n                self._array_mutable = None\n            else:\n                self._array = np.array(self._array_mutable, self._dtype)\n                self._array.flags.writeable = False\n                self._array_mutable = None\n        self._recache = False\n\n    def __iter__(self) -> tp.Iterator[tp.Any]:\n        if self._recache:\n            self._update_array_cache()\n        return iter(self._array) #type: ignore\n\n    def __getitem__(self, key: tp.Any) -> tp.Any:\n        if self._recache:\n            self._update_array_cache()\n        return self._array.__getitem__(key) #type: ignore\n\n    def __len__(self) -> int:\n        if self._recache:\n            self._update_array_cache()\n        return len(self._array) #type: ignore\n\n    def append(self, value: tp.Iterable[object]) -> None:\n        if self._array_mutable is None:\n            self._array_mutable = []\n        self._array_mutable.append(value)\n        self._recache = True\n\n    def extend(self, values: tp.Iterable[object]) -> None:\n        if self._array_mutable is None:\n            self._array_mutable = []\n        self._array_mutable.extend(values)\n        self._recache = True\n\n    @property\n    def values(self) -> np.ndarray:\n        '''Return the immutable labels array\n        '''\n        if self._recache:\n            self._update_array_cache()\n        return self._array\n\n\n    def copy(self) -> 'ArrayGO':\n        '''Return a new ArrayGO with an immutable array from this ArrayGO\n        '''\n        if self._recache:\n            self._update_array_cache()\n        return self.__class__(self._array, own_iterable=True)\n"""
static_frame/core/assign.py,0,"b""from static_frame.core.util import EMPTY_TUPLE\n\nclass Assign:\n    '''\n    Common base class for SeriesAssign and FrameAssign classes.\n    '''\n    __slots__ = EMPTY_TUPLE\n"""
static_frame/core/bus.py,9,"b'\nimport typing as tp\n\nimport numpy as np\n\n\nfrom static_frame.core.series import Series\nfrom static_frame.core.frame import Frame\n# from static_frame.core.frame import Index\n\nfrom static_frame.core.store import Store\nfrom static_frame.core.store_zip import StoreZipCSV\nfrom static_frame.core.store_zip import StoreZipTSV\nfrom static_frame.core.store_zip import StoreZipPickle\nfrom static_frame.core.store_xlsx import StoreXLSX\nfrom static_frame.core.store_sqlite import StoreSQLite\nfrom static_frame.core.store_hdf5 import StoreHDF5\n\n# from static_frame.core.store import StoreConfig\nfrom static_frame.core.store import StoreConfigMap\nfrom static_frame.core.store import StoreConfigMapInitializer\n\n\nfrom static_frame.core.exception import ErrorInitBus\nfrom static_frame.core.util import GetItemKeyType\nfrom static_frame.core.util import DTYPE_OBJECT\nfrom static_frame.core.util import DTYPE_BOOL\n# from static_frame.core.util import DTYPE_INT_DEFAULT\nfrom static_frame.core.util import DTYPE_FLOAT_DEFAULT\nfrom static_frame.core.util import PathSpecifier\nfrom static_frame.core.util import NULL_SLICE\n\n# from static_frame.core.util import DtypesSpecifier\n\nfrom static_frame.core.node_selector import InterfaceGetItem\n\nfrom static_frame.core.hloc import HLoc\n\nfrom static_frame.core.display import DisplayConfig\nfrom static_frame.core.display import DisplayActive\nfrom static_frame.core.display import Display\nfrom static_frame.core.display import DisplayHeader\n\nfrom static_frame.core.doc_str import doc_inject\n\nfrom static_frame.core.container import ContainerBase\n\nfrom static_frame.core.node_selector import TContainer\n\n\n#-------------------------------------------------------------------------------\nclass FrameDefferedMeta(type):\n    def __repr__(cls) -> str:\n        return f\'<{cls.__name__}>\'\n\nclass FrameDeferred(metaclass=FrameDefferedMeta):\n    \'\'\'\n    Token placeholder for :obj:`Frame` not yet loaded.\n    \'\'\'\n\n#-------------------------------------------------------------------------------\nclass Bus(ContainerBase): # not a ContainerOperand\n\n    __slots__ = (\n        \'_loaded\',\n        \'_loaded_all\',\n        \'_series\',\n        \'_store\',\n        \'_config\',\n        )\n\n    _series: Series\n    _store: tp.Optional[Store]\n    _config: StoreConfigMap\n\n    STATIC = False\n\n    @staticmethod\n    def _deferred_series(labels: tp.Iterable[str]) -> Series:\n        \'\'\'\n        Return an object ``Series`` of ``FrameDeferred`` objects, based on the passed in ``labels``.\n        \'\'\'\n        # make an object dtype\n        return Series.from_element(FrameDeferred, index=labels, dtype=object)\n\n    @classmethod\n    def from_frames(cls,\n            frames: tp.Iterable[Frame],\n            *,\n            config: StoreConfigMapInitializer = None,\n            ) -> \'Bus\':\n        \'\'\'Return a ``Bus`` from an iterable of ``Frame``; labels will be drawn from :obj:`Frame.name`.\n        \'\'\'\n        # could take a StoreConfigMap\n        series = Series.from_items(\n                    ((f.name, f) for f in frames),\n                    dtype=object\n                    )\n        return cls(series, config=config)\n\n    #---------------------------------------------------------------------------\n    # constructors by data format\n\n    @classmethod\n    def from_zip_tsv(cls,\n            fp: PathSpecifier,\n            config: StoreConfigMapInitializer = None,\n            ) -> \'Bus\':\n        # take and store a StoreConfigMap\n        store = StoreZipTSV(fp)\n        return cls(cls._deferred_series(store.labels()),\n                store=store,\n                config=config\n                )\n\n    @classmethod\n    def from_zip_csv(cls,\n            fp: PathSpecifier,\n            config: StoreConfigMapInitializer = None\n            ) -> \'Bus\':\n        store = StoreZipCSV(fp)\n        return cls(cls._deferred_series(store.labels()),\n                store=store,\n                config=config\n                )\n\n    @classmethod\n    def from_zip_pickle(cls,\n            fp: PathSpecifier,\n            config: StoreConfigMapInitializer = None\n            ) -> \'Bus\':\n        store = StoreZipPickle(fp)\n        return cls(cls._deferred_series(store.labels()),\n                store=store,\n                config=config\n                )\n\n    @classmethod\n    def from_xlsx(cls,\n            fp: PathSpecifier,\n            config: StoreConfigMapInitializer = None\n            ) -> \'Bus\':\n        # how to pass configuration for multiple sheets?\n        store = StoreXLSX(fp)\n        return cls(cls._deferred_series(store.labels()),\n                store=store,\n                config=config\n                )\n\n    @classmethod\n    def from_sqlite(cls,\n            fp: PathSpecifier,\n            config: StoreConfigMapInitializer = None\n            ) -> \'Bus\':\n        store = StoreSQLite(fp)\n        return cls(cls._deferred_series(store.labels()),\n                store=store,\n                config=config\n                )\n\n    @classmethod\n    def from_hdf5(cls,\n            fp: PathSpecifier,\n            config: StoreConfigMapInitializer = None\n            ) -> \'Bus\':\n        store = StoreHDF5(fp)\n        return cls(cls._deferred_series(store.labels()),\n                store=store,\n                config=config\n                )\n\n\n    #---------------------------------------------------------------------------\n    def __init__(self,\n            series: Series,\n            *,\n            store: tp.Optional[Store] = None,\n            config: StoreConfigMapInitializer = None\n            ):\n        \'\'\'\n        Args:\n            config: StoreConfig for handling ``Frame`` construction and exporting from Store.\n        \'\'\'\n\n        if series.dtype != DTYPE_OBJECT:\n            raise ErrorInitBus(\n                    f\'Series passed to initializer must have dtype object, not {series.dtype}\')\n\n        # do a one time iteration of series\n        def gen() -> tp.Iterator[bool]:\n            for label, value in series.items():\n                if not isinstance(label, str):\n                    raise ErrorInitBus(f\'supplied label {label} is not a string.\')\n\n                if isinstance(value, Frame):\n                    yield True\n                elif value is FrameDeferred:\n                    yield False\n                else:\n                    raise ErrorInitBus(f\'supplied {value.__class__} is not a Frame or FrameDeferred.\')\n\n        self._loaded = np.fromiter(gen(), dtype=DTYPE_BOOL, count=len(series))\n        self._loaded_all = self._loaded.all()\n        self._series = series\n        self._store = store\n\n        # providing None will result in default; providing a StoreConfig or StoreConfigMap will return an appropriate map\n        self._config = StoreConfigMap.from_initializer(config)\n\n\n    #---------------------------------------------------------------------------\n    # delegation\n\n    def __getattr__(self, name: str) -> tp.Any:\n        if name == \'interface\':\n            return getattr(self.__class__, \'interface\')\n\n        try:\n            return getattr(self._series, name)\n        except AttributeError:\n            # fix the attribute error to reference the Bus\n            raise AttributeError(f""\'{self.__class__.__name__}\' object has no attribute \'{name}\'"")\n\n    #---------------------------------------------------------------------------\n    # cache management\n\n    def _iloc_to_labels(self,\n            key: GetItemKeyType\n            ) -> np.ndarray:\n        \'\'\'\n        Given a get-item key, translate to an iterator of loc positions.\n        \'\'\'\n        if isinstance(key, int):\n            return [self.index.values[key],] # needs to be a list for usage in loc assignment\n        return self.index.values[key]\n\n\n    def _update_series_cache_iloc(self, key: GetItemKeyType) -> None:\n        \'\'\'\n        Update the Series cache with the key specified, where key can be any iloc GetItemKeyType.\n        \'\'\'\n\n        # do nothing if all loaded, or if the requested keys are already loadsed\n        if not self._loaded_all and not self._loaded[key].all():\n            if self._store is None:\n                raise RuntimeError(\'no store defined\')\n\n            labels = set(self._iloc_to_labels(key))\n\n            array = np.empty(shape=len(self._series._index), dtype=object)\n            for idx, (label, frame) in enumerate(self._series.items()):\n                if frame is FrameDeferred and label in labels:\n                    frame = self._store.read(label, config=self._config[label])\n                    self._loaded[idx] = True # update loaded status\n                array[idx] = frame\n            array.flags.writeable = False\n\n            self._series = Series(array, index=self._series._index, dtype=object)\n            self._loaded_all = self._loaded.all()\n\n    def _update_series_cache_all(self) -> None:\n        \'\'\'Load all Tables contained in this Bus.\n        \'\'\'\n        if not self._loaded_all:\n            self._update_series_cache_iloc(NULL_SLICE)\n\n    #---------------------------------------------------------------------------\n    # extraction\n\n    def _extract_iloc(self, key: GetItemKeyType) -> \'Bus\':\n        self._update_series_cache_iloc(key=key)\n\n        # iterable selection should be handled by NP\n        values = self._series.values[key]\n\n        if not isinstance(values, np.ndarray): # if we have a single element\n            return values #type: ignore\n        series = Series(\n                values,\n                index=self._series._index.iloc[key],\n                name=self._series._name)\n        return self.__class__(series=series,\n                store=self._store,\n                config=self._config,\n                )\n\n    def _extract_loc(self, key: GetItemKeyType) -> \'Bus\':\n\n        iloc_key = self._series._index.loc_to_iloc(key) #type: ignore\n\n        # NOTE: if we update before slicing, we change the local and the object handed back\n        self._update_series_cache_iloc(key=iloc_key)\n\n        values = self._series.values[iloc_key]\n\n        if not isinstance(values, np.ndarray): # if we have a single element\n            if isinstance(key, HLoc) and key.has_key_multiple():\n                # must return a Series, even though we do not have an array\n                values = np.array(values)\n                values.flags.writeable = False\n            else:\n                return values #type: ignore\n\n        series = Series(values,\n                index=self._series._index.iloc[iloc_key],\n                own_index=True,\n                name=self._series._name)\n        return self.__class__(series=series,\n                store=self._store,\n                config=self._config,\n                )\n\n\n    @doc_inject(selector=\'selector\')\n    def __getitem__(self, key: GetItemKeyType) -> \'Bus\':\n        \'\'\'Selector of values by label.\n\n        Args:\n            key: {key_loc}\n        \'\'\'\n        return self._extract_loc(key)\n\n\n\n    #---------------------------------------------------------------------------\n    # interfaces\n\n    @property\n    def loc(self) -> InterfaceGetItem[TContainer]:\n        return InterfaceGetItem(self._extract_loc)  #type: ignore\n\n    @property\n    def iloc(self) -> InterfaceGetItem[TContainer]:\n        return InterfaceGetItem(self._extract_iloc)  #type: ignore\n\n\n    # ---------------------------------------------------------------------------\n    def __reversed__(self) -> tp.Iterator[tp.Hashable]:\n        return reversed(self._series._index) #type: ignore\n\n    def __len__(self) -> int:\n        return self._series.__len__()\n\n\n    #---------------------------------------------------------------------------\n    # dictionary-like interface\n\n    def items(self) -> tp.Iterator[tp.Tuple[tp.Any, tp.Any]]:\n        \'\'\'Iterator of pairs of index label and value.\n        \'\'\'\n        self._update_series_cache_all()\n        yield from self._series.items()\n\n    @property\n    def values(self) -> np.ndarray:\n        \'\'\'A 1D array of values.\n        \'\'\'\n        self._update_series_cache_all()\n        return self._seires.values\n\n\n    #---------------------------------------------------------------------------\n    @doc_inject()\n    def display(self,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> Display:\n        \'\'\'{doc}\n\n        Args:\n            {config}\n        \'\'\'\n        # NOTE: the key change over serires is providing the Bus as the displayed class\n        config = config or DisplayActive.get()\n        display_cls = Display.from_values((),\n                header=DisplayHeader(self.__class__, self._series._name),\n                config=config)\n        return self._series._display(config, display_cls)\n\n\n    #---------------------------------------------------------------------------\n    # extended disciptors\n\n    @property\n    def mloc(self) -> Series:\n        \'\'\'Returns a Series of tuples of dtypes, one for each loaded Frame.\n        \'\'\'\n        if not self._loaded.any():\n            return Series(None, index=self._series._index)\n\n        def gen() -> tp.Iterator[tp.Tuple[tp.Hashable, tp.Optional[tp.Tuple[int, ...]]]]:\n            for label, f in zip(self._series._index, self._series.values):\n                if f is FrameDeferred:\n                    yield label, None\n                else:\n                    yield label, tuple(f.mloc)\n\n        return Series.from_items(gen())\n\n    @property\n    def dtypes(self) -> Frame:\n        \'\'\'Returns a Frame of dtypes for all loaded Frames.\n        \'\'\'\n        if not self._loaded.any():\n            return Frame(index=self._series.index)\n\n        f = Frame.from_concat(\n                frames=(f.dtypes for f in self._series.values if f is not FrameDeferred),\n                fill_value=None,\n                ).reindex(index=self._series.index, fill_value=None)\n        return tp.cast(Frame, f)\n\n    @property\n    def shapes(self) -> Series:\n        \'\'\'A :obj:`Series` describing the shape of each loaded :obj:`Frame`.\n\n        Returns:\n            :obj:`tp.Tuple[int]`\n        \'\'\'\n        values = (f.shape if f is not FrameDeferred else None for f in self._series.values)\n        return Series(values, index=self._series._index, dtype=object, name=\'shape\')\n\n\n    @property\n    def nbytes(self) -> int:\n        \'\'\'Total bytes of data currently loaded in the Bus.\n        \'\'\'\n        return sum(f.nbytes if f is not FrameDeferred else 0 for f in self._series.values)\n\n    @property\n    def status(self) -> Frame:\n        \'\'\'\n        Return a\n        \'\'\'\n        def gen() -> tp.Iterator[Series]:\n\n            yield Series(self._loaded,\n                    index=self._series._index,\n                    dtype=DTYPE_BOOL,\n                    name=\'loaded\')\n\n            for attr, dtype, missing in (\n                    (\'size\', DTYPE_FLOAT_DEFAULT, np.nan),\n                    (\'nbytes\', DTYPE_FLOAT_DEFAULT, np.nan),\n                    (\'shape\', DTYPE_OBJECT, None)\n                    ):\n\n                values = (getattr(f, attr) if f is not FrameDeferred\n                        else missing for f in self._series.values)\n                yield Series(values, index=self._series._index, dtype=dtype, name=attr)\n\n        return tp.cast(Frame, Frame.from_concat(gen(), axis=1))\n\n    #---------------------------------------------------------------------------\n    @doc_inject()\n    def equals(self,\n            other: tp.Any,\n            *,\n            compare_name: bool = False,\n            compare_dtype: bool = False,\n            compare_class: bool = False,\n            skipna: bool = True,\n            ) -> bool:\n        \'\'\'\n        {doc}\n\n        Args:\n            {compare_name}\n            {compare_dtype}\n            {compare_class}\n            {skipna}\n        \'\'\'\n\n        if id(other) == id(self):\n            return True\n\n        if compare_class and self.__class__ != other.__class__:\n            return False\n        elif not isinstance(other, Bus):\n            return False\n\n        # defer updating cache\n        self._update_series_cache_all()\n\n        if len(self._series) != len(other._series):\n            return False\n\n        if compare_name and self._series._name != other._series._name:\n            return False\n\n        # NOTE: dtype self._series is always object\n\n        if not self._series.index.equals(\n                other._series.index,\n                compare_name=compare_name,\n                compare_dtype=compare_dtype,\n                compare_class=compare_class,\n                skipna=skipna,\n                ):\n            return False\n\n        # can zip because length of Series already match\n        for (frame_self, frame_other) in zip(\n                self._series.values, other._series.values):\n            if not frame_self.equals(frame_other,\n                    compare_name=compare_name,\n                    compare_dtype=compare_dtype,\n                    compare_class=compare_class,\n                    skipna=skipna,\n                    ):\n                return False\n\n        return True\n\n\n\n\n\n    #---------------------------------------------------------------------------\n    # exporters\n\n    def to_zip_tsv(self,\n            fp: PathSpecifier,\n            config: StoreConfigMapInitializer = None\n            ) -> None:\n        store = StoreZipTSV(fp)\n        config = config if not None else self._config\n        store.write(self.items(), config=config)\n\n    def to_zip_csv(self,\n            fp: PathSpecifier,\n            config: StoreConfigMapInitializer = None\n            ) -> None:\n        store = StoreZipCSV(fp)\n        config = config if not None else self._config\n        store.write(self.items(), config=config)\n\n    def to_zip_pickle(self,\n            fp: PathSpecifier,\n            config: StoreConfigMapInitializer = None\n            ) -> None:\n        store = StoreZipPickle(fp)\n        # config must be None for pickels, will raise otherwise\n        store.write(self.items(), config=config)\n\n    def to_xlsx(self,\n            fp: PathSpecifier,\n            config: StoreConfigMapInitializer = None\n            ) -> None:\n        store = StoreXLSX(fp)\n        config = config if not None else self._config\n        store.write(self.items())\n\n    def to_sqlite(self,\n            fp: PathSpecifier,\n            config: StoreConfigMapInitializer = None\n            ) -> None:\n        store = StoreSQLite(fp)\n        config = config if not None else self._config\n        store.write(self.items())\n\n    def to_hdf5(self,\n            fp: PathSpecifier,\n            config: StoreConfigMapInitializer = None\n            ) -> None:\n        store = StoreHDF5(fp)\n        config = config if not None else self._config\n        store.write(self.items())\n'"
static_frame/core/container.py,67,"b'import typing as tp\n\nfrom itertools import chain\nfrom itertools import product\nfrom functools import wraps\n\nimport operator as operator_mod\nfrom collections import namedtuple\n\nimport numpy as np\n\nfrom static_frame.core.util import EMPTY_TUPLE\nfrom static_frame.core.util import AnyCallable\nfrom static_frame.core.util import DTYPE_INT_KIND\nfrom static_frame.core.util import DTYPE_STR_KIND\nfrom static_frame.core.util import DTYPE_NAN_KIND\nfrom static_frame.core.util import DTYPE_NAT_KIND\nfrom static_frame.core.util import isna_array\n\n# from static_frame.core.util import DTYPE_BOOL\n# from static_frame.core.util import DTYPE_FLOAT_DEFAULT\n\nfrom static_frame.core.util import DTYPES_BOOL\nfrom static_frame.core.util import DTYPES_INEXACT\nfrom static_frame.core.util import DTYPE_FLOAT_DEFAULT\n# from static_frame.core.util import NAT\n\nfrom static_frame.core.doc_str import DOC_TEMPLATE\n\nfrom static_frame.core.display import Display\nfrom static_frame.core.display import DisplayConfig\nfrom static_frame.core.display import DisplayActive\nfrom static_frame.core.display import DisplayFormats\n\nfrom static_frame.core.doc_str import doc_inject\n\n\nif tp.TYPE_CHECKING:\n    from static_frame.core.frame import Frame #pylint: disable=W0611 #pragma: no cover\n\nT = tp.TypeVar(\'T\')\n\n\n_UFUNC_UNARY_OPERATORS = (\n        \'__pos__\',\n        \'__neg__\',\n        \'__abs__\',\n        \'__invert__\')\n\n_UFUNC_BINARY_OPERATORS = (\n        \'__add__\',\n        \'__sub__\',\n        \'__mul__\',\n        \'__matmul__\',\n        \'__truediv__\',\n        \'__floordiv__\',\n        \'__mod__\',\n        #\'__divmod__\', this returns two np.arrays when called on an np array\n        \'__pow__\',\n        \'__lshift__\',\n        \'__rshift__\',\n        \'__and__\',\n        \'__xor__\',\n        \'__or__\',\n        \'__lt__\',\n        \'__le__\',\n        \'__eq__\',\n        \'__ne__\',\n        \'__gt__\',\n        \'__ge__\',\n        )\n\n_UFUNC_OPERATORS_MAP = {k: getattr(operator_mod, k)\n        for k in chain(_UFUNC_UNARY_OPERATORS, _UFUNC_BINARY_OPERATORS)\n        }\n\n# all right are binary\n_RIGHT_OPERATOR_MAP = {\n        \'__radd__\': \'__add__\',\n        \'__rsub__\': \'__sub__\',\n        \'__rmul__\': \'__mul__\',\n        \'__rmatmul__\': \'__matmul__\',\n        \'__rtruediv__\': \'__truediv__\',\n        \'__rfloordiv__\': \'__floordiv__\',\n        }\n\n\n#-------------------------------------------------------------------------------\n# NOTE: this was an approach to doing nan propagation when skipna=False; this approach could not use the out argument, which is used in TypeBlocks. Forcing type coercion to object occasionally is hard to reason about, and makes TypeBlock implementation difficult. Thus, we raise a TypeError.\n\n# def _ufunc_logical_withna(\n#         array: np.ndarray,\n#         isna: np.ndarray,\n#         ufunc: AnyCallable,\n#         axis: int,\n#         element_missing: tp.Any,\n#         out: tp.Optional[np.ndarray] = None\n#         ) -> np.ndarray:\n#     \'\'\'\n#     Perform a logical (and, or) ufunc on an array that has already been identified as having a NULL (as given in the `isna` array), propagating `element_missing` (NaN or NaT) on an axis if ndim > 1.\n#     \'\'\'\n#     if array.ndim == 1:\n#         return element_missing\n#     if out is not None:\n#         # cann use out if we need to do an astype conversion\n#         raise NotImplementedError()\n#     # do not need fill values, as will set to nan after eval\n#     v = array.astype(bool) # object, datetime64 arrays can be converted to bool\n#     v = ufunc(v, axis=axis).astype(object) # get the axis result\n#     v[np.any(isna, axis=axis)] = np.nan # propagate NaN\n#     return v\n\n#-------------------------------------------------------------------------------\ndef _ufunc_logical_skipna(\n        array: np.ndarray,\n        ufunc: AnyCallable,\n        skipna: bool,\n        axis: int = 0,\n        out: tp.Optional[np.ndarray] = None\n        ) -> np.ndarray:\n    \'\'\'\n    Given a logical (and, or) ufunc that does not support skipna, implement skipna behavior.\n    \'\'\'\n    if ufunc != np.all and ufunc != np.any:\n        raise NotImplementedError(f\'unsupported ufunc ({ufunc}); use np.all or np.any\')\n\n    if len(array) == 0:\n        # TODO: handle if this is ndim == 2 and has no length\n        # any() of an empty array is False\n        return ufunc == np.all\n\n    kind = array.dtype.kind\n\n    #---------------------------------------------------------------------------\n    # types that cannot have NA\n    if kind == \'b\':\n        return ufunc(array, axis=axis, out=out)\n    if kind in DTYPE_INT_KIND:\n        return ufunc(array, axis=axis, out=out)\n    if kind in DTYPE_STR_KIND:\n        # only string in object arrays can be converted to bool, where the empty string will be evaluated as False; here, manually check\n        return ufunc(array != \'\', axis=axis, out=out)\n\n    #---------------------------------------------------------------------------\n    # types that can have NA\n\n    if kind in DTYPE_NAN_KIND:\n        isna = isna_array(array)\n        hasna = isna.any() # returns single value for 1d, 2d\n        if hasna and skipna:\n            fill_value = 0.0 if ufunc == np.any else 1.0\n            v = array.copy()\n            v[isna] = fill_value\n            return ufunc(v, axis=axis, out=out)\n        elif hasna and not skipna:\n            # if array.ndim == 1:\n            #     return np.nan\n            raise TypeError(\'cannot propagate NaN without expanding to object array result\')\n        return ufunc(array, axis=axis, out=out)\n\n    if kind in DTYPE_NAT_KIND:\n        isna = isna_array(array)\n        hasna = isna.any() # returns single value for 1d, 2d\n        # all dates are truthy, special handling only to propagate NaNs\n        if hasna and not skipna:\n            # if array.ndim == 1:\n            #     return NAT\n            raise TypeError(\'cannot propagate NaN without expanding to object array result\')\n        # to ignore NaN, simply fall back on all-truth behavior, below\n\n    if kind == \'O\':\n        # all object types: convert to boolean aray then process\n        isna = isna_array(array)\n        hasna = isna.any() # returns single value for 1d, 2d\n        if hasna and skipna:\n            # supply True for np.all, False for np.any\n            fill_value = False if ufunc == np.any else True\n            v = array.copy()\n            v = v.astype(bool) # nan will be converted to True\n            v[isna] = fill_value\n        elif hasna and not skipna:\n            # if array.ndim == 1:\n            #     return np.nan\n            raise TypeError(\'cannot propagate NaN without expanding to object array result\')\n        else:\n            v = array.astype(bool)\n        return ufunc(v, axis=axis, out=out)\n\n    # all types other than strings or objects assume truthy\n    if array.ndim == 1:\n        return True\n    return np.full(array.shape[0 if axis else 1], fill_value=True, dtype=bool)\n\n\ndef _all(array: np.ndarray,\n        axis: int = 0,\n        out: tp.Optional[np.ndarray] = None\n        ) -> np.ndarray:\n    return _ufunc_logical_skipna(array,\n            ufunc=np.all,\n            skipna=False,\n            axis=axis,\n            out=out)\n\n_all.__doc__ = np.all.__doc__\n\ndef _any(array: np.ndarray,\n        axis: int = 0,\n        out: tp.Optional[np.ndarray] = None\n        ) -> np.ndarray:\n    return _ufunc_logical_skipna(array,\n            ufunc=np.any,\n            skipna=False,\n            axis=axis,\n            out=out)\n\n_any.__doc__ = np.any.__doc__\n\ndef _nanall(array: np.ndarray,\n        axis: int = 0,\n        out: tp.Optional[np.ndarray] = None\n        ) -> np.ndarray:\n    return _ufunc_logical_skipna(array,\n            ufunc=np.all,\n            skipna=True,\n            axis=axis,\n            out=out)\n\ndef _nanany(array: np.ndarray,\n        axis: int = 0,\n        out: tp.Optional[np.ndarray] = None\n        ) -> np.ndarray:\n    return _ufunc_logical_skipna(array,\n            ufunc=np.any,\n            skipna=True,\n            axis=axis,\n            out=out)\n\nUfuncSkipnaAttrs = namedtuple(\'UfuncSkipnaAttrs\', (\n        \'ufunc\',\n        \'ufunc_skipna\',\n        \'dtypes\', # iterable of valid dtypes that can be returned; first is default of not match\n        \'composable\', # if partial solutions can be processed per block for axis 1 computations\n        \'doc_header\',\n        \'size_one_unity\', # if the result of the operation on size 1 objects is that value\n))\n\n# class UfuncSkipnaAttrs(tp.NamedTuple):\n#     ufunc: AnyCallable\n#     ufunc_skipna: AnyCallable\n#     dtypes: tp.Tuple[np.dtype, ...] # iterable of valid dtypes that can be returned; first is default of not match\n#     composable: bool # if partial solutions can be processed per block\n#     doc_header: str\n#     size_one_unity: bool # if the result of the operation on size 1 objects is that value\n\n# ufuncs that are applied along an axis, reducing dimensionality. NOTE: as argmin and argmax have iloc/loc interetaions, they are implemetned on derived containers\nUFUNC_AXIS_SKIPNA: tp.Dict[str, UfuncSkipnaAttrs] = {\n        \'all\': UfuncSkipnaAttrs(\n            _all,\n            _nanall,\n            DTYPES_BOOL,\n            True,\n            \'Logical and over values along the specified axis.\',\n            False,\n            ),\n        \'any\': UfuncSkipnaAttrs(\n            _any,\n            _nanany,\n            DTYPES_BOOL,\n            True,\n            \'Logical or over values along the specified axis.\',\n            False, # Overflow amongst hetergenoust tyes accross columns\n            ),\n        \'sum\': UfuncSkipnaAttrs(\n            np.sum,\n            np.nansum,\n            EMPTY_TUPLE, # float or int, row type will match\n            False,\n            \'Sum values along the specified axis.\',\n            True,\n            ),\n        \'min\': UfuncSkipnaAttrs(\n            np.min,\n            np.nanmin,\n            EMPTY_TUPLE,\n            True,\n            \'Return the minimum along the specified axis.\',\n            True,\n            ),\n        \'max\': UfuncSkipnaAttrs(\n            np.max,\n            np.nanmax,\n            EMPTY_TUPLE,\n            True,\n            \'Return the maximum along the specified axis.\',\n            True,\n            ),\n        \'mean\': UfuncSkipnaAttrs(\n            np.mean,\n            np.nanmean,\n            DTYPES_INEXACT, # neads to at least be float, but complex if necessary\n            False,\n            \'Return the mean along the specified axis.\',\n            True,\n            ),\n        \'median\': UfuncSkipnaAttrs(\n            np.median,\n            np.nanmedian,\n            DTYPES_INEXACT,\n            False,\n            \'Return the median along the specified axis.\',\n            True,\n            ),\n        \'std\': UfuncSkipnaAttrs(\n            np.std,\n            np.nanstd,\n            (DTYPE_FLOAT_DEFAULT,), # Ufuncs only return real result.\n            False,\n            \'Return the standard deviaton along the specified axis.\',\n            False,\n            ),\n        \'var\': UfuncSkipnaAttrs(\n            np.var,\n            np.nanvar,\n            (DTYPE_FLOAT_DEFAULT,), # Ufuncs only return real result.\n            False,\n            \'Return the variance along the specified axis.\',\n            False,\n            ),\n        \'prod\': UfuncSkipnaAttrs(\n            np.prod,\n            np.nanprod,\n            EMPTY_TUPLE, # float or int, row type will match\n            False, # Block compbinations with overflow and NaNs require this.\n            \'Return the product along the specified axis.\',\n            True,\n            ),\n        }\n\n# ufuncs that retain the shape and dimensionality\nUFUNC_SHAPE_SKIPNA: tp.Dict[str, UfuncSkipnaAttrs] = {\n        \'cumsum\': UfuncSkipnaAttrs(\n            np.cumsum,\n            np.nancumsum,\n            EMPTY_TUPLE,\n            False,\n            \'Return the cumulative sum over the specified axis.\',\n            True,\n            ),\n        \'cumprod\': UfuncSkipnaAttrs(\n            np.cumprod,\n            np.nancumprod,\n            EMPTY_TUPLE,\n            False,\n            \'Return the cumulative product over the specified axis.\',\n            True,\n            ),\n        }\n\n\n#-------------------------------------------------------------------------------\nclass ContainerMeta(type):\n    \'\'\'Lowest level metaclass for providing interface property on class.\n    \'\'\'\n\n    @property #type: ignore\n    @doc_inject()\n    def interface(cls) -> \'Frame\':\n        \'\'\'{}\'\'\'\n        from static_frame.core.interface import InterfaceSummary\n        return InterfaceSummary.to_frame(cls) #type: ignore\n\n\nclass ContainerOperandMeta(ContainerMeta):\n    \'\'\'Auto-populate binary and unary methods based on instance methods named `_ufunc_unary_operator` and `_ufunc_binary_operator`.\n    \'\'\'\n\n    @staticmethod\n    def create_ufunc_operator(\n            func_name: str,\n            opperand_count: int = 1,\n            reverse: bool = False,\n            ) -> tp.Union[tp.Callable[[tp.Any], tp.Any], tp.Callable[[tp.Any, tp.Any], tp.Any]]:\n        \'\'\'\n        Given a func_name, derive the method to live on the Container.\n        \'\'\'\n        # operator module defines alias to funcs with names like __add__, etc\n        if not reverse:\n            operator_func = getattr(operator_mod, func_name)\n            func_wrapper = operator_func\n        else:\n            unreversed_operator_func = getattr(\n                    operator_mod,\n                    _RIGHT_OPERATOR_MAP[func_name])\n            # flip the order of the arguments\n            operator_func = lambda rhs, lhs: unreversed_operator_func(lhs, rhs)\n            # construct a __name__ that will look the name we get from for the unreversed operator; these are names without the leading and trailing dunders, like ""matmul"", we we just add an r for reverse.\n            operator_func.__name__ = \'r\' + unreversed_operator_func.__name__\n            func_wrapper = unreversed_operator_func\n\n        func: tp.Union[tp.Callable[[tp.Any], tp.Any],\n                tp.Callable[[tp.Any, tp.Any], tp.Any]]\n\n        if opperand_count == 1:\n            assert not reverse # cannot reverse a single opperand\n            def func(self: tp.Any) -> tp.Any: #pylint: disable=E0102\n                return self._ufunc_unary_operator(operator_func)\n        elif opperand_count == 2:\n            def func(self: tp.Any, other: tp.Any) -> tp.Any: #pylint: disable=E0102\n                return self._ufunc_binary_operator(operator=operator_func, other=other)\n        else:\n            raise NotImplementedError() #pragma: no cover\n\n        f = wraps(func_wrapper)(func)\n        f.__name__ = func_name\n        return f\n\n    @staticmethod\n    def create_ufunc_axis_skipna(func_name: str) -> AnyCallable:\n        nt = UFUNC_AXIS_SKIPNA[func_name]\n        ufunc = nt.ufunc\n        ufunc_skipna = nt.ufunc_skipna\n        dtypes = nt.dtypes\n        composable = nt.composable\n        doc = nt.doc_header\n        size_one_unity = nt.size_one_unity\n\n        # these become the common defaults for all of these functions\n        def func(self: tp.Any,\n                axis: int = 0,\n                skipna: bool = True,\n                out: tp.Optional[np.ndarray] = None,\n                ) -> tp.Any:\n            # some NP contexts might call this function with out given as a kwarg; add it here for compatibility, but ensrue it is not being used\n            assert out is None\n            return self._ufunc_axis_skipna(\n                    axis=axis,\n                    skipna=skipna,\n                    ufunc=ufunc,\n                    ufunc_skipna=ufunc_skipna,\n                    composable=composable,\n                    dtypes=dtypes,\n                    size_one_unity=size_one_unity\n                    )\n\n        func.__name__ = func_name\n        func.__doc__ = DOC_TEMPLATE.ufunc_skipna.format(header=doc)\n        return func\n\n\n    @staticmethod\n    def create_ufunc_shape_skipna(func_name: str) -> AnyCallable:\n        # ufunc, ufunc_skipna, dtypes, composable, doc, suze_one_unity = UFUNC_SHAPE_SKIPNA[func_name]\n        nt = UFUNC_SHAPE_SKIPNA[func_name]\n        ufunc = nt.ufunc\n        ufunc_skipna = nt.ufunc_skipna\n        dtypes = nt.dtypes\n        composable = nt.composable\n        doc = nt.doc_header\n        size_one_unity = nt.size_one_unity\n\n        # these become the common defaults for all of these functions\n        def func(self: tp.Any,\n                axis: int = 0,\n                skipna: bool = True,\n                ) -> tp.Any:\n            return self._ufunc_shape_skipna(\n                    axis=axis,\n                    skipna=skipna,\n                    ufunc=ufunc,\n                    ufunc_skipna=ufunc_skipna,\n                    composable=composable,\n                    dtypes=dtypes,\n                    size_one_unity=size_one_unity\n                    )\n\n        func.__name__ = func_name\n        func.__doc__ = DOC_TEMPLATE.ufunc_skipna.format(header=doc)\n        return func\n\n\n    def __new__(mcs, #type: ignore\n            name: str,\n            bases: tp.Tuple[type, ...],\n            attrs: tp.Dict[str, object\n            ]) -> type: #must return a subtype of ""ContainerOperandMeta""\n        \'\'\'\n        Create and assign all autopopulated functions. This __new__ is on the metaclass, not the class, and is thus only called once per class.\n        \'\'\'\n        for opperand_count, func_name in chain(\n                product((1,), _UFUNC_UNARY_OPERATORS),\n                product((2,), _UFUNC_BINARY_OPERATORS)):\n\n            attrs[func_name] = mcs.create_ufunc_operator(\n                    func_name,\n                    opperand_count=opperand_count)\n\n        for func_name in _RIGHT_OPERATOR_MAP:\n            attrs[func_name] = mcs.create_ufunc_operator(\n                    func_name,\n                    opperand_count=2,\n                    reverse=True)\n\n        for func_name in UFUNC_AXIS_SKIPNA:\n            attrs[func_name] = mcs.create_ufunc_axis_skipna(func_name)\n\n        for func_name in UFUNC_SHAPE_SKIPNA:\n            attrs[func_name] = mcs.create_ufunc_shape_skipna(func_name)\n\n        return type.__new__(mcs, name, bases, attrs)\n\n\nclass ContainerBase(metaclass=ContainerMeta):\n    \'\'\'\n    Root of all containers. Most containers, like Series, Frame, and Index, inherit from ContainerOperaand; only Bus inherits from ContainerBase.\n    \'\'\'\n    __slots__ = EMPTY_TUPLE\n\n    #---------------------------------------------------------------------------\n    # class attrs\n\n    STATIC: bool = True\n\n    #---------------------------------------------------------------------------\n    # common display functions\n\n    @property #type: ignore\n    @doc_inject()\n    def interface(self) -> \'Frame\':\n        \'\'\'{}\'\'\'\n        from static_frame.core.interface import InterfaceSummary\n        return InterfaceSummary.to_frame(self.__class__)\n\n    def display(self,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> Display:\n        raise NotImplementedError()\n\n    def __repr__(self) -> str:\n        return repr(self.display())\n\n    @doc_inject(selector=\'display\')\n    def display_tall(self,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> Display:\n        \'\'\'Maximize vertical presentation. {doc}\n\n        Args:\n            {config}\n        \'\'\'\n        config = config or DisplayActive.get()\n        args = config.to_dict()\n        args.update(dict(\n                display_rows=np.inf,\n                cell_max_width=np.inf,\n                cell_max_width_leftmost=np.inf,\n                ))\n        return self.display(config=DisplayConfig(**args))\n\n    @doc_inject(selector=\'display\')\n    def display_wide(self,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> Display:\n        \'\'\'Maximize horizontal presentation. {doc}\n\n        Args:\n            {config}\n        \'\'\'\n        config = config or DisplayActive.get()\n        args = config.to_dict()\n        args.update(dict(\n                display_columns=np.inf,\n                cell_max_width=np.inf,\n                cell_max_width_leftmost=np.inf,\n                ))\n        return self.display(config=DisplayConfig(**args))\n\n\n\n\nclass ContainerOperand(ContainerBase, metaclass=ContainerOperandMeta):\n    \'\'\'Base class of all containers that support opperators.\'\'\'\n\n    __slots__ = EMPTY_TUPLE\n\n    interface: \'Frame\' # property that returns a Frame\n    values: np.ndarray\n\n    __pos__: tp.Callable[[T], T]\n    __neg__: tp.Callable[[T], T]\n    __abs__: tp.Callable[[T], T]\n    __invert__: tp.Callable[[T], T]\n    __add__: tp.Callable[[T, object], T]\n    __sub__: tp.Callable[[T, object], T]\n    __mul__: tp.Callable[[T, object], T]\n    __matmul__: tp.Callable[[T, object], T]\n    __truediv__: tp.Callable[[T, object], T]\n    __floordiv__: tp.Callable[[T, object], T]\n    __mod__: tp.Callable[[T, object], T]\n    # __divmod__: tp.Callable[[T, object], T]\n    __pow__: tp.Callable[[T, object], T]\n    __lshift__: tp.Callable[[T, object], T]\n    __rshift__: tp.Callable[[T, object], T]\n    __and__: tp.Callable[[T, object], T]\n    __xor__: tp.Callable[[T, object], T]\n    __or__: tp.Callable[[T, object], T]\n    __lt__: tp.Callable[[T, object], T]\n    __le__: tp.Callable[[T, object], T]\n    __eq__: tp.Callable[[T, object], T]  #type: ignore\n    __ne__: tp.Callable[[T, object], T]  #type: ignore\n    __gt__: tp.Callable[[T, object], T]\n    __ge__: tp.Callable[[T, object], T]\n    __radd__: tp.Callable[[T, object], T]\n    __rsub__: tp.Callable[[T, object], T]\n    __rmul__: tp.Callable[[T, object], T]\n    __rtruediv__: tp.Callable[[T, object], T]\n    __rfloordiv__: tp.Callable[[T, object], T]\n\n\n    # methods are overwritten by metaclass, but defined here for typing\n\n    def all(self, axis: int = 0, skipna: bool = True) -> bool:\n        raise NotImplementedError() # pragma: no cover\n\n    def any(self, axis: int = 0, skipna: bool = True) -> bool:\n        raise NotImplementedError() # pragma: no cover\n\n    def sum(self, axis: int = 0, skipna: bool = True) -> tp.Any:\n        raise NotImplementedError() # pragma: no cover\n\n    def min(self, axis: int = 0, skipna: bool = True) -> tp.Any:\n        raise NotImplementedError() # pragma: no cover\n\n    def max(self, axis: int = 0, skipna: bool = True) -> tp.Any:\n        raise NotImplementedError() # pragma: no cover\n\n    def mean(self, axis: int = 0, skipna: bool = True) -> tp.Any:\n        raise NotImplementedError() # pragma: no cover\n\n    def median(self, axis: int = 0, skipna: bool = True) -> tp.Any:\n        raise NotImplementedError() # pragma: no cover\n\n    def std(self, axis: int = 0, skipna: bool = True) -> tp.Any:\n        raise NotImplementedError() # pragma: no cover\n\n    def var(self, axis: int = 0, skipna: bool = True) -> tp.Any:\n        raise NotImplementedError() # pragma: no cover\n\n    def prod(self, axis: int = 0, skipna: bool = True) -> tp.Any:\n        raise NotImplementedError() # pragma: no cover\n\n    def cumsum(self, axis: int = 0, skipna: bool = True) -> tp.Any:\n        raise NotImplementedError() # pragma: no cover\n\n    def cumprod(self, axis: int = 0, skipna: bool = True) -> tp.Any:\n        raise NotImplementedError() # pragma: no cover\n\n\n    def _repr_html_(self) -> str:\n        \'\'\'\n        Provide HTML representation for Jupyter Notebooks.\n        \'\'\'\n        # NOTE: We observe that Jupyter will window big content into scrollable component, so do not limit output and introduce ellipsis.\n\n        config = DisplayActive.get(\n                display_format=DisplayFormats.HTML_TABLE,\n                type_show=False,\n                display_columns=np.inf,\n                display_rows=np.inf,\n                )\n        # modify the active display to be for HTML\n        return repr(self.display(config))'"
static_frame/core/container_util.py,39,"b'\'\'\'\nThis module us for utilty functions that take as input and / or return Container subclasses such as Index, Series, or Frame, and that need to be shared by multiple such Container classes.\n\'\'\'\n\nfrom collections import defaultdict\nfrom itertools import zip_longest\n\nimport numpy as np\nfrom numpy import char as npc\n\nimport typing as tp\n\nif tp.TYPE_CHECKING:\n    import pandas as pd #pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.type_blocks import TypeBlocks #pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.series import Series #pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.frame import Frame #pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.index_hierarchy import IndexHierarchy #pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.index_auto import IndexAutoFactoryType #pylint: disable=W0611 #pragma: no cover\n\nfrom static_frame.core.util import IndexConstructor\nfrom static_frame.core.util import IndexConstructors\nfrom static_frame.core.util import IndexInitializer\nfrom static_frame.core.util import STATIC_ATTR\nfrom static_frame.core.util import AnyCallable\nfrom static_frame.core.util import NULL_SLICE\nfrom static_frame.core.util import Bloc2DKeyType\nfrom static_frame.core.util import DtypesSpecifier\nfrom static_frame.core.util import slice_to_ascending_slice\nfrom static_frame.core.util import GetItemKeyType\nfrom static_frame.core.util import DEFAULT_SORT_KIND\nfrom static_frame.core.util import iterable_to_array_1d\nfrom static_frame.core.util import UFunc\nfrom static_frame.core.util import column_2d_filter\n\nfrom static_frame.core.util import DTYPE_OBJECT\nfrom static_frame.core.util import DTYPE_STR\nfrom static_frame.core.util import DTYPE_BOOL\nfrom static_frame.core.util import DTYPE_STR_KIND\nfrom static_frame.core.util import DepthLevelSpecifier\n\nfrom static_frame.core.index_base import IndexBase\n\n\n\ndef dtypes_mappable(dtypes: DtypesSpecifier) -> bool:\n    \'\'\'\n    Determine if the dtypes argument can be used by name lookup, rather than index.\n    \'\'\'\n    from static_frame.core.series import Series\n    return isinstance(dtypes, (dict, Series))\n\n\ndef is_static(value: IndexConstructor) -> bool:\n    try:\n        # if this is a class constructor\n        return getattr(value, STATIC_ATTR) #type: ignore\n    except AttributeError:\n        pass\n    # assume this is a class method\n    return getattr(value.__self__, STATIC_ATTR) #type: ignore\n\n\ndef pandas_version_under_1() -> bool:\n    import pandas\n    return not hasattr(pandas, \'NA\') # object introduced in 1.0\n\ndef pandas_to_numpy(\n        container: tp.Union[\'pd.Index\', \'pd.Series\', \'pd.DataFrame\'],\n        own_data: bool,\n        fill_value: tp.Any = np.nan\n        ) -> np.ndarray:\n    \'\'\'Convert Pandas container to a numpy array in pandas 1.0, where we might have Pandas extension dtypes that may have pd.NA. If no pd.NA, can go back to numpy types.\n\n    If coming from a Pandas extension type, will convert pd.NA to `fill_value` in the resulting object array. For object dtypes, pd.NA may pass on into SF; the only way to find them is an expensive iteration and `is` comparison, which we are not sure we want to do at this time.\n\n    Args:\n        fill_value: if replcaing pd.NA, what to replace it with. Ultimately, this can use FillValueAuto to avoid going to object in all cases.\n    \'\'\'\n    # NOTE: only to be used with pandas 1.0 and greater\n\n    if container.ndim == 1: # Series, Index\n        dtype_src = container.dtype\n        ndim = 1\n    elif container.ndim == 2: # DataFrame, assume contiguous dtypes\n        dtypes = container.dtypes.unique()\n        assert len(dtypes) == 1\n        dtype_src = dtypes[0]\n        ndim = 2\n    else:\n        raise NotImplementedError(f\'no handling for ndim {container.ndim}\') #pragma: no cover\n\n    if isinstance(dtype_src, np.dtype):\n        dtype = dtype_src\n        is_extension_dtype = False\n    elif hasattr(dtype_src, \'numpy_dtype\'):\n        # only int, uint dtypes have this attribute\n        dtype = dtype_src.numpy_dtype\n        is_extension_dtype = True\n    else:\n        dtype = None # resolve below\n        is_extension_dtype = True\n\n    if is_extension_dtype:\n        isna = container.isna() # returns a NumPy Boolean type\n        hasna = isna.values.any() # will work for ndim 1 and 2\n\n        from pandas import StringDtype #pylint: disable=E0611\n        from pandas import BooleanDtype #pylint: disable=E0611\n        # from pandas import DatetimeTZDtype\n        # from pandas import Int8Dtype\n        # from pandas import Int16Dtype\n        # from pandas import Int32Dtype\n        # from pandas import Int64Dtype\n        # from pandas import UInt16Dtype\n        # from pandas import UInt32Dtype\n        # from pandas import UInt64Dtype\n        # from pandas import UInt8Dtype\n\n        if isinstance(dtype_src, BooleanDtype):\n            dtype = DTYPE_OBJECT if hasna else DTYPE_BOOL\n        elif isinstance(dtype_src, StringDtype):\n            # trying to use a dtype argument for strings results in a converting pd.NA to a string ""<NA>""\n            dtype = DTYPE_OBJECT if hasna else DTYPE_STR\n        else:\n            # if an extension type and it hasna, have to go to object; otherwise, set to None or the dtype obtained above\n            dtype = DTYPE_OBJECT if hasna else dtype\n\n        try:\n            array = container.to_numpy(copy=not own_data, dtype=dtype)\n        except (ValueError, TypeError):\n            # cannot convert to \'<class \'int\'>\'-dtype NumPy array with missing values. Specify an appropriate \'na_value\' for this dtype; this will go to object\n            # TypeError: boolean value of NA is ambiguous\n            array = container.to_numpy(copy=not own_data)\n\n        if hasna:\n            # if hasna and extension dtype, should be an object array; please pd.NA objects with fill_value (np.nan)\n            assert array.dtype == DTYPE_OBJECT\n            array[isna] = fill_value\n\n    else: # not an extension dtype\n        if own_data:\n            array = container.values\n        else:\n            array = container.values.copy()\n\n    array.flags.writeable = False\n    return array\n\n\n\ndef index_from_optional_constructor(\n        value: IndexInitializer,\n        *,\n        default_constructor: IndexConstructor,\n        explicit_constructor: tp.Optional[IndexConstructor] = None,\n        ) -> IndexBase:\n    \'\'\'\n    Given a value that is an IndexInitializer (which means it might be an Index), determine if that value is really an Index, and if so, determine if a copy has to be made; otherwise, use the default_constructor. If an explicit_constructor is given, that is always used.\n    \'\'\'\n    # NOTE: this might return an own_index flag to show callers when a new index has been created\n\n    if explicit_constructor:\n        return explicit_constructor(value)\n\n    # default constructor could be a function with a STATIC attribute\n    if isinstance(value, IndexBase):\n        # if default is STATIC, and value is not STATIC, get an immutabel\n        if is_static(default_constructor):\n            if not value.STATIC:\n                # v: ~S, dc: S, use immutable alternative\n                return value._IMMUTABLE_CONSTRUCTOR(value)\n            # v: S, dc: S, both immutable\n            return value\n        else: # default constructor is mutable\n            if not value.STATIC:\n                # v: ~S, dc: ~S, both are mutable\n                return value.copy()\n            # v: S, dc: ~S, return a mutable version of something that is not mutable\n            return value._MUTABLE_CONSTRUCTOR(value)\n\n    # cannot always deterine satic status from constructors; fallback on using default constructor\n    return default_constructor(value)\n\ndef index_constructor_empty(\n        index: tp.Union[IndexInitializer, \'IndexAutoFactoryType\']\n        ) -> bool:\n    \'\'\'\n    Determine if an index is empty (if possible) or an IndexAutoFactory.\n    \'\'\'\n    from static_frame.core.index_auto import IndexAutoFactory\n    return index is None or index is IndexAutoFactory or (\n            hasattr(index, \'__len__\') and len(index) == 0) #type: ignore\n\ndef matmul(\n        lhs: tp.Union[\'Series\', \'Frame\', np.ndarray],\n        rhs: tp.Union[\'Series\', \'Frame\', np.ndarray],\n        ) -> tp.Any: #tp.Union[\'Series\', \'Frame\']:\n    \'\'\'\n    Implementation of matrix multiplication for Series and Frame\n    \'\'\'\n    from static_frame.core.series import Series\n    from static_frame.core.frame import Frame\n\n    # for a @ b = c\n    # if a is 2D: a.columns must align b.index\n    # if b is 1D, a.columns bust align with b.index\n    # if a is 1D: len(a) == b.index (len of b), returns w columns of B\n\n    if not isinstance(rhs, (np.ndarray, Series, Frame)):\n        # try to make it into an array\n        rhs = np.array(rhs)\n\n    if not isinstance(lhs, (np.ndarray, Series, Frame)):\n        # try to make it into an array\n        lhs = np.array(lhs)\n\n    if isinstance(lhs, np.ndarray):\n        lhs_type = np.ndarray\n    elif isinstance(lhs, Series):\n        lhs_type = Series\n    else: # normalize subclasses\n        lhs_type = Frame\n\n    if isinstance(rhs, np.ndarray):\n        rhs_type = np.ndarray\n    elif isinstance(rhs, Series):\n        rhs_type = Series\n    else: # normalize subclasses\n        rhs_type = Frame\n\n    if rhs_type == np.ndarray and lhs_type == np.ndarray:\n        return np.matmul(lhs, rhs)\n\n\n    own_index = True\n    constructor = None\n\n    if lhs.ndim == 1: # Series, 1D array\n        # result will be 1D or 0D\n        columns = None\n\n        if lhs_type == Series and (rhs_type == Series or rhs_type == Frame):\n            aligned = lhs._index.union(rhs._index)\n            # if the aligned shape is not the same size as the originals, we do not have the same values in each and cannot proceed (all values go to NaN)\n            if len(aligned) != len(lhs._index) or len(aligned) != len(rhs._index):\n                raise RuntimeError(\'shapes not alignable for matrix multiplication\') #pragma: no cover\n\n        if lhs_type == Series:\n            if rhs_type == np.ndarray:\n                if lhs.shape[0] != rhs.shape[0]: # works for 1D and 2D\n                    raise RuntimeError(\'shapes not alignable for matrix multiplication\')\n                ndim = rhs.ndim - 1 # if 2D, result is 1D, of 1D, result is 0\n                left = lhs.values\n                right = rhs # already np\n                if ndim == 1:\n                    index = None # force auto increment integer\n                    own_index = False\n                    constructor = lhs.__class__\n            elif rhs_type == Series:\n                ndim = 0\n                left = lhs.reindex(aligned).values\n                right = rhs.reindex(aligned).values\n            else: # rhs is Frame\n                ndim = 1\n                left = lhs.reindex(aligned).values\n                right = rhs.reindex(index=aligned).values\n                index = rhs._columns\n                constructor = lhs.__class__\n        else: # lhs is 1D array\n            left = lhs\n            right = rhs.values\n            if rhs_type == Series:\n                ndim = 0\n            else: # rhs is Frame, len(lhs) == len(rhs.index)\n                ndim = 1\n                index = rhs._columns\n                constructor = Series # cannot get from argument\n\n    elif lhs.ndim == 2: # Frame, 2D array\n\n        if lhs_type == Frame and (rhs_type == Series or rhs_type == Frame):\n            aligned = lhs._columns.union(rhs._index)\n            # if the aligned shape is not the same size as the originals, we do not have the same values in each and cannot proceed (all values go to NaN)\n            if len(aligned) != len(lhs._columns) or len(aligned) != len(rhs._index):\n                raise RuntimeError(\'shapes not alignable for matrix multiplication\')\n\n        if lhs_type == Frame:\n            if rhs_type == np.ndarray:\n                if lhs.shape[1] != rhs.shape[0]: # works for 1D and 2D\n                    raise RuntimeError(\'shapes not alignable for matrix multiplication\')\n                ndim = rhs.ndim\n                left = lhs.values\n                right = rhs # already np\n                index = lhs._index\n\n                if ndim == 1:\n                    constructor = Series\n                else:\n                    constructor = lhs.__class__\n                    columns = None # force auto increment index\n            elif rhs_type == Series:\n                # a.columns must align with b.index\n                ndim = 1\n                left = lhs.reindex(columns=aligned).values\n                right = rhs.reindex(aligned).values\n                index = lhs._index  # this axis is not changed\n                constructor = rhs.__class__\n            else: # rhs is Frame\n                # a.columns must align with b.index\n                ndim = 2\n                left = lhs.reindex(columns=aligned).values\n                right = rhs.reindex(index=aligned).values\n                index = lhs._index\n                columns = rhs._columns\n                constructor = lhs.__class__ # give left precedence\n        else: # lhs is 2D array\n            left = lhs\n            right = rhs.values\n            if rhs_type == Series: # returns unindexed Series\n                ndim = 1\n                index = None\n                own_index = False\n                constructor = rhs.__class__\n            else: # rhs is Frame, lhs.shape[1] == rhs.shape[0]\n                if lhs.shape[1] != rhs.shape[0]: # works for 1D and 2D\n                    raise RuntimeError(\'shapes not alignable for matrix multiplication\')\n                ndim = 2\n                index = None\n                own_index = False\n                columns = rhs._columns\n                constructor = rhs.__class__\n\n    # NOTE: np.matmul is not the same as np.dot for some arguments\n    data = np.matmul(left, right)\n\n    if ndim == 0:\n        return data\n\n    assert constructor is not None\n\n    data.flags.writeable = False\n    if ndim == 1:\n        return constructor(data,\n                index=index,\n                own_index=own_index,\n                )\n    return constructor(data,\n            index=index,\n            own_index=own_index,\n            columns=columns\n            )\n\n\ndef axis_window_items( *,\n        source: tp.Union[\'Series\', \'Frame\'],\n        size: int,\n        axis: int = 0,\n        step: int = 1,\n        window_sized: bool = True,\n        window_func: tp.Optional[AnyCallable] = None,\n        window_valid: tp.Optional[AnyCallable] = None,\n        label_shift: int = 0,\n        start_shift: int = 0,\n        size_increment: int = 0,\n        as_array: bool = False,\n        ) -> tp.Iterator[tp.Tuple[tp.Hashable, tp.Any]]:\n    \'\'\'Generator of index, window pairs pairs.\n\n    Args:\n        size: integer greater than 0\n        step: integer greater than 0 to determine the step size between windows. A step of 1 shifts the window 1 data point; a step equal to window size results in non-overlapping windows.\n        window_sized: if True, windows that do not meet the size are skipped.\n        window_func: Array processor of window values, pre-function application; useful for applying weighting to the window.\n        window_valid: Function that, given an array window, returns True if the window meets requirements and should be returned.\n        label_shift: shift, relative to the right-most data point contained in the window, to derive the label to be paired with the window; e.g., to return the first label of the window, the shift will be the size minus one.\n        start_shift: shift from 0 to determine where the collection of windows begins.\n        size_increment: value to be added to each window aftert the first, so as to, in combination with setting the step size to 0, permit expanding windows.\n        as_array: if True, the window is returned as an array instead of a SF object.\n    \'\'\'\n    from static_frame.core.frame import Frame\n    from static_frame.core.series import Series\n\n    if size <= 0:\n        raise RuntimeError(\'window size must be greater than 0\')\n    if step < 0:\n        raise RuntimeError(\'window step cannot be less than than 0\')\n\n    source_ndim = source.ndim\n\n    if source_ndim == 1:\n        assert isinstance(source, Series) # for mypy\n        labels = source._index\n        if as_array:\n            values = source.values\n    else:\n        assert isinstance(source, Frame) # for mypy\n        labels = source._index if axis == 0 else source._columns\n        if as_array:\n            values = source._blocks.values\n\n    if start_shift >= 0:\n        count_window_max = len(labels)\n    else: # add for iterations when less than 0\n        count_window_max = len(labels) + abs(start_shift)\n\n    idx_left_max = count_window_max - 1\n    idx_left = start_shift\n    count = 0\n\n    while True:\n        # idx_left, size can change over iterations\n        idx_right = idx_left + size - 1\n\n        # floor idx_left at 0 so as to not wrap\n        idx_left_floored = max(idx_left, 0)\n        idx_right_floored = max(idx_right, -1) # will add one\n\n        key = slice(idx_left_floored, idx_right_floored + 1)\n\n        if source_ndim == 1:\n            if as_array:\n                window = values[key]\n            else:\n                window = source._extract_iloc(key)\n        else:\n            if axis == 0:\n                if as_array:\n                    window = values[key]\n                else: # use low level iloc selector\n                    window = source._extract(row_key=key) #type: ignore\n            else:\n                if as_array:\n                    window = values[NULL_SLICE, key]\n                else:\n                    window = source._extract(column_key=key) #type: ignore\n\n        valid = True\n        try:\n            idx_label = idx_right + label_shift\n            if idx_label < 0: # do not wrap around\n                raise IndexError()\n            #if we cannot get a lable, the window is invalid\n            label = labels.iloc[idx_label]\n        except IndexError: # an invalid label has to be dropped\n            valid = False\n\n        if valid and window_sized and window.shape[axis] != size:\n            valid = False\n        if valid and window_valid and not window_valid(window):\n            valid = False\n\n        if valid:\n            if window_func:\n                window = window_func(window)\n            yield label, window\n\n        idx_left += step\n        size += size_increment\n        count += 1\n\n        # import ipdb; ipdb.set_trace()\n\n        if count > count_window_max or idx_left > idx_left_max or size < 0:\n            break\n\n\ndef bloc_key_normalize(\n        key: Bloc2DKeyType,\n        container: \'Frame\'\n        ) -> np.ndarray:\n    \'\'\'\n    Normalize and validate a bloc key. Return a same sized Boolean array.\n    \'\'\'\n    from static_frame.core.frame import Frame\n\n    if isinstance(key, Frame):\n        bloc_frame = key.reindex(\n                index=container._index,\n                columns=container._columns,\n                fill_value=False\n                )\n        bloc_key = bloc_frame.values # shape must match post reindex\n    elif isinstance(key, np.ndarray):\n        bloc_key = key\n        if bloc_key.shape != container.shape:\n            raise RuntimeError(f\'bloc {bloc_key.shape} must match shape {container.shape}\')\n    else:\n        raise RuntimeError(f\'invalid bloc_key, must be Frame or array, not {key}\')\n\n    if not bloc_key.dtype == bool:\n        raise RuntimeError(\'cannot use non-Bolean dtype as bloc key\')\n\n    return bloc_key\n\n\ndef key_to_ascending_key(key: GetItemKeyType, size: int) -> GetItemKeyType:\n    \'\'\'\n    Normalize all types of keys into an ascending formation.\n\n    Args:\n        size: the length of the container on this axis\n    \'\'\'\n    from static_frame.core.frame import Frame\n    from static_frame.core.series import Series\n\n    if isinstance(key, slice):\n        return slice_to_ascending_slice(key, size=size)\n\n    if isinstance(key, str) or not hasattr(key, \'__len__\'):\n        return key\n\n    if isinstance(key, np.ndarray):\n        # array first as not truthy\n        return np.sort(key, kind=DEFAULT_SORT_KIND)\n\n    if not key:\n        return key\n\n    if isinstance(key, list):\n        return sorted(key)\n\n    if isinstance(key, Series):\n        return key.sort_index()\n\n    if isinstance(key, Frame):\n        # for usage in assignment we need columns to be sorted\n        return key.sort_columns()\n\n    raise RuntimeError(f\'unhandled key {key}\')\n\n\ndef rehierarch_from_type_blocks(*,\n        labels: \'TypeBlocks\',\n        depth_map: tp.Sequence[int],\n        index_cls: tp.Type[\'IndexHierarchy\'],\n        index_constructors: tp.Optional[IndexConstructors] = None,\n        name: tp.Optional[tp.Hashable] = None,\n        ) -> tp.Tuple[\'IndexBase\', np.ndarray]:\n    \'\'\'\n    Given labels suitable for a hierarchical index, order them into a hierarchy using the given depth_map.\n\n    Args:\n        index_cls: provide a class, form which the constructor will be called.\n    \'\'\'\n\n    depth = labels.shape[1] # number of columns\n\n    if depth != len(depth_map):\n        raise RuntimeError(\'must specify new depths for all depths\')\n    if set(range(depth)) != set(depth_map):\n        raise RuntimeError(\'all depths must be specified\')\n\n    labels_post = labels._extract(row_key=NULL_SLICE, column_key=list(depth_map))\n    labels_sort = np.full(labels_post.shape, 0)\n\n    # get ordering of values found in each level\n    order: tp.List[tp.Dict[tp.Hashable, int]] = [defaultdict(int) for _ in range(depth)]\n\n    for (idx_row, idx_col), label in labels.element_items():\n        if label not in order[idx_col]:\n            # Map label to an integer representing the observed order.\n            order[idx_col][label] = len(order[idx_col])\n        # Fill array for sorting based on observed order.\n        labels_sort[idx_row, idx_col] = order[idx_col][label]\n\n    # Reverse depth_map for lexical sorting, which sorts by rightmost column first.\n    order_lex = np.lexsort(\n            [labels_sort[NULL_SLICE, i] for i in reversed(depth_map)])\n\n    labels_post = labels_post._extract(row_key=order_lex)\n\n    index = index_cls._from_type_blocks(\n            blocks=labels_post,\n            index_constructors=index_constructors,\n            name=name,\n            own_blocks=True,\n            )\n    return index, order_lex\n\ndef rehierarch_from_index_hierarchy(*,\n        labels: \'IndexHierarchy\',\n        depth_map: tp.Sequence[int],\n        index_constructors: tp.Optional[IndexConstructors] = None,\n        name: tp.Optional[tp.Hashable] = None,\n        ) -> tp.Tuple[\'IndexBase\', np.ndarray]:\n    \'\'\'\n    Alternate interface that updates IndexHierarchy cache before rehierarch.\n    \'\'\'\n    if labels._recache:\n        labels._update_array_cache()\n\n    return rehierarch_from_type_blocks(\n            labels=labels._blocks,\n            depth_map=depth_map,\n            index_cls=labels.__class__,\n            index_constructors=index_constructors,\n            name=name,\n            )\n\ndef array_from_value_iter(\n        key: tp.Hashable,\n        idx: int,\n        get_value_iter: tp.Callable[[tp.Hashable], tp.Iterator[tp.Any]],\n        get_col_dtype: tp.Optional[tp.Callable[[int], np.dtype]],\n        row_count: int,\n        ) -> np.ndarray:\n    \'\'\'\n    Return a single array given keys and collections.\n\n    Args:\n        get_value_iter: Iterator of a values\n        dtypes: if an\n        key: hashable for looking up field in `get_value_iter`.\n        idx: integer position to extract from dtypes\n    \'\'\'\n    # for each column, try to get a dtype, or None\n    if get_col_dtype is None:\n        dtype = None\n    else: # dtype returned here can be None.\n        dtype = get_col_dtype(idx)\n        # if this value is None we cannot tell if it was explicitly None or just was not specified\n\n    # NOTE: shown to be faster to try fromiter in some performance tests\n    # values, _ = iterable_to_array_1d(get_value_iter(key), dtype=dtype)\n\n    values = None\n    if dtype is not None:\n        try:\n            values = np.fromiter(\n                    get_value_iter(key),\n                    count=row_count,\n                    dtype=dtype)\n            values.flags.writeable = False\n        except (ValueError, TypeError):\n            # the dtype may not be compatible, so must fall back on using np.array to determine the type, i.e., ValueError: cannot convert float NaN to integer\n            pass\n    if values is None:\n        # returns an immutable array\n        values, _ = iterable_to_array_1d(\n                get_value_iter(key),\n                dtype=dtype\n                )\n    return values\n\n\ndef apply_binary_operator(*,\n        values: np.ndarray,\n        other: tp.Any,\n        other_is_array: bool,\n        operator: UFunc,\n        ) -> np.ndarray:\n    \'\'\'\n    Utility to handle binary operator application.\n    \'\'\'\n\n    if (values.dtype.kind in DTYPE_STR_KIND or\n            (other_is_array and other.dtype.kind in DTYPE_STR_KIND)):\n        operator_name = operator.__name__\n\n        if operator_name == \'add\':\n            result = npc.add(values, other)\n        elif operator_name == \'radd\':\n            result = npc.add(other, values)\n        elif operator_name == \'mul\' or operator_name == \'rmul\':\n            result = npc.multiply(values, other)\n        else:\n            result = operator(values, other)\n    else:\n        result = operator(values, other)\n\n    if result is False or result is True:\n        # in comparison to Booleans, if values is of length 1 and a character type, we will get a Boolean back, not an array; this issues the following warning: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n        # if the arguement is a tuple of length equal to an erray, NP will perform element wise comparison; but if the argment is a tuple of length greater or equal, each value in value will be compared to that tuple\n        result = np.full(values.shape, result)\n\n    result.flags.writeable = False\n    return result\n\n\ndef apply_binary_operator_blocks(*,\n        values: tp.Iterable[np.ndarray],\n        other: tp.Iterable[np.ndarray],\n        operator: UFunc,\n        apply_column_2d_filter: bool,\n    ) -> tp.Iterator[np.ndarray]:\n    \'\'\'\n    Application from iterators of arrays, to iterators of arrays.\n    \'\'\'\n\n    if apply_column_2d_filter:\n        values = (column_2d_filter(op) for op in values)\n        other = (column_2d_filter(op) for op in other)\n\n    for a, b in zip_longest(values, other):\n        yield apply_binary_operator(\n                values=a,\n                other=b,\n                other_is_array=True,\n                operator=operator,\n                )\n\n\ndef arrays_from_index_frame(\n        container: \'Frame\',\n        depth_level: tp.Optional[DepthLevelSpecifier],\n        columns: GetItemKeyType\n        ) -> tp.Iterator[np.ndarray]:\n    \'\'\'\n    Given a Frame, return an iterator of index and / or columns as 1D or 2D arrays.\n    \'\'\'\n    if depth_level is not None:\n        # NOTE: a 1D index of tuples will be taken as a 1D array of tuples; there is no obvious way to treat this as 2D array without guessing that we are trying to match an IndexHierarchy\n        # NOTE: if a multi-column selection, might be better to yield one depth at a time\n        yield container.index.values_at_depth(depth_level)\n    if columns is not None:\n        column_key = container.columns.loc_to_iloc(columns)\n        yield from container._blocks._slice_blocks(column_key=column_key)\n'"
static_frame/core/display.py,40,"b'import typing as tp\nimport sys\nimport json\nimport os\nimport html\nimport inspect\nimport platform\nimport re\n\n\nfrom enum import Enum\n\nfrom functools import partial\nfrom collections import namedtuple\n\nimport numpy as np\n\n\nfrom static_frame.core.util import _gen_skip_middle\nfrom static_frame.core.display_color import HexColor\nfrom static_frame.core import display_html_datatables\n\nfrom static_frame.core.util import DTYPE_INT_KIND\nfrom static_frame.core.util import DTYPE_STR_KIND\nfrom static_frame.core.util import FLOAT_TYPES\nfrom static_frame.core.util import COMPLEX_TYPES\n\n_module = sys.modules[__name__]\n\nColorConstructor = tp.Union[int, str]\n\n#-------------------------------------------------------------------------------\n# display infrastructure\n\n#-------------------------------------------------------------------------------\nclass DisplayTypeCategory:\n    \'\'\'\n    Display Type Categories are used for identifying types to which to apply specific formatting.\n    \'\'\'\n    CONFIG_ATTR = \'type_color_default\'\n\n    @staticmethod\n    def in_category(t: tp.Union[type, np.dtype]) -> bool: #pylint: disable=W0613\n        return True\n\n\nclass DisplayTypeInt(DisplayTypeCategory):\n    CONFIG_ATTR = \'type_color_int\'\n\n    @staticmethod\n    def in_category(t: tp.Union[type, np.dtype]) -> bool:\n        return isinstance(t, np.dtype) and t.kind in DTYPE_INT_KIND\n\nclass DisplayTypeFloat(DisplayTypeCategory):\n    CONFIG_ATTR = \'type_color_float\'\n\n    @staticmethod\n    def in_category(t: tp.Union[type, np.dtype]) -> bool:\n        return isinstance(t, np.dtype) and t.kind == \'f\'\n\nclass DisplayTypeComplex(DisplayTypeCategory):\n    CONFIG_ATTR = \'type_color_complex\'\n\n    @staticmethod\n    def in_category(t: tp.Union[type, np.dtype]) -> bool:\n        return isinstance(t, np.dtype) and t.kind == \'c\'\n\nclass DisplayTypeBool(DisplayTypeCategory):\n    CONFIG_ATTR = \'type_color_bool\'\n\n    @staticmethod\n    def in_category(t: tp.Union[type, np.dtype]) -> bool:\n        return isinstance(t, np.dtype) and t.kind == \'b\'\n\nclass DisplayTypeObject(DisplayTypeCategory):\n    CONFIG_ATTR = \'type_color_object\'\n\n    @staticmethod\n    def in_category(t: tp.Union[type, np.dtype]) -> bool:\n        return isinstance(t, np.dtype) and t.kind == \'O\'\n\nclass DisplayTypeStr(DisplayTypeCategory):\n    CONFIG_ATTR = \'type_color_str\'\n\n    @staticmethod\n    def in_category(t: tp.Union[type, np.dtype]) -> bool:\n        return isinstance(t, np.dtype) and t.kind in DTYPE_STR_KIND\n\nclass DisplayTypeDateTime(DisplayTypeCategory):\n    CONFIG_ATTR = \'type_color_datetime\'\n\n    @staticmethod\n    def in_category(t: tp.Union[type, np.dtype]) -> bool:\n        return isinstance(t, np.dtype) and t.kind == \'M\'\n\nclass DisplayTypeTimeDelta(DisplayTypeCategory):\n    CONFIG_ATTR = \'type_color_timedelta\'\n\n    @staticmethod\n    def in_category(t: tp.Union[type, np.dtype]) -> bool:\n        return isinstance(t, np.dtype) and t.kind == \'m\'\n\n\nclass DisplayTypeIndex(DisplayTypeCategory):\n    CONFIG_ATTR = \'type_color_index\'\n\n    @staticmethod\n    def in_category(t: tp.Union[type, np.dtype]) -> bool:\n        from static_frame.core.index_base import IndexBase\n        if not inspect.isclass(t):\n            return False\n        return issubclass(t, IndexBase)\n\nclass DisplayTypeSeries(DisplayTypeCategory):\n    CONFIG_ATTR = \'type_color_series\'\n\n    @staticmethod\n    def in_category(t: tp.Union[type, np.dtype]) -> bool:\n        from static_frame import Series\n        if not inspect.isclass(t):\n            return False\n        return issubclass(t, Series)\n\nclass DisplayTypeFrame(DisplayTypeCategory):\n    CONFIG_ATTR = \'type_color_frame\'\n\n    @staticmethod\n    def in_category(t: tp.Union[type, np.dtype]) -> bool:\n        from static_frame import Frame\n        if not inspect.isclass(t):\n            return False\n        return issubclass(t, Frame)\n\n\nclass DisplayTypeCategoryFactory:\n\n    _DISPLAY_TYPE_CATEGORIES = (\n            DisplayTypeInt,\n            DisplayTypeFloat,\n            DisplayTypeComplex,\n            DisplayTypeBool,\n            DisplayTypeObject,\n            DisplayTypeStr,\n            DisplayTypeDateTime,\n            DisplayTypeTimeDelta,\n            DisplayTypeIndex,\n            DisplayTypeSeries,\n            DisplayTypeFrame\n            )\n\n    _TYPE_TO_CATEGORY_CACHE: tp.Dict[tp.Union[type, np.dtype], tp.Type[DisplayTypeCategory]] = {}\n\n    @classmethod\n    def to_category(cls, dtype: tp.Optional[tp.Union[type, np.dtype]]) -> tp.Type[DisplayTypeCategory]:\n        if dtype not in cls._TYPE_TO_CATEGORY_CACHE:\n            category = None\n            for dtc in cls._DISPLAY_TYPE_CATEGORIES:\n                if dtc.in_category(dtype):\n                    category = dtc\n                    break\n            if not category:\n                category = DisplayTypeCategory\n\n            cls._TYPE_TO_CATEGORY_CACHE[dtype] = category\n            # if not match, assign default\n\n        return cls._TYPE_TO_CATEGORY_CACHE[dtype]\n\n\n\n#-------------------------------------------------------------------------------\n\n# NOTE: needs to be jsonable to use directly in enum\n\nclass DisplayFormats(str, Enum):\n    \'\'\'\n    Define display output format.\n    \'\'\'\n    HTML_PRE = \'html_pre\'\n    HTML_TABLE = \'html_table\'\n    HTML_DATATABLES = \'html_datatables\'\n    TERMINAL = \'terminal\'\n    RST = \'rst\'\n    MARKDOWN = \'markdown\'\n    LATEX = \'latex\'\n\n_DISPLAY_FORMAT_HTML = {\n        DisplayFormats.HTML_PRE,\n        DisplayFormats.HTML_TABLE,\n        DisplayFormats.HTML_DATATABLES\n        }\n_DISPLAY_FORMAT_TERMINAL = {DisplayFormats.TERMINAL}\n\n\nclass DisplayFormat:\n\n    CELL_WIDTH_NORMALIZE = True\n    LINE_SEP = \'\\n\'\n\n    @staticmethod\n    def markup_row(\n            row: tp.Iterable[str],\n            header_depth: int #pylint: disable=W0613\n            ) -> tp.Iterator[str]:\n        \'\'\'\n        Called with each row, post cell-width normalization (if enabled).\n\n        Args:\n            header_depth: number of columns that should be treated as headers.\n        \'\'\'\n        for msg in row:\n            yield msg\n\n    @staticmethod\n    def markup_header(msg: str) -> str:\n        \'\'\'\n        Called with all `LINE_SEP` joined header lines..\n        \'\'\'\n        return msg\n\n    @staticmethod\n    def markup_body(msg: str) -> str:\n        \'\'\'\n        Called with all `LINE_SEP` joined body lines.\n        \'\'\'\n        return msg\n\n    @staticmethod\n    def markup_outermost(msg: str,\n            identifier: tp.Optional[str] = None #pylint: disable=W0613\n            ) -> str:\n        \'\'\'\n        Called with combination of header and body joined with `LINE_SEP`.\n        \'\'\'\n        return msg\n\n\nclass DisplayFormatTerminal(DisplayFormat):\n    pass\n\nclass DisplayFormatHTMLTable(DisplayFormat):\n\n    CELL_WIDTH_NORMALIZE = False\n    LINE_SEP = \'\'\n\n    @staticmethod\n    def markup_row(\n            row: tp.Iterable[str],\n            header_depth: int\n            ) -> tp.Iterator[str]:\n        yield \'<tr>\'\n        for count, msg in enumerate(row):\n            # header depth here refers potentially to a header that is the index\n            if count < header_depth:\n                yield \'<th>{}</th>\'.format(msg)\n            else:\n                yield \'<td>{}</td>\'.format(msg)\n        yield \'</tr>\'\n\n    @staticmethod\n    def markup_header(msg: str) -> str:\n        return \'<thead>{}</thead>\'.format(msg)\n\n    @staticmethod\n    def markup_body(msg: str) -> str:\n        return \'<tbody>{}</tbody>\'.format(msg)\n\n    @staticmethod\n    def markup_outermost(msg: str,\n            identifier: tp.Optional[str] = None\n            ) -> str:\n        if identifier:\n            id_str = \'id=""{}"" \'.format(identifier)\n        else:\n            id_str = \'\'\n        return \'<table {id_str}border=""1"">{content}</table>\'.format(\n                id_str=id_str,\n                content=msg)\n\n\nclass DisplayFormatHTMLDataTables(DisplayFormatHTMLTable):\n\n    @staticmethod\n    def markup_outermost(msg: str,\n            identifier: tp.Optional[str] = \'SFTable\'\n            ) -> str:\n        # embed the table HTML in the datatables template\n        html_table = DisplayFormatHTMLTable.markup_outermost(msg,\n                identifier=identifier)\n        return display_html_datatables.TEMPLATE(identifier, html_table)\n\n\nclass DisplayFormatHTMLPre(DisplayFormat):\n\n    CELL_WIDTH_NORMALIZE = True\n\n    @staticmethod\n    def markup_outermost(msg: str,\n            identifier: tp.Optional[str] = None\n            ) -> str:\n\n        style = \'style=""white-space: pre; font-family: monospace""\'\n        id_str = \'id=""{}"" \'.format(identifier) if identifier else \'\'\n        return f\'<div {id_str}{style}>{msg}</div>\'\n\n\nclass DisplayFormatRST(DisplayFormat):\n\n    CELL_WIDTH_NORMALIZE = True\n    LINE_SEP = \'\\n\'\n    _RE_NOT_PIPE = re.compile(r\'[^|]\')\n\n    @staticmethod\n    def markup_row(\n            row: tp.Iterable[str],\n            header_depth: int) -> tp.Iterator[str]:\n\n        yield f""|{\'|\'.join(row)}|""\n\n    @classmethod\n    def markup_header(cls, msg: str) -> str:\n        # header does boundary lines above, and one additional = line below\n        def lines() -> tp.Iterator[str]:\n            for line in msg.split(cls.LINE_SEP):\n                yield cls._RE_NOT_PIPE.sub(\'-\', line).replace(\'|\', \'+\')\n                yield line\n            yield cls._RE_NOT_PIPE.sub(\'=\', line).replace(\'|\', \'+\')\n\n        return cls.LINE_SEP.join(lines())\n\n    @classmethod\n    def markup_body(cls, msg: str) -> str:\n        # body lines add boundary lines below\n        def lines() -> tp.Iterator[str]:\n            for line in msg.split(cls.LINE_SEP):\n                yield line\n                yield cls._RE_NOT_PIPE.sub(\'-\', line).replace(\'|\', \'+\')\n\n        return cls.LINE_SEP.join(lines())\n\n\nclass DisplayFormatMarkdown(DisplayFormat):\n\n    CELL_WIDTH_NORMALIZE = True\n    LINE_SEP = \'\\n\'\n    _RE_NOT_PIPE = re.compile(r\'[^|]\')\n\n    @staticmethod\n    def markup_row(\n            row: tp.Iterable[str],\n            header_depth: int) -> tp.Iterator[str]:\n        yield f""|{\'|\'.join(row)}|""\n\n    @classmethod\n    def markup_header(cls, msg: str) -> str:\n        # header does boundary lines above, and one additional = line below\n        def lines() -> tp.Iterator[str]:\n            for line in msg.split(cls.LINE_SEP):\n                yield line\n            yield cls._RE_NOT_PIPE.sub(\'-\', line)\n\n        return cls.LINE_SEP.join(lines())\n\n\n\nclass DisplayFormatLaTeX(DisplayFormat):\n\n    CELL_WIDTH_NORMALIZE = True\n    LINE_SEP = \'\\n\'\n    _CELL_SEP = \' & \'\n\n    @classmethod\n    def markup_row(cls,\n            row: tp.Iterable[str],\n            header_depth: int) -> tp.Iterator[str]:\n        yield f\'{cls._CELL_SEP.join(row)} \\\\\\\\\' # need 2 backslashes\n\n    @classmethod\n    def markup_header(cls, msg: str) -> str:\n        # assume that the header is small and the wasteful split is acceptable\n        def lines() -> tp.Iterator[str]:\n            lines_header = msg.split(\'\\n\')\n            col_count = lines_header[0].count(cls._CELL_SEP) + 1\n            col_spec = \' \'.join(\'c\' * col_count)\n            yield f\'\\\\begin{{tabular}}{{{col_spec}}}\'\n            yield r\'\\hline\\hline\'\n            yield from lines_header\n            yield r\'\\hline\'\n\n        return cls.LINE_SEP.join(lines())\n\n    @classmethod\n    def markup_body(cls,\n            msg: str) -> str:\n        return msg + cls.LINE_SEP + r\'\\hline\\end{tabular}\'\n\n    @classmethod\n    def markup_outermost(cls,\n            msg: str,\n            # caption: tp.Optional[str] = None,\n            identifier: tp.Optional[str] = None\n            ) -> str:\n\n        def lines() -> tp.Iterator[str]:\n            yield r\'\\begin{table}[ht]\'\n            # if caption:\n            #     yield f\'\\caption{{caption}}\'\n            yield r\'\\centering\'\n            yield msg\n            if identifier:\n                yield f\'\\\\label{{table:{identifier}}}\'\n            yield r\'\\end{table}\'\n\n        return cls.LINE_SEP.join(lines())\n\n_DISPLAY_FORMAT_MAP: tp.Dict[str, tp.Type[DisplayFormat]] = {\n        DisplayFormats.HTML_TABLE: DisplayFormatHTMLTable,\n        DisplayFormats.HTML_DATATABLES: DisplayFormatHTMLDataTables,\n        DisplayFormats.HTML_PRE: DisplayFormatHTMLPre,\n        DisplayFormats.TERMINAL: DisplayFormatTerminal,\n        DisplayFormats.RST: DisplayFormatRST,\n        DisplayFormats.MARKDOWN: DisplayFormatMarkdown,\n        DisplayFormats.LATEX: DisplayFormatLaTeX,\n        }\n\n#-------------------------------------------------------------------------------\n\ndef terminal_ansi(stream: tp.TextIO = sys.stdout) -> bool:\n    \'\'\'\n    Return True if the terminal is ANSI color compatible.\n    \'\'\'\n    environ = os.environ\n    if \'ANSICON\' in environ or \'PYCHARM_HOSTED\' in environ:\n        return True #pragma: no cover\n    if \'TERM\' in environ and environ[\'TERM\'] == \'ANSI\':\n        return True #pragma: no cover\n    if \'INSIDE_EMACS\' in environ:\n        return False #pragma: no cover\n\n    if getattr(stream, \'closed\', False): # if has closed attr and closed\n        return False #pragma: no cover\n\n    if hasattr(stream, \'isatty\') and stream.isatty() and platform.system() != \'Windows\':\n        return True #pragma: no cover\n\n    return False\n\n\n#-------------------------------------------------------------------------------\nclass DisplayConfig:\n    \'\'\'\n    Storage container for all display settings.\n    \'\'\'\n\n    __slots__ = (\n            \'type_show\',\n            \'type_color\',\n\n            \'type_color_default\',\n            \'type_color_int\',\n            \'type_color_float\',\n            \'type_color_complex\',\n            \'type_color_bool\',\n            \'type_color_object\',\n            \'type_color_str\',\n            \'type_color_datetime\',\n            \'type_color_timedelta\',\n            \'type_color_index\',\n            \'type_color_series\',\n            \'type_color_frame\',\n\n            \'type_delimiter_left\',\n            \'type_delimiter_right\',\n\n            \'value_format_float_positional\',\n            \'value_format_float_scientific\',\n            \'value_format_complex_positional\',\n            \'value_format_complex_scientific\',\n\n            \'display_format\',\n            \'display_columns\',\n            \'display_rows\',\n\n            \'include_columns\',\n            \'include_index\',\n\n            \'cell_max_width\',\n            \'cell_max_width_leftmost\',\n            \'cell_align_left\',\n            )\n\n    @classmethod\n    def from_json(cls, json_str: str) -> \'DisplayConfig\':\n        args = json.loads(json_str.strip())\n        # filter arguments by current slots\n        args_valid = {}\n        for k in cls.__slots__:\n            if k in args:\n                args_valid[k] = args[k]\n        return cls(**args_valid)\n\n    @classmethod\n    def from_file(cls, fp: str) -> \'DisplayConfig\':\n        with open(fp) as f:\n            return cls.from_json(f.read())\n\n    @classmethod\n    def from_default(cls, **kwargs: tp.Any) -> \'DisplayConfig\':\n        return cls(**kwargs)\n\n    def __init__(self, *,\n            type_show: bool = True,\n            type_color: bool = True,\n\n            type_color_default: ColorConstructor = 0x505050,\n            type_color_int: ColorConstructor = 0x505050,\n            type_color_float: ColorConstructor = 0x505050,\n            type_color_complex: ColorConstructor = 0x505050,\n            type_color_bool: ColorConstructor = 0x505050,\n            type_color_object: ColorConstructor = 0x505050,\n            type_color_str: ColorConstructor = 0x505050,\n\n            type_color_datetime: ColorConstructor = 0x505050,\n            type_color_timedelta: ColorConstructor = 0x505050,\n\n            type_color_index: ColorConstructor = 0x777777,\n            type_color_series: ColorConstructor = 0x777777,\n            type_color_frame: ColorConstructor = 0x777777,\n\n            type_delimiter_left: str = \'<\',\n            type_delimiter_right: str = \'>\',\n\n            # for positional, default to {} to avoid a fixed floating point size\n            value_format_float_positional: str = \'{}\',\n            value_format_float_scientific: str = \'{:.8e}\',\n            value_format_complex_positional: str = \'{}\',\n            value_format_complex_scientific: str = \'{:.2e}\',\n\n            display_format: str = DisplayFormats.TERMINAL,\n            display_columns: int = 12,\n            display_rows: int = 36,\n\n            include_columns: bool = True,\n            include_index: bool = True,\n\n            cell_max_width: int = 20,\n            cell_max_width_leftmost: int = 36,\n            cell_align_left: bool = True\n            ) -> None:\n\n        self.type_show = type_show\n        self.type_color = type_color\n\n        self.type_color_default = type_color_default\n        self.type_color_int = type_color_int\n        self.type_color_float = type_color_float\n        self.type_color_complex = type_color_complex\n        self.type_color_bool = type_color_bool\n        self.type_color_object = type_color_object\n        self.type_color_str = type_color_str\n\n        self.type_color_datetime = type_color_datetime\n        self.type_color_timedelta = type_color_timedelta\n        self.type_color_index = type_color_index\n        self.type_color_series = type_color_series\n        self.type_color_frame = type_color_frame\n\n        self.type_delimiter_left = type_delimiter_left\n        self.type_delimiter_right = type_delimiter_right\n\n        self.value_format_float_positional = value_format_float_positional\n        self.value_format_float_scientific = value_format_float_scientific\n        self.value_format_complex_positional = value_format_complex_positional\n        self.value_format_complex_scientific = value_format_complex_scientific\n\n        self.display_format = display_format\n        self.display_columns = display_columns\n        self.display_rows = display_rows\n\n        self.include_columns = include_columns\n        self.include_index = include_index\n\n        self.cell_max_width = cell_max_width\n        self.cell_max_width_leftmost = cell_max_width_leftmost\n        self.cell_align_left = cell_align_left\n\n        #-----------------------------------------------------------------------\n        # handle any inter-dependent configurations\n\n        if not self.include_columns or not self.include_index:\n            if self.type_show:\n                raise RuntimeError(\'cannot show types if not including columns or index.\')\n\n\n    def write(self, fp: str) -> None:\n        \'\'\'Write a JSON file.\n        \'\'\'\n        with open(fp, \'w\') as f:\n            f.write(self.to_json() + \'\\n\')\n\n    def __repr__(self) -> str:\n        return \'<\' + self.__class__.__name__ + \' \' + \' \'.join(\n                \'{k}={v}\'.format(k=k, v=getattr(self, k))\n                for k in self.__slots__) + \'>\'\n\n    def to_dict(self, **kwargs: object) -> tp.Dict[str, tp.Any]:\n        # overrides with passed in kwargs if provided\n        return {k: kwargs.get(k, getattr(self, k))\n                for k in self.__slots__}\n\n    def to_json(self) -> str:\n        return json.dumps(self.to_dict())\n\n    def to_transpose(self) -> \'DisplayConfig\':\n        kwargs = self.to_dict()\n        kwargs[\'display_columns\'], kwargs[\'display_rows\'] = (\n                kwargs[\'display_rows\'], kwargs[\'display_columns\'])\n        return self.__class__(**kwargs)\n\n    def to_display_config(self, **kwargs: object) -> \'DisplayConfig\':\n        return self.__class__(**self.to_dict(**kwargs))\n\n#-------------------------------------------------------------------------------\nclass DisplayConfigs:\n    \'\'\'\n    Container of common default configs.\n    \'\'\'\n\n    DEFAULT = DisplayConfig()\n\n    HTML_PRE = DisplayConfig(\n            display_format=DisplayFormats.HTML_PRE,\n            type_color=True\n            )\n\n    COLOR = DisplayConfig(\n            display_format=DisplayFormats.TERMINAL,\n            type_color=True,\n            type_color_default=\'gray\',\n            type_color_int=\'yellowgreen\',\n            type_color_float=\'DeepSkyBlue\',\n            type_color_complex=\'LightSkyBlue\',\n            type_color_bool=\'darkorange\',\n            type_color_object=\'DarkSlateBlue\',\n            type_color_str=\'lightcoral\',\n            type_color_datetime=\'peru\',\n            type_color_timedelta=\'sienna\',\n            type_color_index=\'DarkSlateGray\',\n            type_color_series=\'dimgray\',\n            type_color_frame=\'lightslategray\',\n            )\n\n    UNBOUND = DisplayConfig(\n            display_columns=np.inf,\n            display_rows=np.inf,\n            cell_max_width=np.inf,\n            cell_max_width_leftmost=np.inf,\n            )\n    UNBOUND_COLUMNS = DisplayConfig(\n            display_columns=np.inf,\n            cell_max_width=np.inf,\n            cell_max_width_leftmost=np.inf,\n            )\n    UNBOUND_ROWS = DisplayConfig(\n            display_rows=np.inf,\n            cell_max_width=np.inf,\n            cell_max_width_leftmost=np.inf,\n            )\n\n#-------------------------------------------------------------------------------\n\n_module._display_active = DisplayConfig()  # type: ignore\n\nclass DisplayActive:\n    \'\'\'Utility interface for setting module-level display configuration.\n    \'\'\'\n    FILE_NAME = \'.static_frame.conf\'\n\n    @staticmethod\n    def set(dc: DisplayConfig) -> None:\n        _module._display_active = dc  # type: ignore\n\n    @staticmethod\n    def get(**kwargs: tp.Union[bool, int, str]) -> DisplayConfig:\n        config: DisplayConfig = _module._display_active  # type: ignore\n        if not kwargs:\n            return config\n        args = config.to_dict()\n        args.update(kwargs)\n        return DisplayConfig(**args)\n\n    @classmethod\n    def update(cls, **kwargs: object) -> None:\n        args = cls.get().to_dict()\n        args.update(kwargs)\n        cls.set(DisplayConfig(**args))\n\n    @classmethod\n    def _default_fp(cls) -> str:\n        # TODO: improve cross platform support\n        return os.path.join(os.path.expanduser(\'~\'), cls.FILE_NAME)\n\n    @classmethod\n    def write(cls, fp: tp.Optional[str] = None) -> None:\n        fp = fp or cls._default_fp()\n        dc = cls.get()\n        dc.write(fp)\n\n    @classmethod\n    def read(cls, fp: tp.Optional[str] = None) -> None:\n        fp = fp or cls._default_fp()\n        cls.set(DisplayConfig.from_file(fp))\n\n\n#-------------------------------------------------------------------------------\nclass DisplayHeader:\n    \'\'\'\n    Wraper for passing in display header that have a name attribute.\n    \'\'\'\n    __slots__ = (\'cls\', \'name\')\n\n    def __init__(self,\n            cls: type,\n            name: tp.Optional[object] = None) -> None:\n        \'\'\'\n        Args:\n            cls: the Class to be displayed.\n            name: an optional name attribute stored on the instance.\n        \'\'\'\n        self.cls = cls\n        self.name = name\n\n    def __repr__(self) -> str:\n        \'\'\'\n        Provide string representation before additon of outer delimiters.\n        \'\'\'\n        if self.name:\n            return \'{}: {}\'.format(self.cls.__name__, self.name)\n        return self.cls.__name__\n\n\nHeaderInitializer = tp.Optional[tp.Union[str, DisplayHeader]]\n\n# store formating string, raw string\nDisplayCell = namedtuple(\'DisplayCell\', (\'format_str\', \'raw\'))\nFORMAT_EMPTY = \'{}\'\n\nclass Display:\n    \'\'\'\n    A Display is a string representation of a table, encoded as a list of lists, where list components are equal-width strings, keyed by row index\n    \'\'\'\n    __slots__ = (\n        \'_rows\',\n        \'_config\',\n        \'_outermost\',\n        \'_index_depth\',\n        \'_header_depth\',\n        )\n\n    CHAR_MARGIN = 1\n    CELL_EMPTY = DisplayCell(FORMAT_EMPTY, \'\')\n    ELLIPSIS = \'...\' # this string is appended to truncated entries\n    CELL_ELLIPSIS = DisplayCell(FORMAT_EMPTY, ELLIPSIS)\n    ELLIPSIS_CENTER_SENTINEL = object()\n\n    #---------------------------------------------------------------------------\n    # utility methods\n\n    @staticmethod\n    def type_attributes(\n            type_input: tp.Union[np.dtype, type, DisplayHeader],\n            config: DisplayConfig\n            ) -> tp.Tuple[str, tp.Type[DisplayTypeCategory]]:\n        \'\'\'\n        Apply delimters to type, for either numpy types or Python classes.\n        \'\'\'\n        if isinstance(type_input, np.dtype):\n            type_str = str(type_input)\n            type_ref = type_input\n        elif inspect.isclass(type_input):\n            assert isinstance(type_input, type)\n            type_str = type_input.__name__\n            type_ref = type_input\n        elif isinstance(type_input, DisplayHeader):\n            type_str = repr(type_input)\n            type_ref = type_input.cls\n        else:\n            raise NotImplementedError(\'no handling for this input\', type_input)\n\n        type_category = DisplayTypeCategoryFactory.to_category(type_ref)\n\n        # if config.type_delimiter_left or config.type_delimiter_right:\n        left = config.type_delimiter_left or \'\'\n        right = config.type_delimiter_right or \'\'\n        type_label = f\'{left}{type_str}{right}\'\n\n        return type_label, type_category\n\n    @staticmethod\n    def type_color_markup(\n            type_category: tp.Type[DisplayTypeCategory],\n            config: DisplayConfig\n            ) -> str:\n        \'\'\'\n        Return a format string for applying color to a type based on type category and config.\n\n        Returns:\n            A templated string with a ""text"" field for formatting.\n        \'\'\'\n        color = getattr(config, type_category.CONFIG_ATTR)\n        if config.display_format in _DISPLAY_FORMAT_HTML:\n            return HexColor.format_html(color, FORMAT_EMPTY)\n\n        if config.display_format in _DISPLAY_FORMAT_TERMINAL:\n            if terminal_ansi():\n                return HexColor.format_terminal(color, FORMAT_EMPTY)\n            # if not a compatible terminal, return label unaltered\n            return FORMAT_EMPTY\n\n        # RST and other text displays\n        return FORMAT_EMPTY\n\n    @classmethod\n    def to_cell(cls,\n            value: object, # dtype, HeaderInitializer, or a type\n            config: DisplayConfig,\n            is_dtype: bool = False) -> DisplayCell:\n        \'\'\'\n        Given a raw value, return a :obj:`DisplayCell`.\n        \'\'\'\n        if is_dtype or inspect.isclass(value) or isinstance(value, DisplayHeader):\n            type_str_raw, type_category = cls.type_attributes(\n                    value,\n                    config=config)\n            if config.type_color:\n                format_str = cls.type_color_markup(\n                        type_category,\n                        config)\n            else:\n                format_str = FORMAT_EMPTY\n            return DisplayCell(format_str, type_str_raw)\n\n        # ContainerOperand needs to import Display\n        from static_frame.core.container import ContainerOperand\n\n        # handling for all other values that are stringable\n        if isinstance(value, ContainerOperand):\n            # NOTE: we do not use type delimieters as ths is an instance, not a class\n            msg = value.__class__.__name__\n        else:\n            msg = str(value)\n\n        # handling for float, complex if str() produces an \'e\', then we use the scientific template; otherwise, we use the postional; users can config both to be the same to always get one or the other\n        if isinstance(value, FLOAT_TYPES):\n            if \'e\' in msg:\n                msg = config.value_format_float_scientific.format(value)\n            else:\n                msg = config.value_format_float_positional.format(value)\n        elif isinstance(value, COMPLEX_TYPES):\n            if \'e\' in msg:\n                msg = config.value_format_complex_scientific.format(value)\n            else:\n                msg = config.value_format_complex_positional.format(value)\n\n        return DisplayCell(FORMAT_EMPTY, msg)\n\n    #---------------------------------------------------------------------------\n    # alternate constructor\n\n    @classmethod\n    def from_values(cls,\n            values: np.ndarray,\n            header: object,\n            config: tp.Optional[DisplayConfig] = None,\n            outermost: bool = False,\n            index_depth: int = 0,\n            header_depth: int = 0\n            ) -> \'Display\':\n        \'\'\'\n        Given a 1 or 2D ndarray, return a Display instance. Generally 2D arrays are passed here only from TypeBlocks.\n        \'\'\'\n        # return a list of lists, where each inner list represents multiple columns\n        config = config or DisplayActive.get()\n\n        # create a list of lists, always starting with the header\n        rows = []\n        if header is not None:\n            # NOTE: controlling if the header is applied with type_show is moving to display() methods; this approach will no longer be needed\n            # assume that all headers are SF types; skip if type_show is False\n            if config.type_show:\n                rows.append([cls.to_cell(header, config=config)])\n            else:\n                rows.append([cls.CELL_EMPTY])\n\n        if isinstance(values, np.ndarray) and values.ndim == 2:\n            # get rows from numpy string formatting\n            np_rows = np.array_str(values).split(\'\\n\')\n            last_idx = len(np_rows) - 1\n            for idx, row in enumerate(np_rows):\n                # trim brackets\n                end_slice_len = 2 if idx == last_idx else 1\n                row = row[2: len(row) - end_slice_len].strip()\n                rows.append([cls.to_cell(row, config=config)])\n        else:\n            count_max = config.display_rows\n            # print(\'comparing values to count_max\', len(values), count_max)\n            if len(values) > config.display_rows:\n                data_half_count = Display.truncate_half_count(count_max)\n                value_gen = partial(_gen_skip_middle,\n                        forward_iter=values.__iter__,\n                        forward_count=data_half_count,\n                        reverse_iter=partial(reversed, values),\n                        reverse_count=data_half_count,\n                        center_sentinel=cls.ELLIPSIS_CENTER_SENTINEL\n                        )\n            else:\n                value_gen = values.__iter__\n\n            for v in value_gen():\n                if v is cls.ELLIPSIS_CENTER_SENTINEL: # center sentinel\n                    rows.append([cls.CELL_ELLIPSIS])\n                else:\n                    rows.append([cls.to_cell(v, config=config)])\n\n        # add the types to the last row\n        if isinstance(values, np.ndarray) and config.type_show:\n            rows.append([cls.to_cell(values.dtype, config=config, is_dtype=True)])\n\n        return cls(rows,\n                config=config,\n                outermost=outermost,\n                index_depth=index_depth,\n                header_depth=header_depth)\n\n\n    #---------------------------------------------------------------------------\n    # core cell-to-rwo expansion routines\n\n    @staticmethod\n    def truncate_half_count(count_target: int) -> int:\n        \'\'\'Given a target number of rows or columns, return the count of half as found in presentation where one column is used for the elipsis. The number returned will always be odd. For example, given a target of 5 we allocate 2 per half (plus 1 reserved for middle).\n        \'\'\'\n        if count_target <= 4:\n            return 1 # practical floor for all values of 4 or less\n        return (count_target - 1) // 2\n\n\n    @classmethod\n    def _get_max_width_pad_width(cls, *,\n            rows: tp.Sequence[tp.Sequence[DisplayCell]],\n            col_idx_src: int,\n            col_last_src: int,\n            row_indices: tp.Iterable[int],\n            config: tp.Optional[DisplayConfig] = None,\n            ) -> tp.Tuple[int, int]:\n        \'\'\'\n        Called once for each column to determine the maximum_width and pad_width for a particular column. All row data is passed to this function, and cell values are looked up directly with the `col_idx_src` argument.\n\n        Args:\n            rows: iterable of all rows, containing DisplayCell instances\n            col_idx_src: the integer index for the currrent column\n            col_last_src: the integer index for the last column\n            row_indices: passed here so same range() can be reused.\n        \'\'\'\n        config = config or DisplayActive.get()\n\n        is_last = col_idx_src == col_last_src\n        is_first = col_idx_src == 0\n\n        if is_first:\n            width_limit = config.cell_max_width_leftmost\n        else:\n            width_limit = config.cell_max_width\n\n        max_width = 0\n        for row_idx_src in row_indices:\n            # get existing max width, up to the max\n            if row_idx_src is not None:\n                row = rows[row_idx_src]\n                if col_idx_src >= len(row): # this row does not have this column\n                    continue\n                cell = row[col_idx_src]\n                max_width = max(max_width, len(cell.raw))\n            else:\n                max_width = max(max_width, len(cls.ELLIPSIS))\n            # if already exceeded max width, stop iterating\n            if max_width >= width_limit:\n                break\n\n        # get most binding constraint\n        max_width = min(max_width, width_limit)\n\n        if ((config.cell_align_left is True and is_last) or\n                (config.cell_align_left is False and is_first)):\n            pad_width = max_width\n        else:\n            pad_width = max_width + cls.CHAR_MARGIN\n\n        return max_width, pad_width\n\n    @classmethod\n    def _to_rows_cells(cls,\n            display: \'Display\',\n            config: tp.Optional[DisplayConfig] = None,\n            # index_depth: int = 0,\n            # columns_depth: int = 0,\n            ) -> tp.Iterable[tp.Iterable[str]]:\n        \'\'\'\n        Given a Display object, return an iterable of iterables of strings, where each iterable contains strings for all cells in that row, with appropriate padding (if necessary) applied. Based on configruation, align cells left or right with space and return one joined string per row.\n\n        Returns:\n            Returns an iterable of formatted strings, generally one per row.\n        \'\'\'\n        config = config or DisplayActive.get()\n\n        # find max columns for all defined rows\n        col_count_src = max(len(row) for row in display._rows)\n        col_last_src = col_count_src - 1\n\n        row_count_src = len(display._rows)\n        row_indices = range(row_count_src)\n\n        rows: tp.List[tp.List[str]] = [[] for _ in row_indices]\n\n        # if we normalize, we truncate cells and pad\n        dfc = _DISPLAY_FORMAT_MAP[config.display_format]\n        is_html = config.display_format in _DISPLAY_FORMAT_HTML\n\n        for col_idx_src in range(col_count_src):\n\n            # for each column, get the max width\n            if dfc.CELL_WIDTH_NORMALIZE:\n                max_width, pad_width = cls._get_max_width_pad_width(\n                        rows=display._rows,\n                        col_idx_src=col_idx_src,\n                        col_last_src=col_last_src,\n                        row_indices=row_indices,\n                        config=config\n                        )\n\n            for row_idx_src in row_indices:\n                display_row = display._rows[row_idx_src]\n                if col_idx_src >= len(display_row):\n                    cell = cls.CELL_EMPTY\n                else:\n                    cell = display_row[col_idx_src]\n\n                cell_format_str = cell.format_str\n                cell_raw = cell.raw\n\n                if dfc.CELL_WIDTH_NORMALIZE:\n\n                    if len(cell.raw) > max_width:\n                        # must truncate if cell width is greater than max width\n                        width_truncate = max_width - len(cls.CELL_ELLIPSIS.raw)\n\n                        # TODO: this is truncating scientific notation\n                        cell_raw = cell_raw[:width_truncate] + cls.ELLIPSIS\n                        if is_html:\n                            cell_raw = html.escape(cell_raw)\n\n                        cell_formatted = cell_format_str.format(cell_raw)\n                        cell_fill_width = cls.CHAR_MARGIN # should only be margin left\n                    else:\n                        if is_html:\n                            cell_raw = html.escape(cell_raw)\n                        cell_formatted = cell_format_str.format(cell_raw)\n                        cell_fill_width = pad_width - len(cell.raw) # this includes margin\n\n                    # print(col_idx, row_idx, cell, max_width, pad_width, cell_fill_width)\n                    if config.cell_align_left:\n                        # must manually add space as color chars make ljust not work\n                        msg = cell_formatted + \' \' * cell_fill_width\n                    else:\n                        msg = \' \' * cell_fill_width + cell_formatted\n\n                else: # no width normalization\n                    if is_html:\n                        cell_raw = html.escape(cell_raw)\n                    msg = cell_format_str.format(cell_raw)\n\n                rows[row_idx_src].append(msg)\n\n        return rows\n\n    #---------------------------------------------------------------------------\n    def __init__(self,\n            rows: tp.List[tp.List[DisplayCell]],\n            config: tp.Optional[DisplayConfig] = None,\n            outermost: bool = False,\n            index_depth: int = 0,\n            header_depth: int = 0,\n            ) -> None:\n        \'\'\'Define rows as a list of lists, for each row; the contained DisplayCell instances may be of different size, but they are expected to be aligned vertically in final presentation.\n\n        Args:\n            header_depth: columns depth plus any addtional lines used for headers\n        \'\'\'\n        config = config or DisplayActive.get()\n\n        self._rows = rows\n        self._config = config\n        self._outermost = outermost\n        self._index_depth = index_depth\n        self._header_depth = header_depth\n\n    def __repr__(self) -> str:\n        rows = self._to_rows_cells(self,\n                self._config,\n                )\n\n        if self._outermost:\n            dfc = _DISPLAY_FORMAT_MAP[self._config.display_format]\n            header = []\n            body = []\n            for idx, row in enumerate(rows):\n                if idx < self._header_depth:\n                    row = \'\'.join(dfc.markup_row(row,\n                            header_depth=np.inf\n                            )).rstrip()\n                    header.append(row)\n                else:\n                    row = \'\'.join(dfc.markup_row(row,\n                            header_depth=self._index_depth\n                            )).rstrip()\n                    body.append(row)\n\n            outermost = []\n            if header:\n                header_str = dfc.markup_header(dfc.LINE_SEP.join(header))\n                outermost.append(header_str)\n\n            body_str = dfc.markup_body(dfc.LINE_SEP.join(body))\n            outermost.append(body_str)\n            return dfc.markup_outermost(dfc.LINE_SEP.join(outermost))\n\n        return dfc.LINE_SEP.join(\'\'.join(r) for r in rows)\n\n    def to_rows(self) -> tp.Iterable[str]:\n        \'\'\'\n        Alternate output method for observing rows as strings within a list. Useful for testing.\n        \'\'\'\n        post = []\n        for idx, row in enumerate(self._to_rows_cells(self, self._config)):\n            line = \'\'.join(row).rstrip()\n            if idx < self._header_depth:\n                if line == \'\': # type removal led to an empty line\n                    continue\n            post.append(line)\n        return post\n\n    def __iter__(self) -> tp.Iterator[tp.List[str]]:\n        for row in self._rows:\n            yield [cell.format_str.format(cell.raw) for cell in row]\n\n    def __len__(self) -> int:\n        return len(self._rows)\n\n    #---------------------------------------------------------------------------\n    # in place mutation\n\n    def extend_display(self, display: \'Display\') -> None:\n        \'\'\'\n        Mutate this display by extending to the right (adding columns) with the passed display.\n        \'\'\'\n        # NOTE: do not want to pass config or call format here as we call this for each column or block we add\n        for row_idx, row in enumerate(display._rows):\n            if row_idx == len(self._rows):\n                self._rows.append([])\n            self._rows[row_idx].extend(row)\n\n    def extend_iterable(self,\n            iterable: tp.Sequence[tp.Any],\n            header: HeaderInitializer\n            ) -> None:\n        \'\'\'\n        Add a single iterable (as a column) to the display.\n        \'\'\'\n        row_idx_start = 0\n        if header is not None:\n            self._rows[0].append(self.to_cell(header, config=self._config))\n            row_idx_start = 1\n\n        # truncate iterable if necessary\n        count_max = self._config.display_rows\n\n        if len(iterable) > count_max:\n            data_half_count = self.truncate_half_count(count_max)\n            value_gen: tp.Callable[[], tp.Iterator[tp.Any]] = partial(_gen_skip_middle,\n                    forward_iter=iterable.__iter__,\n                    forward_count=data_half_count,\n                    reverse_iter=partial(reversed, iterable),\n                    reverse_count=data_half_count,\n                    center_sentinel=self.ELLIPSIS_CENTER_SENTINEL\n                    )\n        else:\n            value_gen = iterable.__iter__\n\n        # start at 1 as 0 is header\n        idx = 0 # store in case value gen is empty\n        for idx, value in enumerate(value_gen(), start=row_idx_start):\n            if value is self.ELLIPSIS_CENTER_SENTINEL:\n                self._rows[idx].append(self.CELL_ELLIPSIS)\n            else:\n                self._rows[idx].append(self.to_cell(value, config=self._config))\n\n        if isinstance(iterable, np.ndarray) and self._config.type_show:\n            self._rows[idx + 1].append(self.to_cell(iterable.dtype,\n                    config=self._config,\n                    is_dtype=True))\n\n    def extend_ellipsis(self) -> None:\n        \'\'\'Append an ellipsis over all rows.\n        \'\'\'\n        for row in self._rows:\n            row.append(self.CELL_ELLIPSIS)\n\n    def insert_displays(self,\n            *displays: \'Display\',\n            insert_index: int = 0) -> None:\n        \'\'\'\n        Insert rows on top of existing rows.\n        args:\n            Each arg in args is an instance of Display\n            insert_index: the index at which to start insertion\n        \'\'\'\n        # each arg is a list, to be a new row\n        # assume each row in display becomes a column\n        new_rows: tp.List[tp.List[DisplayCell]] = []\n        for display in displays:\n            new_rows.extend(display._rows)\n\n        rows = []\n        rows.extend(self._rows[:insert_index])\n        rows.extend(new_rows)\n        rows.extend(self._rows[insert_index:])\n        self._rows = rows\n\n    # def drop_row(self, index: int = 0) -> None:\n    #     \'\'\'Remove a row in place.\n    #     \'\'\'\n    #     self._rows = self._rows[:index] + self._rows[index+1:]\n\n\n    #---------------------------------------------------------------------------\n    # return a new display\n\n\n    def flatten(self) -> \'Display\':\n        \'\'\'\n        Return a Display from this Display that is a single row, formed the contents of all rows put in a single senquece, from the top down. Through this a single-column index Display can be made into a single row Display.\n        \'\'\'\n        row = []\n        for part in self._rows:\n            row.extend(part)\n        rows = [row]\n        return self.__class__(rows, config=self._config)\n\n    def transform(self) -> \'Display\':\n        \'\'\'\n        Return Display transformed (rotated) on its upper left; i.e., the first column becomes the first row.\n        \'\'\'\n\n        # assume first row gives us column count\n        col_count = len(self._rows[0])\n        rows: tp.List[tp.List[DisplayCell]] = [[] for _ in range(col_count)]\n        for r in self._rows:\n            for idx, cell in enumerate(r):\n                rows[idx].append(cell)\n        return self.__class__(rows, config=self._config)\n\n\n'"
static_frame/core/display_color.py,0,"b'\nimport typing as tp\n\n\n#-------------------------------------------------------------------------------\n# https://www.w3.org/TR/css-color-3/#svg-color\n\n_COLOR_NAME_X11 = {\n    \'aliceblue\': 0xf0f8ff,\n    \'antiquewhite\': 0xfaebd7,\n    \'aqua\': 0xffff,\n    \'aquamarine\': 0x7fffd4,\n    \'azure\': 0xf0ffff,\n    \'beige\': 0xf5f5dc,\n    \'bisque\': 0xffe4c4,\n    \'black\': 0x0,\n    \'blanchedalmond\': 0xffebcd,\n    \'blue\': 0xff,\n    \'blueviolet\': 0x8a2be2,\n    \'brown\': 0xa52a2a,\n    \'burlywood\': 0xdeb887,\n    \'cadetblue\': 0x5f9ea0,\n    \'chartreuse\': 0x7fff00,\n    \'chocolate\': 0xd2691e,\n    \'coral\': 0xff7f50,\n    \'cornflowerblue\': 0x6495ed,\n    \'cornsilk\': 0xfff8dc,\n    \'crimson\': 0xdc143c,\n    \'cyan\': 0xffff,\n    \'darkblue\': 0x8b,\n    \'darkcyan\': 0x8b8b,\n    \'darkgoldenrod\': 0xb8860b,\n    \'darkgray\': 0xa9a9a9,\n    \'darkgreen\': 0x6400,\n    \'darkgrey\': 0xa9a9a9,\n    \'darkkhaki\': 0xbdb76b,\n    \'darkmagenta\': 0x8b008b,\n    \'darkolivegreen\': 0x556b2f,\n    \'darkorange\': 0xff8c00,\n    \'darkorchid\': 0x9932cc,\n    \'darkred\': 0x8b0000,\n    \'darksalmon\': 0xe9967a,\n    \'darkseagreen\': 0x8fbc8f,\n    \'darkslateblue\': 0x483d8b,\n    \'darkslategray\': 0x2f4f4f,\n    \'darkslategrey\': 0x2f4f4f,\n    \'darkturquoise\': 0xced1,\n    \'darkviolet\': 0x9400d3,\n    \'deeppink\': 0xff1493,\n    \'deepskyblue\': 0xbfff,\n    \'dimgray\': 0x696969,\n    \'dimgrey\': 0x696969,\n    \'dodgerblue\': 0x1e90ff,\n    \'firebrick\': 0xb22222,\n    \'floralwhite\': 0xfffaf0,\n    \'forestgreen\': 0x228b22,\n    \'fuchsia\': 0xff00ff,\n    \'gainsboro\': 0xdcdcdc,\n    \'ghostwhite\': 0xf8f8ff,\n    \'gold\': 0xffd700,\n    \'goldenrod\': 0xdaa520,\n    \'gray\': 0x808080,\n    \'green\': 0x8000,\n    \'greenyellow\': 0xadff2f,\n    \'grey\': 0x808080,\n    \'honeydew\': 0xf0fff0,\n    \'hotpink\': 0xff69b4,\n    \'indianred\': 0xcd5c5c,\n    \'indigo\': 0x4b0082,\n    \'ivory\': 0xfffff0,\n    \'khaki\': 0xf0e68c,\n    \'lavender\': 0xe6e6fa,\n    \'lavenderblush\': 0xfff0f5,\n    \'lawngreen\': 0x7cfc00,\n    \'lemonchiffon\': 0xfffacd,\n    \'lightblue\': 0xadd8e6,\n    \'lightcoral\': 0xf08080,\n    \'lightcyan\': 0xe0ffff,\n    \'lightgoldenrodyellow\': 0xfafad2,\n    \'lightgray\': 0xd3d3d3,\n    \'lightgreen\': 0x90ee90,\n    \'lightgrey\': 0xd3d3d3,\n    \'lightpink\': 0xffb6c1,\n    \'lightsalmon\': 0xffa07a,\n    \'lightseagreen\': 0x20b2aa,\n    \'lightskyblue\': 0x87cefa,\n    \'lightslategray\': 0x778899,\n    \'lightslategrey\': 0x778899,\n    \'lightsteelblue\': 0xb0c4de,\n    \'lightyellow\': 0xffffe0,\n    \'lime\': 0xff00,\n    \'limegreen\': 0x32cd32,\n    \'linen\': 0xfaf0e6,\n    \'magenta\': 0xff00ff,\n    \'maroon\': 0x800000,\n    \'mediumaquamarine\': 0x66cdaa,\n    \'mediumblue\': 0xcd,\n    \'mediumorchid\': 0xba55d3,\n    \'mediumpurple\': 0x9370db,\n    \'mediumseagreen\': 0x3cb371,\n    \'mediumslateblue\': 0x7b68ee,\n    \'mediumspringgreen\': 0xfa9a,\n    \'mediumturquoise\': 0x48d1cc,\n    \'mediumvioletred\': 0xc71585,\n    \'midnightblue\': 0x191970,\n    \'mintcream\': 0xf5fffa,\n    \'mistyrose\': 0xffe4e1,\n    \'moccasin\': 0xffe4b5,\n    \'navajowhite\': 0xffdead,\n    \'navy\': 0x80,\n    \'oldlace\': 0xfdf5e6,\n    \'olive\': 0x808000,\n    \'olivedrab\': 0x6b8e23,\n    \'orange\': 0xffa500,\n    \'orangered\': 0xff4500,\n    \'orchid\': 0xda70d6,\n    \'palegoldenrod\': 0xeee8aa,\n    \'palegreen\': 0x98fb98,\n    \'paleturquoise\': 0xafeeee,\n    \'palevioletred\': 0xdb7093,\n    \'papayawhip\': 0xffefd5,\n    \'peachpuff\': 0xffdab9,\n    \'peru\': 0xcd853f,\n    \'pink\': 0xffc0cb,\n    \'plum\': 0xdda0dd,\n    \'powderblue\': 0xb0e0e6,\n    \'purple\': 0x800080,\n    \'red\': 0xff0000,\n    \'rosybrown\': 0xbc8f8f,\n    \'royalblue\': 0x4169e1,\n    \'saddlebrown\': 0x8b4513,\n    \'salmon\': 0xfa8072,\n    \'sandybrown\': 0xf4a460,\n    \'seagreen\': 0x2e8b57,\n    \'seashell\': 0xfff5ee,\n    \'sienna\': 0xa0522d,\n    \'silver\': 0xc0c0c0,\n    \'skyblue\': 0x87ceeb,\n    \'slateblue\': 0x6a5acd,\n    \'slategray\': 0x708090,\n    \'slategrey\': 0x708090,\n    \'snow\': 0xfffafa,\n    \'springgreen\': 0xff7f,\n    \'steelblue\': 0x4682b4,\n    \'tan\': 0xd2b48c,\n    \'teal\': 0x8080,\n    \'thistle\': 0xd8bfd8,\n    \'tomato\': 0xff6347,\n    \'turquoise\': 0x40e0d0,\n    \'violet\': 0xee82ee,\n    \'webgray\': 0x808080,\n    \'wheat\': 0xf5deb3,\n    \'white\': 0xffffff,\n    \'whitesmoke\': 0xf5f5f5,\n    \'yellow\': 0xffff00,\n    \'yellowgreen\': 0x9acd32,\n}\n\n\n\n#-------------------------------------------------------------------------------\n\n# Based largely on broadinstitute/xtermcolor\n# https://github.com/broadinstitute/xtermcolor\n# Copyright (C) 2012 The Broad Institute\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n\nclass HexColor:\n\n    _ANSI_TO_HEX = None\n    _HEX_TO_ANSI_CACHE: tp.Dict[int, int] = {}\n\n    @staticmethod\n    def _rgb(color: int) -> tp.Tuple[int, int, int]:\n        return ((color >> 16) & 0xff, (color >> 8) & 0xff, color & 0xff)\n\n    @classmethod\n    def _diff(cls, color1: int, color2: int) -> int:\n        (r1, g1, b1) = cls._rgb(color1)\n        (r2, g2, b2) = cls._rgb(color2)\n        return abs(r1 - r2) + abs(g1 - g2) + abs(b1 - b2)\n\n    @staticmethod\n    def _get_ansi_to_hex_map() -> tp.Dict[int, int]:\n        \'\'\'\n        Called once (lazzily) to get the ANSI to hex color mapping. This will return a dictionary mapping integers 0 to 255 to corresponding hex values (as integers).\n        \'\'\'\n        primary = (\n            0x000000,\n            0x800000,\n            0x008000,\n            0x808000,\n            0x000080,\n            0x800080,\n            0x008080,\n            0xc0c0c0\n            )\n\n        bright = (\n            0x808080,\n            0xff0000,\n            0x00ff00,\n            0xffff00,\n            0x0000ff,\n            0xff00ff,\n            0x00ffff,\n            0xffffff\n            )\n\n        colors = {}\n\n        for index, color in enumerate(primary + bright):\n            colors[index] = color\n\n        intensities = (0x00, 0x5F, 0x87, 0xAF, 0xD7, 0xFF)\n\n        c = 16\n        for i in intensities:\n            color = i << 16\n            for j in intensities:\n                color &= ~(0xff << 8)\n                color |= j << 8\n                for k in intensities:\n                    color &= ~0xff\n                    color |= k\n                    colors[c] = color\n                    c += 1\n\n        grayscale_start = 0x08\n        grayscale_end = 0xf8\n        grayscale_step = 10\n        c = 232\n        for hex_int in range(grayscale_start, grayscale_end, grayscale_step):\n            color = (hex_int << 16) | (hex_int << 8) | hex_int\n            colors[c] = color\n            c += 1\n\n        return colors\n\n    @staticmethod\n    def _hex_str_to_int(hex_color: str) -> int:\n        \'\'\'\n        Convert string hex representations, color names, to hex int.\n        \'\'\'\n        hex_str = hex_color.strip().lower()\n        if hex_str.startswith(\'#\'):\n            hex_str = hex_str[1:]\n        elif hex_str.startswith(\'0x\'):\n            hex_str = hex_str[2:]\n        else: # will raise key error\n            return _COLOR_NAME_X11[hex_str]\n        return int(hex_str, 16)\n\n    @classmethod\n    def _to_ansi(cls, hex_color: tp.Union[int, str]) -> int:\n        \'\'\'\n        Find the nearest ANSI color given the hex value, encoded either as string or integer.\n        \'\'\'\n\n        # normalize hex colors as integers pre cache\n        if isinstance(hex_color, str):\n            hex_color = cls._hex_str_to_int(hex_color)\n\n        if hex_color not in cls._HEX_TO_ANSI_CACHE:\n\n            if not cls._ANSI_TO_HEX:\n                cls._ANSI_TO_HEX = cls._get_ansi_to_hex_map()\n\n            diffs = {}\n            for ansi, rgb in cls._ANSI_TO_HEX.items():\n                # for all ansi, find distance, store as key\n                diffs[cls._diff(rgb, hex_color)] = ansi\n\n            ansi = diffs[min(diffs.keys())]\n            cls._HEX_TO_ANSI_CACHE[hex_color] = ansi\n\n        return cls._HEX_TO_ANSI_CACHE[hex_color]\n\n    @classmethod\n    def format_terminal(cls,\n            hex_color: tp.Union[int, str],\n            text: str) -> str:\n        \'\'\'\n        Given a hex color and text, return a string formatted for ANSI colors\n        \'\'\'\n        ansi = cls._to_ansi(hex_color)\n        return \'\\033[38;5;{ansi:d}m{text:s}\\033[0m\'.format(\n                ansi=ansi,\n                text=text)\n\n    @classmethod\n    def format_html(cls,\n            hex_color: tp.Union[int, str],\n            text: str) -> str:\n        \'\'\'\n        Given a hex color and text, return a string formatted for ANSI colors\n        \'\'\'\n        if isinstance(hex_color, str):\n            hex_int = cls._hex_str_to_int(hex_color)\n        else:\n            hex_int = hex_color\n\n        color = \'#\' + format(hex_int, \'x\')\n        return \'<span style=""color: {color}"">{text}</span>\'.format(\n                color=color,\n                text=text)\n'"
static_frame/core/display_html_datatables.py,0,"b'\n\nfrom functools import partial\n\nVERSION_BOOTSTRAP = \'4.3.1\'\nVERSION_DATATABLES = \'1.10.19\'\nVERSION_JQUERY = \'3.3.1\'\nVERSION_JQUERY_UI = \'1.12.0\'\n\nTEMPLATE = partial(\'\'\'\n<!doctype html>\n<html>\n<head>\n    <title>StaticFrame</title>\n\n    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">\n    <meta charset=""UTF-8"">\n\n    <link rel=""stylesheet""\n        href=""https://maxcdn.bootstrapcdn.com/bootstrap/{version_bootstrap}/css/bootstrap.min.css"" />\n    <link rel=""stylesheet""\n        href=""https://cdn.datatables.net/{version_datatables}/css/jquery.dataTables.min.css"" />\n\n    <script src=""https://code.jquery.com/jquery-{version_jquery}.min.js""></script>\n    <script src=""https://code.jquery.com/ui/{version_jquery_ui}/jquery-ui.min.js""></script>\n    <script src=""https://cdn.datatables.net/{version_datatables}/js/jquery.dataTables.min.js""></script>\n    <script src=""https://maxcdn.bootstrapcdn.com/bootstrap/{version_bootstrap}/js/bootstrap.min.js""></script>\n\n    <script>\n        $(document).ready(function() {{\n            $(""#{}"").DataTable();\n            }});\n    </script>\n    <style>\n        body {{\n            padding-top: 50px;\n            overflow-y:scroll;\n        }}\n        table td, th {{\n            font-size: 80%;\n        }}\n        table th {{\n            background: #dddddd;\n        }}\n    </style>\n</head>\n\n<body>\n<div class=""container-fluid body-main"">\n{}\n</div>\n</body>\n</html>\n\'\'\'.format, version_bootstrap=VERSION_BOOTSTRAP,\n        version_datatables=VERSION_DATATABLES,\n        version_jquery=VERSION_JQUERY,\n        version_jquery_ui=VERSION_JQUERY_UI)\n'"
static_frame/core/doc_str.py,0,"b'\'\'\'\nStorage for common doc strings and templates shared in non-related classes and methods.\n\'\'\'\n\n\nimport typing as tp\n\nfrom static_frame.core.util import AnyCallable\n\n#NOTE: for kwargs, it is sometimes useful to only define the string, not the variable name, as in some contexts different variable names are use same conceptual entity.\n\nOWN_INDEX = \'\'\'own_index: Flag the passed index as ownable by this :obj:`static_frame.{class_name}`. Primarily used by internal clients.\'\'\'\n\nOWN_DATA = \'\'\'own_data: Flag the data values as ownable by this :obj:`static_frame.{class_name}`. Primarily used by internal clients.\'\'\'\n\nOWN_COLUMNS = \'\'\'own_columns: Flag the passed columns as ownable by this :obj:`static_frame.{class_name}`. Primarily used by internal clients.\'\'\'\n\nINDEX_INITIALIZER = \'\'\'An iterable of unique, hashable values, or another ``Index`` or ``IndexHierarchy``, to be used as the labels of the index.\'\'\'\n\nLOC_SELECTOR = \'\'\'A loc selector, either a label, a list of labels, a slice of labels, or a Boolean array.\'\'\'\n\nILOC_SELECTOR = \'\'\'An iloc selector, either an index, a list of indicces, a slice of indices, or a Boolean array.\'\'\'\n\nDTYPE_SPECIFIER = \'\'\'dtype: A value suitable for specyfying a NumPy dtype, such as a Python type (float), NumPy array protocol strings (\'f8\'), or a dtype instance.\'\'\'\n\nAXIS = \'\'\'axis: Integer specifying axis, where 0 is rows and 1 is columns. Axis 0 is set by default.\'\'\'\n\nclass DOC_TEMPLATE:\n\n    #---------------------------------------------------------------------------\n    # functions\n\n    to_html = \'\'\'\n    Return an HTML table representation of this :obj:`static_frame.{class_name}` using standard TABLE, TR, and TD tags. This is not a complete HTML page.\n\n    Args:\n        config: Optional :obj:`static_frame.DisplayConfig` instance.\n\n    Returns:\n        :obj:`str`\n    \'\'\'\n\n    to_html_datatables = \'\'\'\n    Return a complete HTML representation of this :obj:`static_frame.{class_name}` using the DataTables JS library for table naviagation and search. The page links to CDNs for JS resources, and thus will not fully render without an internet connection.\n\n    Args:\n        fp: optional file path to write; if not provided, a temporary file will be created. Note: the caller is responsible for deleting this file.\n        show: if True, the file will be opened with a webbrowser.\n        config: Optional :obj:`static_frame.DisplayConfig` instance.\n\n    Returns:\n        :obj:`str`, absolute file path to the file written.\n    \'\'\'\n\n    clip = \'\'\'Apply a clip opertion to this :obj:`static_frame.{class_name}`. Note that clip operations can be applied to object types, but cannot be applied to non-numerical objects (e.g., strings, None)\'\'\'\n\n    ufunc_skipna = \'\'\'{header}\n\n    Args:\n        axis: Axis, defaulting to axis 0.\n        skipna: Skip missing (NaN) values, defaulting to True.\n\n    \'\'\'\n\n    label_widths_at_depth = \'\'\'\n    A generator of pairs, where each pair is the label and the count of that label found at the depth specified by  ``depth_level``.\n\n    Args:\n        depth_level: a depth level, starting from zero.\n    \'\'\'\n\n    interface = \'\'\'\n    A :obj:`static_frame.Frame` documenting the interface of this class.\n    \'\'\'\n\n    name = \'\'\'\n    A hashable label attached to this container.\n\n    Returns:\n        :obj:`Hashable`\n    \'\'\'\n\n    values_2d = \'\'\'\n    A 2D NumPy array of all values in the :obj:`{class_name}`. As this is a single array, hereogenous columnar types might be coerced to a compatible type.\n    \'\'\'\n\n    values_1d = \'\'\'\n    A 1D NumPy array of the values in the :obj:`{class_name}`. This array will have the same dtype as the container.\n    \'\'\'\n\n\n    #---------------------------------------------------------------------------\n    # dict entries\n\n    apply = dict(\n            doc=\'Apply a function to each value.\',\n            func=\'func: A function that takes a value.\',\n            dtype=DTYPE_SPECIFIER\n            )\n    argminmax = dict(\n            skipna=\'skipna: if True, NaN or None values will be ignored; if False, a found NaN will propagate.\',\n            axis=\'axis: Axis upon which to evaluate contiguous missing values, where 0 is vertically (between row values) and 1 is horizontally (between column values).\'\n    )\n\n    astype = dict(\n            dtype=DTYPE_SPECIFIER\n            )\n\n\n\n    container_init = dict(\n            index=\'\'\'index: Optional index initializer. If provided in addition to data values, lengths must be compatible.\'\'\',\n            columns=\'\'\'columns: Optional column initializer. If provided in addition to data values, lengths must be compatible.\n            \'\'\',\n            own_index=OWN_INDEX,\n            own_data=OWN_DATA,\n            own_columns=OWN_COLUMNS\n            )\n\n    constructor_frame = dict(\n            dtypes=\'\'\'dtypes: Optionally provide an iterable of dtypes, equal in length to the length of each row, or a mapping by column name. If a dtype is given as None, NumPy\'s default type determination will be used.\n            \'\'\',\n            name=\'name: A hashable object to name the Frame.\',\n            consolidate_blocks=\'consolidate_blocks: Optionally consolidate adjacent same-typed columns into contiguous arrays.\'\n    )\n\n    duplicated = dict(\n            exclude_first=\'exclude_first: Boolean to select if the first duplicated value is excluded.\',\n            exclude_last=\'exclude_last: Boolean to select if the last duplicated value is excluded.\',\n            axis=AXIS,\n            )\n\n    display = dict(\n            doc=\'Return a :obj:`static_frame.Display`, capable of providing a string representation.\',\n            config=\'config: A :obj:`static_frame.DisplayConfig` instance. If not provided, the :obj:`static_frame.DisplayActive` will be used.\'\n            )\n\n    equals = dict(\n            doc=\'Return a :obj:`bool` from comparison to any other object.\',\n            compare_name=""compare_name: Include equality of the container\'s name (and all composed containers) in the comparison."",\n            compare_dtype=""compare_dtype: Include equality of the container\'s dtype (and all composed containers) in the comparison."",\n            compare_class=""compare_class: Include equality of the container\'s class (and all composed containers) in the comparison."",\n            skipna=""skipna: If True, comparisons between missing valeus are equal."",\n            )\n    insert = dict(\n            key_before=""key: Label before which the new container will be inserted."",\n            key_after=""key: Label after which the new container will be inserted."",\n            container=""container: Container to be inserted."",\n            fill_value=\'fill_value: A value to be used to fill space after reindexing the new container.\'\n            )\n    join = dict(\n            left_depth_level=""left_depth_level: Specify one or more left index depths to include in the join predicate."",\n            left_columns=""left_columns: Specify one or more left columns to include in the join predicate."",\n            right_depth_level=""right_depth_level: Specify one or more right index depths to include in the join predicate."",\n            right_columns=""right_columns: Specify one or more right columns to include in the join predicate."",\n            left_template=""left_template: Provide a format string for naming left columns in the joined result."",\n            right_template=""right_template: Provide a format string for naming right columns in the joined result."",\n            fill_value=\'fill_value: A value to be used to fill space created in the join.\',\n            composite_index=\'composite_index: If True, an index of tuples will be returned, formed from the left index label and the right index label; if False, an index of matching labels, if unique, will be returned.\',\n            composite_index_fill_value=\'composite_index_fill_value: Value to be used when forming a composite index when a label is missing.\'\n            )\n\n    reindex = dict(\n            doc=\'\'\'Return a new :obj:`{class_name}` with labels defined by the provided index. The size and ordering of the data is determined by the newly provided index, where data will continue to be aligned under labels found in both the new and the old index. Labels found only in the new index will be filled with ``fill_value``.\n            \'\'\',\n            index_initializer=INDEX_INITIALIZER,\n            fill_value=\'\'\'fill_value: A value to be used to fill space created by a new index that has values not found in the previous index.\'\'\',\n            own_index=OWN_INDEX,\n            own_columns=OWN_COLUMNS\n            )\n\n    relabel = dict(\n            doc =\'\'\'Return a new :obj:`{class_name}` with transformed labels on the index. The size and ordering of the data is never chagned in a relabeling operation. The resulting index must be unique.\n            \'\'\',\n            count=\'\'\'A positive integer drops that many outer-most levels; a negative integer drops that many inner-most levels.\'\'\',\n            level=\'\'\'A hashable value to be used as a new root level, extending or creating an ``IndexHierarchy``\'\'\',\n            relabel_input=\'\'\'One of the following types, used to create a new ``Index`` with the same size as the previous index. (a) A mapping (as a dictionary or ``Series``), used to lookup and transform the labels in the previous index. Previous labels not found in the mapping will be reused. (b) A function, returning a hashable, that is applied to each label in the previous index. (c) The ``IndexAutoFactory`` type, to apply an auto-incremented integer index. (d) An index initializer, i.e., either an iterable of hashables or an ``Index`` instance.\'\'\'\n            )\n\n    relabel_flat = dict(\n            doc=\'\'\'Return a new :obj:`{class_name}`, where an ``IndexHierarchy`` (if defined) is replaced with a flat, one-dimension index of tuples.\n            \'\'\',\n            )\n\n    relabel_add_level = dict(\n            doc=\'\'\'Return a new :obj:`{class_name}`, adding a new root level to an existing ``IndexHierarchy``, or creating an ``IndexHierarchy`` if one is not yet defined.\n            \'\'\',\n            level=\'\'\'A hashable value to be used as a new root level, extending or creating an ``IndexHierarchy``\'\'\',\n            )\n\n    relabel_drop_level = dict(\n            doc=\'\'\'Return a new :obj:`{class_name}`, dropping one or more levels from a either the root or the leaves of an ``IndexHierarchy``. The resulting index must be unique.\n            \'\'\',\n            count=\'\'\'A positive integer drops that many outer-most (root) levels; a negative integer drops that many inner-most (leaf)levels.\'\'\',\n            )\n    selector = dict(\n            key_loc=LOC_SELECTOR,\n            key_iloc=ILOC_SELECTOR,\n            )\n\n    head = dict(\n            doc=\'\'\'Return a :obj:`{class_name}` consisting only of the top elements as specified by ``count``.\n            \'\'\',\n            count=\'\'\'count: Number of elements to be returned from the top of the :obj:`{class_name}`\'\'\',\n            )\n\n    tail = dict(\n            doc=\'\'\'Return a :obj:`{class_name}` consisting only of the bottom elements as specified by ``count``.\n            \'\'\',\n            count=\'\'\'count: Number of elements to be returned from the bottom of the :obj:`{class_name}`\'\'\',\n            )\n\n\n    index_init = dict(\n            args = f\'\'\'\n        Args:\n            labels: {INDEX_INITIALIZER}\n            name: A hashable object to name the Index.\n            loc_is_iloc: Optimization when a contiguous integer index is provided as labels. Generally only set by internal clients.\n            {DTYPE_SPECIFIER}\'\'\'\n            )\n\n    index_date_time_init = dict(\n            args = \'\'\'\n        Args:\n            labels: Iterable of hashable values to be used as the index labels. If strings, NumPy datetime conversions will be applied.\n            name: A hashable object to name the Index.\n            \'\'\'\n            )\n\n    from_pandas = dict(\n            own_data=\'\'\'own_data: If True, the underlying NumPy data array will be made immutable and used without a copy.\'\'\',\n            own_index=\'\'\'own_index: If True, the underlying NumPy index label array will be made immutable and used without a copy.\'\'\',\n            own_columns=\'\'\'own_columns: If True, the underlying NumPy column label array will be made immutable and used without a copy.\'\'\',\n            )\n\n\n    fillna = dict(\n            limit=\'limit: Set the maximum count of missing values (NaN or None) to be filled per contiguous region of missing vlaues. A value of 0 is equivalent to no limit.\',\n            value=\'value: Value to be used to replace missing values (NaN or None).\',\n            axis=\'axis: Axis upon which to evaluate contiguous missing values, where 0 is vertically (between row values) and 1 is horizontally (between column values).\'\n    )\n\n\n\n    mloc = dict(\n            doc_int=\'The memory location, represented as an integer, of the underlying NumPy array.\',\n            doc_array=\'The memory locations, represented as an array of integers, of the underlying NumPy arrays.\',\n    )\n\n\n    map_any = dict(\n            doc=\'Apply a mapping; for values not in the mapping, the value is returned.\',\n            mapping=\'mapping: A mapping type, such as a dictionary or Series.\',\n            dtype=DTYPE_SPECIFIER,\n            )\n\n    map_fill = dict(\n            doc = \'Apply a mapping; for values not in the mapping, the ``fill_value`` is returned.\',\n            mapping = \'mapping: A mapping type, such as a dictionary or Series.\',\n            fill_value = \'fill_value: Value to be returned if the values is not a key in the mapping.\',\n            dtype=DTYPE_SPECIFIER\n            )\n\n    map_all = dict(\n            doc = \'Apply a mapping; for values not in the mapping, an Exception is raised.\',\n            mapping=\'mapping: A mapping type, such as a dictionary or Series.\',\n            dtype=DTYPE_SPECIFIER\n            )\n\n\n\ndef doc_inject(*,\n        selector: tp.Optional[str] = None,\n        **kwargs: object\n        ) -> tp.Callable[[AnyCallable], AnyCallable]:\n    \'\'\'\n    Args:\n        selector: optionally specify name of doc template dictionary to use; if not provided, the name of the function will be used.\n    \'\'\'\n    def decorator(f: AnyCallable) -> AnyCallable:\n\n        assert f.__doc__ is not None, f\'{f} must have a docstring!\'\n\n        nonlocal selector\n        selector = f.__name__ if selector is None else selector\n        # get doc string, template with decorator args, then template existing doc string\n        doc_src = getattr(DOC_TEMPLATE, selector)\n        if isinstance(doc_src, str):\n            doc = doc_src.format(**kwargs)\n            f.__doc__ = f.__doc__.format(doc)\n        else: # assume it is a dictionary\n            # try to format each value\n            doc_dict = {k: v.format(**kwargs) for k, v in doc_src.items()}\n            f.__doc__ = f.__doc__.format(**doc_dict)\n\n        return f\n\n    return decorator\n'"
static_frame/core/exception.py,0,"b""import warnings\n\nclass ErrorInit(RuntimeError):\n    '''Error in Container initialization.\n    '''\n\nclass ErrorInitTypeBlocks(ErrorInit):\n    '''Error in TypeBlocks initialization.\n    '''\n\nclass ErrorInitSeries(ErrorInit):\n    '''Error in Series initialization.\n    '''\n\nclass ErrorInitFrame(ErrorInit):\n    '''Error in Frame (and derived Frame) initialization.\n    '''\n\nclass ErrorInitIndex(ErrorInit):\n    '''Error in IndexBase (and derived Index) initialization.\n    '''\n\nclass ErrorInitIndexLevel(ErrorInit):\n    '''Error in IndexBase (and derived Index) initialization.\n    '''\n\nclass ErrorInitBus(ErrorInit):\n    '''Error in Bus initialization.\n    '''\n\nclass ErrorInitStore(ErrorInit):\n    '''Error in Store initialization.\n    '''\n\nclass ErrorInitStoreConfig(ErrorInit):\n    '''Error in StoreConfig initialization.\n    '''\n\n#-------------------------------------------------------------------------------\n\nclass LocEmpty(RuntimeError):\n    pass\n\nclass LocInvalid(RuntimeError):\n    pass\n\nclass AxisInvalid(RuntimeError):\n    pass\n\n\n\n#-------------------------------------------------------------------------------\n\nclass StoreFileMutation(RuntimeError):\n    '''\n    A Stores file was mutated in an unexpected way.\n    '''\n#-------------------------------------------------------------------------------\n\ndef deprecated(message: str = '') -> None:\n    # using UserWarning to get out of pytest with  -p no:warnings\n    warnings.warn(message, UserWarning, stacklevel=2) #pragma: no cover\n    # raise DeprecationWarning()"""
static_frame/core/frame.py,70,"b'import typing as tp\nimport sqlite3\nimport csv\nimport json\n\nfrom functools import partial\nfrom itertools import chain\nfrom itertools import repeat\nfrom itertools import product\n\nimport numpy as np\n\nfrom numpy.ma import MaskedArray\n\nfrom static_frame.core.util import UFunc\nfrom static_frame.core.util import DEFAULT_SORT_KIND\nfrom static_frame.core.util import DTYPE_FLOAT_DEFAULT\nfrom static_frame.core.util import EMPTY_TUPLE\nfrom static_frame.core.util import DTYPE_OBJECT\n\nfrom static_frame.core.util import NULL_SLICE\nfrom static_frame.core.util import KEY_MULTIPLE_TYPES\nfrom static_frame.core.util import INT_TYPES\nfrom static_frame.core.util import GetItemKeyType\nfrom static_frame.core.util import GetItemKeyTypeCompound\nfrom static_frame.core.util import KeyOrKeys\nfrom static_frame.core.util import PathSpecifier\nfrom static_frame.core.util import PathSpecifierOrFileLike\nfrom static_frame.core.util import PathSpecifierOrFileLikeOrIterator\nfrom static_frame.core.util import NameType\nfrom static_frame.core.util import NAME_DEFAULT\n\nfrom static_frame.core.util import DtypesSpecifier\nfrom static_frame.core.util import DtypeSpecifier\n\nfrom static_frame.core.util import FILL_VALUE_DEFAULT\nfrom static_frame.core.util import path_filter\nfrom static_frame.core.util import Bloc2DKeyType\n\nfrom static_frame.core.util import IndexSpecifier\nfrom static_frame.core.util import IndexInitializer\nfrom static_frame.core.util import IndexConstructor\nfrom static_frame.core.util import IndexConstructors\n\nfrom static_frame.core.util import FrameInitializer\nfrom static_frame.core.util import FRAME_INITIALIZER_DEFAULT\nfrom static_frame.core.util import column_2d_filter\nfrom static_frame.core.util import column_1d_filter\n\nfrom static_frame.core.util import name_filter\nfrom static_frame.core.util import _gen_skip_middle\nfrom static_frame.core.util import iterable_to_array_1d\nfrom static_frame.core.util import iterable_to_array_nd\n\nfrom static_frame.core.util import isin\nfrom static_frame.core.util import array_to_duplicated\nfrom static_frame.core.util import ufunc_set_iter\nfrom static_frame.core.util import array2d_to_tuples\nfrom static_frame.core.util import _read_url\nfrom static_frame.core.util import write_optional_file\nfrom static_frame.core.util import ufunc_unique\nfrom static_frame.core.util import concat_resolved\nfrom static_frame.core.util import DepthLevelSpecifier\nfrom static_frame.core.util import array_to_groups_and_locations\nfrom static_frame.core.util import is_callable_or_mapping\nfrom static_frame.core.util import CallableOrCallableMap\nfrom static_frame.core.util import ufunc_axis_skipna\nfrom static_frame.core.util import AnyCallable\n\nfrom static_frame.core.util import argmin_2d\nfrom static_frame.core.util import argmax_2d\nfrom static_frame.core.util import resolve_dtype\nfrom static_frame.core.util import key_normalize\nfrom static_frame.core.util import get_tuple_constructor\nfrom static_frame.core.util import dtype_to_na\nfrom static_frame.core.util import is_hashable\nfrom static_frame.core.util import reversed_iter\n\nfrom static_frame.core.util import Join\nfrom static_frame.core.util import Pair\nfrom static_frame.core.util import PairLeft\nfrom static_frame.core.util import PairRight\n\nfrom static_frame.core.node_selector import InterfaceGetItem\nfrom static_frame.core.node_selector import InterfaceSelectTrio\nfrom static_frame.core.node_selector import InterfaceAssignQuartet\nfrom static_frame.core.node_selector import InterfaceAsType\nfrom static_frame.core.node_str import InterfaceString\nfrom static_frame.core.node_dt import InterfaceDatetime\n\nfrom static_frame.core.index_correspondence import IndexCorrespondence\nfrom static_frame.core.container import ContainerOperand\n\nfrom static_frame.core.container_util import matmul\nfrom static_frame.core.container_util import index_from_optional_constructor\nfrom static_frame.core.container_util import axis_window_items\nfrom static_frame.core.container_util import bloc_key_normalize\nfrom static_frame.core.container_util import rehierarch_from_type_blocks\nfrom static_frame.core.container_util import rehierarch_from_index_hierarchy\nfrom static_frame.core.container_util import array_from_value_iter\nfrom static_frame.core.container_util import dtypes_mappable\nfrom static_frame.core.container_util import key_to_ascending_key\nfrom static_frame.core.container_util import index_constructor_empty\nfrom static_frame.core.container_util import pandas_version_under_1\nfrom static_frame.core.container_util import pandas_to_numpy\nfrom static_frame.core.container_util import arrays_from_index_frame\n\nfrom static_frame.core.node_iter import IterNodeApplyType\nfrom static_frame.core.node_iter import IterNodeType\n\nfrom static_frame.core.node_iter import IterNodeAxis\nfrom static_frame.core.node_iter import IterNodeDepthLevelAxis\nfrom static_frame.core.node_iter import IterNodeWindow\nfrom static_frame.core.node_iter import IterNodeGroupAxis\nfrom static_frame.core.node_iter import IterNodeNoArg\n\n\nfrom static_frame.core.display import DisplayConfig\nfrom static_frame.core.display import DisplayActive\nfrom static_frame.core.display import Display\nfrom static_frame.core.display import DisplayFormats\nfrom static_frame.core.display import DisplayHeader\n\nfrom static_frame.core.type_blocks import TypeBlocks\n\nfrom static_frame.core.series import Series\nfrom static_frame.core.series import RelabelInput\n\nfrom static_frame.core.index_base import IndexBase\n\nfrom static_frame.core.index import Index\nfrom static_frame.core.index import IndexGO\nfrom static_frame.core.index import _index_initializer_needs_init\nfrom static_frame.core.index import immutable_index_filter\n\nfrom static_frame.core.index_hierarchy import IndexHierarchy\nfrom static_frame.core.index_hierarchy import IndexHierarchyGO\n\nfrom static_frame.core.index_auto import IndexAutoFactory\nfrom static_frame.core.index_auto import IndexAutoFactoryType\n\nfrom static_frame.core.assign import Assign\n\nfrom static_frame.core.store_filter import StoreFilter\nfrom static_frame.core.store_filter import STORE_FILTER_DEFAULT\n\nfrom static_frame.core.exception import ErrorInitFrame\nfrom static_frame.core.exception import AxisInvalid\n\nfrom static_frame.core.doc_str import doc_inject\n\n# Alias str for type annotations so as to not get confused with str property on\nString = str\n\nif tp.TYPE_CHECKING:\n    import pandas #pylint: disable=W0611 #pragma: no cover\n    from xarray import Dataset #pylint: disable=W0611 #pragma: no cover\n    import pyarrow #pylint: disable=W0611 #pragma: no cover\n\n\n\n@doc_inject(selector=\'container_init\', class_name=\'Frame\')\nclass Frame(ContainerOperand):\n    \'\'\'\n    A two-dimensional ordered, labelled collection, immutable and of fixed size.\n\n    Args:\n        data: A Frame initializer, given as either a NumPy array, a single value (to be used to fill a shape defined by ``index`` and ``columns``), or an iterable suitable to given to the NumPy array constructor.\n        {index}\n        {columns}\n        {own_data}\n        {own_index}\n        {own_columns}\n    \'\'\'\n\n    __slots__ = (\n            \'_blocks\',\n            \'_columns\',\n            \'_index\',\n            \'_name\'\n            )\n\n    _blocks: TypeBlocks\n    _columns: IndexBase\n    _index: IndexBase\n    _name: tp.Hashable\n\n    _COLUMNS_CONSTRUCTOR = Index\n    _COLUMNS_HIERARCHY_CONSTRUCTOR = IndexHierarchy\n\n    _NDIM: int = 2\n\n    #---------------------------------------------------------------------------\n    # constructors\n\n    @classmethod\n    def from_series(cls,\n            series: Series,\n            *,\n            name: tp.Hashable = None,\n            columns_constructor: IndexConstructor = None,\n            ):\n        \'\'\'\n        Frame constructor from a Series:\n\n        Args:\n            series: A Series instance, to be realized as single column, with the column label taken from the `name` attribute.\n        \'\'\'\n        return cls(TypeBlocks.from_blocks(series.values),\n                index=series.index,\n                columns=(series.name,),\n                name=name,\n                columns_constructor=columns_constructor,\n                own_data=True,\n                own_index=True,\n                )\n\n    @classmethod\n    def from_element(cls,\n            element: tp.Any,\n            *,\n            index: IndexInitializer,\n            columns: IndexInitializer,\n            dtype: DtypeSpecifier = None,\n            name: tp.Hashable = None,\n            index_constructor: IndexConstructor = None,\n            columns_constructor: IndexConstructor = None,\n            own_index: bool = False,\n            own_columns: bool = False\n            ):\n        \'\'\'\n        Create a Frame from an element, i.e., a single value stored in a single cell. Both ``index`` and ``columns`` are required, and cannot be specified with ``IndexAutoFactory``.\n        \'\'\'\n        if own_columns:\n            columns_final = columns\n        else:\n            columns_final = index_from_optional_constructor(columns,\n                    default_constructor=cls._COLUMNS_CONSTRUCTOR,\n                    explicit_constructor=columns_constructor\n                    )\n        if own_index:\n            index_final = index\n        else:\n            index_final = index_from_optional_constructor(index,\n                    default_constructor=Index,\n                    explicit_constructor=index_constructor\n                    )\n\n        array = np.full(\n                (len(index_final), len(columns_final)),\n                fill_value=element,\n                dtype=dtype)\n        array.flags.writeable = False\n\n        return cls(TypeBlocks.from_blocks(array),\n                index=index_final,\n                columns=columns_final,\n                name=name,\n                own_data=True,\n                own_index=True,\n                own_columns=True,\n                )\n\n\n    @classmethod\n    def from_elements(cls,\n            elements: tp.Iterable[tp.Any],\n            *,\n            index: tp.Union[IndexInitializer, IndexAutoFactoryType] = None,\n            columns: tp.Union[IndexInitializer, IndexAutoFactoryType] = None,\n            dtype: DtypeSpecifier = None,\n            name: tp.Hashable = None,\n            index_constructor: IndexConstructor = None,\n            columns_constructor: IndexConstructor = None,\n            own_index: bool = False,\n            own_columns: bool = False\n            ):\n        \'\'\'\n        Create a Frame from an iterable of elements, to be formed into a ``Frame`` with a single column.\n        \'\'\'\n\n        # will be immutable\n        array, _ = iterable_to_array_1d(elements, dtype=dtype)\n\n        columns_empty = index_constructor_empty(columns)\n        index_empty = index_constructor_empty(index)\n\n        #-----------------------------------------------------------------------\n        if own_columns:\n            columns_final = columns\n            col_count = len(columns_final)\n        elif columns_empty:\n            col_count = 1\n            columns_final = IndexAutoFactory.from_optional_constructor(\n                    col_count, # default to one colmns\n                    default_constructor=cls._COLUMNS_CONSTRUCTOR,\n                    explicit_constructor=columns_constructor\n                    )\n        else:\n            columns_final = index_from_optional_constructor(columns,\n                    default_constructor=cls._COLUMNS_CONSTRUCTOR,\n                    explicit_constructor=columns_constructor\n                    )\n            col_count = len(columns_final)\n\n        #-----------------------------------------------------------------------\n        row_count = len(array)\n        if own_index:\n            index_final = index\n        elif index_empty:\n            index_final = IndexAutoFactory.from_optional_constructor(\n                    row_count,\n                    default_constructor=Index,\n                    explicit_constructor=index_constructor\n                    )\n        else:\n            index_final = index_from_optional_constructor(index,\n                    default_constructor=Index,\n                    explicit_constructor=index_constructor\n                    )\n\n        #-----------------------------------------------------------------------\n        if col_count > 1:\n            array = np.tile(array.reshape((row_count, 1)), (1, col_count))\n            array.flags.writeable = False\n\n        return cls(TypeBlocks.from_blocks(array),\n                index=index_final,\n                columns=columns_final,\n                name=name,\n                own_data=True,\n                own_index=True,\n                own_columns=True,\n                )\n\n\n\n    #---------------------------------------------------------------------------\n    @classmethod\n    @doc_inject(selector=\'constructor_frame\')\n    def from_concat(cls,\n            frames: tp.Iterable[tp.Union[\'Frame\', Series]],\n            *,\n            axis: int = 0,\n            union: bool = True,\n            index: tp.Union[IndexInitializer, IndexAutoFactoryType] = None,\n            columns: tp.Union[IndexInitializer, IndexAutoFactoryType] = None,\n            name: tp.Hashable = None,\n            fill_value: object = np.nan,\n            consolidate_blocks: bool = False\n            ) -> \'Frame\':\n        \'\'\'\n        Concatenate multiple Frames into a new Frame. If index or columns are provided and appropriately sized, the resulting Frame will use those indices. If the axis along concatenation (index for axis 0, columns for axis 1) is unique after concatenation, it will be preserved; otherwise, a new index or an :obj:`IndexAutoFactory` must be supplied.\n\n        Args:\n            frames: Iterable of Frames.\n            axis: Integer specifying 0 to concatenate supplied Frames vertically (aligning on columns), 1 to concatenate horizontally (aligning on rows).\n            union: If True, the union of the aligned indices is used; if False, the intersection is used.\n            index: Optionally specify a new index.\n            columns: Optionally specify new columns.\n            {name}\n            {consolidate_blocks}\n\n        Returns:\n            :obj:`static_frame.Frame`\n        \'\'\'\n\n        # when doing axis 1 concat (growin horizontally) Series need to be presented as rows (axis 0)\n        # TODO: check for Series that do not have names\n        frames = [f if isinstance(f, Frame) else f.to_frame(axis) for f in frames]\n\n        own_columns = False\n        own_index = False\n\n        # End quickly if empty iterable\n        if not frames:\n            return cls(\n                    index=index,\n                    columns=columns,\n                    name=name,\n                    own_columns=own_columns,\n                    own_index=own_index)\n\n        # switch if we have reduced the columns argument to an array\n        from_array_columns = False\n        from_array_index = False\n\n        if axis == 1: # stacks columns (extends rows horizontally)\n            # index can be the same, columns must be redefined if not unique\n            if columns is IndexAutoFactory:\n                columns = None # let default creation happen\n            elif columns is None:\n                # returns immutable array\n                columns = concat_resolved([frame._columns.values for frame in frames])\n                from_array_columns = True\n                # avoid sort for performance; always want rows if ndim is 2\n                if len(ufunc_unique(columns, axis=0)) != len(columns):\n                    raise ErrorInitFrame(\'Column names after horizontal concatenation are not unique; supply a columns argument or IndexAutoFactory.\')\n\n            if index is IndexAutoFactory:\n                raise ErrorInitFrame(\'for axis 1 concatenation, index must be used for reindexing row alignment: IndexAutoFactory is not permitted\')\n            elif index is None:\n                # get the union index, or the common index if identical\n                index = ufunc_set_iter(\n                        (frame._index.values for frame in frames),\n                        union=union,\n                        assume_unique=True # all from indices\n                        )\n                index.flags.writeable = False\n                from_array_index = True\n\n            def blocks():\n                for frame in frames:\n                    if len(frame.index) != len(index) or (frame.index != index).any():\n                        frame = frame.reindex(index=index, fill_value=fill_value)\n                    for block in frame._blocks._blocks:\n                        yield block\n\n        elif axis == 0: # stacks rows (extends columns vertically)\n            if index is IndexAutoFactory:\n                index = None # let default creationn happen\n            elif index is None:\n                # returns immutable array\n                index = concat_resolved([frame._index.values for frame in frames])\n                from_array_index = True\n                # avoid sort for performance; always want rows if ndim is 2\n                if len(ufunc_unique(index, axis=0)) != len(index):\n                    raise ErrorInitFrame(\'Index names after vertical concatenation are not unique; supply an index argument or IndexAutoFactory.\')\n\n            if columns is IndexAutoFactory:\n                raise ErrorInitFrame(\'for axis 0 concatenation, columns must be used for reindexing and column alignment: IndexAutoFactory is not permitted\')\n            elif columns is None:\n                columns = ufunc_set_iter(\n                        (frame._columns.values for frame in frames),\n                        union=union,\n                        assume_unique=True\n                        )\n                columns.flags.writeable = False\n                from_array_columns = True\n\n            def blocks():\n                aligned_frames = []\n                previous_frame = None\n                block_compatible = True\n                reblock_compatible = True\n\n                for frame in frames:\n                    if len(frame.columns) != len(columns) or (frame.columns != columns).any():\n                        frame = frame.reindex(columns=columns, fill_value=fill_value)\n\n                    aligned_frames.append(frame)\n                    # column size is all the same by this point\n                    if previous_frame is not None: # after the first\n                        if block_compatible:\n                            block_compatible &= frame._blocks.block_compatible(\n                                    previous_frame._blocks,\n                                    axis=1) # only compare columns\n                        if reblock_compatible:\n                            reblock_compatible &= frame._blocks.reblock_compatible(\n                                    previous_frame._blocks)\n                    previous_frame = frame\n\n                if block_compatible or reblock_compatible:\n                    if not block_compatible and reblock_compatible:\n                        # after reblocking, will be compatible\n                        type_blocks = [f._blocks.consolidate() for f in aligned_frames]\n                    else: # blocks by column are compatible\n                        type_blocks = [f._blocks for f in aligned_frames]\n\n                    # all TypeBlocks have the same number of blocks by here\n                    for block_idx in range(len(type_blocks[0]._blocks)):\n                        block_parts = []\n                        for frame_idx in range(len(type_blocks)):\n                            b = column_2d_filter(\n                                    type_blocks[frame_idx]._blocks[block_idx])\n                            block_parts.append(b)\n                        # returns immutable array\n                        yield concat_resolved(block_parts)\n                else: # blocks not alignable\n                    # break into single column arrays for maximum type integrity; there might be an alternative reblocking that could be more efficient, but determining that shape might be complex\n                    for i in range(len(columns)):\n                        block_parts = [\n                            f._blocks._extract_array(column_key=i) for f in aligned_frames]\n                        yield concat_resolved(block_parts)\n        else:\n            raise NotImplementedError(f\'no support for {axis}\')\n\n        if from_array_columns:\n            if columns.ndim == 2: # we have a hierarchical index\n                columns = cls._COLUMNS_HIERARCHY_CONSTRUCTOR.from_labels(columns)\n                own_columns = True\n\n        if from_array_index:\n            if index.ndim == 2: # we have a hierarchical index\n                # NOTE: could pass index_constructors here\n                index = IndexHierarchy.from_labels(index)\n                own_index = True\n\n        if consolidate_blocks:\n            block_gen = lambda: TypeBlocks.consolidate_blocks(blocks())\n        else:\n            block_gen = blocks\n\n        return cls(TypeBlocks.from_blocks(block_gen()),\n                index=index,\n                columns=columns,\n                name=name,\n                own_data=True,\n                own_columns=own_columns,\n                own_index=own_index)\n\n    @classmethod\n    def from_concat_items(cls,\n            items: tp.Iterable[tp.Tuple[tp.Hashable, \'Frame\']],\n            *,\n            axis: int = 0,\n            union: bool = True,\n            name: tp.Hashable = None,\n            fill_value: object = np.nan,\n            consolidate_blocks: bool = False\n            ) -> \'Frame\':\n        \'\'\'\n        Produce a :obj:`Frame` with a hierarchical index from an iterable of pairs of labels, :obj:`Frame`. The :obj:`IndexHierarchy` is formed from the provided labels and the :obj:`Index` if each :obj:`Frame`.\n\n        Args:\n            items: Iterable of pairs of label, :obj:`Series`\n        \'\'\'\n        frames = []\n\n        def gen():\n            for label, frame in items:\n                # must normalize Series here to avoid down-stream confusion\n                if isinstance(frame, Series):\n                    frame = frame.to_frame(axis)\n\n                frames.append(frame)\n                if axis == 0:\n                    yield label, frame._index\n                elif axis == 1:\n                    yield label, frame._columns\n                # we have already evaluated AxisInvalid\n\n\n        # populates array_values as side effect\n        if axis == 0:\n            ih = IndexHierarchy.from_index_items(gen())\n            kwargs = dict(index=ih)\n        elif axis == 1:\n            ih = cls._COLUMNS_HIERARCHY_CONSTRUCTOR.from_index_items(gen())\n            kwargs = dict(columns=ih)\n        else:\n            raise AxisInvalid(f\'invalid axis: {axis}\')\n\n        return cls.from_concat(frames,\n                axis=axis,\n                union=union,\n                name=name,\n                fill_value=fill_value,\n                consolidate_blocks=consolidate_blocks,\n                **kwargs\n                )\n\n    @classmethod\n    @doc_inject(selector=\'constructor_frame\')\n    def from_records(cls,\n            records: tp.Iterable[tp.Any],\n            *,\n            index: tp.Optional[IndexInitializer] = None,\n            columns: tp.Optional[IndexInitializer] = None,\n            dtypes: DtypesSpecifier = None,\n            name: tp.Hashable = None,\n            consolidate_blocks: bool = False,\n            index_constructor: IndexConstructor = None,\n            columns_constructor: IndexConstructor = None,\n            own_index: bool = False,\n            own_columns: bool = False\n            ) -> \'Frame\':\n        \'\'\'Construct a Frame from an iterable of rows, where rows are defined as iterables, including tuples, lists, and arrays. If each row is a NamedTuple, and ``columns`` is not provided, column names will be derived from the NamedTuple fields.\n\n        Supplying ``dtypes`` will significantly improve performance, as otherwise columnar array types must be derived by element-wise examination.\n\n        For records defined as ``Series``, use ``Frame.from_concat``; for records defined as dictionary, use ``Frame.from_dict_records``; for creating a ``Frame`` from a single dictionary, where keys are column labels and values are columns, use ``Frame.from_dict``.\n\n        Args:\n            records: Iterable of row values, where row values are arrays, tuples, lists, or namedtuples. For dictionary records, use ``Frame.from_dict_records``.\n            index: Optionally provide an iterable of index labels, equal in length to the number of records. If a generator, this value will not be evaluated until after records are loaded.\n            columns: Optionally provide an iterable of column labels, equal in length to the number of elements in a row.\n            {dtypes}\n            {name}\n            {consolidate_blocks}\n\n        Returns:\n            :obj:`static_frame.Frame`\n        \'\'\'\n        # if records is np; we can just pass it to constructor, as is alrady a consolidate type\n        if isinstance(records, np.ndarray):\n            if dtypes is not None:\n                raise ErrorInitFrame(\'specifying dtypes when using NP records is not permitted\')\n            return cls(records,\n                    index=index,\n                    columns=columns,\n                    index_constructor=index_constructor,\n                    columns_constructor=columns_constructor,\n                    own_index=own_index,\n                    own_columns=own_columns,\n                    name=name,\n                    )\n\n        if not hasattr(records, \'__len__\'):\n            # might be a generator; must convert to sequence\n            rows = list(records)\n        else: # could be a sequence, or something like a dict view\n            rows = records\n        row_count = len(rows)\n\n        if not row_count:\n            if columns is not None: # we can create a zero-record Frame\n                return cls(\n                        columns=columns,\n                        columns_constructor=columns_constructor,\n                        own_columns=own_columns,\n                        name=name,\n                        )\n            raise ErrorInitFrame(\'no rows available in records, and no columns defined.\')\n\n        if hasattr(rows, \'__getitem__\'):\n            rows_to_iter = False\n            row_reference = rows[0]\n        else: # dict view, or other sized iterable that does not support getitem\n            rows_to_iter = True\n            row_reference = next(iter(rows))\n\n        if isinstance(row_reference, Series):\n            raise ErrorInitFrame(\'Frame.from_records() does not support Series records. Use Frame.from_concat() instead.\')\n        if isinstance(row_reference, dict):\n            raise ErrorInitFrame(\'Frame.from_records() does not support dictionary records. Use Frame.from_dict_records() instead.\')\n\n        column_name_getter = None\n        if columns is None and hasattr(row_reference, \'_fields\'): # NamedTuple\n            column_name_getter = row_reference._fields.__getitem__\n            columns = []\n\n        if dtypes:\n            dtypes_is_map = dtypes_mappable(dtypes)\n            def get_col_dtype(col_idx):\n                if dtypes_is_map:\n                    return dtypes.get(columns[col_idx], None)\n                return dtypes[col_idx]\n        else:\n            get_col_dtype = None\n\n        col_count = len(row_reference)\n\n        def get_value_iter(col_key):\n            rows_iter = rows if not rows_to_iter else iter(rows)\n            # this is possible to support ragged lists, but it noticeably reduces performance\n            return (row[col_key] for row in rows_iter)\n\n        def blocks() -> tp.Iterator[np.ndarray]:\n            # iterate over final column order, yielding 1D arrays\n            for col_idx in range(col_count):\n                if column_name_getter: # append as side effect of generator!\n                    columns.append(column_name_getter(col_idx))\n\n                values = array_from_value_iter(\n                        key=col_idx,\n                        idx=col_idx,\n                        get_value_iter=get_value_iter, get_col_dtype=get_col_dtype,\n                        row_count=row_count\n                        )\n                yield values\n\n        if consolidate_blocks:\n            block_gen = lambda: TypeBlocks.consolidate_blocks(blocks())\n        else:\n            block_gen = blocks\n\n        return cls(TypeBlocks.from_blocks(block_gen()),\n                index=index,\n                columns=columns,\n                name=name,\n                own_data=True,\n                index_constructor=index_constructor,\n                columns_constructor=columns_constructor,\n                own_index=own_index,\n                own_columns=own_columns,\n                )\n\n    @classmethod\n    @doc_inject(selector=\'constructor_frame\')\n    def from_dict_records(cls,\n            records: tp.Iterable[tp.Dict[tp.Hashable, tp.Any]],\n            *,\n            index: tp.Optional[IndexInitializer] = None,\n            dtypes: DtypesSpecifier = None,\n            name: tp.Hashable = None,\n            fill_value: object = np.nan,\n            consolidate_blocks: bool = False,\n            index_constructor: IndexConstructor = None,\n            columns_constructor: IndexConstructor = None,\n            own_index: bool = False,\n            ) -> \'Frame\':\n        \'\'\'Frame constructor from an iterable of dictionaries; column names will be derived from the union of all keys.\n\n        Args:\n            records: Iterable of row values, where row values are dictionaries.\n            index: Optionally provide an iterable of index labels, equal in length to the number of records. If a generator, this value will not be evaluated until after records are loaded.\n            {dtypes}\n            {name}\n            {consolidate_blocks}\n\n        Returns:\n            :obj:`static_frame.Frame`\n        \'\'\'\n        columns = []\n\n        if dtypes:\n            dtypes_is_map = dtypes_mappable(dtypes)\n            def get_col_dtype(col_idx):\n                if dtypes_is_map:\n                    return dtypes.get(columns[col_idx], None)\n                return dtypes[col_idx]\n        else:\n            get_col_dtype = None\n\n        if not hasattr(records, \'__len__\'):\n            # might be a generator; must convert to sequence\n            rows = list(records)\n        else: # could be a sequence, or something like a dict view\n            rows = records\n        row_count = len(rows)\n\n        if not row_count:\n            raise ErrorInitFrame(\'no rows available in records.\')\n\n        if hasattr(rows, \'__getitem__\'):\n            rows_to_iter = False\n        else: # dict view, or other sized iterable that does not support getitem\n            rows_to_iter = True\n\n        row_reference = {}\n        for row in rows: # produce a row that has a value for all observed keys\n            row_reference.update(row)\n\n        col_count = len(row_reference)\n\n        # define function to get generator of row values; may need to call twice, so need to get fresh row_iter each time\n        def get_value_iter(col_key):\n            rows_iter = rows if not rows_to_iter else iter(rows)\n            return (row.get(col_key, fill_value) for row in rows_iter)\n\n        def blocks():\n            # iterate over final column order, yielding 1D arrays\n            for col_idx, col_key in enumerate(row_reference.keys()):\n                columns.append(col_key)\n\n                values = array_from_value_iter(\n                        key=col_key,\n                        idx=col_idx,\n                        get_value_iter=get_value_iter,\n                        get_col_dtype=get_col_dtype,\n                        row_count=row_count\n                        )\n                yield values\n\n        if consolidate_blocks:\n            block_gen = lambda: TypeBlocks.consolidate_blocks(blocks())\n        else:\n            block_gen = blocks\n\n        return cls(TypeBlocks.from_blocks(block_gen()),\n                index=index,\n                columns=columns,\n                name=name,\n                own_data=True,\n                index_constructor=index_constructor,\n                columns_constructor=columns_constructor,\n                own_index=own_index,\n                )\n\n    @classmethod\n    @doc_inject(selector=\'constructor_frame\')\n    def from_records_items(cls,\n            items: tp.Iterator[tp.Tuple[tp.Hashable, tp.Iterable[tp.Any]]],\n            *,\n            columns: tp.Optional[IndexInitializer] = None,\n            dtypes: DtypesSpecifier = None,\n            name: tp.Hashable = None,\n            consolidate_blocks: bool = False) -> \'Frame\':\n        \'\'\'Frame constructor from iterable of pairs of index value, row (where row is an iterable).\n\n        Args:\n            items: Iterable of pairs of index label, row values, where row values are arrays, tuples, lists, dictionaries, or namedtuples.\n            columns: Optionally provide an iterable of column labels, equal in length to the length of each row.\n            {dtypes}\n            {name}\n            {consolidate_blocks}\n\n        Returns:\n            :obj:`static_frame.Frame`\n\n        \'\'\'\n        index = []\n\n        def gen():\n            for label, values in items:\n                index.append(label)\n                yield values\n\n        return cls.from_records(gen(),\n                index=index,\n                columns=columns,\n                dtypes=dtypes,\n                name=name,\n                consolidate_blocks=consolidate_blocks\n                )\n\n    @classmethod\n    @doc_inject(selector=\'constructor_frame\')\n    def from_dict_records_items(cls,\n            items: tp.Iterator[tp.Tuple[tp.Hashable, tp.Iterable[tp.Any]]],\n            *,\n            dtypes: DtypesSpecifier = None,\n            name: tp.Hashable = None,\n            consolidate_blocks: bool = False) -> \'Frame\':\n        \'\'\'Frame constructor from iterable of pairs of index value, row (where row is an iterable).\n\n        Args:\n            items: Iterable of pairs of index label, row values, where row values are arrays, tuples, lists, dictionaries, or namedtuples.\n            {dtypes}\n            {name}\n            {consolidate_blocks}\n\n        Returns:\n            :obj:`static_frame.Frame`\n\n        \'\'\'\n        index = []\n\n        def gen():\n            for label, values in items:\n                index.append(label)\n                yield values\n\n        return cls.from_dict_records(gen(),\n                index=index,\n                dtypes=dtypes,\n                name=name,\n                consolidate_blocks=consolidate_blocks\n                )\n\n\n\n    @classmethod\n    @doc_inject(selector=\'constructor_frame\')\n    def from_items(cls,\n            pairs: tp.Iterable[tp.Tuple[tp.Hashable, tp.Iterable[tp.Any]]],\n            *,\n            index: IndexInitializer = None,\n            fill_value: object = np.nan,\n            dtypes: DtypesSpecifier = None,\n            name: tp.Hashable = None,\n            index_constructor: IndexConstructor = None,\n            columns_constructor: IndexConstructor = None,\n            consolidate_blocks: bool = False\n            ):\n        \'\'\'Frame constructor from an iterator or generator of pairs, where the first value is the column label and the second value is an iterable of a column\'s values.\n\n        Args:\n            pairs: Iterable of pairs of column name, column values.\n            index: Iterable of values to create an Index.\n            fill_value: If pairs include Series, they will be reindexed with the provided index; reindexing will use this fill value.\n            {dtypes}\n            {name}\n            {consolidate_blocks}\n\n        Returns:\n            :obj:`static_frame.Frame`\n        \'\'\'\n        columns = []\n\n        # if an index initializer is passed, and we expect to get Series, we need to create the index in advance of iterating blocks\n        own_index = False\n        if _index_initializer_needs_init(index):\n            index = index_from_optional_constructor(index,\n                    default_constructor=Index,\n                    explicit_constructor=index_constructor\n                    )\n            own_index = True\n\n        dtypes_is_map = dtypes_mappable(dtypes)\n        def get_col_dtype(col_idx):\n            if dtypes_is_map:\n                return dtypes.get(columns[col_idx], None)\n            return dtypes[col_idx]\n\n        def blocks():\n            for col_idx, (k, v) in enumerate(pairs):\n                columns.append(k) # side effet of generator!\n\n                if dtypes:\n                    column_type = get_col_dtype(col_idx)\n                else:\n                    column_type = None\n\n                if isinstance(v, np.ndarray):\n                    # NOTE: we rely on TypeBlocks constructor to check that these are same sized\n                    if column_type is not None:\n                        yield v.astype(column_type)\n                    else:\n                        yield v\n                elif isinstance(v, Series):\n                    if index is None:\n                        raise ErrorInitFrame(\'can only consume Series in Frame.from_items if an Index is provided.\')\n\n                    if not v.index.equals(index):\n                        v = v.reindex(index,\n                                fill_value=fill_value,\n                                check_equals=False,\n                                )\n                    # return values array post reindexing\n                    if column_type is not None:\n                        yield v.values.astype(column_type)\n                    else:\n                        yield v.values\n\n                elif isinstance(v, Frame):\n                    raise ErrorInitFrame(\'Frames are not supported in from_items constructor.\')\n                else:\n                    # returned array is immutable\n                    values, _ = iterable_to_array_1d(v, column_type)\n                    yield values\n\n        if consolidate_blocks:\n            block_gen = lambda: TypeBlocks.consolidate_blocks(blocks())\n        else:\n            block_gen = blocks\n\n        return cls(TypeBlocks.from_blocks(block_gen()),\n                index=index,\n                columns=columns,\n                name=name,\n                own_data=True,\n                own_index=own_index,\n                columns_constructor=columns_constructor\n                )\n\n    @classmethod\n    @doc_inject(selector=\'constructor_frame\')\n    def from_dict(cls,\n            mapping: tp.Dict[tp.Hashable, tp.Iterable[tp.Any]],\n            *,\n            index: IndexInitializer = None,\n            fill_value: object = np.nan,\n            dtypes: DtypesSpecifier = None,\n            name: tp.Hashable = None,\n            index_constructor: IndexConstructor = None,\n            columns_constructor: IndexConstructor = None,\n            consolidate_blocks: bool = False\n            ) -> \'Frame\':\n        \'\'\'\n        Create a Frame from a dictionary, or any object that has an items() method.\n\n        Args:\n            mapping: a dictionary or similar mapping interface.\n            {dtypes}\n            {name}\n            {consolidate_blocks}\n        \'\'\'\n        return cls.from_items(mapping.items(),\n                index=index,\n                fill_value=fill_value,\n                name=name,\n                dtypes=dtypes,\n                index_constructor=index_constructor,\n                columns_constructor=columns_constructor,\n                consolidate_blocks=consolidate_blocks,\n                )\n\n    @staticmethod\n    def _structured_array_to_d_ia_cl(\n            array: np.ndarray,\n            *,\n            index_depth: int = 0,\n            index_column_first: tp.Optional[IndexSpecifier] = None,\n            dtypes: DtypesSpecifier = None,\n            consolidate_blocks: bool = False,\n            store_filter: tp.Optional[StoreFilter] = STORE_FILTER_DEFAULT,\n            columns: tp.Optional[IndexBase] = None\n            ) -> tp.Tuple[TypeBlocks, tp.Sequence[np.ndarray], tp.Sequence[tp.Hashable]]:\n        \'\'\'\n        Expanded function name: _structure_array_to_data_index_arrays_columns_labels\n\n        Utility function for creating TypeBlocks from structure array (or a 2D array that np.genfromtxt might have returned) while extracting index and columns labels. Does not form Index objects for columns or index, allowing down-stream processes to do so.\n\n        Args:\n            index_column_first: optionally name the column that will start the block of index columns.\n            columns: optionally provide a columns Index to resolve dtypes specified by name.\n        \'\'\'\n        names = array.dtype.names # using names instead of fields, as this is NP convention\n        is_structured_array = True\n        if names is None:\n            is_structured_array = False\n            # raise ErrorInitFrame(\'array is not a structured array\')\n            # could use np.rec.fromarrays, but that makes a copy; better to use the passed in array\n            # must be a 2D array\n            names = tuple(range(array.shape[1]))\n\n        index_start_pos = -1 # will be ignored\n        index_end_pos = -1\n        if index_column_first is not None:\n            if index_depth <= 0:\n                raise ErrorInitFrame(\'index_column_first specified but index_depth is 0\')\n            elif isinstance(index_column_first, INT_TYPES):\n                index_start_pos = index_column_first\n            else:\n                index_start_pos = names.index(index_column_first) # linear performance\n            index_end_pos = index_start_pos + index_depth - 1\n        else: # no index_column_first specified, if index depth > 0, set start to 0\n            if index_depth > 0:\n                index_start_pos = 0\n                # Subtract one for inclusive boun\n                index_end_pos = index_start_pos + index_depth - 1\n\n        # assign in generator\n        index_arrays = []\n        # collect whatever labels are found on structured arrays; these may not be the same as the passed in columns, if columns are provided\n        columns_labels = []\n\n        index_field_placeholder = object()\n        columns_by_col_idx = []\n\n        if columns is None:\n            use_dtype_names = True\n        else:\n            use_dtype_names = False\n            columns_idx = 0 # relative position in index object\n            # construct columns_by_col_idx from columns, adding Nones for index columns\n            for i in range(len(names)):\n                if i >= index_start_pos and i <= index_end_pos:\n                    columns_by_col_idx.append(index_field_placeholder)\n                    continue\n                columns_by_col_idx.append(columns[columns_idx])\n                columns_idx += 1\n\n        dtypes_is_map = dtypes_mappable(dtypes)\n\n        def get_col_dtype(col_idx: int):\n            if dtypes_is_map:\n                # columns_by_col_idx may have a index_field_placeholder: will return None\n                return dtypes.get(columns_by_col_idx[col_idx], None)\n            # assume dytpes is an ordered sequences\n            return dtypes[col_idx]\n\n        def blocks():\n            # iterate over column names and yield one at a time for block construction; collect index arrays and column labels as we go\n            for col_idx, name in enumerate(names):\n                if use_dtype_names:\n                    # append here as we iterate for usage in get_col_dtype\n                    columns_by_col_idx.append(name)\n\n                # this is not expected to make a copy\n                if is_structured_array:\n                    array_final = array[name]\n                else: # a 2D array, name is integer for column\n                    array_final = array[NULL_SLICE, name]\n\n                # do StoreFilter conversions before dtype\n                if array_final.ndim == 0:\n                    # some structured arrays give 0 ndim arrays by name\n                    array_final = np.reshape(array_final, (1,))\n\n                if store_filter is not None:\n                    array_final = store_filter.to_type_filter_array(array_final)\n\n                if dtypes:\n                    dtype = get_col_dtype(col_idx)\n                    if dtype is not None:\n                        array_final = array_final.astype(dtype)\n\n                array_final.flags.writeable = False\n\n                if col_idx >= index_start_pos and col_idx <= index_end_pos:\n                    index_arrays.append(array_final)\n                    continue\n\n                columns_labels.append(name)\n                yield array_final\n\n        if consolidate_blocks:\n            data = TypeBlocks.from_blocks(TypeBlocks.consolidate_blocks(blocks()))\n        else:\n            data = TypeBlocks.from_blocks(blocks())\n\n        return data, index_arrays, columns_labels\n\n    @classmethod\n    def _from_data_index_arrays_column_labels(cls,\n            data: TypeBlocks,\n            index_depth: int,\n            index_arrays: tp.Sequence[np.ndarray],\n            columns_depth: int,\n            columns_labels: tp.Sequence[tp.Hashable],\n            name: tp.Hashable,\n            ) -> \'Frame\':\n        \'\'\'\n        Private constructor used for specialized construction from NP Structured array, as well as StoreHDF5.\n        \'\'\'\n        columns_constructor = None\n        if columns_depth == 0:\n            columns = None\n        elif columns_depth == 1:\n            columns = columns_labels\n        elif columns_depth > 1:\n            # assume deliminted IH extracted from SA labels\n            columns = columns_labels\n            columns_constructor = partial(\n                    cls._COLUMNS_HIERARCHY_CONSTRUCTOR.from_labels_delimited,\n                    delimiter=\' \')\n\n        kwargs = dict(\n                data=data,\n                own_data=True,\n                columns=columns,\n                columns_constructor=columns_constructor,\n                name=name\n                )\n\n        if index_depth == 0:\n            return cls(\n                index=None,\n                **kwargs)\n        if index_depth == 1:\n            return cls(\n                index=index_arrays[0],\n                **kwargs)\n        return cls(\n                index=zip(*index_arrays),\n                index_constructor=IndexHierarchy.from_labels,\n                **kwargs\n                )\n\n    @classmethod\n    @doc_inject(selector=\'constructor_frame\')\n    def from_structured_array(cls,\n            array: np.ndarray,\n            *,\n            index_depth: int = 0,\n            index_column_first: tp.Optional[IndexSpecifier] = None,\n            columns_depth: int = 1,\n            dtypes: DtypesSpecifier = None,\n            name: tp.Hashable = None,\n            consolidate_blocks: bool = False,\n            store_filter: tp.Optional[StoreFilter] = STORE_FILTER_DEFAULT\n            ) -> \'Frame\':\n        \'\'\'\n        Convert a NumPy structed array into a Frame.\n\n        Args:\n            array: Structured NumPy array.\n            index_depth: Depth if index levels, where (for example) 0 is no index, 1 is a single column index, and 2 is a two-columns IndexHierarchy.\n            index_column_first: Optionally provide the name or position offset of the column to use as the index.\n            {dtypes}\n            {name}\n            {consolidate_blocks}\n\n        Returns:\n            :obj:`static_frame.Frame`\n        \'\'\'\n        # from a structured array, we assume we want to get the columns labels\n        data, index_arrays, columns_labels = cls._structured_array_to_d_ia_cl(\n                array=array,\n                index_depth=index_depth,\n                index_column_first=index_column_first,\n                dtypes=dtypes,\n                consolidate_blocks=consolidate_blocks,\n                store_filter=store_filter,\n                )\n        return cls._from_data_index_arrays_column_labels(\n                data=data,\n                index_depth=index_depth,\n                index_arrays=index_arrays,\n                columns_depth=columns_depth,\n                columns_labels=columns_labels,\n                name=name\n                )\n\n    #---------------------------------------------------------------------------\n    # iloc/loc pairs constructors: these are not public, not sure if they should be\n\n    @classmethod\n    def from_element_iloc_items(cls,\n            items,\n            *,\n            index,\n            columns,\n            dtype,\n            name: tp.Hashable = None\n            ) -> \'Frame\':\n        \'\'\'\n        Given an iterable of pairs of iloc coordinates and values, populate a Frame as defined by the given index and columns. The dtype must be specified, and must be the same for all values.\n\n        Returns:\n            :obj:`static_frame.Frame`\n        \'\'\'\n        index = Index(index)\n        columns = cls._COLUMNS_CONSTRUCTOR(columns)\n\n        tb = TypeBlocks.from_element_items(items,\n                shape=(len(index), len(columns)),\n                dtype=dtype)\n        return cls(tb,\n                index=index,\n                columns=columns,\n                name=name,\n                own_data=True,\n                own_index=True,\n                own_columns=True)\n\n    @classmethod\n    def from_element_loc_items(cls,\n            items: tp.Iterable[tp.Tuple[\n                    tp.Tuple[tp.Hashable, tp.Hashable], tp.Any]],\n            *,\n            index: IndexInitializer,\n            columns: IndexInitializer,\n            dtype=None,\n            name: tp.Hashable = None,\n            fill_value: object = FILL_VALUE_DEFAULT,\n            index_constructor: IndexConstructor = None,\n            columns_constructor: IndexConstructor = None,\n            own_index: bool = False,\n            own_columns: bool = False,\n            ) -> \'Frame\':\n        \'\'\'\n        This function is partialed (setting the index and columns) and used by ``IterNodeDelegate`` as the apply constructor for doing application on element iteration.\n\n        Args:\n            items: an iterable of pairs of 2-tuples of row, column loc labels and values.\n\n\n        Returns:\n            :obj:`static_frame.Frame`\n        \'\'\'\n        if not own_index:\n            index = index_from_optional_constructor(index,\n                    default_constructor=Index,\n                    explicit_constructor=index_constructor\n                    )\n\n        if not own_columns:\n            columns = index_from_optional_constructor(columns,\n                    default_constructor=cls._COLUMNS_CONSTRUCTOR,\n                    explicit_constructor=columns_constructor\n                    )\n\n        items = (((index.loc_to_iloc(k[0]), columns.loc_to_iloc(k[1])), v)\n                for k, v in items)\n\n        dtype = dtype if dtype is not None else object\n\n        tb = TypeBlocks.from_element_items(\n                items,\n                shape=(len(index), len(columns)),\n                dtype=dtype,\n                fill_value=fill_value)\n\n        return cls(tb,\n                index=index,\n                columns=columns,\n                name=name,\n                own_data=True,\n                own_index=True, # always true as either provided or created new\n                own_columns=True\n                )\n\n    #---------------------------------------------------------------------------\n    # file, data format loaders\n\n    @classmethod\n    @doc_inject(selector=\'constructor_frame\')\n    def from_sql(cls,\n            query: str,\n            *,\n            connection: sqlite3.Connection,\n            index_depth: int = 0,\n            columns_depth: int = 1,\n            dtypes: DtypesSpecifier = None,\n            name: tp.Hashable = None,\n            consolidate_blocks: bool = False,\n            ) -> \'Frame\':\n        \'\'\'\n        Frame constructor from an SQL query and a database connection object.\n\n        Args:\n            query: A query string.\n            connection: A DBAPI2 (PEP 249) Connection object, such as those returned from SQLite (via the sqlite3 module) or PyODBC.\n            {dtypes}\n            {name}\n            {consolidate_blocks}\n        \'\'\'\n        row_gen = connection.execute(query)\n\n        own_columns = False\n        columns = None\n        if columns_depth == 1:\n            columns = cls._COLUMNS_CONSTRUCTOR(b[0] for b in row_gen.description[index_depth:])\n            own_columns = True\n        elif columns_depth > 1:\n            # use IH: get via static attr of columns const\n            constructor = cls._COLUMNS_HIERARCHY_CONSTRUCTOR.from_labels_delimited\n            labels = (b[0] for b in row_gen.description[index_depth:])\n            columns = constructor(labels, delimiter=\' \')\n            own_columns = True\n\n        index_constructor = None\n\n        if index_depth > 0:\n            index = [] # lazily populate\n            if index_depth == 1:\n                index_constructor = Index\n\n                def row_gen_final() -> tp.Iterator[tp.Sequence[tp.Any]]:\n                    for row in row_gen:\n                        index.append(row[0])\n                        yield row[1:]\n\n            else: # > 1\n                index_constructor = IndexHierarchy.from_labels\n\n                def row_gen_final() -> tp.Iterator[tp.Sequence[tp.Any]]:\n                    for row in row_gen:\n                        index.append(row[:index_depth])\n                        yield row[index_depth:]\n        else:\n            index = None\n            row_gen_final = lambda: row_gen\n\n        # let default type induction do its work\n        return cls.from_records(\n                row_gen_final(),\n                columns=columns,\n                index=index,\n                dtypes=dtypes,\n                name=name,\n                own_columns=own_columns,\n                index_constructor=index_constructor,\n                consolidate_blocks=consolidate_blocks,\n                )\n\n    @classmethod\n    @doc_inject(selector=\'constructor_frame\')\n    def from_json(cls,\n            json_data: str,\n            *,\n            dtypes: DtypesSpecifier = None,\n            name: tp.Hashable = None,\n            consolidate_blocks: bool = False\n            ) -> \'Frame\':\n        \'\'\'Frame constructor from an in-memory JSON document.\n\n        Args:\n            json_data: a string of JSON, encoding a table as an array of JSON objects.\n            {dtypes}\n            {name}\n            {consolidate_blocks}\n\n        Returns:\n            :obj:`static_frame.Frame`\n        \'\'\'\n        data = json.loads(json_data)\n        return cls.from_dict_records(data,\n                name=name,\n                dtypes=dtypes,\n                consolidate_blocks=consolidate_blocks\n                )\n\n    @classmethod\n    @doc_inject(selector=\'constructor_frame\')\n    def from_json_url(cls,\n            url: str,\n            *,\n            dtypes: DtypesSpecifier = None,\n            name: tp.Hashable = None,\n            consolidate_blocks: bool = False\n            ) -> \'Frame\':\n        \'\'\'Frame constructor from a JSON documenst provided via a URL.\n\n        Args:\n            url: URL to the JSON resource.\n            {dtypes}\n            {name}\n            {consolidate_blocks}\n\n        Returns:\n            :obj:`static_frame.Frame`\n        \'\'\'\n        return cls.from_json(_read_url(url), #pragma: no cover\n                name=name,\n                dtypes=dtypes,\n                consolidate_blocks=consolidate_blocks\n                )\n\n    @classmethod\n    @doc_inject(selector=\'constructor_frame\')\n    def from_delimited(cls,\n            fp: PathSpecifierOrFileLikeOrIterator,\n            *,\n            delimiter: str,\n            index_depth: int = 0,\n            index_column_first: tp.Optional[tp.Union[int, str]] = None,\n            columns_depth: int = 1,\n            skip_header: int = 0,\n            skip_footer: int = 0,\n            quote_char: str = \'""\',\n            encoding: tp.Optional[str] = None,\n            dtypes: DtypesSpecifier = None,\n            name: tp.Hashable = None,\n            consolidate_blocks: bool = False,\n            store_filter: tp.Optional[StoreFilter] = STORE_FILTER_DEFAULT\n            ) -> \'Frame\':\n        \'\'\'\n        Create a Frame from a file path or a file-like object defining a delimited (CSV, TSV) data file.\n\n        Args:\n            fp: A file path or a file-like object.\n            delimiter: The character used to seperate row elements.\n            index_depth: Specify the number of columns used to create the index labels; a value greater than 1 will attempt to create a hierarchical index.\n            index_column_first: Optionally specify a column, by position or name, to become the start of the index if index_depth is greater than 0. If not set and index_depth is greater than 0, the first column will be used.\n            columns_depth: Specify the number of rows after the skip_header used to create the column labels. A value of 0 will be no header; a value greater than 1 will attempt to create a hierarchical index.\n            skip_header: Number of leading lines to skip.\n            skip_footer: Number of trailing lines to skip.\n            store_filter: A StoreFilter instance, defining translation between unrepresentable types. Presently only the ``to_nan`` attributes is used.\n            {dtypes}\n            {name}\n            {consolidate_blocks}\n\n        Returns:\n            :obj:`static_frame.Frame`\n        \'\'\'\n        # https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\n        # https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\n\n        # TODO: add columns_select as usecols styles selective loading\n\n        if skip_header < 0:\n            raise ErrorInitFrame(\'skip_header must be greater than or equal to 0\')\n\n        fp = path_filter(fp)\n        delimiter_native = \'\\t\'\n\n        if delimiter != delimiter_native:\n            # this is necessary if there are quoted cells that include the delimiter\n            def file_like() -> tp.Iterator[str]:\n                if isinstance(fp, str):\n                    with open(fp, \'r\') as f:\n                        for row in csv.reader(f, delimiter=delimiter, quotechar=quote_char):\n                            yield delimiter_native.join(row)\n                else: # handling file like object works for stringio but not for bytesio\n                    for row in csv.reader(fp, delimiter=delimiter, quotechar=quote_char):\n                        yield delimiter_native.join(row)\n        else:\n            def file_like() -> tp.Iterator[str]: # = fp\n                if isinstance(fp, str):\n                    with open(fp, \'r\') as f:\n                        for row in f:\n                            yield row\n                else: # iterable of string lines, StringIO\n                    for row in fp:\n                        yield row\n\n        # always accumulate columns rows, as np.genfromtxt will mutate the headers: adding enderscore, removing invalid characters, etc.\n        columns_rows = []\n\n        def row_source() -> tp.Iterator[str]:\n            # set equal to skip header unless column depth is > 1\n            column_max = skip_header + columns_depth\n            for i, row in enumerate(file_like()):\n                if i < skip_header:\n                    continue\n                if i < column_max:\n                    columns_rows.append(row)\n                    continue\n                yield row\n\n        # genfromtxt takes missing_values, but this can only be a list, and does not work under some condition (i.e., a cell with no value). thus, this is deferred to from_sructured_array\n\n        array = np.genfromtxt(\n                row_source(),\n                delimiter=delimiter_native,\n                skip_header=0, # done in row_source\n                skip_footer=skip_footer,\n                comments=None,\n                # strange NP convention for this parameter: False is not supported, must use None to not parase headers\n                names= None,\n                dtype=None,\n                encoding=encoding,\n                invalid_raise=False,\n                )\n        array.flags.writeable = False\n\n        # construct columns prior to preparing data from structured array, as need columns to map dtypes\n        # columns_constructor = None\n        if columns_depth == 0:\n            columns = None\n            own_columns = False\n        else:\n            # Process each row one at a time, as types align by row.\n            columns_arrays = []\n            for row in columns_rows:\n                columns_array = np.genfromtxt(\n                        (row,),\n                        delimiter=delimiter_native,\n                        comments=None,\n                        names=None,\n                        dtype=None,\n                        encoding=encoding,\n                        invalid_raise=False,\n                        )\n                # the array might be ndim=1, or ndim=0; must get a list before slicing\n                columns_arrays.append(columns_array.tolist()[index_depth:])\n\n            if columns_depth == 1:\n                columns_constructor = cls._COLUMNS_CONSTRUCTOR\n                columns = columns_constructor(columns_arrays[0])\n                own_columns = True\n            else:\n                columns_constructor = cls._COLUMNS_HIERARCHY_CONSTRUCTOR.from_labels\n                columns = columns_constructor(\n                        zip(*(store_filter.to_type_filter_iterable(x) for x in columns_arrays))\n                        )\n                own_columns = True\n\n        if array.dtype.names is None: # not a structured array\n            # genfromtxt may, in some situations, not return a structured array\n            if array.ndim == 1:\n                # got a single row\n                array = array.reshape((1, len(array)))\n            # NOTE: genfromtxt will return a one column input file as a 2D array with the vertical data as a horizontal row. There does not appear to be a way to distinguish this from a single row file\n\n        if array.size > 0: # an empty, or column only table\n            data, index_arrays, _ = cls._structured_array_to_d_ia_cl(\n                    array=array,\n                    index_depth=index_depth,\n                    index_column_first=index_column_first,\n                    dtypes=dtypes,\n                    consolidate_blocks=consolidate_blocks,\n                    store_filter=store_filter,\n                    columns = columns\n                    )\n        else: # only column data in table\n            if index_depth > 0:\n                raise ErrorInitFrame(f\'no data from which to extract index_depth {index_depth}\')\n            data = FRAME_INITIALIZER_DEFAULT\n\n        kwargs = dict(\n                data=data,\n                own_data=True,\n                columns=columns,\n                own_columns=own_columns,\n                name=name\n                )\n\n        if index_depth == 0:\n            return cls(\n                index=None,\n                **kwargs)\n        if index_depth == 1:\n            return cls(\n                index=index_arrays[0],\n                **kwargs)\n        return cls(\n                index=zip(*index_arrays),\n                index_constructor=IndexHierarchy.from_labels,\n                **kwargs\n                )\n\n    @classmethod\n    def from_csv(cls,\n            fp: PathSpecifierOrFileLikeOrIterator,\n            *,\n            index_depth: int = 0,\n            index_column_first: tp.Optional[tp.Union[int, str]] = None,\n            columns_depth: int = 1,\n            skip_header: int = 0,\n            skip_footer: int = 0,\n            quote_char: str = \'""\',\n            encoding: tp.Optional[str] = None,\n            dtypes: DtypesSpecifier = None,\n            name: tp.Hashable = None,\n            consolidate_blocks: bool = False,\n            store_filter: tp.Optional[StoreFilter] = STORE_FILTER_DEFAULT\n            ) -> \'Frame\':\n        \'\'\'\n        Specialized version of :py:meth:`Frame.from_delimited` for CSV files.\n\n        Returns:\n            :obj:`static_frame.Frame`\n        \'\'\'\n        return cls.from_delimited(fp,\n                delimiter=\',\',\n                index_depth=index_depth,\n                index_column_first=index_column_first,\n                columns_depth=columns_depth,\n                skip_header=skip_header,\n                skip_footer=skip_footer,\n                quote_char=quote_char,\n                encoding=encoding,\n                dtypes=dtypes,\n                name=name,\n                consolidate_blocks=consolidate_blocks,\n                store_filter=store_filter,\n                )\n\n    @classmethod\n    def from_tsv(cls,\n            fp: PathSpecifierOrFileLikeOrIterator,\n            *,\n            index_depth: int = 0,\n            index_column_first: tp.Optional[tp.Union[int, str]] = None,\n            columns_depth: int = 1,\n            skip_header: int = 0,\n            skip_footer: int = 0,\n            quote_char: str = \'""\',\n            encoding: tp.Optional[str] = None,\n            dtypes: DtypesSpecifier = None,\n            name: tp.Hashable = None,\n            consolidate_blocks: bool = False,\n            store_filter: tp.Optional[StoreFilter] = STORE_FILTER_DEFAULT\n            ) -> \'Frame\':\n        \'\'\'\n        Specialized version of :py:meth:`Frame.from_delimited` for TSV files.\n\n        Returns:\n            :obj:`static_frame.Frame`\n        \'\'\'\n        return cls.from_delimited(fp,\n                delimiter=\'\\t\',\n                index_depth=index_depth,\n                index_column_first=index_column_first,\n                columns_depth=columns_depth,\n                skip_header=skip_header,\n                skip_footer=skip_footer,\n                quote_char=quote_char,\n                encoding=encoding,\n                dtypes=dtypes,\n                name=name,\n                consolidate_blocks=consolidate_blocks,\n                store_filter=store_filter,\n                )\n\n    @classmethod\n    def from_xlsx(cls,\n            fp: PathSpecifier,\n            *,\n            label: tp.Optional[str] = None,\n            index_depth: int = 0,\n            columns_depth: int = 1,\n            dtypes: DtypesSpecifier = None,\n            consolidate_blocks: bool = False,\n            ) -> \'Frame\':\n        \'\'\'\n        Load Frame from the contents of a sheet in an XLSX workbook.\n\n        Args:\n            label: Optionally provide the sheet name from with to read. If not provided, the first sheet will be used.\n        \'\'\'\n        from static_frame.core.store import StoreConfig\n        from static_frame.core.store_xlsx import StoreXLSX\n\n        st = StoreXLSX(fp)\n        config = StoreConfig(\n                index_depth=index_depth,\n                columns_depth=columns_depth,\n                dtypes=dtypes,\n                consolidate_blocks=consolidate_blocks,\n                )\n        return st.read(label, config=config, container_type=cls)\n\n    @classmethod\n    def from_sqlite(cls,\n            fp: PathSpecifier,\n            *,\n            label: tp.Optional[str] = None,\n            index_depth: int = 0,\n            columns_depth: int = 1,\n            dtypes: DtypesSpecifier = None,\n            consolidate_blocks: bool = False,\n            ) -> \'Frame\':\n        \'\'\'\n        Load Frame from the contents of a table in an SQLite database file.\n        \'\'\'\n        from static_frame.core.store import StoreConfig\n        from static_frame.core.store_sqlite import StoreSQLite\n\n        st = StoreSQLite(fp)\n        config = StoreConfig(\n                index_depth=index_depth,\n                columns_depth=columns_depth,\n                dtypes=dtypes,\n                consolidate_blocks=consolidate_blocks,\n                )\n        return st.read(label, config=config, container_type=cls)\n\n    @classmethod\n    def from_hdf5(cls,\n            fp: PathSpecifier,\n            *,\n            label: str,\n            index_depth: int = 0,\n            columns_depth: int = 1,\n            consolidate_blocks: bool = False,\n            ) -> \'Frame\':\n        \'\'\'\n        Load Frame from the contents of a table in an HDF5 file.\n        \'\'\'\n        from static_frame.core.store import StoreConfig\n        from static_frame.core.store_hdf5 import StoreHDF5\n\n        st = StoreHDF5(fp)\n        config = StoreConfig(\n                index_depth=index_depth,\n                columns_depth=columns_depth,\n                consolidate_blocks=consolidate_blocks,\n                )\n        return st.read(label, config=config, container_type=cls)\n\n    @classmethod\n    @doc_inject()\n    def from_pandas(cls,\n            value: \'pandas.DataFrame\',\n            *,\n            index_constructor: IndexConstructor = None,\n            columns_constructor: IndexConstructor = None,\n            consolidate_blocks: bool = False,\n            own_data: bool = False\n            ) -> \'Frame\':\n        \'\'\'Given a Pandas DataFrame, return a Frame.\n\n        Args:\n            value: Pandas DataFrame.\n            {own_data}\n\n        Returns:\n            :obj:`static_frame.Frame`\n        \'\'\'\n        pdvu1 = pandas_version_under_1()\n\n        def part_to_array(part):\n            if pdvu1:\n                array = part.values\n                if own_data:\n                    array.flags.writeable = False\n            else:\n                array = pandas_to_numpy(part, own_data=own_data)\n            return array\n\n        # create generator of contiguous typed data\n        # calling .values will force type unification accross all columns\n        def blocks() -> tp.Iterator[np.ndarray]:\n            pairs = value.dtypes.items()\n            column_start, dtype_current = next(pairs)\n            column_last = column_start\n            yield_block = False\n\n            for column, dtype in pairs:\n                try:\n                    if dtype != dtype_current:\n                        yield_block = True\n                except TypeError:\n                    # data type not understood, happens with pd datatypes to np dtypes in pd >= 1\n                    yield_block = True\n\n                if yield_block:\n                    # use loc to select before calling .values\n                    part = value.loc[NULL_SLICE, slice(column_start, column_last)]\n                    yield part_to_array(part)\n\n                    column_start = column\n                    dtype_current = dtype\n                    yield_block = False\n\n                column_last = column\n\n            # always have left over\n            part = value.loc[NULL_SLICE, slice(column_start, None)]\n            yield part_to_array(part)\n\n        if consolidate_blocks:\n            blocks = TypeBlocks.from_blocks(TypeBlocks.consolidate_blocks(blocks()))\n        else:\n            blocks = TypeBlocks.from_blocks(blocks())\n\n        # avoid getting a Series if a column\n        if \'name\' not in value.columns and hasattr(value, \'name\'):\n            name = value.name\n        else:\n            name = None\n\n        own_index = True\n        if index_constructor is IndexAutoFactory:\n            index = None\n            own_index = False\n        elif index_constructor is not None:\n            index = index_constructor(value.index)\n        else:\n            index = Index.from_pandas(value.index)\n\n        own_columns = True\n        if columns_constructor is IndexAutoFactory:\n            columns = None\n            own_columns = False\n        elif columns_constructor is not None:\n            columns = columns_constructor(value.columns)\n        else:\n            columns = cls._COLUMNS_CONSTRUCTOR.from_pandas(value.columns)\n\n        return cls(blocks,\n                index=index,\n                columns=columns,\n                name=name,\n                own_data=True,\n                own_index=own_index,\n                own_columns=own_columns\n                )\n\n    @classmethod\n    def from_arrow(cls,\n            value: \'pyarrow.Table\',\n            *,\n            index_depth: int = 0,\n            columns_depth: int = 1,\n            consolidate_blocks: bool = False,\n            name: tp.Hashable = None\n            ) -> \'Frame\':\n        \'\'\'Convert an Arrow Table into a Frame.\n        \'\'\'\n        # this is similar to from_structured_array\n        index_start_pos = -1 # will be ignored\n        index_end_pos = -1\n        if index_depth > 0:\n            index_start_pos = 0\n            index_end_pos = index_start_pos + index_depth - 1\n\n        index_arrays = []\n        if columns_depth > 0:\n            columns = []\n\n        pdvu1 = pandas_version_under_1()\n\n        def blocks():\n            for col_idx, (name, chunked_array) in enumerate(\n                    zip(value.column_names, value.columns)):\n                # This creates a Series with an index; better to find a way to go only to numpy, but does not seem available on ChunkedArray, even with pyarrow==0.16.0\n                series = chunked_array.to_pandas(\n                        date_as_object=False, # get an np array\n                        self_destruct=True, # documented as ""experimental""\n                        ignore_metadata=True,\n                        )\n                if pdvu1:\n                    array_final = series.values\n                    array_final.flags.writeable = False\n                else:\n                    array_final = pandas_to_numpy(series, own_data=True)\n\n                if col_idx >= index_start_pos and col_idx <= index_end_pos:\n                    index_arrays.append(array_final)\n                    continue\n\n                if columns_depth > 0:\n                    columns.append(name)\n\n                yield array_final\n\n        if consolidate_blocks:\n            block_gen = lambda: TypeBlocks.consolidate_blocks(blocks())\n        else:\n            block_gen = blocks\n\n        columns_constructor = None\n        if columns_depth == 0:\n            columns = None\n        elif columns_depth > 1:\n            columns_constructor = partial(\n                    cls._COLUMNS_HIERARCHY_CONSTRUCTOR.from_labels_delimited,\n                    delimiter=\' \')\n\n        kwargs = dict(\n                data=TypeBlocks.from_blocks(block_gen()),\n                own_data=True,\n                columns=columns,\n                columns_constructor=columns_constructor,\n                name=name\n                )\n\n        if index_depth == 0:\n            return cls(index=None, **kwargs)\n        if index_depth == 1:\n            return cls(index=index_arrays[0], **kwargs)\n        return cls(\n                index=zip(*index_arrays),\n                index_constructor=IndexHierarchy.from_labels,\n                **kwargs\n                )\n\n    @classmethod\n    def from_parquet(cls,\n            fp: PathSpecifier,\n            *,\n            index_depth: int = 0,\n            columns_depth: int = 1,\n            columns_select: tp.Optional[tp.Iterable[str]] = None,\n            consolidate_blocks: bool = False,\n            name: tp.Hashable = None,\n            ) -> \'Frame\':\n        \'\'\'\n        Realize a ``Frame`` from a Parquet file.\n        \'\'\'\n        import pyarrow.parquet as pq\n\n        if columns_select and index_depth != 0:\n            raise ErrorInitFrame(f\'cannot create load index_depth {index_depth} when columns_select is specified.\')\n\n        # NOTE: the order of columns_select will determine their order\n        table = pq.read_table(fp,\n                columns=columns_select,\n                use_pandas_metadata=False,\n                )\n        return cls.from_arrow(table,\n                index_depth=index_depth,\n                columns_depth=columns_depth,\n                consolidate_blocks=consolidate_blocks,\n                name=name\n                )\n\n    #---------------------------------------------------------------------------\n    def __init__(self,\n            data: FrameInitializer = FRAME_INITIALIZER_DEFAULT,\n            *,\n            index: tp.Union[IndexInitializer, IndexAutoFactoryType] = None,\n            columns: tp.Union[IndexInitializer, IndexAutoFactoryType] = None,\n            name: NameType = NAME_DEFAULT,\n            index_constructor: IndexConstructor = None,\n            columns_constructor: IndexConstructor = None,\n            own_data: bool = False,\n            own_index: bool = False,\n            own_columns: bool = False\n            ) -> None:\n        # doc string at class def\n\n        # we can determine if columns or index are empty only if they are not iterators; those cases will have to use a deferred evaluation\n        columns_empty = index_constructor_empty(columns)\n        index_empty = index_constructor_empty(index)\n\n        #-----------------------------------------------------------------------\n        # blocks assignment\n\n        blocks_constructor = None\n\n        if isinstance(data, TypeBlocks):\n            if own_data:\n                self._blocks = data\n            else:\n                # assume we need to create a new TB instance; this will not copy underlying arrays as all blocks are immutable\n                self._blocks = TypeBlocks.from_blocks(data._blocks)\n        elif isinstance(data, np.ndarray):\n            if own_data:\n                data.flags.writeable = False\n            # from_blocks will apply immutable filter\n            self._blocks = TypeBlocks.from_blocks(data)\n        elif data is FRAME_INITIALIZER_DEFAULT:\n            # NOTE: this will not catch all cases where index or columns is empty, as they might be iterators; those cases will be handled below.\n            def blocks_constructor(shape): #pylint: disable=E0102\n                if shape[0] > 0 and shape[1] > 0:\n                    # if fillable and we still have default initializer, this is a problem\n                    raise RuntimeError(\'must supply a non-default value for constructing a Frame with non-zero size.\')\n                self._blocks = TypeBlocks.from_zero_size_shape(shape)\n        elif isinstance(data, Frame):\n            self._blocks = data._blocks.copy()\n            if index is None and index_constructor is None:\n                # set up for direct assignment below; index is always immutable\n                index = data.index\n                own_index = True\n            if columns is None and columns_constructor is None:\n                # cannot own, but can let constructors handle potential mutability\n                columns = data.columns\n                columns_empty = index_constructor_empty(columns)\n            if name is NAME_DEFAULT:\n                name = data.name\n\n        elif isinstance(data, dict):\n            raise ErrorInitFrame(\'use Frame.from_dict to create a Frame from a mapping.\')\n        elif isinstance(data, Series):\n            raise ErrorInitFrame(\'use Frame.from_series to create a Frame from a Series.\')\n        else:\n            raise ErrorInitFrame(\'use Frame.from_element, Frame.from_elements, or Frame.from_records to create a Frame from 0, 1, or 2 dimensional untyped data (respectively).\')\n\n        # counts can be zero (not None) if _block was created but is empty\n        row_count, col_count = (self._blocks._shape\n                if not blocks_constructor else (None, None))\n\n        self._name = None if name is NAME_DEFAULT else name_filter(name)\n\n        #-----------------------------------------------------------------------\n        # columns assignment\n\n        if own_columns:\n            self._columns = columns\n            col_count = len(self._columns)\n        elif columns_empty:\n            col_count = 0 if col_count is None else col_count\n            self._columns = IndexAutoFactory.from_optional_constructor(\n                    col_count,\n                    default_constructor=self._COLUMNS_CONSTRUCTOR,\n                    explicit_constructor=columns_constructor\n                    )\n        else:\n            self._columns = index_from_optional_constructor(columns,\n                    default_constructor=self._COLUMNS_CONSTRUCTOR,\n                    explicit_constructor=columns_constructor\n                    )\n            col_count = len(self._columns)\n\n        # check after creation, as we cannot determine from the constructor (it might be a method on a class)\n        if self._COLUMNS_CONSTRUCTOR.STATIC != self._columns.STATIC:\n            raise ErrorInitFrame(f\'supplied column constructor does not match required static attribute: {self._COLUMNS_CONSTRUCTOR.STATIC}\')\n        #-----------------------------------------------------------------------\n        # index assignment\n\n        if own_index:\n            self._index = index\n            row_count = len(self._index)\n        elif index_empty:\n            row_count = 0 if row_count is None else row_count\n            self._index = IndexAutoFactory.from_optional_constructor(\n                    row_count,\n                    default_constructor=Index,\n                    explicit_constructor=index_constructor\n                    )\n        else:\n            self._index = index_from_optional_constructor(index,\n                    default_constructor=Index,\n                    explicit_constructor=index_constructor\n                    )\n            row_count = len(self._index)\n\n        if not self._index.STATIC:\n            raise ErrorInitFrame(\'non-static index cannot be assigned to Frame\')\n\n        #-----------------------------------------------------------------------\n        # final evaluation\n\n        # for indices that are created by generators, need to reevaluate if data has been given for an empty index or columns\n        columns_empty = col_count == 0\n        index_empty = row_count == 0\n\n        if blocks_constructor:\n            # if we have a blocks_constructor, we are determining final size from index and/or columns; we might have a legitamate single value for data, but it cannot be FRAME_INITIALIZER_DEFAULT\n            if data is not FRAME_INITIALIZER_DEFAULT and (\n                    columns_empty or index_empty):\n                raise ErrorInitFrame(\'cannot supply a data argument to Frame constructor when index or columns is empty\')\n            # must update the row/col counts, sets self._blocks\n            blocks_constructor((row_count, col_count))\n\n        # final check of block/index coherence\n\n        if self._blocks.ndim != self._NDIM:\n            raise ErrorInitFrame(\'dimensionality of final values not supported\')\n\n        if self._blocks.shape[0] != row_count:\n            # row count might be 0 for an empty DF\n            raise ErrorInitFrame(\n                f\'Index has incorrect size (got {self._blocks.shape[0]}, expected {row_count})\'\n                )\n        if self._blocks.shape[1] != col_count:\n            raise ErrorInitFrame(\n                f\'Columns has incorrect size (got {self._blocks.shape[1]}, expected {col_count})\'\n                )\n\n    #---------------------------------------------------------------------------\n    # name interface\n\n    @property\n    @doc_inject()\n    def name(self) -> tp.Hashable:\n        \'\'\'{}\'\'\'\n        return self._name\n\n    def rename(self, name: tp.Hashable) -> \'Frame\':\n        \'\'\'\n        Return a new Frame with an updated name attribute.\n        \'\'\'\n        # copying blocks does not copy underlying data\n        return self.__class__(self._blocks.copy(),\n                index=self._index,\n                columns=self._columns, # let constructor handle if GO\n                name=name,\n                own_data=True,\n                own_index=True)\n\n    #---------------------------------------------------------------------------\n    # interfaces\n\n    @property\n    def loc(self) -> InterfaceGetItem:\n        return InterfaceGetItem(self._extract_loc)\n\n    @property\n    def iloc(self) -> InterfaceGetItem:\n        return InterfaceGetItem(self._extract_iloc)\n\n    @property\n    def bloc(self) -> InterfaceGetItem:\n        return InterfaceGetItem(self._extract_bloc)\n\n    @property\n    def drop(self) -> InterfaceSelectTrio:\n        return InterfaceSelectTrio(\n            func_iloc=self._drop_iloc,\n            func_loc=self._drop_loc,\n            func_getitem=self._drop_getitem)\n\n    @property\n    def mask(self) -> InterfaceSelectTrio:\n        return InterfaceSelectTrio(\n            func_iloc=self._extract_iloc_mask,\n            func_loc=self._extract_loc_mask,\n            func_getitem=self._extract_getitem_mask)\n\n    @property\n    def masked_array(self) -> InterfaceSelectTrio:\n        return InterfaceSelectTrio(\n            func_iloc=self._extract_iloc_masked_array,\n            func_loc=self._extract_loc_masked_array,\n            func_getitem=self._extract_getitem_masked_array)\n\n    @property\n    def assign(self) -> InterfaceAssignQuartet:\n        # all functions that return a FrameAssign\n        return InterfaceAssignQuartet(\n            func_iloc=self._extract_iloc_assign,\n            func_loc=self._extract_loc_assign,\n            func_getitem=self._extract_getitem_assign,\n            func_bloc=self._extract_bloc_assign,\n            delegate=FrameAssign,\n            )\n\n    @property\n    @doc_inject(select=\'astype\')\n    def astype(self) -> InterfaceAsType:\n        \'\'\'\n        Retype one or more columns. Can be used as as function to retype the entire ``Frame``; alternatively, a ``__getitem__`` interface permits retyping selected columns.\n\n        Args:\n            {dtype}\n        \'\'\'\n        return InterfaceAsType(func_getitem=self._extract_getitem_astype)\n\n    #---------------------------------------------------------------------------\n    @property\n    def via_str(self) -> InterfaceString[\'Frame\']:\n        \'\'\'\n        Interface for applying string methods to elements in this container.\n        \'\'\'\n        def blocks_to_container(blocks: tp.Iterator[np.ndarray]) -> \'Frame\':\n            tb = TypeBlocks.from_blocks(blocks)\n            return self.__class__(\n                    tb,\n                    index=self._index,\n                    columns=self._columns,\n                    name=self._name,\n                    own_index=True,\n                    own_data=True,\n                    )\n\n        return InterfaceString(\n                blocks=self._blocks._blocks,\n                blocks_to_container=blocks_to_container,\n                )\n\n    @property\n    def via_dt(self) -> InterfaceDatetime[\'Frame\']:\n        \'\'\'\n        Interface for applying datetime properties and methods to elements in this container.\n        \'\'\'\n\n        # NOTE: we only process object dt64 types; strings have to be converted explicitly\n\n        def blocks_to_container(blocks: tp.Iterator[np.ndarray]) -> \'Frame\':\n            tb = TypeBlocks.from_blocks(blocks)\n            return self.__class__(\n                    tb,\n                    index=self._index,\n                    columns=self._columns,\n                    name=self._name,\n                    own_index=True,\n                    own_data=True,\n                    )\n\n        return InterfaceDatetime(\n                blocks=self._blocks._blocks,\n                blocks_to_container=blocks_to_container,\n                )\n\n    #---------------------------------------------------------------------------\n    # generators\n    @property\n    def iter_array(self) -> IterNodeAxis:\n        \'\'\'\n        Iterator of 1D NumPy array, where arrays are drawn from columns (axis=0) or rows (axis=1)\n        \'\'\'\n        return IterNodeAxis(\n            container=self,\n            function_values=self._axis_array,\n            function_items=self._axis_array_items,\n            yield_type=IterNodeType.VALUES\n            )\n\n    @property\n    def iter_array_items(self) -> IterNodeAxis:\n        \'\'\'\n        Iterator of pairs of label, 1D NumPy array, where arrays are drawn from columns (axis=0) or rows (axis=1)\n        \'\'\'\n        return IterNodeAxis(\n            container=self,\n            function_values=self._axis_array,\n            function_items=self._axis_array_items,\n            yield_type=IterNodeType.ITEMS\n            )\n\n    @property\n    def iter_tuple(self) -> IterNodeAxis:\n        return IterNodeAxis(\n            container=self,\n            function_values=self._axis_tuple,\n            function_items=self._axis_tuple_items,\n            yield_type=IterNodeType.VALUES\n            )\n\n    @property\n    def iter_tuple_items(self) -> IterNodeAxis:\n        return IterNodeAxis(\n            container=self,\n            function_values=self._axis_tuple,\n            function_items=self._axis_tuple_items,\n            yield_type=IterNodeType.ITEMS\n            )\n\n    @property\n    def iter_series(self) -> IterNodeAxis:\n        return IterNodeAxis(\n            container=self,\n            function_values=self._axis_series,\n            function_items=self._axis_series_items,\n            yield_type=IterNodeType.VALUES\n            )\n\n    @property\n    def iter_series_items(self) -> IterNodeAxis:\n        return IterNodeAxis(\n            container=self,\n            function_values=self._axis_series,\n            function_items=self._axis_series_items,\n            yield_type=IterNodeType.ITEMS\n            )\n\n    #---------------------------------------------------------------------------\n    @property\n    def iter_group(self) -> IterNodeGroupAxis:\n        \'\'\'\n        Iterate over Frames grouped by unique values in one or more rows or columns.\n        \'\'\'\n        return IterNodeGroupAxis(\n            container=self,\n            function_values=self._axis_group_loc,\n            function_items=self._axis_group_loc_items,\n            yield_type=IterNodeType.VALUES,\n            apply_type=IterNodeApplyType.SERIES_ITEMS_FLAT,\n            )\n\n    @property\n    def iter_group_items(self) -> IterNodeGroupAxis:\n        return IterNodeGroupAxis(\n            container=self,\n            function_values=self._axis_group_loc,\n            function_items=self._axis_group_loc_items,\n            yield_type=IterNodeType.ITEMS,\n            apply_type=IterNodeApplyType.SERIES_ITEMS_FLAT,\n            )\n\n    @property\n    def iter_group_labels(self) -> IterNodeDepthLevelAxis:\n        return IterNodeDepthLevelAxis(\n            container=self,\n            function_values=self._axis_group_labels,\n            function_items=self._axis_group_labels_items,\n            yield_type=IterNodeType.VALUES,\n            apply_type=IterNodeApplyType.SERIES_ITEMS_FLAT,\n            )\n\n    @property\n    def iter_group_labels_items(self) -> IterNodeDepthLevelAxis:\n        return IterNodeDepthLevelAxis(\n            container=self,\n            function_values=self._axis_group_labels,\n            function_items=self._axis_group_labels_items,\n            yield_type=IterNodeType.ITEMS,\n            apply_type=IterNodeApplyType.SERIES_ITEMS_FLAT,\n            )\n\n    #---------------------------------------------------------------------------\n\n    @property\n    def iter_window(self) -> IterNodeWindow:\n        function_values = partial(self._axis_window, as_array=False)\n        function_items = partial(self._axis_window_items, as_array=False)\n        return IterNodeWindow(\n            container=self,\n            function_values=function_values,\n            function_items=function_items,\n            yield_type=IterNodeType.VALUES\n            )\n\n    @property\n    def iter_window_items(self) -> IterNodeWindow:\n        function_values = partial(self._axis_window, as_array=False)\n        function_items = partial(self._axis_window_items, as_array=False)\n        return IterNodeWindow(\n            container=self,\n            function_values=function_values,\n            function_items=function_items,\n            yield_type=IterNodeType.ITEMS\n            )\n\n    @property\n    def iter_window_array(self) -> IterNodeWindow:\n        function_values = partial(self._axis_window, as_array=True)\n        function_items = partial(self._axis_window_items, as_array=True)\n        return IterNodeWindow(\n            container=self,\n            function_values=function_values,\n            function_items=function_items,\n            yield_type=IterNodeType.VALUES\n            )\n\n    @property\n    def iter_window_array_items(self) -> IterNodeWindow:\n        function_values = partial(self._axis_window, as_array=True)\n        function_items = partial(self._axis_window_items, as_array=True)\n        return IterNodeWindow(\n            container=self,\n            function_values=function_values,\n            function_items=function_items,\n            yield_type=IterNodeType.ITEMS\n            )\n\n    #---------------------------------------------------------------------------\n    @property\n    def iter_element(self) -> IterNodeNoArg:\n        return IterNodeNoArg(\n            container=self,\n            function_values=self._iter_element_loc,\n            function_items=self._iter_element_loc_items,\n            yield_type=IterNodeType.VALUES,\n            apply_type=IterNodeApplyType.FRAME_ELEMENTS\n            )\n\n    @property\n    def iter_element_items(self) -> IterNodeNoArg:\n        return IterNodeNoArg(\n            container=self,\n            function_values=self._iter_element_loc,\n            function_items=self._iter_element_loc_items,\n            yield_type=IterNodeType.ITEMS,\n            apply_type=IterNodeApplyType.FRAME_ELEMENTS\n            )\n\n    #---------------------------------------------------------------------------\n    # index manipulation\n\n    def _reindex_other_like_iloc(self,\n            value: tp.Union[Series, \'Frame\'],\n            iloc_key: GetItemKeyTypeCompound,\n            fill_value=np.nan\n            ) -> \'Frame\':\n        \'\'\'Given a value that is a Series or Frame, reindex it to the index components, drawn from this Frame, that are specified by the iloc_key.\n        \'\'\'\n        if isinstance(iloc_key, tuple):\n            row_key, column_key = iloc_key\n        else:\n            row_key, column_key = iloc_key, None\n\n        # within this frame, get Index objects by extracting based on passed-in iloc keys\n        nm_row, nm_column = self._extract_axis_not_multi(row_key, column_key)\n        v = None\n\n        if nm_row and not nm_column:\n            # only column is multi selection, reindex by column\n            if isinstance(value, Series):\n                v = value.reindex(self._columns._extract_iloc(column_key),\n                        fill_value=fill_value)\n        elif not nm_row and nm_column:\n            # only row is multi selection, reindex by index\n            if isinstance(value, Series):\n                v = value.reindex(self._index._extract_iloc(row_key),\n                        fill_value=fill_value)\n        elif not nm_row and not nm_column:\n            # both multi, must be a Frame\n            if isinstance(value, Frame):\n                target_column_index = self._columns._extract_iloc(column_key)\n                target_row_index = self._index._extract_iloc(row_key)\n                # this will use the default fillna type, which may or may not be what is wanted\n                v = value.reindex(\n                        index=target_row_index,\n                        columns=target_column_index,\n                        fill_value=fill_value)\n        if v is None:\n            raise RuntimeError((\'cannot assign \'\n                    + value.__class__.__name__\n                    + \' with key configuration\'), (nm_row, nm_column))\n        return v\n\n    @doc_inject(selector=\'reindex\', class_name=\'Frame\')\n    def reindex(self,\n            index: tp.Optional[IndexInitializer] = None,\n            columns: tp.Optional[IndexInitializer] = None,\n            *,\n            fill_value=np.nan,\n            own_index: bool = False,\n            own_columns: bool = False,\n            check_equals: bool = True,\n            ) -> \'Frame\':\n        \'\'\'\n        {doc}\n\n        Args:\n            index: {index_initializer}\n            columns: {index_initializer}\n            {fill_value}\n            {own_index}\n            {own_columns}\n        \'\'\'\n        if index is None and columns is None:\n            raise RuntimeError(\'must specify one of index or columns\')\n\n        if index is not None:\n            if not own_index:\n                index = index_from_optional_constructor(index,\n                        default_constructor=Index)\n\n            if check_equals and self._index.equals(index):\n                index_ic = None\n            else:\n                index_ic = IndexCorrespondence.from_correspondence(self._index, index)\n        else:\n            index = self._index\n            index_ic = None\n        # index can always be owned by this point, as self._index is STATIC, or  we have created a new Index, or we have bbeen given own_index\n        own_index_frame = True\n\n        if columns is not None:\n            if not own_columns:\n                columns = index_from_optional_constructor(columns,\n                        default_constructor=self._COLUMNS_CONSTRUCTOR)\n\n            if check_equals and self._columns.equals(columns):\n                columns_ic = None\n            else:\n                columns_ic = IndexCorrespondence.from_correspondence(self._columns, columns)\n            own_columns_frame = True\n        else:\n            columns = self._columns\n            columns_ic = None\n            own_columns_frame = self._COLUMNS_CONSTRUCTOR.STATIC\n\n        return self.__class__(\n                TypeBlocks.from_blocks(\n                        self._blocks.resize_blocks(\n                                index_ic=index_ic,\n                                columns_ic=columns_ic,\n                                fill_value=fill_value),\n                        shape_reference=(len(index), len(columns))\n                        ),\n                index=index,\n                columns=columns,\n                name=self._name,\n                own_data=True,\n                own_index=own_index_frame,\n                own_columns=own_columns_frame\n                )\n\n    @doc_inject(selector=\'relabel\', class_name=\'Frame\')\n    def relabel(self,\n            index: tp.Optional[RelabelInput] = None,\n            columns: tp.Optional[RelabelInput] = None\n            ) -> \'Frame\':\n        \'\'\'\n        {doc}\n\n        Args:\n            index: {relabel_input}\n            columns: {relabel_input}\n        \'\'\'\n        # create new index objects in both cases so as to call with own*\n        if index is None and columns is None:\n            raise RuntimeError(\'must specify one of index or columns\')\n\n        own_index = False\n        if index is IndexAutoFactory:\n            index = None\n        elif is_callable_or_mapping(index):\n            index = self._index.relabel(index)\n            own_index = True\n        elif index is None:\n            index = self._index\n\n        own_columns = False\n        if columns is IndexAutoFactory:\n            columns = None\n        elif is_callable_or_mapping(columns):\n            columns = self._columns.relabel(columns)\n            own_columns = True\n        elif columns is None:\n            columns = self._columns\n\n        return self.__class__(\n                self._blocks.copy(), # does not copy arrays\n                index=index,\n                columns=columns,\n                name=self._name,\n                own_data=True,\n                own_index=own_index,\n                own_columns=own_columns)\n\n    @doc_inject(selector=\'relabel_flat\', class_name=\'Frame\')\n    def relabel_flat(self,\n            index: bool = False,\n            columns: bool = False\n            ) -> \'Frame\':\n        \'\'\'\n        {doc}\n\n        Args:\n            index: Boolean to flag flatening on the index.\n            columns: Boolean to flag flatening on the columns.\n        \'\'\'\n        if not index and not columns:\n            raise RuntimeError(\'must specify one or both of columns, index\')\n\n        index = self._index.flat() if index else self._index.copy()\n        columns = self._columns.flat() if columns else self._columns.copy()\n\n        return self.__class__(\n                self._blocks.copy(), # does not copy arrays\n                index=index,\n                columns=columns,\n                name=self._name,\n                own_data=True,\n                own_index=True,\n                own_columns=True)\n\n    @doc_inject(selector=\'relabel_add_level\', class_name=\'Frame\')\n    def relabel_add_level(self,\n            index: tp.Hashable = None,\n            columns: tp.Hashable = None\n            ) -> \'Frame\':\n        \'\'\'\n        {doc}\n\n        Args:\n            index: {level}\n            columns: {level}\n        \'\'\'\n\n        index = self._index.add_level(index) if index else self._index.copy()\n        columns = self._columns.add_level(columns) if columns else self._columns.copy()\n\n        return self.__class__(\n                self._blocks.copy(), # does not copy arrays\n                index=index,\n                columns=columns,\n                name=self._name,\n                own_data=True,\n                own_index=True,\n                own_columns=True)\n\n    @doc_inject(selector=\'relabel_drop_level\', class_name=\'Frame\')\n    def relabel_drop_level(self,\n            index: int = 0,\n            columns: int = 0\n            ) -> \'Frame\':\n        \'\'\'\n        {doc}\n\n        Args:\n            index: {count} Default is zero.\n            columns: {count} Default is zero.\n        \'\'\'\n\n        index = self._index.drop_level(index) if index else self._index.copy()\n        columns = self._columns.drop_level(columns) if columns else self._columns.copy()\n\n        return self.__class__(\n                self._blocks.copy(), # does not copy arrays\n                index=index,\n                columns=columns,\n                name=self._name,\n                own_data=True,\n                own_index=True,\n                own_columns=True)\n\n    def rehierarch(self,\n            index: tp.Optional[tp.Iterable[int]] = None,\n            columns: tp.Optional[tp.Iterable[int]] = None,\n            ) -> \'Frame\':\n        \'\'\'\n        Produce a new `Frame` with index and/or columns constructed with a transformed hierarchy.\n        \'\'\'\n        if index and self.index.depth == 1:\n            raise RuntimeError(\'cannot rehierarch on index when there is no hierarchy\')\n        if columns and self.columns.depth == 1:\n            raise RuntimeError(\'cannot rehierarch on columns when there is no hierarchy\')\n\n        if index:\n            index, index_iloc = rehierarch_from_index_hierarchy(\n                    labels=self._index,\n                    depth_map=index,\n                    name=self._index.name\n                    )\n        else:\n            index = self._index\n            index_iloc = None\n\n        if columns:\n            columns, columns_iloc = rehierarch_from_index_hierarchy(\n                    labels=self._columns,\n                    depth_map=columns,\n                    name=self._columns.name\n                    )\n            own_columns = True\n        else:\n            columns = self._columns\n            own_columns = False # let constructor determine\n            columns_iloc = None\n\n        blocks = self._blocks._extract(index_iloc, columns_iloc)\n\n        return self.__class__(\n                blocks,\n                index=index,\n                columns=columns,\n                name=self._name,\n                own_data=True,\n                own_index=True,\n                own_columns=own_columns\n                )\n\n\n\n    #---------------------------------------------------------------------------\n    # na handling\n\n    def isna(self) -> \'Frame\':\n        \'\'\'\n        Return a same-indexed, Boolean Frame indicating True which values are NaN or None.\n        \'\'\'\n        # always return a Frame, even if this is a FrameGO\n        return Frame(self._blocks.isna(),\n                index=self._index,\n                columns=self._columns,\n                own_data=True)\n\n\n    def notna(self) -> \'Frame\':\n        \'\'\'\n        Return a same-indexed, Boolean Frame indicating True which values are not NaN or None.\n        \'\'\'\n        # always return a Frame, even if this is a FrameGO\n        return Frame(self._blocks.notna(),\n                index=self._index,\n                columns=self._columns,\n                own_data=True)\n\n    def dropna(self,\n            axis: int = 0,\n            condition: tp.Callable[[np.ndarray], bool] = np.all) -> \'Frame\':\n        \'\'\'\n        Return a new Frame after removing rows (axis 0) or columns (axis 1) where condition is True, where condition is an NumPy ufunc that process the Boolean array returned by isna().\n        \'\'\'\n        # returns Boolean areas that define axis to keep\n        row_key, column_key = self._blocks.dropna_to_keep_locations(\n                axis=axis,\n                condition=condition)\n\n        # NOTE: if not values to drop and this is a Frame (not a FrameGO) we can return self as it is immutable\n        if self.__class__ is Frame:\n            if (row_key is not None and column_key is not None\n                    and row_key.all() and column_key.all()):\n                return self\n        return self._extract(row_key, column_key)\n\n    @doc_inject(selector=\'fillna\')\n    def fillna(self, value: tp.Any) -> \'Frame\':\n        \'\'\'Return a new ``Frame`` after replacing null (NaN or None) with the supplied value.\n\n        Args:\n            {value}\n        \'\'\'\n        if hasattr(value, \'__iter__\') and not isinstance(value, str):\n            if not isinstance(value, Frame):\n                raise RuntimeError(\'unlabeled iterables cannot be used for fillna: use a Frame\')\n            # not sure what fill_value is best here, as value Frame might have hetergenous types; this might result in some undesirable type coercion\n            fill_value = dtype_to_na(value._blocks._row_dtype)\n\n            fill = value.reindex(\n                    index=self.index,\n                    columns=self.columns,\n                    fill_value=fill_value\n                    ).values\n            # produce a Boolean array that shows True only for labels (index, columns) found in the original `value` argument (before reindexing) and also in the target; this will be used to not set a NA when the value to fill was produced by reindexing.\n            fill_valid = self._blocks.extract_iloc_mask((\n                    self.index.isin(value.index.values),\n                    self.columns.isin(value.columns.values)\n                    )).values\n        else:\n            fill = value\n            fill_valid = None\n\n        return self.__class__(\n                self._blocks.fillna(fill, fill_valid),\n                index=self._index,\n                columns=self._columns,\n                name=self._name,\n                own_data=True\n                )\n\n    @doc_inject(selector=\'fillna\')\n    def fillna_leading(self,\n            value: tp.Any,\n            *,\n            axis: int = 0) -> \'Frame\':\n        \'\'\'\n        Return a new ``Frame`` after filling leading (and only leading) null (NaN or None) with the supplied value.\n\n        Args:\n            {value}\n            {axis}\n        \'\'\'\n        return self.__class__(self._blocks.fillna_leading(value, axis=axis),\n                index=self._index,\n                columns=self._columns,\n                name=self._name,\n                own_data=True)\n\n    @doc_inject(selector=\'fillna\')\n    def fillna_trailing(self,\n            value: tp.Any,\n            *,\n            axis: int = 0) -> \'Frame\':\n        \'\'\'\n        Return a new ``Frame`` after filling trailing (and only trailing) null (NaN or None) with the supplied value.\n\n        Args:\n            {value}\n            {axis}\n        \'\'\'\n        return self.__class__(self._blocks.fillna_trailing(value, axis=axis),\n                index=self._index,\n                columns=self._columns,\n                name=self._name,\n                own_data=True)\n\n    @doc_inject(selector=\'fillna\')\n    def fillna_forward(self,\n            limit: int = 0,\n            *,\n            axis: int = 0) -> \'Frame\':\n        \'\'\'\n        Return a new ``Frame`` after filling forward null (NaN or None) with the supplied value.\n\n        Args:\n            {limit}\n            {axis}\n        \'\'\'\n        return self.__class__(self._blocks.fillna_forward(limit=limit, axis=axis),\n                index=self._index,\n                columns=self._columns,\n                name=self._name,\n                own_data=True)\n\n    @doc_inject(selector=\'fillna\')\n    def fillna_backward(self,\n            limit: int = 0,\n            *,\n            axis: int = 0) -> \'Frame\':\n        \'\'\'\n        Return a new ``Frame`` after filling backward null (NaN or None) with the supplied value.\n\n        Args:\n            {limit}\n            {axis}\n        \'\'\'\n        return self.__class__(self._blocks.fillna_backward(limit=limit, axis=axis),\n                index=self._index,\n                columns=self._columns,\n                name=self._name,\n                own_data=True)\n\n    #---------------------------------------------------------------------------\n\n    def __len__(self) -> int:\n        \'\'\'Length of rows in values.\n        \'\'\'\n        return self._blocks._shape[0]\n\n    @doc_inject()\n    def display(self,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> Display:\n        \'\'\'{doc}\n\n        Args:\n            {config}\n        \'\'\'\n        config = config or DisplayActive.get()\n        index_depth = self._index.depth if config.include_index else 0\n\n        # always get the index Display (even if we are not going to use it) to dettermine how many rows we need (which may include types, as well as truncation with elipsis).\n        display_index = self._index.display(config=config)\n\n        # header depth useod for HTML and other foramtting; needs to be adjusted if removing types and/or columns and types, When showing types on a Frame, we need 2: one for the Frame type, the other for the index type.\n        header_depth = (self._columns.depth * config.include_columns) + (2 * config.type_show)\n\n        # create an empty display based on index display\n        d = Display([list() for _ in range(len(display_index))],\n                config=config,\n                outermost=True,\n                index_depth=index_depth,\n                header_depth=header_depth)\n\n\n        if config.include_index:\n            # this will add more rows to accomodate the index if it is bigger due to types\n            d.extend_display(display_index)\n            header_column = \'\' if config.type_show else None\n        else:\n            header_column = None\n\n        if self._blocks._shape[1] > config.display_columns:\n            # columns as they will look after application of truncation and insertion of ellipsis\n            data_half_count = Display.truncate_half_count(\n                    config.display_columns)\n            column_gen = partial(_gen_skip_middle,\n                    forward_iter=partial(self._blocks.axis_values, axis=0),\n                    forward_count=data_half_count,\n                    reverse_iter=partial(self._blocks.axis_values, axis=0, reverse=True),\n                    reverse_count=data_half_count,\n                    center_sentinel=Display.ELLIPSIS_CENTER_SENTINEL\n                    )\n        else:\n            column_gen = partial(self._blocks.axis_values, axis=0)\n\n        for column in column_gen():\n            if column is Display.ELLIPSIS_CENTER_SENTINEL:\n                d.extend_ellipsis()\n            else:\n                d.extend_iterable(column, header=header_column)\n\n        #-----------------------------------------------------------------------\n        config_transpose = config.to_transpose()\n\n        #-----------------------------------------------------------------------\n        # prepare header display of container class\n        header_displays = []\n        if config.type_show:\n            display_cls = Display.from_values((),\n                    header=DisplayHeader(self.__class__, self._name),\n                    config=config_transpose)\n            header_displays.append(display_cls.flatten())\n\n        #-----------------------------------------------------------------------\n        # prepare columns display\n        if config.include_columns:\n            # need to apply the config_transpose such that it truncates it based on the the max columns, not the max rows\n            display_columns = self._columns.display(config=config_transpose)\n\n            if config.type_show:\n                index_depth_extend = self._index.depth - 1\n                spacer_insert_index = 1 # after the first, the name\n            elif not config.type_show and config.include_index:\n                index_depth_extend = self._index.depth\n                spacer_insert_index = 0\n            elif not config.include_index: # type_show must be False\n                index_depth_extend = 0\n                spacer_insert_index = 0\n\n            # add spacers to from of columns when we have a hierarchical index\n            for _ in range(index_depth_extend):\n                # will need a width equal to the column depth\n                row = [Display.to_cell(\'\', config=config)\n                        for _ in range(self._columns.depth)]\n                spacer = Display([row])\n                display_columns.insert_displays(spacer,\n                        insert_index=spacer_insert_index)\n\n            if self._columns.depth > 1:\n                display_columns_horizontal = display_columns.transform()\n            else: # can just flatten a single column into one row\n                display_columns_horizontal = display_columns.flatten()\n\n            header_displays.append(display_columns_horizontal)\n\n        if header_displays:\n            d.insert_displays(*header_displays)\n\n        return d\n\n    #---------------------------------------------------------------------------\n    # accessors\n\n    @property\n    @doc_inject(selector=\'values_2d\', class_name=\'Frame\')\n    def values(self) -> np.ndarray:\n        \'\'\'\n        {}\n        \'\'\'\n        return self._blocks.values\n\n    @property\n    def index(self) -> Index:\n        \'\'\'The ``IndexBase`` instance assigned for row labels.\n        \'\'\'\n        return self._index\n\n    @property\n    def columns(self) -> Index:\n        \'\'\'The ``IndexBase`` instance assigned for column labels.\n        \'\'\'\n        return self._columns\n\n    #---------------------------------------------------------------------------\n    # common attributes from the numpy array\n\n    @property\n    def dtypes(self) -> Series:\n        \'\'\'\n        Return a Series of dytpes for each realizable column.\n\n        Returns:\n            :obj:`static_frame.Series`\n        \'\'\'\n        return Series(self._blocks.dtypes,\n                index=immutable_index_filter(self._columns),\n                name=self._name\n                )\n\n    @property\n    @doc_inject()\n    def mloc(self) -> np.ndarray:\n        \'\'\'{doc_array}\n        \'\'\'\n        return self._blocks.mloc\n\n    #---------------------------------------------------------------------------\n\n    @property\n    def shape(self) -> tp.Tuple[int, int]:\n        \'\'\'\n        Return a tuple describing the shape of the underlying NumPy array.\n\n        Returns:\n            :obj:`tp.Tuple[int]`\n        \'\'\'\n        return self._blocks._shape\n\n    @property\n    def ndim(self) -> int:\n        \'\'\'\n        Return the number of dimensions, which for a `Frame` is always 2.\n\n        Returns:\n            :obj:`int`\n        \'\'\'\n        return self._NDIM\n\n    @property\n    def size(self) -> int:\n        \'\'\'\n        Return the size of the underlying NumPy array.\n\n        Returns:\n            :obj:`int`\n        \'\'\'\n\n        return self._blocks.size\n\n    @property\n    def nbytes(self) -> int:\n        \'\'\'\n        Return the total bytes of the underlying NumPy array.\n\n        Returns:\n            :obj:`int`\n        \'\'\'\n        return self._blocks.nbytes\n\n    def __bool__(self) -> bool:\n        \'\'\'\n        True if this container has size.\n        \'\'\'\n        return bool(self._blocks.size)\n\n\n\n    #---------------------------------------------------------------------------\n    @staticmethod\n    def _extract_axis_not_multi(\n                row_key,\n                column_key,\n                ) -> tp.Tuple[bool, bool]:\n        \'\'\'\n        If either row or column is given with a non-multiple type of selection (a single scalar), reduce dimensionality.\n        \'\'\'\n        row_nm = False\n        column_nm = False\n        if row_key is not None and not isinstance(row_key, KEY_MULTIPLE_TYPES):\n            row_nm = True # axis 0\n        if column_key is not None and not isinstance(column_key, KEY_MULTIPLE_TYPES):\n            column_nm = True # axis 1\n        return row_nm, column_nm\n\n\n    def _extract(self,\n            row_key: GetItemKeyType = None,\n            column_key: GetItemKeyType = None,\n            ) -> tp.Union[\'Frame\', Series]:\n        \'\'\'\n        Extract based on iloc selection (indices have already mapped)\n        \'\'\'\n        blocks = self._blocks._extract(row_key=row_key, column_key=column_key)\n\n        if not isinstance(blocks, TypeBlocks):\n            return blocks # reduced to an element\n\n        own_index = True # the extracted Frame can always own this index\n        row_key_is_slice = isinstance(row_key, slice)\n        if row_key is None or (row_key_is_slice and row_key == NULL_SLICE):\n            index = self._index\n        else:\n            index = self._index._extract_iloc(row_key)\n            if not row_key_is_slice:\n                name_row = self._index.values[row_key]\n                if self._index.depth > 1:\n                    name_row = tuple(name_row)\n\n        # can only own columns if _COLUMNS_CONSTRUCTOR is static\n        column_key_is_slice = isinstance(column_key, slice)\n        if column_key is None or (column_key_is_slice and column_key == NULL_SLICE):\n            columns = self._columns\n            own_columns = self._COLUMNS_CONSTRUCTOR.STATIC\n        else:\n            columns = self._columns._extract_iloc(column_key)\n            own_columns = True\n            if not column_key_is_slice:\n                name_column = self._columns.values[column_key]\n                if self._columns.depth > 1:\n                    name_column = tuple(name_column)\n\n        # determine if an axis is not multi; if one axis is not multi, we return a Series instead of a Frame\n        axis_nm = self._extract_axis_not_multi(row_key, column_key)\n        blocks_shape = blocks._shape\n\n        if blocks_shape[0] == 0 or blocks_shape[1] == 0:\n            # return a 0-sized Series\n            if axis_nm[0]: # if row not multi\n                return Series(EMPTY_TUPLE,\n                        index=immutable_index_filter(columns),\n                        name=name_row)\n            elif axis_nm[1]:\n                return Series(EMPTY_TUPLE,\n                        index=index,\n                        name=name_column)\n        elif blocks_shape == (1, 1):\n            # if TypeBlocks did not return an element, need to determine which axis to use for Series index\n            if axis_nm[0]: # if row not multi\n                return Series(blocks.values[0],\n                        index=immutable_index_filter(columns),\n                        name=name_row)\n            elif axis_nm[1]:\n                return Series(blocks.values[0],\n                        index=index,\n                        name=name_column)\n            # if both are multi, we return a Frame\n        elif blocks_shape[0] == 1: # if one row\n            if axis_nm[0]: # if row key not multi\n                # best to use blocks.values, as will need to consolidate dtypes; will always return a 2D array\n                return Series(blocks.values[0],\n                        index=immutable_index_filter(columns),\n                        name=name_row)\n        elif blocks_shape[1] == 1: # if one column\n            if axis_nm[1]: # if column key is not multi\n                return Series(\n                        column_1d_filter(blocks._blocks[0]),\n                        index=index,\n                        name=name_column)\n\n        return self.__class__(blocks,\n                index=index,\n                columns=columns,\n                name=self._name,\n                own_data=True, # always get new TypeBlock instance above\n                own_index=own_index,\n                own_columns=own_columns\n                )\n\n\n    def _extract_iloc(self, key: GetItemKeyTypeCompound) -> \'Frame\':\n        \'\'\'\n        Give a compound key, return a new Frame. This method simply handles the variabiliyt of single or compound selectors.\n        \'\'\'\n        if isinstance(key, tuple):\n            return self._extract(*key)\n        return self._extract(row_key=key)\n\n    def _compound_loc_to_iloc(self,\n            key: GetItemKeyTypeCompound) -> tp.Tuple[GetItemKeyType, GetItemKeyType]:\n        \'\'\'\n        Given a compound iloc key, return a tuple of row, column keys. Assumes the first argument is always a row extractor.\n        \'\'\'\n        if isinstance(key, tuple):\n            loc_row_key, loc_column_key = key\n            iloc_column_key = self._columns.loc_to_iloc(loc_column_key)\n        else:\n            loc_row_key = key\n            iloc_column_key = None\n\n        iloc_row_key = self._index.loc_to_iloc(loc_row_key)\n        return iloc_row_key, iloc_column_key\n\n    def _compound_loc_to_getitem_iloc(self,\n            key: GetItemKeyTypeCompound) -> tp.Tuple[GetItemKeyType, GetItemKeyType]:\n        \'\'\'Handle a potentially compound key in the style of __getitem__. This will raise an appropriate exception if a two argument loc-style call is attempted.\n        \'\'\'\n        iloc_column_key = self._columns.loc_to_iloc(key)\n        return None, iloc_column_key\n\n    def _extract_loc(self, key: GetItemKeyTypeCompound) -> \'Frame\':\n        iloc_row_key, iloc_column_key = self._compound_loc_to_iloc(key)\n        return self._extract(\n                row_key=iloc_row_key,\n                column_key=iloc_column_key\n                )\n\n    def _extract_bloc(self, key: Bloc2DKeyType) -> Series:\n        \'\'\'\n        2D Boolean selector, selected by either a Boolean 2D Frame or array.\n        \'\'\'\n        bloc_key = bloc_key_normalize(key=key, container=self)\n        values = self.values[bloc_key]\n        values.flags.writeable = False\n\n        index = Index(\n                (self._index[x], self._columns[y])\n                for x, y in zip(*np.nonzero(bloc_key))\n                )\n        return Series(values, index=index, own_index=True)\n\n    @doc_inject(selector=\'selector\')\n    def __getitem__(self, key: GetItemKeyType) -> tp.Union[\'Frame\', Series]:\n        \'\'\'Selector of columns by label.\n\n        Args:\n            key: {key_loc}\n        \'\'\'\n        return self._extract(*self._compound_loc_to_getitem_iloc(key))\n\n\n    #---------------------------------------------------------------------------\n\n    def _drop_iloc(self, key: GetItemKeyTypeCompound) -> \'Frame\':\n        \'\'\'\n        Args:\n            key: If a Boolean Series was passed, it has been converted to Boolean NumPy array already in loc to iloc.\n        \'\'\'\n\n        blocks = self._blocks.drop(key)\n\n        if isinstance(key, tuple):\n            iloc_row_key, iloc_column_key = key\n\n            index = self._index._drop_iloc(iloc_row_key)\n            own_index = True\n\n            columns = self._columns._drop_iloc(iloc_column_key)\n            own_columns = True\n        else:\n            iloc_row_key = key # no column selection\n\n            index = self._index._drop_iloc(iloc_row_key)\n            own_index = True\n\n            columns = self._columns\n            own_columns = False\n\n        return self.__class__(blocks,\n                columns=columns,\n                index=index,\n                name=self._name,\n                own_data=True,\n                own_columns=own_columns,\n                own_index=own_index\n                )\n\n    def _drop_loc(self, key: GetItemKeyTypeCompound) -> \'Frame\':\n        key = self._compound_loc_to_iloc(key)\n        return self._drop_iloc(key=key)\n\n    def _drop_getitem(self, key: GetItemKeyTypeCompound) -> \'Frame\':\n        key = self._compound_loc_to_getitem_iloc(key)\n        return self._drop_iloc(key=key)\n\n\n    #---------------------------------------------------------------------------\n    def _extract_iloc_mask(self, key: GetItemKeyTypeCompound) -> \'Frame\':\n        masked_blocks = self._blocks.extract_iloc_mask(key)\n        return self.__class__(masked_blocks,\n                columns=self._columns,\n                index=self._index,\n                own_data=True)\n\n    def _extract_loc_mask(self, key: GetItemKeyTypeCompound) -> \'Frame\':\n        key = self._compound_loc_to_iloc(key)\n        return self._extract_iloc_mask(key=key)\n\n    def _extract_getitem_mask(self, key: GetItemKeyTypeCompound) -> \'Frame\':\n        key = self._compound_loc_to_getitem_iloc(key)\n        return self._extract_iloc_mask(key=key)\n\n    #---------------------------------------------------------------------------\n    def _extract_iloc_masked_array(self, key: GetItemKeyTypeCompound) -> MaskedArray:\n        masked_blocks = self._blocks.extract_iloc_mask(key)\n        return MaskedArray(data=self.values, mask=masked_blocks.values)\n\n    def _extract_loc_masked_array(self, key: GetItemKeyTypeCompound) -> MaskedArray:\n        key = self._compound_loc_to_iloc(key)\n        return self._extract_iloc_masked_array(key=key)\n\n    def _extract_getitem_masked_array(self, key: GetItemKeyTypeCompound) -> \'Frame\':\n        key = self._compound_loc_to_getitem_iloc(key)\n        return self._extract_iloc_masked_array(key=key)\n\n    #---------------------------------------------------------------------------\n    def _extract_iloc_assign(self, key: GetItemKeyTypeCompound) -> \'FrameAssign\':\n        return FrameAssign(self, iloc_key=key)\n\n    def _extract_loc_assign(self, key: GetItemKeyTypeCompound) -> \'FrameAssign\':\n        # extract if tuple, then pack back again\n        key = self._compound_loc_to_iloc(key)\n        return self._extract_iloc_assign(key=key)\n\n    def _extract_getitem_assign(self, key: GetItemKeyTypeCompound) -> \'FrameAssign\':\n        # extract if tuple, then pack back again\n        key = self._compound_loc_to_getitem_iloc(key)\n        return self._extract_iloc_assign(key=key)\n\n    def _extract_bloc_assign(self, key: Bloc2DKeyType) -> \'FrameAssign\':\n        \'\'\'Assignment based on a Boolean Frame or array.\'\'\'\n        return FrameAssign(self, bloc_key=key)\n\n    #---------------------------------------------------------------------------\n\n    def _extract_getitem_astype(self, key: GetItemKeyType) -> \'FrameAsType\':\n        # extract if tuple, then pack back again\n        _, key = self._compound_loc_to_getitem_iloc(key)\n        return FrameAsType(self, column_key=key)\n\n\n\n    #---------------------------------------------------------------------------\n    # dictionary-like interface\n\n    def keys(self):\n        \'\'\'Iterator of column labels.\n        \'\'\'\n        return self._columns\n\n    def __iter__(self):\n        \'\'\'\n        Iterator of column labels, same as :py:meth:`Frame.keys`.\n        \'\'\'\n        return self._columns.__iter__()\n\n    def __contains__(self, value) -> bool:\n        \'\'\'\n        Inclusion of value in column labels.\n        \'\'\'\n        return self._columns.__contains__(value)\n\n    def items(self) -> tp.Iterator[tp.Tuple[tp.Any, Series]]:\n        \'\'\'Iterator of pairs of column label and corresponding column :obj:`Series`.\n        \'\'\'\n        for label, array in zip(self._columns.values, self._blocks.axis_values(0)):\n            # array is assumed to be immutable\n            yield label, Series(array, index=self._index, name=label)\n\n    def get(self, key, default=None):\n        \'\'\'\n        Return the value found at the columns key, else the default if the key is not found. This method is implemented to complete the dictionary-like interface.\n        \'\'\'\n        if key not in self._columns:\n            return default\n        return self.__getitem__(key)\n\n\n    #---------------------------------------------------------------------------\n    # operator functions\n\n\n    def _ufunc_unary_operator(self, operator: tp.Callable) -> \'Frame\':\n        # call the unary operator on _blocks\n        return self.__class__(\n                self._blocks._ufunc_unary_operator(operator=operator),\n                index=self._index,\n                columns=self._columns,\n                name=self._name,\n                )\n\n    def _ufunc_binary_operator(self, *,\n            operator,\n            other\n            ) -> \'Frame\':\n\n        if operator.__name__ == \'matmul\':\n            return matmul(self, other)\n        elif operator.__name__ == \'rmatmul\':\n            return matmul(other, self)\n\n        #TODO: use equals on columns, index before calling reindex\n\n        if isinstance(other, Frame):\n            name = None\n            # reindex both dimensions to union indices\n            columns = self._columns.union(other._columns)\n            index = self._index.union(other._index)\n\n            # NOTE: always own column, index, as we will just extract Typeblocks\n            self_tb = self.reindex(\n                    columns=columns,\n                    index=index,\n                    own_index=True,\n                    own_columns=True)._blocks\n            # NOTE: we create columns from self._columns, and thus other can only own it if STATIC matches\n            own_columns = other.STATIC == self.STATIC\n            other_tb = other.reindex(\n                    columns=columns,\n                    index=index,\n                    own_index=True,\n                    own_columns=own_columns)._blocks\n            return self.__class__(self_tb._ufunc_binary_operator(\n                            operator=operator,\n                            other=other_tb),\n                    index=index,\n                    columns=columns,\n                    own_data=True,\n                    own_index=True,\n                    )\n        elif isinstance(other, Series):\n            name = None\n            # when operating on a Series, we treat it as a row-wise operation, and thus take the union of the Series.index and Frame.columns\n            columns = self._columns.union(other._index)\n            self_tb = self.reindex(columns=columns, own_columns=True)._blocks\n            other_array = other.reindex(columns, own_index=True).values\n            return self.__class__(self_tb._ufunc_binary_operator(\n                            operator=operator,\n                            other=other_array),\n                    index=self._index,\n                    columns=columns,\n                    own_data=True,\n                    own_index=True,\n                    )\n        elif isinstance(other, np.ndarray):\n            name = None\n        else:\n            other = iterable_to_array_nd(other)\n            if other.ndim == 0:# only for elements should we keep name\n                name = self._name\n            else:\n                name = None\n\n        # assume we will keep dimensionality\n        return self.__class__(self._blocks._ufunc_binary_operator(\n                        operator=operator,\n                        other=other),\n                index=self._index,\n                columns=self._columns,\n                own_data=True,\n                own_index=True,\n                name=name,\n                )\n\n    #---------------------------------------------------------------------------\n    # axis functions\n\n    def _ufunc_axis_skipna(self, *,\n            axis: int,\n            skipna: bool,\n            ufunc: UFunc,\n            ufunc_skipna: UFunc,\n            composable: bool,\n            dtypes: tp.Tuple[np.dtype, ...],\n            size_one_unity: bool\n            ) -> \'Series\':\n        # axis 0 processes ros, deliveres column index\n        # axis 1 processes cols, delivers row index\n        assert axis < 2\n\n        post = self._blocks.ufunc_axis_skipna(\n                skipna=skipna,\n                axis=axis,\n                ufunc=ufunc,\n                ufunc_skipna=ufunc_skipna,\n                composable=composable,\n                dtypes=dtypes,\n                size_one_unity=size_one_unity\n                )\n\n        # post has been made immutable so Series will own\n        if axis == 0:\n            return Series(\n                    post,\n                    index=immutable_index_filter(self._columns)\n                    )\n        return Series(post, index=self._index)\n\n    def _ufunc_shape_skipna(self, *,\n            axis,\n            skipna,\n            ufunc,\n            ufunc_skipna,\n            composable: bool,\n            dtypes: tp.Tuple[np.dtype, ...],\n            size_one_unity: bool\n            ) -> \'Frame\':\n        # axis 0 processes ros, deliveres column index\n        # axis 1 processes cols, delivers row index\n        assert axis < 2\n\n        dtype = None if not dtypes else dtypes[0]\n\n        # assumed not composable for axis 1, full-shape processing requires processing contiguous values\n        v = self.values\n        if skipna:\n            post = ufunc_skipna(v, axis=axis, dtype=dtype)\n        else:\n            post = ufunc(v, axis=axis, dtype=dtype)\n\n        post.flags.writeable = False\n\n        return self.__class__(\n                TypeBlocks.from_blocks(post),\n                index=self._index,\n                columns=self._columns,\n                own_data=True,\n                own_index=True\n                )\n\n    #---------------------------------------------------------------------------\n    # axis iterators\n    # NOTE: if there is more than one argument, the axis argument needs to be key-word only\n\n    def _axis_array(self, axis):\n        \'\'\'Generator of arrays across an axis\n        \'\'\'\n        yield from self._blocks.axis_values(axis)\n\n    def _axis_array_items(self, axis):\n        keys = self._index if axis == 1 else self._columns\n        yield from zip(keys, self._blocks.axis_values(axis))\n\n\n    def _axis_tuple(self, axis):\n        \'\'\'Generator of named tuples across an axis.\n\n        Args:\n            axis: 0 iterates over columns (index axis), 1 iterates over rows (column axis)\n        \'\'\'\n        if axis == 1:\n            Tuple = get_tuple_constructor(self._columns.values)\n        elif axis == 0:\n            Tuple = get_tuple_constructor(self._index.values)\n        else:\n            raise NotImplementedError()\n\n        for axis_values in self._blocks.axis_values(axis):\n            yield Tuple(*axis_values)\n\n    def _axis_tuple_items(self, axis):\n        keys = self._index if axis == 1 else self._columns\n        yield from zip(keys, self._axis_tuple(axis=axis))\n\n\n    def _axis_series(self, axis):\n        \'\'\'Generator of Series across an axis\n        \'\'\'\n        # reference the indices and let the constructor reuse what is reusable\n        if axis == 1:\n            index = self._columns\n            labels = self._index\n        elif axis == 0:\n            index = self._index\n            labels = self._columns\n        for label, axis_values in zip(labels, self._blocks.axis_values(axis)):\n            yield Series(axis_values, index=index, name=label)\n\n    def _axis_series_items(self, axis):\n        keys = self._index if axis == 1 else self._columns\n        yield from zip(keys, self._axis_series(axis=axis))\n\n\n    #---------------------------------------------------------------------------\n    # grouping methods naturally return their ""index"" as the group element\n\n    def _axis_group_iloc_items(self,\n            key,\n            *,\n            axis: int):\n\n        for group, selection, tb in self._blocks.group(axis=axis, key=key):\n\n            if axis == 0:\n                # axis 0 is a row iter, so need to slice index, keep columns\n                yield group, self.__class__(tb,\n                        index=self._index[selection],\n                        columns=self._columns, # let constructor determine ownership\n                        own_index=True,\n                        own_data=True)\n            elif axis == 1:\n                # axis 1 is a column iterators, so need to slice columns, keep index\n                yield group, self.__class__(tb,\n                        index=self._index,\n                        columns=self._columns[selection],\n                        own_index=True,\n                        own_columns=True,\n                        own_data=True)\n            else:\n                raise AxisInvalid(f\'invalid axis: {axis}\') #pragma: no cover (already caught above)\n\n    def _axis_group_sort_items(self,\n            key,\n            iloc_key,\n            axis: int\n            ) -> tp.Iterator[tp.Tuple[tp.Hashable, \'Frame\']]:\n        # Create a sorted copy since we do not want to change the underlying data\n        frame_sorted: Frame = self.sort_values(key, axis=not axis)\n\n        def build_frame(key, index):\n            if axis == 0:\n                return Frame(frame_sorted._blocks._extract(row_key=key),\n                        columns=self.columns,\n                        index=index,\n                        own_data=True)\n            else:\n                return Frame(frame_sorted._blocks._extract(column_key=key),\n                        columns=index,\n                        index=self.index,\n                        own_data=True)\n\n        if axis == 0:\n            max_iloc: int = len(self._index)\n            index: Index = frame_sorted.index\n            def get_group(i: int) -> tp.Hashable:\n                return frame_sorted.iloc[i, iloc_key]\n        else:\n            max_iloc = len(self._columns)\n            index = frame_sorted.columns\n            def get_group(i: int) -> tp.Hashable:\n                return frame_sorted.iloc[iloc_key, i]\n\n        group: tp.Hashable = get_group(0)\n        start = 0\n        i = 0\n\n        while i < max_iloc:\n            next_group: tp.Hashable = get_group(i)\n\n            if group != next_group:\n                slc: slice = slice(start, i)\n                sliced_index: Index = index[slc]\n                yield group, build_frame(slc, sliced_index)\n\n                start = i\n                group = next_group\n            i += 1\n\n        yield group, build_frame(slice(start, None), index[start:])\n\n\n    def _axis_group_loc_items(self,\n            key: GetItemKeyType,\n            *,\n            axis: int = 0\n            ):\n        \'\'\'\n        Args:\n            key: We accept any thing that can do loc to iloc. Note that a tuple is permitted as key, where it would be interpreted as a single label for an IndexHierarchy.\n        \'\'\'\n        if axis == 0: # row iterator, selecting columns for group by\n            iloc_key = self._columns.loc_to_iloc(key)\n        elif axis == 1: # column iterator, selecting rows for group by\n            iloc_key = self._index.loc_to_iloc(key)\n        else:\n            raise AxisInvalid(f\'invalid axis: {axis}\')\n\n        # NOTE: might identify when key is a list of one item\n\n        # Optimized sorting approach is only supported in a limited number of cases\n        if (self.columns.depth == 1 and\n                self.index.depth == 1 and\n                not isinstance(key, KEY_MULTIPLE_TYPES)\n                ):\n            if axis == 0:\n                has_object = self._blocks.dtypes[iloc_key] == DTYPE_OBJECT\n            else:\n                has_object = self._blocks._row_dtype == DTYPE_OBJECT\n            if not has_object:\n                yield from self._axis_group_sort_items(key=key,\n                        iloc_key=iloc_key,\n                        axis=axis)\n            else:\n                yield from self._axis_group_iloc_items(key=iloc_key, axis=axis)\n        else:\n            yield from self._axis_group_iloc_items(key=iloc_key, axis=axis)\n\n\n    def _axis_group_loc(self,\n            key: GetItemKeyType,\n            *,\n            axis: int = 0\n            ):\n        yield from (x for _, x in self._axis_group_loc_items(key=key, axis=axis))\n\n\n    def _axis_group_labels_items(self,\n            depth_level: DepthLevelSpecifier = 0,\n            *,\n            axis=0):\n\n        if axis == 0: # maintain columns, group by index\n            ref_index = self._index\n        elif axis == 1: # maintain index, group by columns\n            ref_index = self._columns\n        else:\n            raise AxisInvalid(f\'invalid axis: {axis}\')\n\n        values = ref_index.values_at_depth(depth_level)\n        group_to_tuple = values.ndim > 1\n\n        groups, locations = array_to_groups_and_locations(values)\n\n        for idx, group in enumerate(groups):\n            selection = locations == idx\n\n            if axis == 0:\n                # axis 0 is a row iter, so need to slice index, keep columns\n                tb = self._blocks._extract(row_key=selection)\n                yield group, self.__class__(tb,\n                        index=self._index[selection],\n                        columns=self._columns, # let constructor determine ownership\n                        own_index=True,\n                        own_data=True)\n\n            elif axis == 1:\n                # axis 1 is a column iterators, so need to slice columns, keep index\n                tb = self._blocks._extract(column_key=selection)\n                yield group, self.__class__(tb,\n                        index=self._index,\n                        columns=self._columns[selection],\n                        own_index=True,\n                        own_columns=True,\n                        own_data=True)\n\n    def _axis_group_labels(self,\n            depth_level: DepthLevelSpecifier = 0,\n            *,\n            axis=0):\n        yield from (x for _, x in self._axis_group_labels_items(\n                depth_level=depth_level, axis=axis))\n\n    #---------------------------------------------------------------------------\n    def _axis_window_items(self, *,\n            size: int,\n            axis: int = 0,\n            step: int = 1,\n            window_sized: bool = True,\n            window_func: tp.Optional[AnyCallable] = None,\n            window_valid: tp.Optional[AnyCallable] = None,\n            label_shift: int = 0,\n            start_shift: int = 0,\n            size_increment: int = 0,\n            as_array: bool = False,\n            ) -> tp.Iterator[tp.Tuple[tp.Hashable, tp.Any]]:\n        \'\'\'Generator of index, processed-window pairs.\n        \'\'\'\n        yield from axis_window_items(\n                source=self,\n                size=size,\n                axis=axis,\n                step=step,\n                window_sized=window_sized,\n                window_func=window_func,\n                window_valid=window_valid,\n                label_shift=label_shift,\n                start_shift=start_shift,\n                size_increment=size_increment,\n                as_array=as_array\n                )\n\n\n    def _axis_window(self, *,\n            size: int,\n            axis: int = 0,\n            step: int = 1,\n            window_sized: bool = True,\n            window_func: tp.Optional[AnyCallable] = None,\n            window_valid: tp.Optional[AnyCallable] = None,\n            label_shift: int = 0,\n            start_shift: int = 0,\n            size_increment: int = 0,\n            as_array: bool = False,\n            ):\n        yield from (x for _, x in self._axis_window_items(\n                size=size,\n                axis=axis,\n                step=step,\n                window_sized=window_sized,\n                window_func=window_func,\n                window_valid=window_valid,\n                label_shift=label_shift,\n                start_shift=start_shift,\n                size_increment=size_increment,\n                as_array=as_array\n                ))\n\n\n    #---------------------------------------------------------------------------\n\n    def _iter_element_iloc_items(self):\n        yield from self._blocks.element_items()\n\n    # def _iter_element_iloc(self):\n    #     yield from (x for _, x in self._iter_element_iloc_items())\n\n    def _iter_element_loc_items(self) -> tp.Iterator[\n            tp.Tuple[tp.Tuple[tp.Hashable, tp.Hashable], tp.Any]]:\n        \'\'\'\n        Generator of pairs of (index, column), value.\n        \'\'\'\n        yield from (\n                ((self._index[k[0]], self._columns[k[1]]), v)\n                for k, v in self._blocks.element_items()\n                )\n\n    def _iter_element_loc(self):\n        yield from (x for _, x in self._iter_element_loc_items())\n\n\n    #---------------------------------------------------------------------------\n    # transformations resulting in the same dimensionality\n\n    def __reversed__(self) -> tp.Iterator[tp.Hashable]:\n        \'\'\'\n        Returns a reverse iterator on the frame\'s columns.\n        \'\'\'\n        return reversed(self._columns)\n\n    def sort_index(self,\n            *,\n            ascending: bool = True,\n            kind: str = DEFAULT_SORT_KIND\n            ) -> \'Frame\':\n        \'\'\'\n        Return a new Frame ordered by the sorted Index.\n        \'\'\'\n        if self._index.depth > 1:\n            v = self._index.values\n            order = np.lexsort([v[:, i] for i in range(v.shape[1]-1, -1, -1)])\n        else:\n            # argsort lets us do the sort once and reuse the results\n            order = np.argsort(self._index.values, kind=kind)\n\n        if not ascending:\n            order = order[::-1]\n\n        index_values = self._index.values[order]\n        index_values.flags.writeable = False\n        index = self._index.from_labels(index_values, name=self._index.name)\n\n        blocks = self._blocks.iloc[order]\n        return self.__class__(blocks,\n                index=index,\n                columns=self._columns,\n                name=self._name,\n                own_data=True,\n                own_index=True,\n                )\n\n    def sort_columns(self,\n            *,\n            ascending: bool = True,\n            kind: str = DEFAULT_SORT_KIND) -> \'Frame\':\n        \'\'\'\n        Return a new Frame ordered by the sorted Columns.\n        \'\'\'\n        if self._columns.depth > 1:\n            v = self._columns.values\n            order = np.lexsort([v[:, i] for i in range(v.shape[1]-1, -1, -1)])\n        else:\n            # argsort lets us do the sort once and reuse the results\n            order = np.argsort(self._columns.values, kind=kind)\n\n        if not ascending:\n            order = order[::-1]\n\n        columns_values = self._columns.values[order]\n        columns_values.flags.writeable = False\n        columns = self._columns.from_labels(columns_values,  name=self._columns.name)\n\n        blocks = self._blocks[order]\n        return self.__class__(blocks,\n                index=self._index,\n                columns=columns,\n                name=self._name,\n                own_data=True,\n                own_columns=True,\n                )\n\n    def sort_values(self,\n            key: KeyOrKeys,\n            *,\n            ascending: bool = True,\n            axis: int = 1,\n            kind=DEFAULT_SORT_KIND) -> \'Frame\':\n        \'\'\'\n        Return a new Frame ordered by the sorted values, where values is given by single column or iterable of columns.\n\n        Args:\n            key: a key or iterable of keys.\n        \'\'\'\n        # argsort lets us do the sort once and reuse the results\n        if axis == 0: # get a column ordering based on one or more rows\n            col_count = self._columns.__len__()\n            if is_hashable(key) and key in self._index:\n                iloc_key = self._index.loc_to_iloc(key)\n                sort_array = self._blocks._extract_array(row_key=iloc_key)\n                order = np.argsort(sort_array, kind=kind)\n            else: # assume an iterable of keys\n                # order so that highest priority is last\n                iloc_keys = (self._index.loc_to_iloc(k) for k in reversed_iter(key))\n                sort_array = [self._blocks._extract_array(row_key=key)\n                        for key in iloc_keys]\n                order = np.lexsort(sort_array)\n        elif axis == 1: # get a row ordering based on one or more columns\n            if is_hashable(key) and key in self._columns:\n                iloc_key = self._columns.loc_to_iloc(key)\n                sort_array = self._blocks._extract_array(column_key=iloc_key)\n                order = np.argsort(sort_array, kind=kind)\n            else: # assume an iterable of keys\n                # order so that highest priority is last\n                iloc_keys = (self._columns.loc_to_iloc(k) for k in reversed_iter(key))\n                sort_array = [self._blocks._extract_array(column_key=key)\n                        for key in iloc_keys]\n                order = np.lexsort(sort_array)\n        else:\n            raise AxisInvalid(f\'invalid axis: {axis}\')\n\n        if not ascending:\n            order = order[::-1]\n\n        if axis == 0:\n            column_values = self._columns.values[order]\n            column_values.flags.writeable = False\n            columns = self._columns.from_labels(\n                    column_values,\n                    name=self._columns._name\n                    )\n            blocks = self._blocks[order] # order columns\n            return self.__class__(blocks,\n                    index=self._index,\n                    columns=columns,\n                    name=self._name,\n                    own_data=True,\n                    own_columns=True\n                    )\n\n        index_values = self._index.values[order]\n        index_values.flags.writeable = False\n        index = self._index.from_labels(\n                index_values,\n                name=self._index._name\n                )\n        blocks = self._blocks.iloc[order]\n        return self.__class__(blocks,\n                index=index,\n                columns=self._columns,\n                name=self._name,\n                own_data=True,\n                own_index=True\n                )\n\n    def isin(self, other) -> \'Frame\':\n        \'\'\'\n        Return a same-sized Boolean Frame that shows if the same-positioned element is in the iterable passed to the function.\n        \'\'\'\n        array = isin(self.values, other)\n        return self.__class__(array, columns=self._columns, index=self._index)\n\n    @doc_inject(class_name=\'Frame\')\n    def clip(self, *,\n            lower=None,\n            upper=None,\n            axis: tp.Optional[int] = None):\n        \'\'\'{}\n\n        Args:\n            lower: value, :obj:`static_frame.Series`, :obj:`static_frame.Frame`\n            upper: value, :obj:`static_frame.Series`, :obj:`static_frame.Frame`\n            axis: required if ``lower`` or ``upper`` are given as a :obj:`static_frame.Series`.\n        \'\'\'\n        if lower is None and upper is None:\n            return self.__class__(self._blocks.copy(),\n                    index=self._index,\n                    columns=self._columns,\n                    own_data=True,\n                    name=self._name\n                    )\n\n        args = [lower, upper]\n        for idx, arg in enumerate(args):\n            if arg is None:\n                continue\n            bound = -np.inf if idx == 0 else np.inf\n            if isinstance(arg, Series):\n                if axis is None:\n                    raise RuntimeError(\'cannot use a Series argument without specifying an axis\')\n                target = self._index if axis == 0 else self._columns\n                values = arg.reindex(target).fillna(bound).values\n                if axis == 0: # duplicate the same column over the width\n                    args[idx] = np.vstack([values] * self.shape[1]).T\n                else:\n                    args[idx] = np.vstack([values] * self.shape[0])\n            elif isinstance(arg, Frame):\n                args[idx] = arg.reindex(\n                        index=self._index,\n                        columns=self._columns).fillna(bound).values\n            elif hasattr(arg, \'__iter__\'):\n                raise RuntimeError(\'only Series or Frame are supported as iterable lower/upper arguments\')\n            # assume single value otherwise, no change necessary\n\n        array = np.clip(self.values, *args)\n        array.flags.writeable = False\n\n        return self.__class__(array,\n                columns=self._columns,\n                index=self._index,\n                name=self._name\n                )\n\n\n    def transpose(self) -> \'Frame\':\n        \'\'\'Transpose. Return a :obj:`static_frame.Frame` with ``index`` as ``columns`` and vice versa.\n        \'\'\'\n        return self.__class__(self._blocks.transpose(),\n                index=self._columns,\n                columns=self._index,\n                own_data=True,\n                name=self._name)\n\n    @property\n    def T(self) -> \'Frame\':\n        \'\'\'Transpose. Return a :obj:`static_frame.Frame` with ``index`` as ``columns`` and vice versa.\n        \'\'\'\n        return self.transpose()\n\n    @doc_inject(selector=\'duplicated\')\n    def duplicated(self, *,\n            axis=0,\n            exclude_first=False,\n            exclude_last=False) -> \'Series\':\n        \'\'\'\n        Return an axis-sized Boolean Series that shows True for all rows (axis 0) or columns (axis 1) duplicated.\n\n        Args:\n            {axis}\n            {exclude_first}\n            {exclude_last}\n        \'\'\'\n        # TODO: might be able to do this witnout calling .values and passing in TypeBlocks, but TB needs to support roll\n        duplicates = array_to_duplicated(self.values,\n                axis=axis,\n                exclude_first=exclude_first,\n                exclude_last=exclude_last)\n        duplicates.flags.writeable = False\n        if axis == 0: # index is index\n            return Series(duplicates, index=self._index)\n        return Series(duplicates, index=self._columns)\n\n    @doc_inject(selector=\'duplicated\')\n    def drop_duplicated(self, *,\n            axis=0,\n            exclude_first: bool = False,\n            exclude_last: bool = False\n            ) -> \'Frame\':\n        \'\'\'\n        Return a Frame with duplicated rows (axis 0) or columns (axis 1) removed. All values in the row or column are compared to determine duplication.\n\n        Args:\n            {axis}\n            {exclude_first}\n            {exclude_last}\n        \'\'\'\n        # NOTE: can avoid calling .vaalues with extensions to TypeBlocks\n        duplicates = array_to_duplicated(self.values,\n                axis=axis,\n                exclude_first=exclude_first,\n                exclude_last=exclude_last)\n\n        if not duplicates.any():\n            return self.__class__(\n                    self._blocks.copy(),\n                    index=self._index,\n                    columns=self._columns,\n                    own_data=True,\n                    own_index=True,\n                    name=self._name)\n\n        keep = ~duplicates\n\n        if axis == 0: # return rows with index indexed\n            return self.__class__(\n                    self.values[keep],\n                    index=self._index[keep],\n                    columns=self._columns,\n                    own_index=True,\n                    name=self._name\n                    )\n        elif axis == 1:\n            return self.__class__(\n                    self.values[:, keep],\n                    index=self._index,\n                    columns=self._columns[keep],\n                    own_index=True,\n                    name=self._name\n                    )\n        # invalid axis will raise in array_to_duplicated\n\n    def set_index(self,\n            column: GetItemKeyType,\n            *,\n            drop: bool = False,\n            index_constructor=Index) -> \'Frame\':\n        \'\'\'\n        Return a new frame produced by setting the given column as the index, optionally removing that column from the new Frame.\n        \'\'\'\n        column_iloc = self._columns.loc_to_iloc(column)\n\n        if drop:\n            blocks = TypeBlocks.from_blocks(\n                    self._blocks._drop_blocks(column_key=column_iloc))\n            columns = self._columns._drop_iloc(column_iloc)\n            own_data = True\n            own_columns = True\n        else:\n            blocks = self._blocks\n            columns = self._columns\n            own_data = False\n            own_columns = False\n\n        index_values = self._blocks._extract_array(column_key=column_iloc)\n        index = index_constructor(index_values, name=column)\n\n        return self.__class__(blocks,\n                columns=columns,\n                index=index,\n                own_data=own_data,\n                own_columns=own_columns,\n                own_index=True,\n                name=self._name\n                )\n\n    def set_index_hierarchy(self,\n            columns: GetItemKeyType,\n            *,\n            drop: bool = False,\n            index_constructors: tp.Optional[IndexConstructors] = None,\n            reorder_for_hierarchy: bool = False,\n            ) -> \'Frame\':\n        \'\'\'\n        Given an iterable of column labels, return a new ``Frame`` with those columns as an ``IndexHierarchy`` on the index.\n\n        Args:\n            columns: Iterable of column labels.\n            drop: Boolean to determine if selected columns should be removed from the data.\n            index_constructors: Optionally provide a sequence of ``Index`` constructors, of length equal to depth, to be used in converting columns Index components in the ``IndexHierarchy``.\n            reorder_for_hierarchy: reorder the rows to produce a hierarchible Index from the selected columns, assuming hierarchability is possible.\n\n        Returns:\n            :obj:`Frame`\n        \'\'\'\n        if isinstance(columns, tuple):\n            column_loc = list(columns)\n            column_name = columns\n        else:\n            column_loc = columns\n            column_name = None # could be a slice, must get post iloc conversion\n\n        column_iloc = self._columns.loc_to_iloc(column_loc)\n\n        if column_name is None:\n            column_name = tuple(self._columns.values[column_iloc])\n\n        # index_labels = self._blocks._extract_array(column_key=column_iloc)\n        index_labels = self._blocks._extract(column_key=column_iloc)\n\n        if reorder_for_hierarchy:\n            index, order_lex = rehierarch_from_type_blocks(\n                    labels=index_labels,\n                    depth_map=range(index_labels.shape[1]), # keep order\n                    index_cls=IndexHierarchy,\n                    index_constructors=index_constructors,\n                    name=column_name,\n                    )\n            blocks_src = self._blocks._extract(row_key=order_lex)\n        else:\n            index = IndexHierarchy._from_type_blocks(\n                    index_labels,\n                    index_constructors=index_constructors,\n                    name=column_name,\n                    own_blocks=True,\n                    )\n            blocks_src = self._blocks\n\n\n        if drop:\n            blocks = TypeBlocks.from_blocks(\n                    blocks_src._drop_blocks(column_key=column_iloc))\n            columns = self._columns._drop_iloc(column_iloc)\n            own_data = True\n            own_columns = True\n        else:\n            blocks = blocks_src\n            columns = self._columns\n            own_data = False\n            own_columns = False\n\n\n        return self.__class__(blocks,\n                columns=columns,\n                index=index,\n                own_data=own_data,\n                own_columns=own_columns,\n                own_index=True,\n                name=self._name\n                )\n\n    def unset_index(self, *,\n            names: tp.Iterable[tp.Hashable] = EMPTY_TUPLE,\n            # index_column_first: tp.Optional[tp.Union[int, str]] = 0,\n            consolidate_blocks: bool = False\n            ) -> \'Frame\':\n        \'\'\'\n        Return a new ``Frame`` where the index is added to the front of the data, and an ``IndexAutoFactory`` is used to populate a new index. If the ``Index`` has a ``name``, that name will be used for the column name, otherwise a suitable default will be used. As underlying NumPy arrays are immutable, data is not copied.\n\n        Args:\n            names: An iterable of hashables to be used to name the unset index. If an ``Index``, a single hashable should be provided; if an ``IndexHierarchy``, as many hashables as the depth must be provided.\n        \'\'\'\n\n        def blocks():\n            yield self.index.values # 2D immutable array\n            for b in self._blocks._blocks:\n                yield b\n\n        if consolidate_blocks:\n            block_gen = lambda: TypeBlocks.consolidate_blocks(blocks())\n        else:\n            block_gen = blocks\n\n        if self._columns.depth > 1:\n            raise ErrorInitFrame(\'cannot unset index with a columns with depth greater than 1\')\n\n        if names:\n            columns = chain(names, self._columns.values)\n        else:\n            columns = chain(self._index.names, self._columns.values)\n\n        return self.__class__(\n                TypeBlocks.from_blocks(block_gen()),\n                columns=columns,\n                index=None,\n                own_data=True,\n                )\n\n    def __round__(self, decimals: int = 0) -> \'Frame\':\n        \'\'\'\n        Return a :obj:`Frame` rounded to the given decimals. Negative decimals round to the left of the decimal point.\n\n        Args:\n            decimals: number of decimals to round to.\n\n        Returns:\n            :obj:`Frame`\n        \'\'\'\n        return self.__class__(\n                self._blocks.__round__(decimals=decimals),\n                index=self._index,\n                columns=self._columns,\n                name=self._name,\n                own_data=True,\n                own_index=True,\n                )\n\n    def roll(self,\n            index: int = 0,\n            columns: int = 0,\n            include_index: bool = False,\n            include_columns: bool = False) -> \'Frame\':\n        \'\'\'\n        Args:\n            include_index: Determine if index is included in index-wise rotation.\n            include_columns: Determine if column index is included in index-wise rotation.\n        \'\'\'\n        shift_index = index\n        shift_column = columns\n\n        blocks = TypeBlocks.from_blocks(\n                self._blocks._shift_blocks(\n                row_shift=shift_index,\n                column_shift=shift_column,\n                wrap=True\n                ))\n\n        if include_index:\n            index = self._index.roll(shift_index)\n            own_index = True\n        else:\n            index = self._index\n            own_index = False\n\n        if include_columns:\n            columns = self._columns.roll(shift_column)\n            own_columns = True\n        else:\n            columns = self._columns\n            own_columns = False\n\n        return self.__class__(blocks,\n                columns=columns,\n                index=index,\n                name=self._name,\n                own_data=True,\n                own_columns=own_columns,\n                own_index=own_index,\n                )\n\n    def shift(self,\n            index: int = 0,\n            columns: int = 0,\n            fill_value=np.nan) -> \'Frame\':\n\n        shift_index = index\n        shift_column = columns\n\n        blocks = TypeBlocks.from_blocks(\n                self._blocks._shift_blocks(\n                row_shift=shift_index,\n                column_shift=shift_column,\n                wrap=False,\n                fill_value=fill_value\n                ))\n\n        return self.__class__(blocks,\n                columns=self._columns,\n                index=self._index,\n                name=self._name,\n                own_data=True,\n                )\n\n    #---------------------------------------------------------------------------\n    # transformations resulting in changed dimensionality\n\n    @doc_inject(selector=\'head\', class_name=\'Frame\')\n    def head(self, count: int = 5) -> \'Frame\':\n        \'\'\'{doc}\n\n        Args:\n            {count}\n        \'\'\'\n        return self.iloc[:count]\n\n    @doc_inject(selector=\'tail\', class_name=\'Frame\')\n    def tail(self, count: int = 5) -> \'Frame\':\n        \'\'\'{doc}\n\n        Args:\n            {count}\n        \'\'\'\n        return self.iloc[-count:]\n\n    @doc_inject(selector=\'argminmax\')\n    def loc_min(self, *,\n            skipna: bool = True,\n            axis: int = 0\n            ) -> Series:\n        \'\'\'\n        Return the labels corresponding to the minimum value found.\n\n        Args:\n            {skipna}\n            {axis}\n        \'\'\'\n        # this operation is not composable for axis 1; cannot use _ufunc_axis_skipna interface as do not have out argument, and need to determine returned dtype in advance\n\n        # if this has NaN we cannot get a loc\n        post = argmin_2d(self.values, skipna=skipna, axis=axis)\n        if post.dtype == DTYPE_FLOAT_DEFAULT:\n            raise RuntimeError(\'cannot produce loc representation from NaNs\')\n\n        # post has been made immutable so Series will own\n        if axis == 0:\n            return Series(\n                    self.index.values[post],\n                    index=immutable_index_filter(self._columns)\n                    )\n        return Series(self.columns.values[post], index=self._index)\n\n    @doc_inject(selector=\'argminmax\')\n    def iloc_min(self, *,\n            skipna: bool = True,\n            axis: int = 0\n            ) -> Series:\n        \'\'\'\n        Return the integer indices corresponding to the minimum values found.\n\n        Args:\n            {skipna}\n            {axis}\n        \'\'\'\n        # if this has NaN can continue\n        post = argmin_2d(self.values, skipna=skipna, axis=axis)\n        post.flags.writeable = False\n        if axis == 0:\n            return Series(post, index=immutable_index_filter(self._columns))\n        return Series(post, index=self._index)\n\n    @doc_inject(selector=\'argminmax\')\n    def loc_max(self, *,\n            skipna: bool = True,\n            axis: int = 0\n            ) -> Series:\n        \'\'\'\n        Return the labels corresponding to the maximum values found.\n\n        Args:\n            {skipna}\n            {axis}\n        \'\'\'\n        # if this has NaN we cannot get a loc\n        post = argmax_2d(self.values, skipna=skipna, axis=axis)\n        if post.dtype == DTYPE_FLOAT_DEFAULT:\n            raise RuntimeError(\'cannot produce loc representation from NaNs\')\n\n        if axis == 0:\n            return Series(\n                    self.index.values[post],\n                    index=immutable_index_filter(self._columns)\n                    )\n        return Series(self.columns.values[post], index=self._index)\n\n    @doc_inject(selector=\'argminmax\')\n    def iloc_max(self, *,\n            skipna: bool = True,\n            axis: int = 0\n            ) -> Series:\n        \'\'\'\n        Return the integer indices corresponding to the maximum values found.\n\n        Args:\n            {skipna}\n            {axis}\n        \'\'\'\n        # if this has NaN can continue\n        post = argmax_2d(self.values, skipna=skipna, axis=axis)\n        post.flags.writeable = False\n        if axis == 0:\n            return Series(post, index=immutable_index_filter(self._columns))\n        return Series(post, index=self._index)\n\n    def pivot(self,\n            index_fields: KeyOrKeys,\n            columns_fields: KeyOrKeys = EMPTY_TUPLE,\n            data_fields: KeyOrKeys = EMPTY_TUPLE,\n            *,\n            func: CallableOrCallableMap = None,\n            fill_value: object = FILL_VALUE_DEFAULT,\n            ) -> \'Frame\':\n        \'\'\'\n        Produce a pivot table, where one or more columns is selected for each of index_fields, columns_fields, and data_fields. Unique values from the provided ``index_fields`` will be used to create a new index; unique values from the provided ``columns_fields`` will be used to create a new columns; if one ``data_fields`` value is selected, that is the value that will be displayed; if more than one values is given, those values will be presented with a hierarchical index on the columns; if not ``data_fields`` ar provided, all unused fields will be displayed.\n\n        Args:\n            index_fields\n            columns_fields\n            data_fields\n            fill_value: If the index expansion produces coordinates that have no existing data value, fill that position with this value.\n            func: function to apply to ``data_fields``, or a dictionary of labelled functions to apply to data fields, producing an additional hierarchical level.\n        \'\'\'\n        if func is None:\n            # form the equivalent Series function for summing\n            func = partial(ufunc_axis_skipna,\n                    skipna=True,\n                    axis=0,\n                    ufunc=np.sum,\n                    ufunc_skipna=np.nansum\n                    )\n            func_map = ((\'\', func),)\n        elif callable(func):\n            func_map = ((\'\', func),) # store iterable of pairs\n        else:\n            func_map = tuple(func.items())\n\n        index_fields = key_normalize(index_fields)\n        columns_fields = key_normalize(columns_fields)\n        data_fields = key_normalize(data_fields)\n\n        if not data_fields:\n            used = set(chain(index_fields, columns_fields))\n            data_fields = [x for x in self.columns if x not in used]\n            if not data_fields:\n                raise ErrorInitFrame(\'no fields remain to populate data.\')\n\n        idx_start_columns = len(index_fields)\n\n        # Take fields_group before extending columns with values\n        fields_group = index_fields + columns_fields\n        for field in fields_group:\n            if field not in self._columns:\n                raise ErrorInitFrame(f\'cannot create a pivot Frame from a field ({field}) that is not a column\')\n\n        if idx_start_columns == 1:\n            # reduce loc to single itme to get 1D array\n            index_loc = index_fields[0]\n        else:\n            index_loc = index_fields\n        # will return np.ndarray or frozen set\n        index_values = ufunc_unique(\n                self._blocks._extract_array(\n                        column_key=self._columns.loc_to_iloc(index_loc)),\n                axis=0)\n\n        if idx_start_columns == 1:\n            index = Index(index_values, name=index_fields[0])\n        else:\n            index = IndexHierarchy.from_labels(index_values, name=tuple(index_fields))\n\n        # Colect bundle of values for from_product constrcution if columns.\n        columns_product = []\n        for field in columns_fields:\n            # Take one at a time\n            columns_values = ufunc_unique(\n                    self._blocks._extract_array(column_key=self._columns.loc_to_iloc(field)))\n            columns_product.append(columns_values)\n\n        # For data fields, we add the field name, not the field values, to the columns.\n        columns_name = tuple(columns_fields)\n        if len(data_fields) > 1 or not columns_fields:\n            # if no columns fields, have to add values fields\n            columns_product.append(data_fields)\n            columns_name = tuple(chain(*columns_fields, (\'values\',)))\n\n        if len(func_map) > 1:\n            # add the labels as another product level\n            labels = tuple(x for x, _ in func_map)\n            columns_product.append(labels)\n            columns_name = columns_name + (\'func\',)\n\n        if len(columns_product) > 1:\n            columns = self._COLUMNS_HIERARCHY_CONSTRUCTOR.from_product(\n                    *columns_product,\n                    name=columns_name)\n        else:\n            columns = self._COLUMNS_CONSTRUCTOR(\n                    columns_product[0],\n                    name=columns_name[0])\n\n        def items():\n            func_single = func_map[0][1] if len(func_map) == 1 else None\n\n            for group, sub in self.iter_group_items(fields_group):\n                index_label = group[:idx_start_columns]\n                index_label = tuple(index_label) if len(index_label) > 1 else index_label[0]\n\n                # get the found parts of the columns labels from the group; this will never have data_fields\n                columns_label_raw = group[idx_start_columns:]\n\n                if len(columns_label_raw) == 0:\n                    # if none, it means we just have data fields\n                    columns_labels = data_fields\n                elif len(columns_label_raw) == 1 and len(data_fields) == 1:\n                    # only one data field, do not need to display it\n                    columns_labels = repeat(columns_label_raw[0])\n                elif len(columns_label_raw) > 1 and len(data_fields) == 1:\n                    # only one data field\n                    columns_labels = repeat(tuple(columns_label_raw))\n                elif len(columns_label_raw) >= 1 and len(data_fields) > 1:\n                    # create column labels that has an entry for each data field\n                    if len(columns_label_raw) == 1:\n                        columns_labels = ((columns_label_raw[0], v) for v in data_fields)\n                    else:\n                        columns_labels = (tuple(chain(columns_label_raw, (v,))) for v in data_fields)\n\n                for field, column_label in zip(data_fields, columns_labels):\n                    # NOTE: sub[field] produces a Series, which is not needed; better to go to blocks and extract array\n                    if func_single:\n                        yield (index_label, column_label), func_single(sub[field].values)\n                    else:\n                        for label, func in func_map:\n                            if isinstance(column_label, tuple):\n                                column_label_final = column_label + (label,)\n                            else: # a single hashable\n                                column_label_final = (column_label, label)\n                            yield (index_label, column_label_final), func(sub[field].values)\n\n        return self.__class__.from_element_loc_items(\n                items(),\n                index=index,\n                columns=columns,\n                dtype=None, # supply if possible,\n                fill_value=fill_value,\n                own_index=True,\n                own_columns=True\n                )\n\n    #---------------------------------------------------------------------------\n    def _join(self,\n            other: \'Frame\', # support a named Series as a 1D frame?\n            *,\n            index_source: Join, # intersect, left, right, union,\n            left_depth_level: tp.Optional[DepthLevelSpecifier] = None,\n            left_columns: GetItemKeyType = None,\n            right_depth_level: tp.Optional[DepthLevelSpecifier] = None,\n            right_columns: GetItemKeyType = None,\n            left_template: str = \'{}\',\n            right_template: str = \'{}\',\n            fill_value: tp.Any = np.nan,\n            composite_index: bool = True,\n            composite_index_fill_value: tp.Hashable = None,\n            ):\n\n        left_index = self.index\n        right_index = other.index\n\n        #-----------------------------------------------------------------------\n        # find matches\n\n        # for now we reduce the targets to arrays; possible coercion in some cases, but seems inevitable as we will be doing row-wise comparisons\n        target_left = TypeBlocks.from_blocks(\n                arrays_from_index_frame(self, left_depth_level, left_columns)).values\n        target_right = TypeBlocks.from_blocks(\n                arrays_from_index_frame(other, right_depth_level, right_columns)).values\n\n        if target_left.shape[1] != target_right.shape[1]:\n            raise RuntimeError(\'left and right selections must be the same width.\')\n\n        # Find matching pairs. Get iloc of left to iloc of right.\n        # If composite_index is True, is_many is True, if False, need to check if it is possible to not havea composite index.\n        is_many = composite_index # one to many or many to many\n\n        map_iloc = {}\n        seen = set()\n\n        # NOTE: this could be optimized by always iterating over the shorter target\n        for idx_left, row_left in enumerate(target_left):\n            # Get 1D vector showing matches along right\'s full heigh\n            matched = row_left == target_right\n            if matched is False:\n                continue\n            matched = matched.all(axis=1)\n            if not matched.any():\n                continue\n            matched_idx = np.flatnonzero(matched)\n\n            if not is_many:\n                if len(matched_idx) > 1:\n                    is_many = True\n                elif len(matched_idx) == 1:\n                    if matched_idx[0] in seen:\n                        is_many = True\n                    seen.add(matched_idx[0])\n\n            map_iloc[idx_left] = matched_idx\n\n        if not composite_index and is_many:\n            raise RuntimeError(\'A composite index is required in this join.\')\n\n        #-----------------------------------------------------------------------\n        # store collections of matches, derive final index\n\n        left_loc_set = set()\n        right_loc_set = set()\n        many_loc = []\n        many_iloc = []\n\n        cifv = composite_index_fill_value\n\n        # NOTE: doing selection and using iteration (from set, and with zip, below) reduces chances for type coercion in IndexHierarchy\n        left_loc = left_index[list(map_iloc.keys())]\n\n        for (k, v), left_loc_element in zip(map_iloc.items(), left_loc):\n            left_loc_set.add(left_loc_element)\n            # right at v is an array\n            right_loc_part = right_index[v] # iloc to loc\n            right_loc_set.update(right_loc_part)\n\n            if is_many:\n                many_loc.extend(Pair(p) for p in product((left_loc_element,), right_loc_part))\n                many_iloc.extend(Pair(p) for p in product((k,), v))\n\n        if index_source is Join.INNER:\n            if is_many:\n                final_index = Index(many_loc)\n            else: # just those matched from the left, which are also on right\n                final_index = Index(left_loc)\n        elif index_source is Join.LEFT:\n            if is_many:\n                extend = (PairLeft((x, cifv))\n                        for x in left_index if x not in left_loc_set)\n                # What if we are extending an index that already has a tuple\n                final_index = Index(chain(many_loc, extend))\n            else:\n                final_index = left_index\n        elif index_source is Join.RIGHT:\n            if is_many:\n                extend = (PairRight((cifv, x))\n                        for x in right_index if x not in right_loc_set)\n                final_index = Index(chain(many_loc, extend))\n            else:\n                final_index = right_index\n        elif index_source is Join.OUTER:\n            extend_left = (PairLeft((x, cifv))\n                    for x in left_index if x not in left_loc_set)\n            extend_right = (PairRight((cifv, x))\n                    for x in right_index if x not in right_loc_set)\n            if is_many:\n                # must revese the many_loc so as to preserent right id first\n                final_index = Index(chain(many_loc, extend_left, extend_right))\n            else:\n                final_index = left_index.union(right_index)\n        else:\n            raise NotImplementedError(f\'index source must be one of {tuple(Join)}\')\n\n        #-----------------------------------------------------------------------\n        # construct final frame\n\n        if not is_many:\n            final = FrameGO(index=final_index)\n            left_columns = (left_template.format(c) for c in self.columns)\n            final.extend(self.relabel(columns=left_columns), fill_value=fill_value)\n            # build up a Series for each new column\n            for idx_col, col in enumerate(other.columns):\n                values = []\n                for loc in final_index:\n                    # what if loc is in both left and rihgt?\n                    if loc in left_index and left_index.loc_to_iloc(loc) in map_iloc:\n                        iloc = map_iloc[left_index.loc_to_iloc(loc)]\n                        assert len(iloc) == 1 # not is_many, so all have to be length 1\n                        values.append(other.iloc[iloc[0], idx_col])\n                    elif loc in right_index:\n                        values.append(other.loc[loc, col])\n                    else:\n                        values.append(fill_value)\n                final[right_template.format(col)] = values\n            return final.to_frame()\n\n        # From here, is_many is True\n        row_key = []\n        final_index_left = []\n        for p in final_index:\n            if p.__class__ is Pair: # in both\n                iloc = left_index.loc_to_iloc(p[0])\n                row_key.append(iloc)\n                final_index_left.append(p)\n            elif p.__class__ is PairLeft:\n                row_key.append(left_index.loc_to_iloc(p[0]))\n                final_index_left.append(p)\n\n        # extract potentially repeated rows\n        tb = self._blocks._extract(row_key=row_key)\n        left_columns = (left_template.format(c) for c in self.columns)\n\n        final = FrameGO(tb,\n                index=Index(final_index_left),\n                columns=left_columns,\n                own_data=True)\n\n        # only do this if we have PairRight above\n        if len(final_index_left) < len(final_index):\n            final = final.reindex(final_index, fill_value=fill_value)\n\n        # populate from right columns\n        for idx_col, col in enumerate(other.columns):\n            values = []\n            for pair in final_index:\n                if isinstance(pair, Pair):\n                    loc_left, loc_right = pair\n                    if pair.__class__ is PairRight: # get from right\n                        values.append(other.loc[loc_right, col])\n                    elif pair.__class__ is PairLeft:\n                        # get from left, but we do not have col, so fill value\n                        values.append(fill_value)\n                    else: # is this case needed?\n                        values.append(other.loc[loc_right, col])\n                else:\n                    values.append(fill_value)\n            final[right_template.format(col)] = values\n        return final.to_frame()\n\n\n    @doc_inject(selector=\'join\')\n    def join_inner(self,\n            other: \'Frame\', # support a named Series as a 1D frame?\n            *,\n            left_depth_level: tp.Optional[DepthLevelSpecifier] = None,\n            left_columns: GetItemKeyType = None,\n            right_depth_level: tp.Optional[DepthLevelSpecifier] = None,\n            right_columns: GetItemKeyType = None,\n            left_template: str = \'{}\',\n            right_template: str = \'{}\',\n            fill_value: tp.Any = np.nan,\n            composite_index: bool = True,\n            composite_index_fill_value: tp.Hashable = None,\n            ):\n        \'\'\'\n        Perform an inner join.\n\n        Args:\n            {left_depth_level}\n            {left_columns}\n            {right_depth_level}\n            {right_columns}\n            {left_template}\n            {right_template}\n            {fill_value}\n            {composite_index}\n            {composite_index_fill_value}\n\n        Returns:\n            :obj:`Frame`\n        \'\'\'\n        return self._join(other=other,\n                index_source=Join.INNER,\n                left_depth_level=left_depth_level,\n                left_columns=left_columns,\n                right_depth_level=right_depth_level,\n                right_columns=right_columns,\n                left_template=left_template,\n                right_template=right_template,\n                fill_value=fill_value,\n                composite_index=composite_index,\n                composite_index_fill_value=composite_index_fill_value,\n                )\n\n    @doc_inject(selector=\'join\')\n    def join_left(self,\n            other: \'Frame\', # support a named Series as a 1D frame?\n            *,\n            left_depth_level: tp.Optional[DepthLevelSpecifier] = None,\n            left_columns: GetItemKeyType = None,\n            right_depth_level: tp.Optional[DepthLevelSpecifier] = None,\n            right_columns: GetItemKeyType = None,\n            left_template: str = \'{}\',\n            right_template: str = \'{}\',\n            fill_value: tp.Any = np.nan,\n            composite_index: bool = True,\n            composite_index_fill_value: tp.Hashable = None,\n            ):\n        \'\'\'\n        Perform a left outer join.\n\n        Args:\n            {left_depth_level}\n            {left_columns}\n            {right_depth_level}\n            {right_columns}\n            {left_template}\n            {right_template}\n            {fill_value}\n            {composite_index}\n            {composite_index_fill_value}\n\n        Returns:\n            :obj:`Frame`\n        \'\'\'\n        return self._join(other=other,\n                index_source=Join.LEFT,\n                left_depth_level=left_depth_level,\n                left_columns=left_columns,\n                right_depth_level=right_depth_level,\n                right_columns=right_columns,\n                left_template=left_template,\n                right_template=right_template,\n                fill_value=fill_value,\n                composite_index=composite_index,\n                composite_index_fill_value=composite_index_fill_value,\n                )\n\n    @doc_inject(selector=\'join\')\n    def join_right(self,\n            other: \'Frame\', # support a named Series as a 1D frame?\n            *,\n            left_depth_level: tp.Optional[DepthLevelSpecifier] = None,\n            left_columns: GetItemKeyType = None,\n            right_depth_level: tp.Optional[DepthLevelSpecifier] = None,\n            right_columns: GetItemKeyType = None,\n            left_template: str = \'{}\',\n            right_template: str = \'{}\',\n            fill_value: tp.Any = np.nan,\n            composite_index: bool = True,\n            composite_index_fill_value: tp.Hashable = None,\n            ):\n        \'\'\'\n        Perform a right outer join.\n\n        Args:\n            {left_depth_level}\n            {left_columns}\n            {right_depth_level}\n            {right_columns}\n            {left_template}\n            {right_template}\n            {fill_value}\n            {composite_index}\n            {composite_index_fill_value}\n\n        Returns:\n            :obj:`Frame`\n        \'\'\'\n        return self._join(other=other,\n                index_source=Join.RIGHT,\n                left_depth_level=left_depth_level,\n                left_columns=left_columns,\n                right_depth_level=right_depth_level,\n                right_columns=right_columns,\n                left_template=left_template,\n                right_template=right_template,\n                fill_value=fill_value,\n                composite_index=composite_index,\n                composite_index_fill_value=composite_index_fill_value,\n                )\n\n    @doc_inject(selector=\'join\')\n    def join_outer(self,\n            other: \'Frame\', # support a named Series as a 1D frame?\n            *,\n            left_depth_level: tp.Optional[DepthLevelSpecifier] = None,\n            left_columns: GetItemKeyType = None,\n            right_depth_level: tp.Optional[DepthLevelSpecifier] = None,\n            right_columns: GetItemKeyType = None,\n            left_template: str = \'{}\',\n            right_template: str = \'{}\',\n            fill_value: tp.Any = np.nan,\n            composite_index: bool = True,\n            composite_index_fill_value: tp.Hashable = None,\n            ):\n        \'\'\'\n        Perform an outer join.\n\n        Args:\n            {left_depth_level}\n            {left_columns}\n            {right_depth_level}\n            {right_columns}\n            {left_template}\n            {right_template}\n            {fill_value}\n            {composite_index}\n            {composite_index_fill_value}\n\n        Returns:\n            :obj:`Frame`\n        \'\'\'\n        return self._join(other=other,\n                index_source=Join.OUTER,\n                left_depth_level=left_depth_level,\n                left_columns=left_columns,\n                right_depth_level=right_depth_level,\n                right_columns=right_columns,\n                left_template=left_template,\n                right_template=right_template,\n                fill_value=fill_value,\n                composite_index=composite_index,\n                composite_index_fill_value=composite_index_fill_value,\n                )\n\n    #---------------------------------------------------------------------------\n    def _insert(self,\n            key: int, # iloc positions\n            container: tp.Union[\'Frame\', Series],\n            *,\n            fill_value: tp.Any = np.nan,\n            ) -> \'Frame\':\n        \'\'\'\n        Return a new Frame with the provided container inserted at the position determined by the column key; values existing at that key come after the inserted container.\n        \'\'\'\n        if not isinstance(container, (Series, Frame)):\n            raise NotImplementedError(\n                    f\'No support for inserting with {type(container)}\')\n\n        if not len(container.index): # must be empty data, empty index container\n            return self if self.STATIC else self.copy() # always return a new Frame\n\n        # self\'s index will never change; we only take what aligns in the passed container\n        if not self._index.equals(container._index):\n            container = container.reindex(self._index,\n                    fill_value=fill_value,\n                    check_equals=False,\n                    )\n\n        # NOTE: might introduce coercions in IndexHierarchy\n        labels_prior = self._columns.values\n\n        if isinstance(container, Frame):\n            labels_insert = container.columns.__iter__()\n            if not len(container.columns):\n                return\n            blocks_insert = container._blocks._blocks\n\n        elif isinstance(container, Series):\n            labels_insert = (container.name,)\n            blocks_insert = (container.values,)\n\n        columns = self._columns.__class__.from_labels(chain(\n                labels_prior[:key],\n                labels_insert,\n                labels_prior[key:],\n                ))\n\n        blocks = TypeBlocks.from_blocks(chain(\n                self._blocks._slice_blocks(column_key=slice(0, key)),\n                blocks_insert,\n                self._blocks._slice_blocks(column_key=slice(key, None)),\n                ))\n\n        return self.__class__(blocks,\n                index=self._index,\n                columns=columns,\n                name=self._name,\n                own_data=True,\n                own_columns=True,\n                own_index=True,\n                )\n\n    @doc_inject(selector=\'insert\')\n    def insert_before(self,\n            key: tp.Hashable,\n            container: tp.Union[\'Frame\', Series],\n            *,\n            fill_value: tp.Any = np.nan,\n            ) -> \'Frame\':\n        \'\'\'\n        Create a new :obj:`Frame` by inserting a named :obj:`Series` or :obj:`Frame` at the position before the label specified by ``key``.\n\n        Args:\n            {key_before}\n            {container}\n            {fill_value}\n\n        Returns:\n            :obj:`Frame`\n        \'\'\'\n        iloc_key = self._columns.loc_to_iloc(key)\n        if not isinstance(iloc_key, INT_TYPES):\n            raise RuntimeError(f\'Unsupported key type: {key}\')\n        return self._insert(iloc_key, container, fill_value=fill_value)\n\n    @doc_inject(selector=\'insert\')\n    def insert_after(self,\n            key: tp.Hashable, # iloc positions\n            container: tp.Union[\'Frame\', Series],\n            *,\n            fill_value: tp.Any = np.nan,\n            ) -> \'Frame\':\n        \'\'\'\n        Create a new :obj:`Frame` by inserting a named :obj:`Series` or :obj:`Frame` at the position after the label specified by ``key``.\n\n        Args:\n            {key_after}\n            {container}\n            {fill_value}\n\n        Returns:\n            :obj:`Frame`\n        \'\'\'\n        iloc_key = self._columns.loc_to_iloc(key)\n        if not isinstance(iloc_key, INT_TYPES):\n            raise RuntimeError(f\'Unsupported key type: {key}\')\n        return self._insert(iloc_key + 1, container, fill_value=fill_value)\n\n\n\n\n    #---------------------------------------------------------------------------\n    # utility function to numpy array or other types\n\n    @doc_inject()\n    def equals(self,\n            other: tp.Any,\n            *,\n            compare_name: bool = False,\n            compare_dtype: bool = False,\n            compare_class: bool = False,\n            skipna: bool = True,\n            ) -> bool:\n        \'\'\'\n        {doc}\n\n        Args:\n            {compare_name}\n            {compare_dtype}\n            {compare_class}\n            {skipna}\n        \'\'\'\n        if id(other) == id(self):\n            return True\n\n        if compare_class and self.__class__ != other.__class__:\n            return False\n        elif not isinstance(other, Frame):\n            return False\n\n        if self._blocks.shape != other._blocks.shape:\n            return False\n        if compare_name and self._name != other._name:\n            return False\n\n        # dtype check will happen in TypeBlocks\n        if not self._blocks.equals(other._blocks,\n                compare_dtype=compare_dtype,\n                compare_class=compare_class,\n                skipna=skipna,\n                ):\n            return False\n\n        if not self._index.equals(other._index,\n                compare_name=compare_name,\n                compare_dtype=compare_dtype,\n                compare_class=compare_class,\n                skipna=skipna,\n                ):\n            return False\n\n        if not self._columns.equals(other._columns,\n                compare_name=compare_name,\n                compare_dtype=compare_dtype,\n                compare_class=compare_class,\n                skipna=skipna,\n                ):\n            return False\n\n        return True\n\n\n    def unique(self, axis: tp.Optional[int] = None) -> np.ndarray:\n        \'\'\'\n        Return a NumPy array of unqiue values. If the axis argument is provied, uniqueness is determined by columns or row.\n        \'\'\'\n        return ufunc_unique(self.values, axis=axis)\n\n    #---------------------------------------------------------------------------\n    # exporters\n\n    def to_pairs(self, axis) -> tp.Iterable[\n            tp.Tuple[tp.Hashable, tp.Iterable[tp.Tuple[tp.Hashable, tp.Any]]]]:\n        \'\'\'\n        Return a tuple of major axis key, minor axis key vlaue pairs, where major axis is determined by the axis argument.\n        \'\'\'\n        # TODO: find a common interfave on IndexHierarchy that cna give hashables\n        if isinstance(self._index, IndexHierarchy):\n            index_values = list(array2d_to_tuples(self._index.values))\n        else:\n            index_values = self._index.values\n\n        if isinstance(self._columns, IndexHierarchy):\n            columns_values = list(array2d_to_tuples(self._columns.values))\n        else:\n            columns_values = self._columns.values\n\n        if axis == 1:\n            major = index_values\n            minor = columns_values\n        elif axis == 0:\n            major = columns_values\n            minor = index_values\n        else:\n            raise AxisInvalid(f\'invalid axis: {axis}\')\n\n        return tuple(\n                zip(major, (tuple(zip(minor, v))\n                for v in self._blocks.axis_values(axis))))\n\n    def to_pandas(self) -> \'pandas.DataFrame\':\n        \'\'\'\n        Return a Pandas DataFrame.\n        \'\'\'\n        import pandas\n\n        df = pandas.DataFrame(index=self._index.to_pandas())\n\n        # iter columns to preserve types\n        # use integer columns for initial loading\n        for i, array in enumerate(self._blocks.axis_values(0)):\n            df[i] = array\n\n        df.columns = self._columns.to_pandas()\n\n        if \'name\' not in df.columns and self._name is not None:\n            df.name = self._name\n        return df\n\n    def to_arrow(self,\n            *,\n            include_index: bool = True,\n            include_columns: bool = True,\n            ) -> \'pyarrow.Table\':\n        \'\'\'\n        Return a ``pyarrow.Table`` from this :obj:`Frame`.\n        \'\'\'\n        import pyarrow\n        from static_frame.core.store import Store\n\n        field_names, _ = Store.get_field_names_and_dtypes(\n                frame=self,\n                include_index=include_index,\n                include_columns=include_columns,\n                force_str_names=True\n                )\n        arrays = tuple(Store.get_column_iterator(\n                frame=self,\n                include_index=include_index)\n                )\n        # field_names have to be strings\n        return pyarrow.Table.from_arrays(arrays, names=field_names)\n\n\n    def to_parquet(self,\n            fp: PathSpecifier,\n            *,\n            include_index: bool = True,\n            include_columns: bool = True,\n            ) -> None:\n        \'\'\'\n        Write an Arrow Parquet binary file.\n        \'\'\'\n        import pyarrow.parquet as pq\n\n        table = self.to_arrow(\n                include_index=include_index,\n                include_columns=include_columns\n                )\n        fp = path_filter(fp)\n        pq.write_table(table, fp)\n\n    def to_xarray(self) -> \'Dataset\':\n        \'\'\'\n        Return an xarray Dataset.\n\n        In order to preserve columnar types, and following the precedent of Pandas, the :obj:`Frame`, with a 1D index, is translated as a Dataset of 1D arrays, where each DataArray is a 1D array. If the index is an :obj:`IndexHierarhcy`, each column is mapped into an ND array of shape equal to the unique values found at each depth of the index.\n        \'\'\'\n        import xarray\n\n        columns = self.columns\n        index = self.index\n\n        if index.depth == 1:\n            index_name = index.names[0]\n            coords = {index_name: index.values}\n        else:\n            index_name = index.names\n            # index values are reduced to unique values for 2d presentation\n            coords = {index_name[d]: ufunc_unique(index.values_at_depth(d))\n                    for d in range(index.depth)}\n            # create dictionary version\n            coords_index = {k: Index(v) for k, v in coords.items()}\n\n        # columns form the keys in data_vars dict\n        if columns.depth == 1:\n            columns_values = columns.values\n            # needs to be called with axis argument\n            columns_arrays = partial(self._blocks.axis_values, axis=0)\n        else: # must be hashable\n            columns_values = array2d_to_tuples(columns.values)\n\n            def columns_arrays() -> tp.Iterator[np.ndarray]:\n                for c in self.iter_series(axis=0):\n                    # dtype must be able to accomodate a float NaN\n                    resolved = resolve_dtype(c.dtype, DTYPE_FLOAT_DEFAULT)\n                    # create multidimensional arsdfray of all axis for each\n                    array = np.full(\n                            shape=[len(coords[v]) for v in coords],\n                            fill_value=np.nan,\n                            dtype=resolved)\n\n                    for index_labels, value in c.items():\n                        # translate to index positions\n                        insert_pos = [coords_index[k].loc_to_iloc(label)\n                                for k, label in zip(coords, index_labels)]\n                        # must convert to tuple to give position per dimension\n                        array[tuple(insert_pos)] = value\n\n                    yield array\n\n        data_vars = {k: (index_name, v)\n                for k, v in zip(columns_values, columns_arrays())}\n\n        return xarray.Dataset(data_vars, coords=coords)\n\n    def to_frame(self) -> \'Frame\':\n        \'\'\'\n        Return Frame version of this Frame, which is (as the Frame is immutable) is self.\n        \'\'\'\n        return self\n\n    def to_frame_go(self) -> \'FrameGO\':\n        \'\'\'\n        Return a FrameGO view of this Frame. As underlying data is immutable, this is a no-copy operation.\n        \'\'\'\n        # copying blocks does not copy underlying data\n        return FrameGO(\n                self._blocks.copy(),\n                index=self.index, # can reuse\n                columns=self.columns,\n                columns_constructor=self.columns._MUTABLE_CONSTRUCTOR,\n                name=self._name,\n                own_data=True,\n                own_index=True,\n                own_columns=False # need to make grow only\n                )\n\n    def to_delimited(self,\n            fp: PathSpecifierOrFileLike,\n            *,\n            delimiter: str,\n            include_index: bool = True,\n            include_columns: bool = True,\n            encoding: tp.Optional[str] = None,\n            line_terminator: str = \'\\n\',\n            store_filter: tp.Optional[StoreFilter] = STORE_FILTER_DEFAULT\n            ):\n        \'\'\'\n        Given a file path or file-like object, write the Frame as delimited text.\n\n        Args:\n            delimiter: character to be used for delimiterarating elements.\n        \'\'\'\n        fp = path_filter(fp)\n\n        if isinstance(fp, str):\n            f = open(fp, \'w\', encoding=encoding)\n            is_file = True\n        else:\n            f = fp # assume an open file like\n            is_file = False\n\n        index = self._index\n        columns = self._columns\n\n        if include_index:\n            index_values = index.values # get once for caching\n            index_names = index.names # normalized presentation\n\n        if store_filter:\n            filter_func = store_filter.from_type_filter_element\n\n        try: # manage finally closing of file\n            if include_columns:\n                if columns.depth == 1:\n                    columns_rows = (columns,)\n                else:\n                    columns_rows = columns.values.T\n                for row_idx, columns_row in enumerate(columns_rows):\n                    if include_index:\n                        for name in index_names:\n                            if row_idx == 0:\n                                f.write(f\'{name}{delimiter}\')\n                            else:\n                                f.write(f\'{delimiter}\')\n                    if store_filter:\n                        f.write(delimiter.join(f\'{filter_func(x)}\' for x in columns_row))\n                    else:\n                        f.write(delimiter.join(f\'{x}\' for x in columns_row))\n                    f.write(line_terminator)\n\n            col_idx_last = self._blocks._shape[1] - 1\n            # avoid row creation to avoid joining types; avoide creating a list for each row\n            row_current_idx = None\n            for (row_idx, col_idx), element in self._iter_element_iloc_items():\n                if row_idx != row_current_idx:\n                    if row_current_idx is not None:\n                        f.write(line_terminator)\n                    if include_index:\n                        if index.depth == 1:\n                            index_value = index_values[row_idx]\n                            if store_filter:\n                                f.write(f\'{filter_func(index_value)}{delimiter}\')\n                            else:\n                                f.write(f\'{index_value}{delimiter}\')\n                        else:\n                            for index_value in index_values[row_idx]:\n                                if store_filter:\n                                    f.write(f\'{filter_func(index_value)}{delimiter}\')\n                                else:\n                                    f.write(f\'{index_value}{delimiter}\')\n\n                    row_current_idx = row_idx\n                if store_filter:\n                    f.write(f\'{filter_func(element)}\')\n                else:\n                    f.write(f\'{element}\')\n                if col_idx != col_idx_last:\n                    f.write(delimiter)\n        finally:\n            if is_file:\n                f.close()\n\n    def to_csv(self,\n            fp: PathSpecifierOrFileLike,\n            *,\n            include_index: bool = True,\n            include_columns: bool = True,\n            encoding: tp.Optional[str] = None,\n            line_terminator: str = \'\\n\',\n            store_filter: tp.Optional[StoreFilter] = STORE_FILTER_DEFAULT\n            ):\n        \'\'\'\n        Given a file path or file-like object, write the Frame as tab-delimited text.\n        \'\'\'\n        return self.to_delimited(fp=fp,\n                delimiter=\',\',\n                include_index=include_index,\n                include_columns=include_columns,\n                encoding=encoding,\n                line_terminator=line_terminator,\n                store_filter=store_filter\n                )\n\n    def to_tsv(self,\n            fp: PathSpecifierOrFileLike,\n            *,\n            include_index: bool = True,\n            include_columns: bool = True,\n            encoding: tp.Optional[str] = None,\n            line_terminator: str = \'\\n\',\n            store_filter: tp.Optional[StoreFilter] = STORE_FILTER_DEFAULT\n            ):\n        \'\'\'\n        Given a file path or file-like object, write the Frame as tab-delimited text.\n        \'\'\'\n        return self.to_delimited(fp=fp,\n                delimiter=\'\\t\',\n                include_index=include_index,\n                include_columns=include_columns,\n                encoding=encoding,\n                line_terminator=line_terminator,\n                store_filter=store_filter\n                )\n\n\n    def to_xlsx(self,\n            fp: PathSpecifier, # not sure I can take a file like yet\n            *,\n            label: tp.Optional[str] = None,\n            include_index: bool = True,\n            include_columns: bool = True,\n            merge_hierarchical_labels: bool = True\n            ) -> None:\n        \'\'\'\n        Write the Frame as single-sheet XLSX file.\n        \'\'\'\n        from static_frame.core.store_xlsx import StoreXLSX\n        from static_frame.core.store import StoreConfig\n\n        config = StoreConfig(\n                include_index=include_index,\n                include_columns=include_columns,\n                merge_hierarchical_labels=merge_hierarchical_labels\n                )\n        st = StoreXLSX(fp)\n        st.write(((label, self),), config=config)\n\n\n    def to_sqlite(self,\n            fp: PathSpecifier, # not sure file-like StringIO works\n            *,\n            label: tp.Optional[str] = None,\n            include_index: bool = True,\n            include_columns: bool = True,\n            ) -> None:\n        \'\'\'\n        Write the Frame as single-table SQLite file.\n        \'\'\'\n        from static_frame.core.store_sqlite import StoreSQLite\n        from static_frame.core.store import StoreConfig\n\n        config = StoreConfig(\n                include_index=include_index,\n                include_columns=include_columns,\n                )\n\n        st = StoreSQLite(fp)\n        st.write(((label, self),), config=config)\n\n    def to_hdf5(self,\n            fp: PathSpecifier, # not sure file-like StringIO works\n            *,\n            label: tp.Optional[str] = None,\n            include_index: bool = True,\n            include_columns: bool = True,\n            ) -> None:\n        \'\'\'\n        Write the Frame as single-table SQLite file.\n        \'\'\'\n        from static_frame.core.store_hdf5 import StoreHDF5\n        from static_frame.core.store import StoreConfig\n\n        config = StoreConfig(\n                include_index=include_index,\n                include_columns=include_columns,\n                )\n\n        if not label:\n            if not self.name:\n                raise RuntimeError(\'must provide a label or define Frame name.\')\n            label = self.name\n\n        st = StoreHDF5(fp)\n        st.write(((label, self),), config=config)\n\n\n    @doc_inject(class_name=\'Frame\')\n    def to_html(self,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> str:\n        \'\'\'\n        {}\n        \'\'\'\n        # if a config is given, try to use all settings; if using active, hide types\n        config = config or DisplayActive.get(type_show=False)\n        config = config.to_display_config(\n                display_format=DisplayFormats.HTML_TABLE,\n                )\n        return repr(self.display(config))\n\n    @doc_inject(class_name=\'Frame\')\n    def to_html_datatables(self,\n            fp: tp.Optional[PathSpecifierOrFileLike] = None,\n            show: bool = True,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> str:\n        \'\'\'\n        {}\n        \'\'\'\n        config = config or DisplayActive.get(type_show=False)\n        config = config.to_display_config(\n                display_format=DisplayFormats.HTML_DATATABLES,\n                )\n        content = repr(self.display(config))\n\n        # path_filter called internally\n        fp = write_optional_file(content=content, fp=fp)\n\n        if show:\n            import webbrowser #pragma: no cover\n            webbrowser.open_new_tab(fp) #pragma: no cover\n        return fp\n\n    def to_rst(self,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> str:\n        \'\'\'\n        Display the Frame as an RST formatted table.\n        \'\'\'\n        # if a config is given, try to use all settings; if using active, hide types\n        config = config or DisplayActive.get(type_show=False)\n        config = config.to_display_config(\n                display_format=DisplayFormats.RST,\n                )\n        return repr(self.display(config))\n\n    def to_markdown(self,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> str:\n        \'\'\'\n        Display the Frame as a Markdown formatted table.\n        \'\'\'\n        # if a config is given, try to use all settings; if using active, hide types\n        config = config or DisplayActive.get(type_show=False)\n        config = config.to_display_config(\n                display_format=DisplayFormats.MARKDOWN,\n                )\n        return repr(self.display(config))\n\n\n    def to_latex(self,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> str:\n        \'\'\'\n        Display the Frame as a LaTeX formatted table.\n        \'\'\'\n        # if a config is given, try to use all settings; if using active, hide types\n        config = config or DisplayActive.get(type_show=False)\n        config = config.to_display_config(\n                display_format=DisplayFormats.LATEX,\n                type_delimiter_left=\'$<$\', # escape\n                type_delimiter_right=\'$>$\',\n                )\n        return repr(self.display(config))\n\n#-------------------------------------------------------------------------------\n\nclass FrameGO(Frame):\n    \'\'\'A two-dimensional, ordered, labelled collection, immutable with grow-only columns. Initialization arguments are the same as for :obj:`Frame`.\n    \'\'\'\n\n    __slots__ = (\n            \'_blocks\',\n            \'_columns\',\n            \'_index\',\n            \'_name\'\n            )\n\n    STATIC = False\n    _COLUMNS_CONSTRUCTOR = IndexGO\n    _COLUMNS_HIERARCHY_CONSTRUCTOR = IndexHierarchyGO\n\n\n    def __setitem__(self,\n            key: tp.Hashable,\n            value: tp.Any,\n            fill_value=np.nan\n            ) -> None:\n        \'\'\'For adding a single column, one column at a time.\n        \'\'\'\n        if key in self._columns:\n            raise RuntimeError(f\'The provided key ({key}) is already defined in columns; if you want to change or replace this column, use .assign to get new Frame\')\n\n        row_count = len(self._index)\n\n        if isinstance(value, Series):\n            # select only the values matching our index\n            block = value.reindex(self.index, fill_value=fill_value).values\n        elif isinstance(value, Frame):\n            raise RuntimeError(\n                    f\'cannot use setitem with a Frame; use {self.__class__.__name__}.extend()\')\n        elif isinstance(value, np.ndarray): # is numpy array\n            # this permits unaligned assignment as no index is used, possibly remove\n            if value.ndim != 1:\n                raise RuntimeError(\n                        f\'can only use setitem with 1D containers\')\n            if len(value) != row_count:\n                # block may have zero shape if created without columns\n                raise RuntimeError(f\'incorrectly sized unindexed value: {len(value)} != {row_count}\')\n            block = value # NOTE: could own_data here with additional argument\n\n        else:\n            if not hasattr(value, \'__iter__\') or isinstance(value, str):\n                block = np.full(row_count, value)\n                block.flags.writeable = False\n            else:\n                block, _ = iterable_to_array_1d(value) # returns immutable\n\n            if block.ndim != 1 or len(block) != row_count:\n                raise RuntimeError(\'incorrectly sized, unindexed value\')\n\n        # Wait until after extracting block from value before updating _columns, as value evaluation might fail.\n        self._columns.append(key)\n        self._blocks.append(block)\n\n\n    def extend_items(self,\n            pairs: tp.Iterable[tp.Tuple[tp.Hashable, Series]],\n            fill_value=np.nan):\n        \'\'\'\n        Given an iterable of pairs of column name, column value, extend this FrameGO. Columns values can be any iterable suitable for usage in __setitem__.\n        \'\'\'\n        for k, v in pairs:\n            self.__setitem__(k, v, fill_value)\n\n\n    def extend(self,\n            container: tp.Union[\'Frame\', Series],\n            fill_value=np.nan\n            ):\n        \'\'\'Extend this FrameGO (in-place) with another Frame\'s blocks or Series array; as blocks are immutable, this is a no-copy operation when indices align. If indices do not align, the passed-in Frame or Series will be reindexed (as happens when adding a column to a FrameGO).\n\n        If a Series is passed in, the column name will be taken from the Series ``name`` attribute.\n\n        This method differs from FrameGO.extend_items() by permitting contiguous underlying blocks to be extended from another Frame into this Frame.\n        \'\'\'\n        if not isinstance(container, (Series, Frame)):\n            raise NotImplementedError(\n                    f\'no support for extending with {type(container)}\')\n\n        if not len(container.index): # must be empty data, empty index container\n            return\n\n        # self\'s index will never change; we only take what aligns in the passed container\n        if not self._index.equals(container._index):\n            container = container.reindex(self._index,\n                    fill_value=fill_value,\n                    check_equals=False,\n                    )\n\n        if isinstance(container, Frame):\n            if not len(container.columns):\n                return\n            self._columns.extend(container.keys())\n            self._blocks.extend(container._blocks)\n        elif isinstance(container, Series):\n            self._columns.append(container.name)\n            self._blocks.append(container.values)\n\n        # this should never happen, and is hard to test!\n        assert len(self._columns) == self._blocks._shape[1] #pragma: no cover\n\n    #---------------------------------------------------------------------------\n\n    def _to_frame(self,\n            constructor: tp.Type[ContainerOperand]\n            ) -> Frame:\n        return constructor(self._blocks.copy(),\n                index=self.index,\n                columns=self.columns.values,\n                name=self._name,\n                own_data=True,\n                own_index=True,\n                own_columns=False # need to make static only\n                )\n\n    def to_frame(self) -> Frame:\n        \'\'\'\n        Return Frame version of this FrameGO.\n        \'\'\'\n        return self._to_frame(Frame)\n\n    def to_frame_go(self) -> \'FrameGO\':\n        \'\'\'\n        Return a FrameGO version of this FrameGO.\n        \'\'\'\n        return self._to_frame(FrameGO)\n\n\n#-------------------------------------------------------------------------------\n# utility delegates returned from selection routines and exposing the __call__ interface.\n\nclass FrameAssign(Assign):\n    __slots__ = (\n        \'container\',\n        \'iloc_key\',\n        \'bloc_key\',\n        )\n\n    def __init__(self,\n            container: Frame,\n            iloc_key: GetItemKeyTypeCompound = None,\n            bloc_key: tp.Optional[Bloc2DKeyType] = None,\n            ) -> None:\n        \'\'\'Store a reference to ``Frame``, as well as a key to be used for assignment with ``__call__``\n        \'\'\'\n        self.container = container\n        self.iloc_key = iloc_key\n        self.bloc_key = bloc_key\n\n        if not (self.iloc_key is not None) ^ (self.bloc_key is not None):\n            raise RuntimeError(\'must set only one of ``iloc_key``, ``bloc_key``\')\n\n    def __call__(self,\n            value,\n            fill_value: tp.Any = np.nan\n            ) -> \'Frame\':\n        \'\'\'\n        Assign the ``value`` in the position specified by the selector. The `name` attribute is propagated to the returned container.\n\n        Args:\n            value: Value to assign, which can be a :obj:`Series`, :obj:`Frame`, np.ndarray, or element.\n            fill_value: If the ``value`` parameter has to be reindexed, this element will be used to fill newly created elements.\n        \'\'\'\n        if self.iloc_key is not None:\n            # NOTE: the iloc key\'s order is not relevant in assignment\n            if isinstance(value, (Series, Frame)):\n                if isinstance(value, Series):\n                    iloc_key = self.iloc_key\n                elif isinstance(value, Frame):\n                    # block assignment requires that column keys are ascending\n                    iloc_key = (self.iloc_key[0],\n                            key_to_ascending_key(self.iloc_key[1], self.container.shape[1]))\n                # conform the passed in value to the targets given by self.iloc_key\n                assigned_container = self.container._reindex_other_like_iloc(value,\n                        iloc_key,\n                        fill_value=fill_value)\n                # NOTE: taking .values here forces a single-type array from Frame\n                assigned = assigned_container.values\n            else: # could be array or single element\n                iloc_key = self.iloc_key\n                assigned = value\n\n            blocks = self.container._blocks.extract_iloc_assign(iloc_key, assigned)\n\n        else: # use bloc\n            bloc_key = bloc_key_normalize(\n                    key=self.bloc_key,\n                    container=self.container\n                    )\n\n            if isinstance(value, Frame):\n                invalid = object()\n                value = value.reindex(\n                        index=self.container._index,\n                        columns=self.container._columns,\n                        fill_value=invalid\n                        ).values\n\n                # if we produced any invalid entries, cannot select them\n                invalid_found = value == invalid\n                if invalid_found.any():\n                    bloc_key = bloc_key.copy() # mutate a copy\n                    bloc_key[invalid_found] = False\n\n            elif isinstance(value, np.ndarray):\n                if value.shape != self.container.shape:\n                    raise RuntimeError(f\'value must match shape {self.container.shape}\')\n\n            blocks = self.container._blocks.extract_bloc_assign(bloc_key, value)\n\n\n        return self.container.__class__(\n                data=blocks,\n                columns=self.container._columns,\n                index=self.container._index,\n                name=self.container._name,\n                own_data=True\n                )\n\n\nclass FrameAsType:\n    \'\'\'\n    The object returned from the getitem selector, exposing the functional (__call__) interface to pass in the dtype, as well as (optionally) whether blocks are consolidated.\n    \'\'\'\n    __slots__ = (\'container\', \'column_key\',)\n\n    def __init__(self,\n            container: Frame,\n            column_key: GetItemKeyType\n            ) -> None:\n        self.container = container\n        self.column_key = column_key\n\n    def __call__(self, dtype, consolidate_blocks: bool = True) -> \'Frame\':\n\n        blocks = self.container._blocks._astype_blocks(self.column_key, dtype)\n\n        if consolidate_blocks:\n            blocks = TypeBlocks.consolidate_blocks(blocks)\n\n        blocks = TypeBlocks.from_blocks(blocks)\n\n        return self.container.__class__(\n                data=blocks,\n                columns=self.container.columns,\n                index=self.container.index,\n                name=self.container._name,\n                own_data=True)\n'"
static_frame/core/hloc.py,0,"b""import typing as tp\n\n\nfrom static_frame.core.util import GetItemKeyType\nfrom static_frame.core.util import KEY_MULTIPLE_TYPES\nfrom static_frame.core.util import NULL_SLICE\n\n\nclass HLocMeta(type):\n\n    def __getitem__(cls,\n            key: GetItemKeyType\n            ) -> tp.Iterable[GetItemKeyType]:\n        if not isinstance(key, tuple):\n            key = (key,)\n        # NOTE: tp.case is a performance hit and should be removed\n        return tp.cast(tp.Iterable[GetItemKeyType], cls(key))\n\nclass HLoc(metaclass=HLocMeta):\n    '''A wrapper for embedding hierarchical specificiations for :obj:`static_frame.IndexHierarchy` within a single axis argument of a ``loc`` selection.\n\n    Implemented as a container of hiearchical keys that defiines NULL slices for all lower dimensions that are not defined at construction.\n    '''\n\n    __slots__ = (\n            'key',\n            )\n\n    def __init__(self, key: tp.Sequence[GetItemKeyType]):\n        self.key = key\n\n    def __iter__(self) -> tp.Iterator[GetItemKeyType]:\n        return self.key.__iter__()\n\n    def __len__(self) -> int:\n        return self.key.__len__()\n\n    def __getitem__(self, key: int) -> GetItemKeyType:\n        '''\n        Each key reprsents a hierarchical level; if a key is not specified, the default should be to return the null slice.\n        '''\n        if key >= len(self.key):\n            return NULL_SLICE\n        return self.key.__getitem__(key)\n\n    def has_key_multiple(self) -> bool:\n        return any(isinstance(k, KEY_MULTIPLE_TYPES) for k in self.key)\n"""
static_frame/core/index.py,55,"b'import typing as tp\nfrom collections.abc import KeysView\nimport operator as operator_mod\nfrom itertools import zip_longest\nfrom functools import reduce\n\nimport numpy as np\n\nfrom automap import AutoMap\nfrom automap import FrozenAutoMap\n\nfrom static_frame.core.util import DEFAULT_SORT_KIND\nfrom static_frame.core.util import NULL_SLICE\nfrom static_frame.core.util import EMPTY_TUPLE\nfrom static_frame.core.util import EMPTY_SLICE\nfrom static_frame.core.util import SLICE_ATTRS\nfrom static_frame.core.util import SLICE_START_ATTR\nfrom static_frame.core.util import SLICE_STOP_ATTR\nfrom static_frame.core.util import SLICE_STEP_ATTR\nfrom static_frame.core.util import NAME_DEFAULT\n\nfrom static_frame.core.util import BOOL_TYPES\nfrom static_frame.core.util import KEY_ITERABLE_TYPES\nfrom static_frame.core.util import EMPTY_ARRAY\nfrom static_frame.core.util import DTYPE_DATETIME_KIND\n\nfrom static_frame.core.util import GetItemKeyType\nfrom static_frame.core.util import KeyTransformType\nfrom static_frame.core.util import CallableOrMapping\nfrom static_frame.core.util import DtypeSpecifier\nfrom static_frame.core.util import KeyIterableTypes\nfrom static_frame.core.util import UFunc\nfrom static_frame.core.util import NameType\n\nfrom static_frame.core.util import IndexInitializer\nfrom static_frame.core.util import DepthLevelSpecifier\nfrom static_frame.core.util import ufunc_axis_skipna\nfrom static_frame.core.util import iterable_to_array_1d\nfrom static_frame.core.util import isin\n\nfrom static_frame.core.util import immutable_filter\nfrom static_frame.core.util import name_filter\nfrom static_frame.core.util import array_shift\nfrom static_frame.core.util import array2d_to_tuples\nfrom static_frame.core.util import slice_to_inclusive_slice\nfrom static_frame.core.util import isna_array\n\nfrom static_frame.core.util import DTYPE_INT_DEFAULT\n\nfrom static_frame.core.node_selector import InterfaceGetItem\nfrom static_frame.core.node_selector import InterfaceSelectDuo\nfrom static_frame.core.node_selector import TContainer\n\nfrom static_frame.core.node_str import InterfaceString\nfrom static_frame.core.node_dt import InterfaceDatetime\nfrom static_frame.core.util import union1d\nfrom static_frame.core.util import intersect1d\nfrom static_frame.core.util import setdiff1d\nfrom static_frame.core.util import to_datetime64\nfrom static_frame.core.util import INT_TYPES\nfrom static_frame.core.util import mloc\n\nfrom static_frame.core.util import resolve_dtype\nfrom static_frame.core.container import ContainerOperand\nfrom static_frame.core.container_util import matmul\nfrom static_frame.core.container_util import apply_binary_operator\n\n\nfrom static_frame.core.doc_str import doc_inject\nfrom static_frame.core.index_base import IndexBase\n# from static_frame.core.node_iter import IterNode\nfrom static_frame.core.node_iter import IterNodeDepthLevel\nfrom static_frame.core.node_iter import IterNodeType\nfrom static_frame.core.node_iter import IterNodeApplyType\n\n\nfrom static_frame.core.display import DisplayConfig\nfrom static_frame.core.display import DisplayActive\nfrom static_frame.core.display import Display\nfrom static_frame.core.display import DisplayHeader\n\nfrom static_frame.core.exception import ErrorInitIndex\nfrom static_frame.core.exception import LocEmpty\nfrom static_frame.core.exception import LocInvalid\n\n\nif tp.TYPE_CHECKING:\n    import pandas #pylint: disable=W0611 #pragma: no cover\n    from static_frame import Series #pylint: disable=W0611 #pragma: no cover\n    from static_frame import IndexHierarchy #pylint: disable=W0611 #pragma: no cover\n\n\nI = tp.TypeVar(\'I\', bound=IndexBase)\n\n\nclass ILocMeta(type):\n\n    def __getitem__(cls,\n            key: GetItemKeyType\n            ) -> tp.Iterable[GetItemKeyType]:\n        return cls(key) #type: ignore\n\nclass ILoc(metaclass=ILocMeta):\n    \'\'\'A wrapper for embedding ``iloc`` specificiations within a single axis argument of a ``loc`` selection.\n    \'\'\'\n\n    __slots__ = (\n            \'key\',\n            )\n\n    def __init__(self, key: GetItemKeyType):\n        self.key = key\n\nclass LocMap:\n\n    @staticmethod\n    def map_slice_args(\n            label_to_pos: tp.Callable[[tp.Iterable[tp.Hashable]], int],\n            key: slice,\n            labels: tp.Optional[np.ndarray] = None,\n            offset: tp.Optional[int] = 0\n            ) -> tp.Iterator[tp.Union[int, None]]:\n        \'\'\'Given a slice ``key`` and a label-to-position mapping, yield each integer argument necessary to create a new iloc slice. If the ``key`` defines a region with no constituents, raise ``LocEmpty``\n\n        Args:\n            label_to_pos: callable into mapping (can be a get() method from a dictionary)\n        \'\'\'\n        offset_apply = not offset is None\n\n        for field in SLICE_ATTRS:\n            attr = getattr(key, field)\n            if attr is None:\n                yield None\n\n            elif isinstance(attr, np.datetime64):\n                # if a datetime, we assume that the labels are ordered;\n                if attr.dtype == labels.dtype: #type: ignore\n                    if field != SLICE_STEP_ATTR:\n                        pos: int = label_to_pos(attr)\n                        if pos is None:\n                            # if same type, and that atter is not in labels, we fail, just as we do in then non-datetime64 case. Only when datetimes are given in a different unit are we ""loose"" about matching.\n                            raise LocInvalid(\'Invalid loc given in a slice\', attr, field)\n                    else: # step\n                        pos = attr # should be an integer\n\n                    if field == SLICE_STOP_ATTR:\n                        pos += 1 # stop is inclusive\n\n                elif field == SLICE_START_ATTR:\n                    # convert to the type of the atrs; this should get the relevant start\n                    pos = label_to_pos(attr.astype(labels.dtype)) #type: ignore\n                    if pos is None: # we did not find a start position\n                        matches = np.flatnonzero(labels.astype(attr.dtype) == attr)\n                        if len(matches):\n                            pos = matches[0]\n                        else:\n                            raise LocEmpty()\n\n                elif field == SLICE_STOP_ATTR:\n                    # convert labels to the slice attr value, compare, then get last\n                    # add one, as this is an inclusive stop\n                    # pos = np.flatnonzero(labels.astype(attr.dtype) == attr)[-1] + 1\n                    matches = np.flatnonzero(labels.astype(attr.dtype) == attr) #type: ignore\n                    if len(matches):\n                        pos = matches[-1] + 1\n                    else:\n                        raise LocEmpty()\n\n                elif field == SLICE_STEP_ATTR:\n                    pos = attr\n\n                if offset_apply and field != SLICE_STEP_ATTR:\n                    pos += offset #type: ignore\n\n                yield pos\n\n            else:\n                if field != SLICE_STEP_ATTR:\n                    pos = label_to_pos(attr)\n                    if pos is None:\n                        # NOTE: could raise LocEmpty() to silently handle this\n                        raise LocInvalid(\'Invalid loc given in a slice\', attr, field)\n                    if offset_apply:\n                        pos += offset #type: ignore\n                else: # step\n                    pos = attr # should be an integer\n\n                if field == SLICE_STOP_ATTR:\n                    # loc selections are inclusive, so iloc gets one more\n                    pos += 1\n\n                yield pos\n\n    @classmethod\n    def loc_to_iloc(cls, *,\n            label_to_pos: tp.Dict[tp.Hashable, int],\n            labels: np.ndarray,\n            positions: np.ndarray,\n            key: GetItemKeyType,\n            offset: tp.Optional[int] = None\n            ) -> GetItemKeyType:\n        \'\'\'\n        Note: all SF objects (Series, Index) need to be converted to basic types before being passed as `key` to this function.\n\n        Args:\n            offset: in the contect of an IndexHierarchical, the iloc positions returned from this funcition need to be shifted.\n        Returns:\n            An integer mapped slice, or GetItemKey type that is based on integers, compatible with TypeBlocks\n        \'\'\'\n        offset_apply = not offset is None\n\n        # ILoc is handled prior to this call, in the Index.loc_to_iloc method\n\n        if isinstance(key, slice):\n            if offset_apply and key == NULL_SLICE:\n                # when offset is defined (even if it is zero), null slice is not sufficiently specific; need to convert to an explict slice relative to the offset\n                return slice(offset, len(positions) + offset) #type: ignore\n            try:\n                return slice(*cls.map_slice_args(\n                        label_to_pos.get, #type: ignore\n                        key,\n                        labels,\n                        offset)\n                        )\n            except LocEmpty:\n                return EMPTY_SLICE\n\n        if isinstance(key, np.datetime64):\n            # convert this to the target representation, do a Boolean selection\n            if labels.dtype != key.dtype:\n                key = labels.astype(key.dtype) == key\n            # if not different type, keep it the same so as to do a direct, single element selection\n\n        # handles only lists and arrays; break out comparisons to avoid multiple\n        is_array = isinstance(key, np.ndarray)\n        is_list = isinstance(key, list)\n\n        # can be an iterable of labels (keys) or an iterable of Booleans\n        if is_array or is_list:\n            if is_array and key.dtype.kind == DTYPE_DATETIME_KIND:\n                if labels.dtype != key.dtype:\n                    labels_ref = labels.astype(key.dtype)\n                    # let Boolean key hit next branch\n                    key = reduce(operator_mod.or_,\n                            (labels_ref == k for k in key))\n                    # NOTE: may want to raise instead of support this\n                    # raise NotImplementedError(f\'selecting {labels.dtype} with {key.dtype} is not presently supported\')\n\n            if is_array and key.dtype == bool:\n                if offset_apply:\n                    return positions[key] + offset\n                return positions[key]\n\n            # map labels to integer positions\n            # NOTE: we may miss the opportunity to get a reference from values when we have contiguous keys\n            if offset_apply:\n                return [label_to_pos[x] + offset for x in key] #type: ignore\n            return [label_to_pos[x] for x in key]\n\n        # if a single element (an integer, string, or date, we just get the integer out of the map\n        if offset_apply:\n            return label_to_pos[key] + offset #type: ignore\n        return label_to_pos[key]\n\n\ndef immutable_index_filter(index: I) -> IndexBase:\n    \'\'\'Return an immutable index. All index objects handle converting from mutable to immutable via the __init__ constructor; but need to use appropriate class between Index and IndexHierarchy.\'\'\'\n\n    if index.STATIC:\n        return index\n    return index._IMMUTABLE_CONSTRUCTOR(index)\n\n\ndef mutable_immutable_index_filter(\n        target_static: bool,\n        index: I\n        ) -> IndexBase:\n    if target_static:\n        return immutable_index_filter(index)\n    # target mutable\n    if index.STATIC:\n        return index._MUTABLE_CONSTRUCTOR(index)\n    return index.__class__(index) # create new instance\n\n#-------------------------------------------------------------------------------\n\nclass PositionsAllocator:\n\n    _size: int = 0\n    _array: np.ndarray = np.arange(_size, dtype=DTYPE_INT_DEFAULT)\n    _array.flags.writeable = False\n\n    @classmethod\n    def get(cls, size: int) -> np.ndarray:\n        if size > cls._size:\n            cls._size = size * 2\n            cls._array = np.arange(cls._size, dtype=DTYPE_INT_DEFAULT)\n            cls._array.flags.writeable = False\n        # slices of immutable arrays are immutable\n        return cls._array[:size]\n\n#-------------------------------------------------------------------------------\n_INDEX_SLOTS = (\n        \'_map\',\n        \'_labels\',\n        \'_positions\',\n        \'_recache\',\n        \'_name\'\n        )\n\n@doc_inject(selector=\'index_init\')\nclass Index(IndexBase):\n    \'\'\'A mapping of labels to positions, immutable and of fixed size. Used by default in :obj:`Series` and as index and columns in :obj:`Frame`. Base class of all 1D indices.\n\n    {args}\n    \'\'\'\n\n    __slots__ = _INDEX_SLOTS\n\n    # _IMMUTABLE_CONSTRUCTOR is None from IndexBase\n    # _MUTABLE_CONSTRUCTOR will be set after IndexGO defined\n\n    _UFUNC_UNION = union1d\n    _UFUNC_INTERSECTION = intersect1d\n    _UFUNC_DIFFERENCE = setdiff1d\n\n    _DTYPE: tp.Optional[np.dtype] = None # for specialized indices requiring a typed labels\n\n    # for compatability with IndexHierarchy, where this is implemented as a property method\n    depth: int = 1\n\n    _map: tp.Optional[FrozenAutoMap]\n    _labels: np.ndarray\n    _positions: np.ndarray\n    _recache: bool\n    _name: NameType\n\n    #---------------------------------------------------------------------------\n    # methods used in __init__ that are customized in dervied classes; there, we need to mutate instance state, this these are instance methods\n    @staticmethod\n    def _extract_labels(\n            mapping: tp.Optional[tp.Dict[tp.Hashable, int]],\n            labels: tp.Iterable[tp.Hashable],\n            dtype: tp.Optional[np.dtype] = None\n            ) -> np.ndarray:\n        \'\'\'Derive labels, a cache of the mapping keys in a sequence type (either an ndarray or a list).\n\n        If the labels passed at instantiation are an ndarray, they are used after immutable filtering. Otherwise, the mapping keys are used to create an ndarray.\n\n        This method is overridden in the derived class.\n\n        Args:\n            mapping: Can be None if loc_is_iloc.\n            labels: might be an expired Generator, but if it is an immutable ndarray, we can use it without a copy.\n        \'\'\'\n        # pre-fetching labels for faster get_item construction\n        if isinstance(labels, np.ndarray):\n            if dtype is not None and dtype != labels.dtype:\n                raise ErrorInitIndex(\'invalid label dtype for this Index\')\n            return immutable_filter(labels)\n\n        if hasattr(labels, \'__len__\'): # not a generator, not an array\n            # resolving the dtype is expensive, pass if possible\n            if len(labels) == 0: #type: ignore\n                labels = EMPTY_ARRAY\n            else:\n                labels, _ = iterable_to_array_1d(labels, dtype=dtype)\n        else: # labels may be an expired generator, must use the mapping\n            if len(mapping) == 0: #type: ignore\n                labels = EMPTY_ARRAY\n            else:\n                labels, _ = iterable_to_array_1d(mapping, dtype=dtype) #type: ignore\n        # all arrays are immutable\n        # assert labels.flags.writeable == False\n        return labels\n\n    @staticmethod\n    def _extract_positions(\n            size: int,\n            positions: tp.Optional[tp.Sequence[int]]\n            ) -> np.ndarray:\n        # positions is either None or an ndarray\n        if isinstance(positions, np.ndarray):\n            return immutable_filter(positions)\n        return PositionsAllocator.get(size)\n\n    #---------------------------------------------------------------------------\n    # constructors\n\n    @classmethod\n    def from_labels(cls: tp.Type[I],\n            labels: tp.Iterable[tp.Sequence[tp.Hashable]],\n            *,\n            name: NameType = None\n            ) -> I:\n        \'\'\'\n        Construct an ``Index`` from an iterable of labels, where each label is a hashable. Provided for a compatible interface to ``IndexHierarchy``.\n        \'\'\'\n        return cls(labels, name=name)\n\n    #---------------------------------------------------------------------------\n    def __init__(self,\n            labels: IndexInitializer,\n            *,\n            loc_is_iloc: bool = False,\n            name: NameType = NAME_DEFAULT,\n            dtype: DtypeSpecifier = None\n            ) -> None:\n\n        self._recache: bool = False\n        self._map: tp.Optional[FrozenAutoMap] = None\n\n        positions = None\n        is_typed = self._DTYPE is not None\n\n        # resolve the targetted labels dtype, by lookin at the class attr _DTYPE and/or the passed dtype argument\n        if dtype is None:\n            dtype_extract = self._DTYPE # set in some specialized Index classes\n        else: # passed dtype is not None\n            if is_typed and dtype != self._DTYPE:\n                # NOTE: should never get to this branch, as derived Index classes that set _DTYPE remove dtype from __init__\n                raise ErrorInitIndex(\'invalid dtype argument for this Index\', dtype, self._DTYPE) #pragma: no cover\n            # self._DTYPE is None, passed dtype is not None, use dtype\n            dtype_extract = dtype\n\n        #-----------------------------------------------------------------------\n        # handle all Index subclasses\n        if isinstance(labels, IndexBase):\n            if labels._recache:\n                labels._update_array_cache()\n            if name is NAME_DEFAULT:\n                name = labels.name # immutable, so no copy necessary\n            if isinstance(labels, Index): # not an IndexHierarchy\n                if (labels.STATIC and self.STATIC and dtype is None):\n                    if not is_typed or (is_typed and self._DTYPE == labels.dtype):\n                        # can take the map if static and if types in the dict are the same as those in the labels (or to become the labels after conversion)\n                        self._map = labels._map\n                # get a reference to the immutable arrays, even if this is an IndexGO index, we can take the cached arrays, assuming they are up to date; for datetime64 indices, we might need to translate to a different type\n                positions = labels._positions\n                loc_is_iloc = labels._map is None\n                labels = labels._labels\n            else: # IndexHierarchy\n                # will be a generator of tuples; already updated caches\n                labels = array2d_to_tuples(labels.__iter__())\n        elif isinstance(labels, ContainerOperand):\n            # it is a Series or similar\n            array = labels.values\n            if array.ndim == 1:\n                labels = array\n            else:\n                labels = array2d_to_tuples(array)\n        # else: assume an iterable suitable for labels usage\n\n        #-----------------------------------------------------------------------\n        if is_typed:\n            # do not need to check arrays, as will and checked to match dtype_extract in _extract_labels\n            if not isinstance(labels, np.ndarray):\n                # for now, assume that if _DTYPE is defined, we have a date\n                labels = (to_datetime64(v, dtype_extract) for v in labels)\n            # coerce to target type\n            elif labels.dtype != dtype_extract:\n                labels = labels.astype(dtype_extract)\n                labels.flags.writeable = False #type: ignore\n\n        self._name = None if name is NAME_DEFAULT else name_filter(name)\n\n        if self._map is None: # if _map not shared from another Index\n            if not loc_is_iloc:\n                try:\n                    self._map = FrozenAutoMap(labels) if self.STATIC else AutoMap(labels)\n                except ValueError: # Automap will raise ValueError of non-unique values are encountered\n                    pass\n                if self._map is None:\n                    raise ErrorInitIndex(f\'labels ({len(tuple(labels))}) have non-unique values ({len(set(labels))})\')\n                size = len(self._map)\n            else: # must assume labels are unique\n                # labels must not be a generator, but we assume that internal clients that provided loc_is_iloc will not give a generator\n                size = len(labels) #type: ignore\n                if positions is None:\n                    positions = PositionsAllocator.get(size)\n        else: # map shared from another Index\n            size = len(self._map)\n\n        # this might be NP array, or a list, depending on if static or grow only; if an array, dtype will be compared with passed dtype_extract\n        self._labels = self._extract_labels(self._map, labels, dtype_extract)\n        self._positions = self._extract_positions(size, positions)\n\n        if self._DTYPE and self._labels.dtype != self._DTYPE:\n            raise ErrorInitIndex(\'invalid label dtype for this Index\', #pragma: no cover\n                    self._labels.dtype, self._DTYPE)\n\n\n    #---------------------------------------------------------------------------\n    def __setstate__(self, state: tp.Tuple[None, tp.Dict[str, tp.Any]]) -> None:\n        \'\'\'\n        Ensure that reanimated NP arrays are set not writeable.\n        \'\'\'\n        for key, value in state[1].items():\n            setattr(self, key, value)\n        self._labels.flags.writeable = False\n\n    #---------------------------------------------------------------------------\n    # name interface\n\n    def rename(self: I, name: NameType) -> I:\n        \'\'\'\n        Return a new Frame with an updated name attribute.\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n        # let the constructor handle reuse\n        return self.__class__(self, name=name)\n\n    #---------------------------------------------------------------------------\n    # interfaces\n\n    @property\n    def loc(self) -> InterfaceGetItem[TContainer]:\n        return InterfaceGetItem(self._extract_loc) #type: ignore\n\n    @property\n    def iloc(self) -> InterfaceGetItem[TContainer]:\n        return InterfaceGetItem(self._extract_iloc) #type: ignore\n\n    # # on Index, getitem is an iloc selector; on Series, getitem is a loc selector; for this extraction interface, we do not implement a getitem level function (using iloc would be consistent), as it is better to be explicit between iloc loc\n\n    def _iter_label(self,\n            depth_level: DepthLevelSpecifier = 0\n            ) -> tp.Iterator[tp.Hashable]:\n        yield from self._labels\n\n    def _iter_label_items(self,\n            depth_level: DepthLevelSpecifier = 0\n            ) -> tp.Iterator[tp.Tuple[int, tp.Hashable]]:\n        yield from zip(self._positions, self._labels)\n\n    @property\n    def iter_label(self) -> IterNodeDepthLevel[tp.Any]:\n        return IterNodeDepthLevel(\n                container=self,\n                function_items=self._iter_label_items,\n                function_values=self._iter_label,\n                yield_type=IterNodeType.VALUES,\n                apply_type=IterNodeApplyType.INDEX_LABELS\n                )\n\n\n    #---------------------------------------------------------------------------\n    # common attributes from the numpy array\n\n    @property # type: ignore\n    @doc_inject()\n    def mloc(self) -> int:\n        \'\'\'{doc_int}\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n        return mloc(self._labels)\n\n    @property\n    def dtype(self) -> np.dtype:\n        \'\'\'\n        Return the dtype of the underlying NumPy array.\n\n        Returns:\n            :obj:`numpy.dtype`\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n        return self._labels.dtype\n\n    @property\n    def shape(self) -> tp.Tuple[int, ...]:\n        \'\'\'\n        Return a tuple describing the shape of the underlying NumPy array.\n\n        Returns:\n            :obj:`tp.Tuple[int]`\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n        return tp.cast(tp.Tuple[int, ...], self._labels.shape)\n\n    @property\n    def ndim(self) -> int:\n        \'\'\'\n        Return the number of dimensions.\n\n        Returns:\n            :obj:`int`\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n        return tp.cast(int, self._labels.ndim)\n\n    @property\n    def size(self) -> int:\n        \'\'\'\n        Return the size of the underlying NumPy array.\n\n        Returns:\n            :obj:`int`\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n        return tp.cast(int, self._labels.size)\n\n    @property\n    def nbytes(self) -> int:\n        \'\'\'\n        Return the total bytes of the underlying NumPy array.\n\n        Returns:\n            :obj:`int`\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n        return tp.cast(int, self._labels.nbytes)\n\n    def __bool__(self) -> bool:\n        \'\'\'\n        True if this container has size.\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n        return bool(self._labels.size)\n\n    #---------------------------------------------------------------------------\n    def _drop_iloc(self, key: GetItemKeyType) -> \'IndexBase\':\n        \'\'\'Create a new index after removing the values specified by the loc key.\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n\n        if key is None:\n            if self.STATIC: # immutable, no selection, can return self\n                return self\n            labels = self._labels # already immutable\n        elif isinstance(key, np.ndarray) and key.dtype == bool:\n            # can use labels, as we already recached\n            # use Boolean area to select indices from positions, as np.delete does not work with arrays\n            labels = np.delete(self._labels, self._positions[key], axis=0)\n            labels.flags.writeable = False\n        else:\n            labels = np.delete(self._labels, key, axis=0)\n            labels.flags.writeable = False\n\n        # from labels will work with both Index and IndexHierarchy\n        return self.__class__.from_labels(labels, name=self._name)\n\n    def _drop_loc(self, key: GetItemKeyType) -> \'IndexBase\':\n        \'\'\'Create a new index after removing the values specified by the loc key.\n        \'\'\'\n        return self._drop_iloc(self.loc_to_iloc(key))\n\n\n    @property\n    def drop(self) -> InterfaceSelectDuo[TContainer]:\n        return InterfaceSelectDuo( #type: ignore\n            func_iloc=self._drop_iloc,\n            func_loc=self._drop_loc,\n            )\n\n\n    @doc_inject(select=\'astype\')\n    def astype(self, dtype: DtypeSpecifier) -> \'Index\':\n        \'\'\'\n        Return an Index with type determined by `dtype` argument. Note that for Index, this is a simple function, whereas for ``IndexHierarchy``, this is an interface exposing both a callable and a getitem interface.\n\n        Args:\n            {dtype}\n        \'\'\'\n        from static_frame.core.index_datetime import _dtype_to_index_cls\n        array = self.values.astype(dtype)\n        cls = _dtype_to_index_cls(self.STATIC, array.dtype)\n        return cls(\n                array,\n                name=self._name\n                )\n\n\n    #---------------------------------------------------------------------------\n    @property\n    def via_str(self) -> InterfaceString[np.ndarray]:\n        \'\'\'\n        Interface for applying string methods to elements in this container.\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n\n        blocks = (self._labels,)\n        cls = Index if self.STATIC else IndexGO\n\n        def blocks_to_container(blocks: tp.Iterator[np.ndarray]) -> np.ndarray:\n            return next(blocks)\n\n        return InterfaceString(\n                blocks=blocks,\n                blocks_to_container=blocks_to_container,\n                )\n\n    @property\n    def via_dt(self) -> InterfaceDatetime[np.ndarray]:\n        \'\'\'\n        Interface for applying datetime properties and methods to elements in this container.\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n\n        blocks = (self.values,)\n        cls = Index if self.STATIC else IndexGO\n\n        def blocks_to_container(blocks: tp.Iterator[np.ndarray]) -> np.ndarray:\n            return next(blocks)\n\n        return InterfaceDatetime(\n                blocks=blocks,\n                blocks_to_container=blocks_to_container,\n                )\n\n\n    #---------------------------------------------------------------------------\n\n    def _update_array_cache(self) -> None:\n        \'\'\'Derived classes can use this to set stored arrays, self._labels and self._positions.\n        \'\'\'\n\n    #---------------------------------------------------------------------------\n\n    def __len__(self) -> int:\n        if self._recache:\n            self._update_array_cache()\n        return len(self._labels)\n\n    @doc_inject()\n    def display(self,\n            config: tp.Optional[DisplayConfig] = None,\n            ) -> Display:\n        \'\'\'{doc}\n\n        Args:\n            {config}\n        \'\'\'\n        config = config or DisplayActive.get()\n\n        if self._recache:\n            self._update_array_cache()\n\n        header: tp.Optional[DisplayHeader]\n\n        if config.type_show:\n            header = DisplayHeader(self.__class__, self._name)\n            header_depth = 1\n        else:\n            header = None\n            header_depth = 0\n\n        return Display.from_values(self.values,\n                header=header,\n                config=config,\n                outermost=True,\n                index_depth=0,\n                header_depth=header_depth,\n                )\n\n    #---------------------------------------------------------------------------\n    # core internal representation\n\n    @property #type: ignore\n    @doc_inject(selector=\'values_1d\', class_name=\'Index\')\n    def values(self) -> np.ndarray:\n        \'\'\'\n        {}\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n        return self._labels\n\n    @property\n    def positions(self) -> np.ndarray:\n        \'\'\'Return the immutable positions array. This is needed by some clients, such as Series and Frame, to support Boolean usage in drop.\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n        return self._positions\n\n    def values_at_depth(self,\n            depth_level: DepthLevelSpecifier = 0\n            ) -> np.ndarray:\n        \'\'\'\n        Return an NP array for the `depth_level` specified.\n        \'\'\'\n        if depth_level != 0:\n            raise RuntimeError(\'invalid depth_level\', depth_level)\n        return self.values\n\n    @doc_inject()\n    def label_widths_at_depth(self,\n            depth_level: DepthLevelSpecifier = 0\n            ) -> tp.Iterator[tp.Tuple[tp.Hashable, int]]:\n        \'\'\'{}\'\'\'\n        if depth_level != 0:\n            raise RuntimeError(\'invalid depth_level\', depth_level)\n        yield from zip_longest(self.values, EMPTY_TUPLE, fillvalue=1)\n\n\n    #---------------------------------------------------------------------------\n\n    def copy(self: I) -> I:\n        \'\'\'\n        Return a new Index.\n        \'\'\'\n        # this is not a complete deepcopy, as _labels here is an immutable np array (a new map will be created); if this is an IndexGO, we will pass the cached, immutable NP array\n        if self._recache:\n            self._update_array_cache()\n\n        return self.__class__(self, name=self._name)\n\n    def relabel(self, mapper: CallableOrMapping) -> \'Index\':\n        \'\'\'\n        Return a new Index with labels replaced by the callable or mapping; order will be retained. If a mapping is used, the mapping need not map all origin keys.\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n\n        if not callable(mapper):\n            # if a mapper, it must support both __getitem__ and __contains__\n            getitem = getattr(mapper, \'__getitem__\')\n            return self.__class__(\n                    (getitem(x) if x in mapper else x for x in self._labels),\n                    name=self._name\n                    )\n\n        return self.__class__(\n                (mapper(x) for x in self._labels),\n                name=self._name\n                )\n\n    #---------------------------------------------------------------------------\n    # extraction and selection\n\n    def loc_to_iloc(self,\n            key: GetItemKeyType,\n            offset: tp.Optional[int] = None,\n            key_transform: KeyTransformType = None\n            ) -> GetItemKeyType:\n        \'\'\'\n        Note: Boolean Series are reindexed to this index, then passed on as all Boolean arrays.\n\n        Args:\n            offset: A default of None is critical to avoid large overhead in unnecessary application of offsets.\n            key_transform: A function that transforms keys to specialized type; used by IndexDate indices.\n        Returns:\n            Return GetItemKey type that is based on integers, compatible with TypeBlocks\n        \'\'\'\n        from static_frame.core.series import Series\n\n        if self._recache:\n            self._update_array_cache()\n\n        if isinstance(key, ILoc):\n            return key.key\n        elif isinstance(key, Index):\n            # if an Index, we simply use the values of the index\n            key = key.values\n        elif isinstance(key, Series):\n            if key.dtype == bool:\n                if not key.index.equals(self):\n                    key = key.reindex(self,\n                            fill_value=False,\n                            check_equals=False,\n                            ).values\n                else: # the index is equal\n                    key = key.values\n            else:\n                key = key.values\n\n        if self._map is None: # loc_is_iloc\n            if isinstance(key, np.ndarray):\n                if key.dtype == bool:\n                    return key\n                if key.dtype != DTYPE_INT_DEFAULT:\n                    # if key is an np.array, it must be an int or bool type\n                    # could use tolist(), but we expect all keys to be integers\n                    return key.astype(DTYPE_INT_DEFAULT)\n            elif isinstance(key, slice):\n                key = slice_to_inclusive_slice(key)\n            return key\n\n        if key_transform:\n            key = key_transform(key)\n\n        return LocMap.loc_to_iloc(\n                label_to_pos=self._map,\n                labels=self._labels,\n                positions=self._positions, # always an np.ndarray\n                key=key,\n                offset=offset\n                )\n\n    def _extract_iloc(self,\n            key: GetItemKeyType\n            ) -> tp.Union[\'Index\', tp.Hashable]:\n        \'\'\'Extract a new index given an iloc key\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n\n        if key is None:\n            labels = self._labels\n        elif isinstance(key, slice):\n            if key == NULL_SLICE:\n                labels = self._labels\n            else:\n                # if labels is an np array, this will be a view; if a list, a copy\n                labels = self._labels[key]\n        elif isinstance(key, KEY_ITERABLE_TYPES):\n            # we assume Booleans have been normalized to integers here\n            # can select directly from _labels[key] if if key is a list\n            labels = self._labels[key]\n        else: # select a single label value\n            return self._labels[key] #type: ignore\n\n        return self.__class__(labels=labels, name=self._name)\n\n    def _extract_loc(self: I,\n            key: GetItemKeyType\n            ) -> tp.Union[\'Index\', tp.Hashable]:\n        return self._extract_iloc(self.loc_to_iloc(key)) #type: ignore\n\n    def __getitem__(self: I,\n            key: GetItemKeyType\n            ) -> tp.Union[\'Index\', tp.Hashable]:\n        \'\'\'Extract a new index given an iloc key.\n        \'\'\'\n        return self._extract_iloc(key)\n\n    #---------------------------------------------------------------------------\n    # operators\n\n    def _ufunc_unary_operator(self,\n            operator: UFunc\n            ) -> np.ndarray:\n        \'\'\'Always return an NP array.\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n\n        array = operator(self._labels)\n        array.flags.writeable = False\n        return array\n\n    def _ufunc_binary_operator(self, *,\n            operator: UFunc,\n            other: tp.Any\n            ) -> np.ndarray:\n        \'\'\'\n        Binary operators applied to an index always return an NP array. This deviates from Pandas, where some operations (multipling an int index by an int) result in a new Index, while other operations result in a np.array (using == on two Index).\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n\n        values = self._labels\n        other_is_array = False\n\n        if issubclass(other.__class__, Index):\n            other = other.values # operate on labels to labels\n            other_is_array = True\n        elif isinstance(other, np.ndarray):\n            other_is_array = True\n\n        if operator.__name__ == \'matmul\':\n            return matmul(values, other)\n        elif operator.__name__ == \'rmatmul\':\n            return matmul(other, values)\n\n        return apply_binary_operator(\n                values=values,\n                other=other,\n                other_is_array=other_is_array,\n                operator=operator,\n                )\n\n    def _ufunc_axis_skipna(self, *,\n            axis: int,\n            skipna: bool,\n            ufunc: UFunc,\n            ufunc_skipna: UFunc,\n            composable: bool,\n            dtypes: tp.Tuple[np.dtype, ...],\n            size_one_unity: bool\n            ) -> np.ndarray:\n        \'\'\'\n\n        Args:\n            dtype: Not used in 1D application, but collected here to provide a uniform signature.\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n\n        # do not need to pass on composabel here\n        return ufunc_axis_skipna(\n                array=self._labels,\n                skipna=skipna,\n                axis=0,\n                ufunc=ufunc,\n                ufunc_skipna=ufunc_skipna\n                )\n\n    # _ufunc_shape_skipna defined in IndexBase\n\n    #---------------------------------------------------------------------------\n    # dictionary-like interface\n\n    # NOTE: we intentionally exclude keys(), items(), and get() from Index classes, as they return inconsistent result when thought of as a dictionary\n\n\n    def __iter__(self) -> tp.Iterator[tp.Hashable]:\n        \'\'\'Iterate over labels.\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n        return tp.cast(tp.Iterator[tp.Hashable], self._labels.__iter__())\n\n    def __reversed__(self) -> tp.Iterator[tp.Hashable]:\n        \'\'\'\n        Returns a reverse iterator on the index labels.\n        \'\'\'\n        if self._recache:\n            self._update_array_cache()\n        return reversed(self._labels)\n\n    def __contains__(self, value: tp.Any) -> bool:\n        \'\'\'Return True if value in the labels.\n        \'\'\'\n        if self._map is None: # loc_is_iloc\n            if isinstance(value, INT_TYPES):\n                return value >= 0 and value < len(self) #type: ignore\n            return False\n        return self._map.__contains__(value) #type: ignore\n\n\n    #---------------------------------------------------------------------------\n    # utility functions\n\n    @doc_inject()\n    def equals(self,\n            other: tp.Any,\n            *,\n            compare_name: bool = False,\n            compare_dtype: bool = False,\n            compare_class: bool = False,\n            skipna: bool = True,\n            ) -> bool:\n        \'\'\'\n        {doc}\n\n        Args:\n            {compare_name}\n            {compare_dtype}\n            {compare_class}\n            {skipna}\n        \'\'\'\n\n        if id(other) == id(self):\n            return True\n\n        if compare_class and self.__class__ != other.__class__:\n            return False\n        elif not isinstance(other, Index):\n            return False\n\n        # defer updating cache\n        if self._recache:\n            self._update_array_cache()\n\n        # same type from here\n        if len(self) != len(other):\n            return False\n        if compare_name and self.name != other.name:\n            return False\n        if compare_dtype and self.dtype != other.dtype:\n            return False\n\n        eq = self.values == other.values\n\n        # NOTE: will only be False, or an array\n        if eq is False:\n            return eq #type: ignore\n\n        if skipna:\n            isna_both = (isna_array(self.values, include_none=False)\n                    & isna_array(other.values, include_none=False))\n            eq[isna_both] = True\n\n        if not eq.all():\n            return False\n        return True\n\n    def sort(self,\n            ascending: bool = True,\n            kind: str = DEFAULT_SORT_KIND) -> \'Index\':\n        \'\'\'Return a new Index with the labels sorted.\n\n        Args:\n            kind: Sort algorithm passed to NumPy.\n        \'\'\'\n        # force usage of property for caching\n        v = np.sort(self.values, kind=kind)\n        if not ascending:\n            v = v[::-1]\n\n        v.flags.writeable = False\n        return self.__class__(v, name=self._name)\n\n    def isin(self, other: tp.Iterable[tp.Any]) -> np.ndarray:\n        \'\'\'\n        Return a Boolean array showing True where a label is found in other. If other is a multidimensional array, it is flattened.\n        \'\'\'\n        return isin(self.values, other, array_is_unique=True)\n\n    def roll(self, shift: int) -> \'Index\':\n        \'\'\'Return an Index with values rotated forward and wrapped around (with a postive shift) or backward and wrapped around (with a negative shift).\n        \'\'\'\n        values = self.values # force usage of property for cache update\n        if shift % len(values):\n            values = array_shift(\n                    array=values,\n                    shift=shift,\n                    axis=0,\n                    wrap=True)\n            values.flags.writeable = False\n        return self.__class__(values, name=self._name)\n\n    #---------------------------------------------------------------------------\n    # export\n\n    def to_series(self) -> \'Series\':\n        \'\'\'Return a Series with values from this Index\'s labels.\n        \'\'\'\n        # NOTE: while we might re-use the index on the index returned from this Series, such an approach will not work with IndexHierarchy.to_frame, as we do not know if the index should be on the index or columns; thus, returning an unindexed Series is appropriate\n        from static_frame import Series\n        return Series(self.values, name=self._name)\n\n    def add_level(self, level: tp.Hashable) -> \'IndexHierarchy\':\n        \'\'\'Return an IndexHierarhcy with an added root level.\n        \'\'\'\n        from static_frame import IndexHierarchy\n        return IndexHierarchy.from_tree({level: self.values})\n\n    def to_pandas(self) -> \'pandas.Index\':\n        \'\'\'Return a Pandas Index.\n        \'\'\'\n        import pandas\n        # must copy to remove immutability, decouple reference\n        return pandas.Index(self.values.copy(),\n                name=self._name)\n\n#-------------------------------------------------------------------------------\n_INDEX_GO_SLOTS = (\n        \'_map\',\n        \'_labels\',\n        \'_positions\',\n        \'_recache\',\n        \'_name\',\n        \'_labels_mutable\',\n        \'_labels_mutable_dtype\',\n        \'_positions_mutable_count\',\n        )\n\n\nclass _IndexGOMixin:\n\n    STATIC = False\n    __slots__ = () # define in derived class\n\n    _map: tp.Optional[AutoMap]\n    _labels_mutable: tp.List[tp.Hashable]\n    _labels_mutable_dtype: np.dtype\n    _positions_mutable_count: int\n    _positions: np.ndarray\n    _labels: np.ndarray\n\n\n    def _extract_labels(self,\n            mapping: tp.Optional[tp.Dict[tp.Hashable, int]],\n            labels: np.ndarray,\n            dtype: tp.Optional[np.dtype] = None\n            ) -> np.ndarray:\n        \'\'\'Called in Index.__init__(). This creates and populates mutable storage as a side effect of array derivation; this storage will be grown as needed.\n        \'\'\'\n        labels = Index._extract_labels(mapping, labels, dtype)\n        self._labels_mutable = labels.tolist()\n        if len(labels):\n            self._labels_mutable_dtype = labels.dtype\n        else: # avoid setting to float default when labels is empty\n            self._labels_mutable_dtype = None\n        return labels\n\n    def _extract_positions(self,\n            size: int,\n            positions: tp.Optional[tp.Sequence[int]]\n            ) -> np.ndarray:\n        \'\'\'Called in Index.__init__(). This creates and populates mutable storage. This creates and populates mutable storage as a side effect of array derivation.\n        \'\'\'\n        positions = Index._extract_positions(size, positions)\n        self._positions_mutable_count = size\n        return positions\n\n    def _update_array_cache(self) -> None:\n\n        if self._labels_mutable_dtype is not None and len(self._labels):\n            # only update if _labels_mutable_dtype has been set and _labels exist\n            self._labels_mutable_dtype = resolve_dtype(\n                    self._labels.dtype,\n                    self._labels_mutable_dtype)\n\n        self._labels = np.array(self._labels_mutable, dtype=self._labels_mutable_dtype)\n        self._labels.flags.writeable = False\n        self._positions = PositionsAllocator.get(self._positions_mutable_count)\n        self._recache = False\n\n    #---------------------------------------------------------------------------\n    # grow only mutation\n\n    def append(self, value: tp.Hashable) -> None:\n        \'\'\'append a value\n        \'\'\'\n        if self.__contains__(value): #type: ignore\n            raise KeyError(f\'duplicate key append attempted: {value}\')\n\n        # we might need to initialize map if not an increment that keeps loc_is_iloc relationship\n        initialize_map = False\n        if self._map is None: # loc_is_iloc\n            if not (isinstance(value, INT_TYPES)\n                    and value == self._positions_mutable_count):\n                initialize_map = True\n        else:\n            self._map.add(value)\n\n        if self._labels_mutable_dtype is not None:\n            self._labels_mutable_dtype = resolve_dtype(\n                    np.array(value).dtype,\n                    self._labels_mutable_dtype)\n        else:\n            self._labels_mutable_dtype = np.array(value).dtype\n\n        self._labels_mutable.append(value)\n\n        if initialize_map:\n            self._map = AutoMap(self._labels_mutable)\n\n        self._positions_mutable_count += 1\n        self._recache = True\n\n    def extend(self, values: KeyIterableTypes) -> None:\n        \'\'\'Append multiple values\n        Args:\n            values: can be a generator.\n        \'\'\'\n        for value in values:\n            self.append(value)\n\n\n@doc_inject(selector=\'index_init\')\nclass IndexGO(_IndexGOMixin, Index):\n    \'\'\'A mapping of labels to positions, immutable with grow-only size. Used as columns in :obj:`FrameGO`.\n\n    {args}\n    \'\'\'\n    _IMMUTABLE_CONSTRUCTOR = Index\n\n    __slots__ = _INDEX_GO_SLOTS\n\n\n# update class attr on Index after class initialziation\nIndex._MUTABLE_CONSTRUCTOR = IndexGO\n\n\n\n#-------------------------------------------------------------------------------\n\ndef _index_initializer_needs_init(\n        value: tp.Optional[IndexInitializer]\n        ) -> bool:\n    \'\'\'Determine if value is a non-empty index initializer. This could almost just be a truthy test, but ndarrays need to be handled in isolation. Generators should return True.\n    \'\'\'\n    if value is None:\n        return False\n    if isinstance(value, IndexBase):\n        return False\n    if isinstance(value, np.ndarray):\n        return bool(len(value))\n    return bool(value)\n\n'"
static_frame/core/index_auto.py,0,"b""\nimport typing as tp\n\nfrom static_frame.core.index_base import IndexBase  # pylint: disable = W0611\n\n\nfrom static_frame.core.index import Index\n# from static_frame.core.index_hierarchy import IndexHierarchy\n\nfrom static_frame.core.index import IndexGO\nfrom static_frame.core.index import PositionsAllocator\n\nfrom static_frame.core.util import IndexConstructor\n\nfrom static_frame.core.util import DTYPE_INT_DEFAULT\n\n\nIndexAutoInitializer = int\n\n\n# could create trival subclasses for these indices, but the type would would not always describe the instance; for example, an IndexAutoGO could grow inot non-contiguous integer index, as loc_is_iloc is reevaluated with each append can simply go to false.\n\nclass IndexAutoFactory:\n\n    @classmethod\n    def from_optional_constructor(cls,\n            initializer: IndexAutoInitializer,\n            *,\n            default_constructor: tp.Type['IndexBase'],\n            explicit_constructor: tp.Optional[IndexConstructor] = None,\n            ) -> 'IndexBase':\n\n        # get an immutable array, shared from positions allocator\n        labels = PositionsAllocator.get(initializer)\n\n        if explicit_constructor:\n            return explicit_constructor(labels)\n\n        else: # get from default constructor\n            constructor = Index if default_constructor.STATIC else IndexGO\n            return constructor(\n                    labels=labels,\n                    loc_is_iloc=True,\n                    dtype=DTYPE_INT_DEFAULT\n                    )\n\n\nIndexAutoFactoryType = tp.Type[IndexAutoFactory]\n"""
static_frame/core/index_base.py,39,"b""import typing as tp\nimport numpy as np\n\n\nfrom static_frame.core.util import PathSpecifierOrFileLike\nfrom static_frame.core.util import write_optional_file\n# from static_frame.core.util import IndexInitializer\n# from static_frame.core.util import IndexConstructor\nfrom static_frame.core.util import UFunc\nfrom static_frame.core.util import GetItemKeyType\nfrom static_frame.core.util import KeyTransformType\nfrom static_frame.core.util import NameType\n\nfrom static_frame.core.display import DisplayFormats\nfrom static_frame.core.display import DisplayActive\nfrom static_frame.core.display import DisplayConfig\nfrom static_frame.core.display import Display\n\nfrom static_frame.core.doc_str import doc_inject\nfrom static_frame.core.container import ContainerOperand\n# from static_frame.core.node_selector import InterfaceGetItem\n# from static_frame.core.node_selector import TContainer\n# from static_frame.core.exception import ErrorInitIndex\n\nif tp.TYPE_CHECKING:\n    import pandas #pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.series import Series #pylint: disable=W0611 #pragma: no cover\n\n\nI = tp.TypeVar('I', bound='IndexBase')\n\nclass IndexBase(ContainerOperand):\n    '''\n    All indices are dervied from ``IndexBase``, including ``Index`` and ``IndexHierarchy``.\n    '''\n\n    __slots__ = () # defined in dervied classes\n\n    #---------------------------------------------------------------------------\n    # type defs\n\n    _recache: bool\n    _name: NameType\n    values: np.ndarray\n    depth: int\n    iloc: tp.Any # this does not work: InterfaceGetItem[I]\n\n    __pos__: tp.Callable[['IndexBase'], np.ndarray]\n    __neg__: tp.Callable[['IndexBase'], np.ndarray]\n    __abs__: tp.Callable[['IndexBase'], np.ndarray]\n    __invert__: tp.Callable[['IndexBase'], np.ndarray]\n    __add__: tp.Callable[['IndexBase', object], np.ndarray]\n    __sub__: tp.Callable[['IndexBase', object], np.ndarray]\n    __mul__: tp.Callable[['IndexBase', object], np.ndarray]\n    __matmul__: tp.Callable[['IndexBase', object], np.ndarray]\n    __truediv__: tp.Callable[['IndexBase', object], np.ndarray]\n    __floordiv__: tp.Callable[['IndexBase', object], np.ndarray]\n    __mod__: tp.Callable[['IndexBase', object], np.ndarray]\n    # __divmod__: tp.Callable[['IndexBase', object], np.ndarray]\n    __pow__: tp.Callable[['IndexBase', object], np.ndarray]\n    __lshift__: tp.Callable[['IndexBase', object], np.ndarray]\n    __rshift__: tp.Callable[['IndexBase', object], np.ndarray]\n    __and__: tp.Callable[['IndexBase', object], np.ndarray]\n    __xor__: tp.Callable[['IndexBase', object], np.ndarray]\n    __or__: tp.Callable[['IndexBase', object], np.ndarray]\n    __lt__: tp.Callable[['IndexBase', object], np.ndarray]\n    __le__: tp.Callable[['IndexBase', object], np.ndarray]\n    __eq__: tp.Callable[['IndexBase', object], np.ndarray]\n    __ne__: tp.Callable[['IndexBase', object], np.ndarray]\n    __gt__: tp.Callable[['IndexBase', object], np.ndarray]\n    __ge__: tp.Callable[['IndexBase', object], np.ndarray]\n    __radd__: tp.Callable[['IndexBase', object], np.ndarray]\n    __rsub__: tp.Callable[['IndexBase', object], np.ndarray]\n    __rmul__: tp.Callable[['IndexBase', object], np.ndarray]\n    __rtruediv__: tp.Callable[['IndexBase', object], np.ndarray]\n    __rfloordiv__: tp.Callable[['IndexBase', object], np.ndarray]\n    # __len__: tp.Callable[['IndexBase'], int]\n\n    _IMMUTABLE_CONSTRUCTOR: tp.Callable[..., 'IndexBase']\n    _MUTABLE_CONSTRUCTOR: tp.Callable[..., 'IndexBase']\n\n    _UFUNC_UNION: tp.Callable[[np.ndarray, np.ndarray, bool], np.ndarray]\n    _UFUNC_INTERSECTION: tp.Callable[[np.ndarray, np.ndarray, bool], np.ndarray]\n    _UFUNC_DIFFERENCE: tp.Callable[[np.ndarray, np.ndarray, bool], np.ndarray]\n\n    label_widths_at_depth: tp.Callable[[I, int], tp.Iterator[tp.Tuple[tp.Hashable, int]]]\n\n    loc_to_iloc: tp.Callable[\n            [GetItemKeyType, tp.Optional[int], KeyTransformType],\n            GetItemKeyType\n            ]\n\n    #---------------------------------------------------------------------------\n    # base class interface, mostly for mypy\n\n    def _ufunc_axis_skipna(self, *,\n            axis: int,\n            skipna: bool,\n            ufunc: UFunc,\n            ufunc_skipna: UFunc,\n            composable: bool,\n            dtypes: tp.Tuple[np.dtype, ...],\n            size_one_unity: bool\n            ) -> np.ndarray:\n        raise NotImplementedError()\n\n    def _extract_iloc(self: I, key: GetItemKeyType) -> tp.Union[I, tp.Hashable]:\n        raise NotImplementedError()\n\n    def _update_array_cache(self) -> None:\n        raise NotImplementedError()\n\n    def copy(self: I) -> I:\n        raise NotImplementedError()\n\n    def display(self, config: tp.Optional[DisplayConfig] = None) -> Display:\n        raise NotImplementedError()\n\n    @classmethod\n    def from_labels(cls: tp.Type[I],\n            labels: tp.Iterable[tp.Sequence[tp.Hashable]],\n            *,\n            name: tp.Optional[tp.Hashable] = None\n            ) -> I:\n        raise NotImplementedError()\n\n    def __init__(self, initializer: tp.Any = None,\n            *,\n            name: tp.Optional[tp.Hashable] = None\n            ):\n        # trivial init for mypy; not called by derived class\n        pass\n\n    def __len__(self) -> int:\n        raise NotImplementedError()\n\n    def __iter__(self) -> tp.Iterator[tp.Hashable]:\n        raise NotImplementedError()\n\n\n    #---------------------------------------------------------------------------\n    # constructors\n\n    @classmethod\n    def from_pandas(cls,\n            value: 'pandas.DataFrame',\n            ) -> 'IndexBase':\n        '''\n        Given a Pandas index, return the appropriate IndexBase derived class.\n        '''\n        import pandas\n        from static_frame import Index\n        from static_frame import IndexGO\n        from static_frame import IndexHierarchy\n        from static_frame import IndexHierarchyGO\n        from static_frame import IndexNanosecond\n        from static_frame import IndexNanosecondGO\n        from static_frame.core.index_datetime import IndexDatetime\n\n        if isinstance(value, pandas.MultiIndex):\n            # iterating over a hierarchucal index will iterate over labels\n            name = tuple(value.names)\n            if not cls.STATIC:\n                return IndexHierarchyGO.from_labels(value, name=name)\n            return IndexHierarchy.from_labels(value, name=name)\n        elif isinstance(value, pandas.DatetimeIndex):\n            # if IndexDatetime, use cls, else use IndexNanosecond\n            if issubclass(cls, IndexDatetime):\n                return cls(value, name=value.name)\n            else:\n                if not cls.STATIC:\n                    return IndexNanosecondGO(value, name=value.name)\n                return IndexNanosecond(value, name=value.name)\n\n        if not cls.STATIC:\n            return IndexGO(value, name=value.name)\n        return Index(value, name=value.name)\n\n    #---------------------------------------------------------------------------\n    # name interface\n\n    @property #type: ignore\n    @doc_inject()\n    def name(self) -> NameType:\n        '''{}'''\n        return self._name\n\n    @property\n    def names(self) -> tp.Tuple[str, ...]:\n        '''\n        Provide a suitable iterable of names for usage in output formats that require a field name as string for the index.\n        '''\n        template = '__index{}__' # arrow does __index_level_0__\n        depth = self.depth\n        name = self._name\n\n        def gen() -> tp.Iterator[str]:\n            if name and depth == 1:\n                yield str(name)\n            # try to use name only if it is a tuple of the right size\n            elif name and isinstance(name, tuple) and len(name) == depth:\n                for n in name:\n                    yield str(n)\n            else:\n                for i in range(depth):\n                    yield template.format(i)\n\n        names = tuple(gen())\n        # if len(names) != depth:\n        #     raise RuntimeError(f'unexpected names formation: {names}, does not meet depth {depth}')\n        return names\n\n    #---------------------------------------------------------------------------\n    # transformations resulting in reduced dimensionality\n\n    @doc_inject(selector='head', class_name='Index')\n    def head(self: I, count: int = 5) -> I:\n        '''{doc}\n\n        Args:\n            {count}\n        '''\n        return self.iloc[:count] #type: ignore\n\n    @doc_inject(selector='tail', class_name='Index')\n    def tail(self: I, count: int = 5) -> I:\n        '''{doc}\n\n        Args:\n            {count}\n        '''\n        return self.iloc[-count:] #type: ignore\n\n    #---------------------------------------------------------------------------\n    # set operations\n\n    def _ufunc_set(self: I,\n            func: tp.Callable[[np.ndarray, np.ndarray, bool], np.ndarray],\n            other: tp.Union['IndexBase', 'Series']\n            ) -> I:\n        '''\n        Utility function for preparing and collecting values for Indices to produce a new Index.\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        if isinstance(other, np.ndarray):\n            opperand = other\n            assume_unique = False\n        elif isinstance(other, IndexBase):\n            opperand = other.values\n            assume_unique = True # can always assume unique\n        elif isinstance(other, ContainerOperand):\n            opperand = other.values\n            assume_unique = False\n        else:\n            raise NotImplementedError(f'no support for {other}')\n\n        cls = self.__class__\n\n        # using assume_unique will permit retaining order when opperands are identical\n        labels = func(self.values, opperand, assume_unique=assume_unique) # type: ignore\n\n        if id(labels) == id(self.values):\n            # NOTE: favor using cls constructor here as it permits maximal sharing of static resources and the underlying dictionary\n            return cls(self)\n        return cls.from_labels(labels)\n\n\n    def intersection(self: I, other: tp.Union['IndexBase', 'Series']) -> I:\n        '''\n        Perform intersection with another Index, container, or NumPy array. Identical comparisons retain order.\n        '''\n        # NOTE: must get UFunc off of class to avoid automatic addition of self to signature\n        return self._ufunc_set(\n                self.__class__._UFUNC_INTERSECTION,\n                other)\n\n    def union(self: I, other: 'IndexBase') -> I:\n        '''\n        Perform union with another Index, container, or NumPy array. Identical comparisons retain order.\n        '''\n        return self._ufunc_set(\n                self.__class__._UFUNC_UNION,\n                other)\n\n    def difference(self: I, other: 'IndexBase') -> I:\n        '''\n        Perform difference with another Index, container, or NumPy array. Retains order.\n        '''\n        return self._ufunc_set(\n                self.__class__._UFUNC_DIFFERENCE,\n                other)\n\n\n    #---------------------------------------------------------------------------\n    # metaclass-applied functions\n\n    def _ufunc_shape_skipna(self, *,\n            axis: int,\n            skipna: bool,\n            ufunc: UFunc,\n            ufunc_skipna: UFunc,\n            composable: bool,\n            dtypes: tp.Tuple[np.dtype, ...],\n            size_one_unity: bool\n            ) -> np.ndarray:\n        '''\n        For Index and IndexHierarchy, _ufunc_shape_skipna and _ufunc_axis_skipna are defined the same.\n\n        Returns:\n            immutable NumPy array.\n        '''\n        return self._ufunc_axis_skipna(\n                axis=axis,\n                skipna=skipna,\n                ufunc=ufunc,\n                ufunc_skipna=ufunc_skipna,\n                composable=composable, # shape on axis 1 is never composable\n                dtypes=dtypes,\n                size_one_unity=size_one_unity\n                )\n\n\n\n    #---------------------------------------------------------------------------\n    # exporters\n\n    @doc_inject(class_name='Index')\n    def to_html(self,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> str:\n        '''\n        {}\n        '''\n        config = config or DisplayActive.get(type_show=False)\n        config = config.to_display_config(\n                display_format=DisplayFormats.HTML_TABLE,\n                )\n        return repr(self.display(config))\n\n    @doc_inject(class_name='Index')\n    def to_html_datatables(self,\n            fp: tp.Optional[PathSpecifierOrFileLike] = None,\n            *,\n            show: bool = True,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> tp.Optional[str]:\n        '''\n        {}\n        '''\n        config = config or DisplayActive.get(type_show=False)\n        config = config.to_display_config(\n                display_format=DisplayFormats.HTML_DATATABLES,\n                )\n        content = repr(self.display(config))\n        # path_filter called internally\n        fp = write_optional_file(content=content, fp=fp)\n\n        if fp and show:\n            import webbrowser #pragma: no cover\n            webbrowser.open_new_tab(fp) #pragma: no cover\n\n        return fp\n"""
static_frame/core/index_correspondence.py,1,"b""import typing as tp\n\nimport numpy as np\n\nfrom static_frame.core.util import GetItemKeyType\nfrom static_frame.core.util import intersect1d\nfrom static_frame.core.util import intersect2d\nfrom static_frame.core.util import array2d_to_tuples\nfrom static_frame.core.util import DTYPE_BOOL\n\n\nif tp.TYPE_CHECKING:\n\n    from static_frame.core.index import Index  # pylint: disable = W0611 #pragma: no cover\n\n\nclass IndexCorrespondence:\n    '''\n    All iloc data necessary for reindexing.\n    '''\n\n    __slots__ = (\n            'has_common',\n            'is_subset',\n            'iloc_src',\n            'iloc_dst',\n            'size',\n            )\n\n    has_common: bool\n    is_subset: bool\n    iloc_src: GetItemKeyType\n    iloc_dst: GetItemKeyType\n    size: int\n\n    @classmethod\n    def from_correspondence(cls,\n            src_index: 'Index',\n            dst_index: 'Index') -> 'IndexCorrespondence':\n        '''\n        Return an IndexCorrespondence instance from the correspondence of two Index or IndexHierarchy objects.\n        '''\n        mixed_depth = False\n        if src_index.depth == dst_index.depth:\n            depth = src_index.depth\n        else:\n            # if dimensions are mixed, the only way there can be a match is if the 1D index is of object type (so it can hold a tuple); otherwise, there can be no matches;\n            if src_index.depth == 1 and src_index.values.dtype.kind == 'O':\n                depth = dst_index.depth\n                mixed_depth = True\n            elif dst_index.depth == 1 and dst_index.values.dtype.kind == 'O':\n                depth = src_index.depth\n                mixed_depth = True\n            else:\n                depth = 0\n\n        # need to use lower level array methods go get intersection, rather than Index methods, as need arrays, not Index objects\n        if depth == 1:\n            # NOTE: this can fail in some cases: comparing two object arrays with NaNs and strings.\n            common_labels = intersect1d(\n                    src_index.values,\n                    dst_index.values,\n                    assume_unique=True\n                    )\n            has_common = len(common_labels) > 0\n            assert not mixed_depth\n        elif depth > 1:\n            # if either values arrays are object, we have to covert all values to tuples\n            common_labels = intersect2d(\n                    src_index.values,\n                    dst_index.values,\n                    assume_unique=True\n                    )\n            if mixed_depth:\n                # when mixed, on the 1D index we have to use loc_to_iloc with tuples\n                common_labels = list(array2d_to_tuples(common_labels))\n            has_common = len(common_labels) > 0\n        else:\n            has_common = False\n\n        size = len(dst_index.values)\n\n        # either a reordering or a subset\n        if has_common:\n\n            if len(common_labels) == len(dst_index):\n                # use new index to retain order\n                values_dst = dst_index.values\n                if values_dst.dtype == DTYPE_BOOL:\n                    # if the index values are a Boolean array, loc_to_iloc will try to do a Boolean selection, which is incorrect. Using a list avoids this problem.\n                    iloc_src = src_index.loc_to_iloc(values_dst.tolist())\n                else:\n                    iloc_src = src_index.loc_to_iloc(values_dst)\n                iloc_dst = np.arange(size)\n                return cls(has_common=has_common,\n                        is_subset=True,\n                        iloc_src=iloc_src,\n                        iloc_dst=iloc_dst,\n                        size=size\n                        )\n\n            # these will be equal sized\n            iloc_src = src_index.loc_to_iloc(common_labels)\n            iloc_dst = dst_index.loc_to_iloc(common_labels)\n\n            return cls(has_common=has_common,\n                    is_subset=False,\n                    iloc_src=iloc_src,\n                    iloc_dst=iloc_dst,\n                    size=size)\n\n        # if no common values, nothing to transfer from src to dst\n        return cls(has_common=has_common,\n                is_subset=False,\n                iloc_src=None,\n                iloc_dst=None,\n                size=size)\n\n\n    def __init__(self,\n            has_common: bool,\n            is_subset: bool,\n            iloc_src: GetItemKeyType,\n            iloc_dst: GetItemKeyType,\n            size: int) -> None:\n        '''\n        Args:\n            has_common: True if any of the indices align\n            is_subset: True if the destination is a reordering or subset\n            iloc_src: An iterable of iloc values to be taken from the source\n            iloc_dst: An iterable of iloc values to be written to\n            size: The size of the destination.\n        '''\n        self.has_common = has_common\n        self.is_subset = is_subset\n        self.iloc_src = iloc_src\n        self.iloc_dst = iloc_dst\n        self.size = size\n\n    def iloc_src_fancy(self) -> tp.List[tp.List[int]]:\n        '''\n        Convert an iloc iterable of integers into one that is combitable with fancy indexing.\n        '''\n        return [[x] for x in self.iloc_src] #type: ignore\n"""
static_frame/core/index_datetime.py,24,"b""import typing as tp\nimport datetime\n\nimport numpy as np\n\nfrom automap import AutoMap  # pylint: disable = E0611\n\n# from static_frame.core.util import DTYPE_DATETIME_KIND\n\nfrom static_frame.core.util import GetItemKeyType\nfrom static_frame.core.util import IndexInitializer\n# from static_frame.core.util import mloc\nfrom static_frame.core.util import key_to_datetime_key\n\nfrom static_frame.core.util import DateInitializer\nfrom static_frame.core.util import YearMonthInitializer\nfrom static_frame.core.util import YearInitializer\n\nfrom static_frame.core.util import to_datetime64\nfrom static_frame.core.util import to_timedelta64\n\nfrom static_frame.core.util import DT64_YEAR\nfrom static_frame.core.util import DT64_MONTH\nfrom static_frame.core.util import DT64_DAY\nfrom static_frame.core.util import DT64_M\nfrom static_frame.core.util import DT64_H\nfrom static_frame.core.util import DT64_S\nfrom static_frame.core.util import DT64_MS\nfrom static_frame.core.util import DT64_US\nfrom static_frame.core.util import DT64_NS\n\nfrom static_frame.core.util import TD64_DAY\nfrom static_frame.core.util import TD64_MONTH\nfrom static_frame.core.util import TD64_YEAR\n\nfrom static_frame.core.index import _IndexGOMixin\nfrom static_frame.core.index import _INDEX_SLOTS\nfrom static_frame.core.index import _INDEX_GO_SLOTS\n\nfrom static_frame.core.index import Index\nfrom static_frame.core.index import IndexGO\n\nfrom static_frame.core.doc_str import doc_inject\n\n\nif tp.TYPE_CHECKING:\n    import pandas  #pylint: disable = W0611 #pragma: no cover\n\nI = tp.TypeVar('I', bound='IndexDatetime')\n\n#-------------------------------------------------------------------------------\n# Specialized index for dates\n\nclass IndexDatetime(Index):\n    '''\n    Derivation of Index to support Datetime operations. Derived classes must define _DTYPE.\n    '''\n\n    STATIC = True\n    _DTYPE = None # define in derived class\n    __slots__ = _INDEX_SLOTS\n\n    def __init__(self,\n            labels: IndexInitializer,\n            *,\n            name: tp.Optional[tp.Hashable] = None\n            ):\n        # __init__ here leaves out the dtype argument, reducing the signature to arguments relevant for these derived classes\n        Index.__init__(self, labels=labels, name=name)\n\n    #---------------------------------------------------------------------------\n    # dict like interface\n\n    def __contains__(self, value: tp.Any) -> bool:\n        '''Return True if value in the labels. Will only return True for an exact match to the type of dates stored within.\n        '''\n        return self._map.__contains__(to_datetime64(value)) #type: ignore\n\n    #---------------------------------------------------------------------------\n    # operators\n\n    def _ufunc_binary_operator(self, *,\n            operator: tp.Callable[..., tp.Any],\n            other: object) -> np.ndarray:\n\n        if self._recache:\n            self._update_array_cache()\n\n        if operator.__name__ == 'matmul' or operator.__name__ == 'rmatmul':\n            raise NotImplementedError('matrix multiplication not supported')\n\n        if isinstance(other, Index):\n            other = other.values # operate on labels to labels\n        elif isinstance(other, str):\n            # do not pass dtype, as want to coerce to this parsed type, not the type of sled\n            other = to_datetime64(other)\n\n        if isinstance(other, np.datetime64):\n            # convert labels to other's datetime64 type to enable matching on month, year, etc.\n            array = operator(self._labels.astype(other.dtype), other)\n        elif isinstance(other, datetime.timedelta):\n            array = operator(self._labels, to_timedelta64(other))\n        else:\n            # np.timedelta64 should work fine here\n            array = operator(self._labels, other)\n\n        array.flags.writeable = False\n        return array\n\n    def loc_to_iloc(self,  # type: ignore\n            key: GetItemKeyType,\n            offset: tp.Optional[int] = None,\n            ) -> GetItemKeyType:\n        '''\n        Specialized for IndexData indices to convert string data representations into np.datetime64 objects as appropriate.\n        '''\n        # not passing self.dtype to key_to_datetime_key so as to allow translation to a foreign datetime; slice comparison will be handled by map_slice_args\n        return Index.loc_to_iloc(self,\n                key=key,\n                offset=offset,\n                key_transform=key_to_datetime_key)\n\n    #---------------------------------------------------------------------------\n    def to_pandas(self) -> 'pandas.DatetimeIndex':\n        '''Return a Pandas Index.\n        '''\n        import pandas\n        return pandas.DatetimeIndex(self.values.copy(),\n                name=self._name)\n\n#-------------------------------------------------------------------------------\nclass _IndexDatetimeGOMixin(_IndexGOMixin):\n\n    _DTYPE: tp.Optional[np.dtype]\n    _map: tp.Optional[AutoMap]\n    __slots__ = () # define in derived class\n\n    def append(self, value: tp.Hashable) -> None:\n        '''Specialize for fixed-typed indices: convert `value` argument; do not need to resolve_dtype with each addition; self._map is never None\n        '''\n        value = to_datetime64(value, self._DTYPE)\n        if self._map is not None:\n            try:\n                self._map.add(value)\n            except ValueError:\n                raise KeyError(f'duplicate key append attempted: {value}')\n        self._labels_mutable.append(value)\n        self._positions_mutable_count += 1 #pylint: disable=E0237\n        self._recache = True #pylint: disable=E0237\n\n#-------------------------------------------------------------------------------\n@doc_inject(selector='index_date_time_init')\nclass IndexYear(IndexDatetime):\n    '''A mapping of years (via NumPy datetime64[Y]) to positions, immutable and of fixed size.\n\n    {args}\n    '''\n    STATIC = True\n    _DTYPE = DT64_YEAR\n    __slots__ = _INDEX_SLOTS\n\n    @classmethod\n    def from_date_range(cls: tp.Type[I],\n            start: DateInitializer,\n            stop: DateInitializer,\n            step: int = 1,\n            *,\n            name: tp.Optional[tp.Hashable] = None) -> I:\n        '''\n        Get an IndexYearMonth instance over a range of dates, where start and stop are inclusive.\n        '''\n        labels = np.arange(\n                to_datetime64(start, DT64_DAY),\n                to_datetime64(stop, DT64_DAY).astype(DT64_YEAR) + TD64_YEAR,\n                np.timedelta64(step, 'Y'),\n                dtype=DT64_YEAR)\n        labels.flags.writeable = False\n        return cls(labels, name=name)\n\n    @classmethod\n    def from_year_month_range(cls: tp.Type[I],\n            start: YearMonthInitializer,\n            stop: YearMonthInitializer,\n            step: int = 1,\n            *,\n            name: tp.Optional[tp.Hashable] = None\n            ) -> I:\n        '''\n        Get an IndexYearMonth instance over a range of months, where start and end are inclusive.\n        '''\n\n        labels = np.arange(\n                to_datetime64(start, DT64_MONTH),\n                to_datetime64(stop, DT64_MONTH).astype(DT64_YEAR) + TD64_YEAR,\n                np.timedelta64(step, 'Y'),\n                dtype=DT64_YEAR)\n        labels.flags.writeable = False\n        return cls(labels, name=name)\n\n\n    @classmethod\n    def from_year_range(cls: tp.Type[I],\n            start: YearInitializer,\n            stop: YearInitializer,\n            step: int = 1,\n            *,\n            name: tp.Optional[tp.Hashable] = None\n            ) -> I:\n        '''\n        Get an IndexDate instance over a range of years, where start and end are inclusive.\n        '''\n        labels = np.arange(\n                to_datetime64(start, DT64_YEAR),\n                to_datetime64(stop, DT64_YEAR) + TD64_YEAR,\n                step=np.timedelta64(step, 'Y'),\n                )\n        labels.flags.writeable = False\n        return cls(labels, name=name)\n\n    #---------------------------------------------------------------------------\n    def to_pandas(self) -> None:\n        '''Return a Pandas Index.\n        '''\n        raise NotImplementedError('Pandas does not support a year type, and it is ambiguous if a date proxy should be the first of the year or the last of the year.')\n\n\nclass IndexYearGO(_IndexDatetimeGOMixin, IndexYear):\n\n    _IMMUTABLE_CONSTRUCTOR = IndexYear\n    __slots__ = _INDEX_GO_SLOTS\n\nIndexYear._MUTABLE_CONSTRUCTOR = IndexYearGO\n\n#-------------------------------------------------------------------------------\n@doc_inject(selector='index_date_time_init')\nclass IndexYearMonth(IndexDatetime):\n    '''A mapping of year months (via NumPy datetime64[M]) to positions, immutable and of fixed size.\n\n    {args}\n    '''\n    STATIC = True\n    _DTYPE = DT64_MONTH\n    __slots__ = _INDEX_SLOTS\n\n    @classmethod\n    def from_date_range(cls: tp.Type[I],\n            start: DateInitializer,\n            stop: DateInitializer,\n            step: int = 1,\n            *,\n            name: tp.Optional[tp.Hashable] = None\n            ) -> I:\n        '''\n        Get an IndexYearMonth instance over a range of dates, where start and stop is inclusive.\n        '''\n        labels = np.arange(\n                to_datetime64(start, DT64_DAY),\n                to_datetime64(stop, DT64_DAY).astype(DT64_MONTH) + TD64_MONTH,\n                np.timedelta64(step, 'M'),\n                dtype=DT64_MONTH)\n\n        labels.flags.writeable = False\n        return cls(labels, name=name)\n\n    @classmethod\n    def from_year_month_range(cls: tp.Type[I],\n            start: YearMonthInitializer,\n            stop: YearMonthInitializer,\n            step: int = 1,\n            *,\n            name: tp.Optional[tp.Hashable] = None\n            ) -> I:\n        '''\n        Get an IndexYearMonth instance over a range of months, where start and end are inclusive.\n        '''\n\n        labels = np.arange(\n                to_datetime64(start, DT64_MONTH),\n                to_datetime64(stop, DT64_MONTH) + TD64_MONTH,\n                np.timedelta64(step, 'M'),\n                dtype=DT64_MONTH)\n        labels.flags.writeable = False\n        return cls(labels, name=name)\n\n\n    @classmethod\n    def from_year_range(cls: tp.Type[I],\n            start: YearInitializer,\n            stop: YearInitializer,\n            step: int = 1,\n            *,\n            name: tp.Optional[tp.Hashable] = None\n            ) -> I:\n        '''\n        Get an IndexYearMonth instance over a range of years, where start and end are inclusive.\n        '''\n        labels = np.arange(\n                to_datetime64(start, DT64_YEAR),\n                to_datetime64(stop, DT64_YEAR) + TD64_YEAR,\n                step=np.timedelta64(step, 'M'),\n                dtype=DT64_MONTH)\n        labels.flags.writeable = False\n        return cls(labels, name=name)\n\n    #---------------------------------------------------------------------------\n    def to_pandas(self) -> None:\n        '''Return a Pandas Index.\n        '''\n        raise NotImplementedError('Pandas does not support a year month type, and it is ambiguous if a date proxy should be the first of the month or the last of the month.')\n\n\nclass IndexYearMonthGO(_IndexDatetimeGOMixin, IndexYearMonth):\n\n    _IMMUTABLE_CONSTRUCTOR = IndexYearMonth\n    __slots__ = _INDEX_GO_SLOTS\n\nIndexYearMonth._MUTABLE_CONSTRUCTOR = IndexYearMonthGO\n\n#-------------------------------------------------------------------------------\n\n@doc_inject(selector='index_date_time_init')\nclass IndexDate(IndexDatetime):\n    '''A mapping of dates (via NumPy datetime64[D]) to positions, immutable and of fixed size.\n\n    {args}\n    '''\n    STATIC = True\n    _DTYPE = DT64_DAY\n    __slots__ = _INDEX_SLOTS\n\n    @classmethod\n    def from_date_range(cls: tp.Type[I],\n            start: DateInitializer,\n            stop: DateInitializer,\n            step: int = 1,\n            *,\n            name: tp.Optional[tp.Hashable] = None\n            ) -> I:\n        '''\n        Get an IndexDate instance over a range of dates, where start and stop is inclusive.\n        '''\n        labels = np.arange(\n                to_datetime64(start, DT64_DAY),\n                to_datetime64(stop, DT64_DAY) + TD64_DAY,\n                np.timedelta64(step, 'D'))\n        labels.flags.writeable = False\n        return cls(labels, name=name)\n\n    @classmethod\n    def from_year_month_range(cls: tp.Type[I],\n            start: YearMonthInitializer,\n            stop: YearMonthInitializer,\n            step: int = 1,\n            *,\n            name: tp.Optional[tp.Hashable] = None) -> I:\n        '''\n        Get an IndexDate instance over a range of months, where start and end are inclusive.\n        '''\n        labels = np.arange(\n                to_datetime64(start, DT64_MONTH),\n                to_datetime64(stop, DT64_MONTH) + TD64_MONTH,\n                step=np.timedelta64(step, 'D'),\n                dtype=DT64_DAY)\n        labels.flags.writeable = False\n        return cls(labels, name=name)\n\n    @classmethod\n    def from_year_range(cls: tp.Type[I],\n            start: YearInitializer,\n            stop: YearInitializer,\n            step: int = 1,\n            *,\n            name: tp.Optional[tp.Hashable] = None\n            ) -> I:\n        '''\n        Get an IndexDate instance over a range of years, where start and end are inclusive.\n        '''\n        labels = np.arange(\n                to_datetime64(start, DT64_YEAR),\n                to_datetime64(stop, DT64_YEAR) + TD64_YEAR,\n                step=np.timedelta64(step, 'D'),\n                dtype=DT64_DAY)\n        labels.flags.writeable = False\n        return cls(labels, name=name)\n\n\nclass IndexDateGO(_IndexDatetimeGOMixin, IndexDate):\n\n    _IMMUTABLE_CONSTRUCTOR = IndexDate\n    __slots__ = _INDEX_GO_SLOTS\n\nIndexDate._MUTABLE_CONSTRUCTOR = IndexDateGO\n\n#-------------------------------------------------------------------------------\n@doc_inject(selector='index_date_time_init')\nclass IndexHour(IndexDatetime):\n    '''A mapping of time stamps at the resolution of minutes (via NumPy datetime64[m]) to positions, immutable and of fixed size.\n\n    {args}\n    '''\n    STATIC = True\n    _DTYPE = DT64_H\n    __slots__ = _INDEX_SLOTS\n\nclass IndexHourGO(_IndexDatetimeGOMixin, IndexHour):\n\n    _IMMUTABLE_CONSTRUCTOR = IndexHour\n    __slots__ = _INDEX_GO_SLOTS\n\nIndexHour._MUTABLE_CONSTRUCTOR = IndexHourGO\n\n#-------------------------------------------------------------------------------\n@doc_inject(selector='index_date_time_init')\nclass IndexMinute(IndexDatetime):\n    '''A mapping of time stamps at the resolution of minutes (via NumPy datetime64[m]) to positions, immutable and of fixed size.\n\n    {args}\n    '''\n    STATIC = True\n    _DTYPE = DT64_M\n    __slots__ = _INDEX_SLOTS\n\nclass IndexMinuteGO(_IndexDatetimeGOMixin, IndexMinute):\n\n    _IMMUTABLE_CONSTRUCTOR = IndexMinute\n    __slots__ = _INDEX_GO_SLOTS\n\nIndexMinute._MUTABLE_CONSTRUCTOR = IndexMinuteGO\n\n#-------------------------------------------------------------------------------\n\n@doc_inject(selector='index_date_time_init')\nclass IndexSecond(IndexDatetime):\n    '''A mapping of time stamps at the resolution of seconds (via NumPy datetime64[s]) to positions, immutable and of fixed size.\n\n    {args}\n    '''\n    STATIC = True\n    _DTYPE = DT64_S\n    __slots__ = _INDEX_SLOTS\n\nclass IndexSecondGO(_IndexDatetimeGOMixin, IndexSecond):\n\n    _IMMUTABLE_CONSTRUCTOR = IndexSecond\n    __slots__ = _INDEX_GO_SLOTS\n\nIndexSecond._MUTABLE_CONSTRUCTOR = IndexSecondGO\n\n#-------------------------------------------------------------------------------\n\n@doc_inject(selector='index_date_time_init')\nclass IndexMillisecond(IndexDatetime):\n    '''A mapping of time stamps at the resolution of milliseconds (via NumPy datetime64[ms]) to positions, immutable and of fixed size.\n\n    {args}\n    '''\n    STATIC = True\n    _DTYPE = DT64_MS\n    __slots__ = _INDEX_SLOTS\n\nclass IndexMillisecondGO(_IndexDatetimeGOMixin, IndexMillisecond):\n\n    _IMMUTABLE_CONSTRUCTOR = IndexMillisecond\n    __slots__ = _INDEX_GO_SLOTS\n\nIndexMillisecond._MUTABLE_CONSTRUCTOR = IndexMillisecondGO\n\n#-------------------------------------------------------------------------------\n\n@doc_inject(selector='index_date_time_init')\nclass IndexMicrosecond(IndexDatetime):\n    '''A mapping of time stamps at the resolution of microseconds (via NumPy datetime64[us]) to positions, immutable and of fixed size.\n\n    {args}\n    '''\n    STATIC = True\n    _DTYPE = DT64_US\n    __slots__ = _INDEX_SLOTS\n\nclass IndexMicrosecondGO(_IndexDatetimeGOMixin, IndexMicrosecond):\n\n    _IMMUTABLE_CONSTRUCTOR = IndexMicrosecond\n    __slots__ = _INDEX_GO_SLOTS\n\nIndexMicrosecond._MUTABLE_CONSTRUCTOR = IndexMicrosecondGO\n\n#-------------------------------------------------------------------------------\n\n@doc_inject(selector='index_date_time_init')\nclass IndexNanosecond(IndexDatetime):\n    '''A mapping of time stamps at the resolution of nanoseconds (via NumPy datetime64[ns]) to positions, immutable and of fixed size.\n\n    {args}\n    '''\n    STATIC = True\n    _DTYPE = DT64_NS\n    __slots__ = _INDEX_SLOTS\n\nclass IndexNanosecondGO(_IndexDatetimeGOMixin, IndexNanosecond):\n\n    _IMMUTABLE_CONSTRUCTOR = IndexNanosecond\n    __slots__ = _INDEX_GO_SLOTS\n\nIndexNanosecond._MUTABLE_CONSTRUCTOR = IndexNanosecondGO\n\n\n\n#-------------------------------------------------------------------------------\n_DTYPE_TO_CLASS = {cls._DTYPE: cls for cls in (\n        IndexYear,\n        IndexYearMonth,\n        IndexDate,\n        IndexHour,\n        IndexMinute,\n        IndexSecond,\n        IndexMillisecond,\n        IndexMicrosecond,\n        IndexNanosecond\n        )}\n\ndef _dtype_to_index_cls(static: bool, dtype: np.dtype) -> tp.Type[Index]:\n    '''\n    Given an the clazss of the Index from which this is valled, as well as the dtype of the resultant array, return the appropriate Index class.\n    '''\n\n    resolved_static = _DTYPE_TO_CLASS.get(dtype)\n    if resolved_static is not None:\n        if static:\n            return resolved_static\n        return resolved_static._MUTABLE_CONSTRUCTOR #type: ignore\n    # if origin is a dt64, and dtype is not a dt64, we can go to Index or IndexGO\n    if static:\n        return Index\n    return IndexGO"""
static_frame/core/index_hierarchy.py,21,"b""import typing as tp\nfrom itertools import chain\nfrom ast import literal_eval\n\nimport numpy as np\n\nfrom static_frame.core.container import ContainerOperand\n\nfrom static_frame.core.util import DEFAULT_SORT_KIND\nfrom static_frame.core.util import IndexConstructor\nfrom static_frame.core.util import IndexConstructors\nfrom static_frame.core.util import GetItemKeyType\nfrom static_frame.core.util import intersect2d\nfrom static_frame.core.util import union2d\nfrom static_frame.core.util import setdiff2d\nfrom static_frame.core.util import name_filter\nfrom static_frame.core.util import isin\nfrom static_frame.core.util import INT_TYPES\nfrom static_frame.core.util import NameType\nfrom static_frame.core.util import CallableOrMapping\nfrom static_frame.core.util import DepthLevelSpecifier\nfrom static_frame.core.util import NULL_SLICE\nfrom static_frame.core.util import NAME_DEFAULT\n\nfrom static_frame.core.index_base import IndexBase\nfrom static_frame.core.index import Index\nfrom static_frame.core.index import IndexGO\nfrom static_frame.core.index import mutable_immutable_index_filter\nfrom static_frame.core.index_level import IndexLevel\nfrom static_frame.core.index_level import IndexLevelGO\n\nfrom static_frame.core.node_selector import InterfaceGetItem\nfrom static_frame.core.node_selector import InterfaceAsType\nfrom static_frame.core.node_str import InterfaceString\nfrom static_frame.core.node_dt import InterfaceDatetime\n\nfrom static_frame.core.container_util import matmul\nfrom static_frame.core.container_util import index_from_optional_constructor\nfrom static_frame.core.container_util import rehierarch_from_type_blocks\nfrom static_frame.core.container_util import apply_binary_operator\n\nfrom static_frame.core.array_go import ArrayGO\nfrom static_frame.core.type_blocks import TypeBlocks\nfrom static_frame.core.display import DisplayConfig\nfrom static_frame.core.display import DisplayActive\nfrom static_frame.core.display import Display\nfrom static_frame.core.display import DisplayHeader\nfrom static_frame.core.node_iter import IterNodeType\nfrom static_frame.core.node_iter import IterNodeDepthLevel\nfrom static_frame.core.node_iter import IterNodeApplyType\nfrom static_frame.core.hloc import HLoc\nfrom static_frame.core.exception import ErrorInitIndex\nfrom static_frame.core.doc_str import doc_inject\n\nif tp.TYPE_CHECKING:\n    from pandas import DataFrame #pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.frame import Frame #pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.frame import FrameGO #pylint: disable=W0611 #pragma: no cover\n\nIH = tp.TypeVar('IH', bound='IndexHierarchy')\n\nCONTINUATION_TOKEN_INACTIVE = object()\n\n#-------------------------------------------------------------------------------\nclass IndexHierarchy(IndexBase):\n    '''\n    A hierarchy of :obj:`static_frame.Index` objects, defined as strict tree of uniform depth across all branches.\n    '''\n    __slots__ = (\n            '_levels',\n            '_blocks',\n            '_recache',\n            '_name',\n            )\n    _levels: IndexLevel\n    _blocks: TypeBlocks\n    _recache: bool\n    _name: NameType\n\n    # Temporary type overrides, until indices are generic.\n    __getitem__: tp.Callable[['IndexHierarchy', tp.Hashable], tp.Tuple[tp.Hashable, ...]]\n\n    # _IMMUTABLE_CONSTRUCTOR is None from IndexBase\n    # _MUTABLE_CONSTRUCTOR will be defined after IndexHierarhcyGO defined\n\n    _INDEX_CONSTRUCTOR = Index\n    _LEVEL_CONSTRUCTOR = IndexLevel\n    _UFUNC_UNION = union2d\n    _UFUNC_INTERSECTION = intersect2d\n    _UFUNC_DIFFERENCE = setdiff2d\n    _NDIM: int = 2\n    #---------------------------------------------------------------------------\n    # constructors\n\n    @classmethod\n    def from_product(cls: tp.Type[IH],\n            *levels,\n            name: NameType = None\n            ) -> IH:\n        '''\n        Given groups of iterables, return an ``IndexHierarchy`` made of the product of a values in those groups, where the first group is the top-most hierarchy.\n\n        Returns:\n            :obj:`static_frame.IndexHierarchy`\n\n        '''\n        indices = [] # store in a list, where index is depth\n        for lvl in levels:\n            if not isinstance(lvl, Index): # Index, not IndexBase\n                lvl = cls._INDEX_CONSTRUCTOR(lvl)\n            indices.append(lvl)\n\n        if len(indices) == 1:\n            raise RuntimeError('only one level given')\n\n        # build name from index names, assuming they are all specified\n        if name is None:\n            name = tuple(index.name for index in indices)\n            if any(n is None for n in name):\n                name = None\n\n        targets_previous = None\n\n        # need to walk up from bottom to top\n        # get depth pairs and iterate over those\n        depth = len(indices) - 1\n        while depth > 0:\n            index = indices[depth]\n            index_up = indices[depth - 1]\n            # for each label in the next-up index, we need a reference to this index with an offset of that index (or level)\n            targets = np.empty(len(index_up), dtype=object)\n\n            offset = 0\n            for idx, _ in enumerate(index_up):\n                # this level does not have targets, only an index (as a leaf)\n                level = cls._LEVEL_CONSTRUCTOR(index=index,\n                        offset=offset,\n                        targets=targets_previous)\n\n                targets[idx] = level\n                offset += len(level)\n            targets_previous = ArrayGO(targets, own_iterable=True)\n            depth -= 1\n\n        level = cls._LEVEL_CONSTRUCTOR(index=index_up, targets=targets_previous)\n        return cls(level, name=name)\n\n    # NOTE: this should be a constructor on IndexLevel\n    @classmethod\n    def _tree_to_index_level(cls,\n            tree,\n            index_constructors: tp.Optional[IndexConstructors] = None\n            ) -> IndexLevel:\n        '''\n        Convert a tree structure to an IndexLevel instance.\n        '''\n        # tree: tp.Dict[tp.Hashable, tp.Union[Sequence[tp.Hashable], tp.Dict]]\n\n        def get_index(labels, depth: int):\n            if index_constructors is not None:\n                explicit_constructor = index_constructors[depth]\n            else:\n                explicit_constructor = None\n\n            return index_from_optional_constructor(labels,\n                    default_constructor=cls._INDEX_CONSTRUCTOR,\n                    explicit_constructor=explicit_constructor)\n\n        def get_level(level_data, offset=0, depth=0):\n\n            if isinstance(level_data, dict):\n                level_labels = []\n                targets = np.empty(len(level_data), dtype=object)\n                offset_local = 0\n\n                # ordered key, value pairs, where the key is the label, the value is a list or dictionary; enmerate for insertion pre-allocated object array\n                for idx, (k, v) in enumerate(level_data.items()):\n                    level_labels.append(k)\n                    level = get_level(v, offset=offset_local, depth=depth + 1)\n                    targets[idx] = level\n                    offset_local += len(level) # for lower level offsetting\n\n                index = get_index(level_labels, depth=depth)\n                targets = ArrayGO(targets, own_iterable=True)\n\n            else: # an iterable, terminal node, no offsets needed\n                index = get_index(level_data, depth=depth)\n                targets = None\n\n            return cls._LEVEL_CONSTRUCTOR(\n                    index=index,\n                    offset=offset,\n                    targets=targets,\n                    )\n        return get_level(tree)\n\n    @classmethod\n    def from_tree(cls: tp.Type[IH],\n            tree,\n            *,\n            name: NameType = None\n            ) -> IH:\n        '''\n        Convert into a ``IndexHierarchy`` a dictionary defining keys to either iterables or nested dictionaries of the same.\n\n        Returns:\n            :obj:`static_frame.IndexHierarchy`\n        '''\n        return cls(cls._tree_to_index_level(tree), name=name)\n\n    @classmethod\n    def from_labels(cls: tp.Type[IH],\n            labels: tp.Iterable[tp.Sequence[tp.Hashable]],\n            *,\n            name: NameType = None,\n            reorder_for_hierarchy: bool = False,\n            index_constructors: tp.Optional[IndexConstructors] = None,\n            continuation_token: tp.Union[tp.Hashable, None] = CONTINUATION_TOKEN_INACTIVE\n            ) -> IH:\n        '''\n        Construct an ``IndexHierarhcy`` from an iterable of labels, where each label is tuple defining the component labels for all hierarchies.\n\n        Args:\n            labels: an iterator or generator of tuples.\n            reorder_for_hierarchy: reorder the labels to produce a hierarchible Index, assuming hierarchability is possible.\n            continuation_token: a Hashable that will be used as a token to identify when a value in a label should use the previously encountered value at the same depth.\n\n        Returns:\n            :obj:`static_frame.IndexHierarchy`\n        '''\n        if reorder_for_hierarchy:\n            if continuation_token != CONTINUATION_TOKEN_INACTIVE:\n                raise RuntimeError('continuation_token not supported when reorder_for_hiearchy')\n            # use from_records to ensure approprate columnar types\n            from static_frame import Frame\n            index_labels = Frame.from_records(labels)._blocks\n            # this will reorder and create the index using this smae method, passed as cls.from_labels\n            index, _ = rehierarch_from_type_blocks(\n                    labels=index_labels,\n                    depth_map=range(index_labels.shape[1]), # keep order\n                    index_cls=cls,\n                    index_constructors=index_constructors,\n                    name=name,\n                    )\n            return index\n\n        labels_iter = iter(labels)\n        try:\n            first = next(labels_iter)\n        except StopIteration:\n            # if iterable is empty, return empty index\n            return cls(levels=cls._LEVEL_CONSTRUCTOR(\n                    cls._INDEX_CONSTRUCTOR(())\n                    ), name=name)\n\n        depth = len(first)\n        # minimum permitted depth is 2\n        if depth < 2:\n            raise ErrorInitIndex('Cannot create an IndexHierarchy from only one level.')\n        if index_constructors and len(index_constructors) != depth:\n            raise ErrorInitIndex('If providing index constructors, number of index constructors must equal depth of IndexHierarchy.')\n\n        depth_max = depth - 1\n        depth_pre_max = depth - 2\n\n        token = object()\n        observed_last = [token for _ in range(depth)]\n\n        tree = dict() # order assumed and necessary\n        # put first back in front\n        for label in chain((first,), labels_iter):\n            if len(label) != depth:\n                raise ErrorInitIndex(f'Inconsistent label depth: expected {depth}, got {len(label)}')\n\n            current = tree # NOTE: over the life of this loop, current can be a dict or a list\n            # each label is an iterable\n            for d, v in enumerate(label):\n                # print('d', d, 'v', v, 'depth_pre_max', depth_pre_max, 'depth_max', depth_max)\n                if continuation_token is not CONTINUATION_TOKEN_INACTIVE:\n                    if v == continuation_token:\n                        # might check that observed_last[d] != token\n                        v = observed_last[d]\n\n                # shared implementation with from_labels -----------------------\n                if d < depth_pre_max:\n                    if v not in current:\n                        current[v] = dict() # order necessary\n                    else: # can only fetch this node (and not create a new node) if this is the sequential predecessor\n                        if v != observed_last[d]:\n                            raise ErrorInitIndex(f'invalid tree-form for IndexHierarchy: {v} in {label} cannot follow {observed_last[d]} when {v} has already been defined.')\n                    current = current[v]\n                    observed_last[d] = v\n                elif d < depth_max:\n                    if v not in current:\n                        current[v] = list()\n                    else: # cannot just fetch this list if it is not the predecessor\n                        if v != observed_last[d]:\n                            raise ErrorInitIndex(f'invalid tree-form for IndexHierarchy: {v} in {label} cannot follow {observed_last[d]} when {v} has already been defined.')\n                    current = current[v]\n                    observed_last[d] = v\n                elif d == depth_max:\n                    # if there are redundancies here they will be caught in index creation\n                    current.append(v)\n                else:\n                    raise ErrorInitIndex('label exceeded expected depth', label)\n                # shared implementation with _from_type_blocks -----------------\n\n        levels = cls._tree_to_index_level(\n                tree,\n                index_constructors=index_constructors\n                )\n        return cls(levels=levels, name=name)\n\n    @classmethod\n    def from_index_items(cls: tp.Type[IH],\n            items: tp.Iterable[tp.Tuple[tp.Hashable, Index]],\n            *,\n            index_constructor: tp.Optional[IndexConstructor] = None\n            ) -> IH:\n        '''\n        Given an iterable of pairs of label, :obj:`Index`, produce an :obj:`IndexHierarchy` where the labels are depth 0, the indices are depth 1.\n\n        Args:\n            items: iterable of pairs of label, :obj:`Index`.\n            index_constructor: Optionally provide index constructor for outermost index.\n        '''\n        labels = []\n        index_levels = []\n\n        offset = 0\n        for label, index in items:\n            labels.append(label)\n            index = mutable_immutable_index_filter(cls.STATIC, index)\n            index_levels.append(cls._LEVEL_CONSTRUCTOR(\n                    index,\n                    offset=offset,\n                    own_index=True)\n            )\n            offset += len(index)\n\n        targets = ArrayGO(np.array(index_levels, dtype=object), own_iterable=True)\n\n        index_outer = index_from_optional_constructor(labels,\n                    default_constructor=cls._INDEX_CONSTRUCTOR,\n                    explicit_constructor=index_constructor)\n        levels = cls._LEVEL_CONSTRUCTOR(\n                index=index_outer,\n                targets=targets,\n                own_index=True\n                )\n        # import ipdb; ipdb.set_trace()\n        return cls(levels)\n\n    @classmethod\n    def from_labels_delimited(cls: tp.Type[IH],\n            labels: tp.Iterable[str],\n            *,\n            delimiter: str = ' ',\n            name: NameType = None,\n            index_constructors: tp.Optional[IndexConstructors] = None,\n            ) -> IH:\n        '''\n        Construct an ``IndexHierarhcy`` from an iterable of labels, where each label is string defining the component labels for all hierarchies using a string delimiter. All components after splitting the string by the delimited will be literal evaled to produce proper types; thus, strings must be quoted.\n\n        Args:\n            labels: an iterator or generator of tuples.\n\n        Returns:\n            :obj:`static_frame.IndexHierarchy`\n        '''\n\n\n        def to_label(label: str) -> tp.Tuple[tp.Hashable, ...]:\n\n            start, stop = None, None\n            if label[0] in ('[', '('):\n                start = 1\n            if label[-1] in (']', ')'):\n                stop = -1\n\n            if start is not None or stop is not None:\n                label = label[start: stop]\n\n            parts = label.split(delimiter)\n            if len(parts) <= 1:\n                raise RuntimeError(f'Could not not parse more than one label from delimited string: {label}')\n\n            return tuple(literal_eval(p) for p in parts)\n\n        return cls.from_labels(\n                (to_label(label) for label in labels),\n                name=name,\n                index_constructors=index_constructors\n                )\n\n\n    @classmethod\n    def _from_type_blocks(cls: tp.Type[IH],\n            blocks: TypeBlocks,\n            *,\n            name: NameType = None,\n            index_constructors: tp.Optional[IndexConstructors] = None,\n            own_blocks: bool = False,\n            ) -> IH:\n        '''\n        Construct an :obj:`IndexHierarchy` from a :obj:`TypeBlocks` instance.\n\n        Args:\n            blocks: a TypeBlocks instance\n\n        Returns:\n            :obj:`IndexHierarchy`\n        '''\n\n        depth = blocks.shape[1]\n\n        # minimum permitted depth is 2\n        if depth < 2:\n            raise ErrorInitIndex('cannot create an IndexHierarchy from only one level.')\n        if index_constructors is not None and len(index_constructors) != depth:\n            raise ErrorInitIndex('if providing index constructors, number of index constructors must equal depth of IndexHierarchy.')\n\n        depth_max = depth - 1\n        depth_pre_max = depth - 2\n\n        token = object()\n        observed_last = [token for _ in range(depth)]\n        # range_depth = range(depth)\n\n        tree = dict() # order assumed and necessary\n\n        idx_row_last = -1\n        for (idx_row, d), v in blocks.element_items():\n            if idx_row_last != idx_row:\n                # for each row, we re-set current to the outermost reference\n                current = tree\n                idx_row_last = idx_row\n\n            # shared implementation with from_labels ---------------------------\n            if d < depth_pre_max:\n                if v not in current:\n                    current[v] = dict() # order necessary\n                else: # can only fetch this node (and not create a new node) if this is the sequential predecessor\n                    if v != observed_last[d]:\n                        raise ErrorInitIndex(f'invalid tree-form for IndexHierarchy: {v} cannot follow {observed_last[d]} when {v} has already been defined.')\n                current = current[v]\n                observed_last[d] = v\n            elif d < depth_max: # premax means inner values are a list\n                if v not in current:\n                    current[v] = list()\n                else: # cannot just fetch this list if it is not the predecessor\n                    if v != observed_last[d]:\n                        raise ErrorInitIndex(f'invalid tree-form for IndexHierarchy: {v} cannot follow {observed_last[d]} when {v} has already been defined.')\n                current = current[v]\n                observed_last[d] = v\n            elif d == depth_max:\n                # if there are redundancies here they will be caught in index creation\n                current.append(v)\n            else:\n                # cannot happen with TypeBlocks\n                raise ErrorInitIndex('label exceeded expected depth', v) #pragma: no cover\n            # shared implementation with from_labels ---------------------------\n\n        levels = cls._tree_to_index_level(\n                tree,\n                index_constructors=index_constructors\n                )\n\n        if index_constructors is not None:\n            # If defined, we may have changed columnar dtypes in IndexLevels, and cannot reuse blocks\n            if tuple(blocks.dtypes) != tuple(levels.dtype_per_depth()):\n                blocks = None\n                own_blocks = False\n\n        return cls(levels=levels, name=name, blocks=blocks, own_blocks=own_blocks)\n\n\n    #---------------------------------------------------------------------------\n    def __init__(self,\n            levels: tp.Union[IndexLevel, 'IndexHierarchy'],\n            *,\n            name: NameType = NAME_DEFAULT,\n            blocks: tp.Optional[TypeBlocks] = None,\n            own_blocks: bool = False,\n            ):\n        '''\n        Args:\n            levels: IndexLevels instance, or, optionally, an IndexHierarchy to be used to construct a new IndexHierarchy.\n            labels: a client can optionally provide the labels used to construct the levels, as an optional optimization in forming the IndexHierarchy.\n        '''\n\n        self._blocks = None\n\n        if isinstance(levels, IndexHierarchy):\n            if not blocks is None:\n                raise ErrorInitIndex('cannot provide blocks when initializing with IndexHierarchy')\n            index_level = levels._levels\n            # if cache is updated, can get blocks\n            if not levels._recache:\n                self._blocks = levels._blocks.copy()\n            # transfer name if not given as arg\n            if name is NAME_DEFAULT:\n                name = levels.name\n\n        elif isinstance(levels, IndexLevel):\n            index_level = levels\n            if not blocks is None:\n                self._blocks = blocks if own_blocks else blocks.copy()\n\n        else:\n            raise NotImplementedError(f'no handling for creation from {levels}')\n\n        if self.STATIC and index_level.STATIC:\n            self._levels = index_level\n        else: # must deepcopy IndexLevels if not IndexHierarchy not static\n            self._levels = index_level.to_index_level(\n                    cls=self._LEVEL_CONSTRUCTOR\n                    )\n\n        self._recache = self._blocks is None\n        self._name = None if name is NAME_DEFAULT else name_filter(name)\n\n    #---------------------------------------------------------------------------\n    # name interface\n\n    def rename(self: IH, name: NameType) -> IH:\n        '''\n        Return a new Frame with an updated name attribute.\n        '''\n        # do not need to recache\n        # let the constructor handle reuse\n        return self.__class__(self, name=name)\n\n    #---------------------------------------------------------------------------\n    # interfaces\n\n    @property\n    def loc(self) -> InterfaceGetItem:\n        return InterfaceGetItem(self._extract_loc)\n\n    @property\n    def iloc(self) -> InterfaceGetItem:\n        return InterfaceGetItem(self._extract_iloc)\n\n\n    def _iter_label(self, depth_level: int = 0):\n        yield from self._levels.label_nodes_at_depth(depth_level=depth_level)\n\n    def _iter_label_items(self, depth_level: int = 0):\n        yield from enumerate(self._levels.label_nodes_at_depth(depth_level=depth_level))\n\n    @property\n    def iter_label(self) -> IterNodeDepthLevel:\n        return IterNodeDepthLevel(\n                container=self,\n                function_items=self._iter_label_items,\n                function_values=self._iter_label,\n                yield_type=IterNodeType.VALUES,\n                apply_type=IterNodeApplyType.INDEX_LABELS\n                )\n\n    # NOTE: Index implements drop property\n\n    @property\n    @doc_inject(select='astype')\n    def astype(self) -> InterfaceAsType:\n        '''\n        Retype one or more depths. Can be used as as function to retype the entire ``IndexHierarchy``; alternatively, a ``__getitem__`` interface permits retyping selected depths.\n\n        Args:\n            {dtype}\n        '''\n        return InterfaceAsType(func_getitem=self._extract_getitem_astype)\n\n    #---------------------------------------------------------------------------\n    @property\n    def via_str(self) -> InterfaceString[np.ndarray]:\n        '''\n        Interface for applying string methods to elements in this container.\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        def blocks_to_container(blocks: tp.Iterator[np.ndarray]) -> np.ndarray:\n            return TypeBlocks.from_blocks(blocks).values\n\n        return InterfaceString(\n                blocks=self._blocks._blocks,\n                blocks_to_container=blocks_to_container,\n                )\n\n    @property\n    def via_dt(self) -> InterfaceDatetime[np.ndarray]:\n        '''\n        Interface for applying datetime properties and methods to elements in this container.\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        def blocks_to_container(blocks: tp.Iterator[np.ndarray]) -> np.ndarray:\n            return TypeBlocks.from_blocks(blocks).values\n\n        return InterfaceDatetime(\n                blocks=self._blocks._blocks,\n                blocks_to_container=blocks_to_container,\n                )\n\n\n    #---------------------------------------------------------------------------\n\n    def _update_array_cache(self) -> None:\n        self._blocks = self._levels.to_type_blocks()\n        self._recache = False\n\n    #---------------------------------------------------------------------------\n\n    @property # type: ignore\n    @doc_inject()\n    def mloc(self) -> int:\n        '''{doc_int}\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        return self._blocks.mloc\n\n    @property\n    def dtypes(self) -> 'Series':\n        '''\n        Return a Series of dytpes for each index depth.\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        from static_frame.core.series import Series\n\n        if self._recache:\n            # might use self._levels.dtype_per_depth\n            self._update_array_cache()\n\n        if self._name and len(self._name) == self.depth:\n            labels = self._name\n        else:\n            labels = None\n\n        return Series(self._blocks.dtypes, index=labels)\n\n    @property\n    def shape(self) -> tp.Tuple[int, ...]:\n        '''\n        Return a tuple describing the shape of the underlying NumPy array.\n\n        Returns:\n            :obj:`tp.Tuple[int]`\n        '''\n        if self._recache:\n            return self._levels.__len__(), self._levels.depth\n        return self._blocks._shape\n\n    @property\n    def ndim(self) -> int:\n        '''\n        Return the number of dimensions.\n\n        Returns:\n            :obj:`int`\n        '''\n        return self._NDIM\n\n    @property\n    def size(self) -> int:\n        '''\n        Return the size of the underlying NumPy array.\n\n        Returns:\n            :obj:`int`\n        '''\n        if self._recache:\n            return self._levels.__len__() * self._levels.depth\n        return self._blocks.size\n\n    @property\n    def nbytes(self) -> int:\n        '''\n        Return the total bytes of the underlying NumPy array.\n\n        Returns:\n            :obj:`int`\n        '''\n        if self._recache:\n            self._update_array_cache()\n        return self._blocks.nbytes\n\n    def __bool__(self) -> bool:\n        '''\n        True if this container has size.\n        '''\n        if self._recache:\n            return bool(self._levels.__len__()) and bool(self._levels.depth)\n        return bool(self._blocks.size)\n\n    #---------------------------------------------------------------------------\n\n    def __len__(self) -> int:\n        if self._recache:\n            # avoid full recache\n            return self._levels.__len__()\n        return self._blocks.__len__()\n\n    @doc_inject()\n    def display(self,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> Display:\n        '''{doc}\n\n        Args:\n            {config}\n        '''\n        config = config or DisplayActive.get()\n\n        if self._recache:\n            self._update_array_cache()\n\n        sub_display = None\n\n        if config.type_show:\n            header = DisplayHeader(self.__class__, self._name)\n            header_depth = 1\n            header_sub = '' # need spacer\n        else:\n            header = None\n            header_depth = 0\n            header_sub = None\n\n        for col in self._blocks.axis_values(0):\n            # as a slice this is far more efficient as no copy is made\n            if sub_display is None: # the first\n                sub_display = Display.from_values(\n                        col,\n                        header=header,\n                        config=config,\n                        outermost=True,\n                        index_depth=0,\n                        header_depth=header_depth)\n            else:\n                sub_display.extend_iterable(col, header=header_sub)\n\n        return sub_display\n\n\n    #---------------------------------------------------------------------------\n    def _drop_iloc(self, key: GetItemKeyType) -> 'IndexBase':\n        '''Create a new index after removing the values specified by the loc key.\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        blocks = TypeBlocks.from_blocks(self._blocks._drop_blocks(row_key=key))\n        index_constructors = tuple(self._levels.index_types())\n\n        return self.__class__._from_type_blocks(blocks,\n                index_constructors=index_constructors,\n                name=self._name,\n                own_blocks=True\n                )\n\n    def _drop_loc(self, key: GetItemKeyType) -> 'IndexBase':\n        '''Create a new index after removing the values specified by the loc key.\n        '''\n        return self._drop_iloc(self.loc_to_iloc(key)) #type: ignore\n\n    #---------------------------------------------------------------------------\n\n    @property\n    @doc_inject(selector='values_2d', class_name='IndexHierarchy')\n    def values(self) -> np.ndarray:\n        '''\n        {}\n        '''\n        if self._recache:\n            self._update_array_cache()\n        return self._blocks.values\n\n    @property\n    def depth(self) -> int:\n        if self._recache:\n            # avoid full recache to get depth\n            return self._levels.depth\n        return self._blocks.shape[1]\n\n    def values_at_depth(self,\n            depth_level: DepthLevelSpecifier = 0\n            ) -> np.ndarray:\n        '''\n        Return an NP array for the ``depth_level`` specified.\n\n        Args:\n            depth_level: a single depth level, or iterable depth of depth levels.\n        '''\n        if self._recache:\n            self._update_array_cache()\n        if isinstance(depth_level, int):\n            sel = depth_level\n        else:\n            sel = list(depth_level)\n        return self._blocks._extract_array(column_key=sel)\n\n    @doc_inject()\n    def label_widths_at_depth(self,\n            depth_level: DepthLevelSpecifier = 0\n            ) -> tp.Iterator[tp.Tuple[tp.Hashable, int]]:\n        '''{}'''\n        if isinstance(depth_level, int):\n            sel = depth_level\n        else:\n            raise NotImplementedError('selection from iterables is not implemented')\n        yield from self._levels.label_widths_at_depth(depth_level=depth_level)\n\n    @property\n    def index_types(self) -> 'Series':\n        '''\n        Return a Series of Index classes for each index depth.\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        from static_frame.core.series import Series\n\n        if self._name and len(self._name) == self.depth:\n            labels = self._name\n        else:\n            labels = None\n\n        # NOTE: consider caching index_types\n        return Series(self._levels.index_types(), index=labels)\n\n    #---------------------------------------------------------------------------\n\n    def copy(self: IH) -> IH:\n        '''\n        Return a new IndexHierarchy. This is not a deep copy.\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        blocks = self._blocks.copy()\n        return self.__class__(\n                levels=self._levels,\n                name=self._name,\n                blocks=blocks,\n                own_blocks=True\n                )\n\n    def relabel(self, mapper: CallableOrMapping) -> 'IndexHierarchy':\n        '''\n        Return a new IndexHierarchy with labels replaced by the callable or mapping; order will be retained. If a mapping is used, the mapping should map tuple representation of labels, and need not map all origin keys.\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        index_constructors = tuple(self._levels.index_types())\n\n        if not callable(mapper):\n            # if a mapper, it must support both __getitem__ and __contains__\n            getitem = getattr(mapper, 'get')\n\n            def gen() -> tp.Iterator[tp.Tuple[tp.Hashable, ...]]:\n                for array in self._blocks.axis_values(axis=1):\n                    # as np.ndarray are not hashable, must tuplize\n                    label = tuple(array)\n                    yield getitem(label, label)\n\n            return self.__class__.from_labels(gen(),\n                    name=self._name,\n                    index_constructors=index_constructors,\n                    )\n\n        return self.__class__.from_labels(\n                (mapper(x) for x in self._blocks.axis_values(axis=1)),\n                name=self._name,\n                index_constructors=index_constructors,\n                )\n\n    def rehierarch(self,\n            depth_map: tp.Iterable[int]\n            ) -> 'IndexHierarchy':\n        '''\n        Return a new `IndexHierarchy` that conforms to the new depth assignments given be `depth_map`.\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        index_constructors = tuple(self._levels.index_types())\n\n        index, _ = rehierarch_from_type_blocks(\n                labels=self._blocks,\n                index_cls=self.__class__,\n                index_constructors=index_constructors,\n                depth_map=depth_map,\n                )\n        return index\n\n    #---------------------------------------------------------------------------\n\n    def loc_to_iloc(self,\n            key: tp.Union[GetItemKeyType, HLoc]\n            ) -> GetItemKeyType:\n        '''\n        Given iterable of GetItemKeyTypes, apply to each level of levels.\n        '''\n        from static_frame.core.series import Series\n\n        # NOTE: this implementation is different from Index.loc_to_iloc: here, we explicitly translate Series, Index, and IndexHierarchy before passing on to IndexLevels\n\n        if isinstance(key, Index):\n            # if an Index, we simply use the values of the index\n            key = key.values\n\n        if isinstance(key, IndexHierarchy):\n            # default iteration of IH is as tuple\n            return [self._levels.leaf_loc_to_iloc(k) for k in key]\n\n        if isinstance(key, Series):\n            if key.dtype == bool:\n                # if a Boolean series, sort and reindex\n                if not key.index.equals(self):\n                    key = key.reindex(self,\n                            fill_value=False,\n                            check_equals=False,\n                            ).values\n                else: # the index is equal\n                    key = key.values\n            else:\n                # For all other Series types, we simply assume that the values are to be used as keys in the IH. This ignores the index, but it does not seem useful to require the Series, used like this, to have a matching index value, as the index and values would need to be identical to have the desired selection.\n                key = key.values\n\n        # if an HLoc, will pass on to loc_to_iloc\n        return self._levels.loc_to_iloc(key)\n\n    def _extract_iloc(self,\n            key\n            ) -> tp.Union['IndexHierarchy', tp.Tuple[tp.Hashable]]:\n        '''Extract a new index given an iloc key\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        if isinstance(key, INT_TYPES):\n            # return a tuple if selecting a single row\n            # NOTE: if extracting a single row, should be able to get it from IndexLevel without forcing a complete recache\n            # NOTE: Selecting a single row may force type coercion before values are added to the tuple; i.e., a datetime64 will go to datetime.date before going to the tuple\n            return tuple(self._blocks._extract_array(row_key=key))\n\n        index_constructors = tuple(self._levels.index_types())\n        tb = self._blocks._extract(row_key=key)\n\n        return self.__class__._from_type_blocks(tb,\n                name=self._name,\n                index_constructors=index_constructors,\n                own_blocks=True,\n                )\n\n    def _extract_loc(self,\n            key: GetItemKeyType\n            ) -> tp.Union['IndexHierarchy', tp.Tuple[tp.Hashable]]:\n        return self._extract_iloc(self.loc_to_iloc(key))\n\n    def __getitem__(self, #pylint: disable=E0102\n            key: GetItemKeyType\n            ) -> tp.Union['IndexHierarchy', tp.Tuple[tp.Hashable]]:\n        '''Extract a new index given an iloc key.\n        '''\n        return self._extract_iloc(key)\n\n    #---------------------------------------------------------------------------\n    def _extract_getitem_astype(self, key: GetItemKeyType) -> 'IndexHierarchyAsType':\n        '''Given an iloc key (using integer positions for columns) return a configured IndexHierarchyAsType instance.\n        '''\n        # key is an iloc key\n        if isinstance(key, tuple):\n            raise KeyError('__getitem__ does not support multiple indexers')\n        return IndexHierarchyAsType(self, key=key)\n\n\n    #---------------------------------------------------------------------------\n    # operators\n\n    def _ufunc_unary_operator(self, operator: tp.Callable) -> np.ndarray:\n        '''Always return an NP array.\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        values = self._blocks.values\n        array = operator(values)\n        array.flags.writeable = False\n        return array\n\n    def _ufunc_binary_operator(self, *,\n            operator: tp.Callable,\n            other: tp.Any,\n            ) -> np.ndarray:\n        '''\n        Binary operators applied to an index always return an NP array. This deviates from Pandas, where some operations (multipling an int index by an int) result in a new Index, while other operations result in a np.array (using == on two Index).\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        # NOTE: might use TypeBlocks._ufunc_binary_operator\n        values = self._blocks.values\n\n        other_is_array = False\n        if isinstance(other, Index):\n            # if this is a 1D index, must rotate labels before using an operator\n            other = other.values.reshape((len(other), 1)) # operate on labels to labels\n            other_is_array = True\n        elif isinstance(other, IndexHierarchy):\n            # already 2D\n            other = other.values # operate on labels to labels\n            other_is_array = True\n        elif isinstance(other, np.ndarray):\n            other_is_array = True\n\n        if operator.__name__ == 'matmul':\n            return matmul(values, other)\n        elif operator.__name__ == 'rmatmul':\n            return matmul(other, values)\n\n        return apply_binary_operator(\n                values=values,\n                other=other,\n                other_is_array=other_is_array,\n                operator=operator,\n                )\n\n    def _ufunc_axis_skipna(self, *,\n            axis,\n            skipna,\n            ufunc,\n            ufunc_skipna,\n            composable: bool,\n            dtypes: tp.Tuple[np.dtype, ...],\n            size_one_unity: bool\n            ) -> np.ndarray:\n        '''\n        Returns:\n            immutable NumPy array.\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        dtype = None if not dtypes else dtypes[0]\n        values = self._blocks.values\n\n        if skipna:\n            post = ufunc_skipna(values, axis=axis, dtype=dtype)\n        else:\n            post = ufunc(values, axis=axis, dtype=dtype)\n\n        post.flags.writeable = False\n        return post\n\n    # _ufunc_shape_skipna defined in IndexBase\n\n    #---------------------------------------------------------------------------\n    # dictionary-like interface\n\n    # NOTE: we intentionally exclude keys(), items(), and get() from Index classes, as they return inconsistent result when thought of as a dictionary\n\n    def __iter__(self) -> tp.Iterator[tp.Tuple[tp.Hashable, ...]]:\n        '''Iterate over labels.\n        '''\n        # NOTE: by iterating from levels, we avoid type casting to a row\n        yield from self._levels.__iter__()\n\n    def __reversed__(self) -> tp.Iterator[tp.Tuple[tp.Hashable, ...]]:\n        '''\n        Returns a reverse iterator on the index labels.\n        '''\n        if self._recache:\n            self._update_array_cache()\n        for array in self._blocks.axis_values(1, reverse=True):\n            yield tuple(array)\n\n    def __contains__(self, value) -> bool:\n        '''Determine if a leaf loc is contained in this Index.\n        '''\n        # levels only, no need to recache as this is what has been mutated\n        return self._levels.__contains__(value)\n    #---------------------------------------------------------------------------\n    # utility functions\n\n    @doc_inject()\n    def equals(self,\n            other: tp.Any,\n            *,\n            compare_name: bool = False,\n            compare_dtype: bool = False,\n            compare_class: bool = False,\n            skipna: bool = True,\n            ) -> bool:\n        '''\n        {doc}\n\n        Args:\n            {compare_name}\n            {compare_dtype}\n            {compare_class}\n            {skipna}\n        '''\n        # NOTE: do not need to udpate array cache, as can compare elemetns in levels\n        if id(other) == id(self):\n            return True\n\n        if compare_class and self.__class__ != other.__class__:\n            return False\n        elif not isinstance(other, IndexHierarchy):\n            return False\n\n        # same type from here\n        if self.shape != other.shape:\n            return False\n        if compare_name and self.name != other.name:\n            return False\n\n        return self._levels.equals(other._levels,\n                compare_name=compare_name,\n                compare_dtype=compare_dtype,\n                compare_class=compare_class,\n                skipna=skipna,\n                )\n\n\n    def sort(self,\n            ascending: bool = True,\n            kind: str = DEFAULT_SORT_KIND) -> 'Index':\n        '''Return a new Index with the labels sorted.\n\n        Args:\n            kind: Sort algorithm passed to NumPy.\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        v = self._blocks.values\n        order = np.lexsort([v[:, i] for i in range(v.shape[1]-1, -1, -1)])\n\n        if not ascending:\n            order = order[::-1]\n\n        blocks = self._blocks._extract(row_key=order)\n        index_constructors = tuple(self._levels.index_types())\n\n        return self.__class__._from_type_blocks(blocks,\n                index_constructors=index_constructors,\n                name=self._name,\n                own_blocks=True\n                )\n\n    def isin(self, other: tp.Iterable[tp.Iterable[tp.Hashable]]) -> np.ndarray:\n        '''\n        Return a Boolean array showing True where one or more of the passed in iterable of labels is found in the index.\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        matches = []\n        for seq in other:\n            if not hasattr(seq, '__iter__'):\n                raise RuntimeError('must provide one or more iterables within an iterable')\n            # Coerce to hashable type\n            as_tuple = tuple(seq)\n            if len(as_tuple) == self.depth:\n                # can pre-filter if iterable matches to length\n                matches.append(as_tuple)\n\n        if not matches:\n            return np.full(self.__len__(), False, dtype=bool)\n\n        return isin(self.flat().values, matches)\n\n    def roll(self, shift: int) -> 'IndexHierarchy':\n        '''Return an Index with values rotated forward and wrapped around (with a postive shift) or backward and wrapped around (with a negative shift).\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        blocks = TypeBlocks.from_blocks(\n                self._blocks._shift_blocks(row_shift=shift, wrap=True)\n                )\n        index_constructors = tuple(self._levels.index_types())\n\n        return self.__class__._from_type_blocks(blocks,\n                index_constructors=index_constructors,\n                name=self._name,\n                own_blocks=True\n                )\n\n    #---------------------------------------------------------------------------\n    # export\n\n    def _to_frame(self,\n            constructor: tp.Type[ContainerOperand]\n            ) -> 'Frame':\n\n        if self._recache:\n            self._update_array_cache()\n\n        return constructor(\n                self._blocks.copy(),\n                columns=None,\n                index=None,\n                own_data=True\n                )\n\n    def to_frame(self) -> 'Frame':\n        '''\n        Return :obj:`Frame` version of this :obj:`IndexHiearchy`.\n        '''\n        from static_frame import Frame\n        return self._to_frame(Frame)\n\n    def to_frame_go(self) -> 'FrameGO':\n        '''\n        Return a :obj:`FrameGO` version of this :obj:`IndexHierarchy`.\n        '''\n        from static_frame import FrameGO\n        return self._to_frame(FrameGO)\n\n    def to_pandas(self) -> 'DataFrame':\n        '''Return a Pandas MultiIndex.\n        '''\n        import pandas\n\n        if self._recache:\n            self._update_array_cache()\n\n        # must copy to get a mutable array\n        arrays = tuple(a.copy() for a in self._blocks.axis_values(axis=0))\n        mi = pandas.MultiIndex.from_arrays(arrays)\n\n        mi.name = self._name\n        mi.names = self.names\n        return mi\n\n    def flat(self) -> IndexBase:\n        '''Return a flat, one-dimensional index of tuples for each level.\n        '''\n        return self._INDEX_CONSTRUCTOR(self.__iter__())\n\n    def add_level(self, level: tp.Hashable):\n        '''Return an IndexHierarchy with a new root (outer) level added.\n        '''\n        if self.STATIC: # can reuse levels\n            levels_src = self._levels\n        else:\n            levels_src = self._levels.to_index_level()\n\n        levels = self._LEVEL_CONSTRUCTOR(\n                index=self._INDEX_CONSTRUCTOR((level,)),\n                targets=ArrayGO([levels_src], own_iterable=True),\n                offset=0,\n                own_index=True\n                )\n        # can transfrom TypeBlocks appropriately and pass to constructor\n        if not self._recache: # if we have TypeBlocks\n            array = np.full(self.__len__(), level)\n            array.flags.writeable = False\n            blocks = TypeBlocks.from_blocks(chain((array,), self._blocks._blocks))\n            return self.__class__(levels,\n                    name=self._name,\n                    blocks=blocks,\n                    own_blocks=True)\n\n        return self.__class__(levels, name=self._name)\n\n    def drop_level(self, count: int = 1) -> tp.Union[Index, 'IndexHierarchy']:\n        '''Return an IndexHierarchy with one or more leaf levels removed. This might change the size of the resulting index if the resulting levels are not unique.\n\n        Args:\n            count: A positive value is the number of depths to remove from the root (outer) side of the hierarhcy; a negative values is the number of depths to remove from the leaf (inner) side of the hierarchy.\n        '''\n        if count < 0: # remove from inner\n            levels = self._levels.to_index_level()\n            for _ in range(abs(count)):\n                levels_stack = [levels]\n                while levels_stack:\n                    level = levels_stack.pop()\n                    # check to see if children of this target are leaves\n                    if level.targets[0].targets is None:\n                        level.targets = None\n                    else:\n                        levels_stack.extend(level.targets)\n                if levels.targets is None:  # if no targets, at the root\n                    break\n            if levels.targets is None: # fall back to 1D index\n                return levels.index\n\n            # if we have TypeBlocks and levels is the same length\n            if not self._recache and levels.__len__() == self.__len__():\n                blocks = self._blocks.iloc[NULL_SLICE, :count]\n                return self.__class__(levels,\n                        name=self._name,\n                        blocks=blocks,\n                        own_blocks=True\n                        )\n            return self.__class__(levels, name=self._name)\n\n        elif count > 0: # remove from outer\n            levels = self._levels.to_index_level()\n            for _ in range(count):\n                targets = []\n                labels = []\n                for target in levels.targets:\n                    labels.extend(target.index)\n                    if target.targets is not None:\n                        targets.extend(target.targets)\n                index = levels.index.__class__(labels)\n                if not targets:\n                    return index\n                levels = levels.__class__(index=index, targets=targets)\n\n            # if we have TypeBlocks and levels is the same length\n            if not self._recache and levels.__len__() == self.__len__():\n                blocks = self._blocks.iloc[NULL_SLICE, count:]\n                return self.__class__(levels,\n                        name=self._name,\n                        blocks=blocks,\n                        own_blocks=True\n                        )\n            return self.__class__(levels, name=self._name)\n\n        raise NotImplementedError('no handling for a 0 count drop level.')\n\n\nclass IndexHierarchyGO(IndexHierarchy):\n    '''\n    A hierarchy of :obj:`static_frame.Index` objects that permits mutation only in the addition of new hierarchies or labels.\n    '''\n\n    STATIC = False\n\n    _IMMUTABLE_CONSTRUCTOR = IndexHierarchy\n\n    _LEVEL_CONSTRUCTOR = IndexLevelGO\n    _INDEX_CONSTRUCTOR = IndexGO\n\n    __slots__ = (\n            '_levels', # IndexLevel\n            '_blocks',\n            '_recache',\n            '_name'\n            )\n\n    def append(self, value: tuple):\n        '''\n        Append a single label to this index.\n        '''\n        self._levels.append(value)\n        self._recache = True\n\n    def extend(self, other: IndexHierarchy):\n        '''\n        Extend this IndexHiearchy in-place\n        '''\n        self._levels.extend(other._levels)\n        self._recache = True\n\n    def copy(self: IH) -> IH:\n        '''\n        Return a new IndexHierarchy. This is not a deep copy.\n        '''\n        if self._recache:\n            self._update_array_cache()\n\n        blocks = self._blocks.copy()\n        return self.__class__(\n                levels=self._levels.to_index_level(),\n                name=self._name,\n                blocks=blocks,\n                own_blocks=True,\n                )\n\n# update class attr on Index after class initialziation\nIndexHierarchy._MUTABLE_CONSTRUCTOR = IndexHierarchyGO\n\n\nclass IndexHierarchyAsType:\n\n    __slots__ = ('container', 'key',)\n\n    def __init__(self,\n            container: IndexHierarchy,\n            key: GetItemKeyType\n            ) -> None:\n        self.container = container\n        self.key = key\n\n    def __call__(self, dtype) -> IndexHierarchy:\n\n        from static_frame.core.index_datetime import _dtype_to_index_cls\n        container = self.container\n\n        if container._recache:\n            container._update_array_cache()\n\n        # use TypeBlocks in both situations to avoid double casting\n        blocks = TypeBlocks.from_blocks(\n                container._blocks._astype_blocks(column_key=self.key, dtype=dtype)\n                )\n\n        # avoid coercion of datetime64 arrays that were not targetted in the selection\n        index_constructors = container.index_types.values.copy()\n\n        dtype_post = blocks.dtypes[self.key] # can select element or array\n        if isinstance(dtype_post, np.dtype):\n            index_constructors[self.key] = _dtype_to_index_cls(\n                    container.STATIC,\n                    dtype_post)\n        else: # assign iterable\n            index_constructors[self.key] = [\n                    _dtype_to_index_cls(container.STATIC, dt)\n                    for dt in dtype_post]\n\n        return container.__class__._from_type_blocks(\n                blocks,\n                index_constructors=index_constructors,\n                own_blocks=True\n                )\n\n"""
static_frame/core/index_level.py,23,"b""\n\nimport typing as tp\nfrom collections import deque\nfrom itertools import zip_longest\n\nimport numpy as np\n\nfrom static_frame.core.hloc import HLoc\nfrom static_frame.core.index import Index\nfrom static_frame.core.index import ILoc\nfrom static_frame.core.index import IndexGO\nfrom static_frame.core.array_go import ArrayGO\n\nfrom static_frame.core.util import KEY_MULTIPLE_TYPES\nfrom static_frame.core.util import KEY_ITERABLE_TYPES\nfrom static_frame.core.util import INT_TYPES\nfrom static_frame.core.util import GetItemKeyType\n\nfrom static_frame.core.util import resolve_dtype_iter\nfrom static_frame.core.util import GetItemKeyTypeCompound\n\nfrom static_frame.core.type_blocks import TypeBlocks\n\nfrom static_frame.core.index import LocMap\nfrom static_frame.core.index import mutable_immutable_index_filter\nfrom static_frame.core.exception import ErrorInitIndexLevel\n\nfrom static_frame.core.doc_str import doc_inject\n\n\n# if tp.TYPE_CHECKING:\n#     from static_frame.core.type_blocks import TypeBlocks #pylint: disable=W0611 #pragma: no cover\n\nINDEX_LEVEL_SLOTS = (\n            'index',\n            'targets',\n            'offset',\n            '_depth',\n            '_length',\n            )\n\nclass IndexLevel:\n    '''\n    A nestable representation of an Index, where labels in that index optionally point to other Index objects.\n    '''\n    __slots__ = INDEX_LEVEL_SLOTS\n    index: Index\n    targets: tp.Optional[ArrayGO]\n    offset: int\n    _depth: tp.Optional[int]\n    _length: tp.Optional[int]\n\n    STATIC: bool = True\n\n    def __init__(self,\n            index: Index,\n            targets: tp.Optional[ArrayGO] = None,\n            offset: int = 0,\n            own_index: bool = False\n            ):\n        '''\n        Args:\n            index: a 1D Index defining outer-most labels to integers in the `targets` ArrayGO.\n            offset: integer offset for this level.\n            targets: None, or an ArrayGO of IndexLevel objects\n            own_index: Boolean to determine whether the Index can be owned by this IndexLevel; if False, a static index will be reused if appropriate for this IndexLevel class.\n        '''\n        if not isinstance(index, Index) or index.depth > 1:\n            # all derived Index should be depth == 1\n            raise ErrorInitIndexLevel('cannot create an IndexLevel from a higher-dimensional Index.')\n        # NOTE: indices that conatain tuples will take additional work to support; we are not at this time checking for them, though values_at_depth will fail\n\n        if own_index:\n            self.index = index\n        else:\n            self.index = mutable_immutable_index_filter(self.STATIC, index) #type: ignore\n\n        self.targets = targets\n        self.offset = offset\n        self._depth = None\n        self._length = None\n\n    #---------------------------------------------------------------------------\n    def depths(self) -> tp.Iterator[int]:\n        '''\n        Get the depth of all leaves (which should all be the same). Mostly for integrity validation.\n        '''\n        # NOTE: as this uses a list instead of deque, the depths given will not be in the order of the actual leaves\n        if self.targets is None:\n            yield 1\n        else:\n            levels = [(self, 0)]\n            while levels:\n                level, depth = levels.pop()\n                if level.targets is None: # terminus\n                    yield depth + 1\n                else:\n                    next_depth = depth + 1\n                    levels.extend([(lvl, next_depth) for lvl in level.targets])\n\n    def _get_depth(self) -> int:\n        '''\n        Assuming all depths are uniform, can get the depth without storing levels list. Could store a depth attribute, but all nested components with provide overlapping depth descriptions that are never examined.\n        '''\n        if not len(self.index): # if zero sized, depth is zero\n            # TODO: need a way to represent 0-length IndexLevels with non-zero depth\n            return 1\n        if self.targets is None:\n            return 1\n        level, depth = self, 1\n        while True:\n            if level.targets is None: # terminus\n                return depth\n            level, depth = level.targets[0], depth + 1\n\n    @property\n    def depth(self) -> int:\n        if self._depth is None:\n            self._depth = self._get_depth()\n        return self._depth\n\n    def _get_length(self) -> int:\n        if self.targets is None:\n            return self.index.__len__()\n\n        count = 0\n        levels = [self]\n        while levels:\n            level = levels.pop()\n            if level.targets is None: # terminus\n                count += level.index.__len__()\n            else:\n                levels.extend(level.targets)\n        return count\n\n    def __len__(self) -> int:\n        '''\n        The length is the sum of all leaves\n        '''\n        if self._length is None:\n            self._length = self._get_length()\n        return self._length\n\n    #---------------------------------------------------------------------------\n    def label_widths_at_depth(self,\n            depth_level: int = 0\n            ) -> tp.Iterator[tp.Tuple[tp.Hashable, int]]:\n        '''\n        Generator of pairs of label, width, for all labels found at a specified level.\n        '''\n        # given a: 1, 2, b: 1, 2, return ('a', 2), ('b', 2)\n\n        def get_widths(index: Index,\n                targets: tp.Optional[ArrayGO]\n                ) -> tp.Iterator[tp.Tuple[tp.Hashable, int]]:\n            if targets is None:\n                for label in index:\n                    yield (label, 1)\n            else: # observe the offsets of the next\n                transversed = 0\n                for i, (label, level_next) in enumerate(\n                        zip_longest(index, targets[1:], fillvalue=None)\n                        ):\n                    if level_next is not None:\n                        # print(label, level_next.offset, transversed)\n                        # if the next offset is zero, we are moving to a component that is under a fresh hierarchy\n                        if level_next.offset > 0:\n                            delta = level_next.offset - transversed\n                        else:\n                            delta = len(targets[i])\n                        yield label, delta\n                        # get only the incremental addition for this label\n                        transversed += delta\n                    else:\n                        # we cannot use offset; must to more expensive length of component Levels\n                        yield label, len(targets[i])\n\n        levels = deque(((self, 0),))\n        while levels:\n            level, depth = levels.popleft()\n            if depth == depth_level:\n                yield from get_widths(level.index, level.targets)\n                continue # do not need to descend\n            if level.targets is not None: # terminus\n                next_depth = depth + 1\n                levels.extend([(lvl, next_depth) for lvl in level.targets])\n\n\n    def labels_at_depth(self,\n            depth_level: int = 0\n            ) -> tp.Iterator[np.ndarray]:\n        '''\n        Generator of arrays found at a depth level.\n        '''\n        levels = deque(((self, 0),))\n        while levels:\n            level, depth = levels.popleft()\n            if depth == depth_level:\n                yield level.index.values\n                continue # do not need to descend\n            if level.targets is not None: # terminus\n                next_depth = depth + 1\n                levels.extend([(lvl, next_depth) for lvl in level.targets])\n\n\n    def label_nodes_at_depth(self, depth_level: int) -> tp.Iterator[tp.Hashable]:\n        '''Given a depth position, iterate over label nodes at that depth. Only nodes will be provided, which for outer depths may not be of length equal to the entir index.\n        '''\n        if depth_level == 0:\n            yield from self.index\n        else:\n            levels = deque(((self, 0),))\n            while levels:\n                level, depth = levels.popleft()\n                if depth == depth_level:\n                    yield from level.index\n                    continue # do not need to descend\n                if level.targets is not None: # terminus\n                    next_depth = depth + 1\n                    levels.extend([(lvl, next_depth) for lvl in level.targets])\n\n\n    # TODO: consider a different name; was dtypes()\n    def dtypes_iter(self) -> tp.Iterator[np.dtype]:\n        '''Return an iterator of all dtypes from every depth level.'''\n        if self.targets is None:\n            yield self.index.values.dtype\n        else:\n            levels = [self]\n            while levels:\n                level = levels.pop()\n                yield level.index.values.dtype\n                if level.targets is not None: # not terminus\n                    levels.extend(level.targets)\n\n\n    def dtypes_at_depth(self, depth_level: int) -> tp.Iterator[np.dtype]:\n        '''\n        Return all dtypes found on a depth.\n        '''\n        levels = deque(((self, 0),))\n        while levels:\n            level, depth = levels.popleft()\n            if depth == depth_level:\n                yield level.index.dtype\n                continue # do not need to descend\n            if level.targets is not None: # terminus\n                next_depth = depth + 1\n                levels.extend([(lvl, next_depth) for lvl in level.targets])\n\n    def dtype_per_depth(self) -> tp.Iterator[np.dtype]:\n        '''Return a tuple of resolved dtypes, one from each depth level.'''\n        depth_count = self.depth\n        for d in range(depth_count):\n            yield resolve_dtype_iter(self.dtypes_at_depth(d))\n\n    # consider renaming index_types_per_depth\n    def index_types(self) -> tp.Iterator[np.dtype]:\n        '''Return an iterator of reprsentative Index classes, one from each depth level.'''\n        if self.targets is None:\n            yield self.index.__class__\n        else:\n            levels = [self]\n            while levels:\n                level = levels.pop()\n                yield level.index.__class__\n                if level.targets is not None: # not terminus\n                    levels.append(level.targets[0])\n                else:\n                    break\n\n    #---------------------------------------------------------------------------\n    def __contains__(self, key: tp.Iterable[tp.Hashable]) -> bool:\n        '''Given an iterable of single-element level keys (a leaf loc), return a bool.\n        '''\n        if not hasattr(key, '__iter__') or isinstance(key, str):\n            return False\n\n        node = self\n        for k in key:\n            if not node.index.__contains__(k):\n                return False\n\n            if node.targets is not None:\n                node = node.targets[node.index.loc_to_iloc(k)]\n                continue\n\n            node.index.loc_to_iloc(k)\n            return True # if above does not raise\n\n        return False\n\n    def leaf_loc_to_iloc(self,\n            key: tp.Union[tp.Iterable[tp.Hashable], tp.Type[ILoc], tp.Type[HLoc]]\n            ) -> int:\n        '''Given an iterable of single-element level keys (a leaf loc), return the iloc value.\n\n        Note that key components (level selectors) cannot be slices, lists, or np.ndarray.\n        '''\n        if isinstance(key, ILoc):\n            return key.key\n\n        node = self\n        pos = 0\n        key_depth_max = len(key) - 1 #type: ignore\n\n        # NOTE: rather than a for/enumerate, this could use a while loop on an iter() and explicitly look at next() results to determine if the key matches\n        for key_depth, k in enumerate(key): #type: ignore\n            if isinstance(k, KEY_MULTIPLE_TYPES):\n                raise RuntimeError(f'slices cannot be used in a leaf selection into an IndexHierarchy; try HLoc[{key}].')\n            if node.targets is not None:\n                node = node.targets[node.index.loc_to_iloc(k)]\n                pos += node.offset\n            else: # targets is None, meaning we are at max depth\n                # k returns an integer\n                offset = node.index.loc_to_iloc(k)\n                assert isinstance(offset, INT_TYPES) # enforces leaf loc\n                if key_depth == key_depth_max:\n                    return pos + offset\n                break # return exception below if key_depth not max depth\n\n        raise KeyError(f'Invalid key length {key_depth_max + 1}; must be length {self.depth}.')\n\n    def loc_to_iloc(self, key: GetItemKeyTypeCompound) -> GetItemKeyType:\n        '''\n        This is the low-level loc_to_iloc, analagous to LocMap.loc_to_iloc as used by Index. As such, the key at this point should not be a Series or Index object.\n\n        If key is an np.ndarray, a Boolean array will be passed through; otherwise, it will be treated as an iterable of values to be passed to leaf_loc_to_iloc.\n        '''\n        if isinstance(key, slice):\n            # given a top-level definition of a slice (and if that slice results in a single value), we can get a value range\n            return slice(*LocMap.map_slice_args(self.leaf_loc_to_iloc, key))\n\n        # this should not match tuples that are leaf-locs\n        if isinstance(key, KEY_ITERABLE_TYPES):\n            if isinstance(key, np.ndarray) and key.dtype == bool:\n                return key # keep as Boolean\n            return [self.leaf_loc_to_iloc(x) for x in key]\n\n        if not isinstance(key, HLoc):\n            # assume it is a leaf loc tuple\n            if not isinstance(key, tuple):\n                raise KeyError(f'{key} cannot be used for loc selection from IndexHierarchy; try HLoc')\n            return self.leaf_loc_to_iloc(key)\n\n        # everything after this is an HLoc\n        # collect all ilocs for all leaf indices matching HLoc patterns\n        ilocs = []\n        levels = deque(((self, 0, 0),)) # order matters\n\n        while levels:\n            level, depth, offset = levels.popleft()\n            depth_key = key[depth]\n            next_offset = offset + level.offset\n\n            # print(level, depth, offset, depth_key, next_offset)\n            if level.targets is None:\n                try:\n                    ilocs.append(level.index.loc_to_iloc(depth_key, offset=next_offset))\n                except KeyError:\n                    pass\n            else: # target is iterable np.ndaarray\n                try:\n                    iloc = level.index.loc_to_iloc(depth_key) # no offset\n                except KeyError:\n                    pass\n                else:\n                    level_targets = level.targets[iloc] # get one or more IndexLevel objects\n                    next_depth = depth + 1\n                    # if not an ndarray, iloc has extracted a single IndexLevel\n                    if isinstance(level_targets, IndexLevel):\n                        levels.append((level_targets, next_depth, next_offset))\n                    else:\n                        levels.extend([(lvl, next_depth, next_offset)\n                                for lvl in level_targets])\n\n        iloc_count = len(ilocs)\n        if iloc_count == 0:\n            # import ipdb; ipdb.set_trace()\n            raise KeyError('no matching keys across all levels')\n\n        if iloc_count == 1 and not key.has_key_multiple():\n            # drop to a single iloc selection\n            return ilocs[0]\n\n        # NOTE: might be able to combine contiguous ilocs into a single slice\n        iloc_flat: tp.List[GetItemKeyType] = [] # combine into one flat iloc\n        length = self.__len__()\n        for part in ilocs:\n            if isinstance(part, slice):\n                iloc_flat.extend(range(*part.indices(length)))\n            # just look for ints\n            elif isinstance(part, INT_TYPES):\n                iloc_flat.append(part)\n            else: # assume it is an iterable\n                assert part is not None\n                iloc_flat.extend(part)\n        return iloc_flat\n\n    #---------------------------------------------------------------------------\n    @property\n    def values(self) -> np.ndarray:\n        '''\n        Return an immutable NumPy 2D array of all labels found in this IndexLevels instance. This may coerce types.\n        '''\n        depth_count = self.depth\n        shape = self.__len__(), depth_count\n\n        # need to get a compatible dtype for all dtypes\n        dtype = resolve_dtype_iter(self.dtypes_iter())\n        labels = np.empty(shape, dtype=dtype)\n        row_count = 0\n\n        levels = deque(((self, 0, None),)) # order matters\n\n        while levels:\n            level, depth, row_previous = levels.popleft()\n\n            if level.targets is None:\n                rows = len(level.index.values)\n                row_slice = slice(row_count, row_count + rows)\n                labels[row_slice, :] = row_previous\n                labels[row_slice, depth] = level.index.values\n                row_count += rows\n\n            else: # target is iterable np.ndaarray\n                depth_next = depth + 1\n                for label, level_target in zip(level.index.values, level.targets):\n                    if row_previous is None:\n                        # shown to be faster to allocate entire row width\n                        row = np.empty(depth_count, dtype=dtype)\n                    else:\n                        row = row_previous.copy()\n                    row[depth] = label\n                    levels.append((level_target, depth_next, row))\n\n        labels.flags.writeable = False\n        return labels\n\n    def __iter__(self) -> tp.Iterator[tp.Tuple[tp.Hashable, ...]]:\n        # NOTE: this implementation shown to be faster than a recursive purely recursive implementation.\n        depth_count = self.depth\n        levels = deque(((self, 0, None),)) # order matters\n\n        # level: IndexLevel\n        # depth: int\n        # row_previous: tp.Optional[tp.List[tp.Hashable]]\n\n        while levels:\n            level, depth, row_previous = levels.popleft()\n\n            if level.targets is None:\n                for v in level.index.values:\n                    row_previous[depth] = v #type: ignore\n                    yield tuple(row_previous) #type: ignore\n            else: # target is iterable np.ndaarray\n                depth_next = depth + 1\n                for label, level_target in zip(level.index.values, level.targets):\n                    if row_previous is None:\n                        # shown to be faster to allocate entire row width\n                        row = [None] * depth_count\n                    else:\n                        row = row_previous.copy()\n                    row[depth] = label\n                    levels.append((level_target, depth_next, row)) #type: ignore\n\n    # def __iter__(self) -> tp.Iterator[tp.Tuple[tp.Hashable, ...]]:\n    #     part = [None] * self.depth\n    #     yield from _iter_recurse(self, part, 0)\n\n    def values_at_depth(self,\n            depth_level: int\n            ) -> np.ndarray:\n        '''\n        For the given depth, return a correctly typed immutable array of length equal to the number of rows in the cosolidate values presentation.\n        '''\n        depth_count = self.depth\n        dtype = tuple(self.dtype_per_depth())[depth_level]\n        length = self.__len__()\n        # pre allocate array to ensure we use a resovled type\n        array = np.empty(length, dtype=dtype)\n\n        if depth_level == depth_count - 1:\n            # at maximal depth, can concat underlying arrays\n            np.concatenate(\n                    tuple(self.labels_at_depth(depth_level)),\n                    out=array\n                    )\n        else:\n            def gen() -> tp.Iterator[np.ndarray]:\n                for value, size in self.label_widths_at_depth(\n                        depth_level=depth_level):\n                    if dtype.kind == 'O' and isinstance(value, tuple):\n                        # this appears to the only way to do this:\n                        part = np.empty(size, dtype=dtype)\n                        for i in range(size):\n                            part[i] = value\n                        yield part\n                    else:\n                        yield np.full(size, value, dtype=dtype)\n\n            np.concatenate(tuple(gen()), out=array)\n\n            #NOTE: This alternative form produced a unicode error only on Windows up ot NP 1.17 for some tests\n            # start = 0\n            # for value, size in self.label_widths_at_depth(depth_level):\n            #     end = start + size\n            #     array[start: end] = value\n            #     start = end\n\n        array.flags.writeable = False\n        return array\n\n    #---------------------------------------------------------------------------\n    @doc_inject()\n    def equals(self,\n            other: tp.Any,\n            *,\n            compare_name: bool = False,\n            compare_dtype: bool = False,\n            compare_class: bool = False,\n            skipna: bool = True,\n            ) -> bool:\n        '''\n        {doc}\n\n        Args:\n            {compare_name}\n            {compare_dtype}\n            {compare_class}\n            {skipna}\n        '''\n        if id(other) == id(self):\n            return True\n\n        if compare_class and self.__class__ != other.__class__:\n            return False\n        elif not isinstance(other, IndexLevel):\n            return False\n\n        # same type from here\n        if self.__len__() != other.__len__():\n            return False\n        if self.depth != other.depth:\n            return False\n\n        kwargs = dict(\n                compare_name=compare_name,\n                compare_dtype=compare_dtype,\n                compare_class=compare_class,\n                skipna=skipna,\n                )\n\n        if self.targets is None and other.targets is None:\n            return self.index.equals(other.index, **kwargs) #type: ignore\n\n        # same length and depth, can traverse trees\n        # can store tuple of object ids to note those that have already been examine.\n        equal_pairs = set()\n\n        levels_self = [self]\n        levels_other = [other]\n        while levels_self and levels_other:\n            level_self = levels_self.pop()\n            level_other = levels_other.pop()\n\n            pair = (id(level_self.index), id(level_other.index))\n            pair_found = pair in equal_pairs\n\n            if not pair_found and not level_self.index.equals(level_other.index, **kwargs):\n                return False\n\n            if not pair_found: # but we know it is equal\n                equal_pairs.add(pair)\n\n            if level_self.targets is not None and level_other.targets is not None: # not terminus\n                levels_self.extend(level_self.targets)\n                levels_other.extend(level_other.targets)\n            if level_self.targets is None and level_other.targets is None: # not terminus\n                continue\n            if level_self.targets is None or level_other.targets is None: # not terminus\n                # at least one is at a terminus, but maybe both\n                return False\n\n        if not levels_self and not levels_other:\n            return True # both exhausted\n        return False # one excited early: will we ever get here?\n\n    #---------------------------------------------------------------------------\n    # exporters\n\n    def to_index_level(self,\n            offset: tp.Optional[int] = 0,\n            cls: tp.Optional[tp.Type['IndexLevel']] = None,\n            ) -> 'IndexLevel':\n        '''\n        A deepcopy with optional adjustments, such as a different offset and possibly a different class. The supplied class will be used to construct the IndexLevel instance (as well as internal indices), permitting the production of an IndexLevelGO.\n\n        Args:\n            offset: optionally provide a new offset for the copy. This is not applied recursively\n        '''\n        cls = cls if cls else self.__class__\n\n        index = mutable_immutable_index_filter(cls.STATIC, self.index)\n\n        if self.targets is not None:\n            targets: tp.Optional[ArrayGO] = ArrayGO(\n                [t.to_index_level(offset=None, cls=cls) for t in self.targets],\n                own_iterable=True)\n        else:\n            targets = None\n\n        offset = self.offset if offset is None else offset\n        return cls(index=index, targets=targets, offset=offset) #type: ignore\n\n\n    def to_type_blocks(self) -> TypeBlocks:\n        '''\n        Provide a correctly typed TypeBlocks representation.\n        '''\n        try:\n            depth_count = self.depth\n        except StopIteration:\n            # assume we have no depth or length\n            return TypeBlocks.from_zero_size_shape()\n\n        return TypeBlocks.from_blocks(\n                self.values_at_depth(d) for d in range(depth_count)\n                )\n\n#-------------------------------------------------------------------------------\nclass IndexLevelGO(IndexLevel):\n    '''Grow only variant of IndexLevel\n    '''\n    __slots__ = INDEX_LEVEL_SLOTS\n    index: IndexGO\n    targets: tp.Optional[np.ndarray]\n    offset: int\n    _depth: tp.Optional[int]\n    _length: tp.Optional[int]\n\n    STATIC: bool = False\n\n    #---------------------------------------------------------------------------\n    # grow only mutation\n    # depth cannot change over the life of IndexLevel\n\n    def extend(self, level: IndexLevel) -> None:\n        '''Extend this IndexLevel with another IndexLevel, assuming that it has compatible depth and Index types.\n        '''\n        depth = self.depth\n\n        if level.targets is None:\n            raise RuntimeError('found IndexLevel with None as targets')\n        if depth != level.depth:\n            raise RuntimeError('level for extension does not have necessary levels.')\n        if tuple(self.index_types()) != tuple(level.index_types()):\n            raise RuntimeError('level for extension does not have corresponding types.')\n\n        # this will raise for duplicates\n        self.index.extend(level.index.values)\n\n        def target_gen() -> tp.Iterator[GetItemKeyType]:\n            offset_prior = self.__len__()\n            for t in level.targets: #type: ignore\n                # only need to update offsets at this level, as lower levels are relative to this\n                target = t.to_index_level(offset_prior, cls=self.__class__)\n                offset_prior += len(target)\n                yield target\n\n        if self.targets is None:\n            raise RuntimeError('found IndexLevel with None as targets')\n\n        self.targets.extend(target_gen())\n\n        # defer calculation be setting _length to None\n        self._length = None\n\n    def append(self, key: tp.Sequence[tp.Hashable]) -> None:\n        '''Add a single, full-depth leaf loc.\n        '''\n        # find fist depth that does not contain key\n        depth_count = self.depth\n        index_types = tuple(self.index_types())\n\n        if len(key) != depth_count:\n            raise RuntimeError('appending key {} of insufficent depth {}'.format(\n                        key, depth_count))\n\n        depth_not_found = -1\n        edge_nodes = np.empty(depth_count, dtype=object)\n\n        node = self\n        for depth, k in enumerate(key):\n            edge_nodes[depth] = node\n            # only set on first encounter in descent\n            if depth_not_found == -1 and not node.index.__contains__(k):\n                depth_not_found = depth\n            if node.targets is not None:\n                node = node.targets[-1]\n\n        if depth_not_found == -1:\n            raise RuntimeError('unable to set depth_not_found') #pragma: no cover\n\n        level_previous = None\n\n        # iterate from the innermost depth out\n        for depth in range(depth_count - 1, depth_not_found - 1, -1):\n            node = edge_nodes[depth]\n            k = key[depth]\n\n            if depth == depth_not_found:\n                # when at the the depth not found, we always update the index\n                node.index.append(k)\n                # if we have targets, must update them\n                if node.targets is not None:\n                    assert level_previous is not None\n                    level_previous.offset = node.__len__()\n                    node.targets.append(level_previous)\n            else: # depth not found is higher up\n                # NOTE: do not need to use index_from_optional_constructor, as no explicit constructor is being supplied, and we can expect that the existing types must be valid\n                index_constructor = index_types[depth]\n                if node.targets is None:\n                    # we are at the max depth; will need to create a LevelGO to append in the next level\n                    level_previous = IndexLevelGO(\n                            index=index_constructor((k,)),\n                            offset=0,\n                            targets=None\n                            )\n                else:\n                    targets = ArrayGO([level_previous,], own_iterable=True)\n                    level_previous = IndexLevelGO(\n                            index=index_constructor((k,)),\n                            offset=0,\n                            targets=targets\n                            )\n        # defer calculation be setting _length to None for all edge levels\n        for node in edge_nodes:\n            node._length = None\n\n\n\n\n"""
static_frame/core/interface.py,2,"b""'''\nTools for documenting the SF interface.\n'''\nimport typing as tp\nimport inspect\nfrom itertools import chain\n\nimport numpy as np\n\nfrom static_frame.core.frame import Frame\nfrom static_frame.core.bus import Bus\n\nfrom static_frame.core.util import DT64_S\nfrom static_frame.core.util import AnyCallable\n\nfrom static_frame.core.container import ContainerBase\nfrom static_frame.core.container import ContainerOperand\n\nfrom static_frame.core.type_blocks import TypeBlocks\nfrom static_frame.core.index_base import IndexBase\n\nfrom static_frame.core.index_datetime import IndexDate\nfrom static_frame.core.index_datetime import IndexYearMonth\nfrom static_frame.core.index_datetime import IndexYear\n\nfrom static_frame.core.index_hierarchy import IndexHierarchy\nfrom static_frame.core.display import Display\nfrom static_frame.core.frame import FrameAsType\n\nfrom static_frame.core.node_iter import IterNodeDelegate\n\nfrom static_frame.core.container import _UFUNC_BINARY_OPERATORS\nfrom static_frame.core.container import _RIGHT_OPERATOR_MAP\nfrom static_frame.core.container import _UFUNC_UNARY_OPERATORS\n\nfrom static_frame.core.node_selector import TContainer\nfrom static_frame.core.node_selector import Interface\nfrom static_frame.core.node_selector import InterfaceSelectDuo\nfrom static_frame.core.node_selector import InterfaceSelectTrio\nfrom static_frame.core.node_selector import InterfaceAssignTrio\nfrom static_frame.core.node_selector import InterfaceAssignQuartet\n\nfrom static_frame.core.node_selector import InterfaceAsType\nfrom static_frame.core.node_selector import InterfaceGetItem\n\nfrom static_frame.core.node_dt import InterfaceDatetime\nfrom static_frame.core.node_str import InterfaceString\n\n\n#-------------------------------------------------------------------------------\n# function inspection utilities\n\nMAX_ARGS = 3\n\ndef _get_parameters(\n        func: AnyCallable,\n        is_getitem: bool = False,\n        max_args: int = MAX_ARGS,\n        ) -> str:\n    # might need special handling for methods on built-ins\n    try:\n        sig = inspect.signature(func)\n    except ValueError:\n        # on Python 3.6, this error happens:\n        # ValueError: no signature found for builtin <built-in function abs>\n        return '[]' if is_getitem else '()'\n\n    positional = []\n    kwarg_only = ['*'] # preload\n    var_positional = ''\n    var_keyword = ''\n\n    count = 0\n    count_total = 0\n    for p in sig.parameters.values():\n        if count == 0 and p.name == 'self':\n            continue # do not increment counts\n\n        if count < max_args:\n            if p.kind == p.KEYWORD_ONLY:\n                kwarg_only.append(p.name)\n            elif p.kind == p.VAR_POSITIONAL:\n                var_positional = p.name\n            elif p.kind == p.VAR_KEYWORD:\n                var_keyword = p.name\n            else:\n                positional.append(p.name)\n            count += 1\n        count_total += 1\n\n    suffix = '' if count >= count_total else f', {Display.ELLIPSIS}'\n\n    # if truthy, update to a proper iterable\n    if var_positional:\n        var_positional = ('*' + var_positional,) #type: ignore\n    if var_keyword:\n        var_keyword = ('**' + var_keyword,)  #type: ignore\n\n    if len(kwarg_only) > 1: # do not count the preload\n        param_repr = ', '.join(chain(positional, kwarg_only, var_positional, var_keyword))\n    else:\n        param_repr = ', '.join(chain(positional, var_positional, var_keyword))\n\n    if is_getitem:\n        return f'[{param_repr}{suffix}]'\n    return f'({param_repr}{suffix})'\n\n\ndef _get_signatures(\n        name: str,\n        func: AnyCallable,\n        *,\n        is_getitem: bool = False,\n        delegate_func: tp.Optional[AnyCallable] = None,\n        delegate_name: str = '',\n        max_args: int = MAX_ARGS,\n        ) -> tp.Tuple[str, str]:\n\n    if delegate_func:\n        delegate = _get_parameters(delegate_func, max_args=max_args)\n        if delegate_name:\n            # prefix with name\n            delegate = f'.{delegate_name}{delegate}'\n            delegate_no_args = f'.{delegate_name}()'\n        else:\n            delegate_no_args = '()'\n    else:\n        delegate = ''\n        delegate_no_args = ''\n\n    signature = f'{name}{_get_parameters(func, is_getitem, max_args=max_args)}{delegate}'\n\n    if is_getitem:\n        signature_no_args = f'{name}[]{delegate_no_args}'\n    else:\n        signature_no_args = f'{name}(){delegate_no_args}'\n\n    return signature, signature_no_args\n\n\n#-------------------------------------------------------------------------------\nclass Features:\n    '''\n    Core utilities neede by both Interface and InterfaceSummary\n    '''\n\n    DOC_CHARS = 80\n\n    GETITEM = '__getitem__'\n\n    EXCLUDE_PRIVATE = {\n        '__class__',\n        '__class_getitem__',\n        '__annotations__',\n        '__doc__',\n        '__delattr__',\n        '__dir__',\n        '__dict__',\n        '__format__',\n        '__getattribute__',\n        '__hash__',\n        '__init_sbclass__',\n        '__lshift__',\n        '__module__',\n        '__init_subclass__',\n        '__new__',\n        '__setattr__',\n        '__setstate__',\n        '__setitem__',\n        '__slots__',\n        '__slotnames__',\n        '__subclasshook__',\n        '__weakref__',\n        '__reduce__',\n        '__reduce_ex__',\n        '__sizeof__',\n        }\n\n    DICT_LIKE = {\n        'get',\n        'keys',\n        'values',\n        'items',\n        '__contains__',\n        '__iter__',\n        '__reversed__'\n        }\n\n    DISPLAY = {\n        'display',\n        'display_tall',\n        'display_wide',\n        '__repr__',\n        '__str__',\n        'interface',\n        }\n\n\n    @classmethod\n    def scrub_doc(cls, doc: tp.Optional[str]) -> str:\n        if not doc:\n            return ''\n        doc = doc.replace('`', '')\n        doc = doc.replace(':py:meth:', '')\n        doc = doc.replace(':obj:', '')\n        doc = doc.replace('static_frame.', '')\n\n        # split and join removes contiguous whitespace\n        msg = ' '.join(doc.split())\n        if len(msg) <= cls.DOC_CHARS:\n            return msg\n        return msg[:cls.DOC_CHARS].strip() + Display.ELLIPSIS\n\n\n#-------------------------------------------------------------------------------\nclass InterfaceGroup:\n    Attribute = 'Attribute'\n    Constructor = 'Constructor'\n    DictLike = 'Dictionary-Like'\n    Display = 'Display'\n    Exporter = 'Exporter'\n    Iterator = 'Iterator'\n    Method = 'Method'\n    OperatorBinary = 'Operator Binary'\n    OperatorUnary = 'Operator Unary'\n    Selector = 'Selector'\n    Assignment = 'Assignment'\n    AccessorString = 'Accessor String'\n    AccessorDatetime = 'Accessor Datetime'\n\n\nclass InterfaceRecord(tp.NamedTuple):\n\n    cls_name: str\n    group: str # should be InterfaceGroup\n    signature: str\n    doc: str\n    reference: str = '' # a qualified name as a string for doc gen\n    use_signature: bool = False\n    is_attr: bool = False\n    delegate_reference: str = ''\n    delegate_is_attr: bool = False\n    signature_no_args: str = ''\n\n    @classmethod\n    def gen_from_dict_like(cls, *,\n            cls_name: str,\n            name: str,\n            obj: AnyCallable,\n            reference: str,\n            doc: str,\n            max_args: int,\n            ) -> tp.Iterator['InterfaceRecord']:\n        if name == 'values':\n            signature = signature_no_args = name\n        else:\n            signature, signature_no_args = _get_signatures(\n                    name,\n                    obj,\n                    is_getitem=False,\n                    max_args=max_args,\n                    )\n        yield cls(cls_name,\n                InterfaceGroup.DictLike,\n                signature,\n                doc,\n                reference,\n                signature_no_args=signature_no_args\n                )\n\n    @classmethod\n    def gen_from_display(cls, *,\n            cls_name: str,\n            name: str,\n            obj: AnyCallable,\n            reference: str,\n            doc: str,\n            max_args: int,\n            ) -> tp.Iterator['InterfaceRecord']:\n        if name != 'interface':\n            # signature = f'{name}()'\n            signature, signature_no_args = _get_signatures(\n                    name,\n                    obj,\n                    is_getitem=False,\n                    max_args=max_args,\n                    )\n            yield cls(cls_name,\n                    InterfaceGroup.Display,\n                    signature,\n                    doc,\n                    reference,\n                    signature_no_args=signature_no_args\n                    )\n        else: # interface attr\n            yield cls(cls_name,\n                    InterfaceGroup.Display,\n                    name,\n                    doc,\n                    use_signature=True,\n                    signature_no_args=name\n                    )\n\n    @classmethod\n    def gen_from_astype(cls, *,\n            cls_name: str,\n            name: str,\n            obj: AnyCallable,\n            reference: str,\n            doc: str,\n            max_args: int,\n            ) -> tp.Iterator['InterfaceRecord']:\n        # InterfaceAsType found on Frame, IndexHierarchy\n        if isinstance(obj, InterfaceAsType):\n            for field in obj.INTERFACE:\n\n                delegate_obj = getattr(obj, field)\n                delegate_reference = f'{obj.__class__.__name__}.{field}'\n\n                if field == Features.GETITEM:\n                    # the cls.getitem version returns a FrameAsType\n                    signature, signature_no_args = _get_signatures(\n                            name,\n                            delegate_obj,\n                            is_getitem=True,\n                            delegate_func=FrameAsType.__call__,\n                            max_args=max_args,\n                            )\n                else:\n                    signature, signature_no_args = _get_signatures(\n                            name,\n                            delegate_obj,\n                            is_getitem=False,\n                            max_args=max_args,\n                            )\n                doc = Features.scrub_doc(getattr(InterfaceAsType, field).__doc__)\n                yield cls(cls_name,\n                        InterfaceGroup.Method,\n                        signature,\n                        doc,\n                        reference,\n                        use_signature=True,\n                        is_attr=True,\n                        delegate_reference=delegate_reference,\n                        signature_no_args=signature_no_args\n                        )\n        else: # Series, Index, astype is just a method\n            signature, signature_no_args = _get_signatures(name, obj, max_args=max_args)\n            yield cls(cls_name,\n                    InterfaceGroup.Method,\n                    signature,\n                    doc,\n                    reference,\n                    signature_no_args=signature_no_args\n                    )\n\n\n    @classmethod\n    def gen_from_constructor(cls, *,\n            cls_name: str,\n            name: str,\n            obj: AnyCallable,\n            reference: str,\n            doc: str,\n            max_args: int,\n            ) -> tp.Iterator['InterfaceRecord']:\n\n        signature, signature_no_args = _get_signatures(\n                name,\n                obj,\n                is_getitem=False,\n                max_args=max_args,\n                )\n        yield cls(cls_name,\n                InterfaceGroup.Constructor,\n                signature,\n                doc,\n                reference,\n                signature_no_args=signature_no_args\n                )\n\n    @classmethod\n    def gen_from_exporter(cls, *,\n            cls_name: str,\n            name: str,\n            obj: AnyCallable,\n            reference: str,\n            doc: str,\n            max_args: int,\n            ) -> tp.Iterator['InterfaceRecord']:\n\n        signature, signature_no_args = _get_signatures(\n                name,\n                obj,\n                is_getitem=False,\n                max_args=max_args,\n                )\n        yield cls(cls_name,\n                InterfaceGroup.Exporter,\n                signature,\n                doc,\n                reference,\n                signature_no_args=signature_no_args\n                )\n\n    @classmethod\n    def gen_from_iterator(cls, *,\n            cls_name: str,\n            name: str,\n            obj: AnyCallable,\n            reference: str,\n            doc: str,\n            max_args: int,\n            ) -> tp.Iterator['InterfaceRecord']:\n\n        signature, signature_no_args = _get_signatures(\n                name,\n                obj.__call__, #type: ignore\n                is_getitem=False,\n                max_args=max_args,\n                )\n\n        yield cls(cls_name,\n                InterfaceGroup.Iterator,\n                signature,\n                doc,\n                reference,\n                use_signature=True,\n                is_attr=True, # doc as attr so sphinx does not add parens to sig\n                signature_no_args=signature_no_args,\n                )\n\n        for field in IterNodeDelegate.INTERFACE: # apply, map, etc\n            delegate_obj = getattr(IterNodeDelegate, field)\n            delegate_reference = f'{IterNodeDelegate.__name__}.{field}'\n            doc = Features.scrub_doc(delegate_obj.__doc__)\n\n            signature, signature_no_args = _get_signatures(\n                    name,\n                    obj.__call__, #type: ignore\n                    is_getitem=False,\n                    delegate_func=delegate_obj,\n                    delegate_name=field,\n                    max_args=max_args,\n                    )\n            yield cls(cls_name,\n                    InterfaceGroup.Iterator,\n                    signature,\n                    doc,\n                    reference,\n                    use_signature=True,\n                    is_attr=True,\n                    delegate_reference=delegate_reference,\n                    signature_no_args=signature_no_args\n                    )\n\n    @classmethod\n    def gen_from_accessor(cls, *,\n            cls_name: str,\n            name: str,\n            obj: AnyCallable,\n            reference: str,\n            doc: str,\n            cls_interface: tp.Type[Interface[TContainer]],\n            max_args: int,\n            ) -> tp.Iterator['InterfaceRecord']:\n\n        group = (InterfaceGroup.AccessorString\n                if cls_interface is InterfaceString\n                else InterfaceGroup.AccessorDatetime)\n\n        for field in cls_interface.INTERFACE: # apply, map, etc\n            delegate_obj = getattr(cls_interface, field)\n            delegate_reference = f'{cls_interface.__name__}.{field}'\n            doc = Features.scrub_doc(delegate_obj.__doc__)\n\n            terminus_name = f'{name}.{field}'\n\n            if isinstance(delegate_obj, property):\n                # some date tools are properties\n                yield InterfaceRecord(cls_name,\n                        group,\n                        terminus_name,\n                        doc,\n                        reference,\n                        is_attr=True,\n                        use_signature=True,\n                        delegate_reference=delegate_reference,\n                        delegate_is_attr=True,\n                        signature_no_args=terminus_name\n                        )\n            else:\n                signature, signature_no_args = _get_signatures(\n                        terminus_name,\n                        delegate_obj,\n                        max_args=max_args,\n                        )\n                yield cls(cls_name,\n                        group,\n                        signature,\n                        doc,\n                        reference,\n                        is_attr=True,\n                        use_signature=True,\n                        delegate_reference=delegate_reference,\n                        signature_no_args=signature_no_args\n                        )\n\n    @classmethod\n    def from_getitem(cls, *,\n            cls_name: str,\n            name: str,\n            obj: AnyCallable,\n            reference: str,\n            doc: str,\n            max_args: int,\n            ) -> tp.Iterator['InterfaceRecord']:\n        '''\n        For root __getitem__ methods, as well as __getitem__ on InterfaceGetItem objects.\n        '''\n        if name != Features.GETITEM:\n            target = obj.__getitem__ #type: ignore\n        else:\n            target = obj\n            name = ''\n\n        signature, signature_no_args = _get_signatures(\n                name,\n                target,\n                is_getitem=True,\n                max_args=max_args,\n                )\n\n        yield InterfaceRecord(cls_name,\n                InterfaceGroup.Selector,\n                signature,\n                doc,\n                reference,\n                use_signature=True,\n                is_attr=True,\n                signature_no_args=signature_no_args\n                )\n\n\n    @classmethod\n    def gen_from_selection(cls, *,\n            cls_name: str,\n            name: str,\n            obj: AnyCallable,\n            reference: str,\n            doc: str,\n            cls_interface: tp.Type[Interface[TContainer]],\n            max_args: int,\n            ) -> tp.Iterator['InterfaceRecord']:\n\n        for field in cls_interface.INTERFACE:\n            # get from object, not class\n            delegate_obj = getattr(obj, field)\n            delegate_reference = f'{cls_interface.__name__}.{field}'\n            doc = Features.scrub_doc(delegate_obj.__doc__)\n\n            if field != Features.GETITEM:\n                delegate_is_attr = True\n                signature, signature_no_args = _get_signatures(\n                        f'{name}.{field}', # make compound interface\n                        delegate_obj.__getitem__,\n                        is_getitem=True,\n                        max_args=max_args,\n                        )\n            else: # is getitem\n                delegate_is_attr = False\n                signature, signature_no_args = _get_signatures(\n                        name, # on the root, no change necessary\n                        delegate_obj,\n                        is_getitem=True,\n                        max_args=max_args,\n                        )\n\n            yield InterfaceRecord(cls_name,\n                    InterfaceGroup.Selector,\n                    signature,\n                    doc,\n                    reference,\n                    use_signature=True,\n                    is_attr=True,\n                    delegate_reference=delegate_reference,\n                    delegate_is_attr=delegate_is_attr,\n                    signature_no_args=signature_no_args\n                    )\n\n\n    @classmethod\n    def gen_from_assignment(cls, *,\n            cls_name: str,\n            name: str,\n            obj: tp.Union[InterfaceAssignTrio[TContainer],\n                    InterfaceAssignQuartet[TContainer]],\n            reference: str,\n            doc: str,\n            cls_interface: tp.Type[Interface[TContainer]],\n            max_args: int,\n            ) -> tp.Iterator['InterfaceRecord']:\n\n        for field in cls_interface.INTERFACE:\n\n            # get from object, not class\n            delegate_obj = getattr(obj, field)\n            delegate_reference = f'{cls_interface.__name__}.{field}'\n            delegate_doc = Features.scrub_doc(delegate_obj.__doc__)\n\n            # will be either SeriesAssign or FrameAssign\n            terminus_obj = obj.delegate.__call__\n            terminus_reference = f'{obj.delegate.__name__}.__call__'\n            terminus_doc = Features.scrub_doc(terminus_obj.__doc__)\n\n            # use the delegate to get the root signature, as the root is just a property that returns an InterfaceAssignTrio or similar\n            if field != Features.GETITEM:\n                delegate_is_attr = True\n                signature, signature_no_args = _get_signatures(\n                        f'{name}.{field}', # make compound interface\n                        delegate_obj.__getitem__,\n                        is_getitem=True,\n                        delegate_func=terminus_obj,\n                        max_args=max_args,\n                        )\n            else: # is getitem\n                delegate_is_attr = False\n                signature, signature_no_args = _get_signatures(\n                        name, # on the root, no change necessary\n                        delegate_obj,\n                        is_getitem=True,\n                        delegate_func=terminus_obj,\n                        max_args=max_args,\n                        )\n\n            yield InterfaceRecord(cls_name,\n                    InterfaceGroup.Assignment,\n                    signature,\n                    terminus_doc,\n                    reference,\n                    use_signature=True,\n                    is_attr=False,\n                    delegate_reference=terminus_reference,\n                    signature_no_args=signature_no_args\n                    )\n\n    @classmethod\n    def gen_from_method(cls, *,\n            cls_name: str,\n            name: str,\n            obj: AnyCallable,\n            reference: str,\n            doc: str,\n            max_args: int,\n            ) -> tp.Iterator['InterfaceRecord']:\n\n        signature, signature_no_args = _get_signatures(name, obj, max_args=max_args)\n\n        if name in _UFUNC_UNARY_OPERATORS:\n            yield InterfaceRecord(cls_name,\n                    InterfaceGroup.OperatorUnary,\n                    signature,\n                    doc,\n                    reference,\n                    signature_no_args=signature_no_args\n                    )\n        elif name in _UFUNC_BINARY_OPERATORS or name in _RIGHT_OPERATOR_MAP:\n            yield InterfaceRecord(cls_name,\n                    InterfaceGroup.OperatorBinary,\n                    signature,\n                    doc,\n                    reference,\n                    signature_no_args=signature_no_args\n                    )\n        else:\n            yield InterfaceRecord(cls_name,\n                    InterfaceGroup.Method,\n                    signature,\n                    doc,\n                    reference,\n                    signature_no_args=signature_no_args\n                    )\n\n\n#-------------------------------------------------------------------------------\n\nclass InterfaceSummary(Features):\n\n    _CLS_TO_INSTANCE_CACHE: tp.Dict[tp.Type[ContainerBase], ContainerBase] = {}\n\n    @classmethod\n    def is_public(cls, field: str) -> bool:\n        if field.startswith('_') and not field.startswith('__'):\n            return False\n        if field in cls.EXCLUDE_PRIVATE:\n            return False\n        return True\n\n\n    @classmethod\n    def get_instance(cls, target: tp.Type[ContainerBase]) -> ContainerBase:\n        '''\n        Get a sample instance from any ContainerBase; cache to only create one per life of process.\n        '''\n        if target not in cls._CLS_TO_INSTANCE_CACHE:\n            if target is TypeBlocks:\n                instance = target.from_blocks(np.array((0,))) #type: ignore\n            elif target is Bus:\n                f = Frame.from_elements((0,), name='frame')\n                instance = target.from_frames((f,)) #type: ignore\n            elif issubclass(target, IndexHierarchy):\n                instance = target.from_labels(((0,0),))\n            elif issubclass(target, (IndexYearMonth, IndexYear, IndexDate)):\n                instance = target(np.array((0,), dtype=DT64_S))\n            elif target in (ContainerOperand, ContainerBase, IndexBase):\n                instance = target()\n            elif issubclass(target, Frame):\n                instance = target.from_elements((0,))\n            else:\n                instance = target((0,)) #type: ignore\n            cls._CLS_TO_INSTANCE_CACHE[target] = instance\n        return cls._CLS_TO_INSTANCE_CACHE[target]\n\n    @classmethod\n    def name_obj_iter(cls,\n            target: tp.Type[ContainerBase],\n            ) -> tp.Iterator[tp.Tuple[str, tp.Any, tp.Any]]:\n        instance = cls.get_instance(target=target)\n\n        for name_attr in dir(target.__class__): # get metaclass\n            if name_attr == 'interface':\n                # getting interface off of the class will recurse\n                yield name_attr, None, ContainerBase.__class__.interface #type: ignore\n\n        # force tehse to be ordered at the bottom\n        selectors = ('__getitem__', 'iloc', 'loc')\n        selectors_found = set()\n\n        for name_attr in sorted(dir(target)):\n            if name_attr == 'interface':\n                continue # skip, provided by metaclass\n            if not cls.is_public(name_attr):\n                continue\n            if name_attr in selectors:\n                selectors_found.add(name_attr)\n                continue\n            yield name_attr, getattr(instance, name_attr), getattr(target, name_attr)\n\n\n        for name_attr in selectors:\n            if name_attr in selectors_found:\n                yield name_attr, getattr(instance, name_attr), getattr(target, name_attr)\n\n\n    #---------------------------------------------------------------------------\n    @classmethod\n    def interrogate(cls,\n            target: tp.Type[ContainerBase],\n            *,\n            max_args: int = MAX_ARGS\n            ) -> tp.Iterator[InterfaceRecord]:\n\n        for name_attr, obj, obj_cls in cls.name_obj_iter(target):\n            # properties resdie on the class\n            doc = ''\n            # reference = '' # reference attribute to use\n\n            if isinstance(obj_cls, property):\n                doc = cls.scrub_doc(obj_cls.__doc__)\n            elif hasattr(obj, '__doc__'):\n                doc = cls.scrub_doc(obj.__doc__)\n\n            if hasattr(obj, '__name__'):\n                name = obj.__name__\n            else: # some attributes yield objects like arrays, Series, or Frame\n                name = name_attr\n\n            cls_name = target.__name__\n            reference = f'{cls_name}.{name}'\n\n            kwargs = dict(\n                    cls_name=cls_name,\n                    name=name,\n                    obj=obj,\n                    reference=reference,\n                    doc=doc,\n                    max_args=max_args,\n                    )\n\n            if name in cls.DICT_LIKE:\n                yield from InterfaceRecord.gen_from_dict_like(**kwargs)\n            elif name in cls.DISPLAY:\n                yield from InterfaceRecord.gen_from_display(**kwargs)\n            elif name == 'astype':\n                yield from InterfaceRecord.gen_from_astype(**kwargs)\n            elif name.startswith('from_') or name == '__init__':\n                yield from InterfaceRecord.gen_from_constructor(**kwargs)\n            elif name.startswith('to_'):\n                yield from InterfaceRecord.gen_from_exporter(**kwargs)\n            elif name.startswith('iter_'):\n                yield from InterfaceRecord.gen_from_iterator(**kwargs)\n            elif isinstance(obj, InterfaceGetItem) or name == cls.GETITEM:\n                yield from InterfaceRecord.from_getitem(**kwargs)\n            elif isinstance(obj, InterfaceString):\n                yield from InterfaceRecord.gen_from_accessor(\n                            cls_interface=InterfaceString,\n                            **kwargs,\n                            )\n            elif isinstance(obj, InterfaceDatetime):\n                yield from InterfaceRecord.gen_from_accessor(\n                            cls_interface=InterfaceDatetime,\n                            **kwargs,\n                            )\n            elif obj.__class__ in (InterfaceSelectDuo, InterfaceSelectTrio):\n                yield from InterfaceRecord.gen_from_selection(\n                        cls_interface=obj.__class__,\n                        **kwargs)\n            elif obj.__class__ in (InterfaceAssignTrio, InterfaceAssignQuartet):\n                yield from InterfaceRecord.gen_from_assignment(\n                        cls_interface=obj.__class__,\n                        **kwargs)\n            elif callable(obj): # general methods\n                yield from InterfaceRecord.gen_from_method(**kwargs)\n            else: # attributes\n                yield InterfaceRecord(cls_name,\n                        InterfaceGroup.Attribute,\n                        name,\n                        doc,\n                        reference,\n                        signature_no_args=name\n                        )\n\n    @classmethod\n    def to_frame(cls,\n            target: tp.Type[ContainerBase],\n            *,\n            minimized: bool = True,\n            max_args: int = MAX_ARGS,\n            ) -> Frame:\n        '''\n        Reduce to key fields.\n        '''\n        f = Frame.from_records(\n                cls.interrogate(target, max_args=max_args),\n                name=target.__name__\n                )\n        f = f.sort_values(('cls_name', 'group',))\n        f = f.set_index('signature', drop=True)\n        if minimized:\n            return f[['cls_name', 'group', 'doc']] #type: ignore\n        return f #type: ignore\n\n\n"""
static_frame/core/node_dt.py,11,"b""\nimport typing as tp\nimport numpy as np\n\nfrom static_frame.core.util import DT64_YEAR\nfrom static_frame.core.util import DT64_MONTH\nfrom static_frame.core.util import DT64_DAY\n\nfrom static_frame.core.util import DT64_H\nfrom static_frame.core.util import DT64_M\nfrom static_frame.core.util import DT64_S\nfrom static_frame.core.util import DT64_MS\nfrom static_frame.core.util import DT64_US\nfrom static_frame.core.util import DT64_NS\nfrom static_frame.core.util import DT64_PS\nfrom static_frame.core.util import DT64_FS\nfrom static_frame.core.util import DT64_AS\n\nfrom static_frame.core.util import DTYPE_INT_DEFAULT\nfrom static_frame.core.util import DTYPE_OBJECT\nfrom static_frame.core.util import DTYPE_DATETIME_KIND\nfrom static_frame.core.util import EMPTY_TUPLE\nfrom static_frame.core.util import DTYPE_STR\n\nfrom static_frame.core.util import array_from_element_attr\nfrom static_frame.core.util import array_from_element_method\n\nfrom static_frame.core.node_selector import Interface\nfrom static_frame.core.node_selector import TContainer\n\nif tp.TYPE_CHECKING:\n\n    from static_frame.core.frame import Frame  #pylint: disable = W0611 #pragma: no cover\n    from static_frame.core.index import Index  #pylint: disable = W0611 #pragma: no cover\n    from static_frame.core.index_hierarchy import IndexHierarchy  #pylint: disable = W0611 #pragma: no cover\n    from static_frame.core.series import Series  #pylint: disable = W0611 #pragma: no cover\n    from static_frame.core.type_blocks import TypeBlocks  #pylint: disable = W0611 #pragma: no cover\n\n# only ContainerOperand subclasses\n# TContainer = tp.TypeVar('TContainer', 'Index', 'IndexHierarchy', 'Series', 'Frame', 'TypeBlocks')\n\nBlocksType = tp.Iterable[np.ndarray]\nToContainerType = tp.Callable[[tp.Iterator[np.ndarray]], TContainer]\n\n#https://docs.python.org/3/library/datetime.html\n\nclass InterfaceDatetime(Interface[TContainer]):\n\n    __slots__ = (\n            '_blocks', # function that returns iterable of arrays\n            '_blocks_to_container', # partialed function that will return a new container\n            )\n    INTERFACE = (\n            'year',\n            'month',\n            'day',\n            'weekday',\n            'timetuple',\n            'isoformat',\n            'strftime',\n            )\n\n    DT64_EXCLUDE_YEAR = (DT64_YEAR,)\n    DT64_EXCLUDE_YEAR_MONTH = (DT64_YEAR, DT64_MONTH)\n    DT64_TIME = {\n            DT64_H,\n            DT64_M,\n            DT64_S,\n            DT64_MS,\n            DT64_US,\n            DT64_NS,\n            DT64_PS,\n            DT64_FS,\n            DT64_AS,\n            }\n\n    DT64_EXCLUDE_YEAR_MONTH_SUB_MICRO = {\n            DT64_YEAR,\n            DT64_MONTH,\n            DT64_NS,\n            DT64_PS,\n            DT64_FS,\n            DT64_AS,\n            }\n\n    def __init__(self,\n            blocks: BlocksType,\n            blocks_to_container: ToContainerType[TContainer]\n            ) -> None:\n        self._blocks: BlocksType = blocks\n        self._blocks_to_container: ToContainerType[TContainer] = blocks_to_container\n\n    @staticmethod\n    def _validate_dtype(\n            dtype: np.dtype,\n            exclude: tp.Iterable[np.dtype] = EMPTY_TUPLE,\n            ) -> None:\n        if ((dtype.kind == DTYPE_DATETIME_KIND\n                or dtype == DTYPE_OBJECT)\n                and dtype not in exclude\n                ):\n            return\n        raise RuntimeError(f'invalid dtype ({dtype}) for date operation')\n\n    #---------------------------------------------------------------------------\n\n    @property\n    def year(self) -> TContainer:\n        'Return the year of each element.'\n\n        def blocks() -> tp.Iterator[np.ndarray]:\n            for block in self._blocks:\n                self._validate_dtype(block.dtype)\n\n                if block.dtype.kind == DTYPE_DATETIME_KIND:\n                    array = block.astype(DT64_YEAR).astype(DTYPE_INT_DEFAULT) + 1970\n                    array.flags.writeable = False\n                else: # must be object type\n                    array = array_from_element_attr(\n                            array=block,\n                            attr_name='year',\n                            dtype=DTYPE_INT_DEFAULT)\n                yield array\n\n        return self._blocks_to_container(blocks())\n\n    @property\n    def month(self) -> TContainer:\n        '''\n        Return the month of each element, between 1 and 12 inclusive.\n        '''\n\n        def blocks() -> tp.Iterator[np.ndarray]:\n            for block in self._blocks:\n                self._validate_dtype(block.dtype, exclude=self.DT64_EXCLUDE_YEAR)\n\n                if block.dtype.kind == DTYPE_DATETIME_KIND:\n                    array = block.astype(DT64_MONTH).astype(DTYPE_INT_DEFAULT) % 12 + 1\n                    array.flags.writeable = False\n                else: # must be object type\n                    array = array_from_element_attr(\n                            array=block,\n                            attr_name='month',\n                            dtype=DTYPE_INT_DEFAULT)\n                yield array\n\n        return self._blocks_to_container(blocks())\n\n    @property\n    def day(self) -> TContainer:\n        '''\n        Return the day of each element, between 1 and the number of days in the given month of the given year.\n        '''\n\n        def blocks() -> tp.Iterator[np.ndarray]:\n            for block in self._blocks:\n                self._validate_dtype(block.dtype, exclude=self.DT64_EXCLUDE_YEAR_MONTH)\n\n                if block.dtype.kind == DTYPE_DATETIME_KIND:\n                    if block.dtype != DT64_DAY:\n                        block = block.astype(DT64_DAY)\n                    # subtract the first of the month, then shfit\n                    array = (block - block.astype(DT64_MONTH)).astype(DTYPE_INT_DEFAULT) + 1\n                    array.flags.writeable = False\n                else: # must be object type\n                    array = array_from_element_attr(\n                            array=block,\n                            attr_name='day',\n                            dtype=DTYPE_INT_DEFAULT)\n\n                yield array\n\n        return self._blocks_to_container(blocks())\n\n\n    #---------------------------------------------------------------------------\n\n    # replace: akward to implement, as cannot provide None for the parameters that you do not want to set\n\n    def weekday(self) -> TContainer:\n        '''\n        Return the day of the week as an integer, where Monday is 0 and Sunday is 6.\n        '''\n\n        def blocks() -> tp.Iterator[np.ndarray]:\n            for block in self._blocks:\n                self._validate_dtype(block.dtype, exclude=self.DT64_EXCLUDE_YEAR_MONTH)\n\n                if block.dtype.kind == DTYPE_DATETIME_KIND:\n                    if block.dtype != DT64_DAY: # go to day first, then object\n                        block = block.astype(DT64_DAY)\n                    block = block.astype(DTYPE_OBJECT)\n                # all object arrays by this point\n\n                # returns an immutable array\n                array = array_from_element_method(\n                        array=block,\n                        method_name='weekday',\n                        args=EMPTY_TUPLE,\n                        dtype=DTYPE_INT_DEFAULT\n                        )\n                yield array\n\n        return self._blocks_to_container(blocks())\n\n\n    #---------------------------------------------------------------------------\n    # time methods\n\n    def timetuple(self) -> TContainer:\n        '''\n        Return a ``time.struct_time`` such as returned by time.localtime().\n        '''\n\n        def blocks() -> tp.Iterator[np.ndarray]:\n            for block in self._blocks:\n\n                # NOTE: nanosecond and lower will return integers; should exclud\n                self._validate_dtype(block.dtype,\n                        exclude=self.DT64_EXCLUDE_YEAR_MONTH_SUB_MICRO)\n\n                if block.dtype.kind == DTYPE_DATETIME_KIND:\n                    block = block.astype(DTYPE_OBJECT)\n                # all object arrays by this point\n\n                # returns an immutable array\n                array = array_from_element_method(\n                        array=block,\n                        method_name='timetuple',\n                        args=EMPTY_TUPLE,\n                        dtype=DTYPE_OBJECT\n                        )\n                yield array\n\n        return self._blocks_to_container(blocks())\n\n    def isoformat(self, sep: str = 'T', timespec: str = 'auto') -> TContainer:\n        '''\n        Return a string representing the date in ISO 8601 format, YYYY-MM-DD.\n        '''\n\n        def blocks() -> tp.Iterator[np.ndarray]:\n            for block in self._blocks:\n\n                self._validate_dtype(block.dtype,\n                        exclude=self.DT64_EXCLUDE_YEAR_MONTH_SUB_MICRO)\n\n                args = EMPTY_TUPLE\n                if block.dtype.kind == DTYPE_DATETIME_KIND:\n                    if block.dtype in self.DT64_TIME:\n                        # if we know this is a time type, we can pass args\n                        args = (sep, timespec) #type: ignore\n                    block = block.astype(DTYPE_OBJECT)\n\n                # all object arrays by this point\n                # NOTE: we cannot determine if an Object array has date or datetime objects with a full iteration, so we cannot be sure if we need to pass args or not.\n\n                # returns an immutable array\n                array = array_from_element_method(\n                        array=block,\n                        method_name='isoformat',\n                        args=args,\n                        dtype=DTYPE_STR,\n                        )\n                yield array\n\n        return self._blocks_to_container(blocks())\n\n    def strftime(self, format: str) -> TContainer:\n        '''\n        Return a string representing the date, controlled by an explicit format string.\n        '''\n\n        def blocks() -> tp.Iterator[np.ndarray]:\n            for block in self._blocks:\n\n                # NOTE: nanosecond and lower will return integers; should exclud\n                self._validate_dtype(block.dtype,\n                        exclude=self.DT64_EXCLUDE_YEAR_MONTH_SUB_MICRO)\n\n                if block.dtype.kind == DTYPE_DATETIME_KIND:\n                    block = block.astype(DTYPE_OBJECT)\n                # all object arrays by this point\n\n                # returns an immutable array\n                array = array_from_element_method(\n                        array=block,\n                        method_name='strftime',\n                        args=(format,),\n                        dtype=DTYPE_STR,\n                        )\n                yield array\n\n        return self._blocks_to_container(blocks())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
static_frame/core/node_iter.py,3,"b""'''\nTools for iterators in Series and Frame. These components are imported by both series.py and frame.py; these components also need to be able to return Series and Frame, and thus use deferred, function-based imports.\n'''\n\nimport typing as tp\nfrom enum import Enum\nfrom functools import partial\n\nfrom concurrent.futures import ProcessPoolExecutor\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport numpy as np\n\n# from static_frame.core.util import CallableOrMapping\nfrom static_frame.core.util import AnyCallable\nfrom static_frame.core.util import DtypeSpecifier\nfrom static_frame.core.util import Mapping\nfrom static_frame.core.util import NameType\nfrom static_frame.core.doc_str import doc_inject\nfrom static_frame.core.util import DepthLevelSpecifier\nfrom static_frame.core.util import KEY_ITERABLE_TYPES\n\n\nif tp.TYPE_CHECKING:\n    from static_frame.core.frame import Frame # pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.series import Series # pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.index import Index # pylint: disable=W0611 #pragma: no cover\n\n\nFrameOrSeries = tp.TypeVar('FrameOrSeries', 'Frame', 'Series')\n# FrameSeriesIndex = tp.TypeVar('FrameSeriesIndex', 'Frame', 'Series', 'Index')\n\n\nclass IterNodeApplyType(Enum):\n    SERIES_ITEMS = 1\n    SERIES_ITEMS_FLAT = 2 # do not use index class on container\n    FRAME_ELEMENTS = 3\n    INDEX_LABELS = 4\n\n\nclass IterNodeType(Enum):\n    VALUES = 1\n    ITEMS = 2\n\n\nclass IterNodeDelegate(tp.Generic[FrameOrSeries]):\n    '''\n    Delegate returned from :obj:`static_frame.IterNode`, providing iteration as well as a family of apply methods.\n    '''\n\n    __slots__ = (\n            '_func_values',\n            '_func_items',\n            '_yield_type',\n            '_apply_constructor'\n            )\n\n    INTERFACE = (\n            'apply',\n            'apply_iter',\n            'apply_iter_items',\n            'apply_pool',\n            'map_all',\n            'map_all_iter',\n            'map_all_iter_items',\n            'map_any',\n            'map_any_iter',\n            'map_any_iter_items',\n            'map_fill',\n            'map_fill_iter',\n            'map_fill_iter_items',\n            )\n\n    def __init__(self,\n            func_values: tp.Callable[..., tp.Iterable[tp.Any]],\n            func_items: tp.Callable[..., tp.Iterable[tp.Tuple[tp.Any, tp.Any]]],\n            yield_type: IterNodeType,\n            apply_constructor: tp.Callable[..., FrameOrSeries]\n        ) -> None:\n        '''\n        Args:\n            apply_constructor: Callable (generally a class) used to construct the object returned from apply(); must take an iterator of items.\n        '''\n        self._func_values = func_values\n        self._func_items = func_items\n        self._yield_type = yield_type\n        self._apply_constructor: tp.Callable[..., FrameOrSeries] = apply_constructor\n\n    #---------------------------------------------------------------------------\n\n    def _apply_iter_items_parallel(self,\n            func: AnyCallable,\n            max_workers: tp.Optional[int] = None,\n            chunksize: int = 1,\n            use_threads: bool = False,\n            ) -> tp.Iterator[tp.Tuple[tp.Any, tp.Any]]:\n\n        pool_executor = ThreadPoolExecutor if use_threads else ProcessPoolExecutor\n\n        yt_is_values = self._yield_type is IterNodeType.VALUES\n\n        if not callable(func):\n            func = getattr(func, '__getitem__')\n\n        # use side effect list population to create keys when iterating over values\n        func_keys = []\n\n        arg_gen: tp.Callable[[], tp.Union[tp.Iterator[tp.Any], tp.Iterator[tp.Tuple[tp.Any, tp.Any]]]]\n\n        if yt_is_values:\n            def arg_gen() -> tp.Iterator[tp.Any]: #pylint: disable=E0102\n                for k, v in self._func_items():\n                    func_keys.append(k)\n                    yield v\n        else:\n            def arg_gen() -> tp.Iterator[tp.Tuple[tp.Any, tp.Any]]: #pylint: disable=E0102\n                for k, v in self._func_items():\n                    func_keys.append(k)\n                    yield k, v\n\n        with pool_executor(max_workers=max_workers) as executor:\n            yield from zip(func_keys,\n                    executor.map(func, arg_gen(), chunksize=chunksize)\n                    )\n\n    #---------------------------------------------------------------------------\n    # public interface\n\n    @doc_inject(selector='map_any')\n    def map_any_iter_items(self,\n            mapping: Mapping\n            ) -> tp.Iterator[tp.Tuple[tp.Any, tp.Any]]:\n        '''\n        {doc} A generator of resulting key, value pairs.\n\n        Args:\n            {mapping}\n        '''\n        get = getattr(mapping, 'get')\n        if self._yield_type is IterNodeType.VALUES:\n            yield from ((k, get(v, v)) for k, v in self._func_items())\n        else:\n            yield from ((k, get((k,  v), v)) for k, v in self._func_items())\n\n    @doc_inject(selector='map_any')\n    def map_any_iter(self,\n            mapping: Mapping,\n            ) -> tp.Iterator[tp.Any]:\n        '''\n        {doc} A generator of resulting values.\n\n        Args:\n            {mapping}\n        '''\n        yield from (v for _, v in self.map_any_iter_items(mapping))\n\n    @doc_inject(selector='map_any')\n    def map_any(self,\n            mapping: Mapping,\n            *,\n            dtype: DtypeSpecifier = None,\n            name: NameType = None,\n            ) -> FrameOrSeries:\n        '''\n        {doc} Returns a new container.\n\n        Args:\n            {mapping}\n            {dtype}\n        '''\n        return self._apply_constructor(\n                self.map_any_iter_items(mapping),\n                dtype=dtype,\n                name=name,\n                )\n\n\n    #---------------------------------------------------------------------------\n    @doc_inject(selector='map_fill')\n    def map_fill_iter_items(self,\n            mapping: Mapping,\n            *,\n            fill_value: tp.Any = np.nan,\n            ) -> tp.Iterator[tp.Tuple[tp.Any, tp.Any]]:\n        '''\n        {doc} A generator of resulting key, value pairs.\n\n        Args:\n            {mapping}\n            {fill_value}\n        '''\n        get = getattr(mapping, 'get')\n        if self._yield_type is IterNodeType.VALUES:\n            yield from ((k, get(v, fill_value)) for k, v in self._func_items())\n        else:\n            yield from ((k, get((k,  v), fill_value)) for k, v in self._func_items())\n\n    @doc_inject(selector='map_fill')\n    def map_fill_iter(self,\n            mapping: Mapping,\n            *,\n            fill_value: tp.Any = np.nan,\n            ) -> tp.Iterator[tp.Any]:\n        '''\n        {doc} A generator of resulting values.\n\n        Args:\n            {mapping}\n            {fill_value}\n        '''\n        yield from (v for _, v in self.map_fill_iter_items(mapping, fill_value=fill_value))\n\n    @doc_inject(selector='map_fill')\n    def map_fill(self,\n            mapping: Mapping,\n            *,\n            fill_value: tp.Any = np.nan,\n            dtype: DtypeSpecifier = None,\n            name: NameType = None,\n            ) -> FrameOrSeries:\n        '''\n        {doc} Returns a new container.\n\n        Args:\n            {mapping}\n            {fill_value}\n            {dtype}\n        '''\n        return self._apply_constructor(\n                self.map_fill_iter_items(mapping, fill_value=fill_value),\n                dtype=dtype,\n                name=name,\n                )\n\n    #---------------------------------------------------------------------------\n    @doc_inject(selector='map_all')\n    def map_all_iter_items(self,\n            mapping: Mapping\n            ) -> tp.Iterator[tp.Tuple[tp.Any, tp.Any]]:\n        '''\n        {doc} A generator of resulting key, value pairs.\n\n        Args:\n            {mapping}\n        '''\n        # want exception to raise if key not found\n        func = getattr(mapping, '__getitem__')\n        if self._yield_type is IterNodeType.VALUES:\n            yield from ((k, func(v)) for k, v in self._func_items())\n        else:\n            yield from ((k, func((k,  v))) for k, v in self._func_items())\n\n    @doc_inject(selector='map_all')\n    def map_all_iter(self,\n            mapping: Mapping\n            ) -> tp.Iterator[tp.Any]:\n        '''\n        {doc} A generator of resulting values.\n\n        Args:\n            {mapping}\n        '''\n        yield from (v for _, v in self.map_all_iter_items(mapping=mapping))\n\n    @doc_inject(selector='map_all')\n    def map_all(self,\n            mapping: Mapping,\n            *,\n            dtype: DtypeSpecifier = None,\n            name: NameType = None,\n            ) -> FrameOrSeries:\n        '''\n        {doc} Returns a new container.\n\n        Args:\n            {mapping}\n            {dtype}\n        '''\n        return self._apply_constructor(\n                self.map_all_iter_items(mapping),\n                dtype=dtype,\n                name=name,\n                )\n\n\n    #---------------------------------------------------------------------------\n    @doc_inject(selector='apply')\n    def apply_iter_items(self,\n            func: AnyCallable) -> tp.Iterator[tp.Tuple[tp.Any, tp.Any]]:\n        '''\n        {doc} A generator of resulting key, value pairs.\n\n        Args:\n            {func}\n        '''\n        # depend on yield type, we determine what the passed in function expects to take\n        if self._yield_type is IterNodeType.VALUES:\n            yield from ((k, func(v)) for k, v in self._func_items())\n        else:\n            yield from ((k, func(k, v)) for k, v in self._func_items())\n\n    @doc_inject(selector='apply')\n    def apply_iter(self,\n            func: AnyCallable\n            ) -> tp.Iterator[tp.Any]:\n        '''\n        {doc} A generator of resulting values.\n\n        Args:\n            {func}\n        '''\n        yield from (v for _, v in self.apply_iter_items(func=func))\n\n    @doc_inject(selector='apply')\n    def apply(self,\n            func: AnyCallable,\n            *,\n            dtype: DtypeSpecifier = None,\n            name: NameType = None,\n            ) -> FrameOrSeries:\n        '''\n        {doc} Returns a new container.\n\n        Args:\n            {func}\n            {dtype}\n        '''\n        if not callable(func):\n            raise RuntimeError('use map_fill(), map_any(), or map_all() for applying a mapping type')\n\n        return self._apply_constructor(\n                self.apply_iter_items(func),\n                dtype=dtype,\n                name=name,\n                )\n\n    @doc_inject(selector='apply')\n    def apply_pool(self,\n            func: AnyCallable,\n            *,\n            dtype: DtypeSpecifier = None,\n            name: NameType = None,\n            max_workers: tp.Optional[int] = None,\n            chunksize: int = 1,\n            use_threads: bool = False\n            ) -> FrameOrSeries:\n        '''\n        {doc} Employ parallel processing with either the ProcessPoolExecutor or ThreadPoolExecutor.\n\n        Args:\n            {func}\n            {dtype}\n            max_workers: Passed to the pool_executor, where None defaults to the max number of machine processes.\n            chunksize: Passed to the pool executor.\n            use_thread: When True, the ThreadPoolExecutor will be used rather than the default ProcessPoolExecutor.\n        '''\n        return self._apply_constructor(\n                self._apply_iter_items_parallel(\n                        func=func,\n                        max_workers=max_workers,\n                        chunksize=chunksize,\n                        use_threads=use_threads),\n                dtype=dtype,\n                name=name,\n                )\n\n    def __iter__(self) -> tp.Union[\n            tp.Iterator[tp.Any],\n            tp.Iterator[tp.Tuple[tp.Any, tp.Any]]\n            ]:\n        '''\n        Return a generator based on the yield type.\n        '''\n        if self._yield_type is IterNodeType.VALUES:\n            yield from self._func_values()\n        else:\n            yield from self._func_items()\n\n\n\n#-------------------------------------------------------------------------------\n\n_ITER_NODE_SLOTS = (\n        '_container',\n        '_func_values',\n        '_func_items',\n        '_yield_type',\n        '_apply_type'\n        )\n\nclass IterNode(tp.Generic[FrameOrSeries]):\n    '''Interface to a type of iteration on :obj:`static_frame.Series` and :obj:`static_frame.Frame`.\n    '''\n    # Stores two version of a generator function: one to yield single values, another to yield items pairs. The latter is needed in all cases, as when we use apply we return a Series, and need to have recourse to an index.\n\n    __slots__ = _ITER_NODE_SLOTS\n\n    def __init__(self, *,\n            container: FrameOrSeries,\n            function_values: tp.Callable[..., tp.Iterable[tp.Any]],\n            function_items: tp.Callable[..., tp.Iterable[tp.Tuple[tp.Any, tp.Any]]],\n            yield_type: IterNodeType,\n            apply_type: IterNodeApplyType = IterNodeApplyType.SERIES_ITEMS\n            ) -> None:\n        '''\n        Args:\n            function_values: will be partialed with arguments given with __call__.\n            function_items: will be partialed with arguments given with __call__.\n        '''\n        self._container: FrameOrSeries = container\n        self._func_values = function_values\n        self._func_items = function_items\n        self._yield_type = yield_type\n        self._apply_type = apply_type\n\n    def get_delegate(self,\n            *args: object,\n            **kwargs: object\n            ) -> IterNodeDelegate[FrameOrSeries]:\n        '''\n        In usage as an iteator, the args passed here are expected to be argument for the core iterators, i.e., axis arguments.\n        '''\n        from static_frame.core.series import Series\n        from static_frame.core.frame import Frame\n\n        assert not args # force all kwarg\n\n        func_values = partial(self._func_values, **kwargs)\n        func_items = partial(self._func_items, **kwargs)\n\n        apply_constructor: tp.Callable[..., tp.Union[Frame, Series]]\n\n        if self._apply_type is IterNodeApplyType.SERIES_ITEMS:\n            if isinstance(self._container, Frame) and kwargs['axis'] == 0:\n                index_constructor = self._container._columns.from_labels\n            else:\n                index_constructor = self._container._index.from_labels\n            # always return a Series\n            apply_constructor = partial(\n                    Series.from_items,\n                    index_constructor=index_constructor\n                    )\n        elif self._apply_type is IterNodeApplyType.SERIES_ITEMS_FLAT:\n            # use default index constructor\n            apply_constructor = Series.from_items\n\n        elif self._apply_type is IterNodeApplyType.FRAME_ELEMENTS:\n            assert isinstance(self._container, Frame) # for typing\n            apply_constructor = partial(\n                    self._container.__class__.from_element_loc_items,\n                    index=self._container._index,\n                    columns=self._container._columns,\n                    index_constructor=self._container._index.from_labels,\n                    columns_constructor=self._container._columns.from_labels\n                    )\n        elif self._apply_type is IterNodeApplyType.INDEX_LABELS:\n            # when this is used with hierarchical indices, we are likely to not get a unique values; thus, passing this to an Index constructor is awkward. instead, simply create a Series\n            apply_constructor = Series.from_items\n        else:\n            raise NotImplementedError() #pragma: no cover\n\n        return IterNodeDelegate(\n                func_values=func_values,\n                func_items=func_items,\n                yield_type=self._yield_type,\n                apply_constructor=tp.cast(tp.Callable[..., FrameOrSeries], apply_constructor)\n                )\n\n\n#-------------------------------------------------------------------------------\n# specialize IterNode based on arguments given to __call__\n\nclass IterNodeNoArg(IterNode[FrameOrSeries]):\n\n    __slots__ = _ITER_NODE_SLOTS\n\n    def __call__(self,\n            ) -> IterNodeDelegate[FrameOrSeries]:\n        return IterNode.get_delegate(self)\n\n\nclass IterNodeAxis(IterNode[FrameOrSeries]):\n\n    __slots__ = _ITER_NODE_SLOTS\n\n    def __call__(self,\n            axis: int = 0\n            ) -> IterNodeDelegate[FrameOrSeries]:\n        return IterNode.get_delegate(self, axis=axis)\n\n\nclass IterNodeGroup(IterNode[FrameOrSeries]):\n    '''\n    Iterator on 1D groupings where no args are required (but axis is retained for compatibility)\n    '''\n\n    __slots__ = _ITER_NODE_SLOTS\n\n    def __call__(self,\n            *,\n            axis: int = 0\n            ) -> IterNodeDelegate[FrameOrSeries]:\n        return IterNode.get_delegate(self, axis=axis)\n\nclass IterNodeGroupAxis(IterNode[FrameOrSeries]):\n    '''\n    Iterator on 2D groupings where key and axis are required.\n    '''\n\n    __slots__ = _ITER_NODE_SLOTS\n\n    def __call__(self,\n            key: KEY_ITERABLE_TYPES, # type: ignore\n            *,\n            axis: int = 0\n            ) -> IterNodeDelegate[FrameOrSeries]:\n        return IterNode.get_delegate(self, key=key, axis=axis)\n\n\nclass IterNodeDepthLevel(IterNode[FrameOrSeries]):\n\n    __slots__ = _ITER_NODE_SLOTS\n\n    def __call__(self,\n            depth_level: DepthLevelSpecifier = 0\n            ) -> IterNodeDelegate[FrameOrSeries]:\n        return IterNode.get_delegate(self, depth_level=depth_level)\n\n\nclass IterNodeDepthLevelAxis(IterNode[FrameOrSeries]):\n\n    __slots__ = _ITER_NODE_SLOTS\n\n    def __call__(self,\n            depth_level: DepthLevelSpecifier = 0,\n            *,\n            axis: int = 0\n            ) -> IterNodeDelegate[FrameOrSeries]:\n        return IterNode.get_delegate(self, depth_level=depth_level, axis=axis)\n\n\nclass IterNodeWindow(IterNode[FrameOrSeries]):\n\n    __slots__ = _ITER_NODE_SLOTS\n\n    def __call__(self, *,\n            size: int,\n            axis: int = 0,\n            step: int = 1,\n            window_sized: bool = True,\n            window_func: tp.Optional[AnyCallable] = None,\n            window_valid: tp.Optional[AnyCallable] = None,\n            label_shift: int = 0,\n            start_shift: int = 0,\n            size_increment: int = 0,\n            ) -> IterNodeDelegate[FrameOrSeries]:\n        return IterNode.get_delegate(self,\n                axis=axis,\n                size=size,\n                step=step,\n                window_sized=window_sized,\n                window_func=window_func,\n                window_valid=window_valid,\n                label_shift=label_shift,\n                start_shift=start_shift,\n                size_increment=size_increment,\n                )\n\n"""
static_frame/core/node_selector.py,1,"b""import typing as tp\n\nimport numpy as np\n\n\nfrom static_frame.core.util import GetItemKeyType\nfrom static_frame.core.util import NULL_SLICE\nfrom static_frame.core.util import EMPTY_TUPLE\nfrom static_frame.core.doc_str import doc_inject\nfrom static_frame.core.assign import Assign\n\n\nif tp.TYPE_CHECKING:\n\n    from static_frame.core.bus import Bus  #pylint: disable = W0611 #pragma: no cover\n    from static_frame.core.frame import Frame  #pylint: disable = W0611 #pragma: no cover\n    from static_frame.core.frame import FrameAsType  #pylint: disable = W0611 #pragma: no cover\n    from static_frame.core.index import Index  #pylint: disable = W0611 #pragma: no cover\n    from static_frame.core.series import Series  #pylint: disable = W0611 #pragma: no cover\n    from static_frame.core.type_blocks import TypeBlocks  #pylint: disable = W0611 #pragma: no cover\n\n#-------------------------------------------------------------------------------\n\nTContainer = tp.TypeVar('TContainer', 'Index', 'Series', 'Frame', 'TypeBlocks', 'Bus')\nGetItemFunc = tp.TypeVar('GetItemFunc', bound=tp.Callable[[GetItemKeyType], TContainer])\n\n\nclass Interface(tp.Generic[TContainer]):\n    __slots__ = EMPTY_TUPLE\n    INTERFACE: tp.Tuple[str, ...] = EMPTY_TUPLE\n\n\nclass InterfaceGetItem(Interface[TContainer]):\n\n    __slots__ = ('_func',)\n    INTERFACE = ('__getitem__',)\n\n    def __init__(self, func: tp.Callable[[GetItemKeyType], TContainer]) -> None:\n        self._func: tp.Callable[[GetItemKeyType], TContainer] = func\n\n    def __getitem__(self, key: GetItemKeyType) -> TContainer:\n        return self._func(key)\n\n#-------------------------------------------------------------------------------\n\nclass InterfaceSelectDuo(Interface[TContainer]):\n    '''An instance to serve as an interface to all of iloc and loc\n    '''\n\n    __slots__ = (\n            '_func_iloc',\n            '_func_loc',\n            )\n    INTERFACE = ('iloc', 'loc')\n\n    def __init__(self, *,\n            func_iloc: GetItemFunc,\n            func_loc: GetItemFunc) -> None:\n\n        self._func_iloc = func_iloc\n        self._func_loc = func_loc\n\n    @property\n    def iloc(self) -> InterfaceGetItem[TContainer]:\n        return InterfaceGetItem(self._func_iloc)\n\n    @property\n    def loc(self) -> InterfaceGetItem[TContainer]:\n        return InterfaceGetItem(self._func_loc)\n\nclass InterfaceSelectTrio(Interface[TContainer]):\n    '''An instance to serve as an interface to all of iloc, loc, and __getitem__ extractors.\n    '''\n\n    __slots__ = (\n            '_func_iloc',\n            '_func_loc',\n            '_func_getitem',\n            )\n    INTERFACE = ('__getitem__', 'iloc', 'loc')\n\n    def __init__(self, *,\n            func_iloc: GetItemFunc,\n            func_loc: GetItemFunc,\n            func_getitem: GetItemFunc,\n            ) -> None:\n\n        self._func_iloc = func_iloc\n        self._func_loc = func_loc\n        self._func_getitem = func_getitem\n\n    def __getitem__(self, key: GetItemKeyType) -> tp.Any:\n        '''Label-based selection.\n        '''\n        return self._func_getitem(key)\n\n    @property\n    def iloc(self) -> InterfaceGetItem[TContainer]:\n        '''Integer-position based selection.'''\n        return InterfaceGetItem(self._func_iloc)\n\n    @property\n    def loc(self) -> InterfaceGetItem[TContainer]:\n        '''Label-based selection.\n        '''\n        return InterfaceGetItem(self._func_loc)\n\n\nclass InterfaceSelectQuartet(Interface[TContainer]):\n    '''An instance to serve as an interface to all of iloc, loc, and __getitem__ extractors.\n    '''\n\n    __slots__ = (\n            '_func_iloc',\n            '_func_loc',\n            '_func_getitem',\n            '_func_bloc',\n            )\n    INTERFACE = ('__getitem__', 'iloc', 'loc', 'bloc')\n\n    def __init__(self, *,\n            func_iloc: GetItemFunc,\n            func_loc: GetItemFunc,\n            func_getitem: GetItemFunc,\n            func_bloc: tp.Any, # not sure what is the right type\n            ) -> None:\n\n        self._func_iloc = func_iloc\n        self._func_loc = func_loc\n        self._func_getitem = func_getitem\n        self._func_bloc = func_bloc\n\n    def __getitem__(self, key: GetItemKeyType) -> tp.Any:\n        '''Label-based selection.\n        '''\n        return self._func_getitem(key)\n\n    @property\n    def bloc(self) -> InterfaceGetItem[TContainer]:\n        '''Boolean based assignment.'''\n        return InterfaceGetItem(self._func_bloc)\n\n    @property\n    def iloc(self) -> InterfaceGetItem[TContainer]:\n        '''Integer-position based assignment.'''\n        return InterfaceGetItem(self._func_iloc)\n\n    @property\n    def loc(self) -> InterfaceGetItem[TContainer]:\n        '''Label-based assignment.\n        '''\n        return InterfaceGetItem(self._func_loc)\n\n\n#-------------------------------------------------------------------------------\n\nclass InterfaceAssignTrio(InterfaceSelectTrio[TContainer]):\n    '''For assignment with __getitem__, iloc, loc.\n    '''\n\n    __slots__ = (\n            '_func_iloc',\n            '_func_loc',\n            '_func_getitem',\n            'delegate'\n            )\n\n    def __init__(self, *,\n            func_iloc: GetItemFunc,\n            func_loc: GetItemFunc,\n            func_getitem: GetItemFunc,\n            delegate: tp.Type[Assign]\n            ) -> None:\n\n        self._func_iloc = func_iloc #pylint: disable=E0237\n        self._func_loc = func_loc #pylint: disable=E0237\n        self._func_getitem = func_getitem #pylint: disable=E0237\n\n        self.delegate = delegate #pylint: disable=E0237\n\n\nclass InterfaceAssignQuartet(InterfaceSelectQuartet[TContainer]):\n    '''For assignment with __getitem__, iloc, loc, bloc.\n    '''\n    __slots__ = (\n            '_func_iloc',\n            '_func_loc',\n            '_func_getitem',\n            '_func_bloc',\n            'delegate'\n            )\n\n    def __init__(self, *,\n            func_iloc: GetItemFunc,\n            func_loc: GetItemFunc,\n            func_getitem: GetItemFunc,\n            func_bloc: tp.Any, # not sure what is the right type\n            delegate: tp.Type[Assign]\n            ) -> None:\n\n        self._func_iloc = func_iloc #pylint: disable=E0237\n        self._func_loc = func_loc #pylint: disable=E0237\n        self._func_getitem = func_getitem #pylint: disable=E0237\n        self._func_bloc = func_bloc #pylint: disable=E0237\n\n        self.delegate = delegate #pylint: disable=E0237\n\n\n#-------------------------------------------------------------------------------\n\nclass InterfaceAsType(Interface[TContainer]):\n    '''An instance to serve as an interface to __getitem__ extractors. Used by both :obj:`Frame` and :obj:`IndexHierarchy`.\n    '''\n\n    __slots__ = ('_func_getitem',)\n    INTERFACE = ('__getitem__', '__call__')\n\n    def __init__(self,\n            func_getitem: tp.Callable[[GetItemKeyType], 'FrameAsType']\n            ) -> None:\n        '''\n        Args:\n            _func_getitem: a callable that expects a _func_getitem key and returns a FrameAsType interface; for example, Frame._extract_getitem_astype.\n        '''\n        self._func_getitem = func_getitem\n\n    @doc_inject(selector='selector')\n    def __getitem__(self, key: GetItemKeyType) -> 'FrameAsType':\n        '''Selector of columns by label.\n\n        Args:\n            key: {key_loc}\n        '''\n        return self._func_getitem(key)\n\n    def __call__(self, dtype: np.dtype) -> 'Frame':\n        '''\n        Apply a single ``dtype`` to all columns.\n        '''\n        return self._func_getitem(NULL_SLICE)(dtype)\n"""
static_frame/core/node_str.py,5,"b""\nimport typing as tp\nimport numpy as np\nfrom numpy import char as npc\n\nfrom static_frame.core.util import EMPTY_TUPLE\nfrom static_frame.core.util import DTYPE_STR_KIND\nfrom static_frame.core.util import DTYPE_STR\nfrom static_frame.core.util import UFunc\n\nfrom static_frame.core.util import array_from_element_method\n\nfrom static_frame.core.node_selector import Interface\nfrom static_frame.core.node_selector import TContainer\n\nif tp.TYPE_CHECKING:\n    from static_frame.core.frame import Frame  #pylint: disable = W0611 #pragma: no cover\n    from static_frame.core.index import Index  #pylint: disable = W0611 #pragma: no cover\n    from static_frame.core.index_hierarchy import IndexHierarchy  #pylint: disable = W0611 #pragma: no cover\n    from static_frame.core.series import Series  #pylint: disable = W0611 #pragma: no cover\n    from static_frame.core.type_blocks import TypeBlocks  #pylint: disable = W0611 #pragma: no cover\n\n# only ContainerOperand subclasses\n# TContainer = tp.TypeVar('TContainer',\n#         'Index',\n#         'IndexHierarchy',\n#         'Series',\n#         'Frame',\n#         'TypeBlocks'\n#         )\n\nBlocksType = tp.Iterable[np.ndarray]\nToContainerType = tp.Callable[[tp.Iterator[np.ndarray]], TContainer]\n\n\nclass InterfaceString(Interface[TContainer]):\n\n    # NOTE: based on https://numpy.org/doc/stable/reference/routines.char.html\n\n    __slots__ = (\n            '_blocks', # function that returns array of strings\n            '_blocks_to_container', # partialed function that will return a new container\n            )\n    INTERFACE = (\n            'capitalize',\n            'center',\n            'count',\n            'decode',\n            'encode',\n            'endswith',\n            'find',\n            'index',\n            'isalnum',\n            'isalpha',\n            'isdecimal',\n            'isdigit',\n            'islower',\n            'isnumeric',\n            'isspace',\n            'istitle',\n            'isupper',\n            'ljust',\n            'lower',\n            'lstrip',\n            'partition',\n            'replace',\n            'rfind',\n            'rindex',\n            'rjust',\n            'rpartition',\n            'rsplit',\n            'rstrip',\n            'split',\n            'startswith',\n            'strip',\n            'swapcase',\n            'title',\n            'upper',\n            'zfill',\n    )\n\n\n    def __init__(self,\n            blocks: BlocksType,\n            blocks_to_container: ToContainerType[TContainer]\n            ) -> None:\n        self._blocks: BlocksType = blocks\n        self._blocks_to_container: ToContainerType[TContainer] = blocks_to_container\n\n    #---------------------------------------------------------------------------\n\n    @staticmethod\n    def _process_blocks(\n            blocks: BlocksType,\n            func: UFunc,\n            args: tp.Tuple[tp.Any, ...] = EMPTY_TUPLE,\n            astype_str: bool = True,\n            ) -> tp.Iterator[np.ndarray]:\n        '''\n        Block-wise processing of blocks after optional string conversion. Non-string conversion is necessary for ``decode``.\n        '''\n        for block in blocks:\n            if astype_str and block.dtype not in DTYPE_STR_KIND:\n                block = block.astype(DTYPE_STR)\n            array = func(block, *args)\n            array.flags.writeable = False\n            yield array\n\n    @staticmethod\n    def _process_tuple_blocks(*,\n            blocks: BlocksType,\n            method_name: str,\n            dtype: np.dtype,\n            args: tp.Tuple[tp.Any, ...] = EMPTY_TUPLE,\n            ) -> tp.Iterator[np.ndarray]:\n        '''\n        Element-wise processing of a methods on objects in a block, with pre-insert conversion to a tuple.\n        '''\n        for block in blocks:\n            if block.dtype not in DTYPE_STR_KIND:\n                block = block.astype(DTYPE_STR)\n\n            # resultant array is immutable\n            array = array_from_element_method(\n                    array=block,\n                    method_name=method_name,\n                    args=args,\n                    dtype=dtype,\n                    pre_insert=tuple,\n                    )\n            yield array\n\n    #---------------------------------------------------------------------------\n    def capitalize(self) -> TContainer:\n        '''\n        Return a container with only the first character of each element capitalized.\n        '''\n        # return self._blocks_to_container(npc.capitalize(self._blocks()))\n        block_gen = self._process_blocks(self._blocks, npc.capitalize)\n        return self._blocks_to_container(block_gen)\n\n    def center(self,\n            width: int,\n            fillchar: str = ' '\n            ) -> TContainer:\n        '''\n        Return a container with its elements centered in a string of length ``width``.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.center, (width, fillchar))\n        return self._blocks_to_container(block_gen)\n\n    def count(self,\n            sub: str,\n            start: tp.Optional[int] = None,\n            end: tp.Optional[int] = None\n            ) -> TContainer:\n        '''\n        Returns a container with the number of non-overlapping occurrences of substring sub in the optional range ``start``, ``end``.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.count, (sub, start, end))\n        return self._blocks_to_container(block_gen)\n\n    def decode(self,\n            encoding: tp.Optional[str] = None,\n            errors: tp.Optional[str] = None,\n            ) -> TContainer:\n        '''\n        Apply str.decode() to each element. Elements must be bytes.\n        '''\n        block_gen = self._process_blocks(\n                blocks=self._blocks,\n                func=npc.decode,\n                args=(encoding, errors),\n                astype_str=False, # needs to be bytes\n                )\n        return self._blocks_to_container(block_gen)\n\n    def encode(self,\n            encoding: tp.Optional[str] = None,\n            errors: tp.Optional[str] = None,\n            ) -> TContainer:\n        '''\n        Apply str.encode() to each element. Elements must be strings.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.encode, (encoding, errors))\n        return self._blocks_to_container(block_gen)\n\n    def endswith(self,\n            suffix: str,\n            start: tp.Optional[int] = None,\n            end: tp.Optional[int] = None\n            ) -> TContainer:\n        '''\n        Returns a container with the number of non-overlapping occurrences of substring sub in the optional range ``start``, ``end``.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.endswith, (suffix, start, end))\n        return self._blocks_to_container(block_gen)\n\n    def find(self,\n            sub: str,\n            start: tp.Optional[int] = None,\n            end: tp.Optional[int] = None\n            ) -> TContainer:\n        '''\n        For each element, return the lowest index in the string where substring ``sub`` is found.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.find, (sub, start, end))\n        return self._blocks_to_container(block_gen)\n\n    def index(self,\n            sub: str,\n            start: tp.Optional[int] = None,\n            end: tp.Optional[int] = None\n            ) -> TContainer:\n        '''\n        Like ``find``, but raises ``ValueError`` when the substring is not found.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.index, (sub, start, end))\n        return self._blocks_to_container(block_gen)\n\n    def isalnum(self) -> TContainer:\n        '''\n        Returns true for each element if all characters in the string are alphanumeric and there is at least one character, false otherwise.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.isalnum)\n        return self._blocks_to_container(block_gen)\n\n    def isalpha(self) -> TContainer:\n        '''\n        Returns true for each element if all characters in the string are alphabetic and there is at least one character, false otherwise.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.isalpha)\n        return self._blocks_to_container(block_gen)\n\n    def isdecimal(self) -> TContainer:\n        '''\n        For each element, return True if there are only decimal characters in the element.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.isdecimal)\n        return self._blocks_to_container(block_gen)\n\n    def isdigit(self) -> TContainer:\n        '''\n        Returns true for each element if all characters in the string are digits and there is at least one character, false otherwise.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.isdigit)\n        return self._blocks_to_container(block_gen)\n\n    def islower(self) -> TContainer:\n        '''\n        Returns true for each element if all cased characters in the string are lowercase and there is at least one cased character, false otherwise.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.islower)\n        return self._blocks_to_container(block_gen)\n\n    def isnumeric(self) -> TContainer:\n        '''\n        For each element in self, return True if there are only numeric characters in the element.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.isnumeric)\n        return self._blocks_to_container(block_gen)\n\n    def isspace(self) -> TContainer:\n        '''\n        Returns true for each element if there are only whitespace characters in the string and there is at least one character, false otherwise.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.isspace)\n        return self._blocks_to_container(block_gen)\n\n    def istitle(self) -> TContainer:\n        '''\n        Returns true for each element if the element is a titlecased string and there is at least one character, false otherwise.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.istitle)\n        return self._blocks_to_container(block_gen)\n\n    def isupper(self) -> TContainer:\n        '''\n        Returns true for each element if all cased characters in the string are uppercase and there is at least one character, false otherwise.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.isupper)\n        return self._blocks_to_container(block_gen)\n\n    def ljust(self,\n            width: int,\n            fillchar: str = ' '\n            ) -> TContainer:\n        '''\n        Return a container with its elements ljusted in a string of length ``width``.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.ljust, (width, fillchar))\n        return self._blocks_to_container(block_gen)\n\n    def lower(self) -> TContainer:\n        '''\n        Return an array with the elements of self converted to lowercase.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.lower)\n        return self._blocks_to_container(block_gen)\n\n    def lstrip(self,\n            chars: tp.Optional[str] = None,\n            ) -> TContainer:\n        '''\n        For each element, return a copy with the leading characters removed.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.lstrip, (chars,))\n        return self._blocks_to_container(block_gen)\n\n    def partition(self,\n            sep: str,\n            ) -> TContainer:\n        '''\n        Partition each element around ``sep``.\n        '''\n        # NOTE: py str.partition gives a tuple.\n        block_gen = self._process_tuple_blocks(\n                blocks=self._blocks,\n                method_name='partition',\n                args=(sep,),\n                dtype=object\n                )\n        return self._blocks_to_container(block_gen)\n\n    def replace(self,\n            old: str,\n            new: str,\n            count: tp.Optional[int] = None,\n            ) -> TContainer:\n        '''\n        Return a container with its elements replaced in a string of length ``width``.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.replace, (old, new, count))\n        return self._blocks_to_container(block_gen)\n\n    def rfind(self,\n            sub: str,\n            start: tp.Optional[int] = None,\n            end: tp.Optional[int] = None\n            ) -> TContainer:\n        '''\n        For each element, return the highest index in the string where substring ``sub`` is found, such that sub is contained within ``start``, ``end``.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.rfind, (sub, start, end))\n        return self._blocks_to_container(block_gen)\n\n    def rindex(self,\n            sub: str,\n            start: tp.Optional[int] = None,\n            end: tp.Optional[int] = None\n            ) -> TContainer:\n        '''\n        Like ``rfind``, but raises ``ValueError`` when the substring ``sub`` is not found.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.rindex, (sub, start, end))\n        return self._blocks_to_container(block_gen)\n\n    def rjust(self,\n            width: int,\n            fillchar: str = ' '\n            ) -> TContainer:\n        '''\n        Return a container with its elements rjusted in a string of length ``width``.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.rjust, (width, fillchar))\n        return self._blocks_to_container(block_gen)\n\n    def rpartition(self,\n            sep: str,\n            ) -> TContainer:\n        '''\n        Partition (split) each element around the right-most separator.\n        '''\n        # NOTE: py str.rpartition gives a tuple.\n        block_gen = self._process_tuple_blocks(\n                blocks=self._blocks,\n                method_name='rpartition',\n                args=(sep,),\n                dtype=object\n                )\n        return self._blocks_to_container(block_gen)\n\n    def rsplit(self,\n            sep: str,\n            maxsplit: int = -1,\n            ) -> TContainer:\n        '''\n        For each element, return a tuple of the words in the string, using sep as the delimiter string.\n        '''\n        # NOTE: npc.rsplit gives an array of lists, so implement our own routine to get an array of tuples.\n        block_gen = self._process_tuple_blocks(\n                blocks=self._blocks,\n                method_name='rsplit',\n                args=(sep, maxsplit),\n                dtype=object\n                )\n        return self._blocks_to_container(block_gen)\n\n    def rstrip(self,\n            chars: tp.Optional[str] = None,\n            ) -> TContainer:\n        '''\n        For each element, return a copy with the trailing characters removed.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.rstrip, (chars,))\n        return self._blocks_to_container(block_gen)\n\n    def split(self,\n            sep: str,\n            maxsplit: int = -1,\n            ) -> TContainer:\n        '''\n        For each element, return a tuple of the words in the string, using sep as the delimiter string.\n        '''\n        # NOTE: npc.split gives an array of lists, so implement our own routine to get an array of tuples.\n        block_gen = self._process_tuple_blocks(\n                blocks=self._blocks,\n                method_name='split',\n                args=(sep, maxsplit),\n                dtype=object\n                )\n        return self._blocks_to_container(block_gen)\n\n    #splitlines: not likely useful\n\n    def startswith(self,\n            prefix: str,\n            start: tp.Optional[int] = None,\n            end: tp.Optional[int] = None\n            ) -> TContainer:\n        '''\n        Returns a container with the number of non-overlapping occurrences of substring sub in the optional range ``start``, ``end``.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.startswith, (prefix, start, end))\n        return self._blocks_to_container(block_gen)\n\n    def strip(self,\n            chars: tp.Optional[str] = None,\n            ) -> TContainer:\n        '''\n        For each element, return a copy with the leading and trailing characters removed.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.strip, (chars,))\n        return self._blocks_to_container(block_gen)\n\n    def swapcase(self) -> TContainer:\n        '''\n        Return a container with uppercase characters converted to lowercase and vice versa.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.swapcase)\n        return self._blocks_to_container(block_gen)\n\n    def title(self) -> TContainer:\n        '''\n        Return a container with uppercase characters converted to lowercase and vice versa.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.title)\n        return self._blocks_to_container(block_gen)\n\n    # translate: akward input\n\n    def upper(self) -> TContainer:\n        '''\n        Return a container with uppercase characters converted to lowercase and vice versa.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.upper)\n        return self._blocks_to_container(block_gen)\n\n    def zfill(self,\n            width: int,\n            ) -> TContainer:\n        '''\n        Return the string left-filled with zeros.\n        '''\n        block_gen = self._process_blocks(self._blocks, npc.zfill, (width,))\n        return self._blocks_to_container(block_gen)\n"""
static_frame/core/series.py,49,"b""import typing as tp\nfrom functools import partial\nfrom itertools import chain\n\nimport numpy as np\nfrom numpy.ma import MaskedArray\n\nfrom static_frame.core.util import DEFAULT_SORT_KIND\nfrom static_frame.core.util import FLOAT_TYPES\nfrom static_frame.core.util import EMPTY_TUPLE\nfrom static_frame.core.util import NAME_DEFAULT\nfrom static_frame.core.util import NameType\nfrom static_frame.core.util import INT_TYPES\n\nfrom static_frame.core.util import GetItemKeyType\nfrom static_frame.core.util import resolve_dtype\nfrom static_frame.core.util import isna_array\nfrom static_frame.core.util import iterable_to_array_1d\nfrom static_frame.core.util import array_to_groups_and_locations\nfrom static_frame.core.util import array_to_duplicated\nfrom static_frame.core.util import full_for_fill\nfrom static_frame.core.util import mloc\nfrom static_frame.core.util import immutable_filter\nfrom static_frame.core.util import name_filter\nfrom static_frame.core.util import ufunc_axis_skipna\nfrom static_frame.core.util import array2d_to_tuples\nfrom static_frame.core.util import array_shift\nfrom static_frame.core.util import write_optional_file\nfrom static_frame.core.util import ufunc_unique\nfrom static_frame.core.util import concat_resolved\nfrom static_frame.core.util import NULL_SLICE\nfrom static_frame.core.util import binary_transition\nfrom static_frame.core.util import isin\nfrom static_frame.core.util import slices_from_targets\nfrom static_frame.core.util import is_callable_or_mapping\n\nfrom static_frame.core.util import AnyCallable\nfrom static_frame.core.util import CallableOrMapping\nfrom static_frame.core.util import SeriesInitializer\nfrom static_frame.core.util import PathSpecifierOrFileLike\nfrom static_frame.core.util import DepthLevelSpecifier\n\nfrom static_frame.core.util import DtypeSpecifier\nfrom static_frame.core.util import IndexInitializer\nfrom static_frame.core.util import IndexConstructor\nfrom static_frame.core.util import dtype_to_na\nfrom static_frame.core.util import argmin_1d\nfrom static_frame.core.util import argmax_1d\nfrom static_frame.core.util import intersect1d\n\nfrom static_frame.core.node_selector import InterfaceGetItem\nfrom static_frame.core.node_selector import InterfaceAssignTrio\nfrom static_frame.core.node_selector import InterfaceSelectTrio\n\nfrom static_frame.core.index_correspondence import IndexCorrespondence\nfrom static_frame.core.container import ContainerOperand\n\nfrom static_frame.core.display import DisplayConfig\nfrom static_frame.core.display import DisplayActive\nfrom static_frame.core.display import Display\nfrom static_frame.core.display import DisplayFormats\nfrom static_frame.core.display import DisplayHeader\n\nfrom static_frame.core.node_iter import IterNodeType\nfrom static_frame.core.node_iter import IterNodeGroup\nfrom static_frame.core.node_iter import IterNodeDepthLevel\nfrom static_frame.core.node_iter import IterNodeWindow\nfrom static_frame.core.node_iter import IterNodeNoArg\nfrom static_frame.core.node_iter import IterNodeApplyType\n\nfrom static_frame.core.node_str import InterfaceString\nfrom static_frame.core.node_dt import InterfaceDatetime\n\nfrom static_frame.core.index import Index\nfrom static_frame.core.index_hierarchy import IndexHierarchy\nfrom static_frame.core.index_base import IndexBase\nfrom static_frame.core.index_auto import IndexAutoFactory\nfrom static_frame.core.index_auto import IndexAutoFactoryType\n\nfrom static_frame.core.container_util import index_from_optional_constructor\nfrom static_frame.core.container_util import matmul\nfrom static_frame.core.container_util import axis_window_items\nfrom static_frame.core.container_util import rehierarch_from_index_hierarchy\nfrom static_frame.core.container_util import pandas_version_under_1\nfrom static_frame.core.container_util import pandas_to_numpy\nfrom static_frame.core.container_util import apply_binary_operator\n\nfrom static_frame.core.assign import Assign\n\nfrom static_frame.core.exception import ErrorInitSeries\nfrom static_frame.core.exception import AxisInvalid\n\nfrom static_frame.core.doc_str import doc_inject\n\nif tp.TYPE_CHECKING:\n    from static_frame import Frame # pylint: disable=W0611 #pragma: no cover\n    from pandas import DataFrame # pylint: disable=W0611 #pragma: no cover\n\n\nRelabelInput = tp.Union[CallableOrMapping, IndexAutoFactoryType, IndexInitializer]\n\n\n#-------------------------------------------------------------------------------\n@doc_inject(selector='container_init', class_name='Series')\nclass Series(ContainerOperand):\n    '''\n    A one-dimensional ordered, labelled container, immutable and of fixed size.\n\n    Args:\n        values: An iterable of values to be aligned with the supplied (or automatically generated) index.\n        {index}\n        {own_index}\n    '''\n\n    __slots__ = (\n            'values',\n            '_index',\n            '_name',\n            )\n\n    sum: tp.Callable[['Series'], tp.Any]\n\n    values: np.ndarray\n\n    _index: IndexBase\n\n    _NDIM: int = 1\n\n    #---------------------------------------------------------------------------\n    @classmethod\n    def from_element(cls,\n            element: tp.Any,\n            *,\n            index: IndexInitializer,\n            dtype: DtypeSpecifier = None,\n            name: tp.Hashable = None,\n            index_constructor: IndexConstructor = None,\n            own_index: bool = False,\n            ) -> 'Series':\n        '''\n        Create a :obj:`static_frame.Series` from a single element. The size of the resultant container will be determined by the ``index`` argument.\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        if own_index:\n            index_final = index\n        else:\n            index_final = index_from_optional_constructor(index,\n                    default_constructor=Index,\n                    explicit_constructor=index_constructor\n                    )\n        array = np.full(\n                len(index_final),\n                fill_value=element,\n                dtype=dtype)\n        array.flags.writeable = False\n        return cls(array,\n                index=index_final,\n                name=name,\n                own_index=True,\n                )\n\n\n    @classmethod\n    def from_items(cls,\n            pairs: tp.Iterable[tp.Tuple[tp.Hashable, tp.Any]],\n            *,\n            dtype: DtypeSpecifier = None,\n            name: tp.Hashable = None,\n            index_constructor: tp.Optional[tp.Callable[..., IndexBase]] = None\n            ) -> 'Series':\n        '''Series construction from an iterator or generator of pairs, where the first pair value is the index and the second is the value.\n\n        Args:\n            pairs: Iterable of pairs of index, value.\n            dtype: dtype or valid dtype specifier.\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        index = []\n        def values():\n            for k, v in pairs:\n                # populate index list as side effect of iterating values\n                index.append(k)\n                yield v\n\n        return cls(values(),\n                index=index,\n                dtype=dtype,\n                name=name,\n                index_constructor=index_constructor)\n\n\n    @classmethod\n    def from_dict(cls,\n            mapping: tp.Dict[tp.Hashable, tp.Any],\n            *,\n            dtype: DtypeSpecifier = None,\n            name: tp.Hashable = None,\n            index_constructor: tp.Optional[tp.Callable[..., IndexBase]] = None\n            ) -> 'Series':\n        '''Series construction from a dictionary, where the first pair value is the index and the second is the value.\n\n        Args:\n            mapping: a dictionary or similar mapping interface.\n            dtype: dtype or valid dtype specifier.\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        return cls.from_items(mapping.items(),\n                name=name,\n                dtype=dtype,\n                index_constructor=index_constructor)\n\n    @classmethod\n    def from_concat(cls,\n            containers: tp.Iterable['Series'],\n            *,\n            index: tp.Union[IndexInitializer, IndexAutoFactoryType] = None,\n            name: tp.Hashable = None\n            ) -> 'Series':\n        '''\n        Concatenate multiple :obj:`Series` into a new :obj:`Series`.\n\n        Args:\n            containers: Iterable of ``Series`` from which values in the new ``Series`` are drawn.\n            index: If None, the resultant index will be the concatenation of all indices (assuming they are unique in combination). If ``IndexAutoFactory``, the resultant index is a auto-incremented integer index. Otherwise, the value is used as a index initializer.\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        array_values = []\n        if index is None:\n            array_index = []\n        for c in containers:\n            array_values.append(c.values)\n            if index is None:\n                array_index.append(c.index.values)\n\n        # End quickly if empty iterable\n        if not array_values:\n            return cls(EMPTY_TUPLE, index=index, name=name)\n\n        # returns immutable arrays\n        values = concat_resolved(array_values)\n\n        if index is None:\n            index = concat_resolved(array_index)\n            if index.ndim == 2:\n                index = IndexHierarchy.from_labels(index)\n        elif index is IndexAutoFactory:\n            # set index arg to None to force IndexAutoFactory usage in creation\n            index = None\n\n        return cls(values, index=index, name=name)\n\n    @classmethod\n    def from_concat_items(cls,\n            items: tp.Iterable[tp.Tuple[tp.Hashable, 'Series']]\n            ) -> 'Series':\n        '''\n        Produce a :obj:`Series` with a hierarchical index from an iterable of pairs of labels, :obj:`Series`. The :obj:`IndexHierarchy` is formed from the provided labels and the :obj:`Index` if each :obj:`Series`.\n\n        Args:\n            items: Iterable of pairs of label, :obj:`Series`\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        array_values = []\n\n        def gen():\n            for label, series in items:\n                array_values.append(series.values)\n                yield label, series._index\n\n        try:\n            # populates array_values as side effect\n            ih = IndexHierarchy.from_index_items(gen())\n            # returns immutable array\n            values = concat_resolved(array_values)\n            own_index = True\n        except StopIteration:\n            # Default to empty when given an empty iterable\n            ih = None\n            values = EMPTY_TUPLE\n            own_index= False\n\n        return cls(values, index=ih, own_index=own_index)\n\n\n    @classmethod\n    @doc_inject()\n    def from_pandas(cls,\n            value,\n            *,\n            index_constructor: IndexConstructor = None,\n            own_data: bool = False) -> 'Series':\n        '''Given a Pandas Series, return a Series.\n\n        Args:\n            value: Pandas Series.\n            {own_data}\n            {own_index}\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        if pandas_version_under_1():\n            if own_data:\n                data = value.values\n                data.flags.writeable = False\n            else:\n                data = immutable_filter(value.values)\n        else:\n            data = pandas_to_numpy(value, own_data=own_data)\n\n        own_index = True\n        if index_constructor is IndexAutoFactory:\n            index = None\n            own_index = False\n        elif index_constructor is not None:\n            index = index_constructor(value.index)\n        else: # if None\n            index = Index.from_pandas(value.index)\n\n        return cls(data,\n                index=index,\n                name=value.name,\n                own_index=own_index\n                )\n\n\n    #---------------------------------------------------------------------------\n    def __init__(self,\n            values: SeriesInitializer,\n            *,\n            index: tp.Union[IndexInitializer, IndexAutoFactoryType, None] = None,\n            name: NameType = NAME_DEFAULT,\n            dtype: DtypeSpecifier = None,\n            index_constructor: IndexConstructor = None,\n            own_index: bool = False\n            ) -> None:\n        '''\n        Default constructor of :obj:`static_frame.Series`.\n        '''\n\n        if own_index and index is None:\n            raise ErrorInitSeries('cannot own_index if no index is provided.')\n\n        #-----------------------------------------------------------------------\n        # values assignment\n\n        values_constructor = None # if deferred\n\n        if not isinstance(values, np.ndarray):\n            if isinstance(values, dict):\n                raise ErrorInitSeries('use Series.from_dict to create a Series from a mapping.')\n            elif isinstance(values, Series):\n                self.values = values.values # take immutable array\n                if dtype is not None and dtype != values.dtype:\n                    raise ErrorInitSeries(f'when supplying values via Series, the dtype argument is not required; if provided ({dtype}), it must agree with the dtype of the Series ({values.dtype})')\n                if index is None and index_constructor is None:\n                    # set up for direct assignment below; index is always immutable\n                    index = values.index\n                    own_index = True\n                if name is NAME_DEFAULT:\n                    name = values.name # propagate Series.name\n            elif hasattr(values, '__iter__') and not isinstance(values, str):\n                # returned array is already immutable\n                self.values, _ = iterable_to_array_1d(values, dtype=dtype)\n            else: # it must be an element, or a string\n                raise ErrorInitSeries('Use Series.from_element to create a Series from an element.')\n\n        else: # is numpy array\n            if dtype is not None and dtype != values.dtype:\n                raise ErrorInitSeries(f'when supplying values via array, the dtype argument is not required; if provided ({dtype}), it must agree with the dtype of the array ({values.dtype})')\n\n            if values.shape == (): # handle special case of NP element\n                def values_constructor(count): #pylint: disable=E0102\n                    self.values = np.repeat(values, count)\n                    self.values.flags.writeable = False\n            else:\n                self.values = immutable_filter(values)\n\n        self._name = None if name is NAME_DEFAULT else name_filter(name)\n\n        #-----------------------------------------------------------------------\n        # index assignment\n\n        if own_index:\n            self._index = index\n        elif index is None or index is IndexAutoFactory:\n            # if a values constructor is defined, self.values is not yet defined, and no index is supplied, the resultant shape will be of length 1. (If an index is supplied, the shape might be larger than one if an array element was given\n            if values_constructor:\n                value_count = 1\n            else:\n                value_count = len(self.values)\n            self._index = IndexAutoFactory.from_optional_constructor(\n                    value_count,\n                    default_constructor=Index,\n                    explicit_constructor=index_constructor\n                    )\n        else: # an iterable of labels, or an index subclass\n            self._index = index_from_optional_constructor(index,\n                    default_constructor=Index,\n                    explicit_constructor=index_constructor\n                    )\n        index_count = self._index.__len__()\n\n        if not self._index.STATIC:\n            raise ErrorInitSeries('non-static index cannot be assigned to Series')\n\n        if values_constructor:\n            values_constructor(index_count) # updates self.values\n            # must update after calling values constructor\n        value_count = len(self.values)\n\n        #-----------------------------------------------------------------------\n        # final evaluation\n\n        if self.values.ndim != self._NDIM:\n            raise ErrorInitSeries('dimensionality of final values not supported')\n        if value_count != index_count:\n            raise ErrorInitSeries(\n                f'Index has incorrect size (got {index_count}, expected {value_count})'\n                )\n\n    # ---------------------------------------------------------------------------\n    def __reversed__(self) -> tp.Iterator[tp.Hashable]:\n        '''\n        Returns a reverse iterator on the series' index.\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        return reversed(self._index)\n\n    #---------------------------------------------------------------------------\n    def __setstate__(self, state) -> None:\n        '''\n        Ensure that reanimated NP arrays are set not writeable.\n        '''\n        for key, value in state[1].items():\n            setattr(self, key, value)\n        self.values.flags.writeable = False\n\n    #---------------------------------------------------------------------------\n    # name interface\n\n    @property\n    @doc_inject()\n    def name(self) -> tp.Hashable:\n        '''{}'''\n        return self._name\n\n    def rename(self, name: NameType) -> 'Series':\n        '''\n        Return a new Series with an updated name attribute.\n        '''\n        return self.__class__(self.values,\n                index=self._index,\n                name=name,\n                )\n\n    #---------------------------------------------------------------------------\n    # interfaces\n\n    @property\n    def loc(self) -> InterfaceGetItem:\n        '''\n        Interface for label-based selection.\n        '''\n        return InterfaceGetItem(self._extract_loc)\n\n    @property\n    def iloc(self) -> InterfaceGetItem:\n        '''\n        Interface for position-based selection.\n        '''\n        return InterfaceGetItem(self._extract_iloc)\n\n    @property\n    def drop(self) -> InterfaceSelectTrio:\n        '''\n        Interface for dropping elements from :obj:`static_frame.Series`.\n        '''\n        return InterfaceSelectTrio(\n                func_iloc=self._drop_iloc,\n                func_loc=self._drop_loc,\n                func_getitem=self._drop_loc\n                )\n\n    @property\n    def mask(self) -> InterfaceSelectTrio:\n        '''\n        Interface for extracting Boolean :obj:`static_frame.Series`.\n        '''\n        return InterfaceSelectTrio(\n                func_iloc=self._extract_iloc_mask,\n                func_loc=self._extract_loc_mask,\n                func_getitem=self._extract_loc_mask\n                )\n\n    @property\n    def masked_array(self) -> InterfaceSelectTrio:\n        '''\n        Interface for extracting NumPy Masked Arrays.\n        '''\n        return InterfaceSelectTrio(\n                func_iloc=self._extract_iloc_masked_array,\n                func_loc=self._extract_loc_masked_array,\n                func_getitem=self._extract_loc_masked_array\n                )\n\n    @property\n    def assign(self) -> InterfaceAssignTrio:\n        '''\n        Interface for doing assignment-like selection and replacement.\n        '''\n        # NOTE: this is not a InterfaceAssignQuartet, like on Frame\n        return InterfaceAssignTrio(\n                func_iloc=self._extract_iloc_assign,\n                func_loc=self._extract_loc_assign,\n                func_getitem=self._extract_loc_assign,\n                delegate=SeriesAssign\n                )\n\n    #---------------------------------------------------------------------------\n    @property\n    def via_str(self) -> InterfaceString['Series']:\n        '''\n        Interface for applying string methods to elements in this container.\n        '''\n        blocks = (self.values,)\n\n        def blocks_to_container(blocks: tp.Iterator[np.ndarray]) -> 'Frame':\n            return self.__class__(\n                next(blocks), # assume only one\n                index=self._index,\n                name=self._name,\n                own_index=True,\n                )\n\n        return InterfaceString(\n                blocks=blocks,\n                blocks_to_container=blocks_to_container,\n                )\n\n    @property\n    def via_dt(self) -> InterfaceDatetime['Series']:\n        '''\n        Interface for applying datetime properties and methods to elements in this container.\n        '''\n        blocks = (self.values,)\n\n        def blocks_to_container(blocks: tp.Iterator[np.ndarray]) -> 'Frame':\n            return self.__class__(\n                next(blocks), # assume only one\n                index=self._index,\n                name=self._name,\n                own_index=True,\n                )\n\n        return InterfaceDatetime(\n                blocks=blocks,\n                blocks_to_container=blocks_to_container,\n                )\n\n    #---------------------------------------------------------------------------\n    @property\n    def iter_group(self) -> IterNodeGroup:\n        '''\n        Iterator of :obj:`static_frame.Series`, where each :obj:`static_frame.Series` is matches unique values.\n        '''\n        return IterNodeGroup(\n                container=self,\n                function_items=self._axis_group_items,\n                function_values=self._axis_group,\n                yield_type=IterNodeType.VALUES\n                )\n\n    @property\n    def iter_group_items(self) -> IterNodeGroup:\n        return IterNodeGroup(\n                container=self,\n                function_items=self._axis_group_items,\n                function_values=self._axis_group,\n                yield_type=IterNodeType.ITEMS\n                )\n\n    #---------------------------------------------------------------------------\n    @property\n    def iter_group_labels(self) -> IterNodeDepthLevel:\n        return IterNodeDepthLevel(\n                container=self,\n                function_items=self._axis_group_labels_items,\n                function_values=self._axis_group_labels,\n                yield_type=IterNodeType.VALUES,\n                apply_type=IterNodeApplyType.SERIES_ITEMS_FLAT\n                )\n\n    @property\n    def iter_group_labels_items(self) -> IterNodeDepthLevel:\n        return IterNodeDepthLevel(\n                container=self,\n                function_items=self._axis_group_labels_items,\n                function_values=self._axis_group_labels,\n                yield_type=IterNodeType.ITEMS,\n                apply_type=IterNodeApplyType.SERIES_ITEMS_FLAT\n                )\n\n    #---------------------------------------------------------------------------\n    @property\n    def iter_element(self) -> IterNodeNoArg:\n        '''\n        Iterator of elements.\n        '''\n        return IterNodeNoArg(\n                container=self,\n                function_items=self._axis_element_items,\n                function_values=self._axis_element,\n                yield_type=IterNodeType.VALUES\n                )\n\n    @property\n    def iter_element_items(self) -> IterNodeNoArg:\n        '''\n        Iterator of label, element pairs.\n        '''\n        return IterNodeNoArg(\n                container=self,\n                function_items=self._axis_element_items,\n                function_values=self._axis_element,\n                yield_type=IterNodeType.ITEMS\n                )\n\n    #---------------------------------------------------------------------------\n    @property\n    def iter_window(self) -> IterNodeWindow:\n        function_values = partial(self._axis_window, as_array=False)\n        function_items = partial(self._axis_window_items, as_array=False)\n        return IterNodeWindow(\n                container=self,\n                function_values=function_values,\n                function_items=function_items,\n                yield_type=IterNodeType.VALUES\n                )\n\n    @property\n    def iter_window_items(self) -> IterNodeWindow:\n        function_values = partial(self._axis_window, as_array=False)\n        function_items = partial(self._axis_window_items, as_array=False)\n        return IterNodeWindow(\n                container=self,\n                function_values=function_values,\n                function_items=function_items,\n                yield_type=IterNodeType.ITEMS\n                )\n\n\n    @property\n    def iter_window_array(self) -> IterNodeWindow:\n        function_values = partial(self._axis_window, as_array=True)\n        function_items = partial(self._axis_window_items, as_array=True)\n        return IterNodeWindow(\n                container=self,\n                function_values=function_values,\n                function_items=function_items,\n                yield_type=IterNodeType.VALUES\n                )\n\n    @property\n    def iter_window_array_items(self) -> IterNodeWindow:\n        function_values = partial(self._axis_window, as_array=True)\n        function_items = partial(self._axis_window_items, as_array=True)\n        return IterNodeWindow(\n                container=self,\n                function_values=function_values,\n                function_items=function_items,\n                yield_type=IterNodeType.ITEMS\n                )\n    #---------------------------------------------------------------------------\n    # index manipulation\n\n    def _reindex_other_like_iloc(self,\n            value: 'Series',\n            iloc_key: GetItemKeyType,\n            fill_value: tp.Any = np.nan) -> 'Series':\n        '''Given a value that is a Series, reindex it to the index components, drawn from this Series, that are specified by the iloc_key.\n        '''\n        return value.reindex(\n                self._index._extract_iloc(iloc_key),\n                fill_value=fill_value\n                )\n\n    @doc_inject(selector='reindex', class_name='Series')\n    def reindex(self,\n            index: IndexInitializer,\n            *,\n            fill_value=np.nan,\n            own_index: bool = False,\n            check_equals: bool = True\n            ) -> 'Series':\n        '''\n        {doc}\n\n        Args:\n            index: {index_initializer}\n            columns: {index_initializer}\n            {fill_value}\n            {own_index}\n        '''\n        if not own_index:\n            index = index_from_optional_constructor(index,\n                    default_constructor=Index)\n\n        # NOTE: it is assumed that the equals comparison is faster than continuing with this method\n        if check_equals and self._index.equals(index):\n            # if labels are equal (even if a different Index subclass), we can simply use the new Index\n            return self.__class__(\n                    self.values,\n                    index=index,\n                    own_index=True,\n                    name=self._name)\n\n        ic = IndexCorrespondence.from_correspondence(self._index, index)\n\n        if ic.is_subset: # must have some common\n            values = self.values[ic.iloc_src]\n            values.flags.writeable = False\n            return self.__class__(\n                    values,\n                    index=index,\n                    own_index=True,\n                    name=self._name)\n\n        values = full_for_fill(self.values.dtype, len(index), fill_value)\n        # if some intersection of values\n        if ic.has_common:\n            values[ic.iloc_dst] = self.values[ic.iloc_src]\n        values.flags.writeable = False\n\n        return self.__class__(values,\n                index=index,\n                own_index=True,\n                name=self._name)\n\n    @doc_inject(selector='relabel', class_name='Series')\n    def relabel(self,\n            index: RelabelInput\n            ) -> 'Series':\n        '''\n        {doc}\n\n        Args:\n            index: {relabel_input}\n        '''\n        own_index = False\n        if index is IndexAutoFactory:\n            index = None\n        elif is_callable_or_mapping(index):\n            index = self._index.relabel(index)\n            own_index = True\n        elif index is None:\n            index = self._index\n        # else: # assume index IndexInitializer\n        #     index = index\n\n        return self.__class__(self.values,\n                index=index,\n                own_index=own_index,\n                name=self._name)\n\n    @doc_inject(selector='relabel_flat', class_name='Series')\n    def relabel_flat(self) -> 'Series':\n        '''\n        {doc}\n        '''\n        return self.__class__(self.values,\n                index=self._index.flat(),\n                name=self._name)\n\n    @doc_inject(selector='relabel_add_level', class_name='Series')\n    def relabel_add_level(self,\n            level: tp.Hashable\n            ) -> 'Series':\n        '''\n        {doc}\n\n        Args:\n            level: {level}\n        '''\n        return self.__class__(self.values,\n                index=self._index.add_level(level),\n                name=self._name)\n\n    @doc_inject(selector='relabel_drop_level', class_name='Series')\n    def relabel_drop_level(self,\n            count: int = 1\n            ) -> 'Series':\n        '''\n        {doc}\n\n        Args:\n            count: {count}\n        '''\n        return self.__class__(self.values,\n                index=self._index.drop_level(count),\n                name=self._name)\n\n\n    def rehierarch(self,\n            depth_map: tp.Iterable[int]\n            ) -> 'Series':\n        '''\n        Return a new :obj:`Series` with new a hierarchy based on the supplied ``depth_map``.\n        '''\n        if self.index.depth == 1:\n            raise RuntimeError('cannot rehierarch when there is no hierarchy')\n\n        index, iloc_map = rehierarch_from_index_hierarchy(\n                labels=self._index,\n                depth_map=depth_map,\n                name=self._index.name,\n                )\n        values = self.values[iloc_map]\n        values.flags.writeable = False\n        return self.__class__(values,\n                index=index,\n                name=self._name)\n\n\n    #---------------------------------------------------------------------------\n    # na handling\n\n    def isna(self) -> 'Series':\n        '''\n        Return a same-indexed, Boolean ``Series`` indicating which values are NaN or None.\n        '''\n        # consider returning self if not values.any()?\n        values = isna_array(self.values)\n        values.flags.writeable = False\n        return self.__class__(values, index=self._index)\n\n    def notna(self) -> 'Series':\n        '''\n        Return a same-indexed, Boolean Series indicating which values are NaN or None.\n        '''\n        values = np.logical_not(isna_array(self.values))\n        values.flags.writeable = False\n        return self.__class__(values, index=self._index)\n\n    def dropna(self) -> 'Series':\n        '''\n        Return a new :obj:`static_frame.Series` after removing values of NaN or None.\n        '''\n        # get positions that we want to keep\n        sel = np.logical_not(isna_array(self.values))\n        if not np.any(sel):\n            return self.__class__(())\n\n        values = self.values[sel]\n        values.flags.writeable = False\n\n        return self.__class__(values,\n                index=self._index.loc[sel],\n                name=self._name,\n                own_index=True)\n\n    @doc_inject(selector='fillna')\n    def fillna(self,\n            value: tp.Any # an element or a Series\n            ) -> 'Series':\n        '''Return a new :obj:`static_frame.Series` after replacing null (NaN or None) with the supplied value. The value can be element or\n\n        Args:\n            {value}\n        '''\n        sel = isna_array(self.values)\n        if not np.any(sel):\n            return self\n\n        if hasattr(value, '__iter__') and not isinstance(value, str):\n            if not isinstance(value, Series):\n                raise RuntimeError('unlabeled iterables cannot be used for fillna: use a Series')\n            value_dtype = value.dtype\n            # choose a fill value that will not force a type coercion\n            fill_value = dtype_to_na(value_dtype)\n            # find targets that are NaN in self and have labels in value; otherwise, might fill values after reindexing, and end up filling a fill_value rather than keeping original (na) value\n            sel = self.index.isin(\n                    intersect1d(self.index.values[sel], value.index.values))\n            if not np.any(sel): # avoid copying, retyping\n                return self\n\n            # must reindex to align ordering; just get array\n            value = self._reindex_other_like_iloc(value,\n                    sel,\n                    fill_value=fill_value).values\n        else:\n            value_dtype = np.array(value).dtype\n\n        assignable_dtype = resolve_dtype(value_dtype, self.values.dtype)\n\n        if self.values.dtype == assignable_dtype:\n            assigned = self.values.copy()\n        else:\n            assigned = self.values.astype(assignable_dtype)\n\n        assigned[sel] = value\n        assigned.flags.writeable = False\n\n        return self.__class__(assigned,\n                index=self._index,\n                name=self._name)\n\n\n\n    @staticmethod\n    def _fillna_directional(\n            array: np.ndarray,\n            directional_forward: bool,\n            limit: int = 0) -> np.ndarray:\n        '''Return a new ``Series`` after feeding forward the last non-null (NaN or None) observation across contiguous nulls.\n\n        Args:\n            count: Set the limit of nan values to be filled per nan region. A value of 0 is equivalent to no limit.\n        '''\n        sel = isna_array(array)\n        if not np.any(sel):\n            return array # assume immutable\n\n        def slice_condition(target_slice: slice) -> bool:\n            # NOTE: start is never None\n            return sel[target_slice.start]\n\n        # type is already compatible, no need for check\n        assigned = array.copy()\n        target_index = binary_transition(sel)\n        target_values = array[target_index]\n        length = len(array)\n\n        for target_slice, value in slices_from_targets(\n                target_index=target_index,\n                target_values=target_values,\n                length=length,\n                directional_forward=directional_forward,\n                limit=limit,\n                slice_condition=slice_condition # isna True in region\n                ):\n            assigned[target_slice] = value\n\n        assigned.flags.writeable = False\n        return assigned\n\n    @doc_inject(selector='fillna')\n    def fillna_forward(self, limit: int = 0) -> 'Series':\n        '''Return a new ``Series`` after feeding forward the last non-null (NaN or None) observation across contiguous nulls.\n\n        Args:\n            {limit}\n        '''\n        return self.__class__(self._fillna_directional(\n                    array=self.values,\n                    directional_forward=True,\n                    limit=limit),\n                index=self._index,\n                name=self._name)\n\n    @doc_inject(selector='fillna')\n    def fillna_backward(self, limit: int = 0) -> 'Series':\n        '''Return a new ``Series`` after feeding backward the last non-null (NaN or None) observation across contiguous nulls.\n\n        Args:\n            {limit}\n        '''\n        return self.__class__(self._fillna_directional(\n                    array=self.values,\n                    directional_forward=False,\n                    limit=limit),\n                index=self._index,\n                name=self._name)\n\n\n    @staticmethod\n    def _fillna_sided(array: np.ndarray,\n            value: tp.Any,\n            sided_leading: bool):\n        '''\n        Args:\n            sided_leading: True sets the side to fill is the leading side; False sets the side to fill to the trailiing side.\n        '''\n        sel = isna_array(array)\n\n        if not np.any(sel):\n            return array\n\n        sided_index = 0 if sided_leading else -1\n\n        if sel[sided_index] == False:\n            # sided value is not null: nothing to do\n            return array # assume immutable\n\n        if isinstance(value, np.ndarray):\n            raise RuntimeError('cannot assign an array to fillna')\n\n        assignable_dtype = resolve_dtype(np.array(value).dtype, array.dtype)\n\n        if array.dtype == assignable_dtype:\n            assigned = array.copy()\n        else:\n            assigned = array.astype(assignable_dtype)\n\n        targets = np.nonzero(~sel)[0] # as 1D, can just take index 0 resuilts\n        if len(targets):\n            if sided_leading:\n                sel_slice = slice(0, targets[0])\n            else: # trailing\n                sel_slice = slice(targets[-1]+1, None)\n        else: # all are NaN\n            sel_slice = NULL_SLICE\n\n        assigned[sel_slice] = value\n        assigned.flags.writeable = False\n        return assigned\n\n    @doc_inject(selector='fillna')\n    def fillna_leading(self, value: tp.Any) -> 'Series':\n        '''Return a new ``Series`` after filling leading (and only leading) null (NaN or None) with the supplied value.\n\n        Args:\n            {value}\n        '''\n        return self.__class__(self._fillna_sided(\n                    array=self.values,\n                    value=value,\n                    sided_leading=True),\n                index=self._index,\n                name=self._name)\n\n    @doc_inject(selector='fillna')\n    def fillna_trailing(self, value: tp.Any) -> 'Series':\n        '''Return a new ``Series`` after filling trailing (and only trailing) null (NaN or None) with the supplied value.\n\n        Args:\n            {value}\n        '''\n        return self.__class__(self._fillna_sided(\n                    array=self.values,\n                    value=value,\n                    sided_leading=False),\n                index=self._index,\n                name=self._name)\n\n    #---------------------------------------------------------------------------\n    # operators\n\n    def _ufunc_unary_operator(self, operator: tp.Callable) -> 'Series':\n        '''\n        For unary operations, the `name` attribute propagates.\n        '''\n        values = operator(self.values)\n        return self.__class__(values,\n                index=self._index,\n                dtype=values.dtype, # some operators might change the dtype\n                name=self._name\n                )\n\n    def _ufunc_binary_operator(self, *,\n            operator: tp.Callable,\n            other: tp.Any,\n            ) -> 'Series':\n        '''\n        For binary operations, the `name` attribute does not propagate unless other is a scalar.\n        '''\n        # get both reverse and regular\n        if operator.__name__ == 'matmul':\n            return matmul(self, other)\n        elif operator.__name__ == 'rmatmul':\n            return matmul(other, self)\n\n        values = self.values\n        index = self._index\n        other_is_array = False\n\n        if isinstance(other, Series):\n            name = None\n            other_is_array = True\n            if not self._index.equals(other._index):\n                # if not equal, create a new Index by forming the union\n                index = self._index.union(other._index)\n                # now need to reindex the Series\n                values = self.reindex(index, own_index=True, check_equals=False).values\n                other = other.reindex(index, own_index=True, check_equals=False).values\n            else:\n                other = other.values\n        elif isinstance(other, np.ndarray):\n            name = None\n            other_is_array = True\n            if other.ndim > 1:\n                raise NotImplementedError('Operator application to greater dimensionalities will result in an array with more than 1 dimension.')\n        else:\n            name = self._name\n\n        result = apply_binary_operator(\n                values=values,\n                other=other,\n                other_is_array=other_is_array,\n                operator=operator,\n                )\n        return self.__class__(result, index=index, name=name)\n\n    def _ufunc_axis_skipna(self, *,\n            axis: int,\n            skipna: bool,\n            ufunc,\n            ufunc_skipna,\n            composable: bool,\n            dtypes: tp.Tuple[np.dtype, ...],\n            size_one_unity: bool\n            ) -> np.ndarray:\n        '''\n        For a Series, all functions of this type reduce the single axis of the Series to a single element, so Index has no use here.\n\n        Args:\n            dtype: not used, part of signature for a common interface\n        '''\n        return ufunc_axis_skipna(\n                array=self.values,\n                skipna=skipna,\n                axis=0,\n                ufunc=ufunc,\n                ufunc_skipna=ufunc_skipna\n                )\n\n    def _ufunc_shape_skipna(self, *,\n            axis: int,\n            skipna: bool,\n            ufunc,\n            ufunc_skipna,\n            composable: bool,\n            dtypes: tp.Tuple[np.dtype, ...],\n            size_one_unity: bool\n            ) -> 'Series':\n        '''\n        NumPy ufunc proccessors that retain the shape of the processed.\n\n        Args:\n            dtypes: not used, part of signature for a common interface\n        '''\n        values = ufunc_axis_skipna(\n                array=self.values,\n                skipna=skipna,\n                axis=0,\n                ufunc=ufunc,\n                ufunc_skipna=ufunc_skipna\n                )\n        values.flags.writeable = False\n        return self.__class__(values, index=self._index)\n\n    #---------------------------------------------------------------------------\n    def __len__(self) -> int:\n        '''Length of values.\n        '''\n        return self.values.__len__()\n\n    def _display(self,\n            config: DisplayConfig,\n            display_cls: Display,\n            ) -> Display:\n        '''\n        Private display interface to be shared by Bus and Series.\n        '''\n        index_depth = self._index.depth if config.include_index else 0\n        display_index = self._index.display(config=config)\n\n        # When showing type we need 2: one for the Series type, the other for the index type.\n        header_depth = 2 * config.type_show\n\n        # create an empty display based on index display\n        d = Display([list() for _ in range(len(display_index))],\n                config=config,\n                outermost=True,\n                index_depth=index_depth,\n                header_depth=header_depth\n                )\n\n        if config.include_index:\n            d.extend_display(display_index)\n            header_values = '' if config.type_show else None\n        else:\n            header_values = None\n\n        d.extend_display(Display.from_values(\n                self.values,\n                header=header_values,\n                config=config))\n\n        if config.type_show:\n            d.insert_displays(display_cls.flatten())\n\n        return d\n\n    @doc_inject()\n    def display(self,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> Display:\n        '''{doc}\n\n        Args:\n            {config}\n        '''\n        config = config or DisplayActive.get()\n        display_cls = Display.from_values((),\n                header=DisplayHeader(self.__class__, self._name),\n                config=config)\n        return self._display(config, display_cls)\n\n        # config = config or DisplayActive.get()\n        # index_depth = self._index.depth if config.include_index else 0\n        # display_index = self._index.display(config=config)\n\n        # # When showing type we need 2: one for the Series type, the other for the index type.\n        # header_depth = 2 * config.type_show\n\n        # # create an empty display based on index display\n        # d = Display([list() for _ in range(len(display_index))],\n        #         config=config,\n        #         outermost=True,\n        #         index_depth=index_depth,\n        #         header_depth=header_depth\n        #         )\n\n        # if config.include_index:\n        #     d.extend_display(display_index)\n        #     header_values = '' if config.type_show else None\n        # else:\n        #     header_values = None\n\n        # d.extend_display(Display.from_values(\n        #         self.values,\n        #         header=header_values,\n        #         config=config))\n\n        # if config.type_show:\n        #     display_cls = Display.from_values((),\n        #             header=DisplayHeader(self.__class__, self._name),\n        #             config=config)\n        #     d.insert_displays(display_cls.flatten())\n\n        # return d\n\n    #---------------------------------------------------------------------------\n    # common attributes from the numpy array\n\n    @property\n    @doc_inject()\n    def mloc(self) -> int:\n        '''{doc_int}\n        '''\n        return mloc(self.values)\n\n    @property\n    def dtype(self) -> np.dtype:\n        '''\n        Return the dtype of the underlying NumPy array.\n\n        Returns:\n            :obj:`numpy.dtype`\n        '''\n        return self.values.dtype\n\n    @property\n    def shape(self) -> tp.Tuple[int]:\n        '''\n        Return a tuple describing the shape of the underlying NumPy array.\n\n        Returns:\n            :obj:`Tuple[int]`\n        '''\n        return self.values.shape\n\n    @property\n    def ndim(self) -> int:\n        '''\n        Return the number of dimensions, which for a `Series` is always 1.\n\n        Returns:\n            :obj:`int`\n        '''\n        return self._NDIM\n\n    @property\n    def size(self) -> int:\n        '''\n        Return the size of the underlying NumPy array.\n\n        Returns:\n            :obj:`int`\n        '''\n        return self.values.size\n\n    @property\n    def nbytes(self) -> int:\n        '''\n        Return the total bytes of the underlying NumPy array.\n\n        Returns:\n            :obj:`int`\n        '''\n        return self.values.nbytes\n\n    def __bool__(self) -> bool:\n        '''\n        True if this container has size.\n        '''\n        return bool(self.values.size)\n\n\n    #---------------------------------------------------------------------------\n    # extraction\n\n    def _extract_iloc(self, key: GetItemKeyType) -> 'Series':\n        # iterable selection should be handled by NP\n        values = self.values[key]\n\n        if not isinstance(values, np.ndarray): # if we have a single element\n            return values\n        return self.__class__(\n                values,\n                index=self._index.iloc[key],\n                name=self._name)\n\n    def _extract_loc(self, key: GetItemKeyType) -> 'Series':\n        '''\n        Compatibility:\n            Pandas supports taking in iterables of keys, where some keys are not found in the index; a Series is returned as if a reindex operation was performed. This is undesirable. Better instead is to use reindex()\n        '''\n        iloc_key = self._index.loc_to_iloc(key)\n        values = self.values[iloc_key]\n\n        if not isinstance(values, np.ndarray): # if we have a single element\n            # NOTE: this branch is not encountered and may not be necessary\n            # if isinstance(key, HLoc) and key.has_key_multiple():\n            #     # must return a Series, even though we do not have an array\n            #     values = np.array(values)\n            #     values.flags.writeable = False\n            return values\n\n        return self.__class__(values,\n                index=self._index.iloc[iloc_key],\n                own_index=True,\n                name=self._name)\n\n    @doc_inject(selector='selector')\n    def __getitem__(self, key: GetItemKeyType) -> 'Series':\n        '''Selector of values by label.\n\n        Args:\n            key: {key_loc}\n\n        Compatibility:\n            Pandas supports using both loc and iloc style selections with the __getitem__ interface on Series. This is undesirable, so here we only expose the loc interface (making the Series dictionary like, but unlike the Index, where __getitem__ is an iloc).\n        '''\n        return self._extract_loc(key)\n\n    #---------------------------------------------------------------------------\n    # utilites for alternate extraction: drop, mask and assignment\n\n    def _drop_iloc(self, key: GetItemKeyType) -> 'Series':\n        if isinstance(key, np.ndarray) and key.dtype == bool:\n            # use Boolean area to select indices from Index positions, as np.delete does not work with arrays\n            values = np.delete(self.values, self._index.positions[key])\n        else:\n            values = np.delete(self.values, key)\n        values.flags.writeable = False\n\n        index = self._index._drop_iloc(key)\n\n        return self.__class__(values,\n                index=index,\n                name=self._name,\n                own_index=True\n                )\n\n    def _drop_loc(self, key: GetItemKeyType) -> 'Series':\n        return self._drop_iloc(self._index.loc_to_iloc(key))\n\n    #---------------------------------------------------------------------------\n\n    def _extract_iloc_mask(self, key: GetItemKeyType) -> 'Series':\n        '''Produce a new boolean Series of the same shape, where the values selected via iloc selection are True. The `name` attribute is not propagated.\n        '''\n        mask = np.full(self.values.shape, False, dtype=bool)\n        mask[key] = True\n        mask.flags.writeable = False\n        return self.__class__(mask, index=self._index)\n\n    def _extract_loc_mask(self, key: GetItemKeyType) -> 'Series':\n        '''Produce a new boolean Series of the same shape, where the values selected via loc selection are True. The `name` attribute is not propagated.\n        '''\n        iloc_key = self._index.loc_to_iloc(key)\n        return self._extract_iloc_mask(key=iloc_key)\n\n    #---------------------------------------------------------------------------\n\n    def _extract_iloc_masked_array(self, key: GetItemKeyType) -> MaskedArray:\n        '''Produce a new boolean Series of the same shape, where the values selected via iloc selection are True.\n        '''\n        mask = self._extract_iloc_mask(key=key)\n        return MaskedArray(data=self.values, mask=mask.values)\n\n    def _extract_loc_masked_array(self, key: GetItemKeyType) -> MaskedArray:\n        '''Produce a new boolean Series of the same shape, where the values selected via loc selection are True.\n        '''\n        iloc_key = self._index.loc_to_iloc(key)\n        return self._extract_iloc_masked_array(key=iloc_key)\n\n    #---------------------------------------------------------------------------\n\n    def _extract_iloc_assign(self, key: GetItemKeyType) -> 'SeriesAssign':\n        return SeriesAssign(self, iloc_key=key)\n\n    def _extract_loc_assign(self, key: GetItemKeyType) -> 'SeriesAssign':\n        iloc_key = self._index.loc_to_iloc(key)\n        return SeriesAssign(self, iloc_key=iloc_key)\n\n    #---------------------------------------------------------------------------\n    # axis functions\n\n    def _axis_group_items(self, *,\n            axis: int = 0\n            ):\n        if axis != 0:\n            raise AxisInvalid(f'invalid axis {axis}')\n\n        groups, locations = array_to_groups_and_locations(self.values)\n        for idx, g in enumerate(groups):\n            selection = locations == idx\n            yield g, self._extract_iloc(selection)\n\n    def _axis_group(self, *,\n            axis: int = 0\n            ):\n        yield from (x for _, x in self._axis_group_items(axis=axis))\n\n\n    def _axis_element_items(self,\n            ) -> tp.Iterator[tp.Tuple[tp.Hashable, tp.Any]]:\n        '''Generator of index, value pairs, equivalent to Series.items(). Rpeated to have a common signature as other axis functions.\n        '''\n        yield from zip(self._index, self.values)\n\n    def _axis_element(self,\n            ) -> tp.Iterator[tp.Any]:\n        yield from self.values\n\n\n\n    def _axis_group_labels_items(self,\n            depth_level: DepthLevelSpecifier = 0,\n            ):\n\n        values = self.index.values_at_depth(depth_level)\n        group_to_tuple = values.ndim == 2\n\n        groups, locations = array_to_groups_and_locations(\n                values)\n\n        for idx, g in enumerate(groups):\n            selection = locations == idx\n            if group_to_tuple:\n                g = tuple(g)\n            yield g, self._extract_iloc(selection)\n\n    def _axis_group_labels(self,\n            depth_level: DepthLevelSpecifier = 0,\n            ):\n        yield from (x for _, x in self._axis_group_labels_items(\n                depth_level=depth_level))\n\n\n\n    def _axis_window_items(self, *,\n            size: int,\n            axis: int = 0,\n            step: int = 1,\n            window_sized: bool = True,\n            window_func: tp.Optional[AnyCallable] = None,\n            window_valid: tp.Optional[AnyCallable] = None,\n            label_shift: int = 0,\n            start_shift: int = 0,\n            size_increment: int = 0,\n            as_array: bool = False,\n            ) -> tp.Iterator[tp.Tuple[tp.Hashable, tp.Any]]:\n        '''Generator of index, processed-window pairs.\n        '''\n        yield from axis_window_items(\n                source=self,\n                axis=axis,\n                size=size,\n                step=step,\n                window_sized=window_sized,\n                window_func=window_func,\n                window_valid=window_valid,\n                label_shift=label_shift,\n                start_shift=start_shift,\n                size_increment=size_increment,\n                as_array=as_array\n                )\n\n    def _axis_window(self, *,\n            size: int,\n            axis: int = 0,\n            step: int = 1,\n            window_sized: bool = True,\n            window_func: tp.Optional[AnyCallable] = None,\n            window_valid: tp.Optional[AnyCallable] = None,\n            label_shift: int = 0,\n            start_shift: int = 0,\n            size_increment: int = 0,\n            as_array: bool = False,\n            ):\n        yield from (x for _, x in self._axis_window_items(\n                axis=axis,\n                size=size,\n                step=step,\n                window_sized=window_sized,\n                window_func=window_func,\n                window_valid=window_valid,\n                label_shift=label_shift,\n                start_shift=start_shift,\n                size_increment=size_increment,\n                as_array=as_array\n                ))\n\n\n\n    #---------------------------------------------------------------------------\n\n    @property\n    def index(self):\n        '''\n        The index instance assigned to this container.\n\n        Returns:\n            :obj:`static_frame.Index`\n        '''\n        return self._index\n\n    #---------------------------------------------------------------------------\n    # dictionary-like interface\n\n    def keys(self) -> Index:\n        '''\n        Iterator of index labels.\n\n        Returns:\n            :obj:`Iterator[Hashable]`\n        '''\n        return self._index\n\n    def __iter__(self) -> tp.Iterator[tp.Hashable]:\n        '''\n        Iterator of index labels, same as :obj:`static_frame.Series.keys`.\n\n        Returns:\n            :obj:`Iterator[Hashasble]`\n        '''\n        return self._index.__iter__()\n\n    def __contains__(self, value) -> bool:\n        '''\n        Inclusion of value in index labels.\n\n        Returns:\n            :obj:`bool`\n        '''\n        return self._index.__contains__(value)\n\n    def items(self) -> tp.Iterator[tp.Tuple[tp.Any, tp.Any]]:\n        '''Iterator of pairs of index label and value.\n\n        Returns:\n            :obj:`Iterator[Tuple[Hashable, Any]]`\n        '''\n        return zip(self._index.values, self.values)\n\n    def get(self, key: tp.Hashable, default=None) -> tp.Any:\n        '''\n        Return the value found at the index key, else the default if the key is not found.\n\n        Returns:\n            :obj:`Any`\n        '''\n        if key not in self._index:\n            return default\n        return self.__getitem__(key)\n\n    #---------------------------------------------------------------------------\n    # transformations resulting in the same dimensionality\n\n    def sort_index(self,\n            *,\n            ascending: bool = True,\n            kind: str = DEFAULT_SORT_KIND\n            ) -> 'Series':\n        '''\n        Return a new Series ordered by the sorted Index.\n\n        Args:\n            *\n            ascending: if True, values are sorted low to high\n            kind: sort algorithm\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        # argsort lets us do the sort once and reuse the results\n        if self._index.depth > 1:\n            v = self._index.values\n            order = np.lexsort([v[:, i] for i in range(v.shape[1]-1, -1, -1)])\n        else:\n            # this technique does not work when values is a 2d array\n            order = np.argsort(self._index.values, kind=kind)\n\n        if not ascending:\n            order = order[::-1]\n\n        index_values = self._index.values[order]\n        index_values.flags.writeable = False\n        index = self._index.from_labels(index_values, name=self._index._name)\n\n        values = self.values[order]\n        values.flags.writeable = False\n\n        return self.__class__(values,\n                index=index,\n                name=self._name,\n                own_index=True\n                )\n\n    def sort_values(self,\n            *,\n            ascending: bool = True,\n            kind: str = DEFAULT_SORT_KIND\n            ) -> 'Series':\n        '''\n        Return a new Series ordered by the sorted values.\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        # argsort lets us do the sort once and reuse the results\n        order = np.argsort(self.values, kind=kind)\n        if not ascending:\n            order = order[::-1]\n\n        index_values = self._index.values[order]\n        index_values.flags.writeable = False\n        index = self._index.from_labels(index_values, name=self._index._name)\n\n        values = self.values[order]\n        values.flags.writeable = False\n\n        return self.__class__(values,\n                index=index,\n                name=self._name,\n                own_index=True\n                )\n\n    def isin(self, other) -> 'Series':\n        '''\n        Return a same-sized Boolean Series that shows if the same-positioned element is in the iterable passed to the function.\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        array = isin(self.values, other)\n        return self.__class__(array, index=self._index, name=self._name)\n\n    @doc_inject(class_name='Series')\n    def clip(self, *,\n            lower=None,\n            upper=None\n            ):\n        '''{}\n\n        Args:\n            lower: value or ``Series`` to define the inclusive lower bound.\n            upper: value or ``Series`` to define the inclusive upper bound.\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        args = [lower, upper]\n        for idx, arg in enumerate(args):\n            # arg = args[idx]\n            if isinstance(arg, Series):\n                # after reindexing, strip away the index\n                # NOTE: using the bound forces going to a float type; this may not be the best approach\n                bound = -np.inf if idx == 0 else np.inf\n                args[idx] = arg.reindex(self.index).fillna(bound).values\n            elif hasattr(arg, '__iter__'):\n                raise RuntimeError('only Series are supported as iterable lower/upper arguments')\n            # assume single value otherwise, no change necessary\n\n        array = np.clip(self.values, *args)\n        array.flags.writeable = False\n        return self.__class__(array, index=self._index, name=self._name)\n\n    def transpose(self) -> 'Series':\n        '''Transpose. For a 1D immutable container, this returns a reference to self.\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        return self\n\n    @property\n    def T(self) -> 'Series':\n        '''Transpose. For a 1D immutable container, this returns a reference to self.\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        return self.transpose()\n\n    @doc_inject(selector='duplicated')\n    def duplicated(self, *,\n            exclude_first=False,\n            exclude_last=False) -> np.ndarray:\n        '''\n        Return a same-sized Boolean Series that shows True for all b values that are duplicated.\n\n        Args:\n            {exclude_first}\n            {exclude_last}\n\n        Returns:\n            :obj:`numpy.ndarray`\n        '''\n        duplicates = array_to_duplicated(self.values,\n                exclude_first=exclude_first,\n                exclude_last=exclude_last)\n        duplicates.flags.writeable = False\n        return self.__class__(duplicates, index=self._index)\n\n    @doc_inject(selector='duplicated')\n    def drop_duplicated(self, *,\n            exclude_first: bool = False,\n            exclude_last: bool = False\n            ) -> 'Series':\n        '''\n        Return a Series with duplicated values removed.\n\n        Args:\n            {exclude_first}\n            {exclude_last}\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        duplicates = array_to_duplicated(self.values,\n                exclude_first=exclude_first,\n                exclude_last=exclude_last)\n        keep = ~duplicates\n        return self.__class__(self.values[keep],\n                index=self._index[keep],\n                name=self._name\n                )\n\n    @doc_inject(select='astype')\n    def astype(self, dtype: DtypeSpecifier) -> 'Series':\n        '''\n        Return a Series with type determined by `dtype` argument. Note that for Series, this is a simple function, whereas for ``Frame``, this is an interface exposing both a callable and a getitem interface.\n\n        Args:\n            {dtype}\n\n        Returns:\n            :obj:`Series`\n        '''\n        return self.__class__(\n                self.values.astype(dtype),\n                index=self._index,\n                name=self._name\n                )\n\n    def __round__(self, decimals: int = 0) -> 'Series':\n        '''\n        Return a Series rounded to the given decimals. Negative decimals round to the left of the decimal point.\n\n        Args:\n            decimals: number of decimals to round to.\n\n        Returns:\n            :obj:`Series`\n        '''\n        return self.__class__(\n                np.round(self.values, decimals),\n                index=self._index,\n                name=self._name\n                )\n\n    def roll(self,\n            shift: int,\n            *,\n            include_index: bool = False) -> 'Series':\n        '''Return a Series with values rotated forward and wrapped around the index (with a postive shift) or backward and wrapped around the index (with a negative shift).\n\n        Args:\n            shift: Postive or negative integer shift.\n            include_index: Determine if the Index is shifted with the underlying data.\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        if shift % len(self.values):\n            values = array_shift(\n                    array=self.values,\n                    shift=shift,\n                    axis=0,\n                    wrap=True)\n            values.flags.writeable = False\n        else:\n            values = self.values\n\n        if include_index:\n            index = self._index.roll(shift=shift)\n            own_index = True\n        else:\n            index = self._index\n            own_index = False\n\n        return self.__class__(values,\n                index=index,\n                name=self._name,\n                own_index=own_index)\n\n\n    def shift(self,\n            shift: int,\n            *,\n            fill_value: tp.Any = np.nan) -> 'Series':\n        '''Return a Series with values shifted forward on the index (with a postive shift) or backward on the index (with a negative shift).\n\n        Args:\n            shift: Postive or negative integer shift.\n            fill_value: Value to be used to fill data missing after the shift.\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n\n        if shift:\n            values = array_shift(\n                    array=self.values,\n                    shift=shift,\n                    axis=0,\n                    wrap=False,\n                    fill_value=fill_value)\n            values.flags.writeable = False\n        else:\n            values = self.values\n\n        return self.__class__(values,\n                index=self._index,\n                name=self._name)\n\n\n    #---------------------------------------------------------------------------\n    # transformations resulting in changed dimensionality\n\n    @doc_inject(selector='head', class_name='Series')\n    def head(self, count: int = 5) -> 'Series':\n        '''{doc}\n\n        Args:\n            {count}\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        return self.iloc[:count]\n\n    @doc_inject(selector='tail', class_name='Series')\n    def tail(self, count: int = 5) -> 'Series':\n        '''{doc}\n\n        Args:\n            {count}\n\n        Returns:\n            :obj:`static_frame.Series`\n        '''\n        return self.iloc[-count:]\n\n    @doc_inject(selector='argminmax')\n    def loc_min(self, *,\n            skipna: bool = True\n            ) -> tp.Hashable:\n        '''\n        Return the label corresponding to the minimum value found.\n\n        Args:\n            {skipna}\n\n        Returns:\n            tp.Hashable\n        '''\n        # if skipna is False and a NaN is returned, this will raise\n        post = argmin_1d(self.values, skipna=skipna)\n        if isinstance(post, FLOAT_TYPES): # NaN was returned\n            raise RuntimeError('cannot produce loc representation from NaN')\n        return self.index[post]\n\n    @doc_inject(selector='argminmax')\n    def iloc_min(self, *,\n            skipna: bool = True,\n            ) -> int:\n        '''\n        Return the integer index corresponding to the minimum value found.\n\n        Args:\n            {skipna}\n\n        Returns:\n            int\n        '''\n        return argmin_1d(self.values, skipna=skipna)\n\n    @doc_inject(selector='argminmax')\n    def loc_max(self, *,\n            skipna: bool = True,\n            ) -> tp.Hashable:\n        '''\n        Return the label corresponding to the maximum value found.\n\n        Args:\n            {skipna}\n\n        Returns:\n            tp.Hashable\n        '''\n        post = argmax_1d(self.values, skipna=skipna)\n        if isinstance(post, FLOAT_TYPES): # NaN was returned\n            raise RuntimeError('cannot produce loc representation from NaN')\n        return self.index[post]\n\n    @doc_inject(selector='argminmax')\n    def iloc_max(self, *,\n                skipna: bool = True,\n                ) -> int:\n        '''\n        Return the integer index corresponding to the maximum value.\n\n        Args:\n            {skipna}\n\n        Returns:\n            int\n        '''\n        return argmax_1d(self.values, skipna=skipna)\n\n    #---------------------------------------------------------------------------\n    def _insert(self,\n            key: int, # iloc positions\n            container: 'Series',\n            ) -> 'Series':\n        if not isinstance(container, Series):\n            raise NotImplementedError(\n                    f'No support for inserting with {type(container)}')\n\n        if not len(container.index): # must be empty data, empty index container\n            return self\n\n        dtype = resolve_dtype(self.values.dtype, container.dtype)\n        values = np.empty(len(self) + len(container), dtype=dtype)\n        key_end = key + len(container)\n\n        values_prior = self.values\n\n        values[:key] = values_prior[:key]\n        values[key: key_end] = container.values\n        values[key_end:] = values_prior[key:]\n        values.flags.writeable = False\n\n        labels_prior = self._index.values\n\n        index = self._index.__class__.from_labels(chain(\n                labels_prior[:key],\n                container._index.__iter__(),\n                labels_prior[key:],\n                ))\n\n        return self.__class__(values,\n                index=index,\n                name=self._name,\n                own_index=True,\n                )\n\n    @doc_inject(selector='insert')\n    def insert_before(self,\n            key: tp.Hashable,\n            container: 'Series',\n            ) -> 'Series':\n        '''\n        Create a new :obj:`Series` by inserting a :obj:`Series` at the position before the label specified by ``key``.\n\n        Args:\n            {key_before}\n            {container}\n\n        Returns:\n            :obj:`Series`\n        '''\n        iloc_key = self._index.loc_to_iloc(key)\n        if not isinstance(iloc_key, INT_TYPES):\n            raise RuntimeError(f'Unsupported key type: {key}')\n        return self._insert(iloc_key, container)\n\n    @doc_inject(selector='insert')\n    def insert_after(self,\n            key: tp.Hashable, # iloc positions\n            container: 'Series',\n            ) -> 'Series':\n        '''\n        Create a new :obj:`Series` by inserting a :obj:`Series` at the position after the label specified by ``key``.\n\n        Args:\n            {key_after}\n            {container}\n\n        Returns:\n            :obj:`Series`\n        '''\n        iloc_key = self._index.loc_to_iloc(key)\n        if not isinstance(iloc_key, INT_TYPES):\n            raise RuntimeError(f'Unsupported key type: {key}')\n        return self._insert(iloc_key + 1, container)\n\n\n\n\n\n    #---------------------------------------------------------------------------\n    # utility function to numpy array or other types\n\n    def unique(self) -> np.ndarray:\n        '''\n        Return a NumPy array of unqiue values.\n\n        Returns:\n            :obj:`numpy.ndarray`\n        '''\n        return ufunc_unique(self.values)\n\n    @doc_inject()\n    def equals(self,\n            other: tp.Any,\n            *,\n            compare_name: bool = False,\n            compare_dtype: bool = False,\n            compare_class: bool = False,\n            skipna: bool = True,\n            ) -> bool:\n        '''\n        {doc}\n\n        Args:\n            {compare_name}\n            {compare_dtype}\n            {compare_class}\n            {skipna}\n        '''\n        if id(other) == id(self):\n            return True\n\n        # NOTE: there are presently no Series subclasses, but better to be consistent\n        if compare_class and self.__class__ != other.__class__:\n            return False\n        elif not isinstance(other, Series):\n            return False\n\n        if len(self.values) != len(other.values):\n            return False\n        if compare_name and self._name != other._name:\n            return False\n        if compare_dtype and self.values.dtype != other.values.dtype:\n            return False\n\n        eq = self.values == other.values\n\n        # NOTE: will only be False, or an array\n        if eq is False:\n            return eq\n\n        if skipna:\n            isna_both = (isna_array(self.values, include_none=False) &\n                    isna_array(other.values, include_none=False))\n            eq[isna_both] = True\n\n        if not eq.all():\n            return False\n\n        return self._index.equals(other._index,\n                compare_name=compare_name,\n                compare_dtype=compare_dtype,\n                compare_class=compare_class,\n                skipna=skipna,\n                )\n\n    #---------------------------------------------------------------------------\n    # export\n\n    def to_pairs(self) -> tp.Iterable[tp.Tuple[tp.Hashable, tp.Any]]:\n        '''\n        Return a tuple of tuples, where each inner tuple is a pair of index label, value.\n\n        Returns:\n            tp.Iterable[tp.Tuple[tp.Hashable, tp.Any]]\n        '''\n        if isinstance(self._index, IndexHierarchy):\n            index_values = list(array2d_to_tuples(self._index.values))\n        else:\n            index_values = self._index.values\n\n        return tuple(zip(index_values, self.values))\n\n\n\n    def _to_frame(self,\n            constructor: tp.Type[ContainerOperand],\n            axis: int = 1\n            ):\n        '''\n        Common Frame construction utilities.\n        '''\n        from static_frame import TypeBlocks\n\n        if axis == 1:\n            # present as a column\n            def block_gen():\n                yield self.values\n\n            index = self._index\n            own_index = True\n            columns = None if self._name is None else (self._name,)\n            own_columns = False\n        elif axis == 0:\n            def block_gen():\n                yield self.values.reshape((1, self.values.shape[0]))\n\n            index = None if self._name is None else (self._name,)\n            own_index = False\n            columns = self._index\n            # if column constuctor is static, we can own the static index\n            own_columns = constructor._COLUMNS_CONSTRUCTOR.STATIC\n        else:\n            raise NotImplementedError(f'no handling for axis {axis}')\n\n        return constructor(\n                TypeBlocks.from_blocks(block_gen()),\n                index=index,\n                columns=columns,\n                own_data=True,\n                own_index=own_index,\n                own_columns=own_columns,\n                )\n\n\n    def to_frame(self, axis: int = 1) -> 'Frame':\n        '''\n        Return a :obj:`static_frame.Frame` view of this :obj:`static_frame.Series`. As underlying data is immutable, this is a no-copy operation.\n\n        Returns:\n            :obj:`static_frame.Frame`\n        '''\n        from static_frame import Frame\n        return self._to_frame(constructor=Frame, axis=axis)\n\n    def to_frame_go(self, axis: int = 1) -> 'FrameGO':\n        '''\n        Return :obj:`static_frame.FrameGO` view of this :obj:`static_frame.Series`. As underlying data is immutable, this is a no-copy operation.\n\n        Returns:\n            :obj:`static_frame.FrameGO`\n        '''\n        from static_frame import FrameGO\n        return self._to_frame(constructor=FrameGO, axis=axis)\n\n    def to_pandas(self) -> 'pd.Series':\n        '''\n        Return a Pandas Series.\n\n        Returns:\n            :obj:`pandas.Series`\n        '''\n        import pandas\n        return pandas.Series(self.values.copy(),\n                index=self._index.to_pandas(),\n                name=self._name)\n\n    @doc_inject(class_name='Series')\n    def to_html(self,\n            config: tp.Optional[DisplayConfig] = None\n            ):\n        '''\n        {}\n        '''\n        config = config or DisplayActive.get(type_show=False)\n        config = config.to_display_config(\n                display_format=DisplayFormats.HTML_TABLE,\n                )\n        return repr(self.display(config))\n\n    @doc_inject(class_name='Series')\n    def to_html_datatables(self,\n            fp: tp.Optional[PathSpecifierOrFileLike] = None,\n            show: bool = True,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> str:\n        '''\n        {}\n        '''\n        config = config or DisplayActive.get(type_show=False)\n        config = config.to_display_config(\n                display_format=DisplayFormats.HTML_DATATABLES,\n                )\n        content = repr(self.display(config))\n        # path_filter applied in call\n        fp = write_optional_file(content=content, fp=fp)\n\n        if show:\n            import webbrowser #pragma: no cover\n            webbrowser.open_new_tab(fp) #pragma: no cover\n        return fp\n\n\n#-------------------------------------------------------------------------------\nclass SeriesAssign(Assign):\n    __slots__ = ('container', 'iloc_key')\n\n    def __init__(self,\n            container: Series,\n            iloc_key: GetItemKeyType\n            ) -> None:\n        self.container = container\n        self.iloc_key = iloc_key\n\n    def __call__(self,\n            value, # any possible assignment type\n            fill_value: tp.Any = np.nan\n            ):\n        '''\n        Assign the ``value`` in the position specified by the selector. The `name` attribute is propagated to the returned container.\n\n        Args:\n            value:  Value to assign, which can be a :obj:`Series`, np.ndarray, or element.\n            fill_value: If the ``value`` parameter has to be reindexed, this element will be used to fill newly created elements.\n        '''\n        if isinstance(value, Series):\n            # instead of using fill_value here, might be better to use dtype_to_na, so as to not coerce the type of the value to be assigned\n            value = self.container._reindex_other_like_iloc(value,\n                    self.iloc_key,\n                    fill_value=fill_value).values\n\n        if isinstance(value, np.ndarray):\n            value_dtype = value.dtype\n        else:\n            value_dtype = np.array(value).dtype\n\n        dtype = resolve_dtype(self.container.dtype, value_dtype)\n\n        # create or copy the array to return\n        if dtype == self.container.dtype:\n            array = self.container.values.copy()\n        else:\n            array = self.container.values.astype(dtype)\n\n        array[self.iloc_key] = value\n        array.flags.writeable = False\n\n        return self.container.__class__(array,\n                index=self.container._index,\n                name=self.container._name)\n"""
static_frame/core/store.py,5,"b""\n\nimport typing as tp\nimport os\n\n\n\nfrom itertools import chain\nfrom functools import partial\nfrom functools import wraps\nimport numpy as np\n\nfrom static_frame.core.util import AnyCallable\n\n\nfrom static_frame.core.frame import Frame\nfrom static_frame.core.exception import ErrorInitStore\nfrom static_frame.core.exception import ErrorInitStoreConfig\nfrom static_frame.core.exception import StoreFileMutation\n\nfrom static_frame.core.util import PathSpecifier\nfrom static_frame.core.util import path_filter\n# from static_frame.core.util import array2d_to_tuples\nfrom static_frame.core.index_hierarchy import IndexHierarchy\nfrom static_frame.core.util import DtypesSpecifier\n\n\n\n\n#-------------------------------------------------------------------------------\nclass StoreConfig:\n    index_depth: int\n    columns_depth: int\n    dtypes: DtypesSpecifier\n    include_index: bool\n    include_columns: bool\n    format_index: tp.Optional[tp.Dict[str, tp.Any]]\n    format_columns: tp.Optional[tp.Dict[str, tp.Any]]\n    merge_hierarchical_labels: bool\n\n    @classmethod\n    def from_frame(cls, frame: Frame) -> 'StoreConfig':\n        '''Derive a config from a Frame.\n        '''\n        include_index = frame.index.depth > 1 or not frame.index._map is None\n        index_depth = 0 if not include_index else frame.index.depth\n\n        include_columns = frame.columns.depth > 1 or not frame.columns._map is None\n        columns_depth = 0 if not include_columns else frame.columns.depth\n\n        return cls(\n                index_depth=index_depth,\n                columns_depth=columns_depth,\n                include_index=include_index,\n                include_columns=include_columns\n                )\n\n    __slots__ = (\n            'index_depth',\n            'columns_depth',\n            'dtypes',\n            'consolidate_blocks',\n            'include_index',\n            'include_columns',\n            'format_index',\n            'format_columns',\n            'merge_hierarchical_labels',\n            )\n\n    def __init__(self, *,\n            # constructors\n            index_depth: int = 0, # this default does not permit round trip\n            columns_depth: int = 1,\n            dtypes: DtypesSpecifier = None,\n            consolidate_blocks: bool = False,\n            # exporters\n            include_index: bool = True,\n            include_columns: bool = True,\n            # not used by all exporters\n            format_index: tp.Optional[tp.Dict[str, tp.Any]] = None,\n            format_columns: tp.Optional[tp.Dict[str, tp.Any]] = None,\n            merge_hierarchical_labels: bool = True,\n            ):\n        '''\n        Args:\n            include_index: Boolean to determine if the ``index`` is included in output.\n            include_columns: Boolean to determine if the ``columns`` is included in output.\n            format_index: dictionary of writer format specfications.\n            format_columns: dictionary of writer format specfications.\n        '''\n\n        # constructor\n        self.index_depth = index_depth\n        self.columns_depth = columns_depth\n        self.dtypes = dtypes\n        self.consolidate_blocks = consolidate_blocks\n\n        # exporter\n        self.include_index = include_index\n        self.include_columns = include_columns\n\n        self.format_index = format_index\n        self.format_columns = format_columns\n        self.merge_hierarchical_labels = merge_hierarchical_labels\n\n# NOTE: key should be tp.Optional[str], but cannot get mypy to accept\nSCMMapType = tp.Mapping[tp.Any, StoreConfig]\nSCMMapInitializer = tp.Optional[SCMMapType]\n\nStoreConfigMapInitializer = tp.Union[\n        StoreConfig,\n        SCMMapInitializer,\n        'StoreConfigMap'\n        ]\n\n\nclass StoreConfigMap:\n    '''\n    Container of one or more StoreConfig, with the optional specification of a default StoreConfig. Assumed immutable over the life of the instance.\n    '''\n    __slots__ = (\n            '_map',\n            '_default'\n            )\n\n    _DEFAULT: StoreConfig = StoreConfig()\n\n    @classmethod\n    def from_frames(cls, frames: tp.Iterable[Frame]) -> 'StoreConfigMap':\n        '''\n        Derive a config map from an iterable of Frames\n        '''\n        config_map = {f.name: StoreConfig.from_frame(f) for f in frames}\n        return cls(config_map, own_config_map=True)\n    @classmethod\n    def from_config(cls, config: StoreConfig) -> 'StoreConfigMap':\n        return cls(default=config)\n\n    @classmethod\n    def from_initializer(\n            cls,\n            initializer: StoreConfigMapInitializer\n            ) -> 'StoreConfigMap':\n        if isinstance(initializer, StoreConfig):\n            return cls.from_config(initializer)\n        if isinstance(initializer, cls):\n            # return same instance\n            return initializer\n        if initializer is None: # will get default configurtation\n            return cls()\n        assert isinstance(initializer, dict)\n        return cls(initializer)\n\n    def __init__(self,\n            config_map: SCMMapInitializer = None,\n            default: tp.Optional[StoreConfig] = None,\n            own_config_map: bool = False\n            ):\n\n        # initialize new dict and transfer to support checking Config classes\n        self._map: SCMMapType = {}\n\n        if own_config_map and config_map is not None:\n            self._map = config_map\n        elif config_map:\n            for label, config in config_map.items():\n                if not isinstance(config, self._DEFAULT.__class__):\n                    raise ErrorInitStoreConfig(\n                        f'unspported class {config}, must be {self._DEFAULT.__class__}')\n                self._map[label] = config\n\n        if default is None:\n            self._default = self._DEFAULT\n        elif not isinstance(default, StoreConfig):\n            raise ErrorInitStoreConfig(\n                f'unspported class {default}, must be {StoreConfig}')\n        else:\n            self._default = default\n\n    def __getitem__(self, key: tp.Optional[str]) -> StoreConfig:\n        return self._map.get(key, self._default)\n\n\n\n#-------------------------------------------------------------------------------\nclass Store:\n\n    _EXT: tp.FrozenSet[str]\n\n    __slots__ = (\n            '_fp',\n            '_last_modified'\n            )\n\n    def __init__(self, fp: PathSpecifier):\n        # Redefine fp variable as only string after the filter.\n        fp = tp.cast(str, path_filter(fp))\n\n        if not os.path.splitext(fp)[1] in self._EXT:\n            raise ErrorInitStore(\n                    f'file path {fp} does not match one of the required extensions: {self._EXT}')\n\n        self._fp: str = fp\n\n        self._last_modified = np.nan\n        self._mtime_update()\n\n\n    def _mtime_update(self) -> None:\n        if os.path.exists(self._fp):\n            self._last_modified = os.path.getmtime(self._fp)\n        else:\n            self._last_modified = np.nan\n\n    def _mtime_coherent(self) -> None:\n        '''Raise if a file exists at self._fp and its mtime is not as expected\n        '''\n        if os.path.exists(self._fp):\n            if os.path.getmtime(self._fp) != self._last_modified:\n                raise StoreFileMutation(f'file {self._fp} was unexpectedly changed')\n        elif not np.isnan(self._last_modified):\n            # file existed previously and we got a modification time, but now it does not exist\n            raise StoreFileMutation(f'expected file {self._fp} no longer exists')\n\n\n    #---------------------------------------------------------------------------\n    @staticmethod\n    def get_field_names_and_dtypes(*,\n            frame: Frame,\n            include_index: bool,\n            include_columns: bool,\n            force_str_names: bool = False,\n            force_brackets: bool = False\n            ) -> tp.Tuple[tp.Sequence[str], tp.Sequence[np.dtype]]:\n\n        index = frame.index\n        columns = frame.columns\n        columns_values = columns.values\n\n        if columns.depth > 1:\n            # The str() of an array produces a space-delimited representation that includes list brackets; we could trim these brackets here, but need them for SQLite usage; thus, clients will have to trim if necessary.\n            columns_values = tuple(str(c) for c in columns_values)\n\n        if not include_index:\n            dtypes = frame._blocks.dtypes\n            if include_columns:\n                field_names = columns_values\n            else: # name fields with integers?\n                field_names = range(frame._blocks.shape[1])\n        else:\n            if index.depth == 1:\n                dtypes = [index.dtype]\n            else:\n                assert isinstance(index, IndexHierarchy) # for typing\n                dtypes = index.dtypes.values.tolist()\n            # Get a list to mutate.\n            field_names = list(index.names)\n\n            # add fram dtypes tp those from index\n            dtypes.extend(frame._blocks.dtypes)\n\n            # add index names in front of column names\n            if include_columns:\n                field_names.extend(columns_values)\n            else: # name fields with integers?\n                field_names.extend(range(frame._blocks.shape[1]))\n\n        if force_str_names:\n            field_names = [str(n) for n in field_names]\n        if force_brackets:\n            def gen() -> tp.Iterator[str]:\n                for name in field_names:\n                    name = str(name)\n                    if name.startswith('[') and name.endswith(']'):\n                        yield name\n                    else:\n                        yield f'[{name}]'\n            field_names = tuple(gen())\n\n        return field_names, dtypes\n\n    @staticmethod\n    def _get_row_iterator(\n            frame: Frame,\n            include_index: bool\n            ) -> tp.Callable[[], tp.Iterator[tp.Sequence[tp.Any]]]:\n\n        if include_index:\n            index = frame._index\n            index_values = index.values\n\n            def values() -> tp.Iterator[tp.Sequence[tp.Any]]:\n                for idx, row in enumerate(frame.iter_array(axis=1)):\n                    if index.depth > 1:\n                        index_row = index_values[idx] # this is an array\n                    else:\n                        index_row = (index_values[idx],)\n                    yield tuple(chain(index_row, row))\n            return values\n\n        return partial(frame.iter_array, 1) #type: ignore\n\n    @staticmethod\n    def get_column_iterator(\n            frame: Frame,\n            include_index: bool\n            ) -> tp.Iterator[np.ndarray]:\n        if include_index:\n            index_values = frame._index.values\n            index_depth = frame._index.depth\n\n            if index_depth == 1:\n                return chain(\n                        (index_values,),\n                        frame._blocks.axis_values(0)\n                        )\n            # this approach is the same as IndexHierarchy.values_at_depth\n            return chain(\n                    (index_values[:, d] for d in range(index_depth)),\n                    frame._blocks.axis_values(0)\n                    )\n        # avoid creating a Series per column by going to blocks\n        return frame._blocks.axis_values(0)\n\n    #---------------------------------------------------------------------------\n    def read(self,\n            label: str,\n            *,\n            config: tp.Optional[StoreConfig] = None,\n            container_type: tp.Type[Frame] = Frame,\n            ) -> Frame:\n        '''Read a single Frame, given by `label`, from the Store. Return an instance of `container_type`.\n        '''\n        raise NotImplementedError()\n\n    def write(self,\n            items: tp.Iterable[tp.Tuple[str, Frame]],\n            *,\n            config: StoreConfigMapInitializer = None\n            ) -> None:\n        '''Write all ``Frames`` in the Store.\n        '''\n        raise NotImplementedError()\n\n    def labels(self, strip_ext: bool = True) -> tp.Iterator[str]:\n        raise NotImplementedError()\n\n\n\ndef store_coherent_non_write(f: AnyCallable) -> AnyCallable:\n\n    @wraps(f)\n    def wrapper(self: Store, *args: tp.Any, **kwargs: tp.Any) -> Frame:\n        '''Decprator for derived Store class implementation of reaad(), labels().\n        '''\n        self._mtime_coherent()\n        return f(self, *args, **kwargs) # type: ignore\n\n    return wrapper\n\n\ndef store_coherent_write(f: AnyCallable) -> AnyCallable:\n    '''Decorator for dervied Store classes implementation of write()\n    '''\n    @wraps(f)\n    def wrapper(self: Store, *args: tp.Any, **kwargs: tp.Any) -> tp.Any:\n        post = f(self,  *args, **kwargs)\n        self._mtime_update()\n        return post\n\n    return wrapper\n"""
static_frame/core/store_filter.py,21,"b""\nimport typing as tp\n\nimport numpy as np\n\nfrom static_frame.core.util import DTYPE_INT_KIND\nfrom static_frame.core.util import DTYPE_STR_KIND\nfrom static_frame.core.util import DTYPE_NAN_KIND\nfrom static_frame.core.util import DTYPE_NAT_KIND\nfrom static_frame.core.util import DTYPE_COMPLEX_KIND\n\n# from static_frame.core.util import DTYPE_DATETIME_KIND\nfrom static_frame.core.util import DTYPE_BOOL\nfrom static_frame.core.util import DTYPE_OBJECT\n\nfrom static_frame.core.util import FLOAT_TYPES\n# from static_frame.core.util import AnyCallable\nfrom static_frame.core.util import EMPTY_SET\n\n\nclass StoreFilter:\n    '''\n    Uitlity for defining and applying translation in stored values, as needed for XLSX and other writers.\n    '''\n\n    __slots__ = (\n            'from_nan',\n            'from_none',\n            'from_posinf',\n            'from_neginf',\n            'to_nan',\n            'to_none',\n            'to_posinf',\n            'to_neginf',\n\n            '_FLOAT_FUNC_TO_FROM',\n            '_EQUAL_FUNC_TO_FROM',\n            '_TYPE_TO_TO_SET',\n            '_TYPE_TO_TO_TUPLE',\n            )\n\n    # from type to string\n    from_nan: tp.Optional[str]\n    from_none: tp.Optional[str]\n    from_posinf: tp.Optional[str]\n    from_neginf: tp.Optional[str]\n\n    # fro string to type\n    to_nan: tp.FrozenSet[str]\n    to_none: tp.FrozenSet[str]\n    to_posinf: tp.FrozenSet[str]\n    to_neginf: tp.FrozenSet[str]\n\n    # cannot use AnyCallable here\n    _FLOAT_FUNC_TO_FROM: tp.Tuple[tp.Tuple[tp.Any, tp.Optional[str]], ...]\n    _EQUAL_FUNC_TO_FROM: tp.Tuple[tp.Tuple[tp.Any, tp.Optional[str]], ...]\n    _TYPE_TO_TO_SET: tp.Tuple[tp.Tuple[tp.Any, tp.FrozenSet[str]], ...]\n    _TYPE_TO_TO_TUPLE: tp.Tuple[tp.Tuple[tp.Any, tp.Tuple[str, ...]], ...]\n\n    def __init__(self,\n            # from type to str\n            from_nan: tp.Optional[str] = '',\n            from_none: tp.Optional[str] = 'None',\n            from_posinf: tp.Optional[str] = 'inf',\n            from_neginf: tp.Optional[str] = '-inf',\n            # str to type\n            to_nan: tp.FrozenSet[str] = frozenset(('', 'nan', 'NaN', 'NAN', 'NULL', '#N/A')),\n            to_none: tp.FrozenSet[str] = frozenset(('None',)),\n            to_posinf: tp.FrozenSet[str] = frozenset(('inf',)),\n            to_neginf: tp.FrozenSet[str] = frozenset(('-inf',)),\n            ) -> None:\n\n        self.from_nan = from_nan\n        self.from_none = from_none\n        self.from_posinf = from_posinf\n        self.from_neginf = from_neginf\n\n        self.to_nan = to_nan\n        self.to_none = to_none\n        self.to_posinf = to_posinf\n        self.to_neginf = to_neginf\n\n        # assumed faster to define these per instance than at the class level; this avoids having to use a getattr call to get a handle to the instance method, as wold be necessary if this was on th eclass\n\n\n        # None has to be handled separately\n        self._FLOAT_FUNC_TO_FROM = (\n                (np.isnan, self.from_nan),\n                (np.isposinf, self.from_posinf),\n                (np.isneginf, self.from_neginf)\n                )\n\n        # for object array processing\n        self._EQUAL_FUNC_TO_FROM = (\n                # NOTE: this using the same heuristic as util.isna_array,, which may not be the besr choice for non-standard objects\n                (lambda x: np.not_equal(x, x), self.from_nan),\n                (lambda x: np.equal(x, None), self.from_none),\n                (lambda x: np.equal(x, np.inf), self.from_posinf),\n                (lambda x: np.equal(x, -np.inf), self.from_neginf)\n                )\n\n        self._TYPE_TO_TO_SET = (\n                (np.nan, self.to_nan),\n                (None, self.to_none),\n                (np.inf, self.to_posinf),\n                (-np.inf, self.to_neginf)\n                )\n\n        # for using isin, cannot use a set, so pre-convert to tuples here\n        self._TYPE_TO_TO_TUPLE = (\n                (np.nan, tuple(self.to_nan)),\n                (None, tuple(self.to_none)),\n                (np.inf, tuple(self.to_posinf)),\n                (-np.inf, tuple(self.to_neginf)),\n                )\n\n    def from_type_filter_array(self,\n            array: np.ndarray\n            ) -> np.ndarray:\n        '''Given an array, replace types with strings\n        '''\n        kind = array.dtype.kind\n        dtype = array.dtype\n\n        if kind in DTYPE_INT_KIND or kind in DTYPE_STR_KIND or dtype == DTYPE_BOOL:\n            return array # no replacements posible\n\n        if kind in DTYPE_NAN_KIND:\n            # if all(v is None for _, v in self._FLOAT_FUNC_TO_FROM):\n            #     return array\n\n            post = None # defer creating until we have a match\n            for func, value_replace in self._FLOAT_FUNC_TO_FROM:\n                if value_replace is not None:\n                    # cannot use these ufuncs on complex array\n                    if (array.dtype.kind == DTYPE_COMPLEX_KIND\n                            and (func == np.isposinf or func == np.isneginf)):\n                        continue\n                    found = func(array)\n\n                    if found.any():\n                        if post is None:\n                            # need to store string replacements in object type\n                            post = array.astype(object) # get a copy to mutate\n                        post[found] = value_replace\n            return post if post is not None else array\n\n        if kind in DTYPE_NAT_KIND:\n            raise NotImplementedError() # np.isnat\n\n        if dtype == DTYPE_OBJECT:\n            post = None\n            for func, value_replace in self._EQUAL_FUNC_TO_FROM:\n                if value_replace is not None:\n                    found = func(array)\n                    if found.any():\n                        if post is None:\n                            post = array.copy() # get a copy to mutate\n                        post[found] = value_replace\n            return post if post is not None else array\n\n        return array\n\n    def from_type_filter_element(self,\n            value: tp.Any\n            ) -> tp.Any:\n        '''\n        Filter single values to string.\n        '''\n        # apply to all types\n        if self.from_none is not None and value is None:\n            return self.from_none\n\n        if isinstance(value, FLOAT_TYPES):\n            for func, value_replace in self._FLOAT_FUNC_TO_FROM:\n                if value_replace is not None and func(value):\n                    return value_replace\n        return value\n\n\n    def to_type_filter_array(self,\n            array: np.ndarray\n            ) -> np.ndarray:\n        '''Given an array, replace strings with types.\n        '''\n        kind = array.dtype.kind\n        dtype = array.dtype\n\n        # nothin to do with ints, floats, or bools\n        if (kind in DTYPE_INT_KIND\n                or kind in DTYPE_NAN_KIND\n                or dtype == DTYPE_BOOL\n                ):\n            return array # no replacements posible\n\n        # need to only check object or float\n        if kind in DTYPE_STR_KIND or dtype == DTYPE_OBJECT:\n            # for string types, cannot use np.equal\n            post = None\n            for value_replace, matching in self._TYPE_TO_TO_TUPLE:\n                if matching:\n                    found = np.isin(array, matching)\n                    if found.any():\n                        if post is None:\n                            post = array.astype(object) # get a copy to mutate\n                        post[found] = value_replace\n            return post if post is not None else array\n\n        return array\n\n\n    def to_type_filter_element(self,\n            value: tp.Any\n            ) -> tp.Any:\n        '''\n        Given a value wich may be an encoded string, decode into a type.\n        '''\n        if isinstance(value, str):\n            for value_replace, matching in self._TYPE_TO_TO_SET:\n                if value in matching:\n                    return value_replace\n        return value\n\n    def to_type_filter_iterable(self, iterable: tp.Iterable[tp.Any]) -> tp.Iterator[tp.Any]:\n        for value in iterable:\n            yield self.to_type_filter_element(value)\n\n\n\nSTORE_FILTER_DEFAULT = StoreFilter()\n\nSTORE_FILTER_DISABLE = StoreFilter(\n            from_nan=None,\n            from_none=None,\n            from_posinf=None,\n            from_neginf=None,\n            # str to type\n            to_nan=EMPTY_SET,\n            to_none=EMPTY_SET,\n            to_posinf=EMPTY_SET,\n            to_neginf=EMPTY_SET,\n            )\n"""
static_frame/core/store_hdf5.py,1,"b""\nimport typing as tp\n\nimport numpy as np\n\nfrom static_frame.core.frame import Frame\nfrom static_frame.core.type_blocks import TypeBlocks\n\nfrom static_frame.core.store import Store\nfrom static_frame.core.store import StoreConfigMapInitializer\nfrom static_frame.core.store import StoreConfig\nfrom static_frame.core.store import StoreConfigMap\n\n# from static_frame.core.store_filter import StoreFilter\n# from static_frame.core.store_filter import STORE_FILTER_DEFAULT\n\nfrom static_frame.core.doc_str import doc_inject\nfrom static_frame.core.util import DTYPE_STR_KIND\n\nfrom static_frame.core.store import store_coherent_non_write\nfrom static_frame.core.store import store_coherent_write\n\n\nclass StoreHDF5(Store):\n\n    _EXT: tp.FrozenSet[str] =  frozenset(('.h5', '.hdf5'))\n\n    @store_coherent_write\n    def write(self,\n            items: tp.Iterable[tp.Tuple[tp.Optional[str], Frame]],\n            *,\n            config: StoreConfigMapInitializer = None\n            # include_index: bool = True,\n            # include_columns: bool = True,\n            # store_filter: tp.Optional[StoreFilter] = STORE_FILTER_DEFAULT\n            ) -> None:\n\n        config_map = StoreConfigMap.from_initializer(config)\n\n        import tables\n\n        with tables.open_file(self._fp, mode='w') as file:\n            for label, frame in items:\n                c = config_map[label]\n\n                # should all tables be under a common group?\n                field_names, dtypes = self.get_field_names_and_dtypes(\n                        frame=frame,\n                        include_index=c.include_index,\n                        include_columns=c.include_columns\n                        )\n\n                # Must set pos to have stable position\n                description = {}\n                for i, (k, v) in enumerate(zip(field_names, dtypes)):\n                    if v == object:\n                        raise RuntimeError('cannot store object dtypes in HDF5')\n                    description[k] = tables.Col.from_dtype(v, pos=i)\n\n                table = file.create_table('/', # create off root from sring\n                        name=label,\n                        description=description,\n                        expectedrows=len(frame),\n                        )\n\n                values = self._get_row_iterator(frame=frame,\n                        include_index=c.include_index)\n                table.append(tuple(values()))\n                table.flush()\n\n\n    # @doc_inject(selector='constructor_frame')\n    # @store_coherent_non_write\n    # def read(self,\n    #         label: tp.Optional[str] = None,\n    #         *,\n    #         config: tp.Optional[StoreConfig] = None\n    #         # index_depth: int=1,\n    #         # columns_depth: int=1,\n    #         ) -> Frame:\n    #     '''\n    #     Args:\n    #         {dtypes}\n    #     '''\n    #     import tables\n\n    #     if config is None:\n    #         config = StoreConfig() # get default\n    #     if config.dtypes:\n    #         raise NotImplementedError('using config.dtypes on HDF5 not yet supported')\n\n    #     with tables.open_file(self._fp, mode='r') as file:\n    #         table = file.get_node(f'/{label}')\n\n    #         array = table.read()\n    #         array.flags.writeable = False\n\n    #         # Discover all string dtypes and replace the dtype with a generic `str` function; first element in values array is the dtype object.\n    #         dtypes =  {k: str\n    #                 for i, (k, v) in enumerate(array.dtype.fields.items())\n    #                 if v[0].kind in DTYPE_STR_KIND\n    #                 }\n    #         # this works, but does not let us pull off columns yet\n    #         f = tp.cast(Frame,\n    #                 Frame.from_structured_array(\n    #                         array,\n    #                         name=label,\n    #                         index_depth=config.index_depth,\n    #                         columns_depth=config.columns_depth,\n    #                         dtypes=dtypes,\n    #                 ))\n    #         return f\n\n\n    @doc_inject(selector='constructor_frame')\n    @store_coherent_non_write\n    def read(self,\n            label: tp.Optional[str] = None,\n            *,\n            config: tp.Optional[StoreConfig] = None,\n            container_type: tp.Type[Frame] = Frame,\n            ) -> Frame:\n        '''\n        Args:\n            {dtypes}\n        '''\n        import tables\n\n        if config is None:\n            config = StoreConfig() # get default\n        if config.dtypes:\n            raise NotImplementedError('using config.dtypes on HDF5 not yet supported')\n\n        index_depth = config.index_depth\n        columns_depth = config.columns_depth\n\n        index_arrays = []\n        columns_labels = []\n\n        with tables.open_file(self._fp, mode='r') as file:\n            table = file.get_node(f'/{label}')\n            colnames = table.cols._v_colnames\n\n            def blocks() -> tp.Iterator[np.ndarray]:\n                for col_idx, colname in enumerate(colnames):\n\n                    # can also do: table.read(field=colname)\n                    array = table.col(colname)\n\n                    if array.dtype.kind in DTYPE_STR_KIND:\n                        array = array.astype(str)\n                    array.flags.writeable = False\n\n                    if col_idx < index_depth:\n                        index_arrays.append(array)\n                        continue\n                    # only store column labels for those yielded\n                    columns_labels.append(colname)\n                    yield array\n\n            if config.consolidate_blocks:\n                data = TypeBlocks.from_blocks(TypeBlocks.consolidate_blocks(blocks()))\n            else:\n                data = TypeBlocks.from_blocks(blocks())\n\n        return container_type._from_data_index_arrays_column_labels(\n                data=data,\n                index_depth=index_depth,\n                index_arrays=index_arrays,\n                columns_depth=columns_depth,\n                columns_labels=columns_labels,\n                name=tp.cast(tp.Hashable, label) # not sure why this is necessary\n                )\n\n\n    @store_coherent_non_write\n    def labels(self, strip_ext: bool = True) -> tp.Iterator[str]:\n        '''\n        Iterator of labels.\n        '''\n        import tables\n\n        with tables.open_file(self._fp, mode='r') as file:\n            for node in file.iter_nodes(where='/',\n                    classname=tables.Table.__name__):\n                # NOTE: this is not the complete path\n                yield node.name\n"""
static_frame/core/store_sqlite.py,6,"b'\nimport sqlite3\nimport typing as tp\nfrom fractions import Fraction\n\nimport numpy as np\n\nfrom static_frame.core.frame import Frame\nfrom static_frame.core.store import Store\n\n# from static_frame.core.index_hierarchy import IndexHierarchy\n# from static_frame.core.store_filter import StoreFilter\n# from static_frame.core.store_filter import STORE_FILTER_DEFAULT\n\nfrom static_frame.core.store import StoreConfigMapInitializer\nfrom static_frame.core.store import StoreConfigMap\nfrom static_frame.core.store import StoreConfig\n\nfrom static_frame.core.doc_str import doc_inject\n\nfrom static_frame.core.util import DTYPE_INT_KIND\nfrom static_frame.core.util import DTYPE_STR_KIND\nfrom static_frame.core.util import DTYPE_NAN_KIND\nfrom static_frame.core.util import DTYPE_BOOL\n\n\nfrom static_frame.core.store import store_coherent_non_write\nfrom static_frame.core.store import store_coherent_write\n\n\nclass StoreSQLite(Store):\n\n    _EXT: tp.FrozenSet[str] =  frozenset((\'.db\', \'.sqlite\'))\n\n    # _EXT: str = \'.sqlite\'\n    _BYTES_ONE = b\'1\'\n\n    # _BYTES_NONE = b\'None\'\n    # _BYTES_NEGINF = b\'-Inf\'\n    # _BYTES_POSINF = b\'Inf\'\n\n    @staticmethod\n    def _dtype_to_affinity_type(\n            dtype: np.dtype,\n            ) -> str:\n        \'\'\'\n        Return a pair of writer function, Boolean, where Boolean denotes if replacements need be applied.\n        \'\'\'\n        kind = dtype.kind\n        if dtype == DTYPE_BOOL:\n            return \'BOOLEAN\' # maps to NUMERIC\n        elif kind in DTYPE_STR_KIND:\n            return \'TEXT\'\n        elif kind in DTYPE_INT_KIND:\n            return \'INTEGER\'\n        elif kind in DTYPE_NAN_KIND:\n            return \'REAL\'\n        return \'NONE\'\n\n    @classmethod\n    def _frame_to_table(cls,\n            *,\n            frame: Frame,\n            label: tp.Optional[str], # can be None\n            cursor: sqlite3.Cursor,\n            include_columns: bool,\n            include_index: bool,\n            # store_filter: tp.Optional[StoreFilter]\n            ) -> None:\n\n        # here we provide a row-based represerntation that is externally usable as an slqite db; an alternative approach would be to store one cell pre column, where the column iststored as as binary BLOB; see here https://stackoverflow.com/questions/18621513/python-insert-numpy-array-into-sqlite3-database\n\n        # for interface compatibility with StoreXLSX, where label can be None\n        if label is None:\n            label = \'None\'\n\n        field_names, dtypes = cls.get_field_names_and_dtypes(\n                frame=frame,\n                include_index=include_index,\n                include_columns=include_columns,\n                force_brackets=True # needed for having nu7mbers as field names\n                )\n\n        index = frame._index\n        columns = frame._columns\n\n        if not include_index:\n            create_primary_key = \'\'\n        else:\n            primary_fields = \', \'.join(field_names[:index.depth])\n            # need leading comma\n            create_primary_key = f\', PRIMARY KEY ({primary_fields})\'\n\n        field_name_to_field_type = (\n                (field, cls._dtype_to_affinity_type(dtype))\n                for field, dtype in zip(field_names, dtypes)\n                )\n\n        create_fields = \', \'.join(f\'{k} {v}\' for k, v in field_name_to_field_type)\n        create = f\'CREATE TABLE {label} ({create_fields}{create_primary_key})\'\n        cursor.execute(create)\n\n        # works for IndexHierarchy too\n        insert_fields = \', \'.join(f\'{k}\' for k in field_names)\n        insert_template = \', \'.join(\'?\' for _ in field_names)\n        insert = f\'INSERT INTO {label} ({insert_fields}) VALUES ({insert_template})\'\n\n        values = cls._get_row_iterator(frame=frame, include_index=include_index)\n        cursor.executemany(insert, values())\n\n    @store_coherent_write\n    def write(self,\n            items: tp.Iterable[tp.Tuple[tp.Optional[str], Frame]],\n            *,\n            config: StoreConfigMapInitializer = None\n            # include_index: bool = True,\n            # include_columns: bool = True,\n            # store_filter: tp.Optional[StoreFilter] = STORE_FILTER_DEFAULT\n            ) -> None:\n\n        config_map = StoreConfigMap.from_initializer(config)\n\n        # NOTE: register adapters for NP types:\n        # numpy types go in as blobs if they are not individualy converted tp python types\n        sqlite3.register_adapter(np.int64, int)\n        sqlite3.register_adapter(np.int32, int)\n        sqlite3.register_adapter(np.int16, int)\n        # common python types\n        sqlite3.register_adapter(Fraction, str)\n        sqlite3.register_adapter(complex, lambda x: f\'{x.real}:{x.imag}\')\n\n\n        # hierarchical columns might be stored as tuples\n        with sqlite3.connect(self._fp, detect_types=sqlite3.PARSE_DECLTYPES) as conn:\n            cursor = conn.cursor()\n            for label, frame in items:\n                c = config_map[label]\n\n                self._frame_to_table(frame=frame,\n                        label=label,\n                        cursor=cursor,\n                        include_columns=c.include_columns,\n                        include_index=c.include_index,\n                        # store_filter=store_filter\n                        )\n\n            conn.commit()\n            # conn.close()\n\n\n\n    @doc_inject(selector=\'constructor_frame\')\n    @store_coherent_non_write\n    def read(self,\n            label: tp.Optional[str] = None,\n            *,\n            config: tp.Optional[StoreConfig] = None,\n            container_type: tp.Type[Frame] = Frame,\n            # store_filter: tp.Optional[StoreFilter] = STORE_FILTER_DEFAULT\n            ) -> Frame:\n        \'\'\'\n        Args:\n            {dtypes}\n        \'\'\'\n        if config is None:\n            config = StoreConfig() # get default\n\n        sqlite3.register_converter(\'BOOLEAN\', lambda x: x == self._BYTES_ONE)\n\n        # def bytes_to_types(x):\n        #     if x == self._BYTES_NONE:\n        #         return None\n        #     elif x == self._BYTES_NEGINF:\n        #         return -np.inf\n        #     elif x == self._BYTES_POSINF:\n        #         return np.inf\n        #     # import ipdb; ipdb.set_trace()\n        #     return x.decode()\n            # return x\n        # sqlite3.register_converter(\'NONE\', bytes_to_types)\n\n        with sqlite3.connect(self._fp,\n                detect_types=sqlite3.PARSE_DECLTYPES\n                ) as conn:\n            # cursor = conn.cursor()\n            query = f\'SELECT * from {label}\'\n            return tp.cast(Frame, container_type.from_sql(query=query,\n                    connection=conn,\n                    index_depth=config.index_depth,\n                    columns_depth=config.columns_depth,\n                    dtypes=config.dtypes,\n                    name=label,\n                    consolidate_blocks=config.consolidate_blocks\n                    ))\n\n    @store_coherent_non_write\n    def labels(self, strip_ext: bool = True) -> tp.Iterator[str]:\n        with sqlite3.connect(self._fp) as conn:\n            cursor = conn.cursor()\n            cursor.execute(""SELECT name FROM sqlite_master WHERE type=\'table\';"")\n            for row in cursor:\n                yield row[0]\n\n'"
static_frame/core/store_xlsx.py,3,"b'\nimport typing as tp\n\nimport numpy as np\n\n# from static_frame.core.util import DtypesSpecifier\n\nfrom static_frame.core.util import DTYPE_INT_KIND\nfrom static_frame.core.util import DTYPE_STR_KIND\nfrom static_frame.core.util import DTYPE_NAN_KIND\n# from static_frame.core.util import DTYPE_DATETIME_KIND\nfrom static_frame.core.util import DTYPE_BOOL\nfrom static_frame.core.util import COMPLEX_TYPES\n\nfrom static_frame.core.util import BOOL_TYPES\nfrom static_frame.core.util import NUMERIC_TYPES\n\nfrom static_frame.core.util import DT64_S\nfrom static_frame.core.util import DT64_DAY\nfrom static_frame.core.util import AnyCallable\n\n\nfrom static_frame.core.frame import Frame\n\nfrom static_frame.core.store import Store\nfrom static_frame.core.store import StoreConfigMapInitializer\nfrom static_frame.core.store import StoreConfig\nfrom static_frame.core.store import StoreConfigMap\n\nfrom static_frame.core.store import store_coherent_non_write\nfrom static_frame.core.store import store_coherent_write\n\n\nfrom static_frame.core.store_filter import StoreFilter\nfrom static_frame.core.store_filter import STORE_FILTER_DEFAULT\n\nfrom static_frame.core.index import Index\nfrom static_frame.core.index_base import IndexBase\nfrom static_frame.core.index_hierarchy import IndexHierarchy\n\nfrom static_frame.core.doc_str import doc_inject\n\nif tp.TYPE_CHECKING:\n    from xlsxwriter.worksheet import Worksheet  # pylint: disable=W0611 #pragma: no cover\n    from xlsxwriter.workbook import Workbook  # pylint: disable=W0611 #pragma: no cover\n    from xlsxwriter.format import Format  # pylint: disable=W0611 #pragma: no cover\n\n\nMAX_XLSX_ROWS = 1048576\nMAX_XLSX_COLUMNS = 16384 #1024 on libre office\n\n\nclass StoreXLSX(Store):\n\n    _EXT: tp.FrozenSet[str] =  frozenset((\'.xlsx\',))\n\n    # _EXT: str = \'.xlsx\'\n\n    @staticmethod\n    def _dtype_to_writer_attr(\n            dtype: np.dtype,\n            ) -> tp.Tuple[str, bool]:\n        \'\'\'\n        Return a pair of writer function, Boolean, where Boolean denotes if replacements need be applied.\n        \'\'\'\n        kind = dtype.kind\n        if dtype == DT64_S or dtype == DT64_DAY:\n            return \'write_datetime\', True\n        elif dtype == DTYPE_BOOL:\n            return \'write_boolean\', False\n        elif kind in DTYPE_STR_KIND:\n            return \'write_string\', False\n        elif kind in DTYPE_INT_KIND:\n            return \'write_number\', False\n        elif kind in DTYPE_NAN_KIND:\n            return \'write_number\', True\n        return \'write\', True\n\n    @staticmethod\n    def _get_format_or_default(\n            workbook: \'Workbook\', # do not want module level import o\n            format_specifier: tp.Optional[tp.Dict[str, tp.Any]]\n            ) -> \'Format\':\n        if format_specifier:\n            return workbook.add_format(format_specifier)\n        f = workbook.add_format()\n        f.set_bold()\n        return f\n\n    @classmethod\n    def _get_writer(cls,\n            dtype: np.dtype,\n            ws: \'Worksheet\'\n            ) -> AnyCallable: # find better type\n        \'\'\'\n        Return a writer function of the passed in Worksheet.\n        \'\'\'\n        assert isinstance(dtype, np.dtype)\n\n        import xlsxwriter\n\n        writer_attr, replace_active = cls._dtype_to_writer_attr(dtype)\n        writer_native = getattr(ws, writer_attr)\n\n        def writer(\n                row: int,\n                col: int,\n                value: tp.Any,\n                cell_format: tp.Optional[xlsxwriter.format.Format] = None\n                ) -> tp.Any:\n\n            # cannot yet write complex types directly, so covert to string\n            if isinstance(value, COMPLEX_TYPES):\n                return ws.write_string(row, col, str(value), cell_format)\n\n            if writer_attr == \'write\':\n                # determine type for aach value\n                if isinstance(value, BOOL_TYPES):\n                    return ws.write_boolean(row, col, value, cell_format)\n                if isinstance(value, str):\n                    return ws.write_string(row, col, value, cell_format)\n                if isinstance(value, NUMERIC_TYPES):\n                    return ws.write_number(row, col, value, cell_format)\n\n            # use the type specific writer_native\n            return writer_native(row, col, value, cell_format)\n        return writer\n\n    @classmethod\n    def _frame_to_worksheet(cls,\n            frame: Frame,\n            ws: \'Worksheet\',\n            *,\n            include_columns: bool,\n            include_index: bool,\n            format_columns: \'Format\',\n            format_index: \'Format\',\n            merge_hierarchical_labels: bool,\n            store_filter: tp.Optional[StoreFilter]\n            ) -> None:\n\n        index_depth = frame._index.depth\n        index_depth_effective = 0 if not include_index else index_depth\n\n        columns_iter = cls.get_column_iterator(frame=frame,\n                include_index=include_index)\n\n        columns_depth = frame._columns.depth\n        columns_depth_effective = 0 if not include_columns else columns_depth\n\n        columns_total = frame.shape[1] + index_depth_effective\n        rows_total = frame.shape[0] + columns_depth_effective\n\n        if rows_total > MAX_XLSX_ROWS:\n            raise RuntimeError(f\'Frame rows do not fit into XLSX sheet ({rows_total} > {MAX_XLSX_ROWS})\')\n        if columns_total > MAX_XLSX_COLUMNS:\n            raise RuntimeError(f\'Frame columns do not fit into XLSX sheet ({columns_total} > {MAX_XLSX_COLUMNS})\')\n\n        if include_columns:\n            columns_values = frame._columns.values\n            if store_filter:\n                columns_values = store_filter.from_type_filter_array(columns_values)\n            writer_columns = cls._get_writer(columns_values.dtype, ws)\n\n        # TODO: need to determine if .name attr on index or columns should be populated in upper left corner ""dead"" zone.\n\n        # write by column\n        for col, values in enumerate(columns_iter):\n            if include_columns:\n                # The col integers will include index depth, so if including index, must wait until after index depth to write column field names; if include_index is False, can begin reading from columns_values\n                if col >= index_depth_effective:\n                    if columns_depth == 1:\n                        writer_columns(0,\n                                col,\n                                columns_values[col - index_depth_effective],\n                                format_columns)\n                    elif columns_depth > 1:\n                        for i in range(columns_depth):\n                            # here, row selection is column count, column selection is depth\n                            writer_columns(i,\n                                    col,\n                                    columns_values[col - index_depth_effective, i],\n                                    format_columns\n                                    )\n            if store_filter:\n                # thi might change the dtype\n                values = store_filter.from_type_filter_array(values)\n            writer = cls._get_writer(values.dtype, ws)\n            # start enumeration of row after the effective column depth\n            for row, v in enumerate(values, columns_depth_effective):\n                writer(row,\n                        col,\n                        v,\n                        format_index if col < index_depth_effective else None)\n\n        # post process to merge cells; need to get width of at depth\n        if include_columns and merge_hierarchical_labels and columns_depth > 1:\n            for depth in range(columns_depth - 1): # never most deep\n                row = depth\n                col = index_depth_effective # start after index\n                for label, width in frame._columns.label_widths_at_depth(depth):\n                    # TODO: use store_filter\n                    ws.merge_range(row, col, row, col + width - 1, label, format_columns)\n                    col += width\n\n        if include_index and merge_hierarchical_labels and index_depth > 1:\n            for depth in range(index_depth - 1): # never most deep\n                row = columns_depth_effective\n                col = depth\n                for label, width in frame._index.label_widths_at_depth(depth):\n                    # TODO: use store_filter\n                    ws.merge_range(row, col, row + width - 1, col, label, format_columns)\n                    row += width\n\n    @store_coherent_write\n    def write(self,\n            items: tp.Iterable[tp.Tuple[tp.Optional[str], Frame]],\n            *,\n            config: StoreConfigMapInitializer = None,\n            # include_index: bool = True,\n            # include_columns: bool = True,\n            # format_index: tp.Optional[tp.Dict[str, tp.Any]] = None,\n            # format_columns: tp.Optional[tp.Dict[str, tp.Any]] = None,\n            # merge_hierarchical_labels: bool = True,\n            store_filter: tp.Optional[StoreFilter] = STORE_FILTER_DEFAULT\n            ) -> None:\n        \'\'\'\n        Args:\n            store_filter: a dictionary of objects to string, enabling replacement of NaN and None values when writng to XLSX.\n\n        \'\'\'\n        # format_data: tp.Optional[tp.Dict[tp.Hashable, tp.Dict[str, tp.Any]]]\n        # format_data: dictionary of dictionaries, keyed by column label, that contains dictionaries of XlsxWriter format specifications.\n\n        # will create default from None, will pass let a map pass through\n        config_map = StoreConfigMap.from_initializer(config)\n\n        import xlsxwriter\n\n        wb = xlsxwriter.Workbook(self._fp)\n\n        for label, frame in items:\n            c = config_map[label]\n            format_columns = self._get_format_or_default(wb, c.format_columns)\n            format_index = self._get_format_or_default(wb, c.format_index)\n\n            ws = wb.add_worksheet(label)\n            self._frame_to_worksheet(frame,\n                    ws,\n                    format_columns=format_columns,\n                    format_index=format_index,\n                    include_index=c.include_index,\n                    include_columns=c.include_columns,\n                    merge_hierarchical_labels=c.merge_hierarchical_labels,\n                    store_filter=store_filter\n                    )\n        wb.close()\n\n    @staticmethod\n    def _load_workbook(fp: str) -> \'Workbook\':\n        import openpyxl\n        return openpyxl.load_workbook(\n                filename=fp,\n                read_only=True,\n                data_only=True\n                )\n\n    @doc_inject(selector=\'constructor_frame\')\n    @store_coherent_non_write\n    def read(self,\n            label: tp.Optional[str] = None,\n            *,\n            config: tp.Optional[StoreConfig] = None,\n            store_filter: tp.Optional[StoreFilter] = STORE_FILTER_DEFAULT,\n            container_type: tp.Type[Frame] = Frame,\n            ) -> Frame:\n        \'\'\'\n        Args:\n            label: Name of sheet to read from XLSX.\n            container_type: Type of container to be returned, either Frame or a Frame subclass\n\n        \'\'\'\n        if config is None:\n            config = StoreConfig() # get default\n\n        index_depth = config.index_depth\n        columns_depth = config.columns_depth\n\n        wb = self._load_workbook(self._fp)\n\n        if label is None:\n            ws = wb[wb.sheetnames[0]]\n            name = None # do not set to default sheet name\n        else:\n            ws = wb[label]\n            name = ws.title\n\n        if ws.max_column <= 1 or ws.max_row <= 1:\n            # https://openpyxl.readthedocs.io/en/stable/optimized.html\n            # says that some clients might not repare correct dimensions; not sure what conditions are best to show this\n            ws.calculate_dimension()\n\n        max_column = ws.max_column\n        max_row = ws.max_row\n\n        index_values: tp.List[tp.Any] = []\n        columns_values: tp.List[tp.Any] = []\n\n        data = [] # pre-size with None?\n\n        for row_count, row in enumerate(ws.iter_rows(max_row=max_row)):\n            if store_filter is None:\n                row = tuple(c.value for c in row)\n            else: # only need to filter string values, but probably too expensive to pre-check\n                row = tuple(store_filter.to_type_filter_element(c.value) for c in row)\n\n            if row_count <= columns_depth - 1:\n                if columns_depth == 1:\n                    columns_values.extend(row[index_depth:])\n                elif columns_depth > 1:\n                    # NOTE: this orientation will need to be rotated\n                    columns_values.append(row[index_depth:])\n                continue\n\n            if index_depth == 0:\n                data.append(row)\n            elif index_depth == 1:\n                index_values.append(row[0])\n                data.append(row[1:])\n            else:\n                index_values.append(row[:index_depth])\n                data.append(row[index_depth:])\n\n        wb.close()\n\n        # Trim all-empty trailing rows created from style formatting GH#146. As the wb is opened in read-only mode, reverse iterating on the wb is not an option, nor is direct row access by integer; alos, evaluating all rows on forward iteration is expensive. Instead, after collecting all the data in a list and closing the wb, reverse iterate and find rows that are all empty.\n        # NOTE: need to handle case where there are valid index values\n\n        empty_token = (None if store_filter is None\n                else store_filter.to_type_filter_element(None))\n\n        for row_count in range(len(data) - 1, -2, -1):\n            if row_count < 0:\n                break\n            if any(c != empty_token for c in data[row_count]): # try to break early with any\n                break\n            if index_depth == 1 and index_values[row_count] != empty_token:\n                break\n            if index_depth > 1 and any(c != empty_token for c in index_values[row_count]):\n                break\n\n        # row_count is set to the first row that has data or index; can be -1\n        empty_row_idx = row_count + 1 # index of all-empty row\n        if empty_row_idx != len(data):\n            # trim data and index_values, if index_depth > 0\n            data = data[:empty_row_idx]\n            if index_depth > 0:\n                index_values = index_values[:empty_row_idx]\n\n        # continue with Index and Frame creation\n        index: tp.Optional[IndexBase] = None\n        own_index = False\n        if index_depth == 1:\n            index = Index(index_values)\n            own_index = True\n        elif index_depth > 1:\n            index = IndexHierarchy.from_labels(\n                    index_values,\n                    continuation_token=None\n                    )\n            own_index = True\n\n        columns: tp.Optional[IndexBase] = None\n        own_columns = False\n        if columns_depth == 1:\n            columns = container_type._COLUMNS_CONSTRUCTOR(columns_values)\n            own_columns = True\n        elif columns_depth > 1:\n            columns = container_type._COLUMNS_HIERARCHY_CONSTRUCTOR.from_labels(\n                    zip(*columns_values),\n                    continuation_token=None\n                    )\n            own_columns = True\n\n        # NOTE: this might be a Frame or a FrameGO\n        return tp.cast(Frame, container_type.from_records(data,\n                        index=index,\n                        columns=columns,\n                        dtypes=config.dtypes,\n                        own_index=own_index,\n                        own_columns=own_columns,\n                        name=name,\n                        consolidate_blocks=config.consolidate_blocks\n                        ))\n\n    @store_coherent_non_write\n    def labels(self, strip_ext: bool = True) -> tp.Iterator[str]:\n        wb = self._load_workbook(self._fp)\n        labels = tuple(wb.sheetnames)\n        wb.close()\n        yield from labels\n\n\n# p q I I II II\n# r s A B A  B\n# 1 A 0 0 0  0\n# 1 B 0 0 0  0\n'"
static_frame/core/store_zip.py,0,"b""import typing as tp\nimport zipfile\nimport pickle\nfrom io import StringIO\n\nfrom static_frame.core.util import AnyCallable\nfrom static_frame.core.store import Store\nfrom static_frame.core.store import StoreConfig\nfrom static_frame.core.store import StoreConfigMapInitializer\nfrom static_frame.core.store import StoreConfigMap\n\nfrom static_frame.core.store import store_coherent_non_write\nfrom static_frame.core.store import store_coherent_write\n\nfrom static_frame.core.frame import Frame\nfrom static_frame.core.frame import FrameGO\n\nfrom static_frame.core.exception import ErrorInitStore\n\nclass _StoreZip(Store):\n\n    _EXT: tp.FrozenSet[str] = frozenset(('.zip',))\n    _EXT_CONTAINED: str = ''\n\n    @store_coherent_non_write\n    def labels(self, strip_ext: bool = True) -> tp.Iterator[str]:\n        with zipfile.ZipFile(self._fp) as zf:\n            for name in zf.namelist():\n                if strip_ext:\n                    yield name.replace(self._EXT_CONTAINED, '')\n                else:\n                    yield name\n\nclass _StoreZipDelimited(_StoreZip):\n    # store attribute of passed-in container_type to use for construction\n    _EXPORTER: AnyCallable\n    _CONSTRUCTOR_ATTR: str\n\n    @store_coherent_non_write\n    def read(self,\n            label: str,\n            config: tp.Optional[StoreConfig] = None,\n            container_type: tp.Type[Frame] = Frame,\n            ) -> Frame:\n\n        if config is None:\n            raise ErrorInitStore('a StoreConfig is required on delimited Stores')\n\n        # NOTE: labels need to be strings\n        with zipfile.ZipFile(self._fp) as zf:\n            src = StringIO()\n            # labels may not be present\n            src.write(zf.read(label + self._EXT_CONTAINED).decode())\n            src.seek(0)\n            # call from class to explicitly pass self as frame\n            constructor = getattr(container_type, self._CONSTRUCTOR_ATTR)\n            return tp.cast(Frame, constructor(src,\n                    index_depth=config.index_depth,\n                    columns_depth=config.columns_depth,\n                    dtypes=config.dtypes,\n                    name=label,\n                    consolidate_blocks=config.consolidate_blocks\n                    ))\n\n    @store_coherent_write\n    def write(self,\n            items: tp.Iterable[tp.Tuple[str, Frame]],\n            config: StoreConfigMapInitializer = None\n            ) -> None:\n\n        # will create default from None, will pass let a map pass through\n        config_map = StoreConfigMap.from_initializer(config)\n\n        with zipfile.ZipFile(self._fp, 'w', zipfile.ZIP_DEFLATED) as zf:\n            for label, frame in items:\n                c = config_map[label]\n                dst = StringIO()\n                # call from class to explicitly pass self as frame\n                self.__class__._EXPORTER(frame,\n                        dst,\n                        include_index=c.include_index,\n                        include_columns=c.include_columns\n                        )\n                dst.seek(0)\n                # this will write it without a container\n                zf.writestr(label + self._EXT_CONTAINED, dst.read())\n\n\nclass StoreZipTSV(_StoreZipDelimited):\n    '''\n    Store of TSV files contained within a ZIP file.\n    '''\n    _EXT_CONTAINED = '.txt'\n    _EXPORTER = Frame.to_tsv\n    _CONSTRUCTOR_ATTR = Frame.from_tsv.__name__\n\nclass StoreZipCSV(_StoreZipDelimited):\n    '''\n    Store of CSV files contained within a ZIP file.\n    '''\n    _EXT_CONTAINED = '.csv'\n    _EXPORTER = Frame.to_csv\n    _CONSTRUCTOR_ATTR = Frame.from_csv.__name__\n\n\n#-------------------------------------------------------------------------------\n\nclass StoreZipPickle(_StoreZip):\n    '''A zip of pickles, permitting incremental loading of Frames.\n    '''\n\n    _EXT_CONTAINED = '.pickle'\n\n    @store_coherent_non_write\n    def read(self,\n            label: str,\n            *,\n            config: tp.Optional[StoreConfig] = None,\n            container_type: tp.Type[Frame] = Frame,\n            ) -> Frame:\n        # config does not do anything for pickles\n        # if config is not None:\n        #     raise ErrorInitStore('cannot use a StoreConfig on pickled Stores')\n\n        with zipfile.ZipFile(self._fp) as zf:\n            frame = pickle.loads(zf.read(label + self._EXT_CONTAINED))\n\n            # assume the stored frame is not a FrameGO\n            if issubclass(container_type, FrameGO):\n                frame = frame.to_frame_go()\n\n            return tp.cast(Frame, frame)\n\n    @store_coherent_write\n    def write(self,\n            items: tp.Iterable[tp.Tuple[str, Frame]],\n            *,\n            config: StoreConfigMapInitializer = None\n            ) -> None:\n\n        # if config is not None:\n        #     raise ErrorInitStore('cannot use a StoreConfig on pickled Stores')\n\n        with zipfile.ZipFile(self._fp, 'w', zipfile.ZIP_DEFLATED) as zf:\n            for label, frame in items:\n                if isinstance(frame, FrameGO):\n                    raise NotImplementedError('convert FrameGO to Frame before pickling.')\n                zf.writestr(label + self._EXT_CONTAINED, pickle.dumps(frame))\n\n\n\n"""
static_frame/core/type_blocks.py,135,"b""\n\nimport typing as tp\n\nfrom itertools import zip_longest\nfrom itertools import chain\nfrom functools import partial\n# from collections import deque\nimport numpy as np\n\n\nfrom static_frame.core.util import NULL_SLICE\nfrom static_frame.core.util import UNIT_SLICE\nfrom static_frame.core.util import DTYPE_OBJECT\n# from static_frame.core.util import EMPTY_TUPLE\nfrom static_frame.core.util import DTYPE_BOOL\n\nfrom static_frame.core.util import INT_TYPES\nfrom static_frame.core.util import KEY_ITERABLE_TYPES\nfrom static_frame.core.util import KEY_MULTIPLE_TYPES\n# from static_frame.core.util import DTYPE_INT_DEFAULT\nfrom static_frame.core.util import DTYPE_NAN_KIND\n\nfrom static_frame.core.util import GetItemKeyType\nfrom static_frame.core.util import GetItemKeyTypeCompound\nfrom static_frame.core.util import DtypeSpecifier\nfrom static_frame.core.util import UFunc\n\nfrom static_frame.core.util import row_1d_filter\nfrom static_frame.core.util import column_2d_filter\n\nfrom static_frame.core.util import mloc\nfrom static_frame.core.util import array_shift\nfrom static_frame.core.util import full_for_fill\nfrom static_frame.core.util import resolve_dtype\nfrom static_frame.core.util import resolve_dtype_iter\nfrom static_frame.core.util import dtype_to_na\nfrom static_frame.core.util import array_to_groups_and_locations\nfrom static_frame.core.util import isna_array\nfrom static_frame.core.util import slice_to_ascending_slice\nfrom static_frame.core.util import binary_transition\nfrom static_frame.core.util import ufunc_axis_skipna\nfrom static_frame.core.util import shape_filter\nfrom static_frame.core.util import array2d_to_tuples\nfrom static_frame.core.util import iterable_to_array_nd\n\n\nfrom static_frame.core.node_selector import InterfaceGetItem\n\nfrom static_frame.core.util import immutable_filter\nfrom static_frame.core.util import slices_from_targets\nfrom static_frame.core.util import FILL_VALUE_DEFAULT\n\nfrom static_frame.core.doc_str import doc_inject\n\nfrom static_frame.core.index_correspondence import IndexCorrespondence\n\nfrom static_frame.core.display import DisplayConfig\nfrom static_frame.core.display import DisplayActive\nfrom static_frame.core.display import Display\n\nfrom static_frame.core.container import ContainerOperand\nfrom static_frame.core.container_util import apply_binary_operator_blocks\n\nfrom static_frame.core.exception import ErrorInitTypeBlocks\nfrom static_frame.core.exception import AxisInvalid\n\n#-------------------------------------------------------------------------------\nclass TypeBlocks(ContainerOperand):\n    '''An ordered collection of type-heterogenous, immutable NumPy arrays, providing an external array-like interface of a single, 2D array. Used by :obj:`Frame` for core, unindexed array management.\n\n    A TypeBlocks instance can have a zero size shape (where the length of one axis is zero). Internally, when axis 0 (rows) is of size 0, we store similarly sized arrays. When axis 1 (columns) is of size 0, we do not store arrays, as such arrays do not define a type (as tyupes are defined by columns).\n    '''\n    # related to Pandas BlockManager\n    __slots__ = (\n            '_blocks',\n            '_dtypes',\n            '_index',\n            '_shape',\n            '_row_dtype',\n            'iloc',\n            )\n\n    STATIC = False\n\n    #---------------------------------------------------------------------------\n    # constructors\n\n    @classmethod\n    def from_blocks(cls,\n            raw_blocks: tp.Iterable[np.ndarray],\n            shape_reference: tp.Optional[tp.Tuple[int, int]] = None\n            ) -> 'TypeBlocks':\n        '''\n        Main constructor using iterator (or generator) of TypeBlocks; the order of the blocks defines the order of the columns contained.\n\n        It is acceptable to construct blocks with a 0-sided shape.\n\n        Args:\n            raw_blocks: iterable (generator compatible) of NDArrays, or a single NDArray.\n            shape_reference: optional argument to support cases where no blocks are found in the ``raw_blocks`` iterable, but the outer context is one with rows but no columns, or columns and no rows.\n\n        '''\n        blocks: tp.List[np.ndarray] = [] # ordered blocks\n        dtypes: tp.List[np.dtype] = [] # column position to dtype\n        index: tp.List[tp.Tuple[int, int]] = [] # columns position to blocks key\n        block_count = 0\n\n        row_count: tp.Optional[int]\n\n        # if a single block, no need to loop\n        if isinstance(raw_blocks, np.ndarray):\n            if raw_blocks.ndim > 2:\n                raise ErrorInitTypeBlocks('arrays of dimensionality greater than 2 cannot be used to create TypeBlocks')\n\n            row_count, column_count = shape_filter(raw_blocks)\n            if column_count == 0:\n                # set shape but do not store array\n                return cls(blocks=blocks,\n                        dtypes=dtypes,\n                        index=index,\n                        shape=(row_count, column_count)\n                        )\n            blocks.append(immutable_filter(raw_blocks))\n            for i in range(column_count):\n                index.append((block_count, i))\n                dtypes.append(raw_blocks.dtype)\n\n        else: # an iterable of blocks\n            row_count = None\n            column_count = 0\n\n            for block in raw_blocks:\n                if not isinstance(block, np.ndarray):\n                    raise ErrorInitTypeBlocks(f'found non array block: {block}')\n\n                if block.ndim > 2:\n                    raise ErrorInitTypeBlocks(f'cannot include array with {block.ndim} dimensions')\n\n                r, c = shape_filter(block)\n\n                # check number of rows is the same for all blocks\n                if row_count is not None and r != row_count:\n                    raise ErrorInitTypeBlocks(f'mismatched row count: {r}: {row_count}')\n                else: # assign on first\n                    row_count = r\n\n                # we keep array with 0 rows but > 0 columns, as they take type spce in the TypeBlocks object; arrays with 0 columns do not take type space and thus can be skipped entirely\n                if c == 0:\n                    continue\n\n                blocks.append(immutable_filter(block))\n\n                # store position to key of block, block columns\n                for i in range(c):\n                    index.append((block_count, i))\n                    dtypes.append(block.dtype)\n\n                column_count += c\n                block_count += 1\n\n        # blocks cam be empty\n        if row_count is None:\n            if shape_reference is not None:\n                # if columns have gone to zero, and this was created from a TB that had rows, continue to represent those rows\n                row_count = shape_reference[0]\n            else:\n                raise ErrorInitTypeBlocks('cannot derive a row_count from blocks; provide a shape reference')\n\n        return cls(\n                blocks=blocks,\n                dtypes=dtypes,\n                index=index,\n                shape=(row_count, column_count),\n                )\n\n    @classmethod\n    def from_element_items(cls,\n            items: tp.Iterable[tp.Tuple[tp.Tuple[int, ...], object]],\n            shape: tp.Tuple[int, ...],\n            dtype: np.dtype,\n            fill_value: object = FILL_VALUE_DEFAULT\n            ) -> 'TypeBlocks':\n        '''Given a generator of pairs of iloc coords and values, return a TypeBlock of the desired shape and dtype.\n        '''\n        fill_value = (fill_value if fill_value is not FILL_VALUE_DEFAULT\n                else dtype_to_na(dtype))\n\n        a = np.full(shape, fill_value=fill_value, dtype=dtype)\n        for iloc, v in items:\n            a[iloc] = v\n        a.flags.writeable = False\n        return cls.from_blocks(a)\n\n    @classmethod\n    def from_zero_size_shape(cls,\n            shape: tp.Tuple[int, int] = (0, 0)\n            ) -> 'TypeBlocks':\n        '''\n        Given a shape where one or both axis is 0 (a zero sized array), return a TypeBlocks instance.\n        '''\n        #NOTE: might want to take dtypes here, so as we can create a zero row Frame with properly defined dtypes. The challenge is that DtypesSpecifier includes column name maps, and we do not have access to an index-like map in this context.\n\n        rows, columns = shape\n\n        if not (rows == 0 or columns == 0):\n            raise RuntimeError(f'invalid shape for empty TypeBlocks: {shape}')\n\n        # as types are organized vertically, storing an array with 0 rows but > 0 columns is appropriate as it takes type space\n        if rows == 0 and columns > 0:\n            a = np.empty(shape)\n            a.flags.writeable = False\n            return cls.from_blocks(a)\n\n        # for arrays with no width, favor storing shape alone and not creating an array object; the shape will be binding for future appending\n        return cls(blocks=list(), dtypes=list(), index=list(), shape=shape)\n\n    #---------------------------------------------------------------------------\n\n    def __init__(self, *,\n            blocks: tp.List[np.ndarray],\n            dtypes: tp.List[np.dtype],\n            index: tp.List[tp.Tuple[int, int]],\n            shape: tp.Tuple[int, int]\n            ) -> None:\n        '''\n        Default constructor. We own all lists passed in to this constructor.\n\n        Args:\n            blocks: A list of one or two-dimensional NumPy arrays\n            dtypes: list of dtypes per external column\n            index: list of pairs, where the first element is the block index, the second elemetns is the intra-block column\n            shape: two-element tuple defining row and column count. A (0, 0) shape is permitted for empty TypeBlocks.\n        '''\n        self._blocks = blocks\n        self._dtypes = dtypes\n        self._index = index # list where index, as column, gets block, offset\n        self._shape = shape\n\n        if self._blocks:\n            self._row_dtype = resolve_dtype_iter(b.dtype for b in self._blocks)\n        else:\n            # NOTE: this violates the type; however, this is desirable when appending such that this value does not force an undesirable type resolution\n            self._row_dtype = None\n\n        self.iloc = InterfaceGetItem(self._extract_iloc)\n\n    #---------------------------------------------------------------------------\n    def __setstate__(self, state: tp.Tuple[object, tp.Mapping[str, tp.Any]]) -> None:\n        '''\n        Ensure that reanimated NP arrays are set not writeable.\n        '''\n        for key, value in state[1].items():\n            setattr(self, key, value)\n\n        for b in self._blocks:\n            b.flags.writeable = False\n\n    def copy(self) -> 'TypeBlocks':\n        '''\n        Return a new TypeBlocks. Underlying arrays are not copied.\n        '''\n        return self.__class__(\n                blocks=[b for b in self._blocks],\n                dtypes=self._dtypes.copy(), # list\n                index=self._index.copy(),\n                shape=self._shape)\n\n    #---------------------------------------------------------------------------\n    # new properties\n\n    @property\n    def dtypes(self) -> np.ndarray:\n        '''\n        Return an immutable array that, for each realizable column (not each block), the dtype is given.\n        '''\n        # this creates a new array every time it is called; could cache\n        a = np.array(self._dtypes, dtype=np.dtype)\n        a.flags.writeable = False\n        return a\n\n    @property\n    def shapes(self) -> np.ndarray:\n        '''\n        Return an immutable array that, for each block, reports the shape as a tuple.\n        '''\n        a = np.empty(len(self._blocks), dtype=object)\n        a[:] = [b.shape for b in self._blocks]\n        a.flags.writeable = False\n        return a\n\n\n    @property #type: ignore\n    @doc_inject()\n    def mloc(self) -> np.ndarray:\n        '''{doc_array}\n        '''\n        a = np.fromiter(\n                (mloc(b) for b in self._blocks),\n                count=len(self._blocks),\n                dtype=np.int64)\n        a.flags.writeable = False\n        return a\n\n    @property\n    def unified(self) -> bool:\n        return len(self._blocks) <= 1\n\n    #---------------------------------------------------------------------------\n    # common NP-style properties\n\n    @property\n    def shape(self) -> tp.Tuple[int, int]:\n        # make this a property so as to be immutable\n        return self._shape\n\n    @property\n    def ndim(self) -> int:\n        return 2\n\n    @property\n    def size(self) -> int:\n        return sum(b.size for b in self._blocks)\n\n    @property\n    def nbytes(self) -> int:\n        return sum(b.nbytes for b in self._blocks)\n\n    #---------------------------------------------------------------------------\n    # value extraction\n\n    @staticmethod\n    def _blocks_to_array(*,\n            blocks: tp.Sequence[np.ndarray],\n            shape: tp.Tuple[int, int],\n            row_dtype: tp.Optional[np.dtype],\n            row_multiple: bool\n            ) -> np.ndarray:\n        '''\n        Given blocks and a combined shape, return a consolidated 2D or 1D array.\n\n        Args:\n            shape: used in construting returned array; not ussed as a constraint.\n            row_multiple: if False, a single row reduces to a 1D\n        '''\n        # assume column_multiple is True, as this routine is called after handling extraction of single columns\n        if len(blocks) == 1:\n            if not row_multiple:\n                return row_1d_filter(blocks[0])\n            else:\n                return column_2d_filter(blocks[0])\n\n        # get empty array and fill parts\n        # NOTE: row_dtype may be None if an unfillable array; defaults to NP default\n        if not row_multiple:\n            # return 1 row TypeBlock as a 1D array with length equal to the number of columns\n            array = np.empty(shape[1], dtype=row_dtype)\n        else: # get ndim 2 shape array\n            array = np.empty(shape, dtype=row_dtype)\n\n        pos = 0\n        for block in blocks:\n            if block.ndim == 1:\n                end = pos + 1\n            else:\n                end = pos + block.shape[1]\n\n            if array.ndim == 1:\n                array[pos: end] = block[:] # gets a row from array\n            else:\n                if block.ndim == 1:\n                    array[:, pos] = block[:] # a 1d array\n                else:\n                    array[:, pos: end] = block[:] # gets a row / row slice from array\n            pos = end\n\n        array.flags.writeable = False\n        return array\n\n\n    @property\n    def values(self) -> np.ndarray:\n        '''Returns a consolidated NP array of the all blocks.\n        '''\n        # always return a 2D array\n        return self._blocks_to_array(\n                blocks=self._blocks,\n                shape=self._shape,\n                row_dtype=self._row_dtype,\n                row_multiple=True)\n\n    def axis_values(self,\n            axis: int = 0,\n            reverse: bool = False\n            ) -> tp.Iterator[np.ndarray]:\n        '''Generator of arrays produced along an axis. Clients can expect to get an immutable array.\n\n        Args:\n            axis: 0 iterates over columns, 1 iterates over rows\n        '''\n        if axis == 1: # iterate over rows\n            unified = self.unified\n            # iterate over rows; might be faster to create entire values\n            if not reverse:\n                row_idx_iter = range(self._shape[0])\n            else:\n                row_idx_iter = range(self._shape[0] - 1, -1, -1)\n\n            for i in row_idx_iter:\n                if unified:\n                    b = self._blocks[0]\n                    if b.ndim == 1:\n                        # single element slice to force array creation (not an element)\n                        yield b[i: i+1]\n                    else:\n                        # if a 2d array, we can yield rows through simple indexing\n                        yield b[i]\n                else:\n                    # cannot use a generator w/ np concat\n                    # use == for type comparisons\n                    parts = []\n                    for b in self._blocks:\n                        if b.ndim == 1:\n                            # get a slice to permit concatenation\n                            key: tp.Union[int, slice] = slice(i, i+1)\n                        else:\n                            key = i\n                        if b.dtype == self._row_dtype:\n                            parts.append(b[key])\n                        else:\n                            parts.append(b[key].astype(self._row_dtype))\n                    part = np.concatenate(parts)\n                    part.flags.writeable = False\n                    yield part\n\n        elif axis == 0: # iterate over columns\n            if not reverse:\n                block_column_iter: tp.Iterable[tp.Tuple[int, int]] = self._index\n            else:\n                block_column_iter = reversed(self._index)\n\n            for block_idx, column in block_column_iter:\n                b = self._blocks[block_idx]\n                if b.ndim == 1:\n                    yield b\n                else:\n                    yield b[:, column] # excpeted to be immutable\n        else:\n            raise AxisInvalid(f'no support for axis: {axis}')\n\n\n    def element_items(self) -> tp.Iterator[tp.Tuple[tp.Tuple[int, int], tp.Any]]:\n        '''\n        Generator of pairs of iloc locations, values across entire TypeBlock. Used in creating a IndexHierarchy instance from a TypeBlocks.\n        '''\n        for iloc in np.ndindex(self._shape):\n            block_idx, column = self._index[iloc[1]]\n            b = self._blocks[block_idx]\n            if b.ndim == 1:\n                yield iloc, b[iloc[0]]\n            else:\n                yield iloc, b[iloc[0], column]\n\n    #---------------------------------------------------------------------------\n    # methods for evaluating compatibility with other blocks, and reblocking\n    def _reblock_signature(self) -> tp.Iterator[tp.Tuple[np.dtype, int]]:\n        '''For anticipating if a reblock will result in a compatible block configuration for operator application, get the reblock signature, providing the dtype and size for each block without actually reblocking.\n\n        This is a generator to permit lazy pairwise comparison.\n        '''\n        group_dtype = None # store type found along contiguous blocks\n        group_cols = 0\n        for block in self._blocks:\n            if group_dtype is None: # first block of a type\n                group_dtype = block.dtype\n                if block.ndim == 1:\n                    group_cols += 1\n                else:\n                    group_cols += block.shape[1]\n                continue\n            if block.dtype != group_dtype:\n                yield (group_dtype, group_cols)\n                group_dtype = block.dtype\n                group_cols = 0\n            if block.ndim == 1:\n                group_cols += 1\n            else:\n                group_cols += block.shape[1]\n        if group_cols > 0:\n            yield (group_dtype, group_cols)\n\n    def block_compatible(self,\n            other: 'TypeBlocks',\n            axis: tp.Optional[int] = None) -> bool:\n        '''Block compatible means that the blocks are the same shape. Type is not yet included in this evaluation.\n\n        Args:\n            axis: If True, the full shape is compared; if False, only the columns width is compared.\n        '''\n        # if shape characteristics do not match, blocks cannot be compatible\n        if axis is None and self.shape != other.shape:\n            return False\n        elif axis is not None and self.shape[axis] != other.shape[axis]:\n            return False\n\n        for a, b in zip_longest(self._blocks, other._blocks, fillvalue=None):\n            if a is None or b is None:\n                return False\n            if axis is None:\n                if shape_filter(a) != shape_filter(b):\n                    return False\n            else:\n                if shape_filter(a)[axis] != shape_filter(b)[axis]:\n                    return False\n        return True\n\n    def reblock_compatible(self, other: 'TypeBlocks') -> bool:\n        '''\n        Return True if post reblocking these TypeBlocks are compatible. This only compares columns in blocks, not the entire shape.\n        '''\n        if self.shape[1] != other.shape[1]:\n            return False\n        # we only compare size, not the type\n        return not any(a is None or b is None or a[1] != b[1]\n                for a, b in zip_longest(\n                self._reblock_signature(),\n                other._reblock_signature()))\n\n    @classmethod\n    def _concatenate_blocks(cls,\n            group: tp.Iterable[np.ndarray],\n            dtype: DtypeSpecifier = None,\n            ) -> np.array:\n        '''Join blocks on axis 1, assuming the they have an appropriate dtype. This will always return a 2D array.\n        '''\n        # NOTE: if len(group) is 1, can return\n        post = np.concatenate([column_2d_filter(x) for x in group], axis=1)\n        # NOTE: if give non-native byteorder dtypes, will convert them to native\n        if dtype is not None and post.dtype != dtype:\n            # could use `out` arguement of np.concatenate to avoid copy, but would have to calculate resultant size first\n            return post.astype(dtype)\n        return post\n\n    @classmethod\n    def consolidate_blocks(cls,\n            raw_blocks: tp.Iterable[np.ndarray]) -> tp.Iterator[np.ndarray]:\n        '''\n        Generator consumer, generator producer of np.ndarray, consolidating if types are exact matches.\n        '''\n        group_dtype = None # store type found along contiguous blocks\n        group = []\n\n        for block in raw_blocks:\n            if group_dtype is None: # first block of a type\n                group_dtype = block.dtype\n                group.append(block)\n                continue\n\n            # NOTE: could be less strict and look for compatibility within dtype kind (or other compatible types)\n            if block.dtype != group_dtype:\n                # new group found, return stored\n                if len(group) == 1: # return reference without copy\n                    # NOTE: using pop() here not shown to be faster\n                    yield group[0]\n                else: # combine groups\n                    # could pre allocating and assing as necessary for large groups\n                    yield cls._concatenate_blocks(group, group_dtype)\n                group_dtype = block.dtype\n                group = [block]\n            else: # new block has same group dtype\n                group.append(block)\n\n        # always have one or more leftover\n        if group:\n            if len(group) == 1:\n                yield group[0]\n            else:\n                yield cls._concatenate_blocks(group, group_dtype)\n\n\n    def _reblock(self) -> tp.Iterator[np.ndarray]:\n        '''Generator of new block that consolidate adjacent types that are the same.\n        '''\n        yield from self.consolidate_blocks(raw_blocks=self._blocks)\n\n    def consolidate(self) -> 'TypeBlocks':\n        '''Return a new TypeBlocks that unifies all adjacent types.\n        '''\n        # note: not sure if we have a single block if we should return a new TypeBlocks instance (as done presently), or simply return self; either way, no new np arrays will be created\n        return self.from_blocks(self.consolidate_blocks(raw_blocks=self._blocks))\n\n\n    def resize_blocks(self, *,\n            index_ic: tp.Optional[IndexCorrespondence],\n            columns_ic: tp.Optional[IndexCorrespondence],\n            fill_value: tp.Any\n            ) -> tp.Iterator[np.ndarray]:\n        '''\n        Given index and column IndexCorrespondence objects, return a generator of resized blocks, extracting from self based on correspondence. Used for Frame.reindex()\n        '''\n        if columns_ic is None and index_ic is None:\n            for b in self._blocks:\n                yield b\n\n        elif columns_ic is None and index_ic is not None:\n            for b in self._blocks:\n                if index_ic.is_subset:\n                    # works for both 1d and 2s arrays\n                    yield b[index_ic.iloc_src]\n                else:\n                    shape: tp.Union[int, tp.Tuple[int, int]] = index_ic.size if b.ndim == 1 else (index_ic.size, b.shape[1])\n                    values = full_for_fill(b.dtype, shape, fill_value)\n                    if index_ic.has_common:\n                        values[index_ic.iloc_dst] = b[index_ic.iloc_src]\n                    values.flags.writeable = False\n                    yield values\n\n        elif columns_ic is not None and index_ic is None:\n            if not columns_ic.has_common:\n                # just return an empty frame; what type it shold be is not clear\n                shape = self.shape[0], columns_ic.size\n                values = full_for_fill(self._row_dtype, shape, fill_value)\n                values.flags.writeable = False\n                yield values\n            else:\n                if self.unified and columns_ic.is_subset:\n                    b = self._blocks[0]\n                    if b.ndim == 1:\n                        yield b\n                    else:\n                        yield b[:, columns_ic.iloc_src]\n                else:\n                    dst_to_src = dict(\n                            zip(\n                                    tp.cast(tp.Iterable[int], columns_ic.iloc_dst),\n                                    tp.cast(tp.Iterable[int], columns_ic.iloc_src),\n                            )\n                    )\n                    for idx in range(columns_ic.size):\n                        if idx in dst_to_src:\n                            block_idx, block_col = self._index[dst_to_src[idx]]\n                            b = self._blocks[block_idx]\n                            if b.ndim == 1:\n                                yield b\n                            else:\n                                yield b[:, block_col]\n                        else:\n                            # just get an empty position\n                            # dtype should be the same as the column replacing?\n                            values = full_for_fill(self._row_dtype,\n                                    self.shape[0],\n                                    fill_value)\n                            values.flags.writeable = False\n                            yield values\n\n        else: # both defined\n            assert columns_ic is not None and index_ic is not None\n            if not columns_ic.has_common and not index_ic.has_common:\n                # just return an empty frame; what type it shold be is not clear\n                shape = index_ic.size, columns_ic.size\n                values = full_for_fill(self._row_dtype, shape, fill_value)\n                values.flags.writeable = False\n                yield values\n            else:\n                if self.unified and index_ic.is_subset and columns_ic.is_subset:\n                    b = self._blocks[0]\n                    if b.ndim == 1:\n                        yield b[index_ic.iloc_src]\n                    else:\n                        yield b[index_ic.iloc_src_fancy(), columns_ic.iloc_src]\n                else:\n                    columns_dst_to_src = dict(\n                            zip(\n                                    tp.cast(tp.Iterable[int], columns_ic.iloc_dst),\n                                    tp.cast(tp.Iterable[int], columns_ic.iloc_src),\n                            )\n                    )\n\n                    for idx in range(columns_ic.size):\n                        if idx in columns_dst_to_src:\n                            block_idx, block_col = self._index[columns_dst_to_src[idx]]\n                            b = self._blocks[block_idx]\n\n                            if index_ic.is_subset:\n                                if b.ndim == 1:\n                                    yield b[index_ic.iloc_src]\n                                else:\n                                    yield b[index_ic.iloc_src, block_col]\n                            else: # need an empty to fill\n                                values = full_for_fill(self._row_dtype,\n                                        index_ic.size,\n                                        fill_value)\n                                if b.ndim == 1:\n                                    values[index_ic.iloc_dst] = b[index_ic.iloc_src]\n                                else:\n                                    values[index_ic.iloc_dst] = b[index_ic.iloc_src, block_col]\n                                values.flags.writeable = False\n                                yield values\n                        else:\n                            values = full_for_fill(self._row_dtype,\n                                        index_ic.size,\n                                        fill_value)\n                            values.flags.writeable = False\n                            yield values\n\n\n    def group(self,\n            axis: int,\n            key: GetItemKeyTypeCompound\n            ) -> tp.Iterator[tp.Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n        '''\n        Args:\n            key: iloc selector on opposite axis\n\n        Returns:\n            Generator of group, selection pairs, where selection is an np.ndaarray. Returned is as an np.ndarray if key is more than one column.\n        '''\n        # in worse case this will make a copy of the values extracted; this is probably still cheaper than iterating manually through rows/columns\n        unique_axis = None\n\n        if axis == 0:\n            # axis 0 means we return row groups; key is a column key\n            group_source = self._extract_array(column_key=key)\n            if group_source.ndim > 1:\n                unique_axis = 0\n        elif axis == 1:\n            # axis 1 means we return column groups; key is a row key\n            group_source = self._extract_array(row_key=key)\n            if group_source.ndim > 1 and group_source.shape[0] > 1:\n                unique_axis = 1\n        else:\n            raise AxisInvalid(f'invalid axis: {axis}')\n\n        groups, locations = array_to_groups_and_locations(\n                group_source,\n                unique_axis)\n\n        if unique_axis is not None:\n            # make the groups hashable for usage in index construction\n            if axis == 0:\n                groups = array2d_to_tuples(groups)\n            elif axis == 1:\n                groups = array2d_to_tuples(groups.T)\n\n        for idx, g in enumerate(groups):\n            selection = locations == idx\n            if axis == 0: # return row extractions\n                yield g, selection, self._extract(row_key=selection)\n            elif axis == 1: # return columns extractions\n                yield g, selection, self._extract(column_key=selection)\n\n\n    #---------------------------------------------------------------------------\n    # transformations resulting in reduced dimensionality\n\n    def ufunc_axis_skipna(self, *,\n            skipna: bool,\n            axis: int,\n            ufunc: UFunc,\n            ufunc_skipna: UFunc,\n            composable: bool,\n            dtypes: tp.Tuple[np.dtype, ...],\n            size_one_unity: bool\n            ) -> np.ndarray:\n        '''Apply a function that reduces blocks to a single axis. Note that this only works in axis 1 if the operation can be applied more than once, first by block, then by reduced blocks. This will not work for a ufunc like argmin, argmax, where the result of the function cannot be compared to the result of the function applied on a different block.\n\n        Args:\n            composable: when True, the function application will return a correct result by applying the function to blocks first, and then the result of the blocks (i.e., add, prod); where observation count is relevant (i.e., mean, var, std), this must be False.\n            dtypes: if we know the return type of func, we can provide it here to avoid having to use the row dtype.\n\n        Returns:\n            As this is a reduction of axis where the caller (a Frame) is likely to return a Series, this function is not a generator of blocks, but instead just returns a consolidated 1d array.\n        '''\n        if axis < 0 or axis > 1:\n            raise RuntimeError(f'invalid axis: {axis}')\n\n        func = partial(ufunc_axis_skipna,\n                skipna=skipna,\n                ufunc=ufunc,\n                ufunc_skipna=ufunc_skipna,\n                )\n\n        if self.unified:\n            result = func(array=column_2d_filter(self._blocks[0]), axis=axis)\n            result.flags.writeable = False\n            return result\n        else:\n            if axis == 0:\n                # reduce all rows to 1d with column width\n                shape: tp.Union[int, tp.Tuple[int, int]] = self._shape[1]\n                pos = 0\n            elif composable: # axis 1\n                # reduce all columns to 2d blocks with 1 column\n                shape = (self._shape[0], len(self._blocks))\n            else: # axis 1, not block composable\n                # Cannot do block-wise processing, must resolve to single array and return\n                array = self._blocks_to_array(\n                        blocks=self._blocks,\n                        shape=self._shape,\n                        row_dtype=self._row_dtype,\n                        row_multiple=True)\n                result = func(array=array, axis=axis)\n                result.flags.writeable = False\n                return result\n\n            # this will be uninitialzied and thus, if a value is not assigned, will have garbage\n            if dtypes:\n                # Favor self._row_dtype's kind if it is in dtypes, else take first of passed dtypes\n                for dt in dtypes:\n                    if self._row_dtype.kind == dt.kind:\n                        dtype = self._row_dtype\n                        break\n                else: # no break encountered\n                    dtype = dtypes[0]\n                astype_pre = dtype.kind in DTYPE_NAN_KIND\n            else:\n                dtype = self._row_dtype\n                astype_pre = True # if no dtypes given (like bool) we can coerce\n\n            # If dtypes were specified, we know we have specific targets in mind for output\n            out = np.empty(shape, dtype=dtype)\n            # print('out', out, out.dtype, self._row_dtype)\n            for idx, b in enumerate(self._blocks):\n\n                if astype_pre and b.dtype != dtype:\n                    b = b.astype(dtype)\n\n                if axis == 0: # Combine rows, end with columns shape.\n                    if b.size == 1 and size_one_unity and not skipna:\n                        # No function call is necessary; if skipna could turn NaN to zero.\n                        end = pos + 1\n                        # Can assign an array, even 2D, as an element if size is 1\n                        out[pos] = b\n                    elif b.ndim == 1:\n                        end = pos + 1\n                        out[pos] = func(array=b, axis=axis)\n                    else:\n                        end = pos + b.shape[1]\n                        func(array=b, axis=axis, out=out[pos: end])\n                    pos = end\n                else:\n                    # Combine columns, end with block length shape and then call func again, for final result\n                    if b.size == 1 and size_one_unity and not skipna:\n                        out[:, idx] = b\n                    elif b.ndim == 1:\n                        # if this is a composable, numeric single columns we just copy it and process it later; but if this is a logical application (and, or) then it is already Boolean\n                        if out.dtype == DTYPE_BOOL and b.dtype != DTYPE_BOOL:\n                            # making 2D with axis 0 func will result in element-wise operation\n                            out[:, idx] = func(array=column_2d_filter(b), axis=1)\n                        else: # otherwise, keep as is\n                            out[:, idx] = b\n                    else:\n                        func(array=b, axis=axis, out=out[:, idx])\n\n        if axis == 0: # nothing more to do\n            out.flags.writeable = False\n            return out\n        # If axis 1 and composable, can call function one more time on remaining components. Note that composability is problematic in cases where overflow is possible\n        result = func(array=out, axis=1)\n        result.flags.writeable = False\n        return result\n\n\n    #---------------------------------------------------------------------------\n    def __round__(self, decimals: int = 0) -> 'TypeBlocks':\n        '''\n        Return a TypeBlocks rounded to the given decimals. Negative decimals round to the left of the decimal point.\n        '''\n        func = partial(np.round, decimals=decimals)\n        # for now, we do not expose application of rounding on a subset of blocks, but is doable by setting the column_key\n        return self.__class__(\n                blocks=list(self._ufunc_blocks(column_key=NULL_SLICE, func=func)),\n                dtypes=self._dtypes.copy(), # list\n                index=self._index.copy(),\n                shape=self._shape\n                )\n\n    def __len__(self) -> int:\n        '''Length, as with NumPy and Pandas, is the number of rows. Note that A shape of (3, 0) will return a length of 3, even though there is no data.\n        '''\n        return self._shape[0]\n\n    @doc_inject()\n    def display(self,\n            config: tp.Optional[DisplayConfig] = None\n            ) -> Display:\n        '''{doc}\n\n        Args:\n            {config}\n        '''\n        # NOTE: the TypeBlocks Display is not composed into other Displays\n\n        config = config or DisplayActive.get()\n        d = None\n        outermost = True # only for the first\n        idx = 0\n        for block in self._blocks:\n            block = column_2d_filter(block)\n            if block.shape[1] == 0:\n                continue\n\n            h = '' if idx > 0 else self.__class__\n\n            display = Display.from_values(block,\n                    h,\n                    config=config,\n                    outermost=outermost)\n            if not d: # assign first\n                d = display\n                outermost = False\n            else:\n                d.extend_display(display)\n            # explicitly enumerate so as to not count no-width blocks\n            idx += 1\n\n        assert d is not None # for mypy\n        return d\n\n\n    #---------------------------------------------------------------------------\n    # extraction utilities\n\n    @staticmethod\n    def _cols_to_slice(indices: tp.Sequence[int]) -> slice:\n        '''Translate an iterable of contiguous integers into a slice. Integers are assumed to be intentionally ordered and contiguous.\n        '''\n        start_idx = indices[0]\n        # single column as a single slice\n        if len(indices) == 1:\n            return slice(start_idx, start_idx + 1)\n\n        stop_idx = indices[-1]\n        if stop_idx > start_idx: # ascending indices\n            return slice(start_idx, stop_idx + 1)\n\n        if stop_idx == 0:\n            return slice(start_idx, None, -1)\n        # stop is less than start, need to reduce by 1 to cover range\n        return slice(start_idx, stop_idx - 1, -1)\n\n\n    @classmethod\n    def _indices_to_contiguous_pairs(cls, indices: tp.Iterable[tp.Tuple[int, int]]\n        ) -> tp.Iterator[tp.Tuple[int, slice]]:\n        '''Indices are pairs of (block_idx, value); convert these to pairs of (block_idx, slice) when we identify contiguous indices within a block (these are block slices)\n\n        Args:\n            indices: can be a generator\n        '''\n        # store pairs of block idx, ascending col list\n        last = None\n        for block_idx, col in indices:\n            if not last:\n                last = (block_idx, col)\n                bundle = [col]\n                continue\n            if last[0] == block_idx and abs(col - last[1]) == 1:\n                # if contiguous, update last, add to bundle\n                last = (block_idx, col)\n                # do not need to store all col, only the last, however probably easier to just accumulate all\n                bundle.append(col)\n                continue\n            # either new block, or not contiguous on same block\n            yield (last[0], cls._cols_to_slice(bundle))\n            # start a new bundle\n            bundle = [col]\n            last = (block_idx, col)\n\n        # last can be None\n        if last and bundle:\n            yield (last[0], cls._cols_to_slice(bundle))\n\n    def _all_block_slices(self) -> tp.Iterator[tp.Tuple[int, slice]]:\n        '''\n        Alternaitve to _indices_to_contiguous_pairs when we need all indices per block in a slice.\n        '''\n        for idx, b in enumerate(self._blocks):\n            if b.ndim == 1:\n                yield (idx, UNIT_SLICE) # cannot give an integer here instead of a slice\n            else:\n                yield (idx, slice(0, b.shape[1]))\n\n    def _key_to_block_slices(self,\n            key: GetItemKeyTypeCompound,\n            retain_key_order: bool = True\n            ) -> tp.Iterator[tp.Tuple[int, tp.Union[slice, int]]]:\n        '''\n        For a column key (an integer, slice, or iterable), generate pairs of (block_idx, slice or integer) to cover all extractions. First, get the relevant index values (pairs of block id, column id), then convert those to contiguous slices.\n\n        Args:\n            retain_key_order: if False, returned slices will be in ascending order.\n\n        Returns:\n            A generator iterable of pairs, where values are block index, slice or column index\n        '''\n        if key is None or (isinstance(key, slice) and key == NULL_SLICE):\n            yield from self._all_block_slices() # slow from line profiler, 80% of this function call\n\n        else:\n            if isinstance(key, INT_TYPES):\n                # the index has the pair block, column integer\n                yield self._index[key]\n            else: # all cases where we try to get contiguous slices\n                if isinstance(key, slice):\n                    #  slice the index; null slice already handled\n                    if not retain_key_order:\n                        key = slice_to_ascending_slice(key, self._shape[1])\n                    indices: tp.Iterable[tp.Tuple[int, int]] = self._index[key]\n                elif isinstance(key, np.ndarray) and key.dtype == bool:\n                    # NOTE: if self._index was an array we could use Boolean selection directly\n                    indices = (self._index[idx] for idx, v in enumerate(key) if v)\n                elif isinstance(key, KEY_ITERABLE_TYPES):\n                    # an iterable of keys, may not have contiguous regions; provide in the order given; set as a generator; self._index is a list, not an np.array, so cannot slice self._index; requires iteration in passed generator so probably this is as fast as it can be.\n                    if retain_key_order:\n                        indices = (self._index[x] for x in key)\n                    else:\n                        indices = (self._index[x] for x in sorted(key))\n                elif key is None: # get all\n                    indices = self._index\n                else:\n                    raise NotImplementedError('Cannot handle key', key)\n                yield from self._indices_to_contiguous_pairs(indices)\n\n\n    #---------------------------------------------------------------------------\n    def _mask_blocks(self,\n            row_key: tp.Optional[GetItemKeyTypeCompound] = None,\n            column_key: tp.Optional[GetItemKeyTypeCompound] = None) -> tp.Iterator[np.ndarray]:\n        '''Return Boolean blocks of the same size and shape, where key selection sets values to True.\n        '''\n\n        # this selects the columns; but need to return all blocks\n\n        # block slices must be in ascending order, not key order\n        block_slices = iter(self._key_to_block_slices(\n                column_key,\n                retain_key_order=False))\n        target_block_idx = target_slice = None\n        targets_remain = True\n\n        for block_idx, b in enumerate(self._blocks):\n            mask = np.full(b.shape, False, dtype=bool)\n\n            while targets_remain:\n                # get target block and slice\n                if target_block_idx is None: # can be zero\n                    try:\n                        target_block_idx, target_slice = next(block_slices)\n                    except StopIteration:\n                        targets_remain = False\n                        break\n\n                if block_idx != target_block_idx:\n                    break # need to advance blocks\n\n                if b.ndim == 1: # given 1D array, our row key is all we need\n                    mask[row_key] = True\n                else:\n                    if row_key is None:\n                        mask[:, target_slice] = True\n                    else:\n                        mask[row_key, target_slice] = True\n\n                target_block_idx = target_slice = None\n\n            yield mask\n\n\n    def _astype_blocks(self,\n            column_key: GetItemKeyType,\n            dtype: DtypeSpecifier\n            ) -> tp.Iterator[np.ndarray]:\n        '''\n        Generator producer of np.ndarray.\n        '''\n        # block slices must be in ascending order, not key order\n        block_slices = iter(self._key_to_block_slices(\n                column_key,\n                retain_key_order=False))\n\n        target_slice: tp.Optional[tp.Union[slice, int]]\n\n        target_block_idx = target_slice = None\n        targets_remain = True\n\n        for block_idx, b in enumerate(self._blocks):\n            parts = []\n            part_start_last = 0\n\n            while targets_remain:\n                # get target block and slice\n                if target_block_idx is None: # can be zero\n                    try:\n                        target_block_idx, target_slice = next(block_slices)\n                    except StopIteration:\n                        targets_remain = False\n                        break\n\n                if block_idx != target_block_idx:\n                    break # need to advance blocks\n\n                if dtype == b.dtype:\n                    target_block_idx = target_slice = None\n                    continue # there may be more slices for this block\n\n                if b.ndim == 1: # given 1D array, our row key is all we need\n                    parts.append(b.astype(dtype))\n                    part_start_last = 1\n                    target_block_idx = target_slice = None\n                    break\n\n                assert target_slice is not None\n                # target_slice can be a slice or an integer\n                if isinstance(target_slice, slice):\n                    target_start = target_slice.start\n                    target_stop = target_slice.stop\n                else: # it is an integer\n                    target_start = target_slice\n                    target_stop = target_slice + 1\n\n                assert target_start is not None and target_stop is not None\n                if target_start > part_start_last:\n                    # yield un changed components before and after\n                    parts.append(b[:, slice(part_start_last, target_start)])\n\n                parts.append(b[:, target_slice].astype(dtype))\n                part_start_last = target_stop\n\n                target_block_idx = target_slice = None\n\n            # if this is a 1D block, we either convert it or do not, and thus either have parts or not, and do not need to get other part pieces of the block\n            if b.ndim != 1 and part_start_last < b.shape[1]:\n                parts.append(b[:, slice(part_start_last, None)])\n\n            if not parts:\n                yield b # no change for this block\n            else:\n                yield from parts\n\n\n    def _ufunc_blocks(self,\n            column_key: GetItemKeyType,\n            func: UFunc\n            ) -> tp.Iterator[np.ndarray]:\n        '''\n        Return a new blocks after processing each columnar block with the passed ufunc. It is assumed the ufunc will retain the shape of the input 1D or 2D array. All blocks must be processed, which is different than _astype_blocks, which can check the type and skip procesing some blocks.\n\n        Generator producer of np.ndarray.\n        '''\n        # block slices must be in ascending order, not key order\n        block_slices = iter(self._key_to_block_slices(\n                column_key,\n                retain_key_order=False))\n\n        target_slice: tp.Optional[tp.Union[slice, int]]\n\n        target_block_idx = target_slice = None\n        targets_remain = True\n\n        for block_idx, b in enumerate(self._blocks):\n            parts = []\n            part_start_last = 0\n\n            while targets_remain:\n                # get target block and slice\n                if target_block_idx is None: # can be zero\n                    try:\n                        target_block_idx, target_slice = next(block_slices)\n                    except StopIteration:\n                        targets_remain = False\n                        break\n\n                if block_idx != target_block_idx:\n                    break # need to advance blocks\n\n                if b.ndim == 1: # given 1D array, our row key is all we need\n                    parts.append(func(b))\n                    part_start_last = 1\n                    target_block_idx = target_slice = None\n                    break\n\n                assert target_slice is not None\n                # target_slice can be a slice or an integer\n                if isinstance(target_slice, slice):\n                    target_start = target_slice.start\n                    target_stop = target_slice.stop\n                else: # it is an integer\n                    target_start = target_slice\n                    target_stop = target_slice + 1\n\n                assert target_start is not None and target_stop is not None\n                if target_start > part_start_last:\n                    # yield un changed components before and after\n                    parts.append(b[:, slice(part_start_last, target_start)])\n\n                # apply func\n                parts.append(func(b[:, target_slice]))\n                part_start_last = target_stop\n\n                target_block_idx = target_slice = None\n\n            # if this is a 1D block, we either convert it or do not, and thus either have parts or not, and do not need to get other part pieces of the block\n            if b.ndim != 1 and part_start_last < b.shape[1]:\n                parts.append(b[:, slice(part_start_last, None)])\n\n            if not parts:\n                yield b # no change for this block\n            else:\n                yield from parts\n\n    def _drop_blocks(self,\n            row_key: GetItemKeyType = None,\n            column_key: GetItemKeyType = None,\n            ) -> tp.Iterator[np.ndarray]:\n        '''\n        Generator producer of np.ndarray. Note that this appraoch should be more efficient than using selection/extraction, as here we are only concerned with columns.\n\n        Args:\n            column_key: Selection of columns to leave out of blocks.\n        '''\n        if column_key is None:\n            # the default should not be the null slice, which would drop all\n            block_slices: tp.Iterator[tp.Tuple[int, tp.Union[slice, int]]] = iter(())\n        else:\n            if not self._blocks:\n                raise IndexError('cannot drop columns from zero-blocks')\n            # block slices must be in ascending order, not key order\n            block_slices = iter(self._key_to_block_slices(\n                    column_key,\n                    retain_key_order=False))\n\n        if isinstance(row_key, np.ndarray) and row_key.dtype == bool:\n            # row_key is used with np.delete, which does not support Boolean arrays; instead, convert to an array of integers\n            row_key = np.arange(len(row_key))[row_key]\n\n        target_block_idx = target_slice = None\n        targets_remain = True\n\n        for block_idx, b in enumerate(self._blocks):\n            # for each block, we evaluate if we have any targets in that block and update the block accordingly; otherwise, we yield the block unchanged\n\n            parts = []\n            drop_block = False # indicate entire block is dropped\n            part_start_last = 0 # within this block, keep track of where our last change was started\n\n            while targets_remain:\n                # get target block and slice; this is what we want to remove\n                if target_block_idx is None: # can be zero\n                    try:\n                        target_block_idx, target_slice = next(block_slices)\n                    except StopIteration:\n                        targets_remain = False\n                        break\n\n                if block_idx != target_block_idx:\n                    break # need to advance blocks\n\n                if b.ndim == 1 or b.shape[1] == 1: # given 1D array or 2D, 1 col array\n                    part_start_last = 1\n                    target_block_idx = target_slice = None\n                    drop_block = True\n                    break\n\n                # target_slice can be a slice or an integer\n                if isinstance(target_slice, slice):\n                    target_start = target_slice.start\n                    target_stop = target_slice.stop\n                else: # it is an integer\n                    target_start = target_slice # can be zero\n                    target_stop = target_slice + 1\n\n                assert target_start is not None and target_stop is not None\n                # if the target start (what we want to remove) is greater than 0 or our last starting point, then we need to slice off everything that came before, so as to keep it\n                if target_start == 0 and target_stop == b.shape[1]:\n                    drop_block = True\n                elif target_start > part_start_last:\n                    # yield retained components before and after\n                    parts.append(b[:, slice(part_start_last, target_start)])\n                part_start_last = target_stop\n                # reset target block index, forcing fetchin next target info\n                target_block_idx = target_slice = None\n\n            # if this is a 1D block we can rely on drop_block Boolean and parts list to determine action\n            if b.ndim != 1 and 0 < part_start_last < b.shape[1]:\n                # if a 2D block, and part_start_last is less than the shape, collect the remaining slice\n                parts.append(b[:, slice(part_start_last, None)])\n\n            # for row deletions, we use np.delete, which handles finding the inverse of a slice correctly; the returned array requires writeability re-set; np.delete does not work correctly with Boolean selectors\n            if not drop_block and not parts:\n                if row_key is not None:\n                    b = np.delete(b, row_key, axis=0)\n                    b.flags.writeable = False\n                yield b\n            elif parts:\n                if row_key is not None:\n                    for part in parts:\n                        part = np.delete(part, row_key, axis=0)\n                        part.flags.writeable = False\n                        yield part\n                else:\n                    yield from parts\n\n\n    def _shift_blocks(self,\n            row_shift: int = 0,\n            column_shift: int = 0,\n            wrap: bool = True,\n            fill_value: object = np.nan\n            ) -> tp.Iterator[np.ndarray]:\n        '''\n        Shift type blocks independently on rows or columns. When ``wrap`` is True, the operation is a roll-style shift; when ``wrap`` is False, shifted-out values are not replaced and are filled with ``fill_value``.\n        '''\n        row_count, column_count = self._shape\n\n        # new start index is the opposite of the shift; if shifting by 2, the new start is the second from the end\n        index_start_pos = -(column_shift % column_count)\n        row_start_pos = -(row_shift % row_count)\n\n        # possibly be truthy\n        # index is columns here\n        if wrap and index_start_pos == 0 and row_start_pos == 0:\n            yield from self._blocks\n        elif not wrap and column_shift == 0 and row_shift == 0:\n            yield from self._blocks\n        else:\n            block_start_idx, block_start_column = self._index[index_start_pos]\n            block_start = self._blocks[block_start_idx]\n\n            if block_start_column == 0:\n                # we are starting at the block, no tail, always yield;  captures all 1 dim block cases\n                block_head_iter: tp.Iterable[np.ndarray] = chain(\n                        (block_start,),\n                        self._blocks[block_start_idx + 1:])\n                block_tail_iter: tp.Iterable[np.ndarray] = self._blocks[:block_start_idx]\n            else:\n                block_head_iter = chain(\n                        (block_start[:, block_start_column:],),\n                        self._blocks[block_start_idx + 1:])\n                block_tail_iter = chain(\n                        self._blocks[:block_start_idx],\n                        (block_start[:, :block_start_column],)\n                        )\n\n            if not wrap:\n                shape = (self._shape[0], min(self._shape[1], abs(column_shift)))\n                empty = np.full(shape, fill_value)\n                if column_shift > 0:\n                    block_head_iter = (empty,)\n                elif column_shift < 0:\n                    block_tail_iter = (empty,)\n\n            # NOTE: might consider not rolling when yielding an empty array\n            for b in chain(block_head_iter, block_tail_iter):\n                if (wrap and row_start_pos == 0) or (not wrap and row_shift == 0):\n                    yield b\n                else:\n                    b = array_shift(\n                            array=b,\n                            shift=row_shift,\n                            axis=0,\n                            wrap=wrap,\n                            fill_value=fill_value)\n                    b.flags.writeable = False\n                    yield b\n\n\n    def _assign_blocks_from_keys(self,\n            row_key: tp.Optional[GetItemKeyTypeCompound] = None,\n            column_key: tp.Optional[GetItemKeyTypeCompound] = None,\n            value: object = None\n            ) -> tp.Iterator[np.ndarray]:\n        '''Assign value into all blocks, returning blocks of the same size and shape.\n\n        Args:\n            column_key: must be sorted in ascending order.\n        '''\n        if isinstance(value, np.ndarray):\n            value_dtype = value.dtype\n        else:\n            value_dtype = np.array(value).dtype\n\n        # NOTE: this requires column_key to be ordered to work; we cannot use retain_key_order=False, as the passed `value` is ordered by that key\n        target_block_slices = iter(self._key_to_block_slices(\n                column_key,\n                retain_key_order=True),\n                )\n        target_key: tp.Optional[tp.Union[int, slice]] = None\n        target_block_idx: tp.Optional[int] = None\n        targets_remain: bool = True\n        target_is_slice: bool\n        start: int\n\n        row_key_is_null_slice = row_key is None or (\n                isinstance(row_key, slice) and row_key == NULL_SLICE)\n\n        assigned_components: tp.List[np.ndarray] = []\n\n        for block_idx, b in enumerate(self._blocks):\n            assigned_components.clear()\n            assigned_stop: int = 0 # exclusive maximum\n\n            while targets_remain:\n                if target_block_idx is None: # can be zero\n                    try:\n                        target_block_idx, target_key = next(target_block_slices)\n                    except StopIteration:\n                        targets_remain = False # stop entering while loop\n                        break\n                    target_is_slice = isinstance(target_key, slice)\n\n                if block_idx != target_block_idx:\n                    break # need to advance blocks, keep targets\n\n                #---------------------------------------------------------------\n                # from here, we have at least one target we need to apply in the current block.\n\n                block_is_column = b.ndim == 1 or (b.ndim > 1 and b.shape[1] == 1)\n                start = target_key if not target_is_slice else target_key.start # type: ignore\n\n                if start > assigned_stop:\n                    # backfill, from block, from the last assigned position\n                    b_component = b[NULL_SLICE, slice(assigned_stop, start)]\n                    b_component.flags.writeable = False\n                    assigned_components.append(b_component)\n\n                # add empty components for the assignment region\n                if target_is_slice and not block_is_column:\n                    # can assume this slice has no strides\n                    t_width = target_key.stop - target_key.start # type: ignore\n                    t_shape = (b.shape[0], t_width)\n                else: # b.ndim == 1 or target is an integer: get a 1d array\n                    t_width = 1\n                    t_shape = b.shape[0]\n\n                if row_key_is_null_slice:\n                    # use value_dtype; will replace entire sub block, can be empty\n                    assigned_target = np.empty(t_shape, dtype=value_dtype)\n                else: # will need to mix types\n                    assigned_dtype = resolve_dtype(value_dtype, b.dtype)\n                    if block_is_column:\n                        if b.ndim == 1:\n                            assigned_target_pre = b\n                        else: # reshape into 1d array\n                            assigned_target_pre = b.reshape(b.shape[0])\n                    else:\n                        assigned_target_pre = b[NULL_SLICE, target_key]\n\n                    if b.dtype == assigned_dtype:\n                        assigned_target = assigned_target_pre.copy()\n                    else:\n                        assigned_target = assigned_target_pre.astype(assigned_dtype)\n\n                # add assigned_target to assigned_components below, after mutating\n                assigned_stop = start + t_width\n\n                #---------------------------------------------------------------\n                # match sliceable, when target_key is a slice (can be an element)\n                if (target_is_slice and\n                        not isinstance(value, str)\n                        and hasattr(value, '__len__')):\n                    if block_is_column:\n                        v_width = 1\n                        # if block is 1D, then we can only take 1 column if we have a 2d value\n                        value_piece_column_key: tp.Union[slice, int] = 0\n                    else:\n                        v_width = len(range(*target_key.indices(b.shape[1]))) # type: ignore\n                        # if block id 2D, can take up to v_width from value\n                        value_piece_column_key = slice(0, v_width)\n\n                    if isinstance(value, np.ndarray) and value.ndim > 1:\n                        # if value is 2D array, we want value[:, 0]\n                        value_piece = value[:, value_piece_column_key]\n                        value = value[:, slice(v_width, None)]\n                        # reassign remainder for next iteration\n                    else: # value is 1D array or tuple\n                        # we assume we assigning into a horizontal position\n                        value_piece = value[value_piece_column_key] #type: ignore\n                        value = value[slice(v_width, None)] #type: ignore\n                else: # not sliceable; this can be a single column\n                    value_piece = value\n\n                #---------------------------------------------------------------\n                # write `value` into assigned (or assigned components\n                row_target = NULL_SLICE if row_key_is_null_slice else row_key\n\n                if assigned_target.ndim == 1:\n                    assigned_target[row_target] = value_piece\n                else: # we are editing the entire assigned target sub block\n                    assigned_target[row_target, NULL_SLICE] = value_piece\n\n                assigned_target.flags.writeable = False\n                assigned_components.append(assigned_target)\n                # reset after completing assignment to get new target\n                target_block_idx = target_key = None\n\n            #-------------------------------------------------------------------\n            # all assignments for this block have been made\n\n            if assigned_components:\n                if block_is_column:\n                    # only have one sub-component and no remaining parts, do not need to get from list\n                    yield assigned_target\n                else:\n                    yield from assigned_components\n                    # get any remaining part of the block; will only have remainders if b.ndim > 1\n                    if assigned_stop < b.shape[1]:\n                        sub = b[NULL_SLICE, assigned_stop:]\n                        sub.flags.writeable = False\n                        yield sub\n            else:\n                yield b # no change\n\n    #---------------------------------------------------------------------------\n    # There are two approaches to setting values from Boolean indicators; the difference is if the Booleans are given in a single array, or in block-aligned arrays.\n\n    def _assign_blocks_from_boolean_blocks(self,\n            targets: tp.Iterable[np.ndarray],\n            value: object,\n            value_valid: tp.Optional[np.ndarray]\n            ) -> tp.Iterator[np.ndarray]:\n        '''Assign value (a single element or a matching array) into all blocks based on a Bolean arrays of shape equal to each block in these blocks, returning blocks of the same size and shape. Value is set where the Boolean is True.\n\n        Args:\n            value: Must be a single value or an array\n            value_valid: same size Boolean area to be combined with targets\n        '''\n        if isinstance(value, np.ndarray):\n            value_dtype = value.dtype\n            is_element = False\n            assert value.shape == self.shape\n            assert value_valid.shape == self.shape #type: ignore\n        else: # assumed to be non-string, non-iterable\n            value_dtype = np.array(value).dtype\n            is_element = True\n\n        start = 0\n        value_slice: tp.Union[int, slice]\n\n        for block, target in zip_longest(self._blocks, targets):\n            if block is None or target is None:\n                raise Exception('blocks or targets do not align')\n\n            if not is_element:\n                if block.ndim == 1:\n                    end = start + 1\n                    value_slice = start\n                else:\n                    end = start + block.shape[1]\n                    value_slice = slice(start, end)\n\n                # update target to valid values\n                value_valid_part = value_valid[NULL_SLICE, value_slice] #type: ignore\n                target &= value_valid_part\n                value_part = value[NULL_SLICE, value_slice][target] #type: ignore\n                start = end # always update start\n            else:\n                value_part = value\n\n            # evalaute after updating target\n            if not target.any(): # works for ndim 1 and 2\n                yield block\n\n            else:\n                assigned_dtype = resolve_dtype(value_dtype, block.dtype)\n                if block.dtype == assigned_dtype:\n                    assigned = block.copy()\n                else:\n                    assigned = block.astype(assigned_dtype)\n\n                assigned[target] = value_part\n                assigned.flags.writeable = False\n                yield assigned\n\n\n    def _assign_blocks_from_bloc_key(self,\n            bloc_key: np.ndarray,\n            value: tp.Any # an array, or element for single assigment\n            ) -> tp.Iterator[np.ndarray]:\n        '''\n        Given an Boolean array of targets, fill targets from value, where value is either a single value or an array. Unlike with _assign_blocks_from_boolean_blocks, this method takes a single block_key.\n        '''\n        if isinstance(value, np.ndarray):\n            value_dtype = value.dtype\n            is_element = False\n            assert value.shape == self.shape\n        else:\n            value_dtype = np.array(value).dtype\n            is_element = True\n\n        start = 0\n        target_slice: tp.Union[int, slice]\n\n        for block in self._blocks:\n\n            if block.ndim == 1:\n                end = start + 1\n                target_slice = start\n            else:\n                end = start + block.shape[1]\n                target_slice = slice(start, end)\n\n            target = bloc_key[NULL_SLICE, target_slice]\n\n            if not target.any():\n                yield block\n            else:\n                assigned_dtype = resolve_dtype(value_dtype, block.dtype)\n                if block.dtype == assigned_dtype:\n                    assigned = block.copy()\n                else:\n                    assigned = block.astype(assigned_dtype)\n\n                if is_element:\n                    assigned[target] = value\n                else:\n                    assigned[target] = value[NULL_SLICE, target_slice][target]\n\n                assigned.flags.writeable = False\n                yield assigned\n\n            start = end # always update start\n\n    #---------------------------------------------------------------------------\n    def _slice_blocks(self,\n            row_key: tp.Optional[GetItemKeyTypeCompound] = None,\n            column_key: tp.Optional[GetItemKeyTypeCompound] = None\n            ) -> tp.Iterator[np.ndarray]:\n        '''\n        Generator of sliced blocks, given row and column key selectors.\n        The result is suitable for passing to TypeBlocks constructor.\n        '''\n        row_key_null = (row_key is None or\n                (isinstance(row_key, slice) and row_key == NULL_SLICE))\n\n        single_row = False\n        if row_key_null:\n            if self._shape[0] == 1:\n                # this codition used to only hold if the arg is a null slice; now if None too and shape has one row\n                single_row = True\n        elif isinstance(row_key, INT_TYPES):\n            single_row = True\n        elif isinstance(row_key, slice):\n            # need to determine if there is only one index returned by range (after getting indices from the slice); do this without creating a list/tuple, or walking through the entire range; get constant time look-up of range length after uses slice.indicies\n            if len(range(*row_key.indices(self._shape[0]))) == 1:\n                single_row = True\n        elif isinstance(row_key, np.ndarray) and row_key.dtype == bool:\n            # must check this case before general iterables, below\n            if row_key.sum() == 1:\n                single_row = True\n        elif isinstance(row_key, KEY_ITERABLE_TYPES) and len(row_key) == 1:\n            # an iterable of index integers is expected here\n            single_row = True\n\n        # convert column_key into a series of block slices; we have to do this as we stride blocks; do not have to convert row_key as can use directly per block slice\n        for block_idx, slc in self._key_to_block_slices(column_key): # slow from line profiler\n            b = self._blocks[block_idx]\n            if b.ndim == 1: # given 1D array, our row key is all we need\n                if row_key_null:\n                    block_sliced = b\n                else:\n                    block_sliced = b[row_key] # slow from line profiler\n            else: # given 2D, use row key and column slice\n                if row_key_null:\n                    block_sliced = b[:, slc]\n                else:\n                    block_sliced = b[row_key, slc]\n\n            # optionally, apply additional selection, reshaping, or adjustments to what we got out of the block\n            if isinstance(block_sliced, np.ndarray):\n                # if we have a single row and the thing we sliced is 1d, we need to rotate it\n                if single_row and block_sliced.ndim == 1:\n                    block_sliced = block_sliced.reshape(1, block_sliced.shape[0])\n                # if we have a single column as 2d, unpack it; however, we have to make sure this is not a single row in a 2d\n                elif (block_sliced.ndim == 2\n                        and block_sliced.shape[0] == 1\n                        and not single_row):\n                    block_sliced = block_sliced[0]\n            else: # a single element, wrap back up in array\n                block_sliced = np.array((block_sliced,), dtype=b.dtype)\n\n            yield block_sliced\n\n\n    def _extract_array(self,\n            row_key: tp.Optional[GetItemKeyTypeCompound] = None,\n            column_key: tp.Optional[GetItemKeyTypeCompound] = None\n            ) -> np.ndarray:\n        '''Alternative extractor that returns just an np array, concatenating blocks as necessary. Used by internal clients that need to process row/column with an array.\n\n        This will be consistent with NumPy as to the dimensionality returned: if a non-multi selection is made, 1D array will be returned.\n        '''\n        # identifying column_key as integer, then we only access one block, and can return directly without iterating over blocks\n        if isinstance(column_key, INT_TYPES):\n            block_idx, column = self._index[column_key]\n            b = self._blocks[block_idx]\n            if b.ndim == 1:\n                if row_key is None:\n                    return b\n                return b[row_key]\n            if row_key is None:\n                return b[:, column]\n            return b[row_key, column]\n\n        # figure out shape from keys so as to not accumulate?\n        blocks = []\n        rows = 0\n        columns = 0\n        for b in tuple(self._slice_blocks( # a generator\n                row_key=row_key,\n                column_key=column_key)):\n            if b.ndim == 1: # it is a single column\n                if not rows: # assume all the same after first\n                    # if 1d, then the length should be the number of rows\n                    rows = b.shape[0]\n                columns += 1\n            else:\n                if not rows: # assume all the same after first\n                    rows = b.shape[0]\n                columns += b.shape[1]\n            blocks.append(b)\n\n        row_dtype = resolve_dtype_iter(b.dtype for b in blocks)\n        row_multiple = row_key is None or isinstance(row_key, KEY_MULTIPLE_TYPES)\n\n        return self._blocks_to_array(\n                blocks=blocks,\n                shape=(rows, columns),\n                row_dtype=row_dtype,\n                row_multiple=row_multiple)\n\n    def _extract(self,\n            row_key: GetItemKeyType = None,\n            column_key: GetItemKeyType = None\n            ) -> tp.Union['TypeBlocks', np.ndarray]: # but sometimes an element\n        '''\n        Return a TypeBlocks after performing row and column selection using iloc selection.\n\n        Row and column keys can be:\n            integer: single row/column selection\n            slices: one or more contiguous selections\n            iterable of integers: one or more non-contiguous and/or repeated selections\n\n        Returns:\n            TypeBlocks, or a single element if both are coordinats\n        '''\n        # identifying column_key as integer, then we only access one block, and can return directly without iterating over blocks\n        if isinstance(column_key, INT_TYPES):\n            block_idx, column = self._index[column_key]\n            b = self._blocks[block_idx]\n            row_key_null = (row_key is None or\n                    (isinstance(row_key, slice)\n                    and row_key == NULL_SLICE))\n            if b.ndim == 1:\n                if row_key_null: # return a column\n                    return TypeBlocks.from_blocks(b)\n                elif isinstance(row_key, INT_TYPES):\n                    return b[row_key] # return single item\n                return TypeBlocks.from_blocks(b[row_key])\n\n            if row_key_null:\n                return TypeBlocks.from_blocks(b[:, column])\n            elif isinstance(row_key, INT_TYPES):\n                return b[row_key, column] # return single item\n            return TypeBlocks.from_blocks(b[row_key, column])\n\n        # pass a generator to from_block; will return a TypeBlocks or a single element\n        return self.from_blocks(\n                self._slice_blocks(\n                        row_key=row_key,\n                        column_key=column_key),\n                shape_reference=self._shape\n                )\n\n\n    def _extract_iloc(self,\n            key: GetItemKeyTypeCompound\n            ) -> 'TypeBlocks':\n        if isinstance(key, tuple):\n            return self._extract(*key)\n        return self._extract(row_key=key)\n\n    def extract_iloc_mask(self,\n            key: GetItemKeyTypeCompound\n            ) -> 'TypeBlocks':\n        if isinstance(key, tuple):\n            return TypeBlocks.from_blocks(self._mask_blocks(*key))\n        return TypeBlocks.from_blocks(self._mask_blocks(row_key=key))\n\n    def extract_iloc_assign(self,\n            key: GetItemKeyTypeCompound,\n            value: object\n            ) -> 'TypeBlocks':\n        if isinstance(key, tuple):\n            key = tp.cast(tp.Tuple[int, int], key)\n            return TypeBlocks.from_blocks(self._assign_blocks_from_keys(*key, value=value))\n        return TypeBlocks.from_blocks(self._assign_blocks_from_keys(row_key=key, value=value))\n\n    def extract_bloc_assign(self,\n            key: np.ndarray,\n            value: tp.Any\n            ) -> 'TypeBlocks':\n        return TypeBlocks.from_blocks(self._assign_blocks_from_bloc_key(\n                bloc_key=key,\n                value=value\n                ))\n\n\n    def drop(self, key: GetItemKeyTypeCompound) -> 'TypeBlocks':\n        '''\n        Drop rows or columns from a TyepBlocks instance.\n\n        Args:\n            key: if a single value, treated as a row key; if a tuple, treated as a pair of row, column keys.\n        '''\n        if isinstance(key, tuple):\n            # column dropping can leed to a TB with generator that yields nothing;\n            return TypeBlocks.from_blocks(\n                    self._drop_blocks(*key),\n                    shape_reference=self._shape\n                    )\n        return TypeBlocks.from_blocks(\n                self._drop_blocks(row_key=key),\n                shape_reference=self._shape\n                )\n\n\n    def __getitem__(self, key: GetItemKeyTypeCompound) -> 'TypeBlocks':\n        '''\n        Returns a column, or a column slice.\n        '''\n        # NOTE: if key is a tuple it means that multiple indices are being provided\n        if isinstance(key, tuple):\n            raise KeyError('__getitem__ does not support multiple indexers')\n        return self._extract(row_key=None, column_key=key)\n\n    #---------------------------------------------------------------------------\n    # operators\n\n    def _ufunc_unary_operator(self, operator: tp.Callable[[np.ndarray], np.ndarray]) -> 'TypeBlocks':\n        # for now, do no reblocking; though, in many cases, operating on a unified block will be faster\n        def operation() -> tp.Iterator[np.ndarray]:\n            for b in self._blocks:\n                result = operator(b)\n                result.flags.writeable = False\n                yield result\n\n        return self.from_blocks(operation())\n\n    #---------------------------------------------------------------------------\n\n    def _block_shape_slices(self) -> tp.Iterator[slice]:\n        '''Generator of slices necessary to slice a 1d array of length equal to the number of columns into a lenght suitable for each block.\n        '''\n        start = 0\n        for b in self._blocks:\n            end = start + (1 if b.ndim == 1 else b.shape[1])\n            yield slice(start, end)\n            start = end\n\n    def _ufunc_binary_operator(self, *,\n            operator: tp.Callable[[np.ndarray, np.ndarray], np.ndarray],\n            other: tp.Iterable[tp.Any]\n            ) -> 'TypeBlocks':\n\n        self_operands: tp.Iterable[np.ndarray]\n        other_operands: tp.Iterable[np.ndarray]\n\n        if operator.__name__ == 'matmul' or operator.__name__ == 'rmatmul':\n            # this could be implemented but would force block consolidation\n            raise NotImplementedError('matrix multiplication not supported')\n\n        if isinstance(other, TypeBlocks):\n            apply_column_2d_filter = True\n\n            if self.block_compatible(other, axis=None):\n                # this means that the blocks are the same shape; we do not check types\n                self_operands = self._blocks\n                other_operands = other._blocks\n            elif self._shape == other._shape:\n                # if the result of reblock does not result in compatible shapes, we have to use .values as operands; the dtypes can be different so we only have to check that they columns sizes, the second element of the signature, all match.\n                if not self.reblock_compatible(other):\n                    self_operands = (self.values,)\n                    other_operands = (other.values,)\n                else:\n                    self_operands = self._reblock()\n                    other_operands = other._reblock()\n            else: # raise same error as NP\n                raise NotImplementedError('cannot apply binary operators to arbitrary TypeBlocks')\n        else:\n            # process other as an array\n            self_operands = self._blocks\n            if not isinstance(other, np.ndarray):\n                other = iterable_to_array_nd(other)\n            # handle dimensions\n            if other.ndim == 0 or (other.ndim == 1 and len(other) == 1):\n                # a scalar: reference same value for each block position\n                apply_column_2d_filter = False\n                other_operands = (other for _ in range(len(self._blocks)))\n            elif other.ndim == 1 and len(other) == self._shape[1]:\n                apply_column_2d_filter = False\n                # if given a 1d array, we apply it to the rows\n                # one dimensional array of same size: chop to block width\n                other_operands = (other[s] for s in self._block_shape_slices())\n            elif other.ndim == 2 and other.shape == self._shape:\n                apply_column_2d_filter = True\n                other_operands = (other[NULL_SLICE, s] for s in self._block_shape_slices())\n            else:\n                raise NotImplementedError(f'cannot apply binary operators to arrays without alignable shapes: {self._shape}, {other.shape}.')\n\n        return self.from_blocks(apply_binary_operator_blocks(\n                values=self_operands,\n                other=other_operands,\n                operator=operator,\n                apply_column_2d_filter=apply_column_2d_filter,\n                ))\n\n\n\n    def _ufunc_axis_skipna(self, *,\n            axis: int,\n            skipna: bool,\n            ufunc: UFunc,\n            ufunc_skipna: UFunc,\n            composable: bool,\n            dtypes: tp.Tuple[np.dtype, ...],\n            size_one_unity: bool\n            ) -> np.ndarray:\n        # not sure if these make sense on TypeBlocks, as they reduce dimensionality\n        raise NotImplementedError() #pragma: no cover\n\n    def _ufunc_shape_skipna(self, *,\n            axis: int,\n            skipna: bool,\n            ufunc: UFunc,\n            ufunc_skipna: UFunc,\n            composable: bool,\n            dtypes: tp.Tuple[np.dtype, ...],\n            size_one_unity: bool\n            ) -> np.ndarray:\n        # not sure if these make sense on TypeBlocks, as they reduce dimensionality\n        raise NotImplementedError() #pragma: no cover\n\n\n    #---------------------------------------------------------------------------\n    # transformations resulting in the same dimensionality\n\n    def transpose(self) -> 'TypeBlocks':\n        '''Return a new TypeBlocks that transposes and concatenates all blocks.\n        '''\n        blocks = []\n        for b in self._blocks:\n            b = column_2d_filter(b).transpose()\n            if b.dtype != self._row_dtype:\n                b = b.astype(self._row_dtype)\n            blocks.append(b)\n        a = np.concatenate(blocks)\n        a.flags.writeable = False # keep this array\n        return self.from_blocks(a)\n\n\n    def isna(self, include_none: bool = True) -> 'TypeBlocks':\n        '''Return a Boolean TypeBlocks where True is NaN or None.\n        '''\n        def blocks() -> tp.Iterator[np.ndarray]:\n            for b in self._blocks:\n                bool_block = isna_array(b, include_none)\n                bool_block.flags.writeable = False\n                yield bool_block\n\n        return self.from_blocks(blocks())\n\n\n    def notna(self, include_none: bool = True) -> 'TypeBlocks':\n        '''Return a Boolean TypeBlocks where True is not NaN or None.\n        '''\n        def blocks() -> tp.Iterator[np.ndarray]:\n            for b in self._blocks:\n                bool_block = np.logical_not(isna_array(b, include_none))\n                bool_block.flags.writeable = False\n                yield bool_block\n\n        return self.from_blocks(blocks())\n\n    #---------------------------------------------------------------------------\n    # fillna sided\n\n    @staticmethod\n    def _fillna_sided_axis_0(\n            blocks: tp.Iterable[np.ndarray],\n            value: tp.Any,\n            sided_leading: bool) -> tp.Iterator[np.ndarray]:\n        '''Return a TypeBlocks where NaN or None are replaced in sided (leading or trailing) segments along axis 0, meaning vertically.\n\n        Args:\n            sided_leading: True sets the side to fill is the leading side; False sets the side to fill to the trailiing side.\n\n        '''\n        if isinstance(value, np.ndarray):\n            raise RuntimeError('cannot assign an array to fillna')\n\n        sided_index = 0 if sided_leading else -1\n\n        # store flag for when non longer need to check blocks, yield immediately\n\n        for b in blocks:\n            sel = isna_array(b) # True for is NaN\n            ndim = sel.ndim\n\n            if ndim == 1 and not sel[sided_index]:\n                # if last value (bottom row) is not NaN, we can return block\n                yield b\n            elif ndim > 1 and ~sel[sided_index].any(): # if not any are NaN\n                # can use this last-row observation below\n                yield b\n            else:\n                assignable_dtype = resolve_dtype(np.array(value).dtype, b.dtype)\n                if b.dtype == assignable_dtype:\n                    assigned = b.copy()\n                else:\n                    assigned = b.astype(assignable_dtype)\n\n                # because np.nonzero is easier / faster to parse if applied on a 1D array, w can make 2d look like 1D here\n                if ndim == 1:\n                    sel_nonzeros = ((0, sel),)\n                else:\n                    # only collect columns for sided NaNs\n                    sel_nonzeros = ((i, sel[:, i]) for i, j in enumerate(sel[sided_index]) if j) #type: ignore\n\n                for idx, sel_nonzero in sel_nonzeros:\n                    # indices of not-nan values, per column\n                    targets = np.nonzero(~sel_nonzero)[0]\n                    if len(targets):\n                        if sided_leading:\n                            sel_slice = slice(0, targets[0])\n                        else: # trailings\n                            sel_slice = slice(targets[-1]+1, None)\n                    else: # all are NaN\n                        sel_slice = NULL_SLICE\n\n                    if ndim == 1:\n                        assigned[sel_slice] = value\n                    else:\n                        assigned[sel_slice, idx] = value\n\n                # done writing\n                assigned.flags.writeable = False\n                yield assigned\n\n\n    @staticmethod\n    def _fillna_sided_axis_1(\n            blocks: tp.Iterable[np.ndarray],\n            value: tp.Any,\n            sided_leading: bool) -> tp.Iterator[np.ndarray]:\n        '''Return a TypeBlocks where NaN or None are replaced in sided (leading or trailing) segments along axis 1.\n\n        NOTE: blocks are generated in reverse order when sided_leading is False.\n\n        Args:\n            sided_leading: True sets the side to fill is the leading side; False sets the side to fill to the trailiing side.\n\n        '''\n        if isinstance(value, np.ndarray):\n            raise RuntimeError('cannot assign an array to fillna')\n\n        sided_index = 0 if sided_leading else -1\n\n        # will need to re-reverse blocks coming out of this\n        block_iter = blocks if sided_leading else reversed(blocks) #type: ignore\n\n        isna_exit_previous = None\n\n        # iterate over blocks to observe NaNs contiguous horizontally\n        for b in block_iter:\n            sel = isna_array(b) # True for is NaN\n            ndim = sel.ndim\n\n            if isna_exit_previous is None:\n                # for first block, model as all True\n                isna_exit_previous = np.full(sel.shape[0], True, dtype=bool)\n\n            # to contunue nan propagation, the exit previous musy be NaN, as well as this start\n            if ndim == 1:\n                isna_entry = sel & isna_exit_previous\n            else:\n                isna_entry = sel[:, sided_index] & isna_exit_previous\n\n            if not isna_entry.any():\n                yield b\n            else:\n                assignable_dtype = resolve_dtype(np.array(value).dtype, b.dtype)\n                if b.dtype == assignable_dtype:\n                    assigned = b.copy()\n                else:\n                    assigned = b.astype(assignable_dtype)\n\n                if ndim == 1:\n                    # if one dim, we simply fill nan values\n                    assigned[isna_entry] = value\n                else:\n                    # only collect rows that have a sided NaN\n                    # could use np.nonzero()\n                    candidates = (i for i, j in enumerate(isna_entry) if j == True)\n                    sels_nonzero = ((i, sel[i]) for i in candidates)\n\n                    for idx, sel_nonzero in sels_nonzero:\n                        # indices of not-nan values, per row\n                        targets = np.nonzero(~sel_nonzero)[0]\n                        if len(targets):\n                            if sided_leading:\n                                sel_slice = slice(0, targets[0])\n                            else: # trailing\n                                sel_slice = slice(targets[-1]+1, None)\n                        else: # all are NaN\n                            sel_slice = NULL_SLICE\n\n                        if ndim == 1:\n                            assigned[sel_slice] = value\n                        else:\n                            assigned[idx, sel_slice] = value\n\n                assigned.flags.writeable = False\n                yield assigned\n\n            # always execute these lines after each yield\n            # return True for next block only if all values are NaN in the row\n            if ndim == 1:\n                isna_exit_previous = isna_entry\n            else:\n                isna_exit_previous = sel.all(axis=1) & isna_exit_previous\n\n\n    def fillna_leading(self,\n            value: tp.Any,\n            *,\n            axis: int = 0) -> 'TypeBlocks':\n        '''Return a TypeBlocks instance replacing leading values with the passed `value`. Leading, axis 0 fills columns, going from top to bottom. Leading axis 1 fills rows, going from left to right.\n        '''\n        if axis == 0:\n            return self.from_blocks(self._fillna_sided_axis_0(\n                    blocks=self._blocks,\n                    value=value,\n                    sided_leading=True))\n        elif axis == 1:\n            return self.from_blocks(self._fillna_sided_axis_1(\n                    blocks=self._blocks,\n                    value=value,\n                    sided_leading=True))\n        raise NotImplementedError(f'no support for axis {axis}')\n\n    def fillna_trailing(self,\n            value: tp.Any,\n            *,\n            axis: int = 0) -> 'TypeBlocks':\n        '''Return a TypeBlocks instance replacing trailing NaNs with the passed `value`. Trailing, axis 0 fills columns, going from bottom to top. Trailing axis 1 fills rows, going from right to left.\n        '''\n        if axis == 0:\n            return self.from_blocks(self._fillna_sided_axis_0(\n                    blocks=self._blocks,\n                    value=value,\n                    sided_leading=False))\n        elif axis == 1:\n            # must reverse when not leading\n            blocks = reversed(tuple(self._fillna_sided_axis_1(\n                    blocks=self._blocks,\n                    value=value,\n                    sided_leading=False)))\n            return self.from_blocks(blocks)\n\n        raise NotImplementedError(f'no support for axis {axis}')\n\n    #---------------------------------------------------------------------------\n    # fillna directional\n\n    @staticmethod\n    def _fillna_directional_axis_0(\n            blocks: tp.Iterable[np.ndarray],\n            directional_forward: bool,\n            limit: int = 0\n            ) -> tp.Iterator[np.ndarray]:\n        '''\n        Do a directional fill along axis 0, meaning filling vertically, going top/bottom or bottom/top.\n\n        Args:\n            directional_forward: if True, start from the forward (top or left) side.\n        '''\n\n        for b in blocks:\n            sel = isna_array(b) # True for is NaN\n            ndim = sel.ndim\n\n            if ndim == 1 and not np.any(sel):\n                yield b\n            elif ndim == 2 and not np.any(sel).any():\n                yield b\n            else:\n                target_indexes = binary_transition(sel)\n\n                if ndim == 1:\n                    # make single array look like iterable of tuples\n                    slots = 1\n                    length = len(sel)\n\n                elif ndim == 2:\n                    slots = b.shape[1] # axis 0 has column width\n                    length = b.shape[0]\n\n                # type is already compatible, no need for check\n                assigned = b.copy()\n\n                for i in range(slots):\n\n                    if ndim == 1:\n                        target_index = target_indexes\n                        if not len(target_index):\n                            continue\n                        target_values = b[target_index]\n\n                        def slice_condition(target_slice: slice) -> bool:\n                            # NOTE: start is never None\n                            return sel[target_slice.start] # type: ignore\n\n                    else: # 2D blocks\n                        target_index = target_indexes[i]\n                        if not target_index:\n                            continue\n                        target_values = b[target_index, i]\n\n                        def slice_condition(target_slice: slice) -> bool:\n                            # NOTE: start is never None\n                            return sel[target_slice.start, i] # type: ignore\n\n                    for target_slice, value in slices_from_targets(\n                            target_index=target_index,\n                            target_values=target_values,\n                            length=length,\n                            directional_forward=directional_forward,\n                            limit=limit,\n                            slice_condition=slice_condition\n                            ):\n\n                        if ndim == 1:\n                            assigned[target_slice] = value\n                        else:\n                            assigned[target_slice, i] = value\n\n                assigned.flags.writeable = False\n                yield assigned\n\n\n\n    @staticmethod\n    def _fillna_directional_axis_1(\n            blocks: tp.Iterable[np.ndarray],\n            directional_forward: bool,\n            limit: int = 0\n            ) -> tp.Iterator[np.ndarray]:\n        '''\n        Do a directional fill along axis 1, or horizontally, going left to right or right to left.\n\n        NOTE: blocks are generated in reverse order when directional_forward is False.\n\n        '''\n        bridge_src_index = -1 if directional_forward else 0\n        bridge_dst_index = 0 if directional_forward else -1\n\n        # will need to re-reverse blocks coming out of this\n        block_iter = blocks if directional_forward else reversed(blocks) # type: ignore\n\n        bridging_values: tp.Optional[np.ndarray] = None\n        bridging_count: tp.Optional[np.ndarray] = None\n        bridging_isna: tp.Optional[np.ndarray] = None # Boolean array describing isna of bridging values\n\n        for b in block_iter:\n            sel = isna_array(b) # True for is NaN\n            ndim = sel.ndim\n\n            if ndim == 1 and not np.any(sel):\n                bridging_values = b\n                bridging_isna = sel\n                bridging_count = np.full(b.shape[0], 0)\n                yield b\n            elif ndim == 2 and not np.any(sel).any():\n                bridging_values = b[:, bridge_src_index]\n                bridging_isna = sel[:, bridge_src_index]\n                bridging_count = np.full(b.shape[0], 0)\n                yield b\n            else: # some NA in this block\n                if bridging_values is None:\n                    assigned = b.copy()\n                    bridging_count = np.full(b.shape[0], 0)\n                else:\n                    assignable_dtype = resolve_dtype(bridging_values.dtype, b.dtype)\n                    assigned = b.astype(assignable_dtype)\n\n                if ndim == 1:\n                    # a single array has either NaN or non-NaN values; will only fill in NaN if we have a caried value from the previous block\n                    if bridging_values is not None: # sel has at least one NaN\n                        bridging_isnotna = ~bridging_isna # type: ignore #pylint: disable=E1130\n\n                        sel_sided = sel & bridging_isnotna\n                        if limit:\n                            # set to false those values where bridging already at limit\n                            sel_sided[bridging_count >= limit] = False # type: ignore\n\n                        # set values in assigned if there is a NaN here (sel_sided) and we are not beyond the count\n                        assigned[sel_sided] = bridging_values[sel_sided]\n                        # only increment positions that are NaN here and have not-nan bridging values\n                        sel_count_increment = sel & bridging_isnotna\n                        bridging_count[sel_count_increment] += 1 # type: ignore\n                        # set unassigned to zero\n                        bridging_count[~sel_count_increment] = 0 # type: ignore\n                    else:\n                        bridging_count = np.full(b.shape[0], 0)\n\n                    bridging_values = assigned\n                    bridging_isna = isna_array(bridging_values) # must reevaluate if assigned\n\n                elif ndim == 2:\n\n                    slots = b.shape[0] # axis 0 has column width\n                    length = b.shape[1]\n\n                    # set to True when can reset count to zero; this is always the case if the bridge src value is not NaN (before we do any filling)\n                    bridging_count_reset = ~sel[:, bridge_src_index]\n\n                    if bridging_values is not None:\n                        bridging_isnotna = ~bridging_isna #type: ignore #pylint: disable=E1130\n\n                        # find leading NaNs segments if they exist, and if there is as corrresponding non-nan value to bridge\n                        isna_entry = sel[:, bridge_dst_index] & bridging_isnotna\n                        # get a row of Booleans for plausible candidates\n                        candidates = (i for i, j in enumerate(isna_entry) if j == True)\n                        sels_nonzero = ((i, sel[i]) for i in candidates)\n\n                        # get appropriate leading slice to cover nan region\n                        for idx, sel_nonzero in sels_nonzero:\n                            # indices of not-nan values, per row\n                            targets = np.nonzero(~sel_nonzero)[0]\n                            if len(targets):\n                                if directional_forward:\n                                    sel_slice = slice(0, targets[0])\n                                else: # backward\n                                    sel_slice = slice(targets[-1]+1, length)\n                            else: # all are NaN\n                                sel_slice = slice(0, length)\n\n                            # truncate sel_slice by limit-\n                            sided_len = len(range(*sel_slice.indices(length)))\n\n                            if limit and bridging_count[idx] >= limit: # type: ignore #pylint: disable=R1724\n                                # if already at limit, do not assign\n                                bridging_count[idx] += sided_len # type: ignore\n                                continue\n                            elif limit and (bridging_count[idx] + sided_len) >= limit: # type: ignore\n                                # trim slice to fit\n                                shift = bridging_count[idx] + sided_len - limit # type: ignore\n                                # shift should only be positive only here\n                                if directional_forward:\n                                    sel_slice = slice(\n                                            sel_slice.start,\n                                            sel_slice.stop - shift)\n                                else:\n                                    sel_slice = slice(\n                                            sel_slice.start + shift,\n                                            sel_slice.stop)\n\n                            # update with full length or limited length?\n                            bridging_count[idx] += sided_len # type: ignore\n                            assigned[idx, sel_slice] = bridging_values[idx]\n\n                    # handle each row (going horizontally) in isolation\n                    target_indexes = binary_transition(sel, axis=1)\n                    for i in range(slots):\n\n                        target_index = target_indexes[i]\n                        if target_index is None:\n                            # found no transitions, so either all NaN or all not NaN; if all NaN, might have been filled in bridging; if had values, will aready identify as bridging_count_reset[i] == True\n                            continue\n\n                        target_values = b[i, target_index]\n\n                        def slice_condition(target_slice: slice) -> bool:\n                            # NOTE: start is never None\n                            return sel[i, target_slice.start] # type: ignore\n\n                        target_slice = None\n                        for target_slice, value in slices_from_targets(\n                                target_index=target_index,\n                                target_values=target_values,\n                                length=length,\n                                directional_forward=directional_forward,\n                                limit=limit,\n                                slice_condition=slice_condition\n                                ):\n                            assigned[i, target_slice] = value\n\n                        # update counts from the last slice; this will have already been limited if necessary, but need to reflext contiguous values going into the next block; if slices does not go to edge; will identify as needing as reset\n                        if target_slice is not None:\n                            bridging_count[i] = len(range(*target_slice.indices(length))) # type: ignore\n\n                    bridging_values = assigned[:, bridge_src_index]\n                    bridging_isna = isna_array(bridging_values) # must reevaluate if assigned\n\n                    # if the birdging values is NaN now, it could not be filled, or was not filled enough, and thus does not continue a count; can set to zero\n                    bridging_count_reset |= bridging_isna\n                    bridging_count[bridging_count_reset] = 0 # type: ignore\n\n                assigned.flags.writeable = False\n                yield assigned\n\n\n    def fillna_forward(self,\n            limit: int = 0,\n            *,\n            axis: int = 0) -> 'TypeBlocks':\n        '''Return a new ``TypeBlocks`` after feeding forward the last non-null (NaN or None) observation across contiguous nulls. Forward axis 0 fills columns, going from top to bottom. Forward axis 1 fills rows, going from left to right.\n        '''\n        if axis == 0:\n            return self.from_blocks(self._fillna_directional_axis_0(\n                    blocks=self._blocks,\n                    directional_forward=True,\n                    limit=limit\n                    ))\n        elif axis == 1:\n            return self.from_blocks(self._fillna_directional_axis_1(\n                    blocks=self._blocks,\n                    directional_forward=True,\n                    limit=limit\n                    ))\n\n        raise AxisInvalid(f'no support for axis {axis}')\n\n\n    def fillna_backward(self,\n            limit: int = 0,\n            *,\n            axis: int = 0) -> 'TypeBlocks':\n        '''Return a new ``TypeBlocks`` after feeding backward the last non-null (NaN or None) observation across contiguous nulls. Backward, axis 0 fills columns, going from bottom to top. Backward axis 1 fills rows, going from right to left.\n        '''\n        if axis == 0:\n            return self.from_blocks(self._fillna_directional_axis_0(\n                    blocks=self._blocks,\n                    directional_forward=False,\n                    limit=limit\n                    ))\n        elif axis == 1:\n            blocks = reversed(tuple(self._fillna_directional_axis_1(\n                    blocks=self._blocks,\n                    directional_forward=False,\n                    limit=limit\n                    )))\n            return self.from_blocks(blocks)\n\n        raise AxisInvalid(f'no support for axis {axis}')\n\n\n\n    #---------------------------------------------------------------------------\n\n    def dropna_to_keep_locations(self,\n            axis: int = 0,\n            condition: tp.Callable[..., bool] = np.all,\n    ) -> tp.Tuple[tp.Optional[np.ndarray], tp.Optional[np.ndarray]]:\n        '''\n        Return the row and column slices to extract the new TypeBlock. This is to be used by Frame, where the slices will be needed on the indices as well.\n\n        Args:\n            axis: Dimension to drop, where 0 will drop rows and 1 will drop columns based on the condition function applied to a Boolean array.\n        '''\n        # get a unified boolean array; as iisna will always return a Boolean, we can simply take the firtst block out of consolidation\n        unified = next(self.consolidate_blocks(isna_array(b) for b in self._blocks))\n\n        # flip axis to condition funcion\n        condition_axis = 0 if axis else 1\n        to_drop = condition(unified, axis=condition_axis)\n        to_keep = np.logical_not(to_drop)\n\n        if axis == 1:\n            row_key = None\n            column_key = to_keep\n        else:\n            row_key = to_keep\n            column_key = None\n\n        return row_key, column_key\n\n\n    def fillna(self,\n            value: object,\n            value_valid: tp.Optional[np.ndarray] = None,\n            ) -> 'TypeBlocks':\n        '''\n        Return a new TypeBlocks instance that fills missing values with the passed value.\n\n        Args:\n            value: value to fill missing with; can be an element or a same-sized array.\n            value_valid: Optionally provide a same-size array mask of the value setting (useful for carrying forward information from labels).\n        '''\n        return self.from_blocks(\n                self._assign_blocks_from_boolean_blocks(\n                        targets=(isna_array(b) for b in self._blocks),\n                        value=value,\n                        value_valid=value_valid\n                        )\n                )\n\n    @doc_inject()\n    def equals(self,\n            other: tp.Any,\n            *,\n            compare_dtype: bool = False,\n            compare_class: bool = False,\n            skipna: bool = True,\n            ) -> bool:\n        '''\n        {doc} Underlying block structure is not considered in determining equality.\n\n        Args:\n            {compare_dtype}\n            {compare_class}\n            {skipna}\n        '''\n        if id(other) == id(self):\n            return True\n\n        # NOTE: there is only one TypeBlocks class, but better to be consistent\n        if compare_class and self.__class__ != other.__class__:\n            return False\n        elif not isinstance(other, TypeBlocks):\n            return False\n\n        # same type from here\n        if self._shape != other._shape:\n            return False\n        if compare_dtype and self._dtypes != other._dtypes: # these are lists\n            return False\n\n        # NOTE: TypeBlocks handles array operations that return Boolean\n        eq = self == other # returns a Boolean TypeBlocks instance\n\n        if skipna:\n            isna_self = self.isna(include_none=False) # returns type blocks\n            isna_other = other.isna(include_none=False)\n            isna_both = isna_self & isna_self\n\n        start = 0\n        end = 0\n        for block in eq._blocks:\n            # permit short circuiting on iteration\n            if skipna:\n                end = start + block.shape[1]\n                target = isna_both._extract_array(column_key=slice(start, end))\n                start = end\n                # fill-in NaN values with True\n                block.flags.writeable = True\n                block[target] = True\n\n            if not block.all():\n                return False\n        return True\n\n    #---------------------------------------------------------------------------\n    # mutate\n\n    def append(self, block: np.ndarray) -> None:\n        '''Add a block; an array copy will not be made unless the passed in block is not immutable'''\n        # NOTE: shape can be 0, 0 if empty, or any one dimension can be 0. if columns is 0 and rows is non-zero, that row count is binding for appending (though the array need no tbe appended); if columns is > 0 and rows is zero, that row is binding for appending (and the array should be appended).\n\n        row_count = self._shape[0]\n\n        # update shape\n        if block.shape[0] != row_count:\n            raise RuntimeError(f'appended block shape {block.shape} does not align with shape {self._shape}')\n\n        if block.ndim == 1:\n            # length already confirmed to match row count; even if this is a zero length 1D array, we keep it as it (by definition) defines a column (if the existing row_count is zero). said another way, a zero length, 1D array always has a shape of (0, 1)\n            block_columns = 1\n        else:\n            block_columns = block.shape[1]\n            if block_columns == 0:\n                # do not append 0 width arrays\n                return\n\n        # extend shape, or define it if not yet set\n        self._shape = (row_count, self._shape[1] + block_columns)\n\n        # add block, dtypes, index\n        block_idx = len(self._blocks) # next block\n        for i in range(block_columns):\n            self._index.append((block_idx, i))\n            self._dtypes.append(block.dtype)\n\n        # make immutable copy if necessary before appending\n        self._blocks.append(immutable_filter(block))\n\n        # if already aligned, nothing to do\n        if not self._row_dtype: # if never set as shape is empty\n            self._row_dtype = block.dtype\n        elif block.dtype != self._row_dtype:\n            # we do not use resolve_dtype here as we want to preserve types, not safely cooerce them (i.e., int to float)\n            self._row_dtype = DTYPE_OBJECT\n\n    def extend(self,\n            other: tp.Union['TypeBlocks', tp.Iterable[np.ndarray]]\n            ) -> None:\n        '''Extend this TypeBlock with the contents of another TypeBlocks instance, or an iterable of arrays. Note that an iterable of TypeBlocks is not currently supported.\n        '''\n        if isinstance(other, TypeBlocks):\n            if self._shape[0]:\n                if self._shape[0] != other._shape[0]:\n                    raise RuntimeError('cannot extend unaligned shapes')\n            blocks: tp.Iterable[np.ndarray] = other._blocks\n        else: # accept iterables of np.arrays\n            blocks = other\n        # row count must be the same\n        for block in blocks:\n            self.append(block)\n"""
static_frame/core/util.py,273,"b'import typing as tp\nimport os\nimport operator\n\nfrom collections import abc\nfrom collections import defaultdict\nfrom collections import namedtuple\nfrom functools import partial\nfrom functools import reduce\nfrom itertools import chain\nfrom itertools import zip_longest\nfrom io import StringIO\nfrom enum import Enum\nimport datetime\nfrom urllib import request\nimport tempfile\nfrom pathlib import Path\n\nfrom automap import FrozenAutoMap  # pylint: disable = E0611\nimport numpy as np\n\n\nif tp.TYPE_CHECKING:\n\n    from static_frame.core.index_base import IndexBase #pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.index import Index #pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.series import Series #pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.frame import Frame #pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.frame import FrameAsType #pylint: disable=W0611 #pragma: no cover\n    from static_frame.core.type_blocks import TypeBlocks #pylint: disable=W0611 #pragma: no cover\n\n\n# dtype.kind\n#     A character code (one of \xe2\x80\x98biufcmMOSUV\xe2\x80\x99) identifying the general kind of data.\n#     b \tboolean\n#     i \tsigned integer\n#     u \tunsigned integer\n#     f \tfloating-point\n#     c \tcomplex floating-point\n#     m \ttimedelta\n#     M \tdatetime\n#     O \tobject\n#     S \t(byte-)string\n#     U \tUnicode\n#     V \tvoid\n\n# https://docs.scipy.org/doc/numpy-1.10.1/reference/arrays.scalars.html\n\n\nDEFAULT_SORT_KIND = \'mergesort\'\nDEFAULT_STABLE_SORT_KIND = \'mergesort\'\n\n# ARCHITECTURE_SIZE = struct.calcsize(\'P\') * 8 # size of pointer\n# ARCHITECTURE_INT_DTYPE = np.int64 if ARCHITECTURE_SIZE == 64 else np.int32\n\nDTYPE_STR_KIND = (\'U\', \'S\') # S is np.bytes_\nDTYPE_INT_KIND = (\'i\', \'u\') # signed and unsigned\nDTYPE_NAN_KIND = (\'f\', \'c\') # kinds that support NaN values\nDTYPE_DATETIME_KIND = \'M\'\nDTYPE_TIMEDELTA_KIND = \'m\'\nDTYPE_COMPLEX_KIND = \'c\'\nDTYPE_NAT_KIND = (\'M\', \'m\')\n# DTYPE_BOOL_KIND = (\'b\',)\n\nDTYPE_OBJECT = np.dtype(object)\nDTYPE_BOOL = np.dtype(bool)\nDTYPE_STR = np.dtype(str)\nDTYPE_INT_DEFAULT = np.dtype(np.int64)\nDTYPE_FLOAT_DEFAULT = np.dtype(np.float64)\nDTYPE_COMPLEX_DEFAULT = np.dtype(np.complex128)\n\n# used in UFUNC_AXIS_SKIPNA defintion\nDTYPES_BOOL = (DTYPE_BOOL,)\nDTYPES_INEXACT = (DTYPE_FLOAT_DEFAULT, DTYPE_COMPLEX_DEFAULT)\n\nNULL_SLICE = slice(None) # gathers everything\nUNIT_SLICE = slice(0, 1)\nEMPTY_SLICE = slice(0, 0) # gathers nothing\n\nSLICE_START_ATTR = \'start\'\nSLICE_STOP_ATTR = \'stop\'\nSLICE_STEP_ATTR = \'step\'\nSLICE_ATTRS = (SLICE_START_ATTR, SLICE_STOP_ATTR, SLICE_STEP_ATTR)\n\nSTATIC_ATTR = \'STATIC\'\n\nEMPTY_TUPLE = ()\nEMPTY_SET: tp.FrozenSet[tp.Any] = frozenset()\n\n# defaults to float64\nEMPTY_ARRAY = np.array(EMPTY_TUPLE, dtype=None)\nEMPTY_ARRAY.flags.writeable = False\n\nEMPTY_ARRAY_BOOL = np.array(EMPTY_TUPLE, dtype=DTYPE_BOOL)\nEMPTY_ARRAY_BOOL.flags.writeable = False\n\nEMPTY_ARRAY_INT = np.array(EMPTY_TUPLE, dtype=DTYPE_INT_DEFAULT)\nEMPTY_ARRAY_INT.flags.writeable = False\n\nNAT = np.datetime64(\'nat\')\n# define missing for timedelta as an untyped 0\nEMPTY_TIMEDELTA = np.timedelta64(0)\n\n# _DICT_STABLE = sys.version_info >= (3, 6)\n\n# map from datetime.timedelta attrs to np.timedelta64 codes\nTIME_DELTA_ATTR_MAP = (\n        (\'days\', \'D\'),\n        (\'seconds\', \'s\'),\n        (\'microseconds\', \'us\')\n        )\n\n# ufunc functions that will not work with DTYPE_STR_KIND, but do work if converted to object arrays; see UFUNC_AXIS_SKIPNA for the matching functions\nUFUNC_AXIS_STR_TO_OBJ = {np.min, np.max, np.sum}\n\n#-------------------------------------------------------------------------------\n# utility type groups\n\nINT_TYPES = (int, np.integer) # np.integer catches all np int\nFLOAT_TYPES = (float, np.floating) # np.floating catches all np float\nCOMPLEX_TYPES = (complex, np.complexfloating) # np.complexfloating catches all np complex\nINEXACT_TYPES = (float, complex, np.inexact) # inexact matches floating, complexfloating\nNUMERIC_TYPES = (int, float, complex, np.number)\n\nBOOL_TYPES = (bool, np.bool_)\nDICTLIKE_TYPES = (abc.Set, dict, FrozenAutoMap)\n\n\n# iterables that cannot be used in NP array constructors; asumes that dictlike types have already been identified\nINVALID_ITERABLE_FOR_ARRAY = (abc.ValuesView, abc.KeysView)\nNON_STR_TYPES = {int, float, bool}\n\n# integers above this value will occassionally, once coerced to a float (64 or 128) in an NP array, will not match a hash lookup as a key in a dictionary; an NP array of int or object will work\nINT_MAX_COERCIBLE_TO_FLOAT = 1_000_000_000_000_000\n\n# for getitem / loc selection\nKEY_ITERABLE_TYPES = (list, np.ndarray)\nKeyIterableTypes = tp.Union[tp.Iterable[tp.Any], np.ndarray]\n\n# types of keys that return multiple items, even if the selection reduces to 1\nKEY_MULTIPLE_TYPES = (slice, list, np.ndarray)\n\n# for type hinting\n# keys once dimension has been isolated\nGetItemKeyType = tp.Union[\n        int,\n        np.integer,\n        slice,\n        tp.List[tp.Any],\n        None,\n        \'Index\',\n        \'Series\',\n        np.ndarray\n        ]\n\n# keys that might include a multiple dimensions speciation; tuple is used to identify compound extraction\nGetItemKeyTypeCompound = tp.Union[\n        tp.Tuple[tp.Any, ...],\n        int,\n        np.integer,\n        slice,\n        tp.List[tp.Any],\n        None,\n        \'Index\',\n        \'Series\',\n        np.ndarray]\n\nKeyTransformType = tp.Optional[tp.Callable[[GetItemKeyType], GetItemKeyType]]\nNameType = tp.Optional[tp.Hashable]\n\nBloc2DKeyType = tp.Union[\'Frame\', np.ndarray]\n# Bloc1DKeyType = tp.Union[\'Series\', np.ndarray]\n\nUFunc = tp.Callable[..., np.ndarray]\nAnyCallable = tp.Callable[..., tp.Any]\n\nMapping = tp.Union[tp.Mapping[tp.Hashable, tp.Any], \'Series\']\nCallableOrMapping = tp.Union[AnyCallable, tp.Mapping[tp.Hashable, tp.Any], \'Series\']\n\ndef is_callable_or_mapping(value: CallableOrMapping) -> bool:\n    from static_frame import Series\n    return callable(value) or isinstance(value, dict) or isinstance(value, Series)\n\nCallableOrCallableMap = tp.Union[AnyCallable, tp.Mapping[tp.Hashable, AnyCallable]]\n\n# for explivitl selection hashables, or things that will be converted to lists of hashables (explicitly lists)\nKeyOrKeys = tp.Union[tp.Hashable, tp.Iterable[tp.Hashable]]\n\nPathSpecifier = tp.Union[str, Path]\nPathSpecifierOrFileLike = tp.Union[str, Path, tp.TextIO]\nPathSpecifierOrFileLikeOrIterator = tp.Union[str, Path, tp.TextIO, tp.Iterator[str]]\n\n\nDtypeSpecifier = tp.Optional[tp.Union[str, np.dtype, type]]\n\n# support an iterable of specifiers, or mapping based on column names\nDtypesSpecifier = tp.Optional[\n        tp.Union[tp.Iterable[DtypeSpecifier], tp.Dict[tp.Hashable, DtypeSpecifier]]]\n\n# specifiers that are equivalent to object\nDTYPE_SPECIFIERS_OBJECT = {DTYPE_OBJECT, object, tuple}\n\nDepthLevelSpecifier = tp.Union[int, tp.Iterable[int]]\n\nCallableToIterType = tp.Callable[[], tp.Iterable[tp.Any]]\n\nIndexSpecifier = tp.Union[int, tp.Hashable] # specify a postiion in an index\nIndexInitializer = tp.Union[\n        \'IndexBase\',\n        tp.Iterable[tp.Hashable],\n        tp.Iterable[tp.Sequence[tp.Hashable]], # only for IndexHierarhcy\n        ]\nIndexConstructor = tp.Callable[..., \'IndexBase\']\n\nIndexConstructors = tp.Sequence[IndexConstructor]\n\n\n# take integers for size; otherwise, extract size from any other index initializer\n\nSeriesInitializer = tp.Union[\n        tp.Iterable[tp.Any],\n        np.ndarray,\n        tp.Mapping[tp.Hashable, tp.Any],\n        int, float, str, bool]\n\n# support single items, or numpy arrays, or values that can be made into a 2D array\nFRAME_INITIALIZER_DEFAULT = object()\n\nFrameInitializer = tp.Union[\n        tp.Iterable[tp.Iterable[tp.Any]],\n        np.ndarray,\n        ] # need to add FRAME_INITIALIZER_DEFAULT\n\nFILL_VALUE_DEFAULT = object()\nNAME_DEFAULT = object()\n\n\nDateInitializer = tp.Union[str, datetime.date, np.datetime64]\nYearMonthInitializer = tp.Union[str, datetime.date, np.datetime64]\nYearInitializer = tp.Union[str, datetime.date, np.datetime64]\n\n#-------------------------------------------------------------------------------\n# join utils\n\nclass Join(Enum):\n    INNER = 0\n    LEFT = 1\n    RIGHT = 2\n    OUTER = 3\n\nclass Pair(tuple): #type: ignore\n    pass\n\nclass PairLeft(Pair):\n    pass\n\nclass PairRight(Pair):\n    pass\n\n#-------------------------------------------------------------------------------\n\ndef is_hashable(value: tp.Any) -> bool:\n    \'\'\'\n    Determine if a value is hashable without raising an exception.\n    \'\'\'\n    try:\n        hash(value)\n        return True\n    except TypeError:\n        pass\n    return False\n\ndef reversed_iter(value: tp.Iterator[tp.Any]) -> tp.Iterator[tp.Any]:\n    \'\'\'Wrapper around built-in reversed() that handles generators by realizing them and then reversing.\n    \'\'\'\n    try:\n        yield from reversed(value) #type: ignore\n    except TypeError:\n        pass\n    yield from reversed(tuple(value))\n\n\ndef mloc(array: np.ndarray) -> int:\n    \'\'\'Return the memory location of an array.\n    \'\'\'\n    return tp.cast(int, array.__array_interface__[\'data\'][0])\n\n\ndef immutable_filter(src_array: np.ndarray) -> np.ndarray:\n    \'\'\'Pass an immutable array; otherwise, return an immutable copy of the provided array.\n    \'\'\'\n    if src_array.flags.writeable:\n        dst_array = src_array.copy()\n        dst_array.flags.writeable = False\n        return dst_array\n    return src_array # keep it as is\n\ndef name_filter(name: NameType) -> NameType:\n    \'\'\'\n    For name attributes on containers, only permit recursively hashable objects.\n    \'\'\'\n    try:\n        hash(name)\n    except TypeError:\n        raise TypeError(\'unhashable name attribute\', name)\n    return name\n\ndef shape_filter(array: np.ndarray) -> tp.Tuple[int, int]:\n    \'\'\'Represent a 1D array as a 2D array with length as rows of a single-column array.\n\n    Return:\n        row, column count for a block of ndim 1 or ndim 2.\n    \'\'\'\n    if array.ndim == 1:\n        return array.shape[0], 1\n    return array.shape #type: ignore\n\ndef column_2d_filter(array: np.ndarray) -> np.ndarray:\n    \'\'\'Reshape a flat ndim 1 array into a 2D array with one columns and rows of length. This is used (a) for getting string representations and (b) for using np.concatenate and np binary operators on 1D arrays.\n    \'\'\'\n    # it is not clear when reshape is a copy or a view\n    if array.ndim == 1:\n        return np.reshape(array, (array.shape[0], 1))\n    return array\n\ndef column_1d_filter(array: np.ndarray) -> np.ndarray:\n    \'\'\'\n    Ensure that a column that might be 2D or 1D is returned as a 1D array.\n    \'\'\'\n    if array.ndim == 2:\n        # could assert that array.shape[1] == 1, but this will raise if does not fit\n        return np.reshape(array, array.shape[0])\n    return array\n\ndef row_1d_filter(array: np.ndarray) -> np.ndarray:\n    \'\'\'\n    Ensure that a row that might be 2D or 1D is returned as a 1D array.\n    \'\'\'\n    if array.ndim == 2:\n        # could assert that array.shape[0] == 1, but this will raise if does not fit\n        return np.reshape(array, array.shape[1])\n    return array\n\n\ndef _gen_skip_middle(\n        forward_iter: CallableToIterType,\n        forward_count: int,\n        reverse_iter: CallableToIterType,\n        reverse_count: int,\n        center_sentinel: tp.Any) -> tp.Iterator[tp.Any]:\n    \'\'\'\n    Provide a generator to yield the count values from each side.\n    \'\'\'\n    assert forward_count > 0 and reverse_count > 0\n    # for the forward gen, we take one more column to serve as the center column ellipsis; thus, we start enumeration at 0\n    for idx, value in enumerate(forward_iter(), start=1):\n        yield value\n        if idx == forward_count:\n            break\n    # center sentinel\n    yield center_sentinel\n\n    values = []\n    for idx, col in enumerate(reverse_iter(), start=1):\n        values.append(col)\n        if idx == reverse_count:\n            break\n    yield from reversed(values)\n\n\ndef resolve_dtype(dt1: np.dtype, dt2: np.dtype) -> np.dtype:\n    \'\'\'\n    Given two dtypes, return a compatible dtype that can hold both contents without truncation.\n    \'\'\'\n    # NOTE: this is not taking into account endianness; it is not clear if this is important\n    # NOTE: np.dtype(object) == np.object_, so we can return np.object_\n\n    # if the same, return that dtype\n    if dt1 == dt2:\n        return dt1\n\n    # if either is object, we go to object\n    if dt1.kind == \'O\' or dt2.kind == \'O\':\n        return DTYPE_OBJECT\n\n    dt1_is_str = dt1.kind in DTYPE_STR_KIND\n    dt2_is_str = dt2.kind in DTYPE_STR_KIND\n    if dt1_is_str and dt2_is_str:\n        # if both are string or string-like, we can use result type to get the longest string\n        return np.result_type(dt1, dt2)\n\n    dt1_is_dt = dt1.kind == DTYPE_DATETIME_KIND\n    dt2_is_dt = dt2.kind == DTYPE_DATETIME_KIND\n    if dt1_is_dt and dt2_is_dt:\n        # if both are datetime, result type will work\n        return np.result_type(dt1, dt2)\n\n    dt1_is_tdelta = dt1.kind == DTYPE_TIMEDELTA_KIND\n    dt2_is_tdelta = dt2.kind == DTYPE_TIMEDELTA_KIND\n    if dt1_is_tdelta and dt2_is_tdelta:\n        # this may or may not work\n        # TypeError: Cannot get a common metadata divisor for NumPy datetime metadata [D] and [Y] because they have incompatible nonlinear base time units\n        try:\n            return np.result_type(dt1, dt2)\n        except TypeError:\n            return DTYPE_OBJECT\n\n    dt1_is_bool = dt1.type is np.bool_\n    dt2_is_bool = dt2.type is np.bool_\n\n    # if any one is a string or a bool, we have to go to object; we handle both cases being the same above; result_type gives a string in mixed cases\n    if (dt1_is_str or dt2_is_str\n            or dt1_is_bool or dt2_is_bool\n            or dt1_is_dt or dt2_is_dt\n            or dt1_is_tdelta or dt2_is_tdelta\n            ):\n        return DTYPE_OBJECT\n\n    # if not a string or an object, can use result type\n    return np.result_type(dt1, dt2)\n\ndef resolve_dtype_iter(dtypes: tp.Iterable[np.dtype]) -> np.dtype:\n    \'\'\'Given an iterable of one or more dtypes, do pairwise comparisons to determine compatible overall type. Once we get to object we can stop checking and return object.\n\n    Args:\n        dtypes: iterable of one or more dtypes.\n    \'\'\'\n    dtypes = iter(dtypes)\n    dt_resolve = next(dtypes)\n\n    for dt in dtypes:\n        dt_resolve = resolve_dtype(dt_resolve, dt)\n        if dt_resolve == DTYPE_OBJECT:\n            return dt_resolve\n    return dt_resolve\n\n\n\ndef concat_resolved(\n        arrays: tp.Iterable[np.ndarray],\n        axis: int = 0) -> np.ndarray:\n    \'\'\'\n    Concatenation of 2D arrays that uses resolved dtypes to avoid truncation.\n\n    Axis 0 stacks rows (extends columns); axis 1 stacks columns (extends rows).\n\n    No shape manipulation will happen, so it is always assumed that all dimensionalities will be common.\n    \'\'\'\n    #all the input array dimensions except for the concatenation axis must match exactly\n    if axis is None:\n        raise NotImplementedError(\'no handling of concatenating flattened arrays\')\n\n    # first pass to determine shape and resolved type\n    arrays_iter = iter(arrays)\n    first = next(arrays_iter)\n\n    ndim = first.ndim\n    dt_resolve = first.dtype\n    shape = list(first.shape)\n\n    for array in arrays_iter:\n        if dt_resolve != DTYPE_OBJECT:\n            dt_resolve = resolve_dtype(array.dtype, dt_resolve)\n        shape[axis] += array.shape[axis]\n\n    out = np.empty(shape=shape, dtype=dt_resolve)\n    np.concatenate(arrays, out=out, axis=axis)\n    out.flags.writeable = False\n    return out\n\n\ndef full_for_fill(\n        dtype: np.dtype,\n        shape: tp.Union[int, tp.Tuple[int, ...]],\n        fill_value: object) -> np.ndarray:\n    \'\'\'\n    Return a ""full"" NP array for the given fill_value\n    Args:\n        dtype: target dtype, which may or may not be possible given the fill_value.\n    \'\'\'\n    dtype = resolve_dtype(dtype, np.array(fill_value).dtype)\n    return np.full(shape, fill_value, dtype=dtype)\n\n\ndef dtype_to_na(dtype: DtypeSpecifier) -> tp.Any:\n    \'\'\'Given a dtype, return an appropriate and compatible null value.\n    \'\'\'\n    if not isinstance(dtype, np.dtype):\n        # we permit things like object, float, etc.\n        dtype = np.dtype(dtype)\n\n    kind = dtype.kind\n\n    if kind in DTYPE_INT_KIND:\n        return 0 # cannot support NaN\n    elif kind == \'b\':\n        return False\n    elif kind in DTYPE_NAN_KIND:\n        return np.nan\n    elif kind == \'O\':\n        return None\n    elif kind in DTYPE_STR_KIND:\n        return \'\'\n    elif kind in DTYPE_DATETIME_KIND:\n        return NAT\n    elif kind in DTYPE_TIMEDELTA_KIND:\n        return EMPTY_TIMEDELTA\n\n    raise NotImplementedError(\'no support for this dtype\', kind)\n\n\ndef ufunc_axis_skipna(\n        array: np.ndarray,\n        *,\n        skipna: bool,\n        axis: int,\n        ufunc: UFunc,\n        ufunc_skipna: UFunc,\n        out: tp.Optional[np.ndarray]=None\n        ) -> np.ndarray:\n    \'\'\'For ufunc array application, when two ufunc versions are available. Expected to always reduce dimensionality.\n    \'\'\'\n\n    if array.dtype.kind == \'O\':\n        # replace None with nan\n        if skipna:\n            is_not_none = np.not_equal(array, None)\n\n        if array.ndim == 1:\n            if skipna:\n                v = array[is_not_none]\n                if len(v) == 0: # all values were None\n                    return np.nan\n            else:\n                v = array\n        else:\n            # for 2D array, replace None with NaN\n            if skipna:\n                v = array.copy() # already an object type\n                v[~is_not_none] = np.nan\n            else:\n                v = array\n\n    elif array.dtype.kind == \'M\' or array.dtype.kind == \'m\':\n        # dates do not support skipna functions\n        return ufunc(array, axis=axis, out=out)\n\n    elif array.dtype.kind in DTYPE_STR_KIND and ufunc in UFUNC_AXIS_STR_TO_OBJ:\n        v = array.astype(object)\n    else:\n        v = array\n\n    if skipna:\n        return ufunc_skipna(v, axis=axis, out=out)\n    return ufunc(v, axis=axis, out=out)\n\n\ndef ufunc_unique(\n        array: np.ndarray,\n        *,\n        axis: tp.Optional[int] = None,\n        non_array_type: type = frozenset,\n        ) -> np.ndarray:\n    \'\'\'\n    Extended functionality of the np.unique ufunc, to handle cases of mixed typed objects, where NP will fail in finding unique values for a hetergenous object type.\n\n    Args:\n        non_array_type: for cases where unique will not work, determine type to return. This can be frozenset or a\n    \'\'\'\n    if array.dtype.kind == \'O\':\n        if axis is None or array.ndim < 2:\n            try:\n                return np.unique(array)\n            except TypeError: # if unorderable types\n                # np.unique will give TypeError: The axis argument to unique is not supported for dtype object\n                pass\n            # this may or may not work, depending on contained types\n            if array.ndim > 1: # axis is None, need to flatten\n                array_iter = array.flat\n            else:\n                array_iter = array\n        else:\n            # ndim == 2 and axis is not None\n            if axis == 0:\n                array_iter = array2d_to_tuples(array)\n            else:\n                array_iter = array2d_to_tuples(array.T)\n\n        # Use a dict to retain order; this will break for non hashables\n        store = dict.fromkeys(array_iter)\n        array = np.empty(len(store), dtype=object)\n        array[:] = tuple(store)\n        return array\n\n    # all other types, use the main ufunc\n    return np.unique(array, axis=axis)\n\n\ndef roll_1d(array: np.ndarray,\n            shift: int\n            ) -> np.ndarray:\n    \'\'\'\n    Specialized form of np.roll that, by focusing on the 1D solution, is at least four times faster.\n    \'\'\'\n    size = len(array)\n    if size <= 1:\n        return array.copy()\n\n    # result will be positive\n    shift = shift % size\n    if shift == 0:\n        return array.copy()\n\n    post = np.empty(size, dtype=array.dtype)\n\n    post[0:shift] = array[-shift:]\n    post[shift:] = array[0:-shift]\n    return post\n\n\ndef roll_2d(array: np.ndarray,\n            shift: int,\n            axis: int\n            ) -> np.ndarray:\n    \'\'\'\n    Specialized form of np.roll that, by focusing on the 2D solution\n    \'\'\'\n    post = np.empty(array.shape, dtype=array.dtype)\n\n    if axis == 0: # roll rows\n        size = array.shape[0]\n        if size <= 1:\n            return array.copy()\n\n        # result will be positive\n        shift = shift % size\n        if shift == 0:\n            return array.copy()\n\n        post[0:shift, :] = array[-shift:, :]\n        post[shift:, :] = array[0:-shift, :]\n        return post\n\n    elif axis == 1: # roll columns\n        size = array.shape[1]\n        if size <= 1:\n            return array.copy()\n\n        # result will be positive\n        shift = shift % size\n        if shift == 0:\n            return array.copy()\n\n        post[:, 0:shift] = array[:, -shift:]\n        post[:, shift:] = array[:, 0:-shift]\n        return post\n\n    raise NotImplementedError()\n\n#-------------------------------------------------------------------------------\n\ndef _argminmax_1d(\n        array: np.ndarray,\n        ufunc: UFunc,\n        ufunc_skipna: UFunc,\n        skipna: bool = True,\n        ) -> tp.Any: # tp.Union[int, float]:\n    \'\'\'\n    Perform argmin or argmax, handling NaN as needed.\n    \'\'\'\n    # always need to to check for nans, even if skipna is False, as np will raise if all NaN, and will not return Nan if there skipna is false\n    isna = isna_array(array)\n\n    if isna.all():\n        return np.nan\n\n    if isna.any():\n        if not skipna:\n            return np.nan\n        # always use skipna ufunc if any NaNs are present, as otherwise the wrong indices are returned when a nan is encountered (rather than a nan)\n        return ufunc_skipna(array)\n\n    return ufunc(array)\n\n\nargmin_1d = partial(_argminmax_1d, ufunc=np.argmin, ufunc_skipna=np.nanargmin)\nargmax_1d = partial(_argminmax_1d, ufunc=np.argmax, ufunc_skipna=np.nanargmax)\n\n\ndef _argminmax_2d(\n        array: np.ndarray,\n        ufunc: UFunc,\n        ufunc_skipna: UFunc,\n        skipna: bool = True,\n        axis: int = 0\n        ) -> np.ndarray: # int or float array\n    \'\'\'\n    Perform argmin or argmax, handling NaN as needed.\n    \'\'\'\n    # always need to to check for nans, even if skipna is False, as np will raise if all NaN, and will not return Nan if there skipna is false\n    isna = isna_array(array)\n\n    isna_axis = isna.any(axis=axis)\n    if isna_axis.all(): # nan in every axis remaining position\n        if not skipna:\n            return np.full(isna_axis.shape, np.nan, dtype=DTYPE_FLOAT_DEFAULT)\n\n    if isna_axis.any():\n        # always use skipna ufunc if any NaNs are present, as otherwise the wrong indices are returned when a nan is encountered (rather than a nan)\n        post = ufunc_skipna(array, axis=axis)\n        if not skipna:\n            post = post.astype(DTYPE_FLOAT_DEFAULT  )\n            post[isna_axis] = np.nan\n        return post\n\n    return ufunc(array, axis=axis)\n\nargmin_2d = partial(_argminmax_2d, ufunc=np.argmin, ufunc_skipna=np.nanargmin)\nargmax_2d = partial(_argminmax_2d, ufunc=np.argmax, ufunc_skipna=np.nanargmax)\n\n#-------------------------------------------------------------------------------\n# array constructors\n\n\ndef is_gen_copy_values(values: tp.Iterable[tp.Any]) -> tp.Tuple[bool, bool]:\n    \'\'\'\n    Returns:\n        copy_values: True if values cannot be used in an np.array constructor.\n    \'\'\'\n    is_gen = not hasattr(values, \'__len__\')\n    copy_values = is_gen\n    if not is_gen:\n        is_dictlike = isinstance(values, DICTLIKE_TYPES)\n        copy_values |= is_dictlike\n        if not is_dictlike:\n            is_iifa = isinstance(values, INVALID_ITERABLE_FOR_ARRAY)\n            copy_values |= is_iifa\n    return is_gen, copy_values\n\n\ndef resolve_type_iter(\n        values: tp.Iterable[tp.Any],\n        restrict_copy: bool = False\n        ) -> tp.Tuple[DtypeSpecifier, bool, tp.Sequence[tp.Any]]:\n    \'\'\'\n    Determine an appropriate DtypeSpecifier for values in an iterable. This does not try to determine the actual dtype, but instead, if the DtypeSpecifier needs to be object rather than None (which lets NumPy auto detect). This is expected to only operate on 1D data.\n\n    Args:\n        values: can be a generator that will be exhausted in processing; if a generator, a copy will be made and returned as values.\n        restrict_copy: if True, reject making a copy, even if a generator is given\n    Returns:\n        resolved, has_tuple, values\n    \'\'\'\n\n    is_gen, copy_values = is_gen_copy_values(values)\n\n    if not is_gen and len(values) == 0: #type: ignore\n        return None, False, values #type: ignore\n\n    if restrict_copy:\n        copy_values = False\n\n    v_iter = iter(values)\n\n    if copy_values:\n        values_post = []\n\n    resolved = None # None is valid specifier if the type is not ambiguous\n    has_tuple = False\n    has_str = False\n    has_enum = False\n    has_non_str = False\n    has_inexact = False\n    has_big_int = False\n\n    for v in v_iter:\n        if copy_values:\n            # if a generator, have to make a copy while iterating\n            # for array construcdtion, cannot use dictlike, so must convert to list\n            values_post.append(v)\n\n        if resolved != object:\n            value_type = type(v)\n\n            # need to get tuple subclasses, like NamedTuple\n            if isinstance(v, (tuple, list)):\n                has_tuple = True\n            elif isinstance(v, Enum):\n                # must check isinstance, as Enum types are alwyas derived from Enum\n                has_enum = True\n            elif value_type == str or value_type == np.str_:\n                # must compare to both sring types\n                has_str = True\n            else:\n                has_non_str = True\n                if value_type in INEXACT_TYPES:\n                    has_inexact = True\n                elif value_type == int and abs(v) > INT_MAX_COERCIBLE_TO_FLOAT:\n                    has_big_int = True\n\n            if has_tuple or has_enum or (has_str and has_non_str):\n                resolved = object\n            elif has_big_int and has_inexact:\n                resolved = object\n\n        else: # resolved is object, can exit\n            if copy_values:\n                values_post.extend(v_iter)\n            break\n\n    # NOTE: we break before finding a tuple, but our treatment of object types, downstream, will always assign them in the appropriate way\n    if copy_values:\n        return resolved, has_tuple, values_post\n\n    return resolved, has_tuple, values #type: ignore\n\n\n\n\ndef iterable_to_array_1d(\n        values: tp.Iterable[tp.Any],\n        dtype: DtypeSpecifier=None\n        ) -> tp.Tuple[np.ndarray, bool]:\n    \'\'\'\n    Convert an arbitrary Python iterable to a 1D NumPy array without any undesirable type coercion.\n\n    Returns:\n        pair of array, Boolean, where the Boolean can be used when necessary to establish uniqueness.\n    \'\'\'\n    if isinstance(values, np.ndarray):\n        if dtype is not None and dtype != values.dtype:\n            raise RuntimeError(f\'Supplied dtype {dtype} not set on supplied array.\')\n        return values, len(values) <= 1\n\n    if isinstance(values, range):\n        # translate range to np.arange to avoid iteration\n        array = np.arange(start=values.start,\n                stop=values.stop,\n                step=values.step,\n                dtype=dtype)\n        array.flags.writeable = False\n        return array, True\n\n    if hasattr(values, \'values\') and hasattr(values, \'index\'):\n        raise RuntimeError(f\'Supplied iterable {type(values)} appears to be labeled, though labels are not being used. Convert to a Series.\')\n\n    values_for_construct: tp.Sequence[tp.Any]\n\n    # values for construct will only be a copy when necessary in iteration to find type\n    if isinstance(values, str):\n        # if a we get a single string, it is an iterable of characters, but if given to NumPy will produce an array of a single element; here, we convert it to an iterable of a single element; let dtype argument (if passed or None) be used in creation, below\n        has_tuple = False\n        values_for_construct = (values,)\n    elif dtype is None:\n        # this gives as dtype only None, or object, letting array constructor do the rest\n        dtype, has_tuple, values_for_construct = resolve_type_iter(values)\n        if len(values_for_construct) == 0:\n            return EMPTY_ARRAY, True # no dtype given, so return empty float array\n    else:\n        is_gen, copy_values = is_gen_copy_values(values)\n        if copy_values:\n            # we have to realize into sequence for numpy creation\n            values_for_construct = tuple(values)\n        else:\n            values_for_construct = values #type: ignore\n\n        if len(values_for_construct) == 0:\n            # dtype was given, return an empty array with that dtype\n            v = np.empty(0, dtype=dtype)\n            v.flags.writeable = False\n            return v, True\n\n        #as we have not iterated iterable, assume that there might be tuples if the dtype is object\n        has_tuple = dtype in DTYPE_SPECIFIERS_OBJECT\n\n    if len(values_for_construct) == 1 or isinstance(values, DICTLIKE_TYPES):\n        # check values for dictlike, not values_for_construct\n        is_unique = True\n    else:\n        is_unique = False\n\n    # construction\n    if has_tuple:\n        # this matches cases where dtype is given and dtype is an object specifier\n        # this is the only way to assign from a sequence that contains a tuple; this does not work for dict or set (they must be copied into an iterabel), and is little slower than creating array directly\n        v = np.empty(len(values_for_construct), dtype=DTYPE_OBJECT)\n        v[NULL_SLICE] = values_for_construct\n    elif dtype == int:\n        # large python ints can overflow default NumPy int type\n        try:\n            v = np.array(values_for_construct, dtype=dtype)\n        except OverflowError:\n            v = np.array(values_for_construct, dtype=DTYPE_OBJECT)\n    else:\n        # if dtype was None, we might have discovered this was object but has no tuples; faster to do this constructor instead of null slice assignment\n        v = np.array(values_for_construct, dtype=dtype)\n\n    v.flags.writeable = False\n    return v, is_unique\n\n\ndef iterable_to_array_2d(\n        values: tp.Iterable[tp.Iterable[tp.Any]],\n        ) -> np.ndarray:\n    \'\'\'\n    Convert an arbitrary Python iterable of iterables to a 2D NumPy array without any undesirable type coercion. Useful IndexHierarchy construction.\n\n    Returns:\n        pair of array, Boolean, where the Boolean can be used when necessary to establish uniqueness.\n    \'\'\'\n    if isinstance(values, np.ndarray):\n        # could check for ndim==2\n        return values\n\n    # consume values into a tuple\n    values = tuple(values)\n\n    # if we provide whole generator to resolve_type_iter, it will copy the entire sequence unless restrrict copy is True\n    dtype, _, _ = resolve_type_iter(\n            (y for z in values for y in z),\n            restrict_copy=True\n            )\n\n    array = np.array(values, dtype=dtype)\n    if array.ndim != 2:\n        raise RuntimeError(\'failed to convert iterable to 2d array\')\n\n    array.flags.writeable = False\n    return array\n\ndef iterable_to_array_nd(\n        values: tp.Any,\n        ) -> np.ndarray:\n    \'\'\'\n    Attempt to determiine if a value is 0, 1, or 2D array; this will interpret lists of tuples as 2D, as NumPy does.\n    \'\'\'\n    if hasattr(values, \'__iter__\') and not isinstance(values, str):\n\n        values = iter(values)\n        try:\n            first = next(values)\n        except StopIteration:\n            return EMPTY_ARRAY\n\n        if hasattr(first, \'__iter__\') and not isinstance(first, str):\n            return iterable_to_array_2d(chain((first,), values))\n\n        array, _ = iterable_to_array_1d(chain((first,), values))\n        return array\n    # its an element\n    return np.array(values)\n\n#-------------------------------------------------------------------------------\n\ndef slice_to_ascending_slice(\n        key: slice,\n        size: int\n        ) -> slice:\n    \'\'\'\n    Given a slice, return a slice that, with ascending integers, covers the same values.\n\n    Args:\n        size: the length of the container on this axis\n    \'\'\'\n    # NOTE: a slice can have start > stop, and None as step: should that case be handled here?\n\n    if key.step is None or key.step > 0:\n        return key\n\n    stop = key.start if key.start is None else key.start + 1\n\n    if key.step == -1:\n        # if 6, 1, -1, then\n        start = key.stop if key.stop is None else key.stop + 1\n        return slice(start, stop, 1)\n\n    # if 6, 1, -2: 6, 4, 2; then\n    start = next(reversed(range(*key.indices(size))))\n    return slice(start, stop, -key.step)\n\ndef slice_to_inclusive_slice(key: slice) -> slice:\n    \'\'\'Make a stop exclusive key inclusive by adding one to the stop value.\n    \'\'\'\n    stop = None if key.stop is None else key.stop + 1\n    return slice(key.start, stop, key.step)\n\n\n\n\n#-------------------------------------------------------------------------------\n# dates\n\nDT64_YEAR = np.dtype(\'datetime64[Y]\')\nDT64_MONTH = np.dtype(\'datetime64[M]\')\nDT64_DAY = np.dtype(\'datetime64[D]\')\nDT64_H = np.dtype(\'datetime64[h]\')\nDT64_M = np.dtype(\'datetime64[m]\')\nDT64_S = np.dtype(\'datetime64[s]\')\nDT64_MS = np.dtype(\'datetime64[ms]\')\nDT64_US = np.dtype(\'datetime64[us]\')\nDT64_NS = np.dtype(\'datetime64[ns]\')\nDT64_PS = np.dtype(\'datetime64[ps]\')\nDT64_FS = np.dtype(\'datetime64[fs]\')\nDT64_AS = np.dtype(\'datetime64[as]\')\n\nTD64_YEAR = np.timedelta64(1, \'Y\')\nTD64_MONTH = np.timedelta64(1, \'M\')\nTD64_DAY = np.timedelta64(1, \'D\')\nTD64_H = np.timedelta64(1, \'h\')\nTD64_M = np.timedelta64(1, \'m\')\nTD64_S = np.timedelta64(1, \'s\')\nTD64_MS = np.timedelta64(1, \'ms\')\nTD64_US = np.timedelta64(1, \'us\')\nTD64_NS = np.timedelta64(1, \'ns\')\n\n_DT_NOT_FROM_INT = (DT64_DAY, DT64_MONTH)\n\ndef to_datetime64(\n        value: DateInitializer,\n        dtype: tp.Optional[np.dtype] = None\n        ) -> np.datetime64:\n    \'\'\'\n    Convert a value ot a datetime64; this must be a datetime64 so as to be hashable.\n    \'\'\'\n    # for now, only support creating from a string, as creation from integers is based on offset from epoch\n    if not isinstance(value, np.datetime64):\n        if dtype is None:\n            # let constructor figure it out\n            dt = np.datetime64(value)\n        else: # assume value is single value;\n            # note that integers will be converted to units from epoch\n            if isinstance(value, int):\n                if dtype == DT64_YEAR:\n                    # convert to string as that is likely what is wanted\n                    value = str(value)\n                elif dtype in _DT_NOT_FROM_INT:\n                    raise RuntimeError(\'attempting to create {} from an integer, which is generally not desired as the result will be offset from the epoch.\'.format(dtype))\n            # cannot use the datetime directly\n            if dtype != np.datetime64:\n                dt = np.datetime64(value, np.datetime_data(dtype)[0])\n            else: # cannot use a generic datetime type\n                dt = np.datetime64(value)\n    else: # if a dtype was explicitly given, check it\n        # value is an instance of a datetime64, and has a dtype attr\n        dt = value\n        if dtype:\n            # dtype can be either generic, or a matching specific dtype\n            if dtype != np.datetime64 and dtype != dt.dtype:\n                raise RuntimeError(f\'value ({dt}) is not a supported dtype ({dtype})\')\n    return dt\n\ndef to_timedelta64(value: datetime.timedelta) -> np.timedelta64:\n    \'\'\'\n    Convert a datetime.timedelta into a NumPy timedelta64. This approach is better than using np.timedelta64(value), as that reduces all values to microseconds.\n    \'\'\'\n    return reduce(operator.add,\n        (np.timedelta64(getattr(value, attr), code) for attr, code in TIME_DELTA_ATTR_MAP if getattr(value, attr) > 0))\n\ndef _slice_to_datetime_slice_args(key: slice,\n        dtype: tp.Optional[np.dtype] = None\n        ) -> tp.Iterator[tp.Optional[np.datetime64]]:\n    \'\'\'\n    Given a slice representing a datetime region, convert to arguments for a new slice, possibly using the appropriate dtype for conversion.\n    \'\'\'\n    for attr in SLICE_ATTRS:\n        value = getattr(key, attr)\n        if value is None:\n            yield None\n        elif attr == SLICE_STEP_ATTR:\n            # steps are never transformed\n            yield value\n        else:\n            yield to_datetime64(value, dtype=dtype)\n\ndef key_to_datetime_key(\n        key: GetItemKeyType,\n        dtype: np.dtype = np.datetime64,\n        ) -> GetItemKeyType:\n    \'\'\'\n    Given an get item key for a Date index, convert it to np.datetime64 representation.\n    \'\'\'\n    if isinstance(key, slice):\n        return slice(*_slice_to_datetime_slice_args(key, dtype=dtype))\n\n    if isinstance(key, (datetime.date, datetime.datetime)):\n        return np.datetime64(key)\n\n    if isinstance(key, np.datetime64):\n        return key\n\n    if isinstance(key, str):\n        return to_datetime64(key, dtype=dtype)\n\n    if isinstance(key, np.ndarray):\n        if key.dtype.kind == \'b\' or key.dtype.kind == \'M\':\n            return key\n        return key.astype(dtype)\n\n    if hasattr(key, \'__len__\'):\n        # use dtype via array constructor to determine type; or just use datetime64 to parse to the passed-in representation\n        return np.array(key, dtype=dtype)\n\n    if hasattr(key, \'__next__\'): # a generator-like\n        return np.array(tuple(key), dtype=dtype) #type: ignore\n\n    # for now, return key unaltered\n    return key\n\n#-------------------------------------------------------------------------------\n\ndef array_to_groups_and_locations(\n        array: np.ndarray,\n        unique_axis: tp.Optional[int] = 0) -> tp.Tuple[np.ndarray, np.ndarray]:\n    \'\'\'Locations are index positions for each group.\n    \'\'\'\n    try:\n        groups, locations = np.unique(\n                array,\n                return_inverse=True,\n                axis=unique_axis)\n    except TypeError:\n        # group by string representations, necessary when types are not comparable\n        _, group_index, locations = np.unique(\n                array.astype(str),\n                return_index=True,\n                return_inverse=True,\n                axis=unique_axis)\n        # groups here are the strings; need to restore to values\n        groups = array[group_index]\n\n    return groups, locations\n\n\ndef isna_element(value: tp.Any) -> bool:\n    \'\'\'Return Boolean if value is an NA. This does not yet handle pd.NA\n    \'\'\'\n    try:\n        return np.isnan(value) #type: ignore\n    except TypeError:\n        pass\n    try:\n        return np.isnat(value) #type: ignore\n    except TypeError:\n        pass\n    return value is None\n\n\ndef isna_array(array: np.ndarray,\n        include_none: bool = True,\n        ) -> np.ndarray:\n    \'\'\'Given an np.ndarray, return a bolean array setting True for missing values.\n\n    Note: the returned array is not made immutable.\n    \'\'\'\n    kind = array.dtype.kind\n    # matches all floating point types\n    if kind in DTYPE_NAN_KIND:\n        return np.isnan(array)\n    elif kind in DTYPE_NAT_KIND:\n        return np.isnat(array)\n    # match everything that is not an object; options are: biufcmMOSUV\n    elif kind != \'O\':\n        return np.full(array.shape, False, dtype=bool)\n    # only check for None if we have an object type\n    # NOTE: this will not work for Frames contained within a Series\n    if include_none:\n        return np.not_equal(array, array) | np.equal(array, None)\n    return np.not_equal(array, array)\n\n\ndef binary_transition(\n        array: np.ndarray,\n        axis: int = 0\n        ) -> np.ndarray:\n    \'\'\'\n    Given a Boolean 1D array, return the index positions (integers) at False values where that False was previously True, or will be True\n\n    Returns:\n        For a 1D input, a 1D array of integers; for a 2D input, a 1D object array of lists, where each position corresponds to a found index position. Returning a list is undesirable, but more efficient as a list will be neede for selection downstream.\n    \'\'\'\n\n    if len(array) == 0:\n        # NOTE: on some platforms this may not be the same dtype as returned from np.nonzero\n        return EMPTY_ARRAY_INT\n\n    not_array = ~array\n\n    if array.ndim == 1:\n        # non-nan values that go (from left to right) to NaN\n        target_sel_leading = (array ^ roll_1d(array, -1)) & not_array\n        target_sel_leading[-1] = False # wrap around observation invalid\n        # non-nan values that were previously NaN (from left to right)\n        target_sel_trailing = (array ^ roll_1d(array, 1)) & not_array\n        target_sel_trailing[0] = False # wrap around observation invalid\n\n        return np.nonzero(target_sel_leading | target_sel_trailing)[0]\n\n    elif array.ndim == 2:\n        # if axis == 0, we compare rows going down/up, looking at column values\n        # non-nan values that go (from left to right) to NaN\n        target_sel_leading = (array ^ roll_2d(array, -1, axis=axis)) & not_array\n        # non-nan values that were previously NaN (from left to right)\n        target_sel_trailing = (array ^ roll_2d(array, 1, axis=axis)) & not_array\n\n        # wrap around observation invalid\n        if axis == 0:\n            # process an entire row\n            target_sel_leading[-1, :] = False\n            target_sel_trailing[0, :] = False\n        else:\n            # process entire column\n            target_sel_leading[:, -1] = False\n            target_sel_trailing[:, 0] = False\n\n        # this dictionary could be very sparse compared to axis dimensionality\n        indices_by_axis: tp.DefaultDict[int, tp.List[int]] = defaultdict(list)\n        for y, x in zip(*np.nonzero(target_sel_leading | target_sel_trailing)):\n            if axis == 0:\n                # store many rows values for each column\n                indices_by_axis[x].append(y)\n            else:\n                indices_by_axis[y].append(x)\n\n        # if axis is 0, return column width, else return row height\n        post = np.empty(dtype=object, shape=array.shape[not axis])\n        for k, v in indices_by_axis.items():\n            post[k] = v\n\n        return post\n\n    raise NotImplementedError(f\'no handling for array with ndim: {array.ndim}\')\n\n\n#-------------------------------------------------------------------------------\n# tools for handling duplicates\n\ndef _array_to_duplicated_hashable(\n        array: np.ndarray,\n        axis: int = 0,\n        exclude_first: bool = False,\n        exclude_last: bool = False) -> np.ndarray:\n    \'\'\'\n    Algorithm for finding duplicates in unsortable arrays for hashables. This will always be an object array.\n    \'\'\'\n    # np.unique fails under the same conditions that sorting fails, so there is no need to try np.unique: must go to set drectly.\n    len_axis = array.shape[axis]\n\n    if array.ndim == 1:\n        value_source = array\n        to_hashable = None\n    else:\n        if axis == 0:\n            value_source = array # will iterate rows\n        else:\n            value_source = (array[:, i] for i in range(len_axis))\n        # values will be arrays; must convert to tuples to make hashable\n        to_hashable = tuple\n\n\n    is_dupe = np.full(len_axis, False)\n\n    # could exit early with a set, but would have to hash all array twice to go to set and dictionary\n    # creating a list for each entry and tracking indices would be very expensive\n\n    unique_to_first: tp.Dict[tp.Hashable, int] = {} # value to first occurence\n    dupe_to_first: tp.Dict[tp.Hashable, int] = {}\n    dupe_to_last: tp.Dict[tp.Hashable, int] = {}\n\n    for idx, v in enumerate(value_source):\n\n        if to_hashable:\n            v = to_hashable(v)\n\n        if v not in unique_to_first:\n            unique_to_first[v] = idx\n        else:\n            # v has been seen before; upate Boolean array\n            is_dupe[idx] = True\n\n            # if no entry in dupe to first, no update with value in unique to first, which is the index this values was first seen\n            if v not in dupe_to_first:\n                dupe_to_first[v] = unique_to_first[v]\n            # always update last\n            dupe_to_last[v] = idx\n\n    if exclude_last: # overwrite with False\n        is_dupe[list(dupe_to_last.values())] = False\n\n    if not exclude_first: # add in first values\n        is_dupe[list(dupe_to_first.values())] = True\n\n    return is_dupe\n\n\ndef _array_to_duplicated_sortable(\n        array: np.ndarray,\n        axis: int = 0,\n        exclude_first: bool = False,\n        exclude_last: bool = False) -> np.ndarray:\n    \'\'\'\n    Algorithm for finding duplicates in sortable arrays. This may or may not be an object array, as some object arrays (those of compatible types) are sortable.\n    \'\'\'\n    # based in part on https://stackoverflow.com/questions/11528078/determining-duplicate-values-in-an-array\n    # https://stackoverflow.com/a/43033882/388739\n    # indices to sort and sorted array\n    # a right roll on the sorted array, comparing to the original sorted array. creates a boolean array, with all non-first duplicates marked as True\n\n    # NOTE: this is not compatible with heterogenous typed object arrays, raises TypeError\n\n    if array.ndim == 1:\n        o_idx = np.argsort(array, axis=None, kind=DEFAULT_STABLE_SORT_KIND)\n        array_sorted = array[o_idx]\n        opposite_axis = 0\n        # f_flags is True where there are duplicated values in the sorted array\n        f_flags = array_sorted == roll_1d(array_sorted, 1)\n    else:\n        if axis == 0: # sort rows\n            # first should be last\n            arg = [array[:, x] for x in range(array.shape[1] - 1, -1, -1)]\n            o_idx = np.lexsort(arg)\n            array_sorted = array[o_idx]\n        elif axis == 1: # sort columns\n            arg = [array[x] for x in range(array.shape[0] - 1, -1, -1)]\n            o_idx = np.lexsort(arg)\n            array_sorted = array[:, o_idx]\n        else:\n            raise NotImplementedError(f\'no handling for axis: {axis}\')\n\n        opposite_axis = int(not bool(axis))\n        # rolling axis 1 rotates columns; roll axis 0 rotates rows\n        match = array_sorted == roll_2d(array_sorted, 1, axis=axis)\n        f_flags = match.all(axis=opposite_axis)\n\n    if not f_flags.any():\n        # we always return a 1 dim array\n        return np.full(len(f_flags), False)\n\n    # The first element of f_flags should always be False.\n    # In certain edge cases, this doesn\'t happen naturally.\n    # Index 0 should always exist, due to `.any()` behavior.\n    f_flags[0] = np.False_\n\n    if exclude_first and not exclude_last:\n        dupes = f_flags\n    else:\n        # non-LAST duplicates is a left roll of the non-first flags.\n        l_flags = roll_1d(f_flags, -1)\n\n        if not exclude_first and exclude_last:\n            dupes = l_flags\n        elif not exclude_first and not exclude_last:\n            # all duplicates is the union.\n            dupes = f_flags | l_flags\n        else:\n            # all non-first, non-last duplicates is the intersection.\n            dupes = f_flags & l_flags\n\n    # undo the sort: get the indices to extract Booleans from dupes; in some cases r_idx is the same as o_idx, but not all\n    r_idx = np.argsort(o_idx, axis=None, kind=DEFAULT_STABLE_SORT_KIND)\n    return dupes[r_idx]\n\n\n\ndef array_to_duplicated(\n        array: np.ndarray,\n        axis: int = 0,\n        exclude_first: bool = False,\n        exclude_last: bool = False) -> np.ndarray:\n    \'\'\'Given a numpy array (1D or 2D), return a Boolean array along the specified axis that shows which values are duplicated. By default, all duplicates are indicated. For 2d arrays, axis 0 compares rows and returns a row-length Boolean array; axis 1 compares columns and returns a column-length Boolean array.\n\n    Args:\n        exclude_first: Mark as True all duplicates except the first encountared.\n        exclude_last: Mark as True all duplicates except the last encountared.\n    \'\'\'\n    try:\n        return _array_to_duplicated_sortable(\n                array=array,\n                axis=axis,\n                exclude_first=exclude_first,\n                exclude_last=exclude_last\n                )\n    except TypeError: # raised if not sorted\n        return _array_to_duplicated_hashable(\n                array=array,\n                axis=axis,\n                exclude_first=exclude_first,\n                exclude_last=exclude_last\n                )\n\n\n#-------------------------------------------------------------------------------\ndef array_shift(*,\n        array: np.ndarray,\n        shift: int,\n        axis: int, # 0 is rows, 1 is columns\n        wrap: bool,\n        fill_value: tp.Any = np.nan) -> np.ndarray:\n    \'\'\'\n    Apply an np-style roll to a 1D or 2D array; if wrap is False, fill values out-shifted values with fill_value.\n\n    Args:\n        fill_value: only used if wrap is False.\n    \'\'\'\n\n    # works for all shapes\n    if shift > 0:\n        shift_mod = shift % array.shape[axis]\n    elif shift < 0:\n        # do negative modulo to force negative value\n        shift_mod = shift % -array.shape[axis]\n    else:\n        shift_mod = 0\n\n    if (not wrap and shift == 0) or (wrap and shift_mod == 0):\n        # must copy so as not let caller mutate arguement\n        return array.copy()\n\n    if wrap:\n        # roll functions will handle finding noop rolls\n        if array.ndim == 1:\n            return roll_1d(array, shift_mod)\n        return roll_2d(array, shift_mod, axis=axis)\n\n    # will insure that the result can contain the fill and the original values\n    result = full_for_fill(array.dtype, array.shape, fill_value)\n\n    if axis == 0:\n        if shift > 0:\n            result[shift:] = array[:-shift]\n        elif shift < 0:\n            result[:shift] = array[-shift:]\n    elif axis == 1:\n        if shift > 0:\n            result[:, shift:] = array[:, :-shift]\n        elif shift < 0:\n            result[:, :shift] = array[:, -shift:]\n\n    return result\n\ndef array2d_to_tuples(array: np.ndarray) -> tp.Iterator[tp.Tuple[tp.Any, ...]]:\n    for row in array: # assuming 2d\n        yield tuple(row)\n\n#-------------------------------------------------------------------------------\n# extension to union and intersection handling\n\ndef _ufunc_set_1d(\n        func: tp.Callable[[np.ndarray, np.ndarray], np.ndarray],\n        array: np.ndarray,\n        other: np.ndarray,\n        *,\n        assume_unique: bool=False\n        ) -> np.ndarray:\n    \'\'\'\n    Peform 1D set operations. When possible, short-circuit comparison and return array with original order.\n\n    Args:\n        assume_unique: if arguments are assumed unique, can implement optional identity filtering, which retains order (un sorted) for opperands that are equal. This is important in numerous operations on the matching Indices where order should not be perterbed.\n    \'\'\'\n    is_union = func == np.union1d\n    is_intersection = func == np.intersect1d\n    is_difference = func == np.setdiff1d\n\n    if not (is_union or is_intersection or is_difference):\n        raise NotImplementedError(\'unexpected func\', func)\n\n    dtype = resolve_dtype(array.dtype, other.dtype)\n\n    # optimizations for empty arrays\n    if is_intersection:\n        if len(array) == 0 or len(other) == 0:\n            # not sure what DTYPE is correct to return here\n            return np.array(EMPTY_TUPLE, dtype=dtype)\n    elif is_difference:\n        if len(array) == 0:\n            return np.array(EMPTY_TUPLE, dtype=dtype)\n\n    if assume_unique:\n        # can only return arguments, and use length to determine unique comparison condition, if arguments are assumed to already be unique\n        if is_union:\n            if len(array) == 0:\n                return other\n            elif len(other) == 0:\n                return array\n        elif is_difference:\n            if len(other) == 0:\n                return array\n\n        if len(array) == len(other):\n            arrays_are_equal = False\n            compare = array == other\n\n            # if sizes are the same, the result of == is mostly a bool array; comparison to some arrays (e.g. string), will result in a single Boolean, but it should always be False\n            if isinstance(compare, BOOL_TYPES) and compare:\n                arrays_are_equal = True #pragma: no cover\n            elif isinstance(compare, np.ndarray) and compare.all(axis=None):\n                arrays_are_equal = True\n            if arrays_are_equal:\n                if is_difference:\n                    return np.array(EMPTY_TUPLE, dtype=dtype)\n                else:\n                    return array\n\n    set_compare = False\n    array_is_str = array.dtype.kind in DTYPE_STR_KIND\n    other_is_str = other.dtype.kind in DTYPE_STR_KIND\n\n    if array_is_str ^ other_is_str:\n        # if only one is string\n        set_compare = True\n\n    if set_compare or dtype.kind == \'O\':\n        if is_union:\n            result = frozenset(array) | frozenset(other)\n        elif is_intersection:\n            result = frozenset(array) & frozenset(other)\n        else:\n            result = frozenset(array).difference(frozenset(other))\n        # NOTE: try to sort, as set ordering is not stable\n        try:\n            result = sorted(result) #type: ignore\n        except TypeError:\n            pass\n        v, _ = iterable_to_array_1d(result, dtype)\n        return v\n\n    if is_union:\n        return func(array, other)\n    return func(array, other, assume_unique=assume_unique) #type: ignore\n\ndef _ufunc_set_2d(\n        func: tp.Callable[[np.ndarray, np.ndarray], np.ndarray],\n        array: np.ndarray,\n        other: np.ndarray,\n        *,\n        assume_unique: bool=False\n        ) -> np.ndarray:\n    \'\'\'\n    Peform 2D set operations. When possible, short-circuit comparison and return array with original order.\n\n    Args:\n        func: a 1d set operation\n        array: can be a 2D array, or a 1D object array of tuples.\n        other: can be a 2D array, or a 1D object array of tuples.\n        assume_unique: if True, array operands are assumed unique and order is preserved for matching operands.\n    Returns:\n        Either a 2D array, or a 1D object array of tuples.\n    \'\'\'\n    is_union = func == np.union1d\n    is_intersection = func == np.intersect1d\n    is_difference = func == np.setdiff1d\n\n    if not (is_union or is_intersection or is_difference):\n        raise NotImplementedError(\'unexpected func\', func)\n\n    # if either are object, or combination resovle to object, get object\n    dtype = resolve_dtype(array.dtype, other.dtype)\n\n    # optimizations for empty arrays\n    if is_intersection: # intersection with empty\n        if len(array) == 0 or len(other) == 0:\n            # not sure what DTYPE is correct to return here\n            return np.array(EMPTY_TUPLE, dtype=dtype)\n    elif is_difference:\n        if len(array) == 0:\n            return np.array(EMPTY_TUPLE, dtype=dtype)\n\n    if assume_unique:\n        # can only return arguments, and use length to determine unique comparison condition, if arguments are assumed to already be unique\n        if is_union:\n            if len(array) == 0:\n                return other\n            elif len(other) == 0:\n                return array\n        elif is_difference:\n            if len(other) == 0:\n                return array\n\n        if array.shape == other.shape:\n            arrays_are_equal = False\n            compare = array == other\n\n            # will not match a 2D array of integers and 1D array of tuples containing integers (would have to do a post-set comparison, but would loose order)\n            if isinstance(compare, BOOL_TYPES) and compare:\n                arrays_are_equal = True #pragma: no cover\n            elif isinstance(compare, np.ndarray) and compare.all(axis=None):\n                arrays_are_equal = True\n\n            if arrays_are_equal:\n                if is_difference:\n                    return np.array(EMPTY_TUPLE, dtype=dtype)\n                else:\n                    return array\n\n    if dtype.kind == \'O\':\n        # assume that 1D arrays arrays are arrays of tuples\n        if array.ndim == 1:\n            array_set = frozenset(array)\n        else: # assume row-wise comparison\n            array_set = frozenset(tuple(row) for row in array)\n\n        if other.ndim == 1:\n            other_set = frozenset(other)\n        else: # assume row-wise comparison\n            other_set = frozenset(tuple(row) for row in other)\n\n        if is_union:\n            result = array_set | other_set\n        elif is_intersection:\n            result = array_set & other_set\n        else:\n            result = array_set.difference(other_set)\n\n        # NOTE: this sort may not always be succesful\n        try:\n            values: tp.Sequence[tp.Tuple[tp.Hashable, ...]] = sorted(result)\n        except TypeError:\n            values = tuple(result)\n\n        # returns a 1D object array of tuples\n        post = np.empty(len(values), dtype=object)\n        post[:] = values\n        return post\n\n    # from here, we assume we have two 2D arrays\n    if array.ndim != 2 or other.ndim != 2:\n        raise RuntimeError(\'non-object arrays have to both be 2D\')\n\n    # number of columns must be the same, as doing row-wise comparison, and determines the length of each row\n    assert array.shape[1] == other.shape[1]\n    width = array.shape[1]\n\n    if array.dtype != dtype:\n        array = array.astype(dtype)\n    if other.dtype != dtype:\n        other = other.astype(dtype)\n\n    func_kwargs = {} if is_union else dict(assume_unique=assume_unique)\n\n    if width == 1:\n        # let the function flatten the array, then reshape into 2D\n        post = func(array, other, **func_kwargs)  # type: ignore\n        return post.reshape(len(post), width)\n\n    # this approach based on https://stackoverflow.com/questions/9269681/intersection-of-2d-numpy-ndarrays\n    # we can use a the 1D function on the rows, once converted to a structured array\n\n    dtype_view = [(\'\', array.dtype)] * width\n    # creates a view of tuples for 1D operation\n    array_view = array.view(dtype_view)\n    other_view = other.view(dtype_view)\n\n    return func(array_view, other_view, **func_kwargs).view(dtype).reshape(-1, width) # type: ignore\n\n\ndef union1d(array: np.ndarray,\n        other: np.ndarray,\n        assume_unique: bool=False\n        ) -> np.ndarray:\n    \'\'\'\n    Union on 1D array, handling diverse types and short-circuiting to preserve order where appropriate.\n    \'\'\'\n    return _ufunc_set_1d(np.union1d,\n            array,\n            other,\n            assume_unique=assume_unique)\n\ndef intersect1d(\n        array: np.ndarray,\n        other: np.ndarray,\n        assume_unique: bool=False\n        ) -> np.ndarray:\n    \'\'\'\n    Intersect on 1D array, handling diverse types and short-circuiting to preserve order where appropriate.\n    \'\'\'\n    return _ufunc_set_1d(np.intersect1d,\n            array,\n            other,\n            assume_unique=assume_unique)\n\ndef setdiff1d(\n        array: np.ndarray,\n        other: np.ndarray,\n        assume_unique: bool=False\n        ) -> np.ndarray:\n    \'\'\'\n    Difference on 1D array, handling diverse types and short-circuiting to preserve order where appropriate\n    \'\'\'\n    return _ufunc_set_1d(np.setdiff1d,\n        array,\n        other,\n        assume_unique=assume_unique)\n\ndef union2d(\n        array: np.ndarray,\n        other: np.ndarray,\n        *,\n        assume_unique: bool=False\n        ) -> np.ndarray:\n    \'\'\'\n    Union on 2D array, handling diverse types and short-circuiting to preserve order where appropriate.\n    \'\'\'\n    return _ufunc_set_2d(np.union1d,\n            array,\n            other,\n            assume_unique=assume_unique)\n\ndef intersect2d(\n        array: np.ndarray,\n        other: np.ndarray,\n        *,\n        assume_unique: bool=False\n        ) -> np.ndarray:\n    \'\'\'\n    Intersect on 2D array, handling diverse types and short-circuiting to preserve order where appropriate.\n    \'\'\'\n    return _ufunc_set_2d(np.intersect1d,\n            array,\n            other,\n            assume_unique=assume_unique)\n\ndef setdiff2d(\n        array: np.ndarray,\n        other: np.ndarray,\n        *,\n        assume_unique: bool=False\n        ) -> np.ndarray:\n    \'\'\'\n    Difference on 2D array, handling diverse types and short-circuiting to preserve order where appropriate.\n    \'\'\'\n    return _ufunc_set_2d(np.setdiff1d,\n        array,\n        other,\n        assume_unique=assume_unique)\n\ndef ufunc_set_iter(\n        arrays: tp.Iterable[np.ndarray],\n        union: bool = False,\n        *,\n        assume_unique: bool=False\n        ) -> np.ndarray:\n    \'\'\'\n    Iteratively apply a set operation ufunc to 1D or 2D arrays; if all are equal, no operation is performed and order is retained.\n\n    Args:\n        arrays: iterator of arrays; can be a Generator.\n        union: if True, a union is taken, else, an intersection.\n    \'\'\'\n    arrays = iter(arrays)\n    result = next(arrays)\n\n    # will detect ndim by first value, but insure that all other arrays have the same ndim\n    if result.ndim == 1:\n        ufunc = union1d if union else intersect1d\n        ndim = 1\n    else: # ndim == 2\n        ufunc = union2d if union else intersect2d #type: ignore\n        ndim = 2\n\n    for array in arrays:\n        if array.ndim != ndim:\n            raise RuntimeError(\'arrays do not all have the same ndim\')\n\n        # to retain order on identity, assume_unique must be True\n        result = ufunc(result, array, assume_unique=assume_unique)\n\n        if not union and len(result) == 0:\n            # short circuit intersection that results in no common values\n            return result\n\n    return result\n\n\ndef _isin_1d(\n        array: np.ndarray,\n        other: tp.FrozenSet[tp.Any]\n        ) -> np.ndarray:\n    \'\'\'\n    Iterate over an 1D array to build a 1D Boolean ndarray representing whether or not the original element is in the set\n\n    Args:\n        array: The source array\n        other: The set of elements being looked for\n    \'\'\'\n    result: np.ndarray = np.empty(array.shape, dtype=DTYPE_BOOL)\n\n    for i, element in enumerate(array):\n        result[i] = element in other\n\n    return result\n\n\ndef _isin_2d(\n        array: np.ndarray,\n        other: tp.FrozenSet[tp.Any]\n        ) -> np.ndarray:\n    \'\'\'\n    Iterate over an 2D array to build a 2D, immutable, Boolean ndarray representing whether or not the original element is in the set\n\n    Args:\n        array: The source array\n        other: The set of elements being looked for\n    \'\'\'\n    result: np.ndarray = np.empty(array.shape, dtype=DTYPE_BOOL)\n\n    for (i, j), v in np.ndenumerate(array):\n        result[i, j] = v in other\n\n    return result\n\n\ndef isin(\n        array: np.ndarray,\n        other: tp.Iterable[tp.Any],\n        array_is_unique: bool = False,\n        ) -> np.ndarray:\n    \'\'\'\n    Builds a same-size, immutable, Boolean ndarray representing whether or not the original element is in another ndarray\n\n    numpy\'s has very poor isin performance, as it converts both arguments to array-like objects.\n    This implementation optimizes that by converting the lookup argument into a set, providing constant comparison time.\n\n    Args:\n        array: The source array\n        other: The elements being looked for\n        array_is_unique: if array is known to be unique\n    \'\'\'\n    result: tp.Optional[np.ndarray] = None\n\n    if isinstance(other, abc.Sized) and len(other) == 0:\n        result = np.full(array.shape, False, dtype=DTYPE_BOOL)\n        result.flags.writeable = False\n        return result\n\n    other, other_is_unique = iterable_to_array_1d(other)\n\n    if array.dtype == DTYPE_OBJECT or other.dtype == DTYPE_OBJECT: # type: ignore\n        try:\n            if array.ndim == 1:\n                result = _isin_1d(array, frozenset(other))\n            else:\n                result = _isin_2d(array, frozenset(other))\n        except TypeError:\n            # TypeErrors *should* only occur when something is unhashable, hence the inability to use sets. Fall back to numpy\'s isin.\n            pass\n\n    if result is None:\n        assume_unique = array_is_unique and other_is_unique\n        if array.ndim == 1:\n            result = np.in1d(array, other, assume_unique=assume_unique)\n        else:\n            # NOTE: likely faster to do this at the block level\n            result = np.isin(array, other, assume_unique=assume_unique)\n\n    result.flags.writeable = False\n    return result\n\n#-------------------------------------------------------------------------------\n\n\ndef array_from_element_attr(*,\n        array: np.ndarray,\n        attr_name: str,\n        dtype: np.dtype\n        ) -> np.array:\n    \'\'\'\n    Handle element-wise attribute acesss on arrays of Python date/datetime objects.\n    \'\'\'\n    if array.ndim == 1:\n        post = np.fromiter(\n                (getattr(d, attr_name) for d in array),\n                count=len(array),\n                dtype=dtype,\n                )\n    else:\n        post = np.empty(shape=array.shape, dtype=dtype)\n        for iloc, e in np.ndenumerate(array):\n            post[iloc] = getattr(e, attr_name)\n\n    post.flags.writeable = False\n    return post\n\n\ndef array_from_element_method(*,\n        array: np.ndarray,\n        method_name: str,\n        args: tp.Tuple[tp.Any, ...],\n        dtype: np.dtype,\n        pre_insert: tp.Optional[AnyCallable] = None,\n        ) -> np.array:\n    \'\'\'\n    Handle element-wise method calling on arrays of Python date/datetime objects.\n    \'\'\'\n    if dtype == DTYPE_STR:\n        # build into a list first, then construct array to determine size\n        if array.ndim == 1:\n            if pre_insert:\n                proto = [pre_insert(getattr(d, method_name)(*args)) for d in array]\n            else:\n                proto = [getattr(d, method_name)(*args) for d in array]\n        else:\n            proto = [[None for _ in range(array.shape[1])]\n                    for _ in range(array.shape[0])]\n            if pre_insert:\n                for (y, x), e in np.ndenumerate(array):\n                    proto[y][x] = pre_insert(getattr(e, method_name)(*args))\n            else:\n                for (y, x), e in np.ndenumerate(array):\n                    proto[y][x] = getattr(e, method_name)(*args)\n        post = np.array(proto, dtype=dtype)\n\n    else:\n        if array.ndim == 1 and dtype != DTYPE_OBJECT:\n            # NOTE: can I get the method off the clas and pass self\n            if pre_insert:\n                post = np.fromiter(\n                        (pre_insert(getattr(d, method_name)(*args)) for d in array),\n                        count=len(array),\n                        dtype=dtype,\n                        )\n            else:\n                post = np.fromiter(\n                        (getattr(d, method_name)(*args) for d in array),\n                        count=len(array),\n                        dtype=dtype,\n                        )\n        else:\n            post = np.empty(shape=array.shape, dtype=dtype)\n            if pre_insert:\n                for iloc, e in np.ndenumerate(array):\n                    post[iloc] = pre_insert(getattr(e, method_name)(*args))\n            else:\n                for iloc, e in np.ndenumerate(array):\n                    post[iloc] = getattr(e, method_name)(*args)\n\n    post.flags.writeable = False\n    return post\n\n\n\n\n#-------------------------------------------------------------------------------\n\ndef slices_from_targets(\n        target_index: tp.Sequence[int],\n        target_values: tp.Sequence[tp.Any],\n        length: int,\n        directional_forward: bool,\n        limit: int,\n        slice_condition: tp.Callable[[slice], bool]\n        ) -> tp.Iterator[tp.Tuple[slice, tp.Any]]:\n    \'\'\'\n    Utility function used in fillna_directional implementations for Series and Frame. Yields slices and values for setting contiguous ranges of values.\n\n    NOTE: slice_condition is still needed to check if a slice actually has missing values; see if there is a way to determine these cases in advance, so as to not call a function on each slice.\n\n    Args:\n        target_index: iterable of integers, where integers are positions where (as commonly used) values along an axis were previously NA, or will be NA. Often the result of binary_transition()\n        target_values: values found at the index positions\n        length: the maximum length in the target array\n        directional_forward: determine direction\n        limit: set a max size for all slices\n        slice_condition: optional function for filtering slices.\n    \'\'\'\n    if directional_forward:\n        target_slices = (\n                slice(start+1, stop)\n                for start, stop in\n                zip_longest(target_index, target_index[1:], fillvalue=length)\n                )\n    else:\n        # use None to signal first value, but store 0\n        target_slices = (\n                slice((start+1 if start is not None else 0), stop)\n                for start, stop in\n                zip(chain((None,), target_index[:-1]), target_index)\n                )\n\n    for target_slice, value in zip(target_slices, target_values):\n        # asserts not necessary as slices are created above; but mypy needs them\n        assert target_slice.start is not None and target_slice.stop is not None\n\n        # all conditions that are noop slices\n        if target_slice.start == target_slice.stop: #pylint: disable=R1724\n            # matches case where start is 0 and stop is 0\n            continue\n        elif directional_forward and target_slice.start >= length:\n            continue\n\n        # only process if first value of slice is NaN\n        if slice_condition(target_slice):\n\n            if limit > 0:\n                # get the length of the range resulting from the slice; if bigger than limit, reduce the by that amount\n                shift = len(range(*target_slice.indices(length))) - limit\n                if shift > 0:\n\n                    if directional_forward:\n                        target_slice = slice(\n                                target_slice.start,\n                                target_slice.stop - shift)\n                    else:\n                        target_slice = slice(\n                                (target_slice.start or 0) + shift,\n                                target_slice.stop)\n\n            yield target_slice, value\n\n\n\n#-------------------------------------------------------------------------------\n# URL handling, file downloading, file writing\n\ndef path_filter(fp: PathSpecifierOrFileLike) -> tp.Union[str, tp.TextIO]:\n    \'\'\'Realize Path objects as strings, let TextIO pass through, if given.\n    \'\'\'\n    if isinstance(fp, Path):\n        return str(fp)\n    return fp\n\n\ndef _read_url(fp: str) -> str:\n    \'\'\'\n    Read a URL into memory, return a decoded string.\n    \'\'\'\n    with request.urlopen(fp) as response: #pragma: no cover\n        return tp.cast(str, response.read().decode(\'utf-8\')) #pragma: no cover\n\n\ndef write_optional_file(\n        content: str,\n        fp: tp.Optional[PathSpecifierOrFileLike] = None,\n        ) -> tp.Optional[str]:\n\n    if fp is not None:\n        fp = path_filter(fp)\n\n    fd = f = None\n    if not fp: # get a temp file\n        fd, fp = tempfile.mkstemp(suffix=\'.html\', text=True)\n    elif isinstance(fp, StringIO):\n        f = fp\n        fp = None\n    # nothing to do if we have an fp\n\n    if f is None: # do not have a file object\n        try:\n            assert isinstance(fp, str)\n            with tp.cast(StringIO, open(fp, \'w\')) as f:\n                f.write(content)\n        finally:\n            if fd is not None:\n                os.close(fd)\n    else: # string IO\n        f.write(content)\n        f.seek(0)\n    return tp.cast(str, fp)\n\n\n#-------------------------------------------------------------------------------\n# trivial, non NP util\n\ndef get_tuple_constructor(fields: np.ndarray) -> tp.Type[tp.Tuple[tp.Any, ...]]:\n    \'\'\'\n    Given fields, try to create a Namedtuple; if that fails, return a normal tuple.\n    \'\'\'\n    try:\n        return namedtuple(\'Axis\', fields)\n    except ValueError:\n        # take positiona args\n        return lambda *args: tuple(args) # type: ignore\n\n\n\n\ndef key_normalize(key: KeyOrKeys) -> tp.List[tp.Hashable]:\n    \'\'\'\n    Normalizing a key that might be a single element or an iterable of keys; expected return is always a list, as it will be used for getitem selection.\n    \'\'\'\n    if isinstance(key, str) or not hasattr(key, \'__len__\'):\n        return [key]\n    return key if isinstance(key, list) else list(key) # type: ignore\n'"
static_frame/performance/__init__.py,0,b''
static_frame/performance/core.py,51,"b""\nimport typing as tp\nimport itertools as it\nimport string\nimport hashlib\n\nimport pandas as pd\nimport numpy as np\n\nimport static_frame as sf\n\nfrom static_frame.performance.perf_test import PerfTest\n\n#-------------------------------------------------------------------------------\n\ndef get_sample_series_float(size: int = 10000) -> tp.Tuple[pd.Series, sf.Series, np.ndarray]:\n    a1 = np.arange(size) * .001\n    a1[size // 2:] = np.nan\n    pds = pd.Series(a1)\n    sfs = sf.Series(a1)\n    return pds, sfs, a1\n\ndef get_sample_series_string_index_float_values(size: int = 10000) -> tp.Tuple[pd.Series, sf.Series, np.ndarray]:\n    a1 = np.arange(size) * .001\n    a1[size // 2:] = np.nan\n    # create hsa indices\n    index = [hashlib.sha224(str(x).encode('utf-8')).hexdigest() for x in range(size)]\n    pds = pd.Series(a1, index=index)\n    sfs = sf.Series(a1, index=index)\n    return pds, sfs, a1\n\ndef get_sample_series_obj(size: int = 10000) -> tp.Tuple[pd.Series, sf.Series, np.ndarray]:\n    sample = [None, 3, 0.123, np.nan]\n    a1 = np.array(sample * int(size / len(sample)))\n\n    pds = pd.Series(a1)\n    sfs = sf.Series(a1)\n\n    return pds, sfs, a1\n\ndef get_sample_series_objstr(size: int = 10000) -> tp.Tuple[pd.Series, sf.Series, np.ndarray]:\n    sample = [None, 3, 0.123, np.nan, 'str']\n    a1 = np.array(sample * int(size / len(sample)))\n\n    pds = pd.Series(a1)\n    sfs = sf.Series(a1)\n\n    return pds, sfs, a1\n\n\n#-------------------------------------------------------------------------------\n# frame generators\n\n\ndef get_sample_frame_float_string_index(\n        size: int = 10000,\n        columns: int = 100\n        ) -> tp.Tuple[pd.DataFrame, sf.Frame, np.ndarray]:\n    a1 = (np.arange(size * columns)).reshape((size, columns)) * .001\n    # insert random nan in very other columns\n    for col in range(0, 100, 2):\n        a1[:100, col] = np.nan\n    index = [hashlib.sha224(str(x).encode('utf-8')).hexdigest() for x in range(size)]\n    cols = [hashlib.sha224(str(x).encode('utf-8')).hexdigest() for x in range(columns)]\n    sff = sf.Frame(a1, index=index, columns=cols)\n    pdf = pd.DataFrame(a1, index=index, columns=cols)\n    return pdf, sff, a1\n\n\n_mixed_types = ('foo', 'bar', True, None, 234.34, 90)\n\ndef _typed_array(dtype: type, size: int, shift: int = 0) -> np.ndarray:\n    if dtype == float:\n        return np.roll(np.arange(size) * .001, shift)\n    if dtype == int:\n        return np.roll(np.arange(size), shift)\n    if dtype == bool:\n        return np.roll(np.isin(np.arange(size) % 5, (1, 4)), shift)\n    if dtype == object:\n        return np.roll(np.array([_mixed_types[x % len(_mixed_types)] for x in range(size)]), shift)\n    raise NotImplementedError()\n\ndef get_sample_frame_mixed_string_index(size: int = 10000, columns: int = 100) -> tp.Tuple[\n            pd.DataFrame, sf.FrameGO, np.ndarray]:\n    '''Get frames with mixed types.\n    '''\n    # produces 14950 strings\n    source_ids = list(''.join(x) for x in it.combinations(string.ascii_lowercase, 4))\n    assert size <= len(source_ids)\n\n    index = source_ids[:size]\n    cols = source_ids[:columns]\n\n    dtypes = (float, int, object, bool)\n\n    sff = sf.FrameGO(index=index)\n    for idx, col in enumerate(cols):\n        s = sf.Series(_typed_array(dtypes[idx % 4], size=size, shift=idx), index=index)\n        sff[col] = s\n\n    npf = sff.values\n\n    pdf = pd.DataFrame(index=index)\n    for idx, col in enumerate(cols):\n        s = pd.Series(_typed_array(dtypes[idx % 4], size=size, shift=idx), index=index)\n        pdf[col] = s\n\n\n    return pdf, sff, npf\n\n\ndef get_series_float_h2d_str_index(size: int = 1000) -> tp.Tuple[pd.Series, sf.Series]:\n    '''\n    Get a hierarchical index with\n    '''\n    labels = list(''.join(x) for x in it.combinations(string.ascii_lowercase, 4))\n    labels0 = labels[:int(size / 10)]\n    labels1 = labels[:size]\n    values = np.arange(len(labels0) * len(labels1)) * .001\n\n    ih = sf.IndexHierarchy.from_product(labels0, labels1)\n    sfs = sf.Series(values, index=ih)\n\n    mi = pd.MultiIndex.from_product((labels0, labels1))\n    pds = pd.Series(values, index=mi)\n    return pds, sfs\n\n\ndef get_series_float_h3d_str_index(size: int = 1000) -> tp.Tuple[pd.Series, sf.Series]:\n    '''\n    Get a hierarchical index with\n    '''\n    labels = list(''.join(x) for x in it.combinations(string.ascii_lowercase, 4))\n    labels0 = labels[:int(size / 100)]\n    labels1 = labels[:int(size / 10)]\n    labels2 = labels[:size]\n\n    values = np.arange(len(labels0) * len(labels1) * len(labels2)) * .001\n\n    ih = sf.IndexHierarchy.from_product(labels0, labels1, labels2)\n    sfs = sf.Series(values, index=ih)\n\n    mi = pd.MultiIndex.from_product((labels0, labels1, labels2))\n    pds = pd.Series(values, index=mi)\n    return pds, sfs\n\nclass SampleData:\n\n    _store: tp.Dict[str, tp.Any] = {}\n\n    @classmethod\n    def create(cls) -> None:\n        pds_int_float_10k, sfs_int_float_10k, npa_int_float_10k = get_sample_series_float(10000)\n        pds_obj_10k, sfs_obj_10k, npa_obj_10k = get_sample_series_obj(10000)\n        pds_str_float_10k, sfs_str_float_10k, npa_str_float_10k = get_sample_series_string_index_float_values(10000)\n        pds_objstr_10k, sfs_objstr_10k, npa_objstr_10k = get_sample_series_objstr(10000)\n        pdf_float_10k, sff_float_10k, npf_float_10k = get_sample_frame_float_string_index(10000)\n        pdf_mixed_10k, sff_mixed_10k, npf_mixed_10k = get_sample_frame_mixed_string_index()\n\n        pds_float_h2d_str_index, sfs_float_h2d_str_index = get_series_float_h2d_str_index()\n        pds_float_h3d_str_index, sfs_float_h3d_str_index = get_series_float_h3d_str_index()\n\n\n        for k, v in locals().items():\n            if k == 'cls' or k.startswith('__'):\n                continue\n            cls._store[k] = v\n\n        # additional resources\n        label_str = list(''.join(x) for x in it.combinations(string.ascii_lowercase, 4))\n        cls._store['label_str'] = label_str\n\n        cls._store['label_tuple2_int_10000'] = [(int(x / 10), x)\n                for x in range(10000)]\n        cls._store['label_tuple3_int_100000'] = [(int(x / 100), int(x / 10), x)\n                for x in range(100000)]\n        cls._store['label_tuple4_int_100000'] = [(int(x / 1000), int(x / 100), int(x / 10), x)\n                for x in range(100000)]\n\n        label_tuple2_str = []\n        label_tuple3_str = []\n        label_tuple4_str = []\n\n        for i, label in enumerate(label_str):\n            if i % 10 == 0:\n                outer10 = label\n            if i % 100 == 0:\n                outer100 = label\n            if i % 1000 == 0:\n                outer1000 = label\n\n            label_tuple2_str.append((outer10, label))\n            label_tuple3_str.append((outer100, outer10, label))\n            label_tuple4_str.append((outer1000, outer100, outer10, label))\n\n        cls._store['label_tuple2_str'] = label_tuple2_str\n        cls._store['label_tuple3_str'] = label_tuple3_str\n        cls._store['label_tuple4_str'] = label_tuple4_str\n\n\n\n    @classmethod\n    def get(cls, key: str) -> tp.Any:\n        return cls._store[key]\n\n\n\n#-------------------------------------------------------------------------------\n# index Tests\n\nclass IndexStr_init(PerfTest):\n    '''Index construction for string labels.\n    '''\n\n    @classmethod\n    def pd(cls) -> None:\n        pd.Index(SampleData.get('label_str'))\n\n    @classmethod\n    def sf(cls) -> None:\n        sf.Index(SampleData.get('label_str'))\n\n\nclass IndexHierarchy2d_from_product(PerfTest):\n\n    NUMBER = 100\n\n    _size0 = 100\n    _size1 = 1000\n\n    @classmethod\n    def pd(cls) -> None:\n        labels0 = SampleData.get('label_str')[:cls._size0]\n        labels1 = SampleData.get('label_str')[:cls._size1]\n        ih = pd.MultiIndex.from_product((labels0, labels1))\n        assert len(ih) == cls._size0 * cls._size1\n\n    @classmethod\n    def sf(cls) -> None:\n        labels0 = SampleData.get('label_str')[:cls._size0]\n        labels1 = SampleData.get('label_str')[:cls._size1]\n        ih = sf.IndexHierarchy.from_product(labels0, labels1)\n        assert len(ih) == cls._size0 * cls._size1\n\nclass IndexHierarchy3d_from_product(PerfTest):\n\n    NUMBER = 10\n\n    _size0 = 10\n    _size1 = 100\n    _size2 = 1000\n\n    @classmethod\n    def pd(cls) -> None:\n        labels0 = SampleData.get('label_str')[:cls._size0]\n        labels1 = SampleData.get('label_str')[:cls._size1]\n        labels2 = SampleData.get('label_str')[:cls._size2]\n        ih = pd.MultiIndex.from_product((labels0, labels1, labels2))\n        assert len(ih) == cls._size0 * cls._size1 * cls._size2\n\n    @classmethod\n    def sf(cls) -> None:\n        labels0 = SampleData.get('label_str')[:cls._size0]\n        labels1 = SampleData.get('label_str')[:cls._size1]\n        labels2 = SampleData.get('label_str')[:cls._size2]\n        ih = sf.IndexHierarchy.from_product(labels0, labels1, labels2)\n        assert len(ih) == cls._size0 * cls._size1 * cls._size2\n\n\nclass IndexHierarchy4d_from_product(PerfTest):\n\n    NUMBER = 10\n\n    _size0 = 10\n    _size1 = 50\n    _size2 = 100\n    _size3 = 500\n\n    @classmethod\n    def pd(cls) -> None:\n        labels0 = SampleData.get('label_str')[:cls._size0]\n        labels1 = SampleData.get('label_str')[:cls._size1]\n        labels2 = SampleData.get('label_str')[:cls._size2]\n        labels3 = SampleData.get('label_str')[:cls._size3]\n        ih = pd.MultiIndex.from_product((labels0, labels1, labels2, labels3))\n        assert len(ih) == cls._size0 * cls._size1 * cls._size2 * cls._size3\n\n    @classmethod\n    def sf(cls) -> None:\n        labels0 = SampleData.get('label_str')[:cls._size0]\n        labels1 = SampleData.get('label_str')[:cls._size1]\n        labels2 = SampleData.get('label_str')[:cls._size2]\n        labels3 = SampleData.get('label_str')[:cls._size3]\n        ih = sf.IndexHierarchy.from_product(labels0, labels1, labels2, labels3)\n        assert len(ih) == cls._size0 * cls._size1 * cls._size2 * cls._size3\n\n\n\nclass IndexHierarchy2d_from_labels_int(PerfTest):\n\n    NUMBER = 20\n\n    @classmethod\n    def pd(cls) -> None:\n        ih = pd.MultiIndex.from_tuples(SampleData.get('label_tuple2_int_10000'))\n\n    @classmethod\n    def sf(cls) -> None:\n        ih = sf.IndexHierarchy.from_labels(SampleData.get('label_tuple2_int_10000'))\n\n\nclass IndexHierarchy2d_from_labels_str(PerfTest):\n\n    NUMBER = 20\n\n    @classmethod\n    def pd(cls) -> None:\n        ih = pd.MultiIndex.from_tuples(SampleData.get('label_tuple2_str'))\n\n    @classmethod\n    def sf(cls) -> None:\n        ih = sf.IndexHierarchy.from_labels(SampleData.get('label_tuple2_str'))\n\n\nclass IndexHierarchy3d_from_labels_int(PerfTest):\n\n    NUMBER = 10\n\n    @classmethod\n    def pd(cls) -> None:\n        ih = pd.MultiIndex.from_tuples(SampleData.get('label_tuple3_int_100000'))\n\n    @classmethod\n    def sf(cls) -> None:\n        ih = sf.IndexHierarchy.from_labels(SampleData.get('label_tuple3_int_100000'))\n\n\n\nclass IndexHierarchy3d_from_labels_str(PerfTest):\n\n    NUMBER = 10\n\n    @classmethod\n    def pd(cls) -> None:\n        ih = pd.MultiIndex.from_tuples(SampleData.get('label_tuple3_str'))\n\n    @classmethod\n    def sf(cls) -> None:\n        ih = sf.IndexHierarchy.from_labels(SampleData.get('label_tuple3_str'))\n\n\nclass IndexHierarchy4d_from_labels_int(PerfTest):\n\n    NUMBER = 10\n\n    @classmethod\n    def pd(cls) -> None:\n        ih = pd.MultiIndex.from_tuples(SampleData.get('label_tuple4_int_100000'))\n\n    @classmethod\n    def sf(cls) -> None:\n        ih = sf.IndexHierarchy.from_labels(SampleData.get('label_tuple4_int_100000'))\n\n\nclass IndexHierarchy4d_from_labels_str(PerfTest):\n\n    NUMBER = 10\n\n    @classmethod\n    def pd(cls) -> None:\n        ih = pd.MultiIndex.from_tuples(SampleData.get('label_tuple4_str'))\n\n    @classmethod\n    def sf(cls) -> None:\n        ih = sf.IndexHierarchy.from_labels(SampleData.get('label_tuple4_str'))\n\n\n\n\n\n\n\nclass IndexHierarchy2d_iter(PerfTest):\n    NUMBER = 20\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_float_h2d_str_index').index\n        tuple(post)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_float_h2d_str_index').index\n        tuple(post)\n\n\nclass IndexHierarchy3d_iter(PerfTest):\n    NUMBER = 10\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_float_h3d_str_index').index\n        tuple(post)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_float_h3d_str_index').index\n        tuple(post)\n\n\n\n\n\n\n\n\n#-------------------------------------------------------------------------------\n# series tests\n\nclass SeriesIntFloat_init(PerfTest):\n    @staticmethod\n    def pd() -> None:\n        post = pd.Series(SampleData.get('npa_int_float_10k'))\n\n    @staticmethod\n    def sf() -> None:\n        post = pd.Series(SampleData.get('npa_int_float_10k'))\n\n\nclass SeriesStrObj_init(PerfTest):\n    @staticmethod\n    def pd() -> None:\n        a = SampleData.get('npa_obj_10k')\n        post = pd.Series(a, index=SampleData.get('label_str')[:len(a)])\n\n    @staticmethod\n    def sf() -> None:\n        a = SampleData.get('npa_obj_10k')\n        post = sf.Series(a, index=SampleData.get('label_str')[:len(a)])\n\n\n\n\nclass SeriesIntFloat_isnull(PerfTest):\n    @staticmethod\n    def np() -> None:\n        post = np.isnan(SampleData.get('npa_int_float_10k'))\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_int_float_10k').isnull()\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_int_float_10k').isna()\n\n\nclass SeriesIntFloat_dropna(PerfTest):\n    @staticmethod\n    def np() -> None:\n        post = SampleData.get('npa_int_float_10k')[np.isnan(SampleData.get('npa_int_float_10k'))]\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_int_float_10k').dropna()\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_int_float_10k').dropna()\n\n\nclass SeriesIntFloat_fillna(PerfTest):\n    @staticmethod\n    def np() -> None:\n        sel = np.isnan(SampleData.get('npa_int_float_10k'))\n        post = SampleData.get('npa_int_float_10k').copy()\n        post[sel] = 0.0\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_int_float_10k').fillna(0.0)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_int_float_10k').fillna(0.0)\n\n\nclass SeriesIntFloat_fillna_forward(PerfTest):\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_int_float_10k').fillna(method='ffill')\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_int_float_10k').fillna_forward()\n\n\nclass SeriesIntFloat_drop_duplicated(PerfTest):\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_int_float_10k').drop_duplicates(keep=False)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_int_float_10k').drop_duplicated()\n\n\n\nclass SeriesIntFloat_apply(PerfTest):\n\n    NUMBER = 100\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_int_float_10k').apply(str)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_int_float_10k').iter_element().apply(str)\n\n\n\n\n\n\nclass SeriesStrFloat_isna(PerfTest):\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_str_float_10k').isnull()\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_str_float_10k').isna()\n\n\nclass SeriesStrFloat_dropna(PerfTest):\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_str_float_10k').dropna()\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_str_float_10k').dropna()\n\n\nclass SeriesStrFloat_fillna(PerfTest):\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_str_float_10k').fillna(0.0)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_str_float_10k').fillna(0.0)\n\n\nclass SeriesStrFloat_fillna_forward(PerfTest):\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_str_float_10k').fillna(method='ffill')\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_str_float_10k').fillna_forward()\n\n\n\nclass SeriesStrFloat_apply(PerfTest):\n\n    NUMBER = 100\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_str_float_10k').apply(str)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_str_float_10k').iter_element().apply(str)\n\n\n\n\nclass SeriesIntObj_isnull(PerfTest):\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_obj_10k').isnull()\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_obj_10k').isna()\n\n\nclass SeriesIntObj_dropna(PerfTest):\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_obj_10k').dropna()\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_obj_10k').dropna()\n\n\nclass SeriesIntObj_fillna(PerfTest):\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_obj_10k').fillna(0.0)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_obj_10k').fillna(0.0)\n\n\nclass SeriesIntObj_fillna_forward(PerfTest):\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_obj_10k').fillna(method='ffill')\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_obj_10k').fillna_forward()\n\n\nclass SeriesIntObj_drop_duplicated(PerfTest):\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_obj_10k').drop_duplicates(keep=False)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_obj_10k').drop_duplicated()\n\n\nclass SeriesIntObj_apply(PerfTest):\n\n    NUMBER = 100\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_obj_10k').apply(str)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_obj_10k').iter_element().apply(str)\n\n\n\n\n\n\nclass SeriesIntObjStr_isnull(PerfTest):\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_objstr_10k').isnull()\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_objstr_10k').isna()\n\nclass SeriesIntObjStr_dropna(PerfTest):\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_objstr_10k').dropna()\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_objstr_10k').dropna()\n\nclass SeriesIntObjStr_fillna(PerfTest):\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_objstr_10k').fillna('wrong')\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_objstr_10k').fillna('wrong')\n\n\nclass SeriesIntObjStr_fillna_forward(PerfTest):\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_objstr_10k').fillna(method='ffill')\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_objstr_10k').fillna_forward()\n\n\nclass SeriesIntObjStr_apply(PerfTest):\n\n    NUMBER = 100\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_objstr_10k').apply(str)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_objstr_10k').iter_element().apply(str)\n\n\nclass SeriesIntObjStr_reindex_identity(PerfTest):\n\n    NUMBER = 200\n\n    @staticmethod\n    def pd() -> None:\n        s1 = SampleData.get('pds_objstr_10k')\n        s2 = s1.reindex(s1.index)\n\n    @staticmethod\n    def sf() -> None:\n        s1 = SampleData.get('sfs_objstr_10k')\n        s2 = s1.reindex(s1.index)\n\nclass SeriesIntObjStr_reindex_reverse(PerfTest):\n\n    NUMBER = 200\n\n    @staticmethod\n    def pd() -> None:\n        s1 = SampleData.get('pds_objstr_10k')\n        s2 = s1.reindex(reversed(s1.index))\n\n    @staticmethod\n    def sf() -> None:\n        s1 = SampleData.get('sfs_objstr_10k')\n        s2 = s1.reindex(reversed(s1.index))\n\n\n\nclass SeriesFloatH2DString_loc_target(PerfTest):\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_float_h2d_str_index').loc[('abgu', 'abcf')]\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_float_h2d_str_index').loc[('abgu', 'abcf')]\n\n\nclass SeriesFloatH2DString_loc_slice(PerfTest):\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_float_h2d_str_index').loc[pd.IndexSlice[:, 'abcf']]\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_float_h2d_str_index').loc[sf.HLoc[:, 'abcf']]\n\n\n\n\nclass SeriesFloatH3DString_loc_target(PerfTest):\n    '''\n    Selecting single value from 3-level hierarchy.\n    '''\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_float_h3d_str_index').loc[('abce', 'abgu', 'afgx')]\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_float_h3d_str_index').loc[('abce', 'abgu', 'afgx')]\n\n\nclass SeriesFloatH3DString_loc_slice_target_slice(PerfTest):\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_float_h3d_str_index').loc[\n                pd.IndexSlice[:, 'abcf', 'abcl':'abco']]  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        assert len(post) == 40\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_float_h3d_str_index').loc[\n                sf.HLoc[:, 'abcf', 'abcl':'abco']]  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        assert len(post) == 40\n\n\nclass SeriesFloatH3DString_loc_slice_slice_target(PerfTest):\n    NUMBER = 50\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pds_float_h3d_str_index').loc[\n                pd.IndexSlice[:, :, 'abcl']]\n        assert len(post) == 1000\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sfs_float_h3d_str_index').loc[\n                sf.HLoc[:, :, 'abcl']]\n        assert len(post) == 1000\n\n\n\nclass SeriesIntObj_isin(PerfTest):\n    '''isin with objects.\n    Numpy's performance as the lookup array grows deteriorates at an exponential rate.\n    '''\n    NUMBER = 50\n    _lower = 2\n    _upper = 5\n\n    @classmethod\n    def sf(cls) -> None:\n        sf_series = SampleData.get('sfs_obj_10k')\n        for i in range(cls._lower, cls._upper):\n            lookup_arr = np.array([str(i) for i in range(10**i)], dtype=object)\n            sf_series.isin(lookup_arr)\n\n    @classmethod\n    def pd(cls) -> None:\n        pd_series = SampleData.get('pds_obj_10k')\n        for i in range(cls._lower, cls._upper):\n            lookup_arr = np.array([str(i) for i in range(10**i)], dtype=object)\n            pd_series.isin(lookup_arr)\n\n    # @classmethod\n    # def np(cls) -> None:\n    #     np_series = SampleData.get('npa_obj_10k')\n    #     for i in range(cls._lower, cls._upper):\n    #         lookup_arr = np.array([str(i) for i in range(10**i)], dtype=object)\n    #         np.isin(np_series, lookup_arr)\n\n\nclass SeriesStrFloat_isin(PerfTest):\n    '''isin with primitives.\n    Static Frame's performance should be in line with numpy\n\n    As n gets large, static frame begins to outperform pandas\n    100 = 1.7124\n    1000 = 1.4958\n    10000 = 1.1766\n    100000 = 1.0496\n    1000000 = 0.9719\n    '''\n    NUMBER = 50\n    _lower = 2\n    _upper = 5\n\n    @classmethod\n    def sf(cls) -> None:\n        sf_series = SampleData.get('sfs_str_float_10k')\n        for exponent in range(cls._lower, cls._upper):\n            lookup_arr = np.array([i / 100 for i in range(10**exponent)])\n            sf_series.isin(lookup_arr)\n\n    @classmethod\n    def pd(cls) -> None:\n        pd_series = SampleData.get('pds_str_float_10k')\n        for exponent in range(cls._lower, cls._upper):\n            lookup_arr = np.array([i / 100 for i in range(10**exponent)])\n            pd_series.isin(lookup_arr)\n\n    @classmethod\n    def np(cls) -> None:\n        np_series = SampleData.get('npa_str_float_10k')\n        for exponent in range(cls._lower, cls._upper):\n            lookup_arr = np.array([i / 100 for i in range(10**exponent)])\n            np.isin(np_series, lookup_arr)\n\n\n#-------------------------------------------------------------------------------\n# frame tests\n\n# this is deemed invalid, as Pandas just holds a reference\n# class FrameFloat_init(PerfTest):\n#     @staticmethod\n#     def pd() -> None:\n#         post = pd.DataFrame(SampleData.get('npf_float_10k'))\n\n#     @staticmethod\n#     def sf() -> None:\n#         post = sf.Frame(SampleData.get('npf_float_10k'))\n\n\nclass FrameStrFloat_init(PerfTest):\n    NUMBER = 100\n\n    @staticmethod\n    def pd() -> None:\n        data = SampleData.get('npf_float_10k')\n        labels = SampleData.get('label_str')\n        post = pd.DataFrame(data, index=labels[:data.shape[0]], columns=labels[:data.shape[1]])\n\n    @staticmethod\n    def sf() -> None:\n        data = SampleData.get('npf_float_10k')\n        labels = SampleData.get('label_str')\n        post = sf.Frame(data, index=labels[:data.shape[0]], columns=labels[:data.shape[1]])\n\n\nclass FrameFloat_from_records(PerfTest):\n\n    NUMBER = 10\n\n    @staticmethod\n    def pd() -> None:\n        # make data into a list to force type identification\n        post = pd.DataFrame.from_records(list(SampleData.get('npf_float_10k')))\n        assert post.shape == (10000, 100)\n\n    @staticmethod\n    def sf() -> None:\n        post = sf.Frame.from_records(list(SampleData.get('npf_float_10k')), dtypes=[float]*100)\n        assert post.shape == (10000, 100)\n\n\n\nclass FrameMixed_from_records(PerfTest):\n\n    NUMBER = 10\n\n    @staticmethod\n    def pd() -> None:\n        # make data into a list to force type identification\n        post = pd.DataFrame.from_records(list(SampleData.get('npf_mixed_10k')))\n        assert post.shape == (10000, 100)\n        assert post.dtypes[2] == object\n\n    @staticmethod\n    def sf() -> None:\n        post = sf.Frame.from_records(list(SampleData.get('npf_mixed_10k')))\n        assert post.shape == (10000, 100)\n        assert post.dtypes[2] == object\n\n\n\n\n\n#-------------------------------------------------------------------------------\n# frame util functions\n\nclass FrameFloat_sum_skipna_axis0(PerfTest):\n    @staticmethod\n    def np() -> None:\n        post = np.nansum(SampleData.get('npf_float_10k'), axis=0)\n        assert post.shape == (100,)\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pdf_float_10k').sum(axis=0, skipna=True)\n        assert post.shape == (100,)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sff_float_10k').sum(axis=0, skipna=True)\n        assert post.shape == (100,)\n\n\nclass FrameFloat_sum_skipna_axis1(PerfTest):\n    @staticmethod\n    def np() -> None:\n        post = np.nansum(SampleData.get('npf_float_10k'), axis=1)\n        assert post.shape == (10000,)\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pdf_float_10k').sum(axis=1, skipna=True)\n        assert post.shape == (10000,)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sff_float_10k').sum(axis=1, skipna=True)\n        assert post.shape == (10000,)\n\n\nclass FrameFloat_dropna_any_axis0(PerfTest):\n    NUMBER = 100\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pdf_float_10k').dropna(axis=0, how='any')\n        assert post.shape == (9900, 100)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sff_float_10k').dropna(axis=0, condition=np.any)\n        assert post.shape == (9900, 100)\n\n\nclass FrameFloat_dropna_any_axis1(PerfTest):\n    NUMBER = 100\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pdf_float_10k').dropna(axis=1, how='any')\n        assert post.shape == (10000, 50)\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sff_float_10k').dropna(axis=1, condition=np.any)\n        assert post.shape == (10000, 50)\n\n\nclass FrameFloat_isna(PerfTest):\n    NUMBER = 100\n\n    @staticmethod\n    def np() -> None:\n        post = np.isnan(SampleData.get('npf_float_10k'))\n\n    @staticmethod\n    def pd() -> None:\n        post = SampleData.get('pdf_float_10k').isnull()\n\n    @staticmethod\n    def sf() -> None:\n        post = SampleData.get('sff_float_10k').isna()\n\n\nclass FrameFloat_apply_axis0(PerfTest):\n\n    NUMBER = 50\n\n    @staticmethod\n    def pd() -> None:\n        func = lambda a: np.nanmean(a ** 2)\n        post = SampleData.get('pdf_float_10k').apply(func, axis=0) # apply to columns\n        assert post.shape == (100,)\n        assert post.sum() > 33501616.16668333\n\n    @staticmethod\n    def sf() -> None:\n        func = lambda a: np.nanmean(a ** 2)\n        post = SampleData.get('sff_float_10k').iter_array(0).apply(func) # apply to columns\n        assert post.shape == (100,)\n        assert post.sum() > 33501616.16668333\n\n\nclass FrameFloat_apply_axis1(PerfTest):\n    NUMBER = 5\n\n    @staticmethod\n    def pd() -> None:\n        func = lambda a: np.nanmean(a ** 2)\n        post = SampleData.get('pdf_float_10k').apply(func, axis=1) # apply to rows\n        assert post.shape == (10000,)\n        assert post.sum() > 3333328333.8349\n\n    @staticmethod\n    def sf() -> None:\n        func = lambda a: np.nanmean(a ** 2)\n        post = SampleData.get('sff_float_10k').iter_array(1).apply(func)\n        assert post.shape == (10000,)\n        assert post.sum() > 3333328333.8349\n\n\n#-------------------------------------------------------------------------------\n# frame loc float\n\n\nclass FrameFloat_slice_loc_indices(PerfTest):\n    NUMBER = 100\n\n    @staticmethod\n    def pd() -> None:\n        for i in range(0, 10000, 1000):\n            start = SampleData.get('pdf_float_10k').index[i]\n            SampleData.get('pdf_float_10k').loc[start:]\n\n    @staticmethod\n    def sf() -> None:\n        for i in range(0, 10000, 1000):\n            start = SampleData.get('sff_float_10k').index.values[i]\n            SampleData.get('sff_float_10k').loc[start:]\n\n\nclass FrameFloat_slice_loc_index(PerfTest):\n    NUMBER = 100\n\n    @staticmethod\n    def pd() -> None:\n        for i in range(0, 10000, 1000):\n            start = SampleData.get('pdf_float_10k').index[i]\n            SampleData.get('pdf_float_10k').loc[start]\n\n    @staticmethod\n    def sf() -> None:\n        for i in range(0, 10000, 1000):\n            start = SampleData.get('sff_float_10k').index.values[i]\n            SampleData.get('sff_float_10k').loc[start]\n\n\n\nclass FrameFloat_slice_loc_columns(PerfTest):\n\n    NUMBER = 100\n\n    @staticmethod\n    def pd() -> None:\n        for i in range(0, 100, 10):\n            start = SampleData.get('pdf_float_10k').index[i]\n            SampleData.get('pdf_float_10k').loc[:, start:]\n\n    @staticmethod\n    def sf() -> None:\n        for i in range(0, 100, 10):\n            start = SampleData.get('sff_float_10k').index.values[i]\n            SampleData.get('sff_float_10k').loc[:, start:]\n\n\n\nclass FrameFloat_slice_loc_column(PerfTest):\n\n    @staticmethod\n    def pd() -> None:\n        for i in range(0, 100, 10):\n            start = SampleData.get('pdf_float_10k').index[i]\n            SampleData.get('pdf_float_10k').loc[:, start]\n\n    @staticmethod\n    def sf() -> None:\n        for i in range(0, 100, 10):\n            start = SampleData.get('sff_float_10k').index.values[i]\n            SampleData.get('sff_float_10k').loc[:, start]\n\n\n#-------------------------------------------------------------------------------\n# frame loc mixed\n\n\nclass FrameMixed_slice_loc_indices(PerfTest):\n\n    NUMBER = 100\n\n    @staticmethod\n    def pd() -> None:\n        for i in range(0, 10000, 1000):\n            start = SampleData.get('pdf_mixed_10k').index[i]\n            SampleData.get('pdf_mixed_10k').loc[start:]\n\n    @staticmethod\n    def sf() -> None:\n        for i in range(0, 10000, 1000):\n            start = SampleData.get('sff_mixed_10k').index.values[i]\n            SampleData.get('sff_mixed_10k').loc[start:]\n\n\nclass FrameMixed_slice_loc_index(PerfTest):\n\n    NUMBER = 100\n\n    @staticmethod\n    def pd() -> None:\n        for i in range(0, 10000, 1000):\n            start = SampleData.get('pdf_mixed_10k').index[i]\n            SampleData.get('pdf_mixed_10k').loc[start]\n\n    @staticmethod\n    def sf() -> None:\n        for i in range(0, 10000, 1000):\n            start = SampleData.get('sff_mixed_10k').index.values[i]\n            SampleData.get('sff_mixed_10k').loc[start]\n\n\n\nclass FrameMixed_slice_loc_columns(PerfTest):\n\n    NUMBER = 100\n\n    @staticmethod\n    def pd() -> None:\n        for i in range(0, 100, 10):\n            start = SampleData.get('pdf_mixed_10k').index[i]\n            SampleData.get('pdf_mixed_10k').loc[:, start:]\n\n    @staticmethod\n    def sf() -> None:\n        for i in range(0, 100, 10):\n            start = SampleData.get('sff_mixed_10k').index.values[i]\n            SampleData.get('sff_mixed_10k').loc[:, start:]\n\n\n\nclass FrameMixed_slice_loc_column(PerfTest):\n\n    @staticmethod\n    def pd() -> None:\n        for i in range(0, 100, 10):\n            start = SampleData.get('pdf_mixed_10k').index[i]\n            SampleData.get('pdf_mixed_10k').loc[:, start]\n\n    @staticmethod\n    def sf() -> None:\n        for i in range(0, 100, 10):\n            start = SampleData.get('sff_mixed_10k').index.values[i]\n            SampleData.get('sff_mixed_10k').loc[:, start]\n\n\n\n#-------------------------------------------------------------------------------\n# frame creation and growth\n\nclass FrameFloat_H1D_add_series_partial(PerfTest):\n    '''Adding series that only partially match the index\n    '''\n\n    NUMBER = 10\n\n    # 325 two character strings\n    _index = list(''.join(x) for x in it.combinations(string.ascii_lowercase, 2))\n\n    @classmethod\n    def pd(cls) -> None:\n        f1 = pd.DataFrame(index=cls._index)\n        for col in range(100):\n            s = pd.Series(col * .1, index=cls._index[col: col+20])\n            f1[col] = s\n        assert f1.sum().sum() == 9900.0\n\n    @classmethod\n    def sf(cls) -> None:\n        f1 = sf.FrameGO(index=cls._index)\n        for col in range(100):\n            s = sf.Series.from_element(col * .1, index=cls._index[col: col+20])\n            f1[col] = s\n        assert f1.sum().sum() == 9900.0\n\n\nclass FrameFloat_H2D_add_series_partial(PerfTest):\n    '''Adding series that only partially match the index\n    '''\n    NUMBER = 10\n    _index_leaves = list(''.join(x) for x in it.combinations(string.ascii_lowercase, 2))\n\n    @classmethod\n    def sf(cls) -> None:\n        index = sf.IndexHierarchy.from_product(list(string.ascii_lowercase),\n                list(string.ascii_lowercase))\n        f1 = sf.FrameGO(index=index)\n        for col in range(100):\n            s = sf.Series.from_element(col * .1, index=index[col: col+6]) #type: ignore #pylint: disable=E1136\n            f1[col] = s\n        assert f1.sum().sum() == 2970.0\n\n    @classmethod\n    def pd(cls) -> None:\n        index = pd.MultiIndex.from_product((list(string.ascii_lowercase),\n                list(string.ascii_lowercase)))\n        f1 = pd.DataFrame(index=index)\n        for col in range(100):\n            s = pd.Series(col * .1, index=index[col: col+6])\n            f1[col] = s\n        assert f1.sum().sum() == 2970.0\n\n\nclass FrameObj_isin(PerfTest):\n    '''isin with objects.\n    Will noticeably underperform pandas due to pandas' use of C at a constant rate\n    Numpy's performance as the lookup array grows deteriorates at an exponential rate.\n    '''\n    NUMBER = 5\n    _lower = 2\n    _upper = 5\n\n    @classmethod\n    def pd(cls) -> None:\n        pd_frame = SampleData.get('pdf_mixed_10k')\n        for i in range(cls._lower, cls._upper):\n            lookup_arr = np.array([str(i) for i in range(10 ** i)], dtype=object)\n            pd_frame.isin(lookup_arr)\n\n    @classmethod\n    def sf(cls) -> None:\n        sf_frame = SampleData.get('sff_mixed_10k')\n        for i in range(cls._lower, cls._upper):\n            lookup_arr = np.array([str(i) for i in range(10 ** i)], dtype=object)\n            sf_frame.isin(lookup_arr)\n\n    # @classmethod\n    # def np(cls) -> None:\n    #     np_frame = SampleData.get('npf_mixed_10k')\n    #     for i in range(cls._lower, cls._upper):\n    #         lookup_arr = np.array([str(i) for i in range(10 ** i)])\n    #         np.isin(np_frame, lookup_arr)\n\n\nclass FrameFloat_isin(PerfTest):\n    '''isin with floats. As n gets large, pandas outperformance significantly drops:\n\n    100 = 6.2133x\n    1000 = 3.5506x\n    10000 = 2.9428x\n    100000 = 2.4097x\n    1000000 = 1.3527x\n    '''\n\n    NUMBER = 5\n\n    _lower = 2\n    _upper = 7\n\n    @classmethod\n    def pd(cls) -> None:\n        pd_frame = SampleData.get('pdf_float_10k')\n        for i in range(cls._lower, cls._upper):\n            lookup_arr = np.array([i / 100 for i in range(10 ** i)])\n            pd_frame.isin(lookup_arr)\n\n    @classmethod\n    def sf(cls) -> None:\n        sf_frame = SampleData.get('sff_float_10k')\n        for i in range(cls._lower, cls._upper):\n            lookup_arr = np.array([i / 100 for i in range(10 ** i)])\n            sf_frame.isin(lookup_arr)\n\n    @classmethod\n    def np(cls) -> None:\n        np_frame = SampleData.get('npf_float_10k')\n        for i in range(cls._lower, cls._upper):\n            lookup_arr = np.array([i / 100 for i in range(10 ** i)])\n            np.isin(np_frame, lookup_arr)\n"""
static_frame/performance/iter_group_perf.py,22,"b""import typing as tp\nimport timeit\nimport random\nfrom datetime import datetime\nfrom functools import partial\n\nimport numpy as np\nimport static_frame as sf\nimport pandas as pd\n\nfrom static_frame.core.frame import Frame\n\nfrom static_frame.performance.perf_test import PerfTest\n\n\nHMS = '%H:%M:%S'\nGROUPBY_COL = 'groupby'\n\n\nframe_func_t = tp.Callable[[Frame], tp.Any]\n\n\n\nclass _PerfTest(PerfTest):\n    NUMBER = 3\n\n\n\nclass BuildTestFrames:\n    __slots__ = (\n            'dims',\n            'nan_chance',\n            'none_chance'\n    )\n\n    _DTYPES = ['int', 'float', 'bool', 'str', 'object', 'mixed']\n    _NUMBER = 3\n    _REPEAT = 10\n\n    def __init__(self,\n            dims: tp.Tuple[int, ...] = (5, 20, 100, 1000),\n            nan_chance: float = 0.33,\n            none_chance: float = 0.33):\n        self.dims = dims\n        self.nan_chance = nan_chance\n        self.none_chance = none_chance\n\n    class Test_Object:\n        def __init__(self, x: int):\n            self.x = x\n\n        def __str__(self) -> str:\n            return f'Test_Object({self.x})'\n\n        def __repr__(self) -> str:\n            return str(self)\n\n    def _make_float(self, val: int) -> float:\n        if np.random.random() <= self.nan_chance:\n            return np.nan # type: ignore\n        else:\n            return val * 0.1\n\n    def _make_bool(self, val: int) -> bool:\n        return val % 2 == 0\n\n    def _make_str(self, val: int) -> str:\n        return ''.join(chr((val + i) % 26 + 65) for i in range(3))\n\n    def _make_object(self, val: int) -> tp.Optional['Test_Object']:\n        if np.random.random() <= self.none_chance:\n            return None\n        else:\n            return self.Test_Object(val)\n\n    def _make_mixed(self, val: int): # type: ignore\n        r: int = np.random.randint(len(BuildTestFrames._DTYPES))\n        if r == 0:\n            return val\n        if r == 1:\n            return self._make_float(val)\n        if r == 2:\n            return self._make_bool(val)\n        if r == 3:\n            return self._make_str(val)\n        if r == 4:\n            return self._make_object(val)\n\n    def _build_col(self, rows: int, dtype: str) -> np.ndarray:\n        if dtype == 'int':\n            return np.arange(rows)\n\n        if dtype == 'float':\n            return np.vectorize(self._make_float)(np.arange(rows))\n\n        if dtype == 'bool':\n            return np.vectorize(self._make_bool)(np.arange(rows))\n\n        if dtype == 'str':\n            return np.vectorize(self._make_str)(np.arange(rows))\n\n        if dtype == 'object':\n            return np.vectorize(self._make_object)(np.arange(rows))\n\n        if dtype == 'mixed':\n            # Cannot vectorize this call :(\n            return np.array([self._make_mixed(val) for val in range(rows)])\n\n    @staticmethod\n    def _build_groups(num_of_groups: int, num_of_rows: int) -> np.ndarray:\n        assert num_of_groups > 0\n        i: int = 0\n        build: tp.List[int] = []\n        while i < num_of_rows:\n            build.append(i % num_of_groups)\n            i += 1\n        return np.array(build)\n\n    def _next_frame_dims(self,\n            mixed_data_options: tp.Iterable[bool]\n            ) -> tp.Iterator[tp.Tuple[int, int, int, bool]]:\n        for rows in self.dims:\n            for cols in self.dims:\n                for groups in self.dims:\n                    if groups <= rows:\n                        for mixed_data in mixed_data_options:\n                            yield rows, cols - 1, groups, mixed_data\n\n    @staticmethod\n    def _shuffle(frame: Frame) -> Frame:\n        random.seed(0)\n        return frame.loc[random.sample(frame.index.values.tolist(), len(frame))] # type: ignore\n\n    def build_frame(self, rows: int, cols: int, groups: int, mixed_data: bool) -> Frame:\n        group_col: np.ndarray = self._build_groups(groups, rows)\n\n        if mixed_data:\n            built_cols: tp.List[tp.Tuple[str, np.ndarray]] = []\n            for col in range(cols):\n                dtype = BuildTestFrames._DTYPES[col % len(BuildTestFrames._DTYPES)]\n                built_cols.append((str(col), self._build_col(rows, dtype)))\n\n            built_cols.append((GROUPBY_COL, group_col))\n            f = Frame.from_items(built_cols)\n        else:\n            arr = np.arange(rows*cols).reshape(rows, cols)\n            arr = np.hstack((arr, group_col.reshape(rows, 1)))\n\n            columns = [str(i) for i in range(cols)] + [GROUPBY_COL]\n            f = Frame(arr, columns=columns)\n\n        return BuildTestFrames._shuffle(f)\n\n    def next_frame(self,\n            mixed_data_options: tp.Iterable[bool] = (True, False)\n            ) -> tp.Iterator[Frame]:\n        for rows, cols, groups, mixed_data in self._next_frame_dims(mixed_data_options):\n            yield self.build_frame(rows, cols, groups, mixed_data)\n\n    @staticmethod\n    def get_perf(\n            frame: Frame,\n            func: frame_func_t,\n            repeat: int = _REPEAT,\n            number: int = _NUMBER\n            ) -> float:\n        timer = timeit.Timer(partial(func, frame))\n        return round(np.mean(timer.repeat(repeat=repeat, number=number)), 4) # type: ignore\n\n    @staticmethod\n    def test_frames(\n            frames: tp.List[Frame],\n            funcs: tp.List[frame_func_t],\n            repeat: int = _REPEAT,\n            number: int = _NUMBER\n            ) -> tp.List[tp.Dict[str, float]]:\n        results: tp.List[tp.Dict[str, float]] = []\n\n        for frame in frames:\n            # Frame metadata shared across tests\n            rows, cols = frame.shape\n            groups: int = len(frame[GROUPBY_COL].unique())\n\n            result: tp.Dict[str, float] = {}\n            for func in funcs:\n                key: str = func.__name__ + f': {rows}, {groups}, {cols}'\n                result[key] = BuildTestFrames.get_perf(frame, func, repeat, number)\n            results.append(result)\n\n        return results\n\n\n#-------------------------------------------------------------------------------\n# performance tests\n\nclass SampleData:\n\n    _store: tp.Dict[str, tp.Any] = {}\n\n    @classmethod\n    def create(cls) -> None:\n        print(f'({datetime.now().strftime(HMS) }) Building cache.')\n        rows = 20_000_000\n        cols = 9\n        num_groups = 100_000\n        columns = tuple('abcdefghi') + (GROUPBY_COL,)\n\n        arr = np.random.random(rows * cols).reshape(rows, cols)\n        groups = np.array([i % num_groups for i in np.random.permutation(rows)]).reshape(rows, 1)\n\n        int_arr = np.hstack((arr, groups))\n        df_int = pd.DataFrame(int_arr, columns=columns)\n        frame_int = sf.Frame(int_arr, columns=columns)\n\n        obj_arr = np.hstack((arr, groups)).astype(object)\n        df_obj = pd.DataFrame(obj_arr, columns=columns).astype({GROUPBY_COL: int})\n        frame_obj = sf.Frame(obj_arr, columns=columns).astype[GROUPBY_COL](int)\n        print(f'({datetime.now().strftime(HMS) }) Finished building cache.')\n\n        cls._store['pdf_20mil_int'] = df_int\n        cls._store['sff_20mil_int'] = frame_int\n        cls._store['pdf_20mil_obj'] = df_obj\n        cls._store['sff_20mil_obj'] = frame_obj\n\n        print(f'({datetime.now().strftime(HMS) }) Priming generators.')\n        df_int_iterable_primed = iter(df_int.groupby(GROUPBY_COL, sort=False))\n        next(df_int_iterable_primed)\n        frame_int_iterable_primed = iter(frame_int.iter_group_items(GROUPBY_COL))\n        next(frame_int_iterable_primed)\n        df_obj_iterable_primed = iter(df_obj.groupby(GROUPBY_COL, sort=False))\n        next(df_obj_iterable_primed)\n        frame_obj_iterable_primed = iter(frame_obj.iter_group_items(GROUPBY_COL))\n        next(frame_obj_iterable_primed)\n        print(f'({datetime.now().strftime(HMS) }) Finisehd priming generators.')\n\n        cls._store['pdf_20mil_int_iterable_primed'] = df_int_iterable_primed\n        cls._store['sff_20mil_int_iterable_primed'] = frame_int_iterable_primed\n        cls._store['pdf_20mil_obj_iterable_primed'] = df_obj_iterable_primed\n        cls._store['sff_20mil_obj_iterable_primed'] = frame_obj_iterable_primed\n\n\n    @classmethod\n    def get(cls, key: str) -> tp.Any:\n        return cls._store[key]\n\n#-------------------------------------------------------------------------------\n\n\nclass FrameInt_iter_group_items_setup(_PerfTest):\n    @classmethod\n    def pd(cls) -> None:\n        pd_frame = SampleData.get('pdf_20mil_int')\n        for _ in pd_frame.groupby(GROUPBY_COL, sort=False):\n            break\n\n    @classmethod\n    def sf(cls) -> None:\n        sf_frame = SampleData.get('sff_20mil_int')\n        for _ in sf_frame.iter_group_items(GROUPBY_COL):\n            break\n\n\nclass FrameObj_iter_group_items_setup(_PerfTest):\n    @classmethod\n    def pd(cls) -> None:\n        pd_frame = SampleData.get('pdf_20mil_obj')\n        for _ in pd_frame.groupby(GROUPBY_COL, sort=False):\n            break\n\n    @classmethod\n    def sf(cls) -> None:\n        sf_frame = SampleData.get('sff_20mil_obj')\n        for _ in sf_frame.iter_group_items(GROUPBY_COL):\n            break\n\n\nclass FrameInt_iter_group_items_iterate(_PerfTest):\n    @classmethod\n    def pd(cls) -> None:\n        iterator = SampleData.get('pdf_20mil_int_iterable_primed')\n        for _ in iterator:\n            pass\n\n    @classmethod\n    def sf(cls) -> None:\n        iterator = SampleData.get('sff_20mil_int_iterable_primed')\n        for _ in iterator:\n            pass\n\nclass FrameObj_iter_group_items_iterate(_PerfTest):\n    @classmethod\n    def pd(cls) -> None:\n        iterator = SampleData.get('pdf_20mil_obj_iterable_primed')\n        for _ in iterator:\n            pass\n\n    @classmethod\n    def sf(cls) -> None:\n        iterator = SampleData.get('sff_20mil_obj_iterable_primed')\n        for _ in iterator:\n            pass\n"""
static_frame/performance/main.py,4,"b""import io\nimport argparse\nimport typing as tp\nimport types\nimport fnmatch\nimport collections\nimport timeit\nimport cProfile\nimport pstats\nimport sys\nimport datetime\n\nfrom pyinstrument import Profiler #type: ignore\nimport numpy as np\nimport pandas as pd\nimport static_frame as sf\n\n\nfrom static_frame.performance.perf_test import PerfTest\n\n\n#-------------------------------------------------------------------------------\n\ndef get_arg_parser() -> argparse.ArgumentParser:\n    p = argparse.ArgumentParser(\n            description='Performance testing and profiling',\n            formatter_class=argparse.RawDescriptionHelpFormatter,\n            epilog='''Example:\n\nPerformance comparison of all dropna tests:\n\npython3 test_performance.py '*dropna' --performance\n\nProfiling outpout for static-frame dropna:\n\npython3 test_performance.py SeriesIntFloat_dropna --profile\n            '''\n            )\n    p.add_argument('patterns',\n            help='Names of classes to match using fn_match syntax',\n            nargs='+',\n            )\n    p.add_argument('--modules',\n            help='Names of modules to find tests',\n            nargs='+',\n            default=('core',),\n            )\n    p.add_argument('--profile',\n            help='Turn on profiling with cProfile',\n            action='store_true',\n            default=False,\n            )\n    p.add_argument('--instrument',\n            help='Turn on instrumenting with pyinstrument',\n            action='store_true',\n            default=False,\n            )\n    p.add_argument('--performance',\n            help='Turn on performance measurements',\n            action='store_true',\n            default=False,\n            )\n    return p\n\n\ndef yield_classes(\n        module: types.ModuleType,\n        pattern: str\n        ) -> tp.Iterator[tp.Type[PerfTest]]:\n    # this will not find children of children\n    for attr_name, attr in vars(module).items():\n        if attr_name.startswith('_'):\n            continue\n        if isinstance(attr, type) and issubclass(attr, PerfTest) and not attr is PerfTest:\n            if fnmatch.fnmatch(attr_name.lower(), pattern.lower()):\n                yield attr\n\ndef profile(cls: tp.Type[PerfTest],\n        function: str = 'sf'\n        ) -> None:\n    '''\n    Profile the `sf` function from the supplied class.\n    '''\n\n    f = getattr(cls, function)\n    pr = cProfile.Profile()\n\n    pr.enable()\n    for _ in range(cls.NUMBER):\n        f()\n    pr.disable()\n\n    s = io.StringIO()\n    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')\n    ps.print_stats()\n    print(s.getvalue())\n\ndef instrument(cls: tp.Type[PerfTest],\n        function: str = 'sf'\n        ) -> None:\n    '''\n    Profile the `sf` function from the supplied class.\n    '''\n\n    f = getattr(cls, function)\n    profiler = Profiler()\n\n    profiler.start()\n    for _ in range(cls.NUMBER):\n        f()\n    profiler.stop()\n\n    print(profiler.output_text(unicode=True, color=True))\n\n\nPerformanceRecord = tp.MutableMapping[str, tp.Union[str, float]]\n\ndef performance(\n        module: types.ModuleType,\n        cls: tp.Type[PerfTest]\n        ) -> PerformanceRecord:\n    #row = []\n    row: PerformanceRecord = collections.OrderedDict()\n    row['name'] = cls.__name__\n    row['iterations'] = cls.NUMBER\n    for f in cls.FUNCTION_NAMES:\n        if hasattr(cls, f):\n            result = timeit.timeit(cls.__name__ + '.' + f + '()',\n                    globals=vars(module),\n                    number=cls.NUMBER)\n            row[f] = result\n        else:\n            row[f] = np.nan\n    return row\n\n\ndef performance_tables_from_records(\n        records: tp.Iterable[PerformanceRecord]\n        ) -> tp.Tuple[sf.Frame, sf.Frame]:\n\n    frame = sf.FrameGO.from_dict_records(records)\n    frame = frame.set_index('name', drop=True)\n\n    if PerfTest.SF_NAME in frame.columns and PerfTest.PD_NAME in frame.columns:\n        frame['sf/pd'] = frame[PerfTest.SF_NAME] / frame[PerfTest.PD_NAME]\n        frame['pd_outperform'] = frame['sf/pd'].loc[frame['sf/pd'] > 1]\n\n        frame['pd/sf'] = frame[PerfTest.PD_NAME] / frame[PerfTest.SF_NAME]\n        frame['sf_outperform'] = frame['pd/sf'].loc[frame['pd/sf'] > 1]\n\n    def format(v: object) -> str:\n        if isinstance(v, float):\n            if np.isnan(v):\n                return ''\n            return str(round(v, 4))\n        return str(v)\n\n    display = frame.iter_element().apply(format)\n    display = display[[c for c in display.columns if '/' not in c]]\n    return frame, display\n\ndef main() -> None:\n\n    options = get_arg_parser().parse_args()\n\n    module_targets = []\n    for module in options.modules:\n        if module == 'core':\n            from static_frame.performance import core\n            module_targets.append(core)\n        elif module == 'pydata_2018':\n            from static_frame.performance import pydata_2018\n            module_targets.append(pydata_2018)\n        elif module == 'pydata_2019':\n            from static_frame.performance import pydata_2019\n            module_targets.append(pydata_2019)\n        elif module == 'iter_group_perf':\n            from static_frame.performance import iter_group_perf\n            module_targets.append(iter_group_perf)\n        else:\n            raise NotImplementedError()\n\n    records = []\n\n    for module in module_targets:\n        module.SampleData.create()\n        for pattern in options.patterns:\n            for cls in sorted(yield_classes(module, pattern), key=lambda c: c.__name__):\n                print(cls.__name__)\n                if options.performance:\n                    records.append(performance(module, cls))\n                if options.profile:\n                    profile(cls)\n                if options.instrument:\n                    instrument(cls)\n\n    itemize = False # make CLI option maybe\n\n    if records:\n\n        from static_frame import DisplayConfig\n\n        print(str(datetime.datetime.now()))\n\n        pairs = []\n        pairs.append(('python', sys.version.split(' ')[0]))\n        for package in (np, pd, sf):\n            pairs.append((package.__name__, package.__version__))\n        print('|'.join(':'.join(pair) for pair in pairs))\n\n        frame, display = performance_tables_from_records(records)\n\n        config = DisplayConfig(\n                cell_max_width_leftmost=np.inf,\n                cell_max_width=np.inf,\n                type_show=False,\n                display_rows=200\n                )\n        print(display.display(config))\n\n        if itemize:\n            alt = display.T\n            for c in alt.columns:\n                print(c)\n                print(alt[c].sort_values().display(config))\n\n        # import ipdb; ipdb.set_trace()\n        if 'sf/pd' in frame.columns:\n            print('mean: {}'.format(round(frame['sf/pd'].mean(), 6)))\n            print('wins: {}/{}'.format((frame['sf/pd'] < 1.05).sum(), len(frame)))\n\n\n\nif __name__ == '__main__':\n    main()\n"""
static_frame/performance/perf_test.py,0,"b""\n\n\nclass PerfTest:\n    PD_NAME = 'pd'\n    SF_NAME = 'sf'\n    FUNCTION_NAMES = ('np', PD_NAME, SF_NAME)\n    NUMBER = 200\n\n\n\n"""
static_frame/performance/pydata_2018.py,7,"b""import typing as tp\nimport itertools as it\nimport string\nimport os\nfrom urllib import request\n\nimport numpy as np\nimport pandas as pd\nimport static_frame as sf\n\n\nfrom static_frame.performance.perf_test import PerfTest\n\n\n\nclass SampleData:\n\n    _store: tp.Dict[str, tp.Any] = {}\n\n    URL_CSV = 'https://data.ny.gov/api/views/xe9x-a24f/rows.csv?accessType=DOWNLOAD'\n    FP_CSV = '/tmp/sf_pydata_2018.csv'\n    URL_JSON = 'https://jsonplaceholder.typicode.com/photos'\n\n    @classmethod\n    def create(cls) -> None:\n\n        if not os.path.exists(cls.FP_CSV):\n            with request.urlopen(cls.URL_CSV) as response:\n                with open(cls.FP_CSV, 'w') as f:\n                    f.write(response.read().decode('utf-8'))\n\n        cls._store['data_csv_fp'] = cls.FP_CSV\n        cls._store['data_json_url'] = cls.URL_JSON\n\n        labels_src = list(''.join(x) for x in it.combinations(string.ascii_lowercase, 4))\n        assert len(labels_src) > 10000\n\n        index = labels_src[:10000]\n        columns = labels_src[:1000]\n\n        data_float = np.random.rand(len(index), len(columns))\n\n        # alt floats, Bools\n        data_func = [\n                lambda: np.random.rand(len(index)),\n                lambda: np.random.randint(-1, 1, len(index)).astype(bool)\n                ]\n\n        cls._store['index'] = index\n        cls._store['index_target'] = [idx for idx in index if 'd' in idx]\n        cls._store['columns'] = columns\n        cls._store['columns_target'] = [c for c in columns if 'd' in c]\n        cls._store['data_float'] = data_float\n        cls._store['data_func'] = data_func\n\n        cls._store['sf.FrameFloat'] = sf.Frame(data_float, index=index, columns=columns)\n        cls._store['pd.FrameFloat'] = pd.DataFrame(data_float, index=index, columns=columns)\n\n        # mypy hates this:\n        data_cols = {i: data_func[i%2]() for i in range(len(columns))}  # type: ignore\n        cls._store['sf.FrameMixed'] = sf.Frame.from_dict(data_cols, index=index)\n        cls._store['pd.FrameMixed'] = pd.DataFrame(data_cols, index=index)\n\n    @classmethod\n    def get(cls, key: str) -> tp.Any:\n        return cls._store[key]\n\n\nclass FloatFrameStrLabel_100_Init(PerfTest):\n\n    NUMBER = 100\n\n    @classmethod\n    def pd(cls) -> None:\n        post = pd.DataFrame(SampleData.get('data_float'),\n                index=SampleData.get('index'),\n                columns=SampleData.get('columns')\n                )\n        assert post.shape == (10000, 1000)\n\n    @classmethod\n    def sf(cls) -> None:\n        post = sf.Frame(SampleData.get('data_float'),\n                index=SampleData.get('index'),\n                columns=SampleData.get('columns')\n                )\n        assert post.shape == (10000, 1000)\n\n\nclass FloatFrameStrLabel_101_ApplyArrayAxis1(PerfTest):\n\n    NUMBER = 10\n\n    @classmethod\n    def pd(cls) -> None:\n        frame = SampleData.get('pd.FrameFloat')\n        post = frame.apply(np.sum, axis=1, raw=True)\n        assert post.shape == (10000,)\n\n\n    @classmethod\n    def sf(cls) -> None:\n        frame = SampleData.get('sf.FrameFloat')\n        post = frame.iter_array(axis=1).apply(np.sum)\n        assert post.shape == (10000,)\n\nclass FloatFrameStrLabel_102_ApplyArrayAxis0(PerfTest):\n\n    NUMBER = 10\n\n    @classmethod\n    def pd(cls) -> None:\n        frame = SampleData.get('pd.FrameFloat')\n        post = frame.apply(np.sum, axis=0, raw=True)\n        assert post.shape == (1000,)\n\n    @classmethod\n    def sf(cls) -> None:\n        frame = SampleData.get('sf.FrameFloat')\n        post = frame.iter_array(axis=0).apply(np.sum)\n        assert post.shape == (1000,)\n\n\n\nclass FloatFrameStrLabel_103_ApplySeriesAxis1(PerfTest):\n\n    NUMBER = 2\n\n    @classmethod\n    def pd(cls) -> None:\n        frame = SampleData.get('pd.FrameFloat')\n        cols = SampleData.get('columns_target')\n        post = frame.apply(lambda s: s[cols].sum(), axis=1)\n        assert post.shape == (10000,)\n\n    @classmethod\n    def sf(cls) -> None:\n        frame = SampleData.get('sf.FrameFloat')\n        cols = SampleData.get('columns_target')\n        post = frame.iter_series(axis=1).apply(lambda s: s[cols].sum())\n        assert post.shape == (10000,)\n\nclass FloatFrameStrLabel_104_ApplySeriesAxis0(PerfTest):\n\n    NUMBER = 2\n\n    @classmethod\n    def pd(cls) -> None:\n        frame = SampleData.get('pd.FrameFloat')\n        indices = SampleData.get('index_target')\n        post = frame.apply(lambda s: s[indices].sum(), axis=0)\n        assert post.shape == (1000,)\n\n    @classmethod\n    def sf(cls) -> None:\n        frame = SampleData.get('sf.FrameFloat')\n        indices = SampleData.get('index_target')\n        post = frame.iter_series(axis=0).apply(lambda s: s[indices].sum())\n        assert post.shape == (1000,)\n\nclass FloatFrameStrLabel_105_SubsetConcat(PerfTest):\n\n    NUMBER = 2\n\n    @classmethod\n    def pd(cls) -> None:\n        frame = SampleData.get('pd.FrameFloat')\n        post = pd.concat((\n                frame[[c for c in frame.columns if c.startswith('ad')]],\n                frame[[c for c in frame.columns if c.startswith('ae')]]\n                ), axis=1)\n        assert post.shape == (10000, 441)\n\n    @classmethod\n    def sf(cls) -> None:\n        frame = SampleData.get('sf.FrameFloat')\n        post = sf.Frame.from_concat((\n                frame[[c for c in frame.columns if c.startswith('ad')]],\n                frame[[c for c in frame.columns if c.startswith('ae')]]\n                ), axis=1)\n        assert post.shape == (10000, 441)\n\n\n\n\n\n\n\nclass MixedFrameIntLabel_200_Init(PerfTest):\n\n    NUMBER = 10\n\n    @classmethod\n    def pd(cls) -> None:\n\n        data_func = SampleData.get('data_func')\n        columns = SampleData.get('columns')\n        index = SampleData.get('index')\n        post = pd.DataFrame({i: data_func[i%2]() for i in range(len(columns))}, index=index)\n        assert post.shape == (10000, 1000)\n\n    @classmethod\n    def sf(cls) -> None:\n        data_func = SampleData.get('data_func')\n        columns = SampleData.get('columns')\n        index = SampleData.get('index')\n        post = sf.Frame.from_dict({i: data_func[i%2]() for i in range(len(columns))}, index=index)\n        assert post.shape == (10000, 1000)\n\n\n\nclass MixedFrameIntLabel_201_SubsetSumAxis0(PerfTest):\n\n    NUMBER = 10\n\n    @classmethod\n    def pd(cls) -> None:\n\n        frame = SampleData.get('pd.FrameMixed')\n        post = frame[[c for c in frame.columns if c%2 == 0]].sum()\n        assert post.shape == (500,)\n\n    @classmethod\n    def sf(cls) -> None:\n        frame = SampleData.get('sf.FrameMixed')\n        post = frame[[c for c in frame.columns if c%2 == 0]].sum()\n        assert post.shape == (500,)\n\n\n\nclass MixedFrameIntLabel_202_SubsetSumAxis1(PerfTest):\n\n    NUMBER = 10\n\n    @classmethod\n    def pd(cls) -> None:\n\n        frame = SampleData.get('pd.FrameMixed')\n        post = frame[[c for c in frame.columns if c%2 == 0]].sum(axis=1)\n        assert post.shape == (10000,)\n\n    @classmethod\n    def sf(cls) -> None:\n        frame = SampleData.get('sf.FrameMixed')\n        post = frame[[c for c in frame.columns if c%2 == 0]].sum(axis=1)\n        assert post.shape == (10000,)\n\n\n\n\n\n\n\nclass MixedFrame_300_CSV(PerfTest):\n\n    NUMBER = 2\n\n    @classmethod\n    def pd(cls) -> None:\n        data_fp = SampleData.get('data_csv_fp')\n        post = pd.read_csv(data_fp)\n        assert post.shape == (1654482, 19)\n\n    @classmethod\n    def sf(cls) -> None:\n        data_fp = SampleData.get('data_csv_fp')\n        post = sf.Frame.from_csv(data_fp)\n        assert post.shape == (1654482, 19)\n\n\n\n\nclass MixedFrame_301_JSON(PerfTest):\n\n    NUMBER = 6\n\n    @classmethod\n    def pd(cls) -> None:\n        data_url = SampleData.get('data_json_url')\n        post = pd.read_json(data_url)\n        assert post.shape == (5000, 5)\n\n    @classmethod\n    def sf(cls) -> None:\n        data_url = SampleData.get('data_json_url')\n        post = sf.Frame.from_json_url(data_url)\n        assert post.shape == (5000, 5)\n\n\nclass MixedFrame_302_CSVHybrid(PerfTest):\n\n    NUMBER = 2\n\n    @classmethod\n    def pd(cls) -> None:\n        data_fp = SampleData.get('data_csv_fp')\n        post = pd.read_csv(data_fp)\n        assert post.shape == (1654482, 19)\n\n    @classmethod\n    def sf(cls) -> None:\n        data_fp = SampleData.get('data_csv_fp')\n        post = sf.Frame.from_pandas(pd.read_csv(data_fp))\n        assert post.shape == (1654482, 19)\n"""
static_frame/performance/pydata_2019.py,21,"b""\n\n\n# https://www.ocregister.com/2018/12/17/big-surf-possible-flooding-and-erosion-as-massive-swell-hits-the-coast-this-week/\n\n# field definitions\n# https://www.ndbc.noaa.gov/measdes.shtml\n\n# full data set\n# https://www.ndbc.noaa.gov/view_text_file.php?filename=46222h2018.txt.gz&dir=data/historical/stdmet/\n\n\n# Fitting Many Dimensions into One: The Promise of Hierarchical Indices for Data Beyond Two Dimensions\n\n# -----------------------------------------------\nimport typing as tp\nimport os\nimport pickle\n# import datetime\nfrom urllib import request\nfrom typing import NamedTuple\nimport functools\n\n\nimport numpy as np\nimport static_frame as sf\nimport pandas as pd\n\nfrom static_frame.performance.perf_test import PerfTest\n\n\nclass _PerfTestPanel(PerfTest):\n\n    NUMBER = 500\n    FUNCTION_NAMES = ('dict_df', 'panel_float', 'np', 'xarray_da', 'pd_mi_2D', 'sf_ih_2D', 'pd_mi_1D', 'sf_ih_1D')\n\n\n\n#-------------------------------------------------------------------------------\n#\n\nclass Buoy(NamedTuple):\n    station_id: int\n    name: str\n\nBUOYS = (\n    Buoy(46222, 'San Pedro'),\n    Buoy(46253, 'San Pedro South'),\n    Buoy(46221, 'Santa Monica Bay'),\n)\n\ndef cache_buoy(prefix, active=True):\n    def decorator(func):\n        def wrapper(cls, buoy, year):\n            fp = f'/tmp/{prefix}-{buoy.station_id}-{year}.p'\n            load_source = True\n            if active and os.path.exists(fp):\n                with open(fp, 'rb') as f:\n                    try:\n                        post = pickle.load(f)\n                        load_source = False\n                    except ModuleNotFoundError:\n                        pass\n            if load_source:\n                post = func(cls, buoy, year)\n                with open(fp, 'wb') as f:\n                    pickle.dump(post, f)\n            return post\n        return wrapper\n    return decorator\n\n\nclass BuoyLoader:\n\n    FIELD_DATETIME = 'datetime'\n    FIELD_STATION_ID = 'station_id'\n    FIELD_WAVE_HEIGHT = 'WVHT'\n    FIELD_WAVE_PERIOD = 'DPD'\t# Dominant wave period\n    FIELD_WAVE_DIRECTION = 'MWD'\n\n    COMPASS = ('N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE', 'S', 'SSW', 'SW', 'WSW', 'W', 'WNW', 'NW', 'NNW')\n    URL_TEMPLATE = 'https://www.ndbc.noaa.gov/view_text_file.php?filename={station_id}h{year}.txt.gz&dir=data/historical/stdmet/'\n\n    @classmethod\n    def buoy_record(cls,\n            line: str,\n            line_number: int,\n            station_id: int\n            ) -> tp.Sequence[str]:\n        timestamp = []\n\n        def gen() -> tp.Iterator[tp.Union[int, str]]:\n            yield cls.FIELD_STATION_ID if not line_number else station_id\n            cell_pos = -1 # increment before usage\n            for cell in line.split(' '):\n                cell = cell.strip()\n                if cell:\n                    cell_pos += 1\n                    if cell_pos < 5:\n                        timestamp.append(cell)\n                    elif cell_pos == 5:\n                        yield cls.FIELD_DATETIME if not line_number else '{}-{}-{}T{}:{}'.format(*timestamp)\n                        yield cell\n                    else:\n                        yield cell\n\n        return tuple(gen())\n\n    @classmethod\n    def buoy_to_records(cls,\n            buoy: Buoy,\n            year: int,\n            ) -> tp.Iterator[tp.Sequence[str]]:\n\n        url = cls.URL_TEMPLATE.format(station_id=buoy.station_id, year=year)\n\n        with request.urlopen(url) as response:\n            raw = response.read().decode('utf-8')\n\n        line_pos = -1 # increment before usage to allow skipped lines\n        for line in raw.split('\\n'):\n            if not line.strip():\n                continue\n            line_pos += 1\n            yield cls.buoy_record(line, line_pos, station_id=buoy.station_id)\n\n\n    @classmethod\n    def degree_to_compass(cls, degrees: np.array):\n        indices = np.floor(((degrees + 12.25) % 360) / 22.5).astype(int)\n        return np.array([cls.COMPASS[i] for i in indices])\n\n    @classmethod\n    @cache_buoy('sf')\n    def buoy_to_sf(cls, buoy: Buoy, year: int) -> sf.Frame:\n        '''\n        Return a simple Frame presentation without an index.\n        '''\n\n        records = cls.buoy_to_records(buoy, year=year)\n        columns = next(records)\n        units = next(records)\n\n        # can pass dtypes here, but doing it below consolidates blocks\n        dtypes = {\n                cls.FIELD_WAVE_HEIGHT: float,\n                cls.FIELD_WAVE_PERIOD: float,\n                cls.FIELD_DATETIME: np.datetime64}\n        f = sf.Frame.from_records(records, columns=columns, dtypes=dtypes)\n\n        direction = cls.degree_to_compass(f[cls.FIELD_WAVE_DIRECTION].astype(int).values)\n\n        f = f[[cls.FIELD_STATION_ID,\n                cls.FIELD_DATETIME,\n                cls.FIELD_WAVE_HEIGHT,\n                cls.FIELD_WAVE_PERIOD]].to_frame_go()\n\n        f[cls.FIELD_WAVE_DIRECTION] = direction\n\n        return f.to_frame()\n\n    @classmethod\n    @cache_buoy('df')\n    def buoy_to_pd(cls, buoy: Buoy, year: int):\n\n        records = cls.buoy_to_records(buoy, year=year)\n        columns = next(records)\n        units = next(records)\n\n        df = pd.DataFrame.from_records(records, columns=columns)\n\n        direction = cls.degree_to_compass(df[cls.FIELD_WAVE_DIRECTION].astype(int).values)\n\n        df = df[[cls.FIELD_STATION_ID,\n                cls.FIELD_DATETIME,\n                cls.FIELD_WAVE_HEIGHT,\n                cls.FIELD_WAVE_PERIOD]]\n\n        df[cls.FIELD_WAVE_DIRECTION] = direction\n\n        return df.astype({\n                cls.FIELD_WAVE_HEIGHT: float,\n                cls.FIELD_WAVE_PERIOD: float,\n                cls.FIELD_DATETIME: np.datetime64})\n\n\n#-------------------------------------------------------------------------------\n\n\nclass BuoySingleYear2D:\n\n    @staticmethod\n    def to_sf(year: int = 2018) -> sf.Frame:\n\n        frames = []\n        for buoy in BUOYS:\n            f = BuoyLoader.buoy_to_sf(buoy, year)\n            frames.append(f)\n\n        f = sf.Frame.from_concat(frames,\n                index=sf.IndexAutoFactory,\n                )\n        f = f.set_index_hierarchy(('station_id', 'datetime'),\n                index_constructors=(sf.Index, sf.IndexMinute),\n                drop=True)\n        return f\n\n    @staticmethod\n    def to_pd(year: int = 2018):\n\n        dfs = []\n        for buoy in BUOYS:\n            f = BuoyLoader.buoy_to_pd(buoy, year)\n            dfs.append(f)\n\n        df = pd.concat(dfs)\n\n        return df.set_index(['station_id', 'datetime']).sort_index()\n\n        # this sets to datetime, but does not a\n    @staticmethod\n    def to_pd_dict(year: int = 2018):\n\n        dfs = {}\n        for buoy in BUOYS:\n            df = BuoyLoader.buoy_to_pd(buoy, year)\n            df = df.set_index('datetime')[['DPD', 'WVHT', 'MWD']]\n            dfs[buoy.station_id] = df\n\n        return dfs\n\n\n    @staticmethod\n    def to_pd_panel_obj(year: int = 2018):\n        dfs = {}\n        for buoy in BUOYS:\n            df = BuoyLoader.buoy_to_pd(buoy, year)\n            df = df.set_index('datetime')[['DPD', 'WVHT', 'MWD']]\n            dfs[buoy.station_id] = df\n\n        return pd.Panel(dfs)\n\n\n    @staticmethod\n    def to_pd_panel_float(year: int = 2018):\n        dfs = {}\n        for buoy in BUOYS:\n            df = BuoyLoader.buoy_to_pd(buoy, year)\n            # remove station id, set datetime as indexnns\n            # df = df.set_index('datetime')[['DPD', 'WVHT']]\n            df = df.set_index('datetime')[['DPD', 'WVHT']]\n            dfs[buoy.station_id] = df\n\n        return pd.Panel(dfs)\n\n\n    @classmethod\n    def to_xarray_float(cls, year: int = 2018):\n        # this is an object typed DataArray\n        return cls.to_pd_panel_float(year=year).to_xarray()\n\n\n    @classmethod\n    def to_np(cls, year: int = 2018):\n        arrays = []\n\n        frames = [BuoyLoader.buoy_to_sf(buoy, year) for buoy in BUOYS]\n\n        # NOTE: take union and inssert NaNs\n        date_intersect = functools.reduce(lambda x, y: x | y, (set(f['datetime'].values) for f in frames))\n        date_intersect = sorted(date_intersect)\n\n        frames_aligned = []\n        for f in frames:\n            # frames_aligned.append(f.loc[f['datetime'].isin(date_intersect)])\n            f = f.set_index('datetime', drop=True)\n            f = f.reindex(date_intersect)\n            f = f.unset_index()\n            f = f.relabel(columns={'__index0__':'datetime'})\n            frames_aligned.append(f)\n\n\n        station_ids = {}\n        for idx, f in enumerate(frames_aligned):\n            datetime = {d: x for x, d in enumerate(f['datetime'].values)} # store the last one\n            station_id = int(f.loc[0, 'station_id']) # new index counts from zero\n            arrays.append(f[['DPD', 'WVHT']].values)\n            station_ids[station_id] = idx\n\n        indices = {'station_id': station_ids, 'datetime':  datetime, 'attr': {'DPD':0, 'WVHT':1}}\n        # import ipdb; ipdb.set_trace()\n        return np.array(arrays), indices\n\n\n    #---------------------------------------------------------------------------\n\n    @classmethod\n    def process_sf(cls) -> None:\n\n        fsf = cls.to_sf()\n\n        #-----------------------------------------------------------------------\n        # getting values out\n\n        # iterating parts of the index\n        part = tuple(fsf.index.iter_label(0))\n\n        # getting all the values at an array\n        part = fsf.index.values_at_depth(0)\n\n        # convert to a frame\n        part = fsf.index.to_frame()\n\n        # rehiearch (make dates outer, station id inner); note that changing the hiearchy order forces a reordering\n        part = fsf.rehierarch((1, 0))\n\n        # to tuples\n        part = fsf.relabel_flat(index=True)\n\n        # adding a level\n        part = fsf.relabel_add_level(index='A')\n\n        # dropping a level\n        #         ipdb> fsf.relabel_drop_level(index=1)\n        # *** static_frame.core.exception.ErrorInitIndex: labels (50350) have non-unique values (17034)\n        # ipdb>\n\n        #-----------------------------------------------------------------------\n        # show different types of selection\n\n        # getting a sample from a partial match\n        part = fsf.loc[sf.HLoc[:, '2018-12-18T07'], 'DPD']\n\n        # select based on partial time\n        post1 = fsf.loc[sf.HLoc[:, '2018-12-18T20']]\n\n        # getting a slice\n        part = fsf.loc[sf.HLoc[:, '2018-12-18T07':'2018-12-18T08'], 'DPD']\n\n        # dsicrete selection (this works on Pandas)\n        part = fsf.loc[sf.HLoc[:, ['2018-12-18T20:00', '2018-12-18T20:30']], 'DPD']\n\n        # can show iloc\n        part = fsf.loc[sf.HLoc[:, ['2018-12-18T20:00', '2018-12-18T20:30']], sf.ILoc[-1]]\n\n\n\n        # show getting labels with iter_label (unique values)\n        # show converting to a Frame\n\n        #-----------------------------------------------------------------------\n        # doing some analysis\n\n        # find max for givne day\n        fsf.loc[sf.HLoc[:, '2018-12-18']].max()\n\n\n        max_dpd = [fsf.loc[sf.HLoc[station_id], 'DPD'].loc_max() for station_id in fsf.index.iter_label(0)]\n        max_wvht = [fsf.loc[sf.HLoc[station_id], 'WVHT'].loc_max() for station_id in fsf.index.iter_label(0)]\n\n        # get the peaks of the two fields, but this does not get us to the date\n        peaks = fsf.loc[fsf.index.isin(max_dpd + max_wvht)]\n\n        # use 2 to get 1.731622836825856\n        threshold_wvht = fsf.loc[:, 'WVHT'].mean() + (fsf.loc[:, 'WVHT'].std() * 2)\n\n        # use 1 to get 15.889409302831822\n        threshold_dpd = fsf.loc[:, 'DPD'].mean() + fsf.loc[:, 'DPD'].std()\n\n        # this isolates the relevant days; but does not get 46253\n        # 2 and 18 gets all\n        targets = fsf.loc[(fsf.loc[:, 'WVHT'] > threshold_wvht) & (fsf.loc[:, 'DPD'] > threshold_dpd)]\n\n        targets = targets.to_frame_go()\n        targets['date'] = [d.date() for d in targets.index.values_at_depth(1)]\n\n\n        targets['station_id'] = targets.index.values_at_depth(0)\n        targets.iter_group(['date', 'station_id']).apply(len)\n\n        # targets.iter_group('date').apply(lambda x: len(x))\n        peaks_per_day = targets.iter_group('date').apply(len)\n        print(peaks_per_day)\n\n        def gen():\n            for date, date_frame in targets.iter_group_items('date'):\n                for station_id, station in date_frame.iter_group_labels_items(0):\n                    yield date, station_id, len(station)\n\n        post = sf.Frame.from_records(gen(), columns=('date', 'station_id', 'count'))\n        post = post.set_index_hierarchy(('date', 'station_id'),\n                drop=True,\n                index_constructors=(sf.IndexDate, sf.Index))\n\n        print(post)\n\n\n#-------------------------------------------------------------------------------\nclass BuoySingleYear1D:\n    '''\n    Fit all the data into a Series\n    '''\n\n    FIELD_ATTR = 'attr'\n\n    @staticmethod\n    def to_sf(year: int = 2018) -> sf.Frame:\n\n        labels = []\n        values = []\n\n        for buoy in BUOYS:\n            f = BuoyLoader.buoy_to_sf(buoy, year)\n            for row in f.iter_series(1):\n                for attr in (\n                        BuoyLoader.FIELD_WAVE_HEIGHT,\n                        BuoyLoader.FIELD_WAVE_PERIOD):\n                    label = (row[BuoyLoader.FIELD_STATION_ID],\n                            row[BuoyLoader.FIELD_DATETIME],\n                            attr)\n                    labels.append(label)\n                    values.append(row[attr])\n\n        index = sf.IndexHierarchy.from_labels(labels,\n                index_constructors=(sf.Index, sf.IndexMinute, sf.Index))\n\n        return sf.Series(values, index=index)\n\n\n    @staticmethod\n    def to_pd_obj(year: int = 2018) -> sf.Frame:\n\n        labels = []\n        values = []\n\n        for buoy in BUOYS:\n            df = BuoyLoader.buoy_to_pd(buoy, year)\n            for _, row in df.iterrows():\n                for attr in (\n                        BuoyLoader.FIELD_WAVE_HEIGHT,\n                        BuoyLoader.FIELD_WAVE_PERIOD,\n                        BuoyLoader.FIELD_WAVE_DIRECTION,\n                        ):\n                    label = (row[BuoyLoader.FIELD_STATION_ID],\n                            row[BuoyLoader.FIELD_DATETIME],\n                            attr)\n                    labels.append(label)\n                    values.append(row[attr])\n\n        # display of this index is terrible\n        index = pd.MultiIndex.from_tuples(labels)\n        return pd.Series(values, index=index).sort_index()\n\n\n\n    @staticmethod\n    def to_pd_float(year: int = 2018) -> sf.Frame:\n\n        labels = []\n        values = []\n\n        for buoy in BUOYS:\n            df = BuoyLoader.buoy_to_pd(buoy, year)\n            for _, row in df.iterrows():\n                for attr in (\n                        BuoyLoader.FIELD_WAVE_HEIGHT,\n                        BuoyLoader.FIELD_WAVE_PERIOD):\n                    label = (row[BuoyLoader.FIELD_STATION_ID],\n                            row[BuoyLoader.FIELD_DATETIME],\n                            attr)\n                    labels.append(label)\n                    values.append(row[attr])\n\n        # display of this index is terrible\n        index = pd.MultiIndex.from_tuples(labels)\n        return pd.Series(values, index=index).sort_index()\n\n\n#-------------------------------------------------------------------------------\n# performance tests\n\nclass SampleData:\n\n    _store: tp.Dict[str, tp.Any] = {}\n\n\n    @classmethod\n    def create(cls) -> None:\n        # fsf = BuoyLoader.buoy_to_sf(BUOYS[0], 2018)\n        # fpd = BuoyLoader.buoy_to_pd(BUOYS[0], 2018)\n\n\n        # cls._store['array_datetime'] = fsf['datetime'].values\n        # cls._store['array_station_id'] = tuple(b.station_id for b in BUOYS)\n        # cls._store['array_attr'] = ('WVHT', 'DPD', 'MWD')\n\n        # cls._store['sf_index_datetime'] = sf.IndexMinute(cls.get('array_datetime'))\n        # cls._store['sf_index_station_id'] = sf.Index(cls.get('array_station_id'))\n        # cls._store['sf_index_attr'] = sf.Index(cls.get('array_attr'))\n\n        # cls._store['pd_index_datetime'] = pd.Index(cls.get('array_datetime'))\n        # cls._store['pd_index_station_id'] = pd.Index(cls.get('array_station_id'))\n        # cls._store['pd_index_attr'] = pd.Index(cls.get('array_attr'))\n\n        # cls._store['sf_index_2D'] = sf.IndexHierarchy.from_product(\n        #         cls.get('array_datetime'),\n        #         cls.get('array_station_id')\n        #         )\n        # cls._store['sf_index_3D'] = sf.IndexHierarchy.from_product(\n        #         cls.get('array_datetime'),\n        #         cls.get('array_station_id'),\n        #         cls.get('array_attr')\n        #         )\n\n        # cls._store['tuple_index_2D'] = tuple(array2d_to_tuples(cls.get('sf_index_2D').values))\n        # cls._store['tuple_index_3D'] = tuple(array2d_to_tuples(cls.get('sf_index_3D').values))\n\n\n\n        cls._store['bsy2D_dict_df'] = BuoySingleYear2D.to_pd_dict()\n\n        # cls._store['bsy2D_panel_obj'] = BuoySingleYear2D.to_pd_panel_obj()\n\n        cls._store['bsy2D_panel_float'] = BuoySingleYear2D.to_pd_panel_float()\n\n        cls._store['bsy2D_np'] = BuoySingleYear2D.to_np()\n\n        cls._store['bsy2D_xarray_ds'] = BuoySingleYear2D.to_xarray_float()\n\n        cls._store['bsy2D_pd_mi_2D'] = BuoySingleYear2D.to_pd()\n\n        cls._store['bsy2D_sf_ih_2D'] = BuoySingleYear2D.to_sf()\n\n        cls._store['bsy2D_pd_mi_1D_obj'] = BuoySingleYear1D.to_pd_obj()\n\n        cls._store['bsy2D_pd_mi_1D_float'] = BuoySingleYear1D.to_pd_float()\n\n        cls._store['bsy2D_sf_ih_1D'] = BuoySingleYear1D.to_sf()\n\n\n\n    @classmethod\n    def get(cls, key: str) -> tp.Any:\n        return cls._store[key]\n\n\n#-------------------------------------------------------------------------------\n\nclass OuterSingleMiddleSingleInnerSingle(_PerfTestPanel):\n\n    @classmethod\n    def dict_df(cls) -> None:\n        data = SampleData.get('bsy2D_dict_df')\n        post = data[46253].loc['2018-12-17T10:00:00', 'WVHT']\n        assert post == 0.91\n\n    @classmethod\n    def panel_float(cls) -> None:\n        data = SampleData.get('bsy2D_panel_float')\n        post = data[46253, '2018-12-17T10:00:00', 'WVHT']\n        assert post == 0.91\n\n    @classmethod\n    def np(cls) -> None:\n        data, maps = SampleData.get('bsy2D_np')\n        # using dictionary lookups;\n        post = data[maps['station_id'][46253], maps['datetime'][np.datetime64('2018-12-17T10:00')], maps['attr']['WVHT']]\n        assert post == 0.91\n\n    @classmethod\n    def xarray_da(cls) -> None:\n        data = SampleData.get('bsy2D_xarray_ds')\n        post = data.loc[46253, '2018-12-17T10:00:00', 'WVHT']\n        assert float(post) == 0.91\n\n    @classmethod\n    def pd_mi_2D(cls) -> None:\n        data = SampleData.get('bsy2D_pd_mi_2D')\n        post = data.loc[pd.IndexSlice[46253, '2018-12-17T10:00:00'], 'WVHT']\n        assert post.values[0] == 0.91\n\n    @classmethod\n    def sf_ih_2D(cls) -> None:\n        data = SampleData.get('bsy2D_sf_ih_2D')\n        post = data.loc[sf.HLoc[46253, '2018-12-17T10:00:00'], 'WVHT']\n        assert post.values[0] == 0.91\n\n    @classmethod\n    def pd_mi_1D(cls) -> None:\n        data = SampleData.get('bsy2D_pd_mi_1D_float')\n        post = data.loc[46253, '2018-12-17T10:00:00', 'WVHT']\n        assert post == 0.91\n\n    @classmethod\n    def sf_ih_1D(cls) -> None:\n        data = SampleData.get('bsy2D_sf_ih_1D')\n        post = data[sf.HLoc[46253, '2018-12-17T10:00:00', 'WVHT']]\n        assert post == 0.91\n\n\n\n\nclass OuterSingleMiddleSliceInnerSingleMean(_PerfTestPanel):\n\n    @classmethod\n    def dict_df(cls) -> None:\n        data = SampleData.get('bsy2D_dict_df')\n        post = data[46253].loc['2018-12-17', 'WVHT'].mean()\n        np.testing.assert_almost_equal(post, 1.2519148936170212)\n\n\n    @classmethod\n    def panel_float(cls) -> None:\n        data = SampleData.get('bsy2D_panel_float')\n        post = data[46253, '2018-12-17', 'WVHT'].mean()\n        np.testing.assert_almost_equal(post, 1.2519148936170212)\n\n\n    @classmethod\n    def np(cls) -> None:\n        # this is a float array\n        data, maps = SampleData.get('bsy2D_np')\n        # using dictionary lookups;\n        post = np.nanmean(data[maps['station_id'][46253],\n                maps['datetime'][np.datetime64('2018-12-17T00:00')]:maps['datetime'][np.datetime64('2018-12-18T00:00')],\n                maps['attr']['WVHT']])\n        np.testing.assert_almost_equal(post, 1.2519148936170212)\n\n\n\n    @classmethod\n    def xarray_da(cls) -> None:\n        data = SampleData.get('bsy2D_xarray_ds')\n        post = data.loc[46253, '2018-12-17', 'WVHT'].mean()\n        np.testing.assert_almost_equal(float(post), 1.2519148936170212)\n\n\n\n    @classmethod\n    def pd_mi_2D(cls) -> None:\n        data = SampleData.get('bsy2D_pd_mi_2D')\n        post = data.loc[pd.IndexSlice[46253, '2018-12-17'], 'WVHT'].mean()\n        np.testing.assert_almost_equal(post, 1.2519148936170212)\n\n    @classmethod\n    def sf_ih_2D(cls) -> None:\n        data = SampleData.get('bsy2D_sf_ih_2D')\n        post = data.loc[sf.HLoc[46253, '2018-12-17'], 'WVHT'].mean()\n        np.testing.assert_almost_equal(post, 1.2519148936170212)\n\n\n\n    @classmethod\n    def pd_mi_1D(cls) -> None:\n        data = SampleData.get('bsy2D_pd_mi_1D_float')\n        # NOTE: could not get partial selection to work\n        # pandas.core.indexing.IndexingError: Too many indexers\n        post = data.loc[46253, '2018-12-17T00:00:00':'2018-12-17T23:30:00', 'WVHT'].mean()\n        np.testing.assert_almost_equal(post, 1.2519148936170212)\n\n    @classmethod\n    def sf_ih_1D(cls) -> None:\n        data = SampleData.get('bsy2D_sf_ih_1D')\n        post = data[sf.HLoc[46253, '2018-12-17', 'WVHT']].mean()\n        # post = data.loc[sf.HLoc[46253, '2018-12-17T00:00:00':'2018-12-17T23:30:00', 'WVHT']].mean()\n        np.testing.assert_almost_equal(post, 1.2519148936170212)\n\n\n\n\nclass OuterAllMiddleSliceInnerSelectionMax(_PerfTestPanel):\n\n    @classmethod\n    def dict_df(cls) -> None:\n        data = SampleData.get('bsy2D_dict_df')\n        post = pd.concat([data[k].loc['2018-12-17'] for k in data.keys()])[['DPD', 'WVHT']].max()\n        assert tuple(post.items()), (('DPD', 22.22), ('WVHT', 2.64))\n\n\n    @classmethod\n    def panel_float(cls) -> None:\n        data = SampleData.get('bsy2D_panel_float')\n        post = data[:, '2018-12-17'].max().max(axis=1)\n        assert tuple(post.items()), (('DPD', 22.22), ('WVHT', 2.64))\n\n\n    @classmethod\n    def np(cls) -> None:\n        # this is a float array\n        data, maps = SampleData.get('bsy2D_np')\n        date_slice = slice(maps['datetime'][np.datetime64('2018-12-17T00:00')],\n                maps['datetime'][np.datetime64('2018-12-18T00:00')])\n        post = np.vstack([data[maps['station_id'][k], date_slice] for k in maps['station_id'].keys()]).max(axis=0)\n        assert post.tolist(), [22.22, 2.64]\n\n\n    @classmethod\n    def xarray_da(cls) -> None:\n        data = SampleData.get('bsy2D_xarray_ds')\n        post = data.loc[:, '2018-12-17'].max(axis=1).max(axis=0).values\n        assert post.tolist(), [22.22, 2.64]\n\n\n\n    @classmethod\n    def pd_mi_2D(cls) -> None:\n        data = SampleData.get('bsy2D_pd_mi_2D')\n        post = data.loc[pd.IndexSlice[:, '2018-12-17'], ['WVHT', 'DPD']].max()\n        assert tuple(post.items()), (('DPD', 22.22), ('WVHT', 2.64))\n\n    @classmethod\n    def sf_ih_2D(cls) -> None:\n        data = SampleData.get('bsy2D_sf_ih_2D')\n        post = data.loc[sf.HLoc[:, '2018-12-17'], ['WVHT', 'DPD']].max()\n        assert tuple(post.items()), (('DPD', 22.22), ('WVHT', 2.64))\n\n\n\n    @classmethod\n    def pd_mi_1D(cls) -> None:\n        data = SampleData.get('bsy2D_pd_mi_1D_float')\n        post = tuple((attr, data.loc[:, '2018-12-17T00:00:00':'2018-12-17T23:30:00', attr].max()) for attr in ('DPD', 'WVHT'))\n        assert post, (('DPD', 22.22), ('WVHT', 2.64))\n\n    @classmethod\n    def sf_ih_1D(cls) -> None:\n        data = SampleData.get('bsy2D_sf_ih_1D')\n\n        post = tuple((attr, data[sf.HLoc[:, '2018-12-17', attr]].max()) for attr in ('DPD', 'WVHT'))\n        # post = data.loc[sf.HLoc[46253, '2018-12-17T00:00:00':'2018-12-17T23:30:00', 'WVHT']].mean()\n        assert post, (('DPD', 22.22), ('WVHT', 2.64))\n\n\n\n#-------------------------------------------------------------------------------\n# import typing as tp\n\n\nclass BestDefense:\n\n    @staticmethod\n    def processor(\n            functions: tp.Iterable[tp.Callable[[tp.Mapping[str, pd.Series]], pd.Series]],\n            init: pd.Series\n            ):\n\n        results = {'init': init}\n\n        for func in functions:\n            results[func.__name__] = func(results.copy())\n\n        return results\n\n    @classmethod\n    def processor_run(cls):\n\n        def square(vm: tp.Mapping[str, pd.Series]):\n            return vm['init'] ** 2\n\n        def invert(vm: tp.Mapping[str, pd.Series]):\n            return -vm['square']\n\n        init = pd.Series((5, 2, 6), index=tuple('abc'))\n        print(cls.processor(functions=[square, invert], init=init))\n\n\n    class DataInterface:\n\n        def __init__(self, size: int):\n            self._square = pd.Series(np.arange(size) ** 2)\n            self._invert = -self._square\n\n        @property\n        def square(self) -> pd.Series:\n            return self._square.copy()\n\n        @property\n        def invert(self) -> pd.Series:\n            return self._invert.copy()\n\n    @classmethod\n    def data_interface_run(cls):\n\n        di = cls.DataInterface(6)\n        print(di.square)\n        print(di.invert)\n\n\n\n\n#-------------------------------------------------------------------------------\n\n\nif __name__ == '__main__':\n\n    BestDefense.data_interface_run()\n\n\n#     # fsf = BuoyLoader.buoy_to_sf(BUOYS[0], 2018)\n#     fpd = BuoyLoader.buoy_to_pd(BUOYS[0], 2018)\n\n#     #-----------------------------------------------------------\n#     dfs = BuoySingleYear2D.to_pd_dict()\n#     # ipdb> {station_id: df.loc['2018-12-17', 'WVHT'].mean() for station_id, df in dfs.items()}\n#     # {46222: 1.5556249999999998, 46253: 1.2519148936170212, 46221: 1.6397916666666665}\n\n#     # ipdb> pd.DataFrame.from_records(([station_id,] + df[['WVHT', 'DPD']].mean().tolist() for station_id, df in dfs.items()), columns=('station_id', 'WVHT', 'DPD'))\n#     #    station_id      WVHT        DPD\n#     # 0       46222  0.970099  11.813405\n#     # 1       46253  0.927338  12.479156\n#     # 2       46221  1.012615  12.883838\n\n\n#     pna = BuoySingleYear2D.to_pd_panel_obj()\n#     # <class 'pandas.core.panel.Panel'>\n#     # Dimensions: 3 (items) x 17034 (major_axis) x 3 (minor_axis)\n#     # Items axis: 46222 to 46221\n#     # Major_axis axis: 2018-01-01 00:00:00 to 2018-12-31 23:30:00\n#     # Minor_axis axis: DPD to MWD\n\n#     # pna[:, '2018-12-17', ['WVHT', 'DPD']].mean()\n#     #           46222      46253      46221\n#     # WVHT   1.555625   1.251915   1.639792\n#     # DPD   14.927500  14.190851  14.811667\n\n#     # ipdb> pna[46222, :, 'DPD'].values\n#     # array([11.76, 10.53, 11.11, ..., 15.38, 15.38, 15.38], dtype=object)\n\n#     # ipdb> pna[46222].dtypes\n#     # DPD     object\n#     # WVHT    object\n#     # MWD     object\n#     # dtype: object\n\n#     # ipdb> pna.values.shape\n#     # (3, 17034, 3)\n\n#     pnb = BuoySingleYear2D.to_pd_panel_float()\n\n#     # ipdb> pnb\n#     # <class 'pandas.core.panel.Panel'>\n#     # Dimensions: 3 (items) x 17034 (major_axis) x 2 (minor_axis)\n#     # Items axis: 46222 to 46221\n#     # Major_axis axis: 2018-01-01 00:00:00 to 2018-12-31 23:30:00\n#     # Minor_axis axis: DPD to WVHT\n\n#     fpd = BuoySingleYear2D.to_pd()\n\n#     # ipdb> fpd.loc[pd.IndexSlice[46221, datetime.datetime(2018, 12, 16, 10, 30)], 'WVHT']\n#     #1.47\n\n# # ipdb> fpd.loc[pd.IndexSlice[46221, '2018-12-17'], 'WVHT']\n#     # *** pandas.errors.UnsortedIndexError: 'MultiIndex slicing requires the index to be lexsorted: slicing on levels [1], lexsort depth 0'\n\n#     # >>> fpd.sort_index(inplace=True)\n\n#     # ipdb> fpd.loc[pd.IndexSlice[46221, '2018-12-17'], 'WVHT'].head()\n#     # station_id  datetime\n#     # 46221       2018-12-17 00:00:00    1.43\n#     #             2018-12-17 00:30:00    1.32\n#     #             2018-12-17 01:00:00    1.38\n#     #             2018-12-17 01:30:00    1.42\n#     #             2018-12-17 02:00:00    1.36\n#     # Name: WVHT, dtype: float64\n\n#     # ipdb> fpd.loc[pd.IndexSlice[:, '2018-12-17'], ['WVHT', 'DPD']].mean()\n#     # WVHT     1.484056\n#     # DPD     14.646503\n#     # dtype: float64\n#     # ipdb> fpd.loc[pd.IndexSlice[:, '2018-12-18'], ['WVHT', 'DPD']].mean()\n#     # WVHT     2.155594\n#     # DPD     16.404965\n#     # dtype: float64\n\n\n\n#     fsf = BuoySingleYear2D.to_sf()\n\n#     BuoySingleYear2D.process_sf()\n\n\n\n#     # BuoySingleYear2D.process_np()\n#     # BuoySingleYear2D.process_pd_panel()\n\n\n#     # ssf = BuoySingleYear1D.to_sf()\n#     spda = BuoySingleYear1D.to_pd_obj()\n#     spdb = BuoySingleYear1D.to_pd_float()\n\n#     # df = BuoySingleYear2D.process_pd_multi_index()\n\n\n\n\n\n\n\n\n\n\n\n\n\n# example of pandas series that supports partial matching\n# In : s1 = pd.Series(range(60), index=pd.date_range('1999-12', freq='D', periods=60))\n\n# s1 = pd.Series(range(120), pd.MultiIndex.from_product((('a', 'b'), pd.date_range('1999-12', freq='D', periods=60))))\n# # this does not work\n# s1[pd.IndexSlice['a', '2000']]\n\n\n# partial date string matching\n# https://github.com/pandas-dev/pandas/issues/25165\n\n\n# class BuoyMultiYear:\n\n#     @staticmethod\n#     def to_sf(years: tp.Iterable[int] = (2017, 2017, 2018)) -> sf.Frame:\n\n#         frames = []\n#         for buoy in BUOYS:\n#             for year in years:\n#                 f = BuoyLoader.buoy_to_sf(buoy, year).to_frame_go()\n#                 f['year'] = year\n#                 f['month'] = [int(x.split('-')[1]) for x in f['datetime'].values]\n#                 # import ipdb; ipdb.set_trace()\n#                 frames.append(f)\n\n#         f = sf.Frame.from_concat(frames,\n#                 axis=0,\n#                 index=sf.IndexAutoFactory,\n#                 name='buos_multi_year'\n#                 )\n#         # NOTE: this fails unexpectedly\n#         f = f.set_index_hierarchy(('station_id', 'datetime'),\n#                 index_constructors=(sf.Index, sf.IndexMinute),\n#                 drop=True)\n#         return f\n\n"""
static_frame/test/__init__.py,0,b''
static_frame/test/test_case.py,24,"b'\nimport unittest\nimport os\nimport sys\nfrom itertools import zip_longest\nimport typing as tp\nimport itertools as it\nimport string\nimport cmath\nimport sqlite3\nimport contextlib\nimport tempfile\nfrom pathlib import Path\n\nimport numpy as np\nimport pytest\n\n\nfrom static_frame import TypeBlocks\nfrom static_frame.core.container import ContainerOperand\nfrom static_frame.core.index_base import IndexBase\nfrom static_frame.core.frame import Frame\nfrom static_frame.core.util import PathSpecifier\n\n\n# for running with coverage\n# pytest -s --color no --disable-pytest-warnings --cov=static_frame --cov-report html static_frame/test\n# for running with native traveback\n# pytest -s --color no --disable-pytest-warnings --tb=native\n\n\nskip_win = pytest.mark.skipif(\n        sys.platform == \'win32\',\n        reason=\'Windows default dtypes\'\n        )\n\n@contextlib.contextmanager\ndef temp_file(suffix: tp.Optional[str] = None,\n        path: bool = False\n        ) -> tp.Iterator[PathSpecifier]:\n    try:\n        f = tempfile.NamedTemporaryFile(suffix=suffix, delete=False)\n        tmp_name = f.name\n        f.close()\n        if path:\n            yield Path(tmp_name)\n        else:\n            yield tmp_name\n    finally:\n        if os.path.exists(tmp_name):\n            os.unlink(tmp_name)\n\n\nclass TestCase(unittest.TestCase):\n    \'\'\'\n    TestCase specialized for usage with StaticFrame\n    \'\'\'\n\n    @staticmethod\n    def get_arrays_a() -> tp.Iterator[np.ndarray]:\n        \'\'\'\n        Return sample array suitable for TypeBlock creation, testing. Unique values required.\n        \'\'\'\n\n        a1 = np.array([[1, 2, 3], [10, 20, 30], [100, 200, 300]])\n        a1.flags.writeable = False\n\n        a2 = np.array([[4], [5], [6]])\n        a2.flags.writeable = False\n\n        a3 = np.array([[None, \'a\', None], [\'q\', \'x\', \'c\'], [\'f\', \'y\', \'e\']])\n        a3.flags.writeable = False\n\n        a4 = np.array([1.2, np.nan, 30.5])\n        a4.flags.writeable = False\n\n        for arrays in it.permutations((a1, a2, a3, a4)):\n            yield arrays\n\n\n    @staticmethod\n    def get_arrays_b() -> tp.Iterator[np.ndarray]:\n        \'\'\'\n        Return sample array suitable for TypeBlock creation, testing. Many NaNs.\n        \'\'\'\n\n        a1 = np.array([[1, 2, 3], [10, 20, 30], [100, 200, 300]])\n        a1.flags.writeable = False\n\n        a2 = np.array([[4], [5], [6]])\n        a2.flags.writeable = False\n\n        a3 = np.array([[None, \'a\', None], [None, None, \'c\'], [\'f\', None, \'e\']])\n        a3.flags.writeable = False\n\n        a4 = np.array([np.nan, np.nan, np.nan])\n        a4.flags.writeable = False\n\n        for arrays in it.permutations((a1, a2, a3, a4)):\n            yield arrays\n\n\n    @staticmethod\n    def get_letters(*slice_args: tp.Optional[int]) -> tp.Iterator[str]:\n        for letter in string.ascii_lowercase[slice(*slice_args)]:\n            yield letter\n\n    @staticmethod\n    def get_test_input(file_name: str) -> str:\n        # input dir should be a sibling of this module\n        fp_module = os.path.join(os.getcwd(), __file__)\n        fp = os.path.join(os.path.dirname(fp_module), \'input\', file_name)\n        if not os.path.isfile(fp):\n            raise RuntimeError(\'file not found\', fp)\n        return fp\n\n\n    @staticmethod\n    def get_containers() -> tp.Iterator[tp.Type[ContainerOperand]]:\n\n        def yield_sub(cls: tp.Type[ContainerOperand]) -> tp.Iterator[tp.Type[ContainerOperand]]:\n            for cls in cls.__subclasses__():\n                if cls is not IndexBase:\n                    yield cls\n                if issubclass(cls, ContainerOperand):\n                    yield from yield_sub(cls)\n\n        yield from yield_sub(ContainerOperand)\n\n    @staticmethod\n    def get_test_db_a() -> sqlite3.Connection:\n        conn = sqlite3.connect(\':memory:\')\n        c = conn.cursor()\n        c.execute(\'\'\'CREATE TABLE events\n             (date text, identifier text, value real, count int)\'\'\')\n        for identifier in (\'a1\', \'b2\'):\n            for date in (\'2006-01-01\', \'2006-01-02\'):\n                c.execute(f""INSERT INTO events VALUES (\'{date}\',\'{identifier}\',12.5,8)"")\n        conn.commit()\n        return conn\n\n\n    #---------------------------------------------------------------------------\n\n    def assertEqualWithNaN(self,\n            v1: object,\n            v2: object,\n            ) -> None:\n        if ((isinstance(v1, float) or isinstance(v1, np.floating))\n                and np.isnan(v1)\n                and (isinstance(v2, float) or isinstance(v2, np.floating))\n                and np.isnan(v2)\n                ):\n            return\n\n        if ((isinstance(v1, complex) or isinstance(v1, np.complexfloating))\n                and cmath.isnan(v1)\n                and (isinstance(v2, complex) or isinstance(v1, np.complexfloating))\n                and cmath.isnan(v2)  # type: ignore\n                ):\n            return\n        return self.assertEqual(v1, v2)\n\n\n    def assertAlmostEqualArray(self, a1: np.ndarray, a2: np.ndarray) -> None:\n        # NaNs are treated as equal\n        np.testing.assert_allclose(a1, a2)\n        # np.testing.assert_array_almost_equal(a1, a2, decimal=5)\n\n    def assertTypeBlocksArrayEqual(self,\n            tb: TypeBlocks,\n            match: tp.Iterable[object],\n            match_dtype: tp.Optional[tp.Union[type, np.dtype, str]] = None) -> None:\n        \'\'\'\n        Args:\n            tb: a TypeBlocks instance\n            match: can be anything that can be used to create an array.\n        \'\'\'\n        # NOTE: this is comparing the potentially casted .values view, not each block\n        # could use np.testing\n        if not isinstance(match, np.ndarray):\n            match = np.array(match, dtype=match_dtype)\n        self.assertTrue((tb.values == match).all())\n\n\n    def assertAlmostEqualValues(self,\n            values1: tp.Iterable[object], values2: tp.Iterable[object]) -> None:\n\n        for v1, v2 in zip_longest(values1, values2):\n            self.assertEqualWithNaN(v1, v2)\n\n    def assertAlmostEqualItems(self,\n            pairs1: tp.Iterable[tp.Tuple[tp.Hashable, object]],\n            pairs2: tp.Iterable[tp.Tuple[tp.Hashable, object]]) -> None:\n\n        for (k1, v1), (k2, v2) in zip_longest(pairs1, pairs2):\n            self.assertEqual(k1, k2)\n\n            if isinstance(v1, float) and np.isnan(v1) and isinstance(v2, float) and np.isnan(v2):\n                continue\n\n            self.assertEqual(v1, v2)\n\n\n    def assertAlmostEqualFramePairs(self,\n            pairs1: tp.Iterable[tp.Tuple[tp.Hashable, tp.Iterable[object]]],\n            pairs2: tp.Iterable[tp.Tuple[tp.Hashable, tp.Iterable[object]]]) -> None:\n        \'\'\'\n        For comparing nested tuples returned by Frame.to_pairs()\n        \'\'\'\n        # NOTE: this does not look at dtype or container classes\n        for (k1, v1), (k2, v2) in zip_longest(pairs1, pairs2):\n            self.assertEqual(k1, k2)\n            self.assertAlmostEqualItems(v1, v2)\n\n\n    def assertEqualFrames(self,\n            f1: Frame,\n            f2: Frame,\n            compare_dtype: bool = True\n            ) -> None:\n\n        if not f1.equals(f2, compare_dtype=compare_dtype):\n            self.assertTrue(f1.index.equals(f2.index, compare_dtype=compare_dtype), \'index do not match\')\n            self.assertTrue(f1.columns.equals(f2.columns, compare_dtype=compare_dtype), \'columns do not match\')\n            self.assertTrue(f1._blocks.equals(f2._blocks, compare_dtype=compare_dtype), \'_blocks do not match\')\n            self.fail(\'class or name do not match\')\n\n\n\n    def assertEqualLines(self, lines1: str, lines2: str) -> None:\n        \'\'\'After splitting and stripping, compare non-empty lines.\n        \'\'\'\n        def clean_lines(lines: str) -> tp.Iterator[str]:\n            for line in lines.split(\'\\n\'):\n                line = line.strip()\n                if line:\n                    yield line\n        for l1, l2 in zip(clean_lines(lines1), clean_lines(lines2)):\n            self.assertEqual(l1, l2)\n\n\n# Helpful base types for testing\nclass UnHashable:\n    \'\'\'UnHashable means __eq__ without defining __hash__\'\'\'\n\n    def __init__(self, val: tp.Any) -> None:\n        self.val = val\n\n    def __eq__(self, other: tp.Any) -> bool:\n        return hasattr(other, \'val\') and self.val == other.val\n'"
static_frame/test/integration/__init__.py,0,b''
static_frame/test/integration/test_field_stats.py,6,"b""\nimport unittest\nimport typing as tp\n\nimport numpy as np\nimport static_frame as sf\n\nfrom static_frame.test.test_case import TestCase\n\nCHARACTERS_REFERENCE = dict((\n        ('count', len),\n        ('count<0', lambda x: len(x.loc[x < 0])),\n        ('count>0', lambda x: len(x.loc[x > 0])),\n        ('count_unique', lambda x: len(x.unique())),\n        ('min', lambda x: x.min()),\n        ('max', lambda x: x.max()),\n        ('sum', lambda x: x.sum()),\n        ('mean', lambda x: x.mean()),\n        ('median', lambda x: x.median()),\n        ('nanfill', lambda x: x.isna().sum() / len(x)),\n        ))\n\nclass FieldStatConfig:\n\n    CHARACTERS = tuple((k, v) for k, v in CHARACTERS_REFERENCE.items())\n\n    CHARACTERS_AGGREGATE = tuple((k, v)\n            for k, v in CHARACTERS_REFERENCE.items()\n            if not (k.startswith('count') or k.startswith('nan')))\n\n    GROUP_BY = 'group_by'\n    CATEGORY_A_FIELDS: tp.Iterable[tp.Tuple[str, tp.Any]] = (('a1', None), ('a2', None))\n    CATEGORY_B_FIELDS: tp.Iterable[tp.Tuple[str, tp.Any]] = (('b1', None), ('b2', None))\n\n    # store pairs of lable, config attr\n    CATEGORY_LABELS = (\n            ('category_a', 'CATEGORY_A_FIELDS'),\n            ('category_b', 'CATEGORY_B_FIELDS'),\n            )\n\n\n\ndef process(\n        frame: sf.Frame,\n        config: tp.Type[FieldStatConfig] = FieldStatConfig\n        ) -> sf.Frame:\n    '''\n    Perform statistical analysis, returning a new Frame.\n    '''\n    # print(frame.columns.display(sf.DisplayConfigs.UNBOUnND))\n\n    def observations(\n            fields: tp.Iterable[tp.Tuple[str, tp.Any]],\n            ) -> tp.Iterator[sf.Series]:\n        for k, group in frame.iter_group_items(config.GROUP_BY):\n\n            def gen() -> tp.Iterator[tp.Tuple[tp.Tuple[str, str], sf.Series]]:\n                for field, converter in fields:\n                    # the key here becomes columns\n                    converter = converter if converter else lambda x: x\n                    for label, func in config.CHARACTERS:\n                        yield (field, label), func(converter(group[field])) # type: ignore\n\n            # name will become row (index) identifier\n            yield sf.Series.from_items(gen(),\n                    name=k,\n                    index_constructor=sf.IndexHierarchy.from_labels)\n\n    def field_categories() -> tp.Iterator[sf.Frame]:\n\n        for label, attr in config.CATEGORY_LABELS:\n            # for each category, get all fields, and process each character, for each field, into one Frame\n            fields = getattr(config, attr)\n\n\n            post = sf.Frame.from_concat(observations(fields))\n\n            # import ipdb; ipdb.set_trace()\n\n            # create more rows with axis config.CHARACTERS over the groups\n            def gen() -> tp.Iterator[sf.Frame]:\n                for label, func in config.CHARACTERS_AGGREGATE:\n                    yield func(post).rename(label) # type: ignore\n\n            post = sf.Frame.from_concat((post, sf.Frame.from_concat(gen())))\n\n            # round float, rotate table, and new index for this level\n            # NOTe: after iter_lement apply, we are loosing hierarchucal index\n            yield post.iter_element().apply(lambda e: round(e, 3)).T.relabel_add_level(label)\n\n    # vertically stack all Frame for each category\n    return sf.Frame.from_concat(field_categories()) # type: ignore\n\n\n\nclass TestUnit(TestCase):\n\n\n    def test_process(self) -> None:\n\n        a1 = np.array([.1, -.1, np.nan, .2])\n        a2 = np.array([.4, np.nan, .7, -.5])\n\n        records = (\n            a1,\n            a1 * .2,\n            a1 * -.5,\n            a1 * 0,\n            np.flip(a1),\n            np.flip(a1) * .2,\n            a2,\n            a2 * .2,\n            a2 * -.5,\n            a2 * 0,\n            np.flip(a2),\n            np.flip(a2) * .2,\n        )\n\n        f = sf.FrameGO.from_records(records, columns=('a1', 'a2', 'b1', 'b2'))\n        f['group_by'] = ['x'] * (len(f) // 2) + ['y'] * (len(f) // 2)\n\n\n        post = process(f)\n\n        self.assertEqual(post.shape, (40, 7))\n        self.assertEqual(post.index.depth, 3)\n\n        self.assertEqual(post.iloc[0].to_pairs(),\n            (('x', 6.0), ('y', 6.0), ('min', 6.0), ('max', 6.0), ('sum', 12.0), ('mean', 6.0), ('median', 6.0))\n            )\n\n        self.assertEqual(post['x'].to_pairs(),\n            ((('category_a', 'a1', 'count'), 6.0), (('category_a', 'a1', 'count<0'), 1.0), (('category_a', 'a1', 'count>0'), 4.0), (('category_a', 'a1', 'count_unique'), 6.0), (('category_a', 'a1', 'min'), -0.05), (('category_a', 'a1', 'max'), 0.2), (('category_a', 'a1', 'sum'), 0.31), (('category_a', 'a1', 'mean'), 0.052), (('category_a', 'a1', 'median'), 0.03), (('category_a', 'a1', 'nanfill'), 0.0), (('category_a', 'a2', 'count'), 6.0), (('category_a', 'a2', 'count<0'), 2.0), (('category_a', 'a2', 'count>0'), 1.0), (('category_a', 'a2', 'count_unique'), 6.0), (('category_a', 'a2', 'min'), -0.1), (('category_a', 'a2', 'max'), 0.05), (('category_a', 'a2', 'sum'), -0.07), (('category_a', 'a2', 'mean'), -0.018), (('category_a', 'a2', 'median'), -0.01), (('category_a', 'a2', 'nanfill'), 0.333), (('category_b', 'b1', 'count'), 6.0), (('category_b', 'b1', 'count<0'), 2.0), (('category_b', 'b1', 'count>0'), 0.0), (('category_b', 'b1', 'count_unique'), 6.0), (('category_b', 'b1', 'min'), -0.1), (('category_b', 'b1', 'max'), -0.02), (('category_b', 'b1', 'sum'), -0.12), (('category_b', 'b1', 'mean'), -0.06), (('category_b', 'b1', 'median'), -0.06), (('category_b', 'b1', 'nanfill'), 0.667), (('category_b', 'b2', 'count'), 6.0), (('category_b', 'b2', 'count<0'), 1.0), (('category_b', 'b2', 'count>0'), 4.0), (('category_b', 'b2', 'count_unique'), 6.0), (('category_b', 'b2', 'min'), -0.1), (('category_b', 'b2', 'max'), 0.2), (('category_b', 'b2', 'sum'), 0.26), (('category_b', 'b2', 'mean'), 0.043), (('category_b', 'b2', 'median'), 0.03), (('category_b', 'b2', 'nanfill'), 0.0))\n        )\n\n        self.assertAlmostEqualValues(post.sum().values.tolist(),\n            [64.467, 67.12100000000001, 56.066999999999986, 75.52100000000004, 131.587, 65.794, 65.794])\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/integration/test_large_file.py,0,"b""\nimport unittest\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.core.frame import Frame\nfrom static_frame.test.test_case import temp_file\nfrom static_frame.core.index_auto import IndexAutoFactory\n\nclass TestUnit(TestCase):\n\n    def test_exceed_columns(self) -> None:\n\n        f1 = Frame.from_element('x', index='x', columns=range(16384))\n\n        with temp_file('.xlsx') as fp:\n\n            with self.assertRaises(RuntimeError):\n                # with the index, the limit is exceeded\n                f1.to_xlsx(fp, include_index=True)\n\n\n            f1.to_xlsx(fp, include_index=False)\n            f2 = Frame.from_xlsx(fp, index_depth=0, columns_depth=1)\n            # need to remove index on original for appropriate comparison\n            self.assertEqualFrames(f1.relabel(index=IndexAutoFactory), f2)\n\n\n    def test_exceed_rows(self) -> None:\n\n        f1 = Frame.from_element('x', index=range(1048576), columns='x')\n\n        with temp_file('.xlsx') as fp:\n\n            with self.assertRaises(RuntimeError):\n                # with the index, the limit is exceeded\n                f1.to_xlsx(fp, include_columns=True)\n\n            # NOTE: it takes almost 60s to write this file, so we will skip testing it\n            # f1.to_xlsx(fp, include_columns=False)\n            # f2 = Frame.from_xlsx(fp, index_depth=1, columns_depth=0)\n            # # need to remove index on original for appropriate comparison\n            # self.assertEqualFrames(f1.relabel(columns=IndexAutoFactory), f2)\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n\n"""
static_frame/test/integration/test_main.py,1,"b'from subprocess import PIPE\nfrom subprocess import Popen\nfrom sys import executable\n\nfrom pytest import mark\nimport numpy as np\nimport pandas as pd\n\nimport static_frame as sf\n\n\ntry:\n    import IPython # pylint: disable=W0611\nexcept ImportError:\n    HAS_IPYTHON = False\nelse:\n    HAS_IPYTHON = True\n\n\nCOMMAND = b\'(np.__name__, np.__version__, pd.__name__, pd.__version__, sf.__name__, sf.__version__)\'\n\n\ndef _test_main(python: str) -> None:\n\n    result = f""{eval(COMMAND.decode(), {\'np\': np, \'pd\': pd, \'sf\': sf})}"".encode()\n\n    args = (python, \'-m\', \'static_frame\')\n\n    process = Popen(args, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n    stdout = process.communicate(COMMAND)[0]\n\n    assert not process.returncode\n    # disabling this check as it fails in a TOX context\n    #assert result in stdout\n\n\ndef test_main_python() -> None:\n    _test_main(executable)\n\n\n@mark.skipif(not HAS_IPYTHON, reason=\'Requires IPython.\')  # type: ignore\ndef test_main_ipython() -> None:\n    _test_main(\'ipython\')\n'"
static_frame/test/integration/test_pydoc.py,0,"b""\nimport unittest\nimport pydoc\nfrom static_frame.test.test_case import TestCase\n\nclass TestUnit(TestCase):\n\n    # this test is slow\n    def test_interface_help_a(self) -> None:\n\n        for target in self.get_containers():\n            post = pydoc.render_doc(target, renderer=pydoc.plaintext) # type: ignore\n            self.assertTrue(len(post) > 0)\n\nif __name__ == '__main__':\n    unittest.main()\n\n"""
static_frame/test/property/__init__.py,0,b''
static_frame/test/property/strategies.py,23,"b'import typing as tp\nfrom enum import Enum\n\nfrom functools import partial\nfrom functools import lru_cache\nfrom itertools import chain\nfrom itertools import repeat\n\nfrom hypothesis import strategies as st\nfrom hypothesis.extra import numpy as hypo_np\nfrom hypothesis import settings as hypo_settings\nfrom hypothesis import HealthCheck\n\n\nimport numpy as np\n\nfrom static_frame.core.util import DTYPE_OBJECT\nfrom static_frame.core.util import DTYPE_BOOL\n# from static_frame.core.util import DTYPE_NAN_KIND\n\nfrom static_frame import TypeBlocks\n\nfrom static_frame import Index\nfrom static_frame import IndexGO\nfrom static_frame import IndexYear\nfrom static_frame import IndexYearGO\nfrom static_frame import IndexYearMonth\nfrom static_frame import IndexYearMonthGO\nfrom static_frame import IndexDate\nfrom static_frame import IndexDateGO\nfrom static_frame import IndexHour\nfrom static_frame import IndexHourGO\nfrom static_frame import IndexMinute\nfrom static_frame import IndexMinuteGO\nfrom static_frame import IndexMillisecond\nfrom static_frame import IndexMillisecondGO\nfrom static_frame import IndexMicrosecond\nfrom static_frame import IndexMicrosecondGO\nfrom static_frame import IndexNanosecond\nfrom static_frame import IndexNanosecondGO\n\n\n\nfrom static_frame import IndexHierarchy\nfrom static_frame import IndexHierarchyGO\n\n\nfrom static_frame import Series\nfrom static_frame import Frame\nfrom static_frame import FrameGO\n\n\nMAX_ROWS = 8\nMAX_COLUMNS = 10\n\n\nhypo_settings.register_profile(""sf"",\n        suppress_health_check=[HealthCheck.too_slow],\n        deadline=None,\n        )\nhypo_settings.load_profile(""sf"")\n\n#-------------------------------------------------------------------------------\n# spacings\n\n# @lru_cache(maxsize=32)\ndef subset_contiguous_sum(target: int) -> tp.Tuple[tp.Tuple[int, ...], ...]:\n    \'\'\'\n    Return an iterabel of integers that sum to the target. This does not find all combinations or permutations, just all combintation of the range from 1 to the number (inclusive).\n    \'\'\'\n    # based on https://stackoverflow.com/questions/4632322/finding-all-possible-combinations-of-numbers-to-reach-a-given-sum\n\n    if target == 0:\n        return ()\n\n    if not (0 < target <= 32):\n        # over sizes of 60 or so performance is noticieable\n        raise RuntimeError(f\'target is too large: {target}\')\n\n    @lru_cache()\n    def subset_sum(\n            numbers: tp.Sequence[int], partial: tp.Tuple[int, ...] = (), partial_sum: int = 0,\n        ) -> tp.Iterator[tp.Tuple[int, ...]]:\n        if partial_sum == target:\n            yield partial\n        if partial_sum > target:\n            return\n        for i, n in enumerate(numbers, start=1):\n            # get pairs of index (starting at 1) and n (the value at each position)\n            yield from subset_sum(numbers[i:], partial + (n,), partial_sum + n)\n\n    return tuple(subset_sum(range(1, target+1)))\n\n\ndef get_spacing(size: int = MAX_COLUMNS) -> st.SearchStrategy:\n    # generate permutations of the orderings of the integers\n    return st.one_of((st.permutations(c) for c in subset_contiguous_sum(size)))\n\n\n#-------------------------------------------------------------------------------\n# values\n\n# 55203 is just before ""high surrogates"", and avoids this exception\n# UnicodeDecodeError: \'utf-32-le\' codec can\'t decode bytes in position 0-3: code point in surrogate code point range(0xd800, 0xe000)\nST_CODEPOINT_LIMIT = dict(min_codepoint=1, max_codepoint=55203)\n\nST_TYPES_COMMON: tp.Tuple[tp.Callable[..., st.SearchStrategy], ...] = (\n        st.integers,\n        # st.decimals,\n        st.fractions,\n        st.dates,\n        st.datetimes,\n        partial(st.characters, **ST_CODEPOINT_LIMIT),\n        partial(st.text, st.characters(**ST_CODEPOINT_LIMIT))  # type: ignore\n        )\n\nST_TYPES_FLOAT_NAN: tp.Tuple[st.SearchStrategy, ...] = (\n        st.floats,\n        st.complex_numbers,\n        )\n\nfilter_nan = lambda x: not np.isnan(x)\n\nST_TYPES_FLOAT_NO_NAN: tp.Tuple[tp.Callable[[], st.SearchStrategy], ...] = (\n        lambda: st.floats().filter(filter_nan),\n        lambda: st.complex_numbers().filter(filter_nan)\n        )\n\nST_TYPES_UNARY_BINARY = (st.booleans, st.none)\n\n# common collections\n\nST_TYPES_FOR_UNIQUE = ST_TYPES_FLOAT_NO_NAN + ST_TYPES_COMMON\nST_TYPES_FOR_UNIQUE_MIXED = ST_TYPES_FLOAT_NO_NAN + ST_TYPES_COMMON + ST_TYPES_UNARY_BINARY\nST_VALUE = ST_TYPES_FLOAT_NAN + ST_TYPES_COMMON + ST_TYPES_UNARY_BINARY\n\ndef get_value() -> st.SearchStrategy:\n    \'\'\'\n    Any plausible value.\n    \'\'\'\n    return st.one_of(strat() for strat in ST_VALUE)\n\ndef get_label() -> st.SearchStrategy:\n    \'\'\'\n    A hashable suitable for use in an Index. While NaNs are supported as labels in Index objects, the unique constraint used below does not enforce uniqueness for NaNs, and thus we must filter out NaNs in advance.\n    \'\'\'\n    return st.one_of((strat() for strat in ST_TYPES_FOR_UNIQUE_MIXED))\n\n\ndef get_labels(\n        min_size: int = 0,\n        max_size: int = MAX_ROWS) -> st.SearchStrategy:\n    \'\'\'\n    Labels are suitable for creating non-date Indices (though they might include dates)\n    \'\'\'\n    def gen() -> tp.Iterator[st.SearchStrategy]:\n\n        yield st.lists(get_label(),\n                min_size=min_size,\n                max_size=max_size,\n                unique=True)\n\n        for strat in ST_TYPES_FOR_UNIQUE:\n            yield st.lists(\n                    strat(),\n                    min_size=min_size,\n                    max_size=max_size,\n                    unique=True)\n\n    return st.one_of(gen())\n\n\n#-------------------------------------------------------------------------------\n# dtypes\n\n\nclass DTGroup(Enum):\n    # strategy constantly generating object dtype\n    # NOTE: we branch on this type in get_array_from_dtype_group\n    OBJECT = (partial(st.just, DTYPE_OBJECT),)\n    ALL = (hypo_np.scalar_dtypes,)\n\n    NUMERIC = (\n            hypo_np.floating_dtypes,\n            hypo_np.integer_dtypes,\n            hypo_np.complex_number_dtypes\n            )\n\n    BOOL = (partial(st.just, DTYPE_BOOL),)\n    STRING = (hypo_np.unicode_string_dtypes,)\n\n    YEAR = (partial(hypo_np.datetime64_dtypes, min_period=\'Y\', max_period=\'Y\'),)\n    YEAR_MONTH = (partial(hypo_np.datetime64_dtypes, min_period=\'M\', max_period=\'M\'),)\n    DATE = (partial(hypo_np.datetime64_dtypes, min_period=\'D\', max_period=\'D\'),)\n    HOUR = (partial(hypo_np.datetime64_dtypes, min_period=\'h\', max_period=\'h\'),)\n    MINUTE = (partial(hypo_np.datetime64_dtypes, min_period=\'m\', max_period=\'m\'),)\n    SECOND = (partial(hypo_np.datetime64_dtypes, min_period=\'s\', max_period=\'s\'),)\n    MILLISECOND = (partial(hypo_np.datetime64_dtypes, min_period=\'ms\', max_period=\'ms\'),)\n    MICROSECOND = (partial(hypo_np.datetime64_dtypes, min_period=\'us\', max_period=\'us\'),)\n    NANOSECOND = (partial(hypo_np.datetime64_dtypes, min_period=\'ns\', max_period=\'ns\'),)\n\n    # derived\n    NUMERIC_REAL = (\n            hypo_np.floating_dtypes,\n            hypo_np.integer_dtypes,\n            )\n    DATETIME = tuple(chain(\n            YEAR,\n            YEAR_MONTH,\n            DATE,\n            HOUR,\n            SECOND,\n            MILLISECOND,\n            MICROSECOND,\n            NANOSECOND,\n            ))\n\n    BASIC = NUMERIC + BOOL + STRING\n\n    # NOTE: duplicate non-datetime to produce more balanced distribution\n    CORE = tuple(chain(\n            # OBJECT, # object has to be handled with get_array_from_dtype_group\n            NUMERIC, NUMERIC, NUMERIC,\n            BOOL, BOOL, BOOL,\n            STRING, STRING, STRING,\n            DATETIME,\n            ))\n\ndef get_dtype(dtype_group: DTGroup = DTGroup.ALL) -> st.SearchStrategy:\n\n    def st_dts() -> tp.Iterator[st.SearchStrategy]:\n        for st_dt in dtype_group.value:\n            yield st_dt()\n\n    return st.one_of(st_dts())\n\ndef get_dtypes(\n        min_size: int = 0,\n        max_size: int = MAX_COLUMNS,\n        dtype_group: DTGroup = DTGroup.ALL,\n        ) -> st.SearchStrategy:\n    return st.lists(get_dtype(dtype_group), min_size=min_size)\n\ndef get_dtype_pairs(\n        dtype_group: DTGroup = DTGroup.ALL,\n        ) -> st.SearchStrategy:\n    return st.tuples(get_dtype(dtype_group), get_dtype(dtype_group))\n\n#-------------------------------------------------------------------------------\n# shape generation\n\ndef get_shape_1d(min_size: int = 0, max_size: int = MAX_ROWS) -> st.SearchStrategy:\n    return st.tuples(st.integers(min_value=min_size, max_value=max_size))\n\ndef get_shape_2d(\n        min_rows: int = 1,\n        max_rows: int = MAX_ROWS,\n        min_columns: int = 1,\n        max_columns: int = MAX_COLUMNS,\n        ) -> st.SearchStrategy:\n    return st.tuples(\n            st.integers(min_value=min_rows, max_value=max_rows),\n            st.integers(min_value=min_columns, max_value=max_columns)\n            )\n\ndef get_shape_1d2d(\n        min_rows: int = 1,\n        max_rows: int = MAX_ROWS,\n        min_columns: int = 1,\n        max_columns: int = MAX_COLUMNS) -> st.SearchStrategy:\n\n    return st.one_of(\n            get_shape_2d(\n                    min_rows=min_rows,\n                    max_rows=max_rows,\n                    min_columns=min_columns,\n                    max_columns=max_columns),\n            get_shape_1d(\n                    min_size=min_rows,\n                    max_size=max_rows)\n            )\n\n#-------------------------------------------------------------------------------\n# array generation\n\ndef get_array_object(\n        shape: tp.Tuple[int, ...] = (MAX_ROWS, MAX_COLUMNS),\n        unique: bool = True) -> st.SearchStrategy:\n    if unique:\n        # if unique, cannot use fill\n        return hypo_np.arrays(\n                shape=shape,\n                dtype=get_dtype(DTGroup.OBJECT),\n                elements=get_value(),\n                fill=st.nothing(),\n                unique=unique\n                )\n    return hypo_np.arrays(\n            shape=shape,\n            dtype=get_dtype(DTGroup.OBJECT),\n            elements=get_value(),\n            fill=st.none(),\n            unique=unique\n            )\n\n\ndef get_array_from_dtype_group(\n        dtype_group: DTGroup = DTGroup.ALL,\n        shape: tp.Tuple[int, ...] = (MAX_ROWS, MAX_COLUMNS),\n        unique: bool = True) -> st.SearchStrategy:\n    \'\'\'\n    Given a dtype group and shape, get array. Handles manually creating and filling object arrays when dtype group is object or ALL.\n    \'\'\'\n\n    # TODO: can remove floating-point NaNs when necessary with .map call with this function on array generators; can apply based on DTYPE group\n\n    # def fill_na(array: np.ndarray) -> np.ndarray:\n    #     if array.dtype.kind in DTYPE_NAN_KIND:\n    #         is_nan = np.isnan(array)\n    #         if is_nan.any():\n    #             fill = np.empty(array.shape, dtype=array.dtype)\n    #             array[is_nan] = fill[is_nan]\n    #             return array\n    #     return array\n\n    array_object = get_array_object(\n            shape=shape,\n            unique=unique\n            )\n    array_non_object = hypo_np.arrays(\n            get_dtype(dtype_group),\n            shape,\n            unique=unique\n            )\n\n    if dtype_group == DTGroup.OBJECT:\n        return array_object\n    if dtype_group in (DTGroup.ALL, DTGroup.CORE):\n        return st.one_of(array_non_object, array_non_object, array_object)\n    return array_non_object\n\n\ndef get_array_1d(\n        min_size: int = 0,\n        max_size: int = MAX_ROWS,\n        unique: bool = False,\n        dtype_group: DTGroup = DTGroup.ALL\n        ) -> st.SearchStrategy:\n\n    shape = get_shape_1d(min_size=min_size, max_size=max_size)\n    return get_array_from_dtype_group(\n            dtype_group=dtype_group,\n            shape=shape,\n            unique=unique\n            )\n\n\ndef get_array_2d(\n        min_rows: int = 1,\n        max_rows: int = MAX_ROWS,\n        min_columns: int = 1,\n        max_columns: int = MAX_COLUMNS,\n        unique: bool = False,\n        dtype_group: DTGroup = DTGroup.ALL\n        ) -> st.SearchStrategy:\n\n    shape = get_shape_2d(\n            min_rows=min_rows,\n            max_rows=max_rows,\n            min_columns=min_columns,\n            max_columns=max_columns\n            )\n\n    return get_array_from_dtype_group(\n            dtype_group=dtype_group,\n            shape=shape,\n            unique=unique\n            )\n\n\ndef get_array_1d2d(\n        min_rows: int = 1,\n        max_rows: int = MAX_ROWS,\n        min_columns: int = 1,\n        max_columns: int = MAX_COLUMNS,\n        dtype_group: DTGroup = DTGroup.ALL\n        ) -> st.SearchStrategy:\n    \'\'\'\n    For convenience in building blocks, treat row constraints as 1d size constraints.\n    \'\'\'\n    array_2d = get_array_2d(\n            min_rows=min_rows,\n            max_rows=max_rows,\n            min_columns=min_columns,\n            max_columns=max_columns,\n            dtype_group=dtype_group\n            )\n\n    if 1 in range(min_columns, max_columns + 1):\n        # if min/max columns are given, and column of 1 is not supported, it is incorrect to give back a 1D array (in the context of the usage of this in blocks)\n        return st.one_of(\n                array_2d,\n                get_array_1d(\n                        min_size=min_rows,\n                        max_size=max_rows,\n                        dtype_group=dtype_group\n                        )\n                )\n    return array_2d\n\n#-------------------------------------------------------------------------------\n# aligend arrays for concatenation and type blocks\n\ndef get_arrays_2d_aligned_columns(\n        min_size: int = 1,\n        max_size: int = 10) -> st.SearchStrategy:\n\n    return st.integers(min_value=1, max_value=MAX_COLUMNS).flatmap(\n        lambda columns: st.lists(\n            get_array_2d(\n                min_columns=columns,\n                max_columns=columns\n                ),\n            min_size=min_size,\n            max_size=max_size\n            )\n    )\n\ndef get_arrays_2d_aligned_rows(\n        min_size: int = 1,\n        max_size: int = 10) -> st.SearchStrategy:\n\n    return st.integers(min_value=1, max_value=MAX_ROWS).flatmap(\n        lambda rows: st.lists(\n            get_array_2d(\n                min_rows=rows,\n                max_rows=rows,\n                ),\n            min_size=min_size,\n            max_size=max_size\n            )\n    )\n\ndef get_arrays_2d_aligned(\n        min_size: int = 1,\n        max_size: int = 10) -> st.SearchStrategy:\n\n    return get_shape_2d().flatmap(\n        lambda shape: st.lists(\n            get_array_2d(\n                min_rows=shape[0],\n                max_rows=shape[0],\n                min_columns=shape[1],\n                max_columns=shape[1],\n                ),\n            min_size=min_size,\n            max_size=max_size\n            )\n    )\n\ndef get_blocks(\n        min_rows: int = 1,\n        max_rows: int = MAX_ROWS,\n        min_columns: int = 1,\n        max_columns: int = MAX_COLUMNS,\n        dtype_group: DTGroup = DTGroup.ALL\n        ) -> st.SearchStrategy:\n    \'\'\'\n    Args:\n        min_columns: number of resultant columns in combination of all arrays.\n    \'\'\'\n\n    def constructor(rows_column_widths: tp.Tuple[int, tp.Iterator[int]]) -> st.SearchStrategy:\n        rows, column_widths = rows_column_widths\n\n        def array_gen() -> tp.Iterator[st.SearchStrategy]:\n            for width in column_widths:\n                yield get_array_1d2d(\n                    min_rows=rows,\n                    max_rows=rows,\n                    min_columns=width,\n                    max_columns=width,\n                    dtype_group=dtype_group\n                    )\n\n        return st.tuples(*array_gen())\n\n    def get_column_widths(shape: tp.Tuple[int, int]) -> st.SearchStrategy:\n        rows, columns = shape\n        return st.tuples(st.just(rows), get_spacing(columns)).flatmap(constructor)\n\n    return get_shape_2d(\n            min_rows=min_rows,\n            max_rows=max_rows,\n            min_columns=min_columns,\n            max_columns=max_columns).flatmap(get_column_widths)\n\n\ndef get_type_blocks(\n        min_rows: int = 0,\n        max_rows: int = MAX_ROWS,\n        min_columns: int = 0,\n        max_columns: int = MAX_COLUMNS,\n        dtype_group: DTGroup = DTGroup.ALL\n        ) -> st.SearchStrategy:\n    return st.builds(TypeBlocks.from_blocks,\n            get_blocks(min_rows=min_rows,\n                    max_rows=max_rows,\n                    min_columns=min_columns,\n                    max_columns=max_columns,\n                    dtype_group=dtype_group)\n            )\n\n\nget_type_blocks_numeric: tp.Callable[..., st.SearchStrategy] = partial(get_type_blocks, dtype_group=DTGroup.NUMERIC)\nget_type_blocks_numeric.__name__ = \'get_type_blocks_numeric\'\n\n\ndef get_type_blocks_aligned_array(\n        min_rows: int = 0,\n        max_rows: int = MAX_ROWS,\n        min_columns: int = 0,\n        max_columns: int = MAX_COLUMNS,\n        dtype_group: DTGroup = DTGroup.ALL\n        ) -> st.SearchStrategy:\n    \'\'\'\n    Return TypeBlocks instance, as well as an array aligned by row size.\n    \'\'\'\n    def constructor(shape: tp.Tuple[int, int]) -> st.SearchStrategy:\n        rows, columns = shape\n        return st.tuples(\n                get_type_blocks(\n                        min_rows=rows,\n                        max_rows=rows,\n                        min_columns=columns,\n                        max_columns=columns,\n                        dtype_group=dtype_group\n                        ),\n                get_array_1d2d( # let columns be variable3\n                        min_rows=rows,\n                        max_rows=rows,\n                        dtype_group=dtype_group\n                        )\n                )\n\n    return get_shape_2d(\n            min_rows=min_rows,\n            max_rows=max_rows,\n            min_columns=min_columns,\n            max_columns=max_columns,\n            ).flatmap(constructor)\n\ndef get_type_blocks_aligned_type_blocks(\n        min_size: int = 0,\n        max_size: int = MAX_ROWS,\n        min_rows: int = 0,\n        max_rows: int = MAX_ROWS,\n        min_columns: int = 0,\n        max_columns: int = MAX_COLUMNS,\n        dtype_group: DTGroup = DTGroup.ALL\n        ) -> st.SearchStrategy:\n    \'\'\'\n    Return an iterable of TypeBlocks instances, all alligned by row count\n    \'\'\'\n    def constructor(shape: tp.Tuple[int, int]) -> st.SearchStrategy:\n        rows, columns = shape\n        return st.lists(\n                get_type_blocks(\n                        min_rows=rows,\n                        max_rows=rows,\n                        dtype_group=dtype_group\n                        ),\n                min_size=min_size,\n                max_size=max_size\n                )\n\n    return get_shape_2d(\n            min_rows=min_rows,\n            max_rows=max_rows,\n            min_columns=min_columns,\n            max_columns=max_columns,\n            ).flatmap(constructor)\n\n#-------------------------------------------------------------------------------\n# index objects\n\ndef get_index(\n        min_size: int = 0,\n        max_size: int = MAX_ROWS,\n        dtype_group: tp.Optional[DTGroup] = None,\n        cls: tp.Type[Index] = Index\n        ) -> st.SearchStrategy:\n    # NOTE: have observed cases where a non-unqiue index is returned: with float/int 0, or two NaNs. Need to filter\n    # using get_labels here forces Index construction from lists, rather than from arrays\n    if dtype_group is not None:\n        return st.builds(cls, get_array_1d(\n                min_size=min_size,\n                max_size=max_size,\n                unique=True,\n                dtype_group=dtype_group\n                ))\n    return st.builds(cls, get_labels(min_size=min_size, max_size=max_size))\n\nget_index_date: tp.Callable[..., st.SearchStrategy] = partial(get_index,\n        cls=IndexDate,\n        dtype_group=DTGroup.DATE)\nget_index_date.__name__ = \'get_index_date\'\n\nget_index_year: tp.Callable[..., st.SearchStrategy] = partial(get_index,\n        cls=IndexYear,\n        dtype_group=DTGroup.YEAR)\nget_index_year.__name__ = \'get_index_year\'\n\n\nget_index_go: tp.Callable[..., st.SearchStrategy] = partial(get_index, cls=IndexGO)\nget_index_go.__name__ = \'get_index_go\'\n\ndef get_index_any(\n        min_size: int = 0,\n        max_size: int = MAX_ROWS,\n        ) -> st.SearchStrategy:\n\n    strategies = []\n    for cls, dtype_group in (\n            (Index, DTGroup.CORE),\n            (IndexGO, DTGroup.CORE),\n            (IndexYear, DTGroup.YEAR),\n            (IndexYearGO, DTGroup.YEAR),\n            (IndexYearMonth, DTGroup.YEAR_MONTH),\n            (IndexYearMonthGO, DTGroup.YEAR_MONTH),\n            (IndexDate, DTGroup.DATE),\n            (IndexDateGO, DTGroup.DATE),\n            (IndexHour, DTGroup.HOUR),\n            (IndexHourGO, DTGroup.HOUR),\n            (IndexMinute, DTGroup.MINUTE),\n            (IndexMinuteGO, DTGroup.MINUTE),\n            (IndexMillisecond, DTGroup.MILLISECOND),\n            (IndexMillisecondGO, DTGroup.MILLISECOND),\n            (IndexMicrosecond, DTGroup.MICROSECOND),\n            (IndexMicrosecondGO, DTGroup.MICROSECOND),\n            (IndexNanosecond, DTGroup.NANOSECOND),\n            (IndexNanosecondGO, DTGroup.NANOSECOND),\n            ):\n        st_index = get_index(\n                min_size=min_size,\n                max_size=max_size,\n                dtype_group=dtype_group,\n                cls=cls,\n                )\n        strategies.append(st_index)\n\n    return st.one_of(strategies)\n\n\ndef get_index_hierarchy(\n        min_size: int = 1,\n        max_size: int = MAX_ROWS,\n        min_depth: int = 2,\n        max_depth: int = 5,\n        dtype_group: tp.Optional[DTGroup] = None,\n        cls: tp.Callable[..., IndexHierarchy] = IndexHierarchy.from_labels\n        ) -> st.SearchStrategy:\n\n    def constructor(\n            labels_spacings: tp.Tuple[tp.Sequence[tp.Sequence[str]], tp.Sequence[tp.Iterable[int]]]\n            ) -> st.SearchStrategy:\n        # returns an iterable of labels\n        labels_proto, spacings = labels_spacings\n        depth = len(labels_proto)\n        size = len(labels_proto[0])\n\n        # update all labels (except the deepest) by repeating values a number of times, as determined by spacings\n        labels: tp.List[tp.Optional[tp.Sequence[str]]] = [None for _ in range(depth)]\n        for d in range(depth):\n            if d >= depth - 1:\n                labels[d] = labels_proto[d]\n            else:\n                spacing = spacings[d]\n\n                def spans() -> tp.Iterator[tp.Iterator[str]]:\n                    idx = 0\n                    for count in spacing:\n                        if count == 0:\n                            continue\n                        yield repeat(labels_proto[d][idx], count)\n                        idx += count\n\n                labels[d] = list(chain.from_iterable(spans()))\n\n        def label_gen() -> tp.Iterator[tp.List[str]]:\n            for i in range(size):\n                yield [tp.cast(tp.Sequence[str], labels[d])[i] for d in range(depth)]\n\n        return st.builds(\n                cls,\n                st.just(label_gen()) # can just handle a generator\n                )\n\n    # generate depth-sized lists of candidate leabels and spacings\n    def get_labels_spacings(depth_size: tp.Tuple[int, int]) -> st.SearchStrategy:\n        depth, size = depth_size\n\n        if dtype_group is not None:\n            level = get_array_1d(min_size=size,\n                    max_size=size,\n                    unique=True,\n                    dtype_group=dtype_group)\n        else:\n            level = get_labels(min_size=size, max_size=size)\n\n        labels = st.lists(level, min_size=depth, max_size=depth)\n        # could do depth minus 1\n        spacings = st.lists(get_spacing(size), min_size=depth, max_size=depth)\n        return st.tuples(labels, spacings).flatmap(constructor)\n\n    # generate depth and size, pass to get get_labels\n    return st.tuples(\n            st.integers(min_value=min_depth, max_value=max_depth),\n            st.integers(min_value=min_size, max_value=max_size)\n            ).flatmap(get_labels_spacings)\n\n\ndef get_index_hierarchy_any(\n        min_size: int = 1,\n        max_size: int = MAX_ROWS,\n        min_depth: int = 2,\n        max_depth: int = 5,\n        ) -> st.SearchStrategy:\n\n\n    def get_labels_spacings(depth_size: tp.Tuple[int, int]) -> st.SearchStrategy:\n        depth, size = depth_size\n        args = []\n        for _ in range(depth):\n            args.append(get_index_any(\n                    min_size=size,\n                    max_size=size,\n                    ))\n\n        return st.one_of(\n                st.builds(IndexHierarchy.from_product, *args),\n                st.builds(IndexHierarchyGO.from_product, *args),\n                )\n\n    return st.tuples(\n            st.integers(min_value=min_depth, max_value=max_depth),\n            st.integers(min_value=min_size, max_value=max_size)\n            ).flatmap(get_labels_spacings)\n\n\n\n#-------------------------------------------------------------------------------\n# series objects\n\ndef get_series(\n        min_size: int = 0,\n        max_size: int = MAX_ROWS,\n        cls: tp.Type[Series] = Series,\n        dtype_group: DTGroup = DTGroup.ALL,\n        index_cls: tp.Type[Index] = Index,\n        index_dtype_group: tp.Optional[DTGroup] = None\n        ) -> st.SearchStrategy:\n\n    def constructor(shape: tp.Tuple[int]) -> st.SearchStrategy:\n        size = shape[0] # tuple len 1\n\n        if issubclass(index_cls, IndexHierarchy):\n            index = get_index_hierarchy(\n                    cls=index_cls.from_labels,\n                    min_size=size,\n                    max_size=size,\n                    dtype_group=index_dtype_group,\n            )\n        else:\n            index = get_index(\n                    cls=index_cls,\n                    min_size=size,\n                    max_size=size,\n                    dtype_group=index_dtype_group,\n            )\n\n        return st.builds(cls,\n            get_array_1d(\n                    min_size=size,\n                    max_size=size,\n                    dtype_group=dtype_group\n                    ),\n            index=index\n            )\n\n    return get_shape_1d(min_size=min_size, max_size=max_size).flatmap(constructor)\n\n# label index, values\nget_series_date_numeric: tp.Callable[..., st.SearchStrategy] = partial(get_series,\n        dtype_group=DTGroup.NUMERIC,\n        index_cls=IndexDate,\n        index_dtype_group=DTGroup.DATE\n        )\nget_series_date_numeric.__name__ = \'get_series_date_numeric\'\n\n# depth greater than 1 index\nget_series_str_dgt1_numeric: tp.Callable[..., st.SearchStrategy] = partial(get_series,\n        min_size=1,\n        dtype_group=DTGroup.NUMERIC,\n        index_cls=IndexHierarchy,\n        index_dtype_group=DTGroup.STRING\n        )\nget_series_str_dgt1_numeric.__name__ = \'get_series_str_dgt1_numeric\'\n\nget_series_obj_dgt1_numeric: tp.Callable[..., st.SearchStrategy] = partial(get_series,\n        min_size=1,\n        dtype_group=DTGroup.NUMERIC,\n        index_cls=IndexHierarchy,\n        index_dtype_group=DTGroup.OBJECT\n        )\nget_series_obj_dgt1_numeric.__name__ = \'get_series_obj_dgt1_numeric\'\n\n\n#-------------------------------------------------------------------------------\n# frames\n\ndef get_frame(\n        min_rows: int = 1,\n        max_rows: int = MAX_ROWS,\n        min_columns: int = 1,\n        max_columns: int = MAX_COLUMNS,\n        cls: tp.Type[Frame] = Frame,\n        dtype_group: DTGroup = DTGroup.ALL,\n        index_cls: tp.Type[Index] = Index,\n        index_dtype_group: tp.Optional[DTGroup] = None,\n        columns_cls: tp.Type[Index] = Index,\n        columns_dtype_group: tp.Optional[DTGroup] = None\n        ) -> st.SearchStrategy:\n\n    def constructor(shape: tp.Tuple[int, int]) -> st.SearchStrategy:\n\n        row_count, column_count = shape\n\n        if issubclass(index_cls, IndexHierarchy):\n            index = get_index_hierarchy(\n                    cls=index_cls.from_labels,\n                    min_size=row_count,\n                    max_size=row_count,\n                    dtype_group=index_dtype_group,\n            )\n        else:\n            index = get_index(\n                    cls=index_cls,\n                    min_size=row_count,\n                    max_size=row_count,\n                    dtype_group=index_dtype_group,\n            )\n\n        if issubclass(index_cls, IndexHierarchy):\n            columns = get_index_hierarchy(\n                    cls=columns_cls.from_labels,\n                    min_size=column_count,\n                    max_size=column_count,\n                    dtype_group=columns_dtype_group,\n            )\n        else:\n            columns = get_index(\n                    cls=columns_cls,\n                    min_size=column_count,\n                    max_size=column_count,\n                    dtype_group=columns_dtype_group,\n            )\n\n        return st.builds(cls,\n                get_type_blocks(\n                        min_rows=row_count,\n                        max_rows=row_count,\n                        min_columns=column_count,\n                        max_columns=column_count,\n                        dtype_group=dtype_group\n                        ),\n                index=index,\n                columns=columns\n                )\n\n    return get_shape_2d(min_rows=min_rows,\n            max_rows=max_rows,\n            min_columns=min_columns,\n            max_columns=max_columns\n            ).flatmap(constructor)\n\n\n# label index, columns, values\nget_frame_date_str_numeric: tp.Callable[..., st.SearchStrategy] = partial(get_frame,\n        dtype_group=DTGroup.NUMERIC,\n        index_cls=IndexDate,\n        index_dtype_group=DTGroup.DATE,\n        columns_cls=Index,\n        columns_dtype_group=DTGroup.STRING\n        )\nget_frame_date_str_numeric.__name__ = \'get_frame_date_str_numeric\'\n\nget_frame_go: tp.Callable[..., st.SearchStrategy] = partial(get_frame, cls=FrameGO)\nget_frame_go.__name__ = \'get_frame_go\'\n\ndef get_frame_or_frame_go(\n        min_rows: int = 1,\n        max_rows: int = MAX_ROWS,\n        min_columns: int = 1,\n        max_columns: int = MAX_COLUMNS,\n        dtype_group: DTGroup = DTGroup.ALL,\n        index_cls: tp.Type[Index] = Index,\n        index_dtype_group: tp.Optional[DTGroup] = None,\n        columns_cls: tp.Type[Index] = Index,\n        columns_dtype_group: tp.Optional[DTGroup] = None\n        ) -> st.SearchStrategy:\n    \'\'\'\n    Return either a ``Frame`` or a ``FrameGO``,\n    \'\'\'\n    st_frame = get_frame(\n            min_rows=min_rows,\n            max_rows=max_rows,\n            min_columns=min_columns,\n            max_columns=max_columns,\n            cls=Frame,\n            dtype_group=dtype_group,\n            index_cls=index_cls,\n            index_dtype_group=index_dtype_group,\n            columns_cls=columns_cls,\n            columns_dtype_group=columns_dtype_group\n            )\n    st_frame_go = get_frame(\n            min_rows=min_rows,\n            max_rows=max_rows,\n            min_columns=min_columns,\n            max_columns=max_columns,\n            cls=FrameGO,\n            dtype_group=dtype_group,\n            index_cls=index_cls,\n            index_dtype_group=index_dtype_group,\n            columns_cls=columns_cls,\n            columns_dtype_group=columns_dtype_group\n            )\n    return st.one_of((st_frame, st_frame_go))\n\n\n\n\nif __name__ == \'__main__\':\n    import fnmatch\n    from argparse import ArgumentParser\n    from static_frame.core.display_color import HexColor\n\n    parser = ArgumentParser()\n    parser.add_argument(\'-n\', \'--name\', default=None)\n    parser.add_argument(\'-c\', \'--count\', default=2, type=int)\n\n    options = parser.parse_args()\n\n    local_items = tuple(locals().items())\n    for v in (v for k, v in local_items if callable(v) and k.startswith(\'get\')):\n\n        if options.name:\n            if not fnmatch.fnmatch(v.__name__, options.name):\n                continue\n\n        print(HexColor.format_terminal(\'grey\', \'.\' * 50))\n        print(HexColor.format_terminal(\'hotpink\', str(v.__name__)))\n\n        for x in range(options.count):\n            print(HexColor.format_terminal(\'grey\', \'.\' * 50))\n            example = v().example()\n            print(repr(example))\n'"
static_frame/test/property/test_frame.py,4,"b""\nimport typing as tp\nimport unittest\nimport operator\nimport os\nimport sqlite3\n\nimport numpy as np\n\n# from hypothesis import strategies as st\nfrom hypothesis import given\n\n\nfrom static_frame.core.frame import Frame\nfrom static_frame.core.frame import FrameGO\nfrom static_frame.core.series import Series\nfrom static_frame.core.container import _UFUNC_UNARY_OPERATORS\nfrom static_frame.core.container import _UFUNC_BINARY_OPERATORS\nfrom static_frame.core.container import UFUNC_AXIS_SKIPNA\nfrom static_frame.core.util import isna_element\n\nfrom static_frame.test.property import strategies as sfst\nfrom static_frame.test.test_case import temp_file\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.test.test_case import skip_win\n\n\n\n\nclass TestUnit(TestCase):\n\n\n    @given(sfst.get_frame_or_frame_go())\n    def test_basic_attributes(self, f1: Frame) -> None:\n\n        self.assertEqual(len(f1.dtypes), f1.shape[1])\n        # self.assertEqual(f1.shape, f1.shape)\n        self.assertEqual(f1.ndim, 2)\n        # self.assertEqual(f1.unified, len(f1.mloc) <= 1)\n\n        if f1.shape[0] > 0 and f1.shape[1] > 0:\n            self.assertTrue(f1.size > 0)\n            self.assertTrue(f1.nbytes > 0)\n        else:\n            self.assertTrue(f1.size == 0)\n            self.assertTrue(f1.nbytes == 0)\n\n\n    @given(sfst.get_frame_or_frame_go(dtype_group=sfst.DTGroup.NUMERIC))\n    def test_unary_operators_numeric(self, f1: Frame) -> None:\n        for op in _UFUNC_UNARY_OPERATORS:\n            if op == '__invert__': # invalid on non Boolean\n                continue\n            func = getattr(operator, op)\n            values = f1.values\n            # must coerce all blocks to same type to compare to what NP does\n            a = func(f1.astype(values.dtype)).values\n            b = func(values)\n            self.assertAlmostEqualArray(a, b)\n\n\n    @given(sfst.get_frame_or_frame_go(dtype_group=sfst.DTGroup.BOOL))\n    def test_unary_operators_boolean(self, f1: Frame) -> None:\n        for op in _UFUNC_UNARY_OPERATORS:\n            if op != '__invert__': # valid on Boolean\n                continue\n            func = getattr(operator, op)\n            a = func(f1).values\n            b = func(f1.values)\n            self.assertAlmostEqualArray(a, b)\n\n\n    @given(sfst.get_frame_or_frame_go(dtype_group=sfst.DTGroup.NUMERIC))\n    def test_binary_operators_numeric(self, f1: Frame) -> None:\n        for op in _UFUNC_BINARY_OPERATORS:\n            if op in {\n                    '__matmul__',\n                    '__pow__',\n                    '__lshift__',\n                    '__rshift__',\n                    '__and__',\n                    '__xor__',\n                    '__or__',\n                    '__mod__',\n                    }:\n                continue\n            func = getattr(operator, op)\n            values = f1.values\n            # must coerce all blocks to same type to compare to what NP does\n            f2 = f1.astype(values.dtype)\n            a = func(f2, f2).values\n            b = func(values, values)\n            self.assertAlmostEqualArray(a, b)\n\n\n    @given(sfst.get_frame_or_frame_go(dtype_group=sfst.DTGroup.BOOL))\n    def test_binary_operators_boolean(self, f1: Frame) -> None:\n        for op in _UFUNC_BINARY_OPERATORS:\n            if op not in {\n                    '__and__',\n                    '__xor__',\n                    '__or__',\n                    }:\n                continue\n            func = getattr(operator, op)\n            a = func(f1, f1).values\n            values = f1.values\n            b = func(values, values)\n            self.assertAlmostEqualArray(a, b)\n\n    @given(sfst.get_frame_or_frame_go(\n            dtype_group=sfst.DTGroup.NUMERIC,\n            index_dtype_group=sfst.DTGroup.STRING,\n            min_rows=3,\n            max_rows=3,\n            min_columns=3,\n            max_columns=3)\n            )\n    def test_binary_operators_matmul(self,\n            f1: Frame,\n            ) -> None:\n\n        f2 = f1.relabel(columns=f1.index)\n        f3 = f2 @ f1\n        self.assertAlmostEqualArray(f3.values, f2.values @ f1.values)\n\n    # from hypothesis import reproduce_failure\n    # NOTE: was able to improve many of these, but continued to get compliated type cases, and complications\n    @given(sfst.get_frame_or_frame_go(\n            dtype_group=sfst.DTGroup.NUMERIC_REAL,\n            min_rows=1,\n            min_columns=1))\n    def test_ufunc_axis(self, f1: Frame) -> None:\n\n        for attr, attrs in UFUNC_AXIS_SKIPNA.items():\n\n            if attr in ('std', 'var'):\n                continue\n\n            for axis in (0, 1):\n                values = f1.values\n                # must coerce all blocks to same type to compare to what NP does\n                # f2 = f1.astype(values.dtype)\n\n                a = getattr(f1, attr)(axis=axis).values # call the method\n                b = attrs.ufunc_skipna(values, axis=axis)\n\n    #             if a.dtype != b.dtype:\n    #                 continue\n    #             try:\n    #                 self.assertAlmostEqualArray(a, b)\n    #             except:\n    #                 import ipdb; ipdb.set_trace()\n    #                 raise\n\n    @given(sfst.get_frame())\n    def test_frame_isin(self, f1: Frame) -> None:\n        value = f1.iloc[0, 0]\n        if (not isna_element(value) and\n                not isinstance(value, np.datetime64) and\n                not isinstance(value, np.timedelta64)):\n            self.assertTrue(f1.isin((value,)).iloc[0, 0])\n\n\n\n    # # TODO: intger tests with pow, mod\n\n    #---------------------------------------------------------------------------\n\n    @given(sfst.get_frame_go(), sfst.get_label())\n    def test_frame_go_setitem(self, f1: Frame, label: tp.Hashable) -> None:\n\n        shape = f1.shape\n        f1['foo'] = label # type: ignore\n        self.assertEqual(shape[1] + 1, f1.shape[1])\n\n\n    @given(sfst.get_arrays_2d_aligned_rows(min_size=2, max_size=2))\n    def test_frame_go_extend(self, arrays: tp.Sequence[np.ndarray]) -> None:\n        f1 = FrameGO(arrays[0], columns=self.get_letters(arrays[0].shape[1]))\n        shape = f1.shape\n        f2 = Frame(arrays[1])\n        f1.extend(f2)\n        self.assertEqual(f1.shape[1], shape[1] + f2.shape[1])\n\n\n    @given(sfst.get_arrays_2d_aligned_rows(min_size=3))\n    def test_frame_go_extend_items(self, arrays: tp.Sequence[np.ndarray]) -> None:\n        frame_array = arrays[0]\n        # just take first columm form 2d arrays\n        series_arrays = [a[:, 0] for a in arrays[1:]]\n\n        f1 = FrameGO(frame_array)\n        shape = f1.shape\n\n        letters = self.get_letters(len(series_arrays))\n\n        def items() -> tp.Iterator[tp.Tuple[tp.Hashable, Series]]:\n            for idx, label in enumerate(letters):\n                s = Series(series_arrays[idx], index=f1.index)\n                yield label, s\n\n        f1.extend_items(items())\n\n        self.assertEqual(f1.shape[1], shape[1] + len(series_arrays))\n\n\n    #---------------------------------------------------------------------------\n    # exporters\n\n    @given(sfst.get_frame_or_frame_go())\n    def test_frame_to_pairs(self, f1: Frame) -> None:\n        for i in range(0, 1):\n            post = f1.to_pairs(i)\n            if i == 1:\n                self.assertEqual(len(post), f1.shape[1]) # type: ignore\n            else:\n                self.assertEqual(len(post[0][1]), f1.shape[0]) # type: ignore\n            self.assertTrue(isinstance(post, tuple))\n\n\n    @given(sfst.get_frame_or_frame_go(\n            dtype_group=sfst.DTGroup.BASIC,\n            index_dtype_group=sfst.DTGroup.BASIC,\n            ))\n    def test_frame_to_pandas(self, f1: Frame) -> None:\n        post = f1.to_pandas()\n        self.assertTrue(post.shape == f1.shape)\n        if not f1.isna().any().any(): # type: ignore\n            self.assertTrue((post.values == f1.values).all())\n\n\n    @given(sfst.get_frame_or_frame_go(\n            dtype_group=sfst.DTGroup.BASIC,\n            index_dtype_group=sfst.DTGroup.BASIC,\n            ))\n    def test_frame_to_parquet(self, f1: Frame) -> None:\n        import pyarrow\n        with temp_file('.parquet') as fp:\n            try:\n                f1.to_parquet(fp)\n                self.assertTrue(os.stat(fp).st_size > 0)\n            except pyarrow.lib.ArrowNotImplementedError:\n                # could be Byte-swapped arrays not supported\n                pass\n\n    @given(sfst.get_frame_or_frame_go(\n            dtype_group=sfst.DTGroup.BASIC,\n            index_dtype_group=sfst.DTGroup.BASIC,\n            ))\n    def test_frame_to_xarray(self, f1: Frame) -> None:\n        xa = f1.to_xarray()\n        self.assertTrue(tuple(xa.keys()) == tuple(f1.columns))\n\n\n    @given(sfst.get_frame(\n            dtype_group=sfst.DTGroup.BASIC,\n            index_dtype_group=sfst.DTGroup.BASIC,\n            ))\n    def test_frame_to_frame_go(self, f1: Frame) -> None:\n        f2 = f1.to_frame_go()\n        f2['__new__'] = 10\n        self.assertTrue(len(f2.columns) == len(f1.columns) + 1)\n\n    @skip_win  # type: ignore # get UnicodeEncodeError: 'charmap' codec can't encode character '\\u0162' in position 0: character maps to <undefined>\n    @given(sfst.get_frame_or_frame_go(\n            dtype_group=sfst.DTGroup.BASIC,\n            ))\n    def test_frame_to_csv(self, f1: Frame) -> None:\n        with temp_file('.csv') as fp:\n            f1.to_csv(fp)\n            self.assertTrue(os.stat(fp).st_size > 0)\n\n            # not yet validating result, as edge cases with unusual unicode and non-unique indices are a problem\n            # f2 = Frame.from_csv(fp,\n            #         index_depth=f1.index.depth,\n            #         columns_depth=f1.columns.depth)\n\n\n    @skip_win  # type: ignore # UnicodeEncodeError\n    @given(sfst.get_frame_or_frame_go(\n            dtype_group=sfst.DTGroup.BASIC,\n            ))\n    def test_frame_to_tsv(self, f1: Frame) -> None:\n        with temp_file('.txt') as fp:\n            f1.to_tsv(fp)\n            self.assertTrue(os.stat(fp).st_size > 0)\n\n\n    @given(sfst.get_frame_or_frame_go(\n            dtype_group=sfst.DTGroup.BASIC,\n            ))\n    def test_frame_to_xlsx(self, f1: Frame) -> None:\n        with temp_file('.xlsx') as fp:\n            f1.to_xlsx(fp)\n            self.assertTrue(os.stat(fp).st_size > 0)\n\n    @given(sfst.get_frame_or_frame_go(\n            dtype_group=sfst.DTGroup.BASIC,\n            ))\n    def test_frame_to_sqlite(self, f1: Frame) -> None:\n        with temp_file('.sqlite') as fp:\n\n            try:\n                f1.to_sqlite(fp)\n                self.assertTrue(os.stat(fp).st_size > 0)\n            except (sqlite3.IntegrityError, sqlite3.OperationalError, OverflowError):\n                # some indices, after translation, are not unique\n                # SQLite is no case sensitive, and does not support unicide\n                # OverflowError: Python int too large to convert to SQLite INTEGER\n                pass\n\n\n    @given(sfst.get_frame_or_frame_go(\n            dtype_group=sfst.DTGroup.BASIC,\n            columns_dtype_group=sfst.DTGroup.STRING,\n            index_dtype_group=sfst.DTGroup.STRING\n            ))\n    def test_frame_to_hdf5(self, f1: Frame) -> None:\n        f1 = f1.rename('f1')\n        with temp_file('.hdf5') as fp:\n\n            try:\n                f1.to_hdf5(fp)\n                self.assertTrue(os.stat(fp).st_size > 0)\n            except ValueError:\n                # will happen for empty strings and unicde that cannot be handled by HDF5\n                pass\n\n\n    @given(sfst.get_frame_or_frame_go())\n    def test_frame_to_html(self, f1: Frame) -> None:\n        post = f1.to_html()\n        self.assertTrue(len(post) > 0)\n\n    @skip_win  # type: ignore # UnicodeEncodeError\n    @given(sfst.get_frame_or_frame_go())\n    def test_frame_to_html_datatables(self, f1: Frame) -> None:\n        post = f1.to_html_datatables(show=False)\n        self.assertTrue(len(post) > 0)\n\n    @given(sfst.get_frame_or_frame_go())\n    def test_frame_to_rst(self, f1: Frame) -> None:\n        post = f1.to_rst()\n        self.assertTrue(len(post) > 0)\n\n    @given(sfst.get_frame_or_frame_go())\n    def test_frame_to_markdown(self, f1: Frame) -> None:\n        post = f1.to_markdown()\n        self.assertTrue(len(post) > 0)\n\n    @given(sfst.get_frame_or_frame_go())\n    def test_frame_to_latex(self, f1: Frame) -> None:\n        post = f1.to_latex()\n        self.assertTrue(len(post) > 0)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/property/test_index.py,1,"b""\nimport typing as tp\nimport unittest\n\nimport numpy as np\n\n# from hypothesis import strategies as st\nfrom hypothesis import given\n# from hypothesis import reproduce_failure  # type: ignore\n\nfrom static_frame.test.property.strategies import get_labels\nfrom static_frame.test.property.strategies import get_index_any\n\nfrom static_frame.test.test_case import TestCase\n\nfrom static_frame import Index\nfrom static_frame import IndexGO\n# from static_frame import Series\n# from static_frame import Frame\n# from static_frame import FrameGO\n# from static_frame import TypeBlocks\n# from static_frame import Display\n\n\n\nclass TestUnit(TestCase):\n\n\n    @given(get_labels())\n    def test_index_values_len(self, values: tp.Sequence[tp.Hashable]) -> None:\n\n        def property_values(cls: tp.Type[Index], values: tp.Sequence[tp.Hashable]) -> None:\n            #Property: that the length of the index is the length of the (unique) values.\n            index = cls(values)\n            self.assertEqual(len(index), len(values))\n            self.assertEqual(len(index.values), len(values))\n\n        property_values(Index, values)\n        property_values(IndexGO, values)\n\n    @given(get_labels())\n    def test_index_values_list(self, values: tp.Sequence[tp.Hashable]) -> None:\n\n        def property_values(cls: tp.Type[Index], values: tp.Iterable[tp.Hashable]) -> None:\n            index = cls(values)\n            # must cast both sides to the dtype, as some int to float conversions result in different floats\n            self.assertAlmostEqualValues(index.values, np.array(values, dtype=index.values.dtype))\n\n        property_values(Index, values)\n        property_values(IndexGO, values)\n\n\n    @given(get_labels())\n    def test_index_loc_to_iloc_element(self, values: tp.Sequence[tp.Hashable]) -> None:\n\n        def property_loc_to_iloc_element(cls: tp.Type[Index], values: tp.Iterable[tp.Hashable]) -> None:\n            index = cls(values)\n            for i, v in enumerate(values):\n                self.assertEqual(index.loc_to_iloc(v), i)\n\n        property_loc_to_iloc_element(Index, values)\n        property_loc_to_iloc_element(IndexGO, values)\n\n    @given(get_labels(min_size=1))\n    def test_index_loc_to_iloc_slice(self, values: tp.Sequence[tp.Hashable]) -> None:\n\n        def property_loc_to_iloc_slice(cls: tp.Type[Index], values: tp.Iterable[tp.Hashable]) -> None:\n            # Property: that the key translates to the appropriate slice.\n            index = cls(values)\n            for i, v in enumerate(values):\n                # insure that we get teh same slice going through loc that we would get by direct iloc\n                if v is None:\n                    self.assertEqual(index.loc_to_iloc(slice(v, None)), slice(None))\n                else:\n                    self.assertEqual(index.loc_to_iloc(slice(v, None)), slice(i, None))\n\n        property_loc_to_iloc_slice(Index, values)\n        property_loc_to_iloc_slice(IndexGO, values)\n\n\n\n    @given(get_labels(min_size=2))\n    def test_index_go_append(self, values: tp.Sequence[tp.Hashable]) -> None:\n\n        index = IndexGO(values[:-1])\n        length_start = len(index)\n        index.append(values[-1])\n        length_end = len(index)\n        self.assertEqual(length_start + 1, length_end)\n\n\n    @given(get_labels(min_size=1))\n    def test_index_isin(self, labels: tp.Sequence[tp.Hashable]) -> None:\n        index = Index(labels)\n        self.assertTrue(index.isin((labels[0],))[0])\n\n\n    #---------------------------------------------------------------------------\n    @given(get_index_any())\n    def test_index_display(self, index: Index) -> None:\n\n        d1 = index.display()\n        self.assertTrue(len(d1) > 0)\n\n        d2 = index.display_tall()\n        self.assertTrue(len(d2) > 0)\n\n        d3 = index.display_wide()\n        self.assertTrue(len(d3) > 0)\n\n    @given(get_index_any())\n    def test_index_to_series(self, index: Index) -> None:\n        s1 = index.to_series()\n        self.assertEqual(len(s1), len(index))\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/property/test_index_hierarchy.py,0,"b""\n# import typing as tp\nimport unittest\n\n# import numpy as np\n\nfrom hypothesis import given\nfrom static_frame.test.test_case import TestCase\n\nfrom static_frame.test.property.strategies import get_index_hierarchy_any\n\n\nfrom static_frame import IndexHierarchy\n# from static_frame import IndexHierarchyGO\n\n\n\n\nclass TestUnit(TestCase):\n\n\n\n    #---------------------------------------------------------------------------\n    @given(get_index_hierarchy_any())\n    def test_index_display(self, ih: IndexHierarchy) -> None:\n\n        d1 = ih.display()\n        self.assertTrue(len(d1) > 0)\n\n        d2 = ih.display_tall()\n        self.assertTrue(len(d2) > 0)\n\n        d3 = ih.display_wide()\n        self.assertTrue(len(d3) > 0)\n\n    @given(get_index_hierarchy_any())\n    def test_index_to_frame(self, ih: IndexHierarchy) -> None:\n        f1 = ih.to_frame()\n        self.assertEqual(f1.shape, ih.shape)\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/property/test_series.py,0,"b""# import typing as tp\nimport unittest\nimport operator\n\n# import numpy as np\n\n# from hypothesis import strategies as st\nfrom hypothesis import given\n\nfrom static_frame.core.container import _UFUNC_UNARY_OPERATORS\nfrom static_frame.core.container import _UFUNC_BINARY_OPERATORS\nfrom static_frame.core.container import UFUNC_AXIS_SKIPNA\n\nfrom static_frame.test.property import strategies as sfst\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.core.util import isna_element\n\nfrom static_frame import Series\n\n\n\nclass TestUnit(TestCase):\n\n\n    @given(sfst.get_series())\n    def test_basic_attributes(self, s1: Series) -> None:\n\n        self.assertEqual(s1.dtype, s1.values.dtype)\n        # self.assertEqual(s1.shape, s1.shape)\n        self.assertEqual(s1.ndim, 1)\n\n        if s1.shape[0] > 0:\n            self.assertTrue(s1.size > 0)\n            self.assertTrue(s1.nbytes > 0)\n\n\n    @given(sfst.get_series(dtype_group=sfst.DTGroup.NUMERIC, min_size=1))\n    def test_unary_operators_numeric(self, s1: Series) -> None:\n        for op in _UFUNC_UNARY_OPERATORS:\n            if op == '__invert__': # invalid on non Boolean\n                continue\n            func = getattr(operator, op)\n            a = func(s1).values\n            b = func(s1.values)\n            self.assertAlmostEqualArray(a, b)\n\n\n    @given(sfst.get_series(dtype_group=sfst.DTGroup.BOOL, min_size=1))\n    def test_unary_operators_boolean(self, s1: Series) -> None:\n        for op in _UFUNC_UNARY_OPERATORS:\n            if op != '__invert__': # valid on Boolean\n                continue\n            func = getattr(operator, op)\n            a = func(s1).values\n            b = func(s1.values)\n            self.assertAlmostEqualArray(a, b)\n\n\n\n    @given(sfst.get_series(dtype_group=sfst.DTGroup.NUMERIC))\n    def test_binary_operators_numeric(self, s1: Series) -> None:\n        for op in _UFUNC_BINARY_OPERATORS:\n            if op in {\n                    '__matmul__',\n                    '__pow__',\n                    '__lshift__',\n                    '__rshift__',\n                    '__and__',\n                    '__xor__',\n                    '__or__',\n                    '__mod__',\n                    }:\n                continue\n            func = getattr(operator, op)\n            values = s1.values\n            a = func(s1, s1).values\n            b = func(values, values)\n            self.assertAlmostEqualArray(a, b)\n\n    @given(sfst.get_series(dtype_group=sfst.DTGroup.BOOL))\n    def test_binary_operators_boolean(self, s1: Series) -> None:\n        for op in _UFUNC_BINARY_OPERATORS:\n            if op not in {\n                    '__and__',\n                    '__xor__',\n                    '__or__',\n                    }:\n                continue\n            func = getattr(operator, op)\n            values = s1.values\n            a = func(s1, s1).values\n            b = func(values, values)\n            self.assertAlmostEqualArray(a, b)\n\n\n    @given(sfst.get_series(dtype_group=sfst.DTGroup.NUMERIC, min_size=1))\n    def test_ufunc_axis(self, s1: Series) -> None:\n        for attr, attrs in UFUNC_AXIS_SKIPNA.items():\n            a = getattr(s1, attr)()\n            func = attrs.ufunc_skipna\n            b = func(s1.values)\n            self.assertEqualWithNaN(a, b)\n\n    @given(sfst.get_series(min_size=1))\n    def test_isin(self, s1: Series) -> None:\n\n        value = s1.iloc[0]\n        if not isna_element(value):\n            self.assertTrue(s1.isin((value,)).iloc[0])\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/property/test_strategies.py,8,"b""\nimport typing as tp\nimport unittest\n\nimport numpy as np\n\n# from hypothesis import strategies as st\nfrom hypothesis import given\nfrom hypothesis import settings as hypo_settings\n\nfrom static_frame.test.property import strategies as sfst\n\nfrom static_frame.test.test_case import TestCase\n\nfrom static_frame import TypeBlocks\nfrom static_frame import Index\n\n# from static_frame import IndexDate\n# from static_frame import IndexYear\n# from static_frame import IndexYearMonth\n# from static_frame import IndexSecond\n# from static_frame import IndexMillisecond\n\nfrom static_frame import IndexHierarchy\n# from static_frame import IndexHierarchyGO\n# from static_frame import IndexGO\nfrom static_frame import Series\nfrom static_frame import Frame\n# from static_frame import FrameGO\n\n\nclass TestUnit(TestCase):\n\n\n    @given(sfst.get_labels())\n    def test_get_labels(self, values: tp.Iterable[tp.Hashable]) -> None:\n        for value in values:\n            self.assertTrue(isinstance(hash(value), int))\n\n    @given(sfst.get_dtypes())\n    def test_get_dtypes(self, dtypes: tp.Iterable[np.dtype]) -> None:\n        for dt in dtypes:\n            self.assertTrue(isinstance(dt, np.dtype))\n\n    @given(sfst.get_spacing(10))\n    def test_get_spacing_10(self, spacing: tp.Iterable[int]) -> None:\n        self.assertEqual(sum(spacing), 10)\n\n    @hypo_settings(max_examples=10)  # type: ignore\n    @given(sfst.get_shape_1d2d())\n    def test_get_shape_1d2d(self, shape: tp.Tuple[int, ...]) -> None:\n        self.assertTrue(isinstance(shape, tuple))\n        self.assertTrue(len(shape) in (1, 2))\n\n    @hypo_settings(max_examples=10)  # type: ignore\n    @given(sfst.get_array_1d2d())\n    def test_get_array_1d2d(self, array: np.ndarray) -> None:\n        self.assertTrue(isinstance(array, np.ndarray))\n        self.assertTrue(array.ndim in (1, 2))\n\n    @hypo_settings(max_examples=10)  # type: ignore\n    @given(sfst.get_arrays_2d_aligned_columns(min_size=2))\n    def test_get_arrays_2s_aligned_columns(self, arrays: tp.Iterable[np.ndarray]) -> None:\n        array_iter = iter(arrays)\n        a1 = next(array_iter)\n        match = a1.shape[1]\n        for array in array_iter:\n            self.assertEqual(array.shape[1], match)\n\n    @given(sfst.get_arrays_2d_aligned_rows(min_size=2))\n    def test_get_arrays_2s_aligned_rows(self, arrays: tp.Iterable[np.ndarray]) -> None:\n        array_iter = iter(arrays)\n        a1 = next(array_iter)\n        match = a1.shape[0]\n        for array in array_iter:\n            self.assertEqual(array.shape[0], match)\n\n    @hypo_settings(max_examples=10)  # type: ignore\n    @given(sfst.get_blocks())\n    def test_get_blocks(self, blocks: tp.Tuple[np.ndarray]) -> None:\n        self.assertTrue(isinstance(blocks, tuple))\n        for b in blocks:\n            self.assertTrue(isinstance(b, np.ndarray))\n            self.assertTrue(b.ndim in (1, 2))\n\n    @hypo_settings(max_examples=10)  # type: ignore\n    @given(sfst.get_type_blocks())\n    def test_get_type_blocks(self, tb: TypeBlocks) -> None:\n        self.assertTrue(isinstance(tb, TypeBlocks))\n        rows, cols = tb.shape\n        col_count = 0\n        for b in tb._blocks:\n            if b.ndim == 1:\n                self.assertEqual(len(b), rows)\n                col_count += 1\n            else:\n                self.assertEqual(b.ndim, 2)\n                self.assertEqual(b.shape[0], rows)\n                col_count += b.shape[1]\n\n        self.assertEqual(col_count, cols)\n\n    @hypo_settings(max_examples=10) # type: ignore\n    @given(sfst.get_index())\n    def test_get_index(self, idx: Index) -> None:\n        self.assertTrue(isinstance(idx, Index))\n        self.assertEqual(len(idx), len(idx.values))\n\n    @hypo_settings(max_examples=10)  # type: ignore\n    @given(sfst.get_index_hierarchy())\n    def test_get_index_hierarchy(self, idx: IndexHierarchy) -> None:\n        self.assertTrue(isinstance(idx, IndexHierarchy))\n        self.assertTrue(idx.depth > 1)\n        self.assertEqual(len(idx), len(idx.values))\n\n    @hypo_settings(max_examples=10)  # type: ignore\n    @given(sfst.get_series())\n    def test_get_series(self, series: Series) -> None:\n        self.assertTrue(isinstance(series, Series))\n        self.assertEqual(len(series), len(series.values))\n\n    @hypo_settings(max_examples=10)  # type: ignore\n    @given(sfst.get_frame())\n    def test_get_frame(self, frame: Frame) -> None:\n        self.assertTrue(isinstance(frame, Frame))\n        self.assertEqual(frame.shape, frame.values.shape)\n\n    @hypo_settings(max_examples=10)  # type: ignore\n    @given(sfst.get_frame(index_cls=IndexHierarchy, columns_cls=IndexHierarchy))  # type: ignore\n    def test_get_frame_hierarchy(self, frame: Frame) -> None:\n        self.assertTrue(isinstance(frame, Frame))\n        self.assertTrue(frame.index.depth > 1)\n        self.assertTrue(frame.columns.depth > 1)\n        self.assertEqual(frame.shape, frame.values.shape)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/property/test_type_blocks.py,3,"b""\nimport typing as tp\nimport unittest\n\nimport numpy as np\n\nfrom hypothesis import strategies as st\nfrom hypothesis import given\n\nfrom static_frame.test.property import strategies as sfst\n\nfrom static_frame.test.test_case import TestCase\n\nfrom static_frame import TypeBlocks\n# from static_frame import Index\n# from static_frame import IndexHierarchy\n# from static_frame import Series\n# from static_frame import Frame\n\n\nclass TestUnit(TestCase):\n\n\n    @given(st.lists(sfst.get_shape_2d(), min_size=1), sfst.get_labels(min_size=1))\n    def test_from_element_items(self,\n            shapes: tp.List[tp.Tuple[int, int]],\n            labels: tp.Sequence[tp.Hashable]\n            ) -> None:\n\n        # use shapes to get coordinates, where the max shape + 1 is the final shape\n        shape = tuple(np.array(shapes).max(axis=0) + 1)\n\n        def values() -> tp.Iterator[tp.Tuple[tp.Tuple[int, int], tp.Hashable]]:\n            for idx, coord in enumerate(shapes):\n                yield coord, labels[idx % len(labels)]\n\n        post = TypeBlocks.from_element_items(values(), shape=shape, dtype=object)\n        self.assertEqual(post.shape, shape)\n\n\n    @given(st.integers(max_value=sfst.MAX_COLUMNS))\n    def test_from_zero_size_shape(self, value: int) -> None:\n\n        for shape in ((0, value), (value, 0)):\n            post = TypeBlocks.from_zero_size_shape(shape=shape)\n            self.assertEqual(post.shape, shape)\n\n\n    @given(sfst.get_type_blocks())\n    def test_basic_attributes(self, tb: TypeBlocks) -> None:\n        self.assertEqual(len(tb.dtypes), tb.shape[1])\n        self.assertEqual(len(tb.shapes), len(tb.mloc))\n        self.assertEqual(tb.copy().shape, tb.shape)\n        self.assertEqual(tb.ndim, 2)\n        self.assertEqual(tb.unified, len(tb.mloc) <= 1)\n\n        if tb.shape[0] > 0 and tb.shape[1] > 0:\n            self.assertTrue(tb.size > 0)\n            self.assertTrue(tb.nbytes > 0)\n        else:\n            self.assertTrue(tb.size == 0)\n            self.assertTrue(tb.nbytes == 0)\n\n\n\n    @given(sfst.get_type_blocks())\n    def test_values(self, tb: TypeBlocks) -> None:\n        values = tb.values\n        self.assertEqual(values.shape, tb.shape)\n        self.assertEqual(values.dtype, tb._row_dtype)\n\n\n    @given(sfst.get_type_blocks())\n    def test_axis_values(self, tb: TypeBlocks) -> None:\n        # this test found a flaw in axis_values when dealing with axis 1 and unified,  1D type blocks\n        for axis in (0, 1):\n            for reverse in (True, False):\n                post = tuple(tb.axis_values(axis=axis, reverse=reverse))\n                for idx, array in enumerate(post):\n                    self.assertTrue(len(array) == tb.shape[axis])\n                    if axis == 0 and not reverse: # colums\n                        self.assertTrue(array.dtype == tb.dtypes[idx])\n                    elif axis == 0 and reverse: # colums\n                        self.assertTrue(array.dtype == tb.dtypes[tb.shape[1] - 1 - idx])\n                    else:\n                        # NOTE: only checking kinde because found cases where byte-order deviates\n                        self.assertTrue(array.dtype.kind == tb._row_dtype.kind)\n\n\n    @given(sfst.get_type_blocks())\n    def test_element_items(self, tb: TypeBlocks) -> None:\n        # NOTE: this found a flaw in _extract_iloc where we tried to optimize selection with a unified array\n        count = 0\n        for k, v in tb.element_items():\n            count += 1\n            v_extract = tb.iloc[k]\n            self.assertEqualWithNaN(v, v_extract)\n        self.assertEqual(count, tb.size)\n\n    @given(sfst.get_type_blocks())\n    def test_reblock_signature(self, tb: TypeBlocks) -> None:\n        post = tuple(tb._reblock_signature())\n        unique_dtypes = np.unique(tb.dtypes)\n        # the reblock signature must be have at least as many entries as types\n        self.assertTrue(len(post) >= len(unique_dtypes))\n        # sum of column widths is qual to columns in shape\n        self.assertTrue(sum(p[1] for p in post), tb.shape[1])\n\n\n    @given(sfst.get_type_blocks(), sfst.get_type_blocks())\n    def test_block_compatible(self, tb1: TypeBlocks, tb2: TypeBlocks) -> None:\n\n        for axis in (None, 0, 1):\n            post1 = tb1.block_compatible(tb2, axis)\n            post2 = tb2.block_compatible(tb1, axis)\n            # either direction gets the same result\n            self.assertTrue(post1 == post2)\n            # if the shapes are different, they cannot be block compatible\n            if axis is None and tb1.shape != tb2.shape:\n                self.assertFalse(post1)\n\n\n    @given(sfst.get_type_blocks(), sfst.get_type_blocks())\n    def test_reblock_compatible(self, tb1: TypeBlocks, tb2: TypeBlocks) -> None:\n\n        post1 = tb1.reblock_compatible(tb2)\n        post2 = tb2.reblock_compatible(tb1)\n        # either direction gets the same result\n        self.assertTrue(post1 == post2)\n        # if the shapes are different, they cannot be block compatible\n        if tb1.shape[1] != tb2.shape[1]:\n            self.assertFalse(post1)\n\n    @unittest.skip('pending')\n    def test_concatenate_blocks(self) -> None:\n        pass\n\n    @given(sfst.get_type_blocks())\n    def test_consolidate_blocks(self, tb: TypeBlocks) -> None:\n\n        tb_post = TypeBlocks.from_blocks(tb.consolidate_blocks(tb._blocks))\n        self.assertEqual(tb_post.shape, tb.shape)\n        self.assertTrue((tb_post.dtypes == tb.dtypes).all())\n\n    @given(sfst.get_type_blocks())\n    def test_reblock(self, tb: TypeBlocks) -> None:\n        tb_post = TypeBlocks.from_blocks(tb._reblock())\n        self.assertEqual(tb_post.shape, tb.shape)\n        self.assertTrue((tb_post.dtypes == tb.dtypes).all())\n\n    @given(sfst.get_type_blocks())\n    def test_consolidate(self, tb: TypeBlocks) -> None:\n        tb_post = tb.consolidate()\n        self.assertEqual(tb_post.shape, tb.shape)\n        self.assertTrue((tb_post.dtypes == tb.dtypes).all())\n\n    @unittest.skip('pending')\n    def test_resize_blocks(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_group(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_ufunc_axis_skipna(self) -> None:\n        pass\n\n    @given(sfst.get_type_blocks())\n    def test_display(self, tb: TypeBlocks) -> None:\n        post = tb.display()\n        self.assertTrue(len(post) > 0)\n\n    @unittest.skip('pending')\n    def test_cols_to_slice(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_indices_to_contiguous_pairs(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_all_block_slices(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_key_to_block_slices(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_mask_blocks(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_astype_blocks(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_shift_blocks(self) -> None:\n        pass\n\n    @given(sfst.get_type_blocks())\n    def test_assign_blocks_from_keys(self, tb1: TypeBlocks) -> None:\n\n        # assigning a single value from a list of column keys\n        for i in range(tb1.shape[1]):\n            tb2 = TypeBlocks.from_blocks(tb1._assign_blocks_from_keys(\n                    column_key=[i], value=300))\n            self.assertTrue(tb1.shape == tb2.shape)\n            # no more than one type should be changed\n            self.assertTrue((tb1.dtypes != tb2.dtypes).sum() <= 1)\n\n        # assigning a single value from a list of row keys\n        for i in range(tb1.shape[0]):\n            tb3 = TypeBlocks.from_blocks(tb1._assign_blocks_from_keys(\n                    row_key=[i], value=300))\n            self.assertTrue(tb1.shape == tb3.shape)\n            self.assertTrue(tb3.iloc[i, 0] == 300)\n\n        # column slices to the end\n        for i in range(tb1.shape[1]):\n            tb4 = TypeBlocks.from_blocks(tb1._assign_blocks_from_keys(\n                    column_key=slice(i, None), value=300))\n            self.assertTrue(tb1.shape == tb4.shape)\n            # we have as many or more blocks\n            self.assertTrue(len(tb4.shapes) >= len(tb1.shapes))\n\n\n    @unittest.skip('pending')\n    def test_assign_blocks_from_boolean_blocks(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_slice_blocks(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_extract_array(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_extract(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_extract_iloc(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_extract_iloc_mask(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_extract_iloc_assign(self) -> None:\n        pass\n\n    @given(sfst.get_type_blocks(min_rows=1, min_columns=1))\n    def test_drop(self, tb: TypeBlocks) -> None:\n\n        for row in range(tb.shape[0]):\n            tb_post1 = tb.drop(row)\n            self.assertTrue(tb_post1.shape[0] == tb.shape[0] - 1)\n\n        if tb.shape[0] > 2:\n            for start in range(1, tb.shape[0]):\n                tb_post2 = tb.drop(slice(start, None))\n                self.assertTrue(tb_post2.shape[0] == start)\n\n        for col in range(tb.shape[1]):\n            tb_post3 = tb.drop((None, col))\n            self.assertTrue(tb_post3.shape[1] == tb.shape[1] - 1)\n\n        if tb.shape[1] > 2:\n            for start in range(1, tb.shape[1]):\n                tb_post4 = tb.drop((None, slice(start, None)))\n                self.assertTrue(tb_post4.shape[1] == start)\n\n\n\n    @unittest.skip('pending')\n    def test_getitem(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_ufunc_unary_operator(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_block_shape_slices(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_ufunc_binary_operator(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_transpose(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_isna(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_notna(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_fillna_leading(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_fillna_trailing(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_fillna_forward(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_fillna_backward(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_dropna_to_keep_locations(self) -> None:\n        pass\n\n    @unittest.skip('pending')\n    def test_fillna(self) -> None:\n        pass\n\n\n    @given(sfst.get_type_blocks_aligned_array())\n    def test_append(self, tb_aligned_array: tp.Tuple[TypeBlocks, np.ndarray]) -> None:\n        tb, aa = tb_aligned_array\n        shape_original = tb.shape\n        tb.append(aa)\n        if aa.ndim == 1:\n            self.assertEqual(tb.shape[1], shape_original[1] + 1)\n        else:\n            self.assertEqual(tb.shape[1], shape_original[1] + aa.shape[1])\n\n    @given(sfst.get_type_blocks_aligned_type_blocks(min_size=2, max_size=2))\n    def test_extend(self, tbs: tp.Sequence[TypeBlocks]) -> None:\n        front = tbs[0]\n        back = tbs[1]\n        shape_original = front.shape\n        # extend with type blocks\n        front.extend(back)\n        self.assertEqual(front.shape,\n                (shape_original[0], shape_original[1] + back.shape[1]))\n\n        # extend with iterable of arrays\n        front.extend(back._blocks)\n        self.assertEqual(front.shape,\n                (shape_original[0], shape_original[1] + back.shape[1] * 2))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/property/test_util.py,45,"b""\nimport typing as tp\nimport datetime\nimport unittest\nimport fractions\nimport numpy as np\n\nfrom hypothesis import strategies as st\nfrom hypothesis import given\n\nfrom static_frame.core.util import DTYPE_NAN_KIND\nfrom static_frame.test.property.strategies import DTGroup\n\nfrom static_frame.test.property.strategies import get_shape_1d2d\nfrom static_frame.test.property.strategies import get_array_1d\nfrom static_frame.test.property.strategies import get_array_1d2d\nfrom static_frame.test.property.strategies import get_array_2d\nfrom static_frame.test.property.strategies import get_dtype_pairs\n\nfrom static_frame.test.property.strategies import get_dtype\nfrom static_frame.test.property.strategies import get_dtypes\n# from static_frame.test.property.strategies import get_label\nfrom static_frame.test.property.strategies import get_value\nfrom static_frame.test.property.strategies import get_labels\n# from static_frame.test.property.strategies import get_arrays_2d_aligned\nfrom static_frame.test.property.strategies import get_arrays_2d_aligned_columns\nfrom static_frame.test.property.strategies import get_arrays_2d_aligned_rows\n# from static_frame.test.property.strategies import get_blocks\n\nfrom static_frame.core.container import UFUNC_AXIS_SKIPNA\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.core import util\n\n\nclass TestUnit(TestCase):\n\n\n    @given(get_array_1d2d())\n    def test_mloc(self, array: np.ndarray) -> None:\n\n        x = util.mloc(array)\n        self.assertTrue(isinstance(x, int))\n\n\n    @given(get_array_1d2d())\n    def test_shape_filter(self, shape: np.ndarray) -> None:\n        self.assertTrue(len(util.shape_filter(shape)), 2)\n\n    @given(get_dtype_pairs())\n    def test_resolve_dtype(self, dtype_pair: tp.Tuple[np.dtype, np.dtype]) -> None:\n\n        x = util.resolve_dtype(*dtype_pair)\n        self.assertTrue(isinstance(x, np.dtype))\n\n    @given(get_dtypes(min_size=1))\n    def test_resolve_dtype_iter(self, dtypes: tp.Iterable[np.dtype]) -> None:\n\n        x = util.resolve_dtype_iter(dtypes)\n        self.assertTrue(isinstance(x, np.dtype))\n\n    @given(get_labels(min_size=1))\n    def test_resolve_type_iter(self, objects: tp.Iterable[object]) -> None:\n\n        known_types = set((\n                None,\n                type(None),\n                bool,\n                str,\n                object,\n                int,\n                float,\n                complex,\n                datetime.date,\n                datetime.datetime,\n                fractions.Fraction\n                ))\n        resolved, has_tuple, values_post = util.resolve_type_iter(objects)\n        self.assertTrue(resolved in known_types)\n\n\n\n    @given(get_arrays_2d_aligned_columns())\n    def test_concat_resolved_axis_0(self, arrays: tp.List[np.ndarray]) -> None:\n        array = util.concat_resolved(arrays, axis=0)\n        self.assertEqual(array.ndim, 2)\n        self.assertEqual(array.dtype, util.resolve_dtype_iter((x.dtype for x in arrays)))\n\n    @given(get_arrays_2d_aligned_rows())\n    def test_concat_resolved_axis_1(self, arrays: tp.List[np.ndarray]) -> None:\n        array = util.concat_resolved(arrays, axis=1)\n        self.assertEqual(array.ndim, 2)\n        self.assertEqual(array.dtype, util.resolve_dtype_iter((x.dtype for x in arrays)))\n\n    @given(get_dtype(), get_shape_1d2d(), get_value())\n    def test_full_or_fill(self,\n            dtype: np.dtype,\n            shape: tp.Union[tp.Tuple[int], tp.Tuple[int, int]],\n            value: object) -> None:\n        array = util.full_for_fill(dtype, shape, fill_value=value)\n        self.assertTrue(array.shape == shape)\n        if isinstance(value, (float, complex)) and np.isnan(value):\n            pass\n        else:\n            self.assertTrue(value in array)\n\n    @given(get_dtype())\n    def test_dtype_to_na(self, dtype: util.DtypeSpecifier) -> None:\n        post = util.dtype_to_na(dtype)\n        self.assertTrue(post in {0, False, None, '', np.nan, util.NAT})\n\n\n    @given(get_array_1d2d(dtype_group=DTGroup.NUMERIC))\n    def test_ufunc_axis_skipna(self, array: np.ndarray) -> None:\n\n        has_na = util.isna_array(array).any()\n\n        for nt in UFUNC_AXIS_SKIPNA.values():\n            ufunc = nt.ufunc\n            ufunc_skipna = nt.ufunc_skipna\n            # dtypes = nt.dtypes\n            # composable = nt.composable\n            # doc = nt.doc_header\n            # size_one_unity = nt.size_one_unity\n\n            with np.errstate(over='ignore', under='ignore', divide='ignore'):\n\n                post = util.ufunc_axis_skipna(array=array,\n                        skipna=True,\n                        axis=0,\n                        ufunc=ufunc,\n                        ufunc_skipna=ufunc_skipna\n                        )\n                if array.ndim == 2:\n                    self.assertTrue(post.ndim == 1)\n\n    @given(get_array_1d2d())\n    def test_ufunc_unique(self, array: np.ndarray) -> None:\n        post = util.ufunc_unique(array, axis=0)\n        self.assertTrue(len(post) <= array.shape[0])\n\n    @given(get_array_1d(min_size=1), st.integers())\n    def test_roll_1d(self, array: np.ndarray, shift: int) -> None:\n        post = util.roll_1d(array, shift)\n        self.assertEqual(len(post), len(array))\n        self.assertEqualWithNaN(array[-(shift % len(array))], post[0])\n\n    @given(get_array_2d(min_rows=1, min_columns=1), st.integers())\n    def test_roll_2d(self, array: np.ndarray, shift: int) -> None:\n        for axis in (0, 1):\n            post = util.roll_2d(array, shift=shift, axis=axis)\n            self.assertEqual(post.shape, array.shape)\n\n            start = -(shift % array.shape[axis])\n\n            if axis == 0:\n                a = array[start]\n                b = post[0]\n            else:\n                a = array[:, start]\n                b = post[:, 0]\n\n            self.assertAlmostEqualValues(a, b)\n\n\n\n    @given(get_array_1d(dtype_group=DTGroup.OBJECT))\n    def test_iterable_to_array_a(self, array: np.ndarray) -> None:\n        values = array.tolist()\n        post, _ = util.iterable_to_array_1d(values)\n        self.assertAlmostEqualValues(post, values)\n\n        # explicitly giving object dtype\n        post, _ = util.iterable_to_array_1d(values, dtype=util.DTYPE_OBJECT)\n        self.assertAlmostEqualValues(post, values)\n\n\n    @given(get_labels())\n    def test_iterable_to_array_b(self, labels: tp.Iterable[tp.Any]) -> None:\n        post, _ = util.iterable_to_array_1d(labels)\n        self.assertAlmostEqualValues(post, labels)\n        self.assertTrue(isinstance(post, np.ndarray))\n\n\n    @given(get_labels())\n    def test_iterable_to_array_nd(self, labels: tp.Iterable[tp.Any]) -> None:\n        post = util.iterable_to_array_nd(labels)\n        self.assertAlmostEqualValues(post, labels)\n        self.assertTrue(isinstance(post, np.ndarray))\n\n        if len(labels):\n            sample = post[0]\n            post = util.iterable_to_array_nd(sample)\n            self.assertTrue(isinstance(post, np.ndarray))\n\n    @given(st.slices(10))  #pylint: disable=E1120\n    def test_slice_to_ascending_slice(self, key: slice) -> None:\n\n        post_key = util.slice_to_ascending_slice(key, size=10)\n        self.assertEqual(\n            set(range(*key.indices(10))),\n            set(range(*post_key.indices(10)))\n            )\n\n# to_datetime64\n# to_timedelta64\n# key_to_datetime_key\n\n    @given(get_array_1d2d())\n    def test_array_to_groups_and_locations(self, array: np.ndarray) -> None:\n\n        groups, locations = util.array_to_groups_and_locations(array, 0)\n\n        if len(array) > 0:\n            self.assertTrue(len(groups) >= 1)\n\n        # always 1dm locations\n        self.assertTrue(locations.ndim == 1)\n        self.assertTrue(len(np.unique(locations)) == len(groups))\n\n\n    @given(get_array_1d2d())\n    def test_isna_array(self, array: np.ndarray) -> None:\n\n        post = util.isna_array(array)\n        self.assertTrue(post.dtype == bool)\n\n        values = np.ravel(array)\n        count_na = sum(util.isna_element(x) for x in values)\n\n        self.assertTrue(np.ravel(post).sum() == count_na)\n\n\n    @given(get_array_1d(dtype_group=DTGroup.BOOL))\n    def test_binary_transition(self, array: np.ndarray) -> None:\n        post = util.binary_transition(array)\n\n        # could be 32 via result of np.nonzero\n        self.assertTrue(post.dtype in (np.int32, np.int64))\n\n        # if no True in original array, result will be empty\n        if array.sum() == 0:\n            self.assertTrue(len(post) == 0)\n        # if all True, result is empty\n        elif array.sum() == len(array):\n            self.assertTrue(len(post) == 0)\n        else:\n            # the post selection shold always be indices that are false\n            self.assertTrue(array[post].sum() == 0)\n\n\n    @given(get_array_1d2d())\n    def test_array_to_duplicated(self, array: np.ndarray) -> None:\n        if array.ndim == 2:\n            for axis in (0, 1):\n                post = util.array_to_duplicated(array, axis=axis)\n                if axis == 0:\n                    unique_count = len(set(tuple(x) for x in array))\n                else:\n                    unique_count = len(set(\n                        tuple(array[:, i]) for i in range(array.shape[1]))\n                        )\n                if unique_count < array.shape[axis]:\n                    self.assertTrue(post.sum() > 0)\n        else:\n            post = util.array_to_duplicated(array)\n            # if not all value are unique, we must have some duplicated\n            if len(set(array)) < len(array):\n                self.assertTrue(post.sum() > 0)\n\n        self.assertTrue(post.dtype == bool)\n\n    @given(get_array_1d2d())\n    def test_array_shift(self, array: np.ndarray) -> None:\n\n        for shift in (-1, 1):\n            for wrap in (True, False):\n\n                tests = []\n                post1 = util.array_shift(\n                        array=array,\n                        shift=shift,\n                        axis=0,\n                        wrap=wrap)\n                tests.append(post1)\n\n                if array.ndim == 2:\n                    post2 = util.array_shift(\n                        array=array,\n                        shift=shift,\n                        axis=1,\n                        wrap=wrap)\n                    tests.append(post2)\n\n                for post in tests:\n                    self.assertTrue(array.shape == post.shape)\n\n                    # type is only always maintained if we are wrapping\n                    if wrap:\n                        self.assertTrue(array.dtype == post.dtype)\n\n\n    @given(st.lists(get_array_1d(), min_size=2, max_size=2))\n    def test_union1d(self, arrays: tp.Sequence[np.ndarray]) -> None:\n        post = util.union1d(\n                arrays[0],\n                arrays[1],\n                assume_unique=False)\n        self.assertTrue(post.ndim == 1)\n        # nan values in complex numbers make direct comparison tricky\n        self.assertTrue(len(post) == len(set(arrays[0]) | set(arrays[1])))\n\n        # complex results are tricky to compare after forming sets\n        if (post.dtype.kind not in ('O', 'M', 'm', 'c', 'f')\n                and not np.isnan(post).any()):\n            self.assertSetEqual(set(post), (set(arrays[0]) | set(arrays[1])))\n\n\n    @given(st.lists(get_array_1d(), min_size=2, max_size=2))\n    def test_intersect1d(self, arrays: tp.Sequence[np.ndarray]) -> None:\n        post = util.intersect1d(\n                arrays[0],\n                arrays[1],\n                assume_unique=False)\n        self.assertTrue(post.ndim == 1)\n        # nan values in complex numbers make direct comparison tricky\n        self.assertTrue(len(post) == len(set(arrays[0]) & set(arrays[1])))\n\n        if (post.dtype.kind not in ('O', 'M', 'm', 'c', 'f')\n                and not np.isnan(post).any()):\n            self.assertSetEqual(set(post), (set(arrays[0]) & set(arrays[1])))\n\n\n    @given(st.lists(get_array_1d(), min_size=2, max_size=2))\n    def test_setdiff1d(self, arrays: tp.Sequence[np.ndarray]) -> None:\n        post = util.setdiff1d(\n                arrays[0],\n                arrays[1],\n                assume_unique=False)\n        self.assertTrue(post.ndim == 1)\n\n        if post.dtype.kind in ('f', 'c', 'i', 'u'):\n            # Compare directly to numpy behavior for number values.\n            self.assertTrue(len(post) == len(np.setdiff1d(arrays[0], arrays[1], assume_unique=False)))\n        else:\n            # nan values in complex numbers make direct comparison tricky\n            self.assertTrue(len(post) == len(set(arrays[0]).difference(set(arrays[1]))))\n\n        if (post.dtype.kind not in ('O', 'M', 'm', 'c', 'f')\n                and not np.isnan(post).any()):\n            self.assertSetEqual(set(post), (set(arrays[0]).difference(set(arrays[1]))))\n\n\n    @given(get_arrays_2d_aligned_columns(min_size=2, max_size=2))\n    def test_union2d(self, arrays: tp.Sequence[np.ndarray]) -> None:\n        post = util.union2d(arrays[0], arrays[1], assume_unique=False)\n        if post.dtype == object:\n            self.assertTrue(post.ndim == 1)\n        else:\n            self.assertTrue(post.ndim == 2)\n\n        self.assertTrue(len(post) == len(\n                set(util.array2d_to_tuples(arrays[0]))\n                | set(util.array2d_to_tuples(arrays[1])))\n                )\n\n\n    @given(get_arrays_2d_aligned_columns(min_size=2, max_size=2))\n    def test_intersect2d(self, arrays: tp.Sequence[np.ndarray]) -> None:\n        post = util.intersect2d(arrays[0], arrays[1], assume_unique=False)\n        if post.dtype == object:\n            self.assertTrue(post.ndim == 1)\n        else:\n            self.assertTrue(post.ndim == 2)\n\n        self.assertTrue(len(post) == len(\n                set(util.array2d_to_tuples(arrays[0]))\n                & set(util.array2d_to_tuples(arrays[1])))\n                )\n\n    @given(get_arrays_2d_aligned_columns(min_size=2, max_size=2))\n    def test_setdiff2d(self, arrays: tp.Sequence[np.ndarray]) -> None:\n        post = util.setdiff2d(arrays[0], arrays[1], assume_unique=False)\n        if post.dtype == object:\n            self.assertTrue(post.ndim == 1)\n        else:\n            self.assertTrue(post.ndim == 2)\n\n        self.assertTrue(len(post) == len(\n                set(util.array2d_to_tuples(arrays[0])).difference(\n                set(util.array2d_to_tuples(arrays[1]))))\n                )\n\n    @given(get_arrays_2d_aligned_columns())\n    def test_array_set_ufunc_many(self, arrays: tp.Sequence[np.ndarray]) -> None:\n\n        for union in (True, False):\n            post = util.ufunc_set_iter(arrays, union=union)\n            if post.dtype == object:\n                # returned object arrays might be 2D or 1D of tuples\n                self.assertTrue(post.ndim in (1, 2))\n            else:\n                self.assertTrue(post.ndim == 2)\n\n    @given(get_array_1d2d(min_rows=1, min_columns=1))\n    def test_isin(self, array: np.ndarray) -> None:\n\n        container_factory = (list, set, np.array)\n        result = None\n\n        if array.ndim == 1:\n            sample = array[0]\n            if np.array(sample).dtype.kind in DTYPE_NAN_KIND and np.isnan(sample):\n                pass\n            else:\n                for factory in container_factory:\n                    result = util.isin(array, factory((sample,)))\n                    self.assertTrue(result[0])\n        elif array.ndim == 2:\n            sample = array[0, 0]\n            if np.array(sample).dtype.kind in DTYPE_NAN_KIND and np.isnan(sample):\n                pass\n            else:\n                for factory in container_factory:\n                    result = util.isin(array, factory((sample,)))\n                    self.assertTrue(result[0, 0])\n\n        if result is not None:\n            self.assertTrue(array.shape == result.shape)\n            self.assertTrue(result.dtype == bool)\n\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/unit/__init__.py,0,b''
static_frame/test/unit/test_array_go.py,5,"b""\nimport unittest\nimport numpy as np\n\nfrom static_frame import mloc\n\nfrom static_frame.core.array_go import ArrayGO\n\nfrom static_frame.test.test_case import TestCase\n\n\nclass TestUnit(TestCase):\n\n\n\n    def test_array_init_a(self) -> None:\n        with self.assertRaises(NotImplementedError):\n            _ = ArrayGO(np.array((3, 4, 5)))\n\n    def test_array_append_a(self) -> None:\n\n        ag1 = ArrayGO(('a', 'b', 'c', 'd'))\n\n        self.assertEqual([x for x in ag1],\n            ['a', 'b', 'c', 'd'])\n\n        self.assertEqual(ag1.values.tolist(),\n                ['a', 'b', 'c', 'd'])\n\n\n        ag1.append('e')\n        ag1.extend(('f', 'g'))\n\n        self.assertEqual(ag1.values.tolist(),\n            ['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n\n        self.assertEqual([x for x in ag1],\n            ['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n\n\n    def test_array_append_b(self) -> None:\n\n        ag1 = ArrayGO(np.array(('a', 'b', 'c', 'd'), object))\n\n        self.assertEqual([x for x in ag1],\n            ['a', 'b', 'c', 'd'])\n\n        self.assertEqual(ag1.values.tolist(),\n                ['a', 'b', 'c', 'd'])\n\n\n        ag1.append('e')\n        ag1.extend(('f', 'g'))\n\n        self.assertEqual(ag1.values.tolist(),\n            ['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n\n        self.assertEqual([x for x in ag1],\n            ['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n\n\n    def test_array_getitem_a(self) -> None:\n\n        a = np.array(('a', 'b', 'c', 'd'), object)\n        a.flags.writeable = False\n\n        ag1 = ArrayGO(a)\n        # insure no copy for immutable\n        self.assertEqual(mloc(ag1.values), mloc(a))\n\n        ag1.append('b')\n\n        post = ag1[ag1.values == 'b']\n\n        self.assertEqual(post.tolist(), ['b', 'b'])\n        self.assertEqual(ag1[[2,1,1,1]].tolist(),\n                ['c', 'b', 'b', 'b'])\n\n    def test_array_copy_a(self) -> None:\n\n        ag1 = ArrayGO(np.array(('a', 'b', 'c', 'd'), dtype=object))\n        ag1.append('e')\n\n        ag2 = ag1.copy()\n        ag1.extend(('f', 'g'))\n\n        self.assertEqual(ag1.values.tolist(),\n                ['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n\n        self.assertEqual(ag2.values.tolist(),\n                ['a', 'b', 'c', 'd', 'e'])\n\n\n\n    def test_array_len_a(self) -> None:\n\n        ag1 = ArrayGO(np.array(('a', 'b', 'c', 'd'), object))\n        ag1.append('e')\n\n        self.assertEqual(len(ag1), 5)\n\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n\n\n"""
static_frame/test/unit/test_bus.py,3,"b""import unittest\n# from io import StringIO\nimport numpy as np\n\nfrom static_frame.core.frame import Frame\nfrom static_frame.core.bus import Bus\nfrom static_frame.core.bus import FrameDeferred\n\nfrom static_frame.core.series import Series\n\nfrom static_frame.core.store_zip import StoreZipTSV\n\nfrom static_frame.core.store import StoreConfigMap\nfrom static_frame.core.store import StoreConfig\nfrom static_frame.core.display import DisplayConfig\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.test.test_case import temp_file\nfrom static_frame.test.test_case import skip_win\n\n# from static_frame.test.test_case import skip_win\nfrom static_frame.core.exception import ErrorInitBus\nfrom static_frame.core.exception import StoreFileMutation\n\n\nclass TestUnit(TestCase):\n\n    def test_frame_deferred_a(self) -> None:\n\n        self.assertEqual(str(FrameDeferred), '<FrameDeferred>')\n\n    def test_bus_slotted_a(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='foo')\n\n        b1 = Bus.from_frames((f1,))\n\n        with self.assertRaises(AttributeError):\n            b1.g = 30 # type: ignore #pylint: disable=E0237\n        with self.assertRaises(AttributeError):\n            b1.__dict__ #pylint: disable=W0104\n\n    def test_bus_init_a(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='foo')\n        f2 = Frame.from_dict(\n                dict(a=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='bar')\n\n        config = StoreConfigMap.from_config(StoreConfig(index_depth=1))\n        b1 = Bus.from_frames((f1, f2), config=config)\n\n        self.assertEqual(b1.keys().values.tolist(),\n                ['foo', 'bar'])\n\n\n        with temp_file('.zip') as fp:\n            b1.to_zip_tsv(fp)\n            b2 = Bus.from_zip_tsv(fp)\n\n            f3 = b2['bar']\n            f4 = b2['foo']\n            # import ipdb; ipdb.set_trace()\n            zs = StoreZipTSV(fp)\n            zs.write(b1.items())\n\n            # how to show that this derived getitem has derived type?\n            f3 = zs.read('foo', config=config['foo'])\n            self.assertEqual(\n                f3.to_pairs(0),\n                (('a', (('x', 1), ('y', 2))), ('b', (('x', 3), ('y', 4))))\n            )\n\n    def test_bus_init_b(self) -> None:\n\n        with self.assertRaises(ErrorInitBus):\n            Bus(Series([1, 2, 3]))\n\n        with self.assertRaises(ErrorInitBus):\n            Bus(Series([3, 4], dtype=object))\n\n        with self.assertRaises(ErrorInitBus):\n            Bus(Series([3, 4], index=('a', 'b'), dtype=object))\n\n\n    def test_bus_init_c(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='foo')\n        f2 = Frame.from_dict(\n                dict(a=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='bar')\n\n        config = StoreConfigMap.from_config(StoreConfig(index_depth=1))\n        b1 = Bus.from_frames((f1, f2), config=config)\n\n        self.assertEqual(b1.keys().values.tolist(),\n                ['foo', 'bar'])\n\n        with temp_file('.zip') as fp:\n            b1.to_zip_csv(fp)\n            b2 = Bus.from_zip_csv(fp, config=config)\n\n            f1_loaded = b2['foo']\n            f2_loaded = b2['bar']\n\n            self.assertEqualFrames(f1, f1_loaded)\n            self.assertEqualFrames(f2, f2_loaded)\n\n    def test_bus_interface_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='foo')\n        f2 = Frame.from_dict(\n                dict(a=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='bar')\n\n        b1 = Bus.from_frames((f1, f2))\n        post = b1.interface\n        self.assertTrue(isinstance(post, Frame))\n        self.assertTrue(len(post) > 38)\n\n    def test_bus_shapes_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(a=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(a=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        b1 = Bus.from_frames((f1, f2, f3))\n\n        with temp_file('.zip') as fp:\n\n            b1.to_zip_pickle(fp)\n\n            b2 = Bus.from_zip_pickle(fp)\n\n            f2_loaded = b2['f2']\n\n            self.assertEqual(b2.shapes.to_pairs(),\n                    (('f1', None), ('f2', (3, 2)), ('f3', None)))\n\n            f3_loaded = b2['f3']\n\n            self.assertEqual(b2.shapes.to_pairs(),\n                    (('f1', None), ('f2', (3, 2)), ('f3', (2, 2 )))\n                    )\n\n    @skip_win # type: ignore\n    def test_bus_nbytes_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(a=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(a=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        b1 = Bus.from_frames((f1, f2, f3))\n\n        with temp_file('.zip') as fp:\n            b1.to_zip_pickle(fp)\n            b2 = Bus.from_zip_pickle(fp)\n\n            f2_loaded = b2['f2']\n\n            self.assertEqual(b2.nbytes, 48)\n\n            f3_loaded = b2['f3']\n\n            self.assertEqual(b2.nbytes, 80)\n\n            f1_loaded = b2['f1']\n\n            self.assertEqual(b2.nbytes, 112)\n\n\n    @skip_win # type: ignore\n    def test_bus_dtypes_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(c=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(d=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        b1 = Bus.from_frames((f1, f2, f3))\n\n        with temp_file('.zip') as fp:\n            b1.to_zip_pickle(fp)\n            b2 = Bus.from_zip_pickle(fp)\n\n            self.assertEqual(b2.dtypes.to_pairs(0), ())\n\n            f2_loaded = b2['f2']\n\n            self.assertEqual(b2.dtypes.to_pairs(0),\n                    (('c', (('f1', None), ('f2', np.dtype('int64')), ('f3', None))), ('b', (('f1', None), ('f2', np.dtype('int64')), ('f3', None))))\n            )\n\n            f3_loaded = b2['f3']\n\n            self.assertEqual(b2.dtypes.to_pairs(0),\n                    (('b', (('f1', None), ('f2', np.dtype('int64')), ('f3', np.dtype('int64')))), ('c', (('f1', None), ('f2', np.dtype('int64')), ('f3', None))), ('d', (('f1', None), ('f2', None), ('f3', np.dtype('int64')))))\n                    )\n\n\n    def test_bus_mloc_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(c=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(d=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        b1 = Bus.from_frames((f1, f2, f3))\n\n        with temp_file('.zip') as fp:\n            b1.to_zip_pickle(fp)\n            b2 = Bus.from_zip_pickle(fp)\n\n            f2_loaded = b2['f2']\n\n            mloc1 = b2.mloc\n\n            f3_loaded = b2['f3']\n            f1_loaded = b2['f1']\n\n            self.assertEqual(mloc1['f2'], b2.mloc.loc['f2'])\n\n\n    @skip_win # type: ignore\n    def test_bus_status_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(c=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(d=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        b1 = Bus.from_frames((f1, f2, f3))\n\n        with temp_file('.zip') as fp:\n            b1.to_zip_pickle(fp)\n            b2 = Bus.from_zip_pickle(fp)\n\n            status = b2.status\n            self.assertEqual(status.shape, (3, 4))\n            # force load all\n            tuple(b2.items())\n\n            self.assertEqual(\n                    b2.status.to_pairs(0),                                                           (('loaded', (('f1', True), ('f2', True), ('f3', True))), ('size', (('f1', 4.0), ('f2', 6.0), ('f3', 4.0))), ('nbytes', (('f1', 32.0), ('f2', 48.0), ('f3', 32.0))),('shape', (('f1', (2, 2)), ('f2', (3, 2)), ('f3', (2, 2)))))\n            )\n\n\n    def test_bus_keys_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(c=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(d=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n        f4 = Frame.from_dict(\n                dict(q=(None,None), r=(np.nan,np.nan)),\n                index=(1000, 1001),\n                name='f4')\n\n        b1 = Bus.from_frames((f1, f2, f3, f4))\n\n        self.assertEqual(b1.keys().values.tolist(), ['f1', 'f2', 'f3', 'f4'])\n        self.assertEqual(b1.values[2].name, 'f3')\n\n        with temp_file('.zip') as fp:\n            b1.to_zip_pickle(fp)\n            b2 = Bus.from_zip_pickle(fp)\n            self.assertFalse(b2._loaded_all)\n\n            self.assertEqual(b2.keys().values.tolist(), ['f1', 'f2', 'f3', 'f4'])\n            self.assertFalse(b2._loaded.any())\n            # accessing values forces loading all\n            self.assertEqual(b2.values[2].name, 'f3')\n            self.assertTrue(b2._loaded_all)\n\n\n    def test_bus_reversed_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(c=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(d=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        b1 = Bus.from_frames((f1, f2, f3))\n        self.assertEqual(list(reversed(b1)), ['f3', 'f2', 'f1'])\n\n\n    def test_bus_display_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(c=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(d=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        b1 = Bus.from_frames((f1, f2, f3))\n        self.assertEqual(\n                b1.display(config=DisplayConfig(type_color=False)).to_rows(),\n                ['<Bus>',\n                '<Index>',\n                'f1      Frame',\n                'f2      Frame',\n                'f3      Frame',\n                '<<U2>   <object>'])\n\n        rows1 = b1.display(config=DisplayConfig(\n                type_color=False,\n                type_show=False)).to_rows()\n        self.assertEqual(rows1, ['f1 Frame', 'f2 Frame', 'f3 Frame'])\n\n        rows2 = b1.display(config=DisplayConfig(\n                type_color=False,\n                type_show=False,\n                include_index=False)).to_rows()\n        self.assertEqual(rows2, ['Frame', 'Frame', 'Frame'])\n\n\n\n    #---------------------------------------------------------------------------\n    def test_bus_iloc_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(c=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(d=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        b1 = Bus.from_frames((f1, f2, f3))\n\n        with temp_file('.zip') as fp:\n            b1.to_zip_pickle(fp)\n            b2 = Bus.from_zip_pickle(fp)\n\n            self.assertEqual(\n                    b2.iloc[[0, 2]].status['loaded'].to_pairs(), # type: ignore\n                    (('f1', True), ('f3', True))\n                    )\n\n    def test_bus_iloc_b(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n\n        b1 = Bus.from_frames((f1,))\n        f2 = b1.iloc[0]\n        self.assertTrue(f1 is f2)\n\n    def test_bus_loc_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n\n        b1 = Bus.from_frames((f1,))\n        f2 = b1.loc['f1']\n        self.assertTrue(f1 is f2)\n\n    def test_bus_loc_b(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(c=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(d=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        b1 = Bus.from_frames((f1, f2, f3))\n        b2 = b1.loc['f2':] #type: ignore\n        self.assertEqual(len(b2), 2)\n        self.assertEqual(b2.index.values.tolist(), ['f2', 'f3']) #type: ignore\n\n\n    def test_bus_getitem_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(c=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(d=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        b1 = Bus.from_frames((f1, f2, f3))\n\n        with temp_file('.zip') as fp:\n            b1.to_zip_pickle(fp)\n            b2 = Bus.from_zip_pickle(fp)\n\n            self.assertEqual(b2['f2':].status['loaded'].to_pairs(), #type: ignore\n                    (('f2', True), ('f3', True))\n                    )\n\n\n    def test_bus_to_xlsx_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(c=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(d=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        config = StoreConfigMap.from_config(\n                StoreConfig(\n                        index_depth=1,\n                        columns_depth=1,\n                        include_columns=True,\n                        include_index=True\n                        ))\n        b1 = Bus.from_frames((f1, f2, f3), config=config)\n\n        with temp_file('.xlsx') as fp:\n            b1.to_xlsx(fp)\n\n            b2 = Bus.from_xlsx(fp, config=config)\n            tuple(b2.items()) # force loading all\n\n        for frame in (f1, f2, f3):\n            self.assertEqualFrames(frame, b2[frame.name])\n\n\n    def test_bus_to_xlsx_b(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(a=(1,2,3)),\n                index=('x', 'y', 'z'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(A=(10,20,30)),\n                index=('q', 'r', 's'),\n                name='f2')\n\n        config = StoreConfig(include_index=True, index_depth=1)\n        b1 = Bus.from_frames((f1, f2), config=config)\n\n        with temp_file('.xlsx') as fp:\n            b1.to_xlsx(fp)\n\n            b2 = Bus.from_xlsx(fp, config=config)\n            tuple(b2.items()) # force loading all\n\n        for frame in (f1, f2):\n            self.assertEqualFrames(frame, b2[frame.name])\n\n\n\n    def test_bus_to_xlsx_c(self) -> None:\n        '''\n        Test manipulating a file behind the Bus.\n        '''\n        f1 = Frame.from_dict(\n                dict(a=(1,2,3)),\n                index=('x', 'y', 'z'),\n                name='f1')\n\n        f2 = Frame.from_dict(\n                dict(x=(10,20,30)),\n                index=('q', 'r', 's'),\n                name='f2')\n\n        b1 = Bus.from_frames((f1,),)\n\n        with temp_file('.xlsx') as fp:\n\n            b1.to_xlsx(fp)\n\n            b2 = Bus.from_xlsx(fp)\n\n            f2.to_xlsx(fp)\n\n            with self.assertRaises(StoreFileMutation):\n                tuple(b2.items())\n\n\n    def test_bus_to_xlsx_d(self) -> None:\n        '''\n        Test manipulating a file behind the Bus.\n        '''\n        f1 = Frame.from_dict(\n                dict(a=(1,2,3)),\n                index=('x', 'y', 'z'),\n                name='f1')\n\n        b1 = Bus.from_frames((f1,),)\n\n        with temp_file('.xlsx') as fp:\n\n            b1.to_xlsx(fp)\n\n            b2 = Bus.from_xlsx(fp)\n\n        with self.assertRaises(StoreFileMutation):\n            tuple(b2.items())\n\n\n    #---------------------------------------------------------------------------\n    def test_bus_to_sqlite_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(c=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(d=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        frames = (f1, f2, f3)\n        config = StoreConfigMap.from_frames(frames)\n        b1 = Bus.from_frames(frames, config=config)\n\n        with temp_file('.sqlite') as fp:\n            b1.to_sqlite(fp)\n            b2 = Bus.from_sqlite(fp, config=config)\n            tuple(b2.items()) # force loading all\n\n        for frame in frames:\n            self.assertEqualFrames(frame, b2[frame.name])\n\n\n    def test_bus_to_sqlite_b(self) -> None:\n        '''\n        Test manipulating a file behind the Bus.\n        '''\n        f1 = Frame.from_dict(\n                dict(a=(1,2,3)),\n                index=('x', 'y', 'z'),\n                name='f1')\n\n        b1 = Bus.from_frames((f1,),)\n\n        with temp_file('.db') as fp:\n\n            b1.to_sqlite(fp)\n\n            b2 = Bus.from_sqlite(fp)\n\n        with self.assertRaises(StoreFileMutation):\n            tuple(b2.items())\n\n    def test_bus_to_hdf5_a(self) -> None:\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(c=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(d=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        frames = (f1, f2, f3)\n        config = StoreConfigMap.from_frames(frames)\n        b1 = Bus.from_frames(frames, config=config)\n\n        with temp_file('.h5') as fp:\n            b1.to_hdf5(fp)\n            b2 = Bus.from_hdf5(fp, config=config)\n            tuple(b2.items()) # force loading all\n\n        for frame in frames:\n            self.assertEqualFrames(frame, b2[frame.name])\n\n\n    def test_bus_to_hdf5_b(self) -> None:\n        '''\n        Test manipulating a file behind the Bus.\n        '''\n        f1 = Frame.from_dict(\n                dict(a=(1,2,3)),\n                index=('x', 'y', 'z'),\n                name='f1')\n\n        b1 = Bus.from_frames((f1,),)\n\n        with temp_file('.h5') as fp:\n\n            b1.to_hdf5(fp)\n\n            b2 = Bus.from_hdf5(fp)\n\n        with self.assertRaises(StoreFileMutation):\n            tuple(b2.items())\n\n    #---------------------------------------------------------------------------\n\n\n    def test_bus_equals_a(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(c=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_dict(\n                dict(d=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        f4 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='f1')\n        f5 = Frame.from_dict(\n                dict(c=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f6 = Frame.from_dict(\n                dict(d=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='f3')\n\n        f7 = Frame.from_dict(\n                dict(d=(10,20), b=(50,61)),\n                index=('p', 'q'),\n                name='f3')\n\n\n        b1 = Bus.from_frames((f1, f2, f3))\n        b2 = Bus.from_frames((f1, f2, f3))\n        b3 = Bus.from_frames((f4, f5, f6))\n        b4 = Bus.from_frames((f4, f5, f7))\n        b5 = Bus.from_frames((f4, f5))\n        b6 = Bus.from_frames((f3, f2, f1))\n\n        self.assertTrue(b1.equals(b2))\n        self.assertTrue(b1.equals(b3))\n\n        self.assertFalse(b1.equals(b4))\n        self.assertFalse(b1.equals(b5))\n        self.assertFalse(b1.equals(b6))\n\n\n\n\n\n\n\nif __name__ == '__main__':\n\n    unittest.main()\n\n"""
static_frame/test/unit/test_container.py,86,"b""\n\n\n\nimport numpy as np\nimport unittest\n\nfrom static_frame.core.container import ContainerOperand\nfrom static_frame.core.container import UFUNC_AXIS_SKIPNA\nfrom static_frame.core.container import UFUNC_SHAPE_SKIPNA\nfrom static_frame.core.container import _any\nfrom static_frame.core.container import _all\nfrom static_frame.core.container import _nanall\nfrom static_frame.core.container import _nanany\nfrom static_frame.core.container import _ufunc_logical_skipna\n\nfrom static_frame.test.test_case import TestCase\n\n\n\nclass TestUnit(TestCase):\n\n    def test_container_attrs(self) -> None:\n\n        for attr in UFUNC_AXIS_SKIPNA.keys() | UFUNC_SHAPE_SKIPNA.keys():\n            c = ContainerOperand\n            self.assertTrue(hasattr(c, attr))\n\n        with self.assertRaises(NotImplementedError):\n            c().display()\n\n    def test_container_any_a(self) -> None:\n\n        self.assertTrue(_nanany(np.array([np.nan, False, True])))\n        self.assertTrue(_nanany(np.array(['foo', '', np.nan], dtype=object)))\n        self.assertTrue(_nanany(np.array(['', None, 1], dtype=object)))\n\n        self.assertFalse(_nanany(np.array([False, np.nan], dtype=object)))\n        self.assertFalse(_nanany(np.array([False, None])))\n        self.assertFalse(_nanany(np.array(['', np.nan], dtype=object)))\n        self.assertFalse(_nanany(np.array(['', None], dtype=object)))\n\n\n    def test_container_any_b(self) -> None:\n\n        self.assertTrue(_any(np.array([False, True])))\n        self.assertTrue(_any(np.array([False, True])))\n        self.assertTrue(_any(np.array([False, True], dtype=object)))\n        self.assertTrue(_any(np.array(['foo', ''])))\n        self.assertTrue(_any(np.array(['foo', ''], dtype=object)))\n\n\n        self.assertFalse(_any(np.array([False, False])))\n        self.assertFalse(_any(np.array([False, False], dtype=object)))\n        self.assertFalse(_any(np.array(['', ''])))\n        self.assertFalse(_any(np.array(['', ''], dtype=object)))\n\n\n        # self.assertTrue(\n        #         np.isnan(_any(np.array([False, np.nan], dtype=object)))\n        #         )\n        # self.assertTrue(\n        #         np.isnan(_any(np.array([False, None], dtype=object)))\n        #         )\n\n\n\n\n    def test_container_all_a(self) -> None:\n\n        self.assertTrue(_nanall(np.array([np.nan, True, True], dtype=object)))\n        self.assertTrue(_nanall(np.array([np.nan, True], dtype=object)))\n        self.assertTrue(_nanall(np.array([np.nan, 1.0])))\n\n\n        self.assertFalse(_nanall(np.array([None, False, False], dtype=object)))\n        self.assertFalse(_nanall(np.array([np.nan, False, False], dtype=object)))\n        self.assertFalse(_nanall(np.array([None, False, False], dtype=object)))\n\n\n    def test_container_all_b(self) -> None:\n        self.assertTrue(_all(np.array([True, True])))\n        self.assertTrue(_all(np.array([1, 2])))\n\n\n        self.assertFalse(_all(np.array([1, 0])))\n        self.assertFalse(_all(np.array([False, False])))\n\n        with self.assertRaises(TypeError):\n            np.isnan(_all(np.array([False, np.nan], dtype=object)))\n        with self.assertRaises(TypeError):\n            np.isnan(_all(np.array([False, None], dtype=object)))\n\n\n\n    def test_ufunc_logical_skipna_a(self) -> None:\n\n        # empty arrays\n        a1 = np.array([], dtype=float)\n        self.assertEqual(_ufunc_logical_skipna(a1, np.all, skipna=False), True)\n\n        a1 = np.array([], dtype=float)\n        self.assertEqual(_ufunc_logical_skipna(a1, np.any, skipna=False), False)\n\n\n        # float arrays 1d\n        a1 = np.array([2.4, 5.4], dtype=float)\n        self.assertEqual(_ufunc_logical_skipna(a1, np.all, skipna=True), True)\n\n        # skippna is False, but there is non NaN, so we do not raise\n        a1 = np.array([2.4, 0], dtype=float)\n        self.assertEqual(_ufunc_logical_skipna(a1, np.all, skipna=False), False)\n\n        a1 = np.array([0, np.nan, 0], dtype=float)\n        self.assertEqual(_ufunc_logical_skipna(a1, np.any, skipna=True), False)\n\n        with self.assertRaises(TypeError):\n            a1 = np.array([0, np.nan, 0], dtype=float)\n            self.assertEqual(_ufunc_logical_skipna(a1, np.any, skipna=False), True)\n\n\n        # float arrays 2d\n        a1 = np.array([[2.4, 5.4, 3.2], [2.4, 5.4, 3.2]], dtype=float)\n        self.assertEqual(_ufunc_logical_skipna(a1, np.all, skipna=False, axis=0).tolist(),\n                [True, True, True])\n\n        a1 = np.array([[2.4, 5.4, 3.2], [2.4, 5.4, 3.2]], dtype=float)\n        self.assertEqual(_ufunc_logical_skipna(a1, np.all, skipna=False, axis=1).tolist(),\n                [True, True])\n\n        a1 = np.array([[2.4, 5.4, 0], [2.4, 5.4, 3.2]], dtype=float)\n        self.assertEqual(_ufunc_logical_skipna(a1, np.all, skipna=False, axis=0).tolist(),\n                [True, True, False])\n\n        a1 = np.array([[2.4, 5.4, 0], [2.4, 5.4, 3.2]], dtype=float)\n        self.assertEqual(_ufunc_logical_skipna(a1, np.all, skipna=False, axis=1).tolist(),\n                [False, True])\n\n\n        # object arrays\n        a1 = np.array([[2.4, 5.4, 0], [2.4, None, 3.2]], dtype=object)\n\n\n        with self.assertRaises(TypeError):\n            self.assertAlmostEqualValues(\n                    _ufunc_logical_skipna(a1, np.all, skipna=False, axis=1).tolist(),\n                    [False, np.nan])\n\n        with self.assertRaises(TypeError):\n            self.assertAlmostEqualValues(\n                    _ufunc_logical_skipna(a1, np.any, skipna=False, axis=1).tolist(),\n                    [True, np.nan])\n\n        with self.assertRaises(TypeError):\n            self.assertAlmostEqualValues(_ufunc_logical_skipna(a1, np.all, skipna=False, axis=0).tolist(),\n                    [True, np.nan, False])\n\n        with self.assertRaises(TypeError):\n            self.assertAlmostEqualValues(_ufunc_logical_skipna(a1, np.any, skipna=False, axis=0).tolist(),\n                    [True, np.nan, True])\n\n\n        a2 = np.array([[2.4, 5.4, 0], [2.4, np.nan, 3.2]], dtype=object)\n\n        with self.assertRaises(TypeError):\n            self.assertAlmostEqualValues(\n                    _ufunc_logical_skipna(a2, np.any, skipna=False, axis=1).tolist(),\n                    [True, np.nan])\n\n        with self.assertRaises(TypeError):\n            self.assertAlmostEqualValues(_ufunc_logical_skipna(a2, np.all, skipna=False, axis=0).tolist(),\n                    [True, np.nan, False])\n\n        with self.assertRaises(TypeError):\n            self.assertAlmostEqualValues(_ufunc_logical_skipna(a2, np.any, skipna=False, axis=0).tolist(),\n                    [True, np.nan, True])\n\n\n    def test_ufunc_logical_skipna_b(self) -> None:\n        # object arrays\n\n        a1 = np.array([['sdf', '', 'wer'], [True, False, True]], dtype=object)\n\n        self.assertEqual(\n                _ufunc_logical_skipna(a1, np.all, skipna=False, axis=0).tolist(),\n                [True, False, True]\n                )\n        self.assertEqual(\n                _ufunc_logical_skipna(a1, np.all, skipna=False, axis=1).tolist(),\n                [False, False]\n                )\n\n\n        # string arrays\n        a1 = np.array(['sdf', ''], dtype=str)\n        self.assertEqual(_ufunc_logical_skipna(a1, np.all, skipna=False, axis=0), False)\n        self.assertEqual(_ufunc_logical_skipna(a1, np.all, skipna=True, axis=0), False)\n\n\n        a1 = np.array([['sdf', '', 'wer'], ['sdf', '', 'wer']], dtype=str)\n        self.assertEqual(\n                _ufunc_logical_skipna(a1, np.all, skipna=False, axis=0).tolist(),\n                [True,  False,  True])\n\n        self.assertEqual(\n                _ufunc_logical_skipna(a1, np.all, skipna=False, axis=1).tolist(),\n                [False, False])\n\n        self.assertEqual(\n                _ufunc_logical_skipna(a1, np.any, skipna=False, axis=0).tolist(),\n                [True,  False,  True])\n\n        self.assertEqual(\n                _ufunc_logical_skipna(a1, np.any, skipna=False, axis=1).tolist(),\n                [True, True])\n\n\n    def test_ufunc_logical_skipna_c(self) -> None:\n\n        a1 = np.array([], dtype=float)\n        with self.assertRaises(NotImplementedError):\n            _ufunc_logical_skipna(a1, np.sum, skipna=True)\n\n\n    def test_ufunc_logical_skipna_d(self) -> None:\n\n        a1 = np.array(['2018-01-01', '2018-02-01'], dtype=np.datetime64)\n        post1 = _ufunc_logical_skipna(a1, np.all, skipna=True)\n        self.assertTrue(post1)\n\n        a2 = np.array(['2018-01-01', '2018-02-01', None], dtype=np.datetime64)\n        with self.assertRaises(TypeError):\n            post2 = _ufunc_logical_skipna(a2, np.all, skipna=False)\n\n\n    def test_ufunc_logical_skipna_e(self) -> None:\n\n        a1 = np.array([['2018-01-01', '2018-02-01'],\n                ['2018-01-01', '2018-02-01']], dtype=np.datetime64)\n        post = _ufunc_logical_skipna(a1, np.all, skipna=True)\n        self.assertEqual(post.tolist(), [True, True])\n\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/unit/test_container_util.py,16,"b""import unittest\n\nimport numpy as np\n\n\nfrom static_frame.core.container_util import is_static\nfrom static_frame.core.container_util import index_from_optional_constructor\nfrom static_frame.core.container_util import matmul\nfrom static_frame.core.container_util import key_to_ascending_key\n\nfrom static_frame.core.container_util import pandas_to_numpy\nfrom static_frame.core.container_util import pandas_version_under_1\nfrom static_frame.core.container_util import bloc_key_normalize\n\n\nfrom static_frame import Series\nfrom static_frame import Frame\n\nfrom static_frame import Index\nfrom static_frame import IndexGO\n# from static_frame import IndexDate\nfrom static_frame import IndexHierarchy\nfrom static_frame import IndexHierarchyGO\n\n# from static_frame import IndexYearMonth\n# from static_frame import IndexYear\nfrom static_frame import IndexSecond\n# from static_frame import IndexMillisecond\n\nfrom static_frame.test.test_case import TestCase\n\n\n\nclass TestUnit(TestCase):\n\n\n    def test_is_static_a(self) -> None:\n        self.assertTrue(is_static(Index))\n        self.assertFalse(is_static(IndexGO))\n\n        self.assertTrue(is_static(IndexHierarchy))\n        self.assertFalse(is_static(IndexHierarchyGO))\n\n    def test_is_static_b(self) -> None:\n\n        self.assertTrue(is_static(Index.from_labels))\n        self.assertTrue(is_static(IndexHierarchy.from_labels))\n        self.assertTrue(is_static(IndexHierarchy.from_product))\n        self.assertTrue(is_static(IndexHierarchy.from_labels_delimited))\n        self.assertTrue(is_static(IndexHierarchy.from_tree))\n        self.assertTrue(is_static(IndexHierarchy.from_index_items))\n\n        self.assertFalse(is_static(IndexGO.from_labels))\n        self.assertFalse(is_static(IndexHierarchyGO.from_labels))\n        self.assertFalse(is_static(IndexHierarchyGO.from_product))\n        self.assertFalse(is_static(IndexHierarchyGO.from_labels_delimited))\n        self.assertFalse(is_static(IndexHierarchyGO.from_tree))\n        self.assertFalse(is_static(IndexHierarchyGO.from_index_items))\n\n\n    def test_index_from_optional_constructor_a(self) -> None:\n        idx1 = index_from_optional_constructor([1, 3, 4],\n                default_constructor=Index)\n        self.assertEqual(idx1.__class__, Index)\n\n        # given a mutable index and an immutable default, get immutable version\n        idx2 = index_from_optional_constructor(IndexGO((1, 3, 4)),\n                default_constructor=Index)\n        self.assertEqual(idx2.__class__, Index)\n\n        # given a mutable index and an immutable default, get immutable version\n        idx3 = index_from_optional_constructor(IndexGO((1, 3, 4)),\n                default_constructor=IndexGO)\n        self.assertEqual(idx3.__class__, IndexGO)\n\n        # given a mutable index and an immutable default, get immutable version\n        idx4 = index_from_optional_constructor(\n                IndexSecond((1, 3, 4)),\n                default_constructor=Index)\n        self.assertEqual(idx4.__class__, IndexSecond)\n\n\n    def test_index_from_optional_constructor_b(self) -> None:\n        idx0 = IndexHierarchy.from_labels(\n                [('a', 0), ('a', 1), ('b', 0), ('b', 1)])\n        idx1 = index_from_optional_constructor(\n                idx0,\n                default_constructor=IndexHierarchy.from_labels)\n\n        # Since the default constructo is static, we should be able to reuse the index\n        self.assertEqual(id(idx0), id(idx1))\n\n\n    def test_index_from_optional_constructor_c(self) -> None:\n        idx0 = IndexHierarchyGO.from_labels(\n                [('a', 0), ('a', 1), ('b', 0), ('b', 1)])\n        idx1 = index_from_optional_constructor(\n                idx0,\n                default_constructor=IndexHierarchy.from_labels)\n\n        # Since the default constructo is static, we should be able to reuse the index\n        self.assertNotEqual(id(idx0), id(idx1))\n        self.assertTrue(idx1.STATIC)\n\n\n    def test_index_from_optional_constructor_d(self) -> None:\n        idx0 = IndexHierarchy.from_labels(\n                [('a', 0), ('a', 1), ('b', 0), ('b', 1)])\n        idx1 = index_from_optional_constructor(\n                idx0,\n                default_constructor=IndexHierarchyGO.from_labels)\n\n        # Since the default constructo is static, we should be able to reuse the index\n        self.assertNotEqual(id(idx0), id(idx1))\n        self.assertFalse(idx1.STATIC)\n\n\n\n    def test_matmul_a(self) -> None:\n        # lhs: frame, rhs: array\n\n        f1 = Frame.from_items((('a', (1, 2, 3)), ('b', (3, 4, 5))),\n                index=('x', 'y', 'z'))\n\n        self.assertEqual(\n                matmul(f1, [4, 3]).to_pairs(),\n                (('x', 13), ('y', 20), ('z', 27))\n                )\n\n        self.assertEqual(\n                matmul(f1, np.array([4, 3])).to_pairs(),\n                (('x', 13), ('y', 20), ('z', 27))\n                )\n\n\n        self.assertEqual(\n                matmul(f1, [3, 4]).to_pairs(),\n                (('x', 15), ('y', 22), ('z', 29))\n                )\n\n        self.assertEqual(\n                matmul(f1, np.array([3, 4])).to_pairs(),\n                (('x', 15), ('y', 22), ('z', 29))\n                )\n\n\n    def test_matmul_b(self) -> None:\n        # lhs: frame, rhs: array\n\n        f1 = Frame.from_items((('a', (1, 2, 3)), ('b', (3, 4, 5))),\n                index=('x', 'y', 'z'))\n\n        # get an auto incremented integer columns\n        self.assertEqual(\n            matmul(f1, np.arange(10).reshape(2, 5)).to_pairs(0),\n            ((0, (('x', 15), ('y', 20), ('z', 25))), (1, (('x', 19), ('y', 26), ('z', 33))), (2, (('x', 23), ('y', 32), ('z', 41))), (3, (('x', 27), ('y', 38), ('z', 49))), (4, (('x', 31), ('y', 44), ('z', 57))))\n            )\n\n    def test_matmul_c(self) -> None:\n        # lhs: frame, rhs: Series, 1D array\n\n        f1 = Frame.from_items((('a', (1, 2, 3)), ('b', (3, 4, 5))),\n                index=('x', 'y', 'z'))\n        s1 = Series((10, 11), index=('a', 'b'))\n\n        self.assertEqual(matmul(f1, s1).to_pairs(),\n                (('x', 43), ('y', 64), ('z', 85)))\n\n        self.assertEqual(matmul(f1, s1.values).to_pairs(),\n                (('x', 43), ('y', 64), ('z', 85)))\n\n        with self.assertRaises(RuntimeError):\n            matmul(f1, np.arange(20).reshape(5, 4))\n\n\n    def test_matmul_d(self) -> None:\n        # lhs: series, rhs: frame\n\n        f1 = Frame.from_items((('a', (1, 2, 3)), ('b', (3, 4, 5))),\n                index=('x', 'y', 'z'))\n\n        s1 = Series((3, 4, 2), index=('x', 'y', 'z'))\n\n        self.assertEqual(\n            matmul(s1, f1).to_pairs(),\n            (('a', 17), ('b', 35))\n            )\n\n        # produces a Series indexed 0, 1\n        self.assertEqual(matmul(s1, f1.values).to_pairs(),\n            ((0, 17), (1, 35)))\n\n    def test_matmul_e(self) -> None:\n        # lhs: series, rhs: series\n\n        s1 = Series((3, 4, 2), index=('x', 'y', 'z'))\n\n        s2 = Series((10, 11, 12), index=('x', 'y', 'z'))\n\n        self.assertEqual(matmul(s1, s2), 98)\n        self.assertEqual(matmul(s1, s2.values), 98)\n\n\n    def test_matmul_f(self) -> None:\n        # lhs: array 1D, rhs: array 2D, Frame\n\n        f1 = Frame.from_items((('a', (1, 2, 3)), ('b', (3, 4, 5))),\n                index=('x', 'y', 'z'))\n\n        self.assertEqual(matmul([3, 4, 5], f1.values).tolist(),\n                [26, 50])\n\n        self.assertEqual(matmul([3, 4, 5], f1).to_pairs(),\n                (('a', 26), ('b', 50))\n                )\n\n\n    def test_matmul_g(self) -> None:\n        # lhs: array 1D, rhs: array 1D, Series\n\n        s1 = Series((3, 4, 2), index=('x', 'y', 'z'))\n        self.assertEqual(matmul([10, 11, 12], s1.values), 98)\n        self.assertEqual(matmul([10, 11, 12], s1), 98)\n\n        with self.assertRaises(RuntimeError):\n            self.assertEqual(matmul(s1, [10, 11]), 98)\n\n\n    def test_matmul_h(self) -> None:\n        # lhs: array 2D, rhs: array 2D, Frame\n\n        f1 = Frame.from_dict(dict(a=(1, 2, 3, 4), b=(5, 6, 7, 8)), index=tuple('wxyz'))\n        f2 = Frame.from_dict(dict(p=(1, 2), q=(3, 4), r=(5, 6)), index=tuple('ab'))\n\n\n        self.assertEqual(matmul(f1.values, f2).to_pairs(0),\n                (('p', ((0, 11), (1, 14), (2, 17), (3, 20))), ('q', ((0, 23), (1, 30), (2, 37), (3, 44))), ('r', ((0, 35), (1, 46), (2, 57), (3, 68))))\n                )\n\n        self.assertEqual(matmul(f1, f2.values).to_pairs(0),\n                ((0, (('w', 11), ('x', 14), ('y', 17), ('z', 20))), (1, (('w', 23), ('x', 30), ('y', 37), ('z', 44))), (2, (('w', 35), ('x', 46), ('y', 57), ('z', 68))))\n                )\n\n        with self.assertRaises(RuntimeError):\n            matmul(f1, np.arange(25).reshape(5, 5))\n\n\n    def test_matmul_i(self) -> None:\n        import itertools as it\n\n        f1 = Frame.from_dict(dict(a=(1, 2), b=(5, 6)), index=tuple('yz'))\n\n        f_container = lambda x: x\n        f_values = lambda x: x.values\n\n        for pair in ((f1, f1.T), (f1, f1.loc['y']), (f1['a'], f1), (f1.loc['y'], f1.loc['z'])):\n            for x, y in it.combinations((f_container, f_values, f_container, f_values), 2):\n                post = matmul(x(pair[0]), y(pair[1])) # type: ignore\n                if isinstance(post, (Series, Frame)):\n                    self.assertTrue(post.values.tolist(), (pair[0].values @ pair[1].values).tolist())\n                elif isinstance(post, np.ndarray):\n                    self.assertTrue(post.tolist(), (pair[0].values @ pair[1].values).tolist())\n\n\n\n    def test_key_to_ascending_key_a(self) -> None:\n        self.assertEqual(key_to_ascending_key([9, 5, 1], 3), [1, 5, 9])\n        self.assertEqual(key_to_ascending_key(np.array([9, 5, 1]), 3).tolist(), [1, 5, 9]) # type: ignore\n\n        self.assertEqual(key_to_ascending_key(slice(3, 0, -1), 3), slice(1, 4, 1))\n        self.assertEqual(key_to_ascending_key(100, 3), 100)\n\n        self.assertEqual(key_to_ascending_key([], 3), [])\n\n        self.assertEqual(key_to_ascending_key( # type: ignore\n                Series(('a', 'b', 'c'), index=(9, 5, 1)), 3).values.tolist(),\n                ['c', 'b', 'a'])\n\n        f1 = Frame.from_dict(dict(b=(1, 2), a=(5, 6)), index=tuple('yz'))\n        f2 = key_to_ascending_key(f1, f1.shape[1])\n        self.assertEqual(f2.columns.values.tolist(), ['a', 'b']) # type: ignore\n\n\n    def test_key_to_ascending_key_b(self) -> None:\n\n        with self.assertRaises(RuntimeError):\n            key_to_ascending_key(dict(a=3), size=3)\n\n    def test_pandas_to_numpy_a(self) -> None:\n        import pandas as pd\n        pdvu1 = pandas_version_under_1()\n\n        if not pdvu1:\n\n            s1 = pd.Series([3, 4, np.nan]).convert_dtypes()\n\n            a1 = pandas_to_numpy(s1, own_data=False)\n            self.assertEqual(a1.dtype, np.dtype('O'))\n            self.assertAlmostEqualValues(a1.tolist(), [3, 4, np.nan])\n\n            a2 = pandas_to_numpy(s1[:2], own_data=False)\n            self.assertEqual(a2.dtype, np.dtype('int64'))\n\n            s2 = pd.Series([False, True, np.nan]).convert_dtypes()\n\n            a3 = pandas_to_numpy(s2, own_data=False)\n            self.assertEqual(a3.dtype, np.dtype('O'))\n            self.assertAlmostEqualValues(a3.tolist(), [False, True, np.nan])\n\n            a4 = pandas_to_numpy(s2[:2], own_data=False)\n            self.assertEqual(a4.dtype, np.dtype('bool'))\n\n\n    def test_bloc_key_normalize_a(self) -> None:\n        f1 = Frame.from_dict(dict(b=(1, 2), a=(5, 6)), index=tuple('yz'))\n\n        with self.assertRaises(RuntimeError):\n            bloc_key_normalize(np.arange(4).reshape(2, 2), f1)\n\n        post1 = bloc_key_normalize(f1['a':] >= 5, f1) #type: ignore\n        self.assertEqual(post1.tolist(), [[False, True], [False, True]])\n\n        post2 = bloc_key_normalize(f1 < 5, f1)\n        self.assertEqual(post2.tolist(), [[True, False], [True, False]])\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/unit/test_display.py,16,"b'from itertools import combinations\nimport unittest\nimport string\nimport typing as tp\nimport hashlib\nimport json\n\nimport numpy as np\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.test.test_case import skip_win\n\nimport static_frame as sf\n# assuming located in the same directory\nfrom static_frame import Index\nfrom static_frame import IndexHierarchy\nfrom static_frame import Series\nfrom static_frame import Frame\nfrom static_frame import FrameGO\nfrom static_frame import Display\nfrom static_frame import DisplayConfig\nfrom static_frame import DisplayConfigs\nfrom static_frame import DisplayFormats\nfrom static_frame import DisplayActive\n\nfrom static_frame.core.display import DisplayFormatLaTeX\nfrom static_frame.core.display import DisplayTypeCategoryFactory\nfrom static_frame.core.display import terminal_ansi\n# from static_frame.core.display import DisplayTypeBool\nfrom static_frame.core.display import DisplayTypeInt\nfrom static_frame.test.test_case import temp_file\n\n\nnan = np.nan\n\nLONG_SAMPLE_STR = \'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\'\n\n\nclass TestUnit(TestCase):\n\n    #---------------------------------------------------------------------------\n    # display tests\n\n    def test_display_config_a(self) -> None:\n        config = DisplayConfig.from_default(type_color=False)\n        d = Display.from_values(np.array([[1, 2], [3, 4]], dtype=np.int64), \'header\', config=config)\n        self.assertEqual(d.to_rows(),\n                [\'header\', \'1 2\', \'3 4\', \'<int64>\'])\n\n    def test_display_config_b(self) -> None:\n        post = sf.DisplayConfig.from_default(cell_align_left=False)\n\n        self.assertFalse(post.cell_align_left)\n\n\n\n    def test_display_config_c(self) -> None:\n        config_right = sf.DisplayConfig.from_default(cell_align_left=False)\n        config_left = sf.DisplayConfig.from_default(cell_align_left=True)\n\n        msg = config_right.to_json()\n\n\n    def test_display_config_d(self) -> None:\n\n        with temp_file(\'.json\') as fp:\n\n            dc1 = DisplayConfig()\n            dc1.write(fp) #type: ignore\n\n            dc2 = DisplayConfig.from_file(fp) #type: ignore\n            self.assertTrue(dc1.to_dict() == dc2.to_dict())\n\n    def test_display_config_e(self) -> None:\n\n        dc1 = DisplayConfig()\n        self.assertTrue(str(dc1).startswith(\'<DisplayConfig\'))\n\n\n    #---------------------------------------------------------------------------\n\n    @skip_win  #type: ignore\n    def test_display_active_a(self) -> None:\n\n        fp1 = DisplayActive._default_fp()\n\n        with temp_file(\'.json\') as fp2:\n            DisplayActive.write(fp2) #type: ignore\n            DisplayActive.read(fp2) #type: ignore\n\n\n    #---------------------------------------------------------------------------\n\n    def test_display_cell_align_left_a(self) -> None:\n        config_right = sf.DisplayConfig.from_default(cell_align_left=False, type_color=False)\n        config_left = sf.DisplayConfig.from_default(cell_align_left=True, type_color=False)\n\n        index = Index((x for x in \'abc\'))\n\n        self.assertEqual(index.display(config=config_left).to_rows(),\n                [\'<Index>\', \'a\', \'b\', \'c\', \'<<U1>\'])\n\n        self.assertEqual(\n                index.display(config=config_right).to_rows(),\n                [\'<Index>\', \'      a\', \'      b\', \'      c\', \'  <<U1>\'])\n\n\n\n\n    @skip_win  #type: ignore\n    def test_display_cell_align_left_b(self) -> None:\n        config_right = sf.DisplayConfig.from_default(cell_align_left=False, type_color=False)\n        config_left = sf.DisplayConfig.from_default(cell_align_left=True, type_color=False)\n\n        s = Series(range(3), index=(\'a\', \'b\', \'c\'), dtype=np.int64)\n\n        self.assertEqual(s.display(config_right).to_rows(),\n                [\'<Series>\', \' <Index>\', \'       a       0\', \'       b       1\', \'       c       2\', \'   <<U1> <int64>\']\n                )\n\n        self.assertEqual(s.display(config_left).to_rows(),\n                [\'<Series>\', \'<Index>\', \'a        0\', \'b        1\', \'c        2\', \'<<U1>    <int64>\'])\n\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\'))\n\n        self.assertEqual(f1.display(config_left).to_rows(),\n                [\'<Frame>\',\n                \'<Index> p       q       r     s      t      <<U1>\',\n                \'<Index>\',\n                \'w       2       2       a     False  False\',\n                \'x       30      34      b     True   False\',\n                \'y       2       95      c     False  False\',\n                \'<<U1>   <int64> <int64> <<U1> <bool> <bool>\'])\n\n        self.assertEqual(f1.display(config_right).to_rows(),\n                [\'<Frame>\',\n                \'<Index>       p       q     r      s      t <<U1>\',\n                \'<Index>\',\n                \'      w       2       2     a  False  False\',\n                \'      x      30      34     b   True  False\',\n                \'      y       2      95     c  False  False\',\n                \'  <<U1> <int64> <int64> <<U1> <bool> <bool>\'])\n\n\n    @skip_win  # type: ignore\n    def test_display_type_show_a(self) -> None:\n        config_type = sf.DisplayConfig.from_default(\n                type_show=True,\n                type_color=False)\n        config_type_no = sf.DisplayConfig.from_default(\n                type_show=False,\n                type_color=False)\n        config_type_no_index_no = sf.DisplayConfig.from_default(\n                type_show=False,\n                type_color=False,\n                include_index=False)\n\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\'))\n\n        self.assertEqual(f1.display(config_type_no).to_rows(),\n                [\'  p  q  r s     t\',\n                 \'w 2  2  a False False\',\n                 \'x 30 34 b True  False\',\n                 \'y 2  95 c False False\'\n                 ]\n                 )\n\n        self.assertEqual(f1.display(config_type_no_index_no).to_rows(),\n                [\'p  q  r s     t\',\n                 \'2  2  a False False\',\n                 \'30 34 b True  False\',\n                 \'2  95 c False False\'\n                 ]\n                 )\n\n\n        self.assertEqual(f1.display(config_type).to_rows(),\n                [\'<Frame>\',\n                \'<Index> p       q       r     s      t      <<U1>\',\n                \'<Index>\',\n                \'w       2       2       a     False  False\',\n                \'x       30      34      b     True   False\',\n                \'y       2       95      c     False  False\',\n                \'<<U1>   <int64> <int64> <<U1> <bool> <bool>\'])\n\n\n    @skip_win  # type: ignore\n    def test_display_type_show_b(self) -> None:\n        config_type = sf.DisplayConfig.from_default(\n                type_show=True,\n                type_color=False)\n        config_type_no = sf.DisplayConfig.from_default(\n                type_show=False,\n                type_color=False)\n        config_type_no_index_no = sf.DisplayConfig.from_default(\n                type_show=False,\n                type_color=False,\n                include_index=False)\n\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (300, -4, \'x\', False, False),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=IndexHierarchy.from_labels(((\'A\', 1, 2), (\'A\', 1, 3), (\'B\', 1, 2), (\'B\', 1, 3), )))\n\n        self.assertEqual(\n                f1.display(config_type_no).to_rows(),\n                [\'      p   q  r s     t\',\n                \'A 1 2 2   2  a False False\',\n                \'A 1 3 30  34 b True  False\',\n                \'B 1 2 2   95 c False False\',\n                \'B 1 3 300 -4 x False False\']\n                 )\n\n        self.assertEqual(\n                f1.display(config_type_no_index_no).to_rows(),\n                [\'p   q  r s     t\', \'2   2  a False False\', \'30  34 b True  False\', \'2   95 c False False\', \'300 -4 x False False\']\n                )\n\n    @skip_win #type: ignore\n    def test_display_cell_fill_width_a(self) -> None:\n\n        config_width_12 = sf.DisplayConfig.from_default(cell_max_width=12, cell_max_width_leftmost=12, type_color=False)\n        config_width_6 = sf.DisplayConfig.from_default(cell_max_width=6, cell_max_width_leftmost=6, type_color=False)\n\n        def chunks(size: int, count: int) -> tp.Iterator[str]:\n            pos = 0\n            for _ in range(count):\n                yield LONG_SAMPLE_STR[pos: pos + size]\n                pos = pos + size\n\n        s = Series(chunks(20, 3), index=(\'a\', \'b\', \'c\'))\n\n\n        self.assertEqual(s.display(config=config_width_12).to_rows(),\n                [\n                \'<Series>\',\n                \'<Index>\',\n                \'a        Lorem ips...\',\n                \'b        t amet, c...\',\n                \'c        adipiscin...\',\n                \'<<U1>    <<U20>\'])\n\n        self.assertEqual(s.display(config=config_width_6).to_rows(),\n                [\n                \'<Se...\',\n                \'<In...\',\n                \'a      Lor...\',\n                \'b      t a...\',\n                \'c      adi...\',\n                \'<<U1>  <<U20>\']\n                )\n\n        config = sf.DisplayConfig.from_default(type_color=False, cell_max_width_leftmost=20)\n\n        row_count = 2\n        index = [str(chr(x)) for x in range(97, 97+row_count)]\n        f = FrameGO(index=index)\n\n        for i in range(4):\n            chunker = iter(chunks(10, row_count))\n            s = Series((x for x in chunker), index=index)\n            f[i] = s\n\n        f.columns._update_array_cache()\n\n        self.assertEqual(f.display(config=config).to_rows(),\n                [\'<FrameGO>\',\n                \'<IndexGO> 0          1          2          3          <int64>\',\n                \'<Index>\',\n                \'a         Lorem ipsu Lorem ipsu Lorem ipsu Lorem ipsu\',\n                \'b         m dolor si m dolor si m dolor si m dolor si\',\n                \'<<U1>     <<U10>     <<U10>     <<U10>     <<U10>\'])\n\n        self.assertEqual(f.display(config=config_width_6).to_rows(),\n                [\'<Fr...\',\n                \'<In... 0      1      2      3      <in...\',\n                \'<In...\',\n                \'a      Lor... Lor... Lor... Lor...\',\n                \'b      m d... m d... m d... m d...\',\n                \'<<U1>  <<U10> <<U10> <<U10> <<U10>\']\n                )\n\n\n    def test_display_display_rows_a(self) -> None:\n\n        config_rows_12 = sf.DisplayConfig.from_default(display_rows=12, type_color=False)\n        config_rows_7 = sf.DisplayConfig.from_default(display_rows=7, type_color=False)\n\n        index = list(\'\'.join(x) for x in combinations(string.ascii_lowercase, 2))\n        s = Series(range(len(index)), index=index, dtype=np.int64)\n\n        # import ipdb; ipdb.set_trace()\n        self.assertEqual(s.display(config_rows_12).to_rows(),\n                [\n                \'<Series>\',\n                \'<Index>\',\n                \'ab       0\',\n                \'ac       1\',\n                \'ad       2\',\n                \'ae       3\',\n                \'af       4\',\n                \'...      ...\',\n                \'wy       320\',\n                \'wz       321\',\n                \'xy       322\',\n                \'xz       323\',\n                \'yz       324\',\n                \'<<U2>    <int64>\'])\n\n        self.assertEqual(s.display(config_rows_7).to_rows(),\n                [\n                \'<Series>\',\n                \'<Index>\',\n                \'ab       0\',\n                \'ac       1\',\n                \'ad       2\',\n                \'...      ...\',\n                \'xy       322\',\n                \'xz       323\',\n                \'yz       324\',\n                \'<<U2>    <int64>\'])\n\n\n    def test_display_rows_b(self) -> None:\n        # this isseu was found only with Frame, not with Series\n        dc = DisplayConfig(display_rows=8, type_color=False)\n        self.assertEqual(Frame(np.arange(7, dtype=np.int64)).display(dc).to_rows(),\n                [\'<Frame>\',\n                \'<Index> 0       <int64>\',\n                \'<Index>\',\n                \'0       0\',\n                \'1       1\',\n                \'2       2\',\n                \'3       3\',\n                \'4       4\',\n                \'5       5\',\n                \'6       6\',\n                \'<int64> <int64>\']\n                )\n\n        self.assertEqual(Frame(np.arange(8, dtype=np.int64)).display(dc).to_rows(),\n                [\'<Frame>\',\n                \'<Index> 0       <int64>\',\n                \'<Index>\',\n                \'0       0\',\n                \'1       1\',\n                \'2       2\',\n                \'3       3\',\n                \'4       4\',\n                \'5       5\',\n                \'6       6\',\n                \'7       7\',\n                \'<int64> <int64>\']\n                )\n\n        self.assertEqual(Frame(np.arange(9, dtype=np.int64)).display(dc).to_rows(),\n                [\'<Frame>\',\n                \'<Index> 0       <int64>\',\n                \'<Index>\',\n                \'0       0\',\n                \'1       1\',\n                \'2       2\',\n                \'...     ...\',\n                \'6       6\',\n                \'7       7\',\n                \'8       8\',\n                \'<int64> <int64>\']\n                )\n\n\n    @skip_win  # type: ignore\n    def test_display_display_columns_a(self) -> None:\n\n        config_columns_8 = sf.DisplayConfig.from_default(display_columns=8, type_color=False)\n        config_columns_5 = sf.DisplayConfig.from_default(display_columns=5, type_color=False)\n\n        columns = list(\'\'.join(x) for x in combinations(string.ascii_lowercase, 2))\n        f = FrameGO(index=range(4))\n        for i, col in enumerate(columns):\n            f[col] = Series.from_element(i, index=range(4))\n\n        self.assertEqual(\n                f.display(config_columns_8).to_rows(),\n                [\'<FrameGO>\',\n                \'<IndexGO> ab      ac      ad      ... xy      xz      yz      <<U2>\',\n                \'<Index>                           ...\',\n                \'0         0       1       2       ... 322     323     324\',\n                \'1         0       1       2       ... 322     323     324\',\n                \'2         0       1       2       ... 322     323     324\',\n                \'3         0       1       2       ... 322     323     324\',\n                \'<int64>   <int64> <int64> <int64> ... <int64> <int64> <int64>\']\n                )\n\n        self.assertEqual(\n                f.display(config_columns_5).to_rows(),\n                [\'<FrameGO>\',\n                \'<IndexGO> ab      ac      ... xz      yz      <<U2>\',\n                \'<Index>                   ...\',\n                \'0         0       1       ... 323     324\',\n                \'1         0       1       ... 323     324\',\n                \'2         0       1       ... 323     324\',\n                \'3         0       1       ... 323     324\',\n                \'<int64>   <int64> <int64> ... <int64> <int64>\'])\n\n\n    @skip_win  # type: ignore\n    def test_display_display_columns_b(self) -> None:\n\n        config_columns_4 = sf.DisplayConfig.from_default(display_columns=4, type_color=False)\n        config_columns_5 = sf.DisplayConfig.from_default(display_columns=5, type_color=False)\n\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                )\n\n        f = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\'))\n\n        self.assertEqual(\n                f.display(config_columns_4).to_rows(),\n                [\'<Frame>\',\n                \'<Index> p       ... t      <<U1>\',\n                \'<Index>         ...\',\n                \'w       2       ... False\',\n                \'x       30      ... False\',\n                \'y       2       ... False\',\n                \'<<U1>   <int64> ... <bool>\'])\n\n        # at one point the columns woiuld be truncated shorter than the frame when the max xolumns was the same\n        self.assertEqual(\n                f.display(config_columns_5).to_rows(),\n                [\'<Frame>\',\n                \'<Index> p       q       r     s      t      <<U1>\',\n                \'<Index>\',\n                \'w       2       2       a     False  False\',\n                \'x       30      34      b     True   False\',\n                \'y       2       95      c     False  False\',\n                \'<<U1>   <int64> <int64> <<U1> <bool> <bool>\']\n                )\n\n\n    def test_display_truncate_a(self) -> None:\n\n        config_rows_12_cols_8 = sf.DisplayConfig.from_default(display_rows=12, display_columns=8)\n        config_rows_7_cols_5 = sf.DisplayConfig.from_default(display_rows=7, display_columns=5)\n\n\n        size = 10000\n        columns = 100\n        a1 = (np.arange(size * columns)).reshape((size, columns)) * .001\n        # insert random nan in very other columns\n        for col in range(0, 100, 2):\n            a1[:100, col] = np.nan\n\n        index = (hashlib.sha224(str(x).encode(\'utf-8\')).hexdigest()\n                for x in range(size))\n        cols = (hashlib.sha224(str(x).encode(\'utf-8\')).hexdigest()\n                for x in range(columns))\n\n        f = Frame(a1, index=index, columns=cols)\n\n        self.assertEqual(\n                len(tuple(f.display(config_rows_12_cols_8).to_rows())), 15)\n\n        self.assertEqual(\n                len(tuple(f.display(config_rows_7_cols_5).to_rows())), 11)\n\n\n    def test_display_type_attributes_a(self) -> None:\n\n        x, z = Display.type_attributes(np.dtype(\'int8\'), DisplayConfigs.DEFAULT)\n        self.assertEqual(x, \'<int8>\')\n\n    def test_display_type_attributes_b(self) -> None:\n\n        with self.assertRaises(NotImplementedError):\n            x, z = Display.type_attributes(None, DisplayConfigs.DEFAULT)\n\n\n    def test_display_type_category_a(self) -> None:\n\n        x = DisplayTypeCategoryFactory.to_category(np.dtype(int))\n        self.assertEqual(x.__name__, \'DisplayTypeInt\')\n\n        x = DisplayTypeCategoryFactory.to_category(np.dtype(object))\n        self.assertEqual(x.__name__, \'DisplayTypeObject\')\n\n    def test_display_type_category_b(self) -> None:\n        # force getting the default\n        x = DisplayTypeCategoryFactory.to_category(None)\n        self.assertEqual(x.__name__, \'DisplayTypeCategory\')\n        self.assertTrue(x.in_category(None))\n\n    def test_display_config_from_json_a(self) -> None:\n        json_data = json.dumps(dict(type_show=False))\n        dc = DisplayConfig.from_json(json_data)\n        self.assertEqual(dc.type_show, False)\n\n        # with a bad name, we filter out the key\n        json_data = json.dumps(dict(show=False))\n        dc = DisplayConfig.from_json(json_data)\n        self.assertEqual(dc.type_show, True)\n\n    def test_display_flatten_a(self) -> None:\n        config = DisplayConfig.from_default(type_color=False)\n\n        d1 = Display.from_values(np.array([1, 2, 3, 4], dtype=np.int64), \'header\', config=config)\n        self.assertEqual(d1.flatten().to_rows(), [\'header 1 2 3 4 <int64>\'])\n\n\n        d2 = Display.from_values(np.array([5, 6, 7, 8], dtype=np.int64), \'header\', config=config)\n\n        # mutates in place\n        d1.extend_display(d2)\n        self.assertEqual(\n                d1.to_rows(),\n                [\'header  header\', \'1       5\', \'2       6\', \'3       7\', \'4       8\', \'<int64> <int64>\'])\n\n        self.assertEqual(d1.flatten().to_rows(),\n                [\'header header 1 5 2 6 3 7 4 8 <int64> <int64>\'])\n\n        self.assertEqual(d1.transform().to_rows(),\n                [\'header 1 2 3 4 <int64>\', \'header 5 6 7 8 <int64>\'])\n\n\n    @skip_win  # type: ignore\n    def test_display_html_pre_a(self) -> None:\n        f = Frame.from_dict(dict(a=(1, 2),\n                b=(1.2, 3.4),\n                c=(False, True)))\n\n        expected = f.display(sf.DisplayConfig(\n                display_format=\'html_pre\', type_color=False))\n\n        html = \'\'\'\n<div style=""white-space: pre; font-family: monospace"">&lt;Frame&gt;\n&lt;Index&gt; a       b         c      &lt;&lt;U1&gt;\n&lt;Index&gt;\n0       1       1.2       False\n1       2       3.4       True\n&lt;int64&gt; &lt;int64&gt; &lt;float64&gt; &lt;bool&gt;</div>\'\'\'\n\n        self.assertEqualLines(html, str(expected))\n\n\n    @skip_win  # type: ignore\n    def test_display_html_table_a(self) -> None:\n        f = sf.Frame.from_dict(\n            dict(a=(1,2,3,4), b=(True, False, True, False), c=list(\'qrst\')))\n        f = f.set_index_hierarchy([\'a\', \'b\'])\n        f = f.relabel_add_level(columns=\'I\')\n        f = f.relabel_add_level(columns=\'J\')\n\n        expected1 = f.display(sf.DisplayConfig(\n                display_format=\'html_table\', type_color=False))\n\n        html1 = \'\'\'<table border=""1""><thead><tr><th>&lt;Frame&gt;</th><th></th><th></th><th></th><th></th><th></th></tr><tr><th>&lt;IndexHierarchy&gt;</th><th></th><th>J</th><th>J</th><th>J</th><th>&lt;&lt;U1&gt;</th></tr><tr><th></th><th></th><th>I</th><th>I</th><th>I</th><th>&lt;&lt;U1&gt;</th></tr><tr><th></th><th></th><th>a</th><th>b</th><th>c</th><th>&lt;&lt;U1&gt;</th></tr><tr><th>&lt;IndexHierarchy: (&#x27;a&#x27;, &#x27;b&#x27;)&gt;</th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th>1</th><th>True</th><td>1</td><td>True</td><td>q</td><td></td></tr><tr><th>2</th><th>False</th><td>2</td><td>False</td><td>r</td><td></td></tr><tr><th>3</th><th>True</th><td>3</td><td>True</td><td>s</td><td></td></tr><tr><th>4</th><th>False</th><td>4</td><td>False</td><td>t</td><td></td></tr><tr><th>&lt;int64&gt;</th><th>&lt;bool&gt;</th><td>&lt;int64&gt;</td><td>&lt;bool&gt;</td><td>&lt;&lt;U1&gt;</td><td></td></tr></tbody></table>\n        \'\'\'\n        self.assertEqual(html1.strip(), str(expected1).strip())\n\n        expected2 = f.display(sf.DisplayConfig(\n                display_format=\'html_table\', type_color=False, type_show=False))\n\n        html2 = \'\'\'<table border=""1""><thead><tr><th></th><th></th><th>J</th><th>J</th><th>J</th></tr><tr><th></th><th></th><th>I</th><th>I</th><th>I</th></tr><tr><th></th><th></th><th>a</th><th>b</th><th>c</th></tr></thead><tbody><tr><th>1</th><th>True</th><td>1</td><td>True</td><td>q</td></tr><tr><th>2</th><th>False</th><td>2</td><td>False</td><td>r</td></tr><tr><th>3</th><th>True</th><td>3</td><td>True</td><td>s</td></tr><tr><th>4</th><th>False</th><td>4</td><td>False</td><td>t</td></tr></tbody></table>\n        \'\'\'\n        self.assertEqual(html2.strip(), str(expected2).strip())\n\n        expected3 = f.display(sf.DisplayConfig(\n                display_format=\'html_table\', type_color=False, type_show=False, include_index=False))\n\n        html3 = \'\'\'<table border=""1""><thead><tr><th>J</th><th>J</th><th>J</th></tr><tr><th>I</th><th>I</th><th>I</th></tr><tr><th>a</th><th>b</th><th>c</th></tr></thead><tbody><tr><td>1</td><td>True</td><td>q</td></tr><tr><td>2</td><td>False</td><td>r</td></tr><tr><td>3</td><td>True</td><td>s</td></tr><tr><td>4</td><td>False</td><td>t</td></tr></tbody></table>\n        \'\'\'\n        self.assertEqual(html3.strip(), str(expected3).strip())\n\n\n        expected4 = f.display(sf.DisplayConfig(\n                display_format=\'html_table\', type_color=False, type_show=False, include_columns=False))\n\n        html4 = \'\'\'<table border=""1""><tbody><tr><th>1</th><th>True</th><td>1</td><td>True</td><td>q</td></tr><tr><th>2</th><th>False</th><td>2</td><td>False</td><td>r</td></tr><tr><th>3</th><th>True</th><td>3</td><td>True</td><td>s</td></tr><tr><th>4</th><th>False</th><td>4</td><td>False</td><td>t</td></tr></tbody></table>\n        \'\'\'\n        self.assertEqual(html4.strip(), str(expected4).strip())\n\n\n        expected5 = f.display(sf.DisplayConfig(\n                display_format=\'html_table\',\n                type_color=False,\n                type_show=False,\n                include_index=False,\n                include_columns=False))\n\n        html5 = \'\'\'<table border=""1""><tbody><tr><td>1</td><td>True</td><td>q</td></tr><tr><td>2</td><td>False</td><td>r</td></tr><tr><td>3</td><td>True</td><td>s</td></tr><tr><td>4</td><td>False</td><td>t</td></tr></tbody></table>\n        \'\'\'\n        self.assertEqual(html5.strip(), str(expected5).strip())\n\n\n\n\n    def test_display_html_table_b(self) -> None:\n        records = (\n                (2, \'a\', False),\n                (30, \'b\', False),\n                )\n        f = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\'))\n        self.assertEqual(f._repr_html_(),\n            \'<table border=""1""><thead><tr><th></th><th>p</th><th>q</th><th>r</th></tr></thead><tbody><tr><th>w</th><td>2</td><td>a</td><td>False</td></tr><tr><th>x</th><td>30</td><td>b</td><td>False</td></tr></tbody></table>\')\n\n\n    def test_display_html_series_a(self) -> None:\n        records = (\n                (2, \'a\', False),\n                (30, \'b\', False),\n                )\n        f = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\'))\n\n        self.assertEqual(f[\'q\']._repr_html_(),\n            \'<table border=""1""><tbody><tr><th>w</th><td>a</td></tr><tr><th>x</th><td>b</td></tr></tbody></table>\')\n\n    def test_display_html_index_a(self) -> None:\n        records = (\n                (2, \'a\', False),\n                (30, \'b\', False),\n                )\n        f = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\'))\n\n        self.assertEqual(f.index._repr_html_(),\n                \'<table border=""1""><tbody><tr><td>w</td></tr><tr><td>x</td></tr></tbody></table>\')\n\n        f1 = f.set_index_hierarchy((\'p\', \'q\'))\n        self.assertEqual(f1.index._repr_html_(),\n                \'<table border=""1""><tbody><tr><td>2</td><td>a</td></tr><tr><td>30</td><td>b</td></tr></tbody></table>\'\n                )\n\n    def test_display_html_index_b(self) -> None:\n        records = (\n                (2, \'a\', False),\n                (30, \'b\', False),\n                )\n        f = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\'))\n        f = f.set_index_hierarchy((\'p\', \'q\'))\n\n        # this uses cell width normaliz\n        post = f.display(sf.DisplayConfig(\n                display_format=DisplayFormats.HTML_PRE,\n                cell_max_width_leftmost=20)).to_rows()\n\n        self.assertEqual(post[2],\n                \'<span style=""color: #777777"">&lt;IndexHierarchy: ...</span>\')\n\n        post = f.display(sf.DisplayConfig(\n                display_format=DisplayFormats.HTML_PRE,\n                cell_max_width_leftmost=36)).to_rows()\n\n        self.assertEqual(post[2],\n                \'<span style=""color: #777777"">&lt;IndexHierarchy: (&#x27;p&#x27;, &#x27;q&#x27;)&gt;</span>\')\n\n\n\n    def test_display_max_width_a(self) -> None:\n        records = (\n                (2, \'a\', False),\n                (30, \'b\', False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\'))\n        f2 = f1.set_index_hierarchy((\'p\', \'q\'))\n\n        post = f2.display(sf.DisplayConfig(type_color=False, cell_max_width_leftmost=20)).to_rows()\n        self.assertEqual(post[2], \'<IndexHierarchy: ...\')\n\n        post = f2.display(sf.DisplayConfig(type_color=False, cell_max_width_leftmost=30)).to_rows()\n        self.assertEqual(post[2], ""<IndexHierarchy: (\'p\', \'q\')>"")\n\n    def test_display_float_scientific_a(self) -> None:\n\n        s1 = sf.Series([3.1, 5.2]) ** 40\n\n        self.assertEqual(\n                s1.display(sf.DisplayConfig(type_color=False)).to_rows(),\n                [\'<Series>\',\n                \'<Index>\',\n                \'0        4.51302515e+19\',\n                \'1        4.36650282e+28\',\n                \'<int64>  <float64>\']\n                )\n\n        # non default config for scientifici will truncate values\n        self.assertEqual(\n                s1.display(sf.DisplayConfig(type_color=False, value_format_float_scientific=\'{:f}\')).to_rows(),                          [\'<Series>\',\n                \'<Index>\',\n                \'0        45130251461102272...\',\n                \'1        43665028242109266...\',\n                \'<int64>  <float64>\']\n                )\n\n        self.assertEqual(\n                s1.display(sf.DisplayConfig(type_color=False, value_format_float_scientific=\'{:.2e}\')).to_rows(),\n                [\'<Series>\',\n                \'<Index>\',\n                \'0        4.51e+19\',\n                \'1        4.37e+28\',\n                \'<int64>  <float64>\']\n                )\n\n\n\n    def test_display_float_scientific_b(self) -> None:\n\n        s1 = sf.Series([3.1j, 5.2j]) ** 40\n\n        self.assertEqual(\n                s1.display(sf.DisplayConfig(type_color=False)).to_rows(),\n                [\'<Series>\',\n                \'<Index>\',\n                \'0        4.51e+19+0.00e+00j\',\n                \'1        4.37e+28+0.00e+00j\',\n                \'<int64>  <complex128>\']\n                )\n\n        # non default config for scientifici will truncate values\n        self.assertEqual(\n                s1.display(sf.DisplayConfig(type_color=False, value_format_complex_scientific=\'{:f}\')).to_rows(),                          [\'<Series>\',\n                \'<Index>\',\n                \'0        45130251461102338...\',\n                \'1        43665028242109283...\',\n                \'<int64>  <complex128>\']\n                )\n\n        self.assertEqual(\n                s1.display(sf.DisplayConfig(type_color=False, value_format_complex_scientific=\'{:.1e}\')).to_rows(),\n                [\'<Series>\',\n                \'<Index>\',\n                \'0        4.5e+19+0.0e+00j\',\n                \'1        4.4e+28+0.0e+00j\',\n                \'<int64>  <complex128>\']\n                )\n\n\n    def test_display_tall(self) -> None:\n        f = Frame.from_element(None, index=range(40), columns=range(20))\n        self.assertEqual(len(f.display_tall().to_rows()), 44)\n        self.assertEqual(len(f.display_wide().to_rows()), 39)\n\n    #---------------------------------------------------------------------------\n    def test_display_format_latex_a(self) -> None:\n\n        post = DisplayFormatLaTeX.markup_outermost(\'x\', identifier=\'foo\')\n        self.assertEqual(post,\n            \'\'\'\\\\begin{table}[ht]\\n\\\\centering\\nx\\n\\\\label{table:foo}\\n\\\\end{table}\'\'\'\n            )\n\n\n    @skip_win #type: ignore\n    def test_display_type_color_markup_a(self) -> None:\n\n        config1 = DisplayConfig(display_format=DisplayFormats.TERMINAL)\n        post1 = Display.type_color_markup(DisplayTypeInt, config1)\n\n        if terminal_ansi():\n            self.assertEqual(post1, \'\\x1b[38;5;239m{}\\x1b[0m\')\n        else:\n            self.assertEqual(post1, \'{}\')\n\n        config2 = DisplayConfig(display_format=DisplayFormats.HTML_TABLE)\n        post2 = Display.type_color_markup(DisplayTypeInt, config2)\n        self.assertEqual(post2, \'<span style=""color: #505050"">{}</span>\')\n\n\n\n    def test_display_include_index_a(self) -> None:\n\n        f1 = Frame.from_element(\'b\', index=range(3), columns=range(2))\n\n        with self.assertRaises(RuntimeError):\n            DisplayConfig(include_index=False, type_show=True)\n\n\n        config1 = DisplayConfig(include_index=False, type_show=False)\n        d1 = f1.display(config1)\n        self.assertEqual(d1.to_rows(),\n                [\'0 1\', \'b b\', \'b b\', \'b b\']\n                )\n\n        config2 = DisplayConfig(include_index=False,\n                type_show=False,\n                include_columns=False)\n        d2 = f1.display(config2)\n        self.assertEqual(d2.to_rows(),\n                [\'b b\', \'b b\', \'b b\']\n                )\n\n\n\n\n    #---------------------------------------------------------------------------\n\n    @unittest.skip(\'too colorful\')\n    def test_display_type_color_a(self) -> None:\n\n        f = sf.Frame.from_dict(dict(a=(1, 2),\n                b=(1.2, 3.4),\n                c=(False, True),\n                e=(1j, 3j),\n                f=(np.datetime64(\'2014\'), np.datetime64(\'2015\')),\n                ),\n                index=tuple(\'xy\'))\n        print(f)\n        print(f.loc[\'x\'])\n\n        sf.DisplayActive.set(sf.DisplayConfigs.COLOR)\n\n        print(f.display(sf.DisplayConfigs.COLOR))\n        print(f.loc[\'x\'].display(sf.DisplayConfigs.COLOR))\n\n        f = sf.Frame.from_dict(dict(a=(1,2,3,4), b=(True, False, True, False), c=list(\'qrst\')))\n        f = f.set_index_hierarchy([\'a\', \'b\'])\n        f = f.relabel_add_level(columns=\'I\')\n        f = f.relabel_add_level(columns=\'J\')\n        print(f)\n\n        # columns = sf.IndexHierarchy.from_product((96361, 96345), (0, 1))\n        # index = sf.IndexHierarchy.from_product((32155, 32175), (0, 4))\n        # columns = range(4)\n        # index = range(4)\n        # f = sf.Frame.from_records(\n        #     ([y for y in range(x, x + 4)] for x in range(4)),\n        #     index=index, columns=columns)\n\n        from itertools import product\n        index: tp.Iterable[tp.Hashable] = (0x2210, 0x2330)\n        columns: tp.Iterable[tp.Hashable] = (0x1, 0xe)\n        f = sf.Frame.from_element_loc_items(\n                ((x, chr(sum(x))) for x in product(index, columns)),  # type: ignore  # Should probably open a typeshed issue for this.\n                index=index,\n                columns=columns,\n                dtype=str)\n        print(f)\n\n\n        columns = list(\'abcdefgh\')\n        index = range(1, 9)\n\n        f = sf.Frame(np.empty((8, 8), dtype=\'U1\'), columns=columns, index=index)\n        print(f)\n\n        columns = tuple(\'efgh\')\n        index = range(3, 0, -1)\n\n        f = sf.Frame.from_element_loc_items(\n                (\n                ((2, \'f\'), chr(0x265F)), # pawn\n                ((2, \'g\'), chr(0x265F)),\n                ((2, \'h\'), chr(0x265F)),\n                ((1, \'e\'), chr(0x265A)), # king\n                ((1, \'h\'), chr(0x265C)), # rook\n                ),\n                index=index,\n                columns=columns,\n                dtype=str)\n\n        #part of Sicilian Defense Najdorf Variation\n        columns = tuple(\'hgfe\')\n        index = range(6, 9)\n\n        f = Frame.from_element_loc_items(\n                (\n                ((7, \'h\'), chr(0x265F)), # pawn\n                ((6, \'g\'), chr(0x265F)),\n                ((7, \'f\'), chr(0x265F)),\n                ((7, \'e\'), chr(0x265F)),\n                ((8, \'e\'), chr(0x265A)), # king\n                ((7, \'g\'), chr(0x265D)), # biship\n                ((6, \'f\'), chr(0x265E)), # horse\n                ((8, \'h\'), chr(0x265C)), # rook\n                ),\n                index=index,\n                columns=columns,\n                dtype=str)\n\n        s1 = Series.from_items(((\'f\', chr(0x265C)), (\'g\', chr(0x265A))))\n\n        f.assign.loc[8, :](s1, fill_value=\'\')\n\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
static_frame/test/unit/test_display_color.py,0,"b'\n\nfrom static_frame.test.test_case import TestCase\n\n# assuming located in the same directory\n# from static_frame import Index\n# from static_frame import IndexGO\n# from static_frame import Series\n# from static_frame import Frame\n# from static_frame import FrameGO\n# from static_frame import TypeBlocks\n# from static_frame import Display\n# from static_frame import mloc\n# from static_frame import DisplayConfig\n# from static_frame import DisplayConfigs\n\nfrom static_frame.core.display_color import HexColor\n\nclass TestUnit(TestCase):\n\n    def test_hex_str_to_int_a(self) -> None:\n        post = HexColor._hex_str_to_int(\'aqua\')\n        self.assertEqual(post, 65535)\n\n    def test_format_html_a(self) -> None:\n        post = HexColor.format_html(\'aqua\', \'test\')\n        self.assertEqual(post, \'<span style=""color: #ffff"">test</span>\')\n\n    def test_hex_color_format_a(self) -> None:\n\n        msg = HexColor.format_terminal(\'#4b006e\', \'test\')\n        self.assertEqual(msg, \'\\x1b[38;5;53mtest\\x1b[0m\')\n\n        msg = HexColor.format_terminal(0xaaaaaa, \'test\')\n        self.assertEqual(msg, \'\\x1b[38;5;248mtest\\x1b[0m\')\n\n        msg = HexColor.format_terminal(\'0xaaaaaa\', \'test\')\n        self.assertEqual(msg, \'\\x1b[38;5;248mtest\\x1b[0m\')\n\n        msg = HexColor.format_terminal(\'#040273\', \'test\')\n        self.assertEqual(msg, \'\\x1b[38;5;4mtest\\x1b[0m\')\n\n\n\nif __name__ == \'__main__\':\n    import unittest\n    unittest.main()\n'"
static_frame/test/unit/test_doc.py,68,"b'import doctest\nimport os\nimport typing as tp\n\napi_example_str = \'\'\'\n\n\n#-------------------------------------------------------------------------------\n# import and setup\n\n>>> import static_frame as sf\n>>> _display_config_active = sf.DisplayActive.get()\n>>> sf.DisplayActive.set(sf.DisplayConfig(type_color=False))\n>>> import numpy as np\n>>> import static_frame as sf\n\n#-------------------------------------------------------------------------------\n# documentation introduction\n\n#start_immutability\n\n>>> import static_frame as sf\n>>> import numpy as np\n\n>>> s = sf.Series((67, 62, 27, 14), index=(\'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'), dtype=np.int64)\n>>> s #doctest: +NORMALIZE_WHITESPACE\n<Series>\n<Index>\nJupiter  67\nSaturn   62\nUranus   27\nNeptune  14\n<<U7>    <int64>\n>>> s[\'Jupiter\'] = 68\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nTypeError: \'Series\' object does not support item assignment\n>>> s.iloc[0] = 68\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nTypeError: \'InterfaceGetItem\' object does not support item assignment\n>>> s.values[0] = 68\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nValueError: assignment destination is read-only\n\n#end_immutability\n\n#start_assign\n>>> s.assign[\'Jupiter\'](69) #doctest: +NORMALIZE_WHITESPACE\n<Series>\n<Index>\nJupiter  69\nSaturn   62\nUranus   27\nNeptune  14\n<<U7>    <int64>\n>>> s.assign[\'Uranus\':](s[\'Uranus\':] - 2) #doctest: +NORMALIZE_WHITESPACE\n<Series>\n<Index>\nJupiter  67\nSaturn   62\nUranus   25\nNeptune  12\n<<U7>   <int64>\n>>> s.assign.iloc[[0, 3]]((68, 11)) #doctest: +NORMALIZE_WHITESPACE\n<Series>\n<Index>\nJupiter  68\nSaturn   62\nUranus   27\nNeptune  11\n<<U7>    <int64>\n\n#end_assign\n\n\n#-------------------------------------------------------------------------------\n# series\n\n#start_Series-via_dt.year\n>>> s = sf.Series.from_dict({\'Halley\': \'1986-02-09\', \'Encke\': \'2003-12-28\', ""d\'Arrest"": \'2008-08-01\', \'Tempel 1\': \'2005-07-05\'}, name=\'Perihelion Date\', dtype=np.datetime64)\n\n>>> s\n<Series: Perihelion Date>\n<Index>\nHalley                    1986-02-09\nEncke                     2003-12-28\nd\'Arrest                  2008-08-01\nTempel 1                  2005-07-05\n<<U8>                     <datetime64[D]>\n>>> s.via_dt.year\n<Series: Perihelion Date>\n<Index>\nHalley                    1986\nEncke                     2003\nd\'Arrest                  2008\nTempel 1                  2005\n<<U8>                     <int64>\n\n#end_Series-via_dt.year\n\n\n#start_Series-via_dt.month\n>>> s = sf.Series.from_dict({\'Halley\': \'1986-02-09\', \'Encke\': \'2003-12-28\', ""d\'Arrest"": \'2008-08-01\', \'Tempel 1\': \'2005-07-05\'}, name=\'Perihelion Date\', dtype=np.datetime64)\n\n>>> s\n<Series: Perihelion Date>\n<Index>\nHalley                    1986-02-09\nEncke                     2003-12-28\nd\'Arrest                  2008-08-01\nTempel 1                  2005-07-05\n<<U8>                     <datetime64[D]>\n>>> s.via_dt.month\n<Series: Perihelion Date>\n<Index>\nHalley                    2\nEncke                     12\nd\'Arrest                  8\nTempel 1                  7\n<<U8>                     <int64>\n\n#end_Series-via_dt.month\n\n\n#start_Series-via_dt.day\n>>> s = sf.Series.from_dict({\'Halley\': \'1986-02-09\', \'Encke\': \'2003-12-28\', ""d\'Arrest"": \'2008-08-01\', \'Tempel 1\': \'2005-07-05\'}, name=\'Perihelion Date\', dtype=np.datetime64)\n\n>>> s\n<Series: Perihelion Date>\n<Index>\nHalley                    1986-02-09\nEncke                     2003-12-28\nd\'Arrest                  2008-08-01\nTempel 1                  2005-07-05\n<<U8>                     <datetime64[D]>\n>>> s.via_dt.day\n<Series: Perihelion Date>\n<Index>\nHalley                    9\nEncke                     28\nd\'Arrest                  1\nTempel 1                  5\n<<U8>                     <int64>\n\n#end_Series-via_dt.day\n\n#start_Series-via_dt.weekday()\n>>> s = sf.Series.from_dict({\'Halley\': \'1986-02-09\', \'Encke\': \'2003-12-28\', ""d\'Arrest"": \'2008-08-01\', \'Tempel 1\': \'2005-07-05\'}, name=\'Perihelion Date\', dtype=np.datetime64)\n>>> s\n<Series: Perihelion Date>\n<Index>\nHalley                    1986-02-09\nEncke                     2003-12-28\nd\'Arrest                  2008-08-01\nTempel 1                  2005-07-05\n<<U8>                     <datetime64[D]>\n>>> s.via_dt.weekday()\n<Series: Perihelion Date>\n<Index>\nHalley                    6\nEncke                     6\nd\'Arrest                  4\nTempel 1                  1\n<<U8>                     <int64>\n\n#end_Series-via_dt.weekday()\n\n\n\n#start_Series-via_dt.isoformat()\n>>> s = sf.Series.from_dict({\'Halley\': \'1986-02-09\', \'Encke\': \'2003-12-28\', ""d\'Arrest"": \'2008-08-01\', \'Tempel 1\': \'2005-07-05\'}, name=\'Perihelion Date\', dtype=np.datetime64)\n>>> s\n<Series: Perihelion Date>\n<Index>\nHalley                    1986-02-09\nEncke                     2003-12-28\nd\'Arrest                  2008-08-01\nTempel 1                  2005-07-05\n<<U8>                     <datetime64[D]>\n>>> s.via_dt.isoformat()\n<Series: Perihelion Date>\n<Index>\nHalley                    1986-02-09\nEncke                     2003-12-28\nd\'Arrest                  2008-08-01\nTempel 1                  2005-07-05\n<<U8>                     <<U10>\n\n#end_Series-via_dt.isoformat()\n\n\n#start_Series-via_dt.strftime()\n>>> s = sf.Series.from_dict({\'Halley\': \'1986-02-09\', \'Encke\': \'2003-12-28\', ""d\'Arrest"": \'2008-08-01\', \'Tempel 1\': \'2005-07-05\'}, name=\'Perihelion Date\', dtype=np.datetime64)\n>>> s\n<Series: Perihelion Date>\n<Index>\nHalley                    1986-02-09\nEncke                     2003-12-28\nd\'Arrest                  2008-08-01\nTempel 1                  2005-07-05\n<<U8>                     <datetime64[D]>\n>>> s.via_dt.strftime(\'%m/%d/%y\')\n<Series: Perihelion Date>\n<Index>\nHalley                    02/09/86\nEncke                     12/28/03\nd\'Arrest                  08/01/08\nTempel 1                  07/05/05\n<<U8>                     <<U8>\n\n#end_Series-via_dt.strftime()\n\n\n\n\n\n#start_Series-via_str.capitalize()\n>>> s = sf.Series((\'lepton\', \'lepton\', \'quark\'), index=(\'muon\', \'tau\', \'strange\'))\n>>> s.via_str.capitalize()\n<Series>\n<Index>\nmuon     Lepton\ntau      Lepton\nstrange  Quark\n<<U7>    <<U6>\n\n#end_Series-via_str.capitalize()\n\n\n#start_Series-via_str.center()\n>>> s = sf.Series((\'lepton\', \'lepton\', \'quark\'), index=(\'muon\', \'tau\', \'strange\'))\n>>> s.via_str.center(20, \'-\')\n<Series>\n<Index>\nmuon     -------lepton-------\ntau      -------lepton-------\nstrange  -------quark--------\n<<U7>    <<U20>\n\n#end_Series-via_str.center()\n\n\n#start_Series-via_str.endswith()\n>>> s = sf.Series((\'lepton\', \'lepton\', \'quark\'), index=(\'muon\', \'tau\', \'strange\'))\n>>> s.via_str.endswith(\'ton\')\n<Series>\n<Index>\nmuon     True\ntau      True\nstrange  False\n<<U7>    <bool>\n\n#end_Series-via_str.endswith()\n\n\n#start_Series-via_str.isdigit()\n>>> s = sf.Series((\'lepton\', \'lepton\', \'quark\'), index=(\'muon\', \'tau\', \'strange\'))\n>>> s.via_str.isdigit()\n<Series>\n<Index>\nmuon     False\ntau      False\nstrange  False\n<<U7>    <bool>\n\n#end_Series-via_str.isdigit()\n\n\n#start_Series-via_str.islower()\n>>> s = sf.Series((\'lepton\', \'lepton\', \'quark\'), index=(\'muon\', \'tau\', \'strange\'))\n>>> s.via_str.islower()\n<Series>\n<Index>\nmuon     True\ntau      True\nstrange  True\n<<U7>    <bool>\n\n#end_Series-via_str.islower()\n\n\n#start_Series-via_str.ljust()\n>>> s = sf.Series((\'lepton\', \'lepton\', \'quark\'), index=(\'muon\', \'tau\', \'strange\'))\n>>> s.via_str.ljust(10, \'-\')\n<Series>\n<Index>\nmuon     lepton----\ntau      lepton----\nstrange  quark-----\n<<U7>    <<U10>\n\n#end_Series-via_str.ljust()\n\n\n#start_Series-via_str.isupper()\n>>> s = sf.Series((\'lepton\', \'lepton\', \'quark\'), index=(\'muon\', \'tau\', \'strange\'))\n>>> s.via_str.isupper()\n<Series>\n<Index>\nmuon     False\ntau      False\nstrange  False\n<<U7>    <bool>\n\n#end_Series-via_str.isupper()\n\n\n#start_Series-via_str.rjust()\n>>> s = sf.Series((\'lepton\', \'lepton\', \'quark\'), index=(\'muon\', \'tau\', \'strange\'))\n>>> s.via_str.rjust(10, \'-\')\n<Series>\n<Index>\nmuon     ----lepton\ntau      ----lepton\nstrange  -----quark\n<<U7>    <<U10>\n\n#end_Series-via_str.rjust()\n\n\n#start_Series-via_str.startswith()\n>>> s = sf.Series((\'lepton\', \'lepton\', \'quark\'), index=(\'muon\', \'tau\', \'strange\'))\n>>> s.via_str.startswith(\'lep\')\n<Series>\n<Index>\nmuon     True\ntau      True\nstrange  False\n<<U7>    <bool>\n\n#end_Series-via_str.startswith()\n\n\n#start_Series-via_str.title()\n>>> s.via_str.title()\n<Series>\n<Index>\nmuon     Lepton\ntau      Lepton\nstrange  Quark\n<<U7>    <<U6>\n\n#end_Series-via_str.title()\n\n\n\n#start_Series-via_str.upper()\n>>> s = sf.Series((\'lepton\', \'lepton\', \'quark\'), index=(\'muon\', \'tau\', \'strange\'))\n>>> s.via_str.upper()\n<Series>\n<Index>\nmuon     LEPTON\ntau      LEPTON\nstrange  QUARK\n<<U7>    <<U6>\n\n#end_Series-via_str.upper()\n\n\n\n\n#start_Series-from_dict()\n>>> sf.Series.from_dict(dict(Mercury=167, Neptune=-200), dtype=np.int64)\n<Series>\n<Index>\nMercury  167\nNeptune  -200\n<<U7>    <int64>\n\n#end_Series-from_dict()\n\n\n#start_Series-__init__()\n>>> sf.Series((167, -200), index=(\'Mercury\', \'Neptune\'), dtype=np.int64)\n<Series>\n<Index>\nMercury  167\nNeptune  -200\n<<U7>    <int64>\n\n#end_Series-__init__()\n\n\n#start_Series-from_items()\n>>> sf.Series.from_items(zip((\'Mercury\', \'Jupiter\'), (4879, 12756)), dtype=np.int64)\n<Series>\n<Index>\nMercury  4879\nJupiter  12756\n<<U7>    <int64>\n\n#end_Series-from_items()\n\n\n#start_Series-from_element()\n>>> sf.Series.from_element(\'lepton\', index=(\'electron\', \'muon\', \'tau\'))\n<Series>\n<Index>\nelectron lepton\nmuon     lepton\ntau      lepton\n<<U8>    <<U6>\n\n#end_Series-from_element()\n\n\n\n#start_Series-items()\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'), dtype=np.int64)\n>>> s\n<Series>\n<Index>\nEarth    1\nMars     2\nJupiter  67\nSaturn   62\nUranus   27\nNeptune  14\n<<U7>    <int64>\n>>> [k for k, v in s.items() if v > 60]\n[\'Jupiter\', \'Saturn\']\n\n#end_Series-items()\n\n\n#start_Series-get()\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'), dtype=np.int64)\n>>> [s.get(k, None) for k in (\'Mercury\', \'Neptune\', \'Pluto\')]\n[None, 14, None]\n\n#end_Series-get()\n\n\n#start_Series-__len__()\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'), dtype=np.int64)\n>>> len(s)\n6\n\n#end_Series-__len__()\n\n\n#start_Series-__sub__()\n>>> s = sf.Series.from_items(((\'Venus\', 108.2), (\'Earth\', 149.6), (\'Saturn\', 1433.5)))\n>>> s\n<Series>\n<Index>\nVenus    108.2\nEarth    149.6\nSaturn   1433.5\n<<U6>    <float64>\n\n>>> abs(s - s[\'Earth\'])\n<Series>\n<Index>\nVenus    41.39999999999999\nEarth    0.0\nSaturn   1283.9\n<<U6>    <float64>\n\n#end_Series-__sub__()\n\n\n#start_Series-__gt__()\n>>> s = sf.Series.from_items(((\'Venus\', 108.2), (\'Earth\', 149.6), (\'Saturn\', 1433.5)))\n>>> s > s[\'Earth\']\n<Series>\n<Index>\nVenus    False\nEarth    False\nSaturn   True\n<<U6>    <bool>\n\n#end_Series-__gt__()\n\n\n#start_Series-__truediv__()\n>>> s = sf.Series.from_items(((\'Venus\', 108.2), (\'Earth\', 149.6), (\'Saturn\', 1433.5)))\n\n>>> s / s[\'Earth\']\n<Series>\n<Index>\nVenus    0.7232620320855615\nEarth    1.0\nSaturn   9.582219251336898\n<<U6>    <float64>\n\n#end_Series-__truediv__()\n\n\n#start_Series-__mul__()\n>>> s1 = sf.Series((1, 2), index=(\'Earth\', \'Mars\'))\n>>> s2 = sf.Series((2, 0), index=(\'Mars\', \'Mercury\'))\n>>> s1 * s2\n<Series>\n<Index>\nEarth    nan\nMars     4.0\nMercury  nan\n<<U7>    <float64>\n\n#end_Series-__mul__()\n\n\n#start_Series-__eq__()\n>>> s1 = sf.Series((1, 2), index=(\'Earth\', \'Mars\'))\n>>> s2 = sf.Series((2, 0), index=(\'Mars\', \'Mercury\'))\n\n>>> s1 == s2\n<Series>\n<Index>\nEarth    False\nMars     True\nMercury  False\n<<U7>    <bool>\n\n#end_Series-__eq__()\n\n\n#start_Series-relabel()\n>>> s = sf.Series((0, 62, 13), index=(\'Venus\', \'Saturn\', \'Neptune\'), dtype=np.int64)\n\n>>> s.relabel({\'Venus\': \'Mercury\'})\n<Series>\n<Index>\nMercury  0\nSaturn   62\nNeptune  13\n<<U7>    <int64>\n\n>>> s.relabel(lambda x: x[:2].upper())\n<Series>\n<Index>\nVE       0\nSA       62\nNE       13\n<<U2>    <int64>\n\n#end_Series-relabel()\n\n\n#start_Series-reindex()\n>>> s = sf.Series((0, 62, 13), index=(\'Venus\', \'Saturn\', \'Neptune\'))\n\n>>> s.reindex((\'Venus\', \'Earth\', \'Mars\', \'Neptune\'))\n<Series>\n<Index>\nVenus    0.0\nEarth    nan\nMars     nan\nNeptune  13.0\n<<U7>    <float64>\n\n#end_Series-reindex()\n\n\n#start_Series-shape\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'), dtype=np.int64)\n>>> s\n<Series>\n<Index>\nEarth    1\nMars     2\nJupiter  67\nSaturn   62\nUranus   27\nNeptune  14\n<<U7>    <int64>\n>>> s.shape\n(6,)\n\n#end_Series-shape\n\n\n#start_Series-ndim\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'), dtype=np.int64)\n>>> s.ndim\n1\n\n#end_Series-ndim\n\n\n#start_Series-size\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'), dtype=np.int64)\n>>> s.size\n6\n\n#end_Series-size\n\n\n#start_Series-nbytes\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'), dtype=np.int64)\n>>> s.nbytes\n48\n\n#end_Series-nbytes\n\n\n#start_Series-dtype\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'), dtype=np.int64)\n>>> s.dtype\ndtype(\'int64\')\n\n#end_Series-dtype\n\n\n\n\n#start_Series-interface\n>>> sf.Series.interface.loc[sf.Series.interface.index.via_str.startswith(\'sort\')]\n<Frame: Series>\n<Index>                         cls_name group  doc                  <<U18>\n<Index: signature>\nsort_index(*, ascending, kind)  Series   Method Return a new Seri...\nsort_values(*, ascending, kind) Series   Method Return a new Seri...\n<<U94>                          <<U6>    <<U17> <<U83>\n\n#end_Series-interface\n\n\n#start_Series-iter_element()\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'))\n>>> [x for x in s.iter_element()]\n[1, 2, 67, 62, 27, 14]\n\n#end_Series-iter_element()\n\n\n#start_Series-iter_element().apply()\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'))\n>>> s.iter_element().apply(lambda v: v > 20)\n<Series>\n<Index>\nEarth    False\nMars     False\nJupiter  True\nSaturn   True\nUranus   True\nNeptune  False\n<<U7>    <bool>\n\n#end_Series-iter_element().apply()\n\n\n#start_Series-iter_element().apply_iter()\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'))\n>>> [x for x in s.iter_element().apply_iter(lambda v: v > 20)]\n[False, False, True, True, True, False]\n\n#end_Series-iter_element().apply_iter()\n\n\n#start_Series-iter_element_items()\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'))\n>>> [x for x in s.iter_element_items()]\n[(\'Earth\', 1), (\'Mars\', 2), (\'Jupiter\', 67), (\'Saturn\', 62), (\'Uranus\', 27), (\'Neptune\', 14)]\n\n#end_Series-iter_element_items()\n\n\n#start_Series-iter_element_items().apply()\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'))\n>>> s.iter_element_items().apply(lambda k, v: v if \'u\' in k else None)\n<Series>\n<Index>\nEarth    None\nMars     None\nJupiter  67\nSaturn   62\nUranus   27\nNeptune  14\n<<U7>    <object>\n\n#end_Series-iter_element_items().apply()\n\n\n#start_Series-iter_element_items().apply_iter_items()\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'))\n\n>>> [x for x in s.iter_element_items().apply_iter_items(lambda k, v: k.upper() if v > 20 else None)]\n[(\'Earth\', None), (\'Mars\', None), (\'Jupiter\', \'JUPITER\'), (\'Saturn\', \'SATURN\'), (\'Uranus\', \'URANUS\'), (\'Neptune\', None)]\n\n\n#end_Series-iter_element_items().apply_iter_items()\n\n\n#start_Series-iter_group()\n>>> s = sf.Series((0, 0, 1, 2), index=(\'Mercury\', \'Venus\', \'Earth\', \'Mars\'), dtype=np.int64)\n>>> next(iter(s.iter_group()))\n<Series>\n<Index>\nMercury  0\nVenus    0\n<<U7>    <int64>\n>>> [x.values.tolist() for x in s.iter_group()]\n[[0, 0], [1], [2]]\n\n#end_Series-iter_group()\n\n\n#start_Series-iter_group_items()\n>>> s = sf.Series((0, 0, 1, 2), index=(\'Mercury\', \'Venus\', \'Earth\', \'Mars\'))\n>>> [(k, v.index.values.tolist()) for k, v in iter(s.iter_group_items()) if k > 0]\n[(1, [\'Earth\']), (2, [\'Mars\'])]\n\n#end_Series-iter_group_items()\n\n\n#start_Series-assign[]()\n>>> s = sf.Series.from_items(((\'Venus\', 108.2), (\'Earth\', 149.6), (\'Saturn\', 1433.5)))\n>>> s\n<Series>\n<Index>\nVenus    108.2\nEarth    149.6\nSaturn   1433.5\n<<U6>    <float64>\n>>> s.assign[\'Earth\'](150)\n<Series>\n<Index>\nVenus    108.2\nEarth    150.0\nSaturn   1433.5\n<<U6>    <float64>\n>>> s.assign[\'Earth\':](0)\n<Series>\n<Index>\nVenus    108.2\nEarth    0.0\nSaturn   0.0\n<<U6>    <float64>\n\n#end_Series-assign[]()\n\n\n#start_Series-assign.loc[]()\n>>> s = sf.Series.from_items(((\'Venus\', 108.2), (\'Earth\', 149.6), (\'Saturn\', 1433.5)))\n>>> s.assign.loc[s < 150](0)\n<Series>\n<Index>\nVenus    0.0\nEarth    0.0\nSaturn   1433.5\n<<U6>    <float64>\n\n#end_Series-assign.loc[]()\n\n\n#start_Series-assign.iloc[]()\n>>> s = sf.Series.from_items(((\'Venus\', 108.2), (\'Earth\', 149.6), (\'Saturn\', 1433.5)))\n>>> s.assign.iloc[-1](0)\n<Series>\n<Index>\nVenus    108.2\nEarth    149.6\nSaturn   0.0\n<<U6>    <float64>\n\n#end_Series-assign.iloc[]()\n\n\n#start_Series-drop[]\n>>> s = sf.Series((0, 0, 1, 2), index=(\'Mercury\', \'Venus\', \'Earth\', \'Mars\'), dtype=np.int64)\n>>> s\n<Series>\n<Index>\nMercury  0\nVenus    0\nEarth    1\nMars     2\n<<U7>    <int64>\n>>> s.drop[s < 1]\n<Series>\n<Index>\nEarth    1\nMars     2\n<<U7>    <int64>\n>>> s.drop[[\'Mercury\', \'Mars\']]\n<Series>\n<Index>\nVenus    0\nEarth    1\n<<U7>    <int64>\n\n#end_Series-drop[]\n\n\n#start_Series-drop.iloc[]\n>>> s = sf.Series((0, 0, 1, 2), index=(\'Mercury\', \'Venus\', \'Earth\', \'Mars\'), dtype=np.int64)\n>>> s.drop.iloc[-2:]\n<Series>\n<Index>\nMercury  0\nVenus    0\n<<U7>    <int64>\n\n#end_Series-drop.iloc[]\n\n\n#start_Series-[]\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'), dtype=np.int64)\n>>> s\n<Series>\n<Index>\nEarth    1\nMars     2\nJupiter  67\nSaturn   62\nUranus   27\nNeptune  14\n<<U7>    <int64>\n\n>>> s[\'Mars\']\n2\n>>> s[\'Mars\':]\n<Series>\n<Index>\nMars     2\nJupiter  67\nSaturn   62\nUranus   27\nNeptune  14\n<<U7>    <int64>\n>>> s[[\'Mars\', \'Saturn\']]\n<Series>\n<Index>\nMars     2\nSaturn   62\n<<U7>    <int64>\n>>> s[s > 60]\n<Series>\n<Index>\nJupiter  67\nSaturn   62\n<<U7>    <int64>\n\n#end_Series-[]\n\n\n#start_Series-iloc[]\n>>> s = sf.Series((1, 2, 67, 62, 27, 14), index=(\'Earth\', \'Mars\', \'Jupiter\', \'Saturn\', \'Uranus\', \'Neptune\'), dtype=np.int64)\n>>> s.iloc[-2:]\n<Series>\n<Index>\nUranus   27\nNeptune  14\n<<U7>    <int64>\n\n#end_Series-iloc[]\n\n\n#-------------------------------------------------------------------------------\n# Frame\n\n\n#start_Frame-via_str.center()\n>>> f = sf.Frame.from_records(((\'76.1 yrs.\', \'0.587 AU\'), (\'3.30 yrs.\', \'0.340 AU\'), (\'6.51 yrs.\', \'1.346 AU\')), index=(\'Halley\', \'Encke\', ""d\'Arrest""), columns=(\'Orbital Period\', \'Perihelion Distance\'))\n>>> f\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   76.1 yrs.      0.587 AU\nEncke    3.30 yrs.      0.340 AU\nd\'Arrest 6.51 yrs.      1.346 AU\n<<U8>    <<U9>          <<U8>\n>>> f.via_str.center(18, \'-\')\n<Frame>\n<Index>  Orbital Period     Perihelion Distance <<U19>\n<Index>\nHalley   ----76.1 yrs.----- -----0.587 AU-----\nEncke    ----3.30 yrs.----- -----0.340 AU-----\nd\'Arrest ----6.51 yrs.----- -----1.346 AU-----\n<<U8>    <<U18>             <<U18>\n\n#end_Frame-via_str.center()\n\n\n#start_Frame-via_str.count()\n>>> f = sf.Frame.from_records(((\'76.1 yrs.\', \'0.587 AU\'), (\'3.30 yrs.\', \'0.340 AU\'), (\'6.51 yrs.\', \'1.346 AU\')), index=(\'Halley\', \'Encke\', ""d\'Arrest""), columns=(\'Orbital Period\', \'Perihelion Distance\'))\n>>> f\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   76.1 yrs.      0.587 AU\nEncke    3.30 yrs.      0.340 AU\nd\'Arrest 6.51 yrs.      1.346 AU\n<<U8>    <<U9>          <<U8>\n>>> f.via_str.count(\'3\')\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   0              0\nEncke    2              1\nd\'Arrest 0              1\n<<U8>    <int64>        <int64>\n\n#end_Frame-via_str.count()\n\n\n\n\n#start_Frame-via_str.endswith()\n>>> f = sf.Frame.from_records(((\'76.1 yrs.\', \'0.587 AU\'), (\'3.30 yrs.\', \'0.340 AU\'), (\'6.51 yrs.\', \'1.346 AU\')), index=(\'Halley\', \'Encke\', ""d\'Arrest""), columns=(\'Orbital Period\', \'Perihelion Distance\'))\n>>> f\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   76.1 yrs.      0.587 AU\nEncke    3.30 yrs.      0.340 AU\nd\'Arrest 6.51 yrs.      1.346 AU\n<<U8>    <<U9>          <<U8>\n>>> f.via_str.endswith(\'AU\')\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   False          True\nEncke    False          True\nd\'Arrest False          True\n<<U8>    <bool>         <bool>\n\n#end_Frame-via_str.endswith()\n\n\n\n#start_Frame-via_str.find()\n>>> f = sf.Frame.from_records(((\'76.1 yrs.\', \'0.587 AU\'), (\'3.30 yrs.\', \'0.340 AU\'), (\'6.51 yrs.\', \'1.346 AU\')), index=(\'Halley\', \'Encke\', ""d\'Arrest""), columns=(\'Orbital Period\', \'Perihelion Distance\'))\n>>> f\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   76.1 yrs.      0.587 AU\nEncke    3.30 yrs.      0.340 AU\nd\'Arrest 6.51 yrs.      1.346 AU\n<<U8>    <<U9>          <<U8>\n>>> f.via_str.find(\'.\')\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   2              1\nEncke    1              1\nd\'Arrest 1              1\n<<U8>    <int64>        <int64>\n\n#end_Frame-via_str.find()\n\n\n\n#start_Frame-via_str.partition()\n>>> f = sf.Frame.from_records(((\'76.1 yrs.\', \'0.587 AU\'), (\'3.30 yrs.\', \'0.340 AU\'), (\'6.51 yrs.\', \'1.346 AU\')), index=(\'Halley\', \'Encke\', ""d\'Arrest""), columns=(\'Orbital Period\', \'Perihelion Distance\'))\n>>> f\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   76.1 yrs.      0.587 AU\nEncke    3.30 yrs.      0.340 AU\nd\'Arrest 6.51 yrs.      1.346 AU\n<<U8>    <<U9>          <<U8>\n>>> f.via_str.partition(\' \').display_wide()\n<Frame>\n<Index>  Orbital Period        Perihelion Distance  <<U19>\n<Index>\nHalley   (\'76.1\', \' \', \'yrs.\') (\'0.587\', \' \', \'AU\')\nEncke    (\'3.30\', \' \', \'yrs.\') (\'0.340\', \' \', \'AU\')\nd\'Arrest (\'6.51\', \' \', \'yrs.\') (\'1.346\', \' \', \'AU\')\n<<U8>    <object>              <object>\n\n#end_Frame-via_str.partition()\n\n\n#start_Frame-via_str.rfind()\n>>> f = sf.Frame.from_records(((\'76.1 yrs.\', \'0.587 AU\'), (\'3.30 yrs.\', \'0.340 AU\'), (\'6.51 yrs.\', \'1.346 AU\')), index=(\'Halley\', \'Encke\', ""d\'Arrest""), columns=(\'Orbital Period\', \'Perihelion Distance\'))\n>>> f\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   76.1 yrs.      0.587 AU\nEncke    3.30 yrs.      0.340 AU\nd\'Arrest 6.51 yrs.      1.346 AU\n<<U8>    <<U9>          <<U8>\n>>> f.via_str.rfind(\'.\')\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   8              1\nEncke    8              1\nd\'Arrest 8              1\n<<U8>    <int64>        <int64>\n\n#end_Frame-via_str.rfind()\n\n\n#start_Frame-via_str.ljust()\n>>> f = sf.Frame.from_records(((\'76.1 yrs.\', \'0.587 AU\'), (\'3.30 yrs.\', \'0.340 AU\'), (\'6.51 yrs.\', \'1.346 AU\')), index=(\'Halley\', \'Encke\', ""d\'Arrest""), columns=(\'Orbital Period\', \'Perihelion Distance\'))\n>>> f\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   76.1 yrs.      0.587 AU\nEncke    3.30 yrs.      0.340 AU\nd\'Arrest 6.51 yrs.      1.346 AU\n<<U8>    <<U9>          <<U8>\n>>> f.via_str.ljust(20, \'.\')\n<Frame>\n<Index>  Orbital Period       Perihelion Distance  <<U19>\n<Index>\nHalley   76.1 yrs............ 0.587 AU............\nEncke    3.30 yrs............ 0.340 AU............\nd\'Arrest 6.51 yrs............ 1.346 AU............\n<<U8>    <<U20>               <<U20>\n\n#end_Frame-via_str.ljust()\n\n\n#start_Frame-via_str.rjust()\n>>> f = sf.Frame.from_records(((\'76.1 yrs.\', \'0.587 AU\'), (\'3.30 yrs.\', \'0.340 AU\'), (\'6.51 yrs.\', \'1.346 AU\')), index=(\'Halley\', \'Encke\', ""d\'Arrest""), columns=(\'Orbital Period\', \'Perihelion Distance\'))\n>>> f\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   76.1 yrs.      0.587 AU\nEncke    3.30 yrs.      0.340 AU\nd\'Arrest 6.51 yrs.      1.346 AU\n<<U8>    <<U9>          <<U8>\n>>> f.via_str.rjust(20, \'.\')\n<Frame>\n<Index>  Orbital Period       Perihelion Distance  <<U19>\n<Index>\nHalley   ...........76.1 yrs. ............0.587 AU\nEncke    ...........3.30 yrs. ............0.340 AU\nd\'Arrest ...........6.51 yrs. ............1.346 AU\n<<U8>    <<U20>               <<U20>\n\n#end_Frame-via_str.rjust()\n\n\n\n\n#start_Frame-via_str.split()\n>>> f = sf.Frame.from_records(((\'76.1 yrs.\', \'0.587 AU\'), (\'3.30 yrs.\', \'0.340 AU\'), (\'6.51 yrs.\', \'1.346 AU\')), index=(\'Halley\', \'Encke\', ""d\'Arrest""), columns=(\'Orbital Period\', \'Perihelion Distance\'))\n>>> f\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   76.1 yrs.      0.587 AU\nEncke    3.30 yrs.      0.340 AU\nd\'Arrest 6.51 yrs.      1.346 AU\n<<U8>    <<U9>          <<U8>\n>>> f.via_str.split(\' \')\n<Frame>\n<Index>  Orbital Period   Perihelion Distance <<U19>\n<Index>\nHalley   (\'76.1\', \'yrs.\') (\'0.587\', \'AU\')\nEncke    (\'3.30\', \'yrs.\') (\'0.340\', \'AU\')\nd\'Arrest (\'6.51\', \'yrs.\') (\'1.346\', \'AU\')\n<<U8>    <object>         <object>\n\n#end_Frame-via_str.split()\n\n\n\n\n#start_Frame-via_str.startswith()\n>>> f = sf.Frame.from_records(((\'76.1 yrs.\', \'0.587 AU\'), (\'3.30 yrs.\', \'0.340 AU\'), (\'6.51 yrs.\', \'1.346 AU\')), index=(\'Halley\', \'Encke\', ""d\'Arrest""), columns=(\'Orbital Period\', \'Perihelion Distance\'))\n>>> f\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   76.1 yrs.      0.587 AU\nEncke    3.30 yrs.      0.340 AU\nd\'Arrest 6.51 yrs.      1.346 AU\n<<U8>    <<U9>          <<U8>\n>>> f.via_str.startswith(\'0.\')\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   False          True\nEncke    False          True\nd\'Arrest False          False\n<<U8>    <bool>         <bool>\n\n#end_Frame-via_str.startswith()\n\n\n\n#start_Frame-via_str.replace()\n>>> f = sf.Frame.from_records(((\'76.1 yrs.\', \'0.587 AU\'), (\'3.30 yrs.\', \'0.340 AU\'), (\'6.51 yrs.\', \'1.346 AU\')), index=(\'Halley\', \'Encke\', ""d\'Arrest""), columns=(\'Orbital Period\', \'Perihelion Distance\'))\n>>> f\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   76.1 yrs.      0.587 AU\nEncke    3.30 yrs.      0.340 AU\nd\'Arrest 6.51 yrs.      1.346 AU\n<<U8>    <<U9>          <<U8>\n>>> f.via_str.replace(\' AU\', \'\').via_str.replace(\' yrs.\', \'\').astype(float)\n<Frame>\n<Index>  Orbital Period Perihelion Distance <<U19>\n<Index>\nHalley   76.1           0.587\nEncke    3.3            0.34\nd\'Arrest 6.51           1.346\n<<U8>    <float64>      <float64>\n\n#end_Frame-via_str.replace()\n\n\n#start_Frame-interface\n>>> sf.Frame.interface.loc[sf.Frame.interface.index.via_str.startswith(\'sort\')]\n<Frame: Frame>\n<Index>                              cls_name group  doc                  <<U18>\n<Index: signature>\nsort_columns(*, ascending, kind)     Frame    Method Return a new Fram...\nsort_index(*, ascending, kind)       Frame    Method Return a new Fram...\nsort_values(key, *, ascending, ax... Frame    Method Return a new Fram...\n<<U94>                               <<U5>    <<U17> <<U83>\n\n#end_Frame-interface\n\n\n\n#start_Frame-from_dict()\n>>> sf.Frame.from_dict(dict(diameter=(12756, 142984, 120536), mass=(5.97, 1898, 568)), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64))\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nJupiter 142984   1898.0\nSaturn  120536   568.0\n<<U7>   <int64>  <float64>\n\n#end_Frame-from_dict()\n\n\n#start_FrameGO-from_dict()\n>>> f = sf.FrameGO.from_dict(dict(diameter=(12756, 142984, 120536), mass=(5.97, 1898, 568)), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64))\n>>> f[\'radius\'] = f[\'diameter\'] * 0.5\n>>> f\n<FrameGO>\n<IndexGO> diameter mass      radius    <<U8>\n<Index>\nEarth     12756    5.97      6378.0\nJupiter   142984   1898.0    71492.0\nSaturn    120536   568.0     60268.0\n<<U7>     <int64>  <float64> <float64>\n\n#end_FrameGO-from_dict()\n\n\n#start_Frame-from_records()\n>>> index = (\'Mercury\', \'Venus\', \'Earth\', \'Mars\')\n>>> columns = (\'diameter\', \'gravity\', \'temperature\')\n>>> records = ((4879, 3.7, 167), (12104, 8.9, 464), (12756, 9.8, 15), (6792, 3.7, -65))\n>>> sf.Frame.from_records(records, index=index, columns=columns, dtypes=dict(diameter=np.int64, temperature=np.int64))\n<Frame>\n<Index> diameter gravity   temperature <<U11>\n<Index>\nMercury 4879     3.7       167\nVenus   12104    8.9       464\nEarth   12756    9.8       15\nMars    6792     3.7       -65\n<<U7>   <int64>  <float64> <int64>\n\n#end_Frame-from_records()\n\n\n\n#start_Frame-from_items()\n>>> sf.Frame.from_items(((\'diameter\', (12756, 142984, 120536)), (\'mass\', (5.97, 1898, 568))), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64))\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nJupiter 142984   1898.0\nSaturn  120536   568.0\n<<U7>   <int64>  <float64>\n\n#end_Frame-from_items()\n\n\n\n#start_Frame-from_concat()\n>>> f1 = sf.Frame.from_dict(dict(diameter=(12756, 142984), mass=(5.97, 1898)), index=(\'Earth\', \'Jupiter\'))\n>>> f2 = sf.Frame.from_dict(dict(mass=(0.642, 102), moons=(2, 14)), index=(\'Mars\', \'Neptune\'))\n>>> sf.Frame.from_concat((f1, f2))\n<Frame>\n<Index> diameter  mass      moons     <<U8>\n<Index>\nEarth   12756.0   5.97      nan\nJupiter 142984.0  1898.0    nan\nMars    nan       0.642     2.0\nNeptune nan       102.0     14.0\n<<U7>   <float64> <float64> <float64>\n\n>>> sf.Frame.from_concat((f1, f2), union=False)\n<Frame>\n<Index> mass      <<U8>\n<Index>\nEarth   5.97\nJupiter 1898.0\nMars    0.642\nNeptune 102.0\n<<U7>   <float64>\n\n#end_Frame-from_concat()\n\n\n\n#start_Frame-from_structured_array()\n>>> a = np.array([(\'Venus\', 4.87, 464), (\'Neptune\', 102, -200)], dtype=[(\'name\', object), (\'mass\', \'f4\'), (\'temperature\', \'i4\')])\n>>> sf.Frame.from_structured_array(a, index_depth=1)\n<Frame>\n<Index>  mass              temperature <<U11>\n<Index>\nVenus    4.869999885559082 464\nNeptune  102.0             -200\n<object> <float32>         <int32>\n\n#end_Frame-from_structured_array()\n\n\n\n#start_Frame-from_csv()\n>>> from io import StringIO\n>>> filelike = StringIO(\'name,mass,temperature\\\\nVenus,4.87,464\\\\nNeptune,102,-200\')\n>>> sf.Frame.from_csv(filelike, index_depth=1, dtypes=dict(temperature=np.int64))\n<Frame>\n<Index> mass      temperature <<U11>\n<Index>\nVenus   4.87      464\nNeptune 102.0     -200\n<<U7>   <float64> <int64>\n\n#end_Frame-from_csv()\n\n\n\n#start_Frame-items()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 142984, 120536), temperature=(15, -110, -140)), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(temperature=np.int64, diameter=np.int64))\n>>> f\n<Frame>\n<Index> diameter temperature <<U11>\n<Index>\nEarth   12756    15\nJupiter 142984   -110\nSaturn  120536   -140\n<<U7>   <int64>  <int64>\n>>> len(f)\n3\n>>> [k for k, v in f.items() if (v < 0).any()]\n[\'temperature\']\n\n#end_Frame-items()\n\n\n#start_Frame-get()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 142984, 120536), temperature=(15, -110, -140)), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(temperature=np.int64, diameter=np.int64))\n\n>>> f.get(\'diameter\')\n<Series: diameter>\n<Index>\nEarth              12756\nJupiter            142984\nSaturn             120536\n<<U7>              <int64>\n\n>>> f.get(\'mass\', np.nan)\nnan\n\n#end_Frame-get()\n\n\n#start_Frame-__contains__()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 142984, 120536), temperature=(15, -110, -140)), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(temperature=np.int64, diameter=np.int64))\n\n>>> \'temperature\' in f\nTrue\n\n#end_Frame-__contains__()\n\n\n#start_Frame-values\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 142984, 120536), temperature=(15, -110, -140)), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(temperature=np.int64, diameter=np.int64))\n\n>>> f.values.tolist()\n[[12756, 15], [142984, -110], [120536, -140]]\n\n#end_Frame-values\n\n\n#start_Frame-__truediv__()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'), dtypes=dict(diameter=np.int64))\n>>> f\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nMars    6792     0.642\nJupiter 142984   1898.0\n<<U7>   <int64>  <float64>\n\n>>> f / f.loc[\'Earth\']\n<Frame>\n<Index> diameter           mass                <<U8>\n<Index>\nEarth   1.0                1.0\nMars    0.5324553151458138 0.10753768844221107\nJupiter 11.209156475384132 317.92294807370183\n<<U7>   <float64>          <float64>\n\n#end_Frame-__truediv__()\n\n\n#start_Frame-max()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 142984, 120536), mass=(5.97, 1898, 568)), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64))\n>>> f\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nJupiter 142984   1898.0\nSaturn  120536   568.0\n<<U7>   <int64>  <float64>\n\n>>> f.max()\n<Series>\n<Index>\ndiameter 142984.0\nmass     1898.0\n<<U8>    <float64>\n\n#end_Frame-max()\n\n\n#start_Frame-min()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 142984, 120536), mass=(5.97, 1898, 568)), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64))\n\n>>> f.min()\n<Series>\n<Index>\ndiameter 12756.0\nmass     5.97\n<<U8>    <float64>\n\n#end_Frame-min()\n\n\n#start_Frame-std()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 142984, 120536), mass=(5.97, 1898, 568)), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64))\n\n>>> f.std()\n<Series>\n<Index>\ndiameter 56842.64155250587\nmass     793.344204533358\n<<U8>    <float64>\n\n#end_Frame-std()\n\n\n#start_Frame-sum()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 142984, 120536), mass=(5.97, 1898, 568)), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64))\n>>> f\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nJupiter 142984   1898.0\nSaturn  120536   568.0\n<<U7>   <int64>  <float64>\n\n>>> f.sum()\n<Series>\n<Index>\ndiameter 276276.0\nmass     2471.9700000000003\n<<U8>    <float64>\n\n#end_Frame-sum()\n\n\n#start_Frame-mean()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 142984, 120536), mass=(5.97, 1898, 568)), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64))\n\n>>> f.mean()\n<Series>\n<Index>\ndiameter 92092.0\nmass     823.9900000000001\n<<U8>    <float64>\n\n#end_Frame-mean()\n\n\n#start_Frame-relabel()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'), dtypes=dict(diameter=np.int64))\n\n>>> f.relabel(index=lambda x: x[:2].upper(), columns={\'mass\': \'mass(1e24kg)\'})\n<Frame>\n<Index> diameter mass(1e24kg) <<U12>\n<Index>\nEA      12756    5.97\nMA      6792     0.642\nJU      142984   1898.0\n<<U2>   <int64>  <float64>\n\n#end_Frame-relabel()\n\n\n#start_Frame-reindex()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'), dtypes=dict(diameter=np.int64))\n>>> f\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nMars    6792     0.642\nJupiter 142984   1898.0\n<<U7>   <int64>  <float64>\n\n>>> f.reindex(index=(\'Jupiter\', \'Mars\', \'Mercury\'), columns=(\'density\', \'mass\'))\n<Frame>\n<Index> density   mass      <<U7>\n<Index>\nJupiter nan       1898.0\nMars    nan       0.642\nMercury nan       nan\n<<U7>   <float64> <float64>\n\n#end_Frame-reindex()\n\n\n#start_Frame-iter_element()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'), dtypes=dict(diameter=np.int64))\n\n>>> [x for x in f.iter_element()]\n[12756, 5.97, 6792, 0.642, 142984, 1898.0]\n\n#end_Frame-iter_element()\n\n\n#start_Frame-iter_element().apply()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'), dtypes=dict(diameter=np.int64))\n\n>>> f.iter_element().apply(lambda x: x ** 2)\n<Frame>\n<Index> diameter    mass                <<U8>\n<Index>\nEarth   162715536   35.640899999999995\nMars    46131264    0.41216400000000003\nJupiter 20444424256 3602404.0\n<<U7>   <object>    <object>\n\n#end_Frame-iter_element().apply()\n\n\n#start_Frame-iter_element_items()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'))\n\n>>> [x for x in f.iter_element_items()]\n[((\'Earth\', \'diameter\'), 12756), ((\'Earth\', \'mass\'), 5.97), ((\'Mars\', \'diameter\'), 6792), ((\'Mars\', \'mass\'), 0.642), ((\'Jupiter\', \'diameter\'), 142984), ((\'Jupiter\', \'mass\'), 1898.0)]\n\n#end_Frame-iter_element_items()\n\n\n#start_Frame-iter_element_items().apply()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'))\n\n>>> f.iter_element_items().apply(lambda k, v: v ** 2 if k[0] == \'Mars\' else None)\n<Frame>\n<Index> diameter mass                <<U8>\n<Index>\nEarth   None     None\nMars    46131264 0.41216400000000003\nJupiter None     None\n<<U7>   <object> <object>\n\n#end_Frame-iter_element_items().apply()\n\n\n#start_Frame-iter_array()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'), dtypes=dict(diameter=np.int64))\n>>> f\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nMars    6792     0.642\nJupiter 142984   1898.0\n<<U7>   <int64>  <float64>\n\n>>> [x.tolist() for x in f.iter_array(axis=0)]\n[[12756, 6792, 142984], [5.97, 0.642, 1898.0]]\n\n>>> [x.tolist() for x in f.iter_array(axis=1)]\n[[12756.0, 5.97], [6792.0, 0.642], [142984.0, 1898.0]]\n\n#end_Frame-iter_array()\n\n\n#start_Frame-iter_array().apply()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'), dtypes=dict(diameter=np.int64))\n>>> f\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nMars    6792     0.642\nJupiter 142984   1898.0\n<<U7>   <int64>  <float64>\n\n>>> f.iter_array(axis=0).apply(np.sum)\n<Series>\n<Index>\ndiameter 162532.0\nmass     1904.612\n<<U8>    <float64>\n\n#end_Frame-iter_array().apply()\n\n\n#start_Frame-iter_array_items()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'))\n\n>>> [x for x in f.iter_array_items(axis=0)]\n[(\'diameter\', array([ 12756,   6792, 142984])), (\'mass\', array([5.970e+00, 6.420e-01, 1.898e+03]))]\n\n>>> [x for x in f.iter_array_items(axis=1)]\n[(\'Earth\', array([1.2756e+04, 5.9700e+00])), (\'Mars\', array([6.792e+03, 6.420e-01])), (\'Jupiter\', array([142984.,   1898.]))]\n\n>>> f.iter_array_items(axis=1).apply(lambda k, v: v.sum() if k == \'Earth\' else 0)\n<Series>\n<Index>\nEarth    12761.97\nMars     0.0\nJupiter  0.0\n<<U7>    <float64>\n\n#end_Frame-iter_array_items()\n\n\n#start_Frame-iter_array_items().apply()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'))\n\n>>> f.iter_array_items(axis=1).apply(lambda k, v: v.sum() if k == \'Earth\' else 0)\n<Series>\n<Index>\nEarth    12761.97\nMars     0.0\nJupiter  0.0\n<<U7>    <float64>\n\n#end_Frame-iter_array_items().apply()\n\n\n#start_Frame-iter_tuple()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'))\n\n>>> [x for x in f.iter_tuple(axis=0)]\n[Axis(Earth=12756, Mars=6792, Jupiter=142984), Axis(Earth=5.97, Mars=0.642, Jupiter=1898.0)]\n\n>>> [x for x in f.iter_tuple(axis=1)]\n[Axis(diameter=12756.0, mass=5.97), Axis(diameter=6792.0, mass=0.642), Axis(diameter=142984.0, mass=1898.0)]\n\n#end_Frame-iter_tuple()\n\n\n#start_Frame-iter_tuple().apply()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'))\n\n>>> f.iter_tuple(axis=1).apply(lambda nt: nt.mass / (4 / 3 * np.pi * (nt.diameter * 0.5) ** 3))\n<Series>\n<Index>\nEarth    5.49328558e-12\nMars     3.91330208e-12\nJupiter  1.24003876e-12\n<<U7>    <float64>\n\n#end_Frame-iter_tuple().apply()\n\n\n#start_Frame-iter_tuple_items()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'))\n\n>>> [x for x in f.iter_tuple_items(axis=0)]\n[(\'diameter\', Axis(Earth=12756, Mars=6792, Jupiter=142984)), (\'mass\', Axis(Earth=5.97, Mars=0.642, Jupiter=1898.0))]\n\n>>> [x for x in f.iter_tuple_items(axis=1)]\n[(\'Earth\', Axis(diameter=12756.0, mass=5.97)), (\'Mars\', Axis(diameter=6792.0, mass=0.642)), (\'Jupiter\', Axis(diameter=142984.0, mass=1898.0))]\n\n#end_Frame-iter_tuple_items()\n\n\n#start_Frame-iter_tuple_items().apply()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'))\n\n>>> f.iter_tuple_items(axis=1).apply(lambda k, v: v.diameter if k == \'Earth\' else 0)\n<Series>\n<Index>\nEarth    12756.0\nMars     0.0\nJupiter  0.0\n<<U7>    <float64>\n\n#end_Frame-iter_tuple_items().apply()\n\n\n#start_Frame-iter_series()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'), dtypes=dict(diameter=np.int64))\n\n>>> next(iter(f.iter_series(axis=0)))\n<Series: diameter>\n<Index>\nEarth              12756\nMars               6792\nJupiter            142984\n<<U7>              <int64>\n\n>>> next(iter(f.iter_series(axis=1)))\n<Series: Earth>\n<Index>\ndiameter        12756.0\nmass            5.97\n<<U8>           <float64>\n\n#end_Frame-iter_series()\n\n\n#start_Frame-iter_series().apply()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'), dtypes=dict(diameter=np.int64))\n\n>>> f.iter_series(axis=0).apply(lambda s: s.mean())\n<Series>\n<Index>\ndiameter 54177.333333333336\nmass     634.8706666666667\n<<U8>    <float64>\n\n#end_Frame-iter_series().apply()\n\n\n#start_Frame-iter_series_items()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'))\n\n>>> [(k, v.mean()) for k, v in f.iter_series_items(axis=0)]\n[(\'diameter\', 54177.333333333336), (\'mass\', 634.8706666666667)]\n\n>>> [(k, v.max()) for k, v in f.iter_series_items(axis=1)]\n[(\'Earth\', 12756.0), (\'Mars\', 6792.0), (\'Jupiter\', 142984.0)]\n\n>>> f.iter_series_items(axis=0).apply(lambda k, v: v.mean() if k == \'diameter\' else v.sum())\n<Series>\n<Index>\ndiameter 54177.333333333336\nmass     1904.612\n<<U8>    <float64>\n\n#end_Frame-iter_series_items()\n\n\n#start_Frame-iter_group()\n>>> f = sf.Frame.from_dict(dict(mass=(0.33, 4.87, 5.97, 0.642), moons=(0, 0, 1, 2)), index=(\'Mercury\', \'Venus\', \'Earth\', \'Mars\'), dtypes=dict(moons=np.int64))\n>>> next(iter(f.iter_group(\'moons\')))\n<Frame>\n<Index> mass      moons   <<U5>\n<Index>\nMercury 0.33      0\nVenus   4.87      0\n<<U7>   <float64> <int64>\n>>> [x.shape for x in f.iter_group(\'moons\')]\n[(2, 2), (1, 2), (1, 2)]\n\n#end_Frame-iter_group()\n\n\n#start_Frame-iter_group_items()\n>>> f = sf.Frame.from_dict(dict(mass=(0.33, 4.87, 5.97, 0.642), moons=(0, 0, 1, 2)), index=(\'Mercury\', \'Venus\', \'Earth\', \'Mars\'))\n>>> [(k, v.index.values.tolist(), v[\'mass\'].mean()) for k, v in f.iter_group_items(\'moons\')]\n[(0, [\'Mercury\', \'Venus\'], 2.6), (1, [\'Earth\'], 5.97), (2, [\'Mars\'], 0.642)]\n\n#end_Frame-iter_group_items()\n\n\n\n#start_Frame-assign[]()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'), dtypes=dict(diameter=np.int64))\n>>> f\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nMars    6792     0.642\nJupiter 142984   1898.0\n<<U7>   <int64>  <float64>\n\n>>> f.assign[\'mass\'](f[\'mass\'] * .001)\n<Frame>\n<Index> diameter mass               <<U8>\n<Index>\nEarth   12756    0.00597\nMars    6792     0.000642\nJupiter 142984   1.8980000000000001\n<<U7>   <int64>  <float64>\n\n#end_Frame-assign[]()\n\n\n#start_Frame-assign.loc[]()\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 6792, 142984), mass=(5.97, 0.642, 1898)), index=(\'Earth\', \'Mars\', \'Jupiter\'), dtypes=dict(diameter=np.int64))\n\n>>> f.assign.loc[\'Mars\', \'mass\'](0)\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nMars    6792     0.0\nJupiter 142984   1898.0\n<<U7>   <int64>  <float64>\n\n>>> f.assign.loc[\'Mars\':, \'diameter\'](0)\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nMars    0        0.642\nJupiter 0        1898.0\n<<U7>   <int64>  <float64>\n\n>>> f.assign.loc[f[\'diameter\'] > 10000, \'mass\'](0)\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    0.0\nMars    6792     0.642\nJupiter 142984   0.0\n<<U7>   <int64>  <float64>\n\n#end_Frame-assign.loc[]()\n\n\n#start_Frame-drop[]\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 142984, 120536), temperature=(15, -110, -140)), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64, temperature=np.int64))\n\n>>> f\n<Frame>\n<Index> diameter temperature <<U11>\n<Index>\nEarth   12756    15\nJupiter 142984   -110\nSaturn  120536   -140\n<<U7>   <int64>  <int64>\n\n>>> f.drop[\'diameter\']\n<Frame>\n<Index> temperature <<U11>\n<Index>\nEarth   15\nJupiter -110\nSaturn  -140\n<<U7>   <int64>\n\n#end_Frame-drop[]\n\n\n#start_Frame-drop.loc[]\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 142984, 120536), temperature=(15, -110, -140)), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64, temperature=np.int64))\n>>> f\n<Frame>\n<Index> diameter temperature <<U11>\n<Index>\nEarth   12756    15\nJupiter 142984   -110\nSaturn  120536   -140\n<<U7>   <int64>  <int64>\n\n>>> f.drop.loc[f[\'temperature\'] < 0]\n<Frame>\n<Index> diameter temperature <<U11>\n<Index>\nEarth   12756    15\n<<U7>   <int64>  <int64>\n\n#end_Frame-drop.loc[]\n\n\n#start_Frame-drop.iloc[]\n>>> f = sf.Frame.from_dict(dict(diameter=(12756, 142984, 120536), temperature=(15, -110, -140)), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64, temperature=np.int64))\n>>> f\n<Frame>\n<Index> diameter temperature <<U11>\n<Index>\nEarth   12756    15\nJupiter 142984   -110\nSaturn  120536   -140\n<<U7>   <int64>  <int64>\n\n>>> f.drop.iloc[-1, -1]\n<Frame>\n<Index> diameter <<U11>\n<Index>\nEarth   12756\nJupiter 142984\n<<U7>   <int64>\n\n#end_Frame-drop.iloc[]\n\n\n#start_Frame-shape\n>>> f = sf.Frame.from_items(((\'diameter\', (12756, 142984, 120536)), (\'mass\', (5.97, 1898, 568))), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64))\n\n>>> f\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nJupiter 142984   1898.0\nSaturn  120536   568.0\n<<U7>   <int64>  <float64>\n\n>>> f.shape\n(3, 2)\n\n#end_Frame-shape\n\n\n#start_Frame-ndim\n>>> f = sf.Frame.from_items(((\'diameter\', (12756, 142984, 120536)), (\'mass\', (5.97, 1898, 568))), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64))\n>>> f\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nJupiter 142984   1898.0\nSaturn  120536   568.0\n<<U7>   <int64>  <float64>\n\n>>> f.ndim\n2\n\n#end_Frame-ndim\n\n\n#start_Frame-size\n>>> f = sf.Frame.from_items(((\'diameter\', (12756, 142984, 120536)), (\'mass\', (5.97, 1898, 568))), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64))\n>>> f\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nJupiter 142984   1898.0\nSaturn  120536   568.0\n<<U7>   <int64>  <float64>\n\n>>> f.size\n6\n\n#end_Frame-size\n\n\n#start_Frame-nbytes\n>>> f = sf.Frame.from_items(((\'diameter\', (12756, 142984, 120536)), (\'mass\', (5.97, 1898, 568))), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64))\n>>> f\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nJupiter 142984   1898.0\nSaturn  120536   568.0\n<<U7>   <int64>  <float64>\n\n>>> f.nbytes\n48\n\n#end_Frame-nbytes\n\n\n#start_Frame-dtypes\n>>> f = sf.Frame.from_items(((\'diameter\', (12756, 142984, 120536)), (\'mass\', (5.97, 1898, 568))), index=(\'Earth\', \'Jupiter\', \'Saturn\'), dtypes=dict(diameter=np.int64))\n\n>>> f\n<Frame>\n<Index> diameter mass      <<U8>\n<Index>\nEarth   12756    5.97\nJupiter 142984   1898.0\nSaturn  120536   568.0\n<<U7>   <int64>  <float64>\n\n>>> f.dtypes\n<Series>\n<Index>\ndiameter int64\nmass     float64\n<<U8>    <object>\n\n#end_Frame-dtypes\n\n\n#start_Frame-[]\n>>> index = (\'Mercury\', \'Venus\', \'Earth\', \'Mars\')\n>>> columns = (\'diameter\', \'gravity\', \'temperature\')\n>>> records = ((4879, 3.7, 167), (12104, 8.9, 464), (12756, 9.8, 15), (6792, 3.7, -65))\n>>> f = sf.Frame.from_records(records, index=index, columns=columns, dtypes=dict(diameter=np.int64, temperature=np.int64))\n>>> f\n<Frame>\n<Index> diameter gravity   temperature <<U11>\n<Index>\nMercury 4879     3.7       167\nVenus   12104    8.9       464\nEarth   12756    9.8       15\nMars    6792     3.7       -65\n<<U7>   <int64>  <float64> <int64>\n\n>>> f[\'gravity\']\n<Series: gravity>\n<Index>\nMercury           3.7\nVenus             8.9\nEarth             9.8\nMars              3.7\n<<U7>             <float64>\n>>> f[\'gravity\':]\n<Frame>\n<Index> gravity   temperature <<U11>\n<Index>\nMercury 3.7       167\nVenus   8.9       464\nEarth   9.8       15\nMars    3.7       -65\n<<U7>   <float64> <int64>\n>>> f[[\'diameter\', \'temperature\']]\n<Frame>\n<Index> diameter temperature <<U11>\n<Index>\nMercury 4879     167\nVenus   12104    464\nEarth   12756    15\nMars    6792     -65\n<<U7>   <int64>  <int64>\n\n#end_Frame-[]\n\n\n#start_Frame-loc[]\n>>> index = (\'Mercury\', \'Venus\', \'Earth\', \'Mars\')\n>>> columns = (\'diameter\', \'gravity\', \'temperature\')\n>>> records = ((4879, 3.7, 167), (12104, 8.9, 464), (12756, 9.8, 15), (6792, 3.7, -65))\n>>> f = sf.Frame.from_records(records, index=index, columns=columns, dtypes=dict(diameter=np.int64, temperature=np.int64))\n>>> f\n<Frame>\n<Index> diameter gravity   temperature <<U11>\n<Index>\nMercury 4879     3.7       167\nVenus   12104    8.9       464\nEarth   12756    9.8       15\nMars    6792     3.7       -65\n<<U7>   <int64>  <float64> <int64>\n\n>>> f.loc[\'Earth\', \'temperature\']\n15\n>>> f.loc[\'Earth\':, \'temperature\']\n<Series: temperature>\n<Index>\nEarth                 15\nMars                  -65\n<<U7>                 <int64>\n>>> f.loc[f[\'temperature\'] > 100, \'diameter\']\n<Series: diameter>\n<Index>\nMercury            4879\nVenus              12104\n<<U7>              <int64>\n>>> f.loc[sf.ILoc[-1], [\'gravity\', \'temperature\']]\n<Series: Mars>\n<Index>\ngravity        3.7\ntemperature    -65.0\n<<U11>         <float64>\n\n#end_Frame-loc[]\n\n\n#start_Frame-iloc[]\n>>> index = (\'Mercury\', \'Venus\', \'Earth\', \'Mars\')\n>>> columns = (\'diameter\', \'gravity\', \'temperature\')\n>>> records = ((4879, 3.7, 167), (12104, 8.9, 464), (12756, 9.8, 15), (6792, 3.7, -65))\n>>> f = sf.Frame.from_records(records, index=index, columns=columns, dtypes=dict(diameter=np.int64, temperature=np.int64))\n>>> f\n<Frame>\n<Index> diameter gravity   temperature <<U11>\n<Index>\nMercury 4879     3.7       167\nVenus   12104    8.9       464\nEarth   12756    9.8       15\nMars    6792     3.7       -65\n<<U7>   <int64>  <float64> <int64>\n\n>>> f.iloc[-2:, -1]\n<Series: temperature>\n<Index>\nEarth                 15\nMars                  -65\n<<U7>                 <int64>\n\n#end_Frame-iloc[]\n\n#-------------------------------------------------------------------------------\n# FrameGO\n\n#start_FrameGO-interface\n>>> sf.FrameGO.interface.loc[sf.FrameGO.interface.index.via_str.startswith(\'drop\')]\n<Frame: FrameGO>\n<Index>                              cls_name group    doc                  <<U18>\n<Index: signature>\ndrop_duplicated(*, axis, exclude_... FrameGO  Method   Return a Frame wi...\ndropna(axis, condition)              FrameGO  Method   Return a new Fram...\ndrop[key]                            FrameGO  Selector Label-based selec...\ndrop.iloc[key]                       FrameGO  Selector\ndrop.loc[key]                        FrameGO  Selector\n<<U94>                               <<U7>    <<U17>   <<U83>\n\n#end_FrameGO-interface\n\n\n#-------------------------------------------------------------------------------\n# Bus\n\n#start_Bus-interface\n<Frame: Bus>\n<Index>                   cls_name group    doc    <<U18>\n<Index: signature>\nto_hdf5(fp, config)       Bus      Exporter\nto_sqlite(fp, config)     Bus      Exporter\nto_xlsx(fp, config)       Bus      Exporter\nto_zip_csv(fp, config)    Bus      Exporter\nto_zip_pickle(fp, config) Bus      Exporter\nto_zip_tsv(fp, config)    Bus      Exporter\n<<U34>                    <<U3>    <<U15>   <<U83>\n\n#end_Bus-interface\n\n\n\n\n#-------------------------------------------------------------------------------\n# Index\n\n#start_Index-interface\n>>> sf.Index.interface.loc[sf.Index.interface.index.via_str.startswith(\'re\')]\n<Frame: Index>\n<Index>            cls_name group  doc                  <<U18>\n<Index: signature>\nrelabel(mapper)    Index    Method Return a new Inde...\nrename(name)       Index    Method Return a new Fram...\n<<U68>             <<U5>    <<U17> <<U83>\n\n#end_Index-interface\n\n\n#start_Index-__init__()\n>>> sf.Index((\'Mercury\', \'Mars\'), dtype=object)\n<Index>\nMercury\nMars\n<object>\n\n>>> sf.Index(name[:2].upper() for name in (\'Mercury\', \'Mars\'))\n<Index>\nME\nMA\n<<U2>\n\n#end_Index-__init__()\n\n\n#start_Index-relabel()\n>>> index = sf.Index((\'Venus\', \'Saturn\', \'Neptune\'))\n>>> index.relabel({\'Venus\': \'Mars\'})\n<Index>\nMars\nSaturn\nNeptune\n<<U7>\n\n>>> index = sf.Index((\'Venus\', \'Saturn\', \'Neptune\'))\n>>> index.relabel({\'Neptune\': \'Uranus\'})\n<Index>\nVenus\nSaturn\nUranus\n<<U6>\n\n>>> index.relabel(lambda x: x[:2].upper())\n<Index>\nVE\nSA\nNE\n<<U2>\n\n#end_Index-relabel()\n\n\n#-------------------------------------------------------------------------------\n# IndexGO\n\n#start_IndexGO-interface\n>>> sf.IndexGO.interface.loc[sf.IndexGO.interface.index.via_str.startswith(\'to_\')]\n<Frame: IndexGO>\n<Index>                              cls_name group    doc                  <<U18>\n<Index: signature>\nto_html(config)                      IndexGO  Exporter Return an HTML ta...\nto_html_datatables(fp, *, show, c... IndexGO  Exporter Return a complete...\nto_pandas()                          IndexGO  Exporter Return a Pandas I...\nto_series()                          IndexGO  Exporter Return a Series w...\n<<U68>                               <<U7>    <<U17>   <<U83>\n\n#end_IndexGO-interface\n\n\n#start_IndexGO-append()\n>>> a = sf.IndexGO((\'Uranus\', \'Neptune\'))\n>>> a.append(\'Pluto\')\n>>> a\n<IndexGO>\nUranus\nNeptune\nPluto\n<<U7>\n\n#end_IndexGO-append()\n\n#-------------------------------------------------------------------------------\n# IndexHierarchy\n\n#start_IndexHierarchy-interface\n>>> sf.IndexHierarchy.interface.loc[sf.IndexHierarchy.interface.index.via_str.startswith(\'from_\')]\n<Frame: IndexHierarchy>\n<Index>                              cls_name       group       doc                  <<U18>\n<Index: signature>\nfrom_index_items(items, *, index_... IndexHierarchy Constructor Given an iterable...\nfrom_labels(labels, *, name, reor... IndexHierarchy Constructor Construct an Inde...\nfrom_labels_delimited(labels, *, ... IndexHierarchy Constructor Construct an Inde...\nfrom_pandas(value)                   IndexHierarchy Constructor Given a Pandas in...\nfrom_product(*, name, *levels)       IndexHierarchy Constructor Given groups of i...\nfrom_tree(tree, *, name)             IndexHierarchy Constructor Convert into a In...\n<<U68>                               <<U14>         <<U17>      <<U83>\n\n#end_IndexHierarchy-interface\n\n\n\n#-------------------------------------------------------------------------------\n# IndexHierarchyGO\n\n#start_IndexHierarchyGO-interface\n>>> sf.IndexHierarchyGO.interface.loc[sf.IndexHierarchyGO.interface.index.via_str.startswith(\'re\')]\n<Frame: IndexHierarchyGO>\n<Index>                   cls_name         group  doc                  <<U18>\n<Index: signature>\nrehierarch(depth_map)     IndexHierarchyGO Method Return a new Inde...\nrelabel(mapper)           IndexHierarchyGO Method Return a new Inde...\nrename(name)              IndexHierarchyGO Method Return a new Fram...\n<<U68>                    <<U16>           <<U17> <<U83>\n\n#end_IndexHierarchyGO-interface\n\n\n#-------------------------------------------------------------------------------\n# IndexYear\n\n#start_IndexYear-interface\n>>> sf.IndexYear.interface.loc[sf.IndexYear.interface.index.via_str.startswith(\'from_\')]\n<Frame: IndexYear>\n<Index>                              cls_name  group       doc                  <<U18>\n<Index: signature>\nfrom_date_range(start, stop, step... IndexYear Constructor Get an IndexYearM...\nfrom_labels(labels, *, name)         IndexYear Constructor Construct an Inde...\nfrom_pandas(value)                   IndexYear Constructor Given a Pandas in...\nfrom_year_month_range(start, stop... IndexYear Constructor Get an IndexYearM...\nfrom_year_range(start, stop, step... IndexYear Constructor Get an IndexDate ...\n<<U68>                               <<U9>     <<U17>      <<U83>\n\n#end_IndexYear-interface\n\n#-------------------------------------------------------------------------------\n# IndexYearGO\n\n#start_IndexYearGO-interface\n>>> sf.IndexYearGO.interface.loc[sf.IndexYearGO.interface.index.via_str.startswith(\'from_\')]\n<Frame: IndexYearGO>\n<Index>                              cls_name    group       doc                  <<U18>\n<Index: signature>\nfrom_date_range(start, stop, step... IndexYearGO Constructor Get an IndexYearM...\nfrom_labels(labels, *, name)         IndexYearGO Constructor Construct an Inde...\nfrom_pandas(value)                   IndexYearGO Constructor Given a Pandas in...\nfrom_year_month_range(start, stop... IndexYearGO Constructor Get an IndexYearM...\nfrom_year_range(start, stop, step... IndexYearGO Constructor Get an IndexDate ...\n<<U68>                               <<U11>      <<U17>      <<U83>\n\n#end_IndexYearGO-interface\n\n\n#-------------------------------------------------------------------------------\n# IndexYearMonth\n\n#start_IndexYearMonth-interface\n>>> sf.IndexYearMonthGO.interface.loc[sf.IndexYearMonthGO.interface.index.via_str.startswith(\'from_\')]\n<Frame: IndexYearMonthGO>\n<Index>                              cls_name         group       doc                  <<U18>\n<Index: signature>\nfrom_date_range(start, stop, step... IndexYearMonthGO Constructor Get an IndexYearM...\nfrom_labels(labels, *, name)         IndexYearMonthGO Constructor Construct an Inde...\nfrom_pandas(value)                   IndexYearMonthGO Constructor Given a Pandas in...\nfrom_year_month_range(start, stop... IndexYearMonthGO Constructor Get an IndexYearM...\nfrom_year_range(start, stop, step... IndexYearMonthGO Constructor Get an IndexYearM...\n<<U68>                               <<U16>           <<U17>      <<U83>\n\n#end_IndexYearMonth-interface\n\n\n#-------------------------------------------------------------------------------\n# IndexYearMonthGO\n\n#start_IndexYearMonthGO-interface\n>>> sf.IndexYearMonthGO.interface.loc[sf.IndexYearMonthGO.interface.index.via_str.startswith(\'from_\')]\n<Frame: IndexYearMonthGO>\n<Index>                              cls_name         group       doc                  <<U18>\n<Index: signature>\nfrom_date_range(start, stop, step... IndexYearMonthGO Constructor Get an IndexYearM...\nfrom_labels(labels, *, name)         IndexYearMonthGO Constructor Construct an Inde...\nfrom_pandas(value)                   IndexYearMonthGO Constructor Given a Pandas in...\nfrom_year_month_range(start, stop... IndexYearMonthGO Constructor Get an IndexYearM...\nfrom_year_range(start, stop, step... IndexYearMonthGO Constructor Get an IndexYearM...\n<<U68>                               <<U16>           <<U17>      <<U83>\n\n#end_IndexYearMonthGO-interface\n\n\n#-------------------------------------------------------------------------------\n# IndexDate\n\n#start_IndexDate-interface\n>>> sf.IndexDate.interface.loc[sf.IndexDate.interface.index.via_str.startswith(\'from_\')]\n<Frame: IndexDate>\n<Index>                              cls_name  group       doc                  <<U18>\n<Index: signature>\nfrom_date_range(start, stop, step... IndexDate Constructor Get an IndexDate ...\nfrom_labels(labels, *, name)         IndexDate Constructor Construct an Inde...\nfrom_pandas(value)                   IndexDate Constructor Given a Pandas in...\nfrom_year_month_range(start, stop... IndexDate Constructor Get an IndexDate ...\nfrom_year_range(start, stop, step... IndexDate Constructor Get an IndexDate ...\n<<U68>                               <<U9>     <<U17>      <<U83>\n\n#end_IndexDate-interface\n\n\n#-------------------------------------------------------------------------------\n# IndexDateGO\n\n#start_IndexDateGO-interface\n>>> sf.IndexDateGO.interface.loc[sf.IndexDateGO.interface.index.via_str.startswith(\'from_\')]\n<Frame: IndexDateGO>\n<Index>                              cls_name    group       doc                  <<U18>\n<Index: signature>\nfrom_date_range(start, stop, step... IndexDateGO Constructor Get an IndexDate ...\nfrom_labels(labels, *, name)         IndexDateGO Constructor Construct an Inde...\nfrom_pandas(value)                   IndexDateGO Constructor Given a Pandas in...\nfrom_year_month_range(start, stop... IndexDateGO Constructor Get an IndexDate ...\nfrom_year_range(start, stop, step... IndexDateGO Constructor Get an IndexDate ...\n<<U68>                               <<U11>      <<U17>      <<U83>\n\n#end_IndexDateGO-interface\n\n\n\n\n#-------------------------------------------------------------------------------\n# restore initial configuration\n>>> sf.DisplayActive.set(_display_config_active)\n\n\n\'\'\'\n\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.test.test_case import skip_win\n\n@skip_win\nclass TestUnit(doctest.DocTestCase, TestCase):\n\n    @staticmethod\n    def get_readme_fp() -> str:\n\n        target_fn = \'README.rst\'\n\n        fp = os.path.join(os.getcwd(), __file__)\n        if not os.path.exists(fp):\n            raise RuntimeError(\'got bad module path\', fp)\n\n        while len(fp) > len(os.sep):\n            fp = os.path.dirname(fp)\n            if target_fn in os.listdir(fp):\n                return os.path.join(fp, target_fn)\n\n        raise RuntimeError(\'could not find target fn\', target_fn)\n\n    @classmethod\n    def get_readme_str(cls) -> str:\n        # mutate the README\n        fp_alt = cls.get_test_input(\'jph_photos.txt\')\n\n        readme_fp = cls.get_readme_fp()\n        with open(readme_fp) as f:\n            readme_str = f.read()\n\n        # update display config to remove colors\n        readme_str = \'\'\'\n>>> _display_config = sf.DisplayActive.get()\n>>> sf.DisplayActive.update(type_color=False)\n>>>\n        \'\'\' + readme_str\n\n        # inject content from local files\n        src = "">>> frame = sf.Frame.from_json_url(\'https://jsonplaceholder.typicode.com/photos\', dtypes=dict(albumId=np.int64, id=np.int64))""\n\n        # using a raw string to avoid unicode decoding issues on windows\n        dst = "">>> frame = sf.Frame.from_tsv(r\'%s\', dtypes=dict(albumId=np.int64, id=np.int64), encoding=\'utf-8\')"" % fp_alt\n\n        if src not in readme_str:\n            raise RuntimeError(\'did not find expected string\')\n\n        readme_str = readme_str.replace(src, dst)\n\n        # restore active config\n        readme_str = readme_str + \'\'\'\n>>> sf.DisplayActive.set(_display_config)\n        \'\'\'\n\n        return readme_str\n\n    @staticmethod\n    def update_readme(source: object) -> None:\n        target = ""sf.Frame.from_json_url(\'https://jsonplaceholder.typicode.com/photos\')""\n\n\n    def __init__(self, *args: tp.Any, **kwargs: tp.Any) -> None:\n\n        doctest_str = \'\\n\'.join((api_example_str, self.get_readme_str()))\n\n        sample = doctest.DocTestParser().get_doctest(\n                doctest_str,\n                globs={},\n                name=\'test_doc\',\n                filename=None,\n                lineno=None)\n\n        super().__init__(sample, **kwargs)\n\n\nif __name__ == ""__main__"":\n    import unittest\n    unittest.main()\n\n\n\n\n\n# UNUSED\n\n\n# #start_Frame-from_records()\n# >>> sf.Frame.from_records(((-65, 227.9), (-200, 4495.1)), columns=(\'temperature\', \'distance\'), index=(\'Mars\', \'Neptune\'), dtypes=dict(temperature=np.int64))\n# <Frame>\n# <Index> temperature distance  <<U11>\n# <Index>\n# Mars    -65         227.9\n# Neptune -200        4495.1\n# <<U7>   <int64>     <float64>\n# #end_Frame-from_records()\n'"
static_frame/test/unit/test_frame.py,262,"b'import unittest\nfrom collections import OrderedDict\nimport itertools as it\nfrom collections import namedtuple\nfrom io import StringIO\nimport string\nimport pickle\nimport sqlite3\nimport datetime\n\nimport numpy as np\nimport typing as tp\n\nimport static_frame as sf\n\n# assuming located in the same directory\nfrom static_frame import Index\n# from static_frame import IndexGO\nfrom static_frame import IndexHierarchy\nfrom static_frame import IndexHierarchyGO\nfrom static_frame import IndexYearMonth\nfrom static_frame import IndexYearGO\nfrom static_frame import IndexYear\nfrom static_frame import IndexDate\n\nfrom static_frame import Series\nfrom static_frame import Frame\nfrom static_frame import FrameGO\nfrom static_frame import TypeBlocks\n# from static_frame import Display\nfrom static_frame import mloc\nfrom static_frame import ILoc\nfrom static_frame import HLoc\nfrom static_frame import DisplayConfig\nfrom static_frame import IndexAutoFactory\n\nfrom static_frame.core.store_xlsx import StoreXLSX\nfrom static_frame.core.store import StoreConfig\nfrom static_frame.core.frame import FrameAssign\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.test.test_case import skip_win\nfrom static_frame.test.test_case import temp_file\nfrom static_frame.core.exception import ErrorInitFrame\nfrom static_frame.core.exception import AxisInvalid\n\nnan = np.nan\n\n\nclass TestUnit(TestCase):\n\n    def test_frame_slotted_a(self) -> None:\n\n        f1 = Frame.from_element(1, index=(1,2), columns=(3,4,5))\n\n        with self.assertRaises(AttributeError):\n            f1.g = 30 #pylint: disable=E0237\n        with self.assertRaises(AttributeError):\n            f1.__dict__ #pylint: disable=W0104\n\n\n    def test_frame_init_a(self) -> None:\n\n        f = Frame.from_dict(OrderedDict([(\'a\', (1,2)), (\'b\', (3,4))]), index=(\'x\', \'y\'))\n        self.assertEqual(f.to_pairs(0),\n                ((\'a\', ((\'x\', 1), (\'y\', 2))), (\'b\', ((\'x\', 3), (\'y\', 4))))\n                )\n\n        f = Frame.from_dict(OrderedDict([(\'b\', (3,4)), (\'a\', (1,2))]), index=(\'x\', \'y\'))\n        self.assertEqual(f.to_pairs(0),\n                ((\'b\', ((\'x\', 3), (\'y\', 4))), (\'a\', ((\'x\', 1), (\'y\', 2)))))\n\n\n    def test_frame_init_b(self) -> None:\n        # test unusual instantiation cases\n\n        # create a frame with a single value\n        f1 = Frame.from_element(1, index=(1,2), columns=(3,4,5))\n        self.assertEqual(f1.to_pairs(0),\n                ((3, ((1, 1), (2, 1))), (4, ((1, 1), (2, 1))), (5, ((1, 1), (2, 1))))\n                )\n\n        # with columns not defined, we create a DF with just an index\n        f2 = FrameGO(index=(1,2))\n        f2[\'a\'] = (-1, -1)\n        self.assertEqual(f2.to_pairs(0),\n                ((\'a\', ((1, -1), (2, -1))),)\n                )\n\n        # with columns and index defined, we fill the value even if None\n        f3 = Frame.from_element(None, index=(1,2), columns=(3,4,5))\n        self.assertEqual(f3.to_pairs(0),\n                ((3, ((1, None), (2, None))), (4, ((1, None), (2, None))), (5, ((1, None), (2, None)))))\n\n        # auto populated index/columns based on shape\n        f4 = Frame.from_records([[1,2], [3,4], [5,6]])\n        self.assertEqual(f4.to_pairs(0),\n                ((0, ((0, 1), (1, 3), (2, 5))), (1, ((0, 2), (1, 4), (2, 6))))\n                )\n        self.assertTrue(f4._index._map is None)\n        self.assertTrue(f4._columns._map is None)\n\n\n    def test_frame_init_c(self) -> None:\n        f = sf.FrameGO.from_dict(dict(color=(\'black\',)))\n        s = f[\'color\']\n        self.assertEqual(s.to_pairs(),\n                ((0, \'black\'),))\n\n    def test_frame_init_d(self) -> None:\n        a1 = np.array([[1, 2, 3], [4, 5, 6]])\n\n        f = sf.Frame(a1, own_data=True)\n        self.assertEqual(mloc(a1), f.mloc[0])\n\n    def test_frame_init_e(self) -> None:\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([4, 5, 6])\n\n        f = sf.Frame.from_dict(dict(a=a1, b=a2))\n\n    def test_frame_init_f(self) -> None:\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([4, 5, 6])\n\n        f = sf.Frame.from_dict(dict(a=a1, b=a2))\n\n        self.assertEqual(f.to_pairs(0),\n            ((\'a\', ((0, 1), (1, 2), (2, 3))), (\'b\', ((0, 4), (1, 5), (2, 6))))\n            )\n\n    def test_frame_init_g(self) -> None:\n\n        f1 = sf.Frame(index=tuple(\'abc\'))\n        self.assertEqual(f1.shape, (3, 0))\n\n        f2 = sf.Frame(columns=tuple(\'abc\'))\n        self.assertEqual(f2.shape, (0, 3))\n\n        f3 = sf.Frame()\n        self.assertEqual(f3.shape, (0, 0))\n\n    def test_frame_init_h(self) -> None:\n\n        f1 = sf.Frame(index=tuple(\'abc\'), columns=())\n        self.assertEqual(f1.shape, (3, 0))\n\n        f2 = sf.Frame(columns=tuple(\'abc\'), index=())\n        self.assertEqual(f2.shape, (0, 3))\n\n        f3 = sf.Frame(columns=(), index=())\n        self.assertEqual(f3.shape, (0, 0))\n\n\n    def test_frame_init_i(self) -> None:\n\n        f1 = sf.FrameGO(index=tuple(\'abc\'))\n        f1[\'x\'] = (3, 4, 5)\n        f1[\'y\'] = Series.from_dict(dict(b=10, c=11, a=12))\n\n        self.assertEqual(f1.to_pairs(0),\n            ((\'x\', ((\'a\', 3), (\'b\', 4), (\'c\', 5))), (\'y\', ((\'a\', 12), (\'b\', 10), (\'c\', 11)))))\n\n    def test_frame_init_j(self) -> None:\n        f1 = sf.Frame.from_element(\'q\', index=tuple(\'ab\'), columns=tuple(\'xy\'))\n        self.assertEqual(f1.to_pairs(0),\n            ((\'x\', ((\'a\', \'q\'), (\'b\', \'q\'))), (\'y\', ((\'a\', \'q\'), (\'b\', \'q\'))))\n            )\n\n    def test_frame_init_k(self) -> None:\n        # check that we got autoincrement indices if no col/index provided\n        f1 = Frame.from_records([[0, 1], [2, 3]])\n        self.assertEqual(f1.to_pairs(0), ((0, ((0, 0), (1, 2))), (1, ((0, 1), (1, 3)))))\n\n    def test_frame_init_m(self) -> None:\n        # cannot create a single element filled Frame specifying a shape (with index and columns) but not specifying a data value\n        with self.assertRaises(RuntimeError):\n            f1 = Frame(index=(3,4,5), columns=list(\'abv\'))\n\n    def test_frame_init_n(self) -> None:\n\n        f1 = Frame.from_element(None, index=(3,4,5), columns=())\n        self.assertEqual(f1.shape, (3, 0))\n\n    def test_frame_init_o(self) -> None:\n        f1 = Frame()\n        self.assertEqual(f1.shape, (0, 0))\n\n\n    def test_frame_init_p(self) -> None:\n\n        # raise when a data values ir provided but an axis is size zero\n\n        f1 = sf.Frame.from_element(\'x\', index=(1,2,3), columns=iter(()))\n        self.assertEqual(f1.shape, (3, 0))\n\n        f2 = sf.Frame.from_element(None, index=(1,2,3), columns=iter(()))\n        self.assertEqual(f2.shape, (3, 0))\n\n\n    def test_frame_init_q(self) -> None:\n\n        f1 = sf.Frame(index=(1,2,3), columns=iter(()))\n        self.assertEqual(f1.shape, (3, 0))\n        self.assertEqual(f1.to_pairs(0), ())\n\n\n    def test_frame_init_r(self) -> None:\n\n        f1 = sf.Frame(index=(), columns=iter(range(3)))\n\n        self.assertEqual(f1.shape, (0, 3))\n        self.assertEqual(f1.to_pairs(0),\n                ((0, ()), (1, ()), (2, ())))\n\n        # can create an un fillable Frame when using from_element\n        f1 = sf.Frame.from_element(\'x\', index=(), columns=iter(range(3)))\n\n    def test_frame_init_s(self) -> None:\n        # check that we got autoincrement indices if no col/index provided\n        f1 = Frame.from_records([[0, 1], [2, 3]],\n                index=IndexAutoFactory,\n                columns=IndexAutoFactory)\n\n        self.assertEqual(f1.to_pairs(0),\n                ((0, ((0, 0), (1, 2))), (1, ((0, 1), (1, 3))))\n                )\n\n        f2 = Frame.from_records([[0, 1], [2, 3]],\n                index=IndexAutoFactory,\n                columns=list(\'ab\')\n                )\n        self.assertEqual(\n                f2.to_pairs(0),\n                ((\'a\', ((0, 0), (1, 2))), (\'b\', ((0, 1), (1, 3))))\n                )\n\n    def test_frame_init_t(self) -> None:\n\n        # 3d array raises exception\n        a1 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\n        with self.assertRaises(RuntimeError):\n            f1 = Frame(a1)\n\n\n    def test_frame_init_u1(self) -> None:\n        # 3d array raises exception\n        a1 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n        with self.assertRaises(RuntimeError):\n            f1 = Frame(a1)\n\n\n    def test_frame_init_u2(self) -> None:\n\n        # NOTE: presently the inner lists get flattend when used in from records\n        a1 = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n        f1 = Frame.from_records(a1)\n\n        self.assertEqual(f1.to_pairs(0),\n                ((0, ((0, [1, 2]), (1, [5, 6]))), (1, ((0, [3, 4]), (1, [7, 8]))))\n                )\n\n\n    def test_frame_init_v(self) -> None:\n\n        s1 = Series([\'a\', \'b\', \'c\'])\n\n        with self.assertRaises(ErrorInitFrame):\n            f1 = Frame(s1)\n\n        with self.assertRaises(ErrorInitFrame):\n            f1 = Frame(dict(a=3, b=4))\n\n        with self.assertRaises(ErrorInitFrame):\n            f1 = Frame(None, index=range(3), columns=range(3))\n\n    def test_frame_init_w(self) -> None:\n\n        f1 = Frame.from_dict(dict(a=(1,2), b=(3,4)), index=(\'x\', \'y\'))\n        f2 = Frame(f1)\n        self.assertEqual(f1._blocks.mloc.tolist(), f2._blocks.mloc.tolist())\n        self.assertEqualFrames(f1, f2)\n\n        f3 = Frame(f1, index=IndexAutoFactory, columns=IndexAutoFactory)\n        self.assertEqual(f3.to_pairs(0),\n                ((0, ((0, 1), (1, 2))), (1, ((0, 3), (1, 4))))\n                )\n        self.assertEqual(f1._blocks.mloc.tolist(), f3._blocks.mloc.tolist())\n\n        f4 = FrameGO(f1, index=(\'p\', \'q\'))\n        f4[\'c\'] = None\n        self.assertEqual(f4.to_pairs(0),\n                ((\'a\', ((\'p\', 1), (\'q\', 2))), (\'b\', ((\'p\', 3), (\'q\', 4))), (\'c\', ((\'p\', None), (\'q\', None)))))\n        # first two values are stil equal\n        self.assertEqual(f1._blocks.mloc.tolist(), f4._blocks.mloc.tolist()[:2])\n\n        f5 = Frame(f4)\n        self.assertEqual(f5.to_pairs(0),\n                ((\'a\', ((\'p\', 1), (\'q\', 2))), (\'b\', ((\'p\', 3), (\'q\', 4))), (\'c\', ((\'p\', None), (\'q\', None))))\n                )\n        self.assertTrue(f5.columns.STATIC)\n\n    #---------------------------------------------------------------------------\n    def test_frame_init_index_constructor_a(self) -> None:\n\n        f1 = sf.Frame.from_element(\'q\',\n                index=[(\'a\', \'b\'), (1, 2)],\n                columns=tuple(\'xy\'),\n                index_constructor=IndexHierarchy.from_labels\n                )\n        self.assertTrue(isinstance(f1.index, IndexHierarchy))\n        self.assertEqual(f1.to_pairs(0),\n                ((\'x\', (((\'a\', \'b\'), \'q\'), ((1, 2), \'q\'))), (\'y\', (((\'a\', \'b\'), \'q\'), ((1, 2), \'q\'))))\n                )\n\n        with self.assertRaises(RuntimeError):\n            f1 = sf.Frame.from_element(\'q\',\n                    index=[(\'a\', \'b\'), (1, 2)],\n                    columns=tuple(\'xy\'),\n                    index_constructor=IndexHierarchyGO.from_labels\n                    )\n\n\n    def test_frame_init_columns_constructor_a(self) -> None:\n\n        # using from_priduct is awkard, as it does not take a single iterable of products, but multiple args; we can get around this with a simple lambda\n        f1 = sf.Frame.from_element(\'q\',\n                index=tuple(\'xy\'),\n                columns=[(\'a\', \'b\'), (1, 2)],\n                columns_constructor=lambda args: IndexHierarchy.from_product(*args)\n                )\n        self.assertTrue(isinstance(f1.columns, IndexHierarchy))\n        self.assertEqual(f1.to_pairs(0),\n                (((\'a\', 1), ((\'x\', \'q\'), (\'y\', \'q\'))), ((\'a\', 2), ((\'x\', \'q\'), (\'y\', \'q\'))), ((\'b\', 1), ((\'x\', \'q\'), (\'y\', \'q\'))), ((\'b\', 2), ((\'x\', \'q\'), (\'y\', \'q\'))))\n                )\n\n        with self.assertRaises(RuntimeError):\n            f1 = sf.Frame.from_element(\'q\',\n                index=tuple(\'xy\'),\n                columns=[(\'a\', \'b\'), (1, 2)],\n                columns_constructor=lambda args: IndexHierarchyGO.from_product(*args)\n                )\n\n\n    def test_frame_init_iter(self) -> None:\n\n        f1 = Frame.from_element(None, index=iter(range(3)), columns=(""A"",))\n        self.assertEqual(\n            f1.to_pairs(0),\n            ((\'A\', ((0, None), (1, None), (2, None))),)\n        )\n\n        f2 = Frame.from_element(None, index=(""A"",), columns=iter(range(3)))\n        self.assertEqual(\n            f2.to_pairs(0),\n            ((0, ((\'A\', None),)), (1, ((\'A\', None),)), (2, ((\'A\', None),)))\n        )\n\n    def test_frame_values_a(self) -> None:\n        f = sf.Frame.from_records([[3]])\n        self.assertEqual(f.values.tolist(), [[3]])\n\n\n    def test_frame_values_b(self) -> None:\n        f = sf.Frame(np.array([[3, 2, 1]]))\n        self.assertEqual(f.values.tolist(), [[3, 2, 1]])\n\n    def test_frame_values_c(self) -> None:\n        f = sf.Frame(np.array([[3], [2], [1]]))\n        self.assertEqual(f.values.tolist(), [[3], [2], [1]])\n\n\n\n    def test_frame_from_series_a(self) -> None:\n        s1 = Series((False, True, False), index=tuple(\'abc\'))\n        f1 = Frame.from_series(s1, name=\'foo\')\n\n        self.assertEqual(f1.to_pairs(0),\n                ((None, ((\'a\', False), (\'b\', True), (\'c\', False))),))\n\n    def test_frame_from_series_b(self) -> None:\n        s1 = Series((False, True, False), index=tuple(\'abc\'), name=\'2018-05\')\n        f1 = Frame.from_series(s1, name=\'foo\', columns_constructor=IndexYearMonth)\n        self.assertEqual(f1.columns.__class__, IndexYearMonth)\n        self.assertEqual(f1.to_pairs(0),\n                ((np.datetime64(\'2018-05\'), ((\'a\', False), (\'b\', True), (\'c\', False))),))\n\n    #---------------------------------------------------------------------------\n    def test_frame_from_element_a(self) -> None:\n\n        f1 = Frame.from_element(0, index=(\'a\', \'b\'), columns=(\'x\', \'y\', \'z\'))\n        self.assertEqual(f1.shape, (2, 3))\n        self.assertEqual(f1.to_pairs(0),\n                ((\'x\', ((\'a\', 0), (\'b\', 0))), (\'y\', ((\'a\', 0), (\'b\', 0))), (\'z\', ((\'a\', 0), (\'b\', 0)))))\n\n    def test_frame_from_element_b(self) -> None:\n\n        f1 = Frame.from_element(\'2019\',\n                index=(\'a\', \'b\'),\n                columns=(\'x\', \'y\', \'z\'),\n                dtype=\'datetime64[Y]\'\n                )\n        self.assertEqual(f1.shape, (2, 3))\n        self.assertEqual(f1.to_pairs(0),\n                ((\'x\', ((\'a\', np.datetime64(\'2019\')), (\'b\', np.datetime64(\'2019\')))), (\'y\', ((\'a\', np.datetime64(\'2019\')), (\'b\', np.datetime64(\'2019\')))), (\'z\', ((\'a\', np.datetime64(\'2019\')), (\'b\', np.datetime64(\'2019\')))))\n        )\n\n    def test_frame_from_element_c(self) -> None:\n        # not an error to create 0-sized frames\n        f1 = Frame.from_element(\'2019\',\n                index=(\'a\', \'b\'),\n                columns=(),\n                )\n        self.assertEqual(f1.shape, (2, 0))\n\n        f2 = Frame.from_element(\'2019\',\n                index=(),\n                columns=(\'x\', \'y\', \'z\'),\n                )\n        self.assertEqual(f2.shape, (0, 3))\n\n\n    def test_frame_from_element_d(self) -> None:\n        idx1 = Index((\'a\', \'b\'))\n        idx2 = Index((3, 4))\n        f1 = Frame.from_element(\'x\',\n                index=idx1,\n                columns=idx2,\n                own_index=True,\n                own_columns=True\n                )\n        self.assertTrue(id(idx1) == id(f1.index))\n        self.assertTrue(id(idx2) == id(f1.columns))\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_from_elements_a(self) -> None:\n        f1 = Frame.from_elements([\'a\', 3, \'b\'])\n        self.assertEqual(f1.to_pairs(0),\n                ((0, ((0, \'a\'), (1, 3), (2, \'b\'))),)\n                )\n\n        f2 = Frame.from_elements([\'a\', 3, \'b\'], index=tuple(\'xyz\'))\n        self.assertEqual(f2.to_pairs(0),\n                ((0, ((\'x\', \'a\'), (\'y\', 3), (\'z\', \'b\'))),))\n\n        f3 = Frame.from_elements([\'a\', 3, \'b\'], index=tuple(\'xyz\'), columns=(\'p\',))\n        self.assertEqual(f3.to_pairs(0),\n                ((\'p\', ((\'x\', \'a\'), (\'y\', 3), (\'z\', \'b\'))),))\n\n\n    def test_frame_from_elements_b(self) -> None:\n\n        f1 = Frame.from_elements([5, False, \'X\'])\n        self.assertEqual(f1.to_pairs(0),\n                ((0, ((0, 5), (1, False), (2, \'X\'))),)\n                )\n\n        f2 = Frame.from_elements([5, False, \'X\'], columns=(\'a\', \'b\'))\n        self.assertEqual(f2.to_pairs(0),\n                ((\'a\', ((0, 5), (1, False), (2, \'X\'))), (\'b\', ((0, 5), (1, False), (2, \'X\'))))\n                )\n\n\n    def test_frame_from_elements_c(self) -> None:\n        idx1 = Index((\'a\', \'b\'))\n        idx2 = Index((3, 4))\n        f1 = Frame.from_elements((10, 20),\n                index=idx1,\n                columns=idx2,\n                own_index=True,\n                own_columns=True\n                )\n\n        self.assertEqual(f1.to_pairs(0),\n                ((3, ((\'a\', 10), (\'b\', 20))), (4, ((\'a\', 10), (\'b\', 20))))\n                )\n        self.assertTrue(id(idx1) == id(f1.index))\n        self.assertTrue(id(idx2) == id(f1.columns))\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_from_pairs_a(self) -> None:\n\n        frame = Frame.from_items(sorted(dict(a=[3,4,5], b=[6,3,2]).items()))\n        self.assertEqual(\n            list((k, list(v.items())) for k, v in frame.items()),\n            [(\'a\', [(0, 3), (1, 4), (2, 5)]), (\'b\', [(0, 6), (1, 3), (2, 2)])])\n\n        frame = Frame.from_items(OrderedDict(((\'b\', [6,3,2]), (\'a\', [3,4,5]))).items())\n        self.assertEqual(list((k, list(v.items())) for k, v in frame.items()),\n            [(\'b\', [(0, 6), (1, 3), (2, 2)]), (\'a\', [(0, 3), (1, 4), (2, 5)])])\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_from_pandas_a(self) -> None:\n        import pandas as pd\n\n        df = pd.DataFrame(dict(a=(1,2), b=(3,4)))\n        df.name = \'foo\'\n\n        f = Frame.from_pandas(df)\n        self.assertEqual(f.to_pairs(0),\n                ((\'a\', ((0, 1), (1, 2))), (\'b\', ((0, 3), (1, 4))))\n                )\n\n\n    def test_frame_from_pandas_b(self) -> None:\n        import pandas as pd\n\n        df = pd.DataFrame(dict(a=(1,2), b=(False, True)), index=(\'x\', \'y\'))\n\n        f = Frame.from_pandas(df)\n\n        self.assertEqual(f.to_pairs(0),\n                ((\'a\', ((\'x\', 1), (\'y\', 2))), (\'b\', ((\'x\', False), (\'y\', True))))\n                )\n\n        with self.assertRaises(Exception):\n            f[\'c\'] = 0 #pylint: disable=E1137\n\n\n    def test_frame_from_pandas_c(self) -> None:\n        import pandas as pd\n\n        df = pd.DataFrame(dict(a=(1,2), b=(False, True)), index=(\'x\', \'y\'))\n\n        f = FrameGO.from_pandas(df)\n        f[\'c\'] = -1\n\n        self.assertEqual(f.to_pairs(0),\n                ((\'a\', ((\'x\', 1), (\'y\', 2))), (\'b\', ((\'x\', False), (\'y\', True))), (\'c\', ((\'x\', -1), (\'y\', -1)))))\n\n    def test_frame_from_pandas_d(self) -> None:\n        import pandas as pd\n\n        df = pd.DataFrame(dict(a=(1,2), b=(True, False)))\n        df.name = \'foo\'\n\n        f = Frame.from_pandas(df, own_data=True)\n\n        self.assertEqual(f.to_pairs(0),\n                ((\'a\', ((0, 1), (1, 2))), (\'b\', ((0, True), (1, False))))\n                )\n\n    def test_frame_from_pandas_e(self) -> None:\n        import pandas as pd\n\n        df = pd.DataFrame(dict(a=(1,2), b=(3, 4)), index=(\'x\', \'y\'))\n        f = Frame.from_pandas(df, own_data=True, consolidate_blocks=True)\n        self.assertEqual(f.to_pairs(0),\n                ((\'a\', ((\'x\', 1), (\'y\', 2))), (\'b\', ((\'x\', 3), (\'y\', 4)))))\n\n        self.assertEqual(f._blocks.shapes.tolist(), [(2, 2)])\n\n    def test_frame_from_pandas_f(self) -> None:\n        import pandas as pd\n\n        df = pd.DataFrame(dict(a=(1,2), b=(\'3\',\'4\'), c=(1.5,2.5), d=(\'a\',\'b\')))\n\n        if hasattr(df, \'convert_dtypes\'):\n            df = df.convert_dtypes()\n\n        df.name = \'foo\'\n\n        f = Frame.from_pandas(df)\n        self.assertEqual(f.to_pairs(0),\n                ((\'a\', ((0, 1), (1, 2))),\n                 (\'b\', ((0, \'3\'), (1, \'4\'))),\n                 (\'c\', ((0, 1.5), (1, 2.5))),\n                 (\'d\', ((0, \'a\'), (1, \'b\'))))\n                )\n\n\n    @skip_win #type: ignore\n    def test_frame_from_pandas_g(self) -> None:\n        import pandas as pd\n\n        df = pd.DataFrame(dict(a=(1,2), b=(1.5,2.5), c=(\'3\',\'4\'), d=(\'a\',\'b\')))\n\n        if hasattr(df, \'convert_dtypes\'):\n            df = df.convert_dtypes()\n            f = Frame.from_pandas(df)\n            self.assertEqual(f.dtypes.to_pairs(),\n                    ((\'a\', np.dtype(\'int64\')), (\'b\', np.dtype(\'float64\')), (\'c\', np.dtype(\'<U1\')), (\'d\', np.dtype(\'<U1\'))))\n        else:\n            f = Frame.from_pandas(df)\n            self.assertEqual(f.dtypes.to_pairs(),\n                    ((\'a\', np.dtype(\'int64\')), (\'b\', np.dtype(\'float64\')), (\'c\', np.dtype(\'O\')), (\'d\', np.dtype(\'O\'))))\n\n    def test_frame_from_pandas_h(self) -> None:\n        import pandas as pd\n\n        df = pd.DataFrame(\n                dict(a=(False, True), b=(True, False), c=(\'q\',\'r\'), d=(\'a\',\'b\'), e=(False, False)))\n\n        if hasattr(df, \'convert_dtypes\'):\n            df = df.convert_dtypes()\n            f = Frame.from_pandas(df)\n            self.assertEqual(f.dtypes.to_pairs(),\n                    ((\'a\', np.dtype(\'bool\')), (\'b\', np.dtype(\'bool\')), (\'c\', np.dtype(\'<U1\')), (\'d\', np.dtype(\'<U1\')), (\'e\', np.dtype(\'bool\'))))\n        else:\n            f = Frame.from_pandas(df)\n            # we do not have a chance to re-evaluate string types, so they come in as object\n            self.assertEqual(f.dtypes.to_pairs(),\n                    ((\'a\', np.dtype(\'bool\')), (\'b\', np.dtype(\'bool\')), (\'c\', np.dtype(\'O\')), (\'d\', np.dtype(\'O\')), (\'e\', np.dtype(\'bool\'))))\n\n    def test_frame_from_pandas_i(self) -> None:\n        import pandas as pd\n\n        df = pd.DataFrame(\n                dict(a=(False, True), b=(True, np.nan), c=(\'q\',\'r\'), d=(\'a\', np.nan), e=(False, False)))\n\n        if hasattr(df, \'convert_dtypes\'):\n            df = df.convert_dtypes()\n            f = Frame.from_pandas(df)\n            self.assertEqual(f.dtypes.to_pairs(),\n                    ((\'a\', np.dtype(\'O\')), (\'b\', np.dtype(\'O\')), (\'c\', np.dtype(\'O\')), (\'d\', np.dtype(\'O\')), (\'e\', np.dtype(\'bool\'))))\n        else:\n            f = Frame.from_pandas(df)\n            self.assertEqual(f.dtypes.to_pairs(),\n                    ((\'a\', np.dtype(\'bool\')), (\'b\', np.dtype(\'O\')), (\'c\', np.dtype(\'O\')), (\'d\', np.dtype(\'O\')), (\'e\', np.dtype(\'bool\'))))\n\n\n    def test_frame_from_pandas_j(self) -> None:\n        import pandas as pd\n\n        df = pd.DataFrame(dict(a=(1,2), b=(\'3\',\'4\'), c=(1.5,2.5), d=(\'a\',\'b\')))\n\n        f = Frame.from_pandas(df,\n                index_constructor=IndexAutoFactory,\n                columns_constructor=IndexAutoFactory\n                )\n\n        self.assertTrue(f.index._map is None)\n        self.assertTrue(f.columns._map is None)\n\n        self.assertEqual(f.to_pairs(0),\n                ((0, ((0, 1), (1, 2))), (1, ((0, \'3\'), (1, \'4\'))), (2, ((0, 1.5), (1, 2.5))), (3, ((0, \'a\'), (1, \'b\'))))\n                )\n\n    def test_frame_from_pandas_k(self) -> None:\n        import pandas as pd\n\n        df = pd.DataFrame.from_records(\n                [(1,2), (\'3\',\'4\'), (1.5, 2.5), (\'a\',\'b\')],\n                index=(\'2012\', \'2013\', \'2014\', \'2015\'),\n                columns=(\'2020-01\', \'2020-02\')\n                )\n\n        f = Frame.from_pandas(df,\n                index_constructor=IndexYear,\n                columns_constructor=IndexYearMonth\n                )\n\n        self.assertEqual(f.to_pairs(0),\n                ((np.datetime64(\'2020-01\'), ((np.datetime64(\'2012\'), 1), (np.datetime64(\'2013\'), \'3\'), (np.datetime64(\'2014\'), 1.5), (np.datetime64(\'2015\'), \'a\'))), (np.datetime64(\'2020-02\'), ((np.datetime64(\'2012\'), 2), (np.datetime64(\'2013\'), \'4\'), (np.datetime64(\'2014\'), 2.5), (np.datetime64(\'2015\'), \'b\'))))\n                )\n\n    def test_frame_from_pandas_m(self) -> None:\n        import pandas as pd\n\n        df = pd.DataFrame.from_records(\n                [(1,2), (3,4), (1.5, 2.5)],\n                index=pd.date_range(\'2012-01-01\', \'2012-01-03\'),\n                )\n        f = sf.Frame.from_pandas(df, index_constructor=IndexDate)\n        self.assertTrue(f.index.__class__ is IndexDate)\n\n        self.assertEqual(f.to_pairs(0),\n                ((0, ((np.datetime64(\'2012-01-01\'), 1.0), (np.datetime64(\'2012-01-02\'), 3.0), (np.datetime64(\'2012-01-03\'), 1.5))), (1, ((np.datetime64(\'2012-01-01\'), 2.0), (np.datetime64(\'2012-01-02\'), 4.0), (np.datetime64(\'2012-01-03\'), 2.5))))\n                )\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_to_pandas_a(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        index = IndexHierarchy.from_product((100, 200), (True, False))\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=index)\n\n        df = f1.to_pandas()\n\n        self.assertEqual(df.index.values.tolist(),\n            [(100, True), (100, False), (200, True), (200, False)]\n            )\n\n        self.assertEqual(df.columns.values.tolist(),\n            [(\'a\', 1), (\'a\', 2), (\'b\', 1), (\'b\', 2)]\n            )\n\n        self.assertEqual(df.values.tolist(),\n            [[1, 2, \'a\', False], [30, 34, \'b\', True], [54, 95, \'c\', False], [65, 73, \'d\', True]])\n\n\n    def test_frame_to_pandas_b(self) -> None:\n        f1 = sf.Frame.from_dict_records(\n                [dict(a=1,b=1), dict(a=2,b=3), dict(a=1,b=1), dict(a=2,b=3)], index=sf.IndexHierarchy.from_labels(\n                [(1,\'dd\',0),(1,\'b\',0),(2,\'cc\',0),(2,\'ee\',0)]))\n        df = f1.loc[sf.HLoc[(1,\'dd\')]].to_pandas()\n\n        self.assertEqual(df.index.values.tolist(),\n                [(1, \'dd\', 0)])\n        self.assertEqual(df.values.tolist(),\n                [[1, 1]]\n                )\n\n\n    def test_frame_to_pandas_c(self) -> None:\n        f = sf.FrameGO.from_elements([\'a\' for x in range(5)], columns=[\'a\'])\n        f[\'b\'] = [1.0 for i in range(5)]\n        df = f.to_pandas()\n        self.assertEqual(df.dtypes.tolist(), [np.dtype(object), np.dtype(np.float64)])\n\n\n    @skip_win  # type: ignore\n    def test_frame_to_pandas_d(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        index = IndexHierarchy.from_product((100, 200), (True, False))\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=index)\n\n        df = f1.to_pandas()\n\n        self.assertEqual( df.dtypes.tolist(),\n                [np.dtype(\'int64\'), np.dtype(\'int64\'), np.dtype(\'O\'), np.dtype(\'bool\')]\n                )\n\n\n    def test_frame_to_pandas_e(self) -> None:\n        f = Frame.from_records(\n            [[\'a\', 1, 10], [\'a\', 2, 200], [\'b\', 1, -3], [\'b\', 2, 7]],\n            columns=(\'x\', \'y\', \'z\'))\n        df = f.set_index_hierarchy([\'x\', \'y\']).to_pandas()\n        self.assertEqual(list(df.index.names), [\'x\', \'y\'])\n\n\n    def test_frame_to_pandas_f(self) -> None:\n        # check name transfer\n        f = Frame.from_records(\n            [[\'a\', 1, 10], [\'a\', 2, 200],],\n            columns=(\'x\', \'y\', \'z\'),\n            name=\'foo\')\n        df = f.to_pandas()\n        self.assertEqual(df.name, f.name)\n\n    #---------------------------------------------------------------------------\n\n\n    def test_frame_to_arrow_a(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        index = IndexHierarchy.from_product((100, 200), (True, False))\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=index)\n\n        at = f1.to_arrow()\n        self.assertEqual(at.shape, (4, 6))\n        self.assertEqual(at.column_names,\n                [\'__index0__\', \'__index1__\', ""[\'a\' 1]"", ""[\'a\' 2]"", ""[\'b\' 1]"", ""[\'b\' 2]""])\n        self.assertEqual(at.to_pydict(),\n                {\'__index0__\': [100, 100, 200, 200], \'__index1__\': [True, False, True, False], ""[\'a\' 1]"": [1, 30, 54, 65], ""[\'a\' 2]"": [2, 34, 95, 73], ""[\'b\' 1]"": [\'a\', \'b\', \'c\', \'d\'], ""[\'b\' 2]"": [False, True, False, True]}\n                )\n\n\n    def test_frame_from_arrow_a(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        index = IndexHierarchy.from_product((100, 200), (True, False))\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=index)\n        at = f1.to_arrow()\n\n        f2 = Frame.from_arrow(at,\n                index_depth=f1.index.depth,\n                columns_depth=f1.columns.depth\n                )\n        # String arrays will come in as objects\n        self.assertEqualFrames(f1, f2, compare_dtype=False)\n\n\n    def test_frame_from_arrow_b(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        index = IndexHierarchy.from_product((100, 200), (True, False))\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=index)\n        at = f1.to_arrow()\n\n        f2 = Frame.from_arrow(at,\n                index_depth=f1.index.depth,\n                columns_depth=f1.columns.depth,\n                consolidate_blocks=True\n                )\n        self.assertEqual(f2._blocks.shapes.tolist(),\n                [(4, 2), (4,), (4,)])\n\n\n\n    def test_frame_from_arrow_c(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        f1 = Frame.from_records(records)\n        at = f1.to_arrow(include_index=False, include_columns=False)\n        f2 = Frame.from_arrow(at,\n                index_depth=0,\n                columns_depth=0,\n                consolidate_blocks=True\n                )\n\n        self.assertEqual(f2.to_pairs(0),\n                ((0, ((0, 1), (1, 30), (2, 54), (3, 65))), (1, ((0, 2), (1, 34), (2, 95), (3, 73))), (2, ((0, \'a\'), (1, \'b\'), (2, \'c\'), (3, \'d\'))), (3, ((0, False), (1, True), (2, False), (3, True))))\n                )\n\n\n    def test_frame_from_arrow_d(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        f1 = Frame.from_records(records)\n        f1 = f1.set_index(0, drop=True)\n        at = f1.to_arrow(include_index=True, include_columns=False)\n        f2 = Frame.from_arrow(at,\n                index_depth=1,\n                columns_depth=0,\n                consolidate_blocks=True\n                )\n        self.assertEqual(f2.to_pairs(0),\n                ((0, ((1, 2), (30, 34), (54, 95), (65, 73))), (1, ((1, \'a\'), (30, \'b\'), (54, \'c\'), (65, \'d\'))), (2, ((1, False), (30, True), (54, False), (65, True))))\n                )\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_to_parquet_a(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        index = IndexHierarchy.from_product((100, 200), (True, False))\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=index)\n\n        with temp_file(\'.parquet\') as fp:\n            f1.to_parquet(fp)\n\n    def test_frame_to_parquet_b(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        index = IndexHierarchy.from_product((100, 200), (True, False))\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=index)\n\n        with temp_file(\'.parquet\') as fp:\n            f1.to_parquet(fp, include_index=False, include_columns=False)\n            f2 = Frame.from_parquet(fp)\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'0\', ((0, 1), (1, 30), (2, 54), (3, 65))), (\'1\', ((0, 2), (1, 34), (2, 95), (3, 73))), (\'2\', ((0, \'a\'), (1, \'b\'), (2, \'c\'), (3, \'d\'))), (\'3\', ((0, False), (1, True), (2, False), (3, True))))\n                )\n\n    def test_frame_to_parquet_c(self) -> None:\n        records = (\n                (1, \'a\', False),\n                (30, \'b\', True),\n                (54, \'c\', False),\n                (65, \'d\', True),\n                )\n        index = IndexDate.from_date_range(\'2017-12-15\', \'2017-12-18\')\n        f1 = FrameGO.from_records(records,\n                columns=(\'a\', \'b\', \'c\'))\n        f1[\'d\'] = index.values\n\n        with temp_file(\'.parquet\') as fp:\n            f1.to_parquet(fp, include_index=False, include_columns=True)\n            f2 = Frame.from_parquet(fp, columns_depth=1)\n\n        self.assertEqual(\n                f2.to_pairs(0),\n                ((\'a\', ((0, 1), (1, 30), (2, 54), (3, 65))), (\'b\', ((0, \'a\'), (1, \'b\'), (2, \'c\'), (3, \'d\'))), (\'c\', ((0, False), (1, True), (2, False), (3, True))), (\'d\', ((0, np.datetime64(\'2017-12-15T00:00:00.000000000\')), (1, np.datetime64(\'2017-12-16T00:00:00.000000000\')), (2, np.datetime64(\'2017-12-17T00:00:00.000000000\')), (3, np.datetime64(\'2017-12-18T00:00:00.000000000\')))))\n                )\n        self.assertTrue(f2.index._map is None)\n\n    def test_frame_from_parquet_a(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        index = IndexHierarchy.from_product((100, 200), (True, False))\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=index)\n\n        with temp_file(\'.parquet\') as fp:\n            f1.to_parquet(fp)\n            f2 = Frame.from_parquet(fp,\n                    index_depth=f1.index.depth,\n                    columns_depth=f1.columns.depth)\n\n        self.assertEqualFrames(f1, f2, compare_dtype=False)\n\n\n    def test_frame_from_parquet_b(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        columns = (\'a\', \'b\', \'c\', \'d\')\n        f1 = Frame.from_records(records,\n                columns=columns,\n                )\n\n        with temp_file(\'.parquet\') as fp:\n            f1.to_parquet(fp)\n            f2 = Frame.from_parquet(fp,\n                    index_depth=0,\n                    columns_select=(\'d\', \'a\'),\n                    columns_depth=1)\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'d\', ((0, False), (1, True), (2, False), (3, True))), (\'a\', ((0, 1), (1, 30), (2, 54), (3, 65))))\n                )\n\n        self.assertTrue(f2.index._map is None)\n\n\n    def test_frame_from_parquet_c(self) -> None:\n        f = sf.FrameGO.from_element(\'a\',\n                index=range(3),\n                columns=sf.IndexHierarchy.from_labels(((\'a\', \'b\'),))\n                )\n\n        with temp_file(\'.parquet\') as fp:\n\n            f.to_parquet(fp)\n            f1 = sf.Frame.from_parquet(fp, index_depth=1, columns_depth=2)\n            # strings come back as object\n            self.assertTrue(f.equals(f1, compare_dtype=False, compare_class=False))\n\n            # when index_depth is not provided an exception is raised\n            with self.assertRaises(RuntimeError):\n                sf.Frame.from_parquet(fp, columns_depth=2)\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_to_xarray_a(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2), name=(\'a\', \'b\'))\n        index = IndexHierarchy.from_labels(((200, False, \'a\'), (200, True, \'b\'), (100, False, \'a\'), (300, True, \'b\')))\n\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=index)\n        ds1 = f1.to_xarray()\n        self.assertEqual(tuple(ds1.data_vars.keys()),\n                ((\'a\', 1), (\'a\', 2), (\'b\', 1), (\'b\', 2))\n                )\n        self.assertEqual(tuple(ds1.coords.keys()),\n                (\'__index0__\', \'__index1__\', \'__index2__\')\n                )\n        self.assertEqual(ds1[(\'b\', 1)].values.ndim, 3)\n\n        f2 = Frame.from_records(records)\n        ds2 = f2.to_xarray()\n        self.assertEqual(tuple(ds2.data_vars.keys()), (0, 1, 2, 3))\n        self.assertEqual(tuple(ds2.coords.keys()), (\'__index0__\',))\n        self.assertEqual(ds2[3].values.tolist(),\n                [False, True, False, True])\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_getitem_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        f2 = f1[\'r\':]  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(f2.columns.values.tolist(), [\'r\', \'s\', \'t\'])\n        self.assertTrue((f2.index == f1.index).all())\n        self.assertEqual(mloc(f2.index.values), mloc(f1.index.values))\n\n    def test_frame_getitem_b(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        # using an Index object for selection\n        self.assertEqual(\n                f1[f1.columns.loc[\'r\':]].to_pairs(0),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                ((\'r\', ((\'x\', \'a\'), (\'y\', \'b\'))), (\'s\', ((\'x\', False), (\'y\', True))), (\'t\', ((\'x\', True), (\'y\', False))))\n                )\n\n\n    def test_frame_getitem_c(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False),\n                (30, 50, \'b\', True))\n\n        f1 = FrameGO.from_records(records,\n                columns=IndexHierarchyGO.from_product((\'A\', \'B\'), (1, 2)),\n                index=(\'x\',\'y\'))\n\n        # we can use a tuple to select a single column if a hierarchical index\n        self.assertEqual(f1[(\'A\', 2)].to_pairs(),\n                ((\'x\', 2), (\'y\', 50))\n                )\n\n\n    def test_frame_getitem_d(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False),\n                (30, 50, \'b\', True))\n\n        f1 = FrameGO.from_records(records,\n                columns=Index([(\'A\', 1), (\'A\', 2), (\'B\', 1), (\'B\', 2)]),\n                index=(\'x\',\'y\'))\n\n        self.assertEqual(f1[(\'A\', 2)].to_pairs(),\n                ((\'x\', 2), (\'y\', 50))\n                )\n        self.assertEqual(f1[[(\'A\', 2), (\'B\', 1)]].to_pairs(0),\n                (((\'A\', 2), ((\'x\', 2), (\'y\', 50))), ((\'B\', 1), ((\'x\', \'a\'), (\'y\', \'b\'))))\n                )\n\n\n    #---------------------------------------------------------------------------\n\n\n    def test_frame_length_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        self.assertEqual(len(f1), 2)\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_iloc_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        self.assertEqual((f1.iloc[0].values == f1.loc[\'x\'].values).all(), True)\n        self.assertEqual((f1.iloc[1].values == f1.loc[\'y\'].values).all(), True)\n\n\n    def test_frame_iloc_b(self) -> None:\n        # this is example dervied from this question:\n        # https://stackoverflow.com/questions/22927181/selecting-specific-rows-and-columns-from-numpy-array\n\n        a = np.arange(20).reshape((5,4))\n        f1 = FrameGO(a)\n        a[1,1] = 3000 # ensure we made a copy\n        self.assertEqual(f1.loc[[0,1,3], [0,2]].values.tolist(),\n                [[0, 2], [4, 6], [12, 14]])\n        self.assertEqual(f1.iloc[[0,1,3], [0,2]].values.tolist(),\n                [[0, 2], [4, 6], [12, 14]])\n\n        self.assertTrue(f1._index._map is None) #type: ignore\n        self.assertTrue(f1._columns._map is None) #type: ignore\n\n        f1[4] = list(range(5))\n        self.assertTrue(f1._columns._map is None) #type: ignore\n\n        f1[20] = list(range(5))\n        self.assertFalse(f1._columns._map is None) #type: ignore\n\n        self.assertEqual(f1.values.tolist(),\n                [[0, 1, 2, 3, 0, 0],\n                [4, 5, 6, 7, 1, 1],\n                [8, 9, 10, 11, 2, 2],\n                [12, 13, 14, 15, 3, 3],\n                [16, 17, 18, 19, 4, 4]])\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_iter_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        self.assertEqual((f1.keys() == f1.columns).all(), True)\n        self.assertEqual([x for x in f1.columns], [\'p\', \'q\', \'r\', \'s\', \'t\'])\n        self.assertEqual([x for x in f1], [\'p\', \'q\', \'r\', \'s\', \'t\'])\n\n\n\n\n    def test_frame_iter_array_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        self.assertEqual(\n                next(iter(f1.iter_array(axis=0))).tolist(),\n                [1, 30])\n\n        self.assertEqual(\n                next(iter(f1.iter_array(axis=1))).tolist(),\n                [1, 2, \'a\', False, True])\n\n\n    def test_frame_iter_array_b(self) -> None:\n\n        arrays = list(np.random.rand(1000) for _ in range(100))\n        f1 = Frame.from_items(\n                zip(range(100), arrays)\n                )\n\n        # iter columns\n        post = f1.iter_array(axis=0).apply_pool(np.sum, max_workers=4, use_threads=True)\n        self.assertEqual(post.shape, (100,))\n        self.assertAlmostEqual(f1.sum().sum(), post.sum())\n\n        post = f1.iter_array(axis=0).apply_pool(np.sum, max_workers=4, use_threads=False)\n        self.assertEqual(post.shape, (100,))\n        self.assertAlmostEqual(f1.sum().sum(), post.sum())\n\n    def test_frame_iter_array_c(self) -> None:\n        arrays = []\n        for _ in range(8):\n            arrays.append(list(range(8)))\n        f1 = Frame.from_items(\n                zip(range(8), arrays)\n                )\n\n        func = {x: chr(x+65) for x in range(8)}\n        # iter columns\n        post = f1.iter_element().apply_pool(func, max_workers=4, use_threads=True)\n\n        self.assertEqual(post.to_pairs(0),\n                ((0, ((0, \'A\'), (1, \'B\'), (2, \'C\'), (3, \'D\'), (4, \'E\'), (5, \'F\'), (6, \'G\'), (7, \'H\'))), (1, ((0, \'A\'), (1, \'B\'), (2, \'C\'), (3, \'D\'), (4, \'E\'), (5, \'F\'), (6, \'G\'), (7, \'H\'))), (2, ((0, \'A\'), (1, \'B\'), (2, \'C\'), (3, \'D\'), (4, \'E\'), (5, \'F\'), (6, \'G\'), (7, \'H\'))), (3, ((0, \'A\'), (1, \'B\'), (2, \'C\'), (3, \'D\'), (4, \'E\'), (5, \'F\'), (6, \'G\'), (7, \'H\'))), (4, ((0, \'A\'), (1, \'B\'), (2, \'C\'), (3, \'D\'), (4, \'E\'), (5, \'F\'), (6, \'G\'), (7, \'H\'))), (5, ((0, \'A\'), (1, \'B\'), (2, \'C\'), (3, \'D\'), (4, \'E\'), (5, \'F\'), (6, \'G\'), (7, \'H\'))), (6, ((0, \'A\'), (1, \'B\'), (2, \'C\'), (3, \'D\'), (4, \'E\'), (5, \'F\'), (6, \'G\'), (7, \'H\'))), (7, ((0, \'A\'), (1, \'B\'), (2, \'C\'), (3, \'D\'), (4, \'E\'), (5, \'F\'), (6, \'G\'), (7, \'H\'))))\n                )\n\n    def test_frame_iter_array_d(self) -> None:\n        arrays = []\n        for _ in range(8):\n            arrays.append(list(range(8)))\n        f1 = Frame.from_items(\n                zip(range(8), arrays)\n                )\n\n        # when called with a pool, values are gien the func as a single argument, which for an element iteration is a tuple of coord, value\n        func = lambda arg: arg[0][1]\n        # iter columns\n        post = f1.iter_element_items().apply_pool(func, max_workers=4, use_threads=True)\n\n        self.assertEqual(post.to_pairs(0),\n                ((0, ((0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0))), (1, ((0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1))), (2, ((0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2))), (3, ((0, 3), (1, 3), (2, 3), (3, 3), (4, 3), (5, 3), (6, 3), (7, 3))), (4, ((0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4), (6, 4), (7, 4))), (5, ((0, 5), (1, 5), (2, 5), (3, 5), (4, 5), (5, 5), (6, 5), (7, 5))), (6, ((0, 6), (1, 6), (2, 6), (3, 6), (4, 6), (5, 6), (6, 6), (7, 6))), (7, ((0, 7), (1, 7), (2, 7), (3, 7), (4, 7), (5, 7), (6, 7), (7, 7))))\n                )\n\n\n    def test_frame_iter_array_e(self) -> None:\n\n        f = sf.Frame.from_dict(\n                dict(diameter=(12756, 6792, 142984),\n                mass=(5.97, 0.642, 1898)),\n                index=(\'Earth\', \'Mars\', \'Jupiter\'),\n                dtypes=dict(diameter=np.int64))\n\n        post = f.iter_array(axis=0).apply(np.sum)\n        self.assertTrue(post.dtype == float)\n\n\n    def test_frame_iter_array_f(self) -> None:\n\n        f = sf.Frame(np.arange(12).reshape(3,4),\n                index=IndexDate.from_date_range(\'2020-01-01\', \'2020-01-03\'))\n\n        post = f.iter_array(0).apply(np.sum, name=\'foo\')\n        self.assertEqual(post.name, \'foo\')\n\n        self.assertEqual(\n                f.iter_array(0).apply(np.sum).to_pairs(),\n                ((0, 12), (1, 15), (2, 18), (3, 21))\n                )\n\n        self.assertEqual(\n                f.iter_array(1).apply(np.sum).to_pairs(),\n                ((np.datetime64(\'2020-01-01\'), 6), (np.datetime64(\'2020-01-02\'), 22), (np.datetime64(\'2020-01-03\'), 38))\n                )\n\n\n\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_iter_tuple_a(self) -> None:\n        post = tuple(sf.Frame.from_elements(range(5)).iter_tuple(axis=0))\n        self.assertEqual(post, ((0, 1, 2, 3, 4),))\n\n\n\n\n    def test_frame_iter_tuple_b(self) -> None:\n        post = tuple(sf.Frame.from_elements(range(3), index=tuple(\'abc\')).iter_tuple(axis=0))\n        self.assertEqual(post, ((0, 1, 2),))\n\n        self.assertEqual(tuple(post[0]._asdict().items()),\n                ((\'a\', 0), (\'b\', 1), (\'c\', 2))\n                )\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_setitem_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        f1[\'a\'] = (False, True)\n        self.assertEqual(f1[\'a\'].values.tolist(), [False, True])\n\n        # test index alginment\n        f1[\'b\'] = Series((3,2,5), index=(\'y\', \'x\', \'g\'))\n        self.assertEqual(f1[\'b\'].values.tolist(), [2, 3])\n\n        f1[\'c\'] = Series((300,200,500), index=(\'y\', \'j\', \'k\'))\n        self.assertAlmostEqualItems(f1[\'c\'].items(), [(\'x\', nan), (\'y\', 300)])\n\n\n    def test_frame_setitem_b(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        f1[\'u\'] = 0\n\n        self.assertEqual(f1.loc[\'x\'].values.tolist(),\n                [1, 2, \'a\', False, True, 0])\n\n        # with self.assertRaises(Exception):\n        f1[\'w\'] = [[1,2], [4,5]]\n        self.assertEqual(f1[\'w\'].to_pairs(),\n                ((\'x\', [1, 2]), (\'y\', [4, 5])))\n\n\n    def test_frame_setitem_c(self) -> None:\n\n\n        f1 = FrameGO(index=sf.Index(tuple(\'abcde\')))\n        f1[\'a\'] = 30\n        self.assertEqual(f1.to_pairs(0),\n                ((\'a\', ((\'a\', 30), (\'b\', 30), (\'c\', 30), (\'d\', 30), (\'e\', 30))),))\n\n\n    def test_frame_setitem_d(self) -> None:\n\n        # 3d array raises exception\n        f = sf.FrameGO(index=range(3))\n        f[\'a\'] = 5\n        self.assertEqual(f.sum(), 15)\n\n\n\n    def test_frame_setitem_e(self) -> None:\n\n        # 3d array raises exception\n        f = sf.FrameGO(index=range(3))\n        f[\'a\'] = \'foo\'\n        self.assertEqual(f.to_pairs(0),\n                ((\'a\', ((0, \'foo\'), (1, \'foo\'), (2, \'foo\'))),)\n                )\n\n    def test_frame_setitem_f(self) -> None:\n\n        # 3d array raises exception\n        f = sf.FrameGO(index=range(3))\n        f[\'a\'] = \'foo\'\n\n        with self.assertRaises(RuntimeError):\n            f[\'a\'] = \'bar4\'\n\n\n    def test_frame_setitem_g(self) -> None:\n\n        # 3d array raises exception\n        f = sf.FrameGO(index=range(3))\n        f[\'a\'] = \'foo\'\n\n        # with self.assertRaises(RuntimeError):\n        with self.assertRaises(RuntimeError):\n            f[\'b\'] = np.array([[1, 2], [2, 5]])\n\n        with self.assertRaises(RuntimeError):\n            f[\'b\'] = np.array([1, 2])\n\n        with self.assertRaises(RuntimeError):\n            f[\'b\'] = [1, 2]\n\n    def test_frame_setitem_h(self) -> None:\n\n        # 3d array raises exception\n        f = sf.FrameGO.from_element(\'a\', index=range(3), columns=sf.IndexHierarchy.from_labels(((\'a\', 1),)))\n\n        f[sf.HLoc[\'a\', 2]] = 3\n        # this was resulting in a mal-formed blocks\n        with self.assertRaises(RuntimeError):\n            f[sf.HLoc[\'a\', 2]] = False\n\n        self.assertEqual(f.shape, (3, 2))\n        self.assertEqual(f.columns.shape, (2, 2))\n        self.assertEqual(f.to_pairs(0),\n                (((\'a\', 1), ((0, \'a\'), (1, \'a\'), (2, \'a\'))), ((\'a\', 2), ((0, 3), (1, 3), (2, 3))))\n                )\n\n\n    def test_frame_setitem_i(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False),\n                (30, 50, \'b\', True))\n\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\'),\n                index=(\'x\',\'y\'))\n\n        with self.assertRaises(RuntimeError):\n            f1[\'t\'] = [1, 2, 4]\n\n\n    def test_frame_setitem_j(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False),\n                (30, 50, \'b\', True))\n\n        f1 = FrameGO.from_records(records,\n                columns=IndexHierarchyGO.from_product((\'A\', \'B\'), (1, 2)),\n                index=(\'x\',\'y\'))\n\n        # set and retrieve with the same kye\n        key = (\'C\', 1)\n        f1[key] = 3\n        post = f1[key]\n        self.assertEqual(post.to_pairs(), ((\'x\', 3), (\'y\', 3)))\n\n        with self.assertRaises(RuntimeError):\n            f1[(\'C\', 2, 3)] = False\n\n        with self.assertRaises(RuntimeError):\n            f1[HLoc[\'C\', 2, 3]] = False\n\n        with self.assertRaises(RuntimeError):\n            f1[(\'C\',)] = False\n\n        with self.assertRaises(RuntimeError):\n            f1[HLoc[\'C\',]] = False\n\n        # can assign to a right-sized HLoc\n        f1[HLoc[\'C\', 2]] = False\n\n        self.assertEqual(f1.to_pairs(0),\n                (((\'A\', 1), ((\'x\', 1), (\'y\', 30))), ((\'A\', 2), ((\'x\', 2), (\'y\', 50))), ((\'B\', 1), ((\'x\', \'a\'), (\'y\', \'b\'))), ((\'B\', 2), ((\'x\', False), (\'y\', True))), ((\'C\', 1), ((\'x\', 3), (\'y\', 3))), ((\'C\', 2), ((\'x\', False), (\'y\', False))))\n                )\n\n\n\n    def test_frame_setitem_k(self) -> None:\n        f1 = sf.FrameGO.from_records(np.arange(9).reshape(3,3))\n\n        def gen1() -> tp.Iterator[int]:\n            yield 1\n            raise ValueError(\'gen1\')\n\n        try:\n            f1[\'a\'] = gen1()\n        except ValueError:\n            pass\n\n        self.assertEqual(f1.shape, (3, 3))\n        self.assertEqual(len(f1.columns), 3)\n\n    def test_frame_setitem_m(self) -> None:\n        f1 = sf.FrameGO.from_records(np.arange(9).reshape(3,3))\n\n        def gen1() -> tp.Iterator[int]:\n            raise ValueError(\'gen1\')\n            yield 1 #pylint: disable=W0101\n\n        try:\n            f1[\'a\'] = gen1()\n        except ValueError:\n            pass\n\n        self.assertEqual(f1.shape, (3, 3))\n        self.assertEqual(len(f1.columns), 3)\n\n\n    def test_frame_setitem_n(self) -> None:\n\n        f = sf.FrameGO.from_element(\'a\',\n                index=range(3),\n                columns=sf.IndexHierarchy.from_labels(((\'a\', \'b\'),)))\n        with self.assertRaises(RuntimeError):\n            f[\'s\'] = f\n\n\n    def test_frame_setitem_o(self) -> None:\n        import pandas as pd\n\n        # insure that you cannot set a Pandas Series inot a Frame\n\n        pds = pd.Series(dict(a=2, b=3, c=4))\n\n        f = sf.FrameGO(index=(\'c\', \'b\', \'a\'))\n        with self.assertRaises(RuntimeError):\n            f[\'x\'] = pds\n\n        f[\'x\'] = pds.values # can apply array out of ordewr\n        f[\'y\'] = Series.from_pandas(pds)\n\n        self.assertEqual(f.to_pairs(0),\n                ((\'x\', ((\'c\', 2), (\'b\', 3), (\'a\', 4))), (\'y\', ((\'c\', 4), (\'b\', 3), (\'a\', 2)))))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_extend_items_a(self) -> None:\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        columns = OrderedDict(\n            ((\'c\', np.array([0, -1])), (\'d\', np.array([3, 5]))))\n\n        f1.extend_items(columns.items())\n\n        self.assertEqual(f1.columns.values.tolist(),\n                [\'p\', \'q\', \'r\', \'s\', \'t\', \'c\', \'d\'])\n\n        self.assertTypeBlocksArrayEqual(f1._blocks,\n                [[1, 2, \'a\', False, True, 0, 3],\n                [30, 50, \'b\', True, False, -1, 5]],\n                match_dtype=object)\n\n    def test_frame_extend_a(self) -> None:\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        blocks = (np.array([[50, 40], [30, 20]]),\n                np.array([[50, 40], [30, 20]]))\n        columns = (\'a\', \'b\', \'c\', \'d\')\n        f2 = Frame(TypeBlocks.from_blocks(blocks), columns=columns, index=(\'y\', \'z\'))\n\n        f1.extend(f2)\n        f3 = f1.fillna(None)\n\n        self.assertEqual(f1.columns.values.tolist(),\n                [\'p\', \'q\', \'r\', \'s\', \'t\', \'a\', \'b\', \'c\', \'d\'])\n\n        self.assertEqual(f3.to_pairs(0),\n                ((\'p\', ((\'x\', 1), (\'y\', 30))), (\'q\', ((\'x\', 2), (\'y\', 50))), (\'r\', ((\'x\', \'a\'), (\'y\', \'b\'))), (\'s\', ((\'x\', False), (\'y\', True))), (\'t\', ((\'x\', True), (\'y\', False))), (\'a\', ((\'x\', None), (\'y\', 50))), (\'b\', ((\'x\', None), (\'y\', 40))), (\'c\', ((\'x\', None), (\'y\', 50))), (\'d\', ((\'x\', None), (\'y\', 40))))\n                )\n\n    def test_frame_extend_b(self) -> None:\n        records = (\n                (\'a\', False, True),\n                (\'b\', True, False))\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'x\',\'y\'))\n\n        s1 = Series((200, -3), index=(\'y\', \'x\'))\n\n        # this will work with a None name\n\n        f1.extend(s1)\n\n        self.assertEqual(f1.columns.values.tolist(), [\'p\', \'q\', \'r\', None])\n        self.assertEqual(f1[None].values.tolist(), [-3, 200])\n\n\n    def test_frame_extend_c(self) -> None:\n        records = (\n                (\'a\', False, True),\n                (\'b\', True, False))\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'x\',\'y\'))\n\n        s1 = Series((200, -3), index=(\'y\', \'x\'), name=\'s\')\n\n        f1.extend(s1)\n\n        self.assertEqual(f1.columns.values.tolist(), [\'p\', \'q\', \'r\', \'s\'])\n        self.assertEqual(f1[\'s\'].values.tolist(), [-3, 200])\n\n    def test_frame_extend_d(self) -> None:\n        records = (\n                (\'a\', False, True),\n                (\'b\', True, False))\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'x\',\'y\'))\n\n        s1 = Series((200, -3), index=(\'q\', \'x\'), name=\'s\')\n\n        f1.extend(s1, fill_value=0)\n\n        self.assertEqual(f1.columns.values.tolist(), [\'p\', \'q\', \'r\', \'s\'])\n        self.assertEqual(f1[\'s\'].values.tolist(), [-3, 0])\n\n\n    def test_frame_extend_e(self) -> None:\n        records = (\n                (\'a\', False, True),\n                (\'b\', True, False))\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'x\',\'y\'))\n\n        with self.assertRaises(NotImplementedError):\n            f1.extend(\'a\')\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_extend_empty_a(self) -> None:\n        # full Frame, empty extensions with no index\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                )\n        f1 = FrameGO.from_records(records,\n                columns=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                index=(\'x\', \'y\', \'z\'))\n\n        f2 = FrameGO() # no index or columns\n\n        f1.extend(f2)\n        self.assertEqual(f1.shape, (3, 5)) # extension happens, but no change in shape\n\n\n\n    def test_frame_extend_empty_b(self) -> None:\n        # full Frame, empty extension with index\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                )\n        f1 = FrameGO.from_records(records,\n                columns=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                index=(\'x\', \'y\', \'z\'))\n\n        f2 = FrameGO(index=(\'x\', \'y\', \'z\'))\n        f1.extend(f2)\n        self.assertEqual(f1.shape, (3, 5)) # extension happens, but no change in shape\n\n\n    def test_frame_extend_empty_c(self) -> None:\n        # empty with index, full frame extension\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                )\n        f1 = FrameGO.from_records(records,\n                columns=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                index=(\'x\', \'y\', \'z\'))\n\n        f2 = FrameGO(index=(\'x\', \'y\', \'z\'))\n        f2.extend(f1)\n        self.assertEqual(f2.shape, (3, 5)) # extension happens, but no change in shape\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'a\', ((\'x\', 1), (\'y\', 30), (\'z\', 54))), (\'b\', ((\'x\', 2), (\'y\', 34), (\'z\', 95))), (\'c\', ((\'x\', \'a\'), (\'y\', \'b\'), (\'z\', \'c\'))), (\'d\', ((\'x\', False), (\'y\', True), (\'z\', False))), (\'e\', ((\'x\', True), (\'y\', False), (\'z\', False))))\n                )\n\n    def test_frame_extend_empty_d(self) -> None:\n        # full Frame, empty extension with different index\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                )\n        f1 = FrameGO.from_records(records,\n                columns=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                index=(\'x\', \'y\', \'z\'))\n\n        f2 = FrameGO(index=(\'w\', \'x\', \'y\', \'z\'))\n\n        f1.extend(f2)\n        self.assertEqual(f1.shape, (3, 5)) # extension happens, but no change in shape\n\n\n\n    def test_frame_extend_empty_e(self) -> None:\n        # empty Frame with no index extended by full frame\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                )\n        f1 = FrameGO.from_records(records,\n                columns=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                index=(\'x\', \'y\', \'z\'))\n\n        f2 = FrameGO() # no index or columns\n\n        f2.extend(f1)\n        # as we align on the caller\'s index, if that index is empty, there is nothing to take from the passed Frame; however, since we observe columns, we add those (empty columns). this falls out of lower-level implementations: could be done differently if desirable.\n        self.assertEqual(f2.shape, (0, 5))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_insert_a(self) -> None:\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        blocks = (np.array([[50, 40], [30, 20]]),\n                np.array([[50, 40], [30, 20]]))\n        columns = (\'a\', \'b\', \'c\', \'d\')\n        f2 = Frame(TypeBlocks.from_blocks(blocks), columns=columns, index=(\'y\', \'z\'))\n\n        f3 = f1._insert(2, f2, fill_value=None)\n\n        self.assertEqual(f3.to_pairs(0),\n                ((\'p\', ((\'x\', 1), (\'y\', 30))), (\'q\', ((\'x\', 2), (\'y\', 50))), (\'a\', ((\'x\', None), (\'y\', 50))), (\'b\', ((\'x\', None), (\'y\', 40))), (\'c\', ((\'x\', None), (\'y\', 50))), (\'d\', ((\'x\', None), (\'y\', 40))), (\'r\', ((\'x\', \'a\'), (\'y\', \'b\'))), (\'s\', ((\'x\', False), (\'y\', True))), (\'t\', ((\'x\', True), (\'y\', False))))\n                )\n\n    def test_frame_insert_b(self) -> None:\n        records = (\n                (\'a\', False, True),\n                (\'b\', True, False))\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'x\',\'y\'))\n\n        with self.assertRaises(NotImplementedError):\n            f1._insert(0, \'a\')\n\n\n    def test_frame_insert_c(self) -> None:\n        records = (\n                (\'a\', False, True),\n                (\'b\', True, False))\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'x\',\'y\'))\n\n        s1 = Series((200, -3), index=(\'y\', \'x\'), name=\'s\')\n\n        f2 = f1._insert(0, s1)\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'s\', ((\'x\', -3), (\'y\', 200))), (\'p\', ((\'x\', \'a\'), (\'y\', \'b\'))), (\'q\', ((\'x\', False), (\'y\', True))), (\'r\', ((\'x\', True), (\'y\', False))))\n                )\n\n        f3 = f1._insert(-1, s1)\n        self.assertEqual(f3.to_pairs(0),\n                ((\'p\', ((\'x\', \'a\'), (\'y\', \'b\'))), (\'q\', ((\'x\', False), (\'y\', True))), (\'s\', ((\'x\', -3), (\'y\', 200))), (\'r\', ((\'x\', True), (\'y\', False))))\n                )\n\n        f4 = f1._insert(3, s1) # same as appending\n        self.assertEqual(f4.to_pairs(0),\n                ((\'p\', ((\'x\', \'a\'), (\'y\', \'b\'))), (\'q\', ((\'x\', False), (\'y\', True))), (\'r\', ((\'x\', True), (\'y\', False))), (\'s\', ((\'x\', -3), (\'y\', 200))))\n                )\n\n\n    def test_frame_insert_d(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        index = IndexHierarchy.from_product((100, 200), (True, False))\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=index)\n\n        f2 = Frame(np.arange(8).reshape(4,2),\n                index=index,\n                columns=((\'c\', 1), (\'c\', 2))\n                )\n\n        f3 = f1._insert(2, f2)\n\n        self.assertEqual(f3.to_pairs(0),\n                (((\'a\', 1), (((100, True), 1), ((100, False), 30), ((200, True), 54), ((200, False), 65))), ((\'a\', 2), (((100, True), 2), ((100, False), 34), ((200, True), 95), ((200, False), 73))), ((\'c\', 1), (((100, True), 0), ((100, False), 2), ((200, True), 4), ((200, False), 6))), ((\'c\', 2), (((100, True), 1), ((100, False), 3), ((200, True), 5), ((200, False), 7))), ((\'b\', 1), (((100, True), \'a\'), ((100, False), \'b\'), ((200, True), \'c\'), ((200, False), \'d\'))), ((\'b\', 2), (((100, True), False), ((100, False), True), ((200, True), False), ((200, False), True))))\n                )\n\n\n    def test_frame_insert_e(self) -> None:\n        records = (\n                (\'a\', False, True),\n                (\'b\', True, False))\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'x\',\'y\'))\n\n        s1 = Series((200, -3), index=(\'y\', \'x\'), name=\'s\')\n\n        self.assertEqual(f1.insert_after(\'q\', s1).to_pairs(0),\n                ((\'p\', ((\'x\', \'a\'), (\'y\', \'b\'))), (\'q\', ((\'x\', False), (\'y\', True))), (\'s\', ((\'x\', -3), (\'y\', 200))), (\'r\', ((\'x\', True), (\'y\', False))))\n                )\n\n        self.assertEqual(f1.insert_before(\'q\', s1).to_pairs(0),\n                ((\'p\', ((\'x\', \'a\'), (\'y\', \'b\'))), (\'s\', ((\'x\', -3), (\'y\', 200))), (\'q\', ((\'x\', False), (\'y\', True))), (\'r\', ((\'x\', True), (\'y\', False))))\n                )\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_extract_a(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n\n        f2 = f1._extract(row_key=np.array((False, True, True, False), dtype=bool))\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'p\', ((\'x\', 30), (\'y\', 2))), (\'q\', ((\'x\', 34), (\'y\', 95))), (\'r\', ((\'x\', \'b\'), (\'y\', \'c\'))), (\'s\', ((\'x\', True), (\'y\', False))), (\'t\', ((\'x\', False), (\'y\', False)))))\n\n\n        f3 = f1._extract(row_key=np.array((True, False, False, True), dtype=bool))\n\n        self.assertEqual(f3.to_pairs(0),\n                ((\'p\', ((\'w\', 2), (\'z\', 30))), (\'q\', ((\'w\', 2), (\'z\', 73))), (\'r\', ((\'w\', \'a\'), (\'z\', \'d\'))), (\'s\', ((\'w\', False), (\'z\', True))), (\'t\', ((\'w\', False), (\'z\', True)))))\n\n\n        # attempting to select any single row results in a problem, as the first block given to the TypeBlocks constructor is a 1d array that looks it is a (2,1) instead of a (1, 2)\n        f4 = f1._extract(row_key=np.array((False, False, True, False), dtype=bool))\n\n        self.assertEqual(\n                f4.to_pairs(0),\n                ((\'p\', ((\'y\', 2),)), (\'q\', ((\'y\', 95),)), (\'r\', ((\'y\', \'c\'),)), (\'s\', ((\'y\', False),)), (\'t\', ((\'y\', False),)))\n                )\n\n\n    def test_frame_extract_b(self) -> None:\n        # examining cases where shape goes to zero in one dimension\n\n        f1 = Frame.from_element(None, index=tuple(\'ab\'), columns=(\'c\',))\n        f2 = f1[[]]\n        self.assertEqual(len(f2.columns), 0)\n        self.assertEqual(len(f2.index), 2)\n        self.assertEqual(f2.shape, (2, 0))\n\n\n    def test_frame_extract_c(self) -> None:\n        # examining cases where shape goes to zero in one dimension\n        f1 = Frame.from_element(None, columns=tuple(\'ab\'), index=(\'c\',))\n        f2 = f1.loc[[]]\n        self.assertEqual(f2.shape, (0, 2))\n        self.assertEqual(len(f2.columns), 2)\n        self.assertEqual(len(f2.index), 0)\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_loc_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        # cases of single series extraction\n        s1 = f1.loc[\'x\']\n        self.assertEqual(list(s1.items()),\n                [(\'p\', 1), (\'q\', 2), (\'r\', \'a\'), (\'s\', False), (\'t\', True)])\n\n        s2 = f1.loc[:, \'p\']\n        self.assertEqual(list(s2.items()),\n                [(\'x\', 1), (\'y\', 30)])\n\n        self.assertEqual(\n                f1.loc[[\'y\', \'x\']].index.values.tolist(),\n                [\'y\', \'x\'])\n\n        self.assertEqual(f1[\'r\':].columns.values.tolist(),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                [\'r\', \'s\', \'t\'])\n\n\n    def test_frame_loc_b(self) -> None:\n        # dimensionality of returned item based on selectors\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        # return a series if one axis is multi\n        post = f1.loc[\'x\', \'t\':]  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(post.__class__, Series)\n        self.assertEqual(post.index.values.tolist(), [\'t\'])\n\n        post = f1.loc[\'y\':, \'t\']  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(post.__class__, Series)\n        self.assertEqual(post.index.values.tolist(), [\'y\'])\n\n        # if both are multi than we get a Frame\n        post = f1.loc[\'y\':, \'t\':]  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(post.__class__, Frame)\n        self.assertEqual(post.index.values.tolist(), [\'y\'])\n        self.assertEqual(post.columns.values.tolist(), [\'t\'])\n\n        # return a series\n        post = f1.loc[\'x\', \'s\':]  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(post.__class__, Series)\n        self.assertEqual(post.index.values.tolist(),[\'s\', \'t\'])\n\n        post = f1.loc[:, \'s\']\n        self.assertEqual(post.__class__, Series)\n        self.assertEqual(post.index.values.tolist(), [\'x\', \'y\'])\n\n        self.assertEqual(f1.loc[\'x\', \'s\'], False)\n        self.assertEqual(f1.loc[\'y\', \'p\'], 30)\n\n\n    def test_frame_loc_c(self) -> None:\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        post = f1.loc[\'x\':]  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(post.index.values.tolist(),\n                [\'x\', \'y\', \'z\'])\n\n\n    def test_frame_loc_d(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'),\n                name=\'foo\')\n\n        f2 = f1[\'r\':]  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        f3 = f1.loc[[\'y\'], [\'r\']]\n        self.assertEqual(f1.name, \'foo\')\n        self.assertEqual(f2.name, \'foo\')\n        self.assertEqual(f3.name, \'foo\')\n\n        s1 = f2.loc[:, \'s\']\n        self.assertEqual(s1.name, \'s\')\n\n        s2 = f1.loc[\'x\', :\'r\']  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(s2.name, \'x\')\n\n\n    def test_frame_loc_e(self) -> None:\n        fp = self.get_test_input(\'jph_photos.txt\')\n        # using a raw string to avoid unicode decoding issues on windows\n        f = sf.Frame.from_tsv(fp, dtypes=dict(albumId=np.int64, id=np.int64), encoding=\'utf-8\')\n        post = f.loc[f[\'albumId\'] >= 98]\n        self.assertEqual(post.shape, (150, 5))\n\n\n    def test_frame_loc_f(self) -> None:\n        f = Frame.from_elements(range(3), index=sf.Index(tuple(\'abc\'), name=\'index\'))\n        self.assertEqual(f.loc[\'b\':].index.name, \'index\') # type: ignore\n\n\n    def test_frame_loc_g(self) -> None:\n        f = Frame.from_dict(dict(a=[None], b=[1]))\n        self.assertEqual(f.shape, (1, 2))\n        post = f.loc[f[\'a\'] == True]\n        self.assertEqual(post.shape, (0, 2))\n\n    def test_frame_loc_h(self) -> None:\n\n        f1 = Frame(index=(\'a\', \'b\', \'c\'))\n        s1 = f1.loc[\'b\']\n        self.assertEqual(s1.name, \'b\')\n        self.assertEqual(len(s1), 0)\n\n        f2 = Frame(columns=(\'a\', \'b\', \'c\'))\n        s2 = f2[\'c\']\n        self.assertEqual(s2.name, \'c\')\n        self.assertEqual(len(s2), 0)\n\n    def test_frame_loc_i(self) -> None:\n\n        f1 = Frame(np.arange(16).reshape((4, 4)))\n\n        self.assertEqual(f1.loc[:2, :2].to_pairs(0),\n                ((0, ((0, 0), (1, 4), (2, 8))), (1, ((0, 1), (1, 5), (2, 9))), (2, ((0, 2), (1, 6), (2, 10))))\n                )\n\n        self.assertEqual(f1.iloc[:2, :2].to_pairs(0),\n                ((0, ((0, 0), (1, 4))), (1, ((0, 1), (1, 5))))\n                )\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_items_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        self.assertEqual(\n                list((k, list(v.items())) for k, v in f1.items()),\n                [(\'p\', [(\'x\', 1), (\'y\', 30)]), (\'q\', [(\'x\', 2), (\'y\', 50)]), (\'r\', [(\'x\', \'a\'), (\'y\', \'b\')]), (\'s\', [(\'x\', False), (\'y\', True)]), (\'t\', [(\'x\', True), (\'y\', False)])]\n                )\n\n\n    def test_frame_items_b(self) -> None:\n\n        records = (\n                (1, True),\n                (30,False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\'),\n                index=(\'x\',\'y\'))\n\n        for label, series in f1.items():\n            self.assertEqual(series.name, label)\n\n\n    #---------------------------------------------------------------------------\n\n\n    @skip_win  # type: ignore\n    def test_frame_attrs_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        self.assertEqual(str(f1.dtypes.values.tolist()),\n                ""[dtype(\'int64\'), dtype(\'int64\'), dtype(\'<U1\'), dtype(\'bool\'), dtype(\'bool\')]"")\n\n        self.assertEqual(f1.size, 10)\n        self.assertEqual(f1.ndim, 2)\n        self.assertEqual(f1.shape, (2, 5))\n\n    #---------------------------------------------------------------------------\n    def test_frame_assign_getitem_a(self) -> None:\n\n        f1 = FrameGO(index=(0,1,2))\n        for idx, col in enumerate(string.ascii_uppercase + string.ascii_lowercase):\n            f1[col] = idx\n\n        fields = [\'m\',\'V\',\'P\',\'c\',\'Y\',\'r\',\'q\',\'R\',\'j\',\'X\',\'a\',\'E\',\'K\',\'p\',\'u\',\'G\',\'D\',\'w\',\'d\',\'e\',\'H\',\'i\',\'h\',\'N\',\'O\',\'k\',\'l\',\'F\',\'g\',\'o\',\'M\',\'T\',\'n\',\'L\',\'Q\',\'W\',\'t\',\'v\',\'s\',\'Z\',\'J\',\'I\',\'b\']\n\n        # check that normal selection works\n        f1_sub = f1[fields]\n        self.assertEqual(f1_sub.columns.values.tolist(), fields)\n\n        f2 = f1.assign[fields](f1[fields] * 0.5)\n\n        self.assertTrue((f2.columns == f1.columns).all())\n        self.assertTrue((f2.index == f1.index).all())\n\n        # as expected, values is coercing all to floats\n        self.assertEqual(f2.values.tolist(),\n                [[0.0, 1.0, 2.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 18.0, 9.5, 20.0, 10.5, 11.0, 11.5, 12.0, 12.5, 13.0, 13.5, 14.0, 14.5, 15.0, 31.0, 16.0, 16.5, 17.0, 17.5, 18.0, 18.5, 19.0, 19.5, 20.0, 20.5, 21.0, 21.5, 22.0, 22.5, 23.0, 23.5, 24.0, 49.0, 50.0, 51.0], [0.0, 1.0, 2.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 18.0, 9.5, 20.0, 10.5, 11.0, 11.5, 12.0, 12.5, 13.0, 13.5, 14.0, 14.5, 15.0, 31.0, 16.0, 16.5, 17.0, 17.5, 18.0, 18.5, 19.0, 19.5, 20.0, 20.5, 21.0, 21.5, 22.0, 22.5, 23.0, 23.5, 24.0, 49.0, 50.0, 51.0], [0.0, 1.0, 2.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 18.0, 9.5, 20.0, 10.5, 11.0, 11.5, 12.0, 12.5, 13.0, 13.5, 14.0, 14.5, 15.0, 31.0, 16.0, 16.5, 17.0, 17.5, 18.0, 18.5, 19.0, 19.5, 20.0, 20.5, 21.0, 21.5, 22.0, 22.5, 23.0, 23.5, 24.0, 49.0, 50.0, 51.0]]\n                )\n\n    def test_frame_assign_getitem_b(self) -> None:\n\n        f1 = FrameGO(index=(0,1,2))\n        for idx, col in enumerate(\'abcdef\'):\n            f1[col] = idx\n\n        f2 = f1.assign[\'b\'](Series((\'b\',\'c\',\'d\'), index=(2, 0, 1)))\n        self.assertEqual(f2[\'b\'].to_pairs(),\n                ((0, \'c\'), (1, \'d\'), (2, \'b\')))\n\n\n    def test_frame_assign_getitem_c(self) -> None:\n        f1 = sf.Frame.from_element(False, index=range(2), columns=tuple(\'ab\'))\n        f2 = f1.assign[\'a\']([1.1, 2.1])\n        self.assertEqual(f2._blocks.shapes.tolist(), [(2,), (2,1)])\n        self.assertEqual(f2.dtypes.values.tolist(), [np.dtype(\'float64\'), np.dtype(\'bool\')])\n        self.assertEqual(f2.to_pairs(0),\n                ((\'a\', ((0, 1.1), (1, 2.1))), (\'b\', ((0, False), (1, False))))\n                )\n\n    def test_frame_assign_getitem_d(self) -> None:\n        f1 = sf.Frame.from_element(False, index=range(2), columns=tuple(\'abcd\'))\n        f2 = f1.assign[\'b\']([1.1, 2.1])\n        self.assertEqual(f2._blocks.shapes.tolist(), [(2, 1), (2,), (2, 2)])\n        self.assertEqual(f2.dtypes.values.tolist(),\n                [np.dtype(\'bool\'), np.dtype(\'float64\'), np.dtype(\'bool\'), np.dtype(\'bool\')]\n                )\n        self.assertEqual( f2.to_pairs(0),\n                ((\'a\', ((0, False), (1, False))), (\'b\', ((0, 1.1), (1, 2.1))), (\'c\', ((0, False), (1, False))), (\'d\', ((0, False), (1, False))))\n                )\n\n    def test_frame_assign_getitem_e(self) -> None:\n        f1 = sf.Frame.from_element(False, index=range(2), columns=tuple(\'abcd\'))\n        f2 = f1.assign[\'c\']([1.1, 2.1])\n        self.assertEqual(f2._blocks.shapes.tolist(), [(2, 2), (2,), (2, 1)])\n        self.assertEqual(f2.dtypes.values.tolist(),\n                [np.dtype(\'bool\'), np.dtype(\'bool\'), np.dtype(\'float64\'), np.dtype(\'bool\')]\n                )\n        self.assertEqual(f2.to_pairs(0),\n                ((\'a\', ((0, False), (1, False))), (\'b\', ((0, False), (1, False))), (\'c\', ((0, 1.1), (1, 2.1))), (\'d\', ((0, False), (1, False))))\n                )\n\n    def test_frame_assign_getitem_f(self) -> None:\n        f1 = sf.Frame.from_element(False, index=range(2), columns=tuple(\'abcd\'))\n        f2 = f1.assign[\'d\']([1.1, 2.1])\n        self.assertEqual(f2._blocks.shapes.tolist(), [(2, 3), (2,),])\n        self.assertEqual(f2.dtypes.values.tolist(),\n                [np.dtype(\'bool\'), np.dtype(\'bool\'), np.dtype(\'bool\'), np.dtype(\'float64\')]\n                )\n        self.assertEqual(f2.to_pairs(0),\n                ((\'a\', ((0, False), (1, False))), (\'b\', ((0, False), (1, False))), (\'c\', ((0, False), (1, False))), (\'d\', ((0, 1.1), (1, 2.1))))\n                )\n\n    def test_frame_assign_getitem_g(self) -> None:\n        f1 = sf.Frame.from_element(False, index=range(2), columns=tuple(\'abcde\'))\n        f2 = f1.assign[\'b\':\'d\'](\'x\') # type: ignore\n        self.assertEqual(f2._blocks.shapes.tolist(), [(2, 1), (2, 3), (2, 1)])\n        self.assertEqual(f2.dtypes.values.tolist(),\n                [np.dtype(\'bool\'), np.dtype(\'<U1\'), np.dtype(\'<U1\'), np.dtype(\'<U1\'), np.dtype(\'bool\')]\n                )\n        self.assertEqual(f2.to_pairs(0),\n                ((\'a\', ((0, False), (1, False))), (\'b\', ((0, \'x\'), (1, \'x\'))), (\'c\', ((0, \'x\'), (1, \'x\'))), (\'d\', ((0, \'x\'), (1, \'x\'))), (\'e\', ((0, False), (1, False)))))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_assign_iloc_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n\n        self.assertEqual(f1.assign.iloc[1,1](3000).iloc[1,1], 3000)\n\n\n    def test_frame_assign_loc_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        self.assertEqual(f1.assign.loc[\'x\', \'s\'](3000).values.tolist(),\n                [[1, 2, \'a\', 3000, True], [30, 50, \'b\', True, False]])\n\n        # can assign to a columne\n        self.assertEqual(\n                f1.assign[\'s\'](\'x\').values.tolist(),\n                [[1, 2, \'a\', \'x\', True], [30, 50, \'b\', \'x\', False]])\n\n\n    def test_frame_assign_loc_b(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        # unindexed tuple/list assingment\n        self.assertEqual(\n                f1.assign[\'s\']([50, 40]).values.tolist(),\n                [[1, 2, \'a\', 50, True], [30, 50, \'b\', 40, False]]\n                )\n\n        self.assertEqual(\n                f1.assign.loc[\'y\'](list(range(5))).values.tolist(),\n                [[1, 2, \'a\', False, True], [0, 1, 2, 3, 4]])\n\n\n\n\n    def test_frame_assign_loc_c(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        # assinging a series to a part of wone row\n        post = f1.assign.loc[\'x\', \'r\':](Series((-1, -2, -3), index=(\'t\', \'r\', \'s\')))  # type: ignore  # https://github.com/python/typeshed/pull/3024\n\n        self.assertEqual(post.values.tolist(),\n                [[1, 2, -2, -3, -1], [30, 50, \'b\', True, False]])\n\n        post = f1.assign.loc[[\'x\', \'y\'], \'r\'](Series((-1, -2), index=(\'y\', \'x\')))\n\n        self.assertEqual(post.values.tolist(),\n                [[1, 2, -2, False, True], [30, 50, -1, True, False]])\n\n        # ordere here does not matter\n        post = f1.assign.loc[[\'y\', \'x\'], \'r\'](Series((-1, -2), index=(\'y\', \'x\')))\n\n        self.assertEqual(post.values.tolist(),\n                [[1, 2, -2, False, True], [30, 50, -1, True, False]])\n\n\n    def test_frame_assign_loc_d(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'),\n                consolidate_blocks=True)\n\n        value1 = Frame.from_records(((20, 21, 22),(23, 24, 25)),\n                index=(\'x\', \'y\'),\n                columns=(\'s\', \'t\', \'w\'),\n                consolidate_blocks=True)\n\n        f2 = f1.assign.loc[[\'x\', \'y\'], [\'s\', \'t\']](value1)\n        self.assertEqual(f2.to_pairs(0),\n                ((\'p\', ((\'x\', 1), (\'y\', 30))), (\'q\', ((\'x\', 2), (\'y\', 50))), (\'r\', ((\'x\', \'a\'), (\'y\', \'b\'))), (\'s\', ((\'x\', 20), (\'y\', 23))), (\'t\', ((\'x\', 21), (\'y\', 24)))))\n\n\n    def test_frame_assign_loc_e(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False),\n                (30, 50, \'c\', False, False),\n                (3, 5, \'c\', False, True),\n                (30, 500, \'d\', True, True),\n                (30, 2, \'e\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=list(\'abcdef\')\n                )\n\n        f3 = f1.assign.iloc[5](f1.iloc[0])\n        self.assertEqual(f3.to_pairs(0),\n                ((\'p\', ((\'a\', 1), (\'b\', 30), (\'c\', 30), (\'d\', 3), (\'e\', 30), (\'f\', 1))), (\'q\', ((\'a\', 2), (\'b\', 50), (\'c\', 50), (\'d\', 5), (\'e\', 500), (\'f\', 2))), (\'r\', ((\'a\', \'a\'), (\'b\', \'b\'), (\'c\', \'c\'), (\'d\', \'c\'), (\'e\', \'d\'), (\'f\', \'a\'))), (\'s\', ((\'a\', False), (\'b\', True), (\'c\', False), (\'d\', False), (\'e\', True), (\'f\', False))), (\'t\', ((\'a\', True), (\'b\', False), (\'c\', False), (\'d\', True), (\'e\', True), (\'f\', True))))\n                )\n\n        f4 = f1.assign[\'q\'](f1[\'q\'] * 2)\n        self.assertEqual(f4.to_pairs(0),\n                ((\'p\', ((\'a\', 1), (\'b\', 30), (\'c\', 30), (\'d\', 3), (\'e\', 30), (\'f\', 30))), (\'q\', ((\'a\', 4), (\'b\', 100), (\'c\', 100), (\'d\', 10), (\'e\', 1000), (\'f\', 4))), (\'r\', ((\'a\', \'a\'), (\'b\', \'b\'), (\'c\', \'c\'), (\'d\', \'c\'), (\'e\', \'d\'), (\'f\', \'e\'))), (\'s\', ((\'a\', False), (\'b\', True), (\'c\', False), (\'d\', False), (\'e\', True), (\'f\', True))), (\'t\', ((\'a\', True), (\'b\', False), (\'c\', False), (\'d\', True), (\'e\', True), (\'f\', True))))\n                )\n\n        f5 = f1.assign[[\'p\', \'q\']](f1[[\'p\', \'q\']] * 2)\n        self.assertEqual(f5.to_pairs(0),\n                ((\'p\', ((\'a\', 2), (\'b\', 60), (\'c\', 60), (\'d\', 6), (\'e\', 60), (\'f\', 60))), (\'q\', ((\'a\', 4), (\'b\', 100), (\'c\', 100), (\'d\', 10), (\'e\', 1000), (\'f\', 4))), (\'r\', ((\'a\', \'a\'), (\'b\', \'b\'), (\'c\', \'c\'), (\'d\', \'c\'), (\'e\', \'d\'), (\'f\', \'e\'))), (\'s\', ((\'a\', False), (\'b\', True), (\'c\', False), (\'d\', False), (\'e\', True), (\'f\', True))), (\'t\', ((\'a\', True), (\'b\', False), (\'c\', False), (\'d\', True), (\'e\', True), (\'f\', True))))\n                )\n\n\n    def test_frame_assign_loc_f(self) -> None:\n        f1 = sf.Frame.from_element(False, index=range(2), columns=tuple(\'abcde\'))\n        f2 = f1.assign.loc[1, \'b\':\'d\'](\'x\') # type: ignore\n        self.assertEqual(f2.dtypes.values.tolist(),\n                [np.dtype(\'bool\'), np.dtype(\'O\'), np.dtype(\'O\'), np.dtype(\'O\'), np.dtype(\'bool\')])\n        self.assertEqual(f2.to_pairs(0),\n                ((\'a\', ((0, False), (1, False))), (\'b\', ((0, False), (1, \'x\'))), (\'c\', ((0, False), (1, \'x\'))), (\'d\', ((0, False), (1, \'x\'))), (\'e\', ((0, False), (1, False)))))\n\n\n    def test_frame_assign_coercion_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n        f2 = f1.assign.loc[\'x\', \'r\'](None)\n        self.assertEqual(f2.to_pairs(0),\n                ((\'p\', ((\'x\', 1), (\'y\', 30))), (\'q\', ((\'x\', 2), (\'y\', 50))), (\'r\', ((\'x\', None), (\'y\', \'b\'))), (\'s\', ((\'x\', False), (\'y\', True))), (\'t\', ((\'x\', True), (\'y\', False)))))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_assign_bloc_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        sel = np.array([[False, True, False, True, False],\n                [True, False, True, False, True]])\n        f2 = f1.assign.bloc[sel](-10)\n        self.assertEqual(f2.to_pairs(0),\n                ((\'p\', ((\'x\', 1), (\'y\', -10))), (\'q\', ((\'x\', -10), (\'y\', 50))), (\'r\', ((\'x\', \'a\'), (\'y\', -10))), (\'s\', ((\'x\', -10), (\'y\', True))), (\'t\', ((\'x\', True), (\'y\', -10))))\n                )\n\n        f3 = f1.assign.bloc[sel](None)\n        self.assertEqual(f3.to_pairs(0),\n                ((\'p\', ((\'x\', 1), (\'y\', None))), (\'q\', ((\'x\', None), (\'y\', 50))), (\'r\', ((\'x\', \'a\'), (\'y\', None))), (\'s\', ((\'x\', None), (\'y\', True))), (\'t\', ((\'x\', True), (\'y\', None))))\n                )\n\n    def test_frame_assign_bloc_b(self) -> None:\n\n        records = (\n                (1, 2, 20, 40),\n                (30, 50, -4, 5))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\',),\n                index=(\'x\',\'y\'))\n\n        sel = np.array([[False, True, False, True],\n                [True, False, True, False]])\n\n        # assignment from a frame of the same size\n        f2 =f1.assign.bloc[sel](f1 * 100)\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'p\', ((\'x\', 1), (\'y\', 3000))), (\'q\', ((\'x\', 200), (\'y\', 50))), (\'r\', ((\'x\', 20), (\'y\', -400))), (\'s\', ((\'x\', 4000), (\'y\', 5))))\n                )\n\n\n    def test_frame_assign_bloc_c(self) -> None:\n\n        records = (\n                (1, 2, 20, 40),\n                (30, 50, -4, 5))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\',),\n                index=(\'x\',\'y\'))\n\n        sel = f1 >= 20\n\n        f2 = f1.assign.bloc[sel](f1 * 100)\n\n        match = ((\'p\', ((\'x\', 1), (\'y\', 3000))), (\'q\', ((\'x\', 2), (\'y\', 5000))), (\'r\', ((\'x\', 2000), (\'y\', -4))), (\'s\', ((\'x\', 4000), (\'y\', 5))))\n\n        self.assertEqual(f2.to_pairs(0), match)\n\n        # reording the value will have no affect\n        f3 = f1.reindex(columns=(\'r\',\'q\',\'s\',\'p\'))\n        f4 = f1.assign.bloc[sel](f3 * 100)\n\n        self.assertEqual(f4.to_pairs(0), match)\n\n\n    def test_frame_assign_bloc_d(self) -> None:\n\n        records = (\n                (1, 2, 20, 40),\n                (30, 50, 4, 5))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\',),\n                index=(\'x\',\'y\'))\n\n        sel = f1 < 100 # get all true\n\n        # get a value that will require reindexing\n        f2 = Frame.from_element(-100,\n                columns=(\'q\', \'r\',),\n                index=(\'y\',))\n\n        self.assertEqual(f1.assign.bloc[sel](f2).to_pairs(0),\n                ((\'p\', ((\'x\', 1), (\'y\', 30))), (\'q\', ((\'x\', 2), (\'y\', -100))), (\'r\', ((\'x\', 20), (\'y\', -100))), (\'s\', ((\'x\', 40), (\'y\', 5))))\n            )\n\n\n    def test_frame_assign_bloc_e(self) -> None:\n\n        records = (\n                (1, 20),\n                (30, 5))\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\',),\n                index=(\'x\',\'y\'))\n\n        with self.assertRaises(RuntimeError):\n            # invalid bloc_key\n            f1.assign.bloc[[[True, False], [False, True]]](3)\n\n        with self.assertRaises(RuntimeError):\n            f1.assign.bloc[np.array([[True, False, False], [False, True, True]])](3)\n\n        with self.assertRaises(RuntimeError):\n            f1.assign.bloc[np.array([[True, False], [False, True]])](np.array([[100, 200, 10], [200, 300, 30]]))\n\n\n        a1 = np.array([[True, False], [False, True]])\n        a2 = np.array([[100, 200], [200, 300]])\n        self.assertEqual(\n                f1.assign.bloc[a1](a2).to_pairs(0),\n                ((\'p\', ((\'x\', 100), (\'y\', 30))), (\'q\', ((\'x\', 20), (\'y\', 300))))\n                )\n\n    def test_frame_assign_bloc_f(self) -> None:\n        f = sf.Frame.from_records(np.arange(9).reshape(3,3))\n        fgo = f.to_frame_go()\n        f1 = f.assign.bloc[f % 2 == 0](-f)\n        f2 = f.assign.bloc[f % 2 == 0](-fgo)\n\n        self.assertTrue((f1.values == f2.values).all())\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_mask_loc_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        self.assertEqual(\n                f1.mask.loc[\'x\', \'r\':].values.tolist(),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                [[False, False, True, True, True], [False, False, False, False, False]])\n\n\n        self.assertEqual(f1.mask[\'s\'].values.tolist(),\n                [[False, False, False, True, False], [False, False, False, True, False]])\n\n\n    def test_frame_masked_array_loc_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        # mask our the non-integers\n        self.assertEqual(\n                f1.masked_array.loc[:, \'r\':].sum(), 83)  # type: ignore  # https://github.com/python/typeshed/pull/3024\n\n\n    def test_frame_masked_array_getitem_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        self.assertEqual(f1.masked_array[\'r\':].tolist(), #type: ignore\n                [[1, 2, None, None, None], [30, 50, None, None, None]])\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_reindex_other_like_iloc_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        value1 = Series((100, 200, 300), index=(\'s\', \'u\', \'t\'))\n        iloc_key1 = (0, slice(2, None))\n        v1 = f1._reindex_other_like_iloc(value1, iloc_key1)\n\n        self.assertAlmostEqualItems(v1.items(),\n                [(\'r\', nan), (\'s\', 100), (\'t\', 300)])\n\n\n        value2 = Series((100, 200), index=(\'y\', \'x\'))\n        iloc_key2 = (slice(0, None), 2)\n        v2 = f1._reindex_other_like_iloc(value2, iloc_key2)\n\n        self.assertAlmostEqualItems(v2.items(),\n                [(\'x\', 200), (\'y\', 100)])\n\n\n    def test_frame_reindex_other_like_iloc_b(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        value1 = Frame.from_records(((20, 21, 22),(23, 24, 25)),\n                index=(\'x\', \'y\'),\n                columns=(\'s\', \'t\', \'w\'))\n\n        iloc_key1 = (slice(0, None), slice(3, None))\n        v1 = f1._reindex_other_like_iloc(value1, iloc_key1)\n\n        self.assertEqual(v1.to_pairs(0),\n                ((\'s\', ((\'x\', 20), (\'y\', 23))), (\'t\', ((\'x\', 21), (\'y\', 24)))))\n\n\n    def test_frame_reindex_other_like_iloc_c(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        iloc_key1 = (slice(0, None), slice(3, None))\n\n        with self.assertRaises(RuntimeError):\n            _ = f1._reindex_other_like_iloc([3, 4, 5], iloc_key1)\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_axis_flat_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                (65, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual(f1.to_pairs(axis=1),\n                ((\'w\', ((\'p\', 1), (\'q\', 2), (\'r\', \'a\'), (\'s\', False), (\'t\', True))), (\'x\', ((\'p\', 30), (\'q\', 34), (\'r\', \'b\'), (\'s\', True), (\'t\', False))), (\'y\', ((\'p\', 54), (\'q\', 95), (\'r\', \'c\'), (\'s\', False), (\'t\', False))), (\'z\', ((\'p\', 65), (\'q\', 73), (\'r\', \'d\'), (\'s\', True), (\'t\', True)))))\n\n\n        self.assertEqual(f1.to_pairs(axis=0),\n                ((\'p\', ((\'w\', 1), (\'x\', 30), (\'y\', 54), (\'z\', 65))), (\'q\', ((\'w\', 2), (\'x\', 34), (\'y\', 95), (\'z\', 73))), (\'r\', ((\'w\', \'a\'), (\'x\', \'b\'), (\'y\', \'c\'), (\'z\', \'d\'))), (\'s\', ((\'w\', False), (\'x\', True), (\'y\', False), (\'z\', True))), (\'t\', ((\'w\', True), (\'x\', False), (\'y\', False), (\'z\', True)))))\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_reindex_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                (65, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        # subset index reindex\n        self.assertEqual(\n                f1.reindex((\'z\', \'w\')).to_pairs(axis=0),\n                ((\'p\', ((\'z\', 65), (\'w\', 1))), (\'q\', ((\'z\', 73), (\'w\', 2))), (\'r\', ((\'z\', \'d\'), (\'w\', \'a\'))), (\'s\', ((\'z\', True), (\'w\', False))), (\'t\', ((\'z\', True), (\'w\', True)))))\n\n        # index only with nan filling\n        self.assertEqual(\n                f1.reindex((\'z\', \'b\', \'w\'), fill_value=None).to_pairs(0),\n                ((\'p\', ((\'z\', 65), (\'b\', None), (\'w\', 1))), (\'q\', ((\'z\', 73), (\'b\', None), (\'w\', 2))), (\'r\', ((\'z\', \'d\'), (\'b\', None), (\'w\', \'a\'))), (\'s\', ((\'z\', True), (\'b\', None), (\'w\', False))), (\'t\', ((\'z\', True), (\'b\', None), (\'w\', True)))))\n\n\n    def test_frame_reindex_b(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                (65, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual(\n                f1.reindex(columns=(\'q\', \'p\', \'w\'), fill_value=None).to_pairs(0),\n                ((\'q\', ((\'w\', 2), (\'x\', 34), (\'y\', 95), (\'z\', 73))), (\'p\', ((\'w\', 1), (\'x\', 30), (\'y\', 54), (\'z\', 65))), (\'w\', ((\'w\', None), (\'x\', None), (\'y\', None), (\'z\', None)))))\n\n        self.assertEqual(\n                f1.reindex(columns=(\'q\', \'p\', \'s\')).to_pairs(0),\n                ((\'q\', ((\'w\', 2), (\'x\', 34), (\'y\', 95), (\'z\', 73))), (\'p\', ((\'w\', 1), (\'x\', 30), (\'y\', 54), (\'z\', 65))), (\'s\', ((\'w\', False), (\'x\', True), (\'y\', False), (\'z\', True)))))\n\n        f2 = f1[[\'p\', \'q\']]\n\n        self.assertEqual(\n                f2.reindex(columns=(\'q\', \'p\')).to_pairs(0),\n                ((\'q\', ((\'w\', 2), (\'x\', 34), (\'y\', 95), (\'z\', 73))), (\'p\', ((\'w\', 1), (\'x\', 30), (\'y\', 54), (\'z\', 65)))))\n\n        self.assertEqual(\n                f2.reindex(columns=(\'a\', \'b\'), fill_value=None).to_pairs(0),\n                ((\'a\', ((\'w\', None), (\'x\', None), (\'y\', None), (\'z\', None))), (\'b\', ((\'w\', None), (\'x\', None), (\'y\', None), (\'z\', None)))))\n\n\n    def test_frame_reindex_c(self) -> None:\n        # reindex both axis\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                (65, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n\n        self.assertEqual(\n                f1.reindex(index=(\'y\', \'x\'), columns=(\'s\', \'q\')).to_pairs(0),\n                ((\'s\', ((\'y\', False), (\'x\', True))), (\'q\', ((\'y\', 95), (\'x\', 34)))))\n\n        self.assertEqual(\n                f1.reindex(index=(\'x\', \'y\'), columns=(\'s\', \'q\', \'u\'),\n                        fill_value=None).to_pairs(0),\n                ((\'s\', ((\'x\', True), (\'y\', False))), (\'q\', ((\'x\', 34), (\'y\', 95))), (\'u\', ((\'x\', None), (\'y\', None)))))\n\n        self.assertEqual(\n                f1.reindex(index=(\'a\', \'b\'), columns=(\'c\', \'d\'),\n                        fill_value=None).to_pairs(0),\n                ((\'c\', ((\'a\', None), (\'b\', None))), (\'d\', ((\'a\', None), (\'b\', None)))))\n\n\n        f2 = f1[[\'p\', \'q\']]\n\n        self.assertEqual(\n                f2.reindex(index=(\'x\',), columns=(\'q\',)).to_pairs(0),\n                ((\'q\', ((\'x\', 34),)),))\n\n        self.assertEqual(\n                f2.reindex(index=(\'y\', \'x\', \'q\'), columns=(\'q\',),\n                        fill_value=None).to_pairs(0),\n                ((\'q\', ((\'y\', 95), (\'x\', 34), (\'q\', None))),))\n\n\n    def test_frame_reindex_d(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                )\n\n        columns = IndexHierarchy.from_labels(((\'a\', 1), (\'a\', 2), (\'b\', 1), (\'b\', 2), (\'b\', 3)))\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=(\'x\', \'y\', \'z\'))\n\n        # NOTE: must use HLoc on getting a single columns as otherwise looks like a multiple axis selection\n        self.assertEqual(f1[sf.HLoc[\'a\', 1]].to_pairs(),\n                ((\'x\', 1), (\'y\', 30), (\'z\', 54))\n                )\n\n        self.assertEqual(f1[sf.HLoc[\'b\', 1]:].to_pairs(0),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                (((\'b\', 1), ((\'x\', \'a\'), (\'y\', \'b\'), (\'z\', \'c\'))), ((\'b\', 2), ((\'x\', False), (\'y\', True), (\'z\', False))), ((\'b\', 3), ((\'x\', True), (\'y\', False), (\'z\', False)))))\n\n        # reindexing with no column matches results in NaN for all values\n        self.assertTrue(\n                f1.iloc[:, 1:].reindex(columns=IndexHierarchy.from_product((\'b\', \'a\'), (10, 20))).isna().all().all())\n\n        columns = IndexHierarchy.from_product((\'b\', \'a\'), (3, 2))\n        f2 = f1.reindex(columns=columns, fill_value=None)\n        self.assertEqual(f2.to_pairs(0),\n                (((\'b\', 3), ((\'x\', True), (\'y\', False), (\'z\', False))), ((\'b\', 2), ((\'x\', False), (\'y\', True), (\'z\', False))), ((\'a\', 3), ((\'x\', None), (\'y\', None), (\'z\', None))), ((\'a\', 2), ((\'x\', 2), (\'y\', 34), (\'z\', 95)))))\n\n\n    def test_frame_reindex_e(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        index = IndexHierarchy.from_product((100, 200), (True, False))\n\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=index)\n\n        self.assertEqual(f1.loc[(200, True):, (\'b\',1):].to_pairs(0),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                (((\'b\', 1), (((200, True), \'c\'), ((200, False), \'d\'))), ((\'b\', 2), (((200, True), False), ((200, False), True)))))\n\n        # reindex from IndexHierarchy to Index with tuples\n        f2 = f1.reindex(\n                index=IndexHierarchy.from_product((200, 300), (False, True)),\n                columns=[(\'b\',1),(\'a\',1)],\n                fill_value=None)\n        self.assertEqual(f2.to_pairs(0),\n                (((\'b\', 1), (((200, False), \'d\'), ((200, True), \'c\'), ((300, False), None), ((300, True), None))), ((\'a\', 1), (((200, False), 65), ((200, True), 54), ((300, False), None), ((300, True), None)))))\n\n\n\n    def test_frame_reindex_f(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                )\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        f1 = Frame.from_records(records, columns=columns, name=\'foo\')\n        f2 = f1.reindex(index=(0,1,2), fill_value=0)\n\n        self.assertEqual(f1.name, \'foo\')\n        self.assertEqual(f2.name, \'foo\')\n\n\n    def test_frame_reindex_g(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                )\n        columns1 = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        f1 = Frame.from_records(records, columns=columns1, name=\'foo\')\n\n        index = Index((0, 1, 2))\n        f2 = f1.reindex(index=index, fill_value=0, own_index=True)\n        self.assertEqual(id(f2.index), id(index))\n\n        columns2 = IndexHierarchy.from_labels(((\'a\', 2), (\'b\', 1)))\n        f2 = f1.reindex(columns=columns2, own_columns=True)\n        self.assertEqual(id(f2.columns), id(columns2))\n\n        self.assertEqual(f2.to_pairs(0),\n                (((\'a\', 2), ((0, 2), (1, 34))), ((\'b\', 1), ((0, \'a\'), (1, \'b\'))))\n                )\n\n\n    def test_frame_reindex_h(self) -> None:\n        # reindex both axis\n        records = (\n                (2, \'a\', False),\n                (34, \'b\', True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\'))\n\n        with self.assertRaises(RuntimeError):\n            _ = f1.reindex()\n\n    #---------------------------------------------------------------------------\n    def test_frame_axis_interface_a(self) -> None:\n        # reindex both axis\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                (65, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual(f1.to_pairs(1),\n                ((\'w\', ((\'p\', 1), (\'q\', 2), (\'r\', \'a\'), (\'s\', False), (\'t\', True))), (\'x\', ((\'p\', 30), (\'q\', 34), (\'r\', \'b\'), (\'s\', True), (\'t\', False))), (\'y\', ((\'p\', 54), (\'q\', 95), (\'r\', \'c\'), (\'s\', False), (\'t\', False))), (\'z\', ((\'p\', 65), (\'q\', 73), (\'r\', \'d\'), (\'s\', True), (\'t\', True)))))\n\n        for x in f1.iter_tuple(axis=0):\n            self.assertTrue(len(x), 4)\n\n        for x in f1.iter_tuple(axis=1):\n            self.assertTrue(len(x), 5)\n\n\n        f2 = f1[[\'p\', \'q\']]\n\n        s1 = f2.iter_array(axis=0).apply(np.sum)\n        self.assertEqual(list(s1.items()), [(\'p\', 150), (\'q\', 204)])\n\n        s2 = f2.iter_array(axis=1).apply(np.sum)\n        self.assertEqual(list(s2.items()),\n                [(\'w\', 3), (\'x\', 64), (\'y\', 149), (\'z\', 138)])\n\n        def sum_if(idx: tp.Hashable, vals: tp.Iterable[int]) -> tp.Optional[int]:\n            if idx in (\'x\', \'z\'):\n                return tp.cast(int, np.sum(vals))\n            return None\n\n        s3 = f2.iter_array_items(axis=1).apply(sum_if)\n        self.assertEqual(list(s3.items()),\n                [(\'w\', None), (\'x\', 64), (\'y\', None), (\'z\', 138)])\n\n\n\n    def test_frame_group_a(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        with self.assertRaises(AxisInvalid):\n            post = tuple(f1._axis_group_iloc_items(4, axis=-1))\n\n\n        post = tuple(f1._axis_group_iloc_items(4, axis=0)) # row iter, group by column 4\n\n        group1, group_frame_1 = post[0]\n        group2, group_frame_2 = post[1]\n\n        self.assertEqual(group1, False)\n        self.assertEqual(group2, True)\n\n        self.assertEqual(group_frame_1.to_pairs(0),\n                ((\'p\', ((\'w\', 2), (\'x\', 30), (\'y\', 2))), (\'q\', ((\'w\', 2), (\'x\', 34), (\'y\', 95))), (\'r\', ((\'w\', \'a\'), (\'x\', \'b\'), (\'y\', \'c\'))), (\'s\', ((\'w\', False), (\'x\', True), (\'y\', False))), (\'t\', ((\'w\', False), (\'x\', False), (\'y\', False)))))\n\n        self.assertEqual(group_frame_2.to_pairs(0),\n                ((\'p\', ((\'z\', 30),)), (\'q\', ((\'z\', 73),)), (\'r\', ((\'z\', \'d\'),)), (\'s\', ((\'z\', True),)), (\'t\', ((\'z\', True),))))\n\n\n    def test_frame_group_b(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        # column iter, group by row 0\n        post = list(f1._axis_group_iloc_items(0, axis=1))\n\n        self.assertEqual(post[0][0], 2)\n        self.assertEqual(post[0][1].to_pairs(0),\n                ((\'p\', ((\'w\', 2), (\'x\', 30), (\'y\', 2), (\'z\', 30))), (\'q\', ((\'w\', 2), (\'x\', 34), (\'y\', 95), (\'z\', 73)))))\n\n        self.assertEqual(post[1][0], False)\n        self.assertEqual(post[1][1].to_pairs(0),\n                ((\'s\', ((\'w\', False), (\'x\', True), (\'y\', False), (\'z\', True))), (\'t\', ((\'w\', False), (\'x\', False), (\'y\', False), (\'z\', True)))))\n\n        self.assertEqual(post[2][0], \'a\')\n\n        self.assertEqual(post[2][1].to_pairs(0),\n                ((\'r\', ((\'w\', \'a\'), (\'x\', \'b\'), (\'y\', \'c\'), (\'z\', \'d\'))),))\n\n\n\n    def test_frame_axis_interface_b(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        post = list(f1.iter_group_items(\'s\', axis=0))\n\n        self.assertEqual(post[0][1].to_pairs(0),\n                ((\'p\', ((\'w\', 2), (\'y\', 2))), (\'q\', ((\'w\', 2), (\'y\', 95))), (\'r\', ((\'w\', \'a\'), (\'y\', \'c\'))), (\'s\', ((\'w\', False), (\'y\', False))), (\'t\', ((\'w\', False), (\'y\', False)))))\n\n        self.assertEqual(post[1][1].to_pairs(0),\n                ((\'p\', ((\'x\', 30), (\'z\', 30))), (\'q\', ((\'x\', 34), (\'z\', 73))), (\'r\', ((\'x\', \'b\'), (\'z\', \'d\'))), (\'s\', ((\'x\', True), (\'z\', True))), (\'t\', ((\'x\', False), (\'z\', True)))))\n\n\n        s1 = f1.iter_group(\'p\', axis=0).apply(lambda f: f[\'q\'].values.sum())\n        self.assertEqual(list(s1.items()), [(2, 97), (30, 107)])\n\n\n    def test_frame_contains_a(self) -> None:\n\n        f1 = Frame.from_items(zip((\'a\', \'b\'), ([20, 30, 40], [80, 10, 30])),\n                index=(\'x\', \'y\', \'z\'))\n\n        self.assertTrue(\'a\' in f1)\n        self.assertTrue(\'b\' in f1)\n        self.assertFalse(\'x\' in f1)\n        self.assertFalse(\'y\' in f1)\n\n\n\n    def test_frame_sum_a(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, 3, 4.23, 50.234),\n                (30, 34, 60, 80.6, 90.123),\n                (2, 95, 1, 1.96, 1.54),\n                (30, 73, 50, 40.23, 30.234),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        post = f1.sum(axis=0)\n        self.assertAlmostEqualItems(list(post.items()),\n                [(\'p\', 64.0), (\'q\', 204.0), (\'r\', 114.0), (\'s\', 127.01999999999998), (\'t\', 172.131)])\n        self.assertEqual(post.dtype, np.float64)\n\n        post = f1.sum(axis=1) # sum columns, return row index\n        self.assertEqual(list(f1.sum(axis=1).items()),\n                [(\'w\', 61.463999999999999), (\'x\', 294.72300000000001), (\'y\', 101.5), (\'z\', 223.464)])\n        self.assertEqual(post.dtype, np.float64)\n\n\n    def test_frame_sum_b(self) -> None:\n\n        records = (\n                (2, 2, 3),\n                (30, 34, 60),\n                (2, 95, 1),\n                (30, 73, 50),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        post = f1.sum(axis=0)\n\n        self.assertEqual(list(post.items()),\n                [(\'p\', 64), (\'q\', 204), (\'r\', 114)])\n\n        self.assertEqual(list(f1.sum(axis=1).items()),\n                [(\'w\', 7), (\'x\', 124), (\'y\', 98), (\'z\', 153)])\n\n\n    def test_frame_sum_c(self) -> None:\n\n        index = list(\'\'.join(x) for x in it.combinations(string.ascii_lowercase, 2))\n\n        f1 = FrameGO(index=index)\n        for col in range(100):\n            s = Series.from_element(col * .1, index=index[col: col+20])\n            f1[col] = s\n        assert f1.sum().sum() == 9900.0\n\n\n    def test_frame_sum_d(self) -> None:\n\n        a1 = np.array([\n                (2, 2, 3, 4.23, np.nan),\n                (30, 34, None, 80.6, 90.123),\n                ], dtype=object)\n        f1 = Frame(a1,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\'),\n                )\n\n        self.assertEqual(f1.sum(axis=0).values.tolist(),\n                [32, 36, 3, 84.83, 90.123])\n\n        self.assertEqual(f1.sum(axis=1).values.tolist(),\n                [11.23, 234.723])\n\n        with self.assertRaises(TypeError):\n            f1.sum(skipna=False)\n\n\n    def test_frame_mean_a(self) -> None:\n\n        a1 = np.array([\n            [1, 2, 3, -5],\n            [10, 50, 30, -7],\n            [1345, 2234, 3345, -200]])\n        a2 = np.array([2, 50, 2234])\n        a3 = np.array([3, 30, 3345])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        f1 = Frame(tb1,\n                columns=tuple(\'pqrstu\'),\n                index=(\'w\', \'x\', \'y\'))\n\n        self.assertEqual(\n                f1.mean(axis=0).values.tolist(),\n                f1.values.mean(axis=0).tolist())\n\n        self.assertEqual(\n                f1.mean(axis=1).values.tolist(),\n                f1.values.mean(axis=1).tolist())\n\n    def test_frame_median_a(self) -> None:\n\n        a1 = np.array([\n            [1, 2, 3, -5],\n            [10, 50, 30, -7],\n            [1345, 2234, 3345, -200]])\n        a2 = np.array([2, 50, 2234])\n        a3 = np.array([3, 30, 3345])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        f1 = Frame(tb1,\n                columns=tuple(\'pqrstu\'),\n                index=(\'w\', \'x\', \'y\'))\n\n        self.assertEqual(\n                f1.median(axis=0).values.tolist(),\n                np.median(f1.values, axis=0).tolist())\n\n        self.assertEqual(\n                f1.median(axis=1).values.tolist(),\n                np.median(f1.values, axis=1).tolist())\n\n\n    def test_frame_std_a(self) -> None:\n\n        a1 = np.array([\n            [1, 2, 3, -5],\n            [10, 50, 30, -7],\n            [1345, 2234, 3345, -200]])\n        a2 = np.array([2, 50, 2234])\n        a3 = np.array([3, 30, 3345])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        f1 = Frame(tb1,\n                columns=tuple(\'pqrstu\'),\n                index=(\'w\', \'x\', \'y\'))\n\n        self.assertEqual(\n                f1.std(axis=0).values.tolist(),\n                np.std(f1.values, axis=0).tolist())\n\n        self.assertEqual(\n                f1.std(axis=1).values.tolist(),\n                np.std(f1.values, axis=1).tolist())\n\n\n\n    def test_frame_var_a(self) -> None:\n\n        a1 = np.array([\n            [1, 2, 3, -5],\n            [10, 50, 30, -7],\n            [1345, 2234, 3345, -200]])\n        a2 = np.array([2, 50, 2234])\n        a3 = np.array([3, 30, 3345])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        f1 = Frame(tb1,\n                columns=tuple(\'pqrstu\'),\n                index=(\'w\', \'x\', \'y\'))\n\n        self.assertEqual(\n                f1.var(axis=0).values.tolist(),\n                np.var(f1.values, axis=0).tolist())\n\n        self.assertEqual(\n                f1.var(axis=1).values.tolist(),\n                np.var(f1.values, axis=1).tolist())\n\n\n\n    def test_frame_prod_a(self) -> None:\n\n        records = (\n                (2, 2, 3),\n                (30, 34, 60),\n                (2, 95, 1),\n                (30, 73, 50),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual(\n            f1.prod(axis=0).to_pairs(),\n            ((\'p\', 3600), (\'q\', 471580), (\'r\', 9000))\n            )\n\n        self.assertEqual(f1.prod(axis=1).to_pairs(),\n            ((\'w\', 12), (\'x\', 61200), (\'y\', 190), (\'z\', 109500))\n            )\n\n\n    def test_frame_prod_b(self) -> None:\n\n        a1 = np.array([\n            [1, 2, 3, -5],\n            [10, 50, 30, -7],\n            [1345, 2234, 3345, -200]])\n        a2 = np.array([2, 50, 2234])\n        a3 = np.array([3, 30, 3345.2])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        f1 = Frame(tb1,\n                columns=tuple(\'pqrstu\'),\n                index=(\'w\', \'x\', \'y\'))\n\n        self.assertEqual(\n                f1.prod(axis=0).values.tolist(),\n                np.prod(f1.values, axis=0).tolist())\n\n        self.assertEqual(\n                f1.prod(axis=1).values.tolist(),\n                np.prod(f1.values, axis=1).tolist())\n\n\n    def test_frame_cumsum_a(self) -> None:\n\n        records = (\n                (2, 2, 3),\n                (30, 34, 60),\n                (2, 95, 1),\n                (30, 73, 50),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        f2 = f1.cumsum()\n\n        self.assertEqual(\n                f2.to_pairs(0),\n                ((\'p\', ((\'w\', 2), (\'x\', 32), (\'y\', 34), (\'z\', 64))), (\'q\', ((\'w\', 2), (\'x\', 36), (\'y\', 131), (\'z\', 204))), (\'r\', ((\'w\', 3), (\'x\', 63), (\'y\', 64), (\'z\', 114))))\n                )\n        self.assertEqual(f1.cumsum(1).to_pairs(0),\n                ((\'p\', ((\'w\', 2), (\'x\', 30), (\'y\', 2), (\'z\', 30))), (\'q\', ((\'w\', 4), (\'x\', 64), (\'y\', 97), (\'z\', 103))), (\'r\', ((\'w\', 7), (\'x\', 124), (\'y\', 98), (\'z\', 153))))\n                )\n\n\n    def test_frame_cumsum_b(self) -> None:\n\n        records = (\n                (2, 2, 3),\n                (30, 34, 60),\n                (2, np.nan, 1),\n                (30, np.nan, 50),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        f2 = f1.cumsum(skipna=False)\n\n        self.assertEqual(f1.cumsum(skipna=False, axis=1).fillna(None).to_pairs(0),\n                ((\'p\', ((\'w\', 2.0), (\'x\', 30.0), (\'y\', 2.0), (\'z\', 30.0))), (\'q\', ((\'w\', 4.0), (\'x\', 64.0), (\'y\', None), (\'z\', None))), (\'r\', ((\'w\', 7.0), (\'x\', 124.0), (\'y\', None), (\'z\', None))))\n                )\n\n    def test_frame_cumprod_a(self) -> None:\n\n        records = (\n                (2, 2, 3),\n                (30, 34, 60),\n                (2, 95, 1),\n                (30, 73, 50),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual(f1.cumprod(0).to_pairs(0),\n                ((\'p\', ((\'w\', 2), (\'x\', 60), (\'y\', 120), (\'z\', 3600))), (\'q\', ((\'w\', 2), (\'x\', 68), (\'y\', 6460), (\'z\', 471580))), (\'r\', ((\'w\', 3), (\'x\', 180), (\'y\', 180), (\'z\', 9000))))\n                )\n\n        self.assertEqual(f1.cumprod(1).to_pairs(0),\n                ((\'p\', ((\'w\', 2), (\'x\', 30), (\'y\', 2), (\'z\', 30))), (\'q\', ((\'w\', 4), (\'x\', 1020), (\'y\', 190), (\'z\', 2190))), (\'r\', ((\'w\', 12), (\'x\', 61200), (\'y\', 190), (\'z\', 109500))))\n                )\n\n    def test_frame_min_a(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, 3, 4.23, 50.234),\n                (30, 34, 60, 80.6, 90.123),\n                (2, 95, 1, 1.96, 1.54),\n                (30, 73, 50, 40.23, 30.234),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertAlmostEqualItems(tuple(f1.min().items()),\n                ((\'p\', 2.0), (\'q\', 2.0), (\'r\', 1.0), (\'s\', 1.96), (\'t\', 1.54)))\n\n        self.assertAlmostEqualItems(tuple(f1.min(axis=1).items()),\n                ((\'w\', 2.0), (\'x\', 30.0), (\'y\', 1.0), (\'z\', 30.0)))\n\n    @skip_win  # type: ignore\n    def test_frame_row_dtype_a(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, 3, 4.23, 50.234),\n                (30, 34, 60, 80.6, 90.123),\n                (2, 95, 1, 1.96, 1.54),\n                (30, 73, 50, 40.23, 30.234),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual(f1[\'t\'].dtype, np.float64)\n        self.assertEqual(f1[\'p\'].dtype, np.int64)\n\n        self.assertEqual(f1.loc[\'w\'].dtype, np.float64)\n        self.assertEqual(f1.loc[\'z\'].dtype, np.float64)\n\n        self.assertEqual(f1[[\'r\', \'s\']].values.dtype, np.float64)\n\n    def test_frame_unary_operator_a(self) -> None:\n\n        records = (\n                (2, 2, 3, False, True),\n                (30, 34, 60, True, False),\n                (2, 95, 1, True, True),\n                (30, 73, 50, False, False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        # raises exception with NP14\n        # self.assertEqual((-f1).to_pairs(0),\n        #         ((\'p\', ((\'w\', -2), (\'x\', -30), (\'y\', -2), (\'z\', -30))), (\'q\', ((\'w\', -2), (\'x\', -34), (\'y\', -95), (\'z\', -73))), (\'r\', ((\'w\', -3), (\'x\', -60), (\'y\', -1), (\'z\', -50))), (\'s\', ((\'w\', True), (\'x\', False), (\'y\', False), (\'z\', True))), (\'t\', ((\'w\', False), (\'x\', True), (\'y\', False), (\'z\', True)))))\n\n        self.assertEqual((~f1).to_pairs(0),\n                ((\'p\', ((\'w\', -3), (\'x\', -31), (\'y\', -3), (\'z\', -31))), (\'q\', ((\'w\', -3), (\'x\', -35), (\'y\', -96), (\'z\', -74))), (\'r\', ((\'w\', -4), (\'x\', -61), (\'y\', -2), (\'z\', -51))), (\'s\', ((\'w\', True), (\'x\', False), (\'y\', False), (\'z\', True))), (\'t\', ((\'w\', False), (\'x\', True), (\'y\', False), (\'z\', True)))))\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_binary_operator_a(self) -> None:\n        # constants\n\n        records = (\n                (2, 2, 3.5),\n                (30, 34, 60.2),\n                (2, 95, 1.2),\n                (30, 73, 50.2),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual((f1 * 30).to_pairs(0),\n                ((\'p\', ((\'w\', 60), (\'x\', 900), (\'y\', 60), (\'z\', 900))), (\'q\', ((\'w\', 60), (\'x\', 1020), (\'y\', 2850), (\'z\', 2190))), (\'r\', ((\'w\', 105.0), (\'x\', 1806.0), (\'y\', 36.0), (\'z\', 1506.0))))\n                )\n\n\n\n    def test_frame_binary_operator_b(self) -> None:\n\n        records = (\n                (2, 2, 3.5),\n                (30, 34, 60.2),\n                (2, 95, 1.2),\n                (30, 73, 50.2),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        f2 = f1.loc[[\'y\', \'z\'], [\'r\']]\n        f3 = f1 * f2\n\n        self.assertAlmostEqualItems(list(f3[\'p\'].items()),\n                [(\'w\', nan), (\'x\', nan), (\'y\', nan), (\'z\', nan)])\n        self.assertAlmostEqualItems(list(f3[\'r\'].items()),\n                [(\'w\', nan), (\'x\', nan), (\'y\', 1.4399999999999999), (\'z\', 2520.0400000000004)])\n\n    def test_frame_binary_operator_c(self) -> None:\n\n        records = (\n                (2, 2, 3.5),\n                (30, 34, 60.2),\n                (2, 95, 1.2),\n                (30, 73, 50.2),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        s1 = Series([0, 1, 2], index=(\'r\', \'q\', \'p\'))\n\n        self.assertEqual((f1 * s1).to_pairs(0),\n                ((\'p\', ((\'w\', 4), (\'x\', 60), (\'y\', 4), (\'z\', 60))), (\'q\', ((\'w\', 2), (\'x\', 34), (\'y\', 95), (\'z\', 73))), (\'r\', ((\'w\', 0.0), (\'x\', 0.0), (\'y\', 0.0), (\'z\', 0.0)))))\n\n        self.assertEqual((f1 * [0, 1, 0]).to_pairs(0),\n                ((\'p\', ((\'w\', 0), (\'x\', 0), (\'y\', 0), (\'z\', 0))), (\'q\', ((\'w\', 2), (\'x\', 34), (\'y\', 95), (\'z\', 73))), (\'r\', ((\'w\', 0.0), (\'x\', 0.0), (\'y\', 0.0), (\'z\', 0.0)))))\n\n\n    def test_frame_binary_operator_d(self) -> None:\n\n        records = (\n                (2, True, \'\'),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'))\n\n        self.assertEqual((f1[\'q\'] == True).to_pairs(),\n                ((0, True),))\n\n        # this handles the case where, because we are comparing to an empty string, NumPy returns a single Boolean. This is manually handled in Series._ufunc_binary_operator\n        self.assertEqual((f1[\'r\'] == True).to_pairs(),\n                ((0, False),))\n\n\n    def test_frame_binary_operator_e(self) -> None:\n        # keep column order when columns are the same\n        f = sf.Frame.from_element(1, columns=[\'dog\', 3, \'bat\'], index=[1, 2])\n        post = f / f.sum()\n        self.assertEqual(post.columns.values.tolist(), f.columns.values.tolist())\n\n\n    def test_frame_binary_operator_f(self) -> None:\n\n        # matrix multiplication, with realigment of same sized axis\n\n        a = Frame.from_dict(dict(a=(1, 2, 3, 4), b=(5, 6, 7, 8)), index=tuple(\'wxyz\'))\n        b = Frame.from_dict(dict(p=(1, 2), q=(3, 4), r=(5, 6)), index=tuple(\'ab\'))\n\n        post1 = a @ b\n\n        self.assertEqual(\n                post1.to_pairs(0),\n                ((\'p\', ((\'w\', 11), (\'x\', 14), (\'y\', 17), (\'z\', 20))), (\'q\', ((\'w\', 23), (\'x\', 30), (\'y\', 37), (\'z\', 44))), (\'r\', ((\'w\', 35), (\'x\', 46), (\'y\', 57), (\'z\', 68))))\n                )\n\n        # with reorded index on b, we get the same result, as opposite axis align\n        post2 = a @ b.reindex(index=(\'b\', \'a\'))\n\n        self.assertEqual(\n                post2.to_pairs(0),\n                ((\'p\', ((\'w\', 11), (\'x\', 14), (\'y\', 17), (\'z\', 20))), (\'q\', ((\'w\', 23), (\'x\', 30), (\'y\', 37), (\'z\', 44))), (\'r\', ((\'w\', 35), (\'x\', 46), (\'y\', 57), (\'z\', 68))))\n                )\n\n        post3 = a @ Series([1, 2], index=tuple(\'ab\'))\n        self.assertEqual(post3.to_pairs(),\n                ((\'w\', 11), (\'x\', 14), (\'y\', 17), (\'z\', 20)))\n\n        # index is aligned\n        post4 = a @ Series([2, 1], index=tuple(\'ba\'))\n        self.assertEqual(post3.to_pairs(),\n                ((\'w\', 11), (\'x\', 14), (\'y\', 17), (\'z\', 20)))\n\n\n    def test_frame_binary_operator_g(self) -> None:\n\n        # matrix multiplication, with realigment of different sized axis\n\n        a = FrameGO.from_dict(dict(a=(1, 2, 3, 4), b=(5, 6, 7, 8)), index=tuple(\'wxyz\'))\n        a[\'c\'] = 30\n\n        b = Frame.from_dict(dict(p=(1, 2), q=(3, 4), r=(5, 6)), index=tuple(\'ab\'))\n\n        with self.assertRaises(RuntimeError):\n            post1 = a @ b\n            # all values would go to NaN\n\n        a = FrameGO.from_dict(dict(a=(1, 2, 3, 4), b=(5, 6, 7, 8)), index=tuple(\'wxyz\'))\n\n        b = Frame.from_dict(dict(p=(1, 2, 3), q=(3, 4, 5), r=(5, 6, 7)), index=tuple(\'abc\'))\n\n        with self.assertRaises(RuntimeError):\n            post2 = a @ b\n\n\n    def test_frame_binary_operator_h(self) -> None:\n\n        a = Frame.from_dict(dict(a=(1, 2, 3, 4), b=(5, 6, 7, 8)), index=tuple(\'wxyz\'))\n        b = Frame.from_dict(dict(p=(1, 2), q=(3, 4), r=(5, 6)), index=tuple(\'ab\'))\n\n\n        self.assertEqual(\n                (a @ b.values).to_pairs(0),\n                ((0, ((\'w\', 11), (\'x\', 14), (\'y\', 17), (\'z\', 20))), (1, ((\'w\', 23), (\'x\', 30), (\'y\', 37), (\'z\', 44))), (2, ((\'w\', 35), (\'x\', 46), (\'y\', 57), (\'z\', 68))))\n                )\n        # NOTE: the following yields a ValueError from the interpreter\n        # post2 = a.values @ b\n\n\n    def test_frame_binary_operator_i(self) -> None:\n\n        a = sf.Frame.from_elements((1, 2, 3))\n        post = a == a.to_frame_go()\n\n        self.assertEqual(post.__class__, FrameGO)\n        self.assertEqual(post.to_pairs(0),\n            ((0, ((0, True), (1, True), (2, True))),))\n\n\n    def test_frame_binary_operator_j(self) -> None:\n\n        f1 = sf.FrameGO.from_element(\'q\', index=range(3), columns=(\'x\', \'y\'))\n\n        f1[\'z\'] = \'foo\'\n\n        f2 = f1 + \'_\'\n        self.assertEqual(f2.to_pairs(0),\n                ((\'x\', ((0, \'q_\'), (1, \'q_\'), (2, \'q_\'))), (\'y\', ((0, \'q_\'), (1, \'q_\'), (2, \'q_\'))), (\'z\', ((0, \'foo_\'), (1, \'foo_\'), (2, \'foo_\'))))\n                )\n        self.assertEqual(f2.dtypes.values.tolist(),\n                [np.dtype(\'<U2\'), np.dtype(\'<U2\'), np.dtype(\'<U4\')]\n                )\n\n        f3 = \'_\' + f1\n        self.assertEqual(f3.to_pairs(0),\n                ((\'x\', ((0, \'_q\'), (1, \'_q\'), (2, \'_q\'))), (\'y\', ((0, \'_q\'), (1, \'_q\'), (2, \'_q\'))), (\'z\', ((0, \'_foo\'), (1, \'_foo\'), (2, \'_foo\'))))\n                )\n\n        f4 = f1 * 3\n        self.assertEqual(f4.to_pairs(0),\n                ((\'x\', ((0, \'qqq\'), (1, \'qqq\'), (2, \'qqq\'))), (\'y\', ((0, \'qqq\'), (1, \'qqq\'), (2, \'qqq\'))), (\'z\', ((0, \'foofoofoo\'), (1, \'foofoofoo\'), (2, \'foofoofoo\'))))\n                )\n        self.assertEqual(f4.dtypes.values.tolist(),\n                [np.dtype(\'<U3\'), np.dtype(\'<U3\'), np.dtype(\'<U9\')])\n\n\n    def test_frame_binary_operator_k(self) -> None:\n\n        # handling of name attr\n\n        f1 = Frame.from_dict(dict(a=(1, 2, 3), b=(5, 6, 7)),\n                index=tuple(\'xyz\'),\n                name=\'foo\')\n\n        f2 = f1 * [[3, 5], [0, 0], [1, 1]]\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'a\', ((\'x\', 3), (\'y\', 0), (\'z\', 3))), (\'b\', ((\'x\', 25), (\'y\', 0), (\'z\', 7)))))\n        self.assertEqual(f2.name, None)\n\n        f3 = f1 * 20\n        self.assertEqual(f3.name, \'foo\')\n\n\n    def test_frame_binary_operator_l(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(a=(1, \'2\', 3), b=(5, \'6\', 7)),\n                index=tuple(\'xyz\'),\n                name=\'foo\')\n\n        # test comparison to a single string\n        self.assertEqual((f1 == \'2\').to_pairs(0),\n                ((\'a\', ((\'x\', False), (\'y\', True), (\'z\', False))), (\'b\', ((\'x\', False), (\'y\', False), (\'z\', False))))\n                )\n\n        # should be all true of we do our array conversion right\n        f2 = f1 == f1.values.tolist()\n        self.assertTrue(f2.all().all())\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_isin_a(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        post = f1.isin({\'a\', 73, 30})\n        self.assertEqual(post.to_pairs(0),\n                ((\'p\', ((\'w\', False), (\'x\', True), (\'y\', False), (\'z\', True))), (\'q\', ((\'w\', False), (\'x\', False), (\'y\', False), (\'z\', True))), (\'r\', ((\'w\', True), (\'x\', False), (\'y\', False), (\'z\', False))), (\'s\', ((\'w\', False), (\'x\', False), (\'y\', False), (\'z\', False))), (\'t\', ((\'w\', False), (\'x\', False), (\'y\', False), (\'z\', False)))))\n\n\n        post = f1.isin([\'a\', 73, 30])\n        self.assertEqual(post.to_pairs(0),\n                ((\'p\', ((\'w\', False), (\'x\', True), (\'y\', False), (\'z\', True))), (\'q\', ((\'w\', False), (\'x\', False), (\'y\', False), (\'z\', True))), (\'r\', ((\'w\', True), (\'x\', False), (\'y\', False), (\'z\', False))), (\'s\', ((\'w\', False), (\'x\', False), (\'y\', False), (\'z\', False))), (\'t\', ((\'w\', False), (\'x\', False), (\'y\', False), (\'z\', False)))))\n\n\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_transpose_a(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'),\n                name=\'foo\')\n\n        f2 = f1.transpose()\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'w\', ((\'p\', 2), (\'q\', 2), (\'r\', \'a\'), (\'s\', False), (\'t\', False))), (\'x\', ((\'p\', 30), (\'q\', 34), (\'r\', \'b\'), (\'s\', True), (\'t\', False))), (\'y\', ((\'p\', 2), (\'q\', 95), (\'r\', \'c\'), (\'s\', False), (\'t\', False))), (\'z\', ((\'p\', 30), (\'q\', 73), (\'r\', \'d\'), (\'s\', True), (\'t\', True)))))\n\n        self.assertEqual(f2.name, f1.name)\n\n\n    def test_frame_transpose_b(self) -> None:\n        # reindex both axis\n        records = (\n                (False, False),\n                (True, False),\n                )\n\n        f1 = FrameGO.from_records(records,\n                columns=IndexYearGO((\'2019\', \'2020\')),\n                index=(\'x\', \'y\'),\n                name=\'foo\'\n                )\n\n        f1[\'2021\'] = True\n        self.assertTrue(f1.to_pairs(0),\n                ((np.datetime64(\'2019\'), ((\'x\', False), (\'y\', True))), (np.datetime64(\'2020\'), ((\'x\', False), (\'y\', False))), (np.datetime64(\'2021\'), ((\'x\', True), (\'y\', True))))\n                )\n        self.assertTrue(f1.T.to_pairs(0),\n                ((\'x\', ((np.datetime64(\'2019\'), False), (np.datetime64(\'2020\'), False), (np.datetime64(\'2021\'), True))), (\'y\', ((np.datetime64(\'2019\'), True), (np.datetime64(\'2020\'), False), (np.datetime64(\'2021\'), True))))\n                )\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_from_element_iloc_items_a(self) -> None:\n        items = (((0,1), \'g\'), ((1,0), \'q\'))\n\n        f1 = Frame.from_element_iloc_items(items,\n                index=(\'a\', \'b\'),\n                columns=(\'x\', \'y\'),\n                dtype=object,\n                name=\'foo\'\n                )\n\n        self.assertEqual(f1.to_pairs(0),\n                ((\'x\', ((\'a\', None), (\'b\', \'q\'))), (\'y\', ((\'a\', \'g\'), (\'b\', None)))))\n\n\n        self.assertEqual(f1.name, \'foo\')\n\n\n    def test_frame_from_element_iloc_items_b(self) -> None:\n\n        items = (((0,1), .5), ((1,0), 1.5))\n\n        f2 = Frame.from_element_iloc_items(items,\n                index=(\'a\', \'b\'),\n                columns=(\'x\', \'y\'),\n                dtype=float\n                )\n\n        self.assertAlmostEqualItems(tuple(f2[\'x\'].items()),\n                ((\'a\', nan), (\'b\', 1.5)))\n\n        self.assertAlmostEqualItems(tuple(f2[\'y\'].items()),\n                ((\'a\', 0.5), (\'b\', nan)))\n\n\n    def test_frame_from_element_loc_items_a(self) -> None:\n        items = (((\'b\', \'x\'), \'g\'), ((\'a\',\'y\'), \'q\'))\n\n        f1 = Frame.from_element_loc_items(items,\n                index=(\'a\', \'b\'),\n                columns=(\'x\', \'y\'),\n                dtype=object,\n                name=\'foo\'\n                )\n\n        self.assertEqual(f1.to_pairs(0),\n                ((\'x\', ((\'a\', None), (\'b\', \'g\'))), (\'y\', ((\'a\', \'q\'), (\'b\', None)))))\n        self.assertEqual(f1.name, \'foo\')\n\n    #---------------------------------------------------------------------------\n    def test_frame_from_items_a(self) -> None:\n\n        f1 = Frame.from_items(\n                zip(range(10), (np.random.rand(1000) for _ in range(10))),\n                name=\'foo\'\n                )\n        self.assertEqual(f1.name, \'foo\')\n\n    def test_frame_from_items_b(self) -> None:\n\n        s1 = Series((1, 2, 3), index=(\'a\', \'b\', \'c\'))\n        s2 = Series((4, 5, 6), index=(\'a\', \'b\', \'c\'))\n\n        with self.assertRaises(RuntimeError):\n            # must have an index to consume Series\n            Frame.from_items(zip(list(\'xy\'), (s1, s2)))\n\n    def test_frame_from_items_c(self) -> None:\n\n        s1 = Series((1, 2, 3), index=(\'a\', \'b\', \'c\'))\n        s2 = Series((4, 5, 6), index=(\'a\', \'b\', \'c\'))\n\n        f1 = Frame.from_items(zip(list(\'xy\'), (s1, s2)), index=s1.index)\n\n        self.assertEqual(f1.to_pairs(0),\n                ((\'x\', ((\'a\', 1), (\'b\', 2), (\'c\', 3))), (\'y\', ((\'a\', 4), (\'b\', 5), (\'c\', 6)))))\n\n    def test_frame_from_items_d(self) -> None:\n\n        s1 = Series((1, 2, 3), index=(\'a\', \'b\', \'c\'))\n        s2 = Series((4, 5, 6), index=(\'a\', \'b\', \'c\'))\n\n        f1 = Frame.from_items(zip(list(\'xy\'), (s1, s2)), index=(\'c\', \'a\'))\n\n        self.assertEqual(f1.to_pairs(0),\n            ((\'x\', ((\'c\', 3), (\'a\', 1))), (\'y\', ((\'c\', 6), (\'a\', 4)))))\n\n\n    def test_frame_from_items_e(self) -> None:\n\n        s1 = Series((1, 2, 3), index=(\'a\', \'b\', \'c\'))\n        s2 = Series((4, 5, 6), index=(\'a\', \'b\', \'c\'))\n        s3 = Series((7, 8, 9), index=(\'a\', \'b\', \'c\'))\n\n        f1 = Frame.from_items(zip(list(\'xy\'), (s1, s2, s3)), index=(\'c\', \'a\'),\n                consolidate_blocks=True)\n\n        self.assertEqual(len(f1._blocks._blocks), 1)\n\n    def test_frame_from_items_f(self) -> None:\n\n        def gen() -> tp.Iterator[tp.Tuple[int, tp.Tuple[int, int]]]:\n            for i in range(4):\n                yield i, (2 * i, 3 * i)\n\n        f1 = Frame.from_items(\n                gen(),\n                name=\'foo\',\n                dtypes = (str, str, str, str)\n                )\n        self.assertEqual(f1.to_pairs(0),\n                ((0, ((0, \'0\'), (1, \'0\'))), (1, ((0, \'2\'), (1, \'3\'))), (2, ((0, \'4\'), (1, \'6\'))), (3, ((0, \'6\'), (1, \'9\'))))\n                )\n\n\n    def test_frame_from_items_g(self) -> None:\n        def gen() -> tp.Iterator[tp.Tuple[tp.Tuple[str, int], tp.Tuple[int, int]]]:\n            for i in range(4):\n                yield (\'a\', i), (2 * i, 3 * i)\n\n        f1 = Frame.from_items(\n                gen(),\n                name=\'foo\',\n                index=((\'a\', 1), (\'a\', 2)),\n                index_constructor=IndexHierarchy.from_labels,\n                columns_constructor=IndexHierarchy.from_labels,\n                )\n        self.assertEqual(f1.index.__class__, IndexHierarchy)\n        self.assertEqual(f1.columns.__class__, IndexHierarchy)\n\n        self.assertEqual(f1.to_pairs(0),\n                (((\'a\', 0), (((\'a\', 1), 0), ((\'a\', 2), 0))), ((\'a\', 1), (((\'a\', 1), 2), ((\'a\', 2), 3))), ((\'a\', 2), (((\'a\', 1), 4), ((\'a\', 2), 6))), ((\'a\', 3), (((\'a\', 1), 6), ((\'a\', 2), 9))))\n                )\n\n\n    def test_frame_from_items_h(self) -> None:\n\n        def gen() -> tp.Iterator[tp.Tuple[int, tp.Tuple[int, int]]]:\n            for i in range(2):\n                yield i, np.array(tuple(str(1000 + j + i) for j in range(3)))\n\n        f1 = Frame.from_items(gen(), dtypes=(\n                np.dtype(\'datetime64[Y]\'), np.dtype(\'datetime64[Y]\'))\n                )\n\n        self.assertEqual(f1.to_pairs(0),\n                ((0, ((0, np.datetime64(\'1000\')), (1, np.datetime64(\'1001\')), (2, np.datetime64(\'1002\')))), (1, ((0, np.datetime64(\'1001\')), (1, np.datetime64(\'1002\')), (2, np.datetime64(\'1003\')))))\n                )\n\n\n    def test_frame_from_items_i(self) -> None:\n\n        def gen() -> tp.Iterator[tp.Tuple[int, Series]]:\n            for i in range(2):\n                yield i, Series(\n                        tuple(str(1000 + j + i) for j in range(3)),\n                        index=(\'a\', \'b\', \'c\')\n                        )\n        with self.assertRaises(ErrorInitFrame):\n            # must provide an index\n            _ = Frame.from_items(gen(), dtypes=(\n                    np.dtype(\'datetime64[Y]\'), np.dtype(\'datetime64[Y]\'))\n                    )\n\n        f1 = Frame.from_items(gen(), dtypes=(\n                np.dtype(\'datetime64[Y]\'), np.dtype(\'datetime64[Y]\')),\n                index=(\'a\', \'c\'))\n\n        self.assertEqual( f1.to_pairs(0),\n                ((0, ((\'a\', np.datetime64(\'1000\')), (\'c\', np.datetime64(\'1002\')))), (1, ((\'a\', np.datetime64(\'1001\')), (\'c\', np.datetime64(\'1003\')))))\n                )\n\n\n    def test_frame_from_items_j(self) -> None:\n\n        def gen() -> tp.Iterator[tp.Tuple[int, tp.Tuple[int, int]]]:\n            for i in range(2):\n                yield i, Frame.from_element(\'x\', index=(\'a\',), columns=(\'a\',))\n\n        with self.assertRaises(ErrorInitFrame):\n            # must provide an index\n            _ = Frame.from_items(gen())\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_from_structured_array_a(self) -> None:\n        a = np.array([(\'Venus\', 4.87, 464), (\'Neptune\', 102, -200)],\n                dtype=[(\'name\', object), (\'mass\', \'f4\'), (\'temperature\', \'i4\')])\n\n        f = sf.Frame.from_structured_array(a,\n                index_depth=1,\n                index_column_first=\'name\',\n                name=\'foo\')\n\n        self.assertEqual(f.shape, (2, 2))\n        self.assertEqual(f.name, \'foo\')\n        self.assertEqual(f[\'temperature\'].sum(), 264)\n\n\n    def test_frame_from_structured_array_b(self) -> None:\n        a = np.array([(\'Venus\', 4.87, 464), (\'Neptune\', 102, -200)],\n                dtype=[(\'name\', object), (\'mass\', \'f4\'), (\'temperature\', \'i4\')])\n\n        f = sf.Frame.from_structured_array(a,\n                index_column_first=2,\n                index_depth=1,\n                name=\'foo\')\n        self.assertEqual(f[\'name\'].to_pairs(),\n                ((464, \'Venus\'), (-200, \'Neptune\')))\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_iter_element_a(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual(\n                [x for x in f1.iter_element()],\n                [2, 2, \'a\', False, False, 30, 34, \'b\', True, False, 2, 95, \'c\', False, False, 30, 73, \'d\', True, True])\n\n        self.assertEqual([x for x in f1.iter_element_items()],\n                [((\'w\', \'p\'), 2), ((\'w\', \'q\'), 2), ((\'w\', \'r\'), \'a\'), ((\'w\', \'s\'), False), ((\'w\', \'t\'), False), ((\'x\', \'p\'), 30), ((\'x\', \'q\'), 34), ((\'x\', \'r\'), \'b\'), ((\'x\', \'s\'), True), ((\'x\', \'t\'), False), ((\'y\', \'p\'), 2), ((\'y\', \'q\'), 95), ((\'y\', \'r\'), \'c\'), ((\'y\', \'s\'), False), ((\'y\', \'t\'), False), ((\'z\', \'p\'), 30), ((\'z\', \'q\'), 73), ((\'z\', \'r\'), \'d\'), ((\'z\', \'s\'), True), ((\'z\', \'t\'), True)])\n\n\n        post = f1.iter_element().apply(lambda x: \'_\' + str(x) + \'_\')\n\n        self.assertEqual(post.to_pairs(0),\n                ((\'p\', ((\'w\', \'_2_\'), (\'x\', \'_30_\'), (\'y\', \'_2_\'), (\'z\', \'_30_\'))), (\'q\', ((\'w\', \'_2_\'), (\'x\', \'_34_\'), (\'y\', \'_95_\'), (\'z\', \'_73_\'))), (\'r\', ((\'w\', \'_a_\'), (\'x\', \'_b_\'), (\'y\', \'_c_\'), (\'z\', \'_d_\'))), (\'s\', ((\'w\', \'_False_\'), (\'x\', \'_True_\'), (\'y\', \'_False_\'), (\'z\', \'_True_\'))), (\'t\', ((\'w\', \'_False_\'), (\'x\', \'_False_\'), (\'y\', \'_False_\'), (\'z\', \'_True_\')))))\n\n\n\n\n    def test_frame_iter_element_b(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        # support working with mappings\n        post = f1.iter_element().map_any({2: 200, False: 200})\n\n        self.assertEqual(post.to_pairs(0),\n                ((\'p\', ((\'w\', 200), (\'x\', 30), (\'y\', 200), (\'z\', 30))), (\'q\', ((\'w\', 200), (\'x\', 34), (\'y\', 95), (\'z\', 73))), (\'r\', ((\'w\', \'a\'), (\'x\', \'b\'), (\'y\', \'c\'), (\'z\', \'d\'))), (\'s\', ((\'w\', 200), (\'x\', True), (\'y\', 200), (\'z\', True))), (\'t\', ((\'w\', 200), (\'x\', 200), (\'y\', 200), (\'z\', True))))\n                )\n\n    def test_frame_iter_element_c(self) -> None:\n\n        a2 = np.array([\n                [None, None],\n                [None, 1],\n                [None, 5]\n                ], dtype=object)\n        a1 = np.array([True, False, True])\n        a3 = np.array([[\'a\'], [\'b\'], [\'c\']])\n\n        tb1 = TypeBlocks.from_blocks((a3, a1, a2))\n\n        f1 = Frame(tb1,\n                index=self.get_letters(None, tb1.shape[0]),\n                columns=IndexHierarchy.from_product((\'i\', \'ii\'), (\'a\', \'b\'))\n                )\n        values = list(f1.iter_element())\n        self.assertEqual(values,\n                [\'a\', True, None, None, \'b\', False, None, 1, \'c\', True, None, 5]\n                )\n\n        f2 = f1.iter_element().apply(lambda x: str(x).lower().replace(\'e\', \'\'))\n\n        self.assertEqual(f1.columns.__class__, f2.columns.__class__,)\n\n        self.assertEqual(f2.to_pairs(0),\n                (((\'i\', \'a\'), ((\'a\', \'a\'), (\'b\', \'b\'), (\'c\', \'c\'))), ((\'i\', \'b\'), ((\'a\', \'tru\'), (\'b\', \'fals\'), (\'c\', \'tru\'))), ((\'ii\', \'a\'), ((\'a\', \'non\'), (\'b\', \'non\'), (\'c\', \'non\'))), ((\'ii\', \'b\'), ((\'a\', \'non\'), (\'b\', \'1\'), (\'c\', \'5\'))))\n                )\n\n\n    def test_frame_iter_element_d(self) -> None:\n        f1 = sf.Frame.from_elements([\'I\', \'II\', \'III\'], columns=(\'A\',))\n        f2 = sf.Frame.from_elements([67, 28, 99], columns=(\'B\',), index=(\'I\', \'II\', \'IV\'))\n\n        post = f1[\'A\'].iter_element().map_any(f2[\'B\'])\n\n        # if we do not match the mapping, we keep the value.\n        self.assertEqual(post.to_pairs(),\n                ((0, 67), (1, 28), (2, \'III\')))\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_iter_group_a(self) -> None:\n        columns = tuple(\'pqrst\')\n        index = tuple(\'zxwy\')\n        records = ((\'A\', 1, \'a\', False, False),\n                   (\'A\', 2, \'b\', True, False),\n                   (\'B\', 1, \'c\', False, False),\n                   (\'B\', 2, \'d\', True, True))\n\n        f = Frame.from_records(\n                records, columns=columns, index=index,name=\'foo\')\n        f = f.set_index_hierarchy((\'p\', \'q\'), drop=True)\n\n        with self.assertRaises(AxisInvalid):\n            _ = f.iter_group(\'s\', axis=-1).apply(lambda x: x.shape)\n\n        post = f.iter_group(\'s\').apply(lambda x: x.shape)\n        self.assertEqual(post.to_pairs(),\n                ((False, (2, 3)), (True, (2, 3)))\n                )\n\n\n    def test_frame_iter_group_b(self) -> None:\n        columns = tuple(\'pqrst\')\n        index = tuple(\'zxwy\')\n        records = ((\'A\', 1, \'a\', False, False),\n                   (\'A\', 2, \'b\', True, False),\n                   (\'B\', 1, \'c\', False, False),\n                   (\'B\', 2, \'d\', True, True))\n\n        f = Frame.from_records(\n                records, columns=columns, index=index, name=\'foo\')\n        post = f.iter_group([\'p\', \'q\']).apply(len)\n        self.assertEqual(post.to_pairs(),\n                (((\'A\', 1), 1), ((\'A\', 2), 1), ((\'B\', 1), 1), ((\'B\', 2), 1))\n                )\n\n\n    def test_frame_iter_group_c(self) -> None:\n        columns = tuple(\'pqrst\')\n        index = tuple(\'zxwy\')\n        records = ((\'A\', 1, \'a\', False, False),\n                   (\'A\', 2, \'b\', True, False),\n                   (\'B\', 1, \'c\', False, False),\n                   (\'B\', 2, \'d\', True, True))\n\n        f = Frame.from_records(\n                records, columns=columns, index=index, name=\'foo\')\n\n        with self.assertRaises(TypeError):\n            next(iter(f.iter_group(foo=\'x\')))\n\n        with self.assertRaises(TypeError):\n            next(iter(f.iter_group(3, 5)))\n\n        self.assertEqual(next(iter(f.iter_group(\'q\'))).to_pairs(0),\n                ((\'p\', ((\'z\', \'A\'), (\'w\', \'B\'))), (\'q\', ((\'z\', 1), (\'w\', 1))), (\'r\', ((\'z\', \'a\'), (\'w\', \'c\'))), (\'s\', ((\'z\', False), (\'w\', False))), (\'t\', ((\'z\', False), (\'w\', False))))\n                )\n\n\n    def test_frame_iter_group_items_a(self) -> None:\n\n        # testing a hierarchical index and columns, selecting column with a tuple\n\n        records = (\n                (\'a\', 999999, 0.1),\n                (\'a\', 201810, 0.1),\n                (\'b\', 999999, 0.4),\n                (\'b\', 201810, 0.4))\n        f1 = Frame.from_records(records, columns=list(\'abc\'))\n\n        f1 = f1.set_index_hierarchy([\'a\', \'b\'], drop=False)\n        f1 = f1.relabel_add_level(columns=\'i\')\n\n        groups = list(f1.iter_group_items((\'i\', \'a\'), axis=0))\n\n        self.assertEqual(groups[0][0], \'a\')\n        self.assertEqual(groups[0][1].to_pairs(0),\n                (((\'i\', \'a\'), (((\'a\', 999999), \'a\'), ((\'a\', 201810), \'a\'))), ((\'i\', \'b\'), (((\'a\', 999999), 999999), ((\'a\', 201810), 201810))), ((\'i\', \'c\'), (((\'a\', 999999), 0.1), ((\'a\', 201810), 0.1)))))\n\n        self.assertEqual(groups[1][0], \'b\')\n        self.assertEqual(groups[1][1].to_pairs(0),\n                (((\'i\', \'a\'), (((\'b\', 999999), \'b\'), ((\'b\', 201810), \'b\'))), ((\'i\', \'b\'), (((\'b\', 999999), 999999), ((\'b\', 201810), 201810))), ((\'i\', \'c\'), (((\'b\', 999999), 0.4), ((\'b\', 201810), 0.4)))))\n\n\n    def test_frame_iter_group_items_b(self) -> None:\n        columns = tuple(\'pqrst\')\n        index = tuple(\'zxwy\')\n        records = ((\'A\', 1, \'a\', False, False),\n                   (\'A\', 2, \'b\', True, False),\n                   (\'B\', 1, \'c\', False, False),\n                   (\'B\', 2, \'d\', True, True))\n\n        f = Frame.from_records(\n                records, columns=columns, index=index,name=\'foo\')\n        f = f.set_index_hierarchy((\'p\', \'q\'), drop=True)\n        post = f.iter_group_items(\'s\').apply(\n                lambda k, x: f\'{k}: {len(x)}\')\n        self.assertEqual(post.to_pairs(),\n                ((False, \'False: 2\'), (True, \'True: 2\'))\n                )\n\n\n    def test_frame_iter_group_items_c(self) -> None:\n        # Test optimized sorting approach. Data must have a non-object dtype and key must be single\n        data = np.array([[0, 1, 1, 3],\n                         [3, 3, 2, 3],\n                         [5, 5, 1, 3],\n                         [7, 2, 2, 4]])\n\n        frame = sf.Frame(data, columns=tuple(\'abcd\'), index=tuple(\'wxyz\'))\n\n        # Column\n        groups = list(frame.iter_group_items(\'c\', axis=0))\n        expected_pairs = [\n                ((\'a\', ((\'w\', 0), (\'y\', 5))),\n                 (\'b\', ((\'w\', 1), (\'y\', 5))),\n                 (\'c\', ((\'w\', 1), (\'y\', 1))),\n                 (\'d\', ((\'w\', 3), (\'y\', 3)))),\n                ((\'a\', ((\'x\', 3), (\'z\', 7))),\n                 (\'b\', ((\'x\', 3), (\'z\', 2))),\n                 (\'c\', ((\'x\', 2), (\'z\', 2))),\n                 (\'d\', ((\'x\', 3), (\'z\', 4))))]\n\n        self.assertEqual([1, 2], [group[0] for group in groups])\n        self.assertEqual(expected_pairs, [group[1].to_pairs(axis=0) for group in groups])\n\n\n        # Index\n        groups = list(frame.iter_group_items(\'w\', axis=1))\n        expected_pairs = [\n                ((\'a\', ((\'w\', 0), (\'x\', 3), (\'y\', 5), (\'z\', 7))),), #type: ignore\n                ((\'b\', ((\'w\', 1), (\'x\', 3), (\'y\', 5), (\'z\', 2))), #type: ignore\n                 (\'c\', ((\'w\', 1), (\'x\', 2), (\'y\', 1), (\'z\', 2)))),\n                ((\'d\', ((\'w\', 3), (\'x\', 3), (\'y\', 3), (\'z\', 4))),)] #type: ignore\n\n        self.assertEqual([0, 1, 3], [group[0] for group in groups])\n        self.assertEqual(expected_pairs, [group[1].to_pairs(axis=0) for group in groups])\n\n\n    def test_frame_iter_group_items_d(self) -> None:\n        # Test iterating with multiple key selection\n        data = np.array([[0, 1, 1, 3],\n                         [3, 3, 2, 3],\n                         [5, 5, 1, 3],\n                         [7, 2, 2, 4]])\n\n        frame = sf.Frame(data, columns=tuple(\'abcd\'), index=tuple(\'wxyz\'))\n\n        # Column\n        groups = list(frame.iter_group_items([\'c\', \'d\'], axis=0))\n        expected_pairs = [\n                ((\'a\', ((\'w\', 0), (\'y\', 5))),\n                 (\'b\', ((\'w\', 1), (\'y\', 5))),\n                 (\'c\', ((\'w\', 1), (\'y\', 1))),\n                 (\'d\', ((\'w\', 3), (\'y\', 3)))),\n                ((\'a\', ((\'x\', 3),)),\n                 (\'b\', ((\'x\', 3),)),\n                 (\'c\', ((\'x\', 2),)),\n                 (\'d\', ((\'x\', 3),))),\n                ((\'a\', ((\'z\', 7),)),\n                 (\'b\', ((\'z\', 2),)),\n                 (\'c\', ((\'z\', 2),)),\n                 (\'d\', ((\'z\', 4),)))]\n\n        self.assertEqual([(1, 3), (2, 3), (2, 4)], [group[0] for group in groups])\n        self.assertEqual(expected_pairs, [group[1].to_pairs(axis=0) for group in groups])\n\n\n        # Index\n        groups = list(frame.iter_group_items([\'x\', \'y\'], axis=1))\n        expected_pairs = [\n                ((\'c\', ((\'w\', 1), (\'x\', 2), (\'y\', 1), (\'z\', 2))),), #type: ignore\n                ((\'d\', ((\'w\', 3), (\'x\', 3), (\'y\', 3), (\'z\', 4))),), #type: ignore\n                ((\'a\', ((\'w\', 0), (\'x\', 3), (\'y\', 5), (\'z\', 7))), #type: ignore\n                 (\'b\', ((\'w\', 1), (\'x\', 3), (\'y\', 5), (\'z\', 2)))),\n        ]\n\n        self.assertEqual([(2, 1), (3, 3), (3, 5)], [group[0] for group in groups])\n        self.assertEqual(expected_pairs, [group[1].to_pairs(axis=0) for group in groups])\n\n\n    def test_frame_iter_group_items_e(self) -> None:\n        columns = tuple(\'pqrst\')\n        index = tuple(\'zxwy\')\n        records = ((\'A\', 1, \'a\', False, False),\n                   (\'A\', 2, \'b\', True, False),\n                   (\'B\', 1, \'c\', False, False),\n                   (\'B\', 2, \'d\', True, True))\n\n        f = Frame.from_records(\n                records, columns=columns, index=index,name=\'foo\')\n\n        #  using an array to select\n        self.assertEqual(\n                tuple(k for k, v in f.iter_group_items(f.columns == \'s\')),\n                ((False,), (True,))\n                )\n\n        self.assertEqual(\n                tuple(k for k, v in f.iter_group_items(f.columns.isin((\'p\', \'t\')))),\n                ((\'A\', False), (\'B\', False), (\'B\', True))\n                )\n        self.assertEqual(\n                tuple(k for k, v in f.iter_group_items([\'s\', \'t\'])),\n                ((False, False), (True, False), (True, True))\n                )\n\n        self.assertEqual(\n                tuple(k for k, v in f.iter_group_items(slice(\'s\',\'t\'))),\n                ((False, False), (True, False), (True, True))\n                )\n\n\n    def test_frame_iter_group_items_f(self) -> None:\n\n        objs = [object() for _ in range(2)]\n        data = [[1, 2, objs[0]], [3, 4, objs[0]], [5, 6, objs[1]]]\n        f = sf.Frame.from_records(data, columns=tuple(\'abc\'))\n\n        post1 = {k: v for k, v in f.iter_group_items(\'c\')}\n        post2 = {k[0]: v for k, v in f.iter_group_items([\'c\'])} # as a list, this gets a multiple key\n\n        self.assertEqual(len(post1), 2)\n        self.assertEqual(len(post1), len(post2))\n\n        obj_a = objs[0]\n        obj_b = objs[1]\n\n        self.assertEqual(post1[obj_a].shape, (2, 3))\n        self.assertEqual(post1[obj_a].shape, post2[obj_a].shape)\n        self.assertEqual(post1[obj_a].to_pairs(0),\n                ((\'a\', ((0, 1), (1, 3))), (\'b\', ((0, 2), (1, 4))), (\'c\', ((0, obj_a), (1, obj_a)))))\n        self.assertEqual(post2[obj_a].to_pairs(0),\n                ((\'a\', ((0, 1), (1, 3))), (\'b\', ((0, 2), (1, 4))), (\'c\', ((0, obj_a), (1, obj_a)))))\n\n\n        self.assertEqual(post1[obj_b].shape, (1, 3))\n        self.assertEqual(post1[obj_b].shape, post2[obj_b].shape)\n        self.assertEqual(post1[obj_b].to_pairs(0),\n                ((\'a\', ((2, 5),)), (\'b\', ((2, 6),)), (\'c\', ((2, obj_b),))))\n        self.assertEqual(post2[obj_b].to_pairs(0),\n                ((\'a\', ((2, 5),)), (\'b\', ((2, 6),)), (\'c\', ((2, obj_b),))))\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_iter_group_index_a(self) -> None:\n\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\', \'y\', \'z\'))\n\n        with self.assertRaises(TypeError):\n            f1.iter_group_labels(3, 4)\n\n        with self.assertRaises(TypeError):\n            f1.iter_group_labels(foo=4)\n\n\n        post = tuple(f1.iter_group_labels(0, axis=0))\n\n        self.assertEqual(len(post), 3)\n        self.assertEqual(\n                f1.iter_group_labels(0, axis=0).apply(lambda x: x[[\'p\', \'q\']].values.sum()).to_pairs(),\n                ((\'x\', 4), (\'y\', 64), (\'z\', 97))\n                )\n\n    def test_frame_iter_group_index_b(self) -> None:\n\n        records = (\n                (2, 2, \'a\', \'q\', False, False),\n                (30, 34, \'b\', \'c\', True, False),\n                (2, 95, \'c\', \'d\', False, False),\n                )\n        f1 = Frame.from_records(records,\n                columns=IndexHierarchy.from_product((1, 2, 3), (\'a\', \'b\')),\n                index=(\'x\', \'y\', \'z\'))\n\n        # with axis 1, we are grouping based on columns while maintain the index\n        post_tuple = tuple(f1.iter_group_labels(1, axis=1))\n\n        self.assertEqual(len(post_tuple), 2)\n\n        post = f1[HLoc[f1.columns[0]]]\n        self.assertEqual(post.__class__, Series)\n        self.assertEqual(post.to_pairs(),\n            ((\'x\', 2), (\'y\', 30), (\'z\', 2))\n            )\n\n        post = f1.loc[:, HLoc[f1.columns[0]]]\n        self.assertEqual(post.__class__, Series)\n        self.assertEqual(post.to_pairs(),\n            ((\'x\', 2), (\'y\', 30), (\'z\', 2))\n            )\n\n        self.assertEqual(\n                f1.iter_group_labels(1, axis=1).apply(lambda x: x.iloc[:, 0].sum()).to_pairs(),\n                ((\'a\', 34), (\'b\', 131))\n                )\n\n\n    def test_frame_iter_group_index_c(self) -> None:\n        columns = tuple(\'pqrst\')\n        index = tuple(\'zxwy\')\n        records = ((\'A\', 1, \'a\', False, False),\n                   (\'A\', 2, \'b\', True, False),\n                   (\'B\', 1, \'c\', False, False),\n                   (\'B\', 2, \'d\', True, True))\n\n        f = Frame.from_records(\n                records, columns=columns, index=index,name=\'foo\')\n        f = f.set_index_hierarchy((\'p\', \'q\'), drop=True)\n\n        with self.assertRaises(AxisInvalid):\n            _ = f.iter_group_labels_items(0, axis=-1).apply(lambda k, x: f\'{k}:{x.size}\')\n\n        post = f.iter_group_labels_items(0).apply(lambda k, x: f\'{k}:{x.size}\')\n\n        self.assertEqual(post.to_pairs(),\n                ((\'A\', \'A:6\'), (\'B\', \'B:6\'))\n        )\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_reversed(self) -> None:\n        columns = tuple(\'pqrst\')\n        index = tuple(\'zxwy\')\n        records = ((2, 2, \'a\', False, False),\n                   (30, 34, \'b\', True, False),\n                   (2, 95, \'c\', False, False),\n                   (30, 73, \'d\', True, True))\n\n        f = Frame.from_records(\n                records, columns=columns, index=index,name=\'foo\')\n\n        self.assertTrue(tuple(reversed(f)) == tuple(reversed(columns)))\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_sort_index_a(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'z\', \'x\', \'w\', \'y\'),\n                name=\'foo\')\n\n        f2 = f1.sort_index()\n        self.assertEqual(f2.to_pairs(0),\n                ((\'p\', ((\'w\', 2), (\'x\', 30), (\'y\', 30), (\'z\', 2))), (\'q\', ((\'w\', 95), (\'x\', 34), (\'y\', 73), (\'z\', 2))), (\'r\', ((\'w\', \'c\'), (\'x\', \'b\'), (\'y\', \'d\'), (\'z\', \'a\'))), (\'s\', ((\'w\', False), (\'x\', True), (\'y\', True), (\'z\', False))), (\'t\', ((\'w\', False), (\'x\', False), (\'y\', True), (\'z\', False)))))\n        self.assertEqual(f1.name, f2.name)\n\n        self.assertEqual(f1.sort_index(ascending=False).to_pairs(0),\n                ((\'p\', ((\'z\', 2), (\'y\', 30), (\'x\', 30), (\'w\', 2))), (\'q\', ((\'z\', 2), (\'y\', 73), (\'x\', 34), (\'w\', 95))), (\'r\', ((\'z\', \'a\'), (\'y\', \'d\'), (\'x\', \'b\'), (\'w\', \'c\'))), (\'s\', ((\'z\', False), (\'y\', True), (\'x\', True), (\'w\', False))), (\'t\', ((\'z\', False), (\'y\', True), (\'x\', False), (\'w\', False)))))\n\n\n\n    def test_frame_sort_index_b(self) -> None:\n        # reindex both axis\n        records = (\n                (\'a\', False, False),\n                (\'b\', True, False),\n                (\'c\', False, False),\n                (\'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=IndexHierarchy.from_product((1, 2), (10, 20), name=\'foo\'),\n                )\n\n        post = f1.sort_index(ascending=False)\n\n        self.assertEqual(post.index.name, f1.index.name)\n        self.assertEqual(post.to_pairs(0),\n                ((\'p\', (((2, 20), \'d\'), ((2, 10), \'c\'), ((1, 20), \'b\'), ((1, 10), \'a\'))), (\'q\', (((2, 20), True), ((2, 10), False), ((1, 20), True), ((1, 10), False))), (\'r\', (((2, 20), True), ((2, 10), False), ((1, 20), False), ((1, 10), False))))\n                )\n\n\n    #---------------------------------------------------------------------------\n\n\n    def test_frame_sort_columns_a(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'t\', \'s\', \'r\', \'q\', \'p\'),\n                index=(\'z\', \'x\', \'w\', \'y\'),\n                name=\'foo\')\n\n        f2 = f1.sort_columns()\n        self.assertEqual(\n                f2.to_pairs(0),\n                ((\'p\', ((\'z\', False), (\'x\', False), (\'w\', False), (\'y\', True))), (\'q\', ((\'z\', False), (\'x\', True), (\'w\', False), (\'y\', True))), (\'r\', ((\'z\', \'a\'), (\'x\', \'b\'), (\'w\', \'c\'), (\'y\', \'d\'))), (\'s\', ((\'z\', 2), (\'x\', 34), (\'w\', 95), (\'y\', 73))), (\'t\', ((\'z\', 2), (\'x\', 30), (\'w\', 2), (\'y\', 30)))))\n\n        self.assertEqual(f2.name, f1.name)\n\n\n    def test_frame_sort_columns_b(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, False, False),\n                (30, 34, True, False),\n                (2, 95, False, False),\n                (30, 73, True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=IndexHierarchy.from_product((1, 2), (10, 20), name=\'foo\'),\n                index=(\'z\', \'x\', \'w\', \'y\'),\n                )\n\n        f2 = f1.sort_columns(ascending=False)\n\n        self.assertEqual(f2.columns.name, f1.columns.name)\n\n        self.assertEqual(\n            f2.to_pairs(0),\n            (((2, 20), ((\'z\', False), (\'x\', False), (\'w\', False), (\'y\', True))), ((2, 10), ((\'z\', False), (\'x\', True), (\'w\', False), (\'y\', True))), ((1, 20), ((\'z\', 2), (\'x\', 34), (\'w\', 95), (\'y\', 73))), ((1, 10), ((\'z\', 2), (\'x\', 30), (\'w\', 2), (\'y\', 30))))\n            )\n\n\n    def test_frame_sort_columns_c(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, False, False),\n                (30, 34, True, False),\n                (2, 95, False, False),\n                (30, 73, True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=IndexYearMonth.from_year_month_range(\'2018-01\', \'2018-04\'),\n                index=(\'z\', \'x\', \'w\', \'y\'),\n                )\n        f2 = f1.sort_columns(ascending=False)\n\n        self.assertEqual(f2.to_pairs(0),\n            ((np.datetime64(\'2018-04\'), ((\'z\', False), (\'x\', False), (\'w\', False), (\'y\', True))), (np.datetime64(\'2018-03\'), ((\'z\', False), (\'x\', True), (\'w\', False), (\'y\', True))), (np.datetime64(\'2018-02\'), ((\'z\', 2), (\'x\', 34), (\'w\', 95), (\'y\', 73))), (np.datetime64(\'2018-01\'), ((\'z\', 2), (\'x\', 30), (\'w\', 2), (\'y\', 30))))\n        )\n\n        self.assertEqual(f2.columns.__class__, IndexYearMonth)\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_sort_values_a(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'c\', False, False),\n                (30, 34, \'d\', True, False),\n                (2, 95, \'a\', False, False),\n                (30, 73, \'b\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'r\', \'q\', \'t\', \'s\'),\n                index=(\'z\', \'x\', \'w\', \'y\'),\n                name=\'foo\')\n\n        post = f1.sort_values(\'q\')\n        self.assertEqual(post.name, f1.name)\n\n        self.assertEqual(post.to_pairs(0),\n                ((\'p\', ((\'w\', 2), (\'y\', 30), (\'z\', 2), (\'x\', 30))), (\'r\', ((\'w\', 95), (\'y\', 73), (\'z\', 2), (\'x\', 34))), (\'q\', ((\'w\', \'a\'), (\'y\', \'b\'), (\'z\', \'c\'), (\'x\', \'d\'))), (\'t\', ((\'w\', False), (\'y\', True), (\'z\', False), (\'x\', True))), (\'s\', ((\'w\', False), (\'y\', True), (\'z\', False), (\'x\', False)))))\n\n\n        self.assertEqual(f1.sort_values(\'p\').to_pairs(0),\n                ((\'p\', ((\'z\', 2), (\'w\', 2), (\'x\', 30), (\'y\', 30))), (\'r\', ((\'z\', 2), (\'w\', 95), (\'x\', 34), (\'y\', 73))), (\'q\', ((\'z\', \'c\'), (\'w\', \'a\'), (\'x\', \'d\'), (\'y\', \'b\'))), (\'t\', ((\'z\', False), (\'w\', False), (\'x\', True), (\'y\', True))), (\'s\', ((\'z\', False), (\'w\', False), (\'x\', False), (\'y\', True))))\n                )\n\n\n    def test_frame_sort_values_b(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'c\', False, False),\n                (30, 34, \'d\', True, False),\n                (2, 95, \'a\', True, False),\n                (30, 73, \'b\', False, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'r\', \'q\', \'t\', \'s\'),\n                index=(\'z\', \'x\', \'w\', \'y\'))\n\n        post = f1.sort_values((\'p\', \'t\'))\n\n        self.assertEqual(post.to_pairs(0),\n                ((\'p\', ((\'z\', 2), (\'w\', 2), (\'y\', 30), (\'x\', 30))), (\'r\', ((\'z\', 2), (\'w\', 95), (\'y\', 73), (\'x\', 34))), (\'q\', ((\'z\', \'c\'), (\'w\', \'a\'), (\'y\', \'b\'), (\'x\', \'d\'))), (\'t\', ((\'z\', False), (\'w\', True), (\'y\', False), (\'x\', True))), (\'s\', ((\'z\', False), (\'w\', False), (\'y\', True), (\'x\', False)))))\n\n\n\n    def test_frame_sort_values_c(self) -> None:\n\n        records = (\n                (2, 2, 3.5),\n                (30, 34, 60.2),\n                (2, 95, 1.2),\n                (30, 73, 50.2),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\', \'y\', \'z\'),\n                name=\'foo\')\n\n        f2 = f1.sort_values(\'y\', axis=0)\n        self.assertEqual(f2.to_pairs(0),\n                ((\'r\', ((\'w\', 3.5), (\'x\', 60.2), (\'y\', 1.2), (\'z\', 50.2))), (\'p\', ((\'w\', 2), (\'x\', 30), (\'y\', 2), (\'z\', 30))), (\'q\', ((\'w\', 2), (\'x\', 34), (\'y\', 95), (\'z\', 73)))))\n\n        self.assertEqual(f2.name, \'foo\')\n\n\n\n    def test_frame_sort_values_d(self) -> None:\n\n        a1 = np.arange(8).reshape(4, 2) / 10\n        match = ((\'a\', ((0.6, 0.6), (0.4, 0.4), (0.2, 0.2), (0.0, 0.0))), (\'b\', ((0.6, 0.7), (0.4, 0.5), (0.2, 0.3), (0.0, 0.1))))\n\n        f1 = Frame(a1, columns=(\'a\', \'b\'))\n        f1 = f1.set_index(\'a\')\n        f1 = f1.sort_values(\'b\', ascending=False)\n        self.assertEqual(f1.to_pairs(0), match)\n        self.assertEqual(f1.index.name, \'a\')\n\n        f2 = FrameGO(a1, columns=(\'a\', \'b\'))\n        f2 = f2.set_index(\'a\') # type: ignore\n        f2 = f2.sort_values(\'b\', ascending=False) # type: ignore\n        self.assertEqual(f2.to_pairs(0), match)\n        self.assertEqual(f2.index.name, \'a\')\n\n\n    def test_frame_sort_values_e(self) -> None:\n        # Ensure index sorting works on internally homogenous frames\n        data = np.array([[3, 7, 3],\n                         [8, 1, 4],\n                         [2, 9, 6]])\n        f1 = sf.Frame(data, columns=tuple(\'abc\'), index=tuple(\'xyz\'))\n        assert len(f1._blocks._blocks) == 1, \'f1 must be internally homogenous.\'\n\n        f1_sorted = f1.sort_values(\'x\', axis=0)\n\n        expected1 = ((\'x\', ((\'a\', 3), (\'c\', 3), (\'b\', 7))),\n                     (\'y\', ((\'a\', 8), (\'c\', 4), (\'b\', 1))),\n                     (\'z\', ((\'a\', 2), (\'c\', 6), (\'b\', 9))))\n        self.assertEqual(expected1, f1_sorted.to_pairs(axis=1))\n\n\n        # Ensure index sorting works on internally heterogeneous frames\n        records = ((4, 2, 3), (2, 3.1, False), (6, False, 3.4))\n        f2 = sf.Frame.from_records(records,\n                columns=tuple(\'abc\'),\n                index=tuple(\'xyz\'),\n                dtypes=(object, object, object))\n\n        assert len(f2._blocks._blocks) > 1, \'f2 must be internally heterogeneous.\'\n        f2_sorted = f2.sort_values(\'x\', axis=0)\n\n        expected2 = ((\'x\', ((\'b\', 2), (\'c\', 3), (\'a\', 4))),\n                     (\'y\', ((\'b\', 3.1), (\'c\', False), (\'a\', 2))),\n                     (\'z\', ((\'b\', False), (\'c\', 3.4), (\'a\', 6))))\n        self.assertEqual(expected2, f2_sorted.to_pairs(axis=1))\n\n\n    def test_frame_sort_values_f(self) -> None:\n        # Ensure index sorting works on internally homogenous frames\n        data = np.array([[3, 7, 3],\n                         [8, 1, 4],\n                         [2, 9, 6]])\n        f1 = sf.Frame(data, columns=tuple(\'abc\'), index=tuple(\'xyz\'))\n\n        with self.assertRaises(AxisInvalid):\n            _ = f1.sort_values((\'x\', \'z\'), axis=-1)\n\n        f1_sorted = f1.sort_values((\'x\', \'z\'), axis=0)\n        self.assertEqual(f1_sorted.to_pairs(0),\n                ((\'a\', ((\'x\', 3), (\'y\', 8), (\'z\', 2))), (\'c\', ((\'x\', 3), (\'y\', 4), (\'z\', 6))), (\'b\', ((\'x\', 7), (\'y\', 1), (\'z\', 9))))\n                )\n\n        f2_sorted = f1.sort_values([\'x\', \'z\'], axis=0)\n        self.assertEqual(f2_sorted.to_pairs(0),\n                ((\'a\', ((\'x\', 3), (\'y\', 8), (\'z\', 2))), (\'c\', ((\'x\', 3), (\'y\', 4), (\'z\', 6))), (\'b\', ((\'x\', 7), (\'y\', 1), (\'z\', 9))))\n                )\n\n        f3_sorted = f1.sort_values((k for k in \'xz\'), axis=0)\n        self.assertEqual(f3_sorted.to_pairs(0),\n                ((\'a\', ((\'x\', 3), (\'y\', 8), (\'z\', 2))), (\'c\', ((\'x\', 3), (\'y\', 4), (\'z\', 6))), (\'b\', ((\'x\', 7), (\'y\', 1), (\'z\', 9))))\n                )\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_relabel_a(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'c\', False, False),\n                (30, 34, \'d\', True, False),\n                (2, 95, \'a\', False, False),\n                (30, 73, \'b\', True, True),\n                )\n\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'r\', \'q\', \'t\', \'s\'),\n                index=(\'z\', \'x\', \'w\', \'y\'))\n\n        f2 = f1.relabel(columns={\'q\': \'QQQ\'})\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'p\', ((\'z\', 2), (\'x\', 30), (\'w\', 2), (\'y\', 30))), (\'r\', ((\'z\', 2), (\'x\', 34), (\'w\', 95), (\'y\', 73))), (\'QQQ\', ((\'z\', \'c\'), (\'x\', \'d\'), (\'w\', \'a\'), (\'y\', \'b\'))), (\'t\', ((\'z\', False), (\'x\', True), (\'w\', False), (\'y\', True))), (\'s\', ((\'z\', False), (\'x\', False), (\'w\', False), (\'y\', True))))\n                )\n\n        f3 = f1.relabel(index={\'y\': \'YYY\'})\n\n        self.assertEqual(f3.to_pairs(0),\n                ((\'p\', ((\'z\', 2), (\'x\', 30), (\'w\', 2), (\'YYY\', 30))), (\'r\', ((\'z\', 2), (\'x\', 34), (\'w\', 95), (\'YYY\', 73))), (\'q\', ((\'z\', \'c\'), (\'x\', \'d\'), (\'w\', \'a\'), (\'YYY\', \'b\'))), (\'t\', ((\'z\', False), (\'x\', True), (\'w\', False), (\'YYY\', True))), (\'s\', ((\'z\', False), (\'x\', False), (\'w\', False), (\'YYY\', True)))))\n\n        self.assertTrue((f1.mloc == f2.mloc).all())\n        self.assertTrue((f2.mloc == f3.mloc).all())\n\n\n    def test_frame_relabel_b(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'c\', False),\n                (30, 34, \'d\', True),\n                )\n\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'r\', \'q\', \'t\'),\n                index=(\'x\', \'y\')\n                )\n\n        f2 = f1.relabel(columns=IndexAutoFactory)\n        self.assertEqual(f2.columns.values.tolist(), [0, 1, 2, 3])\n\n\n    def test_frame_relabel_c(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                )\n        f1 = Frame.from_records(records,\n                columns=tuple(\'pqrs\'),\n                index=tuple(\'ab\'),\n                name=\'foo\')\n\n        f2 = f1.relabel(index=IndexAutoFactory)\n        self.assertEqual(f2.to_pairs(0),\n            ((\'p\', ((0, 1), (1, 30))), (\'q\', ((0, 2), (1, 34))), (\'r\', ((0, \'a\'), (1, \'b\'))), (\'s\', ((0, False), (1, True))))\n            )\n\n        f3 = f1.relabel(columns=IndexAutoFactory)\n        self.assertEqual(\n            f3.relabel(columns=IndexAutoFactory).to_pairs(0),\n            ((0, ((\'a\', 1), (\'b\', 30))), (1, ((\'a\', 2), (\'b\', 34))), (2, ((\'a\', \'a\'), (\'b\', \'b\'))), (3, ((\'a\', False), (\'b\', True))))\n            )\n        self.assertTrue(f3.columns.STATIC)\n\n\n    def test_frame_relabel_d(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                )\n        f1 = FrameGO.from_records(records,\n                columns=tuple(\'pqrs\'),\n                index=tuple(\'ab\'),\n                name=\'foo\')\n\n        f2 = f1.relabel(columns=IndexAutoFactory)\n        self.assertEqual(\n            f2.relabel(columns=IndexAutoFactory).to_pairs(0),\n            ((0, ((\'a\', 1), (\'b\', 30))), (1, ((\'a\', 2), (\'b\', 34))), (2, ((\'a\', \'a\'), (\'b\', \'b\'))), (3, ((\'a\', False), (\'b\', True))))\n            )\n        self.assertFalse(f2.columns.STATIC)\n        f2[4] = None\n        self.assertTrue(f2.columns._map is None)\n        f2[6] = None\n        self.assertFalse(f2.columns._map is None)\n\n        self.assertEqual(f2.to_pairs(0),\n                ((0, ((\'a\', 1), (\'b\', 30))), (1, ((\'a\', 2), (\'b\', 34))), (2, ((\'a\', \'a\'), (\'b\', \'b\'))), (3, ((\'a\', False), (\'b\', True))), (4, ((\'a\', None), (\'b\', None))), (6, ((\'a\', None), (\'b\', None))))\n                )\n\n\n    def test_frame_relabel_e(self) -> None:\n        f1 = FrameGO.from_dict(\n                {(\'A\', 1): (10, 20), (\'A\', 2): (40, 50), (\'B\', 1): (30, 50)}\n                )\n        # we have to convert the IH to an IHGO\n        f2 = f1.relabel(columns=IndexHierarchy.from_labels(f1.columns))\n        self.assertEqual(f2.columns.__class__, IndexHierarchyGO)\n        self.assertEqual(f2.to_pairs(0),\n                (((\'A\', 1), ((0, 10), (1, 20))), ((\'A\', 2), ((0, 40), (1, 50))), ((\'B\', 1), ((0, 30), (1, 50))))\n                )\n\n\n    def test_frame_relabel_f(self) -> None:\n        records = (\n                (2, \'a\', False),\n                (34, \'b\', True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\'))\n\n        with self.assertRaises(RuntimeError):\n            _ = f1.relabel()\n\n    def test_frame_relabel_g(self) -> None:\n\n        f1 = FrameGO.from_elements([1, 2], columns=[\'a\'])\n        f1[\'b\'] = f1[\'a\']\n        f2 = f1.relabel(columns={\'a\': \'c\',})\n        self.assertEqual(f2.to_pairs(0),\n                ((\'c\', ((0, 1), (1, 2))), (\'b\', ((0, 1), (1, 2))))\n                )\n\n    #---------------------------------------------------------------------------\n    def test_frame_rehierarch_a(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        index = IndexHierarchy.from_product((100, 200), (True, False))\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=index)\n\n        f2 = f1.rehierarch(index=(1,0), columns=(1,0))\n        self.assertEqual(f2.to_pairs(0),\n                (((1, \'a\'), (((True, 100), 1), ((True, 200), 54), ((False, 100), 30), ((False, 200), 65))), ((1, \'b\'), (((True, 100), \'a\'), ((True, 200), \'c\'), ((False, 100), \'b\'), ((False, 200), \'d\'))), ((2, \'a\'), (((True, 100), 2), ((True, 200), 95), ((False, 100), 34), ((False, 200), 73))), ((2, \'b\'), (((True, 100), False), ((True, 200), False), ((False, 100), True), ((False, 200), True))))\n                )\n\n    def test_frame_rehierarch_b(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                )\n        f1 = FrameGO.from_records(records,\n                columns=tuple(\'pqrs\'),\n                index=tuple(\'ab\'),\n                name=\'foo\')\n\n        # no hierarchy fails\n        with self.assertRaises(RuntimeError):\n            f1.rehierarch(index=(0, 1))\n\n        with self.assertRaises(RuntimeError):\n            f1.rehierarch(columns=(0, 1))\n\n\n    def test_frame_rehierarch_c(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        index = IndexHierarchy.from_product((100, 200), (True, False))\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=index)\n\n        f2 = f1.rehierarch(index=(1,0))\n        self.assertEqual(f2.to_pairs(0),\n                (((\'a\', 1), (((True, 100), 1), ((True, 200), 54), ((False, 100), 30), ((False, 200), 65))), ((\'a\', 2), (((True, 100), 2), ((True, 200), 95), ((False, 100), 34), ((False, 200), 73))), ((\'b\', 1), (((True, 100), \'a\'), ((True, 200), \'c\'), ((False, 100), \'b\'), ((False, 200), \'d\'))), ((\'b\', 2), (((True, 100), False), ((True, 200), False), ((False, 100), True), ((False, 200), True))))\n                )\n\n        f3 = f1.rehierarch(columns=(1,0))\n        self.assertEqual(f3.to_pairs(0),\n                (((1, \'a\'), (((100, True), 1), ((100, False), 30), ((200, True), 54), ((200, False), 65))), ((1, \'b\'), (((100, True), \'a\'), ((100, False), \'b\'), ((200, True), \'c\'), ((200, False), \'d\'))), ((2, \'a\'), (((100, True), 2), ((100, False), 34), ((200, True), 95), ((200, False), 73))), ((2, \'b\'), (((100, True), False), ((100, False), True), ((200, True), False), ((200, False), True))))\n                )\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_get_a(self) -> None:\n        # reindex both axis\n        records = (\n                (2, 2, \'c\', False, False),\n                (30, 34, \'d\', True, False),\n                (2, 95, \'a\', False, False),\n                (30, 73, \'b\', True, True),\n                )\n\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'r\', \'q\', \'t\', \'s\'),\n                index=(\'z\', \'x\', \'w\', \'y\'))\n\n        self.assertEqual(f1.get(\'r\').values.tolist(),\n                [2, 34, 95, 73])\n\n        self.assertEqual(f1.get(\'a\'), None)\n        self.assertEqual(f1.get(\'w\'), None)\n        self.assertEqual(f1.get(\'a\', -1), -1)\n\n    def test_frame_isna_a(self) -> None:\n        f1 = FrameGO.from_records([\n                [np.nan, 2, np.nan, 0],\n                [3, 4, np.nan, 1],\n                [np.nan, np.nan, np.nan, 5]],\n                columns=list(\'ABCD\'))\n\n        self.assertEqual(f1.isna().to_pairs(0),\n                ((\'A\', ((0, True), (1, False), (2, True))), (\'B\', ((0, False), (1, False), (2, True))), (\'C\', ((0, True), (1, True), (2, True))), (\'D\', ((0, False), (1, False), (2, False)))))\n\n        self.assertEqual(f1.notna().to_pairs(0),\n                ((\'A\', ((0, False), (1, True), (2, False))), (\'B\', ((0, True), (1, True), (2, False))), (\'C\', ((0, False), (1, False), (2, False))), (\'D\', ((0, True), (1, True), (2, True)))))\n\n    def test_frame_dropna_a(self) -> None:\n        f1 = FrameGO.from_records([\n                [np.nan, 2, np.nan, 0],\n                [3, 4, np.nan, 1],\n                [np.nan, np.nan, np.nan, np.nan]],\n                columns=list(\'ABCD\'))\n\n        self.assertAlmostEqualFramePairs(\n                f1.dropna(axis=0, condition=np.all).to_pairs(0),\n                ((\'A\', ((0, nan), (1, 3.0))), (\'B\', ((0, 2.0), (1, 4.0))), (\'C\', ((0, nan), (1, nan))), (\'D\', ((0, 0.0), (1, 1.0)))))\n\n        self.assertAlmostEqualFramePairs(\n                f1.dropna(axis=1, condition=np.all).to_pairs(0),\n                ((\'A\', ((0, nan), (1, 3.0), (2, nan))), (\'B\', ((0, 2.0), (1, 4.0), (2, nan))), (\'D\', ((0, 0.0), (1, 1.0), (2, nan)))))\n\n\n        f2 = f1.dropna(axis=0, condition=np.any)\n        # dropping to zero results in an empty DF in the same manner as Pandas; not sure if this is correct or ideal\n        self.assertEqual(f2.shape, (0, 4))\n\n        f3 = f1.dropna(axis=1, condition=np.any)\n        self.assertEqual(f3.shape, (3, 0))\n\n    def test_frame_dropna_b(self) -> None:\n        f1 = FrameGO.from_records([\n                [np.nan, 2, 3, 0],\n                [3, 4, np.nan, 1],\n                [0, 1, 2, 3]],\n                columns=list(\'ABCD\'))\n\n        self.assertEqual(f1.dropna(axis=0, condition=np.any).to_pairs(0),\n                ((\'A\', ((2, 0.0),)), (\'B\', ((2, 1.0),)), (\'C\', ((2, 2.0),)), (\'D\', ((2, 3.0),))))\n        self.assertEqual(f1.dropna(axis=1, condition=np.any).to_pairs(0),\n                ((\'B\', ((0, 2.0), (1, 4.0), (2, 1.0))), (\'D\', ((0, 0.0), (1, 1.0), (2, 3.0)))))\n\n    def test_frame_dropna_c(self) -> None:\n        f1 = Frame.from_records([\n                [np.nan, np.nan],\n                [np.nan, np.nan],],\n                columns=list(\'AB\'))\n        f2 = f1.dropna()\n        self.assertEqual(f2.shape, (0, 2))\n\n\n    @skip_win #type: ignore\n    def test_frame_fillna_a(self) -> None:\n        dtype = np.dtype\n\n        f1 = FrameGO.from_records([\n                [np.nan, 2, 3, 0],\n                [3, 4, np.nan, 1],\n                [0, 1, 2, 3]],\n                columns=list(\'ABCD\'))\n\n        f2 = f1.fillna(0)\n        self.assertEqual(f2.to_pairs(0),\n                ((\'A\', ((0, 0.0), (1, 3.0), (2, 0.0))), (\'B\', ((0, 2.0), (1, 4.0), (2, 1.0))), (\'C\', ((0, 3.0), (1, 0.0), (2, 2.0))), (\'D\', ((0, 0.0), (1, 1.0), (2, 3.0)))))\n\n        post = f2.dtypes\n        self.assertEqual(post.to_pairs(),\n                ((\'A\', dtype(\'float64\')), (\'B\', dtype(\'int64\')), (\'C\', dtype(\'float64\')), (\'D\', dtype(\'int64\'))))\n\n        f3 = f1.fillna(None)\n        self.assertEqual(f3.to_pairs(0),\n                ((\'A\', ((0, None), (1, 3.0), (2, 0.0))), (\'B\', ((0, 2.0), (1, 4.0), (2, 1.0))), (\'C\', ((0, 3.0), (1, None), (2, 2.0))), (\'D\', ((0, 0.0), (1, 1.0), (2, 3.0)))))\n\n        post = f3.dtypes\n        self.assertEqual(post.to_pairs(),\n                ((\'A\', dtype(\'O\')), (\'B\', dtype(\'int64\')), (\'C\', dtype(\'O\')), (\'D\', dtype(\'int64\'))))\n\n    @skip_win #type: ignore\n    def test_frame_fillna_b(self) -> None:\n\n        f1 = Frame.from_records([\n                [np.nan, 2, 3, 0],\n                [3, np.nan, 20, 1],\n                [0, 1, 2, 3]],\n                columns=tuple(\'ABCD\'),\n                index=tuple(\'wxy\'),\n                )\n\n        f2 = Frame.from_records([\n                [300, 2],\n                [3, 200],\n                ],\n                columns=tuple(\'AB\'),\n                index=tuple(\'wx\'),\n                )\n\n        f3 = f1.fillna(f2)\n\n        self.assertEqual(f3.dtypes.values.tolist(),\n                [np.dtype(\'float64\'), np.dtype(\'float64\'), np.dtype(\'int64\'), np.dtype(\'int64\')]\n                )\n        self.assertEqual(f3.to_pairs(0),\n                ((\'A\', ((\'w\', 300.0), (\'x\', 3.0), (\'y\', 0.0))), (\'B\', ((\'w\', 2.0), (\'x\', 200.0), (\'y\', 1.0))), (\'C\', ((\'w\', 3), (\'x\', 20), (\'y\', 2))), (\'D\', ((\'w\', 0), (\'x\', 1), (\'y\', 3))))\n                )\n\n    def test_frame_fillna_c(self) -> None:\n\n        f1 = Frame.from_records([\n                [np.nan, 2, 3, 0],\n                [3, 30, None, None],\n                [0, np.nan, 2, 3]],\n                columns=tuple(\'ABCD\'),\n                index=tuple(\'wxy\'),\n                )\n\n        f2 = Frame.from_records([\n                [300, 230],\n                [110, 200],\n                [580, 750],\n                ],\n                columns=tuple(\'AB\'),\n                index=tuple(\'yxw\'),\n                )\n\n        f3 = f1.fillna(f2)\n\n        self.assertEqual(f3.dtypes.values.tolist(),\n                [np.dtype(\'float64\'), np.dtype(\'float64\'), np.dtype(\'O\'), np.dtype(\'O\')]\n                )\n        self.assertEqual(f3.to_pairs(0),\n                ((\'A\', ((\'w\', 580.0), (\'x\', 3.0), (\'y\', 0.0))), (\'B\', ((\'w\', 2.0), (\'x\', 30.0), (\'y\', 230.0))), (\'C\', ((\'w\', 3), (\'x\', None), (\'y\', 2))), (\'D\', ((\'w\', 0), (\'x\', None), (\'y\', 3))))\n                )\n\n\n    def test_frame_fillna_d(self) -> None:\n\n        f1 = Frame.from_records([\n                [None, 2, 3, 0],\n                [3, 30, None, None],\n                [0, None, 2, 3]],\n                columns=tuple(\'ABCD\'),\n                index=tuple(\'wxy\'),\n                )\n\n        f2 = Frame.from_records([\n                [300, 230],\n                [110, 200],\n                ],\n                columns=tuple(\'CB\'),\n                index=tuple(\'yx\'),\n                )\n\n        f3 = f1.fillna(f2)\n\n        self.assertEqual(f3.to_pairs(0),\n                ((\'A\', ((\'w\', None), (\'x\', 3), (\'y\', 0))), (\'B\', ((\'w\', 2), (\'x\', 30), (\'y\', 230))), (\'C\', ((\'w\', 3), (\'x\', 110), (\'y\', 2))), (\'D\', ((\'w\', 0), (\'x\', None), (\'y\', 3))))\n                )\n\n        # assure we do not fill with float when reindexing\n        self.assertEqual([type(v) for v in f3[\'B\'].values.tolist()], [int, int, int])\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_fillna_leading_a(self) -> None:\n        a2 = np.array([\n                [None, None, None, None],\n                [None, 1, None, 6],\n                [None, 5, None, None]\n                ], dtype=object)\n        a1 = np.array([None, None, None], dtype=object)\n        a3 = np.array([\n                [None, 4],\n                [None, 1],\n                [None, 5]\n                ], dtype=object)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        f1 = Frame(tb1,\n                index=self.get_letters(None, tb1.shape[0]),\n                columns=self.get_letters(-tb1.shape[1], None)\n                )\n\n        self.assertEqual(f1.fillna_leading(0, axis=0).to_pairs(0),\n                ((\'t\', ((\'a\', 0), (\'b\', 0), (\'c\', 0))), (\'u\', ((\'a\', 0), (\'b\', 0), (\'c\', 0))), (\'v\', ((\'a\', 0), (\'b\', 1), (\'c\', 5))), (\'w\', ((\'a\', 0), (\'b\', 0), (\'c\', 0))), (\'x\', ((\'a\', 0), (\'b\', 6), (\'c\', None))), (\'y\', ((\'a\', 0), (\'b\', 0), (\'c\', 0))), (\'z\', ((\'a\', 4), (\'b\', 1), (\'c\', 5)))))\n\n        self.assertEqual(f1.fillna_leading(0, axis=1).to_pairs(0),\n                ((\'t\', ((\'a\', 0), (\'b\', 0), (\'c\', 0))), (\'u\', ((\'a\', 0), (\'b\', 0), (\'c\', 0))), (\'v\', ((\'a\', 0), (\'b\', 1), (\'c\', 5))), (\'w\', ((\'a\', 0), (\'b\', None), (\'c\', None))), (\'x\', ((\'a\', 0), (\'b\', 6), (\'c\', None))), (\'y\', ((\'a\', 0), (\'b\', None), (\'c\', None))), (\'z\', ((\'a\', 4), (\'b\', 1), (\'c\', 5)))))\n\n\n    def test_frame_fillna_trailing_a(self) -> None:\n        a2 = np.array([\n                [None, None, None, None],\n                [None, 1, None, 6],\n                [None, 5, None, None]\n                ], dtype=object)\n        a1 = np.array([None, None, None], dtype=object)\n        a3 = np.array([\n                [None, 4],\n                [None, 1],\n                [None, 5]\n                ], dtype=object)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        f1 = Frame(tb1,\n                index=self.get_letters(None, tb1.shape[0]),\n                columns=self.get_letters(-tb1.shape[1], None)\n                )\n\n        self.assertEqual(f1.fillna_trailing(0, axis=0).to_pairs(0),\n                ((\'t\', ((\'a\', 0), (\'b\', 0), (\'c\', 0))), (\'u\', ((\'a\', 0), (\'b\', 0), (\'c\', 0))), (\'v\', ((\'a\', None), (\'b\', 1), (\'c\', 5))), (\'w\', ((\'a\', 0), (\'b\', 0), (\'c\', 0))), (\'x\', ((\'a\', None), (\'b\', 6), (\'c\', 0))), (\'y\', ((\'a\', 0), (\'b\', 0), (\'c\', 0))), (\'z\', ((\'a\', 4), (\'b\', 1), (\'c\', 5))))\n                )\n\n        self.assertEqual(f1.fillna_trailing(0, axis=1).to_pairs(0),\n                ((\'t\', ((\'a\', None), (\'b\', None), (\'c\', None))), (\'u\', ((\'a\', None), (\'b\', None), (\'c\', None))), (\'v\', ((\'a\', None), (\'b\', 1), (\'c\', 5))), (\'w\', ((\'a\', None), (\'b\', None), (\'c\', None))), (\'x\', ((\'a\', None), (\'b\', 6), (\'c\', None))), (\'y\', ((\'a\', None), (\'b\', None), (\'c\', None))), (\'z\', ((\'a\', 4), (\'b\', 1), (\'c\', 5))))\n                )\n\n\n\n    def test_frame_fillna_forward_a(self) -> None:\n        a2 = np.array([\n                [8, None, None, None],\n                [None, 1, None, 6],\n                [0, 5, None, None]\n                ], dtype=object)\n        a1 = np.array([None, 3, None], dtype=object)\n        a3 = np.array([\n                [None, 4],\n                [None, 1],\n                [None, 5]\n                ], dtype=object)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        f1 = Frame(tb1,\n                index=self.get_letters(None, tb1.shape[0]),\n                columns=self.get_letters(-tb1.shape[1], None)\n                )\n\n        self.assertEqual(\n                f1.fillna_forward().to_pairs(0),\n                ((\'t\', ((\'a\', None), (\'b\', 3), (\'c\', 3))), (\'u\', ((\'a\', 8), (\'b\', 8), (\'c\', 0))), (\'v\', ((\'a\', None), (\'b\', 1), (\'c\', 5))), (\'w\', ((\'a\', None), (\'b\', None), (\'c\', None))), (\'x\', ((\'a\', None), (\'b\', 6), (\'c\', 6))), (\'y\', ((\'a\', None), (\'b\', None), (\'c\', None))), (\'z\', ((\'a\', 4), (\'b\', 1), (\'c\', 5))))\n                )\n\n        self.assertEqual(\n                f1.fillna_backward().to_pairs(0),\n                ((\'t\', ((\'a\', 3), (\'b\', 3), (\'c\', None))), (\'u\', ((\'a\', 8), (\'b\', 0), (\'c\', 0))), (\'v\', ((\'a\', 1), (\'b\', 1), (\'c\', 5))), (\'w\', ((\'a\', None), (\'b\', None), (\'c\', None))), (\'x\', ((\'a\', 6), (\'b\', 6), (\'c\', None))), (\'y\', ((\'a\', None), (\'b\', None), (\'c\', None))), (\'z\', ((\'a\', 4), (\'b\', 1), (\'c\', 5))))\n                )\n\n\n\n    def test_frame_fillna_forward_b(self) -> None:\n        a2 = np.array([\n                [8, None, None, None],\n                [None, 1, None, 6],\n                [0, 5, None, None]\n                ], dtype=object)\n        a1 = np.array([None, 3, None], dtype=object)\n        a3 = np.array([\n                [None, 4],\n                [None, 1],\n                [None, 5]\n                ], dtype=object)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        f1 = Frame(tb1,\n                index=self.get_letters(None, tb1.shape[0]),\n                columns=self.get_letters(-tb1.shape[1], None)\n                )\n        # axis 1 tests\n        self.assertEqual(\n                f1.fillna_forward(axis=1).to_pairs(0),\n                ((\'t\', ((\'a\', None), (\'b\', 3), (\'c\', None))), (\'u\', ((\'a\', 8), (\'b\', 3), (\'c\', 0))), (\'v\', ((\'a\', 8), (\'b\', 1), (\'c\', 5))), (\'w\', ((\'a\', 8), (\'b\', 1), (\'c\', 5))), (\'x\', ((\'a\', 8), (\'b\', 6), (\'c\', 5))), (\'y\', ((\'a\', 8), (\'b\', 6), (\'c\', 5))), (\'z\', ((\'a\', 4), (\'b\', 1), (\'c\', 5))))\n                )\n\n        self.assertEqual(\n                f1.fillna_backward(axis=1).to_pairs(0),\n                ((\'t\', ((\'a\', 8), (\'b\', 3), (\'c\', 0))), (\'u\', ((\'a\', 8), (\'b\', 1), (\'c\', 0))), (\'v\', ((\'a\', 4), (\'b\', 1), (\'c\', 5))), (\'w\', ((\'a\', 4), (\'b\', 6), (\'c\', 5))), (\'x\', ((\'a\', 4), (\'b\', 6), (\'c\', 5))), (\'y\', ((\'a\', 4), (\'b\', 1), (\'c\', 5))), (\'z\', ((\'a\', 4), (\'b\', 1), (\'c\', 5))))\n                )\n\n    def test_frame_fillna_forward_c(self) -> None:\n        a2 = np.array([\n                [8, None, None, None],\n                [None, 1, None, 6],\n                [0, 5, None, None]\n                ], dtype=object)\n        a1 = np.array([None, 3, None], dtype=object)\n        a3 = np.array([\n                [None, 4],\n                [None, None],\n                [None, 5]\n                ], dtype=object)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        f1 = Frame(tb1,\n                index=self.get_letters(None, tb1.shape[0]),\n                columns=self.get_letters(-tb1.shape[1], None)\n                )\n        post = f1.fillna_forward(axis=1)\n\n        self.assertEqual(f1.fillna_forward(axis=1, limit=1).to_pairs(0),\n                ((\'t\', ((\'a\', None), (\'b\', 3), (\'c\', None))), (\'u\', ((\'a\', 8), (\'b\', 3), (\'c\', 0))), (\'v\', ((\'a\', 8), (\'b\', 1), (\'c\', 5))), (\'w\', ((\'a\', None), (\'b\', 1), (\'c\', 5))), (\'x\', ((\'a\', None), (\'b\', 6), (\'c\', None))), (\'y\', ((\'a\', None), (\'b\', 6), (\'c\', None))), (\'z\', ((\'a\', 4), (\'b\', None), (\'c\', 5))))\n                )\n\n        self.assertEqual(f1.fillna_forward(axis=1, limit=2).to_pairs(0),\n                ((\'t\', ((\'a\', None), (\'b\', 3), (\'c\', None))), (\'u\', ((\'a\', 8), (\'b\', 3), (\'c\', 0))), (\'v\', ((\'a\', 8), (\'b\', 1), (\'c\', 5))), (\'w\', ((\'a\', 8), (\'b\', 1), (\'c\', 5))), (\'x\', ((\'a\', None), (\'b\', 6), (\'c\', 5))), (\'y\', ((\'a\', None), (\'b\', 6), (\'c\', None))), (\'z\', ((\'a\', 4), (\'b\', 6), (\'c\', 5))))\n                )\n\n        self.assertEqual(f1.fillna_backward(axis=1, limit=2).to_pairs(0),\n                ((\'t\', ((\'a\', 8), (\'b\', 3), (\'c\', 0))), (\'u\', ((\'a\', 8), (\'b\', 1), (\'c\', 0))), (\'v\', ((\'a\', None), (\'b\', 1), (\'c\', 5))), (\'w\', ((\'a\', None), (\'b\', 6), (\'c\', None))), (\'x\', ((\'a\', 4), (\'b\', 6), (\'c\', 5))), (\'y\', ((\'a\', 4), (\'b\', None), (\'c\', 5))), (\'z\', ((\'a\', 4), (\'b\', None), (\'c\', 5))))\n                )\n\n\n    def test_frame_empty_a(self) -> None:\n\n        f1 = FrameGO(index=(\'a\', \'b\', \'c\'))\n        f1[\'w\'] = Series.from_items(zip(\'cebga\', (10, 20, 30, 40, 50)))\n        f1[\'x\'] = Series.from_items(zip(\'abc\', range(3, 6)))\n        f1[\'y\'] = Series.from_items(zip(\'abcd\', range(2, 6)))\n        f1[\'z\'] = Series.from_items(zip(\'qabc\', range(7, 11)))\n\n        self.assertEqual(f1.to_pairs(0),\n                ((\'w\', ((\'a\', 50), (\'b\', 30), (\'c\', 10))), (\'x\', ((\'a\', 3), (\'b\', 4), (\'c\', 5))), (\'y\', ((\'a\', 2), (\'b\', 3), (\'c\', 4))), (\'z\', ((\'a\', 8), (\'b\', 9), (\'c\', 10)))))\n\n\n    #---------------------------------------------------------------------------\n    @skip_win  # type: ignore\n    def test_frame_from_csv_a(self) -> None:\n        # header, mixed types, no index\n\n        s1 = StringIO(\'count,score,color\\n1,1.3,red\\n3,5.2,green\\n100,3.4,blue\\n4,9.0,black\')\n\n        f1 = Frame.from_csv(s1)\n\n        post = f1.iloc[:, :2].sum(axis=0)\n        self.assertEqual(post.to_pairs(),\n                ((\'count\', 108.0), (\'score\', 18.9)))\n        self.assertEqual(f1.shape, (4, 3))\n\n        self.assertEqual(f1.dtypes.iter_element().apply(str).to_pairs(),\n                ((\'count\', \'int64\'), (\'score\', \'float64\'), (\'color\', \'<U5\')))\n\n\n        s2 = StringIO(\'color,count,score\\nred,1,1.3\\ngreen,3,5.2\\nblue,100,3.4\\nblack,4,9.0\')\n\n        f2 = Frame.from_csv(s2)\n        self.assertEqual(f2[\'count\':].sum().to_pairs(),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                ((\'count\', 108.0), (\'score\', 18.9)))\n        self.assertEqual(f2.shape, (4, 3))\n        self.assertEqual(f2.dtypes.iter_element().apply(str).to_pairs(),\n                ((\'color\', \'<U5\'), (\'count\', \'int64\'), (\'score\', \'float64\')))\n\n\n        # add junk at beginning and end\n        s3 = StringIO(\'junk\\ncolor,count,score\\nred,1,1.3\\ngreen,3,5.2\\nblue,100,3.4\\nblack,4,9.0\\njunk\')\n\n        f3 = Frame.from_csv(s3, skip_header=1, skip_footer=1)\n        self.assertEqual(f3.shape, (4, 3))\n        self.assertEqual(f3.dtypes.iter_element().apply(str).to_pairs(),\n                ((\'color\', \'<U5\'), (\'count\', \'int64\'), (\'score\', \'float64\')))\n\n\n\n    def test_frame_from_csv_b(self) -> None:\n        filelike = StringIO(\'\'\'count,number,weight,scalar,color,active\n0,4,234.5,5.3,\'red\',False\n30,50,9.234,5.434,\'blue\',True\'\'\')\n        f1 = Frame.from_csv(filelike)\n\n        self.assertEqual(f1.columns.values.tolist(),\n                [\'count\', \'number\', \'weight\', \'scalar\', \'color\', \'active\'])\n\n\n    def test_frame_from_csv_c(self) -> None:\n        s1 = StringIO(\'color,count,score\\nred,1,1.3\\ngreen,3,5.2\\nblue,100,3.4\\nblack,4,9.0\')\n        f1 = Frame.from_csv(s1, index_depth=1)\n        self.assertEqual(f1.to_pairs(0),\n                ((\'count\', ((\'red\', 1), (\'green\', 3), (\'blue\', 100), (\'black\', 4))), (\'score\', ((\'red\', 1.3), (\'green\', 5.2), (\'blue\', 3.4), (\'black\', 9.0)))))\n\n\n    def test_frame_from_csv_d(self) -> None:\n        s1 = StringIO(\'color,count,score\\n\')\n        f1 = Frame.from_csv(s1, columns_depth=1)\n        self.assertEqual(f1.to_pairs(0),\n            ((\'color\', ()), (\'count\', ()), (\'score\', ()))\n            )\n\n    def test_frame_from_csv_e(self) -> None:\n        s1 = StringIO(\'group,count,score,color\\nA,1,1.3,red\\nA,3,5.2,green\\nB,100,3.4,blue\\nB,4,9.0,black\')\n\n        f1 = sf.Frame.from_csv(\n                s1,\n                index_depth=2,\n                columns_depth=1)\n        self.assertEqual(f1.index.__class__, IndexHierarchy)\n        self.assertEqual(f1.to_pairs(0),\n                ((\'score\', (((\'A\', 1), 1.3), ((\'A\', 3), 5.2), ((\'B\', 100), 3.4), ((\'B\', 4), 9.0))), (\'color\', (((\'A\', 1), \'red\'), ((\'A\', 3), \'green\'), ((\'B\', 100), \'blue\'), ((\'B\', 4), \'black\')))))\n\n\n    def test_frame_from_csv_f(self) -> None:\n        s1 = StringIO(\'group,count,score,color\\nA,nan,1.3,red\\nB,NaN,5.2,green\\nC,NULL,3.4,blue\\nD,,9.0,black\')\n\n        f1 = sf.Frame.from_csv(\n                s1,\n                index_depth=1,\n                columns_depth=1)\n\n        self.assertAlmostEqualFramePairs(f1.to_pairs(0),\n                ((\'count\', ((\'A\', np.nan), (\'B\', np.nan), (\'C\', np.nan), (\'D\', np.nan))), (\'score\', ((\'A\', 1.3), (\'B\', 5.2), (\'C\', 3.4), (\'D\', 9.0))), (\'color\', ((\'A\', \'red\'), (\'B\', \'green\'), (\'C\', \'blue\'), (\'D\', \'black\'))))\n                )\n\n\n    def test_frame_from_csv_g(self) -> None:\n        filelike = StringIO(\'\'\'0,4,234.5,5.3,\'red\',False\n30,50,9.234,5.434,\'blue\',True\'\'\')\n        f1 = Frame.from_csv(filelike, columns_depth=0)\n        self.assertEqual(f1.to_pairs(0),\n            ((0, ((0, 0), (1, 30))), (1, ((0, 4), (1, 50))), (2, ((0, 234.5), (1, 9.234))), (3, ((0, 5.3), (1, 5.434))), (4, ((0, ""\'red\'""), (1, ""\'blue\'""))), (5, ((0, False), (1, True))))\n            )\n\n    def test_frame_from_csv_h(self) -> None:\n        s1 = StringIO(\'group,count,score,color\\nA,nan,1.3,red\\nB,NaN,5.2,green\\nC,NULL,3.4,blue\\nD,,9.0,black\')\n\n        f1 = sf.Frame.from_csv(\n                s1,\n                index_depth=1,\n                columns_depth=1,\n                dtypes=dict(score=np.float16))\n\n        self.assertEqual(f1.dtypes.to_pairs(),\n                ((\'count\', np.dtype(\'O\')),\n                (\'score\', np.dtype(\'float16\')),\n                (\'color\', np.dtype(\'<U5\'))))\n\n    @skip_win  #type: ignore\n    def test_frame_from_csv_i(self) -> None:\n        s1 = StringIO(\'1,2,3\\n4,5,6\')\n\n        f1 = sf.Frame.from_csv(\n                s1,\n                index_depth=0,\n                columns_depth=0,\n                dtypes=[np.int64, str, np.int64]\n                )\n\n        self.assertEqual(f1.dtypes.values.tolist(),\n                [np.dtype(\'int64\'), np.dtype(\'<U21\'), np.dtype(\'int64\')]\n                )\n\n    def test_frame_from_csv_j(self) -> None:\n        s1 = StringIO(\'1,2,3\\n4,5,6\')\n\n        f2 = sf.Frame.from_csv(\n                s1,\n                index_depth=2,\n                columns_depth=0,\n                dtypes=[np.int64, str, np.int64]\n                )\n\n        self.assertEqual(f2.to_pairs(0,),\n                ((0, (((1, \'2\'), 3), ((4, \'5\'), 6))),)\n                )\n\n    def test_frame_from_csv_k(self) -> None:\n        s1 = StringIO(\'1\\t2\\t3\\t4\\n\')\n        f1 = Frame.from_csv(s1, index_depth=0, columns_depth=0)\n        self.assertEqual(f1.to_pairs(0),\n                ((0, ((0, 1),)), (1, ((0, 2),)), (2, ((0, 3),)), (3, ((0, 4),)))\n                )\n\n    #---------------------------------------------------------------------------\n\n    @skip_win  # type: ignore\n    def test_structured_array_to_d_ia_cl_a(self) -> None:\n\n        a1 = np.array(np.arange(12).reshape((3, 4)))\n        post, _, _ = Frame._structured_array_to_d_ia_cl(\n                a1,\n                dtypes=[np.int64, str, np.int64, str]\n                )\n\n        self.assertEqual(post.dtypes.tolist(),\n                [np.dtype(\'int64\'), np.dtype(\'<U21\'), np.dtype(\'int64\'), np.dtype(\'<U21\')]\n                )\n\n    def test_structured_arrayto_d_ia_cl_b(self) -> None:\n\n        a1 = np.array(np.arange(12).reshape((3, 4)))\n        post, _, _ = Frame._structured_array_to_d_ia_cl(\n                a1,\n                dtypes=[np.int64, str, str, str],\n                consolidate_blocks=True,\n                )\n        self.assertEqual(post.shapes.tolist(), [(3,), (3, 3)])\n\n\n    def test_structured_arrayto_d_ia_cl_c(self) -> None:\n\n        a1 = np.array(np.arange(12).reshape((3, 4)))\n\n        with self.assertRaises(ErrorInitFrame):\n            # cannot specify index_column_first if index_depth is 0\n            post, _, _ = Frame._structured_array_to_d_ia_cl(\n                    a1,\n                    index_depth=0,\n                    index_column_first=1,\n                    dtypes=[np.int64, str, str, str],\n                    consolidate_blocks=True,\n                    )\n\n    #---------------------------------------------------------------------------\n    def test_from_data_index_arrays_column_labels_a(self) -> None:\n\n        tb = TypeBlocks.from_blocks(np.array([3,4,5]))\n\n        f1 = Frame._from_data_index_arrays_column_labels(\n                data=tb,\n                index_depth=0,\n                index_arrays=(),\n                columns_depth=0,\n                columns_labels=(),\n                name=\'foo\',\n                )\n        self.assertEqual(f1.to_pairs(0),\n                ((0, ((0, 3), (1, 4), (2, 5))),))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_from_delimited_a(self) -> None:\n\n        with temp_file(\'.txt\', path=True) as fp:\n\n            with open(fp, \'w\') as file:\n                file.write(\'\\n\'.join((\'index|A|B\', \'a|True|20.2\', \'b|False|85.3\')))\n                file.close()\n\n            with self.assertRaises(ErrorInitFrame):\n                f = Frame.from_delimited(fp, index_depth=1, delimiter=\'|\', skip_header=-1)\n\n            f = Frame.from_delimited(fp, index_depth=1, delimiter=\'|\')\n            self.assertEqual(f.to_pairs(0),\n                    ((\'A\', ((\'a\', True), (\'b\', False))), (\'B\', ((\'a\', 20.2), (\'b\', 85.3)))))\n\n    def test_frame_from_tsv_a(self) -> None:\n\n        with temp_file(\'.txt\', path=True) as fp:\n\n            with open(fp, \'w\') as file:\n                file.write(\'\\n\'.join((\'index\\tA\\tB\', \'a\\tTrue\\t20.2\', \'b\\tFalse\\t85.3\')))\n                file.close()\n\n            f = Frame.from_tsv(fp, index_depth=1, dtypes={\'a\': bool})\n            self.assertEqual(\n                    f.to_pairs(0),\n                    ((\'A\', ((\'a\', True), (\'b\', False))), (\'B\', ((\'a\', 20.2), (\'b\', 85.3))))\n                    )\n\n\n    def test_frame_from_tsv_b(self) -> None:\n        # a generator of delimited strings also works\n\n        def lines() -> tp.Iterator[str]:\n            yield \'a\\tb\\tc\\td\'\n            for i in range(4):\n                yield f\'{i}\\t{i + 1}\\t{i + 2}\\t{i + 3}\'\n\n        f = Frame.from_tsv(lines())\n        self.assertEqual(f.to_pairs(0),\n                ((\'a\', ((0, 0), (1, 1), (2, 2), (3, 3))), (\'b\', ((0, 1), (1, 2), (2, 3), (3, 4))), (\'c\', ((0, 2), (1, 3), (2, 4), (3, 5))), (\'d\', ((0, 3), (1, 4), (2, 5), (3, 6))))\n                )\n\n    def test_frame_from_tsv_c(self) -> None:\n        input_stream = StringIO(\'\'\'\n        196412\t0.0\n        196501\t0.0\n        196502\t0.0\n        196503\t0.0\n        196504\t0.0\n        196505\t0.0\'\'\')\n\n\n        f1 = sf.Frame.from_tsv(\n                input_stream,\n                index_depth=1,\n                columns_depth=0)\n\n        self.assertEqual(f1.to_pairs(0),\n                ((0, ((196412, 0.0), (196501, 0.0), (196502, 0.0), (196503, 0.0), (196504, 0.0), (196505, 0.0))),))\n\n        input_stream = StringIO(\'\'\'\n        196412\t0.0\t0.1\n        196501\t0.0\t0.1\n        196502\t0.0\t0.1\n        196503\t0.0\t0.1\n        196504\t0.0\t0.1\n        196505\t0.0\t0.1\'\'\')\n\n\n        f2 = sf.Frame.from_tsv(\n                input_stream,\n                index_depth=1,\n                columns_depth=0)\n\n        self.assertEqual(f2.to_pairs(0),\n                ((0, ((196412, 0.0), (196501, 0.0), (196502, 0.0), (196503, 0.0), (196504, 0.0), (196505, 0.0))), (1, ((196412, 0.1), (196501, 0.1), (196502, 0.1), (196503, 0.1), (196504, 0.1), (196505, 0.1)))))\n\n\n\n    def test_frame_from_tsv_d(self) -> None:\n\n        f1 = sf.Frame.from_elements([1], columns=[\'a\'])\n\n        with temp_file(\'.txt\', path=True) as fp:\n            f1.to_tsv(fp)\n            f2 = sf.Frame.from_tsv(fp, index_depth=1)\n            self.assertEqual(f2.to_pairs(0),\n                    ((\'a\', ((0, 1),)),))\n\n\n    def test_frame_from_tsv_e(self) -> None:\n\n        f1 = sf.Frame.from_elements([1], columns=[\'with space\'])\n\n        with temp_file(\'.txt\', path=True) as fp:\n            f1.to_tsv(fp)\n            f2 = sf.Frame.from_tsv(fp, index_depth=1)\n            self.assertEqual(\n                    f2.columns.values.tolist(),\n                    [\'with space\']\n                    )\n\n    def test_frame_from_tsv_f(self) -> None:\n\n        f1 = sf.Frame.from_elements([1], columns=[\':with:colon:\'])\n\n        with temp_file(\'.txt\', path=True) as fp:\n            f1.to_tsv(fp)\n            f2 = sf.Frame.from_tsv(fp, index_depth=1)\n            self.assertEqual(f2.to_pairs(0),\n                    ((\':with:colon:\', ((0, 1),)),)\n                    )\n\n\n    def test_frame_from_tsv_g(self) -> None:\n\n        f1 = sf.Frame.from_elements([\'#\', \'*\', \'@\'],\n                columns=[\'a\', \'#\', \'c\'],\n                index=(\'q\', \'r\', \'s\'))\n\n        with temp_file(\'.txt\', path=True) as fp:\n            f1.to_tsv(fp)\n            f2 = sf.Frame.from_tsv(fp, index_depth=1)\n            self.assertEqualFrames(f1, f2)\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_to_pairs_a(self) -> None:\n\n        records = (\n                (2, \'a\'),\n                (3, \'b\'),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'r\', \'s\'),\n                index=(\'w\', \'x\'))\n\n        with self.assertRaises(AxisInvalid):\n            x = f1.to_pairs(3)\n\n        post = f1.to_pairs(1)\n        self.assertEqual(post,\n                ((\'w\', ((\'r\', 2), (\'s\', \'a\'))), (\'x\', ((\'r\', 3), (\'s\', \'b\')))))\n\n\n\n    #---------------------------------------------------------------------------\n    @skip_win # type: ignore\n    def test_frame_to_delimited_a(self) -> None:\n\n        records = (\n                (2, None),\n                (3, np.nan),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'r\', \'s\'),\n                index=(\'w\', \'x\'))\n\n        with temp_file(\'.txt\', path=True) as fp:\n            f1.to_delimited(fp, delimiter=\'|\', store_filter=None)\n            f = open(fp)\n            lines = f.readlines()\n            self.assertEqual(lines,\n                    [\'__index0__|r|s\\n\', \'w|2|None\\n\', \'x|3|nan\']\n                    )\n\n    @skip_win # type: ignore\n    def test_frame_to_delimited_b(self) -> None:\n\n        records = (\n                (2, None),\n                (3, np.nan),\n                (0, False),\n                (3, \'x\')\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'r\', \'s\'),\n                index=IndexHierarchy.from_product((1, 2), (\'a\', \'b\')))\n\n        with temp_file(\'.txt\', path=True) as fp:\n            f1.to_delimited(fp, delimiter=\'|\', store_filter=None)\n            f = open(fp)\n            lines = f.readlines()\n            self.assertEqual(lines, [\n                    \'__index0__|__index1__|r|s\\n\',\n                    \'1|a|2|None\\n\',\n                    \'1|b|3|nan\\n\',\n                    \'2|a|0|False\\n\',\n                    \'2|b|3|x\'\n                    ])\n\n    #---------------------------------------------------------------------------\n    def test_frame_to_csv_a(self) -> None:\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        file = StringIO()\n        f1.to_csv(file)\n        file.seek(0)\n        self.assertEqual(file.read(),\n\'__index0__,p,q,r,s,t\\nw,2,2,a,False,False\\nx,30,34,b,True,False\\ny,2,95,c,False,False\\nz,30,73,d,True,True\')\n\n        file = StringIO()\n        f1.to_csv(file, include_index=False)\n        file.seek(0)\n        self.assertEqual(file.read(),\n\'p,q,r,s,t\\n2,2,a,False,False\\n30,34,b,True,False\\n2,95,c,False,False\\n30,73,d,True,True\')\n\n        file = StringIO()\n        f1.to_csv(file, include_index=False, include_columns=False)\n        file.seek(0)\n        self.assertEqual(file.read(),\n\'2,2,a,False,False\\n30,34,b,True,False\\n2,95,c,False,False\\n30,73,d,True,True\')\n\n\n    def test_frame_to_csv_b(self) -> None:\n\n        f = sf.Frame.from_elements([1, 2, 3],\n                columns=[\'a\'],\n                index=sf.Index(range(3), name=\'Important Name\'))\n        file = StringIO()\n        f.to_csv(file)\n        file.seek(0)\n        self.assertEqual(file.read(), \'Important Name,a\\n0,1\\n1,2\\n2,3\')\n\n\n    def test_frame_to_csv_c(self) -> None:\n        records = (\n                (2, np.nan, \'a\', False, None),\n                (30, np.nan, \'b\', True, None),\n                (2, np.inf, \'c\', False, None),\n                (30, -np.inf, \'d\', True, None),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        with temp_file(\'.csv\') as fp:\n            f1.to_csv(fp)\n\n            with open(fp) as f:\n                lines = f.readlines()\n                # nan has been converted to string\n                self.assertEqual(lines[1], \'w,2,,a,False,None\\n\')\n                self.assertEqual(lines[4], \'z,30,-inf,d,True,None\')\n\n\n    def test_frame_to_csv_d(self) -> None:\n        f1 = Frame.from_records(\n                ((10, 20, 50, 60), (50.0, 60.4, -50, -60)),\n                index=(\'p\', \'q\'),\n                columns=IndexHierarchy.from_product((\'I\', \'II\'), (\'a\', \'b\')),\n                name=\'f3\')\n\n        with temp_file(\'.csv\') as fp:\n            f1.to_csv(fp)\n\n            with open(fp) as f:\n                lines = f.readlines()\n\n            self.assertEqual(lines,\n                    [\'__index0__,I,I,II,II\\n\', \',a,b,a,b\\n\', \'p,10.0,20.0,50,60\\n\', \'q,50.0,60.4,-50,-60\']\n                    )\n\n            f2 = Frame.from_csv(fp, columns_depth=2, index_depth=1)\n            self.assertEqual(f2.to_pairs(0),\n                    (((\'I\', \'a\'), ((\'p\', 10.0), (\'q\', 50.0))), ((\'I\', \'b\'), ((\'p\', 20.0), (\'q\', 60.4))), ((\'II\', \'a\'), ((\'p\', 50), (\'q\', -50))), ((\'II\', \'b\'), ((\'p\', 60), (\'q\', -60))))\n                    )\n\n\n    def test_frame_to_csv_e(self) -> None:\n        f1 = Frame.from_records(\n                ((10, 20, 50, 60), (50.0, 60.4, -50, -60)),\n                index=(\'p\', \'q\'),\n                columns=IndexHierarchy.from_product((10, 20),(\'I\', \'II\'),),\n                name=\'f3\')\n\n        with temp_file(\'.csv\') as fp:\n            f1.to_csv(fp)\n\n            with open(fp) as f:\n                lines = f.readlines()\n\n            self.assertEqual(lines,\n                    [\'__index0__,10,10,20,20\\n\', \',I,II,I,II\\n\', \'p,10.0,20.0,50,60\\n\', \'q,50.0,60.4,-50,-60\']\n                    )\n            f2 = Frame.from_csv(fp, columns_depth=2, index_depth=1)\n            self.assertEqual(\n                    f2.to_pairs(0),\n                    (((10, \'I\'), ((\'p\', 10.0), (\'q\', 50.0))), ((10, \'II\'), ((\'p\', 20.0), (\'q\', 60.4))), ((20, \'I\'), ((\'p\', 50), (\'q\', -50))), ((20, \'II\'), ((\'p\', 60), (\'q\', -60))))\n                    )\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_to_tsv_a(self) -> None:\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        file = StringIO()\n        f1.to_tsv(file)\n        file.seek(0)\n        self.assertEqual(file.read(),\n\'__index0__\\tp\\tq\\tr\\ts\\tt\\nw\\t2\\t2\\ta\\tFalse\\tFalse\\nx\\t30\\t34\\tb\\tTrue\\tFalse\\ny\\t2\\t95\\tc\\tFalse\\tFalse\\nz\\t30\\t73\\td\\tTrue\\tTrue\')\n\n\n    def test_frame_to_tsv_b(self) -> None:\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=IndexHierarchy.from_product((\'A\', \'B\'), (1, 2))\n                )\n\n        with temp_file(\'.txt\', path=True) as fp:\n            f1.to_tsv(fp, include_index=True)\n            f2 = Frame.from_tsv(fp, index_depth=2)\n            self.assertEqualFrames(f1, f2)\n\n    def test_frame_to_tsv_c(self) -> None:\n        f1 = sf.Frame(\n                np.arange(16).reshape((4,4)),\n                index=sf.IndexHierarchy.from_product((\'I\', \'II\'), (\'a\', \'b\')),\n                columns=sf.IndexHierarchy.from_product((\'III\', \'IV\'), (10, 20))\n                )\n\n        with temp_file(\'.txt\', path=True) as fp:\n            f1.to_tsv(fp, include_index=True)\n            f2 = Frame.from_tsv(fp, index_depth=2, columns_depth=2)\n            self.assertEqualFrames(f1, f2)\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_to_html_a(self) -> None:\n        records = (\n                (2, \'a\', False),\n                (3, \'b\', False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\'))\n        post = f1.to_html()\n\n        self.assertEqual(post, \'<table border=""1""><thead><tr><th></th><th>r</th><th>s</th><th>t</th></tr></thead><tbody><tr><th>w</th><td>2</td><td>a</td><td>False</td></tr><tr><th>x</th><td>3</td><td>b</td><td>False</td></tr></tbody></table>\'\n        )\n\n        msg = str(f1.display(sf.DisplayConfig(type_show=False, include_columns=False)))\n        self.assertEqual(msg, \'w 2 a False\\nx 3 b False\')\n\n\n    def test_frame_to_html_datatables_a(self) -> None:\n        records = (\n                (2, \'a\', False),\n                (3, \'b\', False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\'))\n\n        sio = StringIO()\n\n        post = f1.to_html_datatables(sio, show=False)\n\n        self.assertEqual(post, None)\n        self.assertTrue(len(sio.read()) > 1300)\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_to_rst_a(self) -> None:\n        records = (\n                (2, \'a\', False),\n                (3, \'b\', False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\'))\n        post = f1.to_rst()\n        msg = \'\'\'\n                +--+--+--+-----+\n                |  |r |s |t    |\n                +==+==+==+=====+\n                |w |2 |a |False|\n                +--+--+--+-----+\n                |x |3 |b |False|\n                +--+--+--+-----+\n                \'\'\'\n        self.assertEqualLines(post, msg)\n\n\n    def test_frame_to_markdown_a(self) -> None:\n        records = (\n                (2, \'a\', False),\n                (3, \'b\', False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\'))\n        post = f1.to_markdown()\n\n        msg = \'\'\'\n                |  |r |s |t    |\n                |--|--|--|-----|\n                |w |2 |a |False|\n                |x |3 |b |False|\n                \'\'\'\n        self.assertEqualLines(post, msg)\n\n\n    def test_frame_to_latex_a(self) -> None:\n        records = (\n                (2, \'a\', False),\n                (3, \'b\', False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\'))\n        post = f1.to_latex()\n        msg = r\'\'\'\n                \\begin{table}[ht]\n                \\centering\n                \\begin{tabular}{c c c c}\n                \\hline\\hline\n                   & r  & s  & t     \\\\\n                \\hline\n                w  & 2  & a  & False \\\\\n                x  & 3  & b  & False \\\\\n                \\hline\\end{tabular}\n                \\end{table}\n                \'\'\'\n        self.assertEqualLines(post, msg)\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_to_xlsx_a(self) -> None:\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        config = StoreConfig(index_depth=1)\n\n        with temp_file(\'.xlsx\') as fp:\n            f1.to_xlsx(fp)\n            st = StoreXLSX(fp)\n            f2 = st.read(label=None, config=config)\n            self.assertEqualFrames(f1, f2)\n\n\n\n    def test_frame_from_xlsx_a(self) -> None:\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        with temp_file(\'.xlsx\') as fp:\n            f1.to_xlsx(fp)\n            f2 = Frame.from_xlsx(fp, index_depth=f1.index.depth)\n            self.assertEqualFrames(f1, f2)\n\n            f3 = FrameGO.from_xlsx(fp, index_depth=f1.index.depth)\n            self.assertEqual(f3.__class__, FrameGO)\n            self.assertEqual(f3.shape, (4, 5))\n\n\n    def test_frame_from_xlsx_b(self) -> None:\n\n        f1 = Frame.from_records((\n                (10, 20, 50, False, 10, 20, 50, False),\n                (50.0, 60.4, -50, True, 50.0, 60.4, -50, True),\n                (234, 44452, 0, False, 234, 44452, 0, False),\n                (4, -4, 2000, True, 4, -4, 2000, True),\n                (10, 20, 50, False, 10, 20, 50, False),\n                (50.0, 60.4, -50, True, 50.0, 60.4, -50, True),\n                (234, 44452, 0, False, 234, 44452, 0, False),\n                (4, -4, 2000, True, 4, -4, 2000, True),\n                ),\n                index=IndexHierarchy.from_product((\'top\', \'bottom\'), (\'far\', \'near\'), (\'left\', \'right\')),\n                columns=IndexHierarchy.from_product((\'I\', \'II\'), (\'a\', \'b\'), (1, 2))\n                )\n\n        with temp_file(\'.xlsx\') as fp:\n            f1.to_xlsx(fp)\n            f2 = Frame.from_xlsx(fp,\n                    index_depth=f1.index.depth,\n                    columns_depth=f1.columns.depth)\n            self.assertEqualFrames(f1, f2, compare_dtype=False)\n\n    @unittest.skip(\'need to progrmatically generate bad_sheet.xlsx\')\n    def test_frame_from_xlsx_c(self) -> None:\n        # https://github.com/InvestmentSystems/static-frame/issues/146\n        fp = \'/tmp/bad_sheet.xlsx\'\n        f2 = Frame.from_xlsx(fp)\n        self.assertEqual(f2.shape, (5, 6))\n\n    def test_frame_from_xlsx_d(self) -> None:\n        # isolate case of all None data that has a valid index\n\n        f1 = Frame.from_element(None, index=(\'a\', \'b\', \'c\'), columns=(\'x\', \'y\', \'z\'))\n\n        with temp_file(\'.xlsx\') as fp:\n            f1.to_xlsx(fp)\n            f2 = Frame.from_xlsx(fp,\n                    index_depth=f1.index.depth,\n                    columns_depth=f1.columns.depth)\n            self.assertEqualFrames(f1, f2)\n\n    def test_frame_from_xlsx_e(self) -> None:\n        # isolate case of all None data that has a valid IndexHierarchy\n\n        f1 = Frame.from_element(None,\n                index=IndexHierarchy.from_product((0, 1), (\'a\', \'b\')),\n                columns=(\'x\', \'y\', \'z\')\n                )\n\n        with temp_file(\'.xlsx\') as fp:\n            f1.to_xlsx(fp)\n            f2 = Frame.from_xlsx(fp,\n                    index_depth=f1.index.depth,\n                    columns_depth=f1.columns.depth)\n            self.assertEqualFrames(f1, f2)\n\n    def test_frame_from_xlsx_f(self) -> None:\n        # isolate case of all None data and only columns\n        f1 = Frame.from_element(None, index=(\'a\', \'b\', \'c\'), columns=(\'x\', \'y\', \'z\'))\n\n        with temp_file(\'.xlsx\') as fp:\n            f1.to_xlsx(fp, include_index=False)\n            f2 = Frame.from_xlsx(fp,\n                    index_depth=0,\n                    columns_depth=f1.columns.depth)\n            # with out the index, we only have columns, and drop all-empty rows\n            self.assertEqual(f2.shape, (0, 3))\n\n    def test_frame_from_xlsx_g(self) -> None:\n        # isolate case of all None data, no index, no columns\n        f1 = Frame.from_element(None, index=(\'a\', \'b\', \'c\'), columns=(\'x\', \'y\', \'z\'))\n\n        with temp_file(\'.xlsx\') as fp:\n            f1.to_xlsx(fp, include_index=False, include_columns=False)\n            with self.assertRaises(ErrorInitFrame):\n                f2 = Frame.from_xlsx(fp,\n                        index_depth=0,\n                        columns_depth=0)\n\n    #---------------------------------------------------------------------------\n    def test_frame_from_sqlite_a(self) -> None:\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        with temp_file(\'.sqlite\') as fp:\n            f1.to_sqlite(fp)\n            f2 = Frame.from_sqlite(fp, index_depth=f1.index.depth)\n            self.assertEqualFrames(f1, f2)\n\n            f3 = FrameGO.from_sqlite(fp, index_depth=f1.index.depth)\n            self.assertEqual(f3.__class__, FrameGO)\n            self.assertEqual(f3.shape, (4, 5))\n\n\n    def test_frame_from_sqlite_b(self) -> None:\n\n        f1 = Frame.from_records((\n                (10, 20, 50, False, 10, 20, 50, False),\n                (50.0, 60.4, -50, True, 50.0, 60.4, -50, True),\n                (234, 44452, 0, False, 234, 44452, 0, False),\n                (4, -4, 2000, True, 4, -4, 2000, True),\n                (10, 20, 50, False, 10, 20, 50, False),\n                (50.0, 60.4, -50, True, 50.0, 60.4, -50, True),\n                (234, 44452, 0, False, 234, 44452, 0, False),\n                (4, -4, 2000, True, 4, -4, 2000, True),\n                ),\n                index=IndexHierarchy.from_product((\'top\', \'bottom\'), (\'far\', \'near\'), (\'left\', \'right\')),\n                columns=IndexHierarchy.from_product((\'I\', \'II\'), (\'a\', \'b\'), (1, 2))\n                )\n\n        with temp_file(\'.sqlite\') as fp:\n            f1.to_sqlite(fp)\n            f2 = Frame.from_sqlite(fp,\n                    index_depth=f1.index.depth,\n                    columns_depth=f1.columns.depth)\n            self.assertEqualFrames(f1, f2)\n\n\n    def test_frame_from_hdf5_a(self) -> None:\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'),\n                name=\'f1\'\n                )\n\n        with temp_file(\'.h5\') as fp:\n            f1.to_hdf5(fp)\n            f2 = Frame.from_hdf5(fp, label=f1.name, index_depth=f1.index.depth)\n            self.assertEqualFrames(f1, f2)\n\n            f3 = FrameGO.from_hdf5(fp, label=f1.name, index_depth=f1.index.depth)\n            self.assertEqual(f3.__class__, FrameGO)\n            self.assertEqual(f3.shape, (4, 5))\n\n\n    def test_frame_from_hdf5_b(self) -> None:\n        records = (\n                (2, False),\n                (30, False),\n                (2, False),\n                (30, True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\'),\n                index=(\'w\', \'x\', \'y\', \'z\'),\n                )\n\n        with temp_file(\'.h5\') as fp:\n            # no .name, and no label provided\n            with self.assertRaises(RuntimeError):\n                f1.to_hdf5(fp)\n\n            f1.to_hdf5(fp, label=\'foo\')\n            f2 = Frame.from_hdf5(fp, label=\'foo\', index_depth=f1.index.depth)\n            f1 = f1.rename(\'foo\') # will come back with label as name\n            self.assertEqualFrames(f1, f2)\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_and_a(self) -> None:\n\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual(f1.all(axis=0).to_pairs(),\n                ((\'p\', True), (\'q\', True), (\'r\', True), (\'s\', False), (\'t\', False)))\n\n        self.assertEqual(f1.any(axis=0).to_pairs(),\n                ((\'p\', True), (\'q\', True), (\'r\', True), (\'s\', True), (\'t\', True)))\n\n        self.assertEqual(f1.all(axis=1).to_pairs(),\n                ((\'w\', False), (\'x\', False), (\'y\', False), (\'z\', True)))\n\n        self.assertEqual(f1.any(axis=1).to_pairs(),\n                ((\'w\', True), (\'x\', True), (\'y\', True), (\'z\', True)))\n\n\n\n    def test_frame_unique_a(self) -> None:\n\n        records = (\n                (2, 2, 3.5),\n                (30, 34, 60.2),\n                (2, 95, 1.2),\n                (30, 73, 50.2),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual(f1.unique().tolist(),\n                [1.2, 2.0, 3.5, 30.0, 34.0, 50.2, 60.2, 73.0, 95.0])\n\n        records = (\n                (2, 2, 2),\n                (30, 34, 34),\n                (2, 2, 2),\n                (30, 73, 73),\n                )\n        f2 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual(f2.unique().tolist(), [2, 30, 34, 73])\n\n        self.assertEqual(f2.unique(axis=0).tolist(),\n                [[2, 2, 2], [30, 34, 34], [30, 73, 73]])\n        self.assertEqual(f2.unique(axis=1).tolist(),\n                [[2, 2], [30, 34], [2, 2], [30, 73]])\n\n    def test_frame_unique_b(self) -> None:\n\n        records = (\n                (None, 2, None),\n                (\'30\', 34, \'30\'),\n                (None, 2, None),\n                (\'30\', 34, \'30\'),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual(len(f1.unique()), 4)\n\n        self.assertEqual(len(f1.unique(axis=0)), 2)\n\n        self.assertEqual(len(f1.unique(axis=1)), 2)\n\n\n    def test_frame_duplicated_a(self) -> None:\n\n        a1 = np.array([[50, 50, 32, 17, 17], [2,2,1,3,3]])\n        f1 = Frame(a1, index=(\'a\', \'b\'), columns=(\'p\', \'q\', \'r\', \'s\',\'t\'))\n\n        self.assertEqual(f1.duplicated(axis=1).to_pairs(),\n                ((\'p\', True), (\'q\', True), (\'r\', False), (\'s\', True), (\'t\', True)))\n\n        self.assertEqual(f1.duplicated(axis=0).to_pairs(),\n                ((\'a\', False), (\'b\', False)))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_drop_duplicated_a(self) -> None:\n\n        a1 = np.array([[50, 50, 32, 17, 17], [2,2,1,3,3]])\n        f1 = Frame(a1, index=(\'a\', \'b\'), columns=(\'p\', \'q\', \'r\', \'s\',\'t\'))\n\n        self.assertEqual(f1.drop_duplicated(axis=1, exclude_first=True).to_pairs(1),\n                ((\'a\', ((\'p\', 50), (\'r\', 32), (\'s\', 17))), (\'b\', ((\'p\', 2), (\'r\', 1), (\'s\', 3)))))\n\n\n    def test_frame_drop_duplicated_b(self) -> None:\n\n        a1 = np.arange(6).reshape((2, 3))\n        f1 = Frame(a1, index=(\'a\', \'b\'), columns=(\'p\', \'q\', \'r\'))\n        f2 = f1.drop_duplicated(axis=0)\n        self.assertEqualFrames(f1, f2)\n\n        with self.assertRaises(NotImplementedError):\n            _ = f1.drop_duplicated(axis=-1)\n\n\n    def test_frame_drop_duplicated_c(self) -> None:\n        f1 = Frame.from_records(\n                [[1, 2], [1, 2], [3, 3]],\n                index=(\'a\', \'b\', \'c\'),\n                columns=(\'p\', \'q\'))\n        f2 = f1.drop_duplicated(axis=0)\n        self.assertEqual(f2.to_pairs(0),\n                ((\'p\', ((\'c\', 3),)), (\'q\', ((\'c\', 3),)))\n                )\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_from_concat_a(self) -> None:\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\', \'a\'))\n\n        records = (\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f2 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\', \'a\'))\n\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f3 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\', \'a\'))\n\n        f = Frame.from_concat((f1, f2, f3), axis=1, columns=range(15))\n\n        # no blocks are copied or reallcoated\n        self.assertEqual(f.mloc.tolist(),\n                f1.mloc.tolist() + f2.mloc.tolist() + f3.mloc.tolist()\n                )\n        # order of index is retained\n        self.assertEqual(f.to_pairs(1),\n                ((\'x\', ((0, 2), (1, 2), (2, \'a\'), (3, False), (4, False), (5, 2), (6, 95), (7, \'c\'), (8, False), (9, False), (10, 2), (11, 2), (12, \'a\'), (13, False), (14, False))), (\'a\', ((0, 30), (1, 34), (2, \'b\'), (3, True), (4, False), (5, 30), (6, 73), (7, \'d\'), (8, True), (9, True), (10, 30), (11, 73), (12, \'d\'), (13, True), (14, True)))))\n\n\n    def test_frame_from_concat_b(self) -> None:\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\', \'a\'))\n\n        records = (\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f2 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\', \'b\'))\n\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n\n        f3 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\', \'c\'))\n\n        f = Frame.from_concat((f1, f2, f3), axis=1, columns=range(15))\n\n        self.assertEqual(f.index.values.tolist(),\n                [\'a\', \'b\', \'c\', \'x\'])\n\n        self.assertAlmostEqualFramePairs(f.to_pairs(1),\n                ((\'a\', ((0, 30), (1, 34), (2, \'b\'), (3, True), (4, False), (5, nan), (6, nan), (7, nan), (8, nan), (9, nan), (10, nan), (11, nan), (12, nan), (13, nan), (14, nan))), (\'b\', ((0, nan), (1, nan), (2, nan), (3, nan), (4, nan), (5, 30), (6, 73), (7, \'d\'), (8, True), (9, True), (10, nan), (11, nan), (12, nan), (13, nan), (14, nan))), (\'c\', ((0, nan), (1, nan), (2, nan), (3, nan), (4, nan), (5, nan), (6, nan), (7, nan), (8, nan), (9, nan), (10, 30), (11, 73), (12, \'d\'), (13, True), (14, True))), (\'x\', ((0, 2), (1, 2), (2, \'a\'), (3, False), (4, False), (5, 2), (6, 95), (7, \'c\'), (8, False), (9, False), (10, 2), (11, 2), (12, \'a\'), (13, False), (14, False))))\n                )\n\n\n        f = Frame.from_concat((f1, f2, f3), union=False, axis=1, columns=range(15))\n\n        self.assertEqual(f.index.values.tolist(),\n                [\'x\'])\n        self.assertEqual(f.to_pairs(0),\n                ((0, ((\'x\', 2),)), (1, ((\'x\', 2),)), (2, ((\'x\', \'a\'),)), (3, ((\'x\', False),)), (4, ((\'x\', False),)), (5, ((\'x\', 2),)), (6, ((\'x\', 95),)), (7, ((\'x\', \'c\'),)), (8, ((\'x\', False),)), (9, ((\'x\', False),)), (10, ((\'x\', 2),)), (11, ((\'x\', 2),)), (12, ((\'x\', \'a\'),)), (13, ((\'x\', False),)), (14, ((\'x\', False),))))\n\n\n    def test_frame_from_concat_c(self) -> None:\n        records1 = (\n                (2, 2, False),\n                (30, 34, False),\n                )\n\n        f1 = Frame.from_records(records1,\n                columns=(\'p\', \'q\', \'t\'),\n                index=(\'x\', \'a\'))\n\n        records2 = (\n                (\'c\', False),\n                (\'d\', True),\n                )\n        f2 = Frame.from_records(records2,\n                columns=(\'r\', \'s\',),\n                index=(\'x\', \'a\'))\n\n        # get combined columns as they are unique\n        f = Frame.from_concat((f1, f2), axis=1)\n        self.assertEqual(f.to_pairs(0),\n                ((\'p\', ((\'x\', 2), (\'a\', 30))), (\'q\', ((\'x\', 2), (\'a\', 34))), (\'t\', ((\'x\', False), (\'a\', False))), (\'r\', ((\'x\', \'c\'), (\'a\', \'d\'))), (\'s\', ((\'x\', False), (\'a\', True))))\n                )\n\n\n    @skip_win  #type: ignore\n    def test_frame_from_concat_d(self) -> None:\n        records = (\n                (2, 2, False),\n                (30, 34, False),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'a\', \'b\'))\n\n        records = (\n                (2, 2, False),\n                (30, 34, False),\n                )\n\n        f2 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'c\', \'d\'))\n\n        f = Frame.from_concat((f1, f2), axis=0)\n\n        # block copmatible will result in attempt to keep vertical types\n        self.assertEqual(\n                [str(x) for x in f.dtypes.values.tolist()],\n                [\'int64\', \'int64\', \'bool\'])\n\n        self.assertEqual(f.to_pairs(0),\n                ((\'p\', ((\'a\', 2), (\'b\', 30), (\'c\', 2), (\'d\', 30))), (\'q\', ((\'a\', 2), (\'b\', 34), (\'c\', 2), (\'d\', 34))), (\'r\', ((\'a\', False), (\'b\', False), (\'c\', False), (\'d\', False)))))\n\n\n    @skip_win  # type: ignore\n    def test_frame_from_concat_e(self) -> None:\n\n        f1 = Frame.from_items(zip(\n                (\'a\', \'b\', \'c\'),\n                ((1, 2), (1, 2), (False, True))\n                ))\n\n        f = Frame.from_concat((f1, f1, f1), index=range(6))\n        self.assertEqual(\n                f.to_pairs(0),\n                ((\'a\', ((0, 1), (1, 2), (2, 1), (3, 2), (4, 1), (5, 2))), (\'b\', ((0, 1), (1, 2), (2, 1), (3, 2), (4, 1), (5, 2))), (\'c\', ((0, False), (1, True), (2, False), (3, True), (4, False), (5, True)))))\n        self.assertEqual(\n                [str(x) for x in f.dtypes.values.tolist()],\n                [\'int64\', \'int64\', \'bool\'])\n\n        f = Frame.from_concat((f1, f1, f1), axis=1, columns=range(9))\n\n        self.assertEqual(f.to_pairs(0),\n                ((0, ((0, 1), (1, 2))), (1, ((0, 1), (1, 2))), (2, ((0, False), (1, True))), (3, ((0, 1), (1, 2))), (4, ((0, 1), (1, 2))), (5, ((0, False), (1, True))), (6, ((0, 1), (1, 2))), (7, ((0, 1), (1, 2))), (8, ((0, False), (1, True)))))\n\n        self.assertEqual([str(x) for x in f.dtypes.values.tolist()],\n                [\'int64\', \'int64\', \'bool\', \'int64\', \'int64\', \'bool\', \'int64\', \'int64\', \'bool\'])\n\n    def test_frame_from_concat_f(self) -> None:\n        # force a reblock before concatenating\n\n        a1 = np.array([1, 2, 3], dtype=np.int64)\n        a2 = np.array([10,50,30], dtype=np.int64)\n        a3 = np.array([1345,2234,3345], dtype=np.int64)\n        a4 = np.array([False, True, False])\n        a5 = np.array([False, False, False])\n        a6 = np.array([\'g\', \'d\', \'e\'])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3, a4, a5, a6))\n\n        f1 = Frame(TypeBlocks.from_blocks((a1, a2, a3, a4, a5, a6)),\n                columns = (\'a\', \'b\', \'c\', \'d\', \'e\', \'f\'),\n                own_data=True)\n        self.assertEqual(len(f1._blocks._blocks), 6)\n\n        f2 = Frame(f1.iloc[1:]._blocks.consolidate(),\n                columns = (\'a\', \'b\', \'c\', \'d\', \'e\', \'f\'),\n                own_data=True)\n        self.assertEqual(len(f2._blocks._blocks), 3)\n\n        f = Frame.from_concat((f1 ,f2), index=range(5))\n\n        self.assertEqual(\n                [str(x) for x in f.dtypes.values.tolist()],\n                [\'int64\', \'int64\', \'int64\', \'bool\', \'bool\', \'<U1\'])\n\n        self.assertEqual(\n                [str(x.dtype) for x in f._blocks._blocks],\n                [\'int64\', \'bool\', \'<U1\'])\n\n\n    def test_frame_from_concat_g(self) -> None:\n        records1 = (\n                (2, 2, False),\n                (30, 34, False),\n                )\n\n        f1 = Frame.from_records(records1,\n                columns=(\'p\', \'q\', \'t\'),\n                index=(\'x\', \'a\'))\n\n        records2 = (\n                (\'c\', False),\n                (\'d\', True),\n                )\n        f2 = Frame.from_records(records2,\n                columns=(\'r\', \'s\',),\n                index=(\'x\', \'a\'))\n\n        # get combined columns as they are unique\n        f = Frame.from_concat((f1, f2), axis=1)\n        self.assertEqual(f.to_pairs(0),\n                ((\'p\', ((\'x\', 2), (\'a\', 30))), (\'q\', ((\'x\', 2), (\'a\', 34))), (\'t\', ((\'x\', False), (\'a\', False))), (\'r\', ((\'x\', \'c\'), (\'a\', \'d\'))), (\'s\', ((\'x\', False), (\'a\', True))))\n                )\n\n\n    def test_frame_from_concat_h(self) -> None:\n\n        index = list(\'\'.join(x) for x in it.combinations(string.ascii_lowercase, 3))\n        columns = list(\'\'.join(x) for x in it.combinations(string.ascii_uppercase, 2))\n        data = np.random.rand(len(index), len(columns))\n        f1 = Frame(data, index=index, columns=columns)\n\n        f2 = f1[[c for c in f1.columns if tp.cast(str, c).startswith(\'D\')]]\n        f3 = f1[[c for c in f1.columns if tp.cast(str, c).startswith(\'G\')]]\n        post = sf.Frame.from_concat((f2, f3), axis=1)\n\n        # this form of concatenation has no copy\n        assert post.mloc.tolist() == [f2.mloc[0], f3.mloc[0]]\n        self.assertEqual(post.shape, (2600, 41))\n\n\n    def test_frame_from_concat_i(self) -> None:\n\n        sf1 = sf.Frame.from_dict(dict(a=[1,2,3],b=[1,2,3]),index=[100,200,300]).relabel_add_level(columns=\'A\')\n        sf2 = sf.Frame.from_dict(dict(a=[1,2,3],b=[1,2,3]),index=[100,200,300]).relabel_add_level(columns=\'B\')\n\n        f = sf.Frame.from_concat((sf1, sf2), axis=1)\n        self.assertEqual(f.to_pairs(0),\n                (((\'A\', \'a\'), ((100, 1), (200, 2), (300, 3))), ((\'A\', \'b\'), ((100, 1), (200, 2), (300, 3))), ((\'B\', \'a\'), ((100, 1), (200, 2), (300, 3))), ((\'B\', \'b\'), ((100, 1), (200, 2), (300, 3)))))\n\n\n    def test_frame_from_concat_j(self) -> None:\n\n        sf1 = sf.Frame.from_dict(dict(a=[1,2,3],b=[1,2,3]),index=[100,200,300]).relabel_add_level(index=\'A\')\n        sf2 = sf.Frame.from_dict(dict(a=[1,2,3],b=[1,2,3]),index=[100,200,300]).relabel_add_level(index=\'B\')\n\n        f = sf.Frame.from_concat((sf1, sf2), axis=0)\n\n        self.assertEqual(f.to_pairs(0),\n                ((\'a\', (((\'A\', 100), 1), ((\'A\', 200), 2), ((\'A\', 300), 3), ((\'B\', 100), 1), ((\'B\', 200), 2), ((\'B\', 300), 3))), (\'b\', (((\'A\', 100), 1), ((\'A\', 200), 2), ((\'A\', 300), 3), ((\'B\', 100), 1), ((\'B\', 200), 2), ((\'B\', 300), 3))))\n                )\n\n\n    def test_frame_from_concat_k(self) -> None:\n        records1 = (\n                (2, 2, False),\n                (30, 34, False),\n                )\n        f1 = Frame.from_records(records1,\n                columns=(\'p\', \'q\', \'t\'),\n                index=(\'x\', \'a\'))\n\n        records2 = (\n                (\'c\', False),\n                (\'d\', True),\n                )\n        f2 = Frame.from_records(records2,\n                columns=(\'r\', \'s\',),\n                index=(\'x\', \'a\'))\n\n        # get combined columns as they are unique\n        f = Frame.from_concat((f1, f2), axis=1, name=\'foo\')\n        self.assertEqual(f.name, \'foo\')\n\n\n    def test_frame_from_concat_m(self) -> None:\n        records1 = (\n                (2, 2, False),\n                (30, 34, False),\n                )\n        f1 = Frame.from_records(records1,\n                columns=(\'p\', \'q\', \'t\'),\n                index=(\'x\', \'a\'))\n\n        records2 = (\n                (\'c\', False),\n                (\'d\', True),\n                )\n        f2 = Frame.from_records(records2,\n                columns=(3, 4,),\n                index=(\'x\', \'a\'))\n\n        f = Frame.from_concat((f1, f2), axis=1, name=\'foo\')\n\n        self.assertEqual(f.columns.values.tolist(),\n                [\'p\', \'q\', \'t\', 3, 4])\n        self.assertEqual(f.to_pairs(0),\n                ((\'p\', ((\'x\', 2), (\'a\', 30))), (\'q\', ((\'x\', 2), (\'a\', 34))), (\'t\', ((\'x\', False), (\'a\', False))), (3, ((\'x\', \'c\'), (\'a\', \'d\'))), (4, ((\'x\', False), (\'a\', True))))\n                )\n\n    def test_frame_from_concat_n(self) -> None:\n        records1 = (\n                (2, False),\n                (30, False),\n                )\n        f1 = Frame.from_records(records1,\n                columns=(\'p\', \'q\'),\n                index=(\'x\', \'a\'))\n\n        records2 = (\n                (\'c\', False),\n                (\'d\', True),\n                )\n        f2 = Frame.from_records(records2,\n                columns=(\'p\', \'q\'),\n                index=(3, 10))\n\n        f = Frame.from_concat((f1, f2), axis=0, name=\'foo\')\n\n        self.assertEqual(f.index.values.tolist(),\n                [\'x\', \'a\', 3, 10])\n        self.assertEqual(f.to_pairs(0),\n                ((\'p\', ((\'x\', 2), (\'a\', 30), (3, \'c\'), (10, \'d\'))), (\'q\', ((\'x\', False), (\'a\', False), (3, False), (10, True))))\n                )\n\n\n    def test_frame_from_concat_o(self) -> None:\n        records1 = (\n                (2, False),\n                (34, False),\n                )\n\n        f1 = Frame.from_records(records1,\n                columns=(\'p\', \'q\',),\n                index=(\'x\', \'z\'))\n\n        records2 = (\n                (\'c\', False),\n                (\'d\', True),\n                )\n        f2 = Frame.from_records(records2,\n                columns=(\'r\', \'s\',),\n                index=(\'x\', \'z\'))\n\n\n        s1 = Series((0, 100), index=(\'x\', \'z\'), name=\'t\')\n\n        f = Frame.from_concat((f1, f2, s1), axis=1)\n\n        self.assertEqual(f.to_pairs(0),\n                ((\'p\', ((\'x\', 2), (\'z\', 34))), (\'q\', ((\'x\', False), (\'z\', False))), (\'r\', ((\'x\', \'c\'), (\'z\', \'d\'))), (\'s\', ((\'x\', False), (\'z\', True))), (\'t\', ((\'x\', 0), (\'z\', 100))))\n                )\n\n\n\n    def test_frame_from_concat_p(self) -> None:\n        records = (\n                (2, False),\n                (34, False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\',),\n                index=(\'a\', \'b\'))\n\n        s1 = Series((0, True), index=(\'p\', \'q\'), name=\'c\', dtype=object)\n        s2 = Series((-2, False), index=(\'p\', \'q\'), name=\'d\', dtype=object)\n\n        f = Frame.from_concat((s2, f1, s1), axis=0)\n\n        self.assertEqual(f.to_pairs(0),\n                ((\'p\', ((\'d\', -2), (\'a\', 2), (\'b\', 34), (\'c\', 0))), (\'q\', ((\'d\', False), (\'a\', False), (\'b\', False), (\'c\', True)))))\n\n\n\n    def test_frame_from_concat_q(self) -> None:\n        s1 = Series((2, 3, 0,), index=list(\'abc\'), name=\'x\').relabel_add_level(\'i\')\n        s2 = Series((\'10\', \'20\', \'100\'), index=list(\'abc\'), name=\'y\').relabel_add_level(\'i\')\n\n        # stack horizontally\n        f = Frame.from_concat((s1, s2), axis=1)\n\n        self.assertEqual(f.to_pairs(0),\n                ((\'x\', (((\'i\', \'a\'), 2), ((\'i\', \'b\'), 3), ((\'i\', \'c\'), 0))), (\'y\', (((\'i\', \'a\'), \'10\'), ((\'i\', \'b\'), \'20\'), ((\'i\', \'c\'), \'100\'))))\n            )\n\n        # stack vertically\n        f = Frame.from_concat((s1, s2), axis=0)\n        self.assertEqual(f.to_pairs(0),\n                (((\'i\', \'a\'), ((\'x\', 2), (\'y\', \'10\'))), ((\'i\', \'b\'), ((\'x\', 3), (\'y\', \'20\'))), ((\'i\', \'c\'), ((\'x\', 0), (\'y\', \'100\'))))\n            )\n\n\n    def test_frame_from_concat_r(self) -> None:\n        f1 = sf.Frame.from_dict_records(\n                [dict(a=1,b=1),dict(a=2,b=3),dict(a=1,b=1),dict(a=2,b=3)],\n                index=sf.IndexHierarchy.from_labels([(1,\'dd\'),(1,\'bb\'),(2,\'cc\'),(2,\'dd\')]))\n\n        f2 = sf.Frame.from_dict_records(\n                [dict(a=1,b=1),dict(a=2,b=3),dict(a=1,b=1),dict(a=2,b=3)],\n                index=sf.IndexHierarchy.from_labels([(3,\'ddd\'),(3,\'bbb\'),(4,\'ccc\'),(4,\'ddd\')])) * 100\n\n        self.assertEqual(Frame.from_concat((f1, f2), axis=0).to_pairs(0),\n                ((\'a\', (((1, \'dd\'), 1), ((1, \'bb\'), 2), ((2, \'cc\'), 1), ((2, \'dd\'), 2), ((3, \'ddd\'), 100), ((3, \'bbb\'), 200), ((4, \'ccc\'), 100), ((4, \'ddd\'), 200))), (\'b\', (((1, \'dd\'), 1), ((1, \'bb\'), 3), ((2, \'cc\'), 1), ((2, \'dd\'), 3), ((3, \'ddd\'), 100), ((3, \'bbb\'), 300), ((4, \'ccc\'), 100), ((4, \'ddd\'), 300))))\n                )\n\n    def test_frame_from_concat_s(self) -> None:\n        records1 = (\n                (2, False),\n                (34, False),\n                )\n\n        f1 = Frame.from_records(records1,\n                columns=(\'p\', \'q\',),\n                index=(\'x\', \'z\'))\n\n        records2 = (\n                (\'c\', False),\n                (\'d\', True),\n                )\n        f2 = Frame.from_records(records2,\n                columns=(\'r\', \'s\',),\n                index=(\'x\', \'z\'))\n\n        with self.assertRaises(NotImplementedError):\n            f = Frame.from_concat((f1, f2), axis=None)\n\n\n    def test_frame_from_concat_t(self) -> None:\n        frame1 = sf.Frame.from_dict_records(\n                [dict(a=1,b=1), dict(a=2,b=3), dict(a=1,b=1), dict(a=2,b=3)], index=sf.IndexHierarchy.from_labels([(1,\'dd\',0), (1,\'bb\',0), (2,\'cc\',0), (2,\'ee\',0)]))\n        frame2 = sf.Frame.from_dict_records(\n                [dict(a=100,b=200), dict(a=20,b=30), dict(a=101,b=101), dict(a=201,b=301)], index=sf.IndexHierarchy.from_labels([(1,\'ddd\',0), (1,\'bbb\',0), (2,\'ccc\',0), (2,\'eee\',0)]))\n\n        # produce invalid index labels into an IndexHierarchy constructor\n        with self.assertRaises(RuntimeError):\n            sf.Frame.from_concat((frame1, frame2))\n\n\n    def test_frame_from_concat_u(self) -> None:\n        # this fails; figure out why\n        a = sf.Series((\'a\', \'b\', \'c\'), index=range(3, 6))\n        f = sf.Frame.from_concat((\n                a,\n                sf.Series(a.index.values, index=a.index)),\n                axis=0,\n                columns=(3, 4, 5), index=(1,2))\n\n        self.assertEqual(f.to_pairs(0),\n                ((3, ((1, \'a\'), (2, 3))), (4, ((1, \'b\'), (2, 4))), (5, ((1, \'c\'), (2, 5))))\n                )\n\n\n    def test_frame_from_concat_v(self) -> None:\n        records1 = (\n                (2, False),\n                (34, False),\n                )\n\n        f1 = Frame.from_records(records1,\n                columns=(\'p\', \'q\'),\n                index=(\'x\', \'y\'))\n\n        records2 = (\n                (\'c\', False),\n                (\'d\', True),\n                )\n        f2 = Frame.from_records(records2,\n                columns=(\'p\', \'q\',),\n                index=(\'x\', \'y\'))\n\n        # get combined columns as they are unique\n        post1 = Frame.from_concat((f1, f2), axis=1, columns=IndexAutoFactory)\n        self.assertEqual(post1.to_pairs(0),\n                ((0, ((\'x\', 2), (\'y\', 34))), (1, ((\'x\', False), (\'y\', False))), (2, ((\'x\', \'c\'), (\'y\', \'d\'))), (3, ((\'x\', False), (\'y\', True))))\n                )\n\n        with self.assertRaises(ErrorInitFrame):\n            Frame.from_concat((f1, f2), axis=1, columns=IndexAutoFactory, index=IndexAutoFactory)\n\n        post2 = Frame.from_concat((f1, f2), axis=0, index=IndexAutoFactory)\n        self.assertEqual(post2.to_pairs(0),\n                ((\'p\', ((0, 2), (1, 34), (2, \'c\'), (3, \'d\'))), (\'q\', ((0, False), (1, False), (2, False), (3, True))))\n                )\n\n        with self.assertRaises(ErrorInitFrame):\n            Frame.from_concat((f1, f2), axis=0, index=IndexAutoFactory, columns=IndexAutoFactory)\n\n\n\n\n    @skip_win  # type: ignore\n    def test_frame_from_concat_w(self) -> None:\n\n        a = sf.Frame.from_dict({0:(1,2), 1:(2,3), 2:(True, True)})\n        b = sf.Frame.from_dict({0:(1,2), 1:(np.nan, np.nan), 2:(False, False)})\n\n        # reblock first two columns into integers\n        c = a.astype[[0,1]](int)\n        self.assertEqual(c._blocks.shapes.tolist(),\n                [(2, 2), (2,)])\n\n        # unaligned blocks compared column to column\n        post1 = sf.Frame.from_concat([c, b], index=sf.IndexAutoFactory)\n\n        self.assertEqual(post1.dtypes.values.tolist(),\n                [np.dtype(\'int64\'), np.dtype(\'float64\'), np.dtype(\'bool\')]\n                )\n\n        post2 = sf.Frame.from_concat([a, b], index=sf.IndexAutoFactory)\n\n        self.assertEqual(post2.dtypes.values.tolist(),\n                [np.dtype(\'int64\'), np.dtype(\'float64\'), np.dtype(\'bool\')]\n                )\n\n\n    def test_frame_from_concat_x(self) -> None:\n        f1 = Frame.from_concat([])\n        self.assertEqual((0,0), f1.shape)\n\n        f2 = Frame.from_concat([], columns=\'a\')\n        self.assertEqual((0,1), f2.shape)\n        self.assertEqual((1,),  f2.columns.shape)\n\n        f3 = Frame.from_concat([], index=[])\n        self.assertEqual((0,0), f3.shape)\n        self.assertEqual((0,),  f3.index.shape)\n\n        f4 = Frame.from_concat([], name=\'f4\')\n        self.assertEqual((0,0), f4.shape)\n        self.assertEqual(\'f4\',  f4.name)\n\n        f5 = Frame.from_concat([], columns=\'a\', index=[], name=\'f5\')\n        self.assertEqual((0,1), f5.shape)\n        self.assertEqual((1,),  f5.columns.shape)\n        self.assertEqual((0,),  f5.index.shape)\n        self.assertEqual(\'f5\',  f5.name)\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_from_concat_error_init_a(self) -> None:\n        f1 = Frame.from_element(10,\n                columns=(\'p\', \'q\',),\n                index=(\'x\', \'z\'))\n        f2 = Frame.from_element(\'x\',\n                columns=(\'p\', \'q\',),\n                index=(\'x\', \'z\'))\n\n        with self.assertRaises(ErrorInitFrame):\n            _ = Frame.from_concat((f1, f2), axis=0)\n\n        with self.assertRaises(ErrorInitFrame):\n            _ = Frame.from_concat((f1, f2), axis=1)\n\n        f3 = Frame.from_concat((f1, f2), axis=0, index=IndexAutoFactory)\n        self.assertEqual(f3.to_pairs(0),\n                ((\'p\', ((0, 10), (1, 10), (2, \'x\'), (3, \'x\'))), (\'q\', ((0, 10), (1, 10), (2, \'x\'), (3, \'x\'))))\n                )\n\n        f4 = Frame.from_concat((f1, f2), axis=1, columns=IndexAutoFactory)\n        self.assertEqual(f4.to_pairs(0),\n                ((0, ((\'x\', 10), (\'z\', 10))), (1, ((\'x\', 10), (\'z\', 10))), (2, ((\'x\', \'x\'), (\'z\', \'x\'))), (3, ((\'x\', \'x\'), (\'z\', \'x\'))))\n                )\n\n\n\n    def test_frame_from_concat_consolidate_blocks_a(self) -> None:\n        f1 = Frame.from_element(False,\n                columns=(\'p\', \'q\'),\n                index=(\'x\', \'z\'))\n\n        f2 = Frame.from_element(True,\n                columns=(\'r\', \'s\',),\n                index=(\'x\', \'z\'))\n\n        self.assertEqual(\n                Frame.from_concat((f1, f2), axis=1, consolidate_blocks=True)._blocks.shapes.tolist(),\n                [(2, 4)]\n                )\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_from_concat_items_a(self) -> None:\n        records1 = (\n                (2, 2, False),\n                (30, 34, False),\n                )\n\n        f1 = Frame.from_records(records1,\n                columns=(\'p\', \'q\', \'t\'),\n                index=(\'x\', \'a\'))\n\n        records2 = (\n                (\'c\', False),\n                (\'d\', True),\n                )\n        f2 = Frame.from_records(records2,\n                columns=(\'r\', \'s\',),\n                index=(\'x\', \'a\'))\n\n        f3 = Frame.from_concat_items(dict(A=f1, B=f2).items(), axis=1)\n\n        self.assertEqual(f3.to_pairs(0),\n                (((\'A\', \'p\'), ((\'x\', 2), (\'a\', 30))), ((\'A\', \'q\'), ((\'x\', 2), (\'a\', 34))), ((\'A\', \'t\'), ((\'x\', False), (\'a\', False))), ((\'B\', \'r\'), ((\'x\', \'c\'), (\'a\', \'d\'))), ((\'B\', \'s\'), ((\'x\', False), (\'a\', True)))))\n\n        f4 = FrameGO.from_concat_items(dict(A=f1, B=f2).items(), axis=1)\n        self.assertEqual(f4.__class__, FrameGO)\n        self.assertEqual(f4.columns.__class__, IndexHierarchyGO)\n        self.assertEqual(f4.index.__class__, Index)\n\n        self.assertEqual(f4.to_pairs(0),\n                (((\'A\', \'p\'), ((\'x\', 2), (\'a\', 30))), ((\'A\', \'q\'), ((\'x\', 2), (\'a\', 34))), ((\'A\', \'t\'), ((\'x\', False), (\'a\', False))), ((\'B\', \'r\'), ((\'x\', \'c\'), (\'a\', \'d\'))), ((\'B\', \'s\'), ((\'x\', False), (\'a\', True)))))\n\n    def test_frame_from_concat_items_b(self) -> None:\n\n        f1 = Frame.from_records(((2, False), (34, False)),\n                columns=(\'p\', \'q\',),\n                index=(\'d\', \'c\'))\n\n        s1 = Series((0, True), index=(\'p\', \'q\'), name=\'c\', dtype=object)\n        s2 = Series((-2, False), index=(\'p\', \'q\'), name=\'d\', dtype=object)\n\n        f2 = Frame.from_concat_items(dict(A=s2, B=f1, C=s1).items(), axis=0)\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'p\', (((\'A\', \'d\'), -2), ((\'B\', \'d\'), 2), ((\'B\', \'c\'), 34), ((\'C\', \'c\'), 0))), (\'q\', (((\'A\', \'d\'), False), ((\'B\', \'d\'), False), ((\'B\', \'c\'), False), ((\'C\', \'c\'), True))))\n                )\n        self.assertEqual(f2.index.__class__, IndexHierarchy)\n\n    def test_frame_from_concat_items_c(self) -> None:\n        f1 = Frame.from_concat_items([])\n        self.assertEqual((0,0), f1.shape)\n\n        f2 = Frame.from_concat_items([], name=\'f2\')\n        self.assertEqual((0,0), f2.shape)\n        self.assertEqual(\'f2\',  f2.name)\n\n        # Demonstrate the other arguments are inconsequential\n        f3 = Frame.from_concat_items([],\n                axis=1,\n                union=False,\n                name=\'f3\',\n                fill_value=True,\n                consolidate_blocks=False)\n        self.assertEqual((0,0), f3.shape)\n        self.assertEqual(\'f3\',  f3.name)\n\n\n    def test_frame_from_concat_items_d(self) -> None:\n\n        s1 = Series((0, True), index=(\'p\', \'q\'), name=\'c\', dtype=object)\n        s2 = Series((-2, False), index=(\'p\', \'q\'), name=\'d\', dtype=object)\n\n        with self.assertRaises(AxisInvalid):\n            f1 = Frame.from_concat_items(dict(A=s1, B=s2).items(), axis=2) # type: ignore\n\n\n    def test_frame_from_concat_items_e(self) -> None:\n        records1 = (\n                (2, 2, False),\n                (30, 34, False),\n                )\n        f1 = Frame.from_records(records1,\n                columns=(\'p\', \'q\', \'t\'),\n                index=(\'x\', \'a\'))\n\n        records2 = (\n                (\'c\', False),\n                (\'d\', True),\n                )\n        f2 = Frame.from_records(records2,\n                columns=(\'r\', \'s\',),\n                index=(\'x\', \'a\'))\n\n        # this produces an IH with an outer level of\n        f3 = Frame.from_concat_items({(\'a\', \'b\'):f1, (\'x\', \'y\'):f2}.items(), axis=1)\n\n        self.assertEqual(f3.shape, (2, 5))\n        self.assertEqual(f3.columns.shape, (5, 2))\n        self.assertEqual(f3.T.index.shape, (5, 2))\n\n        self.assertEqual(f3.to_pairs(0),\n                ((((\'a\', \'b\'), \'p\'), ((\'x\', 2), (\'a\', 30))), (((\'a\', \'b\'), \'q\'), ((\'x\', 2), (\'a\', 34))), (((\'a\', \'b\'), \'t\'), ((\'x\', False), (\'a\', False))), (((\'x\', \'y\'), \'r\'), ((\'x\', \'c\'), (\'a\', \'d\'))), (((\'x\', \'y\'), \'s\'), ((\'x\', False), (\'a\', True))))\n                )\n\n        self.assertEqual(\n                f3.columns._levels.values.tolist(), #type: ignore\n                [[(\'a\', \'b\'), \'p\'], [(\'a\', \'b\'), \'q\'], [(\'a\', \'b\'), \'t\'], [(\'x\', \'y\'), \'r\'], [(\'x\', \'y\'), \'s\']]\n                )\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_set_index_a(self) -> None:\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\', \'y\'),\n                consolidate_blocks=True)\n\n        self.assertEqual(f1.set_index(\'r\').to_pairs(0),\n                ((\'p\', ((\'a\', 2), (\'b\', 30))), (\'q\', ((\'a\', 2), (\'b\', 34))), (\'r\', ((\'a\', \'a\'), (\'b\', \'b\'))), (\'s\', ((\'a\', False), (\'b\', True))), (\'t\', ((\'a\', False), (\'b\', False)))))\n\n        self.assertEqual(f1.set_index(\'r\', drop=True).to_pairs(0),\n                ((\'p\', ((\'a\', 2), (\'b\', 30))), (\'q\', ((\'a\', 2), (\'b\', 34))), (\'s\', ((\'a\', False), (\'b\', True))), (\'t\', ((\'a\', False), (\'b\', False)\n                ))))\n\n        f2 = f1.set_index(\'r\', drop=True)\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'p\', ((\'a\', 2), (\'b\', 30))), (\'q\', ((\'a\', 2), (\'b\', 34))), (\'s\', ((\'a\', False), (\'b\', True))), (\'t\', ((\'a\', False), (\'b\', False))))\n                )\n\n        self.assertTrue(f1.mloc[[0, 2]].tolist() == f2.mloc.tolist())\n\n\n    def test_frame_set_index_b(self) -> None:\n        records = (\n                (2, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\', \'y\'),\n                consolidate_blocks=True)\n\n        for col in f1.columns:\n            f2 = f1.set_index(col)\n            self.assertEqual(f2.index.name, col)\n\n\n    def test_frame_set_index_c(self) -> None:\n        records = (\n                (2, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                )\n\n    def test_frame_set_index_d(self) -> None:\n\n        for arrays in self.get_arrays_a():\n            tb1 = TypeBlocks.from_blocks(arrays)\n\n            f1 = FrameGO(tb1)\n            f1[tb1.shape[1]] = range(tb1.shape[0])\n\n            for i in range(f1.shape[1]):\n                f2 = f1.set_index(i, drop=True)\n                self.assertTrue(f2.shape == (3, f1.shape[1] - 1))\n\n\n    def test_frame_head_tail_a(self) -> None:\n\n        # thest of multi threaded apply\n\n        f1 = Frame.from_items(\n                zip(range(10), (np.random.rand(1000) for _ in range(10)))\n                )\n        self.assertEqual(f1.head(3).index.values.tolist(),\n                [0, 1, 2])\n        self.assertEqual(f1.tail(3).index.values.tolist(),\n                [997, 998, 999])\n\n    #---------------------------------------------------------------------------\n    def test_frame_from_records_date_a(self) -> None:\n\n        d = np.datetime64\n\n        records = (\n                (d(\'2018-01-02\'), d(\'2018-01-02\'), \'a\', False, False),\n                (d(\'2017-01-02\'), d(\'2017-01-02\'), \'b\', True, False),\n                (d(\'2016-01-02\'), d(\'2016-01-02\'), \'c\', False, False),\n                (d(\'2015-01-02\'), d(\'2015-01-02\'), \'d\', True, True),\n                )\n\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=None)\n\n        dtype = np.dtype\n\n        self.assertEqual(list(f1._blocks._reblock_signature()),\n                [(dtype(\'<M8[D]\'), 2), (dtype(\'<U1\'), 1), (dtype(\'bool\'), 2)])\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_from_records_a(self) -> None:\n\n        NT = namedtuple(\'Sample\', (\'a\', \'b\', \'c\'))\n        records = [NT(x, x, x) for x in range(4)]\n        f1 = Frame.from_records(records)\n        self.assertEqual(f1.columns.values.tolist(), [\'a\', \'b\', \'c\'])\n        self.assertEqual(f1.sum().to_pairs(),\n                ((\'a\', 6), (\'b\', 6), (\'c\', 6)))\n\n    def test_frame_from_records_b(self) -> None:\n\n        f1 = sf.Frame.from_records([[1, 2], [2, 3]], columns=[\'a\', \'b\'])\n        self.assertEqual(f1.to_pairs(0),\n                ((\'a\', ((0, 1), (1, 2))), (\'b\', ((0, 2), (1, 3)))))\n\n        with self.assertRaises(Exception):\n            f2 = sf.Frame.from_records([[1, 2], [2, 3]], columns=[\'a\'])\n\n\n    def test_frame_from_records_c(self) -> None:\n\n        s1 = Series([3, 4, 5], index=(\'x\', \'y\', \'z\'))\n        s2 = Series(list(\'xyz\'), index=(\'x\', \'y\', \'z\'))\n\n        with self.assertRaises(Exception):\n            # cannot use Series in from_records\n            f1 = sf.Frame.from_records([s1, s2], columns=[\'a\', \'b\', \'c\'])\n\n\n    def test_frame_from_records_d(self) -> None:\n\n        a1 = np.array([[1,2,3], [4,5,6]])\n\n        f1 = sf.Frame.from_records(a1, index=(\'x\', \'y\'), columns=[\'a\', \'b\', \'c\'], name=\'foo\')\n\n        self.assertEqual(f1.to_pairs(0),\n                ((\'a\', ((\'x\', 1), (\'y\', 4))), (\'b\', ((\'x\', 2), (\'y\', 5))), (\'c\', ((\'x\', 3), (\'y\', 6)))))\n        self.assertEqual(f1.name, \'foo\')\n\n\n    def test_frame_from_records_e(self) -> None:\n\n        records = [[1,\'2\',3], [4,\'5\',6]]\n        dtypes = (np.int64, str, str)\n        f1 = sf.Frame.from_records(records,\n                index=(\'x\', \'y\'),\n                columns=[\'a\', \'b\', \'c\'],\n                dtypes=dtypes)\n        self.assertEqual(f1.dtypes.iter_element().apply(str).to_pairs(),\n                ((\'a\', \'int64\'), (\'b\', \'<U1\'), (\'c\', \'<U1\'))\n                )\n\n    def test_frame_from_records_f(self) -> None:\n\n        records = [[1,\'2\',3], [4,\'5\',6]]\n        dtypes = {\'b\': np.int64}\n        f1 = sf.Frame.from_records(records,\n                index=(\'x\', \'y\'),\n                columns=[\'a\', \'b\', \'c\'],\n                dtypes=dtypes)\n\n        self.assertEqual(str(f1.dtypes[\'b\']), \'int64\')\n\n\n    def test_frame_from_records_g(self) -> None:\n\n        NT = namedtuple(\'NT\', (\'a\', \'b\', \'c\'))\n\n        records = [NT(1,\'2\',3), NT(4,\'5\',6)]\n        dtypes = {\'b\': np.int64}\n        f1 = sf.Frame.from_records(records, dtypes=dtypes)\n\n        self.assertEqual(str(f1.dtypes[\'b\']), \'int64\')\n\n\n    def test_frame_from_records_h(self) -> None:\n\n        with self.assertRaises(ErrorInitFrame):\n            Frame.from_records(())\n\n        with self.assertRaises(ErrorInitFrame):\n            Frame.from_records(((0, 1, 2) for x in range(3) if x < 0))\n\n    def test_frame_from_records_i(self) -> None:\n\n        f1 = sf.Frame.from_records([\n            (88,),\n            (27, ),\n            (27,),\n            (None,)])\n\n        self.assertEqual(f1.to_pairs(0),\n                ((0, ((0, 88), (1, 27), (2, 27), (3, None))),))\n\n\n    def test_frame_from_records_j(self) -> None:\n\n        records = [\n            dict(a=1, b=2),\n            dict(b=10, c=4),\n            dict(c=20, d=-1)\n        ]\n        with self.assertRaises(ErrorInitFrame):\n            # cannot supply columns when records are dictionaries\n            f1 = Frame.from_records(records, columns=(\'b\', \'c\', \'d\'))\n\n\n    def test_frame_from_records_k(self) -> None:\n        def gen() -> tp.Iterator[int]:\n            empty: tp.Iterable[int] = ()\n            for k in empty:\n                yield k\n\n        f1 = Frame.from_records(gen(), columns=(\'a\', \'b\', \'c\'))\n        self.assertEqual(f1.to_pairs(0),\n                ((\'a\', ()), (\'b\', ()), (\'c\', ())))\n\n\n    def test_frame_from_records_m(self) -> None:\n\n        records = np.arange(4).reshape((2, 2))\n        dtypes = (bool, bool)\n        with self.assertRaises(ErrorInitFrame):\n            f1 = sf.Frame.from_records(records, dtypes=dtypes)\n\n\n\n    def test_frame_from_records_n(self) -> None:\n\n        mapping = {10: (3, 4), 50: (5, 6)}\n        f1 = sf.Frame.from_records(mapping.values())\n        self.assertEqual(f1.to_pairs(0),\n                ((0, ((0, 3), (1, 5))), (1, ((0, 4), (1, 6))))\n                )\n\n    def test_frame_from_records_o(self) -> None:\n\n        records = ((\'x\', \'t\'), (1, 2))\n        f1 = sf.Frame.from_records(records)\n\n        self.assertEqual(\n                f1.to_pairs(0),\n                ((0, ((0, \'x\'), (1, 1))), (1, ((0, \'t\'), (1, 2))))\n                )\n\n\n\n    def test_frame_from_records_p(self) -> None:\n\n        records = [(\'x\', \'t\')] * 10 + [(1, 2)] #type: ignore\n        f1 = sf.Frame.from_records(records)\n        self.assertEqual(f1.to_pairs(0),\n                ((0, ((0, \'x\'), (1, \'x\'), (2, \'x\'), (3, \'x\'), (4, \'x\'), (5, \'x\'), (6, \'x\'), (7, \'x\'), (8, \'x\'), (9, \'x\'), (10, 1))), (1, ((0, \'t\'), (1, \'t\'), (2, \'t\'), (3, \'t\'), (4, \'t\'), (5, \'t\'), (6, \'t\'), (7, \'t\'), (8, \'t\'), (9, \'t\'), (10, 2))))\n                )\n\n    def test_frame_from_records_q(self) -> None:\n\n        # Y: tp.Type[tp.NamedTuple]\n\n        class Y(tp.NamedTuple): # pylint: disable=E0102\n            x: str\n            y: int\n\n        f0 = Frame.from_records([(Y(""foo"", 1), 1, 2)])\n        f1 = Frame.from_records([(1, 2, Y(""foo"", 1))])\n        f2 = Frame.from_records([(1, 2, (""foo"", 1))])\n\n        self.assertEqual(f0.shape, f1.shape)\n        self.assertEqual(f0.shape, f2.shape)\n\n        self.assertEqual(f0.to_pairs(0),\n                ((0, ((0, Y(x=\'foo\', y=1)),)), (1, ((0, 1),)), (2, ((0, 2),))) #pylint: disable=E1120\n                )\n        self.assertEqual(f1.to_pairs(0),\n                ((0, ((0, 1),)), (1, ((0, 2),)), (2, ((0, Y(x=\'foo\', y=1)),))) #pylint: disable=E1120\n                )\n        self.assertEqual(f2.to_pairs(0),\n                ((0, ((0, 1),)), (1, ((0, 2),)), (2, ((0, (\'foo\', 1)),)))\n                )\n\n\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_from_dict_records_a(self) -> None:\n\n        records = [{\'a\':x, \'b\':x, \'c\':x} for x in range(4)]\n        f1 = Frame.from_dict_records(records)\n        self.assertEqual(f1.columns.values.tolist(), [\'a\', \'b\', \'c\'])\n        self.assertEqual(f1.sum().to_pairs(),\n                ((\'a\', 6), (\'b\', 6), (\'c\', 6)))\n\n    def test_frame_from_dict_records_b(self) -> None:\n        # handle case of dict views\n        a = {1: {\'a\': 1, \'b\': 2,}, 2: {\'a\': 4, \'b\': 3,}}\n\n        post = Frame.from_dict_records(a.values(), index=list(a.keys()))\n\n        self.assertEqual(post.to_pairs(0),\n                ((\'a\', ((1, 1), (2, 4))), (\'b\', ((1, 2), (2, 3)))))\n\n\n    def test_frame_from_dict_records_c(self) -> None:\n\n        records = [dict(a=1, b=\'2\', c=3), dict(a=4, b=\'5\', c=6)]\n        dtypes = {\'b\': np.int64}\n        f1 = sf.Frame.from_dict_records(records, dtypes=dtypes)\n\n        self.assertEqual(str(f1.dtypes[\'b\']), \'int64\')\n\n    def test_frame_from_dict_records_d(self) -> None:\n\n        records = [\n            dict(a=1, b=2),\n            dict(b=10, c=4),\n            dict(c=20, d=-1)\n        ]\n        f1 = Frame.from_dict_records(records, fill_value=0)\n        self.assertEqual(f1.to_pairs(0),\n                ((\'a\', ((0, 1), (1, 0), (2, 0))), (\'b\', ((0, 2), (1, 10), (2, 0))), (\'c\', ((0, 0), (1, 4), (2, 20))), (\'d\', ((0, 0), (1, 0), (2, -1))))\n                )\n\n    def test_frame_from_dict_records_e(self) -> None:\n        # handle case of dict views\n        a = {1: {\'a\': 1, \'b\': 2,}, 2: {\'a\': 4, \'b\': 3,}}\n\n        post = Frame.from_dict_records(a.values(), index=list(a.keys()), dtypes=(int, str))\n        self.assertEqual(post.to_pairs(0),\n                ((\'a\', ((1, 1), (2, 4))), (\'b\', ((1, \'2\'), (2, \'3\'))))\n                )\n\n    def test_frame_from_dict_records_f(self) -> None:\n        # handle case of dict views\n        data = ()\n        with self.assertRaises(ErrorInitFrame):\n            _ = Frame.from_dict_records(data)\n\n    def test_frame_from_dict_records_g(self) -> None:\n\n        records = [\n                dict(a=True, b=False),\n                dict(a=True, b=False),\n                ]\n        f1 = Frame.from_dict_records(records, consolidate_blocks=True)\n        self.assertEqual(f1._blocks.shapes.tolist(), [(2, 2)])\n\n\n    def test_frame_from_dict_records_h(self) -> None:\n\n        records = [\n                dict(a=True, b=False),\n                dict(b=True, c=False),\n                ]\n        f1 = Frame.from_dict_records(records, fill_value=\'x\')\n        self.assertEqual(f1.to_pairs(0),\n                ((\'a\', ((0, True), (1, \'x\'))), (\'b\', ((0, False), (1, True))), (\'c\', ((0, \'x\'), (1, False))))\n                )\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_from_json_a(self) -> None:\n\n        msg = """"""[\n        {\n        ""userId"": 1,\n        ""id"": 1,\n        ""title"": ""delectus aut autem"",\n        ""completed"": false\n        },\n        {\n        ""userId"": 1,\n        ""id"": 2,\n        ""title"": ""quis ut nam facilis et officia qui"",\n        ""completed"": false\n        },\n        {\n        ""userId"": 1,\n        ""id"": 3,\n        ""title"": ""fugiat veniam minus"",\n        ""completed"": false\n        },\n        {\n        ""userId"": 1,\n        ""id"": 4,\n        ""title"": ""et porro tempora"",\n        ""completed"": true\n        }]""""""\n\n        f1 = Frame.from_json(msg, name=msg)\n        self.assertEqual(sorted(f1.columns.values.tolist()),\n                sorted([\'completed\', \'id\', \'title\', \'userId\']))\n        self.assertEqual(f1[\'id\'].sum(), 10)\n\n        self.assertEqual(f1.name, msg)\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_relabel_flat_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                )\n\n        columns = IndexHierarchy.from_labels(\n                ((\'a\', 1), (\'a\', 2), (\'b\', 1), (\'b\', 2), (\'b\', 3)))\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=(\'x\', \'y\', \'z\'))\n\n        f2 = f1.relabel_flat(columns=True)\n\n        self.assertEqual(f2.to_pairs(0),\n                (((\'a\', 1), ((\'x\', 1), (\'y\', 30), (\'z\', 54))), ((\'a\', 2), ((\'x\', 2), (\'y\', 34), (\'z\', 95))), ((\'b\', 1), ((\'x\', \'a\'), (\'y\', \'b\'), (\'z\', \'c\'))), ((\'b\', 2), ((\'x\', False), (\'y\', True), (\'z\', False))), ((\'b\', 3), ((\'x\', True), (\'y\', False), (\'z\', False)))))\n\n    def test_frame_relabel_flat_b(self) -> None:\n        records = (\n                (2, \'a\', False),\n                (34, \'b\', True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\'))\n\n        with self.assertRaises(RuntimeError):\n            _ = f1.relabel_flat()\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_rename_a(self) -> None:\n\n        records = (\n                (2, \'a\', False),\n                (34, \'b\', True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\'),\n                name=\'foo\')\n        self.assertEqual(f1.name, \'foo\')\n\n        f2 = Frame(f1)\n        self.assertEqual(f2.name, \'foo\')\n\n\n    #---------------------------------------------------------------------------\n\n\n    def test_frame_add_level_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                index=(\'x\', \'y\', \'z\'))\n\n        f2 = f1.relabel_add_level(index=\'I\', columns=\'II\')\n\n        self.assertEqual(f2.to_pairs(0),\n                (((\'II\', \'a\'), (((\'I\', \'x\'), 1), ((\'I\', \'y\'), 30), ((\'I\', \'z\'), 54))), ((\'II\', \'b\'), (((\'I\', \'x\'), 2), ((\'I\', \'y\'), 34), ((\'I\', \'z\'), 95))), ((\'II\', \'c\'), (((\'I\', \'x\'), \'a\'), ((\'I\', \'y\'), \'b\'), ((\'I\', \'z\'), \'c\'))), ((\'II\', \'d\'), (((\'I\', \'x\'), False), ((\'I\', \'y\'), True), ((\'I\', \'z\'), False))), ((\'II\', \'e\'), (((\'I\', \'x\'), True), ((\'I\', \'y\'), False), ((\'I\', \'z\'), False))))\n                )\n\n\n    def test_frame_from_from_pandas_a(self) -> None:\n        import pandas as pd\n\n        pdf = pd.DataFrame(\n                dict(a=(False, True, False),\n                b=(False, False,False),\n                c=(1,2,3),\n                d=(4,5,6),\n                e=(None, None, None)))\n\n        sff = Frame.from_pandas(pdf)\n        self.assertTrue((pdf.dtypes.values == sff.dtypes.values).all())\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_to_frame_go_a(self) -> None:\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                index=(\'x\', \'y\', \'z\'))\n\n        f2 = f1.to_frame_go()\n        f2[\'f\'] = None\n        self.assertEqual(f1.columns.values.tolist(),\n                [\'a\', \'b\', \'c\', \'d\', \'e\'])\n        self.assertEqual(f2.columns.values.tolist(),\n                [\'a\', \'b\', \'c\', \'d\', \'e\', \'f\'])\n\n        # underlying map objects must be different\n        self.assertTrue(id(f1.columns._map) != id(f2.columns._map))\n\n\n    def test_frame_to_frame_go_b(self) -> None:\n        records = (\n                (1, 2, \'a\', False, True),\n                (54, 95, \'c\', False, False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                index=(\'x\', \'y\'),\n                name=\'foo\')\n\n        f2 = f1.to_frame_go()\n        f3 = f2.to_frame()\n\n        self.assertTrue(f1.name, \'foo\')\n        self.assertTrue(f2.name, \'foo\')\n        self.assertTrue(f3.name, \'foo\')\n\n\n\n    def test_frame_to_frame_go_c(self) -> None:\n        records = (\n                (1, \'a\', False, True),\n                (1, \'b\', False, False),\n                (2, \'a\', False, True),\n                (2, \'b\', False, False),\n                )\n        f1 = Frame.from_records(records,\n                columns=IndexHierarchy.from_product((1, 2), (\'a\', \'b\')),\n                index=(\'w\', \'x\', \'y\', \'z\'),\n                name=\'foo\')\n\n        f2 = f1.to_frame_go()\n\n        self.assertTrue(isinstance(f2.columns, IndexHierarchyGO))\n\n        f2[(3, \'a\')] = 10\n\n        self.assertEqual(\n                f2.to_pairs(0),\n                (((1, \'a\'), ((\'w\', 1), (\'x\', 1), (\'y\', 2), (\'z\', 2))), ((1, \'b\'), ((\'w\', \'a\'), (\'x\', \'b\'), (\'y\', \'a\'), (\'z\', \'b\'))), ((2, \'a\'), ((\'w\', False), (\'x\', False), (\'y\', False), (\'z\', False))), ((2, \'b\'), ((\'w\', True), (\'x\', False), (\'y\', True), (\'z\', False))), ((3, \'a\'), ((\'w\', 10), (\'x\', 10), (\'y\', 10), (\'z\', 10))))\n        )\n\n\n    def test_frame_to_frame_go_d(self) -> None:\n\n        records = (\n                (2, \'a\', False),\n                (34, \'b\', True),\n                )\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\'))\n\n        f2 = f1.to_frame_go()\n\n        f1[\'x\'] = None\n        f2[\'a\'] = -1\n\n        self.assertEqual(f1.to_pairs(0),\n                ((\'p\', ((\'w\', 2), (\'x\', 34))), (\'q\', ((\'w\', \'a\'), (\'x\', \'b\'))), (\'r\', ((\'w\', False), (\'x\', True))), (\'x\', ((\'w\', None), (\'x\', None))))\n                )\n        self.assertEqual(f2.to_pairs(0),\n                ((\'p\', ((\'w\', 2), (\'x\', 34))), (\'q\', ((\'w\', \'a\'), (\'x\', \'b\'))), (\'r\', ((\'w\', False), (\'x\', True))), (\'a\', ((\'w\', -1), (\'x\', -1))))\n                )\n\n    def test_frame_to_frame_go_e(self) -> None:\n\n        records = (\n                (2, \'a\', False),\n                (34, \'b\', True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\'),\n                index=(\'w\', \'x\'))\n\n        f2 = f1.to_frame()\n        f3 = f1.to_frame_go()\n\n        self.assertTrue(id(f1) == id(f2))\n        self.assertTrue(id(f1) != id(f3))\n\n        f3[\'x\'] = None\n\n        self.assertEqual(f3.to_pairs(0),\n                ((\'p\', ((\'w\', 2), (\'x\', 34))), (\'q\', ((\'w\', \'a\'), (\'x\', \'b\'))), (\'r\', ((\'w\', False), (\'x\', True))), (\'x\', ((\'w\', None), (\'x\', None))))\n                )\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_astype_a(self) -> None:\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 34, \'b\', True, False),\n                (54, 95, \'c\', False, False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                index=(\'x\', \'y\', \'z\'))\n\n        f2 = f1.astype[\'d\':](int)  #type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(f2.to_pairs(0),\n                ((\'a\', ((\'x\', 1), (\'y\', 30), (\'z\', 54))), (\'b\', ((\'x\', 2), (\'y\', 34), (\'z\', 95))), (\'c\', ((\'x\', \'a\'), (\'y\', \'b\'), (\'z\', \'c\'))), (\'d\', ((\'x\', 0), (\'y\', 1), (\'z\', 0))), (\'e\', ((\'x\', 1), (\'y\', 0), (\'z\', 0))))\n                )\n\n        f3 = f1.astype[[\'a\', \'b\']](bool)\n        self.assertEqual(f3.to_pairs(0),\n                ((\'a\', ((\'x\', True), (\'y\', True), (\'z\', True))), (\'b\', ((\'x\', True), (\'y\', True), (\'z\', True))), (\'c\', ((\'x\', \'a\'), (\'y\', \'b\'), (\'z\', \'c\'))), (\'d\', ((\'x\', False), (\'y\', True), (\'z\', False))), (\'e\', ((\'x\', True), (\'y\', False), (\'z\', False))))\n                )\n\n\n    def test_frame_pickle_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 50, \'b\', True, False))\n\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\',\'y\'))\n\n        pbytes = pickle.dumps(f1)\n        f2 = pickle.loads(pbytes)\n\n        self.assertEqual([b.flags.writeable for b in f2._blocks._blocks],\n                [False, False, False, False, False])\n\n    def test_frame_set_index_hierarchy_a(self) -> None:\n\n        records = (\n                (1, 2, \'a\', False, True),\n                (30, 2, \'b\', True, False),\n                (30, 50, \'a\', True, False),\n                (30, 50, \'b\', True, False),\n                )\n\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        f2 = f1.set_index_hierarchy([\'q\', \'r\'])\n        self.assertEqual(f2.index.name, (\'q\', \'r\'))\n\n        # we reuse the same block data\n        self.assertTrue((f2.index._blocks.mloc == f1._blocks[[1, 2]].mloc).all())\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'p\', (((2, \'a\'), 1), ((2, \'b\'), 30), ((50, \'a\'), 30), ((50, \'b\'), 30))), (\'q\', (((2, \'a\'), 2), ((2, \'b\'), 2), ((50, \'a\'), 50), ((50, \'b\'), 50))), (\'r\', (((2, \'a\'), \'a\'), ((2, \'b\'), \'b\'), ((50, \'a\'), \'a\'), ((50, \'b\'), \'b\'))), (\'s\', (((2, \'a\'), False), ((2, \'b\'), True), ((50, \'a\'), True), ((50, \'b\'), True))), (\'t\', (((2, \'a\'), True), ((2, \'b\'), False), ((50, \'a\'), False), ((50, \'b\'), False)))))\n\n        f3 = f1.set_index_hierarchy([\'q\', \'r\'], drop=True)\n        self.assertEqual(f3.index.name, (\'q\', \'r\'))\n\n        self.assertEqual(f3.to_pairs(0),\n                ((\'p\', (((2, \'a\'), 1), ((2, \'b\'), 30), ((50, \'a\'), 30), ((50, \'b\'), 30))), (\'s\', (((2, \'a\'), False), ((2, \'b\'), True), ((50, \'a\'), True), ((50, \'b\'), True))), (\'t\', (((2, \'a\'), True), ((2, \'b\'), False), ((50, \'a\'), False), ((50, \'b\'), False))))\n                )\n\n        f4 = f1.set_index_hierarchy(slice(\'q\', \'r\'), drop=True)\n        self.assertEqual(f4.index.name, (\'q\', \'r\'))\n\n        self.assertEqual(f4.to_pairs(0),\n                ((\'p\', (((2, \'a\'), 1), ((2, \'b\'), 30), ((50, \'a\'), 30), ((50, \'b\'), 30))), (\'s\', (((2, \'a\'), False), ((2, \'b\'), True), ((50, \'a\'), True), ((50, \'b\'), True))), (\'t\', (((2, \'a\'), True), ((2, \'b\'), False), ((50, \'a\'), False), ((50, \'b\'), False))))\n                )\n\n\n    def test_frame_set_index_hierarchy_b(self) -> None:\n\n        labels = (\n                (1, 1, \'a\'),\n                (1, 2, \'b\'),\n                (1, 3, \'c\'),\n                (2, 1, \'d\'),\n                (2, 2, \'e\'),\n                (2, 3, \'f\'),\n                (3, 1, \'g\'),\n                (3, 2, \'h\'),\n                (3, 3, \'i\'),\n                )\n\n        f = Frame.from_records(labels)\n        f = f.astype[[0, 1]](int)\n\n\n        fh = f.set_index_hierarchy([0, 1], drop=True)\n\n        self.assertEqual(fh.columns.values.tolist(),\n                [2]\n                )\n\n        # we reuse the block arrays in the Index\n        self.assertTrue((fh.index._blocks.mloc == f._blocks[:2].mloc).all())\n\n        self.assertEqual( fh.to_pairs(0),\n                ((2, (((1, 1), \'a\'), ((1, 2), \'b\'), ((1, 3), \'c\'), ((2, 1), \'d\'), ((2, 2), \'e\'), ((2, 3), \'f\'), ((3, 1), \'g\'), ((3, 2), \'h\'), ((3, 3), \'i\'))),))\n\n        fh = f.set_index_hierarchy([0, 1], drop=False)\n        self.assertEqual(\n                fh.loc[HLoc[:, 3]].to_pairs(0),\n                ((0, (((1, 3), 1), ((2, 3), 2), ((3, 3), 3))), (1, (((1, 3), 3), ((2, 3), 3), ((3, 3), 3))), (2, (((1, 3), \'c\'), ((2, 3), \'f\'), ((3, 3), \'i\'))))\n                )\n\n\n    def test_frame_set_index_hierarchy_d(self) -> None:\n        f1 = sf.Frame.from_records([(\'one\', 1, \'hello\')],\n                columns=[\'name\', \'val\', \'msg\'])\n\n        f2 = f1.set_index_hierarchy([\'name\', \'val\'], drop=True)\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'msg\', (((\'one\', 1), \'hello\'),)),))\n\n\n\n    def test_frame_set_index_hierarchy_e(self) -> None:\n\n        records = (\n                (1, \'2018-12\', 10),\n                (1, \'2019-01\', 20),\n                (1, \'2019-02\', 30),\n                (2, \'2018-12\', 40),\n                (2, \'2019-01\', 50),\n                (2, \'2019-02\', 60),\n                )\n        f = Frame.from_records(records)\n        fh = f.set_index_hierarchy([0, 1],\n                drop=True,\n                index_constructors=(Index, IndexYearMonth))\n\n        self.assertEqual(fh.loc[HLoc[:, \'2018\']].to_pairs(0),\n                ((2, (((1, datetime.date(2018, 12, 1)), 10), ((2, datetime.date(2018, 12, 1)), 40))),))\n\n        # because we passed index_constructors, we may not be able to reuse blocks\n        self.assertTrue((fh.index._blocks.mloc != f._blocks[:2].mloc).all())\n\n\n    def test_frame_set_index_hierarchy_f(self) -> None:\n\n        records = (\n                (1, \'a\', 10),\n                (2, \'c\', 60),\n                (1, \'c\', 30),\n                (2, \'a\', 40),\n                (2, \'b\', 50),\n                (1, \'b\', 20),\n                )\n        f = Frame.from_records(records)\n        fh = f.set_index_hierarchy([0, 1],\n                drop=True,\n                reorder_for_hierarchy=True,\n                )\n\n        self.assertEqual(fh.to_pairs(0),\n                ((2, (((1, \'a\'), 10), ((1, \'c\'), 30), ((1, \'b\'), 20), ((2, \'a\'), 40), ((2, \'c\'), 60), ((2, \'b\'), 50))),)\n                )\n\n    #---------------------------------------------------------------------------\n\n\n    def test_frame_iloc_in_loc_a(self) -> None:\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual(f1.loc[ILoc[-2:], [\'q\', \'t\']].to_pairs(0),\n                ((\'q\', ((\'y\', 95), (\'z\', 73))), (\'t\', ((\'y\', False), (\'z\', True)))))\n\n        self.assertEqual(f1.loc[ILoc[[0, -1]], \'s\':].to_pairs(0),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                ((\'s\', ((\'w\', False), (\'z\', True))), (\'t\', ((\'w\', False), (\'z\', True)))))\n\n        self.assertEqual(f1.loc[[\'w\', \'x\'], ILoc[[0, -1]]].to_pairs(0),\n                ((\'p\', ((\'w\', 2), (\'x\', 30))), (\'t\', ((\'w\', False), (\'x\', False))))\n                )\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_drop_a(self) -> None:\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual(f1.drop[\'r\':].to_pairs(0),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                ((\'p\', ((\'w\', 2), (\'x\', 30), (\'y\', 2), (\'z\', 30))), (\'q\', ((\'w\', 2), (\'x\', 34), (\'y\', 95), (\'z\', 73)))))\n\n        self.assertEqual(f1.drop.loc[[\'x\', \'z\'], \'s\':].to_pairs(0),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                ((\'p\', ((\'w\', 2), (\'y\', 2))), (\'q\', ((\'w\', 2), (\'y\', 95))), (\'r\', ((\'w\', \'a\'), (\'y\', \'c\')))))\n\n        self.assertEqual(f1.drop.loc[\'x\':, \'q\':].to_pairs(0),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                ((\'p\', ((\'w\', 2),)),))\n\n\n    def test_frame_drop_b(self) -> None:\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\'))\n        self.assertEqual(f1.drop.iloc[1].to_pairs(0),\n                ((\'p\', ((\'w\', 2),)), (\'q\', ((\'w\', 2),)), (\'r\', ((\'w\', \'a\'),)), (\'s\', ((\'w\', False),)), (\'t\', ((\'w\', False),))))\n\n\n    def test_frame_drop_c(self) -> None:\n\n        index = IndexHierarchy.from_product([\'x\'], [\'a\', \'b\'])\n        f1 = Frame.from_elements([1, 2], index=index, columns=[\'a\'])\n        f2 = f1.drop[\'a\']\n        self.assertEqual(f2.shape, (2, 0))\n\n\n    def test_frame_drop_d(self) -> None:\n\n        columns = sf.IndexHierarchy.from_product([10, 20], [\'a\', \'b\'])\n\n        f1 = Frame(np.arange(8).reshape(2, 4), columns=columns)\n        f2 = f1.drop[(20, \'a\')]\n        self.assertEqual(f2.to_pairs(0),\n                (((10, \'a\'), ((0, 0), (1, 4))), ((10, \'b\'), ((0, 1), (1, 5))), ((20, \'b\'), ((0, 3), (1, 7))))\n                )\n\n        f3 = f1.drop[(10, \'b\'):] #type: ignore\n        self.assertEqual(f3.to_pairs(0),\n                (((10, \'a\'), ((0, 0), (1, 4))),)\n                )\n\n        f4 = f1.drop[[(10, \'b\'), (20, \'b\')]]\n        self.assertEqual(f4.to_pairs(0),\n                (((10, \'a\'), ((0, 0), (1, 4))), ((20, \'a\'), ((0, 2), (1, 6))))\n                )\n\n        f5 = f1.drop[:]\n        self.assertEqual(f5.shape, (2, 0))\n\n        # Check that we can represent the IndexHierarchy\n        d = f5.display(DisplayConfig(type_color=False))\n        self.assertEqual(tuple(d), ([\'<Frame>\'], [\'<IndexHierarchy>\', \'<float64>\'], [\'<Index>\'], [\'0\'], [\'1\'], [\'<int64>\']))\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_roll_a(self) -> None:\n\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        self.assertEqual(f1.roll(1).to_pairs(0),\n                ((\'p\', ((\'w\', 30), (\'x\', 2), (\'y\', 30), (\'z\', 2))), (\'q\', ((\'w\', 73), (\'x\', 2), (\'y\', 34), (\'z\', 95))), (\'r\', ((\'w\', \'d\'), (\'x\', \'a\'), (\'y\', \'b\'), (\'z\', \'c\'))), (\'s\', ((\'w\', True), (\'x\', False), (\'y\', True), (\'z\', False))), (\'t\', ((\'w\', True), (\'x\', False), (\'y\', False), (\'z\', False))))\n                )\n\n        self.assertEqual(f1.roll(-2, include_index=True).to_pairs(0),\n                ((\'p\', ((\'y\', 2), (\'z\', 30), (\'w\', 2), (\'x\', 30))), (\'q\', ((\'y\', 95), (\'z\', 73), (\'w\', 2), (\'x\', 34))), (\'r\', ((\'y\', \'c\'), (\'z\', \'d\'), (\'w\', \'a\'), (\'x\', \'b\'))), (\'s\', ((\'y\', False), (\'z\', True), (\'w\', False), (\'x\', True))), (\'t\', ((\'y\', False), (\'z\', True), (\'w\', False), (\'x\', False))))\n                )\n\n        self.assertEqual(f1.roll(-3, 3).to_pairs(0),\n                ((\'p\', ((\'w\', \'d\'), (\'x\', \'a\'), (\'y\', \'b\'), (\'z\', \'c\'))), (\'q\', ((\'w\', True), (\'x\', False), (\'y\', True), (\'z\', False))), (\'r\', ((\'w\', True), (\'x\', False), (\'y\', False), (\'z\', False))), (\'s\', ((\'w\', 30), (\'x\', 2), (\'y\', 30), (\'z\', 2))), (\'t\', ((\'w\', 73), (\'x\', 2), (\'y\', 34), (\'z\', 95))))\n                )\n\n        self.assertEqual(\n                f1.roll(-3, 3, include_index=True, include_columns=True).to_pairs(0),\n                ((\'r\', ((\'z\', \'d\'), (\'w\', \'a\'), (\'x\', \'b\'), (\'y\', \'c\'))), (\'s\', ((\'z\', True), (\'w\', False), (\'x\', True), (\'y\', False))), (\'t\', ((\'z\', True), (\'w\', False), (\'x\', False), (\'y\', False))), (\'p\', ((\'z\', 30), (\'w\', 2), (\'x\', 30), (\'y\', 2))), (\'q\', ((\'z\', 73), (\'w\', 2), (\'x\', 34), (\'y\', 95)))))\n\n\n    def test_frame_shift_a(self) -> None:\n\n\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                (30, 73, \'d\', True, True),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'w\', \'x\', \'y\', \'z\'))\n\n        dtype = np.dtype\n\n        # nan as default forces floats and objects\n        self.assertEqual(f1.shift(2).dtypes.values.tolist(),\n                [dtype(\'float64\'), dtype(\'float64\'), dtype(\'O\'), dtype(\'O\'), dtype(\'O\')])\n\n        self.assertEqual(f1.shift(1, fill_value=-1).to_pairs(0),\n                ((\'p\', ((\'w\', -1), (\'x\', 2), (\'y\', 30), (\'z\', 2))), (\'q\', ((\'w\', -1), (\'x\', 2), (\'y\', 34), (\'z\', 95))), (\'r\', ((\'w\', -1), (\'x\', \'a\'), (\'y\', \'b\'), (\'z\', \'c\'))), (\'s\', ((\'w\', -1), (\'x\', False), (\'y\', True), (\'z\', False))), (\'t\', ((\'w\', -1), (\'x\', False), (\'y\', False), (\'z\', False))))\n                )\n\n        self.assertEqual(f1.shift(1, 1, fill_value=-1).to_pairs(0),\n                ((\'p\', ((\'w\', -1), (\'x\', -1), (\'y\', -1), (\'z\', -1))), (\'q\', ((\'w\', -1), (\'x\', 2), (\'y\', 30), (\'z\', 2))), (\'r\', ((\'w\', -1), (\'x\', 2), (\'y\', 34), (\'z\', 95))), (\'s\', ((\'w\', -1), (\'x\', \'a\'), (\'y\', \'b\'), (\'z\', \'c\'))), (\'t\', ((\'w\', -1), (\'x\', False), (\'y\', True), (\'z\', False))))\n                )\n\n        self.assertEqual(f1.shift(0, 5, fill_value=-1).to_pairs(0),\n                ((\'p\', ((\'w\', -1), (\'x\', -1), (\'y\', -1), (\'z\', -1))), (\'q\', ((\'w\', -1), (\'x\', -1), (\'y\', -1), (\'z\', -1))), (\'r\', ((\'w\', -1), (\'x\', -1), (\'y\', -1), (\'z\', -1))), (\'s\', ((\'w\', -1), (\'x\', -1), (\'y\', -1), (\'z\', -1))), (\'t\', ((\'w\', -1), (\'x\', -1), (\'y\', -1), (\'z\', -1))))\n                )\n\n\n    def test_frame_name_a(self) -> None:\n\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\', \'y\', \'z\'),\n                name=\'test\')\n\n        self.assertEqual(f1.name, \'test\')\n\n        f2 = f1.rename(\'alt\')\n\n        self.assertEqual(f1.name, \'test\')\n        self.assertEqual(f2.name, \'alt\')\n\n\n\n    def test_frame_name_b(self) -> None:\n\n        with self.assertRaises(TypeError):\n            f = Frame.from_dict(dict(a=(1,2), b=(3,4)), name=[\'test\'])\n\n        with self.assertRaises(TypeError):\n            f = Frame.from_dict(dict(a=(1,2), b=(3,4)), name={\'a\': 30})\n\n        with self.assertRaises(TypeError):\n            f = Frame.from_dict(dict(a=(1,2), b=(3,4)), name=(\'a\', [1, 2]))\n\n\n\n    def test_frame_name_c(self) -> None:\n\n        records = (\n                (2, 2, \'a\', False, False),\n                (30, 34, \'b\', True, False),\n                (2, 95, \'c\', False, False),\n                )\n        f1 = FrameGO.from_records(records,\n                columns=(\'p\', \'q\', \'r\', \'s\', \'t\'),\n                index=(\'x\', \'y\', \'z\'),\n                name=\'test\')\n\n        self.assertEqual(f1.name, \'test\')\n\n        f2 = f1.rename(\'alt\')\n\n        self.assertEqual(f1.name, \'test\')\n        self.assertEqual(f2.name, \'alt\')\n\n        f2[\'u\'] = -1\n\n        self.assertEqual(f1.columns.values.tolist(), [\'p\', \'q\', \'r\', \'s\', \'t\'])\n        self.assertEqual(f2.columns.values.tolist(), [\'p\', \'q\', \'r\', \'s\', \'t\', \'u\'])\n\n\n\n    @skip_win  # type: ignore\n    def test_frame_display_a(self) -> None:\n\n        f1 = Frame.from_records(((1,2),(True,False)), name=\'foo\',\n                index=Index((\'x\', \'y\'), name=\'bar\'),\n                columns=Index((\'a\', \'b\'), name=\'rig\')\n                )\n\n        d1 = f1.display(DisplayConfig(\n                type_color=False,\n                type_show=False,\n                include_columns=False))\n\n        match = tuple(f1.display(DisplayConfig(type_color=False)))\n\n        self.assertEqual(\n            match,\n            ([\'<Frame: foo>\'], [\'<Index: rig>\', \'a\', \'b\', \'<<U1>\'], [\'<Index: bar>\', \'\', \'\'], [\'x\', \'1\', \'2\'], [\'y\', \'1\', \'0\'], [\'<<U1>\', \'<int64>\', \'<int64>\'])\n            )\n\n\n\n    def test_frame_reindex_drop_level_a(self) -> None:\n\n        f1 = Frame.from_dict_records(\n                (dict(a=x, b=x) for x in range(4)),\n                index=sf.IndexHierarchy.from_labels([(1,1),(1,2),(2,3),(2,4)]))\n\n        with self.assertRaises(Exception):\n            # this results in an index of size 2 being created, as we dro the leves with a postive depth; next support negative depth?\n            f2 = f1.relabel_drop_level(index=-1)\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_clip_a(self) -> None:\n\n        records = (\n                (2, 2),\n                (30, 34),\n                (2, 95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        self.assertEqual(f1.clip(upper=0).to_pairs(0),\n                ((\'a\', ((\'x\', 0), (\'y\', 0), (\'z\', 0))), (\'b\', ((\'x\', 0), (\'y\', 0), (\'z\', 0)))))\n\n        self.assertEqual(f1.clip(lower=90).to_pairs(0),\n                ((\'a\', ((\'x\', 90), (\'y\', 90), (\'z\', 90))), (\'b\', ((\'x\', 90), (\'y\', 90), (\'z\', 95)))))\n\n\n    def test_frame_clip_b(self) -> None:\n\n        records = (\n                (2, 2),\n                (30, 34),\n                (2, 95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        s1 = Series((1, 20), index=(\'a\', \'b\'))\n\n        self.assertEqual(f1.clip(upper=s1, axis=1).to_pairs(0),\n            ((\'a\', ((\'x\', 1), (\'y\', 1), (\'z\', 1))), (\'b\', ((\'x\', 2), (\'y\', 20), (\'z\', 20)))))\n\n        s2 = Series((3, 33, 80), index=(\'x\', \'y\', \'z\'))\n\n        self.assertEqual(f1.clip(lower=s2, axis=0).to_pairs(0),\n            ((\'a\', ((\'x\', 3), (\'y\', 33), (\'z\', 80))), (\'b\', ((\'x\', 3), (\'y\', 34), (\'z\', 95)))))\n\n\n    def test_frame_clip_c(self) -> None:\n\n        records = (\n                (2, 2),\n                (30, 34),\n                (2, 95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        f2 = sf.Frame.from_records([[5, 4], [0, 10]],\n                index=list(\'yz\'),\n                columns=list(\'ab\'))\n\n        self.assertEqual(f1.clip(upper=f2).to_pairs(0),\n                ((\'a\', ((\'x\', 2.0), (\'y\', 5.0), (\'z\', 0.0))), (\'b\', ((\'x\', 2.0), (\'y\', 4.0), (\'z\', 10.0)))))\n\n\n    @unittest.skip(\'precedence of min/max changed in numpy 1.17.4\')\n    def test_frame_clip_d(self) -> None:\n\n        records = (\n                (2, 2),\n                (30, 34),\n                (2, 95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        f2 = sf.Frame([[5, 4], [0, 10]], index=list(\'yz\'), columns=list(\'ab\'))\n\n        self.assertEqual(f1.clip(lower=3, upper=f2).to_pairs(0),\n            ((\'a\', ((\'x\', 3.0), (\'y\', 5.0), (\'z\', 3.0))), (\'b\', ((\'x\', 3.0), (\'y\', 4.0), (\'z\', 10.0))))\n            )\n\n\n    def test_frame_clip_e(self) -> None:\n\n        records = (\n                (2, 2),\n                (30, 34),\n                (22, 95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n        f2 = f1.clip(lower=20, upper=31)\n        self.assertEqual(f2.to_pairs(0),\n                ((\'a\', ((\'x\', 20), (\'y\', 30), (\'z\', 22))), (\'b\', ((\'x\', 20), (\'y\', 31), (\'z\', 31)))))\n\n\n    def test_frame_clip_f(self) -> None:\n\n        records = (\n                (2, 2),\n                (30, 34),\n                (22, 95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n        f2 = f1.clip()\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'a\', ((\'x\', 2), (\'y\', 30), (\'z\', 22))), (\'b\', ((\'x\', 2), (\'y\', 34), (\'z\', 95))))\n                )\n\n\n    def test_frame_clip_g(self) -> None:\n\n        records = (\n                (2, 2),\n                (30, 34),\n                (22, 95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        with self.assertRaises(RuntimeError):\n            _ = f1.clip(upper=Series((1, 10), index=(\'b\', \'c\')))\n\n        with self.assertRaises(RuntimeError):\n            _ = f1.clip(upper=(3, 4))\n\n\n        f2 = f1.clip(upper=Series((1, 10), index=(\'b\', \'c\')), axis=1)\n        self.assertEqual(f2.to_pairs(0),\n                ((\'a\', ((\'x\', 2.0), (\'y\', 30.0), (\'z\', 22.0))), (\'b\', ((\'x\', 1.0), (\'y\', 1.0), (\'z\', 1.0)))))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_from_dict_a(self) -> None:\n\n        with self.assertRaises(RuntimeError):\n            # mismatched length\n            sf.Frame.from_dict(dict(a=(1,2,3,4,5), b=tuple(\'abcdef\')))\n\n\n\n    def test_frame_from_dict_b(self) -> None:\n\n        f = Frame.from_dict({(\'a\', 1): (1, 2), (\'a\', 2): (3, 4)}, columns_constructor=IndexHierarchy.from_labels)\n\n        self.assertEqual(f.columns.__class__, IndexHierarchy)\n        self.assertEqual(f.to_pairs(0),\n                (((\'a\', 1), ((0, 1), (1, 2))), ((\'a\', 2), ((0, 3), (1, 4))))\n                )\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_from_sql_a(self) -> None:\n\n        conn: sqlite3.Connection = self.get_test_db_a()\n\n        f1 = sf.Frame.from_sql(\'select * from events\', connection=conn)\n\n        # this might be different on windows\n        self.assertEqual([x.kind for x in f1.dtypes.values],\n                [\'U\', \'U\', \'f\', \'i\'])\n\n        self.assertEqual(f1.to_pairs(0),\n                ((\'date\', ((0, \'2006-01-01\'), (1, \'2006-01-02\'), (2, \'2006-01-01\'), (3, \'2006-01-02\'))), (\'identifier\', ((0, \'a1\'), (1, \'a1\'), (2, \'b2\'), (3, \'b2\'))), (\'value\', ((0, 12.5), (1, 12.5), (2, 12.5), (3, 12.5))), (\'count\', ((0, 8), (1, 8), (2, 8), (3, 8))))\n                )\n\n\n    def test_frame_from_records_items_a(self) -> None:\n\n        def gen() -> tp.Iterator[tp.Tuple[tp.Hashable, tp.Dict[tp.Hashable, tp.Any]]]:\n            for i in range(3):\n                yield f\'000{i}\', {\'squared\': i**2, \'cubed\': i**3}\n\n        f = Frame.from_dict_records_items(gen())\n\n        self.assertEqual(\n                f.to_pairs(0),\n                ((\'squared\', ((\'0000\', 0), (\'0001\', 1), (\'0002\', 4))), (\'cubed\', ((\'0000\', 0), (\'0001\', 1), (\'0002\', 8))))\n        )\n\n\n    def test_frame_from_records_items_b(self) -> None:\n\n        def gen() -> tp.Iterator[tp.Tuple[tp.Hashable, tp.Tuple[str, str]]]:\n            for i in range(3):\n                yield f\'000{i}\', (\'a\' * i, \'b\' * i)\n\n        f = Frame.from_records_items(gen())\n        self.assertEqual(\n                f.to_pairs(0),\n                ((0, ((\'0000\', \'\'), (\'0001\', \'a\'), (\'0002\', \'aa\'))), (1, ((\'0000\', \'\'), (\'0001\', \'b\'), (\'0002\', \'bb\'))))\n                )\n\n\n    def test_frame_loc_min_a(self) -> None:\n\n        records = (\n                (2, 2),\n                (30, 34),\n                (2, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        self.assertEqual(f1.loc_min().to_pairs(),\n                ((\'a\', \'x\'), (\'b\', \'z\')))\n\n        self.assertEqual(f1.loc_min(axis=1).to_pairs(),\n                ((\'x\', \'a\'), (\'y\', \'a\'), (\'z\', \'b\')))\n\n\n    def test_frame_loc_min_b(self) -> None:\n\n        records = (\n                (2, 2),\n                (np.nan, 34),\n                (2, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        with self.assertRaises(RuntimeError):\n            f1.loc_min(skipna=False)\n\n    def test_frame_iloc_min_a(self) -> None:\n\n        records = (\n                (2, 2),\n                (30, 34),\n                (2, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        self.assertEqual(f1.iloc_min().to_pairs(),\n                ((\'a\', 0), (\'b\', 2)))\n\n        self.assertEqual(f1.iloc_min(axis=1).to_pairs(),\n                ((\'x\', 0), (\'y\', 0), (\'z\', 1)))\n\n\n    def test_frame_iloc_min_b(self) -> None:\n\n        records = (\n                (2, 2),\n                (30, np.nan),\n                (2, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        self.assertAlmostEqualItems(\n                f1.iloc_min(skipna=False).to_pairs(),\n                ((\'a\', 0), (\'b\', np.nan)))\n\n        self.assertAlmostEqualItems(\n                f1.iloc_min(axis=1, skipna=False).to_pairs(),\n                ((\'x\', 0), (\'y\', np.nan), (\'z\', 1)))\n\n\n    def test_frame_loc_max_a(self) -> None:\n\n        records = (\n                (2000, 2),\n                (30, 34),\n                (2, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        self.assertEqual(f1.loc_max().to_pairs(),\n                ((\'a\', \'x\'), (\'b\', \'y\')))\n\n        self.assertEqual(f1.loc_max(axis=1).to_pairs(),\n                ((\'x\', \'a\'), (\'y\', \'b\'), (\'z\', \'a\')))\n\n    def test_frame_loc_max_b(self) -> None:\n\n        records = (\n                (2, 2),\n                (np.nan, 34),\n                (2, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        with self.assertRaises(RuntimeError):\n            f1.loc_max(skipna=False)\n\n    def test_frame_iloc_max_a(self) -> None:\n\n        records = (\n                (2000, 2),\n                (30, 34),\n                (2, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        self.assertEqual(f1.iloc_max().to_pairs(),\n                ((\'a\', 0), (\'b\', 1)))\n\n        self.assertEqual(f1.iloc_max(axis=1).to_pairs(),\n                ((\'x\', 0), (\'y\', 1), (\'z\', 0)))\n\n\n    def test_frame_bloc_a(self) -> None:\n\n        f1= Frame.from_dict(\n                dict(a=(3,2,1), b=(4,5,6)),\n                index=(\'x\', \'y\', \'z\'),\n                name=\'f2\')\n        f2 = Frame.from_dict(\n                dict(x=(1,2,-5,200), y=(3,4,-5,-3000)),\n                index=IndexHierarchy.from_product((\'I\', \'II\'), (\'a\', \'b\')),\n                name=\'f1\')\n        f3 = Frame.from_records(\n                ((10, 20, 50, 60), (50.0, 60.4, -50, -60)),\n                index=(\'p\', \'q\'),\n                columns=IndexHierarchy.from_product((\'I\', \'II\'), (\'a\', \'b\')),\n                name=\'f3\')\n\n        s1 = f1.bloc[(f1 <= 2) | (f1 > 4)]\n        self.assertEqual(s1.to_pairs(),\n                (((\'y\', \'a\'), 2), ((\'y\', \'b\'), 5), ((\'z\', \'a\'), 1), ((\'z\', \'b\'), 6))\n                )\n\n        s2 = f2.bloc[(f2 < 0)]\n        self.assertEqual(s2.to_pairs(),\n                ((((\'II\', \'a\'), \'x\'), -5), (((\'II\', \'a\'), \'y\'), -5), (((\'II\', \'b\'), \'y\'), -3000))\n                )\n\n        s3 = f3.bloc[f3 < 11]\n        self.assertEqual(s3.to_pairs(),\n                (((\'p\', (\'I\', \'a\')), 10), ((\'q\', (\'II\', \'a\')), -50), ((\'q\', (\'II\', \'b\')), -60))\n                )\n\n\n    def test_frame_bloc_b(self) -> None:\n\n        f = sf.Frame.from_records(\n                [[True, False], [False, True]],\n                index=(\'a\', \'b\'),\n                columns=[\'d\', \'c\'])\n        self.assertEqual(\n                f.assign.bloc[f](\'T\').assign.bloc[~f](\'\').to_pairs(0),\n                ((\'d\', ((\'a\', \'T\'), (\'b\', \'\'))), (\'c\', ((\'a\', \'\'), (\'b\', \'T\'))))\n                )\n\n    #---------------------------------------------------------------------------\n    def test_frame_unset_index_a(self) -> None:\n        records = (\n                (2, 2),\n                (30, 3),\n                (2, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n        self.assertEqual(f1.unset_index().to_pairs(0),\n                ((\'__index0__\', ((0, \'x\'), (1, \'y\'), (2, \'z\'))), (\'a\', ((0, 2), (1, 30), (2, 2))), (\'b\', ((0, 2.0), (1, 3), (2, -95.0))))\n                )\n\n\n    def test_frame_unset_index_b(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        columns = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        index = IndexHierarchy.from_product((100, 200), (True, False))\n        f1 = Frame.from_records(records,\n                columns=columns,\n                index=index)\n\n        with self.assertRaises(ErrorInitFrame):\n            f1.unset_index()\n\n\n    def test_frame_unset_index_c(self) -> None:\n        records = (\n                (1, 2, \'a\', False),\n                (30, 34, \'b\', True),\n                (54, 95, \'c\', False),\n                (65, 73, \'d\', True),\n                )\n        index = IndexHierarchy.from_product((100, 200), (True, False), name=(\'a\', \'b\'))\n        f1 = Frame.from_records(records,\n                index=index)\n        self.assertEqual(f1.unset_index().to_pairs(0),\n                ((\'a\', ((0, 100), (1, 100), (2, 200), (3, 200))), (\'b\', ((0, True), (1, False), (2, True), (3, False))), (0, ((0, 1), (1, 30), (2, 54), (3, 65))), (1, ((0, 2), (1, 34), (2, 95), (3, 73))), (2, ((0, \'a\'), (1, \'b\'), (2, \'c\'), (3, \'d\'))), (3, ((0, False), (1, True), (2, False), (3, True))))\n                )\n\n\n    def test_frame_unset_index_d(self) -> None:\n        records = (\n                (2, 2),\n                (30, 3),\n                (2, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n        f2 = f1.unset_index(names=(\'index\',))\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'index\', ((0, \'x\'), (1, \'y\'), (2, \'z\'))), (\'a\', ((0, 2), (1, 30), (2, 2))), (\'b\', ((0, 2), (1, 3), (2, -95))))\n                )\n\n\n    def test_frame_unset_index_e(self) -> None:\n        # using ILoc after unset led to an error because of no handling when loc is iloc\n        f1 = sf.Frame.from_records([\'a\', \'b\'], index=tuple((\'c\', \'d\')))\n        self.assertEqual(f1.loc[\'d\', 0], \'b\')\n        self.assertEqual(f1.loc[sf.ILoc[0], 0], \'a\')\n\n        f2 = f1.unset_index()\n        self.assertEqual(\n                f2.to_pairs(0),\n                ((\'__index0__\', ((0, \'c\'), (1, \'d\'))), (0, ((0, \'a\'), (1, \'b\'))))\n        )\n\n        self.assertEqual(f2.loc[0, 0], \'a\')\n        self.assertEqual(f2.loc[sf.ILoc[0], 0], \'a\')\n\n    @skip_win #type: ignore\n    def test_frame_unset_index_f(self) -> None:\n        records = (\n                (2, 2),\n                (30, 3),\n                (2, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                )\n        f2 = f1.unset_index(names=(\'index\',), consolidate_blocks=True)\n        self.assertEqual(f2._blocks.shapes.tolist(), [(3, 3)])\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_pivot_a(self) -> None:\n\n        index = IndexHierarchy.from_product(\n                (\'far\', \'near\'), (\'up\', \'down\'), (\'left\', \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        f1 = FrameGO(index=index)\n        f1[\'a\'] = range(len(f1))\n        f1[\'b\'] = (len(str(f1.index.values[i])) for i in range(len(f1)))\n\n        f2 = f1.unset_index()\n        post = f2.pivot(\n                index_fields=(\'x\', \'y\'), # values in this field become the index\n                columns_fields=(\'z\',), # values in this field become columns\n                data_fields=(\'a\', \'b\')\n                )\n\n        self.assertEqual(post.to_pairs(0),\n                (((\'far\', \'a\'), (((\'left\', \'down\'), 2), ((\'left\', \'up\'), 0), ((\'right\', \'down\'), 3), ((\'right\', \'up\'), 1))), ((\'far\', \'b\'), (((\'left\', \'down\'), 21), ((\'left\', \'up\'), 19), ((\'right\', \'down\'), 22), ((\'right\', \'up\'), 20))), ((\'near\', \'a\'), (((\'left\', \'down\'), 6), ((\'left\', \'up\'), 4), ((\'right\', \'down\'), 7), ((\'right\', \'up\'), 5))), ((\'near\', \'b\'), (((\'left\', \'down\'), 22), ((\'left\', \'up\'), 20), ((\'right\', \'down\'), 23), ((\'right\', \'up\'), 21))))\n                )\n\n\n    def test_frame_pivot_b(self) -> None:\n\n        index = IndexHierarchy.from_product(\n                (\'far\', \'near\'), (\'up\', \'down\'), (\'left\', \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        f1 = FrameGO(index=index)\n        f1[\'a\'] = range(len(f1))\n        f1[\'b\'] = (len(str(f1.index.values[i])) for i in range(len(f1)))\n\n        f2 = f1.unset_index()\n        post = f2.pivot(\n                index_fields=(\'x\', \'y\'), # values in this field become the index\n                columns_fields=(\'z\',), # values in this field become columns\n                data_fields=(\'a\',)\n                )\n\n        self.assertEqual(post.to_pairs(0),\n                ((\'far\', (((\'left\', \'down\'), 2), ((\'left\', \'up\'), 0), ((\'right\', \'down\'), 3), ((\'right\', \'up\'), 1))), (\'near\', (((\'left\', \'down\'), 6), ((\'left\', \'up\'), 4), ((\'right\', \'down\'), 7), ((\'right\', \'up\'), 5))))\n                )\n\n\n    def test_frame_pivot_c(self) -> None:\n\n        index = IndexHierarchy.from_product(\n                (\'far\', \'near\'), (\'up\', \'down\'), (\'left\', \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        f1 = FrameGO(index=index)\n        f1[\'a\'] = range(len(f1))\n        f1[\'b\'] = (len(str(f1.index.values[i])) for i in range(len(f1)))\n\n        f2 = f1.unset_index()\n        post = f2.pivot(\n                index_fields=(\'x\', \'y\'), # values in this field become the index\n                data_fields=(\'b\',)\n                )\n\n        self.assertEqual(post.to_pairs(0),\n                ((\'b\', (((\'left\', \'down\'), 43), ((\'left\', \'up\'), 39), ((\'right\', \'down\'), 45), ((\'right\', \'up\'), 41))),)\n                )\n\n\n    def test_frame_pivot_d(self) -> None:\n\n        index = IndexHierarchy.from_product(\n                (\'far\', \'near\'), (\'up\', \'down\'), (\'left\', \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        f1 = FrameGO(index=index)\n        f1[\'a\'] = range(len(f1))\n        f1[\'b\'] = (len(str(f1.index.values[i])) for i in range(len(f1)))\n\n        f2 = f1.unset_index()\n\n        # single argument specs\n        self.assertEqual(f2.pivot(\'z\', \'x\', \'a\').to_pairs(0),\n                ((\'left\', ((\'far\', 2), (\'near\', 10))), (\'right\', ((\'far\', 4), (\'near\', 12))))\n                )\n\n        self.assertEqual(f2.pivot(\'z\', \'x\', \'b\').to_pairs(0),\n                ((\'left\', ((\'far\', 40), (\'near\', 42))), (\'right\', ((\'far\', 42), (\'near\', 44))))\n                )\n\n        self.assertEqual(f2.pivot(\'x\', \'y\', \'a\').to_pairs(0),\n                ((\'down\', ((\'left\', 8), (\'right\', 10))), (\'up\', ((\'left\', 4), (\'right\', 6))))\n                )\n\n        self.assertEqual(f2.pivot(\'x\', \'y\', \'b\').to_pairs(0),\n                ((\'down\', ((\'left\', 43), (\'right\', 45))), (\'up\', ((\'left\', 39), (\'right\', 41))))\n                )\n\n    def test_frame_pivot_e(self) -> None:\n\n        index = IndexHierarchy.from_product(\n                (\'far\', \'near\'), (\'up\', \'down\'), (\'left\', \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        f1 = FrameGO(index=index)\n        f1[\'a\'] = range(len(f1))\n        f1[\'b\'] = (len(str(f1.index.values[i])) for i in range(len(f1)))\n\n        f2 = f1.unset_index()\n\n        # no columns provided\n\n        self.assertEqual(\n                f2.pivot(\'z\', data_fields=\'b\').to_pairs(0),\n                ((\'b\', ((\'far\', 82), (\'near\', 86))),)\n                )\n\n        self.assertEqual(\n                f2.pivot(\'z\', data_fields=(\'a\', \'b\')).to_pairs(0),\n                ((\'a\', ((\'far\', 6), (\'near\', 22))), (\'b\', ((\'far\', 82), (\'near\', 86))))\n                )\n\n\n    def test_frame_pivot_f(self) -> None:\n\n        index = IndexHierarchy.from_product(\n                (\'far\', \'near\'), (\'up\', \'down\'), (\'left\', \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        f1 = FrameGO(index=index)\n        f1[\'a\'] = range(len(f1))\n        f1[\'b\'] = (len(str(f1.index.values[i])) for i in range(len(f1)))\n\n        f2 = f1.unset_index()\n\n        # shows unique values of \'b\' as columns, then shows values for z, a\n        post = f2.pivot((\'x\', \'y\'), (\'b\',), fill_value=\'\')\n\n        self.assertEqual(post.to_pairs(0),\n                (((19, \'z\'), (((\'left\', \'down\'), \'\'), ((\'left\', \'up\'), \'far\'), ((\'right\', \'down\'), \'\'), ((\'right\', \'up\'), \'\'))), ((19, \'a\'), (((\'left\', \'down\'), \'\'), ((\'left\', \'up\'), 0), ((\'right\', \'down\'), \'\'), ((\'right\', \'up\'), \'\'))), ((20, \'z\'), (((\'left\', \'down\'), \'\'), ((\'left\', \'up\'), \'near\'), ((\'right\', \'down\'), \'\'), ((\'right\', \'up\'), \'far\'))), ((20, \'a\'), (((\'left\', \'down\'), \'\'), ((\'left\', \'up\'), 4), ((\'right\', \'down\'), \'\'), ((\'right\', \'up\'), 1))), ((21, \'z\'), (((\'left\', \'down\'), \'far\'), ((\'left\', \'up\'), \'\'), ((\'right\', \'down\'), \'\'), ((\'right\', \'up\'), \'near\'))), ((21, \'a\'), (((\'left\', \'down\'), 2), ((\'left\', \'up\'), \'\'), ((\'right\', \'down\'), \'\'), ((\'right\', \'up\'), 5))), ((22, \'z\'), (((\'left\', \'down\'), \'near\'), ((\'left\', \'up\'), \'\'), ((\'right\', \'down\'), \'far\'), ((\'right\', \'up\'), \'\'))), ((22, \'a\'), (((\'left\', \'down\'), 6), ((\'left\', \'up\'), \'\'), ((\'right\', \'down\'), 3), ((\'right\', \'up\'), \'\'))), ((23, \'z\'), (((\'left\', \'down\'), \'\'), ((\'left\', \'up\'), \'\'), ((\'right\', \'down\'), \'near\'), ((\'right\', \'up\'), \'\'))), ((23, \'a\'), (((\'left\', \'down\'), \'\'), ((\'left\', \'up\'), \'\'), ((\'right\', \'down\'), 7), ((\'right\', \'up\'), \'\'))))\n                )\n\n    def test_frame_pivot_g(self) -> None:\n\n        index = IndexHierarchy.from_product(\n                (\'far\', \'near\'), (\'up\', \'down\'), (\'left\', \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        f1 = FrameGO(index=index)\n        f1[\'a\'] = range(len(f1))\n        f1[\'b\'] = (len(str(f1.index.values[i])) for i in range(len(f1)))\n\n        f2 = f1.unset_index()\n\n        # multiple columns; all reamining fields go to data_fields\n        post1 = f2.pivot(\'z\', (\'y\', \'x\'))\n\n        self.assertEqual(post1.to_pairs(0),\n                (((\'down\', \'left\', \'a\'), ((\'far\', 2), (\'near\', 6))), ((\'down\', \'left\', \'b\'), ((\'far\', 21), (\'near\', 22))), ((\'down\', \'right\', \'a\'), ((\'far\', 3), (\'near\', 7))), ((\'down\', \'right\', \'b\'), ((\'far\', 22), (\'near\', 23))), ((\'up\', \'left\', \'a\'), ((\'far\', 0), (\'near\', 4))), ((\'up\', \'left\', \'b\'), ((\'far\', 19), (\'near\', 20))), ((\'up\', \'right\', \'a\'), ((\'far\', 1), (\'near\', 5))), ((\'up\', \'right\', \'b\'), ((\'far\', 20), (\'near\', 21))))\n                )\n\n\n    def test_frame_pivot_h(self) -> None:\n\n        index = IndexHierarchy.from_product(\n                (\'far\', \'near\'), (\'up\', \'down\'), (\'left\', \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        f1 = FrameGO(index=index)\n        f1[\'a\'] = range(len(f1))\n        f1[\'b\'] = (len(str(f1.index.values[i])) for i in range(len(f1)))\n\n        f2 = f1.unset_index()\n\n\n        # specifying a data_fields value\n        post1 = f2.pivot(\'z\', (\'y\', \'x\'), \'a\')\n\n        self.assertEqual(post1.to_pairs(0),\n                (((\'down\', \'left\'), ((\'far\', 2), (\'near\', 6))), ((\'down\', \'right\'), ((\'far\', 3), (\'near\', 7))), ((\'up\', \'left\'), ((\'far\', 0), (\'near\', 4))), ((\'up\', \'right\'), ((\'far\', 1), (\'near\', 5)))))\n\n\n\n    def test_frame_pivot_i(self) -> None:\n\n        index = IndexHierarchy.from_product(\n                (\'far\', \'near\'), (\'up\', \'down\'), (\'left\', \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        f1 = FrameGO(index=index)\n        f1[\'a\'] = range(len(f1))\n        f1[\'b\'] = (len(str(f1.index.values[i])) for i in range(len(f1)))\n\n        f2 = f1.unset_index()\n\n        post1 = f2.pivot(\'z\', \'y\', \'a\', func={\'min\': np.min, \'max\': np.max})\n\n        self.assertEqual(post1.to_pairs(0),\n                (((\'down\', \'min\'), ((\'far\', 2), (\'near\', 6))), ((\'down\', \'max\'), ((\'far\', 3), (\'near\', 7))), ((\'up\', \'min\'), ((\'far\', 0), (\'near\', 4))), ((\'up\', \'max\'), ((\'far\', 1), (\'near\', 5))))\n                )\n\n\n    def test_frame_pivot_j(self) -> None:\n\n        index = IndexHierarchy.from_product(\n                (\'far\', \'near\'), (\'up\', \'down\'), (\'left\', \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        f1 = FrameGO(index=index)\n        f1[\'a\'] = range(len(f1))\n        f1[\'b\'] = (len(str(f1.index.values[i])) for i in range(len(f1)))\n\n        f2 = f1.unset_index()\n\n        post1 = f2.pivot(\'z\', (\'y\', \'x\'), \'b\', func={\'min\': np.min, \'max\': np.max})\n        self.assertEqual(\n                post1.to_pairs(0),\n                (((\'down\', \'left\', \'min\'), ((\'far\', 21), (\'near\', 22))), ((\'down\', \'left\', \'max\'), ((\'far\', 21), (\'near\', 22))), ((\'down\', \'right\', \'min\'), ((\'far\', 22), (\'near\', 23))), ((\'down\', \'right\', \'max\'), ((\'far\', 22), (\'near\', 23))), ((\'up\', \'left\', \'min\'), ((\'far\', 19), (\'near\', 20))), ((\'up\', \'left\', \'max\'), ((\'far\', 19), (\'near\', 20))), ((\'up\', \'right\', \'min\'), ((\'far\', 20), (\'near\', 21))), ((\'up\', \'right\', \'max\'), ((\'far\', 20), (\'near\', 21))))\n                )\n\n        # default populates data values for a, b\n        post2 = f2.pivot(\'z\', (\'y\', \'x\'), func={\'min\': np.min, \'max\': np.max})\n        self.assertEqual(\n                post2.to_pairs(0),\n                (((\'down\', \'left\', \'a\', \'min\'), ((\'far\', 2), (\'near\', 6))), ((\'down\', \'left\', \'a\', \'max\'), ((\'far\', 2), (\'near\', 6))), ((\'down\', \'left\', \'b\', \'min\'), ((\'far\', 21), (\'near\', 22))), ((\'down\', \'left\', \'b\', \'max\'), ((\'far\', 21), (\'near\', 22))), ((\'down\', \'right\', \'a\', \'min\'), ((\'far\', 3), (\'near\', 7))), ((\'down\', \'right\', \'a\', \'max\'), ((\'far\', 3), (\'near\', 7))), ((\'down\', \'right\', \'b\', \'min\'), ((\'far\', 22), (\'near\', 23))), ((\'down\', \'right\', \'b\', \'max\'), ((\'far\', 22), (\'near\', 23))), ((\'up\', \'left\', \'a\', \'min\'), ((\'far\', 0), (\'near\', 4))), ((\'up\', \'left\', \'a\', \'max\'), ((\'far\', 0), (\'near\', 4))), ((\'up\', \'left\', \'b\', \'min\'), ((\'far\', 19), (\'near\', 20))), ((\'up\', \'left\', \'b\', \'max\'), ((\'far\', 19), (\'near\', 20))), ((\'up\', \'right\', \'a\', \'min\'), ((\'far\', 1), (\'near\', 5))), ((\'up\', \'right\', \'a\', \'max\'), ((\'far\', 1), (\'near\', 5))), ((\'up\', \'right\', \'b\', \'min\'), ((\'far\', 20), (\'near\', 21))), ((\'up\', \'right\', \'b\', \'max\'), ((\'far\', 20), (\'near\', 21))))\n                )\n\n\n    def test_frame_pivot_k(self) -> None:\n\n        index = IndexHierarchy.from_product(\n                (\'far\', 20), (None, \'down\'), (False, \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        f1 = FrameGO(index=index)\n        f1[\'a\'] = range(len(f1))\n        f1[\'b\'] = (len(str(f1.index.values[i])) for i in range(len(f1)))\n\n        f2 = f1.unset_index()\n        post1 = f2.pivot(\'z\', \'y\', \'a\')\n\n        self.assertEqual(post1.to_pairs(0),\n                ((None, ((\'far\', 1), (20, 9))), (\'down\', ((\'far\', 5), (20, 13))))\n                )\n\n\n\n    def test_frame_pivot_m(self) -> None:\n\n        index = IndexHierarchy.from_product(\n                (\'far\', 20), (None, \'down\'), (False, \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        f1 = FrameGO(index=index)\n        f1[\'a\'] = range(len(f1))\n\n        f2 = f1.unset_index()\n        post1 = f2.pivot(\'z\', \'y\', \'a\', func=np.sum)\n        self.assertEqual(post1.to_pairs(0),\n            ((None, ((\'far\', 1), (20, 9))), (\'down\', ((\'far\', 5), (20, 13))))\n            )\n\n        with self.assertRaises(ErrorInitFrame):\n            # no fields remain to populate data.\n            _ = f2[[\'z\', \'y\']].pivot(\'z\', \'y\')\n\n        with self.assertRaises(ErrorInitFrame):\n            # cannot create a pivot Frame from a field (q) that is not a column\n            _ = f2.pivot(\'q\')\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_axis_window_items_a(self) -> None:\n\n        base = np.array([1, 2, 3, 4])\n        records = (base * n for n in range(1, 21))\n\n        f1 = Frame.from_records(records,\n                columns=list(\'ABCD\'),\n                index=self.get_letters(20))\n\n        post0 = tuple(f1._axis_window_items(size=2, axis=0))\n        self.assertEqual(len(post0), 19)\n        self.assertEqual(post0[0][0], \'b\')\n        self.assertEqual(post0[0][1].__class__, Frame)\n        self.assertEqual(post0[0][1].shape, (2, 4))\n\n        self.assertEqual(post0[-1][0], \'t\')\n        self.assertEqual(post0[-1][1].__class__, Frame)\n        self.assertEqual(post0[-1][1].shape, (2, 4))\n\n        post1 = tuple(f1._axis_window_items(size=2, axis=1))\n        self.assertEqual(len(post1), 3)\n\n        self.assertEqual(post1[0][0], \'B\')\n        self.assertEqual(post1[0][1].__class__, Frame)\n        self.assertEqual(post1[0][1].shape, (20, 2))\n\n        self.assertEqual(post1[-1][0], \'D\')\n        self.assertEqual(post1[-1][1].__class__, Frame)\n        self.assertEqual(post1[-1][1].shape, (20, 2))\n\n\n\n    def test_frame_axis_window_items_b(self) -> None:\n\n        base = np.array([1, 2, 3, 4])\n        records = (base * n for n in range(1, 21))\n\n        f1 = Frame.from_records(records,\n                columns=list(\'ABCD\'),\n                index=self.get_letters(20))\n\n        post0 = tuple(f1._axis_window_items(size=2, axis=0, as_array=True))\n        self.assertEqual(len(post0), 19)\n        self.assertEqual(post0[0][0], \'b\')\n        self.assertEqual(post0[0][1].__class__, np.ndarray)\n        self.assertEqual(post0[0][1].shape, (2, 4))\n\n        self.assertEqual(post0[-1][0], \'t\')\n        self.assertEqual(post0[-1][1].__class__, np.ndarray)\n        self.assertEqual(post0[-1][1].shape, (2, 4))\n\n        post1 = tuple(f1._axis_window_items(size=2, axis=1, as_array=True))\n        self.assertEqual(len(post1), 3)\n\n        self.assertEqual(post1[0][0], \'B\')\n        self.assertEqual(post1[0][1].__class__, np.ndarray)\n        self.assertEqual(post1[0][1].shape, (20, 2))\n\n        self.assertEqual(post1[-1][0], \'D\')\n        self.assertEqual(post1[-1][1].__class__, np.ndarray)\n        self.assertEqual(post1[-1][1].shape, (20, 2))\n\n\n\n    def test_frame_iter_window_a(self) -> None:\n\n        base = np.array([1, 2, 3, 4])\n        records = (base * n for n in range(1, 21))\n\n        f1 = Frame.from_records(records,\n                columns=list(\'ABCD\'),\n                index=self.get_letters(20))\n\n        self.assertEqual(\n                f1.iter_window(size=3).apply(lambda f: f[\'B\'].sum()).to_pairs(),\n                ((\'c\', 12), (\'d\', 18), (\'e\', 24), (\'f\', 30), (\'g\', 36), (\'h\', 42), (\'i\', 48), (\'j\', 54), (\'k\', 60), (\'l\', 66), (\'m\', 72), (\'n\', 78), (\'o\', 84), (\'p\', 90), (\'q\', 96), (\'r\', 102), (\'s\', 108), (\'t\', 114))\n        )\n\n\n    def test_frame_bool_a(self) -> None:\n        records = (\n                (2, 2),\n                (30, 3),\n                (2, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n        self.assertTrue(bool(f1))\n        self.assertTrue(bool(f1.T))\n\n\n\n    def test_frame_bool_b(self) -> None:\n        f1 = Frame(columns=(\'a\', \'b\'))\n\n        self.assertFalse(bool(f1))\n        self.assertFalse(bool(f1.T))\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_frame_assign_a(self) -> None:\n\n        f1 = Frame(columns=(\'a\', \'b\'))\n        with self.assertRaises(RuntimeError):\n            # can only set one at at ime\n            fa0 = FrameAssign(f1, iloc_key=(0, 0), bloc_key=f1)\n\n        fa1 = FrameAssign(f1, iloc_key=(0, 0), bloc_key=None)\n        fa2 = FrameAssign(f1, iloc_key=None, bloc_key=f1)\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_any_a(self) -> None:\n        records = (\n                (2, 2),\n                (30, 0),\n                (2, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        self.assertEqual(f1.all().to_pairs(),\n                ((\'a\', True), (\'b\', False)))\n        self.assertEqual(f1.any().to_pairs(),\n                ((\'a\', True), (\'b\', True)))\n\n        self.assertEqual(f1.all(axis=1).to_pairs(),\n                ((\'x\', True), (\'y\', False), (\'z\', True)))\n        self.assertEqual(f1.any(axis=1).to_pairs(),\n                ((\'x\', True), (\'y\', True), (\'z\', True)))\n\n\n    def test_frame_any_b(self) -> None:\n        records = (\n                (2, 2),\n                (np.nan, 0),\n                (np.nan, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        self.assertEqual(f1.all().to_pairs(),\n                ((\'a\', True), (\'b\', False)))\n        self.assertEqual(f1.any().to_pairs(),\n                ((\'a\', True), (\'b\', True)))\n\n        self.assertEqual(f1.all(axis=1).to_pairs(),\n                ((\'x\', True), (\'y\', False), (\'z\', True)))\n        self.assertEqual(f1.any(axis=1).to_pairs(),\n                ((\'x\', True), (\'y\', False), (\'z\', True)))\n\n\n\n    def test_frame_any_c(self) -> None:\n        records = (\n                (2, 2),\n                (np.nan, 0),\n                (np.nan, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        with self.assertRaises(TypeError):\n            self.assertEqual(f1.all(skipna=False).to_pairs(),\n                    ((\'a\', True), (\'b\', False)))\n\n        with self.assertRaises(TypeError):\n            self.assertEqual(f1.any(skipna=False).to_pairs(),\n                    ((\'a\', True), (\'b\', True)))\n\n        with self.assertRaises(TypeError):\n            self.assertEqual(f1.all(axis=1, skipna=False).to_pairs(),\n                    ((\'x\', True), (\'y\', False), (\'z\', True)))\n\n        with self.assertRaises(TypeError):\n            self.assertEqual(f1.any(axis=1, skipna=False).to_pairs(),\n                    ((\'x\', True), (\'y\', False), (\'z\', True)))\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_round_a(self) -> None:\n        a1 = np.full(4, .33333, )\n        a2 = np.full((4, 2), .88888, )\n        a3 = np.full(4, .55555)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        f1 = Frame(tb1)\n        f2 = round(f1) #type: ignore\n\n        self.assertEqual(f2.to_pairs(0),\n                ((0, ((0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0))), (1, ((0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0))), (2, ((0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0))), (3, ((0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0))))\n                )\n\n        f3 = round(f1, 2) #type: ignore\n        self.assertEqual(f3.to_pairs(0),\n                ((0, ((0, 0.33), (1, 0.33), (2, 0.33), (3, 0.33))), (1, ((0, 0.89), (1, 0.89), (2, 0.89), (3, 0.89))), (2, ((0, 0.89), (1, 0.89), (2, 0.89), (3, 0.89))), (3, ((0, 0.56), (1, 0.56), (2, 0.56), (3, 0.56))))\n                )\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_str_capitalize_a(self) -> None:\n\n        f1 = Frame(np.array([[\'foo\', \'bar\'], [\'baz\', \'baz\']]),\n                index=(\'a\', \'b\'),\n                columns=(\'x\', \'y\')\n                )\n        f2 = f1.via_str.capitalize()\n\n        self.assertEqual(f2.to_pairs(0),\n            ((\'x\', ((\'a\', \'Foo\'), (\'b\', \'Baz\'))), (\'y\', ((\'a\', \'Bar\'), (\'b\', \'Baz\'))))\n            )\n\n    def test_frame_str_center_a(self) -> None:\n\n        f1 = Frame.from_records(\n                [[\'p\', 0, True, \'foo\'], [\'q\', 20, None, \'bar\']],\n                index=(\'a\', \'b\'),\n                columns=(\'w\', \'x\', \'y\', \'z\')\n                )\n\n        self.assertEqual(f1.via_str.center(8, \'-\').to_pairs(0),\n                ((\'w\', ((\'a\', \'---p----\'), (\'b\', \'---q----\'))), (\'x\', ((\'a\', \'---0----\'), (\'b\', \'---20---\'))), (\'y\', ((\'a\', \'--True--\'), (\'b\', \'--None--\'))), (\'z\', ((\'a\', \'--foo---\'), (\'b\', \'--bar---\')))))\n\n\n    def test_frame_str_partition_a(self) -> None:\n\n        f1 = Frame(np.array([[\'aoc\', \'bar\'], [\'baz\', \'baq\']]),\n                index=(\'a\', \'b\'),\n                columns=(\'x\', \'y\')\n                )\n        f2 = f1.via_str.partition(\'a\')\n        self.assertEqual(f2.to_pairs(0),\n                ((\'x\', ((\'a\', (\'\', \'a\', \'oc\')), (\'b\', (\'b\', \'a\', \'z\')))), (\'y\', ((\'a\', (\'b\', \'a\', \'r\')), (\'b\', (\'b\', \'a\', \'q\'))))))\n\n    def test_frame_str_islower_a(self) -> None:\n\n        f1 = Frame(np.array([[\'aoc\', \'BAR\'], [\'baz\', \'BAQ\']]),\n                index=(\'a\', \'b\'),\n                columns=(\'x\', \'y\')\n                )\n        f2 = f1.via_str.islower()\n        self.assertEqual(f2.to_pairs(0),\n                ((\'x\', ((\'a\', True), (\'b\', True))), (\'y\', ((\'a\', False), (\'b\', False)))))\n\n    def test_frame_str_count_a(self) -> None:\n\n        f1 = Frame(np.array([[\'aoc\', \'BAR\'], [\'baz\', \'BAQ\']]),\n                index=(\'a\', \'b\'),\n                columns=(\'x\', \'y\')\n                )\n        f2 = f1.via_str.count(\'BA\')\n        self.assertEqual(f2.to_pairs(0), ((\'x\', ((\'a\', 0), (\'b\', 0))), (\'y\', ((\'a\', 1), (\'b\', 1)))))\n\n    #---------------------------------------------------------------------------\n    def test_frame_as_dt_year_a(self) -> None:\n\n        dt64 = np.datetime64\n\n        f1 = Frame.from_records(\n                [[\'2012\', datetime.date(2012,4,5), dt64(\'2020-05\')],\n                [\'2013\', datetime.date(2014,1,1), dt64(\'1919-03\')]],\n                index=(\'a\', \'b\'),\n                columns=(\'w\', \'x\', \'y\')\n                )\n\n        with self.assertRaises(RuntimeError):\n            f2 = f1.via_dt.year\n\n        f3 = f1[\'x\':].via_dt.year #type: ignore\n        self.assertEqual(\n                f3.to_pairs(0),\n                ((\'x\', ((\'a\', 2012), (\'b\', 2014))), (\'y\', ((\'a\', 2020), (\'b\', 1919))))\n                )\n\n\n    def test_frame_as_dt_month_b(self) -> None:\n\n        dt64 = np.datetime64\n\n        f1 = Frame.from_records(\n                [[datetime.date(2012,4,5),\n                datetime.date(2012,4,2),\n                dt64(\'2020-05-03T20:30\'),\n                dt64(\'2017-05-02T05:55\')\n                ],\n                [datetime.date(2014,1,1),\n                datetime.date(2012,4,1),\n                dt64(\'2020-01-03T20:30\'),\n                dt64(\'2025-03-02T03:20\')\n                ]],\n                index=(\'a\', \'b\'),\n                columns=(\'w\', \'x\', \'y\', \'z\'),\n                consolidate_blocks=True\n                )\n\n        f2 = f1.via_dt.month\n        self.assertEqual(f2.to_pairs(0),\n                ((\'w\', ((\'a\', 4), (\'b\', 1))), (\'x\', ((\'a\', 4), (\'b\', 4))), (\'y\', ((\'a\', 5), (\'b\', 1))), (\'z\', ((\'a\', 5), (\'b\', 3))))\n                )\n\n\n    def test_frame_as_dt_weekday_a(self) -> None:\n\n        dt64 = np.datetime64\n\n        f1 = Frame.from_records(\n                [[\'2012\', datetime.date(2012,4,5), dt64(\'2020-05-03\')],\n                [\'2013\', datetime.date(2014,1,1), dt64(\'1919-03-02\')]],\n                index=(\'a\', \'b\'),\n                columns=(\'w\', \'x\', \'y\')\n                )\n\n        with self.assertRaises(RuntimeError):\n            f2 = f1.via_dt.weekday()\n\n        self.assertEqual(\n                f1[\'x\':].via_dt.weekday().to_pairs(0), #type: ignore\n                ((\'x\', ((\'a\', 3), (\'b\', 2))), (\'y\', ((\'a\', 6), (\'b\', 6))))\n                )\n\n\n    def test_frame_as_dt_weekday_b(self) -> None:\n\n        dt64 = np.datetime64\n\n        f1 = Frame.from_records(\n                [[\'2012\', datetime.date(2012,4,5), datetime.date(2012,4,2)],\n                [\'2013\', datetime.date(2014,1,1), datetime.date(2012,4,1)]],\n                index=(\'a\', \'b\'),\n                columns=(\'w\', \'x\', \'y\'),\n                consolidate_blocks=True\n                )\n\n        self.assertEqual(\n                f1[\'x\':].via_dt.weekday().to_pairs(0), #type: ignore\n                ((\'x\', ((\'a\', 3), (\'b\', 2))), (\'y\', ((\'a\', 0), (\'b\', 6))))\n                )\n\n    def test_frame_as_dt_day_a(self) -> None:\n\n        dt64 = np.datetime64\n\n        f1 = Frame.from_records(\n                [[datetime.date(2012,4,5),\n                datetime.date(2012,4,2),\n                dt64(\'2020-05-03T20:30\'),\n                dt64(\'2017-05-02T05:55\')\n                ],\n                [datetime.date(2014,1,1),\n                datetime.date(2012,4,1),\n                dt64(\'2020-01-03T20:30\'),\n                dt64(\'2025-03-02T03:20\')\n                ]],\n                index=(\'a\', \'b\'),\n                columns=(\'w\', \'x\', \'y\', \'z\'),\n                consolidate_blocks=True\n                )\n\n        f2 = f1.via_dt.day\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'w\', ((\'a\', 5), (\'b\', 1))), (\'x\', ((\'a\', 2), (\'b\', 1))), (\'y\', ((\'a\', 3), (\'b\', 3))), (\'z\', ((\'a\', 2), (\'b\', 2))))\n                )\n\n    def test_frame_as_dt_timetuple_a(self) -> None:\n\n        dt64 = np.datetime64\n\n        f1 = Frame.from_records(\n                [[datetime.date(2012,4,5),\n                datetime.date(2012,4,2),\n                dt64(\'2020-05-03T20:30\'),\n                dt64(\'2017-05-02T05:55\')\n                ],\n                [datetime.date(2014,1,1),\n                datetime.date(2012,4,1),\n                dt64(\'2020-01-03T20:30\'),\n                dt64(\'2025-03-02T03:20\')\n                ]],\n                index=(\'a\', \'b\'),\n                columns=(\'w\', \'x\', \'y\', \'z\'),\n                consolidate_blocks=True\n                )\n\n        import time\n        tots = lambda *args: time.struct_time(args)\n\n        self.assertEqual(f1.via_dt.timetuple().values.tolist(),\n                [[tots(2012, 4, 5, 0, 0, 0, 3, 96, -1), tots(2012, 4, 2, 0, 0, 0, 0, 93, -1), tots(2020, 5, 3, 20, 30, 0, 6, 124, -1), tots(2017, 5, 2, 5, 55, 0, 1, 122, -1)], [tots(2014, 1, 1, 0, 0, 0, 2, 1, -1), tots(2012, 4, 1, 0, 0, 0, 6, 92, -1), tots(2020, 1, 3, 20, 30, 0, 4, 3, -1), tots(2025, 3, 2, 3, 20, 0, 6, 61, -1)]] #type: ignore\n                )\n\n\n    def test_frame_as_dt_strftime_a(self) -> None:\n\n        dt64 = np.datetime64\n\n        f1 = Frame.from_records(\n                [[datetime.date(2012,4,5),\n                datetime.date(2012,4,2),\n                dt64(\'2020-05-03T20:30\'),\n                dt64(\'2017-05-02T05:55\')\n                ],\n                [datetime.date(2014,1,1),\n                datetime.date(2012,4,1),\n                dt64(\'2020-01-03T20:30\'),\n                dt64(\'2025-03-02T03:20\')\n                ]],\n                index=(\'a\', \'b\'),\n                columns=(\'w\', \'x\', \'y\', \'z\'),\n                consolidate_blocks=True\n                )\n\n        f2 = f1.via_dt.strftime(\'%y|%m|%d\')\n\n        self.assertEqual(f2.dtypes.values.tolist(),\n                [np.dtype(\'<U8\'), np.dtype(\'<U8\'), np.dtype(\'<U8\'), np.dtype(\'<U8\')]\n                )\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'w\', ((\'a\', \'12|04|05\'), (\'b\', \'14|01|01\'))), (\'x\', ((\'a\', \'12|04|02\'), (\'b\', \'12|04|01\'))), (\'y\', ((\'a\', \'20|05|03\'), (\'b\', \'20|01|03\'))), (\'z\', ((\'a\', \'17|05|02\'), (\'b\', \'25|03|02\'))))\n                )\n\n\n    #---------------------------------------------------------------------------\n    def test_frame_equals_a(self) -> None:\n\n        idx1 = IndexHierarchy.from_product(\n                (\'far\', 20), (None, \'down\'), (False, \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        idx2 = IndexHierarchy.from_product(\n                (\'far\', 20), (None, \'down\'), (False, \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        f1 = FrameGO(np.arange(16, dtype=np.int64).reshape(8, 2), index=idx1)\n        f2 = FrameGO(np.arange(16, dtype=np.int64).reshape(8, 2), index=idx2)\n        f3 = FrameGO(np.arange(16, dtype=np.int64).reshape(8, 2), index=idx2, name=\'foo\')\n        f4 = FrameGO(np.arange(16, dtype=np.int32).reshape(8, 2), index=idx2)\n        f5 = Frame(np.arange(16, dtype=np.int64).reshape(8, 2), index=idx2)\n\n        self.assertTrue(f1.equals(f1))\n        self.assertTrue(f1.equals(f2))\n\n        self.assertFalse(f1.equals(f3, compare_name=True))\n        self.assertTrue(f1.equals(f3, compare_name=False))\n\n        self.assertFalse(f1.equals(f4, compare_dtype=True))\n        self.assertTrue(f1.equals(f4, compare_dtype=False))\n\n        self.assertFalse(f1.equals(f5, compare_class=True))\n        self.assertTrue(f1.equals(f5, compare_class=False))\n\n\n    def test_frame_equals_b(self) -> None:\n\n        idx1 = IndexHierarchy.from_product(\n                (\'far\', 20), (None, \'down\'), (False, \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        idx2 = IndexHierarchy.from_product(\n                (\'far\', 20), (None, \'down\'), (False, \'right\'),\n                name=(\'z\', \'y\', \'q\')\n                )\n        f1 = FrameGO(np.arange(16, dtype=np.int64).reshape(8, 2), index=idx1)\n        f2 = FrameGO(np.arange(16, dtype=np.int64).reshape(8, 2), index=idx2)\n\n        self.assertFalse(f1.equals(f2, compare_name=True))\n        self.assertTrue(f1.equals(f2, compare_name=False))\n\n\n    def test_frame_equals_c(self) -> None:\n\n        idx1 = IndexHierarchy.from_product(\n                (\'far\', 20), (None, \'down\'), (False, \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        idx2 = IndexHierarchy.from_product(\n                (\'far\', 20), (None, \'down\'), (False, \'right\'),\n                name=(\'z\', \'y\', \'x\')\n                )\n        f1 = FrameGO(np.arange(16, dtype=np.int64).reshape(8, 2), index=idx1)\n        f2 = FrameGO(np.arange(16, dtype=np.int64).reshape(8, 2),\n                index=idx2,\n                columns=(\'a\', \'b\')\n                )\n        f3 = FrameGO(np.arange(24, dtype=np.int64).reshape(8, 3), index=idx2)\n\n        self.assertFalse(f1.equals(f2))\n        self.assertFalse(f1.equals(f3))\n\n    def test_frame_equals_d(self) -> None:\n\n        records = (\n                (2, 2),\n                (np.nan, 0),\n                (np.nan, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n        f2 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', \'z\')\n                )\n\n\n        self.assertTrue(f1.equals(f2))\n        self.assertFalse(f1.equals(f2, skipna=False))\n\n\n\n    def test_frame_equals_e(self) -> None:\n\n        records = (\n                (2, 2),\n                (3, 0),\n                (5, -95),\n                )\n        f1 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', np.nan)\n                )\n\n        f2 = Frame.from_records(records,\n                columns=(\'a\', \'b\'),\n                index=(\'x\', \'y\', np.nan)\n                )\n\n\n        self.assertTrue(f1.equals(f2))\n        self.assertFalse(f1.equals(f2, skipna=False))\n\n\n    def test_frame_equals_f(self) -> None:\n\n        f1 = Frame.from_element(\'a\', index=range(2), columns=range(2))\n        f2 = Frame.from_element(3, index=range(2), columns=range(2))\n\n        self.assertFalse(f1.equals(f2, compare_dtype=False))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_frame_join_a(self) -> None:\n\n        # joiing index to index\n\n        f1 = Frame.from_dict(\n                dict(a=(10,10,np.nan,20,20), b=(\'x\',\'x\',\'y\',\'y\',\'z\')),\n                index=(0, 1, 2, \'foo\', \'x\'))\n        f2 = Frame.from_dict(\n                dict(c=(\'foo\', \'bar\'), d=(10, 20)),\n                index=(\'x\', \'y\'))\n\n        # df1 = f1.to_pandas()\n        # df2 = f2.to_pandas()\n\n        f3 = f1.join_inner(f2, left_depth_level=0, right_depth_level=0, composite_index=False)\n\n        self.assertEqual(f3.to_pairs(0),\n                ((\'a\', ((\'x\', 20.0),)), (\'b\', ((\'x\', \'z\'),)), (\'c\', ((\'x\', \'foo\'),)), (\'d\', ((\'x\', 10),)))\n                )\n\n        f4 = f1.join_outer(f2,\n                left_depth_level=0,\n                right_depth_level=0,\n                composite_index=False).fillna(None)\n\n        # NOTE: this indexes ordering after union is not stable, so do an explict selection before testing\n        locs4 = [0, 1, 2, \'foo\', \'x\', \'y\']\n        f4 = f4.reindex(locs4)\n\n        self.assertEqual(f4.to_pairs(0),\n                ((\'a\', ((0, 10.0), (1, 10.0), (2, None), (\'foo\', 20.0), (\'x\', 20.0), (\'y\', None))), (\'b\', ((0, \'x\'), (1, \'x\'), (2, \'y\'), (\'foo\', \'y\'), (\'x\', \'z\'), (\'y\', None))), (\'c\', ((0, None), (1, None), (2, None), (\'foo\', None), (\'x\', \'foo\'), (\'y\', \'bar\'))), (\'d\', ((0, None), (1, None), (2, None), (\'foo\', None), (\'x\', 10.0), (\'y\', 20.0))))\n                )\n\n        f5 = f1.join_left(f2,\n                left_depth_level=0,\n                right_depth_level=0,\n                composite_index=False).fillna(None)\n\n        self.assertEqual(f5.to_pairs(0),\n                ((\'a\', ((0, 10.0), (1, 10.0), (2, None), (\'foo\', 20.0), (\'x\', 20.0))), (\'b\', ((0, \'x\'), (1, \'x\'), (2, \'y\'), (\'foo\', \'y\'), (\'x\', \'z\'))), (\'c\', ((0, None), (1, None), (2, None), (\'foo\', None), (\'x\', \'foo\'))), (\'d\', ((0, None), (1, None), (2, None), (\'foo\', None), (\'x\', 10.0))))\n                )\n\n        f6 = f1.join_right(f2,\n                left_depth_level=0,\n                right_depth_level=0,\n                composite_index=False).fillna(None)\n        self.assertEqual(f6.to_pairs(0),\n                ((\'a\', ((\'x\', 20.0), (\'y\', None))), (\'b\', ((\'x\', \'z\'), (\'y\', None))), (\'c\', ((\'x\', \'foo\'), (\'y\', \'bar\'))), (\'d\', ((\'x\', 10), (\'y\', 20))))\n                )\n\n\n    def test_frame_join_b(self) -> None:\n\n        # joining on column to column\n\n        f1 = Frame.from_dict(\n            {\n            \'LastName\': (\'Raf\', \'Jon\', \'Hei\', \'Rob\', \'Smi\', \'Wil\'),\n            \'DepartmentID\': (31, 33, 33, 34, 34, None),\n            },\n            index=tuple(\'abcdef\'),\n            )\n\n        f2 = Frame.from_dict(\n            {\n            \'DepartmentID\': (31, 33, 34, 35),\n            \'DepartmentName\': (\'Sales\', \'Engineering\', \'Clerical\', \'Marketing\'),\n            },\n            index=range(10, 14),\n            )\n\n        df1 = f1.to_pandas()\n        df2 = f2.to_pandas()\n\n        f3 = f1.join_outer(f2,\n                left_columns=\'DepartmentID\',\n                left_template=\'Employee.{}\',\n                right_columns=\'DepartmentID\',\n                right_template=\'Department.{}\',\n                )\n        self.assertEqual(f3.shape, (7, 4))\n        self.assertEqual(f3.fillna(None).to_pairs(0),\n                ((\'Employee.LastName\', (((\'a\', 10), \'Raf\'), ((\'b\', 11), \'Jon\'), ((\'c\', 11), \'Hei\'), ((\'d\', 12), \'Rob\'), ((\'e\', 12), \'Smi\'), ((\'f\', None), \'Wil\'), ((None, 13), None))), (\'Employee.DepartmentID\', (((\'a\', 10), 31), ((\'b\', 11), 33), ((\'c\', 11), 33), ((\'d\', 12), 34), ((\'e\', 12), 34), ((\'f\', None), None), ((None, 13), None))), (\'Department.DepartmentID\', (((\'a\', 10), 31.0), ((\'b\', 11), 33.0), ((\'c\', 11), 33.0), ((\'d\', 12), 34.0), ((\'e\', 12), 34.0), ((\'f\', None), None), ((None, 13), 35.0))), (\'Department.DepartmentName\', (((\'a\', 10), \'Sales\'), ((\'b\', 11), \'Engineering\'), ((\'c\', 11), \'Engineering\'), ((\'d\', 12), \'Clerical\'), ((\'e\', 12), \'Clerical\'), ((\'f\', None), None), ((None, 13), \'Marketing\'))))\n\n                )\n\n        f4 = f1.join_inner(f2,\n                left_columns=\'DepartmentID\',\n                left_template=\'Employee.{}\',\n                right_columns=\'DepartmentID\',\n                right_template=\'Department.{}\',\n                )\n        self.assertEqual(f4.shape, (5, 4))\n\n        self.assertEqual(f4.fillna(None).to_pairs(0),\n                ((\'Employee.LastName\', (((\'a\', 10), \'Raf\'), ((\'b\', 11), \'Jon\'), ((\'c\', 11), \'Hei\'), ((\'d\', 12), \'Rob\'), ((\'e\', 12), \'Smi\'))), (\'Employee.DepartmentID\', (((\'a\', 10), 31), ((\'b\', 11), 33), ((\'c\', 11), 33), ((\'d\', 12), 34), ((\'e\', 12), 34))), (\'Department.DepartmentID\', (((\'a\', 10), 31), ((\'b\', 11), 33), ((\'c\', 11), 33), ((\'d\', 12), 34), ((\'e\', 12), 34))), (\'Department.DepartmentName\', (((\'a\', 10), \'Sales\'), ((\'b\', 11), \'Engineering\'), ((\'c\', 11), \'Engineering\'), ((\'d\', 12), \'Clerical\'), ((\'e\', 12), \'Clerical\'))))\n\n                )\n\n        f5 = f1.join_left(f2,\n                left_columns=\'DepartmentID\',\n                left_template=\'Employee.{}\',\n                right_columns=\'DepartmentID\',\n                right_template=\'Department.{}\',\n                )\n        self.assertEqual(f5.shape, (6, 4))\n        self.assertEqual(f5.fillna(None).to_pairs(0),\n                ((\'Employee.LastName\', (((\'a\', 10), \'Raf\'), ((\'b\', 11), \'Jon\'), ((\'c\', 11), \'Hei\'), ((\'d\', 12), \'Rob\'), ((\'e\', 12), \'Smi\'), ((\'f\', None), \'Wil\'))), (\'Employee.DepartmentID\', (((\'a\', 10), 31), ((\'b\', 11), 33), ((\'c\', 11), 33), ((\'d\', 12), 34), ((\'e\', 12), 34), ((\'f\', None), None))), (\'Department.DepartmentID\', (((\'a\', 10), 31.0), ((\'b\', 11), 33.0), ((\'c\', 11), 33.0), ((\'d\', 12), 34.0), ((\'e\', 12), 34.0), ((\'f\', None), None))), (\'Department.DepartmentName\', (((\'a\', 10), \'Sales\'), ((\'b\', 11), \'Engineering\'), ((\'c\', 11), \'Engineering\'), ((\'d\', 12), \'Clerical\'), ((\'e\', 12), \'Clerical\'), ((\'f\', None), None))))\n                )\n\n\n        # df1.merge(df2, how=\'right\', left_on=\'DepartmentID\', right_on=\'DepartmentID\')\n\n        f6 = f1.join_right(f2,\n                left_columns=\'DepartmentID\',\n                left_template=\'Employee.{}\',\n                right_columns=\'DepartmentID\',\n                right_template=\'Department.{}\',\n                )\n\n        self.assertEqual(f6.shape, (6, 4))\n        self.assertEqual(f6.fillna(None).to_pairs(0),\n                ((\'Employee.LastName\', (((\'a\', 10), \'Raf\'), ((\'b\', 11), \'Jon\'), ((\'c\', 11), \'Hei\'), ((\'d\', 12), \'Rob\'), ((\'e\', 12), \'Smi\'), ((None, 13), None))), (\'Employee.DepartmentID\', (((\'a\', 10), 31), ((\'b\', 11), 33), ((\'c\', 11), 33), ((\'d\', 12), 34), ((\'e\', 12), 34), ((None, 13), None))), (\'Department.DepartmentID\', (((\'a\', 10), 31), ((\'b\', 11), 33), ((\'c\', 11), 33), ((\'d\', 12), 34), ((\'e\', 12), 34), ((None, 13), 35))), (\'Department.DepartmentName\', (((\'a\', 10), \'Sales\'), ((\'b\', 11), \'Engineering\'), ((\'c\', 11), \'Engineering\'), ((\'d\', 12), \'Clerical\'), ((\'e\', 12), \'Clerical\'), ((None, 13), \'Marketing\'))))\n                )\n\n        with self.assertRaises(RuntimeError):\n            f1.join_right(f2,\n                    left_columns=\'DepartmentID\',\n                    left_template=\'Employee.{}\',\n                    right_columns=\'DepartmentID\',\n                    right_template=\'Department.{}\',\n                    composite_index=False,\n                    )\n\n\n    def test_frame_join_c(self) -> None:\n        f1 = sf.Frame.from_dict(dict(a=(10,10,20,20,20), b=(\'x\',\'x\',\'y\',\'y\',\'z\')))\n        f2 = sf.Frame.from_dict(dict(c=(\'foo\', \'bar\'), d=(10, 20)), index=(\'x\', \'y\'))\n\n        f3 = f1.join_left(f2, left_columns=\'b\', right_depth_level=0)\n        self.assertEqual(f3.fillna(None).to_pairs(0),\n                ((\'a\', (((0, \'x\'), 10), ((1, \'x\'), 10), ((2, \'y\'), 20), ((3, \'y\'), 20), ((4, None), 20))), (\'b\', (((0, \'x\'), \'x\'), ((1, \'x\'), \'x\'), ((2, \'y\'), \'y\'), ((3, \'y\'), \'y\'), ((4, None), \'z\'))), (\'c\', (((0, \'x\'), \'foo\'), ((1, \'x\'), \'foo\'), ((2, \'y\'), \'bar\'), ((3, \'y\'), \'bar\'), ((4, None), None))), (\'d\', (((0, \'x\'), 10.0), ((1, \'x\'), 10.0), ((2, \'y\'), 20.0), ((3, \'y\'), 20.0), ((4, None), None))))\n                )\n\n        f4 = f1.join_inner(f2, left_columns=\'b\', right_depth_level=0)\n        self.assertEqual(f4.to_pairs(0),\n                ((\'a\', (((0, \'x\'), 10), ((1, \'x\'), 10), ((2, \'y\'), 20), ((3, \'y\'), 20))), (\'b\', (((0, \'x\'), \'x\'), ((1, \'x\'), \'x\'), ((2, \'y\'), \'y\'), ((3, \'y\'), \'y\'))), (\'c\', (((0, \'x\'), \'foo\'), ((1, \'x\'), \'foo\'), ((2, \'y\'), \'bar\'), ((3, \'y\'), \'bar\'))), (\'d\', (((0, \'x\'), 10), ((1, \'x\'), 10), ((2, \'y\'), 20), ((3, \'y\'), 20))))\n                )\n\n        # right is same as inner\n        f5 = f1.join_right(f2, left_columns=\'b\', right_depth_level=0)\n        self.assertTrue(f5.equals(f4, compare_dtype=True))\n\n        # left is same as outer\n        f6 = f1.join_outer(f2, left_columns=\'b\', right_depth_level=0)\n        self.assertTrue(f6.equals(f3, compare_dtype=True))\n\n\n    @skip_win #type: ignore\n    def test_frame_join_d(self) -> None:\n        index1 = IndexDate.from_date_range(\'2020-05-04\', \'2020-05-08\')\n        index2 = IndexHierarchy.from_product((\'A\', \'B\'), index1)\n\n        f1 = Frame.from_dict(dict(a=tuple(range(10)), b=tuple(\'pqrstuvwxy\')), index=index2)\n        f2 = Frame.from_dict(dict(c=tuple(range(10, 15)), d=tuple(\'fffgg\')), index=index1)\n\n        # TODO: left not working due to datedate/ datetime64 mismatch, inner is working\n        f3 = f1.join_left(f2, left_depth_level=1, right_depth_level=0)\n\n        self.assertEqual(f3.dtypes.values.tolist(),\n                [np.dtype(\'int64\'), np.dtype(\'<U1\'), np.dtype(\'int64\'), np.dtype(\'<U1\')]\n                )\n        self.assertEqual(f3.shape, (10, 4))\n\n        self.assertEqual(\n                f3.to_pairs(0),\n                ((\'a\', ((((\'A\', np.datetime64(\'2020-05-04\')), np.datetime64(\'2020-05-04\')), 0), (((\'A\', np.datetime64(\'2020-05-05\')), np.datetime64(\'2020-05-05\')), 1), (((\'A\', np.datetime64(\'2020-05-06\')), np.datetime64(\'2020-05-06\')), 2), (((\'A\', np.datetime64(\'2020-05-07\')), np.datetime64(\'2020-05-07\')), 3), (((\'A\', np.datetime64(\'2020-05-08\')), np.datetime64(\'2020-05-08\')), 4), (((\'B\', np.datetime64(\'2020-05-04\')), np.datetime64(\'2020-05-04\')), 5), (((\'B\', np.datetime64(\'2020-05-05\')), np.datetime64(\'2020-05-05\')), 6), (((\'B\', np.datetime64(\'2020-05-06\')), np.datetime64(\'2020-05-06\')), 7), (((\'B\', np.datetime64(\'2020-05-07\')), np.datetime64(\'2020-05-07\')), 8), (((\'B\', np.datetime64(\'2020-05-08\')), np.datetime64(\'2020-05-08\')), 9))), (\'b\', ((((\'A\', np.datetime64(\'2020-05-04\')), np.datetime64(\'2020-05-04\')), \'p\'), (((\'A\', np.datetime64(\'2020-05-05\')), np.datetime64(\'2020-05-05\')), \'q\'), (((\'A\', np.datetime64(\'2020-05-06\')), np.datetime64(\'2020-05-06\')), \'r\'), (((\'A\', np.datetime64(\'2020-05-07\')), np.datetime64(\'2020-05-07\')), \'s\'), (((\'A\', np.datetime64(\'2020-05-08\')), np.datetime64(\'2020-05-08\')), \'t\'), (((\'B\', np.datetime64(\'2020-05-04\')), np.datetime64(\'2020-05-04\')), \'u\'), (((\'B\', np.datetime64(\'2020-05-05\')), np.datetime64(\'2020-05-05\')), \'v\'), (((\'B\', np.datetime64(\'2020-05-06\')), np.datetime64(\'2020-05-06\')), \'w\'), (((\'B\', np.datetime64(\'2020-05-07\')), np.datetime64(\'2020-05-07\')), \'x\'), (((\'B\', np.datetime64(\'2020-05-08\')), np.datetime64(\'2020-05-08\')), \'y\'))), (\'c\', ((((\'A\', np.datetime64(\'2020-05-04\')), np.datetime64(\'2020-05-04\')), 10), (((\'A\', np.datetime64(\'2020-05-05\')), np.datetime64(\'2020-05-05\')), 11), (((\'A\', np.datetime64(\'2020-05-06\')), np.datetime64(\'2020-05-06\')), 12), (((\'A\', np.datetime64(\'2020-05-07\')), np.datetime64(\'2020-05-07\')), 13), (((\'A\', np.datetime64(\'2020-05-08\')), np.datetime64(\'2020-05-08\')), 14), (((\'B\', np.datetime64(\'2020-05-04\')), np.datetime64(\'2020-05-04\')), 10), (((\'B\', np.datetime64(\'2020-05-05\')), np.datetime64(\'2020-05-05\')), 11), (((\'B\', np.datetime64(\'2020-05-06\')), np.datetime64(\'2020-05-06\')), 12), (((\'B\', np.datetime64(\'2020-05-07\')), np.datetime64(\'2020-05-07\')), 13), (((\'B\', np.datetime64(\'2020-05-08\')), np.datetime64(\'2020-05-08\')), 14))), (\'d\', ((((\'A\', np.datetime64(\'2020-05-04\')), np.datetime64(\'2020-05-04\')), \'f\'), (((\'A\', np.datetime64(\'2020-05-05\')), np.datetime64(\'2020-05-05\')), \'f\'), (((\'A\', np.datetime64(\'2020-05-06\')), np.datetime64(\'2020-05-06\')), \'f\'), (((\'A\', np.datetime64(\'2020-05-07\')), np.datetime64(\'2020-05-07\')), \'g\'), (((\'A\', np.datetime64(\'2020-05-08\')), np.datetime64(\'2020-05-08\')), \'g\'), (((\'B\', np.datetime64(\'2020-05-04\')), np.datetime64(\'2020-05-04\')), \'f\'), (((\'B\', np.datetime64(\'2020-05-05\')), np.datetime64(\'2020-05-05\')), \'f\'), (((\'B\', np.datetime64(\'2020-05-06\')), np.datetime64(\'2020-05-06\')), \'f\'), (((\'B\', np.datetime64(\'2020-05-07\')), np.datetime64(\'2020-05-07\')), \'g\'), (((\'B\', np.datetime64(\'2020-05-08\')), np.datetime64(\'2020-05-08\')), \'g\'))))\n                )\n\n        # inner join is equivalent to left, right, outer\n        self.assertTrue(f1.join_inner(f2, left_depth_level=1, right_depth_level=0).equals(f3))\n        self.assertTrue(f1.join_right(f2, left_depth_level=1, right_depth_level=0).equals(f3))\n        self.assertTrue(f1.join_outer(f2, left_depth_level=1, right_depth_level=0).equals(f3))\n\n\n    def test_frame_join_e(self) -> None:\n\n        # matching on hierarchical indices\n\n        index1 = IndexHierarchy.from_product((\'A\', \'B\'), (1, 2, 3, 4, 5))\n        index2 = IndexHierarchy.from_labels(((\'B\', 3), (\'B\', 5), (\'A\', 2)))\n        f1 = Frame.from_dict(dict(a=tuple(range(10)), b=tuple(\'pqrstuvwxy\')),\n                index=index1)\n        f2 = Frame.from_dict(dict(c=tuple(range(10, 13)), d=tuple(\'fgh\')),\n                index=index2)\n\n        f3 = f1.join_left(f2,\n                left_depth_level=(0, 1),\n                right_depth_level=(0, 1),\n                fill_value=None,\n                composite_index=False,\n                )\n\n        self.assertEqual(f3.to_pairs(0),\n                ((\'a\', (((\'A\', 1), 0), ((\'A\', 2), 1), ((\'A\', 3), 2), ((\'A\', 4), 3), ((\'A\', 5), 4), ((\'B\', 1), 5), ((\'B\', 2), 6), ((\'B\', 3), 7), ((\'B\', 4), 8), ((\'B\', 5), 9))), (\'b\', (((\'A\', 1), \'p\'), ((\'A\', 2), \'q\'), ((\'A\', 3), \'r\'), ((\'A\', 4), \'s\'), ((\'A\', 5), \'t\'), ((\'B\', 1), \'u\'), ((\'B\', 2), \'v\'), ((\'B\', 3), \'w\'), ((\'B\', 4), \'x\'), ((\'B\', 5), \'y\'))), (\'c\', (((\'A\', 1), None), ((\'A\', 2), 12), ((\'A\', 3), None), ((\'A\', 4), None), ((\'A\', 5), None), ((\'B\', 1), None), ((\'B\', 2), None), ((\'B\', 3), 10), ((\'B\', 4), None), ((\'B\', 5), 11))), (\'d\', (((\'A\', 1), None), ((\'A\', 2), \'h\'), ((\'A\', 3), None), ((\'A\', 4), None), ((\'A\', 5), None), ((\'B\', 1), None), ((\'B\', 2), None), ((\'B\', 3), \'f\'), ((\'B\', 4), None), ((\'B\', 5), \'g\'))))\n                )\n\n        f4 = f1.join_left(f2,\n                left_depth_level=(0, 1),\n                right_depth_level=(0, 1),\n                fill_value=None,\n                composite_index=False,\n                )\n\n        self.assertEqual(f4.to_pairs(0),\n                ((\'a\', (((\'A\', 1), 0), ((\'A\', 2), 1), ((\'A\', 3), 2), ((\'A\', 4), 3), ((\'A\', 5), 4), ((\'B\', 1), 5), ((\'B\', 2), 6), ((\'B\', 3), 7), ((\'B\', 4), 8), ((\'B\', 5), 9))), (\'b\', (((\'A\', 1), \'p\'), ((\'A\', 2), \'q\'), ((\'A\', 3), \'r\'), ((\'A\', 4), \'s\'), ((\'A\', 5), \'t\'), ((\'B\', 1), \'u\'), ((\'B\', 2), \'v\'), ((\'B\', 3), \'w\'), ((\'B\', 4), \'x\'), ((\'B\', 5), \'y\'))), (\'c\', (((\'A\', 1), None), ((\'A\', 2), 12), ((\'A\', 3), None), ((\'A\', 4), None), ((\'A\', 5), None), ((\'B\', 1), None), ((\'B\', 2), None), ((\'B\', 3), 10), ((\'B\', 4), None), ((\'B\', 5), 11))), (\'d\', (((\'A\', 1), None), ((\'A\', 2), \'h\'), ((\'A\', 3), None), ((\'A\', 4), None), ((\'A\', 5), None), ((\'B\', 1), None), ((\'B\', 2), None), ((\'B\', 3), \'f\'), ((\'B\', 4), None), ((\'B\', 5), \'g\'))))\n                )\n\n\n\n    def test_frame_join_f(self) -> None:\n        # column on column\n\n        f1 = Frame.from_dict(\n                dict(a=(10,10,np.nan,20,20), b=(\'x\',\'x\',\'y\',\'y\',\'z\')),\n                index=tuple(\'abcde\'))\n\n        f2 = Frame.from_dict(\n                dict(c=(\'y\', \'y\', \'w\'), d=(1000, 3000, 2000)),\n                index=(\'q\', \'p\', \'r\'))\n\n        # case of when a non-index value is joined on, where the right as repeated values; Pandas df1.merge(df2, how=\'left\', left_on=\'b\', right_on=\'c\') will add rows for all unique combinations and drop the resulting index.\n\n        f3 = f1.join_left(f2, left_columns=\'b\', right_columns=\'c\')\n        self.assertEqual(f3.fillna(None).to_pairs(0),\n                ((\'a\', (((\'c\', \'q\'), None), ((\'c\', \'p\'), None), ((\'d\', \'q\'), 20.0), ((\'d\', \'p\'), 20.0), ((\'a\', None), 10.0), ((\'b\', None), 10.0), ((\'e\', None), 20.0))), (\'b\', (((\'c\', \'q\'), \'y\'), ((\'c\', \'p\'), \'y\'), ((\'d\', \'q\'), \'y\'), ((\'d\', \'p\'), \'y\'), ((\'a\', None), \'x\'), ((\'b\', None), \'x\'), ((\'e\', None), \'z\'))), (\'c\', (((\'c\', \'q\'), \'y\'), ((\'c\', \'p\'), \'y\'), ((\'d\', \'q\'), \'y\'), ((\'d\', \'p\'), \'y\'), ((\'a\', None), None), ((\'b\', None), None), ((\'e\', None), None))), (\'d\', (((\'c\', \'q\'), 1000.0), ((\'c\', \'p\'), 3000.0), ((\'d\', \'q\'), 1000.0), ((\'d\', \'p\'), 3000.0), ((\'a\', None), None), ((\'b\', None), None), ((\'e\', None), None))))\n                )\n\n        f4 = f1.join_right(f2, left_columns=\'b\', right_columns=\'c\', fill_value=None)\n        self.assertEqual(f4.fillna(None).to_pairs(0),\n                ((\'a\', (((\'c\', \'q\'), None), ((\'c\', \'p\'), None), ((\'d\', \'q\'), 20.0), ((\'d\', \'p\'), 20.0), ((None, \'r\'), None))), (\'b\', (((\'c\', \'q\'), \'y\'), ((\'c\', \'p\'), \'y\'), ((\'d\', \'q\'), \'y\'), ((\'d\', \'p\'), \'y\'), ((None, \'r\'), None))), (\'c\', (((\'c\', \'q\'), \'y\'), ((\'c\', \'p\'), \'y\'), ((\'d\', \'q\'), \'y\'), ((\'d\', \'p\'), \'y\'), ((None, \'r\'), \'w\'))), (\'d\', (((\'c\', \'q\'), 1000), ((\'c\', \'p\'), 3000), ((\'d\', \'q\'), 1000), ((\'d\', \'p\'), 3000), ((None, \'r\'), 2000))))\n                )\n\n        f5 = f1.join_inner(f2, left_columns=\'b\', right_columns=\'c\')\n        self.assertEqual(f5.fillna(None).to_pairs(0),\n                ((\'a\', (((\'c\', \'q\'), None), ((\'c\', \'p\'), None), ((\'d\', \'q\'), 20.0), ((\'d\', \'p\'), 20.0))), (\'b\', (((\'c\', \'q\'), \'y\'), ((\'c\', \'p\'), \'y\'), ((\'d\', \'q\'), \'y\'), ((\'d\', \'p\'), \'y\'))), (\'c\', (((\'c\', \'q\'), \'y\'), ((\'c\', \'p\'), \'y\'), ((\'d\', \'q\'), \'y\'), ((\'d\', \'p\'), \'y\'))), (\'d\', (((\'c\', \'q\'), 1000), ((\'c\', \'p\'), 3000), ((\'d\', \'q\'), 1000), ((\'d\', \'p\'), 3000))))\n                )\n\n        f6 = f1.join_outer(f2, left_columns=\'b\', right_columns=\'c\', fill_value=None)\n        self.assertEqual(f6.fillna(None).to_pairs(0),\n                ((\'a\', (((\'c\', \'q\'), None), ((\'c\', \'p\'), None), ((\'d\', \'q\'), 20.0), ((\'d\', \'p\'), 20.0), ((\'a\', None), 10.0), ((\'b\', None), 10.0), ((\'e\', None), 20.0), ((None, \'r\'), None))), (\'b\', (((\'c\', \'q\'), \'y\'), ((\'c\', \'p\'), \'y\'), ((\'d\', \'q\'), \'y\'), ((\'d\', \'p\'), \'y\'), ((\'a\', None), \'x\'), ((\'b\', None), \'x\'), ((\'e\', None), \'z\'), ((None, \'r\'), None))), (\'c\', (((\'c\', \'q\'), \'y\'), ((\'c\', \'p\'), \'y\'), ((\'d\', \'q\'), \'y\'), ((\'d\', \'p\'), \'y\'), ((\'a\', None), None), ((\'b\', None), None), ((\'e\', None), None), ((None, \'r\'), \'w\'))), (\'d\', (((\'c\', \'q\'), 1000), ((\'c\', \'p\'), 3000), ((\'d\', \'q\'), 1000), ((\'d\', \'p\'), 3000), ((\'a\', None), None), ((\'b\', None), None), ((\'e\', None), None), ((None, \'r\'), 2000))))\n                )\n\n\n\n    def test_frame_join_g(self) -> None:\n\n        f1 = Frame.from_records(\n                ((1,\'apple\'),\n                (2,\'banana\'),\n                (3,\'kiwi fruit\'),\n                (4,\'strawberries\'),\n                (5,\'flour\'),\n                (6,\'fruit juice\'),\n                (7,\'butter\'),\n                (8,\'sugar\')),\n                columns=(\'ingredient_id\', \'ingredient_name\'),\n                index=tuple(\'abcdefgh\'))\n\n        f2 = Frame.from_records(\n                ((1,\'Apple Crumble\'),\n                (2,\'Fruit Salad\',),\n                (3,\'Weekday Risotto\',),\n                (4,\'Beans Chili\',),\n                (5,\'Chicken Casserole\',)),\n                columns=(\'recipe_id\', \'recipe_name\'),\n                index=tuple(\'stuvw\')\n                )\n\n        f3 = Frame.from_records(\n                ((1,1),(1,5),(1,7),(1,8),(2,6),(2,2),(2,1),(2,3),(2,4)),\n                index=tuple(\'ijklmnopq\'),\n                columns=(\'recipe_id\', \'ingredient_id\')\n                )\n\n        f4 = f2.join_inner(f3,\n                left_columns=\'recipe_id\',\n                right_columns=\'recipe_id\',\n                right_template=\'new_{}\'\n                )\n        self.assertEqual(f4.to_pairs(0),\n                ((\'recipe_id\', (((\'s\', \'i\'), 1), ((\'s\', \'j\'), 1), ((\'s\', \'k\'), 1), ((\'s\', \'l\'), 1), ((\'t\', \'m\'), 2), ((\'t\', \'n\'), 2), ((\'t\', \'o\'), 2), ((\'t\', \'p\'), 2), ((\'t\', \'q\'), 2))), (\'recipe_name\', (((\'s\', \'i\'), \'Apple Crumble\'), ((\'s\', \'j\'), \'Apple Crumble\'), ((\'s\', \'k\'), \'Apple Crumble\'), ((\'s\', \'l\'), \'Apple Crumble\'), ((\'t\', \'m\'), \'Fruit Salad\'), ((\'t\', \'n\'), \'Fruit Salad\'), ((\'t\', \'o\'), \'Fruit Salad\'), ((\'t\', \'p\'), \'Fruit Salad\'), ((\'t\', \'q\'), \'Fruit Salad\'))), (\'new_recipe_id\', (((\'s\', \'i\'), 1), ((\'s\', \'j\'), 1), ((\'s\', \'k\'), 1), ((\'s\', \'l\'), 1), ((\'t\', \'m\'), 2), ((\'t\', \'n\'), 2), ((\'t\', \'o\'), 2), ((\'t\', \'p\'), 2), ((\'t\', \'q\'), 2))), (\'new_ingredient_id\', (((\'s\', \'i\'), 1), ((\'s\', \'j\'), 5), ((\'s\', \'k\'), 7), ((\'s\', \'l\'), 8), ((\'t\', \'m\'), 6), ((\'t\', \'n\'), 2), ((\'t\', \'o\'), 1), ((\'t\', \'p\'), 3), ((\'t\', \'q\'), 4))))\n                )\n\n        f7 = f2.join_outer(f3,\n                left_columns=\'recipe_id\',\n                right_columns=\'recipe_id\',\n                right_template=\'new_{}\'\n                )\n\n        self.assertEqual(f7.fillna(None).to_pairs(0),\n                ((\'recipe_id\', (((\'s\', \'i\'), 1), ((\'s\', \'j\'), 1), ((\'s\', \'k\'), 1), ((\'s\', \'l\'), 1), ((\'t\', \'m\'), 2), ((\'t\', \'n\'), 2), ((\'t\', \'o\'), 2), ((\'t\', \'p\'), 2), ((\'t\', \'q\'), 2), ((\'u\', None), 3), ((\'v\', None), 4), ((\'w\', None), 5))), (\'recipe_name\', (((\'s\', \'i\'), \'Apple Crumble\'), ((\'s\', \'j\'), \'Apple Crumble\'), ((\'s\', \'k\'), \'Apple Crumble\'), ((\'s\', \'l\'), \'Apple Crumble\'), ((\'t\', \'m\'), \'Fruit Salad\'), ((\'t\', \'n\'), \'Fruit Salad\'), ((\'t\', \'o\'), \'Fruit Salad\'), ((\'t\', \'p\'), \'Fruit Salad\'), ((\'t\', \'q\'), \'Fruit Salad\'), ((\'u\', None), \'Weekday Risotto\'), ((\'v\', None), \'Beans Chili\'), ((\'w\', None), \'Chicken Casserole\'))), (\'new_recipe_id\', (((\'s\', \'i\'), 1.0), ((\'s\', \'j\'), 1.0), ((\'s\', \'k\'), 1.0), ((\'s\', \'l\'), 1.0), ((\'t\', \'m\'), 2.0), ((\'t\', \'n\'), 2.0), ((\'t\', \'o\'), 2.0), ((\'t\', \'p\'), 2.0), ((\'t\', \'q\'), 2.0), ((\'u\', None), None), ((\'v\', None), None), ((\'w\', None), None))), (\'new_ingredient_id\', (((\'s\', \'i\'), 1.0), ((\'s\', \'j\'), 5.0), ((\'s\', \'k\'), 7.0), ((\'s\', \'l\'), 8.0), ((\'t\', \'m\'), 6.0), ((\'t\', \'n\'), 2.0), ((\'t\', \'o\'), 1.0), ((\'t\', \'p\'), 3.0), ((\'t\', \'q\'), 4.0), ((\'u\', None), None), ((\'v\', None), None), ((\'w\', None), None))))\n                )\n\n\n        f5 = f2.join_right(f3,\n                left_columns=\'recipe_id\',\n                right_columns=\'recipe_id\',\n                right_template=\'new_{}\'\n                )\n\n        self.assertEqual(f5.to_pairs(0),\n                ((\'recipe_id\', (((\'s\', \'i\'), 1), ((\'s\', \'j\'), 1), ((\'s\', \'k\'), 1), ((\'s\', \'l\'), 1), ((\'t\', \'m\'), 2), ((\'t\', \'n\'), 2), ((\'t\', \'o\'), 2), ((\'t\', \'p\'), 2), ((\'t\', \'q\'), 2))), (\'recipe_name\', (((\'s\', \'i\'), \'Apple Crumble\'), ((\'s\', \'j\'), \'Apple Crumble\'), ((\'s\', \'k\'), \'Apple Crumble\'), ((\'s\', \'l\'), \'Apple Crumble\'), ((\'t\', \'m\'), \'Fruit Salad\'), ((\'t\', \'n\'), \'Fruit Salad\'), ((\'t\', \'o\'), \'Fruit Salad\'), ((\'t\', \'p\'), \'Fruit Salad\'), ((\'t\', \'q\'), \'Fruit Salad\'))), (\'new_recipe_id\', (((\'s\', \'i\'), 1), ((\'s\', \'j\'), 1), ((\'s\', \'k\'), 1), ((\'s\', \'l\'), 1), ((\'t\', \'m\'), 2), ((\'t\', \'n\'), 2), ((\'t\', \'o\'), 2), ((\'t\', \'p\'), 2), ((\'t\', \'q\'), 2))), (\'new_ingredient_id\', (((\'s\', \'i\'), 1), ((\'s\', \'j\'), 5), ((\'s\', \'k\'), 7), ((\'s\', \'l\'), 8), ((\'t\', \'m\'), 6), ((\'t\', \'n\'), 2), ((\'t\', \'o\'), 1), ((\'t\', \'p\'), 3), ((\'t\', \'q\'), 4))))\n                )\n\n\n        f6 = f2.join_left(f3,\n                left_columns=\'recipe_id\',\n                right_columns=\'recipe_id\',\n                right_template=\'new_{}\'\n                )\n\n        self.assertEqual(f6.fillna(None).to_pairs(0),\n                ((\'recipe_id\', (((\'s\', \'i\'), 1), ((\'s\', \'j\'), 1), ((\'s\', \'k\'), 1), ((\'s\', \'l\'), 1), ((\'t\', \'m\'), 2), ((\'t\', \'n\'), 2), ((\'t\', \'o\'), 2), ((\'t\', \'p\'), 2), ((\'t\', \'q\'), 2), ((\'u\', None), 3), ((\'v\', None), 4), ((\'w\', None), 5))), (\'recipe_name\', (((\'s\', \'i\'), \'Apple Crumble\'), ((\'s\', \'j\'), \'Apple Crumble\'), ((\'s\', \'k\'), \'Apple Crumble\'), ((\'s\', \'l\'), \'Apple Crumble\'), ((\'t\', \'m\'), \'Fruit Salad\'), ((\'t\', \'n\'), \'Fruit Salad\'), ((\'t\', \'o\'), \'Fruit Salad\'), ((\'t\', \'p\'), \'Fruit Salad\'), ((\'t\', \'q\'), \'Fruit Salad\'), ((\'u\', None), \'Weekday Risotto\'), ((\'v\', None), \'Beans Chili\'), ((\'w\', None), \'Chicken Casserole\'))), (\'new_recipe_id\', (((\'s\', \'i\'), 1.0), ((\'s\', \'j\'), 1.0), ((\'s\', \'k\'), 1.0), ((\'s\', \'l\'), 1.0), ((\'t\', \'m\'), 2.0), ((\'t\', \'n\'), 2.0), ((\'t\', \'o\'), 2.0), ((\'t\', \'p\'), 2.0), ((\'t\', \'q\'), 2.0), ((\'u\', None), None), ((\'v\', None), None), ((\'w\', None), None))), (\'new_ingredient_id\', (((\'s\', \'i\'), 1.0), ((\'s\', \'j\'), 5.0), ((\'s\', \'k\'), 7.0), ((\'s\', \'l\'), 8.0), ((\'t\', \'m\'), 6.0), ((\'t\', \'n\'), 2.0), ((\'t\', \'o\'), 1.0), ((\'t\', \'p\'), 3.0), ((\'t\', \'q\'), 4.0), ((\'u\', None), None), ((\'v\', None), None), ((\'w\', None), None))))\n                )\n\n\n    def test_frame_join_h(self) -> None:\n\n        f1 = sf.Frame.from_dict(dict(a=(10,10,20,20,20), b=(\'x\',\'x\',\'y\',\'y\',\'z\')))\n        f2 = sf.Frame.from_dict(dict(c=(\'foo\', \'bar\'), d=(10, 20)), index=(\'x\', \'y\'))\n\n        # df1 = f1.to_pandas()\n        # df2 = f2.to_pandas()\n        #df1.merge(df2, left_on=\'b\', right_index=True)\n\n        f3 = f2.join_inner(f1, left_depth_level=0, right_depth_level=0)\n        self.assertEqual(f3.to_pairs(0),\n                ((\'c\', ()), (\'d\', ()), (\'a\', ()), (\'b\', ()))\n                )\n\n        f4 = f2.join_right(f1,\n                left_depth_level=0,\n                right_depth_level=0,\n                fill_value=None,\n                composite_index=False,\n                )\n        self.assertEqual(f4.to_pairs(0),\n                ((\'c\', ((0, None), (1, None), (2, None), (3, None), (4, None))), (\'d\', ((0, None), (1, None), (2, None), (3, None), (4, None))), (\'a\', ((0, 10), (1, 10), (2, 20), (3, 20), (4, 20))), (\'b\', ((0, \'x\'), (1, \'x\'), (2, \'y\'), (3, \'y\'), (4, \'z\'))))\n                )\n\n        f5 = f2.join_left(f1,\n                left_depth_level=0,\n                right_depth_level=0,\n                fill_value=None,\n                composite_index=False,\n                )\n        self.assertEqual(f5.to_pairs(0),\n                ((\'c\', ((\'x\', \'foo\'), (\'y\', \'bar\'))), (\'d\', ((\'x\', 10), (\'y\', 20))), (\'a\', ((\'x\', None), (\'y\', None))), (\'b\', ((\'x\', None), (\'y\', None))))\n                )\n\n        f6 = f2.join_outer(f1,\n                left_depth_level=0,\n                right_depth_level=0,\n                fill_value=None,\n                composite_index=False,\n                )\n        f6 = f6.loc[[0, 1, 2, 3, 4, \'y\', \'x\']] # get stable ordering\n        self.assertEqual(f6.to_pairs(0),\n                ((\'c\', ((0, None), (1, None), (2, None), (3, None), (4, None), (\'y\', \'bar\'), (\'x\', \'foo\'))), (\'d\', ((0, None), (1, None), (2, None), (3, None), (4, None), (\'y\', 20), (\'x\', 10))), (\'a\', ((0, 10), (1, 10), (2, 20), (3, 20), (4, 20), (\'y\', None), (\'x\', None))), (\'b\', ((0, \'x\'), (1, \'x\'), (2, \'y\'), (3, \'y\'), (4, \'z\'), (\'y\', None), (\'x\', None))))\n                )\n\n    def test_frame_join_i(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(a=(10,10,20,20), b=(\'x\',\'x\',\'y\',\'z\')),\n                index=(\'a\', \'b\', \'c\', \'d\'))\n        f2 = Frame.from_dict(\n                dict(c=(\'foo\', \'bar\'), d=(10, 20)),\n                index=(\'c\', \'d\'))\n\n        f3 = f1.join_left(f2, left_depth_level=0,\n                right_depth_level=0,\n                fill_value=None,\n                composite_index=False)\n\n        self.assertEqual(f3.to_pairs(0),\n                ((\'a\', ((\'a\', 10), (\'b\', 10), (\'c\', 20), (\'d\', 20))), (\'b\', ((\'a\', \'x\'), (\'b\', \'x\'), (\'c\', \'y\'), (\'d\', \'z\'))), (\'c\', ((\'a\', None), (\'b\', None), (\'c\', \'foo\'), (\'d\', \'bar\'))), (\'d\', ((\'a\', None), (\'b\', None), (\'c\', 10), (\'d\', 20))))\n                )\n\n        f4 = f1.join_inner(f2, left_depth_level=0,\n                right_depth_level=0,\n                fill_value=None,\n                composite_index=False,\n                )\n        self.assertEqual( f4.to_pairs(0),\n                ((\'a\', ((\'c\', 20), (\'d\', 20))), (\'b\', ((\'c\', \'y\'), (\'d\', \'z\'))), (\'c\', ((\'c\', \'foo\'), (\'d\', \'bar\'))), (\'d\', ((\'c\', 10), (\'d\', 20))))\n                )\n        # import ipdb; ipdb.set_trace()\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n\n\n'"
static_frame/test/unit/test_index.py,24,"b'\nimport unittest\nimport numpy as np\nimport pickle\nimport datetime\nimport typing as tp\nfrom io import StringIO\n\nfrom static_frame import Index\nfrom static_frame import IndexGO\nfrom static_frame import IndexDate\nfrom static_frame import IndexDateGO\nfrom static_frame import IndexHierarchy\nfrom static_frame import Series\n# from static_frame import Frame\nfrom static_frame import IndexYear\nfrom static_frame import Frame\n\n# from static_frame import HLoc\nfrom static_frame import ILoc\n\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.core.index import _index_initializer_needs_init\n\nfrom static_frame.core.exception import ErrorInitIndex\nfrom static_frame.core.index import PositionsAllocator\nfrom static_frame.core.util import mloc\n\n\nclass TestUnit(TestCase):\n\n    def test_positions_allocator_a(self) -> None:\n\n        a1 = PositionsAllocator.get(3)\n        a2 = PositionsAllocator.get(4)\n        a3 = PositionsAllocator.get(5)\n        self.assertTrue(mloc(a1) == mloc(a2))\n        self.assertTrue(mloc(a3) == mloc(a2))\n\n    def test_index_slotted_a(self) -> None:\n        idx1 = Index((\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n\n        with self.assertRaises(AttributeError):\n            idx1.g = 30 #type: ignore #pylint: disable=E0237\n        with self.assertRaises(AttributeError):\n            idx1.__dict__ #pylint: disable=W0104\n\n\n    #---------------------------------------------------------------------------\n    def test_index_init_a(self) -> None:\n        idx1 = Index((\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        idx2 = Index(idx1)\n\n        self.assertEqual(idx1.name, \'foo\')\n        self.assertEqual(idx2.name, \'foo\')\n\n\n    def test_index_init_b(self) -> None:\n\n        idx1 = IndexHierarchy.from_product([\'A\', \'B\'], [1, 2])\n\n        idx2 = Index(idx1)\n\n        self.assertEqual(idx2.values.tolist(),\n            [(\'A\', 1), (\'A\', 2), (\'B\', 1), (\'B\', 2)])\n\n\n    def test_index_init_c(self) -> None:\n\n        s1 = Series((\'a\', \'b\', \'c\'))\n        idx2 = Index(s1)\n        self.assertEqual(idx2.values.tolist(),\n                [\'a\', \'b\', \'c\']\n                )\n\n    def test_index_init_d(self) -> None:\n        idx = Index((0, \'1\', 2))\n        self.assertEqual(idx.values.tolist(),\n                [0, \'1\', 2]\n                )\n\n    def test_index_init_e(self) -> None:\n        labels = [0.0, 36028797018963969]\n        idx = Index(labels)\n        # cannot extract the value once converted to float\n        self.assertEqual(idx.loc[idx.values[1]], 36028797018963969)\n\n    def test_index_init_f(self) -> None:\n\n        labels = np.arange(3)\n        mapping = {x:x for x in range(3)}\n\n        with self.assertRaises(RuntimeError):\n            _ = Index._extract_labels(\n                    mapping=mapping, #type: ignore\n                    labels=labels,\n                    dtype=float\n                    )\n\n    def test_index_init_g(self) -> None:\n        index = Index(Frame(np.arange(6).reshape((2, 3))))\n        self.assertEqual(\n                index.values.tolist(),\n                [(0, 1, 2), (3, 4, 5)]\n                )\n\n\n    def test_index_init_h(self) -> None:\n        index = Index(range(10, 20, 2))\n        self.assertEqual(index.values.tolist(), list(range(10, 20, 2)))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_loc_to_iloc_a(self) -> None:\n\n        idx = Index((\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(\n                tp.cast(np.ndarray, idx.loc_to_iloc(np.array([True, False, True, False]))).tolist(),\n                [0, 2])\n\n        self.assertEqual(idx.loc_to_iloc(slice(\'c\',)), slice(None, 3, None))\n        self.assertEqual(idx.loc_to_iloc(slice(\'b\',\'d\')), slice(1, 4, None))\n        self.assertEqual(idx.loc_to_iloc(\'d\'), 3)\n\n\n    def test_index_loc_to_iloc_b(self) -> None:\n        idx = Index((\'a\', \'b\', \'c\', \'d\'))\n        post = idx.loc_to_iloc(Series([\'b\', \'c\']))\n        self.assertEqual(post, [1, 2])\n\n    #---------------------------------------------------------------------------\n    def test_index_mloc_a(self) -> None:\n        idx = Index((\'a\', \'b\', \'c\', \'d\'))\n        self.assertTrue(idx.mloc == idx[:2].mloc) #type: ignore\n\n    def test_index_mloc_b(self) -> None:\n        idx = IndexGO((\'a\', \'b\', \'c\', \'d\'))\n        idx.append(\'e\')\n        self.assertTrue(idx.mloc == idx[:2].mloc) #type: ignore\n\n\n    def test_index_dtype_a(self) -> None:\n        idx = IndexGO((\'a\', \'b\', \'c\', \'d\'))\n        self.assertEqual(str(idx.dtype), \'<U1\')\n        idx.append(\'eee\')\n        self.assertEqual(str(idx.dtype), \'<U3\')\n\n\n    def test_index_shape_a(self) -> None:\n        idx = IndexGO((\'a\', \'b\', \'c\', \'d\'))\n        self.assertEqual(idx.shape, (4,))\n        idx.append(\'e\')\n        self.assertEqual(idx.shape, (5,))\n\n\n    def test_index_ndim_a(self) -> None:\n        idx = IndexGO((\'a\', \'b\', \'c\', \'d\'))\n        self.assertEqual(idx.ndim, 1)\n        idx.append(\'e\')\n        self.assertEqual(idx.ndim, 1)\n\n\n    def test_index_size_a(self) -> None:\n        idx = IndexGO((\'a\', \'b\', \'c\', \'d\'))\n        self.assertEqual(idx.size, 4)\n        idx.append(\'e\')\n        self.assertEqual(idx.size, 5)\n\n    def test_index_nbytes_a(self) -> None:\n        idx = IndexGO((\'a\', \'b\', \'c\', \'d\'))\n        self.assertEqual(idx.nbytes, 16)\n        idx.append(\'e\')\n        self.assertEqual(idx.nbytes, 20)\n\n    #---------------------------------------------------------------------------\n\n    def test_index_rename_a(self) -> None:\n        idx1 = IndexGO((\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        idx1.append(\'e\')\n        idx2 = idx1.rename(\'bar\')\n        self.assertEqual(idx2.name, \'bar\')\n\n    def test_index_rename_b(self) -> None:\n        a = Index([1], name=\'foo\')\n        self.assertEqual(a.name, \'foo\')\n        b = a.rename(None)\n        self.assertEqual(b.name, None)\n\n    #---------------------------------------------------------------------------\n\n    def test_index_positions_a(self) -> None:\n        idx1 = IndexGO((\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        self.assertEqual(idx1.positions.tolist(), list(range(4)))\n\n        idx1.append(\'e\')\n        self.assertEqual(idx1.positions.tolist(), list(range(5)))\n\n\n    def test_index_unique(self) -> None:\n\n        with self.assertRaises(ErrorInitIndex):\n            idx = Index((\'a\', \'b\', \'c\', \'a\'))\n        with self.assertRaises(ErrorInitIndex):\n            idx = IndexGO((\'a\', \'b\', \'c\', \'a\'))\n\n        with self.assertRaises(ErrorInitIndex):\n            idx = Index([\'a\', \'a\'])\n        with self.assertRaises(ErrorInitIndex):\n            idx = IndexGO([\'a\', \'a\'])\n\n        with self.assertRaises(ErrorInitIndex):\n            idx = Index(np.array([True, False, True], dtype=bool))\n        with self.assertRaises(ErrorInitIndex):\n            idx = IndexGO(np.array([True, False, True], dtype=bool))\n\n        # acceptable but not advisiable\n        idx = Index([0, \'0\'])\n\n\n    def test_index_creation_a(self) -> None:\n        idx = Index((\'a\', \'b\', \'c\', \'d\'))\n\n        #idx2 = idx[\'b\':\'d\']\n\n        self.assertEqual(idx.values.tolist(), [\'a\', \'b\', \'c\', \'d\'])\n\n        self.assertEqual(idx[2:].values.tolist(), [\'c\', \'d\']) #type: ignore\n\n        self.assertEqual(idx.loc[\'b\':].values.tolist(), [\'b\', \'c\', \'d\'])  # type: ignore  # https://github.com/python/typeshed/pull/3024\n\n        self.assertEqual(idx.loc[\'b\':\'d\'].values.tolist(), [\'b\', \'c\', \'d\'])  # type: ignore  # https://github.com/python/typeshed/pull/3024\n\n        self.assertEqual(idx.loc_to_iloc([\'b\', \'b\', \'c\']), [1, 1, 2])\n\n        self.assertEqual(idx.loc[\'c\'], \'c\')\n\n        idxgo = IndexGO((\'a\', \'b\', \'c\', \'d\'))\n        self.assertEqual(idxgo.values.tolist(), [\'a\', \'b\', \'c\', \'d\'])\n\n        idxgo.append(\'e\')\n        self.assertEqual(idxgo.values.tolist(), [\'a\', \'b\', \'c\', \'d\', \'e\'])\n\n        idxgo.extend((\'f\', \'g\'))\n        self.assertEqual(idxgo.values.tolist(), [\'a\', \'b\', \'c\', \'d\', \'e\', \'f\', \'g\'])\n\n\n    def test_index_creation_b(self) -> None:\n        idx = Index((x for x in (\'a\', \'b\', \'c\', \'d\') if x in {\'b\', \'d\'}))\n        self.assertEqual(idx.loc_to_iloc(\'b\'), 0)\n        self.assertEqual(idx.loc_to_iloc(\'d\'), 1)\n\n    #---------------------------------------------------------------------------\n\n    def test_index_unary_operators_a(self) -> None:\n        idx = Index((20, 30, 40, 50))\n\n        invert_idx = -idx\n        self.assertEqual(invert_idx.tolist(),\n                [-20, -30, -40, -50],)\n\n        # this is strange but consistent with NP\n        not_idx = ~idx\n        self.assertEqual(not_idx.tolist(),\n                [-21, -31, -41, -51],)\n\n\n    def test_index_unary_operators_b(self) -> None:\n        idx = IndexGO((20, 30, 40))\n        idx.append(50)\n        a1 = -idx\n        self.assertEqual(a1.tolist(), [-20, -30, -40, -50])\n\n\n    def test_index_binary_operators_a(self) -> None:\n        idx = Index((20, 30, 40, 50))\n\n        self.assertEqual((idx + 2).tolist(),\n                [22, 32, 42, 52])\n        self.assertEqual((2 + idx).tolist(),\n                [22, 32, 42, 52])\n        self.assertEqual((idx * 2).tolist(),\n                [40, 60, 80, 100])\n        self.assertEqual((2 * idx).tolist(),\n                [40, 60, 80, 100])\n        self.assertEqual((idx - 2).tolist(),\n                [18, 28, 38, 48])\n        self.assertEqual(\n                (2 - idx).tolist(),\n                [-18, -28, -38, -48])\n\n\n    def test_index_binary_operators_b(self) -> None:\n        \'\'\'Both operands are Index instances\n        \'\'\'\n        idx1 = Index((20, 30, 40, 50))\n        idx2 = Index((20, 3, 4, 5))\n\n        self.assertEqual((idx1 == idx2).tolist(), [True, False, False, False])\n\n    def test_index_binary_operators_c(self) -> None:\n        idx1 = Index((20, 30, 40, 50))\n        idx2 = Index((20, 3, 4, 5))\n        self.assertEqual(idx1 @ idx2, idx1.values @ idx2.values)\n\n    def test_index_binary_operators_d(self) -> None:\n        idx = IndexGO((20, 30, 40))\n        idx.extend((50, 60))\n        a1 = idx * 2\n        self.assertEqual(a1.tolist(), [40, 60, 80, 100, 120])\n\n    def test_index_binary_operators_e(self) -> None:\n        idx1 = Index((20, 30, 40, 50))\n        idx2 = Index((20, 3, 4, 5))\n        self.assertEqual(idx1.values @ idx2, idx1.values @ idx2.values)\n        self.assertEqual(idx1.values.tolist() @ idx2, idx1.values @ idx2.values)\n\n    def test_index_binary_operators_f(self) -> None:\n        idx1 = Index((\'a\', \'b\', \'c\'))\n\n        self.assertEqual((idx1 + \'_\').tolist(), [\'a_\', \'b_\', \'c_\'])\n        self.assertEqual((\'_\' + idx1).tolist(), [\'_a\', \'_b\', \'_c\'])\n        self.assertEqual((idx1 * 3).tolist(), [\'aaa\', \'bbb\', \'ccc\'])\n\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_ufunc_axis_a(self) -> None:\n\n        idx = Index((30, 40, 50))\n\n        self.assertEqual(idx.min(), 30)\n        self.assertEqual(idx.max(), 50)\n        self.assertEqual(idx.sum(), 120)\n\n    def test_index_ufunc_axis_b(self) -> None:\n\n        idx = IndexGO((30, 40, 20))\n        idx.append(10)\n        self.assertEqual(idx.sum(), 100)\n\n\n    def test_index_isin_a(self) -> None:\n\n        idx = Index((30, 40, 50))\n\n        self.assertEqual(idx.isin([40, 50]).tolist(), [False, True, True])\n        self.assertEqual(idx.isin({40, 50}).tolist(), [False, True, True])\n\n        self.assertEqual(idx.isin(frozenset((40, 50))).tolist(), [False, True, True])\n\n        self.assertEqual(idx.isin({40: \'a\', 50: \'b\'}).tolist(), [False, True, True])\n\n        self.assertEqual(idx.isin(range(35, 45)).tolist(), [False, True, False])\n\n        self.assertEqual(idx.isin((x * 10 for x in (3, 4, 5, 6, 6))).tolist(), [True, True, True])\n\n    def test_index_isin_b(self) -> None:\n        idx = Index((\'a\', \'b\', \'c\'))\n        self.assertEqual(\n                idx.isin((\'b\',\'c\')).tolist(),\n                [False, True, True]\n                )\n\n        self.assertEqual(\n                idx.isin((\'b\', \'c\', \'b\', \'c\')).tolist(),\n                [False, True, True]\n                )\n\n    #---------------------------------------------------------------------------\n    def test_index_copy_a(self) -> None:\n        idx1 = IndexGO((\'a\', \'b\', \'c\'))\n        idx1.append(\'d\')\n        idx2 = idx1.copy()\n        idx2.append(\'e\')\n        self.assertEqual(idx2.values.tolist(), [\'a\', \'b\', \'c\', \'d\', \'e\'])\n        self.assertEqual(idx1.values.tolist(), [\'a\', \'b\', \'c\', \'d\'])\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_contains_a(self) -> None:\n\n        index = Index((\'a\', \'b\', \'c\'))\n        self.assertTrue(\'a\' in index)\n        self.assertTrue(\'d\' not in index)\n\n    #---------------------------------------------------------------------------\n\n    def test_index_go_a(self) -> None:\n\n        index = IndexGO((\'a\', \'b\', \'c\'))\n        index.append(\'d\')\n        self.assertEqual(index.loc_to_iloc(\'d\'), 3)\n\n        index.extend((\'e\', \'f\'))\n        self.assertEqual(index.loc_to_iloc(\'e\'), 4)\n        self.assertEqual(index.loc_to_iloc(\'f\'), 5)\n\n        # creating an index form an Index go takes the np arrays, but not the mutable bits\n        index2 = Index(index)\n        index.append(\'h\')\n\n        self.assertEqual(len(index2), 6)\n        self.assertEqual(len(index), 7)\n\n        index3 = index[2:]\n        index3.append(\'i\') #type: ignore\n\n        self.assertEqual(index3.values.tolist(), [\'c\', \'d\', \'e\', \'f\', \'h\', \'i\']) #type: ignore\n        self.assertEqual(index.values.tolist(), [\'a\', \'b\', \'c\', \'d\', \'e\', \'f\', \'h\'])\n\n\n    def test_index_go_b(self) -> None:\n\n        index = IndexGO((\'a\', \'b\', \'c\'))\n        index.append(\'d\')\n        self.assertEqual(len(index.__slots__), 8)\n        self.assertFalse(index.STATIC)\n        self.assertEqual(index._IMMUTABLE_CONSTRUCTOR, Index)\n        self.assertEqual(Index._MUTABLE_CONSTRUCTOR, IndexGO)\n\n    def test_index_go_c(self) -> None:\n\n        index = IndexGO((\'a\', (2, 5), \'c\'))\n        with self.assertRaises(KeyError):\n            index.append((2, 5))\n\n\n    def test_index_go_d(self) -> None:\n\n        index = IndexGO((), loc_is_iloc=True)\n        index.append(0)\n        self.assertTrue(index._map is None)\n\n        index.append(1)\n        self.assertTrue(1 in index)\n        self.assertFalse(\'a\' in index)\n        self.assertTrue(index._map is None)\n\n        index.append(\'a\')\n        self.assertFalse(index._map is None)\n        self.assertTrue(\'a\' in index)\n        self.assertTrue(1 in index)\n\n\n\n    def test_index_go_e(self) -> None:\n\n        index = IndexGO((), loc_is_iloc=True)\n        index.append(0)\n        self.assertTrue(index._map is None)\n\n        index.append(1)\n        self.assertTrue(1 in index)\n        self.assertFalse(\'a\' in index)\n        self.assertTrue(index._map is None)\n\n        index.append(-1)\n        self.assertFalse(index._map is None)\n        self.assertTrue(-1 in index)\n        self.assertTrue(1 in index)\n\n\n\n\n    #---------------------------------------------------------------------------\n\n\n    def test_index_sort_a(self) -> None:\n\n        index = Index((\'a\', \'c\', \'d\', \'e\', \'b\'))\n        self.assertEqual(\n                [index.sort().loc_to_iloc(x) for x in sorted(index.values)],\n                [0, 1, 2, 3, 4])\n        self.assertEqual(\n                [index.sort(ascending=False).loc_to_iloc(x) for x in sorted(index.values)],\n                [4, 3, 2, 1, 0])\n\n    def test_index_relable(self) -> None:\n\n        index = Index((\'a\', \'c\', \'d\', \'e\', \'b\'))\n\n        self.assertEqual(\n                index.relabel(lambda x: x.upper()).values.tolist(),\n                [\'A\', \'C\', \'D\', \'E\', \'B\'])\n\n        self.assertEqual(\n                index.relabel(lambda x: \'pre_\' + x.upper()).values.tolist(),\n                [\'pre_A\', \'pre_C\', \'pre_D\', \'pre_E\', \'pre_B\'])\n\n\n        # letter to number\n        s1 = Series(range(5), index=index.values)\n\n        self.assertEqual(\n                index.relabel(s1).values.tolist(),\n                [0, 1, 2, 3, 4]\n                )\n\n        self.assertEqual(index.relabel({\'e\': \'E\'}).values.tolist(),\n                [\'a\', \'c\', \'d\', \'E\', \'b\'])\n\n\n\n\n    def test_index_tuples_a(self) -> None:\n\n        index = Index([(\'a\',\'b\'), (\'b\',\'c\'), (\'c\',\'d\')])\n        s1 = Series(range(3), index=index)\n\n        self.assertEqual(s1[(\'b\', \'c\'):].values.tolist(), [1, 2])  # type: ignore  # https://github.com/python/typeshed/pull/3024\n\n        self.assertEqual(s1[[(\'b\', \'c\'), (\'a\', \'b\')]].values.tolist(), [1, 0])\n\n        self.assertEqual(s1[(\'b\', \'c\')], 1)\n        self.assertEqual(s1[(\'c\', \'d\')], 2)\n\n        s2 = Series(range(10), index=((1, x) for x in range(10)))\n        self.assertEqual(s2[(1, 5):].values.tolist(),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                [5, 6, 7, 8, 9])\n\n        self.assertEqual(s2[[(1, 7), (1, 5), (1, 0)]].values.tolist(),\n                [7, 5, 0])\n\n\n    def test_index_pickle_a(self) -> None:\n        a = Index([(\'a\',\'b\'), (\'b\',\'c\'), (\'c\',\'d\')])\n        b = Index([1, 2, 3, 4])\n        c = IndexYear.from_date_range(\'2014-12-15\', \'2018-03-15\')\n\n        for index in (a, b, c):\n            pbytes = pickle.dumps(index)\n            index_new = pickle.loads(pbytes)\n            for v in index: # iter labels\n                # import ipdb; ipdb.set_trace()\n                # this compares Index objects\n                self.assertFalse(index_new._labels.flags.writeable)\n                self.assertEqual(index_new.loc[v], index.loc[v])\n\n    def test_index_drop_a(self) -> None:\n\n        index = Index(list(\'abcdefg\'))\n\n        self.assertEqual(index._drop_loc(\'d\').values.tolist(),\n                [\'a\', \'b\', \'c\', \'e\', \'f\', \'g\'])\n\n        self.assertEqual(index._drop_loc([\'a\', \'g\']).values.tolist(),\n                [\'b\', \'c\', \'d\', \'e\', \'f\'])\n\n        self.assertEqual(index._drop_loc(slice(\'b\', None)).values.tolist(),\n                [\'a\'])\n\n\n    #---------------------------------------------------------------------------\n    def test_index_iloc_loc_to_iloc_a(self) -> None:\n\n        idx = Index((\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(idx.loc_to_iloc(ILoc[1]), 1)\n        self.assertEqual(idx.loc_to_iloc(ILoc[[0, 2]]), [0, 2])\n\n    def test_index_extract_iloc_a(self) -> None:\n\n        idx = IndexGO((\'a\', \'b\', \'c\', \'d\'))\n        idx.append(\'e\')\n        post = idx._extract_iloc(slice(None))\n        self.assertEqual(post.values.tolist(), #type: ignore\n                [\'a\', \'b\', \'c\', \'d\', \'e\'])\n\n\n    def test_index_drop_iloc_a(self) -> None:\n\n        idx = IndexGO((\'a\', \'b\', \'c\', \'d\'))\n        idx.append(\'e\')\n        post = idx._drop_iloc([1, 2])\n        self.assertEqual(post.values.tolist(),\n                [\'a\', \'d\', \'e\'])\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_loc_to_iloc_boolen_a(self) -> None:\n\n        idx = Index((\'a\', \'b\', \'c\', \'d\'))\n\n        # unlike Pandas, both of these presently fail\n        with self.assertRaises(KeyError):\n            idx.loc_to_iloc([False, True])\n\n        with self.assertRaises(KeyError):\n            idx.loc_to_iloc([False, True, False, True])\n\n        # but a Boolean array works\n        post = idx.loc_to_iloc(np.array([False, True, False, True]))\n        assert isinstance(post, np.ndarray)\n        self.assertEqual(post.tolist(), [1, 3])\n\n\n    def test_index_loc_to_iloc_boolen_b(self) -> None:\n\n        idx = Index((\'a\', \'b\', \'c\', \'d\'))\n\n        # returns nothing as index does not match anything\n        post = idx.loc_to_iloc(Series([False, True, False, True]))\n        self.assertTrue(len(tp.cast(tp.Sized, post)) == 0)\n\n        post = idx.loc_to_iloc(Series([False, True, False, True],\n                index=(\'b\', \'c\', \'d\', \'a\')))\n        assert isinstance(post, np.ndarray)\n        self.assertEqual(post.tolist(), [0, 2])\n\n        post = idx.loc_to_iloc(Series([False, True, False, True],\n                index=list(\'abcd\')))\n        assert isinstance(post, np.ndarray)\n        self.assertEqual(post.tolist(), [1,3])\n\n\n    def test_index_drop_b(self) -> None:\n\n        idx = Index((\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(idx.drop.iloc[2].values.tolist(), [\'a\', \'b\', \'d\']) #type: ignore\n        self.assertEqual(idx.drop.iloc[2:].values.tolist(), [\'a\', \'b\']) #type: ignore\n        self.assertEqual(\n                idx.drop.iloc[np.array([True, False, False, True])].values.tolist(), #type: ignore\n                [\'b\', \'c\'])\n\n    def test_index_drop_c(self) -> None:\n\n        idx = Index((\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(idx.drop.loc[\'c\'].values.tolist(), [\'a\', \'b\', \'d\']) #type: ignore\n        self.assertEqual(idx.drop.loc[\'b\':\'c\'].values.tolist(), [\'a\', \'d\'])  # type: ignore  # https://github.com/python/typeshed/pull/3024\n\n        self.assertEqual(\n                idx.drop.loc[np.array([True, False, False, True])].values.tolist(), #type: ignore\n                [\'b\', \'c\']\n                )\n\n    def test_index_roll_a(self) -> None:\n\n        idx = Index((\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(idx.roll(-2).values.tolist(),\n                [\'c\', \'d\', \'a\', \'b\'])\n\n        self.assertEqual(idx.roll(1).values.tolist(),\n                [\'d\', \'a\', \'b\', \'c\'])\n\n\n    def test_index_attributes_a(self) -> None:\n        idx = Index((\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(idx.shape, (4,))\n        self.assertEqual(idx.dtype.kind, \'U\')\n        self.assertEqual(idx.ndim, 1)\n        self.assertEqual(idx.nbytes, 16)\n\n\n    def test_index_name_a(self) -> None:\n\n        idx1 = Index((\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        self.assertEqual(idx1.name, \'foo\')\n        self.assertEqual(idx1.names, (\'foo\',))\n\n        idx2 = idx1.rename(\'bar\')\n        self.assertEqual(idx2.name, \'bar\')\n        self.assertEqual(idx2.names, (\'bar\',))\n\n    def test_name_b(self) -> None:\n\n        with self.assertRaises(TypeError):\n            Index((\'a\', \'b\', \'c\', \'d\'), name=[\'x\', \'y\']) #type: ignore\n\n        with self.assertRaises(TypeError):\n            Index((\'a\', \'b\', \'c\', \'d\'), name={\'x\', \'y\'}) #type: ignore\n\n\n    def test_index_name_c(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        self.assertEqual(idx1.name, \'foo\')\n\n        idx2 = idx1.rename(\'bar\')\n        self.assertEqual(idx2.name, \'bar\')\n\n        idx1.append(\'e\')\n        idx2.append(\'x\')\n\n        self.assertEqual(idx1.values.tolist(),\n                [\'a\', \'b\', \'c\', \'d\', \'e\'])\n\n        self.assertEqual(idx2.values.tolist(),\n                [\'a\', \'b\', \'c\', \'d\', \'x\'])\n\n    #---------------------------------------------------------------------------\n    def test_index_to_series_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        s1 = idx1.to_series()\n        self.assertFalse(s1.values.flags.writeable)\n        self.assertEqual(s1.to_pairs(),\n                ((0, \'a\'), (1, \'b\'), (2, \'c\'), (3, \'d\'))\n                )\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_to_pandas_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        pdidx = idx1.to_pandas()\n        self.assertEqual(pdidx.name, idx1.name)\n        self.assertTrue((pdidx.values == idx1.values).all())\n\n\n    def test_index_to_pandas_b(self) -> None:\n        import pandas\n        idx1 = IndexDate((\'2018-01-01\', \'2018-06-01\'), name=\'foo\')\n        pdidx = idx1.to_pandas()\n        self.assertEqual(pdidx.name, idx1.name)\n        self.assertTrue((pdidx.values == idx1.values).all())\n        self.assertTrue(pdidx[1].__class__ == pandas.Timestamp)\n\n    #---------------------------------------------------------------------------\n\n\n    def test_index_from_pandas_a(self) -> None:\n        import pandas\n\n        pdidx = pandas.Index(list(\'abcd\'))\n        idx = Index.from_pandas(pdidx)\n        self.assertEqual(idx.values.tolist(), [\'a\', \'b\', \'c\', \'d\'])\n\n\n    def test_index_from_pandas_b(self) -> None:\n        import pandas\n\n        pdidx = pandas.DatetimeIndex((\'2018-01-01\', \'2018-06-01\'), name=\'foo\')\n        idx = IndexDate.from_pandas(pdidx)\n        self.assertEqual(idx.values.tolist(),\n                [datetime.date(2018, 1, 1), datetime.date(2018, 6, 1)])\n\n    def test_index_from_pandas_c(self) -> None:\n        import pandas\n\n        pdidx = pandas.DatetimeIndex((\'2018-01-01\', \'2018-06-01\'), name=\'foo\')\n        idx = IndexDateGO.from_pandas(pdidx)\n        self.assertFalse(idx.STATIC)\n        self.assertEqual(idx.values.tolist(),\n                [datetime.date(2018, 1, 1), datetime.date(2018, 6, 1)]\n                )\n\n\n    def test_index_from_pandas_d(self) -> None:\n        import pandas\n        pdidx = pandas.DatetimeIndex((\'2018-01-01\', \'2018-06-01\'), name=\'foo\')\n        idx = Index.from_pandas(pdidx)\n        self.assertEqual(\n                idx.values.tolist(),\n                [1514764800000000000, 1527811200000000000]\n                )\n\n    def test_index_from_pandas_e(self) -> None:\n        import pandas\n        idx = pandas.DatetimeIndex([datetime.date(2014, 12, i) for i in range(1, 10)])\n        index1 = Index.from_pandas(idx)\n        self.assertTrue(index1.STATIC)\n        self.assertEqual(index1.values.tolist(),\n                [1417392000000000000, 1417478400000000000, 1417564800000000000, 1417651200000000000, 1417737600000000000, 1417824000000000000, 1417910400000000000, 1417996800000000000, 1418083200000000000]\n                )\n        index2 = IndexGO.from_pandas(idx)\n        self.assertFalse(index2.STATIC)\n        self.assertEqual(index2.values.tolist(),\n                [1417392000000000000, 1417478400000000000, 1417564800000000000, 1417651200000000000, 1417737600000000000, 1417824000000000000, 1417910400000000000, 1417996800000000000, 1418083200000000000]\n                )\n\n\n    #---------------------------------------------------------------------------\n    def test_index_iter_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\'), name=\'foo\')\n        idx1.append(\'d\')\n        self.assertEqual(list(idx1), [\'a\', \'b\', \'c\', \'d\'])\n\n    #---------------------------------------------------------------------------\n\n    def test_index_reversed_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\'), name=\'foo\')\n        idx1.append(\'d\')\n        self.assertEqual(list(reversed(idx1)), [\'d\', \'c\', \'b\', \'a\'])\n\n    def test_index_reversed_b(self) -> None:\n\n        labels = tuple(\'acdeb\')\n        index = Index(labels=labels)\n        index_reversed_generator = reversed(index)\n        self.assertEqual(\n            tuple(index_reversed_generator),\n            tuple(reversed(labels)))\n\n    #---------------------------------------------------------------------------\n\n    def test_index_iter_label_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        self.assertEqual(list(idx1.iter_label(0)), [\'a\', \'b\', \'c\', \'d\'])\n\n        post = idx1.iter_label(0).apply(lambda x: x.upper())\n        self.assertEqual(post.to_pairs(),\n                ((0, \'A\'), (1, \'B\'), (2, \'C\'), (3, \'D\')))\n\n\n    #---------------------------------------------------------------------------\n    def test_index_intersection_a(self) -> None:\n\n        idx1 = Index((\'a\', \'b\', \'c\', \'d\', \'e\'))\n\n        a1 = np.array([\'c\', \'dd\', \'b\', \'a\'])\n\n        idx2 = idx1.intersection(a1)\n\n        self.assertEqual(idx2.values.tolist(),\n                [\'a\', \'b\', \'c\'])\n\n\n    def test_index_intersection_b(self) -> None:\n\n        idx1 = Index((\'c\', \'b\', \'a\'))\n        idx2 = Index((\'c\', \'b\', \'a\'))\n\n        idx3 = idx1.intersection(idx2)\n        self.assertEqual(idx3.values.tolist(),\n                [\'c\', \'b\', \'a\']\n                )\n\n    def test_index_intersection_c(self) -> None:\n        idx1 = Index((10, 20))\n        idx2 = idx1.intersection(Series([20, 30]))\n        self.assertEqual(idx2.values.tolist(), [20])\n\n\n    def test_index_intersection_d(self) -> None:\n        idx1 = Index((10, 20))\n        with self.assertRaises(NotImplementedError):\n            idx2 = idx1.intersection(\'b\') #type: ignore\n\n    #---------------------------------------------------------------------------\n\n    def test_index_union_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\', \'d\', \'e\'))\n        idx1.append(\'f\')\n        a1 = np.array([\'c\', \'dd\', \'b\', \'a\'])\n\n        idx2 = idx1.union(a1)\n\n        self.assertEqual(idx2.values.tolist(),\n                [\'a\', \'b\', \'c\', \'d\', \'dd\', \'e\', \'f\'])\n\n\n    def test_index_union_b(self) -> None:\n\n        idx1 = Index((\'c\', \'b\', \'a\'))\n        idx2 = Index((\'c\', \'b\', \'a\'))\n\n        idx3 = idx1.union(idx2)\n        self.assertEqual(idx3.values.tolist(),\n                [\'c\', \'b\', \'a\']\n                )\n\n    def test_index_difference_a(self) -> None:\n        idx1 = Index((\'c\', \'b\', \'a\'))\n        idx2 = Index((\'c\', \'b\', \'a\'))\n\n        idx3 = idx1.difference(idx2)\n        self.assertEqual(idx3.values.tolist(), [])\n\n    def test_index_difference_b(self) -> None:\n        idx1 = Index(())\n        idx2 = Index((\'c\', \'b\', \'a\'))\n\n        idx3 = idx1.difference(idx2)\n        self.assertEqual(idx3.values.tolist(), [])\n\n        idx4 = Index((\'c\', \'b\', \'a\'))\n        idx5 = Index(())\n\n        idx6 = idx4.difference(idx5)\n        self.assertEqual(idx6.values.tolist(),\n                [\'c\', \'b\', \'a\']\n                )\n\n    def test_index_difference_c(self) -> None:\n        obj = object()\n        idx1 = Index((1, None, \'3\', np.nan, 4.4, obj)) # type: ignore\n        idx2 = Index((2, 3, \'4\', \'five\', 6.6, object()))\n\n        idx3 = idx1.difference(idx2)\n        self.assertEqual(set(idx3.values.tolist()),\n                set([np.nan, 1, 4.4, obj, \'3\', None])\n                ) # Note: order is lost...\n\n    def test_index_difference_d(self) -> None:\n        obj = object()\n        idx1 = Index((1, None, \'3\', np.nan, 4.4, obj)) # type: ignore\n        idx2 = Index((2, 1, \'3\', \'five\', object()))\n\n        idx3 = idx1.difference(idx2)\n        self.assertEqual(set(idx3.values.tolist()),\n                set([np.nan, None, 4.4, obj])\n                ) # Note: order is lost...\n\n\n\n    def test_index_to_html_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\'))\n        self.assertEqual(idx1.to_html(),\n                \'<table border=""1""><tbody><tr><td>a</td></tr><tr><td>b</td></tr><tr><td>c</td></tr></tbody></table>\')\n\n    def test_index_to_html_datatables_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\'))\n\n        sio = StringIO()\n\n        post = idx1.to_html_datatables(sio, show=False)\n\n        self.assertEqual(post, None)\n        self.assertTrue(len(sio.read()) > 1200)\n\n\n    def test_index_empty_a(self) -> None:\n        idx1 = Index(())\n        idx2 = Index(iter(()))\n        self.assertEqual(idx1.dtype, idx2.dtype)\n\n    def test_index_cumprod_a(self) -> None:\n        idx1 = IndexGO(range(1, 11, 2))\n\n        # sum applies to the labels\n        self.assertEqual(idx1.sum(), 25)\n\n        self.assertEqual(\n                idx1.cumprod().tolist(),\n                [1, 3, 15, 105, 945]\n                )\n\n    #---------------------------------------------------------------------------\n\n    def test_index_label_widths_at_depth_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\', \'d\', \'e\'))\n        self.assertEqual(tuple(idx1.label_widths_at_depth(0)),\n            ((\'a\', 1), (\'b\', 1), (\'c\', 1), (\'d\', 1), (\'e\', 1))\n            )\n\n\n    def test_index_label_widths_at_depth_b(self) -> None:\n        idx1 = IndexGO((\'a\', \'b\', \'c\', \'d\', \'e\'))\n        with self.assertRaises(RuntimeError):\n            next(idx1.label_widths_at_depth(1))\n\n    #---------------------------------------------------------------------------\n\n    def test_index_bool_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\', \'d\', \'e\'))\n        self.assertTrue(bool(idx1))\n\n        idx2 = IndexGO(())\n        self.assertFalse(bool(idx2))\n\n    def test_index_bool_b(self) -> None:\n\n        idx1 = IndexGO(())\n        self.assertFalse(bool(idx1))\n        idx1.append(\'a\')\n        self.assertTrue(bool(idx1))\n\n    def test_index_astype_a(self) -> None:\n\n        idx1 = Index((3, 10, 50))\n        self.assertEqual(idx1.astype(float).values.dtype, np.dtype(float))\n\n    #---------------------------------------------------------------------------\n\n    def test_index_initializer_needs_init(self) -> None:\n        self.assertEqual(_index_initializer_needs_init(None), False)\n        self.assertEqual(_index_initializer_needs_init(Index((1, 2))), False)\n\n        self.assertEqual(_index_initializer_needs_init(np.arange(0)), False)\n        self.assertEqual(_index_initializer_needs_init(np.arange(3)), True)\n\n        self.assertEqual(_index_initializer_needs_init(()), False)\n        self.assertEqual(_index_initializer_needs_init((3, 5)), True)\n\n    #---------------------------------------------------------------------------\n\n    def test_index_values_at_depth_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\'))\n        self.assertEqual(idx1.values_at_depth(0).tolist(),\n                [\'a\', \'b\', \'c\'])\n        with self.assertRaises(RuntimeError):\n            idx1.values_at_depth(1)\n\n    #---------------------------------------------------------------------------\n\n    def test_index_head_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\', \'d\', \'e\'))\n        self.assertEqual(idx1.head(2).values.tolist(), [\'a\' ,\'b\'])\n\n    def test_index_tail_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\', \'d\', \'e\'))\n        self.assertEqual(idx1.tail(2).values.tolist(), [\'d\' ,\'e\'])\n\n\n    #---------------------------------------------------------------------------\n    def test_index_via_str_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\', \'d\', \'e\'))\n        a1 = idx1.via_str.upper()\n\n        self.assertEqual(a1.tolist(),\n                [\'A\', \'B\', \'C\', \'D\', \'E\']\n                )\n\n\n    def test_index_via_dt_a(self) -> None:\n\n        idx1 = IndexDate((\'2020-01-01\', \'2021-02-05\', \'2019-03-17\'))\n\n        self.assertEqual(idx1.via_dt.day.tolist(),\n                [1, 5, 17]\n                )\n\n        self.assertEqual(idx1.via_dt.weekday().tolist(),\n                [2, 4, 6]\n                )\n\n    #---------------------------------------------------------------------------\n\n    def test_index_equals_a(self) -> None:\n\n        idx1 = IndexGO((\'a\', \'b\', \'c\', \'d\', \'e\'))\n        idx2 = Index((\'a\', \'b\', \'c\', \'d\', \'e\'))\n        idx3 = IndexGO((\'a\', \'b\', \'c\', \'d\', \'e\'), name=\'foo\')\n        idx4 = IndexGO((\'a\', \'b\', \'c\', \'d\', \'e\'))\n        idx5 = IndexGO((\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(idx1.equals(3), False)\n        self.assertEqual(idx1.equals(False), False)\n        self.assertEqual(idx1.equals([3, 4, 5]), False)\n\n        self.assertEqual(idx1.equals(idx2, compare_class=True), False)\n        self.assertEqual(idx1.equals(idx2, compare_class=False), True)\n\n        self.assertEqual(idx1.equals(idx3, compare_name=True), False)\n        self.assertEqual(idx1.equals(idx3, compare_name=False), True)\n        self.assertEqual(idx1.equals(idx5), False)\n\n        self.assertEqual(idx1.equals(idx1), True)\n        self.assertEqual(idx1.equals(idx4), True)\n\n\n    def test_index_equals_b(self) -> None:\n\n        idx1 = Index((5, 3, 20), dtype=np.int64)\n        idx2 = Index((5, 3, 20), dtype=np.int32)\n\n        self.assertFalse(idx1.equals(idx2, compare_dtype=True))\n        self.assertTrue(idx1.equals(idx2, compare_dtype=False))\n\n\n    def test_index_equals_c(self) -> None:\n\n        idx1 = IndexDate.from_year_range(\'2010\', \'2011\')\n        idx2 = Index(idx1.values)\n\n        self.assertFalse(idx1.equals(idx2, compare_class=True))\n        self.assertTrue(idx1.equals(idx2, compare_class=False),)\n\n    def test_index_equals_d(self) -> None:\n\n        idx1 = Index((5, 3, np.nan))\n        idx2 = Index((5, 3, np.nan))\n        idx3 = Index((5, 3, None)) #type: ignore\n\n        self.assertTrue(idx1.equals(idx2))\n        self.assertFalse(idx1.equals(idx2, skipna=False))\n\n        # nan and None are not treated equivalent, even with skipna true\n        self.assertFalse(idx1.equals(idx3, compare_dtype=False, skipna=True))\n\n\n    def test_index_equals_e(self) -> None:\n        a = Index([1, 2, 3])\n        b = Index([1, 2, 3])\n        c = Index([1, 3, 2])\n        d = Index([1, 2, 3, 4])\n        e = Index([\'a\', 2, 3])\n\n        self.assertFalse(not a.equals(b))\n        self.assertTrue(not a.equals(c))\n        self.assertTrue(not a.equals(c))\n        self.assertTrue(not a.equals(d))\n        self.assertTrue(not a.equals(e))\n\n\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n\n'"
static_frame/test/unit/test_index_auto.py,0,"b""\nimport unittest\n\nfrom static_frame.core.index import Index\nfrom static_frame.core.index import IndexGO\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.core.index_auto import IndexAutoFactory\n\nclass TestUnit(TestCase):\n\n    def test_index_auto_factory_a(self) -> None:\n\n        idx1 = IndexAutoFactory.from_optional_constructor(4,\n                default_constructor=Index)\n        self.assertEqual(idx1._map is None, True) #type: ignore\n        self.assertEqual(len(idx1), 4)\n        self.assertEqual(idx1.STATIC, True)\n\n    def test_index_auto_factory_b(self) -> None:\n\n        idx1 = IndexAutoFactory.from_optional_constructor(8,\n                default_constructor=IndexGO)\n        self.assertEqual(idx1._map is None, True) #type: ignore\n        self.assertEqual(len(idx1), 8)\n        self.assertEqual(idx1.STATIC, False)\n\n        # go funcitonality\n        assert isinstance(idx1, IndexGO)\n        idx1.append(8)\n        self.assertEqual(idx1._map is None, True)\n        self.assertEqual(len(idx1), 9)\n\n\n    def test_index_auto_factory_c(self) -> None:\n\n        idx1 = IndexAutoFactory.from_optional_constructor(5,\n                default_constructor=IndexGO,\n                explicit_constructor=Index)\n        # when using an alternate constructor, loc_is_iloc will not be set\n        self.assertEqual(idx1._map is None, False) #type: ignore\n        self.assertEqual(len(idx1), 5)\n        self.assertEqual(idx1.STATIC, True)\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/unit/test_index_base.py,2,"b""\nimport unittest\nimport numpy as np\n# import typing as tp\n\nfrom static_frame.core.index_base import IndexBase\n\n# from static_frame.core.exception import ErrorInitIndex\nfrom static_frame.test.test_case import TestCase\n\n\n\nclass TestUnit(TestCase):\n\n    def test_index_base_slotted_a(self) -> None:\n        idx1 = IndexBase()\n\n        with self.assertRaises(AttributeError):\n            idx1.g = 30 # type: ignore #pylint: disable=E0237\n        with self.assertRaises(AttributeError):\n            idx1.__dict__ #pylint: disable=W0104\n\n    def test_index_base_not_implemented(self) -> None:\n\n        idx1 = IndexBase()\n\n        with self.assertRaises(NotImplementedError):\n            idx1._ufunc_axis_skipna(axis=0,\n                    skipna=False,\n                    ufunc=np.sum,\n                    ufunc_skipna=np.nansum,\n                    composable=True,\n                    dtypes=(),\n                    size_one_unity=True)\n\n\n        with self.assertRaises(NotImplementedError):\n            idx1._update_array_cache()\n\n        with self.assertRaises(NotImplementedError):\n            idx1.copy()\n\n        with self.assertRaises(NotImplementedError):\n            idx1.copy()\n\n        with self.assertRaises(NotImplementedError):\n            idx1.display()\n\n        with self.assertRaises(NotImplementedError):\n            idx1.from_labels(())\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n"""
static_frame/test/unit/test_index_correspondence.py,1,"b""\n\n\n\n\nimport unittest\n\n\nimport numpy as np\n\n\nfrom static_frame.core.index_correspondence import IndexCorrespondence\nfrom static_frame.core.index import Index\n\nfrom static_frame.test.test_case import TestCase\n\n\nclass TestUnit(TestCase):\n\n\n    def test_index_correspondence_a(self) -> None:\n        idx0 = Index([0, 1, 2, 3, 4], loc_is_iloc=True)\n        idx1 = Index([0, 1, 2, 3, 4, '100185', '100828', '101376', '100312', '101092'], dtype=object)\n        ic = IndexCorrespondence.from_correspondence(idx0, idx1)\n        self.assertFalse(ic.is_subset)\n        self.assertTrue(ic.has_common)\n        # this is an array, due to loc_is_iloc being True\n        assert isinstance(ic.iloc_src, np.ndarray)\n        self.assertEqual(ic.iloc_src.tolist(),\n                [0, 1, 2, 3, 4]\n                )\n        self.assertEqual(ic.iloc_dst,\n                [0, 1, 2, 3, 4]\n                )\n\n\n    def test_index_correspondence_b(self) -> None:\n        # issue found with a hypothesis test\n\n        idx = Index([False], loc_is_iloc=False)\n        ic = IndexCorrespondence.from_correspondence(idx, idx)\n        self.assertTrue(ic.is_subset)\n        self.assertTrue(ic.has_common)\n        self.assertEqual(ic.size, 1)\n        self.assertEqual(ic.iloc_src, [0]) # this is as list in this use case\n        self.assertEqual(ic.iloc_dst.tolist(), [0]) # type: ignore\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/unit/test_index_datetime.py,64,"b""\nimport unittest\nimport numpy as np\nimport datetime\nfrom  itertools import product\n\nfrom static_frame import Index\nfrom static_frame import IndexGO\n\n# from static_frame import IndexHierarchy\nfrom static_frame import Series\nfrom static_frame import Frame\n\nfrom static_frame import IndexYear\nfrom static_frame import IndexYearGO\n\nfrom static_frame import IndexYearMonth\nfrom static_frame import IndexYearMonthGO\n\nfrom static_frame import IndexDate\nfrom static_frame import IndexDateGO\n\nfrom static_frame import IndexSecond\nfrom static_frame import IndexSecondGO\n\nfrom static_frame import IndexHour\nfrom static_frame import IndexHourGO\n\nfrom static_frame import IndexMinute\nfrom static_frame import IndexMinuteGO\n\nfrom static_frame import IndexMillisecond\nfrom static_frame import IndexMillisecondGO\n\nfrom static_frame import IndexMicrosecond\nfrom static_frame import IndexMicrosecondGO\n\nfrom static_frame import IndexNanosecond\nfrom static_frame import IndexNanosecondGO\n\n# from static_frame import HLoc\n# from static_frame import ILoc\nfrom static_frame.core.index import _INDEX_SLOTS\nfrom static_frame.core.index import _INDEX_GO_SLOTS\nfrom static_frame.core.index_datetime import _dtype_to_index_cls\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.core.exception import LocInvalid\nfrom static_frame.core.exception import ErrorInitIndex\n\n\nclass TestUnit(TestCase):\n\n\n    def test_index_datetime_go_config(self) -> None:\n\n        for base, base_go in (\n                (IndexYear, IndexYearGO),\n                (IndexYearMonth, IndexYearMonthGO),\n                (IndexDate, IndexDateGO),\n                (IndexMinute, IndexMinuteGO),\n                (IndexSecond, IndexSecondGO),\n                (IndexMillisecond, IndexMillisecondGO),\n                (IndexNanosecond, IndexNanosecondGO),\n                ):\n            self.assertEqual(base._MUTABLE_CONSTRUCTOR, base_go)\n            self.assertEqual(base_go._IMMUTABLE_CONSTRUCTOR, base)\n            self.assertEqual(base.STATIC, True)\n            self.assertEqual(base_go.STATIC, False)\n            self.assertEqual(len(base.__slots__), len(_INDEX_SLOTS))\n            self.assertEqual(len(base_go.__slots__), len(_INDEX_GO_SLOTS))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_date_a(self) -> None:\n\n        index = IndexDate.from_date_range('2018-01-01', '2018-03-01')\n        self.assertEqual(index.values[0], np.datetime64('2018-01-01'))\n        self.assertEqual(index.values[-1], np.datetime64('2018-03-01'))\n        self.assertEqual(index.loc['2018-02-22'],\n                np.datetime64('2018-02-22'))\n\n\n    def test_index_date_b(self) -> None:\n\n        with self.assertRaises(Exception):\n            IndexDate([3,4,5], dtype=np.int64)  #type: ignore #pylint: disable=E1123\n\n        idx1 = IndexDate(['2017', '2018'])\n        self.assertTrue(idx1[0].__class__ == np.datetime64)\n        self.assertEqual(idx1.loc_to_iloc('2018-01-01'), 1)\n\n        idx2 = IndexDate(['2017-01', '2018-07'])\n        self.assertTrue(idx2[0].__class__ == np.datetime64)\n        self.assertEqual(idx2.loc['2017-01-01'],\n                np.datetime64('2017-01-01'))\n\n    def test_index_date_c(self) -> None:\n        index = IndexDate.from_date_range('2017-12-15', '2018-03-15', 2)\n\n        self.assertEqual((index == '2017').sum(), 9)\n        self.assertEqual((index == '2018-02').sum(), 14)\n        self.assertEqual((index == '2018').sum(), 37)\n\n    def test_index_date_d(self) -> None:\n        index = IndexDate.from_date_range('2017-12-15', '2018-03-15', 2)\n        # selct by year and year month\n        self.assertAlmostEqualValues(index.loc['2017'].values, #type: ignore\n                np.array(['2017-12-15', '2017-12-17', '2017-12-19', '2017-12-21',\n               '2017-12-23', '2017-12-25', '2017-12-27', '2017-12-29',\n               '2017-12-31'], dtype='datetime64[D]'))\n\n        self.assertAlmostEqualValues(index.loc['2018-02'].values, #type: ignore\n                np.array(['2018-02-01', '2018-02-03', '2018-02-05', '2018-02-07',\n               '2018-02-09', '2018-02-11', '2018-02-13', '2018-02-15',\n               '2018-02-17', '2018-02-19', '2018-02-21', '2018-02-23',\n               '2018-02-25', '2018-02-27'], dtype='datetime64[D]'))\n\n        self.assertEqual(index.loc['2018-02-19'],\n                np.datetime64('2018-02-19'))\n\n\n    def test_index_date_e(self) -> None:\n        index = IndexDate.from_date_range('2017-12-15', '2018-03-15', 2)\n\n        post = index + np.timedelta64(2, 'D')\n\n        self.assertEqual(post[0], np.datetime64('2017-12-17'))\n\n\n    def test_index_date_f(self) -> None:\n        index = IndexDate.from_date_range('2017-12-15', '2018-01-15')\n\n        post = index + datetime.timedelta(days=10)\n\n        self.assertEqual(post[0], np.datetime64('2017-12-25'))\n        self.assertEqual(post[-1], np.datetime64('2018-01-25'))\n\n\n    def test_index_date_g(self) -> None:\n        index = IndexDate.from_date_range('2017-12-15', '2018-02-15')\n\n        post = index.loc['2018':'2018-01']  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(len(post), 31)\n        self.assertEqual(post[0], np.datetime64('2018-01-01')) #type: ignore\n        self.assertEqual(post[-1], np.datetime64('2018-01-31')) #type: ignore\n\n\n    def test_index_date_h(self) -> None:\n        index = IndexDate.from_date_range('2017-12-15', '2018-02-15')\n\n        post = index.loc['2018':'2018-01-15']  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(len(post), 15)\n        self.assertEqual(post[0], np.datetime64('2018-01-01')) #type: ignore\n        self.assertEqual(post[-1], np.datetime64('2018-01-15')) #type: ignore\n\n\n    def test_index_date_i(self) -> None:\n        index = IndexDate.from_date_range('2017-11-15', '2018-02-15')\n\n        post = index.loc['2017-12': '2018-01']  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(len(post), 62)\n        self.assertEqual(post[0], np.datetime64('2017-12-01')) #type: ignore\n        self.assertEqual(post[-1], np.datetime64('2018-01-31')) #type: ignore\n\n\n    def test_index_date_j(self) -> None:\n        index = IndexDate.from_date_range('2017-11-15', '2018-02-15')\n\n        post = index.loc['2017-12': '2018']  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(len(post), 77)\n        self.assertEqual(post[0], np.datetime64('2017-12-01')) #type: ignore\n        self.assertEqual(post[-1], np.datetime64('2018-02-15')) #type: ignore\n\n\n    def test_index_date_k(self) -> None:\n        index = IndexDate.from_date_range('2017-11-15', '2018-02-15')\n        post = index.loc[['2017-12-10', '2018-02-06']]\n        self.assertEqual(len(post), 2)\n        self.assertEqual(post[0], np.datetime64('2017-12-10')) #type: ignore\n        self.assertEqual(post[-1], np.datetime64('2018-02-06')) #type: ignore\n\n\n    def test_index_date_m(self) -> None:\n        index = IndexDate.from_date_range('2017-11-15', '2018-02-15')\n        # NOTE: this type of selection should possibly not be permitted\n        post = index.loc[['2017', '2018']]\n        self.assertEqual(len(post), 93)\n        self.assertEqual(post[0], np.datetime64('2017-11-15')) #type: ignore\n        self.assertEqual(post[-1], np.datetime64('2018-02-15')) #type: ignore\n\n    def test_index_date_n(self) -> None:\n        index = IndexDate.from_date_range('2017-11-15', '2018-02-15')\n        # NOTE: this type of selection should possibly not be permitted\n        post = index.loc[['2017-12', '2018-02']]\n        self.assertEqual(len(post), 46)\n        self.assertEqual(post[0], np.datetime64('2017-12-01')) #type: ignore\n        self.assertEqual(post[-1], np.datetime64('2018-02-15')) #type: ignore\n        self.assertEqual(\n            set(post.values.astype('datetime64[M]')), #type: ignore\n            {np.datetime64('2018-02'), np.datetime64('2017-12')}\n            )\n\n\n    def test_index_date_o(self) -> None:\n        index = IndexDate.from_year_month_range('2017-12', '2018-01')\n        s1 = Series(range(len(index)), index=index)\n        self.assertEqual(s1['2018-01':].shape, (31,)) # type: ignore\n        self.assertEqual(s1[datetime.date(2018, 1, 15):].shape, (17,)) # type: ignore\n        self.assertEqual(s1['2016':].shape, (0,)) # type: ignore\n        self.assertEqual(s1['2017':].shape, (62,)) # type: ignore\n        self.assertEqual(s1['2019':].shape, (0,)) # type: ignore\n\n\n    def test_index_date_p(self) -> None:\n        index = IndexDate.from_year_month_range('2017-12', '2018-01')\n        s1 = Series(range(len(index)), index=index)\n\n        with self.assertRaises(LocInvalid):\n            # out of range end date\n            s1['2017-12-28':'2019-01-04'] # type: ignore #pylint: disable=W0104\n\n        with self.assertRaises(LocInvalid):\n            # out of range start date\n            s1['2016-01-01':'2018-01-04'] # type: ignore #pylint: disable=W0104\n\n\n    def test_index_date_q(self) -> None:\n        index = IndexDate(('2017-12-30', '2017-12-31', '2018-01-05'))\n        s1 = Series(range(len(index)), index=index)\n        # a range beyond the observed values cannot determine a match,\n        self.assertEqual(s1[:'2019'].shape, (0,)) # type: ignore\n        self.assertEqual(s1['2016':].shape, (0,)) # type: ignore\n\n\n    #---------------------------------------------------------------------------\n    def test_index_datetime_init_a(self) -> None:\n\n        dates = [datetime.date(*x) for x in product((2017,), (4,5,), range(1, 4))]\n        s1 = Series(range(len(dates)), index=IndexDate(dates))\n\n        with self.assertRaises(ErrorInitIndex):\n            index = IndexYearMonth(s1.index)\n\n        with self.assertRaises(ErrorInitIndex):\n            index = IndexYear(s1.index) #type: ignore\n\n        # can reuse the map if going from dt64 index to normal index\n        idx2 = Index(s1.index)\n        self.assertTrue(id(idx2._map) == id(s1.index._map))\n\n        idx3 = IndexDate(idx2)\n        self.assertTrue(id(idx3._map) == id(s1.index._map))\n\n        with self.assertRaises(ErrorInitIndex):\n            index = IndexYear(idx3) #type: ignore\n\n        # from a date to a finer resolution has to create a new map\n        idx4 = IndexMinute(idx3)\n        self.assertTrue(id(idx4._map) != id(s1.index._map))\n\n        # a GO has to create a new map\n        idx5 = IndexGO(s1.index)\n        self.assertTrue(id(idx4._map) != id(s1.index._map))\n\n        # supplying a dtype to coerce the labels\n        with self.assertRaises(ErrorInitIndex):\n            idx6 = Index(s1.index, dtype='datetime64[Y]')\n\n        with self.assertRaises(ErrorInitIndex):\n            idx7 = Index(s1.index.values.astype('datetime64[Y]'))\n\n        # final resolution from a normal index\n        idx8 = IndexMinute(idx2)\n        self.assertTrue(id(idx8._map) != id(idx2._map))\n\n\n    #---------------------------------------------------------------------------\n    def test_index_date_from_year_month_range_a(self) -> None:\n        index = IndexDate.from_year_month_range('2017-12', '2018-03')\n\n        self.assertEqual((index == '2017').sum(), 31)\n        self.assertEqual((index == '2018').sum(), 90)\n\n        self.assertEqual(\n            [str(d) for d in np.unique(index.values.astype('datetime64[M]'))],\n            ['2017-12', '2018-01', '2018-02', '2018-03'])\n\n\n    def test_index_date_from_year_range_a(self) -> None:\n        index = IndexDate.from_year_range('2016', '2018')\n        self.assertEqual(len(index), 1096)\n        self.assertEqual(\n                [str(d) for d in np.unique(index.values.astype('datetime64[Y]'))],\n                ['2016', '2017', '2018'])\n\n        index = IndexDate.from_year_range('2016', '2018', 2)\n        self.assertEqual(len(index), 548)\n        self.assertEqual(\n                [str(d) for d in np.unique(index.values.astype('datetime64[Y]'))],\n                ['2016', '2017', '2018'])\n\n\n    def test_index_date_series_a(self) -> None:\n\n        s = Series(range(62),\n                index=IndexDate.from_year_month_range('2017-12', '2018-01'))\n\n        self.assertEqual(s.sum(), 1891)\n        self.assertEqual(s.loc[s.index == '2018-01'].sum(), 1426)\n        self.assertEqual(s.loc[s.index == '2017-12'].sum(), 465)\n\n        self.assertEqual(s['2018-01-24'], 54)\n\n        self.assertEqual(\n                s['2018-01-28':].to_pairs(),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                ((np.datetime64('2018-01-28'), 58), (np.datetime64('2018-01-29'), 59), (np.datetime64('2018-01-30'), 60), (np.datetime64('2018-01-31'), 61))\n                )\n\n\n    def test_index_year_month_a(self) -> None:\n        idx1 = IndexYearMonth(('2018-01', '2018-06'))\n\n        self.assertEqual(idx1.values.tolist(),\n            [datetime.date(2018, 1, 1), datetime.date(2018, 6, 1)])\n\n\n    def test_index_year_month_b(self) -> None:\n        idx1 = IndexYearMonth(('2017-12', '2018-01', '2018-02', '2018-03', '2018-04'))\n\n        post1 = idx1.loc[np.datetime64('2018-02'):]\n        self.assertEqual(\n                post1.values.tolist(), #type: ignore\n                [datetime.date(2018, 2, 1), datetime.date(2018, 3, 1), datetime.date(2018, 4, 1)]\n                )\n\n        # a year datetime64\n        post2 = idx1.loc[np.datetime64('2018'):]\n        self.assertEqual(\n                post2.values.tolist(), #type: ignore\n                [datetime.date(2018, 1, 1), datetime.date(2018, 2, 1), datetime.date(2018, 3, 1), datetime.date(2018, 4, 1)]\n                )\n\n\n    def test_index_year_month_from_date_range_a(self) -> None:\n        index = IndexYearMonth.from_date_range('2017-12-15', '2018-03-15')\n        self.assertEqual(len(index), 4)\n\n        index = IndexYearMonth.from_date_range('2017-12-15', '2018-03-15', 2)\n        self.assertEqual(len(index), 2)\n\n    def test_index_year_month_from_year_month_range_a(self) -> None:\n\n        index = IndexYearMonth.from_year_month_range(\n                '2017-12-15', '2018-03-15')\n        self.assertAlmostEqualValues(index.values,\n                np.array(['2017-12', '2018-01', '2018-02', '2018-03'],\n                dtype='datetime64[M]'))\n\n        index = IndexYearMonth.from_year_month_range('2017-12', '2018-03')\n        self.assertEqual(len(index), 4)\n\n        self.assertEqual([str(d) for d in index.values],\n                ['2017-12', '2018-01', '2018-02', '2018-03'])\n\n        index = IndexYearMonth.from_year_month_range('2017-12', '2018-03', step=2)\n        self.assertEqual([str(d) for d in index], ['2017-12', '2018-02'])\n\n\n    def test_index_year_month_from_year_range_a(self) -> None:\n\n        index = IndexYearMonth.from_year_range('2010', '2018')\n\n        self.assertEqual(len(index), 108)\n        self.assertEqual(str(index.min()), '2010-01')\n        self.assertEqual(str(index.max()), '2018-12')\n\n        index = IndexYearMonth.from_year_range('2010', '2018', 6)\n\n        self.assertEqual(\n                [str(d) for d in IndexYearMonth.from_year_range('2010', '2018', 6)],\n                ['2010-01', '2010-07', '2011-01', '2011-07', '2012-01', '2012-07', '2013-01', '2013-07', '2014-01', '2014-07', '2015-01', '2015-07', '2016-01', '2016-07', '2017-01', '2017-07', '2018-01', '2018-07'])\n\n\n    def test_index_year_from_date_range_a(self) -> None:\n\n        index = IndexYear.from_date_range('2014-12-15', '2018-03-15')\n        self.assertEqual(len(index), 5)\n\n        index = IndexYear.from_date_range('2014-12-15', '2018-03-15', step=2)\n        self.assertEqual([str(d) for d in index.values],\n                ['2014', '2016', '2018'])\n\n\n    def test_index_year_from_year_month_range_a(self) -> None:\n\n        index = IndexYear.from_year_month_range('2014-12', '2018-03')\n        self.assertEqual(len(index), 5)\n\n\n    def test_index_year_from_year_range_a(self) -> None:\n\n        index = IndexYear.from_year_range('2010', '2018')\n        self.assertEqual(len(index), 9)\n\n\n    def test_index_year_from_year_range_b(self) -> None:\n\n        index = IndexYearGO.from_year_range('2010', '2018')\n        self.assertEqual(len(index), 9)\n        index.append('2019')\n        self.assertEqual(len(index), 10)\n        index.append('2020')\n        self.assertTrue('2020' in index)\n        self.assertTrue(len(index.__slots__), 9)\n\n        with self.assertRaises(RuntimeError):\n            index.append(np.datetime64('2009-03-01'))\n\n        index.append(np.datetime64('2009'))\n\n        # will strip off year from a datetime object\n        index.append(datetime.date(2021, 3, 15))\n\n        self.assertEqual(index.values.tolist(),\n                [datetime.date(2010, 1, 1), datetime.date(2011, 1, 1), datetime.date(2012, 1, 1), datetime.date(2013, 1, 1), datetime.date(2014, 1, 1), datetime.date(2015, 1, 1), datetime.date(2016, 1, 1), datetime.date(2017, 1, 1), datetime.date(2018, 1, 1), datetime.date(2019, 1, 1), datetime.date(2020, 1, 1), datetime.date(2009, 1, 1), datetime.date(2021, 1, 1)]\n                )\n\n    def test_index_date_loc_to_iloc_a(self) -> None:\n\n        index = IndexDate.from_date_range('2018-01-01', '2018-03-01')\n\n        self.assertEqual(\n                index.loc_to_iloc(np.datetime64('2018-02-11')),\n                41)\n\n        self.assertEqual(index.loc_to_iloc('2018-02-11'), 41)\n\n        self.assertEqual(\n                index.loc_to_iloc(slice('2018-02-11', '2018-02-24')),\n                slice(41, 55, None))\n\n\n\n\n\n    def test_index_millisecond_a(self) -> None:\n\n        msg = '''2016-04-28 04:22:12.226\n2016-04-28 16:29:21.32\n2016-04-28 17:36:13.733\n2016-04-30 20:21:07.848\n2016-05-01 00:00:33.483\n2016-05-01 03:02:03.584\n2016-05-01 09:26:43.185\n2016-05-01 13:45:22.576\n2016-05-01 15:25:46.15'''\n\n        idx = IndexMillisecond(msg.split('\\n'))\n        self.assertEqual(str(idx.dtype), 'datetime64[ms]')\n\n        self.assertEqual(idx.loc['2016-04-30T20:21:07.848'],\n                np.datetime64('2016-04-30T20:21:07.848'))\n\n        self.assertAlmostEqualValues(\n                idx.loc['2016-05-01T09:26:43.185':].values,  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                np.array(['2016-05-01T09:26:43.185', '2016-05-01T13:45:22.576',\n       '2016-05-01T15:25:46.150'], dtype='datetime64[ms]'))\n\n        self.assertAlmostEqualValues(idx.loc['2016-05'].values, #type: ignore\n                np.array(['2016-05-01T00:00:33.483', '2016-05-01T03:02:03.584',\n               '2016-05-01T09:26:43.185', '2016-05-01T13:45:22.576',\n               '2016-05-01T15:25:46.150'], dtype='datetime64[ms]')\n                )\n\n        self.assertEqual(idx.loc['2016-05-01T00'].values, #type: ignore\n                np.array(['2016-05-01T00:00:33.483'], dtype='datetime64[ms]'))\n\n\n\n\n    def test_index_millisecond_b(self) -> None:\n        # integer arguments are interpreted as milliseconds from the epoch\n        idx = IndexMillisecond(range(10))\n        self.assertAlmostEqualValues(idx.loc['1970-01-01T00:00:00.007':].values,  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                np.array(['1970-01-01T00:00:00.007', '1970-01-01T00:00:00.008',\n               '1970-01-01T00:00:00.009'], dtype='datetime64[ms]'))\n\n\n    def test_index_second_a(self) -> None:\n        # integer arguments are interpreted as seconds from the epoch\n        idx = IndexSecond(range(10))\n        self.assertAlmostEqualValues(idx.loc['1970-01-01T00:00:07':].values,  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                np.array(['1970-01-01T00:00:07', '1970-01-01T00:00:08',\n               '1970-01-01T00:00:09'], dtype='datetime64[s]')\n                )\n\n\n    def test_index_millisecond_series_a(self) -> None:\n\n        msg = '''2016-04-28 04:22:12.226\n2016-04-28 16:29:21.32\n2016-04-28 17:36:13.733\n2016-04-30 20:21:07.848\n2016-05-01 00:00:33.483\n2016-05-01 03:02:03.584\n2016-05-01 09:26:43.185\n2016-05-01 13:45:22.576\n2016-05-01 15:25:46.15'''\n\n        idx = IndexMillisecond(msg.split('\\n'))\n        s = Series(range(9), index=idx)\n\n        self.assertEqual(s['2016-05-01T00:00:33.483'], 4)\n\n        self.assertEqual(s['2016-05-01T00:00:33.483':].values.tolist(),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                [4, 5, 6, 7, 8])\n\n        self.assertEqual(s['2016-05'].to_pairs(),\n                ((np.datetime64('2016-05-01T00:00:33.483'), 4), (np.datetime64('2016-05-01T03:02:03.584'), 5), (np.datetime64('2016-05-01T09:26:43.185'), 6), (np.datetime64('2016-05-01T13:45:22.576'), 7), (np.datetime64('2016-05-01T15:25:46.150'), 8)))\n\n        self.assertEqual(s['2016-05-01T09'].to_pairs(),\n                ((np.datetime64('2016-05-01T09:26:43.185'), 6),))\n\n\n    def test_index_millisecond_frame_a(self) -> None:\n\n        msg = '''2016-04-28 04:22:12.226\n2016-04-28 16:29:21.32\n2016-04-28 17:36:13.733\n2016-04-30 20:21:07.848\n2016-05-01 00:00:33.483\n2016-05-01 03:02:03.584\n2016-05-01 09:26:43.185\n2016-05-01 13:45:22.576\n2016-05-01 15:25:46.15'''\n\n        f = Frame.from_records((x, y) for x, y in enumerate(msg.split('\\n')))\n\n        idx1 = IndexMillisecond(f[1])\n        self.assertAlmostEqualValues(idx1.values,\n                np.array(['2016-04-28T04:22:12.226', '2016-04-28T16:29:21.320',\n               '2016-04-28T17:36:13.733', '2016-04-30T20:21:07.848',\n               '2016-05-01T00:00:33.483', '2016-05-01T03:02:03.584',\n               '2016-05-01T09:26:43.185', '2016-05-01T13:45:22.576',\n               '2016-05-01T15:25:46.150'], dtype='datetime64[ms]'))\n\n\n        idx2 = IndexSecond(f[1])\n\n        self.assertAlmostEqualValues(idx2.values,\n            np.array(['2016-04-28T04:22:12', '2016-04-28T16:29:21',\n           '2016-04-28T17:36:13', '2016-04-30T20:21:07',\n           '2016-05-01T00:00:33', '2016-05-01T03:02:03',\n           '2016-05-01T09:26:43', '2016-05-01T13:45:22',\n           '2016-05-01T15:25:46'], dtype='datetime64[s]'))\n\n\n        f2 = f.set_index(1, index_constructor=IndexMillisecond)\n        self.assertEqual(f2.loc['2016-05', 0].values.tolist(),\n                [4, 5, 6, 7, 8])\n\n\n    def test_index_minute_a(self) -> None:\n\n        idx1 = IndexMinute(('2018-01-01T03:30', '2018-01-01T03:45', '2019-01-02T03:45'))\n\n        self.assertEqual(idx1.loc['2019'].values.tolist(), #type: ignore\n                [datetime.datetime(2019, 1, 2, 3, 45)]\n                )\n\n        self.assertEqual(idx1.loc['2018-01'].values.tolist(), #type: ignore\n                [datetime.datetime(2018, 1, 1, 3, 30), datetime.datetime(2018, 1, 1, 3, 45)])\n\n\n    def test_index_nanosecond_a(self) -> None:\n\n        idx1 = IndexNanosecond(('2018-01-01T03:30', '2018-01-01T03:45', '2019-01-02T03:45'))\n        self.assertTrue(len(idx1.loc['2019']), 1)\n        self.assertTrue(len(idx1.loc['2018']), 2)\n\n        # NP reduces nanoseconds to integers\n        self.assertEqual(idx1.values.tolist(),\n                [1514777400000000000, 1514778300000000000, 1546400700000000000])\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_datetime_binary_operator_a(self) -> None:\n        index = IndexDateGO.from_date_range('2018-03-12', '2018-03-15')\n        index.append('2018-03-16')\n\n        self.assertEqual((index + 2).tolist(),\n                [datetime.date(2018, 3, 14), datetime.date(2018, 3, 15), datetime.date(2018, 3, 16), datetime.date(2018, 3, 17), datetime.date(2018, 3, 18)])\n\n        with self.assertRaises(NotImplementedError):\n            _ = index @ []\n\n\n    def test_index_datetime_binary_operator_b(self) -> None:\n        index = IndexDateGO.from_date_range('2018-03-12', '2018-03-14')\n        a1 = index + Index((1, 2, 3))\n        self.assertEqual(a1.tolist(),\n                [datetime.date(2018, 3, 13), datetime.date(2018, 3, 15), datetime.date(2018, 3, 17)])\n\n\n    #---------------------------------------------------------------------------\n    def test_index_datetime_append_a(self) -> None:\n        index = IndexDateGO.from_date_range('2018-03-12', '2018-03-14')\n        with self.assertRaises(KeyError):\n            index.append('2018-03-12')\n\n        index.append('2018-03-11')\n\n        self.assertEqual(index.values.tolist(),\n                [datetime.date(2018, 3, 12),\n                datetime.date(2018, 3, 13),\n                datetime.date(2018, 3, 14),\n                datetime.date(2018, 3, 11)])\n\n\n    def test_index_datetime_to_pandas_a(self) -> None:\n\n        for cls in (IndexYear, IndexYearMonth):\n            index = cls.from_year_range('2010', '2018') #type: ignore\n            with self.assertRaises(NotImplementedError):\n                _ = index.to_pandas()\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_datetime_nanosecond_a(self) -> None:\n        index1 = IndexNanosecond(('2020-01-01', '2020-02-01'))\n        index2 = IndexYearMonth(index1)\n        self.assertEqual(\n                index2.values.tolist(),\n                [datetime.date(2020, 1, 1), datetime.date(2020, 2, 1)]\n                )\n\n\n\n    #---------------------------------------------------------------------------\n    def test_index_datetime_hour_a(self) -> None:\n        index1 = IndexHour(('2020-01-01', '2020-02-01'))\n        self.assertEqual(index1.dtype, np.dtype('<M8[h]'))\n        index2 = IndexYearMonth(index1)\n        self.assertEqual(\n                index2.values.tolist(),\n                [datetime.date(2020, 1, 1), datetime.date(2020, 2, 1)]\n                )\n\n    def test_index_datetime_hour_b(self) -> None:\n        index1 = IndexHourGO(('2020-01-01', '2020-02-01'))\n        index1.append('2020-03-01')\n        index2 = IndexYearMonth(index1)\n        self.assertEqual(\n                index2.values.tolist(),\n                [datetime.date(2020, 1, 1), datetime.date(2020, 2, 1), datetime.date(2020, 3, 1)]\n                )\n\n    #---------------------------------------------------------------------------\n    def test_index_datetime_microsecond_a(self) -> None:\n        index1 = IndexMicrosecond(('2020-01-01', '2020-02-01'))\n        self.assertEqual(index1.dtype, np.dtype('<M8[us]'))\n        index2 = IndexYearMonth(index1)\n        self.assertEqual(\n                index2.values.tolist(),\n                [datetime.date(2020, 1, 1), datetime.date(2020, 2, 1)]\n                )\n\n    def test_index_datetime_microsecond_b(self) -> None:\n        index1 = IndexMicrosecondGO(('2020-01-01', '2020-02-01'))\n        index1.append('2020-03-01')\n        self.assertEqual(index1.dtype, np.dtype('<M8[us]'))\n        index2 = IndexYearMonth(index1)\n        self.assertEqual(\n                index2.values.tolist(),\n                [datetime.date(2020, 1, 1), datetime.date(2020, 2, 1), datetime.date(2020, 3, 1)]\n                )\n\n    #---------------------------------------------------------------------------\n    def test_dtype_to_index_cls_a(self) -> None:\n        t1 = _dtype_to_index_cls(True, np.dtype('datetime64[D]'))\n        self.assertEqual(t1, IndexDate)\n\n        t2 = _dtype_to_index_cls(False, np.dtype('datetime64[D]'))\n        self.assertEqual(t2, IndexDateGO)\n\n        t3 = _dtype_to_index_cls(True, np.dtype('datetime64[s]'))\n        self.assertEqual(t3, IndexSecond)\n\n        t4 = _dtype_to_index_cls(False, np.dtype('datetime64[s]'))\n        self.assertEqual(t4, IndexSecondGO)\n\n        t5 = _dtype_to_index_cls(True, np.dtype('datetime64[Y]'))\n        self.assertEqual(t5, IndexYear)\n\n        t6 = _dtype_to_index_cls(False, np.dtype('datetime64[Y]'))\n        self.assertEqual(t6, IndexYearGO)\n\n    def test_dtype_to_index_cls_b(self) -> None:\n        t1 = _dtype_to_index_cls(True, np.dtype(str))\n        self.assertEqual(t1, Index)\n\n        t2 = _dtype_to_index_cls(False, np.dtype(str))\n        self.assertEqual(t2, IndexGO)\n\n        t3 = _dtype_to_index_cls(True, np.dtype(float))\n        self.assertEqual(t3, Index)\n\n        t4 = _dtype_to_index_cls(False, np.dtype(float))\n        self.assertEqual(t4, IndexGO)\n\n\n    #---------------------------------------------------------------------------\n    def test_index_datetime_astype_a(self) -> None:\n\n        idx1 = IndexDate(('2020-01-01', '2022-05-10'))\n\n        self.assertEqual(\n            idx1.astype('datetime64[ns]').__class__, IndexNanosecond\n            )\n\n        self.assertEqual(\n            idx1.astype('datetime64[Y]').__class__, IndexYear\n            )\n        self.assertEqual(\n            idx1.astype(str).__class__, Index\n            )\n\n    def test_index_datetime_astype_b(self) -> None:\n\n        idx1 = IndexDateGO(('2020-01-01', '2022-05-10'))\n\n        self.assertEqual(\n            idx1.astype('datetime64[ns]').__class__, IndexNanosecondGO\n            )\n        self.assertEqual(\n            idx1.astype('datetime64[Y]').__class__, IndexYearGO\n            )\n        self.assertEqual(\n            idx1.astype(str).__class__, IndexGO\n            )\n\n    #---------------------------------------------------------------------------\n\n    def test_index_datetime_from_year_month_range(self) -> None:\n\n        date_min, date_max = (np.datetime64('2007-02'), np.datetime64('2020-04'))\n\n        idx1 = IndexYearMonth.from_year_month_range(date_min, date_max)\n        self.assertEqual(len(idx1), 159)\n\n        idx2 = IndexYear.from_year_month_range(date_min, date_max)\n        self.assertEqual(len(idx2), 14)\n\n        idx3 = IndexDate.from_year_month_range(date_min, date_max)\n        self.assertEqual(len(idx3), 4838)\n\n    def test_index_datetime_from_date_range(self) -> None:\n\n        date_min, date_max = (np.datetime64('2007-02'), np.datetime64('2020-04'))\n\n        # we reject year-mos when calling from_date_range\n        with self.assertRaises(RuntimeError):\n            _ = IndexYearMonth.from_date_range(date_min, date_max)\n\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n\n"""
static_frame/test/unit/test_index_hierarchy.py,23,"b'\nimport unittest\nimport pickle\nimport datetime\nimport numpy as np\n\nfrom collections import OrderedDict\n\nfrom static_frame import Index\n# from static_frame import IndexGO\nfrom static_frame import IndexDate\nfrom static_frame import Series\nfrom static_frame import Frame\nfrom static_frame import FrameGO\n# from static_frame import IndexYearMonth\n# from static_frame import IndexYear\nfrom static_frame import DisplayConfig\n\nfrom static_frame import IndexHierarchy\nfrom static_frame import IndexHierarchyGO\nfrom static_frame import IndexLevel\nfrom static_frame import IndexYearMonth\n\n# from static_frame import IndexLevelGO\nfrom static_frame import HLoc\nfrom static_frame.core.array_go import ArrayGO\nfrom static_frame.core.exception import ErrorInitIndex\n\nfrom static_frame.test.test_case import temp_file\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.test.test_case import skip_win\n\n\nclass TestUnit(TestCase):\n\n    def test_hierarchy_slotted_a(self) -> None:\n\n        labels = ((\'I\', \'A\'),\n                (\'I\', \'B\'),\n                )\n        ih1 = IndexHierarchy.from_labels(labels, name=\'foo\')\n\n        with self.assertRaises(AttributeError):\n            ih1.g = 30 # type: ignore #pylint: disable=E0237\n        with self.assertRaises(AttributeError):\n            ih1.__dict__ #pylint: disable=W0104\n\n    def test_hierarchy_init_a(self) -> None:\n\n        labels = ((\'I\', \'A\'),\n                (\'I\', \'B\'),\n                )\n\n        ih1 = IndexHierarchy.from_labels(labels, name=\'foo\')\n        ih2 = IndexHierarchy(ih1)\n        self.assertEqual(ih1.name, \'foo\')\n        self.assertEqual(ih2.name, \'foo\')\n\n\n    def test_hierarchy_init_b(self) -> None:\n\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'B\'),\n                (\'I\', \'C\')\n                )\n\n        with self.assertRaises(RuntimeError):\n            ih1 = IndexHierarchy.from_labels(labels)\n\n    def test_hierarchy_init_c(self) -> None:\n\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'B\'),\n                (\'III\', \'B\'),\n                (\'III\', \'A\')\n                )\n\n        ih1 = IndexHierarchy.from_labels(labels)\n        self.assertEqual(ih1.values.tolist(),\n            [[\'I\', \'A\'], [\'I\', \'B\'], [\'II\', \'B\'], [\'III\', \'B\'], [\'III\', \'A\']])\n\n\n    def test_hierarchy_init_d(self) -> None:\n\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'B\'),\n                (\'III\', \'B\'),\n                (\'III\', \'B\')\n                )\n        with self.assertRaises(ErrorInitIndex):\n            ih1 = IndexHierarchy.from_labels(labels)\n\n    def test_hierarchy_init_e(self) -> None:\n\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'B\'),\n                (\'III\', \'B\'),\n                (\'I\', \'B\'),\n                )\n\n        with self.assertRaises(RuntimeError):\n            ih1 = IndexHierarchy.from_labels(labels)\n\n\n\n    def test_hierarchy_init_f(self) -> None:\n\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'B\'),\n                (\'III\', \'B\'),\n                (\'I\', \'B\'),\n                )\n\n        with self.assertRaises(RuntimeError):\n            ih1 = IndexHierarchy.from_labels(labels)\n\n    def test_hierarchy_init_g(self) -> None:\n\n        labels = (\n                (\'I\', \'A\', 1),\n                (\'I\', \'B\', 1),\n                (\'II\', \'A\', 1),\n                (\'II\', \'A\', 2),\n                (\'II\', \'A\', 1),\n                )\n        with self.assertRaises(ErrorInitIndex):\n            ih1 = IndexHierarchy.from_labels(labels)\n\n    def test_hierarchy_init_h(self) -> None:\n\n        labels = (\n                (\'I\', \'A\', 1),\n                (\'I\', \'B\', 1),\n                (\'II\', \'A\', 1),\n                (\'II\', \'A\', 2),\n                (\'II\', \'B\', 1),\n                (\'II\', \'A\', 3),\n                )\n        with self.assertRaises(RuntimeError):\n            ih1 = IndexHierarchy.from_labels(labels)\n\n\n    def test_hierarchy_init_i(self) -> None:\n        with self.assertRaises(RuntimeError):\n            ih1 = IndexHierarchy((3,))  # type: ignore\n\n    def test_hierarchy_init_j(self) -> None:\n\n        labels = ((\'I\', \'A\'),\n                (\'I\', \'B\'),\n                )\n\n        ih1 = IndexHierarchy.from_labels(labels, name=(\'a\', \'b\', \'c\'))\n\n        # can access as a .name, but not a .names\n        self.assertEqual(ih1.name, (\'a\', \'b\', \'c\'))\n        # names does not use name as it is the wrong size\n        self.assertEqual(ih1.names, (\'__index0__\', \'__index1__\'))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_hierarchy_loc_to_iloc_a(self) -> None:\n\n\n        groups = Index((\'A\', \'B\', \'C\'))\n        dates = IndexDate.from_date_range(\'2018-01-01\', \'2018-01-04\')\n        observations = Index((\'x\', \'y\'))\n\n\n        lvl2a = IndexLevel(index=observations)\n        lvl2b = IndexLevel(index=observations, offset=2)\n        lvl2c = IndexLevel(index=observations, offset=4)\n        lvl2d = IndexLevel(index=observations, offset=6)\n        lvl2_targets = ArrayGO((lvl2a, lvl2b, lvl2c, lvl2d))\n\n\n        lvl1a = IndexLevel(index=dates,\n                targets=lvl2_targets, offset=0)\n        lvl1b = IndexLevel(index=dates,\n                targets=lvl2_targets, offset=len(lvl1a))\n        lvl1c = IndexLevel(index=dates,\n                targets=lvl2_targets, offset=len(lvl1a) * 2)\n\n        # we need as many targets as len(index)\n        lvl0 = IndexLevel(index=groups,\n                targets=ArrayGO((lvl1a, lvl1b, lvl1c)))\n\n\n        self.assertEqual(len(lvl2a), 2)\n        self.assertEqual(len(lvl1a), 8)\n        self.assertEqual(len(lvl0), 24)\n\n        self.assertEqual(list(lvl2a.depths()),\n                [1])\n        self.assertEqual(list(lvl1a.depths()),\n                [2, 2, 2, 2])\n        self.assertEqual(list(lvl0.depths()),\n                [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n\n        ih = IndexHierarchy(lvl0)\n\n        self.assertEqual(len(ih), 24)\n\n        post = ih.loc_to_iloc(HLoc[\n                [\'A\', \'B\', \'C\'],\n                slice(\'2018-01-01\', \'2018-01-04\'),\n                [\'x\', \'y\']])\n        # this will break if we recognize this can be a slice\n        self.assertEqual(post, list(range(len(ih))))\n\n        post = ih.loc_to_iloc(HLoc[\n                [\'A\', \'B\', \'C\'],\n                slice(\'2018-01-01\', \'2018-01-04\'),\n                \'x\'])\n\n        self.assertEqual(post, list(range(0, len(ih), 2)))\n\n        post = ih.loc_to_iloc(HLoc[\n                \'C\',\n                \'2018-01-03\',\n                \'y\'])\n\n        self.assertEqual(post, 21)\n\n        post = ih.loc_to_iloc(HLoc[\'B\', \'2018-01-03\':, \'y\'])  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(post, [13, 15])\n\n\n        post = ih.loc_to_iloc(HLoc[[\'B\', \'C\'], \'2018-01-03\'])\n        self.assertEqual(post, [12, 13, 20, 21])\n\n        post = ih.loc_to_iloc(HLoc[[\'A\', \'C\'], :, \'y\'])\n        self.assertEqual(post, [1, 3, 5, 7, 17, 19, 21, 23])\n\n        post = ih.loc_to_iloc(HLoc[[\'A\', \'C\'], :, \'x\'])\n        self.assertEqual(post, [0, 2, 4, 6, 16, 18, 20, 22])\n\n\n\n    def test_hierarchy_loc_to_iloc_b(self) -> None:\n        OD = OrderedDict\n        tree = OD([\n                (\'I\', OD([\n                        (\'A\', (1, 2)), (\'B\', (1, 2, 3)), (\'C\', (2, 3))\n                        ])\n                ),\n                (\'II\', OD([\n                        (\'A\', (1, 2, 3)), (\'B\', (1,))\n                        ])\n                ),\n                ])\n\n        ih = IndexHierarchy.from_tree(tree)\n\n        post = ih.loc_to_iloc(HLoc[\'I\', \'B\', 1])\n        self.assertEqual(post, 2)\n\n        post = ih.loc_to_iloc(HLoc[\'I\', \'B\', 3])\n        self.assertEqual(post, 4)\n\n        post = ih.loc_to_iloc(HLoc[\'II\', \'A\', 3])\n        self.assertEqual(post, 9)\n\n        post = ih.loc_to_iloc(HLoc[\'II\', \'A\'])\n        self.assertEqual(post, slice(7, 10))\n\n        post = ih.loc_to_iloc(HLoc[\'I\', \'C\'])\n        self.assertEqual(post, slice(5, 7))\n\n\n        post = ih.loc_to_iloc(HLoc[\'I\', [\'A\', \'C\']])\n        self.assertEqual(post, [0, 1, 5, 6])\n\n\n        post = ih.loc_to_iloc(HLoc[:, \'A\', :])\n        self.assertEqual(post, [0, 1, 7, 8, 9])\n\n\n        post = ih.loc_to_iloc(HLoc[:, \'C\', 3])\n        self.assertEqual(post, [6])\n\n        post = ih.loc_to_iloc(HLoc[:, :, 3])\n        self.assertEqual(post, [4, 6, 9])\n\n        post = ih.loc_to_iloc(HLoc[:, :, 1])\n        self.assertEqual(post, [0, 2, 7, 10])\n\n        # TODO: not sure what to do when a multiple selection, [1, 2], is a superset of the leaf index; we do not match with a normal loc\n        # ih.loc_to_iloc((slice(None), slice(None), [1,2]))\n\n\n    def test_hierarchy_loc_to_iloc_c(self) -> None:\n        OD = OrderedDict\n        tree = OD([\n                (\'I\', OD([\n                        (\'A\', (1, 2)), (\'B\', (1, 2, 3)), (\'C\', (2, 3))\n                        ])\n                ),\n                (\'II\', OD([\n                        (\'A\', (1, 2, 3)), (\'B\', (1,))\n                        ])\n                ),\n                ])\n\n        ih = IndexHierarchy.from_tree(tree)\n\n        # TODO: add additional validaton\n        post = ih.loc[(\'I\', \'B\', 2): (\'II\', \'A\', 2)]  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertTrue(len(post), 6)\n\n        post = ih.loc[[(\'I\', \'B\', 2), (\'II\', \'A\', 2)]]\n        self.assertTrue(len(post), 2)\n\n\n    def test_hierarchy_loc_to_iloc_d(self) -> None:\n\n        labels = (\n                (\'I\', \'A\', 1),\n                (\'I\', \'B\', 1),\n                (\'II\', \'A\', 1),\n                (\'II\', \'A\', 2),\n                (\'II\', \'B\', 1),\n                (\'II\', \'B\', 2),\n                )\n\n        ih = IndexHierarchy.from_labels(labels)\n\n        # selection with an Index objext\n        iloc1 = ih.loc_to_iloc(Index((labels[2], labels[5])))\n        self.assertEqual(iloc1, [2, 5])\n\n        iloc2 = ih.loc_to_iloc(Index(labels))\n        self.assertEqual(iloc2, [0, 1, 2, 3, 4, 5])\n\n\n\n    def test_hierarchy_loc_to_iloc_e(self) -> None:\n\n        labels = (\n                (\'I\', \'A\', 1),\n                (\'I\', \'B\', 1),\n                (\'II\', \'A\', 1),\n                (\'II\', \'A\', 2),\n                (\'II\', \'B\', 1),\n                (\'II\', \'B\', 2),\n                )\n\n        ih1 = IndexHierarchy.from_labels(labels)\n\n        ih2 = IndexHierarchy.from_labels(labels[:3])\n        ih3 = IndexHierarchy.from_labels(labels[-3:])\n\n        # selection with an IndexHierarchy\n        self.assertEqual(ih1.loc_to_iloc(ih2), [0, 1, 2])\n        self.assertEqual(ih1.loc_to_iloc(ih3), [3, 4, 5])\n\n\n\n    def test_hierarchy_loc_to_iloc_f(self) -> None:\n\n        labels = (\n                (\'I\', \'A\', 1),\n                (\'I\', \'B\', 1),\n                (\'II\', \'A\', 1),\n                (\'II\', \'A\', 2),\n                (\'II\', \'B\', 1),\n                (\'II\', \'B\', 2),\n                )\n\n        ih1 = IndexHierarchy.from_labels(labels)\n\n        # selection with Boolean and non-Bolean Series\n        a1 = ih1.loc_to_iloc(Series((True, True), index=(labels[1], labels[4])))\n        self.assertEqual(a1.tolist(), [False, True, False, False, True, False]) #type: ignore\n\n        a2 = ih1.loc_to_iloc(Series((labels[5], labels[2], labels[4])))\n        self.assertEqual(a2, [5, 2, 4])\n\n    #---------------------------------------------------------------------------\n\n    def test_hierarchy_extract_iloc_a(self) -> None:\n\n        labels = (\n                (\'I\', \'A\', 1),\n                (\'I\', \'B\', 1),\n                (\'II\', \'A\', 1),\n                (\'II\', \'A\', 2),\n                (\'II\', \'B\', 1),\n                (\'II\', \'B\', 2),\n                )\n\n        ih1 = IndexHierarchy.from_labels(labels)\n\n        ih2 = ih1._extract_iloc(None) # will get a copy\n        self.assertTrue((ih1.values == ih2.values).all()) #type: ignore\n\n        ih3 = ih1._extract_iloc(slice(None)) # will get a copy\n        self.assertTrue((ih1.values == ih3.values).all()) #type: ignore\n        # reduces to a tuple\n        ih4 = ih1._extract_iloc(3)\n        self.assertEqual(ih4, (\'II\', \'A\', 2))\n\n    #---------------------------------------------------------------------------\n\n    def test_hierarchy_extract_getitem_astype_a(self) -> None:\n\n        labels = (\n                (\'I\', \'A\', 1),\n                (\'I\', \'B\', 1),\n                (\'II\', \'A\', 1),\n                (\'II\', \'A\', 2),\n                (\'II\', \'B\', 1),\n                (\'II\', \'B\', 2),\n                )\n\n        ih1 = IndexHierarchy.from_labels(labels)\n\n        with self.assertRaises(KeyError):\n            ih1._extract_getitem_astype((\'A\', 1))\n\n    #--------------------------------------------------------------------------\n\n    def test_hierarchy_from_product_a(self) -> None:\n\n        groups = Index((\'A\', \'B\', \'C\'))\n        dates = IndexDate.from_date_range(\'2018-01-01\', \'2018-01-04\')\n        observations = Index((\'x\', \'y\'))\n\n        ih = IndexHierarchy.from_product(groups, dates, observations)\n\n\n    def test_hierarchy_from_product_b(self) -> None:\n\n        with self.assertRaises(RuntimeError):\n            IndexHierarchy.from_product((1, 2))\n\n    #--------------------------------------------------------------------------\n\n    def test_hierarchy_from_tree_a(self) -> None:\n\n        OD = OrderedDict\n\n        tree = OD([(\'A\', (1, 2, 3, 4)), (\'B\', (1, 2))])\n\n        ih = IndexHierarchy.from_tree(tree)\n\n        self.assertEqual(ih.to_frame().to_pairs(0),\n                ((0, ((0, \'A\'), (1, \'A\'), (2, \'A\'), (3, \'A\'), (4, \'B\'), (5, \'B\'))), (1, ((0, 1), (1, 2), (2, 3), (3, 4), (4, 1), (5, 2))))\n                )\n\n\n    def test_hierarchy_from_tree_b(self) -> None:\n\n        OD = OrderedDict\n\n        tree = OD([\n                (\'I\', OD([\n                        (\'A\', (1, 2)), (\'B\', (1, 2, 3)), (\'C\', (2, 3))\n                        ])\n                ),\n                (\'II\', OD([\n                        (\'A\', (1, 2, 3)), (\'B\', (1,))\n                        ])\n                ),\n                ])\n\n        ih = IndexHierarchy.from_tree(tree)\n        self.assertEqual(ih.to_frame().to_pairs(0),\n                ((0, ((0, \'I\'), (1, \'I\'), (2, \'I\'), (3, \'I\'), (4, \'I\'), (5, \'I\'), (6, \'I\'), (7, \'II\'), (8, \'II\'), (9, \'II\'), (10, \'II\'))), (1, ((0, \'A\'), (1, \'A\'), (2, \'B\'), (3, \'B\'), (4, \'B\'), (5, \'C\'), (6, \'C\'), (7, \'A\'), (8, \'A\'), (9, \'A\'), (10, \'B\'))), (2, ((0, 1), (1, 2), (2, 1), (3, 2), (4, 3), (5, 2), (6, 3), (7, 1), (8, 2), (9, 3), (10, 1))))\n                )\n    #---------------------------------------------------------------------------\n\n    def test_hierarchy_from_labels_a(self) -> None:\n\n        labels1 = ((\'I\', \'A\', 1),\n                (\'I\', \'A\', 2),\n                (\'I\', \'B\', 1),\n                (\'I\', \'B\', 2),\n                (\'II\', \'A\', 1),\n                (\'II\', \'A\', 2),\n                (\'II\', \'B\', 1),\n                (\'II\', \'B\', 2),\n                )\n\n        ih = IndexHierarchy.from_labels(labels1)\n        self.assertEqual(len(ih), 8)\n        self.assertEqual(ih.depth, 3)\n\n        self.assertEqual([ih.loc_to_iloc(x) for x in labels1],\n                [0, 1, 2, 3, 4, 5, 6, 7])\n\n\n        labels2 = ((\'I\', \'A\', 1),\n                (\'I\', \'A\', 2),\n                (\'I\', \'B\', 1),\n                (\'II\', \'B\', 2),\n                )\n\n        ih = IndexHierarchy.from_labels(labels2)\n        self.assertEqual(len(ih), 4)\n        self.assertEqual(ih.depth, 3)\n\n        self.assertEqual([ih.loc_to_iloc(x) for x in labels2], [0, 1, 2, 3])\n\n\n    def test_hierarchy_from_labels_b(self) -> None:\n\n        labels = ((\'I\', \'A\'),\n                (\'I\', \'B\'),\n                )\n\n        ih = IndexHierarchy.from_labels(labels)\n\n        self.assertEqual(ih.to_frame().to_pairs(0),\n                ((0, ((0, \'I\'), (1, \'I\'))), (1, ((0, \'A\'), (1, \'B\')))))\n\n\n    def test_hierarchy_from_labels_c(self) -> None:\n\n        ih = IndexHierarchy.from_labels(tuple())\n        self.assertEqual(len(ih), 0)\n\n\n    def test_hierarchy_from_labels_d(self) -> None:\n\n        with self.assertRaises(RuntimeError):\n            ih = IndexHierarchy.from_labels([(3,), (4,)])\n\n\n    def test_hierarchy_from_labels_e(self) -> None:\n\n        index_constructors = (Index, IndexDate)\n\n        labels = (\n            (\'a\', \'2019-01-01\'),\n            (\'a\', \'2019-02-01\'),\n            (\'b\', \'2019-01-01\'),\n            (\'b\', \'2019-02-01\'),\n        )\n\n        with self.assertRaises(ErrorInitIndex):\n            ih = IndexHierarchy.from_labels(labels, index_constructors=(Index,))\n\n\n        ih = IndexHierarchy.from_labels(labels, index_constructors=index_constructors)\n\n        self.assertEqual(ih.loc[HLoc[:, \'2019-02\']].values.tolist(),\n                [[\'a\', datetime.date(2019, 2, 1)],\n                [\'b\', datetime.date(2019, 2, 1)]])\n\n        self.assertEqual(ih.loc[HLoc[:, \'2019\']].values.tolist(),\n                [[\'a\', datetime.date(2019, 1, 1)],\n                [\'a\', datetime.date(2019, 2, 1)],\n                [\'b\', datetime.date(2019, 1, 1)],\n                [\'b\', datetime.date(2019, 2, 1)]])\n\n        self.assertEqual(ih.loc[HLoc[:, \'2019-02-01\']].values.tolist(),\n                [[\'a\', datetime.date(2019, 2, 1)],\n                [\'b\', datetime.date(2019, 2, 1)]]\n                )\n\n    def test_hierarchy_from_labels_f(self) -> None:\n\n        labels1 = ((\'I\', \'A\', 1),\n                (\'I\', \'A\', 2),\n                (None, \'B\', 1),\n                (\'I\', None, 2),\n                (\'II\', \'A\', 1),\n                (None, \'A\', 2),\n                (None, \'B\', 1),\n                (None, \'B\', 2),\n                )\n\n        ih = IndexHierarchy.from_labels(labels1, continuation_token=None) # type: ignore\n\n        self.assertEqual(ih.values.tolist(),\n                [[\'I\', \'A\', 1], [\'I\', \'A\', 2], [\'I\', \'B\', 1], [\'I\', \'B\', 2], [\'II\', \'A\', 1], [\'II\', \'A\', 2], [\'II\', \'B\', 1], [\'II\', \'B\', 2]]\n                )\n\n    def test_hierarchy_from_labels_g(self) -> None:\n\n        labels1 = ((\'II\', \'A\', 1),\n                (\'I\', \'B\', 1),\n                (\'II\', \'B\', 2),\n                (\'I\', \'A\', 2),\n                (\'I\', \'B\', 2),\n                (\'II\', \'A\', 2),\n                (\'II\', \'B\', 1),\n                (\'I\', \'A\', 1),\n                )\n\n        with self.assertRaises(ErrorInitIndex):\n            ih1 = IndexHierarchy.from_labels(labels1)\n\n        ih2 = IndexHierarchy.from_labels(labels1, reorder_for_hierarchy=True)\n        self.assertEqual(ih2.shape, (8, 3))\n        self.assertEqual(ih2.iloc[-1], (\'I\', \'B\', 2))\n\n\n    def test_hierarchy_from_labels_h(self) -> None:\n\n        labels = (\n                (\'I\', \'A\', 1),\n                (\'II\', \'B\', 1),\n                (\'II\', \'A\', 1),\n                (\'II\', \'A\', 2),\n                (\'II\', \'A\', 3),\n                (\'I\', \'B\', 1),\n                )\n        with self.assertRaises(RuntimeError):\n            ih1 = IndexHierarchy.from_labels(labels,\n                    reorder_for_hierarchy=True,\n                    continuation_token=\'\')\n\n\n    def test_hierarchy_from_labels_i(self) -> None:\n        labels = ((\'I\', \'A\', 1),\n                (\'I\', \'A\', 2),\n                (\'I\', \'B\'),\n                (\'II\', \'B\', 2),\n                )\n        with self.assertRaises(ErrorInitIndex):\n            ih1 = IndexHierarchy.from_labels(labels)\n\n    #---------------------------------------------------------------------------\n\n    def test_hierarchy_from_index_items_a(self) -> None:\n\n        idx1 = Index((\'A\', \'B\', \'C\'))\n        idx2 = Index((\'x\', \'y\'))\n        idx3 = Index((4, 5, 6))\n\n        ih = IndexHierarchy.from_index_items(dict(a=idx1, b=idx2, c=idx3).items())\n\n        self.assertEqual(\n                ih.values.tolist(),\n                [[\'a\', \'A\'], [\'a\', \'B\'], [\'a\', \'C\'], [\'b\', \'x\'], [\'b\', \'y\'], [\'c\', 4], [\'c\', 5], [\'c\', 6]]\n                )\n\n    def test_hierarchy_from_index_items_b(self) -> None:\n\n        idx1 = Index((\'A\', \'B\', \'C\'))\n        idx2 = Index((\'x\', \'y\'))\n        idx3 = Index((4, 5, 6))\n\n        ih = IndexHierarchyGO.from_index_items(dict(a=idx1, b=idx2, c=idx3).items())\n        ih.append((\'c\', 7))\n\n        self.assertEqual(ih.values.tolist(),\n                [[\'a\', \'A\'], [\'a\', \'B\'], [\'a\', \'C\'], [\'b\', \'x\'], [\'b\', \'y\'], [\'c\', 4], [\'c\', 5], [\'c\', 6], [\'c\', 7]])\n\n\n\n    def test_hierarchy_from_labels_delimited_a(self) -> None:\n\n        labels = (""\'I\' \'A\'"", ""\'I\' \'B\'"")\n\n        ih = IndexHierarchy.from_labels_delimited(labels)\n\n        self.assertEqual(ih.values.tolist(),\n                [[\'I\', \'A\'], [\'I\', \'B\']])\n\n\n\n    def test_hierarchy_from_labels_delimited_b(self) -> None:\n\n        labels = (\n                ""\'I\' \'A\' 0"",\n                ""\'I\' \'A\' 1"",\n                ""\'I\' \'B\' 0"",\n                ""\'I\' \'B\' 1"",\n                ""\'II\' \'A\' 0"",\n                )\n\n        ih = IndexHierarchy.from_labels_delimited(labels)\n\n        self.assertEqual(ih.values.tolist(),\n                [[\'I\', \'A\', 0], [\'I\', \'A\', 1], [\'I\', \'B\', 0], [\'I\', \'B\', 1], [\'II\', \'A\', 0]]\n                )\n\n\n    def test_hierarchy_from_labels_delimited_c(self) -> None:\n\n        labels = (\n                ""[\'I\' \'A\' 0]"",\n                ""[\'I\' \'A\' 1]"",\n                ""[\'I\' \'B\' 0]"",\n                ""[\'I\' \'B\' 1]"",\n                ""[\'II\' \'A\' 0]"",\n                )\n\n        ih = IndexHierarchy.from_labels_delimited(labels)\n\n        self.assertEqual(ih.values.tolist(),\n                [[\'I\', \'A\', 0], [\'I\', \'A\', 1], [\'I\', \'B\', 0], [\'I\', \'B\', 1], [\'II\', \'A\', 0]]\n                )\n\n    #---------------------------------------------------------------------------\n\n    def test_hierarchy_from_type_blocks_a(self) -> None:\n        f1 = Frame.from_element(\'a\', index=range(3), columns=(\'a\',))\n        f2 = Frame.from_items(((\'a\', tuple(\'AABB\')), (\'b\', (1, 2, 1, 2))))\n        f3 = Frame.from_items(((\'a\', tuple(\'AABA\')), (\'b\', (1, 2, 1, 2))))\n\n        with self.assertRaises(ErrorInitIndex):\n            ih = IndexHierarchy._from_type_blocks(f1._blocks)\n\n        with self.assertRaises(ErrorInitIndex):\n            IndexHierarchy._from_type_blocks(f2._blocks, index_constructors=(IndexDate,))\n\n        with self.assertRaises(ErrorInitIndex):\n            IndexHierarchy._from_type_blocks(f3._blocks)\n\n\n    #---------------------------------------------------------------------------\n\n    def test_hierarchy_contains_a(self) -> None:\n        labels = ((\'I\', \'A\'),\n                (\'I\', \'B\'),\n                )\n        ih = IndexHierarchy.from_labels(labels)\n\n        self.assertTrue((\'I\', \'A\') in ih)\n\n\n    def test_hierarchy_extract_a(self) -> None:\n        idx = IndexHierarchy.from_product([\'A\', \'B\'], [1, 2])\n\n        self.assertEqual(idx.iloc[1], (\'A\', 2))\n        self.assertEqual(idx.loc[(\'B\', 1)], (\'B\', 1))\n        self.assertEqual(idx[2], (\'B\', 1)) #pylint: disable=E1136\n        self.assertEqual(idx.loc[HLoc[\'B\', 1]], (\'B\', 1))\n\n\n\n    def test_hierarchy_iter_a(self) -> None:\n        OD = OrderedDict\n        tree = OD([\n                (\'I\', OD([\n                        (\'A\', (1, 2)), (\'B\', (1, 2))\n                        ])\n                ),\n                (\'II\', OD([\n                        (\'A\', (1, 2)), (\'B\', (1, 2))\n                        ])\n                ),\n                ])\n\n        ih = IndexHierarchy.from_tree(tree)\n\n        # this iterates over numpy arrays, which can be used with contains\n        self.assertEqual([k in ih for k in ih], #pylint: disable=E1133\n                [True, True, True, True, True, True, True, True]\n                )\n\n\n    def test_hierarchy_rename_a(self) -> None:\n        labels = ((\'a\', 1), (\'a\', 2), (\'b\', 1), (\'b\', 2))\n        ih1 = IndexHierarchy.from_labels(labels, name=\'foo\')\n        self.assertEqual(ih1.name, \'foo\')\n        ih2 = ih1.rename(None)\n        self.assertEqual(ih2.name, None)\n\n    def test_hierarchy_reversed(self) -> None:\n        labels = ((\'a\', 1), (\'a\', 2), (\'b\', 1), (\'b\', 2))\n        hier_idx = IndexHierarchy.from_labels(labels)\n        self.assertTrue(\n            all(hidx_1 == hidx_2\n                for hidx_1, hidx_2 in zip(reversed(hier_idx), reversed(labels)))\n        )\n\n\n    def test_hierarchy_keys_a(self) -> None:\n        OD = OrderedDict\n        tree = OD([\n                (\'I\', OD([\n                        (\'A\', (1, 2)), (\'B\', (1, 2))\n                        ])\n                ),\n                (\'II\', OD([\n                        (\'A\', (1, 2)), (\'B\', (1, 2))\n                        ])\n                ),\n                ])\n\n        ih = IndexHierarchyGO.from_tree(tree)\n\n        self.assertEqual([k in ih for k in ih], #pylint: disable=E1133\n                [True, True, True, True, True, True, True, True]\n                )\n\n        ih.append((\'III\', \'A\', 1))\n\n        self.assertEqual(set(ih),\n                {(\'I\', \'B\', 1), (\'I\', \'A\', 2), (\'II\', \'B\', 2), (\'II\', \'A\', 2), (\'I\', \'A\', 1), (\'III\', \'A\', 1), (\'II\', \'B\', 1), (\'II\', \'A\', 1), (\'I\', \'B\', 2)}\n                )\n\n\n    def test_hierarchy_display_a(self) -> None:\n        OD = OrderedDict\n        tree = OD([\n                (\'I\', OD([\n                        (\'A\', (1, 2)), (\'B\', (1, 2))\n                        ])\n                ),\n                (\'II\', OD([\n                        (\'A\', (1, 2)), (\'B\', (1, 2))\n                        ])\n                ),\n                ])\n\n        ih = IndexHierarchy.from_tree(tree)\n\n        post = ih.display()\n        self.assertEqual(len(post), 10)\n\n        s = Series(range(8), index=ih)\n        post = s.display()\n        self.assertEqual(len(post), 11)\n\n    def test_hierarchy_loc_a(self) -> None:\n        OD = OrderedDict\n        tree = OD([\n                (\'I\', OD([\n                        (\'A\', (1, 2)), (\'B\', (1, 2))\n                        ])\n                ),\n                (\'II\', OD([\n                        (\'A\', (1, 2)), (\'B\', (1, 2))\n                        ])\n                ),\n                ])\n\n        ih = IndexHierarchy.from_tree(tree)\n\n        s = Series(range(8), index=ih)\n\n        self.assertEqual(\n                s.loc[HLoc[\'I\']].values.tolist(),\n                [0, 1, 2, 3])\n\n        self.assertEqual(\n                s.loc[HLoc[:, \'A\']].values.tolist(),\n                [0, 1, 4, 5])\n\n    def test_hierarchy_series_a(self) -> None:\n        f1 = IndexHierarchy.from_tree\n        s1 = Series.from_element(23, index=f1(dict(a=(1,2,3))))\n        self.assertEqual(s1.values.tolist(), [23, 23, 23])\n\n        f2 = IndexHierarchy.from_product\n        s2 = Series.from_element(3, index=f2(Index((\'a\', \'b\')), Index((1,2))))\n        self.assertEqual(s2.to_pairs(),\n                (((\'a\', 1), 3), ((\'a\', 2), 3), ((\'b\', 1), 3), ((\'b\', 2), 3)))\n\n\n    def test_hierarchy_frame_a(self) -> None:\n        OD = OrderedDict\n        tree = OD([\n                (\'I\', OD([\n                        (\'A\', (1,)), (\'B\', (1, 2))\n                        ])\n                ),\n                (\'II\', OD([\n                        (\'A\', (1,)), (\'B\', (1, 2))\n                        ])\n                ),\n                ])\n\n        ih = IndexHierarchy.from_tree(tree)\n\n        data = np.arange(6*6).reshape(6, 6)\n        f1 = Frame(data, index=ih, columns=ih)\n        # self.assertEqual(len(f.to_pairs(0)), 8)\n\n\n        f2 = f1.assign.loc[(\'I\', \'B\', 2), (\'II\', \'A\', 1)](200)\n\n        post = f2.to_pairs(0)\n        self.assertEqual(post,\n                (((\'I\', \'A\', 1), (((\'I\', \'A\', 1), 0), ((\'I\', \'B\', 1), 6), ((\'I\', \'B\', 2), 12), ((\'II\', \'A\', 1), 18), ((\'II\', \'B\', 1), 24), ((\'II\', \'B\', 2), 30))), ((\'I\', \'B\', 1), (((\'I\', \'A\', 1), 1), ((\'I\', \'B\', 1), 7), ((\'I\', \'B\', 2), 13), ((\'II\', \'A\', 1), 19), ((\'II\', \'B\', 1), 25), ((\'II\', \'B\', 2), 31))), ((\'I\', \'B\', 2), (((\'I\', \'A\', 1), 2), ((\'I\', \'B\', 1), 8), ((\'I\', \'B\', 2), 14), ((\'II\', \'A\', 1), 20), ((\'II\', \'B\', 1), 26), ((\'II\', \'B\', 2), 32))), ((\'II\', \'A\', 1), (((\'I\', \'A\', 1), 3), ((\'I\', \'B\', 1), 9), ((\'I\', \'B\', 2), 200), ((\'II\', \'A\', 1), 21), ((\'II\', \'B\', 1), 27), ((\'II\', \'B\', 2), 33))), ((\'II\', \'B\', 1), (((\'I\', \'A\', 1), 4), ((\'I\', \'B\', 1), 10), ((\'I\', \'B\', 2), 16), ((\'II\', \'A\', 1), 22), ((\'II\', \'B\', 1), 28), ((\'II\', \'B\', 2), 34))), ((\'II\', \'B\', 2), (((\'I\', \'A\', 1), 5), ((\'I\', \'B\', 1), 11), ((\'I\', \'B\', 2), 17), ((\'II\', \'A\', 1), 23), ((\'II\', \'B\', 1), 29), ((\'II\', \'B\', 2), 35))))\n        )\n\n\n        f3 = f1.assign.loc[(\'I\', \'B\', 2):, HLoc[:, :, 2]](200)  # type: ignore  # https://github.com/python/typeshed/pull/3024\n\n        self.assertEqual(f3.to_pairs(0),\n                (((\'I\', \'A\', 1), (((\'I\', \'A\', 1), 0), ((\'I\', \'B\', 1), 6), ((\'I\', \'B\', 2), 12), ((\'II\', \'A\', 1), 18), ((\'II\', \'B\', 1), 24), ((\'II\', \'B\', 2), 30))), ((\'I\', \'B\', 1), (((\'I\', \'A\', 1), 1), ((\'I\', \'B\', 1), 7), ((\'I\', \'B\', 2), 13), ((\'II\', \'A\', 1), 19), ((\'II\', \'B\', 1), 25), ((\'II\', \'B\', 2), 31))), ((\'I\', \'B\', 2), (((\'I\', \'A\', 1), 2), ((\'I\', \'B\', 1), 8), ((\'I\', \'B\', 2), 200), ((\'II\', \'A\', 1), 200), ((\'II\', \'B\', 1), 200), ((\'II\', \'B\', 2), 200))), ((\'II\', \'A\', 1), (((\'I\', \'A\', 1), 3), ((\'I\', \'B\', 1), 9), ((\'I\', \'B\', 2), 15), ((\'II\', \'A\', 1), 21), ((\'II\', \'B\', 1), 27), ((\'II\', \'B\', 2), 33))), ((\'II\', \'B\', 1), (((\'I\', \'A\', 1), 4), ((\'I\', \'B\', 1), 10), ((\'I\', \'B\', 2), 16), ((\'II\', \'A\', 1), 22), ((\'II\', \'B\', 1), 28), ((\'II\', \'B\', 2), 34))), ((\'II\', \'B\', 2), (((\'I\', \'A\', 1), 5), ((\'I\', \'B\', 1), 11), ((\'I\', \'B\', 2), 200), ((\'II\', \'A\', 1), 200), ((\'II\', \'B\', 1), 200), ((\'II\', \'B\', 2), 200))))\n        )\n\n\n\n    def test_hierarchy_frame_b(self) -> None:\n        OD = OrderedDict\n        tree = OD([\n                (\'I\', OD([\n                        (\'A\', (1,)), (\'B\', (1, 2))\n                        ])\n                ),\n                (\'II\', OD([\n                        (\'A\', (1,)), (\'B\', (1, 2))\n                        ])\n                ),\n                ])\n\n        ih = IndexHierarchyGO.from_tree(tree)\n        data = np.arange(6*6).reshape(6, 6)\n        # TODO: this only works if own_columns is True for now\n        f1 = FrameGO(data, index=range(6), columns=ih, own_columns=True)\n        f1[(\'II\', \'B\', 3)] = 0\n\n        f2 = f1[HLoc[:, \'B\']]\n        self.assertEqual(f2.shape, (6, 5))\n\n        self.assertEqual(f2.to_pairs(0),\n                (((\'I\', \'B\', 1), ((0, 1), (1, 7), (2, 13), (3, 19), (4, 25), (5, 31))), ((\'I\', \'B\', 2), ((0, 2), (1, 8), (2, 14), (3, 20), (4, 26), (5, 32))), ((\'II\', \'B\', 1), ((0, 4), (1, 10), (2, 16), (3, 22), (4, 28), (5, 34))), ((\'II\', \'B\', 2), ((0, 5), (1, 11), (2, 17), (3, 23), (4, 29), (5, 35))), ((\'II\', \'B\', 3), ((0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0))))\n                )\n\n        f3 = f1[HLoc[:, :, 1]]\n        self.assertEqual(f3.to_pairs(0), (((\'I\', \'A\', 1), ((0, 0), (1, 6), (2, 12), (3, 18), (4, 24), (5, 30))), ((\'I\', \'B\', 1), ((0, 1), (1, 7), (2, 13), (3, 19), (4, 25), (5, 31))), ((\'II\', \'A\', 1), ((0, 3), (1, 9), (2, 15), (3, 21), (4, 27), (5, 33))), ((\'II\', \'B\', 1), ((0, 4), (1, 10), (2, 16), (3, 22), (4, 28), (5, 34)))))\n\n\n        f4 = f1.loc[[2, 5], HLoc[:, \'A\']]\n        self.assertEqual(f4.to_pairs(0),\n                (((\'I\', \'A\', 1), ((2, 12), (5, 30))), ((\'II\', \'A\', 1), ((2, 15), (5, 33)))))\n\n\n\n    def test_hierarchy_index_go_a(self) -> None:\n\n        OD = OrderedDict\n        tree1 = OD([\n                (\'I\', OD([\n                        (\'A\', (1,)), (\'B\', (1, 2))\n                        ])\n                ),\n                (\'II\', OD([\n                        (\'A\', (1,)), (\'B\', (1, 2))\n                        ])\n                ),\n                ])\n        ih1 = IndexHierarchyGO.from_tree(tree1)\n\n        tree2 = OD([\n                (\'III\', OD([\n                        (\'A\', (1,)), (\'B\', (1, 2))\n                        ])\n                ),\n                (\'IV\', OD([\n                        (\'A\', (1,)), (\'B\', (1, 2))\n                        ])\n                ),\n                ])\n        ih2 = IndexHierarchyGO.from_tree(tree2)\n\n        ih1.extend(ih2)\n\n        # self.assertEqual(ih1.loc_to_iloc((\'IV\', \'B\', 2)), 11)\n        self.assertEqual(len(ih2), 6)\n\n        # need tuple here to distinguish from iterable type selection\n        self.assertEqual([ih1.loc_to_iloc(tuple(v)) for v in ih1.values],\n                [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n                )\n\n\n\n    def test_hierarchy_relabel_a(self) -> None:\n\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'A\'),\n                (\'II\', \'B\'),\n                )\n\n        ih = IndexHierarchy.from_labels(labels)\n\n        ih.relabel({(\'I\', \'B\'): (\'I\', \'C\')})\n\n        ih2 = ih.relabel({(\'I\', \'B\'): (\'I\', \'C\')})\n\n        self.assertEqual(ih2.values.tolist(),\n                [[\'I\', \'A\'], [\'I\', \'C\'], [\'II\', \'A\'], [\'II\', \'B\']])\n\n        with self.assertRaises(Exception):\n            ih3 = ih.relabel({(\'I\', \'B\'): (\'I\', \'C\', 1)})\n\n        ih3 = ih.relabel(lambda x: tuple(e.lower() for e in x))\n\n        self.assertEqual(\n                ih3.values.tolist(),\n                [[\'i\', \'a\'], [\'i\', \'b\'], [\'ii\', \'a\'], [\'ii\', \'b\']])\n\n\n\n    def test_hierarchy_rehierarch_a(self) -> None:\n        ih1 = IndexHierarchy.from_product((\'I\', \'II\'), (\'B\', \'A\'), (2, 1))\n        ih2 = ih1.rehierarch((1, 0, 2))\n\n        self.assertEqual(ih2.values.tolist(),\n                [[\'B\', \'I\', 2], [\'B\', \'I\', 1], [\'B\', \'II\', 2], [\'B\', \'II\', 1], [\'A\', \'I\', 2], [\'A\', \'I\', 1], [\'A\', \'II\', 2], [\'A\', \'II\', 1]]\n                )\n\n        ih3 = ih1.rehierarch((2, 1, 0))\n        self.assertEqual(\n                ih3.values.tolist(),\n                [[2, \'B\', \'I\'], [2, \'B\', \'II\'], [2, \'A\', \'I\'], [2, \'A\', \'II\'], [1, \'B\', \'I\'], [1, \'B\', \'II\'], [1, \'A\', \'I\'], [1, \'A\', \'II\']]\n                )\n\n\n    def test_hierarchy_rehierarch_b(self) -> None:\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'C\'),\n                (\'II\', \'B\'),\n                (\'II\', \'D\'),\n                (\'III\', \'D\'),\n                (\'IV\', \'A\'),\n                )\n\n        ih1 = IndexHierarchyGO.from_labels(labels)\n        self.assertEqual(ih1.rehierarch([1, 0]).values.tolist(),\n                [[\'A\', \'I\'], [\'A\', \'IV\'], [\'B\', \'I\'], [\'B\', \'II\'], [\'C\', \'II\'], [\'D\', \'II\'], [\'D\', \'III\']]\n                )\n\n    def test_hierarchy_rehierarch_c(self) -> None:\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'A\'),\n                (\'II\', \'B\'),\n                )\n        ih1 = IndexHierarchy.from_labels(labels)\n\n        with self.assertRaises(RuntimeError):\n            ih1.rehierarch([0, 0])\n\n        with self.assertRaises(RuntimeError):\n            ih1.rehierarch([0,])\n\n\n    def test_hierarchy_set_operators_a(self) -> None:\n\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'A\'),\n                (\'II\', \'B\'),\n                )\n\n        ih1 = IndexHierarchy.from_labels(labels)\n\n        labels = (\n                (\'II\', \'A\'),\n                (\'II\', \'B\'),\n                (\'III\', \'A\'),\n                (\'III\', \'B\'),\n                )\n\n        ih2 = IndexHierarchy.from_labels(labels)\n\n        post1 = ih1.intersection(ih2)\n        self.assertEqual(post1.values.tolist(),\n                [[\'II\', \'A\'], [\'II\', \'B\']])\n\n        post2 = ih1.union(ih2)\n        self.assertEqual(post2.values.tolist(),\n                [[\'I\', \'A\'], [\'I\', \'B\'], [\'II\', \'A\'], [\'II\', \'B\'], [\'III\', \'A\'], [\'III\', \'B\']])\n\n        post3 = ih1.difference(ih2)\n        self.assertEqual(post3.values.tolist(),\n                [[\'I\', \'A\'], [\'I\', \'B\']])\n\n\n    def test_hierarchy_set_operators_b(self) -> None:\n\n        labels = (\n                (\'II\', \'B\'),\n                (\'II\', \'A\'),\n                (\'I\', \'B\'),\n                (\'I\', \'A\'),\n                )\n\n        ih1 = IndexHierarchy.from_labels(labels)\n        ih2 = IndexHierarchy.from_labels(labels)\n\n        post1 = ih1.union(ih2)\n        self.assertEqual(post1.values.tolist(),\n                [[\'II\', \'B\'], [\'II\', \'A\'], [\'I\', \'B\'], [\'I\', \'A\']])\n\n        post2 = ih1.intersection(ih2)\n        self.assertEqual(post2.values.tolist(),\n                [[\'II\', \'B\'], [\'II\', \'A\'], [\'I\', \'B\'], [\'I\', \'A\']])\n\n        post3 = ih1.difference(ih2)\n        self.assertEqual(post3.values.tolist(),\n                [])\n\n\n    def test_hierarchy_set_operators_c(self) -> None:\n\n        labels = (\n                (\'II\', \'B\'),\n                (\'II\', \'A\'),\n                (\'I\', \'B\'),\n                (\'I\', \'A\'),\n                )\n\n        ih1 = IndexHierarchy.from_labels(())\n        ih2 = IndexHierarchy.from_labels(labels)\n\n        post1 = ih1.union(ih2)\n        self.assertEqual(post1.values.tolist(),\n                [[\'II\', \'B\'], [\'II\', \'A\'], [\'I\', \'B\'], [\'I\', \'A\']])\n\n        post2 = ih1.intersection(ih2)\n        self.assertEqual(post2.values.tolist(),\n                [])\n\n        post3 = ih1.difference(ih2)\n        self.assertEqual(post3.values.tolist(),\n                [])\n\n\n    def test_hierarchy_set_operators_d(self) -> None:\n\n        labels = (\n                (\'II\', \'B\'),\n                (\'II\', \'A\'),\n                (\'I\', \'B\'),\n                (\'I\', \'A\'),\n                )\n\n        ih1 = IndexHierarchy.from_labels(labels)\n        ih2 = IndexHierarchy.from_labels(())\n\n        post1 = ih1.union(ih2)\n        self.assertEqual(post1.values.tolist(),\n                [[\'II\', \'B\'], [\'II\', \'A\'], [\'I\', \'B\'], [\'I\', \'A\']])\n\n        post2 = ih1.intersection(ih2)\n        self.assertEqual(post2.values.tolist(),\n                [])\n\n        post3 = ih1.difference(ih2)\n        self.assertEqual(post3.values.tolist(),\n                [[\'II\', \'B\'], [\'II\', \'A\'], [\'I\', \'B\'], [\'I\', \'A\']])\n\n    #---------------------------------------------------------------------------\n\n    def test_hierarchy_unary_operators_a(self) -> None:\n\n        labels = (\n                (1, 1),\n                (1, 2),\n                (2, 1),\n                (2, 2),\n                )\n        ih1 = IndexHierarchyGO.from_labels(labels)\n        ih1.append((3, 1)) # force a recache state\n        self.assertTrue(ih1._recache)\n\n        self.assertEqual((-ih1).tolist(),\n                [[-1, -1], [-1, -2], [-2, -1], [-2, -2], [-3, -1]]\n                )\n\n\n\n    #---------------------------------------------------------------------------\n    def test_hierarchy_binary_operators_a(self) -> None:\n\n        labels = (\n                (1, 1),\n                (1, 2),\n                (2, 1),\n                (2, 2),\n                )\n        ih1 = IndexHierarchy.from_labels(labels)\n        self.assertEqual((ih1*2).tolist(),\n                [[2, 2], [2, 4], [4, 2], [4, 4]])\n\n        self.assertEqual((-ih1).tolist(),\n                [[-1, -1], [-1, -2], [-2, -1], [-2, -2]])\n\n\n    def test_hierarchy_binary_operators_b(self) -> None:\n\n        labels = (\n                (1, 1),\n                (1, 2),\n                )\n        ih1 = IndexHierarchy.from_labels(labels)\n\n        labels = (\n                (3, 3),\n                (1, 2),\n                )\n        ih2 = IndexHierarchy.from_labels(labels)\n\n        self.assertEqual((ih1 @ ih2).tolist(),\n                [[4, 5], [5, 7]]\n                )\n\n        self.assertEqual((ih1.values @ ih2).tolist(),\n                [[4, 5], [5, 7]]\n                )\n\n        self.assertEqual((ih1 @ ih2.values).tolist(),\n                [[4, 5], [5, 7]]\n                )\n\n        self.assertEqual((ih1.values @ ih2.values).tolist(),\n                [[4, 5], [5, 7]]\n                )\n\n\n\n    def test_hierarchy_binary_operators_c(self) -> None:\n\n        labels = (\n                (1, 1),\n                (1, 2),\n                (2, 1),\n                (2, 2),\n                )\n        ih1 = IndexHierarchyGO.from_labels(labels)\n\n        a1 = ih1 * Index((1, 2, 3, 4))\n        self.assertEqual(a1.tolist(), [[1, 1], [2, 4], [6, 3], [8, 8]])\n\n        a2 = ih1 + ih1\n        self.assertEqual(a2.tolist(), [[2, 2], [2, 4], [4, 2], [4, 4]])\n\n\n    def test_hierarchy_binary_operators_d(self) -> None:\n\n        labels = (\n                (1, 1),\n                (1, 2),\n                )\n        ih1 = IndexHierarchy.from_labels(labels)\n\n        labels = (\n                (3, 3),\n                (1, 2),\n                )\n        ih2 = IndexHierarchy.from_labels(labels)\n\n        a1 = ih1 @ ih2\n        a2 = ih1.values.tolist() @ ih2 # force rmatmul\n        self.assertEqual(a1.tolist(), a2.tolist())\n\n\n    def test_hierarchy_binary_operators_e(self) -> None:\n\n        labels = (\n                (1, 1, 1),\n                (2, 2, 2),\n                )\n        ih1 = IndexHierarchy.from_labels(labels)\n\n        labels = (\n                (1, 1, 1),\n                (2, 2, 2),\n                )\n        ih2 = IndexHierarchy.from_labels(labels)\n\n        a1 = ih1 == ih2\n        self.assertEqual(a1.tolist(), [[True, True, True], [True, True, True]])\n\n\n    def test_hierarchy_binary_operators_f(self) -> None:\n\n        a1 = np.arange(25).reshape(5,5)\n        a2 = np.arange(start=24, stop=-1, step=-1).reshape(5,5)\n\n        f1_idx_labels = [\n                [\'i_I\', 1, \'i\'],\n                [\'i_I\', 2, \'i\'],\n                [\'i_I\', 3, \'i\'],\n                [\'i_II\', 1, \'i\'],\n                [\'i_II\', 3, \'i\']]\n\n        f2_idx_labels = [\n                [\'i_II\', 2, \'i\'],\n                [\'i_II\', 1, \'i\'],\n                [\'i_I\', 3, \'i\'],\n                [\'i_I\', 1, \'i\'],\n                [\'i_I\', 4, \'i\']]\n\n        f1 = Frame(a1, index=IndexHierarchy.from_labels(f1_idx_labels))\n        f2 = Frame(a2, index=IndexHierarchy.from_labels(f2_idx_labels))\n        int_index = f1.index.intersection(f2.index)\n\n        post = f1.reindex(int_index).index == f2.reindex(int_index).index\n        self.assertEqual(post.tolist(),\n                [[True, True, True], [True, True, True], [True, True, True]])\n\n\n\n    def test_hierarchy_binary_operators_g(self) -> None:\n\n        a1 = np.arange(25).reshape(5,5)\n        a2 = np.arange(start=24, stop=-1, step=-1).reshape(5,5)\n        f1_idx_labels = [\n                [\'i_I\', \'i\'],\n                [\'i_I\', 2],\n                [\'i_I\', \'iii\'],\n                [\'i_II\', \'i\'],\n                [\'i_III\', \'ii\']]\n\n        f2_idx_labels = [\n                [\'i_IV\', \'i\'],\n                [\'i_II\', \'ii\'],\n                [\'i_I\', 2],\n                [\'i_I\', \'ii\'],\n                [\'i_I\', \'iii\']]\n\n        f1 = Frame(a1, index=IndexHierarchy.from_labels(f1_idx_labels)) #type: ignore\n        f2 = Frame(a2, index=IndexHierarchy.from_labels(f2_idx_labels)) #type: ignore\n        int_index = f1.index.intersection(f2.index)\n\n        post = f1.reindex(int_index).index == f2.reindex(int_index).index\n        self.assertEqual(post.tolist(),\n                [[True, True], [True, True]]\n                )\n\n    def test_hierarchy_binary_operators_h(self) -> None:\n\n        labels1 = (\n                (1, 1),\n                (2, 2),\n                )\n        ih1 = IndexHierarchy.from_labels(labels1)\n\n        labels2 = (\n                (1, 1, 1),\n                (2, 2, 2),\n                )\n        ih2 = IndexHierarchy.from_labels(labels2)\n\n        # for now, this returns na array of the shape of the left-hand opperatnd\n        post1 = ih1 != ih2\n        self.assertEqual(post1.shape, (2, 2))\n        self.assertEqual(post1.all(), True)\n\n        post2 = ih1 == ih2\n        self.assertEqual(post2.shape, (2, 2))\n        self.assertEqual(post2.any(), False)\n\n    def test_hierarchy_binary_operators_i(self) -> None:\n\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'A\'),\n                (\'II\', \'B\'),\n                )\n\n        ih1 = IndexHierarchy.from_labels(labels)\n\n        ih2 = ih1 + \'_\'\n        self.assertEqual(ih2.tolist(),\n            [[\'I_\', \'A_\'], [\'I_\', \'B_\'], [\'II_\', \'A_\'], [\'II_\', \'B_\']])\n\n        ih3 = \'_\' + ih1\n        self.assertEqual(ih3.tolist(),\n            [[\'_I\', \'_A\'], [\'_I\', \'_B\'], [\'_II\', \'_A\'], [\'_II\', \'_B\']])\n\n\n        ih4 = ih1 * 2\n        self.assertEqual(ih4.tolist(),\n            [[\'II\', \'AA\'], [\'II\', \'BB\'], [\'IIII\', \'AA\'], [\'IIII\', \'BB\']])\n\n\n    #---------------------------------------------------------------------------\n    def test_hierarchy_flat_a(self) -> None:\n\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'A\'),\n                (\'II\', \'B\'),\n                )\n\n        ih = IndexHierarchy.from_labels(labels)\n        self.assertEqual(ih.flat().values.tolist(),\n                [(\'I\', \'A\'), (\'I\', \'B\'), (\'II\', \'A\'), (\'II\', \'B\')]\n                )\n\n\n    #---------------------------------------------------------------------------\n    def test_hierarchy_add_level_a(self) -> None:\n\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'A\'),\n                (\'II\', \'B\'),\n                )\n\n        ih = IndexHierarchy.from_labels(labels)\n        ih2 = ih.add_level(\'b\')\n\n        self.assertEqual(ih2.values.tolist(),\n                [[\'b\', \'I\', \'A\'], [\'b\', \'I\', \'B\'], [\'b\', \'II\', \'A\'], [\'b\', \'II\', \'B\']])\n        self.assertEqual([ih2.loc_to_iloc(tuple(x)) for x in ih2.values],\n                [0, 1, 2, 3])\n\n\n    def test_hierarchy_add_level_b(self) -> None:\n\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'A\'),\n                (\'II\', \'B\'),\n                )\n\n        ih1 = IndexHierarchyGO.from_labels(labels)\n        ih1.append((\'III\', \'A\'))\n        ih2 = ih1.add_level(\'x\')\n\n        self.assertEqual(ih1.values.tolist(),\n                [[\'I\', \'A\'], [\'I\', \'B\'], [\'II\', \'A\'], [\'II\', \'B\'], [\'III\', \'A\']])\n\n        self.assertEqual(ih2.values.tolist(),\n                [[\'x\', \'I\', \'A\'], [\'x\', \'I\', \'B\'], [\'x\', \'II\', \'A\'], [\'x\', \'II\', \'B\'], [\'x\', \'III\', \'A\']])\n\n\n    def test_hierarchy_add_level_c(self) -> None:\n\n        labels = (\n                (1, \'A\'),\n                (1, \'B\'),\n                (2, \'A\'),\n                (2, \'B\'),\n                )\n\n        ih1 = IndexHierarchyGO.from_labels(labels)\n        # force TB creation\n        part = ih1.iloc[1:]\n        ih2 = ih1.add_level(\'x\')\n        # proove we reused the underlying block arrays\n        self.assertEqual(ih1._blocks.mloc.tolist(), ih2._blocks.mloc[1:].tolist())\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_hierarchy_drop_level_a(self) -> None:\n\n        labels = (\n                (\'I\', \'A\', 1),\n                (\'I\', \'B\', 1),\n                (\'II\', \'A\', 1),\n                (\'II\', \'B\', 2),\n                )\n\n        ih = IndexHierarchy.from_labels(labels)\n        ih2 = ih.drop_level(-1)\n\n        self.assertEqual(ih2.values.tolist(),\n                [[\'I\', \'A\'], [\'I\', \'B\'], [\'II\', \'A\'], [\'II\', \'B\']])\n\n\n    def test_hierarchy_drop_level_b(self) -> None:\n\n        labels = (\n                (\'I\', \'A\', 1),\n                (\'I\', \'B\', 1),\n                (\'II\', \'C\', 1),\n                (\'II\', \'C\', 2),\n                )\n\n        ih = IndexHierarchy.from_labels(labels)\n        ih2 = ih.drop_level(1)\n        assert isinstance(ih2, IndexHierarchy)\n        self.assertEqual(ih2.values.tolist(),\n            [[\'A\', 1], [\'B\', 1], [\'C\', 1], [\'C\', 2]])\n\n        with self.assertRaises(ErrorInitIndex):\n            ih2.drop_level(1)\n\n    def test_hierarchy_drop_level_c(self) -> None:\n\n        labels = (\n                (\'I\', \'A\', 1),\n                (\'I\', \'B\', 2),\n                (\'II\', \'C\', 3),\n                (\'II\', \'C\', 4),\n                )\n\n        ih = IndexHierarchy.from_labels(labels)\n        self.assertEqual(ih.drop_level(1).values.tolist(),\n                [[\'A\', 1], [\'B\', 2], [\'C\', 3], [\'C\', 4]])\n\n    def test_hierarchy_drop_level_d(self) -> None:\n\n        labels = (\n                (\'A\', 1),\n                (\'B\', 2),\n                (\'C\', 3),\n                (\'C\', 4),\n                )\n\n        ih = IndexHierarchy.from_labels(labels)\n        self.assertEqual(ih.drop_level(1).values.tolist(),\n                [1, 2, 3, 4])\n\n\n    def test_hierarchy_drop_level_e(self) -> None:\n\n        ih = IndexHierarchy.from_product((\'a\',), (1,), (\'x\', \'y\'))\n        self.assertEqual(ih.drop_level(2).values.tolist(),\n                [\'x\', \'y\'])\n\n        self.assertEqual(ih.drop_level(1).values.tolist(),\n                [[1, \'x\'], [1, \'y\']])\n\n\n    def test_hierarchy_drop_level_f(self) -> None:\n\n        ih = IndexHierarchy.from_product((\'a\',), (1,), (\'x\',))\n        self.assertEqual(ih.drop_level(1).values.tolist(),\n                [[1, \'x\']])\n\n    def test_hierarchy_drop_level_g(self) -> None:\n\n        ih = IndexHierarchy.from_product((\'a\',), (1,), (\'x\',))\n        with self.assertRaises(NotImplementedError):\n            _ = ih.drop_level(0)\n\n\n    def test_hierarchy_drop_level_h(self) -> None:\n\n        labels = (\n                (\'I\', \'A\', 1, False),\n                (\'I\', \'B\', 2, True),\n                (\'II\', \'C\', 3, False),\n                (\'II\', \'C\', 4, True),\n                )\n\n        ih = IndexHierarchy.from_labels(labels)\n\n        part = ih.iloc[1:] # force TB creation\n\n        post1 = ih.drop_level(-1)\n        assert isinstance(post1, IndexHierarchy) # mypy\n        self.assertEqual(ih._blocks.mloc[:-1].tolist(), post1._blocks.mloc.tolist())\n        # we changed shape after dropping two depths\n        self.assertEqual(ih.drop_level(-2).shape, (3, 2))\n\n        post2 = ih.drop_level(1)\n        assert isinstance(post2, IndexHierarchy) # mypy\n        self.assertEqual(ih._blocks.mloc[1:].tolist(), post2._blocks.mloc.tolist())\n\n        post3 = ih.drop_level(2)\n        assert isinstance(post3, IndexHierarchy) # mypy\n        self.assertEqual(ih._blocks.mloc[2:].tolist(), post3._blocks.mloc.tolist())\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_hierarchy_boolean_loc(self) -> None:\n        records = (\n                (\'a\', 999999, 0.1),\n                (\'a\', 201810, 0.1),\n                (\'b\', 999999, 0.4),\n                (\'b\', 201810, 0.4))\n        f1 = Frame.from_records(records, columns=list(\'abc\'))\n\n        f1 = f1.set_index_hierarchy([\'a\', \'b\'], drop=False)\n        self.assertEqual(f1.index.names, (\'a\', \'b\'))\n\n        f2 = f1.loc[f1[\'b\'] == 999999]\n\n        self.assertEqual(f2.to_pairs(0),\n                ((\'a\', (((\'a\', 999999), \'a\'), ((\'b\', 999999), \'b\'))), (\'b\', (((\'a\', 999999), 999999), ((\'b\', 999999), 999999))), (\'c\', (((\'a\', 999999), 0.1), ((\'b\', 999999), 0.4)))))\n\n        f3 = f1.loc[Series([False, True], index=((\'b\', 999999), (\'b\', 201810)))]\n        self.assertEqual(f3.to_pairs(0),\n                ((\'a\', (((\'b\', 201810), \'b\'),)), (\'b\', (((\'b\', 201810), 201810),)), (\'c\', (((\'b\', 201810), 0.4),))))\n\n\n    def test_hierarchy_name_a(self) -> None:\n\n        idx1 = IndexHierarchy.from_product(list(\'ab\'), list(\'xy\'), name=\'q\')\n        self.assertEqual(idx1.name, \'q\')\n\n        idx2 = idx1.rename(\'w\')\n        self.assertEqual(idx2.name, \'w\')\n        # will provide one for each level\n        self.assertEqual(idx2.names, (\'__index0__\', \'__index1__\'))\n\n\n    def test_hierarchy_name_b(self) -> None:\n\n        idx1 = IndexHierarchyGO.from_product(list(\'ab\'), list(\'xy\'), name=\'q\')\n        idx2 = idx1.rename(\'w\')\n\n        self.assertEqual(idx1.name, \'q\')\n        self.assertEqual(idx2.name, \'w\')\n\n        idx1.append((\'c\', \'c\'))\n        idx2.append((\'x\', \'x\'))\n\n        self.assertEqual(\n                idx1.values.tolist(),\n                [[\'a\', \'x\'], [\'a\', \'y\'], [\'b\', \'x\'], [\'b\', \'y\'], [\'c\', \'c\']]\n                )\n\n        self.assertEqual(\n                idx2.values.tolist(),\n                [[\'a\', \'x\'], [\'a\', \'y\'], [\'b\', \'x\'], [\'b\', \'y\'], [\'x\', \'x\']]\n                )\n\n    def test_hierarchy_name_c(self) -> None:\n\n        idx1 = IndexHierarchyGO.from_product(list(\'ab\'), list(\'xy\'), name=\'q\')\n        idx2 = idx1.rename((\'a\', \'b\', \'c\'))\n\n        # since the name attr is the wrong size, names use the generic from\n        self.assertEqual(idx2.names, (\'__index0__\', \'__index1__\'))\n\n    def test_hierarchy_name_d(self) -> None:\n\n        idx1 = IndexHierarchy.from_product(list(\'ab\'), list(\'xy\'), name=\'q\')\n        self.assertEqual(idx1.name, \'q\')\n\n\n    def test_hierarchy_display_b(self) -> None:\n\n        idx1 = IndexHierarchy.from_product(list(\'ab\'), list(\'xy\'), name=\'q\')\n\n        match = tuple(idx1.display(DisplayConfig(type_color=False)))\n\n        self.assertEqual(\n                match,\n                ([\'<IndexHierarchy: q>\', \'\'], [\'a\', \'x\'], [\'a\', \'y\'], [\'b\', \'x\'], [\'b\', \'y\'], [\'<<U1>\', \'<<U1>\'])\n                )\n\n    #---------------------------------------------------------------------------\n    def test_hierarchy_to_frame_a(self) -> None:\n\n        ih1 = IndexHierarchy.from_product(list(\'ab\'), list(\'xy\'), name=\'q\')\n\n        self.assertEqual(ih1.to_frame().to_pairs(0),\n                ((0, ((0, \'a\'), (1, \'a\'), (2, \'b\'), (3, \'b\'))), (1, ((0, \'x\'), (1, \'y\'), (2, \'x\'), (3, \'y\'))))\n                )\n\n        f2 = ih1.to_frame_go()\n        f2[-1] = None\n\n        self.assertEqual(f2.to_pairs(0),\n                ((0, ((0, \'a\'), (1, \'a\'), (2, \'b\'), (3, \'b\'))), (1, ((0, \'x\'), (1, \'y\'), (2, \'x\'), (3, \'y\'))), (-1, ((0, None), (1, None), (2, None), (3, None))))\n                )\n\n\n    def test_hierarchy_to_frame_b(self) -> None:\n\n        ih1 = IndexHierarchy.from_product(list(\'ab\'), [10.1, 20.2], name=\'q\')\n        f1 = ih1.to_frame()\n        self.assertEqual(f1.dtypes.to_pairs(),\n                ((0, np.dtype(\'<U1\')), (1, np.dtype(\'float64\')))\n                )\n\n    #---------------------------------------------------------------------------\n\n    def test_hierarchy_to_html_datatables(self) -> None:\n\n        ih1 = IndexHierarchy.from_product(list(\'ab\'), list(\'xy\'), name=\'q\')\n\n        with temp_file(\'.html\', path=True) as fp:\n            ih1.to_html_datatables(fp, show=False)\n            with open(fp) as file:\n                data = file.read()\n                self.assertTrue(\'SFTable\' in data)\n                self.assertTrue(len(data) > 1000)\n\n\n    def test_hierarchy_to_pandas_a(self) -> None:\n\n        idx1 = IndexHierarchy.from_product(list(\'ab\'), list(\'xy\'), name=\'q\')\n\n        pdidx = idx1.to_pandas()\n        self.assertEqual(pdidx.name, \'q\')\n        self.assertEqual(\n                idx1.values.tolist(),\n                [list(x) for x in pdidx.values.tolist()])\n\n\n\n    def test_hierarchy_from_pandas_a(self) -> None:\n        import pandas\n\n        pdidx = pandas.MultiIndex.from_product(((\'I\', \'II\'), (\'A\', \'B\')))\n\n        idx = IndexHierarchy.from_pandas(pdidx)\n\n        self.assertEqual(idx.values.tolist(),\n                [[\'I\', \'A\'], [\'I\', \'B\'], [\'II\', \'A\'], [\'II\', \'B\']]\n                )\n\n\n    def test_hierarchy_from_pandas_b(self) -> None:\n        import pandas\n\n        idx = IndexHierarchy.from_product((\'I\', \'II\'), (\'A\', \'B\'), (1, 2))\n\n        self.assertEqual(list(idx.iter_label(0)), [\'I\', \'II\'])\n        self.assertEqual(list(idx.iter_label(1)), [\'A\', \'B\', \'A\', \'B\'])\n        self.assertEqual(list(idx.iter_label(2)), [1, 2, 1, 2, 1, 2, 1, 2])\n\n        post = idx.iter_label(1).apply(lambda x: x.lower())\n        self.assertEqual(post.to_pairs(),\n                ((0, \'a\'), (1, \'b\'), (2, \'a\'), (3, \'b\')))\n\n\n\n    def test_hierarchy_from_pandas_c(self) -> None:\n        import pandas\n\n        pdidx = pandas.MultiIndex.from_product(((\'I\', \'II\'), (\'A\', \'B\')))\n\n        idx = IndexHierarchyGO.from_pandas(pdidx)\n\n        self.assertEqual(idx.values.tolist(),\n                [[\'I\', \'A\'], [\'I\', \'B\'], [\'II\', \'A\'], [\'II\', \'B\']]\n                )\n\n        self.assertEqual(idx.values.tolist(),\n                [[\'I\', \'A\'], [\'I\', \'B\'], [\'II\', \'A\'], [\'II\', \'B\']])\n\n        idx.append((\'III\', \'A\')) #type: ignore\n\n        self.assertEqual(idx.values.tolist(),\n                [[\'I\', \'A\'], [\'I\', \'B\'], [\'II\', \'A\'], [\'II\', \'B\'], [\'III\', \'A\']])\n\n\n    def test_hierarchy_copy_a(self) -> None:\n\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'A\'),\n                (\'II\', \'B\'),\n                )\n\n        ih1 = IndexHierarchy.from_labels(labels)\n        ih2 = ih1.copy()\n\n        self.assertEqual(ih2.values.tolist(),\n            [[\'I\', \'A\'], [\'I\', \'B\'], [\'II\', \'A\'], [\'II\', \'B\']])\n\n\n\n    def test_hierarchy_copy_b(self) -> None:\n\n        labels = (\n                (\'I\', \'A\'),\n                (\'I\', \'B\'),\n                (\'II\', \'A\'),\n                (\'II\', \'B\'),\n                )\n\n        ih1 = IndexHierarchyGO.from_labels(labels)\n        ih2 = ih1.copy()\n        ih2.append((\'II\', \'C\'))\n\n        self.assertEqual(ih2.values.tolist(),\n            [[\'I\', \'A\'], [\'I\', \'B\'], [\'II\', \'A\'], [\'II\', \'B\'], [\'II\', \'C\']]\n            )\n\n        self.assertEqual(ih1.values.tolist(),\n            [[\'I\', \'A\'], [\'I\', \'B\'], [\'II\', \'A\'], [\'II\', \'B\']]\n            )\n\n    #---------------------------------------------------------------------------\n\n    def test_hierarchy_ufunc_axis_skipna_a(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((10, 20), (3.1, np.nan))\n\n        self.assertAlmostEqualValues(\n                ih1.sum(axis=1, skipna=False).tolist(),\n                [13.1, np.nan, 23.1, np.nan])\n        self.assertAlmostEqualValues(\n                ih1.sum(axis=0, skipna=False).tolist(),\n                [60.0, np.nan]\n                )\n\n    def test_hierarchy_ufunc_axis_skipna_b(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((10, 20), (3, 7))\n\n        # sum applies to the labels\n        self.assertEqual(ih1.sum().tolist(),\n                [60, 20]\n                )\n\n        self.assertEqual(ih1.cumprod().tolist(),\n                [[10, 3], [100, 21], [2000, 63], [40000, 441]]\n                )\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_hierarchy_pickle_a(self) -> None:\n\n        a = IndexHierarchy.from_product((10, 20), (3, 7))\n        b = IndexHierarchy.from_product((\'a\', \'b\'), (\'x\', \'y\'))\n\n        for index in (a, b):\n            # force creating of ._labels\n            self.assertTrue(len(index.values), len(index))\n\n            pbytes = pickle.dumps(index)\n            index_new = pickle.loads(pbytes)\n\n            for v in index: # iter labels (arrays here)\n                self.assertFalse(index_new.values.flags.writeable)\n                self.assertEqual(index_new.loc[tuple(v)], index.loc[tuple(v)])\n\n\n    # def test_index_hierarchy_get_a(self) -> None:\n\n    #     ih1 = IndexHierarchy.from_product((10, 20), (3, 7))\n    #     self.assertEqual(ih1.get((20, 3)), 2)\n    #     self.assertEqual(ih1.get((20, 7)), 3)\n    #     self.assertEqual(ih1.get((20, 200)), None)\n\n\n    def test_index_hierarchy_sort_a(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((1, 2), (30, 70))\n\n        self.assertEqual(ih1.sort(ascending=False).values.tolist(),\n            [[2, 70], [2, 30], [1, 70], [1, 30]]\n            )\n\n\n\n    def test_index_hierarchy_isin_a(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((1, 2), (30, 70), (2, 5))\n\n        post = ih1.isin([(2, 30, 2),])\n        self.assertEqual(post.dtype, bool)\n        self.assertEqual(post.tolist(),\n            [False, False, False, False, True, False, False, False])\n\n        extract = ih1.loc[post]\n        self.assertEqual(extract.values.shape, (1, 3))\n\n\n    def test_index_hierarchy_isin_b(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((1, 2), (30, 70), (2, 5))\n\n        with self.assertRaises(RuntimeError):\n            ih1.isin([3,4,5]) #type: ignore # not an iterable of iterables\n\n        post = ih1.isin(([3,4], [2,5,1,5]))\n        self.assertEqual(post.sum(), 0)\n\n\n    def test_index_hierarchy_isin_c(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((1, 2), (\'a\', \'b\'), (2, 5))\n\n        # multiple matches\n\n        post1 = ih1.isin([(1, \'a\', 5), (2, \'b\', 2)])\n        self.assertEqual(post1.tolist(),\n                [False, True, False, False, False, False, True, False])\n\n        post2 = ih1.isin(ih1)\n        self.assertEqual(post2.sum(), len(ih1))\n\n\n    def test_index_hierarchy_isin_d(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((1, 2), (30, 70), (2, 5))\n\n        # Index is an iterable\n        index_iter1 = (val for val in (2, 30, 2))\n        index_non_iter = (1, 70, 5)\n\n        post = ih1.isin([index_iter1, index_non_iter])\n        self.assertEqual(post.dtype, bool)\n        self.assertEqual(post.tolist(),\n            [False, False, False, True, True, False, False, False])\n\n        extract = ih1.loc[post]\n        self.assertEqual(extract.values.shape, (2, 3))\n\n    def test_index_hierarchy_roll_a(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((1, 2), (30, 70))\n\n        with self.assertRaises(RuntimeError):\n            ih1.roll(1) # result in invalid tree form\n\n        self.assertEqual(ih1.roll(2).values.tolist(),\n            [[2, 30], [2, 70], [1, 30], [1, 70]]\n            )\n\n\n    def test_index_hierarchy_roll_b(self) -> None:\n\n        ih1 = IndexHierarchy.from_labels(((\'a\', 1), (\'b\', 20), (\'c\', 400), (\'d\', 50)))\n\n        self.assertEqual(\n                ih1.roll(1).values.tolist(),\n                [[\'d\', 50], [\'a\', 1], [\'b\', 20], [\'c\', 400]]\n                )\n\n        self.assertEqual(\n                ih1.roll(-1).values.tolist(),\n                [[\'b\', 20], [\'c\', 400], [\'d\', 50], [\'a\', 1]]\n                )\n\n    def test_index_hierarchy_dtypes_a(self) -> None:\n        idx1 = Index((\'A\', \'B\'))\n        idx2 = IndexDate.from_date_range(\'2019-01-05\', \'2019-01-08\')\n        idx3 = Index((1, 2))\n        hidx = IndexHierarchy.from_product(idx1, idx2, idx3)\n\n        self.assertEqual(\n            [(x, y.kind) for x, y in hidx.dtypes.to_pairs()],\n            [(0, \'U\'), (1, \'M\'), (2, \'i\')]\n            )\n\n    def test_index_hierarchy_dtypes_b(self) -> None:\n        idx1 = Index((\'A\', \'B\'), name=\'a\')\n        idx2 = IndexDate.from_date_range(\'2019-01-05\', \'2019-01-08\', name=\'b\')\n        idx3 = Index((1, 2), name=\'c\')\n        hidx = IndexHierarchy.from_product(idx1, idx2, idx3)\n\n        self.assertEqual(\n            [(x, y.kind) for x, y in hidx.dtypes.to_pairs()],\n            [(\'a\', \'U\'), (\'b\', \'M\'), (\'c\', \'i\')]\n            )\n\n    def test_index_hierarchy_index_types_a(self) -> None:\n        idx1 = Index((\'A\', \'B\'))\n        idx2 = IndexDate.from_date_range(\'2019-01-05\', \'2019-01-08\')\n        idx3 = Index((1, 2))\n        hidx = IndexHierarchy.from_product(idx1, idx2, idx3)\n\n        self.assertEqual(\n            [(x, y.__name__) for x, y in hidx.index_types.to_pairs()],\n            [(0, \'Index\'), (1, \'IndexDate\'), (2, \'Index\')]\n            )\n\n    def test_index_hierarchy_index_types_b(self) -> None:\n        idx1 = Index((\'A\', \'B\'), name=\'a\')\n        idx2 = IndexDate.from_date_range(\'2019-01-05\', \'2019-01-08\', name=\'b\')\n        idx3 = Index((1, 2), name=\'c\')\n        hidx = IndexHierarchy.from_product(idx1, idx2, idx3)\n\n        self.assertEqual(\n            [(x, y.__name__) for x, y in hidx.index_types.to_pairs()],\n            [(\'a\', \'Index\'), (\'b\', \'IndexDate\'), (\'c\', \'Index\')]\n            )\n\n\n    #---------------------------------------------------------------------------\n    def test_index_hierarchy_label_widths_at_depth_a(self) -> None:\n        idx1 = Index((\'A\', \'B\'), name=\'a\')\n        idx2 = IndexDate.from_date_range(\'2019-01-05\', \'2019-01-08\', name=\'b\')\n        idx3 = Index((1, 2), name=\'c\')\n        hidx = IndexHierarchy.from_product(idx1, idx2, idx3)\n\n        self.assertEqual(tuple(hidx.label_widths_at_depth(0)),\n                ((\'A\', 8), (\'B\', 8))\n                )\n\n        self.assertEqual(tuple(hidx.label_widths_at_depth(1)),\n                ((np.datetime64(\'2019-01-05\'), 2), (np.datetime64(\'2019-01-06\'), 2), (np.datetime64(\'2019-01-07\'), 2), (np.datetime64(\'2019-01-08\'), 2), (np.datetime64(\'2019-01-05\'), 2), (np.datetime64(\'2019-01-06\'), 2), (np.datetime64(\'2019-01-07\'), 2), (np.datetime64(\'2019-01-08\'), 2))\n                )\n\n        self.assertEqual(tuple(hidx.label_widths_at_depth(2)),\n                ((1, 1), (2, 1), (1, 1), (2, 1), (1, 1), (2, 1), (1, 1), (2, 1), (1, 1), (2, 1), (1, 1), (2, 1), (1, 1), (2, 1), (1, 1), (2, 1))\n                )\n\n\n    def test_index_hierarchy_label_widths_at_depth_b(self) -> None:\n        idx1 = Index((\'A\', \'B\'), name=\'a\')\n        idx2 = IndexDate.from_date_range(\'2019-01-05\', \'2019-01-08\', name=\'b\')\n        idx3 = Index((1, 2), name=\'c\')\n        hidx = IndexHierarchy.from_product(idx1, idx2, idx3)\n\n\n        with self.assertRaises(NotImplementedError):\n            _ = next(hidx.label_widths_at_depth(None))\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_hierarchy_astype_a(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((1, 2), (\'a\', \'b\'), (2, 5))\n\n        ih2 = ih1.astype[[0, 2]](float)\n\n        self.assertEqual(ih2.dtypes.values.tolist(),\n                [np.dtype(\'float64\'), np.dtype(\'<U1\'), np.dtype(\'float64\')])\n\n\n    def test_index_hierarchy_astype_b(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((1, 2), (100, 200))\n        ih2 = ih1.astype(float)\n        self.assertEqual(ih2.dtypes.values.tolist(),\n                [np.dtype(\'float64\'), np.dtype(\'float64\')])\n\n\n    def test_index_hierarchy_astype_c(self) -> None:\n        ih1 = IndexHierarchy.from_product((1, 2), (100, 200), (\'2020-01\', \'2020-03\'))\n\n        self.assertEqual(\n                ih1.astype[[0, 1]](float).dtypes.to_pairs(),\n                ((0, np.dtype(\'float64\')), (1, np.dtype(\'float64\')), (2, np.dtype(\'<U7\')))\n                )\n\n\n    def test_index_hierarchy_astype_d(self) -> None:\n        ih1 = IndexHierarchy.from_product(\n            (\'1945-01-02\', \'1843-07-07\'), (\'2020-01\', \'2020-03\'))\n\n        self.assertEqual(\n                ih1.astype(\'datetime64[M]\').index_types.to_pairs(),\n                ((0, IndexYearMonth),\n                (1, IndexYearMonth))\n                )\n\n    @skip_win #type: ignore\n    def test_index_hierarchy_astype_e(self) -> None:\n        ih1 = IndexHierarchy.from_product((1, 2), (100, 200), (\'2020-01\', \'2020-03\'))\n\n        self.assertEqual(\n                ih1.astype[[0, 1]](float).dtypes.to_pairs(),\n                ((0, np.dtype(\'float64\')), (1, np.dtype(\'float64\')), (2, np.dtype(\'<U7\')))\n                )\n\n        ih2 = ih1.astype[2](\'datetime64[M]\')\n\n        self.assertEqual(\n                ih2.dtypes.to_pairs(),\n                ((0, np.dtype(\'int64\')), (1, np.dtype(\'int64\')), (2, np.dtype(\'<M8[M]\')))\n                )\n\n        self.assertEqual(ih2.index_types.to_pairs(),\n                ((0, Index), (1, Index), (2, IndexYearMonth))\n                )\n\n        post = ih2.loc[HLoc[:, 200, \'2020-03\']]\n        self.assertEqual(post.shape, (2, 3))\n        self.assertEqual(post.dtypes.to_pairs(),\n                ((0, np.dtype(\'int64\')), (1, np.dtype(\'int64\')), (2, np.dtype(\'<M8[M]\')))\n                )\n\n\n    #---------------------------------------------------------------------------\n\n    @skip_win #type: ignore\n    def test_index_hierarchy_values_at_depth_a(self) -> None:\n        ih1 = IndexHierarchy.from_product((1, 2), (100, 200), (\'2020-01\', \'2020-03\'))\n        post = ih1.values_at_depth([0, 1])\n        self.assertEqual(post.shape, (8, 2))\n        self.assertEqual(post.dtype, np.dtype(int))\n        self.assertEqual(ih1.values_at_depth(2).dtype, np.dtype(\'<U7\'))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_hierarchy_head_a(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((1, 2), (\'a\', \'b\'), (2, 5))\n\n        self.assertEqual(ih1.head().values.tolist(),\n            [[1, \'a\', 2], [1, \'a\', 5], [1, \'b\', 2], [1, \'b\', 5], [2, \'a\', 2]]\n            )\n\n    def test_index_hierarchy_tail_a(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((1, 2), (\'a\', \'b\'), (2, 5))\n\n        self.assertEqual(ih1.tail().values.tolist(),\n            [[1, \'b\', 5], [2, \'a\', 2], [2, \'a\', 5], [2, \'b\', 2], [2, \'b\', 5]]\n            )\n\n    #---------------------------------------------------------------------------\n\n    def test_index_hierarchy_via_str_a(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((\'i\', \'ii\'), (\'a\', \'b\'))\n        ih2 = ih1.via_str.upper()\n\n        self.assertEqual(ih2.tolist(),\n                [[\'I\', \'A\'], [\'I\', \'B\'], [\'II\', \'A\'], [\'II\', \'B\']]\n                )\n\n    def test_index_hierarchy_via_dt_a(self) -> None:\n        index_constructors = (IndexYearMonth, IndexDate)\n\n        labels = (\n            (\'2020-01\', \'2019-01-01\'),\n            (\'2020-01\', \'2019-02-01\'),\n            (\'2019-02\', \'2019-01-01\'),\n            (\'2019-02\', \'2019-02-01\'),\n        )\n\n        ih1 = IndexHierarchy.from_labels(labels, index_constructors=index_constructors)\n        ih2 = ih1.via_dt.month\n\n        self.assertEqual(\n                ih2.tolist(),\n                [[1, 1], [1, 2], [2, 1], [2, 2]]\n                )\n\n\n    def test_index_hierarchy_via_dt_b(self) -> None:\n        index_constructors = (IndexDate, IndexDate)\n\n        labels = (\n            (\'2020-01-03\', \'2019-01-01\'),\n            (\'2020-01-03\', \'2019-02-01\'),\n            (\'2019-02-05\', \'2019-01-01\'),\n            (\'2019-02-05\', \'2019-02-01\'),\n        )\n\n        ih1 = IndexHierarchy.from_labels(labels, index_constructors=index_constructors)\n        ih2 = ih1.via_dt.isoformat()\n\n        self.assertEqual(\n            ih2.dtype, np.dtype(\'<U10\'),\n            )\n        self.assertEqual(\n                ih2.tolist(),\n                [[\'2020-01-03\', \'2019-01-01\'], [\'2020-01-03\', \'2019-02-01\'], [\'2019-02-05\', \'2019-01-01\'], [\'2019-02-05\', \'2019-02-01\']]\n                )\n\n        ih3 = ih1.via_dt.strftime(\'%y|%m|%d\')\n        self.assertEqual(\n            ih3.dtype, np.dtype(\'<U8\'),\n            )\n\n        self.assertEqual(\n            ih3.tolist(),\n            [[\'20|01|03\', \'19|01|01\'], [\'20|01|03\', \'19|02|01\'], [\'19|02|05\', \'19|01|01\'], [\'19|02|05\', \'19|02|01\']]\n            )\n\n    #---------------------------------------------------------------------------\n\n    def test_index_hierarchy_equals_a(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((1, 2), (\'a\', \'b\'), (2, 5))\n        ih2 = IndexHierarchy.from_product((1, 2), (\'a\', \'b\'), (2, 5))\n        ih3 = IndexHierarchy.from_product((1, 2), (\'a\', \'b\'), (2, 4))\n        ih4 = IndexHierarchy.from_product((1, 2), (\'a\', \'b\'), (2, 4), name=\'foo\')\n\n        self.assertTrue(ih1.equals(ih1))\n        self.assertTrue(ih1.equals(ih2))\n        self.assertTrue(ih2.equals(ih1))\n\n        self.assertFalse(ih1.equals(ih3))\n        self.assertFalse(ih3.equals(ih1))\n\n        self.assertFalse(ih3.equals(ih4, compare_name=True))\n        self.assertTrue(ih3.equals(ih4, compare_name=False))\n\n    def test_index_hierarchy_equals_b(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((1, 2), (\'a\', \'b\'), Index((2, 5), dtype=np.int64))\n        ih2 = IndexHierarchy.from_product((1, 2), (\'a\', \'b\'), Index((2, 5), dtype=np.int32))\n\n        self.assertFalse(ih1.equals(ih2, compare_dtype=True))\n        self.assertTrue(ih1.equals(ih2, compare_dtype=False))\n\n    def test_index_hierarchy_equals_c(self) -> None:\n\n        idx = IndexDate.from_year_month_range(\'2020-01\', \'2020-02\')\n\n        ih1 = IndexHierarchy.from_product((1, 2), (\'a\', \'b\'), idx)\n        ih2 = IndexHierarchy.from_product((1, 2), (\'a\', \'b\'), Index(idx.values))\n\n        self.assertFalse(ih1.equals(ih2, compare_class=True))\n        self.assertTrue(ih1.equals(ih2, compare_class=False))\n\n    def test_index_hierarchy_equals_d(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((1, 2), (\'a\', \'b\'), (2, 5))\n        ih2 = IndexHierarchyGO.from_product((1, 2), (\'a\', \'b\'), (2, 5))\n\n        self.assertFalse(ih1.equals(ih2, compare_class=True))\n        self.assertTrue(ih1.equals(ih2, compare_class=False))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
static_frame/test/unit/test_index_level.py,7,"b""\nimport unittest\nimport numpy as np\n\nfrom collections import OrderedDict\n\nfrom static_frame import Index\nfrom static_frame import IndexGO\nfrom static_frame import IndexDate\nfrom static_frame import ILoc\nfrom static_frame import HLoc\nfrom static_frame import Frame\n\nfrom static_frame import IndexHierarchy\n# from static_frame import IndexHierarchyGO\nfrom static_frame import IndexLevel\nfrom static_frame import IndexLevelGO\n# from static_frame import HLoc\n\nfrom static_frame.core.exception import ErrorInitIndexLevel\nfrom static_frame.core.array_go import ArrayGO\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.test.test_case import skip_win\n\n\nclass TestUnit(TestCase):\n\n    def test_index_level_a(self) -> None:\n        groups = IndexGO(('A', 'B'))\n        observations = IndexGO(('x', 'y'))\n        targets = np.array(\n                (IndexLevelGO(index=observations), IndexLevelGO(observations, offset=2)))\n\n        level0 = IndexLevelGO(index=groups, targets=ArrayGO(targets))\n        level1 = level0.to_index_level()\n\n        groups = IndexGO(('C', 'D'))\n        observations = IndexGO(('x', 'y', 'z'))\n        targets = np.array(\n                (IndexLevelGO(index=observations), IndexLevelGO(observations, offset=3)))\n\n        level2 = IndexLevelGO(index=groups, targets=ArrayGO(targets))\n\n        with self.assertRaises(RuntimeError):\n            level0.extend(IndexLevelGO(index=observations))\n\n        level0.extend(level2)\n\n        self.assertEqual(len(level0.values), 10)\n        self.assertEqual(len(level1.values), 4)\n        self.assertEqual(len(level2.values), 6)\n\n        assert level0.targets is not None\n\n        self.assertEqual([lvl.offset for lvl in level0.targets], [0, 2, 4, 7])\n\n        self.assertEqual(level2.depth, next(level2.depths()))\n\n\n    def test_index_level_b(self) -> None:\n        with self.assertRaises(ErrorInitIndexLevel):\n            _ = IndexLevel(('A', 'B')) #type: ignore\n\n\n    def test_index_level_dtypes_all_a(self) -> None:\n        level0 = IndexLevel(index=Index(('a', 'b')), targets=None)\n        post = tuple(level0.dtypes_iter())\n        self.assertEqual(post[0], np.dtype('<U1'))\n        self.assertEqual(len(post), 1)\n        self.assertEqual(level0.depth, next(level0.depths()))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_level_dtypes_a(self) -> None:\n        groups = IndexGO(('A', 'B'))\n        observations = IndexGO(('x', 'y'))\n        targets = ArrayGO(\n                (IndexLevelGO(index=observations),\n                IndexLevelGO(observations, offset=2)))\n        level0 = IndexLevelGO(index=groups, targets=targets)\n        self.assertEqual([d.kind for d in level0.dtype_per_depth()], ['U', 'U'])\n\n\n    def test_index_level_dtypes_b(self) -> None:\n        idx1 = Index(('A', 'B'))\n        idx2 = IndexDate.from_date_range('2019-01-05', '2019-01-08')\n        idx3 = Index((1, 2))\n\n        hidx = IndexHierarchy.from_product(idx1, idx2, idx3)\n\n        self.assertEqual([dt.kind for dt in hidx._levels.dtype_per_depth()],\n                ['U', 'M', 'i'],\n                )\n\n\n    def test_index_level_dtypes_c(self) -> None:\n        level0 = IndexLevel(index=Index(('a', 'b')), targets=None)\n        post = tuple(level0.dtype_per_depth())\n        self.assertEqual(post[0], np.dtype('<U1'))\n        self.assertEqual(len(post), 1)\n\n    #---------------------------------------------------------------------------\n    @skip_win #type: ignore\n    def test_index_level_dtypes_per_depth_a(self) -> None:\n        hidx = IndexHierarchy.from_labels((('a', 1, 'x'), ('a', 2, 'y'), ('b', 1, 'foo'), ('b', 1, 'bar')))\n        lvl = hidx._levels\n\n        self.assertEqual(\n                tuple(lvl.dtypes_at_depth(0)),\n                (np.dtype('<U1'),))\n        self.assertEqual(\n                tuple(lvl.dtypes_at_depth(1)),\n                (np.dtype('int64'), np.dtype('int64')))\n        self.assertEqual(\n                tuple(lvl.dtypes_at_depth(2)),\n                (np.dtype('<U1'), np.dtype('<U1'), np.dtype('<U3'))\n                )\n\n\n    def test_index_level_values_at_depth_a(self) -> None:\n        hidx = IndexHierarchy.from_labels((('a', 1, 'x'), ('a', 2, 'y'), ('b', 1, 'foo'), ('b', 1, 'bar')))\n        lvl = hidx._levels\n        self.assertEqual(lvl.values_at_depth(2).tolist(), ['x', 'y', 'foo', 'bar'])\n        self.assertEqual(lvl.depth, next(lvl.depths()))\n\n    def test_index_level_values_at_depth_b(self) -> None:\n\n        hidx = IndexHierarchy.from_labels((('a', 1, 'x'), ('a', 2, 'y'), ('b', 1, 'foo'), ('b', 2, None))) #type: ignore\n        lvl = hidx._levels\n        self.assertEqual(lvl.values_at_depth(2).tolist(), ['x', 'y', 'foo', None])\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_level_index_types_a(self) -> None:\n        idx1 = Index(('A', 'B'))\n        idx2 = IndexDate.from_date_range('2019-01-05', '2019-01-08')\n        idx3 = Index((1, 2))\n        hidx = IndexHierarchy.from_product(idx1, idx2, idx3)\n        lvl = hidx._levels\n        self.assertEqual(\n                [it.__name__ for it in lvl.index_types()],\n                ['Index', 'IndexDate', 'Index'])\n        self.assertEqual(lvl.depth, next(lvl.depths()))\n\n\n    def test_index_level_index_types_b(self) -> None:\n        level0 = IndexLevel(index=Index(('a', 'b')), targets=None)\n        post = tuple(level0.index_types())\n        self.assertEqual(post[0], Index)\n        self.assertEqual(len(post), 1)\n\n\n    #---------------------------------------------------------------------------\n    def test_index_level_contains_a(self) -> None:\n        level0 = IndexLevel(index=Index(('a', 'b')), targets=None)\n        self.assertFalse(('c',) in level0)\n        self.assertTrue(('a',) in level0)\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_level_extend_a(self) -> None:\n        level0 = IndexLevelGO(index=IndexGO(('a', 'b')), targets=None)\n        with self.assertRaises(RuntimeError):\n            level0.extend(level0)\n\n\n    #---------------------------------------------------------------------------\n\n\n    def test_index_level_get_labels_a(self) -> None:\n        groups = IndexGO(('A', 'B'))\n        observations = IndexGO(('x', 'y'))\n        targets = ArrayGO(\n                (IndexLevelGO(index=observations), IndexLevelGO(observations, offset=2)))\n\n        level0 = IndexLevelGO(index=groups, targets=targets)\n\n        self.assertEqual(level0.values.tolist(),\n                [['A', 'x'], ['A', 'y'], ['B', 'x'], ['B', 'y']])\n\n        self.assertEqual(level0.values.dtype.kind, 'U')\n\n        groups = IndexGO((1, 2))\n        observations = IndexGO((10, 20))\n        targets = ArrayGO(\n                (IndexLevelGO(index=observations), IndexLevelGO(observations, offset=2)))\n        level1 = IndexLevelGO(index=groups, targets=targets)\n        self.assertEqual(level1.values.dtype.kind, 'i')\n\n    #---------------------------------------------------------------------------\n\n    def test_index_level_leaf_loc_to_iloc_a(self) -> None:\n\n        groups = Index(('A', 'B', 'C'))\n        dates = IndexDate.from_date_range('2018-01-01', '2018-01-04')\n        observations = Index(('x', 'y'))\n\n        lvl2a = IndexLevel(index=observations)\n        lvl2b = IndexLevel(index=observations, offset=2)\n        lvl2c = IndexLevel(index=observations, offset=4)\n        lvl2d = IndexLevel(index=observations, offset=6)\n\n        lvl2_targets = ArrayGO((lvl2a, lvl2b, lvl2c, lvl2d))\n\n        lvl1a = IndexLevel(index=dates,\n                targets=lvl2_targets, offset=0)\n        lvl1b = IndexLevel(index=dates,\n                targets=lvl2_targets, offset=len(lvl1a))\n        lvl1c = IndexLevel(index=dates,\n                targets=lvl2_targets, offset=len(lvl1a) * 2)\n\n        # we need as many targets as len(index)\n        lvl0 = IndexLevel(index=groups,\n                targets=ArrayGO((lvl1a, lvl1b, lvl1c)))\n\n        self.assertEqual(lvl0.leaf_loc_to_iloc(('B', '2018-01-04', 'y'),), 15)\n        self.assertEqual(lvl0.leaf_loc_to_iloc(('A', '2018-01-01', 'y')), 1)\n\n\n    def test_index_level_leaf_loc_to_iloc_b(self) -> None:\n\n        level0 = IndexLevel(index=Index(('a', 'b')), targets=None)\n        self.assertEqual(level0.leaf_loc_to_iloc(('b',)), 1)\n        self.assertEqual(level0.leaf_loc_to_iloc(ILoc[1]), 1) #type: ignore\n\n\n    def test_index_level_leaf_loc_to_iloc_c(self) -> None:\n\n        groups = Index(('A', 'B', 'C'))\n        dates = IndexDate.from_date_range('2018-01-01', '2018-01-04')\n        observations = Index(('x', 'y'))\n\n        lvl2a = IndexLevel(index=observations)\n        lvl2b = IndexLevel(index=observations, offset=2)\n        lvl2c = IndexLevel(index=observations, offset=4)\n        lvl2d = IndexLevel(index=observations, offset=6)\n\n        lvl2_targets = ArrayGO((lvl2a, lvl2b, lvl2c, lvl2d))\n\n        lvl1a = IndexLevel(index=dates,\n                targets=lvl2_targets, offset=0)\n        lvl1b = IndexLevel(index=dates,\n                targets=lvl2_targets, offset=len(lvl1a))\n        lvl1c = IndexLevel(index=dates,\n                targets=lvl2_targets, offset=len(lvl1a) * 2)\n\n        lvl0 = IndexLevel(index=groups,\n                targets=ArrayGO((lvl1a, lvl1b, lvl1c)))\n\n        with self.assertRaises(KeyError):\n            lvl0.leaf_loc_to_iloc(('A'))\n\n        self.assertEqual(lvl0.leaf_loc_to_iloc(('A', '2018-01-01', 'y')), 1)\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_level_loc_to_iloc_a(self) -> None:\n\n        level0 = IndexLevel(index=Index(('a', 'b')), targets=None)\n        with self.assertRaises(KeyError):\n            level0.loc_to_iloc('a')\n\n\n    def test_index_level_loc_to_iloc_b(self) -> None:\n        level0 = IndexLevel(index=Index(('a', 'b')), targets=None)\n        with self.assertRaises(KeyError):\n            level0.loc_to_iloc(HLoc['c',])\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_level_append_a(self) -> None:\n\n        category = IndexGO(('I', 'II'))\n        groups = IndexGO(('A', 'B'))\n        observations = IndexGO(('x', 'y'))\n\n        lvl2a = IndexLevelGO(index=observations)\n        lvl2b = IndexLevelGO(index=observations, offset=2)\n        lvl2_targets = ArrayGO((lvl2a, lvl2b))\n        # must defensively copy index\n        assert id(lvl2a.index) != id(lvl2b.index)\n\n        lvl1a = IndexLevelGO(index=groups,\n                targets=lvl2_targets,\n                offset=0)\n\n\n        # must create new index levels for each lower node\n        lvl2c = IndexLevelGO(index=observations)\n        lvl2d = IndexLevelGO(index=observations, offset=2)\n        lvl2_targets = ArrayGO((lvl2c, lvl2d))\n        # must defensively copy index\n        assert id(lvl2c.index) != id(lvl2d.index)\n\n        lvl1b = IndexLevelGO(index=groups,\n                targets=lvl2_targets,\n                offset=len(lvl1a))\n\n        # we need as many targets as len(index)\n        lvl0 = IndexLevelGO(index=category,\n                targets=ArrayGO((lvl1a, lvl1b)))\n\n        with self.assertRaises(RuntimeError):\n            # RuntimeError: level for extension does not have necessary levels.\n            lvl0.extend(lvl1b)\n\n        lvl0.append(('II', 'B', 'z')) # depth not found is 2\n        self.assertEqual(\n                [lvl0.loc_to_iloc(tuple(x)) for x in lvl0.values],\n                list(range(9)))\n\n        lvl0.append(('II', 'C', 'a')) # depth not found is 1\n        self.assertEqual(\n                [lvl0.loc_to_iloc(tuple(x)) for x in lvl0.values],\n                list(range(10)))\n\n        lvl0.append(('III', 'A', 'a')) # 0\n\n        self.assertEqual(\n                [lvl0.loc_to_iloc(tuple(x)) for x in lvl0.values],\n                list(range(11)))\n\n        self.assertEqual(\n                lvl0.values.tolist(),\n                [['I', 'A', 'x'], ['I', 'A', 'y'], ['I', 'B', 'x'], ['I', 'B', 'y'], ['II', 'A', 'x'], ['II', 'A', 'y'], ['II', 'B', 'x'], ['II', 'B', 'y'], ['II', 'B', 'z'], ['II', 'C', 'a'], ['III', 'A', 'a']])\n\n    def test_index_level_append_b(self) -> None:\n\n        groups = IndexGO(('A', 'B'))\n        observations = IndexGO(('x', 'y'))\n        lvl2a = IndexLevelGO(index=observations)\n        lvl2b = IndexLevelGO(index=observations, offset=2)\n        lvl2_targets = ArrayGO((lvl2a, lvl2b))\n        lvl1a = IndexLevelGO(index=groups,\n                targets=lvl2_targets,\n                offset=0)\n\n        with self.assertRaises(RuntimeError):\n            lvl1a.append((1, 2, 3))\n\n        lvl1a.append((1, 2))\n        self.assertEqual(lvl1a.values.tolist(),\n                [['A', 'x'], ['A', 'y'], ['B', 'x'], ['B', 'y'], [1, 2]])\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_index_level_iter_a(self) -> None:\n        OD = OrderedDict\n        tree = OD([\n                ('I', OD([\n                        ('A', (1, 2)), ('B', (1, 2, 3)), ('C', (2, 3))\n                        ])\n                ),\n                ('II', OD([\n                        ('A', (1, 2, 3)), ('B', (1,))\n                        ])\n                ),\n                ])\n\n        levels = IndexHierarchy._tree_to_index_level(tree)\n\n        post = list(levels.label_nodes_at_depth(0))\n        self.assertEqual(post, ['I', 'II'])\n\n        post = list(levels.label_nodes_at_depth(1))\n        self.assertEqual(post, ['A', 'B', 'C', 'A', 'B'])\n\n        post = list(levels.label_nodes_at_depth(2))\n        self.assertEqual(post, [1, 2, 1, 2, 3, 2, 3, 1, 2, 3, 1])\n\n\n    def test_index_level_iter_b(self) -> None:\n        OD = OrderedDict\n        tree = OD([\n                ('I', OD([\n                        ('A', (1, 2)), ('B', (1, 2, 3)), ('C', (2, 3))\n                        ])\n                ),\n                ('II', OD([\n                        ('A', (1, 2, 3)), ('B', (1,))\n                        ])\n                ),\n                ])\n\n        levels = IndexHierarchy._tree_to_index_level(tree)\n        tuples = tuple(levels)\n        self.assertEqual(\n                tuples,\n                (('I', 'A', 1), ('I', 'A', 2), ('I', 'B', 1), ('I', 'B', 2), ('I', 'B', 3), ('I', 'C', 2), ('I', 'C', 3), ('II', 'A', 1), ('II', 'A', 2), ('II', 'A', 3), ('II', 'B', 1))\n                )\n\n\n    def test_index_level_label_widths_at_depth_a(self) -> None:\n        OD = OrderedDict\n        tree = OD([\n                ('I', OD([\n                        ('A', (1, 2)), ('B', (1, 2, 3)), ('C', (2, 3))\n                        ])\n                ),\n                ('II', OD([\n                        ('A', (1,)), ('B', (1,))\n                        ])\n                ),\n                ('III', OD([\n                        ('A', (1, 2, 3)), ('B', (1,))\n                        ])\n                ),\n                ])\n\n        levels = IndexHierarchy._tree_to_index_level(tree)\n\n        post0 = tuple(levels.label_widths_at_depth(0))\n        post1 = tuple(levels.label_widths_at_depth(1))\n        post2 = tuple(levels.label_widths_at_depth(2))\n\n        self.assertEqual(post0, (('I', 7), ('II', 2), ('III', 4)))\n\n        self.assertEqual(post1,\n            (('A', 2), ('B', 3), ('C', 2), ('A', 1), ('B', 1), ('A', 3), ('B', 1))\n        )\n\n        self.assertEqual(post2,\n            (((1, 1), (2, 1), (1, 1), (2, 1), (3, 1), (2, 1), (3, 1), (1, 1), (1, 1), (1, 1), (2, 1), (3, 1), (1, 1)))\n        )\n\n\n    def test_index_level_label_widths_at_depth_b(self) -> None:\n        f = Frame.from_dict(\n            dict(a=(1,2,3,4), b=(True, False, True, False), c=list('qrst')))\n        f = f.set_index_hierarchy(['a', 'b'])\n\n        post1 = tuple(f.index._levels.label_widths_at_depth(0))\n        self.assertEqual(post1, ((1, 1), (2, 1), (3, 1), (4, 1)))\n        post2 = tuple(f.index._levels.label_widths_at_depth(1))\n        self.assertEqual(post2, ((True, 1), (False, 1), (True, 1), (False, 1)))\n\n\n    def test_index_level_label_widths_at_depth_c(self) -> None:\n        labels = (\n                ('I', 'A', 1),\n                ('I', 'B', 2),\n                ('II', 'C', 3),\n                ('II', 'C', 4),\n                )\n        ih = IndexHierarchy.from_labels(labels)\n        lavels = ih._levels\n\n        self.assertEqual(tuple(lavels.label_widths_at_depth(0)),\n                (('I', 2), ('II', 2))\n        )\n        self.assertEqual(tuple(lavels.label_widths_at_depth(1)),\n                (('A', 1), ('B', 1), ('C', 2))\n        )\n        self.assertEqual(tuple(lavels.label_widths_at_depth(2)),\n                ((1, 1), (2, 1), (3, 1), (4, 1))\n        )\n\n    def test_index_level_label_widths_at_depth_d(self) -> None:\n        labels = (\n                ('I', 'A', 1),\n                ('I', 'B', 2),\n                ('I', 'B', 3),\n                ('II', 'C', 3),\n                ('II', 'C', 4),\n                )\n        ih = IndexHierarchy.from_labels(labels)\n        lavels = ih._levels\n\n        self.assertEqual(tuple(lavels.label_widths_at_depth(0)),\n                (('I', 3), ('II', 2))\n        )\n        self.assertEqual(tuple(lavels.label_widths_at_depth(1)),\n                (('A', 1), ('B', 2), ('C', 2))\n        )\n        self.assertEqual(tuple(lavels.label_widths_at_depth(2)),\n                ((1, 1), (2, 1), (3, 1), (3, 1), (4, 1))\n        )\n\n\n    def test_index_levels_with_tuple_a(self) -> None:\n        OD = OrderedDict\n        tree = OD([\n                (('I', 'I'), OD([\n                        ('A', (1, 2)), ('B', (1, 2, 3)), ('C', (2, 3))\n                        ])\n                ),\n                (('II', 'II'), OD([\n                        ('A', (1,)), ('B', (1,))\n                        ])\n                ),\n                ])\n\n        levels = IndexHierarchy._tree_to_index_level(tree)\n        self.assertEqual(levels.depth, 3)\n        self.assertEqual(levels.loc_to_iloc((('II', 'II'), 'B', 1)), 8)\n\n\n\n    #---------------------------------------------------------------------------\n    def test_index_levels_equals_a(self) -> None:\n        OD = OrderedDict\n        tree1 = OD([\n                (('I', 'I'), OD([\n                        ('A', (1, 2)), ('B', (1, 2, 3)), ('C', (2, 3))\n                        ])\n                ),\n                (('II', 'II'), OD([\n                        ('A', (1,)), ('B', (1,))\n                        ])\n                ),\n                ])\n\n        levels1 = IndexHierarchy._tree_to_index_level(tree1)\n\n        tree2 = OD([\n                (('I', 'I'), OD([\n                        ('A', (1, 2)), ('B', (1, 2, 3)), ('C', (2, 3))\n                        ])\n                ),\n                (('II', 'II'), OD([\n                        ('A', (1,)), ('B', (1,))\n                        ])\n                ),\n                ])\n\n        levels2 = IndexHierarchy._tree_to_index_level(tree2)\n\n        tree3 = OD([\n                (('I', 'I'), OD([\n                        ('A', (1, 2)), ('B', (1, 2, 3)), ('C', (2, 3))\n                        ])\n                ),\n                (('II', 'II'), OD([\n                        ('A', (1,)), ('B', (0,)) # diff\n                        ])\n                ),\n                ])\n\n        levels3 = IndexHierarchy._tree_to_index_level(tree3)\n\n\n\n        self.assertTrue(levels1.equals(levels1))\n        self.assertTrue(levels1.equals(levels2))\n        self.assertTrue(levels2.equals(levels1))\n\n        self.assertFalse(levels2.equals(levels3))\n\n    def test_index_levels_equals_b(self) -> None:\n\n        idx1 = Index(('a', 'b', 'c', 'd', 'e'))\n        idx2 = Index(range(10))\n        levels1 = IndexHierarchy.from_product(idx1, idx2)._levels\n\n        idx3 = Index(('a', 'b', 'c', 'd', 'e'))\n        idx4 = Index(range(10))\n        levels2 = IndexHierarchy.from_product(idx3, idx4)._levels\n\n        self.assertTrue(levels1.equals(levels2))\n\n\n\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n"""
static_frame/test/unit/test_interface.py,1,"b""\nimport unittest\n\nimport numpy as np\n\nfrom static_frame.core.interface import InterfaceSummary\nfrom static_frame.core.series import Series\nfrom static_frame.core.frame import FrameGO\n\nfrom static_frame.core.interface import _get_signatures\n\n\n\nfrom static_frame.test.test_case import TestCase\n\n\n\nclass TestUnit(TestCase):\n\n    def test_interface_summary_a(self) -> None:\n\n        for target in self.get_containers():\n            t = InterfaceSummary.to_frame(target)\n            self.assertTrue(len(t) > 30)\n\n\n    def test_interface_summary_b(self) -> None:\n\n        post = FrameGO.interface\n        counts = post.iter_group('group').apply(len)\n\n        self.assertEqual(\n            counts.to_pairs(),\n            (('Accessor Datetime', 7), ('Accessor String', 35), ('Assignment', 4), ('Attribute', 11), ('Constructor', 27), ('Dictionary-Like', 7), ('Display', 6), ('Exporter', 18), ('Iterator', 224), ('Method', 62), ('Operator Binary', 24), ('Operator Unary', 4), ('Selector', 13))\n        )\n\n    def test_interface_summary_c(self) -> None:\n        s = Series(['a', 'b', 'c'])\n        post = s.interface\n\n        counts = post.iter_group('group').apply(len)\n        counts_cls = s.__class__.interface.iter_group('group').apply(len)\n\n        self.assertTrue((counts == counts_cls).all())\n\n\n    def test_interface_get_signatures_a(self) -> None:\n\n        sig, signa = _get_signatures('__init__', Series.__init__)\n\n        self.assertEqual(sig, '__init__(values, *, index, name, ...)')\n        self.assertEqual(signa, '__init__()')\n\n    def test_interface_get_signatures_b(self) -> None:\n\n        sig, signa = _get_signatures('__init__', Series.__init__, max_args=99)\n\n        self.assertEqual(sig, '__init__(values, *, index, name, dtype, index_constructor, own_index)')\n        self.assertEqual(signa, '__init__()')\n\n    def test_interface_get_frame_a(self) -> None:\n\n        f1 = InterfaceSummary.to_frame(Series)\n        f2 = InterfaceSummary.to_frame(Series, minimized=False, max_args=np.inf)\n        self.assertTrue(len(f1) == len(f2))\n\n        self.assertEqual(f1.columns.values.tolist(),\n                ['cls_name', 'group', 'doc']\n                )\n        self.assertEqual(\n                f2.columns.values.tolist(),\n                ['cls_name', 'group', 'doc', 'reference', 'use_signature', 'is_attr', 'delegate_reference', 'delegate_is_attr', 'signature_no_args']\n                )\n\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n\n\n\n"""
static_frame/test/unit/test_series.py,117,"b'\nimport unittest\nfrom collections import OrderedDict\nfrom io import StringIO\nimport string\nimport pickle\nimport datetime\nimport typing as tp\nfrom enum import Enum\nimport numpy as np\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.test.test_case import temp_file\n\nimport static_frame as sf\n# assuming located in the same directory\nfrom static_frame import Index\nfrom static_frame import IndexGO\nfrom static_frame import Series\nfrom static_frame import Frame\nfrom static_frame import FrameGO\n# from static_frame import TypeBlocks\n# from static_frame import Display\nfrom static_frame import mloc\nfrom static_frame import DisplayConfig\nfrom static_frame import IndexHierarchy\nfrom static_frame import IndexHierarchyGO\nfrom static_frame import IndexDate\nfrom static_frame import IndexSecond\nfrom static_frame import IndexYearMonth\nfrom static_frame import IndexAutoFactory\n\nfrom static_frame import HLoc\n\nfrom static_frame.core.exception import AxisInvalid\nfrom static_frame.core.exception import ErrorInitSeries\n\nnan = np.nan\n\nLONG_SAMPLE_STR = \'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\'\n\n\nclass TestUnit(TestCase):\n\n    #---------------------------------------------------------------------------\n    # test series\n\n    def test_series_slotted_a(self) -> None:\n        s1 = Series.from_element(10, index=(\'a\', \'b\', \'c\', \'d\'))\n\n        with self.assertRaises(AttributeError):\n            s1.g = 30 #type: ignore #pylint: disable=E0237\n        with self.assertRaises(AttributeError):\n            s1.__dict__ #pylint: disable=W0104\n\n    def test_series_init_a(self) -> None:\n        s1 = Series.from_element(np.nan, index=(\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertTrue(s1.dtype == float)\n        self.assertTrue(len(s1) == 4)\n\n        s2 = Series.from_element(False, index=(\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertTrue(s2.dtype == bool)\n        self.assertTrue(len(s2) == 4)\n\n        s3 = Series.from_element(None, index=(\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertTrue(s3.dtype == object)\n        self.assertTrue(len(s3) == 4)\n\n\n    def test_series_init_b(self) -> None:\n        s1 = Series([\'a\', \'b\', \'c\', \'d\'], index=(\'a\', \'b\', \'c\', \'d\'))\n        self.assertEqual(s1.to_pairs(),\n                ((\'a\', \'a\'), (\'b\', \'b\'), (\'c\', \'c\'), (\'d\', \'d\')))\n\n        # testing direct specification of string type\n        s2 = Series([\'a\', \'b\', \'c\', \'d\'], index=(\'a\', \'b\', \'c\', \'d\'), dtype=str)\n        self.assertEqual(s2.to_pairs(),\n                ((\'a\', \'a\'), (\'b\', \'b\'), (\'c\', \'c\'), (\'d\', \'d\')))\n\n    def test_series_init_c(self) -> None:\n\n        s1 = Series.from_dict(OrderedDict([(\'b\', 4), (\'a\', 1)]), dtype=np.int64)\n        self.assertEqual(s1.to_pairs(),\n                ((\'b\', 4), (\'a\', 1)))\n\n    def test_series_init_d(self) -> None:\n        # single element, when the element is a string\n        s1 = Series.from_element(\'abc\', index=range(4))\n        self.assertEqual(s1.to_pairs(),\n                ((0, \'abc\'), (1, \'abc\'), (2, \'abc\'), (3, \'abc\')))\n\n        # this is an array with shape == (), or a single element\n        s2 = Series(np.array(\'abc\'), index=range(4))\n        self.assertEqual(s2.to_pairs(),\n                ((0, \'abc\'), (1, \'abc\'), (2, \'abc\'), (3, \'abc\')))\n\n        # single element, generator index\n        s3 = Series.from_element(None, index=(x * 10 for x in (1,2,3)))\n        self.assertEqual(s3.to_pairs(),\n                ((10, None), (20, None), (30, None))\n                )\n\n    def test_series_init_e(self) -> None:\n        s1 = Series.from_dict(dict(a=1, b=2, c=np.nan, d=None), dtype=object)\n        self.assertEqual(s1.to_pairs(),\n                ((\'a\', 1), (\'b\', 2), (\'c\', nan), (\'d\', None))\n                )\n        with self.assertRaises(ValueError):\n            s1.values[1] = 23\n\n    def test_series_init_f(self) -> None:\n        s1 = Series.from_dict({\'a\': \'x\', \'b\': \'y\', \'c\': \'z\'})\n        self.assertEqual(s1.to_pairs(), ((\'a\', \'x\'), (\'b\', \'y\'), (\'c\', \'z\')))\n\n    def test_series_init_g(self) -> None:\n        with self.assertRaises(RuntimeError):\n            s1 = Series(range(4), own_index=True, index=None)\n\n    def test_series_init_h(self) -> None:\n        s1 = Series(range(4), index_constructor=IndexSecond)\n        self.assertEqual(s1.to_pairs(),\n            ((np.datetime64(\'1970-01-01T00:00:00\'), 0),\n            (np.datetime64(\'1970-01-01T00:00:01\'), 1),\n            (np.datetime64(\'1970-01-01T00:00:02\'), 2),\n            (np.datetime64(\'1970-01-01T00:00:03\'), 3)))\n\n    def test_series_init_i(self) -> None:\n        s1 = Series((3, 4, \'a\'))\n        self.assertEqual(s1.values.tolist(),\n                [3, 4, \'a\']\n                )\n\n    def test_series_init_j(self) -> None:\n        s1 = Series((3, 4, \'a\'), index=IndexAutoFactory)\n        self.assertEqual(s1.to_pairs(),\n                ((0, 3), (1, 4), (2, \'a\')))\n\n    def test_series_init_k(self) -> None:\n        s1 = Series.from_element(\'cat\', index=(1, 2, 3))\n        self.assertEqual(s1.to_pairs(),\n                ((1, \'cat\'), (2, \'cat\'), (3, \'cat\'))\n                )\n\n    def test_series_init_l(self) -> None:\n        s1 = Series(([None], [1, 2], [\'a\', \'b\']), index=(1, 2, 3))\n        self.assertEqual(s1[2:].to_pairs(),\n                ((2, [1, 2]), (3, [\'a\', \'b\'])))\n        self.assertEqual((s1 * 2).to_pairs(),\n                ((1, [None, None]), (2, [1, 2, 1, 2]), (3, [\'a\', \'b\', \'a\', \'b\']))\n                )\n\n    def test_series_init_m(self) -> None:\n\n        # if index is None or IndexAutoFactory, we supply an index of 0\n        s1 = Series.from_element(\'a\', index=(0,))\n        self.assertEqual(s1.to_pairs(),\n                ((0, \'a\'),))\n\n        # an element with an explicitl empty index results in an empty series\n        s2 = Series.from_element(\'a\', index=())\n        self.assertEqual(s2.to_pairs(), ())\n\n    def test_series_init_n(self) -> None:\n        with self.assertRaises(RuntimeError):\n            s1 = Series(np.array([[\'a\', \'b\']]))\n\n        s2 = Series([[\'a\', \'b\']], dtype=object)\n        self.assertEqual(s2.to_pairs(),\n            ((0, [\'a\', \'b\']),)\n            )\n\n    def test_series_init_o(self) -> None:\n        with self.assertRaises(ErrorInitSeries):\n            s1 = Series(\'T\', index=range(3))\n\n        s1 = Series.from_element(\'T\', index=())\n        self.assertEqual(s1.to_pairs(), ())\n\n\n    def test_series_init_p(self) -> None:\n        # 3d array raises exception\n        a1 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n        with self.assertRaises(RuntimeError):\n            s1 = Series(a1)\n\n\n    def test_series_init_q(self) -> None:\n        with self.assertRaises(RuntimeError):\n            s1 = Series(dict(a=3, b=4))\n\n\n    def test_series_init_r(self) -> None:\n        with self.assertRaises(RuntimeError):\n            s1 = Series(np.array((3, 4, 5)), dtype=object)\n\n\n    def test_series_init_s(self) -> None:\n        s1 = Series(np.array(\'a\'))\n        self.assertEqual(s1.to_pairs(), ((0, \'a\'),))\n\n\n    def test_series_init_t(self) -> None:\n        s1 = Series((\'a\', \'b\', \'c\'), index=(10, 20, 30))\n        s2 = Series(s1)\n        s3 = Series(values=s1)\n\n        # experimented with, but did not enable, for object aliasing when immutable\n        self.assertTrue(id(s1) != id(s2))\n        self.assertTrue(id(s1) != id(s3))\n\n        # same array is used\n        self.assertTrue(id(s1.values) == id(s2.values))\n        self.assertTrue(id(s1.values) == id(s3.values))\n\n        # same index is used\n        self.assertTrue(id(s1.index) == id(s2.index))\n        self.assertTrue(id(s1.index) == id(s3.index))\n\n        # can swap in a different index\n        s4 = Series(s1, index=(\'x\', \'y\', \'z\'))\n        self.assertEqual(s4.to_pairs(),\n                ((\'x\', \'a\'), (\'y\', \'b\'), (\'z\', \'c\'))\n                )\n        self.assertTrue(id(s1.values) == id(s4.values))\n\n        with self.assertRaises(ErrorInitSeries):\n            Series(s1, dtype=float)\n\n\n\n    def test_series_init_u(self) -> None:\n        with self.assertRaises(ErrorInitSeries):\n            s1 = Series((\'a\', \'b\', \'c\'), index=(10, 30))\n\n        with self.assertRaises(ErrorInitSeries):\n            s1 = Series((\'a\', \'b\', \'c\'), index=())\n\n        with self.assertRaises(ErrorInitSeries):\n            s1 = Series((\'a\', \'b\', \'c\'), index=(10, 20, 30, 40))\n\n        with self.assertRaises(ErrorInitSeries):\n            s1 = Series(range(3), index=range(2))\n\n        with self.assertRaises(ErrorInitSeries):\n            s1 = Series(range(3), index=Index(range(2)), own_index=True)\n\n\n        s1 = Series(np.array(3), index=(10, 20, 30, 40))\n        self.assertEqual(s1.to_pairs(),\n                ((10, 3), (20, 3), (30, 3), (40, 3))\n                )\n        s2 = Series(np.array(3))\n        self.assertEqual(s2.to_pairs(), ((0, 3),))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_slice_a(self) -> None:\n        # create a series from a single value\n\n        # generator based construction of values and index\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        s2 = s1[\'a\':\'c\']   # type: ignore  # https://github.com/python/typeshed/pull/3024  # with Pandas this is inclusive\n        self.assertEqual(s2.values.tolist(), [0, 1, 2])\n        self.assertTrue(s2[\'b\'] == s1[\'b\'])\n\n        s3 = s1[\'c\':]  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(s3.values.tolist(), [2, 3])\n        self.assertTrue(s3[\'d\'] == s1[\'d\'])\n\n        self.assertEqual(s1[\'b\':\'d\'].values.tolist(), [1, 2, 3])  # type: ignore  # https://github.com/python/typeshed/pull/3024\n\n        self.assertEqual(s1[[\'a\', \'c\']].values.tolist(), [0, 2])\n\n    def test_series_slice_b(self) -> None:\n\n        # using step sizes mixed with locs\n        s1 = sf.Series([1, 2, 3], index=[\'a\', \'b\', \'c\'])[\'b\'::-1] #type: ignore\n\n        self.assertEqual(s1.to_pairs(),\n                ((\'b\', 2), (\'a\', 1)))\n\n    def test_series_slice_c(self) -> None:\n\n        # using step sizes mixed with locs\n        s1 = sf.Series(range(10), index=IndexDate.from_date_range(\'2019-12-30\', \'2020-01-08\'))\n\n        s2 = s1.loc[np.datetime64(\'2020-01-07\'): np.datetime64(\'2020-01-02\'): -1]\n\n        self.assertEqual(s2.to_pairs(),\n                ((np.datetime64(\'2020-01-07\'), 8), (np.datetime64(\'2020-01-06\'), 7), (np.datetime64(\'2020-01-05\'), 6), (np.datetime64(\'2020-01-04\'), 5))\n                )\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_keys_a(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        self.assertEqual(list(s1.keys()), [\'a\', \'b\', \'c\', \'d\'])\n\n    def test_series_iter_a(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        self.assertEqual(list(s1), [\'a\', \'b\', \'c\', \'d\'])\n\n    def test_series_items_a(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        self.assertEqual(list(s1.items()), [(\'a\', 0), (\'b\', 1), (\'c\', 2), (\'d\', 3)])\n\n\n    def test_series_intersection_a(self) -> None:\n        # create a series from a single value\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        s3 = s1[\'c\':]  # type: ignore  # https://github.com/python/typeshed/pull/3024\n        self.assertEqual(s1.index.intersection(s3.index).values.tolist(),\n            [\'c\', \'d\'])\n\n\n    def test_series_intersection_b(self) -> None:\n        # create a series from a single value\n        idxa = IndexGO((\'a\', \'b\', \'c\'))\n        idxb = IndexGO((\'b\', \'c\', \'d\'))\n\n        self.assertEqual(idxa.intersection(idxb).values.tolist(),\n            [\'b\', \'c\'])\n\n        self.assertEqual(idxa.union(idxb).values.tolist(),\n            [\'a\', \'b\', \'c\', \'d\'])\n\n    #---------------------------------------------------------------------------\n\n\n    def test_series_binary_operator_a(self) -> None:\n        \'\'\'Test binary operators where one operand is a numeric.\n        \'\'\'\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n\n        self.assertEqual(list((s1 * 3).items()),\n                [(\'a\', 0), (\'b\', 3), (\'c\', 6), (\'d\', 9)])\n        self.assertEqual((s1 * 3).name, \'foo\')\n\n        self.assertEqual(list((s1 / .5).items()),\n                [(\'a\', 0.0), (\'b\', 2.0), (\'c\', 4.0), (\'d\', 6.0)])\n\n        self.assertEqual(list((s1 ** 3).items()),\n                [(\'a\', 0), (\'b\', 1), (\'c\', 8), (\'d\', 27)])\n        self.assertEqual((s1 ** 3).name, \'foo\')\n\n\n    def test_series_binary_operator_b(self) -> None:\n        \'\'\'Test binary operators with Series of same index\n        \'\'\'\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        s2 = Series((x * 2 for x in range(4)), index=(\'a\', \'b\', \'c\', \'d\'), name=\'bar\')\n\n        self.assertEqual(list((s1 + s2).items()),\n                [(\'a\', 0), (\'b\', 3), (\'c\', 6), (\'d\', 9)])\n\n        self.assertEqual((s1 + s2).name, None)\n\n        self.assertEqual(list((s1 * s2).items()),\n                [(\'a\', 0), (\'b\', 2), (\'c\', 8), (\'d\', 18)])\n\n\n    def test_series_binary_operator_c(self) -> None:\n        \'\'\'Test binary operators with Series of different index\n        \'\'\'\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        s2 = Series((x * 2 for x in range(4)), index=(\'c\', \'d\', \'e\', \'f\'))\n\n        self.assertAlmostEqualItems(list((s1 * s2).items()),\n                [(\'a\', nan), (\'b\', nan), (\'c\', 0), (\'d\', 6), (\'e\', nan), (\'f\', nan)]\n                )\n\n\n    def test_series_binary_operator_d(self) -> None:\n        s1 = Series(range(4), index=list(\'abcd\'))\n        s2 = Series(range(3), index=list(\'abc\'))\n        s3 = s1 + s2\n\n        self.assertEqual(s3.fillna(None).to_pairs(),\n                ((\'a\', 0), (\'b\', 2), (\'c\', 4), (\'d\', None))\n                )\n\n        s1 = Series((False, True, False, True), index=list(\'abcd\'))\n        s2 = Series([True] * 3, index=list(\'abc\'))\n\n        # NOTE: for now, we cannot resolve this case, as after reindexing we get an object array that is not compatible with Boolean array for the NaN4\n        with self.assertRaises(TypeError):\n            s3 = s1 | s2\n\n\n    def test_series_binary_operator_e(self) -> None:\n\n        s1 = Series((False, True, False, True), index=list(\'abcd\'), name=\'foo\')\n        s2 = Series([True] * 3, index=list(\'abc\'))\n\n        self.assertEqual((s1 == -1).to_pairs(),\n                ((\'a\', False), (\'b\', False), (\'c\', False), (\'d\', False)))\n\n        self.assertEqual((s1 == s2).to_pairs(),\n                ((\'a\', False), (\'b\', True), (\'c\', False), (\'d\', False)))\n\n        self.assertEqual((s1 == True).to_pairs(),\n                ((\'a\', False), (\'b\', True), (\'c\', False), (\'d\', True)))\n        self.assertEqual((s1 == True).name, \'foo\')\n\n        # NOTE: these are unexpected results that derive from NP Boolean operator behaviors\n\n        self.assertEqual((s1 == (True,)).to_pairs(),\n                ((\'a\', False), (\'b\', True), (\'c\', False), (\'d\', True)))\n\n        self.assertEqual((s1 == (True, False)).to_pairs(),\n                ((\'a\', False), (\'b\', False), (\'c\', False), (\'d\', False)))\n\n        # as this is samed sized, NP does element wise comparison\n        self.assertEqual((s1 == (False, True, False, True)).to_pairs(),\n                ((\'a\', True), (\'b\', True), (\'c\', True), (\'d\', True)))\n\n        self.assertEqual((s1 == (False, True, False, True, False)).to_pairs(),\n                ((\'a\', False), (\'b\', False), (\'c\', False), (\'d\', False)))\n\n    def test_series_binary_operator_f(self) -> None:\n        r = Series([\'100312\', \'101376\', \'100828\', \'101214\', \'100185\'])\n        c = Series([\'100312\', \'101376\', \'101092\', \'100828\', \'100185\'],\n                index=[\'100312\', \'101376\', \'101092\', \'100828\', \'100185\'])\n        post = r == c\n\n        self.assertEqual(set(post.to_pairs()),\n                set(((0, False), (1, False), (2, False), (3, False), (4, False), (\'101376\', False), (\'101092\', False), (\'100828\', False), (\'100312\', False), (\'100185\', False)))\n                )\n\n\n    def test_series_binary_operator_g(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(\n                (s1 - 1).to_pairs(),\n                ((\'a\', -1), (\'b\', 0), (\'c\', 1), (\'d\', 2))\n                )\n\n        self.assertEqual((1 - s1).to_pairs(),\n                ((\'a\', 1), (\'b\', 0), (\'c\', -1), (\'d\', -2))\n                )\n\n\n    def test_series_binary_operator_h(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(\n                s1 @ sf.Series([3, 4, 1, 2], index=(\'a\', \'b\', \'c\', \'d\')),\n                12\n                )\n        self.assertEqual(\n                s1 @ sf.Series([3, 4, 1, 2], index=(\'a\', \'c\', \'b\', \'d\')),\n                15\n                )\n\n    def test_series_binary_operator_i(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        post = [3, 4, 1, 2] @ s1 #type: ignore\n        self.assertEqual(post, 12)\n\n\n\n    def test_series_binary_operator_j(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        with self.assertRaises(NotImplementedError):\n            _ = s1 + np.arange(4).reshape((2, 2))\n\n\n    def test_series_binary_operator_k(self) -> None:\n\n        s3 = sf.Series.from_element(\'b\', index=range(3), name=\'foo\')\n        s4 = 3 * s3\n\n        self.assertEqual(s4.name, \'foo\')\n\n        self.assertEqual(s4.to_pairs(),\n                ((0, \'bbb\'), (1, \'bbb\'), (2, \'bbb\')))\n\n        self.assertEqual((s3 * 3).to_pairs(),\n                ((0, \'bbb\'), (1, \'bbb\'), (2, \'bbb\')))\n\n\n        s5 = s3 + \'_\'\n        self.assertEqual(s5.to_pairs(),\n                ((0, \'b_\'), (1, \'b_\'), (2, \'b_\'))\n                )\n\n        self.assertEqual((\'_\' + s3).to_pairs(),\n                ((0, \'_b\'), (1, \'_b\'), (2, \'_b\'))\n                )\n\n    def test_series_binary_operator_l(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        s2 = s1 * np.arange(10, 14)\n        self.assertEqual(s2.to_pairs(),\n                ((\'a\', 0), (\'b\', 11), (\'c\', 24), (\'d\', 39)))\n        self.assertEqual(s2.name, None)\n\n    #---------------------------------------------------------------------------\n    def test_series_rename_a(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        self.assertEqual(s1.name, \'foo\')\n        s2 = s1.rename(None)\n        self.assertEqual(s2.name, None)\n\n    def test_series_rename_b(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        self.assertEqual(s1.name, \'foo\')\n        s2 = Series(s1)\n        self.assertEqual(s2.name, \'foo\')\n\n    #---------------------------------------------------------------------------\n\n\n    def test_series_reindex_a(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        s2 = s1.reindex((\'c\', \'d\', \'a\'))\n        self.assertEqual(list(s2.items()), [(\'c\', 2), (\'d\', 3), (\'a\', 0)])\n\n        s3 = s1.reindex([\'a\',\'b\'])\n        self.assertEqual(list(s3.items()), [(\'a\', 0), (\'b\', 1)])\n\n\n        # an int-valued array is hard to provide missing values for\n\n        s4 = s1.reindex([\'b\', \'q\', \'g\', \'a\'], fill_value=None)\n        self.assertEqual(list(s4.items()),\n                [(\'b\', 1), (\'q\', None), (\'g\', None), (\'a\', 0)])\n\n        # by default this gets float because filltype is nan by default\n        s5 = s1.reindex([\'b\', \'q\', \'g\', \'a\'])\n        self.assertAlmostEqualItems(list(s5.items()),\n                [(\'b\', 1), (\'q\', nan), (\'g\', nan), (\'a\', 0)])\n\n\n    def test_series_reindex_b(self) -> None:\n        s1 = Series(range(4), index=IndexHierarchy.from_product((\'a\', \'b\'), (\'x\', \'y\')))\n        s2 = Series(range(4), index=IndexHierarchy.from_product((\'b\', \'c\'), (\'x\', \'y\')))\n\n        s3 = s1.reindex(s2.index, fill_value=None)\n\n        self.assertEqual(s3.to_pairs(),\n                (((\'b\', \'x\'), 2), ((\'b\', \'y\'), 3), ((\'c\', \'x\'), None), ((\'c\', \'y\'), None)))\n\n        # can reindex with a different dimensionality if no matches\n        self.assertEqual(\n                s1.reindex((3,4,5,6), fill_value=None).to_pairs(),\n                ((3, None), (4, None), (5, None), (6, None)))\n\n        self.assertEqual(\n                s1.reindex(((\'b\', \'x\'),4,5,(\'a\', \'y\')), fill_value=None).to_pairs(),\n                (((\'b\', \'x\'), 2), (4, None), (5, None), ((\'a\', \'y\'), 1)))\n\n\n\n    def test_series_reindex_c(self) -> None:\n        s1 = Series((\'a\', \'b\', \'c\', \'d\'), index=((0, x) for x in range(4)))\n        self.assertEqual(s1.loc[(0, 2)], \'c\')\n\n        s1.reindex(((0, 1), (0, 3), (4,5)))\n\n        self.assertEqual(\n                s1.reindex(((0, 1), (0, 3), (4,5)), fill_value=None).to_pairs(),\n                (((0, 1), \'b\'), ((0, 3), \'d\'), ((4, 5), None)))\n\n\n        s2 = s1.reindex((\'c\', \'d\', \'a\'))\n        self.assertEqual(sorted(s2.index.values.tolist()), [\'a\', \'c\', \'d\'])\n\n\n    def test_series_reindex_d(self) -> None:\n\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        s2 = s1.reindex((\'c\', \'d\', \'a\'))\n        self.assertEqual(s2.index.values.tolist(), [\'c\', \'d\', \'a\'])\n        self.assertEqual(s2.name, \'foo\')\n\n    def test_series_reindex_e(self) -> None:\n\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        idx = Index((\'c\', \'d\', \'a\'))\n        s2 = s1.reindex(idx, own_index=True)\n        self.assertEqual(s2.index.values.tolist(), [\'c\', \'d\', \'a\'])\n        self.assertEqual(s2.name, \'foo\')\n        # we owned the index, so have the same instance\n        self.assertEqual(id(s2.index), id(idx))\n\n    def test_series_reindex_f(self) -> None:\n\n        index = IndexDate.from_date_range(\'2020-03-05\', \'2020-03-10\')\n\n        s1 = Series(range(6), index=index.values) # create an Index\n        s2 = s1.reindex(index) # same values, different class\n        self.assertTrue(s2.index.__class__, index.__class__)\n\n    #---------------------------------------------------------------------------\n    def test_series_isnull_a(self) -> None:\n\n        s1 = Series((234.3, 3.2, 6.4, np.nan), index=(\'a\', \'b\', \'c\', \'d\'))\n        s2 = Series((234.3, None, 6.4, np.nan), index=(\'a\', \'b\', \'c\', \'d\'))\n        s3 = Series((234.3, 5, 6.4, -234.3), index=(\'a\', \'b\', \'c\', \'d\'))\n        s4 = Series((234.3, None, None, None), index=(\'a\', \'b\', \'c\', \'d\'))\n        s5 = Series((\'p\', \'q\', \'e\', \'g\'), index=(\'a\', \'b\', \'c\', \'d\'))\n        s6 = Series((False, True, False, True), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(list(s1.isna().items()),\n                [(\'a\', False), (\'b\', False), (\'c\', False), (\'d\', True)]\n                )\n        self.assertEqual(list(s2.isna().items()),\n                [(\'a\', False), (\'b\', True), (\'c\', False), (\'d\', True)])\n\n        self.assertEqual(list(s3.isna().items()),\n                [(\'a\', False), (\'b\', False), (\'c\', False), (\'d\', False)])\n\n        self.assertEqual(list(s4.isna().items()),\n                [(\'a\', False), (\'b\', True), (\'c\', True), (\'d\', True)])\n\n        # those that are always false\n        self.assertEqual(list(s5.isna().items()),\n                [(\'a\', False), (\'b\', False), (\'c\', False), (\'d\', False)])\n\n        self.assertEqual(list(s6.isna().items()),\n                [(\'a\', False), (\'b\', False), (\'c\', False), (\'d\', False)])\n\n\n\n    def test_series_isnull_b(self) -> None:\n\n        # NOTE: this is a problematic case as it as a string with numerics and None\n        s1 = Series((234.3, \'a\', None, 6.4, np.nan), index=(\'a\', \'b\', \'c\', \'d\', \'e\'))\n\n        self.assertEqual(list(s1.isna().items()),\n                [(\'a\', False), (\'b\', False), (\'c\', True), (\'d\', False), (\'e\', True)]\n                )\n\n    def test_series_notnull(self) -> None:\n\n        s1 = Series((234.3, 3.2, 6.4, np.nan), index=(\'a\', \'b\', \'c\', \'d\'))\n        s2 = Series((234.3, None, 6.4, np.nan), index=(\'a\', \'b\', \'c\', \'d\'))\n        s3 = Series((234.3, 5, 6.4, -234.3), index=(\'a\', \'b\', \'c\', \'d\'))\n        s4 = Series((234.3, None, None, None), index=(\'a\', \'b\', \'c\', \'d\'))\n        s5 = Series((\'p\', \'q\', \'e\', \'g\'), index=(\'a\', \'b\', \'c\', \'d\'))\n        s6 = Series((False, True, False, True), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(list(s1.notna().items()),\n                [(\'a\', True), (\'b\', True), (\'c\', True), (\'d\', False)]\n                )\n        self.assertEqual(list(s2.notna().items()),\n                [(\'a\', True), (\'b\', False), (\'c\', True), (\'d\', False)])\n\n        self.assertEqual(list(s3.notna().items()),\n                [(\'a\', True), (\'b\', True), (\'c\', True), (\'d\', True)])\n\n        self.assertEqual(list(s4.notna().items()),\n                [(\'a\', True), (\'b\', False), (\'c\', False), (\'d\', False)])\n\n        # those that are always false\n        self.assertEqual(list(s5.notna().items()),\n                [(\'a\', True), (\'b\', True), (\'c\', True), (\'d\', True)])\n\n        self.assertEqual(list(s6.notna().items()),\n                [(\'a\', True), (\'b\', True), (\'c\', True), (\'d\', True)])\n\n\n    def test_series_dropna_a(self) -> None:\n\n        s1 = Series((234.3, 3.2, 6.4, np.nan), index=(\'a\', \'b\', \'c\', \'d\'))\n        s2 = Series((234.3, None, 6.4, np.nan), index=(\'a\', \'b\', \'c\', \'d\'))\n        s4 = Series((234.3, None, None, None), index=(\'a\', \'b\', \'c\', \'d\'))\n        s5 = Series((\'p\', \'q\', \'e\', \'g\'), index=(\'a\', \'b\', \'c\', \'d\'))\n        s6 = Series((False, True, False, True), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(s1.dropna().to_pairs(),\n                ((\'a\', 234.3), (\'b\', 3.2), (\'c\', 6.4)))\n        self.assertEqual(list(s2.dropna().items()),\n                [(\'a\', 234.3), (\'c\', 6.4)])\n        self.assertEqual(s4.dropna().to_pairs(),\n                ((\'a\', 234.3),))\n        self.assertEqual(s5.dropna().to_pairs(),\n                ((\'a\', \'p\'), (\'b\', \'q\'), (\'c\', \'e\'), (\'d\', \'g\')))\n        self.assertEqual(s6.dropna().to_pairs(),\n                ((\'a\', False), (\'b\', True), (\'c\', False), (\'d\', True)))\n\n    def test_series_dropna_b(self) -> None:\n        s1 = sf.Series.from_element(np.nan, index=sf.IndexHierarchy.from_product([\'A\', \'B\'], [1, 2]))\n        s2 = s1.dropna()\n        self.assertEqual(len(s2), 0)\n        self.assertEqual(s1.__class__, s2.__class__)\n\n    def test_series_dropna_c(self) -> None:\n        s1 = sf.Series([1, np.nan, 2, np.nan],\n                index=sf.IndexHierarchy.from_product([\'A\', \'B\'], [1, 2]))\n        s2 = s1.dropna()\n        self.assertEqual(s2.to_pairs(), (((\'A\', 1), 1.0), ((\'B\', 1), 2.0)))\n\n    #---------------------------------------------------------------------------\n\n    def test_series_fillna_a(self) -> None:\n\n        s1 = Series((234.3, 3.2, 6.4, np.nan), index=(\'a\', \'b\', \'c\', \'d\'))\n        s2 = Series((234.3, None, 6.4, np.nan), index=(\'a\', \'b\', \'c\', \'d\'))\n        s3 = Series((234.3, 5, 6.4, -234.3), index=(\'a\', \'b\', \'c\', \'d\'))\n        s4 = Series((234.3, None, None, None), index=(\'a\', \'b\', \'c\', \'d\'))\n        s5 = Series((\'p\', \'q\', \'e\', \'g\'), index=(\'a\', \'b\', \'c\', \'d\'))\n        s6 = Series((False, True, False, True), index=(\'a\', \'b\', \'c\', \'d\'))\n        s7 = Series((10, 20, 30, 40), index=(\'a\', \'b\', \'c\', \'d\'))\n        s8 = Series((234.3, None, 6.4, np.nan, \'q\'), index=(\'a\', \'b\', \'c\', \'d\', \'e\'))\n\n\n        self.assertEqual(s1.fillna(0.0).values.tolist(),\n                [234.3, 3.2, 6.4, 0.0])\n\n        self.assertEqual(s1.fillna(-1).values.tolist(),\n                [234.3, 3.2, 6.4, -1.0])\n\n        # given a float array, inserting None, None is casted to nan\n        self.assertEqual(s1.fillna(None).values.tolist(),\n                [234.3, 3.2, 6.4, None])\n\n        post = s1.fillna(\'wer\')\n        self.assertEqual(post.dtype, object)\n        self.assertEqual(post.values.tolist(),\n                [234.3, 3.2, 6.4, \'wer\'])\n\n\n        post = s7.fillna(None)\n        self.assertEqual(post.dtype, int)\n\n\n    def test_series_fillna_b(self) -> None:\n\n        s1 = Series(())\n        s2 = s1.fillna(0)\n        self.assertTrue(len(s2) == 0)\n\n\n    def test_series_fillna_c(self) -> None:\n\n        s1 = Series((np.nan, 3, np.nan))\n        with self.assertRaises(RuntimeError):\n            _ = s1.fillna(np.arange(3))\n\n\n    def test_series_fillna_d(self) -> None:\n\n        s1 = Series((np.nan, 3, np.nan, 4), index=tuple(\'abcd\'))\n        s2 = Series((100, 200), index=tuple(\'ca\'))\n        s3 = s1.fillna(s2)\n        self.assertEqual(s3.dtype, float)\n        self.assertEqual(s3.to_pairs(),\n                ((\'a\', 200.0), (\'b\', 3.0), (\'c\', 100.0), (\'d\', 4.0))\n                )\n\n    def test_series_fillna_e(self) -> None:\n\n        s1 = Series((None, None, \'foo\', \'bar\'), index=tuple(\'abcd\'))\n        s2 = Series((100, 200), index=tuple(\'ca\'))\n        s3 = s1.fillna(s2)\n        self.assertEqual(s3.dtype, object)\n        self.assertEqual(type(s3[\'a\']), int)\n        self.assertEqual(s3.to_pairs(),\n                ((\'a\', 200), (\'b\', None), (\'c\', \'foo\'), (\'d\', \'bar\'))\n                )\n\n\n    def test_series_fillna_f(self) -> None:\n\n        s1 = Series((None, None, \'foo\', \'bar\'), index=tuple(\'abcd\'))\n        s2 = Series((100, 200))\n        s3 = s1.fillna(s2)\n        # no alignment, return the same Series\n        self.assertEqual(id(s3), id(s1))\n\n\n\n    def test_series_fillna_g(self) -> None:\n\n        s1 = Series((np.nan, 3, np.nan, 4), index=tuple(\'abcd\'))\n        s2 = Series((False, True), index=tuple(\'ba\'))\n        s3 = s1.fillna(s2)\n        self.assertEqual(s3.dtype, object)\n        self.assertEqual(s3.fillna(-1).to_pairs(),\n                ((\'a\', True), (\'b\', 3.0), (\'c\', -1), (\'d\', 4.0))\n                )\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_fillna_directional_a(self) -> None:\n\n        a1 = np.array((3, 4))\n        a2 = Series._fillna_directional(\n                array=a1,\n                directional_forward=True,\n                limit=2)\n\n        self.assertEqual(id(a1), id(a2))\n\n\n    def test_series_fillna_sided_a(self) -> None:\n\n        a1 = np.array((np.nan, 3, np.nan))\n\n        with self.assertRaises(RuntimeError):\n            _ = Series._fillna_sided(\n                    array=a1,\n                    value=a1,\n                    sided_leading=True)\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_fillna_leading_a(self) -> None:\n\n        s1 = Series((234.3, 3.2, 6.4, np.nan), index=(\'a\', \'b\', \'c\', \'d\'))\n        s2 = Series((np.nan, None, 6, np.nan), index=(\'a\', \'b\', \'c\', \'d\'))\n        s3 = Series((np.nan, np.nan, np.nan, 4), index=(\'a\', \'b\', \'c\', \'d\'))\n        s4 = Series((None, None, None, None), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(s1.fillna_leading(-1).fillna(0).to_pairs(),\n                ((\'a\', 234.3), (\'b\', 3.2), (\'c\', 6.4), (\'d\', 0.0)))\n\n        self.assertEqual(s2.fillna_leading(0).fillna(-1).to_pairs(),\n                ((\'a\', 0), (\'b\', 0), (\'c\', 6), (\'d\', -1)))\n\n        self.assertEqual(s3.fillna_leading(\'a\').to_pairs(),\n                ((\'a\', \'a\'), (\'b\', \'a\'), (\'c\', \'a\'), (\'d\', 4.0)))\n\n        self.assertEqual(s4.fillna_leading(\'b\').to_pairs(),\n                ((\'a\', \'b\'), (\'b\', \'b\'), (\'c\', \'b\'), (\'d\', \'b\')))\n\n\n    def test_series_fillna_leading_b(self) -> None:\n\n        s1 = Series((3.2, 6.4), index=(\'a\', \'b\',))\n        s2 = s1.fillna_leading(0)\n        self.assertTrue(s1.to_pairs() == s2.to_pairs())\n\n\n    def test_series_fillna_trailing_a(self) -> None:\n\n        s1 = Series((234.3, 3.2, np.nan, np.nan), index=(\'a\', \'b\', \'c\', \'d\'))\n        s2 = Series((np.nan, None, 6.4, np.nan), index=(\'a\', \'b\', \'c\', \'d\'))\n        s3 = Series((np.nan, 2.3, 6.4, 4), index=(\'a\', \'b\', \'c\', \'d\'))\n        s4 = Series((None, None, None, None), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(s1.fillna_trailing(0).to_pairs(),\n                ((\'a\', 234.3), (\'b\', 3.2), (\'c\', 0.0), (\'d\', 0.0)))\n\n        self.assertEqual(s2.fillna_trailing(0).fillna(-1).to_pairs(),\n                ((\'a\', -1), (\'b\', -1), (\'c\', 6.4), (\'d\', 0)))\n\n        self.assertEqual(s3.fillna_trailing(2).fillna(-1).to_pairs(),\n                ((\'a\', -1.0), (\'b\', 2.3), (\'c\', 6.4), (\'d\', 4.0)))\n\n        self.assertEqual(s4.fillna_trailing(\'c\').to_pairs(),\n                ((\'a\', \'c\'), (\'b\', \'c\'), (\'c\', \'c\'), (\'d\', \'c\')))\n\n\n\n    def test_series_fillna_forward_a(self) -> None:\n\n        index = tuple(string.ascii_lowercase[:8])\n\n        # target_index [0 3 6]\n        s1 = Series((3, None, None, 4, None, None, 5, 6), index=index)\n        self.assertEqual(s1.fillna_forward().to_pairs(),\n                ((\'a\', 3), (\'b\', 3), (\'c\', 3), (\'d\', 4), (\'e\', 4), (\'f\', 4), (\'g\', 5), (\'h\', 6)))\n\n        # target_index [3]\n        s2 = Series((None, None, None, 4, None, None, None, None), index=index)\n        self.assertEqual(s2.fillna_forward().to_pairs(),\n                ((\'a\', None), (\'b\', None), (\'c\', None), (\'d\', 4), (\'e\', 4), (\'f\', 4), (\'g\', 4), (\'h\', 4)))\n\n        # target_index [0 6]\n        s3 = Series((1, None, None, None, None, None, 4, None), index=index)\n        self.assertEqual(s3.fillna_forward().to_pairs(),\n                ((\'a\', 1), (\'b\', 1), (\'c\', 1), (\'d\', 1), (\'e\', 1), (\'f\', 1), (\'g\', 4), (\'h\', 4)))\n\n        # target_index [0 7]\n        s4 = Series((1, None, None, None, None, None, None, 4), index=index)\n        self.assertEqual(s4.fillna_forward().to_pairs(),\n                ((\'a\', 1), (\'b\', 1), (\'c\', 1), (\'d\', 1), (\'e\', 1), (\'f\', 1), (\'g\', 1), (\'h\', 4)))\n\n        # target_index [7]\n        s5 = Series((None, None, None, None, None, None, None, 4), index=index)\n        self.assertEqual(s5.fillna_forward().to_pairs(),\n                ((\'a\', None), (\'b\', None), (\'c\', None), (\'d\', None), (\'e\', None), (\'f\', None), (\'g\', None), (\'h\', 4)))\n\n        # target index = array([0, 3, 6])\n        s6 = Series((2, None, None, 3, 4, 5, 6, None), index=index)\n        self.assertEqual(s6.fillna_forward().to_pairs(),\n                ((\'a\', 2), (\'b\', 2), (\'c\', 2), (\'d\', 3), (\'e\', 4), (\'f\', 5), (\'g\', 6), (\'h\', 6))\n                )\n        # target_index [6]\n        s7 = Series((2, 1, 0, 3, 4, 5, 6, None), index=index)\n        self.assertEqual(s7.fillna_forward().to_pairs(),\n                ((\'a\', 2), (\'b\', 1), (\'c\', 0), (\'d\', 3), (\'e\', 4), (\'f\', 5), (\'g\', 6), (\'h\', 6)))\n\n        s8 = Series((2, None, None, None, 4, None, 6, None), index=index)\n        self.assertEqual(s8.fillna_forward().to_pairs(),\n                ((\'a\', 2), (\'b\', 2), (\'c\', 2), (\'d\', 2), (\'e\', 4), (\'f\', 4), (\'g\', 6), (\'h\', 6)))\n\n        s9 = Series((None, 2, 3, None, 4, None, 6, 7), index=index)\n        self.assertEqual(s9.fillna_forward().to_pairs(),\n                ((\'a\', None), (\'b\', 2), (\'c\', 3), (\'d\', 3), (\'e\', 4), (\'f\', 4), (\'g\', 6), (\'h\', 7)))\n\n\n    def test_series_fillna_forward_b(self) -> None:\n\n        index = tuple(string.ascii_lowercase[:8])\n\n        # target_index [0 3 6]\n        s1 = Series((3, None, None, None, 4, None, None, None), index=index)\n        s2 = s1.fillna_forward(limit=2)\n\n        self.assertEqual(s2.to_pairs(),\n                ((\'a\', 3), (\'b\', 3), (\'c\', 3), (\'d\', None), (\'e\', 4), (\'f\', 4), (\'g\', 4), (\'h\', None))\n                )\n\n        self.assertEqual(s1.fillna_forward(limit=1).to_pairs(),\n                ((\'a\', 3), (\'b\', 3), (\'c\', None), (\'d\', None), (\'e\', 4), (\'f\', 4), (\'g\', None), (\'h\', None)))\n\n        self.assertEqual(s1.fillna_forward(limit=10).to_pairs(),\n                ((\'a\', 3), (\'b\', 3), (\'c\', 3), (\'d\', 3), (\'e\', 4), (\'f\', 4), (\'g\', 4), (\'h\', 4)))\n\n    def test_series_fillna_forward_c(self) -> None:\n\n        # this case shown to justify the slice_condition oassed to slices_from_targets\n        index = tuple(string.ascii_lowercase[:8])\n        s1 = Series((3, 2, None, 4, None, None, 5, 6), index=index)\n\n        self.assertEqual(s1.fillna_forward().to_pairs(),\n                ((\'a\', 3), (\'b\', 2), (\'c\', 2), (\'d\', 4), (\'e\', 4), (\'f\', 4), (\'g\', 5), (\'h\', 6)))\n\n        self.assertEqual(s1.fillna_backward().to_pairs(),\n                ((\'a\', 3), (\'b\', 2), (\'c\', 4), (\'d\', 4), (\'e\', 5), (\'f\', 5), (\'g\', 5), (\'h\', 6)))\n\n\n    def test_series_fillna_backward_a(self) -> None:\n\n        index = tuple(string.ascii_lowercase[:8])\n\n        # target_index [0 3 6]\n        s1 = Series((3, None, None, 4, None, None, 5, 6), index=index)\n        self.assertEqual(s1.fillna_backward().to_pairs(),\n                ((\'a\', 3), (\'b\', 4), (\'c\', 4), (\'d\', 4), (\'e\', 5), (\'f\', 5), (\'g\', 5), (\'h\', 6)))\n\n        s2 = Series((None, None, None, 4, None, None, None, None), index=index)\n        self.assertEqual(s2.fillna_backward().to_pairs(),\n                ((\'a\', 4), (\'b\', 4), (\'c\', 4), (\'d\', 4), (\'e\', None), (\'f\', None), (\'g\', None), (\'h\', None)))\n\n        s3 = Series((1, None, None, None, None, None, 4, None), index=index)\n        self.assertEqual(s3.fillna_backward().to_pairs(),\n                ((\'a\', 1), (\'b\', 4), (\'c\', 4), (\'d\', 4), (\'e\', 4), (\'f\', 4), (\'g\', 4), (\'h\', None)))\n\n        s4 = Series((1, None, None, None, None, None, None, 4), index=index)\n        self.assertEqual(s4.fillna_backward().to_pairs(),\n                ((\'a\', 1), (\'b\', 4), (\'c\', 4), (\'d\', 4), (\'e\', 4), (\'f\', 4), (\'g\', 4), (\'h\', 4)))\n\n        s5 = Series((None, None, None, None, None, None, None, 4), index=index)\n        self.assertEqual(s5.fillna_backward().to_pairs(),\n                ((\'a\', 4), (\'b\', 4), (\'c\', 4), (\'d\', 4), (\'e\', 4), (\'f\', 4), (\'g\', 4), (\'h\', 4)))\n\n        s6 = Series((2, None, None, 3, 4, 5, 6, None), index=index)\n        self.assertEqual(s6.fillna_backward().to_pairs(),\n                ((\'a\', 2), (\'b\', 3), (\'c\', 3), (\'d\', 3), (\'e\', 4), (\'f\', 5), (\'g\', 6), (\'h\', None)))\n\n        s7 = Series((None, 1, 0, 3, 4, 5, 6, 7), index=index)\n        self.assertEqual(s7.fillna_backward().to_pairs(),\n            ((\'a\', 1), (\'b\', 1), (\'c\', 0), (\'d\', 3), (\'e\', 4), (\'f\', 5), (\'g\', 6), (\'h\', 7)))\n\n        s8 = Series((2, None, None, None, 4, None, 6, None), index=index)\n        self.assertEqual(s8.fillna_backward().to_pairs(),\n            ((\'a\', 2), (\'b\', 4), (\'c\', 4), (\'d\', 4), (\'e\', 4), (\'f\', 6), (\'g\', 6), (\'h\', None)))\n\n        s9 = Series((None, 2, 3, None, 4, None, 6, 7), index=index)\n        self.assertEqual(s9.fillna_backward().to_pairs(),\n                ((\'a\', 2), (\'b\', 2), (\'c\', 3), (\'d\', 4), (\'e\', 4), (\'f\', 6), (\'g\', 6), (\'h\', 7)))\n\n\n    def test_series_fillna_backward_b(self) -> None:\n\n        index = tuple(string.ascii_lowercase[:8])\n\n        # target_index [0 3 6]\n        s1 = Series((3, None, None, 4, None, None, 5, 6), index=index)\n        self.assertEqual(s1.fillna_backward(1).to_pairs(),\n                ((\'a\', 3), (\'b\', None), (\'c\', 4), (\'d\', 4), (\'e\', None), (\'f\', 5), (\'g\', 5), (\'h\', 6)))\n\n        s2 = Series((3, None, None, None, 4, None, None, None), index=index)\n        self.assertEqual(s2.fillna_backward(2).to_pairs(),\n                ((\'a\', 3), (\'b\', None), (\'c\', 4), (\'d\', 4), (\'e\', 4), (\'f\', None), (\'g\', None), (\'h\', None)))\n\n        s3 = Series((None, 1, None, None, None, None, None, 5), index=index)\n        self.assertEqual(s3.fillna_backward(4).to_pairs(),\n                ((\'a\', 1), (\'b\', 1), (\'c\', None), (\'d\', 5), (\'e\', 5), (\'f\', 5), (\'g\', 5), (\'h\', 5)))\n\n\n    #---------------------------------------------------------------------------\n    def test_series_from_element_a(self) -> None:\n        s1 = Series.from_element(\'a\', index=range(3))\n        self.assertEqual(s1.to_pairs(),\n                ((0, \'a\'), (1, \'a\'), (2, \'a\'))\n                )\n\n\n    def test_series_from_element_b(self) -> None:\n        s1 = Series.from_element(\'a\', index=Index((3, 4, 5)), own_index=True)\n        self.assertEqual(s1.to_pairs(),\n                ((3, \'a\'), (4, \'a\'), (5, \'a\'))\n                )\n\n\n    #---------------------------------------------------------------------------\n    def test_series_from_items_a(self) -> None:\n\n        def gen() -> tp.Iterator[tp.Tuple[int, int]]:\n            r1 = range(10)\n            r2 = iter(range(10, 20))\n            for x in r1:\n                yield x, next(r2)\n\n        s1 = Series.from_items(gen())\n        self.assertEqual(s1.loc[7:9].values.tolist(), [17, 18, 19])\n\n        s2 = Series.from_items(dict(a=30, b=40, c=50).items())\n        self.assertEqual(s2[\'c\'], 50)\n        self.assertEqual(s2[\'b\'], 40)\n        self.assertEqual(s2[\'a\'], 30)\n\n\n    def test_series_from_items_b(self) -> None:\n\n        s1 = Series.from_items(zip(list(\'abc\'), (1,2,3)), dtype=str, name=\'foo\')\n        self.assertEqual(s1.name, \'foo\')\n        self.assertEqual(s1.values.tolist(), [\'1\', \'2\', \'3\'])\n\n    def test_series_from_items_c(self) -> None:\n\n        s1 = Series.from_items(zip(\n                ((1, \'a\'), (1, \'b\'), (2, \'a\'), (2, \'b\')), range(4)),\n                index_constructor=IndexHierarchy.from_labels)\n        self.assertEqual(s1[HLoc[:, \'b\']].to_pairs(),\n                (((1, \'b\'), 1), ((2, \'b\'), 3))\n                )\n\n    def test_series_from_items_d(self) -> None:\n\n        with self.assertRaises(RuntimeError):\n            s1 = Series.from_items(zip(\n                    ((1, \'a\'), (1, \'b\'), (2, \'a\'), (2, \'b\')), range(4)),\n                    index_constructor=IndexHierarchyGO.from_labels)\n\n    def test_series_from_items_e(self) -> None:\n        s1 = Series.from_items(zip((\'2017-11\', \'2017-12\', \'2018-01\', \'2018-02\'),\n                range(4)),\n                index_constructor=IndexYearMonth)\n\n        self.assertEqual(s1[\'2017\'].to_pairs(),\n                ((np.datetime64(\'2017-11\'), 0),\n                (np.datetime64(\'2017-12\'), 1))\n                )\n\n        self.assertEqual(s1[\'2018\'].to_pairs(),\n                ((np.datetime64(\'2018-01\'), 2),\n                (np.datetime64(\'2018-02\'), 3))\n                )\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_contains_a(self) -> None:\n\n        s1 = Series.from_items(zip((\'a\', \'b\', \'c\'), (10, 20, 30)))\n        self.assertTrue(\'b\' in s1)\n        self.assertTrue(\'c\' in s1)\n        self.assertTrue(\'a\' in s1)\n\n        self.assertFalse(\'d\' in s1)\n        self.assertFalse(\'\' in s1)\n\n    #---------------------------------------------------------------------------\n\n\n    def test_series_sum_a(self) -> None:\n\n        s1 = Series.from_items(zip((\'a\', \'b\', \'c\'), (10, 20, 30)))\n        self.assertEqual(s1.sum(), 60)\n\n        s1 = Series.from_items(zip((\'a\', \'b\', \'c\', \'d\'), (10, 20, 30, np.nan)))\n        self.assertEqual(s1.sum(), 60)\n\n        s1 = Series.from_items(zip((\'a\', \'b\', \'c\', \'d\'), (10, 20, 30, None)))\n        self.assertEqual(s1.sum(), 60)\n\n\n    def test_series_sum_b(self) -> None:\n        s1 = Series(list(\'abc\'), dtype=object)\n        self.assertEqual(s1.sum(), \'abc\')\n        # get the same result from character arrays\n        s2 = sf.Series(list(\'abc\'))\n        self.assertEqual(s2.sum(), \'abc\')\n\n\n    def test_series_cumsum_a(self) -> None:\n\n        s1 = Series.from_items(zip(\'abc\', (10, 20, 30)))\n\n        self.assertEqual(s1.cumsum().to_pairs(),\n                ((\'a\', 10), (\'b\', 30), (\'c\', 60))\n                )\n\n        s2 = Series.from_items(zip(\'abc\', (10, np.nan, 30))).cumsum(skipna=False).fillna(None)\n        self.assertEqual(s2.to_pairs(),\n                ((\'a\', 10.0), (\'b\', None), (\'c\', None))\n                )\n\n\n    def test_series_cumprod_a(self) -> None:\n\n        s1 = Series.from_items(zip(\'abc\', (10, 20, 30)))\n        self.assertEqual(\n                s1.cumprod().to_pairs(),\n                ((\'a\', 10), (\'b\', 200), (\'c\', 6000))\n                )\n\n\n    def test_series_median_a(self) -> None:\n\n        s1 = Series.from_items(zip(\'abcde\', (10, 20, 0, 15, 30)))\n        self.assertEqual(s1.median(), 15)\n        self.assertEqual(s1.median(skipna=False), 15)\n\n        s2 = Series.from_items(zip(\'abcde\', (10, 20, np.nan, 15, 30)))\n        self.assertEqual(s2.median(), 17.5)\n        self.assertTrue(np.isnan(s2.median(skipna=False)))\n\n        with self.assertRaises(TypeError):\n            # should raise with bad keyword argumenty\n            s2.median(skip_na=False) #type: ignore\n\n    #---------------------------------------------------------------------------\n\n    def test_series_mask_a(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(\n                s1.mask.loc[[\'b\', \'d\']].values.tolist(),\n                [False, True, False, True])\n        self.assertEqual(s1.mask.iloc[1:].values.tolist(),\n                [False, True, True, True])\n\n        self.assertEqual(s1.masked_array.loc[[\'b\', \'d\']].sum(), 2)\n        self.assertEqual(s1.masked_array.loc[[\'a\', \'b\']].sum(), 5)\n\n\n\n    def test_series_assign_a(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n\n\n        self.assertEqual(\n                s1.assign.loc[[\'b\', \'d\']](3000).values.tolist(),\n                [0, 3000, 2, 3000])\n\n        self.assertEqual(\n                s1.assign[\'b\':](300).values.tolist(),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                [0, 300, 300, 300])\n\n\n    def test_series_assign_b(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(list(s1.isin([2]).items()),\n                [(\'a\', False), (\'b\', False), (\'c\', True), (\'d\', False)])\n\n        self.assertEqual(list(s1.isin({2, 3}).items()),\n                [(\'a\', False), (\'b\', False), (\'c\', True), (\'d\', True)])\n\n        self.assertEqual(list(s1.isin(range(2, 4)).items()),\n                [(\'a\', False), (\'b\', False), (\'c\', True), (\'d\', True)])\n\n\n    def test_series_assign_c(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        self.assertEqual(s1.assign.loc[\'c\':](0).to_pairs(),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                ((\'a\', 0), (\'b\', 1), (\'c\', 0), (\'d\', 0))\n                )\n        self.assertEqual(s1.assign.loc[\'c\':]((20, 30)).to_pairs(),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                ((\'a\', 0), (\'b\', 1), (\'c\', 20), (\'d\', 30)))\n\n        self.assertEqual(s1.assign[\'c\':](s1[\'c\':] * 10).to_pairs(),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                ((\'a\', 0), (\'b\', 1), (\'c\', 20), (\'d\', 30)))\n\n        self.assertEqual(s1.assign[\'c\':](Series.from_dict({\'d\':40, \'c\':60})).to_pairs(),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                ((\'a\', 0), (\'b\', 1), (\'c\', 60), (\'d\', 40)))\n\n\n    def test_series_assign_d(self) -> None:\n        s1 = Series(tuple(\'pqrs\'), index=(\'a\', \'b\', \'c\', \'d\'))\n        s2 = s1.assign[\'b\'](None)\n        self.assertEqual(s2.to_pairs(),\n                ((\'a\', \'p\'), (\'b\', None), (\'c\', \'r\'), (\'d\', \'s\')))\n        self.assertEqual(s1.assign[\'b\':](None).to_pairs(),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                ((\'a\', \'p\'), (\'b\', None), (\'c\', None), (\'d\', None)))\n\n\n    def test_series_assign_e(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        s2 = Series(range(2), index=(\'c\', \'d\'))\n        self.assertEqual(\n                s1.assign[s2.index](s2).to_pairs(),\n                ((\'a\', 0), (\'b\', 1), (\'c\', 0), (\'d\', 1))\n                )\n    def test_series_assign_f(self) -> None:\n        s1 = Series(range(5), index=(\'a\', \'b\', \'c\', \'d\', \'e\'))\n\n        with self.assertRaises(Exception):\n            # cannot have an assignment target that is not in the Series\n            s1.assign[[\'f\', \'d\']](10)\n\n        self.assertEqual(\n                s1.assign[[\'d\', \'c\']](Series((10, 20), index=(\'d\', \'c\'))).to_pairs(),\n                ((\'a\', 0), (\'b\', 1), (\'c\', 20), (\'d\', 10), (\'e\', 4)))\n\n        self.assertEqual(\n                s1.assign[[\'c\', \'d\']](Series((10, 20), index=(\'d\', \'c\'))).to_pairs(),\n                ((\'a\', 0), (\'b\', 1), (\'c\', 20), (\'d\', 10), (\'e\', 4)))\n\n        self.assertEqual(\n                s1.assign[[\'c\', \'d\']](Series((10, 20, 30), index=(\'d\', \'c\', \'f\'))).to_pairs(),\n                ((\'a\', 0), (\'b\', 1), (\'c\', 20), (\'d\', 10), (\'e\', 4)))\n\n\n        self.assertEqual(\n                s1.assign[[\'c\', \'d\', \'b\']](Series((10, 20), index=(\'d\', \'c\')), fill_value=-1).to_pairs(),\n                ((\'a\', 0), (\'b\', -1), (\'c\', 20), (\'d\', 10), (\'e\', 4))\n                )\n\n    def test_series_assign_g(self) -> None:\n        s1 = Series(range(5), index=(\'a\', \'b\', \'c\', \'d\', \'e\'), name=\'x\')\n\n        s2 = Series(list(\'abc\'), index=list(\'abc\'), name=\'y\')\n\n        post = s1.assign[s2.index](s2)\n        self.assertEqual(post.name, \'x\')\n        self.assertEqual(post.values.tolist(), [\'a\', \'b\', \'c\', 3, 4])\n\n\n    def test_series_iloc_extract_a(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(s1.iloc[0], 0)\n\n        self.assertEqual(s1.iloc[2:].to_pairs(), ((\'c\', 2), (\'d\', 3)))\n\n    #---------------------------------------------------------------------------\n\n\n    def test_series_loc_extract_a(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        with self.assertRaises(KeyError):\n            s1.loc[[\'c\', \'d\', \'e\']] #pylint: disable=W0104\n\n    def test_series_loc_extract_b(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'), name=\'foo\')\n        s2 = s1.loc[[\'b\', \'d\']]\n\n        self.assertEqual(s2.to_pairs(), ((\'b\', 1), (\'d\', 3)))\n        self.assertEqual(s2.name, \'foo\')\n\n    def test_series_loc_extract_c(self) -> None:\n        s = sf.Series(range(5),\n                index=sf.IndexHierarchy.from_labels(\n                ((\'a\', \'a\'), (\'a\', \'b\'), (\'b\', \'a\'), (\'b\', \'b\'), (\'b\', \'c\'))))\n\n        # this selection returns just a single value\n        # import ipdb; ipdb.set_trace()\n        s2 = s.loc[sf.HLoc[:, \'c\']]\n        self.assertEqual(s2.__class__, s.__class__)\n        self.assertEqual(s2.to_pairs(), (((\'b\', \'c\'), 4),))\n\n        # this selection yields a series\n        self.assertEqual(s.loc[sf.HLoc[:, \'a\']].to_pairs(),\n                (((\'a\', \'a\'), 0), ((\'b\', \'a\'), 2)))\n\n\n    def test_series_loc_extract_d(self) -> None:\n        s = sf.Series(range(5),\n                index=sf.IndexHierarchy.from_labels(\n                ((\'a\', \'a\'), (\'a\', \'b\'), (\'b\', \'a\'), (\'b\', \'b\'), (\'b\', \'c\'))))\n        # leaf loc selection must be terminal; using a slice or list is an exception\n        with self.assertRaises(RuntimeError):\n            s.loc[\'a\', :] #pylint: disable=W0104\n\n        with self.assertRaises(RuntimeError):\n            s.loc[[\'a\', \'b\'], \'b\'] #pylint: disable=W0104\n\n\n    def test_series_loc_extract_e(self) -> None:\n        s1 = sf.Series(range(4), index=sf.IndexHierarchy.from_product([\'A\', \'B\'], [1, 2]))\n\n        self.assertEqual(s1.loc[(\'B\', 1)], 2)\n        self.assertEqual(s1.loc[sf.HLoc[\'B\', 1]], 2)\n        self.assertEqual(s1.iloc[2], 2)\n\n\n    def test_series_loc_extract_f(self) -> None:\n        s1 = sf.Series(range(4), index=sf.IndexHierarchy.from_product([\'A\', \'B\'], [1, 2]))\n\n        post1 = s1[HLoc[\'A\', [2]]]\n        self.assertEqual(post1.to_pairs(), (((\'A\', 2), 1),))\n\n        post2 = s1[HLoc[\'A\', 2]]\n        self.assertEqual(post2, 1)\n\n\n    def test_series_loc_extract_g(self) -> None:\n\n        s1 = Series((\'a\', \'b\', \'c\', \'d\'))\n        post = s1.loc[0:2].to_pairs()\n        self.assertEqual(post,\n                ((0, \'a\'), (1, \'b\'), (2, \'c\'))\n                )\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_group_a(self) -> None:\n\n        s1 = Series((0, 1, 0, 1), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        groups = tuple(s1.iter_group_items())\n\n        self.assertEqual([g[0] for g in groups], [0, 1])\n\n        self.assertEqual([g[1].to_pairs() for g in groups],\n                [((\'a\', 0), (\'c\', 0)), ((\'b\', 1), (\'d\', 1))])\n\n    def test_series_group_b(self) -> None:\n\n        s1 = Series((\'foo\', \'bar\', \'foo\', 20, 20),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                dtype=object)\n\n        groups = tuple(s1.iter_group_items())\n\n\n        self.assertEqual([g[0] for g in groups],\n                [20, \'bar\', \'foo\'])\n        self.assertEqual([g[1].to_pairs() for g in groups],\n                [((\'d\', 20), (\'e\', 20)), ((\'b\', \'bar\'),), ((\'a\', \'foo\'), (\'c\', \'foo\'))])\n\n\n    def test_series_group_c(self) -> None:\n\n        s1 = Series((10, 10, 10, 20, 20),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                dtype=object)\n\n        groups = tuple(s1.iter_group())\n        self.assertEqual([g.sum() for g in groups], [30, 40])\n\n        self.assertEqual(\n                s1.iter_group().apply(np.sum).to_pairs(),\n                ((10, 30), (20, 40)))\n\n        self.assertEqual(\n                s1.iter_group_items().apply(lambda g, s: (g * s).values.tolist()).to_pairs(),\n                ((10, [100, 100, 100]), (20, [400, 400])))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_iter_element_a(self) -> None:\n\n        s1 = Series((10, 3, 15, 21, 28),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                dtype=object)\n\n        self.assertEqual([x for x in s1.iter_element()], [10, 3, 15, 21, 28])\n\n        self.assertEqual([x for x in s1.iter_element_items()],\n                        [(\'a\', 10), (\'b\', 3), (\'c\', 15), (\'d\', 21), (\'e\', 28)])\n\n        self.assertEqual(s1.iter_element().apply(lambda x: x * 20).to_pairs(),\n                ((\'a\', 200), (\'b\', 60), (\'c\', 300), (\'d\', 420), (\'e\', 560)))\n\n        self.assertEqual(\n                s1.iter_element_items().apply(lambda k, v: v * 20 if k == \'b\' else 0).to_pairs(),\n                ((\'a\', 0), (\'b\', 60), (\'c\', 0), (\'d\', 0), (\'e\', 0)))\n\n\n    def test_series_iter_element_b(self) -> None:\n\n        s1 = Series((10, 3, 15, 21, 28, 50),\n                index=IndexHierarchy.from_product(tuple(\'ab\'), tuple(\'xyz\')),\n                dtype=object)\n        s2 = s1.iter_element().apply(str, name=\'foo\')\n        self.assertEqual(s2.index.__class__, IndexHierarchy)\n        self.assertEqual(s2.name, \'foo\')\n\n        self.assertEqual(s2.to_pairs(),\n                (((\'a\', \'x\'), \'10\'), ((\'a\', \'y\'), \'3\'), ((\'a\', \'z\'), \'15\'), ((\'b\', \'x\'), \'21\'), ((\'b\', \'y\'), \'28\'), ((\'b\', \'z\'), \'50\')))\n\n\n    def test_series_iter_element_c(self) -> None:\n\n        s1 = Series((10, 3, 15, 21, 28),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                )\n        with self.assertRaises(RuntimeError):\n            s1.iter_element().apply({15:150})\n\n        post1 = tuple(s1.iter_element_items().map_any_iter_items({(\'c\', 15): 150}))\n        self.assertEqual(post1,\n                ((\'a\', 10), (\'b\', 3), (\'c\', 150), (\'d\', 21), (\'e\', 28)))\n\n        post2 = tuple(s1.iter_element_items().map_fill_iter_items(\n                {(\'c\', 15): 150}, fill_value=0))\n        self.assertEqual(post2,\n                ((\'a\', 0), (\'b\', 0), (\'c\', 150), (\'d\', 0), (\'e\', 0)))\n\n        post3 = tuple(s1.iter_element_items().map_all_iter_items(\n                {(k, v): v * 10 for k, v in s1.items()}))\n        self.assertEqual(post3,\n                ((\'a\', 100), (\'b\', 30), (\'c\', 150), (\'d\', 210), (\'e\', 280)))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_iter_element_map_any_a(self) -> None:\n\n        s1 = Series((10, 3, 15, 21, 28),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                dtype=object)\n\n        post = s1.iter_element().map_any({3: 100, 21: 101})\n\n        self.assertEqual(post.to_pairs(),\n                ((\'a\', 10), (\'b\', 100), (\'c\', 15), (\'d\', 101), (\'e\', 28))\n                )\n\n\n    def test_series_iter_element_map_any_b(self) -> None:\n\n        s1 = Series((10, 3, 15, 21, 28),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                dtype=object)\n\n        s2 = Series((100, 101), index=(3, 21))\n\n        post = s1.iter_element().map_any(s2)\n\n        self.assertEqual(post.to_pairs(),\n                ((\'a\', 10), (\'b\', 100), (\'c\', 15), (\'d\', 101), (\'e\', 28))\n                )\n\n\n    def test_series_iter_element_map_any_c(self) -> None:\n\n        s1 = Series((10, 3, 15, 21, 28),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                dtype=object)\n\n        s2 = Series((100, 101), index=(3, 21))\n\n        self.assertEqual(tuple(s2.iter_element().map_any_iter(s2)),\n            (100, 101))\n        self.assertEqual(tuple(s2.iter_element().map_any_iter_items(s2)),\n            ((3, 100), (21, 101)))\n\n\n    def test_series_iter_element_map_all_a(self) -> None:\n\n        s1 = Series((10, 3, 15, 21, 28),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                dtype=object)\n\n        with self.assertRaises(KeyError):\n            post = s1.iter_element().map_all({3: 100, 21: 101})\n\n        post = s1.iter_element().map_all({v: k for k, v in s1.items()})\n\n        self.assertEqual(post.to_pairs(),\n                ((\'a\', \'a\'), (\'b\', \'b\'), (\'c\', \'c\'), (\'d\', \'d\'), (\'e\', \'e\'))\n                )\n\n    def test_series_iter_element_map_all_b(self) -> None:\n\n        s1 = Series((10, 3, 15, 21, 28),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                dtype=object)\n\n        s2 = Series((100, 101), index=(3, 21))\n\n        with self.assertRaises(KeyError):\n            post = s1.iter_element().map_all(s2)\n\n        s3 = Series.from_items((v, i) for i, v in enumerate(s1.values))\n\n        self.assertEqual(s3.to_pairs(),\n                ((10, 0), (3, 1), (15, 2), (21, 3), (28, 4))\n                )\n\n    def test_series_iter_element_map_all_c(self) -> None:\n\n        s1 = Series((10, 3, 15, 21, 28),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                dtype=object)\n\n        s2 = Series.from_items((v, i) for i, v in enumerate(s1.values))\n\n        self.assertEqual(tuple(s1.iter_element().map_all_iter(s2)),\n                (0, 1, 2, 3, 4))\n\n        self.assertEqual(tuple(s1.iter_element().map_all_iter_items(s2)),\n                ((\'a\', 0), (\'b\', 1), (\'c\', 2), (\'d\', 3), (\'e\', 4))\n                )\n\n    def test_series_iter_element_map_fill_a(self) -> None:\n\n        s1 = Series((10, 3, 15, 21, 28),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                dtype=object)\n\n        post = s1.iter_element().map_fill({21: 100, 28: 101}, fill_value=-1)\n        self.assertEqual(post.to_pairs(),\n                ((\'a\', -1), (\'b\', -1), (\'c\', -1), (\'d\', 100), (\'e\', 101))\n                )\n\n    def test_series_iter_element_map_fill_b(self) -> None:\n\n        s1 = Series((10, 3, 15, 21, 28),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                dtype=object)\n\n        s2 = Series((100, 101), index=(21, 28))\n\n        post = s1.iter_element().map_fill(s2, fill_value=-1)\n        self.assertEqual(post.to_pairs(),\n                ((\'a\', -1), (\'b\', -1), (\'c\', -1), (\'d\', 100), (\'e\', 101))\n                )\n\n\n    def test_series_iter_element_map_fill_c(self) -> None:\n\n        s1 = Series((10, 3, 15, 21, 28),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                dtype=object)\n\n        s2 = Series((100, 101), index=(21, 28))\n\n        self.assertEqual(tuple(s1.iter_element().map_fill_iter(s2, fill_value=0)),\n                (0, 0, 0, 100, 101))\n\n        self.assertEqual(tuple(s1.iter_element().map_fill_iter_items(s2, fill_value=0)),\n                ((\'a\', 0), (\'b\', 0), (\'c\', 0), (\'d\', 100), (\'e\', 101))\n                )\n\n\n\n    #---------------------------------------------------------------------------\n    def test_series_sort_index_a(self) -> None:\n\n        s1 = Series((10, 3, 28, 21, 15),\n                index=(\'a\', \'c\', \'b\', \'e\', \'d\'),\n                dtype=object,\n                name=\'foo\')\n\n        s2 = s1.sort_index()\n        self.assertEqual(s2.to_pairs(),\n                ((\'a\', 10), (\'b\', 28), (\'c\', 3), (\'d\', 15), (\'e\', 21)))\n        self.assertEqual(s2.name, s1.name)\n\n        s3 = s1.sort_values()\n        self.assertEqual(s3.to_pairs(),\n                ((\'c\', 3), (\'a\', 10), (\'d\', 15), (\'e\', 21), (\'b\', 28)))\n        self.assertEqual(s3.name, s1.name)\n\n\n    def test_series_sort_index_b(self) -> None:\n\n        index = IndexYearMonth.from_date_range(\'2017-12-15\', \'2018-03-15\')\n        s = Series(list(\'abcd\'), index=index)\n\n        post = s.sort_index(ascending=False)\n\n        self.assertEqual(\n                post.to_pairs(),\n                ((np.datetime64(\'2018-03\'), \'d\'), (np.datetime64(\'2018-02\'), \'c\'), (np.datetime64(\'2018-01\'), \'b\'), (np.datetime64(\'2017-12\'), \'a\'))\n                )\n\n        self.assertEqual(post.index.__class__, IndexYearMonth)\n\n\n    def test_series_sort_index_c(self) -> None:\n\n        index = IndexHierarchy.from_product((0, 1), (10, 20))\n        s = Series(list(\'abcd\'), index=index)\n\n        post = s.sort_index(ascending=False)\n\n        self.assertEqual(post.to_pairs(),\n            (((1, 20), \'d\'), ((1, 10), \'c\'), ((0, 20), \'b\'), ((0, 10), \'a\'))\n            )\n        self.assertEqual(post.index.__class__, IndexHierarchy)\n\n\n    def test_series_sort_index_d(self) -> None:\n\n        index = IndexHierarchy.from_product((0, 1), (10, 20), name=\'foo\')\n        s1 = Series(list(\'abcd\'), index=index)\n        s2 = s1.sort_index()\n        self.assertEqual(s2.index.name, s1.index.name)\n\n\n\n\n\n    #---------------------------------------------------------------------------\n    def test_series_sort_values_a(self) -> None:\n\n        index = IndexYearMonth.from_date_range(\'2017-12-15\', \'2018-03-15\', name=\'foo\')\n        s = Series(list(\'abcd\'), index=index)\n\n        post = s.sort_values(ascending=False)\n\n        self.assertEqual(\n                post.to_pairs(),\n                ((np.datetime64(\'2018-03\'), \'d\'), (np.datetime64(\'2018-02\'), \'c\'), (np.datetime64(\'2018-01\'), \'b\'), (np.datetime64(\'2017-12\'), \'a\'))\n                )\n\n        self.assertEqual(post.index.__class__, IndexYearMonth)\n        self.assertEqual(post.index.name, \'foo\')\n\n    def test_series_sort_values_b(self) -> None:\n\n        index = IndexHierarchy.from_product((0, 1), (10, 20))\n        s = Series(list(\'abcd\'), index=index)\n\n        post = s.sort_values(ascending=False)\n\n        self.assertEqual(post,\n                (((1, 20), \'d\'), ((1, 10), \'c\'), ((0, 20), \'b\'), ((0, 10), \'a\'))\n                )\n\n        self.assertEqual(post.index.__class__, IndexHierarchy)\n\n\n\n    def test_series_reversed(self) -> None:\n\n        idx = tuple(\'abcd\')\n        s = Series(range(4), index=idx)\n        self.assertTrue(tuple(reversed(s)) == tuple(reversed(idx)))\n\n    #---------------------------------------------------------------------------\n\n    def test_series_relabel_a(self) -> None:\n\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        s2 = s1.relabel({\'b\': \'bbb\'})\n        self.assertEqual(s2.to_pairs(),\n                ((\'a\', 0), (\'bbb\', 1), (\'c\', 2), (\'d\', 3)))\n\n        self.assertEqual(mloc(s2.values), mloc(s1.values))\n\n\n    def test_series_relabel_b(self) -> None:\n\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        s2 = s1.relabel({\'a\':\'x\', \'b\':\'y\', \'c\':\'z\', \'d\':\'q\'})\n\n        self.assertEqual(list(s2.items()),\n            [(\'x\', 0), (\'y\', 1), (\'z\', 2), (\'q\', 3)])\n\n\n    def test_series_relabel_c(self) -> None:\n\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        s2 = s1.relabel(IndexAutoFactory)\n        self.assertEqual(\n                s2.to_pairs(),\n                ((0, 0), (1, 1), (2, 2), (3, 3))\n                )\n\n    def test_series_relabel_d(self) -> None:\n\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        idx = IndexHierarchy.from_product((\'a\', \'b\'), (1, 2))\n        s2 = s1.relabel(idx)\n        self.assertEqual(s2.to_pairs(),\n            (((\'a\', 1), 0), ((\'a\', 2), 1), ((\'b\', 1), 2), ((\'b\', 2), 3))\n            )\n\n    def test_series_relabel_e(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        s2 = s1.relabel(IndexAutoFactory)\n        self.assertEqual(s2.to_pairs(),\n                ((0, 0), (1, 1), (2, 2), (3, 3))\n                )\n\n\n    def test_series_relabel_f(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        # reuse the same instance\n        s2 = s1.relabel(None)\n        self.assertEqual(id(s1.index), id(s2.index))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_rehierarch_a(self) -> None:\n\n        colors = (\'red\', \'green\')\n        shapes = (\'square\', \'circle\', \'triangle\')\n        textures = (\'smooth\', \'rough\')\n\n        s1 = sf.Series(range(12), index=sf.IndexHierarchy.from_product(shapes, colors, textures))\n\n        s2 = s1.rehierarch((2,1,0))\n\n        self.assertEqual(s2.to_pairs(),\n                (((\'smooth\', \'red\', \'square\'), 0), ((\'smooth\', \'red\', \'circle\'), 4), ((\'smooth\', \'red\', \'triangle\'), 8), ((\'smooth\', \'green\', \'square\'), 2), ((\'smooth\', \'green\', \'circle\'), 6), ((\'smooth\', \'green\', \'triangle\'), 10), ((\'rough\', \'red\', \'square\'), 1), ((\'rough\', \'red\', \'circle\'), 5), ((\'rough\', \'red\', \'triangle\'), 9), ((\'rough\', \'green\', \'square\'), 3), ((\'rough\', \'green\', \'circle\'), 7), ((\'rough\', \'green\', \'triangle\'), 11))\n                )\n\n\n    def test_series_rehierarch_b(self) -> None:\n        s1 = sf.Series(range(8), index=sf.IndexHierarchy.from_product((\'B\', \'A\'), (100, 2), (\'iv\', \'ii\')))\n\n        self.assertEqual(s1.rehierarch((2,1,0)).to_pairs(),\n                (((\'iv\', 100, \'B\'), 0), ((\'iv\', 100, \'A\'), 4), ((\'iv\', 2, \'B\'), 2), ((\'iv\', 2, \'A\'), 6), ((\'ii\', 100, \'B\'), 1), ((\'ii\', 100, \'A\'), 5), ((\'ii\', 2, \'B\'), 3), ((\'ii\', 2, \'A\'), 7))\n                )\n\n        self.assertEqual(s1.rehierarch((1,2,0)).to_pairs(),\n                (((100, \'iv\', \'B\'), 0), ((100, \'iv\', \'A\'), 4), ((100, \'ii\', \'B\'), 1), ((100, \'ii\', \'A\'), 5), ((2, \'iv\', \'B\'), 2), ((2, \'iv\', \'A\'), 6), ((2, \'ii\', \'B\'), 3), ((2, \'ii\', \'A\'), 7))\n                )\n\n\n    def test_series_rehierarch_c(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        with self.assertRaises(RuntimeError):\n            s1.rehierarch(())\n\n\n    #---------------------------------------------------------------------------\n\n\n    def test_series_get_a(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n        self.assertEqual(s1.get(\'q\'), None)\n        self.assertEqual(s1.get(\'a\'), 0)\n        self.assertEqual(s1.get(\'f\', -1), -1)\n\n    #---------------------------------------------------------------------------\n\n    def test_series_all_a(self) -> None:\n        s1 = Series(range(4), index=(\'a\', \'b\', \'c\', \'d\'))\n\n        self.assertEqual(s1.all(), False)\n        self.assertEqual(s1.any(), True)\n\n    def test_series_all_b(self) -> None:\n        s1 = Series([True, True, np.nan, True], index=(\'a\', \'b\', \'c\', \'d\'), dtype=object)\n\n        self.assertEqual(s1.all(skipna=True), True)\n        self.assertEqual(s1.any(), True)\n\n        with self.assertRaises(TypeError):\n            self.assertTrue(np.isnan(s1.all(skipna=False)))\n\n    def test_series_all_c(self) -> None:\n        s1 = Series([1, np.nan, 1], index=(\'a\', \'b\', \'c\'))\n        self.assertEqual(s1.all(), True)\n        self.assertEqual(s1.any(), True)\n\n    def test_series_all_d(self) -> None:\n        s1 = Series([True, np.nan, True], index=(\'a\', \'b\', \'c\'))\n        self.assertEqual(s1.all(), True)\n        self.assertEqual(s1.any(), True)\n\n    def test_series_all_e(self) -> None:\n        s1 = Series([True, None, True], index=(\'a\', \'b\', \'c\'))\n        self.assertEqual(s1.all(), True)\n        self.assertEqual(s1.any(), True)\n\n\n    def test_series_all_f(self) -> None:\n        s1 = Series([True, None, 1], index=(\'a\', \'b\', \'c\'))\n        with self.assertRaises(TypeError):\n            self.assertTrue(np.isnan(s1.all(skipna=False)))\n        with self.assertRaises(TypeError):\n            self.assertTrue(np.isnan(s1.any(skipna=False)))\n\n    def test_series_all_g(self) -> None:\n        s1 = Series([\'\', \'sdf\', np.nan], index=(\'a\', \'b\', \'c\'))\n        with self.assertRaises(TypeError):\n            self.assertTrue(np.isnan(s1.all(skipna=False)))\n        with self.assertRaises(TypeError):\n            self.assertTrue(np.isnan(s1.any(skipna=False)))\n\n    def test_series_all_h(self) -> None:\n        s1 = Series([\'\', \'sdf\', \'wer\'], index=(\'a\', \'b\', \'c\'))\n        self.assertEqual(s1.all(), False)\n        self.assertEqual(s1.any(), True)\n\n    def test_series_all_i(self) -> None:\n        s1 = Series([\'sdf\', \'wer\'], index=(\'a\', \'b\'))\n        self.assertEqual(s1.all(), True)\n        self.assertEqual(s1.any(), True)\n\n    def test_series_all_j(self) -> None:\n        s1 = Series([\'\', \'sdf\', \'wer\', 30], index=(\'a\', \'b\', \'c\', \'d\'))\n        self.assertEqual(s1.all(), False)\n        self.assertEqual(s1.any(), True)\n\n    def test_series_all_k(self) -> None:\n        s1 = Series([\'sdf\', \'wer\', 30], index=(\'a\', \'b\', \'c\'))\n        self.assertEqual(s1.all(), True)\n        self.assertEqual(s1.any(), True)\n\n    def test_series_all_m(self) -> None:\n        s1 = Series([\'\', 0, False], index=(\'a\', \'b\', \'c\'))\n        self.assertEqual(s1.all(), False)\n        self.assertEqual(s1.any(), False)\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_unique_a(self) -> None:\n        s1 = Series([10, 10, 2, 2], index=(\'a\', \'b\', \'c\', \'d\'), dtype=np.int64)\n\n        self.assertEqual(s1.unique().tolist(), [2, 10])\n\n        s2 = Series([\'b\', \'b\', \'c\', \'c\'], index=(\'a\', \'b\', \'c\', \'d\'), dtype=object)\n        self.assertEqual(s2.unique().tolist(), [\'b\', \'c\'])\n\n\n    def test_series_unique_b(self) -> None:\n        s1 = Series([10, 10, 2, 2], index=(\'a\', \'b\', \'c\', \'d\'), dtype=np.int64)\n\n        self.assertEqual(s1.unique().tolist(), [2, 10])\n\n        s2 = Series([\'b\', \'b\', \'c\', \'c\'], index=(\'a\', \'b\', \'c\', \'d\'), dtype=object)\n        self.assertEqual(s2.unique().tolist(), [\'b\', \'c\'])\n\n\n\n    def test_series_duplicated_a(self) -> None:\n        s1 = Series([1, 10, 10, 5, 2, 2],\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\', \'f\'), dtype=np.int64)\n\n        # this is showing all duplicates, not just the first-found\n        self.assertEqual(s1.duplicated().to_pairs(),\n                ((\'a\', False), (\'b\', True), (\'c\', True), (\'d\', False), (\'e\', True), (\'f\', True)))\n\n        self.assertEqual(s1.duplicated(exclude_first=True).to_pairs(),\n                ((\'a\', False), (\'b\', False), (\'c\', True), (\'d\', False), (\'e\', False), (\'f\', True)))\n\n        self.assertEqual(s1.duplicated(exclude_last=True).to_pairs(),\n                ((\'a\', False), (\'b\', True), (\'c\', False), (\'d\', False), (\'e\', True), (\'f\', False)))\n\n\n    def test_series_duplicated_b(self) -> None:\n        s1 = Series([5, 3, 3, 3, 7, 2, 2, 2, 1],\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\', \'f\', \'g\', \'h\', \'i\'), dtype=np.int64)\n\n        # this is showing all duplicates, not just the first-found\n        self.assertEqual(s1.duplicated().to_pairs(),\n                ((\'a\', False), (\'b\', True), (\'c\', True),\n                (\'d\', True), (\'e\', False), (\'f\', True),\n                (\'g\', True), (\'h\', True), (\'i\', False),\n                ))\n\n        self.assertEqual(s1.duplicated(exclude_first=True).to_pairs(),\n                ((\'a\', False), (\'b\', False), (\'c\', True),\n                (\'d\', True), (\'e\', False), (\'f\', False),\n                (\'g\', True), (\'h\', True), (\'i\', False),\n                ))\n\n        self.assertEqual(s1.duplicated(exclude_last=True).to_pairs(),\n                ((\'a\', False), (\'b\', True), (\'c\', True),\n                (\'d\', False), (\'e\', False), (\'f\', True),\n                (\'g\', True), (\'h\', False), (\'i\', False),\n                ))\n\n\n    def test_series_drop_duplicated_a(self) -> None:\n        s1 = Series([5, 3, 3, 3, 7, 2, 2, 2, 1],\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\', \'f\', \'g\', \'h\', \'i\'), dtype=int)\n\n        self.assertEqual(s1.drop_duplicated().to_pairs(),\n                ((\'a\', 5), (\'e\', 7), (\'i\', 1)))\n\n        self.assertEqual(s1.drop_duplicated(exclude_first=True).to_pairs(),\n                ((\'a\', 5), (\'b\', 3), (\'e\', 7), (\'f\', 2), (\'i\', 1))\n                )\n\n\n    def test_series_reindex_add_level(self) -> None:\n        s1 = Series([\'a\', \'b\', \'c\'])\n\n        s2 = s1.relabel_add_level(\'I\')\n        self.assertEqual(s2.index.depth, 2)\n        self.assertEqual(s2.to_pairs(),\n                (((\'I\', 0), \'a\'), ((\'I\', 1), \'b\'), ((\'I\', 2), \'c\')))\n\n        s3 = s2.relabel_flat()\n        self.assertEqual(s3.index.depth, 1)\n        self.assertEqual(s3.to_pairs(),\n                (((\'I\', 0), \'a\'), ((\'I\', 1), \'b\'), ((\'I\', 2), \'c\')))\n\n\n    def test_series_drop_level_a(self) -> None:\n        s1 = Series([\'a\', \'b\', \'c\'],\n                index=IndexHierarchy.from_labels([(\'A\', 1), (\'B\', 1), (\'C\', 1)]))\n        s2 = s1.relabel_drop_level(-1)\n        self.assertEqual(s2.to_pairs(),\n                ((\'A\', \'a\'), (\'B\', \'b\'), (\'C\', \'c\'))\n                )\n\n    #---------------------------------------------------------------------------\n    def test_series_from_pandas_a(self) -> None:\n        import pandas as pd\n\n        pds = pd.Series([3,4,5], index=list(\'abc\'))\n        sfs = Series.from_pandas(pds)\n        self.assertEqual(list(pds.items()), list(sfs.items()))\n\n        # mutate Pandas\n        pds[\'c\'] = 50\n        self.assertNotEqual(pds[\'c\'], sfs[\'c\'])\n\n        # owning data\n        pds = pd.Series([3,4,5], index=list(\'abc\'))\n        sfs = Series.from_pandas(pds, own_data=True)\n        self.assertEqual(list(pds.items()), list(sfs.items()))\n\n    def test_series_from_pandas_b(self) -> None:\n        import pandas as pd\n\n        pds = pd.Series([3,4,5], index=list(\'abc\'))\n        if hasattr(pds, \'convert_dtypes\'):\n            pds = pds.convert_dtypes()\n        sfs = Series.from_pandas(pds)\n        self.assertEqual(list(pds.items()), list(sfs.items()))\n\n        # mutate Pandas\n        pds[\'c\'] = 50\n        self.assertNotEqual(pds[\'c\'], sfs[\'c\'])\n\n        # owning data\n        pds = pd.Series([3,4,5], index=list(\'abc\'))\n        sfs = Series.from_pandas(pds, own_data=True)\n        self.assertEqual(list(pds.items()), list(sfs.items()))\n\n\n    def test_series_from_pandas_c(self) -> None:\n        import pandas as pd\n\n        pds1 = pd.Series([\'a\', \'b\', np.nan], index=list(\'abc\'))\n        if hasattr(pds1, \'convert_dtypes\'):\n            pds1 = pds1.convert_dtypes()\n            sfs1 = Series.from_pandas(pds1)\n            self.assertEqual(sfs1.dtype, np.dtype(\'O\'))\n\n        pds2 = pd.Series([\'a\', \'b\', \'c\'], index=list(\'abc\'))\n        if hasattr(pds2,  \'convert_dtypes\'):\n            pds2 = pds2.convert_dtypes()\n            sfs2 = Series.from_pandas(pds2)\n            self.assertEqual(sfs2.dtype, np.dtype(\'<U1\'))\n\n        pds3 = pd.Series([False, True, np.nan], index=list(\'abc\'))\n        if hasattr(pds3,  \'convert_dtypes\'):\n            pds3 = pds3.convert_dtypes()\n            sfs3 = Series.from_pandas(pds3)\n            self.assertEqual(sfs3.dtype, np.dtype(\'O\'))\n\n        pds4 = pd.Series([False, True, np.nan], index=list(\'abc\'))\n        if hasattr(pds4,  \'convert_dtypes\'):\n            pds4 = pds4.convert_dtypes()\n            sfs4 = Series.from_pandas(pds4)\n            self.assertEqual(sfs4.dtype, np.dtype(\'O\'))\n\n        pds5 = pd.Series([False, True, False], index=list(\'abc\'))\n        if hasattr(pds5,  \'convert_dtypes\'):\n            pds5 = pds5.convert_dtypes()\n            sfs5 = Series.from_pandas(pds5)\n            self.assertEqual(sfs5.dtype, np.dtype(\'bool\'))\n\n\n    def test_series_from_pandas_d(self) -> None:\n        import pandas as pd\n\n        pds1 = pd.Series([\'a\', \'b\', np.nan], index=list(\'abc\'))\n        if hasattr(pds1, \'convert_dtypes\'):\n            pds1 = pds1.convert_dtypes()\n            sfs1 = Series.from_pandas(pds1, own_data=True)\n            self.assertEqual(sfs1.dtype, np.dtype(\'O\'))\n\n        pds2 = pd.Series([\'a\', \'b\', \'c\'], index=list(\'abc\'))\n        if hasattr(pds2,  \'convert_dtypes\'):\n            pds2 = pds2.convert_dtypes()\n            sfs2 = Series.from_pandas(pds2, own_data=True)\n            self.assertEqual(sfs2.dtype, np.dtype(\'<U1\'))\n\n        pds3 = pd.Series([False, True, np.nan], index=list(\'abc\'))\n        if hasattr(pds3,  \'convert_dtypes\'):\n            pds3 = pds3.convert_dtypes()\n            sfs3 = Series.from_pandas(pds3, own_data=True)\n            self.assertEqual(sfs3.dtype, np.dtype(\'O\'))\n\n        pds4 = pd.Series([False, True, np.nan], index=list(\'abc\'))\n        if hasattr(pds4,  \'convert_dtypes\'):\n            pds4 = pds4.convert_dtypes()\n            sfs4 = Series.from_pandas(pds4, own_data=True)\n            self.assertEqual(sfs4.dtype, np.dtype(\'O\'))\n\n        pds5 = pd.Series([False, True, False], index=list(\'abc\'))\n        if hasattr(pds5,  \'convert_dtypes\'):\n            pds5 = pds5.convert_dtypes()\n            sfs5 = Series.from_pandas(pds5, own_data=True)\n            self.assertEqual(sfs5.dtype, np.dtype(\'bool\'))\n\n\n\n    def test_series_from_pandas_e(self) -> None:\n        import pandas as pd\n\n        pds1 = pd.Series([\'a\', \'b\', None], index=list(\'abc\'))\n        self.assertEqual(sf.Series.from_pandas(pds1,\n                index_constructor=sf.IndexAutoFactory).to_pairs(),\n                ((0, \'a\'), (1, \'b\'), (2, None))\n                )\n\n\n    def test_series_from_pandas_f(self) -> None:\n        import pandas as pd\n\n        pds1 = pd.Series([\'a\', \'b\', None], index=(\'2012\', \'2013\', \'2014\'))\n        self.assertEqual(sf.Series.from_pandas(pds1,\n                index_constructor=sf.IndexYear).to_pairs(),\n                ((np.datetime64(\'2012\'), \'a\'),\n                (np.datetime64(\'2013\'), \'b\'),\n                (np.datetime64(\'2014\'), None))\n                )\n\n\n\n    #---------------------------------------------------------------------------\n    def test_series_to_pandas_a(self) -> None:\n\n        s1 = Series(range(4),\n            index=IndexHierarchy.from_product((\'a\', \'b\'), (\'x\', \'y\')))\n        df = s1.to_pandas()\n\n        self.assertEqual(df.index.values.tolist(),\n                [(\'a\', \'x\'), (\'a\', \'y\'), (\'b\', \'x\'), (\'b\', \'y\')]\n                )\n        self.assertEqual(df.values.tolist(),\n                [0, 1, 2, 3]\n                )\n\n    def test_series_to_pandas_b(self) -> None:\n\n        from pandas import Timestamp\n\n        s1 = Series(range(4),\n            index=IndexDate((\'2018-01-02\', \'2018-01-03\', \'2018-01-04\', \'2018-01-05\')))\n        df = s1.to_pandas()\n\n        self.assertEqual(df.index.tolist(),\n            [Timestamp(\'2018-01-02 00:00:00\'), Timestamp(\'2018-01-03 00:00:00\'), Timestamp(\'2018-01-04 00:00:00\'), Timestamp(\'2018-01-05 00:00:00\')]\n            )\n        self.assertEqual(df.values.tolist(),\n            [0, 1, 2, 3]\n            )\n\n\n\n    def test_series_astype_a(self) -> None:\n\n        s1 = Series([\'a\', \'b\', \'c\'])\n\n        s2 = s1.astype(object)\n        self.assertEqual(s2.to_pairs(),\n                ((0, \'a\'), (1, \'b\'), (2, \'c\')))\n        self.assertTrue(s2.dtype == object)\n\n        # we cannot convert to float\n        with self.assertRaises(ValueError):\n            s1.astype(float)\n\n    def test_series_astype_b(self) -> None:\n\n        s1 = Series([1, 3, 4, 0])\n\n        s2 = s1.astype(bool)\n        self.assertEqual(\n                s2.to_pairs(),\n                ((0, True), (1, True), (2, True), (3, False)))\n        self.assertTrue(s2.dtype == bool)\n\n\n    def test_series_min_max_a(self) -> None:\n\n        s1 = Series([1, 3, 4, 0])\n        self.assertEqual(s1.min(), 0)\n        self.assertEqual(s1.max(), 4)\n\n\n        s2 = sf.Series([-1, 4, None, np.nan])\n        self.assertEqual(s2.min(), -1)\n        with self.assertRaises(TypeError):\n            s2.min(skipna=False)\n\n        self.assertEqual(s2.max(), 4)\n        with self.assertRaises(TypeError):\n            s2.max(skipna=False)\n\n        s3 = sf.Series([-1, 4, None])\n        self.assertEqual(s3.min(), -1)\n        with self.assertRaises(TypeError):\n            s2.max(skipna=False)\n\n\n\n    def test_series_min_max_b(self) -> None:\n        # string objects work as expected; when fixed length strings, however, the do not\n\n        s1 = Series(list(\'abc\'), dtype=object)\n        self.assertEqual(s1.min(), \'a\')\n        self.assertEqual(s1.max(), \'c\')\n\n        # get the same result from character arrays\n        s2 = sf.Series(list(\'abc\'))\n        self.assertEqual(s2.min(), \'a\')\n        self.assertEqual(s2.max(), \'c\')\n\n    #---------------------------------------------------------------------------\n\n    def test_series_clip_a(self) -> None:\n\n        s1 = Series(range(6), index=list(\'abcdef\'))\n\n        self.assertEqual(s1.clip(lower=3).to_pairs(),\n                ((\'a\', 3), (\'b\', 3), (\'c\', 3), (\'d\', 3), (\'e\', 4), (\'f\', 5))\n                )\n\n        self.assertEqual(s1.clip(lower=-1).to_pairs(),\n                ((\'a\', 0), (\'b\', 1), (\'c\', 2), (\'d\', 3), (\'e\', 4), (\'f\', 5))\n                )\n\n        self.assertEqual(s1.clip(upper=-1).to_pairs(),\n                ((\'a\', -1), (\'b\', -1), (\'c\', -1), (\'d\', -1), (\'e\', -1), (\'f\', -1))\n                )\n\n        self.assertEqual(s1.clip(upper=3).to_pairs(),\n                ((\'a\', 0), (\'b\', 1), (\'c\', 2), (\'d\', 3), (\'e\', 3), (\'f\', 3))\n                )\n\n\n    def test_series_clip_b(self) -> None:\n        s1 = Series(range(6), index=list(\'abcdef\'))\n\n        s2 = Series((2, 3, 0, -1, 8, 6), index=list(\'abcdef\'))\n\n        self.assertEqual(s1.clip(lower=s2).to_pairs(),\n                ((\'a\', 2), (\'b\', 3), (\'c\', 2), (\'d\', 3), (\'e\', 8), (\'f\', 6))\n                )\n\n        self.assertEqual(s1.clip(upper=s2).to_pairs(),\n                ((\'a\', 0), (\'b\', 1), (\'c\', 0), (\'d\', -1), (\'e\', 4), (\'f\', 5))\n                )\n\n        s3 = Series((2, 3, 0), index=list(\'abc\'))\n\n        self.assertEqual(s1.clip(lower=s3).to_pairs(),\n                ((\'a\', 2), (\'b\', 3), (\'c\', 2), (\'d\', 3), (\'e\', 4), (\'f\', 5))\n                )\n\n        self.assertEqual(s1.clip(upper=s3).to_pairs(),\n                ((\'a\', 0), (\'b\', 1), (\'c\', 0), (\'d\', 3), (\'e\', 4), (\'f\', 5))\n                )\n\n\n    def test_series_clip_c(self) -> None:\n        s1 = Series(range(6), index=list(\'abcdef\'))\n\n        with self.assertRaises(RuntimeError):\n            _ = s1.clip(lower=(2, 5))\n\n\n    #---------------------------------------------------------------------------\n    def test_series_pickle_a(self) -> None:\n        s1 = Series(range(6), index=list(\'abcdef\'))\n        s2 = Series((2, 3, 0, -1, 8, 6), index=list(\'abcdef\'))\n        s3 = s2.astype(bool)\n\n\n        for series in (s1, s2, s3):\n            pbytes = pickle.dumps(series)\n            series_new = pickle.loads(pbytes)\n            for v in series: # iter labels\n                # this compares series objects\n                self.assertFalse(series_new.values.flags.writeable)\n                self.assertEqual(series_new.loc[v], series.loc[v])\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_drop_loc_a(self) -> None:\n        s1 = Series((2, 3, 0, -1, 8, 6), index=list(\'abcdef\'))\n\n        self.assertEqual(s1.drop.loc[\'d\'].to_pairs(),\n                ((\'a\', 2), (\'b\', 3), (\'c\', 0), (\'e\', 8), (\'f\', 6)))\n\n        self.assertEqual(s1.drop.loc[\'d\':].to_pairs(),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                ((\'a\', 2), (\'b\', 3), (\'c\', 0)))\n\n        self.assertEqual(s1.drop.loc[\'d\':\'e\'].to_pairs(),  # type: ignore  # https://github.com/python/typeshed/pull/3024\n                ((\'a\', 2), (\'b\', 3), (\'c\', 0), (\'f\', 6)))\n\n        self.assertEqual(s1.drop.loc[s1 > 0].to_pairs(),\n                ((\'c\', 0), (\'d\', -1)))\n\n\n    def test_series_drop_loc_b(self) -> None:\n        s1 = Series((2, 3, 0, -1), index=list(\'abcd\'))\n        s2 = s1._drop_iloc((s1 < 1).values)\n        self.assertEqual(s2.to_pairs(), ((\'a\', 2), (\'b\', 3)))\n\n\n\n    def test_series_drop_iloc_a(self) -> None:\n        s1 = Series((2, 3, 0, -1, 8, 6), index=list(\'abcdef\'))\n\n        self.assertEqual(s1.drop.iloc[-1].to_pairs(),\n                ((\'a\', 2), (\'b\', 3), (\'c\', 0), (\'d\', -1), (\'e\', 8))\n                )\n        self.assertEqual(s1.drop.iloc[2:].to_pairs(),\n                ((\'a\', 2), (\'b\', 3)))\n\n        self.assertEqual(s1.drop.iloc[[0, 3]].to_pairs(),\n                ((\'b\', 3), (\'c\', 0), (\'e\', 8), (\'f\', 6)))\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_head_a(self) -> None:\n        s1 = Series(range(100), index=reversed(range(100)))\n        self.assertEqual(s1.head().to_pairs(),\n                ((99, 0), (98, 1), (97, 2), (96, 3), (95, 4)))\n        self.assertEqual(s1.head(2).to_pairs(),\n                ((99, 0), (98, 1)))\n\n\n    def test_series_tail_a(self) -> None:\n        s1 = Series(range(100), index=reversed(range(100)))\n\n        self.assertEqual(s1.tail().to_pairs(),\n                ((4, 95), (3, 96), (2, 97), (1, 98), (0, 99)))\n\n        self.assertEqual(s1.tail(2).to_pairs(),\n                ((1, 98), (0, 99)))\n\n\n    def test_series_roll_a(self) -> None:\n        s1 = Series((2, 3, 0, -1, 8, 6), index=list(\'abcdef\'))\n\n        self.assertEqual(s1.roll(2).to_pairs(),\n                ((\'a\', 8), (\'b\', 6), (\'c\', 2), (\'d\', 3), (\'e\', 0), (\'f\', -1))\n                )\n\n        self.assertEqual(s1.roll(-2).to_pairs(),\n                ((\'a\', 0), (\'b\', -1), (\'c\', 8), (\'d\', 6), (\'e\', 2), (\'f\', 3))\n                )\n\n        # if the roll is a noop, we reuse the same array\n        self.assertEqual(s1.mloc, s1.roll(len(s1)).mloc)\n\n\n    def test_series_roll_b(self) -> None:\n        s1 = Series((2, 3, 0, -1, 8, 6), index=list(\'abcdef\'))\n\n        self.assertEqual(s1.roll(2, include_index=True).to_pairs(),\n            ((\'e\', 8), (\'f\', 6), (\'a\', 2), (\'b\', 3), (\'c\', 0), (\'d\', -1))\n            )\n\n        self.assertEqual(s1.roll(-2, include_index=True).to_pairs(),\n            ((\'c\', 0), (\'d\', -1), (\'e\', 8), (\'f\', 6), (\'a\', 2), (\'b\', 3))\n            )\n\n\n    def test_series_shift_a(self) -> None:\n        s1 = Series((2, 3, 0, -1, 8, 6), index=list(\'abcdef\'))\n\n        # if the shift is a noop, we reuse the same array\n        self.assertEqual(s1.mloc, s1.shift(0).mloc)\n\n        # default fill is NaN\n        self.assertEqual(s1.shift(4).dtype,\n                np.dtype(\'float64\')\n                )\n\n        # import ipdb; ipdb.set_trace()\n        self.assertEqual(s1.shift(4, fill_value=None).to_pairs(),\n                ((\'a\', None), (\'b\', None), (\'c\', None), (\'d\', None), (\'e\', 2), (\'f\', 3))\n                )\n\n        self.assertEqual(s1.shift(-4, fill_value=None).to_pairs(),\n                ((\'a\', 8), (\'b\', 6), (\'c\', None), (\'d\', None), (\'e\', None), (\'f\', None))\n                )\n\n        self.assertEqual(\n                s1.shift(6, fill_value=None).to_pairs(),\n                ((\'a\', None), (\'b\', None), (\'c\', None), (\'d\', None), (\'e\', None), (\'f\', None))\n                )\n\n        self.assertEqual(\n                s1.shift(-6, fill_value=None).to_pairs(),\n                ((\'a\', None), (\'b\', None), (\'c\', None), (\'d\', None), (\'e\', None), (\'f\', None))\n                )\n\n    #---------------------------------------------------------------------------\n    def test_series_isin_a(self) -> None:\n\n        s1 = Series((2, 3, 0, -1, 8, 6), index=list(\'abcdef\'))\n\n        self.assertEqual(s1.isin([]).to_pairs(),\n            ((\'a\', False), (\'b\', False), (\'c\', False), (\'d\', False), (\'e\', False), (\'f\', False))\n            )\n\n        self.assertEqual(s1.isin((-1, 8)).to_pairs(),\n            ((\'a\', False), (\'b\', False), (\'c\', False), (\'d\', True), (\'e\', True), (\'f\', False))\n            )\n\n        self.assertEqual(s1.isin(s1.values).to_pairs(),\n            ((\'a\', True), (\'b\', True), (\'c\', True), (\'d\', True), (\'e\', True), (\'f\', True))\n            )\n\n\n    def test_series_isin_b(self) -> None:\n\n        s1 = Series([\'a\', \'b\', \'c\', \'d\'])\n        self.assertEqual(s1.isin((\'b\', \'c\')).to_pairs(),\n                ((0, False), (1, True), (2, True), (3, False)))\n\n        self.assertEqual(s1.isin((\'b\', \'c\', None)).to_pairs(),\n                ((0, False), (1, True), (2, True), (3, False)))\n\n        self.assertEqual(s1.isin(s1[[1, 2]].values).to_pairs(),\n                ((0, False), (1, True), (2, True), (3, False)))\n\n        self.assertEqual(s1.isin({\'b\', \'c\'}).to_pairs(),\n                ((0, False), (1, True), (2, True), (3, False)))\n\n\n    def test_series_isin_c(self) -> None:\n\n        s1 = Series([\'a\', \'b\', \'c\', \'d\', \'a\', \'b\', \'c\', \'d\'])\n\n        self.assertEqual(s1.isin((\'a\', \'d\')).to_pairs(),\n                ((0, True), (1, False), (2, False), (3, True), (4, True), (5, False), (6, False), (7, True)))\n\n\n    def test_series_isin_d(self) -> None:\n        s1 = Series((1, 1), index=list(\'ab\'))\n        lookup = {2,3,4,5,6,7,8,9,10,11,12,13}\n        # Checks an edge case where if a numpy `assume_unique` flag is incorrectly passed, it returns the wrong result\n        result = s1.isin(lookup)\n        self.assertEqual(result.to_pairs(),\n                ((\'a\', False), (\'b\', False)))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_to_html_a(self) -> None:\n\n        s1 = Series((2, 3, 0, -1, 8, 6), index=list(\'abcdef\'))\n\n        post = s1.to_html(config=DisplayConfig(type_show=False, type_color=False))\n        html = \'<table border=""1""><tbody><tr><th>a</th><td>2</td></tr><tr><th>b</th><td>3</td></tr><tr><th>c</th><td>0</td></tr><tr><th>d</th><td>-1</td></tr><tr><th>e</th><td>8</td></tr><tr><th>f</th><td>6</td></tr></tbody></table>\'\n        self.assertEqual(post.strip(), html.strip())\n\n\n    def test_series_to_html_datatables_a(self) -> None:\n\n        s1 = Series((2, 3, 0, -1, 8, 6), index=list(\'abcdef\'))\n        sio = StringIO()\n        post = s1.to_html_datatables(sio, show=False)\n        self.assertEqual(post, None)\n        self.assertTrue(len(sio.read()) >= 1396)\n\n\n    def test_series_to_html_datatables_b(self) -> None:\n\n        s1 = Series((2, 3, 0, -1, 8, 6), index=list(\'abcdef\'))\n\n        with temp_file(\'.html\', path=True) as fp:\n            s1.to_html_datatables(fp, show=False)\n            with open(fp) as file:\n                data = file.read()\n                self.assertTrue(\'SFTable\' in data)\n                self.assertTrue(len(data) > 800)\n\n\n\n\n    def test_series_disply_a(self) -> None:\n\n        s1 = Series((2, 3), index=list(\'ab\'), name=\'alt\', dtype=np.int64)\n\n        match = tuple(s1.display(DisplayConfig(type_color=False)))\n        self.assertEqual(\n            match,\n            ([\'<Series: alt>\'], [\'<Index>\', \'\'], [\'a\', \'2\'], [\'b\', \'3\'], [\'<<U1>\', \'<int64>\'])\n            )\n\n        s2 = Series((\'a\', \'b\'), index=Index((\'x\', \'y\'), name=\'bar\'), name=\'foo\')\n\n        match = tuple(s2.display(DisplayConfig(type_color=False)))\n\n        self.assertEqual(\n            match,\n            ([\'<Series: foo>\'], [\'<Index: bar>\', \'\'], [\'x\', \'a\'], [\'y\', \'b\'], [\'<<U1>\', \'<<U1>\'])\n            )\n\n\n    def test_series_to_frame_a(self) -> None:\n\n        s1 = Series((2, 3), index=list(\'ab\'), name=\'alt\')\n\n        f1 = s1.to_frame()\n\n        self.assertTrue(f1.__class__ is Frame)\n        self.assertEqual(f1.columns.values.tolist(), [\'alt\'])\n        self.assertEqual(f1.to_pairs(0),\n            ((\'alt\', ((\'a\', 2), (\'b\', 3))),))\n\n        self.assertTrue(s1.mloc == f1.mloc.tolist()[0])\n\n    def test_series_to_frame_b(self) -> None:\n\n        s1 = Series((2, 3), index=list(\'ab\'), name=\'alt\')\n\n        f1 = s1.to_frame_go()\n\n        self.assertTrue(f1.__class__ is FrameGO)\n        self.assertEqual(f1.columns.values.tolist(), [\'alt\'])\n        self.assertEqual(f1.to_pairs(0),\n            ((\'alt\', ((\'a\', 2), (\'b\', 3))),))\n\n        self.assertTrue(s1.mloc == f1.mloc.tolist()[0])\n\n    def test_series_to_frame_c(self) -> None:\n\n        s1 = Series((2, 3, 4), index=list(\'abc\'), name=\'alt\')\n\n        f2 = s1.to_frame(axis=0)\n        self.assertEqual(f2.to_pairs(0),\n            ((\'a\', ((\'alt\', 2),)), (\'b\', ((\'alt\', 3),)), (\'c\', ((\'alt\', 4),))))\n\n    def test_series_to_frame_d(self) -> None:\n\n        s1 = Series((2, 3, 4), index=list(\'abc\'), name=\'alt\')\n        with self.assertRaises(NotImplementedError):\n            s1.to_frame(axis=None)  # type: ignore\n\n\n    def test_series_to_frame_go_a(self) -> None:\n        a = sf.Series((1, 2, 3), name=\'a\')\n        f = a.to_frame_go(axis=0)\n        f[\'b\'] = \'b\'\n\n        self.assertEqual(f.to_pairs(0),\n                ((0, ((\'a\', 1),)), (1, ((\'a\', 2),)), (2, ((\'a\', 3),)), (\'b\', ((\'a\', \'b\'),)))\n                )\n\n\n    def test_series_from_concat_a(self) -> None:\n        s1 = Series((2, 3, 0,), index=list(\'abc\'))\n        s2 = Series((10, 20), index=list(\'de\'))\n        s3 = Series((8, 6), index=list(\'fg\'))\n\n        s = Series.from_concat((s1, s2, s3))\n\n        self.assertEqual(s.to_pairs(),\n                ((\'a\', 2), (\'b\', 3), (\'c\', 0), (\'d\', 10), (\'e\', 20), (\'f\', 8), (\'g\', 6))\n                )\n\n    def test_series_from_concat_b(self) -> None:\n        s1 = Series((2, 3, 0,), index=list(\'abc\'))\n        s2 = Series((\'10\', \'20\'), index=list(\'de\'))\n        s3 = Series((8, 6), index=list(\'fg\'))\n\n        s = Series.from_concat((s1, s2, s3))\n\n        self.assertEqual(s.to_pairs(),\n                ((\'a\', 2), (\'b\', 3), (\'c\', 0), (\'d\', \'10\'), (\'e\', \'20\'), (\'f\', 8), (\'g\', 6))\n                )\n\n\n    def test_series_from_concat_c(self) -> None:\n        s1 = Series((2, 3, 0,), index=list(\'abc\'))\n        s2 = Series((\'10\', \'20\'), index=list(\'de\'))\n        s3 = Series((8, 6), index=(1, 2))\n\n        s = Series.from_concat((s1, s2, s3))\n\n        self.assertEqual(s.to_pairs(),\n                ((\'a\', 2), (\'b\', 3), (\'c\', 0), (\'d\', \'10\'), (\'e\', \'20\'), (1, 8), (2, 6))\n                )\n\n    def test_series_from_concat_d(self) -> None:\n        s1 = Series((2, 3, 0,), index=list(\'abc\')).relabel_add_level(\'i\')\n        s2 = Series((\'10\', \'20\', \'100\'), index=list(\'abc\')).relabel_add_level(\'ii\')\n\n        s3 = Series.from_concat((s1, s2))\n\n        self.assertEqual(s3.to_pairs(),\n                (((\'i\', \'a\'), 2), ((\'i\', \'b\'), 3), ((\'i\', \'c\'), 0), ((\'ii\', \'a\'), \'10\'), ((\'ii\', \'b\'), \'20\'), ((\'ii\', \'c\'), \'100\'))\n                )\n\n    def test_series_from_concat_e(self) -> None:\n        s1 = Series((2, 3, 0,), index=list(\'abc\'))\n        s2 = Series((10, 20), index=list(\'de\'))\n        s3 = Series((8, 6), index=list(\'fg\'))\n\n\n        s = Series.from_concat((s1, s2, s3), index=IndexAutoFactory)\n\n        self.assertEqual(s.to_pairs(),\n                ((0, 2), (1, 3), (2, 0), (3, 10), (4, 20), (5, 8), (6, 6))\n                )\n\n    def test_series_from_concat_f(self) -> None:\n        s1 = Series((2, 3, 0,), index=list(\'abc\'))\n        s2 = Series((10, 20), index=list(\'de\'))\n        s3 = Series((8, 6), index=list(\'fg\'))\n\n        s = Series.from_concat((s1, s2, s3), index=list(\'pqrstuv\'))\n\n        self.assertEqual(s.to_pairs(),\n                ((\'p\', 2), (\'q\', 3), (\'r\', 0), (\'s\', 10), (\'t\', 20), (\'u\', 8), (\'v\', 6))\n                )\n\n    def test_series_from_concat_g(self) -> None:\n\n        s1 = Series.from_concat([])\n        self.assertEqual((0,), s1.shape)\n\n        s2 = Series.from_concat([], index=[])\n        self.assertEqual((0,), s2.shape)\n        self.assertEqual((0,), s2.index.shape)\n\n        s3 = Series.from_concat([], name=\'s3\')\n        self.assertEqual((0,), s3.shape)\n        self.assertEqual(\'s3\', s3.name)\n\n        s4 = Series.from_concat([], index=[], name=\'s4\')\n        self.assertEqual((0,), s4.shape)\n        self.assertEqual((0,), s4.index.shape)\n        self.assertEqual(\'s4\', s4.name)\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_iter_group_a(self) -> None:\n\n        s1 = Series((10, 4, 10, 4, 10),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                dtype=object)\n\n        group = tuple(s1.iter_group(axis=0))\n\n        self.assertEqual(group[0].to_pairs(),\n                ((\'a\', 10), (\'c\', 10), (\'e\', 10)))\n\n        self.assertEqual(group[1].to_pairs(),\n                ((\'b\', 4), (\'d\', 4)))\n\n        with self.assertRaises(AxisInvalid):\n            tuple(s1.iter_group(axis=1))\n\n        with self.assertRaises(TypeError):\n            tuple(s1.iter_group(\'sdf\')) #type: ignore\n\n        with self.assertRaises(TypeError):\n            tuple(s1.iter_group(foo=\'sdf\')) #type: ignore\n\n\n    #---------------------------------------------------------------------------\n    def test_series_iter_group_index_a(self) -> None:\n\n        s1 = Series((10, 3, 15, 21, 28),\n                index=(\'a\', \'b\', \'c\', \'d\', \'e\'),\n                dtype=object)\n\n        post = tuple(s1.iter_group_labels_items())\n        self.assertTrue(len(post), len(s1))\n        self.assertTrue(all(isinstance(x[1], Series) for x in post))\n\n    def test_series_iter_group_index_b(self) -> None:\n\n        colors = (\'red\', \'green\')\n        shapes = (\'square\', \'circle\', \'triangle\')\n        s1 = sf.Series(range(6), index=sf.IndexHierarchy.from_product(shapes, colors))\n\n        post = tuple(s1.iter_group_labels(depth_level=0))\n        self.assertTrue(len(post), 3)\n\n        self.assertEqual(s1.iter_group_labels(depth_level=0).apply(np.sum).to_pairs(),\n                ((\'circle\', 5), (\'square\', 1), (\'triangle\', 9))\n                )\n\n        self.assertEqual(s1.iter_group_labels(depth_level=1).apply(np.sum).to_pairs(),\n                ((\'green\', 9), (\'red\', 6))\n                )\n\n    def test_series_iter_group_index_c(self) -> None:\n\n        colors = (\'red\', \'green\')\n        shapes = (\'square\', \'circle\', \'triangle\')\n        textures = (\'smooth\', \'rough\')\n\n        s1 = sf.Series(range(12),\n                index=sf.IndexHierarchy.from_product(shapes, colors, textures)\n                )\n\n        post = tuple(s1.iter_group_labels(depth_level=[0, 2]))\n        self.assertTrue(len(post), 6)\n\n        self.assertEqual(s1.iter_group_labels(depth_level=[0, 2]).apply(np.sum).to_pairs(),\n                (((\'circle\', \'rough\'), 12), ((\'circle\', \'smooth\'), 10), ((\'square\', \'rough\'), 4), ((\'square\', \'smooth\'), 2), ((\'triangle\', \'rough\'), 20), ((\'triangle\', \'smooth\'), 18))\n                )\n\n\n    def test_series_locmin_a(self) -> None:\n        s1 = Series((2, 3, 0,), index=list(\'abc\'))\n        self.assertEqual(s1.loc_min(), \'c\')\n        self.assertEqual(s1.iloc_min(), 2)\n        self.assertEqual(s1.loc_max(), \'b\')\n        self.assertEqual(s1.iloc_max(), 1)\n\n    def test_series_locmin_b(self) -> None:\n        s1 = Series((2, np.nan, 0, -1), index=list(\'abcd\'))\n        self.assertEqual(s1.loc_min(), \'d\')\n        self.assertEqual(s1.iloc_min(), 3)\n        self.assertEqual(s1.loc_max(), \'a\')\n        self.assertEqual(s1.iloc_max(), 0)\n\n\n    def test_series_locmin_c(self) -> None:\n        s1 = Series((2, np.nan, 0,), index=list(\'abc\'))\n\n        with self.assertRaises(RuntimeError):\n            s1.loc_min(skipna=False)\n\n        with self.assertRaises(RuntimeError):\n            s1.loc_max(skipna=False)\n\n\n    def test_series_from_concat_items_a(self) -> None:\n\n        s1 = Series((2, 3, 0,), index=list(\'abc\'))\n        s2 = Series((2, np.nan, 0, -1), index=list(\'abcd\'))\n\n        s3 = Series.from_concat_items(((\'x\', s1), (\'y\', s2)))\n\n        self.assertAlmostEqualItems(s3.to_pairs(),\n                (((\'x\', \'a\'), 2.0), ((\'x\', \'b\'), 3.0), ((\'x\', \'c\'), 0.0), ((\'y\', \'a\'), 2.0), ((\'y\', \'b\'), np.nan), ((\'y\', \'c\'), 0.0), ((\'y\', \'d\'), -1.0))\n                )\n\n        self.assertAlmostEqualItems(s3[HLoc[:, \'b\']].to_pairs(),\n                (((\'x\', \'b\'), 3.0), ((\'y\', \'b\'), np.nan)))\n\n\n    def test_series_from_concat_items_b(self) -> None:\n        s1 = Series.from_concat_items([])\n\n        self.assertEqual((0,), s1.shape)\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_axis_window_items_a(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n\n        post = tuple(s1._axis_window_items(as_array=True, size=2, step=1, label_shift=0))\n\n        # first window has second label, and first two values\n        self.assertEqual(post[0][1].tolist(), [1, 2])\n        self.assertEqual(post[0][0], \'b\')\n\n        self.assertEqual(post[-1][1].tolist(), [19, 20])\n        self.assertEqual(post[-1][0], \'t\')\n\n\n    def test_series_axis_window_items_b(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n\n        post = tuple(s1._axis_window_items(as_array=True, size=2, step=1, label_shift=-1))\n\n        # first window has first label, and first two values\n        self.assertEqual(post[0][1].tolist(), [1, 2])\n        self.assertEqual(post[0][0], \'a\')\n\n        self.assertEqual(post[-1][1].tolist(), [19, 20])\n        self.assertEqual(post[-1][0], \'s\')\n\n\n\n    def test_series_axis_window_items_c(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n\n        # this is an expanding window anchored at the first index\n        post = tuple(s1._axis_window_items(as_array=True, size=1, step=0, size_increment=1))\n\n        self.assertEqual(post[0][0], \'a\')\n        self.assertEqual(post[0][1].tolist(), [1])\n\n        self.assertEqual(post[-1][0], \'t\')\n        self.assertEqual(post[-1][1].tolist(), list(range(1, 21)))\n\n\n\n    def test_series_axis_window_items_d(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n\n        post = tuple(s1._axis_window_items(as_array=True, size=5, start_shift=-5, window_sized=False))\n\n        self.assertEqual(post[0][0], \'a\')\n        self.assertEqual(post[0][1].tolist(), [1])\n\n        self.assertEqual(post[1][0], \'b\')\n        self.assertEqual(post[1][1].tolist(), [1, 2])\n\n        self.assertEqual(post[-1][0], \'t\')\n        self.assertEqual(post[-1][1].tolist(), [16, 17, 18, 19, 20])\n\n\n\n    def test_series_axis_window_items_e(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n\n        # start shift needs to be 1 less than window to go to start of window\n        post = tuple(s1._axis_window_items(as_array=True, size=5, label_shift=-4, window_sized=False))\n\n        self.assertEqual(post[0][0], \'a\')\n        self.assertEqual(post[0][1].tolist(), [1, 2, 3, 4, 5])\n\n        self.assertEqual(post[1][0], \'b\')\n        self.assertEqual(post[1][1].tolist(), [2, 3, 4, 5, 6])\n\n        self.assertEqual(post[-1][0], \'t\')\n        self.assertEqual(post[-1][1].tolist(), [20])\n\n\n\n    def test_series_axis_window_items_f(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n\n        # start shift needs to be 1 less than window to go to start of window\n        post = tuple(s1._axis_window_items(as_array=True, size=5, label_shift=-4, window_sized=True))\n\n        self.assertEqual(post[0][0], \'a\')\n        self.assertEqual(post[0][1].tolist(), [1, 2, 3, 4, 5])\n\n        self.assertEqual(post[1][0], \'b\')\n        self.assertEqual(post[1][1].tolist(), [2, 3, 4, 5, 6])\n\n        self.assertEqual(post[-1][0], \'p\')\n        self.assertEqual(post[-1][1].tolist(), [16, 17, 18, 19, 20])\n\n\n    def test_series_axis_window_items_g(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n\n        with self.assertRaises(RuntimeError):\n            tuple(s1._axis_window_items(as_array=True, size=0))\n\n        with self.assertRaises(RuntimeError):\n            tuple(s1._axis_window_items(size=2, as_array=True, step=-1))\n\n\n    def test_series_axis_window_items_h(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n\n        post = tuple(s1._axis_window_items(as_array=True, size=1))\n        self.assertEqual(post[0][0], \'a\')\n        self.assertEqual(post[0][1].tolist(), [1])\n\n        self.assertEqual(post[-1][0], \'t\')\n        self.assertEqual(post[-1][1].tolist(), [20])\n\n\n\n    def test_series_axis_window_items_i(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n        # step equal to window size produces adaject windows\n        post = tuple(s1._axis_window_items(as_array=True, size=3, step=3))\n\n        self.assertEqual(post[0][0], \'c\')\n        self.assertEqual(post[0][1].tolist(), [1, 2, 3])\n\n        self.assertEqual(post[1][0], \'f\')\n        self.assertEqual(post[1][1].tolist(), [4, 5, 6])\n\n        self.assertEqual(post[-1][0], \'r\')\n        self.assertEqual(post[-1][1].tolist(), [16, 17, 18])\n\n\n    def test_series_axis_window_items_j(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n        # adjacent windows with label on first value, keeping incomplete windows\n        post = tuple(s1._axis_window_items(as_array=True, size=3, step=3, label_shift=-2, window_sized=False))\n\n        self.assertEqual(post[0][0], \'a\')\n        self.assertEqual(post[0][1].tolist(), [1, 2, 3])\n\n        self.assertEqual(post[1][0], \'d\')\n        self.assertEqual(post[1][1].tolist(), [4, 5, 6])\n\n        self.assertEqual(post[-1][0], \'s\')\n        self.assertEqual(post[-1][1].tolist(), [19, 20])\n\n\n\n    def test_series_axis_window_items_k(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n        # adjacent windows with label on first value, keeping incomplete windows\n        post = tuple(s1._axis_window_items(as_array=True, size=3, window_valid=lambda w: np.sum(w) % 2 == 1))\n\n        self.assertEqual(post[0][0], \'d\')\n        self.assertEqual(post[0][1].tolist(), [2, 3, 4])\n\n        self.assertEqual(post[1][0], \'f\')\n        self.assertEqual(post[1][1].tolist(), [4, 5, 6])\n\n        self.assertEqual(post[-1][0], \'t\')\n        self.assertEqual(post[-1][1].tolist(), [18, 19, 20])\n\n\n\n    def test_series_axis_window_items_m(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n        # adjacent windows with label on first value, keeping incomplete windows\n        weight = np.array([.25, .5, .5, .25])\n        post = tuple(s1._axis_window_items(as_array=True, size=4, window_func=lambda a: a * weight))\n\n        self.assertEqual(post[0][0], \'d\')\n        self.assertEqual(post[0][1].tolist(), [0.25, 1, 1.5, 1])\n\n        self.assertEqual(post[-1][0], \'t\')\n        self.assertEqual(post[-1][1].tolist(), [4.25, 9, 9.5, 5])\n\n    #---------------------------------------------------------------------------\n\n    def test_series_iter_window_array_a(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n\n        self.assertEqual(\n                tuple(tuple(a) for a in s1.iter_window_array(size=2)),\n                ((1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20))\n                )\n\n    def test_series_iter_window_array_b(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n        s2 = s1.iter_window_array(size=2).apply(np.mean)\n        self.assertEqual(s2.to_pairs(),\n                ((\'b\', 1.5), (\'c\', 2.5), (\'d\', 3.5), (\'e\', 4.5), (\'f\', 5.5), (\'g\', 6.5), (\'h\', 7.5), (\'i\', 8.5), (\'j\', 9.5), (\'k\', 10.5), (\'l\', 11.5), (\'m\', 12.5), (\'n\', 13.5), (\'o\', 14.5), (\'p\', 15.5), (\'q\', 16.5), (\'r\', 17.5), (\'s\', 18.5), (\'t\', 19.5))\n        )\n\n\n    #---------------------------------------------------------------------------\n    def test_series_iter_window_a(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n\n        self.assertEqual(\n                tuple(s.index.values.tolist() for s in s1.iter_window(size=2)), # type: ignore\n                ([\'a\', \'b\'], [\'b\', \'c\'], [\'c\', \'d\'], [\'d\', \'e\'], [\'e\', \'f\'], [\'f\', \'g\'], [\'g\', \'h\'], [\'h\', \'i\'], [\'i\', \'j\'], [\'j\', \'k\'], [\'k\', \'l\'], [\'l\', \'m\'], [\'m\', \'n\'], [\'n\', \'o\'], [\'o\', \'p\'], [\'p\', \'q\'], [\'q\', \'r\'], [\'r\', \'s\'], [\'s\', \'t\'])\n                )\n\n        self.assertEqual(\n            s1.iter_window(size=5, label_shift=-4, step=6, window_sized=False\n                    ).apply(lambda s: len(s.index)).to_pairs(),\n            ((\'a\', 5), (\'g\', 5), (\'m\', 5), (\'s\', 2))\n        )\n\n\n    def test_series_iter_window_b(self) -> None:\n\n        s1 = Series(range(10), index=self.get_letters(10))\n\n        with self.assertRaises(TypeError):\n            s1.iter_window() #type: ignore\n\n        with self.assertRaises(TypeError):\n            s1.iter_window(3) #type: ignore\n\n        with self.assertRaises(TypeError):\n            s1.iter_window(foo=3) #type: ignore\n\n        self.assertEqual(\n                tuple(x.to_pairs() for x in s1.iter_window(size=2, step=2)), #type: ignore\n                (((\'a\', 0), (\'b\', 1)), ((\'c\', 2), (\'d\', 3)), ((\'e\', 4), (\'f\', 5)), ((\'g\', 6), (\'h\', 7)), ((\'i\', 8), (\'j\', 9)))\n                )\n\n    def test_series_iter_window_c(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n\n        self.assertEqual(\n                tuple(w.tolist() for w in s1.iter_window_array( #type: ignore\n                        size=7,\n                        step=7,\n                        window_sized=False,\n                        label_shift=-6,\n                        )),\n                ([1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14], [15, 16, 17, 18, 19, 20])\n                )\n\n\n    def test_series_iter_window_d(self) -> None:\n        post1 = sf.Series(range(12)).iter_window_array(\n                size=5,\n                start_shift=-10,\n                window_sized=True).apply(np.mean)\n\n        self.assertEqual(post1.to_pairs(),\n                ((4, 2.0), (5, 3.0), (6, 4.0), (7, 5.0), (8, 6.0), (9, 7.0), (10, 8.0), (11, 9.0)))\n\n        post2 = sf.Series(range(12)).iter_window_array(\n                size=5,\n                start_shift=0,\n                window_sized=True).apply(np.mean)\n\n        self.assertEqual(post2.to_pairs(),\n                ((4, 2.0), (5, 3.0), (6, 4.0), (7, 5.0), (8, 6.0), (9, 7.0), (10, 8.0), (11, 9.0)))\n\n\n    #---------------------------------------------------------------------------\n    def test_series_bool_a(self) -> None:\n        s1 = Series(range(1, 21), index=self.get_letters(20))\n        self.assertTrue(bool(s1))\n\n        s2 = Series(())\n        self.assertFalse(bool(s2))\n\n    #---------------------------------------------------------------------------\n    def test_series_round_a(self) -> None:\n        s1 = Series(np.arange(8) + .001)\n        s2 = round(s1) #type: ignore\n\n        self.assertEqual(s2.to_pairs(),\n                ((0, 0.0), (1, 1.0), (2, 2.0), (3, 3.0), (4, 4.0), (5, 5.0), (6, 6.0), (7, 7.0)))\n\n\n    #---------------------------------------------------------------------------\n    def test_series_str_capitalize_a(self) -> None:\n        s1 = Series((\'foo\', \'bar\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.capitalize()\n\n        self.assertEqual(s2.to_pairs(),\n            ((\'x\', \'Foo\'), (\'y\', \'Bar\'))\n            )\n\n        s3 = Series((20, 30), index=(\'x\', \'y\'))\n        s4 = s3.via_str.capitalize()\n\n        self.assertEqual(s4.to_pairs(),\n            ((\'x\', \'20\'), (\'y\', \'30\'))\n            )\n\n    def test_series_str_center_a(self) -> None:\n        s1 = Series((\'foo\', \'bar\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.center(9, \'-\')\n\n        self.assertEqual(s2.to_pairs(),\n            ((\'x\', \'---foo---\'), (\'y\', \'---bar---\'))\n            )\n\n        s3 = Series((20, 30), index=(\'x\', \'y\'))\n        s4 = s3.via_str.center(4)\n\n        self.assertEqual(s4.to_pairs(),\n            ((\'x\', \' 20 \'), (\'y\', \' 30 \'))\n            )\n\n    def test_series_str_encode_a(self) -> None:\n        s1 = Series((\'foo\', \'bar\'), index=(\'x\', \'y\'))\n\n        s2 = s1.via_str.encode(\'ascii\')\n\n        self.assertEqual(s2.to_pairs(),\n            ((\'x\', b\'foo\'), (\'y\', b\'bar\'))\n            )\n\n    def test_series_str_decode_a(self) -> None:\n        s1 = Series((b\'foo\', b\'bar\'), index=(\'x\', \'y\'))\n\n        s2 = s1.via_str.decode(\'utf-8\')\n        self.assertEqual(s2.to_pairs(),\n            ((\'x\', \'foo\'), (\'y\', \'bar\'))\n            )\n\n    def test_series_str_ljust_a(self) -> None:\n        s1 = Series((\'foo\', \'bar\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.ljust(9, \'-\')\n\n        self.assertEqual(s2.to_pairs(),\n            ((\'x\', \'foo------\'), (\'y\', \'bar------\'))\n            )\n\n        s3 = Series((20, 30), index=(\'x\', \'y\'))\n        s4 = s3.via_str.ljust(4)\n\n        self.assertEqual(s4.to_pairs(),\n            ((\'x\', \'20  \'), (\'y\', \'30  \'))\n            )\n\n\n    def test_series_str_replace_a(self) -> None:\n        s1 = Series((\'*foo*\', \'*bar*\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.replace(\'*\', \'!\')\n\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', \'!foo!\'), (\'y\', \'!bar!\')))\n\n\n    def test_series_str_rjust_a(self) -> None:\n        s1 = Series((\'foo\', \'bar\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.rjust(9, \'-\')\n\n        self.assertEqual(s2.to_pairs(),\n            ((\'x\', \'------foo\'), (\'y\', \'------bar\'))\n            )\n\n        s3 = Series((20, 30), index=(\'x\', \'y\'))\n        s4 = s3.via_str.rjust(4)\n\n        self.assertEqual(s4.to_pairs(),\n            ((\'x\', \'  20\'), (\'y\', \'  30\'))\n            )\n\n\n    def test_series_str_rsplit_a(self) -> None:\n        s1 = Series((\'f*oo\', \'b*ar\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.rsplit(\'*\')\n\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', (\'f\', \'oo\')), (\'y\', (\'b\', \'ar\'))))\n\n\n    def test_series_str_rstrip_a(self) -> None:\n        s1 = Series((\' foo  \', \' bar  \'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.rstrip()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', \' foo\'), (\'y\', \' bar\')))\n\n\n    def test_series_str_split_a(self) -> None:\n        s1 = Series((\'f*oo\', \'b*ar\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.split(\'*\')\n\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', (\'f\', \'oo\')), (\'y\', (\'b\', \'ar\'))))\n\n    def test_series_str_strip_a(self) -> None:\n        s1 = Series((\'*foo*\', \'*bar*\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.strip(\'*\')\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', \'foo\'), (\'y\', \'bar\')))\n\n\n    def test_series_str_swapcase_a(self) -> None:\n        s1 = Series((\'fOO\', \'bAR\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.swapcase()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', \'Foo\'), (\'y\', \'Bar\')))\n\n    def test_series_str_title_a(self) -> None:\n        s1 = Series((\'fOO\', \'bAR\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.title()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', \'Foo\'), (\'y\', \'Bar\')))\n\n    def test_series_str_upper_a(self) -> None:\n        s1 = Series((\'fOO\', \'bAR\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.upper()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', \'FOO\'), (\'y\', \'BAR\')))\n\n\n    def test_series_str_zfill_a(self) -> None:\n        s1 = Series((\'3\', \'40\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.zfill(4)\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', \'0003\'), (\'y\', \'0040\')))\n\n    #---------------------------------------------------------------------------\n    def test_series_str_count_a(self) -> None:\n        s1 = Series((\'foo\', \'foo foo bar\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.count(\'foo\')\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', 1), (\'y\', 2)))\n\n    def test_series_str_endswith_a(self) -> None:\n        s1 = Series((\'foo\', \'foo foo bar\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.endswith(\'bar\')\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', False), (\'y\', True)))\n\n    def test_series_str_startswith_a(self) -> None:\n        s1 = Series((\'foo\', \'foo foo bar\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.startswith(\'foo\')\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', True), (\'y\', True)))\n\n    def test_series_str_find_a(self) -> None:\n        s1 = Series((\'foo\', \'bar foo bar\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.find(\'oo\')\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', 1), (\'y\', 5)))\n\n\n    def test_series_str_index_a(self) -> None:\n        s1 = Series((\'foo\', \'bar foo bar\'), index=(\'x\', \'y\'))\n        with self.assertRaises(ValueError):\n            _ = s1.via_str.index(\'aaa\')\n        s2 = s1.via_str.index(\'oo\')\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', 1), (\'y\', 5)))\n\n\n    def test_series_str_isalnum_a(self) -> None:\n        s1 = Series((\'foo\', \'3234\', \'@#$\'), index=(\'x\', \'y\', \'z\'))\n        s2 = s1.via_str.isalnum()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', True), (\'y\', True), (\'z\', False)))\n\n    def test_series_str_isalpha_a(self) -> None:\n        s1 = Series((\'foo\', \'3234\', \'@#$\'), index=(\'x\', \'y\', \'z\'))\n        s2 = s1.via_str.isalpha()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', True), (\'y\', False), (\'z\', False)))\n\n    def test_series_str_isdecimal_a(self) -> None:\n        s1 = Series((\'foo\', \'3234\', \'@#$\'), index=(\'x\', \'y\', \'z\'))\n        s2 = s1.via_str.isdecimal()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', False), (\'y\', True), (\'z\', False)))\n\n    def test_series_str_isdigit_a(self) -> None:\n        s1 = Series((\'foo\', \'3234\', \'@#$\'), index=(\'x\', \'y\', \'z\'))\n        s2 = s1.via_str.isdigit()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', False), (\'y\', True), (\'z\', False)))\n\n    def test_series_str_islower_a(self) -> None:\n        s1 = Series((\'foo\', \'3234\', \'AAA\'), index=(\'x\', \'y\', \'z\'))\n        s2 = s1.via_str.islower()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', True), (\'y\', False), (\'z\', False)))\n\n    def test_series_str_isnumeric_a(self) -> None:\n        s1 = Series((\'foo\', \'3234\', \'AAA\'), index=(\'x\', \'y\', \'z\'))\n        s2 = s1.via_str.isnumeric()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', False), (\'y\', True), (\'z\', False)))\n\n    def test_series_str_isspace_a(self) -> None:\n        s1 = Series((\'foo\', \'   \', \'AAA\'), index=(\'x\', \'y\', \'z\'))\n        s2 = s1.via_str.isspace()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', False), (\'y\', True), (\'z\', False)))\n\n    def test_series_str_istitle_a(self) -> None:\n        s1 = Series((\'foo\', \'   \', \'Aaa\'), index=(\'x\', \'y\', \'z\'))\n        s2 = s1.via_str.istitle()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', False), (\'y\', False), (\'z\', True)))\n\n    def test_series_str_isupper_a(self) -> None:\n        s1 = Series((\'foo\', \'   \', \'AAA\'), index=(\'x\', \'y\', \'z\'))\n        s2 = s1.via_str.isupper()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', False), (\'y\', False), (\'z\', True)))\n\n\n    def test_series_str_rfind_a(self) -> None:\n        s1 = Series((\'foo\', \'bar foo bar\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.rfind(\'oo\')\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', 1), (\'y\', 5)))\n\n\n    def test_series_str_rindex_a(self) -> None:\n        s1 = Series((\'foo\', \'bar foo bar\'), index=(\'x\', \'y\'))\n        with self.assertRaises(ValueError):\n            _ = s1.via_str.rindex(\'aaa\')\n        s2 = s1.via_str.rindex(\'oo\')\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', 1), (\'y\', 5)))\n\n    def test_series_str_lower_a(self) -> None:\n        s1 = Series((\'foO\', \'AAA\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.lower()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', \'foo\'), (\'y\', \'aaa\')))\n\n    def test_series_str_lstrip_a(self) -> None:\n        s1 = Series((\'  foo\', \' aaa\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.lstrip()\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', \'foo\'), (\'y\', \'aaa\')))\n\n    def test_series_str_partition_a(self) -> None:\n        s1 = Series((\'f*oo\', \'b*ar\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.partition(\'*\')\n\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', (\'f\', \'*\', \'oo\')), (\'y\', (\'b\', \'*\', \'ar\'))))\n\n    def test_series_str_rpartition_a(self) -> None:\n        s1 = Series((\'f*o*o\', \'b*a*r\'), index=(\'x\', \'y\'))\n        s2 = s1.via_str.rpartition(\'*\')\n\n        self.assertEqual(s2.to_pairs(),\n                ((\'x\', (\'f*o\', \'*\', \'o\')), (\'y\', (\'b*a\', \'*\', \'r\'))))\n\n        # import ipdb; ipdb.set_trace()\n    #---------------------------------------------------------------------------\n    def test_series_as_dt_year_a(self) -> None:\n        dt64 = np.datetime64\n\n        s1 = Series((\'2014\', \'2013\'), index=(\'x\', \'y\'))\n\n        with self.assertRaises(RuntimeError):\n            _ = s1.via_dt.year\n\n        s2 = Series((dt64(\'2014-02\'), dt64(\'2013-11\')), index=(\'x\', \'y\')).via_dt.year\n\n        self.assertEqual(\n                s2.to_pairs(),\n                ((\'x\', 2014), (\'y\', 2013))\n                )\n\n\n    def test_series_as_dt_day_a(self) -> None:\n        dt64 = np.datetime64\n\n        s1 = Series((\'2014\', \'2013\'), index=(\'x\', \'y\'))\n\n        with self.assertRaises(RuntimeError):\n            _ = s1.via_dt.day\n\n        s2 = Series((dt64(\'2014-02\'), dt64(\'2013-11\')), index=(\'x\', \'y\'))\n\n        with self.assertRaises(RuntimeError):\n            _ = s2.via_dt.day\n\n        s3 = Series((dt64(\'2014-02-12\'), dt64(\'2013-11-28\')), index=(\'x\', \'y\'))\n\n        post = s3.via_dt.day\n        self.assertEqual(post.to_pairs(),\n                ((\'x\', 12), (\'y\', 28))\n                )\n\n        def todt(date_str: str) -> datetime.date:\n            return datetime.date(*(int(x) for x in date_str.split(\'-\')))\n\n        s4 = Series((todt(\'2014-02-12\'), todt(\'2013-11-28\')), index=(\'x\', \'y\'))\n\n        post = s4.via_dt.day\n        self.assertEqual(post.to_pairs(),\n                ((\'x\', 12), (\'y\', 28))\n                )\n\n\n    def test_series_as_dt_isoformat_a(self) -> None:\n\n        s1 = Series((\'2014-01-02T05:02\', \'2013-02-05T16:55\'),\n                index=(\'x\', \'y\'),\n                dtype=np.datetime64\n                )\n        post = s1.via_dt.isoformat(\'*\')\n        self.assertEqual(post.to_pairs(),\n                ((\'x\', \'2014-01-02*05:02:00\'), (\'y\', \'2013-02-05*16:55:00\'))\n                )\n\n    def test_series_as_dt_weekday_a(self) -> None:\n\n        s1 = Series((\'2014-01-02T05:02\', \'2013-02-05T16:55\'),\n                index=(\'x\', \'y\'),\n                dtype=\'datetime64[ns]\'\n                )\n        self.assertEqual(s1.via_dt.weekday().to_pairs(),\n                ((\'x\', 3), (\'y\', 1)))\n\n        # we do not permit nanosecond to got microsecond\n        with self.assertRaises(RuntimeError):\n            s1.via_dt.isoformat()\n\n\n\n    #---------------------------------------------------------------------------\n\n    def test_series_equals_a(self) -> None:\n\n        s1 = Series(range(1, 21), index=self.get_letters(20), dtype=np.int64)\n        s2 = Series(range(1, 21), index=self.get_letters(20), dtype=np.int64)\n        s3 = Series(range(1, 21), index=self.get_letters(20), dtype=np.int64, name=\'foo\')\n        s4 = Series(range(1, 21), index=self.get_letters(20), dtype=np.int32)\n        s5 = Series(range(0, 20), index=self.get_letters(20), dtype=np.int64)\n        s6 = Series(range(1, 21), dtype=np.int64)\n\n        self.assertTrue(s1.equals(s1))\n        self.assertTrue(s1.equals(s2, compare_class=True))\n        self.assertTrue(s1.equals(s2, compare_class=False))\n\n        self.assertFalse(s1.equals(s3, compare_name=True))\n        self.assertTrue(s1.equals(s3, compare_name=False))\n\n        self.assertFalse(s1.equals(s4, compare_dtype=True))\n        self.assertTrue(s1.equals(s4, compare_dtype=False))\n\n        self.assertFalse(s1.equals(s5))\n        self.assertFalse(s1.equals(s6))\n\n    def test_series_equals_b(self) -> None:\n\n        ih1 = IndexHierarchy.from_product((\'a\', \'b\'), range(10))\n        ih2 = IndexHierarchy.from_product((\'a\', \'b\'), range(10))\n        ih3 = IndexHierarchy.from_product((\'a\', \'c\'), range(10))\n\n        s1 = Series(range(1, 21), index=ih1)\n        s2 = Series(range(1, 21), index=ih2)\n        s3 = Series(range(1, 21), index=ih3)\n\n        self.assertTrue(s1.equals(s2))\n        self.assertFalse(s1.equals(s3))\n\n\n    def test_series_equals_c(self) -> None:\n\n        s1 = Series((1, 2, 5, np.nan), index=self.get_letters(4))\n        s2 = Series((1, 2, 5, np.nan), index=self.get_letters(4))\n\n        self.assertTrue(s1.equals(s2))\n        self.assertFalse(s1.equals(s2, skipna=False))\n\n    def test_series_equals_d(self) -> None:\n\n        s1 = Series((1, 2, 5), index=(\'a\', \'b\', np.nan))\n        s2 = Series((1, 2, 5), index=(\'a\', \'b\', np.nan))\n\n        self.assertTrue(s1.equals(s2))\n        self.assertFalse(s1.equals(s2, skipna=False))\n\n\n    def test_series_equals_e(self) -> None:\n\n        s1 = Series((1, 2, 5), index=(\'a\', \'b\', \'c\'))\n        s2 = Series((\'1\', \'2\', \'5\'), index=(\'a\', \'b\', \'c\'))\n\n        self.assertFalse(s1.equals(s2, compare_dtype=False))\n\n    def test_series_equals_f(self) -> None:\n\n        s1 = Series((1, None, 5), index=(\'a\', \'b\', \'c\'))\n        s2 = Series((1, np.nan, 5), index=(\'a\', \'b\', \'c\'))\n\n        self.assertFalse(s1.equals(s2, compare_dtype=False))\n\n    #---------------------------------------------------------------------------\n    def test_series_enum_a(self) -> None:\n\n        class Bar(str, Enum):\n            a = \'a\'\n            b = \'b\'\n            c = \'c\'\n\n        s1 = sf.Series([Bar.a, Bar.b, Bar.c])\n\n        self.assertEqual(s1.values.tolist(), [Bar.a, Bar.b, Bar.c])\n        self.assertEqual(s1[1], Bar.b)\n        self.assertEqual(s1[1:].values.tolist(), [Bar.b, Bar.c])\n\n\n    def test_series_enum_b(self) -> None:\n\n        class Bar(str, Enum):\n            a = \'a\'\n            b = \'b\'\n            c = \'c\'\n\n        s1 = sf.Series(Bar, index=Bar)\n\n        self.assertEqual(\n                s1[Bar.b:].to_pairs(), #type: ignore\n                ((Bar.b, Bar.b), (Bar.c, Bar.c))\n                )\n\n    #---------------------------------------------------------------------------\n\n    def test_series_insert_a(self) -> None:\n\n        s1 = Series((1, None, 5), index=(\'a\', \'b\', \'c\'))\n        s2 = Series((1, 3.4, 5), index=(\'d\', \'e\', \'f\'))\n\n        s3 = s1._insert(1, s2)\n\n        self.assertEqual(s3.to_pairs(),\n                ((\'a\', 1), (\'d\', 1.0), (\'e\', 3.4), (\'f\', 5.0), (\'b\', None), (\'c\', 5))\n                )\n\n        s4 = s1._insert(3, s2)\n\n        self.assertEqual(s4.to_pairs(),\n                ((\'a\', 1), (\'b\', None), (\'c\', 5), (\'d\', 1.0), (\'e\', 3.4), (\'f\', 5.0))\n                )\n\n    def test_series_insert_b(self) -> None:\n\n        s1 = Series((1, None, 5), index=(\'a\', \'b\', \'c\'))\n        s2 = Series((1, 3.4, 5), index=(\'d\', \'e\', \'f\'))\n\n        s3 = s1.insert_before(\'c\', s2)\n        self.assertEqual(s3.to_pairs(),\n                ((\'a\', 1), (\'b\', None), (\'d\', 1.0), (\'e\', 3.4), (\'f\', 5.0), (\'c\', 5))\n                )\n\n        s4 = s1.insert_after(\'c\', s2)\n        self.assertEqual(s4.to_pairs(),\n                ((\'a\', 1), (\'b\', None), (\'c\', 5), (\'d\', 1.0), (\'e\', 3.4), (\'f\', 5.0))\n                )\n\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
static_frame/test/unit/test_store.py,2,"b""import unittest\n# from io import StringIO\nimport numpy as np\n\nfrom static_frame.core.store import Store\nfrom static_frame.core.store import StoreConfig\nfrom static_frame.core.store import StoreConfigMap\nfrom static_frame.core.frame import Frame\n\nfrom static_frame.test.test_case import TestCase\n# from static_frame.test.test_case import temp_file\nfrom static_frame.core.exception import ErrorInitStoreConfig\n\n\n\nclass TestUnit(TestCase):\n\n\n    #---------------------------------------------------------------------------\n\n    def test_store_config_map_a(self) -> None:\n\n        sc1 = StoreConfig(index_depth=3, columns_depth=3)\n        sc1m = StoreConfigMap.from_config(sc1)\n        self.assertEqual(sc1m['a'].index_depth, 3)\n        self.assertEqual(sc1m['b'].index_depth, 3)\n\n        sc2 = StoreConfig(include_index=False)\n        sc2m = StoreConfigMap.from_config(sc2)\n        self.assertEqual(sc2m['a'].include_index, False)\n        self.assertEqual(sc2m['b'].include_index, False)\n\n\n\n\n    def test_store_config_map_b(self) -> None:\n\n        maps = {'a': StoreConfig(index_depth=2),\n                'b': StoreConfig(index_depth=3)}\n        sc1m = StoreConfigMap(maps)\n        self.assertEqual(sc1m['a'].index_depth, 2)\n        self.assertEqual(sc1m['b'].index_depth, 3)\n        self.assertEqual(sc1m['c'].index_depth, 0)\n\n    def test_store_config_map_c(self) -> None:\n        sc1 = StoreConfig(index_depth=3, columns_depth=3)\n        maps = {'a': StoreConfig(index_depth=2),\n                'b': StoreConfig(index_depth=3)}\n        sc1m = StoreConfigMap(maps)\n\n        sc2m = StoreConfigMap.from_initializer(sc1)\n        self.assertEqual(sc2m['a'].index_depth, 3)\n\n        sc3m = StoreConfigMap.from_initializer(sc1m)\n        self.assertEqual(sc3m['a'].index_depth, 2)\n        self.assertEqual(sc3m['b'].index_depth, 3)\n\n        sc4m = StoreConfigMap.from_initializer(maps)\n        self.assertEqual(sc4m['a'].index_depth, 2)\n        self.assertEqual(sc4m['b'].index_depth, 3)\n\n\n    def test_store_config_map_d(self) -> None:\n        with self.assertRaises(ErrorInitStoreConfig):\n            _ = StoreConfigMap({'a': object()}) #type: ignore\n\n        with self.assertRaises(ErrorInitStoreConfig):\n            _ = StoreConfigMap(default=object()) #type: ignore\n\n\n    def test_store_get_field_names_and_dtypes_a(self) -> None:\n\n        f1 = Frame.from_records((('a', True, None),), index=(('a',)), columns=(('x', 'y', 'z')))\n\n        field_names, dtypes = Store.get_field_names_and_dtypes(frame=f1,\n                include_index=False,\n                include_columns=False,\n                )\n        self.assertEqual(field_names, range(0, 3))\n        self.assertEqual(dtypes.tolist(), #type: ignore\n                [np.dtype('<U1'), np.dtype('bool'), np.dtype('O')])\n\n    def test_store_get_field_names_and_dtypes_b(self) -> None:\n\n        f1 = Frame.from_records((('a', True, None),), index=(('a',)), columns=(('x', 'y', 'z')))\n\n        field_names, dtypes = Store.get_field_names_and_dtypes(frame=f1,\n                include_index=False,\n                include_columns=True)\n\n        self.assertEqual(field_names.tolist(), ['x', 'y', 'z']) #type: ignore\n        self.assertEqual(dtypes.tolist(), #type: ignore\n                [np.dtype('<U1'), np.dtype('bool'), np.dtype('O')])\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/unit/test_store_filter.py,16,"b""import unittest\n# from io import StringIO\nimport numpy as np\nfrom io import StringIO\n\nfrom static_frame.core.store_filter import STORE_FILTER_DEFAULT\nfrom static_frame.core.store_filter import STORE_FILTER_DISABLE\nfrom static_frame.core.store_filter import StoreFilter\nfrom static_frame.test.test_case import TestCase\n\nfrom static_frame.core.frame import Frame\n# from static_frame.test.test_case import temp_file\n\nclass TestUnit(TestCase):\n\n    def test_store_from_type_filter_array_a(self) -> None:\n\n        a1 = np.array([1, 2, np.nan, np.inf, -np.inf], dtype=float)\n        a2 = np.array([1, 2, np.nan, np.inf, -np.inf], dtype=object)\n\n        sfd = STORE_FILTER_DEFAULT\n\n        self.assertEqual(sfd.from_type_filter_array(a1).tolist(),\n                [1.0, 2.0, '', 'inf', '-inf'])\n\n        self.assertEqual(sfd.from_type_filter_array(a2).tolist(),\n                [1.0, 2.0, '', 'inf', '-inf'])\n\n    def test_store_from_type_filter_array_b(self) -> None:\n\n        a1 = np.array([False, True, False], dtype=bool)\n        a2 = np.array([False, True, False], dtype=object)\n\n        sfd = STORE_FILTER_DEFAULT\n\n        self.assertEqual(sfd.from_type_filter_array(a1).tolist(),\n                [False, True, False])\n        self.assertEqual(sfd.from_type_filter_array(a2).tolist(),\n                [False, True, False])\n\n\n    def test_store_from_type_filter_array_c(self) -> None:\n\n        a1 = np.array([1, 20, 1], dtype=int)\n        a2 = np.array([1, 20, 1], dtype=object)\n\n        sfd = STORE_FILTER_DEFAULT\n\n        self.assertEqual(sfd.from_type_filter_array(a1).tolist(),\n                [1, 20, 1])\n        self.assertEqual(sfd.from_type_filter_array(a2).tolist(),\n                [1, 20, 1])\n\n\n    def test_store_from_type_filter_array_d(self) -> None:\n\n        a1 = np.array([1, None, np.nan, -np.inf, np.inf], dtype=object)\n\n        sfd = STORE_FILTER_DEFAULT\n\n        self.assertEqual(sfd.from_type_filter_array(a1).tolist(),\n            [1, 'None', '', '-inf', 'inf'])\n\n\n    def test_store_from_type_filter_array_e(self) -> None:\n\n        a1 = np.array([1, None, np.nan, -np.inf, np.inf], dtype=object)\n\n        sfd = STORE_FILTER_DISABLE\n\n        self.assertAlmostEqualValues(\n            sfd.from_type_filter_array(a1).tolist(),\n            [1, None, np.nan, -np.inf, np.inf])\n\n\n\n\n    def test_store_from_type_filter_element_a(self) -> None:\n        sfd = STORE_FILTER_DEFAULT\n\n        self.assertEqual(sfd.from_type_filter_element(None), 'None')\n        self.assertEqual(sfd.from_type_filter_element(np.nan), '')\n\n\n    def test_store_to_type_filter_element_a(self) -> None:\n        sfd = STORE_FILTER_DEFAULT\n\n        self.assertTrue(np.isnan(sfd.to_type_filter_element('nan')))\n        self.assertTrue(np.isposinf(sfd.to_type_filter_element('inf')))\n        self.assertTrue(np.isneginf(sfd.to_type_filter_element('-inf')))\n        self.assertEqual(sfd.to_type_filter_element('None'), None)\n\n    def test_store_to_type_filter_element_b(self) -> None:\n        sfd = STORE_FILTER_DISABLE\n\n        self.assertEqual(sfd.to_type_filter_element('nan'), 'nan')\n        self.assertEqual(sfd.to_type_filter_element('inf'), 'inf')\n        self.assertEqual(sfd.to_type_filter_element('-inf'), '-inf')\n        self.assertEqual(sfd.to_type_filter_element('None'), 'None')\n\n\n\n    def test_store_to_type_filter_array_a(self) -> None:\n        sfd = STORE_FILTER_DEFAULT\n        a1 = np.array([1, None, 'nan', '', 'inf'], dtype=object)\n        post = sfd.to_type_filter_array(a1)\n        self.assertAlmostEqualValues(post.tolist(), [1, None, np.nan, np.nan, np.inf])\n\n\n\n    def test_store_filter_to_delimited_a(self) -> None:\n        f = Frame.from_records(((None, np.inf), (np.nan, -np.inf)))\n        store_filter = StoreFilter(from_nan='*', from_none='!', from_posinf='&', from_neginf='@')\n        post = StringIO()\n        f.to_csv(post, store_filter=store_filter, include_index=False)\n        post.seek(0)\n        self.assertEqual(post.read(), '0,1\\n!,&\\n*,@')\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/unit/test_store_hdf5.py,0,"b""\nimport unittest\n\n# import numpy as np\n\n\nfrom static_frame.core.frame import Frame\nfrom static_frame.core.index_hierarchy import IndexHierarchy\n# from static_frame.core.hloc import HLoc\n# from static_frame.core.series import Series\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.test.test_case import temp_file\n\n\nfrom static_frame.core.store_hdf5 import StoreHDF5\nfrom static_frame.core.store import StoreConfigMap\n\n\nclass TestUnit(TestCase):\n\n\n    def test_store_hdf5_write_a(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(x=(1,2,-5,200), y=(3,4,-5,-3000)),\n                index=IndexHierarchy.from_product(('I', 'II'), ('a', 'b')),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(a=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_records(\n                ((10, 20, 50, 60), (50.0, 60.4, -50, -60)),\n                index=('p', 'q'),\n                columns=IndexHierarchy.from_product(('I', 'II'), ('a', 'b')),\n                name='f3')\n        f4 = Frame.from_records((\n                (10, 20, 50, False, 10, 20, 50, False),\n                (50.0, 60.4, -50, True, 50.0, 60.4, -50, True),\n                (234, 44452, 0, False, 234, 44452, 0, False),\n                (4, -4, 2000, True, 4, -4, 2000, True),\n                (10, 20, 50, False, 10, 20, 50, False),\n                (50.0, 60.4, -50, True, 50.0, 60.4, -50, True),\n                (234, 44452, 0, False, 234, 44452, 0, False),\n                (4, -4, 2000, True, 4, -4, 2000, True),\n                ),\n                index=IndexHierarchy.from_product(('top', 'bottom'), ('far', 'near'), ('left', 'right')),\n                columns=IndexHierarchy.from_product(('I', 'II'), ('a', 'b'), (1, 2)),\n                name='f4')\n\n        frames = (f1, f2, f3, f4)\n        config = StoreConfigMap.from_frames(frames)\n\n        with temp_file('.hdf5') as fp:\n\n            st1 = StoreHDF5(fp)\n            st1.write(((f.name, f) for f in frames), config=config)\n\n            labels = tuple(st1.labels()) # this will read from file, not in memory\n            self.assertEqual(tuple(f.name for f in frames), labels)\n\n            for i, name in enumerate(labels):\n                f_src = frames[i]\n                c = config[f_src.name]\n                f_loaded = st1.read(name, config=c)\n                self.assertEqualFrames(f_src, f_loaded)\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n\n"""
static_frame/test/unit/test_store_sqlite.py,2,"b""import unittest\nfrom fractions import Fraction\n\nimport numpy as np\n\nfrom static_frame.core.frame import Frame\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.test.test_case import temp_file\nfrom static_frame.core.index_hierarchy import IndexHierarchy\nfrom static_frame.core.store import StoreConfig\nfrom static_frame.core.store_sqlite import StoreSQLite\n\n\nclass TestUnit(TestCase):\n\n    def test_store_sqlite_write_a(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(x=(None,-np.inf,np.inf,None), y=(3,4,-5,-3000)),\n                index=IndexHierarchy.from_product(('I', 'II'), ('a', 'b')),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(a=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_records(\n                ((10.4, 20.1, 50, 60), (50.1, 60.4, -50, -60)),\n                index=('p', 'q'),\n                columns=IndexHierarchy.from_product(('I', 'II'), ('a', 'b')),\n                name='f3')\n        f4 = Frame.from_records((\n                (10, 20, 50, False, 10, 20, 50, False),\n                (50.0, 60.4, -50, True, 50.0, 60.4, -50, True),\n                (234, 44452, 0, False, 234, 44452, 0, False),\n                (4, -4, 2000, True, 4, -4, 2000, True),\n                (10, 20, 50, False, 10, 20, 50, False),\n                (50.0, 60.4, -50, True, 50.0, 60.4, -50, True),\n                (234, 44452, 0, False, 234, 44452, 0, False),\n                (4, -4, 2000, True, 4, -4, 2000, True),\n                ),\n                index=IndexHierarchy.from_product(('top', 'bottom'), ('far', 'near'), ('left', 'right')),\n                columns=IndexHierarchy.from_product(('I', 'II'), ('a', 'b'), (1, 2)),\n                name='f4')\n\n        frames = (f1, f2, f3, f4)\n\n        with temp_file('.sqlite') as fp:\n\n            st1 = StoreSQLite(fp)\n            st1.write((f.name, f) for f in frames)\n\n            sheet_names = tuple(st1.labels()) # this will read from file, not in memory\n            self.assertEqual(tuple(f.name for f in frames), sheet_names)\n\n            for i, name in enumerate(sheet_names):\n                f_src = frames[i]\n                config = StoreConfig.from_frame(f_src)\n                f_loaded = st1.read(name, config=config)\n                self.assertEqualFrames(f_src, f_loaded)\n\n\n\n    def test_store_sqlite_write_b(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(\n                        x=(Fraction(3,2), Fraction(1,2), Fraction(2,3), Fraction(3,7)),\n                        y=(3,4,-5,-3000)),\n                index=IndexHierarchy.from_product(('I', 'II'), ('a', 'b')),\n                name='f1')\n\n        frames = (f1,)\n\n        with temp_file('.sqlite') as fp:\n\n            st1 = StoreSQLite(fp)\n            st1.write((f.name, f) for f in frames)\n\n            config = StoreConfig.from_frame(f1)\n\n            f_loaded = st1.read(f1.name, config=config)\n\n            # for now, Fractions come back as strings\n            self.assertEqual(\n                    f_loaded['x'].to_pairs(),\n                    ((('I', 'a'), '3/2'), (('I', 'b'), '1/2'), (('II', 'a'), '2/3'), (('II', 'b'), '3/7'))\n            )\n\n\n\n    def test_store_sqlite_write_c(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(\n                        x=np.array([1.2, 4.5, 3.2, 6.5], dtype=np.float16),\n                        y=(3,4,-5,-3000)),\n                index=IndexHierarchy.from_product(('I', 'II'), ('a', 'b')),\n                name='f1')\n\n        frames = (f1,)\n\n        with temp_file('.sqlite') as fp:\n            st1 = StoreSQLite(fp)\n            st1.write((f.name, f) for f in frames)\n\n            config = StoreConfig.from_frame(f1)\n\n            f_loaded = st1.read(f1.name, config=config)\n\n            self.assertAlmostEqualItems(f_loaded['x'].to_pairs(),\n                    ((('I', 'a'), 1.2001953125), (('I', 'b'), 4.5), (('II', 'a'), 3.19921875), (('II', 'b'), 6.5))\n                    )\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n\n\n\n"""
static_frame/test/unit/test_store_xlsx.py,1,"b""\nimport unittest\n\nimport numpy as np\n\n\nfrom static_frame.core.frame import Frame\nfrom static_frame.core.index_hierarchy import IndexHierarchy\nfrom static_frame.core.hloc import HLoc\n# from static_frame.core.series import Series\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.test.test_case import temp_file\n\n# from static_frame.test.test_case import skip_win\n# from static_frame.core.exception import ErrorInitStore\n\nfrom static_frame.core.store_xlsx import StoreXLSX\nfrom static_frame.core.store import StoreConfig\nfrom static_frame.core.store import StoreConfigMap\n\n\nclass TestUnit(TestCase):\n\n\n    def test_store_xlsx_write_a(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(x=(1,2,-5,200), y=(3,4,-5,-3000)),\n                index=IndexHierarchy.from_product(('I', 'II'), ('a', 'b')),\n                name='f1')\n        f2 = Frame.from_dict(\n                dict(a=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='f2')\n        f3 = Frame.from_records(\n                ((10, 20, 50, 60), (50.0, 60.4, -50, -60)),\n                index=('p', 'q'),\n                columns=IndexHierarchy.from_product(('I', 'II'), ('a', 'b')),\n                name='f3')\n        f4 = Frame.from_records((\n                (10, 20, 50, False, 10, 20, 50, False),\n                (50.0, 60.4, -50, True, 50.0, 60.4, -50, True),\n                (234, 44452, 0, False, 234, 44452, 0, False),\n                (4, -4, 2000, True, 4, -4, 2000, True),\n                (10, 20, 50, False, 10, 20, 50, False),\n                (50.0, 60.4, -50, True, 50.0, 60.4, -50, True),\n                (234, 44452, 0, False, 234, 44452, 0, False),\n                (4, -4, 2000, True, 4, -4, 2000, True),\n                ),\n                index=IndexHierarchy.from_product(('top', 'bottom'), ('far', 'near'), ('left', 'right')),\n                columns=IndexHierarchy.from_product(('I', 'II'), ('a', 'b'), (1, 2)),\n                name='f4')\n\n        frames = (f1, f2, f3, f4)\n        config_map = StoreConfigMap.from_config(\n                StoreConfig(include_index=True, include_columns=True))\n\n        with temp_file('.xlsx') as fp:\n\n            st1 = StoreXLSX(fp)\n            st1.write(((f.name, f) for f in frames), config=config_map)\n\n            # import ipdb; ipdb.set_trace()\n            sheet_names = tuple(st1.labels()) # this will read from file, not in memory\n            self.assertEqual(tuple(f.name for f in frames), sheet_names)\n\n            for i, name in enumerate(sheet_names):\n                f_src = frames[i]\n                c = StoreConfig(\n                        index_depth=f_src.index.depth,\n                        columns_depth=f_src.columns.depth\n                        )\n                f_loaded = st1.read(name, config=c)\n                self.assertEqualFrames(f_src, f_loaded, compare_dtype=False)\n\n\n\n\n    def test_store_xlsx_write_b(self) -> None:\n\n        f1 = Frame.from_records(\n                ((None, np.nan, 50, 'a'), (None, -np.inf, -50, 'b'), (None, 60.4, -50, 'c')),\n                index=('p', 'q', 'r'),\n                columns=IndexHierarchy.from_product(('I', 'II'), ('a', 'b')),\n                )\n\n        config_map = StoreConfigMap.from_config(\n                StoreConfig(include_index=True, include_columns=True))\n\n        with temp_file('.xlsx') as fp:\n\n            st = StoreXLSX(fp)\n            st.write(((None, f1),), config=config_map)\n\n            c = StoreConfig(\n                    index_depth=f1.index.depth,\n                    columns_depth=f1.columns.depth\n                    )\n            f2 = st.read(None, config=c)\n\n            # just a sample column for now\n            self.assertEqual(\n                    f1[HLoc[('II', 'a')]].values.tolist(),\n                    f2[HLoc[('II', 'a')]].values.tolist() )\n\n            self.assertEqualFrames(f1, f2)\n\n\n    def test_store_xlsx_read_a(self) -> None:\n        f1 = Frame.from_elements([1, 2, 3], index=('a', 'b', 'c'), columns=('x',))\n\n        config_map = StoreConfigMap.from_config(\n                StoreConfig(include_index=False, include_columns=True))\n\n        with temp_file('.xlsx') as fp:\n\n            st = StoreXLSX(fp)\n            st.write(((None, f1),), config=config_map)\n\n            c = StoreConfig(\n                    index_depth=0,\n                    columns_depth=f1.columns.depth\n                    )\n            f2 = st.read(None, config=c)\n\n        self.assertTrue((f1.values == f2.values).all())\n        self.assertEqual(f2.to_pairs(0),\n                (('x', ((0, 1), (1, 2), (2, 3))),)\n                )\n\n\n    def test_store_xlsx_read_b(self) -> None:\n        index = IndexHierarchy.from_product(('left', 'right'), ('up', 'down'))\n        columns = IndexHierarchy.from_labels(((100, -5, 20),))\n\n        f1 = Frame.from_elements([1, 2, 3, 4], index=index, columns=columns)\n\n        config_map = StoreConfigMap.from_config(\n                StoreConfig(include_index=False, include_columns=True))\n\n        with temp_file('.xlsx') as fp:\n\n            st = StoreXLSX(fp)\n            st.write(((None, f1),), config=config_map)\n\n            c = StoreConfig(\n                    index_depth=0,\n                    columns_depth=f1.columns.depth\n                    )\n            f2 = st.read(None, config=c)\n\n        self.assertTrue((f1.values == f2.values).all())\n        self.assertEqual(f2.to_pairs(0),\n                (((100, -5, 20), ((0, 1), (1, 2), (2, 3), (3, 4))),)\n                )\n\n\n    def test_store_xlsx_read_c(self) -> None:\n        index = IndexHierarchy.from_product(('left', 'right'), ('up', 'down'))\n        columns = IndexHierarchy.from_labels(((100, -5, 20),))\n\n        f1 = Frame.from_elements([1, 2, 3, 4], index=index, columns=columns)\n\n        config_map = StoreConfigMap.from_config(\n                StoreConfig(include_index=True, include_columns=False))\n\n        with temp_file('.xlsx') as fp:\n\n            st = StoreXLSX(fp)\n            st.write(((None, f1),), config=config_map[None])\n\n            c = StoreConfig(\n                    index_depth=f1.index.depth,\n                    columns_depth=0\n                    )\n            f2 = st.read(None, config=c)\n\n        self.assertTrue((f1.values == f2.values).all())\n        self.assertEqual(f2.to_pairs(0),\n                ((0, ((('left', 'up'), 1), (('left', 'down'), 2), (('right', 'up'), 3), (('right', 'down'), 4))),)\n                )\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n\n"""
static_frame/test/unit/test_store_zip.py,0,"b""import unittest\n# from io import StringIO\n\nfrom static_frame.core.frame import Frame\nfrom static_frame.core.frame import FrameGO\n# from static_frame.core.bus import Bus\n# from static_frame.core.series import Series\n\nfrom static_frame.core.store import StoreConfig\nfrom static_frame.core.store import StoreConfigMap\n\nfrom static_frame.core.store_zip import StoreZipTSV\nfrom static_frame.core.store_zip import StoreZipCSV\nfrom static_frame.core.store_zip import StoreZipPickle\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.test.test_case import temp_file\n\n# from static_frame.test.test_case import skip_win\nfrom static_frame.core.exception import ErrorInitStore\n# from static_frame.core.exception import ErrorInitStoreConfig\n\n\nclass TestUnit(TestCase):\n\n    #---------------------------------------------------------------------------\n\n    def test_store_init_a(self) -> None:\n        with self.assertRaises(ErrorInitStore):\n            StoreZipTSV('test.txt') # must be a zip\n\n\n    def test_store_zip_tsv_a(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='foo')\n        f2 = Frame.from_dict(\n                dict(a=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='bar')\n        f3 = Frame.from_dict(\n                dict(a=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='baz')\n\n        with temp_file('.zip') as fp:\n\n            st = StoreZipTSV(fp)\n            st.write((f.name, f) for f in (f1, f2, f3))\n\n            labels = tuple(st.labels(strip_ext=False))\n            self.assertEqual(labels, ('foo.txt', 'bar.txt', 'baz.txt'))\n\n            config = StoreConfig(index_depth=1)\n\n            for label, frame in ((f.name, f) for f in (f1, f2, f3)):\n                frame_stored = st.read(label, config=config)\n                self.assertEqual(frame_stored.shape, frame.shape)\n                self.assertTrue((frame_stored == frame).all().all())\n                self.assertEqual(frame.to_pairs(0), frame_stored.to_pairs(0))\n\n                frame_stored_2 = st.read(label, config=config, container_type=FrameGO)\n                self.assertEqual(frame_stored_2.__class__, FrameGO)\n                self.assertEqual(frame_stored_2.shape, frame.shape)\n\n    def test_store_zip_csv_a(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='foo')\n        f2 = Frame.from_dict(\n                dict(a=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='bar')\n        f3 = Frame.from_dict(\n                dict(a=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='baz')\n\n        with temp_file('.zip') as fp:\n\n            st = StoreZipCSV(fp)\n            st.write((f.name, f) for f in (f1, f2, f3))\n\n            labels = tuple(st.labels(strip_ext=False))\n            self.assertEqual(labels, ('foo.csv', 'bar.csv', 'baz.csv'))\n\n            config = StoreConfig(index_depth=1)\n\n            for label, frame in ((f.name, f) for f in (f1, f2, f3)):\n                frame_stored = st.read(label, config=config)\n                self.assertEqual(frame_stored.shape, frame.shape)\n                self.assertTrue((frame_stored == frame).all().all())\n                self.assertEqual(frame.to_pairs(0), frame_stored.to_pairs(0))\n\n\n    def test_store_zip_csv_b(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='foo')\n\n        with temp_file('.zip') as fp:\n\n            st = StoreZipCSV(fp)\n            st.write((f.name, f) for f in (f1,))\n\n            with self.assertRaises(ErrorInitStore):\n                # config is required\n                _ = st.read(f1.name)\n\n\n\n    def test_store_zip_pickle_a(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='foo')\n        f2 = Frame.from_dict(\n                dict(a=(1,2,3), b=(4,5,6)),\n                index=('x', 'y', 'z'),\n                name='bar')\n        f3 = Frame.from_dict(\n                dict(a=(10,20), b=(50,60)),\n                index=('p', 'q'),\n                name='baz')\n\n        with temp_file('.zip') as fp:\n\n            st = StoreZipPickle(fp)\n            st.write((f.name, f) for f in (f1, f2, f3))\n\n            labels = tuple(st.labels(strip_ext=False))\n            self.assertEqual(labels, ('foo.pickle', 'bar.pickle', 'baz.pickle'))\n\n            for label, frame in ((f.name, f) for f in (f1, f2, f3)):\n                frame_stored = st.read(label)\n                self.assertEqual(frame_stored.shape, frame.shape)\n                self.assertTrue((frame_stored == frame).all().all())\n                self.assertEqual(frame.to_pairs(0), frame_stored.to_pairs(0))\n\n                frame_stored_2 = st.read(label, container_type=FrameGO)\n                self.assertEqual(frame_stored_2.__class__, FrameGO)\n                self.assertEqual(frame_stored_2.shape, frame.shape)\n\n\n    def test_store_zip_pickle_b(self) -> None:\n\n        f1 = Frame.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='foo')\n\n        config = StoreConfig(index_depth=1, include_index=True)\n        config_map = StoreConfigMap.from_config(config)\n\n        with temp_file('.zip') as fp:\n\n            st = StoreZipPickle(fp)\n            st.write(((f1.name, f1),))\n\n            frame_stored = st.read(f1.name)\n            self.assertEqual(frame_stored.shape, f1.shape)\n\n    def test_store_zip_pickle_c(self) -> None:\n\n        f1 = FrameGO.from_dict(\n                dict(a=(1,2), b=(3,4)),\n                index=('x', 'y'),\n                name='foo')\n\n        config = StoreConfig(index_depth=1, include_index=True)\n        config_map = StoreConfigMap.from_config(config)\n\n        with temp_file('.zip') as fp:\n\n            st = StoreZipPickle(fp)\n\n            # raise if trying to store a FrameGO\n            with self.assertRaises(NotImplementedError):\n                st.write(((f1.name, f1),))\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/unit/test_type_blocks.py,438,"b""import unittest\nimport pickle\n\nimport numpy as np\n\n\n# assuming located in the same directory\n# from static_frame import Index\n# from static_frame import IndexGO\n# from static_frame import Series\n# from static_frame import Frame\n# from static_frame import FrameGO\nfrom static_frame import TypeBlocks\n# from static_frame import Display\nfrom static_frame import mloc\n# from static_frame import DisplayConfig\n\n\nfrom static_frame.core.util import immutable_filter\nfrom static_frame.core.util import NULL_SLICE\n\nfrom static_frame.core.index_correspondence import IndexCorrespondence\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.test.test_case import skip_win\n\nfrom static_frame.core.exception import ErrorInitTypeBlocks\nfrom static_frame.core.exception import AxisInvalid\n\nnan = np.nan\n\n\nclass TestUnit(TestCase):\n\n    def test_type_blocks_init_a(self) -> None:\n        with self.assertRaises(ErrorInitTypeBlocks):\n            tb1 = TypeBlocks.from_blocks((3, 4))\n        with self.assertRaises(ErrorInitTypeBlocks):\n            tb1 = TypeBlocks.from_blocks((np.arange(8).reshape((2, 2, 2)),))\n\n\n    def test_type_blocks_a(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([10,50,30])\n        a3 = np.array([1345,2234,3345])\n        a4 = np.array([False, True, False])\n        a5 = np.array([False, False, False])\n        a6 = np.array(['g', 'd', 'e'])\n        tb2 = TypeBlocks.from_blocks((a1, a2, a3, a4, a5, a6))\n\n        # can show that with tb2, a6 remains unchanged\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb3 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        # showing that slices keep the same memory location\n        # self.assertTrue((tb1[0:2].mloc == tb1.mloc[:2]).all())\n        # self.assertTrue((tb1.mloc[:2] == tb1.iloc[0:2, 0:2].mloc).all())\n\n    def test_type_blocks_contiguous_pairs(self) -> None:\n\n        a = [(0, 1), (0, 2), (2, 3), (2, 1)]\n        post = list(TypeBlocks._indices_to_contiguous_pairs(a))\n        self.assertEqual(post, [\n                (0, slice(1, 3)),\n                (2, slice(3, 4)),\n                (2, slice(1, 2)),\n                ])\n\n        a = [(0, 0), (0, 1), (0, 2), (1, 4), (2, 1), (2, 3)]\n        post = list(TypeBlocks._indices_to_contiguous_pairs(a))\n        self.assertEqual(post, [\n                (0, slice(0, 3)),\n                (1, slice(4, 5)),\n                (2, slice(1, 2)),\n                (2, slice(3, 4)),\n            ])\n\n\n\n    def test_type_blocks_b(self) -> None:\n\n        # given naturally of a list of rows; this corresponds to what we get with iloc, where we select a row first, then a column\n        a1 = np.array([[1, 2, 3], [4, 5, 6]])\n        # shape is given as rows, columns\n        self.assertEqual(a1.shape, (2, 3))\n\n        a2 = np.array([[.2, .5, .4], [.8, .6, .5]])\n        a3 = np.array([['a', 'b'], ['c', 'd']])\n\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertTypeBlocksArrayEqual(tb1[2], [[3], [6]])\n        self.assertTypeBlocksArrayEqual(tb1[4], [[0.5], [0.6]])\n\n        self.assertEqual(list(tb1[7].values), ['b', 'd'])\n\n        self.assertEqual(tb1.shape, (2, 8))\n\n        self.assertEqual(len(tb1), 2)\n        self.assertEqual(tb1._row_dtype, np.object_)\n\n        slice1 = tb1[2:5]\n        self.assertEqual(slice1.shape, (2, 3))\n\n        slice2 = tb1[0:5]\n        self.assertEqual(slice2.shape, (2, 5))\n\n        # pick columns\n        slice3 = tb1[[2,6,0]]\n        self.assertEqual(slice3.shape, (2, 3))\n\n        # TODO: need to implement values\n\n        self.assertEqual(slice3.iloc[0].values.tolist(), [[3, 'a', 1]])\n        self.assertEqual(slice3.iloc[1].values.tolist(), [[6, 'c', 4]])\n\n        ## slice refers to the same data; not sure if this is accurate test yet\n\n        row1 = tb1.iloc[0].values\n        self.assertEqual(row1.dtype, object)\n        self.assertEqual(row1.shape, (1, 8))\n        self.assertEqual(row1[:, :3].tolist(), [[1, 2, 3]])\n        self.assertEqual(row1[:, -2:].tolist(), [['a', 'b']])\n\n        self.assertEqual(tb1.unified, False)\n\n\n\n    def test_type_blocks_c(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'])\n        a4 = np.array(['gd', 'cd', 'dd'])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3, a4))\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb2 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        row1 = tb1.iloc[2]\n\n        self.assertEqual(tb1.shape, (3, 4))\n\n        self.assertEqual(tb1.iloc[1].values.tolist(), [[2, True, 'c', 'cd']])\n\n        self.assertEqual(tb1.iloc[0, 0:2].shape, (1, 2))\n        self.assertEqual(tb1.iloc[0:2, 0:2].shape, (2, 2))\n\n\n\n    def test_type_blocks_d(self) -> None:\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertEqual(tb1.iloc[0:2].shape, (2, 8))\n        self.assertEqual(tb1.iloc[1:3].shape, (2, 8))\n\n\n\n    def test_type_blocks_indices_to_contiguous_pairs(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n        self.assertEqual(list(tb1._key_to_block_slices(0)), [(0, 0)])\n        self.assertEqual(list(tb1._key_to_block_slices(6)), [(2, 0)])\n        self.assertEqual(list(tb1._key_to_block_slices([3,5,6])),\n            [(1, slice(0, 1, None)), (1, slice(2, 3, None)), (2, slice(0, 1, None))]\n            )\n\n    #---------------------------------------------------------------------------\n\n    def test_type_blocks_key_to_block_slices_a(self) -> None:\n        a1 = np.array([1, 2, -1])\n        a2 = np.array([[False, True], [True, True], [False, True]])\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        self.assertEqual(list(tb1._key_to_block_slices(None)),\n                [(0, slice(0, 1, None)), (1, slice(0, 2, None))]\n                )\n\n        with self.assertRaises(NotImplementedError):\n            list(tb1._key_to_block_slices('a'))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_type_blocks_extract_a(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'])\n        a4 = np.array(['gd', 'cd', 'dd'])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3, a4))\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb2 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        # double point extraction goes to single elements\n        self.assertEqual(tb2._extract(1, 3), True)\n        self.assertEqual(tb1._extract(1, 2), 'c')\n\n        # single row extraction\n        self.assertEqual(tb1._extract(1).shape, (1, 4))\n        self.assertEqual(tb1._extract(0).shape, (1, 4))\n        self.assertEqual(tb2._extract(0).shape, (1, 8))\n\n        # single column _extractions\n        self.assertEqual(tb1._extract(None, 1).shape, (3, 1))\n        self.assertEqual(tb2._extract(None, 1).shape, (3, 1))\n\n        # multiple row selection\n        self.assertEqual(tb2._extract([1,2],).shape, (2, 8))\n        self.assertEqual(tb2._extract([0,2],).shape, (2, 8))\n        self.assertEqual(tb2._extract([0,2], 6).shape, (2, 1))\n        self.assertEqual(tb2._extract([0,2], [6,7]).shape, (2, 2))\n\n        # mixed\n        self.assertEqual(tb2._extract(1,).shape, (1, 8))\n        self.assertEqual(tb2._extract([0,2]).shape, (2, 8))\n        self.assertEqual(tb2._extract(1, 4), False)\n        self.assertEqual(tb2._extract(1, 3), True)\n        self.assertEqual(tb2._extract([0, 2],).shape, (2, 8))\n\n\n        # slices\n        self.assertEqual(tb2._extract(slice(1,3)).shape, (2, 8))\n        self.assertEqual(tb2._extract(slice(1,3), slice(3,6)).shape, (2,3))\n        self.assertEqual(tb2._extract(slice(1,2)).shape, (1,8))\n        # a boundry over extended still gets 1\n        self.assertEqual(tb2._extract(slice(2,4)).shape, (1,8))\n        self.assertEqual(tb2._extract(slice(None), slice(2,4)).shape, (3, 2))\n        self.assertEqual(tb1._extract(slice(2,4)).shape, (1, 4))\n\n\n    def test_type_blocks_extract_b(self) -> None:\n        # test case of a single unified block\n\n        a1 = np.array([\n            [1, 2, 3, -5],\n            [10, 50, 30, -7],\n            [1345, 2234, 3345, -200]])\n        tb1 = TypeBlocks.from_blocks(a1)\n        self.assertEqual(tb1.shape, (3, 4))\n        self.assertEqual(len(tb1.mloc), 1)\n\n        a1 = np.array([1,10,1345])\n        a2 = np.array([2, 50, 2234])\n        a3 = np.array([3, 30, 3345])\n        a4 = np.array([-5, -7, -200])\n        tb2 = TypeBlocks.from_blocks((a1, a2, a3, a4))\n        self.assertEqual(tb2.shape, (3, 4))\n        self.assertEqual(len(tb2.mloc), 4)\n\n\n        tb1_a = tb1._extract(row_key=slice(0,1))\n        tb2_a = tb2._extract(row_key=slice(0,1))\n        self.assertEqual(tb1_a.shape, tb2_a.shape)\n        self.assertTrue((tb1_a.values == tb2_a.values).all())\n\n\n        tb1_b = tb1._extract(row_key=slice(1))\n        tb2_b = tb2._extract(row_key=slice(1))\n\n        self.assertEqual(tb1_b.shape, tb2_b.shape)\n        self.assertTrue((tb1_b.values == tb2_b.values).all())\n\n\n        tb1_c = tb1._extract(row_key=slice(0, 2))\n        tb2_c = tb2._extract(row_key=slice(0, 2))\n\n        self.assertEqual(tb1_c.shape, tb2_c.shape)\n        self.assertTrue((tb1_c.values == tb2_c.values).all())\n\n        tb1_d = tb1._extract(row_key=slice(0, 2), column_key=3)\n        tb2_d = tb2._extract(row_key=slice(0, 2), column_key=3)\n\n        self.assertEqual(tb1_d.shape, tb2_d.shape)\n        self.assertTrue((tb1_d.values == tb2_d.values).all())\n\n        tb1_e = tb1._extract(row_key=slice(0, 2), column_key=slice(2,4))\n        tb2_e = tb2._extract(row_key=slice(0, 2), column_key=slice(2,4))\n\n        self.assertEqual(tb1_e.shape, tb2_e.shape)\n        self.assertTrue((tb1_e.values == tb2_e.values).all())\n\n        self.assertTrue(tb1._extract(row_key=2, column_key=2) ==\n                tb2._extract(row_key=2, column_key=2) ==\n                3345)\n\n    def test_type_blocks_extract_c(self) -> None:\n        # test negative slices\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n\n        # test negative row slices\n        self.assertTypeBlocksArrayEqual(\n                tb1._extract(-1),\n                [0, 0, 1, True, False, True, 'oe', 'od'],\n                match_dtype=object\n                )\n        self.assertTypeBlocksArrayEqual(\n                tb1._extract(slice(-2, None)),\n                [[4, 5, 6, True, False, True, 'c', 'd'],\n                [0, 0, 1, True, False, True, 'oe', 'od']],\n                match_dtype=object\n                )\n        self.assertTypeBlocksArrayEqual(\n                tb1._extract(slice(-3, -1)),\n                [[1, 2, 3, False, False, True, 'a', 'b'],\n                [4, 5, 6, True, False, True, 'c', 'd']],\n                match_dtype=object\n                )\n\n        self.assertTypeBlocksArrayEqual(\n                tb1._extract(slice(None), -2),\n                [['a'], ['c'], ['oe']],\n                match_dtype=object\n                )\n        self.assertTypeBlocksArrayEqual(\n                tb1._extract(slice(None), slice(-6, -1)),\n                [[3, False, False, True, 'a'],\n                [6, True, False, True, 'c'],\n                [1, True, False, True, 'oe']],\n                match_dtype=object\n                )\n\n        self.assertTypeBlocksArrayEqual(\n                tb1._extract(slice(None), slice(-1, -4, -1)),\n                [['b', 'a', True],\n                ['d', 'c', True],\n                ['od', 'oe', True]],\n                match_dtype=object)\n\n    #---------------------------------------------------------------------------\n\n    def test_type_blocks_extract_array_a(self) -> None:\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        a4 = tb1._extract_array(row_key=1)\n        self.assertEqual(a4.tolist(),\n                [4, 5, 6, True, False, True, 'c', 'd'])\n\n        a5 = tb1._extract_array(column_key=5)\n        self.assertEqual(a5.tolist(),\n                [True, True, True])\n\n\n    def test_type_blocks_extract_array_b(self) -> None:\n\n        a1 = np.arange(10).reshape(2, 5)\n        tb1 = TypeBlocks.from_blocks(a1)\n\n        a2 = tb1._extract_array(1, 4)\n        self.assertEqual(a2, 9)\n\n        a2 = tb1._extract_array(NULL_SLICE, 4)\n        self.assertEqual(a2.tolist(), [4, 9])\n\n\n    def test_type_blocks_extract_array_c(self) -> None:\n\n        a1 = np.arange(4)\n        a2 = np.arange(10, 14)\n        a3 = np.arange(20, 24)\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        a2 = tb1._extract_array(1, 2)\n        self.assertEqual(a2, 21)\n\n        a2 = tb1._extract_array(NULL_SLICE, 1)\n        self.assertEqual(a2.tolist(), [10, 11, 12, 13])\n\n    #---------------------------------------------------------------------------\n\n    def test_immutable_filter(self) -> None:\n        a1 = np.array([3, 4, 5])\n        a2 = immutable_filter(a1)\n        with self.assertRaises(ValueError):\n            a2[0] = 34\n        a3 = a2[:2]\n        with self.assertRaises(ValueError):\n            a3[0] = 34\n\n\n    def test_type_blocks_static_frame(self) -> None:\n        a1 = np.array([1, 2, 3], dtype=np.int64)\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertTrue(tb1.dtypes[0] == np.int64)\n\n\n    def test_type_blocks_attributes(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb2 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertEqual(tb1.size, 9)\n        self.assertEqual(tb2.size, 24)\n\n\n\n\n    def test_type_blocks_block_pointers(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb2 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertTrue((tb1[0:2].mloc == tb1.mloc[:2]).all())\n        self.assertTrue((tb1.mloc[:2] == tb1.iloc[0:2, 0:2].mloc).all())\n\n\n\n\n    def test_type_blocks_unary_operator_a(self) -> None:\n\n        a1 = np.array([1,-2,-3])\n        a2 = np.array([False, True, False])\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        tb2 = ~tb1 # tilde\n        self.assertEqual(\n            (~tb1.values).tolist(),\n            [[-2, -1], [1, -2], [2, -1]])\n\n    def test_type_blocks_unary_operator_b(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [-4, 5, 6], [0,0,-1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb2 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertTypeBlocksArrayEqual(\n                -tb2[0:3],\n                [[-1, -2, -3],\n                 [ 4, -5, -6],\n                 [ 0,  0,  1]],\n                )\n\n        self.assertTypeBlocksArrayEqual(\n                ~tb2[3:5],\n                [[ True,  True],\n                [False,  True],\n                [False,  True]],\n                )\n\n\n\n    def test_type_blocks_block_compatible_a(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb2 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        # get slices with unified types\n        tb3 = tb1[[0, 1]]\n        tb4 = tb2[[2, 3]]\n\n\n        self.assertTrue(tb3.block_compatible(tb4))\n        self.assertTrue(tb4.block_compatible(tb3))\n\n        self.assertFalse(tb1.block_compatible(tb2))\n        self.assertFalse(tb2.block_compatible(tb1))\n\n\n    def test_type_blocks_block_compatible_b(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb2 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        tb2a = tb2[[2,3,7]]\n        self.assertTrue(tb1.block_compatible(tb2a))\n\n    #---------------------------------------------------------------------------\n\n    def test_type_blocks_consolidate_a(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([10,50,30])\n        a3 = np.array([1345,2234,3345])\n\n        a4 = np.array([False, True, False])\n        a5 = np.array([False, False, False])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3, a4, a5))\n        self.assertEqual(tb1.shape, (3, 5))\n\n        tb2 = tb1.consolidate()\n\n        self.assertTrue([b.dtype for b in tb2._blocks], [np.int, np.bool])\n        self.assertEqual(tb2.shape, (3, 5))\n\n        # we have perfect correspondence between the two\n        self.assertTrue((tb1.values == tb2.values).all())\n\n\n    def test_type_blocks_consolidate_b(self) -> None:\n        # if we hava part of TB consolidated, we do not reallocate\n\n\n        a1 = np.array([\n            [1, 2, 3],\n            [10,50,30],\n            [1345,2234,3345]])\n\n        a2 = np.array([False, True, False])\n        a3 = np.array([False, False, False])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n        self.assertEqual(tb1.shape, (3, 5))\n        self.assertEqual(len(tb1.mloc), 3)\n\n        tb2 = tb1.consolidate()\n        self.assertEqual(tb1.shape, (3, 5))\n        self.assertEqual(len(tb2.mloc), 2)\n        # the first block is the same instance\n        self.assertEqual(tb1.mloc[0], tb2.mloc[0])\n\n\n    def test_type_blocks_consolidate_c(self) -> None:\n        blocks = [np.empty(shape=(0, 1), dtype=np.dtype('>f4')), np.empty(shape=(0, 2), dtype=np.dtype('>f4'))]\n\n        tb1 = TypeBlocks.from_blocks(blocks)\n        tb2 = tb1.consolidate()\n        self.assertTrue((tb1.dtypes == tb2.dtypes).all())\n\n\n    #---------------------------------------------------------------------------\n\n\n    def test_type_blocks_binary_operator_a(self) -> None:\n\n        a1 = np.array([\n            [1, 2, 3, -5],\n            [10, 50, 30, -7],\n            [1345, 2234, 3345, -200]])\n        tb1 = TypeBlocks.from_blocks(a1)\n\n        a1 = np.array([1,10,1345])\n        a2 = np.array([2, 50, 2234])\n        a3 = np.array([3, 30, 3345])\n        a4 = np.array([-5, -7, -200])\n        tb2 = TypeBlocks.from_blocks((a1, a2, a3, a4))\n\n        self.assertTrue(((tb1 + tb2).values == (tb1 + tb1).values).all())\n\n        post1 = tb1 + tb2\n\n\n\n    def test_type_blocks_binary_operator_b(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'], dtype=object)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']], dtype=object)\n        tb2 = TypeBlocks.from_blocks((a1, a2, a3))\n\n\n        self.assertTypeBlocksArrayEqual(\n                tb2 * 3,\n                [[3, 6, 9, 0, 0, 3, 'aaa', 'bbb'],\n                [12, 15, 18, 3, 0, 3, 'ccc', 'ddd'],\n                [0, 0, 3, 3, 0, 3, 'oeoeoe', 'ododod']],\n                match_dtype=object\n                )\n\n        self.assertTypeBlocksArrayEqual(\n                tb1[:2] + 10,\n                [[11, 10],\n                [12, 11],\n                [13, 10]],\n                )\n\n        self.assertTypeBlocksArrayEqual(\n                tb1[:2] + 10,\n                [[11, 10],\n                [12, 11],\n                [13, 10]],\n                )\n\n\n    def test_type_blocks_binary_operator_c(self) -> None:\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'], dtype=object)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']], dtype=object)\n        tb2 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        # these result in the same thing\n        self.assertTypeBlocksArrayEqual(\n                tb1[:2] * tb2[[2,3]],\n                [[3, False],\n                [12, True],\n                [3, False]]\n                )\n\n        self.assertTypeBlocksArrayEqual(\n                tb1[0:2] * tb1[0:2],\n                [[1, False],\n                [4, True],\n                [9, False]]\n                )\n\n        self.assertTypeBlocksArrayEqual(\n                tb2[:3] % 2,\n                [[1, 0, 1],\n                [0, 1, 0],\n                [0, 0, 1]]\n            )\n\n\n    def test_type_blocks_binary_operator_d(self) -> None:\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[1.5,2.6], [4.2,5.5], [0.2,0.1]])\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        post = tb1 * (1, 0, 2, 0, 1)\n        self.assertTypeBlocksArrayEqual(post,\n                [[  1. ,   0. ,   6. ,   0. ,   2.6],\n                [  4. ,   0. ,  12. ,   0. ,   5.5],\n                [  0. ,   0. ,   2. ,   0. ,   0.1]])\n\n\n    #---------------------------------------------------------------------------\n\n    def test_type_blocks_extend_a(self) -> None:\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'], dtype=object)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']], dtype=object)\n        tb2 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        # mutates in place\n        tb1.extend(tb2)\n        self.assertEqual(tb1.shape, (3, 11))\n\n        self.assertTypeBlocksArrayEqual(\n                tb1.iloc[2],\n                [3, False, 'd', 0, 0, 1, True, False, True, 'oe', 'od'],\n                match_dtype=object,\n                )\n\n    def test_type_blocks_extend_b(self) -> None:\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        tb2 = TypeBlocks.from_blocks(np.array([3, 4]))\n\n        with self.assertRaises(RuntimeError):\n            tb1.extend(tb2)\n\n    #---------------------------------------------------------------------------\n\n    def test_type_blocks_mask_blocks_a(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        mask = TypeBlocks.from_blocks(tb1._mask_blocks(column_key=[2,3,5,6]))\n\n        self.assertTypeBlocksArrayEqual(mask,\n            [[False, False, True, True, False, True, True, False], [False, False, True, True, False, True, True, False], [False, False, True, True, False, True, True, False]]\n            )\n\n    def test_type_blocks_mask_blocks_b(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6]])\n        a2 = np.array([[False, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd']])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        # show that index order is not relevant to selection\n        self.assertTypeBlocksArrayEqual(\n                TypeBlocks.from_blocks(tb1._mask_blocks(column_key=[0, 5, 6])),\n                [[ True, False, False, False, False,  True,  True, False],\n                [ True, False, False, False, False,  True,  True, False]])\n\n        self.assertTypeBlocksArrayEqual(\n                TypeBlocks.from_blocks(tb1._mask_blocks(column_key=[0, 6, 5])),\n                [[ True, False, False, False, False,  True,  True, False],\n                [ True, False, False, False, False,  True,  True, False]])\n\n        self.assertTypeBlocksArrayEqual(\n                TypeBlocks.from_blocks(tb1._mask_blocks(column_key=[6, 5, 0])),\n                [[ True, False, False, False, False,  True,  True, False],\n                [ True, False, False, False, False,  True,  True, False]])\n\n        # with repeated values we get the same result; we are not presently filtering out duplicates, however\n        self.assertTypeBlocksArrayEqual(\n                TypeBlocks.from_blocks(tb1._mask_blocks(column_key=[6, 6, 5, 5, 0])),\n                [[ True, False, False, False, False,  True,  True, False],\n                [ True, False, False, False, False,  True,  True, False]])\n\n\n    def test_type_blocks_mask_blocks_c(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6]])\n        a2 = np.array([[False, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd']])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertTypeBlocksArrayEqual(\n                TypeBlocks.from_blocks(tb1._mask_blocks(column_key=slice(2, None, 2))),\n                [[False, False,  True, False,  True, False,  True, False],\n                [False, False,  True, False,  True, False,  True, False]])\n\n        tb2 = TypeBlocks.from_blocks(tb1._mask_blocks(column_key=slice(4, None, -1)))\n        self.assertTypeBlocksArrayEqual(tb2,\n                [[ True,  True,  True,  True,  True, False, False, False],\n                [ True,  True,  True,  True,  True, False, False, False]])\n\n        tb3 = TypeBlocks.from_blocks(tb1._mask_blocks(column_key=slice(4, 2, -1)))\n        self.assertTypeBlocksArrayEqual(tb3,\n                [[ False,  False,  False,  True,  True, False, False, False],\n                [ False,  False,  False,  True,  True, False, False, False]])\n\n        tb4 = TypeBlocks.from_blocks(tb1._mask_blocks(column_key=slice(6, None, -2)))\n        self.assertTypeBlocksArrayEqual(tb4,\n                [[ True, False,  True, False,  True, False,  True, False],\n                [ True, False,  True, False,  True, False,  True, False]])\n\n        tb5 = TypeBlocks.from_blocks(tb1._mask_blocks(column_key=slice(6, None, -3)))\n        self.assertTypeBlocksArrayEqual(tb5,\n                [[ True, False, False,  True, False, False,  True, False],\n                [ True, False, False,  True, False, False,  True, False]])\n\n\n    #---------------------------------------------------------------------------\n\n    def test_type_blocks_assign_blocks_a(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        tb2 = TypeBlocks.from_blocks(tb1._assign_blocks_from_keys(\n                column_key=[2,3,5], value=300))\n\n        self.assertTypeBlocksArrayEqual(tb2,\n            [[1, 2, 300, 300, False, 300, 'a', 'b'],\n            [4, 5, 300, 300, False, 300, 'c', 'd'],\n            [0, 0, 300, 300, False, 300, 'oe', 'od']], match_dtype=object)\n\n        # blocks not mutated will be the same\n        self.assertEqual(tb1.mloc[2], tb2.mloc[5])\n        self.assertEqual(tb2.shapes.tolist(),\n            [(3, 2), (3, 1), (3, 1), (3, 1), (3, 1), (3, 2)]\n            )\n\n    def test_type_blocks_assign_blocks_b(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        tb2 = TypeBlocks.from_blocks(tb1._assign_blocks_from_keys(\n                column_key=slice(-3, None), value=300))\n\n        self.assertTypeBlocksArrayEqual(tb2,\n            [[1, 2, 3, False, False, 300, 300, 300],\n            [4, 5, 6, True, False, 300, 300, 300],\n            [0, 0, 1, True, False, 300, 300, 300]], match_dtype=object)\n\n        # blocks not mutated will be the same\n        self.assertEqual(tb1.mloc[0], tb2.mloc[0])\n        self.assertEqual(tb2.shapes.tolist(),\n            [(3, 3), (3, 2), (3, 1), (3, 2)]\n            )\n\n    @skip_win  # type: ignore\n    def test_type_blocks_assign_blocks_c(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        tb2 = TypeBlocks.from_blocks(tb1._assign_blocks_from_keys(\n                column_key=[0, 2, 3, 5, 7], value=300))\n\n        self.assertTypeBlocksArrayEqual(tb2,\n            [[300, 2, 300, 300, False, 300, 'a', 300],\n            [300, 5, 300, 300, False, 300, 'c', 300],\n            [300, 0, 300, 300, False, 300, 'oe', 300]], match_dtype=object)\n\n\n        self.assertEqual(tb2.shapes.tolist(),\n            [(3, 1), (3, 1), (3, 1), (3, 1), (3, 1), (3, 1), (3, 1), (3, 1)]\n            )\n\n        self.assertEqual(tb2.dtypes.tolist(),\n            [np.dtype('int64'), np.dtype('int64'), np.dtype('int64'), np.dtype('int64'), np.dtype('bool'), np.dtype('int64'), np.dtype('<U2'), np.dtype('int64')]\n            )\n\n\n    def test_type_blocks_assign_blocks_d(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        value = np.array([1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1, 8.1])\n        tb2 = TypeBlocks.from_blocks(tb1._assign_blocks_from_keys(\n                row_key=[1], value=value))\n\n        self.assertEqual(tb2.dtypes.tolist(),\n                [np.dtype('float64'), np.dtype('float64'), np.dtype('float64'), np.dtype('O'), np.dtype('O'), np.dtype('O'), np.dtype('O'), np.dtype('O')])\n\n        self.assertTypeBlocksArrayEqual(tb2,\n            [[1.0, 2.0, 3.0, False, False, True, 'a', 'b'],\n            [1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1, 8.1],\n            [0.0, 0.0, 1.0, True, False, True, 'oe', 'od']], match_dtype=object)\n\n\n\n    def test_type_blocks_assign_blocks_e(self) -> None:\n\n        a1 = np.array([[True, True, True], [True, True, True], [True, True, True]])\n        a2 = np.array([[False, False, False], [False, False, False], [False, False, False]])\n        a3 = np.array([[True, True, True], [True, True, True], [True, True, True]])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        value = np.array([1.1, 2.1, 3.1, 4.1])\n\n        tb2 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(row_key=[1], column_key=slice(1, 5), value=value))\n\n        self.assertEqual(tb1.shape, tb2.shape)\n        self.assertEqual(tb2.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('O'), np.dtype('O'), np.dtype('O'), np.dtype('O'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool')])\n\n        self.assertEqual(tb2.iloc[1].values[0].tolist(),\n                [True, 1.1, 2.1, 3.1, 4.1, False, True, True, True])\n\n\n\n        tb3 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(row_key=[1], column_key=slice(2, 6), value=value))\n\n        self.assertEqual(tb1.shape, tb3.shape)\n        self.assertEqual(tb3.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('bool'), np.dtype('O'), np.dtype('O'), np.dtype('O'), np.dtype('O'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool')])\n\n        self.assertEqual(tb3.iloc[1].values[0].tolist(),\n                [True, True, 1.1, 2.1, 3.1, 4.1, True, True, True])\n\n\n        tb4 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(row_key=[1], column_key=slice(3, 7), value=value))\n\n        self.assertEqual(tb1.shape, tb4.shape)\n        self.assertEqual(tb4.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('O'), np.dtype('O'), np.dtype('O'), np.dtype('O'), np.dtype('bool'), np.dtype('bool')])\n\n        self.assertEqual(tb4.iloc[1].values[0].tolist(),\n                [True, True, True, 1.1, 2.1, 3.1, 4.1, True, True])\n\n\n        tb5 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(row_key=[1], column_key=slice(4, 8), value=value))\n\n        self.assertEqual(tb1.shape, tb5.shape)\n        self.assertEqual(tb5.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('O'), np.dtype('O'), np.dtype('O'), np.dtype('O'), np.dtype('bool')])\n\n        self.assertEqual(tb5.iloc[1].values[0].tolist(),\n                [True, True, True, False, 1.1, 2.1, 3.1, 4.1, True])\n\n\n        tb6 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(row_key=[1], column_key=slice(5, 9), value=value))\n\n        self.assertEqual(tb1.shape, tb6.shape)\n        self.assertEqual(tb6.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('O'), np.dtype('O'), np.dtype('O'), np.dtype('O'),])\n\n        self.assertEqual(tb6.iloc[1].values[0].tolist(),\n                [True, True, True, False, False, 1.1, 2.1, 3.1, 4.1])\n\n\n\n    def test_type_blocks_assign_blocks_f(self) -> None:\n\n        a1 = np.array([[True, True, True], [True, True, True], [True, True, True]])\n        a2 = np.array([[False, False, False], [False, False, False], [False, False, False]])\n        a3 = np.array([[True, True, True], [True, True, True], [True, True, True]])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        value = np.array([1.1, 2.1, 3.1, 4.1])\n\n        tb2 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(column_key=slice(1, 5), value=value))\n\n        self.assertEqual(tb1.shape, tb2.shape)\n        self.assertEqual(tb2.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('float64'), np.dtype('float64'), np.dtype('float64'), np.dtype('float64'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool')])\n\n        self.assertEqual(tb2.iloc[1].values[0].tolist(),\n                [True, 1.1, 2.1, 3.1, 4.1, False, True, True, True])\n\n\n        tb3 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(column_key=slice(2, 6), value=value))\n\n        self.assertEqual(tb1.shape, tb3.shape)\n        self.assertEqual(tb3.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('bool'), np.dtype('float64'), np.dtype('float64'), np.dtype('float64'), np.dtype('float64'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool')])\n\n        self.assertEqual(tb3.iloc[1].values[0].tolist(),\n                [True, True, 1.1, 2.1, 3.1, 4.1, True, True, True])\n\n\n        tb4 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(column_key=slice(3, 7), value=value))\n\n        self.assertEqual(tb1.shape, tb4.shape)\n        self.assertEqual(tb4.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('float64'), np.dtype('float64'), np.dtype('float64'), np.dtype('float64'), np.dtype('bool'), np.dtype('bool')])\n\n        self.assertEqual(tb4.iloc[1].values[0].tolist(),\n                [True, True, True, 1.1, 2.1, 3.1, 4.1, True, True])\n\n\n        tb5 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(column_key=slice(4, 8), value=value))\n\n        self.assertEqual(tb1.shape, tb5.shape)\n        self.assertEqual(tb5.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('float64'), np.dtype('float64'), np.dtype('float64'), np.dtype('float64'), np.dtype('bool')])\n\n        self.assertEqual(tb5.iloc[1].values[0].tolist(),\n                [True, True, True, False, 1.1, 2.1, 3.1, 4.1, True])\n\n\n        tb6 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(column_key=slice(5, 9), value=value))\n\n        self.assertEqual(tb1.shape, tb6.shape)\n        self.assertEqual(tb6.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('float64'), np.dtype('float64'), np.dtype('float64'), np.dtype('float64'),])\n\n        self.assertEqual(tb6.iloc[1].values[0].tolist(),\n                [True, True, True, False, False, 1.1, 2.1, 3.1, 4.1])\n\n\n\n    def test_type_blocks_assign_blocks_g(self) -> None:\n\n        a1 = np.array([[True, True, True], [True, True, True], [True, True, True]])\n        a2 = np.array([[False, False, False], [False, False, False], [False, False, False]])\n        a3 = np.array([[True, True, True], [True, True, True], [True, True, True]])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        value = np.array([[1.1, 2.1], [3.1, 4.1]])\n\n        tb2 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(row_key=slice(1, 3), column_key=slice(3, 5), value=value))\n\n        self.assertEqual(tb2.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('O'), np.dtype('O'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool')])\n\n        # import ipdb; ipdb.set_trace()\n        self.assertEqual(tb2.shapes.tolist(),\n                [(3, 3), (3, 2), (3, 1), (3, 3)])\n\n\n        tb3 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(row_key=slice(1, 3), column_key=slice(4, 6), value=value))\n\n        self.assertEqual(tb3.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('O'), np.dtype('O'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool')])\n\n        self.assertEqual(tb3.shapes.tolist(),\n                [(3, 3), (3, 1), (3, 2), (3, 3)])\n\n\n        tb4 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(row_key=slice(1, 3), column_key=slice(5, 7), value=value))\n\n        self.assertEqual(tb4.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('O'), np.dtype('O'), np.dtype('bool'), np.dtype('bool')])\n\n        self.assertEqual(tb4.shapes.tolist(),\n                [(3, 3), (3, 2), (3, 1), (3, 1), (3, 2)])\n\n\n\n\n    def test_type_blocks_assign_blocks_h(self) -> None:\n\n        a1 = np.array([[True, True, True], [True, True, True], [True, True, True]])\n        a2 = np.array([[False, False, False], [False, False, False], [False, False, False]])\n        a3 = np.array([[True, True, True], [True, True, True], [True, True, True]])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        value = np.array([[1.1, 2.1], [3.1, 4.1], [5.1, 6.1]])\n\n        tb2 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(column_key=slice(3, 5), value=value))\n\n        self.assertEqual(tb2.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('float64'), np.dtype('float64'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool')])\n\n        # import ipdb; ipdb.set_trace()\n        self.assertEqual(tb2.shapes.tolist(),\n                [(3, 3), (3, 2), (3, 1), (3, 3)])\n\n\n        tb3 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(column_key=slice(4, 6), value=value))\n\n        self.assertEqual(tb3.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('float64'), np.dtype('float64'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool')])\n\n        self.assertEqual(tb3.shapes.tolist(),\n                [(3, 3), (3, 1), (3, 2), (3, 3)])\n\n\n        tb4 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(column_key=slice(5, 7), value=value))\n\n        self.assertEqual(tb4.dtypes.tolist(),\n                [np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('bool'), np.dtype('float64'), np.dtype('float64'), np.dtype('bool'), np.dtype('bool')])\n\n        self.assertEqual(tb4.shapes.tolist(),\n                [(3, 3), (3, 2), (3, 1), (3, 1), (3, 2)])\n\n\n\n    def test_type_blocks_assign_blocks_i(self) -> None:\n\n        a1 = np.array([[1.2], [2.1], [3.1]])\n        a2 = np.array([False, True, True])\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        value = (20.1, 40.1)\n        tb2 = TypeBlocks.from_blocks(\n                tb1._assign_blocks_from_keys(column_key=0, row_key=np.array([True, True, False]), value=value))\n\n        self.assertTypeBlocksArrayEqual(tb2,\n            [[20.1, False], [40.1, True], [3.1, True]], match_dtype=object)\n\n\n\n\n    #--------------------------------------------------------------------------\n    def test_type_blocks_group_a(self) -> None:\n\n        a1 = np.array([\n                [1, 2, 3,4],\n                [4,2,6,3],\n                [0, 0, 1,2],\n                [0, 0, 1,1]\n                ])\n        a2 = np.array([[False, False, True],\n                [False, False, True],\n                [True, False, True],\n                [True, False, True]])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        # return rows, by columns key 1\n        groups = list(tb1.group(axis=0, key=1))\n\n        self.assertEqual(len(groups), 2)\n\n\n        group, selection, subtb = groups[0]\n        self.assertEqual(group, 0)\n        self.assertEqual(subtb.values.tolist(),\n                [[0, 0, 1, 2, True, False, True], [0, 0, 1, 1, True, False, True]])\n\n\n        group, selection, subtb = groups[1]\n        self.assertEqual(group, 2)\n        self.assertEqual(subtb.values.tolist(),\n                [[1, 2, 3, 4, False, False, True], [4, 2, 6, 3, False, False, True]])\n\n\n    def test_type_blocks_group_b(self) -> None:\n\n        a1 = np.array([\n                [1, 2, 3,4],\n                [4,2,6,3],\n                [0, 0, 1,2],\n                [0, 0, 1,1]\n                ])\n        a2 = np.array([[False, False, True],\n                [False, False, True],\n                [True, False, True],\n                [True, False, True]])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        # return rows, by columns key [4, 5]\n        groups = list(tb1.group(axis=0, key=[4, 5]))\n\n        self.assertEqual(len(groups), 2)\n\n\n        group, selection, subtb = groups[0]\n        self.assertEqual(group, (False, False))\n        self.assertEqual(subtb.values.tolist(),\n                [[1, 2, 3, 4, False, False, True], [4, 2, 6, 3, False, False, True]]\n                )\n\n        group, selection, subtb = groups[1]\n        self.assertEqual(group, (True, False))\n        self.assertEqual(subtb.values.tolist(),\n                [[0, 0, 1, 2, True, False, True], [0, 0, 1, 1, True, False, True]])\n\n\n    def test_type_blocks_transpose_a(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        tb2 = tb1.transpose()\n\n        self.assertEqual(tb2.values.tolist(),\n                [[1, 4, 0], [2, 5, 0], [3, 6, 1], [False, True, True], [False, False, False], [True, True, True], ['a', 'c', 'oe'], ['b', 'd', 'od']])\n\n        self.assertEqual(tb1.transpose().transpose().values.tolist(),\n                tb1.values.tolist())\n\n\n\n    def test_type_blocks_display_a(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb = TypeBlocks.from_blocks((a1, a2, a3))\n\n        disp = tb.display()\n        self.assertEqual(len(disp), 5)\n\n\n    #---------------------------------------------------------------------------\n\n    def test_type_blocks_axis_values_a(self) -> None:\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertEqual(\n            list(a.tolist() for a in tb.axis_values(1)),\n            [[1, 2, 3, False, False, True, 'a', 'b'], [4, 5, 6, True, False, True, 'c', 'd'], [0, 0, 1, True, False, True, 'oe', 'od']]\n            )\n\n        self.assertEqual(list(a.tolist() for a in tb.axis_values(0)),\n            [[1, 4, 0], [2, 5, 0], [3, 6, 1], [False, True, True], [False, False, False], [True, True, True], ['a', 'c', 'oe'], ['b', 'd', 'od']]\n            )\n\n        # we are iterating over slices so we get views of columns without copying\n        self.assertEqual(tb.mloc[0], mloc(next(tb.axis_values(0))))\n\n\n    def test_type_blocks_axis_values_b(self) -> None:\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertEqual(\n                [a.tolist() for a in tb.axis_values(axis=0, reverse=True)],\n                [['b', 'd', 'od'], ['a', 'c', 'oe'], [True, True, True], [False, False, False], [False, True, True], [3, 6, 1], [2, 5, 0], [1, 4, 0]])\n        self.assertEqual([a.tolist() for a in tb.axis_values(axis=0, reverse=False)],\n                [[1, 4, 0], [2, 5, 0], [3, 6, 1], [False, True, True], [False, False, False], [True, True, True], ['a', 'c', 'oe'], ['b', 'd', 'od']])\n\n\n    def test_type_blocks_axis_values_c(self) -> None:\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb = TypeBlocks.from_blocks((a1, a2, a3))\n\n        with self.assertRaises(AxisInvalid):\n            _ = next(tb.axis_values(-1))\n\n\n    #---------------------------------------------------------------------------\n    def test_type_blocks_extract_iloc_mask_a(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertEqual(tb.extract_iloc_mask((slice(None), [4, 5])).values.tolist(),\n                [[False, False, False, False, True, True, False, False], [False, False, False, False, True, True, False, False], [False, False, False, False, True, True, False, False]])\n\n        self.assertEqual(tb.extract_iloc_mask(([0,2], slice(None))).values.tolist(),\n                [[True, True, True, True, True, True, True, True], [False, False, False, False, False, False, False, False], [True, True, True, True, True, True, True, True]]\n                )\n\n        self.assertEqual(tb.extract_iloc_mask(([0,2], [3,7])).values.tolist(),\n                [[False, False, False, True, False, False, False, True], [False, False, False, False, False, False, False, False], [False, False, False, True, False, False, False, True]]\n                )\n\n        self.assertEqual(tb.extract_iloc_mask((slice(1, None), slice(4, None))).values.tolist(),\n                [[False, False, False, False, False, False, False, False], [False, False, False, False, True, True, True, True], [False, False, False, False, True, True, True, True]])\n\n\n\n\n    def test_type_blocks_extract_iloc_assign_a(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb = TypeBlocks.from_blocks((a1, a2, a3))\n\n\n        self.assertEqual(tb.extract_iloc_assign(1, 600).values.tolist(),\n                [[1, 2, 3, False, False, True, 'a', 'b'],\n                [600, 600, 600, 600, 600, 600, 600, 600],\n                [0, 0, 1, True, False, True, 'oe', 'od']])\n\n\n    def test_type_blocks_extract_iloc_assign_b(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertEqual(tb.extract_iloc_assign((1, 5), 20).values.tolist(),\n                [[1, 2, 3, False, False, True, 'a', 'b'],\n                [4, 5, 6, True, False, 20, 'c', 'd'],\n                [0, 0, 1, True, False, True, 'oe', 'od']])\n\n\n    def test_type_blocks_extract_iloc_assign_c(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertEqual(tb.extract_iloc_assign((slice(2), slice(5)), 'X').values.tolist(),\n                [['X', 'X', 'X', 'X', 'X', True, 'a', 'b'],\n                ['X', 'X', 'X', 'X', 'X', True, 'c', 'd'],\n                [0, 0, 1, True, False, True, 'oe', 'od']]\n                )\n\n\n    def test_type_blocks_extract_iloc_assign_d(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertEqual(tb.extract_iloc_assign(([0,1], [1,4,7]), -5).values.tolist(),\n                [[1, -5, 3, False, -5, True, 'a', -5],\n                [4, -5, 6, True, -5, True, 'c', -5],\n                [0, 0, 1, True, False, True, 'oe', 'od']])\n\n\n    def test_type_blocks_extract_iloc_assign_e(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertEqual(\n                tb.extract_iloc_assign((1, slice(4)), (-1, -2, -3, -4)).values.tolist(),\n                [[1, 2, 3, False, False, True, 'a', 'b'],\n                [-1, -2, -3, -4, False, True, 'c', 'd'],\n                [0, 0, 1, True, False, True, 'oe', 'od']])\n\n\n    def test_type_blocks_extract_iloc_assign_f(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertEqual(\n                tb.extract_iloc_assign((2, slice(3,7)), (-1, -2, -3, -4)).values.tolist(),\n                [[1, 2, 3, False, False, True, 'a', 'b'],\n                [4, 5, 6, True, False, True, 'c', 'd'],\n                [0, 0, 1, -1, -2, -3, -4, 'od']])\n\n\n    def test_type_blocks_extract_iloc_assign_g(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertEqual(\n                tb.extract_iloc_assign((0, slice(4,8)), (-1, -2, -3, -4)).values.tolist(),\n                [[1, 2, 3, False, -1, -2, -3, -4],\n                [4, 5, 6, True, False, True, 'c', 'd'],\n                [0, 0, 1, True, False, True, 'oe', 'od']])\n\n\n\n\n    def test_type_blocks_elements_items_a(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        a4 = np.array([None, None, None])\n        tb = TypeBlocks.from_blocks((a1, a2, a4, a3))\n\n        post = [x for x in tb.element_items()]\n\n        self.assertEqual(post,\n                [((0, 0), 1), ((0, 1), 2), ((0, 2), 3), ((0, 3), False), ((0, 4), False), ((0, 5), True), ((0, 6), None), ((0, 7), 'a'), ((0, 8), 'b'), ((1, 0), 4), ((1, 1), 5), ((1, 2), 6), ((1, 3), True), ((1, 4), False), ((1, 5), True), ((1, 6), None), ((1, 7), 'c'), ((1, 8), 'd'), ((2, 0), 0), ((2, 1), 0), ((2, 2), 1), ((2, 3), True), ((2, 4), False), ((2, 5), True), ((2, 6), None), ((2, 7), 'oe'), ((2, 8), 'od')]\n                )\n\n        tb2 = TypeBlocks.from_element_items(post, tb.shape, tb._row_dtype)\n        self.assertTrue((tb.values == tb2.values).all())\n\n\n    def test_type_blocks_reblock_signature_a(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]], dtype=np.int64)\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        a4 = np.array([None, None, None])\n        tb = TypeBlocks.from_blocks((a1, a2, a4, a3))\n\n        dtype = np.dtype\n        self.assertEqual(\n                list(tb._reblock_signature()),\n                [(dtype('int64'), 3), (dtype('bool'), 3), (dtype('O'), 1), (dtype('<U2'), 2)])\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]], dtype=np.int64)\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        a4 = np.array([None, None, None])\n        tb = TypeBlocks.from_blocks((a1, a2, a3, a4))\n\n        self.assertEqual(\n                list(tb._reblock_signature()),\n                [(dtype('int64'), 3), (dtype('bool'), 3), (dtype('<U2'), 2), (dtype('O'), 1)])\n\n\n\n#     @unittest.skip('implement operators for same sized but differently typed blocks')\n    def test_type_blocks_binary_operator_e(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        a4 = np.array([None, None, None])\n        tb = TypeBlocks.from_blocks((a1, a2, a4, a3))\n\n        post1 = [x for x in tb.element_items()]\n\n        tb2 = TypeBlocks.from_element_items(post1, tb.shape, tb._row_dtype)\n        self.assertTrue((tb.values == tb2.values).all())\n\n        post2 = tb == tb2\n        self.assertEqual(post2.values.tolist(),\n                [[True, True, True, True, True, True, True, True, True], [True, True, True, True, True, True, True, True, True], [True, True, True, True, True, True, True, True, True]])\n\n\n    def test_type_blocks_copy_a(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        a4 = np.array([None, None, None])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a4, a3))\n\n        tb2 = tb1.copy()\n        tb1.append(np.array((1, 2, 3)))\n\n        self.assertEqual(tb2.shape, (3, 9))\n        self.assertEqual(tb1.shape, (3, 10))\n\n        self.assertEqual(tb1.iloc[2].values.tolist(),\n                [[0, 0, 1, True, False, True, None, 'oe', 'od', 3]])\n\n        self.assertEqual(tb2.iloc[2].values.tolist(),\n                [[0, 0, 1, True, False, True, None, 'oe', 'od']])\n\n\n\n\n    def test_type_blocks_isna_a(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, np.nan, 6], [0, 0, 1]], dtype=object)\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        a4 = np.array([None, None, None])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a4, a3))\n\n        self.assertEqual(tb1.isna().values.tolist(),\n                [[False, False, False, False, False, False, True, False, False], [False, True, False, False, False, False, True, False, False], [False, False, False, False, False, False, True, False, False]])\n\n        self.assertEqual(tb1.notna().values.tolist(),\n                [[True, True, True, True, True, True, False, True, True], [True, False, True, True, True, True, False, True, True], [True, True, True, True, True, True, False, True, True]])\n\n\n    def test_type_blocks_dropna_to_slices(self) -> None:\n\n        a1 = np.array([\n                [1,np.nan,3, 4],\n                [4, np.nan, 6, 2],\n                [np.nan, np.nan, np.nan, np.nan]\n                ], dtype=object)\n        a2 = np.array([\n                [1,np.nan,3, 4],\n                [4, np.nan, 6, 2],\n                [np.nan, np.nan, np.nan, np.nan]\n                ], dtype=object)\n\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        row_key, column_key = tb1.dropna_to_keep_locations(axis=1)\n        assert column_key is not None\n\n        self.assertEqual(column_key.tolist(),\n                [True, False, True, True, True, False, True, True])\n        self.assertEqual(row_key, None)\n\n        row_key, column_key = tb1.dropna_to_keep_locations(axis=0)\n        assert row_key is not None\n        self.assertEqual(row_key.tolist(),\n                [True, True, False])\n        self.assertEqual(column_key, None)\n\n\n    def test_type_blocks_fillna_a(self) -> None:\n\n        a1 = np.array([\n                [1,np.nan,3, 4],\n                [4, np.nan, 6, 2],\n                [np.nan, np.nan, np.nan, np.nan]\n                ], dtype=float)\n        a2 = np.array([\n                [1,np.nan,3, 4],\n                [4, np.nan, 6, 2],\n                [np.nan, np.nan, np.nan, np.nan]\n                ], dtype=object)\n\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n        tb2 = tb1.fillna(0)\n        self.assertEqual([b.dtype for b in tb2._blocks],\n                [np.dtype('float64'), np.dtype('O')])\n        self.assertEqual(tb2.isna().values.any(), False)\n\n        tb3 = tb1.fillna(None)\n        self.assertEqual([b.dtype for b in tb3._blocks],\n                [np.dtype('O'), np.dtype('O')])\n        # we ahve Nones, which are na\n        self.assertEqual(tb3.isna().values.any(), True)\n\n\n\n    def test_type_blocks_fillna_trailing_a(self) -> None:\n\n        for axis in (0, 1):\n            for arrays in self.get_arrays_b():\n                tb = TypeBlocks.from_blocks(arrays)\n                post = tb.fillna_trailing(-1, axis=axis)\n                self.assertEqual(tb.shape, post.shape)\n\n\n    def test_type_blocks_fillna_trailing_b(self) -> None:\n\n        a1 = np.array([\n                [nan, nan,3, 4],\n                [nan, nan, 6, nan],\n                [5, nan, nan, nan]\n                ], dtype=float)\n        a2 = np.array([nan, nan, nan], dtype=object)\n\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        tb2 = tb1.fillna_trailing(0, axis=0)\n\n        self.assertAlmostEqualValues(\n                list(tb2.values.flat),\n                [nan, 0.0, 3.0, 4.0, 0, nan, 0.0, 6.0, 0.0, 0, 5.0, 0.0, 0.0, 0.0, 0])\n\n\n    def test_type_blocks_fillna_trailing_c(self) -> None:\n\n        a2 = np.array([\n                [None, None, None, None],\n                [None, 1, None, 6],\n                [None, 5, None, None]\n                ], dtype=object)\n        a1 = np.array([None, None, None], dtype=object)\n        a3 = np.array([\n                [None, 4],\n                [None, 1],\n                [None, 5]\n                ], dtype=object)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n        tb2 = tb1.fillna_trailing(0, axis=1)\n\n        # no change as no leading values are NaN\n        self.assertEqual(tb1.values.tolist(), tb2.values.tolist())\n\n\n    def test_type_blocks_fillna_trailing_d(self) -> None:\n\n        a2 = np.array([\n                [None, None, None, None],\n                [None, 1, None, 6],\n                [None, 5, None, None]\n                ], dtype=object)\n        a1 = np.array([None, None, None], dtype=object)\n        a3 = np.array([\n                [None, None],\n                [None, 1],\n                [None, 5]\n                ], dtype=object)\n        tb1 = TypeBlocks.from_blocks((a3, a2, a1))\n        tb2 = tb1.fillna_trailing(0, axis=1)\n\n        self.assertEqual(tb2.values.tolist(),\n                [[0, 0, 0, 0, 0, 0, 0],\n                [None, 1, None, 1, None, 6, 0],\n                [None, 5, None, 5, 0, 0, 0]])\n\n\n    def test_type_blocks_fillna_trailing_e(self) -> None:\n\n        a1 = np.array([None, None, None], dtype=object)\n        tb1 = TypeBlocks.from_blocks((a1,))\n        with self.assertRaises(NotImplementedError):\n            tb1.fillna_trailing(value=3, axis=2)\n\n\n    def test_type_blocks_fillna_leading_a(self) -> None:\n\n        for axis in (0, 1):\n            for arrays in self.get_arrays_b():\n                tb = TypeBlocks.from_blocks(arrays)\n                post = tb.fillna_leading(-1, axis=axis)\n                self.assertEqual(tb.shape, post.shape)\n\n\n    def test_type_blocks_fillna_leading_b(self) -> None:\n\n        a1 = np.array([\n                [nan, nan,3, 4],\n                [nan, nan, 6, nan],\n                [5, nan, nan, nan]\n                ], dtype=float)\n        a2 = np.array([nan, nan, nan], dtype=object)\n\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        tb2 = tb1.fillna_leading(0)\n\n        self.assertAlmostEqualValues(list(tb2.values.flat),\n                [0.0, 0.0, 3.0, 4.0, 0, 0.0, 0.0, 6.0, nan, 0, 5.0, 0.0, nan, nan, 0])\n\n\n\n    def test_type_blocks_fillna_leading_c(self) -> None:\n\n        a2 = np.array([\n                [None, None, 3, 4],\n                [1, None, None, 6],\n                [5, None, None, None]\n                ], dtype=object)\n        a1 = np.array([1, None, None], dtype=object)\n\n        tb1 = TypeBlocks.from_blocks((a2, a1))\n\n        tb2 = tb1.fillna_leading(-1, axis=1)\n        self.assertEqual(tb2.values.tolist(),\n                [[-1, -1, 3, 4, 1],\n                [1, None, None, 6, None],\n                [5, None, None, None, None]])\n\n    def test_type_blocks_fillna_leading_d(self) -> None:\n\n        a2 = np.array([\n                [None, None, 3, 4],\n                [None, None, None, 6],\n                [None, None, None, None]\n                ], dtype=object)\n        a1 = np.array([1, None, None], dtype=object)\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n        tb2 = tb1.fillna_leading(0, axis=1)\n\n        self.assertEqual(tb2.values.tolist(),\n                [[1, None, None, 3, 4],\n                [0, 0, 0, 0, 6],\n                [0, 0, 0, 0, 0]])\n\n    def test_type_blocks_fillna_leading_e(self) -> None:\n\n        a2 = np.array([\n                [None, None, None, None],\n                [None, 1, None, 6],\n                [None, 5, None, None]\n                ], dtype=object)\n        a1 = np.array([None, None, None], dtype=object)\n        a3 = np.array([\n                [None, 4],\n                [None, 1],\n                [None, 5]\n                ], dtype=object)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n        tb2 = tb1.fillna_leading(0, axis=1)\n\n        self.assertEqual(tb2.values.tolist(),\n                [[0, 0, 0, 0, 0, 0, 4],\n                [0, 0, 1, None, 6, None, 1],\n                [0, 0, 5, None, None, None, 5]])\n\n\n    def test_type_blocks_fillna_leading_f(self) -> None:\n\n        a1 = np.array([None, None, None], dtype=object)\n        tb1 = TypeBlocks.from_blocks((a1,))\n        with self.assertRaises(NotImplementedError):\n            tb1.fillna_leading(value=3, axis=2)\n\n\n    #---------------------------------------------------------------------------\n\n    def test_type_blocks_fillna_forward_a(self) -> None:\n\n        for axis in (0, 1):\n            for arrays in self.get_arrays_b():\n                tb = TypeBlocks.from_blocks(arrays)\n                post1 = tb.fillna_forward(axis=axis)\n                self.assertEqual(tb.shape, post1.shape)\n\n                post2 = tb.fillna_backward(axis=axis)\n                self.assertEqual(tb.shape, post2.shape)\n\n\n\n    def test_type_blocks_fillna_forward_b(self) -> None:\n\n        a1 = np.array([\n                [nan, nan,3, 4],\n                [nan, nan, 6, nan],\n                [5, nan, nan, nan]\n                ], dtype=float)\n        a2 = np.array([nan, nan, nan], dtype=object)\n\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n        tb2 = tb1.fillna_forward()\n\n        self.assertEqual(\n                tb2.fillna(0).values.tolist(),\n                [[0.0, 0.0, 3.0, 4.0, 0],\n                [0.0, 0.0, 6.0, 4.0, 0],\n                [5.0, 0.0, 6.0, 4.0, 0]]\n                )\n\n        tb3 = tb1.fillna_backward()\n        self.assertEqual(tb3.fillna(0).values.tolist(),\n                [[5.0, 0.0, 3.0, 4.0, 0],\n                [5.0, 0.0, 6.0, 0.0, 0],\n                [5.0, 0.0, 0.0, 0.0, 0]]\n                )\n\n    def test_type_blocks_fillna_forward_c(self) -> None:\n\n        a1 = np.array([\n                [nan, nan,3, 4],\n                [nan, nan, 6, nan],\n                [5, nan, nan, nan]\n                ], dtype=float)\n        a2 = np.array([nan, nan, nan], dtype=object)\n\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n        tb2 = tb1.fillna_forward(axis=1)\n\n        self.assertEqual(\n            tb2.fillna(0).values.tolist(),\n            [[0.0, 0.0, 3.0, 4.0, 4.0], [0.0, 0.0, 6.0, 6.0, 6.0], [5.0, 5.0, 5.0, 5.0, 5.0]]\n        )\n\n        tb3 = tb1.fillna_backward(axis=1)\n        self.assertEqual(\n            tb3.fillna(0).values.tolist(),\n            [[3.0, 3.0, 3.0, 4.0, 0], [6.0, 6.0, 6.0, 0, 0], [5.0, 0, 0, 0, 0]]\n            )\n\n    def test_type_blocks_fillna_forward_d(self) -> None:\n\n        a1 = np.array([\n                [None, 10, None],\n                [None, 88, None],\n                [None, 40, None]\n                ], dtype=object)\n        a2 = np.array([None, None, None], dtype=object)\n        a3 = np.array([543, 601, 234], dtype=object)\n\n        tb1 = TypeBlocks.from_blocks((a3, a2, a1))\n        tb2 = tb1.fillna_forward(axis=1)\n\n        self.assertEqual(tb2.values.tolist(),\n                [[543, 543, 543, 10, 10],\n                [601, 601, 601, 88, 88],\n                [234, 234, 234, 40, 40]]\n                )\n\n        tb3 = tb1.fillna_backward(axis=1)\n        self.assertEqual(tb3.values.tolist(),\n                [[543, 10, 10, 10, None],\n                [601, 88, 88, 88, None],\n                [234, 40, 40, 40, None]]\n                )\n\n    def test_type_blocks_fillna_forward_e(self) -> None:\n\n        a1 = np.array([None, None, None], dtype=object)\n        a2 = np.array([None, 8, None], dtype=object)\n        a3 = np.array([543, 601, 234], dtype=object)\n        a4 = np.array([30, None, 74], dtype=object)\n\n        tb1 = TypeBlocks.from_blocks((a2, a1, a1, a4, a1, a3, a1))\n\n        # axis 1 tests\n        self.assertEqual(tb1.fillna_forward(axis=1).values.tolist(),\n                [[None, None, None, 30, 30, 543, 543],\n                [8, 8, 8, 8, 8, 601, 601],\n                [None, None, None, 74, 74, 234, 234]])\n\n        self.assertEqual(tb1.fillna_backward(axis=1).values.tolist(),\n                [[30, 30, 30, 30, 543, 543, None],\n                [8, 601, 601, 601, 601, 601, None],\n                [74, 74, 74, 74, 234, 234, None]]\n                )\n\n\n    def test_type_blocks_fillna_forward_f(self) -> None:\n\n        a1 = np.array([None, None, 40], dtype=object)\n        a2 = np.array([None, 8, None], dtype=object)\n        a3 = np.array([543, 601, 234], dtype=object)\n        a4 = np.array([30, None, 74], dtype=object)\n\n        tb1 = TypeBlocks.from_blocks((a2, a1, a4, a3))\n\n        self.assertEqual(\n                tb1.fillna_forward().values.tolist(),\n                [[None, None, 30, 543],\n                [8, None, 30, 601],\n                [8, 40, 74, 234]]\n                )\n        self.assertEqual(tb1.fillna_backward().values.tolist(),\n                [[8, 40, 30, 543],\n                [8, 40, 74, 601],\n                [None, 40, 74, 234]]\n                )\n\n\n\n\n    def test_type_blocks_fillna_forward_g(self) -> None:\n\n        a1 = np.array([None, None, None], dtype=object)\n        a2 = np.array([None, 8, None], dtype=object)\n        a3 = np.array([543, 601, 234], dtype=object)\n        a4 = np.array([30, None, 74], dtype=object)\n\n        tb1 = TypeBlocks.from_blocks((a2, a1, a1, a4, a1, a1, a3, a1))\n\n        self.assertEqual(tb1.fillna_forward(limit=1, axis=1).values.tolist(),\n            [[None, None, None, 30, 30, None, 543, 543], [8, 8, None, None, None, None, 601, 601], [None, None, None, 74, 74, None, 234, 234]]\n            )\n\n        self.assertEqual(tb1.fillna_forward(limit=2, axis=1).values.tolist(),\n            [[None, None, None, 30, 30, 30, 543, 543], [8, 8, 8, None, None, None, 601, 601], [None, None, None, 74, 74, 74, 234, 234]])\n\n\n        self.assertEqual(tb1.fillna_forward(limit=3, axis=1).values.tolist(),\n            [[None, None, None, 30, 30, 30, 543, 543], [8, 8, 8, 8, None, None, 601, 601], [None, None, None, 74, 74, 74, 234, 234]]\n            )\n\n        self.assertEqual(tb1.fillna_backward(limit=1, axis=1).values.tolist(),\n            [[None, None, 30, 30, None, 543, 543, None], [8, None, None, None, None, 601, 601, None], [None, None, 74, 74, None, 234, 234, None]])\n\n\n\n    def test_type_blocks_fillna_forward_h(self) -> None:\n\n        a1 = np.array([\n                [None, None, 10, None],\n                [None, None, 88, None],\n                [None, None, 40, None]\n                ], dtype=object)\n        a2 = np.array([\n                [None, 3, None, None],\n                [None, None, 4, None],\n                [3, None, None, None]\n                ], dtype=object)\n        a3 = np.array([None, None, None], dtype=object)\n        a4 = np.array([543, 601, 234], dtype=object)\n\n        tb1 = TypeBlocks.from_blocks((a4, a3, a2, a1))\n\n        self.assertEqual(\n                tb1.fillna_forward(axis=1, limit=1).values.tolist(),\n                [[543, 543, None, 3, 3, None, None, None, 10, 10], [601, 601, None, None, 4, 4, None, None, 88, 88], [234, 234, 3, 3, None, None, None, None, 40, 40]])\n\n        self.assertEqual(\n                tb1.fillna_forward(axis=1, limit=3).values.tolist(),\n                [[543, 543, 543, 3, 3, 3, 3, None, 10, 10], [601, 601, 601, 601, 4, 4, 4, 4, 88, 88], [234, 234, 3, 3, 3, 3, None, None, 40, 40]])\n\n\n    def test_type_blocks_fillna_forward_i(self) -> None:\n\n        a1 = np.array([\n                [None, None, 10, None],\n                [23, None, 88, None],\n                [None, None, 40, None]\n                ], dtype=object)\n        a2 = np.array([\n                [None, 3, None, None],\n                [None, None, 4, None],\n                [3, None, None, None]\n                ], dtype=object)\n        a3 = np.array([None, None, None], dtype=object)\n        a4 = np.array([543, 601, 234], dtype=object)\n\n        tb1 = TypeBlocks.from_blocks((a4, a3, a2, a1))\n\n        self.assertEqual(\n                tb1.fillna_backward(axis=1, limit=1).values.tolist(),\n            \t[[543, None, 3, 3, None, None, None, 10, 10, None], [601, None, None, 4, 4, 23, 23, 88, 88, None], [234, 3, 3, None, None, None, None, 40, 40, None]]\n                )\n\n        self.assertEqual(\n                tb1.fillna_backward(axis=1, limit=3).values.tolist(),\n                [[543, 3, 3, 3, None, 10, 10, 10, 10, None], [601, 4, 4, 4, 4, 23, 23, 88, 88, None], [234, 3, 3, None, None, 40, 40, 40, 40, None]]\n                )\n\n        self.assertEqual(\n                tb1.fillna_backward(axis=1, limit=2).values.tolist(),\n                [[543, 3, 3, 3, None, None, 10, 10, 10, None], [601, None, 4, 4, 4, 23, 23, 88, 88, None], [234, 3, 3, None, None, None, 40, 40, 40, None]])\n\n\n\n    def test_type_blocks_fillna_forward_j(self) -> None:\n\n\n        a2 = np.array([None, None, None])\n        a3 = np.array([543, 601, 234])\n        tb1 = TypeBlocks.from_blocks((a3, a2,))\n\n        with self.assertRaises(AxisInvalid):\n            tb1.fillna_forward(axis=3)\n\n        with self.assertRaises(AxisInvalid):\n            tb1.fillna_backward(axis=3)\n\n    #---------------------------------------------------------------------------\n    def test_type_blocks_from_none_a(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, np.nan, 6], [0, 0, 1]], dtype=object)\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n        a3 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        a4 = np.array([None, None, None])\n\n        tb1 = TypeBlocks.from_zero_size_shape((3, 0))\n        tb1.append(a1)\n        self.assertEqual(tb1.shape, (3, 3))\n        tb1.append(a4)\n        self.assertEqual(tb1.shape, (3, 4))\n\n        tb1 = TypeBlocks.from_zero_size_shape((3, 0))\n        tb1.append(a4)\n        self.assertEqual(tb1.shape, (3, 1))\n        tb1.append(a1)\n        self.assertEqual(tb1.shape, (3, 4))\n\n    def test_type_blocks_from_none_b(self) -> None:\n\n        with self.assertRaises(RuntimeError):\n            tb1 = TypeBlocks.from_zero_size_shape((1, 5))\n\n        with self.assertRaises(RuntimeError):\n            tb1 = TypeBlocks.from_zero_size_shape((5, 1))\n\n    def test_type_blocks_from_none_c(self) -> None:\n\n        for shape in ((0, 3), (3, 0), (0, 0)):\n            tb1 = TypeBlocks.from_zero_size_shape(shape)\n            self.assertEqual(tb1.shape, shape)\n            self.assertEqual(tb1.values.shape, shape)\n            self.assertEqual(tb1.size, 0)\n            self.assertEqual(tb1.nbytes, 0)\n            self.assertEqual(len(tb1), tb1.shape[0])\n\n\n    def test_type_blocks_datetime64_a(self) -> None:\n\n        d = np.datetime64\n        a1 = np.array([d('2018-01-01'), d('2018-01-02'), d('2018-01-03')])\n        a2 = np.array([d('2017-01-01'), d('2017-01-02'), d('2017-01-03')])\n        a3 = np.array([d('2016-01-01'), d('2016-01-02'), d('2018-01-03')])\n\n\n        tb1 = TypeBlocks.from_zero_size_shape((3, 0))\n        tb1.append(a1)\n        tb1.append(a2)\n        tb1.append(a3)\n\n        self.assertEqual(list(tb1._reblock_signature()),\n            [(np.dtype('<M8[D]'), 3)])\n\n        tb2 = tb1.consolidate()\n\n        self.assertEqual(list(tb2._reblock_signature()),\n            [(np.dtype('<M8[D]'), 3)])\n\n        self.assertEqual(len(tb2._blocks), 1)\n\n\n    def test_type_blocks_resize_blocks_a(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        index_ic = IndexCorrespondence(has_common=True,\n                is_subset=True,\n                iloc_src=np.array((1, 2)),\n                iloc_dst=np.array((0, 2)),\n                size=3)\n\n        tb2 = TypeBlocks.from_blocks(tb1.resize_blocks(index_ic=index_ic, columns_ic=None, fill_value=None))\n        self.assertEqual(tb2.shape, (2, 3))\n\n\n\n    def test_type_blocks_astype_a(self) -> None:\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        tb2 = TypeBlocks.from_blocks(tb1._astype_blocks(slice(0, 2), bool))\n\n        self.assertTypeBlocksArrayEqual(tb2,\n                [[True, True, 3, False, False, True],\n                [True, True, 6, True, False, True],\n                [False, False, 1, True, False, True]])\n\n        tb3 = TypeBlocks.from_blocks(tb1._astype_blocks(slice(1, 3), bool))\n        self.assertTypeBlocksArrayEqual(tb3,\n                [[1, True, True, False, False, True],\n                [4, True, True, True, False, True],\n                [0, False, True, True, False, True]]\n                )\n\n        tb4 = TypeBlocks.from_blocks(tb1._astype_blocks([0, 2, 4], bool))\n        self.assertTypeBlocksArrayEqual(tb4,\n                [[True, 2, True, False, False, True],\n                [True, 5, True, True, False, True],\n                [False, 0, True, True, False, True]]\n                )\n\n        tb5 = TypeBlocks.from_blocks(tb1._astype_blocks([4, 2, 0], bool))\n        self.assertTypeBlocksArrayEqual(tb4,\n                [[True, 2, True, False, False, True],\n                [True, 5, True, True, False, True],\n                [False, 0, True, True, False, True]]\n                )\n\n        tb6 = TypeBlocks.from_blocks(tb1._astype_blocks(4, int))\n        self.assertTypeBlocksArrayEqual(tb6,\n                [[1, 2, 3, False, 0, True],\n                [4, 5, 6, True, 0, True],\n                [0, 0, 1, True, 0, True]]\n                )\n\n\n    def test_type_blocks_astype_b(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([4, 5, 6])\n        a3 = np.array([False, False, True])\n        a4 = np.array([True, False, True])\n\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3, a4))\n\n        tb2 = TypeBlocks.from_blocks(tb1._astype_blocks(slice(2, None), int))\n\n        self.assertTypeBlocksArrayEqual(tb2,\n                [[1, 4, 0, 1],\n                [2, 5, 0, 0],\n                [3, 6, 1, 1]]\n                )\n\n        tb3 = TypeBlocks.from_blocks(tb1._astype_blocks([0, 1], bool))\n\n        self.assertTypeBlocksArrayEqual(tb3,\n                [[True, True, False, True],\n                [True, True, False, False],\n                [True, True, True, True]]\n                )\n\n\n    def test_type_blocks_drop_blocks_a(self) -> None:\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n\n        tb2 = TypeBlocks.from_blocks(tb1._drop_blocks(column_key=slice(0, 2)))\n        self.assertEqual(tb2.shape, (3, 4))\n        self.assertTrue(tb1.mloc[1] == tb2.mloc[1])\n\n        tb4 = TypeBlocks.from_blocks(tb1._drop_blocks(column_key=[0, 2, 4]))\n        self.assertTypeBlocksArrayEqual(tb4,\n                [[2, False, True],\n                [5, True, True],\n                [0, True, True]]\n                )\n\n        self.assertTypeBlocksArrayEqual(tb2,\n                [[3, False, False, True],\n                [6, True, False, True],\n                [1, True, False, True]])\n\n        tb3 = TypeBlocks.from_blocks(tb1._drop_blocks(column_key=slice(1, 3)))\n        self.assertTypeBlocksArrayEqual(tb3,\n                [[1, False, False, True],\n                [4, True, False, True],\n                [0, True, False, True]]\n                )\n\n\n        tb5 = TypeBlocks.from_blocks(tb1._drop_blocks(column_key=[4, 2, 0]))\n        self.assertTypeBlocksArrayEqual(tb4,\n                [[2, False, True],\n                [5, True, True],\n                [0, True, True]]\n                )\n\n        tb6 = TypeBlocks.from_blocks(tb1._drop_blocks(column_key=4))\n        self.assertTypeBlocksArrayEqual(tb6,\n                [[1, 2, 3, False, True],\n                [4, 5, 6, True, True],\n                [0, 0, 1, True, True]]\n                )\n        self.assertTrue(tb1.mloc[0] == tb6.mloc[0])\n\n\n    def test_type_blocks_drop_blocks_b(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([4, 5, 6])\n        a3 = np.array([False, False, True])\n        a4 = np.array([True, False, True])\n\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3, a4))\n\n        tb2 = TypeBlocks.from_blocks(tb1._drop_blocks(column_key=slice(2, None)))\n\n        self.assertTypeBlocksArrayEqual(tb2,\n                [[1, 4],\n                [2, 5],\n                [3, 6]]\n                )\n\n        tb3 = TypeBlocks.from_blocks(tb1._drop_blocks(column_key=[0, 1]))\n\n        self.assertTypeBlocksArrayEqual(tb3,\n                [[False, True],\n                [False, False],\n                [True, True]]\n                )\n\n\n    def test_type_blocks_drop_blocks_c(self) -> None:\n        a1 = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 1]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        tb2 = TypeBlocks.from_blocks(tb1._drop_blocks(row_key=-1,\n                column_key=slice(0, 2)))\n\n        self.assertTypeBlocksArrayEqual(tb2,\n                [[3, False, False, True],\n                [6, True, False, True]])\n\n        tb3 = TypeBlocks.from_blocks(tb1._drop_blocks(row_key=[0,2],\n                column_key=slice(1, 3)))\n\n        self.assertTypeBlocksArrayEqual(tb3,\n                [[4, True, False, True]]\n                )\n\n\n    def test_type_blocks_drop_blocks_d(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([4, 5, 6])\n        a3 = np.array([False, False, True])\n        a4 = np.array([True, False, True])\n\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3, a4))\n\n        tb2 = TypeBlocks.from_blocks(tb1._drop_blocks(row_key=1, column_key=[0, 3]))\n\n        self.assertTypeBlocksArrayEqual(tb2,\n                [[4, False], [6, True]]\n                )\n\n        tb3 = TypeBlocks.from_blocks(tb1._drop_blocks(row_key=1))\n\n        self.assertTypeBlocksArrayEqual(tb3,\n                [[1, 4, False, True],\n                [3, 6, True, True]]\n                )\n\n\n    def test_type_blocks_drop_blocks_e(self) -> None:\n\n\n        for arrays in self.get_arrays_a():\n            tb1 = TypeBlocks.from_blocks(arrays)\n\n            for i in range(tb1.shape[1]):\n                tb2 = TypeBlocks.from_blocks(tb1._drop_blocks(column_key=i))\n                self.assertTrue(tb2.shape == (3, tb1.shape[1] - 1))\n\n\n\n    def test_type_blocks_drop_blocks_f(self) -> None:\n        a1 = np.array([[1], [5], [0]])\n        a2 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n        self.assertEqual(tb1.shape, (3, 4))\n\n        tb2 = TypeBlocks.from_blocks(tb1._drop_blocks(column_key=slice(0, 2)))\n        self.assertEqual(tb2.shape, (3, 2))\n        self.assertTrue((tb1[2:].values == tb2.values).all())\n\n\n    def test_type_blocks_drop_blocks_g(self) -> None:\n        a1 = np.array([[1], [5], [0]])\n        a2 = np.array([[2], [6], [10]])\n        a3 = np.array([[3], [7], [11]])\n        a4 = np.array([[4], [8], [2]])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3, a4))\n        self.assertEqual(tb1.shape, (3, 4))\n\n        tb2 = TypeBlocks.from_blocks(tb1._drop_blocks(column_key=slice(0, 2)))\n        self.assertEqual(tb2.shape, (3, 2))\n        self.assertTrue((tb1[2:].values == tb2.values).all())\n\n\n\n    def test_type_blocks_drop_blocks_h(self) -> None:\n        a1 = np.array([[False]])\n        tb1 = TypeBlocks.from_blocks(a1)\n        self.assertEqual(tb1.shape, (1, 1))\n        self.assertEqual(tb1.drop(0).shape, (0, 1))\n        self.assertEqual(tb1.drop((None, 0)).shape, (1, 0))\n\n        # after no rows remain, trying to drop more rows\n        tb2 = tb1.drop(0)\n        with self.assertRaises(IndexError):\n            tb2.drop(0) # raise from NumPy\n\n        tb3 = tb1.drop((None, 0))\n\n        # after no columns remain, tyring to drop more should raise an exception\n        with self.assertRaises(IndexError):\n            tb3.drop((None, 0))\n\n\n    def test_type_blocks_pickle_a(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'])\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        pbytes = pickle.dumps(tb1)\n        tb2 = pickle.loads(pbytes)\n\n        self.assertEqual([b.flags.writeable for b in tb2._blocks],\n                [False, False, False]\n                )\n\n\n    #---------------------------------------------------------------------------\n    def test_type_blocks_roll_blocks_a(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, -1, 6], [0, 0, 1]], dtype=object)\n        a2 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        a3 = np.array([None, None, None])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertTypeBlocksArrayEqual(\n                TypeBlocks.from_blocks(tb1._shift_blocks(1, 1, wrap=True)),\n                [[None, 0, 0, 1, 'oe', 'od'],\n                [None, 1, 2, 3, 'a', 'b'],\n                [None, 4, -1, 6, 'c', 'd']]\n                )\n\n        self.assertTypeBlocksArrayEqual(\n                TypeBlocks.from_blocks(tb1._shift_blocks(-1, -1, wrap=True)),\n                [[-1, 6, 'c', 'd', None, 4],\n                [0, 1, 'oe', 'od', None, 0],\n                [2, 3, 'a', 'b', None, 1]]\n                )\n\n        self.assertTypeBlocksArrayEqual(\n                TypeBlocks.from_blocks(tb1._shift_blocks(-2, 2, wrap=True)),\n                [['od', None, 0, 0, 1, 'oe'],\n                ['b', None, 1, 2, 3, 'a'],\n                ['d', None, 4, -1, 6, 'c']]\n                )\n\n\n    def test_type_blocks_roll_blocks_b(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, -1, 6], [0, 0, 1]], dtype=object)\n        a2 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        a3 = np.array([None, None, None])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        # import ipdb; ipdb.set_trace()\n        self.assertTypeBlocksArrayEqual(\n                TypeBlocks.from_blocks(tb1._shift_blocks(1, 1, wrap=False,fill_value='x')),\n                [['x', 'x', 'x', 'x', 'x', 'x'],\n                ['x', 1, 2, 3, 'a', 'b'],\n                ['x', 4, -1, 6, 'c', 'd']],\n                match_dtype=object\n                )\n\n        self.assertTypeBlocksArrayEqual(\n                TypeBlocks.from_blocks(tb1._shift_blocks(2,\n                        -2,\n                        wrap=False,\n                        fill_value=10)),\n                [[10, 10, 10, 10, 10, 10],\n                [10, 10, 10, 10, 10, 10],\n                [3, 'a', 'b', None, 10, 10]],\n                match_dtype=object\n                )\n\n    def test_type_blocks_roll_blocks_c(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        self.assertEqual(\n                TypeBlocks.from_blocks(tb1._shift_blocks(0, 0, True)).values.tolist(),\n                [[1, 'a', 'b'], [2, 'c', 'd'], [3, 'oe', 'od']]\n                )\n\n    def test_type_blocks_roll_blocks_d(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([['a', 'b'], ['c', 'd'], ['oe', 'od']])\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        self.assertEqual(\n                TypeBlocks.from_blocks(tb1._shift_blocks(0, 0, False)).values.tolist(),\n                [[1, 'a', 'b'], [2, 'c', 'd'], [3, 'oe', 'od']]\n                )\n\n\n    #---------------------------------------------------------------------------\n    def test_type_blocks_from_blocks_a(self) -> None:\n\n        a1 = np.full((3, 0), False)\n        a2 = np.full((3, 4), 'x')\n        tb = TypeBlocks.from_blocks((a1, a2))\n        self.assertEqual(tb.shape, (3, 4))\n        self.assertEqual(tb.values.tolist(),\n            [['x', 'x', 'x', 'x'],\n            ['x', 'x', 'x', 'x'],\n            ['x', 'x', 'x', 'x']])\n\n        a3 = next(tb.axis_values(0))\n        self.assertEqual(a3.tolist(),\n            ['x', 'x', 'x']\n            )\n\n    def test_type_blocks_from_blocks_b(self) -> None:\n\n        a1 = np.full((3, 0), False)\n        a2 = np.full((3, 2), 'x')\n        a3 = np.full((3, 0), False)\n        a4 = np.full((3, 2), 'y')\n        a5 = np.full((3, 0), False)\n\n        tb = TypeBlocks.from_blocks((a1, a2, a3, a4, a5))\n        self.assertEqual(tb.shape, (3, 4))\n        self.assertEqual(tb.values.tolist(),\n            [['x', 'x', 'y', 'y'],\n            ['x', 'x', 'y', 'y'],\n            ['x', 'x', 'y', 'y']])\n\n        a3 = next(tb.axis_values(0, reverse=True))\n        self.assertEqual(a3.tolist(),\n            ['y', 'y', 'y']\n            )\n\n\n    def test_type_blocks_from_blocks_c(self) -> None:\n\n        a1 = np.full((0, 2), False)\n        a2 = np.full((0, 1), 'x')\n\n        tb = TypeBlocks.from_blocks((a1, a2))\n        self.assertEqual(tb.shape, (0, 3))\n        self.assertEqual(len(tb), 0)\n\n        self.assertEqual(tb.dtypes.tolist(),\n            [np.dtype('bool'), np.dtype('bool'), np.dtype('<U1')])\n\n        tb.append(np.empty((0, 2)))\n        self.assertEqual(tb.shape, (0, 5))\n        self.assertEqual(len(tb), 0)\n\n        with self.assertRaises(RuntimeError):\n            tb.append(np.empty((3, 0)))\n\n        # import ipdb; ipdb.set_trace()\n\n\n    def test_type_blocks_from_blocks_d(self) -> None:\n\n        a1 = np.full(0, False)\n        a2 = np.full(0, 'x')\n\n        # if the row lentgh is defined as 0, adding 1D arrays, even if empty, count as adding a column, as a 1D array is by definition shape (0, 1)\n        tb = TypeBlocks.from_blocks((a1, a2))\n        self.assertEqual(tb.shape, (0, 2))\n        self.assertEqual(len(tb), 0)\n        self.assertEqual(len(tb.shapes), 2)\n\n        tb.append(np.empty((0, 2)))\n        self.assertEqual(tb.shape, (0, 4))\n        self.assertEqual(len(tb.shapes), 3)\n\n        tb.append(np.empty((0, 0)))\n        self.assertEqual(tb.shape, (0, 4))\n        self.assertEqual(len(tb.shapes), 3)\n\n        tb.append(np.empty(0))\n        self.assertEqual(tb.shape, (0, 5))\n        self.assertEqual(len(tb.shapes), 4)\n\n\n    def test_type_blocks_append_a(self) -> None:\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n        self.assertTrue(tb1.shape, (3, 2))\n\n        tb1.append(np.array((3,5,4)))\n        self.assertTrue(tb1.shape, (3, 3))\n\n        tb1.append(np.array([(3,5),(4,6),(5,10)]))\n        self.assertTrue(tb1.shape, (3, 5))\n\n        self.assertEqual(tb1.iloc[0].values.tolist(), [[1, False, 3, 3, 5]])\n        self.assertEqual(tb1.iloc[1].values.tolist(), [[2, True, 5, 4, 6]])\n        self.assertEqual(tb1.iloc[:, 3].values.tolist(), [[3], [4], [5]])\n\n\n    def test_type_blocks_append_b(self) -> None:\n\n        a1 = np.full((2, 3), False)\n\n        tb = TypeBlocks.from_blocks(a1)\n        tb.append(np.empty((2, 0)))\n\n        self.assertEqual(tb.shape, (2, 3))\n        # array was not added\n        self.assertEqual(len(tb.shapes), 1)\n\n\n    def test_type_blocks_append_c(self) -> None:\n\n        a1 = np.full((0, 3), False)\n\n        tb = TypeBlocks.from_blocks(a1)\n        tb.append(np.empty((0, 0)))\n\n        self.assertEqual(tb.shape, (0, 3))\n        # array was not added\n        self.assertEqual(len(tb.shapes), 1)\n\n\n\n    def test_type_blocks_append_d(self) -> None:\n\n        tb = TypeBlocks.from_zero_size_shape((0, 0))\n        tb.append(np.empty((0, 0)))\n\n        self.assertEqual(tb.shape, (0, 0))\n        # array was not added\n        self.assertEqual(len(tb.shapes), 0)\n\n\n    def test_type_blocks_append_e(self) -> None:\n\n        # given a zero row TB, appending a zero length array could mean changing the shape, as the row aligns\n        tb = TypeBlocks.from_zero_size_shape((0, 0))\n        tb.append(np.empty(0))\n\n        self.assertEqual(tb.shape, (0, 1))\n        # array was not added\n        self.assertEqual(len(tb.shapes), 1)\n\n\n\n\n    def test_type_blocks_extract_bloc_assign_a(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, -5, 6], [0, 0, 1]])\n        a2 = np.array([1.5, 5.2, 5.5])\n        a3 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        coords = ((1, 1), (2, 4), (0, 3))\n\n        targets = np.full(tb1.shape, False)\n        for coord in coords:\n            targets[coord] = True\n\n        tb2 = tb1.extract_bloc_assign(targets, None)\n        self.assertEqual(tb2.values.tolist(),\n                [[1, 2, 3, None, False, False, True], [4, None, 6, 5.2, True, False, True], [0, 0, 1, 5.5, None, False, True]]\n                )\n\n\n\n    def test_type_blocks_extract_bloc_assign_b(self) -> None:\n\n        a1 = np.array([[1, 2, 3], [4, -5, 6], [0, 0, 1]])\n        a2 = np.array([1.5, 5.2, 5.5])\n        a3 = np.array([[False, False, True], [True, False, True], [True, False, True]])\n\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        coords = ((1, 1), (0, 4), (2, 5), (0, 3), (1, 3), (1, 6 ))\n\n        targets = np.full(tb1.shape, False)\n        for coord in coords:\n            targets[coord] = True\n\n        values = np.arange(np.prod(tb1.shape)).reshape(tb1.shape) * -100\n        tb2 = tb1.extract_bloc_assign(targets, values)\n\n        self.assertEqual(tb2.values.tolist(),\n            [[1, 2, 3, -300.0, -400, False, True], [4, -800, 6, -1000.0, True, False, -1300], [0, 0, 1, 5.5, True, -1900, True]]\n            )\n\n    #---------------------------------------------------------------------------\n    def test_type_blocks_round_a(self) -> None:\n\n        a1 = np.full(4, .33333, )\n        a2 = np.full((4, 2), .88888, )\n        a3 = np.full(4, .55555)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        tb2 = round(tb1, 3) #type: ignore\n        self.assertEqual(\n                tb2.values.tolist(),\n                [[0.333, 0.889, 0.889, 0.556], [0.333, 0.889, 0.889, 0.556], [0.333, 0.889, 0.889, 0.556], [0.333, 0.889, 0.889, 0.556]]\n                )\n        tb3 = round(tb1, 1) #type: ignore\n        self.assertEqual(\n                tb3.values.tolist(),\n                [[0.3, 0.9, 0.9, 0.6], [0.3, 0.9, 0.9, 0.6], [0.3, 0.9, 0.9, 0.6], [0.3, 0.9, 0.9, 0.6]]\n                )\n\n\n    #---------------------------------------------------------------------------\n\n    def test_type_blocks_ufunc_blocks_a(self) -> None:\n\n        a1 = np.arange(8).reshape(2, 4)\n        tb1 = TypeBlocks.from_blocks(a1)\n\n        ufunc = lambda x: x * 2\n        tb2 = TypeBlocks.from_blocks(tb1._ufunc_blocks(NULL_SLICE, ufunc))\n\n        self.assertEqual(tb2.values.tolist(),\n                [[0, 2, 4, 6], [8, 10, 12, 14]])\n\n        tb3 = TypeBlocks.from_blocks(tb1._ufunc_blocks(1, ufunc))\n        self.assertEqual(tb3.values.tolist(),\n                [[0, 2, 2, 3], [4, 10, 6, 7]])\n\n\n    def test_type_blocks_ufunc_blocks_b(self) -> None:\n\n        a1 = np.arange(3)\n        a2 = np.arange(3)\n        a3 = np.arange(3)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        ufunc = lambda x: x * 2\n        tb2 = TypeBlocks.from_blocks(tb1._ufunc_blocks(NULL_SLICE, ufunc))\n\n        self.assertEqual(tb2.values.tolist(),\n                [[0, 0, 0], [2, 2, 2], [4, 4, 4]])\n\n        tb3 = TypeBlocks.from_blocks(tb1._ufunc_blocks(1, ufunc))\n        self.assertEqual(tb3.values.tolist(),\n                [[0, 0, 0], [1, 2, 1], [2, 4, 2]])\n\n\n    def test_type_blocks_ufunc_blocks_c(self) -> None:\n\n        a1 = np.arange(3)\n        a2 = np.arange(6).reshape(3, 2)\n        a3 = np.arange(3)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        ufunc = lambda x: x * 2\n        tb2 = TypeBlocks.from_blocks(tb1._ufunc_blocks(NULL_SLICE, ufunc))\n        self.assertEqual(tb2.values.tolist(),\n                [[0, 0, 2, 0], [2, 4, 6, 2], [4, 8, 10, 4]])\n\n        tb3 = TypeBlocks.from_blocks(tb1._ufunc_blocks(slice(2,4), ufunc))\n        self.assertEqual(tb3.values.tolist(),\n                [[0, 0, 2, 0], [1, 2, 6, 2], [2, 4, 10, 4]])\n\n\n    #---------------------------------------------------------------------------\n    def test_type_blocks_getitem_a(self) -> None:\n\n        a1 = np.arange(3)\n        a2 = np.arange(10, 16).reshape(3, 2)\n        a3 = np.arange(20, 23)\n        tb1 = TypeBlocks.from_blocks((a1, a2, a3))\n\n        self.assertEqual(tb1[2].values.tolist(),\n                [[11], [13], [15]])\n\n        with self.assertRaises(KeyError):\n            _ = tb1[2, 2]\n\n        self.assertEqual(\n                tb1[2:].values.tolist(),\n                [[11, 20], [13, 21], [15, 22]]\n                )\n\n    #---------------------------------------------------------------------------\n    def test_type_blocks_equal_a(self) -> None:\n\n        a1 = np.array([[1, 10], [2, 20], [3, 30]])\n        a2 = np.array([False, True, False])\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        a3 = np.array([1, 2, 3])\n        a4 = np.array([10, 20, 30])\n        a5 = np.array([False, True, False])\n        tb2 = TypeBlocks.from_blocks((a3, a4, a5))\n\n        self.assertTrue(tb1.equals(tb2))\n        self.assertTrue(tb2.equals(tb1))\n        self.assertTrue(tb1.equals(tb1))\n\n        a6 = np.array([1, 2, 3])\n        a7 = np.array([10, 21, 30])\n        a8 = np.array([False, True, False])\n        tb3 = TypeBlocks.from_blocks((a6, a7, a8))\n\n        self.assertFalse(tb1.equals(tb3))\n        self.assertFalse(tb3.equals(tb1))\n\n\n    def test_type_blocks_equal_b(self) -> None:\n\n        a1 = np.array([[1, 10], [2, 20], [3, 30]])\n        a2 = np.array([False, True, False])\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        self.assertFalse(tb1.equals('a'))\n        self.assertFalse(tb1.equals(1))\n        self.assertFalse(tb1.equals([1, 10]))\n        self.assertFalse(tb1.equals(np.arange(3)))\n        self.assertFalse(tb1.equals(None))\n        self.assertFalse(tb1.equals(dict(a=30, b=40)))\n\n\n    def test_type_blocks_equal_c(self) -> None:\n\n        a1 = np.array([False, True, False])\n        a2 = np.array([10, 20, 30], dtype=np.int64)\n        tb1 = TypeBlocks.from_blocks((a1, a2))\n\n        a3 = np.array([False, True, False])\n        a4 = np.array([10, 20, 30], dtype=np.int32)\n        tb2 = TypeBlocks.from_blocks((a3, a4))\n\n        self.assertFalse(tb1.equals(tb2, compare_dtype=True))\n        self.assertTrue(tb1.equals(tb2, compare_dtype=False))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
static_frame/test/unit/test_util.py,363,"b""\n\nimport unittest\nimport datetime\nimport typing as tp\nfrom enum import Enum\n\nimport numpy as np\n\nfrom static_frame.core.util import isna_array\nfrom static_frame.core.util import resolve_dtype\nfrom static_frame.core.util import resolve_dtype_iter\nfrom static_frame.core.util import array_to_duplicated\nfrom static_frame.core.util import ufunc_set_iter\n\nfrom static_frame.core.util import intersect2d\nfrom static_frame.core.util import union2d\nfrom static_frame.core.util import setdiff2d\nfrom static_frame.core.util import concat_resolved\nfrom static_frame.core.util import _isin_1d\nfrom static_frame.core.util import _isin_2d\nfrom static_frame.core.util import isin\n\nfrom static_frame.core.util import _gen_skip_middle\nfrom static_frame.core.util import dtype_to_na\nfrom static_frame.core.util import key_to_datetime_key\nfrom static_frame.core.util import column_1d_filter\nfrom static_frame.core.util import row_1d_filter\nfrom static_frame.core.util import _read_url\nfrom static_frame.core.util import _ufunc_set_2d\nfrom static_frame.core.util import iterable_to_array_1d\nfrom static_frame.core.util import iterable_to_array_2d\nfrom static_frame.core.util import iterable_to_array_nd\n\nfrom static_frame.core.util import slice_to_ascending_slice\nfrom static_frame.core.util import array_shift\nfrom static_frame.core.util import ufunc_unique\nfrom static_frame.core.util import ufunc_axis_skipna\nfrom static_frame.core.util import to_timedelta64\nfrom static_frame.core.util import binary_transition\n\nfrom static_frame.core.util import roll_1d\nfrom static_frame.core.util import roll_2d\n\nfrom static_frame.core.util import union1d\nfrom static_frame.core.util import intersect1d\nfrom static_frame.core.util import setdiff1d\n\nfrom static_frame.core.util import to_datetime64\nfrom static_frame.core.util import DT64_YEAR\nfrom static_frame.core.util import DT64_DAY\n\nfrom static_frame.core.util import resolve_type_iter\n\nfrom static_frame.core.util import argmin_1d\nfrom static_frame.core.util import argmax_1d\nfrom static_frame.core.util import argmin_2d\nfrom static_frame.core.util import argmax_2d\n\nfrom static_frame.core.util import _array_to_duplicated_sortable\nfrom static_frame.core.util import _ufunc_set_1d\nfrom static_frame.core.util import slices_from_targets\n\n\nfrom static_frame.test.test_case import TestCase\nfrom static_frame.test.test_case import UnHashable\n\n\nclass TestUnit(TestCase):\n\n    def test_gen_skip_middle_a(self) -> None:\n\n        forward = lambda: [3, 2, 5]\n        reverse = lambda: [3, 2, 5]\n\n        post = _gen_skip_middle(\n                forward_iter=forward,\n                forward_count=3,\n                reverse_iter=reverse,\n                reverse_count=3,\n                center_sentinel=-1)\n\n        self.assertEqual(list(post), [3, 2, 5, -1, 5, 2, 3])\n\n        post = _gen_skip_middle(\n                forward_iter=forward,\n                forward_count=2,\n                reverse_iter=reverse,\n                reverse_count=2,\n                center_sentinel=0)\n\n        self.assertEqual(list(post), [3, 2, 0, 2, 3])\n\n\n\n    def test_resolve_dtype_a(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'])\n        a4 = np.array([2.3, 3.2])\n        a5 = np.array(['test', 'test again'], dtype='S')\n        a6 = np.array([2.3,5.4], dtype='float32')\n\n        self.assertEqual(resolve_dtype(a1.dtype, a1.dtype), a1.dtype)\n\n        self.assertEqual(resolve_dtype(a1.dtype, a2.dtype), np.object_)\n        self.assertEqual(resolve_dtype(a2.dtype, a3.dtype), np.object_)\n        self.assertEqual(resolve_dtype(a2.dtype, a4.dtype), np.object_)\n        self.assertEqual(resolve_dtype(a3.dtype, a4.dtype), np.object_)\n        self.assertEqual(resolve_dtype(a3.dtype, a6.dtype), np.object_)\n\n        self.assertEqual(resolve_dtype(a1.dtype, a4.dtype), np.float64)\n        self.assertEqual(resolve_dtype(a1.dtype, a6.dtype), np.float64)\n        self.assertEqual(resolve_dtype(a4.dtype, a6.dtype), np.float64)\n\n    def test_resolve_dtype_b(self) -> None:\n\n        self.assertEqual(\n                resolve_dtype(np.array('a').dtype, np.array('aaa').dtype),\n                np.dtype(('U', 3))\n                )\n\n\n\n    def test_resolve_dtype_c(self) -> None:\n\n\n        a1 = np.array(['2019-01', '2019-02'], dtype=np.datetime64)\n        a2 = np.array(['2019-01-01', '2019-02-01'], dtype=np.datetime64)\n        a3 = np.array([0, 1], dtype='datetime64[ns]')\n        a4 = np.array([0, 1])\n\n        self.assertEqual(str(resolve_dtype(a1.dtype, a2.dtype)),\n                'datetime64[D]')\n        self.assertEqual(resolve_dtype(a1.dtype, a3.dtype),\n                np.dtype('<M8[ns]'))\n\n        self.assertEqual(resolve_dtype(a1.dtype, a4.dtype),\n                np.dtype('O'))\n\n\n\n\n\n    def test_resolve_dtype_iter_a(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'])\n        a4 = np.array([2.3, 3.2])\n        a5 = np.array(['test', 'test again'], dtype='S')\n        a6 = np.array([2.3,5.4], dtype='float32')\n\n        self.assertEqual(resolve_dtype_iter((a1.dtype, a1.dtype)), a1.dtype)\n        self.assertEqual(resolve_dtype_iter((a2.dtype, a2.dtype)), a2.dtype)\n\n        # boolean with mixed types\n        self.assertEqual(resolve_dtype_iter((a2.dtype, a2.dtype, a3.dtype)), np.object_)\n        self.assertEqual(resolve_dtype_iter((a2.dtype, a2.dtype, a5.dtype)), np.object_)\n        self.assertEqual(resolve_dtype_iter((a2.dtype, a2.dtype, a6.dtype)), np.object_)\n\n        # numerical types go to float64\n        self.assertEqual(resolve_dtype_iter((a1.dtype, a4.dtype, a6.dtype)), np.float64)\n\n        # add in bool or str, goes to object\n        self.assertEqual(resolve_dtype_iter((a1.dtype, a4.dtype, a6.dtype, a2.dtype)), np.object_)\n        self.assertEqual(resolve_dtype_iter((a1.dtype, a4.dtype, a6.dtype, a5.dtype)), np.object_)\n\n        # mixed strings go to the largest\n        self.assertEqual(resolve_dtype_iter((a3.dtype, a5.dtype)), np.dtype('<U10'))\n\n\n\n    def test_isna_array_a(self) -> None:\n\n        a1 = np.array([1, 2, 3])\n        a2 = np.array([False, True, False])\n        a3 = np.array(['b', 'c', 'd'])\n        a4 = np.array([2.3, 3.2])\n        a5 = np.array(['test', 'test again'], dtype='S')\n        a6 = np.array([2.3, 5.4], dtype='float32')\n\n        self.assertEqual(isna_array(a1).tolist(), [False, False, False])\n        self.assertEqual(isna_array(a2).tolist(), [False, False, False])\n        self.assertEqual(isna_array(a3).tolist(), [False, False, False])\n        self.assertEqual(isna_array(a4).tolist(), [False, False])\n        self.assertEqual(isna_array(a5).tolist(), [False, False])\n        self.assertEqual(isna_array(a6).tolist(), [False, False])\n\n        a1 = np.array([1, 2, 3, None])\n        a2 = np.array([False, True, False, None])\n        a3 = np.array(['b', 'c', 'd', None])\n        a4 = np.array([2.3, 3.2, None])\n        a5 = np.array(['test', 'test again', None])\n        a6 = np.array([2.3, 5.4, None])\n\n        self.assertEqual(isna_array(a1).tolist(), [False, False, False, True])\n        self.assertEqual(isna_array(a2).tolist(), [False, False, False, True])\n        self.assertEqual(isna_array(a3).tolist(), [False, False, False, True])\n        self.assertEqual(isna_array(a4).tolist(), [False, False, True])\n        self.assertEqual(isna_array(a5).tolist(), [False, False, True])\n        self.assertEqual(isna_array(a6).tolist(), [False, False, True])\n\n        a1 = np.array([1, 2, 3, np.nan])\n        a2 = np.array([False, True, False, np.nan])\n        a3 = np.array(['b', 'c', 'd', np.nan], dtype=object)\n        a4 = np.array([2.3, 3.2, np.nan], dtype=object)\n        a5 = np.array(['test', 'test again', np.nan], dtype=object)\n        a6 = np.array([2.3, 5.4, np.nan], dtype='float32')\n\n        self.assertEqual(isna_array(a1).tolist(), [False, False, False, True])\n        self.assertEqual(isna_array(a2).tolist(), [False, False, False, True])\n        self.assertEqual(isna_array(a3).tolist(), [False, False, False, True])\n        self.assertEqual(isna_array(a4).tolist(), [False, False, True])\n        self.assertEqual(isna_array(a5).tolist(), [False, False, True])\n        self.assertEqual(isna_array(a6).tolist(), [False, False, True])\n\n\n    def test_isna_array_b(self) -> None:\n\n        a1 = np.array([[1, 2], [3, 4]])\n        a2 = np.array([[False, True, False], [False, True, False]])\n        a3 = np.array([['b', 'c', 'd'], ['b', 'c', 'd']])\n        a4 = np.array([[2.3, 3.2, np.nan], [2.3, 3.2, np.nan]])\n        a5 = np.array([['test', 'test again', np.nan],\n                ['test', 'test again', np.nan]], dtype=object)\n        a6 = np.array([[2.3, 5.4, np.nan], [2.3, 5.4, np.nan]], dtype='float32')\n\n        self.assertEqual(isna_array(a1).tolist(),\n                [[False, False], [False, False]])\n\n        self.assertEqual(isna_array(a2).tolist(),\n                [[False, False, False], [False, False, False]])\n\n        self.assertEqual(isna_array(a3).tolist(),\n                [[False, False, False], [False, False, False]])\n\n        self.assertEqual(isna_array(a4).tolist(),\n                [[False, False, True], [False, False, True]])\n\n        self.assertEqual(isna_array(a5).tolist(),\n                [[False, False, True], [False, False, True]])\n\n        self.assertEqual(isna_array(a6).tolist(),\n                [[False, False, True], [False, False, True]])\n\n\n    def test_array_to_duplicated_a(self) -> None:\n        a = array_to_duplicated(\n                np.array([0,1,2,2,1,4,5,3,4,5,5,6]),\n                exclude_first=False,\n                exclude_last=False\n                )\n        self.assertEqual(a.tolist(),\n                [False, True, True, True, True, True, True, False, True, True, True, False])\n\n        a = array_to_duplicated(\n                np.array([0,1,2,2,1,4,5,3,4,5,5,6]),\n                exclude_first=True,\n                exclude_last=False\n                )\n        self.assertEqual(a.tolist(),\n                [False, False, False, True, True, False, False, False, True, True, True, False])\n\n\n    def test_array_to_duplicated_b(self) -> None:\n        a = np.array([[50, 50, 32, 17, 17], [2,2,1,3,3]])\n        # find duplicate rows\n        post = array_to_duplicated(a, axis=0)\n        self.assertEqual(post.tolist(),\n                [False, False])\n\n        post = array_to_duplicated(a, axis=1)\n        self.assertEqual(post.tolist(),\n                [True, True, False, True, True])\n\n        post = array_to_duplicated(a, axis=1, exclude_first=True)\n        self.assertEqual(post.tolist(),\n                [False, True, False, False, True])\n\n\n    def test_array_to_duplicated_c(self) -> None:\n        a = np.array([[50, 50, 32, 17, 17], [2,2,1,3,3]])\n        with self.assertRaises(NotImplementedError):\n            # axis cannot be None\n            array_to_duplicated(a, axis=None)  # type: ignore\n\n\n    def test_array_to_duplicated_d(self) -> None:\n        c = array_to_duplicated(\n                np.array(['q','q','q', 'a', 'w', 'w'], dtype=object),\n                exclude_first=False,\n                exclude_last=False\n                )\n        self.assertEqual(c.tolist(), [True, True, True, False, True, True])\n\n\n    def test_array_to_duplicated_e(self) -> None:\n        # NOTE: these cases fail with hetergenous types as we cannot sort\n        a1 = np.array([0,0,1,0,None,None,0,1,None], dtype=object)\n        a2 = np.array([0,0,1,0,'q','q',0,1,'q'], dtype=object)\n\n        for array in (a1, a2):\n            post1 = array_to_duplicated(\n                    array,\n                    exclude_first=False,\n                    exclude_last=False\n                    )\n            self.assertEqual(post1.tolist(),\n                [True, True, True, True, True, True, True, True, True])\n\n            post2 = array_to_duplicated(\n                    array,\n                    exclude_first=True,\n                    exclude_last=False\n                    )\n            self.assertEqual(post2.tolist(),\n                [False, True, False, True, False, True, True, True, True])\n\n            post3 = array_to_duplicated(\n                    array,\n                    exclude_first=False,\n                    exclude_last=True\n                    )\n            self.assertEqual(post3.tolist(),\n                [True, True, True, True, True, True, False, False, False])\n\n            post4 = array_to_duplicated(\n                    array,\n                    exclude_first=True,\n                    exclude_last=True\n                    )\n            self.assertEqual(post4.tolist(),\n                [False, True, False, True, False, True, False, False, False])\n\n\n\n    def test_array_to_duplicated_f(self) -> None:\n\n        array = np.array([\n                [None, None, None, 32, 17, 17],\n                [2,2,2,False,'q','q'],\n                [2,2,2,False,'q','q'],\n                ], dtype=object)\n\n        post1 = array_to_duplicated(\n                array,\n                exclude_first=False,\n                exclude_last=False\n                )\n        self.assertEqual(post1.tolist(),\n            [False, True, True])\n\n        post2 = array_to_duplicated(\n                array,\n                exclude_first=True,\n                exclude_last=False\n                )\n        self.assertEqual(post2.tolist(),\n            [False, False, True])\n\n        post3 = array_to_duplicated(\n                array,\n                exclude_first=False,\n                exclude_last=True\n                )\n        self.assertEqual(post3.tolist(),\n            [False, True, False])\n\n        post4 = array_to_duplicated(\n                array,\n                exclude_first=True,\n                exclude_last=True\n                )\n        self.assertEqual(post4.tolist(),\n            [False, False, False])\n\n\n\n\n    def test_array_to_duplicated_g(self) -> None:\n\n        array = np.array([\n                [None, None, None, 32, 17, 17],\n                [2,2,2,False,'q','q'],\n                [2,2,2,False,'q','q'],\n                ], dtype=object)\n\n        post1 = array_to_duplicated(\n                array,\n                axis=1,\n                exclude_first=False,\n                exclude_last=False\n                )\n        self.assertEqual(post1.tolist(),\n            [True, True, True, False, True, True])\n\n        post2 = array_to_duplicated(\n                array,\n                axis=1,\n                exclude_first=True,\n                exclude_last=False\n                )\n        self.assertEqual(post2.tolist(),\n            [False, True, True, False, False, True])\n\n        post3 = array_to_duplicated(\n                array,\n                axis=1,\n                exclude_first=False,\n                exclude_last=True\n                )\n        self.assertEqual(post3.tolist(),\n            [True, True, False, False, True, False])\n\n        post4 = array_to_duplicated(\n                array,\n                axis=1,\n                exclude_first=True,\n                exclude_last=True\n                )\n        self.assertEqual(post4.tolist(),\n            [False, True, False, False, False, False])\n\n\n\n\n    def test_array_set_ufunc_many_a(self) -> None:\n\n        # this shows that identical arrays return the same ordering\n        a1 = np.array([3, 2, 1])\n        a2 = np.array([3, 2, 1])\n        a3 = np.array([3, 2, 1])\n        a4 = np.array([3, 2, 1])\n\n        post = ufunc_set_iter((a1, a2, a3, a4), union=False, assume_unique=True)\n        self.assertEqual(post.tolist(), [3, 2, 1])\n\n        post = ufunc_set_iter((a1, a2, a3, a4), union=True, assume_unique=True)\n        self.assertEqual(post.tolist(), [3, 2, 1])\n\n\n    def test_array_set_ufunc_many_b(self) -> None:\n        a1 = np.array([3, 2, 1])\n        a2 = np.array([3, 2])\n        a3 = np.array([5, 3, 2, 1])\n        a4 = np.array([2])\n\n        post = ufunc_set_iter((a1, a2, a3, a4), union=False, assume_unique=True)\n        self.assertEqual(post.tolist(), [2])\n\n        post = ufunc_set_iter((a1, a2, a3, a4), union=True, assume_unique=True)\n        self.assertEqual(post.tolist(), [1, 2, 3, 5])\n\n\n    def test_array_set_ufunc_many_c(self) -> None:\n        a1 = np.array([[3, 2, 1], [1, 2, 3]])\n        a2 = np.array([[5, 2, 1], [1, 2, 3]])\n        a3 = np.array([[10, 20, 30], [1, 2, 3]])\n\n        post = ufunc_set_iter((a1, a2, a3), union=False)\n        self.assertEqual(post.tolist(), [[1, 2, 3]])\n\n        post = ufunc_set_iter((a1, a2, a3), union=True)\n        self.assertEqual(post.tolist(),\n                [[1, 2, 3], [3, 2, 1], [5, 2, 1], [10, 20, 30]])\n\n\n    def test_array_set_ufunc_many_d(self) -> None:\n        a1 = np.array([3, 2, 1])\n        a2 = np.array([[5, 2, 1], [1, 2, 3]])\n\n        with self.assertRaises(Exception):\n            post = ufunc_set_iter((a1, a2), union=False)\n\n\n    def test_array_set_ufunc_many_e(self) -> None:\n        a1 = np.array([3, 2, 1])\n        a2 = np.array([30, 20])\n\n        post = ufunc_set_iter((a1, a2), union=False)\n        self.assertEqual(post.tolist(), [])\n\n\n    def test_union1d_a(self) -> None:\n        a1 = np.array([3, 2, 1])\n        a2 = np.array(['3', '2', '1'])\n\n        # need to avoid this\n        # ipdb> np.union1d(a1, a2)                                                             # array(['1', '2', '3'], dtype='<U21')\n        self.assertEqual(set(union1d(a1, a2)),\n                {1, 2, 3, '2', '1', '3'}\n                )\n\n        self.assertEqual(\n                union1d(np.array(['a', 'b', 'c']), np.array(['aaa', 'bbb', 'ccc'])).tolist(),\n                ['a', 'aaa', 'b', 'bbb', 'c', 'ccc']\n                )\n\n        self.assertEqual(\n                set(union1d(np.array([1, 2, 3]), np.array([None, False]))),\n                {False, 2, 3, None, 1}\n                )\n\n        self.assertEqual(\n                set(union1d(np.array([False, True]), np.array([None, 'a']))),\n                {False, True, None, 'a'}\n                )\n\n        self.assertEqual(set(union1d(np.array([None, 1, 'd']), np.array([None, 3, 'ff']))),\n                {'d', 1, 3, None, 'ff'}\n                )\n\n\n    def test_union1d_b(self) -> None:\n        a1 = np.array([False, True, False])\n        a2 = np.array([2, 3])\n        self.assertEqual(union1d(a1, a2).tolist(),\n                [False, True, 2, 3])\n\n    def test_union1d_c(self) -> None:\n        a1 = np.array([])\n        a2 = np.array([9007199254740993], dtype=np.uint64)\n\n        # if we cannot asume unique, the result is a rounded float\n        self.assertEqual(union1d(a1, a2, assume_unique=True).tolist(),\n                [9007199254740993])\n\n\n    def test_intersect1d_a(self) -> None:\n\n        a1 = np.array([3, 2, 1])\n        a2 = np.array(['3', '2', '1'])\n\n        self.assertEqual(len(intersect1d(a1, a2)), 0)\n\n        self.assertEqual(\n                len(intersect1d(np.array([1, 2, 3]), np.array([None, False]))), 0)\n\n        self.assertEqual(\n                set(intersect1d(np.array(['a', 'b', 'c']), np.array(['aa', 'bbb', 'c']))),\n                {'c'}\n                )\n\n    def test_intersect1d_b(self) -> None:\n        # long way of\n        a1 = np.empty(4, dtype=object)\n        a1[:] = [(0, 0), (0, 1), (0, 2), (0, 3)]\n\n        a2 = np.empty(3, dtype=object)\n        a2[:] = [(0, 1), (0, 3), (4, 5)]\n\n        # must get an array of tuples back\n        post = intersect1d(a1, a2)\n        self.assertEqual(post.tolist(),\n                [(0, 1), (0, 3)])\n\n    def test_setdiff1d_a(self) -> None:\n        a1 = np.array([3, 2, 1])\n        a2 = np.array(['3', '2', '1'])\n        self.assertSetEqual(set(setdiff1d(a1, a2)), {3, 2, 1})\n\n        a3 = np.array(['a', 'b', 'c'])\n        a4 = np.array(['aaa', 'bbb', 'ccc'])\n        self.assertSetEqual(set(setdiff1d(a3, a4)), {'a', 'b', 'c'})\n\n        a5 = np.array([1, 2, 3])\n        a6 = np.array([None, False])\n        self.assertSetEqual(set(setdiff1d(a5, a6)), {1, 2, 3})\n\n        a7 = np.array([False, True])\n        a8 = np.array([None, 'a'])\n        self.assertSetEqual(set(setdiff1d(a7, a8)), {False, True})\n\n        a9 = np.array([None, 1, 'd'])\n        a10 = np.array([None, 3, 'ff'])\n        self.assertSetEqual(set(setdiff1d(a9, a10)), {1, 'd'})\n\n        a11 = np.array([False, True, False])\n        a12 = np.array([2, 3])\n        self.assertSetEqual(set(setdiff1d(a11, a12)), {False, True})\n\n\n    def test_setdiff1d_b(self) -> None:\n        a1 = np.array([])\n        a2 = np.array([9007199254740993], dtype=np.uint64)\n        self.assertEqual(setdiff1d(a1, a2).tolist(), [])\n        self.assertEqual(setdiff1d(a2, a1).tolist(), [9007199254740993])\n\n\n    def test_setdiff1d_c(self) -> None:\n        a1 = np.array([3, 2, 1])\n        a2 = np.array(['3', 2, '1'], dtype=object)\n        self.assertSetEqual(set(setdiff1d(a1, a2)), {3, 1})\n\n        a3 = np.array(['aaa', 'b', 'ccc'])\n        a4 = np.array(['aaa', 'bbb', 'ccc'])\n        self.assertSetEqual(set(setdiff1d(a3, a4)), {'b'})\n\n        a5 = np.array([None, 2, 3])\n        a6 = np.array([None, False])\n        self.assertSetEqual(set(setdiff1d(a5, a6)), {2, 3})\n\n        a7 = np.array([False, True])\n        a8 = np.array([None, 'a', True])\n        self.assertSetEqual(set(setdiff1d(a7, a8)), {False})\n\n        obj = object()\n        a9 = np.array([None, obj, 'd'])\n        a10 = np.array([obj, None, 'ff'])\n        self.assertSetEqual(set(setdiff1d(a9, a10)), {'d'})\n\n        a11 = np.array([False, np.nan, False], dtype=object)\n        a12 = np.array([False, None])\n        self.assertSetEqual(set(setdiff1d(a11, a12)), {np.nan})\n\n\n    def test_union2d_a(self) -> None:\n        a1 = np.array([[3, 1], [0, 1]])\n        a2 = np.array([[3, 1], [0, 1]])\n\n        post1 = union2d(a1, a2, assume_unique=True)\n        self.assertEqual(post1.tolist(),\n                [[3, 1], [0, 1]])\n\n        # result will get sorted\n        post2 = union2d(a1, a2, assume_unique=False)\n        self.assertEqual(post2.tolist(),\n                [[0, 1], [3, 1]])\n\n\n    def test_union2d_b(self) -> None:\n        a1 = np.array([[3, 1], [0, 1]])\n        a2 = np.array([['3', '1'], ['0', '1']])\n\n        post1 = union2d(a1, a2, assume_unique=True)\n        self.assertEqual(\n                set(tuple(x) for x in post1),\n                set(((0, 1), ('0', '1'), (3, 1), ('3', '1')))\n                )\n\n    def test_union2d_c(self) -> None:\n        a1 = np.array([[3, 1], [0, 1]])\n        a2 = np.array([[3, 1], [10, 20]])\n\n        post1 = union2d(a1, a2, assume_unique=True)\n        self.assertEqual(\n                set(tuple(x) for x in post1),\n                set(((0, 1), (3, 1), (10, 20)))\n                )\n\n    def test_union2d_d(self) -> None:\n        a1 = np.array([None, None], dtype=object)\n        a1[:] = ((3, 1), (20, 10))\n        a2 = np.array([[3, 1], [10, 20]])\n\n\n        post1 = union2d(a1, a2, assume_unique=True)\n        self.assertEqual(\n                set(tuple(x) for x in post1),\n                set(((20, 10), (3, 1), (10, 20)))\n                )\n\n\n\n    def test_intersect2d_a(self) -> None:\n        a = np.array([('a', 'b'), ('c', 'd'), ('e', 'f')])\n        b = np.array([('a', 'g'), ('c', 'd'), ('e', 'f')])\n\n        post = intersect2d(a, b)\n        self.assertEqual([list(x) for x in post],\n                [['c', 'd'], ['e', 'f']]\n                )\n\n        post = intersect2d(a.astype(object), b.astype(object))\n        self.assertEqual([list(x) for x in post],\n                [['c', 'd'], ['e', 'f']]\n                )\n\n        post = union2d(a, b)\n        self.assertEqual([list(x) for x in post],\n                [['a', 'b'], ['a', 'g'], ['c', 'd'], ['e', 'f']]\n                )\n        post = union2d(a.astype(object), b.astype(object))\n        self.assertEqual([list(x) for x in post],\n                [['a', 'b'], ['a', 'g'], ['c', 'd'], ['e', 'f']]\n                )\n\n\n    def test_intersect2d_b(self) -> None:\n        a1 = np.array([None, None], dtype=object)\n        a1[:] = ((3, 1), (20, 10))\n        a2 = np.array([[3, 1], [10, 20]])\n\n        post1 = intersect2d(a1, a2, assume_unique=True)\n        self.assertEqual(\n                set(tuple(x) for x in post1),\n                set(((3, 1),))\n                )\n\n\n    def test_intersect2d_c(self) -> None:\n        a1 = np.array([None, None], dtype=object)\n        a1[:] = ((3, 1), (20, 10))\n\n        a2 = np.array([None, None], dtype=object)\n        a2[:] = ((3, 1), (1, 2))\n\n        post1 = intersect2d(a1, a2)\n        self.assertEqual(\n                set(tuple(x) for x in post1),\n                set(((3, 1),))\n                )\n\n    def test_setdiff2d_a(self) -> None:\n        a1 = np.array([[3, 1], [0, 1]])\n        a2 = np.array([[3, 1], [0, 1]])\n\n        post1 = setdiff2d(a1, a2, assume_unique=True)\n        self.assertEqual(post1.tolist(),\n                [])\n\n    def test_setdiff2d_b(self) -> None:\n        a1 = np.array([[3, 1], [0, 1]])\n        a2 = np.array([['3', '1'], ['0', '1']])\n\n        post1 = setdiff2d(a1, a2, assume_unique=True)\n        self.assertEqual(\n                set(tuple(x) for x in post1),\n                set(((0, 1), (3, 1)))\n                )\n\n    def test_setdiff2d_c(self) -> None:\n        a1 = np.array([[3, 1], [0, 1]])\n        a2 = np.array([[3, 1], [10, 20]])\n\n        post1 = setdiff2d(a1, a2, assume_unique=True)\n        self.assertEqual(\n                set(tuple(x) for x in post1),\n                set(((0, 1),))\n                )\n\n    def test_setdiff2d_d(self) -> None:\n        a1 = np.array([None, None], dtype=object)\n        a1[:] = ((3, 1), (20, 10))\n        a2 = np.array([[3, 1], [10, 20]])\n\n        post1 = setdiff2d(a1, a2, assume_unique=True)\n        self.assertEqual(\n                set(tuple(x) for x in post1),\n                set(((20, 10),))\n                )\n\n    #---------------------------------------------------------------------------\n    def test_isin_non_empty(self) -> None:\n        # Tests isin's ability to fallback to numpy's isin when the UnHashable types are present in either the frame itself or the iterable being compared against\n        '''\n        Each test in the matrix is run for both 1D and 2D arrays\n        ----------------------------------------------------\n        |   Matrix  |  All Match | Some Match | None Match |\n        |---------------------------------------------------\n        | None Hash |      .     |      .     |     .      |\n        | Some Hash |      .     |      .     |     .      |\n        |  All Hash |      .     |      .     |     .      |\n        ----------------------------------------------------\n        '''\n        a_1 = np.array([UnHashable(1), UnHashable(2), UnHashable(3), UnHashable(4)])\n        a_2 = np.array([UnHashable(1), 2, UnHashable(3), 4])\n        a_3 = np.array([1, 2, 3, 4])\n\n        # All\n        match_all_s1 = [UnHashable(1), UnHashable(2), UnHashable(3), UnHashable(4)]\n        match_all_s2 = (UnHashable(1), 2, UnHashable(3), 4)\n        match_all_s3 = np.array([1, 2, 3, 4])\n        expected_match_all = np.array([True, True, True, True])\n        # 1D\n        self.assertTrue(np.array_equal(expected_match_all, isin(a_1, match_all_s1)))\n        self.assertTrue(np.array_equal(expected_match_all, isin(a_2, match_all_s2)))\n        self.assertTrue(np.array_equal(expected_match_all, isin(a_3, match_all_s3)))\n        # 2D\n        self.assertTrue(np.array_equal(expected_match_all.reshape(2,2), isin(a_1.reshape(2, 2), match_all_s1)))\n        self.assertTrue(np.array_equal(expected_match_all.reshape(2,2), isin(a_2.reshape(2, 2), match_all_s2)))\n        self.assertTrue(np.array_equal(expected_match_all.reshape(2,2), isin(a_3.reshape(2, 2), match_all_s3)))\n\n        # Some\n        match_some_s1 = (UnHashable(1), UnHashable(20), UnHashable(30), UnHashable(4))\n        match_some_s2 = np.array([UnHashable(1), 20, UnHashable(30), 4])\n        match_some_s3 = [1, 20, 30, 4]\n        expected_match_some = np.array([True, False, False, True])\n        # 1D\n        self.assertTrue(np.array_equal(expected_match_some, isin(a_1, match_some_s1)))\n        self.assertTrue(np.array_equal(expected_match_some, isin(a_2, match_some_s2)))\n        self.assertTrue(np.array_equal(expected_match_some, isin(a_3, match_some_s3)))\n        # 2D\n        self.assertTrue(np.array_equal(expected_match_some.reshape(2,2), isin(a_1.reshape(2, 2), match_some_s1)))\n        self.assertTrue(np.array_equal(expected_match_some.reshape(2,2), isin(a_2.reshape(2, 2), match_some_s2)))\n        self.assertTrue(np.array_equal(expected_match_some.reshape(2,2), isin(a_3.reshape(2, 2), match_some_s3)))\n\n        # None\n        match_none_s1 = np.array([UnHashable(10), UnHashable(20), UnHashable(30), UnHashable(40)])\n        match_none_s2 = [UnHashable(10), 20, UnHashable(30), 40]\n        match_none_s3 = (10, 20, 30, 40)\n        expected_match_none = np.array([False, False, False, False])\n        # 1D\n        self.assertTrue(np.array_equal(expected_match_none, isin(a_1, match_none_s1)))\n        self.assertTrue(np.array_equal(expected_match_none, isin(a_2, match_none_s2)))\n        self.assertTrue(np.array_equal(expected_match_none, isin(a_3, match_none_s3)))\n        # 2D\n        self.assertTrue(np.array_equal(expected_match_none.reshape(2,2), isin(a_1.reshape(2, 2), match_none_s1)))\n        self.assertTrue(np.array_equal(expected_match_none.reshape(2,2), isin(a_2.reshape(2, 2), match_none_s2)))\n        self.assertTrue(np.array_equal(expected_match_none.reshape(2,2), isin(a_3.reshape(2, 2), match_none_s3)))\n\n    def test_isin_empty(self) -> None:\n        arr = np.array([1, 2, 3, 4])\n        expected = np.array([False, False, False, False])\n\n        # 1D\n        self.assertTrue(np.array_equal(expected, isin(arr, tuple())))\n        self.assertTrue(np.array_equal(expected, isin(arr, [])))\n        self.assertTrue(np.array_equal(expected, isin(arr, np.array([]))))\n        # 2D\n        self.assertTrue(np.array_equal(expected.reshape(2,2), isin(arr.reshape(2,2), tuple())))\n        self.assertTrue(np.array_equal(expected.reshape(2,2), isin(arr.reshape(2,2), [])))\n        self.assertTrue(np.array_equal(expected.reshape(2,2), isin(arr.reshape(2,2), np.array([]))))\n\n    def test_isin_1d(self) -> None:\n        arr_1d = np.array([1, 2, 3, 4, 5])\n\n        s1 = frozenset({1, 3, 4})\n        expected = np.array([True, False, True, True, False])\n        self.assertTrue(np.array_equal(expected, _isin_1d(arr_1d, s1)))\n\n        s2 = frozenset({7, 8, 9})\n        expected = np.array([False, False, False, False, False])\n        self.assertTrue(np.array_equal(expected, _isin_1d(arr_1d, s2)))\n\n        s3 = frozenset({1, 2, 3, 4, 5})\n        expected = np.array([True, True, True, True, True])\n        self.assertTrue(np.array_equal(expected, _isin_1d(arr_1d, s3)))\n\n        arr_2d = np.array( [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        with self.assertRaises(TypeError):\n            _isin_1d(arr_2d, s3)\n\n    def test_isin_2d(self) -> None:\n        arr_2d = np.array( [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n        s1 = frozenset({1, 3, 4, 9})\n        expected = np.array([[True, False, True], [True, False, False], [False, False, True]])\n        self.assertTrue(np.array_equal(expected, _isin_2d(arr_2d, s1)))\n\n        s2 = frozenset({10, 11, 12})\n        expected = np.array([[False, False, False], [False, False, False], [False, False, False]])\n        self.assertTrue(np.array_equal(expected, _isin_2d(arr_2d, s2)))\n\n        s3 = frozenset({1, 2, 3, 4, 5, 6, 7, 8, 9})\n        expected = np.array([[True, True, True], [True, True, True], [True, True, True]])\n        self.assertTrue(np.array_equal(expected, _isin_2d(arr_2d, s3)))\n\n        arr_1d = np.array([1, 2, 3, 4, 5])\n        with self.assertRaises(ValueError):\n            _isin_2d(arr_1d, s3)\n\n\n\n    #---------------------------------------------------------------------------\n\n    @unittest.skip('requires network')\n    def test_read_url(self) -> None:\n        url = 'https://jsonplaceholder.typicode.com/todos'\n        post = _read_url(url)\n\n\n    def test_slice_to_ascending_slice_a(self) -> None:\n\n        a1 = np.arange(10)\n\n        def compare(slc: slice) -> None:\n            slc_asc = slice_to_ascending_slice(slc, len(a1))\n            self.assertEqual(sorted(a1[slc]), list(a1[slc_asc]))\n\n        compare(slice(4,))\n        compare(slice(6, 1, -1))\n        compare(slice(6, 1, -2))\n        compare(slice(6, None, -3))\n        compare(slice(6, 2, -2))\n        compare(slice(None, 1, -1))\n\n\n    def test_array_shift_a(self) -> None:\n        a1 = np.arange(6)\n\n\n        self.assertEqual(array_shift(array=a1, shift=2, axis=0, wrap=True).tolist(),\n                [4, 5, 0, 1, 2, 3])\n        self.assertEqual(array_shift(array=a1, shift=-2, axis=0, wrap=True).tolist(),\n                [2, 3, 4, 5, 0, 1])\n\n        self.assertEqual(array_shift(array=a1, shift=5, axis=0, wrap=True).tolist(),\n                [1, 2, 3, 4, 5, 0])\n\n        self.assertEqual(\n                array_shift(array=a1, shift=2, axis=0, wrap=False, fill_value=-1).tolist(),\n                [-1, -1, 0, 1, 2, 3])\n\n        self.assertEqual(\n                array_shift(array=a1, shift=2, axis=0, wrap=False, fill_value=1.5).tolist(),\n                [1.5, 1.5, 0, 1, 2, 3])\n\n        self.assertEqual(\n                array_shift(array=a1, shift=-2, axis=0, wrap=False, fill_value=1.5).tolist(),\n                [2, 3, 4, 5, 1.5, 1.5])\n\n\n    def test_array_shift_b(self) -> None:\n        a1 = np.array([('a', 'b', 'e', 'd'),\n                ('c', 'd', 'f', 'w'),\n                ('e', 'f', 's', 'q')])\n\n        self.assertEqual(array_shift(array=a1, shift=2, axis=0, wrap=True).tolist(),\n                [['c', 'd', 'f', 'w'], ['e', 'f', 's', 'q'], ['a', 'b', 'e', 'd']])\n\n        self.assertEqual(array_shift(array=a1, shift=-2, axis=0, wrap=True).tolist(),\n                [['e', 'f', 's', 'q'], ['a', 'b', 'e', 'd'], ['c', 'd', 'f', 'w']])\n\n\n        self.assertEqual(\n                array_shift(array=a1, shift=-2, axis=0, wrap=False, fill_value='XX').dtype,\n                np.dtype('<U2')\n                )\n\n        self.assertEqual(\n                array_shift(array=a1, shift=-2, axis=0, wrap=False, fill_value='XX').tolist(),\n                [['e', 'f', 's', 'q'],\n                ['XX', 'XX', 'XX', 'XX'],\n                ['XX', 'XX', 'XX', 'XX']])\n\n        self.assertEqual(\n                array_shift(array=a1, shift=2, axis=1, wrap=False, fill_value='XX').tolist(),\n                [['XX', 'XX', 'a', 'b'],\n                ['XX', 'XX', 'c', 'd'],\n                ['XX', 'XX', 'e', 'f']])\n\n        self.assertEqual(\n                array_shift(array=a1, shift=-2, axis=1, wrap=False, fill_value='XX').tolist(),\n                [['e', 'd', 'XX', 'XX'],\n                ['f', 'w', 'XX', 'XX'],\n                ['s', 'q', 'XX', 'XX']])\n\n\n    def test_array_shift_c(self) -> None:\n        a1 = np.arange(6)\n        post = array_shift(array=a1, shift=0, axis=0, wrap=False)\n        self.assertEqual(a1.tolist(), post.tolist())\n\n\n    def test_ufunc_skipna_1d_a(self) -> None:\n\n        a1 = np.array([\n                (2, 2, 3, 4.23, np.nan),\n                (30, 34, None, 80.6, 90.123),\n                ], dtype=object)\n\n        a2 = ufunc_axis_skipna(array=a1,\n                skipna=True,\n                axis=0,\n                ufunc=np.sum,\n                ufunc_skipna=np.nansum\n                )\n        self.assertEqual(a2.tolist(),\n                [32, 36, 3, 84.83, 90.123])\n\n        a3 = ufunc_axis_skipna(array=a1,\n                skipna=True,\n                axis=1,\n                ufunc=np.sum,\n                ufunc_skipna=np.nansum\n                )\n        self.assertEqual(a3.tolist(),\n                [11.23, 234.723]\n                )\n\n\n    def test_ufunc_skipna_1d_b(self) -> None:\n\n        a1 = np.array((None, None), dtype=object)\n\n        post = ufunc_axis_skipna(array=a1,\n                skipna=True,\n                axis=0,\n                ufunc=np.sum,\n                ufunc_skipna=np.nansum\n                )\n        self.assertTrue(np.isnan(post))\n\n\n    def test_ufunc_unique_a(self) -> None:\n\n        a1 = np.array([1, 1, 1, 2, 2])\n        post = ufunc_unique(a1)\n        assert isinstance(post, np.ndarray)\n        self.assertEqual(post.tolist(), [1, 2])\n\n        a2 = np.array([1, 1, 1, 2, 2], dtype=object)\n        post = ufunc_unique(a2)\n        assert isinstance(post, np.ndarray)\n        self.assertEqual(post.tolist(), [1, 2])\n\n        a3 = np.array([1, 'x', 1, None, 2], dtype=object)\n        post = ufunc_unique(a3)\n        # order is as used\n        self.assertEqual(post.tolist(), [1, 'x', None, 2])\n\n\n    def test_ufunc_unique_b(self) -> None:\n\n        a1 = np.array([[1, 1], [1, 2], [1, 2]])\n        post = ufunc_unique(a1)\n        assert isinstance(post, np.ndarray)\n        self.assertEqual(post.tolist(), [1, 2])\n\n        post = ufunc_unique(a1, axis=0)\n        assert isinstance(post, np.ndarray)\n        self.assertEqual(post.tolist(), [[1, 1], [1, 2]])\n\n        post = ufunc_unique(a1, axis=1)\n        assert isinstance(post, np.ndarray)\n        self.assertEqual(post.tolist(), [[1, 1], [1, 2], [1, 2]])\n\n\n    def test_ufunc_unique_c(self) -> None:\n\n        a1 = np.array([[1, 'x', 1], [1, None, 1], [1, 'x', 1]], dtype=object)\n\n        post = ufunc_unique(a1)\n        self.assertEqual(post.tolist(), [1, 'x', None])\n\n        post = ufunc_unique(a1, axis=0)\n        self.assertEqual(post.tolist(), [(1, 'x', 1), (1, None, 1)])\n\n        post = ufunc_unique(a1, axis=1)\n        self.assertEqual(post.tolist(), [(1, 1, 1), ('x', None, 'x')])\n\n\n    def test_concat_resolved_a(self) -> None:\n        a1 = np.array([[3,4,5],[0,0,0]])\n        a2 = np.array([1,2,3]).reshape((1,3))\n        a3 = np.array([('3', '4', '5'),('1','1','1')])\n        a4 = np.array(['3', '5'])\n        a5 = np.array([1, 1, 1])\n\n        post = concat_resolved((a1, a3))\n        self.assertEqual(\n                post.tolist(),\n                [[3, 4, 5], [0, 0, 0], ['3', '4', '5'], ['1', '1', '1']]\n                )\n\n        post = concat_resolved((a3, a1, a2))\n        self.assertEqual(post.tolist(),\n                [['3', '4', '5'], ['1', '1', '1'], [3, 4, 5], [0, 0, 0], [1, 2, 3]])\n\n        self.assertEqual(concat_resolved((a1, a3), axis=1).tolist(),\n                [[3, 4, 5, '3', '4', '5'], [0, 0, 0, '1', '1', '1']]\n                )\n\n        self.assertEqual(concat_resolved((a4, a5)).tolist(),\n                ['3', '5', 1, 1, 1])\n\n\n    def test_concat_resolved_b(self) -> None:\n        a1 = np.array([[3,4,5],[0,0,0]])\n        a2 = np.array([1,2,3]).reshape((1,3))\n\n        with self.assertRaises(Exception):\n            concat_resolved((a1, a2), axis=None)  # type: ignore\n\n\n    def test_dtype_to_na_a(self) -> None:\n\n        self.assertEqual(dtype_to_na(np.dtype(int)), 0)\n        self.assertTrue(np.isnan(dtype_to_na(np.dtype(float))))\n        self.assertEqual(dtype_to_na(np.dtype(bool)), False)\n        self.assertEqual(dtype_to_na(np.dtype(object)), None)\n        self.assertEqual(dtype_to_na(np.dtype(str)), '')\n\n        with self.assertRaises(NotImplementedError):\n            _ = dtype_to_na(np.dtype('V'))\n\n    #---------------------------------------------------------------------------\n\n    def test_key_to_datetime_key_a(self) -> None:\n\n        post = key_to_datetime_key(slice('2018-01-01', '2019-01-01'))\n        self.assertEqual(post,\n                slice(np.datetime64('2018-01-01'),\n                np.datetime64('2019-01-01'), None))\n\n        post = key_to_datetime_key(np.datetime64('2019-01-01'))\n        self.assertEqual(post, np.datetime64('2019-01-01'))\n\n        post = key_to_datetime_key('2019-01-01')\n        self.assertEqual(post, np.datetime64('2019-01-01'))\n\n        a1 = np.array(('2019-01-01'), dtype='M')\n        post = key_to_datetime_key(a1)\n        self.assertEqual(post, a1)\n\n        post = key_to_datetime_key(np.array(['2018-01-01', '2019-01-01']))\n        a2 = np.array(['2018-01-01', '2019-01-01'], dtype='datetime64[D]')\n        assert isinstance(post, np.ndarray)\n        self.assertEqual(post.tolist(), a2.tolist())\n\n        post = key_to_datetime_key(['2018-01-01', '2019-01-01'])\n        a3 = np.array(['2018-01-01', '2019-01-01'], dtype='datetime64[D]')\n        assert isinstance(post, np.ndarray)\n        self.assertEqual(post.tolist(), a3.tolist())\n\n        post = key_to_datetime_key(['2018-01', '2019-01'])\n        a4 = np.array(['2018-01', '2019-01'], dtype='datetime64[M]')\n        assert isinstance(post, np.ndarray)\n        self.assertEqual(post.tolist(), a4.tolist())\n\n\n        post = key_to_datetime_key(str(x) for x in range(2012, 2015))\n        a5 = np.array(['2012', '2013', '2014'], dtype='datetime64[Y]')\n        assert isinstance(post, np.ndarray)\n        self.assertEqual(post.tolist(), a5.tolist())\n\n        post = key_to_datetime_key(None)\n        self.assertEqual(post, None)\n\n    def test_key_to_datetime_key_b(self) -> None:\n\n        d = datetime.datetime(2018, 2, 1, 5, 40)\n        self.assertEqual(key_to_datetime_key(d), np.datetime64(d))\n        self.assertEqual(key_to_datetime_key(d.date()), np.datetime64(d.date()))\n\n    #---------------------------------------------------------------------------\n\n    def test_set_ufunc2d_a(self) -> None:\n        # fails due wrong function\n        a1 = np.array([1, 1, 1])\n        with self.assertRaises(NotImplementedError):\n            _ufunc_set_2d(np.sum, a1, a1)\n\n\n    def test_set_ufunc2d_b(self) -> None:\n\n        a1 = np.array([['a', 'b'], ['b', 'c']])\n        a2 = np.array([['b', 'cc'], ['dd', 'ee']])\n\n        post = _ufunc_set_2d(np.union1d, a1, a2)\n        self.assertEqual(len(post), 4)\n        self.assertEqual(str(post.dtype), '<U2')\n\n        post = _ufunc_set_2d(np.union1d, a2, a1)\n        self.assertEqual(len(post), 4)\n        self.assertEqual(str(post.dtype), '<U2')\n\n    def test_set_ufunc2d_c(self) -> None:\n\n        # these values, as tuples, are equivalent and hash to the same value in Python, thus we get one result\n        a1 = np.array([[False]])\n        a2 = np.array([[0]])\n\n        post = _ufunc_set_2d(np.union1d, a1, a2)\n        self.assertEqual(post.tolist(), [(False,)])\n\n\n    def test_set_ufunc2d_d(self) -> None:\n        a1 = np.array([[3], [2], [1]])\n        a2 = np.array([[30], [20], [2], [1]])\n\n        post1 = _ufunc_set_2d(np.union1d, a1, a2)\n        self.assertEqual(post1.tolist(),\n                [[1], [2], [3], [20], [30]])\n\n        post2 = _ufunc_set_2d(np.intersect1d, a1, a2)\n        self.assertEqual(post2.tolist(),\n                [[1], [2]])\n\n\n    def test_set_ufunc2d_e(self) -> None:\n\n        a1 = np.array([[0, 1], [-1, -2]])\n        a2 = np.array([])\n\n        post1 = _ufunc_set_2d(np.union1d, a1, a2, assume_unique=True)\n        self.assertEqual(id(a1), id(post1))\n\n        post2 = _ufunc_set_2d(np.union1d, a2, a1, assume_unique=True)\n        self.assertEqual(id(a1), id(post2))\n\n\n\n    def test_set_ufunc2d_f(self) -> None:\n\n        a1 = np.array([[0, 1], [-1, -2]])\n        a2 = np.array([])\n\n        # intersect with 0 results in 0\n        post1 = _ufunc_set_2d(np.intersect1d, a1, a2)\n        self.assertEqual(len(post1), 0)\n\n        post2 = _ufunc_set_2d(np.intersect1d, a2, a1)\n        self.assertEqual(len(post2), 0)\n\n\n\n    def test_ufunc_set_2d_g(self) -> None:\n        post1 = _ufunc_set_2d(np.union1d,\n                np.arange(4).reshape((2, 2)),\n                np.arange(4).reshape((2, 2)),\n                assume_unique=True)\n\n        self.assertEqual(post1.tolist(),\n                [[0, 1], [2, 3]])\n\n\n    def test_ufunc_set_2d_h(self) -> None:\n        with self.assertRaises(RuntimeError):\n            post1 = _ufunc_set_2d(np.union1d,\n                    np.arange(4).reshape((2, 2)),\n                    np.arange(4),\n                    assume_unique=True)\n\n\n\n    #---------------------------------------------------------------------------\n\n\n\n    def test_to_timedelta64_a(self) -> None:\n        timedelta = datetime.timedelta\n\n        self.assertEqual(\n                to_timedelta64(timedelta(days=4)),\n                np.timedelta64(4, 'D'))\n\n        self.assertEqual(\n                to_timedelta64(timedelta(seconds=4)),\n                np.timedelta64(4, 's'))\n\n        self.assertEqual(\n                to_timedelta64(timedelta(minutes=4)),\n                np.timedelta64(240, 's'))\n\n    def test_binary_transition_a(self) -> None:\n        a1 = np.array([False, True, True, False, False, True, True, False])\n        self.assertEqual(binary_transition(a1).tolist(), [0, 3, 4, 7])\n\n        a1 = np.array([False, False, True, False, True, True, True, True])\n        self.assertEqual(binary_transition(a1).tolist(), [1, 3])\n\n\n        a1 = np.array([True, False, True])\n        self.assertEqual(binary_transition(a1).tolist(), [1])\n\n        a1 = np.array([False, True, False])\n        self.assertEqual(binary_transition(a1).tolist(), [0, 2])\n\n        a1 = np.array([True])\n        self.assertEqual(binary_transition(a1).tolist(), [])\n\n        a1 = np.array([False])\n        self.assertEqual(binary_transition(a1).tolist(), [])\n\n        a1 = np.array([False, True])\n        self.assertEqual(binary_transition(a1).tolist(), [0])\n\n        a1 = np.array([True, False])\n        self.assertEqual(binary_transition(a1).tolist(), [1])\n\n\n    def test_binary_transition_b(self) -> None:\n        # return index per axis (column or row) at False values where False was True, or will be True\n        a1 = np.array([[False, False, True, False],\n                       [True, False, True, False],\n                       [False, False, False, True]\n                       ])\n\n        self.assertEqual(\n                binary_transition(a1, axis=0).tolist(),\n                [[0, 2], None, [2,], [1,]]\n                )\n\n        self.assertEqual(\n                binary_transition(a1, axis=1).tolist(),\n                [[1, 3], [1, 3], [2,]]\n                )\n\n    def test_binary_transition_c(self) -> None:\n        # return index per axis (column or row) at False values where False was True, or will be True\n        a1 = np.array([[False, False, True, False],\n                       [True, False, True, False],\n                       [True, False, True, False],\n                       [True, False, False, True],\n                       [False, False, False, True]\n                       ])\n\n        self.assertEqual(\n                binary_transition(a1, axis=0).tolist(),\n                [[0, 4], None, [3,], [2,]]\n                )\n\n        self.assertEqual(\n                binary_transition(a1, axis=1).tolist(),\n                [[1, 3], [1, 3], [1, 3], [1, 2], [2,]]\n                )\n\n\n    def test_binary_transition_d(self) -> None:\n        with self.assertRaises(NotImplementedError):\n            binary_transition(np.arange(12).reshape((2, 2, 3)), 0)\n\n\n    #---------------------------------------------------------------------------\n\n    def test_roll_1d_a(self) -> None:\n\n        a1 = np.arange(12)\n\n        for i in range(len(a1) + 1):\n            post = roll_1d(a1, i)\n            self.assertEqual(post.tolist(), np.roll(a1, i).tolist())\n\n            post = roll_1d(a1, -i)\n            self.assertEqual(post.tolist(), np.roll(a1, -i).tolist())\n\n    def test_roll_1d_b(self) -> None:\n        post = roll_1d(np.array([]), -4)\n        self.assertEqual(len(post), 0)\n\n\n    def test_roll_1d_c(self) -> None:\n        a1 = np.array([3, 4, 5, 6])\n        self.assertEqual(roll_1d(a1, 1).tolist(), [6, 3, 4, 5])\n        self.assertEqual(roll_1d(a1, -1).tolist(), [4, 5, 6, 3])\n\n    #---------------------------------------------------------------------------\n\n    def test_roll_2d_a(self) -> None:\n\n        a1 = np.arange(12).reshape((3,4))\n\n        for i in range(a1.shape[0] + 1):\n            post = roll_2d(a1, i, axis=0)\n            self.assertEqual(post.tolist(), np.roll(a1, i, axis=0).tolist())\n\n            post = roll_2d(a1, -i, axis=0)\n            self.assertEqual(post.tolist(), np.roll(a1, -i, axis=0).tolist())\n\n        for i in range(a1.shape[1] + 1):\n            post = roll_2d(a1, i, axis=1)\n            self.assertEqual(post.tolist(), np.roll(a1, i, axis=1).tolist())\n\n            post = roll_2d(a1, -i, axis=1)\n            self.assertEqual(post.tolist(), np.roll(a1, -i, axis=1).tolist())\n\n    def test_roll_2d_b(self) -> None:\n        post = roll_2d(np.array([[]]), -4, axis=1)\n        self.assertEqual(post.shape, (1, 0))\n\n\n    def test_roll_2d_c(self) -> None:\n\n        a1 = np.arange(12).reshape((3,4))\n\n        self.assertEqual(roll_2d(a1, -2, axis=0).tolist(),\n                [[8, 9, 10, 11], [0, 1, 2, 3], [4, 5, 6, 7]])\n\n        self.assertEqual(roll_2d(a1, -2, axis=1).tolist(),\n                [[2, 3, 0, 1], [6, 7, 4, 5], [10, 11, 8, 9]])\n\n    def test_roll_2d_d(self) -> None:\n\n        a1 = np.arange(6).reshape((2, 3))\n\n        self.assertEqual(roll_2d(a1, 1, axis=1).tolist(),\n                [[2, 0, 1], [5, 3, 4]])\n        self.assertEqual(roll_2d(a1, -1, axis=1).tolist(),\n                [[1, 2, 0], [4, 5, 3]])\n\n\n    def test_roll_2d_e(self) -> None:\n\n        a1 = np.arange(6).reshape((3, 2))\n\n        self.assertEqual(roll_2d(a1, 1, axis=0).tolist(),\n                [[4, 5], [0, 1], [2, 3]]\n                )\n        self.assertEqual(roll_2d(a1, -1, axis=0).tolist(),\n                [[2, 3], [4, 5], [0, 1]]\n                )\n\n\n    def test_roll_2d_f(self) -> None:\n\n        with self.assertRaises(NotImplementedError):\n            roll_2d(np.arange(4).reshape((2, 2)), 1, axis=2)\n\n    #---------------------------------------------------------------------------\n\n\n    def test_to_datetime64_a(self) -> None:\n\n        dt = to_datetime64('2019')\n        self.assertEqual(dt, np.datetime64('2019'))\n\n        dt = to_datetime64('2019', dtype=np.dtype('datetime64[D]'))\n        self.assertEqual(dt, np.datetime64('2019-01-01'))\n\n        dt = to_datetime64(np.datetime64('2019'), dtype=np.dtype('datetime64[Y]'))\n        self.assertEqual(dt, np.datetime64('2019'))\n\n        with self.assertRaises(RuntimeError):\n            dt = to_datetime64(np.datetime64('2019'), dtype=np.dtype('datetime64[D]'))\n\n\n    def test_to_datetime64_b(self) -> None:\n\n        dt = to_datetime64(2019, DT64_YEAR)\n        self.assertEqual(dt, np.datetime64('2019'))\n\n        with self.assertRaises(RuntimeError):\n            _ = to_datetime64(2019, DT64_DAY)\n\n\n\n    def test_resolve_type_iter_a(self) -> None:\n\n        v1 = ('a', 'b', 'c')\n        resolved, has_tuple, values = resolve_type_iter(v1)\n        self.assertEqual(resolved, None)\n\n        v22 = ('a', 'b', 3)\n        resolved, has_tuple, values = resolve_type_iter(v22)\n        self.assertEqual(resolved, object)\n\n        v3 = ('a', 'b', (1, 2))\n        resolved, has_tuple, values = resolve_type_iter(v3)\n        self.assertEqual(resolved, object)\n        self.assertTrue(has_tuple)\n\n        v4 = (1, 2, 4.3, 2)\n        resolved, has_tuple, values = resolve_type_iter(v4)\n        self.assertEqual(resolved, None)\n\n\n        v5 = (1, 2, 4.3, 2, None)\n        resolved, has_tuple, values = resolve_type_iter(v5)\n        self.assertEqual(resolved, None)\n\n\n        v6 = (1, 2, 4.3, 2, 'g')\n        resolved, has_tuple, values = resolve_type_iter(v6)\n        self.assertEqual(resolved, object)\n\n        v7 = ()\n        resolved, has_tuple, values = resolve_type_iter(v7)\n        self.assertEqual(resolved, None)\n\n\n    def test_resolve_type_iter_b(self) -> None:\n\n        v1 = iter(('a', 'b', 'c'))\n        resolved, has_tuple, values = resolve_type_iter(v1)\n        self.assertEqual(resolved, None)\n\n        v2 = iter(('a', 'b', 3))\n        resolved, has_tuple, values = resolve_type_iter(v2)\n        self.assertEqual(resolved, object)\n\n        v3 = iter(('a', 'b', (1, 2)))\n        resolved, has_tuple, values = resolve_type_iter(v3)\n        self.assertEqual(resolved, object)\n        self.assertTrue(has_tuple)\n\n        v4 = range(4)\n        resolved, has_tuple, values = resolve_type_iter(v4)\n        self.assertEqual(resolved, None)\n\n\n    def test_resolve_type_iter_c(self) -> None:\n\n        a = [True, False, True]\n        resolved, has_tuple, values = resolve_type_iter(a)\n        self.assertEqual(id(a), id(values))\n\n        resolved, has_tuple, values = resolve_type_iter(iter(a))\n        self.assertNotEqual(id(a), id(values))\n\n        self.assertEqual(resolved, None)\n        self.assertEqual(has_tuple, False)\n\n\n    def test_resolve_type_iter_d(self) -> None:\n\n        a = [3, 2, (3,4)]\n        resolved, has_tuple, values = resolve_type_iter(a)\n        self.assertEqual(id(a), id(values))\n        self.assertTrue(has_tuple)\n\n        resolved, has_tuple, values = resolve_type_iter(iter(a))\n        self.assertNotEqual(id(a), id(values))\n\n        self.assertEqual(resolved, object)\n        self.assertEqual(has_tuple, True)\n\n\n    def test_resolve_type_iter_e(self) -> None:\n\n        a = [300000000000000002, 5000000000000000001]\n        resolved, has_tuple, values = resolve_type_iter(a)\n        self.assertEqual(id(a), id(values))\n\n        resolved, has_tuple, values = resolve_type_iter(iter(a))\n        self.assertNotEqual(id(a), id(values))\n        self.assertEqual(resolved, None)\n        self.assertEqual(has_tuple, False)\n\n    def test_resolve_type_iter_f(self) -> None:\n\n        def a() -> tp.Iterator[tp.Any]:\n            for i in range(3):\n                yield i\n            yield None\n\n        resolved, has_tuple, values = resolve_type_iter(a())\n        self.assertEqual(values, [0, 1, 2, None])\n        self.assertEqual(resolved, None)\n        self.assertEqual(has_tuple, False)\n\n    def test_resolve_type_iter_g(self) -> None:\n\n        def a() -> tp.Iterator[tp.Any]:\n            yield None\n            for i in range(3):\n                yield i\n\n        resolved, has_tuple, values = resolve_type_iter(a())\n        self.assertEqual(values, [None, 0, 1, 2])\n        self.assertEqual(resolved, None)\n        self.assertEqual(has_tuple, False)\n\n    def test_resolve_type_iter_h(self) -> None:\n\n        def a() -> tp.Iterator[tp.Any]:\n            yield 10\n            yield None\n            for i in range(3):\n                yield i\n            yield (3,4)\n\n        resolved, has_tuple, values = resolve_type_iter(a())\n        self.assertEqual(values, [10, None, 0, 1, 2, (3,4)])\n        self.assertEqual(resolved, object)\n        # we stop evaluation after finding object\n        self.assertEqual(has_tuple, True)\n\n        post = iterable_to_array_1d(a())\n        self.assertEqual(post[0].tolist(),\n                [10, None, 0, 1, 2, (3, 4)]\n                )\n\n\n    def test_resolve_type_iter_i(self) -> None:\n        a0 = range(3, 7)\n        resolved, has_tuple, values = resolve_type_iter(a0)\n        # a copy is not made\n        self.assertEqual(id(a0), id(values))\n        self.assertEqual(resolved, None)\n\n        post = iterable_to_array_1d(a0)\n        self.assertEqual(post[0].tolist(),\n                [3, 4, 5, 6])\n\n\n\n    def test_resolve_type_iter_j(self) -> None:\n        # this case was found through hypothesis\n        a0 = [0.0, 36_028_797_018_963_969]\n        resolved, has_tuple, values = resolve_type_iter(a0)\n        self.assertEqual(resolved, object)\n\n\n\n    def test_resolve_type_iter_k(self) -> None:\n        resolved, has_tuple, values = resolve_type_iter((x for x in ())) #type: ignore\n        self.assertEqual(resolved, None)\n        self.assertEqual(len(values), 0)\n        self.assertEqual(has_tuple, False)\n\n    #---------------------------------------------------------------------------\n\n    def test_iterable_to_array_a(self) -> None:\n        a1, is_unique = iterable_to_array_1d({3,4,5})\n        self.assertTrue(is_unique)\n        self.assertEqual(set(a1.tolist()), {3,4,5})\n\n        a2, is_unique = iterable_to_array_1d({None: 3, 'f': 4, 39: 0})\n        self.assertTrue(is_unique)\n        self.assertEqual(set(a2.tolist()), {None, 'f', 39})\n\n        a3, is_unique = iterable_to_array_1d((x*10 for x in range(1,4)))\n        self.assertFalse(is_unique)\n        self.assertEqual(a3.tolist(), [10, 20, 30])\n\n        a1, is_unique = iterable_to_array_1d({3,4,5}, dtype=np.dtype(int))\n        self.assertEqual(set(a1.tolist()), {3,4,5})\n\n        a1, is_unique = iterable_to_array_1d((3,4,5), dtype=np.dtype(object))\n        self.assertTrue(a1.dtype == object)\n        self.assertEqual(a1.tolist(), [3,4,5])\n\n        x = [(0, 0), (0, 1), (0, 2), (0, 3)]\n        a1, is_unique = iterable_to_array_1d(x, np.dtype(object))\n        self.assertEqual(a1.tolist(), [(0, 0), (0, 1), (0, 2), (0, 3)])\n        # must get an array of tuples back\n\n        x = [(0, 0), (0, 1), (0, 2), (0, 3)]\n        a1, is_unique = iterable_to_array_1d(iter(x))\n        self.assertEqual(a1.tolist(), [(0, 0), (0, 1), (0, 2), (0, 3)])\n\n        a4 = np.array([np.nan, 0j], dtype=object)\n        post, _ = iterable_to_array_1d(a4)\n        self.assertAlmostEqualValues(a4, post)\n\n\n        self.assertEqual(iterable_to_array_1d((1, 1.1))[0].dtype,\n                np.dtype('float64'))\n\n        self.assertEqual(iterable_to_array_1d((1.1, 0, -29))[0].dtype,\n                np.dtype('float64'))\n\n\n    def test_iterable_to_array_b(self) -> None:\n\n        iterable: tp.Iterable[tp.Any]\n\n        for iterable in (  # type: ignore\n                [1, 2, 3],\n                dict(a=1, b=2, c=3).values(),\n                dict(a=1, b=2, c=3).keys(),\n                {1, 2, 3},\n                frozenset((1, 2, 3)),\n                ('a', 3, None),\n                (1, 2, 'e', 1.1)\n                ):\n\n            a1, _ = iterable_to_array_1d(iterable)\n            self.assertEqual(set(a1), set(iterable))\n\n            a2, _ = iterable_to_array_1d(iter(iterable))\n            self.assertEqual(set(a2), set(iterable))\n\n\n    def test_iterable_to_array_c(self) -> None:\n\n        iterable: tp.Iterable[tp.Any]\n\n        for iterable, dtype in (  # type: ignore\n                ([1, 2, 3], int),\n                (dict(a=1, b=2, c=3).values(), int),\n                (dict(a=1, b=2, c=3).keys(), str),\n                ({1, 2, 3}, int),\n                (frozenset((1, 2, 3)), int),\n                (('a', 3, None), object),\n                ((1, 2, 'e', 1.1), object),\n                ):\n            a1, _ = iterable_to_array_1d(iterable, dtype=dtype)\n            self.assertEqual(set(a1), set(iterable))\n\n            a2, _ = iterable_to_array_1d(iter(iterable), dtype=dtype)\n            self.assertEqual(set(a2), set(iterable))\n\n\n    def test_iterable_to_array_d(self) -> None:\n\n        self.assertEqual(\n                iterable_to_array_1d((True, False, True))[0].dtype,\n                np.dtype('bool')\n        )\n\n        self.assertEqual(\n                iterable_to_array_1d((0, 1, 0), dtype=bool)[0].dtype,\n                np.dtype('bool')\n        )\n\n        self.assertEqual(\n                iterable_to_array_1d((1, 2, 'w'))[0].dtype,\n                np.dtype('O')\n        )\n\n        self.assertEqual(iterable_to_array_1d(((2,3), (3,2)))[0].tolist(),\n                [(2, 3), (3, 2)]\n        )\n\n\n    def test_iterable_to_array_e(self) -> None:\n\n        # this result is surprising but is a result of NumPy's array constructor\n        post = iterable_to_array_1d('cat')\n        self.assertEqual(post[0].tolist(), ['cat'])\n        self.assertEqual(post[1], True)\n\n\n    def test_iterable_to_array_f(self) -> None:\n\n\n        post1, _ = iterable_to_array_1d([[3,],[4,]])\n        self.assertEqual(post1.dtype, object)\n        self.assertEqual(post1.ndim, 1)\n        self.assertEqual(post1.tolist(), [[3], [4]])\n\n        post2, _ = iterable_to_array_1d([[3,],[4,]], dtype=object)\n        self.assertEqual(post2.dtype, object)\n        self.assertEqual(post2.ndim, 1)\n        self.assertEqual(post2.tolist(), [[3], [4]])\n\n\n\n    def test_iterable_to_array_g(self) -> None:\n\n        # this result is surprising but is a result of NumPy's array constructor\n        with self.assertRaises(RuntimeError):\n            _ = iterable_to_array_1d(np.array([None, None]), dtype=np.dtype(float))\n\n\n\n    def test_iterable_to_array_h(self) -> None:\n\n        sample = [10000000000000000000000]\n        post = iterable_to_array_1d(sample, dtype=np.dtype(int))\n        self.assertEqual(post[0].dtype, object)\n        self.assertEqual(post[0].tolist(), sample)\n\n\n    def test_iterable_to_array_i(self) -> None:\n\n        class Color(Enum):\n            RED = 1\n            GREEN = 2\n            BLUE = 3\n\n        a1, _ = iterable_to_array_1d((Color.GREEN, Color.RED, Color.BLUE))\n        self.assertEqual(a1.dtype, object)\n        self.assertEqual(a1.tolist(), [Color.GREEN, Color.RED, Color.BLUE])\n        self.assertTrue(Color.RED in a1)\n\n    def test_iterable_to_array_j(self) -> None:\n\n        from enum import auto\n\n        class FxISO(str, Enum):\n            CAD = auto()\n            CDF = auto()\n            CHF = auto()\n\n        a1, _ = iterable_to_array_1d((FxISO.CAD, FxISO.CDF, FxISO.CHF))\n        self.assertEqual(a1.dtype, object)\n        self.assertEqual(a1.tolist(), [FxISO.CAD, FxISO.CDF, FxISO.CHF])\n        self.assertTrue(a1[1] == FxISO.CDF)\n        # NOTE: in check does not work here\n\n    #---------------------------------------------------------------------------\n\n    def test_iterable_to_array_2d_a(self) -> None:\n\n        values1: tp.Iterable[tp.Iterable[tp.Any]] = [[3, 'a', 1, None], [4, 'b', 2, False]]\n        post1 = iterable_to_array_2d(values1)\n        self.assertEqual(post1.shape, (2, 4))\n        self.assertEqual(post1.dtype, object)\n\n        # this would return all string\n        values2: tp.Iterable[tp.Iterable[tp.Any]] = [['a', 'b', 'c'], [1, 2, 3]]\n        post2 = iterable_to_array_2d(values2)\n        self.assertEqual(post2.shape, (2, 3))\n        self.assertEqual(post2.dtype, object)\n\n        values3: tp.Iterable[tp.Iterable[tp.Any]] = [[1, 3, 10], [1.1, 2.1, 3.4]]\n        post3 = iterable_to_array_2d(values3)\n        self.assertEqual(post3.shape, (2, 3))\n        self.assertEqual(post3.dtype, np.float64)\n\n    def test_iterable_to_array_2d_b(self) -> None:\n        post = iterable_to_array_2d(np.arange(4).reshape((2, 2)))\n        self.assertEqual(post.tolist(), [[0, 1], [2, 3]])\n\n    def test_iterable_to_array_2d_c(self) -> None:\n        with self.assertRaises(RuntimeError):\n            # looks like a 2d array enough to get past type sampling\n            post = iterable_to_array_2d(['asd', 'wer'])\n\n\n    #---------------------------------------------------------------------------\n\n    def test_iterable_to_array_nd_a(self) -> None:\n        n1 = iterable_to_array_nd('foo')\n        self.assertEqual(n1.dtype, np.dtype('<U3'))\n        self.assertEqual(n1.ndim, 0)\n\n\n        n2 = iterable_to_array_nd(['0', 2, 3])\n        self.assertEqual(n2.tolist(), ['0', 2, 3])\n\n        n3 = iterable_to_array_nd(range(4))\n        self.assertEqual(n3.tolist(), [0, 1, 2, 3])\n\n        n4 = iterable_to_array_nd((x**2 for x in (3, 4, 5)))\n        self.assertEqual(n4.tolist(), [9, 16, 25])\n\n        n5 = iterable_to_array_nd([(4, 5), (3, 2), (0, 0)])\n        self.assertEqual(n5.ndim, 2)\n        self.assertEqual(n5.tolist(),\n                [[4, 5], [3, 2], [0, 0]])\n\n        self.assertEqual(len(iterable_to_array_nd(())), 0)\n\n\n    #---------------------------------------------------------------------------\n    def test_argmin_1d_a(self) -> None:\n\n\n        self.assertEqual(argmin_1d(np.array([3,-2,0,1])), 1)\n        self.assertEqualWithNaN(argmin_1d(np.array([np.nan, np.nan])), np.nan)\n\n        self.assertEqual(argmin_1d(np.array([np.nan,-2,0,1])), 1)\n\n        self.assertEqualWithNaN(\n                argmin_1d(np.array([np.nan,-2,0,1]), skipna=False), np.nan)\n\n\n    def test_argmax_1d_a(self) -> None:\n        self.assertEqual(argmax_1d(np.array([3,-2,0,1])), 0)\n        self.assertEqualWithNaN(argmax_1d(np.array([np.nan, np.nan])), np.nan)\n\n        self.assertEqual(argmax_1d(np.array([np.nan,-2,0,1])), 3)\n\n        self.assertEqualWithNaN(\n                argmax_1d(np.array([np.nan,-2,0,1]), skipna=False), np.nan)\n\n\n\n\n    def test_argmin_2d_a(self) -> None:\n        a1 = np.array([[1, 2, -1], [-1, np.nan, 20]])\n\n        self.assertEqual(argmin_2d(a1, axis=1).tolist(),\n                [2, 0]\n                )\n\n        self.assertAlmostEqualValues(\n                argmin_2d(a1, axis=1, skipna=False).tolist(),\n                [2, np.nan]\n                )\n\n        self.assertEqual(argmin_2d(a1, axis=0).tolist(),\n                [1, 0, 0]\n                )\n\n        self.assertAlmostEqualValues(argmin_2d(a1, axis=0, skipna=False).tolist(),\n                [1, np.nan, 0]\n                )\n\n    def test_argmin_2d_b(self) -> None:\n        a1 = np.array([[np.nan, 2, -1], [-1, np.nan, 20]])\n\n        self.assertAlmostEqualValues(\n                argmin_2d(a1, axis=1, skipna=False).tolist(),\n                [np.nan, np.nan]\n                )\n\n        self.assertAlmostEqualValues(\n                argmin_2d(a1, axis=1, skipna=True).tolist(),\n                [2, 0]\n                )\n\n    def test_argmax_2d_a(self) -> None:\n        a1 = np.array([[1, 2, -1], [-1, np.nan, 20]])\n\n        self.assertEqual(argmax_2d(a1, axis=1).tolist(),\n                [1, 2]\n                )\n\n        self.assertAlmostEqualValues(\n                argmax_2d(a1, axis=1, skipna=False).tolist(),\n                [1, np.nan]\n                )\n\n        self.assertEqual(argmax_2d(a1, axis=0).tolist(),\n                [0, 0, 1]\n                )\n\n        self.assertAlmostEqualValues(argmax_2d(a1, axis=0, skipna=False).tolist(),\n                [0, np.nan, 1]\n                )\n\n\n    def test_column_1d_filter_a(self) -> None:\n        a1 = np.arange(4)\n        a2 = np.arange(4).reshape(4, 1)\n        self.assertEqual(column_1d_filter(a1).shape, (4,))\n        self.assertEqual(column_1d_filter(a2).shape, (4,))\n\n\n    def test_row_1d_filter_a(self) -> None:\n        a1 = np.arange(4)\n        a2 = np.arange(4).reshape(1, 4)\n        self.assertEqual(row_1d_filter(a1).shape, (4,))\n        self.assertEqual(row_1d_filter(a2).shape, (4,))\n\n\n    def test_array_to_duplicated_sortable_a(self) -> None:\n\n        post1 = _array_to_duplicated_sortable(np.array([2, 3, 3, 3, 4]),\n                exclude_first=True,\n                exclude_last=True)\n        self.assertEqual(post1.tolist(),\n                [False, False, True, False, False])\n\n        post2 = _array_to_duplicated_sortable(np.array([2, 3, 3, 3, 4]),\n                exclude_first=False,\n                exclude_last=True)\n        self.assertEqual(post2.tolist(),\n                [False, True, True, False, False])\n\n        post3 = _array_to_duplicated_sortable(np.array([2, 3, 3, 3, 4]),\n                exclude_first=True,\n                exclude_last=False)\n        self.assertEqual(post3.tolist(),\n                [False, False, True, True, False])\n\n        post4 = _array_to_duplicated_sortable(np.array([2, 3, 3, 3, 4]),\n                exclude_first=False,\n                exclude_last=False)\n        self.assertEqual(post4.tolist(),\n                [False, True, True, True, False])\n\n\n    def test_ufunc_set_1d_a(self) -> None:\n        with self.assertRaises(NotImplementedError):\n            _ufunc_set_1d(np.any, np.arange(3), np.arange(3))\n\n\n    def test_ufunc_set_1d_b(self) -> None:\n        post1 = _ufunc_set_1d(np.union1d, np.arange(3), np.array(()), assume_unique=True)\n        self.assertEqual(post1.tolist(), [0, 1, 2])\n\n        post2 = _ufunc_set_1d(np.union1d, np.arange(3), np.arange(3), assume_unique=True)\n        self.assertEqual(post1.tolist(), [0, 1, 2])\n\n\n    def test_ufunc_set_1d_c(self) -> None:\n\n        post1 = _ufunc_set_1d(np.union1d, np.array([False, True]), np.array([False, True]), assume_unique=True)\n        self.assertEqual(post1.tolist(), [False, True])\n\n        post2 = _ufunc_set_1d(np.union1d, np.array([False, True]), np.array(['a', 'b']), assume_unique=True)\n        self.assertEqual(set(post2.tolist()), set((False, True, 'b', 'a')))\n\n\n    #---------------------------------------------------------------------------\n\n    def test_slices_from_targets_a(self) -> None:\n\n        target_index = binary_transition(np.array([False, True, True, True, False, False]))\n        target_values = list(range(len(target_index)))\n\n        post_iter = slices_from_targets(\n                target_index=target_index,\n                target_values=target_values,\n                length=len(target_values),\n                directional_forward=True,\n                limit=2,\n                slice_condition=lambda x: True,\n                )\n\n        post = tuple(post_iter)\n        self.assertEqual(post, ((slice(1, 4, None), 0),))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
