file_path,api_count,code
setup.py,1,"b'from setuptools import setup\nimport re\nimport os\nimport subprocess\n\ntry:\n    long_description = subprocess.check_output(\n        \'pandoc --to rst README.md\', shell=True).decode()\nexcept(IOError, ImportError, subprocess.CalledProcessError):\n    long_description = open(\'README.md\').read()\n\nwith open(\'zounds/__init__.py\', \'r\') as fd:\n    version = re.search(\n        r\'^__version__\\s*=\\s*[\\\'""]([^\\\'""]*)[\\\'""]\',\n        fd.read(),\n        re.MULTILINE).group(1)\n\ndownload_url = \'https://github.com/jvinyard/zounds/tarball/{version}\' \\\n    .format(**locals())\n\non_rtd = os.environ.get(\'READTHEDOCS\') == \'True\'\n\nextension_modules = []\n\nif not on_rtd:\n    try:\n        import numpy as np\n        from distutils.extension import Extension\n\n        countbits = Extension(\n            name=\'countbits\',\n            sources=[\'zounds/nputil/countbits.pyx\'],\n            include_dirs=[np.get_include()],\n            extra_compile_args=[\n                \'-c\',\n                \'-mpopcnt\',\n                \'-shared\',\n                \'-pthread\',\n                \'-fPIC\',\n                \'-fwrapv\',\n                \'-O2\',\n                \'-Wall\',\n                \'-fno-strict-aliasing\'\n            ])\n        extension_modules = [countbits]\n    except ImportError:\n        extension_modules = []\n\nsetup(\n    name=\'zounds\',\n    version=version,\n    url=\'https://github.com/JohnVinyard/zounds\',\n    author=\'John Vinyard\',\n    author_email=\'john.vinyard@gmail.com\',\n    description=\'Zounds is a python library for working with audio\',\n    long_description=long_description,\n    download_url=download_url,\n    ext_modules=extension_modules,\n    packages=[\n        \'zounds\',\n        \'zounds.basic\',\n        \'zounds.datasets\',\n        \'zounds.index\',\n        \'zounds.learn\',\n        \'zounds.nputil\',\n        \'zounds.segment\',\n        \'zounds.soundfile\',\n        \'zounds.spectral\',\n        \'zounds.loudness\',\n        \'zounds.synthesize\',\n        \'zounds.timeseries\',\n        \'zounds.ui\',\n        \'zounds.persistence\',\n        \'zounds.core\',\n        \'zounds.util\'\n    ],\n    install_requires=[\n        \'featureflow>=3.0.3\',\n        \'nose\',\n        \'unittest2\',\n        \'certifi==2017.7.27.1\',\n        \'requests\',\n        \'tornado==4.5.3\',\n        \'pysoundfile\',\n        \'matplotlib\',\n        \'argparse\',\n        \'ujson\',\n        \'numpy>=1.15.3\',\n        \'scipy>=1.2.1\',\n        \'torch>=0.4.0\'\n    ],\n    package_data={\n        \'nputil\': [\'*.pyx\', \'*.pyxbld\'],\n        \'ui\': [\'*.html\', \'*.js\']\n    },\n    scripts=[\'bin/zounds-quickstart\'],\n    include_package_data=True,\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n    ],\n)\n'"
docs/__init__.py,0,b''
examples/chroma.py,1,"b""import zounds\nimport numpy as np\n\nsamplerate = zounds.SR22050()\nBaseModel = zounds.stft(resample_to=samplerate, store_fft=True)\n\nband = zounds.FrequencyBand(50, samplerate.nyquist)\nwindow = zounds.HanningWindowingFunc()\n\n\n@zounds.simple_in_memory_settings\nclass Sound(BaseModel):\n    chroma = zounds.ArrayWithUnitsFeature(\n        zounds.Chroma,\n        frequency_band=band,\n        window=window,\n        needs=BaseModel.fft)\n\n\nif __name__ == '__main__':\n    app = zounds.ZoundsApp(\n        model=Sound,\n        visualization_feature=Sound.chroma,\n        audio_feature=Sound.ogg,\n        globals=globals(),\n        locals=locals())\n\n    port = 9999\n\n    with app.start_in_thread(port):\n        url = 'https://ia802606.us.archive.org/9/items/AOC11B/onclassical_luisi_bach_partita_B-flat-major_bwv-825_6.ogg'\n        _id = Sound.process(meta=url)\n        snd = Sound(_id)\n\n        chroma_scale = zounds.ChromaScale(band)\n\n        chroma = chroma_scale.apply(\n            np.abs(snd.fft) * zounds.AWeighting(), window)\n\n        basis = chroma_scale._basis(snd.fft.dimensions[-1].scale, window)\n\n    app.start(port)\n"""
examples/demo.py,0,"b'import zounds\n\nResampled = zounds.resampled(resample_to=zounds.SR11025())\n\n\n@zounds.simple_in_memory_settings\nclass Sound(Resampled):\n    """"""\n    A simple pipeline that computes a perceptually weighted modified discrete\n    cosine transform, and ""persists"" feature data in an in-memory store.\n    """"""\n\n    windowed = zounds.ArrayWithUnitsFeature(\n        zounds.SlidingWindow,\n        needs=Resampled.resampled,\n        wscheme=zounds.HalfLapped(),\n        wfunc=zounds.OggVorbisWindowingFunc(),\n        store=True)\n\n    mdct = zounds.ArrayWithUnitsFeature(\n        zounds.MDCT,\n        needs=windowed)\n\n    weighted = zounds.ArrayWithUnitsFeature(\n        lambda x: x * zounds.AWeighting(),\n        needs=mdct)\n\nif __name__ == \'__main__\':\n\n    # produce some audio to test our pipeline, and encode it as FLAC\n    synth = zounds.SineSynthesizer(zounds.SR44100())\n    samples = synth.synthesize(zounds.Seconds(5), [220., 440., 880.])\n    encoded = samples.encode(fmt=\'FLAC\')\n\n    # process the audio, and fetch features from our in-memory store\n    _id = Sound.process(meta=encoded)\n    sound = Sound(_id)\n\n    # grab all the frequency information, for a subset of the duration\n    start = zounds.Milliseconds(500)\n    end = start + zounds.Seconds(2)\n    snippet = sound.weighted[start: end, :]\n\n    # grab a subset of frequency information for the duration of the sound\n    freq_band = slice(zounds.Hertz(400), zounds.Hertz(500))\n    a440 = sound.mdct[:, freq_band]\n\n    # produce a new set of coefficients where only the 440hz sine wave is\n    # present\n    filtered = sound.mdct.zeros_like()\n    filtered[:, freq_band] = a440\n\n    # apply a geometric scale, which more closely matches human pitch\n    # perception, and apply it to the linear frequency axis\n    scale = zounds.GeometricScale(50, 4000, 0.05, 100)\n    log_coeffs = scale.apply(sound.mdct, zounds.HanningWindowingFunc())\n\n    # reconstruct audio from the MDCT coefficients\n    mdct_synth = zounds.MDCTSynthesizer()\n    reconstructed = mdct_synth.synthesize(sound.mdct)\n    filtered_reconstruction = mdct_synth.synthesize(filtered)\n\n    # start an in-browser REPL that will allow you to listen to and visualize\n    # the variables defined above (and any new ones you create in the session)\n    app = zounds.ZoundsApp(\n        model=Sound,\n        audio_feature=Sound.ogg,\n        visualization_feature=Sound.weighted,\n        globals=globals(),\n        locals=locals())\n    app.start(9999)\n'"
examples/download_musicnet.py,0,"b""import zounds\nimport argparse\n\nsamplerate = zounds.SR11025()\nBaseModel = zounds.stft(resample_to=samplerate, store_fft=True)\n\n\n@zounds.simple_in_memory_settings\nclass Sound(BaseModel):\n    pass\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--local-path',\n        required=True,\n        type=str,\n        help='local path where music net csv and npz files should be stored')\n    parser.add_argument(\n        '--port',\n        default=8888,\n        type=int,\n        help='port to run the in-browser REPL in')\n    args = parser.parse_args()\n\n    args = parser.parse_args()\n\n    mn = zounds.MusicNet(path=args.local_path)\n    zounds.ingest(mn, Sound, multi_threaded=True)\n\n    app = zounds.ZoundsApp(\n        model=Sound,\n        audio_feature=Sound.ogg,\n        visualization_feature=Sound.fft,\n        globals=globals(),\n        locals=locals())\n    app.start(args.port)\n"""
examples/download_nsynth.py,0,"b""import zounds\nimport argparse\n\nsamplerate = zounds.SR11025()\nBaseModel = zounds.stft(resample_to=samplerate, store_fft=True)\n\n\n@zounds.simple_in_memory_settings\nclass Sound(BaseModel):\n    pass\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--local-path',\n        required=True,\n        type=str,\n        help='local path where the nsynth tar files should be stored')\n    parser.add_argument(\n        '--port',\n        default=8888,\n        type=int,\n        help='port to run the in-browser REPL in')\n    args = parser.parse_args()\n\n    ns = zounds.NSynth(path=args.local_path)\n    zounds.ingest(ns, Sound, multi_threaded=True)\n\n    app = zounds.ZoundsApp(\n        model=Sound,\n        audio_feature=Sound.ogg,\n        visualization_feature=Sound.fft,\n        globals=globals(),\n        locals=locals())\n    app.start(args.port)\n"""
examples/effects.py,0,"b""import zounds\nfrom zounds.spectral import time_stretch, pitch_shift\nfrom zounds.ui import AppSettings\nimport argparse\n\nsr = zounds.SR11025()\nBaseModel = zounds.stft(resample_to=sr, store_resampled=True)\n\n\n@zounds.simple_in_memory_settings\nclass Sound(BaseModel):\n    pass\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(parents=[\n        AppSettings()\n    ])\n    parser.add_argument(\n        '--sound-uri',\n        default='https://archive.org/download/LucaBrasi2/06-Kevin_Gates-Out_The_Mud_Prod_By_The_Runners_The_Monarch.ogg')\n    args = parser.parse_args()\n\n    _id = Sound.process(meta=args.sound_uri)\n    snd = Sound(_id)\n\n    original = snd.resampled\n    slow = zounds.AudioSamples(time_stretch(original, 0.75).squeeze(), sr)\n    fast = zounds.AudioSamples(time_stretch(original, 1.25).squeeze(), sr)\n\n    higher = zounds.AudioSamples(pitch_shift(original, 1.0).squeeze(), sr)\n    lower = zounds.AudioSamples(pitch_shift(original, -1.0).squeeze(), sr)\n\n    # apply a sliding window to demonstrate time stretch and pitch shift in\n    # batch mode\n    windowing_sr = zounds.SampleRate(\n        frequency=zounds.Seconds(5),\n        duration=zounds.Seconds(10))\n\n    windowed = snd.resampled.sliding_window(windowing_sr)\n    windowed = zounds.ArrayWithUnits(\n        windowed, [zounds.IdentityDimension(), windowed.dimensions[1]])\n\n    def samples(x):\n        return zounds.AudioSamples(x, sr)\n\n    batch_slow = list(map(samples, time_stretch(windowed, 0.75)))\n    batch_fast = list(map(samples, time_stretch(windowed, 1.25)))\n\n    batch_higher = list(map(samples, pitch_shift(windowed, 1.0)))\n    batch_lower = list(map(samples, pitch_shift(windowed, -1.0)))\n\n    app = zounds.ZoundsApp(\n        model=Sound,\n        visualization_feature=Sound.fft,\n        audio_feature=Sound.resampled,\n        globals=globals(),\n        locals=locals(),\n        secret=args.app_secret)\n\n    app.start(args.port)\n"""
examples/erb_mdct.py,0,"b'""""""\nZounds implementation of something similar to/inspired by:\n\nA QUASI-ORTHOGONAL, INVERTIBLE, AND PERCEPTUALLY RELEVANT TIME-FREQUENCY\nTRANSFORM FOR AUDIO CODING\n\nhttp://www.eurasip.org/Proceedings/Eusipco/Eusipco2015/papers/1570092829.pdf\n\nThis implementation differs in that it does not use the MDCT transform on the\nfrequency domain, as getting the overlaps just right, such that they satisfy\nMDCT invertibility requirements, is tricky, and requires some low level\nknowledge that zounds\' Scale attempts to abstract away.\n\nSee section 3.3 Setting MDCT Sizes for information about what we\'re fudging/\nglossing over in this implementation.  We instead use the DCT2 transform, which\nmakes inversion easier, at the cost of more redundancy.\n""""""\n\n\nimport zounds\nimport scipy\n\nsamplerate = zounds.SR11025()\nBaseModel = zounds.stft(resample_to=samplerate)\n\nwindowing_func = zounds.OggVorbisWindowingFunc()\n\nscale = zounds.GeometricScale(300, 3030, 0.05, 100)\n\n\n@zounds.simple_in_memory_settings\nclass Document(BaseModel):\n    bark = zounds.ArrayWithUnitsFeature(\n        zounds.BarkBands,\n        samplerate=samplerate,\n        stop_freq_hz=samplerate.nyquist,\n        needs=BaseModel.fft,\n        store=True)\n\n    long_windowed = zounds.ArrayWithUnitsFeature(\n        zounds.SlidingWindow,\n        wscheme=zounds.SampleRate(\n            frequency=zounds.Milliseconds(500),\n            duration=zounds.Seconds(1)),\n        wfunc=windowing_func,\n        needs=BaseModel.resampled,\n        store=True)\n\n    dct = zounds.ArrayWithUnitsFeature(\n        zounds.DCT,\n        scale_always_even=True,\n        needs=long_windowed,\n        store=True)\n\n    mdct = zounds.FrequencyAdaptiveFeature(\n        zounds.FrequencyAdaptiveTransform,\n        transform=scipy.fftpack.idct,\n        scale=scale,\n        needs=dct,\n        store=True)\n\n\nif __name__ == \'__main__\':\n    # generate some audio\n    synth = zounds.TickSynthesizer(zounds.SR22050())\n    orig_audio = synth.synthesize(zounds.Seconds(5), zounds.Milliseconds(200))\n\n    # analyze the audio\n    _id = Document.process(meta=orig_audio.encode())\n    doc = Document(_id)\n\n    synth = zounds.FrequencyAdaptiveDCTSynthesizer(scale, samplerate)\n    recon_audio = synth.synthesize(doc.mdct)\n\n    # get a rasterized visualization of the representation\n    img = doc.mdct.square(100, do_overlap_add=True)\n\n    app = zounds.ZoundsApp(\n        model=Document,\n        audio_feature=Document.ogg,\n        visualization_feature=Document.bark,\n        globals=globals(),\n        locals=locals())\n    app.start(8888)\n'"
examples/freesound.py,0,"b'""""""\nDemonstrate how to download and process sounds from https://freesound.org\n""""""\n\nimport zounds\nimport argparse\n\nBaseModel = zounds.stft(resample_to=zounds.SR11025())\n\n\n@zounds.simple_lmdb_settings(\'freesound\', map_size=1e10, user_supplied_id=True)\nclass Sound(BaseModel):\n    bark = zounds.ArrayWithUnitsFeature(\n        zounds.BarkBands,\n        needs=BaseModel.fft,\n        store=True)\n\n    chroma = zounds.ArrayWithUnitsFeature(\n        zounds.Chroma,\n        needs=BaseModel.fft,\n        store=True)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--api-key\',\n        help=\'your Freesound API key (http://freesound.org/apiv2/apply/)\',\n        type=str,\n        required=True)\n    parser.add_argument(\n        \'--query\',\n        help=\'the text query to run against freesound\',\n        type=str,\n        required=True)\n    args = parser.parse_args()\n\n    fss = zounds.FreeSoundSearch(args.api_key, args.query)\n\n    for metadata in fss:\n        request = metadata.request\n        url = request.url\n        if not Sound.exists(url):\n            Sound.process(meta=metadata, _id=url)\n            print(\'processed {url}\'.format(**locals()))\n        else:\n            print(\'already processed {url}\'.format(**locals()))\n\n    snds = list(Sound)\n\n    # start an in-browser REPL that will allow you to listen to and visualize\n    # the variables defined above (and any new ones you create in the session)\n    app = zounds.ZoundsApp(\n        model=Sound,\n        audio_feature=Sound.ogg,\n        visualization_feature=Sound.bark,\n        globals=globals(),\n        locals=locals())\n    app.start(8888)\n'"
examples/hamming_index.py,2,"b'""""""\nDemonstrate how to build a hamming-distance index over a binary/bit-packed\nfeature.\n\nThis example is particularly handy for performance profiling of the hamming\nindex.\n""""""\n\nimport zounds\nimport numpy as np\n\nsamplerate = zounds.SR11025()\nBaseModel = zounds.stft(resample_to=samplerate, store_resampled=False)\n\n\ndef produce_fake_hash(x):\n    """"""\n    Produce random, binary features, totally irrespective of the content of\n    x, but in the same shape as x.\n    """"""\n    h = np.random.binomial(1, 0.5, (x.shape[0], 1024))\n    packed = np.packbits(h, axis=-1).view(np.uint64)\n    return zounds.ArrayWithUnits(\n        packed, [x.dimensions[0], zounds.IdentityDimension()])\n\n\n@zounds.simple_lmdb_settings(\n    \'hamming_index\', map_size=1e11, user_supplied_id=True)\nclass Sound(BaseModel):\n\n    fake_hash = zounds.ArrayWithUnitsFeature(\n        produce_fake_hash,\n        needs=BaseModel.fft,\n        store=True)\n\n\nif __name__ == \'__main__\':\n\n    zounds.ingest(\n        zounds.InternetArchive(\'Kevin_Gates_-_By_Any_Means-2014\'),\n        Sound,\n        multi_threaded=True)\n\n    def web_url(doc, ts):\n        return doc.meta[\'web_url\']\n\n    def total_duration(doc, ts):\n        return doc.fake_hash.dimensions[0].end / zounds.Seconds(1)\n\n    index = zounds.HammingIndex(\n        Sound,\n        Sound.fake_hash,\n        path=\'fake_hash_index\',\n        web_url=web_url,\n        total_duration=total_duration)\n\n    if not len(index):\n        index.add_all()\n\n    for i in range(1000):\n        list(index.random_search(n_results=50, sort=True))\n\n\n'"
examples/mdct_synth.py,0,"b'""""""\nDemonstrate an extremely simple audio encoder that learns basis functions for\nindividual audio frames from a corpus of data\n""""""\n\nimport featureflow as ff\nimport zounds\nfrom random import choice\n\nsamplerate = zounds.SR11025()\nSTFT = zounds.stft(resample_to=samplerate)\n\n\nclass Settings(ff.PersistenceSettings):\n    """"""\n    These settings make it possible to specify an id (rather than automatically\n    generating one) when analyzing a file, so that it\'s easier to reference\n    them later.\n    """"""\n    id_provider = ff.UserSpecifiedIdProvider(key=\'_id\')\n    key_builder = ff.StringDelimitedKeyBuilder()\n    database = ff.LmdbDatabase(\n            \'mdct_synth\', map_size=1e10, key_builder=key_builder)\n\n\nclass Document(STFT, Settings):\n    """"""\n    Inherit from a basic processing graph, and add a Modified Discrete Cosine\n    Transform feature\n    """"""\n    mdct = zounds.ArrayWithUnitsFeature(\n            zounds.MDCT,\n            needs=STFT.windowed,\n            store=True)\n\n\n@zounds.simple_settings\nclass DctKmeans(ff.BaseModel):\n    """"""\n    A pipeline that does example-wise normalization by giving each example\n    unit-norm, and learns 512 centroids from those examples.\n    """"""\n    docs = ff.Feature(\n            ff.IteratorNode,\n            store=False)\n\n    # randomize the order of the data\n    shuffle = ff.NumpyFeature(\n            zounds.ReservoirSampler,\n            nsamples=1e6,\n            needs=docs,\n            store=True)\n\n    # give each frame unit norm, since we care about the shape of the spectrum\n    # and not its magnitude\n    unit_norm = ff.PickleFeature(\n            zounds.UnitNorm,\n            needs=shuffle,\n            store=False)\n\n    # learn 512 centroids, or basis functions\n    kmeans = ff.PickleFeature(\n            zounds.KMeans,\n            centroids=512,\n            needs=unit_norm,\n            store=False)\n\n    # assemble the previous steps into a re-usable pipeline, which can perform\n    # forward and backward transformations\n    pipeline = ff.PickleFeature(\n            zounds.PreprocessingPipeline,\n            needs=(unit_norm, kmeans),\n            store=True)\n\n\n@zounds.simple_settings\nclass DctKmeansWithLogAmplitude(ff.BaseModel):\n    """"""\n    A pipeline that applies a logarithmic weighting to the magnitudes of the\n    spectrum before learning centroids,\n    """"""\n    docs = ff.Feature(\n            ff.IteratorNode,\n            store=False)\n\n    # randomize the order of the data\n    shuffle = ff.NumpyFeature(\n            zounds.ReservoirSampler,\n            nsamples=1e6,\n            needs=docs,\n            store=True)\n\n    log = ff.PickleFeature(\n            zounds.Log,\n            needs=shuffle,\n            store=False)\n\n    # give each frame unit norm, since we care about the shape of the spectrum\n    # and not its magnitude\n    unit_norm = ff.PickleFeature(\n            zounds.UnitNorm,\n            needs=log,\n            store=False)\n\n    # learn 512 centroids, or basis functions\n    kmeans = ff.PickleFeature(\n            zounds.KMeans,\n            centroids=512,\n            needs=unit_norm,\n            store=False)\n\n    # assemble the previous steps into a re-usable pipeline, which can perform\n    # forward and backward transformations\n    pipeline = ff.PickleFeature(\n            zounds.PreprocessingPipeline,\n            needs=(log, unit_norm, kmeans),\n            store=True)\n\n\nif __name__ == \'__main__\':\n\n    # stream all the audio files from the zip archive\n    # you can download the original file here:\n    # https://archive.org/details/FlavioGaete\n    # - https://archive.org/download/FlavioGaete/FlavioGaete22.zip\n    filename = \'FlavioGaete22.zip\'\n    print(\'Processing Audio...\')\n    for zf in ff.iter_zip(filename):\n        if \'._\' in zf.filename:\n            continue\n        try:\n            # check if the feature already exists\n            Document(zf.filename).mdct.shape\n        except KeyError:\n            print(\'processing {filename}\'.format(filename=zf.filename))\n            Document.process(meta=zf, _id=zf.filename)\n\n    print(\'learn k-means clusters\')\n    DctKmeans.process(docs=(doc.mdct for doc in Document))\n\n    print(\'learn k-means clusters with log amplitude\')\n    DctKmeansWithLogAmplitude.process(docs=(doc.mdct for doc in Document))\n\n    synth = zounds.MDCTSynthesizer()\n    docs = list(doc for doc in Document)\n    kmeans = DctKmeans()\n    kmeans_log_amplitude = DctKmeansWithLogAmplitude()\n\n\n    def full_pass(mdct, pipeline):\n        """"""\n        Do a forward and backward pass over the audio, and return the\n        reconstruction of the MDCT coefficients\n        """"""\n        transform_result = pipeline.transform(mdct)\n        recon_mdct = transform_result.inverse_transform()\n        recon_audio = synth.synthesize(recon_mdct)\n        return recon_audio\n\n\n    def reconstruction(_id=None):\n        """"""\n        Do a forward and backward pass over the audio, and return the\n        reconstructed audio\n        """"""\n        if _id:\n            doc = Document(_id)\n        else:\n            doc = choice(docs)\n        print(doc._id)\n        recon_audio = full_pass(doc.mdct, kmeans.pipeline)\n        recon_audio_log_amp = full_pass(doc.mdct, kmeans_log_amplitude.pipeline)\n        return doc.ogg[:], recon_audio, recon_audio_log_amp\n\n\n    mono_orig, mono, mono_log = reconstruction(\n            \'FlavioGaete22/TFS1_TReich11.wav\')\n    bass_orig, bass, bass_log = reconstruction(\n            \'FlavioGaete22/TFS1_TBass05.wav\')\n    beat_orig, beat, beat_log = reconstruction(\n            \'FlavioGaete22/SIKBeat02.wav\')\n    cello_orig, cello, cello_log = reconstruction(\n            \'FlavioGaete22/TFS2_TVla09.wav\')\n\n    app = zounds.ZoundsApp(\n            model=Document,\n            audio_feature=Document.ogg,\n            visualization_feature=Document.mdct,\n            globals=globals(),\n            locals=locals())\n    app.start(8888)\n'"
examples/pytorch_autoencoder.py,4,"b'import featureflow as ff\nimport zounds\nimport numpy as np\nfrom torch import nn, optim\nfrom random import choice\nimport argparse\n\n\nclass Layer(nn.Module):\n    """"""\n    A single layer of our simple autoencoder\n    """"""\n\n    def __init__(self, in_size, out_size):\n        super(Layer, self).__init__()\n        self.linear = nn.Linear(in_size, out_size, bias=False)\n        self.tanh = nn.Tanh()\n\n    def forward(self, inp):\n        x = self.linear(inp)\n        x = self.tanh(x)\n        return x\n\n\nclass AutoEncoder(nn.Module):\n    """"""\n    A simple autoencoder.  No bells, whistles, or convolutions\n    """"""\n\n    def __init__(self):\n        super(AutoEncoder, self).__init__()\n        self.encoder = nn.Sequential(\n            Layer(8192, 1024),\n            Layer(1024, 512),\n            Layer(512, 256))\n        self.decoder = nn.Sequential(\n            Layer(256, 512),\n            Layer(512, 1024),\n            Layer(1024, 8192))\n\n    def forward(self, inp):\n        encoded = self.encoder(inp)\n        decoded = self.decoder(encoded)\n        return decoded\n\n\n@zounds.simple_settings\nclass FreqAdaptiveAutoEncoder(ff.BaseModel):\n    """"""\n    Define a processing pipeline to learn a compressed representation of the\n    Sound.freq_adaptive feature.  Once this is trained and the pipeline is\n    stored, we can apply all the pre-processing steps and the autoencoder\n    forward and in reverse.\n    """"""\n    docs = ff.Feature(ff.IteratorNode)\n\n    shuffle = ff.PickleFeature(\n        zounds.ShuffledSamples,\n        nsamples=500000,\n        dtype=np.float32,\n        needs=docs)\n\n    mu_law = ff.PickleFeature(\n        zounds.MuLawCompressed,\n        needs=shuffle)\n\n    scaled = ff.PickleFeature(\n        zounds.InstanceScaling,\n        needs=mu_law)\n\n    autoencoder = ff.PickleFeature(\n        zounds.PyTorchAutoEncoder,\n        trainer=zounds.SupervisedTrainer(\n            AutoEncoder(),\n            loss=nn.MSELoss(),\n            optimizer=lambda model: optim.Adam(model.parameters(), lr=0.00005),\n            epochs=100,\n            batch_size=64,\n            holdout_percent=0.5),\n        needs=scaled)\n\n    # assemble the previous steps into a re-usable pipeline, which can perform\n    # forward and backward transformations\n    pipeline = ff.PickleFeature(\n        zounds.PreprocessingPipeline,\n        needs=(mu_law, scaled, autoencoder),\n        store=True)\n\n\nsamplerate = zounds.SR11025()\nBaseModel = zounds.stft(resample_to=samplerate, store_fft=True)\n\nscale = zounds.GeometricScale(\n    start_center_hz=300,\n    stop_center_hz=3040,\n    bandwidth_ratio=0.016985,\n    n_bands=300)\nscale.ensure_overlap_ratio(0.5)\n\n\n@zounds.simple_lmdb_settings(\'sounds\', map_size=1e10, user_supplied_id=True)\nclass Sound(BaseModel):\n    """"""\n    An audio processing pipeline that computes a frequency domain representation\n    of the sound that follows a geometric scale\n    """"""\n    bark = zounds.ArrayWithUnitsFeature(\n        zounds.BarkBands,\n        samplerate=samplerate,\n        stop_freq_hz=samplerate.nyquist,\n        needs=BaseModel.fft,\n        store=True)\n\n    long_windowed = zounds.ArrayWithUnitsFeature(\n        zounds.SlidingWindow,\n        wscheme=zounds.SampleRate(\n            frequency=zounds.Milliseconds(340),\n            duration=zounds.Milliseconds(680)),\n        wfunc=zounds.OggVorbisWindowingFunc(),\n        needs=BaseModel.resampled,\n        store=True)\n\n    long_fft = zounds.ArrayWithUnitsFeature(\n        zounds.FFT,\n        needs=long_windowed,\n        store=True)\n\n    freq_adaptive = zounds.FrequencyAdaptiveFeature(\n        zounds.FrequencyAdaptiveTransform,\n        transform=np.fft.irfft,\n        scale=scale,\n        window_func=np.hanning,\n        needs=long_fft,\n        store=False)\n\n    encoded = zounds.ArrayWithUnitsFeature(\n        zounds.Learned,\n        learned=FreqAdaptiveAutoEncoder(),\n        needs=freq_adaptive,\n        store=False)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--internet-archive-id\',\n        help=\'the internet archive id to process\',\n        type=str,\n        required=False,\n        default=\'AOC11B\')\n    args = parser.parse_args()\n\n    zounds.ingest(\n        dataset=zounds.InternetArchive(args.internet_archive_id),\n        cls=Sound,\n        multi_threaded=True)\n\n    # train the pipeline, including the autoencoder\n    if not FreqAdaptiveAutoEncoder.exists():\n        FreqAdaptiveAutoEncoder.process(\n            docs=(snd.freq_adaptive for snd in Sound))\n\n    # get a reference to the trained pipeline\n    autoencoder = FreqAdaptiveAutoEncoder()\n\n    # get references to all the sounds.  features are lazily\n    # loaded/evaluated, so this is a cheap operation\n    snds = list(Sound)\n\n    # create a synthesizer that can invert the frequency adaptive representation\n    synth = zounds.FrequencyAdaptiveFFTSynthesizer(scale, samplerate)\n\n\n    def random_reconstruction(random_encoding=False):\n        # choose a random sound\n        snd = choice(snds)\n\n        # run the model forward\n        encoded = autoencoder.pipeline.transform(snd.freq_adaptive)\n\n        if random_encoding:\n            mean = encoded.data.mean()\n            std = encoded.data.std()\n            encoded.data = np.random.normal(mean, std, encoded.data.shape)\n\n        # then invert the encoded version\n        inverted = encoded.inverse_transform()\n\n        # compare the audio of the original and the reconstruction\n        original = synth.synthesize(snd.freq_adaptive)\n        recon = synth.synthesize(inverted)\n        return original, recon, encoded.data, snd.freq_adaptive, inverted\n\n\n    # get the original audio, and the reconstructed audio\n    orig_audio, recon_audio, encoded, orig_coeffs, inverted_coeffs = \\\n        random_reconstruction()\n\n    # start up an in-browser REPL to interact with the results\n    app = zounds.ZoundsApp(\n        model=Sound,\n        audio_feature=Sound.ogg,\n        visualization_feature=Sound.bark,\n        globals=globals(),\n        locals=locals())\n    app.start(8888)\n'"
examples/pytorch_autoencoder_raw_audio.py,4,"b""import argparse\nfrom random import choice\n\nimport featureflow as ff\nimport numpy as np\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\n\nimport zounds\nfrom zounds.learn import Conv1d, ConvTranspose1d, to_var, from_var\nfrom zounds.timeseries import categorical, inverse_categorical\n\nsamplerate = zounds.SR11025()\nBaseModel = zounds.resampled(resample_to=samplerate, store_resampled=True)\n\nwindow_size = 8192\nwscheme = zounds.SampleRate(\n    frequency=samplerate.frequency * (window_size // 2),\n    duration=samplerate.frequency * window_size)\n\n\n@zounds.simple_lmdb_settings('ae', map_size=1e10, user_supplied_id=True)\nclass Sound(BaseModel):\n    windowed = zounds.ArrayWithUnitsFeature(\n        zounds.SlidingWindow,\n        wscheme=wscheme,\n        needs=BaseModel.resampled)\n\n    mu_law = zounds.ArrayWithUnitsFeature(\n        zounds.mu_law,\n        needs=windowed)\n\n    categorical = zounds.ArrayWithUnitsFeature(\n        categorical,\n        needs=windowed)\n\n\n# TODO: Factor out the part of the pipeline that starts with samples and\n# shuffled\n@zounds.simple_settings\nclass AutoEncoderPipeline(ff.BaseModel):\n    samples = ff.PickleFeature(ff.IteratorNode)\n\n    shuffled = ff.PickleFeature(\n        zounds.ShuffledSamples,\n        nsamples=int(1e5),\n        dtype=np.float32,\n        needs=samples)\n\n    scaled = ff.PickleFeature(\n        zounds.InstanceScaling,\n        needs=shuffled)\n\n    autoencoder = ff.PickleFeature(\n        zounds.PyTorchAutoEncoder,\n        trainer=ff.Var('trainer'),\n        needs=scaled)\n\n    pipeline = ff.PickleFeature(\n        zounds.PreprocessingPipeline,\n        needs=(scaled, autoencoder,),\n        store=True)\n\n\n@zounds.simple_settings\nclass CategoricalAutoEncoderPipeline(ff.BaseModel):\n    samples = ff.PickleFeature(ff.IteratorNode)\n\n    shuffled = ff.PickleFeature(\n        zounds.ShuffledSamples,\n        nsamples=int(1e5),\n        dtype=np.float32,\n        needs=samples)\n\n    autoencoder = ff.PickleFeature(\n        zounds.PyTorchAutoEncoder,\n        trainer=ff.Var('trainer'),\n        needs=shuffled)\n\n    pipeline = ff.PickleFeature(\n        zounds.PreprocessingPipeline,\n        needs=(autoencoder,),\n        store=True)\n\n\nclass EncoderLayer(Conv1d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        super(EncoderLayer, self).__init__(\n            in_channels, out_channels, kernel_size, stride, padding)\n\n\nclass DecoderLayer(ConvTranspose1d):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dropout=True,\n            activation=lambda x: F.leaky_relu(x, 0.2)):\n        super(DecoderLayer, self).__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            activation,\n            dropout)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, in_channels):\n        super(Encoder, self).__init__()\n        self.in_channels = in_channels\n        self.main = nn.Sequential(\n            EncoderLayer(in_channels, 64, 16, 8, 4),\n            EncoderLayer(64, 128, 8, 4, 2),\n            EncoderLayer(128, 128, 8, 4, 2),\n            EncoderLayer(128, 128, 8, 4, 2),\n            EncoderLayer(128, 256, 8, 4, 2),\n            EncoderLayer(256, 512, 4, 1, 0))\n\n    def forward(self, x):\n        x = x.view(-1, self.in_channels, window_size)\n        return self.main(x).view(-1, 512)\n\n\nclass Decoder(nn.Module):\n    def __init__(self, out_channels, output_activation):\n        super(Decoder, self).__init__()\n        act = output_activation\n        self.out_channels = out_channels\n        self.main = nn.Sequential(\n            DecoderLayer(512, 256, 4, 1, 0),\n            DecoderLayer(256, 128, 8, 4, 2),\n            DecoderLayer(128, 128, 8, 4, 2),\n            DecoderLayer(128, 128, 8, 4, 2),\n            DecoderLayer(128, 64, 8, 4, 2),\n            DecoderLayer(\n                64, self.out_channels, 16, 8, 4, dropout=False, activation=act))\n\n    def forward(self, x):\n        x = x.view(-1, 512, 1)\n        x = self.main(x)\n        x = x.view(-1, self.out_channels, window_size)\n        x = x.squeeze()\n        return x\n\n\nclass AutoEncoder(nn.Module):\n    def __init__(self, channels, output_activation):\n        super(AutoEncoder, self).__init__()\n        self.encoder = Encoder(channels)\n        self.decoder = Decoder(channels, output_activation)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n\nclass RawSamplesAutoEncoder(AutoEncoder):\n    def __init__(self):\n        super(RawSamplesAutoEncoder, self).__init__(\n            channels=1, output_activation=F.tanh)\n\n\nclass CategoricalAutoEncoder(AutoEncoder):\n    def __init__(self):\n        super(CategoricalAutoEncoder, self).__init__(\n            channels=256, output_activation=F.log_softmax)\n\n\ndef raw_samples_synthesize(x):\n    # TODO: it should be possible to apply windowing at the synthesis step\n    synth = zounds.WindowedAudioSynthesizer()\n    return synth.synthesize(x)\n\n\ndef categorical_synthesize(x):\n    samples = inverse_categorical(x.reshape(-1, 8192, 256))\n    samples = zounds.ArrayWithUnits(samples, dimensions=[\n        zounds.TimeDimension(*wscheme),\n        zounds.TimeDimension(*samplerate)\n    ])\n    return raw_samples_synthesize(samples)\n\n\ndef preprocess_categorical(x):\n    return categorical(x).reshape((-1, 256, 8192))\n\n\nclass CategoricalLoss(nn.NLLLoss):\n    def __init__(self):\n        super(CategoricalLoss, self).__init__()\n\n    def forward(self, input, target):\n        input = input.view(-1, 256)\n        target = target.view(-1, 256)\n        values, indices = target.max(dim=1)\n        return super(CategoricalLoss, self).forward(input, indices)\n\n\nclass FrequencyBandLoss(nn.MSELoss):\n    def __init__(self):\n        super(FrequencyBandLoss, self).__init__()\n\n    def forward(self, input, target):\n        target_samples = from_var(target).squeeze()\n        target_fft = np.fft.rfft(target_samples, axis=-1, norm='ortho')\n        target_fft[:, :50] = 0\n        recon = np.fft.irfft(target_fft, axis=-1, norm='ortho')\n        recon = to_var(recon)\n        return super(FrequencyBandLoss, self).forward(input, recon)\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--internet-archive-id',\n        type=str,\n        help='the internet archive id to use for training')\n    parser.add_argument(\n        '--epochs',\n        type=int,\n        help='the number of epochs to train the network')\n    parser.add_argument(\n        '--force-train',\n        action='store_true',\n        help='re-train the network, even if it has already been trained')\n    parser.add_argument(\n        '--categorical',\n        action='store_true',\n        help='use a categorical distribution of samples')\n    args = parser.parse_args()\n\n    if args.internet_archive_id:\n        zounds.ingest(\n            zounds.InternetArchive(args.internet_archive_id),\n            Sound,\n            multi_threaded=True)\n\n    if args.categorical:\n        network = CategoricalAutoEncoder()\n        loss = CategoricalLoss()\n        synthesize = categorical_synthesize\n        pipeline_cls = CategoricalAutoEncoderPipeline\n        data_preprocessor = label_preprocessor = preprocess_categorical\n        batch_size = 16\n    else:\n        network = RawSamplesAutoEncoder()\n        loss = FrequencyBandLoss()\n        synthesize = raw_samples_synthesize\n        pipeline_cls = AutoEncoderPipeline\n        data_preprocessor = label_preprocessor = lambda x: x\n        batch_size = 64\n        gen = (snd.windowed for snd in Sound\n               if args.internet_archive_id in snd._id)\n\n    if args.force_train or not AutoEncoderPipeline.exists():\n        trainer = zounds.SupervisedTrainer(\n            network,\n            loss,\n            lambda model: Adam(model.parameters(), lr=0.0001),\n            epochs=args.epochs,\n            batch_size=batch_size,\n            holdout_percent=0.25,\n            data_preprocessor=data_preprocessor,\n            label_preprocessor=label_preprocessor)\n\n        gen = (snd.windowed for snd in Sound\n               if args.internet_archive_id in snd._id)\n        pipeline_cls.process(samples=gen, trainer=trainer)\n\n    # instantiate the trained pipeline\n    pipeline = pipeline_cls()\n\n    snds = [snd for snd in Sound if args.internet_archive_id in snd._id]\n    snd = choice(snds)\n    time_slice = zounds.TimeSlice(duration=zounds.Seconds(10))\n    encoded = pipeline.pipeline.transform(\n        data_preprocessor(snd.windowed[time_slice]))\n    recon = encoded.inverse_transform()\n    samples = synthesize(recon)\n\n    # start up an in-browser REPL to interact with the results\n    app = zounds.ZoundsApp(\n        model=Sound,\n        audio_feature=Sound.ogg,\n        visualization_feature=Sound.windowed,\n        globals=globals(),\n        locals=locals())\n    app.start(8888)\n"""
examples/pytorch_gan.py,6,"b'import featureflow as ff\nimport numpy as np\nimport zounds\nfrom torch import nn\nfrom torch import optim\n\nsamplerate = zounds.SR11025()\nBaseModel = zounds.stft(resample_to=samplerate, store_fft=True)\n\nscale = zounds.GeometricScale(\n    start_center_hz=300,\n    stop_center_hz=3040,\n    bandwidth_ratio=0.07496,\n    n_bands=64)\nscale.ensure_overlap_ratio(0.5)\n\nLATENT_DIM = 100\n\n\n@zounds.simple_lmdb_settings(\'bach\', map_size=1e10, user_supplied_id=True)\nclass Sound(BaseModel):\n    """"""\n    An audio processing pipeline that computes a frequency domain representation\n    of the sound that follows a geometric scale\n    """"""\n    bark = zounds.ArrayWithUnitsFeature(\n        zounds.BarkBands,\n        samplerate=samplerate,\n        stop_freq_hz=samplerate.nyquist,\n        needs=BaseModel.fft,\n        store=True)\n\n    long_windowed = zounds.ArrayWithUnitsFeature(\n        zounds.SlidingWindow,\n        wscheme=zounds.SampleRate(\n            frequency=zounds.Milliseconds(358),\n            duration=zounds.Milliseconds(716)),\n        wfunc=zounds.OggVorbisWindowingFunc(),\n        needs=BaseModel.resampled,\n        store=True)\n\n    long_fft = zounds.ArrayWithUnitsFeature(\n        zounds.FFT,\n        needs=long_windowed,\n        store=True)\n\n    freq_adaptive = zounds.FrequencyAdaptiveFeature(\n        zounds.FrequencyAdaptiveTransform,\n        transform=np.fft.irfft,\n        scale=scale,\n        window_func=np.hanning,\n        needs=long_fft,\n        store=False)\n\n    rasterized = zounds.ArrayWithUnitsFeature(\n        lambda fa: fa.rasterize(64),\n        needs=freq_adaptive,\n        store=False)\n\n\nclass GeneratorLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(GeneratorLayer, self).__init__()\n        self.conv = nn.ConvTranspose2d(\n            in_features,\n            out_features,\n            kernel_size=(2, 2),\n            stride=(2, 2),\n            bias=False)\n        self.batch_norm = nn.BatchNorm2d(out_features)\n        self.relu = nn.LeakyReLU(0.2)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, inp):\n        x = self.conv(inp)\n        x = self.batch_norm(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        return x\n\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            GeneratorLayer(LATENT_DIM, 512),\n            GeneratorLayer(512, 256),\n            GeneratorLayer(256, 128),\n            GeneratorLayer(128, 64),\n            GeneratorLayer(64, 32),\n            nn.ConvTranspose2d(32, 1, (2, 2), (2, 2)),\n            nn.Tanh()\n        )\n\n    def forward(self, inp):\n        return self.main(inp.view(-1, LATENT_DIM, 1, 1))\n\n\nclass DiscriminatorLayer(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DiscriminatorLayer, self).__init__()\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=(2, 2),\n            stride=(2, 2),\n            bias=False)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.relu = nn.LeakyReLU(0.2)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, inp):\n        x = self.conv(inp)\n        x = self.batch_norm(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        return x\n\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            DiscriminatorLayer(1, 32),\n            DiscriminatorLayer(32, 64),\n            DiscriminatorLayer(64, 128),\n            DiscriminatorLayer(128, 256),\n            DiscriminatorLayer(256, 512),\n            nn.Conv2d(512, 1, (2, 2), (2, 2), bias=False),\n            nn.Sigmoid())\n\n    def forward(self, inp):\n        return self.main(inp.view(-1, 1, 64, 64))\n\n\n@zounds.simple_settings\nclass GanPipeline(ff.BaseModel):\n    docs = ff.PickleFeature(\n        ff.IteratorNode,\n        store=False)\n\n    shuffled = ff.PickleFeature(\n        zounds.ShuffledSamples,\n        nsamples=int(1e5),\n        dtype=np.float32,\n        needs=docs,\n        store=False)\n\n    mu_law = ff.PickleFeature(\n        zounds.MuLawCompressed,\n        needs=shuffled,\n        store=False)\n\n    scaled = ff.PickleFeature(\n        zounds.InstanceScaling,\n        needs=mu_law,\n        store=False)\n\n    network = ff.PickleFeature(\n        zounds.PyTorchGan,\n        trainer=zounds.GanTrainer(\n            generator=Generator(),\n            discriminator=Discriminator(),\n            loss=nn.BCELoss(),\n            generator_optim_func=lambda model: optim.Adam(\n                model.parameters(), lr=0.0002, betas=(0.5, 0.999)),\n            discriminator_optim_func=lambda model: optim.Adam(\n                model.parameters(), lr=0.00005, betas=(0.5, 0.999)),\n            latent_dimension=(LATENT_DIM,),\n            epochs=500,\n            batch_size=64),\n        needs=scaled,\n        store=False)\n\n    pipeline = ff.PickleFeature(\n        zounds.PreprocessingPipeline,\n        needs=(mu_law, scaled, network),\n        store=True)\n\n\nif __name__ == \'__main__\':\n    zounds.ingest(\n        zounds.InternetArchive(\'AOC11B\'),\n        Sound,\n        multi_threaded=True)\n\n    if not GanPipeline.exists():\n        GanPipeline.process(docs=(snd.rasterized for snd in Sound))\n\n    gan = GanPipeline()\n    noise = np.random.normal(0, 1, (32, LATENT_DIM))\n    generated_samples = gan.pipeline.transform(noise)\n\n    # start up an in-browser REPL to interact with the results\n    app = zounds.ZoundsApp(\n        model=Sound,\n        audio_feature=Sound.ogg,\n        visualization_feature=Sound.bark,\n        globals=globals(),\n        locals=locals())\n    app.start(8888)\n'"
examples/pytorch_supervised_learning.py,4,"b'""""""\nDemonstrate supervised learning using zounds.  First, perform a\nfrequency-adaptive transform (where time-resolution varies by frequency)\nof some bach pieces, and then rasterize these.\n\nUse a neural network to learn to recover the original frequency-adaptive\ntransform from the rasterized version.\n""""""\n\nimport featureflow as ff\nimport numpy as np\nimport zounds\nfrom torch import nn\nfrom torch import optim\nfrom random import choice\n\nsamplerate = zounds.SR11025()\nBaseModel = zounds.stft(resample_to=samplerate, store_fft=True)\n\nscale = zounds.GeometricScale(\n    start_center_hz=300,\n    stop_center_hz=3040,\n    bandwidth_ratio=0.07496,\n    n_bands=64)\nscale.ensure_overlap_ratio(0.5)\n\n\n@zounds.simple_lmdb_settings(\'bach\', map_size=1e10, user_supplied_id=True)\nclass Sound(BaseModel):\n    """"""\n    An audio processing pipeline that computes a frequency domain representation\n    of the sound that follows a geometric scale\n    """"""\n    bark = zounds.ArrayWithUnitsFeature(\n        zounds.BarkBands,\n        samplerate=samplerate,\n        stop_freq_hz=samplerate.nyquist,\n        needs=BaseModel.fft,\n        store=True)\n\n    long_windowed = zounds.ArrayWithUnitsFeature(\n        zounds.SlidingWindow,\n        wscheme=zounds.SampleRate(\n            frequency=zounds.Milliseconds(358),\n            duration=zounds.Milliseconds(716)),\n        wfunc=zounds.OggVorbisWindowingFunc(),\n        needs=BaseModel.resampled,\n        store=True)\n\n    long_fft = zounds.ArrayWithUnitsFeature(\n        zounds.FFT,\n        needs=long_windowed,\n        store=True)\n\n    freq_adaptive = zounds.FrequencyAdaptiveFeature(\n        zounds.FrequencyAdaptiveTransform,\n        transform=np.fft.irfft,\n        scale=scale,\n        window_func=np.hanning,\n        needs=long_fft,\n        store=False)\n\n    rasterized = zounds.ArrayWithUnitsFeature(\n        lambda fa: fa.rasterize(64),\n        needs=freq_adaptive,\n        store=False)\n\n\nclass Layer(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Layer, self).__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=(2, 2),\n            stride=(2, 2),\n            bias=False)\n        self.activation = nn.Tanh()\n\n    def forward(self, inp):\n        inp = self.conv(inp)\n        inp = self.activation(inp)\n        return inp\n\n\nclass UpScale(nn.Module):\n    def __init__(self):\n        super(UpScale, self).__init__()\n        self.main = nn.Sequential(\n            Layer(1, 4),\n            Layer(4, 16),\n            Layer(16, 64),\n            Layer(64, 256),\n            Layer(256, 1024),\n            Layer(1024, 8192))\n\n    def forward(self, inp):\n        output = self.main(inp.view(-1, 1, 64, 64))\n        return output.view(-1, 8192)\n\n\n@zounds.simple_settings\nclass FeatureTransfer(ff.BaseModel):\n    docs = ff.PickleFeature(\n        ff.IteratorNode,\n        store=False)\n\n    shuffled = ff.PickleFeature(\n        zounds.ShuffledSamples,\n        nsamples=int(1e5),\n        multiplexed=True,\n        dtype=np.float32,\n        needs=docs,\n        store=False)\n\n    mu_law_source = ff.PickleFeature(\n        zounds.MuLawCompressed,\n        needs=shuffled.aspect(\'data\'),\n        store=False)\n\n    scaled_source = ff.PickleFeature(\n        zounds.InstanceScaling,\n        needs=mu_law_source,\n        store=False)\n\n    mu_law_target = ff.PickleFeature(\n        zounds.MuLawCompressed,\n        needs=shuffled.aspect(\'labels\'),\n        store=False)\n\n    scaled_target = ff.PickleFeature(\n        zounds.InstanceScaling,\n        needs=mu_law_target,\n        store=False)\n\n    network = ff.PickleFeature(\n        zounds.PyTorchNetwork,\n        trainer=zounds.SupervisedTrainer(\n            model=UpScale(),\n            loss=nn.MSELoss(),\n            optimizer=lambda model: optim.Adam(model.parameters()),\n            epochs=20,\n            batch_size=64,\n            holdout_percent=0.5),\n        needs=dict(data=scaled_source, labels=scaled_target),\n        store=False)\n\n    pipeline = ff.PickleFeature(\n        zounds.PreprocessingPipeline,\n        needs=(mu_law_source, scaled_source, network),\n        store=True)\n\n\nif __name__ == \'__main__\':\n\n    zounds.ingest(\n        zounds.InternetArchive(\'AOC11B\'),\n        Sound,\n        multi_threaded=True)\n\n    if not FeatureTransfer.exists():\n        FeatureTransfer.process(\n            docs=(dict(data=doc.rasterized, labels=doc.freq_adaptive)\n                  for doc in Sound))\n\n    snds = list(Sound)\n    snd = choice(snds)\n\n    feature_transfer = FeatureTransfer()\n\n    synth = zounds.FrequencyAdaptiveFFTSynthesizer(scale, samplerate)\n\n    original = synth.synthesize(snd.freq_adaptive)\n\n    recon_coeffs = feature_transfer\\\n        .pipeline\\\n        .transform(snd.rasterized, wrapper=snd.freq_adaptive.like_dims)\\\n        .data\n\n    recon = synth.synthesize(zounds.inverse_mu_law(recon_coeffs))\n\n    # start up an in-browser REPL to interact with the results\n    app = zounds.ZoundsApp(\n        model=Sound,\n        audio_feature=Sound.ogg,\n        visualization_feature=Sound.bark,\n        globals=globals(),\n        locals=locals())\n    app.start(8888)\n'"
examples/pytorch_wgan.py,24,"b'\nimport numpy as np\nimport featureflow as ff\nimport zounds\nfrom torch import nn\nimport torch\nfrom torch.autograd import Variable\nimport argparse\nimport glob\nimport os\nfrom pytorch_wgan2 import \\\n    BaseGenerator, BaseCritic, CriticLayer, GeneratorLayer, FinalGeneratorLayer\nfrom scipy.signal import resample, tukey\nfrom random import choice\nimport torch.nn.functional as F\nfrom uuid import uuid4\nfrom functools import reduce\n\nsamplerate = zounds.SR11025()\nBaseModel = zounds.resampled(resample_to=samplerate, store_resampled=True)\n\nLATENT_DIM = 100\nSAMPLE_SIZE = 8192\nbands = (8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096)\nFIRST_FEATURE_MAP_SIZE = 64\nFACTOR = 1.2\n\nstops = tuple(np.cumsum(bands))\nslices = [slice(start, stop) for (start, stop) in zip((0,) + stops, stops)]\n\nSCALING = [\n    0.035643139891883342,\n    0.041599504468638721,\n    0.043825312492803623,\n    0.089081457396139319,\n    0.11216649030248733,\n    0.1755375826822119,\n    0.3011956255933676,\n    0.50373631894723525,\n    0.72654767098659556,\n    1.0668680716129715\n]\n\n\ndef perceptual(x):\n    coeffs = np.fft.rfft(x, norm=\'ortho\', axis=-1)\n    scale = zounds.LinearScale.from_sample_rate(samplerate, coeffs.shape[-1])\n    arr = zounds.ArrayWithUnits(\n        coeffs, [x.dimensions[0], zounds.FrequencyDimension(scale)])\n    arr *= zounds.AWeighting()\n    samples = np.fft.irfft(arr, norm=\'ortho\', axis=-1)\n    return zounds.ArrayWithUnits(samples, x.dimensions)\n\n\n@zounds.simple_lmdb_settings(\'wgan\', map_size=1e10, user_supplied_id=True)\nclass Sound(BaseModel):\n    windowed = zounds.ArrayWithUnitsFeature(\n        zounds.SlidingWindow,\n        wscheme=zounds.SampleRate(\n            frequency=samplerate.frequency * (SAMPLE_SIZE // 2),\n            duration=samplerate.frequency * SAMPLE_SIZE),\n        needs=BaseModel.resampled,\n        store=False)\n\n    perceptual = zounds.ArrayWithUnitsFeature(\n        perceptual,\n        needs=windowed)\n\n    decomposed = zounds.ArrayWithUnitsFeature(\n        lambda x: FrequencyDecomposition(x, bands).as_frequency_adaptive(),\n        needs=windowed)\n\n\ndef feature_map_size(inp, kernel, stride=1, padding=0):\n    return ((inp - kernel + (2 * padding)) / stride) + 1\n\n\nclass FrequencyDecomposition(object):\n    def __init__(self, samples, sizes, window=None):\n        self.window = window\n        self.sizes = sorted(sizes)\n        self.samples = samples\n\n        original = self.samples.copy()\n        self.bands = []\n        self.frequency_bands = []\n        start_hz = 0\n\n        for size in sizes:\n\n            # extract a frequency band\n            if size != self.size:\n                s = self._resample(original, size)\n            else:\n                s = original\n\n            self.bands.append(s)\n            original -= self._resample(s, self.size)\n\n            stop_hz = samplerate.nyquist * (size / self.size)\n            self.frequency_bands.append(zounds.FrequencyBand(start_hz, stop_hz))\n            start_hz = stop_hz\n\n    @classmethod\n    def _rs(cls, samples, desired_size, window=None):\n        axis = -1\n        w = window(samples.shape[axis]) if window else None\n        return resample(samples, desired_size, axis=axis, window=w)\n\n    def _resample(self, samples, desired_size):\n        return self._rs(samples, desired_size, self.window)\n\n    @classmethod\n    def synthesize_block(cls, block, window=None):\n        samples = np.zeros((len(block), SAMPLE_SIZE), dtype=block.dtype)\n        start = 0\n        for i, band in enumerate(bands):\n            stop = start + band\n            b = block[:, start: stop]\n            samples += cls._rs(b * SCALING[i], SAMPLE_SIZE, window=window)\n            start = stop\n        return samples\n\n    @property\n    def size(self):\n        return self.samples.shape[1]\n\n    def as_frequency_adaptive(self):\n        scale = zounds.ExplicitScale(self.frequency_bands)\n        bands = [b / SCALING[i] for i, b in enumerate(self.bands)]\n        return zounds.FrequencyAdaptive(\n            bands, scale=scale, time_dimension=self.samples.dimensions[0])\n\n    def synthesize_iter(self):\n        fa = self.as_frequency_adaptive()\n        samples = self.__class__.synthesize_block(fa)\n        for sample in samples:\n            yield sample, zounds.AudioSamples(sample, samplerate) \\\n                .pad_with_silence(zounds.Seconds(1))\n\n\nclass FDDiscriminator(nn.Module):\n    def __init__(self):\n        super(FDDiscriminator, self).__init__()\n        self.factor = FACTOR\n        self.layer_stacks = [[] for _ in bands]\n\n        self.feature_map_sizes = reduce(\n            lambda x, y: x + [int(x[-1] * FACTOR)],\n            range(len(bands) - 1),\n            [FIRST_FEATURE_MAP_SIZE])\n\n        for i, band in enumerate(bands):\n            for j in range(i + 1):\n                fms = self.feature_map_sizes[j]\n                first_layer = j == 0\n                in_channels = 1 if first_layer else self.feature_map_sizes[\n                    j - 1]\n                params = (8, 4, 0) if first_layer else (3, 2, 0)\n                layer = CriticLayer(in_channels, fms, *params)\n                self.layer_stacks[i].append(layer)\n                self.add_module(\'{i}{j}\'.format(**locals()), layer)\n\n        self.l1 = nn.Linear(sum(self.feature_map_sizes), 256, bias=False)\n        self.l2 = nn.Linear(256, 1, bias=False)\n\n    def forward(self, x):\n        fms = []\n        start_index = 0\n        for i, band in enumerate(bands):\n            stop = start_index + band\n            slce = x[:, start_index: stop]\n            start_index = stop\n            fm = slce.contiguous().view(-1, 1, band)\n            # subset = self.layers[:i + 1]\n            subset = self.layer_stacks[i]\n            for s in subset:\n                fm = s(fm)\n            fms.append(fm)\n\n        # push the band-wise frequency maps through some linear layers\n        flat = torch.cat(fms, dim=1).squeeze()\n        x = self.l1(flat)\n        x = F.leaky_relu(x, 0.2)\n        x = F.dropout(x, 0.2, self.training)\n\n        x = self.l2(x)\n        return x\n\n\nclass FDGenerator(nn.Module):\n    def __init__(self):\n        super(FDGenerator, self).__init__()\n        self.layer_stacks = [[] for _ in bands]\n        self.factor = FACTOR\n        self.feature_map_sizes = reduce(\n            lambda x, y: x + [int(x[-1] * FACTOR)],\n            range(len(bands) - 1),\n            [FIRST_FEATURE_MAP_SIZE])\n\n        for i, band in enumerate(bands):\n            for j in range(i + 1):\n                fms = self.feature_map_sizes[j]\n                first_layer = j == 0\n                out_channels = 1 if first_layer else self.feature_map_sizes[\n                    j - 1]\n                params = (8, 4, 0) if first_layer else (3, 2, 0)\n                cls = FinalGeneratorLayer if first_layer else GeneratorLayer\n                layer = cls(fms, out_channels, *params)\n                self.layer_stacks[i].append(layer)\n                self.add_module(\'{i}{j}\'.format(**locals()), layer)\n\n        total_features = sum(self.feature_map_sizes)\n        self.l1 = nn.Linear(LATENT_DIM, 256, bias=False)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.l2 = nn.Linear(256, total_features, bias=False)\n        self.bn2 = nn.BatchNorm1d(total_features)\n\n    def forward(self, x):\n\n        x = x.view(-1, LATENT_DIM)\n        x = self.l1(x)\n        x = self.bn1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = F.dropout(x, 0.2, self.training)\n\n        x = self.l2(x)\n        x = self.bn2(x)\n        x = F.leaky_relu(x, 0.2)\n        x = F.dropout(x, 0.2, self.training)\n\n        current = 0\n        bands = []\n        for i, fms in enumerate(self.feature_map_sizes):\n            stop = current + fms\n            segment = x[:, current: stop]\n            current = stop\n            fm = segment.contiguous().view(-1, segment.size()[1], 1)\n            # subset = self.layers[-(i + 1):]\n            subset = self.layer_stacks[i][::-1]\n            for s in subset:\n                fm = s(fm)\n            bands.append(fm.squeeze())\n\n        return torch.cat(bands, dim=1)\n\n\nclass Generator(BaseGenerator):\n    def __init__(self):\n        super(Generator, self).__init__(\n            (LATENT_DIM, 512, 4, 1, 0),\n            (512, 256, 8, 4, 2),\n            (256, 128, 8, 4, 2),\n            (128, 128, 8, 4, 2),\n            (128, 128, 8, 4, 2),\n            (128, 1, 16, 8, 4))\n\n\nclass Generator2(BaseGenerator):\n    def __init__(self):\n        super(Generator2, self).__init__(\n            (LATENT_DIM, 256, 4, 1, 0),\n            (256, 256, 8, 4, 2),\n            (256, 128, 8, 4, 2),\n            (128, 128, 8, 4, 2),\n            (128, 1, 126, 32, 47))\n\n\nclass Critic(BaseCritic):\n    def __init__(self):\n        super(Critic, self).__init__(\n            SAMPLE_SIZE,\n            (1, 64, 16, 8, 4),\n            (64, 128, 8, 4, 2),\n            (128, 128, 8, 4, 2),\n            (128, 128, 8, 4, 2),\n            (128, 256, 8, 4, 2),\n            (256, 512, 4, 1, 0),\n            (512, 1))\n\n\nclass Critic2(BaseCritic):\n    def __init__(self):\n        super(Critic2, self).__init__(\n            SAMPLE_SIZE,\n            (1, 128, 126, 32, 47),\n            (128, 128, 8, 4, 2),\n            (128, 256, 8, 4, 2),\n            (256, 256, 8, 4, 2),\n            (256, 512, 4, 1, 0),\n            (512, 1))\n\n\nclass GanPair(nn.Module):\n    def __init__(self):\n        super(GanPair, self).__init__()\n        self.generator = Generator()\n        self.discriminator = Critic()\n\n    def forward(self, x):\n        raise NotImplementedError()\n\n\ndef try_network():\n    z = np.random.normal(0, 1, (64, LATENT_DIM)).astype(np.float32)\n    t = torch.from_numpy(z)\n    v = Variable(t).cuda()\n    network = GanPair().cuda()\n    g = network.generator\n    c = network.discriminator\n\n    result = g(v, debug=True)\n    print(result.size())\n\n    labels = c(result, debug=True)\n    print(labels.size())\n\n\n@zounds.simple_settings\nclass Gan(ff.BaseModel):\n    samples = ff.PickleFeature(ff.IteratorNode)\n\n    shuffled = ff.PickleFeature(\n        zounds.ShuffledSamples,\n        nsamples=int(1e5),\n        dtype=np.float32,\n        needs=samples)\n\n    scaled = ff.PickleFeature(\n        zounds.InstanceScaling,\n        needs=shuffled)\n\n    wgan = ff.PickleFeature(\n        zounds.PyTorchGan,\n        trainer=ff.Var(\'trainer\'),\n        needs=scaled)\n\n    pipeline = ff.PickleFeature(\n        zounds.PreprocessingPipeline,\n        needs=(scaled, wgan,),\n        store=True)\n\n\n""""""\nLog\n- reduced sample size from 8192 to 4096.  No difference\n- Introduced batch norm.  Discriminator loss seems to stall out, and generator\n  produces very periodic sine-like sound with many harmonics\n- leaky RELU in discriminator seems to make little or no difference\n- tanh for all layers produces noise\n- leaky RELU in the generator seem to produce more plausible *looking* waveforms.\n  generated examples are a combination of a single tone and noise\n- add batch norm to the last generator layer - this seems to have really helped\n   with the visual appearance of generated samples\n\n\n- don\'t do instance scaling? there seems to be more variability, but still noise\n- try with mu_law this results in noise, and strong peaks at convolution boundaries.  Why do things totally break down with mu_law?\n- try the mu-law one-hot encoding.  this is very slow, and it\'s producing some pretty awful output.  Perhaps I should train longer with softmax.\n\n- try penalizing the norm (improved WGAN) much more variation.  Still some noise\n- try tanh all the way through - doesn\'t learn at all\n\n- try interpolating in sample space - learns a good deal of variety\n- try learning on downsampled 8192 samples -  there is some movement in the samples\n- try without dropout - this seems to be slightly worse/noisier\n\n- now that I\'m doing WGAN, try instance scaling again - this seems to be OK now, and produces more variation\n- try A-weighing the frequency domain and then IFFT back to samples- this seems to slightly improve variation\n- try without tanh? - doesn\'t get rid of the noise\n- try without batch norm in last layer? - doesn\'t change noise situation\n- try with tanh in first layer of discriminator? - doesn\'t change noise\n\n\n- can I scale up to 8192 and capture meaningful structure, even if noisy?\n    - yes, at least for Bach\n- try with phat drum loops\n    - yes, starts to learn drum-like sounds\n- try with speech\n    - yes, starts to learn speech-like sounds\n- try with a toy dataset so I can understand the biases/problems\n- try with Kevin Gates\n    - yes, learns speech-like sounds plus kick drums and bass\n- try with a mixture of different types of sample\n\n- residuals in the network\n- dilated convolutions\n- try adding batch norm back into discriminator?\n- try training on mdct - nope, learns nothing\n- progressively growing WGAN\n""""""\n\n\ndef load_and_play():\n    files = sorted(\n        glob.glob(\'*.npy\'),\n        cmp=lambda x, y: int(os.stat(x).st_ctime - os.stat(y).st_ctime))\n    most_recent = files[-1]\n    print(\'loading generated examples from\', most_recent)\n    results = np.load(most_recent)\n\n    # synthesized = FrequencyDecomposition.synthesize_block(results)\n    synthesized = results\n\n    for raw, result in zip(results, synthesized):\n        windowed = zounds.sliding_window(result, 512, 256)\n        spec = np.abs(np.fft.rfft(windowed))\n        audio_samples = zounds.AudioSamples(result, samplerate) \\\n            .pad_with_silence(zounds.Seconds(1))\n        yield raw, result, audio_samples / audio_samples.max(), spec\n\n\ndef synthetic():\n    for i in range(100):\n        duration = zounds.Seconds(np.random.randint(2, 20))\n        root = np.random.randint(50, 400)\n        hz = [root]\n        for _ in range(0):\n            hz.append(hz[-1] * 2)\n        synth = zounds.SineSynthesizer(samplerate)\n        s = synth.synthesize(duration, hz)\n        yield s.encode()\n\n\ndef ingest_all():\n    data = [\n        zounds.InternetArchive(\'AOC11B\'),\n        zounds.InternetArchive(\'Greatest_Speeches_of_the_20th_Century\'),\n        zounds.InternetArchive(\'Kevin_Gates_-_By_Any_Means-2014\'),\n        zounds.PhatDrumLoops()\n    ]\n    for d in data:\n        zounds.ingest(d, Sound, multi_threaded=True)\n\n\ndef ingest():\n    zounds.ingest(\n        zounds.InternetArchive(\'AOC11B\'),\n        Sound,\n        multi_threaded=True)\n\n    # for s in synthetic():\n    #     print Sound.process(meta=s, _id=uuid4().hex)\n\n\ndef ingest_and_train(epochs):\n    ingest()\n\n    network = GanPair()\n\n    def arg_maker(epoch):\n        z = np.random.normal(0, 1, (64, LATENT_DIM)).astype(np.float32)\n        t = torch.from_numpy(z)\n        v = Variable(t).cuda()\n        samples = network.generator(v).data.cpu().numpy().squeeze()\n        np.save(\'epoch\' + str(epoch), samples)\n        print(\'saved samples for epoch\', epoch)\n        return dict()\n\n    # out_channels = 128\n    # kernel_size = 126\n    # basis = np.zeros((out_channels, kernel_size), dtype=np.float32)\n    # synth = zounds.SineSynthesizer(samplerate)\n    # space = np.geomspace(50, samplerate.nyquist, out_channels)\n    # for i, freq in enumerate(space):\n    #     basis[i] = synth.synthesize(samplerate.frequency * kernel_size, [freq])\n\n    # basis = torch.from_numpy(basis[:, None, :]).cuda()\n    # basis = Variable(basis)\n\n    #\n    # gen_basis = torch.from_numpy(basis[:, None, :]).cuda()\n    # critic_basis = torch.from_numpy(basis[:, None, :]).cuda()\n    #\n    # print network.generator.main[-1].l1.weight.size()\n    # network.generator.main[-1].l1.weight.data = gen_basis\n    # network.generator.main[-1].l1.weight.requires_grad = False\n    #\n    # print network.discriminator.main[0].l1.weight.size()\n    # network.discriminator.main[0].l1.weight.data = critic_basis\n    # network.discriminator.main[0].l1.weight.requires_grad = False\n\n    # def generator_loss_term(network, samples):\n    #     result = F.conv1d(samples, basis, stride=64)\n    #     result = torch.abs(result)\n    #     mean = result.mean(dim=1)\n    #     std = result.std(dim=1)\n    #     result = mean / std\n    #     return result.mean() * 2500\n\n    if not Gan.exists():\n        trainer = zounds.WassersteinGanTrainer(\n            network=network,\n            latent_dimension=(LATENT_DIM,),\n            n_critic_iterations=10,\n            epochs=epochs,\n            batch_size=64,\n            arg_maker=arg_maker)\n        Gan.process(samples=(snd.perceptual for snd in Sound), trainer=trainer)\n\n    p = Gan()\n\n    def walk2(steps):\n        for i in range(steps):\n            yield np.random.normal(0, 1, LATENT_DIM)\n\n    def listen():\n        padding = zounds.Milliseconds(250)\n        z = np.concatenate(list(walk2(1000)))\n        result = p.pipeline.transform(z).data.squeeze()\n        x = np.concatenate([\n                               zounds.AudioSamples(j,\n                                                   samplerate).pad_with_silence(\n                                   padding)\n                               for j in result])\n        return zounds.AudioSamples(x, zounds.SR11025())\n\n    return listen()\n\n\ndef test_frequency_decomposition():\n    # snds = list(Sound)\n    # snd = choice(snds)\n    # fd = FrequencyDecomposition(snd.windowed, bands)\n    # print [band.shape for band in fd.bands]\n\n    gen = FDGenerator().cuda()\n\n    inp = torch.zeros(64, LATENT_DIM).normal_(0, 1)\n    inp = Variable(inp).cuda()\n    x = gen(inp)\n\n    print(x.size())\n\n    disc = FDDiscriminator().cuda()\n\n    # fa = fd.as_frequency_adaptive()[:64].astype(np.float32)\n    # print fa.shape, [fa[:, band].shape for band in fa.dimensions[1].scale]\n\n    # inp = torch.from_numpy(fa)\n    # inp = Variable(inp).cuda()\n\n    x = disc(x)\n    print(x.size())\n\n    print(gen.layers)\n    print(disc.layers)\n\n\ndef tweak():\n    snd = choice(list(Sound))\n    original = snd.windowed\n    windowed = original * np.hanning(SAMPLE_SIZE)\n    twindowed = original * tukey(SAMPLE_SIZE, 0.2)\n\n    ofd = FrequencyDecomposition(original, bands)\n    ofd2 = FrequencyDecomposition(original, bands, window=np.hanning)\n\n    wfd = FrequencyDecomposition(windowed, bands)\n    wfd2 = FrequencyDecomposition(windowed, bands, window=np.hanning)\n\n    tfd = FrequencyDecomposition(twindowed, bands)\n    tfd2 = FrequencyDecomposition(twindowed, bands, window=np.hanning)\n    return ofd, ofd2, wfd, wfd2, tfd, tfd2\n\n\ndef get_magnitudes():\n    snds = list(Sound)\n    snd = choice(snds)\n    x = snd.decomposed\n\n    magnitudes = []\n    scaled = []\n\n    start = 0\n    for band in bands:\n        stop = start + band\n        b = x[:, start: stop]\n        m = np.abs(b).mean()\n        magnitudes.append(m)\n        scaled.append(np.abs(b / m).mean())\n    print(magnitudes)\n    print(scaled)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--ingest\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'--train\',\n        help=\'ingest audio and train\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'--epochs\',\n        help=\'number of epochs to train\',\n        type=int)\n    parser.add_argument(\n        \'--try-network\',\n        help=\'dry run of network\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'--evaluate\',\n        help=\'listen to and view generated results\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'--test-decomposition\',\n        help=\'test out the frequency decomposition\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'--get-magnitudes\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'--tweak\',\n        action=\'store_true\')\n\n    args = parser.parse_args()\n    if args.train:\n        s = ingest_and_train(args.epochs)\n    elif args.ingest:\n        ingest()\n    elif args.try_network:\n        try_network()\n    elif args.evaluate:\n        result_iter = load_and_play()\n    elif args.test_decomposition:\n        test_frequency_decomposition()\n    elif args.get_magnitudes:\n        get_magnitudes()\n    elif args.tweak:\n        ofd, ofd2, wfd, wfd2, tfd, tfd2 = tweak()\n\n    # start up an in-browser REPL to interact with the results\n    app = zounds.ZoundsApp(\n        model=Sound,\n        audio_feature=Sound.ogg,\n        visualization_feature=Sound.windowed,\n        globals=globals(),\n        locals=locals())\n    app.start(9999)\n'"
examples/richard_nixon_identifier.py,6,"b'import featureflow as ff\nimport numpy as np\nimport zounds\nfrom torch import nn\nfrom torch import optim\nimport argparse\nfrom multiprocessing.pool import ThreadPool, cpu_count\n\nsamplerate = zounds.SR11025()\nBaseModel = zounds.stft(resample_to=samplerate, store_fft=True)\n\nscale = zounds.GeometricScale(\n    start_center_hz=300,\n    stop_center_hz=3040,\n    bandwidth_ratio=0.07496,\n    n_bands=64)\nscale.ensure_overlap_ratio(0.5)\n\n\n@zounds.simple_lmdb_settings(\'speeches\', map_size=1e10, user_supplied_id=True)\nclass Sound(BaseModel):\n    """"""\n    An audio processing pipeline that computes a frequency domain representation\n    of the sound that follows a geometric scale\n    """"""\n    bark = zounds.ArrayWithUnitsFeature(\n        zounds.BarkBands,\n        samplerate=samplerate,\n        stop_freq_hz=samplerate.nyquist,\n        needs=BaseModel.fft,\n        store=True)\n\n    long_windowed = zounds.ArrayWithUnitsFeature(\n        zounds.SlidingWindow,\n        wscheme=zounds.SampleRate(\n            frequency=zounds.Milliseconds(358),\n            duration=zounds.Milliseconds(716)),\n        wfunc=zounds.OggVorbisWindowingFunc(),\n        needs=BaseModel.resampled,\n        store=True)\n\n    long_fft = zounds.ArrayWithUnitsFeature(\n        zounds.FFT,\n        needs=long_windowed,\n        store=True)\n\n    freq_adaptive = zounds.FrequencyAdaptiveFeature(\n        zounds.FrequencyAdaptiveTransform,\n        transform=np.fft.irfft,\n        scale=scale,\n        window_func=np.hanning,\n        needs=long_fft,\n        store=False)\n\n    rasterized = zounds.ArrayWithUnitsFeature(\n        lambda fa: fa.rasterize(64),\n        needs=freq_adaptive,\n        store=False)\n\n\nclass DiscriminatorLayer(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size=(2, 2),\n            stride=None):\n\n        super(DiscriminatorLayer, self).__init__()\n\n        if stride is None:\n            stride = kernel_size\n\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            bias=False)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.relu = nn.LeakyReLU(0.2)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, inp):\n        x = self.conv(inp)\n        x = self.batch_norm(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        return x\n\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            DiscriminatorLayer(1, 32),\n            DiscriminatorLayer(32, 64),\n            DiscriminatorLayer(64, 128),\n            DiscriminatorLayer(128, 256),\n            DiscriminatorLayer(256, 512),\n            nn.Conv2d(512, 1, (2, 2), (2, 2), bias=False),\n            nn.Sigmoid())\n\n    def forward(self, inp):\n        return self.main(inp.view(-1, 1, 64, 64))\n\n\ndef speaker_identification_pipeline(epochs):\n    @zounds.simple_settings\n    class RichardNixonIdentifier(ff.BaseModel):\n        docs = ff.PickleFeature(\n            ff.IteratorNode,\n            store=False)\n\n        shuffled = ff.PickleFeature(\n            zounds.ShuffledSamples,\n            nsamples=int(1e5),\n            multiplexed=True,\n            dtype=np.float32,\n            needs=docs,\n            store=False)\n\n        mu_law_source = ff.PickleFeature(\n            zounds.MuLawCompressed,\n            needs=shuffled.aspect(\'data\'),\n            store=False)\n\n        scaled_source = ff.PickleFeature(\n            zounds.InstanceScaling,\n            needs=mu_law_source,\n            store=False)\n\n        network = ff.PickleFeature(\n            zounds.PyTorchNetwork,\n            trainer=zounds.SupervisedTrainer(\n                model=Discriminator(),\n                loss=nn.BCELoss(),\n                optimizer=lambda model:\n                optim.Adam(model.parameters(), lr=0.00005),\n                epochs=epochs,\n                batch_size=64,\n                holdout_percent=0.5),\n            needs=dict(data=scaled_source, labels=shuffled.aspect(\'labels\')),\n            store=False)\n\n        pipeline = ff.PickleFeature(\n            zounds.PreprocessingPipeline,\n            needs=(mu_law_source, scaled_source, network),\n            store=True)\n\n    return RichardNixonIdentifier\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--epochs\',\n        help=\'how many epochs (full passes over data) should the network train\',\n        default=100,\n        type=int)\n    parser.add_argument(\n        \'--force\',\n        help=\'retrain the network, even if its already been trained\',\n        action=\'store_true\',\n        default=False)\n\n    args = parser.parse_args()\n\n    zounds.ingest(\n        zounds.InternetArchive(\'Greatest_Speeches_of_the_20th_Century\'),\n        Sound,\n        multi_threaded=True)\n\n    def generate_training_and_test_set():\n        snds = list(Sound)\n\n        # get all sounds where Nixon is the speaker\n        nixon = [snd for snd in snds if \'Nixon\' in snd.meta[\'artist\']]\n\n        # get an equal number of speeches by anyone besides Nixon\n        not_nixon = filter(\n            lambda snd: \'Nixon\' not in snd.meta[\'artist\'], snds)[:len(nixon)]\n\n        for snd in nixon:\n            yield dict(\n                data=snd.rasterized,\n                labels=np.ones((len(snd.rasterized), 1)))\n\n        for snd in not_nixon[:len(nixon)]:\n            yield dict(\n                data=snd.rasterized,\n                labels=np.zeros((len(snd.rasterized), 1)))\n\n\n    RichardNixonIdentifier = speaker_identification_pipeline(args.epochs)\n\n    if not RichardNixonIdentifier.exists() or args.force:\n        RichardNixonIdentifier.process(docs=generate_training_and_test_set())\n\n    rni = RichardNixonIdentifier()\n\n    # start up an in-browser REPL to interact with the results\n    app = zounds.ZoundsApp(\n        model=Sound,\n        audio_feature=Sound.ogg,\n        visualization_feature=Sound.bark,\n        globals=globals(),\n        locals=locals())\n    app.start(8888)\n'"
examples/scales.py,0,"b""import zounds\n\nsamplerate = zounds.SR22050()\nBaseModel = zounds.stft(resample_to=samplerate, store_fft=True)\n\n\n@zounds.simple_in_memory_settings\nclass Sound(BaseModel):\n    pass\n\n\nif __name__ == '__main__':\n    url = 'https://ia802606.us.archive.org/9/items/AOC11B/onclassical_luisi_bach_partita_e-minor_bwv-830_3.ogg'\n    _id = Sound.process(meta=url)\n    snd = Sound(_id)\n\n    band = zounds.FrequencyBand(50, samplerate.nyquist)\n    bark_scale = zounds.BarkScale(band, 100)\n    mel_scale = zounds.MelScale(band, 100)\n    chroma_scale = zounds.ChromaScale(band)\n\n    bark_bands = bark_scale.apply(snd.fft, zounds.HanningWindowingFunc())\n    mel_bands = mel_scale.apply(snd.fft, zounds.HanningWindowingFunc())\n    chroma_bands = chroma_scale.apply(snd.fft, zounds.HanningWindowingFunc())\n\n    app = zounds.ZoundsApp(\n        model=Sound,\n        visualization_feature=Sound.fft,\n        audio_feature=Sound.ogg,\n        globals=globals(),\n        locals=locals())\n    app.start(9999)"""
examples/segmentsearch.py,2,"b""import featureflow as ff\nimport zounds\nimport numpy as np\n\nwindowing = zounds.HalfLapped()\n\n\n# Segment audio files #########################################################\n\nclass Settings(ff.PersistenceSettings):\n    id_provider = ff.UserSpecifiedIdProvider(key='_id')\n    key_builder = ff.StringDelimitedKeyBuilder(seperator='|')\n    database = ff.LmdbDatabase(path='onsetdata', key_builder=key_builder)\n    event_log = ff.EventLog(\n        path='onsetdataevents', channel=ff.InMemoryChannel())\n\n\nSTFT = zounds.stft(\n    resample_to=zounds.SR11025(),\n    wscheme=windowing)\n\n\nclass WithOnsets(STFT, Settings):\n    bark = zounds.ArrayWithUnitsFeature(\n        zounds.BarkBands,\n        needs=STFT.fft,\n        store=True)\n\n    transience = zounds.ArrayWithUnitsFeature(\n        zounds.MeasureOfTransience,\n        needs=STFT.fft,\n        store=True)\n\n    sliding_detection = zounds.ArrayWithUnitsFeature(\n        zounds.SlidingWindow,\n        needs=transience,\n        wscheme=windowing * zounds.Stride(frequency=1, duration=11),\n        padwith=5,\n        store=False)\n\n    slices = zounds.TimeSliceFeature(\n        zounds.MovingAveragePeakPicker,\n        needs=sliding_detection,\n        aggregate=np.median,\n        store=True)\n\n\n# Learn K-Means Clusters ######################################################\n\n@zounds.simple_settings\nclass BarkKmeans(ff.BaseModel):\n    docs = ff.Feature(\n        ff.IteratorNode,\n        store=False)\n\n    shuffle = ff.NumpyFeature(\n        zounds.ShuffledSamples,\n        nsamples=int(1e6),\n        needs=docs,\n        store=True)\n\n    unitnorm = ff.PickleFeature(\n        zounds.UnitNorm,\n        needs=shuffle,\n        store=False)\n\n    kmeans = ff.PickleFeature(\n        zounds.KMeans,\n        centroids=128,\n        needs=unitnorm,\n        store=False)\n\n    pipeline = ff.PickleFeature(\n        zounds.PreprocessingPipeline,\n        needs=(unitnorm, kmeans),\n        store=True)\n\n\n# Store the K-Means encoding of Bark Bands ##############################\n\nclass WithCodes(WithOnsets):\n    bark_kmeans = zounds.ArrayWithUnitsFeature(\n        zounds.Learned,\n        # this feature will be computed using the learned K-Means clusters\n        learned=BarkKmeans(),\n        needs=WithOnsets.bark,\n        store=True)\n\n    pooled = zounds.VariableRateTimeSeriesFeature(\n        zounds.Pooled,\n        needs=(bark_kmeans, WithOnsets.slices),\n        op=np.max,\n        axis=0,\n        store=True)\n\n\nif __name__ == '__main__':\n    index = zounds.HammingIndex(WithCodes, WithCodes.pooled, listen=True)\n\n    zounds.ingest(\n        zounds.PhatDrumLoops(),\n        WithOnsets,\n        multi_threaded=True)\n\n    # learn K-Means centroids from the drum hits\n    if not BarkKmeans.exists():\n        print('learning K-Means clusters')\n        BarkKmeans.process(docs=(wo.bark for wo in WithOnsets))\n\n    # bark_kmeans = BarkKmeans()\n\n    # force the new pooled feature to be computed\n    for doc in WithCodes:\n        print(doc.pooled.slicedata.shape)\n\n    results = index.random_search(n_results=5)\n\n    app = zounds.ZoundsSearch(\n        model=WithCodes,\n        audio_feature=WithCodes.ogg,\n        visualization_feature=WithCodes.bark,\n        search=index,\n        n_results=5)\n\n    app.start(8888)\n"""
examples/spectrogram_embedding.py,1,"b'""""""\nUse a triplet-loss to learn a similarity metric between short spectrograms\n\nUNSUPERVISED LEARNING OF SEMANTIC AUDIO REPRESENTATIONS\nhttps://arxiv.org/pdf/1711.02209.pdf\n""""""\n\nimport numpy as np\nimport zounds\nfrom zounds.spectral import apply_scale\n\nsamplerate = zounds.SR11025()\nBaseModel = zounds.resampled(resample_to=samplerate, store_resampled=True)\n\nscale_bands = 96\nspectrogram_duration = 64\n\nanchor_slice = slice(spectrogram_duration, spectrogram_duration * 2)\n\nscale = zounds.GeometricScale(\n    start_center_hz=50,\n    stop_center_hz=samplerate.nyquist,\n    bandwidth_ratio=0.115,\n    n_bands=scale_bands)\nscale.ensure_overlap_ratio()\n\nspectrogram_duration = 64\n\nwindowing_scheme = zounds.HalfLapped()\nspectrogram_sample_rate = zounds.SampleRate(\n    frequency=windowing_scheme.frequency * (spectrogram_duration // 2),\n    duration=windowing_scheme.frequency * spectrogram_duration)\n\n\ndef spectrogram(x):\n    x = apply_scale(\n        np.abs(x.real), scale, window=zounds.OggVorbisWindowingFunc())\n    x = zounds.log_modulus(x * 100)\n    return x * zounds.AWeighting()\n\n\n@zounds.simple_lmdb_settings(\n    \'spectrogram_embedding\', map_size=1e11, user_supplied_id=True)\nclass Sound(BaseModel):\n    short_windowed = zounds.ArrayWithUnitsFeature(\n        zounds.SlidingWindow,\n        wscheme=windowing_scheme,\n        wfunc=zounds.OggVorbisWindowingFunc(),\n        needs=BaseModel.resampled)\n\n    fft = zounds.ArrayWithUnitsFeature(\n        zounds.FFT,\n        padding_samples=1024,\n        needs=short_windowed)\n\n    geom = zounds.ArrayWithUnitsFeature(\n        spectrogram,\n        needs=fft)\n\n    log_spectrogram = zounds.ArrayWithUnitsFeature(\n        zounds.SlidingWindow,\n        wscheme=zounds.SampleRate(\n            frequency=windowing_scheme.frequency * (spectrogram_duration // 2),\n            duration=windowing_scheme.frequency * spectrogram_duration * 3),\n        needs=geom)\n\n    ls = zounds.ArrayWithUnitsFeature(\n        zounds.SlidingWindow,\n        wscheme=spectrogram_sample_rate,\n        needs=geom)\n\nif __name__ == \'__main__\':\n    _id = Sound.process(\n        meta=\'https://ia802606.us.archive.org/9/items/AOC11B/onclassical_luisi_bach_partita_B-flat-major_bwv-825_1.ogg\')\n\n    snd = Sound(_id)\n\n    app = zounds.ZoundsApp(\n        model=Sound,\n        visualization_feature=Sound.geom,\n        audio_feature=Sound.ogg,\n        globals=globals(),\n        locals=locals())\n    app.start(9000)\n'"
examples/timbre.py,0,"b""import os\nfrom urllib.parse import urlparse\nimport featureflow as ff\nimport requests\nimport zounds\n\n\nclass Settings(ff.PersistenceSettings):\n    id_provider = ff.UserSpecifiedIdProvider('_id')\n    key_builder = ff.StringDelimitedKeyBuilder(seperator='|')\n    database = ff.LmdbDatabase(path='timbre', key_builder=key_builder)\n    event_log = ff.EventLog('timbre_events', channel=ff.InMemoryChannel())\n\n\nwindowing = zounds.HalfLapped()\nSTFT = zounds.stft(resample_to=zounds.SR22050(), wscheme=windowing)\n\n\nclass WithTimbre(STFT, Settings):\n    bark = zounds.ArrayWithUnitsFeature(\n        zounds.BarkBands,\n        needs=STFT.fft,\n        store=True)\n\n    bfcc = zounds.ArrayWithUnitsFeature(\n        zounds.BFCC,\n        needs=bark,\n        store=True)\n\n\n@zounds.simple_settings\nclass BfccKmeans(ff.BaseModel):\n    docs = ff.Feature(\n        ff.IteratorNode,\n        store=False)\n\n    shuffle = ff.NumpyFeature(\n        zounds.ReservoirSampler,\n        nsamples=1e6,\n        needs=docs,\n        store=True)\n\n    unitnorm = ff.PickleFeature(\n        zounds.UnitNorm,\n        needs=shuffle,\n        store=False)\n\n    kmeans = ff.PickleFeature(\n        zounds.KMeans,\n        centroids=128,\n        needs=unitnorm,\n        store=False)\n\n    pipeline = ff.PickleFeature(\n        zounds.PreprocessingPipeline,\n        needs=(unitnorm, kmeans),\n        store=True)\n\n\nclass WithCodes(WithTimbre):\n    bfcc_kmeans = zounds.ArrayWithUnitsFeature(\n        zounds.Learned,\n        learned=BfccKmeans(),\n        needs=WithTimbre.bfcc,\n        store=True)\n\n    sliding_bfcc_kmeans = zounds.ArrayWithUnitsFeature(\n        zounds.SlidingWindow,\n        needs=bfcc_kmeans,\n        wscheme=windowing * zounds.Stride(frequency=30, duration=30),\n        store=False)\n\n    bfcc_kmeans_pooled = zounds.ArrayWithUnitsFeature(\n        zounds.Max,\n        needs=sliding_bfcc_kmeans,\n        axis=1,\n        store=True)\n\n\ndef download_zip_archive():\n    # Download the zip archive\n    url = 'https://archive.org/download/FlavioGaete/FlavioGaete22.zip'\n    filename = os.path.split(urlparse(url).path)[-1]\n\n    if not os.path.exists(filename):\n        resp = requests.get(url, stream=True)\n\n        print('Downloading {url} -> {filename}...'.format(**locals()))\n\n        with open(filename, 'wb') as f:\n            for chunk in resp.iter_content(chunk_size=1000000):\n                f.write(chunk)\n\n    return filename\n\nif __name__ == '__main__':\n    index = zounds.HammingIndex(\n        WithCodes, WithCodes.bfcc_kmeans_pooled, listen=True)\n\n    zip_filename = download_zip_archive()\n\n    print('Processing Audio...')\n    for zf in ff.iter_zip(zip_filename):\n\n        if '._' in zf.filename:\n            continue\n\n        try:\n            print('processing {zf.filename}'.format(**locals()))\n            WithTimbre.process(\n                _id=zf.filename, meta=zf, raise_if_exists=True)\n        except ff.ModelExistsError as e:\n            print(e)\n\n    # learn K-Means centroids\n    try:\n        print('learning K-Means centroids')\n        BfccKmeans.process(\n            docs=(wt.bfcc for wt in WithTimbre), raise_if_exists=True)\n    except ff.ModelExistsError:\n        pass\n\n    # force the new features to be computed, so they're pushed into the index\n    for wc in WithCodes:\n        print(wc.bfcc_kmeans_pooled)\n\n    app = zounds.ZoundsSearch(\n        model=WithTimbre,\n        audio_feature=WithTimbre.ogg,\n        visualization_feature=WithTimbre.bark,\n        search=index,\n        n_results=10)\n    app.start(8888)\n"""
zounds/__init__.py,0,"b""__version__ = '1.57.0'\n\nfrom .timeseries import \\\n    Hours, Minutes, Seconds, Milliseconds, Microseconds, Picoseconds, \\\n    SR11025, SR16000, SR22050, SR44100, SR48000, SR96000, HalfLapped, Stride, \\\n    TimeSlice, VariableRateTimeSeries, VariableRateTimeSeriesFeature, \\\n    SampleRate, AudioSamples, TimeDimension, audio_sample_rate, \\\n    ConstantRateTimeSeries, nearest_audio_sample_rate\n\nfrom .soundfile import \\\n    MetaData, AudioMetaData, AudioMetaDataEncoder, OggVorbis, \\\n    OggVorbisDecoder, OggVorbisEncoder, OggVorbisFeature, OggVorbisWrapper, \\\n    AudioStream, Resampler, ChunkSizeBytes\n\nfrom .spectral import \\\n    SlidingWindow, OggVorbisWindowingFunc, WindowingFunc, \\\n    FFT, MDCT, DCT, DCTIV, BarkBands, Chroma, BFCC, SpectralCentroid, \\\n    SpectralFlatness, AWeighting, LinearScale, FrequencyBand, \\\n    FrequencyScale, FrequencyDimension, GeometricScale, HanningWindowingFunc, \\\n    FrequencyAdaptiveTransform, ExplicitScale, ExplicitFrequencyDimension, \\\n    FrequencyAdaptive, FrequencyWeighting, Hertz, Hz, BarkScale, MelScale, \\\n    ChromaScale, fir_filter_bank\n\nfrom .loudness import \\\n    log_modulus, inverse_log_modulus, decibel, mu_law, MuLaw, LogModulus, \\\n    inverse_mu_law, instance_scale, inverse_one_hot\n\nfrom .segment import MeasureOfTransience, MovingAveragePeakPicker, \\\n    ComplexDomain, TimeSliceFeature\n\nfrom .synthesize import \\\n    FFTSynthesizer, DCTSynthesizer, TickSynthesizer, NoiseSynthesizer, \\\n    SineSynthesizer, DCTIVSynthesizer, MDCTSynthesizer, \\\n    FrequencyAdaptiveFFTSynthesizer, FrequencyAdaptiveDCTSynthesizer, \\\n    SilenceSynthesizer, WindowedAudioSynthesizer\n\nfrom .learn import \\\n    KMeans, Learned, MeanStdNormalization, UnitNorm, Log, Multiply, \\\n    PreprocessingPipeline, Slicer, ReservoirSampler, simple_settings, \\\n    SklearnModel, WithComponents, InstanceScaling, Reshape, ShuffledSamples, \\\n    PyTorchNetwork, PyTorchGan, PyTorchAutoEncoder,WassersteinGanTrainer, \\\n    SupervisedTrainer, TripletEmbeddingTrainer, Weighted, MuLawCompressed, \\\n    SimHash, AbsoluteValue, Binarize, Sharpen, learning_pipeline, \\\n    object_store_pipeline_settings, infinite_streaming_learning_pipeline, \\\n    SincLayer\n\nfrom .ui import \\\n    ZoundsApp, ZoundsSearch, TrainingMonitorApp, SupervisedTrainingMonitorApp, \\\n    TripletEmbeddingMonitorApp, GanTrainingMonitorApp, \\\n    RangeUnitUnsupportedException, ObjectStorageSettings, AppSettings, \\\n    NeuralNetworkTrainingSettings\n\nfrom .index import \\\n    SearchResults, HammingDb, HammingIndex, BruteForceSearch, \\\n    HammingDistanceBruteForceSearch\n\nfrom .basic import \\\n    Slice, Sum, Max, Pooled, process_dir, stft, audio_graph, with_onsets, \\\n    resampled, frequency_adaptive, windowed\n\nfrom .util import \\\n    simple_lmdb_settings, simple_in_memory_settings, \\\n    simple_object_storage_settings\n\nfrom .nputil import sliding_window\n\nfrom .core import IdentityDimension, ArrayWithUnits\n\nfrom .persistence import \\\n    ArrayWithUnitsFeature, AudioSamplesFeature, FrequencyAdaptiveFeature\n\nfrom .datasets import \\\n    PhatDrumLoops, InternetArchive, FreeSoundSearch, DataSetCache, Directory, \\\n    ingest, MusicNet, NSynth, CompositeDataset"""
docs/source/__init__.py,0,b''
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# zounds documentation build configuration file, created by\n# sphinx-quickstart on Fri Aug 25 12:43:15 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport os\nimport sys\nimport sphinx_rtd_theme\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, \'.\')\nsys.path.insert(0, os.path.abspath(\'../../\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.napoleon\'\n]\n\nautosummary_generate = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n# source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'zounds\'\ncopyright = \'2017, John Vinyard\'\nauthor = \'John Vinyard\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'0.46.0\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'0.46.0\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = []\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n# default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\nadd_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n# keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n# html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n# html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n# html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n# html_extra_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n# html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n# html_domain_indices = True\n\n# If false, no index is generated.\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n# html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n# html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n# html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\'\n# html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# Now only \'ja\' uses this config value\n# html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n# html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'zoundsdoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'zounds.tex\', \'zounds Documentation\',\n     \'John Vinyard\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n\n# If false, no module index is generated.\n# latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'zounds\', \'zounds Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n# man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'zounds\', \'zounds Documentation\',\n     author, \'zounds\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\nautodoc_member_order = \'bysource\'\n\n# Documents to append as an appendix to all manuals.\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n# texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n# texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n# texinfo_no_detailmenu = False\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n}\n\n# Fake imports\nimport mock\n\nMOCK_MODULES = [\n    \'soundfile\',\n    \'featureflow\',\n\n    \'numpy\',\n    \'numpy.lib\',\n    \'numpy.lib.stride_tricks\',\n    \'numpy.ma\',\n    \'numpy.core\',\n    \'numpy.core.multiarray\',\n    \'numpy.random\',\n\n    \'scipy\',\n    \'scipy.signal\',\n    \'scipy.fftpack\',\n    \'scipy.stats\',\n    \'scipy.stats.mstats\',\n    \'scipy.cluster\',\n    \'scipy.cluster.vq\',\n    \'scipy.spatial\',\n    \'scipy.spatial.distance\',\n\n    \'matplotlib\',\n\n    \'zounds.nputil.countbits\',\n    \'countbits\',\n\n    \'torch\',\n    \'torch.nn\',\n    \'torch.nn.functional\',\n    \'torch.optim\',\n    \'torch.autograd\',\n\n    \'tornado\',\n    \'tornado.ioloop\',\n    \'tornado.web\',\n    \'tornado.websocket\',\n\n    \'ujson\',\n    \'lmdb\'\n]\n\n\nclass ZoundsDocsMock(mock.Mock):\n    __version__ = 1.0\n\n    ndarray = object\n    Module = object\n    Node = object\n    Aggregator = str\n\n    def __init__(self, *args, **kwargs):\n        super(ZoundsDocsMock, self).__init__(side_effect=None)\n\n    def __mul__(self, other):\n        return ZoundsDocsMock()\n\n    def __rmul__(self, other):\n        return ZoundsDocsMock()\n\n    def __floordiv__(self, other):\n        return ZoundsDocsMock()\n\n    def __rfloordiv__(self, other):\n        return ZoundsDocsMock()\n\n    def __truediv__(self, other):\n        return ZoundsDocsMock()\n\n    def __rdiv__(self, other):\n        return ZoundsDocsMock()\n\n    def __truediv__(self, other):\n        return ZoundsDocsMock()\n\n    def __rtruediv__(self, other):\n        return ZoundsDocsMock()\n\n    def __le__(self, other):\n        return None\n\n\nZoundsDocsMock.timedelta64 = ZoundsDocsMock\n\n\nfor mod_name in MOCK_MODULES:\n    sys.modules[mod_name] = ZoundsDocsMock()\n'"
zounds/basic/__init__.py,0,"b'from .basic import Slice, Sum, Max, Pooled, Binarize\n\nfrom .util import process_dir\n\nfrom .audiograph import \\\n    resampled, stft, audio_graph, with_onsets, frequency_adaptive, windowed\n\n'"
zounds/basic/audiograph.py,3,"b'import numpy as np\nfrom featureflow import BaseModel, JSONFeature, ByteStream, ByteStreamFeature\nfrom zounds.soundfile import \\\n    MetaData, AudioMetaDataEncoder, OggVorbis, OggVorbisFeature, AudioStream, \\\n    Resampler, ChunkSizeBytes\nfrom zounds.segment import \\\n    ComplexDomain, MovingAveragePeakPicker, TimeSliceFeature\nfrom zounds.persistence import ArrayWithUnitsFeature, AudioSamplesFeature, \\\n    FrequencyAdaptiveFeature\nfrom zounds.timeseries import SR44100, HalfLapped, Stride, Seconds\nfrom zounds.spectral import \\\n    SlidingWindow, OggVorbisWindowingFunc, FFT, BarkBands, SpectralCentroid, \\\n    Chroma, BFCC, DCT, FrequencyAdaptiveTransform, FrequencyBand\n\nDEFAULT_CHUNK_SIZE = ChunkSizeBytes(\n    samplerate=SR44100(),\n    duration=Seconds(30),\n    bit_depth=16,\n    channels=2)\n\n\ndef resampled(\n        chunksize_bytes=DEFAULT_CHUNK_SIZE,\n        resample_to=SR44100(),\n        store_resampled=False):\n    """"""\n    Create a basic processing pipeline that can resample all incoming audio\n    to a normalized sampling rate for downstream processing, and store a\n    convenient, compressed version for playback\n\n    :param chunksize_bytes: The number of bytes from the raw stream to process\n    at once\n    :param resample_to: The new, normalized sampling rate\n    :return: A simple processing pipeline\n    """"""\n\n    class Resampled(BaseModel):\n        meta = JSONFeature(\n            MetaData,\n            store=True,\n            encoder=AudioMetaDataEncoder)\n\n        raw = ByteStreamFeature(\n            ByteStream,\n            chunksize=chunksize_bytes,\n            needs=meta,\n            store=False)\n\n        ogg = OggVorbisFeature(\n            OggVorbis,\n            needs=raw,\n            store=True)\n\n        pcm = AudioSamplesFeature(\n            AudioStream,\n            needs=raw,\n            store=False)\n\n        resampled = AudioSamplesFeature(\n            Resampler,\n            needs=pcm,\n            samplerate=resample_to,\n            store=store_resampled)\n\n    return Resampled\n\n\ndef windowed(\n        wscheme,\n        chunksize_bytes=DEFAULT_CHUNK_SIZE,\n        resample_to=SR44100(),\n        store_resampled=True,\n        store_windowed=False,\n        wfunc=None):\n\n    rs = resampled(\n        chunksize_bytes=chunksize_bytes,\n        resample_to=resample_to,\n        store_resampled=store_resampled)\n\n    class Sound(rs):\n        windowed = ArrayWithUnitsFeature(\n            SlidingWindow,\n            wscheme=wscheme,\n            wfunc=wfunc,\n            needs=rs.resampled,\n            store=store_windowed)\n\n    return Sound\n\n\ndef frequency_adaptive(\n        long_window_sample_rate,\n        scale,\n        store_freq_adaptive=False,\n        check_scale_overlap_ratio=False,\n        chunksize_bytes=DEFAULT_CHUNK_SIZE,\n        resample_to=SR44100(),\n        store_resampled=False):\n    BaseModel = resampled(chunksize_bytes, resample_to, store_resampled)\n\n    class FrequencyAdaptive(BaseModel):\n        long_windowed = ArrayWithUnitsFeature(\n            SlidingWindow,\n            wscheme=long_window_sample_rate,\n            wfunc=OggVorbisWindowingFunc(),\n            needs=BaseModel.resampled,\n            store=False)\n\n        long_fft = ArrayWithUnitsFeature(\n            FFT,\n            needs=long_windowed,\n            store=False)\n\n        freq_adaptive = FrequencyAdaptiveFeature(\n            FrequencyAdaptiveTransform,\n            transform=np.fft.irfft,\n            scale=scale,\n            check_scale_overlap_ratio=check_scale_overlap_ratio,\n            window_func=np.hanning,\n            needs=long_fft,\n            store=store_freq_adaptive)\n\n    return FrequencyAdaptive\n\n\ndef stft(\n        chunksize_bytes=DEFAULT_CHUNK_SIZE,\n        resample_to=SR44100(),\n        wscheme=HalfLapped(),\n        store_fft=False,\n        fft_padding_samples=0,\n        store_windowed=False,\n        store_resampled=False):\n    class ShortTimeFourierTransform(BaseModel):\n        meta = JSONFeature(\n            MetaData,\n            store=True,\n            encoder=AudioMetaDataEncoder)\n\n        raw = ByteStreamFeature(\n            ByteStream,\n            chunksize=chunksize_bytes,\n            needs=meta,\n            store=False)\n\n        ogg = OggVorbisFeature(\n            OggVorbis,\n            needs=raw,\n            store=True)\n\n        pcm = AudioSamplesFeature(\n            AudioStream,\n            needs=raw,\n            store=False)\n\n        resampled = AudioSamplesFeature(\n            Resampler,\n            needs=pcm,\n            samplerate=resample_to,\n            store=store_resampled)\n\n        windowed = ArrayWithUnitsFeature(\n            SlidingWindow,\n            needs=resampled,\n            wscheme=wscheme,\n            wfunc=OggVorbisWindowingFunc(),\n            store=store_windowed)\n\n        fft = ArrayWithUnitsFeature(\n            FFT,\n            padding_samples=fft_padding_samples,\n            needs=windowed,\n            store=store_fft)\n\n    return ShortTimeFourierTransform\n\n\ndef audio_graph(\n        chunksize_bytes=DEFAULT_CHUNK_SIZE,\n        resample_to=SR44100(),\n        store_fft=False):\n    """"""\n    Produce a base class suitable as a starting point for many audio processing\n    pipelines.  This class resamples all audio to a common sampling rate, and\n    produces a bark band spectrogram from overlapping short-time fourier\n    transform frames.  It also compresses the audio into ogg vorbis format for\n    compact storage.\n    """"""\n\n    band = FrequencyBand(20, resample_to.nyquist)\n\n    class AudioGraph(BaseModel):\n        meta = JSONFeature(\n            MetaData,\n            store=True,\n            encoder=AudioMetaDataEncoder)\n\n        raw = ByteStreamFeature(\n            ByteStream,\n            chunksize=chunksize_bytes,\n            needs=meta,\n            store=False)\n\n        ogg = OggVorbisFeature(\n            OggVorbis,\n            needs=raw,\n            store=True)\n\n        pcm = AudioSamplesFeature(\n            AudioStream,\n            needs=raw,\n            store=False)\n\n        resampled = AudioSamplesFeature(\n            Resampler,\n            needs=pcm,\n            samplerate=resample_to,\n            store=False)\n\n        windowed = ArrayWithUnitsFeature(\n            SlidingWindow,\n            needs=resampled,\n            wscheme=HalfLapped(),\n            wfunc=OggVorbisWindowingFunc(),\n            store=False)\n\n        dct = ArrayWithUnitsFeature(\n            DCT,\n            needs=windowed,\n            store=True)\n\n        fft = ArrayWithUnitsFeature(\n            FFT,\n            needs=windowed,\n            store=store_fft)\n\n        bark = ArrayWithUnitsFeature(\n            BarkBands,\n            needs=fft,\n            frequency_band=band,\n            store=True)\n\n        centroid = ArrayWithUnitsFeature(\n            SpectralCentroid,\n            needs=bark,\n            store=True)\n\n        chroma = ArrayWithUnitsFeature(\n            Chroma,\n            needs=fft,\n            frequency_band=band,\n            store=True)\n\n        bfcc = ArrayWithUnitsFeature(\n            BFCC,\n            needs=fft,\n            store=True)\n\n    return AudioGraph\n\n\ndef with_onsets(fft_feature):\n    """"""\n    Produce a mixin class that extracts onsets\n    :param fft_feature: The short-time fourier transform feature\n    :return: A mixin class that extracts onsets\n    """"""\n\n    class Onsets(BaseModel):\n        onset_prep = ArrayWithUnitsFeature(\n            SlidingWindow,\n            needs=fft_feature,\n            wscheme=HalfLapped() * Stride(frequency=1, duration=3),\n            store=False)\n\n        complex_domain = ArrayWithUnitsFeature(\n            ComplexDomain,\n            needs=onset_prep,\n            store=False)\n\n        sliding_detection = ArrayWithUnitsFeature(\n            SlidingWindow,\n            needs=complex_domain,\n            wscheme=HalfLapped() * Stride(frequency=1, duration=11),\n            padwith=5,\n            store=False)\n\n        slices = TimeSliceFeature(\n            MovingAveragePeakPicker,\n            needs=sliding_detection,\n            aggregate=np.median,\n            store=True)\n\n    return Onsets\n'"
zounds/basic/basic.py,4,"b'from collections import OrderedDict\n\nimport numpy as np\nfrom featureflow import Node, NotEnoughData\n\nfrom zounds.timeseries import VariableRateTimeSeries\nfrom zounds.core import ArrayWithUnits\n\n\nclass Merge(Node):\n    """"""\n    Combine two or more sources into a single feature\n    """"""\n\n    def __init__(self, needs=None):\n        super(Merge, self).__init__(needs=needs)\n        exc_msg = \'you must supply two or more dependencies\'\n        if len(needs) < 2:\n            raise ValueError(exc_msg)\n\n        self._cache = OrderedDict((id(n), None) for n in list(needs.values()))\n\n    def _enqueue(self, data, pusher):\n        key = id(pusher)\n        if self._cache[key] is None or self._cache[key].size == 0:\n            self._cache[key] = data\n        else:\n            self._cache[key] = self._cache[key].concatenate(data)\n\n    def _dequeue(self):\n        if any(v is None or len(v) == 0 for v in self._cache.values()):\n            raise NotEnoughData()\n        shortest = min(len(v) for v in self._cache.values())\n        output = OrderedDict(\n            (k, v[:shortest]) for k, v in self._cache.items())\n        self._cache = OrderedDict(\n            (k, v[shortest:]) for k, v in self._cache.items())\n        return output\n\n    def _process(self, data):\n        yield ArrayWithUnits.concat(list(data.values()), axis=1)\n\n\nclass Pooled(Node):\n    def __init__(self, op=None, axis=None, needs=None):\n        super(Pooled, self).__init__(needs=needs)\n        self._timeslices = VariableRateTimeSeries(())\n        self._timeseries = None\n        self._op = op\n        self._axis = axis\n\n    def _enqueue(self, data, pusher):\n        if isinstance(data, ArrayWithUnits):\n            try:\n                self._timeseries = self._timeseries.concatenate(data)\n            except AttributeError:\n                self._timeseries = data\n        else:\n            self._timeslices = self._timeslices.concat(data)\n\n    def _dequeue(self):\n        if not self._finalized:\n            raise NotEnoughData()\n        return self._timeslices, self._timeseries\n\n    def _process(self, data):\n        slices, series = data\n        slices = slices.slices\n        examples = [(ts, self._op(series[ts], axis=self._axis))\n                    for ts in slices]\n        yield VariableRateTimeSeries(examples)\n\n\nclass Slice(Node):\n    def __init__(self, sl=None, needs=None):\n        super(Slice, self).__init__(needs=needs)\n        self._sl = sl\n\n    def _process(self, data):\n        print(np.array(data), self._sl)\n        yield data[:, self._sl]\n\n\nclass Sum(Node):\n    def __init__(self, axis=0, needs=None):\n        super(Sum, self).__init__(needs=needs)\n        self._axis = axis\n\n    def _process(self, data):\n        # TODO: This should be generalized.  Sum will have this same problem\n        try:\n            # data = np.sum(data, axis=self._axis)\n            data = np.sum(axis=self._axis)\n        except ValueError:\n            print(\'ERROR\')\n            data = data\n        if data.shape[0]:\n            yield data\n\n\nclass Max(Node):\n    def __init__(self, axis=0, needs=None):\n        super(Max, self).__init__(needs=needs)\n        self._axis = axis\n\n    def _process(self, data):\n        # TODO: This should be generalized.  Sum will have this same problem\n        try:\n            # data = np.max(data, axis=self._axis)\n            data = data.max(axis=self._axis)\n        except ValueError:\n            print(\'ERROR\')\n            data = data\n        if data.shape[0]:\n            yield data\n\n\nclass Binarize(Node):\n    def __init__(self, predicate=None, needs=None):\n        super(Binarize, self).__init__(needs=needs)\n        self.predicate = predicate\n\n    def _process(self, data):\n        yield self.predicate(data)\n'"
zounds/basic/fuzz_test.py,4,"b'# from __future__ import division\n#\n# import os\n# from io import BytesIO\n# from random import choice\n# from uuid import uuid4\n#\n# import numpy as np\n# import unittest2\n# from soundfile import *\n#\n# from zounds.soundfile import AudioStream, OggVorbis, Resampler\n# from zounds.timeseries import SR44100\n# from featureflow import \\\n#     BaseModel, ByteStream, ByteStreamFeature, Feature, UuidProvider, \\\n#     InMemoryDatabase, StringDelimitedKeyBuilder, PersistenceSettings\n# from featureflow.nmpy import NumpyFeature\n#\n# _sample_rates = (11025, 22050, 44100, 48000, 88200, 96000)\n# _channels = (1, 2)\n# _formats = (\n#     (\'WAV\', \'PCM_16\'),\n#     (\'WAV\', \'PCM_24\'),\n#     (\'WAV\', \'PCM_32\'),\n#     (\'WAV\', \'FLOAT\'),\n#     (\'WAV\', \'DOUBLE\'),\n#\n#     (\'AIFF\', \'PCM_16\'),\n#     (\'AIFF\', \'PCM_24\'),\n#     (\'AIFF\', \'PCM_32\'),\n#     (\'AIFF\', \'FLOAT\'),\n#     (\'AIFF\', \'DOUBLE\'),\n#\n#     (\'FLAC\', \'PCM_16\'),\n#     (\'FLAC\', \'PCM_24\'),\n#\n#     (\'OGG\', \'VORBIS\')\n# )\n#\n#\n# class FuzzTests(unittest2.TestCase):\n#     def __init__(\n#             self, chunksize_bytes, samplerate, fmt, subtype, channels, seconds):\n#\n#         super(FuzzTests, self).__init__()\n#         self._samplerate = samplerate\n#         self._fmt = fmt\n#         self._subtype = subtype\n#         self._seconds = seconds\n#         self._channels = channels\n#         self._chunksize_bytes = chunksize_bytes\n#\n#     def __repr__(self):\n#         return \'FuzzTests(sr = {_samplerate}, fmt = {_fmt}, st = {_subtype}, secs = {_seconds}, ch = {_channels}, cs = {_chunksize_bytes})\'.format(\n#                 **self.__dict__)\n#\n#     def __str__(self):\n#         return self.__repr__()\n#\n#     def model_cls(self):\n#\n#         class Settings(PersistenceSettings):\n#             id_provider = UuidProvider()\n#             key_builder = StringDelimitedKeyBuilder()\n#             database = InMemoryDatabase(key_builder=key_builder)\n#\n#         class Document(BaseModel, Settings):\n#             raw = ByteStreamFeature(\n#                     ByteStream,\n#                     chunksize=self._chunksize_bytes,\n#                     store=False)\n#\n#             ogg = Feature(\n#                     OggVorbis,\n#                     needs=raw,\n#                     store=True)\n#\n#             pcm = NumpyFeature(\n#                     AudioStream,\n#                     needs=raw,\n#                     store=True)\n#\n#             resampled = NumpyFeature(\n#                     Resampler,\n#                     samplerate=SR44100(),\n#                     needs=pcm,\n#                     store=True)\n#\n#         return Document\n#\n#     def runTest(self):\n#         # TODO: Update this to use BytesIO instead of writing a file to disk\n#         self._fn = \'/tmp/\' + uuid4().hex\n#         print self\n#\n#         n_samples = int(self._samplerate * self._seconds)\n#         samples = np.sin(np.arange(0, n_samples * 440, 440) * (2 * np.pi))\n#         if self._channels == 2:\n#             samples = np.repeat(samples, 2).reshape((n_samples, 2))\n#\n#         try:\n#             with SoundFile(\n#                     self._fn,\n#                     mode=\'w\',\n#                     samplerate=self._samplerate,\n#                     channels=self._channels,\n#                     format=self._fmt,\n#                     subtype=self._subtype) as sf:\n#                 for i in range(0, n_samples, 44100):\n#                     sf.write(samples[i: i + 44100])\n#         except ValueError as e:\n#             self.fail(e)\n#\n#         class HasUri(object):\n#             def __init__(self, uri):\n#                 self.uri = uri\n#\n#         model = self.model_cls()\n#         _id = model.process(raw=HasUri(self._fn))\n#         doc = model(_id)\n#         orig_samples = doc.pcm\n#         self.assertAlmostEqual(\n#                 samples.shape[0], orig_samples.shape[0], delta=1)\n#\n#         del orig_samples\n#         resampled = doc.resampled\n#         seconds = resampled.shape[0] / 44100\n#         self.assertAlmostEqual(self._seconds, seconds, delta=.025)\n#         del resampled\n#         ogg_bytes = doc.ogg\n#\n#         # first, do the ogg conversion ""by hand"" to make sure I\'m not missing\n#         # something\n#         bio = BytesIO()\n#         with SoundFile( \\\n#                 bio,\n#                 format=\'OGG\',\n#                 subtype=\'VORBIS\',\n#                 mode=\'w\',\n#                 samplerate=self._samplerate,\n#                 channels=self._channels) as ogg_sf:\n#             for i in xrange(0, n_samples, 44100):\n#                 ogg_sf.write(samples[i: i + 44100])\n#\n#         bio.seek(0)\n#         ogg_bytes.seek(0)\n#         bio.seek(0)\n#\n#         with SoundFile(ogg_bytes) as ogg_sf:\n#             ogg_samples = ogg_sf.read(samples.shape[0] + 99999)\n#\n#         ogg_seconds = ogg_samples.shape[0] / self._samplerate\n#         self.assertAlmostEqual(self._seconds, ogg_seconds, delta=.025)\n#\n#     def tearDown(self):\n#         os.remove(self._fn)\n#\n#\n# def suite():\n#     suite = unittest2.TestSuite()\n#\n#     for _ in xrange(50):\n#         seconds = (np.random.random_sample() * 50)\n#         min_size = 4 * 96000 * 5 * 2\n#         chunksize = min_size + (np.random.randint(0, 4 * 96000 * 25 * 2))\n#         samplerate = choice(_sample_rates)\n#         fmt = choice(_formats)\n#         channels = choice(_channels)\n#         suite.addTest(\n#             FuzzTests(chunksize, samplerate, fmt[0], fmt[1], channels,\n#                       seconds))\n#\n#     return suite\n#\n#\n# if __name__ == \'__main__\':\n#     unittest2.TextTestRunner().run(suite())\n'"
zounds/basic/test_audiograph.py,2,"b""import unittest2\nfrom .audiograph import resampled, stft, frequency_adaptive\nfrom zounds.timeseries.samplerate import \\\n    SR11025, SR22050, SampleRate, nearest_audio_sample_rate, HalfLapped\nfrom zounds.timeseries.duration import Seconds, Milliseconds\nfrom zounds.util.persistence import simple_in_memory_settings\nfrom zounds.persistence import ArrayWithUnitsFeature\nfrom zounds.synthesize.synthesize import NoiseSynthesizer, SineSynthesizer\nfrom zounds.spectral import GeometricScale, FrequencyAdaptive\nimport zipfile\nfrom io import BytesIO\nimport featureflow\nimport numpy as np\n\n\nclass FrequencyAdaptiveTests(unittest2.TestCase):\n    def test_can_compute_frequency_adaptive_feature(self):\n        scale = GeometricScale(\n            start_center_hz=50,\n            stop_center_hz=5000,\n            bandwidth_ratio=0.07123,\n            n_bands=128)\n\n        fa = frequency_adaptive(\n            SampleRate(frequency=Milliseconds(358), duration=Milliseconds(716)),\n            scale,\n            store_freq_adaptive=True)\n\n        @simple_in_memory_settings\n        class Document(fa):\n            rasterized = ArrayWithUnitsFeature(\n                lambda fa: fa.rasterize(64).astype(np.float32),\n                needs=fa.freq_adaptive,\n                store=True)\n\n        synth = SineSynthesizer(SR22050())\n        samples = synth.synthesize(Seconds(10))\n        _id = Document.process(meta=samples.encode())\n        doc = Document(_id)\n        self.assertIsInstance(doc.freq_adaptive, FrequencyAdaptive)\n        self.assertEqual((64, 128), doc.rasterized.shape[-2:])\n\n\nclass ResampledTests(unittest2.TestCase):\n    def test_audio_is_resampled(self):\n        orig_sample_rate = SR22050()\n        new_sample_rate = SR11025()\n\n        rs = resampled(resample_to=new_sample_rate, store_resampled=True)\n\n        @simple_in_memory_settings\n        class Document(rs):\n            pass\n\n        synth = NoiseSynthesizer(orig_sample_rate)\n        samples = synth.synthesize(Seconds(3))\n        _id = Document.process(meta=samples.encode())\n        doc = Document(_id)\n        self.assertEqual(new_sample_rate, doc.resampled.samplerate)\n        self.assertEqual(len(samples) // 2, len(doc.resampled))\n\n\nclass StftTests(unittest2.TestCase):\n\n    def test_can_pass_padding_samples(self):\n        samplerate = SR11025()\n\n        STFT = stft(\n            resample_to=samplerate,\n            store_fft=True,\n            fft_padding_samples=1024)\n\n        @simple_in_memory_settings\n        class Document(STFT):\n            pass\n\n        samples = SineSynthesizer(samplerate).synthesize(Seconds(2))\n        _id = Document.process(meta=samples.encode())\n        doc = Document(_id)\n        stft_window_duration = int(np.round(\n            HalfLapped().duration / samplerate.frequency))\n        expected_samples = ((stft_window_duration + 1024) // 2) + 1\n        self.assertEqual(expected_samples, doc.fft.shape[-1])\n\n    def test_can_handle_zip_file(self):\n        STFT = stft(resample_to=SR11025(), store_fft=True)\n\n        @simple_in_memory_settings\n        class Document(STFT):\n            pass\n\n        signal = SineSynthesizer(SR11025()).synthesize(Seconds(2))\n\n        bio = BytesIO()\n        filename = 'test.wav'\n        with zipfile.ZipFile(bio, mode='w') as zf:\n            zf.writestr(filename, signal.encode().read())\n        bio.seek(0)\n\n        with list(featureflow.iter_zip(bio))[0] as zip_wrapper:\n            _id = Document.process(meta=zip_wrapper)\n            doc = Document(_id)\n            self.assertEqual(2, doc.ogg.duration_seconds)\n"""
zounds/basic/test_integration.py,2,"b""\n\nfrom io import BytesIO\n\nimport numpy as np\nimport unittest2\nfrom soundfile import SoundFile\n\nfrom zounds.timeseries import TimeSlice, AudioSamples, SR44100, HalfLapped, \\\n    Seconds, Milliseconds, Stride\nfrom zounds.persistence import ArrayWithUnitsFeature, AudioSamplesFeature\nfrom zounds.soundfile import \\\n    AudioStream, OggVorbis, OggVorbisFeature, Resampler\nfrom zounds.spectral import \\\n    SlidingWindow, OggVorbisWindowingFunc, FFT, Chroma, BarkBands, BFCC, \\\n    FrequencyBand\nfrom zounds.basic import Max\nfrom zounds.util import simple_in_memory_settings\nfrom featureflow import *\n\nwindowing_scheme = HalfLapped()\nsamplerate = SR44100()\nband = FrequencyBand(20, samplerate.nyquist)\n\n\n@simple_in_memory_settings\nclass Document(BaseModel):\n    raw = ByteStreamFeature(\n        ByteStream,\n        chunksize=2 * 44100 * 30 * 2,\n        store=True)\n\n    ogg = OggVorbisFeature(\n        OggVorbis,\n        needs=raw,\n        store=True)\n\n    pcm = AudioSamplesFeature(\n        AudioStream,\n        needs=raw,\n        store=True)\n\n    resampled = AudioSamplesFeature(\n        Resampler,\n        needs=pcm,\n        samplerate=samplerate,\n        store=True)\n\n    windowed = ArrayWithUnitsFeature(\n        SlidingWindow,\n        needs=resampled,\n        wscheme=windowing_scheme,\n        wfunc=OggVorbisWindowingFunc(),\n        store=False)\n\n    fft = ArrayWithUnitsFeature(\n        FFT,\n        needs=windowed,\n        store=True)\n\n    chroma = ArrayWithUnitsFeature(\n        Chroma,\n        needs=fft,\n        frequency_band=band,\n        store=True)\n\n    bark = ArrayWithUnitsFeature(\n        BarkBands,\n        needs=fft,\n        frequency_band=band,\n        store=True)\n\n    bfcc = ArrayWithUnitsFeature(\n        BFCC,\n        needs=bark,\n        store=True)\n\n    bfcc_sliding_window = ArrayWithUnitsFeature(\n        SlidingWindow,\n        needs=bfcc,\n        wscheme=windowing_scheme * Stride(frequency=2, duration=4),\n        store=True)\n\n    bfcc_pooled = ArrayWithUnitsFeature(\n        Max,\n        needs=bfcc_sliding_window,\n        axis=1,\n        store=True)\n\n\nclass HasUri(object):\n    def __init__(self, uri):\n        super(HasUri, self).__init__()\n        self.uri = uri\n\n\ndef signal(hz=440, seconds=5., sr=44100.):\n    # cycles per sample\n    cps = hz / sr\n    # total samples\n    ts = seconds * sr\n    mono = np.sin(np.arange(0, ts * cps, cps) * (2 * np.pi))\n    return np.column_stack((mono, mono))\n\n\ndef soundfile(hz=440, seconds=5., sr=44100.):\n    bio = BytesIO()\n    s = signal(hz, seconds, sr)\n    with SoundFile(\n            bio,\n            mode='w',\n            channels=2,\n            format='WAV',\n            subtype='PCM_16',\n            samplerate=int(sr)) as f:\n        f.write(s)\n    bio.seek(0)\n    return s, HasUri(bio)\n\n\nclass IntegrationTests(unittest2.TestCase):\n    def setUp(self):\n        signal, bio = soundfile(seconds=10.)\n        _id = Document.process(raw=bio)\n        self.doc = Document(_id)\n        self.signal = signal\n\n    def test_pcm_returns_audio_samples(self):\n        self.assertIsInstance(self.doc.pcm, AudioSamples)\n\n    def test_pcm_is_summed_to_mono(self):\n        self.assertEqual(1, self.doc.pcm.channels)\n\n    def test_resampled_returns_audio_samples(self):\n        self.assertIsInstance(self.doc.resampled, AudioSamples)\n\n    def test_ogg_vorbis_iter_chunks_returns_audio_samples(self):\n        chunks = list(self.doc.ogg.iter_chunks())\n        self.assertTrue(\n            all(isinstance(chunk, AudioSamples) for chunk in chunks))\n\n    def test_ogg_vorbis_wrapper_returns_audio_samples(self):\n        self.assertIsInstance(self.doc.ogg[:], AudioSamples)\n\n    def test_ogg_wrapper_has_correct_duration_seconds(self):\n        self.assertEqual(10, self.doc.ogg.duration_seconds)\n\n    def test_windowed_and_fft_have_same_first_dimension(self):\n        self.assertEqual(self.doc.windowed.shape[0], self.doc.fft.shape[0])\n\n    def test_fft_dimension_is_half_of_windowsize(self):\n        self.assertEqual(\n            (self.doc.windowed.shape[1] // 2) + 1,\n            self.doc.fft.shape[1])\n\n    def test_bfcc_sliding_window_has_correct_shape(self):\n        self.assertEqual((5, 13), self.doc.bfcc_sliding_window.shape[1:])\n\n    def test_bfcc_sliding_window_has_correct_frequency(self):\n        bfcc_td = self.doc.bfcc.dimensions[0]\n        bfcc_sw_td = self.doc.bfcc_sliding_window.dimensions[0]\n        self.assertEqual(2, bfcc_sw_td.frequency / bfcc_td.frequency)\n\n    def test_bfcc_sliding_window_has_correct_duration(self):\n        bfcc_td = self.doc.bfcc.dimensions[0]\n        bfcc_sw_td = self.doc.bfcc_sliding_window.dimensions[0]\n        self.assertEqual(6, bfcc_sw_td.duration / bfcc_td.frequency)\n\n    def test_bfcc_pooled_has_correct_shape(self):\n        self.assertEqual(2, len(self.doc.bfcc_pooled.shape))\n        self.assertEqual((13,), self.doc.bfcc_pooled.shape[1:])\n\n    def test_bfcc_pooled_has_correct_frequency(self):\n        bfcc_td = self.doc.bfcc.dimensions[0]\n        bfcc_pooled_td = self.doc.bfcc_pooled.dimensions[0]\n        self.assertEqual(2, bfcc_pooled_td.frequency / bfcc_td.frequency)\n\n    def test_bfcc_pooled_has_correct_duration(self):\n        bfcc_td = self.doc.bfcc.dimensions[0]\n        bfcc_pooled_td = self.doc.bfcc_pooled.dimensions[0]\n        self.assertEqual(6, bfcc_pooled_td.duration / bfcc_td.frequency)\n\n    def test_can_get_second_long_slice_from_ogg_vorbis_feature(self):\n        ogg = self.doc.ogg\n        samples = ogg[TimeSlice(Seconds(1), start=Seconds(5))]\n        self.assertEqual(44100, len(samples))\n\n    def test_can_get_entirety_of_ogg_vorbis_feature_with_slice(self):\n        ogg = self.doc.ogg\n        samples = ogg[:]\n        self.assertEqual(441000, len(samples))\n\n    def test_can_read_ogg_samples_twice(self):\n        ogg = self.doc.ogg\n        s1 = ogg[:]\n        s2 = ogg[:]\n        self.assertEqual(s1.shape, s2.shape)\n\n    def test_can_get_end_of_ogg_vorbis_feature_with_slice(self):\n        ogg = self.doc.ogg\n        samples = ogg[TimeSlice(Seconds(1), Milliseconds(9500))]\n        self.assertEqual(22050, len(samples))\n\n    def test_sliding_window_has_correct_relationship_to_bfcc(self):\n        self.assertEqual(\n            2,\n            self.doc.bfcc.shape[0] // self.doc.bfcc_sliding_window.shape[0])\n"""
zounds/basic/test_merge.py,1,"b""import unittest2\nimport numpy as np\nfrom featureflow import BaseModel, Node, PersistenceSettings\nfrom .basic import Merge\nfrom zounds.timeseries import  Milliseconds, TimeDimension\nfrom zounds.persistence import ArrayWithUnitsFeature\nfrom zounds.core import ArrayWithUnits, IdentityDimension\n\n\nclass MergeTester(Node):\n    def __init__(\n            self,\n            total_frames=100,\n            increments_of=30,\n            features=10,\n            needs=None):\n        super(MergeTester, self).__init__(needs=needs)\n        self.features = features\n        self.increments_of = increments_of\n        self.total_frames = total_frames\n\n    def _process(self, data):\n        for i in range(0, self.total_frames, self.increments_of):\n            size = min(self.increments_of, self.total_frames - i)\n            td = TimeDimension(frequency=Milliseconds(500))\n            yield ArrayWithUnits(\n                    np.zeros((size, self.features)), [td, IdentityDimension()])\n\n\nclass MergeTests(unittest2.TestCase):\n    def test_raises_if_single_source(self):\n        class Document(BaseModel, PersistenceSettings):\n            source = ArrayWithUnitsFeature(\n                    MergeTester,\n                    store=True)\n            merged = ArrayWithUnitsFeature(\n                    Merge,\n                    needs=source,\n                    store=True)\n\n        self.assertRaises(ValueError, lambda: Document.process(source=''))\n\n    def test_raises_if_single_element_iterable(self):\n        class Document(BaseModel, PersistenceSettings):\n            source = ArrayWithUnitsFeature(\n                    MergeTester,\n                    store=True)\n            merged = ArrayWithUnitsFeature(\n                    Merge,\n                    needs=[source],\n                    store=True)\n\n        self.assertRaises(ValueError, lambda: Document.process(source=''))\n\n    def test_can_combine_two_sources_at_same_rate(self):\n        class Document(BaseModel, PersistenceSettings):\n            source1 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=200,\n                    store=True)\n            source2 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=200,\n                    store=True)\n            merged = ArrayWithUnitsFeature(\n                    Merge,\n                    needs=[source1, source2],\n                    store=True)\n\n        _id = Document.process(source1='', source2='')\n        doc = Document(_id)\n        self.assertEqual((200, 20), doc.merged.shape)\n\n    def test_can_combine_three_sources_at_same_rate(self):\n        class Document(BaseModel, PersistenceSettings):\n            source1 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=200,\n                    store=True)\n            source2 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=200,\n                    store=True)\n            source3 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=200,\n                    store=True)\n            merged = ArrayWithUnitsFeature(\n                    Merge,\n                    needs=(source1, source2, source3),\n                    store=True)\n\n        _id = Document.process(source1='', source2='', source3='')\n        doc = Document(_id)\n        self.assertEqual((200, 30), doc.merged.shape)\n\n    def test_can_combine_two_sources_at_different_rates(self):\n        class Document(BaseModel, PersistenceSettings):\n            source1 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=200,\n                    increments_of=30,\n                    store=True)\n            source2 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=200,\n                    increments_of=40,\n                    store=True)\n            merged = ArrayWithUnitsFeature(\n                    Merge,\n                    needs=(source1, source2),\n                    store=True)\n\n        _id = Document.process(source1='', source2='')\n        doc = Document(_id)\n        self.assertEqual((200, 20), doc.merged.shape)\n\n    def test_can_combine_three_sources_at_different_rates(self):\n        class Document(BaseModel, PersistenceSettings):\n            source1 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=200,\n                    increments_of=12,\n                    store=True)\n            source2 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=200,\n                    increments_of=17,\n                    store=True)\n            source3 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=200,\n                    increments_of=32,\n                    store=True)\n            merged = ArrayWithUnitsFeature(\n                    Merge,\n                    needs=(source1, source2, source3),\n                    store=True)\n\n        _id = Document.process(source1='', source2='', source3='')\n        doc = Document(_id)\n        self.assertEqual((200, 30), doc.merged.shape)\n\n    def test_shortest_of_two_sources(self):\n        class Document(BaseModel, PersistenceSettings):\n            source1 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=190,\n                    increments_of=30,\n                    store=True)\n            source2 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=200,\n                    increments_of=40,\n                    store=True)\n            merged = ArrayWithUnitsFeature(\n                    Merge,\n                    needs=[source1, source2],\n                    store=True)\n\n        _id = Document.process(source1='', source2='')\n        doc = Document(_id)\n        self.assertEqual((190, 20), doc.merged.shape)\n\n    def test_shortest_of_three_sources(self):\n        class Document(BaseModel, PersistenceSettings):\n            source1 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=200,\n                    increments_of=12,\n                    store=True)\n            source2 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=185,\n                    increments_of=17,\n                    store=True)\n            source3 = ArrayWithUnitsFeature(\n                    MergeTester,\n                    total_frames=50,\n                    increments_of=32,\n                    store=True)\n            merged = ArrayWithUnitsFeature(\n                    Merge,\n                    needs=[source1, source2, source3],\n                    store=True)\n\n        _id = Document.process(source1='', source2='', source3='')\n        doc = Document(_id)\n        self.assertEqual((50, 30), doc.merged.shape)\n"""
zounds/basic/util.py,0,"b'import os\n\n\ndef iter_files(base_path):\n    for fn in os.listdir(base_path):\n        yield os.path.join(base_path, fn)\n\n\ndef process_dir(base_path, process_func):\n    for fp in iter_files(base_path):\n        process_func(fp)'"
zounds/core/__init__.py,0,"b'""""""\nThe core module introduces the key building blocks of the representations zounds\ndeals in: :class:`ArrayWithUnits`, a :class:`numpy.ndarray`-derived class that\nsupports semantically meaningful indexing, and :class:`Dimension`, a common\nbase class for custom, user-defined dimensions.\n""""""\n\nfrom .dimensions import Dimension, IdentityDimension\nfrom .axis import ArrayWithUnits\n\n__all__ = [Dimension, IdentityDimension, ArrayWithUnits]\n'"
zounds/core/axis.py,18,"b'import numpy as np\nfrom zounds.nputil import sliding_window, windowed\nfrom zounds.util import tuplify\nfrom .dimensions import IdentityDimension\n\n\nclass CustomSlice(object):\n    def __init__(self):\n        super(CustomSlice, self).__init__()\n\n\nclass ArrayWithUnits(np.ndarray):\n    """"""\n    `ArrayWithUnits` is an :class:`numpy.ndarray` subclass that allows for\n    indexing by more semantically meaningful slices.\n\n    It supports most methods on :class:`numpy.ndarray`, and makes a best-effort\n    to maintain meaningful dimensions throughout those operations.\n\n    Args:\n        arr (ndarray): The :class:`numpy.ndarray` instance containing the raw\n            data for this instance\n        dimensions (list or tuple): list or tuple of :class:`Dimension`-derived\n            classes\n\n    Raises:\n        ValueError: when `arr.ndim` and `len(dimensions)` do not match\n\n    Examples:\n        >>> from zounds import ArrayWithUnits, TimeDimension, Seconds, TimeSlice\n        >>> import numpy as np\n        >>> data = np.zeros(100)\n        >>> awu = ArrayWithUnits(data, [TimeDimension(Seconds(1))])\n        >>> sliced = awu[TimeSlice(Seconds(10))]\n        >>> sliced.shape\n        (10,)\n\n    See Also:\n        :class:`IdentityDimension`\n        :class:`~zounds.timeseries.TimeDimension`\n        :class:`~zounds.spectral.FrequencyDimension`\n    """"""\n\n    def __new__(cls, arr, dimensions):\n        if arr.ndim != len(dimensions):\n            raise ValueError(\n                \'arr.ndim and len(dimensions) must match.  \'\n                \'They were {arr.shape} and {dimensions}\'.format(**locals()))\n\n        obj = np.asarray(arr).view(cls)\n        obj.dimensions = tuple([IdentityDimension() if d is None else d.copy() for d in dimensions])\n\n        for dim, size in zip(obj.dimensions, obj.shape):\n            try:\n                dim.size = size\n            except AttributeError:\n                pass\n            try:\n                dim.validate(size)\n            except AttributeError:\n                pass\n\n        return obj\n\n    @property\n    def T(self):\n        arr = super(ArrayWithUnits, self).T\n        return ArrayWithUnits(arr, self.dimensions[::-1])\n\n    def kwargs(self):\n        return self.__dict__\n\n    def concatenate(self, other):\n        if self.dimensions == other.dimensions:\n            return self.from_example(np.concatenate([self, other]), self)\n        else:\n            raise ValueError(\'All dimensions must match to concatenate\')\n\n    def reshape(self, shape, order=\'C\'):\n        non_one = lambda x: abs(x) != 1\n\n        if tuple(shape) == tuple(filter(non_one, self.shape)):\n            # the new shape is this array\'s shape will all ones removed\n            return self.squeeze()\n        elif tuple(self.shape) == tuple(filter(non_one, shape)):\n            # the new shape just adds some single dimension axes\n            dims = iter(self.dimensions)\n            new_dims = [\n                IdentityDimension() if abs(size) == 1 else next(dims)\n                for size in shape\n            ]\n            raw = np.asarray(self)\n            return ArrayWithUnits(raw.reshape(shape), new_dims)\n\n        # treat every axis as an IdentityDimension\n        raw = np.asarray(self)\n        return ArrayWithUnits(\n            np.reshape(raw, shape, order),\n            [IdentityDimension() for _ in shape])\n\n    def squeeze(self):\n        zipped = [x for x in zip(self.shape, self.dimensions) if x[0] > 1]\n        return ArrayWithUnits(\n            super().reshape([s for s, _ in zipped]),\n            [d for _, d in zipped])\n\n    @classmethod\n    def concat(cls, arrs, axis=0):\n        for arr in arrs[1:]:\n            if arr.dimensions != arrs[0].dimensions:\n                raise ValueError(\'All dimensions must match\')\n        return cls.from_example(np.concatenate(arrs, axis=axis), arrs[0])\n\n    @classmethod\n    def from_example(cls, data, example):\n        """"""\n        Produce a new :class:`ArrayWithUnits` instance given some raw data and\n        an example instance that has the desired dimensions\n        """"""\n        return ArrayWithUnits(data, example.dimensions)\n\n    @classmethod\n    def zeros(cls, example):\n        return cls.from_example(\n            np.zeros(example.shape, dtype=example.dtype), example)\n\n    def zeros_like(self):\n        return self.__class__.zeros(self)\n\n    def _apply_reduction_to_dimensions(self, result, axis, keepdims):\n        if axis is None:\n            return result\n\n        ndims = len(self.dimensions)\n        reduced_axes = set([ndims + a if a < 0 else a for a in tuplify(axis)])\n        all_axes = set(range(ndims))\n\n        if keepdims:\n            new_dims = [\n                IdentityDimension() if i in reduced_axes else dim\n                for i, dim in enumerate(self.dimensions)]\n        else:\n            remaining_axes = sorted(all_axes - reduced_axes)\n            new_dims = [self.dimensions[i] for i in remaining_axes]\n\n        return ArrayWithUnits(result, new_dims)\n\n    def sum(self, axis=None, dtype=None, keepdims=False, **kwargs):\n        result = super(ArrayWithUnits, self).sum(\n            axis, dtype, keepdims=keepdims, **kwargs)\n        return self._apply_reduction_to_dimensions(result, axis, keepdims)\n\n    def max(self, axis=None, out=None, keepdims=False):\n        result = super(ArrayWithUnits, self).max(\n            axis=axis, out=out, keepdims=keepdims)\n        return self._apply_reduction_to_dimensions(result, axis, keepdims)\n\n    def dot(self, b):\n        result = super(ArrayWithUnits, self).dot(b)\n        return self.__class__(result, self.dimensions[:result.ndim])\n\n    def packbits(self, axis=None):\n        arr = np.packbits(self, axis=axis)\n        return ArrayWithUnits(arr, self.dimensions)\n\n    def __array_finalize__(self, obj):\n        if obj is None:\n            return\n        self.dimensions = getattr(obj, \'dimensions\', None)\n\n    def __array_wrap__(self, obj, context=None):\n        if len(self.dimensions) != obj.ndim:\n            if obj.ndim == 0:\n                return obj.item()\n            return np.asarray(obj)\n        return np.ndarray.__array_wrap__(self, obj, context)\n\n    def sliding_window(self, windowsize, stepsize=None):\n        ws = tuple(self._compute_span(windowsize))\n        ss = tuple(self._compute_span(stepsize)) if stepsize else ws\n        result = sliding_window(self, ws, ss)\n\n        try:\n            new_dims = tuple(self._compute_new_dims(result, ws, ss))\n        except ValueError:\n            new_dims = [IdentityDimension()] * result.ndim\n\n        return ArrayWithUnits(result, new_dims)\n\n    def _sliding_window_integer_slices(self, windowsize, stepsize=None):\n        windowsize = [windowsize] + [slice(None) for _ in self.dimensions[1:]]\n        ws = tuple(self._compute_span(windowsize))\n        if stepsize:\n            stepsize = [stepsize] + [slice(None) for _ in self.dimensions[1:]]\n            ss = tuple(self._compute_span(stepsize))\n        else:\n            ss = ws\n\n        return ws, ss\n\n    def sliding_window_with_leftovers(\n            self, windowsize, stepsize=None, dopad=False):\n\n        ws, ss = \\\n            self._sliding_window_integer_slices(windowsize, stepsize)\n\n        leftovers, result = windowed(self, ws[0], ss[0], dopad)\n\n        if not result.size:\n            return self, ArrayWithUnits(result, self.dimensions)\n\n        try:\n            new_dims = tuple(self._compute_new_dims(result, ws, ss))\n        except ValueError:\n            new_dims = [IdentityDimension()] * result.ndim\n\n        return leftovers, ArrayWithUnits(result, new_dims)\n\n    def _compute_new_dims(self, windowed, ws, ss):\n        for dimension, size, w, s in zip(self.dimensions, self.shape, ws, ss):\n            try:\n                modified = dimension.modified_dimension(size, w, s)\n                for m in modified:\n                    yield m\n            except NotImplementedError:\n                yield dimension\n\n    def _compute_span(self, index):\n        for sl in self._compute_indices(index):\n            try:\n                yield sl.stop - sl.start\n            except (AttributeError, TypeError):\n                yield sl\n\n    def _is_integer_based_slice(self, sl):\n        if not isinstance(sl, slice):\n            return False\n\n        try:\n            return \\\n                (sl.start is None or sl.start.bit_length()) \\\n                and (sl.stop is None or sl.stop.bit_length())\n        except AttributeError:\n            return False\n\n    def _compute_indices(self, index):\n        dims_pos = 0\n        for sl in index:\n            if sl is None \\\n                    or isinstance(sl, int) \\\n                    or self._is_integer_based_slice(sl) \\\n                    or isinstance(sl, list):\n                # burn one\n                dims_pos += 1\n                yield sl\n            elif sl is Ellipsis:\n                dims_pos += len(self.dimensions) - (len(index) - 1)\n                yield Ellipsis\n            else:\n                dim = self.dimensions[dims_pos]\n                yield dim.integer_based_slice(sl)\n                dims_pos += 1\n\n    def _new_dims(self, index, new_arr):\n        dims_pos = 0\n        shape_pos = 0\n        not_ellipsis_or_none = len([x for x in index if x is not Ellipsis and x is not None])\n        for sl in index:\n            if sl is None:\n                # additional dimension via np.newaxis\n                yield IdentityDimension()\n            elif isinstance(sl, int):\n                # burn a dimension\n                dims_pos += 1\n            elif isinstance(sl, list):\n                dims_pos += 1\n                shape_pos += 1\n                yield IdentityDimension()\n            elif sl is Ellipsis:\n                ellipsis_size = len(self.dimensions) - not_ellipsis_or_none\n                for i in range(ellipsis_size):\n                    yield self.dimensions[dims_pos]\n                    dims_pos += 1\n                    shape_pos += 1\n            else:\n                try:\n                    dim = self.dimensions[dims_pos]\n                    shape = new_arr.shape[shape_pos]\n                    yield dim.metaslice(sl, shape)\n                except IndexError:\n                    yield dim\n                dims_pos += 1\n                shape_pos += 1\n\n        # Return any leftover dimensions\n        for dim in self.dimensions[dims_pos:]:\n            yield dim\n\n    def _tuplify(self, a):\n        if isinstance(a, list):\n            t = set([x.__class__ for x in a])\n            if len(t) > 1:\n                raise ValueError(\'a must be homogeneous\')\n            t = list(t)[0]\n            if t == slice:\n                return a\n            else:\n                return a,\n        if isinstance(a, np.ndarray) and a.dtype == np.bool:\n            return a,\n        try:\n            return tuple(a)\n        except TypeError:\n            return a,\n\n    def __getslice__(self, i, j):\n        return self.__getitem__(slice(i, j))\n\n    def __setitem__(self, index, value):\n        index = self._tuplify(index)\n        indices = tuple(self._compute_indices(index))\n        super(ArrayWithUnits, self).__setitem__(indices, value)\n\n    def __getitem__(self, index):\n        if isinstance(index, np.ndarray) \\\n                and index.dtype == np.bool:\n            return np.asarray(self)[index]\n\n        if self.ndim == 1 and isinstance(index, int):\n            return np.asarray(self)[index]\n\n        index = self._tuplify(index)\n        indices = tuple(self._compute_indices(index))\n        arr = super(ArrayWithUnits, self).__getitem__(indices)\n        new_dims = tuple(self._new_dims(index, arr))\n        return ArrayWithUnits(arr, new_dims)\n'"
zounds/core/dimensions.py,1,"b'class Dimension(object):\n    """"""\n    Common base class representing one dimension of a numpy array.  Sub-classes\n    can define behavior making custom slices (e.g., time spans or\n    frequency bands) possible.\n\n    Implementors are primarily responsible for determining how custom slices\n    are transformed into integer indexes and slices that numpy can use directly.\n\n    See Also:\n        :class:`IdentityDimension`\n        :class:`~zounds.timeseries.TimeDimension`\n        :class:`~zounds.spectral.FrequencyDimension`\n    """"""\n\n    def __init__(self):\n        super(Dimension, self).__init__()\n\n    def modified_dimension(self, size, windowsize, stepsize=None):\n        raise NotImplementedError()\n\n    def metaslice(self, index, size):\n        """"""\n        Produce a new instance of this dimension, given a custom slice\n        """"""\n        return self\n\n    def integer_based_slice(self, index):\n        """"""\n        Subclasses define behavior that transforms a custom, user-defined slice\n        into integer indices that numpy can understand\n\n        Args:\n            index (custom slice): A user-defined slice instance\n        """"""\n        raise NotImplementedError()\n\n    def validate(self, size):\n        """"""\n        Subclasses check to ensure that the dimensions size does not validate\n        any assumptions made by this instance\n        """"""\n        pass\n\n    def copy(self):\n        return Dimension()\n\n\nclass IdentityDimension(Dimension):\n    """"""\n    A custom dimension that does not transform indices in any way, simply acting\n    as a pass-through.\n\n    Examples:\n        >>> from zounds import ArrayWithUnits, IdentityDimension\n        >>> import numpy as np\n        >>> data = np.zeros(100)\n        >>> arr = ArrayWithUnits(data, [IdentityDimension()])\n        >>> sliced = arr[4:6]\n        >>> sliced.shape\n        (2,)\n    """"""\n\n    def __init__(self):\n        super(IdentityDimension, self).__init__()\n\n    def modified_dimension(self, size, windowsize, stepsize=None):\n        if windowsize == slice(None) or (size / windowsize) == 1:\n            yield IdentityDimension()\n        else:\n            raise ValueError()\n\n    def integer_based_slice(self, index):\n        return index\n\n    def __eq__(self, other):\n        return self.__class__ == other.__class__\n\n    def copy(self):\n        return IdentityDimension()\n'"
zounds/core/test_core.py,79,"b""import unittest2\nimport numpy as np\nfrom .dimensions import Dimension, IdentityDimension\nfrom .axis import ArrayWithUnits, CustomSlice\nfrom string import ascii_lowercase\n\n\nclass ContrivedSlice(CustomSlice):\n    def __init__(self, start=None, stop=None, step=None):\n        super(ContrivedSlice, self).__init__()\n        self.step = step\n        self.stop = stop\n        self.start = start\n\n\nclass ContrivedDimension(Dimension):\n    def __init__(self, factor):\n        super(ContrivedDimension, self).__init__()\n        self.factor = factor\n        self.size = None\n\n    def copy(self):\n        return ContrivedDimension(self.factor)\n\n    def modified_dimension(self, size, windowsize, stepsize=None):\n        yield ContrivedDimension(self.factor * windowsize)\n        yield ContrivedDimension(self.factor)\n\n    def integer_based_slice(self, index):\n        if isinstance(index, ContrivedSlice):\n            return slice(index.start // self.factor, index.stop // self.factor)\n        else:\n            return index\n\n    def __eq__(self, other):\n        try:\n            return self.factor == other.factor\n        except AttributeError:\n            return False\n\n    def __repr__(self):\n        return 'ContrivedDimension(factor={factor})'.format(**self.__dict__)\n\n\nclass ContrivedDimension2(Dimension):\n    def __init__(self, factor):\n        super(ContrivedDimension2, self).__init__()\n        self.factor = factor\n        self.size = None\n\n    def copy(self):\n        return ContrivedDimension2(self.factor)\n\n    def modified_dimension(self, size, windowsize, stepsize=None):\n        yield ContrivedDimension(self.factor * windowsize)\n        yield ContrivedDimension(self.factor)\n\n    def integer_based_slice(self, index):\n        if isinstance(index, ContrivedSlice):\n            return slice(index.start // self.factor, index.stop // self.factor)\n        else:\n            return index\n\n    def __eq__(self, other):\n        try:\n            return self.factor == other.factor\n        except AttributeError:\n            return False\n\n    def __repr__(self):\n        return 'ContrivedDimension2(factor={factor})'.format(**self.__dict__)\n\n\nclass AsciiCharacterDimension(Dimension):\n    def __init__(self, labels):\n        super(AsciiCharacterDimension, self).__init__()\n        self.labels = labels\n\n    def copy(self):\n        return AsciiCharacterDimension(self.labels)\n\n    def modified_dimension(self, size, windowsize, stepsize=None):\n        raise NotImplementedError()\n\n    def metaslice(self, index, size):\n        return AsciiCharacterDimension(self.labels[index])\n\n    def integer_based_slice(self, index):\n        return index\n\n\nclass ContrivedArray(ArrayWithUnits):\n    def __new__(cls, arr, dimensions):\n        return ArrayWithUnits.__new__(cls, arr, dimensions)\n\n\nclass CoreTests(unittest2.TestCase):\n    def test_maintain_array_with_units_with_boolean_condition(self):\n        arr = ArrayWithUnits(\n            np.random.random_sample((10, 100)) - 0.5,\n            [ContrivedDimension(10), ContrivedDimension2(10)])\n        binary = arr >= 0\n        self.assertIsInstance(binary, ArrayWithUnits)\n        self.assertIsInstance(arr.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(arr.dimensions[1], ContrivedDimension2)\n\n    def test_can_reshape_and_downgrade_to_identity_dimension(self):\n        arr = ArrayWithUnits(\n            np.zeros((100, 10)),\n            [ContrivedDimension(10), ContrivedDimension2(10)])\n        flattened = arr.reshape((-1,))\n        self.assertEqual((1000,), flattened.shape)\n        self.assertIsInstance(flattened, ArrayWithUnits)\n        self.assertIsInstance(flattened.dimensions[0], IdentityDimension)\n\n    def test_can_maintain_array_with_units_when_reshaping_2d(self):\n        arr = ArrayWithUnits(\n            np.zeros((1, 10)),\n            [ContrivedDimension(10), ContrivedDimension2(10)])\n        squeezed = arr.reshape((10,))\n        self.assertEqual((10,), squeezed.shape)\n        self.assertIsInstance(squeezed, ArrayWithUnits)\n        self.assertIsInstance(squeezed.dimensions[0], ContrivedDimension2)\n\n    def test_can_maintain_array_with_units_when_squeezing_2d(self):\n        arr = ArrayWithUnits(\n            np.zeros((1, 10)),\n            [ContrivedDimension(10), ContrivedDimension2(10)])\n        squeezed = arr.squeeze()\n        self.assertEqual((10,), squeezed.shape)\n        self.assertIsInstance(squeezed, ArrayWithUnits)\n        self.assertIsInstance(squeezed.dimensions[0], ContrivedDimension2)\n\n    def test_can_maintain_array_with_units_when_reshaping_3d(self):\n        arr = ArrayWithUnits(\n            np.zeros((3, 1, 10)),\n            [\n                ContrivedDimension(10),\n                IdentityDimension(),\n                ContrivedDimension2(10)\n            ])\n        squeezed = arr.reshape((3, 10))\n        self.assertEqual((3, 10), squeezed.shape)\n        self.assertIsInstance(squeezed, ArrayWithUnits)\n        self.assertIsInstance(squeezed.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(squeezed.dimensions[1], ContrivedDimension2)\n\n    def test_can_maintain_array_with_units_when_squeezing_3d(self):\n        arr = ArrayWithUnits(\n            np.zeros((3, 1, 10)),\n            [\n                ContrivedDimension(10),\n                IdentityDimension(),\n                ContrivedDimension2(10)\n            ])\n        squeezed = arr.squeeze()\n        self.assertEqual((3, 10), squeezed.shape)\n        self.assertIsInstance(squeezed, ArrayWithUnits)\n        self.assertIsInstance(squeezed.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(squeezed.dimensions[1], ContrivedDimension2)\n\n    def test_squeeze_with_no_single_dimension_is_unchanged(self):\n        arr = ArrayWithUnits(\n            np.zeros((3, 2, 10)),\n            [\n                ContrivedDimension(10),\n                IdentityDimension(),\n                ContrivedDimension2(10)\n            ])\n        squeezed = arr.squeeze()\n        self.assertEqual((3, 2, 10), squeezed.shape)\n        self.assertIsInstance(squeezed, ArrayWithUnits)\n        self.assertIsInstance(squeezed.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(squeezed.dimensions[1], IdentityDimension)\n        self.assertIsInstance(squeezed.dimensions[2], ContrivedDimension2)\n\n    def test_can_add_dimension_of_size_one_to_2d_array(self):\n        arr = ArrayWithUnits(\n            np.zeros((3, 10)),\n            [\n                ContrivedDimension(10),\n                ContrivedDimension2(10)\n            ])\n        rs = arr.reshape((3, 1, 10))\n        self.assertIsInstance(rs, ArrayWithUnits)\n        self.assertIsInstance(rs.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(rs.dimensions[1], IdentityDimension)\n        self.assertIsInstance(rs.dimensions[2], ContrivedDimension2)\n\n    def test_can_add_dimension_of_size_one_to_1d_array(self):\n        arr = ArrayWithUnits(np.zeros((3,)), [ContrivedDimension(10)])\n        rs = arr.reshape((1, 1, arr.size))\n        self.assertIsInstance(rs, ArrayWithUnits)\n        self.assertIsInstance(rs.dimensions[0], IdentityDimension)\n        self.assertIsInstance(rs.dimensions[1], IdentityDimension)\n        self.assertIsInstance(rs.dimensions[2], ContrivedDimension)\n\n    def test_can_add_dimension_of_size_one_with_wildcard_axis(self):\n        arr = ArrayWithUnits(np.zeros((3,)), [ContrivedDimension(10)])\n        rs = arr.reshape((-1, 1, arr.size))\n        self.assertIsInstance(rs, ArrayWithUnits)\n        print(rs.dimensions)\n        self.assertIsInstance(rs.dimensions[0], IdentityDimension)\n        self.assertIsInstance(rs.dimensions[1], IdentityDimension)\n        self.assertIsInstance(rs.dimensions[2], ContrivedDimension)\n\n    def test_identity_dimension_when_existing_dim_split_across_axes(self):\n        arr = ArrayWithUnits(np.zeros((12,)), [ContrivedDimension(10)])\n        rs = arr.reshape((2, 2, 3))\n        self.assertIsInstance(rs, ArrayWithUnits)\n        self.assertIsInstance(rs.dimensions[0], IdentityDimension)\n        self.assertIsInstance(rs.dimensions[1], IdentityDimension)\n        self.assertIsInstance(rs.dimensions[2], IdentityDimension)\n\n    def test_identity_dimension_when_existing_dim_split_with_wildcard(self):\n        arr = ArrayWithUnits(np.zeros((12,)), [ContrivedDimension(10)])\n        rs = arr.reshape((2, -1, 3))\n        self.assertIsInstance(rs, ArrayWithUnits)\n        self.assertIsInstance(rs.dimensions[0], IdentityDimension)\n        self.assertIsInstance(rs.dimensions[1], IdentityDimension)\n        self.assertIsInstance(rs.dimensions[2], IdentityDimension)\n\n    def test_zeros_like(self):\n        arr = ArrayWithUnits(\n            np.zeros((3, 2, 10)),\n            [\n                ContrivedDimension(10),\n                IdentityDimension(),\n                ContrivedDimension2(10)\n            ])\n        zeros = arr.zeros_like()\n        self.assertEqual(zeros.shape, arr.shape)\n        self.assertEqual(zeros.dimensions, arr.dimensions)\n        np.testing.assert_allclose(0, zeros)\n\n    def test_assigns_size_where_appropriate(self):\n        arr = ArrayWithUnits(\n            np.zeros((100, 10)),\n            [ContrivedDimension(10), ContrivedDimension2(10)])\n        self.assertEqual(10, arr.dimensions[1].size)\n\n    def test_can_create_new_array_from_example(self):\n        arr = ArrayWithUnits(\n            np.zeros((100, 10)),\n            [ContrivedDimension(10), ContrivedDimension2(10)])\n        arr2 = ArrayWithUnits.from_example(np.zeros((90, 5)), arr)\n        self.assertSequenceEqual(arr.dimensions, arr2.dimensions)\n        self.assertEqual((90, 5), arr2.shape)\n\n    def test_zeros(self):\n        arr = ArrayWithUnits(\n            np.zeros((100, 10)),\n            [ContrivedDimension(10), ContrivedDimension2(10)])\n        arr2 = ArrayWithUnits.zeros(arr)\n        self.assertEqual(arr.shape, arr2.shape)\n        self.assertSequenceEqual(arr.dimensions, arr2.dimensions)\n        np.testing.assert_allclose(arr2, 0)\n\n    def test_zeros_dtype(self):\n        arr = ArrayWithUnits(\n            np.zeros((100, 10), dtype=np.complex128),\n            [ContrivedDimension(10), ContrivedDimension2(10)])\n        arr2 = ArrayWithUnits.zeros(arr)\n        self.assertEqual(arr.dtype, arr2.dtype)\n\n    def test_size_is_not_modified_on_example_dimensions(self):\n        arr = ArrayWithUnits(\n            np.zeros((100, 10)),\n            [ContrivedDimension(10), ContrivedDimension2(10)])\n        arr2 = ArrayWithUnits.from_example(np.zeros((90, 5)), arr)\n        self.assertEqual(100, arr.dimensions[0].size)\n        self.assertEqual(10, arr.dimensions[1].size)\n        self.assertEqual(90, arr2.dimensions[0].size)\n        self.assertEqual(5, arr2.dimensions[1].size)\n\n    def test_can_use_ellipsis_to_get_entire_array(self):\n        raw = np.zeros((10, 10, 10))\n        dims = (\n            ContrivedDimension(10),\n            ContrivedDimension(10),\n            ContrivedDimension(10)\n        )\n        arr = ContrivedArray(raw, dims)\n        result = arr[...]\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((10, 10, 10), result.shape)\n        self.assertEqual(3, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(result.dimensions[1], ContrivedDimension)\n        self.assertIsInstance(result.dimensions[2], ContrivedDimension)\n\n    def test_can_use_ellipsis_to_get_last_two_dimensions(self):\n        raw = np.zeros((10, 10, 10))\n        dims = (\n            ContrivedDimension(10),\n            ContrivedDimension(10),\n            ContrivedDimension(10)\n        )\n        arr = ContrivedArray(raw, dims)\n        result = arr[ContrivedSlice(10, 30), ...]\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((2, 10, 10), result.shape)\n        self.assertEqual(3, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(result.dimensions[1], ContrivedDimension)\n        self.assertIsInstance(result.dimensions[2], ContrivedDimension)\n\n    def test_can_use_ellipsis_to_get_first_two_dimensions(self):\n        raw = np.zeros((10, 10, 10))\n        dims = (\n            ContrivedDimension(10),\n            ContrivedDimension(10),\n            ContrivedDimension(10)\n        )\n        arr = ContrivedArray(raw, dims)\n        result = arr[..., ContrivedSlice(10, 30)]\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((10, 10, 2), result.shape)\n        self.assertEqual(3, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(result.dimensions[1], ContrivedDimension)\n        self.assertIsInstance(result.dimensions[2], ContrivedDimension)\n\n    def test_can_multiply(self):\n        raw = np.ones((8, 9))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr * 10\n        np.testing.assert_allclose(result, 10)\n\n    def test_can_get_single_scalar_from_max_with_no_axis(self):\n        raw = np.zeros((8, 9))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr.max()\n        self.assertIsInstance(result, float)\n        self.assertEqual(0, result)\n\n    def test_max_array_maintains_correct_dimensions_axis_0(self):\n        raw = np.zeros((8, 9))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr.max(axis=0)\n        self.assertEqual((9,), result.shape)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension2)\n\n    def test_max_array_maintains_correct_dimensions_axis_1(self):\n        raw = np.zeros((8, 9))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr.max(axis=1)\n        self.assertEqual((8,), result.shape)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension)\n\n    def test_max_supports_keepdims(self):\n        raw = np.random.random_sample((10, 9, 8, 7))\n        arr = ArrayWithUnits(\n            raw,\n            dimensions=[\n                ContrivedDimension(10),\n                ContrivedDimension2(10),\n                ContrivedDimension(10),\n                ContrivedDimension2(10)\n            ])\n        result = arr.max(axis=(1, 2), keepdims=True)\n        self.assertEqual((10, 1, 1, 7), result.shape)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(4, len(result.dimensions))\n        print(result.dimensions)\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(result.dimensions[1], IdentityDimension)\n        self.assertIsInstance(result.dimensions[2], IdentityDimension)\n        self.assertIsInstance(result.dimensions[3], ContrivedDimension2)\n\n    def test_max_supports_multiple_axes(self):\n        raw = np.random.random_sample((10, 9, 8, 7))\n        arr = ArrayWithUnits(\n            raw,\n            dimensions=[\n                ContrivedDimension(10),\n                IdentityDimension(),\n                IdentityDimension(),\n                ContrivedDimension2(10)\n            ])\n        result = arr.max(axis=(1, 2))\n        self.assertEqual((10, 7), result.shape)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(2, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(result.dimensions[1], ContrivedDimension2)\n\n    def test_get_single_scalar_from_sum_with_no_axis(self):\n        raw = np.zeros((8, 9))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr.sum()\n        self.assertIsInstance(result, float)\n        self.assertEqual(0, result)\n\n    def test_array_maintains_correct_dimension_after_reduction(self):\n        raw = np.zeros((8, 9))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr.sum(axis=1)\n        self.assertEqual((8,), result.shape)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension)\n\n    def test_array_maintains_correct_dimension_after_reduction2(self):\n        raw = np.zeros((8, 9))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr.sum(axis=0)\n        self.assertEqual((9,), result.shape)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension2)\n\n    def test_array_maintains_correct_dimension_after_reduction3(self):\n        raw = np.zeros((8, 9))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = np.sum(arr, axis=0)\n        self.assertEqual((9,), result.shape)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension2)\n\n    def test_array_cannot_maintain_correct_dimension(self):\n        raw = np.zeros((10, 10))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr.sum(axis=1)\n        self.assertEqual((10,), result.shape)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension)\n\n    def test_array_maintains_correct_dimensions_after_dot(self):\n        raw = np.zeros((8, 9))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr.dot(np.zeros(9))\n        self.assertEqual((8,), result.shape)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension)\n\n    @unittest2.skip(\n        'this test fails because there is no hook to intercept this call')\n    def test_array_maintains_correct_dimensions_after_dot2(self):\n        raw = np.zeros((8, 9))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = np.dot(arr, np.zeros(9))\n        self.assertEqual((8,), result.shape)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension)\n\n    def test_can_select_subset_using_boolean_array(self):\n        raw = np.arange(10)\n        arr = ContrivedArray(raw, (ContrivedDimension(10),))\n        result = arr[arr >= 5]\n        self.assertIsInstance(result, np.ndarray)\n        self.assertEqual((5,), result.shape)\n\n    def test_correct_result_of_indexing_using_boolean_array(self):\n        raw = np.random.random_sample((8, 9))\n        arr = ContrivedArray(raw, (ContrivedDimension(10), IdentityDimension()))\n        result = arr[arr > 0.5]\n        self.assertIsInstance(result, np.ndarray)\n        self.assertNotIsInstance(result, ArrayWithUnits)\n\n    def test_can_select_subset_using_equality(self):\n        raw = np.arange(10)\n        arr = ContrivedArray(raw, (ContrivedDimension(10),))\n        result = arr[(arr == 5) | (arr == 6)]\n        self.assertIsInstance(result, np.ndarray)\n        self.assertEqual((2,), result.shape)\n\n    def test_can_set_subset_using_boolean_array(self):\n        raw = np.zeros(10)\n        arr = ContrivedArray(raw, (ContrivedDimension(10),))\n        arr[arr == 0] = 10\n        np.testing.assert_allclose(arr, 10)\n\n    def test_can_apply_new_axis(self):\n        raw = np.zeros((3, 5))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr[None, -1]\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((1, 5), result.shape)\n        self.assertEqual(2, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], IdentityDimension)\n        self.assertIsInstance(result.dimensions[1], ContrivedDimension2)\n\n    def test_can_apply_new_axis_at_end(self):\n        raw = np.zeros((3, 5))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr[-1, None]\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((1, 5), result.shape)\n        self.assertEqual(2, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], IdentityDimension)\n        self.assertIsInstance(result.dimensions[1], ContrivedDimension2)\n\n    def test_can_apply_new_axis_in_middle(self):\n        raw = np.zeros((3, 5))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr[:, None, :]\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((3, 1, 5), result.shape)\n        self.assertEqual(3, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(result.dimensions[1], IdentityDimension)\n        self.assertIsInstance(result.dimensions[2], ContrivedDimension2)\n\n    def test_can_apply_two_new_axes(self):\n        raw = np.zeros((3, 5))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr[None, None, -1]\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((1, 1, 5), result.shape)\n        self.assertEqual(3, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], IdentityDimension)\n        self.assertIsInstance(result.dimensions[1], IdentityDimension)\n        self.assertIsInstance(result.dimensions[2], ContrivedDimension2)\n\n    def test_correct_axis_is_preserved(self):\n        raw = np.zeros((10, 10))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr[0]\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertEqual((10,), result.shape)\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension2)\n\n    def test_multiple_axis_types(self):\n        raw = np.zeros((10, 10))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr[ContrivedSlice(20, 40), ContrivedSlice(20, 50)]\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(2, len(result.dimensions))\n        self.assertEqual((2, 3), result.shape)\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(result.dimensions[1], ContrivedDimension2)\n\n    def test_can_use_list_of_integers_as_index(self):\n        raw = np.zeros((10, 10))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr[[1, 3, 5]]\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((3, 10), result.shape)\n        self.assertEqual(2, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], IdentityDimension)\n        self.assertIsInstance(result.dimensions[1], ContrivedDimension2)\n\n    def test_can_use_multiple_integers(self):\n        raw = np.zeros((10, 10))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        result = arr[0, 0]\n        self.assertEqual(0, result)\n\n    def test_1d(self):\n        raw = np.zeros(10)\n        arr = ContrivedArray(raw, (ContrivedDimension(10),))\n        result = arr[1:4]\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((3,), result.shape)\n        self.assertIsInstance(result.dimensions[0], ContrivedDimension)\n\n    def test_can_get_modified_dimension(self):\n        raw = np.zeros(10)\n        arr = ContrivedArray(\n            raw, (AsciiCharacterDimension(ascii_lowercase[:10]),))\n        result = arr[1:4]\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((3,), result.shape)\n        self.assertIsInstance(result.dimensions[0], AsciiCharacterDimension)\n        self.assertEqual(ascii_lowercase[1:4], result.dimensions[0].labels)\n\n    def test_can_get_custom_slice(self):\n        raw = np.zeros((10, 10))\n        arr = ContrivedArray(raw, (None, ContrivedDimension(10)))\n        custom_slice = ContrivedSlice(50, 70)\n        result = arr[5:7, custom_slice]\n        self.assertEqual((2, 2), result.shape)\n\n    def test_can_set_custom_slice(self):\n        raw = np.zeros((10, 10))\n        arr = ContrivedArray(raw, (None, ContrivedDimension(10)))\n        custom_slice = ContrivedSlice(50, 70)\n        arr[5:7, custom_slice] = 1\n        np.testing.assert_allclose(arr[5:7, 5:7], 1)\n\n    def test_can_set_from_list_of_integers(self):\n        raw = np.zeros((10, 10))\n        arr = ContrivedArray(\n            raw, (ContrivedDimension(10), ContrivedDimension2(10)))\n        arr[[1, 3, 5]] = 5\n        np.testing.assert_allclose(arr[[1, 3, 5]], 5)\n\n    def test_dimensions_must_match(self):\n        raw = np.zeros((3, 3, 3))\n        self.assertRaises(\n            ValueError,\n            lambda: ContrivedArray(raw, (None, ContrivedDimension(5))))\n\n    def test_custom_dimensions_are_preserved_after_slice(self):\n        raw = np.zeros((10, 10))\n        arr = ContrivedArray(raw, (None, ContrivedDimension(10)))\n        custom_slice = ContrivedSlice(50, 70)\n        result = arr[5:7, custom_slice]\n        self.assertIsInstance(result.dimensions[0], IdentityDimension)\n        self.assertEqual(ContrivedDimension(10), result.dimensions[1])\n\n    def test_custom_dimensions_are_preserved_after_slice_that_removes_dimension(\n            self):\n        raw = np.zeros((10, 10))\n        arr = ContrivedArray(raw, (None, ContrivedDimension(10)))\n        custom_slice = ContrivedSlice(50, 70)\n        result = arr[0, custom_slice]\n        self.assertEqual(1, result.ndim)\n        self.assertEqual((2,), result.shape)\n        self.assertEqual(ContrivedDimension(10), result.dimensions[0])\n\n    def test_can_get_single_value_from_custom_dimension(self):\n        raw = np.zeros(10)\n        arr = ContrivedArray(raw, (ContrivedDimension(10),))\n        result = arr[0]\n        self.assertEqual(0, result)\n        self.assertIsInstance(result, float)\n\n    def test_can_set_single_value_in_custom_dimension(self):\n        raw = np.zeros(10)\n        arr = ContrivedArray(raw, (ContrivedDimension(10),))\n        arr[0] = 1\n        self.assertEqual(1, arr[0])\n\n    def test_sliding_window_maintains_dtype(self):\n        raw = np.zeros(10, dtype=np.uint8)\n        arr = ContrivedArray(raw, (ContrivedDimension(10),))\n        new_arr = arr.sliding_window((ContrivedSlice(0, 20),))\n        self.assertEqual(np.uint8, new_arr.dtype)\n\n    def test_custom_dimension_is_maintained_for_sliding_window_1d(self):\n        raw = np.zeros(10)\n        arr = ContrivedArray(raw, (ContrivedDimension(10),))\n        new_arr = arr.sliding_window((ContrivedSlice(0, 20),))\n        self.assertIsInstance(new_arr, ArrayWithUnits)\n        self.assertEqual((5, 2), new_arr.shape)\n        self.assertIsInstance(new_arr.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(new_arr.dimensions[1], ContrivedDimension)\n        self.assertEqual(20, new_arr.dimensions[0].factor)\n        self.assertEqual(10, new_arr.dimensions[1].factor)\n\n    def test_item_access_is_correct_after_sliding_window_1d(self):\n        raw = np.zeros(10)\n        arr = ContrivedArray(raw, (ContrivedDimension(10),))\n        new_arr = arr.sliding_window((ContrivedSlice(0, 20),))\n        sliced = new_arr[ContrivedSlice(0, 40)]\n        self.assertEqual((2, 2), sliced.shape)\n\n    def test_custom_dimension_is_maintained_for_sw_1d_with_step(self):\n        raw = np.zeros(10)\n        arr = ContrivedArray(raw, (ContrivedDimension(10),))\n        new_arr = arr.sliding_window(\n            (ContrivedSlice(0, 20),), (ContrivedSlice(0, 10),))\n        self.assertIsInstance(new_arr, ArrayWithUnits)\n        self.assertEqual((9, 2), new_arr.shape)\n        self.assertIsInstance(new_arr.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(new_arr.dimensions[1], ContrivedDimension)\n        self.assertEqual(20, new_arr.dimensions[0].factor)\n        self.assertEqual(10, new_arr.dimensions[1].factor)\n\n    def test_custom_dimension_is_maintained_for_sliding_window(self):\n        raw = np.zeros((10, 10))\n        arr = ContrivedArray(raw, (ContrivedDimension(10), None))\n        new_arr = arr.sliding_window((ContrivedSlice(0, 20), 10))\n        self.assertIsInstance(new_arr, ArrayWithUnits)\n        self.assertEqual((5, 2, 10), new_arr.shape)\n        self.assertIsInstance(new_arr.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(new_arr.dimensions[1], ContrivedDimension)\n        self.assertIsInstance(new_arr.dimensions[2], IdentityDimension)\n        self.assertEqual(20, new_arr.dimensions[0].factor)\n        self.assertEqual(10, new_arr.dimensions[1].factor)\n\n    def test_custom_dimension_is_maintained_for_sw_with_step(self):\n        raw = np.zeros((10, 10))\n        arr = ContrivedArray(raw, (ContrivedDimension(10), None))\n        new_arr = arr.sliding_window(\n            (ContrivedSlice(0, 20), 10), (ContrivedSlice(0, 10), 10))\n        self.assertIsInstance(new_arr, ArrayWithUnits)\n        self.assertEqual((9, 2, 10), new_arr.shape)\n        self.assertIsInstance(new_arr.dimensions[0], ContrivedDimension)\n        self.assertIsInstance(new_arr.dimensions[1], ContrivedDimension)\n        self.assertIsInstance(new_arr.dimensions[2], IdentityDimension)\n        self.assertEqual(20, new_arr.dimensions[0].factor)\n        self.assertEqual(10, new_arr.dimensions[1].factor)\n\n    def test_custom_dimension_is_not_maintained_for_sliding_window(self):\n        raw = np.zeros((10, 10))\n        arr = ContrivedArray(raw, (ContrivedDimension(10), None))\n        new_arr = arr.sliding_window((ContrivedSlice(0, 20), 2))\n        self.assertIsInstance(new_arr, ArrayWithUnits)\n        self.assertEqual((25, 2, 2), new_arr.shape)\n        self.assertIsInstance(new_arr.dimensions[0], IdentityDimension)\n        self.assertIsInstance(new_arr.dimensions[1], IdentityDimension)\n        self.assertIsInstance(new_arr.dimensions[2], IdentityDimension)\n\n    def test_custom_dimension_is_not_maintained_for_sw_with_step(self):\n        raw = np.zeros((10, 10))\n        arr = ContrivedArray(raw, (ContrivedDimension(10), None))\n        new_arr = arr.sliding_window(\n            (ContrivedSlice(0, 20), 2), (ContrivedSlice(0, 10), 1))\n        self.assertIsInstance(new_arr, ArrayWithUnits)\n        self.assertEqual((81, 2, 2), new_arr.shape)\n        self.assertIsInstance(new_arr.dimensions[0], IdentityDimension)\n        self.assertIsInstance(new_arr.dimensions[1], IdentityDimension)\n        self.assertIsInstance(new_arr.dimensions[2], IdentityDimension)\n"""
zounds/core/test_identitydimension.py,0,"b'import unittest2\nfrom .dimensions import IdentityDimension, Dimension\n\n\nclass IdentityDimensionTests(unittest2.TestCase):\n    def test_equal(self):\n        d1 = IdentityDimension()\n        d2 = IdentityDimension()\n        self.assertEqual(d1, d2)\n\n    def test_not_equal(self):\n        d1 = Dimension()\n        d2 = IdentityDimension()\n        self.assertNotEqual(d1, d2)\n'"
zounds/datasets/__init__.py,0,"b'""""""\nThe datasets module provides access to some common sources of audio on the\ninternet.  In general, a dataset instance is an iterable of\n:class:`zounds.soundfile.AudioMetaData` instances that can be passed to the\nroot node of an audio processing graph.\n""""""\n\nfrom .musicnet import MusicNet\nfrom .phatdrumloops import PhatDrumLoops\nfrom .internetarchive import InternetArchive\nfrom .freesound import FreeSoundSearch\nfrom .filesystem import Directory\nfrom .cache import DataSetCache\nfrom .ingest import ingest\nfrom .predownload import PreDownload\nfrom .nsynth import NSynth\nfrom .composite import CompositeDataset\n'"
zounds/datasets/cache.py,0,"b""import os\nimport hashlib\nimport requests\n\n\nclass DataSetCache(object):\n    def __init__(self, path, dataset):\n        super(DataSetCache, self).__init__()\n        self.dataset = dataset\n        self.path = path\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n    def __iter__(self):\n        for request in self.dataset:\n            h = hashlib.md5(request.url).hexdigest()\n            p = os.path.join(self.path, h)\n\n            if not os.path.exists(p):\n                s = requests.Session()\n                prepped = request.prepare()\n                resp = s.send(prepped, stream=True)\n\n                resp.raise_for_status()\n\n                with open(p, 'wb') as f:\n                    for chunk in resp.iter_content(1024):\n                        f.write(chunk)\n\n            yield p\n"""
zounds/datasets/composite.py,0,"b'\n\nclass CompositeDataset(object):\n    """"""\n    A dataset composed of two or more others\n\n    Args:\n        datasets (list of datasets): One or more other datasets\n\n    Examples:\n        >>> from zounds import InternetArchive, CompositeDataset, ingest\n        >>> dataset1 = InternetArchive(\'beethoven_ingigong_850\')\n        >>> dataset2 = InternetArchive(\'The_Four_Seasons_Vivaldi-10361\')\n        >>> composite = CompositeDataset(dataset1, dataset2)\n        >>> ingest(composite, Sound) # ingest data from both datasets\n    """"""\n\n    def __init__(self, *datasets):\n        super(CompositeDataset, self).__init__()\n        self.datasets = datasets\n\n    def __iter__(self):\n        for dataset in self.datasets:\n            for meta in dataset:\n                yield meta\n'"
zounds/datasets/filesystem.py,0,"b'import os\n\n\nclass Directory(object):\n    def __init__(self, path):\n        self.path = path\n\n    def __iter__(self):\n        for f in os.listdir(self.path):\n            p = os.path.join(self.path, f)\n            if not os.path.isdir(p):\n                yield os.path.join(self.path, f)\n'"
zounds/datasets/freesound.py,0,"b'import requests\nimport time\nfrom zounds.soundfile import AudioMetaData\n\n\nclass FreeSoundSearch(object):\n    """"""\n    Produces an iterable of :class:`zounds.soundfile.AudioMetaData` instances\n    for every result from a https://freesound.org search\n\n    Args:\n        api_key (str): Your freesound.org API key (get one here:\n            (http://freesound.org/apiv2/apply/))\n        query (str): The text query to perform\n\n    Raises\n        ValueError: when `api_key` and/or `query` are not supplied\n\n    Examples:\n        >>> from zounds import FreeSoundSearch\n        >>> fss = FreeSoundSearch(\'YOUR_API_KEY\', \'guitar\')\n        >>> iter(fss).next()\n        {\'description\': u\'Etude of Electric Guitar in Dm. Used chorus and reverberation effects. Size 6/4. Tempo 100. Gloomy and sentimental.\', \'tags\': [u\'Etude\', u\'Experemental\', u\'Guitar\', u\'guitar\', u\'Electric\', u\'Chorus\'], \'uri\': <Request [GET]>, \'channels\': 2, \'licensing\': u\'http://creativecommons.org/licenses/by/3.0/\', \'samplerate\': 44100.0}\n\n    See Also:\n        :class:`InternetArchive`\n        :class:`PhatDrumLoops`\n        :class:`zounds.soundfile.AudioMetaData`\n    """"""\n\n    def __init__(self, api_key, query, n_results=10, delay=0.2):\n        super(FreeSoundSearch, self).__init__()\n\n        if not api_key:\n            raise ValueError(\'You must supply a freesound.org API key\')\n\n        if not query:\n            raise ValueError(\'You must supply a text query\')\n\n        self.delay = delay\n        self.n_results = n_results\n        self.query = query\n        self.api_key = api_key\n\n    def _get_metadata_by_id(self, _id):\n        sound_data = requests.get(\n            \'http://www.freesound.org/apiv2/sounds/{_id}\'.format(**locals()),\n            params={\'token\': self.api_key}\n        )\n        sound_data.raise_for_status()\n        return sound_data.json()\n\n    def _freesound_to_audio_metadata(self, data):\n        request = requests.Request(\n            method=\'GET\',\n            url=data[\'previews\'][\'preview-hq-ogg\'],\n            params={\'token\': self.api_key})\n\n        web_url = \'https://freesound.org/people/{username}/sounds/{id}/\'\\\n            .format(**data)\n\n        return AudioMetaData(\n            uri=request,\n            samplerate=data[\'samplerate\'],\n            channels=data[\'channels\'],\n            licensing=data[\'license\'],\n            description=data[\'description\'],\n            tags=data[\'tags\'],\n            web_url=web_url)\n\n    def get_by_id(self, freesound_id):\n        data = self._get_metadata_by_id(freesound_id)\n        print(data)\n        return self._freesound_to_audio_metadata(data)\n\n    def _iter_results(self, link=None):\n        if link:\n            results = requests.get(link, params={\'token\': self.api_key})\n        else:\n            results = requests.get(\n                \'http://www.freesound.org/apiv2/search/text\',\n                params={\n                    \'query\': self.query,\n                    \'token\': self.api_key\n                })\n        time.sleep(self.delay)\n\n        results.raise_for_status()\n        results = results.json()\n\n        for r in results[\'results\']:\n            yield self._get_metadata_by_id(r[\'id\'])\n\n            # prevent 429 ""Too Many Requests"" responses\n            time.sleep(self.delay)\n\n        for r in self._iter_results(results[\'next\']):\n            yield r\n\n    def __iter__(self):\n        for i, data in enumerate(self._iter_results()):\n            if i > self.n_results:\n                break\n            yield self._freesound_to_audio_metadata(data)\n'"
zounds/datasets/ingest.py,0,"b""from multiprocessing.pool import ThreadPool, Pool\nfrom itertools import repeat\nfrom os import cpu_count\n\n\ndef ingest_one(arg):\n    metadata, cls, skip_if_exists = arg\n    request = metadata.request\n    url = request.url\n    if skip_if_exists and cls.exists(request.url):\n        print('already processed {request.url}'.format(**locals()))\n        return\n\n    try:\n        print('processing {request.url}'.format(**locals()))\n        cls.process(meta=metadata, _id=url)\n    except Exception as e:\n        print(e)\n\n\ndef ingest(\n        dataset,\n        cls,\n        skip_if_exists=True,\n        multi_process=False,\n        multi_threaded=False,\n        cores=None):\n\n    pool = None\n\n    if multi_process:\n        pool = Pool(cores or cpu_count())\n        map_func = pool.imap_unordered\n    elif multi_threaded:\n        pool = ThreadPool(cores or cpu_count())\n        map_func = pool.imap_unordered\n    else:\n        map_func = map\n\n    cls_args = repeat(cls)\n    skip_args = repeat(skip_if_exists)\n\n    map_func(ingest_one, zip(dataset, cls_args, skip_args))\n\n    if pool is not None:\n        # if we're ingesting using multiple processes or threads, the processing\n        # should be parallel, but this method should be synchronous from the\n        # caller's perspective\n        pool.close()\n        pool.join()\n\n"""
zounds/datasets/internetarchive.py,0,"b'import requests\nimport urllib.parse\nfrom zounds.soundfile.audio_metadata import AudioMetaData\n\n\nclass InternetArchive(object):\n    """"""\n    Produces an iterable of :class:`zounds.soundfile.AudioMetaData` instances\n    for every file of a particular format from an internet archive id.\n\n    Args:\n        archive_id (str): the Internet Archive identifier\n        format_filter (str): The file format to return\n        attrs (dict): Extra attributes to add to the :class:`AudioMetaData`\n\n    Raises:\n        ValueError: when archive_id is not provided\n\n    Examples:\n        >>> from zounds import InternetArchive\n        >>> ia = InternetArchive(\'Greatest_Speeches_of_the_20th_Century\')\n        >>> iter(ia).next()\n        {\'creator\': u\'John F. Kennedy\', \'height\': u\'0\', \'channels\': None, \'genre\': u\'Folk\', \'licensing\': None, \'mtime\': u\'1236666800\', \'samplerate\': None, \'size\': u\'7264435\', \'album\': u\'Great Speeches of the 20th Century [Box Set] Disc 2\', \'title\': u\'The Cuban Missile Crisis\', \'format\': u\'128Kbps MP3\', \'source\': u\'original\', \'description\': None, \'tags\': None, \'track\': u\'15\', \'crc32\': u\'ace17eb5\', \'md5\': u\'e00f4e7bd9df7bdba4db7098d1ccdfe0\', \'sha1\': u\'e42d1f348078a11ed9a6ea9c8934a1236235c7b3\', \'artist\': u\'John F. Kennedy\', \'external-identifier\': [u\'urn:acoustid:ff850a0c-2efa-450f-8034-efdb31a9b696\', u\'urn:mb_recording_id:912cedd0-5530-4f26-972c-13d131fef06e\'], \'uri\': <Request [GET]>, \'length\': u\'454.03\', \'width\': u\'0\'}\n\n    See Also:\n        :class:`FreeSoundSearch`\n        :class:`PhatDrumLoops`\n        :class:`zounds.soundfile.AudioMetaData`\n    """"""\n    def __init__(self, archive_id, format_filter=None, **attrs):\n        super(InternetArchive, self).__init__()\n\n        self.attrs = attrs\n        if not archive_id:\n            raise ValueError(\'You must supply an Internet Archive id\')\n\n        self.format_filter = format_filter or \\\n            (lambda x: x[\'format\'] == \'Ogg Vorbis\')\n        self.archive_id = archive_id\n\n    def _get_metadata(self, data, all_files):\n        if data[\'source\'] == \'original\':\n            return data\n        elif data[\'source\'] == \'derivative\':\n            return all_files[\'/\' + data[\'original\']]\n\n    def __iter__(self):\n        base_url = \'https://archive.org/\'\n        archive_id = self.archive_id\n        url = urllib.parse.urljoin(\n            base_url, \'/details/{archive_id}&output=json\'.format(**locals()))\n        resp = requests.get(url)\n\n        try:\n            all_files = resp.json()[\'files\']\n        except ValueError as e:\n            all_files = dict()\n\n        for k, v in all_files.items():\n            if self.format_filter(v):\n                sound_url = urllib.parse.urljoin(\n                    base_url, \'/download/{archive_id}{k}\'.format(**locals()))\n                request = requests.Request(method=\'GET\', url=sound_url)\n                metadata = self._get_metadata(v, all_files)\n                metadata.update(self.attrs)\n                web_url = \'https://archive.org//details/{archive_id}\'\\\n                    .format(**locals())\n                metadata.update(web_url=web_url)\n                yield AudioMetaData(uri=request, **metadata)\n'"
zounds/datasets/musicnet.py,0,"b'import csv\n\nimport numpy as np\n\nfrom .predownload import PreDownload\nfrom .util import ensure_local_file\nfrom zounds.soundfile import AudioMetaData\nfrom zounds.timeseries import SR44100, AudioSamples\nimport os\n\n\nclass MusicNet(object):\n    """"""\n    Provides access to the audio and high-level metadata from MusicNet.  More\n    info can be found here:\n    https://homes.cs.washington.edu/~thickstn/musicnet.html\n\n    This assumes you\'ve downloaded and extracted the files from\n    https://homes.cs.washington.edu/~thickstn/media/musicnet.tar.gz to path\n    """"""\n    def __init__(self, path):\n        super(MusicNet, self).__init__()\n        self.path = path\n        self._metadata = \\\n            \'https://homes.cs.washington.edu/~thickstn/media/musicnet_metadata.csv\'\n        self._samplerate = SR44100()\n\n    def __iter__(self):\n        local_metadata = ensure_local_file(self._metadata, self.path)\n\n        metadata = dict()\n        with open(local_metadata, \'rb\') as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                metadata[row[\'id\']] = row\n\n        train_audio_path = os.path.join(self.path, \'train_data\')\n\n        for filename in os.listdir(train_audio_path):\n            full_path = os.path.join(train_audio_path, filename)\n            _id, ext = os.path.splitext(filename)\n            url = \\\n                \'https://homes.cs.washington.edu/~thickstn/media/{_id}\'\\\n                    .format(**locals())\n            meta = metadata[_id]\n            samples = AudioSamples.from_file(full_path)\n            uri = PreDownload(samples.encode().read(), url)\n            yield AudioMetaData(\n                uri=uri,\n                samplerate=int(self._samplerate),\n                **meta)\n'"
zounds/datasets/nsynth.py,0,"b'import fnmatch\nimport json\nimport os\nimport tarfile\n\nfrom .predownload import PreDownload\nfrom .util import ensure_local_file\nfrom zounds.soundfile import AudioMetaData\n\n\nclass NSynth(object):\n    """"""\n    Provides acess to the NSynth dataset:\n    https://magenta.tensorflow.org/datasets/nsynth\n\n    Currently only downloads and iterates over the validation set\n    """"""\n\n    def __init__(self, path):\n        super(NSynth, self).__init__()\n        self.path = path\n        self._url = \\\n            \'http://download.magenta.tensorflow.org/datasets/nsynth/nsynth-valid.jsonwav.tar.gz\'\n\n    def __iter__(self):\n        local_data = ensure_local_file(self._url, self.path)\n\n        json_data = None\n        with tarfile.open(local_data) as tar:\n            for info in tar:\n\n                if fnmatch.fnmatch(info.name, \'*.json\'):\n                    flo = tar.extractfile(member=info)\n                    json_data = json.load(flo)\n\n        with tarfile.open(local_data) as tar:\n            for info in tar:\n                if fnmatch.fnmatch(info.name, \'*.json\'):\n                    continue\n                if not info.isfile():\n                    continue\n                path_segments = os.path.split(info.name)\n                _id = os.path.splitext(path_segments[1])[0]\n                wav_flo = tar.extractfile(member=info)\n                url = \\\n                    \'https://magenta.tensorflow.org/datasets/nsynth/{_id}\' \\\n                        .format(**locals())\n                pdl = PreDownload(wav_flo.read(), url)\n                yield AudioMetaData(\n                    uri=pdl,\n                    web_url=\'https://magenta.tensorflow.org/datasets/nsynth\',\n                    **json_data[_id])\n'"
zounds/datasets/phatdrumloops.py,0,"b'from zounds.soundfile import AudioMetaData\nimport requests\nimport re\nimport urllib.parse\n\n\nclass PhatDrumLoops(object):\n    """"""\n    Produces an iterable of :class:`zounds.soundfile.AudioMetaData` instances\n    for every drum break from http://phatdrumloops.com/beats.php\n\n    Args:\n        attrs (dict): Extra properties to add to the :class:`AudioMetaData`\n\n    Examples\n        >>> from zounds import PhatDrumLoops\n        >>> pdl = PhatDrumLoops()\n        >>> iter(pdl).next()\n        {\'description\': None, \'tags\': None, \'uri\': <Request [GET]>, \'channels\': None, \'licensing\': None, \'samplerate\': None}\n\n\n    See Also:\n        :class:`InternetArchive`\n        :class:`FreeSoundSearch`\n        :class:`zounds.soundfile.AudioMetaData`\n    """"""\n    def __init__(self, **attrs):\n        super(PhatDrumLoops, self).__init__()\n        self.attrs = attrs\n        self.attrs.update(web_url=\'http://www.phatdrumloops.com/beats.php\')\n\n    def __iter__(self):\n        resp = requests.get(\'http://phatdrumloops.com/beats.php\')\n        pattern = re.compile(\'href=""(?P<uri>/audio/wav/[^\\.]+\\.wav)""\')\n        for m in pattern.finditer(resp.content):\n            url = urllib.parse.urljoin(\'http://phatdrumloops.com\',\n                                   m.groupdict()[\'uri\'])\n            request = requests.Request(\n                method=\'GET\',\n                url=url,\n                headers={\'Range\': \'bytes=0-\'})\n            yield AudioMetaData(uri=request, **self.attrs)\n'"
zounds/datasets/predownload.py,0,"b""from io import BytesIO\n\n\nclass PreDownload(BytesIO):\n    def __init__(self, initial_bytes, url):\n        super(PreDownload, self).__init__(initial_bytes)\n        if not url:\n            raise ValueError('url must be provided')\n        self.url = url\n"""
zounds/datasets/test_composite.py,0,"b""import unittest2\nfrom .predownload import PreDownload\nfrom .composite import CompositeDataset\nfrom zounds.soundfile import AudioMetaData\n\n\nclass DatasetA(object):\n    def __init__(self):\n        super(DatasetA, self).__init__()\n\n    def __iter__(self):\n        yield AudioMetaData(uri=PreDownload(b'', 'http://example.com/1'))\n        yield AudioMetaData(uri=PreDownload(b'', 'http://example.com/2'))\n\n\nclass DatasetB(object):\n    def __init__(self):\n        super(DatasetB, self).__init__()\n\n    def __iter__(self):\n        yield AudioMetaData(uri=PreDownload(b'', 'http://example.com/3'))\n        yield AudioMetaData(uri=PreDownload(b'', 'http://example.com/4'))\n\n\nclass CompositeDatasetTests(unittest2.TestCase):\n\n    def test_composite_iterates_over_multiple_datasets(self):\n        composite = CompositeDataset(DatasetA(), DatasetB())\n        items = [x.uri.url for x in composite]\n        self.assertEqual(4, len(items))\n        self.assertIn('http://example.com/1', items)\n        self.assertIn('http://example.com/2', items)\n        self.assertIn('http://example.com/3', items)\n        self.assertIn('http://example.com/4', items)\n"""
zounds/datasets/test_predownload.py,0,"b""import unittest2\nfrom .predownload import PreDownload\n\n\nclass PreDownloadTest(unittest2.TestCase):\n    def test_can_instantiate_predownload(self):\n        pdl = PreDownload(b'something', 'https://example.com')\n        self.assertEqual(b'something', pdl.read())\n\n    def test_raises_when_no_url_is_provided(self):\n        self.assertRaises(ValueError, lambda: PreDownload(b'something', None))\n"""
zounds/datasets/util.py,0,"b""import requests\nimport urllib.parse\nimport os\n\n\ndef ensure_local_file(remote_url, local_path, chunksize=4096):\n    parsed = urllib.parse.urlparse(remote_url)\n    filename = os.path.split(parsed.path)[-1]\n    local = os.path.join(local_path, filename)\n\n    if not os.path.exists(local):\n        with open(local, 'wb') as f:\n            resp = requests.get(remote_url, stream=True)\n            total_bytes = int(resp.headers['Content-Length'])\n            for i, chunk in enumerate(resp.iter_content(chunk_size=chunksize)):\n                f.write(chunk)\n                progress = ((i * chunksize) / float(total_bytes)) * 100\n                print('{remote_url} {progress:.2f}% complete'.format(**locals()))\n\n    return local\n"""
zounds/index/__init__.py,0,"b'from .index import SearchResults, HammingIndex\n\nfrom .hammingdb import HammingDb\n\nfrom .brute_force import BruteForceSearch, HammingDistanceBruteForceSearch\n'"
zounds/index/brute_force.py,3,"b""import numpy as np\nfrom scipy.spatial.distance import cdist\nfrom .index import SearchResults\nfrom random import choice\nfrom zounds.timeseries import ConstantRateTimeSeries\nfrom zounds.nputil import packed_hamming_distance\n\n\nclass BaseBruteForceSearch(object):\n    def __init__(self, gen):\n        index = []\n        self._ids = []\n        for _id, example in gen:\n            index.append(example)\n            crts = ConstantRateTimeSeries(example)\n            for ts, _ in crts.iter_slices():\n                self._ids.append((_id, ts))\n        self.index = np.concatenate(index)\n\n    def search(self, query, n_results=10):\n        raise NotImplementedError()\n\n    def random_search(self, n_results=10):\n        query = choice(self.index)\n        return self.search(query, n_results)\n\n\nclass BruteForceSearch(BaseBruteForceSearch):\n    def __init__(self, gen, distance_metric='euclidean'):\n        super(BruteForceSearch, self).__init__(gen)\n        self.distance_metric = distance_metric\n\n    def search(self, query, n_results=10):\n        distances = cdist(\n            query[None, ...], self.index, metric=self.distance_metric)\n        indices = np.argsort(distances[0])[:n_results]\n        return SearchResults(query, (self._ids[i] for i in indices))\n\n\nclass HammingDistanceBruteForceSearch(BaseBruteForceSearch):\n    def __init__(self, gen):\n        super(HammingDistanceBruteForceSearch, self).__init__(gen)\n\n    def search(self, query, n_results=10):\n        scores = packed_hamming_distance(query, self.index)\n        indices = np.argsort(scores)[:n_results]\n        return SearchResults(query, (self._ids[i] for i in indices))\n"""
zounds/index/hammingdb.py,6,"b""import lmdb\nfrom zounds.nputil import Growable, packed_hamming_distance\nimport numpy as np\nfrom multiprocessing.dummy import Pool as ThreadPool\nfrom multiprocessing import cpu_count\nimport os\nimport binascii\n\n\nclass HammingDb(object):\n    def __init__(self, path, map_size=1000000000, code_size=8, writeonly=False):\n        super(HammingDb, self).__init__()\n\n        self.writeonly = writeonly\n\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        self.path = path\n        self.env = lmdb.open(\n            self.path,\n            max_dbs=10,\n            map_size=map_size,\n            writemap=True,\n            map_async=True,\n            metasync=True)\n        self.env.reader_check()\n\n        self.metadata = self.env.open_db(b'metadata')\n        try:\n            self.code_size = int(self.get_metadata(b'codesize'))\n            if code_size and code_size != self.code_size:\n                raise ValueError(\n                    'Database is already initialized with code size {code_size}'\n                    ', but {self.code_size} was passed to __init__'\n                        .format(**locals()))\n        except TypeError:\n            if code_size is None:\n                raise ValueError(\n                    'You must supply a code size for an uninitialized database')\n            if code_size % 8:\n                raise ValueError('code_size must be a multiple of 8')\n            self.set_metadata(b'codesize', str(code_size).encode())\n            self.code_size = code_size\n\n        self.index = self.env.open_db(b'index')\n        self._append_buffer = self._recarray(1)\n        self._code_bytearray = bytearray(b'a' * self.code_size)\n        self._code_buffer = np.frombuffer(self._code_bytearray, dtype=np.uint64)\n        self._codes = None\n        self._ids = set()\n        self._catch_up_on_in_memory_store()\n\n        self._thread_count = cpu_count()\n        self._pool = ThreadPool(processes=self._thread_count)\n\n    def close(self):\n        self.env.close()\n\n    def __del__(self):\n        self.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def set_metadata(self, key, value):\n        with self.env.begin(write=True) as txn:\n            txn.put(key, value, db=self.metadata)\n\n    def get_metadata(self, key):\n        with self.env.begin() as txn:\n            return txn.get(key, db=self.metadata)\n\n    def _catch_up_on_in_memory_store(self):\n        self._initialize_in_memory_store()\n        with self.env.begin() as txn:\n            cursor = txn.cursor(db=self.index)\n            for i, bundle in enumerate(cursor.iternext(keys=True, values=True)):\n                _id, value = bundle\n                if _id in self._ids:\n                    continue\n\n                code = value[:self.code_size]\n                self._add_code(_id, code)\n\n    def __len__(self):\n        with self.env.begin() as txn:\n            lmdb_size = txn.stat(self.index)['entries']\n            if not lmdb_size:\n                return 0\n            return lmdb_size\n\n    def _recarray(self, size):\n        return np.recarray(\n            size,\n            dtype=[\n                ('id', 'S32'),\n                ('code', np.uint64, self.code_size // 8)],\n            order='F')\n\n    def _initialize_in_memory_store(self):\n        if self.writeonly:\n            return\n\n        if self._codes is not None:\n            return\n        initial_size = max(int(1e6), len(self))\n        self._codes = Growable(self._recarray(initial_size))\n\n    def _np_code(self, code):\n        self._code_bytearray[:] = code\n        return self._code_buffer\n\n    def _validate_code_size(self, code):\n        code_len = len(code)\n        if code_len != self.code_size:\n            fmt = '''code must be equal to code_size\n                    ({self.code_size}), but was {code_len}'''\n            raise ValueError(fmt.format(**locals()))\n\n    def _add_code(self, _id, code):\n        if self.writeonly:\n            return\n\n        arr = self._append_buffer\n        arr[0]['id'] = _id\n        arr[0]['code'] = self._np_code(code)\n        self._codes.append(arr)\n        self._ids.add(_id)\n\n    def _check_for_external_modifications(self):\n        if self.__len__() != self._codes.logical_size:\n            self._catch_up_on_in_memory_store()\n\n    def _new_id(self):\n        return binascii.hexlify(os.urandom(16))\n\n    def append(self, code, data):\n        self._validate_code_size(code)\n        self._initialize_in_memory_store()\n\n        with self.env.begin(write=True) as txn:\n            _id = self._new_id()\n            try:\n                code = code.encode()\n            except AttributeError:\n                pass\n            try:\n                data = data.encode()\n            except AttributeError:\n                pass\n            txn.put(_id, code + data, db=self.index)\n            self._add_code(_id, code)\n\n    def _random_code(self):\n        with self.env.begin() as txn:\n            with txn.cursor(self.index) as cursor:\n                code = None\n                while not code:\n                    if cursor.set_range(self._new_id()):\n                        return txn.get(\n                            cursor.key(), db=self.index)[:self.code_size]\n                    continue\n\n    def random_search(self, n_results, multithreaded=False, sort=False):\n        code = self._random_code()\n        return code, self.search(code, n_results, multithreaded, sort=sort)\n\n    def search(self, code, n_results, multithreaded=False, sort=False):\n\n        if self.writeonly:\n            error_msg = 'searches may not be performed in writeonly mode'\n            raise RuntimeError(error_msg)\n\n        self._validate_code_size(code)\n        self._check_for_external_modifications()\n        query = self._np_code(code)\n\n        codes = self._codes.logical_data['code']\n\n        if codes.ndim == 1:\n            codes = codes[..., None]\n\n        if not multithreaded:\n            scores = packed_hamming_distance(query, codes)\n        else:\n            n_codes = len(codes)\n            chunksize = max(1, n_codes // self._thread_count)\n            scores = np.concatenate(self._pool.map(\n                lambda x: packed_hamming_distance(query, x),\n                (codes[i: i + chunksize] for i in\n                 range(0, n_codes, chunksize))))\n\n        # argpartition will ensure that the lowest scores will all be\n        # withing the first n_results elements, but makes no guarantees\n        # about the ordering *within* n_results\n        partitioned_indices = np.argpartition(scores, n_results)[:n_results]\n\n        if sort:\n            # since argpartition doesn't guarantee that the results are\n            # sorted *within* n_results, sort the much smaller result set\n            sorted_indices = np.argsort(scores[partitioned_indices])\n\n            indices = partitioned_indices[sorted_indices]\n        else:\n            # the partitioned indices are good enough.  results will all be\n            # within some degree of similarity, but not necessarily in any\n            # particular order\n            indices = partitioned_indices\n\n        nearest = self._codes.logical_data[indices]['id']\n\n        with self.env.begin() as txn:\n            for _id in nearest:\n                yield txn.get(_id, db=self.index)[self.code_size:]\n"""
zounds/index/index.py,5,"b'import ujson as json\nimport os\nimport threading\nimport numpy as np\nfrom .hammingdb import HammingDb\nfrom zounds.persistence import TimeSliceEncoder, TimeSliceDecoder\nfrom zounds.timeseries import ConstantRateTimeSeries\nfrom zounds.timeseries import TimeSlice\n\n\nclass SearchResults(object):\n    def __init__(self, query, results):\n        super(SearchResults, self).__init__()\n        self.query = query\n        self.results = results\n\n    def __iter__(self):\n        for result in self.results:\n            yield result\n\n\nclass HammingIndex(object):\n    def __init__(\n            self,\n            document,\n            feature,\n            version=None,\n            path=\'\',\n            db_size_bytes=1000000000,\n            listen=False,\n            writeonly=False,\n            **extra_data):\n\n        super(HammingIndex, self).__init__()\n        self.document = document\n        self.feature = feature\n        self.db_size_bytes = db_size_bytes\n        self.path = path\n        self.extra_data = extra_data\n        self.writeonly = writeonly\n\n        version = version or self.feature.version\n\n        self.hamming_db_path = os.path.join(\n            self.path, \'index.{self.feature.key}.{version}\'\n                .format(**locals()))\n\n        try:\n            self.event_log = document.event_log\n        except AttributeError:\n            self.event_log = None\n\n        try:\n            self.hamming_db = HammingDb(\n                self.hamming_db_path, code_size=None, writeonly=self.writeonly)\n        except ValueError:\n            self.hamming_db = None\n\n        self.encoder = TimeSliceEncoder()\n        self.decoder = TimeSliceDecoder()\n        self.thread = None\n\n        if listen:\n            self.listen()\n\n    def close(self):\n        try:\n            self.stop()\n        except:\n            pass\n\n        try:\n            self.hamming_db.close()\n        except:\n            pass\n\n    def __del__(self):\n        self.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def __len__(self):\n        if self.hamming_db is None:\n            return 0\n        return len(self.hamming_db)\n\n    def stop(self):\n        self.event_log.unsubscribe()\n\n    def listen(self):\n        self.thread = threading.Thread(target=self._listen)\n        self.thread.daemon = True\n        self.thread.start()\n\n    def _init_hamming_db(self, code=None):\n        if self.hamming_db is not None:\n            return\n        code_size = len(code) if code else None\n        self.hamming_db = HammingDb(\n            self.hamming_db_path, code_size=code_size, writeonly=self.writeonly)\n\n    def _synchronously_process_events(self):\n        self._listen(raise_when_empty=True)\n\n    def add_all(self):\n        for doc in self.document:\n            self.add(doc._id)\n\n    def _collect_extra_data(self, _id, ts):\n        if not self.extra_data:\n            return None\n\n        doc = self.document(_id)\n        return dict(\n            ((key, func(doc, ts)) for key, func in self.extra_data.items()))\n\n    def add(self, _id, timestamp=b\'\'):\n        # load the feature from the feature database\n        feature = self.feature(_id=_id, persistence=self.document)\n\n        try:\n            arr = ConstantRateTimeSeries(feature)\n        except ValueError:\n            arr = feature\n\n        # extract codes and timeslices from the feature\n        for ts, data in arr.iter_slices():\n            code = self.encode_query(data)\n            encoded_ts = dict(\n                _id=_id,\n                **self.encoder.dict(ts))\n            extra_data = self._collect_extra_data(_id, ts)\n            if extra_data:\n                encoded_ts[\'extra_data\'] = extra_data\n            self._init_hamming_db(code)\n            self.hamming_db.append(code, json.dumps(encoded_ts))\n            self.hamming_db.set_metadata(b\'timestamp\', timestamp)\n\n    def _listen(self, raise_when_empty=False):\n\n        if self.hamming_db is not None:\n            last_timestamp = self.hamming_db.get_metadata(\'timestamp\') or \'\'\n        else:\n            last_timestamp = \'\'\n\n        if not self.event_log:\n            raise ValueError(\n                \'{self.document} must have an event log configured\'\n                    .format(**locals()))\n\n        subscription = self.event_log.subscribe(\n            last_id=last_timestamp, raise_when_empty=raise_when_empty)\n\n        for timestamp, data in subscription:\n\n            # parse the data from the event stream\n            data = json.loads(data)\n            _id, name, version = data[\'_id\'], data[\'name\'], data[\'version\']\n\n            # ensure that it\'s about the feature we\'re subscribed to\n            if name != self.feature.key or version != self.feature.version:\n                continue\n\n            self.add(_id, timestamp)\n\n    def _parse_result(self, result):\n        d = json.loads(result)\n        ts = TimeSlice(**self.decoder.kwargs(d))\n\n        if not self.extra_data:\n            return d[\'_id\'], ts\n\n        return d[\'_id\'], ts, d[\'extra_data\']\n\n    def decode_query(self, binary_query):\n        packed = np.fromstring(binary_query, dtype=np.uint8)\n        return np.unpackbits(packed)\n\n    def encode_query(self, feature):\n        if isinstance(feature, bytes):\n            return feature\n        elif feature.dtype == np.uint64:\n            return feature.tostring()\n        elif feature.dtype == np.uint8 or feature.dtype == np.bool:\n            return np.packbits(feature).tostring()\n        else:\n            raise ValueError(\n                \'feature must be a raw bit string, an already packed uint64\'\n                \'array, or an ""unpacked"" uint8 or bool array\')\n\n    def random_search(self, n_results, multithreaded=False, sort=False):\n        self._init_hamming_db()\n        code, raw_results = self.hamming_db.random_search(\n            n_results, multithreaded, sort=sort)\n        parsed_results = (self._parse_result(r) for r in raw_results)\n        return SearchResults(code, parsed_results)\n\n    def search(self, feature, n_results, multithreaded=False, sort=False):\n        self._init_hamming_db()\n        code = self.encode_query(feature)\n        raw_results = self.hamming_db.search(\n            code, n_results, multithreaded, sort=sort)\n        parsed_results = (self._parse_result(r) for r in raw_results)\n        return SearchResults(code, parsed_results)\n'"
zounds/index/test_hammingdb.py,2,"b""import unittest2\nfrom .hammingdb import HammingDb\nfrom uuid import uuid4\nimport shutil\nimport numpy as np\nimport os\n\n\nclass HammingDbTests(unittest2.TestCase):\n    def setUp(self):\n        self._db_name = uuid4().hex\n        self._path = '/tmp/{path}'.format(path=self._db_name)\n\n        def extract_code_from_text(text, n_chunks=1):\n            n_bits = n_chunks * 64\n            code = np.zeros(n_bits, dtype=np.bool)\n            for i in range(len(text)):\n                trigram = text[i: i + 3]\n                hashed = hash(trigram)\n                code[hashed % n_bits] = 1\n            return np.packbits(code).tostring()\n\n        self.extract_code_from_text = extract_code_from_text\n\n    def tearDown(self):\n        try:\n            shutil.rmtree(self._path)\n        except OSError:\n            pass\n\n    def test_can_initialize_with_no_code_size_when_data_already_exists(self):\n        HammingDb(self._path, code_size=32)\n        db2 = HammingDb(self._path, code_size=None)\n        self.assertEqual(32, db2.code_size)\n\n    def test_raises_when_persisted_code_size_does_not_agree_with_init(self):\n        HammingDb(self._path, code_size=32)\n        self.assertRaises(\n            ValueError, lambda: HammingDb(self._path, code_size=16))\n\n    def test_raises_when_code_size_is_none_for_uninitialized_database(self):\n        self.assertRaises(\n            ValueError, lambda: HammingDb(self._path, code_size=None))\n\n    def test_metadata_fetch_does_not_raise_when_unitialized(self):\n        db = HammingDb(self._path, code_size=16)\n        v = db.get_metadata(b'cat')\n        self.assertEqual(None, v)\n\n    def test_can_set_and_get_metadata(self):\n        db = HammingDb(self._path, code_size=16)\n        db.set_metadata(b'cat', b'dog')\n        self.assertEqual(b'dog', db.get_metadata(b'cat'))\n\n    def test_can_get_random_entry(self):\n        db = HammingDb(self._path, code_size=16)\n        for i in range(100):\n            db.append(os.urandom(16), str(i))\n        code, results = db.random_search(10)\n        results = list(results)\n        self.assertEqual(10, len(results))\n\n    def test_can_create_database_with_128_bit_codes(self):\n        db = HammingDb(self._path, code_size=16)\n        self.assertEqual(0, len(db))\n\n    def test_can_append_with_128_bits(self):\n        db = HammingDb(self._path, code_size=16)\n        db.append('a' * 16, 'some data')\n        self.assertEqual(1, len(db))\n\n    def test_can_search_with_128_bits(self):\n        db = HammingDb(self._path, code_size=16)\n        t1 = b'Mary had a little lamb'\n        t2 = b'Mary had a little dog'\n        t3 = b'Permanent Midnight'\n        t4 = b'Mary sad a little cog'\n        extract_code = lambda x: self.extract_code_from_text(x, n_chunks=2)\n        db.append(extract_code(t1), t1)\n        db.append(extract_code(t2), t2)\n        db.append(extract_code(t3), t3)\n        db.append(extract_code(t4), t4)\n        results = list(db.search(extract_code(t1), 3))\n        self.assertEqual(3, len(results))\n        self.assertEqual(t1, results[0])\n        self.assertEqual(t2, results[1])\n        self.assertEqual(t4, results[2])\n\n    def test_can_run_in_write_only_mode(self):\n        db = HammingDb(self._path, code_size=16, writeonly=True)\n        t1 = 'Mary had a little lamb'\n        t2 = 'Mary had a little dog'\n        t3 = 'Permanent Midnight'\n        t4 = 'Mary sad a little cog'\n        extract_code = lambda x: self.extract_code_from_text(x, n_chunks=2)\n        db.append(extract_code(t1), t1)\n        db.append(extract_code(t2), t2)\n        db.append(extract_code(t3), t3)\n        db.append(extract_code(t4), t4)\n        self.assertIsNone(db._codes)\n\n    def test_search_raises_in_write_only_mode(self):\n        db = HammingDb(self._path, code_size=16, writeonly=True)\n        t1 = 'Mary had a little lamb'\n        t2 = 'Mary had a little dog'\n        t3 = 'Permanent Midnight'\n        t4 = 'Mary sad a little cog'\n        extract_code = lambda x: self.extract_code_from_text(x, n_chunks=2)\n        db.append(extract_code(t1), t1)\n        db.append(extract_code(t2), t2)\n        db.append(extract_code(t3), t3)\n        db.append(extract_code(t4), t4)\n        self.assertRaises(\n            RuntimeError, lambda: list(db.search(extract_code(t1), 3)))\n\n    def test_constructor_raises_when_not_multiple_of_eight(self):\n        self.assertRaises(\n            ValueError, lambda: HammingDb(self._path, code_size=7))\n\n    def test_empty_db_has_zero_length(self):\n        db = HammingDb(self._path, code_size=8)\n        self.assertEqual(0, len(db))\n\n    def test_db_has_length_one_after_appending(self):\n        db = HammingDb(self._path, code_size=8)\n        db.append('a' * 8, 'some data')\n        self.assertEqual(1, len(db))\n\n    def test_db_has_length_two_after_appending_twice(self):\n        db = HammingDb(self._path, code_size=8)\n        db.append('a' * 8, 'some data')\n        db.append('a' * 8, 'some data')\n        self.assertEqual(2, len(db))\n\n    @unittest2.skip('This test fails in CI tests, but it is soon to be removed')\n    def test_can_search_over_text_documents(self):\n        db = HammingDb(self._path, code_size=8)\n        t1 = b'Mary had a little lamb'\n        t2 = b'Mary had a little dog'\n        t3 = b'Permanent Midnight'\n        t4 = b'Mary sad a little cog'\n        db.append(self.extract_code_from_text(t1), t1)\n        db.append(self.extract_code_from_text(t2), t2)\n        db.append(self.extract_code_from_text(t3), t3)\n        db.append(self.extract_code_from_text(t4), t4)\n        results = list(db.search(self.extract_code_from_text(t1), 3))\n        self.assertEqual(3, len(results))\n        self.assertEqual(t1, results[0])\n        self.assertEqual(t2, results[1])\n        self.assertEqual(t4, results[2])\n\n    def test_can_search_over_data_added_from_another_instance(self):\n        db = HammingDb(self._path, code_size=8)\n        db2 = HammingDb(self._path, code_size=8)\n        t1 = b'Mary had a little lamb'\n        t2 = b'Mary had a little dog'\n        t3 = b'Permanent Midnight'\n        t4 = b'Mary sad a little cog'\n        db.append(self.extract_code_from_text(t1), t1)\n        db.append(self.extract_code_from_text(t2), t2)\n        db.append(self.extract_code_from_text(t3), t3)\n        db.append(self.extract_code_from_text(t4), t4)\n        results = list(db2.search(self.extract_code_from_text(t1), 3))\n        self.assertEqual(3, len(results))\n        s = set(results)\n        self.assertTrue(t1 in s)\n        self.assertTrue(t2 in s)\n        self.assertTrue(t4 in s)\n\n    def test_cannot_append_wrong_code_size(self):\n        db = HammingDb(self._path, code_size=8)\n        self.assertRaises(ValueError, lambda: db.append('a' * 7, 'some data'))\n\n    def test_cannot_search_for_wrong_code_size(self):\n        db = HammingDb(self._path, code_size=8)\n        self.assertRaises(ValueError, lambda: list(db.search('a' * 7, 10)))\n\n    def test_external_modifications_are_detected(self):\n        db = HammingDb(self._path, code_size=8)\n        db2 = HammingDb(self._path, code_size=8)\n        db2.append('a' * 8, 'some data')\n        self.assertEqual(1, len(db))\n\n    def test_external_modifications_are_detected_when_db_has_keys(self):\n        db = HammingDb(self._path, code_size=8)\n        db2 = HammingDb(self._path, code_size=8)\n        db.append('a' * 8, 'some other data')\n        db2.append('a' * 8, 'some data')\n        self.assertEqual(2, len(db))\n        self.assertEqual(2, len(db2))\n\n    def test_db_starts_with_correct_number_of_keys(self):\n        db2 = HammingDb(self._path, code_size=8)\n        db2.append('a' * 8, 'some data')\n        db = HammingDb(self._path, code_size=8)\n        self.assertEqual(1, len(db))\n\n    def test_can_retrieve_data_from_search(self):\n        db = HammingDb(self._path, code_size=8)\n        t1 = b'Mary had a little lamb'\n        t2 = b'Mary had a little dog'\n        t3 = b'Permanent Midnight'\n        t4 = b'Mary sad a little cog'\n        db.append(self.extract_code_from_text(t1), t1)\n        db.append(self.extract_code_from_text(t2), t2)\n        db.append(self.extract_code_from_text(t3), t3)\n        db.append(self.extract_code_from_text(t4), t4)\n        results = list(db.search(self.extract_code_from_text(t1), 3))\n        data = results[0]\n        self.assertEqual(t1, data)\n"""
zounds/index/test_hammingindex.py,1,"b""import unittest2\nfrom .index import HammingIndex\nfrom featureflow import \\\n    PersistenceSettings, UuidProvider, StringDelimitedKeyBuilder, \\\n    InMemoryDatabase, InMemoryChannel, EventLog\nfrom zounds.core import ArrayWithUnits, IdentityDimension\nfrom zounds.basic import stft, Slice, Binarize\nfrom zounds.timeseries import SR11025, Seconds\nfrom zounds.synthesize import SineSynthesizer\nfrom zounds.persistence import ArrayWithUnitsFeature\nfrom zounds.soundfile import AudioMetaData\nimport shutil\nfrom uuid import uuid4\nimport numpy as np\n\n\nclass HammingIndexTests(unittest2.TestCase):\n    def setUp(self):\n        self.event_log_path = '/tmp/{path}'.format(path=uuid4().hex)\n        self.hamming_db_path = '/tmp/{path}'.format(path=uuid4().hex)\n\n    def tearDown(self):\n        shutil.rmtree(self.event_log_path, ignore_errors=True)\n        shutil.rmtree(self.hamming_db_path, ignore_errors=True)\n\n    def test_listen_raises_if_model_class_has_no_event_log_configured(self):\n        Model = self._model(\n            slice_size=64,\n            settings=self._settings_with_no_event_log())\n\n        index = self._index(Model, Model.sliced)\n        signal = SineSynthesizer(SR11025()) \\\n            .synthesize(Seconds(5), [220, 440, 880])\n        Model.process(meta=signal.encode())\n        self.assertRaises(\n            ValueError, lambda: index._synchronously_process_events())\n\n    def test_correctly_infers_code_size_8(self):\n        Model = self._model(\n            slice_size=64,\n            settings=self._settings_with_event_log())\n\n        index = self._index(Model, Model.sliced)\n        signal = SineSynthesizer(SR11025()) \\\n            .synthesize(Seconds(5), [220, 440, 880])\n        Model.process(meta=signal.encode())\n        index._synchronously_process_events()\n        self.assertEqual(8, index.hamming_db.code_size)\n\n    def test_hamming_index_len_returns_even_when_underlying_db_uniitialized(self):\n        Model = self._model(\n            slice_size=128,\n            settings=self._settings_with_event_log())\n\n        index = self._index(Model, Model.sliced)\n        self.assertEqual(0, len(index))\n\n    def test_hamming_db_is_initialized_if_docs_exist(self):\n        Model = self._model(\n            slice_size=128,\n            settings=self._settings_with_event_log())\n\n        index = self._index(Model, Model.sliced)\n        signal = SineSynthesizer(SR11025()) \\\n            .synthesize(Seconds(5), [220, 440, 880])\n        Model.process(meta=signal.encode())\n        index._synchronously_process_events()\n\n        index2 = self._index(Model, Model.sliced)\n        self.assertIsNotNone(index2.hamming_db)\n        self.assertEqual(16, index2.hamming_db.code_size)\n\n    def test_correctly_infers_code_size_16(self):\n        Model = self._model(\n            slice_size=128,\n            settings=self._settings_with_event_log())\n\n        index = self._index(Model, Model.sliced)\n        signal = SineSynthesizer(SR11025()) \\\n            .synthesize(Seconds(5), [220, 440, 880])\n        Model.process(meta=signal.encode())\n        index._synchronously_process_events()\n        self.assertEqual(16, index.hamming_db.code_size)\n\n    def test_can_roundtrip_query(self):\n        Model = self._model(\n            slice_size=128,\n            settings=self._settings_with_event_log())\n\n        index = self._index(Model, Model.sliced)\n        signal = SineSynthesizer(SR11025()) \\\n            .synthesize(Seconds(5), [220, 440, 880])\n        Model.process(meta=signal.encode())\n        index._synchronously_process_events()\n\n        results = index.random_search(n_results=5)\n        decoded = index.decode_query(results.query)\n        encoded = index.encode_query(decoded)\n        self.assertEqual(results.query, encoded)\n\n    def test_can_search_with_binary_code(self):\n        Model = self._model(\n            slice_size=128,\n            settings=self._settings_with_event_log())\n\n        index = self._index(Model, Model.sliced)\n        signal = SineSynthesizer(SR11025()) \\\n            .synthesize(Seconds(5), [220, 440, 880])\n        Model.process(meta=signal.encode())\n        index._synchronously_process_events()\n\n        results = index.random_search(n_results=5)\n        decoded = index.decode_query(results.query)\n        encoded = index.encode_query(decoded)\n        results = index.search(encoded, 5)\n        self.assertEqual(5, len(list(results)))\n\n    def test_can_add_additional_data_to_index(self):\n        Model = self._model(\n            slice_size=128,\n            settings=self._settings_with_event_log())\n\n        index = self._index(\n            Model,\n            Model.sliced,\n            web_url=lambda doc, ts: doc.meta['web_url'])\n\n        signal = SineSynthesizer(SR11025()) \\\n            .synthesize(Seconds(5), [220, 440, 880])\n        meta = AudioMetaData(uri=signal.encode(), web_url='https://example.com')\n        _id = Model.process(meta=meta)\n        index._synchronously_process_events()\n\n        results = list(index.random_search(n_results=5))\n        result_id, ts, extra_data = results[0]\n        self.assertEqual(_id, result_id)\n        self.assertEqual('https://example.com', extra_data['web_url'])\n\n    def test_can_add_multiple_properties_to_index(self):\n        Model = self._model(\n            slice_size=128,\n            settings=self._settings_with_event_log())\n\n        index = self._index(\n            Model,\n            Model.sliced,\n            web_url=lambda doc, ts: doc.meta['web_url'],\n            total_duration=lambda doc, ts: doc.fft.dimensions[0].end / Seconds(\n                1))\n\n        signal = SineSynthesizer(SR11025()) \\\n            .synthesize(Seconds(5), [220, 440, 880])\n        meta = AudioMetaData(uri=signal.encode(), web_url='https://example.com')\n        _id = Model.process(meta=meta)\n        index._synchronously_process_events()\n\n        results = list(index.random_search(n_results=5))\n        result_id, ts, extra_data = results[0]\n        self.assertEqual(_id, result_id)\n        self.assertEqual('https://example.com', extra_data['web_url'])\n        self.assertAlmostEqual(5, extra_data['total_duration'], 1)\n\n    def correctly_infers_index_name(self):\n        Model = self._model(\n            slice_size=128,\n            settings=self._settings_with_event_log())\n\n        index = self._index(Model, Model.sliced)\n        signal = SineSynthesizer(SR11025()) \\\n            .synthesize(Seconds(5), [220, 440, 880])\n        Model.process(meta=signal.encode())\n        index._synchronously_process_events()\n        self.assertTrue('index.sliced' in index.hamming_db.path)\n\n    def test_can_add_already_packed_feature(self):\n        Model = self._model(\n            slice_size=128,\n            settings=self._settings_with_no_event_log())\n\n        index = self._index(Model, Model.packed)\n        signal = SineSynthesizer(SR11025()) \\\n            .synthesize(Seconds(5), [220, 440, 880])\n        _id = Model.process(meta=signal.encode())\n        model = Model(_id)\n        index.add(_id)\n        self.assertEqual(len(model.packed), len(index))\n\n    def _settings_with_no_event_log(self):\n        class Settings(PersistenceSettings):\n            id_provider = UuidProvider()\n            key_builder = StringDelimitedKeyBuilder()\n            database = InMemoryDatabase(key_builder=key_builder)\n\n        return Settings\n\n    def _settings_with_event_log(self):\n        class Settings(PersistenceSettings):\n            id_provider = UuidProvider()\n            key_builder = StringDelimitedKeyBuilder()\n            database = InMemoryDatabase(key_builder=key_builder)\n            event_log = EventLog(\n                path=self.event_log_path,\n                channel=InMemoryChannel())\n\n        return Settings\n\n    def _model(self, slice_size, settings):\n        STFT = stft(resample_to=SR11025(), store_fft=True)\n\n        def pack(x):\n            arr = np.zeros((len(x), 16), dtype=np.uint64)\n            return ArrayWithUnits(arr, [x.dimensions[0], IdentityDimension()])\n\n        class Model(STFT, settings):\n            binary = ArrayWithUnitsFeature(\n                Binarize,\n                predicate=lambda data: data >= 0,\n                needs=STFT.fft,\n                store=False)\n\n            sliced = ArrayWithUnitsFeature(\n                Slice,\n                sl=slice(0, slice_size),\n                needs=binary,\n                store=True)\n\n            packed = ArrayWithUnitsFeature(\n                pack,\n                needs=sliced,\n                store=True)\n\n        return Model\n\n    def _index(self, document, feature, **extra_data):\n        return HammingIndex(\n            document,\n            feature,\n            path=self.hamming_db_path,\n            **extra_data)\n"""
zounds/learn/__init__.py,0,"b'""""""\nThe `learn` module includes classes that make it possible to define processing\ngraphs whose leaves are trained machine learning models.\n\nWhile much of :mod:`zounds.soundfile`, :mod:`zounds.spectral`, and\n:mod:`zounds.timeseries` focus on processing nodes that can be composed into\na processing graph to extract features from a single piece of audio, the `learn`\nmodule focuses on defining graphs that extract features or trained models from\nan entire corpus of audio.\n""""""\n\nfrom .learn import KMeans, Learned\n\nfrom .preprocess import \\\n    MeanStdNormalization, UnitNorm, Log, PreprocessingPipeline, Multiply, \\\n    Slicer, InstanceScaling, Reshape, Weighted, MuLawCompressed, SimHash, \\\n    AbsoluteValue, Binarize, Sharpen, Pipeline, PreprocessResult, Preprocessor, \\\n    PipelineResult\n\nfrom .sklearn_preprocessor import SklearnModel, WithComponents\n\nfrom .pytorch_model import PyTorchAutoEncoder, PyTorchGan, PyTorchNetwork\n\nfrom .wgan import WassersteinGanTrainer\nfrom .supervised import SupervisedTrainer\nfrom .embedding import TripletEmbeddingTrainer\n\nfrom .random_samples import Reservoir, ReservoirSampler, ShuffledSamples\n\nfrom .util import simple_settings, object_store_pipeline_settings, model_hash, \\\n    batchwise_mean_std_normalization, batchwise_unit_norm\n\nfrom .graph import learning_pipeline, infinite_streaming_learning_pipeline\n\nfrom .functional import hyperplanes, simhash, example_wise_unit_norm\n\nfrom .sinclayer import SincLayer\n\nfrom .util import \\\n    Conv1d, ConvTranspose1d, Conv2d, ConvTranspose2d, to_var, from_var, \\\n    try_network, apply_network, feature_map_size, sample_norm, gradients\nfrom .gan_experiment import GanExperiment\nfrom .sample_embedding import RawSampleEmbedding\nfrom .dct_transform import DctTransform\nfrom .gated import GatedConvTransposeLayer, GatedConvLayer, GatedLinearLayer\nfrom .multiresolution import MultiResolutionConvLayer\nfrom .loss import PerceptualLoss, BandLoss, CategoricalLoss, \\\n    WassersteinCriticLoss, WassersteinGradientPenaltyLoss, \\\n    LearnedWassersteinLoss\nfrom .spectral import FilterBank\n\n\n'"
zounds/learn/dct_transform.py,0,"b'from zounds.spectral import dct_basis\nimport torch\nfrom torch.autograd import Variable\n\n\nclass DctTransform(object):\n    def __init__(self, use_cuda=False):\n        super(DctTransform, self).__init__()\n        self.use_cuda = use_cuda\n        self._init_caches()\n\n    def _init_caches(self):\n        self._basis_cache = dict()\n        self._window_cache = dict()\n\n    def _variable(self, x, *args, **kwargs):\n        v = Variable(x, *args, **kwargs)\n        if self.use_cuda:\n            v = v.cuda()\n        return v\n\n    def cuda(self):\n        self._init_caches()\n        self.use_cuda = True\n        return self\n\n    def dct_basis(self, n):\n        try:\n            return self._basis_cache[n]\n        except KeyError:\n            basis = torch.from_numpy(dct_basis(n)).float()\n            if self.use_cuda:\n                basis = basis.cuda()\n            self._basis_cache[n] = basis\n            return basis\n\n    def window(self, n, window):\n        try:\n            return self._window_cache[n]\n        except KeyError:\n            data = torch.from_numpy(window._wdata(n)).float()\n            if self.use_cuda:\n                data = data.cuda()\n            self._window_cache[n] = data\n            return data\n\n    def _base_dct_transform(self, x, basis, axis=-1):\n        n = torch.FloatTensor(1)\n        n[:] = 2. / x.shape[axis]\n        n = self._variable(n)\n        coeffs = torch.matmul(x, basis) * torch.sqrt(n)\n        return coeffs\n\n    def dct(self, x, axis=-1):\n        basis = self._variable(self.dct_basis(x.shape[axis]))\n        return self._base_dct_transform(x, basis, axis)\n\n    def idct(self, x, axis=-1):\n        basis = self._variable(self.dct_basis(x.shape[axis]))\n        return self._base_dct_transform(x, basis.t(), axis)\n\n    def dct_resample(self, x, factor, axis=-1, zero_slice=None):\n        # figure out how many samples our resampled signal will have\n        n_samples = int(factor * x.shape[axis])\n\n        coeffs = self.dct(x)\n\n        # create the shape of our new coefficients\n        new_coeffs_shape = list(coeffs.shape)\n        new_coeffs_shape[axis] = n_samples\n\n        # fill in the new, resampled coefficients\n        new_coeffs = self._variable(torch.zeros(*new_coeffs_shape))\n\n        new_coeffs_slices = [slice(None)] * x.dim()\n        new_coeffs_slices[axis] = slice(None, coeffs.shape[axis])\n        new_coeffs_slices = tuple(new_coeffs_slices)\n\n        old_coeffs_slices = [slice(None)] * x.dim()\n        old_coeffs_slices[axis] = slice(None, new_coeffs.shape[axis])\n        old_coeffs_slices = tuple(old_coeffs_slices)\n\n        new_coeffs[new_coeffs_slices] = coeffs[old_coeffs_slices]\n\n        if zero_slice is not None:\n            slce = [slice(None)] * x.dim()\n            slce[axis] = zero_slice\n            slce = tuple(slce)\n            new_coeffs[slce] = 0\n\n        return self.idct(new_coeffs)\n\n    def frequency_decomposition(self, x, factors, axis=-1):\n\n        full_size = x.shape[axis]\n        factors = sorted(factors)\n        factors = [0] + list(factors)\n        coeffs = self.dct(x, axis=axis)\n\n        bands = []\n        for i in range(0, len(factors) - 1):\n            start_index = int(factors[i] * full_size)\n            stop_index = int(factors[i + 1] * full_size)\n\n            new_shape = list(coeffs.shape)\n            new_shape[axis] = stop_index\n            new_coeffs = self._variable(torch.zeros(new_shape))\n\n            slce = [slice(None)] * x.dim()\n            slce[axis] = slice(start_index, stop_index)\n\n            new_coeffs[slce] = coeffs[slce]\n\n            resampled = self.idct(new_coeffs)\n            bands.append(resampled)\n\n        return bands\n\n    def short_time_dct(self, x, size, step, window):\n        original_shape = x.shape\n        x = x.unfold(-1, size, step)\n        window = self._variable(self.window(x.shape[-1], window))\n        x = x * window\n        x = self.dct(x, axis=-1)\n        x = x.view((original_shape[0], size, x.shape[2]))\n        return x\n'"
zounds/learn/embedding.py,3,"b'from .trainer import Trainer\nfrom random import choice\nimport numpy as np\nfrom torch import nn\nfrom torch.optim import Adam\nimport torch\nfrom .util import trainable_parameters, batchwise_unit_norm\n\n\nclass TripletEmbeddingTrainer(Trainer):\n    """"""\n    Learn an embedding by applying the triplet loss to anchor examples, negative\n    examples, and deformed or adjacent examples, akin to:\n\n    * `UNSUPERVISED LEARNING OF SEMANTIC AUDIO REPRESENTATIONS` <https://arxiv.org/pdf/1711.02209.pdf>\n\n    Args:\n        network (nn.Module): the neural network to train\n        epochs (int): the desired number of passes over the entire dataset\n        batch_size (int): the number of examples in each minibatch\n        anchor_slice (slice): since choosing examples near the anchor example\n            is one possible transformation that can be applied to find a positive\n            example, batches generally consist of examples that are longer\n            (temporally) than the examples that will be fed to the network, so\n            that adjacent examples may be chosen.  This slice indicates which\n            part of the minibatch examples comprises the anchor\n        deformations (callable): a collection of other deformations or\n            transformations that can be applied to anchor examples to derive\n            positive examples.  These callables should take two arguments: the\n            anchor examples from the minibatch, as well as the ""wider"" minibatch\n            examples that include temporally adjacent events\n    """"""\n\n    def __init__(\n            self,\n            network,\n            epochs,\n            batch_size,\n            anchor_slice,\n            deformations=None,\n            checkpoint_epochs=1):\n\n        super(TripletEmbeddingTrainer, self).__init__(\n            epochs, batch_size, checkpoint_epochs=checkpoint_epochs)\n        self.anchor_slice = anchor_slice\n        self.network = network\n        self.deformations = deformations\n\n        # The margin hyperparameter is set to 0.1 in, according to section 4.2\n        # of the paper https://arxiv.org/pdf/1711.02209.pdf\n        self.margin = 0.1\n        self.register_batch_complete_callback(self._log)\n        self.loss = nn.TripletMarginLoss(margin=self.margin)\n\n    def _cuda(self, device=None):\n        self.loss = self.loss.cuda()\n        self.network = self.network.cuda()\n\n    def _driver(self, data):\n        batches_in_epoch = len(data) // self.batch_size\n        start = self._current_epoch\n        stop = self._current_epoch + self.checkpoint_epochs\n        for epoch in range(start, stop):\n            if epoch > self.epochs:\n                break\n\n            for batch in range(batches_in_epoch):\n                yield epoch, batch\n\n            self._current_epoch += 1\n\n    def _apply_network_and_normalize(self, x):\n        """"""\n        Pass x through the network, and give the output unit norm, as specified\n        by section 4.2 of https://arxiv.org/pdf/1711.02209.pdf\n        """"""\n        x = self.network(x)\n        return batchwise_unit_norm(x)\n\n    def _select_batch(self, training_set):\n        indices = np.random.randint(0, len(training_set), self.batch_size)\n        batch = training_set[indices, self.anchor_slice]\n        return indices, batch.astype(np.float32)\n\n    def train(self, data):\n\n        data = data[\'data\']\n\n        self.network.train()\n\n        optimizer = Adam(trainable_parameters(self.network), lr=1e-5)\n\n        for epoch, batch in self._driver(data):\n            self.network.zero_grad()\n\n            # choose a batch of anchors\n            indices, anchor = self._select_batch(data)\n            anchor_v = self._variable(anchor)\n            a = self._apply_network_and_normalize(anchor_v)\n\n            # choose negative examples\n            negative_indices, negative = self._select_batch(data)\n            negative_v = self._variable(negative)\n            n = self._apply_network_and_normalize(negative_v)\n\n            # choose a deformation for this batch and apply it to produce the\n            # positive examples\n            deformation = choice(self.deformations)\n            positive = deformation(anchor, data[indices, ...]) \\\n                .astype(np.float32)\n            positive_v = self._variable(positive)\n            p = self._apply_network_and_normalize(positive_v)\n\n            error = self.loss.forward(a, p, n)\n            error.backward()\n            optimizer.step()\n\n            self.on_batch_complete(\n                epoch=epoch,\n                batch=batch,\n                error=float(error.data.cpu().numpy().squeeze()),\n                deformation=deformation.__name__)\n\n        return self.network\n\n    def _log(self, *args, **kwargs):\n        print(\'epoch {epoch}, batch {batch}, \' \\\n            \'error {error}, deformation {deformation}\'.format(**kwargs))\n'"
zounds/learn/functional.py,7,"b""import numpy as np\n\n\ndef hyperplanes(means, stds, n_planes):\n    if len(means) != len(stds):\n        raise ValueError('means and stds must have the same length')\n\n    n_features = len(means)\n    a = np.random.normal(means, stds, (n_planes, n_features))\n    b = np.random.normal(means, stds, (n_planes, n_features))\n    plane_vectors = a - b\n    return plane_vectors\n\n\ndef simhash(plane_vectors, data):\n    output = np.zeros((len(data), len(plane_vectors)), dtype=np.uint8)\n    flattened = data.reshape((len(data), -1))\n    x = np.dot(plane_vectors, flattened.T).T\n    output[np.where(x > 0)] = 1\n    return output\n\n\ndef example_wise_unit_norm(x, return_norms=False):\n    original_shape = x.shape\n\n    # flatten all dimensions of x, treating the first axis as examples and all\n    # other axes as features\n    x = x.reshape((len(x), -1))\n    norms = np.linalg.norm(x, axis=-1, keepdims=True)\n    normed = np.divide(x, norms, where=norms != 0)\n    normed = normed.reshape(original_shape)\n\n    if return_norms:\n        return normed, norms\n    else:\n        return normed\n"""
zounds/learn/gan_experiment.py,4,"b""import featureflow as ff\nfrom .util import from_var\nfrom random import choice\nfrom zounds.spectral import stft, rainbowgram\nfrom zounds.learn import \\\n    try_network, infinite_streaming_learning_pipeline, \\\n    object_store_pipeline_settings\nfrom zounds.timeseries import \\\n    SR11025, SampleRate, Seconds, AudioSamples, audio_sample_rate\nfrom .wgan import WassersteinGanTrainer\nfrom .pytorch_model import PyTorchGan\nfrom .preprocess import InstanceScaling\nfrom zounds.ui import GanTrainingMonitorApp\nfrom zounds.util import simple_lmdb_settings\nfrom zounds.basic import windowed\nfrom zounds.spectral import HanningWindowingFunc\nfrom zounds.datasets import ingest\nimport numpy as np\n\n\nclass GanExperiment(object):\n    def __init__(\n            self,\n            experiment_name,\n            dataset,\n            gan_pair,\n            object_storage_username,\n            object_storage_api_key,\n            sound_cls=None,\n            sound_feature=None,\n            epochs=500,\n            n_critic_iterations=10,\n            batch_size=32,\n            n_samples=int(5e5),\n            latent_dim=100,\n            real_sample_transformer=lambda x: x,\n            preprocess_minibatch=lambda x: x,\n            debug_gradients=False,\n            sample_size=8192,\n            sample_hop=1024,\n            samplerate=SR11025(),\n            app_port=8888,\n            object_storage_region='DFW',\n            app_secret=None):\n\n        super(GanExperiment, self).__init__()\n        self.real_sample_transformer = real_sample_transformer\n        self.debug_gradients = debug_gradients\n        self.n_samples = n_samples\n        self.batch_size = batch_size\n        self.n_critic_iterations = n_critic_iterations\n        self.epochs = epochs\n        self.gan_pair = gan_pair\n        self.app_port = app_port\n        self.dataset = dataset\n        self.preprocess_minibatch = preprocess_minibatch\n\n        self.samplerate = samplerate\n        self.sample_hop = sample_hop\n        self.sample_size = sample_size\n        self.latent_dim = latent_dim\n        self.experiment_name = experiment_name\n        self.app_secret = app_secret\n\n        if sound_cls:\n            self.sound_cls = sound_cls\n        else:\n            base_model = windowed(\n                resample_to=self.samplerate,\n                store_resampled=True,\n                wscheme=self.samplerate * (sample_hop, sample_size))\n\n            @simple_lmdb_settings(\n                experiment_name, map_size=1e11, user_supplied_id=True)\n            class Sound(base_model):\n                pass\n\n            self.sound_cls = Sound\n\n        self.sound_feature = sound_feature or self.sound_cls.windowed\n\n        @object_store_pipeline_settings(\n            'Gan-{experiment_name}'.format(**locals()),\n            object_storage_region,\n            object_storage_username,\n            object_storage_api_key)\n        @infinite_streaming_learning_pipeline\n        class Gan(ff.BaseModel):\n            scaled = ff.PickleFeature(\n                InstanceScaling)\n\n            wgan = ff.PickleFeature(\n                PyTorchGan,\n                trainer=ff.Var('trainer'),\n                needs=scaled)\n\n        self.gan_pipeline = Gan()\n        self.fake_samples = None\n        self.app = None\n\n    def batch_complete(self, *args, **kwargs):\n        samples = kwargs['samples']\n        self.fake_samples = from_var(samples).squeeze()\n\n    def fake_audio(self):\n        sample = choice(self.fake_samples)\n        sample = self.real_sample_transformer(sample)\n        return AudioSamples(sample, self.samplerate) \\\n            .pad_with_silence(Seconds(1))\n\n    def _stft(self, samples):\n        samples = samples / np.abs(samples.max())\n        wscheme = SampleRate(\n            frequency=samples.samplerate.frequency * 128,\n            duration=samples.samplerate.frequency * 256)\n        coeffs = stft(samples, wscheme, HanningWindowingFunc())\n        return rainbowgram(coeffs)\n\n    def fake_stft(self):\n        samples = self.fake_audio()\n        return self._stft(samples)\n\n    def real_stft(self):\n        snd = self.sound_cls.random()\n        windowed = choice(snd.windowed)\n        windowed = AudioSamples(\n            windowed,\n            audio_sample_rate(windowed.dimensions[0].samples_per_second))\n        return self._stft(windowed)\n\n    def test(self):\n        z = np.random.normal(\n            0, 1, (self.batch_size, self.latent_dim)).astype(np.float32)\n        samples = try_network(self.gan_pair.generator, z)\n        samples = from_var(samples)\n        print(samples.shape)\n        wasserstein_estimate = try_network(self.gan_pair.discriminator, samples)\n        print(wasserstein_estimate.shape)\n\n    def run(self):\n        ingest(self.dataset, self.sound_cls, multi_threaded=True)\n\n        experiment = self\n        fake_audio = self.fake_audio\n        fake_stft = self.fake_stft\n        real_stft = self.real_stft\n        Sound = self.sound_cls\n\n        try:\n            network = self.gan_pipeline.load_network()\n            print('initialized weights')\n        except RuntimeError as e:\n            print('Error', e)\n            network = self.gan_pair\n            for p in network.parameters():\n                p.data.normal_(0, 0.02)\n\n        trainer = WassersteinGanTrainer(\n            network,\n            latent_dimension=(self.latent_dim,),\n            n_critic_iterations=self.n_critic_iterations,\n            epochs=self.epochs,\n            batch_size=self.batch_size,\n            debug_gradient=self.debug_gradients,\n            preprocess_minibatch=self.preprocess_minibatch)\n        trainer.register_batch_complete_callback(self.batch_complete)\n\n        self.app = GanTrainingMonitorApp(\n            trainer=trainer,\n            model=Sound,\n            visualization_feature=Sound.windowed,\n            audio_feature=Sound.ogg,\n            globals=globals(),\n            locals=locals(),\n            secret=self.app_secret)\n\n        with self.app.start_in_thread(self.app_port):\n            self.gan_pipeline.process(\n                dataset=(Sound, self.sound_feature),\n                trainer=trainer,\n                nsamples=self.n_samples,\n                dtype=np.float32)\n\n        self.app.start(self.app_port)\n"""
zounds/learn/gated.py,0,"b'from torch import nn\nfrom torch.nn import functional as F\n\n\nclass GatedLinearLayer(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(GatedLinearLayer, self).__init__()\n        self.l1 = nn.Linear(in_channels, out_channels)\n        self.gate = nn.Linear(in_channels, out_channels)\n\n    def forward(self, x):\n        return F.tanh(self.l1(x)) * F.sigmoid(self.gate(x))\n\n\nclass GatedLayer(nn.Module):\n    def __init__(\n            self,\n            layer_type,\n            in_channels,\n            out_channels,\n            kernel,\n            stride=1,\n            padding=0,\n            dilation=1,\n            attention_func=F.sigmoid,\n            activation_func=F.tanh,\n            norm=lambda x: x):\n        super(GatedLayer, self).__init__()\n        self.activation_func = activation_func\n        self.norm = norm\n        self.conv = layer_type(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            bias=False)\n        self.gate = layer_type(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            bias=False)\n        self.attention_func = attention_func\n\n    def forward(self, x):\n        c = self.conv(x)\n        c = self.norm(c)\n        g = self.gate(x)\n        g = self.norm(g)\n        out = self.activation_func(c) * self.attention_func(g)\n        return out\n\n\nclass GatedConvLayer(GatedLayer):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel,\n            stride=1,\n            padding=0,\n            dilation=1,\n            attention_func=F.sigmoid,\n            activation_func=F.tanh,\n            norm=lambda x: x):\n        super(GatedConvLayer, self).__init__(\n            nn.Conv1d,\n            in_channels,\n            out_channels,\n            kernel,\n            stride,\n            padding,\n            dilation,\n            attention_func,\n            activation_func,\n            norm)\n\n\nclass GatedConvTransposeLayer(GatedLayer):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel,\n            stride=1,\n            padding=0,\n            dilation=1,\n            attention_func=F.sigmoid,\n            activation_func=F.tanh,\n            norm=lambda x: x):\n        super(GatedConvTransposeLayer, self).__init__(\n            nn.ConvTranspose1d,\n            in_channels,\n            out_channels,\n            kernel,\n            stride,\n            padding,\n            dilation,\n            attention_func,\n            activation_func,\n            norm)\n'"
zounds/learn/graph.py,0,"b""import featureflow as ff\nfrom .random_samples import ShuffledSamples\nfrom .random_samples import InfiniteSampler\nfrom .preprocess import PreprocessingPipeline\n\n\ndef learning_pipeline():\n    class LearningPipeline(ff.BaseModel):\n        samples = ff.PickleFeature(ff.IteratorNode)\n\n        shuffled = ff.PickleFeature(\n            ShuffledSamples,\n            nsamples=ff.Var('nsamples'),\n            dtype=ff.Var('dtype'),\n            needs=samples)\n\n    return LearningPipeline\n\n\ndef infinite_streaming_learning_pipeline(cls):\n    roots = [feature for feature in iter(cls.features.values()) if feature.is_root]\n\n    if len(roots) != 1:\n        raise ValueError('cls must have a single root feature')\n\n    root = roots[0]\n\n    class InfiniteLearningPipeline(cls):\n        dataset = ff.Feature(\n            InfiniteSampler,\n            nsamples=ff.Var('nsamples'),\n            dtype=ff.Var('dtype'),\n            feature_filter=ff.Var('feature_filter'),\n            parallel=ff.Var('parallel'))\n\n        pipeline = ff.ClobberPickleFeature(\n            PreprocessingPipeline,\n            needs=cls.features,\n            store=True)\n\n        @classmethod\n        def load_network(cls):\n            if not cls.exists():\n                raise RuntimeError('No network has been trained or saved')\n\n            instance = cls()\n            for p in instance.pipeline:\n                try:\n                    return p.network\n                except AttributeError:\n                    pass\n\n            raise RuntimeError('There is no network in the pipeline')\n\n    root.needs = InfiniteLearningPipeline.dataset\n    InfiniteLearningPipeline.__name__ = cls.__name__\n    InfiniteLearningPipeline.__module__ = cls.__module__\n\n    return InfiniteLearningPipeline\n"""
zounds/learn/learn.py,4,"b'from scipy.cluster.vq import kmeans\nfrom featureflow import Node\nfrom .preprocess import PreprocessResult, Preprocessor\n\n\nclass KMeans(Preprocessor):\n    """"""\n\n    """"""\n    def __init__(self, centroids=None, needs=None):\n        super(KMeans, self).__init__(needs=needs)\n        self._centroids = centroids\n\n    def _forward_func(self):\n        def x(d, codebook=None):\n            import numpy as np\n            from scipy.spatial.distance import cdist\n            from zounds.core import ArrayWithUnits, IdentityDimension\n            l = d.shape[0]\n            dist = cdist(d, codebook)\n            best = np.argmin(dist, axis=1)\n            feature = np.zeros((l, len(codebook)), dtype=np.uint8)\n            feature[np.arange(l), best] = 1\n            try:\n                return ArrayWithUnits(\n                    feature, [d.dimensions[0], IdentityDimension()])\n            except AttributeError:\n                return feature\n\n        return x\n\n    def _backward_func(self):\n        def x(d, codebook=None):\n            import numpy as np\n            indices = np.where(d == 1)\n            return codebook[indices[1]]\n\n        return x\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        codebook, _ = kmeans(data, self._centroids)\n        op = self.transform(codebook=codebook)\n        inv_data = self.inversion_data(codebook=codebook)\n        inv = self.inverse_transform()\n        data = op(data)\n        yield PreprocessResult(\n            data, op, inversion_data=inv_data, inverse=inv, name=\'KMeans\')\n\n\nclass Learned(Node):\n    """"""\n\n    """"""\n    def __init__(\n            self, \n            learned=None, \n            version=None, \n            wrapper=None,\n            pipeline_func=None,\n            needs=None,\n            dtype=None):\n        super(Learned, self).__init__(needs=needs)\n        self.dtype = dtype\n        self._pipeline_func = pipeline_func or (lambda x: x.pipeline)\n        self._wrapper = wrapper\n        self._learned = learned\n        self._version = version\n\n    @property\n    def version(self):\n        if self._version is not None:\n            return self._version\n\n        pipeline = self._pipeline_func(self._learned)\n        return pipeline.version\n\n    def _process(self, data):\n        pipeline = self._pipeline_func(self._learned)\n        if self.dtype:\n            data = data.astype(self.dtype)\n        transformed = pipeline.transform(data, wrapper=self._wrapper).data\n        yield transformed\n'"
zounds/learn/loss.py,0,"b'import torch\nfrom scipy.signal import gaussian\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\n\nfrom .dct_transform import DctTransform\nfrom zounds.spectral import fir_filter_bank\nfrom zounds.timeseries import SampleRate\n\n\nclass PerceptualLoss(nn.Module):\n    """"""\n    `PerceptualLoss` computes loss/distance in a feature space that roughly\n    approximates early stages of the human audio processing pipeline, instead\n    of computing raw sample loss.  It decomposes a 1D (audio) signal into\n    frequency bands using an FIR filter bank whose frequencies are centered\n    according to a user-defined scale, performs half-wave rectification, puts\n    amplitudes on a log scale, and finally optionally applies a re-weighting\n    of frequency bands.\n\n    Args:\n        scale (FrequencyScale): a scale defining frequencies at which the FIR\n            filters will be centered\n        samplerate (SampleRate): samplerate needed to construct the FIR filter\n            bank\n        frequency_window (ndarray): window determining how narrow or wide filter\n            responses should be\n        basis_size (int): The kernel size, or number of ""taps"" for each filter\n        lap (int): The filter stride\n        log_factor (int): How much compression should be applied in the log\n            amplitude stage\n        frequency_weighting (FrequencyWeighting): an optional frequency\n            weighting to be applied after log amplitude scaling\n        cosine_similarity (bool): If `True`, compute the cosine similarity\n            between spectrograms, otherwise, compute the mean squared error\n    """"""\n\n    def __init__(\n            self,\n            scale,\n            samplerate,\n            frequency_window=gaussian(100, 3),\n            basis_size=512,\n            lap=2,\n            log_factor=100,\n            frequency_weighting=None,\n            cosine_similarity=True):\n\n        super(PerceptualLoss, self).__init__()\n\n        self.cosine_similarity = cosine_similarity\n        self.log_factor = log_factor\n        self.scale = scale\n        basis_size = basis_size\n        self.lap = lap\n        self.basis_size = basis_size\n\n        basis = fir_filter_bank(\n            scale, basis_size, samplerate, frequency_window)\n\n        weights = torch.from_numpy(basis).float()\n        # out channels x in channels x kernel width\n        self.weights = weights.view(len(scale), 1, basis_size).contiguous()\n\n        self.frequency_weights = None\n        if frequency_weighting:\n            fw = frequency_weighting._wdata(self.scale)\n            self.frequency_weights = torch.from_numpy(fw)\\\n                .float().view(1, len(self.scale), 1)\n\n    def cuda(self, device=None):\n        self.weights = self.weights.cuda(device=device)\n        if self.frequency_weights is not None:\n            self.frequency_weights = self.frequency_weights.cuda(device=device)\n        return super(PerceptualLoss, self).cuda(device=device)\n\n    def to(self, device=None):\n        self.weights = self.weights.to(device)\n        if self.frequency_weights is not None:\n            self.frequency_weights = self.frequency_weights.to(device=device)\n        return super(PerceptualLoss, self).to(device=device)\n\n    def _transform(self, x):\n        x = x.view(x.shape[0], 1, -1)\n\n        # frequency decomposition\n        features = F.conv1d(\n            x, self.weights, stride=self.lap, padding=self.basis_size)\n\n        # half-wave rectification\n        features = F.relu(features)\n\n        # log magnitude\n        features = torch.log(1 + features * self.log_factor)\n\n        # perceptual frequency weighting\n        if self.frequency_weights is not None:\n            features = features * self.frequency_weights\n\n        return features\n\n    def forward(self, input, target):\n        input = input.view(input.shape[0], 1, -1)\n        target = target.view(input.shape[0], 1, -1)\n\n        input_features = self._transform(input).view(input.shape[0], -1)\n        target_features = self._transform(target).view(input.shape[0], -1)\n\n        if self.cosine_similarity:\n            spectral_error = \\\n                -(F.cosine_similarity(input_features, target_features).mean())\n            return spectral_error\n        else:\n            return ((input_features - target_features) ** 2).mean()\n\n\nclass BandLoss(nn.MSELoss):\n    def __init__(self, factors, spectral_shape_weight=1):\n        super(BandLoss, self).__init__()\n        self.spectral_shape_weight = spectral_shape_weight\n        self.factors = factors\n        self.dct_transform = DctTransform()\n\n    def cuda(self, device=None):\n        self.dct_transform = DctTransform(use_cuda=True)\n        return super(BandLoss, self).cuda(device=device)\n\n    def _transform(self, x):\n        bands = self.dct_transform.frequency_decomposition(\n            x, self.factors, axis=-1)\n\n        norms = [torch.norm(b, dim=-1, keepdim=True) for b in bands]\n        bands = [b / (n + 1e-8) for (b, n) in zip(bands, norms)]\n        fine = torch.cat(bands, dim=-1)\n\n        coarse = torch.cat(norms, dim=-1)\n        coarse_norms = torch.norm(coarse, dim=-1, keepdim=True)\n        coarse = coarse / (coarse_norms + 1e-8)\n\n        return fine, coarse\n\n    def forward(self, input, target):\n        input_bands, input_coarse = self._transform(input)\n        target_bands, target_coarse = self._transform(target)\n        fine = super(BandLoss, self).forward(input_bands, target_bands)\n        coarse = super(BandLoss, self).forward(input_coarse, target_coarse)\n        return fine + (coarse * self.spectral_shape_weight)\n\n\nclass CategoricalLoss(object):\n    def __init__(self, n_categories):\n        super(CategoricalLoss, self).__init__()\n        self.n_categories = n_categories\n        self.use_cuda = False\n        self.loss = nn.NLLLoss2d()\n\n    def cuda(self, device=None):\n        self.use_cuda = True\n        self.loss = self.loss.cuda(device=device)\n        return self\n\n    def _variable(self, x, *args, **kwargs):\n        v = Variable(x, *args, **kwargs)\n        if self.use_cuda:\n            v = v.cuda()\n        return v\n\n    def _mu_law(self, x):\n        m = self._variable(torch.FloatTensor(1))\n        m[:] = self.n_categories + 1\n        s = torch.sign(x)\n        x = torch.abs(x)\n        x = s * (torch.log(1 + (self.n_categories * x)) / torch.log(m))\n        return x\n\n    def _shift_and_scale(self, x):\n        x = x + 1\n        x = x * ((self.n_categories) / 2.)\n        return x\n\n    def _one_hot(self, x):\n        y = self._variable(torch.arange(0, self.n_categories + 1))\n        x = -(((x[..., None] - y) ** 2) * 1e12)\n        x = F.log_softmax(x, dim=-1)\n        return x\n\n    def _discretized(self, x):\n        x = x.view(-1)\n        x = x / torch.abs(x).max()\n        x = self._mu_law(x)\n        x = self._shift_and_scale(x)\n        return x\n\n    def _categorical(self, x):\n        x = self._discretized(x)\n        x = self._one_hot(x)\n        return x\n\n    def forward(self, input, target):\n\n        if input.shape[1] == self.n_categories + 1:\n            categorical = input\n        else:\n            categorical = self._categorical(input)\n\n        discretized = self._discretized(target)\n        inp = categorical.view(\n            -1, self.n_categories + 1, 2, input.shape[-1] // 2)\n        t = discretized.view(-1, 2, target.shape[-1] // 2).long()\n        error = self.loss(inp, t)\n        return error\n\n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\n\nclass BaseLoss(object):\n    def __init__(self):\n        super(BaseLoss, self).__init__()\n        self.use_cuda = False\n\n    def _cuda(self, device=None):\n        raise NotImplementedError()\n\n    def cuda(self, device=None):\n        self.use_cuda = True\n        self._cuda(device=device)\n        return self\n\n    def forward(self, *args, **kwargs):\n        raise NotImplementedError()\n\n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\n\nclass LearnedWassersteinLoss(BaseLoss):\n    def __init__(self, critic):\n        super(LearnedWassersteinLoss, self).__init__()\n        self.critic = critic\n\n    def _cuda(self, device=None):\n        self.critic.cuda(device=device)\n\n    def forward(self, x, **critic_kwargs):\n        w = self.critic(x, **critic_kwargs)\n        return -torch.mean(w)\n\n\nclass WassersteinCriticLoss(BaseLoss):\n    def __init__(self, critic):\n        super(WassersteinCriticLoss, self).__init__()\n        self.critic = critic\n\n    def _cuda(self, device=None):\n        self.critic.cuda(device=device)\n\n    def forward(self, real, fake, **critic_kwargs):\n        d_real = self.critic(real, **critic_kwargs)\n        d_fake = self.critic(fake, **critic_kwargs)\n        real_mean = torch.mean(d_real)\n        fake_mean = torch.mean(d_fake)\n        return fake_mean - real_mean\n\n\nclass WassersteinGradientPenaltyLoss(BaseLoss):\n    def __init__(self, critic, weight=10):\n        super(WassersteinGradientPenaltyLoss, self).__init__()\n        self.weight = weight\n        self.critic = critic\n\n    def _cuda(self, device=None):\n        self.critic.cuda(device=device)\n\n    def forward(self, real_samples, fake_samples, **critic_kwargs):\n        from torch.autograd import grad\n\n        real_samples = real_samples.view(fake_samples.shape)\n\n        subset_size = real_samples.shape[0]\n\n        real_samples = real_samples[:subset_size]\n        fake_samples = fake_samples[:subset_size]\n\n        alpha = torch.rand(subset_size)\n        if self.use_cuda:\n            alpha = alpha.cuda()\n        alpha = alpha.view((-1,) + ((1,) * (real_samples.dim() - 1)))\n\n        interpolates = alpha * real_samples + ((1 - alpha) * fake_samples)\n        if self.use_cuda:\n            interpolates = interpolates.cuda()\n        interpolates = Variable(interpolates, requires_grad=True)\n\n        d_output = self.critic(interpolates, **critic_kwargs)\n\n        output = torch.ones(d_output.size())\n        if self.use_cuda:\n            output = output.cuda()\n\n        gradients = grad(\n            outputs=d_output,\n            inputs=interpolates,\n            grad_outputs=output,\n            create_graph=True,\n            retain_graph=True,\n            only_inputs=True)[0]\n        return ((gradients.norm(2, dim=1) - 1) ** 2).mean() * self.weight\n'"
zounds/learn/multiresolution.py,0,"b'from torch import nn\nfrom .gated import GatedConvLayer\n\n\nclass MultiResolutionBlock(nn.Module):\n    """"""\n    A layer that convolves several different filter/kernel sizes with the same\n    input features\n    """"""\n\n    def __init__(\n            self,\n            layer,\n            in_channels,\n            out_channels,\n            kernel_sizes,\n            stride=1,\n            padding=None,\n            **kwargs):\n        super(MultiResolutionBlock, self).__init__()\n\n        layers = [\n            layer(\n                in_channels,\n                out_channels,\n                k,\n                stride,\n                padding=k // 2 if padding is None else padding,\n                **kwargs)\n            for k in kernel_sizes]\n\n        self.main = nn.Sequential(*layers)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        results = []\n        for m in self.main:\n            r = m(x)\n            results.append(r)\n        return results\n\n\nclass MultiResolutionConvLayer(MultiResolutionBlock):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_sizes,\n            stride=1,\n            padding=None,\n            **kwargs):\n\n        super(MultiResolutionConvLayer, self).__init__(\n            GatedConvLayer,\n            in_channels,\n            out_channels,\n            kernel_sizes,\n            stride,\n            padding,\n            **kwargs)'"
zounds/learn/preprocess.py,9,"b'from featureflow import Node, NotEnoughData\nimport marshal\nimport types\nfrom collections import OrderedDict\nimport hashlib\nimport numpy as np\nfrom .functional import hyperplanes\nfrom zounds.loudness import log_modulus, inverse_log_modulus\n\n\nclass Op(object):\n    def __init__(self, func, **kwargs):\n        super(Op, self).__init__()\n        self._kwargs = kwargs\n        try:\n            self._func = marshal.dumps(func.__code__)\n        except AttributeError:\n            # func is already a marshalled function\n            self._func = func\n        self._version = self._compute_version()\n\n    @property\n    def version(self):\n        return self._version\n\n    @property\n    def kwargs(self):\n        return self._kwargs\n\n    def _compute_version(self):\n        h = hashlib.md5(self._func)\n        for v in self._kwargs.values():\n            try:\n                h.update(v.version)\n            except AttributeError:\n                h.update(np.array(v))\n        return h.hexdigest()\n\n    def __getattr__(self, key):\n        if key == \'_kwargs\':\n            raise AttributeError()\n\n        try:\n            return self._kwargs[key]\n        except KeyError:\n            raise AttributeError(key)\n\n    def __call__(self, arg, **kwargs):\n        code = marshal.loads(self._func)\n        f = types.FunctionType(\n            code,\n            globals(),\n            \'preprocess\')\n        kwargs.update(self._kwargs)\n        return f(arg, **kwargs)\n\n\nclass PreprocessResult(object):\n    """"""\n    `PreprocessResult` are the output of :class:`Preprocessor` nodes, and can\n    participate in a `Pipeline`.\n\n    Args:\n        data: the data on which the node in the graph was originally trained\n        op (Op): a callable that can transform data\n        inversion_data: data extracted in the forward pass of the model, that\n            can be used to invert the result\n        inverse (Op): a callable that given the output of `op`, and\n            `inversion_data`, can invert the result\n    """"""\n\n    def __init__(self, data, op, inversion_data=None, inverse=None, name=None):\n        super(PreprocessResult, self).__init__()\n        self.name = name\n        self.inverse = inverse\n        self.inversion_data = inversion_data\n        self.data = data\n        self.op = op\n\n    def __str__(self):\n        return \'PreprocessResult(name={name})\'.format(**self.__dict__)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __getattr__(self, key):\n        if key == \'op\':\n            raise AttributeError()\n        return getattr(self.op, key)\n\n    def for_storage(self):\n        return PreprocessResult(\n            None,\n            self.op,\n            self.inversion_data,\n            self.inverse,\n            self.name)\n\n\nclass Preprocessor(Node):\n    """"""\n    `Preprocessor` is the common base class for nodes in a processing graph that\n    will produce :class:`PreprocessingResult` instances that end up as part of\n    a :class:`Pipeline`.\n\n    Args:\n        needs (Node): previous processing node(s) on which this one depends\n            for its data\n\n    See Also:\n        :class:`PreprocessResult`\n        :class:`PreprocessingPipeline`\n        :class:`PipelineResult`\n    """"""\n\n    def __init__(self, needs=None):\n        super(Preprocessor, self).__init__(needs=needs)\n\n    def _forward_func(self):\n        """"""\n        Return a function that represents this processor\'s\n        forward transform\n        """"""\n\n        def x(data, example_arg=None, another_one=None):\n            return data\n\n        return x\n\n    def transform(self, **kwargs):\n        return Op(self._forward_func(), **kwargs)\n\n    def _inversion_data(self):\n        """"""\n        Return a function that computes any data needed for\n        an inverse transform, if one is possible.  Otherwise,\n        raise NotImplemented\n        """"""\n\n        def x(data, **kwargs):\n            return kwargs\n\n        return x\n\n    def inversion_data(self, **kwargs):\n        return Op(self._inversion_data(), **kwargs)\n\n    def _backward_func(self):\n        """"""\n        Return a function that computes this processor\'s\n        inverse transform, if one is possible.  Otherwise,\n        raise NotImplemented\n        """"""\n\n        def x(data, **inversion_args):\n            return data\n\n        return x\n\n    def inverse_transform(self):\n        return Op(self._backward_func())\n\n    def _extract_data(self, data):\n        if isinstance(data, PreprocessResult):\n            return data.data\n        elif isinstance(data, dict):\n            return dict((k, self._extract_data(v)) for k, v in data.items())\n        else:\n            return data\n\n\nclass UnitNorm(Preprocessor):\n    def __init__(self, needs=None):\n        super(UnitNorm, self).__init__(needs=needs)\n\n    def _forward_func(self):\n        def x(d):\n            from zounds.core import ArrayWithUnits\n            from .functional import example_wise_unit_norm\n            normed, norms = example_wise_unit_norm(d, return_norms=True)\n            try:\n                normed = ArrayWithUnits(normed, d.dimensions)\n            except AttributeError:\n                pass\n\n            return ForwardResult(normed, norm=norms)\n\n        return x\n\n    def _backward_func(self):\n        def x(d, norm=None):\n            dim_diff = d.ndim - norm.ndim\n            new_dims = (1,) * dim_diff\n            return d * norm.reshape(norm.shape + new_dims)\n\n        return x\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        op = self.transform()\n        inv_data = self.inversion_data()\n        inv = self.inverse_transform()\n        try:\n            forward_result = op(data)\n            data = forward_result.output_value\n        except AttributeError:\n            data = None\n        yield PreprocessResult(\n            data, op, inversion_data=inv_data, inverse=inv, name=\'UnitNorm\')\n\n\nclass BasePreprocessor(Preprocessor):\n    def __init__(self, forward=None, backward=None, name=None, needs=None):\n        super(BasePreprocessor, self).__init__(needs=needs)\n\n        def not_implemented():\n            raise NotImplementedError()\n\n        self.forward = forward\n        self.backward = backward or not_implemented\n        self.name = name\n\n    def _forward_func(self):\n        return self.forward\n\n    def _backward_func(self):\n        return self.backward\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        op = self.transform()\n        inv_data = self.inversion_data()\n        inv = self.inverse_transform()\n        data = op(data)\n        yield PreprocessResult(\n            data, op, inversion_data=inv_data, inverse=inv, name=self.name)\n\n\n# TODO: what about functions with imports that aren\'t in the calling namespace?\n# TODO: what about functions that require inversion data?\n# TODO: what about functions that need to do some processing first (e.g. SimHash)\nclass Log(BasePreprocessor):\n    def __init__(self, needs=None):\n        super(Log, self).__init__(\n            forward=log_modulus,\n            backward=inverse_log_modulus,\n            name=\'Log\',\n            needs=needs)\n\n\nclass MuLawCompressed(Preprocessor):\n    def __init__(self, needs=None):\n        super(MuLawCompressed, self).__init__(needs=needs)\n\n    def _forward_func(self):\n        def x(d):\n            from zounds.loudness import mu_law\n            return mu_law(d)\n\n        return x\n\n    def _backward_func(self):\n        def x(d):\n            from zounds.loudness import inverse_mu_law\n            return inverse_mu_law(d)\n\n        return x\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        op = self.transform()\n        inv_data = self.inversion_data()\n        inv = self.inverse_transform()\n        data = op(data)\n        yield PreprocessResult(\n            data,\n            op,\n            inversion_data=inv_data,\n            inverse=inv,\n            name=\'MuLawCompressed\')\n\n\nclass Slicer(Preprocessor):\n    def __init__(self, slicex=None, needs=None):\n        super(Slicer, self).__init__(needs=needs)\n        self.fill_func = np.zeros\n        self.slicex = slicex\n\n    def _forward_func(self):\n        def x(d, slicex=None):\n            return d[..., slicex]\n\n        return x\n\n    def _inversion_data(self):\n        def y(d, slicex=None, fill_func=None):\n\n            try:\n                ka = d.kwargs()\n\n                def ff(shape, dtype):\n                    return d.__class__(fill_func(shape, dtype), **ka)\n            except AttributeError:\n\n                def ff(shape, dtype):\n                    return fill_func(shape, dtype)\n\n            return dict(shape=d.shape, slicex=slicex, fill_func=ff)\n\n        return y\n\n    def _backward_func(self):\n        def z(d, shape=None, fill_func=None, slicex=None):\n            new_shape = d.shape[:1] + shape[1:]\n            new_arr = fill_func(new_shape, d.dtype)\n            new_arr[..., slicex] = d\n            return new_arr\n\n        return z\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        op = self.transform(slicex=self.slicex)\n        inv_data = self.inversion_data(\n            fill_func=self.fill_func, slicex=self.slicex)\n        inv = self.inverse_transform()\n        data = op(data)\n        yield PreprocessResult(\n            data, op, inversion_data=inv_data, inverse=inv, name=\'Slicer\')\n\n\nclass Reshape(Preprocessor):\n    def __init__(self, new_shape, needs=None):\n        super(Reshape, self).__init__(needs=needs)\n        self.new_shape = new_shape\n\n    def _forward_func(self):\n        def x(d, new_shape=None):\n            return d.reshape(d.shape[:1] + new_shape)\n\n        return x\n\n    def _inversion_data(self):\n        def x(d):\n            return dict(original_shape=d.shape)\n\n        return x\n\n    def _backward_func(self):\n        def x(d, original_shape=None):\n            return d.reshape(original_shape)\n\n        return x\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        op = self.transform(new_shape=self.new_shape)\n        inv_data = self.inversion_data()\n        inv = self.inverse_transform()\n        data = op(data)\n        yield PreprocessResult(\n            data, op, inversion_data=inv_data, inverse=inv, name=\'Reshape\')\n\n\nclass SimHash(Preprocessor):\n    """"""\n    Hash feature vectors by computing on which side of N hyperplanes those\n    features lie.\n\n    Args:\n        bits (int): The number of hyperplanes, and hence, the number of bits\n            in the resulting hash\n        packbits (bool): Should the result be bit-packed?\n        needs (Preprocessor): the processing node on which this node relies for\n            its data\n    """"""\n\n    def __init__(self, bits=None, packbits=False, needs=None):\n        super(SimHash, self).__init__(needs=needs)\n        self.packbits = packbits\n        self.bits = bits\n\n    def _forward_func(self):\n        def x(d, plane_vectors=None, packbits=None):\n\n            from zounds.core import ArrayWithUnits, IdentityDimension\n            from zounds.learn import simhash\n            import numpy as np\n\n            bits = simhash(plane_vectors, d)\n\n            if packbits:\n                bits = np.packbits(bits, axis=-1).view(np.uint64)\n\n            try:\n                return ArrayWithUnits(\n                    bits, [d.dimensions[0], IdentityDimension()])\n            except AttributeError:\n                return bits\n\n        return x\n\n    def _backward_func(self):\n        def x(d):\n            raise NotImplementedError()\n\n        return x\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        mean = data.mean(axis=0).flatten()\n        std = data.std(axis=0).flatten()\n        plane_vectors = hyperplanes(mean, std, self.bits)\n        op = self.transform(plane_vectors=plane_vectors, packbits=self.packbits)\n        inv_data = self.inversion_data()\n        inv = self.inverse_transform()\n        data = op(data)\n        yield PreprocessResult(\n            data, op, inversion_data=inv_data, inverse=inv, name=\'SimHash\')\n\n\nclass MeanStdNormalization(Preprocessor):\n    def __init__(self, needs=None):\n        super(MeanStdNormalization, self).__init__(needs=needs)\n\n    def _forward_func(self):\n        def x(d, mean=None, std=None):\n            import numpy as np\n            return np.divide(d - mean, std, where=std != 0)\n\n        return x\n\n    def _backward_func(self):\n        def x(d, mean=None, std=None):\n            arr = d.copy()\n            arr *= std\n            arr += mean\n            return arr\n\n        return x\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        mean = data.mean(axis=0)\n        std = data.std(axis=0)\n        op = self.transform(mean=mean, std=std)\n        inv_data = self.inversion_data(mean=mean, std=std)\n        inv = self.inverse_transform()\n        data = op(data)\n        yield PreprocessResult(\n            data, op, inversion_data=inv_data, inverse=inv, name=\'MeanStd\')\n\n\nclass ForwardResult(object):\n    def __init__(self, output_value, **inversion_data):\n        self.output_value = output_value\n        self.inversion_data = inversion_data\n\n\nclass InstanceScaling(Preprocessor):\n    def __init__(self, max_value=1, needs=None):\n        super(InstanceScaling, self).__init__(needs=needs)\n        self.max_value = max_value\n\n    def _forward_func(self):\n        def x(d, max_value=None):\n            import numpy as np\n            axes = tuple(range(1, len(d.shape)))\n            # TODO: it should be possible to hand this intermediate value off\n            # to inversion data, and to ignore the extra\n            m = np.max(np.abs(d), axis=axes, keepdims=True)\n            output = max_value * np.divide(d, m, where=m != 0)\n            return ForwardResult(output, max=m, max_value=max_value)\n\n        return x\n\n    def _backward_func(self):\n        def x(d, max=None, max_value=None):\n            return (d * max) / max_value\n\n        return x\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        op = self.transform(max_value=self.max_value)\n        inv_data = self.inversion_data()\n        inv = self.inverse_transform()\n        forward_result = op(data)\n        data = forward_result.output_value\n        yield PreprocessResult(\n            data,\n            op,\n            inversion_data=inv_data,\n            inverse=inv,\n            name=\'InstanceScaling\')\n\n\nclass Multiply(Preprocessor):\n    def __init__(self, factor=1, needs=None):\n        super(Multiply, self).__init__(needs=needs)\n        self.factor = factor\n\n    def _forward_func(self):\n        def x(d, factor=None):\n            return d * factor\n\n        return x\n\n    def _backward_func(self):\n        def x(d, factor=None):\n            return d * (1.0 / factor)\n\n        return x\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        op = self.transform(factor=self.factor)\n        inv_data = self.inversion_data(factor=self.factor)\n        inv = self.inverse_transform()\n        data = op(data)\n        yield PreprocessResult(\n            data, op, inversion_data=inv_data, inverse=inv, name=\'Multiply\')\n\n\nclass Weighted(Preprocessor):\n    def __init__(self, weighting, needs=None):\n        super(Weighted, self).__init__(needs=needs)\n        self.weighting = weighting\n\n    def _forward_func(self):\n        def x(d, weighting=None):\n            return d * weighting\n\n        return x\n\n    def _backward_func(self):\n        def x(d, weighting=None):\n            return d / weighting\n\n        return x\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        op = self.transform(weighting=self.weighting)\n        inv_data = self.inversion_data(weighting=self.weighting)\n        inv = self.inverse_transform()\n        data = op(data)\n        yield PreprocessResult(\n            data, op, inversion_data=inv_data, inverse=inv, name=\'Weighted\')\n\n\nclass AbsoluteValue(Preprocessor):\n    def __init__(self, needs=None):\n        super(AbsoluteValue, self).__init__(needs=needs)\n\n    def _forward_func(self):\n        def x(d):\n            import numpy as np\n            from zounds.core import ArrayWithUnits\n            processed = np.abs(d)\n            try:\n                return ArrayWithUnits(processed, d.dimensions)\n            except AttributeError:\n                return processed\n\n        return x\n\n    def _backward_func(self):\n        def x(d):\n            raise NotImplementedError()\n\n        return x\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        op = self.transform()\n        inv_data = self.inversion_data()\n        inv = self.inverse_transform()\n\n        try:\n            data = op(data)\n        except:\n            data = None\n\n        yield PreprocessResult(\n            data, op, inversion_data=inv_data, inverse=inv,\n            name=\'AbsoluteValue\')\n\n\nclass Sharpen(Preprocessor):\n    def __init__(self, kernel=None, needs=None):\n        super(Sharpen, self).__init__(needs=needs)\n        self.kernel = kernel\n\n    def _forward_func(self):\n        def x(d, kernel=None):\n            from scipy.signal import convolve\n            from zounds.core import ArrayWithUnits\n            data = convolve(d, kernel[None, ...], mode=\'same\')\n            data[data < 0] = 0\n            try:\n                return ArrayWithUnits(data, d.dimensions)\n            except AttributeError:\n                return data\n\n        return x\n\n    def _backward_func(self):\n        def x(d):\n            raise NotImplementedError()\n\n        return x\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        op = self.transform(kernel=self.kernel)\n        inv_data = self.inversion_data()\n        inv = self.inverse_transform()\n        try:\n            data = op(data)\n        except:\n            data = None\n        yield PreprocessResult(\n            data, op, inversion_data=inv_data, inverse=inv, name=\'Sharpen\')\n\n\nclass Binarize(Preprocessor):\n    def __init__(self, threshold=0.5, needs=None):\n        super(Binarize, self).__init__(needs=needs)\n        self.threshold = threshold\n\n    def _forward_func(self):\n        def x(d, threshold=None):\n            import numpy as np\n            from zounds.core import ArrayWithUnits\n            data = np.zeros(d.shape, dtype=np.uint8)\n            data[np.where(d > threshold)] = 1\n            try:\n                return ArrayWithUnits(data, d.dimensions)\n            except AttributeError:\n                return data\n\n        return x\n\n    def _backward_func(self):\n        def x(d):\n            raise NotImplementedError()\n\n        return x\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        op = self.transform(threshold=self.threshold)\n        inv_data = self.inversion_data()\n        inv = self.inverse_transform()\n        try:\n            data = op(data)\n        except:\n            data = None\n        yield PreprocessResult(\n            data, op, inversion_data=inv_data, inverse=inv, name=\'Binarize\')\n\n\nclass Pipeline(object):\n    """"""\n\n    """"""\n\n    def __init__(self, preprocess_results):\n        self.processors = list(preprocess_results)\n        versions = \'\'.join([p.op.version for p in self.processors])\n        self.version = hashlib.md5(versions.encode()).hexdigest()\n\n    def __getitem__(self, index):\n        if isinstance(index, int):\n            return self.processors[index]\n        elif isinstance(index, list):\n            return Pipeline([self.processors[i] for i in index])\n        return Pipeline(self.processors[index])\n\n    def wrap_data(self, data):\n        cls = data.__class__\n        try:\n            kwargs = data.kwargs()\n        except AttributeError:\n            kwargs = None\n        return cls, kwargs\n\n    def transform(self, data, wrapper=None):\n        inversion_data = []\n        wrap_data = []\n        for p in self.processors:\n            wrap_data.append(self.wrap_data(data))\n            processed_data = p.op(data)\n\n            if isinstance(processed_data, ForwardResult):\n                inv_data = processed_data.inversion_data\n                processed_data = processed_data.output_value\n            else:\n                inv_data = p.inversion_data(data)\n\n            inversion_data.append(inv_data)\n            data = processed_data\n\n        if wrapper is not None:\n            data = wrapper(data)\n\n        return PipelineResult(data, self.processors, inversion_data, wrap_data)\n\n\nclass PipelineResult(object):\n    def __init__(self, data, processors, inversion_data, wrap_data):\n        super(PipelineResult, self).__init__()\n        self.processors = processors[::-1]\n        self.inversion_data = inversion_data[::-1]\n        self.wrap_data = wrap_data[::-1]\n        self.data = data\n\n    def unwrap(self, data, wrap_data):\n        cls, kwargs = wrap_data\n        if kwargs is None:\n            return data\n        return cls(data, **kwargs)\n\n    def inverse_transform(self):\n        data = self.data\n        for inv_data, wrap_data, p in \\\n                zip(self.inversion_data, self.wrap_data, self.processors):\n            data = p.inverse(data, **inv_data)\n            data = self.unwrap(data, wrap_data)\n        return data\n\n\nclass PreprocessingPipeline(Node):\n    """"""\n    A `PreprocessingPipeline` is a node in the graph that can be connected to\n    one or more :class:`Preprocessor` nodes, whose output it will assemble into\n    a re-usable pipeline.\n\n    Args:\n        needs (list or tuple of Node): the :class:`Preprocessor` nodes on whose\n            output this pipeline depends\n\n    Here\'s an example of a learning pipeline that will first find the\n    feature-wise mean and standard deviation of a dataset, and will then learn\n    K-Means clusters from the dataset.  This will result in a re-usable pipeline\n    that can use statistics from the original dataset to normalize new examples,\n    assign them to a cluster, and finally, reconstruct them.\n\n    .. code:: python\n\n        import featureflow as ff\n        import zounds\n        from random import choice\n\n        samplerate = zounds.SR44100()\n        STFT = zounds.stft(resample_to=samplerate)\n\n\n        @zounds.simple_in_memory_settings\n        class Sound(STFT):\n            bark = zounds.ArrayWithUnitsFeature(\n                zounds.BarkBands,\n                samplerate=samplerate,\n                needs=STFT.fft,\n                store=True)\n\n\n        @zounds.simple_in_memory_settings\n        class ExamplePipeline(ff.BaseModel):\n            docs = ff.PickleFeature(\n                ff.IteratorNode,\n                needs=None)\n\n            shuffled = ff.PickleFeature(\n                zounds.ShuffledSamples,\n                nsamples=100,\n                needs=docs,\n                store=False)\n\n            meanstd = ff.PickleFeature(\n                zounds.MeanStdNormalization,\n                needs=docs,\n                store=False)\n\n            kmeans = ff.PickleFeature(\n                zounds.KMeans,\n                needs=meanstd,\n                centroids=32)\n\n            pipeline = ff.PickleFeature(\n                zounds.PreprocessingPipeline,\n                needs=(meanstd, kmeans),\n                store=True)\n\n        # apply the Sound processing graph to individual audio files\n        for metadata in zounds.InternetArchive(\'TheR.H.SFXLibrary\'):\n            print \'processing {url}\'.format(url=metadata.request.url)\n            Sound.process(meta=metadata)\n\n        # apply the ExamplePipeline processing graph to the entire corpus of audio\n        _id = ExamplePipeline.process(docs=(snd.bark for snd in Sound))\n        learned = ExamplePipeline(_id)\n\n        snd = choice(list(Sound))\n        result = learned.pipeline.transform(snd.bark)\n        print result.data  # print the assigned centroids for each FFT frame\n        inverted = result.inverse_transform()\n        print inverted  # the reconstructed FFT frames\n\n\n    See Also:\n        :class:`Pipeline`\n        :class:`Preprocessor`\n        :class:`PreprocessResult`\n        :class:`PipelineResult`\n    """"""\n\n    def __init__(self, needs=None):\n        super(PreprocessingPipeline, self).__init__(needs=needs)\n        self._init_pipeline()\n\n    def _init_pipeline(self):\n        self._pipeline = OrderedDict((id(n), None) for n in list(self.needs.values()))\n\n    def _enqueue(self, data, pusher):\n        self._pipeline[id(pusher)] = data\n\n    def _dequeue(self):\n        if not all(self._pipeline.values()):\n            raise NotEnoughData()\n\n        pipeline = Pipeline([x.for_storage() for x in iter(self._pipeline.values())])\n\n        self._init_pipeline()\n        return pipeline\n'"
zounds/learn/pytorch_model.py,3,"b'import warnings\nimport featureflow as ff\nfrom .preprocess import Preprocessor, PreprocessResult, Op\nfrom zounds.persistence.util import extract_init_args\nimport torch\n\n\nclass PyTorchPreprocessResult(PreprocessResult):\n    def __init__(self, data, op, inversion_data=None, inverse=None, name=None):\n        super(PyTorchPreprocessResult, self).__init__(\n            data, op, inversion_data, inverse, name)\n\n    def __getstate__(self):\n        """"""\n        Extract serializable state from an instance\n        """"""\n        forward_func = self.op._func\n        inv_data_func = self.inversion_data._func\n        backward_func = self.inverse._func\n        network_params = self.op.network.state_dict()\n        weights = dict(\n            ((k, v.cpu().numpy()) for k, v in network_params.items()))\n        cls = self.op.network.__class__\n        name = self.name\n        init_args = extract_init_args(self.op.network)\n\n        kwargs = dict(self.op.kwargs)\n        del kwargs[\'network\']\n\n        return dict(\n            forward_func=forward_func,\n            op_kwargs=kwargs,\n            inv_data_func=inv_data_func,\n            backward_func=backward_func,\n            weights=weights,\n            name=name,\n            cls=cls,\n            init_args=init_args)\n\n    def __setstate__(self, state):\n        """"""\n        Re-hydrate an instance from serialized state\n        """"""\n\n        restored_weights = dict(\n            ((k, torch.from_numpy(v))\n             for k, v in state[\'weights\'].items()))\n        init_args = state[\'init_args\']\n        network = state[\'cls\'](*init_args)\n        network.load_state_dict(restored_weights)\n\n        # KLUDGE: Should we *ever* implicitly move things to the GPU?  If not,\n        # this would need to be done explicitly by the user when re-hydrating\n        # the learning pipeline\n        if torch.cuda.is_available():\n            network = network.cuda()\n\n        network.eval()\n\n        self.op = Op(\n            state[\'forward_func\'],\n            network=network,\n            **state[\'op_kwargs\'])\n\n        self.inversion_data = Op(state[\'inv_data_func\'], network=network)\n        self.inverse = Op(state[\'backward_func\'])\n        self.name = state[\'name\']\n\n    def for_storage(self):\n        return PyTorchPreprocessResult(\n            None,\n            self.op,\n            self.inversion_data,\n            self.inverse,\n            self.name)\n\n\nclass PyTorchNetwork(Preprocessor):\n    def __init__(\n            self,\n            trainer=None,\n            post_training_func=None,\n            needs=None,\n            training_set_prep=None,\n            chunksize=None):\n\n        super(PyTorchNetwork, self).__init__(needs=needs)\n        self.trainer = trainer\n        self.post_training_func = post_training_func or (lambda x: x)\n        self._cache = dict()\n        self.training_set_prep = training_set_prep\n        self.chunksize = chunksize\n\n    def _forward_func(self):\n\n        def x(d, network=None, chunk_size=None):\n            from zounds.core import ArrayWithUnits, IdentityDimension\n            from zounds.learn import apply_network\n\n            result = apply_network(network, d, chunksize=chunk_size)\n            try:\n                return ArrayWithUnits(\n                    result, [d.dimensions[0], IdentityDimension()])\n            except (AttributeError, ValueError):\n                return result\n\n        return x\n\n    def _backward_func(self):\n        def x(_):\n            raise NotImplementedError()\n\n        return x\n\n    def _enqueue(self, data, pusher):\n        if self._cache is None:\n            self._cache = dict()\n        k = self._dependency_name(pusher)\n        self._cache[k] = data\n\n    def _dequeue(self):\n\n        if self._cache is None:\n            raise ff.NotEnoughData()\n\n        if isinstance(self._cache, dict) \\\n                and len(self._cache) != len(self.needs):\n            raise ff.NotEnoughData()\n\n        data = self._cache\n        self._cache = None\n        return data\n\n    def _train(self, data):\n        trained_network = self.trainer.train(data)\n        trained_network.zero_grad()\n        trained_network.eval()\n        return trained_network\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        if self.training_set_prep:\n            data = self.training_set_prep(data)\n\n        trained_network = self._train(data)\n\n        chunksize = self.chunksize or self.trainer.batch_size\n\n        try:\n            forward_func = self._forward_func()\n            x = self.post_training_func(data[\'data\'])\n            processed_data = forward_func(\n                x, network=trained_network, chunk_size=chunksize)\n        except RuntimeError as e:\n            processed_data = None\n            # the dataset may be too large to fit onto the GPU all at once\n            warnings.warn(e.message)\n\n        op = self.transform(\n            network=trained_network, chunk_size=self.trainer.batch_size)\n        inv_data = self.inversion_data()\n        inv = self.inverse_transform()\n\n        yield PyTorchPreprocessResult(\n            processed_data,\n            op,\n            inversion_data=inv_data,\n            inverse=inv,\n            name=\'PyTorchNetwork\')\n\n\nclass PyTorchGan(PyTorchNetwork):\n    def __init__(self, apply_network=\'generator\', trainer=None, needs=None):\n        super(PyTorchGan, self).__init__(trainer=trainer, needs=needs)\n\n        if apply_network not in (\'generator\', \'discriminator\'):\n            raise ValueError(\n                \'apply_network must be one of (generator, discriminator)\')\n\n        self.apply_network = apply_network\n        self._cache = None\n\n    def _forward_func(self):\n        def x(d, network=None, apply_network=None):\n            from zounds.core import ArrayWithUnits, IdentityDimension\n            from zounds.learn import apply_network as apply\n            import numpy as np\n\n            if apply_network == \'generator\':\n                n = network.generator\n            else:\n                n = network.discriminator\n\n            result = apply(n, d.astype(np.float32), chunksize=128)\n\n            try:\n                return ArrayWithUnits(\n                    result, d.dimensions[:-1] + (IdentityDimension(),))\n            except AttributeError:\n                return result\n            except ValueError:\n                # the number of dimensions has likely changed\n                return result\n\n        return x\n\n    def _backward_func(self):\n        def x(_):\n            raise NotImplementedError()\n\n        return x\n\n    def _enqueue(self, data, pusher):\n        self._cache = data\n\n    def _process(self, data):\n        data = self._extract_data(data)\n\n        network = self._train(data)\n\n        try:\n            # note that the processed data passed on to the next step in the\n            # training pipeline will be the labels output by the discriminator\n            forward_func = self._forward_func()\n            processed_data = forward_func(\n                data, network=network, apply_network=\'discriminator\')\n        except RuntimeError as e:\n            processed_data = None\n            # the dataset may be too large to fit onto the GPU all at once\n            warnings.warn(e.message)\n\n        op = self.transform(network=network, apply_network=self.apply_network)\n        inv_data = self.inversion_data()\n        inv = self.inverse_transform()\n\n        yield PyTorchPreprocessResult(\n            processed_data,\n            op,\n            inversion_data=inv_data,\n            inverse=inv,\n            name=\'PyTorchGan\')\n\n\nclass PyTorchAutoEncoder(PyTorchNetwork):\n    def __init__(self, trainer=None, needs=None):\n        super(PyTorchAutoEncoder, self).__init__(trainer=trainer, needs=needs)\n        self._cache = None\n\n    def _forward_func(self):\n        def x(d, network=None):\n            from zounds.core import ArrayWithUnits, IdentityDimension\n            from zounds.learn import apply_network\n            import numpy as np\n\n            encoded = apply_network(\n                network.encoder, d.astype(np.float32), chunksize=128)\n\n            try:\n                extra_dims = (IdentityDimension(),) * (encoded.ndim - 1)\n                return ArrayWithUnits(\n                    encoded, d.dimensions[:1] + extra_dims)\n            except AttributeError:\n                return encoded\n\n        return x\n\n    def _backward_func(self):\n        def x(d, network=None):\n            from zounds.learn import apply_network\n            import numpy as np\n            return apply_network(\n                network.decoder, d.astype(np.float32), chunksize=128)\n\n        return x\n\n    def _enqueue(self, data, pusher):\n        self._cache = data\n\n    def _process(self, data):\n        data = self._extract_data(data)\n\n        data = dict(data=data, labels=data)\n\n        trained_network = self._train(data)\n\n        processed_data = None\n        inp = data[\'data\']\n\n        while processed_data is None:\n            try:\n                forward_func = self._forward_func()\n                processed_data = forward_func(inp, network=trained_network)\n            except RuntimeError as e:\n                processed_data = None\n                warnings.warn(e.message)\n                # we\'ve just experienced an out of memory exception.  Cut the\n                # size of the input data in half, so that downstream nodes that\n                # need some data to initialize themselves can do so\n                inp = inp[:len(inp) // 64]\n            except ValueError:\n                break\n\n        op = self.transform(network=trained_network)\n        inv_data = self.inversion_data(network=trained_network)\n        inv = self.inverse_transform()\n\n        yield PyTorchPreprocessResult(\n            processed_data,\n            op,\n            inversion_data=inv_data,\n            inverse=inv,\n            name=\'PyTorchAutoEncoder\')\n'"
zounds/learn/random_samples.py,9,"b'from featureflow import Node, NotEnoughData\nfrom zounds.core import ArrayWithUnits, IdentityDimension\nimport numpy as np\nfrom multiprocessing.pool import ThreadPool\nfrom os import cpu_count\n\n\nclass Reservoir(object):\n    def __init__(self, nsamples, dtype=None):\n        super(Reservoir, self).__init__()\n\n        if not isinstance(nsamples, int):\n            raise ValueError(\'nsamples must be an integer\')\n\n        if nsamples <= 0:\n            raise ValueError(\'nsamples must be greater than zero\')\n\n        self.nsamples = nsamples\n        self.arr = None\n        self.indices = set()\n        self.dtype = dtype\n\n    def __len__(self):\n        return len(self.indices)\n\n    def percent_full(self):\n        return float(len(self)) / self.nsamples\n\n    def _init_arr(self, samples):\n        if self.arr is not None:\n            return\n\n        shape = (self.nsamples,) + samples.shape[1:]\n        self.arr = np.zeros(shape, dtype=self.dtype or samples.dtype)\n\n        try:\n            self.arr = ArrayWithUnits(\n                self.arr, (IdentityDimension(),) + samples.dimensions[1:])\n        except AttributeError:\n            pass\n\n    def add(self, samples, indices=None):\n        self._init_arr(samples)\n\n        if indices is None:\n            indices = np.random.randint(0, self.nsamples, len(samples))\n\n        if len(indices) != len(samples):\n            raise ValueError(\n                \'number of input samples and indices must match\'\n                \' but they were {samples} and {indices} respectively\'\n                    .format(samples=len(samples), indices=len(indices)))\n\n        self.arr[indices, ...] = samples\n        self.indices.update(indices)\n\n    def get(self):\n        if len(self.indices) == self.nsamples:\n            return self.arr\n\n        x = self.arr[sorted(self.indices), ...]\n        return x\n\n    def get_batch(self, batch_size):\n        if batch_size > self.nsamples:\n            raise ValueError(\n                \'Requested {batch_size} samples, but this instance can provide \'\n                \'at maximum {nsamples}\'\n                    .format(batch_size=batch_size, nsamples=self.nsamples))\n\n        if batch_size > len(self.indices):\n            raise ValueError(\n                \'Requested {batch_size} samples, but this instance only \'\n                \'currently has {n} samples, with a maximum of {nsamples}\'\n                    .format(\n                    batch_size=batch_size,\n                    n=len(self.indices),\n                    nsamples=self.nsamples))\n\n        # TODO: this would be much more efficient for repeated calls if I\n        # instead maintained a sorted set\n        indices = np.random.choice(list(self.indices), batch_size)\n        return self.arr[indices, ...]\n\n\nclass MultiplexedReservoir(object):\n    def __init__(self, nsamples, dtype=None):\n        super(MultiplexedReservoir, self).__init__()\n        self.dtype = dtype\n        self.reservoir = None\n        self.nsamples = nsamples\n\n    def _init_dict(self, samples):\n        if self.reservoir is not None:\n            return\n\n        if self.reservoir is None:\n            self.reservoir = dict(\n                (k, Reservoir(self.nsamples, dtype=self.dtype))\n                for k in samples.keys())\n\n    def _check_sample_keys(self, samples):\n        if set(self.reservoir.keys()) != set(samples.keys()):\n            raise ValueError(\n                \'samples should have keys {keys}\'\n                    .format(keys=list(self.reservoir.keys())))\n\n    def add(self, samples):\n        self._init_dict(samples)\n        self._check_sample_keys(samples)\n\n        indices = None\n        for k, v in samples.items():\n            if indices is None:\n                indices = np.random.randint(0, self.nsamples, len(v))\n\n            self.reservoir[k].add(v, indices=indices)\n\n    def get(self):\n        return dict((k, v.get()) for k, v in self.reservoir.items())\n\n\nclass ShuffledSamples(Node):\n    def __init__(\n            self,\n            nsamples=None,\n            multiplexed=False,\n            dtype=None,\n            needs=None):\n        super(ShuffledSamples, self).__init__(needs=needs)\n        self.reservoir = MultiplexedReservoir(nsamples, dtype=dtype) \\\n            if multiplexed else Reservoir(nsamples, dtype=dtype)\n\n    def _enqueue(self, data, pusher):\n        self.reservoir.add(data)\n\n    def _dequeue(self):\n        if not self._finalized:\n            raise NotEnoughData()\n        return self.reservoir.get()\n\n\nclass InfiniteSampler(Node):\n    def __init__(\n            self,\n            nsamples=None,\n            multiplexed=False,\n            dtype=None,\n            needs=None,\n            feature_filter=lambda x: x,\n            parallel=True):\n\n        super(InfiniteSampler, self).__init__(needs=needs)\n        self.parallel = parallel\n        self.feature_filter = feature_filter\n        self.multiplexed = multiplexed\n        self.reservoir = MultiplexedReservoir(nsamples, dtype=dtype) \\\n            if multiplexed else Reservoir(nsamples, dtype=dtype)\n\n    def _total_samples(self, cls, feature, _ids):\n        pool = ThreadPool(cpu_count())\n\n        feature_filter = self.feature_filter\n\n        def x(_id):\n            f = feature(_id=_id, persistence=cls)\n            filtered = feature_filter(f)\n            return len(filtered)\n\n        if self.parallel:\n            total_samples = sum(pool.imap_unordered(x, _ids))\n        else:\n            total_samples = sum(map(x, _ids))\n        return total_samples\n\n    def _update_reservoir(self, _id, cls, feature, total_samples):\n        # fetch the features from a single document\n        x = feature(_id=_id, persistence=cls)\n        x = self.feature_filter(x)\n\n        # compute the contribution this sample makes to the dataset at\n        # large\n        feature_size = len(x)\n        ratio = float(feature_size) / total_samples\n\n        # determine the appropriate number of samples to contribute to\n        # the reservoir\n        nsamples = max(1, int(self.reservoir.nsamples * ratio))\n\n        print(\'Contributing\', feature_size, ratio, nsamples)\n\n        # select an appropriately-sized and random subset of the feature.\n        # this will be shuffled again as it is added to the reservoir,\n        # but this ensures that samples are drawn evenly from the\n        # duration of the sound\n        indices = np.random.randint(0, feature_size, nsamples)\n\n        self.reservoir.add(x[indices, ...])\n        return len(indices)\n\n    def _process(self, data):\n        cls, feature = data\n\n        # compute the total number of samples in our dataset\n        _ids = list(cls.database.iter_ids())\n        total_samples = self._total_samples(cls, feature, _ids)\n        print(\'Total samples\', total_samples)\n\n        while True:\n            if self.parallel:\n                pool = ThreadPool(cpu_count())\n                list(pool.imap_unordered(\n                    lambda _id: self._update_reservoir(\n                        _id, cls, feature, total_samples),\n                    _ids))\n            else:\n                for _id in _ids:\n                    self._update_reservoir(_id, cls, feature, total_samples)\n\n            yield self.reservoir.get()\n\n\nclass ReservoirSampler(Node):\n    """"""\n    Use reservoir sampling (http://en.wikipedia.org/wiki/Reservoir_sampling) to\n    draw a fixed-size set of random samples from a stream of unknown size.\n\n    This is useful when the samples can fit into memory, but the stream cannot.\n    """"""\n\n    def __init__(self, nsamples=None, wrapper=None, needs=None):\n        super(ReservoirSampler, self).__init__(needs=needs)\n        if wrapper:\n            raise DeprecationWarning(\'wrapper is no longer used or needed\')\n        self._nsamples = int(nsamples)\n        self._r = None\n        self._index = 0\n\n    # TODO: What happens if we have filled up all the sample slots and we run\n    # out of data?\n    def _enqueue(self, data, pusher):\n        if self._r is None:\n            shape = (self._nsamples,) + data.shape[1:]\n            self._r = np.zeros(shape, dtype=data.dtype)\n            try:\n                self._r = ArrayWithUnits(\n                    self._r, (IdentityDimension(),) + data.dimensions[1:])\n            except AttributeError:\n                # samples were likely a plain numpy array, and not an\n                # ArrayWithUnits instance\n                pass\n\n        diff = 0\n        if self._index < self._nsamples:\n            diff = self._nsamples - self._index\n            available = len(data[:diff])\n            self._r[self._index: self._index + available] = data[:diff]\n            self._index += available\n\n        remaining = len(data[diff:])\n        if not remaining:\n            return\n        indices = np.random.random_integers(0, self._index, size=remaining)\n        indices = indices[indices < self._nsamples]\n        self._r[indices, ...] = data[diff:][list(range(len(indices)))]\n        self._index += remaining\n\n    def _dequeue(self):\n        if not self._finalized:\n            raise NotEnoughData()\n\n        if self._index <= self._nsamples:\n            arr = np.asarray(self._r[:self._index])\n            np.random.shuffle(arr)\n            if isinstance(self._r, ArrayWithUnits):\n                arr = ArrayWithUnits(arr, self._r.dimensions)\n            return arr\n\n        return self._r\n'"
zounds/learn/sample_embedding.py,0,"b'import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n\nclass RawSampleEmbedding(nn.Module):\n    """"""\n    Embed raw audio samples after quantizing them and applying a\n    softmax/categorical distribution\n    """"""\n\n    def __init__(self, n_categories, embedding_dim, use_cuda=False):\n        super(RawSampleEmbedding, self).__init__()\n        self.use_cuda = use_cuda\n        self.n_categories = n_categories\n        self.embedding_dim = embedding_dim\n        self.linear = nn.Linear(\n            self.n_categories, self.embedding_dim)\n\n    def _variable(self, x, *args, **kwargs):\n        v = Variable(x, *args, **kwargs)\n        if self.use_cuda:\n            v = v.cuda()\n        return v\n\n    def _mu_law(self, x):\n        m = self._variable(torch.FloatTensor(1))\n        m[:] = self.n_categories + 1\n        s = torch.sign(x)\n        x = torch.abs(x)\n        x = s * (torch.log(1 + (self.n_categories * x)) / torch.log(m))\n        return x\n\n    def _shift_and_scale(self, x):\n        x = x + 1\n        x = x * ((self.n_categories + 1) / 2.)\n        return x\n\n    def _one_hot(self, x):\n        y = self._variable(torch.arange(0, self.n_categories + 1))\n        x = -(((x[..., None] - y) ** 2) * 1e12)\n        x = F.softmax(x, dim=-1)\n        return x\n\n    def categorical(self, x):\n        x = x.view(-1)\n        x = self._mu_law(x)\n        x = self._shift_and_scale(x)\n        x = self._one_hot(x)\n        return x\n\n    def forward(self, x):\n        sample_size = x.shape[-1]\n\n        # one-hot encode the continuous samples\n        x = self.categorical(x)\n\n        # embed the categorical variables into a\n        # dense vector\n        x = self.linear(x)\n\n        # place all embeddings on the unit sphere\n        norms = torch.norm(x, dim=-1)\n        x = x / norms.view(-1, 1)\n        x = x.view(-1, self.embedding_dim, sample_size)\n        return x\n'"
zounds/learn/sinclayer.py,0,"b'from torch import nn\nimport torch\nimport math\nfrom torch.nn import functional as F\n\n\nclass SincLayer(nn.Module):\n    """"""\n    A layer as described in the paper\n    ""Speaker Recognition from raw waveform with SincNet""\n\n    .. epigraph::\n\n        This paper proposes a novel CNN architecture, called SincNet, that\n        encourages the first convolutional layer to discover more meaningful\n        filters. SincNet is based on parametrized sinc functions, which\n        implement band-pass filters. In contrast to standard CNNs, that\n        learn all elements of each filter, only low and high cutoff\n        frequencies are directly learned from data with the proposed method.\n        This offers a very compact and efficient way to derive a customized\n        filter bank specifically tuned for the desired application. Our\n        experiments, conducted on both speaker identification and speaker\n        verification tasks, show that the proposed architecture converges\n        faster and performs better than a standard CNN on raw waveforms.\n\n        -- https://arxiv.org/abs/1808.00158\n\n\n    Args:\n        scale (FrequencyScale): A scale defining the initial bandpass\n            filters\n        taps (int): The length of the filter in samples\n        samplerate (SampleRate): The sampling rate of incoming samples\n\n    See Also:\n        :class:`~zounds.spectral.FrequencyScale`\n        :class:`~zounds.timeseries.SampleRate`\n    """"""\n\n    def __init__(self, scale, taps, samplerate):\n        super(SincLayer, self).__init__()\n        self.samplerate = int(samplerate)\n        self.taps = taps\n        self.scale = scale\n\n        # each filter requires two parameters to define the filter bandwidth\n        filter_parameters = torch.FloatTensor(len(scale), 2)\n\n        self.linear = nn.Parameter(\n            torch.linspace(-math.pi, math.pi, steps=taps), requires_grad=False)\n        self.window = nn.Parameter(\n            torch.hamming_window(self.taps), requires_grad=False)\n\n        for i, band in enumerate(scale):\n            start = self.samplerate / band.start_hz\n            stop = self.samplerate / band.stop_hz\n            filter_parameters[i, 0] = start\n            filter_parameters[i, 1] = stop\n\n        self.filter_parameters = nn.Parameter(filter_parameters)\n\n    def _sinc(self, frequency):\n        x = self.linear[None, :] * frequency[:, None]\n        return torch.sin(x) / x\n\n    def _start_frequencies(self):\n        return torch.abs(self.filter_parameters[:, 0])\n\n    def _stop_frequencies(self):\n        start = self._start_frequencies()\n        return start + torch.abs(self.filter_parameters[:, 1] - start)\n\n    def _filter_bank(self):\n        start = self._start_frequencies()[:, None]\n        stop = self._stop_frequencies()[:, None]\n        start_sinc = self._sinc(start)\n        stop_sinc = self._sinc(stop)\n        filters = \\\n            (2 * stop[..., None] * stop_sinc) \\\n            - (2 * start[..., None] * start_sinc)\n        windowed = filters * self.window[None, None, :]\n        return windowed.squeeze()\n\n    def forward(self, x):\n        x = x.view(-1, 1, x.shape[-1])\n        filters = self._filter_bank().view(len(self.scale), 1, self.taps)\n        x = F.conv1d(x, filters, stride=1, padding=self.taps // 2)\n        return x\n'"
zounds/learn/sklearn_preprocessor.py,1,"b""from .preprocess import Preprocessor, PreprocessResult\n\n\nclass SklearnModel(Preprocessor):\n    def __init__(self, model=None, needs=None):\n        super(SklearnModel, self).__init__(needs=needs)\n        self.model = model\n\n    def _forward_func(self):\n        def x(d, model=None):\n            from zounds.core import ArrayWithUnits, IdentityDimension\n            transformed = model.transform(d.reshape((d.shape[0], -1)))\n            try:\n                return ArrayWithUnits(\n                    transformed, (d.dimensions[0], IdentityDimension()))\n            except AttributeError:\n                return transformed\n\n        return x\n\n    def _backward_func(self):\n        def x(d, model=None, shape=None):\n            return model.inverse_transform(d).reshape((-1,) + shape)\n\n        return x\n\n    def _process(self, data):\n        data = self._extract_data(data)\n        model = self.model.fit(data.reshape((data.shape[0], -1)))\n        shape = data.shape[1:]\n        op = self.transform(model=model)\n        inv_data = self.inversion_data(model=model, shape=shape)\n        inv = self.inverse_transform()\n        data = op(data)\n        model_cls = self.model.__class__.__name__\n        yield PreprocessResult(\n            data,\n            op,\n            inversion_data=inv_data,\n            inverse=inv,\n            name='SklearnModel.{model_cls}'.format(**locals()))\n\n\nclass WithComponents(SklearnModel):\n    def __init__(self, model=None, needs=None):\n        super(WithComponents, self).__init__(model=model, needs=needs)\n\n    def _backward_func(self):\n        def x(d, model=None, shape=None):\n            import numpy as np\n            return np.dot(d, model.components_).reshape((-1,) + shape)\n        return x\n"""
zounds/learn/spectral.py,0,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom zounds.spectral import morlet_filter_bank, AWeighting, FrequencyDimension\nfrom zounds.core import ArrayWithUnits\nfrom zounds.timeseries import TimeDimension\nfrom .util import batchwise_mean_std_normalization\n\n\nclass FilterBank(nn.Module):\n    """"""\n    A torch module that convolves a 1D input signal with a bank of morlet\n    filters.\n\n    Args:\n        samplerate (SampleRate): the samplerate of the input signal\n        kernel_size (int): the length in samples of each filter\n        scale (FrequencyScale): a scale whose center frequencies determine the\n            fundamental frequency of each filer\n        scaling_factors (int or list of int): Scaling factors for each band,\n            which determine the time-frequency resolution tradeoff.\n            The number(s) should fall between 0 and 1, with smaller numbers\n            achieving better frequency resolution, and larget numbers better\n            time resolution\n        normalize_filters (bool): When true, ensure that each filter in the bank\n            has unit norm\n        a_weighting (bool): When true, apply a perceptually-motivated weighting\n            of the filters\n\n    See Also:\n        :class:`~zounds.spectral.AWeighting`\n        :func:`~zounds.spectral.morlet_filter_bank`\n    """"""\n\n    def __init__(\n            self,\n            samplerate,\n            kernel_size,\n            scale,\n            scaling_factors,\n            normalize_filters=True,\n            a_weighting=True):\n\n        super(FilterBank, self).__init__()\n\n        self.samplerate = samplerate\n        filter_bank = morlet_filter_bank(\n            samplerate,\n            kernel_size,\n            scale,\n            scaling_factors,\n            normalize=normalize_filters)\n\n        if a_weighting:\n            filter_bank *= AWeighting()\n\n        self.scale = scale\n\n        filter_bank = torch.from_numpy(filter_bank).float() \\\n            .view(len(scale), 1, kernel_size)\n        self.register_buffer(\'filter_bank\', filter_bank)\n\n    @property\n    def n_bands(self):\n        return len(self.scale)\n\n    def convolve(self, x):\n        x = x.view(-1, 1, x.shape[-1])\n        x = F.conv1d(\n            x, self.filter_bank, padding=self.filter_bank.shape[-1] // 2)\n        return x\n\n    def transposed_convolve(self, x):\n        x = F.conv_transpose1d(\n            x, self.filter_bank, padding=self.filter_bank.shape[-1] // 2)\n        return x\n\n    def log_magnitude(self, x):\n        x = F.relu(x)\n        x = 20 * torch.log10(1 + x)\n        return x\n\n    def temporal_pooling(self, x, kernel_size, stride):\n        x = F.avg_pool1d(x, kernel_size, stride, padding=kernel_size // 2)\n        return x\n\n    def transform(self, samples, pooling_kernel_size, pooling_stride):\n        # convert the raw audio samples to a PyTorch tensor\n        tensor_samples = torch.from_numpy(samples).float() \\\n            .to(self.filter_bank.device)\n\n        # compute the transform\n        spectral = self.convolve(tensor_samples)\n        log_magnitude = self.log_magnitude(spectral)\n        pooled = self.temporal_pooling(\n            log_magnitude, pooling_kernel_size, pooling_stride)\n\n        # convert back to an ArrayWithUnits instance\n        samplerate = samples.samplerate\n        time_frequency = pooled.data.cpu().numpy().squeeze().T\n        time_frequency = ArrayWithUnits(time_frequency, [\n            TimeDimension(\n                frequency=samplerate.frequency * pooling_stride,\n                duration=samplerate.frequency * pooling_kernel_size),\n            FrequencyDimension(self.scale)\n        ])\n        return time_frequency\n\n    def forward(self, x, normalize=True):\n        nsamples = x.shape[-1]\n        x = self.convolve(x)\n        x = self.log_magnitude(x)\n\n        if normalize:\n            x = batchwise_mean_std_normalization(x)\n\n        return x[..., :nsamples].contiguous()\n'"
zounds/learn/supervised.py,2,"b""from .trainer import Trainer\nimport numpy as np\nimport warnings\n\n\nclass SupervisedTrainer(Trainer):\n    def __init__(\n            self,\n            model,\n            loss,\n            optimizer,\n            epochs,\n            batch_size,\n            holdout_percent=0.0,\n            data_preprocessor=lambda x: x,\n            label_preprocessor=lambda x: x,\n            checkpoint_epochs=1):\n\n        super(SupervisedTrainer, self).__init__(\n            epochs,\n            batch_size,\n            checkpoint_epochs=checkpoint_epochs)\n\n        self.label_preprocessor = label_preprocessor\n        self.data_preprocessor = data_preprocessor\n        self.holdout_percent = holdout_percent\n        self.optimizer = optimizer(model)\n        self.loss = loss\n        self.network = model\n        self.register_batch_complete_callback(self._log)\n        self.samples = None\n\n    def _log(self, *args, **kwargs):\n        if kwargs['batch'] % 10:\n            return\n        msg = \\\n            'Epoch {epoch}, batch {batch}, train error ' \\\n            '{train_error}, test error {test_error}'\n        print(msg.format(**kwargs))\n\n    def random_sample(self):\n        if self.samples is None:\n            raise RuntimeError(\n                'There are no samples yet.  Has training started?')\n        index = np.random.randint(0, len(self.samples))\n        inp, label = self.samples[index]\n        return inp, label\n\n    def _cuda(self, device=None):\n        self.network = self.network.cuda()\n        self.loss = self.loss.cuda()\n\n    def train(self, data):\n        data, labels = data['data'], data['labels']\n\n        test_size = int(self.holdout_percent * len(data))\n        test_data, test_labels = data[:test_size], labels[:test_size]\n        data, labels = data[test_size:], labels[test_size:]\n\n        def batch(d, l, test=False):\n            d = self.data_preprocessor(d)\n            l = self.label_preprocessor(l)\n            inp_v = self._variable(d, volatile=test)\n            output = self.network(inp_v)\n\n            labels_v = self._variable(l)\n\n            error = self.loss(output, labels_v)\n\n            if not test:\n                error.backward()\n                self.optimizer.step()\n\n            self.samples = list(zip(inp_v, output))\n            return inp_v, output, error.data.item()\n\n        start = self._current_epoch\n        stop = self._current_epoch + self.checkpoint_epochs\n\n        for epoch in range(start, stop):\n\n            if epoch >= self.epochs:\n                break\n\n            for i in range(0, len(data), self.batch_size):\n\n                self.network.zero_grad()\n\n                # training batch\n                minibatch_slice = slice(i, i + self.batch_size)\n                minibatch_data = data[minibatch_slice]\n                minibatch_labels = labels[minibatch_slice]\n\n                try:\n                    inp, output, e = batch(\n                        minibatch_data, minibatch_labels, test=False)\n                except RuntimeError as e:\n                    if 'Assert' in e.message:\n                        warnings.warn(e.message)\n                        continue\n                    else:\n                        raise\n\n                # test batch\n                if test_size:\n                    indices = np.random.randint(0, test_size, self.batch_size)\n                    test_batch_data = test_data[indices, ...]\n                    test_batch_labels = test_labels[indices, ...]\n\n                    inp, output, te = batch(\n                        test_batch_data, test_batch_labels, test=True)\n                else:\n                    te = 'n/a'\n\n                self.on_batch_complete(\n                    epoch=epoch,\n                    batch=i,\n                    train_error=te,\n                    test_error=e,\n                    samples=self.samples)\n\n            self._current_epoch += 1\n\n        return self.network\n"""
zounds/learn/test_dct.py,0,"b'import unittest2\nfrom .dct_transform import DctTransform\nimport torch\nfrom torch.autograd import Variable\nfrom zounds.spectral import HanningWindowingFunc\n\n\nclass DctTransformTests(unittest2.TestCase):\n    def test_can_do_short_time_dct_transform(self):\n        t = torch.FloatTensor(3, 1, 512)\n        v = Variable(t)\n        dct_trasform = DctTransform()\n        stdct = dct_trasform.short_time_dct(v, 128, 64, HanningWindowingFunc())\n        self.assertEqual((3, 128, 7), tuple(stdct.shape))\n'"
zounds/learn/test_embedding.py,1,"b'import torch\nimport unittest2\nfrom .embedding import TripletEmbeddingTrainer\nfrom torch import nn\nimport numpy as np\n\n\nclass TripletEmbeddingTrainerTests(unittest2.TestCase):\n    def test_normalization_does_not_cause_nans(self):\n        class Network(nn.Module):\n            def __init__(self):\n                super(Network, self).__init__()\n\n            def forward(self, x):\n                return x\n\n        network = Network()\n        trainer = TripletEmbeddingTrainer(network, 100, 32, slice(None))\n        x = torch.zeros(8, 3)\n        result = trainer._apply_network_and_normalize(x).data.numpy()\n        self.assertFalse(np.any(np.isnan(result)))\n'"
zounds/learn/test_instancescaling.py,18,"b'import unittest2\nfrom zounds.util import simple_in_memory_settings\nimport featureflow as ff\nfrom .preprocess import InstanceScaling, PreprocessingPipeline\nimport numpy as np\n\n\nclass InstanceScalingTests(unittest2.TestCase):\n\n    def get_model(self, max_value=1):\n        @simple_in_memory_settings\n        class Model(ff.BaseModel):\n            scaled = ff.PickleFeature(\n                InstanceScaling,\n                max_value=max_value,\n                store=False)\n\n            pipeline = ff.PickleFeature(\n                PreprocessingPipeline,\n                needs=(scaled,),\n                store=True)\n\n        training = np.random.random_sample((10, 3))\n        _id = Model.process(scaled=training)\n        return Model(_id)\n\n    def test_can_scale_to_arbitrary_value(self):\n        model = self.get_model(max_value=50)\n        inp = np.random.random_sample((100, 30)) * 10\n        transformed = model.pipeline.transform(inp).data\n        self.assertEqual(50.0, transformed.max())\n\n    def test_can_invert_when_scaling_to_arbitrary_value(self):\n        model = self.get_model(max_value=50)\n        inp = np.random.random_sample((100, 30)) * 10\n        transformed = model.pipeline.transform(inp)\n        inverted = transformed.inverse_transform()\n        np.testing.assert_allclose(inverted, inp)\n\n    def test_forward_transform_scales_data_2d(self):\n        model = self.get_model()\n        inp = np.random.random_sample((100, 30)) * 10\n        transformed = model.pipeline.transform(inp).data\n        self.assertEqual(1.0, transformed.max())\n\n    def test_each_example_has_same_max(self):\n        model = self.get_model()\n        inp = np.random.normal(0, 1, (100, 30)) * 50\n        transformed = model.pipeline.transform(inp).data\n        np.testing.assert_allclose(np.abs(transformed).max(axis=-1), 1)\n\n    def test_forward_transform_scales_data_3d(self):\n        model = self.get_model()\n        inp = np.random.random_sample((100, 30, 3)) - 0.5\n        transformed = model.pipeline.transform(inp).data\n        self.assertEqual(1.0, np.max(np.abs(transformed)))\n\n    def test_backward_transform_reconstructs_data_2d(self):\n        model = self.get_model()\n        inp = np.random.random_sample((100, 30)) * 10\n        transformed = model.pipeline.transform(inp)\n        inverted = transformed.inverse_transform()\n        np.testing.assert_allclose(inverted, inp)\n\n    def test_backward_transform_reconstructs_data_3d(self):\n        model = self.get_model()\n        inp = np.random.random_sample((100, 30, 3)) - 0.5\n        transformed = model.pipeline.transform(inp)\n        inverted = transformed.inverse_transform()\n        np.testing.assert_allclose(inverted, inp)\n\n    def test_correctly_handles_max_of_zero(self):\n        model = self.get_model()\n        inp = np.random.random_sample((100, 30, 3)) - 0.5\n        inp[0, ...] = 0\n        transformed = model.pipeline.transform(inp)\n        inverted = transformed.inverse_transform()\n        np.testing.assert_allclose(inverted, inp)\n\n    def test_instance_scaling_maintains_dtype(self):\n        model = self.get_model()\n        inp = np.random.random_sample((100, 30)) * 10\n        inp = inp.astype(np.float32)\n        transformed = model.pipeline.transform(inp).data\n        self.assertEqual(np.float32, transformed.dtype)\n'"
zounds/learn/test_learn.py,3,"b""import unittest2\nimport featureflow\nfrom .random_samples import ReservoirSampler\nfrom .preprocess import \\\n    UnitNorm, MeanStdNormalization, PreprocessingPipeline, Pipeline\nfrom .learn import Learned, KMeans\nimport numpy as np\n\n\nclass Iterator(featureflow.Node):\n    def __init__(self, needs=None):\n        super(Iterator, self).__init__(needs=needs)\n\n    def _process(self, data):\n        for d in data:\n            yield d\n\n\ndef build_classes():\n    class Settings(featureflow.PersistenceSettings):\n        _id = 'rbm'\n        id_provider = featureflow.StaticIdProvider(_id)\n        key_builder = featureflow.StringDelimitedKeyBuilder()\n        database = featureflow.InMemoryDatabase(key_builder=key_builder)\n\n    class Rbm(featureflow.BaseModel, Settings):\n        iterator = featureflow.Feature(\n            Iterator,\n            store=False)\n\n        shuffle = featureflow.NumpyFeature(\n            ReservoirSampler,\n            nsamples=1000,\n            needs=iterator,\n            store=True)\n\n        unitnorm = featureflow.PickleFeature(\n            UnitNorm,\n            needs=shuffle,\n            store=False)\n\n        meanstd = featureflow.PickleFeature(\n            MeanStdNormalization,\n            needs=unitnorm,\n            store=False)\n\n        rbm = featureflow.PickleFeature(\n            KMeans,\n            centroids=3,\n            needs=meanstd,\n            store=False)\n\n        pipeline = featureflow.PickleFeature(\n            PreprocessingPipeline,\n            needs=(unitnorm, meanstd, rbm),\n            store=True)\n\n    return Rbm\n\n\ndef data():\n    for i in range(100):\n        yield np.random.random_sample((np.random.randint(10, 100), 9))\n\n\nclass RbmTests(unittest2.TestCase):\n    def test_can_retrieve_rbm_pipeline(self):\n        KMeans = build_classes()\n        KMeans.process(iterator=data())\n        self.assertIsInstance(KMeans().pipeline, Pipeline)\n\n\nclass LearnedTests(unittest2.TestCase):\n    def test_can_use_learned_feature(self):\n        KMeans = build_classes()\n        KMeans.process(iterator=data())\n        l = Learned(learned=KMeans())\n        results = list(l._process(np.random.random_sample((33, 9))))[0]\n        self.assertEqual((33, 3), results.shape)\n\n    def test_pipeline_changes_version_when_recomputed(self):\n        KMeans = build_classes()\n        KMeans.process(iterator=data())\n        v1 = Learned(learned=KMeans()).version\n        v2 = Learned(learned=KMeans()).version\n        self.assertEqual(v1, v2)\n        KMeans.process(iterator=data())\n        v3 = Learned(learned=KMeans()).version\n        self.assertNotEqual(v1, v3)\n\n    def test_pipeline_does_not_store_computed_data_from_training(self):\n        Rbm = build_classes()\n        Rbm.process(iterator=data())\n        rbm = Rbm()\n        pipeline_data = rbm.pipeline.processors[-1].data\n        self.assertIsNone(pipeline_data)\n\n    def test_can_pass_a_pipeline_slice_to_be_applied_at_inference_time(self):\n        KMeans = build_classes()\n        KMeans.process(iterator=data())\n        l = Learned(learned=KMeans(), pipeline_func=lambda x: x.pipeline[:2])\n        results = list(l._process(np.random.random_sample((33, 9))))[0]\n        self.assertEqual((33, 9), results.shape)\n"""
zounds/learn/test_log.py,14,"b'import unittest2\nfrom zounds.util import simple_in_memory_settings\nimport featureflow as ff\nfrom .preprocess import Log, PreprocessingPipeline\nimport numpy as np\n\n\nclass LogTests(unittest2.TestCase):\n    def get_model(self):\n        @simple_in_memory_settings\n        class Model(ff.BaseModel):\n            log = ff.PickleFeature(\n                    Log,\n                    store=False)\n\n            pipeline = ff.PickleFeature(\n                    PreprocessingPipeline,\n                    needs=(log,),\n                    store=True)\n\n        training = np.random.random_sample((10, 3))\n        _id = Model.process(log=training)\n        return Model(_id)\n\n    def test_forward_transform_preserves_sign_large_values(self):\n        model = self.get_model()\n        inp = (np.random.random_sample((100, 3)) * 1000) - 500\n        s1 = np.sign(inp)\n        result = model.pipeline.transform(inp)\n        s2 = np.sign(result.data)\n        self.assertTrue(np.all(s1 == s2))\n\n    def test_forward_transform_preserves_sign(self):\n        model = self.get_model()\n        inp = np.random.random_sample((100, 3)) - 0.5\n        s1 = np.sign(inp)\n        result = model.pipeline.transform(inp)\n        s2 = np.sign(result.data)\n        self.assertTrue(np.all(s1 == s2))\n\n    def test_backward_transform_reconstructs_data_large_values(self):\n        model = self.get_model()\n        inp = (np.random.random_sample((100, 3)) * 1000) - 500\n        result = model.pipeline.transform(inp)\n        inverted = result.inverse_transform()\n        np.testing.assert_allclose(inp, inverted)\n\n    def test_backward_transform_reconstructs_data(self):\n        model = self.get_model()\n        inp = np.random.random_sample((100, 3)) - 0.5\n        result = model.pipeline.transform(inp)\n        inverted = result.inverse_transform()\n        np.testing.assert_allclose(inp, inverted)\n\n    def test_pipeline_requires_no_inversion_data(self):\n        model = self.get_model()\n        inp = (np.random.random_sample((100, 3)) * 1000) - 500\n        result = model.pipeline.transform(inp)\n        self.assertFalse(result.inversion_data[0])\n'"
zounds/learn/test_meanstd.py,3,"b'import unittest2\nfrom zounds.util import simple_in_memory_settings\nfrom .preprocess import MeanStdNormalization, PreprocessingPipeline\nimport featureflow as ff\nimport numpy as np\n\n\nclass MeanStdTests(unittest2.TestCase):\n    def _forward_backward(self, shape):\n        @simple_in_memory_settings\n        class Model(ff.BaseModel):\n            meanstd = ff.PickleFeature(\n                    MeanStdNormalization,\n                    store=False)\n\n            pipeline = ff.PickleFeature(\n                    PreprocessingPipeline,\n                    needs=(meanstd,),\n                    store=True)\n\n        training = np.random.random_sample((100,) + shape)\n        _id = Model.process(meanstd=training)\n        model = Model(_id)\n\n        data_shape = (10,) + shape\n        data = np.random.random_sample(data_shape)\n        result = model.pipeline.transform(data)\n        self.assertEqual(data_shape, result.data.shape)\n        inverted = result.inverse_transform()\n        self.assertEqual(inverted.shape, data.shape)\n        np.testing.assert_allclose(inverted, data)\n\n    def test_can_process_1d(self):\n        self._forward_backward((9,))\n\n    def test_can_process_2d(self):\n        self._forward_backward((3, 4))\n\n    def test_can_process_3d(self):\n        self._forward_backward((5, 4, 7))\n'"
zounds/learn/test_multiply.py,16,"b'import unittest2\nimport numpy as np\nfrom .preprocess import Multiply, PreprocessingPipeline\nfrom zounds.util import simple_in_memory_settings\nimport featureflow as ff\n\n\nclass MultiplyTests(unittest2.TestCase):\n    def get_model(self, factor):\n        @simple_in_memory_settings\n        class Model(ff.BaseModel):\n            multiply = ff.PickleFeature(\n                    Multiply,\n                    factor=factor,\n                    store=False)\n\n            pipeline = ff.PickleFeature(\n                    PreprocessingPipeline,\n                    needs=(multiply,),\n                    store=True)\n\n        return Model\n\n    def test_can_do_forward_transform_with_scalar(self):\n        training = np.random.random_sample((100, 30))\n        factor = 10\n        Model = self.get_model(factor)\n        _id = Model.process(multiply=training)\n        model = Model(_id)\n        data = np.random.random_sample((10, 30))\n        transformed = model.pipeline.transform(data)\n        np.testing.assert_allclose(data * factor, transformed.data)\n\n    def test_can_do_forward_transform_with_array(self):\n        training = np.random.random_sample((100, 30))\n        factor = np.random.random_sample(30)\n        Model = self.get_model(factor)\n        _id = Model.process(multiply=training)\n        model = Model(_id)\n        data = np.random.random_sample((10, 30))\n        transformed = model.pipeline.transform(data)\n        np.testing.assert_allclose(data * factor, transformed.data)\n\n    def test_raises_if_shapes_do_not_match(self):\n        training = np.random.random_sample((100, 30))\n        factor = np.random.random_sample(3)\n        Model = self.get_model(factor)\n        self.assertRaises(ValueError, lambda: Model.process(multiply=training))\n\n    def test_can_do_forward_and_backward_transform_with_scalar(self):\n        training = np.random.random_sample((100, 30))\n        factor = 10\n        Model = self.get_model(factor)\n        _id = Model.process(multiply=training)\n        model = Model(_id)\n        data = np.random.random_sample((10, 30))\n        transformed = model.pipeline.transform(data)\n        recon = transformed.inverse_transform()\n        np.testing.assert_allclose(data, recon)\n\n    def test_can_do_forward_and_backward_transform_with_array(self):\n        training = np.random.random_sample((100, 30))\n        factor = np.random.random_sample(30)\n        Model = self.get_model(factor)\n        _id = Model.process(multiply=training)\n        model = Model(_id)\n        data = np.random.random_sample((10, 30))\n        transformed = model.pipeline.transform(data)\n        recon = transformed.inverse_transform()\n        np.testing.assert_allclose(data, recon)\n'"
zounds/learn/test_pipeline.py,11,"b'import featureflow\nimport numpy as np\nimport unittest2\n\nfrom .preprocess import \\\n    UnitNorm, MeanStdNormalization, Binarize, PreprocessingPipeline, Log\nfrom zounds.spectral import \\\n    GeometricScale, FrequencyAdaptive\nfrom zounds.timeseries import Seconds, TimeDimension\nfrom zounds.util import simple_in_memory_settings\n\n\nclass TestPipeline(unittest2.TestCase):\n    def test_cannot_invert_pipeline_if_any_steps_are_missing(self):\n        class Settings(featureflow.PersistenceSettings):\n            id_provider = featureflow.UuidProvider()\n            key_builder = featureflow.StringDelimitedKeyBuilder()\n            database = featureflow.InMemoryDatabase(key_builder=key_builder)\n\n        class Model(featureflow.BaseModel, Settings):\n            unitnorm = featureflow.PickleFeature(\n                UnitNorm,\n                store=False)\n\n            binary = featureflow.PickleFeature(\n                Binarize,\n                needs=unitnorm,\n                store=False)\n\n            pipeline = featureflow.PickleFeature(\n                PreprocessingPipeline,\n                needs=(unitnorm, binary),\n                store=True)\n\n        data = np.random.random_sample((1000, 4))\n        _id = Model.process(unitnorm=data)\n        example = np.random.random_sample((10, 4))\n        model = Model(_id)\n        transformed = model.pipeline.transform(example)\n        self.assertRaises(\n            NotImplementedError, lambda: transformed.inverse_transform())\n\n    def test_can_invert_pipeline(self):\n        class Settings(featureflow.PersistenceSettings):\n            id_provider = featureflow.UuidProvider()\n            key_builder = featureflow.StringDelimitedKeyBuilder()\n            database = featureflow.InMemoryDatabase(key_builder=key_builder)\n\n        class Model(featureflow.BaseModel, Settings):\n            unitnorm = featureflow.PickleFeature(\n                UnitNorm,\n                store=False)\n\n            meanstd = featureflow.PickleFeature(\n                MeanStdNormalization,\n                needs=unitnorm,\n                store=False)\n\n            pipeline = featureflow.PickleFeature(\n                PreprocessingPipeline,\n                needs=(unitnorm, meanstd),\n                store=True)\n\n        data = np.random.random_sample((1000, 4))\n        _id = Model.process(unitnorm=data)\n        example = np.random.random_sample((10, 4))\n        model = Model(_id)\n        transformed = model.pipeline.transform(example)\n        reconstructed = transformed.inverse_transform()\n        diff = np.abs(example - reconstructed)\n        self.assertTrue(np.all(diff < .00001))\n\n    def test_can_invert_pipeline_with_log(self):\n        class Settings(featureflow.PersistenceSettings):\n            id_provider = featureflow.UuidProvider()\n            key_builder = featureflow.StringDelimitedKeyBuilder()\n            database = featureflow.InMemoryDatabase(key_builder=key_builder)\n\n        class Model(featureflow.BaseModel, Settings):\n            log = featureflow.PickleFeature(\n                Log,\n                store=False)\n\n            meanstd = featureflow.PickleFeature(\n                MeanStdNormalization,\n                needs=log,\n                store=False)\n\n            pipeline = featureflow.PickleFeature(\n                PreprocessingPipeline,\n                needs=(log, meanstd),\n                store=True)\n\n        data = np.random.random_sample((1000, 4))\n        _id = Model.process(log=data)\n        example = np.random.random_sample((10, 4))\n        model = Model(_id)\n        transformed = model.pipeline.transform(example)\n        reconstructed = transformed.inverse_transform()\n        diff = np.abs(example - reconstructed)\n        self.assertTrue(np.all(diff < .00001))\n\n    def test_can_invert_pipeline_that_takes_frequency_adaptive_transform(self):\n        td = TimeDimension(frequency=Seconds(1))\n        scale = GeometricScale(20, 5000, 0.05, 10)\n        arrs = [np.zeros((10, x)) for x in range(1, 11)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n\n        @simple_in_memory_settings\n        class Model(featureflow.BaseModel):\n            log = featureflow.PickleFeature(\n                Log,\n                store=False)\n\n            meanstd = featureflow.PickleFeature(\n                MeanStdNormalization,\n                needs=log,\n                store=False)\n\n            pipeline = featureflow.PickleFeature(\n                PreprocessingPipeline,\n                needs=(log, meanstd),\n                store=True)\n\n        _id = Model.process(log=fa)\n        model = Model(_id)\n        result = model.pipeline.transform(fa)\n        recon = result.inverse_transform()\n        self.assertIsInstance(recon, FrequencyAdaptive)\n        self.assertEqual(fa.shape, recon.shape)'"
zounds/learn/test_pytorch_model.py,43,"b'\n\nimport featureflow as ff\nimport numpy as np\nimport unittest2\n\nfrom .preprocess import \\\n    UnitNorm, Binarize, PreprocessingPipeline, InstanceScaling\nfrom .pytorch_model import PyTorchNetwork, PyTorchAutoEncoder, PyTorchGan\nfrom .supervised import SupervisedTrainer\nfrom .wgan import WassersteinGanTrainer\nfrom .random_samples import ShuffledSamples\nfrom zounds.core import ArrayWithUnits, IdentityDimension\nfrom zounds.spectral import LinearScale, FrequencyBand, FrequencyDimension\nfrom zounds.timeseries import Seconds, TimeDimension\nfrom zounds.util import simple_in_memory_settings\nimport torch\nfrom torch import nn\nfrom torch.optim import SGD, Adam\n\n\nclass SupervisedNetwork(nn.Module):\n    def __init__(self):\n        super(SupervisedNetwork, self).__init__()\n        self.visible = nn.Linear(2, 64, bias=False)\n        self.t1 = nn.Sigmoid()\n        self.hidden = nn.Linear(64, 1, bias=False)\n        self.t2 = nn.Sigmoid()\n\n    def forward(self, inp):\n        x = self.visible(inp)\n        x = self.t1(x)\n        x = self.hidden(x)\n        x = self.t2(x)\n        return x\n\n\nclass AutoEncoder(nn.Module):\n    def __init__(self):\n        super(AutoEncoder, self).__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Linear(3, 2, bias=False),\n            nn.Sigmoid())\n\n        self.decoder = nn.Sequential(\n            nn.Linear(2, 3, bias=False),\n            nn.Sigmoid())\n\n    def forward(self, inp):\n        x = self.encoder(inp)\n        x = self.decoder(x)\n        return x\n\n\nclass GanGenerator(nn.Module):\n    def __init__(self):\n        super(GanGenerator, self).__init__()\n        self.linear = nn.Linear(2, 4)\n        self.tanh = nn.Tanh()\n\n    def forward(self, inp):\n        x = self.linear(inp)\n        x = self.tanh(x)\n        return x\n\n\nclass GanDiscriminator(nn.Module):\n    def __init__(self):\n        super(GanDiscriminator, self).__init__()\n        self.linear = nn.Linear(4, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, inp):\n        x = self.linear(inp)\n        x = self.sigmoid(x)\n        return x\n\n\nclass GanPair(nn.Module):\n    def __init__(self):\n        super(GanPair, self).__init__()\n        self.generator = GanGenerator()\n        self.discriminator = GanDiscriminator()\n\n    def forward(self, x):\n        raise NotImplementedError()\n\n\nclass GanPairWithArgs(nn.Module):\n    def __init__(self, must_pass):\n        super(GanPairWithArgs, self).__init__()\n        self.must_pass = must_pass\n        self.generator = GanGenerator()\n        self.discriminator = GanDiscriminator()\n\n    def forward(self, x):\n        raise NotImplementedError()\n\n\nclass PyTorchModelTests(unittest2.TestCase):\n\n    def test_can_maintain_array_dimensions_with_supervised_learning(self):\n        trainer = SupervisedTrainer(\n            model=SupervisedNetwork(),\n            loss=nn.BCELoss(),\n            optimizer=lambda model: SGD(model.parameters(), lr=0.2),\n            epochs=1,\n            batch_size=64,\n            data_preprocessor=lambda x: x.astype(np.float32),\n            label_preprocessor=lambda x: x.astype(np.float32))\n\n        @simple_in_memory_settings\n        class Pipeline(ff.BaseModel):\n            inp = ff.PickleFeature(\n                ff.IteratorNode,\n                store=False)\n\n            samples = ff.PickleFeature(\n                ShuffledSamples,\n                nsamples=500,\n                multiplexed=True,\n                dtype=np.float32,\n                needs=inp,\n                store=False)\n\n            unitnorm = ff.PickleFeature(\n                UnitNorm,\n                needs=samples.aspect(\'data\'),\n                store=False)\n\n            hard_labels = ff.PickleFeature(\n                Binarize,\n                needs=samples.aspect(\'labels\'),\n                store=False)\n\n            network = ff.PickleFeature(\n                PyTorchNetwork,\n                trainer=trainer,\n                needs=dict(data=unitnorm, labels=hard_labels),\n                store=False)\n\n            pipeline = ff.PickleFeature(\n                PreprocessingPipeline,\n                needs=(unitnorm, network),\n                store=True)\n\n        # Produce some random points on the unit circle\n        samples = np.random.random_sample((1000, 2))\n        samples /= np.linalg.norm(samples, axis=1, keepdims=True)\n\n        # a line extending from the origin to (1, 1)\n        origin = np.array([0, 0])\n        unit = np.array([1, 1])\n\n        # which side of the plane is each sample on?\n        labels = np.sign(np.cross(unit - origin, origin - samples))\n        labels[labels < 0] = 0\n\n        # scale each sample randomly, forcing the pipeline to normalize data\n        factors = np.random.randint(1, 1000, (len(samples), 1))\n        scaled_samples = samples * factors\n        scaled_samples = scaled_samples\n\n        # fuzz the labels, forcing the pipeline to binarize these (i.e., force\n        # them to be 0 or 1)\n        fuzzed_labels = labels + np.random.normal(0, 0.1, labels.shape)\n        fuzzed_labels = fuzzed_labels[..., None]\n\n        def gen(chunksize, s, l):\n            for i in range(0, len(s), chunksize):\n                sl = slice(i, i + chunksize)\n                yield dict(data=s[sl], labels=l[sl])\n\n        _id = Pipeline.process(inp=gen(100, scaled_samples, fuzzed_labels))\n        pipe = Pipeline(_id)\n\n        # produce some new samples\n        new_samples = np.random.random_sample((1000, 2))\n        new_samples /= np.linalg.norm(samples, axis=1, keepdims=True)\n\n        # scale each example randomly, so the pipeline must give it unit norm\n        # to arrive at the correct answer\n        new_factors = np.random.randint(1, 1000, (len(samples), 1))\n        new_scaled_samples = new_factors * new_samples\n\n        arr = ArrayWithUnits(\n            new_scaled_samples,\n            dimensions=[\n                TimeDimension(Seconds(1)),\n                FrequencyDimension(LinearScale(FrequencyBand(100, 1000), 2))\n            ])\n\n        result = pipe.pipeline.transform(arr.astype(np.float32))\n        self.assertIsInstance(result.data, ArrayWithUnits)\n        self.assertIsInstance(result.data.dimensions[0], TimeDimension)\n\n    def test_can_perform_supervised_learning(self):\n        """"""\n        Create and exercise a learning pipeline that learns to classify\n        2d points as being on one side or the other of a plane from (0, 0) to\n        (1, 1)\n        """"""\n\n        trainer = SupervisedTrainer(\n            model=SupervisedNetwork(),\n            loss=nn.BCELoss(),\n            optimizer=lambda model: SGD(model.parameters(), lr=0.2),\n            epochs=100,\n            batch_size=64,\n            data_preprocessor=lambda x: x.astype(np.float32),\n            label_preprocessor=lambda x: x.astype(np.float32),\n            checkpoint_epochs=100)\n\n        @simple_in_memory_settings\n        class Pipeline(ff.BaseModel):\n            inp = ff.PickleFeature(\n                ff.IteratorNode,\n                store=False)\n\n            samples = ff.PickleFeature(\n                ShuffledSamples,\n                nsamples=500,\n                multiplexed=True,\n                dtype=np.float32,\n                needs=inp,\n                store=False)\n\n            unitnorm = ff.PickleFeature(\n                UnitNorm,\n                needs=samples.aspect(\'data\'),\n                store=False)\n\n            hard_labels = ff.PickleFeature(\n                Binarize,\n                needs=samples.aspect(\'labels\'),\n                store=False)\n\n            network = ff.PickleFeature(\n                PyTorchNetwork,\n                trainer=trainer,\n                needs=dict(data=unitnorm, labels=hard_labels),\n                store=False)\n\n            pipeline = ff.PickleFeature(\n                PreprocessingPipeline,\n                needs=(unitnorm, network),\n                store=True)\n\n        # Produce some random points on the unit circle\n        samples = np.random.random_sample((1000, 2))\n        samples /= np.linalg.norm(samples, axis=1, keepdims=True)\n\n        # a line extending from the origin to (1, 1)\n        origin = np.array([0, 0])\n        unit = np.array([1, 1])\n\n        # which side of the plane is each sample on?\n        labels = np.sign(np.cross(unit - origin, origin - samples))\n        labels[labels < 0] = 0\n\n        # scale each sample randomly, forcing the pipeline to normalize data\n        factors = np.random.randint(1, 1000, (len(samples), 1))\n        scaled_samples = samples * factors\n        scaled_samples = scaled_samples\n\n        # fuzz the labels, forcing the pipeline to binarize these (i.e., force\n        # them to be 0 or 1)\n        fuzzed_labels = labels + np.random.normal(0, 0.1, labels.shape)\n        fuzzed_labels = fuzzed_labels[..., None]\n\n        def gen(chunksize, s, l):\n            for i in range(0, len(s), chunksize):\n                sl = slice(i, i + chunksize)\n                yield dict(data=s[sl], labels=l[sl])\n\n        _id = Pipeline.process(inp=gen(100, scaled_samples, fuzzed_labels))\n        pipe = Pipeline(_id)\n\n        # produce some new samples\n        new_samples = np.random.random_sample((1000, 2))\n        new_samples /= np.linalg.norm(samples, axis=1, keepdims=True)\n\n        # which side of the plane is each sample on?\n        new_labels = np.sign(np.cross(unit - origin, origin - new_samples))\n        new_labels[new_labels < 0] = 0\n\n        # scale each example randomly, so the pipeline must give it unit norm\n        # to arrive at the correct answer\n        new_factors = np.random.randint(1, 1000, (len(samples), 1))\n        new_scaled_samples = new_factors * new_samples\n\n        result = pipe.pipeline.transform(new_scaled_samples.astype(np.float32))\n\n        # reshape the data, and normalize to 0 or 1\n        result = np.round(result.data.squeeze())\n\n        # compute the number of labels that are incorrect\n        difference = np.logical_xor(result, new_labels).sum()\n        percent_error = difference / len(result)\n\n        self.assertLess(percent_error, 0.05)\n\n    def test_maintains_array_with_units_dimensions(self):\n        trainer = SupervisedTrainer(\n            AutoEncoder(),\n            loss=nn.MSELoss(),\n            optimizer=lambda model: SGD(model.parameters(), lr=0.1),\n            epochs=2,\n            batch_size=64,\n            checkpoint_epochs=2)\n\n        @simple_in_memory_settings\n        class Pipeline(ff.BaseModel):\n            inp = ff.PickleFeature(\n                ff.IteratorNode,\n                store=False)\n\n            samples = ff.PickleFeature(\n                ShuffledSamples,\n                nsamples=500,\n                dtype=np.float32,\n                needs=inp,\n                store=False)\n\n            unitnorm = ff.PickleFeature(\n                UnitNorm,\n                needs=samples,\n                store=False)\n\n            network = ff.PickleFeature(\n                PyTorchAutoEncoder,\n                trainer=trainer,\n                needs=unitnorm,\n                store=False)\n\n            pipeline = ff.PickleFeature(\n                PreprocessingPipeline,\n                needs=(unitnorm, network),\n                store=True)\n\n        training = np.random.random_sample((1000, 3))\n\n        def gen(chunksize, s):\n            for i in range(0, len(s), chunksize):\n                yield s[i: i + chunksize]\n\n        _id = Pipeline.process(inp=gen(100, training))\n        pipe = Pipeline(_id)\n\n        test = ArrayWithUnits(\n            np.random.random_sample((10, 3)).astype(np.float32),\n            dimensions=[\n                TimeDimension(Seconds(1)),\n                FrequencyDimension(LinearScale(FrequencyBand(100, 1000), 3))\n            ])\n        result = pipe.pipeline.transform(test)\n        self.assertEqual((10, 2), result.data.shape)\n        self.assertIsInstance(result.data, ArrayWithUnits)\n        self.assertIsInstance(result.data.dimensions[0], TimeDimension)\n        self.assertIsInstance(result.data.dimensions[1], IdentityDimension)\n\n        inverted = result.inverse_transform()\n        self.assertEqual((10, 3), inverted.shape)\n        self.assertIsInstance(inverted, ArrayWithUnits)\n        self.assertIsInstance(inverted.dimensions[0], TimeDimension)\n        self.assertIsInstance(inverted.dimensions[1], FrequencyDimension)\n\n    def test_can_perform_unsupervised_learning_autoencoder(self):\n\n        trainer = SupervisedTrainer(\n            AutoEncoder(),\n            loss=nn.MSELoss(),\n            optimizer=lambda model: SGD(model.parameters(), lr=0.1),\n            epochs=10,\n            batch_size=64,\n            checkpoint_epochs=10)\n\n        @simple_in_memory_settings\n        class Pipeline(ff.BaseModel):\n            inp = ff.PickleFeature(\n                ff.IteratorNode,\n                store=False)\n\n            samples = ff.PickleFeature(\n                ShuffledSamples,\n                nsamples=500,\n                dtype=np.float32,\n                needs=inp,\n                store=False)\n\n            unitnorm = ff.PickleFeature(\n                UnitNorm,\n                needs=samples,\n                store=False)\n\n            network = ff.PickleFeature(\n                PyTorchAutoEncoder,\n                trainer=trainer,\n                needs=unitnorm,\n                store=False)\n\n            pipeline = ff.PickleFeature(\n                PreprocessingPipeline,\n                needs=(unitnorm, network),\n                store=True)\n\n        training = np.random.random_sample((1000, 3))\n\n        def gen(chunksize, s):\n            for i in range(0, len(s), chunksize):\n                yield s[i: i + chunksize]\n\n        _id = Pipeline.process(inp=gen(100, training))\n        pipe = Pipeline(_id)\n\n        test = np.random.random_sample((10, 3)).astype(np.float32)\n        result = pipe.pipeline.transform(test)\n        self.assertEqual((10, 2), result.data.shape)\n\n        inverted = result.inverse_transform()\n        self.assertEqual((10, 3), inverted.shape)\n\n    def test_can_train_gan_with_init_args(self):\n        trainer = WassersteinGanTrainer(\n            GanPairWithArgs(10),\n            latent_dimension=(2,),\n            n_critic_iterations=5,\n            epochs=10,\n            batch_size=64)\n\n        @simple_in_memory_settings\n        class Pipeline(ff.BaseModel):\n            inp = ff.PickleFeature(\n                ff.IteratorNode)\n\n            samples = ff.PickleFeature(\n                ShuffledSamples,\n                nsamples=500,\n                needs=inp,\n                dtype=np.float32)\n\n            scaled = ff.PickleFeature(\n                InstanceScaling,\n                needs=samples)\n\n            network = ff.PickleFeature(\n                PyTorchGan,\n                apply_network=\'generator\',\n                trainer=trainer,\n                needs=scaled)\n\n            pipeline = ff.PickleFeature(\n                PreprocessingPipeline,\n                needs=(scaled, network),\n                store=True)\n\n        training_data = np.random.normal(0, 1, (1000, 4))\n\n        def gen(chunksize, s):\n            for i in range(0, len(s), chunksize):\n                yield s[i: i + chunksize]\n\n        _id = Pipeline.process(inp=gen(100, training_data))\n        pipe = Pipeline(_id)\n\n        noise = np.random.normal(0, 1, (10, 2))\n        result = pipe.pipeline.transform(noise)\n        self.assertEqual((10, 4), result.data.shape)\n\n    def test_can_train_gan(self):\n\n        trainer = WassersteinGanTrainer(\n            GanPair(),\n            latent_dimension=(2,),\n            n_critic_iterations=5,\n            epochs=10,\n            batch_size=64)\n\n        @simple_in_memory_settings\n        class Pipeline(ff.BaseModel):\n            inp = ff.PickleFeature(\n                ff.IteratorNode)\n\n            samples = ff.PickleFeature(\n                ShuffledSamples,\n                nsamples=500,\n                needs=inp,\n                dtype=np.float32)\n\n            scaled = ff.PickleFeature(\n                InstanceScaling,\n                needs=samples)\n\n            network = ff.PickleFeature(\n                PyTorchGan,\n                apply_network=\'generator\',\n                trainer=trainer,\n                needs=scaled)\n\n            pipeline = ff.PickleFeature(\n                PreprocessingPipeline,\n                needs=(scaled, network),\n                store=True)\n\n        training_data = np.random.normal(0, 1, (1000, 4))\n\n        def gen(chunksize, s):\n            for i in range(0, len(s), chunksize):\n                yield s[i: i + chunksize]\n\n        _id = Pipeline.process(inp=gen(100, training_data))\n        pipe = Pipeline(_id)\n\n        noise = np.random.normal(0, 1, (10, 2))\n        result = pipe.pipeline.transform(noise)\n        self.assertEqual((10, 4), result.data.shape)\n'"
zounds/learn/test_random_samples.py,28,"b""import unittest2\nfrom .random_samples import \\\n    ReservoirSampler, Reservoir, MultiplexedReservoir\nfrom zounds.timeseries import TimeDimension, Seconds\nfrom zounds.spectral import FrequencyDimension, FrequencyBand, LinearScale\nfrom zounds.core import ArrayWithUnits, IdentityDimension\nimport numpy as np\n\n\nclass TestReservoir(unittest2.TestCase):\n    def test_nsamples_must_be_gt_zero(self):\n        self.assertRaises(ValueError, lambda: Reservoir(0))\n\n    def test_can_dictate_dtype(self):\n        r = Reservoir(100, dtype=np.float32)\n        r.add(np.ones(10, dtype=np.float64))\n        self.assertEqual(np.float32, r.get().dtype)\n\n    def test_reservoir_has_first_input_dtype_when_unspecified(self):\n        r = Reservoir(100)\n        r.add(np.ones(10, dtype=np.float64))\n        self.assertEqual(np.float64, r.get().dtype)\n\n    def test_raises_if_nsamples_is_not_int(self):\n        self.assertRaises(ValueError, lambda: Reservoir(1e2))\n\n    def test_array_has_correct_first_dimension(self):\n        r = Reservoir(100)\n        r.add(np.random.random_sample((10, 3)))\n        self.assertEqual(100, r.arr.shape[0])\n\n    def test_can_add_samples_larger_than_reservoir_size(self):\n        r = Reservoir(100)\n        r.add(np.random.random_sample((1000, 3)))\n        self.assertEqual(100, len(r.get()))\n\n    def test_array_has_correct_subsequent_dimensions(self):\n        r = Reservoir(100)\n        r.add(np.random.random_sample((10, 3, 2)))\n        self.assertEqual((3, 2), r.arr.shape[1:])\n\n    def test_array_with_units(self):\n        r = Reservoir(100)\n\n        frequency_dimension = FrequencyDimension(\n            LinearScale(FrequencyBand(100, 1000), 100))\n\n        samples = ArrayWithUnits(\n            np.ones((20, 100)),\n            [\n                TimeDimension(frequency=Seconds(1)),\n                frequency_dimension\n            ])\n\n        r.add(samples)\n        mixed = r.get()\n        self.assertIsInstance(mixed, ArrayWithUnits)\n        self.assertEqual(100, mixed.shape[1])\n        self.assertIsInstance(mixed.dimensions[0], IdentityDimension)\n        self.assertIsInstance(mixed.dimensions[1], FrequencyDimension)\n\n    def test_reservoir_is_well_mixed(self):\n        r = Reservoir(100)\n        samples = np.arange(100)[..., None]\n        for i in range(0, 100, 10):\n            r.add(samples[i: i + 10])\n        mixed = r.get().squeeze()\n        diff = np.diff(mixed)\n        self.assertFalse(np.all(diff == 1))\n\n    def test_can_provide_explicit_indices_when_adding(self):\n        r = Reservoir(10)\n        samples = np.arange(10)[..., None]\n        r.add(samples, indices=samples.squeeze()[::-1])\n        mixed = r.get()\n        np.testing.assert_allclose(mixed.squeeze(), samples.squeeze()[::-1])\n\n    def test_raises_when_samples_and_explicit_indices_dont_match(self):\n        r = Reservoir(10)\n        samples = np.arange(10)[..., None]\n        self.assertRaises(\n            ValueError, lambda: r.add(samples, indices=samples.squeeze()[:5]))\n\n    def test_can_get_batch(self):\n        r = Reservoir(100)\n        samples = np.arange(100)[..., None]\n        for i in range(0, 100, 10):\n            r.add(samples[i: i + 10])\n        samples = r.get_batch(15)\n        self.assertEqual(15, samples.shape[0])\n\n    def test_raises_if_get_batch_is_larger_than_total_sample_size(self):\n        r = Reservoir(100)\n        samples = np.arange(100)[..., None]\n        for i in range(0, 100, 10):\n            r.add(samples[i: i + 10])\n        self.assertRaises(ValueError, lambda: r.get_batch(1000))\n\n    def test_raises_if_get_batch_is_larger_than_available_sample_size(self):\n        r = Reservoir(100)\n        samples = np.arange(100)[..., None]\n        for i in range(0, 50, 10):\n            r.add(samples[i: i + 10])\n        self.assertRaises(ValueError, lambda: r.get_batch(64))\n\n\nclass TestMultiplexedReservoir(unittest2.TestCase):\n    def test_is_consistent_across_keys(self):\n        r = MultiplexedReservoir(100)\n        samples = np.random.random_sample((10, 3))\n        r.add(dict(cat=samples, dog=samples))\n        mixed = r.get()\n        np.testing.assert_allclose(mixed['cat'], mixed['dog'])\n\n    def test_raises_when_wrong_set_of_keys_passed_to_add(self):\n        r = MultiplexedReservoir(100)\n        samples = np.random.random_sample((10, 3))\n        r.add(dict(cat=samples, dog=samples))\n        self.assertRaises(\n            ValueError, lambda: r.add(dict(rat=samples, frog=samples)))\n\n    def test_raises_when_some_keys_have_mismatched_lengths(self):\n        r = MultiplexedReservoir(100)\n        samples = np.random.random_sample((10, 3))\n        self.assertRaises(\n            ValueError, lambda: r.add(dict(cat=samples, dog=samples[:-1])))\n\n    def test_raises_when_some_keys_have_mismatched_lengths_second_add(self):\n        r = MultiplexedReservoir(100)\n        samples = np.random.random_sample((10, 3))\n        r.add(dict(cat=samples, dog=samples))\n        self.assertRaises(\n            ValueError, lambda: r.add(dict(cat=samples, dog=samples[:-1])))\n\n    def test_get_returns_dict_with_user_specified_keys(self):\n        r = MultiplexedReservoir(100)\n        samples = np.random.random_sample((10, 3))\n        d = dict(cat=samples, dog=samples)\n        r.add(d)\n        mixed = r.get()\n        self.assertEqual(set(d.keys()), set(mixed.keys()))\n\n\nclass TestReservoirSampler(unittest2.TestCase):\n    def test_can_sample_from_one_dimensional_feature(self):\n        sampler = ReservoirSampler(nsamples=10)\n\n        frequency_dimension = FrequencyDimension(\n            LinearScale(FrequencyBand(100, 1000), 100))\n\n        samples = ArrayWithUnits(\n            np.ones((20, 100)),\n            [\n                TimeDimension(frequency=Seconds(1)),\n                frequency_dimension\n            ])\n\n        sampler._enqueue(samples, pusher=None)\n        reservoir = sampler._r\n        self.assertEqual((10, 100), reservoir.shape)\n        self.assertIsInstance(reservoir, ArrayWithUnits)\n        self.assertEqual(reservoir.dimensions[0], IdentityDimension())\n        self.assertEqual(reservoir.dimensions[1], frequency_dimension)\n\n    def test_can_wrap_samples(self):\n        sampler = ReservoirSampler(nsamples=10)\n\n        frequency_dimension = FrequencyDimension(\n            LinearScale(FrequencyBand(100, 1000), 100))\n\n        samples = ArrayWithUnits(\n            np.ones((2, 10, 100)),\n            [\n                TimeDimension(frequency=Seconds(10)),\n                TimeDimension(frequency=Seconds(1)),\n                frequency_dimension\n            ])\n\n        sampler._enqueue(samples, pusher=None)\n        reservoir = sampler._r\n        self.assertEqual((10, 10, 100), reservoir.shape)\n        self.assertIsInstance(reservoir, ArrayWithUnits)\n        self.assertEqual(reservoir.dimensions[0], IdentityDimension())\n        self.assertEqual(reservoir.dimensions[1], samples.dimensions[1])\n        self.assertEqual(reservoir.dimensions[2], samples.dimensions[2])\n\n    def test_can_dequeue_when_reservoir_is_full(self):\n        sampler = ReservoirSampler(nsamples=10)\n\n        frequency_dimension = FrequencyDimension(\n            LinearScale(FrequencyBand(100, 1000), 100))\n\n        samples = ArrayWithUnits(\n            np.ones((10, 10, 100)),\n            [\n                TimeDimension(frequency=Seconds(10)),\n                TimeDimension(frequency=Seconds(1)),\n                frequency_dimension\n            ])\n\n        sampler._enqueue(samples, pusher=None)\n        reservoir = sampler._dequeue()\n\n        self.assertEqual((10, 10, 100), reservoir.shape)\n        self.assertIsInstance(reservoir, ArrayWithUnits)\n        self.assertEqual(reservoir.dimensions[0], IdentityDimension())\n        self.assertEqual(reservoir.dimensions[1], samples.dimensions[1])\n        self.assertEqual(reservoir.dimensions[2], samples.dimensions[2])\n\n    def test_can_dequeue_when_reservoir_is_partially_full(self):\n        sampler = ReservoirSampler(nsamples=10)\n\n        frequency_dimension = FrequencyDimension(\n            LinearScale(FrequencyBand(100, 1000), 100))\n\n        samples = ArrayWithUnits(\n            np.ones((4, 10, 100)),\n            [\n                TimeDimension(frequency=Seconds(10)),\n                TimeDimension(frequency=Seconds(1)),\n                frequency_dimension\n            ])\n\n        sampler._enqueue(samples, pusher=None)\n        reservoir = sampler._dequeue()\n\n        self.assertEqual((4, 10, 100), reservoir.shape)\n        self.assertIsInstance(reservoir, ArrayWithUnits)\n        self.assertEqual(reservoir.dimensions[0], IdentityDimension())\n        self.assertEqual(reservoir.dimensions[1], samples.dimensions[1])\n        self.assertEqual(reservoir.dimensions[2], samples.dimensions[2])\n"""
zounds/learn/test_reshape.py,4,"b'import unittest2\nimport featureflow as ff\nimport numpy as np\n\nfrom zounds.util import simple_in_memory_settings\nfrom .preprocess import Reshape, PreprocessingPipeline\n\n\nclass ReshapeTests(unittest2.TestCase):\n\n    def do_setup(self, shape, new_shape):\n        @simple_in_memory_settings\n        class Model(ff.BaseModel):\n            flattened = ff.PickleFeature(\n                    Reshape,\n                    new_shape=new_shape,\n                    store=False)\n\n            pipeline = ff.PickleFeature(\n                    PreprocessingPipeline,\n                    needs=(flattened,),\n                    store=True)\n\n        training = np.random.random_sample(shape)\n        _id = Model.process(flattened=training)\n        model = Model(_id)\n        data = np.random.random_sample(shape)\n        transformed = model.pipeline.transform(data)\n        inverted = transformed.inverse_transform()\n        return data, transformed.data, inverted\n\n    def do_assertions(self, original_shape, new_shape, expected_shape):\n        data, transformed, inverted = self.do_setup(original_shape, new_shape)\n        self.assertEqual(expected_shape, transformed.shape)\n        self.assertEqual(original_shape, inverted.shape)\n        np.testing.assert_allclose(data.ravel(), transformed.ravel())\n        np.testing.assert_allclose(data.ravel(), inverted.ravel())\n\n    def test_reshape(self):\n        self.do_assertions(\n            original_shape=(10, 100),\n            new_shape=(10, 10),\n            expected_shape=(10, 10, 10))\n\n    def test_expand_multiple_dimensions(self):\n        self.do_assertions(\n            original_shape=(10, 40),\n            new_shape=(1, -1, 1),\n            expected_shape=(10, 1, 40, 1))\n\n    def test_expand_middle_dimension(self):\n        self.do_assertions(\n            original_shape=(10, 40),\n            new_shape=(1, -1),\n            expected_shape=(10, 1, 40))\n\n    def test_expand_last_dimension(self):\n        self.do_assertions(\n            original_shape=(10, 40),\n            new_shape=(-1, 1),\n            expected_shape=(10, 40, 1))\n\n    def test_flatten_one_dimensional(self):\n        self.do_assertions(\n            original_shape=(10,),\n            new_shape=tuple(),\n            expected_shape=(10,))\n\n    def test_flatten_two_dimensional(self):\n        self.do_assertions(\n            original_shape=(10, 40),\n            new_shape=(-1,),\n            expected_shape=(10, 40))\n\n    def test_flatten_three_dimensional(self):\n        self.do_assertions(\n            original_shape=(10, 40, 20),\n            new_shape=(-1,),\n            expected_shape=(10, 800))\n'"
zounds/learn/test_simhash.py,11,"b'import unittest2\nfrom zounds.util import simple_in_memory_settings\nimport featureflow as ff\nfrom .preprocess import SimHash, PreprocessingPipeline\nimport numpy as np\nfrom zounds.timeseries import TimeDimension, Seconds\nfrom zounds.spectral import GeometricScale, FrequencyDimension\nfrom zounds.core import IdentityDimension, ArrayWithUnits\n\n\n@simple_in_memory_settings\nclass SimHashPipeline(ff.BaseModel):\n    simhash = ff.PickleFeature(\n        SimHash,\n        bits=128)\n\n    pipeline = ff.PickleFeature(\n        PreprocessingPipeline,\n        needs=(simhash,),\n        store=True)\n\n\n@simple_in_memory_settings\nclass SimHashPipelineWithPackedBits(ff.BaseModel):\n    simhash = ff.PickleFeature(\n        SimHash,\n        packbits=True,\n        bits=1024)\n\n    pipeline = ff.PickleFeature(\n        PreprocessingPipeline,\n        needs=(simhash,),\n        store=True)\n\n\nclass SimHashTests(unittest2.TestCase):\n    def test_can_compute_hash_and_pack_bits(self):\n        training_data = np.random.normal(0, 1, size=(100, 9))\n        _id = SimHashPipelineWithPackedBits.process(simhash=training_data)\n        p = SimHashPipelineWithPackedBits(_id)\n        test_data = np.random.normal(0, 1, size=(1000, 9))\n        result = p.pipeline.transform(test_data).data\n        self.assertEqual((1000, 16), result.shape)\n        self.assertEqual(np.uint64, result.dtype)\n\n    def test_can_compute_hash_for_1d_vectors(self):\n        training_data = np.random.normal(0, 1, size=(100, 9))\n        _id = SimHashPipeline.process(simhash=training_data)\n        p = SimHashPipeline(_id)\n        test_data = np.random.normal(0, 1, size=(1000, 9))\n        result = p.pipeline.transform(test_data).data\n        self.assertEqual((1000, 128), result.shape)\n\n    def test_can_compute_hash_for_2d_vectors(self):\n        training_data = np.random.normal(0, 1, size=(100, 9, 7))\n        _id = SimHashPipeline.process(simhash=training_data)\n        p = SimHashPipeline(_id)\n        test_data = np.random.normal(0, 1, size=(1000, 9, 7))\n        result = p.pipeline.transform(test_data).data\n        self.assertEqual((1000, 128), result.shape)\n\n    def test_returns_array_with_units_where_possible_1d(self):\n        training_data = np.random.normal(0, 1, size=(100, 9))\n        _id = SimHashPipeline.process(simhash=training_data)\n        p = SimHashPipeline(_id)\n        td = TimeDimension(Seconds(1))\n        test_data = ArrayWithUnits(\n            np.random.normal(0, 1, size=(1000, 9)),\n            dimensions=(\n                td,\n                FrequencyDimension(GeometricScale(20, 20000, 0.1, 9)))\n        )\n        result = p.pipeline.transform(test_data).data\n        self.assertEqual((1000, 128), result.shape)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(result.dimensions[0], td)\n        self.assertIsInstance(result.dimensions[1], IdentityDimension)\n\n    def test_returns_array_with_units_where_possible_2d(self):\n        training_data = np.random.normal(0, 1, size=(100, 9, 7))\n        _id = SimHashPipeline.process(simhash=training_data)\n        p = SimHashPipeline(_id)\n        td1 = TimeDimension(Seconds(10))\n        td2 = TimeDimension(Seconds(1))\n        test_data = ArrayWithUnits(\n            np.random.normal(0, 1, size=(1000, 9, 7)),\n            dimensions=(\n                td1,\n                td2,\n                FrequencyDimension(GeometricScale(20, 20000, 0.1, 7)))\n        )\n        result = p.pipeline.transform(test_data).data\n        self.assertEqual((1000, 128), result.shape)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(result.dimensions[0], td1)\n        self.assertIsInstance(result.dimensions[1], IdentityDimension)\n'"
zounds/learn/test_sklearn_preprocessor.py,10,"b'import unittest2\nfrom .sklearn_preprocessor import SklearnModel\nfrom .preprocess import PreprocessingPipeline\nimport featureflow as ff\nimport numpy as np\nfrom zounds.util import simple_in_memory_settings\nfrom zounds.timeseries import TimeDimension, Milliseconds, Seconds\nfrom zounds.spectral import FrequencyBand, LinearScale, FrequencyDimension\nfrom zounds.core import ArrayWithUnits, IdentityDimension\n\n\nclass MockSklearnModel(object):\n    def __init__(self):\n        super(MockSklearnModel, self).__init__()\n        self.dims = None\n\n    def fit(self, data):\n        self.dims = data.shape[1:]\n        return self\n\n    def transform(self, data):\n        return np.zeros(data.shape)\n\n    def inverse_transform(self, data):\n        return np.zeros((data.shape[0],) + self.dims)\n\n\n@simple_in_memory_settings\nclass Document(ff.BaseModel):\n    l = ff.PickleFeature(\n        SklearnModel,\n        model=MockSklearnModel())\n\n    pipeline = ff.PickleFeature(\n        PreprocessingPipeline,\n        needs=(l,),\n        store=True)\n\n\nclass SklearnTests(unittest2.TestCase):\n    def test_should_handle_forward_transform_of_numpy_array(self):\n        training_data = np.zeros((10, 5, 3))\n        _id = Document.process(l=training_data)\n        doc = Document(_id)\n        test_data = np.zeros((11, 5, 3))\n        result = doc.pipeline.transform(test_data)\n        self.assertEqual((11, 15), result.data.shape)\n\n    def test_should_handle_backward_transform_of_numpy_array(self):\n        training_data = np.zeros((10, 5, 3))\n        _id = Document.process(l=training_data)\n        doc = Document(_id)\n        test_data = np.zeros((11, 5, 3))\n        result = doc.pipeline.transform(test_data)\n        inverted = result.inverse_transform()\n        self.assertEqual((11, 5, 3), inverted.shape)\n\n    def test_should_preserve_time_dimension_in_forward_transform(self):\n        td = TimeDimension(Seconds(1))\n        td2 = TimeDimension(Milliseconds(500))\n        fd = FrequencyDimension(LinearScale(FrequencyBand(10, 100), 3))\n        training_data = ArrayWithUnits(\n            np.zeros((10, 5, 3)), dimensions=(IdentityDimension(), td2, fd))\n        _id = Document.process(l=training_data)\n        doc = Document(_id)\n        test_data = ArrayWithUnits(\n            np.zeros((11, 5, 3)), dimensions=(td, td2, fd))\n        result = doc.pipeline.transform(test_data)\n        self.assertEqual((11, 15), result.data.shape)\n        self.assertIsInstance(result.data, ArrayWithUnits)\n        self.assertEqual(td, result.data.dimensions[0])\n        self.assertEqual(IdentityDimension(), result.data.dimensions[1])\n\n    def test_should_restore_all_dimensions_in_backward_transform(self):\n        td = TimeDimension(Seconds(1))\n        td2 = TimeDimension(Milliseconds(500))\n        fd = FrequencyDimension(LinearScale(FrequencyBand(10, 100), 3))\n        training_data = ArrayWithUnits(\n            np.zeros((10, 5, 3)), dimensions=(IdentityDimension(), td2, fd))\n        _id = Document.process(l=training_data)\n        doc = Document(_id)\n        test_data = ArrayWithUnits(\n            np.zeros((11, 5, 3)), dimensions=(td, td2, fd))\n        result = doc.pipeline.transform(test_data)\n        inverted = result.inverse_transform()\n        self.assertEqual((11, 5, 3), inverted.shape)\n        self.assertIsInstance(inverted, ArrayWithUnits)\n        self.assertEqual(td, inverted.dimensions[0])\n        self.assertEqual(td2, inverted.dimensions[1])\n        self.assertEqual(fd, inverted.dimensions[2])'"
zounds/learn/test_slicer.py,15,"b'import unittest2\nimport featureflow as ff\nimport numpy as np\n\nfrom zounds.util import simple_in_memory_settings\nfrom .preprocess import Slicer, PreprocessingPipeline\n\nfrom zounds.core import IdentityDimension, ArrayWithUnits\nfrom zounds.spectral import FrequencyBand, LinearScale, FrequencyDimension\nfrom zounds.timeseries import TimeDimension, Seconds\n\n\nclass SlicerTests(unittest2.TestCase):\n    def get_model(self, slicex):\n        @simple_in_memory_settings\n        class Model(ff.BaseModel):\n            sliced = ff.PickleFeature(\n                    Slicer,\n                    slicex=slicex,\n                    store=False)\n\n            pipeline = ff.PickleFeature(\n                    PreprocessingPipeline,\n                    needs=(sliced,),\n                    store=True)\n\n        return Model\n\n    def test_inversion_maintains_data_type(self):\n        training = np.ones((100, 30), dtype=np.complex128)\n        Model = self.get_model(slicex=slice(10, 20))\n        _id = Model.process(sliced=training)\n        model = Model(_id)\n        data = np.ones((100, 30), dtype=np.complex128)\n        transformed = model.pipeline.transform(data)\n        self.assertEqual((100, 10), transformed.data.shape)\n        inverted = transformed.inverse_transform()\n        self.assertEqual(np.complex128, inverted.dtype)\n\n    def test_can_slice_array_with_slice_instance(self):\n        training = np.ones((100, 30))\n        Model = self.get_model(slicex=slice(10, 20))\n        _id = Model.process(sliced=training)\n        model = Model(_id)\n        data = np.ones((100, 30))\n        transformed = model.pipeline.transform(data)\n        self.assertEqual((100, 10), transformed.data.shape)\n        inverted = transformed.inverse_transform()\n        self.assertEqual((100, 30), inverted.shape)\n        np.testing.assert_allclose(transformed.data, 1)\n        np.testing.assert_allclose(inverted[:, :10], 0)\n        np.testing.assert_allclose(inverted[:, 20:], 0)\n\n    def test_can_slice_multi_dimensional(self):\n        training = np.ones((100, 30, 50))\n        Model = self.get_model(slicex=slice(10, 20))\n        _id = Model.process(sliced=training)\n        model = Model(_id)\n        data = np.ones((100, 30, 50))\n        transformed = model.pipeline.transform(data)\n        self.assertEqual((100, 30, 10), transformed.data.shape)\n        inverted = transformed.inverse_transform()\n        self.assertEqual((100, 30, 50), inverted.shape)\n        np.testing.assert_allclose(transformed.data, 1)\n        np.testing.assert_allclose(inverted[..., :10], 0)\n        np.testing.assert_allclose(inverted[..., 20:], 0)\n\n    def test_can_invert_array_with_units(self):\n        td = TimeDimension(Seconds(1))\n        fd = FrequencyDimension(LinearScale(FrequencyBand(0, 20000), 100))\n        dimensions = [IdentityDimension(), td, fd]\n        training = ArrayWithUnits(np.zeros((10, 5, 100)), dimensions)\n        Model = self.get_model(slicex=FrequencyBand(1000, 10000))\n        _id = Model.process(sliced=training)\n        model = Model(_id)\n        data = ArrayWithUnits(np.ones((2, 5, 100)), dimensions)\n        transformed = model.pipeline.transform(data)\n        inverted = transformed.inverse_transform()\n        self.assertEqual((2, 5, 100), inverted.shape)\n        self.assertEqual(IdentityDimension(), inverted.dimensions[0])\n        self.assertEqual(td, inverted.dimensions[1])\n        self.assertEqual(fd, inverted.dimensions[2])\n'"
zounds/learn/test_unitnorm.py,17,"b""import featureflow as ff\nimport unittest2\nimport numpy as np\n\nfrom zounds.util import simple_in_memory_settings\nfrom .preprocess import UnitNorm, PreprocessingPipeline\nfrom .learn import KMeans\nfrom zounds.timeseries import Seconds, TimeDimension\nfrom zounds.spectral import FrequencyBand, LinearScale, FrequencyDimension\nfrom zounds.core import ArrayWithUnits, IdentityDimension\n\n\nclass UnitNormTests(unittest2.TestCase):\n    def get_model(self):\n        @simple_in_memory_settings\n        class Model(ff.BaseModel):\n            unitnorm = ff.PickleFeature(\n                    UnitNorm,\n                    store=False)\n\n            kmeans = ff.PickleFeature(\n                    KMeans,\n                    centroids=10,\n                    needs=unitnorm,\n                    store=False)\n\n            pipeline = ff.PickleFeature(\n                    PreprocessingPipeline,\n                    needs=(unitnorm, kmeans),\n                    store=True)\n\n        return Model\n\n    def test_can_process_2d_samples(self):\n        @simple_in_memory_settings\n        class Model(ff.BaseModel):\n            unitnorm = ff.PickleFeature(\n                    UnitNorm,\n                    store=False)\n\n            pipeline = ff.PickleFeature(\n                    PreprocessingPipeline,\n                    needs=(unitnorm,),\n                    store=True)\n\n        data_shape = (10, 3, 4)\n        training = np.random.random_sample(data_shape)\n        _id = Model.process(unitnorm=training)\n        model = Model(_id)\n\n        data = np.random.random_sample(data_shape)\n        result = model.pipeline.transform(data)\n        self.assertEqual(data_shape, result.data.shape)\n        inverted = result.inverse_transform()\n        self.assertEqual(data.shape, inverted.shape)\n        np.testing.assert_allclose(inverted, data)\n\n    def test_can_process_3d_samples(self):\n        @simple_in_memory_settings\n        class Model(ff.BaseModel):\n            unitnorm = ff.PickleFeature(\n                    UnitNorm,\n                    store=False)\n\n            pipeline = ff.PickleFeature(\n                    PreprocessingPipeline,\n                    needs=(unitnorm,),\n                    store=True)\n\n        data_shape = (10, 3, 4, 2)\n        training = np.random.random_sample(data_shape)\n        _id = Model.process(unitnorm=training)\n        model = Model(_id)\n\n        data = np.random.random_sample(data_shape)\n        result = model.pipeline.transform(data)\n        self.assertEqual(data_shape, result.data.shape)\n        inverted = result.inverse_transform()\n        self.assertEqual(data.shape, inverted.shape)\n        np.testing.assert_allclose(inverted, data)\n\n    def test_can_process_samples_with_negative_values(self):\n        @simple_in_memory_settings\n        class Model(ff.BaseModel):\n            unitnorm = ff.PickleFeature(\n                UnitNorm,\n                store=False)\n\n            pipeline = ff.PickleFeature(\n                PreprocessingPipeline,\n                needs=(unitnorm,),\n                store=True)\n\n        data_shape = (10, 3, 4, 2)\n        training = np.random.random_sample(data_shape) - 0.5\n        _id = Model.process(unitnorm=training)\n        model = Model(_id)\n\n        data = np.random.random_sample(data_shape) - 0.5\n        result = model.pipeline.transform(data)\n        self.assertEqual(data_shape, result.data.shape)\n        inverted = result.inverse_transform()\n        self.assertEqual(data.shape, inverted.shape)\n        np.testing.assert_allclose(inverted, data)\n\n    def test_forward_transform_returns_array_with_units_where_possible(self):\n        # train the model on random data\n        training = np.random.random_sample((100, 30))\n        Model = self.get_model()\n        _id = Model.process(unitnorm=training)\n        model = Model(_id)\n\n        # create a time-frequency representation\n        scale = LinearScale(FrequencyBand(20, 20000), 30)\n        data = ArrayWithUnits(\n                np.random.random_sample((10, 30)),\n                [TimeDimension(Seconds(1)), FrequencyDimension(scale)])\n\n        # do a forward pass\n        transformed = model.pipeline.transform(data).data\n\n        self.assertIsInstance(transformed, ArrayWithUnits)\n        self.assertEqual(2, len(transformed.dimensions))\n        self.assertIsInstance(transformed.dimensions[0], TimeDimension)\n        self.assertIsInstance(transformed.dimensions[1], IdentityDimension)\n\n    def invert_and_assert_class(self, data):\n        training = np.random.random_sample((100, 30))\n        Model = self.get_model()\n        _id = Model.process(unitnorm=training)\n        model = Model(_id)\n        transformed = model.pipeline.transform(data)\n        inverted = transformed.inverse_transform()\n        self.assertEqual(data.__class__, inverted.__class__)\n        return inverted\n\n    def test_inversion_returns_array(self):\n        data = np.random.random_sample((10, 30))\n        self.invert_and_assert_class(data)\n\n    def test_inversion_returns_time_series(self):\n        data = np.random.random_sample((33, 30))\n        ts = ArrayWithUnits(\n                data, [TimeDimension(Seconds(1)), IdentityDimension()])\n        inverted = self.invert_and_assert_class(ts)\n        self.assertEqual(Seconds(1), inverted.dimensions[0].frequency)\n\n    def test_inversion_returns_time_frequency_representation(self):\n        data = np.random.random_sample((33, 30))\n        scale = LinearScale(FrequencyBand(20, 20000), 30)\n        tf = ArrayWithUnits(data, [\n            TimeDimension(Seconds(1), Seconds(2)),\n            FrequencyDimension(scale)])\n        inverted = self.invert_and_assert_class(tf)\n        self.assertEqual(Seconds(1), inverted.dimensions[0].frequency)\n        self.assertEqual(Seconds(2), inverted.dimensions[0].duration)\n        self.assertEqual(scale, inverted.dimensions[1].scale)\n\n    def test_can_easily_get_at_codebook(self):\n        # KLUDGE: This doesn't belong here, but putting it here is convenient\n        # right now\n        training = np.random.random_sample((100, 30))\n        Model = self.get_model()\n        _id = Model.process(unitnorm=training)\n        model = Model(_id)\n        self.assertIsInstance(model.pipeline[-1].codebook, np.ndarray)\n"""
zounds/learn/test_util.py,3,"b'import unittest2\nimport torch\nfrom .util import batchwise_unit_norm\nimport numpy as np\n\n\nclass BatchwiseUnitNormTests(unittest2.TestCase):\n    def test_all_elements_have_unit_norm(self):\n        t = torch.FloatTensor(100, 5).normal_(0, 1)\n        t = batchwise_unit_norm(t).data.numpy()\n        norms = np.linalg.norm(t, axis=1)\n        np.testing.assert_allclose(norms, 1, rtol=1e-6)\n\n    def test_maintains_correct_shape_2d(self):\n        t = torch.FloatTensor(100, 5).normal_(0, 1)\n        t = batchwise_unit_norm(t).data.numpy()\n        self.assertEqual((100, 5), t.shape)\n\n    def test_maintains_correct_shape_3d(self):\n        t = torch.FloatTensor(100, 5, 3).normal_(0, 1)\n        t = batchwise_unit_norm(t).data.numpy()\n        self.assertEqual((100, 5, 3), t.shape)\n\n    def test_does_not_introduce_nans(self):\n        t = torch.FloatTensor(100, 5, 3).zero_()\n        t = batchwise_unit_norm(t).data.numpy()\n        self.assertFalse(np.any(np.isnan(t)))\n'"
zounds/learn/test_weighted.py,6,"b'import unittest2\nimport featureflow as ff\nfrom .preprocess import Weighted, InstanceScaling, PreprocessingPipeline\nfrom zounds.spectral import \\\n    AWeighting, GeometricScale, FrequencyDimension, FrequencyAdaptive\nfrom zounds.core import ArrayWithUnits\nfrom zounds.timeseries import TimeDimension, Seconds\nfrom zounds.util import simple_in_memory_settings\nimport numpy as np\n\n\nclass WeightedTests(unittest2.TestCase):\n\n    def test_can_apply_weighting(self):\n\n        @simple_in_memory_settings\n        class Model(ff.BaseModel):\n            weighted = ff.PickleFeature(\n                Weighted,\n                weighting=AWeighting(),\n                store=False)\n\n            scaled = ff.PickleFeature(\n                InstanceScaling,\n                needs=weighted,\n                store=False)\n\n            pipeline = ff.PickleFeature(\n                PreprocessingPipeline,\n                needs=(weighted, scaled),\n                store=True)\n\n        training = ArrayWithUnits(\n            np.random.random_sample((100, 15)),\n            dimensions=[\n                TimeDimension(Seconds(1)),\n                FrequencyDimension(GeometricScale(100, 1000, 0.1, 15))\n            ]\n        )\n\n        _id = Model.process(weighted=training)\n        model = Model(_id)\n\n        test = ArrayWithUnits(\n            np.random.random_sample((10, 15)),\n            dimensions=[\n                TimeDimension(Seconds(1)),\n                FrequencyDimension(GeometricScale(100, 1000, 0.1, 15))\n            ]\n        )\n\n        result = model.pipeline.transform(test)\n        inverted = result.inverse_transform()\n\n        np.testing.assert_allclose(test, inverted)\n\n    def test_can_apply_weighting_to_frequency_adaptive_transform(self):\n        @simple_in_memory_settings\n        class Model(ff.BaseModel):\n            weighted = ff.PickleFeature(\n                Weighted,\n                weighting=AWeighting(),\n                store=False)\n\n            scaled = ff.PickleFeature(\n                InstanceScaling,\n                needs=weighted,\n                store=False)\n\n            pipeline = ff.PickleFeature(\n                PreprocessingPipeline,\n                needs=(weighted, scaled),\n                store=True)\n\n        scale = GeometricScale(100, 1000, 0.1, 15)\n\n        training = FrequencyAdaptive(\n            [np.ones((100, x)) for x in range(1, len(scale) + 1)],\n            time_dimension=TimeDimension(Seconds(1)),\n            scale=scale)\n\n        _id = Model.process(weighted=training)\n        model = Model(_id)\n\n        test = FrequencyAdaptive(\n            [np.ones((10, x)) for x in range(1, len(scale) + 1)],\n            time_dimension=TimeDimension(Seconds(1)),\n            scale=scale)\n\n        result = model.pipeline.transform(test)\n        inverted = result.inverse_transform()\n\n        self.assertIsInstance(inverted, FrequencyAdaptive)\n        np.testing.assert_allclose(test, inverted)\n\n\n'"
zounds/learn/trainer.py,2,"b""import numpy as np\nfrom torch.autograd import Variable\nimport torch\n\n\nclass Trainer(object):\n    def __init__(self, epochs, batch_size, checkpoint_epochs=1):\n        super(Trainer, self).__init__()\n        self.checkpoint_epochs = checkpoint_epochs\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self._batch_complete_callbacks = dict()\n        self._current_epoch = 0\n        self.use_cuda = False\n        self.network = None\n\n    def _cuda(self, device=None):\n        pass\n\n    def cuda(self, device=None):\n        self.use_cuda = True\n        self.network = self.network.cuda(device=device)\n        self._cuda(device=device)\n        return self\n\n    def _variable(self, x, *args, **kwargs):\n\n        if isinstance(x, np.ndarray):\n            x = torch.from_numpy(x)\n\n        v = Variable(x, *args, **kwargs)\n\n        if self.use_cuda:\n            v = v.cuda()\n\n        return v\n\n    def _tensor(self, shape):\n        ft = torch.cuda.FloatTensor if self.use_cuda else torch.FloatTensor\n        return ft(*shape)\n\n    def _log(self, *args, **kwargs):\n        if kwargs['batch'] % 10:\n            return\n        print(kwargs)\n\n    def _zero_grad(self):\n        self.network.zero_grad()\n\n    def _minibatch(self, data):\n        import torch\n        indices = np.random.randint(0, len(data), self.batch_size)\n        batch = torch.from_numpy(data[indices, ...])\n        return self._variable(batch)\n\n    def _training_step(self, epoch, batch, data):\n        raise NotImplementedError()\n\n    def train(self, data):\n        start = self._current_epoch\n        stop = self._current_epoch + self.checkpoint_epochs\n\n        for epoch in range(start, stop):\n            if epoch > self.epochs:\n                break\n\n            for batch in range(0, len(data), self.batch_size):\n                results = self._training_step(epoch, batch, data)\n                results.update(epoch=epoch, batch=batch, network=self.network)\n                self.on_batch_complete(**results)\n\n            self._current_epoch += 1\n\n        return self.network\n\n    def register_batch_complete_callback(self, callback):\n        self._batch_complete_callbacks[id(callback)] = callback\n\n    def unregister_batch_complete_callback(self, callback):\n        try:\n            del self._batch_complete_callbacks[id(callback)]\n        except KeyError:\n            # the callback was never registered\n            pass\n\n    def on_batch_complete(self, *args, **kwargs):\n        for callback in self._batch_complete_callbacks.values():\n            callback(*args, **kwargs)\n"""
zounds/learn/util.py,1,"b'\nimport numpy as np\nimport featureflow as ff\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\nimport hashlib\n\n\ndef simple_settings(cls):\n    """"""\n    Create sane default persistence settings for learning pipelines\n    :param cls: The class to decorate\n    """"""\n\n    class Settings(ff.PersistenceSettings):\n        _id = cls.__name__\n        id_provider = ff.StaticIdProvider(_id)\n        key_builder = ff.StringDelimitedKeyBuilder()\n        database = ff.FileSystemDatabase(\n            path=_id, key_builder=key_builder, createdirs=True)\n\n    class Model(cls, Settings):\n        pass\n\n    Model.__name__ = cls.__name__\n    Model.__module__ = cls.__module__\n    return Model\n\n\ndef object_store_pipeline_settings(container, region, username, api_key):\n    def decorator(cls):\n        class Settings(ff.PersistenceSettings):\n            _id = cls.__name__\n            id_provider = ff.StaticIdProvider(_id)\n            key_builder = ff.StringDelimitedKeyBuilder()\n            database = ff.ObjectStoreDatabase(\n                container, username, api_key, region, key_builder=key_builder)\n\n        class Model(cls, Settings):\n            pass\n\n        Model.__name__ = cls.__name__\n        Model.__module__ = cls.__module__\n        return Model\n\n    return decorator\n\n\ndef trainable_parameters(model):\n    return [x for x in model.parameters() if x.requires_grad]\n\n\ndef model_hash(model):\n    h = hashlib.md5()\n    h.update(str(model))\n    for p in model.parameters():\n        h.update(p.data.cpu().numpy())\n    return h.hexdigest()\n\n\ndef gradients(network):\n    for n, p in network.named_parameters():\n        g = p.grad\n        if g is None:\n            continue\n        yield n, g.min().data[0], g.max().data[0], g.mean().data[0]\n\n\ndef to_var(x, volatile=False):\n    t = torch.from_numpy(x)\n    v = Variable(t, volatile=volatile).cuda()\n    return v\n\n\ndef from_var(x):\n    return x.data.cpu().numpy()\n\n\ndef try_network(network, x, **kwargs):\n    network_is_cuda = next(network.parameters()).is_cuda\n\n    x = Variable(torch.from_numpy(x), volatile=True)\n    if network_is_cuda:\n        x = x.cuda()\n\n    result = network(x, **kwargs)\n    return result\n\n\ndef apply_network(network, x, chunksize=None):\n    """"""\n    Apply a pytorch network, potentially in chunks\n    """"""\n    network_is_cuda = next(network.parameters()).is_cuda\n\n    x = torch.from_numpy(x)\n\n    with torch.no_grad():\n        if network_is_cuda:\n            x = x.cuda()\n\n        if chunksize is None:\n            return from_var(network(x))\n\n        return np.concatenate(\n            [from_var(network(x[i: i + chunksize]))\n             for i in range(0, len(x), chunksize)])\n\n\ndef sample_norm(x):\n    """"""\n    pixel norm as described in section 4.2 here:\n    https://arxiv.org/pdf/1710.10196.pdf\n    """"""\n    original = x\n    # square\n    x = x ** 2\n    # feature-map-wise sum\n    x = torch.sum(x, dim=1)\n    # scale by number of feature maps\n    x *= 1.0 / original.shape[1]\n    x += 10e-8\n    x = torch.sqrt(x)\n    return original / x.view(-1, 1, x.shape[-1])\n\n\ndef batchwise_unit_norm(x, epsilon=1e-8):\n    batch_size = x.shape[0]\n    flattened = x.view(batch_size, -1)\n    norm = torch.norm(flattened, dim=1, keepdim=True)\n    expanded = norm.view(batch_size, *((1,) * (x.dim() - 1)))\n    normed = x / (expanded + epsilon)\n    return normed\n\n\ndef batchwise_mean_std_normalization(x, epsilon=1e-8):\n    orig_shape = x.shape\n    x = x.view(x.shape[0], -1)\n    x = x - x.mean(dim=1, keepdim=True)\n    x = x / (x.std(dim=1, keepdim=True) + epsilon)\n    x = x.view(orig_shape)\n    return x\n\n\ndef feature_map_size(inp, kernel, stride=1, padding=0):\n    return ((inp - kernel + (2 * padding)) / stride) + 1\n\n\nclass ConvLayer(nn.Module):\n    def __init__(\n            self,\n            layer_type,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            activation=lambda x: F.leaky_relu(x, 0.2),\n            dropout=True,\n            batch_norm=True,\n            dilation=1,\n            sample_norm=False):\n\n        super(ConvLayer, self).__init__()\n        self.sample_norm = sample_norm\n        self.dropout = dropout\n        self.l1 = layer_type(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            bias=False,\n            dilation=dilation)\n\n        self.bn = None\n\n        if batch_norm:\n            if \'1d\' in layer_type.__name__:\n                self.bn = nn.BatchNorm1d(out_channels)\n            else:\n                self.bn = nn.BatchNorm2d(out_channels)\n\n        self.activation = activation\n\n    @property\n    def out_channels(self):\n        return self.l1.out_channels\n\n    @property\n    def in_channels(self):\n        return self.l1.in_channels\n\n    @property\n    def kernel_size(self):\n        return self.l1.kernel_size\n\n    @property\n    def stride(self):\n        return self.l1.stride\n\n    @property\n    def padding(self):\n        return self.l1.padding\n\n    def forward(self, x):\n        x = self.l1(x)\n\n        if self.sample_norm:\n            x = sample_norm(x)\n        elif self.bn:\n            x = self.bn(x)\n\n        if self.activation:\n            x = self.activation(x)\n\n        if self.dropout:\n            x = F.dropout(x, 0.2, self.training)\n        return x\n\n\nclass Conv1d(ConvLayer):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dropout=True,\n            batch_norm=True,\n            dilation=1,\n            sample_norm=False,\n            activation=lambda x: F.leaky_relu(x, 0.2)):\n        super(Conv1d, self).__init__(\n            nn.Conv1d,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            activation=activation,\n            dropout=dropout,\n            batch_norm=batch_norm,\n            dilation=dilation,\n            sample_norm=sample_norm)\n\n\nclass ConvTranspose1d(ConvLayer):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            activation=lambda x: F.leaky_relu(x, 0.2),\n            dropout=True,\n            batch_norm=True,\n            dilation=1,\n            sample_norm=False):\n        super(ConvTranspose1d, self).__init__(\n            nn.ConvTranspose1d,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            activation=activation,\n            dropout=dropout,\n            batch_norm=batch_norm,\n            dilation=dilation,\n            sample_norm=sample_norm)\n\n\nclass Conv2d(ConvLayer):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dropout=True,\n            batch_norm=True,\n            dilation=1,\n            sample_norm=False,\n            activation=lambda x: F.leaky_relu(x, 0.2), ):\n        super(Conv2d, self).__init__(\n            nn.Conv2d,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            activation=activation,\n            dropout=dropout,\n            batch_norm=batch_norm,\n            dilation=dilation,\n            sample_norm=sample_norm)\n\n\nclass ConvTranspose2d(ConvLayer):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            activation=lambda x: F.leaky_relu(x, 0.2),\n            dropout=True,\n            batch_norm=True,\n            dilation=1,\n            sample_norm=False):\n        super(ConvTranspose2d, self).__init__(\n            nn.ConvTranspose2d,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            activation=activation,\n            dropout=dropout,\n            batch_norm=batch_norm,\n            dilation=dilation,\n            sample_norm=sample_norm)\n'"
zounds/learn/wgan.py,2,"b'from .trainer import Trainer\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\n\nclass WassersteinGanTrainer(Trainer):\n    """"""\n    Args:\n        network (nn.Module): the network to train\n        latent_dimension (tuple): A tuple that defines the shape of the latent\n            dimension (noise) that is the generator\'s input\n        n_critic_iterations (int): The number of minibatches the critic sees\n            for every minibatch the generator sees\n        epochs: The total number of passes over the training set\n        batch_size: The size of a minibatch\n        preprocess_minibatch (function): function that takes the current\n            epoch, and a minibatch, and mutates the minibatch\n        kwargs_factory (callable): function that takes the current epoch and\n            outputs args to pass to the generator and discriminator\n    """"""\n\n    def __init__(\n            self,\n            network,\n            latent_dimension,\n            n_critic_iterations,\n            epochs,\n            batch_size,\n            preprocess_minibatch=None,\n            kwargs_factory=None,\n            debug_gradient=False,\n            checkpoint_epochs=1):\n\n        super(WassersteinGanTrainer, self).__init__(epochs, batch_size)\n        self.checkpoint_epochs = checkpoint_epochs\n        self.debug_gradient = debug_gradient\n        self.arg_maker = kwargs_factory\n        self.preprocess = preprocess_minibatch\n        self.n_critic_iterations = n_critic_iterations\n        self.latent_dimension = latent_dimension\n        self.network = network\n        self.critic = network.discriminator\n        self.generator = network.generator\n        self.samples = None\n        self.register_batch_complete_callback(self._log)\n        self.generator_optim = None\n        self.critic_optim = None\n\n    def _log(self, *args, **kwargs):\n        if kwargs[\'batch\'] % 10:\n            return\n        msg = \'Epoch {epoch}, batch {batch}, generator {generator_score}, \' \\\n              \'real {real_score}, critic {critic_loss}\'\n        print(msg.format(**kwargs))\n\n    def _minibatch(self, data):\n        indices = np.random.randint(0, len(data), self.batch_size)\n        return data[indices, ...]\n\n    def _gradient_penalty(self, real_samples, fake_samples, kwargs):\n        """"""\n        Compute the norm of the gradients for each sample in a batch, and\n        penalize anything on either side of unit norm\n        """"""\n        import torch\n        from torch.autograd import Variable, grad\n\n        real_samples = real_samples.view(fake_samples.shape)\n\n        subset_size = real_samples.shape[0]\n\n        real_samples = real_samples[:subset_size]\n        fake_samples = fake_samples[:subset_size]\n\n        alpha = torch.rand(subset_size)\n        if self.use_cuda:\n            alpha = alpha.cuda()\n        alpha = alpha.view((-1,) + ((1,) * (real_samples.dim() - 1)))\n\n        interpolates = alpha * real_samples + ((1 - alpha) * fake_samples)\n        interpolates = Variable(interpolates, requires_grad=True)\n        if self.use_cuda:\n            interpolates = interpolates.cuda()\n\n        d_output = self.critic(interpolates, **kwargs)\n\n        grad_ouputs = torch.ones(d_output.size())\n        if self.use_cuda:\n            grad_ouputs = grad_ouputs.cuda()\n\n        gradients = grad(\n            outputs=d_output,\n            inputs=interpolates,\n            grad_outputs=grad_ouputs,\n            create_graph=True,\n            retain_graph=True,\n            only_inputs=True)[0]\n        return ((gradients.norm(2, dim=1) - 1) ** 2).mean() * 10\n\n    def freeze_generator(self):\n        for p in self.generator.parameters():\n            p.requires_grad = False\n\n    def unfreeze_generator(self):\n        for p in self.generator.parameters():\n            p.requires_grad = True\n\n    def freeze_discriminator(self):\n        for p in self.critic.parameters():\n            p.requires_grad = False\n\n    def unfreeze_discriminator(self):\n        for p in self.critic.parameters():\n            p.requires_grad = True\n\n    def _debug_network_gradient(self, network):\n        if not self.debug_gradient:\n            return\n\n        for n, p in network.named_parameters():\n            g = p.grad\n            if g is not None:\n                print((n, g.min().data[0], g.max().data[0], g.mean().data[0]))\n\n    def zero_generator_gradients(self):\n        self._debug_network_gradient(self.generator)\n        self.generator.zero_grad()\n\n    def zero_discriminator_gradients(self):\n        self._debug_network_gradient(self.critic)\n        self.critic.zero_grad()\n\n    def _init_optimizers(self):\n        if self.generator_optim is None or self.critic_optim is None:\n            from torch.optim import Adam\n            trainable_generator_params = (\n                p for p in self.generator.parameters() if p.requires_grad)\n            trainable_critic_params = (\n                p for p in self.critic.parameters() if p.requires_grad)\n\n            self.generator_optim = Adam(\n                trainable_generator_params, lr=0.0001, betas=(0, 0.9))\n            self.critic_optim = Adam(\n                trainable_critic_params, lr=0.0001, betas=(0, 0.9))\n\n    def _cuda(self, device=None):\n        self.generator = self.generator.cuda()\n        self.critic = self.critic.cuda()\n\n    def train(self, data):\n\n        self.network.train()\n        self.unfreeze_discriminator()\n        self.unfreeze_generator()\n\n        data = data.astype(np.float32)\n\n        noise_shape = (self.batch_size,) + self.latent_dimension\n        noise = self._tensor(noise_shape)\n\n        self._init_optimizers()\n\n        start = self._current_epoch\n        stop = self._current_epoch + self.checkpoint_epochs\n\n        for epoch in range(start, stop):\n            if epoch >= self.epochs:\n                break\n\n            if self.arg_maker:\n                kwargs = self.arg_maker(epoch)\n            else:\n                kwargs = dict()\n\n            for i in range(0, len(data), self.batch_size):\n                self.zero_generator_gradients()\n                self.zero_discriminator_gradients()\n\n                self.freeze_generator()\n                self.unfreeze_discriminator()\n\n                for c in range(self.n_critic_iterations):\n\n                    self.zero_discriminator_gradients()\n\n                    input_v = self._variable(self._minibatch(data))\n\n                    if self.preprocess:\n                        input_v = self.preprocess(epoch, input_v)\n\n                    d_real = self.critic.forward(input_v, **kwargs)\n\n                    # train discriminator on fake data\n                    noise.normal_(0, 1)\n                    noise_v = Variable(noise, volatile=True)\n                    fake = Variable(\n                        self.generator.forward(noise_v, **kwargs).data)\n\n                    if self.preprocess:\n                        fake = self.preprocess(epoch, fake)\n\n                    d_fake = self.critic.forward(fake, **kwargs)\n\n                    real_mean = torch.mean(d_real)\n                    fake_mean = torch.mean(d_fake)\n                    gp = self._gradient_penalty(input_v.data, fake.data, kwargs)\n                    d_loss = (fake_mean - real_mean) + gp\n                    d_loss.backward()\n                    self.critic_optim.step()\n\n                self.zero_discriminator_gradients()\n                self.zero_generator_gradients()\n\n                self.unfreeze_generator()\n                self.freeze_discriminator()\n\n                # train generator\n                noise.normal_(0, 1)\n                noise_v = Variable(noise)\n                fake = self.generator.forward(noise_v, **kwargs)\n\n                if self.preprocess:\n                    fake = self.preprocess(epoch, fake)\n\n                self.samples = fake\n\n                d_fake = self.critic.forward(fake, **kwargs)\n                g_loss = -torch.mean(d_fake)\n                g_loss.backward()\n                self.generator_optim.step()\n\n                gl = g_loss.data.item()\n                dl = d_loss.data.item()\n                rl = real_mean.data.item()\n\n                self.on_batch_complete(\n                    epoch=epoch,\n                    batch=i,\n                    generator_score=gl,\n                    real_score=rl,\n                    critic_loss=dl,\n                    samples=self.samples,\n                    network=self.network)\n\n            self._current_epoch += 1\n\n        return self.network\n'"
zounds/loudness/__init__.py,0,"b'from .loudness import \\\n    log_modulus, inverse_log_modulus, decibel, mu_law, MuLaw, LogModulus, \\\n    inverse_mu_law, unit_scale, instance_scale, inverse_one_hot\n'"
zounds/loudness/loudness.py,13,"b'\nimport numpy as np\nfrom featureflow import Node\n\n\ndef log_modulus(x):\n    return np.sign(x) * np.log(np.abs(x) + 1)\n\n\ndef inverse_log_modulus(x):\n    return (np.exp(np.abs(x)) - 1) * np.sign(x)\n\n\ndef decibel(x):\n    return 20 * np.log10(x)\n\n\ndef mu_law(x, mu=255):\n    s = np.sign(x)\n    x = np.abs(x)\n    return s * (np.log(1 + (mu * x)) / np.log(1 + mu))\n\n\ndef inverse_mu_law(x, mu=255):\n    s = np.sign(x)\n    x = np.abs(x)\n    x *= np.log(1 + mu)\n    x = (np.exp(x) - 1) / mu\n    return x * s\n\n\ndef inverse_one_hot(x, axis=-1):\n    n_categories = x.shape[axis]\n    indices = np.argmax(x, axis=axis).astype(np.float32)\n    indices /= float(n_categories)\n    indices = (indices - 0.5) * 2\n    return indices\n\n\ndef instance_scale(x, axis=-1, epsilon=1e-8, return_maxes=False):\n    mx = np.abs(x).max(axis=axis, keepdims=True)\n    scaled = x / (mx + epsilon)\n    if return_maxes:\n        return mx, scaled\n    else:\n        return scaled\n\n\ndef unit_scale(x, axis=None):\n    scaled = x - x.min(axis=axis, keepdims=True)\n    mx = scaled.max(axis=axis, keepdims=True)\n    scaled = np.divide(scaled, mx, where=max != 0)\n    return scaled\n\n\nclass MuLaw(Node):\n    def __init__(self, mu=255, needs=None):\n        super(MuLaw, self).__init__(needs=needs)\n        self.mu = mu\n\n    def _process(self, data):\n        yield mu_law(data, mu=self.mu)\n\n\nclass LogModulus(Node):\n    def __init__(self, factor=1, needs=None):\n        super(LogModulus, self).__init__(needs=needs)\n        self.factor = factor\n\n    def _process(self, data):\n        yield log_modulus(data * self.factor)\n'"
zounds/loudness/test_loudness.py,12,"b'import unittest2\nfrom .loudness import mu_law, inverse_mu_law, inverse_one_hot, instance_scale\nimport numpy as np\n\n\nclass TestLoudness(unittest2.TestCase):\n    def test_can_invert_mu_law(self):\n        a = np.random.normal(0, 1, (100, 4))\n        adjusted = mu_law(a)\n        inverted = inverse_mu_law(adjusted)\n        np.testing.assert_allclose(a, inverted)\n\n\nclass TestInverseOneHot(unittest2.TestCase):\n    def test_inverse_one_hot_1d_produces_scalar(self):\n        arr = np.zeros(10)\n        arr[5] = 1\n        x = inverse_one_hot(arr)\n        self.assertEqual((), x.shape)\n\n    def test_inverse_one_hot_2d_produces_1d_output(self):\n        arr = np.eye(10)\n        x = inverse_one_hot(arr, axis=-1)\n        self.assertEqual((10,), x.shape)\n        np.testing.assert_allclose(\n            np.linspace(-1, 1, 10, endpoint=False), x, rtol=1e-3)\n\n    def test_inverse_one_hot_3d_produces_2d_output(self):\n        arr = np.random.random_sample((3, 128, 256))\n        x = inverse_one_hot(arr, axis=1)\n        self.assertEqual((3, 256), x.shape)\n\n\nclass TestInstanceScale(unittest2.TestCase):\n    def test_handles_zeros(self):\n        x = np.random.random_sample((10, 3))\n        x[4, :] = 0\n        scaled = instance_scale(x, axis=-1)\n        maxes = np.ones(10)\n        maxes[4] = 0\n        np.testing.assert_allclose(maxes, scaled.max(axis=-1))\n\n    def test_handles_negative_numbers_correctly(self):\n        x = np.random.normal(0, 1, (10, 300))\n        scaled = instance_scale(x, axis=-1)\n        np.testing.assert_allclose(1, np.abs(scaled).max(axis=-1))\n'"
zounds/nputil/__init__.py,0,b'from .npx import *\n'
zounds/nputil/npx.py,22,"b'\n\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided as ast\nfrom countbits import *\n\n\ndef norm_shape(shape):\n    """"""\n    Normalize numpy array shapes so they\'re always expressed as a tuple,\n    even for one-dimensional shapes.\n\n    Parameters\n        shape - an int, or a tuple of ints\n\n    Returns\n        a shape tuple\n    """"""\n    try:\n        i = int(shape)\n        return (i,)\n    except TypeError:\n        # shape was not a number\n        pass\n\n    try:\n        t = tuple(shape)\n        return t\n    except TypeError:\n        # shape was not iterable\n        pass\n\n    raise TypeError(\'shape must be an int, or a tuple of ints\')\n\n\ndef safe_log(a):\n    """"""\n    Return the element-wise log of an array, checking for negative\n    array elements and avoiding divide-by-zero errors.\n    """"""\n    if np.any([a < 0]):\n        raise ValueError(\'array contains negative components\')\n\n    return np.log(a + 1e-12)\n\n\ndef safe_unit_norm(a):\n    """"""\n    Ensure that the vector or vectors have unit norm\n    """"""\n    if 1 == len(a.shape):\n        n = np.linalg.norm(a)\n        if n:\n            return a / n\n        return a\n\n    norm = np.sum(np.abs(a) ** 2, axis=-1) ** (1. / 2)\n\n    # Dividing by a norm of zero will cause a warning to be issued. Set those\n    # values to another number. It doesn\'t matter what, since we\'ll be dividing\n    # a vector of zeros by the number, and 0 / N always equals 0.\n    norm[norm == 0] = -1e12\n    return a / norm[:, np.newaxis]\n\n\ndef pad(a, desiredlength):\n    """"""\n    Pad an n-dimensional numpy array with zeros along the zero-th dimension\n    so that it is the desired length.  Return it unchanged if it is greater\n    than or equal to the desired length\n    """"""\n\n    if len(a) >= desiredlength:\n        return a\n\n    islist = isinstance(a, list)\n    a = np.array(a)\n    diff = desiredlength - len(a)\n    shape = list(a.shape)\n    shape[0] = diff\n\n    padded = np.concatenate([a, np.zeros(shape, dtype=a.dtype)])\n    return padded.tolist() if islist else padded\n\n\ndef _wpad(l, windowsize, stepsize):\n    """"""\n    Parameters\n        l - The length of the input array\n        windowsize - the size of each window of samples\n        stepsize - the number of samples to move the window each step\n\n    Returns\n        The length the input array should be so that no samples are leftover\n    """"""\n    if l <= windowsize:\n        return windowsize\n\n    nsteps = ((l // stepsize) * stepsize)\n    overlap = (windowsize - stepsize)\n    if overlap:\n        return nsteps + overlap\n\n    diff = (l - nsteps)\n    left = max(0, windowsize - diff)\n    return l + left if diff else l\n\n\ndef _wcut(l, windowsize, stepsize):\n    """"""\n    Parameters\n        l - The length of the input array\n        windowsize - the size of each window of samples\n        stepsize - the number of samples to move the window each step\n\n    Returns\n        The length the input array should be so that leftover samples are ignored\n    """"""\n    end = l - windowsize\n    if l <= windowsize:\n        return 0, 0\n    elif end % stepsize:\n        l = windowsize + ((end // stepsize) * stepsize)\n\n    return l, l - (windowsize - stepsize)\n\n\ndef windowed(a, windowsize, stepsize=None, dopad=False):\n    """"""\n    Parameters\n        a          - the input array to restructure into overlapping windows\n        windowsize - the size of each window of samples\n        stepsize   - the number of samples to shift the window each step. If not\n                     specified, this defaults to windowsize\n        dopad      - If false (default), leftover samples are returned seperately.\n                     If true, the input array is padded with zeros so that all\n                     samples are used.\n    """"""\n\n    if windowsize < 1:\n        raise ValueError(\'windowsize must be greater than or equal to one\')\n\n    if stepsize is None:\n        stepsize = windowsize\n\n    if stepsize < 1:\n        raise ValueError(\'stepsize must be greater than or equal to one\')\n\n    if not a.flags[\'C_CONTIGUOUS\']:\n        a = a.copy()\n\n    if windowsize == 1 and stepsize == 1:\n        # A windowsize and stepsize of one mean that no windowing is necessary.\n        # Return the array unchanged.\n        return np.zeros((0,) + a.shape[1:], dtype=a.dtype), a\n\n    if windowsize == 1 and stepsize > 1:\n        return np.zeros(0, dtype=a.dtype), a[::stepsize]\n\n    # the original length of the input array\n    l = a.shape[0]\n\n    if dopad:\n        p = _wpad(l, windowsize, stepsize)\n        # pad the array with enough zeros so that there are no leftover samples\n        a = pad(a, p)\n        # no leftovers; an empty array\n        leftover = np.zeros((0,) + a.shape[1:], dtype=a.dtype)\n    else:\n        # cut the array so that any leftover samples are returned seperately\n        c, lc = _wcut(l, windowsize, stepsize)\n        leftover = a[lc:]\n        a = a[:c]\n\n    if 0 == a.shape[0]:\n        return leftover, np.zeros(a.shape, dtype=a.dtype)\n\n    n = 1 + (a.shape[0] - windowsize) // (stepsize)\n    s = a.strides[0]\n    newshape = (n, windowsize) + a.shape[1:]\n    newstrides = (stepsize * s, s) + a.strides[1:]\n\n    out = np.ndarray.__new__( \\\n        np.ndarray,\n        strides=newstrides,\n        shape=newshape,\n        buffer=a,\n        dtype=a.dtype)\n\n    return leftover, out\n\n\ndef sliding_window(a, ws, ss=None, flatten=True):\n    """"""\n    Return a sliding window over a in any number of dimensions\n\n    Parameters:\n        a  - an n-dimensional numpy array\n        ws - an int (a is 1D) or tuple (a is 2D or greater) representing the size\n             of each dimension of the window\n        ss - an int (a is 1D) or tuple (a is 2D or greater) representing the\n             amount to slide the window in each dimension. If not specified, it\n             defaults to ws.\n        flatten - if True, all slices are flattened, otherwise, there is an\n                  extra dimension for each dimension of the input.\n\n    Returns\n        an array containing each n-dimensional window from a\n    """"""\n\n    if None is ss:\n        # ss was not provided. the windows will not overlap in any direction.\n        ss = ws\n    ws = norm_shape(ws)\n    ss = norm_shape(ss)\n\n    # convert ws, ss, and a.shape to numpy arrays so that we can do math in every \n    # dimension at once.\n    ws = np.array(ws)\n    ss = np.array(ss)\n    shape = np.array(a.shape)\n\n    # ensure that ws, ss, and a.shape all have the same number of dimensions\n    ls = [len(shape), len(ws), len(ss)]\n    if 1 != len(set(ls)):\n        raise ValueError( \\\n            \'a.shape, ws and ss must all have the same length. They were %s\' % str(\n                ls))\n\n    # ensure that ws is smaller than a in every dimension\n    if np.any(ws > shape):\n        raise ValueError( \\\n            \'ws cannot be larger than a in any dimension.\\\n a.shape was %s and ws was %s\' % (str(a.shape), str(ws)))\n\n    # how many slices will there be in each dimension?\n    newshape = norm_shape(((shape - ws) // ss) + 1)\n    # the shape of the strided array will be the number of slices in each dimension\n    # plus the shape of the window (tuple addition)\n    newshape += norm_shape(ws)\n    # the strides tuple will be the array\'s strides multiplied by step size, plus\n    # the array\'s strides (tuple addition)\n    newstrides = norm_shape(np.array(a.strides) * ss) + a.strides\n\n    strided = ast(a, shape=newshape, strides=newstrides)\n    if not flatten:\n        return strided\n\n    # Collapse strided so that it has one more dimension than the window.  I.e.,\n    # the new array is a flat list of slices.\n    meat = len(ws) if ws.shape else 0\n    firstdim = (np.product(newshape[:-meat]),) if ws.shape else ()\n    dim = firstdim + (newshape[-meat:])\n    # remove any dimensions with size 1\n    return strided.reshape(dim)\n\n\nclass Growable(object):\n    """"""\n    A thin wrapper around a numpy array that allows it to be treated like a\n    dynamically-sized object.  For numeric types, this should be more memory\n    efficient than using a list, since members are represented by contiguous\n    blocks of memory instead of PyObjects.\n    """"""\n\n    def __init__(self, data, position=0, growth_rate=1):\n        """"""__init__\n\n        :param data: A numpy array to treat as dynamically resizable. Can have \\\n        length 0\n\n        :param position: The logical size of the array.  This can be smaller, \\\n        equal to, or larger than the array passed in as data\n\n        :param growth_rate: The amount by which the array will grow when \\\n        necessary.  A growth rate of 1 means that the array will double in \\\n        size each time.\n        """"""\n        if growth_rate <= 0:\n            raise ValueError(\'growth_rate must be greater than zero\')\n\n        object.__init__(self)\n        self._data = data\n        self._position = position\n        self._growth_rate = growth_rate\n\n    @property\n    def data(self):\n        """"""\n        Return the numpy array wrapped by this instance.  Note that the array\n        may be larger or smaller than logical_size\n        """"""\n        return self._data\n\n    @property\n    def logical_size(self):\n        """"""\n        Return the size of the initialized part of the array.  Values outside\n        this boundary are invalid.\n        """"""\n        return self._position\n\n    @property\n    def physical_size(self):\n        """"""\n        The actual size of the wrapped array\n        """"""\n        return self._data.shape[0]\n\n    @property\n    def logical_data(self):\n        """"""\n        Return only the initialized part of the array\n        """"""\n        return self._data[:self.logical_size]\n\n    def _tmp_size(self):\n        n_items = int(self._data.shape[0] * self._growth_rate)\n        # require that n_items is at least one\n        return 1 if not n_items else n_items\n\n    def _tmp(self, amt=None):\n        n_items = self._tmp_size() if None is amt else amt\n        return np.ndarray(\n            (n_items,) + self._data.shape[1:], dtype=self._data.dtype)\n\n    def _grow(self, amt=None):\n        new_mem = self._tmp(amt=amt)\n        self._data = np.concatenate([self._data, new_mem])\n\n    def append(self, item):\n        """"""\n        append a single item to the array, growing the wrapped numpy array\n        if necessary\n        """"""\n        try:\n            self._data[self._position] = item\n        except IndexError:\n            self._grow()\n            self._data[self._position] = item\n        self._position += 1\n        return self\n\n    def extend(self, items):\n        """"""\n        extend the numpy array with multiple items, growing the wrapped array\n        if necessary\n        """"""\n        items = np.array(items)\n        pos = items.shape[0] + self.logical_size\n        if pos > self.physical_size:\n            # more physical memory is needed than has been allocated so far\n            amt = self._tmp_size()\n            if self.physical_size + amt < pos:\n                # growing the memory by the prescribed amount still won\'t \n                # make enough room. Allocate exactly as much memory as is needed\n                # to accommodate items\n                amt = pos - self.physical_size\n\n            # actually grow the array\n            self._grow(amt=amt)\n\n        stop = self._position + items.shape[0]\n        self._data[self._position: stop] = items\n        self._position += items.shape[0]\n        return self\n\n\ndef hamming_distance(a, b):\n    """"""\n    a - scalar\n    b - array of scalars\n    """"""\n    return count_bits(a ^ b)\n\n\ndef packed_hamming_distance(a, b):\n    """"""\n    Interpret a as a ""packed"" scalar, i.e. an n-bit number where n may not be\n    a power of 2. E.g., a 250-bit number would be represented by 4 64-bit integers.\n\n    Interpret b as an array of ""packed"" scalars. Its second dimension should be\n    the same length as a.\n    """"""\n    xored = a ^ b\n    return count_packed_bits(xored)\n'"
zounds/nputil/test_nputil.py,53,"b'import unittest\nimport numpy as np\nfrom .npx import windowed, sliding_window, Growable\n\n\nclass GrowableTest(unittest.TestCase):\n    def test_negative_growth_rate(self):\n        self.assertRaises(ValueError, lambda: Growable(np.zeros(10), 0, -1))\n\n    def test_initialized_with_zero_elements(self):\n        g = Growable(np.zeros(0), 0, 1)\n        g.append(1)\n        self.assertEqual(1, g.physical_size)\n        self.assertEqual(1, g.logical_size)\n        self.assertEqual(1, g._data[0])\n\n    def test_logical_data(self):\n        g = Growable(np.zeros(10), 0, 1)\n        self.assertEqual(0, g.logical_data.shape[0])\n        g.extend([1, 2])\n        self.assertEqual(2, g.logical_data.shape[0])\n        self.assertTrue(np.all(np.array([1, 2]) == g.logical_data))\n\n    def test_append_no_growth(self):\n        g = Growable(np.zeros(10), 0, 1)\n        g.append(1)\n        self.assertEqual(1, g._data[0])\n        self.assertEqual(10, g.physical_size)\n        self.assertEqual(1, g.logical_size)\n\n    def test_extend_no_growth(self):\n        g = Growable(np.zeros(10), 0, 1)\n        g.extend(np.ones(3))\n        self.assertTrue(np.all(1 == g._data[:3]))\n        self.assertEqual(10, g.physical_size)\n        self.assertEqual(3, g.logical_size)\n\n    def test_append_growth(self):\n        g = Growable(np.zeros(10), 10, 1)\n        g.append(1)\n        self.assertEqual(20, g.physical_size)\n        self.assertEqual(11, g.logical_size)\n        self.assertEqual(1, g._data[10])\n\n    def test_extend_growth(self):\n        g = Growable(np.zeros(10), 10, 1)\n        g.extend(np.ones(5))\n        self.assertEqual(20, g.physical_size)\n        self.assertEqual(15, g.logical_size)\n        self.assertTrue(np.all(1 == g._data[10:15]))\n\n    def test_lt_one_growth_rate_append(self):\n        g = Growable(np.zeros(1), 0, .00001)\n        g.append(1)\n        g.append(2)\n        self.assertEqual(2, g.physical_size)\n        self.assertEqual(2, g.logical_size)\n        self.assertEqual(1, g._data[0])\n        self.assertEqual(2, g._data[1])\n\n    def test_multiple_grow_calls_required(self):\n        g = Growable(np.zeros(10), 10, .1)\n        g.extend(np.ones(4))\n        self.assertEqual(14, g.physical_size)\n        self.assertEqual(14, g.logical_size)\n        self.assertTrue(np.all(1 == g._data[10:14]))\n\n    def test_multidimensional(self):\n        g = Growable(np.zeros((10, 3)), 10, 1)\n        g.append(1)\n        self.assertEqual(20, g.physical_size)\n        self.assertEqual(11, g.logical_size)\n        self.assertEqual((20, 3), g._data.shape)\n\n\nclass SlidingWindowTest(unittest.TestCase):\n    def test_mismatched_dims_ws(self):\n        a = np.zeros(10)\n        self.assertRaises(ValueError, lambda: sliding_window(a, (1, 2)))\n\n    def test_mismatched_dims_ss(self):\n        a = np.zeros(10)\n        self.assertRaises(ValueError, lambda: sliding_window(a, 3, (1, 2)))\n\n    def test_windowsize_too_large_1D(self):\n        a = np.zeros(10)\n        self.assertRaises(ValueError, lambda: sliding_window(a, 11))\n\n    def test_windowsize_too_large_2D(self):\n        a = np.zeros((10, 10))\n        self.assertRaises(ValueError, lambda: sliding_window(a, (3, 11)))\n\n    def test_1D_no_step_specified(self):\n        a = np.arange(10)\n        b = sliding_window(a, 3)\n        self.assertEqual((3, 3), b.shape)\n        self.assertTrue(np.all(b.ravel() == a[:9]))\n\n    def test_1D_with_step(self):\n        a = np.arange(10)\n        b = sliding_window(a, 3, 1)\n        self.assertEqual((8, 3), b.shape)\n\n    def test_1D_flat_nonflat_equivalent(self):\n        a = np.zeros(10)\n        bflat = sliding_window(a, 3)\n        bnonflat = sliding_window(a, 3, flatten=False)\n        self.assertEqual(bflat.shape, bnonflat.shape)\n        self.assertTrue(np.all(bflat == bnonflat))\n\n    def test_2D_no_step_specified(self):\n        a = np.arange(64).reshape((8, 8))\n        b = sliding_window(a, (4, 4))\n        self.assertEqual((4, 4, 4), b.shape)\n\n    def test_2D_with_step(self):\n        a = np.zeros((8, 8))\n        b = sliding_window(a, (4, 4), (1, 1))\n        self.assertEqual((25, 4, 4), b.shape)\n\n    def test_2D_nonflat_no_step_specified(self):\n        a = np.arange(64).reshape((8, 8))\n        b = sliding_window(a, (4, 4), flatten=False)\n        self.assertEqual((2, 2, 4, 4), b.shape)\n\n    def test_2D_nonflat_with_step(self):\n        a = np.zeros((8, 8))\n        b = sliding_window(a, (4, 4), (1, 1), flatten=False)\n        self.assertEqual((5, 5, 4, 4), b.shape)\n\n\nclass WindowedTest(unittest.TestCase):\n    def test_windowsize_ltone(self):\n        a = np.arange(10)\n        self.assertRaises(ValueError, lambda: windowed(a, 0, 1))\n\n    def test_stepsize_ltone(self):\n        a = np.arange(10)\n        self.assertRaises(ValueError, lambda: windowed(a, 1, 0))\n\n    def test_no_windowing(self):\n        a = np.arange(10)\n        l, w = windowed(a, 1, 1)\n        self.assertTrue(a is w)\n        self.assertEqual(0, l.shape[0])\n\n    def test_drop_samples(self):\n        a = np.arange(10)\n        l, w = windowed(a, 1, 2)\n        self.assertEqual(5, w.shape[0])\n        self.assertEqual(0, l.shape[0])\n\n    def test_windowsize_two_stepsize_one_cut(self):\n        a = np.arange(10)\n        l, w = windowed(a, 2, 1)\n        self.assertEqual(1, l.shape[0])\n        self.assertEqual((9, 2), w.shape)\n\n    def test_windowsize_two_stepsize_one_pad(self):\n        a = np.arange(10)\n        l, w = windowed(a, 2, 1, True)\n        self.assertEqual(0, l.shape[0])\n        self.assertEqual((10, 2), w.shape)\n\n    def test_windowsize_two_stepsize_two_cut(self):\n        a = np.arange(10)\n        l, w = windowed(a, 2, 2)\n        self.assertEqual(0, l.shape[0])\n        self.assertEqual((5, 2), w.shape)\n\n    def test_windowsize_two_stepsize_two_pad(self):\n        a = np.arange(10)\n        l, w = windowed(a, 2, 2, True)\n        self.assertEqual(0, l.shape[0])\n        self.assertEqual((5, 2), w.shape)\n\n    def test_windowsize_three_stepsize_two_cut(self):\n        a = np.arange(10)\n        l, w = windowed(a, 3, 2)\n        self.assertEqual(2, l.shape[0])\n        self.assertEqual((4, 3), w.shape)\n\n    def test_windowsize_three_stepsize_two_pad(self):\n        a = np.arange(10)\n        l, w = windowed(a, 3, 2, dopad=True)\n        self.assertEqual(0, l.shape[0])\n        self.assertEqual((5, 3), w.shape)\n        self.assertTrue(np.all([0, 0] == w[-1, -1]))\n\n    def test_windowsize_three_stepsize_three_cut(self):\n        a = np.arange(10)\n        l, w = windowed(a, 3, 3)\n        self.assertEqual(1, l.shape[0])\n        self.assertEqual((3, 3), w.shape)\n\n    def test_windowsize_three_stepsize_three_pad(self):\n        a = np.arange(10)\n        l, w = windowed(a, 3, 3, dopad=True)\n        self.assertEqual(0, l.shape[0])\n        self.assertEqual((4, 3), w.shape)\n\n    def test_windowsize_gt_length_cut(self):\n        a = np.arange(5)\n        l, w = windowed(a, 6, 1)\n        self.assertEqual(5, l.shape[0])\n        self.assertEqual(0, w.shape[0])\n\n    def test_windowsize_gt_length_pad(self):\n        a = np.arange(5)\n        l, w = windowed(a, 6, 1, dopad=True)\n        self.assertEqual(0, l.shape[0])\n        self.assertEqual((1, 6), w.shape)\n\n    def test_twod_cut(self):\n        a = np.arange(20).reshape((10, 2))\n        l, w = windowed(a, 3, 2)\n        self.assertEqual(2, l.shape[0])\n        self.assertEqual((4, 3, 2), w.shape)\n\n    def test_twod_pad(self):\n        a = np.arange(20).reshape((10, 2))\n        l, w = windowed(a, 3, 2, dopad=True)\n        self.assertEqual(0, l.shape[0])\n        self.assertEqual((5, 3, 2), w.shape)\n\n    def test_no_stepsize_specified(self):\n        a = np.arange(10)\n        l, w = windowed(a, 2)\n        self.assertEqual(0, l.shape[0])\n        self.assertEqual((5, 2), w.shape)\n\n    def test_can_apply_windowed_twice(self):\n        a = np.arange(100)\n        _, w = windowed(a, 7, 3)\n        _, w2 = windowed(w, 3, 1)\n        self.assertEqual((3, 7), w2.shape[1:])\n\n    def test_can_apply_windowed_twice_2(self):\n        samples = np.random.random_sample(44100)\n        _, w = windowed(samples, 512, 256)\n        f = np.fft.fft(w)[:, 1:]\n        _, w2 = windowed(f, 3, 1)\n        self.assertEqual((3, 511), w2.shape[1:])\n\n    def test_can_apply_windowed_to_integer_dtype(self):\n        samples = np.zeros(44100).astype(np.int64)\n        l, w = windowed(samples, 8192, 4096)\n        self.assertEqual(w.dtype, np.int64)\n        self.assertEqual(8192, w.shape[1])\n'"
zounds/persistence/__init__.py,0,"b'from .dimension import DimensionEncoder, DimensionDecoder\nfrom .arraywithunits import \\\n    ArrayWithUnitsFeature, PackedArrayWithUnitsFeature, ArrayWithUnitsEncoder, \\\n    PackedArrayWithUnitsEncoder\nfrom .audiosamples import AudioSamplesFeature\nfrom .frequencyadaptive import FrequencyAdaptiveFeature\nfrom .timeslice import TimeSliceEncoder, TimeSliceDecoder\n'"
zounds/persistence/arraywithunits.py,2,"b""from zounds.core import ArrayWithUnits\nfrom zounds.persistence import DimensionEncoder, DimensionDecoder\nfrom featureflow import Node, Decoder, Feature, NumpyMetaData\nimport struct\nimport json\nimport numpy as np\n\n\ndef _np_from_buffer(b, shape, dtype):\n    try:\n        shape = tuple(int(x) for x in shape)\n    except TypeError:\n        shape = int(shape)\n    f = np.frombuffer if len(b) else np.fromstring\n    return f(b, dtype=dtype).reshape(shape)\n\n\nclass ArrayWithUnitsEncoder(Node):\n    content_type = 'application/octet-stream'\n\n    def __init__(self, needs=None):\n        super(ArrayWithUnitsEncoder, self).__init__(needs=needs)\n        self.encoder = DimensionEncoder()\n        self.dimensions = None\n        self.nmpy = None\n\n    def _process(self, data):\n        if self.dimensions is None:\n            self.dimensions = data.dimensions\n            d = list(self.encoder.encode(self.dimensions))\n            encoded = json.dumps(d)\n            yield struct.pack('I', len(encoded))\n            yield encoded\n        if self.nmpy is None:\n            self.nmpy = NumpyMetaData(data.dtype, data.shape[1:])\n            yield self.nmpy.pack()\n\n        yield data.tostring()\n\n\nclass PackedArrayWithUnitsEncoder(Node):\n    content_type = 'application/octet-stream'\n\n    def __init__(self, needs=None):\n        super(PackedArrayWithUnitsEncoder, self).__init__(needs=needs)\n        self.encoder = DimensionEncoder()\n        self.dimensions = None\n        self.nmpy = None\n\n    def _process(self, data):\n        if self.dimensions is None:\n            self.dimensions = data.dimensions\n            d = list(self.encoder.encode(self.dimensions))\n            encoded = json.dumps(d)\n            yield struct.pack('I', len(encoded))\n            yield encoded\n\n        packed = np.packbits(data.astype(np.uint8), axis=-1)\n\n        if self.nmpy is None:\n            self.nmpy = NumpyMetaData(packed.dtype, packed.shape[1:])\n            yield self.nmpy.pack()\n\n        yield packed.tostring()\n\n\nclass ArrayWithUnitsDecoder(Decoder):\n    def __init__(self):\n        super(ArrayWithUnitsDecoder, self).__init__()\n\n    def __call__(self, flo):\n        nbytes = struct.calcsize('I')\n        json_len = struct.unpack('I', flo.read(nbytes))[0]\n        d = json.loads(flo.read(json_len))\n        decoder = DimensionDecoder()\n        dimensions = list(decoder.decode(d))\n\n        metadata, bytes_read = NumpyMetaData.unpack(flo)\n        leftovers = flo.read()\n        leftover_bytes = len(leftovers)\n        first_dim = leftover_bytes / metadata.totalsize\n        dim = (first_dim,) + metadata.shape\n        raw = _np_from_buffer(leftovers, dim, metadata.dtype)\n\n        return ArrayWithUnits(raw, dimensions)\n\n    def __iter__(self, flo):\n        yield self(flo)\n\n\nclass ArrayWithUnitsFeature(Feature):\n    def __init__(\n            self,\n            extractor,\n            needs=None,\n            store=False,\n            key=None,\n            encoder=ArrayWithUnitsEncoder,\n            decoder=ArrayWithUnitsDecoder(),\n            **extractor_args):\n        super(ArrayWithUnitsFeature, self).__init__(\n                extractor,\n                needs=needs,\n                store=store,\n                encoder=encoder,\n                decoder=decoder,\n                key=key,\n                **extractor_args)\n\n\nclass PackedArrayWithUnitsFeature(Feature):\n    def __init__(\n            self,\n            extractor,\n            needs=None,\n            store=False,\n            key=None,\n            encoder=PackedArrayWithUnitsEncoder,\n            decoder=ArrayWithUnitsDecoder(),\n            **extractor_args):\n        super(ArrayWithUnitsFeature, self).__init__(\n                extractor,\n                needs=needs,\n                store=store,\n                encoder=encoder,\n                decoder=decoder,\n                key=key,\n                **extractor_args)\n"""
zounds/persistence/audiosamples.py,0,"b'from featureflow import Feature\nfrom zounds.persistence.arraywithunits import \\\n    ArrayWithUnitsDecoder, ArrayWithUnitsEncoder\nfrom zounds.timeseries import audio_sample_rate, AudioSamples\n\n\nclass AudioSamplesDecoder(ArrayWithUnitsDecoder):\n    def __init__(self):\n        super(ArrayWithUnitsDecoder, self).__init__()\n\n    def __call__(self, flo):\n        raw = super(AudioSamplesDecoder, self).__call__(flo)\n        samplerate = audio_sample_rate(raw.dimensions[0].samples_per_second)\n        return AudioSamples(raw, samplerate)\n\n\nclass AudioSamplesFeature(Feature):\n\n    def __init__(\n            self,\n            extractor,\n            needs=None,\n            store=False,\n            key=None,\n            encoder=ArrayWithUnitsEncoder,\n            decoder=AudioSamplesDecoder(),\n            **extractor_args):\n        super(AudioSamplesFeature, self).__init__(\n                extractor,\n                needs=needs,\n                store=store,\n                encoder=encoder,\n                decoder=decoder,\n                key=key,\n                **extractor_args)\n'"
zounds/persistence/basedimension.py,0,"b""\nclass BaseDimensionEncoder(object):\n    def __init__(self, dim_type):\n        super(BaseDimensionEncoder, self).__init__()\n        self.dim_type = dim_type\n\n    def matches(self, o):\n        return isinstance(o, self.dim_type)\n\n    def dict(self, o):\n        raise NotImplementedError()\n\n    def encode(self, o):\n        return dict(type=self.dim_type.__name__, data=self.dict(o))\n\n\nclass BaseDimensionDecoder(object):\n    def __init__(self, dim_type):\n        super(BaseDimensionDecoder, self).__init__()\n        self.dim_type = dim_type\n\n    def matches(self, d):\n        return d['type'] == self.dim_type.__name__\n\n    def args(self, d):\n        return tuple()\n\n    def kwargs(self, d):\n        return dict()\n\n    def decode(self, d):\n        data = d['data']\n        return self.dim_type(*self.args(data), **self.kwargs(data))\n"""
zounds/persistence/dimension.py,0,"b""from .frequencydimension import \\\n    FrequencyDimensionEncoder, FrequencyDimensionDecoder, \\\n    ExplicitFrequencyDimensionEncoder, ExplicitFrequencyDimensionDecoder\nfrom .identitydimension import IdentityDimensionEncoder, IdentityDimensionDecoder\nfrom .timedimension import TimeDimensionEncoder, TimeDimensionDecoder\n\n\nclass DimensionEncoder(object):\n    encoders = [\n        IdentityDimensionEncoder(),\n        TimeDimensionEncoder(),\n        FrequencyDimensionEncoder(),\n        ExplicitFrequencyDimensionEncoder()\n    ]\n\n    def __init__(self):\n        super(DimensionEncoder, self).__init__()\n\n    def encode(self, o):\n        for dim in o:\n            for encoder in self.encoders:\n                if encoder.matches(dim):\n                    yield encoder.encode(dim)\n                    break\n            else:\n                raise NotImplementedError(\n                        'No matching strategy for {dim}'.format(**locals()))\n\n\nclass DimensionDecoder(object):\n    decoders = [\n        IdentityDimensionDecoder(),\n        TimeDimensionDecoder(),\n        FrequencyDimensionDecoder(),\n        ExplicitFrequencyDimensionDecoder()\n    ]\n\n    def __init__(self):\n        super(DimensionDecoder, self).__init__()\n\n    def decode(self, d):\n        for dim in d:\n            for decoder in self.decoders:\n                if decoder.matches(dim):\n                    yield decoder.decode(dim)\n                    break\n            else:\n                raise NotImplementedError(\n                        'No matching strategy for {dim}'.format(**locals()))\n"""
zounds/persistence/frequencyadaptive.py,0,"b'from featureflow import Feature\nfrom .arraywithunits import ArrayWithUnitsDecoder, ArrayWithUnitsEncoder\nfrom zounds.spectral import FrequencyAdaptive\n\n\nclass FrequencyAdaptiveDecoder(ArrayWithUnitsDecoder):\n    def __init__(self):\n        super(ArrayWithUnitsDecoder, self).__init__()\n\n    def __call__(self, flo):\n        raw = super(FrequencyAdaptiveDecoder, self).__call__(flo)\n        return FrequencyAdaptive.from_array_with_units(raw)\n\n\nclass FrequencyAdaptiveFeature(Feature):\n    def __init__(\n            self,\n            extractor,\n            needs=None,\n            store=False,\n            key=None,\n            encoder=ArrayWithUnitsEncoder,\n            decoder=FrequencyAdaptiveDecoder(),\n            **extractor_args):\n        super(FrequencyAdaptiveFeature, self).__init__(\n            extractor,\n            needs=needs,\n            store=store,\n            encoder=encoder,\n            decoder=decoder,\n            key=key,\n            **extractor_args)\n'"
zounds/persistence/frequencydimension.py,0,"b""from .basedimension import BaseDimensionEncoder, BaseDimensionDecoder\nfrom zounds.spectral import \\\n    FrequencyBand, FrequencyDimension, LinearScale, GeometricScale, \\\n    ExplicitScale, ExplicitFrequencyDimension, BarkScale, MelScale, ChromaScale\n\n\nclass ScaleEncoderDecoder(object):\n    def __init__(self):\n        super(ScaleEncoderDecoder, self).__init__()\n\n    def can_encode(self, scale):\n        raise NotImplementedError()\n\n    def can_decode(self, d):\n        raise NotImplementedError()\n\n    def decode(self, d):\n        raise NotImplementedError()\n\n    def encode(self, scale):\n        raise NotImplementedError()\n\n\nclass GenericScaleEncoderDecoder(ScaleEncoderDecoder):\n    def __init__(self, cls):\n        super(GenericScaleEncoderDecoder, self).__init__()\n        self.cls = cls\n\n    def can_encode(self, scale):\n        return isinstance(scale, self.cls)\n\n    def can_decode(self, d):\n        return d['name'] == self.cls.__name__\n\n    def encode(self, scale):\n        return dict(\n            start_hz=scale.frequency_band.start_hz,\n            stop_hz=scale.frequency_band.stop_hz,\n            n_bands=scale.n_bands,\n            name=scale.__class__.__name__)\n\n    def _decode_frequency_band(self, d):\n        return FrequencyBand(d['start_hz'], d['stop_hz'])\n\n    def _decode_args(self, d):\n        return self._decode_frequency_band(d), d['n_bands']\n\n    def decode(self, d):\n        return self.cls(*self._decode_args(d))\n\n\nclass BarkScaleEncoderDecoder(GenericScaleEncoderDecoder):\n    def __init__(self):\n        super(BarkScaleEncoderDecoder, self).__init__(BarkScale)\n\n\nclass MelScaleEncoderDecoder(GenericScaleEncoderDecoder):\n    def __init__(self):\n        super(MelScaleEncoderDecoder, self).__init__(MelScale)\n\n\nclass ChromaScaleEncoderDecoder(GenericScaleEncoderDecoder):\n    def __init__(self):\n        super(ChromaScaleEncoderDecoder, self).__init__(ChromaScale)\n\n    def _decode_args(self, d):\n        return self._decode_frequency_band(d),\n\n\nclass LinearScaleEncoderDecoder(ScaleEncoderDecoder):\n    def __init__(self):\n        super(LinearScaleEncoderDecoder, self).__init__()\n\n    def can_encode(self, scale):\n        return isinstance(scale, LinearScale)\n\n    def can_decode(self, d):\n        return d['name'] == LinearScale.__name__\n\n    def encode(self, scale):\n        return dict(\n            start_hz=scale.frequency_band.start_hz,\n            stop_hz=scale.frequency_band.stop_hz,\n            n_bands=scale.n_bands,\n            name=scale.__class__.__name__,\n            always_even=scale.always_even)\n\n    def decode(self, d):\n        band = FrequencyBand(d['start_hz'], d['stop_hz'])\n        return LinearScale(band, d['n_bands'], always_even=d['always_even'])\n\n\nclass GeometricScaleEncoderDecoder(ScaleEncoderDecoder):\n    def __init__(self):\n        super(GeometricScaleEncoderDecoder, self).__init__()\n\n    def can_encode(self, scale):\n        return isinstance(scale, GeometricScale)\n\n    def can_decode(self, d):\n        return d['name'] == GeometricScale.__name__\n\n    def encode(self, scale):\n        return dict(\n            start_center_hz=scale.start_center_hz,\n            stop_center_hz=scale.stop_center_hz,\n            bandwidth_ratio=scale.bandwidth_ratio,\n            n_bands=scale.n_bands,\n            name=GeometricScale.__name__,\n            always_even=scale.always_even)\n\n    def decode(self, d):\n        return GeometricScale(\n            d['start_center_hz'],\n            d['stop_center_hz'],\n            d['bandwidth_ratio'],\n            d['n_bands'],\n            d['always_even'])\n\n\nclass ExplicitScaleEncoderDecoder(ScaleEncoderDecoder):\n    def __init__(self):\n        super(ExplicitScaleEncoderDecoder, self).__init__()\n\n    def can_encode(self, scale):\n        return isinstance(scale, ExplicitScale)\n\n    def can_decode(self, d):\n        return d['name'] == ExplicitScale.__name__\n\n    def encode(self, scale):\n        bands = [(b.start_hz, b.stop_hz) for b in scale]\n        return dict(bands=bands, name=ExplicitScale.__name__)\n\n    def decode(self, d):\n        bands = [FrequencyBand(*b) for b in d['bands']]\n        return ExplicitScale(bands)\n\n\nclass CompositeScaleEncoderDecoder(object):\n    strategies = [\n        LinearScaleEncoderDecoder(),\n        GeometricScaleEncoderDecoder(),\n        ExplicitScaleEncoderDecoder(),\n        BarkScaleEncoderDecoder(),\n        MelScaleEncoderDecoder(),\n        ChromaScaleEncoderDecoder()\n    ]\n\n    def __init__(self):\n        super(CompositeScaleEncoderDecoder, self).__init__()\n\n    def _find_encoder(self, scale):\n        try:\n            return next(s for s in self.strategies if s.can_encode(scale))\n        except StopIteration:\n            raise NotImplementedError('No suitable encoder found')\n\n    def _find_decoder(self, d):\n        try:\n            return next(s for s in self.strategies if s.can_decode(d))\n        except StopIteration:\n            raise NotImplementedError('No suitable decoder found')\n\n    def encode(self, scale):\n        return self._find_encoder(scale).encode(scale)\n\n    def decode(self, d):\n        return self._find_decoder(d).decode(d)\n\n\nclass FrequencyDimensionEncoder(BaseDimensionEncoder):\n    def __init__(self):\n        super(FrequencyDimensionEncoder, self).__init__(FrequencyDimension)\n        self.scale_encoder = CompositeScaleEncoderDecoder()\n\n    def dict(self, freq_dim):\n        return self.scale_encoder.encode(freq_dim.scale)\n\n\nclass FrequencyDimensionDecoder(BaseDimensionDecoder):\n    def __init__(self):\n        super(FrequencyDimensionDecoder, self).__init__(FrequencyDimension)\n        self.scale_decoder = CompositeScaleEncoderDecoder()\n\n    def args(self, d):\n        return self.scale_decoder.decode(d),\n\n\nclass ExplicitFrequencyDimensionEncoder(BaseDimensionEncoder):\n    def __init__(self):\n        super(ExplicitFrequencyDimensionEncoder, self).__init__(\n            ExplicitFrequencyDimension)\n        self.scale_encoder = CompositeScaleEncoderDecoder()\n\n    def dict(self, freq_dim):\n        d = self.scale_encoder.encode(freq_dim.scale)\n        slices = [(int(s.start), int(s.stop)) for s in freq_dim.slices]\n        d.update(slices=slices)\n        return d\n\n\nclass ExplicitFrequencyDimensionDecoder(BaseDimensionDecoder):\n    def __init__(self):\n        super(ExplicitFrequencyDimensionDecoder, self).__init__(\n            ExplicitFrequencyDimension)\n        self.scale_decoder = CompositeScaleEncoderDecoder()\n\n    def args(self, d):\n        scale = self.scale_decoder.decode(d)\n        slices = [slice(start, stop) for (start, stop) in d['slices']]\n        return scale, slices\n"""
zounds/persistence/identitydimension.py,0,"b'from .basedimension import BaseDimensionEncoder, BaseDimensionDecoder\nfrom zounds.core import IdentityDimension\n\n\nclass IdentityDimensionEncoder(BaseDimensionEncoder):\n    def __init__(self):\n        super(IdentityDimensionEncoder, self).__init__(IdentityDimension)\n\n    def dict(self, o):\n        return dict()\n\n\nclass IdentityDimensionDecoder(BaseDimensionDecoder):\n    def __init__(self):\n        super(IdentityDimensionDecoder, self).__init__(IdentityDimension)\n'"
zounds/persistence/test_frequencydimension.py,0,"b'import unittest2\nfrom .frequencydimension import \\\n    FrequencyDimensionEncoder, FrequencyDimensionDecoder, \\\n    LinearScaleEncoderDecoder, GeometricScaleEncoderDecoder, \\\n    ExplicitScaleEncoderDecoder, ExplicitFrequencyDimensionEncoder, \\\n    ExplicitFrequencyDimensionDecoder\nfrom zounds.spectral import \\\n    FrequencyDimension, FrequencyBand, LinearScale, \\\n    GeometricScale, ExplicitScale, FrequencyScale, ExplicitFrequencyDimension\n\n\nclass ScaleEncodingTests(unittest2.TestCase):\n    def test_can_round_trip_linear_scale(self):\n        scale = LinearScale(FrequencyBand(20, 4000), n_bands=100)\n        encoder_decoder = LinearScaleEncoderDecoder()\n        self.assertTrue(encoder_decoder.can_encode(scale))\n        encoded = encoder_decoder.encode(scale)\n        self.assertTrue(encoder_decoder.can_decode(encoded))\n        decoded = encoder_decoder.decode(encoded)\n        self.assertEqual(scale, decoded)\n\n    def test_can_round_trip_geometric_scale(self):\n        scale = GeometricScale(20, 5000, 0.05, n_bands=100)\n        encoder_decoder = GeometricScaleEncoderDecoder()\n        self.assertTrue(encoder_decoder.can_encode(scale))\n        encoded = encoder_decoder.encode(scale)\n        self.assertTrue(encoder_decoder.can_decode(encoded))\n        decoded = encoder_decoder.decode(encoded)\n        self.assertEqual(scale, decoded)\n\n    def test_can_round_trip_explicit_scale(self):\n        scale = ExplicitScale(GeometricScale(20, 5000, 0.05, n_bands=100))\n        encoder_decoder = ExplicitScaleEncoderDecoder()\n        self.assertTrue(encoder_decoder.can_encode(scale))\n        encoded = encoder_decoder.encode(scale)\n        self.assertTrue(encoder_decoder.can_decode(encoded))\n        decoded = encoder_decoder.decode(encoded)\n        self.assertEqual(scale, decoded)\n\n\nclass FrequencyDimensionTests(unittest2.TestCase):\n    def setUp(self):\n        self.encoder = FrequencyDimensionEncoder()\n        self.decoder = FrequencyDimensionDecoder()\n\n    def test_can_round_trip(self):\n        band = FrequencyBand(20, 20000)\n        scale = LinearScale(band, 50)\n        dim = FrequencyDimension(scale)\n        encoded = self.encoder.encode(dim)\n        decoded = self.decoder.decode(encoded)\n        self.assertIsInstance(decoded, FrequencyDimension)\n        self.assertEqual(scale, decoded.scale)\n\n    def test_can_round_trip_specific_scale_type(self):\n        band = FrequencyBand(20, 20000)\n        scale = LinearScale(band, 50)\n        dim = FrequencyDimension(scale)\n        encoded = self.encoder.encode(dim)\n        decoded = self.decoder.decode(encoded)\n        self.assertIsInstance(decoded.scale, LinearScale)\n        self.assertEqual(scale, decoded.scale)\n\n    def test_can_round_trip_geometric_scale(self):\n        scale = GeometricScale(20, 5000, bandwidth_ratio=0.01, n_bands=100)\n        dim = FrequencyDimension(scale)\n        encoded = self.encoder.encode(dim)\n        decoded = self.decoder.decode(encoded)\n        self.assertIsInstance(decoded.scale, GeometricScale)\n        self.assertEqual(scale, decoded.scale)\n\n    def test_can_round_trip_explicit_scale(self):\n        scale = ExplicitScale(\n            GeometricScale(20, 5000, bandwidth_ratio=0.01, n_bands=100))\n        dim = FrequencyDimension(scale)\n        encoded = self.encoder.encode(dim)\n        decoded = self.decoder.decode(encoded)\n        self.assertIsInstance(decoded.scale, ExplicitScale)\n        self.assertEqual(scale, decoded.scale)\n\n    def test_raises_when_encountering_unknown_scale(self):\n        band = FrequencyBand(20, 20000)\n        scale = FrequencyScale(band, 50)\n        dim = FrequencyDimension(scale)\n        self.assertRaises(NotImplementedError, lambda: self.encoder.encode(dim))\n\n\nclass ExplicitFrequencyDimensionTests(unittest2.TestCase):\n\n    def setUp(self):\n        self.encoder = ExplicitFrequencyDimensionEncoder()\n        self.decoder = ExplicitFrequencyDimensionDecoder()\n\n    def test_can_round_trip(self):\n        scale = GeometricScale(20, 5000, 0.05, 3)\n        slices = [slice(0, 10), slice(10, 100), slice(100, 1000)]\n        dim = ExplicitFrequencyDimension(scale, slices)\n        encoded = self.encoder.encode(dim)\n        decoded = self.decoder.decode(encoded)\n        self.assertIsInstance(decoded, ExplicitFrequencyDimension)\n        self.assertEqual(dim, decoded)\n\n'"
zounds/persistence/test_identitydimension.py,0,"b""import unittest2\nfrom .identitydimension import IdentityDimensionEncoder, IdentityDimensionDecoder\nfrom zounds.core import IdentityDimension, Dimension\n\n\nclass OtherDimension(Dimension):\n    pass\n\n\nclass IdentityDimensionTests(unittest2.TestCase):\n    def setUp(self):\n        self.encoder = IdentityDimensionEncoder()\n        self.decoder = IdentityDimensionDecoder()\n\n    def test_identity_encoder_matches(self):\n        self.assertTrue(self.encoder.matches(IdentityDimension()))\n\n    def test_identity_encoder_does_not_match(self):\n        self.assertFalse(self.encoder.matches(OtherDimension()))\n\n    def test_identity_encoder_encodes(self):\n        encoded = self.encoder.encode(IdentityDimension())\n        self.assertIsInstance(encoded, dict)\n        self.assertEqual(IdentityDimension.__name__, encoded['type'])\n        self.assertEqual({}, encoded['data'])\n\n    def test_identity_decoder_matches(self):\n        encoded = {'type': IdentityDimension.__name__, 'data': {}}\n        self.assertTrue(self.decoder.matches(encoded))\n\n    def test_identity_decoder_does_not_match(self):\n        encoded = {'type': OtherDimension.__name__, 'data': {}}\n        self.assertFalse(self.decoder.matches(encoded))\n\n    def test_identity_decoder_decodes(self):\n        decoded = self.decoder.decode(\n                {'type': IdentityDimension.__name__, 'data': {}})\n        self.assertIsInstance(decoded, IdentityDimension)\n\n    def test_roundtrip(self):\n        encoded = self.encoder.encode(IdentityDimension())\n        decoded = self.decoder.decode(encoded)\n        self.assertIsInstance(decoded, IdentityDimension)\n"""
zounds/persistence/test_persistence.py,14,"b""import unittest2\nfrom zounds.core import ArrayWithUnits, IdentityDimension\nfrom zounds.timeseries import \\\n    TimeDimension, Seconds, Milliseconds, AudioSamples, SR11025\nfrom zounds.spectral import \\\n    FrequencyDimension, LinearScale, FrequencyBand, FrequencyAdaptive, \\\n    GeometricScale\nfrom .arraywithunits import \\\n    ArrayWithUnitsEncoder, ArrayWithUnitsDecoder, PackedArrayWithUnitsEncoder\nfrom .frequencyadaptive import FrequencyAdaptiveDecoder\nimport numpy as np\nfrom io import BytesIO\n\n\nclass ArrayWithUnitsFeatureTests(unittest2.TestCase):\n    def _roundtrip(self, arr, encoder=None, decoder=None):\n        encoder = encoder or ArrayWithUnitsEncoder()\n        decoder = decoder or ArrayWithUnitsDecoder()\n        items = []\n        for item in encoder._process(arr):\n            try:\n                items.append(item.encode())\n            except AttributeError:\n                items.append(item)\n        encoded = BytesIO(b''.join(items))\n        return decoder(encoded)\n\n    def test_can_pack_bits(self):\n        raw = np.random.binomial(1, 0.5, (100, 64))\n        arr = ArrayWithUnits(\n                raw, [TimeDimension(Seconds(1)), IdentityDimension()])\n        decoded = self._roundtrip(arr, encoder=PackedArrayWithUnitsEncoder())\n        self.assertIsInstance(decoded, ArrayWithUnits)\n        self.assertEqual(2, len(decoded.dimensions))\n        self.assertEqual((100, 8), decoded.shape)\n\n    def test_can_roundtrip_frequency_adaptive_transform(self):\n        td = TimeDimension(\n            duration=Seconds(1),\n            frequency=Milliseconds(500))\n        scale = GeometricScale(20, 5000, 0.05, 120)\n        arrs = [np.zeros((10, x)) for x in range(1, 121)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n        decoded = self._roundtrip(fa, decoder=FrequencyAdaptiveDecoder())\n        self.assertIsInstance(decoded, FrequencyAdaptive)\n        self.assertEqual(fa.dimensions, decoded.dimensions)\n\n    def test_can_round_trip_audio_samples(self):\n        raw = np.random.random_sample(11025 * 10)\n        arr = AudioSamples(raw, SR11025())\n        decoded = self._roundtrip(arr)\n        self.assertIsInstance(decoded, ArrayWithUnits)\n        self.assertEqual(1, len(decoded.dimensions))\n        td = decoded.dimensions[0]\n        self.assertIsInstance(td, TimeDimension)\n        np.testing.assert_allclose(decoded, raw)\n\n    def test_can_round_trip_1d_identity_dimension(self):\n        raw = np.arange(10)\n        arr = ArrayWithUnits(raw, (IdentityDimension(),))\n        decoded = self._roundtrip(arr)\n        self.assertIsInstance(decoded, ArrayWithUnits)\n        self.assertEqual(1, len(decoded.dimensions))\n        d = decoded.dimensions[0]\n        self.assertIsInstance(d, IdentityDimension)\n        np.testing.assert_allclose(decoded, raw)\n\n    def test_can_round_trip_2d_with_identity_dimension(self):\n        raw = np.random.random_sample((10, 10))\n        dim = TimeDimension(Seconds(1), Milliseconds(500))\n        arr = ArrayWithUnits(raw, (IdentityDimension(), dim))\n        decoded = self._roundtrip(arr)\n        self.assertIsInstance(decoded, ArrayWithUnits)\n        self.assertEqual(2, len(decoded.dimensions))\n        idd = decoded.dimensions[0]\n        self.assertIsInstance(idd, IdentityDimension)\n        td = decoded.dimensions[1]\n        self.assertIsInstance(td, TimeDimension)\n        self.assertEqual(Seconds(1), td.frequency)\n        self.assertEqual(Milliseconds(500), td.duration)\n\n        np.testing.assert_allclose(decoded, raw)\n\n    def test_can_round_trip_1d_constant_rate_time_series(self):\n        dim = TimeDimension(Seconds(1), Milliseconds(500))\n        raw = np.arange(10)\n        ts = ArrayWithUnits(raw, (dim,))\n        decoded = self._roundtrip(ts)\n        self.assertIsInstance(decoded, ArrayWithUnits)\n        self.assertEqual(1, len(decoded.dimensions))\n        self.assertIsInstance(decoded.dimensions[0], TimeDimension)\n        td = decoded.dimensions[0]\n        self.assertEqual(Seconds(1), td.frequency)\n        self.assertEqual(Milliseconds(500), td.duration)\n        np.testing.assert_allclose(decoded, raw)\n\n    def test_can_round_trip_2d_constant_rate_time_series(self):\n        dim1 = TimeDimension(Seconds(1), Milliseconds(500))\n        scale = LinearScale(FrequencyBand(20, 20000), 100)\n        dim2 = FrequencyDimension(scale)\n        raw = np.random.random_sample((10, 100))\n        ts = ArrayWithUnits(raw, (dim1, dim2))\n        decoded = self._roundtrip(ts)\n        self.assertIsInstance(decoded, ArrayWithUnits)\n        self.assertEqual(2, len(decoded.dimensions))\n        self.assertIsInstance(decoded.dimensions[0], TimeDimension)\n        td = decoded.dimensions[0]\n        self.assertIsInstance(td, TimeDimension)\n        self.assertEqual(Seconds(1), td.frequency)\n        self.assertEqual(Milliseconds(500), td.duration)\n        fd = decoded.dimensions[1]\n        self.assertIsInstance(fd, FrequencyDimension)\n        self.assertEqual(scale, fd.scale)\n        np.testing.assert_allclose(decoded, raw)\n\n    def test_can_round_trip_3d_constant_rate_time_series_with_frequency_dim(\n            self):\n        dim1 = TimeDimension(Seconds(2), Milliseconds(1000))\n        dim2 = TimeDimension(Seconds(1), Milliseconds(500))\n        scale = LinearScale(FrequencyBand(20, 20000), 100)\n        dim3 = FrequencyDimension(scale)\n        raw = np.random.random_sample((5, 2, 100))\n        ts = ArrayWithUnits(raw, (dim1, dim2, dim3))\n\n        decoded = self._roundtrip(ts)\n        self.assertIsInstance(decoded, ArrayWithUnits)\n        self.assertEqual(3, len(decoded.dimensions))\n\n        td1 = decoded.dimensions[0]\n        self.assertIsInstance(td1, TimeDimension)\n        self.assertEqual(Seconds(2), td1.frequency)\n        self.assertEqual(Milliseconds(1000), td1.duration)\n\n        td2 = decoded.dimensions[1]\n        self.assertIsInstance(td2, TimeDimension)\n        self.assertEqual(Seconds(1), td2.frequency)\n        self.assertEqual(Milliseconds(500), td2.duration)\n\n        fd = decoded.dimensions[2]\n        self.assertIsInstance(fd, FrequencyDimension)\n        self.assertEqual(scale, fd.scale)\n        np.testing.assert_allclose(decoded, raw)\n\n\n"""
zounds/persistence/test_roundtrip.py,0,"b'import unittest2\nfrom .dimension import DimensionEncoder, DimensionDecoder\nfrom zounds.timeseries import TimeDimension, Seconds, Milliseconds\nfrom zounds.spectral import FrequencyBand, LinearScale, FrequencyDimension\nfrom zounds.core import IdentityDimension\n\n\nclass RoundTripTests(unittest2.TestCase):\n\n    def roundtrip(self, o):\n        encoder = DimensionEncoder()\n        decoder = DimensionDecoder()\n        encoded = list(encoder.encode(o))\n        return list(decoder.decode(encoded))\n\n    def test_can_round_trip_single_identity_dimension(self):\n        original = [IdentityDimension()]\n        restored = self.roundtrip(original)\n        self.assertSequenceEqual(original, restored)\n\n    def test_can_round_trip_mixed_dimensions(self):\n        original = [\n            IdentityDimension(),\n            TimeDimension(Seconds(1), Milliseconds(500)),\n            FrequencyDimension(LinearScale(FrequencyBand(100, 1000), 10))\n        ]\n        restored = self.roundtrip(original)\n        self.assertSequenceEqual(original, restored)\n'"
zounds/persistence/test_timedimension.py,0,"b'import unittest2\nfrom .timedimension import TimeDimensionEncoder, TimeDimensionDecoder\nfrom zounds.timeseries import TimeDimension, Seconds, Milliseconds\n\n\nclass TimeDimensionTests(unittest2.TestCase):\n    def setUp(self):\n        self.encoder = TimeDimensionEncoder()\n        self.decoder = TimeDimensionDecoder()\n\n    def test_can_round_trip(self):\n        td = TimeDimension(Seconds(1), Milliseconds(500), 100)\n        encoded = self.encoder.encode(td)\n        decoded = self.decoder.decode(encoded)\n        self.assertIsInstance(decoded, TimeDimension)\n        self.assertEqual(Seconds(1), decoded.frequency)\n        self.assertEqual(Milliseconds(500), decoded.duration)\n        self.assertEqual(100, decoded.size)\n'"
zounds/persistence/test_timeslice.py,0,"b'import unittest2\nfrom .timeslice import TimeSliceEncoder, TimeSliceDecoder\nfrom zounds.timeseries import TimeSlice, Seconds, Milliseconds\n\n\nclass TimeSliceRoundTripTests(unittest2.TestCase):\n    def _roundtrip(self, ts):\n        encoder = TimeSliceEncoder()\n        decoder = TimeSliceDecoder()\n        encoded = encoder.dict(ts)\n        decoded = TimeSlice(**decoder.kwargs(encoded))\n        self.assertEqual(ts, decoded)\n\n    def test_can_roundtrip(self):\n        self._roundtrip(TimeSlice(start=Seconds(1), duration=Seconds(1)))\n\n    def test_can_roundtrip_milliseconds(self):\n        self._roundtrip(TimeSlice(start=Seconds(1), duration=Milliseconds(250)))\n'"
zounds/persistence/test_util.py,0,"b'import unittest2\nfrom .util import decode_timedelta, extract_init_args\nfrom zounds.timeseries import Seconds\n\n\nclass DecodeTimedeltaTests(unittest2.TestCase):\n    def test_already_decoded_instance_is_returned(self):\n        td = Seconds(10)\n        decoded = decode_timedelta(td)\n        self.assertEqual(td, decoded)\n\n\nclass ExtractInitArgsTests(unittest2.TestCase):\n    def test_should_not_include_locals(self):\n        class Blah(object):\n            def __init__(self, x, y):\n                super(Blah, self).__init__()\n                self.x = x\n                z = 10\n                self.y = y + z\n\n        b = Blah(20, 30)\n        args = extract_init_args(b)\n        self.assertEqual(2, len(args))\n        self.assertIn(20, args)\n        self.assertIn(40, args)\n        self.assertNotIn(10, args)\n'"
zounds/persistence/timedimension.py,0,"b""from zounds.timeseries import TimeDimension\nfrom .basedimension import BaseDimensionEncoder, BaseDimensionDecoder\nfrom .util import encode_timedelta, decode_timedelta\n\n\nclass TimeDimensionEncoder(BaseDimensionEncoder):\n\n    def __init__(self):\n        super(TimeDimensionEncoder, self).__init__(TimeDimension)\n\n    def dict(self, o):\n        return dict(\n                frequency=encode_timedelta(o.frequency),\n                duration=encode_timedelta(o.duration),\n                size=o.size)\n\n\nclass TimeDimensionDecoder(BaseDimensionDecoder):\n    def __init__(self):\n        super(TimeDimensionDecoder, self).__init__(TimeDimension)\n\n    def kwargs(self, d):\n        return dict(\n            frequency=decode_timedelta(d['frequency']),\n            duration=decode_timedelta(d['duration']),\n            size=d['size'])\n"""
zounds/persistence/timeslice.py,0,"b""from .util import encode_timedelta, decode_timedelta\n\n\nclass TimeSliceEncoder(object):\n    def __init__(self):\n        super(TimeSliceEncoder, self).__init__()\n\n    def dict(self, ts):\n        return dict(\n            start=encode_timedelta(ts.start),\n            duration=encode_timedelta(ts.duration))\n\n\nclass TimeSliceDecoder(object):\n    def __init__(self):\n        super(TimeSliceDecoder, self).__init__()\n\n    def kwargs(self, d):\n        return dict(\n            start=decode_timedelta(d['start']),\n            duration=decode_timedelta(d['duration']))\n\n"""
zounds/persistence/util.py,3,"b'import base64\nimport re\nimport numpy as np\nimport inspect\n\nTIMEDELTA_DTYPE_RE = re.compile(r\'\\[(?P<dtype>[^\\]]+)\\]\')\n\n\ndef encode_timedelta(td):\n    dtype = TIMEDELTA_DTYPE_RE.search(str(td.dtype)).groupdict()[\'dtype\']\n    # base64 encoded value, encoded as a utf-8\n    encoded = base64.b64encode(td.astype(np.uint64).tostring()).decode()\n    return encoded, dtype\n\n\ndef decode_timedelta(t):\n    try:\n        v = np.frombuffer(base64.b64decode(t[0]), dtype=np.uint64)[0]\n        s = t[1]\n        return np.timedelta64(int(v), s)\n    except IndexError:\n        return t\n\n\ndef extract_init_args(instance):\n    """"""\n    Given an instance, and under the assumption that member variables have the\n    same name as the __init__ arguments, extract the arguments so they can\n    be used to reconstruct the instance when deserializing\n    """"""\n    cls = instance.__class__\n    args = [x for x in inspect.getargspec(cls.__init__).args if x != \'self\']\n    return [instance.__dict__[key] for key in args]\n'"
zounds/segment/__init__.py,0,"b'from .onset import \\\n    MeasureOfTransience, MovingAveragePeakPicker, TimeSliceFeature, \\\n    ComplexDomain'"
zounds/segment/onset.py,23,"b'import numpy as np\nfrom featureflow import Node, Feature\n\nfrom zounds.nputil import safe_unit_norm\nfrom zounds.timeseries import \\\n    TimeSlice, Picoseconds, TimeDimension, VariableRateTimeSeries, \\\n    VariableRateTimeSeriesEncoder, VariableRateTimeSeriesDecoder\nfrom zounds.core import ArrayWithUnits\n\n\nclass MeasureOfTransience(Node):\n    """"""\n    Measure of Transience, as defined in section 5.2.1 of\n    http://www.mp3-tech.org/programmer/docs/Masri_thesis.pdf\n\n    Uses the ratio of high-frequency content in the signal to detect onsets.\n    Effective for percussive onsets.\n    """"""\n\n    def __init__(self, needs=None):\n        super(MeasureOfTransience, self).__init__(needs=needs)\n\n    def _first_chunk(self, data):\n        data = np.abs(data)\n        self._bin_numbers = np.arange(1, data.shape[1] + 1)\n        padding = np.zeros(data.shape[1])\n        padding[:] = data[0]\n\n        return ArrayWithUnits(\n            np.concatenate([padding[None, :], data]), data.dimensions)\n\n    # TODO: this pattern of hanging on to the last sample of the previous chunk,\n    # and appending to the next chunk probably happens somewhere else, and\n    # should be generalized\n    def _enqueue(self, data, pusher):\n        if self._cache is None:\n            self._cache = data\n        else:\n            self._cache = self._cache.concatenate(data)\n\n    def _dequeue(self):\n        data = self._cache\n\n        self._cache = ArrayWithUnits(\n            self._cache[None, -1], self._cache.dimensions)\n\n        return data\n\n    def _process(self, data):\n        data = np.abs(data)\n        magnitude = (data[:, 2:] ** 2)\n        energy = magnitude.sum(axis=1)\n        hfc = (magnitude * self._bin_numbers[2:]).sum(axis=1)\n        energy[energy == 0] = 1e-12\n        hfc[hfc == 0] = 1e-12\n        mot = (hfc[1:] / hfc[:-1]) * (hfc[1:] / energy[1:])\n        yield mot\n\n\nclass ComplexDomain(Node):\n    """"""\n    Complex-domain onset detection as described in\n    http://www.eecs.qmul.ac.uk/legacy/dafx03/proceedings/pdfs/dafx81.pdf\n    """"""\n\n    def __init__(self, needs=None):\n        super(ComplexDomain, self).__init__(needs=needs)\n\n    def _process(self, data):\n        # delta between expected and actual phase\n        # TODO: unwrap phases before computing deltas, to avoid artifacts\n        # or discontinuties from phase boundary wrapping\n        angle = np.angle(data)\n        angle = np.unwrap(angle, axis=1)\n        angle = np.angle(angle[:, 2] - (2 * angle[:, 1]) + angle[:, 0])\n\n        # expected magnitude\n        expected = np.abs(data[:, 1, :])\n        # actual magnitude\n        actual = np.abs(data[:, 2, :])\n        # detection function array\n        detect = np.zeros(angle.shape)\n\n        # where phase delta is zero, detection function is the difference \n        # between expected and actual magnitude\n        zero_phase_delta_indices = np.where(angle == 0)\n        detect[zero_phase_delta_indices] = \\\n            (expected - actual)[zero_phase_delta_indices]\n\n        # where phase delta is non-zero, detection function combines magnitude\n        # and phase deltas\n        nonzero_phase_delta_indices = np.where(angle != 0)\n        detect[nonzero_phase_delta_indices] = (\n            ((expected ** 2) + (actual ** 2) -\n             (2 * expected * actual * np.cos(angle))) ** 0.5)[\n            nonzero_phase_delta_indices]\n\n        dims = \\\n            [TimeDimension(data.frequency, data.duration // 3)] \\\n            + data.dimensions[1:]\n        output = ArrayWithUnits(detect.sum(axis=1), dims)\n        yield output\n\n\nclass Flux(Node):\n    def __init__(self, unit_norm=False, needs=None):\n        super(Flux, self).__init__(needs=needs)\n        self._memory = None\n        self._unit_norm = unit_norm\n\n    def _enqueue(self, data, pusher):\n        if self._memory is None:\n            self._cache = np.vstack((data[0], data))\n        else:\n            self._cache = np.vstack((self._memory, data))\n\n        self._cache = ArrayWithUnits(self._cache, data.dimensions)\n\n        self._memory = data[-1]\n\n    def _process(self, data):\n        if self._unit_norm:\n            data = safe_unit_norm(data)\n        diff = np.diff(data, axis=0)\n\n        yield ArrayWithUnits(\n            np.linalg.norm(diff, axis=-1), data.dimensions)\n\n\nclass BasePeakPicker(Node):\n    def __init__(self, needs=None):\n        super(BasePeakPicker, self).__init__(needs=needs)\n        self._pos = Picoseconds(0)\n        self._leftover_timestamp = self._pos\n\n    def _onset_indices(self, data):\n        raise NotImplementedError()\n\n    def _last_chunk(self):\n        yield VariableRateTimeSeries((\n            (TimeSlice(\n             start=self._leftover_timestamp,\n             duration=self._pos - self._leftover_timestamp), np.zeros(0)),\n        ))\n\n    def _process(self, data):\n        td = data.dimensions[0]\n        frequency = td.frequency\n\n        indices = self._onset_indices(data)\n        timestamps = self._pos + (indices * frequency)\n        self._pos += len(data) * frequency\n\n        timestamps = [self._leftover_timestamp] + list(timestamps)\n        self._leftover_timestamp = timestamps[-1]\n\n        time_slices = TimeSlice.slices(timestamps)\n        vrts = VariableRateTimeSeries([(ts, np.zeros(0)) for ts in time_slices])\n        yield vrts\n\n\nclass MovingAveragePeakPicker(BasePeakPicker):\n    def __init__(self, aggregate=np.mean, needs=None):\n        super(MovingAveragePeakPicker, self).__init__(needs=needs)\n        self._aggregate = aggregate\n\n    def _first_chunk(self, data):\n        self._center = data.shape[1] // 2\n        return data\n\n    def _onset_indices(self, data):\n        # compute the threshold for onsets\n        agg = self._aggregate(data, axis=1) * 1.25\n        # find indices that are peaks\n        diff = np.diff(data[:, self._center - 2: self._center + 1])\n        peaks = (diff[:, 0] > 0) & (diff[:, 1] < 0)\n        # find indices that are above the local average or median\n        over_thresh = data[:, self._center] > agg\n        # return the intersection of the two\n        return np.where(peaks & over_thresh)[0]\n\n\nclass TimeSliceFeature(Feature):\n    def __init__(\n            self,\n            extractor,\n            needs=None,\n            store=False,\n            key=None,\n            encoder=VariableRateTimeSeriesEncoder,\n            decoder=VariableRateTimeSeriesDecoder(),\n            **extractor_args):\n        super(TimeSliceFeature, self).__init__(\n            extractor,\n            needs=needs,\n            store=store,\n            encoder=encoder,\n            decoder=decoder,\n            key=key,\n            **extractor_args)\n'"
zounds/segment/test_onset.py,6,"b""\nimport unittest2\nimport featureflow as ff\nimport numpy as np\n\nfrom zounds.util import simple_in_memory_settings\nfrom zounds.basic import stft, Pooled\nfrom zounds.timeseries import \\\n    HalfLapped, Stride, SR44100, Seconds, VariableRateTimeSeriesFeature\nfrom zounds.spectral import SlidingWindow\nfrom zounds.synthesize import TickSynthesizer\nfrom .onset import \\\n    MeasureOfTransience, MovingAveragePeakPicker, TimeSliceFeature, \\\n    ComplexDomain\nfrom zounds.persistence import ArrayWithUnitsFeature\n\n\nclass OnsetTests(unittest2.TestCase):\n    def setUp(self):\n        self.samplerate = SR44100()\n        self.wscheme = HalfLapped()\n        self.STFT = stft(\n            store_fft=True,\n            resample_to=self.samplerate,\n            wscheme=self.wscheme)\n\n    def ticks(self, samplerate, duration, tick_frequency):\n        synth = TickSynthesizer(samplerate)\n        samples = synth.synthesize(duration, tick_frequency)\n        print('DURATION IN SECONDS', samples.span, samples.shape)\n        return samples.encode()\n\n    def do_assertions(self, onset_class, feature_func):\n        raw = self.ticks(self.samplerate, Seconds(4), Seconds(1))\n        _id = onset_class.process(meta=raw)\n        doc = onset_class(_id)\n        slices = list(doc.slices.slices)\n        self.assertEqual(4, len(slices))\n        frame_hop = self.wscheme.frequency\n\n        self.assertLess(abs(Seconds(0) - slices[0].start), frame_hop)\n        self.assertLess(abs(Seconds(1) - slices[1].start), frame_hop)\n        self.assertLess(abs(Seconds(2) - slices[2].start), frame_hop)\n        self.assertLess(abs(Seconds(3) - slices[3].start), frame_hop)\n\n        self.assertLess(abs(Seconds(1) - slices[0].duration), frame_hop)\n        self.assertLess(abs(Seconds(1) - slices[1].duration), frame_hop)\n        self.assertLess(abs(Seconds(1) - slices[2].duration), frame_hop)\n        # BUG: The last position reported by BasePeakPicker isn't guaranteed\n        # to be the end of the file\n        # self.assertLess(abs(Seconds(1) - slices[3].duration), frame_hop)\n        return doc\n\n    @unittest2.skip\n    def test_complex_domain_onset_positions(self):\n        class Settings(ff.PersistenceSettings):\n            id_provider = ff.UuidProvider()\n            key_builder = ff.StringDelimitedKeyBuilder()\n            database = ff.InMemoryDatabase(key_builder=key_builder)\n\n        class WithOnsets(self.STFT, Settings):\n            onset_prep = ArrayWithUnitsFeature(\n                SlidingWindow,\n                needs=self.STFT.fft,\n                wscheme=self.wscheme * (1, 3),\n                store=False)\n\n            complex_domain = ArrayWithUnitsFeature(\n                ComplexDomain,\n                needs=onset_prep,\n                store=False)\n\n            sliding_detection = ArrayWithUnitsFeature(\n                SlidingWindow,\n                needs=complex_domain,\n                wscheme=self.wscheme * (1, 11),\n                padwith=5,\n                store=False)\n\n            slices = TimeSliceFeature(\n                MovingAveragePeakPicker,\n                needs=sliding_detection,\n                aggregate=np.median,\n                store=True)\n\n        self.do_assertions(WithOnsets, lambda x: x.complex_domain)\n\n    def test_percussive_onset_positions(self):\n        @simple_in_memory_settings\n        class WithOnsets(self.STFT):\n            transience = ArrayWithUnitsFeature(\n                MeasureOfTransience,\n                needs=self.STFT.fft,\n                store=True)\n\n            sliding_detection = ArrayWithUnitsFeature(\n                SlidingWindow,\n                needs=transience,\n                wscheme=self.wscheme * Stride(frequency=1, duration=10),\n                padwith=5,\n                store=False)\n\n            slices = TimeSliceFeature(\n                MovingAveragePeakPicker,\n                needs=sliding_detection,\n                aggregate=np.median,\n                store=True)\n\n        self.do_assertions(WithOnsets, lambda x: x.transience)\n\n    def test_can_pool_stored_time_slice_feature(self):\n        @simple_in_memory_settings\n        class WithOnsets(self.STFT):\n            transience = ArrayWithUnitsFeature(\n                MeasureOfTransience,\n                needs=self.STFT.fft,\n                store=True)\n\n            sliding_detection = ArrayWithUnitsFeature(\n                SlidingWindow,\n                needs=transience,\n                wscheme=self.wscheme * Stride(frequency=1, duration=10),\n                padwith=5,\n                store=False)\n\n            slices = TimeSliceFeature(\n                MovingAveragePeakPicker,\n                needs=sliding_detection,\n                aggregate=np.median,\n                store=True)\n\n            pooled = VariableRateTimeSeriesFeature(\n                Pooled,\n                needs=(self.STFT.fft, slices),\n                op=np.max,\n                axis=0,\n                store=True)\n\n        signal = self.ticks(self.samplerate, Seconds(4), Seconds(1))\n        _id = WithOnsets.process(meta=signal)\n        doc = WithOnsets(_id)\n        self.assertEqual((4, 1025), doc.pooled.slicedata.shape)\n\n    def test_can_pool_non_stored_time_slice_feature(self):\n        @simple_in_memory_settings\n        class WithOnsets(self.STFT):\n            transience = ArrayWithUnitsFeature(\n                MeasureOfTransience,\n                needs=self.STFT.fft,\n                store=True)\n\n            sliding_detection = ArrayWithUnitsFeature(\n                SlidingWindow,\n                needs=transience,\n                wscheme=self.wscheme * Stride(frequency=1, duration=10),\n                padwith=5,\n                store=False)\n\n            slices = TimeSliceFeature(\n                MovingAveragePeakPicker,\n                needs=sliding_detection,\n                aggregate=np.median,\n                store=True)\n\n        signal = self.ticks(self.samplerate, Seconds(4), Seconds(1))\n        _id = WithOnsets.process(meta=signal)\n\n        class WithPooled(WithOnsets):\n            pooled = VariableRateTimeSeriesFeature(\n                Pooled,\n                needs=(self.STFT.fft, WithOnsets.slices),\n                op=np.max,\n                axis=0,\n                store=True)\n\n        doc = WithPooled(_id)\n        self.assertEqual((4, 1025), doc.pooled.slicedata.shape)\n"""
zounds/segment/test_peakpicker.py,2,"b'import unittest2\nfrom zounds.core import ArrayWithUnits\nfrom zounds.timeseries import TimeDimension, Seconds, VariableRateTimeSeries\nfrom .onset import BasePeakPicker\nimport numpy as np\n\n\nclass BasePeakPickerTests(unittest2.TestCase):\n\n    class PeakPicker(BasePeakPicker):\n        def _onset_indices(self, data):\n            indices = np.random.permutation(np.arange(len(data)))[:3]\n            indices.sort()\n            return indices\n\n    def test_peak_picker_returns_variable_rate_time_series(self):\n        data = ArrayWithUnits(\n            np.zeros(100),\n            dimensions=[TimeDimension(frequency=Seconds(1))])\n        picker = BasePeakPickerTests.PeakPicker()\n        results = next(picker._process(data))\n        self.assertEqual(3, len(results))\n        self.assertIsInstance(results, VariableRateTimeSeries)\n'"
zounds/soundfile/__init__.py,0,"b'""""""\nThe soundfile module introduces :class:`featureflow.Node` subclasses that know\nhow to process low-level audio samples and common audio encodings.\n""""""\n\nfrom .audio_metadata import MetaData, AudioMetaData, AudioMetaDataEncoder\n\nfrom .ogg_vorbis import \\\n    OggVorbis, OggVorbisDecoder, OggVorbisEncoder, OggVorbisFeature, \\\n    OggVorbisWrapper\n\nfrom .audiostream import AudioStream\n\nfrom .resample import Resampler\n\nfrom .chunksize import ChunkSizeBytes\n\nfrom .functional import resample\n'"
zounds/soundfile/audio_metadata.py,0,"b'from featureflow import Node, Aggregator\nimport os\nfrom soundfile import SoundFile\nimport requests\nimport json\nimport io\nfrom urllib.parse import urlparse\nimport featureflow as ff\n\n\nclass AudioMetaData(object):\n    """"""\n    Encapsulates metadata about a source audio file, including things like\n    text descriptions and licensing information.\n\n    Args:\n        uri (requests.Request or str): uri may be either a string representing\n            a network resource or a local file, or a :class:`requests.Request`\n            instance\n        samplerate (int): the samplerate of the source audio\n        channels (int): the number of channels of the source audio\n        licensing (str): The licensing agreement (if any) that applies to the\n            source audio\n        description (str): a text description of the source audio\n        tags (str): text tags that apply to the source audio\n        kwargs (dict): other arbitrary properties about the source audio\n\n    Raises:\n        ValueError: when `uri` is not provided\n\n    See Also:\n        :class:`zounds.datasets.FreeSoundSearch`\n        :class:`zounds.datasets.InternetArchive`\n        :class:`zounds.datasets.PhatDrumLoops`\n    """"""\n\n    def __init__(\n            self,\n            uri=None,\n            samplerate=None,\n            channels=None,\n            licensing=None,\n            description=None,\n            tags=None,\n            **kwargs):\n        super(AudioMetaData, self).__init__()\n\n        if not uri:\n            raise ValueError(\'You must at least supply a uri\')\n\n        self.uri = uri\n        self.samplerate = samplerate\n        self.channels = channels\n        self.licensing = licensing\n        self.description = description\n        self.tags = tags\n\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n    @property\n    def request(self):\n        if hasattr(self.uri, \'url\'):\n            return self.uri\n\n    def __eq__(self, other):\n        return self.uri == other.uri \\\n               and self.samplerate == other.samplerate \\\n               and self.channels == other.channels \\\n               and self.licensing == other.licensing \\\n               and self.description == other.description \\\n               and self.tags == other.tags\n\n    def __repr__(self):\n        return self.__dict__.__str__()\n\n    def __str__(self):\n        return self.__repr__()\n\n\nclass AudioMetaDataEncoder(Aggregator, Node):\n    content_type = \'application/json\'\n\n    def __init__(self, needs=None):\n        super(AudioMetaDataEncoder, self).__init__(needs=needs)\n\n    def _uri(self, uri):\n        if isinstance(uri, requests.Request):\n            return uri.url\n        elif isinstance(uri, io.BytesIO) or isinstance(uri, ff.ZipWrapper):\n            return None\n        else:\n            return uri\n\n    def _process(self, data):\n        d = dict(data.__dict__)\n        d[\'uri\'] = self._uri(data.uri)\n        yield json.dumps(d)\n\n\nclass MetaData(Node):\n    def __init__(self, needs=None):\n        super(MetaData, self).__init__(needs=needs)\n\n    @staticmethod\n    def _is_url(s):\n        if not isinstance(s, str):\n            return False\n        parsed = urlparse(s)\n        return parsed.scheme and parsed.netloc\n\n    @staticmethod\n    def _is_local_file(s):\n        try:\n            return os.path.exists(s)\n        except TypeError:\n            return False\n\n    @staticmethod\n    def _is_file(s):\n        try:\n            s.tell()\n            return True\n        except AttributeError:\n            return False\n\n    def _process(self, data):\n        if isinstance(data, AudioMetaData):\n            yield data\n        elif self._is_url(data):\n            req = requests.Request(\n                method=\'GET\',\n                url=data,\n                headers={\'Range\': \'bytes=0-\'})\n            yield AudioMetaData(uri=req)\n        elif isinstance(data, requests.Request):\n            if \'range\' not in data.headers:\n                data.headers[\'range\'] = \'bytes=0-\'\n            yield AudioMetaData(uri=data)\n        elif self._is_local_file(data) or self._is_file(data):\n            sf = SoundFile(data)\n            yield AudioMetaData(\n                uri=data,\n                samplerate=sf.samplerate,\n                channels=sf.channels)\n        else:\n            yield AudioMetaData(uri=data)\n'"
zounds/soundfile/audiostream.py,0,"b'from io import BytesIO\nfrom os import SEEK_END\nfrom soundfile import SoundFile\nfrom zounds.timeseries import audio_sample_rate, AudioSamples\nfrom .byte_depth import chunk_size_samples\nfrom featureflow import Node\n\n\nclass AudioStream(Node):\n    """"""\n    `AudioStream` expects to process a raw stream of bytes (e.g. one\n    produced by :class:`featureflow.ByteStream`) and produces chunks of\n    :class:`~zounds.timeseries.AudioSamples`\n\n    Args:\n        sum_to_mono (bool): True if this node should return a\n            :class:`~zounds.timeseries.AudioSamples` instance with a single\n            channel\n        needs (Feature): a processing node that produces a byte stream (e.g.\n            :class:`~featureflow.ByteStream`\n\n    Here\'s how\'d you typically see :class:`AudioStream` used in a processing\n    graph.\n\n    .. code:: python\n\n        import featureflow as ff\n        import zounds\n\n        chunksize = zounds.ChunkSizeBytes(\n            samplerate=zounds.SR44100(),\n            duration=zounds.Seconds(30),\n            bit_depth=16,\n            channels=2)\n\n        @zounds.simple_in_memory_settings\n        class Document(ff.BaseModel):\n            meta = ff.JSONFeature(\n                zounds.MetaData,\n                store=True,\n                encoder=zounds.AudioMetaDataEncoder)\n\n            raw = ff.ByteStreamFeature(\n                ff.ByteStream,\n                chunksize=chunksize,\n                needs=meta,\n                store=False)\n\n            pcm = zounds.AudioSamplesFeature(\n                zounds.AudioStream,\n                needs=raw,\n                store=True)\n\n\n        synth = zounds.NoiseSynthesizer(zounds.SR11025())\n        samples = synth.synthesize(zounds.Seconds(10))\n        raw_bytes = samples.encode()\n        _id = Document.process(meta=raw_bytes)\n        doc = Document(_id)\n        print doc.pcm.__class__  # returns an AudioSamples instance\n    """"""\n\n    def __init__(\n            self,\n            sum_to_mono=True,\n            needs=None):\n\n        super(AudioStream, self).__init__(needs=needs)\n        self._sum_to_mono = sum_to_mono\n        self._buf = None\n        self._sf = None\n        self._chunk_size_samples = None\n        self._cache = b\'\'\n\n    def _enqueue(self, data, pusher):\n        self._cache += data\n\n    def _dequeue(self):\n        v = self._cache\n        self._cache = b\'\'\n        return v\n\n    def _get_samples(self):\n        raw_samples = self._sf.read(self._chunk_size_samples)\n        sr = audio_sample_rate(self._sf.samplerate)\n        samples = AudioSamples(raw_samples, sr)\n        if self._sum_to_mono:\n            return samples.mono\n        return samples\n\n    def _process(self, data):\n        b = data\n\n        # TODO: Use the _first_chunk() hook instead of the if statement\n        if self._buf is None:\n            self._buf = MemoryBuffer(b.total_length)\n\n        self._buf.write(b)\n\n        if self._sf is None:\n            self._sf = SoundFile(self._buf)\n\n        if self._chunk_size_samples is None:\n            self._chunk_size_samples = chunk_size_samples(self._sf, b)\n\n        yield self._get_samples()\n\n    def _last_chunk(self):\n        samples = self._get_samples()\n        while samples.size:\n            yield samples\n            samples = self._get_samples()\n\n\nclass MemoryBuffer(object):\n    """"""\n    This class is the implementation of the virtual io interface required by\n    PySoundfile/libsndfile.\n\n    Some of the stateful/hacky things in this class (see KLUDGE note below)\n    could be avoided if this class had the following modifications:\n        - it maintains its own position\n        - it maintains the span of its buffer/BytesIO instance\n        -\n    """"""\n\n    def __init__(self, content_length, max_size=10 * 1024 * 1024):\n        super(MemoryBuffer, self).__init__()\n        self._content_length = content_length\n        self._buf = BytesIO()\n        self._max_size = max_size\n        self.tell = self._tell\n\n        self._total_bytes_written = 0\n        self._total_bytes_read = 0\n\n    def read(self, count):\n        if count == -1:\n            return self._buf.read()\n        data = self._buf.read(count)\n        return data\n\n    def readinto(self, buf):\n        data = self.read(len(buf))\n        ld = len(data)\n        buf[:ld] = data\n        return ld\n\n    def write(self, data):\n        read_pos = self._buf.tell()\n        if read_pos > self._max_size:\n            new_buf = BytesIO()\n            new_buf.write(self._buf.read())\n            self._buf = new_buf\n            read_pos = 0\n        self._buf.seek(0, 2)\n        self._buf.write(data)\n        self._buf.seek(read_pos)\n        return len(data)\n\n    def _tell(self):\n        return self._buf.tell()\n\n    def _tell_end(self):\n        self.tell = self._tell\n        return self._content_length\n\n    def seek(self, offset, whence):\n        if whence == SEEK_END:\n            # KLUDGE: PySoundfile no longer supports __len__, which means that\n            # this stateful garbage is required.\n            self.tell = self._tell_end\n        self._buf.seek(offset, whence)\n\n    def flush(self):\n        self._buf.flush()\n'"
zounds/soundfile/byte_depth.py,0,"b'\n\n_lookup = {\n    # https://support.microsoft.com/en-us/kb/89879\n    \'MS_ADPCM\': 1,\n    \'IMA_ADPCM\': 1,\n    \'PCM_S8\': 1,\n    \'PCM_U8\': 1,\n    \'PCM_16\': 2,\n    \'PCM_24\': 3,\n    \'PCM_32\': 4,\n    \'FLOAT\': 4,\n    \'DOUBLE\': 8,\n    # this is a guess, erring on the side of caution\n    \'VORBIS\': 3\n}\n\n\ndef chunk_size_samples(sf, buf):\n    """"""\n    Black magic to account for the fact that libsndfile\'s behavior varies\n    depending on file format when using the virtual io api.\n\n    If you ask for more samples from an ogg or flac file than are available\n    at that moment, libsndfile will give you no more samples ever, even if\n    more bytes arrive in the buffer later.\n    """"""\n    byte_depth = _lookup[sf.subtype]\n    channels = sf.channels\n    bytes_per_second = byte_depth * sf.samplerate * channels\n    secs = len(buf) / bytes_per_second\n    secs = max(1, secs - 6)\n    return int(secs * sf.samplerate)\n'"
zounds/soundfile/chunksize.py,0,"b'\nclass ChunkSizeBytes(object):\n    """"""\n    A convenience class to help describe a chunksize in bytes for the\n    :class:`featureflow.ByteStream` in terms of audio sample batch sizes.\n\n    Args:\n        samplerate (SampleRate): The samples-per-second factor\n        duration (numpy.timedelta64): The length of desired chunks in seconds\n        channels (int): Then audio channels factor\n        bit_depth (int): The bit depth factor\n\n    Examples:\n        >>> from zounds import ChunkSizeBytes, Seconds, SR44100\n        >>> chunksize = ChunkSizeBytes(SR44100(), Seconds(30))\n        >>> chunksize\n        ChunkSizeBytes(samplerate=SR44100(f=2.2675736e-05, d=2.2675736e-05)...\n        >>> int(chunksize)\n        5292000\n\n    """"""\n    def __init__(self, samplerate, duration, channels=2, bit_depth=16):\n        self.duration = duration\n        self.bit_depth = bit_depth\n        self.channels = channels\n        self.samplerate = samplerate\n\n    def __int__(self):\n        byte_depth = self.bit_depth // 8\n        total_samples = int(self.duration / self.samplerate.frequency)\n        return int(total_samples * byte_depth * self.channels)\n\n    def __repr__(self):\n        msg = \'ChunkSizeBytes(samplerate={samplerate}, duration={duration}, \' \\\n              \'channels={channels}, bit_depth={bit_depth})\'\n\n        return msg.format(\n            samplerate=self.samplerate,\n            duration=str(self.duration),\n            channels=self.channels,\n            bit_depth=self.bit_depth)\n'"
zounds/soundfile/functional.py,1,"b'from zounds.timeseries import AudioSamples\nfrom .resample import Resampler\nimport numpy as np\n\n\ndef resample(samples, new_sample_rate):\n    if new_sample_rate == samples.samplerate:\n        return samples\n    rs = Resampler(new_sample_rate)\n    new_samples = np.concatenate(list(rs._process(samples)))\n    return AudioSamples(new_samples, new_sample_rate)\n'"
zounds/soundfile/ogg_vorbis.py,0,"b'\nfrom featureflow import IdentityEncoder, Node, Decoder, Feature\nfrom .audiostream import MemoryBuffer\nfrom zounds.timeseries import audio_sample_rate, TimeSlice, AudioSamples\nfrom soundfile import *\nfrom .byte_depth import chunk_size_samples\nfrom zounds.timeseries import Picoseconds, Seconds\n\n\nclass OggVorbisWrapper(object):\n    def __init__(self, flo):\n        self._flo = flo\n        self._sf = SoundFile(self._flo)\n        self._freq = Picoseconds(int(1e12)) / self._sf.samplerate\n\n    @property\n    def samplerate(self):\n        return self._sf.samplerate\n\n    @property\n    def channels(self):\n        return self._sf.channels\n\n    @property\n    def duration_seconds(self):\n        return len(self._sf) / self.samplerate\n\n    def _n_samples(self, duration):\n        if duration is None:\n            return -1\n        return int(duration / self._freq)\n\n    def __getitem__(self, timeslice):\n        sr = audio_sample_rate(self.samplerate)\n\n        if timeslice == slice(None):\n            self._sf.seek(0)\n            return AudioSamples(self._sf.read(len(self._sf)), sr)\n\n        start_sample = int(timeslice.start / self._freq)\n        n_samples = self._n_samples(timeslice.duration)\n\n        self._sf.seek(start_sample)\n        return AudioSamples(self._sf.read(n_samples), sr)\n\n    def iter_chunks(self):\n        chunksize = Seconds(1)\n        ts = TimeSlice(chunksize)\n        sl = self[ts]\n        yield sl\n        while len(sl) >= self._n_samples(chunksize):\n            ts += chunksize\n            sl = self[ts]\n            yield sl\n\n\nclass OggVorbisEncoder(IdentityEncoder):\n    content_type = \'audio/ogg\'\n\n\nclass OggVorbisDecoder(Decoder):\n    def __init__(self):\n        super(OggVorbisDecoder, self).__init__()\n\n    def __call__(self, flo):\n        return OggVorbisWrapper(flo)\n\n    def __iter__(self, flo):\n        yield self(flo)\n\n\nclass OggVorbisFeature(Feature):\n    def __init__(\n            self,\n            extractor,\n            needs=None,\n            store=False,\n            key=None,\n            **extractor_args):\n        super(OggVorbisFeature, self).__init__( \\\n                extractor,\n                needs=needs,\n                store=store,\n                encoder=OggVorbisEncoder,\n                decoder=OggVorbisDecoder(),\n                key=key,\n                **extractor_args)\n\n\nclass OggVorbis(Node):\n    """"""\n    `OggVorbis` expects to process a stream of raw bytes (e.g. one produced by\n    :class:`featureflow.ByteStream`) and produces a new byte stream where the\n    original audio samples are `ogg-vorbis <https://xiph.org/vorbis/>`_ encoded\n\n    Args:\n        needs (Feature): a feature that produces a byte stream\n            (e.g. :class:`featureflow.Bytestream`)\n\n    Here\'s how you\'d typically see :class:`OggVorbis` used in a processing\n    graph.\n\n    .. code:: python\n\n        import featureflow as ff\n        import zounds\n\n\n        chunksize = zounds.ChunkSizeBytes(\n            samplerate=zounds.SR44100(),\n            duration=zounds.Seconds(30),\n            bit_depth=16,\n            channels=2)\n\n        @zounds.simple_in_memory_settings\n        class Document(ff.BaseModel):\n            meta = ff.JSONFeature(\n                zounds.MetaData,\n                store=True,\n                encoder=zounds.AudioMetaDataEncoder)\n\n            raw = ff.ByteStreamFeature(\n                ff.ByteStream,\n                chunksize=chunksize,\n                needs=meta,\n                store=False)\n\n            ogg = zounds.OggVorbisFeature(\n                zounds.OggVorbis,\n                needs=raw,\n                store=True)\n\n\n        synth = zounds.NoiseSynthesizer(zounds.SR11025())\n        samples = synth.synthesize(zounds.Seconds(10))\n        raw_bytes = samples.encode()\n        _id = Document.process(meta=raw_bytes)\n        doc = Document(_id)\n        # fetch and decode a section of audio\n        ts = zounds.TimeSlice(zounds.Seconds(2))\n        print doc.ogg[ts].shape  # 22050\n    """"""\n    def __init__(self, needs=None):\n        super(OggVorbis, self).__init__(needs=needs)\n        self._in_buf = None\n        self._in_sf = None\n        self._out_buf = None\n        self._out_sf = None\n        self._already_ogg = None\n        self._chunk_size_samples = None\n\n    def _enqueue(self, data, pusher):\n        if self._in_buf is None:\n            self._in_buf = MemoryBuffer(data.total_length)\n            self._in_buf.write(data)\n            self._in_sf = SoundFile(self._in_buf)\n            self._already_ogg = \'OGG\' in self._in_sf.format\n\n        if not self._chunk_size_samples:\n            self._chunk_size_samples = chunk_size_samples(self._in_sf, data)\n\n        if self._already_ogg:\n            super(OggVorbis, self)._enqueue(data, pusher)\n            return\n\n        if self._out_buf is None:\n            self._out_buf = MemoryBuffer(data.total_length)\n            self._out_sf = SoundFile(\n                    self._out_buf,\n                    format=\'OGG\',\n                    subtype=\'VORBIS\',\n                    mode=\'w\',\n                    samplerate=self._in_sf.samplerate,\n                    channels=self._in_sf.channels)\n        else:\n            self._in_buf.write(data)\n\n    def _dequeue(self):\n\n        if self._already_ogg:\n            return super(OggVorbis, self)._dequeue()\n\n        samples = self._in_sf.read(self._chunk_size_samples)\n        factor = 20\n        while samples.size:\n            # KLUDGE: Trying to write too-large chunks to an ogg file seems to\n            # cause a segfault in libsndfile\n            for i in range(0, len(samples), self._in_sf.samplerate * factor):\n                self._out_sf.write(\n                        samples[i: i + self._in_sf.samplerate * factor])\n            samples = self._in_sf.read(self._chunk_size_samples)\n        return self._out_buf\n\n    def _process_other(self, data):\n        if self._finalized:\n            self._out_sf.close()\n        o = data.read(count=-1)\n        return o\n\n    def _process_ogg(self, data):\n        return data\n\n    def _process(self, data):\n        if self._already_ogg:\n            yield self._process_ogg(data)\n        else:\n            yield self._process_other(data)\n'"
zounds/soundfile/resample.py,4,"b'\n\nfrom ctypes import *\n\nimport numpy as np\n\nfrom zounds.timeseries import SR44100, AudioSamples, Seconds\nfrom zounds.core import ArrayWithUnits\n\ntry:\n    libsamplerate = CDLL(\'libsamplerate.so\')\nexcept OSError as e:\n    # KLUDGE: This is here to support building documentation on readthedocs\n    pass\n\nfrom featureflow import Node\n\n\nclass SRC_DATA(Structure):\n    """"""\n    A wrapper for the libsamplerate.SRC_DATA struct\n    """"""\n    _fields_ = [(\'data_in\', POINTER(c_float)),\n                (\'data_out\', POINTER(c_float)),\n                (\'input_frames\', c_long),\n                (\'output_frames\', c_long),\n                (\'input_frames_used\', c_long),\n                (\'output_frames_gen\', c_long),\n                (\'end_of_input\', c_int),\n                (\'src_ratio\', c_double), ]\n\n\nclass SRC_STATE(Structure):\n    """"""\n    A dummy structure to represent the state returned from libsamplerate\n    src_new.\n    """"""\n    _fields_ = []\n\n\nclass Resample(object):\n    """"""\n    A wrapper around the libsamplerate src_process() method.  This class is\n    intended for one-time use. New instances should be created for each sound\\\n    file processed.\n    """"""\n\n    def __init__(\n            self,\n            orig_sample_rate,\n            new_sample_rate,\n            nchannels=1,\n            converter_type=1):\n\n        """"""\n        orig_sample_rate - The sample rate of the incoming samples, in hz\n        new_sample_rate - The sample_rate of the outgoiing samples, in hz\n        n_channels - Number of channels in the incoming and outgoing samples\n        converter_type - See http://www.mega-nerd.com/SRC/api_misc.html#Converters\n                         for a list of conversion types. ""0"" is the best-quality,\n                         and slowest converter\n\n        """"""\n        super(Resample, self).__init__()\n        self._ratio = new_sample_rate / orig_sample_rate\n        # check if the conversion ratio is considered valid by libsamplerate\n        if not libsamplerate.src_is_valid_ratio(c_double(self._ratio)):\n            raise ValueError(\'%1.2f / %1.2f = %1.4f is not a valid ratio\' % \\\n                             (new_sample_rate, orig_sample_rate, self._ratio))\n        # create a pointer to the SRC_STATE struct, which maintains state\n        # between calls to src_process()\n        self.error = pointer(c_int(0))\n        self.nchannels = nchannels\n        self.converter_type = converter_type\n        self.c_int_converter_type = c_int(converter_type)\n        self.c_int_channels = c_int(self.nchannels)\n        libsamplerate.src_new.restype = POINTER(SRC_STATE)\n        self._state = libsamplerate.src_new(\n            self.c_int_converter_type, self.c_int_channels, self.error)\n\n    def _prepare_input(self, insamples):\n        # ensure that the input is float data\n        if np.float32 != insamples.dtype:\n            return insamples.astype(np.float32)\n        return insamples\n\n    def _output_buffer(self, insamples):\n        outsize = (int(np.round(len(insamples) * self._ratio)), self.nchannels)\n        return np.zeros(outsize, dtype=np.float32).squeeze()\n\n    def _check_for_error(self, return_code):\n        if return_code:\n            raise Exception(\n                \'libsamplerate sent non-zero return code {return_code}\'\n                    .format(**locals()))\n\n    def __call__(self, insamples, end_of_input=False):\n\n        normalized_insamples = self._prepare_input(insamples)\n        outsamples = self._output_buffer(normalized_insamples)\n\n        insamples_ptr = normalized_insamples.ctypes.data_as(POINTER(c_float))\n        outsamples_ptr = outsamples.ctypes.data_as(POINTER(c_float))\n\n        sd = SRC_DATA(\n            # a pointer to the input samples\n            data_in=insamples_ptr,\n            # a pointer to the output buffer\n            data_out=outsamples_ptr,\n            # number of input samples\n            input_frames=len(normalized_insamples),\n            # number of output samples\n            output_frames=len(outsamples),\n            # NOT the end of input, i.e., there is more data to process\n            end_of_input=int(end_of_input),\n            # the conversion ratio\n            src_ratio=self._ratio)\n        sd_ptr = pointer(sd)\n        rv = libsamplerate.src_process(self._state, sd_ptr)\n        self._check_for_error(rv)\n        return outsamples\n\n\nclass Resampler(Node):\n    """"""\n    `Resampler` expects to process :class:`~zounds.timeseries.AudioSamples`\n    instances (e.g., those produced by a :class:`AudioStream` node), and will\n    produce a new stream of :class:`AudioSamples` at a new sampling rate.\n\n    Args:\n        samplerate (AudioSampleRate): the desired sampling rate.  If none is\n            provided, the default is :class:`~zounds.timeseries.SR44100`\n        needs (Feature): a processing node that produces\n            :class:`~zounds.timeseries.AudioSamples`\n\n\n    Here\'s how you\'d typically see :class:`Resampler` used in a processing\n    graph.\n\n    .. code:: python\n\n        import featureflow as ff\n        import zounds\n\n        chunksize = zounds.ChunkSizeBytes(\n            samplerate=zounds.SR44100(),\n            duration=zounds.Seconds(30),\n            bit_depth=16,\n            channels=2)\n\n        @zounds.simple_in_memory_settings\n        class Document(ff.BaseModel):\n            meta = ff.JSONFeature(\n                zounds.MetaData,\n                store=True,\n                encoder=zounds.AudioMetaDataEncoder)\n\n            raw = ff.ByteStreamFeature(\n                ff.ByteStream,\n                chunksize=chunksize,\n                needs=meta,\n                store=False)\n\n            pcm = zounds.AudioSamplesFeature(\n                zounds.AudioStream,\n                needs=raw,\n                store=True)\n\n            resampled = zounds.AudioSamplesFeature(\n                zounds.Resampler,\n                samplerate=zounds.SR22050(),\n                needs=pcm,\n                store=True)\n\n\n        synth = zounds.NoiseSynthesizer(zounds.SR11025())\n        samples = synth.synthesize(zounds.Seconds(10))\n        raw_bytes = samples.encode()\n        _id = Document.process(meta=raw_bytes)\n        doc = Document(_id)\n        print doc.pcm.samplerate.__class__.__name__  # SR11025\n        print doc.resampled.samplerate.__class__.__name__  # SR22050\n    """"""\n\n    def __init__(self, samplerate=None, needs=None):\n        super(Resampler, self).__init__(needs=needs)\n        self._samplerate = samplerate or SR44100()\n        self._resample = None\n\n    def _noop(self, data, finalized):\n        return data\n\n    def _process(self, data):\n        sr = data.samples_per_second\n\n        if self._resample is None:\n            target_sr = self._samplerate.samples_per_second\n            self._resample = Resample(\n                sr,\n                target_sr,\n                1 if len(data.shape) == 1 else data.shape[1])\n\n            if target_sr != sr:\n                self._rs = self._resample\n                # KLUDGE: The following line seems to solve a bug whereby \n                # libsamplerate doesn\'t generate enough samples the first time\n                # src_process is called. We\'re calling it once here, so the ""real""\n                # output will come out click-free\n                silence = AudioSamples.silence(\n                    self._samplerate, Seconds(1), channels=data.channels)\n                self._resample(silence)\n            else:\n                self._rs = self._noop\n\n        resampled = self._rs(data, self._finalized)\n        if not isinstance(resampled, ArrayWithUnits):\n            resampled = AudioSamples(resampled, self._samplerate)\n        yield resampled\n'"
zounds/soundfile/test_audiometadata.py,0,"b""import tempfile\n\nimport requests\nimport unittest2\n\nfrom .audio_metadata import AudioMetaData, MetaData\nfrom zounds.synthesize import NoiseSynthesizer\nfrom zounds.timeseries import SR44100, Seconds\n\n\ndef soundfile(flo=None):\n    synth = NoiseSynthesizer(SR44100())\n    samples = synth.synthesize(Seconds(5)).stereo\n    flo = samples.encode(flo=flo)\n    return samples, flo\n\n\nclass AudioMetaDataTests(unittest2.TestCase):\n\n    def test_can_handle_local_file_path(self):\n        with tempfile.NamedTemporaryFile(mode='wb+') as tf:\n            signal, f = soundfile(flo=tf)\n            result = next(MetaData()._process(f.name))\n            self.assertEqual(44100, result.samplerate)\n            self.assertEqual(2, result.channels)\n            self.assertIs(f.name, result.uri)\n\n    def test_can_handle_file_like_object(self):\n        samples, bio = soundfile()\n        result = next(MetaData()._process(bio))\n        self.assertEqual(44100, result.samplerate)\n        self.assertEqual(2, result.channels)\n        self.assertIs(bio, result.uri)\n\n    def test_can_handle_url_string(self):\n        url = 'http://host.com/path'\n        result = next(MetaData()._process(url))\n        self.assertIsInstance(result.uri, requests.Request)\n        self.assertEqual(url, result.uri.url)\n\n    def test_can_handle_request(self):\n        req = requests.Request(method='GET', url='http://host.com/path')\n        result = next(MetaData()._process(req))\n        self.assertEqual(req, result.uri)\n\n    def test_can_handle_audio_metadata(self):\n        meta = AudioMetaData(uri='something')\n        result = next(MetaData()._process(meta))\n        self.assertEqual(meta, result)\n"""
zounds/soundfile/test_chunksize.py,0,"b""import unittest2\nfrom .chunksize import ChunkSizeBytes\nfrom zounds.timeseries import SR44100, Seconds\n\n\nclass ChunkSizeBytesTests(unittest2.TestCase):\n    def test_can_convert_to_integer_number_of_bytes(self):\n        cs = ChunkSizeBytes(SR44100(), Seconds(30), channels=2, bit_depth=16)\n        self.assertEqual(5292000, int(cs))\n\n    def test_can_repr(self):\n        cs = ChunkSizeBytes(SR44100(), Seconds(30), channels=2, bit_depth=16)\n        s = cs.__repr__()\n        self.assertEqual(\n            'ChunkSizeBytes(samplerate=SR44100(f=2.2675736e-05, '\n            'd=2.2675736e-05), duration=30 seconds, channels=2, bit_depth=16)',\n            s)\n"""
zounds/soundfile/test_functional.py,0,"b'import unittest2\nfrom .functional import resample\nfrom zounds import AudioSamples\nfrom zounds import SR11025\nfrom zounds import SR44100\nfrom zounds import Seconds\nfrom zounds import SineSynthesizer, SilenceSynthesizer\n\n\nclass ResampleTests(unittest2.TestCase):\n    def test_can_resample_audio(self):\n        samplerate = SR44100()\n        synth = SineSynthesizer(samplerate)\n        samples = synth.synthesize(Seconds(1), [440, 880, 1760])\n        encoded = samples.encode()\n        new_samples = AudioSamples.from_file(encoded)\n        resampled = resample(new_samples, SR11025())\n        self.assertIsInstance(resampled, AudioSamples)\n        self.assertEqual(int(SR11025()), len(resampled))\n\n    def test_can_resample_stereo(self):\n        samplerate = SR44100()\n        synth = SineSynthesizer(samplerate)\n        samples = synth.synthesize(Seconds(1), [440, 880, 1760])\n        encoded = samples.stereo.encode()\n        new_samples = AudioSamples.from_file(encoded)\n        resampled = resample(new_samples, SR11025())\n        self.assertIsInstance(resampled, AudioSamples)\n        self.assertEqual(int(SR11025()), len(resampled))\n\n    def test_resample_does_not_introduce_pops(self):\n        samplerate = SR44100()\n        synth = SilenceSynthesizer(samplerate)\n        samples = synth.synthesize(Seconds(1))\n        resampled = resample(samples, SR11025())\n        self.assertEqual(0, resampled.max())'"
zounds/soundfile/test_ogg_vorbis.py,0,"b""import unittest2\nfrom .ogg_vorbis import OggVorbisWrapper\nfrom zounds.timeseries import TimeSlice, Seconds, SR11025\nfrom zounds.synthesize import SineSynthesizer\n\n\nclass TestOggVorbisWrapper(unittest2.TestCase):\n    def test_can_apply_empty_time_slice_to_wrapper(self):\n        synth = SineSynthesizer(SR11025())\n        samples = synth.synthesize(Seconds(10))\n        encoded = samples.encode(fmt='OGG', subtype='VORBIS')\n        wrapper = OggVorbisWrapper(encoded)\n        samples = wrapper[TimeSlice()]\n        expected = Seconds(10) / Seconds(1)\n        actual = samples.end / Seconds(1)\n        self.assertAlmostEqual(expected, actual, places=6)\n\n    def test_can_apply_open_ended_slice_to_wrapper(self):\n        synth = SineSynthesizer(SR11025())\n        samples = synth.synthesize(Seconds(10))\n        encoded = samples.encode(fmt='OGG', subtype='VORBIS')\n        wrapper = OggVorbisWrapper(encoded)\n        samples = wrapper[TimeSlice(start=Seconds(1))]\n        expected = Seconds(9) / Seconds(1)\n        actual = samples.end / Seconds(1)\n        self.assertAlmostEqual(expected, actual, places=6)\n"""
zounds/soundfile/test_resample.py,0,"b'from .resample import Resample\nfrom zounds.timeseries import SR44100, SR11025, Seconds\nfrom zounds.synthesize import SilenceSynthesizer\nfrom multiprocessing.pool import ThreadPool\nimport unittest2\n\n\nclass ResampleTests(unittest2.TestCase):\n\n    def test_can_do_multithreaded_resampling(self):\n        synth = SilenceSynthesizer(SR44100())\n        audio = [synth.synthesize(Seconds(5)) for _ in range(10)]\n        pool = ThreadPool(4)\n\n        def x(samples):\n            rs = Resample(int(SR44100()), int(SR11025()))\n            return rs(samples, end_of_input=True)\n\n        resampled = pool.map(x, audio)\n        self.assertEqual(10, len(resampled))\n\n    def test_correct_output_with_stereo(self):\n        synth = SilenceSynthesizer(SR44100())\n        samples = synth.synthesize(Seconds(1)).stereo\n        rs = Resample(int(samples.samplerate), int(SR11025()), nchannels=2)\n        resampled = rs(samples, end_of_input=True)\n        self.assertEqual((11025, 2), resampled.shape)'"
zounds/spectral/__init__.py,0,"b'""""""\nThe spectral module contains classes that aid in dealing with frequency-domain\nrepresentations of sound\n""""""\n\nfrom .sliding_window import \\\n    SlidingWindow, OggVorbisWindowingFunc, WindowingFunc, \\\n    HanningWindowingFunc, IdentityWindowingFunc\n\nfrom .spectral import \\\n    FFT, DCT, DCTIV, MDCT, BarkBands, Chroma, BFCC, SpectralCentroid, \\\n    SpectralFlatness, FrequencyAdaptiveTransform, FrequencyWeighting\n\nfrom .tfrepresentation import FrequencyDimension, ExplicitFrequencyDimension\n\nfrom .weighting import AWeighting\n\nfrom .frequencyscale import \\\n    LinearScale, FrequencyBand, FrequencyScale, GeometricScale, ExplicitScale, \\\n    Hertz, Hz, BarkScale, MelScale, ChromaScale\n\nfrom .frequencyadaptive import FrequencyAdaptive\n\nfrom .functional import \\\n    fft, stft, apply_scale, frequency_decomposition, phase_shift, rainbowgram, \\\n    dct_basis, fir_filter_bank, time_stretch, pitch_shift, morlet_filter_bank\n'"
zounds/spectral/frequencyadaptive.py,8,"b'\n\nimport numpy as np\nfrom scipy.signal import resample\n\nfrom .tfrepresentation import ExplicitFrequencyDimension, FrequencyDimension\nfrom zounds.core import ArrayWithUnits\nfrom zounds.timeseries import ConstantRateTimeSeries\nfrom zounds.timeseries import Picoseconds, TimeDimension\n\n\nclass FrequencyAdaptive(ArrayWithUnits):\n    """"""\n    TODO: This needs some love. Mutually exclusive constructor arguments are no\n    bueno\n\n    Args:\n        arrs: TODO\n        time_dimension (TimeDimension): the time dimension of the first axis of\n            this array\n        scale (FrequencyScale): The frequency scale corresponding to the first\n            axis of this array, mutually exclusive with the\n            :code:`explicit_freq_dimension` argument\n        explicit_freq_dimension (ExplicitFrequencyDimension): TODO\n\n    See Also:\n        :class:`~zounds.spectral.FrequencyAdaptiveTransform`\n    """"""\n    def __new__(\n            cls,\n            arrs,\n            time_dimension=None,\n            scale=None,\n            explicit_freq_dimension=None):\n\n        if not time_dimension:\n            raise ValueError(\'time_dimension is required\')\n\n        if explicit_freq_dimension:\n            if scale:\n                raise ValueError(\n                    \'scale must be None when explicit_freq_dimension is supplied\')\n            if not isinstance(arrs, np.ndarray):\n                raise ValueError(\n                    \'arrs must be a contiguous array when explicit_freq_dimension_is_supplied\')\n            return ArrayWithUnits.__new__(\n                cls, arrs, [time_dimension, explicit_freq_dimension])\n\n        stops = list(np.cumsum([arr.shape[1] for arr in arrs]))\n        slices = [slice(start, stop)\n                  for (start, stop) in zip([0] + stops, stops)]\n        dimensions = [time_dimension, ExplicitFrequencyDimension(scale, slices)]\n\n        array = np.concatenate(arrs, axis=1)\n        return ArrayWithUnits.__new__(cls, array, dimensions)\n\n    def kwargs(self):\n        return dict(\n            time_dimension=self.time_dimension,\n            explicit_freq_dimension=self.frequency_dimension)\n\n    @property\n    def scale(self):\n        return self.frequency_dimension.scale\n\n    @property\n    def time_dimension(self):\n        return self.dimensions[0]\n\n    @property\n    def frequency_dimension(self):\n        return self.dimensions[1]\n\n    @property\n    def n_bands(self):\n        return len(self.scale)\n\n    def rasterize(self, n_coeffs):\n        return self.square(n_coeffs)\n\n    def _resample(self, band, n_coeffs, epsilon=1e-8):\n        rs = resample(band, n_coeffs, axis=1)\n\n        # resample doesn\'t necessarily maintain the correct scale/magnitude, as\n        # it isn\'t using the norm=""ortho"" argument when calling fft, so ensure\n        # that the original scale/magnitude is maintained after calling\n        # resample\n        band_max = band.max(axis=-1, keepdims=True)\n        rs_max = rs.max(axis=-1, keepdims=True)\n        ratio = rs_max / (band_max + epsilon)\n        normalized = rs / (ratio + epsilon)\n        return np.asarray(normalized.flatten())\n\n    def square(self, n_coeffs, do_overlap_add=False):\n        """"""\n        Compute a ""square"" view of the frequency adaptive transform, by\n        resampling each frequency band such that they all contain the same\n        number of samples, and performing an overlap-add procedure in the\n        case where the sample frequency and duration differ\n        :param n_coeffs: The common size to which each frequency band should\n        be resampled\n        """"""\n        resampled_bands = [\n            self._resample(band, n_coeffs)\n            for band in self.iter_bands()]\n\n        stacked = np.vstack(resampled_bands).T\n\n        fdim = FrequencyDimension(self.scale)\n\n        # TODO: This feels like it could be wrapped up nicely elsewhere\n        chunk_frequency = Picoseconds(int(np.round(\n            self.time_dimension.duration / Picoseconds(1) / n_coeffs)))\n\n        td = TimeDimension(frequency=chunk_frequency)\n\n        arr = ConstantRateTimeSeries(ArrayWithUnits(\n            stacked.reshape(-1, n_coeffs, self.n_bands),\n            dimensions=[self.time_dimension, td, fdim]))\n\n        if not do_overlap_add:\n            return arr\n\n        # Begin the overlap add procedure\n        overlap_ratio = self.time_dimension.overlap_ratio\n\n        if overlap_ratio == 0:\n            # no overlap add is necessary\n            return ArrayWithUnits(stacked, [td, fdim])\n\n        step_size_samples = int(n_coeffs * overlap_ratio)\n\n        first_dim = int(np.round(\n            (stacked.shape[0] * overlap_ratio) + (n_coeffs * overlap_ratio)))\n\n        output = ArrayWithUnits(\n            np.zeros((first_dim, self.n_bands)),\n            dimensions=[td, fdim])\n\n        for i, chunk in enumerate(arr):\n            start = step_size_samples * i\n            stop = start + n_coeffs\n            output[start: stop] += chunk.reshape((-1, self.n_bands))\n\n        return output\n\n    def iter_bands(self):\n        return (self[:, band] for band in self.scale)\n\n    def like_dims(self, arr):\n        return self.__class__(\n            arr,\n            time_dimension=self.time_dimension,\n            explicit_freq_dimension=self.frequency_dimension)\n\n    @classmethod\n    def from_array_with_units(cls, arr):\n        fdim = arr.dimensions[1]\n        arrs = [arr[:, band] for band in fdim.scale]\n        fa = FrequencyAdaptive(arrs, arr.dimensions[0], fdim.scale)\n        return fa\n'"
zounds/spectral/frequencyscale.py,23,"b'\nimport numpy as np\nimport bisect\n\n\nclass Hertz(float):\n    def __init__(self, hz):\n        try:\n            self.hz = hz.hz\n        except AttributeError:\n            self.hz = hz\n\n    def __neg__(self):\n        return Hertz(-self.hz)\n\n    def __add__(self, other):\n        try:\n            other = other.hz\n        except AttributeError:\n            pass\n        return Hertz(self.hz + other)\n\n    def __float__(self):\n        return self.hz\n\n\nHz = Hertz\n\n\n# TODO: What commonalities can be factored out of this class and TimeSlice?\nclass FrequencyBand(object):\n    """"""\n    Represents an interval, or band of frequencies in hertz (cycles per second)\n\n    Args:\n        start_hz (float): The lower bound of the frequency band in hertz\n        stop_hz (float): The upper bound of the frequency band in hertz\n\n    Examples::\n        >>> import zounds\n        >>> band = zounds.FrequencyBand(500, 1000)\n        >>> band.center_frequency\n        750.0\n        >>> band.bandwidth\n        500\n    """"""\n\n    def __init__(self, start_hz, stop_hz):\n        super(FrequencyBand, self).__init__()\n        if stop_hz <= start_hz:\n            raise ValueError(\'stop_hz must be greater than start_hz\')\n        self.stop_hz = stop_hz\n        self.start_hz = start_hz\n\n    def __eq__(self, other):\n        try:\n            return \\\n                self.start_hz == other.start_hz \\\n                and self.stop_hz == other.stop_hz\n        except AttributeError:\n            return super(FrequencyBand, self).__eq__(other)\n\n    def __hash__(self):\n        return (self.__class__.__name__, self.start_hz, self.stop_hz).__hash__()\n\n    def intersect(self, other):\n        """"""\n        Return the intersection between this frequency band and another.\n\n        Args:\n            other (FrequencyBand): the instance to intersect with\n\n        Examples::\n            >>> import zounds\n            >>> b1 = zounds.FrequencyBand(500, 1000)\n            >>> b2 = zounds.FrequencyBand(900, 2000)\n            >>> intersection = b1.intersect(b2)\n            >>> intersection.start_hz, intersection.stop_hz\n            (900, 1000)\n        """"""\n        lowest_stop = min(self.stop_hz, other.stop_hz)\n        highest_start = max(self.start_hz, other.start_hz)\n        return FrequencyBand(highest_start, lowest_stop)\n\n    @classmethod\n    def audible_range(cls, samplerate):\n        return FrequencyBand(Hz(20), Hz(samplerate.nyquist))\n\n    def bandwidth_ratio(self, other):\n        return other.bandwidth / self.bandwidth\n\n    def intersection_ratio(self, other):\n        intersection = self.intersect(other)\n        return self.bandwidth_ratio(intersection)\n\n    @staticmethod\n    def from_start(start_hz, bandwidth_hz):\n        """"""\n        Produce a :class:`FrequencyBand` instance from a lower bound and\n        bandwidth\n\n        Args:\n            start_hz (float): the lower bound of the desired FrequencyBand\n            bandwidth_hz (float): the bandwidth of the desired FrequencyBand\n\n        """"""\n        return FrequencyBand(start_hz, start_hz + bandwidth_hz)\n\n    @staticmethod\n    def from_center(center_hz, bandwidth_hz):\n        half_bandwidth = bandwidth_hz / 2\n        return FrequencyBand(\n            center_hz - half_bandwidth, center_hz + half_bandwidth)\n\n    @property\n    def bandwidth(self):\n        """"""\n        The span of this frequency band, in hertz\n        """"""\n        return self.stop_hz - self.start_hz\n\n    @property\n    def center_frequency(self):\n        return self.start_hz + (self.bandwidth / 2)\n\n    def __repr__(self):\n        return \'\'\'FrequencyBand(\nstart_hz={start_hz},\nstop_hz={stop_hz},\ncenter={center},\nbandwidth={bandwidth})\'\'\'.format(\n            start_hz=self.start_hz,\n            stop_hz=self.stop_hz,\n            center=self.center_frequency,\n            bandwidth=self.bandwidth)\n\n\nclass FrequencyScale(object):\n    """"""\n    Represents a set of frequency bands with monotonically increasing start\n    frequencies\n\n    Args:\n        frequency_band (FrequencyBand): A band representing the entire span of\n            this scale.  E.g., one might want to generate a scale spanning the\n            entire range of human hearing by starting with\n            :code:`FrequencyBand(20, 20000)`\n        n_bands (int): The number of bands in this scale\n        always_even (bool): when converting frequency slices to integer indices\n            that numpy can understand, should the slice size always be even?\n\n    See Also:\n        :class:`~zounds.spectral.LinearScale`\n        :class:`~zounds.spectral.GeometricScale`\n    """"""\n\n    def __init__(self, frequency_band, n_bands, always_even=False):\n        super(FrequencyScale, self).__init__()\n        self.always_even = always_even\n        self.n_bands = n_bands\n        self.frequency_band = frequency_band\n        self._bands = None\n        self._starts = None\n        self._stops = None\n\n    @property\n    def bands(self):\n        """"""\n        An iterable of all bands in this scale\n        """"""\n        if self._bands is None:\n            self._bands = self._compute_bands()\n        return self._bands\n\n    @property\n    def band_starts(self):\n        if self._starts is None:\n            self._starts = [b.start_hz for b in self.bands]\n        return self._starts\n\n    @property\n    def band_stops(self):\n        if self._stops is None:\n            self._stops = [b.stop_hz for b in self.bands]\n        return self._stops\n\n    def _compute_bands(self):\n        raise NotImplementedError()\n\n    def __len__(self):\n        return self.n_bands\n\n    @property\n    def center_frequencies(self):\n        """"""\n        An iterable of the center frequencies of each band in this scale\n        """"""\n        return (band.center_frequency for band in self)\n\n    @property\n    def bandwidths(self):\n        """"""\n        An iterable of the bandwidths of each band in this scale\n        """"""\n        return (band.bandwidth for band in self)\n\n    def ensure_overlap_ratio(self, required_ratio=0.5):\n        """"""\n        Ensure that every adjacent pair of frequency bands meets the overlap\n        ratio criteria.  This can be helpful in scenarios where a scale is\n        being used in an invertible transform, and something like the `constant\n        overlap add constraint\n        <https://ccrma.stanford.edu/~jos/sasp/Constant_Overlap_Add_COLA_Cases.html>`_\n        must be met in order to not introduce artifacts in the reconstruction.\n\n        Args:\n            required_ratio (float): The required overlap ratio between all\n                adjacent frequency band pairs\n\n        Raises:\n            AssertionError: when the overlap ratio for one or more adjacent\n                frequency band pairs is not met\n        """"""\n\n        msg = \\\n            \'band {i}: ratio must be at least {required_ratio} but was {ratio}\'\n\n        for i in range(0, len(self) - 1):\n            b1 = self[i]\n            b2 = self[i + 1]\n\n            try:\n                ratio = b1.intersection_ratio(b2)\n            except ValueError:\n                ratio = 0\n\n            if ratio < required_ratio:\n                raise AssertionError(msg.format(**locals()))\n\n    @property\n    def Q(self):\n        """"""\n        The quality factor of the scale, or, the ratio of center frequencies\n        to bandwidths\n        """"""\n        return np.array(list(self.center_frequencies)) \\\n               / np.array(list(self.bandwidths))\n\n    @property\n    def start_hz(self):\n        """"""\n        The lower bound of this frequency scale\n        """"""\n        return self.frequency_band.start_hz\n\n    @property\n    def stop_hz(self):\n        """"""\n        The upper bound of this frequency scale\n        """"""\n        return self.frequency_band.stop_hz\n\n    def _basis(self, other_scale, window):\n        weights = np.zeros((len(self), len(other_scale)))\n        for i, band in enumerate(self):\n            band_slice = other_scale.get_slice(band)\n            slce = weights[i, band_slice]\n            slce[:] = window * np.ones(len(slce))\n        return weights\n\n    def apply(self, time_frequency_repr, window):\n        basis = self._basis(time_frequency_repr.dimensions[-1].scale, window)\n        transformed = np.dot(basis, time_frequency_repr.T).T\n        return transformed\n\n    def __eq__(self, other):\n        return \\\n            self.__class__ == other.__class__ \\\n            and self.frequency_band == other.frequency_band \\\n            and self.n_bands == other.n_bands\n\n    def __iter__(self):\n        return iter(self.bands)\n\n    def _construct_scale_from_slice(self, bands):\n        freq_band = FrequencyBand(bands[0].start_hz, bands[-1].stop_hz)\n        return self.__class__(freq_band, len(bands))\n\n    def get_slice(self, frequency_band):\n        """"""\n        Given a frequency band, and a frequency dimension comprised of\n        n_samples, return a slice using integer indices that may be used to\n        extract only the frequency samples that intersect with the frequency\n        band\n        """"""\n        index = frequency_band\n\n        if isinstance(index, slice):\n            types = {\n                index.start.__class__,\n                index.stop.__class__,\n                index.step.__class__\n            }\n\n            if Hertz not in types:\n                return index\n\n            try:\n                start = Hertz(0) if index.start is None else index.start\n                if start < Hertz(0):\n                    start = self.stop_hz + start\n                stop = self.stop_hz if index.stop is None else index.stop\n                if stop < Hertz(0):\n                    stop = self.stop_hz + stop\n                frequency_band = FrequencyBand(start, stop)\n            except (ValueError, TypeError):\n                pass\n\n        start_index = bisect.bisect_left(\n            self.band_stops, frequency_band.start_hz)\n        stop_index = bisect.bisect_left(\n            self.band_starts, frequency_band.stop_hz)\n\n        if self.always_even and (stop_index - start_index) % 2:\n            # KLUDGE: This is simple, but it may make sense to choose move the\n            # upper *or* lower bound, based on which one introduces a lower\n            # error\n            stop_index += 1\n        return slice(start_index, stop_index)\n\n    def __getitem__(self, index):\n\n        try:\n            # index is an integer or slice\n            bands = self.bands[index]\n        except TypeError:\n            # index is a frequency band\n            bands = self.bands[self.get_slice(index)]\n\n        if isinstance(bands, FrequencyBand):\n            return bands\n\n        return self._construct_scale_from_slice(bands)\n\n    def __str__(self):\n        cls = self.__class__.__name__\n        return \'{cls}(band={self.frequency_band}, n_bands={self.n_bands})\' \\\n            .format(**locals())\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass LinearScale(FrequencyScale):\n    """"""\n    A linear frequency scale with constant bandwidth.  Appropriate for use\n    with transforms whose coefficients also lie on a linear frequency scale,\n    e.g. the FFT or DCT transforms.\n\n    Args:\n        frequency_band (FrequencyBand): A band representing the entire span of\n            this scale.  E.g., one might want to generate a scale spanning the\n            entire range of human hearing by starting with\n            :code:`FrequencyBand(20, 20000)`\n        n_bands (int): The number of bands in this scale\n        always_even (bool): when converting frequency slices to integer indices\n            that numpy can understand, should the slice size always be even?\n\n    Examples:\n        >>> from zounds import FrequencyBand, LinearScale\n        >>> scale = LinearScale(FrequencyBand(20, 20000), 10)\n        >>> scale\n        LinearScale(band=FrequencyBand(\n        start_hz=20,\n        stop_hz=20000,\n        center=10010.0,\n        bandwidth=19980), n_bands=10)\n        >>> scale.Q\n        array([ 0.51001001,  1.51001001,  2.51001001,  3.51001001,  4.51001001,\n                5.51001001,  6.51001001,  7.51001001,  8.51001001,  9.51001001])\n    """"""\n\n    def __init__(self, frequency_band, n_bands, always_even=False):\n        super(LinearScale, self).__init__(frequency_band, n_bands, always_even)\n\n    @staticmethod\n    def from_sample_rate(sample_rate, n_bands, always_even=False):\n        """"""\n        Return a :class:`~zounds.spectral.LinearScale` instance whose upper\n        frequency bound is informed by the nyquist frequency of the sample rate.\n\n        Args:\n            sample_rate (SamplingRate): the sample rate whose nyquist frequency\n                will serve as the upper frequency bound of this scale\n            n_bands (int): the number of evenly-spaced frequency bands\n        """"""\n        fb = FrequencyBand(0, sample_rate.nyquist)\n        return LinearScale(fb, n_bands, always_even=always_even)\n\n    def _compute_bands(self):\n        freqs = np.linspace(\n            self.start_hz, self.stop_hz, self.n_bands, endpoint=False)\n        # constant, non-overlapping bandwidth\n        bandwidth = freqs[1] - freqs[0]\n        return tuple(FrequencyBand(f, f + bandwidth) for f in freqs)\n\n\n# class LogScale(FrequencyScale):\n#     def __init__(self, frequency_band, n_bands, always_even=False):\n#         super(LogScale, self).__init__(\n#             frequency_band, n_bands, always_even=always_even)\n#\n#     def _compute_bands(self):\n#         center_freqs = np.logspace(\n#             np.log10(self.start_hz),\n#             np.log10(self.stop_hz),\n#             self.n_bands + 1)\n#         # variable bandwidth\n#         bandwidths = np.diff(center_freqs)\n#         return tuple(FrequencyBand.from_center(cf, bw)\n#                      for (cf, bw) in zip(center_freqs[:-1], bandwidths))\n\n\nclass GeometricScale(FrequencyScale):\n    """"""\n    A constant-Q scale whose center frequencies progress geometrically rather\n    than linearly\n\n    Args:\n        start_center_hz (int): the center frequency of the first band in the\n            scale\n        stop_center_hz (int): the center frequency of the last band in the scale\n        bandwidth_ratio (float): the center frequency to bandwidth ratio\n        n_bands (int): the total number of bands\n\n    Examples:\n        >>> from zounds import GeometricScale\n        >>> scale = GeometricScale(20, 20000, 0.05, 10)\n        >>> scale\n        GeometricScale(band=FrequencyBand(\n        start_hz=19.5,\n        stop_hz=20500.0,\n        center=10259.75,\n        bandwidth=20480.5), n_bands=10)\n        >>> scale.Q\n        array([ 20.,  20.,  20.,  20.,  20.,  20.,  20.,  20.,  20.,  20.])\n        >>> list(scale.center_frequencies)\n        [20.000000000000004, 43.088693800637671, 92.831776672255558,\n            200.00000000000003, 430.88693800637651, 928.31776672255558,\n            2000.0000000000005, 4308.8693800637648, 9283.1776672255564,\n            20000.000000000004]\n    """"""\n\n    def __init__(\n            self,\n            start_center_hz,\n            stop_center_hz,\n            bandwidth_ratio,\n            n_bands,\n            always_even=False):\n        self.__bands = [\n            FrequencyBand.from_center(cf, cf * bandwidth_ratio)\n            for cf in np.geomspace(start_center_hz, stop_center_hz, num=n_bands)\n            ]\n        band = FrequencyBand(self.__bands[0].start_hz, self.__bands[-1].stop_hz)\n        super(GeometricScale, self).__init__(\n            band, n_bands, always_even=always_even)\n        self.start_center_hz = start_center_hz\n        self.stop_center_hz = stop_center_hz\n        self.bandwidth_ratio = bandwidth_ratio\n\n    def _construct_scale_from_slice(self, bands):\n        return ExplicitScale(bands)\n\n    def __eq__(self, other):\n        return \\\n            super(GeometricScale, self).__eq__(other) \\\n            and self.start_center_hz == other.start_center_hz \\\n            and self.stop_center_hz == other.stop_center_hz \\\n            and self.bandwidth_ratio == other.bandwidth_ratio\n\n    def _compute_bands(self):\n        return self.__bands\n\n\nclass ExplicitScale(FrequencyScale):\n    """"""\n    A scale where the frequency bands are provided explicitly, rather than\n    computed\n\n    Args:\n        bands (list of FrequencyBand): The explicit bands used by this scale\n\n    See Also:\n        :class:`~zounds.spectral.FrequencyAdaptive`\n    """"""\n\n    def __init__(self, bands):\n        bands = list(bands)\n        frequency_band = FrequencyBand(bands[0].start_hz, bands[-1].stop_hz)\n        super(ExplicitScale, self).__init__(\n            frequency_band, len(bands), always_even=False)\n        self._bands = bands\n\n    def _construct_scale_from_slice(self, bands):\n        return ExplicitScale(bands)\n\n    def _compute_bands(self):\n        return self._bands\n\n    def __eq__(self, other):\n        return all([a == b for (a, b) in zip(self, other)])\n\n\nclass Bark(Hertz):\n    def __init__(self, bark):\n        self.bark = bark\n        super(Bark, self).__init__(Bark.to_hz(bark))\n\n    @staticmethod\n    def to_hz(bark):\n        return 300. * ((np.e ** (bark / 6.0)) - (np.e ** (-bark / 6.)))\n\n    @staticmethod\n    def to_bark(hz):\n        return 6. * np.log((hz / 600.) + np.sqrt((hz / 600.) ** 2 + 1))\n\n\ndef equivalent_rectangular_bandwidth(hz):\n    return (0.108 * hz) + 24.7\n\n\nclass BarkScale(FrequencyScale):\n    def __init__(self, frequency_band, n_bands):\n        super(BarkScale, self).__init__(frequency_band, n_bands)\n\n    def _compute_bands(self):\n        start = Bark.to_bark(self.frequency_band.start_hz)\n        stop = Bark.to_bark(self.frequency_band.stop_hz)\n        barks = np.linspace(start, stop, self.n_bands)\n        center_frequencies_hz = Bark.to_hz(barks)\n        bandwidths = equivalent_rectangular_bandwidth(center_frequencies_hz)\n        return [\n            FrequencyBand.from_center(c, b)\n            for c, b in zip(center_frequencies_hz, bandwidths)]\n\n\nclass Mel(Hertz):\n    def __init__(self, mel):\n        self.mel = mel\n        super(Mel, self).__init__(Mel.to_hz(mel))\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * ((np.e ** (mel / 1127)) - 1)\n\n    @staticmethod\n    def to_mel(hz):\n        return 1127 * np.log(1 + (hz / 700))\n\n\nclass MelScale(FrequencyScale):\n    def __init__(self, frequency_band, n_bands):\n        super(MelScale, self).__init__(frequency_band, n_bands)\n\n    def _compute_bands(self):\n        start = Mel.to_mel(self.frequency_band.start_hz)\n        stop = Mel.to_mel(self.frequency_band.stop_hz)\n        mels = np.linspace(start, stop, self.n_bands)\n        center_frequencies_hz = Mel.to_hz(mels)\n        bandwidths = equivalent_rectangular_bandwidth(center_frequencies_hz)\n        return [\n            FrequencyBand.from_center(c, b)\n            for c, b in zip(center_frequencies_hz, bandwidths)]\n\n\nclass ChromaScale(FrequencyScale):\n    def __init__(self, frequency_band):\n        self._a440 = 440.\n        self._a = 2 ** (1 / 12.)\n        super(ChromaScale, self).__init__(frequency_band, n_bands=12)\n\n    def _compute_bands(self):\n        raise NotImplementedError()\n\n    def get_slice(self, frequency_band):\n        raise NotImplementedError()\n\n    def _semitones_to_hz(self, semitone):\n        return self._a440 * (self._a ** semitone)\n\n    def _hz_to_semitones(self, hz):\n        """"""\n        Convert hertz into a number of semitones above or below some reference\n        value, in this case, A440\n        """"""\n        return np.log(hz / self._a440) / np.log(self._a)\n\n    def _basis(self, other_scale, window):\n        basis = np.zeros((self.n_bands, len(other_scale)))\n\n        # for each tone in the twelve-tone scale, generate narrow frequency\n        # bands for every octave of that note that falls within the frequency\n        # band.\n        start_semitones = \\\n            int(np.round(self._hz_to_semitones(self.frequency_band.start_hz)))\n        stop_semitones = \\\n            int(np.round(self._hz_to_semitones(self.frequency_band.stop_hz)))\n\n        semitones = np.arange(start_semitones - 1, stop_semitones)\n        hz = self._semitones_to_hz(semitones)\n\n        bands = []\n        for i in range(0, len(semitones) - 2):\n            fh, mh, lh = hz[i: i + 3]\n            bands.append(FrequencyBand(fh, lh))\n\n        for semitone, band in zip(semitones, bands):\n            slce = other_scale.get_slice(band)\n            chroma_index = semitone % self.n_bands\n            slce = basis[chroma_index, slce]\n            slce[:] += np.ones(len(slce)) * window\n\n        return basis\n'"
zounds/spectral/functional.py,53,"b'from .frequencyscale import LinearScale, FrequencyBand, ExplicitScale\nfrom .tfrepresentation import FrequencyDimension\nfrom .frequencyadaptive import FrequencyAdaptive\nfrom zounds.timeseries import \\\n    audio_sample_rate, TimeSlice, Seconds, TimeDimension, HalfLapped, \\\n    Milliseconds, SampleRate\nfrom zounds.core import ArrayWithUnits, IdentityDimension\nfrom .sliding_window import \\\n    IdentityWindowingFunc, HanningWindowingFunc, WindowingFunc, \\\n    OggVorbisWindowingFunc\nfrom zounds.loudness import log_modulus, unit_scale\nimport numpy as np\nfrom scipy.signal import resample, firwin2\nfrom matplotlib import cm\nfrom scipy.signal import hann, morlet\nfrom itertools import repeat\nfrom zounds.nputil import sliding_window\n\n\ndef fft(x, axis=-1, padding_samples=0):\n    """"""\n    Apply an FFT along the given dimension, and with the specified amount of\n    zero-padding\n\n    Args:\n        x (ArrayWithUnits): an :class:`~zounds.core.ArrayWithUnits` instance\n            which has one or more :class:`~zounds.timeseries.TimeDimension`\n            axes\n        axis (int): The axis along which the fft should be applied\n        padding_samples (int): The number of padding zeros to apply along\n            axis before performing the FFT\n    """"""\n    if padding_samples > 0:\n        padded = np.concatenate(\n            [x, np.zeros((len(x), padding_samples), dtype=x.dtype)],\n            axis=axis)\n    else:\n        padded = x\n\n    transformed = np.fft.rfft(padded, axis=axis, norm=\'ortho\')\n\n    sr = audio_sample_rate(int(Seconds(1) / x.dimensions[axis].frequency))\n    scale = LinearScale.from_sample_rate(sr, transformed.shape[-1])\n    new_dimensions = list(x.dimensions)\n    new_dimensions[axis] = FrequencyDimension(scale)\n    return ArrayWithUnits(transformed, new_dimensions)\n\n\ndef stft(x, window_sample_rate=HalfLapped(), window=HanningWindowingFunc()):\n    duration = TimeSlice(window_sample_rate.duration)\n    frequency = TimeSlice(window_sample_rate.frequency)\n\n    if x.ndim == 1:\n        _, arr = x.sliding_window_with_leftovers(\n            duration, frequency, dopad=True)\n    elif x.ndim == 2 and isinstance(x.dimensions[0], IdentityDimension):\n        arr = x.sliding_window((1, duration), (1, frequency))\n        td = x.dimensions[-1]\n        dims = [IdentityDimension(), TimeDimension(*window_sample_rate), td]\n        arr = ArrayWithUnits(arr.reshape((len(x), -1, arr.shape[-1])), dims)\n    else:\n        raise ValueError(\n            \'x must either have a single TimeDimension, or \'\n            \'(IdentityDimension, TimeDimension)\')\n\n    window = window or IdentityWindowingFunc()\n    windowed = arr * window._wdata(arr.shape[-1])\n    return fft(windowed)\n\n\ndef mdct(data):\n    l = data.shape[-1] // 2\n    t = np.arange(0, 2 * l)\n    f = np.arange(0, l)\n    cpi = -1j * np.pi\n    a = data * np.exp(cpi * t / 2 / l)\n    b = np.fft.fft(a)\n    c = b[..., :l]\n    transformed = np.sqrt(2 / l) * np.real(\n        c * np.exp(cpi * (f + 0.5) * (l + 1) / 2 / l))\n    return transformed\n\n\ndef imdct(frames):\n    l = frames.shape[-1]\n    t = np.arange(0, 2 * l)\n    f = np.arange(0, l)\n    cpi = -1j * np.pi\n    a = frames * np.exp(cpi * (f + 0.5) * (l + 1) / 2 / l)\n    b = np.fft.fft(a, 2 * l)\n    return np.sqrt(2 / l) * np.real(b * np.exp(cpi * t / 2 / l))\n\n\ndef time_stretch(x, factor, frame_sample_rate=None):\n    if frame_sample_rate is None:\n        sr = HalfLapped()\n        sr = SampleRate(frequency=sr.frequency / 2, duration=sr.duration)\n    else:\n        sr = frame_sample_rate\n\n    hop_length, window_length = sr.discrete_samples(x)\n\n    win = WindowingFunc(windowing_func=hann)\n\n    # to simplify, let\'s always compute the stft in ""batch"" mode\n    if x.ndim == 1:\n        x = x.reshape((1,) + x.shape)\n\n    D = stft(x, sr, win)\n\n    n_fft_coeffs = D.shape[-1]\n    n_frames = D.shape[1]\n    n_batches = D.shape[0]\n\n    time_steps = np.arange(0, n_frames, factor, dtype=np.float)\n\n    weights = np.mod(time_steps, 1.0)\n\n    exp_phase_advance = np.linspace(0, np.pi * hop_length, n_fft_coeffs)\n\n    # pad in the time dimension, so no edge/end frames are left out\n    # coeffs = np.pad(D, [(0, 0), (0, 2), (0, 0)], mode=\'constant\')\n    shape = list(D.shape)\n    shape[1] += 2\n    coeffs = np.zeros(shape, dtype=D.dtype)\n    coeffs[:, :-2, :] = D\n\n    coeffs_mags = np.abs(coeffs)\n    coeffs_phases = np.angle(coeffs)\n\n    # we need a phase accumulator for every item in the batch\n    phase_accum = coeffs_phases[:, :1, :]\n\n    sliding_indices = np.vstack([time_steps, time_steps + 1]).T.astype(np.int32)\n\n    windowed_mags = coeffs_mags[:, sliding_indices, :]\n    windowed_phases = coeffs_phases[:, sliding_indices, :]\n\n    first_mags = windowed_mags[:, :, 0, :]\n    second_mags = windowed_mags[:, :, 1, :]\n\n    first_phases = windowed_phases[:, :, 0, :]\n    second_phases = windowed_phases[:, :, 1, :]\n\n    # compute all the phase stuff\n    two_pi = 2.0 * np.pi\n    dphase = (second_phases - first_phases - exp_phase_advance)\n    dphase -= two_pi * np.round(dphase / two_pi)\n    dphase += exp_phase_advance\n\n    all_phases = np.concatenate([phase_accum, dphase], axis=1)\n    dphase = np.cumsum(all_phases, axis=1, out=all_phases)\n    dphase = dphase[:, :-1, :]\n\n    # linear interpolation of FFT coefficient magnitudes\n    weights = weights[None, :, None]\n    mags = ((1.0 - weights) * first_mags) + (weights * second_mags)\n\n    # combine magnitudes and phases\n    new_coeffs = mags * np.exp(1.j * dphase)\n\n    # synthesize the new frames\n    new_frames = np.fft.irfft(new_coeffs, axis=-1, norm=\'ortho\')\n    # new_frames = new_frames * win._wdata(new_frames.shape[-1])\n    new_frames = np.multiply(\n        new_frames, win._wdata(new_frames.shape[-1]), out=new_frames)\n\n    # overlap add the new audio samples\n    new_n_samples = int(x.shape[-1] / factor)\n    output = np.zeros((n_batches, new_n_samples), dtype=x.dtype)\n    for i in range(new_frames.shape[1]):\n        start = i * hop_length\n        stop = start + new_frames.shape[-1]\n        l = output[:, start: stop].shape[1]\n        output[:, start: stop] += new_frames[:, i, :l]\n\n    return ArrayWithUnits(output, [IdentityDimension(), x.dimensions[-1]])\n\n\ndef pitch_shift(x, semitones, frame_sample_rate=None):\n    original_shape = x.shape[1] if x.ndim == 2 else x.shape[0]\n\n    # first, perform a time stretch so that the audio will have the desired\n    # pitch\n    factor = 2.0 ** (-float(semitones) / 12.0)\n    stretched = time_stretch(x, factor, frame_sample_rate=frame_sample_rate)\n\n    # hang on to original dimensions\n    dimensions = stretched.dimensions\n\n    # window the audio using a power-of-2 frame size for more efficient FFT\n    # computations\n    batch_size = stretched.shape[0]\n    window_size = 1024\n    step = (1, window_size)\n    new_window_shape = int(window_size * factor)\n    padding = window_size - int(stretched.shape[-1] % window_size)\n    stretched = np.pad(stretched, ((0, 0), (0, padding)), mode=\'constant\')\n    windowed = sliding_window(stretched, step, step, flatten=False).squeeze()\n\n    # resample the audio so that it has the correct duration\n    rs = resample(windowed, new_window_shape, axis=-1)\n\n    # flatten out the windowed, resampled audio\n    rs = rs.reshape(batch_size, -1)\n\n    # slice the audio to remove residual zeros resulting from our power-of-2\n    # zero padding above\n    rs = rs[:, :original_shape]\n\n    return ArrayWithUnits(rs, dimensions)\n\n\ndef phase_shift(coeffs, samplerate, time_shift, axis=-1, frequency_band=None):\n    frequency_dim = coeffs.dimensions[axis]\n    if not isinstance(frequency_dim, FrequencyDimension):\n        raise ValueError(\n            \'dimension {axis} of coeffs must be a FrequencyDimension instance, \'\n            \'but was {cls}\'.format(axis=axis, cls=frequency_dim.__class__))\n\n    n_coeffs = coeffs.shape[axis]\n    shift_samples = int(time_shift / samplerate.frequency)\n    shift = (np.arange(0, n_coeffs) * 2j * np.pi) / n_coeffs\n    shift = np.exp(-shift * shift_samples)\n    shift = ArrayWithUnits(shift, [frequency_dim])\n\n    frequency_band = frequency_band or slice(None)\n    new_coeffs = coeffs.copy()\n\n    if coeffs.ndim == 1:\n        new_coeffs[frequency_band] *= shift[frequency_band]\n        return new_coeffs\n\n    slices = [slice(None) for _ in range(coeffs.ndim)]\n    slices[axis] = frequency_band\n    new_coeffs[tuple(slices)] *= shift[frequency_band]\n    return new_coeffs\n\n\ndef apply_scale(short_time_fft, scale, window=None):\n    magnitudes = np.abs(short_time_fft.real)\n    spectrogram = scale.apply(magnitudes, window)\n    dimensions = short_time_fft.dimensions[:-1] + (FrequencyDimension(scale),)\n    return ArrayWithUnits(spectrogram, dimensions)\n\n\ndef rainbowgram(time_frequency_repr, colormap=cm.rainbow):\n    # magnitudes on a log scale, and shifted and\n    # scaled to the unit interval\n    magnitudes = np.abs(time_frequency_repr.real)\n    magnitudes = log_modulus(magnitudes * 1000)\n    magnitudes = unit_scale(magnitudes)\n\n    angles = np.angle(time_frequency_repr)\n    angles = np.unwrap(angles, axis=0)\n    angles = np.gradient(angles)[0]\n    angles = unit_scale(angles)\n\n    colors = colormap(angles)\n    colors *= magnitudes[..., None]\n\n    # exclude the alpha channel, if there is one\n    colors = colors[..., :3]\n    arr = ArrayWithUnits(\n        colors, time_frequency_repr.dimensions + (IdentityDimension(),))\n    return arr\n\n\ndef fir_filter_bank(scale, taps, samplerate, window):\n    basis = np.zeros((len(scale), taps))\n    basis = ArrayWithUnits(basis, [\n        FrequencyDimension(scale),\n        TimeDimension(*samplerate)])\n\n    nyq = samplerate.nyquist\n\n    if window.ndim == 1:\n        window = repeat(window, len(scale))\n\n    for i, band, win in zip(range(len(scale)), scale, window):\n        start_hz = max(0, band.start_hz)\n        stop_hz = min(nyq, band.stop_hz)\n        freqs = np.linspace(\n            start_hz / nyq, stop_hz / nyq, len(win), endpoint=False)\n        freqs = [0] + list(freqs) + [1]\n        gains = [0] + list(win) + [0]\n        basis[i] = firwin2(taps, freqs, gains)\n\n    return basis\n\n\ndef morlet_filter_bank(\n        samplerate,\n        kernel_size,\n        scale,\n        scaling_factor,\n        normalize=True):\n    """"""\n    Create a :class:`~zounds.core.ArrayWithUnits` instance with a\n    :class:`~zounds.timeseries.TimeDimension` and a\n    :class:`~zounds.spectral.FrequencyDimension` representing a bank of morlet\n    wavelets centered on the sub-bands of the scale.\n\n    Args:\n        samplerate (SampleRate): the samplerate of the input signal\n        kernel_size (int): the length in samples of each filter\n        scale (FrequencyScale): a scale whose center frequencies determine the\n            fundamental frequency of each filer\n        scaling_factor (int or list of int): Scaling factors for each band,\n            which determine the time-frequency resolution tradeoff.\n            The number(s) should fall between 0 and 1, with smaller numbers\n            achieving better frequency resolution, and larget numbers better\n            time resolution\n        normalize (bool): When true, ensure that each filter in the bank\n            has unit norm\n\n    See Also:\n        :class:`~zounds.spectral.FrequencyScale`\n        :class:`~zounds.timeseries.SampleRate`\n    """"""\n    basis_size = len(scale)\n    basis = np.zeros((basis_size, kernel_size), dtype=np.complex128)\n\n    try:\n        if len(scaling_factor) != len(scale):\n            raise ValueError(\'scaling factor must have same length as scale\')\n    except TypeError:\n        scaling_factor = np.repeat(float(scaling_factor), len(scale))\n\n    sr = int(samplerate)\n\n    for i, band in enumerate(scale):\n        scaling = scaling_factor[i]\n        w = band.center_frequency / (scaling * 2 * sr / kernel_size)\n        basis[i] = morlet(\n            M=kernel_size,\n            w=w,\n            s=scaling)\n    basis = basis.real\n\n    if normalize:\n        basis /= np.linalg.norm(basis, axis=-1, keepdims=True) + 1e-8\n\n    basis = ArrayWithUnits(\n        basis, [FrequencyDimension(scale), TimeDimension(*samplerate)])\n\n    return basis\n\n\ndef auto_correlogram(x, filter_bank, correlation_window=Milliseconds(30)):\n    n_filters = filter_bank.shape[0]\n    filter_size = filter_bank.shape[1]\n\n    corr_win_samples = int(correlation_window / x.samplerate.frequency)\n    windowed = sliding_window(x, filter_size, 1, flatten=False)\n    print(windowed.shape)\n    filtered = np.dot(windowed, filter_bank.T)\n    print(filtered.shape)\n    corr = sliding_window(\n        filtered,\n        ws=(corr_win_samples, n_filters),\n        ss=(1, n_filters),\n        flatten=False)\n    print(corr.shape)\n\n    padded_shape = list(corr.shape)\n    padded_shape[2] = corr_win_samples * 2\n    padded = np.zeros(padded_shape, dtype=np.float32)\n    padded[:, :, :corr_win_samples, :] = corr\n    print(padded.shape)\n\n    coeffs = np.fft.fft(padded, axis=2, norm=\'ortho\')\n    correlated = np.fft.ifft(np.abs(coeffs) ** 2, axis=2, norm=\'ortho\')\n    return np.concatenate([\n        correlated[:, :, corr_win_samples:, :],\n        correlated[:, :, :corr_win_samples, :],\n    ], axis=2)\n    return correlated\n\n\ndef dct_basis(size):\n    r = np.arange(size)\n    basis = np.outer(r, r + 0.5)\n    basis = np.cos((np.pi / size) * basis)\n    return basis\n\n\ndef frequency_decomposition(x, sizes):\n    sizes = sorted(sizes)\n\n    if x.ndim == 1:\n        end = x.dimensions[0].end\n        x = ArrayWithUnits(\n            x[None, ...], [TimeDimension(end, end), x.dimensions[0]])\n\n    original_size = x.shape[-1]\n    time_dimension = x.dimensions[-1]\n    samplerate = audio_sample_rate(time_dimension.samples_per_second)\n    data = x.copy()\n\n    bands = []\n    frequency_bands = []\n    start_hz = 0\n\n    for size in sizes:\n        if size != original_size:\n            s = resample(data, size, axis=-1)\n        else:\n            s = data.copy()\n\n        bands.append(s)\n        data -= resample(s, original_size, axis=-1)\n\n        stop_hz = samplerate.nyquist * (size / original_size)\n        frequency_bands.append(FrequencyBand(start_hz, stop_hz))\n        start_hz = stop_hz\n\n    scale = ExplicitScale(frequency_bands)\n    return FrequencyAdaptive(bands, scale=scale, time_dimension=x.dimensions[0])\n'"
zounds/spectral/sliding_window.py,9,"b'import numpy as np\nfrom featureflow import Node, NotEnoughData\nfrom zounds.core import ArrayWithUnits\nfrom zounds.timeseries import TimeSlice\n\n\ndef oggvorbis(s):\n    """"""\n    This is taken from the ogg vorbis spec\n    (http://xiph.org/vorbis/doc/Vorbis_I_spec.html)\n\n    :param s: the total length of the window, in samples\n    """"""\n    try:\n        s = np.arange(s)\n    except TypeError:\n        s = np.arange(s[0])\n\n    i = np.sin((s + .5) / len(s) * np.pi) ** 2\n    f = np.sin(.5 * np.pi * i)\n    return f * (1. / f.max())\n\n\nclass WindowingFunc(object):\n    """"""\n    `WindowingFunc` is mostly a convenient wrapper around `numpy\'s handy\n    windowing functions\n    <https://docs.scipy.org/doc/numpy/reference/routines.window.html>`_, or any\n    function that takes a size parameter and returns a numpy array-like object.\n\n    A `WindowingFunc` instance can be multiplied with a nother array of any size.\n\n    Args:\n        windowing_func (function): A function that takes a size parameter, and\n            returns a numpy array-like object\n\n    Examples:\n        >>> from zounds import WindowingFunc\n        >>> import numpy as np\n        >>> wf = WindowingFunc(lambda size: np.hanning(size))\n        >>> np.ones(5) *  wf\n        array([ 0. ,  0.5,  1. ,  0.5,  0. ])\n        >>> np.ones(10) * wf\n        array([ 0.        ,  0.11697778,  0.41317591,  0.75      ,  0.96984631,\n                0.96984631,  0.75      ,  0.41317591,  0.11697778,  0.        ])\n\n    See Also:\n        :class:`~IdentityWindowingFunc`\n        :class:`~zounds.spectral.OggVorbisWindowingFunc`\n        :class:`~zounds.spectral.HanningWindowingFunc`\n    """"""\n\n    def __init__(self, windowing_func=None):\n        super(WindowingFunc, self).__init__()\n        self.windowing_func = windowing_func\n        self._cache = dict()\n\n    def _wdata(self, size):\n        if self.windowing_func is None:\n            return None\n        try:\n            return self._cache[size]\n        except KeyError:\n            window = self.windowing_func(size)\n            self._cache[size] = window\n            return window\n\n    def __array_ufunc__(self, ufunc, method, *args, **kwargs):\n\n        if args[0] is self:\n            second_arg = args[1]\n            size = second_arg.shape[-1]\n            dtype = second_arg.dtype\n            wdata = self._wdata(size)\n            if wdata is None:\n                return second_arg\n            first_arg = wdata.astype(dtype)\n        else:\n            first_arg = args[0]\n            size = first_arg.shape[-1]\n            dtype = first_arg.dtype\n            wdata = self._wdata(size)\n            if wdata is None:\n                return first_arg\n            second_arg = wdata.astype(dtype)\n\n        return getattr(ufunc, method)(first_arg, second_arg, **kwargs)\n\n\nclass IdentityWindowingFunc(WindowingFunc):\n    """"""\n    An identity windowing function\n    """"""\n\n    def __init__(self):\n        super(IdentityWindowingFunc, self).__init__()\n\n\nclass OggVorbisWindowingFunc(WindowingFunc):\n    """"""\n    The windowing function described in the `ogg vorbis specification\n    <https://xiph.org/vorbis/doc/Vorbis_I_spec.html#x1-230001.3.2>`_\n    """"""\n\n    def __init__(self):\n        super(OggVorbisWindowingFunc, self).__init__(windowing_func=oggvorbis)\n\n\nclass HanningWindowingFunc(WindowingFunc):\n    """"""\n    A hanning window function\n    """"""\n\n    def __init__(self):\n        super(HanningWindowingFunc, self).__init__(windowing_func=np.hanning)\n\n\nclass SlidingWindow(Node):\n    """"""\n    `SlidingWindow` is a processing node that provides a very common precursor\n    to many frequency domain transforms: a lapped and windowed view of the time-\n    domain signal.\n\n    Args:\n        wscheme (SampleRate): a sample rate that describes the frequency and\n            duration af the sliding window\n        wfunc (WindowingFunc): a windowing function to apply to each frame\n        needs (Node): A processing node on which this node relies for its data.\n            This will generally be a time-domain signal\n\n    Here\'s how you\'d typically see :class:`SlidingWindow` used in a processing\n    graph\n\n    .. code:: python\n\n        import zounds\n\n        Resampled = zounds.resampled(resample_to=zounds.SR11025())\n\n        @zounds.simple_in_memory_settings\n        class Sound(Resampled):\n            windowed = zounds.ArrayWithUnitsFeature(\n                zounds.SlidingWindow,\n                needs=Resampled.resampled,\n                wscheme=zounds.SampleRate(\n                    frequency=zounds.Milliseconds(250),\n                    duration=zounds.Milliseconds(500)),\n                wfunc=zounds.OggVorbisWindowingFunc(),\n                store=True)\n\n\n        synth = zounds.SineSynthesizer(zounds.SR44100())\n        samples = synth.synthesize(zounds.Seconds(5), [220., 440., 880.])\n\n        # process the audio, and fetch features from our in-memory store\n        _id = Sound.process(meta=samples.encode())\n        sound = Sound(_id)\n\n        print sound.windowed.dimensions[0]\n        # TimeDimension(f=0.250068024879, d=0.500045346811)\n        print sound.windowed.dimensions[1]\n        # TimeDimension(f=9.0702947e-05, d=9.0702947e-05)\n\n    See Also:\n        :class:`~zounds.spectral.WindowingFunc`\n        :class:`~zounds.timeseries.SampleRate`\n    """"""\n\n    def __init__(self, wscheme, wfunc=None, padwith=0, needs=None):\n        super(SlidingWindow, self).__init__(needs=needs)\n        self._scheme = wscheme\n        self._func = wfunc\n        self._padwith = padwith\n        self._cache = None\n\n    def _first_chunk(self, data):\n        if self._padwith:\n            padding = np.zeros(\n                (self._padwith,) + data.shape[1:], dtype=data.dtype)\n            padding_ts = ArrayWithUnits(padding, data.dimensions)\n            return padding_ts.concatenate(data)\n        else:\n            return data\n\n    def _enqueue(self, data, pusher):\n        if self._cache is None:\n            self._cache = data\n        else:\n            self._cache = self._cache.concatenate(data)\n\n    def _dequeue(self):\n        duration = TimeSlice(duration=self._scheme.duration)\n        frequency = TimeSlice(duration=self._scheme.frequency)\n        leftover, arr = self._cache.sliding_window_with_leftovers(\n            duration,\n            frequency,\n            dopad=self._finalized)\n\n        if not arr.size:\n            raise NotEnoughData()\n\n        self._cache = leftover\n\n        # BUG: Order matters here (try arr * self._func instead)\n        # why does that statement result in __rmul__ being called for each\n        # scalar value in arr?\n        out = (self._func * arr) if self._func else arr\n        return out\n'"
zounds/spectral/spectral.py,14,"b'\nimport numpy as np\nfrom featureflow import Node\nfrom scipy.fftpack import dct\nfrom scipy.stats.mstats import gmean\n\nfrom .functional import fft, mdct\nfrom .frequencyscale import LinearScale, ChromaScale, BarkScale\nfrom .weighting import AWeighting\nfrom .tfrepresentation import FrequencyDimension\nfrom .frequencyadaptive import FrequencyAdaptive\nfrom zounds.core import ArrayWithUnits, IdentityDimension\nfrom zounds.nputil import safe_log\nfrom zounds.timeseries import audio_sample_rate\nfrom .sliding_window import HanningWindowingFunc\n\n\nclass FrequencyWeighting(Node):\n    """"""\n    `FrequencyWeighting` is a processing node that expects to be passed an\n    :class:`~zounds.core.ArrayWithUnits` instance whose last dimension is a\n    :class:`~zounds.spectral.FrequencyDimension`\n\n    Args:\n        weighting (FrequencyWeighting): the frequency weighting to apply\n        needs (Node): a processing node on which this node depends whose last\n            dimension is a :class:`~zounds.spectral.FrequencyDimension`\n    """"""\n\n    def __init__(self, weighting=None, needs=None):\n        super(FrequencyWeighting, self).__init__(needs=needs)\n        self.weighting = weighting\n\n    def _process(self, data):\n        yield data * self.weighting\n\n\nclass FFT(Node):\n    """"""\n    A processing node that performs an FFT of a real-valued signal\n\n    Args:\n        axis (int): The axis over which the FFT should be computed\n        padding_samples (int): number of zero samples to pad each window with\n            before applying the FFT\n        needs (Node): a processing node on which this one depends\n\n    See Also:\n        :class:`~zounds.synthesize.FFTSynthesizer`\n    """"""\n\n    def __init__(self, needs=None, axis=-1, padding_samples=0):\n        super(FFT, self).__init__(needs=needs)\n        self._axis = axis\n        self._padding_samples = padding_samples\n\n    def _process(self, data):\n        yield fft(data, axis=self._axis, padding_samples=self._padding_samples)\n\n\nclass DCT(Node):\n    """"""\n    A processing node that performs a Type II Discrete Cosine Transform\n    (https://en.wikipedia.org/wiki/Discrete_cosine_transform#DCT-II) of the\n    input\n\n    Args:\n        axis (int): The axis over which to perform the DCT transform\n        needs (Node): a processing node on which this one depends\n\n    See Also:\n        :class:`~zounds.synthesize.DctSynthesizer`\n    """"""\n\n    def __init__(self, axis=-1, scale_always_even=False, needs=None):\n        super(DCT, self).__init__(needs=needs)\n        self.scale_always_even = scale_always_even\n        self._axis = axis\n\n    def _process(self, data):\n        transformed = dct(data, norm=\'ortho\', axis=self._axis)\n\n        sr = audio_sample_rate(\n            int(data.shape[1] / data.dimensions[0].duration_in_seconds))\n        scale = LinearScale.from_sample_rate(\n            sr, transformed.shape[-1], always_even=self.scale_always_even)\n\n        yield ArrayWithUnits(\n            transformed, [data.dimensions[0], FrequencyDimension(scale)])\n\n\nclass DCTIV(Node):\n    """"""\n    A processing node that performs a Type IV Discrete Cosine Transform\n    (https://en.wikipedia.org/wiki/Discrete_cosine_transform#DCT-IV) of the\n    input\n\n    Args:\n        needs (Node): a processing node on which this one depends\n\n    See Also:\n        :class:`~zounds.synthesize.DCTIVSynthesizer`\n    """"""\n\n    def __init__(self, scale_always_even=False, needs=None):\n        super(DCTIV, self).__init__(needs=needs)\n        self.scale_always_even = scale_always_even\n\n    def _process_raw(self, data):\n        l = data.shape[1]\n        tf = np.arange(0, l)\n        z = np.zeros((len(data), l * 2))\n        z[:, :l] = (data * np.exp(-1j * np.pi * tf / 2 / l)).real\n        z = np.fft.fft(z)[:, :l]\n        raw = np.sqrt(2 / l) * \\\n              (z * np.exp(-1j * np.pi * (tf + 0.5) / 2 / l)).real\n        return raw\n\n    def _process(self, data):\n        raw = self._process_raw(data)\n        sr = audio_sample_rate(\n            int(data.shape[1] / data.dimensions[0].duration_in_seconds))\n        scale = LinearScale.from_sample_rate(\n            sr, data.shape[1], always_even=self.scale_always_even)\n        yield ArrayWithUnits(\n            raw, [data.dimensions[0], FrequencyDimension(scale)])\n\n\nclass MDCT(Node):\n    """"""\n    A processing node that performs a modified discrete cosine transform\n    (https://en.wikipedia.org/wiki/Modified_discrete_cosine_transform) of the\n    input.\n\n    This is really just a lapped version of the DCT-IV transform\n\n    Args:\n        needs (Node): a processing node on which this one depends\n\n    See Also:\n        :class:`~zounds.synthesize.MDCTSynthesizer`\n    """"""\n\n    def __init__(self, needs=None):\n        super(MDCT, self).__init__(needs=needs)\n\n    def _process(self, data):\n        transformed = mdct(data)\n\n        sr = audio_sample_rate(data.dimensions[1].samples_per_second)\n        scale = LinearScale.from_sample_rate(sr, transformed.shape[1])\n\n        yield ArrayWithUnits(\n            transformed, [data.dimensions[0], FrequencyDimension(scale)])\n\n\nclass FrequencyAdaptiveTransform(Node):\n    """"""\n    A processing node that expects to receive the input from a frequency domain\n    transformation (e.g. :class:`~zounds.spectral.FFT`), and produces a\n    :class:`~zounds.spectral.FrequencyAdaptive` instance where time resolution\n    can vary by frequency.  This is similar to, but not precisely the same as\n    ideas introduced in:\n\n    * `A quasi-orthogonal, invertible, and perceptually relevant time-frequency transform for audio coding <https://hal-amu.archives-ouvertes.fr/hal-01194806/document>`_\n    * `A FRAMEWORK FOR INVERTIBLE, REAL-TIME CONSTANT-Q TRANSFORMS <http://www.univie.ac.at/nonstatgab/pdf_files/dogrhove12_amsart.pdf>`_\n\n    Args:\n        transform (function): the transform to be applied to each frequency band\n        scale (FrequencyScale): the scale used to take frequency band slices\n        window_func (numpy.ndarray): the windowing function to apply each band\n            before the transform is applied\n        check_scale_overlap_ratio (bool): If this feature is to be used for\n            resynthesis later, ensure that each frequency band overlaps with\n            the previous one by at least half, to ensure artifact-free synthesis\n\n    See Also:\n        :class:`~zounds.spectral.FrequencyAdaptive`\n        :class:`~zounds.synthesize.FrequencyAdaptiveDCTSynthesizer`\n        :class:`~zounds.synthesize.FrequencyAdaptiveFFTSynthesizer`\n    """"""\n\n    def __init__(\n            self,\n            transform=None,\n            scale=None,\n            window_func=None,\n            check_scale_overlap_ratio=False,\n            needs=None):\n        super(FrequencyAdaptiveTransform, self).__init__(needs=needs)\n\n        if check_scale_overlap_ratio:\n            try:\n                scale.ensure_overlap_ratio(0.5)\n            except AssertionError as e:\n                raise ValueError(*e.args)\n\n        self._window_func = window_func or np.ones\n        self._scale = scale\n        self._transform = transform\n\n    def _process_band(self, data, band):\n        try:\n            raw_coeffs = data[:, band]\n        except IndexError:\n            raise ValueError(\n                \'data must have FrequencyDimension as its last dimension, \'\n                \'but it was {dim}\'.format(dim=data.dimensions[-1]))\n        window = self._window_func(raw_coeffs.shape[1])\n        return self._transform(raw_coeffs * window[None, :], norm=\'ortho\')\n\n    def _process(self, data):\n        yield FrequencyAdaptive(\n            [self._process_band(data, band) for band in self._scale],\n            data.dimensions[0],\n            self._scale)\n\n\nclass BaseScaleApplication(Node):\n    def __init__(self, scale, window, needs=None):\n        super(BaseScaleApplication, self).__init__(needs=needs)\n        self.window = window\n        self.scale = scale\n\n    def _new_dim(self):\n        return FrequencyDimension(self.scale)\n\n    def _preprocess(self, data):\n        return data\n\n    def _process(self, data):\n        x = self._preprocess(data)\n        x = self.scale.apply(x, self.window)\n        yield ArrayWithUnits(\n            x, data.dimensions[:-1] + (self._new_dim(),))\n\n\nclass Chroma(BaseScaleApplication):\n    def __init__(\n            self, frequency_band, window=HanningWindowingFunc(), needs=None):\n        super(Chroma, self).__init__(\n            ChromaScale(frequency_band), window, needs=needs)\n\n    def _new_dim(self):\n        return IdentityDimension()\n\n    def _preprocess(self, data):\n        return np.abs(data) * AWeighting()\n\n\nclass BarkBands(BaseScaleApplication):\n    def __init__(\n            self,\n            frequency_band,\n            n_bands=100,\n            window=HanningWindowingFunc(),\n            needs=None):\n        super(BarkBands, self).__init__(\n            BarkScale(frequency_band, n_bands), window, needs=needs)\n\n    def _preprocess(self, data):\n        return np.abs(data)\n\n\nclass SpectralCentroid(Node):\n    """"""\n    Indicates where the ""center of mass"" of the spectrum is. Perceptually,\n    it has a robust connection with the impression of ""brightness"" of a\n    sound.  It is calculated as the weighted mean of the frequencies\n    present in the signal, determined using a Fourier transform, with\n    their magnitudes as the weights...\n\n    -- http://en.wikipedia.org/wiki/Spectral_centroid\n    """"""\n\n    def __init__(self, needs=None):\n        super(SpectralCentroid, self).__init__(needs=needs)\n\n    def _first_chunk(self, data):\n        self._bins = np.arange(1, data.shape[-1] + 1)\n        self._bins_sum = np.sum(self._bins)\n        return data\n\n    def _process(self, data):\n        data = np.abs(data)\n        yield (data * self._bins).sum(axis=1) / self._bins_sum\n\n\nclass SpectralFlatness(Node):\n    """"""\n    Spectral flatness or tonality coefficient, also known as Wiener\n    entropy, is a measure used in digital signal processing to characterize an\n    audio spectrum. Spectral flatness is typically measured in decibels, and\n    provides a way to quantify how tone-like a sound is, as opposed to being\n    noise-like. The meaning of tonal in this context is in the sense of the\n    amount of peaks or resonant structure in a power spectrum, as opposed to\n    flat spectrum of a white noise. A high spectral flatness indicates that\n    the spectrum has a similar amount of power in all spectral bands - this\n    would sound similar to white noise, and the graph of the spectrum would\n    appear relatively flat and smooth. A low spectral flatness indicates that\n    the spectral power is concentrated in a relatively small number of\n    bands - this would typically sound like a mixture of sine waves, and the\n    spectrum would appear ""spiky""...\n\n    -- http://en.wikipedia.org/wiki/Spectral_flatness\n    """"""\n\n    def __init__(self, needs=None):\n        super(SpectralFlatness, self).__init__(needs=needs)\n\n    def _process(self, data):\n        data = np.abs(data)\n        mean = data.mean(axis=1)\n        mean[mean == 0] = -1e5\n        flatness = gmean(data, axis=1) / mean\n        yield ArrayWithUnits(flatness, data.dimensions[:1])\n\n\nclass BFCC(Node):\n    """"""\n    Bark frequency cepstral coefficients\n    """"""\n\n    def __init__(self, needs=None, n_coeffs=13, exclude=1):\n        super(BFCC, self).__init__(needs=needs)\n        self._n_coeffs = n_coeffs\n        self._exclude = exclude\n\n    def _process(self, data):\n        data = np.abs(data)\n        bfcc = dct(safe_log(data), axis=1) \\\n            [:, self._exclude: self._exclude + self._n_coeffs]\n\n        yield ArrayWithUnits(\n            bfcc.copy(), [data.dimensions[0], IdentityDimension()])\n'"
zounds/spectral/test_frequencyadaptive.py,23,"b""import unittest2\nfrom zounds.core import ArrayWithUnits\nfrom zounds.timeseries import TimeDimension, Seconds, Milliseconds, SR11025\nfrom zounds.spectral import \\\n    FrequencyBand, ExplicitFrequencyDimension, GeometricScale, ExplicitScale, \\\n    FrequencyDimension, LinearScale\nfrom .frequencyadaptive import FrequencyAdaptive\nimport numpy as np\n\n\nclass FrequencyAdaptiveTests(unittest2.TestCase):\n    def test_raises_when_time_dimension_is_none(self):\n        scale = GeometricScale(20, 5000, 0.05, 10)\n        arrs = [np.zeros((10, x)) for x in range(1, 11)]\n        self.assertRaises(\n            ValueError, lambda: FrequencyAdaptive(arrs, None, scale))\n\n    def test_raises_when_explicit_freq_dimension_and_non_contiguous_array(self):\n        td = TimeDimension(frequency=Seconds(1))\n        scale = GeometricScale(20, 5000, 0.05, 10)\n        arrs = [np.zeros((10, x)) for x in range(1, 11)]\n        fa1 = FrequencyAdaptive(arrs, td, scale)\n        arr = np.asarray(fa1)\n        self.assertRaises(ValueError, lambda: FrequencyAdaptive(\n            arr,\n            td,\n            scale=scale,\n            explicit_freq_dimension=fa1.frequency_dimension))\n\n    def test_can_construct_from_contiguous_array(self):\n        td = TimeDimension(frequency=Seconds(1))\n        scale = GeometricScale(20, 5000, 0.05, 10)\n        arrs = [np.zeros((10, x)) for x in range(1, 11)]\n        fa1 = FrequencyAdaptive(arrs, td, scale)\n        arr = np.asarray(fa1)\n        fa2 = FrequencyAdaptive(\n            arr, td, explicit_freq_dimension=fa1.frequency_dimension)\n        self.assertEqual(fa1.shape, fa2.shape)\n        self.assertEqual(fa1.scale, fa2.scale)\n        self.assertEqual(fa1.time_dimension, fa2.time_dimension)\n        self.assertEqual(fa1.frequency_dimension, fa2.frequency_dimension)\n\n    def test_can_construct_instance(self):\n        td = TimeDimension(frequency=Seconds(1))\n        scale = GeometricScale(20, 5000, 0.05, 10)\n        arrs = [np.zeros((10, x)) for x in range(1, 11)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n        self.assertEqual((10, 55), fa.shape)\n        self.assertIsInstance(fa.dimensions[0], TimeDimension)\n        self.assertIsInstance(fa.dimensions[1], ExplicitFrequencyDimension)\n\n    def test_can_concatenate_instances(self):\n        td = TimeDimension(frequency=Seconds(1))\n        scale = GeometricScale(20, 5000, 0.05, 10)\n        arrs = [np.zeros((10, x)) for x in range(1, 11)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n\n        arrs2 = [np.zeros((20, x)) for x in range(1, 11)]\n        fa2 = FrequencyAdaptive(arrs2, td, scale)\n\n        result = fa.concatenate(fa2)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((30, 55), result.shape)\n\n    def test_can_get_single_frequency_band_over_entire_duration(self):\n        td = TimeDimension(frequency=Seconds(1))\n        scale = GeometricScale(20, 5000, 0.05, 10)\n        arrs = [np.zeros((10, x)) for x in range(1, 11)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n\n        single_band = fa[:, scale[5]]\n\n        self.assertIsInstance(single_band, ArrayWithUnits)\n        self.assertIsInstance(single_band.dimensions[0], TimeDimension)\n        self.assertIsInstance(\n            single_band.dimensions[1], ExplicitFrequencyDimension)\n        self.assertEqual(1, len(single_band.dimensions[1].scale))\n        self.assertEqual(1, len(single_band.dimensions[1].slices))\n\n    def test_can_apply_frequency_slice_across_multiple_bands(self):\n        td = TimeDimension(frequency=Seconds(1))\n        scale = GeometricScale(20, 5000, 0.05, 10)\n        arrs = [np.zeros((10, x)) for x in range(1, 11)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n        band = FrequencyBand(300, 3030)\n\n        fa2 = fa[:, band]\n\n        self.assertIsInstance(fa2, ArrayWithUnits)\n        self.assertEqual(td, fa2.dimensions[0])\n        self.assertIsInstance(fa2.dimensions[1], ExplicitFrequencyDimension)\n        self.assertIsInstance(fa2.dimensions[1].scale, ExplicitScale)\n\n    def test_can_assign_to_multi_band_frequency_slice(self):\n        td = TimeDimension(frequency=Seconds(1))\n        scale = GeometricScale(20, 5000, 0.05, 10)\n        arrs = [np.zeros((10, x)) for x in range(1, 11)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n        band = FrequencyBand(300, 3030)\n        fa[:, band] = 1\n        int_slice = fa.dimensions[1].integer_based_slice(band)\n        np.testing.assert_allclose(fa[:, int_slice], 1)\n\n    def test_can_access_single_frequency_band(self):\n        td = TimeDimension(\n            duration=Seconds(1),\n            frequency=Milliseconds(500))\n        scale = GeometricScale(20, 5000, 0.05, 120)\n        arrs = [np.zeros((10, x)) for x in range(1, 121)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n        sliced = fa[:, scale[0]]\n        self.assertEqual((10, 1), sliced.shape)\n\n    @unittest2.skip('This test is non-deterministic')\n    def test_rasterize_does_not_distort_spectral_shape(self):\n        n_bands = 8\n        td = TimeDimension(\n            duration=Seconds(1),\n            frequency=Milliseconds(500))\n        scale = GeometricScale(\n            start_center_hz=50,\n            stop_center_hz=5000,\n            bandwidth_ratio=0.07123,\n            n_bands=n_bands)\n        arrs = [np.random.normal(0, 1, (10, 2 ** (i + 1))) for i in\n                range(n_bands)]\n        maxes = [arr.max() for arr in arrs]\n        fa = FrequencyAdaptive(arrs, td, scale)\n        rasterized = fa.rasterize(n_coeffs=32)\n        np.testing.assert_allclose(rasterized.max(axis=(0, 1)), maxes)\n\n    def test_rasterize_gracefully_handles_band_with_no_energy(self):\n        n_bands = 8\n        td = TimeDimension(\n            duration=Seconds(1),\n            frequency=Milliseconds(500))\n        scale = GeometricScale(\n            start_center_hz=50,\n            stop_center_hz=5000,\n            bandwidth_ratio=0.07123,\n            n_bands=n_bands)\n        arrs = [np.random.normal(0, 1, (10, 2 ** (i + 1))) for i in\n                range(n_bands)]\n        arrs[0][:] = 0\n        fa = FrequencyAdaptive(arrs, td, scale)\n        rasterized = fa.rasterize(n_coeffs=32)\n        np.testing.assert_allclose(rasterized[:, :,  0], 0)\n\n    def test_square_form_with_overlap(self):\n        td = TimeDimension(\n            duration=Seconds(1),\n            frequency=Milliseconds(500))\n        scale = GeometricScale(20, 5000, 0.05, 120)\n        arrs = [np.zeros((10, x)) for x in range(1, 121)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n        square = fa.square(50)\n\n        self.assertEqual(3, square.ndim)\n        self.assertEqual(10, square.shape[0])\n        self.assertEqual(50, square.shape[1])\n        self.assertEqual(120, square.shape[2])\n\n        self.assertIsInstance(square, ArrayWithUnits)\n\n        self.assertIsInstance(square.dimensions[0], TimeDimension)\n        self.assertEqual(Milliseconds(5500), square.dimensions[0].end)\n        self.assertEqual(Milliseconds(500), square.dimensions[0].frequency)\n        self.assertEqual(Milliseconds(1000), square.dimensions[0].duration)\n\n        self.assertIsInstance(square.dimensions[1], TimeDimension)\n\n        self.assertIsInstance(square.dimensions[2], FrequencyDimension)\n        self.assertEqual(scale, square.dimensions[2].scale)\n\n    def test_square_form_with_overlap_do_overlap_add(self):\n        td = TimeDimension(\n            duration=Seconds(1),\n            frequency=Milliseconds(500))\n        scale = GeometricScale(20, 5000, 0.05, 120)\n        arrs = [np.zeros((10, x)) for x in range(1, 121)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n        square = fa.square(50, do_overlap_add=True)\n\n        self.assertEqual(2, square.ndim)\n        self.assertEqual(275, square.shape[0])\n        self.assertEqual(120, square.shape[1])\n\n        self.assertIsInstance(square, ArrayWithUnits)\n\n        self.assertIsInstance(square.dimensions[0], TimeDimension)\n        self.assertEqual(Milliseconds(5500), square.dimensions[0].end)\n        self.assertEqual(Milliseconds(20), square.dimensions[0].frequency)\n        self.assertEqual(Milliseconds(20), square.dimensions[0].duration)\n\n        self.assertIsInstance(square.dimensions[1], FrequencyDimension)\n        self.assertEqual(scale, square.dimensions[1].scale)\n\n    def test_square_form_no_overlap(self):\n        td = TimeDimension(\n            duration=Seconds(1),\n            frequency=Seconds(1))\n        scale = GeometricScale(20, 5000, 0.05, 120)\n        arrs = [np.zeros((10, x)) for x in range(1, 121)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n        square = fa.square(50)\n\n        self.assertEqual(3, square.ndim)\n\n        self.assertEqual(10, square.shape[0])\n        self.assertEqual(50, square.shape[1])\n        self.assertEqual(120, square.shape[2])\n\n        self.assertIsInstance(square, ArrayWithUnits)\n\n        self.assertIsInstance(square.dimensions[0], TimeDimension)\n        self.assertEqual(Seconds(10), square.dimensions[0].end)\n        self.assertEqual(Milliseconds(1000), square.dimensions[0].frequency)\n        self.assertEqual(Milliseconds(1000), square.dimensions[0].duration)\n\n        self.assertIsInstance(square.dimensions[1], TimeDimension)\n\n        self.assertIsInstance(square.dimensions[2], FrequencyDimension)\n        self.assertEqual(scale, square.dimensions[2].scale)\n\n    def test_square_form_no_overlap_do_overlap_add(self):\n        td = TimeDimension(\n            duration=Seconds(1),\n            frequency=Seconds(1))\n        scale = GeometricScale(20, 5000, 0.05, 120)\n        arrs = [np.zeros((10, x)) for x in range(1, 121)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n        square = fa.square(50, do_overlap_add=True)\n\n        self.assertEqual(2, square.ndim)\n        self.assertEqual(500, square.shape[0])\n        self.assertEqual(120, square.shape[1])\n\n        self.assertIsInstance(square, ArrayWithUnits)\n\n        self.assertIsInstance(square.dimensions[0], TimeDimension)\n        self.assertEqual(Seconds(10), square.dimensions[0].end)\n        self.assertEqual(Milliseconds(20), square.dimensions[0].frequency)\n        self.assertEqual(Milliseconds(20), square.dimensions[0].duration)\n\n        self.assertIsInstance(square.dimensions[1], FrequencyDimension)\n        self.assertEqual(scale, square.dimensions[1].scale)\n\n    def test_from_arr_with_units(self):\n        td = TimeDimension(frequency=Seconds(1))\n        scale = GeometricScale(20, 5000, 0.05, 10)\n        arrs = [np.zeros((10, x)) for x in range(1, 11)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n\n        raw_arr = ArrayWithUnits(np.array(fa), fa.dimensions)\n\n        fa2 = FrequencyAdaptive.from_array_with_units(raw_arr)\n        self.assertIsInstance(fa2, FrequencyAdaptive)\n        self.assertEqual(fa2.dimensions[0], fa.dimensions[0])\n        self.assertEqual(fa2.dimensions[1], fa.dimensions[1])\n        self.assertEqual(fa.shape, fa2.shape)\n"""
zounds/spectral/test_frequencydimension.py,0,"b""import unittest2\nfrom .tfrepresentation import FrequencyDimension, ExplicitFrequencyDimension\nfrom .frequencyscale import FrequencyBand, LinearScale, GeometricScale\n\n\nclass FrequencyDimensionTests(unittest2.TestCase):\n    def test_equal(self):\n        fd1 = FrequencyDimension(LinearScale(FrequencyBand(20, 10000), 100))\n        fd2 = FrequencyDimension(LinearScale(FrequencyBand(20, 10000), 100))\n        self.assertEqual(fd1, fd2)\n\n    def test_not_equal(self):\n        fd1 = FrequencyDimension(LinearScale(FrequencyBand(20, 10000), 100))\n        fd2 = FrequencyDimension(GeometricScale(20, 10000, 0.01, 100))\n        self.assertNotEqual(fd1, fd2)\n\n\nclass ExplicitFrequencyDimensionTests(unittest2.TestCase):\n    def test_equal(self):\n        scale1 = GeometricScale(20, 5000, 0.02, 3)\n        slices1 = [slice(0, 10), slice(10, 100), slice(100, 1000)]\n        dim1 = ExplicitFrequencyDimension(scale1, slices1)\n\n        scale2 = GeometricScale(20, 5000, 0.02, 3)\n        slices2 = [slice(0, 10), slice(10, 100), slice(100, 1000)]\n        dim2 = ExplicitFrequencyDimension(scale2, slices2)\n\n        self.assertEqual(dim1, dim2)\n\n    def test_not_equal_due_to_slices(self):\n        scale1 = GeometricScale(20, 5000, 0.02, 3)\n        slices1 = [slice(0, 10), slice(10, 100), slice(100, 1000)]\n        dim1 = ExplicitFrequencyDimension(scale1, slices1)\n\n        scale2 = GeometricScale(20, 5000, 0.02, 3)\n        slices2 = [slice(0, 10), slice(10, 100), slice(100, 1001)]\n        dim2 = ExplicitFrequencyDimension(scale2, slices2)\n\n        self.assertNotEqual(dim1, dim2)\n\n    def test_not_equal_due_to_scales(self):\n        scale1 = GeometricScale(20, 5000, 0.02, 3)\n        slices1 = [slice(0, 10), slice(10, 100), slice(100, 1000)]\n        dim1 = ExplicitFrequencyDimension(scale1, slices1)\n\n        scale2 = GeometricScale(20, 4500, 0.02, 3)\n        slices2 = [slice(0, 10), slice(10, 100), slice(100, 1000)]\n        dim2 = ExplicitFrequencyDimension(scale2, slices2)\n\n        self.assertNotEqual(dim1, dim2)\n\n    def test_raises_when_scale_and_slices_are_different_sizes(self):\n        scale1 = GeometricScale(20, 5000, 0.02, 3)\n        slices1 = [slice(0, 10), slice(10, 100)]\n        self.assertRaises(\n            ValueError, lambda: ExplicitFrequencyDimension(scale1, slices1))\n\n    def test_metaslice(self):\n        scale1 = GeometricScale(20, 5000, 0.02, 3)\n        bands = list(scale1)\n        slices1 = [slice(0, 10), slice(10, 100), slice(100, 1000)]\n        dim1 = ExplicitFrequencyDimension(scale1, slices1)\n        dim2 = dim1.metaslice(FrequencyBand(15, 1000), 2)\n        self.assertEqual(bands[:2], list(dim2.scale)[:2])\n        self.assertEqual(slices1[:2], dim2.slices[:2])\n\n    def test_metaslice_exact_matching_band(self):\n        scale = GeometricScale(20, 5000, 0.05, 120)\n        # the values of the slices don't matter for this test\n        slices = [slice(0, 10) for _ in range(len(scale))]\n        dim = ExplicitFrequencyDimension(scale, slices)\n        dim2 = dim.metaslice(scale[0], 1)\n        self.assertEqual(1, len(dim2.scale))\n        self.assertEqual(1, len(dim2.slices))\n        self.assertEqual(dim.scale[0], dim2.scale[0])\n        self.assertEqual(dim.slices[0], dim2.slices[0])\n\n    def test_metaslice_fuzzy_matching_band(self):\n        scale = GeometricScale(20, 5000, 0.05, 120)\n        # the values of the slices don't matter for this test\n        slices = [slice(0, 10) for _ in range(len(scale))]\n        dim = ExplicitFrequencyDimension(scale, slices)\n        first_band = scale[0]\n        band = FrequencyBand(first_band.start_hz, first_band.stop_hz + 1)\n        dim2 = dim.metaslice(band, 3)\n        self.assertEqual(3, len(dim2.scale))\n        self.assertEqual(3, len(dim2.slices))\n        self.assertEqual(dim.scale[0], dim2.scale[0])\n        self.assertEqual(dim.scale[1], dim2.scale[1])\n        self.assertEqual(dim.scale[2], dim2.scale[2])\n        self.assertEqual(dim.slices[0], dim2.slices[0])\n        self.assertEqual(dim.slices[1], dim2.slices[1])\n        self.assertEqual(dim.slices[2], dim2.slices[2])\n\n    def test_can_get_slice_when_perfectly_corresponds_to_band(self):\n        scale1 = GeometricScale(20, 5000, 0.02, 3)\n        bands = list(scale1)\n        slices1 = [slice(0, 10), slice(10, 100), slice(100, 1000)]\n        dim1 = ExplicitFrequencyDimension(scale1, slices1)\n        self.assertEqual(slices1[1], dim1.integer_based_slice(bands[1]))\n\n    def test_can_get_slice_with_overlap(self):\n        scale1 = GeometricScale(20, 5000, 0.02, 3)\n        bands = list(scale1)\n        slices1 = [slice(0, 10), slice(5, 100), slice(50, 1000)]\n        dim1 = ExplicitFrequencyDimension(scale1, slices1)\n        self.assertEqual(slices1[1], dim1.integer_based_slice(bands[1]))\n\n    def test_is_valid_when_size_corresponds_to_last_slice_end(self):\n        scale1 = GeometricScale(20, 5000, 0.02, 3)\n        slices1 = [slice(0, 10), slice(10, 100), slice(100, 1000)]\n        dim1 = ExplicitFrequencyDimension(scale1, slices1)\n        self.assertTrue(dim1.validate(1000))\n\n    def test_is_not_valid_when_size_does_not_correspond_to_last_slice(self):\n        scale1 = GeometricScale(20, 5000, 0.02, 3)\n        slices1 = [slice(0, 10), slice(10, 100), slice(100, 1000)]\n        dim1 = ExplicitFrequencyDimension(scale1, slices1)\n        self.assertFalse(dim1.validate(2000))\n"""
zounds/spectral/test_frequencyscale.py,5,"b""\nimport unittest2\nfrom .frequencyscale import \\\n    FrequencyBand, LinearScale, ExplicitScale, GeometricScale, Hertz, Hz\nfrom zounds.timeseries import SR44100\nimport numpy as np\n\n\nclass HertzTests(unittest2.TestCase):\n    def test_can_negate_hertz(self):\n        hz = -Hertz(10)\n        self.assertIsInstance(hz, Hertz)\n\n    def test_can_add_hertz(self):\n        hz = Hertz(20) + Hertz(30)\n        self.assertEqual(Hertz(50), hz)\n\n    def test_can_subtract_hertz(self):\n        hz = Hz(100) - Hz(20)\n        self.assertEqual(Hz(80), hz)\n\n\nclass FrequencyBandTests(unittest2.TestCase):\n\n    def test_can_create_frequency_band_with_hertz_instances(self):\n        fb = FrequencyBand(Hz(20), Hz(20000))\n        self.assertIsInstance(fb.center_frequency, float)\n        self.assertEqual(20, fb.start_hz)\n        self.assertEqual(20000, fb.stop_hz)\n\n    def test_can_create_from_center_frequency(self):\n        fb = FrequencyBand.from_center(1000, 50)\n        self.assertEqual(FrequencyBand(975, 1025), fb)\n\n    def test_does_not_equal_non_frequency_band_class(self):\n        fb = FrequencyBand(100, 200)\n        self.assertNotEqual(fb, 10)\n\n    def test_cannot_create_with_invalid_interval(self):\n        self.assertRaises(ValueError, lambda: FrequencyBand(200, 100))\n\n    def test_identical_frequency_bands_have_same_hash_value(self):\n        fb1 = FrequencyBand.from_center(1000, 50)\n        fb2 = FrequencyBand.from_center(1000, 50)\n        self.assertEqual(hash(fb1), hash(fb2))\n\n    def test_can_intersect(self):\n        fb1 = FrequencyBand(0, 100)\n        fb2 = FrequencyBand(50, 150)\n        intersection = fb1.intersect(fb2)\n        self.assertEqual(FrequencyBand(50, 100), intersection)\n\n    def test_error_raised_when_no_intersection(self):\n        fb1 = FrequencyBand(0, 100)\n        fb2 = FrequencyBand(200, 500)\n        self.assertRaises(ValueError, lambda: fb1.intersect(fb2))\n\n    def test_intersection_ratio(self):\n        fb1 = FrequencyBand(0, 100)\n        fb2 = FrequencyBand(50, 150)\n        ratio = fb1.intersection_ratio(fb2)\n        self.assertEqual(0.5, ratio)\n\n    def test_audible_range_lower_bound(self):\n        band = FrequencyBand.audible_range(SR44100())\n        self.assertEqual(20, band.start_hz)\n\n    def test_audible_range_upper_bound(self):\n        sr = SR44100()\n        band = FrequencyBand.audible_range(sr)\n        self.assertEqual(int(sr) // 2, band.stop_hz)\n\n\nclass FrequencyScaleTests(unittest2.TestCase):\n    def test_get_slice_converts_frequency_band_to_integer_based_slice(self):\n        scale = LinearScale(FrequencyBand(0, 100), 10)\n        slce = scale.get_slice(FrequencyBand(0, 20))\n        self.assertEqual(slice(0, 2), slce)\n\n    def test_get_slice_converts_hz_based_slice_to_integer_based_slice(self):\n        scale = LinearScale(FrequencyBand(0, 100), 10)\n        slce = scale.get_slice(slice(Hertz(0), Hertz(20)))\n        self.assertEqual(slice(0, 2), slce)\n\n    def test_get_slice_returns_integer_based_slice_unaltered(self):\n        scale = LinearScale(FrequencyBand(0, 100), 10)\n        slce = scale.get_slice(slice(0, 20))\n        self.assertEqual(slice(0, 20), slce)\n\n    def test_can_get_all_even_sized_bands(self):\n        samplerate = SR44100()\n        scale = LinearScale.from_sample_rate(\n            samplerate, 44100, always_even=True)\n        log_scale = GeometricScale(20, 20000, 0.01, 64)\n        slices = [scale.get_slice(band) for band in log_scale]\n        sizes = [s.stop - s.start for s in slices]\n        self.assertTrue(\n            not any([s % 2 for s in sizes]),\n            'All slice sizes should be even but were {sizes}'\n                .format(**locals()))\n\n    def test_can_get_single_band(self):\n        fb1 = FrequencyBand(20, 20000)\n        scale1 = LinearScale(fb1, 100)\n        fb2 = scale1[10]\n        self.assertIsInstance(fb2, FrequencyBand)\n\n    def test_can_get_sub_scale(self):\n        fb1 = FrequencyBand(20, 20000)\n        scale1 = LinearScale(fb1, 100)\n        scale2 = scale1[10:20]\n        self.assertIsInstance(scale2, LinearScale)\n        self.assertEqual(10, scale2.n_bands)\n\n    def test_equals(self):\n        fb1 = FrequencyBand(20, 20000)\n        scale1 = LinearScale(fb1, 100)\n\n        fb2 = FrequencyBand(20, 20000)\n        scale2 = LinearScale(fb2, 100)\n\n        self.assertEqual(scale1, scale2)\n\n    def test_not_equal_when_scale_differs(self):\n        fb1 = FrequencyBand(20, 20000)\n        scale1 = LinearScale(fb1, 100)\n\n        fb2 = FrequencyBand(20, 20000)\n        scale2 = GeometricScale(20, 20000, 0.01, 100)\n\n        self.assertNotEqual(scale1, scale2)\n\n    def test_not_equal_when_span_differs(self):\n        fb1 = FrequencyBand(20, 20000)\n        scale1 = LinearScale(fb1, 100)\n\n        fb2 = FrequencyBand(20, 10000)\n        scale2 = LinearScale(fb2, 100)\n\n        self.assertNotEqual(scale1, scale2)\n\n    def test_not_equal_when_bands_differ(self):\n        fb1 = FrequencyBand(20, 20000)\n        scale1 = LinearScale(fb1, 100)\n\n        fb2 = FrequencyBand(20, 20000)\n        scale2 = LinearScale(fb2, 50)\n\n        self.assertNotEqual(scale1, scale2)\n\n\nclass LinearScaleTests(unittest2.TestCase):\n    def test_matches_fftfreq(self):\n        samplerate = SR44100()\n        n_bands = 2048\n        fft_freqs = np.fft.rfftfreq(n_bands, 1 / int(samplerate))\n        bands = LinearScale.from_sample_rate(samplerate, n_bands // 2)\n        linear_freqs = np.array([b.start_hz for b in bands])\n        np.testing.assert_allclose(linear_freqs, fft_freqs[:-1])\n\n    def test_constant_bandwidth(self):\n        scale = LinearScale(FrequencyBand(0, 22050), 1024)\n        # taking the second-order differential should result in all zeros\n        # if the bandwidths are a constant size\n        diff = np.diff(list(scale.center_frequencies), n=2)\n        np.testing.assert_allclose(diff, np.zeros(len(diff)), atol=1e-11)\n\n    def test_get_slice_on_boundary(self):\n        scale = LinearScale(FrequencyBand(0, 1000), 100)\n        sl = scale.get_slice(FrequencyBand(500, 700))\n        self.assertEqual(slice(49, 70), sl)\n\n    def test_get_slice_between_boundary(self):\n        scale = LinearScale(FrequencyBand(0, 1000), 10)\n        sl = scale.get_slice(FrequencyBand(495, 705))\n        self.assertEqual(slice(4, 8), sl)\n\n    def test_start_hz(self):\n        scale = LinearScale(FrequencyBand(100, 500), 4)\n        start_hz = [b.start_hz for b in scale]\n        self.assertEqual([100, 200, 300, 400], start_hz)\n\n\nclass GeometricScaleTests(unittest2.TestCase):\n    def test_slicing_geometric_scale_returns_explicit_scale(self):\n        scale = GeometricScale(\n            start_center_hz=20,\n            stop_center_hz=5000,\n            bandwidth_ratio=0.05,\n            n_bands=100)\n        sliced = scale[FrequencyBand(100, 1000)]\n        self.assertIsInstance(sliced, ExplicitScale)\n\n    def test_ensure_minimal_inersection_ratio_no_overlap(self):\n        scale = GeometricScale(\n            start_center_hz=300,\n            stop_center_hz=3030,\n            bandwidth_ratio=0.001,\n            n_bands=300)\n\n        self.assertRaises(\n            AssertionError,\n            lambda: scale.ensure_overlap_ratio(0.5))\n\n    def test_ensure_minimal_inersection_ratio_insufficient_overlap(self):\n        scale = GeometricScale(\n            start_center_hz=300,\n            stop_center_hz=3030,\n            bandwidth_ratio=0.01,\n            n_bands=300)\n\n        self.assertRaises(\n            AssertionError,\n            lambda: scale.ensure_overlap_ratio(0.5))\n\n    def test_ensure_minimal_intersection_ratio(self):\n        scale = GeometricScale(\n            start_center_hz=300,\n            stop_center_hz=3030,\n            bandwidth_ratio=0.017,\n            n_bands=300)\n\n        try:\n            scale.ensure_overlap_ratio(0.5)\n        except AssertionError:\n            self.fail('AssertionError was raised')\n\n\nclass ExplicitScaleTests(unittest2.TestCase):\n    def test_can_construct_explicit_scale_from_scale(self):\n        linear_scale = LinearScale(FrequencyBand(100, 1000), n_bands=50)\n        explicit_scale = ExplicitScale(linear_scale)\n        self.assertSequenceEqual(linear_scale.bands, explicit_scale.bands)\n\n    def test_can_construct_explicit_scale_from_iterable_of_bands(self):\n        linear_scale = LinearScale(FrequencyBand(100, 1000), n_bands=50)\n        explicit_scale = ExplicitScale(linear_scale.bands)\n        self.assertSequenceEqual(linear_scale.bands, explicit_scale.bands)\n\n    def test_equals(self):\n        scale1 = ExplicitScale(\n            LinearScale(FrequencyBand(100, 1000), n_bands=50))\n        scale2 = ExplicitScale(\n            LinearScale(FrequencyBand(100, 1000), n_bands=50))\n        self.assertEqual(scale1, scale2)\n"""
zounds/spectral/test_functional.py,20,"b""import numpy as np\nimport unittest2\nfrom .functional import \\\n    fft, stft, apply_scale, frequency_decomposition, phase_shift, rainbowgram, \\\n    fir_filter_bank, auto_correlogram, time_stretch, pitch_shift, \\\n    morlet_filter_bank, mdct, imdct\nfrom zounds.core import ArrayWithUnits, IdentityDimension\nfrom zounds.synthesize import \\\n    SilenceSynthesizer, TickSynthesizer, SineSynthesizer, FFTSynthesizer\nfrom zounds.timeseries import SR22050, Seconds, Milliseconds, TimeDimension, \\\n    TimeSlice, AudioSamples\nfrom zounds.spectral import \\\n    HanningWindowingFunc, FrequencyDimension, LinearScale, GeometricScale, \\\n    ExplicitFrequencyDimension, FrequencyBand, MelScale\nfrom matplotlib import cm\n\n\nclass FIRFilterBankTests(unittest2.TestCase):\n    def test_has_correct_dimensions(self):\n        samplerate = SR22050()\n        scale = GeometricScale(\n            start_center_hz=20,\n            stop_center_hz=10000,\n            bandwidth_ratio=0.2,\n            n_bands=100)\n        scale.ensure_overlap_ratio(0.5)\n        taps = 256\n        filter_bank = fir_filter_bank(scale, taps, samplerate, np.hanning(100))\n        self.assertEqual((len(scale), taps), filter_bank.shape)\n        self.assertEqual(FrequencyDimension(scale), filter_bank.dimensions[0])\n        self.assertEqual(TimeDimension(*samplerate), filter_bank.dimensions[1])\n\n\nclass MorletFilterBankTests(unittest2.TestCase):\n    def test_raises_when_scale_factors_length_does_not_match_scale(self):\n        sr = SR22050()\n        band = FrequencyBand(1, sr.nyquist)\n        scale = MelScale(band, 512)\n        scale_factors = np.linspace(0.1, 1.0, len(scale) // 2)\n        self.assertRaises(\n            ValueError,\n            lambda: morlet_filter_bank(sr, 512, scale, scale_factors))\n\n    def test_raises_when_scale_factors_is_not_a_collection_or_float(self):\n        sr = SR22050()\n        band = FrequencyBand(1, sr.nyquist)\n        scale = MelScale(band, 512)\n        scale_factors = object()\n        self.assertRaises(\n            TypeError,\n            lambda: morlet_filter_bank(sr, 512, scale, scale_factors))\n\n    def test_dimensions_are_correct(self):\n        sr = SR22050()\n        band = FrequencyBand(1, sr.nyquist)\n        scale = MelScale(band, 128)\n        scale_factors = np.linspace(0.1, 1.0, len(scale))\n        filter_bank = morlet_filter_bank(sr, 512, scale, scale_factors)\n        self.assertEqual((128, 512), filter_bank.shape)\n        expected_freq_dimension = FrequencyDimension(scale)\n        expected_time_dimension = TimeDimension(*sr)\n        self.assertEqual(expected_freq_dimension, filter_bank.dimensions[0])\n        self.assertEqual(expected_time_dimension, filter_bank.dimensions[1])\n\n    def test_filters_are_normalized(self):\n        sr = SR22050()\n        band = FrequencyBand(1, sr.nyquist)\n        scale = MelScale(band, 128)\n        scale_factors = np.linspace(0.1, 1.0, len(scale))\n        filter_bank = morlet_filter_bank(\n            sr, 512, scale, scale_factors, normalize=True)\n        norms = np.linalg.norm(filter_bank, axis=-1)\n        np.testing.assert_allclose(norms, 1.0, rtol=1e-6)\n\n\nclass AutoCorrelogramTests(unittest2.TestCase):\n    @unittest2.skip\n    def test_smoke(self):\n        samples = AudioSamples.silence(SR22050(), Seconds(1))\n        samplerate = SR22050()\n        scale = GeometricScale(\n            start_center_hz=20,\n            stop_center_hz=5000,\n            bandwidth_ratio=1.2,\n            n_bands=8)\n        scale.ensure_overlap_ratio(0.5)\n        taps = 16\n        filter_bank = fir_filter_bank(scale, taps, samplerate, np.hanning(3))\n        correlogram = auto_correlogram(samples, filter_bank)\n        self.assertEqual(3, correlogram.ndim)\n\n\nclass FrequencyDecompositionTests(unittest2.TestCase):\n    def test_can_decompose_audio_samples(self):\n        samples = AudioSamples.silence(SR22050(), Seconds(1))\n        bands = frequency_decomposition(samples, [64, 128, 256, 512, 1024])\n        expected_td = TimeDimension(samples.end, samples.end)\n        self.assertEqual(expected_td, bands.dimensions[0])\n        self.assertIsInstance(bands.dimensions[1], ExplicitFrequencyDimension)\n\n    def test_can_decompose(self):\n        sr = SR22050()\n        samples = SilenceSynthesizer(sr).synthesize(Milliseconds(9999))\n        wscheme = sr.windowing_scheme(8192, 4096)\n        duration = TimeSlice(wscheme.duration)\n        frequency = TimeSlice(wscheme.frequency)\n        _, windowed = samples.sliding_window_with_leftovers(\n            duration, frequency, dopad=True)\n        fa = frequency_decomposition(\n            windowed, [32, 64, 128, 256, 512, 1024, 2048, 4096])\n        self.assertEqual(windowed.dimensions[0], fa.dimensions[0])\n        self.assertIsInstance(\n            fa.dimensions[1], ExplicitFrequencyDimension)\n\n\nclass FFTTests(unittest2.TestCase):\n    def test_can_pad_for_better_frequency_resolution(self):\n        samples = SilenceSynthesizer(SR22050()).synthesize(Milliseconds(2500))\n        windowsize = TimeSlice(duration=Milliseconds(200))\n        stepsize = TimeSlice(duration=Milliseconds(100))\n        _, windowed = samples.sliding_window_with_leftovers(\n            windowsize=windowsize, stepsize=stepsize, dopad=True)\n        coeffs = fft(windowed, padding_samples=1024)\n        self.assertIsInstance(coeffs, ArrayWithUnits)\n        self.assertEqual(2, len(coeffs.dimensions))\n        self.assertEqual(windowed.dimensions[0], coeffs.dimensions[0])\n        self.assertIsInstance(coeffs.dimensions[1], FrequencyDimension)\n        expected_size = ((windowed.shape[-1] + 1024) // 2) + 1\n        self.assertEqual(expected_size, coeffs.shape[-1])\n\n    def test_can_take_fft_of_1d_signal(self):\n        samples = SilenceSynthesizer(SR22050()).synthesize(Milliseconds(2500))\n        coeffs = fft(samples)\n        self.assertIsInstance(coeffs, ArrayWithUnits)\n        self.assertEqual(1, len(coeffs.dimensions))\n        self.assertIsInstance(coeffs.dimensions[0], FrequencyDimension)\n\n    def test_can_take_fft_of_2d_stacked_signal(self):\n        samples = SilenceSynthesizer(SR22050()).synthesize(Milliseconds(2500))\n        windowsize = TimeSlice(duration=Milliseconds(200))\n        stepsize = TimeSlice(duration=Milliseconds(100))\n        _, windowed = samples.sliding_window_with_leftovers(\n            windowsize=windowsize, stepsize=stepsize, dopad=True)\n        coeffs = fft(windowed)\n        self.assertIsInstance(coeffs, ArrayWithUnits)\n        self.assertEqual(2, len(coeffs.dimensions))\n        self.assertEqual(windowed.dimensions[0], coeffs.dimensions[0])\n        self.assertIsInstance(coeffs.dimensions[1], FrequencyDimension)\n\n\nclass PhaseShiftTests(unittest2.TestCase):\n    def _mean_squared_error(self, x, y):\n        l = min(len(x), len(y))\n        return ((x[:l] - y[:l]) ** 2).mean()\n\n    def test_1d_phase_shift_returns_correct_size(self):\n        samplerate = SR22050()\n        samples = SineSynthesizer(samplerate) \\\n            .synthesize(Milliseconds(5500), [220, 440, 880])\n        coeffs = fft(samples)\n        shifted = phase_shift(\n            coeffs=coeffs,\n            samplerate=samplerate,\n            time_shift=Milliseconds(5500),\n            frequency_band=FrequencyBand(50, 5000))\n        self.assertEqual(coeffs.shape, shifted.shape)\n\n    def test_can_phase_shift_1d_signal(self):\n        samplerate = SR22050()\n        samples = SineSynthesizer(samplerate) \\\n            .synthesize(Milliseconds(5000), [220, 440, 880])\n        coeffs = fft(samples)\n        shifted = phase_shift(coeffs, samplerate, Milliseconds(10))\n        new_samples = np.fft.irfft(shifted, norm='ortho')\n        self.assertNotEqual(0, self._mean_squared_error(samples, new_samples))\n\n    def test_can_phase_shift_1d_signal_180_degrees(self):\n        samplerate = SR22050()\n        samples = SineSynthesizer(samplerate) \\\n            .synthesize(Seconds(1), [110, 220, 440, 880])\n        coeffs = fft(samples)\n        shifted = phase_shift(\n            coeffs=coeffs,\n            samplerate=samplerate,\n            time_shift=-Milliseconds(1000),\n            frequency_band=FrequencyBand(50, 5000))\n        new_samples = np.fft.irfft(shifted, norm='ortho')\n        self.assertAlmostEqual(\n            0, self._mean_squared_error(samples, new_samples), 1)\n\n    def test_2d_phase_shift_returns_correct_shape(self):\n        samplerate = SR22050()\n        samples = SineSynthesizer(samplerate) \\\n            .synthesize(Milliseconds(2500), [220, 440, 880])\n        windowsize = TimeSlice(duration=Milliseconds(200))\n        stepsize = TimeSlice(duration=Milliseconds(100))\n        _, windowed = samples.sliding_window_with_leftovers(\n            windowsize=windowsize, stepsize=stepsize, dopad=True)\n        coeffs = fft(windowed)\n        shifted = phase_shift(\n            coeffs=coeffs,\n            samplerate=samplerate,\n            time_shift=Milliseconds(40),\n            frequency_band=FrequencyBand(50, 5000))\n        self.assertEqual(coeffs.shape, shifted.shape)\n\n    def test_can_phase_shift_2d_signal(self):\n        samplerate = SR22050()\n        samples = SineSynthesizer(samplerate) \\\n            .synthesize(Milliseconds(2500), [220, 440, 880])\n        windowsize = TimeSlice(duration=Milliseconds(200))\n        stepsize = TimeSlice(duration=Milliseconds(100))\n        _, windowed = samples.sliding_window_with_leftovers(\n            windowsize=windowsize, stepsize=stepsize, dopad=True)\n        coeffs = fft(windowed)\n        shifted = phase_shift(coeffs, samplerate, Milliseconds(40))\n        synth = FFTSynthesizer()\n        new_samples = synth.synthesize(shifted).squeeze()\n        self.assertNotEqual(0, self._mean_squared_error(samples, new_samples))\n\n    def test_can_phase_shift_2d_signal_180_degrees(self):\n        samplerate = SR22050()\n        samples = SineSynthesizer(samplerate) \\\n            .synthesize(Milliseconds(2500), [220, 440, 880])\n        windowsize = TimeSlice(duration=Milliseconds(200))\n        stepsize = TimeSlice(duration=Milliseconds(100))\n        _, windowed = samples.sliding_window_with_leftovers(\n            windowsize=windowsize, stepsize=stepsize, dopad=True)\n        coeffs = fft(windowed)\n        shifted = phase_shift(\n            coeffs=coeffs,\n            samplerate=samplerate,\n            time_shift=Milliseconds(100))\n        synth = FFTSynthesizer()\n        new_samples = synth.synthesize(shifted).squeeze()\n        self.assertAlmostEqual(\n            0, self._mean_squared_error(samples, new_samples), 1)\n\n    def test_raises_value_error_when_specified_axis_not_frequency_dim(self):\n        samplerate = SR22050()\n        samples = SineSynthesizer(samplerate) \\\n            .synthesize(Milliseconds(2500), [220, 440, 880])\n        self.assertRaises(\n            ValueError,\n            lambda: phase_shift(samples, samplerate, Milliseconds(10)))\n\n\nclass MDCTTests(unittest2.TestCase):\n    def test_can_perform_mdct_on_2d_data(self):\n        # (time, window_of_samples)\n        x = np.random.normal(0, 1, (133, 512))\n        coeffs = mdct(x)\n        self.assertEqual((133, 256), coeffs.shape)\n\n    def test_can_invert_2d_data(self):\n        # (time, window_of_samples)\n        x = np.random.normal(0, 1, (133, 512))\n        coeffs = mdct(x)\n        recon = imdct(coeffs)\n        self.assertEqual(x.shape, recon.shape)\n\n    def test_can_perform_mdct_on_3d_data(self):\n        x = np.random.normal(0, 1, (4, 133, 512))\n        coeffs = mdct(x)\n        self.assertEqual((4, 133, 256), coeffs.shape)\n\n    def test_can_invert_3d_data(self):\n        x = np.random.normal(0, 1, (4, 133, 512))\n        coeffs = mdct(x)\n        recon = imdct(coeffs)\n        self.assertEqual(x.shape, recon.shape)\n\n\nclass STFTTests(unittest2.TestCase):\n    def test_has_correct_number_of_bins(self):\n        sr = SR22050()\n        samples = SilenceSynthesizer(sr).synthesize(Milliseconds(6666))\n        wscheme = sr.windowing_scheme(512, 256)\n        tf = stft(samples, wscheme, HanningWindowingFunc())\n        self.assertEqual(tf.shape[1], 257)\n\n    def test_has_correct_dimensions(self):\n        sr = SR22050()\n        samples = SilenceSynthesizer(sr).synthesize(Milliseconds(6666))\n        wscheme = sr.windowing_scheme(512, 256)\n        tf = stft(samples, wscheme, HanningWindowingFunc())\n        self.assertIsInstance(tf, ArrayWithUnits)\n        self.assertEqual(2, len(tf.dimensions))\n        self.assertIsInstance(tf.dimensions[0], TimeDimension)\n        self.assertEqual(tf.dimensions[0].samplerate, wscheme)\n        self.assertIsInstance(tf.dimensions[1], FrequencyDimension)\n        self.assertIsInstance(tf.dimensions[1].scale, LinearScale)\n\n    def test_can_take_stft_of_batch(self):\n        sr = SR22050()\n        samples = SilenceSynthesizer(sr).synthesize(Milliseconds(6666))\n        stacked = ArrayWithUnits(\n            np.zeros((10,) + samples.shape, dtype=samples.dtype),\n            (IdentityDimension(),) + samples.dimensions)\n        stacked[:] = samples\n        wscheme = sr.windowing_scheme(512, 256)\n        tf = stft(stacked, wscheme, HanningWindowingFunc())\n\n        self.assertEqual(10, len(tf))\n        self.assertIsInstance(tf, ArrayWithUnits)\n        self.assertEqual(3, len(tf.dimensions))\n        self.assertIsInstance(tf.dimensions[0], IdentityDimension)\n        self.assertIsInstance(tf.dimensions[1], TimeDimension)\n        self.assertEqual(tf.dimensions[1].samplerate, wscheme)\n        self.assertIsInstance(tf.dimensions[2], FrequencyDimension)\n        self.assertIsInstance(tf.dimensions[2].scale, LinearScale)\n\n    def test_stft_raises_for_invalid_dimensions(self):\n        sr = SR22050()\n        samples = SilenceSynthesizer(sr).synthesize(Milliseconds(6666))\n        wscheme = sr.windowing_scheme(512, 256)\n        tf = stft(samples, wscheme, HanningWindowingFunc())\n        self.assertRaises(\n            ValueError, lambda: stft(tf, wscheme, HanningWindowingFunc()))\n\n\nclass PhaseStretchTests(unittest2.TestCase):\n    def test_can_pitch_shift_audio_samples(self):\n        sr = SR22050()\n        samples = SineSynthesizer(sr).synthesize(Milliseconds(6666), [440])\n        shifted = pitch_shift(samples, 1.0).squeeze()\n        self.assertEqual(len(samples), len(shifted))\n\n    def test_can_pitch_shift_batch(self):\n        sr = SR22050()\n        samples = SilenceSynthesizer(sr).synthesize(Milliseconds(6666))\n        stacked = ArrayWithUnits(\n            np.zeros((10,) + samples.shape, dtype=samples.dtype),\n            (IdentityDimension(),) + samples.dimensions)\n        stacked[:] = samples\n        stretched = pitch_shift(stacked, -2.0)\n        self.assertEqual(10, stretched.shape[0])\n        self.assertEqual(len(samples), stretched.shape[1])\n\n\nclass TimeStretchTests(unittest2.TestCase):\n    def test_can_stretch_audio_samples(self):\n        sr = SR22050()\n        samples = SilenceSynthesizer(sr).synthesize(Milliseconds(1000))\n        stretched = time_stretch(samples, 0.5).squeeze()\n        self.assertEqual(int(2 * len(samples)), len(stretched))\n\n    def test_can_contract_audio_samples(self):\n        sr = SR22050()\n        samples = SilenceSynthesizer(sr).synthesize(Milliseconds(1000))\n        print('First', samples.shape, samples.dimensions)\n        stretched = time_stretch(samples, 2.0).squeeze()\n        print('Second', stretched.shape, stretched.dimensions)\n        self.assertEqual(len(samples) // 2, len(stretched))\n\n    def test_can_stretch_audio_batch(self):\n        sr = SR22050()\n        samples = SilenceSynthesizer(sr).synthesize(Milliseconds(6666))\n        stacked = ArrayWithUnits(\n            np.zeros((10,) + samples.shape, dtype=samples.dtype),\n            (IdentityDimension(),) + samples.dimensions)\n        stacked[:] = samples\n        stretched = time_stretch(stacked, 2.0)\n        self.assertEqual(10, stretched.shape[0])\n        self.assertEqual(int(len(samples) // 2), stretched.shape[1])\n\n\nclass RainbowgramTests(unittest2.TestCase):\n    def test_should_have_correct_shape_and_dimensions(self):\n        samplerate = SR22050()\n        samples = SineSynthesizer(samplerate).synthesize(Milliseconds(8888))\n        wscheme = samplerate.windowing_scheme(256, 128)\n        tf = stft(samples, wscheme, HanningWindowingFunc())\n        rg = rainbowgram(tf, cm.rainbow)\n        self.assertEqual(3, rg.ndim)\n        self.assertEqual(tf.shape + (3,), rg.shape)\n        self.assertEqual(3, len(rg.dimensions))\n        self.assertEqual(tf.dimensions[0], rg.dimensions[0])\n        self.assertEqual(tf.dimensions[1], rg.dimensions[1])\n        self.assertEqual(rg.dimensions[2], IdentityDimension())\n        self.assertEqual(3, rg.shape[-1])\n\n\nclass ApplyScaleTests(unittest2.TestCase):\n    def test_apply_scale_to_self_is_identity_function(self):\n        samplerate = SR22050()\n        samples = SineSynthesizer(samplerate).synthesize(Milliseconds(8888))\n        wscheme = samplerate.windowing_scheme(256, 128)\n        tf = stft(samples, wscheme, HanningWindowingFunc())\n        scale = tf.dimensions[-1].scale\n        transformed = apply_scale(tf, scale, HanningWindowingFunc())\n        self.assertEqual(tf.shape, transformed.shape)\n        self.assertEqual(tf.dimensions[0], transformed.dimensions[0])\n        self.assertEqual(tf.dimensions[1], transformed.dimensions[1])\n\n    def test_has_correct_shape(self):\n        sr = SR22050()\n        samples = SilenceSynthesizer(sr).synthesize(Milliseconds(9999))\n        wscheme = sr.windowing_scheme(256, 128)\n        scale = GeometricScale(50, sr.nyquist, 0.4, 32)\n        scale.ensure_overlap_ratio()\n        tf = stft(samples, wscheme, HanningWindowingFunc())\n        geom = apply_scale(tf, scale, window=HanningWindowingFunc())\n        self.assertEqual(tf.shape[:-1] + (len(scale),), geom.shape)\n\n    def test_has_correct_dimensions(self):\n        sr = SR22050()\n        samples = SilenceSynthesizer(sr).synthesize(Milliseconds(9999))\n        wscheme = sr.windowing_scheme(256, 128)\n        scale = GeometricScale(50, sr.nyquist, 0.4, 32)\n        scale.ensure_overlap_ratio()\n        tf = stft(samples, wscheme, HanningWindowingFunc())\n        geom = apply_scale(tf, scale, window=HanningWindowingFunc())\n        self.assertIsInstance(geom, ArrayWithUnits)\n        self.assertEqual(2, len(geom.dimensions))\n        self.assertIsInstance(geom.dimensions[0], TimeDimension)\n        self.assertEqual(geom.dimensions[0].samplerate, wscheme)\n        self.assertIsInstance(geom.dimensions[1], FrequencyDimension)\n        self.assertEqual(scale, geom.dimensions[1].scale)\n\n    def test_preserves_time_dimension(self):\n        sr = SR22050()\n        samples = TickSynthesizer(sr).synthesize(\n            Milliseconds(10000), Milliseconds(1000))\n        wscheme = sr.windowing_scheme(256, 128)\n        scale = GeometricScale(50, sr.nyquist, 0.4, 32)\n        scale.ensure_overlap_ratio()\n        tf = stft(samples, wscheme, HanningWindowingFunc())\n        geom = apply_scale(tf, scale, window=HanningWindowingFunc())\n\n        # get the loudness envelope of each\n        tf_envelope = np.abs(tf.real).sum(axis=1)\n        geom_envelope = geom.sum(axis=1)\n\n        tf_zeros = np.where(tf_envelope == 0)\n        geom_zeros = np.where(geom_envelope == 0)\n\n        np.testing.assert_allclose(tf_zeros, geom_zeros)\n"""
zounds/spectral/test_sliding_window.py,14,"b'from featureflow import BaseModel\nfrom .sliding_window import \\\n    SlidingWindow, IdentityWindowingFunc, OggVorbisWindowingFunc\nfrom zounds.timeseries import \\\n    AudioSamples, SR22050, SR44100, SR11025, SR48000, SR96000, SampleRate, \\\n    Picoseconds, Seconds, Milliseconds, TimeDimension, HalfLapped, TimeSlice\nfrom zounds.util import simple_in_memory_settings\nfrom zounds.basic import resampled\nfrom zounds.synthesize import NoiseSynthesizer\nfrom zounds.core import ArrayWithUnits\nfrom zounds.persistence import ArrayWithUnitsFeature\nfrom zounds.spectral import FrequencyDimension, LinearScale, FrequencyBand\nfrom zounds.synthesize import SilenceSynthesizer\nimport numpy as np\nimport unittest2\n\n\nclass WindowingFunctionTests(unittest2.TestCase):\n    def test_multiply_lhs(self):\n        samples = np.random.random_sample(44)\n        wf = IdentityWindowingFunc()\n        np.testing.assert_allclose(wf * samples, samples)\n\n    def test_multiply_rhs(self):\n        samples = np.random.random_sample(33)\n        wf = IdentityWindowingFunc()\n        np.testing.assert_allclose(samples * wf, samples)\n\n\nclass OggVorbisWindowingFunctionTests(unittest2.TestCase):\n    def test_multilpy_many_frames(self):\n        samples = np.random.random_sample((10, 3))\n        wf = OggVorbisWindowingFunc()\n        result = wf * samples\n        self.assertEqual(samples.shape, result.shape)\n\n    def test_multiply_one_frame(self):\n        samples = np.random.random_sample((10, 1))\n        wf = OggVorbisWindowingFunc()\n        result = wf * samples\n        self.assertEqual(samples.shape, result.shape)\n\n    def test_multiply_1d(self):\n        samples = np.random.random_sample(10)\n        wf = OggVorbisWindowingFunc()\n        result = wf * samples\n        self.assertEqual(samples.shape, result.shape)\n\n    def test_maintains_dtype(self):\n        samples = np.random.random_sample(10).astype(np.float32)\n        wf = OggVorbisWindowingFunc()\n        result = wf * samples\n        self.assertEqual(np.float32, result.dtype)\n\n\nclass SlidingWindowTests(unittest2.TestCase):\n    def _check(self, samplerate, expected_window_size, expected_step_size):\n        samples = AudioSamples(\n            np.zeros(5 * samplerate.samples_per_second), samplerate)\n        wscheme = samplerate.half_lapped()\n        ws, ss = samples._sliding_window_integer_slices(\n            TimeSlice(wscheme.duration), TimeSlice(wscheme.frequency))\n        self.assertEqual(expected_window_size, ws[0])\n        self.assertEqual(expected_step_size, ss[0])\n\n    def test_correct_window_and_step_size_at_96000(self):\n        self._check(SR96000(), 4096, 2048)\n\n    def test_correct_window_and_step_size_at_48000(self):\n        self._check(SR48000(), 2048, 1024)\n\n    def test_correct_window_and_step_size_at_22050(self):\n        self._check(SR22050(), 1024, 512)\n\n    def test_correct_window_and_step_size_at_44100(self):\n        self._check(SR44100(), 2048, 1024)\n\n    def test_correct_window_and_step_size_at_11025(self):\n        self._check(SR11025(), 512, 256)\n\n    def test_can_apply_sliding_windows_in_succession(self):\n        samplerate = SR11025()\n        short_window = samplerate * (16, 512)\n        long_window = SampleRate(\n            frequency=short_window.frequency * 1,\n            duration=short_window.frequency * 64)\n        rs = resampled(resample_to=samplerate, store_resampled=True)\n\n        samples = AudioSamples.silence(samplerate, Seconds(10))\n\n        @simple_in_memory_settings\n        class Sound(rs):\n            short_windowed = ArrayWithUnitsFeature(\n                SlidingWindow,\n                wscheme=short_window,\n                needs=rs.resampled)\n\n            long_windowed = ArrayWithUnitsFeature(\n                SlidingWindow,\n                wscheme=long_window,\n                needs=short_windowed)\n\n        _id = Sound.process(meta=samples.encode())\n        snd = Sound(_id)\n        self.assertEqual((512,), snd.short_windowed.shape[1:])\n        self.assertEqual((64, 512), snd.long_windowed.shape[1:])\n\n    def test_can_persist_and_retrieve_with_second_long_windowing_scheme(self):\n        samplerate = SR22050()\n        rs = resampled(resample_to=samplerate)\n\n        window_size = Seconds(1)\n        wscheme = SampleRate(window_size, window_size)\n\n        @simple_in_memory_settings\n        class Document(rs):\n            windowed = ArrayWithUnitsFeature(\n                SlidingWindow,\n                wscheme=wscheme,\n                needs=rs.resampled,\n                store=True)\n\n        synth = NoiseSynthesizer(samplerate)\n        audio = synth.synthesize(Milliseconds(5500))\n\n        _id = Document.process(meta=audio.encode())\n        doc = Document(_id)\n\n        self.assertEqual(6, len(doc.windowed))\n\n    def test_has_correct_duration(self):\n        samplerate = SR22050()\n        rs = resampled(resample_to=samplerate)\n\n        @simple_in_memory_settings\n        class Document(rs):\n            windowed = ArrayWithUnitsFeature(\n                SlidingWindow,\n                wscheme=HalfLapped(),\n                needs=rs.resampled,\n                store=True)\n\n        synth = NoiseSynthesizer(samplerate)\n        audio = synth.synthesize(Milliseconds(5500))\n\n        _id = Document.process(meta=audio.encode())\n        doc = Document(_id)\n\n        orig_seconds = audio.dimensions[0].end / Picoseconds(int(1e12))\n        new_seconds = doc.windowed.dimensions[0].end / Picoseconds(int(1e12))\n        self.assertAlmostEqual(orig_seconds, new_seconds, delta=0.01)\n\n    def test_can_apply_sliding_window_to_constant_rate_time_series(self):\n        arr = ArrayWithUnits(np.zeros(100), [TimeDimension(Seconds(1))])\n        sw = SampleRate(Seconds(2), Seconds(2))\n\n        @simple_in_memory_settings\n        class Document(BaseModel):\n            windowed = ArrayWithUnitsFeature(\n                SlidingWindow,\n                wscheme=sw,\n                store=True)\n\n        _id = Document.process(windowed=arr)\n        result = Document(_id).windowed\n\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((50, 2), result.shape)\n        self.assertEqual(2, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], TimeDimension)\n        self.assertEqual(Seconds(2), result.dimensions[0].frequency)\n        self.assertIsInstance(result.dimensions[1], TimeDimension)\n        self.assertEqual(Seconds(1), result.dimensions[1].frequency)\n\n    def test_can_apply_sliding_window_to_time_frequency_representation(self):\n        band = FrequencyBand(0, 22000)\n        scale = LinearScale(band, 100)\n        arr = ArrayWithUnits(\n            np.zeros((200, 100)),\n            [TimeDimension(Seconds(1)), FrequencyDimension(scale)])\n        sw = SampleRate(Seconds(2), Seconds(2))\n\n        @simple_in_memory_settings\n        class Document(BaseModel):\n            windowed = ArrayWithUnitsFeature(\n                SlidingWindow,\n                wscheme=sw,\n                store=True)\n\n        _id = Document.process(windowed=arr)\n        result = Document(_id).windowed\n\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((100, 2, 100), result.shape)\n        self.assertEqual(3, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], TimeDimension)\n        self.assertEqual(Seconds(2), result.dimensions[0].frequency)\n        self.assertIsInstance(result.dimensions[1], TimeDimension)\n        self.assertEqual(Seconds(1), result.dimensions[1].frequency)\n        self.assertIsInstance(result.dimensions[2], FrequencyDimension)\n\n    def test_sliding_window_maintains_dtype(self):\n        band = FrequencyBand(0, 22000)\n        scale = LinearScale(band, 100)\n        arr = ArrayWithUnits(\n            np.zeros((200, 100), dtype=np.uint8),\n            [TimeDimension(Seconds(1)), FrequencyDimension(scale)])\n        sw = SampleRate(Seconds(2), Seconds(2))\n\n        @simple_in_memory_settings\n        class Document(BaseModel):\n            windowed = ArrayWithUnitsFeature(\n                SlidingWindow,\n                wscheme=sw,\n                store=True)\n\n        _id = Document.process(windowed=arr)\n        result = Document(_id).windowed\n        self.assertEqual(np.uint8, result.dtype)\n'"
zounds/spectral/test_spectral.py,19,"b""import numpy as np\nimport scipy\nimport unittest2\n\nfrom .frequencyscale import GeometricScale\nfrom zounds.basic import resampled, stft\nfrom zounds.core import ArrayWithUnits\nfrom zounds.persistence import ArrayWithUnitsFeature, FrequencyAdaptiveFeature\nfrom zounds.spectral import \\\n    SlidingWindow, DCTIV, MDCT, FFT, SpectralCentroid, OggVorbisWindowingFunc, \\\n    SpectralFlatness, FrequencyAdaptiveTransform, DCT, FrequencyAdaptive\nfrom zounds.synthesize import \\\n    SineSynthesizer, DCTIVSynthesizer, MDCTSynthesizer, NoiseSynthesizer, \\\n    TickSynthesizer\nfrom zounds.timeseries import \\\n    SR11025, SR22050, SR44100, Seconds, Milliseconds, Picoseconds, \\\n    AudioSamples, TimeSlice, TimeDimension\nfrom zounds.timeseries.samplerate import SampleRate, HalfLapped\nfrom zounds.util import simple_in_memory_settings\n\n\nclass FrequencyAdaptiveTransformTests(unittest2.TestCase):\n    def test_raises_when_scale_has_insufficient_overlap_and_check_is_requested(\n            self):\n        scale = GeometricScale(\n            start_center_hz=50,\n            stop_center_hz=5000,\n            bandwidth_ratio=0.01,\n            n_bands=128)\n\n        self.assertRaises(ValueError, lambda: FrequencyAdaptiveTransform(\n            transform=np.fft.irfft,\n            scale=scale,\n            check_scale_overlap_ratio=True))\n\n    def test_raises_useful_error_when_unexpected_dimensions_are_received(self):\n        scale = GeometricScale(20, 5000, 0.1, 25)\n\n        transform = FrequencyAdaptiveTransform(\n            transform=scipy.fftpack.idct,\n            scale=scale)\n\n        inp = ArrayWithUnits(\n            np.zeros((100, 10)),\n            dimensions=[\n                TimeDimension(Seconds(1)),\n                TimeDimension(Milliseconds(100))\n            ])\n        self.assertRaises(ValueError, lambda: list(transform._process(inp))[0])\n\n    def test_square_form_with_overlap_add(self):\n        samplerate = SR11025()\n        BaseModel = stft(resample_to=samplerate)\n        windowing_func = OggVorbisWindowingFunc()\n        scale = GeometricScale(20, 5000, 0.1, 25)\n\n        @simple_in_memory_settings\n        class Document(BaseModel):\n            long_windowed = ArrayWithUnitsFeature(\n                SlidingWindow,\n                wscheme=SampleRate(\n                    frequency=Milliseconds(500),\n                    duration=Seconds(1)),\n                wfunc=windowing_func,\n                needs=BaseModel.resampled,\n                store=True)\n\n            dct = ArrayWithUnitsFeature(\n                DCT,\n                scale_always_even=True,\n                needs=long_windowed,\n                store=True)\n\n            mdct = FrequencyAdaptiveFeature(\n                FrequencyAdaptiveTransform,\n                transform=scipy.fftpack.idct,\n                scale=scale,\n                needs=dct,\n                store=True)\n\n        synth = TickSynthesizer(SR22050())\n        samples = synth.synthesize(Seconds(5), Milliseconds(200))\n        _id = Document.process(meta=samples.encode())\n        doc = Document(_id)\n        square = doc.mdct.square(30, do_overlap_add=True)\n        self.assertEqual(2, square.ndim)\n        self.assertEqual(150, square.shape[0])\n        self.assertEqual(25, square.shape[1])\n\n    def test_square_form_no_overlap_add(self):\n        samplerate = SR11025()\n        BaseModel = stft(resample_to=samplerate)\n        windowing_func = OggVorbisWindowingFunc()\n        scale = GeometricScale(20, 5000, 0.1, 25)\n\n        @simple_in_memory_settings\n        class Document(BaseModel):\n            long_windowed = ArrayWithUnitsFeature(\n                SlidingWindow,\n                wscheme=SampleRate(\n                    frequency=Milliseconds(500),\n                    duration=Seconds(1)),\n                wfunc=windowing_func,\n                needs=BaseModel.resampled,\n                store=True)\n\n            dct = ArrayWithUnitsFeature(\n                DCT,\n                scale_always_even=True,\n                needs=long_windowed,\n                store=True)\n\n            mdct = FrequencyAdaptiveFeature(\n                FrequencyAdaptiveTransform,\n                transform=scipy.fftpack.idct,\n                scale=scale,\n                needs=dct,\n                store=True)\n\n        synth = TickSynthesizer(SR22050())\n        samples = synth.synthesize(Seconds(5), Milliseconds(200))\n        _id = Document.process(meta=samples.encode())\n        doc = Document(_id)\n        square = doc.mdct.square(30)\n        self.assertEqual(3, square.ndim)\n        self.assertEqual(30, square.shape[1])\n        self.assertEqual(25, square.shape[2])\n\n\nclass SpectralFlatnessTests(unittest2.TestCase):\n    def setUp(self):\n        self.samplerate = SR22050()\n        rs = resampled(resample_to=self.samplerate)\n\n        wscheme = HalfLapped()\n\n        @simple_in_memory_settings\n        class Document(rs):\n            windowed = ArrayWithUnitsFeature(\n                SlidingWindow,\n                wscheme=wscheme,\n                wfunc=OggVorbisWindowingFunc(),\n                needs=rs.resampled,\n                store=False)\n\n            fft = ArrayWithUnitsFeature(\n                FFT,\n                needs=windowed,\n                store=False)\n\n            flatness = ArrayWithUnitsFeature(\n                SpectralFlatness,\n                needs=fft,\n                store=True)\n\n        # create a pure sine wave that fades out\n        ss = SineSynthesizer(self.samplerate)\n        sine = ss.synthesize(Seconds(5), [440.])\n        sine_envelope = np.linspace(1.0, 0.0, len(sine))\n        sine *= sine_envelope\n\n        # create noise\n        ns = NoiseSynthesizer(self.samplerate)\n        noise = ns.synthesize(Seconds(5))\n        noise_envelope = np.linspace(0.0, 1.0, len(noise))\n        noise *= noise_envelope\n\n        # mix the sine wave and noise together\n        self.audio = sine + noise\n\n        _id = Document.process(meta=self.audio.encode())\n        self.doc = Document(_id)\n\n    def test_has_correct_type(self):\n        self.assertIsInstance(self.doc.flatness, ArrayWithUnits)\n\n    def test_has_correct_duration(self):\n        self.assertAlmostEqual(\n            self.audio.dimensions[0].end_seconds,\n            self.doc.flatness.dimensions[0].end_seconds,\n            delta=0.02)\n\n    def test_has_correct_dimensions(self):\n        self.assertEqual(1, len(self.doc.flatness.dimensions))\n        self.assertIsInstance(self.doc.flatness.dimensions[0], TimeDimension)\n\n    def test_flatness_is_monotonically_increasing(self):\n        chunked = self.doc.flatness \\\n            .sliding_window((TimeSlice(Seconds(1)),)) \\\n            .mean(axis=1)\n        diff = np.diff(chunked)\n        self.assertTrue(np.all(diff >= 0))\n\n\nclass SpectralCentroidTests(unittest2.TestCase):\n    def setUp(self):\n        self.samplerate = SR44100()\n        rs = resampled(resample_to=self.samplerate)\n\n        wscheme = HalfLapped()\n\n        @simple_in_memory_settings\n        class Document(rs):\n            windowed = ArrayWithUnitsFeature(\n                SlidingWindow,\n                wscheme=wscheme,\n                wfunc=OggVorbisWindowingFunc(),\n                needs=rs.resampled,\n                store=False)\n\n            fft = ArrayWithUnitsFeature(\n                FFT,\n                needs=windowed,\n                store=False)\n\n            centroid = ArrayWithUnitsFeature(\n                SpectralCentroid,\n                needs=fft,\n                store=True)\n\n        ss = SineSynthesizer(self.samplerate)\n        chunks = \\\n            [ss.synthesize(Seconds(1), [440 * i]) for i in range(1, 6)]\n        self.audio = \\\n            AudioSamples(ArrayWithUnits.concat(chunks), self.samplerate)\n        _id = Document.process(meta=self.audio.encode())\n        self.doc = Document(_id)\n\n    def test_has_correct_type(self):\n        self.assertIsInstance(self.doc.centroid, ArrayWithUnits)\n\n    def test_has_correct_dimensions(self):\n        self.assertEqual(1, len(self.doc.centroid.dimensions))\n\n    def test_has_correct_duration(self):\n        self.assertAlmostEqual(\n            self.audio.dimensions[0].end_seconds,\n            self.doc.centroid.dimensions[0].end_seconds,\n            delta=0.02)\n\n    def test_centroid_is_monotonically_increasing(self):\n        chunked = self.doc.centroid \\\n            .sliding_window((TimeSlice(Seconds(1)),)) \\\n            .mean(axis=1)\n        diff = np.diff(chunked)\n        self.assertTrue(np.all(diff >= 0))\n\n\nclass MDCTTests(unittest2.TestCase):\n    def setUp(self):\n        self.samplerate = SR11025()\n        rs = resampled(resample_to=self.samplerate)\n\n        wscheme = HalfLapped()\n\n        @simple_in_memory_settings\n        class Document(rs):\n            windowed = ArrayWithUnitsFeature(\n                SlidingWindow,\n                wscheme=wscheme,\n                needs=rs.resampled,\n                store=False)\n\n            mdct = ArrayWithUnitsFeature(\n                MDCT,\n                needs=windowed,\n                store=True)\n\n        ss = SineSynthesizer(self.samplerate)\n        self.audio = ss.synthesize(Seconds(5), [440., 660., 880.])\n\n        _id = Document.process(meta=self.audio.encode())\n        self.doc = Document(_id)\n\n    def test_has_correct_duration(self):\n        self.assertAlmostEqual(\n            self.audio.dimensions[0].end_seconds,\n            self.doc.mdct.dimensions[0].end_seconds,\n            delta=0.02)\n\n    @unittest2.skip(\n        'This test is failing after changes enabling simple slices '\n        'for time and frequency dimensions')\n    def test_perfect_reconstruction_using_overlap_add(self):\n        synth = SineSynthesizer(SR22050())\n        audio = synth.synthesize(Seconds(10), [440., 660., 880.])\n        sr = SampleRate(duration=Seconds(1), frequency=Milliseconds(500))\n        windowed = audio.sliding_window(sr)\n\n        mdct = MDCT()\n\n        coeffs = list(mdct._process(windowed * OggVorbisWindowingFunc()))[0]\n\n        mdct_synth = MDCTSynthesizer()\n        recon = mdct_synth.synthesize(coeffs)\n\n        # take a slice, so we can ignore boundary conditions\n        slce = TimeSlice(start=Seconds(1), duration=Seconds(8))\n\n        np.testing.assert_allclose(recon[slce], audio[slce])\n\n    def test_is_correct_type(self):\n        self.assertIsInstance(self.doc.mdct, ArrayWithUnits)\n\n    def test_has_correct_nyquist_frequency(self):\n        freq_dim = self.doc.mdct.dimensions[1]\n        self.assertEqual(self.samplerate.nyquist, freq_dim.scale.stop_hz)\n\n    def test_reconstruction(self):\n        ds = MDCTSynthesizer()\n        recon = ds.synthesize(self.doc.mdct)\n\n        orig_seconds = self.audio.dimensions[0].end_seconds\n        recon_seconds = recon.dimensions[0].end_seconds\n        self.assertAlmostEqual(orig_seconds, recon_seconds, delta=0.02)\n\n        # ensure that both the original and reconstruction have the exact\n        # same length in samples, so that we can easily compare spectral peaks\n        l = min(len(self.audio), len(recon))\n\n        self.assertEqual(self.audio.samplerate, recon.samplerate)\n        orig_fft = abs(np.fft.rfft(self.audio[:l]))\n        recon_fft = abs(np.fft.rfft(recon[:l]))\n        orig_peaks = set(np.argsort(orig_fft)[-3:])\n        recon_peaks = set(np.argsort(recon_fft)[-3:])\n        # ensure that the original and reconstruction have the same three\n        # spectral peaks\n        self.assertEqual(orig_peaks, recon_peaks)\n\n\nclass DCTIVTests(unittest2.TestCase):\n    def setUp(self):\n        self.samplerate = SR22050()\n        rs = resampled(resample_to=self.samplerate)\n\n        window_size = Picoseconds(int(1e12))\n        wscheme = SampleRate(window_size, window_size)\n\n        @simple_in_memory_settings\n        class Document(rs):\n            windowed = ArrayWithUnitsFeature(\n                SlidingWindow,\n                wscheme=wscheme,\n                needs=rs.resampled,\n                store=False)\n\n            dct = ArrayWithUnitsFeature(\n                DCTIV,\n                needs=windowed,\n                store=True)\n\n        ss = SineSynthesizer(self.samplerate)\n        self.audio = ss.synthesize(Seconds(5), [440., 660., 880.])\n\n        _id = Document.process(meta=self.audio.encode())\n        self.doc = Document(_id)\n\n    @unittest2.skip\n    def test_perfect_reconstruction(self):\n        synth = SineSynthesizer(SR22050())\n        audio = synth.synthesize(Seconds(1), [440., 660., 880.])\n        node = DCTIV()\n        coeffs = node._process_raw(audio[None, :])\n        recon = node._process_raw(coeffs)[0]\n\n        # see? it's do-able w/ dct\n        dct_coeffs = scipy.fftpack.dct(audio[None, :], norm='ortho')\n        inverse_dct = scipy.fftpack.idct(dct_coeffs, norm='ortho')[0]\n\n        np.testing.assert_almost_equal(inverse_dct, audio, decimal=4)\n        np.testing.assert_almost_equal(recon, audio, decimal=4)\n\n    def test_is_correct_type(self):\n        self.assertIsInstance(self.doc.dct, ArrayWithUnits)\n\n    def test_has_correct_nyquist_frequency(self):\n        freq_dim = self.doc.dct.dimensions[-1]\n        self.assertEqual(self.samplerate.nyquist, freq_dim.scale.stop_hz)\n\n    def test_reconstruction(self):\n        ds = DCTIVSynthesizer()\n        recon = ds.synthesize(self.doc.dct)\n\n        orig_seconds = self.audio.dimensions[0].end_seconds\n        recon_seconds = recon.dimensions[0].end_seconds\n        self.assertAlmostEqual(orig_seconds, recon_seconds, delta=0.02)\n\n        # ensure that both the original and reconstruction have the exact\n        # same length in samples, so that we can easily compare spectral peaks\n        l = min(len(self.audio), len(recon))\n\n        self.assertEqual(self.audio.samplerate, recon.samplerate)\n        orig_fft = abs(np.fft.rfft(self.audio[:l]))\n        recon_fft = abs(np.fft.rfft(recon[:l]))\n        orig_peaks = set(np.argsort(orig_fft)[-3:])\n        recon_peaks = set(np.argsort(recon_fft)[-3:])\n        # ensure that the original and reconstruction have the same three\n        # spectral peaks\n        self.assertEqual(orig_peaks, recon_peaks)\n"""
zounds/spectral/test_tfrepresentation.py,41,"b""\nimport numpy as np\nimport unittest2\nfrom .frequencyscale import LinearScale, FrequencyBand, Hertz\nfrom .weighting import AWeighting\nfrom zounds.timeseries import Seconds, TimeDimension, TimeSlice, SR11025\nfrom zounds.spectral import FrequencyDimension, GeometricScale\nfrom zounds.core import ArrayWithUnits, IdentityDimension\n\n\nclass TimeFrequencyRepresentationTests(unittest2.TestCase):\n\n    def test_can_slice_frequency_dim_with_end_hz(self):\n        scale = LinearScale(FrequencyBand(0, 100), 10)\n        arr = ArrayWithUnits(\n            np.zeros((13, 10)),\n            [IdentityDimension(), FrequencyDimension(scale)])\n        sliced = arr[:, :Hertz(50)]\n        self.assertEqual((13, 5), sliced.shape)\n        self.assertEqual(\n            FrequencyDimension(LinearScale(FrequencyBand(0, 50), 5)),\n            sliced.dimensions[-1])\n\n    def test_can_slice_frequency_dim_with_start_hz(self):\n        scale = LinearScale(FrequencyBand(0, 100), 10)\n        arr = ArrayWithUnits(\n            np.zeros((13, 10)),\n            [IdentityDimension(), FrequencyDimension(scale)])\n        sliced = arr[:, Hertz(50):]\n        self.assertEqual((13, 6), sliced.shape)\n\n    def test_can_slice_frequency_dim_with_start_and_end_hz(self):\n        scale = LinearScale(FrequencyBand(0, 100), 10)\n        arr = ArrayWithUnits(\n            np.zeros((13, 10)),\n            [IdentityDimension(), FrequencyDimension(scale)])\n        sliced = arr[:, Hertz(20):Hertz(80)]\n        self.assertEqual((13, 7), sliced.shape)\n\n    def test_can_slice_frequency_dim_with_negative_start_hz(self):\n        scale = LinearScale(FrequencyBand(0, 100), 10)\n        arr = ArrayWithUnits(\n            np.zeros((13, 10)),\n            [IdentityDimension(), FrequencyDimension(scale)])\n        sliced = arr[:, -Hertz(20):]\n        self.assertEqual((13, 3), sliced.shape)\n\n    def test_can_slice_frequency_dim_with_negative_stop_hz(self):\n        scale = LinearScale(FrequencyBand(0, 100), 10)\n        arr = ArrayWithUnits(\n            np.zeros((13, 10)),\n            [IdentityDimension(), FrequencyDimension(scale)])\n        sliced = arr[:, :-Hertz(20)]\n        self.assertEqual((13, 8), sliced.shape)\n        self.assertEqual(\n            FrequencyDimension(LinearScale(FrequencyBand(0, 80), 8)),\n            sliced.dimensions[-1])\n\n    def test_sliding_window_has_correct_dimensions(self):\n        arr = np.random.randint(0, 255, (11025 * 2)).astype(np.int64)\n        sr = SR11025()\n        awu = ArrayWithUnits(arr, [TimeDimension(*sr)])\n        ws = TimeSlice(duration=sr.frequency * 8192)\n        ss = TimeSlice(duration=sr.frequency * 4096)\n        l, x = awu.sliding_window_with_leftovers(ws, ss)\n        self.assertEqual(8192, x.shape[1])\n\n    def test_can_transpose(self):\n        sr = SR11025()\n        hl = sr.half_lapped()\n        scale = GeometricScale(20, sr.nyquist, 0.175, 64)\n        td = TimeDimension(frequency=hl.frequency, duration=hl.duration)\n        fd = FrequencyDimension(scale)\n\n        arr = ArrayWithUnits(np.zeros((99, 64)), [td, fd])\n        transposed = arr.T\n        self.assertEqual((64, 99), transposed.shape)\n        self.assertEqual(arr.dimensions[0], transposed.dimensions[1])\n        self.assertEqual(arr.dimensions[1], transposed.dimensions[0])\n\n    @unittest2.skip('this requires that frequency scales be reversible')\n    def test_can_rotate_90_degrees(self):\n        sr = SR11025()\n        hl = sr.half_lapped()\n        scale = GeometricScale(20, sr.nyquist, 0.175, 64)\n        td = TimeDimension(frequency=hl.frequency, duration=hl.duration)\n        fd = FrequencyDimension(scale)\n\n        arr = ArrayWithUnits(np.zeros((99, 64)), [td, fd])\n        rotated = np.rot90(arr)\n        self.assertEqual((64, 99), rotated.shape)\n        self.assertEqual(arr.dimensions[0], rotated.dimensions[1])\n        self.assertEqual(arr.dimensions[1], rotated.dimensions[0])\n\n    def test_can_apply_sliding_window(self):\n        sr = SR11025()\n        hl = sr.half_lapped()\n        scale = GeometricScale(20, sr.nyquist, 0.175, 64)\n        td = TimeDimension(frequency=hl.frequency, duration=hl.duration)\n        fd = FrequencyDimension(scale)\n\n        arr = ArrayWithUnits(np.zeros((99, 64)), [td, fd])\n\n        ts = TimeSlice(duration=hl.frequency * 64)\n        fs = FrequencyBand(0, sr.nyquist)\n\n        windowed = arr.sliding_window((ts, fs))\n        self.assertEqual((1, 64, 64), windowed.shape)\n\n    def test_can_iterate_over_time_frequency_representation(self):\n        tf = ArrayWithUnits(\n            np.ones((10, 10)),\n            dimensions=[\n                TimeDimension(Seconds(1), Seconds(1)),\n                FrequencyDimension(LinearScale(FrequencyBand(0, 1000), 10))\n            ])\n        rows = [row for row in tf]\n        self.assertEqual(10, len(rows))\n        for row in rows:\n            self.assertIsInstance(row, ArrayWithUnits)\n            self.assertIsInstance(row.dimensions[0], FrequencyDimension)\n            self.assertEqual((10,), row.shape)\n\n    def test_can_iterate_over_timeseries(self):\n        tf = ArrayWithUnits(\n            np.ones((10, 10)),\n            dimensions=[\n                TimeDimension(Seconds(1), Seconds(1)),\n                IdentityDimension()\n            ])\n        rows = [row for row in tf]\n        self.assertEqual(10, len(rows))\n        for row in rows:\n            self.assertIsInstance(row, ArrayWithUnits)\n            self.assertIsInstance(row.dimensions[0], IdentityDimension)\n            self.assertEqual((10,), row.shape)\n\n    def test_can_iterate_after_packbits(self):\n        tf = ArrayWithUnits(\n            np.random.binomial(1, 0.5, (10, 256)).astype(np.uint8),\n            dimensions=[\n                TimeDimension(Seconds(1), Seconds(1)),\n                IdentityDimension()\n            ])\n        tf = tf.packbits(axis=1)\n        self.assertIsInstance(tf, ArrayWithUnits)\n        self.assertEqual((10, 32), tf.shape)\n        self.assertIsInstance(tf.dimensions[0], TimeDimension)\n        self.assertIsInstance(tf.dimensions[1], IdentityDimension)\n        rows = [row for row in tf]\n        self.assertEqual(10, len(rows))\n        for row in rows:\n            self.assertIsInstance(row, ArrayWithUnits)\n            self.assertIsInstance(row.dimensions[0], IdentityDimension)\n            self.assertEqual((32,), row.shape)\n\n    def test_can_use_tuple_indices_for_first_dimension(self):\n        tf = ArrayWithUnits(\n            np.ones((10, 10)),\n            dimensions=[\n                TimeDimension(Seconds(1), Seconds(1)),\n                FrequencyDimension(LinearScale(FrequencyBand(0, 1000), 10))\n            ])\n        subset = tf[tuple([2, 4, 6]), ...]\n        self.assertEqual((3, 10), subset.shape)\n        self.assertIsInstance(subset, ArrayWithUnits)\n        self.assertIsInstance(subset.dimensions[0], TimeDimension)\n        self.assertIsInstance(subset.dimensions[1], FrequencyDimension)\n\n    def test_can_use_tuple_indices_for_first_dimension_id_dim_first(self):\n        tf = ArrayWithUnits(\n            np.ones((10, 9, 8)),\n            dimensions=[\n                IdentityDimension(),\n                TimeDimension(Seconds(1), Seconds(1)),\n                FrequencyDimension(LinearScale(FrequencyBand(0, 1000), 8))\n            ])\n        subset = tf[tuple([2, 4, 6]), ...]\n        self.assertEqual((3, 9, 8), subset.shape)\n        self.assertIsInstance(subset, ArrayWithUnits)\n        self.assertIsInstance(subset.dimensions[0], IdentityDimension)\n        self.assertIsInstance(subset.dimensions[1], TimeDimension)\n        self.assertIsInstance(subset.dimensions[2], FrequencyDimension)\n\n    def test_can_access_int_index_and_frequency_band(self):\n        tf = ArrayWithUnits(\n            np.ones((10, 10)),\n            dimensions=[\n                TimeDimension(Seconds(1), Seconds(1)),\n                FrequencyDimension(LinearScale(FrequencyBand(0, 1000), 10))\n            ])\n        sliced = tf[0, FrequencyBand(201, 400)]\n        self.assertEqual((2,), sliced.shape)\n        self.assertIsInstance(sliced.dimensions[0], FrequencyDimension)\n\n    def test_can_access_time_slice_and_int_index(self):\n        tf = ArrayWithUnits(\n            np.ones((10, 10)),\n            dimensions=[\n                TimeDimension(Seconds(1), Seconds(1)),\n                FrequencyDimension(LinearScale(FrequencyBand(0, 1000), 10))\n            ])\n        sliced = tf[TimeSlice(start=Seconds(1), duration=Seconds(2)), 0]\n        self.assertEqual((2,), sliced.shape)\n        self.assertIsInstance(sliced.dimensions[0], TimeDimension)\n\n    def test_can_add_axis_at_end(self):\n        _id = IdentityDimension()\n        td = TimeDimension(Seconds(1), Seconds(1))\n        fd = FrequencyDimension(LinearScale(FrequencyBand(20, 22050), 100))\n        tf = ArrayWithUnits(np.ones((3, 30, 100)), [_id, td, fd])\n        tf2 = tf[..., None]\n        self.assertEqual(4, tf2.ndim)\n        self.assertIsInstance(tf2.dimensions[0], IdentityDimension)\n        self.assertIsInstance(tf2.dimensions[1], TimeDimension)\n        self.assertIsInstance(tf2.dimensions[2], FrequencyDimension)\n        self.assertIsInstance(tf2.dimensions[3], IdentityDimension)\n\n    def test_sum_along_frequency_axis(self):\n        td = TimeDimension(Seconds(1), Seconds(1))\n        fd = FrequencyDimension(LinearScale(FrequencyBand(20, 22050), 100))\n        tf = ArrayWithUnits(np.ones((30, 100)), [td, fd])\n        result = tf.sum(axis=1)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertEqual((30,), result.shape)\n        self.assertIsInstance(result.dimensions[0], TimeDimension)\n\n    def test_can_use_negative_axis_indices(self):\n        td = TimeDimension(Seconds(1), Seconds(1))\n        fd = FrequencyDimension(LinearScale(FrequencyBand(20, 22050), 100))\n        tf = ArrayWithUnits(np.ones((30, 100)), [td, fd])\n        result = tf.sum(axis=-1)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertEqual((30,), result.shape)\n        self.assertIsInstance(result.dimensions[0], TimeDimension)\n\n    def test_can_use_keepdims_with_sum(self):\n        td = TimeDimension(Seconds(1), Seconds(1))\n        fd = FrequencyDimension(LinearScale(FrequencyBand(20, 22050), 100))\n        tf = ArrayWithUnits(np.ones((30, 100)), [td, fd])\n        result = tf.sum(axis=-1, keepdims=True)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(2, len(result.dimensions))\n        self.assertEqual((30, 1), result.shape)\n        self.assertIsInstance(result.dimensions[0], TimeDimension)\n        self.assertIsInstance(result.dimensions[1], IdentityDimension)\n\n    def test_can_use_negative_axis_indices_max(self):\n        td = TimeDimension(Seconds(1), Seconds(1))\n        fd = FrequencyDimension(LinearScale(FrequencyBand(20, 22050), 100))\n        tf = ArrayWithUnits(np.ones((30, 100)), [td, fd])\n        result = tf.max(axis=-1)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertEqual((30,), result.shape)\n        self.assertIsInstance(result.dimensions[0], TimeDimension)\n\n    def test_from_example(self):\n        td = TimeDimension(Seconds(1), Seconds(1))\n        fd = FrequencyDimension(LinearScale(FrequencyBand(20, 22050), 100))\n        tf = ArrayWithUnits(np.ones((30, 100)), [td, fd])\n        from_example = ArrayWithUnits.from_example(np.ones((30, 100)), tf)\n        self.assertEqual(tf.shape, from_example.shape)\n        self.assertSequenceEqual(tf.dimensions, from_example.dimensions)\n\n    def test_can_multiply_by_frequency_weighting_linear_scale(self):\n        frequency = Seconds(1)\n        duration = Seconds(1)\n        scale = LinearScale(FrequencyBand(20, 22050), 100)\n\n        td = TimeDimension(frequency, duration)\n        fd = FrequencyDimension(scale)\n\n        tf = ArrayWithUnits(np.ones((30, 100)), [td, fd])\n        result = tf * AWeighting()\n        self.assertIsInstance(result, ArrayWithUnits)\n        peak_frequency_band = FrequencyBand(9000, 11000)\n        lower_band = FrequencyBand(100, 300)\n        peak_slice = np.abs(result[:, peak_frequency_band]).max()\n        lower_slice = np.abs(result[:, lower_band]).max()\n        self.assertGreater(peak_slice, lower_slice)\n\n    def test_can_multiply_by_frequency_weighting_log_scale(self):\n        frequency = Seconds(1)\n        duration = Seconds(1)\n        scale = GeometricScale(20, 22050, 0.01, 100)\n\n        td = TimeDimension(frequency, duration)\n        fd = FrequencyDimension(scale)\n\n        tf = ArrayWithUnits(np.ones((30, 100)), [td, fd])\n\n        result = tf * AWeighting()\n        self.assertIsInstance(result, ArrayWithUnits)\n        peak_frequency_band = FrequencyBand(9000, 11000)\n        lower_band = FrequencyBand(100, 300)\n        peak_slice = np.abs(result[:, peak_frequency_band]).max()\n        lower_slice = np.abs(result[:, lower_band]).max()\n        self.assertGreater(peak_slice, lower_slice)\n\n    def test_can_multiply_by_array(self):\n        frequency = Seconds(1)\n        duration = Seconds(1)\n        scale = LinearScale(FrequencyBand(20, 22050), 100)\n        td = TimeDimension(frequency, duration)\n        fd = FrequencyDimension(scale)\n        tf = ArrayWithUnits(np.ones((30, 100)), [td, fd])\n        result = tf * np.ones(100)\n        self.assertIsInstance(result, ArrayWithUnits)\n        np.testing.assert_allclose(tf, result)\n\n    def test_can_use_list_of_integers_as_index(self):\n        frequency = Seconds(1)\n        duration = Seconds(1)\n        scale = LinearScale(FrequencyBand(20, 22050), 100)\n\n        td = TimeDimension(frequency, duration)\n        fd = FrequencyDimension(scale)\n\n        tf = ArrayWithUnits(np.zeros((30, 100)), [td, fd])\n\n        indexed = tf[[0, 10, 14]]\n        self.assertEqual((3, 100), indexed.shape)\n        self.assertIsInstance(indexed, ArrayWithUnits)\n        self.assertIsInstance(indexed.dimensions[0], IdentityDimension)\n        self.assertIsInstance(indexed.dimensions[1], FrequencyDimension)\n\n    def test_can_construct_instance(self):\n        frequency = Seconds(1)\n        duration = Seconds(1)\n        scale = LinearScale(FrequencyBand(20, 22050), 100)\n        td = TimeDimension(frequency, duration)\n        fd = FrequencyDimension(scale)\n        tf = ArrayWithUnits(np.zeros((30, 100)), [td, fd])\n        self.assertIsInstance(tf, ArrayWithUnits)\n\n    def test_raises_if_scale_length_does_not_match_frequency_dimension(self):\n        frequency = Seconds(1)\n        duration = Seconds(1)\n        scale = LinearScale(FrequencyBand(20, 22050), 1000)\n\n        td = TimeDimension(frequency, duration)\n        fd = FrequencyDimension(scale)\n\n        self.assertRaises(\n            ValueError,\n            lambda: ArrayWithUnits(np.ones((30, 100)), [td, fd]))\n\n    def test_can_slice_frequency_dimension_with_integer_indices(self):\n        frequency = Seconds(1)\n        duration = Seconds(1)\n        scale = LinearScale(FrequencyBand(20, 22050), 100)\n        td = TimeDimension(frequency, duration)\n        fd = FrequencyDimension(scale)\n        tf = ArrayWithUnits(np.zeros((30, 100)), [td, fd])\n        sliced = tf[:, 10: 20]\n        self.assertEqual((30, 10), sliced.shape)\n        self.assertIsInstance(sliced, ArrayWithUnits)\n\n    def test_can_slice_frequency_dimensions_with_frequency_band(self):\n        frequency = Seconds(1)\n        duration = Seconds(1)\n        scale = LinearScale(FrequencyBand(20, 22050), 100)\n        td = TimeDimension(frequency, duration)\n        fd = FrequencyDimension(scale)\n        tf = ArrayWithUnits(np.zeros((30, 100)), [td, fd])\n        bands = list(scale)\n        sliced = tf[:, bands[0]]\n        self.assertEqual((30, 1), sliced.shape)\n        self.assertIsInstance(sliced, ArrayWithUnits)\n\n    def test_can_slice_freq_dimension_with_freq_band_spanning_bins(self):\n        frequency = Seconds(1)\n        duration = Seconds(1)\n        scale = LinearScale(FrequencyBand(20, 22050), 100)\n        td = TimeDimension(frequency, duration)\n        fd = FrequencyDimension(scale)\n        tf = ArrayWithUnits(np.zeros((30, 100)), [td, fd])\n        bands = list(scale)\n        wide_band = FrequencyBand(bands[0].start_hz, bands[9].stop_hz)\n        sliced = tf[:, wide_band]\n        self.assertEqual((30, 10), sliced.shape)\n        self.assertIsInstance(sliced, ArrayWithUnits)\n\n    def test_scale_is_modified_after_slice(self):\n        frequency = Seconds(1)\n        duration = Seconds(1)\n        scale = LinearScale(FrequencyBand(20, 22050), 100)\n        td = TimeDimension(frequency, duration)\n        fd = FrequencyDimension(scale)\n        tf = ArrayWithUnits(np.zeros((30, 100)), [td, fd])\n        bands = list(scale)\n        wide_band = FrequencyBand(bands[0].start_hz, bands[9].stop_hz)\n        sliced = tf[:, wide_band]\n        self.assertEqual((30, 10), sliced.shape)\n        self.assertIsInstance(sliced, ArrayWithUnits)\n        self.assertLess(sliced.dimensions[1].scale.stop_hz, scale.stop_hz)\n        self.assertEqual(10, sliced.dimensions[1].scale.n_bands)\n\n    def test_ellipsis(self):\n        scale = LinearScale(FrequencyBand(0, 10000), 100)\n        arr = ArrayWithUnits(\n            np.zeros((10, 3, 100)),\n            [IdentityDimension(),\n             TimeDimension(Seconds(1)),\n             FrequencyDimension(scale)])\n        sliced = arr[..., FrequencyBand(1000, 5000)]\n        self.assertEqual((10, 3, 41), sliced.shape)\n        self.assertIsInstance(sliced.dimensions[0], IdentityDimension)\n        self.assertIsInstance(sliced.dimensions[1], TimeDimension)\n        self.assertIsInstance(sliced.dimensions[2], FrequencyDimension)\n"""
zounds/spectral/test_weighting.py,14,"b'import unittest2\nimport numpy as np\nfrom .weighting import AWeighting\nfrom .frequencyscale import LinearScale, FrequencyBand, GeometricScale, MelScale\nfrom .tfrepresentation import FrequencyDimension\nfrom .frequencyadaptive import FrequencyAdaptive\nfrom zounds.timeseries import Seconds, TimeDimension, Milliseconds, SR11025\nfrom zounds.core import ArrayWithUnits, IdentityDimension\nfrom .functional import fir_filter_bank\n\n\nclass WeightingTests(unittest2.TestCase):\n    def test_cannot_multiply_when_array_does_not_have_expected_dimensions(self):\n        td = TimeDimension(Seconds(1), Seconds(1))\n        tf = ArrayWithUnits(np.ones((90, 100)), [td, IdentityDimension()])\n        weighting = AWeighting()\n        self.assertRaises(ValueError, lambda: tf * weighting)\n\n    def test_can_get_weights_from_tf_representation(self):\n        td = TimeDimension(Seconds(1), Seconds(1))\n        fd = FrequencyDimension(LinearScale(FrequencyBand(20, 22050), 100))\n        tf = ArrayWithUnits(np.ones((90, 100)), [td, fd])\n        weighting = AWeighting()\n        weights = weighting.weights(tf)\n        self.assertEqual((100,), weights.shape)\n\n    def test_can_get_weights_from_scale(self):\n        scale = LinearScale(FrequencyBand(20, 22050), 100)\n        weighting = AWeighting()\n        weights = weighting.weights(scale)\n        self.assertEqual((100,), weights.shape)\n\n    def test_can_apply_a_weighting_to_time_frequency_representation(self):\n        td = TimeDimension(Seconds(1), Seconds(1))\n        fd = FrequencyDimension(LinearScale(FrequencyBand(20, 22050), 100))\n        tf = ArrayWithUnits(np.ones((90, 100)), [td, fd])\n        weighting = AWeighting()\n        result = tf * weighting\n        self.assertGreater(result[0, -1], result[0, 0])\n\n    def test_can_apply_a_weighting_to_frequency_adaptive_representation(self):\n        td = TimeDimension(\n            duration=Seconds(1),\n            frequency=Milliseconds(500))\n        scale = GeometricScale(20, 5000, 0.05, 120)\n        arrs = [np.ones((10, x)) for x in range(1, 121)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n        weighting = AWeighting()\n        result = fa * weighting\n        self.assertGreater(\n            result[:, scale[-1]].sum(), result[:, scale[0]].sum())\n\n    def test_can_invert_frequency_weighting(self):\n        td = TimeDimension(Seconds(1), Seconds(1))\n        fd = FrequencyDimension(LinearScale(FrequencyBand(20, 22050), 100))\n        tf = ArrayWithUnits(np.random.random_sample((90, 100)), [td, fd])\n        weighted = tf * AWeighting()\n        inverted = weighted / AWeighting()\n        np.testing.assert_allclose(tf, inverted)\n\n    def test_can_invert_frequency_weighting_for_adaptive_representation(self):\n        td = TimeDimension(\n            duration=Seconds(1),\n            frequency=Milliseconds(500))\n        scale = GeometricScale(20, 5000, 0.05, 120)\n        arrs = [np.random.random_sample((10, x)) for x in range(1, 121)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n        weighting = AWeighting()\n        result = fa * weighting\n        inverted = result / AWeighting()\n        np.testing.assert_allclose(fa, inverted)\n\n    def test_can_apply_weighting_to_explicit_frequency_dimension(self):\n        td = TimeDimension(\n            duration=Seconds(1),\n            frequency=Milliseconds(500))\n        scale = GeometricScale(20, 5000, 0.05, 120)\n        arrs = [np.ones((10, x)) for x in range(1, 121)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n        fa2 = ArrayWithUnits(fa, fa.dimensions)\n        weighting = AWeighting()\n        result = fa2 * weighting\n        self.assertGreater(\n            result[:, scale[-1]].sum(), result[:, scale[0]].sum())\n\n    def test_can_invert_weighting_for_explicit_frequency_dimension(self):\n        td = TimeDimension(\n            duration=Seconds(1),\n            frequency=Milliseconds(500))\n        scale = GeometricScale(20, 5000, 0.05, 120)\n        arrs = [np.ones((10, x)) for x in range(1, 121)]\n        fa = FrequencyAdaptive(arrs, td, scale)\n        fa2 = ArrayWithUnits(fa, fa.dimensions)\n        weighting = AWeighting()\n        result = fa2 * weighting\n        inverted = result / AWeighting()\n        np.testing.assert_allclose(fa, inverted)\n\n    def test_can_apply_weighting_to_filter_bank(self):\n        sr = SR11025()\n        band = FrequencyBand(20, sr.nyquist)\n        scale = MelScale(band, 100)\n        bank = fir_filter_bank(scale, 256, sr, np.hanning(25))\n        weighted = bank * AWeighting()\n        self.assertSequenceEqual(bank.dimensions, weighted.dimensions)\n\n    def test_multiplication_by_weighting_is_commutative(self):\n        sr = SR11025()\n        band = FrequencyBand(20, sr.nyquist)\n        scale = MelScale(band, 100)\n        bank = fir_filter_bank(scale, 256, sr, np.hanning(25))\n        np.testing.assert_allclose(bank * AWeighting(), AWeighting() * bank)\n'"
zounds/spectral/tfrepresentation.py,2,"b'import numpy as np\nfrom . import frequencyscale\nfrom .frequencyscale import Hertz\nfrom zounds.core import Dimension\nfrom zounds.spectral.frequencyscale import ExplicitScale\n\n\nclass FrequencyDimension(Dimension):\n    """"""\n    When applied to an axis of :class:`~zounds.core.ArrayWithUnits`, that axis\n    can be viewed as representing the energy present in a series of frequency\n    bands\n\n    Args:\n        scale (FrequencyScale): A scale whose frequency bands correspond to the\n            items along the frequency axis\n\n    Examples:\n        >>> from zounds import LinearScale, FrequencyBand, ArrayWithUnits\n        >>> from zounds import FrequencyDimension\n        >>> import numpy as np\n        >>> band = FrequencyBand(20, 20000)\n        >>> scale = LinearScale(frequency_band=band, n_bands=100)\n        >>> raw = np.hanning(100)\n        >>> arr = ArrayWithUnits(raw, [FrequencyDimension(scale)])\n        >>> sliced = arr[FrequencyBand(100, 1000)]\n        >>> sliced.shape\n        (5,)\n        >>> sliced.dimensions\n        (FrequencyDimension(scale=LinearScale(band=FrequencyBand(\n        start_hz=20.0,\n        stop_hz=1019.0,\n        center=519.5,\n        bandwidth=999.0), n_bands=5)),)\n    """"""\n    def __init__(self, scale):\n        super(FrequencyDimension, self).__init__()\n        self.scale = scale\n\n    def copy(self):\n        return FrequencyDimension(self.scale)\n\n    def weights(self, weights, arr, i):\n        shape = [1] * arr.ndim\n        shape[i] = weights.size\n        return weights.reshape(shape)\n\n    def modified_dimension(self, size, windowsize, stepsize=None):\n        raise NotImplementedError()\n\n    def metaslice(self, index, size):\n        return FrequencyDimension(self.scale[index])\n\n    def integer_based_slice(self, index):\n        return self.scale.get_slice(index)\n\n    def validate(self, size):\n        """"""\n        Ensure that the size of the dimension matches the number of bands in the\n        scale\n\n        Raises:\n             ValueError: when the dimension size and number of bands don\'t match\n        """"""\n        msg = \'scale and array size must match, \' \\\n              \'but were scale: {self.scale.n_bands},  array size: {size}\'\n\n        if size != len(self.scale):\n            raise ValueError(msg.format(**locals()))\n\n    def __eq__(self, other):\n        return self.scale == other.scale\n\n    def __str__(self):\n        return \'FrequencyDimension(scale={self.scale})\'.format(**locals())\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass ExplicitFrequencyDimension(Dimension):\n    """"""\n    A frequency dimension where the mapping from frequency bands to integer\n    indices is provided explicitly, rather than computed\n\n    Args:\n        scale (ExplicitScale): the explicit frequency scale that defines how\n            slices are extracted from this dimension\n        slices (iterable of slices): An iterable of :class:`python.slice`\n            instances which correspond to each frequency band from scale\n\n    Raises:\n        ValueError: when the number of slices and number of bands in scale don\'t\n            match\n    """"""\n\n    def __init__(self, scale, slices):\n        super(ExplicitFrequencyDimension, self).__init__()\n        if len(scale) != len(slices):\n            raise ValueError(\'scale and slices must have same length\')\n\n        self.scale = scale\n        self.slices = slices\n        self._lookup = dict(list(zip(self.scale, self.slices)))\n\n    def copy(self):\n        return ExplicitFrequencyDimension(self.scale, self.slices)\n\n    def weights(self, weights, arr, i):\n        w = np.zeros(self.slices[-1].stop - self.slices[0].start)\n        for weight, sl in zip(weights, self.slices):\n            w[sl] = weight\n        return w\n\n    def modified_dimension(self, size, windowsize, stepsize=None):\n        raise NotImplementedError()\n\n    def metaslice(self, index, size):\n        if isinstance(index, frequencyscale.FrequencyBand):\n            try:\n                slce = self._lookup[index]\n                return ExplicitFrequencyDimension(\n                    ExplicitScale([index]), [slce])\n            except KeyError:\n                slce = self.scale.get_slice(index)\n        else:\n            slce = index\n\n        print(slce)\n        return ExplicitFrequencyDimension(self.scale[slce], self.slices[slce])\n\n    def integer_based_slice(self, index):\n        if not isinstance(index, frequencyscale.FrequencyBand):\n            return index\n\n        try:\n            return self._lookup[index]\n        except KeyError:\n            pass\n\n        slce = self.scale.get_slice(index)\n        slices = self.slices[slce]\n        return slice(slices[0].start, slices[-1].stop)\n\n    def validate(self, size):\n        return size == self.slices[-1].stop\n\n    def __eq__(self, other):\n        return self.scale == other.scale and self.slices == other.slices\n\n    def __str__(self):\n        return \\\n            \'ExplicitFrequencyDimension(scale={self.scale}, slices=self.slices)\' \\\n                .format(**locals())\n\n    def __repr__(self):\n        return self.__str__()\n'"
zounds/spectral/weighting.py,7,"b'import numpy as np\nfrom .frequencyadaptive import FrequencyAdaptive\n\n\nclass FrequencyWeighting(object):\n    def __init__(self):\n        super(FrequencyWeighting, self).__init__()\n\n    def __numpy_ufunc__(self, *args, **kwargs):\n        raise NotImplementedError()\n\n    def _wdata(self, scale):\n        return np.ones(len(scale))\n\n    def weights(self, other):\n        """"""\n        Compute weights, given a scale or time-frequency representation\n        :param other: A time-frequency representation, or a scale\n        :return: a numpy array of weights\n        """"""\n        try:\n            return self._wdata(other)\n        except AttributeError:\n            frequency_dim = other.dimensions[-1]\n            return self._wdata(frequency_dim.scale)\n\n    def _get_factors(self, arr):\n        for i, d in enumerate(arr.dimensions):\n            try:\n                weights = self._wdata(d.scale)\n                expanded = d.weights(weights, arr, i)\n                return expanded\n            except AttributeError as e:\n                pass\n\n        raise ValueError(\'arr must have a frequency dimension\')\n\n    def __array_ufunc__(self, ufunc, method, *args, **kwargs):\n        if ufunc == np.multiply or ufunc == np.divide:\n            if args[0] is self:\n                first_arg = self._get_factors(args[1])\n                second_arg = args[1]\n            else:\n                first_arg = args[0]\n                second_arg = self._get_factors(args[0])\n            return getattr(ufunc, method)(first_arg, second_arg, **kwargs)\n        else:\n            return NotImplemented\n\n\nclass AWeighting(FrequencyWeighting):\n    """"""\n    An A-weighting (https://en.wikipedia.org/wiki/A-weighting) that can be\n    applied to a frequency axis via multiplication.\n\n    Examples:\n        >>> from zounds import ArrayWithUnits, GeometricScale\n        >>> from zounds import FrequencyDimension, AWeighting\n        >>> import numpy as np\n        >>> scale = GeometricScale(20, 20000, 0.05, 10)\n        >>> raw = np.ones(len(scale))\n        >>> arr = ArrayWithUnits(raw, [FrequencyDimension(scale)])\n        >>> arr * AWeighting()\n        ArrayWithUnits([  1.        ,  18.3172567 ,  31.19918106,  40.54760374,\n                47.15389876,  51.1554151 ,  52.59655479,  52.24516649,\n                49.39906912,  42.05409205])\n    """"""\n\n    def __init__(self):\n        super(AWeighting, self).__init__()\n\n    def _wdata(self, scale):\n        center_frequencies = np.array(list(scale.center_frequencies)) ** 2\n        a = (12200 ** 2) * (center_frequencies ** 2)\n        b = center_frequencies + (20.6 ** 2)\n        c = center_frequencies + (107.7 ** 2)\n        d = center_frequencies + (737.9 ** 2)\n        e = center_frequencies + (12200 ** 2)\n        f = a / (b * np.sqrt(c * d) * e)\n        result = 2.0 + (20 * np.log10(f))\n        return 1 + (result - np.min(result))\n'"
zounds/synthesize/__init__.py,0,"b'""""""\nThe `synthesize` module includes classes that can produce audio.  Some, like\n:class:`SineSynthesize` can produce simple signals from scratch that are often\nuseful for test-cases, while others are able to invert common frequency-domain\ntransforms, like the :class:`MDCTSynthesizer`\n""""""\n\nfrom .synthesize import \\\n    FFTSynthesizer, DCTSynthesizer, TickSynthesizer, NoiseSynthesizer, \\\n    SineSynthesizer, DCTIVSynthesizer, MDCTSynthesizer, \\\n    FrequencyAdaptiveFFTSynthesizer, FrequencyAdaptiveDCTSynthesizer, \\\n    SilenceSynthesizer, WindowedAudioSynthesizer, \\\n    FrequencyDecompositionSynthesizer\n'"
zounds/synthesize/synthesize.py,25,"b'\n\nimport numpy as np\nfrom scipy.fftpack import dct, idct\nfrom scipy.signal import resample\n\nfrom zounds.core import ArrayWithUnits, IdentityDimension\nfrom zounds.spectral import DCTIV, LinearScale\nfrom zounds.spectral import FrequencyDimension\nfrom zounds.spectral.sliding_window import \\\n    IdentityWindowingFunc, OggVorbisWindowingFunc\nfrom zounds.timeseries import \\\n    nearest_audio_sample_rate, Seconds, AudioSamples, TimeDimension\n\n\nclass ShortTimeTransformSynthesizer(object):\n    def __init__(self):\n        super(ShortTimeTransformSynthesizer, self).__init__()\n\n    def _transform(self, frames):\n        return frames\n\n    def _windowing_function(self):\n        return IdentityWindowingFunc()\n\n    def _overlap_add(self, frames):\n        time_dim = frames.dimensions[0]\n        sample_freq = time_dim.duration / frames.shape[-1]\n        windowsize = int(np.round(time_dim.duration / sample_freq))\n        hopsize = int(np.round(time_dim.frequency / sample_freq))\n\n        # create an empty array of audio samples\n        arr = np.zeros(int(time_dim.end / sample_freq))\n        windowed_frames = self._windowing_function() * frames\n\n        for i, f in enumerate(windowed_frames):\n            start = i * hopsize\n            stop = start + windowsize\n            l = len(arr[start:stop])\n            arr[start:stop] += f[:l]\n\n        sr = nearest_audio_sample_rate(Seconds(1) / sample_freq)\n        return AudioSamples(arr, sr)\n\n    def synthesize(self, frames):\n        audio = self._transform(frames)\n        ts = ArrayWithUnits(audio, [frames.dimensions[0], IdentityDimension()])\n        return self._overlap_add(ts)\n\n\nclass WindowedAudioSynthesizer(ShortTimeTransformSynthesizer):\n    def __init__(self):\n        super(WindowedAudioSynthesizer, self).__init__()\n\n\nclass FFTSynthesizer(ShortTimeTransformSynthesizer):\n    """"""\n    Inverts the short-time fourier transform, e.g. the output of the\n    :class:`~zounds.spectral.FFT` processing node.\n\n    Here\'s an example that extracts a short-time fourier transform, and then\n    inverts it.\n\n    .. code:: python\n\n        import zounds\n\n        STFT = zounds.stft(\n            resample_to=zounds.SR11025(),\n            store_fft=True)\n\n\n        @zounds.simple_in_memory_settings\n        class Sound(STFT):\n            pass\n\n        # produce some additive sine waves\n        sine_synth = zounds.SineSynthesizer(zounds.SR22050())\n        samples = sine_synth.synthesize(\n            zounds.Seconds(4), freqs_in_hz=[220, 400, 880])\n\n        # process the sound, including a short-time fourier transform feature\n        _id = Sound.process(meta=samples.encode())\n        snd = Sound(_id)\n\n        # invert the frequency-domain feature to reover the original audio\n        fft_synth = zounds.FFTSynthesizer()\n        recon = fft_synth.synthesize(snd.fft)\n        print recon.__class__  #  AudioSamples instance with reconstructed audio\n\n    See Also:\n        :class:`~zounds.spectral.FFT`\n    """"""\n\n    def __init__(self):\n        super(FFTSynthesizer, self).__init__()\n\n    def _windowing_function(self):\n        return OggVorbisWindowingFunc()\n\n    def _transform(self, frames):\n        return np.fft.irfft(frames, norm=\'ortho\')\n\n\nclass DCTSynthesizer(ShortTimeTransformSynthesizer):\n    """"""\n    Inverts the short-time discrete cosine transform (type II), e.g., the output\n    of the :class:`~zounds.spectral.DCT` processing node\n\n    Here\'s an example that extracts a short-time discrete cosine transform, and\n    then inverts it.\n\n    .. code:: python\n\n        import zounds\n\n        Resampled = zounds.resampled(resample_to=zounds.SR11025())\n\n\n        @zounds.simple_in_memory_settings\n        class Sound(Resampled):\n            windowed = zounds.ArrayWithUnitsFeature(\n                zounds.SlidingWindow,\n                needs=Resampled.resampled,\n                wscheme=zounds.HalfLapped(),\n                wfunc=zounds.OggVorbisWindowingFunc(),\n                store=False)\n\n            dct = zounds.ArrayWithUnitsFeature(\n                zounds.DCT,\n                needs=windowed,\n                store=True)\n\n        # produce some additive sine waves\n        sine_synth = zounds.SineSynthesizer(zounds.SR22050())\n        samples = sine_synth.synthesize(\n            zounds.Seconds(4), freqs_in_hz=[220, 400, 880])\n\n        # process the sound, including a short-time fourier transform feature\n        _id = Sound.process(meta=samples.encode())\n        snd = Sound(_id)\n\n        # invert the frequency-domain feature to reover the original audio\n        dct_synth = zounds.DCTSynthesizer()\n        recon = dct_synth.synthesize(snd.dct)\n        print recon.__class__  # AudioSamples instance with reconstructed audio\n\n    See Also:\n        :class:`~zounds.spectral.DCT`\n    """"""\n\n    def __init__(self, windowing_func=IdentityWindowingFunc()):\n        super(DCTSynthesizer, self).__init__()\n        self.windowing_func = windowing_func\n\n    def _windowing_function(self):\n        return self.windowing_func\n\n    def _transform(self, frames):\n        return idct(frames, norm=\'ortho\')\n\n\nclass DCTIVSynthesizer(ShortTimeTransformSynthesizer):\n    """"""\n    Inverts the short-time discrete cosine transform (type IV), e.g., the output\n    of the :class:`~zounds.spectral.DCTIV` processing node.\n\n    Here\'s an example that extracts a short-time DCT-IV transform, and inverts\n    it.\n\n    .. code:: python\n\n        import zounds\n\n        Resampled = zounds.resampled(resample_to=zounds.SR11025())\n\n\n        @zounds.simple_in_memory_settings\n        class Sound(Resampled):\n            windowed = zounds.ArrayWithUnitsFeature(\n                zounds.SlidingWindow,\n                needs=Resampled.resampled,\n                wscheme=zounds.HalfLapped(),\n                wfunc=zounds.OggVorbisWindowingFunc(),\n                store=False)\n\n            dct = zounds.ArrayWithUnitsFeature(\n                zounds.DCTIV,\n                needs=windowed,\n                store=True)\n\n        # produce some additive sine waves\n        sine_synth = zounds.SineSynthesizer(zounds.SR22050())\n        samples = sine_synth.synthesize(\n            zounds.Seconds(4), freqs_in_hz=[220, 400, 880])\n\n        # process the sound, including a short-time fourier transform feature\n        _id = Sound.process(meta=samples.encode())\n        snd = Sound(_id)\n\n        # invert the frequency-domain feature to reover the original audio\n        dct_synth = zounds.DCTIVSynthesizer()\n        recon = dct_synth.synthesize(snd.dct)\n        print recon.__class__  # AudioSamples instance with reconstructed audio\n\n    See Also:\n        :class:`~zounds.spectral.DCTIV`\n    """"""\n\n    def __init__(self, windowing_func=IdentityWindowingFunc()):\n        super(DCTIVSynthesizer, self).__init__()\n        self.windowing_func = windowing_func\n\n    def _windowing_function(self):\n        return self.windowing_func\n\n    def _transform(self, frames):\n        return list(DCTIV()._process(frames))[0]\n\n\nclass MDCTSynthesizer(ShortTimeTransformSynthesizer):\n    """"""\n    Inverts the modified discrete cosine transform, e.g., the output of the\n    :class:`~zounds.spectral.MDCT` processing node.\n\n    Here\'s an example that extracts a short-time MDCT transform, and inverts\n    it.\n\n    .. code:: python\n\n        import zounds\n\n        Resampled = zounds.resampled(resample_to=zounds.SR11025())\n\n\n        @zounds.simple_in_memory_settings\n        class Sound(Resampled):\n            windowed = zounds.ArrayWithUnitsFeature(\n                zounds.SlidingWindow,\n                needs=Resampled.resampled,\n                wscheme=zounds.HalfLapped(),\n                wfunc=zounds.OggVorbisWindowingFunc(),\n                store=False)\n\n            mdct = zounds.ArrayWithUnitsFeature(\n                zounds.MDCT,\n                needs=windowed,\n                store=True)\n\n        # produce some additive sine waves\n        sine_synth = zounds.SineSynthesizer(zounds.SR22050())\n        samples = sine_synth.synthesize(\n            zounds.Seconds(4), freqs_in_hz=[220, 400, 880])\n\n        # process the sound, including a short-time fourier transform feature\n        _id = Sound.process(meta=samples.encode())\n        snd = Sound(_id)\n\n        # invert the frequency-domain feature to reover the original audio\n        mdct_synth = zounds.MDCTSynthesizer()\n        recon = mdct_synth.synthesize(snd.mdct)\n        print recon.__class__  # AudioSamples instance with reconstructed audio\n\n    See Also:\n        :class:`~zounds.spectral.MDCT`\n    """"""\n\n    def __init__(self):\n        super(MDCTSynthesizer, self).__init__()\n\n    def _windowing_function(self):\n        return OggVorbisWindowingFunc()\n\n    def _transform(self, frames):\n        l = frames.shape[1]\n        t = np.arange(0, 2 * l)\n        f = np.arange(0, l)\n        cpi = -1j * np.pi\n        a = frames * np.exp(cpi * (f + 0.5) * (l + 1) / 2 / l)\n        b = np.fft.fft(a, 2 * l)\n        return np.sqrt(2 / l) * np.real(b * np.exp(cpi * t / 2 / l))\n\n\nclass FrequencyDecompositionSynthesizer(object):\n    def __init__(self, samplerate, output_size):\n        super(FrequencyDecompositionSynthesizer, self).__init__()\n        self.output_size = output_size\n        self.samplerate = samplerate\n\n    def synthesize(self, x, bands=None):\n        output = ArrayWithUnits(\n            np.zeros((len(x), self.output_size)),\n            dimensions=[x.time_dimension, TimeDimension(*self.samplerate)])\n\n        for i, band in enumerate(x.scale):\n            if bands and i not in bands:\n                continue\n            output += resample(x[:, band], self.output_size, axis=-1)\n\n        return output\n\n\nclass BaseFrequencyAdaptiveSynthesizer(object):\n    def __init__(\n            self,\n            scale,\n            band_transform,\n            short_time_synth,\n            samplerate,\n            coeffs_dtype,\n            scale_slices_always_even):\n        super(BaseFrequencyAdaptiveSynthesizer, self).__init__()\n        self.scale_slices_always_even = scale_slices_always_even\n        self.coeffs_dtype = coeffs_dtype\n        self.scale = scale\n        self.samplerate = samplerate\n        self.short_time_synth = short_time_synth\n        self.band_transform = band_transform\n\n    def _n_linear_scale_bands(self, frequency_adaptive_coeffs):\n        raise NotImplementedError()\n\n    def synthesize(self, freq_adaptive_coeffs):\n        fac = freq_adaptive_coeffs\n\n        linear_scale = LinearScale.from_sample_rate(\n            self.samplerate,\n            self._n_linear_scale_bands(fac),\n            always_even=self.scale_slices_always_even)\n\n        frequency_dimension = FrequencyDimension(linear_scale)\n\n        coeffs = ArrayWithUnits(\n            np.zeros((len(fac), linear_scale.n_bands), dtype=self.coeffs_dtype),\n            dimensions=[fac.dimensions[0], frequency_dimension])\n\n        for band in self.scale:\n            coeffs[:, band] += self.band_transform(fac[:, band], norm=\'ortho\')\n\n        return self.short_time_synth.synthesize(coeffs)\n\n\nclass FrequencyAdaptiveDCTSynthesizer(BaseFrequencyAdaptiveSynthesizer):\n    """"""\n    Invert a frequency-adaptive transform, e.g., one produced by the\n    :class:`zounds.spectral.FrequencyAdaptiveTransform` processing node which\n    has used a discrete cosine transform in its `transform` parameter.\n\n    Args:\n        scale (FrequencyScale): The scale used to produce the frequency-adaptive\n            transform\n        samplerate (SampleRate): The audio samplerate of the audio that was\n            originally transformed\n\n    Here\'s an example of how you might first extract a frequency-adaptive\n    representation, and then invert it:\n\n    .. code:: python\n\n        import zounds\n        import scipy\n        import numpy as np\n\n        samplerate = zounds.SR11025()\n        Resampled = zounds.resampled(resample_to=samplerate)\n\n        scale = zounds.GeometricScale(\n            100, 5000, bandwidth_ratio=0.089, n_bands=100)\n        scale.ensure_overlap_ratio(0.5)\n\n\n        @zounds.simple_in_memory_settings\n        class Sound(Resampled):\n            long_windowed = zounds.ArrayWithUnitsFeature(\n                zounds.SlidingWindow,\n                wscheme=zounds.SampleRate(\n                    frequency=zounds.Milliseconds(500),\n                    duration=zounds.Seconds(1)),\n                wfunc=zounds.OggVorbisWindowingFunc(),\n                needs=Resampled.resampled)\n\n            dct = zounds.ArrayWithUnitsFeature(\n                zounds.DCT,\n                scale_always_even=True,\n                needs=long_windowed)\n\n            freq_adaptive = zounds.FrequencyAdaptiveFeature(\n                zounds.FrequencyAdaptiveTransform,\n                transform=scipy.fftpack.idct,\n                window_func=np.hanning,\n                scale=scale,\n                needs=dct,\n                store=True)\n\n\n        # produce some additive sine waves\n        sine_synth = zounds.SineSynthesizer(zounds.SR22050())\n        samples = sine_synth.synthesize(\n            zounds.Seconds(10), freqs_in_hz=[220, 440, 880])\n\n        # process the sound, including a short-time fourier transform feature\n        _id = Sound.process(meta=samples.encode())\n        snd = Sound(_id)\n\n        # invert the sound\n        synth = zounds.FrequencyAdaptiveDCTSynthesizer(scale, samplerate)\n        recon = synth.synthesize(snd.freq_adaptive)\n        print recon  # AudioSamples instance with the reconstructed sound\n\n    See Also:\n        :class:`~zounds.spectral.DCT`\n        :class:`~zounds.spectral.FrequencyAdaptive`\n        :class:`~zounds.spectral.FrequencyAdaptiveTransform`\n    """"""\n\n    def __init__(self, scale, samplerate):\n        super(FrequencyAdaptiveDCTSynthesizer, self).__init__(\n            scale,\n            dct,\n            DCTSynthesizer(),\n            samplerate,\n            np.float64,\n            scale_slices_always_even=True)\n\n    def _n_linear_scale_bands(self, frequency_adaptive_coeffs):\n        fac = frequency_adaptive_coeffs.dimensions[0]\n        return int(fac.duration / self.samplerate.frequency)\n\n\nclass FrequencyAdaptiveFFTSynthesizer(BaseFrequencyAdaptiveSynthesizer):\n    """"""\n    Invert a frequency-adaptive transform, e.g., one produced by the\n    :class:`zounds.spectral.FrequencyAdaptiveTransform` processing node which\n    has used a fast fouriter transform in its `transform` parameter.\n\n    Args:\n        scale (FrequencyScale): The scale used to produce the frequency-adaptive\n            transform\n        samplerate (SampleRate): The audio samplerate of the audio that was\n            originally transformed\n\n    Here\'s an example of how you might first extract a frequency-adaptive\n    representation, and then invert it:\n\n    .. code:: python\n\n        import zounds\n        import numpy as np\n\n        samplerate = zounds.SR11025()\n        Resampled = zounds.resampled(resample_to=samplerate)\n\n        scale = zounds.GeometricScale(100, 5000, bandwidth_ratio=0.089, n_bands=100)\n        scale.ensure_overlap_ratio(0.5)\n\n\n        @zounds.simple_in_memory_settings\n        class Sound(Resampled):\n            long_windowed = zounds.ArrayWithUnitsFeature(\n                zounds.SlidingWindow,\n                wscheme=zounds.SampleRate(\n                    frequency=zounds.Milliseconds(500),\n                    duration=zounds.Seconds(1)),\n                wfunc=zounds.OggVorbisWindowingFunc(),\n                needs=Resampled.resampled)\n\n            fft = zounds.ArrayWithUnitsFeature(\n                zounds.FFT,\n                needs=long_windowed)\n\n            freq_adaptive = zounds.FrequencyAdaptiveFeature(\n                zounds.FrequencyAdaptiveTransform,\n                transform=np.fft.irfft,\n                window_func=np.hanning,\n                scale=scale,\n                needs=fft,\n                store=True)\n\n\n        # produce some additive sine waves\n        sine_synth = zounds.SineSynthesizer(zounds.SR22050())\n        samples = sine_synth.synthesize(\n            zounds.Seconds(10), freqs_in_hz=[220, 440, 880])\n\n        # process the sound, including a short-time fourier transform feature\n        _id = Sound.process(meta=samples.encode())\n        snd = Sound(_id)\n\n        # invert the sound\n        synth = zounds.FrequencyAdaptiveFFTSynthesizer(scale, samplerate)\n        recon = synth.synthesize(snd.freq_adaptive)\n        print recon  # AudioSamples instance with the reconstructed sound\n\n    See Also:\n        :class:`~zounds.spectral.FFT`\n        :class:`~zounds.spectral.FrequencyAdaptive`\n        :class:`~zounds.spectral.FrequencyAdaptiveTransform`\n    """"""\n\n    def __init__(self, scale, samplerate):\n        super(FrequencyAdaptiveFFTSynthesizer, self).__init__(\n            scale,\n            np.fft.rfft,\n            FFTSynthesizer(),\n            samplerate,\n            np.complex128,\n            scale_slices_always_even=False)\n\n    def _n_linear_scale_bands(self, frequency_adaptive_coeffs):\n        # https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.rfft.html#numpy.fft.rfft\n        fac = frequency_adaptive_coeffs.dimensions[0]\n        raw_samples = int(fac.duration / self.samplerate.frequency)\n        return int(raw_samples // 2) + 1\n\n\nclass SineSynthesizer(object):\n    """"""\n    Synthesize sine waves\n\n    Args:\n        samplerate (Samplerate): the samplerate at which the sine waves should\n            be synthesized\n\n    Examples:\n        >>> import zounds\n        >>> synth = zounds.SineSynthesizer(zounds.SR22050())\n        >>> samples = synth.synthesize( \\\n            zounds.Seconds(1), freqs_in_hz=[220., 440.])\n        >>> samples\n        AudioSamples([ 0.        ,  0.09384942,  0.18659419, ..., -0.27714552,\n               -0.18659419, -0.09384942])\n        >>> len(samples)\n        22050\n\n\n    See Also:\n        :class:`TickSynthesizer`\n        :class:`NoiseSynthesizer`\n        :class:`SilenceSynthesizer`\n    """"""\n\n    def __init__(self, samplerate):\n        super(SineSynthesizer, self).__init__()\n        self.samplerate = samplerate\n\n    def synthesize(self, duration, freqs_in_hz=[440.]):\n        """"""\n        Synthesize one or more sine waves\n\n        Args:\n            duration (numpy.timdelta64): The duration of the sound to be\n                synthesized\n            freqs_in_hz (list of float): Numbers representing the frequencies\n                in hz that should be synthesized\n        """"""\n        freqs = np.array(freqs_in_hz)\n        scaling = 1 / len(freqs)\n        sr = int(self.samplerate)\n        cps = freqs / sr\n        ts = (duration / Seconds(1)) * sr\n        ranges = np.array([np.arange(0, ts * c, c) for c in cps])\n        raw = (np.sin(ranges * (2 * np.pi)) * scaling).sum(axis=0)\n        return AudioSamples(raw, self.samplerate)\n\n\nclass TickSynthesizer(object):\n    """"""\n    Synthesize short, percussive, periodic ""ticks""\n\n    Args:\n        samplerate (SampleRate): the samplerate at which the ticks should be\n            synthesized\n\n    Examples:\n        >>> import zounds\n        >>> synth = zounds.TickSynthesizer(zounds.SR22050())\n        >>> samples = synth.synthesize(\\\n            duration=zounds.Seconds(3), tick_frequency=zounds.Milliseconds(100))\n        >>> samples\n        AudioSamples([ -3.91624993e-01,  -8.96939666e-01,   4.18165378e-01, ...,\n                -4.08054347e-04,  -2.32257899e-04,   0.00000000e+00])\n\n    See Also:\n        :class:`SineSynthesizer`\n        :class:`NoiseSynthesizer`\n        :class:`SilenceSynthesizer`\n    """"""\n\n    def __init__(self, samplerate):\n        super(TickSynthesizer, self).__init__()\n        self.samplerate = samplerate\n\n    def synthesize(self, duration, tick_frequency):\n        """"""\n        Synthesize periodic ""ticks"", generated from white noise and an envelope\n\n        Args:\n            duration (numpy.timedelta64): The total duration of the sound to be\n                synthesized\n            tick_frequency (numpy.timedelta64): The frequency of the ticking\n                sound\n        """"""\n        sr = self.samplerate.samples_per_second\n        # create a short, tick sound\n        tick = np.random.uniform(low=-1., high=1., size=int(sr * .1))\n        tick *= np.linspace(1, 0, len(tick))\n        # create silence\n        samples = np.zeros(int(sr * (duration / Seconds(1))))\n        ticks_per_second = Seconds(1) / tick_frequency\n        # introduce periodic ticking sound\n        step = int(sr // ticks_per_second)\n        for i in range(0, len(samples), step):\n            size = len(samples[i:i + len(tick)])\n            samples[i:i + len(tick)] += tick[:size]\n        return AudioSamples(samples, self.samplerate)\n\n\nclass NoiseSynthesizer(object):\n    """"""\n    Synthesize white noise\n\n    Args:\n        samplerate (SampleRate): the samplerate at which the ticks should be\n            synthesized\n\n    Examples:\n        >>> import zounds\n        >>> synth = zounds.NoiseSynthesizer(zounds.SR44100())\n        >>> samples = synth.synthesize(zounds.Seconds(2))\n        >>> samples\n        AudioSamples([ 0.1137964 , -0.02613194,  0.30963904, ..., -0.71398137,\n               -0.99840281,  0.74310827])\n\n    See Also:\n        :class:`SineSynthesizer`\n        :class:`TickSynthesizer`\n        :class:`SilenceSynthesizer`\n    """"""\n\n    def __init__(self, samplerate):\n        super(NoiseSynthesizer, self).__init__()\n        self.samplerate = samplerate\n\n    def synthesize(self, duration):\n        """"""\n        Synthesize white noise\n\n        Args:\n            duration (numpy.timedelta64): The duration of the synthesized sound\n        """"""\n        sr = self.samplerate.samples_per_second\n        seconds = duration / Seconds(1)\n        samples = np.random.uniform(low=-1., high=1., size=int(sr * seconds))\n        return AudioSamples(samples, self.samplerate)\n\n\nclass SilenceSynthesizer(object):\n    """"""\n    Synthesize silence\n\n    Args:\n        samplerate (SampleRate): the samplerate at which the ticks should be\n            synthesized\n\n    Examples:\n        >>> import zounds\n        >>> synth = zounds.SilenceSynthesizer(zounds.SR11025())\n        >>> samples = synth.synthesize(zounds.Seconds(5))\n        >>> samples\n        AudioSamples([ 0.,  0.,  0., ...,  0.,  0.,  0.])\n    """"""\n\n    def __init__(self, samplerate):\n        super(SilenceSynthesizer, self).__init__()\n        self.samplerate = samplerate\n\n    def synthesize(self, duration):\n        """"""\n        Synthesize silence\n\n        Args:\n            duration (numpy.timedelta64): The duration of the synthesized sound\n        """"""\n        return AudioSamples.silence(self.samplerate, duration)\n'"
zounds/synthesize/test_synthesize.py,4,"b""import numpy as np\nimport unittest2\n\nfrom .synthesize import \\\n    SineSynthesizer, DCTSynthesizer, FFTSynthesizer, NoiseSynthesizer, \\\n    SilenceSynthesizer, FrequencyDecompositionSynthesizer\nfrom zounds.basic import stft, resampled\nfrom zounds.core import ArrayWithUnits\nfrom zounds.persistence import ArrayWithUnitsFeature\nfrom zounds.spectral import \\\n    FrequencyDimension, FrequencyBand, LinearScale, FFT, SlidingWindow, \\\n    OggVorbisWindowingFunc, frequency_decomposition\nfrom zounds.timeseries import \\\n    SR22050, SR44100, SR11025, SR48000, SR96000, HalfLapped, Seconds, \\\n    TimeDimension, AudioSamples, SampleRate, Milliseconds, TimeSlice\nfrom zounds.util import simple_in_memory_settings\n\n\nclass SynthesizeTests(unittest2.TestCase):\n    def test_has_correct_sample_rate(self):\n        half_lapped = HalfLapped()\n        synth = DCTSynthesizer()\n        raw = np.zeros((100, 2048))\n        band = FrequencyBand(0, SR44100().nyquist)\n        scale = LinearScale(band, raw.shape[1])\n        timeseries = ArrayWithUnits(\n            raw, [TimeDimension(*half_lapped), FrequencyDimension(scale)])\n        output = synth.synthesize(timeseries)\n        self.assertIsInstance(output.samplerate, SR44100)\n        self.assertIsInstance(output, AudioSamples)\n\n\nclass FFTSynthesizerTests(unittest2.TestCase):\n    def can_invert_fft(self, samplerate):\n        base_cls = stft(\n            resample_to=samplerate,\n            store_fft=True,\n            store_windowed=True)\n\n        @simple_in_memory_settings\n        class Document(base_cls):\n            pass\n\n        synth = SineSynthesizer(samplerate)\n        audio = synth.synthesize(Seconds(2), freqs_in_hz=[440., 880.])\n\n        _id = Document.process(meta=audio.encode())\n        doc = Document(_id)\n\n        fft_synth = FFTSynthesizer()\n        recon = fft_synth.synthesize(doc.fft)\n\n        self.assertIsInstance(recon, ArrayWithUnits)\n        self.assertEqual(audio.dimensions, recon.dimensions)\n\n    def test_can_invert_long_fft(self):\n        samplerate = SR11025()\n        rs = resampled(resample_to=samplerate)\n\n        @simple_in_memory_settings\n        class Document(rs):\n            long_windowed = ArrayWithUnitsFeature(\n                SlidingWindow,\n                wscheme=SampleRate(\n                    Milliseconds(500),\n                    Seconds(1)),\n                wfunc=OggVorbisWindowingFunc(),\n                needs=rs.resampled,\n                store=True)\n\n            long_fft = ArrayWithUnitsFeature(\n                FFT,\n                needs=long_windowed,\n                store=True)\n\n        synth = SineSynthesizer(samplerate)\n        audio = synth.synthesize(Seconds(2), freqs_in_hz=[440., 880.])\n\n        _id = Document.process(meta=audio.encode())\n        doc = Document(_id)\n\n        fft_synth = FFTSynthesizer()\n        recon = fft_synth.synthesize(doc.long_fft)\n        self.assertIsInstance(recon, AudioSamples)\n        self.assertEqual(audio.dimensions, recon.dimensions)\n\n    def test_can_invert_fft_11025(self):\n        self.can_invert_fft(SR11025())\n\n    def test_can_invert_fft_22050(self):\n        self.can_invert_fft(SR22050())\n\n    def test_can_invert_fft_44100(self):\n        self.can_invert_fft(SR44100())\n\n    def test_can_invert_fft_48000(self):\n        self.can_invert_fft(SR48000())\n\n    @unittest2.skip(\n        'HalfLapped does not compute the right window size in samples')\n    def test_can_invert_fft_96000(self):\n        self.can_invert_fft(SR96000())\n\n\nclass SineSynthesizerTests(unittest2.TestCase):\n    def test_generates_correct_shape(self):\n        ss = SineSynthesizer(SR22050())\n        audio = ss.synthesize(Seconds(4), freqs_in_hz=[440.])\n        self.assertEqual(1, len(audio.shape))\n\n    def test_generates_correct_samplerate(self):\n        ss = SineSynthesizer(SR44100())\n        audio = ss.synthesize(Seconds(4), freqs_in_hz=[440.])\n        self.assertEqual(SR44100(), audio.samplerate)\n\n    def test_generates_correct_number_of_samples(self):\n        samplerate = SR22050()\n        duration = Seconds(1)\n        ss = SineSynthesizer(samplerate)\n        audio = ss.synthesize(duration, freqs_in_hz=[440.])\n        expected_samples = int(samplerate) * int(duration / Seconds(1))\n        self.assertEqual(expected_samples, len(audio))\n\n    def test_can_create_audio_with_single_tone(self):\n        ss = SineSynthesizer(SR22050())\n        audio = ss.synthesize(Seconds(4), freqs_in_hz=[440.])\n        fft = abs(np.fft.rfft(audio))\n        self.assertEqual(1, (fft > 1).sum())\n\n    def test_can_create_audio_with_multiple_tones(self):\n        ss = SineSynthesizer(SR22050())\n        audio = ss.synthesize(Seconds(4), freqs_in_hz=[440., 660.])\n        fft = abs(np.fft.rfft(audio))\n        self.assertEqual(2, (fft > 1).sum())\n\n\nclass NoiseSynthesizerTests(unittest2.TestCase):\n    def test_noise_synth_outputs_values_in_correct_range(self):\n        ns = NoiseSynthesizer(SR11025())\n        audio = ns.synthesize(Seconds(1))\n        self.assertLess(audio.min(), 0)\n        self.assertGreater(audio.max(), 0)\n\n\nclass SilenceSynthesizerTests(unittest2.TestCase):\n    def test_silence_synthesizer_outputs_zero(self):\n        synth = SilenceSynthesizer(SR11025())\n        audio = synth.synthesize(Seconds(1))\n        np.testing.assert_allclose(audio, 0)\n\n\nclass FrequencyDecompositionSynthesizerTests(unittest2.TestCase):\n    def test_can_resynthesize_frequency_decomposition(self):\n        sr = SR22050()\n        samples = SilenceSynthesizer(sr).synthesize(Milliseconds(9999))\n        window_size = 8192\n\n        wscheme = sr.windowing_scheme(window_size, window_size // 2)\n        duration = TimeSlice(wscheme.duration)\n        frequency = TimeSlice(wscheme.frequency)\n        _, windowed = samples.sliding_window_with_leftovers(\n            duration, frequency, dopad=True)\n        fa = frequency_decomposition(\n            windowed, [32, 64, 128, 256, 512, 1024, 2048, 4096])\n\n        fdsynth = FrequencyDecompositionSynthesizer(sr, window_size)\n\n        samples = fdsynth.synthesize(fa)\n        self.assertEqual(2, samples.ndim)\n        self.assertEqual(windowed.dimensions[1], samples.dimensions[1])\n        self.assertEqual(windowed.dimensions[0], samples.dimensions[0])\n"""
zounds/timeseries/__init__.py,0,"b'""""""\nThe timeseries module introduces classes for dealing with time as it relates\nto audio signals\n""""""\n\nfrom .duration import \\\n    Hours, Minutes, Seconds, Milliseconds, Microseconds, Picoseconds, \\\n    Nanoseconds\n\nfrom .audiosamples import AudioSamples\n\nfrom .samplerate import \\\n    SR11025, SR16000, SR22050, SR44100, SR48000, SR96000, HalfLapped, \\\n    audio_sample_rate,Stride, SampleRate, nearest_audio_sample_rate\n\nfrom .timeseries import TimeSlice, TimeDimension\n\nfrom .variablerate import \\\n    VariableRateTimeSeries, VariableRateTimeSeriesFeature, \\\n    VariableRateTimeSeriesEncoder, VariableRateTimeSeriesDecoder\n\nfrom .constantrate import ConstantRateTimeSeries\n\nfrom .functional import categorical, inverse_categorical'"
zounds/timeseries/audiosamples.py,8,"b'from .samplerate import AudioSampleRate, audio_sample_rate\nfrom soundfile import SoundFile\nfrom io import BytesIO\nfrom zounds.core import IdentityDimension, ArrayWithUnits\nfrom .timeseries import TimeDimension, TimeSlice\nfrom .duration import Picoseconds, Seconds\nfrom .samplerate import SampleRate\nimport numpy as np\n\n\nclass AudioSamples(ArrayWithUnits):\n    """"""\n    `AudioSamples` represents constant-rate samples of a continuous audio signal\n    at common sampling rates.\n\n    It is a special case of an :class:`~zounds.core.ArrayWithUnits` whose first\n    dimension is a :class:`~zounds.timeseries.TimeDimension` that has a common\n    audio sampling rate (e.g. :class:`~zounds.timeseries.SR44100`).\n\n    Args:\n        array (np.ndarray): The raw sample data\n        samplerate (SampleRate): The rate at which data was sampled\n\n    Raises:\n        ValueError: When array has a second dimension with size greater than 2\n        TypeError: When samplerate is not a\n            :class:`~zounds.timeseries.AudioSampleRate`\n            (e.g. :class:`~zounds.timeseries.SR22050`)\n\n    Examples::\n        >>> from zounds import AudioSamples, SR44100, TimeSlice, Seconds\n        >>> import numpy as np\n        >>> raw = np.random.normal(0, 1, 44100*10)\n        >>> samples = AudioSamples(raw, SR44100())\n        >>> samples.samples_per_second\n        44100\n        >>> samples.channels\n        1\n        >>> sliced = samples[TimeSlice(Seconds(2))]\n        >>> sliced.shape\n        (88200,)\n    """"""\n\n    def __new__(cls, array, samplerate):\n        if array.ndim == 1:\n            dimensions = [TimeDimension(*samplerate)]\n        elif array.ndim == 2:\n            dimensions = [TimeDimension(*samplerate), IdentityDimension()]\n        else:\n            raise ValueError(\n                \'array must be one (mono) or two (multi-channel) dimensions\')\n\n        if not isinstance(samplerate, AudioSampleRate):\n            raise TypeError(\'samplerate should be an AudioSampleRate instance\')\n\n        return ArrayWithUnits.__new__(cls, array, dimensions)\n\n    def __add__(self, other):\n        try:\n            if self.samplerate != other.samplerate:\n                raise ValueError(\n                    \'Samplerates must match, but they were \'\n                    \'{self.samplerate} and {other.samplerate}\'\n                        .format(**locals()))\n        except AttributeError:\n            pass\n        return super(AudioSamples, self).__add__(other)\n\n    def kwargs(self):\n        return {\'samplerate\': self.samplerate}\n\n    def sum(self, axis=None, dtype=None, **kwargs):\n        result = super(AudioSamples, self).sum(axis, dtype, **kwargs)\n        if self.ndim == 2 and axis == 1:\n            return AudioSamples(result, self.samplerate)\n        else:\n            return result\n\n    @classmethod\n    def from_file(cls, file_like_object):\n        with SoundFile(file_like_object, mode=\'r\') as f:\n            samples = f.read(dtype=np.float32)\n            return AudioSamples(samples, audio_sample_rate(f.samplerate))\n\n    @classmethod\n    def silence(cls, samplerate, duration, dtype=np.float32, channels=1):\n        shape = (int(duration / samplerate.frequency), channels)\n        silence = np.zeros(shape, dtype=dtype).squeeze()\n        return cls(silence, samplerate)\n\n    def silence_like(self, duration):\n        x = self.__class__.silence(\n            self.samplerate, duration, self.dtype, self.channels)\n        x[:] = 1\n        return x\n\n    def pad_with_silence(self, silence_duration=Seconds(1)):\n        silence = self.__class__.silence(\n            self.samplerate, silence_duration, self.dtype)\n        return AudioSamples(np.concatenate([self, silence]), self.samplerate)\n\n    @property\n    def samples_per_second(self):\n        return int(Picoseconds(int(1e12)) / self.frequency)\n\n    @property\n    def duration_in_seconds(self):\n        return self.duration / Picoseconds(int(1e12))\n\n    @property\n    def samplerate(self):\n        return SampleRate(self.frequency, self.duration)\n\n    @property\n    def overlap(self):\n        return self.samplerate.overlap\n\n    @property\n    def span(self):\n        return self.dimensions[0].span\n\n    @property\n    def end(self):\n        return self.dimensions[0].end\n\n    @property\n    def frequency(self):\n        return self.dimensions[0].frequency\n\n    @property\n    def duration(self):\n        return self.dimensions[0].duration\n\n    @classmethod\n    def from_example(cls, arr, example):\n        return cls(arr, example.samplerate)\n\n    @property\n    def channels(self):\n        if len(self.shape) == 1:\n            return 1\n        return self.shape[1]\n\n    @property\n    def samplerate(self):\n        return audio_sample_rate(self.samples_per_second)\n\n    @property\n    def mono(self):\n        """"""\n        Return this instance summed to mono.  If the instance is already mono,\n        this is a no-op.\n        """"""\n        if self.channels == 1:\n            return self\n        x = self.sum(axis=1) * 0.5\n        y = x * 0.5\n        return AudioSamples(y, self.samplerate)\n\n    @property\n    def stereo(self):\n        if self.channels == 2:\n            return self\n        return AudioSamples(np.vstack([self, self]).T, self.samplerate)\n\n    def __getitem__(self, item):\n        sliced = super(AudioSamples, self).__getitem__(item)\n        try:\n            if sliced.dimensions == self.dimensions:\n                return AudioSamples(sliced, self.samplerate)\n        except AttributeError:\n            pass\n        return sliced\n\n    def sliding_window(self, samplerate, padding=True):\n        ws = TimeSlice(duration=samplerate.duration)\n        ss = TimeSlice(duration=samplerate.frequency)\n        _, windowed = self.sliding_window_with_leftovers(ws, ss, dopad=padding)\n        return windowed\n\n    def encode(self, flo=None, fmt=\'WAV\', subtype=\'PCM_16\'):\n        """"""\n        Return audio samples encoded as bytes given a particular audio format\n\n        Args:\n            flo (file-like): A file-like object to write the bytes to.  If flo\n                is not supplied, a new :class:`io.BytesIO` instance will be\n                created and returned\n            fmt (str): A libsndfile-friendly identifier for an audio encoding\n                (detailed here: http://www.mega-nerd.com/libsndfile/api.html)\n            subtype (str): A libsndfile-friendly identifier for an audio\n                encoding subtype (detailed here:\n                http://www.mega-nerd.com/libsndfile/api.html)\n\n        Examples:\n            >>> from zounds import SR11025, AudioSamples\n            >>> import numpy as np\n            >>> silence = np.zeros(11025*10)\n            >>> samples = AudioSamples(silence, SR11025())\n            >>> bio = samples.encode()\n            >>> bio.read(10)\n            \'RIFFx]\\\\x03\\\\x00WA\'\n        """"""\n        flo = flo or BytesIO()\n        with SoundFile(\n                flo,\n                mode=\'w\',\n                channels=self.channels,\n                format=fmt,\n                subtype=subtype,\n                samplerate=self.samples_per_second) as f:\n\n            if fmt == \'OGG\':\n                # KLUDGE: Trying to write too-large chunks to an ogg file seems\n                # to cause a segfault in libsndfile\n                # KLUDGE: This logic is very similar to logic in the OggVorbis\n                # processing node, and should probably be factored into a common\n                # location\n                factor = 20\n                chunksize = self.samples_per_second * factor\n                for i in range(0, len(self), chunksize):\n                    chunk = self[i: i + chunksize]\n                    f.write(chunk)\n            else:\n                # write everything in one chunk\n                f.write(self)\n\n        flo.seek(0)\n        return flo\n\n    def save(self, filename, fmt=\'WAV\', subtype=\'PCM_16\'):\n        with open(filename, \'wb\') as f:\n            self.encode(f, fmt=fmt, subtype=subtype)\n'"
zounds/timeseries/constantrate.py,0,"b""from .timeseries import TimeDimension, TimeSlice\nfrom zounds.core import ArrayWithUnits\n\n\nclass ConstantRateTimeSeries(ArrayWithUnits):\n    def __new__(cls, array):\n        try:\n            dim = array.dimensions[0]\n        except AttributeError:\n            raise ValueError('array must be of type ArrayWithUnits')\n\n        if not isinstance(dim, TimeDimension):\n            raise ValueError('array first dimension must be a TimeDimension')\n\n        return ArrayWithUnits.__new__(cls, array, array.dimensions)\n\n    @property\n    def time_dimension(self):\n        return self.dimensions[0]\n\n    def iter_slices(self):\n        td = self.dimensions[0]\n        for i, data in enumerate(self):\n            yield TimeSlice(duration=td.duration, start=td.frequency * i), data\n"""
zounds/timeseries/duration.py,14,"b'import numpy as np\n\n\nclass Hours(np.timedelta64):\n    """"""\n    Convenience class for creating a duration in hours\n\n    Args:\n        hours (int): duration in hours\n\n    Examples:\n        >>> from zounds import Hours\n        >>> hours = Hours(3)\n        >>> hours\n        numpy.timedelta(3, \'h\')\n    """"""\n    def __new__(cls, hours):\n        return np.timedelta64(int(hours), \'h\')\n\n\nclass Minutes(np.timedelta64):\n    """"""\n    Convenience class for creating a duration in minutes\n\n    Args:\n        minutes (int): duration in minutes\n\n    Examples:\n        >>> from zounds import Minutes\n        >>> minutes = Minutes(3)\n        >>> minutes\n        numpy.timedelta(3, \'m\')\n    """"""\n    def __new__(cls, minutes):\n        return np.timedelta64(int(minutes), \'m\')\n\n\nclass Seconds(np.timedelta64):\n    """"""\n    Convenience class for creating a duration in seconds\n\n    Args:\n        seconds (int): duration in seconds\n\n    Examples:\n        >>> from zounds import Seconds\n        >>> seconds = Seconds(3)\n        >>> seconds\n        numpy.timedelta(3, \'s\')\n    """"""\n    def __new__(cls, seconds):\n        return np.timedelta64(int(seconds), \'s\')\n\n\nclass Milliseconds(np.timedelta64):\n    """"""\n    Convenience class for creating a duration in milliseconds\n\n    Args:\n        milliseconds (int): duration in milliseconds\n\n    Examples:\n        >>> from zounds import Milliseconds\n        >>> ms = Milliseconds(3)\n        >>> ms\n        numpy.timedelta(3, \'ms\')\n    """"""\n    def __new__(cls, milliseconds):\n        return np.timedelta64(int(milliseconds), \'ms\')\n\n\nclass Microseconds(np.timedelta64):\n    """"""\n    Convenience class for creating a duration in microseconds\n\n    Args:\n        microseconds (int): duration in microseconds\n\n    Examples:\n        >>> from zounds import Microseconds\n        >>> us = Microseconds(3)\n        >>> us\n        numpy.timedelta(3, \'us\')\n    """"""\n    def __new__(cls, microseconds):\n        return np.timedelta64(int(microseconds), \'us\')\n\n\nclass Nanoseconds(np.timedelta64):\n    """"""\n    Convenience class for creating a duration in nanoseconds\n\n    Args:\n        nanoseconds (int): duration in nanoseconds\n\n    Examples:\n        >>> from zounds import Nanoseconds\n        >>> ns = Nanoseconds(3)\n        >>> ns\n        numpy.timedelta(3, \'ns\')\n    """"""\n    def __new__(cls, nanoseconds):\n        return np.timedelta64(int(nanoseconds), \'ns\')\n\n\nclass Picoseconds(np.timedelta64):\n    """"""\n    Convenience class for creating a duration in picoseconds\n\n    Args:\n        picoseconds (int): duration in picoseconds\n\n    Examples:\n        >>> from zounds import Picoseconds\n        >>> ps = Picoseconds(3)\n        >>> ps\n        numpy.timedelta(3, \'ps\')\n    """"""\n    def __new__(cls, picoseconds):\n        return np.timedelta64(int(picoseconds), \'ps\')\n'"
zounds/timeseries/functional.py,5,"b'import numpy as np\nfrom zounds.loudness import mu_law, inverse_mu_law\nfrom zounds.core import ArrayWithUnits, IdentityDimension\n\n\n# TODO: decompose this into the quantization, and then a one-hot function\ndef categorical(x, mu=255, normalize=True):\n    """"""\n    Mu-law compress a block of audio samples, and convert them into a\n    categorical distribution\n    """"""\n\n    if normalize:\n        # normalize the signal\n        mx = x.max()\n        x = np.divide(x, mx, where=mx != 0)\n\n    # mu law compression\n    x = mu_law(x)\n\n    # translate and scale to [0, 1]\n    x = (x - x.min()) * 0.5\n\n    # convert to the range [0, 255]\n    x = (x * mu).astype(np.uint8)\n\n    # create the array to house the categorical representation\n    c = np.zeros((np.product(x.shape), mu + 1), dtype=np.uint8)\n    c[np.arange(len(c)), x.flatten()] = 1\n\n    return ArrayWithUnits(\n        c.reshape(x.shape + (mu + 1,)),\n        x.dimensions + (IdentityDimension(),))\n\n\n# TODO: decompose this into de-quantization, and then a one-hot function\ndef inverse_categorical(x, mu=255):\n    """"""\n    Invert categorical samples\n    """"""\n    flat = x.reshape((-1, x.shape[-1]))\n    indices = np.argmax(flat, axis=1).astype(np.float32)\n    indices = (indices / mu) - 0.5\n    inverted = inverse_mu_law(indices, mu=mu).reshape(x.shape[:-1])\n    return ArrayWithUnits(inverted, x.dimensions[:2])\n'"
zounds/timeseries/samplerate.py,7,"b'\nfrom .duration import Picoseconds\nfrom collections import namedtuple\nimport numpy as np\nfrom .duration import Seconds\n\nStride = namedtuple(\'Stride\', [\'frequency\', \'duration\'])\n\n\nclass SampleRate(object):\n    """"""\n    `SampleRate` describes the constant frequency at which samples are taken\n    from a continuous signal, and the duration of each sample.\n\n    Instances of this class could describe an audio sampling rate (e.g. 44.1kHz)\n    or the strided windows often used in short-time fourier transforms\n\n    Args:\n        frequency (numpy.timedelta64): The frequency at which the signal is\n            sampled\n        duration (numpy.timedelta64): The duration of each sample\n\n    Raises:\n        ValueError: when frequency or duration are less than or equal to zero\n\n    Examples:\n        >>> from zounds import Seconds, SampleRate\n        >>> sr = SampleRate(Seconds(1), Seconds(2))\n        >>> sr.frequency\n        numpy.timedelta64(1,\'s\')\n        >>> sr.duration\n        numpy.timedelta64(2,\'s\')\n        >>> sr.overlap\n        numpy.timedelta64(1,\'s\')\n        >>> sr.overlap_ratio\n        0.5\n\n    See Also:\n        :class:`SR96000`\n        :class:`SR48000`\n        :class:`SR44100`\n        :class:`SR22050`\n        :class:`SR11025`\n    """"""\n\n    def __init__(self, frequency, duration):\n        if frequency.astype(np.int) <= 0:\n            raise ValueError(\'frequency must be positive\')\n        if duration.astype(np.int) <= 0:\n            raise ValueError(\'duration must be positive\')\n\n        self.frequency = frequency\n        self.duration = duration\n        super(SampleRate, self).__init__()\n\n    def __str__(self):\n        f = self.frequency / Seconds(1)\n        d = self.duration / Seconds(1)\n        return \'{self.__class__.__name__}(f={f}, d={d})\'.format(**locals())\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __iter__(self):\n        return iter((self.frequency, self.duration))\n\n    def __len__(self):\n        return 2\n\n    def __eq__(self, other):\n        return \\\n            self.frequency == other.frequency \\\n            and self.duration == other.duration\n\n    @property\n    def overlap(self):\n        """"""\n        For sampling schemes that overlap, return a :class:`numpy.timedelta64`\n        instance representing the duration of overlap between each sample\n        """"""\n        return self.duration - self.frequency\n\n    @property\n    def overlap_ratio(self):\n        """"""\n        For sampling schemes that overlap, return the ratio of overlap to\n        sample duration\n        """"""\n        return self.overlap / self.duration\n\n    @property\n    def samples_per_second(self):\n        return int(Picoseconds(int(1e12)) / self.frequency)\n\n    def __int__(self):\n        return self.samples_per_second\n\n    @property\n    def nyquist(self):\n        return self.samples_per_second // 2\n\n    def __mul__(self, other):\n        try:\n            if len(other) == 1:\n                other *= 2\n        except TypeError:\n            other = (other, other)\n\n        freq = self.frequency * other[0]\n        duration = (self.frequency * other[1]) + self.overlap\n        new = SampleRate(freq, duration)\n        return new\n\n    def discrete_samples(self, ts):\n        td = next(dim for dim in ts.dimensions if hasattr(dim, \'frequency\'))\n        windowsize = np.round((self.duration - td.overlap) / td.frequency)\n        stepsize = np.round(self.frequency / td.frequency)\n        return int(stepsize), int(windowsize)\n\n    def resample(self, ratio):\n        orig_freq = Picoseconds(int(self.frequency / Picoseconds(1)))\n        orig_duration = Picoseconds(int(self.duration / Picoseconds(1)))\n        f = orig_freq * ratio\n        d = orig_duration * ratio\n        return SampleRate(f, d)\n\n\nclass AudioSampleRate(SampleRate):\n    def __init__(self, samples_per_second, suggested_window, suggested_hop):\n        self.suggested_hop = suggested_hop\n        self.suggested_window = suggested_window\n        self.one_sample = Picoseconds(int(1e12)) // samples_per_second\n        super(AudioSampleRate, self).__init__(self.one_sample, self.one_sample)\n\n    def half_lapped(self):\n        return SampleRate(\n            self.one_sample * self.suggested_hop,\n            self.one_sample * self.suggested_window)\n\n    def windowing_scheme(self, duration_samples, frequency_samples=None):\n        frequency_samples = frequency_samples or duration_samples\n        return SampleRate(\n            self.frequency * frequency_samples,\n            self.duration * duration_samples)\n\n\nclass SR96000(AudioSampleRate):\n    """"""\n    A :class:`SampleRate` representing the common audio sampling rate 96kHz\n\n    Examples:\n        >>> from zounds import SR96000\n        >>> sr = SR96000()\n        >>> sr.samples_per_second\n        96000\n        >>> int(sr)\n        96000\n        >>> sr.nyquist\n        48000\n    """"""\n\n    def __init__(self):\n        super(SR96000, self).__init__(96000, 4096, 2048)\n\n\nclass SR48000(AudioSampleRate):\n    """"""\n    A :class:`SampleRate` representing the common audio sampling rate 48kHz\n\n    Examples:\n        >>> from zounds import SR48000\n        >>> sr = SR48000()\n        >>> sr.samples_per_second\n        48000\n        >>> int(sr)\n        48000\n        >>> sr.nyquist\n        24000\n    """"""\n\n    def __init__(self):\n        super(SR48000, self).__init__(48000, 2048, 1024)\n\n\nclass SR44100(AudioSampleRate):\n    """"""\n    A :class:`SampleRate` representing the common audio sampling rate 44.1kHz\n\n    Examples:\n        >>> from zounds import SR44100\n        >>> sr = SR44100()\n        >>> sr.samples_per_second\n        44100\n        >>> int(sr)\n        44100\n        >>> sr.nyquist\n        22050\n    """"""\n\n    def __init__(self):\n        super(SR44100, self).__init__(44100, 2048, 1024)\n\n\nclass SR22050(AudioSampleRate):\n    """"""\n    A :class:`SampleRate` representing the common audio sampling rate 22.025kHz\n\n    Examples:\n        >>> from zounds import SR22050\n        >>> sr = SR22050()\n        >>> sr.samples_per_second\n        22050\n        >>> int(sr)\n        22050\n        >>> sr.nyquist\n        11025\n    """"""\n\n    def __init__(self):\n        super(SR22050, self).__init__(22050, 1024, 512)\n\n\nclass SR16000(AudioSampleRate):\n    """"""\n        A :class:`SampleRate` representing the common audio sampling rate 16kHz\n\n        Examples:\n            >>> from zounds import SR16000\n            >>> sr = SR16000()\n            >>> sr.samples_per_second\n            16000\n            >>> int(sr)\n            16000\n            >>> sr.nyquist\n            8000\n        """"""\n\n    def __init__(self):\n        super(SR16000, self).__init__(16000, 512, 256)\n\n\nclass SR11025(AudioSampleRate):\n    """"""\n    A :class:`SampleRate` representing the common audio sampling rate 11.025kHz\n\n    Examples:\n        >>> from zounds import SR11025\n        >>> sr = SR11025()\n        >>> sr.samples_per_second\n        11025\n        >>> int(sr)\n        11025\n        >>> sr.nyquist\n        5512\n    """"""\n\n    def __init__(self):\n        super(SR11025, self).__init__(11025, 512, 256)\n\n\n_samplerates = (\n    SR96000(), SR48000(), SR44100(), SR22050(), SR16000(), SR11025())\n\n\ndef audio_sample_rate(samples_per_second):\n    for sr in _samplerates:\n        if samples_per_second == sr.samples_per_second:\n            return sr\n    raise ValueError(\n        \'{samples_per_second} is an invalid sample rate\'.format(**locals()))\n\n\ndef nearest_audio_sample_rate(samples_per_second):\n    samplerates = np.array([s.samples_per_second for s in _samplerates])\n    diffs = np.abs(samples_per_second - samplerates)\n    return _samplerates[np.argmin(diffs)]\n\n\nclass HalfLapped(SampleRate):\n    def __init__(self, window_at_44100=2048, hop_at_44100=1024):\n        one_sample_at_44100 = Picoseconds(int(1e12)) / 44100.\n        window = one_sample_at_44100 * window_at_44100\n        step = one_sample_at_44100 * hop_at_44100\n        super(HalfLapped, self).__init__(step, window)\n'"
zounds/timeseries/test_audiosamples.py,28,"b""import unittest2\nimport numpy as np\nfrom .duration import Seconds\nfrom .samplerate import SR44100, SR11025, SampleRate, Stride\nfrom zounds.timeseries import TimeDimension, TimeSlice\nfrom zounds.core import IdentityDimension\nfrom .audiosamples import AudioSamples\nfrom zounds.synthesize import SineSynthesizer, SilenceSynthesizer\n\n\nclass AudioSamplesTest(unittest2.TestCase):\n\n    def test_sliding_window(self):\n        samples = AudioSamples.silence(SR11025(), Seconds(30))\n        sr = samples.samplerate * Stride(frequency=16, duration=512)\n        windowed = samples.sliding_window(sr)\n        self.assertEqual((512,), windowed.shape[1:])\n        long_sr = SampleRate(\n            frequency=sr.frequency * 2, duration=sr.frequency * 32)\n        frequency = TimeSlice(duration=long_sr.frequency)\n        duration = TimeSlice(duration=long_sr.duration)\n        _, long_windowed = windowed.sliding_window_with_leftovers(\n            windowsize=duration, stepsize=frequency, dopad=True)\n        self.assertEqual((32, 512), long_windowed.shape[1:])\n        self.assertEqual(3, long_windowed.ndim)\n\n    def test_sliding_window_dimensions(self):\n        samplerate = SR11025()\n        samples = AudioSamples.silence(SR11025(), Seconds(10))\n        window_sr = SampleRate(\n            duration=samplerate.frequency * 512,\n            frequency=samplerate.frequency * (512 - 25))\n        windowed = samples.sliding_window(window_sr)\n        self.assertEqual(window_sr, windowed.dimensions[0].samplerate)\n\n    def test_expanding_first_dimension_should_create_identity_dimension(self):\n        silence = AudioSamples.silence(SR11025(), Seconds(10))\n        expanded = silence[None, ...]\n        self.assertEqual(2, len(expanded.dimensions))\n        self.assertIsInstance(expanded.dimensions[0], IdentityDimension)\n        self.assertEqual(silence.dimensions[0], expanded.dimensions[1])\n\n    def test_time_slice_should_return_audio_samples(self):\n        silence = AudioSamples.silence(SR11025(), Seconds(10))\n        ts = TimeSlice(duration=Seconds(1))\n        sliced = silence[ts]\n        self.assertIsInstance(sliced, AudioSamples)\n        self.assertEqual(int(SR11025()), len(sliced))\n        self.assertEqual(SR11025(), sliced.samplerate)\n\n    def test_integer_slice_should_return_scalar(self):\n        silence = AudioSamples.silence(SR11025(), Seconds(10))\n        sliced = silence[10]\n        self.assertIsInstance(sliced, np.float32)\n\n    def test_stereo(self):\n        silence = AudioSamples.silence(SR11025(), Seconds(10))\n        self.assertEqual(1, silence.channels)\n        stereo = silence.stereo\n        self.assertEqual(2, stereo.channels)\n        self.assertEqual((len(silence), 2), stereo.shape)\n        np.testing.assert_allclose(silence, stereo[:, 0])\n        np.testing.assert_allclose(silence, stereo[:, 1])\n\n    def test_silence_creates_silence(self):\n        silence = AudioSamples.silence(SR11025(), Seconds(10))\n        self.assertEqual(0, silence.sum())\n\n    def test_silence_honors_channels(self):\n        silence = AudioSamples.silence(SR11025(), Seconds(1), channels=2)\n        self.assertEqual((11025, 2), silence.shape)\n        self.assertEqual(2, silence.channels)\n\n    def test_silence_like_creates_silence(self):\n        silence = AudioSamples.silence(SR11025(), Seconds(1), channels=2)\n        silence2 = silence.silence_like(Seconds(2))\n        self.assertEqual((22050, 2), silence2.shape)\n        self.assertEqual(SR11025(), silence2.samplerate)\n        self.assertEqual(2, silence2.channels)\n\n    def test_pad_with_samples_adds_silence_at_end(self):\n        synth = SineSynthesizer(SR11025())\n        samples = synth.synthesize(Seconds(2))\n        padded = samples.pad_with_silence(Seconds(4))\n        silence = padded[TimeSlice(start=Seconds(2))]\n        self.assertEqual(0, silence.sum())\n\n    def test_raises_if_not_audio_samplerate(self):\n        arr = np.zeros(int(44100 * 2.5))\n        one = Seconds(1)\n        self.assertRaises(\n            TypeError, lambda: AudioSamples(arr, SampleRate(one, one)))\n\n    def test_raises_if_array_is_more_than_2d(self):\n        arr = np.zeros((int(44100 * 2.5), 2, 2))\n        self.assertRaises(\n            ValueError, lambda: AudioSamples(arr, SR44100()))\n\n    def test_can_create_instance(self):\n        arr = np.zeros(int(44100 * 2.5))\n        instance = AudioSamples(arr, SR44100())\n        self.assertIsInstance(instance, AudioSamples)\n        length_seconds = instance.end / Seconds(1)\n        self.assertAlmostEqual(2.5, length_seconds, places=6)\n\n    def test_can_mix_two_instances(self):\n        arr = np.ones(int(44100 * 2.5))\n        first = AudioSamples(arr, SR44100())\n        second = AudioSamples(arr, SR44100())\n        mixed = first + second\n        self.assertIsInstance(mixed, AudioSamples)\n        self.assertEqual(SR44100(), mixed.samplerate)\n        np.testing.assert_allclose(mixed, 2)\n\n    def test_cannot_mix_two_instances_with_different_sample_rates(self):\n        arr = np.ones(int(44100 * 2.5))\n        first = AudioSamples(arr, SR44100())\n        second = AudioSamples(arr, SR11025())\n        self.assertRaises(ValueError, lambda: first + second)\n\n    def test_can_add_plain_numpy_array(self):\n        arr = np.ones(int(44100 * 2.5))\n        first = AudioSamples(arr, SR44100())\n        second = arr.copy()\n        mixed = first + second\n        self.assertIsInstance(mixed, AudioSamples)\n        self.assertEqual(SR44100(), mixed.samplerate)\n        np.testing.assert_allclose(mixed, 2)\n\n    def test_channels_returns_one_for_one_dimensional_array(self):\n        arr = np.zeros(int(44100 * 2.5))\n        instance = AudioSamples(arr, SR44100())\n        self.assertEqual(1, instance.channels)\n\n    def test_channels_returns_two_for_two_dimensional_array(self):\n        arr = np.zeros(int(44100 * 2.5))\n        arr = np.column_stack((arr, arr))\n        instance = AudioSamples(arr, SR44100())\n        self.assertEqual(2, instance.channels)\n\n    def test_samplerate_returns_correct_value(self):\n        arr = np.zeros(int(44100 * 2.5))\n        instance = AudioSamples(arr, SR44100())\n        self.assertIsInstance(instance.samplerate, SR44100)\n\n    def test_can_sum_to_mono(self):\n        arr = np.zeros(int(44100 * 2.5))\n        arr = np.column_stack((arr, arr))\n        instance = AudioSamples(arr, SR44100())\n        mono = instance.mono\n        self.assertEqual(1, mono.channels)\n        self.assertIsInstance(mono.samplerate, SR44100)\n\n    def test_class_concat_returns_audio_samples(self):\n        s1 = AudioSamples(np.zeros(44100 * 2), SR44100())\n        s2 = AudioSamples(np.zeros(44100), SR44100())\n        s3 = AudioSamples.concat([s1, s2])\n        self.assertIsInstance(s3, AudioSamples)\n        self.assertEqual(44100 * 3, len(s3))\n\n    def test_instance_concat_returns_audio_samples(self):\n        s1 = AudioSamples(np.zeros(44100 * 2), SR44100())\n        s2 = AudioSamples(np.zeros(44100), SR44100())\n        s3 = s1.concat([s1, s2])\n        self.assertIsInstance(s3, AudioSamples)\n        self.assertEqual(44100 * 3, len(s3))\n\n    def test_concat_raises_for_different_sample_rates(self):\n        s1 = AudioSamples(np.zeros(44100 * 2), SR44100())\n        s2 = AudioSamples(np.zeros(44100), SR11025())\n        self.assertRaises(ValueError, lambda: AudioSamples.concat([s1, s2]))\n\n    def test_sum_along_time_axis(self):\n        arr = np.zeros(int(44100 * 2.5))\n        arr = np.column_stack((arr, arr))\n        ts = AudioSamples(arr, SR44100())\n        result = ts.sum(axis=0)\n        self.assertIsInstance(result, np.ndarray)\n        self.assertNotIsInstance(result, AudioSamples)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], IdentityDimension)\n\n    def test_sum_along_second_axis(self):\n        arr = np.zeros(int(44100 * 2.5))\n        arr = np.column_stack((arr, arr))\n        ts = AudioSamples(arr, SR44100())\n        result = ts.sum(axis=1)\n        self.assertIsInstance(result, AudioSamples)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], TimeDimension)\n\n    def test_encode_large_ogg(self):\n        sr = SR11025()\n        synth = SilenceSynthesizer(sr)\n        samples = synth.synthesize(Seconds(190))\n        raw = samples.encode(fmt='OGG', subtype='VORBIS')\n        # prior to this test, the line above caused a segfault, so the assertion\n        # below is fairly worthless, and mostly a formality\n        self.assertIsNotNone(raw)\n"""
zounds/timeseries/test_constantrate.py,5,"b'import unittest2\nfrom .constantrate import ConstantRateTimeSeries\nfrom zounds.core import ArrayWithUnits, IdentityDimension\nfrom zounds.timeseries import TimeDimension, Seconds, Milliseconds, TimeSlice\nimport numpy as np\n\n\nclass ConstantRateTimeSeriesTests(unittest2.TestCase):\n\n    def test_raises_when_not_array_with_units_instance(self):\n        arr = np.zeros(10)\n        self.assertRaises(ValueError, lambda: ConstantRateTimeSeries(arr))\n\n    def test_raises_when_first_dimension_is_not_time_dimension(self):\n        raw = np.zeros((10, 3))\n        arr = ArrayWithUnits(raw, dimensions=[\n            IdentityDimension(), TimeDimension(frequency=Seconds(1))])\n        self.assertRaises(ValueError, lambda: ConstantRateTimeSeries(arr))\n\n    def test_iter_slices_yields_evenly_spaced_time_slices(self):\n        raw = np.random.random_sample((10, 3))\n        arr = ArrayWithUnits(raw, dimensions=[\n            TimeDimension(frequency=Milliseconds(500), duration=Seconds(1)),\n            IdentityDimension()\n        ])\n        crts = ConstantRateTimeSeries(arr)\n        slices = list(crts.iter_slices())\n        self.assertEqual(10, len(slices))\n\n        ts1, d1 = slices[0]\n        self.assertEqual(\n            TimeSlice(start=Seconds(0), duration=Seconds(1)), ts1)\n        np.testing.assert_allclose(raw[0], d1)\n\n        ts2, d2 = slices[1]\n        self.assertEqual(\n            TimeSlice(start=Milliseconds(500), duration=Seconds(1)), ts2)\n        np.testing.assert_allclose(raw[1], d2)\n'"
zounds/timeseries/test_functional.py,1,"b'import unittest2\nfrom .functional import categorical, inverse_categorical\nfrom zounds.core import ArrayWithUnits\nfrom zounds.synthesize import SineSynthesizer\nfrom zounds.timeseries import Seconds, SR11025, TimeSlice\nimport numpy as np\n\n\nclass CategoricalTests(unittest2.TestCase):\n    def test_can_convert_to_categorical_distribution(self):\n        samplerate = SR11025()\n        synth = SineSynthesizer(samplerate)\n        samples = synth.synthesize(Seconds(4), [220, 440, 880])\n        _, windowed = samples.sliding_window_with_leftovers(\n            TimeSlice(duration=samplerate.frequency * 512),\n            TimeSlice(duration=samplerate.frequency * 256))\n        c = categorical(windowed, mu=255)\n        self.assertEqual(windowed.shape + (255 + 1,), c.shape)\n        np.testing.assert_allclose(c.sum(axis=-1), 1)\n\n    def test_can_invert_categorical_distribution(self):\n        samplerate = SR11025()\n        synth = SineSynthesizer(samplerate)\n        samples = synth.synthesize(Seconds(4), [220, 440, 880])\n        _, windowed = samples.sliding_window_with_leftovers(\n            TimeSlice(duration=samplerate.frequency * 512),\n            TimeSlice(duration=samplerate.frequency * 256))\n        c = categorical(windowed, mu=255)\n        inverted = inverse_categorical(c, mu=255)\n        self.assertEqual(windowed.shape, inverted.shape)\n        self.assertIsInstance(inverted, ArrayWithUnits)\n        self.assertSequenceEqual(windowed.dimensions, inverted.dimensions)\n\n\n'"
zounds/timeseries/test_samplerate.py,4,"b'\nimport unittest2\nimport numpy as np\nfrom .duration import Seconds, Milliseconds\nfrom zounds.core import ArrayWithUnits, IdentityDimension\nfrom .timeseries import TimeDimension\nfrom .samplerate import \\\n    SampleRate, SR96000, SR48000, SR44100, SR22050, SR11025, \\\n    audio_sample_rate, HalfLapped\nfrom zounds.synthesize import SilenceSynthesizer\n\n\nclass SampleRateTests(unittest2.TestCase):\n    def test_raises_value_error_for_zero_frequency(self):\n        self.assertRaises(\n            ValueError, lambda: SampleRate(Seconds(0), Seconds(1)))\n\n    def test_raises_value_error_for_negative_frequency(self):\n        self.assertRaises(\n            ValueError, lambda: SampleRate(Seconds(-1), Seconds(1)))\n\n    def test_raises_value_error_for_zero_duration(self):\n        self.assertRaises(\n            ValueError, lambda: SampleRate(Seconds(1), Seconds(0)))\n\n    def test_raises_value_error_for_negative_duration(self):\n        self.assertRaises(\n            ValueError, lambda: SampleRate(Seconds(1), Seconds(-1)))\n\n    def test_can_unpack_samplerate(self):\n        sr = SampleRate(Seconds(1), Seconds(2))\n        frequency, duration = sr\n        self.assertEqual(Seconds(1), frequency)\n        self.assertEqual(Seconds(2), duration)\n\n    def test_can_unpack_audio_samplerate(self):\n        sr = SR44100()\n        frequency, duration = sr\n        self.assertEqual(sr.frequency, frequency)\n        self.assertEqual(sr.duration, duration)\n\n    def test_discrete_samples_multiple_dimensions(self):\n        sr = SR22050()\n        samples = SilenceSynthesizer(sr).synthesize(Milliseconds(6666))\n        stacked = ArrayWithUnits(\n            np.zeros((10,) + samples.shape, dtype=samples.dtype),\n            (IdentityDimension(),) + samples.dimensions)\n        stacked[:] = samples\n        self.assertEqual((512, 1024), HalfLapped().discrete_samples(stacked))\n\n    def test_discrete_samples_11025(self):\n        sr = SR11025()\n        ts = ArrayWithUnits(\n            np.zeros(sr.samples_per_second), [TimeDimension(*sr)])\n        hl = HalfLapped()\n        freq, duration = hl.discrete_samples(ts)\n        self.assertEqual(256, freq)\n        self.assertEqual(512, duration)\n\n    def test_discrete_samples_22050(self):\n        sr = SR22050()\n        ts = ArrayWithUnits(\n            np.zeros(sr.samples_per_second), [TimeDimension(*sr)])\n        hl = HalfLapped()\n        freq, duration = hl.discrete_samples(ts)\n        self.assertEqual(512, freq)\n        self.assertEqual(1024, duration)\n\n    def test_discrete_samples_44100(self):\n        sr = SR44100()\n        ts = ArrayWithUnits(\n            np.zeros(sr.samples_per_second), [TimeDimension(*sr)])\n        hl = HalfLapped()\n        freq, duration = hl.discrete_samples(ts)\n        self.assertEqual(1024, freq)\n        self.assertEqual(2048, duration)\n\n    def test_nyquist_22050(self):\n        self.assertEqual(11025, SR22050().nyquist)\n\n    def test_nyquist_44100(self):\n        self.assertEqual(22050, SR44100().nyquist)\n\n    def test_can_convert_to_int(self):\n        self.assertEqual(22050, int(SR22050()))\n\n    def test_raises_for_unknown_audio_samplerate(self):\n        self.assertRaises(ValueError, lambda: audio_sample_rate(1))\n\n    def test_sr_96000_frequency(self):\n        self.assertEqual(96000, SR96000().samples_per_second)\n\n    def test_get_96000_frequency(self):\n        self.assertIsInstance(audio_sample_rate(96000), SR96000)\n\n    def test_sr_48000_frequency(self):\n        self.assertEqual(48000, SR48000().samples_per_second)\n\n    def test_get_48000_frequency(self):\n        self.assertIsInstance(audio_sample_rate(48000), SR48000)\n\n    def test_sr_44100_frequency(self):\n        self.assertEqual(44100, SR44100().samples_per_second)\n\n    def test_get_44100_frequency(self):\n        self.assertIsInstance(audio_sample_rate(44100), SR44100)\n\n    def test_sr_22050_frequency(self):\n        self.assertEqual(22050, SR22050().samples_per_second)\n\n    def test_get_22050_freuency(self):\n        self.assertIsInstance(audio_sample_rate(22050), SR22050)\n\n    def test_sr_11025_frequency(self):\n        self.assertEqual(11025, SR11025().samples_per_second)\n\n    def test_get_11025_frequency(self):\n        self.assertIsInstance(audio_sample_rate(11025), SR11025)\n\n    def test_no_overlap(self):\n        self.assertEqual(Seconds(0), SampleRate(Seconds(1), Seconds(1)).overlap)\n\n    def test_some_overlap(self):\n        self.assertEqual(Seconds(1), SampleRate(Seconds(1), Seconds(2)).overlap)\n\n    def test_multiply_no_overlap_number(self):\n        sr = SampleRate(Seconds(1), Seconds(1)) * 2\n        self.assertEqual(Seconds(2), sr.frequency)\n        self.assertEqual(Seconds(2), sr.duration)\n\n    def test_multiply_some_overlap_number(self):\n        sr = SampleRate(Seconds(1), Seconds(2)) * 2\n        self.assertEqual(Seconds(2), sr.frequency)\n        self.assertEqual(Seconds(3), sr.duration)\n\n    def test_multiply_no_overlap_single_value(self):\n        sr = SampleRate(Seconds(1), Seconds(1)) * (2,)\n        self.assertEqual(Seconds(2), sr.frequency)\n        self.assertEqual(Seconds(2), sr.duration)\n\n    def test_multiply_some_overlap_single_value(self):\n        sr = SampleRate(Seconds(1), Seconds(2)) * (2,)\n        self.assertEqual(Seconds(2), sr.frequency)\n        self.assertEqual(Seconds(3), sr.duration)\n\n    def test_multiply_no_overlap_two_values(self):\n        sr = SampleRate(Seconds(1), Seconds(1)) * (2, 4)\n        self.assertEqual(Seconds(2), sr.frequency)\n        self.assertEqual(Seconds(4), sr.duration)\n\n    def test_multiply_some_overlap_two_values(self):\n        sr = SampleRate(Seconds(1), Seconds(2)) * (2, 4)\n        self.assertEqual(Seconds(2), sr.frequency)\n        self.assertEqual(Seconds(5), sr.duration)\n\n    def test_resampled(self):\n        original_frequency = Milliseconds(500)\n        original_duration = Seconds(1)\n        ratio = 0.02\n        orig_sr = SampleRate(original_frequency, original_duration)\n        new_sr = orig_sr.resample(ratio)\n        self.assertEqual(Milliseconds(10), new_sr.frequency)\n        self.assertEqual(Milliseconds(20), new_sr.duration)\n\n    def test_overlap_ratio_zero(self):\n        sr = SampleRate(frequency=Seconds(1), duration=Seconds(1))\n        self.assertEqual(0, sr.overlap_ratio)\n\n    def test_overlap_ratio_half(self):\n        sr = SampleRate(frequency=Milliseconds(500), duration=Seconds(1))\n        self.assertEqual(0.5, sr.overlap_ratio)\n\n    def test_overlap_ratio_type(self):\n        sr = SampleRate(frequency=Milliseconds(500), duration=Seconds(1))\n        self.assertIsInstance(sr.overlap_ratio, float)\n'"
zounds/timeseries/test_timedimension.py,0,"b""import unittest2\nfrom .timeseries import TimeDimension, TimeSlice\nfrom .duration import Seconds, Milliseconds\nfrom .samplerate import SR44100\n\n\nclass TimeDimensionTests(unittest2.TestCase):\n    def test_equals(self):\n        td1 = TimeDimension(Seconds(1), Milliseconds(900))\n        td2 = TimeDimension(Seconds(1), Milliseconds(900))\n        self.assertEqual(td1, td2)\n\n    def test_not_equal(self):\n        td1 = TimeDimension(Seconds(2), Milliseconds(900))\n        td2 = TimeDimension(Seconds(1), Milliseconds(900))\n        self.assertNotEqual(td1, td2)\n\n    def test_raises_if_frequency_is_not_timedelta_instance(self):\n        self.assertRaises(ValueError, lambda: TimeDimension('s'))\n\n    def test_raises_if_duration_is_not_timedelta_instance(self):\n        self.assertRaises(ValueError, lambda: TimeDimension(Seconds(2), 's'))\n\n    def test_duration_is_equal_to_frequency_if_not_provided(self):\n        td = TimeDimension(Seconds(1))\n        self.assertEqual(Seconds(1), td.frequency)\n        self.assertEqual(Seconds(1), td.duration)\n\n    def test_integer_based_slice(self):\n        td = TimeDimension(*SR44100(), size=44100 * 5)\n        sl = td.integer_based_slice(TimeSlice(duration=Seconds(1)))\n        self.assertEqual(slice(0, 44100), sl)"""
zounds/timeseries/test_timeseries.py,62,"b""import numpy as np\nimport unittest2\nfrom .duration import \\\n    Picoseconds, Milliseconds, Seconds, Microseconds, Nanoseconds, Hours\nfrom .timeseries import TimeSlice, TimeDimension\nfrom zounds.core import IdentityDimension, ArrayWithUnits\n\n\nclass ConvenienceClassTests(unittest2.TestCase):\n    def test_seconds_equal(self):\n        a = Seconds(1)\n        b = np.timedelta64(1, 's')\n        self.assertEqual(a, b)\n\n    def test_milliseconds_equal(self):\n        a = Milliseconds(1000)\n        b = np.timedelta64(1000, 'ms')\n        self.assertEqual(a, b)\n\n    def test_microseconds_equal(self):\n        a = Microseconds(10)\n        b = np.timedelta64(10, 'us')\n        self.assertEqual(a, b)\n\n    def test_different_units_equal(self):\n        self.assertEqual(Seconds(1), Milliseconds(1000))\n\n\nclass TimeSliceTests(unittest2.TestCase):\n\n    def test_equivalent_timeslices_hash_to_the_same_value(self):\n        ts1 = TimeSlice(start=Seconds(1), duration=Seconds(7))\n        ts2 = TimeSlice(start=Milliseconds(1000), duration=Milliseconds(7000))\n        self.assertEqual(hash(ts1), hash(ts2))\n\n    def test_exactly_equal_timeslices_hash_to_the_same_value(self):\n        ts1 = TimeSlice(start=Seconds(1), duration=Seconds(7))\n        ts2 = TimeSlice(start=Seconds(1), duration=Seconds(7))\n        self.assertEqual(hash(ts1), hash(ts2))\n\n    def test_different_timeslices_hash_to_different_values(self):\n        ts1 = TimeSlice(start=Seconds(1), duration=Seconds(7))\n        ts2 = TimeSlice(start=Seconds(1), duration=Seconds(8))\n        self.assertNotEqual(hash(ts1), hash(ts2))\n\n    def test_can_produce_time_slice_iterable_from_timestamps(self):\n        slices = TimeSlice.slices([\n            Seconds(1),\n            Milliseconds(1),\n            Seconds(2)\n        ])\n        self.assertEqual(2, len(slices))\n        self.assertEqual(\n            TimeSlice(start=Milliseconds(1), duration=Milliseconds(999)),\n            slices[0])\n        self.assertEqual(\n            TimeSlice(start=Seconds(1), duration=Seconds(1)),\n            slices[1])\n\n    def test_can_repr_empty_slice(self):\n        r = repr(TimeSlice())\n        self.assertIsNotNone(r)\n\n    def test_can_repr_open_ended_slice(self):\n        r = repr(TimeSlice(start=Seconds(2)))\n        self.assertIsNotNone(r)\n\n    def test_raises_if_duration_is_not_timedelta_instance(self):\n        self.assertRaises(ValueError, lambda: TimeSlice(1))\n\n    def test_raises_if_start_is_provided_and_is_not_timedelta_instance(self):\n        self.assertRaises(\n                ValueError, lambda: TimeSlice(Nanoseconds(100), 1))\n\n    def test_can_instantiate_time_slice_instance_without_start_argument(self):\n        duration = Seconds(100)\n        ts = TimeSlice(duration)\n        self.assertEqual(duration, ts.duration)\n        self.assertEqual(Seconds(0), ts.start)\n\n    def test_can_instantiate_time_slice_instance_with_start_argument(self):\n        duration = Microseconds(1000)\n        start = Hours(1)\n        ts = TimeSlice(duration, start=start)\n        self.assertEqual(duration, ts.duration)\n        self.assertEqual(start, ts.start)\n\n    def test_can_intersect_two_time_slices(self):\n        ts1 = TimeSlice(Seconds(100), start=Seconds(100))\n        ts2 = TimeSlice(Seconds(100), start=Seconds(101))\n        intersection = ts1 & ts2\n        self.assertEqual(Seconds(99), intersection.duration)\n\n    def test_can_find_null_intersection(self):\n        ts1 = TimeSlice(Seconds(100), start=Seconds(100))\n        ts2 = TimeSlice(Seconds(100), start=Seconds(200))\n        intersection = ts1 & ts2\n        self.assertEqual(Seconds(0), intersection.duration)\n\n    def test_does_not_contain_point_in_time_before(self):\n        ts = TimeSlice(Seconds(100), start=Seconds(200))\n        self.assertFalse(Seconds(10) in ts)\n\n    def test_contains_point_in_time_during(self):\n        ts = TimeSlice(Seconds(100), start=Seconds(200))\n        self.assertTrue(Seconds(210) in ts)\n\n    def test_does_not_contain_point_in_time_after(self):\n        ts = TimeSlice(Seconds(100), start=Seconds(200))\n        self.assertFalse(Seconds(310) in ts)\n\n    def test_does_not_contain_slice_completely_before(self):\n        ts1 = TimeSlice(Seconds(100), start=Seconds(200))\n        ts2 = TimeSlice(Seconds(10), Seconds(12))\n        self.assertFalse(ts2 in ts1)\n\n    def test_does_not_contain_slice_beginning_before(self):\n        ts1 = TimeSlice(Seconds(100), start=Seconds(200))\n        ts2 = TimeSlice(Seconds(50), Seconds(190))\n        self.assertFalse(ts2 in ts1)\n\n    def test_contains_slice(self):\n        ts1 = TimeSlice(Seconds(100), start=Seconds(200))\n        ts2 = TimeSlice(Seconds(10), Seconds(250))\n        self.assertTrue(ts2 in ts1)\n\n    def test_does_not_contain_slice_completely_after(self):\n        ts1 = TimeSlice(Seconds(100), start=Seconds(200))\n        ts2 = TimeSlice(Seconds(100), Seconds(310))\n        self.assertFalse(ts2 in ts1)\n\n    def test_does_not_contain_slice_beginning_after(self):\n        ts1 = TimeSlice(Seconds(100), start=Seconds(200))\n        ts2 = TimeSlice(Seconds(100), Seconds(210))\n        self.assertFalse(ts2 in ts1)\n\n    def test_raises_value_error_if_item_is_not_timedelta_or_timeslice(self):\n        ts1 = TimeSlice(Seconds(100), start=Seconds(200))\n        self.assertRaises(ValueError, lambda: 's' in ts1)\n\n    def test_eq_when_start_and_duration_equal(self):\n        ts1 = TimeSlice(Seconds(2), start=Seconds(2))\n        ts2 = TimeSlice(Seconds(2), start=Seconds(2))\n        self.assertEqual(ts1, ts2)\n\n    def test_ne_when_durations_differ(self):\n        ts1 = TimeSlice(Seconds(2), start=Seconds(2))\n        ts2 = TimeSlice(Seconds(3), start=Seconds(2))\n        self.assertNotEqual(ts1, ts2)\n\n    def test_ne_when_starts_differ(self):\n        ts1 = TimeSlice(Seconds(2), start=Seconds(2))\n        ts2 = TimeSlice(Seconds(2), start=Seconds(3))\n        self.assertNotEqual(ts1, ts2)\n\n\nclass TimeSeriesTests(unittest2.TestCase):\n\n    def test_can_slice_time_series_with_end_time(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        sliced = ts[:Seconds(7)]\n        self.assertEqual(7, len(sliced))\n\n    def test_can_slice_time_series_with_start_time(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        sliced = ts[Seconds(7):]\n        self.assertEqual(3, len(sliced))\n\n    def test_can_slice_time_series_with_start_and_end_time(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        sliced = ts[Seconds(5):Seconds(7)]\n        self.assertEqual(2, len(sliced))\n\n    def test_can_slice_time_series_with_negative_start_time(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        sliced = ts[-Seconds(7):]\n        self.assertEqual(7, len(sliced))\n\n    def test_can_slice_time_series_with_negative_end_time(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        sliced = ts[:-Seconds(2)]\n        self.assertEqual(8, len(sliced))\n\n    def test_can_slice_time_series_with_time_slice(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        sl = TimeSlice(Seconds(2), start=Seconds(2))\n        ts2 = ts[sl]\n        self.assertEqual(2, len(ts2))\n\n    def test_can_slice_time_series_with_open_ended_time_slice(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        sl = TimeSlice(None, start=Seconds(2))\n        ts2 = ts[sl]\n        self.assertEqual(8, len(ts2))\n\n    def test_can_index_constant_rate_time_series_with_integer_index(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        ts2 = ts[5]\n        self.assertEqual(5, ts2)\n\n    def test_can_index_2d_time_series_with_single_integer_index(self):\n        arr = np.random.random_sample((10, 3))\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq), IdentityDimension()])\n        ts2 = ts[5]\n        self.assertEqual((3,), ts2.shape)\n\n    def test_can_mix_time_slice_and_integer_indices(self):\n        arr = np.ones((10, 5))\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq), IdentityDimension()])\n        sl = TimeSlice(duration=Seconds(5), start=Seconds(5))\n        ts2 = ts[sl, 2:]\n        self.assertEqual((5, 3), ts2.shape)\n\n    def test_can_slice_constant_rate_time_series_with_integer_indices(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        ts2 = ts[:5]\n        self.assertEqual(5, len(ts2))\n        self.assertIsInstance(ts2, ArrayWithUnits)\n\n    def test_can_add_constant_factor_to_time_series(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        ts2 = ts + 10\n        self.assertTrue(np.all(np.arange(10, 20) == ts2))\n        self.assertIsInstance(ts2, ArrayWithUnits)\n\n    def test_get_index_error_when_using_out_of_range_int_index(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        self.assertRaises(IndexError, lambda: ts[100])\n\n    def test_get_empty_time_series_when_using_out_of_range_time_slice(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        sl = TimeSlice(Seconds(2), start=Seconds(11))\n        ts2 = ts[sl]\n        self.assertEqual(0, ts2.size)\n        self.assertIsInstance(ts2, ArrayWithUnits)\n\n    def test_time_slice_spanning_less_than_one_sample_returns_one_sample(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        sl = TimeSlice(Milliseconds(100), start=Milliseconds(1500))\n        ts2 = ts[sl]\n        self.assertIsInstance(ts2, ArrayWithUnits)\n        self.assertEqual(1, ts2.size)\n        self.assertEqual(1, ts2[0])\n\n    def test_time_slice_spanning_multiple_samples_returns_all_samples(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        sl = TimeSlice(Milliseconds(2000), start=Milliseconds(1500))\n        ts2 = ts[sl]\n        self.assertIsInstance(ts2, ArrayWithUnits)\n        self.assertEqual(3, ts2.size)\n        self.assertTrue(np.all(np.arange(1, 4) == ts2))\n\n    def test_frequency_and_duration_differ(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        duration = Seconds(2)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq, duration)])\n        sl = TimeSlice(Seconds(2), start=Seconds(1))\n        ts2 = ts[sl]\n        self.assertIsInstance(ts2, ArrayWithUnits)\n        self.assertEqual(3, ts2.size)\n        self.assertTrue(np.all(np.arange(3) == ts2))\n\n    def test_frequency_and_duration_differ2(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        duration = Seconds(3)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq, duration)])\n        sl = TimeSlice(Seconds(2), start=Seconds(5))\n        ts2 = ts[sl]\n        self.assertIsInstance(ts2, ArrayWithUnits)\n        self.assertEqual(4, ts2.size)\n        self.assertTrue(np.all(np.arange(3, 7) == ts2))\n\n    def test_frequency_and_duration_differ3(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        duration = Seconds(3)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq, duration)])\n        sl = TimeSlice(Seconds(2), start=Seconds(6))\n        ts2 = ts[sl]\n        self.assertIsInstance(ts2, ArrayWithUnits)\n        self.assertEqual(4, ts2.size)\n        self.assertTrue(np.all(np.arange(4, 8) == ts2))\n\n    def test_frequency_less_than_one(self):\n        arr = np.arange(10)\n        freq = Milliseconds(500)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        sl = TimeSlice(Seconds(2), start=Milliseconds(600))\n        ts2 = ts[sl]\n        self.assertIsInstance(ts2, ArrayWithUnits)\n        self.assertEqual(5, ts2.size)\n        self.assertTrue(np.all(np.arange(1, 6) == ts2))\n\n    def test_frequency_less_than_one_freq_and_duration_differ(self):\n        arr = np.arange(10)\n        freq = Milliseconds(500)\n        duration = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq, duration)])\n        sl = TimeSlice(Seconds(3), start=Milliseconds(250))\n        ts2 = ts[sl]\n        self.assertIsInstance(ts2, ArrayWithUnits)\n        self.assertEqual(7, ts2.size)\n        self.assertTrue(np.all(np.arange(0, 7) == ts2))\n\n    def test_frequency_less_than_one_freq_and_duration_differ2(self):\n        arr = np.arange(10)\n        freq = Milliseconds(500)\n        duration = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq, duration)])\n        sl = TimeSlice(Seconds(3), start=Milliseconds(1250))\n        ts2 = ts[sl]\n        self.assertIsInstance(ts2, ArrayWithUnits)\n        self.assertEqual(8, ts2.size)\n        self.assertTrue(np.all(np.arange(1, 9) == ts2))\n\n    def test_duration_less_than_frequency(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        duration = Milliseconds(500)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq, duration)])\n        sl = TimeSlice(Seconds(3), start=Milliseconds(1250))\n        ts2 = ts[sl]\n        self.assertIsInstance(ts2, ArrayWithUnits)\n        self.assertEqual(4, ts2.size)\n        self.assertTrue(np.all(np.arange(1, 5) == ts2))\n\n    def test_can_get_entire_time_series_with_empty_slice(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        duration = Milliseconds(500)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq, duration)])\n        ts2 = ts[:]\n        self.assertIsInstance(ts2, ArrayWithUnits)\n        self.assertTrue(np.all(np.arange(10) == ts2))\n\n    def test_can_get_entire_time_series_with_empty_time_slice(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        sl = TimeSlice()\n        ts2 = ts[sl]\n        self.assertEqual(10, len(ts2))\n\n    def test_can_sum_2d_timeseries(self):\n        arr = np.zeros((10, 3))\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq), IdentityDimension()])\n        ts2 = ts.sum(axis=1)\n        self.assertIsInstance(ts2, ArrayWithUnits)\n        self.assertEqual(1, len(ts2.dimensions))\n        self.assertEqual(freq, ts2.dimensions[0].frequency)\n        self.assertEqual(freq, ts2.dimensions[0].duration)\n\n    def test_span_freq_and_duration_equal(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        self.assertEqual(TimeSlice(Seconds(10)), ts.dimensions[0].span)\n\n    def test_span_duration_greater_than_frequency(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        duration = Milliseconds(2500)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq, duration)])\n        self.assertEqual(TimeSlice(Milliseconds(11500)), ts.dimensions[0].span)\n\n    def test_span_duration_less_than_frequency(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        duration = Milliseconds(500)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq, duration)])\n        self.assertEqual(TimeSlice(Milliseconds(9500)), ts.dimensions[0].span)\n\n    def test_duration_in_seconds_half_second(self):\n        arr = np.arange(10)\n        freq = Milliseconds(500)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        self.assertEqual(0.5, ts.dimensions[0].duration_in_seconds)\n\n    def test_duration_in_seconds_two_seconds(self):\n        arr = np.arange(10)\n        freq = Seconds(2)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        self.assertEqual(2, ts.dimensions[0].duration_in_seconds)\n\n    def test_samplerate_one_per_second(self):\n        arr = np.arange(10)\n        freq = Seconds(1)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        self.assertEqual(1, ts.dimensions[0].samples_per_second)\n\n    def test_samplerate_two_per_second(self):\n        arr = np.arange(10)\n        freq = Milliseconds(500)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        self.assertEqual(2, ts.dimensions[0].samples_per_second)\n\n    def test_samplerate_three_per_second(self):\n        arr = np.arange(10)\n        freq = Milliseconds(333)\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        self.assertEqual(3, int(ts.dimensions[0].samples_per_second))\n\n    def test_samplerate_audio(self):\n        arr = np.arange(10)\n        freq = Picoseconds(int(1e12)) / 44100.\n        ts = ArrayWithUnits(arr, [TimeDimension(freq)])\n        self.assertEqual(44100, int(ts.dimensions[0].samples_per_second))\n\n    def test_concatenation_with_differing_freqs_and_durations_raises(self):\n        ts = ArrayWithUnits(\n                np.arange(10),\n                [TimeDimension(Seconds(1), Seconds(2))])\n        ts2 = ArrayWithUnits(\n                np.arange(10, 20),\n                [TimeDimension(Seconds(1), Seconds(1))])\n        self.assertRaises(ValueError, lambda: ts.concatenate(ts2))\n\n    def test_concatenation_with_matching_freqs_and_duration_results_in_crts(\n            self):\n        ts = ArrayWithUnits(\n                np.ones((10, 3)),\n                [TimeDimension(Seconds(1), Seconds(2)), IdentityDimension()])\n        ts2 = ArrayWithUnits(\n                np.ones((13, 3)),\n                [TimeDimension(Seconds(1), Seconds(2)), IdentityDimension()])\n        result = ts.concatenate(ts2)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((23, 3), result.shape)\n\n    def test_concat_with_differing_freqs(self):\n        ts = ArrayWithUnits(\n                np.ones((10, 3)),\n                [TimeDimension(Seconds(2), Seconds(2)), IdentityDimension()])\n        ts2 = ArrayWithUnits(\n                np.ones((13, 3)),\n                [TimeDimension(Seconds(1), Seconds(2)), IdentityDimension()])\n        self.assertRaises(\n                ValueError, lambda: ArrayWithUnits.concat([ts, ts2]))\n\n    def test_concat_with_differing_durations(self):\n        td1 = TimeDimension(Seconds(1), Seconds(2))\n        ts1 = ArrayWithUnits(np.ones((10, 3)), [td1, IdentityDimension()])\n        td2 = TimeDimension(Seconds(1), Seconds(3))\n        ts2 = ArrayWithUnits(np.ones((13, 3)), [td2, IdentityDimension()])\n        self.assertRaises(\n                ValueError, lambda: ArrayWithUnits.concat([ts1, ts2]))\n\n    def test_concat_along_first_axis(self):\n        td1 = TimeDimension(Seconds(1), Seconds(2))\n        ts1 = ArrayWithUnits(np.ones((10, 3)), [td1, IdentityDimension()])\n        td2 = TimeDimension(Seconds(1), Seconds(2))\n        ts2 = ArrayWithUnits(np.ones((13, 3)), [td2, IdentityDimension()])\n        result = ArrayWithUnits.concat([ts1, ts2])\n        self.assertEqual((23, 3), result.shape)\n\n    def test_concat_along_second_axis(self):\n        td1 = TimeDimension(Seconds(1), Seconds(2))\n        ts1 = ArrayWithUnits(np.ones((10, 3)), [td1, IdentityDimension()])\n        td2 = TimeDimension(Seconds(1), Seconds(2))\n        ts2 = ArrayWithUnits(np.ones((10, 5)), [td2, IdentityDimension()])\n        result = ArrayWithUnits.concat([ts1, ts2], axis=1)\n        self.assertEqual((10, 8), result.shape)\n        self.assertIsInstance(result.dimensions[0], TimeDimension)\n        self.assertIsInstance(result.dimensions[1], IdentityDimension)\n\n    def test_sum_along_time_axis(self):\n        td = TimeDimension(Seconds(1), Seconds(2))\n        ts = ArrayWithUnits(np.ones((10, 3)), [td, IdentityDimension()])\n        result = ts.sum(axis=0)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((3,), result.shape)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], IdentityDimension)\n\n    def test_sum_along_second_axis(self):\n        td = TimeDimension(Seconds(1), Seconds(2))\n        ts = ArrayWithUnits(np.ones((10, 3)), [td, IdentityDimension()])\n        result = ts.sum(axis=1)\n        self.assertIsInstance(result, ArrayWithUnits)\n        self.assertEqual((10,), result.shape)\n        self.assertEqual(1, len(result.dimensions))\n        self.assertIsInstance(result.dimensions[0], TimeDimension)\n"""
zounds/timeseries/test_variablerate.py,53,"b'from random import random\n\nimport featureflow as ff\nimport numpy as np\nimport unittest2\n\nfrom .duration import Milliseconds\nfrom .timeseries import TimeSlice\nfrom .variablerate import VariableRateTimeSeries, VariableRateTimeSeriesFeature\nfrom zounds.basic import Pooled, stft\nfrom zounds.segment import TimeSliceFeature\nfrom zounds.synthesize import NoiseSynthesizer\nfrom zounds.timeseries import Picoseconds, Seconds, SR44100\nfrom zounds.util import simple_in_memory_settings\n\n\nclass VariableRateTimeSeriesFeatureTests(unittest2.TestCase):\n\n    def test_can_encode_and_decode_variable_rate_time_Series(self):\n\n        class TimestampEmitter(ff.Node):\n            def __init__(self, needs=None):\n                super(TimestampEmitter, self).__init__(needs=needs)\n                self.pos = Picoseconds(0)\n\n            def _process(self, data):\n                td = data.dimensions[0]\n                frequency = td.frequency\n                timestamps = [self.pos + (i * frequency)\n                        for i, d in enumerate(data)\n                        if random() > 0.9]\n                slices = TimeSlice.slices(timestamps)\n                yield VariableRateTimeSeries(\n                    (ts, np.zeros(0)) for ts in slices)\n                self.pos += frequency * len(data)\n\n        graph = stft(store_fft=True)\n\n        @simple_in_memory_settings\n        class Document(graph):\n            slices = TimeSliceFeature(\n                    TimestampEmitter,\n                    needs=graph.fft,\n                    store=True)\n\n            pooled = VariableRateTimeSeriesFeature(\n                    Pooled,\n                    op=np.max,\n                    axis=0,\n                    needs=(slices, graph.fft),\n                    store=False)\n\n        signal = NoiseSynthesizer(SR44100())\\\n            .synthesize(Seconds(10))\\\n            .encode()\n        _id = Document.process(meta=signal)\n        doc = Document(_id)\n        self.assertIsInstance(doc.pooled, VariableRateTimeSeries)\n        self.assertEqual(doc.fft.shape[1], doc.pooled.slicedata.shape[1])\n\n\nclass VariableRateTimeSeriesTests(unittest2.TestCase):\n\n    def test_can_concatenate_variable_rate_time_series(self):\n        ts1 = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(1), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(2), duration=Seconds(1)), np.zeros(10)),\n        ))\n        ts2 = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(1), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(2), duration=Seconds(1)), np.zeros(10)),\n        ))\n        ts3 = ts1.concat(ts2)\n        self.assertEqual(4, len(ts3))\n        self.assertEqual((4, 10), ts3.slicedata.shape)\n\n    def test_concat_fails_when_data_shape_is_mismatched(self):\n        ts1 = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(1), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(2), duration=Seconds(1)), np.zeros(10)),\n        ))\n        ts2 = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(1), duration=Seconds(1)), np.zeros(11)),\n            (TimeSlice(start=Seconds(2), duration=Seconds(1)), np.zeros(11)),\n        ))\n        self.assertRaises(ValueError, lambda: ts1.concat(ts2))\n\n    def test_can_create_instance_with_no_slice_data(self):\n        ts = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(1), duration=Seconds(1)), np.zeros(0)),\n            (TimeSlice(start=Seconds(2), duration=Seconds(1)), np.zeros(0)),\n        ))\n        self.assertEqual(2, len(ts))\n        self.assertEqual((2, 0), ts.slicedata.shape)\n\n    def test_can_slice_time_series_with_time_slice(self):\n        ts = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(0), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(1), duration=Seconds(2)), np.zeros(10)),\n            (TimeSlice(start=Seconds(3), duration=Seconds(1)), np.zeros(10))\n        ))\n        sliced = ts[TimeSlice(start=Seconds(1), duration=Seconds(2))]\n        self.assertIsInstance(sliced, VariableRateTimeSeries)\n        self.assertEqual(1, len(sliced))\n\n    def test_can_slice_time_series_with_open_ended_time_slice(self):\n        ts = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(0), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(1), duration=Seconds(2)), np.zeros(10)),\n            (TimeSlice(start=Seconds(3), duration=Seconds(1)), np.zeros(10))\n        ))\n        sliced = ts[TimeSlice(start=Seconds(1))]\n        self.assertIsInstance(sliced, VariableRateTimeSeries)\n        self.assertEqual(2, len(sliced))\n\n    def test_can_index_time_series_with_integer_index(self):\n        ts = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(0), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(1), duration=Seconds(2)), np.zeros(10)),\n            (TimeSlice(start=Seconds(3), duration=Seconds(1)), np.zeros(10))\n        ))\n        sliced = ts[1]\n        self.assertIsInstance(sliced, np.record)\n        timeslice, data = sliced\n        self.assertEqual(\n                TimeSlice(start=Seconds(1), duration=Seconds(2)), timeslice)\n        self.assertIsInstance(data, np.ndarray)\n        self.assertEqual((10,), data.shape)\n\n    def test_can_slice_time_series_with_integer_indices(self):\n        ts = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(0), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(1), duration=Seconds(2)), np.zeros(10)),\n            (TimeSlice(start=Seconds(3), duration=Seconds(1)), np.zeros(10))\n        ))\n        sliced = ts[1:]\n        self.assertIsInstance(sliced, VariableRateTimeSeries)\n        self.assertEqual(2, len(sliced))\n\n    def test_get_index_error_when_using_out_of_range_int_index(self):\n        ts = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(0), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(1), duration=Seconds(2)), np.zeros(10)),\n            (TimeSlice(start=Seconds(3), duration=Seconds(1)), np.zeros(10))\n        ))\n        self.assertRaises(IndexError, lambda: ts[5])\n\n    def test_get_empty_time_series_when_using_out_of_range_time_slice(self):\n        ts = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(0), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(1), duration=Seconds(2)), np.zeros(10)),\n            (TimeSlice(start=Seconds(3), duration=Seconds(1)), np.zeros(10))\n        ))\n        sliced = ts[TimeSlice(start=Seconds(10), duration=Seconds(1))]\n        self.assertIsInstance(sliced, VariableRateTimeSeries)\n        self.assertEqual(0, len(sliced))\n\n    def test_time_slice_spanning_less_than_one_example_returns_one_example(\n            self):\n        ts = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(0), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(1), duration=Seconds(2)), np.zeros(10)),\n            (TimeSlice(start=Seconds(3), duration=Seconds(1)), np.zeros(10))\n        ))\n        timeslice = TimeSlice(\n                start=Milliseconds(500), duration=Milliseconds(100))\n        sliced = ts[timeslice]\n        self.assertIsInstance(sliced, VariableRateTimeSeries)\n        self.assertEqual(1, len(sliced))\n\n    def test_time_slice_spanning_multiple_examples_returns_all_examples(self):\n        ts = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(0), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(1), duration=Seconds(2)), np.zeros(10)),\n            (TimeSlice(start=Seconds(3), duration=Seconds(1)), np.zeros(10))\n        ))\n        timeslice = TimeSlice(start=Milliseconds(500), duration=Seconds(1))\n        sliced = ts[timeslice]\n        self.assertIsInstance(sliced, VariableRateTimeSeries)\n        self.assertEqual(2, len(sliced))\n\n    def test_can_get_entire_time_series_with_empty_slice(self):\n        ts = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(0), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(1), duration=Seconds(2)), np.zeros(10)),\n            (TimeSlice(start=Seconds(3), duration=Seconds(1)), np.zeros(10))\n        ))\n        sliced = ts[:]\n        self.assertIsInstance(sliced, VariableRateTimeSeries)\n        self.assertEqual(3, len(sliced))\n\n    def test_sorts_input(self):\n        ts = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(3), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(0), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(1), duration=Seconds(2)), np.zeros(10))\n        ))\n        timeslice, data = ts[0]\n        self.assertEqual(\n                TimeSlice(start=Seconds(0), duration=Seconds(1)), timeslice)\n\n    def raises_if_data_is_of_variable_size(self):\n        data = ((\n            (TimeSlice(start=Seconds(3), duration=Seconds(1)), np.zeros(11)),\n            (TimeSlice(start=Seconds(0), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(1), duration=Seconds(2)), np.zeros(10))\n        ))\n        self.assertRaises(ValueError, VariableRateTimeSeries(data))\n\n    def test_span(self):\n        ts = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(0), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(1), duration=Seconds(2)), np.zeros(10)),\n            (TimeSlice(start=Seconds(3), duration=Seconds(1)), np.zeros(10))\n        ))\n        self.assertEqual(\n                TimeSlice(start=Seconds(0), duration=Seconds(4)), ts.span)\n\n    def test_span_empty(self):\n        ts = VariableRateTimeSeries(())\n        self.assertEqual(\n                TimeSlice(start=Seconds(0), duration=Seconds(0)), ts.span)\n\n    def test_end(self):\n        ts = VariableRateTimeSeries((\n            (TimeSlice(start=Seconds(0), duration=Seconds(1)), np.zeros(10)),\n            (TimeSlice(start=Seconds(1), duration=Seconds(2)), np.zeros(10)),\n            (TimeSlice(start=Seconds(3), duration=Seconds(1)), np.zeros(10))\n        ))\n        self.assertEqual(Seconds(4), ts.end)\n\n    def test_end_empty(self):\n        ts = VariableRateTimeSeries(())\n        self.assertEqual(Seconds(0), ts.end)\n\n'"
zounds/timeseries/timeseries.py,21,"b'import numpy as np\nfrom .duration import Picoseconds, Seconds\nfrom .samplerate import SampleRate\nfrom zounds.core import Dimension\n\n\nclass TimeSlice(object):\n    """"""\n    A slice that can be applied to a :class:`TimeDimension` to return a subset\n    of samples.\n\n    Args:\n        duration (np.timedelta64): The duration of the slice\n        start (np.timedelta64): A duration representing the start position of\n            this slice, relative to zero or the beginning.  If not provided,\n            defaults to zero\n\n    Raises:\n        ValueError: when duration and/or start are not\n            :class:`numpy.timedelta64` instances\n\n    Examples:\n        >>> from zounds import ArrayWithUnits, TimeDimension, TimeSlice, Seconds\n        >>> import numpy as np\n        >>> raw = np.zeros(100)\n        >>> ts = ArrayWithUnits(raw, [TimeDimension(Seconds(1))])\n        >>> sliced = ts[TimeSlice(duration=Seconds(5), start=Seconds(50))]\n        >>> sliced.shape\n        (5,)\n\n    See Also:\n        :class:`TimeDimension`\n    """"""\n\n    def __init__(self, duration=None, start=None):\n        super(TimeSlice, self).__init__()\n\n        if duration is not None and not isinstance(duration, np.timedelta64):\n            raise ValueError(\'duration must be of type {t} but was {t2}\'.format(\n                t=np.timedelta64, t2=duration.__class__))\n\n        if start is not None and not isinstance(start, np.timedelta64):\n            raise ValueError(\'start must be of type {t} but was {t2}\'.format(\n                t=np.timedelta64, t2=start.__class__))\n\n        self.duration = duration\n        self.start = start or Picoseconds(0)\n\n    @classmethod\n    def slices(cls, timestamps):\n        srt = np.sort(timestamps)\n        diff = np.diff(srt)\n        return [TimeSlice(start=s, duration=d) for s, d in zip(srt, diff)]\n\n    def __add__(self, other):\n        return TimeSlice(self.duration, start=self.start + other)\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    @property\n    def end(self):\n        return self.start + self.duration\n\n    def __lt__(self, other):\n        try:\n            return self.start.__lt__(other.start)\n        except AttributeError:\n            return self.start.__lt__(other)\n\n    def __gt__(self, other):\n        try:\n            return self.start.__gt__(other.start)\n        except AttributeError:\n            return self.start.__gt__(other)\n\n    def __le__(self, other):\n        try:\n            return self.start.__le__(other.start)\n        except AttributeError:\n            return self.start.__le__(other)\n\n    def __ge__(self, other):\n        try:\n            return self.start.__ge__(other.start)\n        except AttributeError:\n            return self.start.__ge__(other)\n\n    def __and__(self, other):\n        delta = max(\n            Picoseconds(0),\n            min(self.end, other.end) - max(self.start, other.start))\n        return TimeSlice(delta)\n\n    def __contains__(self, other):\n        if isinstance(other, np.timedelta64):\n            return self.start < other < self.end\n        if isinstance(other, TimeSlice):\n            return other.start > self.start and other.end < self.end\n        raise ValueError\n\n    def __eq__(self, other):\n        return self.start == other.start and self.duration == other.duration\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        start = self.start / Picoseconds(1)\n        duration = \\\n            None if self.duration is None else (self.duration / Picoseconds(1))\n        return (start, duration).__hash__()\n\n    def __repr__(self):\n        dur = self.duration / Seconds(1) if self.duration is not None else None\n        return \'{cls}(start = {start}, duration = {duration})\'.format(\n            cls=self.__class__.__name__,\n            start=self.start / Seconds(1),\n            duration=dur)\n\n    def __str__(self):\n        return self.__repr__()\n\n\nclass TimeDimension(Dimension):\n    """"""\n    When applied to an axis of :class:`~zounds.core.ArrayWithUnits`, that axis\n    can be viewed as representing a constant-rate time series sampled at a\n    given :class:`~zounds.timeseries.SampleRate`.\n\n    Args:\n        frequency (np.timedelta64): The sampling frequency for this dimension\n        duration (np.timedelta64): The sampling duration for this dimension.\n            When not provided it defaults to the sampling frequency\n        size (int): The size/length of the dimension\n\n    Raises:\n        ValueError: when frequency and/or duration are not\n            :class:`np.timedelta64` instances\n\n    Examples:\n        >>> from zounds import ArrayWithUnits, TimeDimension, Seconds, TimeSlice\n        >>> import numpy as np\n        >>> raw = np.zeros(100)\n        >>> timeseries = ArrayWithUnits(raw, [TimeDimension(Seconds(1))])\n        >>> timeseries.dimensions[0]\n        TimeDimension(f=1.0, d=1.0)\n        >>> timeseries.dimensions[0].end_seconds\n        100.0\n        >>> sliced = timeseries[TimeSlice(Seconds(50))]\n        >>> sliced.shape\n        (50,)\n    """"""\n\n    def copy(self):\n        return TimeDimension(self.frequency, self.duration, self.size)\n\n    def __init__(self, frequency=None, duration=None, size=None):\n        super(TimeDimension, self).__init__()\n        self.size = size\n        if not isinstance(frequency, np.timedelta64):\n            raise ValueError(\'duration must be of type {t} but was {t2}\'.format(\n                t=np.timedelta64, t2=frequency.__class__))\n\n        if duration is not None and not isinstance(duration, np.timedelta64):\n            raise ValueError(\'start must be of type {t} but was {t2}\'.format(\n                t=np.timedelta64, t2=duration.__class__))\n        self.duration = duration or frequency\n        self.frequency = frequency\n\n\n\n    def __str__(self):\n        fs = self.frequency / Picoseconds(int(1e12))\n        ds = self.duration / Picoseconds(int(1e12))\n        return \'TimeDimension(f={fs}, d={ds})\'.format(**locals())\n\n    def __repr__(self):\n        return self.__str__()\n\n    def resample(self, ratio):\n        rs = self.samplerate.resample(ratio)\n        return TimeDimension(frequency=rs.frequency, duration=rs.duration)\n\n    @property\n    def samplerate(self):\n        return SampleRate(self.frequency, self.duration)\n\n    @property\n    def overlap(self):\n        return self.samplerate.overlap\n\n    @property\n    def overlap_ratio(self):\n        return self.samplerate.overlap_ratio\n\n    @property\n    def duration_in_seconds(self):\n        return self.duration / Picoseconds(int(1e12))\n\n    @property\n    def samples_per_second(self):\n        return int(Picoseconds(int(1e12)) / self.frequency)\n\n    @property\n    def span(self):\n        overlap = self.duration - self.frequency\n        return TimeSlice((self.size * self.frequency) + overlap)\n\n    @property\n    def end(self):\n        return self.span.end\n\n    @property\n    def end_seconds(self):\n        return self.end / Picoseconds(int(1e12))\n\n    def metaslice(self, index, size):\n        return TimeDimension(self.frequency, self.duration, size)\n\n    def modified_dimension(self, size, windowsize, stepsize=None):\n        stepsize = stepsize or windowsize\n\n        try:\n            yield TimeDimension(\n                self.frequency * stepsize,\n                (self.frequency * windowsize) + self.overlap)\n        except TypeError:\n            # windowsize and or stepsize were likely slice(None)\n            pass\n\n        yield self\n\n    def integer_based_slice(self, ts):\n        """"""\n        Transform a :class:`TimeSlice` into integer indices that numpy can work\n        with\n\n        Args:\n            ts (slice, TimeSlice): the time slice to translate into integer\n                indices\n        """"""\n\n        if isinstance(ts, slice):\n            try:\n                start = Seconds(0) if ts.start is None else ts.start\n                if start < Seconds(0):\n                    start = self.end + start\n                stop = self.end if ts.stop is None else ts.stop\n                if stop < Seconds(0):\n                    stop = self.end + stop\n                duration = stop - start\n                ts = TimeSlice(start=start, duration=duration)\n            except (ValueError, TypeError):\n                pass\n\n        if not isinstance(ts, TimeSlice):\n            return ts\n\n        diff = self.duration - self.frequency\n        start_index = \\\n            max(0, np.floor((ts.start - diff) / self.frequency))\n        end = self.end if ts.duration is None else ts.end\n\n\n        # KLUDGE: This is basically arbitrary, but the motivation is that we\'d\n        # like to differentiate between cases where the slice\n        # actually/intentionally overlaps a particular sample, and cases where\n        # the slice overlaps the sample by a tiny amount, due to rounding or\n        # lack of precision (e.g. Seconds(1) / SR44100().frequency).\n        ratio = np.round(end / self.frequency, 2)\n\n\n        stop_index = np.ceil(ratio)\n        return slice(int(start_index), int(stop_index))\n\n    def __eq__(self, other):\n        return \\\n            self.frequency == other.frequency \\\n            and self.duration == other.duration\n'"
zounds/timeseries/variablerate.py,6,"b""import numpy as np\nfrom .timeseries import TimeSlice\nfrom .duration import Seconds, Picoseconds\nfrom featureflow import Feature, BaseNumpyDecoder, NumpyEncoder\nfrom itertools import chain\n\n\nclass VariableRateTimeSeries(object):\n    def __init__(self, data):\n        super(VariableRateTimeSeries, self).__init__()\n        if isinstance(data, np.recarray):\n            self._data = data\n            return\n\n        data = sorted(list(data), key=lambda x: x[0])\n        try:\n            example = data[0][1]\n            shape = example.shape\n            dtype = example.dtype\n        except IndexError:\n            shape = (0,)\n            dtype = np.uint8\n        self._data = np.recarray(len(data), dtype=[\n            ('timeslice', TimeSlice),\n            ('slicedata', dtype, shape)])\n        self._data[:] = data\n\n    def __len__(self):\n        return self._data.__len__()\n\n    def concat(self, other):\n        return VariableRateTimeSeries(\n            chain(self.iter_slices(), other.iter_slices()))\n\n    def iter_slices(self):\n        return zip(self.slices, self.slicedata)\n\n    @property\n    def slices(self):\n        return self._data.timeslice\n\n    @property\n    def slicedata(self):\n        return self._data.slicedata\n\n    @property\n    def raw_data(self):\n        return self._data\n\n    @property\n    def span(self):\n        try:\n            start = self._data.timeslice[0].start\n            return TimeSlice(start=start, duration=self.end - start)\n        except IndexError:\n            return TimeSlice(duration=Seconds(0))\n\n    @property\n    def end(self):\n        try:\n            return self._data.timeslice[-1].end\n        except IndexError:\n            return Seconds(0)\n\n    def __getitem__(self, index):\n        if isinstance(index, TimeSlice):\n            # TODO: Consider using a bisection approach here to make this much\n            # faster than this brute-force, O(n) approach\n            # compare the beginning of the index to the _end_ of each sample\n            # compare the end of the index to the beginning of each sample\n            g = ((x.timeslice, x.slicedata) for x in self._data\n                 if x.timeslice.end > index.start and\n                 (index.duration is None or x.timeslice.start < index.end))\n            return VariableRateTimeSeries(g)\n        elif isinstance(index, int):\n            return self._data[index]\n        else:\n            return VariableRateTimeSeries(self._data[index])\n\n\nclass VariableRateTimeSeriesEncoder(NumpyEncoder):\n    def __init__(self, needs=None):\n        super(VariableRateTimeSeriesEncoder, self).__init__(needs=needs)\n\n    def _prepare_data(self, data):\n        output = np.recarray(len(data), dtype=[\n            ('start', np.int64),\n            ('duration', np.int64),\n            ('slicedata', data.slicedata.dtype, data.slicedata.shape[1:])\n        ])\n        ps = Picoseconds(1)\n        output.slicedata[:] = data.slicedata\n        output.start[:] = [ts.start / ps for ts in data.slices]\n        output.duration[:] = [ts.duration / ps for ts in data.slices]\n        return output\n\n\nclass VariableRateTimeSeriesDecoder(BaseNumpyDecoder):\n    def __init__(self):\n        super(VariableRateTimeSeriesDecoder, self).__init__()\n\n    def _gen(self, raw):\n        for r in raw:\n            ts = TimeSlice(\n                    start=Picoseconds(r.start),\n                    duration=Picoseconds(r.duration))\n            yield (ts, r.slicedata)\n\n    def _wrap_array(self, raw, metadata):\n        return VariableRateTimeSeries(self._gen(raw))\n\n\nclass VariableRateTimeSeriesFeature(Feature):\n    def __init__(\n            self,\n            extractor,\n            needs=None,\n            store=False,\n            key=None,\n            encoder=VariableRateTimeSeriesEncoder,\n            decoder=VariableRateTimeSeriesDecoder(),\n            **extractor_args):\n        super(VariableRateTimeSeriesFeature, self).__init__(\n                extractor,\n                needs=needs,\n                store=store,\n                encoder=encoder,\n                decoder=decoder,\n                key=key,\n                **extractor_args)\n"""
zounds/ui/__init__.py,0,"b'from .contentrange import RangeUnitUnsupportedException\nfrom .api import ZoundsApp\nfrom .search import ZoundsSearch\nfrom .training_monitor import \\\n    TrainingMonitorApp, SupervisedTrainingMonitorApp, GanTrainingMonitorApp, \\\n    TripletEmbeddingMonitorApp\nfrom .cli import ObjectStorageSettings, AppSettings, NeuralNetworkTrainingSettings\n'"
zounds/ui/api.py,0,"b'import sys\nimport json\nimport tornado.ioloop\nimport tornado.web\nimport http.client\nimport traceback\nfrom io import StringIO\nimport uuid\nfrom .baseapp import BaseZoundsApp, NoMatchingSerializerException, RequestContext\nfrom .featureparser import FeatureParser\n\n\nclass ZoundsApp(BaseZoundsApp):\n    """"""\n    Adds an in-browser REPL to the base zounds application\n    """"""\n    def __init__(\n            self,\n            base_path=r\'/zounds/\',\n            model=None,\n            visualization_feature=None,\n            audio_feature=None,\n            globals={},\n            locals={},\n            html=\'index.html\',\n            secret=None):\n\n        super(ZoundsApp, self).__init__(\n            base_path=base_path,\n            model=model,\n            visualization_feature=visualization_feature,\n            audio_feature=audio_feature,\n            html=html,\n            secret=secret)\n\n        self.globals = globals\n        self.locals = locals\n        self.temp = {}\n\n    def custom_routes(self):\n        return [\n            (r\'/zounds/temp/(.+?)/?\', self.temp_handler()),\n            (r\'/zounds/repl/?\', self.repl_handler())\n        ]\n\n    def temp_handler(self):\n\n        app = self\n\n        class TempHandler(tornado.web.RequestHandler):\n\n            def get(self, _id):\n                try:\n                    result = app.temp[_id]\n                except KeyError:\n                    self.set_status(http.client.NOT_FOUND)\n                    self.finish()\n                    return\n                self.set_header(\'Content-Type\', result.content_type)\n                self.set_header(\'Accept-Ranges\', \'bytes\')\n                self.write(result.data)\n                self.set_status(http.client.OK)\n                self.finish()\n\n        return TempHandler\n\n    def repl_handler(self):\n        document = self.model\n        globals = self.globals\n        locals = self.locals\n        app = self\n\n        class ReplHandler(tornado.web.RequestHandler):\n\n            def _add_url(self, statement, output, value):\n                parser = FeatureParser(document, locals)\n                doc, feature = parser.parse_feature(statement)\n                try:\n                    context = RequestContext(\n                            document=doc,\n                            feature=feature,\n                            value=value,\n                            slce=slice(None))\n                    result = app.serialize(context)\n                    temp_id = uuid.uuid4().hex\n                    app.temp[temp_id] = result\n                    output[\'url\'] = \'/zounds/temp/{temp_id}\'.format(\n                            temp_id=temp_id)\n                    output[\'contentType\'] = result.content_type\n                except NoMatchingSerializerException:\n                    pass\n\n            def post(self):\n                statement = self.request.body\n                self.set_header(\'Content-Type\', \'application/json\')\n                output = dict()\n\n                try:\n                    orig_stdout = sys.stdout\n                    sys.stdout = sio = StringIO()\n                    try:\n                        value = eval(statement, globals, locals)\n                        output[\'result\'] = str(value)\n                        self._add_url(statement, output, value)\n                    except SyntaxError:\n                        exec (statement, globals, locals)\n                        sio.seek(0)\n                        output[\'result\'] = sio.read()\n                    self.set_status(http.client.OK)\n                except:\n                    output[\'error\'] = traceback.format_exc()\n                    self.set_status(http.client.BAD_REQUEST)\n                finally:\n                    sys.stdout = orig_stdout\n\n                self.write(json.dumps(output))\n                self.finish()\n\n        return ReplHandler\n'"
zounds/ui/baseapp.py,0,"b'import os\nimport re\n\nimport asyncio\nimport tornado.ioloop\nimport tornado.web\nimport http.client\nimport urllib.request, urllib.parse, urllib.error\nfrom .contentrange import RangeRequest, UnsatisfiableRangeRequestException\nfrom .serializer import DefaultSerializer, AudioSamplesSerializer, \\\n    NumpySerializer, OggVorbisSerializer, ConstantRateTimeSeriesSerializer, \\\n    OnsetsSerializer, SearchResultsSerializer\nimport threading\nfrom uuid import uuid4\n\n\nclass NoMatchingSerializerException(Exception):\n    pass\n\n\nclass RequestContext(object):\n    def __init__(\n            self,\n            document=None,\n            feature=None,\n            slce=None,\n            value=None):\n        self.value = value\n        self.slce = slce\n        self.feature = feature\n        self.document = document\n\n    def __repr__(self):\n        return \'\'\'RequestContext(\n    document={document},\n    feature={feature},\n    slce={slce},\n    value={value})\'\'\'.format(**self.__dict__)\n\n    def __str__(self):\n        return self.__repr__()\n\n\nclass BaseZoundsApp(object):\n    def __init__(\n            self,\n            base_path=r\'/zounds/\',\n            model=None,\n            visualization_feature=None,\n            audio_feature=None,\n            html=None,\n            secret=None):\n\n        super(BaseZoundsApp, self).__init__()\n        self.secret = secret\n        self.locals = locals\n        self.globals = globals\n        self.model = model\n        self.visualization_feature = visualization_feature\n        self.audio_feature = audio_feature\n        self.base_path = base_path\n        self.serializers = [\n            AudioSamplesSerializer(),\n            OggVorbisSerializer(),\n            ConstantRateTimeSeriesSerializer(),\n            DefaultSerializer(\'application/json\'),\n            DefaultSerializer(\'audio/ogg\'),\n            NumpySerializer(),\n            OnsetsSerializer(\n                self.visualization_feature,\n                self.audio_feature,\n                self.feature_path),\n            SearchResultsSerializer(\n                self.visualization_feature,\n                self.audio_feature,\n                self.feature_path)\n        ]\n        self._html_content = self._get_html(html)\n        self.server = None\n        self.thread = None\n\n    SCRIPT_TAG = re.compile(\'<script\\s+src=""/(?P<filename>[^""]+)""></script>\')\n\n    def _get_html(self, html_filename):\n        path, fn = os.path.split(__file__)\n        with open(os.path.join(path, html_filename)) as f:\n            html = f.read()\n\n        to_replace = [(m.groupdict()[\'filename\'], html[m.start(): m.end()])\n                      for m in self.SCRIPT_TAG.finditer(html)]\n\n        for filename, tag in to_replace:\n            with open(os.path.join(path, filename)) as scriptfile:\n                html = html.replace(\n                    tag, \'<script>{}</script>\'.format(scriptfile.read()))\n\n        return html\n\n    def feature_path(self, _id, feature):\n        _id = urllib.parse.quote(_id, safe=\'\')\n        return \'{base_path}{_id}/{feature}\'.format(\n            base_path=self.base_path, _id=_id, feature=feature)\n\n    def find_serializer(self, context):\n        try:\n            return next(filter(\n                lambda x: x.matches(context), self.serializers))\n        except StopIteration:\n            raise NoMatchingSerializerException()\n\n    def serialize(self, context):\n        serializer = self.find_serializer(context)\n        return serializer.serialize(context)\n\n    def feature_handler(self):\n\n        document = self.model\n        app = self\n\n        class FeatureHandler(tornado.web.RequestHandler):\n\n            def get(self, _id, feature):\n                doc = document(_id)\n                feature = document.features[feature]\n                try:\n                    slce = RangeRequest(self.request.headers[\'Range\']).range()\n                except KeyError:\n                    slce = slice(None)\n                context = RequestContext(\n                    document=doc, feature=feature, slce=slce)\n                try:\n                    result = app.serialize(context)\n                except UnsatisfiableRangeRequestException:\n                    self.set_status(http.client.REQUESTED_RANGE_NOT_SATISFIABLE)\n                    self.finish()\n                self.set_header(\'Content-Type\', result.content_type)\n                self.set_header(\'Accept-Ranges\', \'bytes\')\n                self.write(result.data)\n                self.set_header(\'ETag\', self.compute_etag())\n                self.set_status(\n                    http.client.PARTIAL_CONTENT if result.is_partial\n                    else http.client.OK)\n                if result.content_range:\n                    self.set_header(\'Content-Range\', str(result.content_range))\n                self.finish()\n\n        return FeatureHandler\n\n    def ui_handler(self):\n        app = self\n\n        class UIHandler(tornado.web.RequestHandler):\n            def get(self):\n                self.set_header(\'Content-Type\', \'text/html\')\n                self.write(app._html_content)\n                self.set_status(http.client.OK)\n                self.finish()\n\n        return UIHandler\n\n    def login_handler(self):\n        app = self\n\n        class LoginHandler(tornado.web.RequestHandler):\n            def get(self):\n                self.write(app._get_html(\'login.html\'))\n                self.finish()\n\n            def post(self):\n                secret = self.get_body_argument(\'secret\')\n                if secret == app.secret:\n                    self.set_secure_cookie(\'user\', secret)\n                    self.redirect(\'/\')\n                else:\n                    self.set_status(http.client.UNAUTHORIZED)\n                    self.write(app._get_html(\'login.html\'))\n                    self.finish()\n\n        return LoginHandler\n\n    def base_routes(self):\n        return [\n            (r\'/\', self.ui_handler()),\n            (r\'/zounds/(.+?)/(.+?)/?\', self.feature_handler()),\n            (r\'/zounds/login/?\', self.login_handler())\n        ]\n\n    def custom_routes(self):\n        return []\n\n    def _secure_route(self, route):\n\n        pattern, handler_cls = route\n\n        if \'login\' in pattern:\n            return pattern, handler_cls\n\n        class Secured(handler_cls):\n            def __init__(self, *args, **kwargs):\n                super(Secured, self).__init__(*args, **kwargs)\n\n            def get_current_user(self):\n                return self.get_secure_cookie(\'user\')\n\n            @tornado.web.authenticated\n            def get(self, *args, **kwargs):\n                super(Secured, self).get(*args, **kwargs)\n\n            @tornado.web.authenticated\n            def post(self, *args, **kwargs):\n                super(Secured, self).post(*args, **kwargs)\n\n            @tornado.web.authenticated\n            def put(self, *args, **kwargs):\n                super(Secured, self).put(*args, **kwargs)\n\n            @tornado.web.authenticated\n            def delete(self, *args, **kwargs):\n                super(Secured, self).delete(*args, **kwargs)\n\n            @tornado.web.authenticated\n            def head(self, *args, **kwargs):\n                super(Secured, self).head(*args, **kwargs)\n\n            @tornado.web.authenticated\n            def options(self, *args, **kwargs):\n                super(Secured, self).options(*args, **kwargs)\n\n        Secured.__name__ = handler_cls.__name__\n        Secured.__module__ = handler_cls.__module__\n\n        return pattern, Secured\n\n    def _make_app(self):\n        routes = self.custom_routes() + self.base_routes()\n\n        if self.secret:\n            routes = list(map(self._secure_route, routes))\n\n        return tornado.web.Application(\n            routes,\n            cookie_secret=uuid4().hex,\n            login_url=\'/zounds/login\')\n\n    def start_in_thread(self, port=8888):\n        self.thread = threading.Thread(target=self._start, args=(port,))\n        self.thread.daemon = True\n        self.thread.start()\n        print(\'Interactive REPL at http://localhost:{port}\'.format(port=port))\n        return self\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        ioloop = tornado.ioloop.IOLoop.instance()\n        ioloop.add_callback(ioloop.stop)\n        self.server.stop()\n        self.thread.join()\n\n    def _start(self, port):\n        asyncio.set_event_loop(asyncio.new_event_loop())\n        app = self._make_app()\n        self.server = app.listen(port)\n        tornado.ioloop.IOLoop.instance().start()\n\n    def start(self, port=8888):\n        print(\'Interactive REPL at http://localhost:{port}\'.format(**locals()))\n        self._start(port)\n'"
zounds/ui/cli.py,0,"b""import argparse\n\n\nclass BasePartialArgumentParser(argparse.ArgumentParser):\n    def __init__(self, groupname, group_description):\n        super(BasePartialArgumentParser, self).__init__(add_help=False)\n        self.group = self.add_argument_group(groupname, group_description)\n\n    def add_argument(self, *args, **kwargs):\n        self.group.add_argument(*args, **kwargs)\n\n\nclass ObjectStorageSettings(BasePartialArgumentParser):\n    def __init__(self):\n        super(ObjectStorageSettings, self).__init__(\n            'object_storage',\n            'Rackspace object storage settings for model checkpoint storage')\n        self.add_argument(\n            '--object-storage-region',\n            help='the rackspace object storage region',\n            default='DFW')\n        self.add_argument(\n            '--object-storage-username',\n            help='rackspace cloud username',\n            required=True)\n        self.add_argument(\n            '--object-storage-api-key',\n            help='rackspace cloud api key',\n            required=True)\n\n\nclass AppSettings(BasePartialArgumentParser):\n    def __init__(self):\n        super(AppSettings, self).__init__(\n            'app',\n            'In-browser REPL settings')\n        self.add_argument(\n            '--app-secret',\n            help='app password. If not provided, REPL is public',\n            required=False)\n        self.add_argument(\n            '--port',\n            help='The port on which the In-Browser REPL app should listen',\n            default=8888)\n\n\nclass NeuralNetworkTrainingSettings(BasePartialArgumentParser):\n    def __init__(self):\n        super(NeuralNetworkTrainingSettings, self).__init__(\n            'training',\n            'Common settings for training neural networks')\n        self.add_argument(\n            '--epochs',\n            help='how many passes over the data should be made during training',\n            type=int)\n        self.add_argument(\n            '--batch-size',\n            help='how many examples constitute a minibatch?',\n            type=int,\n            default=64)\n        self.add_argument(\n            '--nsamples',\n            help='the number of samples to draw from the database for training',\n            type=int)\n"""
zounds/ui/contentrange.py,0,"b'import re\nfrom zounds.timeseries import Seconds, Picoseconds, TimeSlice\n\n\nclass RangeUnitUnsupportedException(Exception):\n    """"""\n    Raised when an HTTP range request is made with a unit not supported by this\n    application\n    """"""\n    pass\n\n\nclass UnsatisfiableRangeRequestException(Exception):\n    """"""\n    Exception raised when an HTTP range request cannot be satisfied for a\n    particular resource\n    """"""\n    pass\n\n\nclass RangeRequest(object):\n    def __init__(self, range_header):\n        self.range_header = range_header\n        self.re = re.compile(\n                r\'^(?P<unit>[^=]+)=(?P<start>[^-]+)-(?P<stop>.*?)$\')\n\n    def time_slice(self, start, stop):\n        start = float(start)\n        try:\n            stop = float(stop)\n        except ValueError:\n            stop = None\n        duration = \\\n            None if stop is None else Picoseconds(int(1e12 * (stop - start)))\n        start = Picoseconds(int(1e12 * start))\n        return TimeSlice(duration, start=start)\n\n    def byte_slice(self, start, stop):\n        start = int(start)\n        try:\n            stop = int(stop)\n        except ValueError:\n            stop = None\n        return slice(start, stop)\n\n    def range(self):\n        raw = self.range_header\n        if not raw:\n            return slice(None)\n\n        m = self.re.match(raw)\n        if not m:\n            return slice(None)\n\n        units = m.groupdict()[\'unit\']\n        start = m.groupdict()[\'start\']\n        stop = m.groupdict()[\'stop\']\n\n        if units == \'bytes\':\n            return self.byte_slice(start, stop)\n        elif units == \'seconds\':\n            return self.time_slice(start, stop)\n        else:\n            raise RangeUnitUnsupportedException(units)\n\n\nclass ContentRange(object):\n    def __init__(self, unit, start, total, stop=None):\n        self.unit = unit\n        self.total = total\n        self.stop = stop\n        self.start = start\n\n    @staticmethod\n    def from_timeslice(timeslice, total):\n        one_second = Seconds(1)\n        stop = None\n        start = timeslice.start / one_second\n        if timeslice.duration is not None:\n            stop = start + (timeslice.duration / one_second)\n        return ContentRange(\n                \'seconds\',\n                timeslice.start / one_second,\n                total / one_second,\n                stop)\n\n    @staticmethod\n    def from_slice(slce, total):\n        return ContentRange(\'bytes\', slce.start, total, slce.stop)\n\n    def __str__(self):\n        unit = self.unit\n        start = self.start\n        stop = self.stop or self.total\n        total = self.total\n        return \'{unit} {start}-{stop}/{total}\'.format(**locals())\n'"
zounds/ui/featureparser.py,0,"b'import ast\n\n\nclass ExpressionVisitor(ast.NodeVisitor):\n    def __init__(self, document, locals):\n        super(ExpressionVisitor, self).__init__()\n        self.locals = locals\n        self.document = document\n        self.feature_name = None\n        self.doc = None\n\n    @property\n    def result(self):\n        if self.feature_name:\n            feature = self.document.features[self.feature_name]\n        else:\n            feature = None\n        return self.doc, feature\n\n    def visit_Expr(self, node):\n        if self.document is None:\n            raise ValueError()\n\n        children = list(ast.iter_child_nodes(node))\n        if len(children) != 1:\n            raise ValueError()\n\n        feature_name = None\n\n        child = children[0]\n        if isinstance(child, ast.Attribute) \\\n                and child.attr in self.document.features:\n            feature_name = child.attr\n        else:\n            raise ValueError()\n\n        grandchildren = list(ast.iter_child_nodes(child))\n        if len(grandchildren) != 2:\n            raise ValueError()\n\n        grandchild = grandchildren[0]\n        if isinstance(grandchild, ast.Name) \\\n                and grandchild.id in self.locals \\\n                and isinstance(self.locals[grandchild.id], self.document):\n            self.doc = self.locals[grandchild.id]\n            self.feature_name = feature_name\n        else:\n            raise ValueError()\n\n\nclass FeatureParser(object):\n    def __init__(self, document, locals):\n        super(FeatureParser, self).__init__()\n        self.visitor = ExpressionVisitor(document, locals)\n\n    def parse_feature(self, statement):\n        root = ast.parse(statement)\n        try:\n            self.visitor.visit(root)\n        except ValueError:\n            pass\n        return self.visitor.result\n'"
zounds/ui/search.py,0,"b""import tornado\nfrom .baseapp import BaseZoundsApp, RequestContext\nimport base64\nimport urllib.request, urllib.parse, urllib.error\n\n\nclass ZoundsSearch(BaseZoundsApp):\n    def __init__(\n            self,\n            base_path=r'/zounds/',\n            model=None,\n            visualization_feature=None,\n            audio_feature=None,\n            search=None,\n            n_results=10):\n        super(ZoundsSearch, self).__init__(\n                base_path=base_path,\n                model=model,\n                visualization_feature=visualization_feature,\n                audio_feature=audio_feature,\n                html='search.html')\n        self.n_results = n_results\n        self.search = search\n\n    def custom_routes(self):\n        return [\n            (r'/zounds/search', self.search_handler())\n        ]\n\n    def search_handler(self):\n        app = self\n\n        class SearchHandler(tornado.web.RequestHandler):\n            def get(self):\n                b64_encoded_query = urllib.parse.unquote(\n                        self.get_argument('query', default=''))\n                if b64_encoded_query:\n                    binary_query = base64.b64decode(b64_encoded_query)\n                    query = app.search.decode_query(binary_query)\n                    results = app.search.search(query, n_results=app.n_results)\n                else:\n                    results = app.search.random_search(n_results=app.n_results)\n                context = RequestContext(value=results)\n                output = app.serialize(context)\n                self.set_header('Content-Type', output.content_type)\n                self.write(output.data)\n\n        return SearchHandler\n"""
zounds/ui/serializer.py,5,"b'import datetime\nfrom zounds.persistence import ArrayWithUnitsFeature\nfrom zounds.timeseries import Seconds, Picoseconds, TimeSlice, AudioSamples\nfrom zounds.segment import TimeSliceFeature\nfrom zounds.index import SearchResults\nfrom zounds.soundfile import OggVorbisFeature\nfrom soundfile import SoundFile\nfrom .contentrange import ContentRange\nimport numpy as np\nfrom featureflow import Decoder\nimport matplotlib\nimport json\nimport base64\n\nmatplotlib.use(\'Agg\')\nfrom matplotlib import pyplot as plt\nfrom io import BytesIO\n\n\nclass TempResult(object):\n    def __init__(\n            self,\n            data,\n            content_type,\n            is_partial=False,\n            content_range=None):\n        self.content_range = content_range\n        self.data = data\n        self.content_type = content_type\n        self.timestamp = datetime.datetime.utcnow()\n        self.is_partial = is_partial\n\n\nclass DefaultSerializer(object):\n    def __init__(self, content_type):\n        super(DefaultSerializer, self).__init__()\n        self._content_type = content_type\n\n    def matches(self, context):\n        if context.feature is None:\n            return False\n        return context.feature.encoder.content_type == self._content_type\n\n    @property\n    def content_type(self):\n        return self._content_type\n\n    def serialize(self, context):\n        document = context.document\n        feature = context.feature\n        slce = context.slce\n        flo = feature(_id=document._id, decoder=Decoder(), persistence=document)\n        if slce.start:\n            flo.seek(slce.start)\n        if slce.stop:\n            value = flo.read(slce.stop - slce.start)\n        else:\n            value = flo.read()\n        key = document.key_builder.build(\n                document._id, feature.key, feature.version)\n        total = document.database.size(key)\n        return TempResult(\n                value,\n                self.content_type,\n                is_partial=slce.start is not None or slce.stop is not None,\n                content_range=ContentRange.from_slice(slce, total))\n\n\nclass AudioSamplesSerializer(object):\n    def __init__(self):\n        super(AudioSamplesSerializer, self).__init__()\n\n    def matches(self, context):\n        return isinstance(context.value, AudioSamples)\n\n    @property\n    def content_type(self):\n        return \'audio/ogg\'\n\n    def serialize(self, context):\n        bio = BytesIO()\n        samples = context.value\n        with SoundFile(\n                bio,\n                mode=\'w\',\n                samplerate=samples.samples_per_second,\n                channels=samples.channels,\n                format=\'OGG\',\n                subtype=\'VORBIS\') as sf:\n            for i in range(0, len(samples), samples.samples_per_second):\n                sf.write(samples[i: i + samples.samples_per_second])\n        bio.seek(0)\n        return TempResult(bio.read(), \'audio/ogg\')\n\n\nclass OggVorbisSerializer(object):\n    """"""\n    Serializer capable of handling range requests against ogg vorbis files\n    """"""\n\n    def __init__(self):\n        super(OggVorbisSerializer, self).__init__()\n\n    def matches(self, context):\n        if context.feature is None:\n            return False\n        return \\\n            isinstance(context.feature, OggVorbisFeature) \\\n            and isinstance(context.slce, TimeSlice)\n\n    @property\n    def content_type(self):\n        return \'audio/ogg\'\n\n    def serialize(self, context):\n        feature = context.feature\n        document = context.document\n        slce = context.slce\n        wrapper = feature(_id=document._id, persistence=document)\n        samples = wrapper[slce]\n        bio = BytesIO()\n        with SoundFile(\n                bio,\n                mode=\'w\',\n                samplerate=wrapper.samplerate,\n                channels=wrapper.channels,\n                format=\'OGG\',\n                subtype=\'VORBIS\') as sf:\n            sf.write(samples)\n        bio.seek(0)\n        content_range = ContentRange.from_timeslice(\n                slce, Picoseconds(int(1e12 * wrapper.duration_seconds)))\n        return TempResult(\n                bio.read(),\n                \'audio/ogg\',\n                is_partial=slce != TimeSlice(),\n                content_range=content_range)\n\n\ndef generate_image(data, is_partial=False, content_range=None):\n    fig = plt.figure()\n    if data.ndim == 1:\n        plt.plot(data)\n    elif data.ndim == 2:\n        data = np.asarray(data).real\n        mat = plt.matshow(np.rot90(data), cmap=plt.cm.viridis)\n        mat.axes.get_xaxis().set_visible(False)\n        mat.axes.get_yaxis().set_visible(False)\n    elif data.ndim == 3 and data.shape[-1] in (3, 4):\n        data = np.array(data)\n        mat = plt.imshow(np.rot90(data))\n        mat.axes.get_xaxis().set_visible(False)\n        mat.axes.get_yaxis().set_visible(False)\n    else:\n        raise ValueError(\'cannot handle dimensions > 3\')\n    bio = BytesIO()\n    plt.savefig(bio, bbox_inches=\'tight\', pad_inches=0, format=\'png\')\n    bio.seek(0)\n    fig.clf()\n    plt.close(\'all\')\n    return TempResult(\n            bio.read(),\n            \'image/png\',\n            is_partial=is_partial,\n            content_range=content_range)\n\n\nclass ConstantRateTimeSeriesSerializer(object):\n    def __init__(self):\n        super(ConstantRateTimeSeriesSerializer, self).__init__()\n\n    def matches(self, context):\n        return \\\n            isinstance(context.feature, ArrayWithUnitsFeature) \\\n            and isinstance(context.slce, TimeSlice)\n\n    @property\n    def content_type(self):\n        return \'image/png\'\n\n    def serialize(self, context):\n        feature = context.feature\n        document = context.document\n        data = feature(_id=document._id, persistence=document)\n        sliced_data = data[context.slce]\n        td = data.dimensions[0]\n        content_range = ContentRange.from_timeslice(\n                context.slce, td.end)\n        return generate_image(\n                sliced_data,\n                is_partial=True,\n                content_range=content_range)\n\n\nclass NumpySerializer(object):\n    def __init__(self):\n        super(NumpySerializer, self).__init__()\n\n    def matches(self, context):\n        return \\\n            isinstance(context.value, np.ndarray) \\\n            and len(context.value.shape) in (1, 2, 3)\n\n    @property\n    def content_type(self):\n        return \'image/png\'\n\n    def serialize(self, context):\n        feature = context.feature\n        document = context.document\n        value = context.value\n        if value is None:\n            data = feature(_id=document._id, persistence=document)\n        else:\n            data = value\n        return generate_image(data)\n\n\nclass AudioSliceSerializer(object):\n    def __init__(\n            self,\n            content_type,\n            visualization_feature,\n            audio_feature,\n            path_builder):\n        self.audio_feature = audio_feature\n        self.visualization_feature = visualization_feature\n        self._content_type = content_type\n        self._path_builder = path_builder\n\n    @property\n    def content_type(self):\n        return self._content_type\n\n    def _seconds(self, ts):\n        return {\n            \'start\': ts.start / Seconds(1),\n            \'duration\': ts.duration / Seconds(1)\n        }\n\n    def _result(self, ts, _id):\n        return {\n            \'audio\': self._path_builder(_id, self.audio_feature.key),\n            \'visualization\': self._path_builder(\n                    _id, self.visualization_feature.key),\n            \'slice\': self._seconds(ts)\n        }\n\n    def iter_results(self, context):\n        raise NotImplementedError()\n\n    def additional_data(self, context):\n        return dict()\n\n    def serialize(self, context):\n        results = [self._result(*x) for x in self.iter_results(context)]\n        output = {\'results\': results}\n        output.update(self.additional_data(context))\n        return TempResult(json.dumps(output), self.content_type)\n\n\nclass OnsetsSerializer(AudioSliceSerializer):\n    def __init__(self, visualization_feature, audio_feature, path_builder):\n        super(OnsetsSerializer, self).__init__(\n                \'application/vnd.zounds.onsets+json\',\n                visualization_feature,\n                audio_feature,\n                path_builder)\n\n    def matches(self, context):\n        return isinstance(context.feature, TimeSliceFeature)\n\n    def iter_results(self, context):\n        for ts in context.value.slices:\n            yield ts, context.document._id\n\n\nclass SearchResultsSerializer(AudioSliceSerializer):\n    def __init__(self, visualization_feature, audio_feature, path_builder):\n        super(SearchResultsSerializer, self).__init__(\n                \'application/vnd.zounds.searchresults+json\',\n                visualization_feature,\n                audio_feature,\n                path_builder)\n\n    def matches(self, context):\n        return isinstance(context.value, SearchResults)\n\n    def additional_data(self, context):\n        return {\'query\': base64.b64encode(context.value.query)}\n\n    def iter_results(self, context):\n        for _id, ts in context.value:\n            yield ts, _id\n'"
zounds/ui/test_api.py,0,"b""import unittest2\nfrom .contentrange import \\\n    ContentRange, RangeUnitUnsupportedException, RangeRequest\nfrom .baseapp import BaseZoundsApp\nfrom zounds.timeseries import TimeSlice, Seconds, Picoseconds, Milliseconds\n\n\nclass ContentRangeTests(unittest2.TestCase):\n    def test_open_content_range(self):\n        self.assertEqual(\n                'bytes 10-100/100',\n                str(ContentRange('bytes', 10, 100)))\n\n    def test_closed_content_range(self):\n        self.assertEqual(\n                'seconds 10-90/100',\n                str(ContentRange('seconds', 10, 100, stop=90)))\n\n    def test_from_timeslce_full_slice(self):\n        ts = TimeSlice()\n        self.assertEqual(\n                'seconds 0.0-100.0/100.0',\n                str(ContentRange.from_timeslice(ts, Seconds(100))))\n\n    def test_from_timeslice_open_ended(self):\n        ts = TimeSlice(start=Picoseconds(int(1e12)) * 2.5)\n        self.assertEqual(\n                'seconds 2.5-100.0/100.0',\n                str(ContentRange.from_timeslice(ts, Seconds(100))))\n\n    def test_from_timeslice_closed(self):\n        ts = TimeSlice(\n                start=Picoseconds(int(1e12)) * 2.5,\n                duration=Milliseconds(2000))\n        self.assertEqual(\n                'seconds 2.5-4.5/100.0',\n                str(ContentRange.from_timeslice(ts, Seconds(100))))\n\n\nclass RangeRequestTests(unittest2.TestCase):\n    def test_raises_for_unsupported_unit(self):\n        rr = RangeRequest('hours=1-2')\n        self.assertRaises(RangeUnitUnsupportedException, lambda: rr.range())\n\n    def test_can_get_open_ended_byte_slice(self):\n        rr = RangeRequest('bytes=10-')\n        sl = rr.range()\n        self.assertIsInstance(sl, slice)\n        self.assertEqual((10, None, None), (sl.start, sl.stop, sl.step))\n\n    def test_can_get_closed_byte_slice(self):\n        rr = RangeRequest('bytes=10-100')\n        sl = rr.range()\n        self.assertIsInstance(sl, slice)\n        self.assertEqual((10, 100, None), (sl.start, sl.stop, sl.step))\n\n    def test_can_get_open_ended_time_slice(self):\n        rr = RangeRequest('seconds=0-')\n        sl = rr.range()\n        self.assertIsInstance(sl, TimeSlice)\n        self.assertEqual(TimeSlice(start=Seconds(0)), sl)\n\n    def test_can_get_closed_time_slice(self):\n        rr = RangeRequest('seconds=10.5-100.5')\n        sl = rr.range()\n        self.assertIsInstance(sl, TimeSlice)\n        expected_start = Picoseconds(int(10.5 * 1e12))\n        expected_duration = Picoseconds(int(90 * 1e12))\n        self.assertEqual(\n                TimeSlice(start=expected_start, duration=expected_duration), sl)\n\n\nclass ZoundsAppTests(unittest2.TestCase):\n    def test_feature_paths_are_url_encoded(self):\n        app = BaseZoundsApp(\n                base_path='/zounds/',\n                html='index.html')\n        _id = 'http://example.com/resource'\n        feature = 'bark'\n        path = app.feature_path(_id, feature)\n        expected = '/zounds/http%3A%2F%2Fexample.com%2Fresource/bark'\n        self.assertEqual(expected, path)\n"""
zounds/ui/test_featureparser.py,0,"b""import unittest2\nfrom .featureparser import FeatureParser\nfrom zounds.basic import stft\nfrom zounds.util import simple_in_memory_settings\nfrom zounds.synthesize import NoiseSynthesizer\nfrom zounds.timeseries import SR44100, Seconds\n\n\nclass SomethingElse(object):\n    def __init__(self, fft):\n        super(SomethingElse, self).__init__()\n        self.fft = fft\n\nclass FeatureParserTests(unittest2.TestCase):\n\n    def setUp(self):\n        @simple_in_memory_settings\n        class Document(stft(store_fft=True)):\n            pass\n\n        synth = NoiseSynthesizer(SR44100())\n        audio = synth.synthesize(Seconds(2))\n\n        _id = Document.process(meta=audio.encode())\n        doc = Document(_id)\n\n        non_doc = SomethingElse(11)\n\n        parser = FeatureParser(Document, locals())\n\n        self.document = Document\n        self.doc = doc\n        self.parser = parser\n\n    def test_can_extract_feature(self):\n        parsed_doc, feature = self.parser.parse_feature('doc.fft')\n        self.assertIs(parsed_doc, self.doc)\n        self.assertIs(feature, self.document.features['fft'])\n\n    def test_can_ignore_feature_in_larger_statement(self):\n        parsed_doc, feature = self.parser.parse_feature('doc.fft.shape')\n        self.assertIsNone(parsed_doc)\n        self.assertIsNone(feature)\n\n    def test_can_ignore_feauture_in_expression(self):\n        parsed_doc, feature = self.parser.parse_feature('doc.fft *= 10')\n        self.assertIsNone(parsed_doc)\n        self.assertIsNone(feature)\n\n    def test_can_ignore_feature_in_multiply_expression(self):\n        parsed_doc, feature = self.parser.parse_feature('doc.fft * 10')\n        self.assertIsNone(parsed_doc)\n        self.assertIsNone(feature)\n\n    def test_can_ignore_static_feature_access(self):\n        parsed_doc, feature = self.parser.parse_feature('Document.fft')\n        self.assertIsNone(parsed_doc)\n        self.assertIsNone(feature)\n\n    def test_can_ignore_non_document_with_matching_attribute_name(self):\n        parsed_doc, feature = self.parser.parse_feature('non_doc.fft')\n        self.assertIsNone(parsed_doc)\n        self.assertIsNone(feature)\n\n    def test_can_ignore_when_document_is_not_supplied(self):\n        non_doc = SomethingElse(11)\n        parser = FeatureParser(None, locals())\n        parsed_doc, feature = parser.parse_feature('non_doc.fft')\n        self.assertIsNone(parsed_doc)\n        self.assertIsNone(feature)\n"""
zounds/ui/training_monitor.py,0,"b""from io import BytesIO\n\nfrom .api import ZoundsApp\nimport tornado.websocket\nimport tornado.web\nimport json\nfrom collections import defaultdict\n\nimport matplotlib\n\nmatplotlib.use('Agg')\nfrom matplotlib import pyplot as plt\n\n\nclass TrainingMonitorApp(ZoundsApp):\n    def __init__(\n            self,\n            trainer,\n            keys_to_graph,\n            batch_frequency=10,\n            n_training_points=100,\n            epoch_key='epoch',\n            batch_key='batch',\n            base_path=r'/zounds/',\n            model=None,\n            visualization_feature=None,\n            audio_feature=None,\n            globals={},\n            locals={},\n            secret=None):\n\n        super(TrainingMonitorApp, self).__init__(\n            base_path=base_path,\n            model=model,\n            visualization_feature=visualization_feature,\n            audio_feature=audio_feature,\n            globals=globals,\n            locals=locals,\n            html='training_monitor.html',\n            secret=secret)\n\n        self.n_training_points = n_training_points\n        self.batch_frequency = batch_frequency\n        self.batch_key = batch_key\n        self.epoch_key = epoch_key\n        self.keys_to_graph = keys_to_graph\n        self.trainer = trainer\n        self.training_history = defaultdict(list)\n        self.trainer.register_batch_complete_callback(\n            self._collect_training_history)\n\n    def custom_routes(self):\n        routes = super(TrainingMonitorApp, self).custom_routes()\n        routes.extend([\n            (r'/zounds/training/?', self.training_handler()),\n            (r'/zounds/graph/?', self.graph_handler())\n        ])\n        return routes\n\n    def _collect_training_history(self, *args, **kwargs):\n        batch = kwargs['batch']\n\n        if batch % self.batch_frequency:\n            return\n\n        for k in self.keys_to_graph:\n            # truncate\n            self.training_history[k] = \\\n                self.training_history[k][-self.n_training_points:]\n            # append the new data\n            try:\n                self.training_history[k].append(kwargs[k])\n            except KeyError:\n                # no data has been added for this key\n                pass\n\n    def training_handler(self):\n        app = self\n\n        class TrainingHandler(tornado.websocket.WebSocketHandler):\n            def _send_message(self):\n                def x(*args, **kwargs):\n                    batch = kwargs['batch']\n\n                    if batch % app.batch_frequency:\n                        return\n\n                    data = dict(epoch=kwargs['epoch'], batch=batch)\n                    for key in app.keys_to_graph:\n                        try:\n                            data[key] = kwargs[key]\n                        except KeyError:\n                            # there's no data to report for this key\n                            pass\n                    self.write_message(json.dumps(data))\n\n                return x\n\n            def open(self):\n                self.func = self._send_message()\n                app.trainer \\\n                    .register_batch_complete_callback(self.func)\n\n            def on_close(self):\n                app.trainer \\\n                    .unregister_batch_complete_callback(self.func)\n\n        return TrainingHandler\n\n    def graph_handler(self):\n        app = self\n\n        class GraphHandler(tornado.web.RequestHandler):\n            def get(self):\n                plt.style.use('dark_background')\n\n                fig = plt.figure()\n                handles = []\n                for k in app.keys_to_graph:\n                    handle, = plt.plot(app.training_history[k], label=k)\n                    handles.append(handle)\n                plt.legend(handles=handles)\n\n                bio = BytesIO()\n                plt.savefig(\n                    bio, bbox_inches='tight', pad_inches=0, format='png')\n                bio.seek(0)\n                fig.clf()\n                plt.close('all')\n                self.set_header('Content-Type', 'image/png')\n                self.write(bio.read())\n                self.finish()\n\n        return GraphHandler\n\n\nclass SupervisedTrainingMonitorApp(TrainingMonitorApp):\n    def __init__(\n            self,\n            trainer,\n            batch_frequency=10,\n            n_training_points=100,\n            epoch_key='epoch',\n            batch_key='batch',\n            base_path=r'/zounds',\n            model=None,\n            visualization_feature=None,\n            audio_feature=None,\n            globals={},\n            locals={},\n            secret=None):\n        super(SupervisedTrainingMonitorApp, self).__init__(\n            trainer=trainer,\n            keys_to_graph=('train_error', 'test_error'),\n            model=model,\n            batch_frequency=batch_frequency,\n            n_training_points=n_training_points,\n            epoch_key=epoch_key,\n            batch_key=batch_key,\n            base_path=base_path,\n            visualization_feature=visualization_feature,\n            audio_feature=audio_feature,\n            globals=globals,\n            locals=locals,\n            secret=secret)\n\n\nclass GanTrainingMonitorApp(TrainingMonitorApp):\n    def __init__(\n            self,\n            trainer,\n            batch_frequency=10,\n            n_training_points=100,\n            epoch_key='epoch',\n            batch_key='batch',\n            base_path=r'/zounds',\n            model=None,\n            visualization_feature=None,\n            audio_feature=None,\n            globals={},\n            locals={},\n            secret=None):\n        super(GanTrainingMonitorApp, self).__init__(\n            trainer=trainer,\n            keys_to_graph=('generator_score', 'real_score', 'critic_loss'),\n            model=model,\n            batch_frequency=batch_frequency,\n            n_training_points=n_training_points,\n            epoch_key=epoch_key,\n            batch_key=batch_key,\n            base_path=base_path,\n            visualization_feature=visualization_feature,\n            audio_feature=audio_feature,\n            globals=globals,\n            locals=locals,\n            secret=secret)\n\n\nclass TripletEmbeddingMonitorApp(TrainingMonitorApp):\n    def __init__(\n            self,\n            trainer,\n            batch_frequency=10,\n            n_training_points=100,\n            epoch_key='epoch',\n            batch_key='batch',\n            base_path=r'/zounds',\n            model=None,\n            visualization_feature=None,\n            audio_feature=None,\n            globals={},\n            locals={},\n            secret=None):\n        super(TripletEmbeddingMonitorApp, self).__init__(\n            trainer=trainer,\n            keys_to_graph=('error',),\n            model=model,\n            batch_frequency=batch_frequency,\n            n_training_points=n_training_points,\n            epoch_key=epoch_key,\n            batch_key=batch_key,\n            base_path=base_path,\n            visualization_feature=visualization_feature,\n            audio_feature=audio_feature,\n            globals=globals,\n            locals=locals,\n            secret=secret)\n"""
zounds/util/__init__.py,0,"b'from .persistence import \\\n    simple_lmdb_settings, simple_in_memory_settings, \\\n    simple_object_storage_settings\n\nfrom .handy import tuplify\n\nfrom .midi import note_to_midi, midi_to_note, midi_instrument\n'"
zounds/util/handy.py,0,"b'\ndef tuplify(a):\n    try:\n        return tuple(a)\n    except TypeError:\n        return (a,)\n\n\n'"
zounds/util/midi.py,0,"b""NOTES = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\nNOTE_INDICES = {n: i for i, n in enumerate(NOTES)}\n\n\ndef midi_to_note(midi_number):\n    octave = midi_number // len(NOTES)\n    note_name = NOTES[int(midi_number % len(NOTES))]\n    return f'{note_name}{octave}'\n\n\ndef note_to_midi(note):\n    if note[1] == '#':\n        note_name_length = 2\n    else:\n        note_name_length = 1\n\n    note_name, octave = note[:note_name_length], int(note[note_name_length:])\n    return (octave * len(NOTES)) + NOTE_INDICES[note_name]\n\n\ndef midi_instrument(midi_instrument_number):\n    return MIDI_INSTRUMENTS[midi_instrument_number]\n\n\nMIDI_INSTRUMENTS = {\n    0: 'Acoustic Grand Piano',\n    1: 'Bright Acoustic Piano',\n    2: 'Electric Grand Piano',\n    3: 'Honky-tonk Piano',\n    4: 'Rhodes Piano',\n    5: 'Chorused Piano',\n    6: 'Harpsichord',\n    7: 'Clavinet',\n    8: 'Celesta',\n    9: 'Glockenspiel',\n    10: 'Music box',\n    11: 'Vibraphone',\n    12: 'Marimba',\n    13: 'Xylophone',\n    14: 'Tubular Bells',\n    15: 'Dulcimer',\n    16: 'Hammond Organ',\n    17: 'Percussive Organ',\n    18: 'Rock Organ',\n    19: 'Church Organ',\n    20: 'Reed Organ',\n    21: 'Accordion',\n    22: 'Harmonica',\n    23: 'Tango Accordion',\n    24: 'Acoustic Guitar (nylon)',\n    25: 'Acoustic Guitar (steel)',\n    26: 'Electric Guitar (jazz)',\n    27: 'Electric Guitar (clean)',\n    28: 'Electric Guitar (muted)',\n    29: 'Overdriven Guitar',\n    30: 'Distortion Guitar',\n    31: 'Guitar Harmonics',\n    32: 'Acoustic Bass',\n    33: 'Electric Bass (finger)',\n    34: 'Electric Bass (pick)',\n    35: 'Fretless Bass',\n    36: 'Slap Bass 1',\n    37: 'Slap Bass 2',\n    38: 'Synth Bass 1',\n    39: 'Synth Bass 2',\n    40: 'Violin',\n    41: 'Viola',\n    42: 'Cello',\n    43: 'Contrabass',\n    44: 'Tremolo Strings',\n    45: 'Pizzicato Strings',\n    46: 'Orchestral Harp',\n    47: 'Timpani',\n    48: 'String Ensemble 1',\n    49: 'String Ensemble 2',\n    50: 'Synth Strings 1',\n    51: 'Synth Strings 2',\n    52: 'Choir Aahs',\n    53: 'Voice Oohs',\n    54: 'Synth Voice',\n    55: 'Orchestra Hit',\n    56: 'Trumpet',\n    57: 'Trombone',\n    58: 'Tuba',\n    59: 'Muted Trumpet',\n    60: 'French Horn',\n    61: 'Brass Section',\n    62: 'Synth Brass 1',\n    63: 'Synth Brass 2',\n    64: 'Soprano Sax',\n    65: 'Alto Sax',\n    66: 'Tenor Sax',\n    67: 'Baritone Sax',\n    68: 'Oboe',\n    69: 'English Horn',\n    70: 'Bassoon',\n    71: 'Clarinet',\n    72: 'Piccolo',\n    73: 'Flute',\n    74: 'Recorder',\n    75: 'Pan Flute',\n    76: 'Bottle Blow',\n    77: 'Shakuhachi',\n    78: 'Whistle',\n    79: 'Ocarina',\n    80: 'Lead 1 (square)',\n    81: 'Lead 2 (sawtooth)',\n    82: 'Lead 3 (calliope lead)',\n    83: 'Lead 4 (chiffer lead)',\n    84: 'Lead 5 (charang)',\n    85: 'Lead 6 (voice)',\n    86: 'Lead 7 (fifths)',\n    87: 'Lead 8 (brass + lead)',\n    88: 'Pad 1 (new age)',\n    89: 'Pad 2 (warm)',\n    90: 'Pad 3 (polysynth)',\n    91: 'Pad 4 (choir)',\n    92: 'Pad 5 (bowed)',\n    93: 'Pad 6 (metallic)',\n    94: 'Pad 7 (halo)',\n    95: 'Pad 8 (sweep)',\n    96: 'FX 1 (rain)',\n    97: 'FX 2 (soundtrack)',\n    98: 'FX 3 (crystal)',\n    99: 'FX 4 (atmosphere)',\n    100: 'FX 5 (brightness)',\n    101: 'FX 6 (goblins)',\n    102: 'FX 7 (echoes)',\n    103: 'FX 8 (sci-fi)',\n    104: 'Sitar',\n    105: 'Banjo',\n    106: 'Shamisen',\n    107: 'Koto',\n    108: 'Kalimba',\n    109: 'Bagpipe',\n    110: 'Fiddle',\n    111: 'Shana',\n    112: 'Tinkle Bell',\n    113: 'Agogo',\n    114: 'Steel Drums',\n    115: 'Woodblock',\n    116: 'Taiko Drum',\n    117: 'Melodic Tom',\n    118: 'Synth Drum',\n    119: 'Reverse Cymbal',\n    120: 'Guitar Fret Noise',\n    121: 'Breath Noise',\n    122: 'Seashore',\n    123: 'Bird Tweet',\n    124: 'Telephone Ring',\n    125: 'Helicopter',\n    126: 'Applause',\n    127: 'Gunshot'\n}\n"""
zounds/util/persistence.py,0,"b'import featureflow as ff\n\n\ndef simple_lmdb_settings(path, map_size=1e9, user_supplied_id=False):\n    """"""\n    Creates a decorator that can be used to configure sane default LMDB\n    persistence settings for a model\n\n    Args:\n        path (str): The path where the LMDB database files will be created\n        map_size (int): The amount of space to allot for the database\n    """"""\n\n    def decorator(cls):\n        provider = \\\n            ff.UserSpecifiedIdProvider(key=\'_id\') \\\n            if user_supplied_id else ff.UuidProvider()\n\n        class Settings(ff.PersistenceSettings):\n            id_provider = provider\n            key_builder = ff.StringDelimitedKeyBuilder(\'|\')\n            database = ff.LmdbDatabase(\n                    path, key_builder=key_builder, map_size=map_size)\n\n        class Model(cls, Settings):\n            pass\n\n        Model.__name__ = cls.__name__\n        Model.__module__ = cls.__module__\n        return Model\n\n    return decorator\n\n\ndef simple_object_storage_settings(container, region, username, api_key):\n    def decorator(cls):\n        class Settings(ff.PersistenceSettings):\n            id_provider = ff.UuidProvider()\n            key_builder = ff.StringDelimitedKeyBuilder()\n            database = ff.ObjectStoreDatabase(\n                container, username, api_key, region, key_builder=key_builder)\n\n        class Model(cls, Settings):\n            pass\n\n        Model.__name__ = cls.__name__\n        Model.__module__ = cls.__module__\n        return Model\n\n    return decorator\n\n\ndef simple_in_memory_settings(cls):\n    """"""\n    Decorator that returns a class that ""persists"" data in-memory.  Mostly\n     useful for testing\n    :param cls: the class whose features should be persisted in-memory\n    :return: A new class that will persist features in memory\n    """"""\n\n    class Settings(ff.PersistenceSettings):\n        id_provider = ff.UuidProvider()\n        key_builder = ff.StringDelimitedKeyBuilder()\n        database = ff.InMemoryDatabase(key_builder=key_builder)\n\n    class Model(cls, Settings):\n        pass\n\n    Model.__name__ = cls.__name__\n    Model.__module__ = cls.__module__\n    return Model\n\n'"
zounds/util/test_handy.py,0,"b'import unittest2\nfrom .handy import tuplify\n\n\nclass TuplifyTests(unittest2.TestCase):\n    def test_tuple_from_tuple(self):\n        self.assertEqual((1, 2, 3), tuplify((1, 2, 3)))\n\n    def test_tuple_from_list(self):\n        self.assertEqual((1, 2, 3), tuplify([1, 2, 3]))\n\n    def test_tuple_from_integer(self):\n        self.assertEqual((1,), tuplify(1))\n'"
zounds/util/test_midi.py,0,"b""import unittest2\nfrom .midi import midi_to_note, note_to_midi\n\n\nclass MidiTests(unittest2.TestCase):\n\n    def test_midi_to_note_0(self):\n        self.assertEqual('C0', midi_to_note(0))\n\n    def test_midi_to_note_127(self):\n        self.assertEqual('G10', midi_to_note(127))\n\n    def test_midi_to_note_33(self):\n        self.assertEqual('A2', midi_to_note(33))\n\n    def test_midi_to_note_99(self):\n        self.assertEqual('D#8', midi_to_note(99))\n\n    def test_note_to_midi_c0(self):\n        self.assertEqual(0, note_to_midi('C0'))\n\n    def test_note_to_midi_g10(self):\n        self.assertEqual(127, note_to_midi('G10'))\n\n    def test_note_to_midi_a_sharp_7(self):\n        self.assertEqual(94, note_to_midi('A#7'))\n"""
zounds/util/test_persistence.py,0,"b""import unittest2\nfrom .persistence import simple_in_memory_settings\n\n\n@simple_in_memory_settings\nclass X(object):\n    pass\n\n\nclass PersitenceDecoratorTests(unittest2.TestCase):\n    def test_in_memory_settings_maintains_class_name(self):\n        self.assertEqual('X', X.__name__)\n        self.assertNotEqual('zounds.util.persistence', X.__module__)\n"""
