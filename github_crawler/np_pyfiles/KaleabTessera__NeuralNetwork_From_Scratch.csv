file_path,api_count,code
NN.py,20,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n \n""""""A Fully-connected Neural Network, build from scratch using only Numpy (and Sklean for standardizing the data).""""""\n \n# Imports\nimport numpy as np\nfrom sklearn import preprocessing\n \n# NN class\nclass NN:\n    def __init__(self,X,numHiddenLayers,numOfNodesPerHiddenLayer,y,showShapeOfNN=False):\n        self.X = X\n        self.numHiddenLayers = numHiddenLayers\n        self.numOfNodesPerHiddenLayer = numOfNodesPerHiddenLayer\n        self.y = y\n\n        self.w1 =  np.random.rand(self.X.shape[1],numOfNodesPerHiddenLayer[0])\n        self.w2 =  np.random.rand(numOfNodesPerHiddenLayer[0],numOfNodesPerHiddenLayer[1])\n        self.w3 =  np.random.rand(numOfNodesPerHiddenLayer[1],self.y.shape[1])\n\n        if(showShapeOfNN):\n            print(""X"",self.X.shape)\n            print(""w1"",self.w1.shape)\n            print(""w2"",self.w2.shape)\n            print(""w3"",self.w3.shape)\n            print(""y"",self.y.shape)\n\n    #Be wary of exploding gradients with sigmoid\n    def activation(self,x,func=""tanh""):\n        if(func == ""relu""):\n            return x * (x > 0)\n        if(func == ""sigmoid""):\n            return 1.0/(1+ np.exp(-x))\n        if(func == ""tanh""):\n            return np.tanh(x)\n    \n    \n    def activation_der(self,x,func=""tanh""):\n        if(func == ""relu""):\n            return 1. * (x > 0)\n        if(func == ""sigmoid""):\n            return x * (1.0 - x)\n        if(func == ""tanh""):\n            return 1.0 - np.tanh(x)**2\n    \n    def loss(self):\n        # Mean Squared Error\n        return np.mean((self.y - self.pred)**2)\n\n    def loss_dir(self):\n        # Loss Derivative of MSE\n        return 2 *(self.y - self.pred)\n      \n\n    def forward(self):\n        b1 = 1\n        b2 = 1\n        b3 = 1\n\n        # Hidden layers\n        self.h1 = self.activation(np.dot(self.X,self.w1)    + b1)\n        self.h2 = self.activation(np.dot(self.h1,self.w2)   + b2)\n        \n        #Output\n        self.pred = self.activation(np.dot(self.h2,self.w3) + b3)\n    \n\n    def backprop(self,learning_rate=0.0001):\n        # dJQ_dW3 - affect of Weight Layer 3 on cost\n        dS3_dW3 = self.h2\n        S3 = np.dot(self.h2,self.w3)\n        dy_DS3 = self.activation_der(S3)\n        dJQ_dy = self.loss_dir()\n        dJQ_dW3 = np.multiply(dy_DS3,dJQ_dy).T.dot(dS3_dW3)\n\n        # dJQ_dW2 - affect of Weight Layer 2 on cost\n        dS2_dW2 = self.h1\n        S2 = np.dot(self.h1,self.w2)\n        dh2_dS2 = self.activation_der(S2)\n        \n        dS3_dh2 = self.w3\n        dJQ_dh2 = dS3_dh2.dot(np.multiply(dJQ_dy,dy_DS3).T)\n        dJQ_dW2 = dJQ_dh2.dot(np.multiply(dh2_dS2,dS2_dW2)) \n        \n        # dJQ_dW1 - affect of Weight Layer 1 on cost\n        dS1_dW1 = self.X\n        S1 = np.dot(self.X,self.w1)\n        dh1_dS1 = self.activation_der(S1)\n        \n        dS2_dh1 = self.w2\n        dh2_dS2 = self.activation_der(S2)\n        dS3_dh2 = self.w3\n        \n        dJQ_dh1 = np.multiply(dJQ_dh2.T,dh2_dS2).dot(dS2_dh1)  \n        dJQ_dW1 = np.multiply(dJQ_dh1,dh1_dS1).T.dot(dS1_dW1) \n        \n        self.w1 += learning_rate * dJQ_dW1.T\n        self.w2 += learning_rate * dJQ_dW2.T\n        self.w3 += learning_rate * dJQ_dW3.T\n\n    def run(self):\n      for i in np.arange(1000):\n        self.forward()\n        loss = self.loss()\n        print(""Loss: "",loss)\n        self.backprop()\n\n\ndef main():\n    data = np.loadtxt(""data_banknote_authentication.txt"",delimiter=\',\')\n    X = data[:,0:-1]\n    \n    scaler = preprocessing.StandardScaler() \n    X = scaler.fit_transform(X)\n    \n    y = data[:,-1].reshape(X.shape[0],1)\n    numHiddenLayers = 2\n    numNodesPerLayer = [10,10]\n    neural_network = NN(X,numHiddenLayers,numNodesPerLayer,y,True)\n\n\n    neural_network.run()\n   \n\nif __name__ == ""__main__"":\n\tmain()'"
