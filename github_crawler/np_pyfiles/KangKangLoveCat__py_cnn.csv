file_path,api_count,code
main.py,2,"b'# -*- coding:utf-8 -*-\n\n  ##########################\n  #                        #\n  #   @Author: KangKang    #\n  #                        #\n  ##########################\n\nimport numpy as np\nfrom network import *\nfrom mnist_data import Mnist_data\n\ndef calc_accuracy(pred, truth):\n    n = np.size(truth)\n    return np.sum(pred.argmax(1) == truth.reshape(-1)) / (n + 0.)\n\nprint(""Reading Data ..."")\ndata = Mnist_data(\'data/mnist\')\nprint(\'Initing Network ...\')\nconv1 = Convolution(data, kernel_size = 5, num_output = 20)\npool1 = Max_pooling(conv1, kernel_size = 2, stride = 2)\nconv2 = Convolution(pool1, kernel_size = 5, num_output = 50)\npool2 = Max_pooling(conv2, kernel_size = 2, stride = 2)\nflat1 = Flatten(pool2)\nfc1 = Full_connection(flat1, 1024)\nrelu1 = Relu(fc1)\nfc2 = Full_connection(relu1, 10)\nsoftmax = Softmax(fc2)\navg_loss = -1\nbase_lr = 0.01\nfor i in range(10000):\n    learning_rate = base_lr * (1 + 0.0001 * i) ** (-0.75)\n    if i % 100 == 0:\n        print(""Testing ..."")\n        data.set_mode(data.TEST)\n        acc_sum = 0\n        for j in range(100):\n            data.next_batch_test_data()\n            accuracy = calc_accuracy(softmax.forward(), data.get_label())\n            acc_sum = acc_sum + accuracy\n        acc_sum = acc_sum / 100\n        print(""Accuracy = %.4f"" % (acc_sum))\n    data.set_mode(data.TRAIN)\n    data.next_batch_train_data()\n    softmax.forward()\n    loss = softmax.calc_loss(data.get_label())\n    if avg_loss == -1:\n        avg_loss = loss\n    else:\n        avg_loss = avg_loss * 0.9 + 0.1 * loss\n    print(""iter = %-5d\\tLoss = %.4f\\tAvg loss = %.4f\\tLearning rate = %f"" % (i + 1, loss, avg_loss, learning_rate))\n    softmax.backward(learning_rate)\n'"
mnist_data.py,10,"b""# -*- coding: utf-8 -*-\n\n  ##########################\n  #                        #\n  #   @Author: KangKang    #\n  #                        #\n  ##########################\n\nimport os\nimport cv2\nimport struct\nimport numpy as np\n\ndef read_images(bin_file_name):\n    binfile = open(bin_file_name, 'rb')\n    buffers = binfile.read()\n    head = struct.unpack_from('>IIII', buffers, 0)\n    offset = struct.calcsize('>IIII')\n    img_num = head[1]\n    img_width = head[2]\n    img_height = head[3]\n    bits_size = img_num * img_height * img_width\n    raw_imgs = struct.unpack_from('>' + str(bits_size) + 'B', buffers, offset)\n    binfile.close()\n    imgs = np.reshape(raw_imgs, head[1:])\n    return imgs\n\ndef read_labels(bind_file_name):\n    binfile = open(bind_file_name, 'rb')\n    buffers = binfile.read()\n    head = struct.unpack_from('>II', buffers, 0)\n    img_num = head[1]\n    offset = struct.calcsize('>II')\n    raw_labels = struct.unpack_from('>' + str(img_num) + 'B', buffers, offset)\n    binfile.close()\n    labels = np.reshape(raw_labels, [img_num, 1])\n    return labels\n\nclass Mnist_data:\n\n    TRAIN = 'TRAIN'\n    TEST = 'TEST'\n\n    def __init__(self, data_dir):\n        self.mode = self.TRAIN\n        self.train_epoch = 0\n        self.test_eopch = 0\n        self.train_num = 64\n        self.test_num = 100\n        self.num = self.train_num # batch_size\n        self.num_output = 1 # channels\n        self.train_images = read_images(os.path.join(data_dir, 'train-images-idx3-ubyte')) / 256.\n        self.train_labels = read_labels(os.path.join(data_dir, 'train-labels-idx1-ubyte'))\n        self.test_images = read_images(os.path.join(data_dir, 't10k-images-idx3-ubyte')) / 256.\n        self.test_labels = read_labels(os.path.join(data_dir, 't10k-labels-idx1-ubyte'))\n        self.train_img_num, self.output_h, self.output_w = self.train_images.shape\n        self.test_img_num, _, _ = self.test_images.shape\n        self.train_cur_index = 0\n        self.test_cur_index = 0\n\n    def next_batch_train_data(self):\n        if self.train_cur_index + self.num >= self.train_img_num:\n            t1 = np.arange(self.train_cur_index, self.train_img_num)\n            t2 = np.arange(0, self.train_cur_index + self.num - self.train_img_num)\n            self.output_train_index = np.append(t1, t2)\n            self.train_epoch = self.train_epoch + 1\n            self.train_cur_index = self.train_cur_index + self.num - self.train_img_num\n        else:\n            self.output_train_index = np.arange(self.train_cur_index, self.train_cur_index + self.num)\n            self.train_cur_index = self.train_cur_index + self.num\n\n\n    def next_batch_test_data(self):\n        if self.test_cur_index + self.num >= self.test_img_num:\n            t1 = np.arange(self.test_cur_index, self.test_img_num)\n            t2 = np.arange(0, self.test_cur_index + self.num - self.test_img_num)\n            self.output_test_index = np.append(t1, t2)\n            self.test_epoch = self.test_eopch = + 1\n            self.test_cur_index = self.test_cur_index + self.num - self.test_img_num\n        else:\n            self.output_test_index = np.arange(self.test_cur_index, self.test_cur_index + self.num)\n            self.test_cur_index = self.test_cur_index + self.num\n\n\n    def forward(self):\n        if self.mode == self.TRAIN:\n            self.output_images = self.train_images[self.output_train_index].reshape(self.num, 1, self.output_h, self.output_w)\n            self.output_labels = self.train_labels[self.output_train_index].reshape(-1)\n        elif self.mode == self.TEST:\n            self.output_images = self.test_images[self.output_test_index].reshape(self.num, 1, self.output_h, self.output_w)\n            self.output_labels = self.test_labels[self.output_test_index].reshape(-1)\n        else:\n            return None\n        return self.output_images\n\n    def backward(self, diff):\n        pass\n\n    def get_data(self):\n        return self.output_images\n\n    def get_label(self):\n        return self.output_labels\n\n    def get_mode(self):\n        return self.mode\n\n    def set_mode(self, mode):\n        self.mode = mode\n        if self.mode == self.TRAIN:\n            self.num = self.train_num\n        elif self.mode == self.TEST:\n            self.num = self.test_num\n\n"""
network/__init__.py,0,b'# -*- coding: utf-8 -*-\n\nfrom network.relu import Relu\nfrom network.flatten import Flatten\nfrom network.softmax import Softmax\nfrom network.convolution import Convolution\nfrom network.max_pooling import Max_pooling\nfrom network.full_connection import Full_connection\n'
network/convolution.py,17,"b""#-*- coding: utf-8 -*-\n\n  ##########################\n  #                        #\n  #   @Author: KangKang    #\n  #                        #\n  ##########################\n\nimport math\nimport numpy as np\n\ndef conv(X, W, b, stride = 1, padding = 0):\n    n_filters, d_filter, kernel_size, _ = W.shape\n    n_x, d_x, h_x, w_x = X.shape\n    h_out = (h_x - kernel_size + 2 * padding) / stride + 1\n    w_out = (w_x - kernel_size + 2 * padding) / stride + 1\n    h_out, w_out = int(h_out), int(w_out)\n    X_col = im2col(X, kernel_size, padding=padding, stride=stride)\n    W_col = W.reshape(n_filters, -1)\n    out = (np.dot(W_col, X_col).T + b).T\n    out = out.reshape(n_filters, h_out, w_out, n_x)\n    out = out.transpose(3, 0, 1, 2)\n    return out\n\ndef im2col(x, kernel_size, padding=0, stride=1):\n    p = padding\n    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n    k, i, j = get_im2col_indices(x.shape, kernel_size, padding, stride)\n    cols = x_padded[:, k, i, j]\n    C = x.shape[1]\n    cols = cols.transpose(1, 2, 0).reshape(kernel_size ** 2 * C, -1)\n    return cols\n\ndef get_im2col_indices(x_shape, kernel_size, padding=0, stride=1):\n    N, C, H, W = x_shape\n    assert (H + 2 * padding - kernel_size) % stride == 0\n    assert (W + 2 * padding - kernel_size) % stride == 0\n    out_height = int((H + 2 * padding - kernel_size) / stride + 1)\n    out_width = int((W + 2 * padding - kernel_size) / stride + 1)\n    i0 = np.repeat(np.arange(kernel_size), kernel_size)\n    i0 = np.tile(i0, C)\n    i1 = stride * np.repeat(np.arange(out_height), out_width)\n    j0 = np.tile(np.arange(kernel_size), kernel_size * C)\n    j1 = stride * np.tile(np.arange(out_width), out_height)\n    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n    k = np.repeat(np.arange(C), kernel_size * kernel_size).reshape(-1, 1)\n    return (k.astype(int), i.astype(int), j.astype(int))\n\ndef col2im(x, img_shape, kernel_size, padding = 0, stride = 1):\n    x_row_num, x_col_num = x.shape\n    channels, img_height, img_width = img_shape\n    x_width = img_width - kernel_size + padding + 1\n    x_height = img_height - kernel_size + padding + 1\n    assert channels * kernel_size ** 2 == x_row_num\n    assert x_width * x_height == x_col_num\n    x_reshape = x.T.reshape(x_height, x_width, channels, kernel_size, kernel_size)\n    output_padded = np.zeros((channels, img_height + 2 * padding, img_width + 2 * padding))\n    for i in range(x_height):\n        for j in range(x_width):\n            output_padded[:, i * stride : i * stride + kernel_size, j * stride : j * stride + kernel_size] = \\\n                    output_padded[:, i * stride : i * stride + kernel_size, j * stride : j * stride + kernel_size] + \\\n                    x_reshape[i, j, ...]\n    return output_padded[:, padding : img_height + padding, padding : img_width + padding]\n\nclass Convolution:\n\n    def __init__(self, layer, kernel_size = 1, num_output = 1, padding = 0):\n        self.upper_layer = layer\n        self.num = layer.num\n        self.num_input = layer.num_output\n        self.input_h = layer.output_h\n        self.input_w = layer.output_w\n        self.output_h = self.input_h + 2 * padding - kernel_size + 1\n        self.output_w = self.input_w + 2 * padding - kernel_size + 1\n        self.num_output = num_output\n        self.kernel_size = kernel_size\n        self.padding = padding\n        scale = math.sqrt(3. / (self.num_input * kernel_size**2))\n        self.weight = np.random.rand(num_output, self.num_input, kernel_size, kernel_size)\n        self.weight = (self.weight - 0.5) * 2 * scale\n        self.weight_diff_his = np.zeros(self.weight.shape)\n        self.bias = np.zeros((num_output))\n        self.bias_diff_his = np.zeros(self.bias.shape)\n\n    def forward(self):\n        self.input_data = self.upper_layer.forward()\n        self.num = self.upper_layer.num\n        self.output_data = conv(self.input_data, self.weight, self.bias, padding = self.padding)\n        return self.output_data\n\n    def backward(self, diff):\n        self.diff = np.zeros(self.input_data.shape)\n        weight_diff = np.zeros(self.weight.shape)\n        weight_diff = weight_diff.reshape(weight_diff.shape[0], -1)\n        bias_diff = np.zeros((self.num_output))\n        weight_reshape_T = self.weight.reshape(self.weight.shape[0], -1).T\n        for i in range(self.num):\n            input_data_col = im2col(self.input_data[[i]], self.kernel_size, self.padding)\n            weight_diff = weight_diff + diff[i].reshape(diff[i].shape[0], -1).dot(input_data_col.T)\n            bias_diff = bias_diff + np.sum(diff[i].reshape(diff[i].shape[0], -1), 1)\n            tmp_diff = weight_reshape_T.dot(diff[i].reshape(diff[i].shape[0], -1))\n            self.diff[i, ...] = col2im(tmp_diff, self.input_data.shape[1:], self.kernel_size, padding = self.padding)\n        self.weight_diff_his = 0.9 * self.weight_diff_his + weight_diff.reshape(self.weight.shape)\n        self.weight = self.weight * 0.9995 - self.weight_diff_his\n        self.bias_diff_his = 0.9 * self.bias_diff_his + 2 * bias_diff\n        self.bias = self.bias * 0.9995 - self.bias_diff_his\n        self.upper_layer.backward(self.diff)\n\n    def get_output(self):\n        return self.output_data\n"""
network/flatten.py,0,"b'# -*- coding: utf-8 -*-\n\n  ##########################\n  #                        #\n  #   @Author: KangKang    #\n  #                        #\n  ##########################\n\nclass Flatten:\n\n    def __init__(self, layer):\n        self.upper_layer = layer\n        self.num_input = layer.num_output\n        self.input_w = layer.output_w\n        self.input_h = layer.output_h\n        self.output_w = 1\n        self.output_h = 1\n        self.num_output = self.num_input * self.input_h * self.input_w\n        self.num = layer.num\n    \n    def forward(self):\n        self.input_data = self.upper_layer.forward()\n        self.num = self.upper_layer.num\n        self.output_data = self.input_data.reshape(self.input_data.shape[0], -1)\n        return self.output_data\n\n    def backward(self, diff):\n        self.diff = diff.reshape(self.input_data.shape)\n        self.upper_layer.backward(self.diff)\n\n    def get_output(self):\n        return self.output_data\n'"
network/full_connection.py,5,"b'# -*- coding: utf-8 -*-\n\n  ##########################\n  #                        #\n  #   @Author: KangKang    #\n  #                        #\n  ##########################\n\nimport math\nimport numpy as np\n\nclass Full_connection:\n\n    def __init__(self, layer, num_output = 1):\n        self.num = layer.num\n        self.upper_layer = layer\n        self.num_input = layer.num_output\n        self.num_output = num_output\n        self.output_w = 1\n        self.output_h = 1\n        scale = math.sqrt(3. / self.num_input)\n        self.weight = np.random.rand(self.num_input, self.num_output)\n        self.weight = (self.weight - 0.5) * 2 * scale\n        self.weight_diff_his = np.zeros(self.weight.shape)\n        self.bias = np.zeros((num_output))\n        self.bias_diff_his = np.zeros(self.bias.shape)\n\n    def forward(self):\n        self.input_data = self.upper_layer.forward()\n        self.num = self.upper_layer.num\n        input_dims = len(self.input_data.shape)\n        input_cols = self.input_data.reshape(self.input_data.shape[0], -1)\n        self.output_data = input_cols.dot(self.weight) + self.bias\n        return self.output_data\n\n    def backward(self, diff):\n        weight_diff = self.input_data.T.dot(diff)\n        bias_diff = np.sum(diff, axis = 0)\n        self.diff = diff.dot(self.weight.T)\n        self.weight_diff_his = 0.9 * self.weight_diff_his + weight_diff\n        self.weight = self.weight * 0.9995 - self.weight_diff_his\n        self.bias_diff_his = 0.9 * self.bias_diff_his + 2 * bias_diff\n        self.bias = self.bias * 0.9995 - self.bias_diff_his\n        self.upper_layer.backward(self.diff)\n\n    def get_output(self):\n        return self.output_data\n'"
network/max_pooling.py,10,"b""# -*- coding: utf-8 -*-\n\n  ##########################\n  #                        #\n  #   @Author: KangKang    #\n  #                        #\n  ##########################\n\nimport math\nimport numpy as np\n\ndef im2col(X, kernel_size = 1, stride = 1):\n    num, channels, height, width = X.shape\n    surplus_height = (height - kernel_size) % stride\n    surplus_width = (width - kernel_size) % stride\n    pad_h = (kernel_size - surplus_height) % kernel_size\n    pad_w = (kernel_size - surplus_width) % kernel_size\n    X = np.pad(X, ((0,0),(0,0),(0,pad_h),(0,pad_w)), mode='constant')\n    k,i,j = get_im2col_indices(X.shape, kernel_size, stride = stride)\n    X_col = X[:,k,i,j].reshape(num * channels, kernel_size**2, -1)\n    X_col = X_col.transpose(0,2,1)\n    return X_col.reshape(-1, kernel_size**2)\n\ndef get_im2col_indices(x_shape, kernel_size, padding=0, stride=1):\n    N, C, H, W = x_shape\n    assert (H + 2 * padding - kernel_size) % stride == 0\n    assert (W + 2 * padding - kernel_size) % stride == 0\n    out_height = int((H + 2 * padding - kernel_size) / stride + 1)\n    out_width = int((W + 2 * padding - kernel_size) / stride + 1)\n    i0 = np.repeat(np.arange(kernel_size), kernel_size)\n    i0 = np.tile(i0, C)\n    i1 = stride * np.repeat(np.arange(out_height), out_width)\n    j0 = np.tile(np.arange(kernel_size), kernel_size * C)\n    j1 = stride * np.tile(np.arange(out_width), out_height)\n    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n    k = np.repeat(np.arange(C), kernel_size * kernel_size).reshape(-1, 1)\n    return (k.astype(int), i.astype(int), j.astype(int))\n\ndef col2ims(x, img_shape, kernel_size, stride):\n    x_row_num, x_col_num = x.shape\n    img_n, img_c, img_h, img_w = img_shape\n    o_h = int(math.ceil((img_h - kernel_size + 0.) / stride)) + 1\n    o_w = int(math.ceil((img_w - kernel_size + 0.) / stride)) + 1\n    assert img_n * img_c * o_h * o_w == x_row_num\n    assert kernel_size**2 == x_col_num\n    surplus_h = (img_h - kernel_size) % stride\n    surplus_w = (img_w - kernel_size) % stride\n    pad_h = (kernel_size - surplus_h) % stride\n    pad_w = (kernel_size - surplus_w) % stride\n    output_padded = np.zeros((img_n, img_c, img_h + pad_h, img_w + pad_w))\n    x_reshape = x.reshape(img_n, img_c, o_h, o_w, kernel_size, kernel_size)\n    for n in range(img_n):\n        for i in range(o_h):\n            for j in range(o_w):\n                output_padded[n, :, i * stride : i * stride + kernel_size, j * stride : j * stride + kernel_size] = \\\n                        output_padded[n, :, i * stride : i * stride + kernel_size, j * stride : j * stride + kernel_size] + \\\n                        x_reshape[n, :, i, j, ...]\n    return output_padded[:, :, 0 : img_h + pad_h, 0 : img_w + pad_w]\n\nclass Max_pooling:\n\n    def __init__(self, layer, kernel_size = 1, stride = 1):\n        self.num = layer.num\n        self.num_output = layer.num_output\n        self.num_input = layer.num_output\n        self.input_h = layer.output_h\n        self.input_w = layer.output_w\n        self.output_h = int(math.ceil((self.input_h - kernel_size + 0.) / stride)) + 1\n        self.output_w = int(math.ceil((self.input_w - kernel_size + 0.) / stride)) + 1\n        self.upper_layer = layer\n        self.kernel_size = kernel_size\n        self.stride = stride\n    \n    def forward(self):\n        self.input_data = self.upper_layer.forward()\n        self.num = self.upper_layer.num\n        input_col = im2col(self.input_data, self.kernel_size, self.stride)\n        tmp_index = np.tile(np.arange(input_col.shape[1]),input_col.shape[0]).reshape(input_col.shape)\n        self.max_index = tmp_index == input_col.argmax(1).reshape(-1,1)\n        self.output_data = input_col[self.max_index].reshape(self.num, self.num_input, self.output_h, self.output_w)\n        return self.output_data\n\n    def backward(self, diff):\n        diff_col = np.zeros((self.num * self.num_input * self.output_h * self.output_w, self.kernel_size**2))\n        diff_col[self.max_index] = diff.reshape(-1)\n        self.diff = col2ims(diff_col, self.input_data.shape, self.kernel_size, self.stride)\n        self.upper_layer.backward(self.diff)\n\n    def get_output(self):\n        return self.output_data\n"""
network/relu.py,0,"b'# -*- coding: utf-8 -*-\n\n  ##########################\n  #                        #\n  #   @Author: KangKang    #\n  #                        #\n  ##########################\n\nimport numpy as np\n\nclass Relu:\n   \n    def __init__(self, layer):\n        self.upper_layer = layer\n        self.num_output = layer.num_output\n        self.num = layer.num\n        self.output_w = layer.output_w\n        self.output_h = layer.output_h\n\n    def forward(self):\n        self.input_data = self.upper_layer.forward()\n        self.num = self.upper_layer.num\n        self.output_data = self.input_data.copy()\n        self.output_data[self.output_data < 0] = 0\n        return self.output_data\n\n    def backward(self, diff):\n        self.diff = diff.copy()\n        self.diff[self.input_data < 0] == 0\n        self.upper_layer.backward(self.diff)\n\n    def get_output(self):\n        return self.output_data\n'"
network/softmax.py,3,"b'# -*- coding: utf-8 -*-\n\n  ##########################\n  #                        #\n  #   @Author: KangKang    #\n  #                        #\n  ##########################\n\nimport numpy as np\n\nclass Softmax:\n    \n    def __init__(self, layer):\n        self.upper_layer = layer\n\n    def forward(self):\n        self.input_data = self.upper_layer.forward()\n        self.num = self.upper_layer.num\n        _, self.dim = self.input_data.shape\n        t = np.exp(self.input_data - self.input_data.max(1).reshape(-1,1))\n        self.softmax_data = t / t.sum(1).reshape(-1,1)\n        self.softmax_data[self.softmax_data < 1e-30] = 1e-30\n        return self.softmax_data\n\n    def calc_loss(self, label):\n        s = np.tile(np.arange(self.dim), self.num).reshape(self.input_data.shape)\n        gt_index = s == label.reshape(-1, 1) \n        loss = 0 - np.average(np.log(self.softmax_data[gt_index]))\n        self.diff = self.softmax_data.copy()\n        self.diff[gt_index] = self.diff[gt_index] - 1.\n        self.diff = self.diff / self.num\n        return loss\n\n    def backward(self, lr):\n        self.upper_layer.backward(self.diff * lr)\n'"
network/softmax_loss.py,3,"b'# -*- coding: utf-8 -*-\n\n  ##########################\n  #                        #\n  #   @Author: KangKang    #\n  #                        #\n  ##########################\n\nimport numpy as np\n\nclass Softmax_loss:\n    \n    def __init__(self, layer1, layer2):\n        self.upper_layer = layer1\n        self.label_layer = layer2\n\n    def forward(self):\n        self.input_data = self.upper_layer.forward()\n        self.num = self.upper_layer.num\n        _, self.dim = self.input_data.shape\n        t = np.exp(self.input_data - self.input_data.max(1).reshape(-1,1))\n        softmax_data = t / t.sum(1).reshape(-1,1)\n        softmax_data[softmax_data < 1e-30] = 1e-30\n        s = np.tile(np.arange(self.dim), self.num).reshape(self.input_data.shape)\n        gt_index = s == self.label_layer.get_label().reshape(-1, 1) \n        self.loss = 0 - np.average(np.log(softmax_data[gt_index]))\n        self.diff = softmax_data.copy()\n        self.diff[gt_index] = self.diff[gt_index] - 1.\n        self.diff = self.diff / self.num\n        return self.loss\n\n    def backward(self, lr):\n        self.upper_layer.backward(self.diff * lr)\n\n    def get_loss(self):\n        return self.loss\n\n\n'"
