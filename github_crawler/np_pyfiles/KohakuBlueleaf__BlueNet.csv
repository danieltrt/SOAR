file_path,api_count,code
Example(Mnist).py,2,"b'import sys,os\nimport time\nimport numpy as np\nfrom numpy.random import choice as rc\n\n## Import BlueNet\nimport BlueNet.Dataset.Mnist as mnist\nfrom BlueNet.Network import Net\nfrom BlueNet.Layer import *\nfrom BlueNet.Activation import GELU\nfrom BlueNet.Optimizer import Adam\nfrom BlueNet.UsualModel import LeNet\n\n\n(x_train,t_train),(x_test,t_test) = mnist.load_mnist(True, False, True, False, np.float32)\n## load train set and test set                    Normalize Flat One-hot Smooth type\n\n##Initialize the neural network(Use LeNet)     \nnet = Net(LeNet, (1,28,28), GELU, Adam, 0.001, 0, \'xaiver\', np.float32)\nnet.update()\n\n##Print the structure of the network\nnet.print_size()\n\n##Set some parameters for training \nbatch_size = 300\ntrain_size = x_train.shape[0]\niter_per_epoch = max((train_size//batch_size), 1)\nmax_acc = net.accuracy(x_test, t_test, batch_size)\nprint(\'Test Acc:{:5.5}\'.format(str(max_acc*100)))\n\n##Input how many epoch You wnat\nEpoch = int(input(\'Epoch:\'))\n\n##Start Training\nprint(\'\\n\xe2\x94\x8c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x90  \')\nprint(\'\xe2\x94\x82 Training start           \xe2\x94\x82  \')\nfor j in range(Epoch):\n    start = time.time()\n    print(""\xe2\x94\x82 =========================\xe2\x94\x82  "")\n    \n    for i in range(iter_per_epoch):\n        batch_mask = rc(train_size, batch_size)                 #Random choose data\n        \n        x_batch = x_train[batch_mask]\n        t_batch = t_train[batch_mask]\n        \n        loss = net.train(x_batch, t_batch)                         #Train&Caculate the loss of the net\n        print(\'\xe2\x94\x82 Epoch %2d  Loss:%5f  \xe2\x94\x82  \'%(j+1,loss), end=\'\\r\', flush=True)\n    \n    cost = time.time()-start\n    test_acc = net.accuracy(x_test, t_test, batch_size)\n    train_acc = net.accuracy(x_train, t_train, batch_size)\n    \n    if test_acc>max_acc:\n        max_acc = test_acc \n        net.save()                                                 #Save the parameters\n    \n    print(""\xe2\x94\x82 Epoch {:3}  Test Acc:{:5.5}\xe2\x94\x82  "".format(j+1,str(test_acc*100)))\n    print(""\xe2\x94\x82           Train Acc:{:5.5}\xe2\x94\x82  "".format(str(train_acc*100)))\n    print(""\xe2\x94\x82           Cost Time:{:5.5}\xe2\x94\x82  "".format(str(cost)))\n    \nprint(\'\xe2\x94\x94\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x98  \')\n\nnet.update()\nprint(""Final Accuracy: %2f%%""%(100*net.accuracy(x_test, t_test)))\n'"
setup.py,0,"b""import os\nfrom os import listdir\nfrom subprocess import run\nfrom distutils.core import setup\n\npackage_path = 'Lib/site-packages/BlueNet/Dataset'\nall_files = {}\ndata_files = []\nqueue = []\n\ntry:\n\tpath = 'BlueNet/Dataset'\n\tDataset = listdir(path)\n\tfor i in Dataset:\n\t\tqueue.append((path,i))\n\n\twhile queue:\n\t\tpath,now = queue.pop(0)\n\t\tthis = '{}/{}'.format(path,now)\n\t\tif os.path.isdir(this):\n\t\t\tfor i in listdir(this):\n\t\t\t\tqueue.append((this,i))\n\t\telse:\n\t\t\ttarget_dir = package_path+path[15:]\n\t\t\t\n\t\t\tif target_dir not in all_files:\n\t\t\t\tall_files[target_dir]=[]\n\t\t\tall_files[target_dir].append(this)\n\n\tfor i in all_files:\n\t\tdata_files.append((i,all_files[i]))\nexcept:\n\tpass\n\nsetup(\n\tname = 'BlueNet',\n\tpackages = ['BlueNet'],\n\tdata_files = data_files,\n\tversion = '1.0',\n\tdescription = 'My first project',\n\tauthor = 'BlueLeaf',\n\tauthor_email = 'apolloyeh0123@gmail.com',\n\tkeywords = ['Neural Network'],\n)\n"""
BlueNet/Activation.py,0,"b""# coding: utf-8\nfrom BlueNet.Functions import *\t\nfrom BlueNet.setting import _exp\t\t\n\n\n'''\nActive Function\n'''\n\nclass ID:\n\t\n\t'''\n\tIdentify y = x\n\t'''\n\tdef __init__(self):\n\t\tself.name='ID'\n\t\n\tdef forward(self, x):\n\t\t\n\t\treturn x\n\t\n\tdef backward(self, dout):\n\n\t\treturn dout\n\n\nclass Relu:\n\t\n\t'''\n\t\xe6\x95\xb4\xe6\xb5\x81\xe7\xb7\x9a\xe6\x80\xa7\xe5\x96\xae\xe5\x85\x83(Rectified Linear Unit)\n\t'''\n\t\n\tdef __init__(self):\n\t\tself.mask = None \t\t\t#mask for x<=0\n\t\tself.name='Relu'\n\t\n\tdef forward(self, x):\n\t\tself.mask = (x <= 0)\n\t\tout = x.copy() \n\t\tout[self.mask] = 0 \t\t\t#if x<=0 \xe2\x86\x92\xe2\x86\x92 x=0\n\n\t\treturn out\n\t\n\tdef backward(self, dout):\n\t\tdout[self.mask] = 0 \t\t#if outx=0 dx=0\n\t\tdx = dout\n\n\t\treturn dx\n\t\n\t\nclass Leaky:\n\t\n\t'''\n\tLeaky ReLU\n\t'''\n\t\n\tdef __init__(self):\n\t\tself.mask = None \t\t\t#mask for x<=0\n\t\tself.name='LReLU'\n\t\n\tdef forward(self, x):\n\t\tself.mask = (x <= 0)\n\t\tout = x.copy() \n\t\tout[self.mask] *= 0.01 \t\t#if x<=0 \xe2\x86\x92\xe2\x86\x92 x=a*x\n\n\t\treturn out\n\t\n\tdef backward(self, dout):\n\t\tdout[self.mask] *= 0.01 \t#if outx=0 dx=a*dx\n\t\tdx = dout\n\n\t\treturn dx\n\n\nclass Elu:\n\t\n\t'''\n\t_exponential Linear Unit\n\n\tIf x>0 out = x\n\tIf x<=0 out = e^x-1 (Gradient = e^(e^x-1)+1\n\t'''\n\t\n\tdef __init__(self):\n\t\tself.mask = None\t\t\t\t\t\t\t\t\t\t\t#mask for x<=0\n\t\tself.out = None\t\t\t\t\t\t\t\t\t\t\t\t#Store the output(for backpropagation)\n\t\tself.name='Elu'\n\t\n\tdef forward(self, x):\n\t\tself.mask = (x <= 0)\t\n\t\tout = x.copy()\n\t\tout[self.mask] = _exp(out[self.mask])-1\t\t\t\t\t\t#if x<0\xe2\x86\x92\xe2\x86\x92out = e^x-1\n\t\tself.out = out\n\t\t\n\t\treturn out\n\t\n\tdef backward(self, dout):\n\t\tdout[self.mask] = dout[self.mask]*(self.out[self.mask]+1)\t#f'(x)=f(x)+1\n\t\tself.mask = None\n\t\tself.out = None\n\t\tdx = dout\n\n\t\treturn dx\n\n\nclass GELU:\n\t\n\t'''\n\tGaussian Error Linear Units\n\tDefinition is 0.5x(1+erf(x/\xe2\x88\x9a2)) \n\tWe use scipy.special.erf/cupyx.scipy.special.erf in here\n\tThe Derivative function of GELU is compute by WolframAlpha\n\t(See functions.py)\n\t'''\n\t\n\tdef __init__(self):\n\t\tself.name='GELU'\n\t\tself.In = None\n\t\tself.size = 0\n\t\n\tdef forward(self,x,require_grad=True):\n\t\tself.In = x\n\t\tout = gelu_erf(x)\n\t\t\n\t\treturn out\n\t\t\n\tdef backward(self,dout):\n\t\tdx = dout*gelu_erf_grad(self.In)\n\t\tself.In = None\n\t\t\n\t\treturn dx\n\n\nclass ISRLU:\n\t\n\t'''\n\tINVERSE SQUARE ROOT LINEAR UNIT\n\t'''\n\t\n\tdef __init__(self, a=1):\n\t\tself.name='ISRLU'\n\t\tself.In = None\n\t\tself.mask = None\n\t\tself.a = a\n\t\n\tdef forward(self, x):\n\t\tself.mask = (x<=0)\n\t\tself.In = x\n\t\tout = x.copy()\n\t\tout[self.mask] = isru(out[self.mask],self.a)\n\t\t\n\t\treturn out\n\t\t\n\tdef backward(self, dout):\n\t\tdout[self.mask] = dout[self.mask]*isru_grad(self.In[self.mask],self.a)\n\t\tdx = dout\n\t\t\n\t\treturn dx\n\n\nclass ISRU:\n\t\n\t'''\n\tINVERSE SQUARE ROOT UNIT\n\t'''\n\t\n\tdef __init__(self, a = 1):\n\t\tself.name='ISRU'\n\t\tself.In = None\n\t\tself.a = a\n\t\n\tdef forward(self, x):\n\t\tself.In = x\n\t\tout = isru(x,self.a)\n\t\t\n\t\treturn out\n\t\t\n\tdef backward(self, dout):\n\t\tdx = dout * isru_grad(self.In,self.a)\n\t\t\n\t\treturn dx\n\n\nclass Sigmoid:\n\t\n\t'''\n\tSigmoid Fuction\n\t'''\n\t\n\tdef __init__(self):\n\t\tself.name= 'Sigmoid'\n\t\tself.out = None\t\t\t\t\t\t\t\t#Store the output(for backpropagation)\n\t\tself.flops = 0\n\t\tself.size = 0\n\t\n\tdef forward(self, x):\n\t\tout = sigmoid(x)\t\t\t\t\t\t\t#See functions.py\n\t\tself.out = out\n\t\n\t\treturn out\n\t\n\tdef backward(self, dout):\n\t\tdx = dout * (1.0 - self.out) * self.out\t\t#f'(x) = f(x)*(1-f(x))\n\n\t\treturn dx\n\n\nclass Softplus:\n\t\n\t'''\n\tSoftplus Function\n\t'''\n\t\n\tdef __init__(self):\n\t\tself.name= 'Softplus'\n\t\tself.In = None\t\t\t\t\t\t\t#Store the input(for backpropagation)\n\t\n\tdef forward(self, x):\n\t\tself.In = x\n\t\tout = softplus(x)\t\t\t\t\t\t#see functions.py\n\t\n\t\treturn out\n\t\n\tdef backward(self, dout):\n\t\tdx = dout * softplus_grad(self.In)\t\t#see functions.py\n\n\t\treturn dx\n\n\nclass Softsign:\n\t\n\t'''\n\tSoft sign function\n\t'''\n\t\n\tdef __init__(self):\n\t\tself.In = None\n\t\tself.name='Softsign'\n\t\n\tdef forward(self, x):\n\t\tself.In = x\n\t\tout = softsign(self.In)\n\t\t\n\t\treturn out\n\t\t\n\tdef backward(self, dout):\n\t\tdx = dout * softsign_grad(self.In)\n\t\t\n\t\treturn dx\n\n\nclass Softclip:\n\t\n\t'''\n\tSoft-clipping function\n\t'''\n\t\n\tdef __init__(self):\n\t\tself.In = None\n\t\tself.name='Softclip'\n\t\n\tdef forward(self, x):\n\t\tself.In = x\n\t\tout = softclip(self.In)\n\t\t\n\t\treturn out\n\t\t\n\tdef backward(self, dout):\n\t\tdx = dout * softclip_grad(self.In)\n\t\t\n\t\treturn dx\n\n\nclass SQNL:\n\n\t'''\n\tSquare Non-Linearity\n\t'''\n\t\n\tdef __init__(self):\n\t\tself.name= 'SQNL'\n\t\tself.mask1 = None \t\t\t\t\t#mask for 2<x\n\t\tself.mask2 = None \t\t\t\t\t#mask for 0<=x<=2\n\t\tself.mask3 = None \t\t\t\t\t#mask for -2<=x<0\n\t\tself.mask4 = None \t\t\t\t\t#mask for x<-2\n\t\tself.IN = None \n\t\n\tdef forward(self, x):\n\t\tself.IN = x\n\t\tself.mask1 = (2<=x) \t\t\t\t#mask for 2<x\n\t\tself.mask2 = (0<=x)\t\t\t\t\t#mask for 0<=x<=2\n\t\tself.mask3 = (x<0) \t\t\t\t\t#mask for -2<=x<0\n\t\tself.mask4 = (x<-2) \t\t\t\t#mask for x<-2\n\t\t\n\t\tout = x.copy()\n\t\tout[self.mask2]=out[self.mask2]-((out[self.mask2]**2)/4)\n\t\tout[self.mask1]=1\n\t\tout[self.mask3]=out[self.mask3]+((out[self.mask3]**2)/4)\n\t\tout[self.mask4]=-1\n\t\t\n\t\treturn out\n\t\n\tdef backward(self, dout):\n\t\tdout[self.mask2] *= (1-(self.IN[self.mask2])/2)\n\t\tdout[self.mask1] = 0\n\t\tdout[self.mask3] *= (1+(self.IN[self.mask3])/2)\n\t\tdout[self.mask4] = 0 \n\t\tdx = dout\n\n\t\treturn dx\n\n\nclass Tanh:\n\t\n\t'''\n\tOne of Hyperbolic functions. Upper bound is 1, Lower Bound is -1.\n\t'''\n\t\n\tdef __init__(self):\n\t\tself.name='Tanh'\n\t\tself.In = None \t\t\t\t\t#Store the input(for backpropagation)\n\t\n\tdef forward(self, x):\n\t\tself.In = x\n\t\tout = tanh(x)\t\t\t\t\t#see functions.py\n\t\n\t\treturn out\n\t\n\tdef backward(self, dout):\n\t\tdx = dout * tanh_grad(self.In)\t#see functions.py\n\n\t\treturn dx\n\n\nclass Arctan:\n\n\t'''\n\tOne of Inverse trigonometric functions. Upper bound is \xcf\x80/2, Lower bound is -\xcf\x80/2\n\t'''\n\n\tdef __init__(self):\n\t\tself.In = None\n\t\tself.name = 'ArcTan'\n\t\tself.size = 0\n\t\tself.flops = 0\n\t\n\tdef forward(self, x):\n\t\tself.In = x\n\t\tout = arctan(x)\n\t\t\n\t\treturn out\n\t\t\n\tdef backward(self, dout):\n\t\tdx = dout * arctan_grad(self.In)\n\t\t\n\t\treturn dx\n\n"""
BlueNet/Functions.py,57,"b""# coding: utf-8\nfrom BlueNet.setting import _np\nfrom BlueNet.setting import _erf\nimport numpy\nimport gc\n\n'''\nActivation Functions\n'''\n\nsqrt2 = 2**0.5\n\ndef identity_function(x):\n\treturn x\n\ndef step_function(x): #Binary\n\treturn _np.array(x > 0, dtype=_np.int)\n\ndef sigmoid(x):\n\treturn 1 / (1 + _np.exp(-x))\t\n\t\ndef sigmoid_grad(x):\n\treturn (1.0 - sigmoid(x)) * sigmoid(x)\n\ndef softplus(x):\n\treturn _np.log(1+_np.exp(x))\n\ndef softplus_grad(x):\n\treturn sigmoid(x)\n\ndef elu(x):\n\treturn _np.exp(x)-1\n\t\ndef elu_grad(x):\n\treturn _np.exp(_np.exp(x)-1)+1\n\ndef sinh(x):\n\treturn ((_np.exp(x) - _np.exp(-x)) / 2)\n\ndef cosh(x):\n\treturn ((_np.exp(x) + _np.exp(-x)) / 2)\n\ndef tanh(x):\n\treturn sinh(x) / cosh(x)\n\ndef tanh_grad(x):\n\treturn 1-(tanh(x)**2)\n\ndef relu(x):\n\treturn _np.maximum(0, x)\n\ndef relu_grad(x):\n\tgrad = _np.zeros(x)\n\tgrad[x>=0] = 1\n\t\n\treturn grad\n\t\ndef arctan(x):\n\treturn(_np.arctan(x))\n\t\ndef arctan_grad(x):\n\treturn(1/(x**2+1))\n\ndef softsign(x):\n\treturn x/(_np.abs(x)+1)\n\t\ndef softsign_grad(x):\n\treturn 1/(_np.abs(x)+1)**2\n\ndef softclip(x):\n\treturn(_np.log((1+_np.exp(x))/(1+_np.exp(x-1))))\n\t\ndef softclip_grad(x):\n\treturn((-1+_np.e)*_np.exp(x))/((_np.exp(x)+1)*(_np.exp(x)+_np.e))\n\ndef isru(x,a=1):\n\treturn x*(1/(1+a*x**2)**0.5)\n\ndef isru_grad(x,a=1):\n\treturn (1/(1+a*x**2)**0.5)**3\n\t\ndef isru_bound(a):\n\treturn (1/a**0.5),(-1/a**0.5)\n\ndef erf_grad(x):\n\treturn (2/_np.pi**0.5)*(_np.exp(-x**2))\n\ndef gelu_erf(x): #use scipy.special._erf/cupyx.scipy.special._erf\n\treturn 0.5*x*(1+_erf(x/sqrt2))\n\ndef gelu_erf_grad(x):\n\treturn 0.25*(sqrt2*x*erf_grad(x/sqrt2)+2*_erf(x/sqrt2)+2)\n\n\n\n''' \nOther Functions\n'''\n\ndef zca_whiten(X):\n\tsigma = _np.cov(X, rowvar=True)\n\tU,S,V = _np.linalg.svd(sigma)\n\tZCAMatrix = _np.dot(U, _np.dot(_np.diag(1.0/_np.sqrt(S + 1e-7)), U.T)) \n\t\n\treturn ZCAMatrix.dot(X)\n\ndef pca_whiten(X):\n   Xcov = _np.dot(X.T,X)\n   d, V = _np.linalg.eigh(Xcov)\n   D = _np.diag(1 / _np.sqrt(d+1e-7))\n   W = _np.dot(_np.dot(V, D), V.T)\n   X_white = _np.dot(X, W)\n\n   return X_white\n\ndef int_2_binary(number, binary_dim):\n\tbinary_list = [int(i) for i in bin(number)[2:]]\n\tmore_length = binary_dim - len(binary_list)\n\t\n\treturn _np.assary([0]*more_length+binary_list)\n\ndef binary2int(binary_array):\n\tout = 0\n\tfor index, x in enumerate(reversed(binary_array)):\n\t\tout += x * pow(2, index)\n\t\n\treturn out\n\ndef get_binary_data(BINARY_DIM):\n\tbinary = numpy.array([int_2_binary(x, BINARY_DIM) for x in range(2**BINARY_DIM)])\n\t\n\tdataX = []\n\tdataY = []\n\t\n\tfor i in range(binary.shape[0]):\n\t\tfor j in range(binary.shape[0]):\n\t\t\tdataX.append(numpy.append(binary[i], binary[j]))\n\t\t\tdataY.append(int_2_binary(i+j, BINARY_DIM+1))\n\t\n\treturn (numpy.reshape(dataX, (len(dataX), BINARY_DIM*2,1)),numpy.array(dataY))\n\ndef mean_squared_error(y, t):\n\treturn 0.5 * _np.sum((y-t+1e-6)**2)\n\ndef cross_entropy_error(y, t):\n\tif y.ndim == 1:\n\t\tt = t.reshape(1, t.size)\n\t\ty = y.reshape(1, y.size)\n\t\t\n\tif t.size == y.size:\n\t\tt = t.argmax(axis=1)\n\t\t\t \n\tbatch_size = y.shape[0]\n\t\n\treturn -_np.sum(_np.log(y[_np.arange(batch_size), t] + 1e-6)) / batch_size\n\ndef softmax(x):\n\tif x.ndim == 2:\n\t\tx = x.T\n\t\tx = x - _np.max(x, axis=0)\n\t\tx = _np.exp(x)\n\t\ty = x / _np.sum(x, axis=0)\n\t\n\t\treturn y.T \n\t\n\tx = x - _np.max(x)\n\tx = _np.exp(x)\n\ty = x / _np.sum(x)\n\t\n\treturn y\n\ndef softmax_loss(X, t):\n\ty = softmax(X)\n\t\n\treturn cross_entropy_error(y, t)\n\ndef numerical_gradient_n(f, x):\n\th = 1e-4 # 0.0001\n\t\n\ta = x+h\n\tb = x-h\n\tA = f(a)\n\tB = f(b)\n\tgrad = (A-B)/(h*2)\n\t\n\treturn grad\n\t\ndef numerical_gradient_1d(f, x):\n\th = 1e-4 # 0.0001\n\tgrad = _np.zeros_like(x)\n\t\n\tfor idx in range(x.size):\n\t\ttmp_val = x[idx]\n\t\tx[idx] = float(tmp_val) + h\n\t\tfxh1 = f(x) # f(x+h)\n\t\t\n\t\tx[idx] = tmp_val - h \n\t\tfxh2 = f(x) # f(x-h)\n\t\tgrad[idx] = (fxh1 - fxh2) / (2*h)\n\t\t\n\t\tx[idx] = tmp_val\n\t\t\n\treturn grad\n\ndef numerical_gradient_2d(f, X):\n\tif X.ndim == 1:\n\t\treturn _numerical_gradient_1d(f, X)\n\telse:\n\t\tgrad = _np.zeros_like(X)\n\t\t\n\t\tfor idx, x in enumerate(X):\n\t\t\tgrad[idx] = _numerical_gradient_1d(f, x)\n\t\t\n\t\treturn grad\n\ndef numerical_gradient(f, x):\n\th = 1e-4 # 0.0001\n\tgrad = _np.zeros_like(x)\n\t\n\tit = _np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n\twhile not it.finished:\n\t\tidx = it.multi_index\n\t\ttmp_val = x[idx]\n\t\tx[idx] = float(tmp_val) + h\n\t\tfxh1 = f(x) # f(x+h)\n\t\t\n\t\tx[idx] = tmp_val - h \n\t\tfxh2 = f(x) # f(x-h)\n\t\tgrad[idx] = (fxh1 - fxh2) / (2*h)\n\t\t\n\t\tx[idx] = tmp_val\n\t\tit.iternext()   \n\t\t\n\treturn grad\n\ndef _change_one_hot_label(X,class_num = 10):\n\tX = _np.asarray(X)\n\tT = _np.zeros((X.size, class_num))\n\tfor idx, row in enumerate(T):\n\t\trow[X[idx]] = 1\n\n\treturn T\n\ndef label_smoothing(input, e=0.01):\n\tk = input.shape[-1]\n\t\n\treturn(1-e)*input+(e/k)\n\n\n'''\nim2col col2im\n'''\n\ndef smooth_curve(x):\n\twindow_len = 11\n\ts = _np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n\tw = _np.kaiser(window_len, 2)\n\ty = _np.convolve(w/w.sum(), s, mode='valid')\n\t\n\treturn y[5:len(y)-5]\n\ndef shuffle_dataset(x, t):\n\tpermutation = _np.random.permutation(x.shape[0])\n\tx = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n\tt = t[permutation]\n\n\treturn x, t\n\ndef conv_output_size(input_size, filter_size, stride=1, pad=0):\n\treturn (input_size + 2*pad - filter_size) / stride + 1\n\ndef im2col(input_data, filter_h, filter_w, stride=1, pad=0, type=_np.float32):\n\tN, C, H, W = input_data.shape\n\t\n\tout_h = (H + 2*pad - filter_h)//stride + 1\n\tout_w = (W + 2*pad - filter_w)//stride + 1\n\n\timg = _np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant').astype(type)\n\tcol = _np.zeros((N, C, filter_h, filter_w, out_h, out_w)).astype(type)\n\n\tfor y in range(filter_h):\n\t\ty_max = y + stride*out_h\n\t\tfor x in range(filter_w):\n\t\t\tx_max = x + stride*out_w\n\t\t\tcol[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n\tcol = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n\t\n\treturn col\n\ndef col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0, type=_np.float32):\n\tN, C, H, W = input_shape\n\t\n\tout_h = (H + 2*pad - filter_h)//stride + 1\n\tout_w = (W + 2*pad - filter_w)//stride + 1\n\t\n\tcol = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2).astype(type)\n\timg = _np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1)).astype(type)\n\t\n\tfor y in range(filter_h):\n\t\ty_max = y + stride*out_h\n\t\tfor x in range(filter_w):\n\t\t\tx_max = x + stride*out_w\n\t\t\timg[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n\t\n\treturn img[:, :, pad:H + pad, pad:W + pad]\n\n\n'''\nFunction for NLP\n'''\n\ndef preprocess(text):\n\twords = text.lower().split()\n\t\n\tword_to_id = {}\n\tid_to_word = {}\n\tfor word in words:\n\t\tif word not in word_to_id:\n\t\t\tnew_id = len(word_to_id)\n\t\t\tword_to_id[word] = new_id\n\t\t\tid_to_word[new_id] = word\n\t\t\t\n\tcorpus = _np.array([word_to_id[w] for w in words])\n\t\n\treturn corpus, word_to_id, id_to_word\n\t\ndef Co_Matrix(corpus, amount, window_size=1):\n\tcorpus_size = len(corpus)\n\tco_matrix = _np.zeros((amount,amount), dtype=_np.int32)\n\t\n\tfor idx, word_id in enumerate(corpus):\n\t\tfor i in range(1, window_size+1):\n\t\t\tleft_idx = idx - i\n\t\t\tright_idx = idx + i\n\t\t\t\n\t\t\tif left_idx >= 0:\n\t\t\t\tleft_word_id = corpus[left_idx]\n\t\t\t\tco_matrix[word_id, left_word_id] +=1\n\t\t\t\n\t\t\tif right_idx < corpus_size:\n\t\t\t\tright_word_id = corpus[right_idx]\n\t\t\t\tco_matrix[word_id, right_word_id] +=1\n\t\t\t\t\n\treturn co_matrix\n\ndef cos_similarity(x,y):\n\tnx = x / _np.sqrt(_np.sum(x**2))\n\tny = y / _np.sqrt(_np.sum(y**2))\n\t\n\treturn _np.dot(nx,ny)\n\ndef most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n\tif query not in word_to_id:\n\t\tprint(query,'is not found')\n\t\treturn\n\t\n\tprint('\\n[query]'+query)\n\tquery_id = word_to_id[query]\n\tquery_vec = word_matrix[query_id]\n\t\n\tvocab_size = len(id_to_word)\n\tsimilarity = _np.zeros(vocab_size)\n\tfor i in range(vocab_size):\n\t\tsimilarity[i] = cos_similarity(word_matrix[i], query_vec)\n\t\n\tcount = 0\n\tfor i in (-1*similarity).argsort():\n\t\tif id_to_word[i] == query:\n\t\t\tcontinue\n\t\t\n\t\tprint(' %s: %s'%(id_to_word[i],similarity[i]))\n\t\tcount += 1\n\t\t\n\t\tif count >= top:\n\t\t\treturn\n\ndef ppmi(C, verbose=False, eps=1e-8):\n\tM = _np.zeros_like(C, dtype=_np.float64)\n\tN = _np.sum(C)\n\tS = _np.sum(C, axis=0)\n\ttotal = C.shape[0] * C.shape[1]\n\t\n\tcnt = 0\n\tfor i in range(C.shape[0]):\n\t\tfor j in range(C.shape[1]):\n\t\t\ttemp = (C[i,j]*N/(S[j]*S[i])+eps)\n\t\t\tpmi = _np.log2(temp)\n\t\t\tM[i,j]=max(0, pmi)\n\t\t\tif verbose:\n\t\t\t\tcnt+=1\n\t\t\t\tif cnt%(total//1000) == 0:\n\t\t\t\t\tprint('%.1f%% done'%(100*cnt/total),end='\\r',flush=True)\n\t\n\tprint('100.0% done')\n\t\n\treturn M"""
BlueNet/Layer.py,70,"b'# coding: utf-8\n\n#Original module\nfrom BlueNet.Functions import *\t\t\t\t#im2col and col2im function(written by \xe6\x96\x8e\xe8\x97\xa4\xe5\xba\xb7\xe6\xaf\x85)\nfrom BlueNet.Optimizer import *\t\t\t\t#SGD/Adam/optimizer/Momentum/RMSprop\nfrom BlueNet.Activation import *\t\t\t#ReLU/Elu/GELU/ISRL/ISRLU/Sigmoid/Softplus/Softsign/Tanh/Arctan\t\t\n\n#Native Module\nimport sys,os\nimport pickle\nfrom BlueNet.setting import _np\nfrom BlueNet.setting import _exp\n\ntry:\n\timport cupy as cp\nexcept:\n\tpass\n\nrn = _np.random.randn\nsqrt = lambda x: x**0.5\n\nclass Layer:\n\tdef __init__(self, AF=GELU, rate=0.001, optimizer=Adam, type=None):\n\t\tself.shapeIn = None\t\t\t\t\t\t\t\t\t\t\t\t\t#shape of data(IN)\n\t\tself.shapeOut = None\t\t\t\t\t\t\t\t\t\t\t\t#shape of data(OUT)\n\t\t\n\t\tself.AF = AF()\t\t\t\t\t\t\t\t\t\t\t\t\t\t#activation function\n\t\tself.optimizer = optimizer(lr=rate)\t\t\t\t\t\t\t\t\t#Optimizer\n\t\tself.size = 0\t\t\t\t\t\t\t\t\t\t\t\t\t\t#amount of params(Weight+bias)\n\t\tself.flops = 0\n\t\t\n\t\tself.params = {}\n\t\tself.grad = {}\n\t\tself.type = type\n\t\n\tdef train(self):\n\t\tself.optimizer.update(self.params,self.grad)\n\t\n\tdef save(self, name="""", path=""""):\t\t\t\t\t\t\t\t\t\t#Save the parameters\n\t\tparams = {}\n\t\tfor key, val in self.params.items():\n\t\t\tparams[key] = val\n\t\t\n\t\twith open(\'./weight/new{}/{}_W_{}\'.format(path,self.name,name), \'wb\') as f:\n\t\t\tpickle.dump(params, f)\n\t\n\tdef load(self, name="""", path=""""):\n\t\twith open(\'./weight/new{}/{}_W_{}\'.format(path,self.name,name), \'rb\') as f:\n\t\t\tparams = pickle.load(f)\n\t\t\n\t\tfor key, val in params.items():\n\t\t\tif key==\'W1\':\n\t\t\t\tkey = \'W\'\n\t\t\t\n\t\t\tif key==\'b1\':\n\t\t\t\tkey = \'b\'\n\n\t\t\tif val.shape == self.params[key].shape: \n\t\t\t\ttry:\n\t\t\t\t\tself.params[key] = _np.asarray(val).astype(self.type)\n\t\t\t\texcept:\n\t\t\t\t\tself.params[key] = cp.asnumpy(val).astype(self.type)\n\t\t\telse:\n\t\t\t\tprint(\'weight shape error\')\n\t\n\t\nclass Dense(Layer):\n\t\n\t\'\'\'\n\tFull Conected Layer\n\t\'\'\'\n\t\n\tdef __init__(self, output_size, AF=Elu, rate=0.01, optimizer=Adam, type=_np.float32):\n\t\tsuper(Dense, self).__init__(AF,rate,optimizer,type)\n\t\tself.name = \'Dense\'\n\t\t\n\t\t#initialize\n\t\tself.output_size = output_size\n\t\t\n\t\t#params\n\t\tself.params[\'W\'] = None\t\t\t\t\t\t\t\t\t\t\t#Weight\n\t\tself.params[\'b\'] = _np.ones(output_size)\t\t\t\t\t\t#Bias\n\t\t\n\t\t#data for backward\n\t\tself.x = None\n\t\n\tdef forward(self, x):\n\t\tif self.params[\'W\'] is None:\n\t\t\tself.params[\'W\'] = rn(x.shape[1], self.output_size)/x.shape[1]**0.5\n\t\t\n\t\tout = _np.dot(x, self.params[\'W\'])+self.params[\'b\']\n\t\tout = self.AF.forward(out)\n\t\t\n\t\tself.x = x\n\t\t\n\t\treturn out\n\t\n\tdef backward(self,error):\n\t\tdout = self.AF.backward(error)\t\n\t\tdx = _np.dot(dout, self.params[\'W\'].T)\t\t\t\t\t\t#BP for input\n\t\t\n\t\tself.grad[\'b\'] = _np.sum(dout, axis=0)\t\t\t\t\t\t#BP for bias\n\t\tself.grad[\'W\'] = _np.dot(self.x.T,dout)\t\t\t\t\t\t#BP for weight\n\t\tself.x = None\n\n\t\treturn dx\n\n\nclass Conv(Layer):\n\n\t\'\'\'\n\tConvolution Layer\n\t\'\'\'\n\t\n\tdef __init__(self, conv_param, AF=Elu, rate=0.01, optimizer=Adam, type=_np.float32):\n\t\tsuper(Conv, self).__init__(AF, rate, optimizer, type)\n\t\tself.name = \'Conv\'\n\t\t\n\t\t#Initialize\n\t\tself.f_num = conv_param[\'f_num\']\t\t\t\t\t\t\t\t\t#amount of filters\n\t\tself.f_size = conv_param[\'f_size\']\t\t\t\t\t\t\t\t\t#Size of filters\n\t\tself.pad = conv_param[\'pad\']\t\t\t\t\t\t\t\t\t\t#Padding size\n\t\tself.stride = conv_param[\'stride\']\t\t\t\t\t\t\t\t\t#Step of filters\n\t\t\n\t\t#params\n\t\tself.params[\'W\'] = None\t\t\t\t\t\t\t\t\t\t\t\t#Set by intial process(see Network.py)\n\t\tself.params[\'b\'] = _np.ones(self.f_num)\t\t\t\t\t\t\t\t#Bias\n\t\t\n\t\t#data for backpropagation\n\t\tself.x_shape = None   \t\t\t\t\t\t\t\t\t\t\t\t#shape of input\n\t\tself.x = None\t\t\t\t\t\t\t\t\t\t\t\t\t\t#colume of input\n\t\n\tdef forward(self, x):\n\t\tif self.params[\'W\'].shape is None:\n\t\t\tself.params[\'W\'] = rn(self.f_num, x.shape[1], self.f_size, self.f_size)\n\t\t\tself.params[\'W\'] /= sqrt(self.params[\'W\'].size)\n\t\n\t\tFN, C, FH, FW = self.params[\'W\'].shape\n\t\tN, C, H, W = x.shape\n\t\t\n\t\tout_h = 1+int((H+2*self.pad-FH)/self.stride)\n\t\tout_w = 1+int((W+2*self.pad-FW)/self.stride)\n\t\t\n\t\tcol = im2col(x, FH, FW, self.stride, self.pad, self.type)\t\t\t#Change the image to colume\n\t\tcol_W = self.params[\'W\'].reshape(FN, -1).T\t\t\t\t\t\t\t#Change the filters to colume\n\t\t\n\t\tout = _np.dot(col, col_W)+self.params[\'b\']\n\t\tout = self.AF.forward(out)\n\t\tout = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\t\t#change colume to image\n\n\t\tself.x_shape = x.shape\n\t\tself.x = x\n\n\t\treturn out\n\t\n\tdef backward(self, dout):\n\t\tFN, C, FH, FW = self.params[\'W\'].shape\n\t\t\n\t\tdout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\t\t\t\t\t#change gradient to colume\n\t\tdout = self.AF.backward(dout)\n\t\t\n\t\tcol = im2col(self.x, FH, FW, self.stride, self.pad, self.type)\n\t\tself.grad[\'b\'] = _np.sum(dout, axis=0)\n\t\tself.grad[\'W\'] = _np.dot(col.T, dout).transpose(1, 0).reshape(FN, C, FH, FW)\n\t\t\n\t\tcol = None\n\t\tself.x = None\n\t\t\n\t\tdcol = _np.dot(dout, self.params[\'W\'].reshape(FN, -1))\n\t\tdx = col2im(dcol, self.x_shape, FH, FW, self.stride, self.pad, self.type)\n\n\t\treturn dx\n\n\nclass DeConv(Layer):\n\n\t\'\'\'\n\tTranspose Convolution Layer\n\tThe forward of DeConv is the same as Convolution\'s backward and backward is the same as conv\'s forward too.\n\t\'\'\'\n\t\n\tdef __init__(self, conv_param, AF=Elu, rate=0.1, optimizer=Adam, type=_np.float32):\n\t\tsuper(DeConv, self).__init__(AF, rate, optimizer, type)\n\t\tself.name = \'DeConv\'\n\t\t\n\t\t#Initialize\n\t\tself.f_num = conv_param[\'f_num\']\t\t\t\t\t\t#Amount of filters\n\t\tself.f_size = conv_param[\'f_size\']\t\t\t\t\t\t#Filter size\n\t\tself.stride = conv_param[\'stride\']\t\t\t\t\t\t#Step\n\t\t\n\t\t#params\n\t\tself.params[\'W\'] = None\t\t\t\t\t\t\t\t\t#Set by intial process(see Network.py)\n\t\tself.grad = {}\n\t\t\n\t\t#data for backward\n\t\tself.x_shape = None   \n\t\tself.col = None\n\t\tself.col_W = None\n\n\tdef forward(self, x):\n\t\tif self.params[\'W\'].shape is None:\n\t\t\tself.params[\'W\'] = rn(x.shape[1],self.f_num,self.f_size,self.f_size)\n\t\t\tself.params[\'W\'] /= sqrt(self.params[\'W\'].size)\n\t\n\t\tFN, C, FH, FW = self.params[\'W\'].shape\n\t\tN, B, H, W = x.shape\n\t\t\n\t\tout_h = FH+int((H-1)*self.stride)\n\t\tout_w = FW+int((W-1)*self.stride)\n\t\t\n\t\tcol = x.transpose(0, 2, 3, 1).reshape(-1,FN)\n\t\tcol_W = self.params[\'W\'].reshape(FN, -1)\n\t\tout = _np.dot(col, col_W)\n\t\tout = col2im(out, (N, C , out_h, out_w), FH, FW, self.stride, 0, self.type)\n\t\tout = self.AF.forward(out)\n\t\t\n\t\tself.x_shape = x.shape\n\t\tself.col = col\n\n\t\treturn out\n\t\n\tdef backward(self, dout):\n\t\tFN, C, FH, FW = self.params[\'W\'].shape\n\t\t\n\t\tdout = self.AF.backward(dout)\n\t\tdout = im2col(dout, FH, FW, self.stride, 0, self.type)\n\t\t\n\t\tself.grad[\'W\'] = _np.dot(self.col.T, dout)\n\t\tself.grad[\'W\'] = self.grad[\'W\'].transpose(1, 0).reshape(FN, C, FH, FW)\n\t\t\n\t\tdcol = _np.dot(dout, self.params[\'W\'].reshape(FN, -1).T)\n\t\tdx = dcol.reshape((self.x_shape[0],self.x_shape[2],self.x_shape[3],-1)).transpose(0,3,1,2)\n\n\t\treturn dx\n\n\nclass Pool(Layer):\n\t\n\t\'\'\'\n\tMax-Pooling\n\tA convolution layer that the filter is to choose the biggest value\n\t\'\'\'\n\t\n\tdef __init__(self, pool_h, pool_w, stride=1, pad=0):\n\t\tsuper(Pool,self).__init__()\n\t\tself.name = \'Max-Pool\'\n\t\t\n\t\t#Setting about pooling\n\t\tself.pool_h = pool_h\t\t\t\t#Height of the region to pool\n\t\tself.pool_w = pool_w\t\t\t\t#width of the region to pool\n\t\tself.stride = stride\n\t\tself.pad = pad\n\n\t\t#data for backward\n\t\tself.x = None\n\t\tself.arg_max = None\n\t\n\tdef forward(self, x, require_grad = True):\n\t\tN, C, H, W = x.shape\n\t\t\n\t\tout_h = int(1+(H+self.pad*2-self.pool_h)/self.stride)\n\t\tout_w = int(1+(W+self.pad*2-self.pool_w)/self.stride)\n\t\t\n\t\tcol = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n\t\tcol = col.reshape(-1, self.pool_h*self.pool_w)\n\t\targ_max = _np.argmax(col, axis=1)\t\t\t\t\t\t\t\t#Choose the highest value\n\t\t\n\t\tout = _np.max(col, axis=1)\n\t\tout = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\t\t#Colume reshape to image\n\t\t\n\t\tself.x = x\n\t\tself.arg_max = arg_max\n\n\t\treturn out\n\t\n\tdef backward(self, dout):\n\t\tdout = dout.transpose(0, 2, 3, 1)\n\t\tpool_size = self.pool_h*self.pool_w\n\t\t\n\t\tdmax = _np.zeros((dout.size, pool_size))\n\t\tdmax[_np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n\t\tdmax = dmax.reshape(dout.shape+(pool_size,)) \n\t\t\n\t\tdcol = dmax.reshape(dmax.shape[0]*dmax.shape[1]*dmax.shape[2], -1)\n\t\tdx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n\t\t\n\t\treturn dx\n\n\nclass PoolAvg(Layer):\n\t\n\t\'\'\'\n\tAvg-Pooling\n\tA convolution layer that the filter is caclulate the average.\n\t\'\'\'\n\t\n\tdef __init__(self, pool_h, pool_w, stride=1, pad=0):\n\t\tsuper(PoolAvg, self).__init__()\n\t\tself.name = \'Avg-Pool\'\n\t\t\n\t\t#Setting about pooling\n\t\tself.pool_h = pool_h\n\t\tself.pool_w = pool_w\n\t\tself.stride = stride\n\t\tself.pad = pad\n\t\t\n\t\t#data for backward\n\t\tself.x_shape = None\n\t\tself.arg_max = None\n\n\tdef forward(self, x):\n\t\tN, C, H, W = x.shape\n\t\t\n\t\tout_h = int(1+(H-self.pool_h)/self.stride)\n\t\tout_w = int(1+(W-self.pool_w)/self.stride)\n\t\t\n\t\tcol = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n\t\tcol = col.reshape(-1, self.pool_h*self.pool_w)\n\t\tout = _np.average(col, axis=1)\t\t\t\t\t\t\t\t\t#caculate the average value\n\t\tout = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n\t\t\n\t\tself.x_shape = x.shape\n\t\t\n\t\treturn out\n\n\tdef backward(self, dout):\n\t\tpool_size = self.pool_h*self.pool_w\n\t\tN, C = dout.shape[0], dout.shape[1]\n\t\t\n\t\tdout = dout.transpose(0, 2, 3, 1)\n\t\tdout = dout.repeat(pool_size).reshape(N, C, -1, pool_size)/pool_size\n\t\tdx = col2im(dout, self.x_shape, self.pool_h, self.pool_w, self.stride, self.pad)\n\t\t\n\t\treturn dx\n\n\nclass Flatten(Layer):\n\t\n\t\'\'\'\n\t3D(2D)data to 1D\n\t\'\'\'\n\t\n\tdef __init__(self):\n\t\tsuper(Flatten,self).__init__()\n\t\tself.name = \'Flatten\'\n\t\t\n\t\t#Initialize\n\t\tself.in_shape = None\n\t\t\n\tdef forward(self, x):\n\t\tself.in_shape = x.shape\n\t\t\n\t\treturn x.reshape((x.shape[0],-1))\n\t\n\tdef backward(self, dout):\n\t\t\n\t\treturn dout.reshape(self.in_shape)\n\t\t\n\t\t\nclass BFlatten(Layer):\n\t\n\t\'\'\'\n\t1D data to 3D(2D)\n\t\'\'\'\n\t\n\tdef __init__(self, shape):\n\t\tsuper(BFlaatten, self).__init__()\n\t\tself.name = \'BFlatten\'\n\t\t\n\t\t#Initialize\n\t\tself.in_shape = None\n\t\tself.out_shape = shape\n\t\t\n\tdef forward(self, x):\n\t\tself.in_shape=x.shape\n\t\tC, W, H = self.out_shape\n\t\t\n\t\treturn x.reshape((x.shape[0], C, W, H))\n\t\n\tdef backward(self, dout):\n\t\t\n\t\treturn dout.reshape(self.in_shape)\n\n\nclass BatchNorm(Layer):\n\t\n\t\'\'\'\n\tBatchNormalization\n\t\'\'\'\n\t\n\tdef __init__(self, gamma=1.0, beta=0.0, momentum=0.9, running_mean=None, running_var=None, optimizer=Adam, rate=0.001):\n\t\tsuper(BatchNorm, self).__init__(optimizer=optimizer, rate=rate)\n\t\tself.name = \'BatchNorm\'\n\t\t\n\t\t#initialize\n\t\tself.params[\'gamma\'] = _np.array([gamma])\n\t\tself.params[\'beta\'] = _np.array([beta])\n\t\tself.momentum = momentum\n\t\tself.input_shape = None \t\t\t# Conv is 4d(N C H W), FCN is 2d(N D)\n\t\t\n\t\t# backward data\n\t\tself.batch_size = None\n\t\tself.xc = None\n\t\tself.std = None\n\t\tself.grad[\'gamma\'] = None\n\t\tself.grad[\'beta\'] = None\n\t\n\tdef forward(self, x):\n\t\tself.input_shape = x.shape\n\t\tif x.ndim != 2:\n\t\t\tN = x.shape[0]\n\t\t\tx = x.reshape(N, -1)\n\n\t\tout = self.__forward(x)\n\t\t\n\t\treturn out.reshape(*self.input_shape)\n\t\t\t\n\tdef __forward(self, x):\n\t\tgamma, beta = self.params[\'gamma\'][0], self.params[\'beta\'][0]\n\n\t\tmu = x.mean(axis=0)\n\t\txc = x-mu\n\t\tvar = _np.mean(xc**2, axis=0)\n\t\tstd = _np.sqrt(var+10e-7)\n\t\txn = xc/std\n\t\t\t\n\t\tself.batch_size = x.shape[0]\n\t\tself.xc = xc\n\t\tself.xn = xn\n\t\tself.std = std\n\t\t\t\n\t\tout = gamma*xn+beta \n\t\t\n\t\treturn out\n\n\tdef backward(self, dout):\n\t\tif dout.ndim != 2:\n\t\t\tN = dout.shape[0]\n\t\t\tdout = dout.reshape(N, -1)\n\n\t\tdx = self.__backward(dout)\n\t\tdx = dx.reshape(*self.input_shape)\n\t\t\n\t\treturn dx\n\t\n\tdef __backward(self, dout):\n\t\tgamma = self.params[\'gamma\']\n\t\t\n\t\tdbeta = dout.sum(axis=0)\n\t\tdgamma = self.xn*dbeta\n\t\tdxn = gamma*dout\n\t\tdxc = dxn/self.std\n\t\tdstd = -_np.sum((dxn*self.xc)/(self.std*self.std), axis=0)\n\t\tdvar = 0.5*dstd/self.std\n\t\tdxc += (2.0/self.batch_size)*self.xc*dvar\n\t\tdmu = _np.sum(dxc, axis=0)\n\t\tdx = dxc-dmu/self.batch_size\n\t\t\n\t\tself.grad[\'gamma\'] = _np.array([dgamma])  \n\t\tself.grad[\'beta\'] = _np.array([dbeta])\n\t\t\n\t\treturn dx\n\n\nclass Dropout(Layer):\n\t\n\tdef __init__(self, dropout_ratio=0.5):\n\t\tsuper(Dropout,self).__init__()\n\t\tself.name=\'DropOut\'\n\t\t\n\t\tself.dropout_ratio = dropout_ratio\n\t\tself.mask = None\n\t\n\tdef forward(self, x, require_grad=True):\n\t\tif require_grad:\n\t\t\tself.mask = _np.random.rand(*x.shape) > self.dropout_ratio\n\t\t\t\n\t\t\treturn x*self.mask\n\t\telse:\n\t\t\t\n\t\t\treturn x*(1.0-self.dropout_ratio)\n\n\tdef backward(self, dout):\n\t\t\n\t\treturn dout*self.mask\n\n\nclass ResBlock:\n\tdef __init__(self):\n\t\tself.AF = None\n\t\tself.layers = []\n\t\t\n\t\t#additional layer to fit the shape\n\t\tself.Conv = None\n\t\tself.use_conv = False\n\t\tself.pool = None\n\t\tself.use_pool = False\n\t\t\n\t\t#other\n\t\tself.size = 0\n\t\tself.flops = 0\n\t\tself.shapeOut = None\n\t\tself.shapeIn = None\n\t\n\tdef layer_init(self, data, init_std=0.001, init_mode=\'normal\', AF=Elu, optimizer=Adam, rate=0.001, type = _np.float32):\n\t\tinit = data\n\t\t\n\t\tfor i in range(len(self.layer)):\n\t\t\tif init_mode == \'xaiver\':\n\t\t\t\tinit_std = 1/(init.size**0.5)\n\t\t\t\n\t\t\tif self.layer[i].name == \'Conv\':\n\t\t\t\tFN, C, S = self.layer[i].f_num, init.shape[1], self.layer[i].f_size\n\t\t\t\tself.layer[i].type = type\n\t\t\t\t\n\t\t\t\t#set the params\n\t\t\t\tself.layer[i].params[\'W\'] = init_std*rn(FN, C, S, S).astype(type)\n\t\t\t\tself.layer[i].params[\'b\'] = self.layer[i].params[\'b\'].astype(type)*init_std\n\t\t\t\tout = self.layer[i].forward(init)\n\t\t\t\t\n\t\t\t\t#Caculate the FLOPs & Amount of params\n\t\t\t\tN, out_C, out_H, out_W = out.shape\n\t\t\t\tself.layer[i].flops = (C *S**2)*out_H*out_W*out_C\n\t\t\t\tself.layer[i].size = FN*C*S*S+FN\n\t\t\t\tself.flops += self.layer[i].flops\n\t\t\t\tself.size += self.layer[i].size\n\t\t\t\t\n\t\t\t\tinit = out\n\t\t\t\t\n\t\t\telif self.layer[i].name == \'Dense\':\n\t\t\t\tif init.ndim!=2:\n\t\t\t\t\tinit = init.reshape(init.shape[0],-1)\n\t\t\t\t\n\t\t\t\t#set the params\n\t\t\t\tout_size =  self.layer[i].output_size\n\t\t\t\tself.layer[i].params[\'W\'] = init_std*rn(init.size, out_size).astype(type)\n\t\t\t\tself.layer[i].params[\'b\'] = self.layer[i].params[\'b\'].astype(type)*init_std\n\t\t\t\t\n\t\t\t\t#Caculate the FLOPs & Amount of params\n\t\t\t\tself.layer[i].size = init.size*out_size+out_size\n\t\t\t\tself.layer[i].flops = (init.shape[1])*out_size\n\t\t\t\tself.size += self.layer[i].size\n\t\t\t\tself.flops += self.layer[i].flops\n\t\t\t\t\n\t\t\t\tinit = self.layer[i].forward(init)\n\t\t\n\t\tif init.ndim == 4:\n\t\t\tif init.shape[1] != data.shape[1]:\n\t\t\t\tFN, C = init.shape[1], data.shape[1]\n\t\t\t\tout_C, out_H, out_W = init.shape[1],data.shape[2],data.shape[3]\n\t\t\t\t\n\t\t\t\t#set the params\n\t\t\t\tself.Conv = Conv({\'f_num\':init.shape[1],\'f_size\':1,\'pad\':0,\'stride\':1})\n\t\t\t\tself.Conv.type = type\n\t\t\t\tself.Conv.params[\'W\'] = init_std*rn(FN,C,1,1).astype(type)\n\t\t\t\tself.Conv.params[\'b\'] = self.Conv.params[\'b\'].astype(type)*init_std\n\t\t\t\t\n\t\t\t\t#set Activation Functions & optimizer\n\t\t\t\tself.Conv.AF = ID()\n\t\t\t\tself.Conv.optimizer = optimizer(rate)\n\t\t\t\t\n\t\t\t\t#Caculate the FLOPs & Amount of params\n\t\t\t\tself.Conv.size = FN*C+FN\n\t\t\t\tself.Conv.flops = (C *S**2)*out_H*out_W*out_C\n\t\t\t\tself.size += self.Conv.size\n\t\t\t\tself.flops += self.Conv.flops\n\t\t\t\t\n\t\t\t\tself.use_conv = True\n\t\t\t\t\t\n\t\t\tif init.shape[2] != data.shape[2]:\n\t\t\t\tif init.shape[2] == data.shape[2]//2:\n\t\t\t\t\tFN, C = init.shape[1], data.shape[1]\n\t\t\t\t\tout_C, out_H, out_W = init.shape[1],data.shape[2],data.shape[3]\n\t\t\t\t\t\n\t\t\t\t\tself.Conv = Conv({\'f_num\':init.shape[1],\'f_size\':1,\'pad\':0,\'stride\':2})\n\t\t\t\t\tself.Conv.type = type\n\t\t\t\t\tself.Conv.params[\'W\'] = init_std*rn(FN,C,1,1).astype(type)\n\t\t\t\t\tself.Conv.params[\'b\'] = self.Conv.params[\'b\'].astype(type)*init_std\n\t\t\t\t\t\n\t\t\t\t\t#set Activation Functions & optimizer\n\t\t\t\t\tself.Conv.AF = ID()\n\t\t\t\t\tself.Conv.optimizer = optimizer(rate)\n\t\t\t\t\t\n\t\t\t\t\t#Caculate the FLOPs & Amount of params\n\t\t\t\t\tself.Conv.size = FN*C+FN\n\t\t\t\t\tself.Conv.flops = (C *S**2)*out_H*out_W*out_C\n\t\t\t\t\tself.size += self.Conv.size\n\t\t\t\t\tself.flops += self.Conv.flops\n\t\t\t\t\t\n\t\t\t\t\tself.use_conv = True\n\t\t\t\t\n\t\t\t\telif init.shape[2] == (data.shape[2]//2)+1:\n\t\t\t\t\tFN, C = init.shape[1], data.shape[1]\n\t\t\t\t\tout_C, out_H, out_W = init.shape[1],data.shape[2],data.shape[3]\n\t\t\t\t\t\n\t\t\t\t\tself.Conv = Conv({\'f_num\':init.shape[1],\'f_size\':1,\'pad\':1,\'stride\':2})\n\t\t\t\t\tself.Conv.type = type\n\t\t\t\t\tself.Conv.params[\'W\'] = init_std*rn(FN,C,1,1).astype(type)\n\t\t\t\t\tself.Conv.params[\'b\'] = self.Conv.params[\'b\'].astype(type)*init_std\n\t\t\t\t\t\n\t\t\t\t\t#set Activation Functions & optimizer\n\t\t\t\t\tself.Conv.AF = ID()\n\t\t\t\t\tself.Conv.optimizer = optimizer(rate)\n\t\t\t\t\t\n\t\t\t\t\t#Caculate the FLOPs & Amount of params\n\t\t\t\t\tself.Conv.size = FN*C+FN\n\t\t\t\t\tself.Conv.flops = (C *S**2)*out_H*out_W*out_C\n\t\t\t\t\tself.size += self.Conv.size\n\t\t\t\t\tself.flops += self.Conv.flops\n\t\t\t\t\t\n\t\t\t\t\tself.use_conv = True\n\t\t\t\t\n\t\t\t\telse:\n\t\t\t\t\tprint(init.shape)\n\t\t\t\t\tprint(\'Shape Error\')\n\t\t\t\t\tsys.exit()\n\t\treturn init\n\t\n\tdef train(self):\n\t\tfor i in self.layers:\n\t\t\ttry:\n\t\t\t\ti.optimizer.update(i.params,i.grad)\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\t\t\n\t\tif self.use_conv:\n\t\t\tself.Conv.optimizer.update(self.Conv.params,self.Conv.grad)\n\t\n\tdef save(self, name):\n\t\tj = 1\n\t\tpath = \'./weight/new/Res_\'+name+\'/\'\n\t\tif not os.path.isdir(path):\n\t\t\tos.mkdir(path)\n\t\t\n\t\tpath = \'/Res_\'+name\n\t\t\n\t\tfor i in self.layers:\n\t\t\ttry:\n\t\t\t\ti.save(str(j),path)\n\t\t\texcept AttributeError: \t#AF\n\t\t\t\tpass\n\t\t\t\n\t\t\tj+=1\n\t\t\n\t\tif self.use_conv:\n\t\t\tself.Conv.save(str(j+1),path)\n\t\t\n\tdef load(self, name):\n\t\tj = 1\n\t\tpath = \'./weight/new/Res_\'+name+\'/\'\n\t\tif not os.path.isdir(path):\n\t\t\tos.mkdir(path)\n\t\t\n\t\tpath = \'/Res_\'+name\n\t\t\n\t\tfor i in self.layers:\n\t\t\ttry:\n\t\t\t\ti.load(str(j),path)\n\t\t\texcept AttributeError:\t\t#AF\n\t\t\t\tpass\n\t\t\texcept FileNotFoundError:\t#file not found(Conv,Deconv,BN,Dense)\n\t\t\t\tpass\n\t\t\t\n\t\t\tj+=1\n\t\t\n\t\tif self.use_conv:\n\t\t\tself.Conv.load(str(j+1))\n\n\nclass ResV1(ResBlock):\n\tdef __init__(self,layer):\n\t\tsuper(ResV1,self).__init__()\n\t\tself.name = \'ResLayer\'\n\t\t\n\t\t#Initialize\n\t\tself.layer = layer\n\t\t\n\tdef initial(self, data, init_std=0.001, init_mode=\'normal\', AF=Elu, optimizer=Adam, rate=0.001, type=_np.float32):\n\t\tinit = self.layer_init(data, init_std, init_mode, AF, optimizer, rate, type)\n\t\t\n\t\tfor i in self.layer:\n\t\t\ti.AF = ID()\n\t\t\ti.optimizer = optimizer(rate)\n\t\t\tself.layers.append(i)\n\t\t\tself.layers.append(BatchNorm())\n\t\t\tself.layers.append(AF())\n\t\t\n\t\treturn init\n\t\n\tdef forward(self, x):\n\t\tout = x\n\t\tout2 = x\n\t\tlength = len(self.layers)\n\t\t\n\t\tfor i in range(length-1):\n\t\t\tout = self.layers[i].forward(out)\n\t\t\n\t\tif self.use_conv:\n\t\t\tout2 = self.Conv.forward(out2)\n\t\t\tself.size += self.Conv.size\n\t\t\t\n\t\treturn self.layers[length-1].forward(out+out2)\n\t\n\tdef backward(self, dout):\n\t\tself.layers.reverse()\n\t\tdout = self.layers[0].backward(dout)\n\t\tdx = dout\n\t\tdx2 = dout\n\n\t\tfor i in range(1, len(self.layers)):\n\t\t\tdx = self.layers[i].backward(dx)\n\t\tself.layers.reverse()\n\t\t\n\t\tif self.use_conv:\n\t\t\tdx2 = self.Conv.backward(dx2)\n\t\t\t\n\t\treturn dx+dx2\n\t\t\n\nclass ResV2(ResBlock):\n\tdef __init__(self, layer):\n\t\tsuper(ResV2,self).__init__()\n\t\tself.name = \'ResLayer\'\n\t\t#initialize\n\t\tself.layer = layer\n\t\t\n\tdef initial(self, data, init_std=0.001, init_mode=\'normal\', AF=Elu, optimizer=Adam, rate=0.001, type=_np.float32):\n\t\tinit = self.layer_init(data, init_std, init_mode, AF, optimizer, rate, type)\n\t\t\n\t\tfor i in self.layer:\n\t\t\ti.AF = ID()\n\t\t\ti.optimizer = optimizer(rate)\n\t\t\tself.layers.append(BatchNorm(optimizer=optimizer, rate=rate))\n\t\t\tself.layers.append(AF())\n\t\t\tself.layers.append(i)\t\n\n\t\treturn init\n\t\n\tdef forward(self, x):\n\t\tout = x\n\t\tout2 = x\n\t\tfor i in self.layers:\n\t\t\tout = i.forward(out)\n\t\t\n\t\tif self.use_conv:\n\t\t\tout2 = self.Conv.forward(out2)\n\t\t\n\t\treturn out+out2\n\t\n\tdef backward(self, dout):\n\t\tdx = dout\n\t\tdx2 = dout\n\n\t\tself.layers.reverse()\n\t\tfor i in self.layers:\n\t\t\tdx = i.backward(dx)\n\t\tself.layers.reverse()\n\t\t\n\t\tif self.use_conv:\n\t\t\tdx2 = self.Conv.backward(dx2)\n\t\t\t\n\t\treturn dx+dx2\n\t\t\n\n\n\'\'\'\nLayer for RNN\n\'\'\'\n\n\nclass Embedding:\n\t\n\t\'\'\'\n\tWord Embedding\n\t\'\'\'\n\t\n\tdef __init__(self, W):\n\t\tself.params = [W]\n\t\tself.grads = [_np.zeros_like(W)]\n\t\tself.idx = None\n\t\n\tdef forward(self, idx):\n\t\tW, = self.params\n\t\tself.idx = idx\n\t\tout = W[idx]\n\t\t\n\t\treturn out\n\t\n\tdef backward(self,dout):\n\t\tdW, = self.grads\n\t\tdW[...] = 0\n\t\t_np.add.at(dW, self.idx, dout)\n\t\t\n\t\treturn None\n\n\nclass TimeEmbedding(Layer):\n\t\n\tdef __init__(self, stateful=False, optimizer=Adam, rate=0.001):\n\t\tsuper(TimeEmbedding,self).__init__(optimizer=optimizer,rate=rate)\n\t\tself.name = \'TimeEmbedding\'\n\t\t\n\t\t#parameters\n\t\tself.params[\'W\'] = None\n\t\tself.grad[\'W\'] = None\t\n\t\n\tdef forward(self, xs):\n\t\tWx = self.params[\'W\']\n\t\tN, T, D = xs.shape\n\t\tH = Wx.shape[1]\n\t\t\n\t\tself.layers = []\n\t\ths = _np.empty((N, T, H), dtype=\'f\')\n\t\t\n\t\tfor t in range(T):\n\t\t\tlayer = Embedding(Wx)\n\t\t\tself.h = layer.forward(xs[:, t, :])\n\t\t\ths[:, t, :] = self.h\n\t\t\t\n\t\t\tself.layers.append(layer)\n\t\t\t\n\t\treturn hs\n\t\t\n\tdef backward(self, dhs):\n\t\tW = self.params[\'W\']\n\t\tN, T, H = dhs.shape\n\t\tD = W.shape[0]\n\t\t\n\t\tdxs = _np.empty((N, T, D), dtype=\'f\')\n\t\tself.grad[\'W\'] = _np.zeros_like(W)\n\t\t\n\t\tfor t in reversed(range(T)):\n\t\t\tlayer = self.layers[t]\n\t\t\tdx = layer.backward(dhs[:, t, :])\n\t\t\t\n\t\t\ttry:\n\t\t\t\tdxs[:, t, :] = dx\n\t\t\texcept:\n\t\t\t\tprint(dxs.shape)\n\t\t\t\t\n\t\t\tfor i in layer.grad:\n\t\t\t\tself.grad[i] += layer.grad[i]\n\t\t\n\t\treturn dxs\n\n\nclass TimeDense(Layer):\n\t\n\tdef __init__(self, output, AF=Elu, optimizer=Adam, rate=0.001):\n\t\tsuper(TimeDense,self).__init__(AF, rate, optimizer)\n\t\tself.name = \'TimeDense\'\n\t\t\n\t\t#initialize\n\t\tself.dh = None\n\t\tself.output_size = output\n\t\t\n\t\t#parameters\n\t\tself.params[\'W\'] = None\n\t\tself.params[\'b\'] = None\n\t\tself.grad[\'W\'] = None\n\t\tself.grad[\'b\'] = None\t\n\t\n\tdef forward(self, xs):\n\t\tWx, b = self.params[\'W\'],self.params[\'b\']\n\t\tN, T, D = xs.shape\n\t\tH = Wx.shape[1]\n\t\t\n\t\tself.layers = []\n\t\ths = _np.empty((N, T, H), dtype=\'f\')\n\t\t\n\t\tfor t in range(T):\n\t\t\tlayer = Dense(H)\n\t\t\tlayer.params[\'W\'] = Wx\n\t\t\tlayer.params[\'b\'] = b\n\t\t\tself.h = layer.forward(xs[:, t, :])\n\t\t\ths[:, t, :] = self.h\n\t\t\t\n\t\t\tself.layers.append(layer)\n\t\t\n\t\treturn hs\n\t\t\n\tdef backward(self,dhs):\n\t\tWx, b = self.params[\'W\'],self.params[\'b\']\n\t\tN, T, H = dhs.shape\n\t\tD = Wx.shape[0]\n\t\t\n\t\tdxs = _np.empty((N, T, D), dtype=\'f\')\n\t\tself.grad[\'W\'] = _np.zeros_like(Wx)\n\t\tself.grad[\'b\'] = _np.zeros_like(b)\n\n\t\tfor t in reversed(range(T)):\n\t\t\tlayer = self.layers[t]\n\t\t\tdx = layer.backward(dhs[:, t, :])\n\t\t\ttry:\n\t\t\t\tdxs[:, t, :] = dx\n\t\t\texcept:\n\t\t\t\tprint(dxs.shape)\n\t\t\tfor i in layer.grad:\n\t\t\t\tself.grad[i] += layer.grad[i]\n\t\t\n\t\treturn dxs\n\t\n\t\nclass LSTM:\n\t\n\t\'\'\'\n\tLong short-term memory\n\t\'\'\'\n\t\n\tdef __init__(self, Wx, Wh, b):\n\t\tself.name = \'LSTM\'\n\t\t#parameters and grads\n\t\tself.params = [Wx, Wh, b]\n\t\tself.grad = [_np.zeros_like(i) for i in self.params]\n\t\t\n\t\t#gate\n\t\tself.sigm1 = Sigmoid()\n\t\tself.sigm2 = Sigmoid()\n\t\tself.sigm3 = Sigmoid()\n\t\tself.tanIn = Tanh()\n\t\tself.tanOu = Tanh()\n\t\t\n\t\t#data for backward\n\t\tself.In = None\n\t\tself.Hin = None\n\t\tself.Tout = None\n\t\tself.Tin = None\n\t\tself.Cin = None\n\t\t\n\t\t#other\n\t\tself.size = 0\n\t\tself.flops = 0\n\t\t\n\tdef forward(self, input, Hin, C):\n\t\tWx,Wh,b = self.params\n\t\tH = Hin.shape[1]\n\t\tA = _np.dot(input,Wx)+_np.dot(Hin,Wh)+b\n\t\t\n\t\t#slice\n\t\tf = A[:, :H]\n\t\tg = A[:, H:2*H]\n\t\ti = A[:, 2*H:3*H]\n\t\to = A[:, 3*H:]\n\t\t\n\t\tSf = self.sigm1.forward(f)\n\t\tTin = self.tanIn.forward(g)\n\t\tSi = self.sigm2.forward(i)\n\t\tSo = self.sigm3.forward(o)\n\t\tCout = C*Sf+Si*Tin\n\t\tTout = self.tanOu.forward(Cout)\n\t\tHout = Tout*So\n\t\t\n\t\tself.Hin = Hin\n\t\tself.In = input\n\t\tself.Cin = C\n\t\tself.Tout = Tout\n\t\tself.Tin = Tin\n\n\t\treturn Hout,Cout\n\t\n\tdef backward(self, dOut, dHout, dC):\n\t\tWx,Wh,b = self.params\n\t\t\n\t\tdHout = dOut+dHout\n\t\tdTout = self.tanOu.backward(dHout*self.sigm3.out)\n\t\tdCin = (dC+dTout)*self.sigm1.out\n\t\t\n\t\tdf = self.sigm1.backward((dC+dTout)*self.Cin)\t\t\t\t\t#df\n\t\tdg = self.tanIn.backward((dC+dTout)*self.sigm2.out)\t\t\t\t#dg\n\t\tdi = self.sigm2.backward((dC+dTout)*self.Tin)\t\t\t\t\t#di\n\t\tdo = self.sigm3.backward(dOut*self.Tout)\t\t\t\t\t\t#do\n\t\t\n\t\tdA = _np.hstack((df,dg,di,do))\n\t\tself.grad[2] = _np.hstack((_np.sum(df,axis=0), _np.sum(dg,axis=0), _np.sum(di,axis=0), _np.sum(do,axis=0)))\n\t\tself.grad[1] = _np.dot(self.Hin.T, dA)\n\t\tself.grad[0] = _np.dot(self.In.T, dA)\n\n\t\tdHin = _np.dot(dA, Wh.T)\n\t\tdIn = _np.dot(dA, Wx.T)\n\t\n\t\treturn dIn,dHin,dCin\n\n\nclass TimeLSTM(Layer):\n\t\n\tdef __init__(self, node, stateful=False, optimizer=Adam, rate=0.001):\n\t\tsuper(TimeLSTM).__init__(GELU, rate, optimizer)\n\t\tself.name = \'TimeLSTM\'\n\t\t\n\t\t#initialize\n\t\tself.h, self.c = None,None\n\t\tself.dh = None\n\t\tself.stateful = stateful\n\t\t\n\t\t#parameters\n\t\tself.node = node\n\t\tself.params[\'Wx\'] = None\n\t\tself.params[\'Wh\'] = None\n\t\tself.params[\'b\'] = None\n\t\tself.grad[\'Wx\'] = None\n\t\tself.grad[\'Wh\'] = None\n\t\tself.grad[\'b\'] = None\t\n\t\t\n\tdef forward(self, xs):\n\t\tWx, Wh, b = self.params[\'Wx\'],self.params[\'Wh\'],self.params[\'b\']\n\t\tN, T, D = xs.shape\n\t\tH = Wh.shape[0]\n\t\t\n\t\tself.layers = []\n\t\ths = _np.empty((N, T, H), dtype=\'f\')\n\t\t\n\t\tif not self.stateful or self.h is None:\n\t\t\tself.h = _np.zeros((N,H), dtype=\'f\')\n\t\t\n\t\tif not self.stateful or self.c is None:\n\t\t\tself.c = _np.zeros((N,H), dtype=\'f\')\n\n\t\tfor t in range(T):\n\t\t\tlayer = LSTM(Wx, Wh, b)\n\t\t\tself.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n\t\t\ths[:, t, :] = self.h\n\t\t\t\n\t\t\tself.layers.append(layer)\n\t\t\t\n\t\treturn hs\n\t\t\n\tdef backward(self,dhs):\n\t\tWx, Wh, b = self.params[\'Wx\'], self.params[\'Wh\'], self.params[\'b\']\n\t\tN, T, H = dhs.shape\n\t\tD = Wx.shape[0]\n\t\t\n\t\tdxs = _np.empty((N, T, D), dtype=\'f\')\n\t\tdh, dc = 0, 0\n\t\t\n\t\tgrads = [0,0,0]\n\t\tfor t in reversed(range(T)):\n\t\t\tlayer = self.layers[t]\n\t\t\tdx, dh, dc = layer.backward(dhs[:, t, :], dh, dc)\n\t\t\ttry:\n\t\t\t\tdxs[:, t, :] = dx\n\t\t\texcept:\n\t\t\t\tprint(dxs.shape)\n\t\t\t\n\t\t\tfor i , grad in enumerate(layer.grad):\n\t\t\t\tgrads[i] += grad\n\t\t\n\t\tif self.train_flag:\n\t\t\tself.grad[\'Wx\'] = grads[0]\n\t\t\tself.grad[\'Wh\'] = grads[1]\n\t\t\tself.grad[\'b\'] = grads[2]\n\t\telse:\n\t\t\tself.grad[\'Wx\'] = 0\n\t\t\tself.grad[\'Wh\'] = 0\n\t\t\tself.grad[\'b\'] = 0\n\n\t\tself.dh = dh\n\t\t\n\t\treturn dxs\n\t\t\n\tdef set_state(self, h, c=None):\n\t\tself.h, self.c= h,c\n\t\t\n\tdef reset_state(self):\n\t\tself.h, self.c = None, None\n\t\n\nclass GRU:\n\t\n\t\'\'\'\n\tGated Recurrent Unit\n\t\'\'\'\n\t\n\tdef __init__(self,Wx,Wh,b):\n\t\tself.name = \'LSTM\'\n\t\t#parameters\n\t\tself.params = [Wx, Wh, b]\n\t\tself.grad = [_np.zeros_like(i) for i in self.params]\n\t\t\n\t\t#gate\n\t\tself.sigm1 = Sigmoid()\n\t\tself.sigm2 = Sigmoid()\n\t\tself.tanIn = Tanh()\n\t\t\n\t\t#data for backward\n\t\tself.In = None\n\t\tself.Hin = None\n\t\tself.To = None\n\t\t\n\t\t#other\n\t\tself.size = 0\n\t\tself.flops = 0\n\t\t\n\tdef forward(self,input,Hin=None):\n\t\tWx,Wh,b = self.params\n\t\tH = Hin.shape[1]\n\t\t\n\t\tx = _np.dot(input,Wx)\n\t\th = _np.dot(Hin,Wh)\n\t\t\n\t\t#slice\n\t\tr = x[:, :H]+h[:, :H]+b[:H]\n\t\tu = x[:, H:2*H]+h[:, H:2*H]+b[H:2*H]\n\t\txo = x[:, 2*H:]+b[2*H:]\n\t\tho = h[:, 2*H:]\n\t\t\n\t\tSr = self.sigm1.forward(r)\t\t\t\t\t#reset Gate(sigmoid)\n\t\tSu = self.sigm2.forward(u)\t\t\t\t\t#update gate(sigmoid)\n\t\tTo = self.tanIn.forward(ho*Sr+xo)\t\t\t#Ouput gate(Tanh)\n\t\tHout = (Su-1)*Hin+(Su*To)\n\t\t\n\t\tself.Hin = Hin\n\t\tself.In = input\n\t\tself.To = To\n\n\t\treturn Hout\n\t\n\tdef backward(self,dOut,dHout):\n\t\tWx,Wh,b = self.params\n\t\tdHout = dOut+dHout\n\t\t\n\t\tdSu = dHout*self.Hin+dHout*self.To\n\t\tdTo = dHout*self.sigm2.out\n\t\tdSr = dTo*self.Hin\n\t\tdHin = dTo*self.sigm1.out\n\t\t\n\t\tdr = self.sigm1.backward(dSr)\n\t\tdu = self.sigm1.backward(dSu)\n\t\tdo = self.tanIn.backward(dTo)\n\t\t\n\t\tdx = _np.hstack((dr,du,do))\n\t\tdh = _np.hstack((dr,du,do*self.sigm1.out))\n\t\t\n\t\tself.grad[2] = _np.hstack((_np.sum(dr,axis=0),_np.sum(du,axis=0),_np.sum(do,axis=0)))\n\t\tself.grad[1] = _np.dot(self.Hin.T, dh)\n\t\tself.grad[0] = _np.dot(self.In.T, dx)\n\n\t\tdHin += _np.dot(dh, Wh.T)\n\t\tdIn = _np.dot(dx, Wx.T)\n\t\n\t\treturn dIn,dHin\n\n\nclass TimeGRU(Layer):\n\t\n\tdef __init__(self,node,stateful=False,optimizer=Adam,rate=0.001,Train_flag = True):\n\t\tsuper(TimeGRU,self).__init__(GELU, rate, optimizer)\n\t\tself.name = \'TimeGRU\'\n\t\t\n\t\t#initialize\n\t\tself.h = None\n\t\tself.dh = None\n\t\tself.stateful = stateful\n\t\t\n\t\t#parameters\n\t\tself.params[\'Wx\'] = None\n\t\tself.params[\'Wh\'] = None\n\t\tself.params[\'b\'] = None\n\t\tself.grad[\'Wx\'] = None\n\t\tself.grad[\'Wh\'] = None\n\t\tself.grad[\'b\'] = None\t\n\t\t\n\t\t#other\n\t\tself.node = node\n\t\tself.train_flag = Train_flag\n\t\t\n\tdef forward(self, xs):\n\t\tWx, Wh, b = self.params[\'Wx\'],self.params[\'Wh\'],self.params[\'b\']\n\t\tN, T, D = xs.shape\n\t\tH = Wh.shape[0]\n\t\t\n\t\tself.layers = []\n\t\ths = _np.empty((N, T, H), dtype=\'f\')\n\t\t\n\t\tif not self.stateful or self.h is None:\n\t\t\tself.h = _np.zeros((N,H), dtype=\'f\')\n\n\t\tfor t in range(T):\n\t\t\tlayer = GRU(Wx,Wh,b)\n\t\t\tself.h = layer.forward(xs[:, t, :], self.h)\n\t\t\ths[:, t, :] = self.h\n\t\t\t\n\t\t\tself.layers.append(layer)\n\t\t\t\n\t\treturn hs\n\t\t\n\tdef backward(self,dhs):\n\t\tWx= self.params[\'Wx\']\n\t\tN, T, H = dhs.shape\n\t\tD = Wx.shape[0]\n\t\t\n\t\tdxs = _np.empty((N, T, D), dtype=\'f\')\n\t\tdh = 0\n\t\t\n\t\tgrads = [0,0,0]\n\t\tfor t in reversed(range(T)):\n\t\t\tlayer = self.layers[t]\n\t\t\tdx, dh = layer.backward(dhs[:, t, :], dh)\n\t\t\ttry:\n\t\t\t\tdxs[:, t, :] = dx\n\t\t\texcept:\n\t\t\t\tprint(dxs.shape)\n\t\t\t\n\t\t\tfor i , grad in enumerate(layer.grad):\n\t\t\t\tgrads[i] += grad\n\t\t\n\t\tif self.train_flag:\n\t\t\tself.grad[\'Wx\'] = grads[0]\n\t\t\tself.grad[\'Wh\'] = grads[1]\n\t\t\tself.grad[\'b\'] = grads[2]\n\t\telse:\n\t\t\tself.grad[\'Wx\'] = 0\n\t\t\tself.grad[\'Wh\'] = 0\n\t\t\tself.grad[\'b\'] = 0\n\t\t\t\n\t\tself.dh = dh\n\t\t\n\t\treturn dxs\n\t\t\n\tdef set_state(self, h, c=None):\n\t\tself.h = h\n\t\t\n\tdef reset_state(self):\n\t\tself.h = None\n\n\n\'\'\'\nOther Layer\n\'\'\'\n\n\nclass SoftmaxWithLoss(Layer):\n\t\n\t\'\'\'\n\tSoftmax layer+CorssEntropyError layer\n\t\'\'\'\n\t\n\tdef __init__(self):\n\t\tsuper(SoftmaxWithLoss, self).__init__()\n\t\tself.name = \'Softmax\'\n\t\t\n\t\t#Initialize\n\t\tself.loss = None\n\t\tself.y = None \n\t\tself.t = None \n\t\t\n\tdef forward(self, x, t=None, loss=True):\n\t\tif not loss:\n\t\t\treturn softmax(x)\n\t\t\n\t\tself.t = t\n\t\tself.y = softmax(x)\n\t\ty = self.y\n\t\tself.loss = cross_entropy_error(self.y, self.t)\n\t\tloss = self.loss\n\t\t\n\t\treturn loss\n\t\n\tdef backward(self, dout=1):\n\t\tbatch_size = self.t.shape[0]\n\t\tif self.t.size == self.y.size:\n\t\t\tdx = (self.y-self.t)/batch_size\n\t\telse:\n\t\t\tdx = self.y.copy()\n\t\t\tdx[_np.arange(batch_size), self.t] -= 1\n\t\t\tdx = dx/batch_size\n\t\t\n\t\treturn dx\n\t\t\n'"
BlueNet/Network.py,14,"b'# coding: utf-8\n\n#Original\nfrom BlueNet.Optimizer import *\nfrom BlueNet.Activation import *\nfrom BlueNet.Functions import *\n\n#native(or usual)\nfrom BlueNet.setting import _np\nimport sys,os\nimport numpy\n\n\npath = ""./weight""\nif not os.path.isdir(path):\n\tos.mkdir(path)\npath = ""./weight/new""\nif not os.path.isdir(path):\n\tos.mkdir(path)\n\nrn = _np.random.randn\nD3 = {\'Conv\',\'DeConv\',\'ResLayer\',\'Flatten\'}\nD2 = {\'Dense\'}\nPRE = {\'Conv\',\'DeConv\',\'ResLayer\',\'Softmax\'}\n\n\nclass Net:\n\t\n\tdef __init__(self, network, data_shape=(3,224,224), AF=Relu, optimizer=Adam, rate=0.001\\\n\t\t\t\t\t, init_std=0.005, init_mode=\'normal\', type=_np.float32):\n\t\tnormalization=\'\'\n\t\tself.net = []\n\t\tfor i in network:\n\t\t\tself.net.append(i)\n\t\tself.layers = len(network)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#amount of layers\n\t\tIni = True\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#Initialized or not\n\t\t\n\t\t#if any layer\'s parameters haven\'t been setted, set Ini=False to run the initial process\n\t\tfor i in self.net:\n\t\t\tif i.name == \'ConvNet\' or i.name == \'DeConvNet\' or i.name == \'Dense\':\n\t\t\t\tif i.params[\'W\'] is None:\n\t\t\t\t\tIni = False\n\t\t\n\t\t#initial process\n\t\tif not Ini:\n\t\t\tinit = rn(data_shape[0],data_shape[1],data_shape[2])\t\t\t\t\t\t \t\t\t#data for initial\n\t\t\tj = 0\n\t\t\tfor i in range(self.layers):\n\t\t\t\tname = self.net[i].name\n\t\t\t\t\n\t\t\t\tif j == 0:\n\t\t\t\t\t#set the shape of data. Falt the data if first layer is dense\n\t\t\t\t\tif name in D3:\n\t\t\t\t\t\tinit = init.reshape(1,init.shape[0],init.shape[1],init.shape[2])\n\t\t\t\t\telif name in D2:\n\t\t\t\t\t\tinit = init.reshape(1,init.size)\n\t\t\t\t\n\t\t\t\tself.net[i].shapeIn = init.shape[1:]\t\t\t\t\t\t\t\t\t\t\t\t#save the input shape\n\t\t\t\t\n\t\t\t\tif init_mode == \'xaiver\':\n\t\t\t\t\tinit_std = 1/(init.size**0.5)\n\t\t\t\t\n\t\t\t\tself.net[i].optimizer = optimizer(rate)\t\t\t\t\t\t\t\t\t\t\t\t#set Optimizer\n\t\t\t\tself.net[i].AF = AF()\n\t\t\t\tself.net[i].type = type\n\t\t\t\t\n\t\t\t\t#Convolution\n\t\t\t\tif name == \'Conv\' or name == \'DeConv\':\n\t\t\t\t\tself.net[i].params[\'b\'] *= init_std\n\t\t\t\t\tFN, C, S = self.net[i].f_num, init.shape[1], self.net[i].f_size\n\n\t\t\t\t\t#Convolution\n\t\t\t\t\tif name == \'Conv\':\n\t\t\t\t\t\tself.net[i].params[\'W\'] = init_std * rn(FN, C, S, S)\t\t\t\t\t\t#weight\'s shape is (F_num,input_channel,F_size,F_Size)\n\t\t\t\t\t\tout = self.net[i].forward(init)\t\t\t\t\t\t\t\t\t\t\t\t#data to set next layer\n\t\t\t\t\t\t\n\t\t\t\t\t\tN, out_C, out_H, out_W = out.shape\n\t\t\t\t\t\tself.net[i].flops = ((C*S**2))*out_H*out_W*out_C\t\t\t\t\t\t\t#caculate the FLOPs\n\t\t\t\t\t\tself.net[i].size = FN*C*S*S + FN\t\t\t\t\t\t\t\t\t\t\t#caculate the amount of parameters\n\t\t\t\t\t\n\t\t\t\t\t#Transpose Convolution\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.net[i].params[\'W\'] = init_std * rn(C, FN, S, S)\t\t\t\t\t\t#weight\'s shape is (Input_channel,F_Num,F_size,F_Size)\n\t\t\t\t\t\tout = self.net[i].forward(init)\t\t\t\t\t\t\t\t\t\t\t\t#data to set next layer\n\t\t\t\t\t\t\n\t\t\t\t\t\tN, out_C, out_H, out_W = out.shape\n\t\t\t\t\t\tself.net[i].flops = ((C*S**2)-1)*out_H*out_W*out_C\t\t\t\t\t\t\t#caculate the FLOPs\n\t\t\t\t\t\tself.net[i].size = self.net[i].params[\'W\'].size\t\t\t\t\t\t\t\t#caculate the amount of parameters\n\t\t\t\t\n\t\t\t\t\tinit = out\n\t\t\t\t\n\t\t\t\t#Fully connected layer\n\t\t\t\telif name == \'Dense\':\n\t\t\t\t\tout_size = self.net[i].output_size\n\t\t\t\t\tself.net[i].params[\'W\'] = init_std*rn(init.size, out_size)\t\t\t\t\t\t#weight\'s shape is (input_size,output_size)\n\t\t\t\t\tself.net[i].params[\'b\'] *= init_std\n\t\t\t\t\tself.net[i].params[\'b\'] = self.net[i].params[\'b\']\n\t\t\t\t\tself.net[i].flops = init.shape[1]*out_size\t\t\t\t\t\t\t\t\t\t#caculate the FLOPs\n\t\t\t\t\tself.net[i].size = init.size*out_size + out_size\t\t\t\t\t\t\t\t#caculate the amount of parameters\n\t\t\t\t\t\n\t\t\t\t#ResLayer(Block of ResNet)\n\t\t\t\telif name == \'ResLayer\':\n\t\t\t\t\tself.net[i].AF = AF\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#set Activation Function\n\t\t\t\t\tinit = self.net[i].initial(init,init_std,init_mode,AF,optimizer,rate,type)\t\t#see layer.py(In fact the function is same as here)\n\t\t\t\t\n\t\t\t\telif name == \'TimeLSTM\':\n\t\t\t\t\tT = init.shape[1]\n\t\t\t\t\tD = init.shape[2]\n\t\t\t\t\tH = self.net[i].node\n\t\t\t\t\tself.net[i].params[\'Wx\'] = rn(D, 4*H)*init_std\n\t\t\t\t\tself.net[i].params[\'Wh\'] = rn(H, 4*H)*init_std\n\t\t\t\t\tself.net[i].params[\'b\'] = _np.ones(4*H)*init_std\n\t\t\t\t\tself.net[i].flops = T*D*4*H+T*H*4*H\t\t\t\t\t\t\t\t\t\t\t\t#caculate the FLOPs\n\t\t\t\t\tself.net[i].size = (D+H+1)*4*H\n\t\t\t\t\n\t\t\t\telif name == \'TimeGRU\':\n\t\t\t\t\tT = init.shape[1]\n\t\t\t\t\tD = init.shape[2]\n\t\t\t\t\tH = self.net[i].node\n\t\t\t\t\tself.net[i].params[\'Wx\'] = rn(D, 3*H)*init_std\n\t\t\t\t\tself.net[i].params[\'Wh\'] = rn(H, 3*H)*init_std\n\t\t\t\t\tself.net[i].params[\'b\'] = _np.ones(3*H)*init_std\n\t\t\t\t\tself.net[i].flops = T*D*3*H+T*H*3*H\t\t\t\t\t\t\t\t\t\t\t\t#caculate the FLOPs\n\t\t\t\t\tself.net[i].size = (D+H+1)*3*H\n\t\t\t\t\n\t\t\t\telse:\n\t\t\t\t\tpass\n\t\t\t\t\n\t\t\t\t#these layers don\'t need to caculate the data for next layer so we just skip it\n\t\t\t\tif name not in PRE:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tinit = self.net[i].forward(init)\n\t\t\t\t\texcept:\n\t\t\t\t\t\tprint(init.shape)\n\t\t\t\t\t\tprint(self.net[i].params[\'W\'].shape)\n\t\t\t\t\n\t\t\t\t#save the output shape\n\t\t\t\tself.net[i].shapeOut = init.shape[1:]\n\t\t\t\tj += 1\n\t\telse:\n\t\t\tpass\n\t\t\n\t\tfor i in range(self.layers):\n\t\t\ttry:\n\t\t\t\tfor x in self.net[i].params.keys():\n\t\t\t\t\ttry:\n\t\t\t\t\t\tself.net[i].params[x] = self.net[i].params[x].astype(type)\n\t\t\t\t\texcept AttributeError:\n\t\t\t\t\t\tpass\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\n\t#print the model(About layer/amount of parameter/FLOPS...)\n\tdef print_size(self):\n\t\ttotal = 0\t\t#Total amount of parameters\n\t\ttotal_f = 0 \t#Total FLOPs\n\t\t\n\t\t#print the table\n\t\tprint(""\xe2\x94\x8c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xac\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xac\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xac\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xac\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x90"")\n\t\tprint(""\xe2\x94\x82   Layer   \xe2\x94\x82 GFLOPs\xe2\x94\x82  Params  \xe2\x94\x82   Shape(In)  \xe2\x94\x82  Shape(Out) \xe2\x94\x82"")\n\t\tfor i in self.net:\n\t\t\ttry:\n\t\t\t\ttotal += i.size\n\t\t\t\ttotal_f += i.flops\n\t\t\t\tprint(""\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xbc\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xbc\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xbc\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xbc\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xa4"")\n\t\t\t\tprint(""\xe2\x94\x82{:^11}\xe2\x94\x82{:^7.3f}\xe2\x94\x82{:>10}\xe2\x94\x82{:>14}\xe2\x94\x82{:>13}\xe2\x94\x82"".format(i.name,i.flops/1000000000,i.size,str(i.shapeIn).replace(\' \',\'\'),str(i.shapeOut).replace(\' \',\'\')))\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\t\t\t\t\n\t\tprint(""\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xbc\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xbc\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xbc\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xbc\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xa4"")\n\t\tprint(""\xe2\x94\x82   Total   \xe2\x94\x82{:^7.2f}\xe2\x94\x82{:>10}\xe2\x94\x82              \xe2\x94\x82\t            \xe2\x94\x82"".format(total_f/1000000000,total))\n\t\tprint(""\xe2\x94\x94\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb4\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb4\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb4\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb4\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x98"")\t\n\n\t#forward process. DropOut is set OFF. SoftmaxWithLoss return the answer\n\tdef process(self,input):\n\t\tinput = _np.asarray(input)\n\t\tfor i in range(self.layers):\n\t\t\tif self.net[i].name == \'DropOut\':continue\n\t\t\t\n\t\t\tif self.net[i].name == \'Softmax\':\n\t\t\t\tinput = self.net[i].forward(input,0,False)\n\t\t\telse:\n\t\t\t\tinput = self.net[i].forward(input)\n\t\treturn input\n\t\t\n\t#forward process. DropOut is set ON. SoftmaxWithLoss return the loss\n\tdef forward(self,input,t=None):\n\t\tinput = _np.asarray(input)\n\t\tt = _np.asarray(t)\n\t\tfor i in range(self.layers):\n\t\t\tif self.net[i].name != \'Softmax\':\n\t\t\t\tinput = self.net[i].forward(input)\n\t\t\telse:\n\t\t\t\tinput = self.net[i].forward(input,t)\n\t\t\n\t\treturn input\n\t\n\t#Backpropagation (will save the gradients)\n\tdef backward(self,error):\n\t\tself.net.reverse()\n\t\t\n\t\t#backpropagation in order\n\t\tfor i in range(self.layers):\n\t\t\terror = self.net[i].backward(error)\n\t\tself.net.reverse()\n\t\t\n\t\t#return final error(for GAN\'s Discriminator or others)\n\t\treturn(error)\n\t\n\t#Backward + train(Change the parameters)\n\tdef back_train(self,error):\n\t\terror = self.backward(error)\t\t\t\t\t#backpropagation first\n\t\t\n\t\tfor i in range(self.layers):\n\t\t\t#call the train function\n\t\t\tself.net[i].train()\n\t\t\n\t\treturn error\n\t\n\t#Train consist of forward, backtrain, call the optimizer\n\tdef train(self,input,t):\n\t\terror = self.forward(input, t)\t\t\t\t\t#forward(get the loss)\n\t\tself.back_train(error)\n\n\t\treturn error\n\t\t\n\tdef reset(self):\n\t\tfor i in range(self.layers):\n\t\t\tif self.net[i].name == \'TimeLSTM\':\n\t\t\t\tself.net[i].reset_state()\n\t\n\t#caculate the accuracy of the net\n\tdef accuracy(self, x, t, batch_size = 100, print_the_result = False):\n\t\tac = 0\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#amount of correct answer\n\t\tfor i in range(x.shape[0]//batch_size):\t\t\t\t\t\t\t\t#process 10 datas in a time\n\t\t\tbatch = numpy.arange(i*batch_size, batch_size+i*batch_size)\t\t#choose the data in order\n\t\t\t\n\t\t\tx_batch = _np.asarray(x[batch])\n\t\t\tt_batch = _np.asarray(t[batch])\n\t\t\t\n\t\t\ty = self.process(x_batch)\n\t\t\ty = _np.argmax(y, axis=1)\t\t\t\n\t\t\ttt = _np.argmax(t_batch, axis=1)\n\t\t\tac += _np.sum(y == tt)\t\t\t\t\t\t\t\t\t\t\t#save the amount of correct answer\n\t\t\t\n\t\taccuracy = ac / x.shape[0]\n\t\tif print_the_result:\n\t\t\tprint(ac,\'/\',x.shape[0],sep=\'\')\n\t\t\n\t\treturn accuracy\n\t\n\t#caculate the loss of the net(CEE)\n\tdef loss(self, x, t): \n\t\tt = _np.asarray(t)\n\t\ty = self.process(x)\n\t\tloss = cross_entropy_error(y, t)\n\t\t\n\t\treturn loss\n\t\n\t#caculate the loss of the net(MSE)\n\tdef loss_MSE(self, x, t): \n\t\tt = _np.asarray(t)\n\t\ty = self.process(x)\n\t\t\n\t\treturn mean_squared_error(y, t)\t\n\t\n\t#Load the parameters\n\tdef update(self):\n\t\tfor i in range(self.layers):\n\t\t\t#call every layer\'s load function\n\t\t\ttry:\n\t\t\t\tself.net[i].load(str(i+1))\n\t\t\texcept FileNotFoundError:\n\t\t\t\tpass\n\t\n\t#Save the parameters\n\tdef save(self):\n\t\tpath = ""./weight""\n\t\tif not os.path.isdir(path):\n\t\t\tos.mkdir(path)\n\t\tpath = ""./weight/new""\n\t\tif not os.path.isdir(path):\n\t\t\tos.mkdir(path)\n\t\t\n\t\tfor i in range(self.layers):\n\t\t\tself.net[i].save(str(i+1))\n'"
BlueNet/Optimizer.py,12,"b'# coding: utf-8\nimport sys\nfrom BlueNet.setting import _np\n\ndef L1_Norm(weight,grad,decay_rate=0.00001):\n\tdecay = _np.sum(_np.abs(weight))\n\tgrad += decay*decay_rate\n\t\n\treturn grad\n\n\ndef L2_Norm(weight,grad,decay_rate=0.00001):\n\tdecay = _np.sum(weight**2)**0.5\n\tgrad -= decay*decay_rate\n\t\n\treturn grad\n\n\nclass SGD:\n\n\t""""""Stochastic Gradient Descent""""""\n\n\tdef __init__(self, lr=0.01):\n\t\tself.lr = lr\n\t\t\n\tdef update(self, params, grads):\n\t\tfor key in params.keys():\n\t\t\tparams[key] -= self.lr * grads[key] \n\n\nclass Momentum:\n\n\t""""""Momentum SGD""""""\n\n\tdef __init__(self, lr=0.01, momentum=0.9):\n\t\tself.lr = lr\n\t\tself.momentum = momentum\n\t\tself.v = None\n\t\t\n\tdef update(self, params, grads):\n\t\tif self.v is None:\n\t\t\tself.v = {}\n\t\t\tfor key, val in params.items():\t\t\t\t\t\t\t\t\n\t\t\t\tself.v[key] = _np.zeros_like(val)\n\t\t\t\t\n\t\tfor key in params.keys():\n\t\t\tself.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n\t\t\tparams[key] += self.v[key]\n\n\nclass Nesterov:\n\n\t""""""Nesterov\'s Accelerated Gradient (http://arxiv.org/abs/1212.0901)""""""\n\n\tdef __init__(self, lr=0.01, momentum=0.9):\n\t\tself.lr = lr\n\t\tself.momentum = momentum\n\t\tself.v = None\n\t\t\n\tdef update(self, params, grads):\n\t\tif self.v is None:\n\t\t\tself.v = {}\n\t\t\tfor key, val in params.items():\n\t\t\t\tself.v[key] = _np.zeros_like(val)\n\t\t\t\n\t\tfor key in params.keys():\n\t\t\tself.v[key] *= self.momentum\n\t\t\tself.v[key] -= self.lr * grads[key]\n\t\t\tparams[key] += self.momentum * self.momentum * self.v[key]\n\t\t\tparams[key] -= (1 + self.momentum) * self.lr * grads[key]\n\n\nclass AdaGrad:\n\n\t""""""AdaGrad""""""\n\n\tdef __init__(self, lr=0.01):\n\t\tself.lr = lr\n\t\tself.h = None\n\t\t\n\tdef update(self, params, grads):\n\t\tif self.h is None:\n\t\t\tself.h = {}\n\t\t\tfor key, val in params.items():\n\t\t\t\tself.h[key] = _np.zeros_like(val)\n\t\t\t\n\t\tfor key in params.keys():\n\t\t\tself.h[key] += grads[key] * grads[key]\n\t\t\tparams[key] -= self.lr * grads[key] / (_np.sqrt(self.h[key]) + 1e-7)\n\n\nclass RMSprop:\n\n\t""""""RMSprop""""""\n\n\tdef __init__(self, lr=0.01, decay_rate = 0.99):\n\t\tself.lr = lr\n\t\tself.decay_rate = decay_rate\n\t\tself.h = None\n\t\t\n\tdef update(self, params, grads):\n\t\tif self.h is None:\n\t\t\tself.h = {}\n\t\t\tfor key, val in params.items():\n\t\t\t\tself.h[key] = _np.zeros_like(val)\n\t\t\t\n\t\tfor key in params.keys():\n\t\t\tself.h[key] *= self.decay_rate\n\t\t\tself.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n\t\t\tparams[key] -= self.lr * grads[key] / (_np.sqrt(self.h[key]) + 1e-7)\n\n\nclass Adam:\n\n\t""""""Adam (http://arxiv.org/abs/1412.6980v8)""""""\n\n\tdef __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n\t\tself.lr = lr\n\t\tself.beta1 = beta1\n\t\tself.beta2 = beta2\n\t\tself.iter = 0\n\t\tself.m = {}\n\t\tself.v = {}\n\t\t\n\tdef update(self, params, grads):\n\t\tif self.m == {}:\n\t\t\tfor key, val in params.items():\n\t\t\t\tself.m[key] = _np.zeros_like(val)\n\t\t\t\tself.v[key] = _np.zeros_like(val)\n\t\t\n\t\tself.iter += 1\n\t\tlr_t  = self.lr * _np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\t\t \n\t\t\t\t\n\t\tfor key in params.keys():\n\t\t\ttry:\n\t\t\t\tself.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n\t\t\t\tself.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n\t\t\t\t\n\t\t\t\tparams[key] -= lr_t * self.m[key] / (_np.sqrt(self.v[key]) + 1e-7)\n\t\t\texcept ValueError:\n\t\t\t\tpass\n\n'"
BlueNet/UsualModel.py,0,"b""# coding: utf-8\nfrom BlueNet.Optimizer import *\nfrom BlueNet.Layer import *\nfrom BlueNet.Activation import *\n\n\n#Hyperparameters\nrate = 0.001\nbatch_size = 200\ninit_std = 0.01\ninit_mode = 'xaiver'\n\n#function for network\noptimizer = Adam\nAF = GELU\n\n#Usual model\n'''\nLeNet\n'''\n\nLeNet = [\n\t\t\tConv({'f_num':6, 'f_size':5, 'pad':2, 'stride':1}),\n\t\t\tPool(2,2,2),\n\t\t\tConv({'f_num':16, 'f_size':5, 'pad':0, 'stride':1}),\n\t\t\tPool(2,2,2),\n\t\t\tConv({'f_num':120, 'f_size':5, 'pad':0, 'stride':1}),\n\t\t\tFlatten(),\n\t\t\tDense(output_size=84),\n\t\t\tDense(output_size=10),\n\t\t\t\t\n\t\t\tSoftmaxWithLoss()\n\t\t]\n\n\n'''\nAlexNet\n'''\n\nAlexNet=[\n\t\t\tConv({'f_num':96, 'f_size':11, 'pad':3, 'stride':4}),\n\t\t\tPool(3,3,2),\n\t\t\tConv({'f_num':256, 'f_size':5, 'pad':2, 'stride':1}),\n\t\t\tPool(3,3,2),\n\t\t\tConv({'f_num':384, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':384, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tPool(3,3,2),\n\t\t\tFlatten(),\n\t\t\tDense(output_size=4096),\n\t\t\tDense(output_size=4096),\n\t\t\tDense(output_size=1000),\n\t\t\t\n\t\t\tSoftmaxWithLoss()\n\t\t\t]\n\n\n'''\nVGG\n'''\n\nVGG16=[\n\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tPool(pool_h=2, pool_w=2, stride=2),\n\t\t\t\n\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tPool(pool_h=2, pool_w=2, stride=2),\n\t\t\t\n\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tPool(pool_h=2, pool_w=2, stride=2),\n\t\t\t\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tPool(pool_h=2, pool_w=2, stride=2),\n\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tPool(pool_h=2, pool_w=2, stride=2),\n\t\t\t\n\t\t\tFlatten(),\n\t\t\t\n\t\t\tDense(output_size=4096),\n\t\t\tDense(output_size=4096),\n\t\t\tDense(output_size=1000),\n\t\t\t\n\t\t\tSoftmaxWithLoss()\n\t\t\t]\n\t\t\t\nVGG19=[\n\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tPool(pool_h=2, pool_w=2, stride=2),\n\t\t\t\n\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tPool(pool_h=2, pool_w=2, stride=2),\n\t\t\t\n\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tPool(pool_h=2, pool_w=2, stride=2),\n\t\t\t\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tPool(pool_h=2, pool_w=2, stride=2),\n\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tPool(pool_h=2, pool_w=2, stride=2),\n\t\t\t\n\t\t\tFlatten(),\n\t\t\t\n\t\t\tDense(output_size=4096),\n\t\t\tDense(output_size=4096),\n\t\t\tDense(output_size=1000),\n\t\t\t\n\t\t\tSoftmaxWithLoss()\n\t\t\t]\n\n\n'''\nResNet\n'''\t\n\nResNet18 = \t[\n\t\t\tConv({'f_num':64, 'f_size':7, 'pad':3, 'stride':2}),\n\t\t\tPool(pool_h=3, pool_w=3, stride=2, pad=1),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1})]),\n\n\t\t\tResV1([Conv({'f_num':128, 'f_size':3, 'pad':1, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':256, 'f_size':3, 'pad':1, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':512, 'f_size':3, 'pad':1, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1})]),\n\n\t\t\tPoolAvg(pool_h=2, pool_w=2, stride=2),\n\t\t\tFlatten(),\n\t\t\t\n\t\t\tDense(output_size=1000),\n\t\t\t\n\t\t\tSoftmaxWithLoss()\n\t\t\t]\n\nResNet34 = \t[\n\t\t\tConv({'f_num':64, 'f_size':7, 'pad':3, 'stride':2}),\n\t\t\tPool(pool_h=3, pool_w=3, stride=2, pad=1),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1})]),\n\n\t\t\tResV1([Conv({'f_num':128, 'f_size':3, 'pad':1, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':256, 'f_size':3, 'pad':1, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':512, 'f_size':3, 'pad':1, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1})]),\n\n\t\t\tPoolAvg(pool_h=2, pool_w=2, stride=2),\n\t\t\tFlatten(),\n\t\t\t\n\t\t\tDense(output_size=1000),\n\t\t\t\n\t\t\tSoftmaxWithLoss()\n\t\t\t]\n\nResNet50 = \t[\n\t\t\tConv({'f_num':64, 'f_size':7, 'pad':3, 'stride':2}),\n\t\t\tPool(pool_h=3, pool_w=3, stride=2, pad=1),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tPoolAvg(pool_h=2, pool_w=2, stride=2),\n\t\t\tFlatten(),\n\t\t\t\n\t\t\tDense(output_size=1000),\n\t\t\t\n\t\t\tSoftmaxWithLoss()\n\t\t\t]\n\nResNet101 = [\n\t\t\tConv({'f_num':64, 'f_size':7, 'pad':3, 'stride':2}),\n\t\t\tPool(pool_h=3, pool_w=3, stride=2, pad=1),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tPoolAvg(pool_h=2, pool_w=2, stride=2),\n\t\t\tFlatten(),\n\t\t\t\n\t\t\tDense(output_size=1000),\n\t\t\t\n\t\t\tSoftmaxWithLoss()\n\t\t\t]\n\t\t\t\nResNet152 = [\n\t\t\tConv({'f_num':64, 'f_size':7, 'pad':3, 'stride':2}),\n\t\t\tPool(pool_h=3, pool_w=3, stride=2, pad=1),\n\t\t\t\n\t\t\tResV1([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tConv({'f_num':128, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\tResV1([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV1([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tPoolAvg(pool_h=2, pool_w=2, stride=2),\n\t\t\tFlatten(),\n\t\t\t\n\t\t\tDense(output_size=1000),\n\t\t\t\n\t\t\tSoftmaxWithLoss()\n\t\t\t]\n\n\n'''\nResNetV2\n'''\t\n\t\t\nResNetV2_18 = \t[\n\t\t\tConv({'f_num':64, 'f_size':7, 'pad':3, 'stride':2}),\n\t\t\tPool(pool_h=3, pool_w=3, stride=2, pad=1),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1})]),\n\n\t\t\tResV2([Conv({'f_num':128, 'f_size':3, 'pad':1, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':256, 'f_size':3, 'pad':1, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':512, 'f_size':3, 'pad':1, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1})]),\n\n\t\t\tPoolAvg(pool_h=2, pool_w=2, stride=2),\n\t\t\tFlatten(),\n\t\t\t\n\t\t\tDense(output_size=1000),\n\t\t\t\n\t\t\tSoftmaxWithLoss()\n\t\t\t]\n\nResNetV2_34 = \t[\n\t\t\tConv({'f_num':64, 'f_size':7, 'pad':3, 'stride':2}),\n\t\t\tPool(pool_h=3, pool_w=3, stride=2, pad=1),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1})]),\n\n\t\t\tResV2([Conv({'f_num':128, 'f_size':3, 'pad':1, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':256, 'f_size':3, 'pad':1, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':512, 'f_size':3, 'pad':1, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1})]),\n\n\t\t\tPoolAvg(pool_h=2, pool_w=2, stride=2),\n\t\t\tFlatten(),\n\t\t\t\n\t\t\tDense(output_size=1000),\n\t\t\t\n\t\t\tSoftmaxWithLoss()\n\t\t\t]\n\nResNetV2_50 = \t[\n\t\t\tConv({'f_num':64, 'f_size':7, 'pad':3, 'stride':2}),\n\t\t\tPool(pool_h=3, pool_w=3, stride=2, pad=1),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tPoolAvg(pool_h=2, pool_w=2, stride=2),\n\t\t\tFlatten(),\n\t\t\t\n\t\t\tDense(output_size=1000),\n\t\t\t\n\t\t\tSoftmaxWithLoss()\n\t\t\t]\n\nResNetV2_101 = [\n\t\t\tConv({'f_num':64, 'f_size':7, 'pad':3, 'stride':2}),\n\t\t\tPool(pool_h=3, pool_w=3, stride=2, pad=1),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tPoolAvg(pool_h=2, pool_w=2, stride=2),\n\t\t\tFlatten(),\n\t\t\t\n\t\t\tDense(output_size=1000),\n\t\t\t\n\t\t\tSoftmaxWithLoss()\n\t\t\t]\n\t\t\t\nResNetV2_152 = [\n\t\t\tConv({'f_num':64, 'f_size':7, 'pad':3, 'stride':2}),\n\t\t\tPool(pool_h=3, pool_w=3, stride=2, pad=1),\n\t\t\t\n\t\t\tResV2([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':64, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':64, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tConv({'f_num':128, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':128, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':128, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tConv({'f_num':256, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':256, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':256, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':1024, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\t\n\t\t\tConv({'f_num':512, 'f_size':1, 'pad':0, 'stride':2}),\n\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\tResV2([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tResV2([Conv({'f_num':512, 'f_size':1, 'pad':0, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':512, 'f_size':3, 'pad':1, 'stride':1}),\n\t\t\t\t\t\tConv({'f_num':2048, 'f_size':1, 'pad':0, 'stride':1})]),\n\t\t\tPoolAvg(pool_h=2, pool_w=2, stride=2),\n\t\t\tFlatten(),\n\t\t\t\n\t\t\tDense(output_size=1000),\n\t\t\t\n\t\t\tSoftmaxWithLoss()\n\t\t\t]"""
BlueNet/__init__.py,0,b''
BlueNet/setting.py,0,b'try:\n\timport cupy as _np\n\timport cupyx.scipy.special.erf as _erf\n\tfrom cupy import exp as _exp\nexcept:\n\timport numpy as _np\n\tfrom scipy.special import erf as _erf\n\tfrom numpy import exp as _exp'
BlueNet/Dataset/Cifar.py,7,"b'# coding: utf-8\nimport sys,os\nsys.path.append("".."") \nimport pickle\nimport numpy as np\nfrom PIL import Image\nfrom BlueNet.Functions import _change_one_hot_label,label_smoothing\n\n\ndataset_dir = os.path.dirname(os.path.abspath(__file__))\nfile = dataset_dir+\'/data/cifar_data/cifar_data_batch_\'\n\ndef load_cifar(normalize=True, flatten=True, one_hot_label=False, smooth=False, type=np.float32):\n\tdataset = {}\n\twith open(file+\'01\', \'rb\') as f:\n\t\tdataset1 = pickle.load(f)\n\t\n\twith open(file+\'02\', \'rb\') as f:\n\t\tdataset2 = pickle.load(f)\n\t\n\twith open(dataset_dir+\'/data/cifar_data/cifar_test_batch\', \'rb\') as f:\n\t\ttestset = pickle.load(f,encoding=\'bytes\')\n\t\n\tdataset[b\'data\'] = np.asarray(np.vstack((dataset1[b\'data\'],dataset2[b\'data\'])))\n\tdataset[b\'labels\'] = np.asarray(np.hstack((dataset1[b\'labels\'],dataset2[b\'labels\'])))\n\ttestset[b\'data\'] = np.asarray(testset[b\'data\'])\n\ttestset[b\'labels\'] = np.asarray(testset[b\'labels\'])\n\t\n\tif normalize:\n\t\tdataset[b\'data\'] = dataset[b\'data\'].astype(type)\n\t\tdataset[b\'data\'] /= 255.0\n\t\ttestset[b\'data\'] = testset[b\'data\'].astype(type)\n\t\ttestset[b\'data\'] /= 255.0\n\n\tif not flatten:\n\t\t#dataset[b\'data\'] = np.vstack((dataset[b\'data\'].reshape(50000, 3, 32, 32),np.flip(dataset[b\'data\'].reshape(50000, 3, 32, 32),(1,3))))\n\t\t#dataset[b\'labels\'] = np.hstack((dataset[b\'labels\'],dataset[b\'labels\']))\n\t\tdataset[b\'data\'] = dataset[b\'data\'].reshape(50000, 3, 32, 32)\n\t\ttestset[b\'data\'] = testset[b\'data\'].reshape(10000, 3, 32, 32)\n\t\n\tif one_hot_label:\n\t\tdataset[b\'labels\'] = _change_one_hot_label(dataset[b\'labels\'],10)\n\t\ttestset[b\'labels\'] = _change_one_hot_label(testset[b\'labels\'],10)\n\t\n\tif smooth:\n\t\tdataset[b\'labels\'] = label_smoothing(dataset[b\'labels\'],0.1)\n\t\ttestset[b\'labels\'] = label_smoothing(testset[b\'labels\'],0.1)\n\t\n\treturn (dataset[b\'data\'], dataset[b\'labels\'].astype(type)), (testset[b\'data\'], testset[b\'labels\'].astype(type))\n\n\nif __name__ == \'__main__\':\n\t(A,B),(C,D) = load_cifar()\n\tprint(A.shape)\n\tprint(B.shape)\n\tprint(C.shape)\n\tprint(D.shape)\n\t'"
BlueNet/Dataset/Cifar100.py,2,"b'# coding: utf-8\nimport sys,os\nsys.path.append("".."")\nimport pickle\nimport numpy as np\nfrom PIL import Image\nfrom BlueNet.Functions import _change_one_hot_label,label_smoothing\n\n\ndataset ={}\ndataset1={}\ndataset2={}\n\ndataset_dir = os.path.dirname(os.path.abspath(__file__))\n\ndef load_cifar100(normalize=True, flatten=True, one_hot_label=False, smooth=False, type=np.float32):\n\twith open(dataset_dir+\'/data/cifar100_data/cifar100_data_train1\', \'rb\',) as f:\n\t\tdataset1 = pickle.load(f,encoding=\'bytes\')\n\t\n\twith open(dataset_dir+\'/data/cifar100_data/cifar100_data_train2\', \'rb\',) as f:\n\t\tdataset2 = pickle.load(f,encoding=\'bytes\')\n\t\n\twith open(dataset_dir+\'/data/cifar100_data/cifar100_data_test\', \'rb\') as f:\n\t\ttestset = pickle.load(f,encoding=\'bytes\')\n\t\n\tdataset[b\'data\'] = np.hstack((dataset1[b\'data\'],dataset2[b\'data\']))\n\tdataset[b\'fine_labels\'] = dataset1[b\'fine_labels\']\n\t\n\tif normalize:\n\t\tdataset[b\'data\'] = dataset[b\'data\'].astype(type)\n\t\tdataset[b\'data\'] /= 255.0\n\t\ttestset[b\'data\'] = testset[b\'data\'].astype(type)\n\t\ttestset[b\'data\'] /= 255.0\n\n\tif one_hot_label:\n\t\tdataset[b\'fine_labels\'] = _change_one_hot_label(dataset[b\'fine_labels\'],100)\n\t\ttestset[b\'fine_labels\'] = _change_one_hot_label(testset[b\'fine_labels\'],100)\n\n\tif not flatten:\n\t\tdataset[b\'data\'] = dataset[b\'data\'].reshape(-1, 3, 32, 32)\n\t\ttestset[b\'data\'] = testset[b\'data\'].reshape(-1, 3, 32, 32)\n\t\n\tif smooth:\n\t\tdataset[b\'fine_labels\'] = label_smoothing(dataset[b\'fine_labels\'],0.1)\n\t\ttestset[b\'fine_labels\'] = label_smoothing(testset[b\'fine_labels\'],0.1)\n\t\n\treturn (dataset[b\'data\'], dataset[b\'fine_labels\']), (testset[b\'data\'], testset[b\'fine_labels\'])\n\nif __name__ == \'__main__\':\n\t(a,b),(c,d) = load_cifar100(False,False,True)\n\tprint(a.shape)\n\tprint(b.shape)'"
BlueNet/Dataset/Emnist.py,15,"b'# coding: utf-8\nimport sys,os\nsys.path.append("".."")  \nimport pickle\nimport gzip\nimport numpy as np\nfrom PIL import Image  \nfrom BlueNet.Functions import _change_one_hot_label, label_smoothing\n\n\nimg_size = 784\ndataset = {}\ntestset = {}\n\ndataset_dir = os.path.dirname(os.path.abspath(__file__))\nfile = {\n\t\'train_img\':dataset_dir+\'/data/emnist_data/emnist-train-images.gz\',\n\t\'train_label\':dataset_dir+\'/data/emnist_data/emnist-train-labels.gz\',\n\t\'test_img\':dataset_dir+\'/data/emnist_data/emnist-test-images.gz\',\n\t\'test_label\':dataset_dir+\'/data/emnist_data/emnist-test-labels.gz\'\n}\n\ndef load_labels(file):\n\twith gzip.open(file, \'r\',) as f:\n\t\tlabels = np.frombuffer(f.read(), np.uint8, offset=8)\n\t\n\treturn labels\n\ndef load_imgs(file):\n\twith gzip.open(file, \'r\',) as f:\n\t\timgs = np.frombuffer(f.read(), np.uint8, offset=16)\n\timgs = imgs.reshape(-1, img_size)\n\treturn imgs\n\ndef load_emnist(normalize=True, flatten=True, one_hot_label=True, smooth=False, choose = 0, type=np.float32):\n\tdataset[\'data\'] = load_imgs(file[\'train_img\'])\n\ttestset[\'data\'] = load_imgs(file[\'test_img\'])\n\tdataset[\'labels\'] = load_labels(file[\'train_label\'])\n\ttestset[\'labels\'] = load_labels(file[\'test_label\'])\n\n\tif normalize:\n\t\tdataset[\'data\'] = dataset[\'data\'].astype(type)\n\t\tdataset[\'data\'] /= 255.0\n\t\ttestset[\'data\'] = testset[\'data\'].astype(type)\n\t\ttestset[\'data\'] /= 255.0\n\n\tif one_hot_label:\n\t\tdataset[\'labels\'] = _change_one_hot_label(dataset[\'labels\']-1,26)\n\t\ttestset[\'labels\'] = _change_one_hot_label(testset[\'labels\']-1,26)\n\n\tif not flatten:\n\t\tdataset[\'data\'] = dataset[\'data\'].reshape(-1, 1, 28, 28).transpose(0,1,3,2)\n\t\ttestset[\'data\'] = testset[\'data\'].reshape(-1, 1, 28, 28).transpose(0,1,3,2)\n\t\n\tif choose == 0:\n\t\tdata_choose = np.arange(0,len(dataset[\'data\']))\n\t\ttest_choose = np.arange(0,len(testset[\'data\']))\n\telif choose == 1:\n\t\tdata_choose = np.arange(1,len(dataset[\'data\'])+1,2)\n\t\ttest_choose = np.arange(1,len(testset[\'data\'])+1,2)\n\telif choose == 2:\n\t\tdata_choose = np.arange(0,len(dataset[\'data\']),2)\n\t\ttest_choose = np.arange(0,len(testset[\'data\']),2)\n\telif choose == 3:\n\t\tdata_choose = np.arange(0,len(dataset[\'data\']),3)\n\t\ttest_choose = np.arange(0,len(testset[\'data\']),3)\n\telif choose == 4:\n\t\tdata_choose = np.arange(1,len(dataset[\'data\']+1),3)\n\t\ttest_choose = np.arange(1,len(testset[\'data\']+1),3)\n\telif choose == 5:\n\t\tdata_choose = np.arange(2,len(dataset[\'data\'])+2,3)\n\t\ttest_choose = np.arange(2,len(testset[\'data\'])+2,3)\n\t\n\tif smooth:\n\t\tdataset[\'labels\'] = label_smoothing(dataset[\'labels\'],0.1)\n\t\ttestset[\'labels\'] = label_smoothing(testset[\'labels\'],0.1)\n\t\n\treturn (dataset[\'data\'][data_choose], dataset[\'labels\'][data_choose]), (testset[\'data\'][test_choose], testset[\'labels\'][test_choose])\n\n\nif __name__ == \'__main__\':\n\t(x_train, t_train), (x_test, t_test) = load_emnist(normalize=False,flatten=False, one_hot_label=False)\n\tImage.fromarray(x_test[15001][0]).show()\n\tprint(t_test[15001])\n'"
BlueNet/Dataset/Mnist.py,1,"b'# coding: utf-8\nimport sys,os\nsys.path.append("".."") \nimport os.path\nimport pickle\nimport numpy as np\nfrom BlueNet.Functions import _change_one_hot_label, label_smoothing\n\n\ndataset_dir = os.path.dirname(os.path.abspath(__file__))\nsave_file = dataset_dir + ""/data/mnist_data/mnist.pkl""\n\ntrain_num = 60000\ntest_num = 10000\nimg_dim = (1, 28, 28)\nimg_size = 784\n\ndef load_mnist(normalize=True, flatten=True, one_hot_label=True, smooth=False, type=np.float32):\n\twith open(save_file, \'rb\') as f:\n\t\tdataset = pickle.load(f)\n\n\tif normalize:\n\t\tfor key in (\'train_img\', \'test_img\'):\n\t\t\tdataset[key] = dataset[key].astype(type)\n\t\t\tdataset[key] /= 255.0\n\n\tif one_hot_label:\n\t\tdataset[\'train_label\'] = _change_one_hot_label(dataset[\'train_label\'],10)\n\t\tdataset[\'test_label\'] = _change_one_hot_label(dataset[\'test_label\'],10)\n\n\tif not flatten:\n\t\tfor key in (\'train_img\', \'test_img\'):\n\t\t\tdataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n\telse:\n\t\tfor key in (\'train_img\', \'test_img\'):\n\t\t\tdataset[key] = dataset[key].reshape(-1, 784)\n\t\n\tif smooth:\n\t\tdataset[\'train_label\'] = label_smoothing(dataset[\'train_label\'],0.01)\n\t\tdataset[\'test_label\'] = label_smoothing(dataset[\'test_label\'],0.01)\n\t\n\treturn (dataset[\'train_img\'], dataset[\'train_label\']), (dataset[\'test_img\'], dataset[\'test_label\'])\n'"
BlueNet/Dataset/__init__.py,0,b''
