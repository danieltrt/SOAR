file_path,api_count,code
Final_Plots.py,18,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Dec 16 16:47:30 2018\n\n@author: dead\n""""""\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\nimport os\nimport pandas as pd\nos.chdir(""final_results/"")\n\n\n\ne4 = np.load(""mean100_44.npy"")\ne666 = np.load(""666mean100.npy"")\ne66 = e666[:100]\nekalo = np.load(""kalomean100.npy"")\n\n\n####DDPG##############\nprint(""Max Average Reward |DDPG+PPO: %.3f | RDDPG: %.3f | DDPG: %.3f"" %(max(ekalo),max(e4),max(e66)))\nlala = np.arange(1,len(e4)+1)*100\nplt.figure(figsize=(10,8))\nplt.tick_params(size=5, labelsize=16.5)\nplt.plot(lala,ekalo, label=""DDPG+PPO"")\nplt.plot(lala,e4, label=""RDDPG"")\nplt.plot(lala,e66, label=""DDPG"")\nplt.xticks(np.arange(0,101,10)*100)\nplt.xlabel(""Episodes"", fontsize = 17)\nplt.ylabel(""Mean Reward"", fontsize = 17)\nplt.title(\'Average Reward per 100 episodes\', fontsize=17)\nplt.legend(loc=0, fontsize=15)\nplt.savefig(""DDPG_PPO_Results.png"")\n######################\ne44 = np.load(""e44mean100.npy"")#\nlamean = np.load(""lalamean100.npy"")#\nprint(""Max Average Reward |Big-DDPG: %.3f | Small-DDPG: %.3f "" %(max(lamean),max(e44[0:100])))\n\nlala = np.arange(1,len(lamean)+1)*100\nplt.figure(figsize=(10,8))\nplt.tick_params(size=5, labelsize=16.5)\nplt.plot(lala,lamean, label=""Big-DDPG"")\nplt.plot(lala,e44[0:100], label=""Small-DDPG"")\nplt.xticks(np.arange(0,101,10)*100)\nplt.xlabel(""Episodes"", fontsize = 17)\nplt.ylabel(""Mean Reward"", fontsize = 17)\nplt.title(\'Average Reward per 100 episodes\', fontsize=17)\nplt.legend(loc=0, fontsize=15)\nplt.savefig(""DDPG_Results.png"")\n\n# Only small ddpg\nlala = np.arange(1,len(e44)+1)*100\nplt.figure(figsize=(10,8))\nplt.tick_params(size=5, labelsize=16.5)\nplt.plot(lala,e44, label=""Small-DDPG"")\nplt.xticks(np.arange(0,501,100)*100)\nplt.xlabel(""Episodes"", fontsize = 17)\nplt.ylabel(""Mean Reward"", fontsize = 17)\nplt.title(\'Average Reward per 100 episodes\', fontsize=17)\nplt.legend(loc=0, fontsize=15)\nplt.savefig(""Small_ddpg.png"")\n\n\n#########################################################################\n### PPO___#####################\n# Average 100 Rewarss PPO\n\nkeras = np.load(""mean100_k.npy"")\ntf = np.load(""mean100_tf.npy"")\nprint(""Max Average Reward |keras: %.3f | tf: %.3f"" %(max(keras),max(tf)))\nlala = np.arange(1,len(keras)+1)*100\nplt.figure(figsize=(10,8))\nplt.tick_params(size=5, labelsize=16.5)\nplt.plot(lala,tf[:-1], label=""Boosted PPO"")\nplt.plot(lala,keras, label=""Simple PPO"")\n#plt.plot(lala,lolo,label=""DDPG"")\nplt.xticks(np.arange(0,101,10)*100)\nplt.xlabel(""Episodes"", fontsize = 17)\nplt.ylabel(""Mean Reward"", fontsize = 17)\nplt.title(\'Average Reward per 100 episodes\', fontsize=17)\nplt.legend(loc=0, fontsize=15)\nplt.savefig(""PPO_avg.png"")\n######################################################\n######################################################\n # Rewards\nkeras_rew = pd.read_csv(""keras.csv"")  \nprint(\'Maximum Total Reward: \', max(keras_rew.iloc[:,2]))\n\nplt.figure(figsize=(9,7))\nplt.tick_params(size=5, labelsize=15)\nplt.plot(keras_rew.iloc[:,2])\nplt.xticks(np.arange(0,1100,200), np.arange(0,1100,200)*10)\nplt.xlabel(""Episodes"", fontsize = 14)\nplt.ylabel(""Episode Total Reward"", fontsize = 14)\nplt.title(\'PPO Episode Rewards\', fontsize=15)\nplt.savefig(""PPO_Rewards.png"")\n\ntf_rew = pd.read_csv(""tf.csv"")\nprint(\'Maximum Total Reward: \', max(tf_rew.iloc[:,2]))\n\nplt.figure(figsize=(9,7))\nplt.tick_params(size=5, labelsize=15)\nplt.plot(tf_rew.iloc[:,2])\nplt.plot(np.repeat(300,1000), \'--\' )\nplt.xticks(np.arange(0,1100,200), np.arange(0,1100,200)*10)\nplt.xlabel(""Episodes"", fontsize = 14)\nplt.ylabel(""Episode Total Reward"", fontsize = 14)\nplt.title(\'Boosted PPO Episode Rewards\', fontsize=15)\nplt.savefig(""Boost_PPO_Rewards.png"")\n#########################################\n########################################\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
DQN/DQN2_BipedalWalker.py,17,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Apr 25 20:41:53 2018\n\n@author: dead\n""""""\n\n#from __future__ import print_function\nimport pandas\nimport keras\nfrom keras.models import load_model\nfrom keras.initializers import RandomUniform\nfrom keras.regularizers import l2\nfrom keras import backend as K\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Input, Activation\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport time\nimport numpy as np\nimport gym\nimport random\nfrom collections import deque\nimport tensorflow as tf\nimport os\n    \nclass AGENT:\n    def __init__(self, nx, ny, lr, gamma, s_link):\n        self.nx = nx  #  Observation array length   nx = env.observation_space.shape[0]\n        self.ny = ny  #   Action space length       ny = env.action_space.n\n        self.gamma = gamma\n        self.lr = lr\n        #self.l_link =l_link \n        self.s_link =s_link \n        self.deck = deque(maxlen=2000)\n        self.e = 1.0\n        self.e_= 0.01\n        self.dc= 0.995\n        self.weight_decay = 10**-2\n        self.los = []\n        \n        \n        if os.path.isfile(\'./\' + self.s_link):\n            print(""LOAD existing keras model...."")\n            self.model = load_model(self.s_link)\n            print(self.model.summary())\n        else:\n            # Call function model to build the model   \n            print(""Build a new model"")\n            self.model = self.MODEL()      \n        \n        self.ep_obs, self.ep_rewards, self.ep_action, self.ep_obs_new, self.ep_flags = [], [], [], [], []\n        \n            \n    def choose_action(self,observation):\n        if np.random.rand() <= self.e : \n            action = np.random.uniform(-1,1,4)\n            return action\n            \n        probs = self.model.predict(observation)    \n        #action = np.argmax(probs[0])\n        action = probs[0]\n        return action\n                \n    \n    def storing(self, observation, action, reward, observation_new, flags ):\n        self.deck.append((observation, action, reward, observation_new, flags ))\n        #self.ep_obs.append(observation)\n        #self.ep_action.append(action)\n        self.ep_rewards.append(reward)\n        #self.ep_obs_new.append(observation_new)\n        #self.ep_flags.append(flag)\n        \n    def save(self,name):\n        self.model.save(name)\n\n    def MODEL(self):                     \n        # Build Network\n        model = Sequential()\n        model.add(Dense(400, input_dim=self.nx, activation=\'relu\' ,kernel_regularizer=l2(self.weight_decay)))\n        model.add(Dense(300,  activation=\'relu\', kernel_regularizer=l2(self.weight_decay)))\n        model.add(Dense(self.ny, activation=\'tanh\',\\\n                        bias_initializer=RandomUniform(minval=-0.003, maxval=0.003)))\n        model.compile(loss=\'mse\',\n                      optimizer=Adam(lr=self.lr))\n                    \n        return model\n        \n    def TRAIN(self, batch):\n        sample_indx = random.sample(self.deck, batch)\n        self.los = []\n        \n        for observation, act, reward, obs_new, done in sample_indx:            \n            target = np.repeat(reward,4).reshape(1,-1)\n            if not done: #((1-ALPHA)*xreward)+ (ALPHA* (GAMMA * futurereward))\n                target = ( (1.0-0.1)*reward + 0.1 * (self.gamma*self.model.predict(obs_new)[0]))                \n                target = target.reshape(1,-1)\n            #target_old = self.model.predict(observation)\n            #target_old[0][act] = target\n            target_old = target\n            # Train\n            #K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_\xe2\x80\x8c\xe2\x80\x8bparallelism_threads=\xe2\x80\x8c\xe2\x80\x8b32, inter_op_parallelism_threads=32)))\n            history = self.model.fit(x=observation, y=target_old,\\\n                                #batch_size=1,\\\n                                verbose=0,\\\n                                epochs=5)\n            self.los.append(history.history[\'loss\'])    \n            \n            self.ep_obs, self.ep_rewards, self.ep_action, self.ep_obs_new, self.ep_flags = [], [], [], [], []\n        \n        mm = np.mean(self.los)                \n        if self.e >= self.e_:\n            self.e *= self.dc\n        #self.save(self.s_link)\n        return history, mm\n\nif __name__ == \'__main__\':\n    \n    BATCH = 256\n    rendering = input(""Visualize rendering ? [y/n]:  "")\n    \n    s_link = ""BipedalWalker_model.h5""\n    \n    RENDER_REWARD_MIN = 5000\n    RENDER_ENV = False\n    if rendering == \'y\': RENDER_ENV = True  #flag for rendering the environment\n    EPISODES = 9000    # Number of episodes\n    \n    env = gym.make(\'BipedalWalker-v2\')\n    env = env.unwrapped\n    \n    # Observation and Action array length\n    nx = env.observation_space.shape[0] \n    ny = env.action_space.shape[0]\n    lr = 0.0001\n    gamma = 0.99\n    agent = AGENT(nx,ny, lr, gamma, s_link)\n    \n    rewards_over_time = []\n    error = []\n    epsilon = []\n    rew_var = []\n    rew_mean = []\n    mean_100 = []\n    seed = np.random.seed(666)\n         \n    print(""-----------------------------------"")        \n    print(""Environment Observation_space: "", env.observation_space)\n    print(""Environment Action_space: "", env.action_space) \n    print(""-----------------------------------\\n"")\n    w = 0\n        \n    # Start running the episodes        \n    for i in range(EPISODES): \n        observation = env.reset()         \n        observation = observation.reshape(1,-1)                \n        start = time.time()\n        counter = 0\n        while True:            \n            if RENDER_ENV:\n                env.render()\n            \n            action = agent.choose_action(observation)\n            observation_new, reward, flag, inf = env.step(action)\n            observation_new = observation_new.reshape(1,-1)                    \n            counter +=1\n            # Store new information\n            agent.storing(observation, action, reward, observation_new, flag)   \n            observation = observation_new         \n            # Measure the time\n            end = time.time()\n            time_space = end - start\n            \n            if time_space > 10:\n                flag = True\n          \n            # Sum the episode rewards\n            ep_rew_total = sum(agent.ep_rewards)\n            mean = np.mean(agent.ep_rewards)\n            var = np.var(agent.ep_rewards)\n            if ep_rew_total < -300:\n                flag = True\n            \n            if flag==True:\n                rewards_over_time.append(ep_rew_total)\n                rew_mean.append(mean)\n                rew_var.append(var)\n                max_reward = np.max(rewards_over_time)\n                episode_max = np.argmax(rewards_over_time)\n                if ep_rew_total >=300 :\n                    w = w + 1\n                    agent.save(s_link)\n                                        \n                print(""++++++++++++++++++++++++++++++++++++++++++++++++++++++++"")\n                print(""Episode: "", i)\n                print(""Time: "", np.round(time_space, 2),""secs"")\n                print(""Traj: "" + str(counter))\n                print(""Reward:"", ep_rew_total)\n                print(""Maximum Reward: "" + str(max_reward) + ""  on Episode: "" + str(episode_max))\n                print(""Times win: "" + str(w))\n                \n                if i % 100 ==0:\n                    print(""Mean reward of the past 100 episodes: "", str(np.mean(rewards_over_time[-100:])))\n                    mean_100.append(np.mean(rewards_over_time[-100:]))\n                    f = open(\'results.txt\',\'a\')\n                    f.write(\'\\n\' + str(np.mean(rewards_over_time[-100:])))\n                    f.close()\n                \n                # Start training the Neural Network\n                #if BATCH >= len() \n                hist, mm= agent.TRAIN(BATCH)\n                \n                epsilon.append(agent.e)\n                                           \n                error.append(mm)\n                \n                if max_reward > RENDER_REWARD_MIN: RENDER_ENV = True\n                \n                break\n    \n    np.save(""rewards_over_time"", rewards_over_time)\n    np.save(""mean100"", mean_100) \n            \n    plt.figure(figsize=(8,6))\n    plt.plot(error)\n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Average Error"")\n    plt.title(""Average_Loss Vs Episodes"")\n    plt.show()\n    plt.savefig(""Average_Loss_Vs_Episodes.png"")\n    \n    plt.figure(figsize=(8,6))\n    plt.plot(epsilon)\n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Epsilon value"")\n    plt.title(""Epsilon Vs Episodes"")\n    plt.show()\n                \n    plt.figure(figsize=(8,6))            \n    plt.plot(rewards_over_time, label=""Rewards"")\n    plt.plot(rew_mean, label=""Mean"")\n    plt.plot(rew_var, label=""Variance"")    \n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Rewards"")\n    plt.title(""Rewards per Episode"")\n    plt.legend(loc=0)\n    plt.show()        \n    plt.savefig(""Rewards_per_Episode.png"")        \n            \n    plt.figure(figsize=(8,6))\n    plt.plot(mean_100)\n    plt.xlabel(""100_episodes"")\n    plt.ylabel(""Mean_value"")\n    plt.xticks(np.arange(0,9000,100))\n    plt.savefig(""mean_100.png"")        \n            \n            \n            \n           \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n\n\n'"
DQN/DQN_BipedalWalker.py,16,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Apr 25 20:41:53 2018\n\n@author: dead\n""""""\n\n#from __future__ import print_function\nimport keras\nfrom keras.models import load_model\nfrom keras import backend as K\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Input, Activation\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport time\nimport numpy as np\nimport gym\nimport random\nfrom collections import deque\nimport tensorflow as tf\nimport os\n    \nclass AGENT:\n    def __init__(self, nx, ny, lr, gamma, s_link):\n        self.nx = nx  #  Observation array length   nx = env.observation_space.shape[0]\n        self.ny = ny  #   Action space length       ny = env.action_space.n\n        self.gamma = gamma\n        self.lr = lr\n        #self.l_link =l_link \n        self.s_link =s_link \n        self.deck = deque(maxlen=2000)\n        self.e = 0.7\n        self.e_= 0.01\n        self.dc= 0.995\n        self.los = []\n        \n        \n        if os.path.isfile(\'./\' + self.s_link):\n            print(""LOAD existing keras model...."")\n            self.model = load_model(self.s_link)\n            print(self.model.summary())\n        else:\n            # Call function model to build the model   \n            print(""Build a new model"")\n            self.model = self.MODEL()      \n        \n        self.ep_obs, self.ep_rewards, self.ep_action, self.ep_obs_new, self.ep_flags = [], [], [], [], []\n        \n            \n    def choose_action(self,observation):\n        if np.random.rand() <= self.e : \n            action = np.random.uniform(-1,1,4)\n            return action\n            \n        probs = self.model.predict(observation)    \n        #action = np.argmax(probs[0])\n        action = probs[0]\n        return action\n                \n    \n    def storing(self, observation, action, reward, observation_new, flags ):\n        self.deck.append((observation, action, reward, observation_new, flags ))\n        #self.ep_obs.append(observation)\n        #self.ep_action.append(action)\n        self.ep_rewards.append(reward)\n        #self.ep_obs_new.append(observation_new)\n        #self.ep_flags.append(flag)\n        \n    def save(self,name):\n        self.model.save(name)\n\n    def MODEL(self):                     \n        # Build Network\n        model = Sequential()\n        model.add(Dense(400, input_dim=self.nx, activation=\'relu\'))\n        model.add(Dense(300,  activation=\'relu\'))\n        model.add(Dense(self.ny, activation=\'linear\'))\n        model.compile(loss=\'mse\',\n                      optimizer=Adam(lr=self.lr))\n                    \n        return model\n        \n    def TRAIN(self, batch):\n        sample_indx = random.sample(self.deck, batch)\n        self.los = []\n        \n        for observation, act, reward, obs_new, done in sample_indx:            \n            target = reward\n            if not done: #((1-ALPHA)*xreward)+ (ALPHA* (GAMMA * futurereward))\n                target = ( (1.0-0.1)*reward + 0.1 * (self.gamma*np.amax(self.model.predict(obs_new)[0])))                \n            \n            target_old = self.model.predict(observation)\n            #target_old[0][act] = target\n            target_old[0] = target\n            # Train\n            #K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_\xe2\x80\x8c\xe2\x80\x8bparallelism_threads=\xe2\x80\x8c\xe2\x80\x8b32, inter_op_parallelism_threads=32)))\n            history = self.model.fit(x=observation, y=target_old,\\\n                                #batch_size=1,\\\n                                verbose=0,\\\n                                epochs=1)\n            self.los.append(history.history[\'loss\'])    \n            \n            self.ep_obs, self.ep_rewards, self.ep_action, self.ep_obs_new, self.ep_flags = [], [], [], [], []\n        \n        mm = np.mean(self.los)                \n        if self.e >= self.e_:\n            self.e *= self.dc\n        #self.save(self.s_link)\n        return history, mm\n\nif __name__ == \'__main__\':\n    \n    BATCH = 16\n    rendering = input(""Visualize rendering ? [y/n]:  "")\n    \n    s_link = ""BipedalWalker_model.h5""\n    \n    RENDER_REWARD_MIN = 5000\n    RENDER_ENV = False\n    if rendering == \'y\': RENDER_ENV = True  #flag for rendering the environment\n    EPISODES = 9000    # Number of episodes\n    \n    env = gym.make(\'BipedalWalker-v2\')\n    env = env.unwrapped\n    \n    # Observation and Action array length\n    nx = env.observation_space.shape[0] \n    ny = env.action_space.shape[0]\n    lr = 0.001\n    gamma = 0.98\n    agent = AGENT(nx,ny, lr, gamma, s_link)\n    \n    rewards_over_time = []\n    error = []\n    epsilon = []\n    rew_var = []\n    rew_mean = []\n    mean_100 = []\n    seed = np.random.seed(666)\n         \n    print(""-----------------------------------"")        \n    print(""Environment Observation_space: "", env.observation_space)\n    print(""Environment Action_space: "", env.action_space) \n    print(""-----------------------------------\\n"")\n    w = 0\n        \n    # Start running the episodes        \n    for i in range(EPISODES): \n        observation = env.reset()         \n        observation = observation.reshape(1,-1)                \n        start = time.time()\n        \n        while True:            \n            if RENDER_ENV:\n                env.render()\n            \n            action = agent.choose_action(observation)\n            observation_new, reward, flag, inf = env.step(action)\n            observation_new = observation_new.reshape(1,-1)                    \n            # Store new information\n            agent.storing(observation, action, reward, observation_new, flag)   \n            observation = observation_new         \n            # Measure the time\n            end = time.time()\n            time_space = end - start\n            \n            if time_space > 20:\n                flag = True\n          \n            # Sum the episode rewards\n            ep_rew_total = sum(agent.ep_rewards)\n            mean = np.mean(agent.ep_rewards)\n            var = np.var(agent.ep_rewards)\n            if ep_rew_total < -300:\n                flag = True\n            \n            if flag==True:\n                rewards_over_time.append(ep_rew_total)\n                rew_mean.append(mean)\n                rew_var.append(var)\n                max_reward = np.max(rewards_over_time)\n                episode_max = np.argmax(rewards_over_time)\n                if ep_rew_total >=300 :\n                    w = w + 1\n                    agent.save(s_link)\n                                        \n                print(""++++++++++++++++++++++++++++++++++++++++++++++++++++++++"")\n                print(""Episode: "", i)\n                print(""Time: "", np.round(time_space, 2),""secs"")\n                print(""Reward:"", ep_rew_total)\n                print(""Maximum Reward: "" + str(max_reward) + ""  on Episode: "" + str(episode_max))\n                print(""Times win: "" + str(w))\n                \n                if i % 100 ==0:\n                    print(""Mean reward of the past 100 episodes: "", str(np.mean(rewards_over_time[-100:])))\n                    mean_100.append(np.mean(rewards_over_time[-100:]))\n                    f = open(\'results.txt\',\'a\')\n                    f.write(\'\\n\' + str(np.mean(rewards_over_time[-100:])))\n                    f.close()\n                \n                # Start training the Neural Network\n                hist, mm= agent.TRAIN(BATCH)\n                \n                epsilon.append(agent.e)\n                                           \n                error.append(mm)\n                \n                if max_reward > RENDER_REWARD_MIN: RENDER_ENV = True\n                \n                break\n            \n    plt.figure(1)\n    plt.plot(error)\n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Average Error"")\n    plt.title(""Average_Loss Vs Episodes"")\n    plt.show()\n    \n    plt.figure(1)\n    plt.plot(epsilon)\n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Epsilon value"")\n    plt.title(""Epsilon Vs Episodes"")\n    plt.show()\n    \n    np.save(""rewards_over_time"", rewards_over_time)\n    np.save(""mean100"", mean_100)            \n            \n            \n    plt.figure(1)            \n    plt.plot(rewards_over_time, label=""Rewards"")\n    plt.plot(rew_mean, label=""Mean"")\n    plt.plot(rew_var, label=""Variance"")    \n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Rewards"")\n    plt.title(""Rewards per Episode"")\n    plt.legend(loc=0)\n    plt.show()        \n            \n            \n            \n            \n            \n            \n           \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n\n\n'"
DQN/DQNlalala.py,20,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Apr 25 20:41:53 2018\n\n@author: dead\nACTOR-CRITIC Network for Robot Locomotion\n\n""""""\n\n#from __future__ import print_function\nfrom keras.models import load_model, Sequential, Model\nfrom keras.initializers import RandomUniform\nfrom keras.regularizers import l2\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras import backend as K\nfrom keras.layers import Dense, Dropout, Input, MaxPooling1D,Conv1D,Conv2D,LSTM, Flatten, Reshape, GaussianNoise\nfrom keras.layers.merge import Add, Multiply, Concatenate\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport time\nimport numpy as np\nimport gym\nimport gym.wrappers\nimport random\nfrom collections import deque\nimport tensorflow as tf\nimport os\n#os.chdir(""/home/dead/Documents/Master_Research/ddpg/"")\n#from OrnsteinUhlenbeckProcess import OrnsteinUhlenbeckProcess\nfrom tqdm import tqdm\n    \nclass AGENT:\n    def __init__(self, nx, ny, s_link, sess):\n        #self.nx = nx[0]  #  Observation array length   nx = env.observation_space.shape[0]\n        self.nx_lidar = 10\n        self.nx_obs = 14\n        self.ny = ny  #   Action space length       ny = env.action_space.n\n        self.lr_actor = 1e-4 # Actor_Learning rate\n        self.lr_critic = 3e-4 # Critic_Learning rate\n        self.gamma = 0.99\n        self.alpha = 0.1\n        self.s_link =s_link \n        self.sess = sess\n        self.deck = deque(maxlen=4000)\n        self.e = 1.0\n        self.e_= 0.01\n        self.dc= 0.9999\n        self.tau = 1e-3\n        self.weight_decay = 0.0001\n        self.los = []\n        self.layers=[512,256]\n        self.k_r = l2(self.weight_decay)\n        self.k_init = RandomUniform()\n        self.b_init = RandomUniform(minval=-0.003, maxval=0.003)\n        self.parameters={\'lr_actor\': self.lr_actor,\'lr_critic\':self.lr_critic,\'gamma\':self.gamma,\n                         \'alpha\':self.alpha, \'tau\':self.tau,\'dc\':self.dc}\n        \n        \n        if os.path.isfile(\'./\' + self.s_link):\n            self.message = [""\\nLOAD existing keras model...."", self.model.summary()]\n            self.model = load_model(self.s_link)\n        else:\n            self.message = \'\\nBuilding New Model >_\'\n            ### Setting Actor ###\n            #######################################################\n            self.actor_lidar_input, self.actor_state_input, self.actor_local = self.Actor()      \n            _,_, self.actor_target = self.Actor()\n            # Open a placeholder for gradiends\n            self.actor_critic_grads = tf.placeholder(tf.float32, [None,self.ny])\n            actor_local_weights = self.actor_local.trainable_weights\n            self.actor_grads = tf.gradients(self.actor_local.output, actor_local_weights, self.actor_critic_grads)\n            grads = zip(self.actor_grads, actor_local_weights)\n            self.optimize = tf.train.AdamOptimizer(self.lr_actor).apply_gradients(grads)\n            ######################################################################\n            \n            ### Setting Critic ###\n            ####################################################################################         \n            self.critic_lidar_input, self.critic_state_input, self.critic_action_input, self.critic_local = self.Critic()      \n            _,_, _, self.critic_target = self.Critic()\n            # Open placeholder for gradients\n            self.critic_grads = tf.gradients(self.critic_local.output,  self.critic_action_input)\n            \n            # Initialize for later grafient calculations\n            self.sess.run(tf.global_variables_initializer())\n            \n        # Empty the lists\n        self.ep_rewards, self.ep_obs, self.ep_act, self.ep_obs_new, self.ep_flags=[], [], [],[], []\n        \n    def choose_action(self,observation):\n        state = observation[0][:14].reshape((1,14))\n        lidar = observation[0][14:].reshape((1,10))\n        # Use epsilon-greedy algorithm\n        if np.random.rand() <= self.e : \n            # epsilon Greedy             \n            if self.e >= self.e_:\n                self.e *= self.dc\n                epsilon.append(self.e)\n            action_ = []\n            values = []\n            for i in range(1000):\n                action_.append(np.random.uniform(-1,1,4).reshape(1,-1))\n                values.append(self.critic_target.predict([lidar,state,action_[-1]])[0][0])\n                \n            action = action_[np.argmax(values)]\n            return action       \n        \n        action = self.actor_local.predict([lidar,state])\n        acts_, values=[], []\n        for i in range(10):\n            gaussian_noise = np.random.normal(0, 0.01 , 4)\n            acts_.append(action + gaussian_noise)\n            values.append(self.critic_target.predict([lidar,state,acts_[-1]])[0][0])\n            \n        action = acts_[np.argmax(values)]\n        return action\n                    \n    def storing(self, observation, action, reward, observation_new, flags ):\n        # Storing for replay-expirience\n        self.deck.append((observation, action, reward, observation_new, flags ))\n        self.ep_rewards.append(reward)\n        self.ep_obs.append(observation)\n        self.ep_act.append(action)\n        self.ep_obs_new.append(observation_new)\n        self.ep_flags.append(flags)\n        \n    def save(self,name):\n        # Save network configuration\n        self.actor_local.save(name)\n        self.critic_local.save(name)\n\n    def Actor(self):                     \n        # Build Network for Actor\n        #input=[lidar_input,state_input]\n        lidar_input = Input(shape=(self.nx_lidar,))\n        lidar_conv = Dense(self.layers[0], activation=\'relu\', kernel_regularizer=self.k_r)(lidar_input)\n        #pool = MaxPooling1D(4)(lidar_conv)\n        #flat = Flatten()(lidar_conv)\n               \n        state_input = Input(shape=(self.nx_obs,))\n        state_h1 = Dense(self.layers[0], activation=\'relu\', kernel_regularizer=self.k_r)(state_input)\n        gauss = GaussianNoise(1.0)(state_h1)\n        #gauss = Flatten()(gauss)\n        \n        merged = Concatenate()([lidar_conv,gauss])\n        #merged_reshaped = Reshape((256,1))(merged)\n        merged_lstm = Dense(self.layers[1],activation=\'relu\')(merged)\n        gauss_ = GaussianNoise(1.0)(merged_lstm)\n        output = Dense(self.ny, activation=\'tanh\', kernel_initializer=self.k_init,\\\n                       bias_initializer=self.b_init)(gauss_)\n        \n        model = Model(input=[lidar_input,state_input], output=output)\n        adam = Adam(lr=self.lr_actor)\n        model.compile(loss=\'mse\', optimizer=adam)\n        return lidar_input,state_input, model\n\n    def Critic(self):                     \n        # Build Network for Critic       \n        #input=[lidar_input,state_input,action_input]\n        lidar_input = Input(shape=(self.nx_lidar,))\n        lidar_conv = Dense(self.layers[0], activation=\'relu\', kernel_regularizer=self.k_r)(lidar_input)\n        #flat= Flatten()(lidar_conv)\n        \n        state_input = Input(shape=(self.nx_obs,))\n        state_h1 = Dense(self.layers[0], activation=\'relu\', kernel_regularizer=self.k_r)(state_input)\n        #state_h1 = Flatten()(state_h1)\n               \n        merge1 = Concatenate()([lidar_conv,state_h1])\n        #merged_dense = Dense(self.layers[0], activation=\'relu\')(merge1)\n\n        action_input = Input(shape=(self.ny,))\n        #action_h1    = Dense(64, activation=\'relu\')(action_input)\n        \n        merge2 = Concatenate()([merge1,action_input])\n        #merge2reshaped = Reshape((320,1))(merge2)\n        merge_lstm = Dense(self.layers[1], activation=\'relu\')(merge2)\n        output= Dense(1,activation=\'linear\', kernel_initializer=self.k_init,\\\n                      bias_initializer=self.b_init)(merge_lstm)\n        \n        model  = Model(input=[lidar_input,state_input,action_input], output=output)\n        adam  = Adam(lr=self.lr_critic)\n        model.compile(loss=""mse"", optimizer=adam)\n        return lidar_input,state_input, action_input, model\n    \n\n    def _train_critic(self, sample_indx):\n        traj = sample_indx\n        for observation, act, reward, obs_new, done in zip(traj[0],traj[1],traj[2],traj[3],traj[4]):  \n            Q_target = np.array(reward).reshape(1,-1)\n            act = act.reshape(1,-1)\n            state = observation[0][:14].reshape((1,14))\n            lidar = observation[0][14:].reshape((1,10))\n            state_new = obs_new[0][:14].reshape((1,14))\n            lidar_new = obs_new[0][14:].reshape((1,10))\n            if not done:\n                target_action = self.actor_target.predict([lidar_new,state_new])\n                future_reward = self.critic_target.predict([lidar_new,state_new, target_action])[0][0]\n                current_reward = self.critic_target.predict([lidar,state,act])[0][0]\n                Q_target =(1-self.alpha)*Q_target + self.alpha* (self.gamma * future_reward - current_reward)\n                Q_target = Q_target.reshape(1,-1)\n            self.critic_local.fit(x=[lidar,state,act],\\\n                                  y=Q_target, verbose=0, epochs=1)   \n            \n            \n    def _train_actor(self, sample_indx):\n        traj = sample_indx\n        for observation, act, reward, observation_new, _ in zip(traj[0],traj[1],traj[2],traj[3],traj[4]):\n            state = observation[0][:14].reshape((1,14))\n            lidar = observation[0][14:].reshape((1,10))\n            act = act.reshape(1,-1)\n\n            predicted_action = self.actor_local.predict([lidar,state])\n            grads = self.sess.run(self.critic_grads, feed_dict = {\n                    self.critic_lidar_input : lidar,\n                    self.critic_state_input: state,\n                    self.critic_action_input: predicted_action})[0]\n            \n            self.sess.run(self.optimize, feed_dict={\n                    self.actor_lidar_input: lidar,\n                    self.actor_state_input: state,\n                    self.actor_critic_grads: grads})   \n            \n            \n    def _update_actor_target(self):\n        """"""Soft update model parameters.\n        \xce\xb8_target = \xcf\x84*\xce\xb8_local + (1 - \xcf\x84)*\xce\xb8_target""""""\n\n        actor_local_weights  = self.actor_local.get_weights()\n        actor_target_weights =self.actor_target.get_weights()\n        \n        for i in range(len(actor_target_weights)):\n            actor_target_weights[i] = self.tau*actor_local_weights[i] + (1-self.tau)*actor_target_weights[i]\n        self.actor_target.set_weights(actor_target_weights)          \n            \n    def _update_critic_target(self):\n        """"""Soft update model parameters.\n        \xce\xb8_target = \xcf\x84*\xce\xb8_local + (1 - \xcf\x84)*\xce\xb8_target""""""\n        critic_local_weights  = self.critic_local.get_weights()\n        critic_target_weights = self.critic_target.get_weights()\n\t\t\n        for i in range(len(critic_target_weights)):\n            critic_target_weights[i] = self.tau*critic_local_weights[i] + (1-self.tau)*critic_target_weights[i]\n        self.critic_target.set_weights(critic_target_weights)\t\t\n\n    def update_target(self):\n        self._update_actor_target()\n        self._update_critic_target()\n\n    def discount_rewards(self, rewards):\n        discounted_rewards = np.zeros_like(rewards)\n        running_add = 0\n        for t in range(len(rewards)):\n            if rewards[t] != 0:\n                running_add = 0\n            running_add = running_add * self.gamma + rewards[t]\n            discounted_rewards[t] = running_add\n        avg = np.mean(discounted_rewards)\n        var = np.std(discounted_rewards)\n        discounted_rewards = (discounted_rewards - avg)/var\n        return discounted_rewards        \n            \n    def TRAIN(self):     \n        discounted_rewards = self.discount_rewards(self.ep_rewards)\n        sample_indx = [self.ep_obs, self.ep_act, discounted_rewards, self.ep_obs_new, self.ep_flags]                \n        start_train = time.time()\n        # Train Critic\n        self._train_critic(sample_indx) # TRain the network critic\n        # Train Actor\n        self._train_actor(sample_indx)\n        #\n        # Update Weights \n        self.update_target() # Update the netokr local and target weights for actor AND critic\n        end = time.time() - start_train\n        # Empty the lists\n        self.ep_rewards, self.ep_obs, self.ep_act, self.ep_obs_new, self.ep_flags=[], [], [],[], []\n        return end\n\nif __name__ == \'__main__\':\n    \n    #BATCH = 16\n    rendering = input(""Visualize rendering ? [y/n]:  "")\n    \n    s_link = ""BipedalWalker_model.h5""\n    \n    RENDER_REWARD_MIN = 5000\n    RENDER_ENV = False\n    if rendering == \'y\': RENDER_ENV = True  #flag for rendering the environment\n    EPISODES = 9000    # Number of episodes\n    \n    env = gym.make(\'BipedalWalker-v2\')\n    env = env.unwrapped\n    \n    # Observation and Action array length\n    nx = env.observation_space.shape[0] \n    ny = env.action_space.shape[0]\n    lr = 0.001\n    gamma = 0.98\n    sess = tf.Session()\n    K.set_session(sess)    \n    agent = AGENT(nx,ny, s_link, sess)\n    \n    rewards_over_time = []\n    error = []\n    epsilon = []\n    rew_var = []\n    rew_mean = []\n    mean_100 = []\n    seed = np.random.seed(666)\n         \n    print(""-----------------------------------"")        \n    print(""Environment Observation_space: "", env.observation_space)\n    print(""Environment Action_space: "", env.action_space) \n    print(""-----------------------------------\\n"")\n    w = 0\n        \n    # Start running the episodes        \n    for i in range(EPISODES): \n        observation = env.reset()         \n        observation = observation.reshape(1,-1)                \n        start = time.time()\n        \n        while True:            \n            if RENDER_ENV:\n                env.render()\n            \n            action = agent.choose_action(observation)\n            action = action.reshape((4,))\n            observation_new, reward, flag, inf = env.step(action)\n            observation_new = observation_new.reshape(1,-1)                    \n            # Store new information\n            agent.storing(observation, action, reward, observation_new, flag)   \n            observation = observation_new         \n            # Measure the time\n            end = time.time()\n            time_space = end - start\n            \n            if time_space > 20:\n                flag = True\n          \n            # Sum the episode rewards\n            ep_rew_total = sum(agent.ep_rewards)\n            mean = np.mean(agent.ep_rewards)\n            var = np.var(agent.ep_rewards)\n            if ep_rew_total < -300:\n                flag = True\n            \n            if flag==True:\n                rewards_over_time.append(ep_rew_total)\n                rew_mean.append(mean)\n                rew_var.append(var)\n                max_reward = np.max(rewards_over_time)\n                episode_max = np.argmax(rewards_over_time)\n                if ep_rew_total >=300 :\n                    w = w + 1\n                    agent.save(s_link)\n                                        \n                print(""++++++++++++++++++++++++++++++++++++++++++++++++++++++++"")\n                print(""Episode: "", i)\n                print(""Time: "", np.round(time_space, 2),""secs"")\n                print(""Reward:"", ep_rew_total)\n                print(""Maximum Reward: "" + str(max_reward) + ""  on Episode: "" + str(episode_max))\n                print(""Times win: "" + str(w))\n                \n                if i % 100 ==0:\n                    print(""Mean reward of the past 100 episodes: "", str(np.mean(rewards_over_time[-100:])))\n                    mean_100.append(np.mean(rewards_over_time[-100:]))\n                    f = open(\'results.txt\',\'a\')\n                    f.write(\'\\n\' + str(np.mean(rewards_over_time[-100:])))\n                    f.close()\n                \n                # Start training the Neural Network\n                mm= agent.TRAIN()\n                \n                epsilon.append(agent.e)\n                                           \n                error.append(mm)\n                \n                if max_reward > RENDER_REWARD_MIN: RENDER_ENV = True\n                \n                break\n            \n    plt.figure(1)\n    plt.plot(error)\n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Average Error"")\n    plt.title(""Average_Loss Vs Episodes"")\n    plt.show()\n    \n    plt.figure(1)\n    plt.plot(epsilon)\n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Epsilon value"")\n    plt.title(""Epsilon Vs Episodes"")\n    plt.show()\n    \n    np.save(""rewards_over_time"", rewards_over_time)\n    np.save(""mean100"", mean_100)            \n            \n            \n    plt.figure(1)            \n    plt.plot(rewards_over_time, label=""Rewards"")\n    plt.plot(rew_mean, label=""Mean"")\n    plt.plot(rew_var, label=""Variance"")    \n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Rewards"")\n    plt.title(""Rewards per Episode"")\n    plt.legend(loc=0)\n    plt.show()        \n            \n            \n            \n            \n            \n            \n           \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n\n\n'"
PPO/PPO_e.py,24,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Apr 25 20:41:53 2018\n\n@author: dead\nACTOR-CRITIC Network for Robot Locomotion\n\n""""""\n\n#from __future__ import print_function\nimport scipy.signal\nimport numba as nb\nfrom keras.models import load_model, Sequential, Model\nfrom keras.initializers import RandomUniform, RandomNormal\nfrom keras.regularizers import l2\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.activations import softplus\nfrom keras import backend as K\nfrom keras.layers import Dense, Dropout, Input, MaxPooling1D,Conv1D,Conv2D,LSTM, Flatten, Reshape, GaussianNoise\nfrom keras.layers.merge import Add, Multiply, Concatenate\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nimport matplotlib.pyplot as plt\nimport time\nimport numpy as np\nimport gym, queue\nimport gym.wrappers\nimport random\nfrom collections import deque\nimport tensorflow as tf\nimport os\n#os.chdir(""/home/dead/Documents/Master_Research/ddpg/"")\nfrom rl.random import OrnsteinUhlenbeckProcess\nfrom tqdm import tqdm\n\nLAMBDA = 0.95\nNOISE = 1.0\nEPSILON = 0.15\nBATCH = 8192\nEPOCHS = 15\nENTROPY= 5 * 1e-3\nSIGMA_FLOOR = 0\nDUMMY_ACTION, DUMMY_VALUE = np.zeros((1, 4)), np.zeros((1, 1))\n\n# Proximal Policy Optimization loss function\ndef PPO_Loss(advantage,old_pred):\n    def loss(y_true, y_pred):\n        var = K.square(NOISE)\n        pi = 3.14159\n        d = K.sqrt(2*pi*var)\n        prob_num = K.exp(- K.square(y_true - y_pred) / (2 * var))\n        old_prob_num = K.exp(- K.square(y_true - old_pred) / (2 * var))\n\n        prob = prob_num/d\n        old_prob = old_prob_num/d\n        r = prob/(old_prob + 1e-10)\n        return -K.mean(K.minimum(r * advantage, K.clip(r,1-EPSILON,1+EPSILON)*advantage)) +\\\n                                                                ENTROPY* (prob*K.log(prob + 1e-8))\n    return loss\n\ndef Actor_loss(advantage,old_pred):\n    def loss(y_true,y_pred):\n        var = K.square(NOISE)\n        var_pred = K.var(y_pred)\n        var_old = K.var(old_pred)\n        var_t = K.var(y_true)\n        pi = 3.141592\n        d = K.sqrt(2*pi*var)\n        dist_pred = K.random_normal(shape=(4,),mean=y_pred, stddev=K.maximum(K.exp(var_pred), var))\n        dist_old = K.random_normal(shape=(4,),mean=old_pred, stddev=K.maximum(tf.exp(var_old), var))\n        ratio = K.maximum(dist_pred,1e-6)/K.maximum(dist_old,1e-6)\n        ratio = K.clip(ratio,0,10)\n        surrogate1 = advantage*ratio\n        surrogate2 = advantage*K.clip(ratio,1-EPSILON,1+EPSILON)\n        error = -K.mean(K.minimum(surrogate1,surrogate2))\n        return error\n    return loss\n\ndef Critic_loss(old):\n    def loss(y_true,y_pred):\n        clipped = old + K.clip(y_pred - old, -EPSILON, EPSILON)\n        loss_q1 = K.square(y_true-clipped)\n        loss_q2 = K.square(y_true - y_pred)\n        error = K.mean(K.maximum(loss_q1,loss_q2))*0.5\n        return error\n    return loss\n\nclass AGENT:\n    def __init__(self, nx, ny, s_link):\n        #self.env = environment\n        self.nx = nx[0]  #  Observation array length   nx = env.observation_space.shape[0]\n        self.ny = ny[0]  #   Action space length       ny = env.action_space.n\n        self.lr_actor = 0.0001 # Actor_Learning rate\n        self.lr_critic = 0.0002 # Critic_Learning rate\n        self.gamma = 0.99\n        self.alpha = 0.1\n        self.s_link =s_link \n        #self.sess = sess\n        self.deck = deque(maxlen=4000)\n        self.val = False\n        \n        self.e = 1.0\n        self.e_= 0.01\n        self.dc= 0.999\n        self.tau = 0.001\n        self.weight_decay =0.001\n        self.los = []\n        self.layers=[256,128]\n        \n        # Network init_functions\n        self.k_r = l2(self.weight_decay)\n        self.initializer = ""glorot_uniform""\n        self.final_initializer = RandomUniform(minval = -0.003, maxval = 0.003)\n        self.parameters={\'lr_actor\': self.lr_actor,\'lr_critic\':self.lr_critic,\'gamma\':self.gamma,\n                         \'alpha\':self.alpha, \'tau\':self.tau,\'dc\':self.dc}\n        \n        if os.path.isfile(\'./\' + self.s_link):\n            self.message = [""\\nLOAD existing keras model...."", self.model.summary()]\n            self.model = load_model(self.s_link)\n        else:\n            self.message = \'\\nBuilding New Model >_\'\n            ### Build Networks Actor-Critic ###\n            self.actor = self.Actor()\n            self.critic = self.Critic()     \n            self.sample_critic = self.sample_Critic()\n            \n        # Empty the lists\n        self.avg_actor=[]\n        self.avg_critic=[]\n        self.epsilon = []\n        self.reward = []\n        self.ep_rewards = []\n        self.ep_obs = []\n        self.ep_act = []\n        self.ep_obs_new = []\n        self.ep_flags=[] \n        \n    @nb.jit    \n    def choose_action(self,state,episode):     \n        if np.random.rand() <= self.e : \n            # epsilon Greedy             \n            if self.e >= self.e_:\n                self.e *= self.dc\n                self.epsilon.append(self.e)\n            values = []\n            action_=np.random.uniform(-1,1,(50,4))                                \n            for i in range(50):\n                values.append(self.sample_critic.predict([state.reshape(1,-1),action_[i].reshape(1,-1)])[0][0])\n            best = np.argmax(values)    \n            action = action_[best]\n            value = values[best]\n            action_matrix = pred_action = action\n            return action, action_matrix, pred_action, value       \n        \n        if episode % 100 !=0 :\n            action = self.actor.predict([state.reshape(1,-1), DUMMY_VALUE, DUMMY_ACTION])[0] + np.random.normal(0,NOISE,4)\n            action_matrix = pred_action = action\n        else:\n            action = self.actor.predict([state.reshape(1,-1),DUMMY_VALUE, DUMMY_ACTION])[0]\n            action_matrix = pred_action = action    \n        value  = self.critic.predict([state.reshape(1,-1), DUMMY_VALUE])[0]   \n        return action, action_matrix, pred_action, value\n    \n    #@nb.jit(nopython=True)                    \n    def storing(self, observation, action, reward, observation_new, flags ):\n        # Storing for replay-expirience\n        self.deck.append((observation, action, reward, observation_new, flags ))\n        self.ep_rewards.append(reward)\n        self.ep_obs.append(observation)\n        self.ep_act.append(action)\n        self.ep_obs_new.append(observation_new)\n        self.ep_flags.append(flags)\n    \n    #@nb.jit(nopython=True)   \n    def save(self):\n        self.actor.save(\'actor\'+self.s_link)\n        self.critic.save(\'critic\'+self.s_link)\n        \n    def sample_Critic(self):                     \n        state_input = Input(shape=(self.nx,))\n        action_input = Input(shape=(self.ny,))\n        #\n        h0 = Dense(self.layers[1], activation=\'relu\',kernel_regularizer=self.k_r)(state_input)\n        h1 = Dense(self.layers[1], activation=\'tanh\',kernel_regularizer=self.k_r)(action_input)\n        #\n        conc = Concatenate()([h0,h1])\n        #conc = Flatten()(conc)\n        h2 = Dense(64, activation=\'relu\', kernel_regularizer=self.k_r)(conc)\n        out = Dense(1, activation=\'linear\', kernel_regularizer=self.k_r,\\\n                    kernel_initializer=self.final_initializer)(h2)\n        #       \n        model  = Model(inputs=[state_input, action_input], outputs=out)\n        adam  = Adam(lr=self.lr_critic)\n        model.compile(loss=""mse"", optimizer=adam)\n        return model   \n    \n    def Actor(self):                     \n        state_input = Input(shape=(self.nx,), name=\'obse\')\n        advantage = Input(shape=(1,), name=\'avanta\')\n        old_pred = Input(shape=(self.ny,), name=\'old-pred\')\n        #\n        h0 = Dense(self.layers[0], activation=\'relu\', kernel_regularizer=self.k_r)(state_input)\n        h1 = Dense(self.layers[1], activation=\'relu\',kernel_regularizer=self.k_r)(h0)\n        #\n        out = Dense(self.ny, activation=\'tanh\',kernel_regularizer=self.k_r,\\\n                    kernel_initializer=self.final_initializer)(h1)\n        #\n        model = Model(inputs=[state_input, advantage, old_pred], outputs=out)\n        adam = Adam(lr=self.lr_actor)\n        model.compile(loss=[Actor_loss(advantage=advantage, old_pred=old_pred)], optimizer=adam)\n        return model\n\n    def Critic(self):                     \n        state_input = Input(shape=(self.nx,))\n        old_value = Input(shape=(1,))\n        #\n        h0 = Dense(self.layers[0], activation=\'relu\',kernel_regularizer=self.k_r)(state_input)\n        h1 = Dense(self.layers[1], activation=\'relu\',kernel_regularizer=self.k_r)(h0)\n        #\n        out = Dense(1, activation=\'linear\', kernel_regularizer=self.k_r,\\\n                    kernel_initializer=self.final_initializer)(h1)\n        #       \n        model  = Model(inputs=[state_input, old_value], outputs=out)\n        adam  = Adam(lr=self.lr_critic)\n        model.compile(loss=[Critic_loss(old=old_value)], optimizer=adam)\n        return model      \n    \n    #@nb.jit\n    def process_rewards(self,x):\n        mm = np.mean(x)\n        var = np.std(x)\n        norm = np.clip((x-mm)/var, -10,10)\n        return norm\n    \n    #@nb.jit    \n    def discount(self,x, gamma, terminal_array=None):\n        if terminal_array is None:\n            return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n        else:\n            y, adv = 0, []\n            terminals_reversed = terminal_array[1:][::-1]\n            for step, dt in enumerate(reversed(x)):\n                y = dt + gamma * y * (1 - terminals_reversed[step])\n                adv.append(y)            \n        return np.array(adv)[::-1]    \n    \n    #@nb.jit\n    def TRAIN(self,obs, action, pred, rewards, advantage, old_pred, pred_values):     \n        actor_loss = []\n        critic_loss = []\n        epoch=[]\n        # TRain in EPOCHS\n        start_train = time.time()\n        for i in range(EPOCHS):\n            epoch_start = time.time()\n            actor_loss.append(self.actor.train_on_batch([obs, advantage, old_pred], [action]))\n            critic_loss.append(self.critic.train_on_batch([obs, pred_values], [rewards]) )\n            self.sample_critic.train_on_batch([obs,action], [rewards])\n            epoch.append(time.time()-epoch_start)\n        end = time.time() - start_train\n        \n        self.avg_actor.append(np.mean(actor_loss))\n        self.avg_critic.append(np.mean(critic_loss))\n        message = \'Training Time: %.3fs | Avg: %.3fs/epoch\' % (end, np.mean(epoch))\n        #print(\'Training Time: %.3fs | Avg: %.3fs/epoch\' % (end, np.mean(epoch)) )\n        return end, self.avg_actor, self.avg_critic, message\n                        \nif __name__ == \'__main__\':\n                \n    rendering = input(""Visualize rendering ? [y/n]:  "")  \n    s_link = ""BipedalWalker_model.h5""  \n    RENDER_REWARD_MIN = 5000\n    RENDER_ENV = False\n    if rendering == \'y\': RENDER_ENV = True  #flag for rendering the environment\n    EPISODES = 10000    # Number of episodes\n    env = gym.make(\'BipedalWalker-v2\')  #video_callable=lambda episode_id: episode_id%10==0\n    #env = gym.wrappers.Monitor(env, directory+\'lala3\',  force=True)\n\n    # Environment Parameters\n    nx = env.observation_space.shape # network-input  \n    ny = env.action_space.shape # network-out\n    agent = AGENT(nx,ny, s_link)\n    seed = np.random.seed(1)\n         \n    print(\'\\n+++++++++++++++++++++++++++++++++\')\n    print(\'BipedalWalker-v2 Starts...  >_\')\n    print(\'+++++++++++++++++++++++++++++++++\') \n    print(agent.message)    \n    print(""Environment Observation_space: "", env.observation_space)\n    print(""Environment Action_space: "", env.action_space) \n    print(""Number of Episodes: "" + str(EPISODES))\n    print(\'-----------------------------------\')\n    print(""\\n:::::Algorithm_Parameters::::::"")\n    print(list(agent.parameters.items()))\n    \n    # Initialize Simulation\n    batch = [[], [], [], [],[], []]  \n    counter = 0\n    mean100 =[]\n    # Start running the SIMULATION       \n    for i in range(EPISODES):         \n        observation = env.reset()         \n        observation = observation.reshape(1,-1)                \n        start = time.time()\n        ep_r, ep_t = 0, 0\n        #noise=OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=.3,size=4)\n        while True:\n            if RENDER_ENV: env.render()\n            act, action_matrix, predicted_action, q_value = agent.choose_action(observation,i)\n            act.reshape((4,))\n            \n            if counter >= BATCH:\n                # UPDATE NETWORKS TRAIN IN BATCHES\n                print(\'mpika!!!!\')\n                r = agent.process_rewards(batch[3]) # Norm-Clip rewards\n                r = r.reshape(-1,1)\n                obs, action, pred = np.vstack(batch[0]), np.array(batch[1]), np.array(batch[2])\n                old_pred = pred\n                v_final = [q_value * (1-flag)]\n                pred_values =np.array(batch[5] + v_final )\n                pred_values = pred_values.reshape(-1,1)# Get Q-values\n                terminals = np.array(batch[4] + [flag])\n                terminals = terminals.reshape(-1,1)\n                \n                deltas = r + agent.gamma * pred_values[1:]*(1-terminals[1:]) - pred_values[:-1]\n                advantage = agent.discount(deltas, agent.gamma * LAMBDA, terminals)\n                returns = advantage + np.array(batch[5]).reshape(-1,1)\n                advantage = (advantage - advantage.mean()) / np.maximum(advantage.std(), 1e-6)\n\n                # Train on batches #\n                train_time, avg_actor_loss, avg_critic_loss, message = agent.TRAIN(obs, action, pred, returns, advantage, old_pred, np.array(batch[5])) \n                print(message)\n                batch = [[], [], [], [],[], []]\n                counter=0        \n\n            batch[0].append(observation)\n            batch[1].append(action_matrix)\n            batch[2].append(predicted_action)\n            # ENVIRONMENT STEP\n            observation_new, reward, flag, info = env.step(np.clip(act,-1,1))   \n            observation_new.reshape(1,-1)\n            batch[3].append(reward)\n            batch[4].append(flag)\n            batch[5].append(q_value)\n\n            ep_r += reward\n            ep_t += 1\n            counter +=1\n            observation = observation_new\n            \n            if flag:\n                # Break episode\n                agent.ep_rewards.append(ep_r)\n                print(\'Episode: %i\' % i, \'| Reward: %.2f\' % ep_r , \' | Trajectories: %i\' % ep_t)\n                \n                if i %100 ==0:\n                    agent.save()\n                    mean100.append(np.mean(agent.ep_rewards[-100:]))\n                    print(\'Average Reward last 100-Episodes: %.3f\' % mean100[-1])\n                break\n    env.close()\n    \n    # Export        \n    np.save(""rewards_over_time"", agent.ep_rewards)\n    np.save(""mean100"", mean100)      \n                \n    plt.figure(figsize=(10,8))\n    plt.plot(agent.avg_actor, label=\'Avg.ActorLoss\')\n    plt.plot(agent.avg_critic, label=\'Avg.CriticLoss\')\n    plt.xlabel(""Training Time-Stamp"")\n    plt.ylabel(""Average Error"")\n    plt.title(""Epsilon Vs Episodes"")\n    plt.savefig(""Plots/Epsilon.png"")  \n                \n    plt.figure(figsize=(10,8))\n    plt.plot(agent.epsilon)\n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Epsilon value"")\n    plt.title(""Epsilon Vs Episodes"")\n    plt.savefig(""Plots/Epsilon.png"") \n\n    plt.figure(figsize=(10,8))            \n    plt.plot(agent.ep_rewards) \n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Rewards"")\n    plt.title(""Rewards per Episode"")\n    plt.legend(loc=0)\n    plt.savefig(""Plots/Rewards.png"")         \n    \n    plt.figure(figsize=(10,8))\n    plt.plot(mean100)\n    plt.xlabel(""100_episodes"")\n    plt.ylabel  (""Mean_value"")\n    plt.title(\'Average Reward per 100 episodes\')\n    plt.savefig(""Plots/mean_100.png"")       \n            \n            \n            \n           \n            \n            \n            \n            \n\n            \n            \n            \n            \n            \n            \n            \n\n\n'"
PPO/PPO_simple.py,17,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Apr 25 20:41:53 2018\n\n@author: dead\nACTOR-CRITIC Network for Robot Locomotion\n\n""""""\n\n#from __future__ import print_function\nfrom keras.models import load_model, Sequential, Model\nfrom keras.initializers import RandomUniform, RandomNormal\nfrom keras.regularizers import l2\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.activations import softplus\nfrom keras import backend as K\nfrom keras.layers import Dense, Dropout, Input, MaxPooling1D,Conv1D,Conv2D,LSTM, Flatten, Reshape, GaussianNoise\nfrom keras.layers.merge import Add, Multiply, Concatenate\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nimport matplotlib.pyplot as plt\nimport time\nimport numpy as np\nimport gym, queue\nimport gym.wrappers\nimport random\nfrom collections import deque\nimport tensorflow as tf\nimport os\n#os.chdir(""/home/dead/Documents/Master_Research/ddpg/"")\nfrom rl.random import OrnsteinUhlenbeckProcess\nfrom tqdm import tqdm\n\nNOISE = 1.0\nEPSILON = 0.15\nBATCH = 8192\nEPOCHS = 10\nENTROPY= 5 * 1e-3\nDUMMY_ACTION, DUMMY_VALUE = np.zeros((1, 4)), np.zeros((1, 1))\n\n# Proximal Policy Optimization loss function\ndef PPO_Loss(advantage,old_pred):\n    def loss(y_true, y_pred):\n        var = K.square(NOISE)\n        pi = 3.14159\n        d = K.sqrt(2*pi*var)\n        prob_num = K.exp(- K.square(y_true - y_pred) / (2 * var))\n        old_prob_num = K.exp(- K.square(y_true - old_pred) / (2 * var))\n\n        prob = prob_num/d\n        old_prob = old_prob_num/d\n        r = prob/(old_prob + 1e-10)\n        return -K.mean(K.minimum(r * advantage, K.clip(r,min_value=1-EPSILON, max_value=1+EPSILON)*advantage)) +\\\n                                                                ENTROPY* (prob*K.log(prob + 1e-8))\n    return loss\n\nclass AGENT:\n    def __init__(self, nx, ny, s_link):\n        #self.env = environment\n        self.nx = nx[0]  #  Observation array length   nx = env.observation_space.shape[0]\n        self.ny = ny[0]  #   Action space length       ny = env.action_space.n\n        self.lr_actor = 0.0001 # Actor_Learning rate\n        self.lr_critic = 0.0002 # Critic_Learning rate\n        self.gamma = 0.99\n        self.alpha = 0.1\n        self.s_link =s_link \n        #self.sess = sess\n        self.deck = deque(maxlen=4000)\n        self.val = False\n        \n        self.e = 1.0\n        self.e_= 0.01\n        self.dc= 0.999\n        self.tau = 0.001\n        self.weight_decay =0.001\n        self.los = []\n        self.layers=[256,128]\n        \n        # Network init_functions\n        self.k_r = l2(self.weight_decay)\n        self.initializer = ""glorot_uniform""\n        self.final_initializer = RandomUniform(minval = -0.003, maxval = 0.003)\n        self.parameters={\'lr_actor\': self.lr_actor,\'lr_critic\':self.lr_critic,\'gamma\':self.gamma,\n                         \'alpha\':self.alpha, \'tau\':self.tau,\'dc\':self.dc}\n        \n        if os.path.isfile(\'./\' + self.s_link):\n            self.message = [""\\nLOAD existing keras model...."", self.model.summary()]\n            self.model = load_model(self.s_link)\n        else:\n            self.message = \'\\nBuilding New Model >_\'\n            ### Build Networks Actor-Critic ###\n            self.actor = self.Actor()\n            self.critic = self.Critic()     \n            self.sample_critic = self.sample_Critic()\n            \n        # Empty the lists\n        self.avg_actor=[]\n        self.avg_critic=[]\n        self.epsilon = []\n        self.reward = []\n        self.ep_rewards = []\n        self.ep_obs = []\n        self.ep_act = []\n        self.ep_obs_new = []\n        self.ep_flags=[] \n        \n    def choose_action(self,state,episode):\n        # Use epsilon-greedy algorithm\n        if np.random.rand() <= self.e : \n            # Predict a random action             \n            if self.e >= self.e_:\n                # Discound epsilon\n                self.e *= self.dc\n                self.epsilon.append(self.e)\n                \n            values = []\n            action_=np.random.uniform(-1,1,(100,4))                                \n            for i in range(100):\n                # Selcet the best action among range\n                values.append(self.sample_critic.predict([state.reshape(1,-1), action_])[0][0])\n            action = action_[np.argmax(values)]\n            action_matrix = pred_action = action\n            return action , action_matrix, pred_action      \n        \n        # Else: use policy action\n        if episode % 100 !=0 :\n            action = self.actor.predict([state.reshape(1,-1), DUMMY_VALUE, DUMMY_ACTION])[0] + np.random.normal(0,NOISE,4)\n            action_matrix = pred_action = action\n        else:\n            action = self.actor.predict([state.reshape(1,-1),DUMMY_VALUE, DUMMY_ACTION])[0]\n            action_matrix = pred_action = action          \n        return action, action_matrix, pred_action\n                        \n    def storing(self, observation, action, reward, observation_new, flags ):\n        # Storing for replay-expirience\n        self.deck.append((observation, action, reward, observation_new, flags ))\n        self.qqq.put(np)\n        self.ep_rewards.append(reward)\n        self.ep_obs.append(observation)\n        self.ep_act.append(action)\n        self.ep_obs_new.append(observation_new)\n        self.ep_flags.append(flags)\n        \n    def save(self):\n        self.actor.save(\'actor\'+self.s_link)\n        self.critic.save(\'critic\'+self.s_link)\n        \n    def sample_Critic(self):                     \n        state_input = Input(shape=(self.nx,))\n        action_input = Input(shape=(self.ny,))\n        #\n        h0 = Dense(self.layers[1], activation=\'relu\',kernel_regularizer=self.k_r)(state_input)\n        h1 = Dense(self.layers[1], activation=\'tanh\',kernel_regularizer=self.k_r)(action_input)\n        #\n        conc = Concatenate()([h0,h1])\n        #conc = Flatten()(conc)\n        h2 = Dense(64, activation=\'relu\', kernel_regularizer=self.k_r)(conc)\n        out = Dense(1, activation=\'linear\', kernel_regularizer=self.k_r,\\\n                    kernel_initializer=self.final_initializer)(h2)\n        #       \n        model  = Model(input=[state_input, action_input], output=out)\n        adam  = Adam(lr=self.lr_critic)\n        model.compile(loss=""mse"", optimizer=adam)\n        return model   \n    \n    def Actor(self):                     \n        state_input = Input(shape=(self.nx,), name=\'obse\')\n        advantage = Input(shape=(1,), name=\'avanta\')\n        old_pred = Input(shape=(self.ny,), name=\'old-pred\')\n        #\n        h0 = Dense(self.layers[0], activation=\'relu\', kernel_regularizer=self.k_r)(state_input)\n        h1 = Dense(self.layers[1], activation=\'relu\',kernel_regularizer=self.k_r)(h0)\n        #\n        out = Dense(self.ny, activation=\'tanh\',kernel_regularizer=self.k_r,\\\n                    kernel_initializer=self.final_initializer)(h1)\n        #\n        model = Model(input=[state_input, advantage, old_pred], output=out)\n        adam = Adam(lr=self.lr_actor)\n        model.compile(loss=[PPO_Loss(advantage, old_pred)], optimizer=adam)\n        return model\n\n    def Critic(self):                     \n        state_input = Input(shape=(self.nx,))\n        #\n        h0 = Dense(self.layers[0], activation=\'relu\',kernel_regularizer=self.k_r)(state_input)\n        h1 = Dense(self.layers[1], activation=\'relu\',kernel_regularizer=self.k_r)(h0)\n        #\n        out = Dense(1, activation=\'linear\', kernel_regularizer=self.k_r,\\\n                    kernel_initializer=self.final_initializer)(h1)\n        #       \n        model  = Model(input=[state_input], output=out)\n        adam  = Adam(lr=self.lr_critic)\n        model.compile(loss=""mse"", optimizer=adam)\n        return model         \n            \n    def discount_rewards(self, rewards):        \n        for j in range(len(rewards) - 2, -1, -1):\n            rewards[j] += rewards[j + 1] * self.gamma\n        avg = np.mean(rewards)\n        var = np.std(rewards)\n        discounted_rewards = (rewards - avg)/var\n        return discounted_rewards        \n                \n    def TRAIN(self,obs, action, pred, rewards, advantage, old_pred):     \n        actor_loss = []\n        critic_loss = []\n        epoch=[]\n        # TRain in EPOCHS\n        start_train = time.time()\n        for i in range(EPOCHS):\n            epoch_start = time.time()\n            actor_loss.append(self.actor.train_on_batch([obs, advantage, old_pred], [action]))\n            critic_loss.append(self.critic.train_on_batch([obs], [rewards]) )\n            self.sample_critic.train_on_batch([obs,action], [rewards])\n            epoch.append(time.time()-epoch_start)\n        end = time.time() - start_train\n        \n        self.avg_actor.append(np.mean(actor_loss))\n        self.avg_critic.append(np.mean(critic_loss))\n        print(\'Training Time: %.3fs | Avg: %.3fs/epoch\' % (end, np.mean(epoch)) )\n        return end, self.avg_actor, self.avg_critic\n                        \n \nif __name__ == \'__main__\':\n                \n    rendering = input(""Visualize rendering ? [y/n]:  "")  \n    s_link = ""BipedalWalker_model.h5""  \n    RENDER_REWARD_MIN = 5000\n    RENDER_ENV = False\n    if rendering == \'y\': RENDER_ENV = True  #flag for rendering the environment\n    EPISODES = 100000    # Number of episodes\n    env = gym.make(\'BipedalWalker-v2\')  #video_callable=lambda episode_id: episode_id%10==0\n    #env = gym.wrappers.Monitor(env, directory+\'lala3\',  force=True)\n\n    # Environment Parameters\n    nx = env.observation_space.shape # network-input  \n    ny = env.action_space.shape # network-out\n    #sess = tf.Session()\n    #K.set_session(sess)\n    agent = AGENT(nx,ny, s_link)\n    seed = np.random.seed(1)\n         \n    print(\'\\n+++++++++++++++++++++++++++++++++\')\n    print(\'BipedalWalker-v2 Starts...  >_\')\n    print(\'+++++++++++++++++++++++++++++++++\') \n    print(agent.message)    \n    print(""Environment Observation_space: "", env.observation_space)\n    print(""Environment Action_space: "", env.action_space) \n    print(""Number of Episodes: "" + str(EPISODES))\n    print(\'-----------------------------------\')\n    \n    print(""\\n:::::Algorithm_Parameters::::::"")\n    print(list(agent.parameters.items()))\n    # Initialize Simulation\n    batch = [[], [], [], []]  \n    counter = 0\n    mean100 =[]\n    # Start running the SIMULATION       \n    for i in range(EPISODES):         \n        observation = env.reset()         \n        observation = observation.reshape(1,-1)                \n        start = time.time()\n        ep_r = 0\n        #noise=OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=.3,size=4)\n        while True:\n            if RENDER_ENV: env.render()\n            act, action_matrix, predicted_action = agent.choose_action(observation,i)\n            act.reshape((4,))\n            \n            if counter >= BATCH:\n                # UPDATE NETWORKS TRAIN IN BATCHES\n                r = agent.discount_rewards(batch[3]) # Discound and Normalize rewards   \n                r = r.reshape(-1,1)\n                obs, action, pred = np.vstack(batch[0]), np.array(batch[1]), np.array(batch[2])\n                #pred = np.reshape(pred, (pred.shape[0], pred.shape[2]))\n                old_pred = pred\n                pred_values = agent.critic.predict(obs)\n                advantage = r - pred_values\n                # Train on batches #\n                train_time, avg_actor_loss, avg_critic_loss = agent.TRAIN(obs, action, pred, r, advantage, old_pred) \n                batch=[[],[],[],[]]\n                counter=0                \n\n            batch[0].append(observation)\n            batch[1].append(action_matrix)\n            batch[2].append(predicted_action)\n            # ENVIRONMENT STEP\n            observation_new, reward, flag, info = env.step(np.clip(act,-1,1))   \n            observation_new.reshape(1,-1)\n            batch[3].append(reward)\n\n            ep_r += reward\n            counter +=1\n            observation = observation_new\n            \n            if flag:\n                # Break episode\n                agent.ep_rewards.append(ep_r)\n                print(\'Episode: %i\' % i, \'| Reward: %.2f\' % ep_r , \' | Trajectories: %i\' % counter)\n                \n                if i %100 ==0:\n                    agent.save()\n                    mean100.append(np.mean(agent.ep_rewards[-100:]))\n                    print(\'Average Reward last 100-Episodes: %.3f\' % mean100[-1])\n                break\n    env.close()\n    \n    # Export        \n    np.save(""rewards_over_time"", agent.ep_rewards)\n    np.save(""mean100"", mean100)      \n                \n    plt.figure(figsize=(10,8))\n    plt.plot(agent.avg_actor, label=\'Avg.ActorLoss\')\n    plt.plot(agent.avg_critic, label=\'Avg.CriticLoss\')\n    plt.xlabel(""Training Time-Stamp"")\n    plt.ylabel(""Average Error"")\n    plt.title(""Epsilon Vs Episodes"")\n    plt.savefig(""Plots/Epsilon.png"")  \n                \n    plt.figure(figsize=(10,8))\n    plt.plot(agent.epsilon)\n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Epsilon value"")\n    plt.title(""Epsilon Vs Episodes"")\n    plt.savefig(""Plots/Epsilon.png"") \n\n    plt.figure(figsize=(10,8))            \n    plt.plot(agent.ep_rewards) \n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Rewards"")\n    plt.title(""Rewards per Episode"")\n    plt.legend(loc=0)\n    plt.savefig(""Plots/Rewards.png"")         \n    \n    plt.figure(figsize=(10,8))\n    plt.plot(mean100)\n    plt.xlabel(""100_episodes"")\n    plt.ylabel  (""Mean_value"")\n    plt.title(\'Average Reward per 100 episodes\')\n    plt.savefig(""Plots/mean_100.png"")       \n            \n            \n            \n           \n            \n            \n            \n            \n\n            \n            \n            \n            \n            \n            \n            \n\n\n'"
PPO/PPO_simple2.py,20,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Apr 25 20:41:53 2018\n\n@author: dead\nACTOR-CRITIC Network for Robot Locomotion\n\n""""""\n\n#from __future__ import print_function\nimport tensorflow as tf\nimport scipy.signal\nimport math\nimport numba as nb\nfrom tensorflow import keras\nfrom keras.models import load_model, Sequential, Model\nfrom keras.initializers import RandomUniform, RandomNormal\nfrom keras.regularizers import l2\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.activations import softplus\nfrom keras import backend as K\nfrom keras.layers import Dense, Dropout, Input, MaxPooling1D,Conv1D,Conv2D,LSTM, Flatten, Reshape, GaussianNoise, Lambda\nfrom keras.layers.merge import Concatenate\nfrom keras.optimizers import Adam\nfrom keras.callbacks import LearningRateScheduler\nfrom helpers import ProcessRewards\nimport matplotlib.pyplot as plt\nimport time\nimport numpy as np\nimport gym, queue\nimport gym.wrappers\nfrom collections import deque\nimport os\n#os.chdir(""/home/dead/Documents/Master_Research/ddpg/"")\n\nLAMBDA = 0.95\nNOISE = 1.0\nEPSILON = 0.2\nBATCH = 8192\nEPOCHS = 15\nENTROPY= 5 * 1e-3\nSIGMA_FLOOR = 0\nMINIBATCH = 32\nLR = 0.0001\nDUMMY_ACTION, DUMMY_VALUE = np.zeros((1, 4)), np.zeros((1, 1))\n\n# Proximal Policy Optimization loss function\ndef PPO_Loss(advantage,old_pred):\n    def loss(y_true, y_pred):\n        var = K.square(NOISE)\n        pi = 3.14159\n        d = K.sqrt(2*pi*var)\n        prob_num = K.exp(- K.square(y_true - y_pred) / (2 * var))\n        old_prob_num = K.exp(- K.square(y_true - old_pred) / (2 * var))\n\n        prob = prob_num/d\n        old_prob = old_prob_num/d\n        r = prob/(old_prob + 1e-10)\n        return -K.mean(K.minimum(r * advantage, K.clip(r,1-EPSILON,1+EPSILON)*advantage))\n    return loss\n\ndef Actor_loss(advantage,old_pred):\n    def loss(y_true,y_pred):\n        var = K.square(NOISE)\n        pi = 3.141592\n        d = K.sqrt(2*pi*var)\n        dist = y_pred\n        dist_ = old_pred\n        ratio = K.maximum(dist,1e-6)/K.maximum(dist_,1e-6)\n        ratio = K.clip(ratio,0,10)\n        surrogate1 = advantage*ratio\n        surrogate2 = advantage*K.clip(ratio,1-EPSILON,1+EPSILON)\n        error = -K.mean(K.minimum(surrogate1,surrogate2))\n        return error\n    return loss\n\ndef Critic_loss(old):\n    def loss(y_true,y_pred):\n        clipped = old + K.clip(y_pred - old, -EPSILON, EPSILON)\n        loss_q1 = K.square(y_true-clipped)\n        loss_q2 = K.square(y_true - y_pred)\n        error = K.mean(K.maximum(loss_q1,loss_q2))*0.5\n        return error\n    return loss\n\nclass AGENT:\n    def __init__(self, nx, ny, s_link):\n        #self.env = environment\n        self.s_state = nx\n        self.nx = nx[0]  #  Observation array length   nx = env.observation_space.shape[0]\n        self.ny = ny[0]  #   Action space length       ny = env.action_space.n\n        self.gamma = 0.99\n        self.alpha = 0.1\n        self.s_link =s_link \n        self.lr=LR\n        #self.sess = sess\n        self.deck = deque(maxlen=4000)\n        self.val = False\n        \n        self.e = 1.0\n        self.e_= 0.01\n        self.dc= 0.999\n        self.tau = 0.001\n        self.weight_decay =0.001\n        self.los = []\n        self.layers=[256,128] \n        \n        # Network init_functions\n        self.k_r = l2(self.weight_decay)\n        self.initializer = ""glorot_uniform""\n        self.final_initializer = RandomUniform(minval = -0.003, maxval = 0.003)\n        self.parameters={\'lr\':self.lr,\'gamma\':self.gamma,\n                         \'alpha\':self.alpha, \'tau\':self.tau,\'dc\':self.dc}\n        \n        self.sess = tf.Session()\n        self.actions = tf.placeholder(tf.float32, [None, 4], \'action\')\n        self.state = tf.placeholder(tf.float32, [None] + list(self.s_state), \'state\')\n        self.advantage = tf.placeholder(tf.float32, [None, 1], \'advantage\')\n        self.rewards = tf.placeholder(tf.float32, [None, 1], \'discounted_r\')\n\n        self.dataset = tf.data.Dataset.from_tensor_slices({""state"": self.state, ""actions"": self.actions,\n                                                           ""rewards"": self.rewards, ""advantage"": self.advantage})\n        self.dataset = self.dataset.shuffle(buffer_size=10000)\n        self.dataset = self.dataset.batch(MINIBATCH)\n        self.dataset = self.dataset.cache()\n        self.dataset = self.dataset.repeat(EPOCHS)\n        self.iterator = self.dataset.make_initializable_iterator()\n        self.batch = self.iterator.get_next()        \n        \n        self.sess.run(tf.global_variables_initializer()) # initialize \n        \n        if os.path.isfile(\'./\' + self.s_link):\n            self.message = [""\\nLOAD existing keras model...."", self.model.summary()]\n            self.model = load_model(self.s_link)\n        else:\n            self.message = \'\\nBuilding New Model >_\'\n            ### Build Networks Actor-Critic ###\n            # Actors\n            self.actor_old  = self.Actor()\n            self.actor_new = self.Actor()\n            self.actor_ = self.Actor()\n            # Critics\n            self.critic_old = self.Critic()     \n            self.critic_new = self.Critic()\n            self.critic_ = self.Critic()\n            \n        # Empty the lists\n        self.avg_actor=[]\n        self.avg_critic=[]\n        self.epsilon = []\n        self.reward = []\n        self.ep_rewards = []\n        self.ep_obs = []\n        self.ep_act = []\n        self.ep_obs_new = []\n        self.ep_flags=[] \n     \n    #@nb.jit    \n    def choose_action(self,state,episode):      \n        if episode % 100 !=0 :\n            action = self.actor_.predict([state.reshape(1,-1), DUMMY_VALUE, DUMMY_ACTION])[0] + np.random.normal(0,NOISE,4)\n            action_matrix = pred_action = action\n        else:\n            action = self.actor_.predict([state.reshape(1,-1),DUMMY_VALUE, DUMMY_ACTION])[0]\n            action_matrix = pred_action = action    \n        \n        #old_act = self.actor_old.predict([self.batch[""state""], DUMMY_VALUE, DUMMY_ACTION])[0]\n        #new_act = self.actor_new.predict([self.batch[""state""], DUMMY_VALUE, DUMMY_ACTION])[0]\n        #old_q = self.critic_old.predict([self.batch[""state""], DUMMY_VALUE, DUMMY_ACTION])[0]\n        #new_q = self.critic_new.predict([self.batch[""state""], DUMMY_VALUE, DUMMY_ACTION])[0]    \n        value  = self.critic_.predict([state.reshape(1,-1), DUMMY_VALUE])[0]   \n        return action, action_matrix, pred_action, value#, old_act, new_act, old_q, new_q\n    \n    #@nb.jit(nopython=True)                    \n    def storing(self, observation, action, reward, observation_new, flags ):\n        # Storing for replay-expirience\n        self.deck.append((observation, action, reward, observation_new, flags ))\n        self.ep_rewards.append(reward)\n        self.ep_obs.append(observation)\n        self.ep_act.append(action)\n        self.ep_obs_new.append(observation_new)\n        self.ep_flags.append(flags)\n    \n    #@nb.jit(nopython=True)   \n    def save(self):\n        self.actor_.save_weights(\'actor\'+self.s_link)\n        self.critic_.save_weights(\'critic\'+self.s_link)\n        \n    def sample_Critic(self):                     \n        state_input = Input(shape=(self.nx,))\n        action_input = Input(shape=(self.ny,))\n        #\n        h0 = Dense(self.layers[1], activation=\'relu\',kernel_regularizer=self.k_r)(state_input)\n        h1 = Dense(self.layers[1], activation=\'tanh\',kernel_regularizer=self.k_r)(action_input)\n        #\n        conc = Concatenate()([h0,h1])\n        #conc = Flatten()(conc)\n        h2 = Dense(64, activation=\'relu\', kernel_regularizer=self.k_r)(conc)\n        out = Dense(1, activation=\'linear\', kernel_regularizer=self.k_r,\\\n                    kernel_initializer=self.final_initializer)(h2)\n        #       \n        model  = Model(inputs=[state_input, action_input], outputs=out)\n        adam  = Adam(lr=LR)\n        model.compile(loss=""mse"", optimizer=adam)\n        return model   \n    \n    def Actor(self):                     \n        state_input = Input(shape=(self.nx,), name=\'obse\')\n        advantage = Input(shape=(1,), name=\'avanta\')\n        old_pred = Input(shape=(self.ny,), name=\'old-pred\')\n        #\n        h0 = Dense(self.layers[0], activation=\'relu\', kernel_regularizer=self.k_r)(state_input)\n        h1 = Dense(self.layers[1], activation=\'relu\',kernel_regularizer=self.k_r)(h0)\n        #\n        out = Dense(self.ny, activation=\'tanh\',kernel_regularizer=self.k_r,\\\n                    kernel_initializer=self.final_initializer)(h1)\n        \n        var = Lambda(lambda x: K.var(x))(out)\n        out = Lambda(lambda x: K.random_normal((self.ny,), out , var ))(out)\n        #out = K.get_value(out)\n        #\n        model = Model(inputs=[state_input, advantage, old_pred], outputs=out)\n        adam = Adam(lr=LR)\n        model.compile(loss=[PPO_Loss(advantage=advantage, old_pred=old_pred)], optimizer=adam)\n        lrate = LearningRateScheduler(self.step_decay)\n        callbacks_list = [lrate]\n        return model\n\n    def Critic(self):                     \n        state_input = Input(shape=(self.nx,))\n        old_value = Input(shape=(1,))\n        #\n        h0 = Dense(self.layers[0], activation=\'relu\',kernel_regularizer=self.k_r)(state_input)\n        h0 = Dropout(0.4)(h0)\n        h1 = Dense(self.layers[1], activation=\'relu\',kernel_regularizer=self.k_r)(h0)\n        h1 = Dropout(0.5)(h1)\n        #\n        out = Dense(1, activation=\'linear\', kernel_regularizer=self.k_r,\\\n                    kernel_initializer=self.final_initializer)(h1)\n        #       \n        model  = Model(inputs=[state_input, old_value], outputs=out)\n        adam  = Adam(lr=0.0)\n        model.compile(loss=[Critic_loss(old=old_value)], optimizer=adam)\n        lrate = LearningRateScheduler(self.step_decay)\n        callbacks_list = [lrate]\n        return model      \n    \n    # learning rate schedule\n    def step_decay(self,epoch):\n        \tinitial_lrate = LR\n        \tdrop = 0.15\n        \tepochs_drop = 10.0\n        \tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n        \treturn lrate\n        \n    #@nb.jit    \n    def discount(self,x, gamma, terminal_array=None):\n        if terminal_array is None:\n            return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n        else:\n            y, adv = 0, []\n            terminals_reversed = terminal_array[1:][::-1]\n            for step, dt in enumerate(reversed(x)):\n                y = dt + gamma * y * (1 - terminals_reversed[step])\n                adv.append(y)            \n        return np.array(adv)[::-1]    \n    \n    def update_nets(self):\n        # Get weights from current policy\n        weights_a = self.actor_new.get_weights()\n        self.actor_old.set_weights(weights_a)\n        weights_c = self.critic_new.get_weights()\n        self.critic_old.set_weights(weights_c)\n        # Update Current NEts Players:\n        self.actor_.set_weights(weights_a)\n        self.critic_.set_weights(weights_c)\n    \n    #@nb.jit\n    def TRAIN(self,obs, action, pred, rewards, advantage, old_pred, pred_values):     \n        actor_loss = []\n        critic_loss = []\n        epoch=[]\n        # Update Networks\n        self.update_nets()\n        # TRain in EPOCHS\n        start_train = time.time()\n        lrate = LearningRateScheduler(self.step_decay)\n        callbacks_list = [lrate]\n        for i in range(EPOCHS):\n            epoch_start = time.time()\n            # Main PLayer train\n            actor_loss.append(self.actor_.train_on_batch([obs, advantage, old_pred], [action]))\n            critic_loss.append(self.critic_.train_on_batch([obs, pred_values], [rewards]) )\n            self.actor_old.train_on_batch([obs, advantage, old_pred], [action])\n            self.actor_new.train_on_batch([obs, advantage, old_pred], [action])\n            self.critic_old.train_on_batch([obs, pred_values], [rewards])\n            self.critic_new.train_on_batch([obs, pred_values], [rewards])\n            \n            epoch.append(time.time()-epoch_start)\n        end = time.time() - start_train\n        \n        self.avg_actor.append(np.mean(actor_loss))\n        self.avg_critic.append(np.mean(critic_loss))\n        message = \'Training Time: %.3fs | Avg: %.3fs/epoch\' % (end, np.mean(epoch))\n        #print(\'Training Time: %.3fs | Avg: %.3fs/epoch\' % (end, np.mean(epoch)) )\n        return end, self.avg_actor, self.avg_critic, message\n                        \nif __name__ == \'__main__\':\n                \n    rendering = input(""Visualize rendering ? [y/n]:  "")  \n    s_link = ""BipedalWalker_model.h5""  \n    RENDER_REWARD_MIN = 5000\n    RENDER_ENV = False\n    if rendering == \'y\': RENDER_ENV = True  #flag for rendering the environment\n    EPISODES = 10000    # Number of episodes\n    env = gym.make(\'BipedalWalker-v2\')  #video_callable=lambda episode_id: episode_id%10==0\n    #env = gym.wrappers.Monitor(env, directory+\'lala3\',  force=True)\n\n    # Environment Parameters\n    nx = env.observation_space.shape # network-input  \n    ny = env.action_space.shape # network-out\n    agent = AGENT(nx,ny, s_link)\n    seed = np.random.seed(1)\n         \n    print(\'\\n+++++++++++++++++++++++++++++++++\')\n    print(\'BipedalWalker-v2 Starts...  >_\')\n    print(\'+++++++++++++++++++++++++++++++++\') \n    print(agent.message)    \n    print(""Environment Observation_space: "", env.observation_space)\n    print(""Environment Action_space: "", env.action_space) \n    print(""Number of Episodes: "" + str(EPISODES))\n    print(\'-----------------------------------\')\n    print(""\\n:::::Algorithm_Parameters::::::"")\n    print(list(agent.parameters.items()))\n    \n    # Initialize Simulation\n    batches = [[], [], [], [],[], []]  \n    counter = 0\n    mean100 =[]\n    traint_t = 0\n    proc = ProcessRewards()\n    # Start running the SIMULATION       \n    for i in range(EPISODES):         \n        observation = env.reset()         \n        observation = observation.reshape(1,-1)                \n        start = time.time()\n        ep_r, ep_t = 0, 0\n        #noise=OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=.3,size=4)\n        while True:\n            if RENDER_ENV: env.render()\n            act, action_matrix, predicted_action, q_value = agent.choose_action(observation,i)\n            #act, action_matrix, predicted_action, q_value, old_act, new_act, old_q, new_q = agent.choose_action(observation,i)\n            act.reshape((4,))\n            \n            if counter >= BATCH:\n                # UPDATE NETWORKS TRAIN IN BATCHES\n                traint_t +=1\n                print(\'Mpika!!!! train_call : %.i\' % traint_t)\n                r = np.array(batches[3])\n                r = r.reshape(-1,1)\n                proc.update(r) \n                r = np.clip(r/proc.std, -10,10)\n                #r = r.reshape(-1,1)\n                obs, action, pred = np.vstack(batches[0]), np.array(batches[1]), np.array(batches[2])\n                old_pred = pred\n                v_final = [q_value * (1-flag)]\n                pred_values =np.array(batches[5] + v_final ) # Get Q-values\n                terminals = np.array(batches[4] + [flag])\n                terminals = terminals.reshape(-1,1)\n                \n                deltas = r + agent.gamma * pred_values[1:]*(1-terminals[1:]) - pred_values[:-1]\n                advantage = agent.discount(deltas, agent.gamma * LAMBDA, terminals)\n                returns = advantage + np.array(batches[5])\n                advantage = (advantage - advantage.mean()) / np.maximum(advantage.std(), 1e-6)\n\n                # Train on batches #\n                train_time, avg_actor_loss, avg_critic_loss, message = agent.TRAIN(obs, action, pred, returns, advantage,\\\n                                                                                   old_pred, np.array(batches[5])) \n                print(message)\n                batches = [[], [], [], [],[], []]\n                counter=0        \n\n            batches[0].append(observation)\n            batches[1].append(action_matrix)\n            batches[2].append(predicted_action)\n            # ENVIRONMENT STEP\n            observation_new, reward, flag, info = env.step(np.clip(act,-1,1))   \n            observation_new.reshape(1,-1)\n            batches[3].append(reward)\n            batches[4].append(flag)\n            batches[5].append(q_value)\n\n            ep_r += reward\n            ep_t += 1\n            counter +=1\n            observation = observation_new\n            \n            if flag:\n                # Break episode\n                agent.ep_rewards.append(ep_r)\n                print(\'Episode: %i\' % i, \' | Reward: %.2f\' % ep_r , \' | Trajectories: %i\' % ep_t)\n                \n                if i %100 ==0:\n                    agent.save()\n                    mean100.append(np.mean(agent.ep_rewards[-100:]))\n                    print(\'Average Reward last 100-Episodes: %.3f\' % mean100[-1])\n                break\n    env.close()\n    \n    os.mkdir(""./XPS"")\n    os.mkdir(""./Plots"")\n    # Export        \n    np.save(""XPS/rewards_over_time"", agent.ep_rewards)\n    np.save(""XPS/mean100"", mean100)      \n                \n    plt.figure(figsize=(10,8))\n    plt.plot(agent.avg_actor, label=\'Avg.ActorLoss\')\n    plt.plot(agent.avg_critic, label=\'Avg.CriticLoss\')\n    plt.xlabel(""Training Time-Stamp"")\n    plt.ylabel(""Average Error"")\n    plt.title(""Epsilon Vs Episodes"")\n    plt.savefig(""Plots/Epsilon.png"")  \n                \n    plt.figure(figsize=(10,8))\n    plt.plot(agent.epsilon)\n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Epsilon value"")\n    plt.title(""Epsilon Vs Episodes"")\n    plt.savefig(""Plots/Epsilon.png"") \n\n    plt.figure(figsize=(10,8))            \n    plt.plot(agent.ep_rewards) \n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Rewards"")\n    plt.title(""Rewards per Episode"")\n    plt.legend(loc=0)\n    plt.savefig(""Plots/Rewards.png"")         \n    \n    plt.figure(figsize=(10,8))\n    plt.plot(mean100)\n    plt.xlabel(""100_episodes"")\n    plt.ylabel  (""Mean_value"")\n    plt.title(\'Average Reward per 100 episodes\')\n    plt.savefig(""Plots/mean_100.png"")       \n            \n            \n            \n           \n            \n            \n            \n            \n\n            \n            \n            \n            \n            \n            \n            \n\n\n'"
PPO/helpers.py,7,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Nov 26 20:19:14 2018\n\n@author: dead\n""""""\nimport numpy as np\n\n\nclass ProcessRewards():\n    def __init__(self, epsilon=1e-4, shape=()):\n        self.mean = np.zeros(shape, \'float64\')\n        self.var = np.ones(shape, \'float64\')\n        self.std = np.ones(shape, \'float64\')\n        self.count = epsilon\n\n    def update(self, x):\n        batch_mean = np.mean(x, axis=0)\n        batch_var = np.var(x, axis=0)\n        batch_count = x.shape[0]\n        self.update_from_moments(batch_mean, batch_var, batch_count)\n\n    def update_from_moments(self, batch_mean, batch_var, batch_count):\n        delta = batch_mean - self.mean\n        new_mean = self.mean + delta * batch_count / (self.count + batch_count)\n        m_a = self.var * self.count\n        m_b = batch_var * batch_count\n        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n        new_var = M2 / (self.count + batch_count)\n\n        self.mean = new_mean\n        self.var = new_var\n        self.std = np.maximum(np.sqrt(self.var), 1e-6)\n        self.count = batch_count + self.count'"
actor_critic/actor_Lstm.py,15,"b'""""""\nCreated on Wed Apr 25 20:41:53 2018\n\n@author: dead\nACTOR-CRITIC Network for Robot Locomotion\n\nTwo deep q-learning networks with two layers each one and mini-batch experience replay update rule.\nActor_local >> predicts the actual action for the next state\nCritic_local >> predicts the q_value\n\nEpisode reset 20 secs.\n\n--------\n-Actor_Local: \n    Input: obs_new\n    H0   : 128 relu\n    H1   : 64 relu\n    Out  : 4  tanh\n    bias_init: Uniform(-0.003, 0.003)\n\n-Actor_Target: Same as Actor_local used for predicting the next action for the Q-learning rule.\n\n--------   \n-Critict_Local:\n    Input1 : obs_new\n    H0     : 128 relu\n    H1     : 64 relu\n    \n    Input2 : pred_action\n    H2     : 64 relu\n    \n    merge  : 32 relu\n    Out    : 1  linear\n   \n-Critic_Target: Same as critic_local used for predicting the q_target value based on the predicted action.    \n\n--------\nUpdate Rule Q-learning:\n    - mini-batch(16)\n    - Q_target = (1-a)reward + a*gamma*mean(predict(obs+obs_new) )\n\n-------    \nSoft-Update Netowrk-Weights:\n    - \xce\xb8_target = \xcf\x84*\xce\xb8_local + (1 - \xcf\x84)*\xce\xb8_target\n    \n-------\nParameters:        \n    BATCH = 16\n    self.e = 1.0\n    self.e_= 0.01\n    self.dc= 0.9999\n    self.tau = 1e-3\n    self.weight_decay = 0.0001\n    self.lr_actor = 1e-4 # Actor_Learning rate\n    self.lr_critic = 3e-4 # Critic_Learning rate\n    self.gamma = 0.99\n    self.alpha = 0.1\n------- \n\n""""""\n\n#from __future__ import print_function\nimport pandas as pd\nfrom keras.models import load_model, Sequential, Model\nfrom keras.initializers import RandomUniform\nfrom keras.regularizers import l2\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras import backend as K\nfrom keras.layers import Dense, Dropout, Input, MaxPooling1D,Conv1D,LSTM, Flatten, Reshape\nfrom keras.layers.merge import Add, Multiply, Concatenate\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport time\nimport numpy as np\nimport gym\nimport random\nfrom collections import deque\nimport tensorflow as tf\nimport os\n\n    \nclass AGENT:\n    def __init__(self, nx, ny, s_link, sess, batch):\n        self.nx = nx[0]  #  Observation array length   nx = env.observation_space.shape[0]\n        self.nx_lidar = 10\n        self.nx_obs = 14\n        self.ny = ny[0]  #   Action space length       ny = env.action_space.n\n        self.lr_actor = 1e-4 # Actor_Learning rate\n        self.lr_critic = 3e-4 # Critic_Learning rate\n        self.batch = batch\n        self.gamma = 0.99\n        self.alpha = 0.1\n        self.s_link =s_link \n        self.sess = sess\n        self.deck = deque(maxlen=4000)\n        self.e = 1.0\n        self.e_= 0.01\n        self.dc= 0.9999\n        self.tau = 1e-3\n        self.weight_decay = 0.0001\n        self.los = []\n        self.parameters={\'lr_actor\': self.lr_actor,\'lr_critic\':self.lr_critic,\'gamma\':self.gamma,\n                         \'alpha\':self.alpha, \'tau\':self.tau,\'dc\':self.dc,\'Batch\':self.batch}\n        \n        \n        if os.path.isfile(\'./\' + self.s_link):\n            self.message = [""\\nLOAD existing keras model...."", self.model.summary()]\n            self.model = load_model(self.s_link)\n        else:\n            self.message = \'\\nBuilding New Model >_\'\n            ### Setting Actor ###\n            #######################################################\n            self.actor_lidar_input, self.actor_state_input, self.actor_local = self.Actor()      \n            _,_, self.actor_target = self.Actor()\n            # Open a placeholder for gradiends\n            self.actor_critic_grads = tf.placeholder(tf.float32, [None,self.ny])\n            actor_local_weights = self.actor_local.trainable_weights\n            self.actor_grads = tf.gradients(self.actor_local.output, actor_local_weights, -self.actor_critic_grads)\n            grads = zip(self.actor_grads, actor_local_weights)\n            self.optimize = tf.train.AdamOptimizer(self.lr_actor).apply_gradients(grads)\n            ######################################################################\n            \n            ### Setting Critic ###\n            ####################################################################################         \n            self.critic_lidar_input, self.critic_state_input, self.critic_action_input, self.critic_local = self.Critic()      \n            _,_, _, self.critic_target = self.Critic()\n            # Open placeholder for gradients\n            self.critic_grads = tf.gradients(self.critic_local.output,  self.critic_action_input)\n            \n            # Initialize for later grafient calculations\n            self.sess.run(tf.global_variables_initializer())\n            \n        # Empty the lists\n        self.ep_rewards=[]\n        \n    def choose_action(self,observation):\n        # Use epsilon-greedy algorithm\n        if np.random.rand() <= self.e : \n            action = np.random.uniform(-1,1,4)\n            return action       \n        state = observation[0][:14].reshape((1,14))\n        lidar = observation[0][14:].reshape((1,10,1))\n        action = self.actor_local.predict([lidar,state])\n        return action\n                    \n    def storing(self, observation, action, reward, observation_new, flags ):\n        # Storing for replay-expirience\n        self.deck.append((observation, action, reward, observation_new, flags ))\n        self.ep_rewards.append(reward)\n        \n    def save(self,name):\n        # Save network configuration\n        self.actor_local.save(name)\n        self.critic_local.save(name)\n\n    def Actor(self):                     \n        # Build Network for Actor\n        #input=[lidar_input,state_input]\n        lidar_input = Input(shape=(self.nx_lidar,1))\n        lidar_conv = Conv1D(64, 4, activation=\'relu\')(lidar_input)\n        pool = MaxPooling1D(4)(lidar_conv)\n        flat = Flatten()(pool)\n               \n        state_input = Input(shape=(self.nx_obs,))\n        state_h1 = Dense(192, activation=\'relu\')(state_input)\n        \n        merged = Concatenate()([flat,state_h1])\n        merged_reshaped = Reshape((256,1))(merged)\n        merged_lstm = LSTM(256,activation=\'relu\',input_shape=(1,256,1))(merged_reshaped)\n        output = Dense(self.ny, activation=\'tanh\')(merged_lstm)\n        \n        model = Model(input=[lidar_input,state_input], output=output)\n        adam = Adam(lr=self.lr_actor)\n        model.compile(loss=\'mse\', optimizer=adam)\n        return lidar_input,state_input, model\n\n    def Critic(self):                     \n        # Build Network for Critic       \n        #input=[lidar_input,state_input,action_input]\n        lidar_input = Input(shape=(self.nx_lidar,1))\n        lidar_conv = Conv1D(64, 4, activation=\'relu\',input_shape=(self.nx_lidar,1))(lidar_input)\n        pool = MaxPooling1D(4)(lidar_conv)\n        flat= Flatten()(pool)\n        \n        state_input = Input(shape=(self.nx_obs,))\n        state_h1 = Dense(192, activation=\'relu\')(state_input)\n        \n        action_input = Input(shape=(self.ny,))\n        action_h1    = Dense(64, activation=\'relu\')(action_input)\n        \n        merge1 = Concatenate()([flat,state_h1])\n        merged_dense = Dense(256, activation=\'relu\')(merge1)\n        \n        merge2 = Concatenate()([merged_dense,action_h1])\n        merge2reshaped = Reshape((320,1))(merge2)\n        merge_lstm = LSTM(320, activation=\'relu\',input_shape=(1,320,1))(merge2reshaped)\n        output= Dense(1,activation=\'linear\')(merge_lstm)\n        \n        model  = Model(input=[lidar_input,state_input,action_input], output=output)\n        adam  = Adam(lr=self.lr_critic)\n        model.compile(loss=""mse"", optimizer=adam)\n        return lidar_input,state_input, action_input, model\n    \n\n    def _train_critic(self, sample_indx):\n        for observation, act, reward, obs_new, done in sample_indx:  \n            Q_target = np.array(reward).reshape(1,-1)\n            act = act.reshape(1,-1)\n            state = observation[0][:14].reshape((1,14))\n            lidar = observation[0][14:].reshape((1,10,1))\n            state_new = obs_new[0][:14].reshape((1,14))\n            lidar_new = obs_new[0][14:].reshape((1,10,1))\n            if not done:\n                target_action = self.actor_target.predict([lidar_new,state_new])\n                future_reward = self.critic_target.predict([lidar_new,state_new, target_action])[0][0]\n                Q_target =(1-self.alpha)*Q_target +  self.alpha* self.gamma * future_reward\n                Q_target = Q_target.reshape(1,-1)\n            self.critic_local.fit(x=[lidar,state,act],\\\n                                  y=Q_target, verbose=0, epochs=1)   \n            \n            \n    def _train_actor(self, sample_indx):\n        for observation, act, reward, observation_new, _ in sample_indx:\n            state = observation[0][:14].reshape((1,14))\n            lidar = observation[0][14:].reshape((1,10,1))\n\n            predicted_action = self.actor_local.predict([lidar,state])\n            grads = self.sess.run(self.critic_grads, feed_dict = {\n                    self.critic_lidar_input : lidar,\n                    self.critic_state_input: state,\n                    self.critic_action_input: predicted_action})[0]\n            \n            self.sess.run(self.optimize, feed_dict={\n                    self.actor_lidar_input: lidar,\n                    self.actor_state_input: state,\n                    self.actor_critic_grads: grads})            \n            \n    def _update_actor_target(self):\n        """"""Soft update model parameters.\n        \xce\xb8_target = \xcf\x84*\xce\xb8_local + (1 - \xcf\x84)*\xce\xb8_target""""""\n\n        actor_local_weights  = self.actor_local.get_weights()\n        actor_target_weights =self.actor_target.get_weights()\n        \n        for i in range(len(actor_target_weights)):\n            actor_target_weights[i] = self.tau*actor_local_weights[i] + (1-self.tau)*actor_target_weights[i]\n        self.actor_target.set_weights(actor_target_weights)          \n            \n    def _update_critic_target(self):\n        """"""Soft update model parameters.\n        \xce\xb8_target = \xcf\x84*\xce\xb8_local + (1 - \xcf\x84)*\xce\xb8_target""""""\n        critic_local_weights  = self.critic_local.get_weights()\n        critic_target_weights = self.critic_target.get_weights()\n\t\t\n        for i in range(len(critic_target_weights)):\n            critic_target_weights[i] = self.tau*critic_local_weights[i] + (1-self.tau)*critic_target_weights[i]\n        self.critic_target.set_weights(critic_target_weights)\t\t\n\n    def update_target(self):\n        self._update_actor_target()\n        self._update_critic_target()\n            \n      \n    def TRAIN(self, batch):\n\n        if len(self.deck) < batch:\n            return\n        # Random sample\n        sample_indx = random.sample(self.deck, batch)\n        \n        # Initialize Dictionary for time\n        time_all = {}\n        start_train = time.time()\n               \n        # Train Critic\n        start_critic = time.time()\n        self._train_critic(sample_indx) # TRain the network critic\n        end_critic = time.time()-start_critic\n                \n        # Train Actor\n        start_actor= time.time()\n        self._train_actor(sample_indx)\n        end_actor = time.time() - start_actor\n                \n        # Update Weights of \n        start_update = time.time()\n        self.update_target() # Update the netowkr local and target weights for actor AND critic\n        end_update = time.time() - start_update\n        \n        # Erase episode rewards list\n        self.ep_rewards= []\n                     \n        if self.e >= self.e_:\n            self.e *= self.dc\n            \n        end = time.time() - start_train\n        time_all={\'Critic_Train\': round(end_critic,1) ,\\\n                  \'Actor_Train\':round(end_actor,1),\\\n                  \'Weights_Update\': round(end_update,1) ,\\\n                  \'Train_Overall\':round(end,1) }\n        return time_all\n\nif __name__ == \'__main__\':\n    \n    rendering = input(""Visualize rendering ? [y/n]:  "")  \n    s_link = ""BipedalWalker_model.h5""  \n    RENDER_REWARD_MIN = 5000\n    RENDER_ENV = False\n    if rendering == \'y\': RENDER_ENV = True  #flag for rendering the environment\n    EPISODES = 100000    # Number of episodes\n    \n    env = gym.make(\'BipedalWalker-v2\')\n    env = env.unwrapped\n    \n    # Network Parameters\n    nx = env.observation_space.shape # network-input  \n    ny = env.action_space.shape # network-out\n    BATCH = 16\n    sess = tf.Session()\n    K.set_session(sess)\n    agent = AGENT(nx,ny, s_link, sess, BATCH)\n    \n    rewards_over_time = []\n    error = []\n    epsilon = []\n    rew_var = []\n    rew_mean = []\n    mean_100 = []\n    seed = np.random.seed(666)\n         \n    print(\'\\n+++++++++++++++++++++++++++++++++\')\n    print(\'BipedalWalker-v2 Starts...  >_\')\n    print(\'+++++++++++++++++++++++++++++++++\')\n    print(""><><><><><><><><><><><><><><><><>"")\n    print(""-----------------------------------"")       \n    print(agent.message)    \n    print(""Environment Observation_space: "", env.observation_space)\n    print(""Environment Action_space: "", env.action_space) \n    print(""Number of Episodes: "" + str(EPISODES))\n    print(\'-----------------------------------\')\n    \n    print(""\\n:::::Algorithm_Parameters::::::"")\n    print(list(agent.parameters.items()))\n    w = 0\n        \n    # Start running the episodes        \n    for i in range(EPISODES): \n        observation = env.reset()         \n        observation = observation.reshape(1,-1)                \n        start = time.time()\n        \n        while True:            \n            if RENDER_ENV:\n                env.render()\n            \n            action = agent.choose_action(observation)\n            action = action.reshape((4,))\n            observation_new, reward, flag, inf = env.step(np.clip(action,-1,1))\n            observation_new = observation_new.reshape((1,24))                    \n            # Store new information\n            agent.storing(observation, action, reward, observation_new, flag)   \n            observation = observation_new         \n            # Measure the time\n            end = time.time()\n            time_space = end - start\n            \n            if time_space > 30:\n                flag = True\n          \n            # Sum the episode rewards\n            ep_rew_total = sum(agent.ep_rewards)\n            mean = np.mean(agent.ep_rewards)\n            var = np.var(agent.ep_rewards)\n            if ep_rew_total < -300:\n                flag = True\n            \n            if flag==True:\n                rewards_over_time.append(ep_rew_total)\n                rew_mean.append(mean)\n                rew_var.append(var)\n                max_reward = np.max(rewards_over_time)\n                episode_max = np.argmax(rewards_over_time)\n                if ep_rew_total >=300 :\n                    w = w + 1\n                    agent.save(s_link)\n                                        \n                print(""++++++++++++++++++++++++++++++++++++++++++++++++++++++++"")\n                print(""Episode: "", i)\n                print(""Time: "", np.round(time_space, 2),""secs"")\n                print(""Reward:"", ep_rew_total)\n                print(""Maximum Reward: "" + str(max_reward) + ""  on Episode: "" + str(episode_max))\n                print(""Times win: "" + str(w))\n                \n                if i % 100 ==0:\n                    print(""Mean reward of the past 100 episodes: "", str(np.mean(rewards_over_time[-100:])))\n                    mean_100.append(np.mean(rewards_over_time[-100:]))\n                    f = open(\'results.txt\',\'a\')\n                    f.write(\'\\n\' + str(np.mean(rewards_over_time[-100:])))\n                    f.close()\n                \n                # Start training the Neural Network\n                training_time = agent.TRAIN(BATCH)\n                print(""Time: "" + str(list(training_time.items())))\n                \n                epsilon.append(agent.e)\n                \n                if max_reward > RENDER_REWARD_MIN: RENDER_ENV = True\n                \n                break\n    \n    np.save(""rewards_over_time"", rewards_over_time)\n    np.save(""mean100"", mean_100)         \n                \n    plt.figure(figsize=(10,8))\n    plt.plot(epsilon)\n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Epsilon value"")\n    plt.title(""Epsilon Vs Episodes"")\n    plt.savefig(""Epsilon.png"") \n\n    plt.figure(figsize=(10,8))            \n    plt.plot(rewards_over_time, label=""Rewards"")\n    plt.plot(rew_mean, label=""Mean"")\n    plt.plot(rew_var, label=""Variance"")    \n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Rewards"")\n    plt.title(""Rewards per Episode"")\n    plt.legend(loc=0)\n    plt.savefig(""Rewards.png"")         \n    \n    plt.figure(figsize=(10,8))\n    plt.plot(mean_100)\n    plt.xlabel(""100_episodes"")\n    plt.ylabel  (""Mean_value"")\n    plt.title(\'Average Reward per 100 episodes\')\n    plt.savefig(""mean_100.png"")       \n            \n            \n            \n           \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n\n\n'"
ddpg/OrnsteinUhlenbeckProcess.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Dec  6 19:53:31 2018\n\n@author: dead\n""""""\nimport numpy as np\nclass OrnsteinUhlenbeckProcess:\n    """""" Ornstein-Uhlenbeck Noise (original code by @slowbull)\n    """"""\n    def __init__(self, theta=0.15, mu=0, sigma=1, x0=0, dt=1e-2, n_steps_annealing=100, size=4):\n        self.theta = theta\n        self.sigma = sigma\n        self.n_steps_annealing = n_steps_annealing\n        self.sigma_step = - self.sigma / float(self.n_steps_annealing)\n        self.x0 = x0\n        self.mu = mu\n        self.dt = dt\n        self.size = size\n\n    def generate(self, step):\n        sigma = max(0, self.sigma_step * step + self.sigma)\n        x = self.x0 + self.theta * (self.mu - self.x0) * self.dt + sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n        self.x0 = x\n        return x\n\n\n#if __name__ == \'__main__\':\n#    noise = OrnsteinUhlenbeckProcess(theta=0.15, mu=0, sigma=1, x0=0, dt=1e-2, n_steps_annealing=100, size=4)\n'"
ddpg/ddpg_batch.py,21,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Apr 25 20:41:53 2018\n\n@author: dead\nACTOR-CRITIC Network for Robot Locomotion\n\n""""""\n\n#from __future__ import print_function\nimport pandas\nfrom keras.models import load_model, Sequential, Model\nfrom keras.initializers import RandomUniform, RandomNormal\nfrom keras.regularizers import l2\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.activations import softplus\nfrom keras import backend as K\nfrom keras.layers import Dense, Dropout, Input, MaxPooling1D,Conv1D,Conv2D,LSTM, Flatten, Reshape, GaussianNoise\nfrom keras.layers.merge import Add, Multiply, Concatenate\nfrom keras.optimizers import Adam, RMSprop\nimport matplotlib.pyplot as plt\nplt.switch_backend(\'agg\')\nimport time\nimport numpy as np\nimport gym\nimport gym.wrappers\n#from collections import deque\nimport tensorflow as tf\nimport os\nimport sys\n#os.chdir(""/home/dead/Documents/Master_Research/ddpg/"")\nfrom OrnsteinUhlenbeckProcess import OrnsteinUhlenbeckProcess\n\nEPISODES = int(sys.argv[1])\nBATCH = int(sys.argv[2])\nlstm = sys.argv[3]\narg = int(sys.argv[4])\nepc = int(sys.argv[5])\nif lstm == \'y\':\n    lstm = True\nelse:\n    lstm = False\nDUMMY_ACTION, DUMMY_VALUE = np.zeros((1, 4)), np.zeros((1,1))\ntf.device(\'cpu=0\')\n\ndef actor_loss(advantage, old_prediction):\n    def loss(y_true, y_pred):\n        var = K.variable(1.0)\n        pi = K.variable(3.1415926)\n        denom = K.sqrt(2 * pi * var)\n        prob_num = K.exp(- K.square(y_true - y_pred) / (2 * var))\n        old_prob_num = K.exp(- K.square(y_true - old_prediction) / (2 * var))\n\n        prob = prob_num/denom\n        old_prob = old_prob_num/denom\n        r = prob/(old_prob + 1e-10)\n\n        return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - 1e-5, max_value=1 + 1e-5) * advantage))\n    return loss\n\n\nclass AGENT:\n    def __init__(self, nx, ny, s_link, sess, lstm):\n        self.nx = nx[0]  #  Observation array length   nx = env.observation_space.shape[0]\n        self.nx_lidar = 10\n        self.nx_obs = 14\n        self.ny = ny[0]  #   Action space length       ny = env.action_space.n\n        self.lr_actor = 0.0001 # Actor_Learning rate\n        self.lr_critic = 0.0002 # Critic_Learning rate\n        self.gamma = 0.99\n        self.alpha = 0.1\n        self.s_link =s_link\n        self.sess = sess\n        self.lstm = lstm\n        #self.deck = deque(maxlen=4000)\n\n        self.e = 1.0\n        self.e_= 0.01\n        self.dc= 0.99\n        self.tau = 0.001\n        self.weight_decay = 0.001\n        self.los = []\n        self.layers=[256,128]\n\n        # Network init_functions\n        self.k_r = l2(self.weight_decay)\n        self.initializer = ""glorot_uniform""\n        self.final_initializer = RandomUniform(minval = -0.003, maxval = 0.003)\n        self.parameters={\'lr\': self.lr_actor, \'gamma\':self.gamma,\n                         \'alpha\':self.alpha, \'tau\':self.tau,\'dc\':self.dc}\n\n        if os.path.isfile(\'./\' + self.s_link):\n            self.message = [""\\nLOAD existing keras model...."", self.model.summary()]\n            self.model = load_model(self.s_link)\n        else:\n            self.message = \'\\nBuilding New Model >_\'\n            ### Setting Actor #########################################################################\n            self.actor_lidar_input, self.actor_state_input, self.actor_local = self.Actor()\n            _,_, self.actor_target = self.Actor()\n            # Take actor grafients################################\n            self.actor_critic_grads = tf.placeholder(tf.float32, [None,self.ny])\n            actor_local_weights = self.actor_local.trainable_weights\n            self.actor_grads = tf.gradients(self.actor_local.output, actor_local_weights, self.actor_critic_grads)\n            grads = zip(self.actor_grads, actor_local_weights)\n            self.optimize = tf.train.AdamOptimizer(self.lr_actor).apply_gradients(grads)\n            ######################################################################\n            ### Setting Critic #################################################\n            self.critic_lidar_input, self.critic_state_input, self.critic_action_input, self.critic_local = self.Critic()\n            _,_, _, self.critic_target = self.Critic()\n            # Take critic gradients for actor training\n            self.critic_grads = tf.gradients(self.critic_local.output,  self.critic_action_input)\n\n            # Initialize variables\n            self.sess.run(tf.global_variables_initializer())\n\n        # Empty the lists\n        self.ep_rewards, self.ep_obs, self.ep_act, self.ep_obs_new, self.ep_flags=[], [], [],[], []\n\n    def choose_action(self,observation):\n        state = observation[0][:14].reshape((1,14))\n        lidar = observation[0][14:].reshape((1,10))\n        # Use epsilon-greedy algorithm\n        if np.random.rand() <= self.e :\n            # epsilon Greedy             \n            if self.e >= self.e_:\n                self.e *= self.dc\n                epsilon.append(self.e)\n            action_=np.random.uniform(-1,1,4)\n            return action_\n\n        action = self.actor_local.predict([DUMMY_ACTION,DUMMY_VALUE,lidar,state])\n        return action\n\n    def storing(self, observation, action, reward, observation_new, flags ):\n        # Storing for replay-expirience\n        #self.deck.append((observation, action, reward, observation_new, flags ))\n        self.ep_rewards.append(reward)\n        self.ep_obs.append(observation)\n        self.ep_act.append(action)\n        self.ep_obs_new.append(observation_new)\n        self.ep_flags.append(flags)\n\n    def save(self,name):\n        # Save network configuration\n        self.actor_local.save(name)\n        self.critic_local.save(name)\n\n    def Actor(self):\n        # Build Network for Actor\n        #input=[lidar_input,state_input]\n        old_act = Input(shape=(4,))\n        advantage = Input(shape=(1,))\n        #\n        lidar_input = Input(shape=(self.nx_lidar,))\n        lidar_conv = Dense(self.layers[0], activation=\'relu\', kernel_regularizer=self.k_r)(lidar_input)\n        #pool = MaxPooling1D(4)(lidar_conv)\n        #flat = Flatten()(lidar_conv)\n        lidar_conv = Dropout(0.05)(lidar_conv)\n        #\n        state_input = Input(shape=(self.nx_obs,))\n        state_h1 = Dense(self.layers[0], activation=\'relu\', kernel_regularizer=self.k_r)(state_input)\n        #state_h1 = GaussianNoise(1.0)(state_h1)\n        #gauss = Flatten()(gauss)\n        #\n        merged = Concatenate()([lidar_conv,state_h1])\n        if self.lstm:\n            merged = Reshape((self.layers[0]*2,1))(merged)\n            merged_lstm = LSTM(self.layers[1],activation=\'relu\',kernel_regularizer=self.k_r ,\\\n                                kernel_initializer=self.initializer)(merged)\n        else:\n            merged_lstm = Dense(self.layers[1],activation=\'relu\',kernel_regularizer=self.k_r ,\\\n                                kernel_initializer=self.initializer)(merged)\n\n        output = Dense(self.ny, activation=\'tanh\', kernel_regularizer=self.k_r,\\\n                       kernel_initializer=self.final_initializer)(merged_lstm)\n\n        ##############\n        model = Model(input=[old_act,advantage,lidar_input,state_input], output=output)\n        adam = Adam(lr=self.lr_actor, decay=0.5)\n        model.compile(loss=actor_loss(advantage,old_act), optimizer=adam)\n        return lidar_input,state_input, model\n\n    def Critic(self):\n        lidar_input = Input(shape=(self.nx_lidar,))\n        lidar_conv = Dense(self.layers[0], activation=\'relu\', kernel_regularizer=self.k_r,\\\n                           kernel_initializer=self.initializer)(lidar_input)\n        #flat= Flatten()(lidar_conv)\n        lidar_conv =Dropout(0.5)(lidar_conv)\n        #\n        #\n        state_input = Input(shape=(self.nx_obs,))\n        state_h1 = Dense(self.layers[0], activation=\'relu\', kernel_regularizer=self.k_r,\\\n                         kernel_initializer=self.initializer)(state_input)\n        #state_h1 = Flatten()(state_h1)\n        state_h1 = Dropout(0.5)(state_h1)\n        #\n        merge1 = Concatenate()([lidar_conv,state_h1])\n        #merged_dense = Dense(self.layers[0], activation=\'relu\')(merge1)\n        #\n        action_input = Input(shape=(self.ny,))\n        action_h1    = Dense(self.layers[0], activation=\'relu\',kernel_regularizer=self.k_r,\\\n                             kernel_initializer=self.initializer)(action_input)\n        action_h1 = Dropout(0.5)(action_h1)\n        #\n        merge2 = Concatenate()([merge1,action_h1])\n        #merge2 = Reshape((self.layers[0]*3,1))(merge2)\n        #\n        if self.lstm:\n            merge2 = Reshape((self.layers[0]*3,1))(merge2)\n            merged_lstm = LSTM(self.layers[1],activation=\'relu\',kernel_regularizer=self.k_r ,\\\n                               kernel_initializer=self.initializer)(merge2)\n        else:\n            merged_lstm = Dense(self.layers[1],activation=\'relu\',kernel_regularizer=self.k_r ,\\\n                                kernel_initializer=self.initializer)(merge2)\n        merged_lstm = Dropout(0.5)(merged_lstm)\n        #\n        output= Dense(1,activation=\'linear\', kernel_regularizer=self.k_r,\\\n                      kernel_initializer=self.final_initializer)(merged_lstm)\n        ##############\n        model  = Model(input=[lidar_input,state_input,action_input], output=output)\n        adam  = Adam(lr=self.lr_critic, decay=0.6)\n        model.compile(loss=""mse"", optimizer=adam)\n        return lidar_input,state_input, action_input, model\n\n    def _train_critic(self, lidar, state, act, Q_target):\n        for _ in range(epc):\n            self.critic_local.train_on_batch([lidar,state,act], [Q_target])\n        #self.critic_local.fit(x=[lidar,state,act],y=Q_target, verbose=0, epochs=10)\n\n    def _train_actor(self, lidar,state,act,rew):\n        current_reward = self.critic_target.predict([lidar,state,act])[0]\n        advantage = rew - current_reward\n\n        predicted_action = self.actor_local.predict([act,advantage,lidar,state])\n        grads = self.sess.run(self.critic_grads, feed_dict = {\n                self.critic_lidar_input : lidar,\n                self.critic_state_input: state,\n                self.critic_action_input: predicted_action})[0]\n\n        for _ in range(epc):\n            self.sess.run(self.optimize, feed_dict={\n                    self.actor_lidar_input: lidar,\n                    self.actor_state_input: state,\n                    self.actor_critic_grads: grads})\n\n    def update_target(self):\n        """"""Soft update model parameters.\n        ?_target = ?*?_local + (1 - ?)*?_target""""""\n        actor_local_weights  = self.actor_local.get_weights()\n        actor_target_weights =self.actor_target.get_weights()\n        #\n        for i in range(len(actor_target_weights)):\n            actor_target_weights[i] = self.tau*actor_local_weights[i] + (1-self.tau)*actor_target_weights[i]\n        self.actor_target.set_weights(actor_target_weights)\n        ##########################################################\n        critic_local_weights  = self.critic_local.get_weights()\n        critic_target_weights = self.critic_target.get_weights()\n        #\n        for i in range(len(critic_target_weights)):\n            critic_target_weights[i] = self.tau*critic_local_weights[i] + (1-self.tau)*critic_target_weights[i]\n        self.critic_target.set_weights(critic_target_weights)\n\n    def discount_rewards(self, rewards):\n        discounted_rewards = np.zeros_like(rewards)\n        for i in range(len(rewards)-2,-1,-1):\n            discounted_rewards[i] = rewards[i] + rewards[i+1]*self.gamma**i\n        return discounted_rewards\n\n    def create_batch(self,BATCH):\n        state, lidar, state_new ,lidar_new, current_reward, advantage =[],[],[],[],[], []\n        discounted_rewards = self.discount_rewards(self.ep_rewards[0:BATCH])\n        traj = [self.ep_obs[0:BATCH], self.ep_act[0:BATCH], discounted_rewards,\\\n                       self.ep_obs_new[0:BATCH], self.ep_flags[0:BATCH]]\n        rew = np.array(traj[2]).reshape(-1,1)\n        act = np.array(traj[1])\n        for observation, obs_new in zip(traj[0],traj[3]):\n            state.append(observation[0][:14].reshape((14,)))\n            lidar.append(observation[0][14:].reshape((10,)))\n            state_new.append(obs_new[0][:14].reshape((14,)))\n            lidar_new.append(obs_new[0][14:].reshape((10,)))\n\n        state = np.array(state)\n        lidar  = np.array(lidar)\n        state_new  = np.array(state_new)\n        lidar_new  = np.array(lidar_new)\n\n        current_reward = self.critic_target.predict([lidar,state,act])\n        advantage = rew - current_reward\n        target_action = self.actor_target.predict([act,advantage,lidar_new,state_new])\n        future_reward = self.critic_target.predict([lidar_new,state_new, target_action])\n        Q_target =self.alpha*(rew + self.gamma * future_reward - current_reward)\n        #print(\'cr: \' + str(current_reward.shape) + \'target: \' + str(target_action.shape) + \'fut: \' + str(future_reward.shape) + \'adv: \' + str(advantage.shape))\n        return lidar, state, act, Q_target , advantage, rew\n\n    def TRAIN(self, BATCH):\n        lidar, state, act, Q_target , advantage, rew = self.create_batch(BATCH)\n        start_train = time.time()\n        # Train Critic\n        #print(\'lidar:\' + str(lidar.shape) + str(state.shape) + \'act: \' + str(act.shape) +\\\n        #      \'Q_target: \' + str(Q_target.shape) + \'adv: \' + str(advantage.shape)+ \'rew: \' + str(rew.shape) )\n        self._train_critic(lidar, state, act, Q_target) # TRain the network critic\n        # Train Actor\n        self._train_actor(lidar,state,act,rew)\n        # Update Weights \n        self.update_target() # Update the netokr local and target weights for actor AND critic\n        end = time.time() - start_train\n        return end\n\n    def Clear(self):\n\n        self.ep_rewards, self.ep_obs, self.ep_act, self.ep_obs_new, self.ep_flags=[], [], [],[], []\n\nif __name__ == \'__main__\':\n\n    rendering = input(""Visualize rendering ? [y/n]:  "")\n    s_link = ""BipedalWalker_model.h5""\n    RENDER_REWARD_MIN = 5000\n    RENDER_ENV = False\n    if rendering == \'y\': RENDER_ENV = True  #flag for rendering the environment       \n    directory = \'Monitor\'\n    env = gym.make(\'BipedalWalker-v2\')  #video_callable=lambda episode_id: episode_id%10==0\n    #env = gym.wrappers.Monitor(env, directory+\'lala3\',  force=True)\n    # Network Parameters\n    nx = env.observation_space.shape # network-input  \n    ny = env.action_space.shape # network-out\n    sess = tf.Session()\n    K.set_session(sess)\n    agent = AGENT(nx,ny, s_link, sess, lstm)\n\n    rewards_over_time = []\n    error = []\n    epsilon = []\n    rew_var = []\n    rew_mean = []\n    mean_100 = []\n    seed = np.random.seed(1)\n\n    print(\'-----------------------------------\')\n    print(agent.message)\n    print(""Environment Observation_space: "", env.observation_space)\n    print(""Environment Action_space: "", env.action_space)\n    print(""Num of Episodes: %i | Batch: %i | Epochs %i"" % (EPISODES,BATCH,epc))\n    if agent.lstm: print(""LSTM layer is ON!"")\n    print(\'-----------------------------------\')\n    print(""\\n:::::Algorithm_Parameters::::::"")\n    print(list(agent.parameters.items()))\n    print(\'\\n+++++++++++++++++++++++++++++++++\')\n    print(\'BipedalWalker-v2 Starts... Enjoy! >_\')\n    print(\'+++++++++++++++++++++++++++++++++\')\n    w = 0\n    traj = 0\n\n    # Start running the SIMULATION      \n    for i in range(EPISODES):\n        observation = env.reset()\n        observation = observation.reshape(1,-1)\n        start = time.time()\n        counter = 0\n        noise=OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=.1,size=4)\n        rew =0\n        # Start EPISODE\n        while True:\n            if RENDER_ENV:\n                env.render()\n            counter +=1\n            traj += 1\n            action = agent.choose_action(observation)\n            action = np.clip(action+noise.generate(counter), -1,1)\n            action = action.reshape((4,))\n            observation_new, reward, flag, inf = env.step(action)\n            observation_new = observation_new.reshape((1,24))\n            rew += reward\n            # Store new information\n            agent.storing(observation, action, reward, observation_new, flag)\n            observation = observation_new\n\n            # Measure the time\n            end = time.time()\n            # Set time constrain: 40secs stop episode\n            time_space = end - start\n            if time_space > 200:\n                flag = True\n\n            if flag==True:\n                # Append rewards history\n                rewards_over_time.append(rew)\n                rew_mean.append(np.mean(agent.ep_rewards))\n                rew_var.append(np.var(agent.ep_rewards))\n                max_reward = np.max(rewards_over_time)\n                episode_max = np.argmax(rewards_over_time)\n                # Output results and terminate episode                \n                # Winning Statement\n                if rew >=300 :\n                    w = w + 1\n                    agent.save(s_link)\n                # Print output per 100 episodes\n                if i % arg == 0:\n                    print(""++++++++++++++++++++++++++++++++++++++++++++++++++++++++"")\n                    print(""Episode: %i | Steps: %i | Reward: %.3f | Time: %.2f"" % (i,counter,rew,time_space))\n                    print(""Maximum Reward: %.2f   on Episode: %i | Times win: %i"" % (max_reward, episode_max,w))\n\n                if i % 100==0:\n                    print(""Mean reward of the past 100 episodes: "", str(np.mean(rewards_over_time[-100:])))\n                    mean_100.append(np.mean(rewards_over_time[-100:]))\n                    f = open(\'results.txt\',\'a\')\n                    f.write(\'\\n\' + str(np.mean(rewards_over_time[-100:])))\n                    f.close()\n\n                if i % 100 <= 20 and rew >= -1.0:\n                    print(\'Trainining Good Episode.. >_\')\n                    training_time = agent.TRAIN(counter)\n                    print(\'Training Time %.2f\' % training_time)\n\n                if traj >= BATCH:\n                    print(\'Training... >_\')\n                    training_time = agent.TRAIN(BATCH)\n                    #agent.deck.clear()\n                    traj = 0\n                    agent.Clear()\n                    print(\'Training Time %.2f\' % training_time)\n                break\n            # END_IF\n        # END_WHILE\n    # EndFor EPISODES \n    np.save(""rewards_over_time"", rewards_over_time)\n    np.save(""mean100"", mean_100)\n\n    plt.figure(figsize=(10,8))\n    plt.plot(epsilon)\n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Epsilon value"")\n    plt.title(""Epsilon Vs Episodes"")\n    plt.savefig(""Epsilon.png"")\n\n    plt.figure(figsize=(10,8))\n    plt.plot(rewards_over_time, label=""Rewards"")\n    plt.plot(rew_mean, label=""Mean"")\n    plt.xlabel(""Episodes"")\n    plt.ylabel(""Rewards"")\n    plt.title(""Rewards per Episode"")\n    plt.legend(loc=0)\n    plt.savefig(""Rewards.png"")\n\n    plt.figure(figsize=(10,8))\n    plt.plot(mean_100)\n    plt.xlabel(""100_episodes"")\n    plt.ylabel  (""Mean_value"")\n    plt.title(\'Average Reward per 100 episodes\')\n    plt.savefig(""mean_100.png"")\n\n                                                                                                                                                        \n'"
PPO/external_PPO/keras_ppo.py,15,"b'# Initial framework taken from https://github.com/jaara/AI-blog/blob/master/CartPole-A3C.py\nimport pandas\nimport numpy as np\nimport gym\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout\nfrom keras import backend as K\nfrom keras.optimizers import Adam\n\nimport numba as nb\nfrom tensorboardX import SummaryWriter\n\nENV = \'BipedalWalker-v2\'\nCONTINUOUS = True\n\nEPISODES = 10000\n\nLOSS_CLIPPING = 0.2 # Only implemented clipping for the surrogate loss, paper said it was best\nEPOCHS = 10\nNOISE = 1.0\n\nGAMMA = 0.99\n\nBATCH_SIZE = 512\nNUM_ACTIONS = 4\nNUM_STATE = 24\nHIDDEN_SIZE = 256\nENTROPY_LOSS = 5 * 1e-3 # Does not converge without entropy penalty\nLR = 1e-4 # Lower lr stabilises training greatly\n\nDUMMY_ACTION, DUMMY_VALUE = np.zeros((1, NUM_ACTIONS)), np.zeros((1, 1))\n\n\n@nb.jit\ndef exponential_average(old, new, b1):\n    return old * b1 + (1-b1) * new\n\n\ndef proximal_policy_optimization_loss(advantage, old_prediction):\n    def loss(y_true, y_pred):\n        prob = K.sum(y_true * y_pred)\n        old_prob = K.sum(y_true * old_prediction)\n        r = prob/(old_prob + 1e-10)\n\n        return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage)) + ENTROPY_LOSS * (prob * K.log(prob + 1e-10))\n    return loss\n\n\ndef proximal_policy_optimization_loss_continuous(advantage, old_prediction):\n    def loss(y_true, y_pred):\n        var = K.square(NOISE)\n        pi = 3.1415926\n        denom = K.sqrt(2 * pi * var)\n        prob_num = K.exp(- K.square(y_true - y_pred) / (2 * var))\n        old_prob_num = K.exp(- K.square(y_true - old_prediction) / (2 * var))\n\n        prob = prob_num/denom\n        old_prob = old_prob_num/denom\n        r = prob/(old_prob + 1e-10)\n\n        return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage))\n    return loss\n\n\nclass Agent:\n    def __init__(self):\n        self.critic = self.build_critic()\n        if CONTINUOUS is False:\n            self.actor = self.build_actor()\n        else:\n            self.actor = self.build_actor_continuous()\n\n        self.env = gym.make(ENV)\n        print(self.env.action_space, \'action_space\', self.env.observation_space, \'observation_space\')\n        self.episode = 0\n        self.observation = self.env.reset()\n        self.val = False\n        self.reward = []\n        self.reward_over_time = []\n        self.rews = []\n        self.mean_100 = []\n        self.writer = SummaryWriter(\'AllRuns/continuous/lunar_lander_v2_no_dropout\')\n        self.gradient_steps = 0\n\n    def build_actor(self):\n\n        state_input = Input(shape=(NUM_STATE,))\n        advantage = Input(shape=(1,))\n        old_prediction = Input(shape=(NUM_ACTIONS,))\n\n        x = Dense(HIDDEN_SIZE, activation=\'relu\')(state_input)\n        x = Dropout(0.5)(x)\n        x = Dense(HIDDEN_SIZE, activation=\'relu\')(x)\n        x = Dropout(0.5)(x)\n\n        out_actions = Dense(NUM_ACTIONS, activation=\'softmax\', name=\'output\')(x)\n\n        model = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n        model.compile(optimizer=Adam(lr=LR),\n                      loss=[proximal_policy_optimization_loss(\n                          advantage=advantage,\n                          old_prediction=old_prediction)])\n        model.summary()\n\n        return model\n\n    def build_actor_continuous(self):\n        state_input = Input(shape=(NUM_STATE,))\n        advantage = Input(shape=(1,))\n        old_prediction = Input(shape=(NUM_ACTIONS,))\n\n        x = Dense(HIDDEN_SIZE, activation=\'relu\')(state_input)\n        x = Dense(HIDDEN_SIZE, activation=\'relu\')(x)\n\n        out_actions = Dense(NUM_ACTIONS, name=\'output\', activation=\'tanh\')(x)\n\n        model = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n        model.compile(optimizer=Adam(lr=LR),\n                      loss=[proximal_policy_optimization_loss_continuous(\n                          advantage=advantage,\n                          old_prediction=old_prediction)])\n        model.summary()\n\n        return model\n\n    def build_critic(self):\n\n        state_input = Input(shape=(NUM_STATE,))\n        x = Dense(HIDDEN_SIZE, activation=\'relu\')(state_input)\n        x = Dropout(0.5)(x)\n        x = Dense(HIDDEN_SIZE, activation=\'relu\')(x)\n        x = Dropout(0.5)(x)\n\n        out_value = Dense(1)(x)\n\n        model = Model(inputs=[state_input], outputs=[out_value])\n        model.compile(optimizer=Adam(lr=LR), loss=\'mse\')\n\n        return model\n\n    def reset_env(self):\n        self.episode += 1\n        if self.episode % 100 == 0:\n            self.val = True\n        else:\n            self.val = False\n        self.observation = self.env.reset()\n        self.reward = []\n\n    def get_action(self):\n        p = self.actor.predict([self.observation.reshape(1, NUM_STATE), DUMMY_VALUE, DUMMY_ACTION])\n        if self.val is False:\n            action = np.random.choice(NUM_ACTIONS, p=np.nan_to_num(p[0]))\n        else:\n            action = np.argmax(np.nan_to_num(p[0]))\n        action_matrix = np.zeros(NUM_ACTIONS)\n        action_matrix[action] = 1\n        return action, action_matrix, p\n\n    def get_action_continuous(self):\n        p = self.actor.predict([self.observation.reshape(1, NUM_STATE), DUMMY_VALUE, DUMMY_ACTION])\n        if self.val is False:\n            action = action_matrix = p[0] + np.random.normal(loc=0, scale=NOISE, size=p[0].shape)\n        else:\n            action = action_matrix = p[0]\n        return action, action_matrix, p\n\n    def transform_reward(self):\n        if self.val is True:\n            self.writer.add_scalar(\'Val episode reward\', np.array(self.reward).sum(), self.episode)\n        else:\n            self.writer.add_scalar(\'Episode reward\', np.array(self.reward).sum(), self.episode)\n        for j in range(len(self.reward) - 2, -1, -1):\n            self.reward[j] += self.reward[j + 1] * GAMMA\n\n    def get_batch(self):\n        batch = [[], [], [], []]\n        ep_r, ep, lala = 0,0,0\n        tmp_batch = [[], [], []]\n        while len(batch[0]) < BATCH_SIZE:\n            if CONTINUOUS is False:\n                action, action_matrix, predicted_action = self.get_action()\n            else:\n                action, action_matrix, predicted_action = self.get_action_continuous()\n            observation, reward, done, info = self.env.step(action)\n            self.reward.append(reward)\n            ep_r +=reward\n            lala +=reward\n\n            tmp_batch[0].append(self.observation)\n            tmp_batch[1].append(action_matrix)\n            tmp_batch[2].append(predicted_action)\n            self.observation = observation\n\n            if done:\n                ep +=1\n                ep_r = 0\n                self.rews.append(lala)\n                self.transform_reward()\n                for i in range(len(tmp_batch[0])):\n                    obs, action, pred = tmp_batch[0][i], tmp_batch[1][i], tmp_batch[2][i]\n                    r = self.reward[i]\n                    batch[0].append(obs)\n                    batch[1].append(action)\n                    batch[2].append(pred)\n                    batch[3].append(r)\n                tmp_batch = [[], [], []]\n                print(\'Episode: %i | Reward: %.2f\' % (self.episode,lala))\n                if self.episode % 100==0:\n                    print(""Mean reward of the past 100 episodes: "", str(np.mean(self.rews[-100:])))\n                    self.mean_100.append(np.mean(self.rews[-100:]))\n                    f = open(\'results.txt\',\'a\')\n                    f.write(\'\\n\' + str(np.mean(self.rews[-100:])))\n                    f.close()                \n\n                self.reset_env()\n\n        obs, action, pred, reward = np.array(batch[0]), np.array(batch[1]), np.array(batch[2]), np.reshape(np.array(batch[3]), (len(batch[3]), 1))\n        pred = np.reshape(pred, (pred.shape[0], pred.shape[2]))\n        return obs, action, pred, reward\n\n    def run(self):\n        while self.episode < EPISODES:\n            obs, action, pred, reward = self.get_batch()\n            old_prediction = pred\n            pred_values = self.critic.predict(obs)\n\n            advantage = reward - pred_values\n\n            actor_loss = []\n            critic_loss = []\n            for e in range(EPOCHS):\n                actor_loss.append(self.actor.train_on_batch([obs, advantage, old_prediction], [action]))\n                critic_loss.append(self.critic.train_on_batch([obs], [reward]))\n            self.writer.add_scalar(\'Actor loss\', np.mean(actor_loss), self.gradient_steps)\n            self.writer.add_scalar(\'Critic loss\', np.mean(critic_loss), self.gradient_steps)\n\n            self.gradient_steps += 1\n\n\nif __name__ == \'__main__\':\n    ag = Agent()\n    ag.run()\n    np.save(""mean100"", ag.mean_100)\n\n    plt.figure(figsize=(10,8))\n    plt.tick_params(size=5, labelsize=15)\n    plt.plot(ag.mean_100)\n    plt.xlabel(""100_episodes"", fontsize = 13)\n    plt.ylabel(""Mean_value"", fontsize = 13)\n    plt.title(\'Average Reward per 100 episodes\', fontsize=15)\n    plt.savefig(""mean_100.png"")\n\n\n'"
PPO/external_PPO/rl-bipedal.py,19,"b'""""""\nA simple version of Proximal Policy Optimization (PPO) using single thread.\n\nBased on:\n1. Emergence of Locomotion Behaviours in Rich Environments (Google Deepmind): [https://arxiv.org/abs/1707.02286]\n2. Proximal Policy Optimization Algorithms (OpenAI): [https://arxiv.org/abs/1707.06347]\n3. Generalized Advantage Estimation [https://arxiv.org/abs/1506.02438]\n\n""""""\n# DEN DOULEUEI\nimport pandas\nimport tensorflow as tf\nimport numpy as np\nimport gym\nimport os\nimport scipy.signal\nfrom gym import wrappers\nfrom datetime import datetime\nfrom time import time\nfrom utils import RunningStats, discount, add_histogram\nOUTPUT_RESULTS_DIR = ""./""\n\nEP_MAX = 10000\nGAMMA = 0.99\nLAMBDA = 0.95\nENTROPY_BETA = 0.01  # 0.01 for discrete, 0.0 for continuous\nLR = 0.0001\nBATCH = 8192  # 128 for discrete, 8192 for continuous\nMINIBATCH = 32\nEPOCHS = 10\nEPSILON = 0.2\nVF_COEFF = 1.0\nL2_REG = 0.001\nSIGMA_FLOOR = 0.0\n\n# MODEL_RESTORE_PATH = ""/path/to/saved/model""\nMODEL_RESTORE_PATH = None\n\n\nclass PPO:\n    def __init__(self, environment, summary_dir=""./"", gpu=True, greyscale=True):\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n        config = tf.ConfigProto(log_device_placement=False, device_count={\'GPU\': gpu})\n        config.gpu_options.per_process_gpu_memory_fraction = 0.1\n        self.summary_dir = summary_dir\n\n########## Arrange the environment\n        self.discrete = False\n        self.s_dim, self.a_dim = environment.observation_space.shape, environment.action_space.shape[0]\n        self.a_bound = (environment.action_space.high - environment.action_space.low) / 2\n        self.actions = tf.placeholder(tf.float32, [None, self.a_dim], \'action\')\n########## Open tensors for data\n        self.sess = tf.Session(config=config)\n        self.state = tf.placeholder(tf.float32, [None, self.s_dim[0]], \'state\')\n        self.advantage = tf.placeholder(tf.float32, [None, 1], \'advantage\')\n        self.rewards = tf.placeholder(tf.float32, [None, 1], \'discounted_r\')\n########## Build synthetic q\n        self.dataset = tf.data.Dataset.from_tensor_slices({""state"": self.state, ""actions"": self.actions,\n                                                           ""rewards"": self.rewards, ""advantage"": self.advantage})\n        self.dataset = self.dataset.shuffle(buffer_size=10000)\n        self.dataset = self.dataset.batch(MINIBATCH)\n        self.dataset = self.dataset.cache()\n        self.dataset = self.dataset.repeat(EPOCHS)\n        self.iterator = self.dataset.make_initializable_iterator()\n        batch = self.iterator.get_next()\n########## Call the nets\n        pi_old, pi_old_params = self._build_anet(batch[""state""], \'oldpi\')\n        pi, pi_params = self._build_anet(batch[""state""], \'pi\')\n        pi_eval, _ = self._build_anet(self.state, \'pi\', reuse=True)\n        #\n        vf_old, vf_old_params = self._build_cnet(batch[""state""], ""oldvf"")\n        self.v, vf_params = self._build_cnet(batch[""state""], ""vf"")\n        self.vf_eval, _ = self._build_cnet(self.state, \'vf\', reuse=True)\n\n        self.sample_op = tf.squeeze(pi_eval.sample(1), axis=0, name=""sample_action"")\n        self.eval_action = pi_eval.mode()  # Used mode for discrete case. Mode should equal mean in continuous\n        self.global_step = tf.train.get_or_create_global_step()\n        self.saver = tf.train.Saver()\n################ LOSS\n        with tf.variable_scope(\'loss\'):\n            epsilon_decay = tf.train.polynomial_decay(EPSILON, self.global_step, 1e5, 0.01, power=0.0)\n\n            with tf.variable_scope(\'policy\'):\n                # Use floor functions for the probabilities to prevent NaNs when prob = 0\n                ratio = tf.maximum(pi.prob(batch[""actions""]), 1e-6) / tf.maximum(pi_old.prob(batch[""actions""]), 1e-6)\n                ratio = tf.clip_by_value(ratio, 0, 10)\n                surr1 = batch[""advantage""] * ratio\n                surr2 = batch[""advantage""] * tf.clip_by_value(ratio, 1 - epsilon_decay, 1 + epsilon_decay)\n                loss_pi = -tf.reduce_mean(tf.minimum(surr1, surr2))\n                tf.summary.scalar(""loss"", loss_pi)\n\n            with tf.variable_scope(\'value_function\'):\n                # Sometimes values clipping helps, sometimes just using raw residuals is better \xc2\xaf\\_(\xe3\x83\x84)_/\xc2\xaf\n                clipped_value_estimate = vf_old + tf.clip_by_value(self.v - vf_old, -epsilon_decay, epsilon_decay)\n                loss_vf1 = tf.squared_difference(clipped_value_estimate, batch[""rewards""])\n                loss_vf2 = tf.squared_difference(self.v, batch[""rewards""])\n                loss_vf = tf.reduce_mean(tf.maximum(loss_vf1, loss_vf2)) * 0.5\n                # loss_vf = tf.reduce_mean(tf.square(self.v - batch[""rewards""])) * 0.5\n                tf.summary.scalar(""loss"", loss_vf)\n\n            with tf.variable_scope(\'entropy\'):\n                entropy = pi.entropy()\n                pol_entpen = -ENTROPY_BETA * tf.reduce_mean(entropy)\n\n            loss = loss_pi + loss_vf * VF_COEFF + pol_entpen\n            tf.summary.scalar(""total"", loss)\n            # tf.summary.scalar(""epsilon"", epsilon_decay)\n\n############### TRAIN\n        with tf.variable_scope(\'train\'):\n            opt = tf.train.AdamOptimizer(LR)\n            self.train_op = opt.minimize(loss, global_step=self.global_step, var_list=pi_params + vf_params)\n\n############### Update thetas\n        with tf.variable_scope(\'update_old\'):\n            self.update_pi_old_op = [oldp.assign(p) for p, oldp in zip(pi_params, pi_old_params)]\n            self.update_vf_old_op = [oldp.assign(p) for p, oldp in zip(vf_params, vf_old_params)]\n\n############### TensorBoard\n        self.writer = tf.summary.FileWriter(self.summary_dir, self.sess.graph)\n        self.sess.run(tf.global_variables_initializer())\n\n        tf.summary.scalar(""value"", tf.reduce_mean(self.v))\n        tf.summary.scalar(""policy_entropy"", tf.reduce_mean(entropy))\n        if not self.discrete:\n            tf.summary.scalar(""sigma"", tf.reduce_mean(pi.stddev()))\n        self.summarise = tf.summary.merge(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n############# Models\n    def _build_anet(self, state_in, name, reuse=False):\n        w_reg = tf.contrib.layers.l2_regularizer(L2_REG)\n\n        with tf.variable_scope(name, reuse=reuse):\n\n            layer_1 = tf.layers.dense(state_in, 400, tf.nn.relu, kernel_regularizer=w_reg, name=""pi_l1"")\n            layer_2 = tf.layers.dense(layer_1, 400, tf.nn.relu, kernel_regularizer=w_reg, name=""pi_l2"")\n            mu = tf.layers.dense(layer_2, self.a_dim, tf.nn.tanh, kernel_regularizer=w_reg, name=""pi_mu"")\n            \n            log_sigma = tf.get_variable(name=""pi_sigma"", shape=self.a_dim, initializer=tf.zeros_initializer())\n            dist = tf.distributions.Normal(loc=mu * self.a_bound, scale=tf.maximum(tf.exp(log_sigma), SIGMA_FLOOR))\n        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)\n        return dist, params\n\n    def _build_cnet(self, state_in, name, reuse=False):\n        w_reg = tf.contrib.layers.l2_regularizer(L2_REG)\n\n        with tf.variable_scope(name, reuse=reuse):\n\n            l1 = tf.layers.dense(state_in, 400, tf.nn.relu, kernel_regularizer=w_reg, name=""vf_l1"")\n            l2 = tf.layers.dense(l1, 400, tf.nn.relu, kernel_regularizer=w_reg, name=""vf_l2"")\n            vf = tf.layers.dense(l2, 1, kernel_regularizer=w_reg, name=""vf_output"")\n\n        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)\n        return vf, params\n\n########## Update function    \n    def update(self, s, a, r, adv):\n        start = time()\n        e_time = []\n\n        self.sess.run([self.update_pi_old_op, self.update_vf_old_op, self.iterator.initializer],\n                      feed_dict={self.state: s, self.actions: a, self.rewards: r, self.advantage: adv})\n\n        while True:\n            try:\n                e_start = time()\n                summary, step, _ = self.sess.run([self.summarise, self.global_step, self.train_op])\n                e_time.append(time() - e_start)\n            except tf.errors.OutOfRangeError:\n                break\n        print(""Trained in %.3fs. Average %.3fs/batch. Global step %i"" % (time() - start, np.mean(e_time), step))\n        return summary\n\n    def save_model(self, model_path, step=None):\n        save_path = self.saver.save(self.sess, os.path.join(model_path, ""model.ckpt""), global_step=step)\n        return save_path\n\n    def restore_model(self, model_path):\n        self.saver.restore(self.sess, os.path.join(model_path, ""model.ckpt""))\n        print(""Model restored from"", model_path)\n\n    def evaluate_state(self, state, stochastic=True):\n        if stochastic:\n            action, value = self.sess.run([self.sample_op, self.vf_eval],\\\n             {self.state: state[np.newaxis, :]})\n        else:\n            action, value = self.sess.run([self.eval_action, self.vf_eval],\\\n             {self.state: state[np.newaxis, :]})\n        return action[0], np.squeeze(value)\n\n\nif __name__ == \'__main__\':\n    # Discrete environments\n    # ENVIRONMENT = \'CartPole-v1\'\n    # ENVIRONMENT = \'MountainCar-v0\'\n    # ENVIRONMENT = \'LunarLander-v2\'\n    # ENVIRONMENT = \'Pong-v0\'\n\n    # Continuous environments\n    # ENVIRONMENT = \'Pendulum-v0\'\n    # ENVIRONMENT = \'MountainCarContinuous-v0\'\n    # ENVIRONMENT = \'LunarLanderContinuous-v2\'\n    ENVIRONMENT = \'BipedalWalker-v2\'\n    # ENVIRONMENT = \'BipedalWalkerHardcore-v2\'\n    # ENVIRONMENT = \'CarRacing-v0\'\n\n    TIMESTAMP = datetime.now().strftime(""%Y%m%d-%H%M%S"")\n    SUMMARY_DIR = os.path.join(OUTPUT_RESULTS_DIR, ""PPO"", ENVIRONMENT, TIMESTAMP)\n\n    env = gym.make(ENVIRONMENT)\n    #env = wrappers.Monitor(env, os.path.join(SUMMARY_DIR, ENVIRONMENT), video_callable=None)\n    ppo = PPO(env, SUMMARY_DIR, gpu=False)\n    rew_list = []\n    mean_100 = []\n    if MODEL_RESTORE_PATH is not None:\n        ppo.restore_model(MODEL_RESTORE_PATH)\n\n    t, terminal = 0, False\n    buffer_s, buffer_a, buffer_r, buffer_v, buffer_terminal = [], [], [], [], []\n    rolling_r = RunningStats()\n\n    for episode in range(EP_MAX + 1):\n\n        s = env.reset()\n        ep_r, ep_t, ep_a = 0, 0, []\n\n        while True:\n            a, v = ppo.evaluate_state(s)\n\n            # Update ppo\n            if t == BATCH:  # or (terminal and t < BATCH):\n                # Normalise rewards\n                rewards = np.array(buffer_r)\n                rolling_r.update(rewards)\n                rewards = np.clip(rewards / rolling_r.std, -10, 10)\n\n                v_final = [v * (1 - terminal)]  # v = 0 if terminal, otherwise use the predicted v\n                values = np.array(buffer_v + v_final)\n                terminals = np.array(buffer_terminal + [terminal])\n\n                # Generalized Advantage Estimation - https://arxiv.org/abs/1506.02438\n                delta = rewards + GAMMA * values[1:] * (1 - terminals[1:]) - values[:-1]\n                advantage = discount(delta, GAMMA * LAMBDA, terminals)\n                returns = advantage + np.array(buffer_v)\n                advantage = (advantage - advantage.mean()) / np.maximum(advantage.std(), 1e-6)\n\n                bs, ba, br, badv = np.reshape(buffer_s, (t,) + ppo.s_dim), np.vstack(buffer_a), \\\n                                   np.vstack(returns), np.vstack(advantage)\n\n                graph_summary = ppo.update(bs, ba, br, badv)\n                buffer_s, buffer_a, buffer_r, buffer_v, buffer_terminal = [], [], [], [], []\n                t = 0\n\n            buffer_s.append(s)\n            buffer_a.append(a)\n            buffer_v.append(v)\n            buffer_terminal.append(terminal)\n            ep_a.append(a)\n\n            if not ppo.discrete:\n                a = np.clip(a, env.action_space.low, env.action_space.high)\n            s, r, terminal, _ = env.step(a)\n            buffer_r.append(r)\n\n            ep_r += r\n            ep_t += 1\n            t += 1\n\n            if terminal:\n                print(\'Episode: %i\' % episode, ""| Reward: %.2f"" % ep_r, \'| Steps: %i\' % ep_t)\n                rew_list.append(ep_r)\n                if episode % 100==0:\n                    print(""Mean reward of the past 100 episodes: "", str(np.mean(rew_list[-100:])))\n                    mean_100.append(np.mean(rew_list[-100:]))\n                    f = open(\'results.txt\',\'a\')\n                    f.write(\'\\n\' + str(np.mean(rew_list[-100:])))\n                    f.close()                \n\n\n                # End of episode summary\n                worker_summary = tf.Summary()\n                worker_summary.value.add(tag=""Reward"", simple_value=ep_r)\n\n                # Create Action histograms for each dimension\n                actions = np.array(ep_a)\n                if ppo.discrete:\n                    add_histogram(ppo.writer, ""Action"", actions, episode, bins=ppo.a_dim)\n                else:\n                    for a in range(ppo.a_dim):\n                        add_histogram(ppo.writer, ""Action/Dim"" + str(a), actions[:, a], episode)\n\n                try:\n                    ppo.writer.add_summary(graph_summary, episode)\n                except NameError:\n                    pass\n                ppo.writer.add_summary(worker_summary, episode)\n                ppo.writer.flush()\n\n                # Save the model\n                if episode % 100 == 0 and episode > 0:\n                    path = ppo.save_model(SUMMARY_DIR, episode)\n                    \n                    print(\'Saved model at episode\', episode, \'in\', path)\n\n                break\n#########################################################\n    env.close()\n    np.save(""mean100"", mean_100) \n    plt.figure(figsize=(10,8))\n    plt.tick_params(size=5, labelsize=15)\n    plt.plot(mean_100)\n    plt.xlabel(""100_episodes"", fontsize = 13)\n    plt.ylabel(""Mean_value"", fontsize = 13)\n    plt.title(\'Average Reward per 100 episodes\', fontsize=15)\n    plt.savefig(""mean_100.png"")  \n\n    # Run trained policy\n    print(\'Run the trained policy\\n\')\n    env = gym.make(ENVIRONMENT)\n    #env = wrappers.Monitor(env, os.path.join(SUMMARY_DIR, ENVIRONMENT + ""_trained""), video_callable=None)\n    while True:\n        s = env.reset()\n        ep_r, ep_t = 0, 0\n        while True:\n            #env.render()\n            a, v = ppo.evaluate_state(s, stochastic=False)\n            if not ppo.discrete:\n                a = np.clip(a, env.action_space.low, env.action_space.high)\n            s, r, terminal, _ = env.step(a)\n            ep_r += r\n            ep_t += 1\n            if terminal:\n                f=open(\'ResultsTest.txt\', \'a\')\n                print(""Reward: %.2f"" % ep_r, \'| Steps: %i\' % ep_t)\n                f.write(""Reward: "" +str(ep_r), \'| Steps: \' + str(ep_r) + \'\\n\')\n                f.close()\n                break\n'"
PPO/external_PPO/utils.py,16,"b'import pandas\nimport numpy as np\nimport tensorflow as tf\nimport scipy.signal\n\n\ndef add_histogram(writer, tag, values, step, bins=1000):\n    """"""\n    Logs the histogram of a list/vector of values.\n    From: https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n    """"""\n\n    # Create histogram using numpy\n    counts, bin_edges = np.histogram(values, bins=bins)\n\n    # Fill fields of histogram proto\n    hist = tf.HistogramProto()\n    hist.min = float(np.min(values))\n    hist.max = float(np.max(values))\n    hist.num = int(np.prod(values.shape))\n    hist.sum = float(np.sum(values))\n    hist.sum_squares = float(np.sum(values ** 2))\n\n    # Requires equal number as bins, where the first goes from -DBL_MAX to bin_edges[1]\n    # See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/summary.proto#L30\n    # Therefore we drop the start of the first bin\n    bin_edges = bin_edges[1:]\n\n    # Add bin edges and counts\n    for edge in bin_edges:\n        hist.bucket_limit.append(edge)\n    for c in counts:\n        hist.bucket.append(c)\n\n    # Create and write Summary\n    summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n    writer.add_summary(summary, step)\n\n\ndef discount(x, gamma, terminal_array=None):\n    if terminal_array is None:\n        return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n    else:\n        y, adv = 0, []\n        terminals_reversed = terminal_array[1:][::-1]\n        for step, dt in enumerate(reversed(x)):\n            y = dt + gamma * y * (1 - terminals_reversed[step])\n            adv.append(y)\n        return np.array(adv)[::-1]\n\n\nclass RunningStats(object):\n    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n    # https://github.com/openai/baselines/blob/master/baselines/common/running_mean_std.py\n    def __init__(self, epsilon=1e-4, shape=()):\n        self.mean = np.zeros(shape, \'float64\')\n        self.var = np.ones(shape, \'float64\')\n        self.std = np.ones(shape, \'float64\')\n        self.count = epsilon\n\n    def update(self, x):\n        batch_mean = np.mean(x, axis=0)\n        batch_var = np.var(x, axis=0)\n        batch_count = x.shape[0]\n        self.update_from_moments(batch_mean, batch_var, batch_count)\n\n    def update_from_moments(self, batch_mean, batch_var, batch_count):\n        delta = batch_mean - self.mean\n        new_mean = self.mean + delta * batch_count / (self.count + batch_count)\n        m_a = self.var * self.count\n        m_b = batch_var * batch_count\n        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n        new_var = M2 / (self.count + batch_count)\n\n        self.mean = new_mean\n        self.var = new_var\n        self.std = np.maximum(np.sqrt(self.var), 1e-6)\n        self.count = batch_count + self.count\n\n\ndef lstm_state_combine(state):\n    return np.reshape([s[0] for s in state], (len(state), -1)), \\\n           np.reshape([s[1] for s in state], (len(state), -1))\n'"
