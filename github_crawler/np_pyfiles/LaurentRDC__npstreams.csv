file_path,api_count,code
setup.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport re\nfrom glob import glob\nfrom unittest import TestLoader\n\nfrom setuptools import Extension, find_packages, setup\n\n# from Cython.Build import cythonize\n# import numpy\n\n# To upload to pypi.org:\n#   >>> python setup.py sdist\n#   >>> twine upload dist/npstreams-x.x.x.tar.gz\n\nBASE_PACKAGE = ""npstreams""\n\nbase_path = os.path.dirname(__file__)\nwith open(os.path.join(base_path, ""npstreams"", ""__init__.py"")) as f:\n    module_content = f.read()\n    VERSION = (\n        re.compile(r"".*__version__ = \\""(.*?)\\"""", re.S).match(module_content).group(1)\n    )\n    LICENSE = (\n        re.compile(r"".*__license__ = \\""(.*?)\\"""", re.S).match(module_content).group(1)\n    )\n\n\nwith open(""README.md"") as f:\n    readme = f.read()\n\nwith open(""requirements.txt"") as f:\n    requirements = [line for line in f.read().split(""\\n"") if len(line.strip())]\n\nexclude = {""exclude"": [""external*"", ""docs"", ""tests"", ""*cache""]}\npackages = [\n    BASE_PACKAGE + ""."" + x\n    for x in find_packages(os.path.join(base_path, BASE_PACKAGE), **exclude)\n]\nif BASE_PACKAGE not in packages:\n    packages.append(BASE_PACKAGE)\n\n\ndef test_suite():\n    return TestLoader().discover(""."")\n\n\nif __name__ == ""__main__"":\n    setup(\n        name=""npstreams"",\n        description=""Streaming operations on NumPy arrays"",\n        long_description=readme,\n        license=LICENSE,\n        url="""",\n        download_url=""http://github.com/LaurentRDC/npstreams"",\n        version=VERSION,\n        author=""Laurent P. Ren\xc3\xa9 de Cotret"",\n        author_email=""laurent.renedecotret@mail.mcgill.ca"",\n        maintainer=""Laurent P. Ren\xc3\xa9 de Cotret"",\n        maintainer_email=""laurent.renedecotret@mail.mcgill.ca"",\n        install_requires=requirements,\n        keywords=[""streaming"", ""numpy"", ""math""],\n        packages=packages,\n        include_package_data=True,\n        python_requires="">=3.6"",\n        zip_safe=False,\n        #        include_dirs = [numpy.get_include()],\n        #        ext_modules = cythonize(""npstreams/*.pyx"",\n        #                                 compiler_directives = {\'language_level\':3,\n        #                                                        \'boundscheck\': False}),\n        test_suite=""setup.test_suite"",\n        classifiers=[\n            ""Environment :: Console"",\n            ""Intended Audience :: Science/Research"",\n            ""Topic :: Scientific/Engineering"",\n            ""License :: OSI Approved :: BSD License"",\n            ""Natural Language :: English"",\n            ""Operating System :: OS Independent"",\n            ""Programming Language :: Python"",\n            ""Programming Language :: Python :: 3.6"",\n            ""Programming Language :: Python :: 3.7"",\n            ""Programming Language :: Python :: 3.8"",\n        ],\n    )\n'"
docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\ncurrentpath = os.path.dirname(__file__)\nsys.path.append(os.path.join(currentpath, ""..""))\n\nimport npstreams\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.5\'\nfrom datetime import datetime\nimport alabaster\n\nyear = datetime.now().year\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    ""alabaster"",\n    ""sphinx.ext.todo"",\n    ""sphinx.ext.intersphinx"",\n    ""sphinx.ext.autosummary"",\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.napoleon"",\n    ""sphinx.ext.mathjax"",\n]\n\nintersphinx_mapping = {""numpy"": (""http://docs.scipy.org/doc/numpy/"", None)}\n\nnapoleon_google_docstring = False\nautosummary_generate = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = "".rst""\n\n# The master toctree document.\nmaster_doc = ""index""\n\n# Releases changelog extension\nreleases_release_uri = ""https://github.com/LaurentRDC/npstreams/tree/%s""\nreleases_issue_uri = ""https://github.com/LaurentRDC/npstreams/issues/%s""\n\n# General information about the project.\nproject = ""npstreams""\ncopyright = ""%d Laurent P. Ren\xc3\xa9 de Cotret"" % year\nauthor = ""Laurent P. Ren\xc3\xa9 de Cotret""\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = npstreams.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = version\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\nexclude_trees = [""_build""]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = ""sphinx""\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_path = [""_themes""]\nhtml_sidebars = {\n    ""**"": [\n        ""about.html"",\n        ""navigation.html"",\n        ""searchbox.html"",\n        ""localtoc.html"",\n        ""sourcelink.html"",\n    ]\n}\n# html_show_sourcelink = True\n\n# Everything intersphinx\'s to Python.\nintersphinx_mapping = {""python"": (""https://docs.python.org/3.6"", None)}\n\n# Autodoc settings\nautodoc_default_flags = [""members"", ""special-members""]\nautoclass_content = ""both""\n\n\ndef autodoc_skip_member(app, what, name, obj, skip, options):\n    exclusions = {""__weakref__"", ""__doc__"", ""__module__"", ""__dict__""}\n    exclude = name in exclusions\n    return skip or exclude\n\n\ndef setup(app):\n    app.connect(""autodoc-skip-member"", autodoc_skip_member)\n\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = []\n\n# Suppress the warning about a non-local URI for status shields.\nsuppress_warnings = [""image.nonlocal_uri""]\n\n# Enable releases \'unstable prehistory\' mode.\nreleases_unstable_prehistory = True\n'"
npstreams/__init__.py,0,"b'# -*- coding: utf-8 -*-\n__author__ = ""Laurent P. Ren\xc3\xa9 de Cotret""\n__email__ = ""laurent.renedecotret@mail.mcgill.ca""\n__license__ = ""BSD""\n__version__ = ""1.6.2""\n\n# Order of import is important\n# because of inter-dependency\nfrom .utils import deprecated\n\nfrom .benchmarks import benchmark\nfrom .array_stream import array_stream, ArrayStream\nfrom .array_utils import nan_to_num\nfrom .linalg import idot, itensordot, ieinsum, iinner\nfrom .parallel import pmap, pmap_unordered, preduce\nfrom .flow import ipipe, iload, pload\nfrom .iter_utils import (\n    cyclic,\n    last,\n    chunked,\n    multilinspace,\n    linspace,\n    peek,\n    itercopy,\n    primed,\n    length_hint,\n)\nfrom .reduce import ireduce_ufunc, preduce_ufunc, reduce_ufunc\nfrom .stacking import stack\nfrom .stats import (\n    iaverage,\n    average,\n    imean,\n    mean,\n    istd,\n    std,\n    ivar,\n    var,\n    isem,\n    sem,\n    average_and_var,\n    ihistogram,\n)\nfrom .numerics import isum, sum, iprod, prod, isub, iall, iany, imax, imin\n'"
npstreams/array_stream.py,2,"b'# -*- coding: utf-8 -*-\n\nfrom collections.abc import Iterator\nfrom functools import wraps\n\nimport numpy as np\nfrom numpy import asanyarray\n\nfrom .iter_utils import length_hint, peek\n\n\nclass ArrayStream(Iterator):\n    """""" \n    Iterator of arrays. Elements from the stream are converted to \n    NumPy arrays. If ``stream`` is a single array, it will be \n    repackaged as a length 1 iterable.\n\n    Arrays in the stream will be cast to the same data-type as the first\n    array in the stream. The stream data-type is located in the `dtype` attribute.\n\n    .. versionadded:: 1.5.2\n    """"""\n\n    def __init__(self, stream):\n        if isinstance(stream, np.ndarray):\n            stream = (stream,)\n\n        self._sequence_length = length_hint(stream, default=NotImplemented)\n\n        # Once length_hint has been determined, we can peek into the stream\n        first, stream = peek(stream)\n        self._iterator = iter(stream)\n\n        first = asanyarray(first)\n        self.dtype = first.dtype\n\n    def __repr__(self):\n        """""" Verbose string representation """"""\n        representation = f""< {self.__class__.__name__} object""\n        representation += f"" of data-type {self.dtype}""\n\n        if not (self._sequence_length is NotImplemented):\n            representation += f"" and a sequence length of {self._sequence_length}""\n        else:\n            representation += "" of unknown length""\n\n        return representation + "" >""\n\n    def __array__(self):\n        """""" Returns a dense array created from this stream. """"""\n        # As of numpy version 1.14, arrays are expanded into a list before contatenation\n        # Therefore, it\'s ok to build that list first\n        arraylist = list(self)\n        return np.stack(arraylist, axis=-1)\n\n    def __length_hint__(self):\n        """""" \n        In certain cases, an ArrayStream can have a definite size. \n        See https://www.python.org/dev/peps/pep-0424/ \n        """"""\n        return self._sequence_length\n\n    def __next__(self):\n        n = self._iterator.__next__()\n        return asanyarray(n, dtype=self.dtype)\n\n\ndef array_stream(func):\n    """""" \n    Decorates streaming functions to make sure that the stream\n    is a stream of ndarrays. Objects that are not arrays are transformed \n    into arrays. If the stream is in fact a single ndarray, this ndarray \n    is repackaged into a sequence of length 1.\n\n    The first argument of the decorated function is assumed to be an iterable of\n    arrays, or an iterable of objects that can be casted to arrays.\n\n    Note that using this decorator also ensures that the stream is only wrapped once\n    by the conversion function.\n    """"""\n\n    @wraps(func)\n    def decorated(arrays, *args, **kwargs):\n        if isinstance(arrays, ArrayStream):\n            return func(arrays, *args, **kwargs)\n        return func(ArrayStream(arrays), *args, **kwargs)\n\n    return decorated\n'"
npstreams/array_utils.py,4,"b'# -*- coding: utf-8 -*-\n""""""\nArray utilities \n---------------\n""""""\nimport numpy as np\n\n\ndef nan_to_num(array, fill_value=0.0, copy=True):\n    """"""\n    Replace NaNs with another fill value. \n\n    Parameters\n    ----------\n    array : array_like\n        Input data.\n    fill_value : float, optional\n        NaNs will be replaced by ``fill_value``. Default is 0.0, in keeping\n        with ``numpy.nan_to_num``.\n    copy : bool, optional\n        Whether to create a copy of `array` (True) or to replace values\n        in-place (False). The in-place operation only occurs if\n        casting to an array does not require a copy.\n    \n    Returns\n    -------\n    out : ndarray\n        Array without NaNs. If ``array`` was not of floating or complearray type,\n        ``array`` is returned unchanged.\n    \n    Notes\n    -----\n    Contrary to ``numpy.nan_to_num``, this functions does not handle\n    infinite values.\n\n    See Also\n    --------\n    numpy.nan_to_num : replace NaNs and Infs with zeroes.\n    """"""\n    array = np.array(array, subok=True, copy=copy)\n    dtype = array.dtype.type\n\n    # Non-inexact types do not have NaNs\n    if not np.issubdtype(dtype, np.inexact):\n        return array\n\n    iscomplex = np.issubdtype(dtype, np.complexfloating)\n    dest = (array.real, array.imag) if iscomplex else (array,)\n    for d in dest:\n        np.copyto(d, fill_value, where=np.isnan(d))\n    return array\n'"
npstreams/benchmarks.py,11,"b'# -*- coding: utf-8 -*-\n"""""" \nReliably benchmarking npstreams performance.\n""""""\nimport inspect\nimport sys\nimport timeit\nfrom collections import namedtuple\nfrom contextlib import redirect_stdout\nfrom functools import partial\nfrom shutil import get_terminal_size\n\nimport numpy as np\n\nfrom . import __version__\nfrom .reduce import _check_binary_ufunc\n\nUFUNC_SETUP = """"""\nfrom npstreams import reduce_ufunc, stack\nimport numpy as np\nfrom numpy import {ufunc.__name__}\n\nnp.random.seed(42056)\n\ndef stream():\n    return (np.random.random({shape}) for _ in range(10))\n""""""\n\nFUNC_SETUP = """"""\nfrom npstreams import stack\nimport numpy as np\nfrom numpy     import {func.__name__} as np_{func.__name__}\nfrom npstreams import {func.__name__} as ns_{func.__name__}\n\nnp.random.seed(42056)\n\ndef stream():\n    return (np.random.random({shape}) for _ in range(10))\n""""""\n\nBenchmarkResults = namedtuple(\n    ""BenchmarkResults"", field_names=[""numpy_time"", ""npstreams_time"", ""shape""]\n)\n\n\ndef autotimeit(statement, setup=""pass"", repeat=3):\n    """""" \n    Time a statement, automatically determining the number of times to\n    run the statement so that the total excecution time is not too short. \n\n    .. versionadded:: 1.5.2\n    \n    Parameters\n    ----------\n    statement: string\n        Statement to time. The statement will be executed after the `setup` statement.\n    setup : string, optional\n        Setup statement executed before timing starts. \n    repeat : int, optional\n        Number of repeated timing to execute.\n    \n    Returns\n    -------\n    time : float\n        Minimal time per execution of `statement` [seconds].\n    """"""\n    timer = timeit.Timer(stmt=statement, setup=setup)\n    number, _ = timer.autorange()\n    return min(timer.repeat(repeat=repeat, number=number)) / number\n\n\ndef benchmark(\n    funcs=[np.average, np.mean, np.std, np.sum, np.prod],\n    ufuncs=[np.add, np.multiply, np.power, np.true_divide, np.mod],\n    shapes=[(4, 4), (8, 8), (16, 16), (64, 64)],\n    file=None,\n):\n    """""" \n    Benchmark npstreams against numpy and print the results.\n\n    There are two categories of benchmarks. The first category compares NumPy functions against\n    npstreams versions of the same functions. The second category compares NumPy universal functions\n    against dynamically-generated npstreams versions of those same universal functions.\n\n    All benchmarks compare a reduction operation on a stream of arrays of varying sizes. The sequence length is fixed.\n    \n    .. versionadded:: 1.5.2\n    \n    Parameters\n    ----------\n    funcs : iterable of NumPy functions, optional\n        NumPy functions to compare. An equivalent must exist in npstreams, e.g. `np.average` and `npstreams.average` .\n        Functions without equivalents will be skipped.\n    ufuncs : iterable of NumPy ufunc, optional\n        Invalid ufuncs (e.g. non-binary ufuncs) will be skipped.\n    shapes : iterable of tuples, optional\n        Shapes of arrays to test. Streams of random numbers will be generated with arrays of those shapes.\n        The sequence lengths are fixed.\n    file : file-like or None, optional\n        File to which the benchmark results will be written. If None, sys.stdout will be used.\n    """"""\n    # Preliminaries\n    console_width = min(get_terminal_size().columns, 80)\n    func_test_name = ""numpy.{f.__name__} vs npstreams.{f.__name__}"".format\n    ufunc_test_name = (\n        ""numpy.{f.__name__} vs npstreams.reduce_ufunc(numpy.{f.__name__}, ...)"".format\n    )\n\n    # Determine justification based on maximal shape functions\n    sh_just = max(map(lambda s: len(str(s)), shapes)) + 10\n\n    # To make it easy to either write the results in a file or print to stdout,\n    # We actually redirect stdout.\n    if file is None:\n        file = sys.stdout\n\n    with redirect_stdout(file):\n        # Start benchmarks --------------------------------------------------------\n        print(\n            """".ljust(console_width, ""*""),\n            ""npstreams performance benchmark"".upper().center(console_width),\n            """",\n            ""    npstreams"".ljust(15) + f"" {__version__}"",\n            ""    NumPy"".ljust(15) + f"" {np.__version__}"",\n            """",\n            ""    Speedup is NumPy time divided by npstreams time (Higher is better)"",\n            """".ljust(console_width, ""*""),\n            sep=""\\n"",\n        )\n\n        # Determine valid ufuncs and funcs first ----------------------------------\n        valid_ufuncs = comparable_ufuncs(ufuncs, file)\n        valid_funcs = comparable_funcs(funcs, file)\n\n        # Benchmarking functions --------------------------------------------------\n        for func in sorted(valid_funcs, key=lambda fn: fn.__name__):\n            print(func_test_name(f=func).center(console_width), ""\\n"")\n\n            for (np_time, ns_time, shape) in benchmark_func(func, shapes):\n                speedup = np_time / ns_time\n                print(\n                    ""    "",\n                    f""shape = {shape}"".ljust(sh_just),\n                    f""speedup = {speedup:.4f}x"",\n                )\n\n            print("""".ljust(console_width, ""-""))\n\n        # Benchmarking universal functions ----------------------------------------\n        for ufunc in sorted(valid_ufuncs, key=lambda fn: fn.__name__):\n            print(ufunc_test_name(f=ufunc).center(console_width), ""\\n"")\n\n            for (np_time, ns_time, shape) in benchmark_ufunc(ufunc, shapes):\n                speedup = np_time / ns_time\n                print(\n                    ""    "",\n                    f""shape = {shape}"".ljust(sh_just),\n                    f""speedup = {speedup:.4f}x"",\n                )\n\n            print("""".ljust(console_width, ""-""))\n\n\ndef benchmark_ufunc(ufunc, shapes):\n    """""" \n    Compare the running time between a NumPy ufunc and the npstreams equivalent.\n    \n    Parameters\n    ----------\n    ufunc : NumPy ufunc\n\n    shapes : iterable of tuples, optional\n        Shapes of arrays to test. Streams of random numbers will be generated with arrays of those shapes.\n        The sequence lengths are fixed.\n    \n    Yields\n    ------\n    results : BenchmarkResults\n    """"""\n    for shape in shapes:\n\n        numpy_statement = f""{ufunc.__name__}.reduce(stack(stream()), axis = -1)""\n        npstreams_statement = f""reduce_ufunc(stream(), {ufunc.__name__}, axis = -1)""\n\n        with np.errstate(invalid=""ignore""):\n            np_time = autotimeit(\n                numpy_statement, UFUNC_SETUP.format(ufunc=ufunc, shape=shape)\n            )\n            ns_time = autotimeit(\n                npstreams_statement, UFUNC_SETUP.format(ufunc=ufunc, shape=shape)\n            )\n\n        yield BenchmarkResults(np_time, ns_time, shape)\n\n\ndef benchmark_func(func, shapes):\n    """""" \n    Compare the running time between a NumPy func and the npstreams equivalent.\n    \n    Parameters\n    ----------\n    func : NumPy func\n\n    shapes : iterable of tuples, optional\n        Shapes of arrays to test. Streams of random numbers will be generated with arrays of those shapes.\n        The sequence lengths are fixed.\n    \n    Yields\n    ------\n    results : BenchmarkResults\n    """"""\n    for shape in shapes:\n\n        numpy_statement = f""np_{func.__name__}(stack(stream()), axis = -1)""\n        npstreams_statement = f""ns_{func.__name__}(stream(), axis = -1)""\n\n        with np.errstate(invalid=""ignore""):\n            np_time = autotimeit(\n                numpy_statement, FUNC_SETUP.format(func=func, shape=shape)\n            )\n            ns_time = autotimeit(\n                npstreams_statement, FUNC_SETUP.format(func=func, shape=shape)\n            )\n\n        yield BenchmarkResults(np_time, ns_time, shape)\n\n\ndef comparable_ufuncs(ufuncs, file):\n    """""" \n    Yields ufuncs that can be compared between numpy and npstreams. \n    \n    Parameters\n    ----------\n    ufuncs : iterable of NumPy ufunc\n        NumPy ufuncs to check. Ufuncs that cannot be compared will be skipped.\n\n    Yields\n    ------\n    ufunc : callable\n        NumPy ufuncs that can be compared with npstreams. \n    """"""\n    for ufunc in ufuncs:\n        if not isinstance(ufunc, np.ufunc):\n            print(\n                f""Skipping function {ufunc.__name__} as it is not a NumPy Universal Function""\n            )\n            continue\n\n        try:\n            _check_binary_ufunc(ufunc)\n        except ValueError:\n            print(\n                f""Skipping function {ufunc.__name__} as it is not a valid binary ufunc""\n            )\n        else:\n            yield ufunc\n\n\ndef comparable_funcs(funcs, file):\n    """""" \n    Yields NumPy functions that have npstreams equivalents. \n    \n    Parameters\n    ----------\n    ufuncs : iterable of NumPy functions\n        NumPy funcs to check.\n\n    Yields\n    ------\n    ufunc : callable\n        NumPy funcs that have npstreams equivalents. \n    """"""\n    import npstreams\n\n    npstreams_functions = set(\n        name for name, value in inspect.getmembers(npstreams, inspect.isfunction)\n    )\n    for func in funcs:\n        if func.__name__ not in npstreams_functions:\n            print(\n                f""Skipping function {func.__name__} as there is no npstreams equivalent""\n            )\n        else:\n            yield func\n\n\nif __name__ == ""__main__"":\n    benchmark()\n'"
npstreams/cuda.py,5,"b'# -*- coding: utf-8 -*-\n""""""\nCUDA-accelerated streaming operations\n-------------------------------------\n""""""\nfrom functools import partial\nfrom itertools import repeat\nfrom operator import iadd, imul\nfrom subprocess import run, PIPE\n\nimport numpy as np\n\nfrom . import array_stream, itercopy, nan_to_num, peek\n\n# Determine if\n#   1. pycuda is installed;\n#   2. pycuda can compile with nvcc\n#   3. a GPU is available\n\ntry:\n    import pycuda.gpuarray as gpuarray\n    import pycuda.autoinit\nexcept ImportError:\n    raise ImportError(""PyCUDA is not installed. CUDA capabilities are not available."")\nelse:\n    import pycuda.driver as driver\n    from pycuda.compiler import SourceModule\n\n# Check if nvcc compiler is installed at all\nnvcc_installed = run([""nvcc"", ""-h""], stdout=PIPE).returncode == 0\nif not nvcc_installed:\n    raise ImportError(""CUDA compiler `nvcc` not installed."")\n\n# Check that nvcc is at least set up properly\n# For example, if nvcc is installed but C++ compiler is not in path\ntry:\n    SourceModule("""")\nexcept driver.CompileError:\n    raise ImportError(""CUDA compiler `nvcc` is not properly set up."")\n\nif driver.Device.count() == 0:\n    raise ImportError(""No GPU is available."")\n\n\n@array_stream\ndef cuda_inplace_reduce(arrays, operator, dtype=None, ignore_nan=False, identity=0):\n    """"""\n    Inplace reduce on GPU arrays.\n\n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    operator : callable\n        Callable of two arguments. This operator should operate in-place, storing the results into\n        the buffer of the first argument, e.g. operator.iadd\n    dtype : numpy.dtype, optional\n        Arrays of the stream are cast to this dtype before reduction.\n    ignore_nan : bool, optional\n        If True, NaNs are replaced with ``identity``. Default is propagation of NaNs.\n    identity : float, optional\n        If ``ignore_nan = True``, NaNs are replaced with this value.\n    \n    Returns\n    -------\n    out : ndarray\n    """"""\n    # No need to cast all arrays if ``dtype`` is the same\n    # type as the stream\n    first, arrays = peek(arrays)\n    if (dtype is not None) and (first.dtype != dtype):\n        arrays = map(lambda arr: arr.astype(dtype), arrays)\n\n    if ignore_nan:\n        arrays = map(partial(nan_to_num, fill_value=identity), arrays)\n\n    acc_gpu = gpuarray.to_gpu(next(arrays))  # Accumulator\n    arr_gpu = gpuarray.empty_like(acc_gpu)  # GPU memory location for each array\n    for arr in arrays:\n        arr_gpu.set(arr)\n        operator(acc_gpu, arr_gpu)\n\n    return acc_gpu.get()\n\n\ndef csum(arrays, dtype=None, ignore_nan=False):\n    """""" \n    CUDA-enabled sum of stream of arrays. Arrays are summed along \n    the streaming axis for performance reasons. \n\n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be summed. \n    ignore_nan : bool, optional\n        If True, NaNs are ignored. Default is propagation of NaNs.\n    \n    Returns\n    -------\n    cuda_sum : ndarray\n\n    See Also\n    --------\n    isum : streaming sum of array elements, possibly along different axes\n    """"""\n    return cuda_inplace_reduce(\n        arrays, operator=iadd, dtype=dtype, ignore_nan=ignore_nan, identity=0\n    )\n\n\ndef cprod(arrays, dtype=None, ignore_nan=False):\n    """""" \n    CUDA-enabled product of a stream of arrays. Arrays are multiplied\n    along the streaming axis for performance reasons.\n\n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be multiplied.\n    dtype : numpy.dtype, optional\n        The type of the yielded array and of the accumulator in which the elements \n        are summed. The dtype of a is used by default unless a has an integer dtype \n        of less precision than the default platform integer. In that case, if a is \n        signed then the platform integer is used while if a is unsigned then an \n        unsigned integer of the same precision as the platform integer is used.\n    ignore_nan : bool, optional\n        If True, NaNs are ignored. Default is propagation of NaNs.\n    \n    Yields\n    ------\n    online_prod : ndarray\n    """"""\n    return cuda_inplace_reduce(\n        arrays, operator=imul, dtype=dtype, ignore_nan=ignore_nan, identity=1\n    )\n\n\n@array_stream\ndef cmean(arrays, ignore_nan=False):\n    """"""\n    CUDA-enabled mean of stream of arrays (i.e. unweighted average). Arrays are averaged\n    along the streaming axis for performance reasons.\n\n    Parameters\n    ----------\n    arrays : iterable of ndarrays\n        Arrays to be averaged. This iterable can also a generator.\n    ignore_nan : bool, optional\n        If True, NaNs are set to zero weight. Default is propagation of NaNs.\n    \n    Returns\n    -------\n    cuda_mean : ndarray\n\n    See also\n    --------\n    caverage : CUDA-enabled weighted average\n    imean : streaming mean of arrays, possibly along different axes\n    """"""\n    first, arrays = peek(arrays)\n\n    # Need to know which array has NaNs, and modify the weights stream accordingly\n    if ignore_nan:\n        arrays, arrays2 = itercopy(arrays)\n        weights = map(\n            lambda arr, wgt: np.logical_not(np.isnan(arr)) * wgt, arrays2, weights\n        )\n        arrays = map(np.nan_to_num, arrays)\n        return caverage(arrays, weights, ignore_nan=False)\n\n    accumulator = gpuarray.to_gpu(next(arrays))\n    array_gpu = gpuarray.empty_like(accumulator)\n    num_arrays = 1\n    for arr in arrays:\n        num_arrays += 1\n        array_gpu.set(arr)\n        accumulator += array_gpu\n\n    return accumulator.get() / num_arrays\n\n\n@array_stream\ndef caverage(arrays, weights=None, ignore_nan=False):\n    """"""\n    CUDA-enabled average of stream of arrays, possibly weighted. Arrays are averaged\n    along the streaming axis for performance reasons.\n\n    Parameters\n    ----------\n    arrays : iterable of ndarrays\n        Arrays to be averaged. This iterable can also a generator.\n    weights : iterable of ndarray, iterable of floats, or None, optional\n        Iterable of weights associated with the values in each item of `images`. \n        Each value in an element of `images` contributes to the average \n        according to its associated weight. The weights array can either be a float\n        or an array of the same shape as any element of `images`. If weights=None, \n        then all data in each element of `images` are assumed to have a weight equal to one.\n    ignore_nan : bool, optional\n        If True, NaNs are set to zero weight. Default is propagation of NaNs.\n    \n    Returns\n    -------\n    cuda_avg : ndarray\n\n    See also\n    --------\n    iaverage : streaming weighted average, possibly along different axes\n    """"""\n    if weights is None:\n        return cmean(arrays, ignore_nan)\n\n    first, arrays = peek(arrays)\n\n    # We make sure that weights is always an array\n    # This simplifies the handling of NaNs.\n    if weights is None:\n        weights = repeat(1)\n    weights = map(partial(np.broadcast_to, shape=first.shape), weights)\n    weights = map(\n        lambda arr: arr.astype(first.dtype), weights\n    )  # Won\'t work without this\n\n    # Need to know which array has NaNs, and modify the weights stream accordingly\n    if ignore_nan:\n        arrays, arrays2 = itercopy(arrays)\n        weights = map(\n            lambda arr, wgt: np.logical_not(np.isnan(arr)) * wgt, arrays2, weights\n        )\n        arrays = map(np.nan_to_num, arrays)\n\n    first = next(arrays)\n    fst_wgt = next(weights)\n\n    arr_gpu = gpuarray.to_gpu(first * fst_wgt)\n    wgt_gpu = gpuarray.to_gpu(fst_wgt)\n    for arr, wgt in zip(arrays, weights):\n        arr_gpu += gpuarray.to_gpu(arr) * gpuarray.to_gpu(wgt)\n        wgt_gpu += gpuarray.to_gpu(wgt)\n\n    arr_gpu /= wgt_gpu\n    return arr_gpu.get()\n'"
npstreams/flow.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nFlow controls\n-------------\n""""""\nfrom functools import partial\nfrom glob import iglob\n\nfrom .array_stream import ArrayStream\nfrom .parallel import pmap, pmap_unordered\n\n\ndef iload(files, load_func, **kwargs):\n    """"""\n    Create a stream of arrays from files, which are loaded lazily.\n\n    In cases where the consumer function is much faster than data loading,\n    consider using :func:`pload` instead.\n\n    Parameters\n    ----------\n    pattern : iterable of str or str\n        Either an iterable of filenames or a glob-like pattern str.\n    load_func : callable, optional\n        Function taking a filename as its first arguments\n    kwargs\n        Keyword arguments are passed to ``load_func``.\n    \n    Yields\n    ------\n    arr: `~numpy.ndarray`\n        Loaded data. \n    \n    See Also\n    --------\n    pload : load files from parallel processes.\n    \n    Examples\n    --------\n    To load images using scikit-image ::\n\n        from skimage.io import imread\n        ims = iload(\'images_*.tif\', imread)\n\n    Keyword arguments are passed to the ``load_func``; for example, \n    to specify the scikit-image plugin ``\'tifffile\'``::\n\n        ims = iload(\'images_*.tif\', imread, plugin = \'tifffile\')\n\n    In case the list of images is already known::\n\n        ims = iload([\'im1.tif\', \'im2.tif\', \'im3.tif\'], imread)\n    """"""\n    if isinstance(files, str):\n        files = iglob(files)\n    files = iter(files)\n\n    yield from map(partial(load_func, **kwargs), files)\n\n\ndef pload(files, load_func, processes=1, **kwargs):\n    """"""\n    Create a stream of arrays from files, which are loaded lazily \n    from multiple processes. \n    \n    This function should be preferred to :func:`iload` in cases where \n    the consumer function is much faster than the data can be loaded.\n\n    Parameters\n    ----------\n    pattern : iterable of str or str\n        Either an iterable of filenames or a glob-like pattern str.\n    load_func : callable, optional\n        Function taking a filename as its first arguments\n    processes : int or None, optional\n        Number of processes to use. If `None`, maximal number of processes\n        is used. Default is one.\n    kwargs\n        Keyword arguments are passed to ``load_func``.\n\n    Yields\n    ------\n    arr: `~numpy.ndarray`\n        Loaded data. \n\n    See Also\n    --------\n    iload : load files lazily\n    """"""\n    if processes == 1:\n        yield from iload(files, load_func, **kwargs)\n        return\n\n    if isinstance(files, str):\n        files = iglob(files)\n    files = iter(files)\n\n    yield from pmap_unordered(partial(load_func, **kwargs), files, processes=processes)\n\n\n# pmap does not support local functions\ndef _pipe(funcs, array):\n    for func in funcs:\n        array = func(array)\n    return array\n\n\ndef ipipe(*args, **kwargs):\n    """"""\n    Pipe arrays through a sequence of functions. For example:\n\n    ``pipe(f, g, h, stream)`` is equivalent to ::\n\n        for arr in stream:\n            yield f(g(h(arr)))\n    \n    Parameters\n    ----------\n    *funcs : callable\n        Callable that support Numpy arrays in their first argument. These\n        should *NOT* be generator functions.\n    arrays : iterable\n        Stream of arrays to be passed.\n    processes : int or None, optional, keyword-only\n        Number of processes to use. If `None`, maximal number of processes\n        is used. Default is one.\n    ntotal : int or None, optional, keyword-only\n        If the length of `arrays` is known, but passing `arrays` as a list\n        would take too much memory, the total number of arrays `ntotal` can be specified. This\n        allows for `pmap` to chunk better in case of ``processes > 1``.\n    \n    Yields\n    ------\n    piped : ndarray\n    """"""\n    arrays = ArrayStream(args[-1])\n    functions = tuple(reversed(args[:-1]))\n    yield from pmap(partial(_pipe, functions), arrays, **kwargs)\n'"
npstreams/iter_utils.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nIterator/Generator utilities \n----------------------------\n""""""\nfrom collections import deque\nfrom functools import wraps\nfrom itertools import chain, islice, tee\n\n\ndef primed(gen):\n    """""" \n    Decorator that primes a generator function, i.e. runs the function\n    until the first ``yield`` statement. Useful in cases where there \n    are preliminary checks when creating the generator.\n    """"""\n\n    @wraps(gen)\n    def primed_gen(*args, **kwargs):\n        generator = gen(*args, **kwargs)\n        next(generator)\n        return generator\n\n    return primed_gen\n\n\n@primed\ndef chunked(iterable, chunksize):\n    """"""\n    Generator yielding multiple iterables of length \'chunksize\'.\n\n    Parameters\n    ----------\n    iterable : iterable\n        Iterable to be chunked. \n    chunksize : int\n        Chunk size. \n\n    Yields\n    ------\n    chunk : iterable\n        Iterable of size `chunksize`. In special case of iterable not being\n        divisible by `chunksize`, the last `chunk` will be smaller.\n    \n    Raises\n    ------\n    TypeError : if `chunksize` is not an integer.\n    """"""\n    if not isinstance(chunksize, int):\n        raise TypeError(\n            f""Expected `chunksize` to be an integer, but received {chunksize}""\n        )\n\n    yield\n\n    iterable = iter(iterable)\n\n    next_chunk = tuple(islice(iterable, chunksize))\n    while next_chunk:\n        yield next_chunk\n        next_chunk = tuple(islice(iterable, chunksize))\n\n\ndef peek(iterable):\n    """"""  \n    Peek ahead in an iterable. \n    \n    Parameters\n    ----------\n    iterable : iterable\n    \n    Returns\n    -------\n    first : object\n        First element of ``iterable``\n    stream : iterable\n        Iterable containing ``first`` and all other elements from ``iterable``\n    """"""\n    iterable = iter(iterable)\n    ahead = next(iterable)\n    return ahead, chain([ahead], iterable)\n\n\ndef itercopy(iterable, copies=2):\n    """"""\n    Split iterable into \'copies\'. Once this is done, the original iterable *should\n    not* be used again.\n\n    Parameters\n    ----------\n    iterable : iterable\n        Iterable to be split. Once it is split, the original iterable\n        should not be used again.\n    copies : int, optional\n        Number of copies. Also determines the number of returned iterables.\n    \n    Returns\n    -------\n    iter1, iter2, ... : iterable\n        Copies of ``iterable``.\n    \n    Examples\n    --------\n    By rebinding the name of the original iterable, we make sure that it\n    will never be used again.\n\n    >>> from npstreams import itercopy\n    >>> evens = (2*n for n in range(1000))\n    >>> evens, evens_copy = itercopy(evens, copies = 2)\n\n    See Also\n    --------\n    itertools.tee : equivalent function\n    """"""\n    # itercopy is included because documentation of itertools.tee isn\'t obvious\n    # to everyone\n    return tee(iterable, copies)\n\n\ndef linspace(start, stop, num, endpoint=True):\n    """""" \n    Generate linear space. This is sometimes more appropriate than\n    using `range`.\n\n    Parameters\n    ----------\n    start : float\n        The starting value of the sequence.\n    stop : float\n        The end value of the sequence.\n    num : int\n        Number of samples to generate.\n    endpoint : bool, optional\n        If True (default), the endpoint is included in the linear space.\n\n    Yields\n    ------\n    val : float\n\n    See also\n    --------\n    numpy.linspace : generate linear space as a dense array.\n    """"""\n    # If endpoint are to be counted in,\n    # step does not count the last yield\n    if endpoint:\n        num -= 1\n\n    step = (stop - start) / num\n\n    val = start\n    for _ in range(num):\n        yield val\n        val += step\n\n    if endpoint:\n        yield stop\n\n\ndef multilinspace(start, stop, num, endpoint=True):\n    """""" \n    Generate multilinear space, for joining the values in two iterables.\n\n    Parameters\n    ----------\n    start : iterable of floats\n        The starting value. This iterable will be consumed.\n    stop : iterable of floats\n        The end value. This iterable will be consumed.\n    num : int\n        Number of samples to generate.\n    endpoint : bool, optional\n        If True (default), the endpoint is included in the linear space.\n\n    Yields\n    ------\n    val : tuple\n        Tuple of the same length as start and stop\n\n    Examples\n    --------\n    >>> multispace = multilinspaces(start = (0, 0), stop = (1, 1), num = 4, endpoint = False)\n    >>> print(list(multispace))\n    [(0, 0), (0.25, 0.25), (0.5, 0.5), (0.75, 0.75)]\n\n    See also\n    --------\n    linspace : generate a linear space between two numbers\n    """"""\n    start, stop = tuple(start), tuple(stop)\n    if len(start) != len(stop):\n        raise ValueError(""start and stop must have the same length"")\n\n    spaces = tuple(\n        linspace(a, b, num=num, endpoint=endpoint) for a, b in zip(start, stop)\n    )\n    yield from zip(*spaces)\n\n\ndef last(stream):\n    """""" \n    Retrieve the last item from a stream/iterator, consuming \n    iterables in the process. If empty stream, a RuntimeError is raised.\n    """"""\n    # Wonderful idea from itertools recipes\n    # https://docs.python.org/3.6/library/itertools.html#itertools-recipes\n    try:\n        return deque(stream, maxlen=1)[0]\n    except IndexError:\n        raise RuntimeError(""Empty stream"")\n\n\ndef cyclic(iterable):\n    """""" \n    Yields cyclic permutations of an iterable.\n\n    Examples\n    --------\n    >>> list(cyclic((1,2,3)))\n    [(1,2,3), (2,3,1), (3,1,2)]\n    """"""\n    iterable = tuple(iterable)\n    n = len(iterable)\n    yield from (tuple(iterable[i - j] for i in range(n)) for j in range(n))\n\n\ndef length_hint(obj, default=0):\n    """"""\n    Return an estimate of the number of items in ``obj``.\n\n    This is useful for presizing containers when building from an\n    iterable.\n\n    If the object supports len(), the result will be\n    exact. Otherwise, it may over- or under-estimate by an\n    arbitrary amount. The result will be an integer >= 0.\n\n    Notes\n    -----\n    Source : https://www.python.org/dev/peps/pep-0424/\n\n    Examples\n    --------\n    >>> length_hint([1,2,3,4,5])         # Should be exact\n    5\n    >>> length_hint(None, default = 15)   # Does not implement __length_hint__\n    15\n    """"""\n    try:\n        return len(obj)\n    except TypeError:\n        try:\n            get_hint = type(obj).__length_hint__\n        except AttributeError:\n            return default\n        try:\n            hint = get_hint(obj)\n        except TypeError:\n            return default\n        if hint is NotImplemented:\n            return default\n        if not isinstance(hint, int):\n            raise TypeError(""Length hint must be an integer, not %r"" % type(hint))\n        if hint < 0:\n            raise ValueError(""__length_hint__() should return >= 0"")\n        return hint\n'"
npstreams/linalg.py,6,"b'# -*- coding: utf-8 -*-\n""""""\nNumerics Functions\n------------------\n""""""\nfrom functools import partial\n\nimport numpy as np\n\nfrom .array_stream import array_stream\n\n\n@array_stream\ndef _ireduce_linalg(arrays, func, **kwargs):\n    """"""\n    Yield the cumulative reduction of a linag algebra function\n    """"""\n    arrays = iter(arrays)\n    first = next(arrays)\n    second = next(arrays)\n\n    func = partial(func, **kwargs)\n\n    accumulator = func(first, second)\n    yield accumulator\n\n    for array in arrays:\n        func(accumulator, array, out=accumulator)\n        yield accumulator\n\n\ndef idot(arrays):\n    """"""\n    Yields the cumulative array inner product (dot product) of arrays.\n\n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    \n    Yields\n    ------\n    online_dot : ndarray\n\n    See Also\n    --------\n    numpy.linalg.multi_dot : Compute the dot product of two or more arrays in a single function call, \n                             while automatically selecting the fastest evaluation order.\n    """"""\n    yield from _ireduce_linalg(arrays=arrays, func=np.dot)\n\n\ndef itensordot(arrays, axes=2):\n    """"""\n    Yields the cumulative array inner product (dot product) of arrays.\n\n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    axes : int or (2,) array_like\n        * integer_like: If an int N, sum over the last N axes of a \n          and the first N axes of b in order. The sizes of the corresponding axes must match.\n        * (2,) array_like: Or, a list of axes to be summed over, first sequence applying to a, \n          second to b. Both elements array_like must be of the same length.\n    \n    Yields\n    ------\n    online_tensordot : ndarray\n\n    See Also\n    --------\n    numpy.tensordot : Compute the tensordot on two tensors.\n    """"""\n    yield from _ireduce_linalg(arrays=arrays, func=np.tensordot, axes=axes)\n\n\ndef iinner(arrays):\n    """"""\n    Cumulative inner product of all arrays in a stream.\n    \n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    \n    Yields\n    ------\n    online_inner : ndarray or scalar\n    """"""\n    yield from _ireduce_linalg(arrays=arrays, func=np.inner)\n\n\ndef ieinsum(arrays, subscripts, **kwargs):\n    """"""\n    Evaluates the Einstein summation convention on the operands.\n\n    Using the Einstein summation convention, many common multi-dimensional \n    array operations can be represented in a simple fashion.\n\n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    subscripts : str\n        Specifies the subscripts for summation.\n    dtype : numpy.dtype or None, optional\n        The type of the yielded array and of the accumulator in which the elements \n        are combined. The dtype of a is used by default unless a has an integer dtype \n        of less precision than the default platform integer. In that case, if a is \n        signed then the platform integer is used while if a is unsigned then an \n        unsigned integer of the same precision as the platform integer is used.\n    order : {\'C\', \'F\', \'A\', \'K\'}, optional\n        Controls the memory layout of the output. \'C\' means it should\n        be C contiguous. \'F\' means it should be Fortran contiguous,\n        \'A\' means it should be \'F\' if the inputs are all \'F\', \'C\' otherwise.\n        \'K\' means it should be as close to the layout as the inputs as\n        is possible, including arbitrarily permuted axes.\n        Default is \'K\'.\n    casting : {\'no\', \'equiv\', \'safe\', \'same_kind\', \'unsafe\'}, optional\n        Controls what kind of data casting may occur.  Setting this to\n        \'unsafe\' is not recommended, as it can adversely affect accumulations.\n\n          * \'no\' means the data types should not be cast at all.\n          * \'equiv\' means only byte-order changes are allowed.\n          * \'safe\' means only casts which can preserve values are allowed.\n          * \'same_kind\' means only safe casts or casts within a kind,\n            like float64 to float32, are allowed.\n          * \'unsafe\' means any data conversions may be done.\n\n        Default is \'safe\'.\n    optimize : {False, True, \'greedy\', \'optimal\'}, optional\n        Controls if intermediate optimization should occur. No optimization\n        will occur if False and True will default to the \'greedy\' algorithm.\n        Also accepts an explicit contraction list from the ``np.einsum_path``\n        function. See ``np.einsum_path`` for more details. Default is False.\n    \n    Yields\n    ------\n    online_einsum : ndarray\n        Cumulative Einstein summation\n    """"""\n    yield from _ireduce_linalg(\n        arrays=arrays, func=partial(np.einsum, subscripts), **kwargs\n    )\n'"
npstreams/numerics.py,9,"b'# -*- coding: utf-8 -*-\n""""""\nNumerics Functions\n------------------\n""""""\nimport numpy as np\n\nfrom .reduce import ireduce_ufunc, reduce_ufunc\n\n\ndef isum(arrays, axis=-1, dtype=None, ignore_nan=False):\n    """""" \n    Streaming sum of array elements.\n\n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be summed. \n    axis : int or None, optional\n        Reduction axis. Default is to sum the arrays in the stream as if \n        they had been stacked along a new axis, then sum along this new axis.\n        If None, arrays are flattened before summing. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, arrays are summed\n        along the new axis.\n    dtype : numpy.dtype, optional\n        The type of the yielded array and of the accumulator in which the elements \n        are summed. The dtype of a is used by default unless a has an integer dtype \n        of less precision than the default platform integer. In that case, if a is \n        signed then the platform integer is used while if a is unsigned then an \n        unsigned integer of the same precision as the platform integer is used.\n    ignore_nan : bool, optional\n        If True, NaNs are ignored. Default is propagation of NaNs.\n    \n    Yields\n    ------\n    online_sum : ndarray\n    """"""\n    yield from ireduce_ufunc(\n        arrays, ufunc=np.add, axis=axis, ignore_nan=ignore_nan, dtype=dtype\n    )\n\n\ndef sum(arrays, axis=-1, dtype=None, ignore_nan=False):\n    """""" \n    Sum of arrays in a stream.\n\n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be summed. \n    axis : int or None, optional\n        Reduction axis. Default is to sum the arrays in the stream as if \n        they had been stacked along a new axis, then sum along this new axis.\n        If None, arrays are flattened before summing. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, arrays are summed\n        along the new axis.\n    dtype : numpy.dtype, optional\n        The type of the yielded array and of the accumulator in which the elements \n        are summed. The dtype of a is used by default unless a has an integer dtype \n        of less precision than the default platform integer. In that case, if a is \n        signed then the platform integer is used while if a is unsigned then an \n        unsigned integer of the same precision as the platform integer is used.\n    ignore_nan : bool, optional\n        If True, NaNs are ignored. Default is propagation of NaNs.\n    \n    Returns\n    -------\n    sum : ndarray\n    """"""\n    return reduce_ufunc(\n        arrays, ufunc=np.add, axis=axis, dtype=dtype, ignore_nan=ignore_nan\n    )\n\n\ndef iprod(arrays, axis=-1, dtype=None, ignore_nan=False):\n    """""" \n    Streaming product of array elements.\n\n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be multiplied.\n    axis : int or None, optional\n        Reduction axis. Default is to multiply the arrays in the stream as if \n        they had been stacked along a new axis, then multiply along this new axis.\n        If None, arrays are flattened before multiplication. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, arrays are multiplied\n        along the new axis.\n    dtype : numpy.dtype, optional\n        The type of the yielded array and of the accumulator in which the elements \n        are summed. The dtype of a is used by default unless a has an integer dtype \n        of less precision than the default platform integer. In that case, if a is \n        signed then the platform integer is used while if a is unsigned then an \n        unsigned integer of the same precision as the platform integer is used.\n    ignore_nan : bool, optional\n        If True, NaNs are ignored. Default is propagation of NaNs.\n    \n    Yields\n    ------\n    online_prod : ndarray\n    """"""\n    yield from ireduce_ufunc(\n        arrays, ufunc=np.multiply, axis=axis, dtype=dtype, ignore_nan=ignore_nan\n    )\n\n\ndef prod(arrays, axis=-1, dtype=None, ignore_nan=False):\n    """""" \n    Product of arrays in a stream.\n\n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be multiplied.\n    axis : int or None, optional\n        Reduction axis. Default is to multiply the arrays in the stream as if \n        they had been stacked along a new axis, then multiply along this new axis.\n        If None, arrays are flattened before multiplication. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, arrays are multiplied\n        along the new axis.\n    dtype : numpy.dtype, optional\n        The type of the yielded array and of the accumulator in which the elements \n        are summed. The dtype of a is used by default unless a has an integer dtype \n        of less precision than the default platform integer. In that case, if a is \n        signed then the platform integer is used while if a is unsigned then an \n        unsigned integer of the same precision as the platform integer is used.\n    ignore_nan : bool, optional\n        If True, NaNs are ignored. Default is propagation of NaNs.\n    \n    Returns\n    -------\n    product : ndarray\n    """"""\n    return reduce_ufunc(\n        arrays, ufunc=np.multiply, axis=axis, dtype=dtype, ignore_nan=ignore_nan\n    )\n\n\ndef isub(arrays, axis=-1, dtype=None):\n    """"""\n    Subtract elements in a reduction fashion. Equivalent to ``numpy.subtract.reduce`` on a dense array.\n\n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be multiplied.\n    axis : int, optional\n        Reduction axis. Since subtraction is not reorderable (unlike a sum, for example),\n        `axis` must be specified as an int; full reduction (``axis = None``) will raise an exception. \n        Default is to subtract the arrays in the stream as if they had been stacked along a new axis, \n        then subtract along this new axis. If None, arrays are flattened before subtraction. \n        If `axis` is an int larger that the number of dimensions in the arrays of the stream, \n        arrays are subtracted along the new axis.\n    dtype : numpy.dtype, optional\n        The type of the yielded array and of the accumulator in which the elements \n        are combined. The dtype of a is used by default unless a has an integer dtype \n        of less precision than the default platform integer. In that case, if a is \n        signed then the platform integer is used while if a is unsigned then an \n        unsigned integer of the same precision as the platform integer is used.\n    \n    Yields\n    ------\n    online_sub : ndarray\n\n    Raises\n    ------\n    ValueError\n        If `axis` is None. Since subtraction is not reorderable (unlike a sum, for example),\n        `axis` must be specified as an int.\n    """"""\n    if axis is None:\n        raise ValueError(\n            ""Subtraction is not a reorderable operation, and \\\n                          therefore a specific axis must be given.""\n        )\n    yield from ireduce_ufunc(arrays, ufunc=np.subtract, axis=axis, dtype=dtype)\n\n\ndef iall(arrays, axis=-1):\n    """""" \n    Test whether all array elements along a given axis evaluate to True \n    \n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    axis : int or None, optional\n        Axis along which a logical AND reduction is performed. The default\n        is to perform a logical AND along the \'stream axis\', as if all arrays in ``array``\n        were stacked along a new dimension. If ``axis = None``, arrays in ``arrays`` are flattened\n        before reduction.\n\n    Yields\n    ------\n    all : ndarray, dtype bool \n    """"""\n    # TODO: use ``where`` keyword to only check places that are already ``True``\n    yield from ireduce_ufunc(arrays, ufunc=np.logical_and, axis=axis)\n\n\ndef iany(arrays, axis=-1):\n    """""" \n    Test whether any array elements along a given axis evaluate to True.\n    \n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    axis : int or None, optional\n        Axis along which a logical OR reduction is performed. The default\n        is to perform a logical AND along the \'stream axis\', as if all arrays in ``array``\n        were stacked along a new dimension. If ``axis = None``, arrays in ``arrays`` are flattened\n        before reduction.\n\n    Yields\n    ------\n    any : ndarray, dtype bool \n    """"""\n    # TODO: use ``where`` keyword to only check places that are not already ``True``\n    yield from ireduce_ufunc(arrays, ufunc=np.logical_or, axis=axis)\n\n\ndef imax(arrays, axis, ignore_nan=False):\n    """""" \n    Maximum of a stream of arrays along an axis.\n\n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    axis : int or None, optional\n        Axis along which the maximum is found. The default\n        is to find the maximum along the \'stream axis\', as if all arrays in ``array``\n        were stacked along a new dimension. If ``axis = None``, arrays in ``arrays`` are flattened\n        before reduction.\n    ignore_nan : bool, optional\n        If True, NaNs are ignored. Default is propagation of NaNs.\n\n    Yields\n    ------\n    online_max : ndarray\n        Cumulative maximum.\n    """"""\n    ufunc = np.fmax if ignore_nan else np.maximum\n    yield from ireduce_ufunc(arrays, ufunc, axis)\n\n\ndef imin(arrays, axis, ignore_nan=False):\n    """""" \n    Minimum of a stream of arrays along an axis.\n\n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    axis : int or None, optional\n        Axis along which the minimum is found. The default\n        is to find the minimum along the \'stream axis\', as if all arrays in ``array``\n        were stacked along a new dimension. If ``axis = None``, arrays in ``arrays`` are flattened\n        before reduction.\n    ignore_nan : bool, optional\n        If True, NaNs are ignored. Default is propagation of NaNs.\n\n    Yields\n    ------\n    online_min : ndarray\n        Cumulative minimum.\n    """"""\n    ufunc = np.fmin if ignore_nan else np.minimum\n    yield from ireduce_ufunc(arrays, ufunc, axis)\n'"
npstreams/parallel.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nParallelization utilities \n-------------------------\n""""""\nfrom collections.abc import Sized\nfrom functools import partial, reduce\nfrom multiprocessing import Pool\n\nfrom .iter_utils import chunked\n\n\ndef preduce(func, iterable, args=None, kwargs=None, processes=1, ntotal=None):\n    """"""\n    Parallel application of the reduce function, with keyword arguments.\n\n    Parameters\n    ----------\n    func : callable\n        Function to be applied to every element of `iterable`.\n    iterable : iterable\n        Iterable of items to be reduced. Generators are consumed.\n    args : tuple or None, optional\n        Positional arguments of `function`.\n    kwargs : dictionary or None, optional\n        Keyword arguments of `function`.\n    processes : int or None, optional\n        Number of processes to use. If `None`, maximal number of processes\n        is used. Default is one.\n    ntotal : int or None, optional\n        If the length of `iterable` is known, but passing `iterable` as a list\n        would take too much memory, the total length `ntotal` can be specified. This\n        allows for `preduce` to chunk better.\n\n    Returns\n    -------\n    reduced : object\n\n    Notes\n    -----\n    If `processes` is 1, `preduce` is equivalent to functools.reduce with the\n    added benefit of using `args` and `kwargs`, but `initializer` is not supported.\n    """"""\n    if kwargs is None:\n        kwargs = dict()\n\n    if args is None:\n        args = tuple()\n\n    func = partial(func, *args, **kwargs)\n\n    if processes == 1:\n        return reduce(func, iterable)\n\n    with Pool(processes) as pool:\n        chunksize = 1\n        if isinstance(iterable, Sized):\n            chunksize = max(1, int(len(iterable) / pool._processes))\n        elif ntotal is not None:\n            chunksize = max(1, int(ntotal / pool._processes))\n\n        # Some reductions are order-sensitive\n        res = pool.imap(partial(reduce, func), tuple(chunked(iterable, chunksize)))\n        return reduce(func, res)\n\n\ndef pmap(func, iterable, args=None, kwargs=None, processes=1, ntotal=None):\n    """"""\n    Parallel application of a function with keyword arguments.\n\n    Parameters\n    ----------\n    func : callable\n        Function to be applied to every element of `iterable`.\n    iterable : iterable\n        Iterable of items to be mapped.\n    args : tuple or None, optional\n        Positional arguments of `function`.\n    kwargs : dictionary or None, optional\n        Keyword arguments of `function`.\n    processes : int or None, optional\n        Number of processes to use. If `None`, maximal number of processes\n        is used. Default is one.\n    ntotal : int or None, optional\n        If the length of `iterable` is known, but passing `iterable` as a list\n        would take too much memory, the total length `ntotal` can be specified. This\n        allows for `pmap` to chunk better.\n\n    Yields\n    ------\n    Mapped values.\n\n    See Also\n    --------\n    pmap_unordered : parallel map that does not preserve order\n\n    Notes\n    -----\n    If `processes` is 1, `pmap` reduces to `map`, with the added benefit of\n    of using `kwargs`\n    """"""\n    if kwargs is None:\n        kwargs = dict()\n\n    if args is None:\n        args = tuple()\n\n    func = partial(func, *args, **kwargs)\n\n    if processes == 1:\n        yield from map(func, iterable)\n        return\n\n    with Pool(processes) as pool:\n        chunksize = 1\n        if isinstance(iterable, Sized):\n            chunksize = max(1, int(len(iterable) / pool._processes))\n        elif ntotal is not None:\n            chunksize = max(1, int(ntotal / pool._processes))\n\n        yield from pool.imap(func=func, iterable=iterable, chunksize=chunksize)\n\n\ndef pmap_unordered(func, iterable, args=None, kwargs=None, processes=1, ntotal=None):\n    """"""\n    Parallel application of a function with keyword arguments in no particular order. \n    This can reduce memory usage because results are not accumulated so that the order is preserved.\n\n    Parameters\n    ----------\n    func : callable\n        Function to be applied to every element of `iterable`.\n    iterable : iterable\n        Iterable of items to be mapped.\n    args : tuple or None, optional\n        Positional arguments of `function`.\n    kwargs : dictionary or None, optional\n        Keyword arguments of `function`.\n    processes : int or None, optional\n        Number of processes to use. If `None`, maximal number of processes\n        is used. Default is one.\n    ntotal : int or None, optional\n        If the length of `iterable` is known, but passing `iterable` as a list\n        would take too much memory, the total length `ntotal` can be specified. This\n        allows for `pmap` to chunk better.\n\n    Yields\n    ------\n    Mapped values.\n\n    See Also\n    --------\n    pmap : parallel map that preserves order\n\n    Notes\n    -----\n    If `processes` is 1, `pmap_unordered` reduces to `map`, with the added benefit of\n    of using `kwargs`\n    """"""\n    if kwargs is None:\n        kwargs = dict()\n\n    if args is None:\n        args = tuple()\n\n    func = partial(func, *args, **kwargs)\n\n    if processes == 1:\n        yield from map(func, iterable)\n        return\n\n    with Pool(processes) as pool:\n        chunksize = 1\n        if isinstance(iterable, Sized):\n            chunksize = max(1, int(len(iterable) / pool._processes))\n        elif ntotal is not None:\n            chunksize = max(1, int(ntotal / pool._processes))\n\n        yield from pool.imap_unordered(\n            func=func, iterable=iterable, chunksize=chunksize\n        )\n'"
npstreams/reduce.py,7,"b'# -*- coding: utf-8 -*-\n""""""\nGeneral stream reduction\n------------------------\n""""""\nfrom functools import lru_cache, partial\nfrom itertools import islice, repeat\nfrom multiprocessing import Pool\n\nimport numpy as np\n\nfrom .array_stream import array_stream\nfrom .array_utils import nan_to_num\nfrom .iter_utils import chunked, last, peek, primed\nfrom .parallel import preduce\n\nidentity = lambda i: i\n\n\n@lru_cache(maxsize=128)\ndef _check_binary_ufunc(ufunc):\n    """""" \n    Check that ufunc is suitable for ``ireduce_ufunc``. \n    \n    Specifically, a binary ``numpy.ufunc`` function is required. Functions \n    that returns a boolean are also not suitable because they cannot be accumulated.\n\n    This function does not return anything. \n\n    Parameters\n    ----------\n    ufunc : callable\n        Function to check.\n\n    Raises\n    ------\n    TypeError : if ``ufunc`` is not a ``numpy.ufunc``\n    ValueError: if ``ufunc`` is not binary or the return type is boolean.\n    """"""\n    if not isinstance(ufunc, np.ufunc):\n        raise TypeError(f""{ufunc.__name__} is not a NumPy Ufunc"")\n    if ufunc.nin != 2:\n        raise ValueError(\n            f""Only binary ufuncs are supported, and {ufunc.__name__} is not one of them""\n        )\n\n\n@primed\n@array_stream\ndef ireduce_ufunc(arrays, ufunc, axis=-1, dtype=None, ignore_nan=False, **kwargs):\n    """"""\n    Streaming reduction generator function from a binary NumPy ufunc. Generator\n    version of `reduce_ufunc`.\n\n    ``ufunc`` must be a NumPy binary Ufunc (i.e. it takes two arguments). Moreover,\n    for performance reasons, ufunc must have the same return types as input types.\n    This precludes the use of ``numpy.greater``, for example.\n\n    Note that performance is much better for the default ``axis = -1``. In such a case,\n    reduction operations can occur in-place. This also allows to operate in constant-memory.\n    \n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    ufunc : numpy.ufunc\n        Binary universal function.\n    axis : int or None, optional\n        Reduction axis. Default is to reduce the arrays in the stream as if \n        they had been stacked along a new axis, then reduce along this new axis.\n        If None, arrays are flattened before reduction. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, arrays are reduced\n        along the new axis. Note that not all of NumPy Ufuncs support \n        ``axis = None``, e.g. ``numpy.subtract``.\n    dtype : numpy.dtype or None, optional\n        Overrides the dtype of the calculation and output arrays.\n    ignore_nan : bool, optional\n        If True and ufunc has an identity value (e.g. ``numpy.add.identity`` is 0), then NaNs\n        are replaced with this identity. An error is raised if ``ufunc`` has no identity \n        (e.g. ``numpy.maximum.identity`` is ``None``).\n    kwargs\n        Keyword arguments are passed to ``ufunc``. Note that some valid ufunc keyword arguments\n        (e.g. ``keepdims``) are not valid for all streaming functions. Also, contrary to NumPy \n        v. 1.10+, ``casting = \'unsafe`` is the default in npstreams.\n    \n    Yields \n    ------\n    reduced : ndarray or scalar\n\n    Raises\n    ------\n    TypeError : if ``ufunc`` is not NumPy ufunc.\n    ValueError : if ``ignore_nan`` is True but ``ufunc`` has no identity\n    ValueError : if ``ufunc`` is not a binary ufunc\n    ValueError : if ``ufunc`` does not have the same input type as output type\n    """"""\n    kwargs.update({""dtype"": dtype, ""axis"": axis})\n\n    _check_binary_ufunc(ufunc)\n\n    if ignore_nan:\n        if ufunc.identity is None:\n            raise ValueError(\n                f""Cannot ignore NaNs because {ufunc.__name__} has no identity value""\n            )\n        # TODO: use the ``where`` keyword in ufuncs instead\n        arrays = map(partial(nan_to_num, fill_value=ufunc.identity, copy=False), arrays)\n\n    # Since ireduce_ufunc is primed, we need to wait here\n    # Priming is a way to start error checking before actually running\n    # any computations.\n    yield\n\n    if kwargs[""axis""] == -1:\n        yield from _ireduce_ufunc_new_axis(arrays, ufunc, **kwargs)\n        return\n\n    if kwargs[""axis""] is None:\n        yield from _ireduce_ufunc_all_axes(arrays, ufunc, **kwargs)\n        return\n\n    first, arrays = peek(arrays)\n\n    if kwargs[""axis""] >= first.ndim:\n        kwargs[""axis""] = -1\n        yield from ireduce_ufunc(arrays, ufunc, **kwargs)\n        return\n\n    yield from _ireduce_ufunc_existing_axis(arrays, ufunc, **kwargs)\n\n\ndef reduce_ufunc(arrays, ufunc, axis=-1, dtype=None, ignore_nan=False, **kwargs):\n    """"""\n    Reduce a stream using a binary NumPy ufunc. Function version of ``ireduce_ufunc``.\n\n    ``ufunc`` must be a NumPy binary Ufunc (i.e. it takes two arguments). Moreover,\n    for performance reasons, ufunc must have the same return types as input types.\n    This precludes the use of ``numpy.greater``, for example.\n\n    Note that performance is much better for the default ``axis = -1``. In such a case,\n    reduction operations can occur in-place. This also allows to operate in constant-memory.\n    \n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    ufunc : numpy.ufunc\n        Binary universal function.\n    axis : int or None, optional\n        Reduction axis. Default is to reduce the arrays in the stream as if \n        they had been stacked along a new axis, then reduce along this new axis.\n        If None, arrays are flattened before reduction. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, arrays are reduced\n        along the new axis. Note that not all of NumPy Ufuncs support \n        ``axis = None``, e.g. ``numpy.subtract``.\n    dtype : numpy.dtype or None, optional\n        Overrides the dtype of the calculation and output arrays.\n    ignore_nan : bool, optional\n        If True and ufunc has an identity value (e.g. ``numpy.add.identity`` is 0), then NaNs\n        are replaced with this identity. An error is raised if ``ufunc`` has no identity (e.g. ``numpy.maximum.identity`` is ``None``).\n    kwargs\n        Keyword arguments are passed to ``ufunc``. Note that some valid ufunc keyword arguments\n        (e.g. ``keepdims``) are not valid for all streaming functions. Note that\n        contrary to NumPy v. 1.10+, ``casting = \'unsafe`` is the default in npstreams.\n    \n    Returns \n    -------\n    reduced : ndarray or scalar\n\n    Raises\n    ------\n    TypeError : if ``ufunc`` is not NumPy ufunc.\n    ValueError : if ``ignore_nan`` is True but ``ufunc`` has no identity\n    ValueError: if ``ufunc`` is not a binary ufunc\n    ValueError: if ``ufunc`` does not have the same input type as output type\n    """"""\n    return last(\n        ireduce_ufunc(\n            arrays, ufunc, axis=axis, dtype=dtype, ignore_nan=ignore_nan, **kwargs\n        )\n    )\n\n\n@array_stream\ndef preduce_ufunc(\n    arrays,\n    ufunc,\n    axis=-1,\n    dtype=None,\n    ignore_nan=False,\n    processes=1,\n    ntotal=None,\n    **kwargs,\n):\n    """"""\n    Parallel reduction of array streams.\n\n    ``ufunc`` must be a NumPy binary Ufunc (i.e. it takes two arguments). Moreover,\n    for performance reasons, ufunc must have the same return types as input types.\n    This precludes the use of ``numpy.greater``, for example.\n\n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    ufunc : numpy.ufunc\n        Binary universal function.\n    axis : int or None, optional\n        Reduction axis. Default is to reduce the arrays in the stream as if \n        they had been stacked along a new axis, then reduce along this new axis.\n        If None, arrays are flattened before reduction. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, arrays are reduced\n        along the new axis. Note that not all of NumPy Ufuncs support \n        ``axis = None``, e.g. ``numpy.subtract``.\n    dtype : numpy.dtype or None, optional\n        Overrides the dtype of the calculation and output arrays.\n    ignore_nan : bool, optional\n        If True and ufunc has an identity value (e.g. ``numpy.add.identity`` is 0), then NaNs\n        are replaced with this identity. An error is raised if ``ufunc`` has no identity (e.g. ``numpy.maximum.identity`` is ``None``).\n    processes : int or None, optional\n        Number of processes to use. If `None`, maximal number of processes\n        is used. Default is 1.\n    kwargs\n        Keyword arguments are passed to ``ufunc``. Note that some valid ufunc keyword arguments\n        (e.g. ``keepdims``) are not valid for all streaming functions. Also, contrary to NumPy \n        v. 1.10+, ``casting = \'unsafe`` is the default in npstreams.\n    """"""\n    if processes == 1:\n        return reduce_ufunc(arrays, ufunc, axis, dtype, ignore_nan, **kwargs)\n\n    kwargs.update(\n        {""ufunc"": ufunc, ""ignore_nan"": ignore_nan, ""dtype"": dtype, ""axis"": axis}\n    )\n    reduce = partial(reduce_ufunc, **kwargs)\n    # return preduce(reduce, arrays, processes = processes, ntotal = ntotal)\n\n    with Pool(processes) as pool:\n        chunksize = 1\n        if ntotal is not None:\n            chunksize = max(1, int(ntotal / pool._processes))\n        res = pool.imap(reduce, chunked(arrays, chunksize))\n        return reduce(res)\n\n\ndef _ireduce_ufunc_new_axis(arrays, ufunc, **kwargs):\n    """"""\n    Reduction operation for arrays, in the direction of a new axis (i.e. stacking).\n    \n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    ufunc : numpy.ufunc\n        Binary universal function. Must have a signature of the form ufunc(x1, x2, ...)\n    kwargs\n        Keyword arguments are passed to ``ufunc``.\n    \n    Yields \n    ------\n    reduced : ndarray\n    """"""\n    arrays = iter(arrays)\n    first = next(arrays)\n\n    kwargs.pop(""axis"")\n\n    dtype = kwargs.get(""dtype"", None)\n    if dtype is None:\n        dtype = first.dtype\n    else:\n        kwargs[""casting""] = ""unsafe""\n\n    # If the out parameter was already given\n    # we create the accumulator from it\n    # Otherwise, it is a copy of the first array\n    accumulator = kwargs.pop(""out"", None)\n    if accumulator is not None:\n        accumulator[:] = first\n    else:\n        accumulator = np.array(first, copy=True).astype(dtype)\n    yield accumulator\n\n    for array in arrays:\n        ufunc(accumulator, array, out=accumulator, **kwargs)\n        yield accumulator\n\n\ndef _ireduce_ufunc_existing_axis(arrays, ufunc, **kwargs):\n    """"""\n    Reduction operation for arrays, in the direction of an existing axis.\n    \n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    ufunc : numpy.ufunc\n        Binary universal function. Must have a signature of the form ufunc(x1, x2, ...)\n    kwargs\n        Keyword arguments are passed to ``ufunc``. The ``out`` parameter is ignored.\n\n    Yields \n    ------\n    reduced : ndarray\n    """"""\n    arrays = iter(arrays)\n    first = next(arrays)\n\n    if kwargs[""axis""] not in range(first.ndim):\n        axis = kwargs[""axis""]\n        raise ValueError(f""Axis {axis} not supported on arrays of shape {first.shape}."")\n\n    # Remove parameters that will not be used.\n    kwargs.pop(""out"", None)\n\n    dtype = kwargs.get(""dtype"")\n    if dtype is None:\n        dtype = first.dtype\n\n    axis_reduce = partial(ufunc.reduce, **kwargs)\n\n    accumulator = np.atleast_1d(axis_reduce(first))\n    yield accumulator\n\n    # On the first pass of the following loop, accumulator is missing a dimensions\n    # therefore, the stacking function cannot be \'concatenate\'\n    second = next(arrays)\n    accumulator = np.stack([accumulator, np.atleast_1d(axis_reduce(second))], axis=-1)\n    yield accumulator\n\n    # On the second pass, the new dimensions exists, and thus we switch to\n    # using concatenate.\n    for array in arrays:\n        reduced = np.expand_dims(\n            np.atleast_1d(axis_reduce(array)), axis=accumulator.ndim - 1\n        )\n        accumulator = np.concatenate([accumulator, reduced], axis=accumulator.ndim - 1)\n        yield accumulator\n\n\ndef _ireduce_ufunc_all_axes(arrays, ufunc, **kwargs):\n    """"""\n    Reduction operation for arrays, over all axes.\n    \n    Parameters\n    ----------\n    arrays : iterable\n        Arrays to be reduced.\n    ufunc : numpy.ufunc\n        Binary universal function. Must have a signature of the form ufunc(x1, x2, ...)\n    kwargs\n        Keyword arguments are passed to ``ufunc``. The ``out`` parameter is ignored.\n\n    Yields \n    ------\n    reduced : scalar\n    """"""\n    arrays = iter(arrays)\n    first = next(arrays)\n\n    kwargs.pop(""out"", None)\n\n    kwargs[""axis""] = None\n    axis_reduce = partial(ufunc.reduce, **kwargs)\n\n    accumulator = axis_reduce(first)\n    yield accumulator\n\n    for array in arrays:\n        accumulator = axis_reduce([accumulator, axis_reduce(array)])\n        yield accumulator\n'"
npstreams/stacking.py,3,"b'# -*- coding: utf-8 -*-\n""""""\nStacking arrays from a stream\n-----------------------------\n""""""\nfrom collections import Sized\nfrom functools import partial\n\nimport numpy as np\n\nfrom .array_stream import array_stream\n\n\n@array_stream\ndef stack(arrays, axis=-1):\n    """""" \n    Stack of all arrays from a stream. Generalization of numpy.stack\n    and numpy.concatenate. \n\n    Parameters\n    ----------\n    arrays : iterable\n        Stream of NumPy arrays. Arrays must have shapes that broadcast together.\n    axis : int, optional\n        Stacking direction. If ``axis = -1``, arrays are stacked along a\n        new dimension.\n    \n    Returns\n    -------\n    stacked : ndarray\n        Cumulative stacked array.\n    """"""\n    # Shortcut : if axis == -1, this is exactly what ArrayStream.__array__\n    if axis == -1:\n        return np.array(arrays)\n\n    # TODO: Shortcut if we already know the stream length\n    # Note : we are guaranteed that `arrays` is a stream of arrays\n    # at worst a tuple (arr,)\n    # Use npstreams.length_hint\n    arrays = iter(arrays)\n    first = next(arrays)\n    stack = np.array(first, copy=True)\n\n    for array in arrays:\n        stack = np.concatenate([stack, array], axis=axis)\n\n    return stack\n'"
npstreams/stats.py,16,"b'# -*- coding: utf-8 -*-\n""""""\nStatistical functions\n---------------------\n""""""\nfrom functools import partial\nfrom itertools import count, repeat, starmap\nfrom operator import truediv\n\nimport numpy as np\n\nfrom .array_stream import array_stream\nfrom .array_utils import nan_to_num\nfrom .iter_utils import itercopy, last, peek\nfrom .numerics import isum\n\n\n@array_stream\ndef _iaverage(arrays, axis=-1, weights=None, ignore_nan=False):\n    """""" \n    Primitive version of weighted averaging that yields the running sum and running weights sum,\n    but avoids the costly division at every step.\n    """"""\n    # Special case: in the easiest case, no need to calculate\n    # weights and ignore nans.\n    # This case is pretty common\n    if (weights is None) and (not ignore_nan) and (axis == -1):\n        yield from zip(\n            isum(arrays, axis=axis, dtype=np.float, ignore_nan=False), count(1)\n        )\n        return\n\n    first, arrays = peek(arrays)\n\n    # We make sure that weights is always an array\n    # This simplifies the handling of NaNs.\n    if weights is None:\n        weights = repeat(1)\n    weights = map(partial(np.broadcast_to, shape=first.shape), weights)\n\n    # Need to know which array has NaNs, and modify the weights stream accordingly\n    if ignore_nan:\n        arrays, arrays2 = itercopy(arrays)\n        weights = map(\n            lambda arr, wgt: np.logical_not(np.isnan(arr)) * wgt, arrays2, weights\n        )\n\n    weights1, weights2 = itercopy(weights)\n\n    sum_of_weights = isum(weights1, axis=axis, dtype=np.float)\n    weighted_arrays = map(lambda arr, wgt: arr * wgt, arrays, weights2)\n    weighted_sum = isum(\n        weighted_arrays, axis=axis, ignore_nan=ignore_nan, dtype=np.float\n    )\n\n    yield from zip(weighted_sum, sum_of_weights)\n\n\n@array_stream\ndef average(arrays, axis=-1, weights=None, ignore_nan=False):\n    """""" \n    Average (weighted) of a stream of arrays. This function consumes the\n    entire stream.\n\n    Parameters\n    ----------\n    arrays : iterable of ndarrays\n        Arrays to be averaged. This iterable can also a generator.\n    axis : int, optional\n        Reduction axis. Default is to average the arrays in the stream as if \n        they had been stacked along a new axis, then average along this new axis.\n        If None, arrays are flattened before averaging. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, arrays are averaged\n        along the new axis.\n    weights : iterable of ndarray, iterable of floats, or None, optional\n        Iterable of weights associated with the values in each item of `arrays`. \n        Each value in an element of `arrays` contributes to the average \n        according to its associated weight. The weights array can either be a float\n        or an array of the same shape as any element of `arrays`. If ``weights=None``, \n        then all data in each element of `arrays` are assumed to have a weight equal to one.\n    ignore_nan : bool, optional\n        If True, NaNs are set to zero weight. Default is propagation of NaNs.\n    \n    Returns\n    -------\n    avg: `~numpy.ndarray`, dtype float\n        Weighted average. \n    \n    See Also\n    --------\n    iaverage : streaming (weighted) average.\n    numpy.average : (weighted) average of dense arrays\n    mean : non-weighted average of a stream.\n    """"""\n    total_sum, total_weight = last(_iaverage(arrays, axis, weights, ignore_nan))\n    return total_sum / total_weight\n\n\n@array_stream\ndef iaverage(arrays, axis=-1, weights=None, ignore_nan=False):\n    """""" \n    Streaming (weighted) average of arrays.\n\n    Parameters\n    ----------\n    arrays : iterable of ndarrays\n        Arrays to be averaged. This iterable can also a generator.\n    axis : int, optional\n        Reduction axis. Default is to average the arrays in the stream as if \n        they had been stacked along a new axis, then average along this new axis.\n        If None, arrays are flattened before averaging. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, arrays are averaged\n        along the new axis.\n    weights : iterable of ndarray, iterable of floats, or None, optional\n        Iterable of weights associated with the values in each item of `arrays`. \n        Each value in an element of `arrays` contributes to the average \n        according to its associated weight. The weights array can either be a float\n        or an array of the same shape as any element of `arrays`. If weights=None, \n        then all data in each element of `arrays` are assumed to have a weight equal to one.\n    ignore_nan : bool, optional\n        If True, NaNs are set to zero weight. Default is propagation of NaNs.\n    \n    Yields\n    ------\n    avg: `~numpy.ndarray`, dtype float\n        Weighted average. \n    \n    See Also\n    --------\n    imean : streaming array mean (non-weighted average).\n    """"""\n    # Primitive stream is composed of tuples (running_sum, running_weights)\n    primitive = _iaverage(arrays, axis, weights, ignore_nan)\n    yield from map(lambda element: truediv(*element), primitive)\n\n\n@array_stream\ndef mean(arrays, axis=-1, ignore_nan=False):\n    """""" \n    Mean of a stream of arrays. This function consumes the\n    entire stream.\n\n    Parameters\n    ----------\n    arrays : iterable of ndarrays\n        Arrays to be averaged. This iterable can also a generator.\n    axis : int, optional\n        Reduction axis. Default is to average the arrays in the stream as if \n        they had been stacked along a new axis, then average along this new axis.\n        If None, arrays are flattened before averaging. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, arrays are averaged\n        along the new axis.\n    ignore_nan : bool, optional\n        If True, NaNs are set to zero weight. Default is propagation of NaNs.\n    \n    Returns\n    -------\n    mean: `~numpy.ndarray`, dtype float\n        Total mean array.\n    """"""\n    total_sum, total_count = last(\n        _iaverage(arrays, axis, weights=None, ignore_nan=ignore_nan)\n    )\n    return total_sum / total_count\n\n\n@array_stream\ndef imean(arrays, axis=-1, ignore_nan=False):\n    """""" \n    Streaming mean of arrays. Equivalent to `iaverage(arrays, weights = None)`.\n\n    Parameters\n    ----------\n    arrays : iterable of ndarrays\n        Arrays to be averaged. This iterable can also a generator.\n    axis : int, optional\n        Reduction axis. Default is to average the arrays in the stream as if \n        they had been stacked along a new axis, then average along this new axis.\n        If None, arrays are flattened before averaging. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, arrays are averaged\n        along the new axis.\n    ignore_nan : bool, optional\n        If True, NaNs are set to zero weight. Default is propagation of NaNs.\n    \n    Yields\n    ------\n    mean: `~numpy.ndarray`, dtype float\n        Online mean array.\n    """"""\n    # Primitive stream is composed of tuples (running_sum, running_count)\n    primitive = _iaverage(arrays, axis, weights=None, ignore_nan=ignore_nan)\n    yield from map(lambda element: truediv(*element), primitive)\n\n\n@array_stream\ndef _ivar(arrays, axis=-1, weights=None, ignore_nan=False):\n    """""" \n    Primitive version of weighted variance that yields the running average, running average of squares and running weights sum,\n    but avoids the costly division and squaring at every step.\n    """"""\n    first, arrays = peek(arrays)\n\n    # We make sure that weights is always an array\n    # This simplifies the handling of NaNs.\n    if weights is None:\n        weights = repeat(1)\n    weights = map(partial(np.broadcast_to, shape=first.shape), weights)\n\n    # Need to know which array has NaNs, and modify the weights stream accordingly\n    if ignore_nan:\n        arrays, arrays2 = itercopy(arrays)\n        weights = map(\n            lambda arr, wgt: np.logical_not(np.isnan(arr)) * wgt, arrays2, weights\n        )\n\n    arrays, arrays2 = itercopy(arrays)\n    weights, weights2, weights3 = itercopy(weights, 3)\n\n    avgs = iaverage(arrays, axis=axis, weights=weights, ignore_nan=ignore_nan)\n    avg_of_squares = iaverage(\n        map(np.square, arrays2), axis=axis, weights=weights2, ignore_nan=ignore_nan\n    )\n    sum_of_weights = isum(weights3, axis=axis, ignore_nan=ignore_nan)\n\n    yield from zip(avgs, avg_of_squares, sum_of_weights)\n\n\n@array_stream\ndef average_and_var(arrays, axis=-1, ddof=0, weights=None, ignore_nan=False):\n    """"""\n    Calculate the simultaneous average and variance of a stream of arrays. This is done in\n    single iteration for maximum performance.\n\n    .. versionadded:: 1.6.1\n\n    Parameters\n    ----------\n    arrays : iterable of ndarrays\n        Arrays to be combined. This iterable can also a generator.\n    axis : int, optional\n        Reduction axis. Default is to combine the arrays in the stream as if \n        they had been stacked along a new axis, then compute the variance along this new axis.\n        If None, arrays are flattened. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, variance is computed\n        along the new axis.\n    ddof : int, optional\n        Means Delta Degrees of Freedom. The divisor used in calculations\n        is ``N - ddof``, where ``N`` represents the number of elements.\n    weights : iterable of ndarray, iterable of floats, or None, optional\n        Iterable of weights associated with the values in each item of `arrays`. \n        Each value in an element of `arrays` contributes to the variance \n        according to its associated weight. The weights array can either be a float\n        or an array of the same shape as any element of `arrays`. If weights=None, \n        then all data in each element of `arrays` are assumed to have a weight equal to one.\n    ignore_nan : bool, optional\n        If True, NaNs are set to zero weight. Default is propagation of NaNs.\n    \n    Returns\n    -------\n    average : `~numpy.ndarray`\n        Average, possibly weighted.\n    var: `~numpy.ndarray`\n        Variance, possibly weighted.\n    \n    Notes\n    -----\n    Since the calculation of the variance requires knowledge of the average, this function is a\n    very thin wrapper around `var`.\n\n    References\n    ----------\n    .. [#] D. H. D. West, Updating the mean and variance estimates: an improved method.\n        Communications of the ACM Vol. 22, Issue 9, pp. 532 - 535 (1979)\n    """"""\n    # Since the variance calculation requires knowing the average,\n    # `average_and_var` runs in the exact same time as `var`\n    avg, sq_avg, swgt = last(\n        _ivar(arrays=arrays, axis=axis, weights=weights, ignore_nan=ignore_nan)\n    )\n    variance = (sq_avg - avg ** 2) * (swgt / (swgt - ddof))\n    return avg, variance\n\n\n@array_stream\ndef var(arrays, axis=-1, ddof=0, weights=None, ignore_nan=False):\n    """""" \n    Total variance of a stream of arrays. Weights are also supported. This function\n    consumes the input stream.\n    \n    Parameters\n    ----------\n    arrays : iterable of ndarrays\n        Arrays to be combined. This iterable can also a generator.\n    axis : int, optional\n        Reduction axis. Default is to combine the arrays in the stream as if \n        they had been stacked along a new axis, then compute the variance along this new axis.\n        If None, arrays are flattened. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, variance is computed\n        along the new axis.\n    ddof : int, optional\n        Means Delta Degrees of Freedom.  The divisor used in calculations\n        is ``N - ddof``, where ``N`` represents the number of elements.\n    weights : iterable of ndarray, iterable of floats, or None, optional\n        Iterable of weights associated with the values in each item of `arrays`. \n        Each value in an element of `arrays` contributes to the variance \n        according to its associated weight. The weights array can either be a float\n        or an array of the same shape as any element of `arrays`. If weights=None, \n        then all data in each element of `arrays` are assumed to have a weight equal to one.\n    ignore_nan : bool, optional\n        If True, NaNs are set to zero weight. Default is propagation of NaNs.\n    \n    Returns\n    -------\n    var: `~numpy.ndarray`\n        Variance. \n    \n    See Also\n    --------\n    ivar : streaming variance\n    numpy.var : variance calculation for dense arrays. Weights are not supported.\n    \n    References\n    ----------\n    .. [#] D. H. D. West, Updating the mean and variance estimates: an improved method.\n        Communications of the ACM Vol. 22, Issue 9, pp. 532 - 535 (1979)\n    """"""\n    _, variance = average_and_var(\n        arrays=arrays, axis=axis, ddof=ddof, weights=weights, ignore_nan=ignore_nan\n    )\n    return variance\n\n\n@array_stream\ndef ivar(arrays, axis=-1, ddof=0, weights=None, ignore_nan=False):\n    """""" \n    Streaming variance of arrays. Weights are also supported.\n    \n    Parameters\n    ----------\n    arrays : iterable of ndarrays\n        Arrays to be combined. This iterable can also a generator.\n    axis : int, optional\n        Reduction axis. Default is to combine the arrays in the stream as if \n        they had been stacked along a new axis, then compute the variance along this new axis.\n        If None, arrays are flattened. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, variance is computed\n        along the new axis.\n    ddof : int, optional\n        Means Delta Degrees of Freedom.  The divisor used in calculations\n        is ``N - ddof``, where ``N`` represents the number of elements.\n    weights : iterable of ndarray, iterable of floats, or None, optional\n        Iterable of weights associated with the values in each item of `arrays`. \n        Each value in an element of `arrays` contributes to the variance \n        according to its associated weight. The weights array can either be a float\n        or an array of the same shape as any element of `arrays`. If weights=None, \n        then all data in each element of `arrays` are assumed to have a weight equal to one.\n    ignore_nan : bool, optional\n        If True, NaNs are set to zero weight. Default is propagation of NaNs.\n    \n    Yields\n    ------\n    var: `~numpy.ndarray`\n        Variance. \n    \n    See Also\n    --------\n    numpy.var : variance calculation for dense arrays. Weights are not supported.\n    \n    References\n    ----------\n    .. [#] D. H. D. West, Updating the mean and variance estimates: an improved method.\n        Communications of the ACM Vol. 22, Issue 9, pp. 532 - 535 (1979)\n    """"""\n    primitive = _ivar(arrays=arrays, axis=axis, weights=weights, ignore_nan=ignore_nan)\n    for avg, sq_avg, swgt in primitive:\n        yield (sq_avg - avg ** 2) * (swgt / (swgt - ddof))\n\n\n@array_stream\ndef std(arrays, axis=-1, ddof=0, weights=None, ignore_nan=False):\n    """""" \n    Total standard deviation of arrays. Weights are also supported. This function\n    consumes the input stream.\n\n    Parameters\n    ----------\n    arrays : iterable of ndarrays\n        Arrays to be combined. This iterable can also a generator.\n    axis : int, optional\n        Reduction axis. Default is to combine the arrays in the stream as if \n        they had been stacked along a new axis, then compute the standard deviation along this new axis.\n        If None, arrays are flattened. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, standard deviation is computed\n        along the new axis.\n    ddof : int, optional\n        Means Delta Degrees of Freedom.  The divisor used in calculations\n        is ``N - ddof``, where ``N`` represents the number of elements.\n    weights : iterable of ndarray, iterable of floats, or None, optional\n        Iterable of weights associated with the values in each item of `arrays`. \n        Each value in an element of `arrays` contributes to the standard deviation \n        according to its associated weight. The weights array can either be a float\n        or an array of the same shape as any element of `arrays`. If weights=None, \n        then all data in each element of `arrays` are assumed to have a weight equal to one.\n    ignore_nan : bool, optional\n        If True, NaNs are set to zero weight. Default is propagation of NaNs.\n    \n    Returns\n    -------\n    std: `~numpy.ndarray`\n        Standard deviation\n\n    See Also\n    --------\n    istd : streaming standard deviation.\n    numpy.std : standard deviation calculation of dense arrays. Weights are not supported.\n    """"""\n    return np.sqrt(\n        var(arrays=arrays, axis=axis, ddof=ddof, weights=weights, ignore_nan=ignore_nan)\n    )\n\n\n@array_stream\ndef istd(arrays, axis=-1, ddof=0, weights=None, ignore_nan=False):\n    """""" \n    Streaming standard deviation of arrays. Weights are also supported.\n    This is equivalent to calling `numpy.std(axis = 2)` on a stack of images.\n\n    Parameters\n    ----------\n    arrays : iterable of ndarrays\n        Arrays to be combined. This iterable can also a generator.\n    axis : int, optional\n        Reduction axis. Default is to combine the arrays in the stream as if \n        they had been stacked along a new axis, then compute the standard deviation along this new axis.\n        If None, arrays are flattened. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, standard deviation is computed\n        along the new axis.\n    ddof : int, optional\n        Means Delta Degrees of Freedom.  The divisor used in calculations\n        is ``N - ddof``, where ``N`` represents the number of elements.\n    weights : iterable of ndarray, iterable of floats, or None, optional\n        Iterable of weights associated with the values in each item of `arrays`. \n        Each value in an element of `arrays` contributes to the standard deviation \n        according to its associated weight. The weights array can either be a float\n        or an array of the same shape as any element of `arrays`. If weights=None, \n        then all data in each element of `arrays` are assumed to have a weight equal to one.\n    ignore_nan : bool, optional\n        If True, NaNs are set to zero weight. Default is propagation of NaNs.\n    \n    Yields\n    ------\n    std: `~numpy.ndarray`\n        Standard deviation\n\n    See Also\n    --------\n    std : total standard deviation.\n    numpy.std : standard deviation calculation of dense arrays. Weights are not supported.\n    """"""\n    yield from map(\n        np.sqrt,\n        ivar(\n            arrays=arrays, axis=axis, ddof=ddof, weights=weights, ignore_nan=ignore_nan\n        ),\n    )\n\n\n@array_stream\ndef sem(arrays, axis=-1, ddof=0, weights=None, ignore_nan=False):\n    """""" \n    Standard error in the mean (SEM) of a stream of arrays. This function consumes\n    the entire stream.\n\n    Parameters\n    ----------\n    arrays : iterable of ndarrays\n        Arrays to be combined. This iterable can also a generator.\n    axis : int, optional\n        Reduction axis. Default is to combine the arrays in the stream as if \n        they had been stacked along a new axis, then compute the standard error along this new axis.\n        If None, arrays are flattened. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, standard error is computed\n        along the new axis.\n    ddof : int, optional\n        Means Delta Degrees of Freedom.  The divisor used in calculations\n        is ``N - ddof``, where ``N`` represents the number of elements.\n    weights : iterable of ndarray, iterable of floats, or None, optional\n        Iterable of weights associated with the values in each item of `arrays`. \n        Each value in an element of `arrays` contributes to the standard error \n        according to its associated weight. The weights array can either be a float\n        or an array of the same shape as any element of `arrays`. If weights=None, \n        then all data in each element of `arrays` are assumed to have a weight equal to one.\n    ignore_nan : bool, optional\n        If True, NaNs are set to zero weight. Default is propagation of NaNs.\n    \n    Returns\n    -------\n    sem: `~numpy.ndarray`, dtype float\n        Standard error in the mean. \n    \n    See Also\n    --------\n    scipy.stats.sem : standard error in the mean of dense arrays.\n    """"""\n    avg, sq_avg, swgt = last(\n        _ivar(arrays=arrays, axis=axis, weights=weights, ignore_nan=ignore_nan)\n    )\n    return np.sqrt((sq_avg - avg ** 2) * (1 / (swgt - ddof)))\n\n\n@array_stream\ndef isem(arrays, axis=-1, ddof=1, weights=None, ignore_nan=False):\n    """""" \n    Streaming standard error in the mean (SEM) of arrays. This is equivalent to\n    calling `scipy.stats.sem(axis = 2)` on a stack of images.\n\n    Parameters\n    ----------\n    arrays : iterable of ndarrays\n        Arrays to be combined. This iterable can also a generator.\n    axis : int, optional\n        Reduction axis. Default is to combine the arrays in the stream as if \n        they had been stacked along a new axis, then compute the standard error along this new axis.\n        If None, arrays are flattened. If `axis` is an int larger that\n        the number of dimensions in the arrays of the stream, standard error is computed\n        along the new axis.\n    ddof : int, optional\n        Means Delta Degrees of Freedom.  The divisor used in calculations\n        is ``N - ddof``, where ``N`` represents the number of elements.\n    weights : iterable of ndarray, iterable of floats, or None, optional\n        Iterable of weights associated with the values in each item of `arrays`. \n        Each value in an element of `arrays` contributes to the standard error \n        according to its associated weight. The weights array can either be a float\n        or an array of the same shape as any element of `arrays`. If weights=None, \n        then all data in each element of `arrays` are assumed to have a weight equal to one.\n    ignore_nan : bool, optional\n        If True, NaNs are set to zero weight. Default is propagation of NaNs.\n    \n    Yields\n    ------\n    sem: `~numpy.ndarray`, dtype float\n        Standard error in the mean. \n    \n    See Also\n    --------\n    scipy.stats.sem : standard error in the mean of dense arrays.\n    """"""\n    primitive = _ivar(arrays=arrays, axis=axis, weights=weights, ignore_nan=ignore_nan)\n    for avg, sq_avg, swgt in primitive:\n        yield np.sqrt((sq_avg - avg ** 2) * (1 / (swgt - ddof)))\n\n\n@array_stream\ndef ihistogram(arrays, bins, range=None, weights=None):\n    """"""\n    Streaming histogram calculation.\n\n    Parameters\n    ----------\n    arrays : iterable of ndarrays\n        Arrays to be combined. This iterable can also a generator. Arrays in this stream\n        can be of any shape; the histogram is computed over the flattened array.\n    bins : iterable\n        Bin edges, including the rightmost edge, allowing for non-uniform bin widths.\n        To determine the appropriate bins automatically, see ``numpy.histogram_bin_edges``.\n    weights : iterable of ndarray, iterable of floats, or None, optional\n        Iterable of weights associated with the values in each item of `arrays`. \n        Each value in a only contributes its associated weight towards the \n        bin count (instead of 1). The weights array can either be a float\n        or an array of the same shape as any element of `arrays`. If ``weights=None``, \n        then all data in each element of `arrays` are assumed to have a weight equal to one.\n\n        .. versionadded:: 1.6.1\n\n    Yields\n    ------\n    hist : `~numpy.ndarray`\n        Streamed histogram.\n    \n    See Also\n    --------\n    numpy.histogram : 1D histogram of dense arrays.\n    numpy.histogram_bin_edges : automatic selection of bins\n    """"""\n    bins = np.asarray(bins)\n    first, arrays = peek(arrays)\n\n    if weights is None:\n        weights = repeat(None)\n    else:\n        weights = map(partial(np.broadcast_to, shape=first.shape), weights)\n\n    # np.histogram also returns the bin edges, which we ignore\n    hist_func = lambda arr, wgt: np.histogram(arr, bins=bins, weights=wgt)[0]\n    yield from isum(starmap(hist_func, zip(arrays, weights)))\n'"
npstreams/utils.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom contextlib import contextmanager\nfrom functools import wraps\nfrom warnings import resetwarnings, simplefilter, warn\n\n\n@contextmanager\ndef contextwarnings(*args, **kwargs):\n    simplefilter(*args, **kwargs)\n    yield\n    resetwarnings()\n\n\ndef deprecated(message):\n    """""" \n    Decorator factory that warns of deprecation \n    \n    Parameters\n    ----------\n    message : str\n        Message will be dressed up with the name of the function.\n    \n    Returns\n    -------\n    decorator : callable\n    """"""\n\n    def decorator(func):\n        @wraps(func)\n        def newfunc(*args, **kwargs):\n            full_message = f""""""Calls to {func.__name__} deprecated: {message}. \n            {name} will be removed in a future release.""""""\n            with contextwarnings(""always"", DeprecationWarning):\n                warn(full_message, category=DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n\n        return newfunc\n\n    return decorator\n'"
tests/__init__.py,0,b''
tests/test_array_stream.py,5,"b'# -*- coding: utf-8 -*-\nimport unittest\nimport numpy as np\n\nfrom npstreams.array_stream import array_stream, ArrayStream\n\n\n@array_stream\ndef iden(arrays):\n    yield from arrays\n\n\nclass TestArrayStreamDecorator(unittest.TestCase):\n    def test_type(self):\n        """""" Test that all object from an array stream are ndarrays """"""\n\n        stream = [0, 1, np.array([1])]\n        for arr in iden(stream):\n            self.assertIsInstance(arr, np.ndarray)\n\n    def test_single_array(self):\n        """""" Test that a \'stream\' consisting of a single array is repackaged into an iterable """"""\n        stream = np.array([1, 2, 3])\n        self.assertEqual(len(list(iden(stream))), 1)\n\n\nclass TestArrayStream(unittest.TestCase):\n    def test_length_hint_sized_iterable(self):\n        """""" Test the accuracy of __length_hint__ for ArrayStream constructed\n        from a sized iterable """"""\n        iterable = [1, 2, 3, 4, 5]\n        a = ArrayStream(iterable)\n        self.assertEqual(len(iterable), a.__length_hint__())\n\n    def test_length_hint_not_sized_iterable(self):\n        """""" Test that __length_hint__ returns NotImplemented for ArrayStream constructed\n        from an unsized iterable """"""\n        iterable = (0 for _ in range(10))\n        a = ArrayStream(iterable)\n        self.assertIs(a.__length_hint__(), NotImplemented)\n\n    def test_conversion_to_array(self):\n        """""" Test that numpy.array(Arraystream(...)) returns an array built as a stack of arrays """"""\n        a = ArrayStream([np.random.random((16, 16)) for _ in range(10)])\n        arr = np.array(a)\n        self.assertEqual(arr.shape, (16, 16, 10))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_array_utils.py,3,"b'import unittest\nimport numpy as np\nfrom npstreams import nan_to_num\n\n\nclass TestNanToNum(unittest.TestCase):\n    def test_generic(self):\n        """""" Test that NaNs are replaced with a fill value """"""\n        with np.errstate(divide=""ignore"", invalid=""ignore""):\n            vals = nan_to_num(np.array([0]) / 0.0, fill_value=14)\n        self.assertEqual(vals[0], 14)\n\n    def test_integer(self):\n        """""" Test that nan_to_num on integers does nothing """"""\n        vals = nan_to_num(1)\n        self.assertEqual(vals, 1)\n        vals = nan_to_num([1])\n        self.assertTrue(np.allclose(vals, np.array([1])))\n\n    def test_complex_good(self):\n        """""" Test nan_to_num on complex input """"""\n        vals = nan_to_num(1 + 1j)\n        self.assertEqual(vals, 1 + 1j)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_cuda.py,30,"b'# -*- coding: utf-8 -*-\n\nfrom itertools import repeat\nimport numpy as np\nimport unittest\n\ntry:\n    from npstreams.cuda import csum, cprod, caverage, cmean\n\n    WITH_CUDA = True\nexcept ImportError:\n    WITH_CUDA = False\n\n\n@unittest.skipIf(not WITH_CUDA, ""PyCUDA is not installed/available"")\nclass CudaTestCase(unittest.TestCase):\n    pass\n\n\nclass TestCSum(CudaTestCase):\n    def test_zero_sum(self):\n        stream = repeat(np.zeros((16, 16), dtype=np.float), times=5)\n        s = csum(stream)\n        self.assertTrue(np.allclose(s, np.zeros((16, 16))))\n\n    def test_dtype(self):\n        stream = repeat(np.zeros((16, 16), dtype=np.float), times=5)\n        s = csum(stream, dtype=np.int16)\n        self.assertTrue(np.allclose(s, np.zeros((16, 16))))\n        self.assertEqual(s.dtype, np.int16)\n\n    def test_ignore_nans(self):\n        """""" Test a sum of zeros with NaNs sprinkled """"""\n        source = [np.zeros((16,), dtype=np.float) for _ in range(10)]\n        source.append(np.full((16,), fill_value=np.nan))\n        summed = csum(source, ignore_nan=True)\n        self.assertTrue(np.allclose(summed, np.zeros_like(summed)))\n\n\nclass TestCProd(CudaTestCase):\n    def test_ones_prod(self):\n        stream = repeat(np.ones((16, 16), dtype=np.float), times=5)\n        s = cprod(stream)\n        self.assertTrue(np.allclose(s, np.ones((16, 16))))\n\n    def test_ignore_nans(self):\n        """""" Test that NaNs are ignored. """"""\n        source = [np.ones((16,), dtype=np.float) for _ in range(10)]\n        source.append(np.full_like(source[0], np.nan))\n        product = cprod(source, ignore_nan=True)\n        self.assertTrue(np.allclose(product, np.ones_like(product)))\n\n    def test_dtype(self):\n        """""" Test that dtype argument is working """"""\n        source = [np.ones((16,), dtype=np.float) for _ in range(10)]\n        product = cprod(source, dtype=np.int)\n        self.assertTrue(np.allclose(product, np.ones_like(product)))\n        self.assertEqual(product.dtype, np.int)\n\n\nclass TestCAverage(CudaTestCase):\n    def test_avg_no_weights(self):\n        stream = [np.random.random(size=(16, 16)) for _ in range(5)]\n        from_caverage = caverage(stream)\n        from_numpy = np.average(np.dstack(stream), axis=2)\n        self.assertTrue(np.allclose(from_caverage, from_numpy))\n\n    def test_weighted_average(self):\n        """""" Test results of weighted average against numpy.average """"""\n        stream = [np.random.random(size=(16, 16)) for _ in range(5)]\n\n        weights = [np.random.random(size=stream[0].shape) for _ in stream]\n        from_caverage = caverage(stream, weights=weights)\n        from_numpy = np.average(np.dstack(stream), axis=2, weights=np.dstack(weights))\n        self.assertTrue(np.allclose(from_caverage, from_numpy))\n\n\nclass TestCMean(CudaTestCase):\n    def test_mean_of_ones(self):\n        stream = repeat(np.ones((16, 16), dtype=np.float), times=5)\n        s = cmean(stream)\n        self.assertTrue(np.allclose(s, np.ones((16, 16))))\n\n    def test_mean_random(self):\n        """""" Test cmean against numpy.mean on random data """"""\n        stream = [np.random.random(size=(16, 16)) for _ in range(5)]\n        from_cmean = cmean(stream)\n        from_numpy = np.mean(np.dstack(stream), axis=2)\n        self.assertTrue(np.allclose(from_cmean, from_numpy))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_flow.py,26,"b'# -*- coding: utf-8 -*-\nimport unittest\nimport numpy as np\n\nfrom npstreams import array_stream, ipipe, last, iload, pload, isum\n\n\n@array_stream\ndef iden(arrays):\n    yield from arrays\n\n\nclass TestIPipe(unittest.TestCase):\n    def test_order(self):\n        """""" Test that ipipe(f, g, h, arrays) -> f(g(h(arr))) for arr in arrays """"""\n        stream = [np.random.random((15, 7, 2, 1)) for _ in range(10)]\n        squared = [np.cbrt(np.square(arr)) for arr in stream]\n        pipeline = ipipe(np.cbrt, np.square, stream)\n\n        self.assertTrue(all(np.allclose(s, p) for s, p in zip(pipeline, squared)))\n\n    def test_multiprocessing(self):\n        """""" Test that ipipe(f, g, h, arrays) -> f(g(h(arr))) for arr in arrays """"""\n        stream = [np.random.random((15, 7, 2, 1)) for _ in range(10)]\n        squared = [np.cbrt(np.square(arr)) for arr in stream]\n        pipeline = ipipe(np.cbrt, np.square, stream, processes=2)\n\n        self.assertTrue(all(np.allclose(s, p) for s, p in zip(pipeline, squared)))\n\n\nclass TestILoad(unittest.TestCase):\n    def test_glob(self):\n        """""" Test that iload works on glob-like patterns """"""\n        stream = iload(""tests\\\\data\\\\test_data*.npy"", load_func=np.load)\n        s = last(isum(stream)).astype(np.float)  # Cast to float for np.allclose\n        self.assertTrue(np.allclose(s, np.zeros_like(s)))\n\n    def test_file_list(self):\n        """""" Test that iload works on iterable of filenames """"""\n        files = [\n            ""tests\\\\data\\\\test_data1.npy"",\n            ""tests\\\\data\\\\test_data2.npy"",\n            ""tests\\\\data\\\\test_data3.npy"",\n        ]\n        stream = iload(files, load_func=np.load)\n        s = last(isum(stream)).astype(np.float)  # Cast to float for np.allclose\n        self.assertTrue(np.allclose(s, np.zeros_like(s)))\n\n\nclass TestPLoad(unittest.TestCase):\n    def test_glob(self):\n        """""" Test that pload works on glob-like patterns """"""\n        with self.subTest(""processes = 1""):\n            stream = pload(""tests\\\\data\\\\test_data*.npy"", load_func=np.load)\n            s = last(isum(stream)).astype(np.float)  # Cast to float for np.allclose\n            self.assertTrue(np.allclose(s, np.zeros_like(s)))\n\n        with self.subTest(""processes = 2""):\n            stream = pload(\n                ""tests\\\\data\\\\test_data*.npy"", load_func=np.load, processes=2\n            )\n            s = last(isum(stream)).astype(np.float)  # Cast to float for np.allclose\n            self.assertTrue(np.allclose(s, np.zeros_like(s)))\n\n    def test_file_list(self):\n        """""" Test that pload works on iterable of filenames """"""\n        with self.subTest(""processes = 1""):\n            files = [\n                ""tests\\\\data\\\\test_data1.npy"",\n                ""tests\\\\data\\\\test_data2.npy"",\n                ""tests\\\\data\\\\test_data3.npy"",\n            ]\n            stream = pload(files, load_func=np.load)\n            s = last(isum(stream)).astype(np.float)  # Cast to float for np.allclose\n            self.assertTrue(np.allclose(s, np.zeros_like(s)))\n\n        with self.subTest(""processes = 2""):\n            files = [\n                ""tests\\\\data\\\\test_data1.npy"",\n                ""tests\\\\data\\\\test_data2.npy"",\n                ""tests\\\\data\\\\test_data3.npy"",\n            ]\n            stream = pload(files, load_func=np.load, processes=2)\n            s = last(isum(stream)).astype(np.float)  # Cast to float for np.allclose\n            self.assertTrue(np.allclose(s, np.zeros_like(s)))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_iter_utils.py,0,"b'# -*- coding: utf-8 -*-\nimport unittest\nfrom itertools import repeat\nfrom npstreams import last, chunked, linspace, multilinspace, cyclic, length_hint\n\n\nclass TestLast(unittest.TestCase):\n    def test_trivial(self):\n        """""" Test last() on iterable of identical values """"""\n        i = repeat(1, 10)\n        self.assertEqual(last(i), 1)\n\n    def test_on_empty_iterable(self):\n        """""" Test that last() raises RuntimeError for empty iterable """"""\n        with self.assertRaises(RuntimeError):\n            last(list())\n\n\nclass TestCyclic(unittest.TestCase):\n    def test_numbers(self):\n        """""" """"""\n        permutations = set(cyclic((1, 2, 3)))\n        self.assertIn((1, 2, 3), permutations)\n        self.assertIn((2, 3, 1), permutations)\n        self.assertIn((3, 1, 2), permutations)\n        self.assertEqual(len(permutations), 3)\n\n\nclass TestLinspace(unittest.TestCase):\n    def test_endpoint(self):\n        """""" Test that the endpoint is included by linspace() when appropriate""""""\n        with self.subTest(""endpoint = True""):\n            space = linspace(0, 1, num=10, endpoint=True)\n            self.assertEqual(last(space), 1)\n\n        with self.subTest(""endpoint = False""):\n            space = linspace(0, 1, num=10, endpoint=False)\n            self.assertAlmostEqual(last(space), 0.9)\n\n    def test_length(self):\n        """""" Test that linspace() returns an iterable of the correct length """"""\n        with self.subTest(""endpoint = True""):\n            space = list(linspace(0, 1, num=13, endpoint=True))\n            self.assertEqual(len(space), 13)\n\n        with self.subTest(""endpoint = False""):\n            space = list(linspace(0, 1, num=13, endpoint=False))\n            self.assertEqual(len(space), 13)\n\n\nclass TestMultilinspace(unittest.TestCase):\n    def test_endpoint(self):\n        """""" Test that the endpoint is included by linspace() when appropriate""""""\n        with self.subTest(""endpoint = True""):\n            space = multilinspace((0, 0), (1, 1), num=10, endpoint=True)\n            self.assertSequenceEqual(last(space), (1, 1))\n\n        with self.subTest(""endpoint = False""):\n            space = multilinspace((0, 0), (1, 1), num=10, endpoint=False)\n            # Unfortunately there is no assertSequenceAlmostEqual\n            self.assertSequenceEqual(\n                last(space), (0.8999999999999999, 0.8999999999999999)\n            )\n\n    def test_length(self):\n        """""" Test that linspace() returns an iterable of the correct length """"""\n        with self.subTest(""endpoint = True""):\n            space = list(multilinspace((0, 0), (1, 1), num=13, endpoint=True))\n            self.assertEqual(len(space), 13)\n\n        with self.subTest(""endpoint = False""):\n            space = list(multilinspace((0, 0), (1, 1), num=13, endpoint=False))\n            self.assertEqual(len(space), 13)\n\n\nclass TestChunked(unittest.TestCase):\n    def test_larger_chunksize(self):\n        """""" Test chunked() with a chunksize larger that the iterable itself """"""\n        i = repeat(1, 10)\n        chunks = chunked(i, chunksize=15)\n        self.assertEqual(len(list(chunks)), 1)  # One single chunk is returned\n\n    def test_on_infinite_generator(self):\n        """""" Test chunked() on an infinite iterable """"""\n        i = repeat(1)\n        chunks = chunked(i, chunksize=15)\n        for _ in range(10):\n            self.assertEqual(len(next(chunks)), 15)\n\n    def test_chunked_nonint_chunksize(self):\n        """""" Test that chunked raises a TypeError immediately if `chunksize` is not an integer """"""\n        with self.assertRaises(TypeError):\n            i = repeat(1)\n            chunks = chunked(i, chunksize=15.0)\n\n\nclass TestLengthHint(unittest.TestCase):\n    def test_on_sized(self):\n        """""" Test length_hint on a sized iterable """"""\n        l = [1, 2, 3, 4, 5]\n        self.assertEqual(length_hint(l), len(l))\n\n    def test_on_unsized(self):\n        """""" Test length_hint on an unsized iterable returns the default """"""\n        l = (0 for _ in range(10))\n        self.assertEqual(length_hint(l, default=0), 0)\n\n    def test_on_method_if_implemented(self):\n        """""" Test length_hint returns the same as __length_hint__ if implemented """"""\n\n        class WithHint:\n            """""" Some dummy class with a length hint """"""\n\n            def __length_hint__(self):\n                return 1\n\n        self.assertEqual(length_hint(WithHint(), default=0), 1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_linalg.py,13,"b'# -*- coding: utf-8 -*-\nimport unittest\nfrom random import randint, random\n\nimport numpy as np\n\nfrom npstreams import idot, itensordot, iinner, ieinsum, last\n\n\nclass TestIDot(unittest.TestCase):\n    def test_against_numpy_multidot(self):\n        """""" Test against numpy.linalg.multi_dot in 2D case """"""\n        stream = [np.random.random((8, 8)) for _ in range(7)]\n\n        from_numpy = np.linalg.multi_dot(stream)\n        from_stream = last(idot(stream))\n\n        self.assertSequenceEqual(from_numpy.shape, from_stream.shape)\n        self.assertTrue(np.allclose(from_numpy, from_stream))\n\n\nclass TestITensordot(unittest.TestCase):\n    def test_against_numpy_tensordot(self):\n        """""" Test against numpy.tensordot in 2D case """"""\n        stream = tuple(np.random.random((8, 8)) for _ in range(2))\n\n        for axis in (0, 1, 2):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.tensordot(*stream)\n                from_stream = last(itensordot(stream))\n\n                self.assertSequenceEqual(from_numpy.shape, from_stream.shape)\n                self.assertTrue(np.allclose(from_numpy, from_stream))\n\n\nclass TestIInner(unittest.TestCase):\n    def test_against_numpy_inner(self):\n        """""" Test against numpy.tensordot in 2D case """"""\n        stream = tuple(np.random.random((8, 8)) for _ in range(2))\n\n        for axis in (0, 1, 2):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.inner(*stream)\n                from_stream = last(iinner(stream))\n\n                self.assertSequenceEqual(from_numpy.shape, from_stream.shape)\n                self.assertTrue(np.allclose(from_numpy, from_stream))\n\n\nclass TestIEinsum(unittest.TestCase):\n    def test_against_numpy_einsum(self):\n        """""" Test against numpy.einsum  """"""\n        a = np.arange(60.0).reshape(3, 4, 5)\n        b = np.arange(24.0).reshape(4, 3, 2)\n        stream = [a, b]\n\n        from_numpy = np.einsum(""ijk,jil->kl"", a, b)\n        from_stream = last(ieinsum(stream, ""ijk,jil->kl""))\n\n        self.assertSequenceEqual(from_numpy.shape, from_stream.shape)\n        self.assertTrue(np.allclose(from_numpy, from_stream))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_numerics.py,83,"b'# -*- coding: utf-8 -*-\nimport unittest\nfrom random import randint, random\n\nimport numpy as np\n\nfrom npstreams import isum, iprod, last, isub, iany, iall, prod\nfrom npstreams import sum as nssum  # avoiding name clashes\n\n\nclass TestISum(unittest.TestCase):\n    def test_trivial(self):\n        """""" Test a sum of zeros """"""\n        source = [np.zeros((16,), dtype=np.float) for _ in range(10)]\n        summed = last(isum(source))\n        self.assertTrue(np.allclose(summed, np.zeros_like(summed)))\n\n    def test_ignore_nans(self):\n        """""" Test a sum of zeros with NaNs sprinkled """"""\n        source = [np.zeros((16,), dtype=np.float) for _ in range(10)]\n        source.append(np.full((16,), fill_value=np.nan))\n        summed = last(isum(source, ignore_nan=True))\n        self.assertTrue(np.allclose(summed, np.zeros_like(summed)))\n\n    def test_length(self):\n        """""" Test that the number of yielded elements is the same as source """"""\n        source = [np.zeros((16,), dtype=np.float) for _ in range(10)]\n        summed = list(isum(source, axis=0))\n        self.assertEqual(10, len(summed))\n\n    def test_dtype(self):\n        """""" Test a sum of floating zeros with an int accumulator """"""\n        source = [np.zeros((16,), dtype=np.float) for _ in range(10)]\n        summed = last(isum(source, dtype=np.int))\n        self.assertTrue(np.allclose(summed, np.zeros_like(summed)))\n        self.assertEqual(summed.dtype, np.int)\n\n    def test_axis(self):\n        """""" Test that isum(axis = 0) yields 0d arrays """"""\n        source = [np.zeros((16,), dtype=np.float) for _ in range(10)]\n\n        with self.subTest(""axis = 0""):\n            summed = last(isum(source, axis=0))\n            self.assertTrue(np.allclose(summed, np.zeros_like(summed)))\n\n        with self.subTest(""axis = None""):\n            summed = last(isum(source, axis=None))\n            self.assertTrue(np.allclose(summed, 0))\n\n    def test_return_shape(self):\n        """""" Test that the shape of output is as expected """"""\n        source = [np.zeros((16,), dtype=np.float) for _ in range(10)]\n\n        with self.subTest(""axis = 0""):\n            summed = last(isum(source, axis=0))\n            self.assertSequenceEqual(summed.shape, (1, 10))\n\n    def test_against_numpy(self):\n        """""" Test that isum() returns the same as numpy.sum() for various axis inputs """"""\n\n        stream = [np.random.random((16, 16)) for _ in range(10)]\n        stack = np.dstack(stream)\n\n        for axis in (0, 1, 2, None):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.sum(stack, axis=axis)\n                from_isum = last(isum(stream, axis=axis))\n                self.assertTrue(np.allclose(from_isum, from_numpy))\n\n\nclass TestSum(unittest.TestCase):\n    def test_trivial(self):\n        """""" Test a sum of zeros """"""\n        source = [np.zeros((16,), dtype=np.float) for _ in range(10)]\n        summed = nssum(source)\n        self.assertTrue(np.allclose(summed, np.zeros_like(summed)))\n\n    def test_ignore_nans(self):\n        """""" Test a sum of zeros with NaNs sprinkled """"""\n        source = [np.zeros((16,), dtype=np.float) for _ in range(10)]\n        source.append(np.full((16,), fill_value=np.nan))\n        summed = nssum(source, ignore_nan=True)\n        self.assertTrue(np.allclose(summed, np.zeros_like(summed)))\n\n    def test_dtype(self):\n        """""" Test a sum of floating zeros with an int accumulator """"""\n        source = [np.zeros((16,), dtype=np.float) for _ in range(10)]\n        summed = nssum(source, dtype=np.int)\n        self.assertTrue(np.allclose(summed, np.zeros_like(summed)))\n        self.assertEqual(summed.dtype, np.int)\n\n    def test_axis(self):\n        """""" Test that isum(axis = 0) yields 0d arrays """"""\n        source = [np.zeros((16,), dtype=np.float) for _ in range(10)]\n\n        with self.subTest(""axis = 0""):\n            summed = nssum(source, axis=0)\n            self.assertTrue(np.allclose(summed, np.zeros_like(summed)))\n\n        with self.subTest(""axis = None""):\n            summed = nssum(source, axis=None)\n            self.assertTrue(np.allclose(summed, 0))\n\n    def test_return_shape(self):\n        """""" Test that the shape of output is as expected """"""\n        source = [np.zeros((16,), dtype=np.float) for _ in range(10)]\n\n        with self.subTest(""axis = 0""):\n            summed = nssum(source, axis=0)\n            self.assertSequenceEqual(summed.shape, (1, 10))\n\n    def test_against_numpy(self):\n        """""" Test that isum() returns the same as numpy.sum() for various axis inputs """"""\n\n        stream = [np.random.random((16, 16)) for _ in range(10)]\n        stack = np.dstack(stream)\n\n        for axis in (0, 1, 2, None):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.sum(stack, axis=axis)\n                from_sum = nssum(stream, axis=axis)\n                self.assertTrue(np.allclose(from_sum, from_numpy))\n\n\nclass TestIProd(unittest.TestCase):\n    def test_trivial(self):\n        """""" Test a product of ones """"""\n        source = [np.ones((16,), dtype=np.float) for _ in range(10)]\n        product = last(iprod(source))\n        self.assertTrue(np.allclose(product, np.ones_like(product)))\n\n    def test_ignore_nans(self):\n        """""" Test that NaNs are ignored. """"""\n        source = [np.ones((16,), dtype=np.float) for _ in range(10)]\n        source.append(np.full_like(source[0], np.nan))\n        product = last(iprod(source, ignore_nan=True))\n        self.assertTrue(np.allclose(product, np.ones_like(product)))\n\n    def test_dtype(self):\n        """""" Test that dtype argument is working """"""\n        source = [np.ones((16,), dtype=np.float) for _ in range(10)]\n        product = last(iprod(source, dtype=np.int))\n        self.assertTrue(np.allclose(product, np.ones_like(product)))\n        self.assertEqual(product.dtype, np.int)\n\n    def test_axis(self):\n        """""" Test that iprod(axis = 0) yields 0d arrays """"""\n        source = [np.ones((16,), dtype=np.float) for _ in range(10)]\n\n        with self.subTest(""axis = 0""):\n            summed = last(iprod(source, axis=0))\n            self.assertTrue(np.all(summed == 1))\n\n        with self.subTest(""axis = None""):\n            summed = last(iprod(source, axis=None))\n            self.assertTrue(np.allclose(summed, np.ones_like(summed)))\n\n    def test_against_numpy(self):\n        """""" Test that iprod() returns the same as numpy.prod() for various axis inputs """"""\n\n        stream = [np.random.random((16, 16)) for _ in range(10)]\n        stack = np.dstack(stream)\n\n        for axis in (0, 1, 2, None):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.prod(stack, axis=axis)\n                from_stream = last(iprod(stream, axis=axis))\n                self.assertTrue(np.allclose(from_stream, from_numpy))\n\n\nclass TestProd(unittest.TestCase):\n    def test_trivial(self):\n        """""" Test a product of ones """"""\n        source = [np.ones((16,), dtype=np.float) for _ in range(10)]\n        product = prod(source)\n        self.assertTrue(np.allclose(product, np.ones_like(product)))\n\n    def test_ignore_nans(self):\n        """""" Test that NaNs are ignored. """"""\n        source = [np.ones((16,), dtype=np.float) for _ in range(10)]\n        source.append(np.full_like(source[0], np.nan))\n        product = prod(source, ignore_nan=True)\n        self.assertTrue(np.allclose(product, np.ones_like(product)))\n\n    def test_dtype(self):\n        """""" Test that dtype argument is working """"""\n        source = [np.ones((16,), dtype=np.float) for _ in range(10)]\n        product = prod(source, dtype=np.int)\n        self.assertTrue(np.allclose(product, np.ones_like(product)))\n        self.assertEqual(product.dtype, np.int)\n\n    def test_axis(self):\n        """""" Test that iprod(axis = 0) yields 0d arrays """"""\n        source = [np.ones((16,), dtype=np.float) for _ in range(10)]\n\n        with self.subTest(""axis = 0""):\n            summed = prod(source, axis=0)\n            self.assertTrue(np.all(summed == 1))\n\n        with self.subTest(""axis = None""):\n            summed = prod(source, axis=None)\n            self.assertTrue(np.allclose(summed, np.ones_like(summed)))\n\n    def test_against_numpy(self):\n        """""" Test that iprod() returns the same as numpy.prod() for various axis inputs """"""\n\n        stream = [np.random.random((16, 16)) for _ in range(10)]\n        stack = np.dstack(stream)\n\n        for axis in (0, 1, 2, None):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.prod(stack, axis=axis)\n                from_stream = prod(stream, axis=axis)\n                self.assertTrue(np.allclose(from_stream, from_numpy))\n\n\nclass TestISub(unittest.TestCase):\n    def test_against_numpy(self):\n        """""" Test against numpy.subtract.reduce """"""\n        stream = [np.random.random((8, 16, 2)) for _ in range(11)]\n        stack = np.stack(stream, axis=-1)\n\n        for axis in range(stack.ndim):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.subtract.reduce(stack, axis=axis)\n                from_stream = last(isub(stream, axis=axis))\n                self.assertTrue(np.allclose(from_numpy, from_stream))\n\n\nclass TestIAll(unittest.TestCase):\n    def test_against_numpy(self):\n        """""" Test iall against numpy.all """"""\n        stream = [np.zeros((8, 16, 2)) for _ in range(11)]\n        stream[3][3, 0, 1] = 1  # so that np.all(axis = None) evaluates to False\n        stack = np.stack(stream, axis=-1)\n\n        with self.subTest(""axis = None""):\n            from_numpy = np.all(stack, axis=None)\n            from_stream = last(iall(stream, axis=None))\n            self.assertEqual(from_numpy, from_stream)\n\n        for axis in range(stack.ndim):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.all(stack, axis=axis)\n                from_stream = last(iall(stream, axis=axis))\n                self.assertTrue(np.allclose(from_numpy, from_stream))\n\n\nclass TestIAny(unittest.TestCase):\n    def test_against_numpy(self):\n        """""" Test iany against numpy.any """"""\n        stream = [np.zeros((8, 16, 2)) for _ in range(11)]\n        stream[3][3, 0, 1] = 1  # so that np.all(axis = None) evaluates to False\n        stack = np.stack(stream, axis=-1)\n\n        with self.subTest(""axis = None""):\n            from_numpy = np.any(stack, axis=None)\n            from_stream = last(iany(stream, axis=None))\n            self.assertEqual(from_numpy, from_stream)\n\n        for axis in range(stack.ndim):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.any(stack, axis=axis)\n                from_stream = last(iany(stream, axis=axis))\n                self.assertTrue(np.allclose(from_numpy, from_stream))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_parallel.py,2,"b'# -*- coding: utf-8 -*-\nfrom npstreams import pmap, pmap_unordered, preduce\nfrom functools import reduce\nimport numpy as np\nfrom operator import add\nimport unittest\n\n\ndef identity(obj, *args, **kwargs):\n    """""" ignores args and kwargs """"""\n    return obj\n\n\nclass TestParallelReduce(unittest.TestCase):\n    def test_preduce_one_process(self):\n        """""" Test that preduce reduces to functools.reduce for a single process """"""\n        integers = list(range(0, 10))\n        preduce_results = preduce(add, integers, processes=1)\n        reduce_results = reduce(add, integers)\n\n        self.assertEqual(preduce_results, reduce_results)\n\n    def test_preduce_multiple_processes(self):\n        """""" Test that preduce reduces to functools.reduce for a single process """"""\n        integers = list(range(0, 10))\n        preduce_results = preduce(add, integers, processes=2)\n        reduce_results = reduce(add, integers)\n\n        self.assertEqual(preduce_results, reduce_results)\n\n    def test_on_numpy_arrays(self):\n        """""" Test sum of numpy arrays as parallel reduce""""""\n        arrays = [np.zeros((32, 32)) for _ in range(10)]\n        s = preduce(add, arrays, processes=2)\n\n        self.assertTrue(np.allclose(s, arrays[0]))\n\n    def test_with_kwargs(self):\n        """""" Test preduce with keyword-arguments """"""\n        pass\n\n\nclass TestParallelMap(unittest.TestCase):\n    def test_trivial_map_no_args(self):\n        """""" Test that pmap is working with no positional arguments """"""\n        integers = list(range(0, 10))\n        result = list(pmap(identity, integers, processes=2))\n        self.assertEqual(integers, result)\n\n    def test_trivial_map_kwargs(self):\n        """""" Test that pmap is working with args and kwargs """"""\n        integers = list(range(0, 10))\n        result = list(pmap(identity, integers, processes=2, kwargs={""test"": True}))\n        self.assertEqual(result, integers)\n\n\nclass TestParallelMap(unittest.TestCase):\n    def test_trivial_map_no_args(self):\n        """""" Test that pmap_unordered is working with no positional arguments """"""\n        integers = list(range(0, 10))\n        result = list(sorted(pmap_unordered(identity, integers, processes=2)))\n        self.assertEqual(integers, result)\n\n    def test_trivial_map_kwargs(self):\n        """""" Test that pmap_unordered is working with args and kwargs """"""\n        integers = list(range(0, 10))\n        result = list(\n            sorted(\n                pmap_unordered(identity, integers, processes=2, kwargs={""test"": True})\n            )\n        )\n        self.assertEqual(result, integers)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_reduce.py,57,"b'# -*- coding: utf-8 -*-\nimport unittest\nimport numpy as np\n\nfrom npstreams import ireduce_ufunc, preduce_ufunc, last, nan_to_num, reduce_ufunc\n\n# Only testing binary ufuncs that support floats\n# i.e. leaving bitwise_* and logical_* behind\n# Also, numpy.ldexp takes in ints and floats separately, so\n# leave it behind\nUFUNCS = (\n    np.add,\n    np.subtract,\n    np.multiply,\n    np.divide,\n    np.logaddexp,\n    np.logaddexp2,\n    np.true_divide,\n    np.floor_divide,\n    np.power,\n    np.remainder,\n    np.mod,\n    np.fmod,\n    np.arctan2,\n    np.hypot,\n    np.maximum,\n    np.fmax,\n    np.minimum,\n    np.fmin,\n    np.copysign,\n    np.nextafter,\n)\n\n\nclass TestIreduceUfunc(unittest.TestCase):\n    def setUp(self):\n        self.source = [np.random.random((16, 5, 8)) for _ in range(10)]\n        self.stack = np.stack(self.source, axis=-1)\n\n    def test_no_side_effects(self):\n        """""" Test that no arrays in the stream are modified """"""\n        for arr in self.source:\n            arr.setflags(write=False)\n        out = last(ireduce_ufunc(self.source, np.add))\n\n    def test_single_array(self):\n        """""" Test ireduce_ufunc on a single array, not a sequence """"""\n        source = np.ones((16, 16), dtype=np.int)\n        out = last(ireduce_ufunc(source, np.add, axis=-1))\n        self.assertTrue(np.allclose(source, out))\n\n    def test_out_parameter(self):\n        """""" Test that the kwargs ``out`` is correctly passed to reduction function """"""\n\n        with self.subTest(""axis = -1""):\n            not_out = last(ireduce_ufunc(self.source, np.add, axis=-1))\n            out = np.empty_like(self.source[0])\n            last(ireduce_ufunc(self.source, ufunc=np.add, out=out))\n\n            self.assertTrue(np.allclose(not_out, out))\n\n        with self.subTest(""axis != -1""):\n            not_out = last(ireduce_ufunc(self.source, np.add, axis=2))\n            out = np.empty_like(self.source[0])\n            from_out = last(ireduce_ufunc(self.source, ufunc=np.add, out=out, axis=2))\n\n            self.assertTrue(np.allclose(not_out, from_out))\n\n    def test_ignore_nan_no_identity(self):\n        """""" Test ireduce_ufunc on an ufunc with no identity raises\n        an error for ignore_nan = True """"""\n        source = [np.ones((16, 16), dtype=np.int) for _ in range(5)]\n        with self.assertRaises(ValueError):\n            ireduce_ufunc(source, np.maximum, axis=-1, ignore_nan=True)\n\n    def test_non_ufunc(self):\n        """""" Test that ireduce_ufunc raises TypeError when a non-ufunc is passed """"""\n        with self.assertRaises(TypeError):\n            ireduce_ufunc(range(10), ufunc=lambda x: x)\n\n    def test_non_binary_ufunc(self):\n        """""" Test that ireduce_ufunc raises ValueError if non-binary ufunc is used """"""\n        with self.assertRaises(ValueError):\n            ireduce_ufunc(range(10), ufunc=np.absolute)\n\n    def test_output_shape(self):\n        """""" Test output shape """"""\n        for axis in (0, 1, 2, 3, None):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.add.reduce(self.stack, axis=axis)\n                out = last(ireduce_ufunc(self.source, np.add, axis=axis))\n                self.assertSequenceEqual(from_numpy.shape, out.shape)\n                self.assertTrue(np.allclose(out, from_numpy))\n\n    def test_length(self):\n        """""" Test that the number of elements yielded by ireduce_ufunc is correct """"""\n        for axis in (0, 1, 2, 3, None):\n            with self.subTest(f""axis = {axis}""):\n                source = (np.zeros((16, 5, 8)) for _ in range(10))\n                out = list(ireduce_ufunc(source, np.add, axis=axis))\n                self.assertEqual(10, len(out))\n\n    def test_ignore_nan(self):\n        """""" Test that ignore_nan is working """"""\n        for axis in (0, 1, 2, 3, None):\n            with self.subTest(f""axis = {axis}""):\n                out = last(\n                    ireduce_ufunc(self.source, np.add, axis=axis, ignore_nan=True)\n                )\n                self.assertFalse(np.any(np.isnan(out)))\n\n\nclass TestPreduceUfunc(unittest.TestCase):\n    def test_trivial(self):\n        """""" Test preduce_ufunc for a sum of zeroes over two processes""""""\n        stream = [np.zeros((8, 8)) for _ in range(10)]\n        s = preduce_ufunc(stream, ufunc=np.add, processes=2, ntotal=10)\n        self.assertTrue(np.allclose(s, np.zeros_like(s)))\n\n    def test_correctess(self):\n        """""" Test preduce_ufunc is equivalent to reduce_ufunc for random sums""""""\n        stream = [np.random.random((8, 8)) for _ in range(20)]\n        s = preduce_ufunc(stream, ufunc=np.add, processes=3, ntotal=10)\n        self.assertTrue(np.allclose(s, reduce_ufunc(stream, np.add)))\n\n\n# Dynamics generation of tests on binary ufuncs\ndef test_binary_ufunc(ufunc):\n    """""" Generate a test to ensure that ireduce_ufunc(..., ufunc, ...) \n    works as intendent.""""""\n\n    def test_ufunc(self):\n        def sufunc(arrays, axis=-1):  # s for stream\n            return last(ireduce_ufunc(arrays, ufunc, axis=axis))\n\n        for axis in (0, 1, 2, -1):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = ufunc.reduce(self.stack, axis=axis)\n                from_sufunc = sufunc(self.source, axis=axis)\n                self.assertSequenceEqual(from_sufunc.shape, from_numpy.shape)\n                self.assertTrue(np.allclose(from_numpy, from_sufunc))\n\n    return test_ufunc\n\n\nclass TestAllBinaryUfuncs(unittest.TestCase):\n    def setUp(self):\n        self.source = [np.random.random((16, 5, 8)) for _ in range(10)]\n        self.stack = np.stack(self.source, axis=-1)\n\n\nfor ufunc in UFUNCS:\n    test_name = f""test_ireduce_ufunc_on_{ufunc.__name__}""\n    test = test_binary_ufunc(ufunc)\n    setattr(TestAllBinaryUfuncs, test_name, test)\n\n\ndef test_binary_ufunc_ignore_nan(ufunc):\n    """""" Generate a test to ensure that ireduce_ufunc(..., ufunc, ...) \n    works as intendent with NaNs in stream.""""""\n\n    def test_ufunc(self):\n        stack = nan_to_num(self.stack, fill_value=ufunc.identity)\n\n        def sufunc(arrays, ignore_nan=False):  # s for stream\n            return last(ireduce_ufunc(arrays, ufunc, axis=1, ignore_nan=True))\n\n        from_numpy = ufunc.reduce(stack, axis=1)\n        from_sufunc = sufunc(self.source)\n        self.assertSequenceEqual(from_numpy.shape, from_sufunc.shape)\n        self.assertTrue(np.allclose(from_numpy, from_sufunc))\n\n    return test_ufunc\n\n\nclass TestAllBinaryUfuncsIgnoreNans(unittest.TestCase):\n    def setUp(self):\n        self.source = [np.random.random((16, 5, 8)) for _ in range(10)]\n        self.source[0][0, 0, 0] = np.nan\n        self.stack = np.stack(self.source, axis=-1)\n\n\nfor ufunc in UFUNCS:\n    if ufunc.identity is None:\n        continue\n    test_name = f""test_ireduce_ufunc_on_{ufunc.__name__}""\n    test = test_binary_ufunc_ignore_nan(ufunc)\n    setattr(TestAllBinaryUfuncsIgnoreNans, test_name, test)\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_stacking.py,8,"b'# -*- coding: utf-8 -*-\nimport unittest\nimport numpy as np\n\nfrom npstreams import stack\n\n\nclass TestStack(unittest.TestCase):\n    def test_against_numpy_stack(self):\n        """""" Test against numpy.stack for axis = -1 and """"""\n        stream = [np.random.random((15, 7, 2, 1)) for _ in range(10)]\n        with self.subTest(""axis = -1""):\n            dense = np.stack(stream, axis=-1)\n            from_stack = stack(stream, axis=-1)\n            self.assertTrue(np.allclose(dense, from_stack))\n\n    def test_on_single_array(self):\n        """""" Test that npstreams.stack works with a single array """"""\n        arr = np.random.random((16, 16))\n        stacked = stack(arr)\n        self.assertTrue(np.allclose(arr[..., np.newaxis], stacked))\n\n    def test_against_numpy_concatenate(self):\n        """""" Test against numpy.concatenate for existing axes """"""\n        stream = [np.random.random((15, 7, 2, 1)) for _ in range(10)]\n        for axis in range(4):\n            with self.subTest(f""axis = {axis}""):\n                dense = np.concatenate(stream, axis=axis)\n                from_stack = stack(stream, axis=axis)\n                self.assertTrue(np.allclose(dense, from_stack))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_stats.py,119,"b'# -*- coding: utf-8 -*-\nimport unittest\nfrom itertools import repeat\nfrom random import randint, random, seed\nfrom warnings import catch_warnings, simplefilter\n\nimport numpy as np\n\ntry:\n    from scipy.stats import sem as scipy_sem\n\n    WITH_SCIPY = True\nexcept ImportError:\n    WITH_SCIPY = False\n\nfrom npstreams import (\n    iaverage,\n    imean,\n    isem,\n    istd,\n    ivar,\n    last,\n    ihistogram,\n    mean,\n    average,\n    sem,\n    std,\n    var,\n)\n\nseed(23)\n\n\nclass TestAverage(unittest.TestCase):\n    def test_trivial(self):\n        """""" Test average() on a stream of zeroes """"""\n        stream = repeat(np.zeros((64, 64), dtype=np.float), times=5)\n        for av in average(stream):\n            self.assertTrue(np.allclose(av, np.zeros_like(av)))\n\n    def test_vs_numpy(self):\n        """""" Test average vs. numpy.average """"""\n        stream = [np.random.random(size=(64, 64)) for _ in range(5)]\n        stack = np.dstack(stream)\n\n        for axis in (0, 1, 2, None):\n            with self.subTest(f""axis = {axis}""):\n                from_stream = average(stream, axis=axis)\n                from_numpy = np.average(stack, axis=axis)\n                self.assertTrue(np.allclose(from_numpy, from_stream))\n\n    def test_weighted_average(self):\n        """""" Test results of weighted average against numpy.average """"""\n        stream = [np.random.random(size=(16, 16)) for _ in range(5)]\n\n        with self.subTest(""float weights""):\n            weights = [random() for _ in stream]\n            from_average = average(stream, weights=weights)\n            from_numpy = np.average(\n                np.dstack(stream), axis=2, weights=np.array(weights)\n            )\n            self.assertTrue(np.allclose(from_average, from_numpy))\n\n        with self.subTest(""array weights""):\n            weights = [np.random.random(size=stream[0].shape) for _ in stream]\n            from_average = average(stream, weights=weights)\n            from_numpy = np.average(\n                np.dstack(stream), axis=2, weights=np.dstack(weights)\n            )\n            self.assertTrue(np.allclose(from_average, from_numpy))\n\n    def test_ignore_nan(self):\n        """""" Test that NaNs are handled correctly """"""\n        stream = [np.random.random(size=(16, 12)) for _ in range(5)]\n        for s in stream:\n            s[randint(0, 15), randint(0, 11)] = np.nan\n\n        with catch_warnings():\n            simplefilter(""ignore"")\n            from_average = average(stream, ignore_nan=True)\n        from_numpy = np.nanmean(np.dstack(stream), axis=2)\n        self.assertTrue(np.allclose(from_average, from_numpy))\n\n\nclass TestIAverage(unittest.TestCase):\n    def test_trivial(self):\n        """""" Test iaverage on stream of zeroes """"""\n        stream = repeat(np.zeros((64, 64), dtype=np.float), times=5)\n        for av in iaverage(stream):\n            self.assertTrue(np.allclose(av, np.zeros_like(av)))\n\n    def test_weighted_average(self):\n        """""" Test results of weighted iverage against numpy.average """"""\n        stream = [np.random.random(size=(16, 16)) for _ in range(5)]\n\n        with self.subTest(""float weights""):\n            weights = [random() for _ in stream]\n            from_iaverage = last(iaverage(stream, weights=weights))\n            from_numpy = np.average(\n                np.dstack(stream), axis=2, weights=np.array(weights)\n            )\n            self.assertTrue(np.allclose(from_iaverage, from_numpy))\n\n        with self.subTest(""array weights""):\n            weights = [np.random.random(size=stream[0].shape) for _ in stream]\n            from_iaverage = last(iaverage(stream, weights=weights))\n            from_numpy = np.average(\n                np.dstack(stream), axis=2, weights=np.dstack(weights)\n            )\n            self.assertTrue(np.allclose(from_iaverage, from_numpy))\n\n    def test_ignore_nan(self):\n        """""" Test that NaNs are handled correctly """"""\n        stream = [np.random.random(size=(16, 12)) for _ in range(5)]\n        for s in stream:\n            s[randint(0, 15), randint(0, 11)] = np.nan\n\n        with catch_warnings():\n            simplefilter(""ignore"")\n            from_iaverage = last(iaverage(stream, ignore_nan=True))\n        from_numpy = np.nanmean(np.dstack(stream), axis=2)\n        self.assertTrue(np.allclose(from_iaverage, from_numpy))\n\n    def test_length(self):\n        """""" Test that the number of yielded elements is the same as source """"""\n        source = (np.zeros((16,)) for _ in range(5))\n        avg = list(iaverage(source, axis=0))\n        self.assertEqual(len(avg), 5)\n\n    def test_output_dtype(self):\n        """""" Test that that yielded arrays are always floats """"""\n        for dtype in (np.uint8, np.bool, np.int16, np.float16):\n            with self.subTest(f""Dtype = {dtype}""):\n                source = (np.zeros((16,), dtype=dtype) for _ in range(5))\n                avg = last(iaverage(source))\n                self.assertEqual(avg.dtype, np.float)\n\n    def test_output_shape(self):\n        """""" Test output shape """"""\n        source = [np.random.random((16, 12, 5)) for _ in range(10)]\n        stack = np.stack(source, axis=-1)\n        for axis in (0, 1, 2, None):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.average(stack, axis=axis)\n                out = last(iaverage(source, axis=axis))\n                self.assertSequenceEqual(from_numpy.shape, out.shape)\n                self.assertTrue(np.allclose(out, from_numpy))\n\n\nclass TestMean(unittest.TestCase):\n    def test_trivial(self):\n        """""" Test mean() on a stream of zeroes """"""\n        stream = repeat(np.zeros((64, 64), dtype=np.float), times=5)\n        for av in mean(stream):\n            self.assertTrue(np.allclose(av, np.zeros_like(av)))\n\n    def test_vs_numpy(self):\n        """""" Test mean vs. numpy.mean """"""\n        stream = [np.random.random(size=(64, 64)) for _ in range(5)]\n        stack = np.dstack(stream)\n\n        for axis in (0, 1, 2, None):\n            with self.subTest(f""axis = {axis}""):\n                from_stream = mean(stream, axis=axis)\n                from_numpy = np.mean(stack, axis=axis)\n                self.assertTrue(np.allclose(from_numpy, from_stream))\n\n    def test_against_numpy_nanmean(self):\n        """""" Test results against numpy.mean""""""\n        source = [np.random.random((16, 12, 5)) for _ in range(10)]\n        for arr in source:\n            arr[randint(0, 15), randint(0, 11), randint(0, 4)] = np.nan\n        stack = np.stack(source, axis=-1)\n        for axis in (0, 1, 2, None):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.nanmean(stack, axis=axis)\n                out = mean(source, axis=axis, ignore_nan=True)\n                self.assertSequenceEqual(from_numpy.shape, out.shape)\n                self.assertTrue(np.allclose(out, from_numpy))\n\n\nclass TestIMean(unittest.TestCase):\n    def test_against_numpy_mean(self):\n        """""" Test results against numpy.mean""""""\n        source = [np.random.random((16, 12, 5)) for _ in range(10)]\n        stack = np.stack(source, axis=-1)\n        for axis in (0, 1, 2, None):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.mean(stack, axis=axis)\n                out = last(imean(source, axis=axis))\n                self.assertSequenceEqual(from_numpy.shape, out.shape)\n                self.assertTrue(np.allclose(out, from_numpy))\n\n    def test_against_numpy_nanmean(self):\n        """""" Test results against numpy.mean""""""\n        source = [np.random.random((16, 12, 5)) for _ in range(10)]\n        for arr in source:\n            arr[randint(0, 15), randint(0, 11), randint(0, 4)] = np.nan\n        stack = np.stack(source, axis=-1)\n        for axis in (0, 1, 2, None):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.nanmean(stack, axis=axis)\n                out = last(imean(source, axis=axis, ignore_nan=True))\n                self.assertSequenceEqual(from_numpy.shape, out.shape)\n                self.assertTrue(np.allclose(out, from_numpy))\n\n\nclass Testvar(unittest.TestCase):\n    def test_vs_numpy(self):\n        """""" Test that the axis parameter is handled correctly """"""\n        stream = [np.random.random((16, 7, 3)) for _ in range(5)]\n        stack = np.stack(stream, axis=-1)\n\n        for axis in (0, 1, 2, None):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.var(stack, axis=axis)\n                from_var = var(stream, axis=axis)\n                self.assertSequenceEqual(from_numpy.shape, from_var.shape)\n                self.assertTrue(np.allclose(from_var, from_numpy))\n\n    def test_ddof(self):\n        """""" Test that the ddof parameter is equivalent to numpy\'s """"""\n        stream = [np.random.random((16, 7, 3)) for _ in range(10)]\n        stack = np.stack(stream, axis=-1)\n\n        with catch_warnings():\n            simplefilter(""ignore"")\n            for axis in (0, 1, 2, None):\n                for ddof in range(4):\n                    with self.subTest(f""axis = {axis}, ddof = {ddof}""):\n                        from_numpy = np.var(stack, axis=axis, ddof=ddof)\n                        from_var = var(stream, axis=axis, ddof=ddof)\n                        self.assertSequenceEqual(from_numpy.shape, from_var.shape)\n                        self.assertTrue(np.allclose(from_var, from_numpy))\n\n\nclass TestIvar(unittest.TestCase):\n    def test_first(self):\n        """""" Test that the first yielded value of ivar is an array fo zeros """"""\n        stream = repeat(np.random.random(size=(64, 64)), times=5)\n        first = next(ivar(stream))\n\n        self.assertTrue(np.allclose(first, np.zeros_like(first)))\n\n    def test_output_shape(self):\n        """""" Test that the axis parameter is handled correctly """"""\n        stream = [np.random.random((16, 7, 3)) for _ in range(5)]\n        stack = np.stack(stream, axis=-1)\n\n        for axis in (0, 1, 2, None):\n            with self.subTest(f""axis = {axis}""):\n                from_numpy = np.var(stack, axis=axis)\n                from_ivar = last(ivar(stream, axis=axis))\n                self.assertSequenceEqual(from_numpy.shape, from_ivar.shape)\n                self.assertTrue(np.allclose(from_ivar, from_numpy))\n\n    def test_ddof(self):\n        """""" Test that the ddof parameter is equivalent to numpy\'s """"""\n        stream = [np.random.random((16, 7, 3)) for _ in range(10)]\n        stack = np.stack(stream, axis=-1)\n\n        with catch_warnings():\n            simplefilter(""ignore"")\n            for axis in (0, 1, 2, None):\n                for ddof in range(4):\n                    with self.subTest(f""axis = {axis}, ddof = {ddof}""):\n                        from_numpy = np.var(stack, axis=axis, ddof=ddof)\n                        from_ivar = last(ivar(stream, axis=axis, ddof=ddof))\n                        self.assertSequenceEqual(from_numpy.shape, from_ivar.shape)\n                        self.assertTrue(np.allclose(from_ivar, from_numpy))\n\n\nclass TestStd(unittest.TestCase):\n    def test_against_numpy_std(self):\n        stream = [np.random.random((16, 7, 3)) for _ in range(10)]\n        stack = np.stack(stream, axis=-1)\n\n        with catch_warnings():\n            simplefilter(""ignore"")\n            for axis in (0, 1, 2, None):\n                for ddof in range(4):\n                    with self.subTest(f""axis = {axis}, ddof = {ddof}""):\n                        from_numpy = np.std(stack, axis=axis, ddof=ddof)\n                        from_ivar = std(stream, axis=axis, ddof=ddof)\n                        self.assertSequenceEqual(from_numpy.shape, from_ivar.shape)\n                        self.assertTrue(np.allclose(from_ivar, from_numpy))\n\n    def test_against_numpy_nanstd(self):\n        source = [np.random.random((16, 12, 5)) for _ in range(10)]\n        for arr in source:\n            arr[randint(0, 15), randint(0, 11), randint(0, 4)] = np.nan\n        stack = np.stack(source, axis=-1)\n\n        for axis in (0, 1, 2, None):\n            for ddof in range(4):\n                with self.subTest(f""axis = {axis}, ddof = {ddof}""):\n                    from_numpy = np.nanstd(stack, axis=axis, ddof=ddof)\n                    from_ivar = std(source, axis=axis, ddof=ddof, ignore_nan=True)\n                    self.assertSequenceEqual(from_numpy.shape, from_ivar.shape)\n                    self.assertTrue(np.allclose(from_ivar, from_numpy))\n\n\nclass TestIStd(unittest.TestCase):\n    def test_against_numpy_std(self):\n        stream = [np.random.random((16, 7, 3)) for _ in range(10)]\n        stack = np.stack(stream, axis=-1)\n\n        with catch_warnings():\n            simplefilter(""ignore"")\n            for axis in (0, 1, 2, None):\n                for ddof in range(4):\n                    with self.subTest(f""axis = {axis}, ddof = {ddof}""):\n                        from_numpy = np.std(stack, axis=axis, ddof=ddof)\n                        from_ivar = last(istd(stream, axis=axis, ddof=ddof))\n                        self.assertSequenceEqual(from_numpy.shape, from_ivar.shape)\n                        self.assertTrue(np.allclose(from_ivar, from_numpy))\n\n    def test_against_numpy_nanstd(self):\n        source = [np.random.random((16, 12, 5)) for _ in range(10)]\n        for arr in source:\n            arr[randint(0, 15), randint(0, 11), randint(0, 4)] = np.nan\n        stack = np.stack(source, axis=-1)\n\n        for axis in (0, 1, 2, None):\n            for ddof in range(4):\n                with self.subTest(f""axis = {axis}, ddof = {ddof}""):\n                    from_numpy = np.nanstd(stack, axis=axis, ddof=ddof)\n                    from_ivar = last(\n                        istd(source, axis=axis, ddof=ddof, ignore_nan=True)\n                    )\n                    self.assertSequenceEqual(from_numpy.shape, from_ivar.shape)\n                    self.assertTrue(np.allclose(from_ivar, from_numpy))\n\n\n@unittest.skipIf(not WITH_SCIPY, ""SciPy is not installed/importable"")\nclass TestSem(unittest.TestCase):\n    def test_against_scipy_no_nans(self):\n        """""" Test that isem outputs the same as scipy.stats.sem """"""\n        source = [np.random.random((16, 12, 5)) for _ in range(10)]\n        stack = np.stack(source, axis=-1)\n\n        for axis in (0, 1, 2, None):\n            for ddof in range(4):\n                with self.subTest(f""axis = {axis}, ddof = {ddof}""):\n                    from_scipy = scipy_sem(stack, axis=axis, ddof=ddof)\n                    from_isem = sem(source, axis=axis, ddof=ddof)\n                    self.assertSequenceEqual(from_scipy.shape, from_isem.shape)\n                    self.assertTrue(np.allclose(from_isem, from_scipy))\n\n    def test_against_scipy_with_nans(self):\n        """""" Test that isem outputs the same as scipy.stats.sem when NaNs are ignored. """"""\n        source = [np.random.random((16, 12, 5)) for _ in range(10)]\n        for arr in source:\n            arr[randint(0, 15), randint(0, 11), randint(0, 4)] = np.nan\n        stack = np.stack(source, axis=-1)\n\n        for axis in (0, 1, 2, None):\n            for ddof in range(4):\n                with self.subTest(f""axis = {axis}, ddof = {ddof}""):\n                    from_scipy = scipy_sem(\n                        stack, axis=axis, ddof=ddof, nan_policy=""omit""\n                    )\n                    from_isem = sem(source, axis=axis, ddof=ddof, ignore_nan=True)\n                    self.assertSequenceEqual(from_scipy.shape, from_isem.shape)\n                    self.assertTrue(np.allclose(from_isem, from_scipy))\n\n\n@unittest.skipIf(not WITH_SCIPY, ""SciPy is not installed/importable"")\nclass TestISem(unittest.TestCase):\n    def test_against_scipy_no_nans(self):\n        """""" Test that isem outputs the same as scipy.stats.sem """"""\n        source = [np.random.random((16, 12, 5)) for _ in range(10)]\n        stack = np.stack(source, axis=-1)\n\n        for axis in (0, 1, 2, None):\n            for ddof in range(4):\n                with self.subTest(f""axis = {axis}, ddof = {ddof}""):\n                    from_scipy = scipy_sem(stack, axis=axis, ddof=ddof)\n                    from_isem = last(isem(source, axis=axis, ddof=ddof))\n                    self.assertSequenceEqual(from_scipy.shape, from_isem.shape)\n                    self.assertTrue(np.allclose(from_isem, from_scipy))\n\n    def test_against_scipy_with_nans(self):\n        """""" Test that isem outputs the same as scipy.stats.sem when NaNs are ignored. """"""\n        source = [np.random.random((16, 12, 5)) for _ in range(10)]\n        for arr in source:\n            arr[randint(0, 15), randint(0, 11), randint(0, 4)] = np.nan\n        stack = np.stack(source, axis=-1)\n\n        for axis in (0, 1, 2, None):\n            for ddof in range(4):\n                with self.subTest(f""axis = {axis}, ddof = {ddof}""):\n                    from_scipy = scipy_sem(\n                        stack, axis=axis, ddof=ddof, nan_policy=""omit""\n                    )\n                    from_isem = last(\n                        isem(source, axis=axis, ddof=ddof, ignore_nan=True)\n                    )\n                    self.assertSequenceEqual(from_scipy.shape, from_isem.shape)\n                    self.assertTrue(np.allclose(from_isem, from_scipy))\n\n\nclass TestIHistogram(unittest.TestCase):\n    def test_against_numpy_no_weights(self):\n        """""" Test ihistogram against numpy.histogram with no weights """"""\n        source = [np.random.random((16, 12, 5)) for _ in range(10)]\n        stack = np.stack(source, axis=-1)\n\n        bins = np.linspace(0, 1, num=10)\n        from_numpy = np.histogram(stack, bins=bins)[0]\n        from_ihistogram = last(ihistogram(source, bins=bins))\n\n        # Since histogram output is int, cannot use allclose\n        self.assertTrue(np.all(np.equal(from_numpy, from_ihistogram)))\n\n    def test_trivial_weights(self):\n        """""" Test ihistogram with weights being all 1s vs. weights=None """"""\n        source = [np.random.random((16, 12, 5)) for _ in range(10)]\n        weights = [np.array([1]) for _ in source]\n\n        bins = np.linspace(0, 1, num=10)\n        none_weights = last(ihistogram(source, bins=bins, weights=None))\n        trivial_weights = last(ihistogram(source, bins=bins, weights=weights))\n\n        self.assertTrue(np.all(np.equal(none_weights, trivial_weights)))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
docs/_themes/sphinx_rtd_theme/__init__.py,0,"b'""""""Sphinx ReadTheDocs theme.\n\nFrom https://github.com/ryan-roemer/sphinx-bootstrap-theme.\n\n""""""\nfrom os import path\n\n__version__ = ""0.3.1""\n__version_full__ = __version__\n\n\ndef get_html_theme_path():\n    """"""Return list of HTML theme paths.""""""\n    cur_dir = path.abspath(path.dirname(path.dirname(__file__)))\n    return cur_dir\n\n\n# See http://www.sphinx-doc.org/en/stable/theming.html#distribute-your-theme-as-a-python-package\ndef setup(app):\n    app.add_html_theme(""sphinx_rtd_theme"", path.abspath(path.dirname(__file__)))\n'"
