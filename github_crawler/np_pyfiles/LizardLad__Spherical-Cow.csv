file_path,api_count,code
Coordinate_Calculations.py,0,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport math\n\ndef screen_depth(RESOLUTION, FOV):\n    SCREEN_DEPTH = RESOLUTION / 2 / math.tan(math.radians(FOV / 2))\n    return SCREEN_DEPTH\n    \ndef pixels_to_camera_angle(SCREEN_DEPTH, COORDINATE):\n    CAMERA_ANGLE = math.degrees(math.atan(COORDINATE / SCREEN_DEPTH))\n    return CAMERA_ANGLE\n\ndef topview_camera_angles_to_internal_angles(CAMERA_A_ANGLE, CAMERA_B_ANGLE):\n    THETA_A = 90.0 - CAMERA_A_ANGLE\n    THETA_B = 90.0 + CAMERA_B_ANGLE\n    return (THETA_A, THETA_B)\n\ndef topview_range_from_cams(LAB, THETA_A, THETA_B):\n    LA = LAB * math.sin(math.radians(THETA_B)) / math.sin(math.radians(180 - THETA_A - THETA_B))\n    LB = LAB * math.sin(math.radians(THETA_A)) / math.sin(math.radians(180 - THETA_A - THETA_B))\n    return (LA, LB)\n\ndef x_coord(THETA_A, THETA_B, LA, LB, LAB):\n    XA = LA * math.sin(math.radians(90 - THETA_A))\n    XB = LB * math.sin(math.radians(90 - THETA_B))\n    X1 = XA - LAB/2\n    X2 = LAB/2 - XB\n    X = (X1 + X2) / 2\n    return X\n\ndef y_coord(THETA_A, THETA_B, LA, LB):\n    Y1 = LA * math.cos(math.radians(90 - THETA_A))\n    Y2 = LB * math.cos(math.radians(90 - THETA_B))\n    Y = (Y1 + Y2) / 2\n    return Y\n\ndef z_coord(CAMERA_A_ANGLE_Y, Y1, CAMERA_B_ANGLE_Y, Y2):\n    Z1 = Y1 * math.tan(math.radians(CAMERA_A_ANGLE_Y))\n    Z2 = Y2 * math.tan(math.radians(CAMERA_B_ANGLE_Y))\n    Z = (Z1 + Z2) / 2\n    return Z\n\ndef ceiling(x):\n    n = math.ceil(x*100)/100\n    return n\n'"
colorDetector.py,1,"b""#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport cv2\nimport numpy as np\n\n\ndef object_pos(lower_color, upper_color, stream):\n\tAllColorsInRange = cv2.inRange(stream, lower_color, upper_color)\n\tBlurred = cv2.GaussianBlur(AllColorsInRange, (121, 121), 0)\n\tThresh = cv2.threshold(Blurred, 37, 255, cv2.THRESH_BINARY)[1]\n\tObject = cv2.dilate(Thresh, (121, 121), iterations=1)\n\t(_, cnts, _) = cv2.findContours(Object.copy(), cv2.RETR_EXTERNAL,\n\t\t\t\t\t\t\t\t\tcv2.CHAIN_APPROX_SIMPLE)\n\tif cnts == []:\n\t\treturn (None, None, None)\n\telse:\n\t\tc = max(cnts, key=cv2.contourArea)\n\t\tmarker = cv2.minAreaRect(c)\n\t\tbox = np.int0(cv2.boxPoints(marker))\n\t\tM = cv2.moments(c)\n\t\tif int(M['m00']) == 0:\n\t\t\treturn (None, None, None)\n\t\telse:\n\t\t\tcX = int(M['m10'] / M['m00'])\n\t\t\tcY = int(M['m01'] / M['m00'])\n\t\treturn (cX, cY, cnts)\n"""
main.py,1,"b'import gi\nimport time\nimport sys\nimport signal\nimport os\nimport json\nimport cv2\nimport numpy as np\nimport Coordinate_Calculations\nimport colorDetector\n\n# Stuff needed to get it to work with vridge/riftcat.\nimport zmq, json, time, sys, struct\nfrom collections import namedtuple\nfrom construct import Int32ub, Int32ul, Float32l, Struct, Const, Padded, Array\n\nsignal.signal(signal.SIGINT, signal.SIG_DFL)\n\ngi.require_version(\'Gtk\', \'3.0\')\nfrom gi.repository import Gtk, Gdk, GLib, GdkPixbuf\n\nbuilder = Gtk.Builder()\nbuilder.add_from_file(""3dpos.glade"")\n\nconfigFilesOpened = False\n\ncap1 = cv2.VideoCapture(0)\ncap2 = cv2.VideoCapture(1)\ncap1.set(cv2.CAP_PROP_FRAME_WIDTH, 640);\ncap1.set(cv2.CAP_PROP_FRAME_HEIGHT, 480);\ncap2.set(cv2.CAP_PROP_FRAME_WIDTH, 640);\ncap2.set(cv2.CAP_PROP_FRAME_HEIGHT, 480);\n\ntry:\n\tconfigFile = open(""./config/config.json"", ""r+"")\n\tconfig = json.loads(configFile.read())\n\tconfigFilesOpened = True\nexcept:\n\tos.system(\'mkdir config\')\n\tos.system(\'rm -rf ./config/config.json\')\n\tos.system(\'touch ./config/config.json\')\n\tconfigFile = open(""./config/config.json"", ""r+"")\n\nif configFilesOpened == True:\n\tgreyscale = config[\'Greyscale\']\n\tunits = config[\'Show_Units\']\n\tHue_Min = float(config[\'Hue_Min\'])\n\tSaturation_Min = float(config[\'Saturation_Min\'])\n\tValue_Min = float(config[\'Value_Min\'])\n\tHue_Max = float(config[\'Hue_Max\'])\n\tSaturation_Max = float(config[\'Saturation_Max\'])\n\tValue_Max = float(config[\'Value_Max\'])\n\talpha = float(config[\'Alpha\'])\n\tSame_Cam = config[\'Are_the_cameras_the_same\']\n\tCam_H_FOV = float(config[\'Camera_FOV_Horizontal\'])\n\tCam_V_FOV = float(config[\'Camera_FOV_Vertical\'])\n\tDis_Between_Cams = int(config[\'Dis_Between_Cams\'])\n\tColor_Min = (Hue_Min, Saturation_Min, Value_Min)\n\tColor_Max = (Hue_Max, Saturation_Max, Value_Max)\nelse:\n\tgreyscale = False\n\tunits = True\n\tHue_Min = 0\n\tHue_Max = 360\n\tSaturation_Min = 0\n\tSaturation_Max = 255\n\tValue_Min = 0\n\tValue_Max = 255\n\talpha=0.5\n\tSame_Cam = True\n\tCam_H_FOV = 0\n\tCam_V_FOV = 0\n\tDis_Between_Cams = 0\n\tColor_Min = (Hue_Min, Saturation_Min, Value_Min)\n\tColor_Max = (Hue_Max, Saturation_Max, Value_Max)\n\t\nNotFirstTimeHFOV = False\nNotFirstTimeVFOV = False\nNotFirstTimeDBC = False\n\nclass Handler:\n\tdef onDeleteWindow(self, *args):\n\t\tGtk.main_quit(*args)\n\t\tSave()\n\t\tconfigFile.close()\n\t\tprint(\'[INFO]Exiting...\')\n\t\tsys.exit(0)\n \n\tdef activateGreyscale(self, *args):\n\t\tglobal greyscale\n\t\tgreyscale = GreyscaleToggle.get_active()\n\t\n\tdef activateUnits(self, *args):\n\t\tglobal units\n\t\tunits = UnitToggle.get_active()\n\t\t\n\tdef onHueMinValueChanged(self, *args):\n\t\tglobal Hue_Min\n\t\tHue_Min = Hue_Min_Scale.get_value()\n\t\n\tdef onHueMaxValueChanged(self, *args):\n\t\tglobal Hue_Max\n\t\tHue_Max = Hue_Max_Scale.get_value()\n\t\t\n\tdef onSaturationMinValueChanged(self, *args):\n\t\tglobal Saturation_Min\n\t\tSaturation_Min = Saturation_Min_Scale.get_value()\n\t\t\n\tdef onSaturationMaxValueChanged(self, *args):\n\t\tglobal Saturation_Max\n\t\tSaturation_Max = Saturation_Max_Scale.get_value()\n\n\tdef onValueMinValueChanged(self, *args):\n\t\tglobal Value_Min\n\t\tValue_Min = Value_Min_Scale.get_value()\n\t\t\n\tdef onValueMaxValueChanged(self, *args):\n\t\tglobal Value_Max\n\t\tValue_Max = Value_Max_Scale.get_value()\n\t\t\n\tdef onAlphaValueChanged(self, *args):\n\t\tglobal alpha\n\t\talpha = alpha_Scale.get_value()\n\t\n\tdef onDestroySettingsWindow(self, *args):\n\t\twindow2.hide()\n\t\treturn True\n\t\n\tdef SettingsWindow(self, *args):\n\t\twindow2.show()\n\t\t\n\tdef on_Cam_Same_toggled(self, *args):\n\t\tglobal Same_Cam\n\t\tSame_Cam = CamSameToggle.get_active()\n\t\n\tdef on_Cam_H_FOV_activate(self, *args):\n\t\tglobal Cam_H_FOV, NotFirstTimeHFOV\n\t\tif NotFirstTimeHFOV == True:\n\t\t\ttry:\n\t\t\t\tCam_H_FOV = float(Cam_H_FOV_Entry.get_text())\n\t\t\texcept ValueError:\n\t\t\t\tCam_H_FOV = 0\n\t\telse:\n\t\t\tpass\n\t\t\n\t\tNotFirstTimeHFOV = True\n\t\t\n\tdef on_Cam_V_FOV_activate(self, *args):\n\t\tglobal Cam_V_FOV, NotFirstTimeVFOV\n\t\tif NotFirstTimeVFOV == True:\n\t\t\ttry:\n\t\t\t\tCam_V_FOV = float(Cam_V_FOV_Entry.get_text())\n\t\t\texcept ValueError:\n\t\t\t\tCam_V_FOV = 0\n\t\telse:\n\t\t\tpass\n\t\t\n\t\tNotFirstTimeVFOV = True\n\t\n\tdef on_Dis_Between_Cams_activate(self, *args):\n\t\tglobal Dis_Between_Cams, NotFirstTimeDBC\n\t\tif NotFirstTimeDBC == True:\n\t\t\ttry:\n\t\t\t\tDis_Between_Cams = int(Dis_Between_Cams_Entry.get_text())\n\t\t\texcept ValueError:\n\t\t\t\tDis_Between_Cams = 0\n\t\telse:\n\t\t\tpass\n\t\tNotFirstTimeDBC = True\n\t\n\tdef SaveSettings_clicked(self, *args):\n\t\tSave()\n\n\ndef Save():\n\tprint(""[INFO]Saving..."")\n\tglobal configFile\n\tconfigFile.close()\n\tos.system(\'rm -rf ./config/config.json\')\n\tos.system(\'touch ./config/config.json\')\n\tconfigFile = open(\'./config/config.json\', \'r+\')\n\tconfigFile.write(\'{\\n\')\n\tconfigFile.write(\'\t""Camera_FOV_Horizontal"" : ""\' + str(Cam_H_FOV) + \'"",\\n\')\n\tconfigFile.write(\'\t""Camera_FOV_Vertical"" : ""\' + str(Cam_V_FOV) + \'"",\\n\')\n\tconfigFile.write(\'\t""Dis_Between_Cams"" : ""\' + str(Dis_Between_Cams) + \'"",\\n\')\n\tconfigFile.write(\'\t""Are_the_cameras_the_same"" : ""\' + str(Same_Cam) +\'"",\\n\')\n\tconfigFile.write(\'\t""Hue_Min"" : ""\' + str(Hue_Min) + \'"",\\n\')\n\tconfigFile.write(\'\t""Hue_Max"" : ""\' + str(Hue_Max) + \'"",\\n\')\n\tconfigFile.write(\'\t""Saturation_Min"" : ""\' + str(Saturation_Min) + \'"",\\n\')\n\tconfigFile.write(\'\t""Saturation_Max"" : ""\' + str(Saturation_Max) + \'"",\\n\')\n\tconfigFile.write(\'\t""Value_Min"" : ""\' + str(Value_Min) + \'"",\\n\')\n\tconfigFile.write(\'\t""Value_Max"" : ""\' + str(Value_Max) + \'"",\\n\')\n\tconfigFile.write(\'\t""Alpha"" : ""\' + str(alpha) + \'"",\\n\')\n\tconfigFile.write(\'\t""Show_Units"" : ""\' + str(units) + \'"",\\n\')\n\tconfigFile.write(\'\t""Greyscale"" : ""\' + str(greyscale) + \'""\\n\')\n\tconfigFile.write(\'}\')\n\tconfigFile.write(\'\\n\')\n\tconfigFile.flush()\n\n\nwindow = builder.get_object(""window1"")\nwindow2 = builder.get_object(""window2"")\nimage = builder.get_object(""image"")\nUnitToggle = builder.get_object(""ShowUnits"")\nGreyscaleToggle = builder.get_object(""Greyscale"")\nCamSameToggle = builder.get_object(""Cam_Same"")\nHue_Min_Scale = builder.get_object(""Hue_Min_Value"")\nHue_Max_Scale = builder.get_object(""Hue_Max_Value"")\nSaturation_Min_Scale = builder.get_object(""Saturation_Min_Value"")\nSaturation_Max_Scale = builder.get_object(""Saturation_Max_Value"")\nValue_Min_Scale = builder.get_object(""Value_Min_Value"")\nValue_Max_Scale = builder.get_object(""Value_Max_Value"")\nalpha_Scale = builder.get_object(""Alpha_Scale"")\nCam_H_FOV_Entry = builder.get_object(""Cam_H_FOV"")\nCam_V_FOV_Entry = builder.get_object(""Cam_V_FOV"")\nDis_Between_Cams_Entry = builder.get_object(""Dis_Between_Cams"")\nwindow.show_all()\nbuilder.connect_signals(Handler())\n\nCam_H_FOV_Entry.set_text(str(Cam_H_FOV))\nCam_V_FOV_Entry.set_text(str(Cam_V_FOV))\nDis_Between_Cams_Entry.set_text(str(Dis_Between_Cams))\n\nif units == \'True\':\n\tUnitToggle.set_active(True)\nelse:\n\tUnitToggle.set_active(False)\n\nif greyscale == \'True\':\n\tGreyscaleToggle.set_active(True)\nelse:\n\tGreyscaleToggle.set_active(False)\n\nif Same_Cam == \'True\':\n\tCamSameToggle.set_active(True)\nelse:\n\tCamSameToggle.set_active(False)\n\nalpha_Scale.set_value(alpha)\n\nHue_Min_Scale.set_value(Hue_Min)\nSaturation_Min_Scale.set_value(Saturation_Min)\nValue_Min_Scale.set_value(Value_Min)\n\nHue_Max_Scale.set_value(Hue_Max)\nSaturation_Max_Scale.set_value(Saturation_Max)\nValue_Max_Scale.set_value(Value_Max)\n\nX_SCREEN_DEPTH = Coordinate_Calculations.screen_depth(640, Cam_H_FOV)\nY_SCREEN_DEPTH = Coordinate_Calculations.screen_depth(480, Cam_V_FOV)\n\n#All this class does is prints messages to the screen so you can see what going on.\nclass debug: \n    def __init__(self, socket):\n        self.socket = socket\n        \n    def send(self, text):\n        self.socket.send(text)\n        print(""Send: "" + str(text))\n\n    def recv(self):\n        recieved = self.socket.recv()\n        print(""Recived: "" + str(recieved))\n        return recieved\n\n# Setup some fancy ZMQ socket stuff and connect to the vridge api\ncontext = zmq.Context()\ncontrol_channel = context.socket(zmq.REQ)\ncontrol_channel.connect(""tcp://127.0.0.1:38219"")\n# First you connect to a control channel (setting up debug class)\nvridge_control = debug(control_channel)\n\n# Say hi (this doesnt do anything just confirms everything works. Acknowledgement packet)\nvridge_control.send(\'{""ProtocolVersion"":1,""Code"":2}\')\nvridge_control.recv()\n\n# Request special connection for head tracking stuff\nvridge_control.send(\'{""RequestedEndpointName"":""HeadTracking"",""ProtocolVersion"":1,""Code"":1}\')\nnewconnection = json.loads(vridge_control.recv())\n#vridge_control.close() # Close socket\n\n# Connect to new socket (timeout is normally 15 seconds)\nendpoint_address = newconnection[\'EndpointAddress\']\nendpoint = context.socket(zmq.REQ)\nendpoint.connect(endpoint_address)\n# Connect to the endpoint channel (setting up debug class)\nvridge_endpoint = debug(endpoint)\n\ndef show_frame(*args):\n\tglobal Color_Min, Color_Max\n\tColor_Min = (Hue_Min, Saturation_Min, Value_Min)\n\tColor_Max = (Hue_Max, Saturation_Max, Value_Max)\n\tret, frame1 = cap1.read()\n\tret, frame2 = cap2.read()\n\thsv1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2HSV)\n\thsv2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2HSV)\n\tcxy, cyy, cntsy = colorDetector.object_pos(Color_Min, Color_Max, hsv1)\n\tcxy2, cyy2, cntsy2 = colorDetector.object_pos(Color_Min, Color_Max, hsv2)\n\tif cxy != None and cxy2 != None and X_SCREEN_DEPTH != None and Y_SCREEN_DEPTH != None:\n\t\t#Start Converting Pixel Coordinates To Angles\n\t\tCAMERA_ANGLE_AY = Coordinate_Calculations.pixels_to_camera_angle(X_SCREEN_DEPTH, cxy)\n\t\tCAMERA_ANGLE_YY1 = Coordinate_Calculations.pixels_to_camera_angle(Y_SCREEN_DEPTH, cyy)\n\t\tCAMERA_ANGLE_BY = Coordinate_Calculations.pixels_to_camera_angle(X_SCREEN_DEPTH, cxy2)\n\t\tCAMERA_ANGLE_YY2 = Coordinate_Calculations.pixels_to_camera_angle(Y_SCREEN_DEPTH, cyy2)\n\t\t#End Converting Pixel Coordinates To Angles\n\t\t#Start Calculating Variables Needed For Coordinate Calculations\n\t\tTHETA_AY, THETA_BY = Coordinate_Calculations.topview_camera_angles_to_internal_angles(CAMERA_ANGLE_AY, CAMERA_ANGLE_BY)\n\t\tif THETA_AY or THETA_BY != 0:\n\t\t\tLAY, LBY = Coordinate_Calculations.topview_range_from_cams(Dis_Between_Cams, THETA_BY, THETA_AY)\n\t\t\t#End Calculating Variables Needed For Coordinate Calculations\n\t\t\t#Start Calculating Coordinates\n\t\t\tXY = Coordinate_Calculations.x_coord(THETA_AY, THETA_BY, LAY, LBY, Dis_Between_Cams)\n\t\t\tYY = Coordinate_Calculations.y_coord(THETA_AY, THETA_BY, LAY, LBY)\n\t\t\tZY = Coordinate_Calculations.z_coord(CAMERA_ANGLE_YY1, YY, CAMERA_ANGLE_YY2, YY)\n\t\t\t#End Calculating Coordinates\n\t\t\t\n\t\t\t\n\t\t\t# Start sending information to Vridge\n\t\t\tprint(XY, YY, ZY)\n\t\t\t# Specify the structure for the fancy position matrix\n\t\t\tstructure = Struct(\n\t\t\t\tConst(Int32ul, 2),\n\t\t\t\tConst(Int32ul, 5),\n\t\t\t\tConst(Int32ul, 24),\n\t\t\t\t""data"" / Padded(64, Array(3, Float32l)),\n\t\t\t)\n\t\t\t\n\t\t\toffset = [0.0, 0.0, 0.0] # offset in case you want to define where the origin is (in centimeters)\n\t\t\txyz = [(XY + offset[0]) / 100, (YY + offset[1]) / 100, (ZY + offset[2]) / 100] # Steam VR requires this information in meters\n\t\t\tbyte_packet = structure.build(dict(data=xyz))\n\t\t\tendpoint.send(byte_packet)\n\t\t\tprint(""Send: "" + str(structure.parse(byte_packet)))\n            endpoint.recv()   \n            # Stop sending information to vridge \n\t\t\t\n\t\t\t\n\t\t\t#Start Drawing Contours\n\t\t\tfor (i, c) in enumerate(cntsy):\n\t\t\t\t# draw the contour\n\t\t\t\t(x, y), radius = cv2.minEnclosingCircle(c)\n\t\t\t\tcv2.drawContours(frame1, [c], -1, (0, 255, 255), 2)\n\t\t\t\toverlay1 = frame1.copy()\n\t\t\t\tcv2.rectangle(overlay1, (int(cxy+radius), cyy - 30), (int(cxy + radius + 200), cyy+60), (0, 0, 0), -1)\n\t\t\t\tcv2.addWeighted(overlay1, alpha, frame1, 1 - alpha, 0, frame1)\n\t\t\t\tif units:\n\t\t\t\t\tcv2.putText(frame1, ""X Axis "" + str(Coordinate_Calculations.ceiling(XY)) +""cm"", (int(cxy + radius + 20), cyy), cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 150, 0), 1)\n\t\t\t\t\tcv2.putText(frame1, ""Y Axis "" + str(Coordinate_Calculations.ceiling(YY)) +""cm"", (int(cxy + radius + 20), cyy + 20), cv2.FONT_HERSHEY_DUPLEX, 0.6, (0, 255, 0), 1)\n\t\t\t\t\tcv2.putText(frame1, ""Z Axis "" + str(Coordinate_Calculations.ceiling(ZY)) +""cm"", (int(cxy + radius + 20), cyy + 40), cv2.FONT_HERSHEY_DUPLEX, 0.6, (0, 0, 255), 1)\n\t\t\t\telse:\n\t\t\t\t\tcv2.putText(frame1, ""X Axis "" + str(Coordinate_Calculations.ceiling(XY)), (int(cxy + radius + 20), cyy), cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 150, 0), 1)\n\t\t\t\t\tcv2.putText(frame1, ""Y Axis "" + str(Coordinate_Calculations.ceiling(YY)), (int(cxy + radius + 20), cyy + 20), cv2.FONT_HERSHEY_DUPLEX, 0.6, (0, 255, 0), 1)\n\t\t\t\t\tcv2.putText(frame1, ""Z Axis "" + str(Coordinate_Calculations.ceiling(ZY)), (int(cxy + radius + 20), cyy + 40), cv2.FONT_HERSHEY_DUPLEX, 0.6, (0, 0, 255), 1)\n\n\t\t\tfor (i, c) in enumerate(cntsy2):\n\t\t\t\t# draw the contour\n\t\t\t\t(x, y), radius = cv2.minEnclosingCircle(c)\n\t\t\t\tcv2.drawContours(frame2, [c], -1, (0, 255, 255), 2)\n\t\t\t\toverlay2 = frame2.copy()\n\t\t\t\tcv2.rectangle(overlay2, (int(cxy2+radius), cyy2 - 30), (int(cxy2 + radius + 200), cyy2 + 60), (0, 0, 0), -1)\n\t\t\t\tcv2.addWeighted(overlay2, alpha, frame2, 1 - alpha, 0, frame2)\n\t\t\t\tif units:\n\t\t\t\t\tcv2.putText(frame2, ""X Axis "" + str(Coordinate_Calculations.ceiling(XY)) +""cm"", (int(cxy2 + radius + 20), cyy2), cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 150, 0), 1)\n\t\t\t\t\tcv2.putText(frame2, ""Y Axis "" + str(Coordinate_Calculations.ceiling(YY)) +""cm"", (int(cxy2 + radius + 20), cyy2 + 20), cv2.FONT_HERSHEY_DUPLEX, 0.6, (0, 255, 0), 1)\n\t\t\t\t\tcv2.putText(frame2, ""Z Axis "" + str(Coordinate_Calculations.ceiling(ZY)) +""cm"", (int(cxy2 + radius + 20), cyy2 + 40), cv2.FONT_HERSHEY_DUPLEX, 0.6, (0, 0, 255), 1)\n\t\t\t\telse:\n\t\t\t\t\tcv2.putText(frame2, ""X Axis "" + str(Coordinate_Calculations.ceiling(XY)), (int(cxy2 + radius + 20), cyy2), cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 150, 0), 1)\n\t\t\t\t\tcv2.putText(frame2, ""Y Axis "" + str(Coordinate_Calculations.ceiling(YY)), (int(cxy2 + radius + 20), cyy2 + 20), cv2.FONT_HERSHEY_DUPLEX, 0.6, (0, 255, 0), 1)\n\t\t\t\t\tcv2.putText(frame2, ""Z Axis "" + str(Coordinate_Calculations.ceiling(ZY)), (int(cxy2 + radius + 20), cyy2 + 40), cv2.FONT_HERSHEY_DUPLEX, 0.6, (0, 0, 255), 1)\n\t\t\t\t#End Draw Contours\n\n\tvis = np.concatenate((frame1, frame2), axis=1)\n\tframe = cv2.resize(vis, None, fx=0.7, fy=0.7, interpolation = cv2.INTER_CUBIC)\n\tif greyscale == True or greyscale == \'True\':\n\t\tframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\t\tframe = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n\telse:\n\t\tframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n\tpb = GdkPixbuf.Pixbuf.new_from_data(frame.tostring(),\n\t\t\t\t\t\t\t\t\t\tGdkPixbuf.Colorspace.RGB,\n\t\t\t\t\t\t\t\t\t\tFalse,\n\t\t\t\t\t\t\t\t\t\t8,\n\t\t\t\t\t\t\t\t\t\tframe.shape[1],\n\t\t\t\t\t\t\t\t\t\tframe.shape[0],\n\t\t\t\t\t\t\t\t\t\tframe.shape[2]*frame.shape[1])\n\timage.set_from_pixbuf(pb.copy())\n\treturn True\n\n\nGLib.idle_add(show_frame)\nGtk.main()\n\n\n##Oh hi there, I\'m the crazy haired blond volunteer who was talking to you about this\n##project at the young ict explorers. Congrats for winning first! This was\n##my first time volunteering there and it was a lot more amazing than I\n##thought it would be. On Sunday the grades 5-6 had some really interesting\n##projects as well. When I competed in 2014 I didn\'t get to look at everybody\n##else\'s in as much depth as this year so it was really good to be able to\n##spend the time to talk to students about what they made.\n##\n##Thanks for giving me a link to your github code, I thought a really cool\n##use of this code would be for things where you sit at the computer and\n##could use a position tracker like a flight simulator in VR. I don\'t\n##have the kind of money to buy an oculus rift or HTC vive so I wanted to\n##create my own VR setup. I know theres this piece of software called\n##Vridge made by Riftcat where you can play VR games on your computer and\n##they get steamed to your phone via wifi (to be used with a VR headset). \n##The only problem is that it only translates rotational information from \n##your phone, and can\'t get position information. \n##So I thought if you used this program to get the position\n##info it would do pretty good as a DIY vive. Sadly I don\'t have 2 identical\n##webcams to test with but this code should (probably) work. When you\n##download/install Vridge, make sure you set it to the beta version as this\n##software uses the API which is only available in the beta version. \n##\n##You will have to install a few modules first before you get it to work,\n##that shouldn\'t be too hard. I think I just got them from pip install or\n##they were already there installed by default. The video I (tried) to show\n##you is https://youtu.be/1FYMBoXsBbE where I was able to use 2 PS3Eye\n##cameras to do position tracking and parse that information to SteamVR,\n##but as you can see its very slow and clearly has bottlenecks. The problem\n##with the PS3Eye cameras is that the windows drivers only allow 1 camera\n##to be detected and can be used, meaning if I plugged 2 cameras in it \n##ignores the second one. So sadly I couldn\'t get it working that way.\n##I have a .dll that allows PS3Eye multiple cameras to work, but it\'s\n##very difficult to try and get dll\'s made for C++/C# to work with opencv\n##and python.\n##\n##I took your suggestion about Linux\'s video API potentially having\n##compatible drivers and to my suprise both the latest versions of Ubuntu\n##and Fedora seem to support multiple PS3 eye cameras. I thought I tested\n##on ubuntu before and it didn\'t work. Oh well. However when running\n##your code I ran into issues when getting the PS3 cameras to work inside\n##python using opencv. Nothing to do with your code, it\'s that \n##cv2.VideoCapture is returning no frames (I believe it returns None).\n##\n##I would one day like to be able to use a set of Wii remotes in place of\n##vive controllers, I was thinking about using an arduino with an 8266 wifi\n##board with a gyroscope/accellerometer with an IR ball, which would be\n##pretty cool I think. If you have a look on my github/BoomBrush/VRTracker\n## page you\'ll see that i\'ve uploaded a version of my tracker code. It\'s\n## designed to work with a piece of software made by somebody else that\n## computes the 3D position, but the way it works is way to complex to\n## explain here.\n##\n##Look, theres a lot more I could have talked about on the day but I kinda\n##ran out of time and this is getting a bit long. If it\'s cool with you\n##and your parents (due to the age different, I am 18) shoot me an email\n##and we can continue this discussion. Chao.\n##\n##boombrush [@] hotmail [.dot] com\n##\n##- Scott Howie\n'"
