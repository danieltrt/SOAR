file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\ntry:\n    from setuptools import setup\nexcept ImportError:\n    from distutils.core import setup\n\nwith open(\'README.rst\') as readme_file:\n    readme = readme_file.read()\n\nwith open(\'HISTORY.rst\') as history_file:\n    history = history_file.read().replace(\'.. :changelog:\', \'\')\n\ntest_requirements = [\n    \'wheel\',\n    \'pytest\',\n]\n\nsetup(\n    name=\'hashedindex\',\n    version=\'0.8.0\',\n    description=""InvertedIndex implementation using hash lists (dictionaries)"",\n    long_description=readme + \'\\n\\n\' + history,\n    author=""Michael Aquilina"",\n    author_email=\'michaelaquilina@gmail.com\',\n    url=\'https://github.com/MichaelAquilina/hashedindex\',\n    packages=[\n        \'hashedindex\',\n    ],\n    package_dir={\'hashedindex\':\n                 \'hashedindex\'},\n    include_package_data=True,\n    license=""BSD"",\n    zip_safe=False,\n    keywords=\'hashedindex\',\n    classifiers=[\n        \'Development Status :: 2 - Pre-Alpha\',\n        \'Intended Audience :: Developers\',\n        \'License :: OSI Approved :: BSD License\',\n        \'Natural Language :: English\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Python :: 3.8\',\n    ],\n    test_suite=\'tests\',\n    setup_requires=[\'pytest-runner\'],\n    tests_require=test_requirements\n)\n'"
docs/conf.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# hashedindex documentation build configuration file, created by\n# sphinx-quickstart on Tue Jul  9 22:26:36 2013.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n\n# If extensions (or modules to document with autodoc) are in another\n# directory, add these directories to sys.path here. If the directory is\n# relative to the documentation root, use os.path.abspath to make it\n# absolute, like shown here.\n#sys.path.insert(0, os.path.abspath(\'.\'))\n\n# Get the project root dir, which is the parent dir of this\ncwd = os.getcwd()\nproject_root = os.path.dirname(cwd)\n\n# Insert the project root dir as the first element in the PYTHONPATH.\n# This lets us ensure that the source package is imported, and that its\n# version is used.\nsys.path.insert(0, project_root)\n\nimport hashedindex\n\n# -- General configuration ---------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom ones.\nextensions = [\'sphinx.ext.autodoc\', \'sphinx.ext.viewcode\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix of source filenames.\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'hashedindex\'\ncopyright = u\'2015, Michael Aquilina\'\n\n# The version info for the project you\'re documenting, acts as replacement\n# for |version| and |release|, also used in various other places throughout\n# the built documents.\n#\n# The short X.Y version.\nversion = hashedindex.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = hashedindex.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to\n# some non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\'_build\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built\n# documents.\n#keep_warnings = False\n\n\n# -- Options for HTML output -------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \'default\'\n\n# Theme options are theme-specific and customize the look and feel of a\n# theme further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as\n# html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the\n# top of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon\n# of the docs.  This file should be a Windows icon file (.ico) being\n# 16x16 or 32x32 pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets)\n# here, relative to this directory. They are copied after the builtin\n# static files, so a file named ""default.css"" will overwrite the builtin\n# ""default.css"".\nhtml_static_path = [\'_static\']\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page\n# bottom, using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names\n# to template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer.\n# Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer.\n# Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages\n# will contain a <link> tag referring to it.  The value of this option\n# must be the base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'hashedindexdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\'preamble\': \'\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass\n# [howto/manual]).\nlatex_documents = [\n    (\'index\', \'hashedindex.tex\',\n     u\'hashedindex Documentation\',\n     u\'Michael Aquilina\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at\n# the top of the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings\n# are parts, not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (\'index\', \'hashedindex\',\n     u\'hashedindex Documentation\',\n     [u\'Michael Aquilina\'], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output ----------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\'index\', \'hashedindex\',\n     u\'hashedindex Documentation\',\n     u\'Michael Aquilina\',\n     \'hashedindex\',\n     \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n'"
hashedindex/__init__.py,2,"b'__author__ = \'Michael Aquilina\'\n__email__ = \'michaelaquilina@gmail.com\'\n__version__ = \'0.6.0\'\n\nimport collections\nimport functools\nimport math\n\n\nDOCUMENT_DOES_NOT_EXIST = \'The specified document does not exist\'\nTERM_DOES_NOT_EXIST = \'The specified term does not exist\'\n\n\nclass HashedIndex:\n    """"""\n    InvertedIndex structure in the form of a hash list implementation.\n    """"""\n\n    def __init__(self, initial_terms=None):\n        """"""\n        Construct a new HashedIndex. An optional list of initial terms\n        may be passed which will be automatically added to the new HashedIndex.\n        """"""\n        self._documents = collections.Counter()\n        self._terms = {}\n        self._freeze = False\n        if initial_terms is not None:\n            for term in initial_terms:\n                self._terms[term] = {}\n\n    def __getitem__(self, term):\n        return self._terms[term]\n\n    def __contains__(self, term):\n        return term in self._terms\n\n    def __repr__(self):\n        return \'<HashedIndex: {} terms, {} documents>\'.format(\n            len(self._terms), len(self._documents)\n        )\n\n    def __eq__(self, other):\n        return self._terms == other._terms and self._documents == other._documents\n\n    def clear(self):\n        """"""\n        Resets the HashedIndex to a clean state without any terms or documents.\n        """"""\n        self._terms = {}\n        self._documents = collections.Counter()\n\n    def freeze(self):\n        """"""\n        Freezes the HashedIndex, preventing any new terms from being added\n        when calling add_term_occurrence.\n        """"""\n        self._freeze = True\n\n    def unfreeze(self):\n        """"""\n        Unfreezes (thaws) the HashedIndex, allowing new terms to be added\n        when calling add_term_occurrence.\n        """"""\n        self._freeze = False\n\n    def add_term_occurrence(self, term, document):\n        """"""\n        Adds an occurrence of the term in the specified document.\n        """"""\n        if document not in self._documents:\n            self._documents[document] = 0\n\n        if term not in self._terms:\n            if self._freeze:\n                return\n            else:\n                self._terms[term] = collections.Counter()\n\n        if document not in self._terms[term]:\n            self._terms[term][document] = 0\n\n        self._documents[document] += 1\n        self._terms[term][document] += 1\n\n    def get_total_term_frequency(self, term):\n        """"""\n        Gets the frequency of the specified term in the entire corpus\n        added to the HashedIndex.\n        """"""\n        if term not in self._terms:\n            raise IndexError(TERM_DOES_NOT_EXIST)\n\n        return sum(self._terms[term].values())\n\n    def get_term_frequency(self, term, document, normalized=False):\n        """"""\n        Returns the frequency of the term specified in the document.\n        """"""\n        if document not in self._documents:\n            raise IndexError(DOCUMENT_DOES_NOT_EXIST)\n\n        if term not in self._terms:\n            raise IndexError(TERM_DOES_NOT_EXIST)\n\n        result = self._terms[term].get(document, 0)\n        if normalized:\n            result /= self.get_document_length(document)\n\n        return float(result)\n\n    def get_document_frequency(self, term):\n        """"""\n        Returns the number of documents the specified term appears in.\n        """"""\n        if term not in self._terms:\n            raise IndexError(TERM_DOES_NOT_EXIST)\n        else:\n            return len(self._terms[term])\n\n    def get_document_length(self, document):\n        """"""\n        Returns the number of terms found within the specified document.\n        """"""\n        if document in self._documents:\n            return self._documents[document]\n        else:\n            raise IndexError(DOCUMENT_DOES_NOT_EXIST)\n\n    def get_documents(self, term):\n        """"""\n        Returns all documents related to the specified term in the\n        form of a Counter object.\n        """"""\n        if term not in self._terms:\n            raise IndexError(TERM_DOES_NOT_EXIST)\n        else:\n            return self._terms[term]\n\n    def terms(self):\n        return list(self._terms)\n\n    def documents(self):\n        return list(self._documents)\n\n    def items(self):\n        return self._terms\n\n    def get_tfidf(self, term, document, normalized=False):\n        """"""\n        Returns the Term-Frequency Inverse-Document-Frequency value for the given\n        term in the specified document. If normalized is True, term frequency will\n        be divided by the document length.\n        """"""\n        tf = self.get_term_frequency(term, document)\n\n        # Speeds up performance by avoiding extra calculations\n        if tf != 0.0:\n            # Add 1 to document frequency to prevent divide by 0\n            # (Laplacian Correction)\n            df = 1 + self.get_document_frequency(term)\n            n = 2 + len(self._documents)\n\n            if normalized:\n                tf /= self.get_document_length(document)\n\n            return tf * math.log10(n / df)\n        else:\n            return 0.0\n\n    def get_total_tfidf(self, term, normalized=False):\n        result = 0.0\n        for document in self._documents:\n            result += self.get_tfidf(term, document, normalized)\n        return result\n\n    def generate_document_vector(self, doc, mode=\'tfidf\'):\n        """"""\n        Returns a representation of the specified document as a feature vector\n        weighted according the mode specified (by default tf-dif).\n\n        A custom weighting function can also be passed which receives the hashedindex\n        instance, the selected term and document as parameters.\n\n        The result will be returned in the form of a list. This can be converted\n        into a numpy array if required using the `np.asarray` method\n        Available built-in modes:\n          * tfidf: Term Frequency Inverse Document Frequency\n          * ntfidf: Normalized Term Frequency Inverse Document Frequency\n          * tf: Term Frequency\n          * ntf: Normalized Term Frequency\n        """"""\n        if mode == \'tfidf\':\n            selected_function = HashedIndex.get_tfidf\n        elif mode == \'ntfidf\':\n            selected_function = functools.partial(HashedIndex.get_tfidf, normalized=True)\n        elif mode == \'tf\':\n            selected_function = HashedIndex.get_term_frequency\n        elif mode == \'ntf\':\n            selected_function = functools.partial(HashedIndex.get_term_frequency, normalized=True)\n        elif hasattr(mode, \'__call__\'):\n            selected_function = mode\n        else:\n            raise ValueError(\'Unexpected mode: %s\', mode)\n\n        result = []\n        for term in self._terms:\n            result.append(selected_function(self, term, doc))\n\n        return result\n\n    def generate_feature_matrix(self, mode=\'tfidf\'):\n        """"""\n        Returns a feature matrix in the form of a list of lists which\n        represents the terms and documents in this Inverted Index using\n        the tf-idf weighting by default. The term counts in each\n        document can alternatively be used by specifying scheme=\'count\'.\n\n        A custom weighting function can also be passed which receives a term\n        and document as parameters.\n\n        The size of the matrix is equal to m x n where m is\n        the number of documents and n is the number of terms.\n\n        The list-of-lists format returned by this function can be very easily\n        converted to a numpy matrix if required using the `np.as_matrix`\n        method.\n        """"""\n        result = []\n\n        for doc in self._documents:\n            result.append(self.generate_document_vector(doc, mode))\n\n        return result\n\n    def prune(self, min_value=None, max_value=None, use_percentile=False):\n        n_documents = len(self._documents)\n\n        garbage = []\n        for term in self.terms():\n            freq = self.get_document_frequency(term)\n            if use_percentile:\n                freq /= n_documents\n\n            if min_value is not None and freq < min_value:\n                garbage.append(term)\n\n            if max_value is not None and freq > max_value:\n                garbage.append(term)\n\n        for term in garbage:\n            del(self._terms[term])\n\n    def to_dict(self):\n        return {\n            \'documents\': self._documents,\n            \'terms\': self._terms,\n        }\n\n    def from_dict(self, data):\n        self._documents = collections.Counter(data[\'documents\'])\n        self._terms = {}\n        for term in data[\'terms\']:\n            self._terms[term] = collections.Counter(data[\'terms\'][term])\n\n\ndef merge(index_list):\n    result = HashedIndex()\n\n    for index in index_list:\n        first_index = result\n        second_index = index\n\n        assert isinstance(second_index, HashedIndex)\n\n        for term in second_index.terms():\n            if term in first_index._terms and term in second_index._terms:\n                result._terms[term] = first_index._terms[term] + second_index._terms[term]\n            elif term in second_index._terms:\n                result._terms[term] = second_index._terms[term]\n            else:  # pragma: nocover\n                raise ValueError(""I dont know how the hell you managed to get here"")\n\n        result._documents = first_index._documents + second_index._documents\n\n    return result\n'"
hashedindex/textparser.py,0,"b'import re\nimport math\nimport unicodedata\n\nfrom copy import copy\nfrom string import ascii_letters, digits, punctuation\n\n\n# Stemmer interface which returns token unchanged\nclass NullStemmer:\n\n    def stem(self, x):\n        return x\n\n    def __repr__(self):\n        return \'<NullStemmer>\'\n\n\nclass InvalidStemmerException(Exception):\n    pass\n\n\n_stopwords = frozenset()\n_accepted = frozenset(ascii_letters + digits + punctuation) - frozenset(\'\\\'\')\n\n# Permit certain punctuation characters within tokens\n_punctuation_exceptions = r\'\\/-\'\n_punctuation = copy(punctuation)\nfor char in _punctuation_exceptions:\n    _punctuation = _punctuation.replace(char, \'\')\n\n_punctuation_class = \'[%s]\' % re.escape(_punctuation)\n_whitespace_class = r\'\\s+\'\n_word_class = \'[A-z0-9%s]+\' % re.escape(_punctuation_exceptions)\n\n_re_punctuation = re.compile(_punctuation_class)\n_re_token = re.compile(\'%s|%s|%s\' % (_punctuation_class, _whitespace_class, _word_class))\n\n_url_pattern = (\n    r\'(https?:\\/\\/)?(([\\da-z-]+)\\.){1,2}.([a-z\\.]{2,6})(/[\\/\\w \\.-]*)*\\/?(\\?(\\w+=\\w+&?)+)?\'\n)\n_re_full_url = re.compile(r\'^%s$\' % _url_pattern)\n_re_url = re.compile(_url_pattern)\n\n\n# Determining the best way to calculate tfidf is proving difficult,\n# might need more advanced techniques\ndef tfidf(tf, df, corpus_size):\n    if df and tf:\n        return (1 + math.log(tf)) * math.log(corpus_size / df)\n    else:\n        return 0.0\n\n\ndef normalize_unicode(text):\n    """"""\n    Normalize any unicode characters to ascii equivalent\n    https://docs.python.org/2/library/unicodedata.html#unicodedata.normalize\n    """"""\n    if isinstance(text, str):\n        return unicodedata.normalize(\'NFKD\', text).encode(\'ascii\', \'ignore\').decode(\'utf8\')\n    else:\n        return text\n\n\ndef match_tokens(text, tokenize_whitespace):\n    for token in re.findall(_re_token, text):\n        if tokenize_whitespace or not token.isspace():\n            yield token\n\n\ndef get_ngrams(token_list, n=2):\n    tokens = iter(token_list)\n    try:\n        ngram = [next(tokens) for _ in range(0, n)]\n        yield ngram\n    except StopIteration:\n        return\n\n    for token in tokens:\n        ngram = ngram[1:] + [token]\n        yield ngram\n\n\ndef validate_stemmer(stemmer):\n    if not hasattr(stemmer, \'stem\'):\n        raise InvalidStemmerException(\'Stemmer is missing a ""stem"" function\')\n    if not callable(stemmer.stem):\n        raise InvalidStemmerException(\'Stemmer has a ""stem"" attribute but it is not a function\')\n\n\ndef word_tokenize(text, stopwords=_stopwords, ngrams=None, min_length=0, ignore_numeric=True,\n                  stemmer=None, retain_casing=False, tokenize_whitespace=False,\n                  retain_punctuation=False):\n    """"""\n    Parses the given text and yields tokens which represent words within\n    the given text. Tokens are assumed to be divided by any form of\n    whitespace character.  A stemmer may optionally be provided, which will\n    apply a transformation to each token.\n\n    The tokenizer ignores numeric tokens by default; the ignore_numeric\n    parameter can be set to False to include them in the output stream.\n\n    Generated tokens are lowercased by default; the retain_casing flag can\n    be set to True to retain upper/lower casing from the original text.\n\n    Whitespace tokens are omitted by default; the tokenize_whitespace flag can\n    be set to True to include whitespace tokens in the output stream.\n\n    Punctuation characters are removed from the input text by default;\n    the retain_punctuation flag can be set to True to retain them.\n    """"""\n    if ngrams is None:\n        ngrams = 1\n    if stemmer is None:\n        stemmer = NullStemmer()\n\n    validate_stemmer(stemmer)\n\n    text = re.sub(re.compile(\'\\\'s\'), \'\', text)  # Simple heuristic\n    text = text if retain_punctuation else re.sub(_re_punctuation, \'\', text)\n    text = text if retain_casing else text.lower()\n\n    matched_tokens = match_tokens(text, tokenize_whitespace)\n    for ngram in get_ngrams(matched_tokens, ngrams):\n        output_tokens = []\n        for token in ngram:\n            token = stemmer.stem(token)\n            if len(token) < min_length or token in stopwords:\n                break\n            if ignore_numeric and isnumeric(token):\n                break\n            output_tokens.append(token)\n        else:\n            yield tuple(output_tokens)\n\n\ndef isnumeric(text):\n    """"""\n    Returns a True if the text is purely numeric and False otherwise.\n    """"""\n    try:\n        float(text)\n    except ValueError:\n        return False\n    else:\n        return True\n\n\ndef is_url(text):\n    """"""\n    Returns a True if the text is a url and False otherwise.\n    """"""\n    return bool(_re_full_url.match(text))\n'"
tests/__init__.py,0,b''
tests/test_hashedindex.py,0,"b'import collections\nimport json\nimport unittest\n\nimport pytest\n\nimport hashedindex\n\n\ndef unordered_list_cmp(list1, list2):\n    # Check lengths first for slight improvement in performance\n    return len(list1) == len(list2) and sorted(list1) == sorted(list2)\n\n\nclass HashedIndexTest(unittest.TestCase):\n    # Note that generate_document_vector and generate_feature_matrix tests\n    # are considered interrelated and test cases are therefore not repeated\n    # between them where possible.\n\n    def setUp(self):\n        self.index = hashedindex.HashedIndex()\n\n        for i in range(3):\n            self.index.add_term_occurrence(\'word\', \'document1.txt\')\n\n        for i in range(5):\n            self.index.add_term_occurrence(\'malta\', \'document1.txt\')\n\n        for i in range(4):\n            self.index.add_term_occurrence(\'phone\', \'document2.txt\')\n\n        for i in range(2):\n            self.index.add_term_occurrence(\'word\', \'document2.txt\')\n\n    def test_repr(self):\n        index = hashedindex.HashedIndex()\n        assert str(index) == ""<HashedIndex: 0 terms, 0 documents>""\n        index.add_term_occurrence(\'foo\', \'doc1.md\')\n        index.add_term_occurrence(\'bar\', \'doc1.md\')\n        assert str(index) == ""<HashedIndex: 2 terms, 1 documents>""\n\n    def test_get_documents(self):\n        assert self.index.get_documents(\'word\') == collections.Counter(\n            {\'document1.txt\': 3, \'document2.txt\': 2}\n        )\n        assert self.index.get_documents(\'malta\') == collections.Counter({\'document1.txt\': 5})\n        assert self.index.get_documents(\'phone\') == collections.Counter({\'document2.txt\': 4})\n\n        assert unordered_list_cmp(self.index.documents(), [\'document1.txt\', \'document2.txt\'])\n\n        self.index.add_term_occurrence(\'test\', \'document3.txt\')\n        assert unordered_list_cmp(\n            self.index.documents(), [\'document1.txt\', \'document2.txt\', \'document3.txt\']\n        )\n\n        assert \'doesnotexist.txt\' not in self.index.documents()\n\n    def test_get_documents_missing_term(self):\n        with pytest.raises(IndexError) as exc:\n            self.index.get_documents(\'idontexist\')\n        assert str(exc.value) == \'The specified term does not exist\'\n\n    def test_hashedindex_constructor_with_terms(self):\n        index2 = hashedindex.HashedIndex(self.index.terms())\n\n        # Terms between the two indexes should be equal\n        assert unordered_list_cmp(index2.terms(), self.index.terms())\n\n        # No documents should be found\n        assert index2.documents() == []\n\n        # All terms should have no referenced documents\n        for term in index2.terms():\n            assert index2[term] == {}\n\n        index2.add_term_occurrence(\'phone\', \'mydoc.doc\')\n        assert index2.get_term_frequency(\'phone\', \'mydoc.doc\') == 1\n\n    def test_case_sensitive_documents(self):\n        self.index.add_term_occurrence(\'word\', \'Document2.txt\')\n\n        assert self.index.get_term_frequency(\'word\', \'document2.txt\') == 2\n        assert self.index.get_term_frequency(\'word\', \'Document2.txt\') == 1\n\n        assert unordered_list_cmp(\n            self.index.documents(), [\'document1.txt\', \'document2.txt\', \'Document2.txt\']\n        )\n\n    def test_getitem(self):\n        assert unordered_list_cmp(self.index[\'word\'].keys(), [\'document1.txt\', \'document2.txt\'])\n        assert unordered_list_cmp(self.index[\'malta\'].keys(), [\'document1.txt\'])\n        assert unordered_list_cmp(self.index[\'phone\'].keys(), [\'document2.txt\'])\n\n    def test_getitem_raises_keyerror(self):\n        # Trying to get a term that does not exist should raise a key error\n        with pytest.raises(KeyError) as exc:\n            self.index[\'doesnotexist\']\n        assert str(exc.value) == ""\'doesnotexist\'""\n\n        # Case Insensitive check\n        with pytest.raises(KeyError) as exc:\n            self.index[\'wORd\']\n        assert str(exc.value) == ""\'wORd\'""\n\n    def test_contains(self):\n        assert \'word\' in self.index\n        assert \'malta\' in self.index\n        assert \'phone\' in self.index\n\n        # Case Insensitive Check\n        assert \'WoRd\' not in self.index\n\n        # Non-Existent check\n        assert \'doesnotexist\' not in self.index\n\n    def test_clear(self):\n        self.index.clear()\n\n        assert self.index.terms() == []\n        assert self.index.documents() == []\n\n    def test_freeze_unfreeze(self):\n        self.index.freeze()\n        self.index.add_term_occurrence(\'myword\', \'document2.txt\')\n\n        # New words should be not added\n        assert \'myword\' not in self.index\n\n        # Adding words that exist should work though\n        assert self.index.get_term_frequency(\'word\', \'document1.txt\') == 3\n\n        # Ensure documents are still added even if its term is not\n        self.index.add_term_occurrence(\'idonotexist\', \'document20.txt\')\n        assert \'document20.txt\' in self.index.documents()\n\n        self.index.add_term_occurrence(\'phone\', \'document9.txt\')\n        self.index.add_term_occurrence(\'word\', \'document1.txt\')\n\n        assert self.index.get_term_frequency(\'word\', \'document1.txt\') == 4\n        assert self.index.get_term_frequency(\'phone\', \'document9.txt\') == 1\n\n        # Terms should be add-able\n        self.index.unfreeze()\n        self.index.add_term_occurrence(\'myword\', \'document2.txt\')\n        assert \'myword\' in self.index\n\n    def test_get_total_term_frequency(self):\n        assert self.index.get_total_term_frequency(\'word\') == 5\n        assert self.index.get_total_term_frequency(\'malta\') == 5\n        assert self.index.get_total_term_frequency(\'phone\') == 4\n\n    def test_get_total_term_frequency_exceptions(self):\n        with pytest.raises(IndexError):\n            self.index.get_total_term_frequency(\'doesnotexist\')\n\n    def test_get_total_term_frequency_case(self):\n        with pytest.raises(IndexError):\n            self.index.get_total_term_frequency(\'WORD\')\n        with pytest.raises(IndexError):\n            self.index.get_total_term_frequency(\'Malta\')\n        with pytest.raises(IndexError):\n            self.index.get_total_term_frequency(\'phonE\')\n\n    def test_get_term_frequency(self):\n        # Check Existing cases\n        assert self.index.get_term_frequency(\'word\', \'document1.txt\') == 3\n        assert self.index.get_term_frequency(\'malta\', \'document1.txt\') == 5\n        assert self.index.get_term_frequency(\'phone\', \'document2.txt\') == 4\n        assert self.index.get_term_frequency(\'word\', \'document2.txt\') == 2\n\n        # Check non existing cases\n        assert self.index.get_term_frequency(\'malta\', \'document2.txt\') == 0\n        assert self.index.get_term_frequency(\'phone\', \'document1.txt\') == 0\n\n    def test_get_term_frequency_exceptions(self):\n        with pytest.raises(IndexError):\n            self.index.get_term_frequency(\'doesnotexist\', \'document1.txt\')\n        with pytest.raises(IndexError):\n            self.index.get_term_frequency(\'malta\', \'deoesnotexist.txt\')\n\n    def test_get_document_frequency(self):\n        assert self.index.get_document_frequency(\'word\') == 2\n        assert self.index.get_document_frequency(\'malta\') == 1\n        assert self.index.get_document_frequency(\'phone\') == 1\n\n    def test_get_document_frequency_exceptions(self):\n        with pytest.raises(IndexError):\n            self.index.get_document_frequency(\'doesnotexist\')\n\n    def test_get_document_length(self):\n        assert self.index.get_document_length(\'document1.txt\') == 8\n        assert self.index.get_document_length(\'document2.txt\') == 6\n\n    def test_get_document_length_exceptions(self):\n        with pytest.raises(IndexError):\n            self.index.get_document_length(\'doesnotexist.txt\')\n\n    def test_get_terms(self):\n        assert unordered_list_cmp(self.index.terms(), [\'word\', \'malta\', \'phone\'])\n\n        self.index.add_term_occurrence(\'test\', \'document3.txt\')\n        assert unordered_list_cmp(self.index.terms(), [\'word\', \'malta\', \'phone\', \'test\'])\n\n        assert \'doesnotexist\' not in self.index.terms()\n\n    def test_get_items(self):\n        assert self.index.items() == {\n            \'word\': {\'document1.txt\': 3, \'document2.txt\': 2},\n            \'malta\': {\'document1.txt\': 5},\n            \'phone\': {\'document2.txt\': 4}\n        }\n\n    def test_get_tfidf_relation(self):\n        # Test Inverse Document Frequency\n        self.assertLess(\n            self.index.get_tfidf(\'word\', \'document1.txt\'),\n            self.index.get_tfidf(\'malta\', \'document1.txt\')\n        )\n\n    def test_get_tfidf_relation_normalized(self):\n        self.assertLess(\n            self.index.get_tfidf(\'word\', \'document1.txt\', normalized=True),\n            self.index.get_tfidf(\'malta\', \'document1.txt\', normalized=True)\n        )\n\n    def test_get_tfidf_empty_document(self):\n        assert self.index.get_tfidf(\'malta\', \'document2.txt\') == 0\n\n    def test_get_tfidf_empty_term(self):\n        assert self.index.get_tfidf(\'phone\', \'document1.txt\') == 0\n\n    def test_get_total_tfidf(self):\n        # Not validated manually, but pinned here to ensure it remains consistent\n        assert self.index.get_total_tfidf(\'malta\') == pytest.approx(1.5051499)\n\n    def test_generate_document_vector_default(self):\n        self.assertListEqual(\n            self.index.generate_document_vector(\'document1.txt\'),\n            self.index.generate_document_vector(\'document1.txt\', mode=\'tfidf\'),\n        )\n\n    def test_generate_docuemnt_vector_normalized(self):\n        vector = self.index.generate_document_vector(\'document1.txt\', mode=\'ntfidf\')\n        assert len(vector) == 3\n\n    def test_generate_document_vector_custom_function(self):\n        def custom_weighting(index, term, document):\n            return index.get_document_length(document)\n\n        self.assertListEqual(\n            self.index.generate_document_vector(\'document1.txt\', mode=custom_weighting),\n            [8, 8, 8],\n        )\n\n        def custom_weighting_2(index, term, document):\n            return 1.0\n\n        self.assertListEqual(\n            self.index.generate_document_vector(\'document1.txt\', mode=custom_weighting_2),\n            [1.0, 1.0, 1.0],\n        )\n\n    def test_generate_feature_matrix_default(self):\n        self.assertListEqual(\n            self.index.generate_feature_matrix(),\n            self.index.generate_feature_matrix(mode=\'tfidf\'),\n        )\n\n    def test_generate_feature_matrix_tfidf(self):\n        features = self.index.terms()\n        instances = self.index.documents()\n\n        matrix = self.index.generate_feature_matrix(mode=\'tfidf\')\n\n        self.assertEqual(\n            matrix[instances.index(\'document1.txt\')][features.index(\'malta\')],\n            self.index.get_tfidf(\'malta\', \'document1.txt\'),\n        )\n\n        self.assertEqual(\n            matrix[instances.index(\'document2.txt\')][features.index(\'word\')],\n            self.index.get_tfidf(\'word\', \'document2.txt\'),\n        )\n\n        self.assertEqual(\n            matrix[instances.index(\'document2.txt\')][features.index(\'phone\')],\n            self.index.get_tfidf(\'phone\', \'document2.txt\'),\n        )\n\n        self.assertEqual(\n            matrix[instances.index(\'document1.txt\')][features.index(\'word\')],\n            self.index.get_tfidf(\'word\', \'document1.txt\'),\n        )\n\n        # Zero Cases\n        assert matrix[instances.index(\'document2.txt\')][features.index(\'malta\')] == 0\n        assert matrix[instances.index(\'document1.txt\')][features.index(\'phone\')] == 0\n\n    def test_generate_document_vector_count(self):\n        features = self.index.terms()\n        vector = self.index.generate_document_vector(\'document1.txt\', mode=\'tf\')\n\n        # Correct vector shape\n        assert len(vector) == len(features)\n\n        assert vector[features.index(\'malta\')] == 5.0\n        assert vector[features.index(\'word\')] == 3.0\n        assert vector[features.index(\'phone\')] == 0.0\n\n    def test_generate_feature_matrix_tf(self):\n        # Extract the feature and document indices\n        features = self.index.terms()\n        instances = self.index.documents()\n\n        matrix = self.index.generate_feature_matrix(mode=\'tf\')\n\n        # Correct matrix dimensions\n        assert len(matrix) == 2\n        for row in matrix:\n            assert len(row) == 3\n\n        # Ensure this method of addressing data works\n        assert matrix[instances.index(\'document1.txt\')][features.index(\'malta\')] == 5\n        assert matrix[instances.index(\'document2.txt\')][features.index(\'word\')] == 2\n        assert matrix[instances.index(\'document1.txt\')][features.index(\'word\')] == 3\n        assert matrix[instances.index(\'document2.txt\')][features.index(\'phone\')] == 4\n\n        # Zero cases\n        assert matrix[instances.index(\'document2.txt\')][features.index(\'malta\')] == 0\n        assert matrix[instances.index(\'document1.txt\')][features.index(\'phone\')] == 0\n\n    def test_generate_feature_matrix_ntf(self):\n        features = self.index.terms()\n        instances = self.index.documents()\n\n        matrix = self.index.generate_feature_matrix(mode=\'ntf\')\n\n        assert matrix[instances.index(\'document1.txt\')][features.index(\'word\')] == 3 / 8\n        assert matrix[instances.index(\'document2.txt\')][features.index(\'phone\')] == 4 / 6\n        assert matrix[instances.index(\'document1.txt\')][features.index(\'malta\')] == 5 / 8\n\n        assert matrix[instances.index(\'document2.txt\')][features.index(\'malta\')] == 0\n        assert matrix[instances.index(\'document2.txt\')][features.index(\'word\')] == 2 / 6\n\n    def test_generate_feature_matrix_invalid(self):\n        with pytest.raises(ValueError):\n            self.index.generate_feature_matrix(mode=\'invalid\')\n        with pytest.raises(ValueError):\n            self.index.generate_feature_matrix(mode=None)\n\n\nclass SerializationTest(unittest.TestCase):\n    def setUp(self):\n        self.index = hashedindex.HashedIndex()\n\n        for i in range(3):\n            self.index.add_term_occurrence(\'word\', \'document1.txt\')\n\n        for i in range(5):\n            self.index.add_term_occurrence(\'malta\', \'document1.txt\')\n\n        for i in range(4):\n            self.index.add_term_occurrence(\'phone\', \'document2.txt\')\n\n        for i in range(2):\n            self.index.add_term_occurrence(\'word\', \'document2.txt\')\n\n    def test_to_dict(self):\n        assert self.index.to_dict() == {\n            \'documents\': {\'document1.txt\': 8, \'document2.txt\': 6},\n            \'terms\': {\n                \'word\': {\'document1.txt\': 3, \'document2.txt\': 2},\n                \'malta\': {\'document1.txt\': 5},\n                \'phone\': {\'document2.txt\': 4},\n            }\n        }\n\n    def test_from_dict(self):\n        index2 = hashedindex.HashedIndex()\n        index2.from_dict({\n            \'documents\': {\'a\': 2, \'b\': 3},\n            # Does not test for validity\n            \'terms\': {\n                \'foo\': {\'a\': 20, \'b\': 40},\n                \'bar\': {\'a\': 65, \'b\': 2},\n            }\n        })\n\n        assert unordered_list_cmp(index2.terms(), [\'foo\', \'bar\'])\n        assert unordered_list_cmp(index2.documents(), [\'a\', \'b\'])\n        assert index2.get_documents(\'foo\') == collections.Counter({\'a\': 20, \'b\': 40})\n        assert index2.get_documents(\'bar\') == collections.Counter({\'a\': 65, \'b\': 2})\n\n    def test_integrity(self):\n        index2 = hashedindex.HashedIndex()\n        index2.from_dict(self.index.to_dict())\n\n        assert index2 == self.index\n\n    def test_json_seriazable(self):\n        assert json.dumps(self.index.to_dict())\n\n\nclass PruneIndexTest(unittest.TestCase):\n    def setUp(self):\n        self.index = hashedindex.HashedIndex()\n\n        for i in range(100):\n            self.index.add_term_occurrence(\'word\', \'document{}.txt\'.format(i))\n\n        for i in range(20):\n            self.index.add_term_occurrence(\'text\', \'document{}.txt\'.format(i))\n\n        self.index.add_term_occurrence(\'lonely\', \'document2.txt\')\n\n    def test_min_prune(self):\n        self.index.prune(min_value=2)\n        assert unordered_list_cmp(self.index.terms(), [\'word\', \'text\'])\n\n        self.index.prune(min_value=25)\n        assert unordered_list_cmp(self.index.terms(), [\'word\'])\n\n    def test_max_prune(self):\n        self.index.prune(max_value=20)\n        assert unordered_list_cmp(self.index.terms(), [\'text\', \'lonely\'])\n\n    def test_min_prune_percentile(self):\n        self.index.prune(min_value=0.25, use_percentile=True)\n        assert unordered_list_cmp(self.index.terms(), [\'word\'])\n\n    def test_max_prune_percentile(self):\n        self.index.prune(max_value=0.20, use_percentile=True)\n        assert unordered_list_cmp(self.index.terms(), [\'text\', \'lonely\'])\n\n\nclass MergeIndexTest(unittest.TestCase):\n    def setUp(self):\n        self.first_index = hashedindex.HashedIndex()\n        self.first_index.add_term_occurrence(\'foo\', \'document2.txt\')\n        self.first_index.add_term_occurrence(\'foo\', \'document1.txt\')\n\n        self.second_index = hashedindex.HashedIndex()\n        self.second_index.add_term_occurrence(\'foo\', \'document1.txt\')\n        self.second_index.add_term_occurrence(\'bar\', \'document9.txt\')\n\n    def test_merge_index_empty(self):\n        assert hashedindex.merge([]) == hashedindex.HashedIndex()\n\n    def test_merge_index_single(self):\n        assert hashedindex.merge([self.first_index]) == self.first_index\n\n    def test_merge_index(self):\n        merged_index = hashedindex.merge([\n            self.first_index,\n            self.second_index,\n        ])\n\n        assert unordered_list_cmp(\n            merged_index.terms(),\n            [\'foo\', \'bar\']\n        )\n        assert unordered_list_cmp(\n            merged_index.documents(),\n            [\'document1.txt\', \'document2.txt\', \'document9.txt\']\n        )\n\n        assert merged_index._terms[\'foo\'] == {\'document1.txt\': 2, \'document2.txt\': 1}\n        assert merged_index._terms[\'bar\'] == {\'document9.txt\': 1}\n\n        assert merged_index._documents[\'document1.txt\'] == 2\n        assert merged_index._documents[\'document2.txt\'] == 1\n        assert merged_index._documents[\'document9.txt\'] == 1\n'"
tests/test_parser.py,0,"b""import unittest\n\nfrom hashedindex import textparser\n\n\nclass TfidfTestCase(unittest.TestCase):\n\n    def test_zero_term_frequency(self):\n        assert textparser.tfidf(tf=0, df=10, corpus_size=1) == 0\n\n    def test_zero_document_frequency(self):\n        assert textparser.tfidf(tf=10, df=0, corpus_size=1) == 0\n\n\nclass ValidateStemmerTestCase(unittest.TestCase):\n\n    class EmptyStemmer(object):\n\n        def stem(self, x):\n            return ''\n\n    def test_null_stemmer(self):\n        stemmer = textparser.NullStemmer()\n        textparser.validate_stemmer(stemmer)\n\n    def test_empty_stemmer(self):\n        stemmer = self.EmptyStemmer()\n        textparser.validate_stemmer(stemmer)\n\n    def test_stemmer_constant(self):\n        kwargs = {'stemmer': 'string'}\n        exception = textparser.InvalidStemmerException\n        self.assertRaises(exception, textparser.validate_stemmer, **kwargs)\n\n    def test_stemmer_function(self):\n        kwargs = {'stemmer': lambda: None}\n        exception = textparser.InvalidStemmerException\n        self.assertRaises(exception, textparser.validate_stemmer, **kwargs)\n\n    def test_stemmer_missing_stem_attribute(self):\n        kwargs = {'stemmer': object()}\n        exception = textparser.InvalidStemmerException\n        self.assertRaises(exception, textparser.validate_stemmer, **kwargs)\n\n    def test_stemmer_stem_not_callable(self):\n        stemmer = type('', (object,), {'stem': None})()\n        kwargs = {'stemmer': stemmer}\n        exception = textparser.InvalidStemmerException\n        self.assertRaises(exception, textparser.validate_stemmer, **kwargs)\n\n\nclass IsNumericTestCase(unittest.TestCase):\n\n    def test_integer(self):\n        assert textparser.isnumeric('23')\n        assert textparser.isnumeric('8431')\n\n    def test_float(self):\n        assert textparser.isnumeric('23.480')\n        assert textparser.isnumeric('9.6502')\n\n    def test_scientific_notation(self):\n        assert textparser.isnumeric('1e-10')\n        assert textparser.isnumeric('2e+54')\n\n    def test_no_numeric(self):\n        assert not textparser.isnumeric('foo')\n        assert not textparser.isnumeric('10 foo')\n\n\nclass GetNGramsTestCase(unittest.TestCase):\n\n    def test_bigram_token_list(self):\n        assert list(textparser.get_ngrams(\n            token_list=['one', 'two', 'three', 'four'],\n        )) == [['one', 'two'], ['two', 'three'], ['three', 'four']]\n\n    def test_trigram_token_list(self):\n        assert list(textparser.get_ngrams(\n            token_list=['one', 'two', 'three', 'four'],\n            n=3,\n        )) == [\n            ['one', 'two', 'three'],\n            ['two', 'three', 'four'],\n        ]\n\n\nclass WordTokenizeTestCase(unittest.TestCase):\n\n    class NaivePluralStemmer(object):\n\n        def stem(self, x):\n            return x.rstrip('s')\n\n    def test_sentence(self):\n        assert list(textparser.word_tokenize(\n            text='Life is about making an impact, not making an income.',\n        )) == [\n            ('life', ), ('is', ), ('about', ),\n            ('making', ), ('an', ), ('impact', ),\n            ('not', ), ('making', ), ('an', ),\n            ('income', )\n        ]\n\n    def test_strips_punctuation(self):\n        assert list(textparser.word_tokenize(\n            text='first. second',\n        )) == [('first', ), ('second', )]\n\n    def test_inner_punctuation(self):\n        assert list(textparser.word_tokenize(\n            text='decision is a and/or b',\n        )) == [('decision',), ('is',), ('a',), ('and/or',), ('b',)]\n\n    def test_ignores_stopwords(self):\n        assert list(textparser.word_tokenize(\n            text='The first rule of python is',\n            stopwords=set(['the', 'of', 'is']),\n            min_length=1,\n        )) == [('first', ), ('rule', ), ('python', )]\n\n    def test_min_length(self):\n        assert list(textparser.word_tokenize(\n            text='one for the money two for the go',\n            min_length=4,\n        )) == [('money', )]\n\n    def test_ignores_numeric(self):\n        assert list(textparser.word_tokenize(\n            text='one two 3 four',\n        )) == [('one', ), ('two', ), ('four', )]\n\n    def test_retains_casing(self):\n        assert list(textparser.word_tokenize(\n            text='Three letter acronym (TLA)',\n            retain_casing=True\n        )) == [('Three', ), ('letter', ), ('acronym', ), ('TLA',)]\n\n    def test_retains_punctuation(self):\n        assert list(textparser.word_tokenize(\n            text='who, where? (question!)',\n            retain_punctuation=True\n        )) == [('who', ), (',', ), ('where',), ('?', ), ('(',), ('question',), ('!',), (')',)]\n\n    def test_retains_punctuation_within_tokens(self):\n        assert list(textparser.word_tokenize(\n            text='is the oven pre-heated?',\n            retain_punctuation=True\n        )) == [('is', ), ('the', ), ('oven',), ('pre-heated', ), ('?',)]\n\n    def test_ngrams(self):\n        assert list(textparser.word_tokenize(\n            text='foo bar bomb blar',\n            ngrams=2,\n        )) == [('foo', 'bar'), ('bar', 'bomb'), ('bomb', 'blar')]\n\n    def test_ngram_unsatisfiable(self):\n        assert list(textparser.word_tokenize(\n            text='foo bar',\n            ngrams=3,\n        )) == []\n\n    def test_stemming(self):\n        assert list(textparser.word_tokenize(\n            text='one examples',\n            stemmer=self.NaivePluralStemmer()\n        )) == [('one',), ('example',)]\n\n    def test_tokenize_whitespace(self):\n        assert list(textparser.word_tokenize(\n            text='around   the world',\n            tokenize_whitespace=True\n        )) == [('around',), ('   ',), ('the',), (' ',), ('world', )]\n\n    def test_tokenize_punctuation_and_whitespace(self):\n        assert list(textparser.word_tokenize(\n            text='who, where, what, when, why?',\n            retain_punctuation=True,\n            tokenize_whitespace=True\n        )) == [\n            ('who',), (',',), (' ',),\n            ('where',), (',',), (' ',),\n            ('what',), (',',), (' ',),\n            ('when',), (',',), (' ',),\n            ('why',), ('?',),\n        ]\n\n\nclass TestNullStemmer(unittest.TestCase):\n    def test_repr(self):\n        stemmer = textparser.NullStemmer()\n        assert str(stemmer) == repr(stemmer) == '<NullStemmer>'\n\n    def test_stem(self0):\n        stemmer = textparser.NullStemmer()\n        assert stemmer.stem('hello  ') == 'hello  '\n\n\nclass NormalizeUnicode(unittest.TestCase):\n    def test_empty(self):\n        assert textparser.normalize_unicode('') == ''\n\n    def test_correct_output(self):\n        assert textparser.normalize_unicode('i\xc3\xa4\xc3\xb6\xc3\xbc') == 'iaou'\n\n\nclass IsUrlTestCase(unittest.TestCase):\n\n    def test_http_url(self):\n        assert textparser.is_url('http://www.google.com')\n\n    def test_https_url(self):\n        assert textparser.is_url('https://www.google.com')\n\n    def test_url_with_path(self):\n        assert textparser.is_url('https://www.facebook.com/some/path/here')\n\n    def test_url_with_query_string(self):\n        assert textparser.is_url('https://www.yplanapp.com?foo=1&bar=2')\n\n    def test_not_a_url(self):\n        assert not textparser.is_url('foo')\n        assert not textparser.is_url('bar')\n        assert not textparser.is_url('waterboat')\n"""
