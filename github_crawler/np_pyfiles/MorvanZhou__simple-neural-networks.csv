file_path,api_count,code
save_model.py,3,"b'import numpy as np\nimport neuralnets as nn\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1)\nx = np.linspace(-1, 1, 200)[:, None]       # [batch, 1]\ny = x ** 2 + np.random.normal(0., 0.1, (200, 1))     # [batch, 1]\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.layers.Dense(1, 10, nn.act.tanh)\n        self.out = nn.layers.Dense(10, 1, )\n\n    def forward(self, x):\n        x = self.l1(x)\n        o = self.out(x)\n        return o\n\n\nnet1 = Net()\nopt = nn.optim.Adam(net1.params, lr=0.1)\nloss_fn = nn.losses.MSE()\n\nfor _ in range(1000):\n    o = net1.forward(x)\n    loss = loss_fn(o, y)\n    net1.backward(loss)\n    opt.step()\n    print(loss)\n\n# save net1 and restore to net2\nnet1.save(""./params.pkl"")\nnet2 = Net()\nnet2.restore(""./params.pkl"")\no2 = net2.forward(x)\n\nplt.scatter(x, y, s=20)\nplt.plot(x, o2.data, c=""red"", lw=3)\nplt.show()\n\n'"
simple_nn.py,13,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1)\nx = np.linspace(-1, 1, 200)[:, None]       # [batch, 1]\ny = x ** 2                                  # [batch, 1]\nlearning_rate = 0.001\n\n\ndef tanh(x):\n    return np.tanh(x)\n\n\ndef derivative_tanh(x):\n    return 1 - tanh(x)**2\n\n\nw1 = np.random.uniform(0, 1, (1, 10))\nw2 = np.random.uniform(0, 1, (10, 10))\nw3 = np.random.uniform(0, 1, (10, 1))\n\nb1 = np.full((1, 10), 0.1)\nb2 = np.full((1, 10), 0.1)\nb3 = np.full((1, 1), 0.1)\n\n\nfor i in range(300):\n    a1 = x\n    z2 = a1.dot(w1) + b1\n    a2 = tanh(z2)\n    z3 = a2.dot(w2) + b2\n    a3 = tanh(z3)\n    z4 = a3.dot(w3) + b3\n\n    cost = np.sum((z4 - y)**2)/2\n\n    # backpropagation\n    z4_delta = z4 - y\n    dw3 = a3.T.dot(z4_delta)\n    db3 = np.sum(z4_delta, axis=0, keepdims=True)\n\n    z3_delta = z4_delta.dot(w3.T) * derivative_tanh(z3)\n    dw2 = a2.T.dot(z3_delta)\n    db2 = np.sum(z3_delta, axis=0, keepdims=True)\n\n    z2_delta = z3_delta.dot(w2.T) * derivative_tanh(z2)\n    dw1 = x.T.dot(z2_delta)\n    db1 = np.sum(z2_delta, axis=0, keepdims=True)\n\n    # update parameters\n    for param, gradient in zip([w1, w2, w3, b1, b2, b3], [dw1, dw2, dw3, db1, db2, db3]):\n        param -= learning_rate * gradient\n\n    print(cost)\n\nplt.plot(x, z4)\nplt.show()'"
train_classifier.py,7,"b'import numpy as np\nimport neuralnets as nn\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1)\nx0 = np.random.normal(-2, 1, (100, 2))\nx1 = np.random.normal(2, 1, (100, 2))\ny0 = np.zeros((100, 1), dtype=np.int32)\ny1 = np.ones((100, 1), dtype=np.int32)\nx = np.concatenate((x0, x1), axis=0)\ny = np.concatenate((y0, y1), axis=0)\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        w_init = nn.init.RandomUniform()\n        b_init = nn.init.Constant(0.1)\n\n        self.l1 = nn.layers.Dense(2, 10, nn.act.tanh, w_init, b_init)\n        self.l2 = nn.layers.Dense(10, 10, nn.act.tanh, w_init, b_init)\n        self.out = nn.layers.Dense(10, 1, nn.act.sigmoid)\n\n    def forward(self, x):\n        x = self.l1(x)\n        x = self.l2(x)\n        o = self.out(x)\n        return o\n\n\nnet = Net()\nopt = nn.optim.Adam(net.params, lr=0.1)\nloss_fn = nn.losses.SigmoidCrossEntropy()\n\nfor step in range(30):\n    o = net.forward(x)\n    loss = loss_fn(o, y)\n    net.backward(loss)\n    opt.step()\n    acc = nn.metrics.accuracy(o.data > 0.5, y)\n    print(""Step: %i | loss: %.5f | acc: %.2f"" % (step, loss.data, acc))\n\nprint(net.forward(x[:10]).data.ravel(), ""\\n"", y[:10].ravel())\nplt.scatter(x[:, 0], x[:, 1], c=(o.data > 0.5).ravel(), s=100, lw=0, cmap=\'RdYlGn\')\nplt.show()\n\n'"
train_cnn.py,3,"b'import neuralnets as nn\nimport numpy as np\n\nnp.random.seed(1)\nf = np.load(\'./mnist.npz\')\ntrain_x, train_y = f[\'x_train\'][:, :, :, None], f[\'y_train\'][:, None]\ntest_x, test_y = f[\'x_test\'][:2000][:, :, :, None], f[\'y_test\'][:2000]\n\ntrain_loader = nn.DataLoader(train_x, train_y, batch_size=64)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.seq_layers = self.sequential(\n            nn.layers.Conv2D(1, 6, (5, 5), (1, 1), ""same"", channels_last=True),  # => [n,28,28,6]\n            nn.layers.MaxPool2D(2, 2),  # => [n, 14, 14, 6]\n            nn.layers.Conv2D(6, 16, 5, 1, ""same"", channels_last=True),  # => [n,14,14,16]\n            nn.layers.MaxPool2D(2, 2),  # => [n,7,7,16]\n            nn.layers.Flatten(),  # => [n,7*7*16]\n            nn.layers.Dense(7 * 7 * 16, 10, )\n        )\n\n    def forward(self, x):\n        o = self.seq_layers.forward(x)\n        return o\n\n\ncnn = CNN()\nopt = nn.optim.Adam(cnn.params, 0.001)\nloss_fn = nn.losses.SparseSoftMaxCrossEntropyWithLogits()\n\n\nfor step in range(300):\n    bx, by = train_loader.next_batch()\n    by_ = cnn.forward(bx)\n    loss = loss_fn(by_, by)\n    cnn.backward(loss)\n    opt.step()\n    if step % 50 == 0:\n        ty_ = cnn.forward(test_x)\n        acc = nn.metrics.accuracy(np.argmax(ty_.data, axis=1), test_y)\n        print(""Step: %i | loss: %.3f | acc: %.2f"" % (step, loss.data, acc))\n\n'"
train_regressor.py,3,"b'import numpy as np\nimport neuralnets as nn\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1)\nx = np.linspace(-1, 1, 200)[:, None]       # [batch, 1]\ny = x ** 2 + np.random.normal(0., 0.1, (200, 1))     # [batch, 1]\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        w_init = nn.init.RandomUniform()\n        b_init = nn.init.Constant(0.1)\n\n        self.l1 = nn.layers.Dense(1, 10, nn.act.tanh, w_init, b_init)\n        self.l2 = nn.layers.Dense(10, 10, nn.act.tanh, w_init, b_init)\n        self.out = nn.layers.Dense(10, 1, w_initializer=w_init, b_initializer=b_init)\n\n    def forward(self, x):\n        x = self.l1(x)\n        x = self.l2(x)\n        o = self.out(x)\n        return o\n\n\nnet = Net()\nopt = nn.optim.Adam(net.params, lr=0.1)\nloss_fn = nn.losses.MSE()\n\nfor step in range(100):\n    o = net.forward(x)\n    loss = loss_fn(o, y)\n    net.backward(loss)\n    opt.step()\n    print(""Step: %i | loss: %.5f"" % (step, loss.data))\n\nplt.scatter(x, y, s=20)\nplt.plot(x, o.data, c=""red"", lw=3)\nplt.show()\n\n'"
neuralnets/__init__.py,0,b'from . import initializers as init\nfrom . import activations as act\nfrom .module import Module\nfrom . import losses\nfrom . import optimizers as optim\nfrom .variable import Variable\nfrom .saver import Saver\nfrom .dataloader import DataLoader\nfrom . import metrics\n\nfrom . import layers\n'
neuralnets/activations.py,16,"b'import numpy as np\n\n\nclass Activation:\n    def forward(self, x):\n        raise NotImplementedError\n\n    def derivative(self, x):\n        raise NotImplementedError\n\n    def __call__(self, *inputs):\n        return self.forward(*inputs)\n\n\nclass Linear(Activation):\n    def forward(self, x):\n        return x\n\n    def derivative(self, x):\n        return np.ones_like(x)\n\n\nclass ReLU(Activation):\n    def forward(self, x):\n        return np.maximum(0, x)\n\n    def derivative(self, x):\n        return np.where(x > 0, np.ones_like(x), np.zeros_like(x))\n\n\nclass LeakyReLU(Activation):\n    def __init__(self, alpha=0.01):\n        self.alpha = alpha\n\n    def forward(self, x):\n        return np.maximum(self.alpha * x, x)\n\n    def derivative(self, x):\n        return np.where(x > 0., np.ones_like(x), np.full_like(x, self.alpha))\n\n\nclass ELU(Activation):\n    def __init__(self, alpha=0.01):\n        self.alpha = alpha\n\n    def forward(self, x):\n        return np.maximum(x, self.alpha*(np.exp(x)-1))\n\n    def derivative(self, x):\n        return np.where(x > 0., np.ones_like(x), self.forward(x) + self.alpha)\n\n\nclass Tanh(Activation):\n    def forward(self, x):\n        return np.tanh(x)\n\n    def derivative(self, x):\n        return 1. - np.square(np.tanh(x))\n\n\nclass Sigmoid(Activation):\n    def forward(self, x):\n        return 1./(1.+np.exp(-x))\n\n    def derivative(self, x):\n        f = self.forward(x)\n        return f*(1.-f)\n\n\nclass SoftPlus(Activation):\n    def forward(self, x):\n        return np.log(1. + np.exp(x))\n\n    def derivative(self, x):\n        return 1. / (1. + np.exp(-x))\n\n\nclass SoftMax(Activation):\n    def forward(self, x, axis=-1):\n        shift_x = x - np.max(x, axis=axis, keepdims=True)   # stable softmax\n        exp = np.exp(shift_x + 1e-6)\n        return exp / np.sum(exp, axis=axis, keepdims=True)\n\n    def derivative(self, x):\n        return np.ones_like(x)\n\n\nrelu = ReLU()\nleakyrelu = LeakyReLU()\nelu = ELU()\ntanh = Tanh()\nsigmoid = Sigmoid()\nsoftplus = SoftPlus()\nsoftmax = SoftMax()\n\n'"
neuralnets/dataloader.py,1,"b'import numpy as np\n\n\nclass DataLoader:\n    def __init__(self, x, y, batch_size):\n        self.x = x\n        self.y = y\n        self.bs = batch_size\n        self.p = 0\n        self.bg = self.batch_generator()\n\n    def batch_generator(self):\n        while True:\n            p_ = self.p + self.bs\n            if p_ > len(self.x):\n                self.p = 0\n                continue\n            if self.p == 0:\n                indices = np.random.permutation(len(self.x))\n                self.x[:] = self.x[indices]\n                self.y[:] = self.y[indices]\n            bx = self.x[self.p:p_]\n            by = self.y[self.p:p_]\n            self.p = p_\n            yield bx, by\n\n    def next_batch(self):\n        return next(self.bg)\n'"
neuralnets/initializers.py,7,"b'import numpy as np\n\n\nclass BaseInitializer:\n    def initialize(self, x):\n        raise NotImplementedError\n\n\nclass RandomNormal(BaseInitializer):\n    def __init__(self, mean=0., std=1.):\n        self._mean = mean\n        self._std = std\n\n    def initialize(self, x):\n        x[:] = np.random.normal(loc=self._mean, scale=self._std, size=x.shape)\n\n\nclass RandomUniform(BaseInitializer):\n    def __init__(self, low=0., high=1.):\n        self._low = low\n        self._high = high\n\n    def initialize(self, x):\n        x[:] = np.random.uniform(self._low, self._high, size=x.shape)\n\n\nclass Zeros(BaseInitializer):\n    def initialize(self, x):\n        x[:] = np.zeros_like(x)\n\n\nclass Ones(BaseInitializer):\n    def initialize(self, x):\n        x[:] = np.ones_like(x)\n\n\nclass TruncatedNormal(BaseInitializer):\n    def __init__(self, mean=0., std=1.):\n        self._mean = mean\n        self._std = std\n\n    def initialize(self, x):\n        x[:] = np.random.normal(loc=self._mean, scale=self._std, size=x.shape)\n        truncated = 2*self._std + self._mean\n        x[:] = np.clip(x, -truncated, truncated)\n\n\nclass Constant(BaseInitializer):\n    def __init__(self, v):\n        self._v = v\n\n    def initialize(self, x):\n        x[:] = np.full_like(x, self._v)\n\n\nrandom_normal = RandomNormal()\nrandom_uniform = RandomUniform()\nzeros = Zeros()\nones = Ones()\ntruncated_normal = TruncatedNormal()\n'"
neuralnets/layers.py,30,"b'import numpy as np\nimport neuralnets as nn\n\n\nclass BaseLayer:\n    def __init__(self):\n        self.order = None\n        self.name = None\n        self._x = None\n        self.data_vars = {}\n\n    def forward(self, x):\n        raise NotImplementedError\n\n    def backward(self):\n        raise NotImplementedError\n\n    def _process_input(self, x):\n        if isinstance(x, np.ndarray):\n            x = x.astype(np.float32)\n            x = nn.Variable(x)\n            x.info[""new_layer_order""] = 0\n\n        self.data_vars[""in""] = x\n        # x is Variable, extract _x value from x.data\n        self.order = x.info[""new_layer_order""]\n        _x = x.data\n        return _x\n\n    def _wrap_out(self, out):\n        out = nn.Variable(out)\n        out.info[""new_layer_order""] = self.order + 1\n        self.data_vars[""out""] = out     # add to layer\'s data_vars\n        return out\n\n    def __call__(self, x):\n        return self.forward(x)\n\n\nclass ParamLayer(BaseLayer):\n    def __init__(self, w_shape, activation, w_initializer, b_initializer, use_bias):\n        super().__init__()\n        self.param_vars = {}\n        self.w = np.empty(w_shape, dtype=np.float32)\n        self.param_vars[""w""] = self.w\n        if use_bias:\n            shape = [1]*len(w_shape)\n            shape[-1] = w_shape[-1]     # only have bias on the last dimension\n            self.b = np.empty(shape, dtype=np.float32)\n            self.param_vars[""b""] = self.b\n        self.use_bias = use_bias\n\n        if activation is None:\n            self._a = nn.act.Linear()\n        elif isinstance(activation, nn.act.Activation):\n            self._a = activation\n        else:\n            raise TypeError\n\n        if w_initializer is None:\n            nn.init.TruncatedNormal(0., 0.01).initialize(self.w)\n        elif isinstance(w_initializer, nn.init.BaseInitializer):\n            w_initializer.initialize(self.w)\n        else:\n            raise TypeError\n\n        if use_bias:\n            if b_initializer is None:\n                nn.init.Constant(0.01).initialize(self.b)\n            elif isinstance(b_initializer, nn.init.BaseInitializer):\n                b_initializer.initialize(self.b)\n            else:\n                raise TypeError\n\n        self._wx_b = None\n        self._activated = None\n\n    def forward(self, x):\n        raise NotImplementedError\n\n    def backward(self):\n        raise NotImplementedError\n\n\nclass Dense(ParamLayer):\n    def __init__(self,\n                 n_in,\n                 n_out,\n                 activation=None,\n                 w_initializer=None,\n                 b_initializer=None,\n                 use_bias=True,\n                 ):\n        super().__init__(\n            w_shape=(n_in, n_out),\n            activation=activation,\n            w_initializer=w_initializer,\n            b_initializer=b_initializer,\n            use_bias=use_bias)\n\n        self._n_in = n_in\n        self._n_out = n_out\n\n    def forward(self, x):\n        self._x = self._process_input(x)\n        self._wx_b = self._x.dot(self.w)\n        if self.use_bias:\n            self._wx_b += self.b\n\n        self._activated = self._a(self._wx_b)   # if act is None, act will be Linear\n        wrapped_out = self._wrap_out(self._activated)\n        return wrapped_out\n\n    def backward(self):\n        # dw, db\n        dz = self.data_vars[""out""].error\n        dz *= self._a.derivative(self._wx_b)\n        grads = {""w"": self._x.T.dot(dz)}\n        if self.use_bias:\n            grads[""b""] = np.sum(dz, axis=0, keepdims=True)\n        # dx\n        self.data_vars[""in""].set_error(dz.dot(self.w.T))     # pass error to the layer before\n        return grads\n\n\nclass Conv2D(ParamLayer):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=(3, 3),\n                 strides=(1, 1),\n                 padding=\'valid\',\n                 channels_last=True,\n                 activation=None,\n                 w_initializer=None,\n                 b_initializer=None,\n                 use_bias=True,\n                 ):\n        self.kernel_size = get_tuple(kernel_size)\n        self.strides = get_tuple(strides)\n        super().__init__(\n            w_shape=(in_channels,) + self.kernel_size + (out_channels,),\n            activation=activation,\n            w_initializer=w_initializer,\n            b_initializer=b_initializer,\n            use_bias=use_bias)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.padding = padding.lower()\n        assert padding in (""valid"", ""same""), ValueError\n\n        self.channels_last = channels_last\n        self._padded = None\n        self._p_tblr = None     # padded dim from top, bottom, left, right\n\n    def forward(self, x):\n        self._x = self._process_input(x)\n        if not self.channels_last:  # channels_first\n            # [batch, channel, height, width] => [batch, height, width, channel]\n            self._x = np.transpose(self._x, (0, 2, 3, 1))\n        self._padded, tmp_conved, self._p_tblr = get_padded_and_tmp_out(\n            self._x, self.kernel_size, self.strides, self.out_channels, self.padding)\n\n        # convolution\n        self._wx_b = self.convolution(self._padded, self.w, tmp_conved)\n        if self.use_bias:   # tied biases\n            self._wx_b += self.b\n\n        self._activated = self._a(self._wx_b)\n        wrapped_out = self._wrap_out(\n            self._activated if self.channels_last else self._activated.transpose((0, 3, 1, 2)))\n        return wrapped_out\n\n    def backward(self):\n        # according to:\n        # https://medium.com/@2017csm1006/forward-and-backpropagation-in-convolutional-neural-network-4dfa96d7b37e\n        dz = self.data_vars[""out""].error\n        dz *= self._a.derivative(self._wx_b)\n\n        # dw, db\n        dw = np.empty_like(self.w)  # [c,h,w,out]\n        dw = self.convolution(self._padded.transpose((3, 1, 2, 0)), dz, dw)\n\n        grads = {""w"": dw}\n        if self.use_bias:   # tied biases\n            grads[""b""] = np.sum(dz, axis=(0, 1, 2), keepdims=True)\n\n        # dx\n        padded_dx = np.zeros_like(self._padded)    # [n, h, w, c]\n        s0, s1, k0, k1 = self.strides + self.kernel_size\n        t_flt = self.w.transpose((3, 1, 2, 0))  # [c, fh, hw, out] => [out, fh, fw, c]\n        for i in range(dz.shape[1]):\n            for j in range(dz.shape[2]):\n                padded_dx[:, i*s0:i*s0+k0, j*s1:j*s1+k1, :] += dz[:, i, j, :].reshape((-1, self.out_channels)).dot(\n                                                            t_flt.reshape((self.out_channels, -1))\n                                                        ).reshape((-1, k0, k1, padded_dx.shape[-1]))\n        t, b, l, r = [self._p_tblr[0], padded_dx.shape[1] - self._p_tblr[1],\n                      self._p_tblr[2], padded_dx.shape[2] - self._p_tblr[3]]\n        self.data_vars[""in""].set_error(padded_dx[:, t:b, l:r, :])      # pass error to the layer before\n        return grads\n\n    def convolution(self, x, flt, conved):\n        batch_size = x.shape[0]\n        t_flt = flt.transpose((1, 2, 0, 3))  # [c,h,w,out] => [h,w,c,out]\n        s0, s1, k0, k1 = self.strides + tuple(flt.shape[1:3])\n        for i in range(0, conved.shape[1]):  # in each row of the convoluted feature map\n            for j in range(0, conved.shape[2]):  # in each column of the convoluted feature map\n                x_seg_matrix = x[:, i*s0:i*s0+k0, j*s1:j*s1+k1, :].reshape((batch_size, -1))  # [n,h,w,c] => [n, h*w*c]\n                flt_matrix = t_flt.reshape((-1, flt.shape[-1]))  # [h,w,c, out] => [h*w*c, out]\n                filtered = x_seg_matrix.dot(flt_matrix)  # sum of filtered window [n, out]\n                conved[:, i, j, :] = filtered\n        return conved\n\n    def fast_convolution(self, x, flt, conved):\n        # according to:\n        # http://fanding.xyz/2017/09/07/CNN%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%9A%84Python%E5%AE%9E%E7%8E%B0III-CNN%E5%AE%9E%E7%8E%B0/\n\n        # create patch matrix\n        oh, ow, sh, sw, fh, fw = [conved.shape[1], conved.shape[2], self.strides[0],\n                                  self.strides[1], flt.shape[1], flt.shape[2]]\n        n, h, w, c = x.shape\n        shape = (n, oh, ow, fh, fw, c)\n        strides = (c * h * w, sh * w, sw, w, 1, h * w)\n        strides = x.itemsize * np.array(strides)\n        x_col = np.lib.stride_tricks.as_strided(x, shape=shape, strides=strides, writeable=False)\n        x_col = np.ascontiguousarray(x_col)\n        x_col.shape = (n * oh * ow, fh * fw * c)    # [n*oh*ow, fh*fw*c]\n        self._padded_col = x_col       # padded [n,h,w,c] => [n*oh*ow, h*w*c]\n        w_t = flt.transpose((1, 2, 0, 3)).reshape(-1, self.out_channels)  # => [hwc, oc]\n\n        # IMPORTANT! as_stride function has some wired behaviours\n        # which gives a not accurate result (precision issue) when performing matrix dot product.\n        # I have compared the fast convolution with normal convolution and cannot explain the precision issue.\n        wx = self._padded_col.dot(w_t)  # [n*oh*ow, fh*fw*c] dot [fh*fw*c, oc] => [n*oh*ow, oc]\n        return wx.reshape(conved.shape)\n\n    def fast_backward(self):\n        dz = self.data_vars[""out""].error\n        dz *= self._a.derivative(self._wx_b)\n\n        # dw, db\n        dz_reshape = dz.reshape(-1, self.out_channels)      # => [n*oh*ow, oc]\n        # self._padded_col.T~[fh*fw*c, n*oh*ow] dot [n*oh*ow, oc] => [fh*fw*c, oc]\n        dw = self._padded_col.T.dot(dz_reshape).reshape(self.kernel_size[0], self.kernel_size[1], -1, self.out_channels)\n        dw = dw.transpose(2, 0, 1, 3)   # => [c, fh, fw, oc]\n        grads = {""w"": dw}\n        if self.use_bias:  # tied biases\n            grads[""b""] = np.sum(dz, axis=(0, 1, 2), keepdims=True)\n\n        # dx\n        padded_dx = np.zeros_like(self._padded)  # [n, h, w, c]\n        s0, s1, k0, k1 = self.strides + self.kernel_size\n        t_flt = self.w.transpose((3, 1, 2, 0))  # [c, fh, hw, out] => [out, fh, fw, c]\n        for i in range(dz.shape[1]):\n            for j in range(dz.shape[2]):\n                padded_dx[:, i * s0:i * s0 + k0, j * s1:j * s1 + k1, :] += dz[:, i, j, :].reshape(\n                    (-1, self.out_channels)).dot(\n                    t_flt.reshape((self.out_channels, -1))\n                ).reshape((-1, k0, k1, padded_dx.shape[-1]))\n        t, b, l, r = self._p_tblr[0], padded_dx.shape[1] - self._p_tblr[1], self._p_tblr[2], padded_dx.shape[2] - \\\n                     self._p_tblr[3]\n        self.data_vars[""in""].set_error(padded_dx[:, t:b, l:r, :])      # pass the error to the layer before\n        return grads\n\n\nclass Pool_(BaseLayer):\n    def __init__(self,\n                 kernal_size=(3, 3),\n                 strides=(1, 1),\n                 padding=""valid"",\n                 channels_last=True,\n                 ):\n        super().__init__()\n        self.kernel_size = get_tuple(kernal_size)\n        self.strides = get_tuple(strides)\n        self.padding = padding.lower()\n        assert padding in (""valid"", ""same""), ValueError\n        self.channels_last = channels_last\n        self._padded = None\n        self._p_tblr = None\n\n    def forward(self, x):\n        self._x = self._process_input(x)\n        if not self.channels_last:  # ""channels_first"":\n            # [batch, channel, height, width] => [batch, height, width, channel]\n            self._x = np.transpose(self._x, (0, 2, 3, 1))\n        self._padded, out, self._p_tblr = get_padded_and_tmp_out(\n            self._x, self.kernel_size, self.strides, self._x.shape[-1], self.padding)\n        s0, s1, k0, k1 = self.strides + self.kernel_size\n        for i in range(0, out.shape[1]):  # in each row of the convoluted feature map\n            for j in range(0, out.shape[2]):  # in each column of the convoluted feature map\n                window = self._padded[:, i*s0:i*s0+k0, j*s1:j*s1+k1, :]  # [n,h,w,c]\n                out[:, i, j, :] = self.agg_func(window)\n        wrapped_out = self._wrap_out(out if self.channels_last else out.transpose((0, 3, 1, 2)))\n        return wrapped_out\n\n    def backward(self):\n        raise NotImplementedError\n\n    @staticmethod\n    def agg_func(x):\n        raise NotImplementedError\n\n\nclass MaxPool2D(Pool_):\n    def __init__(self,\n                 pool_size=(3, 3),\n                 strides=(1, 1),\n                 padding=""valid"",\n                 channels_last=True,\n                 ):\n        super().__init__(\n            kernal_size=pool_size,\n            strides=strides,\n            padding=padding,\n            channels_last=channels_last,)\n\n    @staticmethod\n    def agg_func(x):\n        return x.max(axis=(1, 2))\n\n    def backward(self):\n        dz = self.data_vars[""out""].error\n        grad = None\n        s0, s1, k0, k1 = self.strides + self.kernel_size\n        padded_dx = np.zeros_like(self._padded)  # [n, h, w, c]\n        for i in range(dz.shape[1]):\n            for j in range(dz.shape[2]):\n                window = self._padded[:, i*s0:i*s0+k0, j*s1:j*s1+k1, :]  # [n,fh,fw,c]\n                window_mask = window == np.max(window, axis=(1, 2), keepdims=True)\n                window_dz = dz[:, i:i+1, j:j+1, :] * window_mask.astype(np.float32)\n                padded_dx[:, i*s0:i*s0+k0, j*s1:j*s1+k1, :] += window_dz\n        t, b, l, r = [self._p_tblr[0], padded_dx.shape[1]-self._p_tblr[1],\n                      self._p_tblr[2], padded_dx.shape[2]-self._p_tblr[3]]\n        self.data_vars[""in""].set_error(padded_dx[:, t:b, l:r, :])      # pass the error to the layer before\n        return grad\n\n\nclass AvgPool2D(Pool_):\n    def __init__(self,\n                 kernal_size=(3, 3),\n                 strides=(1, 1),\n                 padding=""valid"",\n                 channels_last=True,\n                 ):\n        super().__init__(\n            kernal_size=kernal_size,\n            strides=strides,\n            padding=padding,\n            channels_last=channels_last,)\n\n    @staticmethod\n    def agg_func(x):\n        return x.mean(axis=(1, 2))\n\n    def backward(self):\n        dz = self.data_vars[""out""].error\n        grad = None\n        s0, s1, k0, k1 = self.strides + self.kernel_size\n        padded_dx = np.zeros_like(self._padded)  # [n, h, w, c]\n        for i in range(dz.shape[1]):\n            for j in range(dz.shape[2]):\n                window_dz = dz[:, i:i + 1, j:j + 1, :] * np.full(\n                    (1, k0, k1, dz.shape[-1]), 1./(k0*k1), dtype=np.float32)\n                padded_dx[:, i * s0:i * s0 + k0, j * s1:j * s1 + k1, :] += window_dz\n        t, b, l, r = [self._p_tblr[0], padded_dx.shape[1] - self._p_tblr[1],\n                      self._p_tblr[2], padded_dx.shape[2] - self._p_tblr[3]]\n        self.data_vars[""in""].set_error(padded_dx[:, t:b, l:r, :])  # pass the error to the layer before\n        return grad\n\n\nclass Flatten(BaseLayer):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        self._x = self._process_input(x)\n        out = self._x.reshape((self._x.shape[0], -1))\n        wrapped_out = self._wrap_out(out)\n        return wrapped_out\n\n    def backward(self):\n        dz = self.data_vars[""out""].error\n        grad = None\n        self.data_vars[""in""].set_error(dz.reshape(self._x.shape))\n        return grad\n\n\ndef get_tuple(inputs):\n    if isinstance(inputs, (tuple, list)):\n        out = tuple(inputs)\n    elif isinstance(inputs, int):\n        out = (inputs, inputs)\n    else:\n        raise TypeError\n    return out\n\n\ndef get_padded_and_tmp_out(img, kernel_size, strides, out_channels, padding):\n    # according to: http://machinelearninguru.com/computer_vision/basics/convolution/convolution_layer.html\n    batch, h, w = img.shape[:3]\n    (fh, fw), (sh, sw) = kernel_size, strides\n\n    if padding == ""same"":\n        out_h = int(np.ceil(h / sh))\n        out_w = int(np.ceil(w / sw))\n        ph = int(np.max([0, (out_h - 1) * sh + fh - h]))\n        pw = int(np.max([0, (out_w - 1) * sw + fw - w]))\n        pt, pl = int(np.floor(ph / 2)), int(np.floor(pw / 2))\n        pb, pr = ph - pt, pw - pl\n    elif padding == ""valid"":\n        out_h = int(np.ceil((h - fh + 1) / sh))\n        out_w = int(np.ceil((w - fw + 1) / sw))\n        pt, pb, pl, pr = 0, 0, 0, 0\n    else:\n        raise ValueError\n    padded_img = np.pad(img, ((0, 0), (pt, pb), (pl, pr), (0, 0)), \'constant\', constant_values=0.).astype(np.float32)\n    tmp_out = np.zeros((batch, out_h, out_w, out_channels), dtype=np.float32)\n    return padded_img, tmp_out, (pt, pb, pl, pr)\n\n\n'"
neuralnets/losses.py,22,"b'import numpy as np\nfrom .activations import softmax\n\n\nclass Loss:\n    def __init__(self, loss, delta):\n        self.data = loss\n        self.delta = delta\n\n    def __repr__(self):\n        return str(self.data)\n\n\nclass LossFunction:\n    def __init__(self):\n        self._pred = None\n        self._target = None\n\n    def apply(self, prediction, target):\n        raise NotImplementedError\n\n    @property\n    def delta(self):\n        raise NotImplementedError\n\n    def _store_pred_target(self, prediction, target):\n        p = prediction.data\n        p = p if p.dtype is np.float32 else p.astype(np.float32)\n        self._pred = p\n        self._target = target\n\n    def __call__(self, prediction, target):\n        return self.apply(prediction, target)\n\n\nclass MSE(LossFunction):\n    def __init__(self):\n        super().__init__()\n\n    def apply(self, prediction, target):\n        t = target if target.dtype is np.float32 else target.astype(np.float32)\n        self._store_pred_target(prediction, t)\n        loss = np.mean(np.square(self._pred - t))/2\n        return Loss(loss, self.delta)\n\n    @property\n    def delta(self):\n        t = self._target if self._target.dtype is np.float32 else self._target.astype(np.float32)\n        return self._pred - t\n\n\nclass CrossEntropy(LossFunction):\n    def __init__(self):\n        super().__init__()\n        self._eps = 1e-6\n\n    def apply(self, prediction, target):\n        raise NotImplementedError\n\n    @property\n    def delta(self):\n        raise NotImplementedError\n\n\nclass SoftMaxCrossEntropy(CrossEntropy):\n    def __init__(self):\n        super().__init__()\n\n    def apply(self, prediction, target):\n        t = target if target.dtype is np.float32 else target.astype(np.float32)\n        self._store_pred_target(prediction, t)\n        loss = - np.mean(np.sum(t * np.log(self._pred), axis=-1))\n        return Loss(loss, self.delta)\n\n    @property\n    def delta(self):\n        # according to: https://deepnotes.io/softmax-crossentropy\n        onehot_mask = self._target.astype(np.bool)\n        grad = self._pred.copy()\n        grad[onehot_mask] -= 1.\n        return grad / len(grad)\n\n\nclass SoftMaxCrossEntropyWithLogits(CrossEntropy):\n    def __init__(self):\n        super().__init__()\n\n    def apply(self, prediction, target):\n        t = target if target.dtype is np.float32 else target.astype(np.float32)\n        self._store_pred_target(prediction, t)\n        sm = softmax(self._pred)\n        loss = - np.mean(np.sum(t * np.log(sm), axis=-1))\n        return Loss(loss, self.delta)\n\n    @property\n    def delta(self):\n        grad = softmax(self._pred)\n        onehot_mask = self._target.astype(np.bool)\n        grad[onehot_mask] -= 1.\n        return grad / len(grad)\n\n\nclass SparseSoftMaxCrossEntropy(CrossEntropy):\n    def __init__(self):\n        super().__init__()\n\n    def apply(self, prediction, target):\n        target = target.astype(np.int32) if target.dtype is not np.int32 else target\n        self._store_pred_target(prediction, target)\n        sm = self._pred\n        log_likelihood = np.log(sm[np.arange(sm.shape[0]), target.ravel()] + self._eps)\n        loss = - np.mean(log_likelihood)\n        return Loss(loss, self.delta)\n\n    @property\n    def delta(self):\n        grad = self._pred.copy()\n        grad[np.arange(grad.shape[0]), self._target.ravel()] -= 1.\n        return grad / len(grad)\n\n\nclass SparseSoftMaxCrossEntropyWithLogits(CrossEntropy):\n    def __init__(self):\n        super().__init__()\n\n    def apply(self, prediction, target):\n        target = target.astype(np.int32) if target.dtype is not np.int32 else target\n        self._store_pred_target(prediction, target)\n        sm = softmax(self._pred)\n        log_likelihood = np.log(sm[np.arange(sm.shape[0]), target.ravel()] + self._eps)\n        loss = - np.mean(log_likelihood)\n        return Loss(loss, self.delta)\n\n    @property\n    def delta(self):\n        grad = softmax(self._pred)\n        grad[np.arange(grad.shape[0]), self._target.ravel()] -= 1.\n        return grad / len(grad)\n\n\nclass SigmoidCrossEntropy(CrossEntropy):\n    def __init__(self):\n        super().__init__()\n\n    def apply(self, prediction, target):\n        t = target if target.dtype is np.float32 else target.astype(np.float32)\n        self._store_pred_target(prediction, t)\n        p = self._pred\n        loss = - np.mean(\n            t * np.log(p + self._eps) + (1. - t) * np.log(1 - p + self._eps),\n        )\n        return Loss(loss, self.delta)\n\n    @property\n    def delta(self):\n        t = self._target if self._target.dtype is np.float32 else self._target.astype(np.float32)\n        return self._pred - t'"
neuralnets/metrics.py,17,"b'import numpy as np\n\n\ndef _check_transfer(predictions, labels):\n    assert predictions.ndim == 1\n    assert labels.ndim == 1\n    return predictions.astype(np.int32), labels.astype(np.int32)\n\n\ndef accuracy(predictions, labels):\n    assert predictions.shape == labels.shape\n    p, l = predictions.astype(np.int32), labels.astype(np.int32)\n    return np.where(p == l, 1., 0.).mean()\n\n\ndef roc(logits, labels, num_thresholds):\n    # Receiver Operating Characteristic curve\n    assert logits.ndim == 1\n    assert labels.ndim == 1\n    if labels.dtype != np.int32:\n        labels = labels.astype(np.int32)\n    zeros, ones = np.zeros_like(logits, dtype=np.int32), np.ones_like(logits, dtype=np.int32)\n    roc_data = np.empty((num_thresholds, 2), dtype=np.float32)\n    for i, threshold in enumerate(np.linspace(0, 1, num=num_thresholds, endpoint=True, dtype=np.float32)):\n        if threshold == 0:\n            roc_data[i, :] = [0, 0]\n            continue\n        if threshold == 1:\n            roc_data[i, :] = [1, 1]\n            break\n        p = np.where(logits < threshold, zeros, ones)\n        tpr = true_pos_rate(p, labels)\n        fpr = false_pos_rate(p, labels)\n        roc_data[i, :] = [fpr, tpr]\n    roc_data = np.sort(roc_data, axis=0)\n    return roc_data\n\n\ndef auc(logits, labels, num_thresholds=200):\n    # Area Under the Curve: for binary classifier who outputs only [0,1]\n    roc_data = roc(logits, labels, num_thresholds)\n    diff = np.diff(roc_data, axis=0)\n    _auc = (diff[:, 0] * diff[:, 1] / 2 + roc_data[:-1, 1] * diff[:, 0]).sum()  # areas of triangles + rectangle\n    return _auc\n\n\ndef true_pos(predictions, labels):\n    return np.count_nonzero((predictions == 1) & (labels == 1))\n\n\ndef true_neg(predictions, labels):\n    return np.count_nonzero((predictions == 0) & (labels == 0))\n\n\ndef false_pos(predictions, labels):\n    return np.count_nonzero((predictions == 1) & (labels == 0))\n\n\ndef false_neg(predictions, labels):\n    return np.count_nonzero((predictions == 0) & (labels == 1))\n\n\ndef true_pos_rate(predictions, labels):\n    # TP / (FP + FN) = TP / P\n    p, l = predictions, labels\n    return true_pos(p, l) / np.count_nonzero(labels)\n\n\ndef false_pos_rate(predictions, labels):\n    # FP / (FP + TN) = FP / N\n    p, l = predictions, labels\n    return false_neg(p, l) / (len(labels) - np.count_nonzero(labels))\n\n'"
neuralnets/module.py,1,"b'import numpy as np\nimport neuralnets as nn\n\n\nclass Module(object):\n    def __init__(self):\n        self._ordered_layers = []\n        self.params = {}\n\n    def forward(self, *inputs):\n        raise NotImplementedError\n\n    def backward(self, loss):\n        assert isinstance(loss, nn.losses.Loss)\n        # find net order\n        layers = []\n        for name, v in self.__dict__.items():\n            if not isinstance(v, nn.layers.BaseLayer):\n                continue\n            layer = v\n            layer.name = name\n            layers.append((layer.order, layer))\n        self._ordered_layers = [l[1] for l in sorted(layers, key=lambda x: x[0])]\n\n        # back propagate through this order\n        last_layer = self._ordered_layers[-1]\n        last_layer.data_vars[""out""].set_error(loss.delta)\n        for layer in self._ordered_layers[::-1]:\n            grads = layer.backward()\n            if isinstance(layer, nn.layers.ParamLayer):\n                for k in layer.param_vars.keys():\n                    self.params[layer.name][""grads""][k][:] = grads[k]\n\n    def save(self, path):\n        saver = nn.Saver()\n        saver.save(self, path)\n\n    def restore(self, path):\n        saver = nn.Saver()\n        saver.restore(self, path)\n\n    def sequential(self, *layers):\n        assert isinstance(layers, (list, tuple))\n        for i, l in enumerate(layers):\n            self.__setattr__(""layer_%i"" % i, l)\n        return SeqLayers(layers)\n\n    def __call__(self, *args):\n        return self.forward(*args)\n\n    def __setattr__(self, key, value):\n        if isinstance(value, nn.layers.ParamLayer):\n            layer = value\n            self.params[key] = {\n                ""vars"": layer.param_vars,\n                ""grads"": {k: np.empty_like(layer.param_vars[k]) for k in layer.param_vars.keys()}\n            }\n        object.__setattr__(self, key, value)\n\n\nclass SeqLayers:\n    def __init__(self, layers):\n        assert isinstance(layers, (list, tuple))\n        for l in layers:\n            assert isinstance(l, nn.layers.BaseLayer)\n        self.layers = layers\n\n    def forward(self, x):\n        for l in self.layers:\n            x = l.forward(x)\n        return x\n\n    def __call__(self, x):\n        return self.forward(x)'"
neuralnets/optimizers.py,19,"b'import numpy as np\n\n\nclass Optimizer:\n    def __init__(self, params, lr):\n        self._params = params\n        self._lr = lr\n        self.vars = []\n        self.grads = []\n        for layer_p in self._params.values():\n            for p_name in layer_p[""vars""].keys():\n                self.vars.append(layer_p[""vars""][p_name])\n                self.grads.append(layer_p[""grads""][p_name])\n\n    def step(self):\n        raise NotImplementedError\n\n\nclass SGD(Optimizer):\n    def __init__(self, params, lr):\n        super().__init__(params=params, lr=lr)\n\n    def step(self):\n        for var, grad in zip(self.vars, self.grads):\n            var -= self._lr * grad\n\n\nclass Momentum(Optimizer):\n    def __init__(self, params, lr=0.001, momentum=0.9):\n        super().__init__(params=params, lr=lr)\n        self._momentum = momentum\n        self._mv = [np.zeros_like(v) for v in self.vars]\n\n    def step(self):\n        for var, grad, mv in zip(self.vars, self.grads, self._mv):\n            dv = self._lr * grad\n            mv[:] = self._momentum * mv + dv\n            var -= mv\n\n\nclass AdaGrad(Optimizer):\n    def __init__(self, params, lr=0.01, eps=1e-06):\n        super().__init__(params=params, lr=lr)\n        self._eps = eps\n        self._v = [np.zeros_like(v) for v in self.vars]\n\n    def step(self):\n        for var, grad, v in zip(self.vars, self.grads, self._v):\n            v += np.square(grad)\n            var -= self._lr * grad / np.sqrt(v + self._eps)\n\n\nclass Adadelta(Optimizer):\n    def __init__(self, params, lr=1., rho=0.9, eps=1e-06):\n        super().__init__(params=params, lr=lr)\n        self._rho = rho\n        self._eps = eps\n        self._m = [np.zeros_like(v) for v in self.vars]\n        self._v = [np.zeros_like(v) for v in self.vars]\n\n    def step(self):\n        # according to: https://pytorch.org/docs/stable/_modules/torch/optim/adadelta.html#Adadelta\n        for var, grad, m, v in zip(self.vars, self.grads, self._m, self._v):\n            v[:] = self._rho * v + (1. - self._rho) * np.square(grad)\n            delta = np.sqrt(m + self._eps) / np.sqrt(v + self._eps) * grad\n            var -= self._lr * delta\n            m[:] = self._rho * m + (1. - self._rho) * np.square(delta)\n\n\nclass RMSProp(Optimizer):\n    def __init__(self, params, lr=0.01, alpha=0.99, eps=1e-08):\n        super().__init__(params=params, lr=lr)\n        self._alpha = alpha\n        self._eps = eps\n        self._v = [np.zeros_like(v) for v in self.vars]\n\n    def step(self):\n        for var, grad, v in zip(self.vars, self.grads, self._v):\n            v[:] = self._alpha * v + (1. - self._alpha) * np.square(grad)\n            var -= self._lr * grad / np.sqrt(v + self._eps)\n\n\nclass Adam(Optimizer):\n    def __init__(self, params, lr=0.01, betas=(0.9, 0.999), eps=1e-08):\n        super().__init__(params=params, lr=lr)\n        self._betas = betas\n        self._eps = eps\n        self._m = [np.zeros_like(v) for v in self.vars]\n        self._v = [np.zeros_like(v) for v in self.vars]\n\n    def step(self):\n        b1, b2 = self._betas\n        b1_crt, b2_crt = b1, b2\n        for var, grad, m, v in zip(self.vars, self.grads, self._m, self._v):\n            m[:] = b1 * m + (1. - b1) * grad\n            v[:] = b2 * v + (1. - b2) * np.square(grad)\n            b1_crt, b2_crt = b1_crt * b1, b2_crt * b2   # bias correction\n            m_crt = m / (1. - b1_crt)\n            v_crt = v / (1. - b2_crt)\n            var -= self._lr * m_crt / np.sqrt(v_crt + self._eps)\n\n\nclass AdaMax(Optimizer):\n    def __init__(self, params, lr=0.01, betas=(0.9, 0.999), eps=1e-08):\n        super().__init__(params=params, lr=lr)\n        self._betas = betas\n        self._eps = eps\n        self._m = [np.zeros_like(v) for v in self.vars]\n        self._v = [np.zeros_like(v) for v in self.vars]\n\n    def step(self):\n        b1, b2 = self._betas\n        b1_crt = b1\n        for var, grad, m, v in zip(self.vars, self.grads, self._m, self._v):\n            m[:] = b1 * m + (1. - b1) * grad\n            v[:] = np.maximum(b2 * v, np.abs(grad))\n            b1_crt = b1_crt * b1  # bias correction\n            m_crt = m / (1. - b1_crt)\n            var -= self._lr * m_crt / (v + self._eps)'"
neuralnets/saver.py,0,"b'import pickle\nimport neuralnets as nn\n\n\nclass Saver:\n    @staticmethod\n    def save(model, path):\n        assert isinstance(model, nn.Module)\n        vars = {name: p[""vars""] for name, p in model.params.items()}\n        with open(path, ""wb"") as f:\n            pickle.dump(vars, f)\n\n    @staticmethod\n    def restore(model, path):\n        assert isinstance(model, nn.Module)\n        with open(path, ""rb"") as f:\n            params = pickle.load(f)\n        for name, param in params.items():\n            for p_name in model.params[name][""vars""].keys():\n                model.params[name][""vars""][p_name][:] = param[p_name]\n                model.params[name][""vars""][p_name][:] = param[p_name]\n'"
neuralnets/variable.py,1,"b'import numpy as np\n\n\nclass Variable:\n    def __init__(self, v):\n        self.data = v\n        self._error = np.empty_like(v)   # for backpropagation of the last layer\n        self.info = {}\n\n    def __repr__(self):\n        return str(self.data)\n\n    def set_error(self, error):\n        assert self._error.shape == error.shape\n        self._error[:] = error\n\n    @property\n    def error(self):\n        return self._error\n\n    @property\n    def shape(self):\n        return self.data.shape\n\n    @property\n    def ndim(self):\n        return self.data.ndim'"
