file_path,api_count,code
neural_net/learn.py,0,"b""from lib.optimizers.adam       import Adam\nfrom lib.data.data             import Data\nfrom lib.hyperparameter_search import HyperparameterSearch\nfrom lib.model                 import Model\n\ndata = Data().load()\n\nfor hyperparameters in HyperparameterSearch().hyperparameters():\n    Model(data,\n          Adam,\n          hyperparameters['learning_rate'],\n          hyperparameters['regularization_strength'],\n          hyperparameters['number_hidden_layers'],\n          hyperparameters['min_number_hidden_nodes']).train()\n"""
lambda/predict/predict.py,11,"b'import sys\nimport numpy as np\nimport boto3\nimport json\n\ns3           = boto3.client(\'s3\')\nweights_file = s3.download_file(\'smile-o-meter.rocks\', \'learned_weights.npy\', \'/tmp/learned_weights.npy\')\nbiases_file  = s3.download_file(\'smile-o-meter.rocks\', \'learned_biases.npy\',  \'/tmp/learned_biases.npy\')\nweights      = np.load(\'/tmp/learned_weights.npy\')\nbiases       = np.load(\'/tmp/learned_biases.npy\')\n\ntraining_set_means_file                    = s3.download_file(\'smile-o-meter.rocks\', \'training_set_feature_means.npy\', \'/tmp/training_set_feature_means.npy\')\nstandard_deviations_file                   = s3.download_file(\'smile-o-meter.rocks\', \'training_set_zero_mean_feature_standard_deviations.npy\',  \'/tmp/training_set_zero_mean_feature_standard_deviations.npy\')\ntraining_set_means                         = np.load(\'/tmp/training_set_feature_means.npy\')\nzero_mean_training_set_standard_deviations = np.load(\'/tmp/training_set_zero_mean_feature_standard_deviations.npy\')\n\ndef relu(Z):\n    return np.maximum(Z, 0)\n\ndef softmax(v):\n    exponentials = np.exp(v - np.max(v, axis = 0))\n    return exponentials / np.sum(exponentials)\n\ndef softmax_activation(Z):\n    return np.atleast_2d(np.apply_along_axis(softmax, 0, Z))\n\ndef forward_prop(A):\n    for i in range(len(weights)):\n        Z = np.dot(weights[i], A) + biases[i]\n        A = relu(Z)\n\n    return softmax_activation(Z)\n\n# Normalizes input data based on the zero-mean one-standard-deviation\n# transformation that was applied to the training set\ndef normalize(matrix):\n    return (matrix - training_set_means) / zero_mean_training_set_standard_deviations\n\ndef predict(event, context):\n    headers = {\n        ""Content-Type"": ""application/json"",\n        ""Access-Control-Allow-Origin"": ""*""\n    }\n\n    try:\n        pixels             = json.loads(event[\'body\'])[\'pixels\']\n        array              = np.array([pixels]).T\n        normalized_array   = normalize(array)\n        softmax_activation = forward_prop(normalized_array)\n        prediction         = np.argmax(softmax_activation)\n\n        return {\n            ""statusCode"": 200,\n            ""body"": str(prediction),\n            ""headers"": headers\n        }\n    except:\n        return {\n            ""statusCode"": 500,\n            ""body"": str(sys.exc_info()),\n            ""headers"": headers\n        }\n'"
neural_net/lib/backward_prop.py,3,"b'import numpy as np\n\nclass BackwardProp:\n    def __init__(self, weights, Z, A, labels, regularization_strength):\n        self.weights                 = weights\n        self.labels                  = labels\n        self.num_examples            = labels.shape[1]\n        self.regularization_strength = regularization_strength\n        self.Z                       = Z # Z = W_current_layer * A_previous_layer + B_current_layer\n        self.A                       = A # A = activation(Z) where activation is ReLU for hidden layers and Softmax for the output layer\n        self.softmax_output          = A[-1]\n\n    def run(self):\n        # These should have a derivative for each weight and bias\n        weight_gradients = []\n        bias_gradients   = []\n\n        for i in range(1, len(self.A)):\n            if i == 1:\n                dz = self.d_cost_d_z()\n            else:\n                dz = self.d_z_d_a(-i).T.dot(dz) * self.d_relu_d_z(-i)\n\n            dw = dz.dot(self.d_z_d_w(-i).T) + self.d_regularization(-i)\n            db = np.sum(dz, axis = 1, keepdims = True) * self.d_z_d_b()\n\n            weight_gradients.append(dw)\n            bias_gradients.append(db)\n\n        return [np.array(list(reversed(weight_gradients))), np.array(list(reversed(bias_gradients)))]\n\n    def d_cost_d_z(self):\n        return (self.softmax_output - self.labels) / self.num_examples\n\n    def d_relu_d_z(self, layer):\n        return np.where(self.Z[layer] > 0, 1, 0)\n\n    def d_z_d_w(self, layer):\n        return self.A[layer - 1]\n\n    def d_z_d_a(self, layer):\n        return self.weights[layer + 1]\n\n    def d_z_d_b(self):\n        return 1.0\n\n    def d_regularization(self, layer):\n        return self.regularization_strength * self.weights[layer]\n'"
neural_net/lib/cost.py,3,"b'import numpy as np\n\nclass Cost:\n    def __init__(self, predictions, labels, weights, regularization_strength, numeric_stabilizer = 0.000001):\n        self.predictions             = predictions\n        self.labels                  = labels\n        self.num_examples            = float(labels.shape[1])\n        self.weights                 = weights\n        self.regularization_strength = regularization_strength\n        self.numeric_stabilizer      = numeric_stabilizer\n\n    def cross_entropy_loss(self):\n        inverse_log         = -np.log(self.predictions + self.numeric_stabilizer) * self.labels\n        average_inverse_log = np.sum(inverse_log) / self.num_examples\n\n        return average_inverse_log + self.l2_regularization_loss()\n\n    def l2_regularization_loss(self):\n        squared_weights = np.square(self.weights)\n        weight_sum      = 0\n\n        for layer in range(len(squared_weights)):\n            weight_sum += squared_weights[layer].sum()\n\n        return 0.5 * self.regularization_strength * weight_sum\n'"
neural_net/lib/forward_prop.py,8,"b'import numpy as np\n\nclass ForwardProp:\n    def __init__(self, weights, biases, examples):\n        self.weights               = weights\n        self.biases                = biases\n        self.examples              = examples\n        self.linear_activations    = []\n        self.nonlinear_activations = [examples]\n\n    def run(self):\n        for i in range(len(self.weights)):\n            linear_activation    = np.dot(self.weights[i], self.nonlinear_activations[i])  + self.biases[i]\n            nonlinear_activation = self.relu(linear_activation)\n\n            self.linear_activations.append(linear_activation)\n            self.nonlinear_activations.append(nonlinear_activation)\n\n        self.network_output            = self.softmax_activation(linear_activation)\n        self.nonlinear_activations[-1] = self.network_output\n\n        return [self.linear_activations, self.nonlinear_activations]\n\n    def relu(self, linear_activation):\n        # NOTE: np.maximum is NOT the same as np.max\n        # np.max finds the maximum value in the matrix (or in each row/column\n        # of the matrix)\n        #\n        # np.maximum maps the given array element-wise, returning the max of\n        # each element and the provided second arg (0 in this case)\n        return np.maximum(linear_activation, 0)\n\n    def softmax(self, vector):\n        exponentials = np.exp(vector - np.max(vector, axis = 0))\n        return exponentials / np.sum(exponentials)\n\n    def softmax_activation(self, linear_activation):\n        return np.atleast_2d(np.apply_along_axis(self.softmax, 0, linear_activation))\n'"
neural_net/lib/hyperparameter_search.py,1,"b""import numpy as np\n\nlearning_rates              = [1e-01, 1e-02, 1e-03, 1e-04]\nregularization_strengths    = [1e-02, 1e-03, 1e-04, 1e-05, 1e-6]\nnumber_hidden_layers        = [1, 2, 3]\nmin_number_hidden_nodes     = [50, 100, 200]\nhyperparameter_combinations = []\n\nclass HyperparameterSearch:\n    def hyperparameters(self):\n        for learning_rate in learning_rates:\n            for regularization_strength in regularization_strengths:\n                for num_layers in number_hidden_layers:\n                    for num_nodes in min_number_hidden_nodes:\n                        hyperparameter_combinations.append({\n                            'learning_rate':           learning_rate,\n                            'regularization_strength': regularization_strength,\n                            'number_hidden_layers':    num_layers,\n                            'min_number_hidden_nodes': num_nodes\n                        })\n\n        np.random.shuffle(hyperparameter_combinations)\n\n        return hyperparameter_combinations\n"""
neural_net/lib/initialize.py,3,"b""import numpy as np\n\nclass Initialize:\n    xavier_numerator = 1\n    he_numerator     = 2\n\n    def __init__(self, network_shape, algorithm = 'xavier'):\n        self.network_shape = network_shape\n        self.algorithm     = algorithm\n\n    def weights_and_biases(self):\n        weights = []\n        biases  = []\n\n        for i in range(0, len(self.network_shape) - 1):\n            num_inputs  = self.network_shape[i]\n            num_outputs = self.network_shape[i + 1]\n\n            weights.append(self.initialize_weights(num_inputs, num_outputs))\n            biases.append(self.initialize_biases(num_outputs))\n\n        return [np.array(weights), np.array(biases)]\n\n    def initialize_weights(self, num_inputs, num_outputs):\n        if self.algorithm == 'xavier':\n            numerator = self.xavier_numerator\n        else:\n            numerator = self.he_numerator\n\n        return np.random.randn(num_outputs, num_inputs) * np.sqrt(numerator / num_inputs)\n\n    def initialize_biases(self, num_outputs):\n       return np.zeros((num_outputs, 1))\n"""
neural_net/lib/model.py,2,"b'import numpy as np\nimport os\n\nfrom lib.utilities.confusion_matrix import ConfusionMatrix\nfrom lib.utilities.graph            import Graph\nfrom lib.initialize                 import Initialize\nfrom lib.optimize                   import Optimize\nfrom lib.predict                    import Predict\nfrom lib.utilities.timer            import Timer\n\nclass Model:\n    def __init__(self, data, optimization_algorithm, learning_rate, regularization_strength, num_layers, min_number_hidden_nodes):\n        self.data                    = data\n        self.optimization_algorithm  = optimization_algorithm\n        self.learning_rate           = learning_rate\n        self.regularization_strength = regularization_strength\n        self.num_hidden_layers       = num_layers\n        self.min_number_hidden_nodes = min_number_hidden_nodes\n        self.timer                   = Timer()\n        self.hyperparameters         = {\n            \'Algorithm\':               optimization_algorithm.__name__,\n            \'Learning Rate\':           learning_rate,\n            \'Regularization Strength\': regularization_strength,\n            \'Num Hidden Layers\':       num_layers,\n            \'Min Number Hidden Nodes\': min_number_hidden_nodes\n        }\n\n    def train(self):\n        if self.hash() in "" "".join(os.listdir(""./output"")):\n            print(f""Model {self.hash()} has already been trained. You can find its learned parameters and data about its effectiveness in {self.dirname()}."")\n            return\n\n        print(f""Training model {self.hash()}"")\n\n        weights, biases = self.initialize_parameters()\n        algorithm       = self.optimization_algorithm(self.learning_rate, weights, biases)\n        optimizer       = Optimize(self.data.training_examples,\n                                   self.data.training_labels,\n                                   algorithm,\n                                   self.regularization_strength)\n\n        training_results = self.timer.time(optimizer.run)\n\n        self.training_costs  = training_results[\'costs\']\n        self.learned_weights = training_results[\'weights\']\n        self.learned_biases  = training_results[\'biases\']\n\n        self.validate()\n        self.save_output()\n\n    def initialize_parameters(self):\n        return Initialize(self.network_architecture()).weights_and_biases()\n\n    def network_architecture(self):\n        num_features = self.data.training_examples.shape[0]\n        num_classes  = self.data.training_labels.shape[0]\n        architecture = [num_features]\n\n        for hidden_layer_index in range(self.num_hidden_layers):\n            architecture.append(self.min_number_hidden_nodes * (self.num_hidden_layers - hidden_layer_index))\n\n        architecture.append(num_classes)\n\n        return architecture\n\n    def validate(self):\n        predictor = Predict(self.data.validation_examples,\n                            self.data.validation_labels,\n                            self.learned_weights,\n                            self.learned_biases)\n\n        validation_results     = predictor.run()\n        self.accuracy          = round(validation_results[\'accuracy\'], 4)\n        self.cost              = round(validation_results[\'cost\'], 4)\n        self.actual_classes    = predictor.actual_classes\n        self.predicted_classes = predictor.predicted_classes()\n\n    def save_output(self):\n        os.mkdir(self.dirname())\n\n        np.save(self.dirname() + ""/weights"", self.learned_weights)\n        np.save(self.dirname() + ""/biases"",  self.learned_biases)\n\n        with open(self.dirname() + ""/results.txt"", ""a"") as results_file:\n            results_file.write(self.results_string())\n            results_file.close()\n\n        self.graph_costs()\n\n        print(f""Results saved to {self.dirname()}\\n"")\n\n    def results_string(self):\n        string = f""Network architecture:\\n{self.network_architecture()}\\n""\n        string = string + ""Hyperparameters:\\n""\n\n        for key, value in self.hyperparameters.items():\n            string = string + f""{key} => {value}\\n""\n\n        string = string + f""\\nTotal training time => {self.timer.string()}\\n""\n        string = string + f""\\nAverage validation cost => {self.cost}\\n""\n        string = string + f""Validation accuracy => {self.accuracy}\\n""\n        string = string + f""\\nConfusion Matrix:\\n{self.confusion_matrix()}\\n""\n\n        return string\n\n    def dirname(self):\n        return (""./output/accuracy-{0}-{1}"").format(self.accuracy, self.hash())\n\n    def hash(self):\n        hyperparameter_string = \'-\'.join([f""{key}-{value}"" for key, value in self.hyperparameters.items()])\n        return str(hash(hyperparameter_string))\n\n    def confusion_matrix(self):\n        return ConfusionMatrix(self.actual_classes, self.predicted_classes).string()\n\n    def graph_costs(self):\n        graph = Graph(ylabel = ""Cost"", xlabel = ""Iteration"", data = self.training_costs)\n        graph.save(f""{self.dirname()}/costs.png"")\n'"
neural_net/lib/optimize.py,4,"b'import numpy as np\n\nfrom lib.forward_prop    import ForwardProp\nfrom lib.cost            import Cost\nfrom lib.backward_prop   import BackwardProp\n\nclass Optimize:\n    def __init__(self, examples, labels, optimizer, regularization_strength, batch_size = 256, logging_enabled = True):\n        self.examples                 = examples\n        self.labels                   = labels\n        self.optimizer                = optimizer\n        self.weights                  = optimizer.weights\n        self.biases                   = optimizer.biases\n        self.regularization_strength  = regularization_strength\n        self.batch_size               = batch_size\n        self.costs                    = []\n        self.num_learning_rate_decays = 0\n        self.logging_enabled          = logging_enabled\n\n    def run(self):\n        epoch = 0\n\n        while True:\n            epoch = epoch + 1\n\n            for batch_number in range(self.num_batches()):\n                examples_batch = self.batch(batch_number, self.examples)\n                labels_batch   = self.batch(batch_number, self.labels)\n\n                self.shuffle_in_unison(examples_batch, labels_batch)\n\n                forward_prop = ForwardProp(self.weights, self.biases, examples_batch)\n                self.linear_activation, self.nonlinear_activation = forward_prop.run()\n\n                self.current_cost = self.calculate_cost(forward_prop.network_output, labels_batch)\n\n                weight_gradients, bias_gradients = self.backward_prop(labels_batch)\n\n                self.optimizer.update_parameters(weight_gradients, bias_gradients)\n                self.weights = self.optimizer.weights\n                self.biases  = self.optimizer.biases\n\n            self.costs.append(self.current_cost)\n            self.log_epoch(epoch)\n\n            if self.cost_below_threshold():\n                print(""\\nTraining complete!"")\n                break\n            if self.cost_not_decreasing(epoch):\n                if self.learning_rate_fully_decayed():\n                    print(""\\nLearning rate fully decayed"")\n                    break\n                else:\n                    print(""\\nHalving the learning rate!"")\n                    self.optimizer.learning_rate  = self.optimizer.learning_rate * 0.5\n                    self.num_learning_rate_decays = self.num_learning_rate_decays + 1\n\n        return self.learned_parameters()\n\n    def learned_parameters(self):\n        return {\n            \'costs\':   self.costs,\n            \'weights\': self.weights,\n            \'biases\':  self.biases\n        }\n\n    def num_batches(self):\n        return int(self.examples.shape[1] / self.batch_size)\n\n    def batch(self, batch_number, array):\n        start_index = batch_number * self.batch_size\n        end_index   = start_index  + self.batch_size\n        return array[:, start_index:end_index]\n\n    def cost_below_threshold(self):\n        return self.current_cost <= 0.05\n\n    def cost_not_decreasing(self, epoch):\n        return len(self.costs) > 4 and epoch % 5 == 0 and (self.costs[-1] - self.costs[-5]) < 0.001\n\n    def learning_rate_fully_decayed(self):\n        return self.num_learning_rate_decays == 4\n\n    def backward_prop(self, labels_batch):\n        return BackwardProp(self.weights,\n                            self.linear_activation,\n                            self.nonlinear_activation,\n                            labels_batch,\n                            self.regularization_strength).run()\n\n    def calculate_cost(self, network_output, labels_batch):\n        return Cost(network_output,\n                    labels_batch,\n                    self.weights,\n                    self.regularization_strength).cross_entropy_loss()\n\n    def log_epoch(self, epoch):\n        if self.logging_enabled:\n            print(f""\\nEnd of epoch {epoch} - Cost {round(self.current_cost, 4)}"")\n\n    # Perform identical in-place shuffles on the *columns* of two arrays\n    def shuffle_in_unison(self, array1, array2):\n        random_state = np.random.get_state()\n        np.random.shuffle(array1.T)\n\n        np.random.set_state(random_state)\n        np.random.shuffle(array2.T)\n'"
neural_net/lib/predict.py,1,"b""import numpy as np\n\nfrom lib.forward_prop import ForwardProp\nfrom lib.cost         import Cost\n\nclass Predict:\n    regularization_strength = 0\n\n    def __init__(self, examples, labels, weights, biases):\n        self.examples       = examples\n        self.labels         = labels\n        self.weights        = weights\n        self.biases         = biases\n        self.num_examples   = labels.shape[1]\n        self.actual_classes = labels.argmax(axis=0)\n\n    def run(self):\n        forward_prop = ForwardProp(self.weights, self.biases, self.examples)\n        forward_prop.run()\n        self.network_output = forward_prop.network_output\n\n        return { 'accuracy': self.percent_correct(), 'cost': self.cost() }\n\n    def cost(self):\n        return Cost(self.network_output,\n                    self.labels,\n                    self.weights,\n                    self.regularization_strength).cross_entropy_loss()\n\n    def num_correct(self):\n        return (self.actual_classes == self.predicted_classes()).sum()\n\n    def percent_correct(self):\n        return self.num_correct() / self.num_examples\n\n    def predicted_classes(self):\n        return self.predictions().argmax(axis=0)\n\n    def predictions(self):\n        return self.bool_to_int(self.network_output == np.max(self.network_output, axis=0))\n\n    def bool_to_int(self, array):\n        return array * 1\n\n    def prediction_class_percentages(self):\n        return self.predictions().sum(axis=1) / self.num_examples\n\n    def actual_class_percentages(self):\n        return self.labels.sum(axis=1) / self.num_examples\n\n    def percentage_diffs(self):\n        return abs(self.prediction_class_percentages() - self.actual_class_percentages())\n"""
neural_net/tests/test_backward_prop.py,8,"b""import unittest\nimport numpy as np\n\nfrom lib.forward_prop                import ForwardProp\nfrom lib.backward_prop               import BackwardProp\nfrom lib.utilities.gradient_check    import GradientCheck\nfrom lib.optimizers.gradient_descent import GradientDescent\n\nlearning_rate          = 0.01\nnum_input_features     = 28\nnum_hidden_layer_nodes = 14\nnum_classes            = 7\nnum_examples           = 100\n\nclass TestBackwardProp(unittest.TestCase):\n    def setUp(self):\n        random_values                = np.random.randn(num_classes, num_examples)\n        self.labels                  = (random_values == random_values.max(axis=0)) * 1\n        self.examples                = np.random.randn(num_input_features, num_examples)\n        self.regularization_strength = 0.001\n\n    def test_analytic_gradients_are_close_to_numeric_gradients(self):\n        weights = np.array([\n            np.random.randn(num_hidden_layer_nodes, num_input_features),\n            np.random.randn(num_classes, num_hidden_layer_nodes),\n        ])\n\n        biases = np.array([\n            np.zeros((num_hidden_layer_nodes, 1)),\n            np.zeros((num_classes, 1)),\n        ])\n\n        for i in range(0, 3):\n            print('Testing backprop, iteration', i + 1)\n\n            forward_prop = ForwardProp(weights, biases, self.examples)\n            linear_activations, nonlinear_activations = forward_prop.run()\n\n            backward_prop = BackwardProp(weights,\n                                         linear_activations,\n                                         nonlinear_activations,\n                                         self.labels,\n                                         self.regularization_strength)\n\n            weight_gradients, bias_gradients = backward_prop.run()\n\n            gradient_check = GradientCheck(weight_gradients,\n                                           weights,\n                                           biases,\n                                           self.examples,\n                                           self.labels,\n                                           self.regularization_strength)\n\n            for layer in range(len(weights)):\n                self.assertTrue(gradient_check.run(layer))\n\n            optimizer = GradientDescent(learning_rate, weights, biases)\n            optimizer.update_parameters(weight_gradients, bias_gradients)\n            weights = optimizer.weights\n            biases  = optimizer.biases\n\nif __name__ == '__main__':\n    unittest.main()\n"""
neural_net/tests/test_cost.py,8,"b""import unittest\nimport numpy as np\n\nfrom lib.cost import Cost\n\nclass TestCost(unittest.TestCase):\n    def setUp(self):\n        # A 1 in a particular row indicates the example belongs to the class\n        # with the index of that row. Eg given the labels below there are 3\n        # classes: 0, 1, and 2, and a single example that belongs to class 1\n        self.labels = np.array([\n            [0], # row/class 0\n            [1], # row/class 1 ... the 1 here indicates this is the example's class\n            [0]  # row/class 2\n        ])\n\n        self.weights = np.array([\n            [1], [1], [1],\n            [1], [1], [1],\n            [1], [1], [1]\n        ])\n\n        self.regularization_strength = 0.001\n\n    def test_cost_is_very_high_for_incorrect_predictions_with_high_confidence(self):\n        predictions = np.array([\n            [0.90], # the network predicts a 90% chance that the example is of class 0\n            [0.05],\n            [0.05]\n        ])\n        cost = Cost(predictions,\n                    self.labels,\n                    self.weights,\n                    self.regularization_strength).cross_entropy_loss()\n\n        self.assertTrue(cost > 2.5)\n\n    def test_cost_is_very_low_for_correct_predictions_with_high_confidence(self):\n        predictions = np.array([\n            [0.05],\n            [0.90], # the network predicts a 90% chance that the example is of class 1\n            [0.05]\n        ])\n        cost = Cost(predictions,\n                    self.labels,\n                    self.weights,\n                    self.regularization_strength).cross_entropy_loss()\n\n        self.assertTrue(cost < 0.2)\n\n    def test_cost_is_moderate_for_correct_predictions_with_low_confidence(self):\n        predictions = np.array([\n            [0.25],\n            [0.50], # the network predicts a 50% chance that the example is of class 1\n            [0.25]\n        ])\n        cost = Cost(predictions,\n                    self.labels,\n                    self.weights,\n                    self.regularization_strength).cross_entropy_loss()\n\n        self.assertTrue(cost < 0.7)\n        self.assertTrue(cost > 0.5)\n\n    def test_cost_is_high_for_incorrect_predictions_with_low_confidence(self):\n        predictions = np.array([\n            [0.50], # the network predicts a 50% chance that the example is of class 0\n            [0.25],\n            [0.25]\n        ])\n        cost = Cost(predictions,\n                    self.labels,\n                    self.weights,\n                    self.regularization_strength).cross_entropy_loss()\n\n        self.assertTrue(cost < 1.5)\n        self.assertTrue(cost > 1.0)\n\n    def test_cost_increases_as_magnitude_of_weights_increases(self):\n        predictions = np.array([\n            [0.50], # the network predicts a 50% chance that the example is of class 0\n            [0.25],\n            [0.25]\n        ])\n\n        weights = np.array([\n            [10], [10], [10],\n            [10], [10], [10],\n            [10], [10], [10]\n        ])\n\n        cost = Cost(predictions,\n                    self.labels,\n                    weights,\n                    self.regularization_strength).cross_entropy_loss()\n\n        self.assertTrue(cost < 2.0)\n        self.assertTrue(cost > 1.5)\n\nif __name__ == '__main__':\n    unittest.main()\n"""
neural_net/tests/test_forward_prop.py,7,"b""import unittest\nimport numpy as np\n\nfrom lib.forward_prop import ForwardProp\n\nnum_input_features     = 10\nnum_hidden_layer_nodes = 5\nnum_classes            = 3\nnum_examples           = 1000\n\nclass TestForwardProp(unittest.TestCase):\n    def setUp(self):\n        weights = np.array([\n            np.random.randn(num_hidden_layer_nodes, num_input_features),\n            np.random.randn(num_classes, num_hidden_layer_nodes),\n        ])\n\n        biases = np.array([\n            np.zeros((num_hidden_layer_nodes, 1)),\n            np.zeros((num_classes, 1)),\n        ])\n\n        examples     = np.random.randn(num_input_features, num_examples)\n        forward_prop = ForwardProp(weights, biases, examples)\n\n        self.linear_activations, self.nonlinear_activations = forward_prop.run()\n        self.output = forward_prop.network_output\n\n    def test_network_output_same_as_last_nonlinear_activation(self):\n        self.assertTrue((self.output == self.nonlinear_activations[-1]).all())\n\n    def test_network_output_shape(self):\n        self.assertEqual(self.output.shape, (num_classes, num_examples))\n\n    def test_network_output_columnwise_sum_equals_1(self):\n        self.assertTrue((self.output.sum(axis=0) > 0.99).all())\n        self.assertTrue((self.output.sum(axis=0) < 1.01).all())\n\nif __name__ == '__main__':\n    unittest.main()\n"""
neural_net/tests/test_initialize.py,0,"b""import unittest\n\nfrom lib.initialize import Initialize\n\nclass TestInitialize(unittest.TestCase):\n    def setUp(self):\n        network_architecture = [2304, 250, 150, 7]\n        self.weights, self.biases = Initialize(network_architecture).weights_and_biases()\n\n    def test_weights_shape(self):\n        self.assertEqual(len(self.weights), 3)\n        self.assertEqual(self.weights[0].shape, (250, 2304))\n        self.assertEqual(self.weights[1].shape, (150, 250))\n        self.assertEqual(self.weights[2].shape, (7, 150))\n\n    def test_weights_magnitude(self):\n        self.assertTrue(abs(self.weights[0].mean()) < abs(self.weights[1].mean()))\n        self.assertTrue(abs(self.weights[1].mean()) < abs(self.weights[2].mean()))\n        self.assertTrue(abs(self.weights[2].mean()) < 0.5)\n\n    def test_biases_shape(self):\n        self.assertEqual(len(self.biases), 3)\n        self.assertEqual(self.biases[0].shape, (250, 1))\n        self.assertEqual(self.biases[1].shape, (150, 1))\n        self.assertEqual(self.biases[2].shape, (7, 1))\n\n    def test_biases_magnitude(self):\n        self.assertTrue((self.biases[0] == 0).all())\n        self.assertTrue((self.biases[1] == 0).all())\n        self.assertTrue((self.biases[2] == 0).all())\n\nif __name__ == '__main__':\n    unittest.main()\n"""
neural_net/tests/test_optimize.py,8,"b""# import unittest\n# import numpy as np\n#\n# from lib.optimizers.adam             import Adam\n# from lib.optimizers.gradient_descent import GradientDescent\n# from lib.optimize                    import Optimize\n# from lib.data.raw                    import Raw\n#\n# learning_rate           = 0.01\n# regularization_strength = 0.0001\n# num_input_features      = 10\n# num_hidden_layer_nodes  = 5\n# num_classes             = 3\n# num_examples            = 100\n# batch_size              = 10\n#\n# # Can't pass a Numpy array to `hash`, so we first convert it to a tuple\n# def hash_array(array):\n#      return hash(tuple(array))\n#\n# class TestOptimize(unittest.TestCase):\n#     def setUp(self):\n#         self.weights = np.array([\n#             np.random.randn(num_hidden_layer_nodes, num_input_features),\n#             np.random.randn(num_classes, num_hidden_layer_nodes),\n#         ])\n#\n#         self.biases = np.array([\n#             np.zeros((num_hidden_layer_nodes, 1)),\n#             np.zeros((num_classes, 1)),\n#         ])\n#         examples = np.random.randn(num_input_features, num_examples)\n#         labels   = np.tile(np.array([\n#             [1, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n#             [0, 0, 0, 0, 1, 0, 1, 0, 0, 1],\n#             [0, 1, 1, 0, 0, 1, 0, 0, 0, 0],\n#         ]), 10)\n#\n#         gradient_descent = GradientDescent(learning_rate, self.weights, self.biases)\n#         adam             = Adam(learning_rate, self.weights, self.biases)\n#\n#         self.gradient_descent_optimizer = Optimize(examples,\n#                                                    labels,\n#                                                    gradient_descent,\n#                                                    regularization_strength,\n#                                                    batch_size      = batch_size,\n#                                                    logging_enabled = False)\n#\n#         self.adam_optimizer = Optimize(examples,\n#                                        labels,\n#                                        adam,\n#                                        regularization_strength,\n#                                        batch_size      = batch_size,\n#                                        logging_enabled = False)\n#\n#     def test_returns_learned_parameters_with_gradient_descent(self):\n#         result = self.gradient_descent_optimizer.run()\n#\n#         self.assertTrue(len(result['weights']) == len(self.weights))\n#         self.assertTrue(result['weights'][0].shape == self.weights[0].shape)\n#         self.assertTrue(result['weights'][1].shape == self.weights[1].shape)\n#         self.assertFalse((result['weights'][1] == self.weights[1]).all())\n#\n#         self.assertTrue(len(result['biases']) == len(self.biases))\n#         self.assertTrue(result['biases'][0].shape == self.biases[0].shape)\n#         self.assertTrue(result['biases'][1].shape == self.biases[1].shape)\n#         self.assertFalse((result['biases'][1] == self.biases[1]).all())\n#\n#     def test_generally_lowers_costs_with_gradient_descent(self):\n#         result = self.gradient_descent_optimizer.run()\n#         costs  = result['costs']\n#\n#         # Assert that cost decreases every 5th epoch\n#         for epoch in range(1, int(len(costs) / 5)):\n#             self.assertTrue(costs[epoch * 5] < costs[(epoch - 1) * 5])\n#\n#     def test_returns_learned_parameters_and_cost_with_adam(self):\n#         result = self.adam_optimizer.run()\n#\n#         self.assertTrue(len(result['weights']) == len(self.weights))\n#         self.assertTrue(result['weights'][0].shape == self.weights[0].shape)\n#         self.assertTrue(result['weights'][1].shape == self.weights[1].shape)\n#         self.assertFalse((result['weights'][1] == self.weights[1]).all())\n#\n#         self.assertTrue(len(result['biases']) == len(self.biases))\n#         self.assertTrue(result['biases'][0].shape == self.biases[0].shape)\n#         self.assertTrue(result['biases'][1].shape == self.biases[1].shape)\n#         self.assertFalse((result['biases'][1] == self.biases[1]).all())\n#\n#     def test_generally_lowers_costs_with_adam(self):\n#         result = self.adam_optimizer.run()\n#         costs  = result['costs']\n#\n#         # Assert that cost decreases every 5th epoch\n#         for epoch in range(1, int(len(costs) / 5)):\n#             self.assertTrue(costs[epoch * 5] < costs[(epoch - 1) * 5])\n#\n#     def test_shuffle_in_unison(self):\n#         raw_data  = Raw('./lib/data/sources/fer_subset.csv').load()\n#         labels    = raw_data.training_labels\n#         examples  = raw_data.training_examples\n#         max_index = labels.shape[1]\n#\n#         label_to_example_mappings = {}\n#\n#         for i in range(max_index):\n#             label_hash   = hash_array(labels[:,i])\n#             example_hash = hash_array(examples[:,i])\n#\n#             label_to_example_mappings.setdefault(label_hash, [])\n#             label_to_example_mappings[label_hash].append(example_hash)\n#\n#         self.adam_optimizer.shuffle_in_unison(labels, examples)\n#\n#         for i in range(max_index):\n#             label_hash   = hash_array(labels[:,i])\n#             example_hash = hash_array(examples[:,i])\n#\n#             self.assertTrue(example_hash in label_to_example_mappings[label_hash])\n#\n# if __name__ == '__main__':\n#     unittest.main()\n"""
neural_net/tests/test_predict.py,5,"b""import unittest\nimport numpy as np\n\nfrom lib.forward_prop import ForwardProp\nfrom lib.predict      import Predict\n\nclass TestPredict(unittest.TestCase):\n    def setUp(self):\n        examples = np.random.randn(5, 2)\n\n        # A 1 in a particular row indicates the example belongs to the class\n        # with the index of that row. Eg given the labels below there are 3\n        # classes: 0, 1, and 2, and two examples. The first example belongs to\n        # class 1, and the second belongs to class 2.\n        labels = np.array([\n            [0, 0], # row/class 0\n            [1, 0], # row/class 1, the first example (column) belongs to this class\n            [0, 1]  # row/class 2, the second example (column) belongs to this class\n        ])\n\n        weights = np.array([np.random.randn(3, 5)])\n        biases  = np.array([np.zeros((3, 1))])\n\n        self.mock_network_output = np.array([\n            [0.10, 0.80], # network predicts an 80% chance example 2 belongs to this class\n            [0.80, 0.10], # network predicts an 80% chance example 1 belongs to this class\n            [0.10, 0.10]\n        ])\n\n        self.predictor = Predict(examples, labels, weights, biases)\n        self.predictor.run()\n\n    def test_num_examples(self):\n        self.predictor.network_output = self.mock_network_output\n        self.assertEqual(self.predictor.num_examples, 2)\n\n    def test_num_correct(self):\n        self.predictor.network_output = self.mock_network_output\n        self.assertEqual(self.predictor.num_correct(), 1)\n\n    def test_percent_correct(self):\n        self.predictor.network_output = self.mock_network_output\n        self.assertEqual(self.predictor.percent_correct(), 0.5)\n\nif __name__ == '__main__':\n    unittest.main()\n"""
neural_net/lib/data/__init__.py,0,b''
neural_net/lib/data/augmenter.py,4,"b'import numpy as np\n\nfrom imgaug import augmenters\n\nclass Augmenter:\n    def __init__(self, images, labels):\n        self.images                          = images\n        self.labels                          = labels\n        self.num_features, self.num_examples = images.shape\n        self.pixel_dimension                 = int(np.sqrt(self.num_features))\n\n        # See https://imgaug.readthedocs.io/en/latest/source/augmenters.html\n        self.transformations = [\n            # Flip horizontally\n            [augmenters.Fliplr(1)],\n            # Change brightness via multiplication\n            [augmenters.Multiply((0.5, 1.5))],\n            # Rotate up to 45 degrees in either direction\n            [augmenters.Affine(rotate=(-45, 45))],\n        ]\n        self.num_transformations = len(self.transformations)\n\n    def augment(self):\n        results = self.format_for_augmentation(self.images)\n\n        for transformation in self.transformations:\n            formatted_images = self.format_for_augmentation(self.images)\n            augmenter        = augmenters.Sequential(transformation)\n            augmented        = augmenter.augment_images(formatted_images)\n            results          = np.concatenate((results, augmented), axis=0)\n\n        # Repeat the labels once for each augmented set of images\n        self.augmented_labels   = np.tile(self.labels, self.num_transformations + 1)\n        self.augmented_examples = self.format_for_training(results)\n\n        return self\n\n    def format_for_augmentation(self, original_images):\n        # ImgAug expects images formatted as a 4D numpy array of shape\n        # `(n, height, width, rgb_channels)`. We use greyscale images, for which\n        # ImgAug expects only a single channel.\n        return original_images.T.reshape(\n            (self.num_examples, self.pixel_dimension, self.pixel_dimension, 1)\n        )\n\n    def format_for_training(self, augmented_images):\n        augmented_num_examples = (self.num_transformations + 1) * self.num_examples\n        return np.array(augmented_images).reshape((augmented_num_examples, self.num_features)).T\n'"
neural_net/lib/data/data.py,12,"b""import numpy as np\n\nfrom lib.data.raw        import Raw\nfrom lib.data.augmenter  import Augmenter\nfrom lib.data.normalizer import Normalizer\n\nclass Data:\n    def __init__(self, filename = './lib/data/sources/fer2013.csv'):\n        self.filename = filename\n\n    def build(self, save_statistics = True):\n        raw_data  = Raw(self.filename).load()\n        augmenter = Augmenter(raw_data.training_examples,\n                              raw_data.training_labels).augment()\n\n        normalizer = Normalizer(augmenter.augmented_examples,\n                                raw_data.validation_examples,\n                                raw_data.test_examples).normalize(save_statistics)\n\n        self.training_examples = normalizer.normalized_training_examples\n        self.training_labels   = augmenter.augmented_labels\n\n        self.validation_examples = normalizer.normalized_validation_examples\n        self.validation_labels   = raw_data.validation_labels\n\n        self.test_examples = normalizer.normalized_test_examples\n        self.test_labels   = raw_data.test_labels\n\n        return self\n\n    def save(self):\n        np.save('./lib/data/sources/training_examples',   self.training_examples)\n        np.save('./lib/data/sources/training_labels',     self.training_labels)\n        np.save('./lib/data/sources/validation_examples', self.validation_examples)\n        np.save('./lib/data/sources/validation_labels',   self.validation_labels)\n        np.save('./lib/data/sources/test_examples',       self.test_examples)\n        np.save('./lib/data/sources/test_labels',         self.test_labels)\n\n        return self\n\n    def load(self):\n        self.training_examples   = np.load('./lib/data/sources/training_examples.npy')\n        self.training_labels     = np.load('./lib/data/sources/training_labels.npy')\n        self.validation_examples = np.load('./lib/data/sources/validation_examples.npy')\n        self.validation_labels   = np.load('./lib/data/sources/validation_labels.npy')\n        self.test_examples       = np.load('./lib/data/sources/test_examples.npy')\n        self.test_labels         = np.load('./lib/data/sources/test_labels.npy')\n\n        return self\n"""
neural_net/lib/data/normalizer.py,2,"b'import numpy as np\n\nclass Normalizer:\n    def __init__(self, training_examples, validation_examples, test_examples):\n        self.training_examples   = training_examples\n        self.validation_examples = validation_examples\n        self.test_examples       = test_examples\n\n    def normalize(self, save_statistics = True):\n        training_set_means                         = self.training_examples.mean(axis = 1, keepdims = True)\n        zero_mean_training_data                    = self.training_examples - training_set_means\n        zero_mean_training_set_standard_deviations = zero_mean_training_data.std(axis = 1, keepdims = True)\n\n        if save_statistics:\n            print(""Saving training set statistics for normalization..."")\n            np.save(\'./lib/data/normalization_statistics/training_set_feature_means\',\n                    zero_mean_training_data)\n\n            np.save(\'./lib/data/normalization_statistics/training_set_zero_mean_feature_standard_deviations\',\n                    zero_mean_training_set_standard_deviations)\n\n        # Transform training data so it has mean 0 and variance 1\n        self.normalized_training_examples   = zero_mean_training_data / zero_mean_training_set_standard_deviations\n\n        # Perform analagous transformation on the validation and test data\n        adjusted_mean_validation_data       = self.validation_examples - training_set_means\n        adjusted_mean_test_data             = self.test_examples       - training_set_means\n        self.normalized_validation_examples = adjusted_mean_validation_data / zero_mean_training_set_standard_deviations\n        self.normalized_test_examples       = adjusted_mean_test_data       / zero_mean_training_set_standard_deviations\n\n        return self\n'"
neural_net/lib/data/raw.py,2,"b'import numpy as np\nimport csv\nimport os\n\n# We\'re using the Facial Expression Recognition (""FER"") dataset from Kaggle:\n# https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data\nclass Raw:\n    num_classes = 7\n\n    def __init__(self, filename = \'./lib/data/sources/fer2013.csv\'):\n        self.file     = open(filename)\n        self.csv      = csv.DictReader(self.file)\n        self.datasets = {\n            \'Training\':    { \'examples\': [], \'labels\': [] },\n            \'PrivateTest\': { \'examples\': [], \'labels\': [] },\n            \'PublicTest\':  { \'examples\': [], \'labels\': [] }\n        }\n\n    def load(self):\n        try:\n            for row in self.csv:\n                klass = int(row[\'emotion\'])\n                if self.is_irrelevant_class(klass): continue\n                label = self.label(klass)\n\n                pixels = row[\'pixels\'].split()\n                if self.bad_data(pixels): continue\n\n                dataset = row[\'Usage\']\n\n                self.datasets[dataset][\'examples\'].append(pixels)\n                self.datasets[dataset][\'labels\'].append(label)\n\n            self.training_examples = self.to_numpy_array(self.datasets[\'Training\'][\'examples\'])\n            self.training_labels   = self.flatten_labels(\n                self.to_numpy_array(self.datasets[\'Training\'][\'labels\'])\n            )\n\n            self.validation_examples = self.to_numpy_array(self.datasets[\'PrivateTest\'][\'examples\'])\n            self.validation_labels   = self.flatten_labels(\n                self.to_numpy_array(self.datasets[\'PrivateTest\'][\'labels\'])\n            )\n\n            self.test_examples = self.to_numpy_array(self.datasets[\'PublicTest\'][\'examples\'])\n            self.test_labels   = self.flatten_labels(\n                self.to_numpy_array(self.datasets[\'PublicTest\'][\'labels\'])\n            )\n\n            return self\n        finally:\n            self.file.close()\n\n    # Labels: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\n    # We only care about happy, sad, and neutral.\n    def is_irrelevant_class(self, klass):\n        return klass not in [3, 4, 6]\n\n    # Convert an integer into a ""one-hot"" vector of 0s and 1s, with the sole 1\n    # at the index corresponding to the integer. Eg, with 7 possible classes the\n    # zero-indexed label `3` becomes `[0, 0, 0, 1, 0, 0, 0]`. The neural network\n    # outputs its predictions as vectors of this form, so it\'s easiest to convert\n    # the labels to the same form for comparison.\n    def label(self, klass):\n        one_hot_vector        = [[0]] * self.num_classes\n        one_hot_vector[klass] = [1]\n\n        return one_hot_vector\n\n    def to_numpy_array(self, list):\n        return np.array(list, \'uint8\').T\n\n    def flatten_labels(self, labels):\n        return labels.reshape((self.num_classes, -1))\n\n    def bad_data(self, pixels):\n        array = np.array(pixels, \'uint8\')\n        return array.std() == 0\n'"
neural_net/lib/optimizers/adam.py,9,"b'import numpy as np\n\nclass Adam:\n    momentum_rate = 0.9\n    rms_prop_rate = 0.999\n    epsilon       = 0.00000001\n\n    def __init__(self, learning_rate, weights, biases):\n        self.learning_rate           = learning_rate\n        self.weights                 = weights\n        self.biases                  = biases\n        self.momentum_weight_average = np.zeros(weights.shape)\n        self.momentum_bias_average   = np.zeros(biases.shape)\n        self.rms_prop_weight_average = np.zeros(weights.shape)\n        self.rms_prop_bias_average   = np.zeros(biases.shape)\n\n    def update_parameters(self, weight_gradients, bias_gradients):\n        self.momentum_weight_average = self.updated_momentum_weight_average(weight_gradients)\n        self.momentum_bias_average   = self.updated_momentum_bias_average(bias_gradients)\n        self.rms_prop_weight_average = self.updated_rms_prop_weight_average(weight_gradients)\n        self.rms_prop_bias_average   = self.updated_rms_prop_bias_average(bias_gradients)\n\n        self.weights = self.weights - (self.learning_rate * self.momentum_weight_average / self.weight_denominator())\n        self.biases  = self.biases  - (self.learning_rate * self.momentum_bias_average   / self.bias_denominator())\n\n    def updated_momentum_weight_average(self, weight_gradients):\n        return (self.momentum_rate * self.momentum_weight_average) + ((1 - self.momentum_rate) * weight_gradients)\n\n    def updated_momentum_bias_average(self, bias_gradients):\n        return (self.momentum_rate * self.momentum_bias_average) + ((1 - self.momentum_rate) * bias_gradients)\n\n    def updated_rms_prop_weight_average(self, weight_gradients):\n        return (self.rms_prop_rate * self.rms_prop_weight_average) + ((1 - self.rms_prop_rate) * np.square(weight_gradients))\n\n    def updated_rms_prop_bias_average(self, bias_gradients):\n        return (self.rms_prop_rate * self.rms_prop_bias_average) + ((1 - self.rms_prop_rate) * np.square(bias_gradients))\n\n    def weight_denominator(self):\n        square_roots = self.square_roots(self.rms_prop_weight_average)\n        return np.array(square_roots) + self.epsilon\n\n    def bias_denominator(self):\n        square_roots = self.square_roots(self.rms_prop_bias_average)\n        return np.array(square_roots) + self.epsilon\n\n    def square_roots(self, n_dimensional_array):\n        return list(np.sqrt(layer) for layer in n_dimensional_array)\n'"
neural_net/lib/optimizers/gradient_descent.py,0,"b'import numpy as np\n\nclass GradientDescent:\n    def __init__(self, learning_rate, weights, biases):\n        self.learning_rate = learning_rate\n        self.weights       = weights\n        self.biases        = biases\n\n    def update_parameters(self, weight_gradients, bias_gradients):\n        self.weights = self.weights - (self.learning_rate * weight_gradients)\n        self.biases  = self.biases  - (self.learning_rate * bias_gradients)\n'"
neural_net/lib/utilities/confusion_matrix.py,0,"b'from sklearn.metrics import confusion_matrix\n\nclass ConfusionMatrix:\n    def __init__(self, actual_classes, predicted_classes):\n        self.actual_classes    = actual_classes\n        self.predicted_classes = predicted_classes\n\n    def string(self):\n        return str(self.array())\n\n    def array(self):\n        return confusion_matrix(self.actual_classes, self.predicted_classes)\n'"
neural_net/lib/utilities/gradient_check.py,2,"b'import numpy as np\n\nfrom lib.cost         import Cost\nfrom lib.forward_prop import ForwardProp\n\n# The job of `BackwardProp` is to *efficiently* calculate gradients\n# (derivatives for a vector/array). A neural network can easily have many\n# thousands of weights, and needs to calculate the derivative for all of\n# in order to ""learn"" (update the weights in such a way that the network\n# gets better at its assigned task). These weight updates will likely need\n# to happen thousands of times before the network becomes suitably effective.\n#\n# Therefore it becomes prohibitively expensive to calculate the *numeric*\n# gradients for each individual weight on every iteration. ""Numeric"" meaning\n# calcuating the approximate derivative based on the two-sided limit formula\n# `(f(x + e) - f(x - e)) / 2e`.\n#\n# Instead, we can calculate the ""analytic"" derivatives, which means\n# algebraically calculating the derivatives for all weights in a layer at\n# once, via the chain rule.\nclass GradientCheck:\n    def __init__(self, weight_gradients, weights, biases, examples, labels, regularization_strength):\n        self.weight_gradients        = weight_gradients\n        self.weights                 = weights\n        self.biases                  = biases\n        self.examples                = examples\n        self.labels                  = labels\n        self.regularization_strength = regularization_strength\n        self.epsilon                 = 0.0000001\n\n    def run(self, layer):\n        gradients = self.numeric_gradients(layer)\n\n        # Calling `allclose` with `atol = 0` means we don\'t care about the\n        # absolute difference between the analytic and numeric gradients.\n        # Using a positive `rtol` value means that we *do* care about the relative\n        # magnitudes of the values being compared. In effect this makes our\n        # gradient check say ""are the numeric gradients within\n        # `rtol * analytic gradients` of the analytic gradients?""\n        return np.allclose(self.weight_gradients[layer], gradients, atol = 0, rtol = 0.001)\n\n    def numeric_gradients(self, layer):\n        gradients = np.zeros(self.weights[layer].shape)\n\n        for row in range(self.num_rows(layer)):\n            for column in range(self.num_columns(layer)):\n                original_weight = self.weights[layer][row][column]\n\n                gradients[row][column] = self.numeric_gradient(original_weight,\n                                                               layer,\n                                                               row,\n                                                               column)\n\n                self.weights[layer][row][column] = original_weight\n\n        return gradients\n\n    def numeric_gradient(self, weight, layer, row, column):\n        self.weights[layer][row][column]       = weight + self.epsilon\n        network_output_with_weight_adjusted_up = self.network_output()\n        cost_with_weight_adjusted_up           = self.cost(network_output_with_weight_adjusted_up)\n\n        self.weights[layer][row][column]         = weight - self.epsilon\n        network_output_with_weight_adjusted_down = self.network_output()\n        cost_with_weight_adjusted_down           = self.cost(network_output_with_weight_adjusted_down)\n\n        cost_difference = (cost_with_weight_adjusted_up - cost_with_weight_adjusted_down)\n\n        return cost_difference / (2 * self.epsilon)\n\n    def cost(self, network_output):\n        return Cost(network_output,\n                    self.labels,\n                    self.weights,\n                    self.regularization_strength,\n                    0.0).cross_entropy_loss()\n\n    def network_output(self):\n        forward_prop = ForwardProp(self.weights, self.biases, self.examples)\n        forward_prop.run()\n\n        return forward_prop.network_output\n\n    def num_rows(self, layer):\n        return self.weights[layer].shape[0]\n\n    def num_columns(self, layer):\n        return self.weights[layer].shape[1]\n'"
neural_net/lib/utilities/graph.py,0,"b""import matplotlib\nmatplotlib.use('TkAgg')\nimport matplotlib.pyplot as pyplot\n\nclass Graph:\n    def __init__(self, ylabel, xlabel, data):\n        self.ylabel = ylabel\n        self.xlabel = xlabel\n        self.data   = data\n\n    def save(self, filename):\n        pyplot.ylabel(self.ylabel)\n        pyplot.xlabel(self.xlabel)\n        pyplot.plot(self.data)\n        pyplot.savefig(filename)\n"""
neural_net/lib/utilities/image_renderer.py,0,"b'import numpy      as np\nimport scipy.misc as spm\n\nclass ImageRenderer:\n    def render(self, pixels):\n        spm.toimage(pixels).show()\n\n    def render_48_by_48(self, pixels):\n        reshaped = pixels.reshape((48, 48))\n        self.render(reshaped)\n'"
neural_net/lib/utilities/timer.py,0,"b'from time     import time\nfrom datetime import datetime, timedelta\n\nclass Timer:\n    def time(self, fn):\n        start_time        = time()\n        result            = fn()\n        end_time          = time()\n        seconds           = timedelta(seconds=int(end_time - start_time))\n        self.time_elapsed = datetime(1,1,1) + seconds\n\n        return result\n\n    def string(self):\n        time_components = [self.day(), self.hour(), self.minute(), self.second()]\n        return "":"".join([str(el) for el in time_components])\n\n    def day(self):\n        return self.time_elapsed.day - 1\n\n    def hour(self):\n        return self.time_elapsed.hour\n\n    def minute(self):\n        return self.time_elapsed.minute\n\n    def second(self):\n        return self.time_elapsed.second\n'"
neural_net/tests/data/test_augmenter.py,0,"b""import unittest\n\nfrom imgaug import augmenters\n\nfrom lib.data.raw       import Raw\nfrom lib.data.augmenter import Augmenter\n\nclass TestAugmenter(unittest.TestCase):\n    def setUp(self):\n        data                   = Raw('./lib/data/sources/fer_subset.csv').load()\n        self.original_examples = data.training_examples\n        self.original_labels   = data.training_labels\n        self.augmenter         = Augmenter(self.original_examples, self.original_labels).augment()\n\n    def test_additional_examples_are_generated(self):\n        num_augmented = self.augmenter.augmented_examples.shape[1]\n        num_original  = self.original_examples.shape[1]\n        self.assertEqual(num_augmented, num_original * 6)\n\n    def test_additional_examples_are_valid(self):\n        augmented                  = self.augmenter.augmented_examples\n        original                   = self.original_examples\n        num_features, num_examples = original.shape\n\n        # The original examples should be included\n        self.assertTrue((original == augmented[:, 0:num_examples]).all())\n\n        # As well as transformations of the original examples\n        formatted_images     = self.augmenter.format_for_augmentation(original)\n        flipper              = augmenters.Sequential([augmenters.Fliplr(1)])\n        flipped              = flipper.augment_images(formatted_images)\n        first_transformation = flipped.reshape((num_examples, num_features)).T\n\n        self.assertTrue(\n            (first_transformation == augmented[:, num_examples:num_examples * 2]).all()\n        )\n\n    def test_additional_labels_are_generated(self):\n        num_augmented = self.augmenter.augmented_labels.shape[1]\n        num_original  = self.original_labels.shape[1]\n        self.assertEqual(num_augmented, num_original * 6)\n\n    # `Augmenter` should copy the labels once for each transformation of the\n    # examples it generates.\n    def test_additional_labels_are_valid(self):\n        augmented  = self.augmenter.augmented_labels\n        original   = self.original_labels\n        num_labels = original.shape[1]\n\n        self.assertTrue((original == augmented[:, 0:num_labels]).all())\n        self.assertTrue((original == augmented[:, num_labels:num_labels * 2]).all())\n\nif __name__ == '__main__':\n    unittest.main()\n"""
neural_net/tests/data/test_data.py,0,"b""import unittest\n\nfrom lib.data.data import Data\n\nnum_pixels  = 48 * 48\nnum_classes = 7\n\nclass TestFormatter(unittest.TestCase):\n    def setUp(self):\n        self.data = Data('./lib/data/sources/fer_subset.csv').build(save_statistics = False)\n\n    def test_examples_shape(self):\n        self.assertEqual(self.data.training_examples.shape[0],   num_pixels)\n        self.assertEqual(self.data.validation_examples.shape[0], num_pixels)\n        self.assertEqual(self.data.test_examples.shape[0],       num_pixels)\n\n    def test_labels_shape(self):\n        self.assertEqual(self.data.training_labels.shape[0],   num_classes)\n        self.assertEqual(self.data.validation_labels.shape[0], num_classes)\n        self.assertEqual(self.data.test_labels.shape[0],       num_classes)\n\n    def test_same_number_of_labels_and_examples(self):\n        self.assertEqual(\n            self.data.training_examples.shape[1],\n            self.data.training_labels.shape[1]\n        )\n\n        self.assertEqual(\n            self.data.validation_examples.shape[1],\n            self.data.validation_labels.shape[1]\n        )\n\n        self.assertEqual(\n            self.data.test_examples.shape[1],\n            self.data.test_labels.shape[1]\n        )\n\n    def test_labels_values(self):\n        self.assertEqual(\n            self.data.training_labels.shape[1],\n            self.data.training_labels.sum()\n        )\n\n        self.assertEqual(\n            self.data.validation_labels.shape[1],\n            self.data.validation_labels.sum()\n        )\n\n        self.assertEqual(\n            self.data.test_labels.shape[1],\n            self.data.test_labels.sum()\n        )\n\nif __name__ == '__main__':\n    unittest.main()\n"""
neural_net/tests/data/test_normalizer.py,9,"b""import unittest\nimport numpy as np\n\nfrom lib.data.normalizer import Normalizer\n\nclass TestFormatter(unittest.TestCase):\n    def setUp(self):\n        self.original_data = np.random.randn(10, 10) * 100\n        self.normalizer    = Normalizer(self.original_data,\n                                        self.original_data,\n                                        self.original_data).normalize(save_statistics = False)\n\n    def test_data_is_normalized_to_have_zero_mean_and_one_standard_deviation(self):\n        self.assertFalse(np.isclose(self.original_data.mean(), 0, atol = 0.01))\n        self.assertFalse(np.isclose(self.original_data.std(),  1, atol = 0.01))\n\n        self.assertTrue(np.isclose(self.normalizer.normalized_training_examples.mean(), 0, atol = 0.01))\n        self.assertTrue(np.isclose(self.normalizer.normalized_training_examples.std(),  1, atol = 0.01))\n\n        self.assertTrue(np.isclose(self.normalizer.normalized_validation_examples.mean(), 0, atol = 0.01))\n        self.assertTrue(np.isclose(self.normalizer.normalized_validation_examples.std(),  1, atol = 0.01))\n\n        self.assertTrue(np.isclose(self.normalizer.normalized_test_examples.mean(), 0, atol = 0.01))\n        self.assertTrue(np.isclose(self.normalizer.normalized_test_examples.std(),  1, atol = 0.01))\n\nif __name__ == '__main__':\n    unittest.main()\n"""
neural_net/tests/data/test_raw.py,0,"b""import unittest\n\nfrom lib.data.raw import Raw\n\nnum_pixels  = 48 * 48\nnum_classes = 7\n\nclass TestRaw(unittest.TestCase):\n    def setUp(self):\n        self.data = Raw('./lib/data/sources/fer_subset.csv').load()\n\n    def test_examples_shape(self):\n        self.assertEqual(self.data.training_examples.shape[0],   num_pixels)\n        self.assertEqual(self.data.validation_examples.shape[0], num_pixels)\n        self.assertEqual(self.data.test_examples.shape[0],       num_pixels)\n\n    def test_labels_shape(self):\n        self.assertEqual(self.data.training_labels.shape[0],   num_classes)\n        self.assertEqual(self.data.validation_labels.shape[0], num_classes)\n        self.assertEqual(self.data.test_labels.shape[0],       num_classes)\n\n    def test_same_number_of_labels_and_examples(self):\n        self.assertEqual(\n            self.data.training_examples.shape[1],\n            self.data.training_labels.shape[1]\n        )\n\n        self.assertEqual(\n            self.data.validation_examples.shape[1],\n            self.data.validation_labels.shape[1]\n        )\n\n        self.assertEqual(\n            self.data.test_examples.shape[1],\n            self.data.test_labels.shape[1]\n        )\n\n    def test_labels_values(self):\n        self.assertEqual(\n            self.data.training_labels.shape[1],\n            self.data.training_labels.sum()\n        )\n\n        self.assertEqual(\n            self.data.validation_labels.shape[1],\n            self.data.validation_labels.sum()\n        )\n\n        self.assertEqual(\n            self.data.test_labels.shape[1],\n            self.data.test_labels.sum()\n        )\n\n    def test_bad_data_removed(self):\n        standard_deviations = self.data.training_examples.std(axis = 0)\n        self.assertFalse((standard_deviations == 0).any())\n\nif __name__ == '__main__':\n    unittest.main()\n"""
neural_net/tests/optimizers/test_adam.py,10,"b'import unittest\nimport numpy as np\n\nfrom lib.optimizers.adam import Adam\n\n# Adam is a ""momentum"" based optimizer. It\'s similar to GradientDescent in\n# that it updates parameters by subtracting the gradients (derivatives) of\n# those parameters multiplied by some small coefficient (the learning rate).\n# Let\'s call that product the ""update amount"".\n#\n# Adam differs from GradientDescent in that its update amount also includes\n# a weighted average of previous update amounts. So, as you iteratively call\n# `Adam#update_parameters(weight_gradients, bias_gradients)`, the update\n# amount is influenced more and more by its previous values, and less by the\n# gradients passed in to that iteration. In this way Adam ""builds momentum""\n# by updating parameters in a way that over time becomes less responsive to\n# the arguments passed in, and more similar to previous updates. This method\n# is far more effective at quickly finding optimal paramters than non-momentum\n# based algorithms like GradientDescent.\nclass TestAdam(unittest.TestCase):\n    def setUp(self):\n        learning_rate          = 0.01\n        num_input_features     = 10\n        num_hidden_layer_nodes = 5\n        num_classes            = 3\n\n        weights = np.array([\n            np.ones((num_hidden_layer_nodes, num_input_features)) * 0.01,\n            np.ones((num_classes, num_hidden_layer_nodes))        * 0.01\n        ])\n\n        biases = np.array([\n            np.zeros((num_hidden_layer_nodes, 1)),\n            np.zeros((num_classes, 1))\n        ])\n\n        self.weight_gradients = np.copy(weights)\n\n        self.bias_gradients = np.array([\n            np.random.randn(num_hidden_layer_nodes, 1),\n            np.random.randn(num_classes, 1)\n        ])\n\n        self.adam = Adam(learning_rate, weights, biases)\n\n    def test_parameters_decrease_at_a_decreasing_rate(self):\n        update_amounts = []\n\n        for i in range(0, 10):\n            weights_before_update = self.adam.weights\n            self.adam.update_parameters(self.weight_gradients, self.bias_gradients)\n\n            update_amount = abs(weights_before_update[0] - self.adam.weights[0]).mean()\n            update_amounts.append(update_amount)\n\n        update_amount_deltas = []\n\n        for i in range(0, len(update_amounts) - 1):\n            self.assertTrue(update_amounts[i] < update_amounts[i + 1])\n\n            update_amount_delta = abs(update_amounts[i] - update_amounts[i + 1])\n            update_amount_deltas.append(update_amount_delta)\n\n        for i in range(0, len(update_amount_deltas) - 1):\n            self.assertTrue(update_amount_deltas[i] > update_amount_deltas[i + 1])\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
neural_net/tests/optimizers/test_gradient_descent.py,14,"b""import unittest\nimport numpy as np\n\nfrom lib.optimizers.gradient_descent import GradientDescent\n\nlearning_rate          = 0.001\nnum_input_features     = 10\nnum_hidden_layer_nodes = 5\nnum_classes            = 3\n\nclass TestGradientDescent(unittest.TestCase):\n    def setUp(self):\n        self.weights = np.array([\n            np.random.randn(num_hidden_layer_nodes, num_input_features),\n            np.random.randn(num_classes, num_hidden_layer_nodes),\n        ])\n\n        self.biases = np.array([\n            np.zeros((num_hidden_layer_nodes, 1)),\n            np.zeros((num_classes, 1)),\n        ])\n\n        self.gradient_descent = GradientDescent(learning_rate, self.weights, self.biases)\n\n    def test_update_parameters(self):\n        weight_gradients = np.array([\n            np.ones((num_hidden_layer_nodes, num_input_features)),\n            np.ones((num_classes, num_hidden_layer_nodes)),\n        ])\n\n        bias_gradients = np.array([\n            np.random.randn(num_hidden_layer_nodes, 1),\n            np.random.randn(num_classes, 1),\n        ])\n\n        self.gradient_descent.update_parameters(weight_gradients, bias_gradients)\n\n        expected_updated_weights = self.weights - learning_rate\n\n        self.assertTrue(\n            np.allclose(self.gradient_descent.weights[0], expected_updated_weights[0])\n        )\n\n        self.assertTrue(\n            np.allclose(self.gradient_descent.weights[1], expected_updated_weights[1])\n        )\n\nif __name__ == '__main__':\n    unittest.main()\n"""
