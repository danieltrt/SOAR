file_path,api_count,code
6.CHATBOT/predict.py,0,"b'import tensorflow as tf\nimport data\nimport sys\nimport model as ml\n\nfrom configs import DEFINES\n\t\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    arg_length = len(sys.argv)\n    \n    if(arg_length < 2):\n        raise Exception(""Don\'t call us. We\'ll call you"")\n  \n    \n    char2idx,  idx2char, vocabulary_length = data.load_vocabulary()\n    input = """"\n    for i in sys.argv[1:]:\n        input += i \n        input += "" ""\n        \n    print(input)\n    # \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 / \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5 / \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    predic_input_enc, predic_input_enc_length = data.enc_processing([input], char2idx)\n    predic_output_dec, predic_output_dec_length = data.dec_input_processing([""""], char2idx)\n    predic_target_dec = data.dec_target_processing([""""], char2idx)\n\n\t# \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0 \xea\xb5\xac\xec\x84\xb1\n    classifier = tf.estimator.Estimator(\n            model_fn=ml.model,\n            model_dir=DEFINES.check_point_path, \n            params={ \n                \'hidden_size\': DEFINES.hidden_size, \n                \'layer_size\': DEFINES.layer_size, \n                \'learning_rate\': DEFINES.learning_rate, \n                \'vocabulary_length\': vocabulary_length, \n                \'embedding_size\': DEFINES.embedding_size, \n                \'embedding\': DEFINES.embedding, \n                \'multilayer\': DEFINES.multilayer, \n            })\n\n    predictions = classifier.predict(\n        input_fn=lambda:data.eval_input_fn(predic_input_enc, predic_output_dec, predic_target_dec, DEFINES.batch_size))\n    \n    data.pred2string(predictions, idx2char)\n'"
5.TEXT_SIM/Appendix/5.3.3_Quora_LSTM_Appendix_2.py,11,"b'""""""\n\xec\x9c\xa0\xec\x82\xac\xed\x95\x9c BI-LSTM \xec\xa0\x91\xea\xb7\xbc\xeb\xb2\x95\xec\x9e\x85\xeb\x8b\x88\xeb\x8b\xa4.\nstatic_bidirectional_rnn\xec\x9d\x84 \xed\x99\x9c\xec\x9a\xa9\xed\x95\x98\xec\x98\x80\xec\x8a\xb5\xeb\x8b\x88\xeb\x8b\xa4.\n""""""\n# coding: utf-8\n\n\n\nimport sys\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport pandas as pd\n\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.keras.layers import Layer\n\nfrom sklearn.model_selection import train_test_split\n\n# from tensorflow.keras import backend as K\n\nimport json\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n# # Initial global var\n\n# In[ ]:\n\n\n## \xeb\xaf\xb8\xeb\xa6\xac Global \xeb\xb3\x80\xec\x88\x98\xeb\xa5\xbc \xec\xa7\x80\xec\xa0\x95\xed\x95\x98\xec\x9e\x90. \xed\x8c\x8c\xec\x9d\xbc \xeb\xaa\x85, \xed\x8c\x8c\xec\x9d\xbc \xec\x9c\x84\xec\xb9\x98, \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac \xeb\x93\xb1\xec\x9d\xb4 \xec\x9e\x88\xeb\x8b\xa4.\n\nDATA_IN_PATH = \'../data_in/\'\nDATA_OUT_PATH = \'../data_out/\'\n\nTRAIN_Q1_DATA_FILE = \'train_q1.npy\'\nTRAIN_Q2_DATA_FILE = \'train_q2.npy\'\nTRAIN_LABEL_DATA_FILE = \'train_label.npy\'\nNB_WORDS_DATA_FILE = \'data_configs.json\'\n\n# # Load Dataset\n\n# In[ ]:\n\n\n## \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xb6\x88\xeb\x9f\xac\xec\x98\xa4\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4. \xed\x9a\xa8\xea\xb3\xbc\xec\xa0\x81\xec\x9d\xb8 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xeb\xb6\x88\xeb\x9f\xac\xec\x98\xa4\xea\xb8\xb0\xeb\xa5\xbc \xec\x9c\x84\xed\x95\xb4, \xeb\xaf\xb8\xeb\xa6\xac \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xed\x98\x95\xed\x83\x9c\xeb\xa1\x9c \xec\xa0\x80\xec\x9e\xa5\xec\x8b\x9c\xed\x82\xa8 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa1\x9c\xeb\x93\x9c\xed\x95\x9c\xeb\x8b\xa4.\n\nq1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, \'rb\'))\nq2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, \'rb\'))\nlabels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, \'rb\'))\nprepro_configs = None\n\nwith open(DATA_IN_PATH + NB_WORDS_DATA_FILE, \'r\') as f:\n    prepro_configs = json.load(f)\n\n## \xed\x95\x99\xec\x8a\xb5\xec\x97\x90 \xed\x95\x84\xec\x9a\x94\xed\x95\x9c \xed\x8c\x8c\xeb\x9d\xbc\xeb\xa9\x94\xed\x84\xb0\xeb\x93\xa4\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4\xec\x84\x9c \xec\xa7\x80\xec\xa0\x95\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n\nprint(""# of dataset: {}"".format(len(labels)))\n\nBATCH_SIZE = 4096\nEPOCH = 20\nHIDDEN = 64\nBUFFER_SIZE = len(q1_data)\n\nNUM_LAYERS = 3\nDROPOUT_RATIO = 0.3\n\nTEST_SPLIT = 0.1\nRNG_SEED = 13371447\nEMBEDDING_DIM = 128\nMAX_SEQ_LEN = 31\n\n# In[ ]:\n\n\nVOCAB_SIZE = prepro_configs[\'vocab_size\']\n\n\n# # Split train and test dataset\n\n# In[ ]:\n\n\nq1_data_len = np.array([min(len(x), MAX_SEQ_LEN) for x in q1_data])\nq2_data_len = np.array([min(len(x), MAX_SEQ_LEN) for x in q2_data])\n\n# In[ ]:\n\n\n## \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\x82\x98\xeb\x88\x84\xec\x96\xb4 \xec\xa0\x80\xec\x9e\xa5\xed\x95\x98\xec\x9e\x90. sklearn\xec\x9d\x98 train_test_split\xec\x9d\x84 \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xeb\xa9\xb4 \xec\x9c\xa0\xec\x9a\xa9\xed\x95\x98\xeb\x8b\xa4. \xed\x95\x98\xec\xa7\x80\xeb\xa7\x8c, \xec\xbf\xbc\xeb\x9d\xbc \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xec\x9d\x98 \xea\xb2\xbd\xec\x9a\xb0\xeb\x8a\x94\n## \xec\x9e\x85\xeb\xa0\xa5\xec\x9d\xb4 1\xea\xb0\x9c\xea\xb0\x80 \xec\x95\x84\xeb\x8b\x88\xeb\x9d\xbc 2\xea\xb0\x9c\xec\x9d\xb4\xeb\x8b\xa4. \xeb\x94\xb0\xeb\x9d\xbc\xec\x84\x9c, np.stack\xec\x9d\x84 \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xec\x97\xac \xeb\x91\x90\xea\xb0\x9c\xeb\xa5\xbc \xed\x95\x98\xeb\x82\x98\xeb\xa1\x9c \xec\x8c\x93\xec\x9d\x80\xeb\x8b\xa4\xec\x9d\x8c \xed\x99\x9c\xec\x9a\xa9\xed\x95\x98\xec\x97\xac \xeb\xb6\x84\xeb\xa5\x98\xed\x95\x9c\xeb\x8b\xa4.\n\nX = np.stack((q1_data, q2_data), axis=1)\ny = labels\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n\ntrain_Q1 = train_X[:,0]\ntrain_Q2 = train_X[:,1]\ntest_Q1 = test_X[:,0]\ntest_Q2 = test_X[:,1]\n\n\n# In[ ]:\n\n\ndef rearrange(base, hypothesis, labels):\n    features = {""base"": base, ""hypothesis"": hypothesis}\n    return features, labels\n\ndef train_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices((train_Q1, train_Q2, train_y))\n    dataset = dataset.shuffle(buffer_size=len(train_Q1))\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.map(rearrange)\n    dataset = dataset.repeat(EPOCH)\n    iterator = dataset.make_one_shot_iterator()\n    \n    return iterator.get_next()\n\ndef eval_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices((test_Q1, test_Q2, test_y))\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.map(rearrange)\n    iterator = dataset.make_one_shot_iterator()\n    \n    return iterator.get_next()\n\nclass ManDist(Layer):\n    """"""\n    Keras Custom Layer that calculates Manhattan Distance.\n    """"""\n\n    # initialize the layer, No need to include inputs parameter!\n    def __init__(self, **kwargs):\n        self.result = None\n        super(ManDist, self).__init__(**kwargs)\n\n    # input_shape will automatic collect input shapes to build layer\n    def build(self, input_shape):\n        super(ManDist, self).build(input_shape)\n\n    # This is where the layer\'s logic lives.\n    def call(self, x, **kwargs):\n        self.result = K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True))\n        return self.result\n\n    # return output shape\n    def compute_output_shape(self, input_shape):\n        return K.int_shape(self.result)\n\n\ndef BiRNN(x, dropout, scope, hidden_units):\n    n_hidden = hidden_units\n    n_layers = 3\n    # Prepare data shape to match `static_rnn` function requirements\n    x = tf.unstack(tf.transpose(x, perm=[1, 0, 2]))\n    print(x)\n    # Define lstm cells with tensorflow\n    # Forward direction cell\n    with tf.name_scope(""fw"" + scope), tf.variable_scope(""fw"" + scope):\n        stacked_rnn_fw = []\n        for _ in range(n_layers):\n            fw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n            lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(fw_cell, output_keep_prob=dropout)\n            stacked_rnn_fw.append(lstm_fw_cell)\n        lstm_fw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_fw, state_is_tuple=True)\n\n    with tf.name_scope(""bw"" + scope), tf.variable_scope(""bw"" + scope):\n        stacked_rnn_bw = []\n        for _ in range(n_layers):\n            bw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n            lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(bw_cell, output_keep_prob=dropout)\n            stacked_rnn_bw.append(lstm_bw_cell)\n        lstm_bw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_bw, state_is_tuple=True)\n    # Get lstm cell output\n\n    with tf.name_scope(""bw"" + scope), tf.variable_scope(""bw"" + scope):\n        outputs, _, _ = tf.nn.static_bidirectional_rnn(lstm_fw_cell_m, lstm_bw_cell_m, x, dtype=tf.float32)\n    return outputs[-1]\n\n\n# # Model setup\n\ndef Malstm(features, labels, mode):\n        \n    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n    EVAL = mode == tf.estimator.ModeKeys.EVAL\n    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n            \n    embedding = tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n    \n    base_embedded_matrix = embedding(features[\'base\'])\n    hypothesis_embedded_matrix = embedding(features[\'hypothesis\'])\n\n    base_sementic_matrix = BiRNN(base_embedded_matrix, DROPOUT_RATIO, \'base\', HIDDEN)\n    hypothesis_sementic_matrix = BiRNN(hypothesis_embedded_matrix, DROPOUT_RATIO, \'hypothesis\', HIDDEN)\n\n    logit_layer = ManDist()([base_sementic_matrix, hypothesis_sementic_matrix])\n    logit_layer = tf.squeeze(logit_layer, axis=-1)\n\n    # self._ma_dist([q1_lstm, q2_lstm])\n\n    # logit_layer = tf.exp(-tf.reduce_sum(tf.abs(base_sementic_matrix - hypothesis_sementic_matrix), axis=1, keepdims=True))\n    # logit_layer = tf.squeeze(logit_layer, axis=-1)\n    #\n    if PREDICT:\n        return tf.estimator.EstimatorSpec(\n                  mode=mode,\n                  predictions={\n                      \'is_duplicate\':logit_layer\n                  })\n    \n    #prediction \xec\xa7\x84\xed\x96\x89 \xec\x8b\x9c, None\n    if labels is not None:\n        labels = tf.to_float(labels)\n    \n#     loss = tf.reduce_mean(tf.keras.metrics.binary_crossentropy(y_true=labels, y_pred=logit_layer))\n    loss = tf.losses.mean_squared_error(labels=labels, predictions=logit_layer)\n#     loss = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(labels, logit_layer))\n    \n    if EVAL:\n        accuracy = tf.metrics.accuracy(labels, tf.round(logit_layer))\n        eval_metric_ops = {\'acc\': accuracy}\n        return tf.estimator.EstimatorSpec(\n                  mode=mode,\n                  eval_metric_ops= eval_metric_ops,\n                  loss=loss)\n\n    elif TRAIN:\n\n        global_step = tf.train.get_global_step()\n        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n\n        return tf.estimator.EstimatorSpec(\n                  mode=mode,\n                  train_op=train_op,\n                  loss=loss)\n\n\n# # Training & Eval\n\n# In[ ]:\n\n\nos.environ[""CUDA_VISIBLE_DEVICES""]=""7"" #For TEST\n\nmodel_dir = os.path.join(os.getcwd(), DATA_OUT_PATH + ""/checkpoint/rnn2/"")\nos.makedirs(model_dir, exist_ok=True)\n\nconfig_tf = tf.estimator.RunConfig()\n\nlstm_est = tf.estimator.Estimator(Malstm, model_dir=model_dir)\n\n\n# In[ ]:\n\n\nlstm_est.train(train_input_fn)\n\n\n# In[ ]:\n\n\nlstm_est.evaluate(eval_input_fn)\n\n\n# # Load test dataset |& create submit dataset to kaggle\n\n# In[ ]:\n\n\nTEST_Q1_DATA_FILE = \'test_q1.npy\'\nTEST_Q2_DATA_FILE = \'test_q2.npy\'\nTEST_ID_DATA_FILE = \'test_id.npy\'\n\ntest_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, \'rb\'))\ntest_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, \'rb\'))\ntest_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA_FILE, \'rb\'))\n\n\n# In[ ]:\n\n\npredict_input_fn = tf.estimator.inputs.numpy_input_fn(x={""base"":test_q1_data, \n                                                         ""hypothesis"":test_q2_data}, \n                                                      shuffle=False)\n\npredictions = np.array([p[\'is_duplicate\'] for p in lstm_est.predict(input_fn=\npredict_input_fn)])\n\n\n# In[ ]:\n\n\nprint(len(predictions)) #2345796\n\noutput = pd.DataFrame( data={""test_id"":test_id_data, ""is_duplicate"": list(predictions)} )\noutput.to_csv( ""rnn_predict.csv"", index=False, quoting=3 )\n\n'"
6.CHATBOT/6.3 seq2seq/configs.py,0,"b""#-*- coding: utf-8 -*-\nimport tensorflow as tf\n\ntf.app.flags.DEFINE_string('f', '', 'kernel') # \xec\xa3\xbc\xed\x94\xbc\xed\x84\xb0\xec\x97\x90\xec\x84\x9c \xec\xbb\xa4\xeb\x84\x90\xec\x97\x90 \xec\xa0\x84\xeb\x8b\xac\xed\x95\x98\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c \xed\x94\x84\xeb\xa0\x88\xea\xb7\xb8 \xeb\xb0\xa9\xeb\xb2\x95\ntf.app.flags.DEFINE_integer('batch_size', 64, 'batch size') # \xeb\xb0\xb0\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0\ntf.app.flags.DEFINE_integer('train_steps', 20000, 'train steps') # \xed\x95\x99\xec\x8a\xb5 \xec\x97\x90\xed\x8f\xac\xed\x81\xac\ntf.app.flags.DEFINE_float('dropout_width', 0.5, 'dropout width') # \xeb\x93\x9c\xeb\xa1\xad\xec\x95\x84\xec\x9b\x83 \xed\x81\xac\xea\xb8\xb0\ntf.app.flags.DEFINE_integer('layer_size', 3, 'layer size') # \xeb\xa9\x80\xed\x8b\xb0 \xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4 \xed\x81\xac\xea\xb8\xb0 (multi rnn)\ntf.app.flags.DEFINE_integer('hidden_size', 128, 'weights size') # \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0\ntf.app.flags.DEFINE_float('learning_rate', 1e-3, 'learning rate') # \xed\x95\x99\xec\x8a\xb5\xeb\xa5\xa0\ntf.app.flags.DEFINE_string('data_path', './../data_in/ChatBotData.csv', 'data path') #  \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\x9c\x84\xec\xb9\x98\ntf.app.flags.DEFINE_string('vocabulary_path', './data_out/vocabularyData.voc', 'vocabulary path') # \xec\x82\xac\xec\xa0\x84 \xec\x9c\x84\xec\xb9\x98\ntf.app.flags.DEFINE_string('check_point_path', './data_out/check_point', 'check point path') # \xec\xb2\xb4\xed\x81\xac \xed\x8f\xac\xec\x9d\xb8\xed\x8a\xb8 \xec\x9c\x84\xec\xb9\x98\ntf.app.flags.DEFINE_integer('shuffle_seek', 1000, 'shuffle random seek') # \xec\x85\x94\xed\x94\x8c \xec\x8b\x9c\xeb\x93\x9c\xea\xb0\x92\ntf.app.flags.DEFINE_integer('max_sequence_length', 25, 'max sequence length') # \xec\x8b\x9c\xed\x80\x80\xec\x8a\xa4 \xea\xb8\xb8\xec\x9d\xb4\ntf.app.flags.DEFINE_integer('embedding_size', 128, 'embedding size') # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xed\x81\xac\xea\xb8\xb0\ntf.app.flags.DEFINE_boolean('tokenize_as_morph', True, 'set morph tokenize') # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c\xec\x97\x90 \xeb\x94\xb0\xeb\xa5\xb8 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\ntf.app.flags.DEFINE_boolean('embedding', True, 'Use Embedding flag') # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xec\x9c\xa0\xeb\xac\xb4 \xec\x84\xa4\xec\xa0\x95\ntf.app.flags.DEFINE_boolean('multilayer', True, 'Use Multi RNN Cell') # \xeb\xa9\x80\xed\x8b\xb0 RNN \xec\x9c\xa0\xeb\xac\xb4\n# Define FLAGS\nDEFINES = tf.app.flags.FLAGS\n"""
6.CHATBOT/6.3 seq2seq/data.py,3,"b'from konlpy.tag import Okt\nimport pandas as pd\nimport tensorflow as tf\nimport enum\nimport os\nimport re\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom configs import DEFINES\n\nfrom tqdm import tqdm\nFILTERS = ""([~.,!?\\""\':;)(])""\nPAD = ""<PADDING>""\nSTD = ""<START>""\nEND = ""<END>""\nUNK = ""<UNKNOWN>""\n\nPAD_INDEX = 0\nSTD_INDEX = 1\nEND_INDEX = 2\nUNK_INDEX = 3\n\nMARKER = [PAD, STD, END, UNK]\nCHANGE_FILTER = re.compile(FILTERS)\n\n# \xed\x8c\x90\xeb\x8b\xa4\xec\x8a\xa4\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xb6\x88\xeb\x9f\xac\xec\x99\x80 \xed\x95\x99\xec\x8a\xb5 \xec\x85\x8b\xea\xb3\xbc \xed\x8f\x89\xea\xb0\x80 \xec\x85\x8b\xec\x9c\xbc\xeb\xa1\x9c\n# \xeb\x82\x98\xeb\x88\x84\xec\x96\xb4 \xea\xb7\xb8 \xea\xb0\x92\xec\x9d\x84 \xeb\xa6\xac\xed\x84\xb4\xed\x95\x9c\xeb\x8b\xa4.\ndef load_data():\n    data_df = pd.read_csv(DEFINES.data_path, header=0)\n    question, answer = list(data_df[\'Q\']), list(data_df[\'A\'])\n    train_input, eval_input, train_label, eval_label = train_test_split(question, answer, test_size=0.33, random_state=42)\n    return train_input, train_label, eval_input, eval_label\n\n# Okt.morphs \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa6\x88 \xeb\x90\x9c \n# \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8 \xea\xb0\x9d\xec\xb2\xb4\xeb\xa5\xbc \xeb\xb0\x9b\xec\x95\x84 \xeb\xac\xb8\xec\x9e\x90\xec\x97\xb4\xec\x9d\x84 \xec\x9e\xac\xea\xb5\xac\xec\x84\xb1\xed\x95\xb4\xec\x84\x9c \xeb\xa6\xac\xed\x84\xb4\xed\x95\x9c\xeb\x8b\xa4.\ndef prepro_like_morphlized(data):\n    morph_analyzer = Okt()\n    result_data = list()\n    for seq in tqdm(data):\n        morphlized_seq = "" "".join(morph_analyzer.morphs(seq.replace(\' \', \'\')))\n        result_data.append(morphlized_seq)\n\n    return result_data\n\n# \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\xa9\xb0 \n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xed\x95\xa0 value\xec\x99\x80 \xed\x82\xa4\xea\xb0\x80 \xeb\x8b\xa8\xec\x96\xb4\xec\x9d\xb4\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\xb4 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb8 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xb0\x9b\xec\x95\x84\n# \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xeb\xb0\xb0\xec\x97\xb4\xea\xb3\xbc \xea\xb7\xb8 \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.  \ndef enc_processing(value, dictionary):\n    sequences_input_index = []\n    sequences_length = []\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\n    if DEFINES.tokenize_as_morph:\n        value = prepro_like_morphlized(value)\n\n    for sequence in value:\n        sequence = re.sub(CHANGE_FILTER, """", sequence)\n        sequence_index = []\n        # \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xeb\x8b\xa8\xec\x9c\x84\xeb\xa1\x9c \xec\x9e\x90\xeb\xa5\xb4\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\n        for word in sequence.split():\n            # \xec\x9e\x98\xeb\xa0\xa4\xec\xa7\x84 \xeb\x8b\xa8\xec\x96\xb4\xeb\x93\xa4\xec\x9d\xb4 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x97\x90 \xec\xa1\xb4\xec\x9e\xac \xed\x95\x98\xeb\x8a\x94\xec\xa7\x80 \xeb\xb3\xb4\xea\xb3\xa0 \n            # \xea\xb7\xb8 \xea\xb0\x92\xec\x9d\x84 \xea\xb0\x80\xec\xa0\xb8\xec\x99\x80 sequence_index\xec\x97\x90 \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\n            if dictionary.get(word) is not None:\n                sequence_index.extend([dictionary[word]])\n            # \xec\x9e\x98\xeb\xa0\xa4\xec\xa7\x84 \xeb\x8b\xa8\xec\x96\xb4\xea\xb0\x80 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x97\x90 \xec\xa1\xb4\xec\x9e\xac \xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94 \n            # \xea\xb2\xbd\xec\x9a\xb0 \xec\x9d\xb4\xeb\xaf\x80\xeb\xa1\x9c UNK(2)\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n            else:\n                sequence_index.extend([dictionary[UNK]])\n        \n        # \xeb\xac\xb8\xec\x9e\xa5 \xec\xa0\x9c\xed\x95\x9c \xea\xb8\xb8\xec\x9d\xb4\xeb\xb3\xb4\xeb\x8b\xa4 \xea\xb8\xb8\xec\x96\xb4\xec\xa7\x88 \xea\xb2\xbd\xec\x9a\xb0 \xeb\x92\xa4\xec\x97\x90 \xed\x86\xa0\xed\x81\xb0\xec\x9d\x84 \xec\x9e\x90\xeb\xa5\xb4\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\n        if len(sequence_index) > DEFINES.max_sequence_length:\n            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n\n        sequences_length.append(len(sequence_index))\n        # max_sequence_length\xeb\xb3\xb4\xeb\x8b\xa4 \xeb\xac\xb8\xec\x9e\xa5 \xea\xb8\xb8\xec\x9d\xb4\xea\xb0\x80 \xec\x9e\x91\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xb9\x88 \xeb\xb6\x80\xeb\xb6\x84\xec\x97\x90 PAD(0)\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4\xec\xa4\x80\xeb\x8b\xa4.\n        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n        sequences_input_index.append(sequence_index)\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xec\x9d\xbc\xeb\xb0\x98 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x9c\xeb\x8b\xa4. \n    # \xec\x9d\xb4\xec\x9c\xa0\xeb\x8a\x94 \xed\x85\x90\xec\x84\x9c\xed\x94\x8c\xeb\xa1\x9c\xec\x9a\xb0 dataset\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa3\xbc\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c \xec\x82\xac\xec\xa0\x84 \xec\x9e\x91\xec\x97\x85\xec\x9d\xb4\xeb\x8b\xa4.\n    return np.asarray(sequences_input_index), sequences_length\n\n\n# \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\ndef dec_input_processing(value, dictionary):\n    sequences_output_index = []\n    sequences_length = []\n\n    if DEFINES.tokenize_as_morph:\n        value = prepro_like_morphlized(value)\n\n    for sequence in value:\n        sequence = re.sub(CHANGE_FILTER, """", sequence)\n        sequence_index = []\n        # \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5\xec\x9d\x98 \xec\xb2\x98\xec\x9d\x8c\xec\x97\x90\xeb\x8a\x94 START\xea\xb0\x80 \xec\x99\x80\xec\x95\xbc \xed\x95\x98\xeb\xaf\x80\xeb\xa1\x9c \n        # \xea\xb7\xb8 \xea\xb0\x92\xec\x9d\x84 \xeb\x84\xa3\xec\x96\xb4 \xec\xa3\xbc\xea\xb3\xa0 \xec\x8b\x9c\xec\x9e\x91\xed\x95\x9c\xeb\x8b\xa4.\n        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n\n        if len(sequence_index) > DEFINES.max_sequence_length:\n            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n        sequences_length.append(len(sequence_index))\n        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n        sequences_output_index.append(sequence_index)\n\n    return np.asarray(sequences_output_index), sequences_length\n\n\n# \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\ndef dec_target_processing(value, dictionary):\n    sequences_target_index = []\n\n    if DEFINES.tokenize_as_morph:\n        value = prepro_like_morphlized(value)\n    for sequence in value:\n        sequence = re.sub(CHANGE_FILTER, """", sequence)\n        # \xeb\xac\xb8\xec\x9e\xa5\xec\x97\x90\xec\x84\x9c \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xeb\x8b\xa8\xec\x9c\x84\xeb\xb3\x84\xeb\xa1\x9c \xeb\x8b\xa8\xec\x96\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa0\xb8\xec\x99\x80\xec\x84\x9c \n        # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x9d\x98 \xea\xb0\x92\xec\x9d\xb8 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n        # \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5\xec\x9d\x98 \xeb\xa7\x88\xec\xa7\x80\xeb\xa7\x89\xec\x97\x90 END\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n        sequence_index = [dictionary[word] for word in sequence.split()]\n        # \xeb\xac\xb8\xec\x9e\xa5 \xec\xa0\x9c\xed\x95\x9c \xea\xb8\xb8\xec\x9d\xb4\xeb\xb3\xb4\xeb\x8b\xa4 \xea\xb8\xb8\xec\x96\xb4\xec\xa7\x88 \xea\xb2\xbd\xec\x9a\xb0 \xeb\x92\xa4\xec\x97\x90 \xed\x86\xa0\xed\x81\xb0\xec\x9d\x84 \xec\x9e\x90\xeb\xa5\xb4\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\n        # \xea\xb7\xb8\xeb\xa6\xac\xea\xb3\xa0 END \xed\x86\xa0\xed\x81\xb0\xec\x9d\x84 \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4\n        if len(sequence_index) >= DEFINES.max_sequence_length:\n            sequence_index = sequence_index[:DEFINES.max_sequence_length-1] + [dictionary[END]]\n        else:\n            sequence_index += [dictionary[END]]\n        # max_sequence_length\xeb\xb3\xb4\xeb\x8b\xa4 \xeb\xac\xb8\xec\x9e\xa5 \xea\xb8\xb8\xec\x9d\xb4\xea\xb0\x80 \n\n        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n        sequences_target_index.append(sequence_index)\n\n    return np.asarray(sequences_target_index)\n\n\n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc \xec\x8a\xa4\xed\x8a\xb8\xeb\xa7\x81\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\ndef pred2string(value, dictionary):\n    sentence_string = []\n    for v in value:\n        # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x97\x90 \xec\x9e\x88\xeb\x8a\x94 \xeb\x8b\xa8\xec\x96\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\xb4\xec\x84\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xeb\x8b\xb4\xeb\x8a\x94\xeb\x8b\xa4.\n        sentence_string = [dictionary[index] for index in v[\'indexs\']]\n\n    print(sentence_string)\n    answer = """"\n    # \xed\x8c\xa8\xeb\x94\xa9\xea\xb0\x92\xea\xb3\xbc \xec\x97\x94\xeb\x93\x9c\xea\xb0\x92\xec\x9d\xb4 \xeb\x8b\xb4\xea\xb2\xa8 \xec\x9e\x88\xec\x9c\xbc\xeb\xaf\x80\xeb\xa1\x9c \xed\x8c\xa8\xeb\x94\xa9\xec\x9d\x80 \xeb\xaa\xa8\xeb\x91\x90 \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xec\xb2\x98\xeb\xa6\xac \xed\x95\x9c\xeb\x8b\xa4.\n    for word in sentence_string:\n        if word not in PAD and word not in END:\n            answer += word\n            answer += "" ""\n\n    print(answer)\n    return answer\n\n# \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb0\x81 \xec\x9a\x94\xec\x86\x8c\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4\xec\x84\x9c rearrange \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc \n# \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xec\x9a\x94\xec\x86\x8c\xeb\xa5\xbc \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xa7\xb5\xec\x9c\xbc\xeb\xa1\x9c \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\ndef rearrange(input, output, target):\n    features = {""input"": input, ""output"": output}\n    return features, target\n\n\n# \xed\x95\x99\xec\x8a\xb5\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4\xea\xb0\x80 \xeb\xb0\xb0\xec\xb9\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\ndef train_input_fn(train_input_enc, train_output_dec, train_target_dec, batch_size):\n    # Dataset\xec\x9d\x84 \xec\x83\x9d\xec\x84\xb1\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9c\xbc\xeb\xa1\x9c\xec\x8d\xa8 from_tensor_slices\xeb\xb6\x80\xeb\xb6\x84\xec\x9d\x80 \n    # \xea\xb0\x81\xea\xb0\x81 \xed\x95\x9c \xeb\xac\xb8\xec\x9e\xa5\xec\x9c\xbc\xeb\xa1\x9c \xec\x9e\x90\xeb\xa5\xb8\xeb\x8b\xa4\xea\xb3\xa0 \xeb\xb3\xb4\xeb\xa9\xb4 \xeb\x90\x9c\xeb\x8b\xa4.\n    # train_input_enc, train_output_dec, train_target_dec \n    # 3\xea\xb0\x9c\xeb\xa5\xbc \xea\xb0\x81\xea\xb0\x81 \xed\x95\x9c\xeb\xac\xb8\xec\x9e\xa5\xec\x9c\xbc\xeb\xa1\x9c \xeb\x82\x98\xeb\x88\x88\xeb\x8b\xa4.\n    dataset = tf.data.Dataset.from_tensor_slices((train_input_enc, train_output_dec, train_target_dec))\n    dataset = dataset.shuffle(buffer_size=len(train_input_enc))\n    # \xeb\xb0\xb0\xec\xb9\x98 \xec\x9d\xb8\xec\x9e\x90 \xea\xb0\x92\xec\x9d\xb4 \xec\x97\x86\xeb\x8b\xa4\xeb\xa9\xb4  \xec\x97\x90\xeb\x9f\xac\xeb\xa5\xbc \xeb\xb0\x9c\xec\x83\x9d \xec\x8b\x9c\xed\x82\xa8\xeb\x8b\xa4.\n    assert batch_size is not None, ""train batchSize must not be None""\n    # from_tensor_slices\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x82\x98\xeb\x88\x88\xea\xb2\x83\xec\x9d\x84 \xeb\xb0\xb0\xec\xb9\x98\xed\x81\xac\xea\xb8\xb0 \xeb\xa7\x8c\xed\x81\xbc \xeb\xac\xb6\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(rearrange)\n    # repeat()\xed\x95\xa8\xec\x88\x98\xec\x97\x90 \xec\x9b\x90\xed\x95\x98\xeb\x8a\x94 \xec\x97\x90\xed\x8f\xac\xed\x81\xac \xec\x88\x98\xeb\xa5\xbc \xeb\x84\xa3\xec\x9d\x84\xec\x88\x98 \xec\x9e\x88\xec\x9c\xbc\xeb\xa9\xb4 \n    # \xec\x95\x84\xeb\xac\xb4 \xec\x9d\xb8\xec\x9e\x90\xeb\x8f\x84 \xec\x97\x86\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xac\xb4\xed\x95\x9c\xec\x9c\xbc\xeb\xa1\x9c \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0 \xeb\x90\x9c\xeb\x8b\xa4.\n    dataset = dataset.repeat()\n    iterator = dataset.make_one_shot_iterator()\n    # \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x8b\xa4\xec\x9d\x8c \xed\x95\xad\xeb\xaa\xa9\xec\x9d\x98 \xed\x85\x90\xec\x84\x9c \xea\xb0\x9c\xec\xb2\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\n    return iterator.get_next()\n\n\n# \xed\x8f\x89\xea\xb0\x80\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4\xea\xb0\x80 \xeb\xb0\xb0\xec\xb9\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\ndef eval_input_fn(eval_input_enc, eval_output_dec, eval_target_dec, batch_size):\n    dataset = tf.data.Dataset.from_tensor_slices((eval_input_enc, eval_output_dec, eval_target_dec))\n    # \xec\xa0\x84\xec\xb2\xb4 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xec\x84\x9e\xeb\x8a\x94\xeb\x8b\xa4.\n    dataset = dataset.shuffle(buffer_size=len(eval_input_enc))\n    assert batch_size is not None, ""eval batchSize must not be None""\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(rearrange)\n    # \xed\x8f\x89\xea\xb0\x80\xec\x9d\xb4\xeb\xaf\x80\xeb\xa1\x9c 1\xed\x9a\x8c\xeb\xa7\x8c \xeb\x8f\x99\xec\x9e\x91 \xec\x8b\x9c\xed\x82\xa8\xeb\x8b\xa4.\n    dataset = dataset.repeat(1)\n    iterator = dataset.make_one_shot_iterator()\n    return iterator.get_next()\n\n# \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xed\x95\xb4\xec\x84\x9c \xeb\x8b\xb4\xec\x9d\x84 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xec\x83\x9d\xec\x84\xb1\xed\x95\x98\xea\xb3\xa0 \n# \xed\x86\xa0\xea\xb7\xb8\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95\xea\xb3\xbc \xec\xa0\x95\xea\xb7\x9c\xed\x91\x9c\xed\x98\x84\xec\x8b\x9d\xec\x9d\x84 \xed\x86\xb5\xed\x95\xb4 \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4\xec\xa7\x84 \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xeb\x84\x98\xea\xb2\xa8 \xec\xa4\x80\xeb\x8b\xa4.\ndef data_tokenizer(data):\n    words = []\n    for sentence in data:\n        sentence = re.sub(CHANGE_FILTER, """", sentence)\n        for word in sentence.split():\n            words.append(word)\n    return [word for word in words if word]\n\n# \xec\xb5\x9c\xec\xb4\x88 \xec\x82\xac\xec\xa0\x84 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\xa9\xb0 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\xb4 \xec\xa1\xb4\xec\x9e\xac \xed\x95\x9c\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xb6\x88\xeb\x9f\xac\xec\x98\xa4\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\ndef load_vocabulary():\n    vocabulary_list = []\n    # \xec\x82\xac\xec\xa0\x84 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x98 \xec\xa1\xb4\xec\x9e\xac \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xed\x99\x95\xec\x9d\xb8\xed\x95\x9c\xeb\x8b\xa4.\n    if (not (os.path.exists(DEFINES.vocabulary_path))):\n        if (os.path.exists(DEFINES.data_path)):\n            data_df = pd.read_csv(DEFINES.data_path, encoding=\'utf-8\')\n            question, answer = list(data_df[\'Q\']), list(data_df[\'A\'])\n            if DEFINES.tokenize_as_morph:  \n                question = prepro_like_morphlized(question)\n                answer = prepro_like_morphlized(answer)\n            data = []\n            data.extend(question)\n            data.extend(answer)\n            words = data_tokenizer(data)\n            words = list(set(words))\n            # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\x97\x86\xeb\x8a\x94 \xeb\x82\xb4\xec\x9a\xa9\xec\xa4\x91\xec\x97\x90 MARKER\xeb\xa5\xbc \xec\x82\xac\xec\xa0\x84\xec\x97\x90 \n            # \xec\xb6\x94\xea\xb0\x80 \xed\x95\x98\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c \xec\x95\x84\xeb\x9e\x98\xec\x99\x80 \xea\xb0\x99\xec\x9d\xb4 \xec\xb2\x98\xeb\xa6\xac \xed\x95\x9c\xeb\x8b\xa4.\n            # \xec\x95\x84\xeb\x9e\x98\xeb\x8a\x94 MARKER \xea\xb0\x92\xec\x9d\xb4\xeb\xa9\xb0 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xec\x9d\x98 \xec\xb2\xab\xeb\xb2\x88\xec\xa7\xb8 \xeb\xb6\x80\xed\x84\xb0 \n            # \xec\x88\x9c\xec\x84\x9c\xeb\x8c\x80\xeb\xa1\x9c \xeb\x84\xa3\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 0\xec\x97\x90 \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\n            # PAD = ""<PADDING>""\n            # STD = ""<START>""\n            # END = ""<END>""\n            # UNK = ""<UNKNOWN>""     \n            words[:0] = MARKER\n        # \xec\x82\xac\xec\xa0\x84 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xec\x82\xac\xec\xa0\x84 \xed\x8c\x8c\xec\x9d\xbc\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.\n        with open(DEFINES.vocabulary_path, \'w\', encoding=\'utf-8\') as vocabulary_file:\n            for word in words:\n                vocabulary_file.write(word + \'\\n\')\n\n    # \xec\x82\xac\xec\xa0\x84 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\xb4 \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xeb\xa9\xb4 \xec\x97\xac\xea\xb8\xb0\xec\x97\x90\xec\x84\x9c \xea\xb7\xb8 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xeb\xb6\x88\xeb\x9f\xac\xec\x84\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n    with open(DEFINES.vocabulary_path, \'r\', encoding=\'utf-8\') as vocabulary_file:\n        for line in vocabulary_file:\n            vocabulary_list.append(line.strip())\n\n    word2idx, idx2word = make_vocabulary(vocabulary_list)\n    # \xeb\x91\x90\xea\xb0\x80\xec\xa7\x80 \xed\x98\x95\xed\x83\x9c\xec\x9d\x98 \xed\x82\xa4\xec\x99\x80 \xea\xb0\x92\xec\x9d\xb4 \xec\x9e\x88\xeb\x8a\x94 \xed\x98\x95\xed\x83\x9c\xeb\xa5\xbc \xeb\xa6\xac\xed\x84\xb4\xed\x95\x9c\xeb\x8b\xa4. \n    # (\xec\x98\x88) \xeb\x8b\xa8\xec\x96\xb4: \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 , \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4: \xeb\x8b\xa8\xec\x96\xb4)\n    return word2idx, idx2word, len(word2idx)\n\n# \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xed\x82\xa4\xea\xb0\x80 \xeb\x8b\xa8\xec\x96\xb4\xec\x9d\xb4\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\xb4 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb8 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n# \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xed\x82\xa4\xea\xb0\x80 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb4\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\xb4 \xeb\x8b\xa8\xec\x96\xb4\xec\x9d\xb8 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\ndef make_vocabulary(vocabulary_list):\n    word2idx = {word: idx for idx, word in enumerate(vocabulary_list)}\n    idx2word = {idx: word for idx, word in enumerate(vocabulary_list)}\n    return word2idx, idx2word\n\n\ndef main(self):\n    char2idx, idx2char, vocabulary_length = load_vocabulary()\n\n\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run(main)\n'"
6.CHATBOT/6.3 seq2seq/main.py,0,"b'import tensorflow as tf\nimport model as ml\nimport data\nimport numpy as np\nimport os\nimport sys\n\nfrom configs import DEFINES\n\nDATA_OUT_PATH = \'./data_out/\'\ndef main(self):\n    data_out_path = os.path.join(os.getcwd(), DATA_OUT_PATH)\n    os.makedirs(data_out_path, exist_ok=True)\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xed\x86\xb5\xed\x95\x9c \xec\x82\xac\xec\xa0\x84 \xea\xb5\xac\xec\x84\xb1 \xed\x95\x9c\xeb\x8b\xa4.\n    word2idx,  idx2word, vocabulary_length = data.load_vocabulary()\n\t# \xed\x9b\x88\xeb\xa0\xa8 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xec\x99\x80 \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xea\xb0\x80\xec\xa0\xb8\xec\x98\xa8\xeb\x8b\xa4.\n    train_input, train_label, eval_input, eval_label = data.load_data()\n\n\t# \xed\x9b\x88\xeb\xa0\xa8\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 / \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5 / \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    train_input_enc, train_input_enc_length = data.enc_processing(train_input, word2idx)\n    train_input_dec, train_input_dec_length = data.dec_input_processing(train_label, word2idx)\n    train_target_dec = data.dec_target_processing(train_label, word2idx)\n\t\n\t# \xed\x8f\x89\xea\xb0\x80\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 / \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5 / \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    eval_input_enc, eval_input_enc_length = data.enc_processing(eval_input,word2idx)\n    eval_input_dec, eval_input_dec_length = data.dec_input_processing(eval_label, word2idx)\n    eval_target_dec = data.dec_target_processing(eval_label, word2idx)\n\n    # \xed\x98\x84\xec\x9e\xac \xea\xb2\xbd\xeb\xa1\x9c\'./\'\xec\x97\x90 \xed\x98\x84\xec\x9e\xac \xea\xb2\xbd\xeb\xa1\x9c \xed\x95\x98\xeb\xb6\x80\xec\x97\x90 \xec\xb2\xb4\xed\x81\xac \xed\x8f\xac\xec\x9d\xb8\xed\x8a\xb8\xeb\xa5\xbc \xec\xa0\x80\xec\x9e\xa5\xed\x95\x9c \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n    check_point_path = os.path.join(os.getcwd(), DEFINES.check_point_path)\n    os.makedirs(check_point_path, exist_ok=True)\n\n\t# \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0 \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\n    classifier = tf.estimator.Estimator(\n            model_fn=ml.model, # \xeb\xaa\xa8\xeb\x8d\xb8 \xeb\x93\xb1\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\n            model_dir=DEFINES.check_point_path, \n            params={\n                \'hidden_size\': DEFINES.hidden_size, \n                \'layer_size\': DEFINES.layer_size, \n                \'learning_rate\': DEFINES.learning_rate, \n                \'vocabulary_length\': vocabulary_length, \n                \'embedding_size\': DEFINES.embedding_size, \n                \'embedding\': DEFINES.embedding, \n                \'multilayer\': DEFINES.multilayer,\n            })\n\n\t# \xed\x95\x99\xec\x8a\xb5 \xec\x8b\xa4\xed\x96\x89\n    classifier.train(input_fn=lambda:data.train_input_fn(\n        train_input_enc, train_input_dec, train_target_dec,  DEFINES.batch_size), steps=DEFINES.train_steps)\n    \n    # \xed\x8f\x89\xea\xb0\x80 \xec\x8b\xa4\xed\x96\x89\n    eval_result = classifier.evaluate(input_fn=lambda:data.eval_input_fn(\n        eval_input_enc, eval_input_dec, eval_target_dec,  DEFINES.batch_size))\n    print(\'\\nEVAL set accuracy: {accuracy:0.3f}\\n\'.format(**eval_result))\n\n\n\t# \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 / \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5 / \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    predic_input_enc, predic_input_enc_length = data.enc_processing([""\xea\xb0\x80\xeb\x81\x94 \xea\xb6\x81\xea\xb8\x88\xed\x95\xb4""], word2idx)\n    predic_input_dec, predic_input_dec_length = data.dec_input_processing([""""], word2idx)       \n    predic_target_dec = data.dec_target_processing([""""], word2idx)      \n\n    # \xec\x98\x88\xec\xb8\xa1 \xec\x8b\xa4\xed\x96\x89\n    predictions = classifier.predict(\n        input_fn=lambda:data.eval_input_fn(predic_input_enc, predic_input_dec, predic_target_dec, DEFINES.batch_size))\n    \n    # \xec\x98\x88\xec\xb8\xa1\xed\x95\x9c \xea\xb0\x92\xec\x9d\x84 \xed\x85\x8d\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    data.pred2string(predictions, idx2word)\n\t\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run(main)\n \ntf.logging.set_verbosity\n\n'"
6.CHATBOT/6.3 seq2seq/model.py,0,"b'#-*- coding: utf-8 -*-\nimport tensorflow as tf\nimport sys\n\nfrom configs import DEFINES\n\n# \xec\x97\x98\xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xec\x97\xa0(LSTM) \xeb\x8b\xa8\xec\xb8\xb5 \xeb\x84\xa4\xed\x8a\xb8\xec\x9b\x8c\xed\x81\xac \xea\xb5\xac\xec\x84\xb1\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\ndef make_lstm_cell(mode, hiddenSize, index):\n    cell = tf.nn.rnn_cell.BasicLSTMCell(hiddenSize, name = ""lstm""+str(index))\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=DEFINES.dropout_width)\n    return cell\n\n# \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0 \xeb\xaa\xa8\xeb\x8d\xb8 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\ndef model(features, labels, mode, params):\n    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n    EVAL = mode == tf.estimator.ModeKeys.EVAL\n    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n    # \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xb6\x80\xeb\xb6\x84 (\xeb\xaf\xb8\xeb\xa6\xac \xec\xa0\x95\xec\x9d\x98\xeb\x90\x9c \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xeb\xb2\xa1\xed\x84\xb0 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4)\n    if params[\'embedding\'] == True:\n        # \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x96\x89\xeb\xa0\xac\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\xb4\x88\xea\xb8\xb0\xed\x99\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\n        # xavier (Xavier Glorot\xec\x99\x80 Yoshua Bengio (2010)\n        # URL : http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n        initializer = tf.contrib.layers.xavier_initializer()\n        embedding = tf.get_variable(name = ""embedding"", # \xec\x9d\xb4\xeb\xa6\x84\n                                 \t  shape=[params[\'vocabulary_length\'], params[\'embedding_size\']], #  \xeb\xaa\xa8\xec\x96\x91\n                                 \t  dtype=tf.float32, # \xed\x83\x80\xec\x9e\x85\n                                 \t  initializer=initializer, # \xec\xb4\x88\xea\xb8\xb0\xed\x99\x94 \xea\xb0\x92\n                                 \t  trainable=True) # \xed\x95\x99\xec\x8a\xb5 \xec\x9c\xa0\xeb\xac\xb4\n    else:   \n        # tf.eye\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xec\x82\xac\xec\xa0\x84\xec\x9d\x98 \xed\x81\xac\xea\xb8\xb0 \xeb\xa7\x8c\xed\x81\xbc\xec\x9d\x98 \xeb\x8b\xa8\xec\x9c\x84\xed\x96\x89\xeb\xa0\xac \n        # \xea\xb5\xac\xec\xa1\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n        embedding = tf.eye(num_rows = params[\'vocabulary_length\'], dtype = tf.float32)\n        embedding = tf.get_variable(name = ""embedding"", # \xec\x9d\xb4\xeb\xa6\x84\n                                            initializer = embedding, # \xec\xb4\x88\xea\xb8\xb0\xed\x99\x94 \xea\xb0\x92\n                                            trainable = False) # \xed\x95\x99\xec\x8a\xb5 \xec\x9c\xa0\xeb\xac\xb4\n\n    # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9\xeb\x90\x9c \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xb0\xb0\xec\xb9\x98\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n    embedding_encoder = tf.nn.embedding_lookup(params = embedding, ids = features[\'input\'])\n\n    # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9\xeb\x90\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xeb\xb0\xb0\xec\xb9\x98\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n    embedding_decoder = tf.nn.embedding_lookup(params = embedding, ids = features[\'output\'])\n\n    with tf.variable_scope(\'encoder_scope\', reuse=tf.AUTO_REUSE):\n        # \xea\xb0\x92\xec\x9d\xb4 True\xec\x9d\xb4\xeb\xa9\xb4 \xeb\xa9\x80\xed\x8b\xb0\xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4\xeb\xa1\x9c \xeb\xaa\xa8\xeb\x8d\xb8\xec\x9d\x84 \xea\xb5\xac\xec\x84\xb1\xed\x95\x98\xea\xb3\xa0 False\xec\x9d\xb4\xeb\xa9\xb4 \n        # \xeb\x8b\xa8\xec\x9d\xbc\xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4\xeb\xa1\x9c \xeb\xaa\xa8\xeb\x8d\xb8\xec\x9d\x84 \xea\xb5\xac\xec\x84\xb1 \xed\x95\x9c\xeb\x8b\xa4.\n        if params[\'multilayer\'] == True:\n            encoder_cell_list = [make_lstm_cell(mode, params[\'hidden_size\'], i) for i in range(params[\'layer_size\'])]\n            rnn_cell = tf.contrib.rnn.MultiRNNCell(encoder_cell_list)\n        else:\n            rnn_cell = make_lstm_cell(mode, params[\'hidden_size\'], """")\n        \n        # rnn_cell\xec\x97\x90 \xec\x9d\x98\xed\x95\xb4 \xec\xa7\x80\xec\xa0\x95\xeb\x90\x9c dynamic_rnn \xeb\xb0\x98\xeb\xb3\xb5\xec\xa0\x81\xec\x9d\xb8 \xec\x8b\xa0\xea\xb2\xbd\xeb\xa7\x9d\xec\x9d\x84 \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4. \n        # encoder_states \xec\xb5\x9c\xec\xa2\x85 \xec\x83\x81\xed\x83\x9c  [batch_size, cell.state_size]\n        encoder_outputs, encoder_states = tf.nn.dynamic_rnn(cell=rnn_cell, # RNN \xec\x85\x80\n                                                                inputs=embedding_encoder, # \xec\x9e\x85\xeb\xa0\xa5 \xea\xb0\x92\n                                                                dtype=tf.float32) # \xed\x83\x80\xec\x9e\x85\n\n    with tf.variable_scope(\'decoder_scope\', reuse=tf.AUTO_REUSE):\n        if params[\'multilayer\'] == True:\n            decoder_cell_list = [make_lstm_cell(mode, params[\'hidden_size\'], i) for i in range(params[\'layer_size\'])]\n            rnn_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list)\n        else:\n            rnn_cell = make_lstm_cell(mode, params[\'hidden_size\'], """")\n\n        decoder_initial_state = encoder_states\n        decoder_outputs, decoder_states = tf.nn.dynamic_rnn(cell=rnn_cell, # RNN \xec\x85\x80\n                       inputs=embedding_decoder, # \xec\x9e\x85\xeb\xa0\xa5 \xea\xb0\x92\n                       initial_state=decoder_initial_state, # \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9\xec\x9d\x98 \xeb\xa7\x88\xec\xa7\x80\xeb\xa7\x89 \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c \xec\xb4\x88\xea\xb8\xb0\xed\x99\x94\n                       dtype=tf.float32) # \xed\x83\x80\xec\x9e\x85\n\n\n    # logits\xeb\x8a\x94 \xeb\xa7\x88\xec\xa7\x80\xeb\xa7\x89 \xed\x9e\x88\xeb\x93\xa0\xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4\xeb\xa5\xbc \xed\x86\xb5\xea\xb3\xbc\xed\x95\x9c \xea\xb2\xb0\xea\xb3\xbc\xea\xb0\x92\xec\x9d\xb4\xeb\x8b\xa4.\n    logits = tf.layers.dense(decoder_outputs, params[\'vocabulary_length\'], activation=None)\n\n\t# argmax\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xec\xb5\x9c\xeb\x8c\x80 \xea\xb0\x92\xec\x9d\x84 \xea\xb0\x80\xec\xa0\xb8 \xec\x98\xa8\xeb\x8b\xa4.\n    predict = tf.argmax(logits, 2)\n\n    if PREDICT:\n        predictions = { # \xec\x98\x88\xec\xb8\xa1 \xea\xb0\x92\xeb\x93\xa4\xec\x9d\xb4 \xec\x97\xac\xea\xb8\xb0\xec\x97\x90 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac \xed\x98\x95\xed\x83\x9c\xeb\xa1\x9c \xeb\x8b\xb4\xea\xb8\xb4\xeb\x8b\xa4.\n            \'indexs\': predict, # \xec\x8b\x9c\xed\x80\x80\xec\x8a\xa4 \xeb\xa7\x88\xeb\x8b\xa4 \xec\x98\x88\xec\xb8\xa1\xed\x95\x9c \xea\xb0\x92\n        }\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n    \n    #  \n    # logits\xea\xb3\xbc \xea\xb0\x99\xec\x9d\x80 \xec\xb0\xa8\xec\x9b\x90\xec\x9d\x84 \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xeb\xa7\x88\xec\xa7\x80\xeb\xa7\x89 \xea\xb2\xb0\xea\xb3\xbc \xea\xb0\x92\xea\xb3\xbc \xec\xa0\x95\xeb\x8b\xb5 \xea\xb0\x92\xec\x9d\x84 \xeb\xb9\x84\xea\xb5\x90\xed\x95\x98\xec\x97\xac \xec\x97\x90\xeb\x9f\xac\xeb\xa5\xbc \xea\xb5\xac\xed\x95\x9c\xeb\x8b\xa4.\n    labels_ = tf.one_hot(labels, params[\'vocabulary_length\'])\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_))\n    # \xeb\x9d\xbc\xeb\xb2\xa8\xea\xb3\xbc \xea\xb2\xb0\xea\xb3\xbc\xea\xb0\x80 \xec\x9d\xbc\xec\xb9\x98\xed\x95\x98\xeb\x8a\x94\xec\xa7\x80 \xeb\xb9\x88\xeb\x8f\x84 \xea\xb3\x84\xec\x82\xb0\xec\x9d\x84 \xed\x86\xb5\xed\x95\xb4 \xec\xa0\x95\xed\x99\x95\xeb\x8f\x84\xeb\xa5\xbc \xec\xb8\xa1\xec\xa0\x95\xed\x95\x98\xeb\x8a\x94 \xeb\xb0\xa9\xeb\xb2\x95\xec\x9d\xb4\xeb\x8b\xa4.\n    accuracy = tf.metrics.accuracy(labels=labels, predictions=predict,name=\'accOp\')\n\n    # accuracy\xeb\xa5\xbc \xec\xa0\x84\xec\xb2\xb4 \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c \xeb\x82\x98\xeb\x88\xa0 \xed\x99\x95\xeb\xa5\xa0 \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c \xed\x95\x9c\xeb\x8b\xa4.\n    metrics = {\'accuracy\': accuracy}\n    tf.summary.scalar(\'accuracy\', accuracy[1])\n    \n    if EVAL:\n        # \xec\x97\x90\xeb\x9f\xac \xea\xb0\x92(loss)\xea\xb3\xbc \xec\xa0\x95\xed\x99\x95\xeb\x8f\x84 \xea\xb0\x92(eval_metric_ops) \xec\xa0\x84\xeb\x8b\xac\n        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n\n    # \xec\x88\x98\xed\x96\x89 mode(tf.estimator.ModeKeys.TRAIN)\xea\xb0\x80 \n    # \xec\x95\x84\xeb\x8b\x8c \xea\xb2\xbd\xec\x9a\xb0\xeb\x8a\x94 \xec\x97\xac\xea\xb8\xb0 \xea\xb9\x8c\xec\xa7\x80 \xec\x98\xa4\xeb\xa9\xb4 \xec\x95\x88\xeb\x90\x98\xeb\x8f\x84\xeb\xa1\x9d \xeb\xb0\xa9\xec\x96\xb4\xec\xa0\x81 \xec\xbd\x94\xeb\x93\x9c\xeb\xa5\xbc \xeb\x84\xa3\xec\x9d\x80\xea\xb2\x83\xec\x9d\xb4\xeb\x8b\xa4.\n    assert TRAIN\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=DEFINES.learning_rate)\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())  \n\n    # \xec\x97\x90\xeb\x9f\xac \xea\xb0\x92(loss)\xea\xb3\xbc \xea\xb7\xb8\xeb\x9d\xbc\xeb\x94\x94\xec\x96\xb8\xed\x8a\xb8 \xeb\xb0\x98\xed\x99\x98\xea\xb0\x92 (train_op) \xec\xa0\x84\xeb\x8b\xac\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n'"
6.CHATBOT/6.3 seq2seq/predict.py,0,"b'import tensorflow as tf\nimport data\nimport sys\nimport model as ml\n\nfrom configs import DEFINES\n\t\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    arg_length = len(sys.argv)\n    \n    if(arg_length < 2):\n        raise Exception(""Don\'t call us. We\'ll call you"")\n  \n    \n    char2idx,  idx2char, vocabulary_length = data.load_vocabulary()\n    input = """"\n    for i in sys.argv[1:]:\n        input += i \n        input += "" ""\n        \n    print(input)\n    # \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 / \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5 / \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    predic_input_enc, predic_input_enc_length = data.enc_processing([input], char2idx)\n    predic_output_dec, predic_output_dec_length = data.dec_input_processing([""""], char2idx)\n    predic_target_dec = data.dec_target_processing([""""], char2idx)\n\n\t# \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0 \xea\xb5\xac\xec\x84\xb1\n    classifier = tf.estimator.Estimator(\n            model_fn=ml.model,\n            model_dir=DEFINES.check_point_path, \n            params={ \n                \'hidden_size\': DEFINES.hidden_size, \n                \'layer_size\': DEFINES.layer_size, \n                \'learning_rate\': DEFINES.learning_rate, \n                \'vocabulary_length\': vocabulary_length, \n                \'embedding_size\': DEFINES.embedding_size, \n                \'embedding\': DEFINES.embedding, \n                \'multilayer\': DEFINES.multilayer, \n            })\n\n    predictions = classifier.predict(\n        input_fn=lambda:data.eval_input_fn(predic_input_enc, predic_output_dec, predic_target_dec, DEFINES.batch_size))\n    \n    data.pred2string(predictions, idx2char)\n'"
6.CHATBOT/6.4 transformer/configs.py,0,"b""# -*- coding: utf-8 -*-\nimport tensorflow as tf\n\ntf.app.flags.DEFINE_integer('batch_size', 64, 'batch size')  # \xeb\xb0\xb0\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0\ntf.app.flags.DEFINE_integer('train_steps', 20000, 'train steps')  # \xed\x95\x99\xec\x8a\xb5 \xec\x97\x90\xed\x8f\xac\xed\x81\xac\ntf.app.flags.DEFINE_float('dropout_width', 0.5, 'dropout width')  # \xeb\x93\x9c\xeb\xa1\xad\xec\x95\x84\xec\x9b\x83 \xed\x81\xac\xea\xb8\xb0\ntf.app.flags.DEFINE_integer('embedding_size', 128, 'embedding size')  # \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0 # \xeb\x85\xbc\xeb\xac\xb8 512 \xec\x82\xac\xec\x9a\xa9\ntf.app.flags.DEFINE_float('learning_rate', 1e-3, 'learning rate')  # \xed\x95\x99\xec\x8a\xb5\xeb\xa5\xa0\ntf.app.flags.DEFINE_integer('shuffle_seek', 1000, 'shuffle random seek')  # \xec\x85\x94\xed\x94\x8c \xec\x8b\x9c\xeb\x93\x9c\xea\xb0\x92\ntf.app.flags.DEFINE_integer('max_sequence_length', 25, 'max sequence length')  # \xec\x8b\x9c\xed\x80\x80\xec\x8a\xa4 \xea\xb8\xb8\xec\x9d\xb4\ntf.app.flags.DEFINE_integer('model_hidden_size', 128, 'model weights size')  # \xeb\xaa\xa8\xeb\x8d\xb8 \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0\ntf.app.flags.DEFINE_integer('ffn_hidden_size', 512, 'ffn weights size')  # ffn \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0\ntf.app.flags.DEFINE_integer('attention_head_size', 4, 'attn head size')  # \xeb\xa9\x80\xed\x8b\xb0 \xed\x97\xa4\xeb\x93\x9c \xed\x81\xac\xea\xb8\xb0\ntf.app.flags.DEFINE_integer('layer_size', 2, 'layer size')  # \xeb\x85\xbc\xeb\xac\xb8\xec\x9d\x80 6\xea\xb0\x9c \xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4\xec\x9d\xb4\xeb\x82\x98 2\xea\xb0\x9c \xec\x82\xac\xec\x9a\xa9 \xed\x95\x99\xec\x8a\xb5 \xec\x86\x8d\xeb\x8f\x84 \xeb\xb0\x8f \xec\x84\xb1\xeb\x8a\xa5 \xed\x8a\x9c\xeb\x8b\x9d\ntf.app.flags.DEFINE_string('data_path', '../data_in/ChatBotData.csv', 'data path')  # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\x9c\x84\xec\xb9\x98\ntf.app.flags.DEFINE_string('vocabulary_path', './data_out/vocabularyData.voc', 'vocabulary path')  # \xec\x82\xac\xec\xa0\x84 \xec\x9c\x84\xec\xb9\x98\ntf.app.flags.DEFINE_string('check_point_path', './data_out/check_point', 'check point path')  # \xec\xb2\xb4\xed\x81\xac \xed\x8f\xac\xec\x9d\xb8\xed\x8a\xb8 \xec\x9c\x84\xec\xb9\x98\ntf.app.flags.DEFINE_boolean('tokenize_as_morph', False, 'set morph tokenize')  # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c\xec\x97\x90 \xeb\x94\xb0\xeb\xa5\xb8 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\ntf.app.flags.DEFINE_boolean('xavier_initializer', True, 'set xavier initializer')  # xavier initializer\xeb\xa5\xbc \xec\x82\xac\xec\x9a\xa9\xed\x95\xa0 \xea\xb2\x83\xec\x9d\xb8\xec\xa7\x80\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \n\n# Define FLAGS\nDEFINES = tf.app.flags.FLAGS\n"""
6.CHATBOT/6.4 transformer/data.py,3,"b'from konlpy.tag import Twitter\nimport pandas as pd\nimport tensorflow as tf\nimport enum\nimport os\nimport re\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom configs import DEFINES\n\nfrom tqdm import tqdm\n\nFILTERS = ""([~.,!?\\""\':;)(])""\nPAD = ""<PAD>""\nSTD = ""<SOS>""\nEND = ""<END>""\nUNK = ""<UNK>""\n\nPAD_INDEX = 0\nSTD_INDEX = 1\nEND_INDEX = 2\nUNK_INDEX = 3\n\nMARKER = [PAD, STD, END, UNK]\nCHANGE_FILTER = re.compile(FILTERS)\n\n\ndef load_data():\n    # \xed\x8c\x90\xeb\x8b\xa4\xec\x8a\xa4\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xb6\x88\xeb\x9f\xac\xec\x98\xa8\xeb\x8b\xa4.\n    data_df = pd.read_csv(DEFINES.data_path, header=0)\n    # \xec\xa7\x88\xeb\xac\xb8\xea\xb3\xbc \xeb\x8b\xb5\xeb\xb3\x80 \xec\x97\xb4\xec\x9d\x84 \xea\xb0\x80\xec\xa0\xb8\xec\x99\x80 question\xea\xb3\xbc answer\xec\x97\x90 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.\n    question, answer = list(data_df[\'Q\']), list(data_df[\'A\'])\n    # skleran\xec\x97\x90\xec\x84\x9c \xec\xa7\x80\xec\x9b\x90\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xed\x95\x99\xec\x8a\xb5 \xec\x85\x8b\xea\xb3\xbc \n    # \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8 \xec\x85\x8b\xec\x9d\x84 \xeb\x82\x98\xeb\x88\x88\xeb\x8b\xa4.\n    train_input, eval_input, train_label, eval_label = train_test_split(question, answer, test_size=0.33,\n                                                                        random_state=42)\n    # \xea\xb7\xb8 \xea\xb0\x92\xec\x9d\x84 \xeb\xa6\xac\xed\x84\xb4\xed\x95\x9c\xeb\x8b\xa4.\n    return train_input, train_label, eval_input, eval_label\n\n\ndef prepro_like_morphlized(data):\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xeb\xb6\x84\xec\x84\x9d \xeb\xaa\xa8\xeb\x93\x88 \xea\xb0\x9d\xec\xb2\xb4\xeb\xa5\xbc\n    # \xec\x83\x9d\xec\x84\xb1\xed\x95\xa9\xeb\x8b\x88\xeb\x8b\xa4.\n\n    morph_analyzer = Twitter()\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa6\x88 \xea\xb2\xb0\xea\xb3\xbc \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xeb\xb0\x9b\xec\x9d\x84\n    #  \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xec\x83\x9d\xec\x84\xb1\xed\x95\xa9\xeb\x8b\x88\xeb\x8b\xa4.\n    result_data = list()\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xec\x97\x90 \xec\x9e\x88\xeb\x8a\x94 \xeb\xa7\xa4 \xeb\xac\xb8\xec\x9e\xa5\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa6\x88\xeb\xa5\xbc\n    # \xed\x95\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8f\x84\xeb\xa1\x9d \xeb\xb0\x98\xeb\xb3\xb5\xeb\xac\xb8\xec\x9d\x84 \xec\x84\xa0\xec\x96\xb8\xed\x95\xa9\xeb\x8b\x88\xeb\x8b\xa4.\n    for seq in tqdm(data):\n        # Twitter.morphs \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa6\x88 \xeb\x90\x9c\n        # \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8 \xea\xb0\x9d\xec\xb2\xb4\xeb\xa5\xbc \xeb\xb0\x9b\xea\xb3\xa0 \xeb\x8b\xa4\xec\x8b\x9c \xea\xb3\xb5\xeb\xb0\xb1\xeb\xac\xb8\xec\x9e\x90\xeb\xa5\xbc \xea\xb8\xb0\xec\xa4\x80\xec\x9c\xbc\xeb\xa1\x9c\n        # \xed\x95\x98\xec\x97\xac \xeb\xac\xb8\xec\x9e\x90\xec\x97\xb4\xeb\xa1\x9c \xec\x9e\xac\xea\xb5\xac\xec\x84\xb1 \xed\x95\xb4\xec\xa4\x8d\xeb\x8b\x88\xeb\x8b\xa4.\n        morphlized_seq = "" "".join(morph_analyzer.morphs(seq.replace(\' \', \'\')))\n        result_data.append(morphlized_seq)\n\n    return result_data\n\n\n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xed\x95\xa0 value\xec\x99\x80 \xed\x82\xa4\xea\xb0\x80 \xec\x9b\x8c\xeb\x93\x9c\xec\x9d\xb4\xea\xb3\xa0 \n# \xea\xb0\x92\xec\x9d\xb4 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb8 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xb0\x9b\xeb\x8a\x94\xeb\x8b\xa4.\ndef enc_processing(value, dictionary):\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8a\x94 \n    # \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\xb4\xeb\x8b\xa4.(\xeb\x88\x84\xec\xa0\x81\xeb\x90\x9c\xeb\x8b\xa4.)\n    sequences_input_index = []\n    # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\x90\x98\xeb\x8a\x94 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x98 \n    # \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.(\xeb\x88\x84\xec\xa0\x81\xeb\x90\x9c\xeb\x8b\xa4.)\n    sequences_length = []\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\n    if DEFINES.tokenize_as_morph:\n        value = prepro_like_morphlized(value)\n\n    # \xed\x95\x9c\xec\xa4\x84\xec\x94\xa9 \xeb\xb6\x88\xec\x96\xb4\xec\x98\xa8\xeb\x8b\xa4.\n    for sequence in value:\n        # FILTERS = ""([~.,!?\\""\':;)(])""\n        # \xec\xa0\x95\xea\xb7\x9c\xed\x99\x94\xeb\xa5\xbc \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xec\x97\xac \xed\x95\x84\xed\x84\xb0\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94 \n        # \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 """" \xec\x9c\xbc\xeb\xa1\x9c \xec\xb9\x98\xed\x99\x98 \xed\x95\x9c\xeb\x8b\xa4.\n        sequence = re.sub(CHANGE_FILTER, """", sequence)\n        # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xed\x95\xa0\xeb\x95\x8c \n        # \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\xb4\xeb\x8b\xa4.\n        sequence_index = []\n        # \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xeb\x8b\xa8\xec\x9c\x84\xeb\xa1\x9c \n        # \xec\x9e\x90\xeb\xa5\xb4\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\n        for word in sequence.split():\n            # \xec\x9e\x98\xeb\xa0\xa4\xec\xa7\x84 \xeb\x8b\xa8\xec\x96\xb4\xeb\x93\xa4\xec\x9d\xb4 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x97\x90 \xec\xa1\xb4\xec\x9e\xac \xed\x95\x98\xeb\x8a\x94\xec\xa7\x80 \xeb\xb3\xb4\xea\xb3\xa0 \n            # \xea\xb7\xb8 \xea\xb0\x92\xec\x9d\x84 \xea\xb0\x80\xec\xa0\xb8\xec\x99\x80 sequence_index\xec\x97\x90 \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\n            if dictionary.get(word) is not None:\n                sequence_index.extend([dictionary[word]])\n            # \xec\x9e\x98\xeb\xa0\xa4\xec\xa7\x84 \xeb\x8b\xa8\xec\x96\xb4\xea\xb0\x80 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x97\x90 \xec\xa1\xb4\xec\x9e\xac \xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94 \n            # \xea\xb2\xbd\xec\x9a\xb0 \xec\x9d\xb4\xeb\xaf\x80\xeb\xa1\x9c UNK(2)\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n            else:\n                sequence_index.extend([dictionary[UNK]])\n        # \xeb\xac\xb8\xec\x9e\xa5 \xec\xa0\x9c\xed\x95\x9c \xea\xb8\xb8\xec\x9d\xb4\xeb\xb3\xb4\xeb\x8b\xa4 \xea\xb8\xb8\xec\x96\xb4\xec\xa7\x88 \xea\xb2\xbd\xec\x9a\xb0 \xeb\x92\xa4\xec\x97\x90 \xed\x86\xa0\xed\x81\xb0\xec\x9d\x84 \xec\x9e\x90\xeb\xa5\xb4\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\n        if len(sequence_index) > DEFINES.max_sequence_length:\n            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n        # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xeb\xac\xb8\xec\x9e\xa5\xec\x97\x90 \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4\xec\xa3\xbc\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\n        sequences_length.append(len(sequence_index))\n        # max_sequence_length\xeb\xb3\xb4\xeb\x8b\xa4 \xeb\xac\xb8\xec\x9e\xa5 \xea\xb8\xb8\xec\x9d\xb4\xea\xb0\x80 \n        # \xec\x9e\x91\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xb9\x88 \xeb\xb6\x80\xeb\xb6\x84\xec\x97\x90 PAD(0)\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4\xec\xa4\x80\xeb\x8b\xa4.\n        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n        # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94 \xea\xb0\x92\xec\x9d\x84 \n        # sequences_input_index\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n        sequences_input_index.append(sequence_index)\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xec\x9d\xbc\xeb\xb0\x98 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x9c\xeb\x8b\xa4. \n    # \xec\x9d\xb4\xec\x9c\xa0\xeb\x8a\x94 \xed\x85\x90\xec\x84\x9c\xed\x94\x8c\xeb\xa1\x9c\xec\x9a\xb0 dataset\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa3\xbc\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c \n    # \xec\x82\xac\xec\xa0\x84 \xec\x9e\x91\xec\x97\x85\xec\x9d\xb4\xeb\x8b\xa4.\n    # \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xeb\xb0\xb0\xec\x97\xb4\xea\xb3\xbc \n    # \xea\xb7\xb8 \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.  \n    return np.asarray(sequences_input_index), sequences_length\n\n\n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xed\x95\xa0 value \xed\x82\xa4\xea\xb0\x80 \xec\x9b\x8c\xeb\x93\x9c \xec\x9d\xb4\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\xb4 \n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb8 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xb0\x9b\xeb\x8a\x94\xeb\x8b\xa4.\ndef dec_output_processing(value, dictionary):\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8a\x94 \n    # \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\xb4\xeb\x8b\xa4.(\xeb\x88\x84\xec\xa0\x81\xeb\x90\x9c\xeb\x8b\xa4)\n    sequences_output_index = []\n    # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5 \xeb\x90\x98\xeb\x8a\x94 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x98 \n    # \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.(\xeb\x88\x84\xec\xa0\x81\xeb\x90\x9c\xeb\x8b\xa4)\n    sequences_length = []\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\n    if DEFINES.tokenize_as_morph:\n        value = prepro_like_morphlized(value)\n    # \xed\x95\x9c\xec\xa4\x84\xec\x94\xa9 \xeb\xb6\x88\xec\x96\xb4\xec\x98\xa8\xeb\x8b\xa4.\n    for sequence in value:\n        # FILTERS = ""([~.,!?\\""\':;)(])""\n        # \xec\xa0\x95\xea\xb7\x9c\xed\x99\x94\xeb\xa5\xbc \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xec\x97\xac \xed\x95\x84\xed\x84\xb0\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94 \n        # \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 """" \xec\x9c\xbc\xeb\xa1\x9c \xec\xb9\x98\xed\x99\x98 \xed\x95\x9c\xeb\x8b\xa4.\n        sequence = re.sub(CHANGE_FILTER, """", sequence)\n        # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xed\x95\xa0\xeb\x95\x8c \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \n        # \xec\x9e\x88\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\xb4\xeb\x8b\xa4.\n        sequence_index = []\n        # \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5\xec\x9d\x98 \xec\xb2\x98\xec\x9d\x8c\xec\x97\x90\xeb\x8a\x94 START\xea\xb0\x80 \xec\x99\x80\xec\x95\xbc \xed\x95\x98\xeb\xaf\x80\xeb\xa1\x9c \n        # \xea\xb7\xb8 \xea\xb0\x92\xec\x9d\x84 \xeb\x84\xa3\xec\x96\xb4 \xec\xa3\xbc\xea\xb3\xa0 \xec\x8b\x9c\xec\x9e\x91\xed\x95\x9c\xeb\x8b\xa4.\n        # \xeb\xac\xb8\xec\x9e\xa5\xec\x97\x90\xec\x84\x9c \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xeb\x8b\xa8\xec\x9c\x84\xeb\xb3\x84\xeb\xa1\x9c \xeb\x8b\xa8\xec\x96\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa0\xb8\xec\x99\x80\xec\x84\x9c \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x9d\x98 \n        # \xea\xb0\x92\xec\x9d\xb8 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n        # \xeb\xac\xb8\xec\x9e\xa5 \xec\xa0\x9c\xed\x95\x9c \xea\xb8\xb8\xec\x9d\xb4\xeb\xb3\xb4\xeb\x8b\xa4 \xea\xb8\xb8\xec\x96\xb4\xec\xa7\x88 \xea\xb2\xbd\xec\x9a\xb0 \xeb\x92\xa4\xec\x97\x90 \xed\x86\xa0\xed\x81\xb0\xec\x9d\x84 \xec\x9e\x90\xeb\xa5\xb4\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\n        if len(sequence_index) > DEFINES.max_sequence_length:\n            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n        # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xeb\xac\xb8\xec\x9e\xa5\xec\x97\x90 \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4\xec\xa3\xbc\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\n        sequences_length.append(len(sequence_index))\n        # max_sequence_length\xeb\xb3\xb4\xeb\x8b\xa4 \xeb\xac\xb8\xec\x9e\xa5 \xea\xb8\xb8\xec\x9d\xb4\xea\xb0\x80 \n        # \xec\x9e\x91\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xb9\x88 \xeb\xb6\x80\xeb\xb6\x84\xec\x97\x90 PAD(0)\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4\xec\xa4\x80\xeb\x8b\xa4.\n        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n        # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94 \xea\xb0\x92\xec\x9d\x84 \n        # sequences_output_index \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n        sequences_output_index.append(sequence_index)\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xec\x9d\xbc\xeb\xb0\x98 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x9c\xeb\x8b\xa4. \n    # \xec\x9d\xb4\xec\x9c\xa0\xeb\x8a\x94 \xed\x85\x90\xec\x84\x9c\xed\x94\x8c\xeb\xa1\x9c\xec\x9a\xb0 dataset\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa3\xbc\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c \n    # \xec\x82\xac\xec\xa0\x84 \xec\x9e\x91\xec\x97\x85\xec\x9d\xb4\xeb\x8b\xa4.\n    # \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xeb\xb0\xb0\xec\x97\xb4\xea\xb3\xbc \xea\xb7\xb8 \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\n    return np.asarray(sequences_output_index), sequences_length\n\n\n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xed\x95\xa0 value\xec\x99\x80 \xed\x82\xa4\xea\xb0\x80 \xec\x9b\x8c\xeb\x93\x9c \xec\x9d\xb4\xea\xb3\xa0\n# \xea\xb0\x92\xec\x9d\xb4 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb8 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xb0\x9b\xeb\x8a\x94\xeb\x8b\xa4.\ndef dec_target_processing(value, dictionary):\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8a\x94 \n    # \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\xb4\xeb\x8b\xa4.(\xeb\x88\x84\xec\xa0\x81\xeb\x90\x9c\xeb\x8b\xa4)\n    sequences_target_index = []\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\n    if DEFINES.tokenize_as_morph:\n        value = prepro_like_morphlized(value)\n    # \xed\x95\x9c\xec\xa4\x84\xec\x94\xa9 \xeb\xb6\x88\xec\x96\xb4\xec\x98\xa8\xeb\x8b\xa4.\n    for sequence in value:\n        # FILTERS = ""([~.,!?\\""\':;)(])""\n        # \xec\xa0\x95\xea\xb7\x9c\xed\x99\x94\xeb\xa5\xbc \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xec\x97\xac \xed\x95\x84\xed\x84\xb0\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94 \n        # \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 """" \xec\x9c\xbc\xeb\xa1\x9c \xec\xb9\x98\xed\x99\x98 \xed\x95\x9c\xeb\x8b\xa4.\n        sequence = re.sub(CHANGE_FILTER, """", sequence)\n        # \xeb\xac\xb8\xec\x9e\xa5\xec\x97\x90\xec\x84\x9c \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xeb\x8b\xa8\xec\x9c\x84\xeb\xb3\x84\xeb\xa1\x9c \xeb\x8b\xa8\xec\x96\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa0\xb8\xec\x99\x80\xec\x84\x9c \n        # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x9d\x98 \xea\xb0\x92\xec\x9d\xb8 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n        # \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5\xec\x9d\x98 \xeb\xa7\x88\xec\xa7\x80\xeb\xa7\x89\xec\x97\x90 END\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n        sequence_index = [dictionary[word] for word in sequence.split()]\n        # \xeb\xac\xb8\xec\x9e\xa5 \xec\xa0\x9c\xed\x95\x9c \xea\xb8\xb8\xec\x9d\xb4\xeb\xb3\xb4\xeb\x8b\xa4 \xea\xb8\xb8\xec\x96\xb4\xec\xa7\x88 \xea\xb2\xbd\xec\x9a\xb0 \xeb\x92\xa4\xec\x97\x90 \xed\x86\xa0\xed\x81\xb0\xec\x9d\x84 \xec\x9e\x90\xeb\xa5\xb4\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\n        # \xea\xb7\xb8\xeb\xa6\xac\xea\xb3\xa0 END \xed\x86\xa0\xed\x81\xb0\xec\x9d\x84 \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4\n        if len(sequence_index) >= DEFINES.max_sequence_length:\n            sequence_index = sequence_index[:DEFINES.max_sequence_length - 1] + [dictionary[END]]\n        else:\n            sequence_index += [dictionary[END]]\n        # max_sequence_length\xeb\xb3\xb4\xeb\x8b\xa4 \xeb\xac\xb8\xec\x9e\xa5 \xea\xb8\xb8\xec\x9d\xb4\xea\xb0\x80 \n        # \xec\x9e\x91\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xb9\x88 \xeb\xb6\x80\xeb\xb6\x84\xec\x97\x90 PAD(0)\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4\xec\xa4\x80\xeb\x8b\xa4.\n        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n        # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94 \xea\xb0\x92\xec\x9d\x84 \n        # sequences_target_index\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n        sequences_target_index.append(sequence_index)\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xec\x9d\xbc\xeb\xb0\x98 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x9c\xeb\x8b\xa4. \n    # \xec\x9d\xb4\xec\x9c\xa0\xeb\x8a\x94 \xed\x85\x90\xec\x84\x9c\xed\x94\x8c\xeb\xa1\x9c\xec\x9a\xb0 dataset\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa3\xbc\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c \xec\x82\xac\xec\xa0\x84 \xec\x9e\x91\xec\x97\x85\xec\x9d\xb4\xeb\x8b\xa4.\n    # \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xeb\xb0\xb0\xec\x97\xb4\xea\xb3\xbc \xea\xb7\xb8 \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\n    return np.asarray(sequences_target_index)\n\n\n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc \xec\x8a\xa4\xed\x8a\xb8\xeb\xa7\x81\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\n# \xeb\xb0\x94\xea\xbe\xb8\xea\xb3\xa0\xec\x9e\x90 \xed\x95\x98\xeb\x8a\x94 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 value\xec\x99\x80 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc \n# \xed\x82\xa4\xeb\xa1\x9c \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xea\xb3\xa0 \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c \xeb\x8b\xa8\xec\x96\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8a\x94 \n# \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xb0\x9b\xeb\x8a\x94\xeb\x8b\xa4.\ndef pred2string(value, dictionary):\n    # \xed\x85\x8d\xec\x8a\xa4\xed\x8a\xb8 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xeb\xb3\xb4\xea\xb4\x80\xed\x95\xa0 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xec\x84\xa0\xec\x96\xb8\xed\x95\x9c\xeb\x8b\xa4.\n    sentence_string = []\n    print(value)\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xeb\xb0\xb0\xec\x97\xb4 \xed\x95\x98\xeb\x82\x98\xeb\xa5\xbc \xea\xba\xbc\xeb\x82\xb4\xec\x84\x9c v\xec\x97\x90 \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\n    for v in value:\n        # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x97\x90 \xec\x9e\x88\xeb\x8a\x94 \xeb\x8b\xa8\xec\x96\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\xb4\xec\x84\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xeb\x8b\xb4\xeb\x8a\x94\xeb\x8b\xa4.\n        print(v[\'indexs\'])\n        for index in v[\'indexs\']:\n            print(index)\n        sentence_string = [dictionary[index] for index in v[\'indexs\']]\n\n    print(""***********************"")\n    print(sentence_string)\n    print(""***********************"")\n    answer = """"\n    # \xed\x8c\xa8\xeb\x94\xa9\xea\xb0\x92\xeb\x8f\x84 \xeb\x8b\xb4\xea\xb2\xa8 \xec\x9e\x88\xec\x9c\xbc\xeb\xaf\x80\xeb\xa1\x9c \xed\x8c\xa8\xeb\x94\xa9\xec\x9d\x80 \xeb\xaa\xa8\xeb\x91\x90 \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xec\xb2\x98\xeb\xa6\xac \xed\x95\x9c\xeb\x8b\xa4.\n    for word in sentence_string:\n        if word not in PAD and word not in END:\n            answer += word\n            answer += "" ""\n    # \xea\xb2\xb0\xea\xb3\xbc\xeb\xa5\xbc \xec\xb6\x9c\xeb\xa0\xa5\xed\x95\x9c\xeb\x8b\xa4.\n    print(answer)\n    return answer\n\n\ndef pred_next_string(value, dictionary):\n    # \xed\x85\x8d\xec\x8a\xa4\xed\x8a\xb8 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xeb\xb3\xb4\xea\xb4\x80\xed\x95\xa0 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xec\x84\xa0\xec\x96\xb8\xed\x95\x9c\xeb\x8b\xa4.\n    sentence_string = []\n    is_finished = False\n\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xeb\xb0\xb0\xec\x97\xb4 \xed\x95\x98\xeb\x82\x98\xeb\xa5\xbc \xea\xba\xbc\xeb\x82\xb4\xec\x84\x9c v\xec\x97\x90 \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\n    for v in value:\n        # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x97\x90 \xec\x9e\x88\xeb\x8a\x94 \xeb\x8b\xa8\xec\x96\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\xb4\xec\x84\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xeb\x8b\xb4\xeb\x8a\x94\xeb\x8b\xa4.\n        sentence_string = [dictionary[index] for index in v[\'indexs\']]\n\n    answer = """"\n    # \xed\x8c\xa8\xeb\x94\xa9\xea\xb0\x92\xeb\x8f\x84 \xeb\x8b\xb4\xea\xb2\xa8 \xec\x9e\x88\xec\x9c\xbc\xeb\xaf\x80\xeb\xa1\x9c \xed\x8c\xa8\xeb\x94\xa9\xec\x9d\x80 \xeb\xaa\xa8\xeb\x91\x90 \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xec\xb2\x98\xeb\xa6\xac \xed\x95\x9c\xeb\x8b\xa4.\n    for word in sentence_string:\n        if word == END:\n            is_finished = True\n            break\n\n        if word != PAD and word != END:\n            answer += word\n            answer += "" ""\n\n    # \xea\xb2\xb0\xea\xb3\xbc\xeb\xa5\xbc \xec\xb6\x9c\xeb\xa0\xa5\xed\x95\x9c\xeb\x8b\xa4.\n    return answer, is_finished\n\n\ndef rearrange(input, output, target):\n    features = {""input"": input, ""output"": output}\n    return features, target\n\n\n# \xed\x95\x99\xec\x8a\xb5\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4\xea\xb0\x80 \xeb\xb0\xb0\xec\xb9\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\ndef train_input_fn(train_input_enc, train_output_dec, train_target_dec, batch_size):\n    # Dataset\xec\x9d\x84 \xec\x83\x9d\xec\x84\xb1\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9c\xbc\xeb\xa1\x9c\xec\x8d\xa8 from_tensor_slices\xeb\xb6\x80\xeb\xb6\x84\xec\x9d\x80 \n    # \xea\xb0\x81\xea\xb0\x81 \xed\x95\x9c \xeb\xac\xb8\xec\x9e\xa5\xec\x9c\xbc\xeb\xa1\x9c \xec\x9e\x90\xeb\xa5\xb8\xeb\x8b\xa4\xea\xb3\xa0 \xeb\xb3\xb4\xeb\xa9\xb4 \xeb\x90\x9c\xeb\x8b\xa4.\n    # train_input_enc, train_output_dec, train_target_dec \n    # 3\xea\xb0\x9c\xeb\xa5\xbc \xea\xb0\x81\xea\xb0\x81 \xed\x95\x9c\xeb\xac\xb8\xec\x9e\xa5\xec\x9c\xbc\xeb\xa1\x9c \xeb\x82\x98\xeb\x88\x88\xeb\x8b\xa4.\n    dataset = tf.data.Dataset.from_tensor_slices((train_input_enc, train_output_dec, train_target_dec))\n    # \xec\xa0\x84\xec\xb2\xb4 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xec\x8d\xa9\xeb\x8a\x94\xeb\x8b\xa4.\n    dataset = dataset.shuffle(buffer_size=len(train_input_enc))\n    # \xeb\xb0\xb0\xec\xb9\x98 \xec\x9d\xb8\xec\x9e\x90 \xea\xb0\x92\xec\x9d\xb4 \xec\x97\x86\xeb\x8b\xa4\xeb\xa9\xb4  \xec\x97\x90\xeb\x9f\xac\xeb\xa5\xbc \xeb\xb0\x9c\xec\x83\x9d \xec\x8b\x9c\xed\x82\xa8\xeb\x8b\xa4.\n    assert batch_size is not None, ""train batchSize must not be None""\n    # from_tensor_slices\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x82\x98\xeb\x88\x88\xea\xb2\x83\xec\x9d\x84 \n    # \xeb\xb0\xb0\xec\xb9\x98\xed\x81\xac\xea\xb8\xb0 \xeb\xa7\x8c\xed\x81\xbc \xeb\xac\xb6\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb0\x81 \xec\x9a\x94\xec\x86\x8c\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4\xec\x84\x9c rearrange \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc \n    # \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xec\x9a\x94\xec\x86\x8c\xeb\xa5\xbc \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xa7\xb5\xec\x9c\xbc\xeb\xa1\x9c \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\n    dataset = dataset.map(rearrange)\n    # repeat()\xed\x95\xa8\xec\x88\x98\xec\x97\x90 \xec\x9b\x90\xed\x95\x98\xeb\x8a\x94 \xec\x97\x90\xed\x8f\xac\xed\x81\xac \xec\x88\x98\xeb\xa5\xbc \xeb\x84\xa3\xec\x9d\x84\xec\x88\x98 \xec\x9e\x88\xec\x9c\xbc\xeb\xa9\xb4 \n    # \xec\x95\x84\xeb\xac\xb4 \xec\x9d\xb8\xec\x9e\x90\xeb\x8f\x84 \xec\x97\x86\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xac\xb4\xed\x95\x9c\xec\x9c\xbc\xeb\xa1\x9c \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0 \xeb\x90\x9c\xeb\x8b\xa4.\n    dataset = dataset.repeat()\n    # make_one_shot_iterator\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \n    # \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n    iterator = dataset.make_one_shot_iterator()\n    # \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x8b\xa4\xec\x9d\x8c \xed\x95\xad\xeb\xaa\xa9\xec\x9d\x98 \xed\x85\x90\xec\x84\x9c \n    # \xea\xb0\x9c\xec\xb2\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\n    return iterator.get_next()\n\n\n# \xed\x8f\x89\xea\xb0\x80\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4\xea\xb0\x80 \xeb\xb0\xb0\xec\xb9\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\ndef eval_input_fn(eval_input_enc, eval_output_dec, eval_target_dec, batch_size):\n    # Dataset\xec\x9d\x84 \xec\x83\x9d\xec\x84\xb1\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9c\xbc\xeb\xa1\x9c\xec\x8d\xa8 from_tensor_slices\xeb\xb6\x80\xeb\xb6\x84\xec\x9d\x80 \n    # \xea\xb0\x81\xea\xb0\x81 \xed\x95\x9c \xeb\xac\xb8\xec\x9e\xa5\xec\x9c\xbc\xeb\xa1\x9c \xec\x9e\x90\xeb\xa5\xb8\xeb\x8b\xa4\xea\xb3\xa0 \xeb\xb3\xb4\xeb\xa9\xb4 \xeb\x90\x9c\xeb\x8b\xa4.\n    # eval_input_enc, eval_output_dec, eval_target_dec \n    # 3\xea\xb0\x9c\xeb\xa5\xbc \xea\xb0\x81\xea\xb0\x81 \xed\x95\x9c\xeb\xac\xb8\xec\x9e\xa5\xec\x9c\xbc\xeb\xa1\x9c \xeb\x82\x98\xeb\x88\x88\xeb\x8b\xa4.\n    dataset = tf.data.Dataset.from_tensor_slices((eval_input_enc, eval_output_dec, eval_target_dec))\n    # \xec\xa0\x84\xec\xb2\xb4 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xec\x84\x9e\xeb\x8a\x94\xeb\x8b\xa4.\n    dataset = dataset.shuffle(buffer_size=len(eval_input_enc))\n    # \xeb\xb0\xb0\xec\xb9\x98 \xec\x9d\xb8\xec\x9e\x90 \xea\xb0\x92\xec\x9d\xb4 \xec\x97\x86\xeb\x8b\xa4\xeb\xa9\xb4  \xec\x97\x90\xeb\x9f\xac\xeb\xa5\xbc \xeb\xb0\x9c\xec\x83\x9d \xec\x8b\x9c\xed\x82\xa8\xeb\x8b\xa4.\n    assert batch_size is not None, ""eval batchSize must not be None""\n    # from_tensor_slices\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x82\x98\xeb\x88\x88\xea\xb2\x83\xec\x9d\x84 \n    # \xeb\xb0\xb0\xec\xb9\x98\xed\x81\xac\xea\xb8\xb0 \xeb\xa7\x8c\xed\x81\xbc \xeb\xac\xb6\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb0\x81 \xec\x9a\x94\xec\x86\x8c\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4\xec\x84\x9c rearrange \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc \n    # \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xec\x9a\x94\xec\x86\x8c\xeb\xa5\xbc \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xa7\xb5\xec\x9c\xbc\xeb\xa1\x9c \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\n    dataset = dataset.map(rearrange)\n    # repeat()\xed\x95\xa8\xec\x88\x98\xec\x97\x90 \xec\x9b\x90\xed\x95\x98\xeb\x8a\x94 \xec\x97\x90\xed\x8f\xac\xed\x81\xac \xec\x88\x98\xeb\xa5\xbc \xeb\x84\xa3\xec\x9d\x84\xec\x88\x98 \xec\x9e\x88\xec\x9c\xbc\xeb\xa9\xb4 \n    # \xec\x95\x84\xeb\xac\xb4 \xec\x9d\xb8\xec\x9e\x90\xeb\x8f\x84 \xec\x97\x86\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xac\xb4\xed\x95\x9c\xec\x9c\xbc\xeb\xa1\x9c \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0 \xeb\x90\x9c\xeb\x8b\xa4.\n    # \xed\x8f\x89\xea\xb0\x80\xec\x9d\xb4\xeb\xaf\x80\xeb\xa1\x9c 1\xed\x9a\x8c\xeb\xa7\x8c \xeb\x8f\x99\xec\x9e\x91 \xec\x8b\x9c\xed\x82\xa8\xeb\x8b\xa4.\n    dataset = dataset.repeat(1)\n    # make_one_shot_iterator\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \n    # \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n    iterator = dataset.make_one_shot_iterator()\n    # \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x8b\xa4\xec\x9d\x8c \xed\x95\xad\xeb\xaa\xa9\xec\x9d\x98 \n    # \xed\x85\x90\xec\x84\x9c \xea\xb0\x9c\xec\xb2\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\n    return iterator.get_next()\n\n\ndef data_tokenizer(data):\n    # \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xed\x95\xb4\xec\x84\x9c \xeb\x8b\xb4\xec\x9d\x84 \xeb\xb0\xb0\xec\x97\xb4 \xec\x83\x9d\xec\x84\xb1\n    words = []\n    for sentence in data:\n        # FILTERS = ""([~.,!?\\""\':;)(])""\n        # \xec\x9c\x84 \xed\x95\x84\xed\x84\xb0\xec\x99\x80 \xea\xb0\x99\xec\x9d\x80 \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xec\xa0\x95\xea\xb7\x9c\xed\x99\x94 \xed\x91\x9c\xed\x98\x84\xec\x8b\x9d\xec\x9d\x84 \n        # \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xeb\xaa\xa8\xeb\x91\x90 """" \xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98 \xed\x95\xb4\xec\xa3\xbc\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n        sentence = re.sub(CHANGE_FILTER, """", sentence)\n        for word in sentence.split():\n            words.append(word)\n    # \xed\x86\xa0\xea\xb7\xb8\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95\xea\xb3\xbc \xec\xa0\x95\xea\xb7\x9c\xed\x91\x9c\xed\x98\x84\xec\x8b\x9d\xec\x9d\x84 \xed\x86\xb5\xed\x95\xb4 \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4\xec\xa7\x84 \n    # \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xeb\x84\x98\xea\xb2\xa8 \xec\xa4\x80\xeb\x8b\xa4.\n    return [word for word in words if word]\n\n\ndef load_vocabulary():\n    # \xec\x82\xac\xec\xa0\x84\xec\x9d\x84 \xeb\x8b\xb4\xec\x9d\x84 \xeb\xb0\xb0\xec\x97\xb4 \xec\xa4\x80\xeb\xb9\x84\xed\x95\x9c\xeb\x8b\xa4.\n    vocabulary_list = []\n    # \xec\x82\xac\xec\xa0\x84\xec\x9d\x84 \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c \xed\x9b\x84 \xed\x8c\x8c\xec\x9d\xbc\xeb\xa1\x9c \xec\xa0\x80\xec\x9e\xa5 \xec\xa7\x84\xed\x96\x89\xed\x95\x9c\xeb\x8b\xa4. \n    # \xea\xb7\xb8 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x98 \xec\xa1\xb4\xec\x9e\xac \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xed\x99\x95\xec\x9d\xb8\xed\x95\x9c\xeb\x8b\xa4.\n    if (not (os.path.exists(DEFINES.vocabulary_path))):\n        # \xec\x9d\xb4\xeb\xaf\xb8 \xec\x83\x9d\xec\x84\xb1\xeb\x90\x9c \xec\x82\xac\xec\xa0\x84 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\xb4 \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xec\x9c\xbc\xeb\xaf\x80\xeb\xa1\x9c \n        # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4\xec\x95\xbc \xed\x95\x9c\xeb\x8b\xa4.\n        # \xea\xb7\xb8\xeb\x9e\x98\xec\x84\x9c \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xea\xb0\x80 \xec\xa1\xb4\xec\x9e\xac \xed\x95\x98\xeb\xa9\xb4 \xec\x82\xac\xec\xa0\x84\xec\x9d\x84 \xeb\xa7\x8c\xeb\x93\xa4\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c \n        # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x98 \xec\xa1\xb4\xec\x9e\xac \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xed\x99\x95\xec\x9d\xb8\xed\x95\x9c\xeb\x8b\xa4.\n        if (os.path.exists(DEFINES.data_path)):\n            # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xea\xb0\x80 \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xeb\x8b\x88 \xed\x8c\x90\xeb\x8b\xa8\xec\x8a\xa4\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \n            # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xb6\x88\xeb\x9f\xac\xec\x98\xa4\xec\x9e\x90\n            data_df = pd.read_csv(DEFINES.data_path, encoding=\'utf-8\')\n            # \xed\x8c\x90\xeb\x8b\xa4\xec\x8a\xa4\xec\x9d\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xed\x94\x84\xeb\xa0\x88\xec\x9e\x84\xec\x9d\x84 \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \n            # \xec\xa7\x88\xeb\xac\xb8\xea\xb3\xbc \xeb\x8b\xb5\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\x97\xb4\xec\x9d\x84 \xea\xb0\x80\xec\xa0\xb8 \xec\x98\xa8\xeb\x8b\xa4.\n            question, answer = list(data_df[\'Q\']), list(data_df[\'A\'])\n            if DEFINES.tokenize_as_morph:  # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c\xec\x97\x90 \xeb\x94\xb0\xeb\xa5\xb8 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa0\xb8 \xec\xb2\x98\xeb\xa6\xac\n                question = prepro_like_morphlized(question)\n                answer = prepro_like_morphlized(answer)\n            data = []\n            # \xec\xa7\x88\xeb\xac\xb8\xea\xb3\xbc \xeb\x8b\xb5\xeb\xb3\x80\xec\x9d\x84 extend\xec\x9d\x84 \n            # \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xea\xb5\xac\xec\xa1\xb0\xea\xb0\x80 \xec\x97\x86\xeb\x8a\x94 \xeb\xb0\xb0\xec\x97\xb4\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n            data.extend(question)\n            data.extend(answer)\n            # \xed\x86\xa0\xed\x81\xb0\xeb\x82\x98\xec\x9d\xb4\xec\xa0\xb8 \xec\xb2\x98\xeb\xa6\xac \xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n            words = data_tokenizer(data)\n            # \xea\xb3\xb5\xed\x86\xb5\xec\xa0\x81\xec\x9d\xb8 \xeb\x8b\xa8\xec\x96\xb4\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4\xec\x84\x9c\xeb\x8a\x94 \xeb\xaa\xa8\xeb\x91\x90 \n            # \xed\x95\x84\xec\x9a\x94 \xec\x97\x86\xec\x9c\xbc\xeb\xaf\x80\xeb\xa1\x9c \xed\x95\x9c\xea\xb0\x9c\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xec\xa3\xbc\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c\n            # set\xed\x95\xb4\xec\xa3\xbc\xea\xb3\xa0 \xec\x9d\xb4\xea\xb2\x83\xeb\x93\xa4\xec\x9d\x84 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n            words = list(set(words))\n            # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\x97\x86\xeb\x8a\x94 \xeb\x82\xb4\xec\x9a\xa9\xec\xa4\x91\xec\x97\x90 MARKER\xeb\xa5\xbc \xec\x82\xac\xec\xa0\x84\xec\x97\x90 \n            # \xec\xb6\x94\xea\xb0\x80 \xed\x95\x98\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c \xec\x95\x84\xeb\x9e\x98\xec\x99\x80 \xea\xb0\x99\xec\x9d\xb4 \xec\xb2\x98\xeb\xa6\xac \xed\x95\x9c\xeb\x8b\xa4.\n            # \xec\x95\x84\xeb\x9e\x98\xeb\x8a\x94 MARKER \xea\xb0\x92\xec\x9d\xb4\xeb\xa9\xb0 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xec\x9d\x98 \xec\xb2\xab\xeb\xb2\x88\xec\xa7\xb8 \xeb\xb6\x80\xed\x84\xb0 \n            # \xec\x88\x9c\xec\x84\x9c\xeb\x8c\x80\xeb\xa1\x9c \xeb\x84\xa3\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 0\xec\x97\x90 \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\n            # PAD = ""<PADDING>""\n            # STD = ""<START>""\n            # END = ""<END>""\n            # UNK = ""<UNKNWON>""     \n            words[:0] = MARKER\n        # \xec\x82\xac\xec\xa0\x84 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xec\x82\xac\xec\xa0\x84 \xed\x8c\x8c\xec\x9d\xbc\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.\n        with open(DEFINES.vocabulary_path, \'w\', encoding=\'utf-8\') as vocabulary_file:\n            for word in words:\n                vocabulary_file.write(word + \'\\n\')\n\n    # \xec\x82\xac\xec\xa0\x84 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\xb4 \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xeb\xa9\xb4 \xec\x97\xac\xea\xb8\xb0\xec\x97\x90\xec\x84\x9c \n    # \xea\xb7\xb8 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xeb\xb6\x88\xeb\x9f\xac\xec\x84\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n    with open(DEFINES.vocabulary_path, \'r\', encoding=\'utf-8\') as vocabulary_file:\n        for line in vocabulary_file:\n            vocabulary_list.append(line.strip())\n\n    # \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xeb\x82\xb4\xec\x9a\xa9\xec\x9d\x84 \xed\x82\xa4\xec\x99\x80 \xea\xb0\x92\xec\x9d\xb4 \xec\x9e\x88\xeb\x8a\x94 \n    # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac \xea\xb5\xac\xec\xa1\xb0\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n    char2idx, idx2char = make_vocabulary(vocabulary_list)\n    # \xeb\x91\x90\xea\xb0\x80\xec\xa7\x80 \xed\x98\x95\xed\x83\x9c\xec\x9d\x98 \xed\x82\xa4\xec\x99\x80 \xea\xb0\x92\xec\x9d\xb4 \xec\x9e\x88\xeb\x8a\x94 \xed\x98\x95\xed\x83\x9c\xeb\xa5\xbc \xeb\xa6\xac\xed\x84\xb4\xed\x95\x9c\xeb\x8b\xa4. \n    # (\xec\x98\x88) \xeb\x8b\xa8\xec\x96\xb4: \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 , \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4: \xeb\x8b\xa8\xec\x96\xb4)\n    return char2idx, idx2char, len(char2idx)\n\n\ndef make_vocabulary(vocabulary_list):\n    # \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xed\x82\xa4\xea\xb0\x80 \xeb\x8b\xa8\xec\x96\xb4\xec\x9d\xb4\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\xb4 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb8 \n    # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n    # \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xed\x82\xa4\xea\xb0\x80 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb4\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\xb4 \xeb\x8b\xa8\xec\x96\xb4\xec\x9d\xb8 \n    # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n    # \xeb\x91\x90\xea\xb0\x9c\xec\x9d\x98 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8 \xec\xa4\x80\xeb\x8b\xa4.\n    return char2idx, idx2char\n\n\ndef main(self):\n    char2idx, idx2char, vocabulary_length = load_vocabulary()\n\n\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run(main)\n'"
6.CHATBOT/6.4 transformer/main.py,0,"b'import tensorflow as tf\nimport model as ml\nimport data\nimport numpy as np\nimport os\nimport sys\n\nfrom configs import DEFINES\n\nDATA_OUT_PATH = \'./data_out/\'\n\n\ndef main(self):\n    data_out_path = os.path.join(os.getcwd(), DATA_OUT_PATH)\n    os.makedirs(data_out_path, exist_ok=True)\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xed\x86\xb5\xed\x95\x9c \xec\x82\xac\xec\xa0\x84 \xea\xb5\xac\xec\x84\xb1 \xed\x95\x9c\xeb\x8b\xa4.\n    char2idx, idx2char, vocabulary_length = data.load_vocabulary()\n    # \xed\x9b\x88\xeb\xa0\xa8 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xec\x99\x80 \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xea\xb0\x80\xec\xa0\xb8\xec\x98\xa8\xeb\x8b\xa4.\n    train_input, train_label, eval_input, eval_label = data.load_data()\n\n    # \xed\x9b\x88\xeb\xa0\xa8\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    train_input_enc, train_input_enc_length = data.enc_processing(train_input, char2idx)\n    # \xed\x9b\x88\xeb\xa0\xa8\xec\x85\x8b \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5 \xeb\xb6\x80\xeb\xb6\x84 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    train_output_dec, train_output_dec_length = data.dec_output_processing(train_label, char2idx)\n    # \xed\x9b\x88\xeb\xa0\xa8\xec\x85\x8b \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\xb6\x80\xeb\xb6\x84 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    train_target_dec = data.dec_target_processing(train_label, char2idx)\n\n    # \xed\x8f\x89\xea\xb0\x80\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    eval_input_enc, eval_input_enc_length = data.enc_processing(eval_input, char2idx)\n    # \xed\x8f\x89\xea\xb0\x80\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    eval_output_dec, eval_output_dec_length = data.dec_output_processing(eval_label, char2idx)\n    # \xed\x8f\x89\xea\xb0\x80\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    eval_target_dec = data.dec_target_processing(eval_label, char2idx)\n\n    # \xed\x98\x84\xec\x9e\xac \xea\xb2\xbd\xeb\xa1\x9c\'./\'\xec\x97\x90 \xed\x98\x84\xec\x9e\xac \xea\xb2\xbd\xeb\xa1\x9c \xed\x95\x98\xeb\xb6\x80\xec\x97\x90 \n    # \xec\xb2\xb4\xed\x81\xac \xed\x8f\xac\xec\x9d\xb8\xed\x8a\xb8\xeb\xa5\xbc \xec\xa0\x80\xec\x9e\xa5\xed\x95\x9c \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n    check_point_path = os.path.join(os.getcwd(), DEFINES.check_point_path)\n    # \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\xa9\xb0 \xeb\x91\x90\xeb\xb2\x88\xec\xa7\xb8 \xec\x9d\xb8\xec\x9e\x90 exist_ok\xea\xb0\x80 \n    # True\xec\x9d\xb4\xeb\xa9\xb4 \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xea\xb0\x80 \xec\x9d\xb4\xeb\xaf\xb8 \xec\xa1\xb4\xec\x9e\xac\xed\x95\xb4\xeb\x8f\x84 OSError\xea\xb0\x80 \n    # \xeb\xb0\x9c\xec\x83\x9d\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94\xeb\x8b\xa4.\n    # exist_ok\xea\xb0\x80 False\xec\x9d\xb4\xeb\xa9\xb4 \xec\x9d\xb4\xeb\xaf\xb8 \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xeb\xa9\xb4 \n    # OSError\xea\xb0\x80 \xeb\xb0\x9c\xec\x83\x9d\xed\x95\x9c\xeb\x8b\xa4.\n    os.makedirs(check_point_path, exist_ok=True)\n\n    # \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0 \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\n    classifier = tf.estimator.Estimator(\n        model_fn=ml.Model,  # \xeb\xaa\xa8\xeb\x8d\xb8 \xeb\x93\xb1\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\n        model_dir=DEFINES.check_point_path,  # \xec\xb2\xb4\xed\x81\xac\xed\x8f\xac\xec\x9d\xb8\xed\x8a\xb8 \xec\x9c\x84\xec\xb9\x98 \xeb\x93\xb1\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\n        params={  # \xeb\xaa\xa8\xeb\x8d\xb8 \xec\xaa\xbd\xec\x9c\xbc\xeb\xa1\x9c \xed\x8c\x8c\xeb\x9d\xbc\xeb\xa9\x94\xed\x84\xb0 \xec\xa0\x84\xeb\x8b\xac\xed\x95\x9c\xeb\x8b\xa4.\n            \'embedding_size\': DEFINES.embedding_size,\n            \'model_hidden_size\': DEFINES.model_hidden_size,  # \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            \'ffn_hidden_size\': DEFINES.ffn_hidden_size,\n            \'attention_head_size\': DEFINES.attention_head_size,\n            \'learning_rate\': DEFINES.learning_rate,  # \xed\x95\x99\xec\x8a\xb5\xec\x9c\xa8 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            \'vocabulary_length\': vocabulary_length,  # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac \xed\x81\xac\xea\xb8\xb0\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            \'embedding_size\': DEFINES.embedding_size,  # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xed\x81\xac\xea\xb8\xb0\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            \'layer_size\': DEFINES.layer_size,\n            \'max_sequence_length\': DEFINES.max_sequence_length,\n            \'xavier_initializer\': DEFINES.xavier_initializer\n        })\n\n    # \xed\x95\x99\xec\x8a\xb5 \xec\x8b\xa4\xed\x96\x89\n    classifier.train(input_fn=lambda: data.train_input_fn(\n        train_input_enc, train_output_dec, train_target_dec, DEFINES.batch_size), steps=DEFINES.train_steps)\n\n    eval_result = classifier.evaluate(input_fn=lambda: data.eval_input_fn(\n        eval_input_enc, eval_output_dec, eval_target_dec, DEFINES.batch_size))\n    print(\'\\nEVAL set accuracy: {accuracy:0.3f}\\n\'.format(**eval_result))\n\n    # \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8\xec\x9a\xa9 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    # \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xb6\x80\xeb\xb6\x84 \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n    predic_input_enc, predic_input_enc_length = data.enc_processing([""\xea\xb0\x80\xeb\x81\x94 \xea\xb6\x81\xea\xb8\x88\xed\x95\xb4""], char2idx)\n    # \xed\x95\x99\xec\x8a\xb5 \xea\xb3\xbc\xec\xa0\x95\xec\x9d\xb4 \xec\x95\x84\xeb\x8b\x88\xeb\xaf\x80\xeb\xa1\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5\xec\x9d\x80 \n    # \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94\xeb\x8b\xa4.(\xea\xb5\xac\xec\xa1\xb0\xeb\xa5\xbc \xeb\xa7\x9e\xec\xb6\x94\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.)\n    predic_output_dec, predic_output_decLength = data.dec_output_processing([""""], char2idx)\n    # \xed\x95\x99\xec\x8a\xb5 \xea\xb3\xbc\xec\xa0\x95\xec\x9d\xb4 \xec\x95\x84\xeb\x8b\x88\xeb\xaf\x80\xeb\xa1\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\xb6\x80\xeb\xb6\x84\xeb\x8f\x84 \n    # \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94\xeb\x8b\xa4.(\xea\xb5\xac\xec\xa1\xb0\xeb\xa5\xbc \xeb\xa7\x9e\xec\xb6\x94\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.)\n    predic_target_dec = data.dec_target_processing([""""], char2idx)\n\n    for i in range(DEFINES.max_sequence_length):\n        if i > 0:\n            predic_output_dec, predic_output_decLength = data.dec_output_processing([answer], char2idx)\n            predic_target_dec = data.dec_target_processing([answer], char2idx)\n        # \xec\x98\x88\xec\xb8\xa1\xec\x9d\x84 \xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n        predictions = classifier.predict(\n            input_fn=lambda: data.eval_input_fn(predic_input_enc, predic_output_dec, predic_target_dec, 1))\n\n        answer, finished = data.pred_next_string(predictions, idx2char)\n\n        if finished:\n            break\n\n    # \xec\x98\x88\xec\xb8\xa1\xed\x95\x9c \xea\xb0\x92\xec\x9d\x84 \xec\x9d\xb8\xec\xa7\x80 \xed\x95\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8f\x84\xeb\xa1\x9d\n    # \xed\x85\x8d\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    print(""answer: "", answer)\n\n\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run(main)\n\ntf.logging.set_verbosity\n'"
6.CHATBOT/6.4 transformer/model.py,3,"b""# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport sys\n\nfrom configs import DEFINES\nimport numpy as np\n\n\ndef layer_norm(inputs, eps=1e-6):\n    # LayerNorm(x + Sublayer(x))\n    feature_shape = inputs.get_shape()[-1:]\n    #  \xed\x8f\x89\xea\xb7\xa0\xea\xb3\xbc \xed\x91\x9c\xec\xa4\x80\xed\x8e\xb8\xec\xb0\xa8\xec\x9d\x84 \xeb\x84\x98\xea\xb2\xa8 \xec\xa4\x80\xeb\x8b\xa4.\n    mean = tf.keras.backend.mean(inputs, [-1], keepdims=True)\n    std = tf.keras.backend.std(inputs, [-1], keepdims=True)\n    beta = tf.Variable(tf.zeros(feature_shape), trainable=False)\n    gamma = tf.Variable(tf.ones(feature_shape), trainable=False)\n\n    return gamma * (inputs - mean) / (std + eps) + beta\n\n\ndef sublayer_connection(inputs, sublayer, dropout=0.2):\n    # LayerNorm(x + Sublayer(x))\n    outputs = layer_norm(inputs + tf.keras.layers.Dropout(dropout)(sublayer))\n    return outputs\n\n\ndef feed_forward(inputs, num_units):\n    # FFN(x) = max(0, xW1 + b1)W2 + b2\n    feature_shape = inputs.get_shape()[-1]\n    inner_layer = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(inputs)\n    outputs = tf.keras.layers.Dense(feature_shape)(inner_layer)\n\n    return outputs\n\n\ndef positional_encoding(dim, sentence_length):\n    # Positional Encoding\n    # paper: https://arxiv.org/abs/1706.03762\n    # P E(pos,2i) = sin(pos/100002i/dmodel)\n    # P E(pos,2i+1) = cos(pos/100002i/dmodel)\n    encoded_vec = np.array([pos / np.power(10000, 2 * i / dim)\n                            for pos in range(sentence_length) for i in range(dim)])\n    encoded_vec[::2] = np.sin(encoded_vec[::2])\n    encoded_vec[1::2] = np.cos(encoded_vec[1::2])\n    return tf.constant(encoded_vec.reshape([sentence_length, dim]), dtype=tf.float32)\n\n\ndef scaled_dot_product_attention(query, key, value, masked=False):\n    # Attention(Q, K, V ) = softmax(QKt / root dk)V\n    key_dim_size = float(key.get_shape().as_list()[-1])\n    key = tf.transpose(key, perm=[0, 2, 1])\n    outputs = tf.matmul(query, key) / tf.sqrt(key_dim_size)\n\n    if masked:\n        diag_vals = tf.ones_like(outputs[0, :, :])\n        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()\n        masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])\n\n        paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\n        outputs = tf.where(tf.equal(masks, 0), paddings, outputs)\n\n    attention_map = tf.nn.softmax(outputs)\n\n    return tf.matmul(attention_map, value)\n\n\ndef multi_head_attention(query, key, value, num_units, heads, masked=False):\n    query = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(query)\n    key = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(key)\n    value = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(value)\n\n    query = tf.concat(tf.split(query, heads, axis=-1), axis=0)\n    key = tf.concat(tf.split(key, heads, axis=-1), axis=0)\n    value = tf.concat(tf.split(value, heads, axis=-1), axis=0)\n\n    attention_map = scaled_dot_product_attention(query, key, value, masked)\n\n    attn_outputs = tf.concat(tf.split(attention_map, heads, axis=0), axis=-1)\n    attn_outputs = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(attn_outputs)\n\n    return attn_outputs\n\n\ndef encoder_module(inputs, model_dim, ffn_dim, heads):\n    self_attn = sublayer_connection(inputs, multi_head_attention(inputs, inputs, inputs,\n                                                                 model_dim, heads))\n    outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n    return outputs\n\n\ndef decoder_module(inputs, encoder_outputs, model_dim, ffn_dim, heads):\n    masked_self_attn = sublayer_connection(inputs, multi_head_attention(inputs, inputs, inputs,\n                                                                        model_dim, heads, masked=True))\n    self_attn = sublayer_connection(masked_self_attn, multi_head_attention(masked_self_attn, encoder_outputs,\n                                                                           encoder_outputs, model_dim, heads))\n    outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n\n    return outputs\n\n\ndef encoder(inputs, model_dim, ffn_dim, heads, num_layers):\n    outputs = inputs\n    for i in range(num_layers):\n        outputs = encoder_module(outputs, model_dim, ffn_dim, heads)\n\n    return outputs\n\n\ndef decoder(inputs, encoder_outputs, model_dim, ffn_dim, heads, num_layers):\n    outputs = inputs\n    for i in range(num_layers):\n        outputs = decoder_module(outputs, encoder_outputs, model_dim, ffn_dim, heads)\n\n    return outputs\n\n\ndef Model(features, labels, mode, params):\n    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n    EVAL = mode == tf.estimator.ModeKeys.EVAL\n    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n\n    position_encode = positional_encoding(params['embedding_size'], params['max_sequence_length'])\n\n    if params['xavier_initializer']:\n        embedding_initializer = 'glorot_normal'\n    else:\n        embedding_initializer = 'uniform'\n\n    embedding = tf.keras.layers.Embedding(params['vocabulary_length'],\n                                          params['embedding_size'],\n                                          embeddings_initializer=embedding_initializer)\n\n    x_embedded_matrix = embedding(features['input']) + position_encode\n    y_embedded_matrix = embedding(features['output']) + position_encode\n\n    encoder_outputs = encoder(x_embedded_matrix, params['model_hidden_size'], params['ffn_hidden_size'],\n                              params['attention_head_size'], params['layer_size'])\n    decoder_outputs = decoder(y_embedded_matrix, encoder_outputs, params['model_hidden_size'],\n                              params['ffn_hidden_size'],\n                              params['attention_head_size'], params['layer_size'])\n\n    logits = tf.keras.layers.Dense(params['vocabulary_length'])(decoder_outputs)\n\n    predict = tf.argmax(logits, 2)\n\n    if PREDICT:\n        predictions = {\n            'indexs': predict,\n            'logits': logits,\n        }\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n    # \xec\xa0\x95\xeb\x8b\xb5 \xec\xb0\xa8\xec\x9b\x90 \xeb\xb3\x80\xea\xb2\xbd\xec\x9d\x84 \xed\x95\x9c\xeb\x8b\xa4. [\xeb\xb0\xb0\xec\xb9\x98 * max_sequence_length * vocabulary_length]  \n    # logits\xea\xb3\xbc \xea\xb0\x99\xec\x9d\x80 \xec\xb0\xa8\xec\x9b\x90\xec\x9d\x84 \xeb\xa7\x8c\xeb\x93\xa4\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xa8\xec\x9d\xb4\xeb\x8b\xa4.\n    labels_ = tf.one_hot(labels, params['vocabulary_length'])\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_))\n\n    accuracy = tf.metrics.accuracy(labels=labels, predictions=predict)\n\n    metrics = {'accuracy': accuracy}\n    tf.summary.scalar('accuracy', accuracy[1])\n\n    if EVAL:\n        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n\n    assert TRAIN\n\n    # lrate = d\xe2\x88\x920.5 *  model \xc2\xb7 min(step_num\xe2\x88\x920.5, step_num \xc2\xb7 warmup_steps\xe2\x88\x921.5)\n    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n"""
6.CHATBOT/6.4 transformer/predict.py,0,"b'import tensorflow as tf\nimport data\nimport os\nimport sys\nimport model as ml\n\nfrom configs import DEFINES\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(tf.logging.ERROR)\n    arg_length = len(sys.argv)\n\n    if (arg_length < 2):\n        raise Exception(""Don\'t call us. We\'ll call you"")\n\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xed\x86\xb5\xed\x95\x9c \xec\x82\xac\xec\xa0\x84 \xea\xb5\xac\xec\x84\xb1 \xed\x95\x9c\xeb\x8b\xa4.\n    char2idx, idx2char, vocabulary_length = data.load_vocabulary()\n\n    # \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8\xec\x9a\xa9 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    # \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xb6\x80\xeb\xb6\x84 \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n    input = """"\n    for i in sys.argv[1:]:\n        input += i\n        input += "" ""\n\n    print(input)\n    predic_input_enc, predic_input_enc_length = data.enc_processing([input], char2idx)\n    # \xed\x95\x99\xec\x8a\xb5 \xea\xb3\xbc\xec\xa0\x95\xec\x9d\xb4 \xec\x95\x84\xeb\x8b\x88\xeb\xaf\x80\xeb\xa1\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5\xec\x9d\x80 \n    # \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94\xeb\x8b\xa4.(\xea\xb5\xac\xec\xa1\xb0\xeb\xa5\xbc \xeb\xa7\x9e\xec\xb6\x94\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.)\n    predic_output_dec, predic_output_dec_length = data.dec_output_processing([""""], char2idx)\n    # \xed\x95\x99\xec\x8a\xb5 \xea\xb3\xbc\xec\xa0\x95\xec\x9d\xb4 \xec\x95\x84\xeb\x8b\x88\xeb\xaf\x80\xeb\xa1\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\xb6\x80\xeb\xb6\x84\xeb\x8f\x84 \n    # \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94\xeb\x8b\xa4.(\xea\xb5\xac\xec\xa1\xb0\xeb\xa5\xbc \xeb\xa7\x9e\xec\xb6\x94\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.)\n    predic_target_dec = data.dec_target_processing([""""], char2idx)\n\n    # \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0 \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\n    classifier = tf.estimator.Estimator(\n        model_fn=ml.Model,  # \xeb\xaa\xa8\xeb\x8d\xb8 \xeb\x93\xb1\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\n        model_dir=DEFINES.check_point_path,  # \xec\xb2\xb4\xed\x81\xac\xed\x8f\xac\xec\x9d\xb8\xed\x8a\xb8 \xec\x9c\x84\xec\xb9\x98 \xeb\x93\xb1\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\n        params={  # \xeb\xaa\xa8\xeb\x8d\xb8 \xec\xaa\xbd\xec\x9c\xbc\xeb\xa1\x9c \xed\x8c\x8c\xeb\x9d\xbc\xeb\xa9\x94\xed\x84\xb0 \xec\xa0\x84\xeb\x8b\xac\xed\x95\x9c\xeb\x8b\xa4.\n            \'embedding_size\': DEFINES.embedding_size,\n            \'model_hidden_size\': DEFINES.model_hidden_size,  # \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            \'ffn_hidden_size\': DEFINES.ffn_hidden_size,\n            \'attention_head_size\': DEFINES.attention_head_size,\n            \'learning_rate\': DEFINES.learning_rate,  # \xed\x95\x99\xec\x8a\xb5\xec\x9c\xa8 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            \'vocabulary_length\': vocabulary_length,  # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac \xed\x81\xac\xea\xb8\xb0\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            \'embedding_size\': DEFINES.embedding_size,  # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xed\x81\xac\xea\xb8\xb0\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            \'layer_size\': DEFINES.layer_size,\n            \'max_sequence_length\': DEFINES.max_sequence_length,\n            \'xavier_initializer\': DEFINES.xavier_initializer\n        })\n\n    for i in range(DEFINES.max_sequence_length):\n        if i > 0:\n            predic_output_dec, predic_output_decLength = data.dec_output_processing([answer], char2idx)\n            predic_target_dec = data.dec_target_processing([answer], char2idx)\n        # \xec\x98\x88\xec\xb8\xa1\xec\x9d\x84 \xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n        predictions = classifier.predict(\n            input_fn=lambda: data.eval_input_fn(predic_input_enc, predic_output_dec, predic_target_dec, 1))\n\n        answer, finished = data.pred_next_string(predictions, idx2char)\n\n        if finished:\n            break\n\n    # \xec\x98\x88\xec\xb8\xa1\xed\x95\x9c \xea\xb0\x92\xec\x9d\x84 \xec\x9d\xb8\xec\xa7\x80 \xed\x95\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8f\x84\xeb\xa1\x9d\n    # \xed\x85\x8d\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    print(""answer: "", answer)\n'"
6.CHATBOT/Appendix-transformer/configs.py,0,"b""# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\n\r\ntf.app.flags.DEFINE_integer('batch_size', 64, 'batch size')  # \xeb\xb0\xb0\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0\r\ntf.app.flags.DEFINE_integer('train_steps', 20000, 'train steps')  # \xed\x95\x99\xec\x8a\xb5 \xec\x97\x90\xed\x8f\xac\xed\x81\xac\r\ntf.app.flags.DEFINE_float('dropout_width', 0.5, 'dropout width')  # \xeb\x93\x9c\xeb\xa1\xad\xec\x95\x84\xec\x9b\x83 \xed\x81\xac\xea\xb8\xb0\r\ntf.app.flags.DEFINE_integer('embedding_size', 128, 'embedding size')  # \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0 # \xeb\x85\xbc\xeb\xac\xb8 512 \xec\x82\xac\xec\x9a\xa9\r\ntf.app.flags.DEFINE_float('learning_rate', 1e-3, 'learning rate')  # \xed\x95\x99\xec\x8a\xb5\xeb\xa5\xa0\r\ntf.app.flags.DEFINE_integer('shuffle_seek', 1000, 'shuffle random seek')  # \xec\x85\x94\xed\x94\x8c \xec\x8b\x9c\xeb\x93\x9c\xea\xb0\x92\r\ntf.app.flags.DEFINE_integer('max_sequence_length', 25, 'max sequence length')  # \xec\x8b\x9c\xed\x80\x80\xec\x8a\xa4 \xea\xb8\xb8\xec\x9d\xb4\r\ntf.app.flags.DEFINE_integer('model_hidden_size', 128, 'model weights size')  # \xeb\xaa\xa8\xeb\x8d\xb8 \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0\r\ntf.app.flags.DEFINE_integer('ffn_hidden_size', 512, 'ffn weights size')  # ffn \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0\r\ntf.app.flags.DEFINE_integer('attention_head_size', 4, 'attn head size')  # \xeb\xa9\x80\xed\x8b\xb0 \xed\x97\xa4\xeb\x93\x9c \xed\x81\xac\xea\xb8\xb0\r\ntf.app.flags.DEFINE_integer('layer_size', 2, 'layer size')  # \xeb\x85\xbc\xeb\xac\xb8\xec\x9d\x80 6\xea\xb0\x9c \xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4\xec\x9d\xb4\xeb\x82\x98 2\xea\xb0\x9c \xec\x82\xac\xec\x9a\xa9 \xed\x95\x99\xec\x8a\xb5 \xec\x86\x8d\xeb\x8f\x84 \xeb\xb0\x8f \xec\x84\xb1\xeb\x8a\xa5 \xed\x8a\x9c\xeb\x8b\x9d\r\ntf.app.flags.DEFINE_string('data_path', '../data_in/ChatBotData.csv', 'data path')  # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\x9c\x84\xec\xb9\x98\r\ntf.app.flags.DEFINE_string('vocabulary_path', './data_out/vocabularyData.voc', 'vocabulary path')  # \xec\x82\xac\xec\xa0\x84 \xec\x9c\x84\xec\xb9\x98\r\ntf.app.flags.DEFINE_string('check_point_path', './data_out/check_point', 'check point path')  # \xec\xb2\xb4\xed\x81\xac \xed\x8f\xac\xec\x9d\xb8\xed\x8a\xb8 \xec\x9c\x84\xec\xb9\x98\r\ntf.app.flags.DEFINE_boolean('tokenize_as_morph', False, 'set morph tokenize')  # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c\xec\x97\x90 \xeb\x94\xb0\xeb\xa5\xb8 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\r\ntf.app.flags.DEFINE_boolean('xavier_initializer', True, 'set xavier initializer')  # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c\xec\x97\x90 \xeb\x94\xb0\xeb\xa5\xb8 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\r\n\r\n# Define FLAGS\r\nDEFINES = tf.app.flags.FLAGS\r\n"""
6.CHATBOT/Appendix-transformer/data.py,3,"b'from konlpy.tag import Twitter\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport enum\r\nimport os\r\nimport re\r\nfrom sklearn.model_selection import train_test_split\r\nimport numpy as np\r\nfrom configs import DEFINES\r\n\r\nfrom tqdm import tqdm\r\n\r\nFILTERS = ""([~.,!?\\""\':;)(])""\r\nPAD = ""<PAD>""\r\nSTD = ""<SOS>""\r\nEND = ""<END>""\r\nUNK = ""<UNK>""\r\n\r\nPAD_INDEX = 0\r\nSTD_INDEX = 1\r\nEND_INDEX = 2\r\nUNK_INDEX = 3\r\n\r\nMARKER = [PAD, STD, END, UNK]\r\nCHANGE_FILTER = re.compile(FILTERS)\r\n\r\n\r\ndef load_data():\r\n    # \xed\x8c\x90\xeb\x8b\xa4\xec\x8a\xa4\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xb6\x88\xeb\x9f\xac\xec\x98\xa8\xeb\x8b\xa4.\r\n    data_df = pd.read_csv(DEFINES.data_path, header=0)\r\n    # \xec\xa7\x88\xeb\xac\xb8\xea\xb3\xbc \xeb\x8b\xb5\xeb\xb3\x80 \xec\x97\xb4\xec\x9d\x84 \xea\xb0\x80\xec\xa0\xb8\xec\x99\x80 question\xea\xb3\xbc answer\xec\x97\x90 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.\r\n    question, answer = list(data_df[\'Q\']), list(data_df[\'A\'])\r\n    # skleran\xec\x97\x90\xec\x84\x9c \xec\xa7\x80\xec\x9b\x90\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xed\x95\x99\xec\x8a\xb5 \xec\x85\x8b\xea\xb3\xbc\r\n    # \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8 \xec\x85\x8b\xec\x9d\x84 \xeb\x82\x98\xeb\x88\x88\xeb\x8b\xa4.\r\n    train_input, eval_input, train_label, eval_label = train_test_split(question, answer, test_size=0.33,\r\n                                                                        random_state=42)\r\n    # \xea\xb7\xb8 \xea\xb0\x92\xec\x9d\x84 \xeb\xa6\xac\xed\x84\xb4\xed\x95\x9c\xeb\x8b\xa4.\r\n    return train_input, train_label, eval_input, eval_label\r\n\r\n\r\ndef prepro_like_morphlized(data):\r\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xeb\xb6\x84\xec\x84\x9d \xeb\xaa\xa8\xeb\x93\x88 \xea\xb0\x9d\xec\xb2\xb4\xeb\xa5\xbc\r\n    # \xec\x83\x9d\xec\x84\xb1\xed\x95\xa9\xeb\x8b\x88\xeb\x8b\xa4.\r\n\r\n    morph_analyzer = Twitter()\r\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa6\x88 \xea\xb2\xb0\xea\xb3\xbc \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xeb\xb0\x9b\xec\x9d\x84\r\n    #  \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xec\x83\x9d\xec\x84\xb1\xed\x95\xa9\xeb\x8b\x88\xeb\x8b\xa4.\r\n    result_data = list()\r\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xec\x97\x90 \xec\x9e\x88\xeb\x8a\x94 \xeb\xa7\xa4 \xeb\xac\xb8\xec\x9e\xa5\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa6\x88\xeb\xa5\xbc\r\n    # \xed\x95\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8f\x84\xeb\xa1\x9d \xeb\xb0\x98\xeb\xb3\xb5\xeb\xac\xb8\xec\x9d\x84 \xec\x84\xa0\xec\x96\xb8\xed\x95\xa9\xeb\x8b\x88\xeb\x8b\xa4.\r\n    for seq in tqdm(data):\r\n        # Twitter.morphs \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa6\x88 \xeb\x90\x9c\r\n        # \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8 \xea\xb0\x9d\xec\xb2\xb4\xeb\xa5\xbc \xeb\xb0\x9b\xea\xb3\xa0 \xeb\x8b\xa4\xec\x8b\x9c \xea\xb3\xb5\xeb\xb0\xb1\xeb\xac\xb8\xec\x9e\x90\xeb\xa5\xbc \xea\xb8\xb0\xec\xa4\x80\xec\x9c\xbc\xeb\xa1\x9c\r\n        # \xed\x95\x98\xec\x97\xac \xeb\xac\xb8\xec\x9e\x90\xec\x97\xb4\xeb\xa1\x9c \xec\x9e\xac\xea\xb5\xac\xec\x84\xb1 \xed\x95\xb4\xec\xa4\x8d\xeb\x8b\x88\xeb\x8b\xa4.\r\n        morphlized_seq = "" "".join(morph_analyzer.morphs(seq.replace(\' \', \'\')))\r\n        result_data.append(morphlized_seq)\r\n\r\n    return result_data\r\n\r\n\r\n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xed\x95\xa0 value\xec\x99\x80 \xed\x82\xa4\xea\xb0\x80 \xec\x9b\x8c\xeb\x93\x9c\xec\x9d\xb4\xea\xb3\xa0\r\n# \xea\xb0\x92\xec\x9d\xb4 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb8 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xb0\x9b\xeb\x8a\x94\xeb\x8b\xa4.\r\ndef enc_processing(value, dictionary):\r\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8a\x94\r\n    # \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\xb4\xeb\x8b\xa4.(\xeb\x88\x84\xec\xa0\x81\xeb\x90\x9c\xeb\x8b\xa4.)\r\n    sequences_input_index = []\r\n    # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\x90\x98\xeb\x8a\x94 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x98\r\n    # \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.(\xeb\x88\x84\xec\xa0\x81\xeb\x90\x9c\xeb\x8b\xa4.)\r\n    sequences_length = []\r\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\r\n    if DEFINES.tokenize_as_morph:\r\n        value = prepro_like_morphlized(value)\r\n\r\n    # \xed\x95\x9c\xec\xa4\x84\xec\x94\xa9 \xeb\xb6\x88\xec\x96\xb4\xec\x98\xa8\xeb\x8b\xa4.\r\n    for sequence in value:\r\n        # FILTERS = ""([~.,!?\\""\':;)(])""\r\n        # \xec\xa0\x95\xea\xb7\x9c\xed\x99\x94\xeb\xa5\xbc \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xec\x97\xac \xed\x95\x84\xed\x84\xb0\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94\r\n        # \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 """" \xec\x9c\xbc\xeb\xa1\x9c \xec\xb9\x98\xed\x99\x98 \xed\x95\x9c\xeb\x8b\xa4.\r\n        sequence = re.sub(CHANGE_FILTER, """", sequence)\r\n        # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xed\x95\xa0\xeb\x95\x8c\r\n        # \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\xb4\xeb\x8b\xa4.\r\n        sequence_index = []\r\n        # \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xeb\x8b\xa8\xec\x9c\x84\xeb\xa1\x9c\r\n        # \xec\x9e\x90\xeb\xa5\xb4\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\r\n        for word in sequence.split():\r\n            # \xec\x9e\x98\xeb\xa0\xa4\xec\xa7\x84 \xeb\x8b\xa8\xec\x96\xb4\xeb\x93\xa4\xec\x9d\xb4 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x97\x90 \xec\xa1\xb4\xec\x9e\xac \xed\x95\x98\xeb\x8a\x94\xec\xa7\x80 \xeb\xb3\xb4\xea\xb3\xa0\r\n            # \xea\xb7\xb8 \xea\xb0\x92\xec\x9d\x84 \xea\xb0\x80\xec\xa0\xb8\xec\x99\x80 sequence_index\xec\x97\x90 \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\r\n            if dictionary.get(word) is not None:\r\n                sequence_index.extend([dictionary[word]])\r\n            # \xec\x9e\x98\xeb\xa0\xa4\xec\xa7\x84 \xeb\x8b\xa8\xec\x96\xb4\xea\xb0\x80 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x97\x90 \xec\xa1\xb4\xec\x9e\xac \xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94\r\n            # \xea\xb2\xbd\xec\x9a\xb0 \xec\x9d\xb4\xeb\xaf\x80\xeb\xa1\x9c UNK(2)\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\r\n            else:\r\n                sequence_index.extend([dictionary[UNK]])\r\n        # \xeb\xac\xb8\xec\x9e\xa5 \xec\xa0\x9c\xed\x95\x9c \xea\xb8\xb8\xec\x9d\xb4\xeb\xb3\xb4\xeb\x8b\xa4 \xea\xb8\xb8\xec\x96\xb4\xec\xa7\x88 \xea\xb2\xbd\xec\x9a\xb0 \xeb\x92\xa4\xec\x97\x90 \xed\x86\xa0\xed\x81\xb0\xec\x9d\x84 \xec\x9e\x90\xeb\xa5\xb4\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\r\n        if len(sequence_index) > DEFINES.max_sequence_length:\r\n            sequence_index = sequence_index[:DEFINES.max_sequence_length]\r\n        # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xeb\xac\xb8\xec\x9e\xa5\xec\x97\x90 \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4\xec\xa3\xbc\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\r\n        sequences_length.append(len(sequence_index))\r\n        # max_sequence_length\xeb\xb3\xb4\xeb\x8b\xa4 \xeb\xac\xb8\xec\x9e\xa5 \xea\xb8\xb8\xec\x9d\xb4\xea\xb0\x80\r\n        # \xec\x9e\x91\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xb9\x88 \xeb\xb6\x80\xeb\xb6\x84\xec\x97\x90 PAD(0)\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4\xec\xa4\x80\xeb\x8b\xa4.\r\n        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\r\n        # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94 \xea\xb0\x92\xec\x9d\x84\r\n        # sequences_input_index\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\r\n        sequences_input_index.append(sequence_index)\r\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xec\x9d\xbc\xeb\xb0\x98 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x9c\xeb\x8b\xa4.\r\n    # \xec\x9d\xb4\xec\x9c\xa0\xeb\x8a\x94 \xed\x85\x90\xec\x84\x9c\xed\x94\x8c\xeb\xa1\x9c\xec\x9a\xb0 dataset\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa3\xbc\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c\r\n    # \xec\x82\xac\xec\xa0\x84 \xec\x9e\x91\xec\x97\x85\xec\x9d\xb4\xeb\x8b\xa4.\r\n    # \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xeb\xb0\xb0\xec\x97\xb4\xea\xb3\xbc\r\n    # \xea\xb7\xb8 \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\r\n    return np.asarray(sequences_input_index), sequences_length\r\n\r\n\r\n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xed\x95\xa0 value \xed\x82\xa4\xea\xb0\x80 \xec\x9b\x8c\xeb\x93\x9c \xec\x9d\xb4\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\xb4\r\n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb8 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xb0\x9b\xeb\x8a\x94\xeb\x8b\xa4.\r\ndef dec_output_processing(value, dictionary):\r\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8a\x94\r\n    # \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\xb4\xeb\x8b\xa4.(\xeb\x88\x84\xec\xa0\x81\xeb\x90\x9c\xeb\x8b\xa4)\r\n    sequences_output_index = []\r\n    # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5 \xeb\x90\x98\xeb\x8a\x94 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x98\r\n    # \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.(\xeb\x88\x84\xec\xa0\x81\xeb\x90\x9c\xeb\x8b\xa4)\r\n    sequences_length = []\r\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\r\n    if DEFINES.tokenize_as_morph:\r\n        value = prepro_like_morphlized(value)\r\n    # \xed\x95\x9c\xec\xa4\x84\xec\x94\xa9 \xeb\xb6\x88\xec\x96\xb4\xec\x98\xa8\xeb\x8b\xa4.\r\n    for sequence in value:\r\n        # FILTERS = ""([~.,!?\\""\':;)(])""\r\n        # \xec\xa0\x95\xea\xb7\x9c\xed\x99\x94\xeb\xa5\xbc \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xec\x97\xac \xed\x95\x84\xed\x84\xb0\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94\r\n        # \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 """" \xec\x9c\xbc\xeb\xa1\x9c \xec\xb9\x98\xed\x99\x98 \xed\x95\x9c\xeb\x8b\xa4.\r\n        sequence = re.sub(CHANGE_FILTER, """", sequence)\r\n        # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xed\x95\xa0\xeb\x95\x8c \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0\r\n        # \xec\x9e\x88\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\xb4\xeb\x8b\xa4.\r\n        sequence_index = []\r\n        # \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5\xec\x9d\x98 \xec\xb2\x98\xec\x9d\x8c\xec\x97\x90\xeb\x8a\x94 START\xea\xb0\x80 \xec\x99\x80\xec\x95\xbc \xed\x95\x98\xeb\xaf\x80\xeb\xa1\x9c\r\n        # \xea\xb7\xb8 \xea\xb0\x92\xec\x9d\x84 \xeb\x84\xa3\xec\x96\xb4 \xec\xa3\xbc\xea\xb3\xa0 \xec\x8b\x9c\xec\x9e\x91\xed\x95\x9c\xeb\x8b\xa4.\r\n        # \xeb\xac\xb8\xec\x9e\xa5\xec\x97\x90\xec\x84\x9c \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xeb\x8b\xa8\xec\x9c\x84\xeb\xb3\x84\xeb\xa1\x9c \xeb\x8b\xa8\xec\x96\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa0\xb8\xec\x99\x80\xec\x84\x9c \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x9d\x98\r\n        # \xea\xb0\x92\xec\x9d\xb8 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\r\n        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\r\n        # \xeb\xac\xb8\xec\x9e\xa5 \xec\xa0\x9c\xed\x95\x9c \xea\xb8\xb8\xec\x9d\xb4\xeb\xb3\xb4\xeb\x8b\xa4 \xea\xb8\xb8\xec\x96\xb4\xec\xa7\x88 \xea\xb2\xbd\xec\x9a\xb0 \xeb\x92\xa4\xec\x97\x90 \xed\x86\xa0\xed\x81\xb0\xec\x9d\x84 \xec\x9e\x90\xeb\xa5\xb4\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\r\n        if len(sequence_index) > DEFINES.max_sequence_length:\r\n            sequence_index = sequence_index[:DEFINES.max_sequence_length]\r\n        # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xeb\xac\xb8\xec\x9e\xa5\xec\x97\x90 \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4\xec\xa3\xbc\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\r\n        sequences_length.append(len(sequence_index))\r\n        # max_sequence_length\xeb\xb3\xb4\xeb\x8b\xa4 \xeb\xac\xb8\xec\x9e\xa5 \xea\xb8\xb8\xec\x9d\xb4\xea\xb0\x80\r\n        # \xec\x9e\x91\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xb9\x88 \xeb\xb6\x80\xeb\xb6\x84\xec\x97\x90 PAD(0)\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4\xec\xa4\x80\xeb\x8b\xa4.\r\n        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\r\n        # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94 \xea\xb0\x92\xec\x9d\x84\r\n        # sequences_output_index \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\r\n        sequences_output_index.append(sequence_index)\r\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xec\x9d\xbc\xeb\xb0\x98 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x9c\xeb\x8b\xa4.\r\n    # \xec\x9d\xb4\xec\x9c\xa0\xeb\x8a\x94 \xed\x85\x90\xec\x84\x9c\xed\x94\x8c\xeb\xa1\x9c\xec\x9a\xb0 dataset\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa3\xbc\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c\r\n    # \xec\x82\xac\xec\xa0\x84 \xec\x9e\x91\xec\x97\x85\xec\x9d\xb4\xeb\x8b\xa4.\r\n    # \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xeb\xb0\xb0\xec\x97\xb4\xea\xb3\xbc \xea\xb7\xb8 \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\r\n    return np.asarray(sequences_output_index), sequences_length\r\n\r\n\r\n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xed\x95\xa0 value\xec\x99\x80 \xed\x82\xa4\xea\xb0\x80 \xec\x9b\x8c\xeb\x93\x9c \xec\x9d\xb4\xea\xb3\xa0\r\n# \xea\xb0\x92\xec\x9d\xb4 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb8 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xb0\x9b\xeb\x8a\x94\xeb\x8b\xa4.\r\ndef dec_target_processing(value, dictionary):\r\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8a\x94\r\n    # \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\xb4\xeb\x8b\xa4.(\xeb\x88\x84\xec\xa0\x81\xeb\x90\x9c\xeb\x8b\xa4)\r\n    sequences_target_index = []\r\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\r\n    if DEFINES.tokenize_as_morph:\r\n        value = prepro_like_morphlized(value)\r\n    # \xed\x95\x9c\xec\xa4\x84\xec\x94\xa9 \xeb\xb6\x88\xec\x96\xb4\xec\x98\xa8\xeb\x8b\xa4.\r\n    for sequence in value:\r\n        # FILTERS = ""([~.,!?\\""\':;)(])""\r\n        # \xec\xa0\x95\xea\xb7\x9c\xed\x99\x94\xeb\xa5\xbc \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xec\x97\xac \xed\x95\x84\xed\x84\xb0\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94\r\n        # \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 """" \xec\x9c\xbc\xeb\xa1\x9c \xec\xb9\x98\xed\x99\x98 \xed\x95\x9c\xeb\x8b\xa4.\r\n        sequence = re.sub(CHANGE_FILTER, """", sequence)\r\n        # \xeb\xac\xb8\xec\x9e\xa5\xec\x97\x90\xec\x84\x9c \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xeb\x8b\xa8\xec\x9c\x84\xeb\xb3\x84\xeb\xa1\x9c \xeb\x8b\xa8\xec\x96\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa0\xb8\xec\x99\x80\xec\x84\x9c\r\n        # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x9d\x98 \xea\xb0\x92\xec\x9d\xb8 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\r\n        # \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5\xec\x9d\x98 \xeb\xa7\x88\xec\xa7\x80\xeb\xa7\x89\xec\x97\x90 END\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\r\n        sequence_index = [dictionary[word] for word in sequence.split()]\r\n        # \xeb\xac\xb8\xec\x9e\xa5 \xec\xa0\x9c\xed\x95\x9c \xea\xb8\xb8\xec\x9d\xb4\xeb\xb3\xb4\xeb\x8b\xa4 \xea\xb8\xb8\xec\x96\xb4\xec\xa7\x88 \xea\xb2\xbd\xec\x9a\xb0 \xeb\x92\xa4\xec\x97\x90 \xed\x86\xa0\xed\x81\xb0\xec\x9d\x84 \xec\x9e\x90\xeb\xa5\xb4\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\r\n        # \xea\xb7\xb8\xeb\xa6\xac\xea\xb3\xa0 END \xed\x86\xa0\xed\x81\xb0\xec\x9d\x84 \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4\r\n        if len(sequence_index) >= DEFINES.max_sequence_length:\r\n            sequence_index = sequence_index[:DEFINES.max_sequence_length - 1] + [dictionary[END]]\r\n        else:\r\n            sequence_index += [dictionary[END]]\r\n        # max_sequence_length\xeb\xb3\xb4\xeb\x8b\xa4 \xeb\xac\xb8\xec\x9e\xa5 \xea\xb8\xb8\xec\x9d\xb4\xea\xb0\x80\r\n        # \xec\x9e\x91\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xb9\x88 \xeb\xb6\x80\xeb\xb6\x84\xec\x97\x90 PAD(0)\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4\xec\xa4\x80\xeb\x8b\xa4.\r\n        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\r\n        # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94 \xea\xb0\x92\xec\x9d\x84\r\n        # sequences_target_index\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\r\n        sequences_target_index.append(sequence_index)\r\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xec\x9d\xbc\xeb\xb0\x98 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x9c\xeb\x8b\xa4.\r\n    # \xec\x9d\xb4\xec\x9c\xa0\xeb\x8a\x94 \xed\x85\x90\xec\x84\x9c\xed\x94\x8c\xeb\xa1\x9c\xec\x9a\xb0 dataset\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa3\xbc\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c \xec\x82\xac\xec\xa0\x84 \xec\x9e\x91\xec\x97\x85\xec\x9d\xb4\xeb\x8b\xa4.\r\n    # \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xeb\xb0\xb0\xec\x97\xb4\xea\xb3\xbc \xea\xb7\xb8 \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\r\n    return np.asarray(sequences_target_index)\r\n\r\n\r\n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc \xec\x8a\xa4\xed\x8a\xb8\xeb\xa7\x81\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\r\n# \xeb\xb0\x94\xea\xbe\xb8\xea\xb3\xa0\xec\x9e\x90 \xed\x95\x98\xeb\x8a\x94 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 value\xec\x99\x80 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc\r\n# \xed\x82\xa4\xeb\xa1\x9c \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xea\xb3\xa0 \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c \xeb\x8b\xa8\xec\x96\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8a\x94\r\n# \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xb0\x9b\xeb\x8a\x94\xeb\x8b\xa4.\r\ndef pred2string(value, dictionary):\r\n    # \xed\x85\x8d\xec\x8a\xa4\xed\x8a\xb8 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xeb\xb3\xb4\xea\xb4\x80\xed\x95\xa0 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xec\x84\xa0\xec\x96\xb8\xed\x95\x9c\xeb\x8b\xa4.\r\n    sentence_string = []\r\n    print(value)\r\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xeb\xb0\xb0\xec\x97\xb4 \xed\x95\x98\xeb\x82\x98\xeb\xa5\xbc \xea\xba\xbc\xeb\x82\xb4\xec\x84\x9c v\xec\x97\x90 \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\r\n    for v in value:\r\n        # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x97\x90 \xec\x9e\x88\xeb\x8a\x94 \xeb\x8b\xa8\xec\x96\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\xb4\xec\x84\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xeb\x8b\xb4\xeb\x8a\x94\xeb\x8b\xa4.\r\n        print(v[\'indexs\'])\r\n        for index in v[\'indexs\']:\r\n            print(index)\r\n        sentence_string = [dictionary[index] for index in v[\'indexs\']]\r\n\r\n    print(""***********************"")\r\n    print(sentence_string)\r\n    print(""***********************"")\r\n    answer = """"\r\n    # \xed\x8c\xa8\xeb\x94\xa9\xea\xb0\x92\xeb\x8f\x84 \xeb\x8b\xb4\xea\xb2\xa8 \xec\x9e\x88\xec\x9c\xbc\xeb\xaf\x80\xeb\xa1\x9c \xed\x8c\xa8\xeb\x94\xa9\xec\x9d\x80 \xeb\xaa\xa8\xeb\x91\x90 \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xec\xb2\x98\xeb\xa6\xac \xed\x95\x9c\xeb\x8b\xa4.\r\n    for word in sentence_string:\r\n        if word not in PAD and word not in END:\r\n            answer += word\r\n            answer += "" ""\r\n    # \xea\xb2\xb0\xea\xb3\xbc\xeb\xa5\xbc \xec\xb6\x9c\xeb\xa0\xa5\xed\x95\x9c\xeb\x8b\xa4.\r\n    print(answer)\r\n    return answer\r\n\r\n\r\ndef pred_next_string(value, dictionary):\r\n    # \xed\x85\x8d\xec\x8a\xa4\xed\x8a\xb8 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xeb\xb3\xb4\xea\xb4\x80\xed\x95\xa0 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xec\x84\xa0\xec\x96\xb8\xed\x95\x9c\xeb\x8b\xa4.\r\n    sentence_string = []\r\n    is_finished = False\r\n\r\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xeb\xb0\xb0\xec\x97\xb4 \xed\x95\x98\xeb\x82\x98\xeb\xa5\xbc \xea\xba\xbc\xeb\x82\xb4\xec\x84\x9c v\xec\x97\x90 \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\r\n    for v in value:\r\n        # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x97\x90 \xec\x9e\x88\xeb\x8a\x94 \xeb\x8b\xa8\xec\x96\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\xb4\xec\x84\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xeb\x8b\xb4\xeb\x8a\x94\xeb\x8b\xa4.\r\n        sentence_string = [dictionary[index] for index in v[\'indexs\']]\r\n\r\n    answer = """"\r\n    # \xed\x8c\xa8\xeb\x94\xa9\xea\xb0\x92\xeb\x8f\x84 \xeb\x8b\xb4\xea\xb2\xa8 \xec\x9e\x88\xec\x9c\xbc\xeb\xaf\x80\xeb\xa1\x9c \xed\x8c\xa8\xeb\x94\xa9\xec\x9d\x80 \xeb\xaa\xa8\xeb\x91\x90 \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xec\xb2\x98\xeb\xa6\xac \xed\x95\x9c\xeb\x8b\xa4.\r\n    for word in sentence_string:\r\n        if word == END:\r\n            is_finished = True\r\n            break\r\n\r\n        if word != PAD and word != END:\r\n            answer += word\r\n            answer += "" ""\r\n\r\n    # \xea\xb2\xb0\xea\xb3\xbc\xeb\xa5\xbc \xec\xb6\x9c\xeb\xa0\xa5\xed\x95\x9c\xeb\x8b\xa4.\r\n    return answer, is_finished\r\n\r\n\r\ndef rearrange(input, output, target):\r\n    features = {""input"": input, ""output"": output}\r\n    return features, target\r\n\r\n\r\n# \xed\x95\x99\xec\x8a\xb5\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4\xea\xb0\x80 \xeb\xb0\xb0\xec\xb9\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\r\ndef train_input_fn(train_input_enc, train_output_dec, train_target_dec, batch_size):\r\n    # Dataset\xec\x9d\x84 \xec\x83\x9d\xec\x84\xb1\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9c\xbc\xeb\xa1\x9c\xec\x8d\xa8 from_tensor_slices\xeb\xb6\x80\xeb\xb6\x84\xec\x9d\x80\r\n    # \xea\xb0\x81\xea\xb0\x81 \xed\x95\x9c \xeb\xac\xb8\xec\x9e\xa5\xec\x9c\xbc\xeb\xa1\x9c \xec\x9e\x90\xeb\xa5\xb8\xeb\x8b\xa4\xea\xb3\xa0 \xeb\xb3\xb4\xeb\xa9\xb4 \xeb\x90\x9c\xeb\x8b\xa4.\r\n    # train_input_enc, train_output_dec, train_target_dec\r\n    # 3\xea\xb0\x9c\xeb\xa5\xbc \xea\xb0\x81\xea\xb0\x81 \xed\x95\x9c\xeb\xac\xb8\xec\x9e\xa5\xec\x9c\xbc\xeb\xa1\x9c \xeb\x82\x98\xeb\x88\x88\xeb\x8b\xa4.\r\n    dataset = tf.data.Dataset.from_tensor_slices((train_input_enc, train_output_dec, train_target_dec))\r\n    # \xec\xa0\x84\xec\xb2\xb4 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xec\x8d\xa9\xeb\x8a\x94\xeb\x8b\xa4.\r\n    dataset = dataset.shuffle(buffer_size=len(train_input_enc))\r\n    # \xeb\xb0\xb0\xec\xb9\x98 \xec\x9d\xb8\xec\x9e\x90 \xea\xb0\x92\xec\x9d\xb4 \xec\x97\x86\xeb\x8b\xa4\xeb\xa9\xb4  \xec\x97\x90\xeb\x9f\xac\xeb\xa5\xbc \xeb\xb0\x9c\xec\x83\x9d \xec\x8b\x9c\xed\x82\xa8\xeb\x8b\xa4.\r\n    assert batch_size is not None, ""train batchSize must not be None""\r\n    # from_tensor_slices\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x82\x98\xeb\x88\x88\xea\xb2\x83\xec\x9d\x84\r\n    # \xeb\xb0\xb0\xec\xb9\x98\xed\x81\xac\xea\xb8\xb0 \xeb\xa7\x8c\xed\x81\xbc \xeb\xac\xb6\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\r\n    dataset = dataset.batch(batch_size, drop_remainder=True)\r\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb0\x81 \xec\x9a\x94\xec\x86\x8c\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4\xec\x84\x9c rearrange \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc\r\n    # \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xec\x9a\x94\xec\x86\x8c\xeb\xa5\xbc \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xa7\xb5\xec\x9c\xbc\xeb\xa1\x9c \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\r\n    dataset = dataset.map(rearrange)\r\n    # repeat()\xed\x95\xa8\xec\x88\x98\xec\x97\x90 \xec\x9b\x90\xed\x95\x98\xeb\x8a\x94 \xec\x97\x90\xed\x8f\xac\xed\x81\xac \xec\x88\x98\xeb\xa5\xbc \xeb\x84\xa3\xec\x9d\x84\xec\x88\x98 \xec\x9e\x88\xec\x9c\xbc\xeb\xa9\xb4\r\n    # \xec\x95\x84\xeb\xac\xb4 \xec\x9d\xb8\xec\x9e\x90\xeb\x8f\x84 \xec\x97\x86\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xac\xb4\xed\x95\x9c\xec\x9c\xbc\xeb\xa1\x9c \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0 \xeb\x90\x9c\xeb\x8b\xa4.\r\n    dataset = dataset.repeat()\r\n    # make_one_shot_iterator\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc\r\n    # \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\r\n    iterator = dataset.make_one_shot_iterator()\r\n    # \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x8b\xa4\xec\x9d\x8c \xed\x95\xad\xeb\xaa\xa9\xec\x9d\x98 \xed\x85\x90\xec\x84\x9c\r\n    # \xea\xb0\x9c\xec\xb2\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\r\n    return iterator.get_next()\r\n\r\n\r\n# \xed\x8f\x89\xea\xb0\x80\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4\xea\xb0\x80 \xeb\xb0\xb0\xec\xb9\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\r\ndef eval_input_fn(eval_input_enc, eval_output_dec, eval_target_dec, batch_size):\r\n    # Dataset\xec\x9d\x84 \xec\x83\x9d\xec\x84\xb1\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9c\xbc\xeb\xa1\x9c\xec\x8d\xa8 from_tensor_slices\xeb\xb6\x80\xeb\xb6\x84\xec\x9d\x80\r\n    # \xea\xb0\x81\xea\xb0\x81 \xed\x95\x9c \xeb\xac\xb8\xec\x9e\xa5\xec\x9c\xbc\xeb\xa1\x9c \xec\x9e\x90\xeb\xa5\xb8\xeb\x8b\xa4\xea\xb3\xa0 \xeb\xb3\xb4\xeb\xa9\xb4 \xeb\x90\x9c\xeb\x8b\xa4.\r\n    # eval_input_enc, eval_output_dec, eval_target_dec\r\n    # 3\xea\xb0\x9c\xeb\xa5\xbc \xea\xb0\x81\xea\xb0\x81 \xed\x95\x9c\xeb\xac\xb8\xec\x9e\xa5\xec\x9c\xbc\xeb\xa1\x9c \xeb\x82\x98\xeb\x88\x88\xeb\x8b\xa4.\r\n    dataset = tf.data.Dataset.from_tensor_slices((eval_input_enc, eval_output_dec, eval_target_dec))\r\n    # \xec\xa0\x84\xec\xb2\xb4 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xec\x84\x9e\xeb\x8a\x94\xeb\x8b\xa4.\r\n    dataset = dataset.shuffle(buffer_size=len(eval_input_enc))\r\n    # \xeb\xb0\xb0\xec\xb9\x98 \xec\x9d\xb8\xec\x9e\x90 \xea\xb0\x92\xec\x9d\xb4 \xec\x97\x86\xeb\x8b\xa4\xeb\xa9\xb4  \xec\x97\x90\xeb\x9f\xac\xeb\xa5\xbc \xeb\xb0\x9c\xec\x83\x9d \xec\x8b\x9c\xed\x82\xa8\xeb\x8b\xa4.\r\n    assert batch_size is not None, ""eval batchSize must not be None""\r\n    # from_tensor_slices\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x82\x98\xeb\x88\x88\xea\xb2\x83\xec\x9d\x84\r\n    # \xeb\xb0\xb0\xec\xb9\x98\xed\x81\xac\xea\xb8\xb0 \xeb\xa7\x8c\xed\x81\xbc \xeb\xac\xb6\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\r\n    dataset = dataset.batch(batch_size, drop_remainder=True)\r\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb0\x81 \xec\x9a\x94\xec\x86\x8c\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4\xec\x84\x9c rearrange \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc\r\n    # \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xec\x9a\x94\xec\x86\x8c\xeb\xa5\xbc \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xa7\xb5\xec\x9c\xbc\xeb\xa1\x9c \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\r\n    dataset = dataset.map(rearrange)\r\n    # repeat()\xed\x95\xa8\xec\x88\x98\xec\x97\x90 \xec\x9b\x90\xed\x95\x98\xeb\x8a\x94 \xec\x97\x90\xed\x8f\xac\xed\x81\xac \xec\x88\x98\xeb\xa5\xbc \xeb\x84\xa3\xec\x9d\x84\xec\x88\x98 \xec\x9e\x88\xec\x9c\xbc\xeb\xa9\xb4\r\n    # \xec\x95\x84\xeb\xac\xb4 \xec\x9d\xb8\xec\x9e\x90\xeb\x8f\x84 \xec\x97\x86\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xac\xb4\xed\x95\x9c\xec\x9c\xbc\xeb\xa1\x9c \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0 \xeb\x90\x9c\xeb\x8b\xa4.\r\n    # \xed\x8f\x89\xea\xb0\x80\xec\x9d\xb4\xeb\xaf\x80\xeb\xa1\x9c 1\xed\x9a\x8c\xeb\xa7\x8c \xeb\x8f\x99\xec\x9e\x91 \xec\x8b\x9c\xed\x82\xa8\xeb\x8b\xa4.\r\n    dataset = dataset.repeat(1)\r\n    # make_one_shot_iterator\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\r\n    # \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\r\n    iterator = dataset.make_one_shot_iterator()\r\n    # \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x8b\xa4\xec\x9d\x8c \xed\x95\xad\xeb\xaa\xa9\xec\x9d\x98\r\n    # \xed\x85\x90\xec\x84\x9c \xea\xb0\x9c\xec\xb2\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\r\n    return iterator.get_next()\r\n\r\n\r\ndef data_tokenizer(data):\r\n    # \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xed\x95\xb4\xec\x84\x9c \xeb\x8b\xb4\xec\x9d\x84 \xeb\xb0\xb0\xec\x97\xb4 \xec\x83\x9d\xec\x84\xb1\r\n    words = []\r\n    for sentence in data:\r\n        # FILTERS = ""([~.,!?\\""\':;)(])""\r\n        # \xec\x9c\x84 \xed\x95\x84\xed\x84\xb0\xec\x99\x80 \xea\xb0\x99\xec\x9d\x80 \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xec\xa0\x95\xea\xb7\x9c\xed\x99\x94 \xed\x91\x9c\xed\x98\x84\xec\x8b\x9d\xec\x9d\x84\r\n        # \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xeb\xaa\xa8\xeb\x91\x90 """" \xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98 \xed\x95\xb4\xec\xa3\xbc\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\r\n        sentence = re.sub(CHANGE_FILTER, """", sentence)\r\n        for word in sentence.split():\r\n            words.append(word)\r\n    # \xed\x86\xa0\xea\xb7\xb8\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95\xea\xb3\xbc \xec\xa0\x95\xea\xb7\x9c\xed\x91\x9c\xed\x98\x84\xec\x8b\x9d\xec\x9d\x84 \xed\x86\xb5\xed\x95\xb4 \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4\xec\xa7\x84\r\n    # \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xeb\x84\x98\xea\xb2\xa8 \xec\xa4\x80\xeb\x8b\xa4.\r\n    return [word for word in words if word]\r\n\r\n\r\ndef load_vocabulary():\r\n    # \xec\x82\xac\xec\xa0\x84\xec\x9d\x84 \xeb\x8b\xb4\xec\x9d\x84 \xeb\xb0\xb0\xec\x97\xb4 \xec\xa4\x80\xeb\xb9\x84\xed\x95\x9c\xeb\x8b\xa4.\r\n    vocabulary_list = []\r\n    # \xec\x82\xac\xec\xa0\x84\xec\x9d\x84 \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c \xed\x9b\x84 \xed\x8c\x8c\xec\x9d\xbc\xeb\xa1\x9c \xec\xa0\x80\xec\x9e\xa5 \xec\xa7\x84\xed\x96\x89\xed\x95\x9c\xeb\x8b\xa4.\r\n    # \xea\xb7\xb8 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x98 \xec\xa1\xb4\xec\x9e\xac \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xed\x99\x95\xec\x9d\xb8\xed\x95\x9c\xeb\x8b\xa4.\r\n    if (not (os.path.exists(DEFINES.vocabulary_path))):\r\n        # \xec\x9d\xb4\xeb\xaf\xb8 \xec\x83\x9d\xec\x84\xb1\xeb\x90\x9c \xec\x82\xac\xec\xa0\x84 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\xb4 \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xec\x9c\xbc\xeb\xaf\x80\xeb\xa1\x9c\r\n        # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4\xec\x95\xbc \xed\x95\x9c\xeb\x8b\xa4.\r\n        # \xea\xb7\xb8\xeb\x9e\x98\xec\x84\x9c \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xea\xb0\x80 \xec\xa1\xb4\xec\x9e\xac \xed\x95\x98\xeb\xa9\xb4 \xec\x82\xac\xec\xa0\x84\xec\x9d\x84 \xeb\xa7\x8c\xeb\x93\xa4\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c\r\n        # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x98 \xec\xa1\xb4\xec\x9e\xac \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xed\x99\x95\xec\x9d\xb8\xed\x95\x9c\xeb\x8b\xa4.\r\n        if (os.path.exists(DEFINES.data_path)):\r\n            # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xea\xb0\x80 \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xeb\x8b\x88 \xed\x8c\x90\xeb\x8b\xa8\xec\x8a\xa4\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c\r\n            # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xb6\x88\xeb\x9f\xac\xec\x98\xa4\xec\x9e\x90\r\n            data_df = pd.read_csv(DEFINES.data_path, encoding=\'utf-8\')\r\n            # \xed\x8c\x90\xeb\x8b\xa4\xec\x8a\xa4\xec\x9d\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xed\x94\x84\xeb\xa0\x88\xec\x9e\x84\xec\x9d\x84 \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c\r\n            # \xec\xa7\x88\xeb\xac\xb8\xea\xb3\xbc \xeb\x8b\xb5\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\x97\xb4\xec\x9d\x84 \xea\xb0\x80\xec\xa0\xb8 \xec\x98\xa8\xeb\x8b\xa4.\r\n            question, answer = list(data_df[\'Q\']), list(data_df[\'A\'])\r\n            if DEFINES.tokenize_as_morph:  # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c\xec\x97\x90 \xeb\x94\xb0\xeb\xa5\xb8 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa0\xb8 \xec\xb2\x98\xeb\xa6\xac\r\n                question = prepro_like_morphlized(question)\r\n                answer = prepro_like_morphlized(answer)\r\n            data = []\r\n            # \xec\xa7\x88\xeb\xac\xb8\xea\xb3\xbc \xeb\x8b\xb5\xeb\xb3\x80\xec\x9d\x84 extend\xec\x9d\x84\r\n            # \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xea\xb5\xac\xec\xa1\xb0\xea\xb0\x80 \xec\x97\x86\xeb\x8a\x94 \xeb\xb0\xb0\xec\x97\xb4\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\r\n            data.extend(question)\r\n            data.extend(answer)\r\n            # \xed\x86\xa0\xed\x81\xb0\xeb\x82\x98\xec\x9d\xb4\xec\xa0\xb8 \xec\xb2\x98\xeb\xa6\xac \xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\r\n            words = data_tokenizer(data)\r\n            # \xea\xb3\xb5\xed\x86\xb5\xec\xa0\x81\xec\x9d\xb8 \xeb\x8b\xa8\xec\x96\xb4\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4\xec\x84\x9c\xeb\x8a\x94 \xeb\xaa\xa8\xeb\x91\x90\r\n            # \xed\x95\x84\xec\x9a\x94 \xec\x97\x86\xec\x9c\xbc\xeb\xaf\x80\xeb\xa1\x9c \xed\x95\x9c\xea\xb0\x9c\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xec\xa3\xbc\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c\r\n            # set\xed\x95\xb4\xec\xa3\xbc\xea\xb3\xa0 \xec\x9d\xb4\xea\xb2\x83\xeb\x93\xa4\xec\x9d\x84 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\r\n            words = list(set(words))\r\n            # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\x97\x86\xeb\x8a\x94 \xeb\x82\xb4\xec\x9a\xa9\xec\xa4\x91\xec\x97\x90 MARKER\xeb\xa5\xbc \xec\x82\xac\xec\xa0\x84\xec\x97\x90\r\n            # \xec\xb6\x94\xea\xb0\x80 \xed\x95\x98\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c \xec\x95\x84\xeb\x9e\x98\xec\x99\x80 \xea\xb0\x99\xec\x9d\xb4 \xec\xb2\x98\xeb\xa6\xac \xed\x95\x9c\xeb\x8b\xa4.\r\n            # \xec\x95\x84\xeb\x9e\x98\xeb\x8a\x94 MARKER \xea\xb0\x92\xec\x9d\xb4\xeb\xa9\xb0 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xec\x9d\x98 \xec\xb2\xab\xeb\xb2\x88\xec\xa7\xb8 \xeb\xb6\x80\xed\x84\xb0\r\n            # \xec\x88\x9c\xec\x84\x9c\xeb\x8c\x80\xeb\xa1\x9c \xeb\x84\xa3\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 0\xec\x97\x90 \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\r\n            # PAD = ""<PADDING>""\r\n            # STD = ""<START>""\r\n            # END = ""<END>""\r\n            # UNK = ""<UNKNWON>""\r\n            words[:0] = MARKER\r\n        # \xec\x82\xac\xec\xa0\x84\xec\x9d\x84 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa4\xec\x97\x88\xec\x9c\xbc\xeb\x8b\x88 \xec\x9d\xb4 \xeb\x82\xb4\xec\x9a\xa9\xec\x9d\x84\r\n        # \xec\x82\xac\xec\xa0\x84 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.\r\n        with open(DEFINES.vocabulary_path, \'w\', encoding=\'utf-8\') as vocabulary_file:\r\n            for word in words:\r\n                vocabulary_file.write(word + \'\\n\')\r\n\r\n    # \xec\x82\xac\xec\xa0\x84 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\xb4 \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xeb\xa9\xb4 \xec\x97\xac\xea\xb8\xb0\xec\x97\x90\xec\x84\x9c\r\n    # \xea\xb7\xb8 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xeb\xb6\x88\xeb\x9f\xac\xec\x84\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\r\n    with open(DEFINES.vocabulary_path, \'r\', encoding=\'utf-8\') as vocabulary_file:\r\n        for line in vocabulary_file:\r\n            vocabulary_list.append(line.strip())\r\n\r\n    # \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xeb\x82\xb4\xec\x9a\xa9\xec\x9d\x84 \xed\x82\xa4\xec\x99\x80 \xea\xb0\x92\xec\x9d\xb4 \xec\x9e\x88\xeb\x8a\x94\r\n    # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac \xea\xb5\xac\xec\xa1\xb0\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\r\n    char2idx, idx2char = make_vocabulary(vocabulary_list)\r\n    # \xeb\x91\x90\xea\xb0\x80\xec\xa7\x80 \xed\x98\x95\xed\x83\x9c\xec\x9d\x98 \xed\x82\xa4\xec\x99\x80 \xea\xb0\x92\xec\x9d\xb4 \xec\x9e\x88\xeb\x8a\x94 \xed\x98\x95\xed\x83\x9c\xeb\xa5\xbc \xeb\xa6\xac\xed\x84\xb4\xed\x95\x9c\xeb\x8b\xa4.\r\n    # (\xec\x98\x88) \xeb\x8b\xa8\xec\x96\xb4: \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 , \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4: \xeb\x8b\xa8\xec\x96\xb4)\r\n    return char2idx, idx2char, len(char2idx)\r\n\r\n\r\ndef make_vocabulary(vocabulary_list):\r\n    # \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xed\x82\xa4\xea\xb0\x80 \xeb\x8b\xa8\xec\x96\xb4\xec\x9d\xb4\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\xb4 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb8\r\n    # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\r\n    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\r\n    # \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xed\x82\xa4\xea\xb0\x80 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb4\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\xb4 \xeb\x8b\xa8\xec\x96\xb4\xec\x9d\xb8\r\n    # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\r\n    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\r\n    # \xeb\x91\x90\xea\xb0\x9c\xec\x9d\x98 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8 \xec\xa4\x80\xeb\x8b\xa4.\r\n    return char2idx, idx2char\r\n\r\n\r\ndef main(self):\r\n    char2idx, idx2char, vocabulary_length = load_vocabulary()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    tf.app.run(main)\r\n'"
6.CHATBOT/Appendix-transformer/main.py,0,"b'import tensorflow as tf\r\nimport model as ml\r\nimport data\r\nimport numpy as np\r\nimport os\r\nimport sys\r\n\r\nfrom configs import DEFINES\r\n\r\nDATA_OUT_PATH = \'./data_out/\'\r\n\r\n\r\ndef main(self):\r\n    data_out_path = os.path.join(os.getcwd(), DATA_OUT_PATH)\r\n    os.makedirs(data_out_path, exist_ok=True)\r\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xed\x86\xb5\xed\x95\x9c \xec\x82\xac\xec\xa0\x84 \xea\xb5\xac\xec\x84\xb1 \xed\x95\x9c\xeb\x8b\xa4.\r\n    char2idx, idx2char, vocabulary_length = data.load_vocabulary()\r\n    # \xed\x9b\x88\xeb\xa0\xa8 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xec\x99\x80 \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xea\xb0\x80\xec\xa0\xb8\xec\x98\xa8\xeb\x8b\xa4.\r\n    train_input, train_label, eval_input, eval_label = data.load_data()\r\n\r\n    # \xed\x9b\x88\xeb\xa0\xa8\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\r\n    train_input_enc, train_input_enc_length = data.enc_processing(train_input, char2idx)\r\n    # \xed\x9b\x88\xeb\xa0\xa8\xec\x85\x8b \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5 \xeb\xb6\x80\xeb\xb6\x84 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\r\n    train_output_dec, train_output_dec_length = data.dec_output_processing(train_label, char2idx)\r\n    # \xed\x9b\x88\xeb\xa0\xa8\xec\x85\x8b \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\xb6\x80\xeb\xb6\x84 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\r\n    train_target_dec = data.dec_target_processing(train_label, char2idx)\r\n\r\n    # \xed\x8f\x89\xea\xb0\x80\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\r\n    eval_input_enc, eval_input_enc_length = data.enc_processing(eval_input, char2idx)\r\n    # \xed\x8f\x89\xea\xb0\x80\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\r\n    eval_output_dec, eval_output_dec_length = data.dec_output_processing(eval_label, char2idx)\r\n    # \xed\x8f\x89\xea\xb0\x80\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\r\n    eval_target_dec = data.dec_target_processing(eval_label, char2idx)\r\n\r\n    # \xed\x98\x84\xec\x9e\xac \xea\xb2\xbd\xeb\xa1\x9c\'./\'\xec\x97\x90 \xed\x98\x84\xec\x9e\xac \xea\xb2\xbd\xeb\xa1\x9c \xed\x95\x98\xeb\xb6\x80\xec\x97\x90\r\n    # \xec\xb2\xb4\xed\x81\xac \xed\x8f\xac\xec\x9d\xb8\xed\x8a\xb8\xeb\xa5\xbc \xec\xa0\x80\xec\x9e\xa5\xed\x95\x9c \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\r\n    check_point_path = os.path.join(os.getcwd(), DEFINES.check_point_path)\r\n    # \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\xa9\xb0 \xeb\x91\x90\xeb\xb2\x88\xec\xa7\xb8 \xec\x9d\xb8\xec\x9e\x90 exist_ok\xea\xb0\x80\r\n    # True\xec\x9d\xb4\xeb\xa9\xb4 \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xea\xb0\x80 \xec\x9d\xb4\xeb\xaf\xb8 \xec\xa1\xb4\xec\x9e\xac\xed\x95\xb4\xeb\x8f\x84 OSError\xea\xb0\x80\r\n    # \xeb\xb0\x9c\xec\x83\x9d\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94\xeb\x8b\xa4.\r\n    # exist_ok\xea\xb0\x80 False\xec\x9d\xb4\xeb\xa9\xb4 \xec\x9d\xb4\xeb\xaf\xb8 \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xeb\xa9\xb4\r\n    # OSError\xea\xb0\x80 \xeb\xb0\x9c\xec\x83\x9d\xed\x95\x9c\xeb\x8b\xa4.\r\n    os.makedirs(check_point_path, exist_ok=True)\r\n\r\n    # \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0 \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\r\n    classifier = tf.estimator.Estimator(\r\n        model_fn=ml.Model,  # \xeb\xaa\xa8\xeb\x8d\xb8 \xeb\x93\xb1\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\r\n        model_dir=DEFINES.check_point_path,  # \xec\xb2\xb4\xed\x81\xac\xed\x8f\xac\xec\x9d\xb8\xed\x8a\xb8 \xec\x9c\x84\xec\xb9\x98 \xeb\x93\xb1\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\r\n        params={  # \xeb\xaa\xa8\xeb\x8d\xb8 \xec\xaa\xbd\xec\x9c\xbc\xeb\xa1\x9c \xed\x8c\x8c\xeb\x9d\xbc\xeb\xa9\x94\xed\x84\xb0 \xec\xa0\x84\xeb\x8b\xac\xed\x95\x9c\xeb\x8b\xa4.\r\n            \'embedding_size\': DEFINES.embedding_size,\r\n            \'model_hidden_size\': DEFINES.model_hidden_size,  # \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\r\n            \'ffn_hidden_size\': DEFINES.ffn_hidden_size,\r\n            \'attention_head_size\': DEFINES.attention_head_size,\r\n            \'learning_rate\': DEFINES.learning_rate,  # \xed\x95\x99\xec\x8a\xb5\xec\x9c\xa8 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\r\n            \'vocabulary_length\': vocabulary_length,  # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac \xed\x81\xac\xea\xb8\xb0\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\r\n            \'embedding_size\': DEFINES.embedding_size,  # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xed\x81\xac\xea\xb8\xb0\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\r\n            \'layer_size\': DEFINES.layer_size,\r\n            \'max_sequence_length\': DEFINES.max_sequence_length,\r\n            \'xavier_initializer\': DEFINES.xavier_initializer\r\n        })\r\n\r\n    # \xed\x95\x99\xec\x8a\xb5 \xec\x8b\xa4\xed\x96\x89\r\n    classifier.train(input_fn=lambda: data.train_input_fn(\r\n        train_input_enc, train_output_dec, train_target_dec, DEFINES.batch_size), steps=DEFINES.train_steps)\r\n\r\n    eval_result = classifier.evaluate(input_fn=lambda: data.eval_input_fn(\r\n        eval_input_enc, eval_output_dec, eval_target_dec, DEFINES.batch_size))\r\n\r\n    print(\'\\nEVAL set accuracy: {accuracy:0.3f}\\n\'.format(**eval_result))\r\n\r\n    # \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8\xec\x9a\xa9 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\r\n    # \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xb6\x80\xeb\xb6\x84 \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\r\n    predic_input_enc, predic_input_enc_length = data.enc_processing([""\xea\xb0\x80\xeb\x81\x94 \xea\xb6\x81\xea\xb8\x88\xed\x95\xb4""], char2idx)\r\n    # \xed\x95\x99\xec\x8a\xb5 \xea\xb3\xbc\xec\xa0\x95\xec\x9d\xb4 \xec\x95\x84\xeb\x8b\x88\xeb\xaf\x80\xeb\xa1\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5\xec\x9d\x80\r\n    # \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94\xeb\x8b\xa4.(\xea\xb5\xac\xec\xa1\xb0\xeb\xa5\xbc \xeb\xa7\x9e\xec\xb6\x94\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.)\r\n    predic_output_dec, predic_output_decLength = data.dec_output_processing([""""], char2idx)\r\n    # \xed\x95\x99\xec\x8a\xb5 \xea\xb3\xbc\xec\xa0\x95\xec\x9d\xb4 \xec\x95\x84\xeb\x8b\x88\xeb\xaf\x80\xeb\xa1\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\xb6\x80\xeb\xb6\x84\xeb\x8f\x84\r\n    # \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94\xeb\x8b\xa4.(\xea\xb5\xac\xec\xa1\xb0\xeb\xa5\xbc \xeb\xa7\x9e\xec\xb6\x94\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.)\r\n    predic_target_dec = data.dec_target_processing([""""], char2idx)\r\n\r\n    predictions = classifier.predict(\r\n        input_fn=lambda: data.eval_input_fn(predic_input_enc, predic_output_dec, predic_target_dec, 1))\r\n\r\n    answer, finished = data.pred_next_string(predictions, idx2char)\r\n\r\n    # \xec\x98\x88\xec\xb8\xa1\xed\x95\x9c \xea\xb0\x92\xec\x9d\x84 \xec\x9d\xb8\xec\xa7\x80 \xed\x95\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8f\x84\xeb\xa1\x9d\r\n    # \xed\x85\x8d\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\r\n    print(""answer: "", answer)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    tf.app.run(main)\r\n\r\ntf.logging.set_verbosity\r\n'"
6.CHATBOT/Appendix-transformer/model.py,3,"b'# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nimport sys\r\n\r\nfrom configs import DEFINES\r\nimport numpy as np\r\n\r\n\r\ndef layer_norm(inputs, eps=1e-6):\r\n    # LayerNorm(x + Sublayer(x))\r\n    feature_shape = inputs.get_shape()[-1:]\r\n    #  \xed\x8f\x89\xea\xb7\xa0\xea\xb3\xbc \xed\x91\x9c\xec\xa4\x80\xed\x8e\xb8\xec\xb0\xa8\xec\x9d\x84 \xeb\x84\x98\xea\xb2\xa8 \xec\xa4\x80\xeb\x8b\xa4.\r\n    mean = tf.keras.backend.mean(inputs, [-1], keepdims=True)\r\n    std = tf.keras.backend.std(inputs, [-1], keepdims=True)\r\n    beta = tf.get_variable(""beta"", initializer=tf.zeros(feature_shape))\r\n    gamma = tf.get_variable(""gamma"", initializer=tf.ones(feature_shape))\r\n\r\n    return gamma * (inputs - mean) / (std + eps) + beta\r\n\r\n\r\ndef sublayer_connection(inputs, sublayer, dropout=0.2):\r\n    outputs = layer_norm(inputs + tf.keras.layers.Dropout(dropout)(sublayer))\r\n    return outputs\r\n\r\n\r\ndef positional_encoding(dim, sentence_length):\r\n    encoded_vec = np.array([pos/np.power(10000, 2*i/dim)\r\n                            for pos in range(sentence_length) for i in range(dim)])\r\n\r\n    encoded_vec[::2] = np.sin(encoded_vec[::2])\r\n    encoded_vec[1::2] = np.cos(encoded_vec[1::2])\r\n\r\n    return tf.constant(encoded_vec.reshape([sentence_length, dim]), dtype=tf.float32)\r\n\r\n\r\nclass MultiHeadAttention(tf.keras.Model):\r\n    def __init__(self, num_units, heads, masked=False):\r\n        super(MultiHeadAttention, self).__init__()\r\n\r\n        self.heads = heads\r\n        self.masked = masked\r\n\r\n        self.query_dense = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\r\n        self.key_dense = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\r\n        self.value_dense = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\r\n\r\n        self.dense = tf.keras.layers.Dense(num_units)\r\n\r\n    def scaled_dot_product_attention(self, query, key, value, masked=False):\r\n        key_seq_length = float(key.get_shape().as_list()[-1])\r\n        key = tf.transpose(key, perm=[0, 2, 1])\r\n        outputs = tf.matmul(query, key) / tf.sqrt(key_seq_length)\r\n\r\n        if masked:\r\n            diag_vals = tf.ones_like(outputs[0, :, :])\r\n            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()\r\n            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])\r\n\r\n            paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\r\n            outputs = tf.where(tf.equal(masks, 0), paddings, outputs)\r\n\r\n        attention_map = tf.nn.softmax(outputs)\r\n\r\n        return tf.matmul(attention_map, value)\r\n\r\n    def call(self, query, key, value):\r\n        query = self.query_dense(query)\r\n        key = self.key_dense(key)\r\n        value = self.value_dense(value)\r\n\r\n        query = tf.concat(tf.split(query, self.heads, axis=-1), axis=0)\r\n        key = tf.concat(tf.split(key, self.heads, axis=-1), axis=0)\r\n        value = tf.concat(tf.split(value, self.heads, axis=-1), axis=0)\r\n\r\n        attention_map = self.scaled_dot_product_attention(query, key, value, self.masked)\r\n\r\n        attn_outputs = tf.concat(tf.split(attention_map, self.heads, axis=0), axis=-1)\r\n        attn_outputs = self.dense(attn_outputs) \r\n\r\n        return attn_outputs\r\n\r\n\r\nclass Encoder(tf.keras.Model):\r\n    def __init__(self, model_dims, ffn_dims, attn_heads, num_layers=1):\r\n        super(Encoder, self).__init__()\r\n\r\n        self.self_attention = [MultiHeadAttention(model_dims, attn_heads) for _ in range(num_layers)]\r\n        self.position_feedforward = [PositionWiseFeedForward(ffn_dims, model_dims) for _ in range(num_layers)]\r\n\r\n    def call(self, inputs):\r\n        output_layer = None\r\n\r\n        for i, (s_a, p_f) in enumerate(zip(self.self_attention, self.position_feedforward)):\r\n            with tf.variable_scope(\'encoder_layer_\' + str(i + 1)):\r\n                attention_layer = sublayer_connection(inputs, s_a(inputs, inputs, inputs))\r\n                output_layer = sublayer_connection(attention_layer, p_f(attention_layer))\r\n\r\n                inputs = output_layer\r\n\r\n        return output_layer\r\n\r\n\r\nclass Decoder(tf.keras.Model):\r\n    def __init__(self, model_dims, ffn_dims, attn_heads, num_layers=1):\r\n        super(Decoder, self).__init__()\r\n\r\n        self.self_attention = [MultiHeadAttention(model_dims, attn_heads, masked=True) for _ in range(num_layers)]\r\n        self.encoder_decoder_attention = [MultiHeadAttention(model_dims, attn_heads) for _ in range(num_layers)]\r\n        self.position_feedforward = [PositionWiseFeedForward(ffn_dims, model_dims) for _ in range(num_layers)]\r\n\r\n    def call(self, inputs, encoder_outputs):\r\n        output_layer = None\r\n\r\n        for i, (s_a, ed_a, p_f) in enumerate(zip(self.self_attention, self.encoder_decoder_attention, self.position_feedforward)):\r\n            with tf.variable_scope(\'decoder_layer_\' + str(i + 1)):\r\n                masked_attention_layer = sublayer_connection(inputs, s_a(inputs, inputs, inputs))\r\n                attention_layer = sublayer_connection(masked_attention_layer, ed_a(masked_attention_layer,\r\n                                                                                           encoder_outputs,\r\n                                                                                           encoder_outputs))\r\n                output_layer = sublayer_connection(attention_layer, p_f(attention_layer))\r\n                inputs = output_layer\r\n\r\n        return output_layer\r\n\r\n\r\nclass PositionWiseFeedForward(tf.keras.Model):\r\n    def __init__(self, num_units, feature_shape):\r\n        super(PositionWiseFeedForward, self).__init__()\r\n\r\n        self.inner_dense = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\r\n        self.output_dense = tf.keras.layers.Dense(feature_shape)\r\n\r\n    def call(self, inputs):\r\n        inner_layer = self.inner_dense(inputs)\r\n        outputs = self.output_dense(inner_layer)\r\n\r\n        return outputs\r\n\r\n\r\ndef Model(features, labels, mode, params):\r\n    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\r\n    EVAL = mode == tf.estimator.ModeKeys.EVAL\r\n    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\r\n\r\n    position_encode = positional_encoding(params[\'embedding_size\'], params[\'max_sequence_length\'])\r\n\r\n    embedding = tf.keras.layers.Embedding(params[\'vocabulary_length\'],\r\n                                          params[\'embedding_size\'])\r\n\r\n    encoder_layers = Encoder(params[\'model_hidden_size\'], params[\'ffn_hidden_size\'],\r\n                      params[\'attention_head_size\'], params[\'layer_size\'])\r\n\r\n    decoder_layers = Decoder(params[\'model_hidden_size\'], params[\'ffn_hidden_size\'],\r\n                      params[\'attention_head_size\'], params[\'layer_size\'])\r\n\r\n    logit_layer = tf.keras.layers.Dense(params[\'vocabulary_length\'])\r\n\r\n    with tf.variable_scope(\'encoder\', reuse=tf.AUTO_REUSE):\r\n        x_embedded_matrix = embedding(features[\'input\']) + position_encode\r\n        encoder_outputs = encoder_layers(x_embedded_matrix)\r\n\r\n    loop_count = params[\'max_sequence_length\'] if PREDICT else 1\r\n\r\n    predict, output, logits = None, None, None\r\n\r\n    for i in range(loop_count):\r\n        with tf.variable_scope(\'decoder\', reuse=tf.AUTO_REUSE):\r\n            if i > 0:\r\n                output = tf.concat([tf.ones((output.shape[0], 1), dtype=tf.int64), predict[:, :-1]], axis=-1)\r\n            else:\r\n                output = features[\'output\']\r\n\r\n            y_embedded_matrix = embedding(output) + position_encode\r\n            decoder_outputs = decoder_layers(y_embedded_matrix, encoder_outputs)\r\n\r\n            logits = logit_layer(decoder_outputs)\r\n            predict = tf.argmax(logits, 2)\r\n\r\n    if PREDICT:\r\n        predictions = {\r\n            \'indexs\': predict,\r\n            \'logits\': logits,\r\n        }\r\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\r\n    accuracy = tf.metrics.accuracy(labels=labels, predictions=predict, name=\'accOp\')\r\n\r\n    metrics = {\'accuracy\': accuracy}\r\n    tf.summary.scalar(\'accuracy\', accuracy[1])\r\n\r\n    if EVAL:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\r\n\r\n    assert TRAIN\r\n\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=params[\'learning_rate\'])\r\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n'"
6.CHATBOT/Appendix-transformer/predict.py,0,"b'import tensorflow as tf\r\nimport data\r\nimport os\r\nimport sys\r\nimport model as ml\r\n\r\nfrom configs import DEFINES\r\n\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.logging.set_verbosity(tf.logging.ERROR)\r\n    arg_length = len(sys.argv)\r\n\r\n    if (arg_length < 2):\r\n        raise Exception(""Don\'t call us. We\'ll call you"")\r\n\r\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xed\x86\xb5\xed\x95\x9c \xec\x82\xac\xec\xa0\x84 \xea\xb5\xac\xec\x84\xb1 \xed\x95\x9c\xeb\x8b\xa4.\r\n    char2idx, idx2char, vocabulary_length = data.load_vocabulary()\r\n\r\n    # \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8\xec\x9a\xa9 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\r\n    # \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xb6\x80\xeb\xb6\x84 \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\r\n    input = """"\r\n    for i in sys.argv[1:]:\r\n        input += i\r\n        input += "" ""\r\n\r\n    print(input)\r\n    predic_input_enc, predic_input_enc_length = data.enc_processing([input], char2idx)\r\n    # \xed\x95\x99\xec\x8a\xb5 \xea\xb3\xbc\xec\xa0\x95\xec\x9d\xb4 \xec\x95\x84\xeb\x8b\x88\xeb\xaf\x80\xeb\xa1\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5\xec\x9d\x80\r\n    # \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94\xeb\x8b\xa4.(\xea\xb5\xac\xec\xa1\xb0\xeb\xa5\xbc \xeb\xa7\x9e\xec\xb6\x94\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.)\r\n    predic_output_dec, predic_output_dec_length = data.dec_output_processing([""""], char2idx)\r\n    # \xed\x95\x99\xec\x8a\xb5 \xea\xb3\xbc\xec\xa0\x95\xec\x9d\xb4 \xec\x95\x84\xeb\x8b\x88\xeb\xaf\x80\xeb\xa1\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\xb6\x80\xeb\xb6\x84\xeb\x8f\x84\r\n    # \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94\xeb\x8b\xa4.(\xea\xb5\xac\xec\xa1\xb0\xeb\xa5\xbc \xeb\xa7\x9e\xec\xb6\x94\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.)\r\n    predic_target_dec = data.dec_target_processing([""""], char2idx)\r\n\r\n    # \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0 \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\r\n    classifier = tf.estimator.Estimator(\r\n        model_fn=ml.Model,  # \xeb\xaa\xa8\xeb\x8d\xb8 \xeb\x93\xb1\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\r\n        model_dir=DEFINES.check_point_path,  # \xec\xb2\xb4\xed\x81\xac\xed\x8f\xac\xec\x9d\xb8\xed\x8a\xb8 \xec\x9c\x84\xec\xb9\x98 \xeb\x93\xb1\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\r\n        params={  # \xeb\xaa\xa8\xeb\x8d\xb8 \xec\xaa\xbd\xec\x9c\xbc\xeb\xa1\x9c \xed\x8c\x8c\xeb\x9d\xbc\xeb\xa9\x94\xed\x84\xb0 \xec\xa0\x84\xeb\x8b\xac\xed\x95\x9c\xeb\x8b\xa4.\r\n            \'embedding_size\': DEFINES.embedding_size,\r\n            \'model_hidden_size\': DEFINES.model_hidden_size,  # \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\r\n            \'ffn_hidden_size\': DEFINES.ffn_hidden_size,\r\n            \'attention_head_size\': DEFINES.attention_head_size,\r\n            \'learning_rate\': DEFINES.learning_rate,  # \xed\x95\x99\xec\x8a\xb5\xec\x9c\xa8 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\r\n            \'vocabulary_length\': vocabulary_length,  # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac \xed\x81\xac\xea\xb8\xb0\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\r\n            \'embedding_size\': DEFINES.embedding_size,  # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xed\x81\xac\xea\xb8\xb0\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\r\n            \'layer_size\': DEFINES.layer_size,\r\n            \'max_sequence_length\': DEFINES.max_sequence_length,\r\n            \'xavier_initializer\': DEFINES.xavier_initializer\r\n        })\r\n\r\n    # for i in range(DEFINES.max_sequence_length):\r\n    #     if i > 0:\r\n    #         predic_output_dec, predic_output_decLength = data.dec_output_processing([answer], char2idx)\r\n    #         predic_target_dec = data.dec_target_processing([answer], char2idx)\r\n    #     # \xec\x98\x88\xec\xb8\xa1\xec\x9d\x84 \xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\r\n    #     predictions = classifier.predict(\r\n    #         input_fn=lambda: data.eval_input_fn(predic_input_enc, predic_output_dec, predic_target_dec, 1))\r\n    #\r\n    #     answer, finished = data.pred_next_string(predictions, idx2char)\r\n    #     print(answer)\r\n    #     if finished:\r\n    #         break\r\n\r\n    # predic_output_dec, predic_output_dec_length = data.dec_output_processing([""""], char2idx)\r\n    # predic_target_dec = data.dec_target_processing([""""], char2idx)\r\n    # \xec\x98\x88\xec\xb8\xa1\xec\x9d\x84 \xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\r\n    predictions = classifier.predict(input_fn=lambda: data.eval_input_fn(predic_input_enc, predic_output_dec, predic_target_dec, 1))\r\n\r\n    answer, finished = data.pred_next_string(predictions, idx2char)\r\n\r\n    # \xec\x98\x88\xec\xb8\xa1\xed\x95\x9c \xea\xb0\x92\xec\x9d\x84 \xec\x9d\xb8\xec\xa7\x80 \xed\x95\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8f\x84\xeb\xa1\x9d\r\n    # \xed\x85\x8d\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\r\n    print(""answer: "", answer)\r\n'"
6.CHATBOT/Appendix/configs.py,0,"b""#*- coding: utf-8 -*-\nimport tensorflow as tf\n\ntf.app.flags.DEFINE_integer('batch_size', 64, 'batch size') # \xeb\xb0\xb0\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0\ntf.app.flags.DEFINE_integer('train_steps', 20000, 'train steps') # \xed\x95\x99\xec\x8a\xb5 \xec\x97\x90\xed\x8f\xac\xed\x81\xac\ntf.app.flags.DEFINE_float('dropout_width', 0.8, 'dropout width') # \xeb\x93\x9c\xeb\xa1\xad\xec\x95\x84\xec\x9b\x83 \xed\x81\xac\xea\xb8\xb0\ntf.app.flags.DEFINE_integer('layer_size', 1, 'layer size') # \xeb\xa9\x80\xed\x8b\xb0 \xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4 \xed\x81\xac\xea\xb8\xb0 (multi rnn)\ntf.app.flags.DEFINE_integer('hidden_size', 128, 'weights size') # \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0\ntf.app.flags.DEFINE_float('learning_rate', 1e-3, 'learning rate') # \xed\x95\x99\xec\x8a\xb5\xeb\xa5\xa0\ntf.app.flags.DEFINE_float('teacher_forcing_rate', 0.7, 'teacher forcing rate') # \xed\x95\x99\xec\x8a\xb5\xec\x8b\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x8d\x94 \xec\x9d\xb8\xed\x92\x8b \xec\xa0\x95\xeb\x8b\xb5 \xec\xa7\x80\xec\x9b\x90\xec\x9c\xa8\ntf.app.flags.DEFINE_string('data_path', '../data_in/ChatBotData.csv', 'data path') #  \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\x9c\x84\xec\xb9\x98\ntf.app.flags.DEFINE_string('vocabulary_path', './data_out/vocabularyData.voc', 'vocabulary path') # \xec\x82\xac\xec\xa0\x84 \xec\x9c\x84\xec\xb9\x98\ntf.app.flags.DEFINE_string('check_point_path', './data_out/check_point', 'check point path') # \xec\xb2\xb4\xed\x81\xac \xed\x8f\xac\xec\x9d\xb8\xed\x8a\xb8 \xec\x9c\x84\xec\xb9\x98\ntf.app.flags.DEFINE_string('save_model_path', './data_out/model', 'save model') # \xeb\xaa\xa8\xeb\x8d\xb8 \xec\xa0\x80\xec\x9e\xa5 \xea\xb2\xbd\xeb\xa1\x9c\ntf.app.flags.DEFINE_integer('shuffle_seek', 1000, 'shuffle random seek') # \xec\x85\x94\xed\x94\x8c \xec\x8b\x9c\xeb\x93\x9c\xea\xb0\x92\ntf.app.flags.DEFINE_integer('max_sequence_length', 25, 'max sequence length') # \xec\x8b\x9c\xed\x80\x80\xec\x8a\xa4 \xea\xb8\xb8\xec\x9d\xb4\ntf.app.flags.DEFINE_integer('embedding_size', 128, 'embedding size') # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xed\x81\xac\xea\xb8\xb0\ntf.app.flags.DEFINE_boolean('embedding', True, 'Use Embedding flag') # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xec\x9c\xa0\xeb\xac\xb4 \xec\x84\xa4\xec\xa0\x95\ntf.app.flags.DEFINE_boolean('multilayer', True, 'Use Multi RNN Cell') # \xeb\xa9\x80\xed\x8b\xb0 RNN \xec\x9c\xa0\xeb\xac\xb4\ntf.app.flags.DEFINE_boolean('attention', True, 'Use Attention') #  \xec\x96\xb4\xed\x85\x90\xec\x85\x98 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\ntf.app.flags.DEFINE_boolean('teacher_forcing', True, 'Use Teacher Forcing') # \xed\x95\x99\xec\x8a\xb5\xec\x8b\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x8d\x94 \xec\x9d\xb8\xed\x92\x8b \xec\xa0\x95\xeb\x8b\xb5 \xec\xa7\x80\xec\x9b\x90 \xec\x9c\xa0\xeb\xac\xb4\ntf.app.flags.DEFINE_boolean('tokenize_as_morph', False, 'set morph tokenize') # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c\xec\x97\x90 \xeb\x94\xb0\xeb\xa5\xb8 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\ntf.app.flags.DEFINE_boolean('serving', False, 'Use Serving') #  \xec\x84\x9c\xeb\xb9\x99 \xea\xb8\xb0\xeb\x8a\xa5 \xec\xa7\x80\xec\x9b\x90 \xec\x97\xac\xeb\xb6\x80\ntf.app.flags.DEFINE_boolean('loss_mask', False, 'Use loss mask') # PAD\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xeb\xa7\x88\xec\x8a\xa4\xed\x81\xac\xeb\xa5\xbc \xed\x86\xb5\xed\x95\x9c loss\xeb\xa5\xbc \xec\xa0\x9c\xed\x95\x9c\n\n# Define FLAGS\nDEFINES = tf.app.flags.FLAGS\n"""
6.CHATBOT/Appendix/data.py,2,"b'# -*- coding: utf-8 -*-\nfrom konlpy.tag import Okt\nimport pandas as pd\nimport tensorflow as tf\nimport enum\nimport os\nimport re\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom configs import DEFINES\n\nfrom tqdm import tqdm\n\nPAD_MASK = 0\nNON_PAD_MASK = 1\n\nFILTERS = ""([~.,!?\\""\':;)(])""\nPAD = ""<PADDING>""\nSTD = ""<START>""\nEND = ""<END>""\nUNK = ""<UNKNOWN>""\n\nPAD_INDEX = 0\nSTD_INDEX = 1\nEND_INDEX = 2\nUNK_INDEX = 3\n\nMARKER = [PAD, STD, END, UNK]\nCHANGE_FILTER = re.compile(FILTERS)\n\n\ndef load_data():\n    # \xed\x8c\x90\xeb\x8b\xa4\xec\x8a\xa4\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xb6\x88\xeb\x9f\xac\xec\x98\xa8\xeb\x8b\xa4.\n    data_df = pd.read_csv(DEFINES.data_path, header=0)\n    # \xec\xa7\x88\xeb\xac\xb8\xea\xb3\xbc \xeb\x8b\xb5\xeb\xb3\x80 \xec\x97\xb4\xec\x9d\x84 \xea\xb0\x80\xec\xa0\xb8\xec\x99\x80 question\xea\xb3\xbc answer\xec\x97\x90 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.\n    question, answer = list(data_df[\'Q\']), list(data_df[\'A\'])\n    # skleran\xec\x97\x90\xec\x84\x9c \xec\xa7\x80\xec\x9b\x90\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xed\x95\x99\xec\x8a\xb5 \xec\x85\x8b\xea\xb3\xbc \n    # \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8 \xec\x85\x8b\xec\x9d\x84 \xeb\x82\x98\xeb\x88\x88\xeb\x8b\xa4.\n    train_input, eval_input, train_label, eval_label = train_test_split(question, answer, test_size=0.33, random_state=42)\n    # \xea\xb7\xb8 \xea\xb0\x92\xec\x9d\x84 \xeb\xa6\xac\xed\x84\xb4\xed\x95\x9c\xeb\x8b\xa4.\n    return train_input, train_label, eval_input, eval_label\n\n\ndef prepro_like_morphlized(data):\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xeb\xb6\x84\xec\x84\x9d \xeb\xaa\xa8\xeb\x93\x88 \xea\xb0\x9d\xec\xb2\xb4\xeb\xa5\xbc\n    # \xec\x83\x9d\xec\x84\xb1\xed\x95\xa9\xeb\x8b\x88\xeb\x8b\xa4.\n\n    morph_analyzer = Okt()\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa6\x88 \xea\xb2\xb0\xea\xb3\xbc \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xeb\xb0\x9b\xec\x9d\x84\n    #  \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xec\x83\x9d\xec\x84\xb1\xed\x95\xa9\xeb\x8b\x88\xeb\x8b\xa4.\n    result_data = list()\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xec\x97\x90 \xec\x9e\x88\xeb\x8a\x94 \xeb\xa7\xa4 \xeb\xac\xb8\xec\x9e\xa5\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa6\x88\xeb\xa5\xbc\n    # \xed\x95\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8f\x84\xeb\xa1\x9d \xeb\xb0\x98\xeb\xb3\xb5\xeb\xac\xb8\xec\x9d\x84 \xec\x84\xa0\xec\x96\xb8\xed\x95\xa9\xeb\x8b\x88\xeb\x8b\xa4.\n    for seq in tqdm(data):\n        # Twitter.morphs \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa6\x88 \xeb\x90\x9c\n        # \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8 \xea\xb0\x9d\xec\xb2\xb4\xeb\xa5\xbc \xeb\xb0\x9b\xea\xb3\xa0 \xeb\x8b\xa4\xec\x8b\x9c \xea\xb3\xb5\xeb\xb0\xb1\xeb\xac\xb8\xec\x9e\x90\xeb\xa5\xbc \xea\xb8\xb0\xec\xa4\x80\xec\x9c\xbc\xeb\xa1\x9c\n        # \xed\x95\x98\xec\x97\xac \xeb\xac\xb8\xec\x9e\x90\xec\x97\xb4\xeb\xa1\x9c \xec\x9e\xac\xea\xb5\xac\xec\x84\xb1 \xed\x95\xb4\xec\xa4\x8d\xeb\x8b\x88\xeb\x8b\xa4.\n        morphlized_seq = "" "".join(morph_analyzer.morphs(seq.replace(\' \', \'\')))\n        result_data.append(morphlized_seq)\n\n    return result_data\n\n\n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xed\x95\xa0 value\xec\x99\x80 \xed\x82\xa4\xea\xb0\x80 \xec\x9b\x8c\xeb\x93\x9c\xec\x9d\xb4\xea\xb3\xa0 \n# \xea\xb0\x92\xec\x9d\xb4 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb8 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xb0\x9b\xeb\x8a\x94\xeb\x8b\xa4.\ndef enc_processing(value, dictionary):\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8a\x94 \n    # \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\xb4\xeb\x8b\xa4.(\xeb\x88\x84\xec\xa0\x81\xeb\x90\x9c\xeb\x8b\xa4.)\n    sequences_input_index = []\n    # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\x90\x98\xeb\x8a\x94 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x98 \n    # \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.(\xeb\x88\x84\xec\xa0\x81\xeb\x90\x9c\xeb\x8b\xa4.)\n    sequences_length = []\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\n    if DEFINES.tokenize_as_morph:\n        value = prepro_like_morphlized(value)\n\n    # \xed\x95\x9c\xec\xa4\x84\xec\x94\xa9 \xeb\xb6\x88\xec\x96\xb4\xec\x98\xa8\xeb\x8b\xa4.\n    for sequence in value:\n        # FILTERS = ""([~.,!?\\""\':;)(])""\n        # \xec\xa0\x95\xea\xb7\x9c\xed\x99\x94\xeb\xa5\xbc \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xec\x97\xac \xed\x95\x84\xed\x84\xb0\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94 \n        # \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 """" \xec\x9c\xbc\xeb\xa1\x9c \xec\xb9\x98\xed\x99\x98 \xed\x95\x9c\xeb\x8b\xa4.\n        sequence = re.sub(CHANGE_FILTER, """", sequence)\n        # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xed\x95\xa0\xeb\x95\x8c \n        # \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\xb4\xeb\x8b\xa4.\n        sequence_index = []\n        # \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xeb\x8b\xa8\xec\x9c\x84\xeb\xa1\x9c \n        # \xec\x9e\x90\xeb\xa5\xb4\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\n        for word in sequence.split():\n            # \xec\x9e\x98\xeb\xa0\xa4\xec\xa7\x84 \xeb\x8b\xa8\xec\x96\xb4\xeb\x93\xa4\xec\x9d\xb4 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x97\x90 \xec\xa1\xb4\xec\x9e\xac \xed\x95\x98\xeb\x8a\x94\xec\xa7\x80 \xeb\xb3\xb4\xea\xb3\xa0 \n            # \xea\xb7\xb8 \xea\xb0\x92\xec\x9d\x84 \xea\xb0\x80\xec\xa0\xb8\xec\x99\x80 sequence_index\xec\x97\x90 \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\n            if dictionary.get(word) is not None:\n                sequence_index.extend([dictionary[word]])\n            # \xec\x9e\x98\xeb\xa0\xa4\xec\xa7\x84 \xeb\x8b\xa8\xec\x96\xb4\xea\xb0\x80 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x97\x90 \xec\xa1\xb4\xec\x9e\xac \xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94 \n            # \xea\xb2\xbd\xec\x9a\xb0 \xec\x9d\xb4\xeb\xaf\x80\xeb\xa1\x9c UNK(2)\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n            else:\n                sequence_index.extend([dictionary[UNK]])\n        # \xeb\xac\xb8\xec\x9e\xa5 \xec\xa0\x9c\xed\x95\x9c \xea\xb8\xb8\xec\x9d\xb4\xeb\xb3\xb4\xeb\x8b\xa4 \xea\xb8\xb8\xec\x96\xb4\xec\xa7\x88 \xea\xb2\xbd\xec\x9a\xb0 \xeb\x92\xa4\xec\x97\x90 \xed\x86\xa0\xed\x81\xb0\xec\x9d\x84 \xec\x9e\x90\xeb\xa5\xb4\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\n        if len(sequence_index) > DEFINES.max_sequence_length:\n            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n        # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xeb\xac\xb8\xec\x9e\xa5\xec\x97\x90 \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4\xec\xa3\xbc\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\n        sequences_length.append(len(sequence_index))\n        # max_sequence_length\xeb\xb3\xb4\xeb\x8b\xa4 \xeb\xac\xb8\xec\x9e\xa5 \xea\xb8\xb8\xec\x9d\xb4\xea\xb0\x80 \n        # \xec\x9e\x91\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xb9\x88 \xeb\xb6\x80\xeb\xb6\x84\xec\x97\x90 PAD(0)\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4\xec\xa4\x80\xeb\x8b\xa4.\n        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n        # \xeb\x92\xa4\xeb\xa1\x9c \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4. \n        sequence_index.reverse()\n        # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94 \xea\xb0\x92\xec\x9d\x84 \n        # sequences_input_index\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n        sequences_input_index.append(sequence_index)\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xec\x9d\xbc\xeb\xb0\x98 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x9c\xeb\x8b\xa4. \n    # \xec\x9d\xb4\xec\x9c\xa0\xeb\x8a\x94 \xed\x85\x90\xec\x84\x9c\xed\x94\x8c\xeb\xa1\x9c\xec\x9a\xb0 dataset\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa3\xbc\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c \n    # \xec\x82\xac\xec\xa0\x84 \xec\x9e\x91\xec\x97\x85\xec\x9d\xb4\xeb\x8b\xa4.\n    # \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xeb\xb0\xb0\xec\x97\xb4\xea\xb3\xbc \n    # \xea\xb7\xb8 \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.  \n    return np.asarray(sequences_input_index), sequences_length\n\n\n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xed\x95\xa0 value\xec\x99\x80 \xed\x82\xa4\xea\xb0\x80 \xec\x9b\x8c\xeb\x93\x9c \xec\x9d\xb4\xea\xb3\xa0\n# \xea\xb0\x92\xec\x9d\xb4 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb8 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xb0\x9b\xeb\x8a\x94\xeb\x8b\xa4.\ndef dec_target_processing(value, dictionary):\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8a\x94 \n    # \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\xb4\xeb\x8b\xa4.(\xeb\x88\x84\xec\xa0\x81\xeb\x90\x9c\xeb\x8b\xa4)\n    sequences_target_index = []\n    sequences_length = []\n    # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\n    if DEFINES.tokenize_as_morph:\n        value = prepro_like_morphlized(value)\n    # \xed\x95\x9c\xec\xa4\x84\xec\x94\xa9 \xeb\xb6\x88\xec\x96\xb4\xec\x98\xa8\xeb\x8b\xa4.\n    for sequence in value:\n        # FILTERS = ""([~.,!?\\""\':;)(])""\n        # \xec\xa0\x95\xea\xb7\x9c\xed\x99\x94\xeb\xa5\xbc \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xec\x97\xac \xed\x95\x84\xed\x84\xb0\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94 \n        # \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 """" \xec\x9c\xbc\xeb\xa1\x9c \xec\xb9\x98\xed\x99\x98 \xed\x95\x9c\xeb\x8b\xa4.\n        sequence = re.sub(CHANGE_FILTER, """", sequence)\n        # \xeb\xac\xb8\xec\x9e\xa5\xec\x97\x90\xec\x84\x9c \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xeb\x8b\xa8\xec\x9c\x84\xeb\xb3\x84\xeb\xa1\x9c \xeb\x8b\xa8\xec\x96\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa0\xb8\xec\x99\x80\xec\x84\x9c \n        # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x9d\x98 \xea\xb0\x92\xec\x9d\xb8 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n        # \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5\xec\x9d\x98 \xeb\xa7\x88\xec\xa7\x80\xeb\xa7\x89\xec\x97\x90 END\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n        sequence_index = [dictionary[word] for word in sequence.split()]\n        # \xeb\xac\xb8\xec\x9e\xa5 \xec\xa0\x9c\xed\x95\x9c \xea\xb8\xb8\xec\x9d\xb4\xeb\xb3\xb4\xeb\x8b\xa4 \xea\xb8\xb8\xec\x96\xb4\xec\xa7\x88 \xea\xb2\xbd\xec\x9a\xb0 \xeb\x92\xa4\xec\x97\x90 \xed\x86\xa0\xed\x81\xb0\xec\x9d\x84 \xec\x9e\x90\xeb\xa5\xb4\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\n        # \xea\xb7\xb8\xeb\xa6\xac\xea\xb3\xa0 END \xed\x86\xa0\xed\x81\xb0\xec\x9d\x84 \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4\n        if len(sequence_index) >= DEFINES.max_sequence_length:\n            sequence_index = sequence_index[:DEFINES.max_sequence_length-1] + [dictionary[END]]\n        else:\n            sequence_index += [dictionary[END]]\n        \n        # \xed\x95\x99\xec\x8a\xb5\xec\x8b\x9c PAD \xeb\xa7\x88\xec\x8a\xa4\xed\x81\xac\xeb\xa5\xbc \xec\x9c\x84\xed\x95\x9c \xeb\xb2\xa1\xed\x84\xb0\xeb\xa5\xbc \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.                   \n        sequences_length.append([PAD_MASK if num > len(sequence_index) else NON_PAD_MASK for num in range (DEFINES.max_sequence_length)])\n        # max_sequence_length\xeb\xb3\xb4\xeb\x8b\xa4 \xeb\xac\xb8\xec\x9e\xa5 \xea\xb8\xb8\xec\x9d\xb4\xea\xb0\x80 \n        # \xec\x9e\x91\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xb9\x88 \xeb\xb6\x80\xeb\xb6\x84\xec\x97\x90 PAD(0)\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4\xec\xa4\x80\xeb\x8b\xa4.\n        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n        # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94 \xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xeb\x8a\x94 \xea\xb0\x92\xec\x9d\x84 \n        # sequences_target_index\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n        sequences_target_index.append(sequence_index)\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xec\x9d\xbc\xeb\xb0\x98 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x9c\xeb\x8b\xa4. \n    # \xec\x9d\xb4\xec\x9c\xa0\xeb\x8a\x94 \xed\x85\x90\xec\x84\x9c\xed\x94\x8c\xeb\xa1\x9c\xec\x9a\xb0 dataset\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa3\xbc\xea\xb8\xb0 \xec\x9c\x84\xed\x95\x9c \xec\x82\xac\xec\xa0\x84 \xec\x9e\x91\xec\x97\x85\xec\x9d\xb4\xeb\x8b\xa4.\n    # \xeb\x84\x98\xed\x8c\x8c\xec\x9d\xb4 \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xed\x99\x94\xeb\x90\x9c \xeb\xb0\xb0\xec\x97\xb4\xea\xb3\xbc \xea\xb7\xb8 \xea\xb8\xb8\xec\x9d\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\n    return np.asarray(sequences_target_index), np.asarray(sequences_length)\n\n\n# \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc \xec\x8a\xa4\xed\x8a\xb8\xeb\xa7\x81\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\n# \xeb\xb0\x94\xea\xbe\xb8\xea\xb3\xa0\xec\x9e\x90 \xed\x95\x98\xeb\x8a\x94 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 value\xec\x99\x80 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc \n# \xed\x82\xa4\xeb\xa1\x9c \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xea\xb3\xa0 \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c \xeb\x8b\xa8\xec\x96\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8a\x94 \n# \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xb0\x9b\xeb\x8a\x94\xeb\x8b\xa4.\ndef pred2string(value, dictionary):\n    # \xed\x85\x8d\xec\x8a\xa4\xed\x8a\xb8 \xeb\xac\xb8\xec\x9e\xa5\xec\x9d\x84 \xeb\xb3\xb4\xea\xb4\x80\xed\x95\xa0 \xeb\xb0\xb0\xec\x97\xb4\xec\x9d\x84 \xec\x84\xa0\xec\x96\xb8\xed\x95\x9c\xeb\x8b\xa4.\n    sentence_string = []\n    # \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xeb\xb0\xb0\xec\x97\xb4 \xed\x95\x98\xeb\x82\x98\xeb\xa5\xbc \xea\xba\xbc\xeb\x82\xb4\xec\x84\x9c v\xec\x97\x90 \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\n    if DEFINES.serving == True:\n        for v in value[\'output\']: \n            sentence_string = [dictionary[index] for index in v]\n    else:\n        for v in value:\n            # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xec\x97\x90 \xec\x9e\x88\xeb\x8a\x94 \xeb\x8b\xa8\xec\x96\xb4\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\xb4\xec\x84\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xeb\x8b\xb4\xeb\x8a\x94\xeb\x8b\xa4.\n            sentence_string = [dictionary[index] for index in v[\'indexs\']]\n    \n    print(sentence_string)\n    answer = """"\n    # \xed\x8c\xa8\xeb\x94\xa9\xea\xb0\x92\xeb\x8f\x84 \xeb\x8b\xb4\xea\xb2\xa8 \xec\x9e\x88\xec\x9c\xbc\xeb\xaf\x80\xeb\xa1\x9c \xed\x8c\xa8\xeb\x94\xa9\xec\x9d\x80 \xeb\xaa\xa8\xeb\x91\x90 \xec\x8a\xa4\xed\x8e\x98\xec\x9d\xb4\xec\x8a\xa4 \xec\xb2\x98\xeb\xa6\xac \xed\x95\x9c\xeb\x8b\xa4.\n    for word in sentence_string:\n        if word not in PAD and word not in END:\n            answer += word\n            answer += "" ""\n    # \xea\xb2\xb0\xea\xb3\xbc\xeb\xa5\xbc \xec\xb6\x9c\xeb\xa0\xa5\xed\x95\x9c\xeb\x8b\xa4.\n    print(answer)\n    return answer\n\n\ndef rearrange(input, target):\n    features = {""input"": input}\n    return features, target\n\ndef train_rearrange(input, length, target):\n    features = {""input"": input, ""length"": length}\n    return features, target\n\n# \xed\x95\x99\xec\x8a\xb5\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4\xea\xb0\x80 \xeb\xb0\xb0\xec\xb9\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\ndef train_input_fn(train_input_enc, train_target_dec_length, train_target_dec, batch_size):\n    # Dataset\xec\x9d\x84 \xec\x83\x9d\xec\x84\xb1\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9c\xbc\xeb\xa1\x9c\xec\x8d\xa8 from_tensor_slices\xeb\xb6\x80\xeb\xb6\x84\xec\x9d\x80 \n    # \xea\xb0\x81\xea\xb0\x81 \xed\x95\x9c \xeb\xac\xb8\xec\x9e\xa5\xec\x9c\xbc\xeb\xa1\x9c \xec\x9e\x90\xeb\xa5\xb8\xeb\x8b\xa4\xea\xb3\xa0 \xeb\xb3\xb4\xeb\xa9\xb4 \xeb\x90\x9c\xeb\x8b\xa4.\n    # train_input_enc, train_target_dec_length, train_target_dec \n    # 3\xea\xb0\x9c\xeb\xa5\xbc \xea\xb0\x81\xea\xb0\x81 \xed\x95\x9c\xeb\xac\xb8\xec\x9e\xa5\xec\x9c\xbc\xeb\xa1\x9c \xeb\x82\x98\xeb\x88\x88\xeb\x8b\xa4.\n    dataset = tf.data.Dataset.from_tensor_slices((train_input_enc, train_target_dec_length, train_target_dec))\n    # \xec\xa0\x84\xec\xb2\xb4 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xec\x8d\xa9\xeb\x8a\x94\xeb\x8b\xa4.\n    dataset = dataset.shuffle(buffer_size=len(train_input_enc))\n    # \xeb\xb0\xb0\xec\xb9\x98 \xec\x9d\xb8\xec\x9e\x90 \xea\xb0\x92\xec\x9d\xb4 \xec\x97\x86\xeb\x8b\xa4\xeb\xa9\xb4  \xec\x97\x90\xeb\x9f\xac\xeb\xa5\xbc \xeb\xb0\x9c\xec\x83\x9d \xec\x8b\x9c\xed\x82\xa8\xeb\x8b\xa4.\n    assert batch_size is not None, ""train batchSize must not be None""\n    # from_tensor_slices\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x82\x98\xeb\x88\x88\xea\xb2\x83\xec\x9d\x84 \n    # \xeb\xb0\xb0\xec\xb9\x98\xed\x81\xac\xea\xb8\xb0 \xeb\xa7\x8c\xed\x81\xbc \xeb\xac\xb6\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n    dataset = dataset.batch(batch_size)\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb0\x81 \xec\x9a\x94\xec\x86\x8c\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4\xec\x84\x9c train_rearrange \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc \n    # \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xec\x9a\x94\xec\x86\x8c\xeb\xa5\xbc \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xa7\xb5\xec\x9c\xbc\xeb\xa1\x9c \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\n    dataset = dataset.map(train_rearrange)\n    # repeat()\xed\x95\xa8\xec\x88\x98\xec\x97\x90 \xec\x9b\x90\xed\x95\x98\xeb\x8a\x94 \xec\x97\x90\xed\x8f\xac\xed\x81\xac \xec\x88\x98\xeb\xa5\xbc \xeb\x84\xa3\xec\x9d\x84\xec\x88\x98 \xec\x9e\x88\xec\x9c\xbc\xeb\xa9\xb4 \n    # \xec\x95\x84\xeb\xac\xb4 \xec\x9d\xb8\xec\x9e\x90\xeb\x8f\x84 \xec\x97\x86\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xac\xb4\xed\x95\x9c\xec\x9c\xbc\xeb\xa1\x9c \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0 \xeb\x90\x9c\xeb\x8b\xa4.\n    dataset = dataset.repeat()\n    # make_one_shot_iterator\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \n    # \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n    iterator = dataset.make_one_shot_iterator()\n    # \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x8b\xa4\xec\x9d\x8c \xed\x95\xad\xeb\xaa\xa9\xec\x9d\x98 \xed\x85\x90\xec\x84\x9c \n    # \xea\xb0\x9c\xec\xb2\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\n    return iterator.get_next()\n\n\n# \xed\x8f\x89\xea\xb0\x80\xec\x97\x90 \xeb\x93\xa4\xec\x96\xb4\xea\xb0\x80 \xeb\xb0\xb0\xec\xb9\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\ndef eval_input_fn(eval_input_enc, eval_target_dec, batch_size):\n    # Dataset\xec\x9d\x84 \xec\x83\x9d\xec\x84\xb1\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9c\xbc\xeb\xa1\x9c\xec\x8d\xa8 from_tensor_slices\xeb\xb6\x80\xeb\xb6\x84\xec\x9d\x80 \n    # \xea\xb0\x81\xea\xb0\x81 \xed\x95\x9c \xeb\xac\xb8\xec\x9e\xa5\xec\x9c\xbc\xeb\xa1\x9c \xec\x9e\x90\xeb\xa5\xb8\xeb\x8b\xa4\xea\xb3\xa0 \xeb\xb3\xb4\xeb\xa9\xb4 \xeb\x90\x9c\xeb\x8b\xa4.\n    # eval_input_enc, eval_target_dec, batch_size \n    # 3\xea\xb0\x9c\xeb\xa5\xbc \xea\xb0\x81\xea\xb0\x81 \xed\x95\x9c\xeb\xac\xb8\xec\x9e\xa5\xec\x9c\xbc\xeb\xa1\x9c \xeb\x82\x98\xeb\x88\x88\xeb\x8b\xa4.\n    dataset = tf.data.Dataset.from_tensor_slices((eval_input_enc, eval_target_dec))\n    # \xec\xa0\x84\xec\xb2\xb4 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xec\x84\x9e\xeb\x8a\x94\xeb\x8b\xa4.\n    dataset = dataset.shuffle(buffer_size=len(eval_input_enc))\n    # \xeb\xb0\xb0\xec\xb9\x98 \xec\x9d\xb8\xec\x9e\x90 \xea\xb0\x92\xec\x9d\xb4 \xec\x97\x86\xeb\x8b\xa4\xeb\xa9\xb4  \xec\x97\x90\xeb\x9f\xac\xeb\xa5\xbc \xeb\xb0\x9c\xec\x83\x9d \xec\x8b\x9c\xed\x82\xa8\xeb\x8b\xa4.\n    assert batch_size is not None, ""eval batchSize must not be None""\n    # from_tensor_slices\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x82\x98\xeb\x88\x88\xea\xb2\x83\xec\x9d\x84 \n    # \xeb\xb0\xb0\xec\xb9\x98\xed\x81\xac\xea\xb8\xb0 \xeb\xa7\x8c\xed\x81\xbc \xeb\xac\xb6\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n    dataset = dataset.batch(batch_size)\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb0\x81 \xec\x9a\x94\xec\x86\x8c\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4\xec\x84\x9c rearrange \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc \n    # \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xec\x9a\x94\xec\x86\x8c\xeb\xa5\xbc \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xa7\xb5\xec\x9c\xbc\xeb\xa1\x9c \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\n    dataset = dataset.map(rearrange)\n    # repeat()\xed\x95\xa8\xec\x88\x98\xec\x97\x90 \xec\x9b\x90\xed\x95\x98\xeb\x8a\x94 \xec\x97\x90\xed\x8f\xac\xed\x81\xac \xec\x88\x98\xeb\xa5\xbc \xeb\x84\xa3\xec\x9d\x84\xec\x88\x98 \xec\x9e\x88\xec\x9c\xbc\xeb\xa9\xb4 \n    # \xec\x95\x84\xeb\xac\xb4 \xec\x9d\xb8\xec\x9e\x90\xeb\x8f\x84 \xec\x97\x86\xeb\x8b\xa4\xeb\xa9\xb4 \xeb\xac\xb4\xed\x95\x9c\xec\x9c\xbc\xeb\xa1\x9c \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0 \xeb\x90\x9c\xeb\x8b\xa4.\n    # \xed\x8f\x89\xea\xb0\x80\xec\x9d\xb4\xeb\xaf\x80\xeb\xa1\x9c 1\xed\x9a\x8c\xeb\xa7\x8c \xeb\x8f\x99\xec\x9e\x91 \xec\x8b\x9c\xed\x82\xa8\xeb\x8b\xa4.\n    dataset = dataset.repeat(1)\n    # make_one_shot_iterator\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \n    # \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n    iterator = dataset.make_one_shot_iterator()\n    # \xec\x9d\xb4\xed\x84\xb0\xeb\xa0\x88\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x8b\xa4\xec\x9d\x8c \xed\x95\xad\xeb\xaa\xa9\xec\x9d\x98 \n    # \xed\x85\x90\xec\x84\x9c \xea\xb0\x9c\xec\xb2\xb4\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8\xec\xa4\x80\xeb\x8b\xa4.\n    return iterator.get_next()\n\n\ndef data_tokenizer(data):\n    # \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95 \xed\x95\xb4\xec\x84\x9c \xeb\x8b\xb4\xec\x9d\x84 \xeb\xb0\xb0\xec\x97\xb4 \xec\x83\x9d\xec\x84\xb1\n    words = []\n    for sentence in data:\n        # FILTERS = ""([~.,!?\\""\':;)(])""\n        # \xec\x9c\x84 \xed\x95\x84\xed\x84\xb0\xec\x99\x80 \xea\xb0\x99\xec\x9d\x80 \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xec\xa0\x95\xea\xb7\x9c\xed\x99\x94 \xed\x91\x9c\xed\x98\x84\xec\x8b\x9d\xec\x9d\x84 \n        # \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xeb\xaa\xa8\xeb\x91\x90 """" \xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98 \xed\x95\xb4\xec\xa3\xbc\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n        sentence = re.sub(CHANGE_FILTER, """", sentence)\n        for word in sentence.split():\n            words.append(word)\n    # \xed\x86\xa0\xea\xb7\xb8\xeb\x82\x98\xec\x9d\xb4\xec\xa7\x95\xea\xb3\xbc \xec\xa0\x95\xea\xb7\x9c\xed\x91\x9c\xed\x98\x84\xec\x8b\x9d\xec\x9d\x84 \xed\x86\xb5\xed\x95\xb4 \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4\xec\xa7\x84 \n    # \xea\xb0\x92\xeb\x93\xa4\xec\x9d\x84 \xeb\x84\x98\xea\xb2\xa8 \xec\xa4\x80\xeb\x8b\xa4.\n    return [word for word in words if word]\n\n\ndef load_vocabulary():\n    # \xec\x82\xac\xec\xa0\x84\xec\x9d\x84 \xeb\x8b\xb4\xec\x9d\x84 \xeb\xb0\xb0\xec\x97\xb4 \xec\xa4\x80\xeb\xb9\x84\xed\x95\x9c\xeb\x8b\xa4.\n    vocabulary_list = []\n    # \xec\x82\xac\xec\xa0\x84\xec\x9d\x84 \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c \xed\x9b\x84 \xed\x8c\x8c\xec\x9d\xbc\xeb\xa1\x9c \xec\xa0\x80\xec\x9e\xa5 \xec\xa7\x84\xed\x96\x89\xed\x95\x9c\xeb\x8b\xa4. \n    # \xea\xb7\xb8 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x98 \xec\xa1\xb4\xec\x9e\xac \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xed\x99\x95\xec\x9d\xb8\xed\x95\x9c\xeb\x8b\xa4.\n    if (not (os.path.exists(DEFINES.vocabulary_path))):\n        # \xec\x9d\xb4\xeb\xaf\xb8 \xec\x83\x9d\xec\x84\xb1\xeb\x90\x9c \xec\x82\xac\xec\xa0\x84 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\xb4 \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xec\x9c\xbc\xeb\xaf\x80\xeb\xa1\x9c \n        # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4\xec\x95\xbc \xed\x95\x9c\xeb\x8b\xa4.\n        # \xea\xb7\xb8\xeb\x9e\x98\xec\x84\x9c \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xea\xb0\x80 \xec\xa1\xb4\xec\x9e\xac \xed\x95\x98\xeb\xa9\xb4 \xec\x82\xac\xec\xa0\x84\xec\x9d\x84 \xeb\xa7\x8c\xeb\x93\xa4\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c \n        # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x98 \xec\xa1\xb4\xec\x9e\xac \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xed\x99\x95\xec\x9d\xb8\xed\x95\x9c\xeb\x8b\xa4.\n        if (os.path.exists(DEFINES.data_path)):\n            # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xea\xb0\x80 \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xeb\x8b\x88 \xed\x8c\x90\xeb\x8b\xa8\xec\x8a\xa4\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \n            # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\xb6\x88\xeb\x9f\xac\xec\x98\xa4\xec\x9e\x90\n            data_df = pd.read_csv(DEFINES.data_path, encoding=\'utf-8\')\n            # \xed\x8c\x90\xeb\x8b\xa4\xec\x8a\xa4\xec\x9d\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xed\x94\x84\xeb\xa0\x88\xec\x9e\x84\xec\x9d\x84 \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \n            # \xec\xa7\x88\xeb\xac\xb8\xea\xb3\xbc \xeb\x8b\xb5\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\x97\xb4\xec\x9d\x84 \xea\xb0\x80\xec\xa0\xb8 \xec\x98\xa8\xeb\x8b\xa4.\n            question, answer = list(data_df[\'Q\']), list(data_df[\'A\'])\n            if DEFINES.tokenize_as_morph:  # \xed\x98\x95\xed\x83\x9c\xec\x86\x8c\xec\x97\x90 \xeb\x94\xb0\xeb\xa5\xb8 \xed\x86\xa0\xed\x81\xac\xeb\x82\x98\xec\x9d\xb4\xec\xa0\xb8 \xec\xb2\x98\xeb\xa6\xac\n                question = prepro_like_morphlized(question)\n                answer = prepro_like_morphlized(answer)\n            data = []\n            # \xec\xa7\x88\xeb\xac\xb8\xea\xb3\xbc \xeb\x8b\xb5\xeb\xb3\x80\xec\x9d\x84 extend\xec\x9d\x84 \n            # \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xea\xb5\xac\xec\xa1\xb0\xea\xb0\x80 \xec\x97\x86\xeb\x8a\x94 \xeb\xb0\xb0\xec\x97\xb4\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n            data.extend(question)\n            data.extend(answer)\n            # \xed\x86\xa0\xed\x81\xb0\xeb\x82\x98\xec\x9d\xb4\xec\xa0\xb8 \xec\xb2\x98\xeb\xa6\xac \xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n            words = data_tokenizer(data)\n            # \xea\xb3\xb5\xed\x86\xb5\xec\xa0\x81\xec\x9d\xb8 \xeb\x8b\xa8\xec\x96\xb4\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4\xec\x84\x9c\xeb\x8a\x94 \xeb\xaa\xa8\xeb\x91\x90 \n            # \xed\x95\x84\xec\x9a\x94 \xec\x97\x86\xec\x9c\xbc\xeb\xaf\x80\xeb\xa1\x9c \xed\x95\x9c\xea\xb0\x9c\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xec\xa3\xbc\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c\n            # set\xed\x95\xb4\xec\xa3\xbc\xea\xb3\xa0 \xec\x9d\xb4\xea\xb2\x83\xeb\x93\xa4\xec\x9d\x84 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n            words = list(set(words))\n            # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\x97\x86\xeb\x8a\x94 \xeb\x82\xb4\xec\x9a\xa9\xec\xa4\x91\xec\x97\x90 MARKER\xeb\xa5\xbc \xec\x82\xac\xec\xa0\x84\xec\x97\x90 \n            # \xec\xb6\x94\xea\xb0\x80 \xed\x95\x98\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c \xec\x95\x84\xeb\x9e\x98\xec\x99\x80 \xea\xb0\x99\xec\x9d\xb4 \xec\xb2\x98\xeb\xa6\xac \xed\x95\x9c\xeb\x8b\xa4.\n            # \xec\x95\x84\xeb\x9e\x98\xeb\x8a\x94 MARKER \xea\xb0\x92\xec\x9d\xb4\xeb\xa9\xb0 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xec\x9d\x98 \xec\xb2\xab\xeb\xb2\x88\xec\xa7\xb8 \xeb\xb6\x80\xed\x84\xb0 \n            # \xec\x88\x9c\xec\x84\x9c\xeb\x8c\x80\xeb\xa1\x9c \xeb\x84\xa3\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 0\xec\x97\x90 \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\n            # PAD = ""<PADDING>""\n            # STD = ""<START>""\n            # END = ""<END>""\n            # UNK = ""<UNKNOWN>""     \n            words[:0] = MARKER\n        # \xec\x82\xac\xec\xa0\x84\xec\x9d\x84 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa4\xec\x97\x88\xec\x9c\xbc\xeb\x8b\x88 \xec\x9d\xb4 \xeb\x82\xb4\xec\x9a\xa9\xec\x9d\x84 \n        # \xec\x82\xac\xec\xa0\x84 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.\n        with open(DEFINES.vocabulary_path, \'w\', encoding=\'utf-8\') as vocabulary_file:\n            for word in words:\n                vocabulary_file.write(word + \'\\n\')\n\n    # \xec\x82\xac\xec\xa0\x84 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\xb4 \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xeb\xa9\xb4 \xec\x97\xac\xea\xb8\xb0\xec\x97\x90\xec\x84\x9c \n    # \xea\xb7\xb8 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xeb\xb6\x88\xeb\x9f\xac\xec\x84\x9c \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n    with open(DEFINES.vocabulary_path, \'r\', encoding=\'utf-8\') as vocabulary_file:\n        for line in vocabulary_file:\n            vocabulary_list.append(line.strip())\n\n    # \xeb\xb0\xb0\xec\x97\xb4\xec\x97\x90 \xeb\x82\xb4\xec\x9a\xa9\xec\x9d\x84 \xed\x82\xa4\xec\x99\x80 \xea\xb0\x92\xec\x9d\xb4 \xec\x9e\x88\xeb\x8a\x94 \n    # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac \xea\xb5\xac\xec\xa1\xb0\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n    char2idx, idx2char = make_vocabulary(vocabulary_list)\n    # \xeb\x91\x90\xea\xb0\x80\xec\xa7\x80 \xed\x98\x95\xed\x83\x9c\xec\x9d\x98 \xed\x82\xa4\xec\x99\x80 \xea\xb0\x92\xec\x9d\xb4 \xec\x9e\x88\xeb\x8a\x94 \xed\x98\x95\xed\x83\x9c\xeb\xa5\xbc \xeb\xa6\xac\xed\x84\xb4\xed\x95\x9c\xeb\x8b\xa4. \n    # (\xec\x98\x88) \xeb\x8b\xa8\xec\x96\xb4: \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 , \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4: \xeb\x8b\xa8\xec\x96\xb4)\n    return char2idx, idx2char, len(char2idx)\n\n\ndef make_vocabulary(vocabulary_list):\n    # \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xed\x82\xa4\xea\xb0\x80 \xeb\x8b\xa8\xec\x96\xb4\xec\x9d\xb4\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\xb4 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb8 \n    # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n    # \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xed\x82\xa4\xea\xb0\x80 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\xb4\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\xb4 \xeb\x8b\xa8\xec\x96\xb4\xec\x9d\xb8 \n    # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n    # \xeb\x91\x90\xea\xb0\x9c\xec\x9d\x98 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac\xeb\xa5\xbc \xeb\x84\x98\xea\xb2\xa8 \xec\xa4\x80\xeb\x8b\xa4.\n    return char2idx, idx2char\n\n\ndef main(self):\n    char2idx, idx2char, vocabulary_length = load_vocabulary()\n\n\n\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run(main)\n'"
6.CHATBOT/Appendix/main.py,0,"b""import tensorflow as tf\nimport model as ml\nimport data\nimport numpy as np\nimport os\nimport sys\n\nfrom configs import DEFINES\n\nDATA_OUT_PATH = './data_out/'\n\n# Serving \xea\xb8\xb0\xeb\x8a\xa5\xec\x9d\x84 \xec\x9c\x84\xed\x95\x98\xec\x97\xac serving \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\ndef serving_input_receiver_fn():\n    receiver_tensor = {\n        'input': tf.placeholder(dtype=tf.int32, shape=[None, DEFINES.max_sequence_length]),\n        'output': tf.placeholder(dtype=tf.int32, shape=[None, DEFINES.max_sequence_length])    \n    }\n    features = {\n        key: tensor for key, tensor in receiver_tensor.items()\n    }\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)\n\n\ndef main(self):\n    data_out_path = os.path.join(os.getcwd(), DATA_OUT_PATH)\n    os.makedirs(data_out_path, exist_ok=True)\n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xed\x86\xb5\xed\x95\x9c \xec\x82\xac\xec\xa0\x84 \xea\xb5\xac\xec\x84\xb1 \xed\x95\x9c\xeb\x8b\xa4.\n    char2idx, idx2char, vocabulary_length = data.load_vocabulary()\n    # \xed\x9b\x88\xeb\xa0\xa8 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xec\x99\x80 \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xea\xb0\x80\xec\xa0\xb8\xec\x98\xa8\xeb\x8b\xa4.\n    train_input, train_label, eval_input, eval_label = data.load_data()\n\n    # \xed\x9b\x88\xeb\xa0\xa8\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    train_input_enc, train_input_enc_length = data.enc_processing(train_input, char2idx)\n    # \xed\x9b\x88\xeb\xa0\xa8\xec\x85\x8b \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\xb6\x80\xeb\xb6\x84 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    train_target_dec, train_target_dec_length = data.dec_target_processing(train_label, char2idx)\n    \n    # \xed\x8f\x89\xea\xb0\x80\xec\x85\x8b \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    eval_input_enc, eval_input_enc_length = data.enc_processing(eval_input,char2idx)\n    # \xed\x8f\x89\xea\xb0\x80\xec\x85\x8b \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\xb6\x80\xeb\xb6\x84 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    eval_target_dec, _ = data.dec_target_processing(eval_label, char2idx)\n\n    # \xed\x98\x84\xec\x9e\xac \xea\xb2\xbd\xeb\xa1\x9c'./'\xec\x97\x90 \xed\x98\x84\xec\x9e\xac \xea\xb2\xbd\xeb\xa1\x9c \xed\x95\x98\xeb\xb6\x80\xec\x97\x90 \n    # \xec\xb2\xb4\xed\x81\xac \xed\x8f\xac\xec\x9d\xb8\xed\x8a\xb8\xeb\xa5\xbc \xec\xa0\x80\xec\x9e\xa5\xed\x95\x9c \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n    check_point_path = os.path.join(os.getcwd(), DEFINES.check_point_path)\n    save_model_path = os.path.join(os.getcwd(), DEFINES.save_model_path)\n    # \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\xa9\xb0 \xeb\x91\x90\xeb\xb2\x88\xec\xa7\xb8 \xec\x9d\xb8\xec\x9e\x90 exist_ok\xea\xb0\x80 \n    # True\xec\x9d\xb4\xeb\xa9\xb4 \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xea\xb0\x80 \xec\x9d\xb4\xeb\xaf\xb8 \xec\xa1\xb4\xec\x9e\xac\xed\x95\xb4\xeb\x8f\x84 OSError\xea\xb0\x80 \n    # \xeb\xb0\x9c\xec\x83\x9d\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94\xeb\x8b\xa4.\n    # exist_ok\xea\xb0\x80 False\xec\x9d\xb4\xeb\xa9\xb4 \xec\x9d\xb4\xeb\xaf\xb8 \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xeb\xa9\xb4 \n    # OSError\xea\xb0\x80 \xeb\xb0\x9c\xec\x83\x9d\xed\x95\x9c\xeb\x8b\xa4.\n    os.makedirs(check_point_path, exist_ok=True)\n    os.makedirs(save_model_path, exist_ok=True)\n\n    # \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0 \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\n    classifier = tf.estimator.Estimator(\n        model_fn=ml.Model,  # \xeb\xaa\xa8\xeb\x8d\xb8 \xeb\x93\xb1\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\n        model_dir=DEFINES.check_point_path,  # \xec\xb2\xb4\xed\x81\xac\xed\x8f\xac\xec\x9d\xb8\xed\x8a\xb8 \xec\x9c\x84\xec\xb9\x98 \xeb\x93\xb1\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\n        params={  # \xeb\xaa\xa8\xeb\x8d\xb8 \xec\xaa\xbd\xec\x9c\xbc\xeb\xa1\x9c \xed\x8c\x8c\xeb\x9d\xbc\xeb\xa9\x94\xed\x84\xb0 \xec\xa0\x84\xeb\x8b\xac\xed\x95\x9c\xeb\x8b\xa4.\n            'hidden_size': DEFINES.hidden_size,  # \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            'layer_size': DEFINES.layer_size,  # \xeb\xa9\x80\xed\x8b\xb0 \xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4 \xec\xb8\xb5 \xea\xb0\x9c\xec\x88\x98\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            'learning_rate': DEFINES.learning_rate,  # \xed\x95\x99\xec\x8a\xb5\xec\x9c\xa8 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            'teacher_forcing_rate': DEFINES.teacher_forcing_rate, # \xed\x95\x99\xec\x8a\xb5\xec\x8b\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x8d\x94 \xec\x9d\xb8\xed\x92\x8b \xec\xa0\x95\xeb\x8b\xb5 \xec\xa7\x80\xec\x9b\x90\xec\x9c\xa8 \xec\x84\xa4\xec\xa0\x95\n            'vocabulary_length': vocabulary_length,  # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac \xed\x81\xac\xea\xb8\xb0\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            'embedding_size': DEFINES.embedding_size,  # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xed\x81\xac\xea\xb8\xb0\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            'embedding': DEFINES.embedding,  # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            'multilayer': DEFINES.multilayer,  # \xeb\xa9\x80\xed\x8b\xb0 \xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            'attention': DEFINES.attention, #  \xec\x96\xb4\xed\x85\x90\xec\x85\x98 \xec\xa7\x80\xec\x9b\x90 \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n\t        'teacher_forcing': DEFINES.teacher_forcing, # \xed\x95\x99\xec\x8a\xb5\xec\x8b\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x8d\x94 \xec\x9d\xb8\xed\x92\x8b \xec\xa0\x95\xeb\x8b\xb5 \xec\xa7\x80\xec\x9b\x90 \xec\x9c\xa0\xeb\xac\xb4 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n            'loss_mask': DEFINES.loss_mask, # PAD\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xeb\xa7\x88\xec\x8a\xa4\xed\x81\xac\xeb\xa5\xbc \xed\x86\xb5\xed\x95\x9c loss\xeb\xa5\xbc \xec\xa0\x9c\xed\x95\x9c \xed\x95\x9c\xeb\x8b\xa4.\n            'serving': DEFINES.serving # \xeb\xaa\xa8\xeb\x8d\xb8 \xec\xa0\x80\xec\x9e\xa5 \xeb\xb0\x8f serving \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n        })\n\n    # \xed\x95\x99\xec\x8a\xb5 \xec\x8b\xa4\xed\x96\x89\n    classifier.train(input_fn=lambda: data.train_input_fn(\n        train_input_enc, train_target_dec_length, train_target_dec, DEFINES.batch_size), steps=DEFINES.train_steps)\n    # \xec\x84\x9c\xeb\xb9\x99 \xea\xb8\xb0\xeb\x8a\xa5 \xec\x9c\xa0\xeb\xac\xb4\xec\x97\x90 \xeb\x94\xb0\xeb\x9d\xbc \xeb\xaa\xa8\xeb\x8d\xb8\xec\x9d\x84 Save \xed\x95\x9c\xeb\x8b\xa4.\n    if DEFINES.serving == True:\n        save_model_path = classifier.export_savedmodel(\n            export_dir_base=DEFINES.save_model_path,\n            serving_input_receiver_fn=serving_input_receiver_fn)\n\n    # \xed\x8f\x89\xea\xb0\x80 \xec\x8b\xa4\xed\x96\x89\n    eval_result = classifier.evaluate(input_fn=lambda: data.eval_input_fn(\n        eval_input_enc,eval_target_dec, DEFINES.batch_size))\n    print('\\nEVAL set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n\nif __name__ == '__main__':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run(main)\n\ntf.logging.set_verbosity\n"""
6.CHATBOT/Appendix/model.py,0,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport sys\nimport numpy as np\nfrom configs import DEFINES\n\n\ndef make_lstm_cell(mode, hiddenSize, index):\n    cell = tf.nn.rnn_cell.BasicLSTMCell(hiddenSize, name=""lstm"" + str(index), state_is_tuple=False)\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        cell = tf.contrib.rnn.DropoutWrapper(cell, state_keep_prob=DEFINES.dropout_width)\n    return cell\n\n# \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0 \xeb\xaa\xa8\xeb\x8d\xb8 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n# freatures : tf.data.Dataset.map\xec\x9d\x84 \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4\xec\xa7\x84 \n# features = {""input"": input, ""length"": length}\n# labels : tf.data.Dataset.map\xec\x9d\x84 \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4\xec\xa7\x84 target\n# mode\xeb\x8a\x94 \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0 \xed\x95\xa8\xec\x88\x98\xeb\xa5\xbc \xed\x98\xb8\xec\xb6\x9c\xed\x95\x98\xeb\xa9\xb4 \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0 \n# \xed\x94\x84\xeb\xa0\x88\xec\x9e\x84\xec\x9b\x8c\xed\x81\xac \xeb\xaa\xa8\xeb\x93\x9c\xec\x9d\x98 \xea\xb0\x92\xec\x9d\xb4 \xed\x95\xb4\xeb\x8b\xb9 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n# params : \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xea\xb5\xac\xec\x84\xb1\xed\x95\xa0\xeb\x95\x8c params \xea\xb0\x92\xeb\x93\xa4\xec\x9d\xb4\xeb\x8b\xa4. \n# (params={ # \xeb\xaa\xa8\xeb\x8d\xb8 \xec\xaa\xbd\xec\x9c\xbc\xeb\xa1\x9c \xed\x8c\x8c\xeb\x9d\xbc\xeb\xa9\x94\xed\x84\xb0 \xec\xa0\x84\xeb\x8b\xac\xed\x95\x9c\xeb\x8b\xa4.)\ndef Model(features, labels, mode, params):\n    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n    EVAL = mode == tf.estimator.ModeKeys.EVAL\n    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n\n    # \xeb\xaf\xb8\xeb\xa6\xac \xec\xa0\x95\xec\x9d\x98\xeb\x90\x9c  \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xed\x99\x95\xec\x9d\xb8 \xed\x95\x9c\xeb\x8b\xa4.\n    # \xea\xb0\x92\xec\x9d\xb4 True\xec\x9d\xb4\xeb\xa9\xb4 \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9\xec\x9d\x84 \xed\x95\xb4\xec\x84\x9c \xed\x95\x99\xec\x8a\xb5\xed\x95\x98\xea\xb3\xa0 False\xec\x9d\xb4\xeb\xa9\xb4 \n    # onehotencoding \xec\xb2\x98\xeb\xa6\xac \xed\x95\x9c\xeb\x8b\xa4.\n    if params[\'embedding\'] == True:\n        # \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x96\x89\xeb\xa0\xac\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\xb4\x88\xea\xb8\xb0\xed\x99\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\n        # xavier (Xavier Glorot\xec\x99\x80 Yoshua Bengio (2010)\n        # URL : http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n        initializer = tf.contrib.layers.xavier_initializer()\n        # \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xb3\x80\xec\x88\x98\xeb\xa5\xbc \xec\x84\xa0\xec\x96\xb8\xed\x95\x98\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\x84 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n        embedding_encoder = tf.get_variable(name=""embedding_encoder"",  # \xec\x9d\xb4\xeb\xa6\x84\n                                           shape=[params[\'vocabulary_length\'], params[\'embedding_size\']],  # \xeb\xaa\xa8\xec\x96\x91\n                                           dtype=tf.float32,  # \xed\x83\x80\xec\x9e\x85\n                                           initializer=initializer,  # \xec\xb4\x88\xea\xb8\xb0\xed\x99\x94 \xea\xb0\x92\n                                           trainable=True)  # \xed\x95\x99\xec\x8a\xb5 \xec\x9c\xa0\xeb\xac\xb4\n    else:\n        # tf.eye\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xec\x82\xac\xec\xa0\x84\xec\x9d\x98 \xed\x81\xac\xea\xb8\xb0 \xeb\xa7\x8c\xed\x81\xbc\xec\x9d\x98 \xeb\x8b\xa8\xec\x9c\x84\xed\x96\x89\xeb\xa0\xac \n        # \xea\xb5\xac\xec\xa1\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n        embedding_encoder = tf.eye(num_rows=params[\'vocabulary_length\'], dtype=tf.float32)\n        # \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xb3\x80\xec\x88\x98\xeb\xa5\xbc \xec\x84\xa0\xec\x96\xb8\xed\x95\x98\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\x84 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n        embedding_encoder = tf.get_variable(name=""embedding_encoder"",  # \xec\x9d\xb4\xeb\xa6\x84\n                                           initializer=embedding_encoder,  # \xec\xb4\x88\xea\xb8\xb0\xed\x99\x94 \xea\xb0\x92\n                                           trainable=False)  # \xed\x95\x99\xec\x8a\xb5 \xec\x9c\xa0\xeb\xac\xb4\n\n    # embedding_lookup\xec\x9d\x84 \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c features[\'input\']\xec\x9d\x98 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc\n    # \xec\x9c\x84\xec\x97\x90\xec\x84\x9c \xeb\xa7\x8c\xeb\x93\xa0 embedding_encoder\xec\x9d\x98 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xec\x9d\x98 \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x98\xec\x97\xac \n    # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9\xeb\x90\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xeb\xb0\xb0\xec\xb9\x98\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n    embedding_encoder_batch = tf.nn.embedding_lookup(params=embedding_encoder, ids=features[\'input\'])\n\n    # \xeb\xaf\xb8\xeb\xa6\xac \xec\xa0\x95\xec\x9d\x98\xeb\x90\x9c  \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xed\x99\x95\xec\x9d\xb8 \xed\x95\x9c\xeb\x8b\xa4.\n    # \xea\xb0\x92\xec\x9d\xb4 True\xec\x9d\xb4\xeb\xa9\xb4 \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9\xec\x9d\x84 \xed\x95\xb4\xec\x84\x9c \xed\x95\x99\xec\x8a\xb5\xed\x95\x98\xea\xb3\xa0 False\xec\x9d\xb4\xeb\xa9\xb4 \n    # onehotencoding \xec\xb2\x98\xeb\xa6\xac \xed\x95\x9c\xeb\x8b\xa4.\n    if params[\'embedding\'] == True:\n        # \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x96\x89\xeb\xa0\xac\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\xb4\x88\xea\xb8\xb0\xed\x99\x94 \xed\x95\xa8\xec\x88\x98\xec\x9d\xb4\xeb\x8b\xa4.\n        # xavier (Xavier Glorot\xec\x99\x80 Yoshua Bengio (2010)\n        # URL : http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n        initializer = tf.contrib.layers.xavier_initializer()\n        # \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xeb\xb3\x80\xec\x88\x98\xeb\xa5\xbc \xec\x84\xa0\xec\x96\xb8\xed\x95\x98\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\x84 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n        embedding_decoder = tf.get_variable(name=""embedding_decoder"",  # \xec\x9d\xb4\xeb\xa6\x84\n                                           shape=[params[\'vocabulary_length\'], params[\'embedding_size\']],  # \xeb\xaa\xa8\xec\x96\x91\n                                           dtype=tf.float32,  # \xed\x83\x80\xec\x9e\x85\n                                           initializer=initializer,  # \xec\xb4\x88\xea\xb8\xb0\xed\x99\x94 \xea\xb0\x92\n                                           trainable=True)  # \xed\x95\x99\xec\x8a\xb5 \xec\x9c\xa0\xeb\xac\xb4\n    else:\n        # tf.eye\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4\xec\x84\x9c \xec\x82\xac\xec\xa0\x84\xec\x9d\x98 \xed\x81\xac\xea\xb8\xb0 \xeb\xa7\x8c\xed\x81\xbc\xec\x9d\x98 \xeb\x8b\xa8\xec\x9c\x84\xed\x96\x89\xeb\xa0\xac \n        # \xea\xb5\xac\xec\xa1\xb0\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n        embedding_decoder = tf.eye(num_rows=params[\'vocabulary_length\'], dtype=tf.float32)\n        # \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xb3\x80\xec\x88\x98\xeb\xa5\xbc \xec\x84\xa0\xec\x96\xb8\xed\x95\x98\xea\xb3\xa0 \xea\xb0\x92\xec\x9d\x84 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n        embedding_decoder = tf.get_variable(name=\'embedding_decoder\',  # \xec\x9d\xb4\xeb\xa6\x84\n                                           initializer=embedding_decoder,  # \xec\xb4\x88\xea\xb8\xb0\xed\x99\x94 \xea\xb0\x92\n                                           trainable=False)  # \xed\x95\x99\xec\x8a\xb5 \xec\x9c\xa0\xeb\xac\xb4\n\n    # \xeb\xb3\x80\xec\x88\x98 \xec\x9e\xac\xec\x82\xac\xec\x9a\xa9\xec\x9d\x84 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c reuse=.AUTO_REUSE\xeb\xa5\xbc \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xeb\xa9\xb0 \xeb\xb2\x94\xec\x9c\x84\xeb\xa5\xbc\n    # \xec\xa0\x95\xed\x95\xb4\xec\xa3\xbc\xea\xb3\xa0 \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4 scope\xec\x84\xa4\xec\xa0\x95\xec\x9d\x84 \xed\x95\x9c\xeb\x8b\xa4.\n    # make_lstm_cell\xec\x9d\xb4 ""cell""\xeb\xb0\x98\xeb\xb3\xb5\xec\xa0\x81\xec\x9c\xbc\xeb\xa1\x9c \xed\x98\xb8\xec\xb6\x9c \xeb\x90\x98\xeb\xa9\xb4\xec\x84\x9c \xec\x9e\xac\xec\x82\xac\xec\x9a\xa9\xeb\x90\x9c\xeb\x8b\xa4.\n    with tf.variable_scope(\'encoder_scope\', reuse=tf.AUTO_REUSE):\n        # \xea\xb0\x92\xec\x9d\xb4 True\xec\x9d\xb4\xeb\xa9\xb4 \xeb\xa9\x80\xed\x8b\xb0\xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4\xeb\xa1\x9c \xeb\xaa\xa8\xeb\x8d\xb8\xec\x9d\x84 \xea\xb5\xac\xec\x84\xb1\xed\x95\x98\xea\xb3\xa0 False\xec\x9d\xb4\xeb\xa9\xb4 \n        # \xeb\x8b\xa8\xec\x9d\xbc\xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4\xeb\xa1\x9c \xeb\xaa\xa8\xeb\x8d\xb8\xec\x9d\x84 \xea\xb5\xac\xec\x84\xb1 \xed\x95\x9c\xeb\x8b\xa4.\n        if params[\'multilayer\'] == True:\n            # layerSize \xeb\xa7\x8c\xed\x81\xbc  LSTMCell\xec\x9d\x84  encoder_cell_list\xec\x97\x90 \xeb\x8b\xb4\xeb\x8a\x94\xeb\x8b\xa4.\n            encoder_cell_list = [make_lstm_cell(mode, params[\'hidden_size\'], i) for i in range(params[\'layer_size\'])]\n            # MUltiLayer RNN CEll\xec\x97\x90 encoder_cell_list\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xeb\xa9\x80\xed\x8b\xb0 \xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n            rnn_cell = tf.contrib.rnn.MultiRNNCell(encoder_cell_list, state_is_tuple=False)\n        else:\n            # \xeb\x8b\xa8\xec\xb8\xb5 LSTMLCell\xec\x9d\x84 \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n            rnn_cell = make_lstm_cell(mode, params[\'hidden_size\'], """")\n        # rnn_cell\xec\x97\x90 \xec\x9d\x98\xed\x95\xb4 \xec\xa7\x80\xec\xa0\x95\xeb\x90\x9c \xeb\xb0\x98\xeb\xb3\xb5\xec\xa0\x81\xec\x9d\xb8 \xec\x8b\xa0\xea\xb2\xbd\xeb\xa7\x9d\xec\x9d\x84 \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n        # encoder_outputs(RNN \xec\xb6\x9c\xeb\xa0\xa5 Tensor)[batch_size, \n        # max_time, cell.output_size]\n        # encoder_states \xec\xb5\x9c\xec\xa2\x85 \xec\x83\x81\xed\x83\x9c  [batch_size, cell.state_size]\n        encoder_outputs, encoder_states = tf.nn.dynamic_rnn(cell=rnn_cell,  # RNN \xec\x85\x80\n                                                              inputs=embedding_encoder_batch,  # \xec\x9e\x85\xeb\xa0\xa5 \xea\xb0\x92\n                                                              dtype=tf.float32)  # \xed\x83\x80\xec\x9e\x85\n        # \xeb\xb3\x80\xec\x88\x98 \xec\x9e\xac\xec\x82\xac\xec\x9a\xa9\xec\x9d\x84 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c reuse=.AUTO_REUSE\xeb\xa5\xbc \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xeb\xa9\xb0 \xeb\xb2\x94\xec\x9c\x84\xeb\xa5\xbc \xec\xa0\x95\xed\x95\xb4\xec\xa3\xbc\xea\xb3\xa0\n        # \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4 scope\xec\x84\xa4\xec\xa0\x95\xec\x9d\x84 \xed\x95\x9c\xeb\x8b\xa4.\n        # make_lstm_cell\xec\x9d\xb4 ""cell""\xeb\xb0\x98\xeb\xb3\xb5\xec\xa0\x81\xec\x9c\xbc\xeb\xa1\x9c \xed\x98\xb8\xec\xb6\x9c \xeb\x90\x98\xeb\xa9\xb4\xec\x84\x9c \xec\x9e\xac\xec\x82\xac\xec\x9a\xa9\xeb\x90\x9c\xeb\x8b\xa4.\n    with tf.variable_scope(\'decoder_scope\', reuse=tf.AUTO_REUSE):\n        # \xea\xb0\x92\xec\x9d\xb4 True\xec\x9d\xb4\xeb\xa9\xb4 \xeb\xa9\x80\xed\x8b\xb0\xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4\xeb\xa1\x9c \xeb\xaa\xa8\xeb\x8d\xb8\xec\x9d\x84 \xea\xb5\xac\xec\x84\xb1\xed\x95\x98\xea\xb3\xa0 False\xec\x9d\xb4\xeb\xa9\xb4 \xeb\x8b\xa8\xec\x9d\xbc\xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4\xeb\xa1\x9c\n        # \xeb\xaa\xa8\xeb\x8d\xb8\xec\x9d\x84 \xea\xb5\xac\xec\x84\xb1 \xed\x95\x9c\xeb\x8b\xa4.\n        if params[\'multilayer\'] == True:\n            # layer_size \xeb\xa7\x8c\xed\x81\xbc  LSTMCell\xec\x9d\x84  decoder_cell_list\xec\x97\x90 \xeb\x8b\xb4\xeb\x8a\x94\xeb\x8b\xa4.\n            decoder_cell_list = [make_lstm_cell(mode, params[\'hidden_size\'], i) for i in range(params[\'layer_size\'])]\n            # MUltiLayer RNN CEll\xec\x97\x90 decoder_cell_list\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xeb\xa9\x80\xed\x8b\xb0 \xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n            rnn_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list, state_is_tuple=False)\n        else:\n            # \xeb\x8b\xa8\xec\xb8\xb5 LSTMLCell\xec\x9d\x84 \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n            rnn_cell = make_lstm_cell(mode, params[\'hidden_size\'], """")\n\n        decoder_state = encoder_states\n        # \xeb\xa7\xa4 \xed\x83\x80\xec\x9e\x84 \xec\x8a\xa4\xed\x85\x9d\xec\x97\x90 \xeb\x82\x98\xec\x98\xa4\xeb\x8a\x94 \xec\x95\x84\xec\x9b\x83\xed\x92\x8b\xec\x9d\x84 \xec\xa0\x80\xec\x9e\xa5\xed\x95\x98\xeb\x8a\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8 \xeb\x91\x90\xea\xb0\x9c\xeb\xa5\xbc \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4. \n        # \xed\x95\x98\xeb\x82\x98\xeb\x8a\x94 \xed\x86\xa0\xed\x81\xb0 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\x8a\x94 predict_tokens \xec\xa0\x80\xec\x9e\xa5\n        # \xeb\x8b\xa4\xeb\xa5\xb8 \xed\x95\x98\xeb\x82\x98\xeb\x8a\x94 temp_logits\xec\x97\x90 logits \xec\xa0\x80\xec\x9e\xa5\xed\x95\x9c\xeb\x8b\xa4.\n        predict_tokens = list()\n        temp_logits = list()\n\n        # \xed\x8f\x89\xea\xb0\x80\xec\x9d\xb8 \xea\xb2\xbd\xec\x9a\xb0\xec\x97\x90\xeb\x8a\x94 teacher forcing\xec\x9d\xb4 \xeb\x90\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8f\x84\xeb\xa1\x9d \xed\x95\xb4\xec\x95\xbc\xed\x95\x9c\xeb\x8b\xa4.\n        # \xeb\x94\xb0\xeb\x9d\xbc\xec\x84\x9c \xed\x95\x99\xec\x8a\xb5\xec\x9d\xb4 \xec\x95\x84\xeb\x8b\x8c\xea\xb2\xbd\xec\x9a\xb0\xec\x97\x90 is_train\xec\x9d\x84 False\xeb\xa1\x9c \xed\x95\x98\xec\x97\xac teacher forcing\xec\x9d\xb4 \xeb\x90\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8f\x84\xeb\xa1\x9d \xed\x95\x9c\xeb\x8b\xa4.\n        output_token = tf.ones(shape=(tf.shape(encoder_outputs)[0],), dtype=tf.int32) * 1\n        # \xec\xa0\x84\xec\xb2\xb4 \xeb\xac\xb8\xec\x9e\xa5 \xea\xb8\xb8\xec\x9d\xb4 \xeb\xa7\x8c\xed\x81\xbc \xed\x83\x80\xec\x9e\x84 \xec\x8a\xa4\xed\x85\x9d\xec\x9d\x84 \xeb\x8f\x8c\xeb\x8f\x84\xeb\xa1\x9d \xed\x95\x9c\xeb\x8b\xa4.\n        for i in range(DEFINES.max_sequence_length):\n            # \xeb\x91\x90 \xeb\xb2\x88\xec\xa8\xb0 \xec\x8a\xa4\xed\x85\x9d \xec\x9d\xb4\xed\x9b\x84\xec\x97\x90\xeb\x8a\x94 teacher forcing\xec\x9d\x84 \xec\xa0\x81\xec\x9a\xa9\xed\x95\x98\xeb\x8a\x94\xec\xa7\x80 \xed\x99\x95\xeb\xa5\xa0\xec\x97\x90 \xeb\x94\xb0\xeb\x9d\xbc \xea\xb2\xb0\xec\xa0\x95\xed\x95\x98\xeb\x8f\x84\xeb\xa1\x9d \xed\x95\x9c\xeb\x8b\xa4.\n            # teacher forcing rate\xec\x9d\x80 teacher forcing\xec\x9d\x84 \xec\x96\xb4\xeb\x8a\x90\xec\xa0\x95\xeb\x8f\x84 \xec\xa4\x84 \xea\xb2\x83\xec\x9d\xb8\xec\xa7\x80\xeb\xa5\xbc \xec\xa1\xb0\xec\xa0\x88\xed\x95\x9c\xeb\x8b\xa4.\n            if TRAIN:\n                if i > 0:\n                    # tf.cond\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 rnn\xec\x97\x90 \xec\x9e\x85\xeb\xa0\xa5\xed\x95\xa0 \xec\x9e\x85\xeb\xa0\xa5 \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xeb\xb2\xa1\xed\x84\xb0\xeb\xa5\xbc \xea\xb2\xb0\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4 \xec\x97\xac\xea\xb8\xb0\xec\x84\x9c true\xec\x9d\xb8 \xea\xb2\xbd\xec\x9a\xb0\xec\x97\x94 \xec\x9e\x85\xeb\xa0\xa5\xeb\x90\x9c output\xea\xb0\x92 \xec\x95\x84\xeb\x8b\x8c\xea\xb2\xbd\xec\x9a\xb0\xec\x97\x90\xeb\x8a\x94 \xec\x9d\xb4\xec\xa0\x84 \xec\x8a\xa4\xed\x85\x9d\xec\x97\x90\n                    # \xeb\x82\x98\xec\x98\xa8 output\xec\x9d\x84 \xec\x82\xac\xec\x9a\xa9\xed\x95\x9c\xeb\x8b\xa4.\n                    input_token_emb = tf.cond(\n                        tf.logical_and( # \xeb\x85\xbc\xeb\xa6\xac and \xec\x97\xb0\xec\x82\xb0\xec\x9e\x90\n                            True,\n                            tf.random_uniform(shape=(), maxval=1) <= params[\'teacher_forcing_rate\'] # \xeb\xa5\xa0\xec\x97\x90 \xeb\x94\xb0\xeb\xa5\xb8 labels\xea\xb0\x92 \xec\xa7\x80\xec\x9b\x90 \xec\x9c\xa0\xeb\xac\xb4\n                        ),\n                        lambda: tf.nn.embedding_lookup(embedding_decoder, labels[:, i-1]),  # labels \xec\xa0\x95\xeb\x8b\xb5\xec\x9d\x84 \xeb\x84\xa3\xec\x96\xb4\xec\xa3\xbc\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4.\n                        lambda: tf.nn.embedding_lookup(embedding_decoder, output_token) # \xeb\xaa\xa8\xeb\x8d\xb8\xec\x9d\xb4 \xec\xa0\x95\xeb\x8b\xb5\xec\x9d\xb4\xeb\x9d\xbc\xea\xb3\xa0 \xec\x83\x9d\xea\xb0\x81 \xed\x95\x98\xeb\x8a\x94 \xea\xb0\x92\n                    )\n                else:\n                    input_token_emb = tf.nn.embedding_lookup(embedding_decoder, output_token) # \xeb\xaa\xa8\xeb\x8d\xb8\xec\x9d\xb4 \xec\xa0\x95\xeb\x8b\xb5\xec\x9d\xb4\xeb\x9d\xbc\xea\xb3\xa0 \xec\x83\x9d\xea\xb0\x81 \xed\x95\x98\xeb\x8a\x94 \xea\xb0\x92\n            else: # \xed\x8f\x89\xea\xb0\x80 \xeb\xb0\x8f \xec\x98\x88\xec\xb8\xa1\xec\x9d\x80 \xec\x97\xac\xea\xb8\xb0\xeb\xa5\xbc \xec\xa7\x84\xed\x96\x89\xed\x95\xb4\xec\x95\xbc \xed\x95\x9c\xeb\x8b\xa4. \n                input_token_emb = tf.nn.embedding_lookup(embedding_decoder, output_token)\n\n            # \xec\x96\xb4\xed\x85\x90\xec\x85\x98 \xec\xa0\x81\xec\x9a\xa9 \xeb\xb6\x80\xeb\xb6\x84\n            if params[\'attention\'] == True:\n                W1 = tf.keras.layers.Dense(params[\'hidden_size\'])\n                W2 = tf.keras.layers.Dense(params[\'hidden_size\'])\n                V = tf.keras.layers.Dense(1)\n                # (?, 256) -> (?, 128)\n                hidden_with_time_axis = W2(decoder_state)\n                # (?, 128) -> (?, 1, 128)\n                hidden_with_time_axis = tf.expand_dims(hidden_with_time_axis, axis=1)\n                # (?, 1, 128) -> (?, 25, 128)\n                hidden_with_time_axis = tf.manip.tile(hidden_with_time_axis, [1, DEFINES.max_sequence_length, 1])\n                # (?, 25, 1)\n                score = V(tf.nn.tanh(W1(encoder_outputs) + hidden_with_time_axis))\n                # score = V(tf.nn.tanh(W1(encoderOutputs) + tf.manip.tile(tf.expand_dims(W2(decoder_state), axis=1), [1, DEFINES.maxSequenceLength, 1])))\n                # (?, 25, 1)\n                attention_weights = tf.nn.softmax(score, axis=-1)\n                # (?, 25, 128)\n                context_vector = attention_weights * encoder_outputs\n                # (?, 25, 128) -> (?, 128)\n                context_vector = tf.reduce_sum(context_vector, axis=1)\n                # (?, 256)\n                input_token_emb = tf.concat([context_vector, input_token_emb], axis=-1)\n\n            # RNNCell\xec\x9d\x84 \xed\x98\xb8\xec\xb6\x9c\xed\x95\x98\xec\x97\xac RNN \xec\x8a\xa4\xed\x85\x9d \xec\x97\xb0\xec\x82\xb0\xec\x9d\x84 \xec\xa7\x84\xed\x96\x89\xed\x95\x98\xeb\x8f\x84\xeb\xa1\x9d \xed\x95\x9c\xeb\x8b\xa4.\n            input_token_emb = tf.keras.layers.Dropout(0.5)(input_token_emb)\n            decoder_outputs, decoder_state = rnn_cell(input_token_emb, decoder_state)\n            decoder_outputs = tf.keras.layers.Dropout(0.5)(decoder_outputs)\n            # feedforward\xeb\xa5\xbc \xea\xb1\xb0\xec\xb3\x90 output\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c logit\xea\xb0\x92\xec\x9d\x84 \xea\xb5\xac\xed\x95\x9c\xeb\x8b\xa4.\n            output_logits = tf.layers.dense(decoder_outputs, params[\'vocabulary_length\'], activation=None)\n\n            # softmax\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x8b\xa8\xec\x96\xb4\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\x98\x88\xec\xb8\xa1 probability\xeb\xa5\xbc \xea\xb5\xac\xed\x95\x9c\xeb\x8b\xa4.\n            output_probs = tf.nn.softmax(output_logits)\n            output_token = tf.argmax(output_probs, axis=-1)\n\n            # \xed\x95\x9c \xec\x8a\xa4\xed\x85\x9d\xec\x97\x90 \xeb\x82\x98\xec\x98\xa8 \xed\x86\xa0\xed\x81\xb0\xea\xb3\xbc logit \xea\xb2\xb0\xea\xb3\xbc\xeb\xa5\xbc \xec\xa0\x80\xec\x9e\xa5\xed\x95\xb4\xeb\x91\x94\xeb\x8b\xa4.\n            predict_tokens.append(output_token)\n            temp_logits.append(output_logits)\n\n        # \xec\xa0\x80\xec\x9e\xa5\xed\x96\x88\xeb\x8d\x98 \xed\x86\xa0\xed\x81\xb0\xea\xb3\xbc logit \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc stack\xec\x9d\x84 \xed\x86\xb5\xed\x95\xb4 \xeb\xa9\x94\xed\x8a\xb8\xeb\xa6\xad\xec\x8a\xa4\xeb\xa1\x9c \xeb\xa7\x8c\xeb\x93\xa4\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.\n        # \xeb\xa7\x8c\xeb\x93\xa4\xea\xb2\x8c \xeb\x99\xa4\xeb\xa9\xb4 \xec\xb0\xa8\xec\x9b\x90\xec\x9d\xb4 [\xec\x8b\x9c\xed\x80\x80\xec\x8a\xa4 X \xeb\xb0\xb0\xec\xb9\x98 X \xeb\x8b\xa8\xec\x96\xb4 feature \xec\x88\x98] \xec\x9d\xb4\xeb\xa0\x87\xea\xb2\x8c \xeb\x90\x98\xeb\x8a\x94\xeb\x8d\xb0\n        # \xec\x9d\xb4\xeb\xa5\xbc transpose\xed\x95\x98\xec\x97\xac [\xeb\xb0\xb0\xec\xb9\x98 X \xec\x8b\x9c\xed\x80\x80\xec\x8a\xa4 X \xeb\x8b\xa8\xec\x96\xb4 feature \xec\x88\x98] \xeb\xa1\x9c \xeb\xa7\x9e\xec\xb6\xb0\xec\xa4\x80\xeb\x8b\xa4.\n        predict = tf.transpose(tf.stack(predict_tokens, axis=0), [1, 0])\n        logits = tf.transpose(tf.stack(temp_logits, axis=0), [1, 0, 2])\n\n        print(predict.shape)\n        print(logits.shape)\n\n    if PREDICT:\n        if params[\'serving\'] == True:\n            export_outputs = {\n                \'indexs\': tf.estimator.export.PredictOutput(predict) # \xec\x84\x9c\xeb\xb9\x99 \xea\xb2\xb0\xea\xb3\xbc\xea\xb0\x92\xec\x9d\x84 \xec\xa4\x80\xeb\x8b\xa4.\n            }\n\n        predictions = {  # \xec\x98\x88\xec\xb8\xa1 \xea\xb0\x92\xeb\x93\xa4\xec\x9d\xb4 \xec\x97\xac\xea\xb8\xb0\xec\x97\x90 \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac \xed\x98\x95\xed\x83\x9c\xeb\xa1\x9c \xeb\x8b\xb4\xea\xb8\xb4\xeb\x8b\xa4.\n            \'indexs\': predict,  # \xec\x8b\x9c\xed\x80\x80\xec\x8a\xa4 \xeb\xa7\x88\xeb\x8b\xa4 \xec\x98\x88\xec\xb8\xa1\xed\x95\x9c \xea\xb0\x92\n            \'logits\': logits,  # \xeb\xa7\x88\xec\xa7\x80\xeb\xa7\x89 \xea\xb2\xb0\xea\xb3\xbc \xea\xb0\x92\n        }\n        # \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0\xec\x97\x90\xec\x84\x9c \xeb\xa6\xac\xed\x84\xb4\xed\x95\x98\xeb\x8a\x94 \xea\xb0\x92\xec\x9d\x80 tf.estimator.EstimatorSpec \n        # \xea\xb0\x9d\xec\xb2\xb4\xeb\xa5\xbc \xeb\xa6\xac\xed\x84\xb4 \xed\x95\x9c\xeb\x8b\xa4.\n        # mode : \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0\xea\xb0\x80 \xec\x88\x98\xed\x96\x89\xed\x95\x98\xeb\x8a\x94 mode (tf.estimator.ModeKeys.PREDICT)\n        # predictions : \xec\x98\x88\xec\xb8\xa1 \xea\xb0\x92\n        if params[\'serving\'] == True:\n            return tf.estimator.EstimatorSpec(mode, predictions=predictions, export_outputs=export_outputs)\n\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n    # \xeb\xa7\x88\xec\xa7\x80\xeb\xa7\x89 \xea\xb2\xb0\xea\xb3\xbc \xea\xb0\x92\xea\xb3\xbc \xec\xa0\x95\xeb\x8b\xb5 \xea\xb0\x92\xec\x9d\x84 \xeb\xb9\x84\xea\xb5\x90\xed\x95\x98\xeb\x8a\x94 \n    # tf.nn.sparse_softmax_cross_entropy_with_logits(\xeb\xa1\x9c\xec\x8a\xa4\xed\x95\xa8\xec\x88\x98)\xeb\xa5\xbc \n    # \xed\x86\xb5\xea\xb3\xbc \xec\x8b\x9c\xec\xbc\x9c \xed\x8b\x80\xeb\xa6\xb0 \xeb\xa7\x8c\xed\x81\xbc\xec\x9d\x98\n    # \xec\x97\x90\xeb\x9f\xac \xea\xb0\x92\xec\x9d\x84 \xea\xb0\x80\xec\xa0\xb8 \xec\x98\xa4\xea\xb3\xa0 \xec\x9d\xb4\xea\xb2\x83\xeb\x93\xa4\xec\x9d\x80 \xec\xb0\xa8\xec\x9b\x90 \xec\xb6\x95\xec\x86\x8c\xeb\xa5\xbc \xed\x86\xb5\xed\x95\xb4 \xeb\x8b\xa8\xec\x9d\xbc \xed\x85\x90\xec\x84\x9c \xea\xb0\x92\xec\x9d\x84 \xeb\xb0\x98\xed\x99\x98 \xed\x95\x9c\xeb\x8b\xa4.\n    # pad\xec\x9d\x98 loss\xea\xb0\x92\xec\x9d\x84 \xeb\xac\xb4\xeb\xa0\xa5\xed\x99\x94 \xec\x8b\x9c\xed\x82\xa8\xeb\x8b\xa4. pad\xea\xb0\x80 \xec\x95\x84\xeb\x8b\x8c\xea\xb0\x92\xec\x9d\x80 1 pad\xec\x9d\xb8 \xea\xb0\x92\xec\x9d\x80 0\xec\x9d\x84 \xec\xa3\xbc\xec\x96\xb4 \xeb\x8f\x99\xec\x9e\x91\n    # \xed\x95\x98\xeb\x8f\x84\xeb\xa1\x9d \xed\x95\x9c\xeb\x8b\xa4.\n    # \xec\xa0\x95\xeb\x8b\xb5 \xec\xb0\xa8\xec\x9b\x90 \xeb\xb3\x80\xea\xb2\xbd\xec\x9d\x84 \xed\x95\x9c\xeb\x8b\xa4. [\xeb\xb0\xb0\xec\xb9\x98 * max_sequence_length * vocabulary_length]  \n    # logits\xea\xb3\xbc \xea\xb0\x99\xec\x9d\x80 \xec\xb0\xa8\xec\x9b\x90\xec\x9d\x84 \xeb\xa7\x8c\xeb\x93\xa4\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xa8\xec\x9d\xb4\xeb\x8b\xa4.\n    labels_ = tf.one_hot(labels, params[\'vocabulary_length\'])\n    \n    if TRAIN and params[\'loss_mask\'] == True:\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_))\n        masks = features[\'length\']\n\n        loss = loss * tf.cast(masks, tf.float32)\n        loss = tf.reduce_mean(loss)\n    else:\n       loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_))\n    # \xeb\x9d\xbc\xeb\xb2\xa8\xea\xb3\xbc \xea\xb2\xb0\xea\xb3\xbc\xea\xb0\x80 \xec\x9d\xbc\xec\xb9\x98\xed\x95\x98\xeb\x8a\x94\xec\xa7\x80 \xeb\xb9\x88\xeb\x8f\x84 \xea\xb3\x84\xec\x82\xb0\xec\x9d\x84 \xed\x86\xb5\xed\x95\xb4 \n    # \xec\xa0\x95\xed\x99\x95\xeb\x8f\x84\xeb\xa5\xbc \xec\xb8\xa1\xec\xa0\x95\xed\x95\x98\xeb\x8a\x94 \xeb\xb0\xa9\xeb\xb2\x95\xec\x9d\xb4\xeb\x8b\xa4.\n    accuracy = tf.metrics.accuracy(labels=labels, predictions=predict, name=\'accOp\')\n\n    # \xec\xa0\x95\xed\x99\x95\xeb\x8f\x84\xeb\xa5\xbc \xec\xa0\x84\xec\xb2\xb4\xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c \xeb\x82\x98\xeb\x88\x88 \xea\xb0\x92\xec\x9d\xb4\xeb\x8b\xa4.\n    metrics = {\'accuracy\': accuracy}\n    tf.summary.scalar(\'accuracy\', accuracy[1])\n\n    # \xed\x8f\x89\xea\xb0\x80 mode \xed\x99\x95\xec\x9d\xb8 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\xa9\xb0 \xed\x8f\x89\xea\xb0\x80\xeb\x8a\x94 \xec\x97\xac\xea\xb8\xb0 \xea\xb9\x8c\xec\xa7\x80 \n    # \xec\x88\x98\xed\x96\x89\xed\x95\x98\xea\xb3\xa0 \xeb\xa6\xac\xed\x84\xb4\xed\x95\x9c\xeb\x8b\xa4.\n    if EVAL:\n        # \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0\xec\x97\x90\xec\x84\x9c \xeb\xa6\xac\xed\x84\xb4\xed\x95\x98\xeb\x8a\x94 \xea\xb0\x92\xec\x9d\x80 \n        # tf.estimator.EstimatorSpec \xea\xb0\x9d\xec\xb2\xb4\xeb\xa5\xbc \xeb\xa6\xac\xed\x84\xb4 \xed\x95\x9c\xeb\x8b\xa4.\n        # mode : \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0\xea\xb0\x80 \xec\x88\x98\xed\x96\x89\xed\x95\x98\xeb\x8a\x94 mode (tf.estimator.ModeKeys.EVAL)\n        # loss : \xec\x97\x90\xeb\x9f\xac \xea\xb0\x92\n        # eval_metric_ops : \xec\xa0\x95\xed\x99\x95\xeb\x8f\x84 \xea\xb0\x92\n        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n\n    # \xed\x8c\x8c\xec\x9d\xb4\xec\x8d\xac assert\xea\xb5\xac\xeb\xac\xb8\xec\x9c\xbc\xeb\xa1\x9c \xea\xb1\xb0\xec\xa7\x93\xec\x9d\xbc \xea\xb2\xbd\xec\x9a\xb0 \xed\x94\x84\xeb\xa1\x9c\xea\xb7\xb8\xeb\x9e\xa8\xec\x9d\xb4 \xec\xa2\x85\xeb\xa3\x8c \xeb\x90\x9c\xeb\x8b\xa4.\n    # \xec\x88\x98\xed\x96\x89 mode(tf.estimator.ModeKeys.TRAIN)\xea\xb0\x80 \n    # \xec\x95\x84\xeb\x8b\x8c \xea\xb2\xbd\xec\x9a\xb0\xeb\x8a\x94 \xec\x97\xac\xea\xb8\xb0 \xea\xb9\x8c\xec\xa7\x80 \xec\x98\xa4\xeb\xa9\xb4 \xec\x95\x88\xeb\x90\x98\xeb\x8f\x84\xeb\xa1\x9d \xeb\xb0\xa9\xec\x96\xb4\xec\xa0\x81 \xec\xbd\x94\xeb\x93\x9c\xeb\xa5\xbc \xeb\x84\xa3\xec\x9d\x80\xea\xb2\x83\xec\x9d\xb4\xeb\x8b\xa4.\n    assert TRAIN\n\n    # \xec\x95\x84\xeb\x8b\xb4 \xec\x98\xb5\xed\x8b\xb0\xeb\xa7\x88\xec\x9d\xb4\xec\xa0\x80\xeb\xa5\xbc \xec\x82\xac\xec\x9a\xa9\xed\x95\x9c\xeb\x8b\xa4.\n    optimizer = tf.train.AdamOptimizer(learning_rate=DEFINES.learning_rate)\n    # \xec\x97\x90\xeb\x9f\xac\xea\xb0\x92\xec\x9d\x84 \xec\x98\xb5\xed\x8b\xb0\xeb\xa7\x88\xec\x9d\xb4\xec\xa0\x80\xeb\xa5\xbc \xec\x82\xac\xec\x9a\xa9\xed\x95\xb4\xec\x84\x9c \xec\xb5\x9c\xec\x86\x8c\xed\x99\x94 \xec\x8b\x9c\xed\x82\xa8\xeb\x8b\xa4.\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n    # \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0\xec\x97\x90\xec\x84\x9c \xeb\xa6\xac\xed\x84\xb4\xed\x95\x98\xeb\x8a\x94 \xea\xb0\x92\xec\x9d\x80 tf.estimator.EstimatorSpec \xea\xb0\x9d\xec\xb2\xb4\xeb\xa5\xbc \xeb\xa6\xac\xed\x84\xb4 \xed\x95\x9c\xeb\x8b\xa4.\n    # mode : \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0\xea\xb0\x80 \xec\x88\x98\xed\x96\x89\xed\x95\x98\xeb\x8a\x94 mode (tf.estimator.ModeKeys.EVAL)\n    # loss : \xec\x97\x90\xeb\x9f\xac \xea\xb0\x92\n    # train_op : \xea\xb7\xb8\xeb\x9d\xbc\xeb\x94\x94\xec\x96\xb8\xed\x8a\xb8 \xeb\xb0\x98\xed\x99\x98\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n'"
6.CHATBOT/Appendix/predict.py,0,"b'import tensorflow as tf\nimport data\nimport sys\nimport model as ml\n\nfrom configs import DEFINES\n\t\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    arg_length = len(sys.argv)\n    \n    if(arg_length < 2):\n        raise Exception(""Don\'t call us. We\'ll call you"")\n  \n    \n    # \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xed\x86\xb5\xed\x95\x9c \xec\x82\xac\xec\xa0\x84 \xea\xb5\xac\xec\x84\xb1 \xed\x95\x9c\xeb\x8b\xa4.\n    char2idx,  idx2char, vocabulary_length = data.load_vocabulary()\n\n    # \xed\x85\x8c\xec\x8a\xa4\xed\x8a\xb8\xec\x9a\xa9 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xeb\xa7\x8c\xeb\x93\x9c\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n    # \xec\x9d\xb8\xec\xbd\x94\xeb\x94\xa9 \xeb\xb6\x80\xeb\xb6\x84 \xeb\xa7\x8c\xeb\x93\xa0\xeb\x8b\xa4.\n    input = """"\n    for i in sys.argv[1:]:\n        input += i \n        input += "" ""\n        \n    print(input)\n    predic_input_enc, predic_input_enc_length = data.enc_processing([input], char2idx)\n    # \xed\x95\x99\xec\x8a\xb5 \xea\xb3\xbc\xec\xa0\x95\xec\x9d\xb4 \xec\x95\x84\xeb\x8b\x88\xeb\xaf\x80\xeb\xa1\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\x9e\x85\xeb\xa0\xa5\xec\x9d\x80 \n    # \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94\xeb\x8b\xa4.(\xea\xb5\xac\xec\xa1\xb0\xeb\xa5\xbc \xeb\xa7\x9e\xec\xb6\x94\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.)\n    # \xed\x95\x99\xec\x8a\xb5 \xea\xb3\xbc\xec\xa0\x95\xec\x9d\xb4 \xec\x95\x84\xeb\x8b\x88\xeb\xaf\x80\xeb\xa1\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x94\xa9 \xec\xb6\x9c\xeb\xa0\xa5 \xeb\xb6\x80\xeb\xb6\x84\xeb\x8f\x84 \n    # \xec\xa1\xb4\xec\x9e\xac\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94\xeb\x8b\xa4.(\xea\xb5\xac\xec\xa1\xb0\xeb\xa5\xbc \xeb\xa7\x9e\xec\xb6\x94\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4 \xeb\x84\xa3\xeb\x8a\x94\xeb\x8b\xa4.)\n    predic_target_dec, _ = data.dec_target_processing([""""], char2idx)\n\n    if DEFINES.serving == True:\n        # \xeb\xaa\xa8\xeb\x8d\xb8\xec\x9d\xb4 \xec\xa0\x80\xec\x9e\xa5\xeb\x90\x9c \xec\x9c\x84\xec\xb9\x98\xeb\xa5\xbc \xeb\x84\xa3\xec\x96\xb4 \xec\xa4\x80\xeb\x8b\xa4.  export_dir\n        predictor_fn = tf.contrib.predictor.from_saved_model(\n            export_dir=""/home/evo_mind/DeepLearning/NLP/Work/ChatBot2_Final/data_out/model/1541575161""\n        )\n    else:\n        # \xec\x97\x90\xec\x8a\xa4\xed\x8b\xb0\xeb\xa9\x94\xec\x9d\xb4\xed\x84\xb0 \xea\xb5\xac\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\n        classifier = tf.estimator.Estimator(\n                model_fn=ml.Model, # \xeb\xaa\xa8\xeb\x8d\xb8 \xeb\x93\xb1\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\n                model_dir=DEFINES.check_point_path, # \xec\xb2\xb4\xed\x81\xac\xed\x8f\xac\xec\x9d\xb8\xed\x8a\xb8 \xec\x9c\x84\xec\xb9\x98 \xeb\x93\xb1\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\n                params={ # \xeb\xaa\xa8\xeb\x8d\xb8 \xec\xaa\xbd\xec\x9c\xbc\xeb\xa1\x9c \xed\x8c\x8c\xeb\x9d\xbc\xeb\xa9\x94\xed\x84\xb0 \xec\xa0\x84\xeb\x8b\xac\xed\x95\x9c\xeb\x8b\xa4.\n                    \'hidden_size\': DEFINES.hidden_size,  # \xea\xb0\x80\xec\xa4\x91\xec\xb9\x98 \xed\x81\xac\xea\xb8\xb0 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n                    \'layer_size\': DEFINES.layer_size,  # \xeb\xa9\x80\xed\x8b\xb0 \xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4 \xec\xb8\xb5 \xea\xb0\x9c\xec\x88\x98\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n                    \'learning_rate\': DEFINES.learning_rate,  # \xed\x95\x99\xec\x8a\xb5\xec\x9c\xa8 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n                    \'teacher_forcing_rate\': DEFINES.teacher_forcing_rate, # \xed\x95\x99\xec\x8a\xb5\xec\x8b\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x8d\x94 \xec\x9d\xb8\xed\x92\x8b \xec\xa0\x95\xeb\x8b\xb5 \xec\xa7\x80\xec\x9b\x90\xec\x9c\xa8 \xec\x84\xa4\xec\xa0\x95\n                    \'vocabulary_length\': vocabulary_length,  # \xeb\x94\x95\xec\x85\x94\xeb\x84\x88\xeb\xa6\xac \xed\x81\xac\xea\xb8\xb0\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n                    \'embedding_size\': DEFINES.embedding_size,  # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xed\x81\xac\xea\xb8\xb0\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n                    \'embedding\': DEFINES.embedding,  # \xec\x9e\x84\xeb\xb2\xa0\xeb\x94\xa9 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n                    \'multilayer\': DEFINES.multilayer,  # \xeb\xa9\x80\xed\x8b\xb0 \xeb\xa0\x88\xec\x9d\xb4\xec\x96\xb4 \xec\x82\xac\xec\x9a\xa9 \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n                    \'attention\': DEFINES.attention, #  \xec\x96\xb4\xed\x85\x90\xec\x85\x98 \xec\xa7\x80\xec\x9b\x90 \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n                    \'teacher_forcing\': DEFINES.teacher_forcing, # \xed\x95\x99\xec\x8a\xb5\xec\x8b\x9c \xeb\x94\x94\xec\xbd\x94\xeb\x8d\x94 \xec\x9d\xb8\xed\x92\x8b \xec\xa0\x95\xeb\x8b\xb5 \xec\xa7\x80\xec\x9b\x90 \xec\x9c\xa0\xeb\xac\xb4 \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n                    \'loss_mask\': DEFINES.loss_mask, # PAD\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xeb\xa7\x88\xec\x8a\xa4\xed\x81\xac\xeb\xa5\xbc \xed\x86\xb5\xed\x95\x9c loss\xeb\xa5\xbc \xec\xa0\x9c\xed\x95\x9c \xed\x95\x9c\xeb\x8b\xa4.\n                    \'serving\': DEFINES.serving # \xeb\xaa\xa8\xeb\x8d\xb8 \xec\xa0\x80\xec\x9e\xa5 \xeb\xb0\x8f serving \xec\x9c\xa0\xeb\xac\xb4\xeb\xa5\xbc \xec\x84\xa4\xec\xa0\x95\xed\x95\x9c\xeb\x8b\xa4.\n                })\n\n    if DEFINES.serving == True:\n        predictions = predictor_fn({\'input\':predic_input_enc, \'output\':predic_target_dec})\n        data.pred2string(predictions, idx2char)\n    else:\n        # \xec\x98\x88\xec\xb8\xa1\xec\x9d\x84 \xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n        predictions = classifier.predict(\n            input_fn=lambda:data.eval_input_fn(predic_input_enc, predic_target_dec, DEFINES.batch_size))\n        # \xec\x98\x88\xec\xb8\xa1\xed\x95\x9c \xea\xb0\x92\xec\x9d\x84 \xec\x9d\xb8\xec\xa7\x80 \xed\x95\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8f\x84\xeb\xa1\x9d \n        # \xed\x85\x8d\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xeb\xb3\x80\xea\xb2\xbd\xed\x95\x98\xeb\x8a\x94 \xeb\xb6\x80\xeb\xb6\x84\xec\x9d\xb4\xeb\x8b\xa4.\n        data.pred2string(predictions, idx2char)\n'"
