file_path,api_count,code
Spam_Filter/NB.py,1,"b'""""""Spam filter using Naive Bayes classifier""""""\n\n\nimport email.parser \nimport os, sys, stat\nfrom tqdm import tqdm\nimport re, cgi\nimport math, pickle\nfrom decimal import Decimal\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support\nimport pandas as pd\nimport numpy as np\n\ndef extract_content(filename):\n\t\'\'\' Extract the subject and payload from the .eml file.\'\'\'\n\twith open(filename, \'rb\') as fp:\n\t\tmsg = email.message_from_bytes(fp.read())\n\tsub = msg.get(\'subject\')\n\t#If it is a multipart message, get_payload returns a list of parts.\n\tif msg.is_multipart():\n\t\tpayload = msg.get_payload()[0]\t\n\t\tpayload = payload.as_bytes() #We will consider the body as bytes so it is easier to decode into a unicode string.\n\telse:\n\t\tpayload =  msg.get_payload()\n\treturn ""{}\\n{}"" . format(sub, payload)\n\ndef get_text_from_email(mail):\n\t"""""" Removes html tags and punctuations.""""""\n\ttag_re = re.compile(r\'(<!--.*?-->|<[^>]*>)\')\n\n\t# Remove well-formed tags, fixing mistakes by legitimate users\n\tmail = tag_re.sub(\'\', mail)\n\n\t# Clean up anything else by escaping\n\tmail = cgi.escape(mail)\n\t\n\tmail = re.sub(r\'([\\\\][n|t|x])\', \' \', mail)                           #Removes \\n\\t\\b strings\n\tmail = re.sub(r\'[=*/&;.,/\\"" ?:<>\\[\\]\\(\\)\\{\\}\\|%#`~\\\\]\', \' \', mail)   #Removes punctuations\n\tmail = re.sub(r\'[- _=+]{2,}|(?=\\s)[-_]|[-_](?=\\s)\', \' \', mail)       #Removes unnecessary hiphens and underscores\n\tmail = re.sub(r\'[\\d]\', \' \', mail)                                    #Revoves all digits\n\tmail = re.sub(r\'[\\\'!=+]\', \'\', mail)                                  #Replaces these punctuations with null string\n\treturn mail.lower()\n\n\ndef preprocess(mail):\n\t""""""Preprocess data""""""\n\t# Currently just one preprocessing step.\n\tmail = get_text_from_email(mail)\n\treturn mail\n\n\ndef add_words_to_dict(word_set, word_dict, ham):\n\t""""""Checks if the word is presnt or not and increments its respective value""""""\n\tfor word in word_set:\n\t\tif word not in word_dict:\n\t\t\tword_dict[word] = {\'spam_count\': 0, \'ham_count\': 0}\n\t\tif ham:\n\t\t\tword_dict[word][\'ham_count\'] = word_dict[word][\'ham_count\'] + 1\n\t\telse:\n\t\t\tword_dict[word][\'spam_count\'] = word_dict[word][\'spam_count\'] + 1 \n\ndef calculate_spaminess(word, word_dict, total_ham, total_spam):\n\t"""""" Calculate the probability of a message being spam provided that the word is present.""""""\n\n\tpr_s, pr_h = 0.5, 0.5  #Assumming equal probability for both ham and spam\n\tthreshold = 2   #Strength factor to handle rare words\n\ttotal_occurance = word_dict[word][\'spam_count\'] + word_dict[word][\'ham_count\']  #Total number of times the word has occured in both ham and spam\n\tfreq_s = word_dict[word][\'spam_count\'] / total_spam \n\tfreq_h = word_dict[word][\'ham_count\'] / total_ham\n\tspamminess = (freq_s * pr_s) / (freq_s * pr_s + freq_h * pr_h)  #The probability that a given mail is spam, provided that this word is present.\n\tcorrected_spaminess = (0.3 * threshold + total_occurance * spamminess) / (threshold + total_occurance)  #Considering the strength factor.\n\tword_dict[word][\'spaminess\'] = corrected_spaminess   \n\ndef generate_dictionary(files, labels):\n\t""""""Generates a dictionary of all the words in both ham and spam mails""""""\n\t#Initializing variables\n\titerator = 0\n\tword_dict = {}\n\ttotal_spam = 0\n\ttotal_ham = 0\n\n\tfor file in tqdm(files):\n\t\t#Read and extract mail contents\n\t\ttry:\n\t\t\tmail = extract_content(file)\n\t\texcept:\n\t\t\tprint(""Corrupted File {}"" . format(file))\n\t\t# Prepare data\n\t\tmail = preprocess(mail)\n\t\tword_list = [s for s in mail.split()]\n\t\tword_set = set(word_list)\n\n\t\t# Incrementing HAM/SPAM count\n\t\tham = (True if int(labels[iterator].split()[0]) == 1 else False)\n\t\tif ham:\n\t\t\ttotal_ham += 1\n\t\telse:\n\t\t\ttotal_spam += 1\n\n\t\tadd_words_to_dict(word_set, word_dict, ham)\n\t\titerator += 1\n\tfor word in word_dict:\n\t\tcalculate_spaminess(word, word_dict, total_ham, total_spam)\n\twith open(\'word_dict.pickle\', \'wb\') as f:\n\t\tpickle.dump(word_dict, f)\n\treturn word_dict\n\ndef get_scores(expected, predicted):\n\t"""""" Compares predicted and expected values and returns various metrics.""""""\n\tscores = {}\n\t# _ implies we do not care about that metric.\n\t_, scores[\'False Positives\'], scores[\'False Negatives\'], _= confusion_matrix(expected, predicted).ravel()\n\tscores[\'Precision\'], scores[\'Recall\'], scores[\'F_score\'], _= precision_recall_fscore_support(expected, predicted, average=\'macro\')\n\treturn scores\n\ndef training(files, labels):\n\t""""""Trains the model and returns a word dictionary""""""\n\ttry:\n\t\twith open(\'word_dict.pickle\', \'rb\') as f:\n\t\t\tprint(""Found pickle file. Skipping training"")\n\t\t\tword_dict = pickle.load(f)\n\texcept:\n\t\t# Generate Dictionary\n\t\tword_dict = generate_dictionary(files, labels)\n\n\treturn word_dict\n\ndef predict(files, word_dict):\n\t""""""Predicts values using the word dictionary and returns a list of predictions""""""\n\tpredictions = []\n\tfor file in tqdm(files):\n\t\t#Read and extract mail contents\n\t\ttry:\n\t\t\tmail = extract_content(file)\n\t\texcept:\n\t\t\tprint(""Corrupted File {}"" . format(file))\n\t\t\n\t\t# Prepare data\n\t\tmail = preprocess(mail)\n\t\tword_list = [s for s in mail.split()]\n\t\tword_set = set(word_list)\n\n\t\tn = 0\n\t\tspaminess_list = []\n\t\tfor word in word_set:\n\t\t\tif word not in word_dict:\n\t\t\t\tcontinue              \t\t\t\t\t\t# Ignore new words (for now)\n\t\t\t\tspaminess = 0.6       \t\t\t\t\t\t# Or... assume it is slightly spam ( Gives better FP, but lower f-score)\n\t\t\telse:\n\t\t\t\tspaminess = word_dict[word][\'spaminess\']\n\t\t\t\tif spaminess < 0.6 and spaminess > 0.4:\n\t\t\t\t\tcontinue                                #ignore the word if spaminess is neutral\n\t\t\tspaminess_list.append(spaminess)\n\n\t\t# Adding up all the word probabilities\n\t\tfor spaminess in spaminess_list:\n\t\t\tn +=  (math.log(1-spaminess) - math.log(spaminess))\n\t\tprobability = 1 / (1 + Decimal(math.e) ** Decimal(n))\n\t\t\n\t\t# Predicting \n\t\tif probability > 0.8:\n\t\t\tprediction = \'0\'\n\t\telse:\n\t\t\tprediction = \'1\'\n\t\tpredictions.append(prediction)\n\treturn predictions\n\n\ndef main():\n\n\t# Default paths for all the inputs. Overrided if script not in the same locations as them.\n\ttrain = \'./TRAINING\'\n\ttest = \'./TESTING\'\n\tspam = \'./SPAM.label\'\n\n\t# Getting user input if defaults are not valid\n\tprint(""Please make sure the script is in the same directory as the Training and testing folders."")\n\tif not (os.path.isdir(train) and os.path.isdir(test) and os.path.exists(spam)):\n\t\tprint(""Testing and training datasets not found: "")\n\t\ttrain = input(""Enter training dataset path: "")\n\t\ttest = input(""Enter testing dataset path: "")\n\t\tspam = input(""Enter labels file path: "")\n\t\n\t# Getting training and testing files\n\ttrain_files = sorted([os.path.join(train, file) for file in os.listdir(train)])[:3000]\n\ttest_files = sorted([os.path.join(test, file) for file in os.listdir(test)])\n\tfiles = train_files + test_files\n\tprint(""Found the datasets."")\n\t\n\t# Spam labels\n\twith open(spam, \'r\') as f:\n\t\tlabels = [line.split()[0] for line in f.readlines()]\n\ttrain_labels = labels[:3000]\n\ttest_labels = labels[3000:]\n\n\t# Training our model\n\tprint(""Training the model..."")\n\tword_dict = training(train_files, train_labels)\n\t\n\t# Predicting labels for both training and testing data.\n\tprint(""Testing on both training and testing datasets..."")\n\tpredictions = predict(files, word_dict )\n\ttrain_predictions = predictions[:3000]\n\ttest_predictions = predictions[3000:]\n\n\t# Get respective scores\n\ttest_scores = get_scores(test_labels, test_predictions)\n\ttrain_scores = get_scores(train_labels, train_predictions)\n\tcombined_scores = get_scores(labels, predictions)\n\n\t# Output results onto the console\n\tprint(""\\nTraining Scores:"")\n\tfor key, value in sorted(train_scores.items()):\n\t\tprint(""{:15} : {:.5}"" .format(key, float(value)))\n\tprint(""\\nTesting Scores: "")\n\tfor key, value in sorted(test_scores.items()):\n\t\tprint(""{:15} : {:.5}"" .format(key, float(value)))\n\tprint(""\\nCombined Scores: "")\n\tfor key, value in sorted(combined_scores.items()):\n\t\tprint(""{:15} : {:.5}"" .format(key, float(value)))\n\n\t# Creating a results file. Pandas object is used to help format our output.\n\tmails = {}\n\tmails[\'files\'] = [os.path.split(file)[1] for file in files]\n\tmails[\'labels\'] = labels\n\tmails[\'predictions\'] = predictions\n\tdf = pd.DataFrame(mails)\n\tdf[\'result\'] = np.where(df[\'predictions\'] == df[\'labels\'], ""CORRECT"", ""WRONG"")\n\tdf.set_index(\'files\', inplace=True)\n\twith open(\'NBresults.txt\', \'w\') as f:\n\t\tf.write(df.to_string())\n\tprint(""Results file created: {}"" . format(os.path.abspath(\'NBresults.txt\')))\n\nmain()\n'"
Spam_Filter/SVM.py,1,"b'"""""" Spam filter using linear SVM and bag of words """"""\n\nfrom sklearn import feature_extraction, svm\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support\nimport pandas as pd\nimport numpy as np\nimport os, sys\nimport email, re, cgi\nfrom tqdm import tqdm\n\ndef extract_content(filename):\n\t\'\'\' Extract the subject and payload from the .eml file.\'\'\'\n\twith open(filename, \'rb\') as fp:\n\t\tmsg = email.message_from_bytes(fp.read())\n\tsub = msg.get(\'subject\')\n\t#If it is a multipart message, get_payload returns a list of parts.\n\tif msg.is_multipart():\n\t\tpayload = msg.get_payload()[0]\t\n\t\tpayload = payload.as_bytes() #We will consider the body as bytes so it is easier to decode into a unicode string.\n\telse:\n\t\tpayload =  msg.get_payload()\n\treturn ""{}\\n{}"" . format(sub, payload)\n\ndef get_text_from_email(mail):\n\t"""""" Removes html tags and punctuations.""""""\n\ttag_re = re.compile(r\'(<!--.*?-->|<[^>]*>)\')\n\n\t# Remove well-formed tags, fixing mistakes by legitimate users\n\tmail = tag_re.sub(\'\', mail)\n\n\t# Clean up anything else by escaping\n\tmail = cgi.escape(mail)\n\t\n\treturn mail.lower()\n\ndef get_scores(expected, predicted):\n\t"""""" Compares predicted and expected values and returns various metrics.""""""\n\tscores = {}\n\t# _ implies we do not care about that metric.\n\t_, scores[\'False Positives\'], scores[\'False Negatives\'], _= confusion_matrix(expected, predicted).ravel()\n\tscores[\'Precision\'], scores[\'Recall\'], scores[\'F_score\'], _= precision_recall_fscore_support(expected, predicted, average=\'macro\')\n\treturn scores\n\ndef main():\n\n\t# Default paths for all the inputs. Overrided if script not in the same locations as them.\n\ttrain = \'./TRAINING\'\n\ttest = \'./TESTING\'\n\tspam = \'./SPAM.label\'\n\n\t# Getting user input if defaults are not valid\n\tprint(""Please make sure the script is in the same directory as the Training and testing folders."")\n\tif not (os.path.isdir(train) and os.path.isdir(test) and os.path.exists(spam)):\n\t\tprint(""Testing and training datasets not found: "")\n\t\ttrain = input(""Enter training dataset path: "")\n\t\ttest = input(""Enter testing dataset path: "")\n\t\tspam = input(""Enter labels file path: "")\n\t\n\t# Getting training and testing files\n\tfiles = sorted([os.path.join(train, file) for file in os.listdir(train)])[:3000]\n\ttest_files = sorted([os.path.join(test, file) for file in os.listdir(test)])\n\tfiles.extend(test_files)\n\tprint(""Found the datasets."")\n\t\n\t# Spam labels\n\twith open(spam, \'r\') as f:\n\t\tlabels = [line.split()[0] for line in f.readlines()]\n\t\n\t# Extracting text from email\n\tbodies = []\n\tvectors = []\n\twords = {}\n\tfor file in tqdm(files):\n\t\tbodies.append(get_text_from_email(extract_content(file)))\n\t\t\n\t# Creating a count vector for each email.\n\t# All the stop words are removed.\n\t# max_df = 0.5 means that the word should not be present in more that 50% of the emails\n\t# min_df = 30 means word should occur atleast 30 times in all emails combined.\n\tcv = feature_extraction.text.CountVectorizer(stop_words=\'english\', max_df=0.5, min_df=30)\n\t# As we are not providing vocabulary, we use the fit_transform function where vocab is automatically generated.\n\tvectors = cv.fit_transform(bodies).toarray()\n\n\t#Creating a dictionary of features and labels\n\tmails = {}\n\tmails[\'files\'] = [os.path.split(file)[1] for file in files]\n\tmails[\'vectors\'] =  list(vectors)\n\tmails[\'labels\'] = labels\n\n\t# Creating training and testing features and labels\n\tx_train = list(vectors)[:3000]\n\ty_train = labels[:3000]\n\tx_test = list(vectors)[3000:]\n\ty_test = labels[3000:]\n\n\t# Initializing classifier\n\tclassifier = svm.SVC(kernel=\'linear\')\n\tprint(""Training dataset..."")\n\tclassifier.fit(x_train, y_train)\n\n\t# Predicting labels of both training and testing\n\tprint(""\\nPredicting values in testing and training dataset..."")\n\tto_predict = vectors.reshape(len(vectors), -1) #Reshaping array so that it is of valid input format\n\tpredictions = classifier.predict(to_predict)\n\n\t# Adding predictins to the dictionary\n\tmails[\'predictions\'] = predictions\n\ttrain_predictions = predictions[:3000]\n\ttest_predictions = predictions[3000:]\n\n\t# Get respective scores\n\ttest_scores = get_scores(y_test, test_predictions)\n\ttrain_scores = get_scores(y_train, train_predictions)\n\tcombined_scores = get_scores(labels, predictions)\n\n\t# Output results onto the console\n\tprint(""\\nTraining Scores:"")\n\tfor key, value in sorted(train_scores.items()):\n\t\tprint(""{:15} : {:.5}"" .format(key, float(value)))\n\tprint(""\\nTesting Scores: "")\n\tfor key, value in sorted(test_scores.items()):\n\t\tprint(""{:15} : {:.5}"" .format(key, float(value)))\n\tprint(""\\nCombined Scores: "")\n\tfor key, value in sorted(combined_scores.items()):\n\t\tprint(""{:15} : {:.5}"" .format(key, float(value)))\n\n\t# Creating a results file. Pandas object is used to help format our output.\n\tdf = pd.DataFrame(mails)\n\tdf[\'result\'] = np.where(df[\'predictions\'] == df[\'labels\'], ""CORRECT"", ""WRONG"")\n\tdf.set_index(\'files\', inplace=True)\n\twith open(\'SVMresults.txt\', \'w\') as f:\n\t\tf.write(df.drop(\'vectors\', 1).to_string())\n\tprint(""Results file created: {}"" . format(os.path.abspath(\'SVMresults.txt\')))\n\t\nmain()\n\n'"
UNSW-Network_Packet_Classification/unsw.py,1,"b'import numpy as np \nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf \nimport os\n\n# Function to encode string features\ndef encode_features_and_labels(training, testing):\n\n\t# To encode string  labels into numbers\n\tle = LabelEncoder()\n\n\t# Creates new dummy columns from each unique string in a particulat feature\n\ttraining = pd.get_dummies(data=training, columns=[\'proto\', \'service\', \'state\'])\n\ttesting = pd.get_dummies(data=testing, columns=[\'proto\', \'service\', \'state\'])\n\n\t# Making sure that the training features are same as testing features.\n\t# The training dataset has more unique protocols and states, therefore number \\\n\t# of dummy columns will be different in both. We make it the same.\n\ttraincols = list(training.columns.values)\n\ttestcols = list(testing.columns.values)\n\n\t# For those in training but not in testing\n\tfor col in traincols:\n\t\t# If a column is missing in the testing dataset, we add it\n\t\tif col not in testcols:\n\t\t\ttesting[col] = 0\n\t\t\ttestcols.append(col)\n\t# For those in testing but not in training\n\tfor col in testcols:\n\t\tif col not in traincols:\n\t\t\ttraining[col] = 0\n\t\t\ttraincols.append(col)\n\n\n\t# Moving the labels and categories to the end and making sure features are in the same order\n\ttraincols.pop(traincols.index(\'attack_cat\'))\n\ttraincols.pop(traincols.index(\'label\'))\n\ttraining = training[traincols+[\'attack_cat\', \'label\']]\n\ttesting = testing[traincols+[\'attack_cat\', \'label\']]\n\n\t# Encoding the category names into numbers so that they can be one hot encoded later.\n\ttraining[\'attack_cat\'] = le.fit_transform(training[\'attack_cat\'])\n\ttesting[\'attack_cat\'] = le.fit_transform(testing[\'attack_cat\'])\n\n\t# Returning modified dataframes and the vocabulary of labels for inverse transform\n\treturn (training, testing, le)\n\n# Parameters\ntraining_epochs = 20\nbatch_size = 9\nstart_rate = 0.0002\n\n# Network Parameters\nn_hidden_1 = 100 # 1st layer number of neurons\nn_hidden_2 = 50 # 2nd layer number of neurons\nn_features = 196 # There are 194 different features for each packet.\nn_classes = 10 # There are 9 different types of malicious packets + Normal\n\n########### Defining tensorflow computational graph ###########\n\n# tf Graph input\n# Features\nX = tf.placeholder(tf.float32, [None, n_features])\n# Labels\nY = tf.placeholder(tf.int32, [None,])\n# decay step for learning rate decay\ndecay_step = tf.placeholder(tf.int32)\n\n\n# Create model\ndef deep_neural_network(x):\n\n    # Hidden fully connected layer with 100 neurons\n    layer_1 = tf.layers.dense(x, n_hidden_1, activation=tf.nn.relu)\n    # Hidden fully connected layer with 50 neurons\n    layer_2 = tf.layers.dense(layer_1, n_hidden_2, activation=tf.nn.relu)\n    # Output fully connected layer with a neuron for each class\n    out_layer = tf.layers.dense(layer_2, n_classes)\n    return out_layer\n\n# Construct model\nlogits = deep_neural_network(X)\n\n# Define loss and optimizer\n# Converting categories into one hot labels\nlabels = tf.one_hot(indices=tf.cast(Y, tf.int32), depth=n_classes)\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    \t\t\t\t\tlogits=logits, labels=labels))\nglobal_step = tf.Variable(0, trainable=False)\n\n# Using a learning rate which has polynomial decay\nstarter_learning_rate = start_rate\nend_learning_rate = 0.00005 # we will use a polynomial decay to reach learning this learning rate.29\ndecay_steps = decay_step\nlearning_rate = tf.train.polynomial_decay(starter_learning_rate, global_step,\n                                          decay_steps, end_learning_rate,\n                                          power=0.5)\n# Using adam optimizer to reduce loss\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss_op)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Model for testing\npred = tf.nn.softmax(logits)  # Apply softmax to logits\n\n# Model for prediction: Used to just return predicted values\nprediction=tf.argmax(pred,1)\n\n########## END of model ############\n\n########## Reading and processing input datasets #########\n\n# Default values. \ntrain_set = \'UNSW_NB15_training-set.csv\'\ntest_set = \'UNSW_NB15_testing-set.csv\'\n\n# Comment if you need to hardcode path\n# train_set = input(""Enter training dataset: "")\n# test_set = input(""Enter testing dataset: "")\n# if not os.path.exists(train_set) or not os.path.exists(test_set):\n# \tprint(""Files not found"")\n# \texit()\n# Read data using pandas\ntraining = pd.read_csv(train_set, index_col=\'id\')\ntesting = pd.read_csv(test_set, index_col=\'id\')\n\n# Encoding string columns\ntraining, testing, le = encode_features_and_labels(training, testing)\n\n# Normalising all numerical features:\ncols_to_normalise = list(training.columns.values)[:39]\ntraining[cols_to_normalise] = training[cols_to_normalise].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\ntesting[cols_to_normalise] = testing[cols_to_normalise].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n\n######## End of preprocessing #######\n\n######## Training and testing #########\n\ndef get_accuracy(df):\n\n\t# Calculate accuracy for label classification\n\tcategories = prediction.eval(feed_dict={X: df.iloc[:, 0:-2]}) # Getting back the predictions\n\n\t# Function to convert categories back into binary labels\n\tf = lambda x: 0 if le.inverse_transform(x) == ""Normal"" else 1\n\n\t# Prepating the necessary predictions and labels for comparision; converting categories to normal/malicious\n\tbinary_prediction = np.fromiter((f(xi) for xi in categories), categories.dtype, count=len(categories))\n\tbinary_labels = df.iloc[:, -1].values\n\t\n\t# Compating predictions and labels to calculate accuracy\n\tcorrect_labels = tf.equal(binary_prediction, binary_labels)\n\tlabel_accuracy = tf.reduce_mean(tf.cast(correct_labels, tf.float32))\n\tresult = label_accuracy.eval()\n\tprint(""Label accuracy: {:.2f}%"".format(result*100))\n\n\t# Calculate accuracy for category classification\n\tcorrect_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(labels, 1))\n\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\tresult = accuracy.eval({X: df.iloc[:, 0:-2], Y: df.iloc[:,-2]})\n\tprint(""Category accuracy: {:.2f}%"".format(result*100))\n\ndef train_and_test_model(training, testing):\n\twith tf.Session() as sess:\n\t\tsess.run(init)\n\n\t\t# Training cycle\n\t\tfor epoch in range(training_epochs):\n\t\t\t# Shuffling dataset before training\n\t\t\tdf = training.sample(frac=1)\n\t\t\tavg_cost = 0.\n\t\t\ttotal_data = df.index.shape[0] \n\t\t\tnum_batches = total_data // batch_size + 1\n\t\t\ti = 0\n\t\t\t# Loop over all batches\n\t\t\twhile i < total_data:\n\t\t\t\tbatch_x = df.iloc[i:i+batch_size, 0:-2].values\n\t\t\t\tbatch_y = df.iloc[i:i+batch_size, -2].values # Last two columns are categories and labels\n\t\t\t\ti += batch_size\n\t\t\t\t# Run optimization op and cost op (to get loss value)\n\t\t\t\t_, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,\n\t\t\t\t                                                Y: batch_y,\n\t\t\t\t                                                decay_step: num_batches * training_epochs})\n\t\t\t\t# Compute average loss\n\t\t\t\tavg_cost += c / num_batches\n\t\t\t# Display logs per epoch step\n\t\t\tprint(""Epoch: {:04} | Cost={:.9f}"".format(epoch+1, avg_cost))\n\t\t\tget_accuracy(testing)\n\t\t\tprint()\n\t\tprint(""Training complete"")\n\n\t\tprint(""Training results: "")\n\t\tget_accuracy(training)\n\t\tprint(""Testing results: "")\n\t\tget_accuracy(testing)\n\n\n# Training the model after shuffling the data.\ntrain_and_test_model(training, testing)\n\n\n\n\n'"
