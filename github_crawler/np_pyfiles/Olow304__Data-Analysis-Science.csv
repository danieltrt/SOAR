file_path,api_count,code
ARIMA Model - House Prediction/home_data_arma_model.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Apr 22 11:29:24 2017\n\n@author: Saleban\n""""""\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.tsa.arima_model import ARIMA, ARIMAResults\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport matplotlib.pylab as plt\nimport matplotlib.dates as dates\nfrom matplotlib.pylab import rcParams\n\ndataMaster = pd.read_csv(\'new_data.csv\')\ndata = dataMaster[\'United States\']\nprint(data.head(12))\n\nran = pd.date_range(\'1995-01\', \'2016-1\', freq = \'M\')\nts = pd.Series(dataMaster[\'United States\'].values, index = ran)\nprint(ts.head(12))\nprint(ts.dtypes)\n\nplt.plot(ts)\nplt.title(\'How home prices in the US has changed over time\')\n#plt.xlim([0, 255])\nplt.xlabel(\'Year\')\nplt.ylabel(\'Price $\')\nplt.show()\nsp500_TR = ts[\'1995\':\'2014\']\nprint(sp500_TR)\n\nacf = plot_acf(ts, lags = 20)\nplt.title(""ACF Plot of SP 500"")\nacf.show()\npacf = plot_pacf(ts, lags = 20)\nplt.title(""PACF Plot of SP 500"")\npacf.show()\n\n# TRANSFORMING OUR DATA TO ADJUST FOR NON-STATIONARITY\nsp500_diff = ts - ts.shift()\ndiff = sp500_diff.dropna()\nprint(diff.head(12))\nprint(diff.dtypes)\n\nplt.figure()\nplt.plot(diff)\nplt.title(\'First Difference Time Series Plot\')\nplt.show()\n\nacfDiff = plot_acf(diff, lags = 20)\nplt.title(""ACF Plot of S 500(Difference)"")\nacfDiff.savefig(""images/timeSeriesACFDiff.png"", format = \'png\')\nacfDiff.show()\n\n# edit this shit on the actual project !\npacfDiff = plot_pacf(diff, lags = 20)\nplt.title(""PACF Plot of SP 500(Difference)"")\npacfDiff.savefig(""images/pacfDiff.png"", format = \'png\')\npacfDiff.show()\n\nmod = ARIMA(sp500_TR, order = (0, 2, 1), freq = \'M\')\nresults = mod.fit()\nprint(results.summary())\n\npredVals = results.predict(139, 291, typ=\'levels\')\nprint(predVals)\n\npredVals = predVals.drop(predVals.index[0])\nprint(predVals)\n\nsp500_for = pd.concat([ts, predVals], axis = 1, keys=[\'original\', \'predicted\'])\nprint(sp500_for[\'2014\':\'2018\'])\n\nplt.figure()\nplt.plot(sp500_for)\nplt.title(""Actual Vs. Forecasted Values"")\nplt.savefig(""images/sp500_for.png"", format = \'png\')\nplt.show()\n\nplt.figure()\nplt.plot(sp500_for)\nplt.title(\'Real Vs. Predicted Values for May 2017 and beyond\')\nplt.savefig(""images/sp500_for2.png"", format = \'png\')\nplt.show()\n\n\n\n\n'"
All Python Codes/2017-23-11-so-adding-removing-split-numpy.py,28,"b'\n# coding: utf-8\n\n# ## Creating, Adding, Split and Removing arrays\n# Numpy, short for Numerical Python, is the fundamental package required for hight performance scientific computing and its best library to learn and apply on data science career.\n# ### Here are the main steps we will go through\n# * How to create array in numpy?\n# * How to add two arrays together?\n# * How to delete array in numpy?\n# * Inserts values into arr given index\n# * Split array in given index\n# \n# This is just little illustration.\n# <img src=""http://community.datacamp.com.s3.amazonaws.com/community/production/ckeditor_assets/pictures/332/content_arrays-axes.png"">\n\n# #### How to create array in numpy?\n# <b>arange([start,] stop[, step,][, dtype])</b> : \n# \n# arange() will create arrays with regularly incrementing values. Check the docstring for complete information on the various ways it can be used. A few examples will be given here:\n\n# In[8]:\n\n\nimport numpy as np\n# we can use the first argument [start]\narr = np.arange(5)\narr\n\n\n# In[17]:\n\n\n# we can pass [start] stop[step]\narr2 = np.arange(1, 10)\nprint(""array containing [start(1) - end(10)]: "",arr2)\n\n#apply step\narr3 = np.arange(1, 10, 2)\nprint(""array containing [start(1) - end(10) - step(2)]: "", arr3)\n\n\n# In[20]:\n\n\n# we can print shape of the array and as well as dtype\nshp = np.arange(1,10)\nprint(""Shape of array: "",shp.shape )\n# dtype\ndty = np.arange(1,20)\nprint(""Dtype: "", dty.shape)\n\n\n# ##### creating 2D array\n# <b>np.array</b>\n# \n# NumPy\xe2\x80\x99s array class is called ndarray. It is also known by the alias array. Note that numpy.array is not the same as the Standard Python Library class array.array, which only handles one-dimensional arrays and offers less functionality. Check the docstring for complete information on the various ways it can be used.\n\n# In[43]:\n\n\n# we can create 2-dimention array\nd_2 = np.array([[1,2,3],[4,5,6]])\nd_2\nprint(""2D shape: "", d_2.shape)\n# we can use random function\nrnd = np.random.random(9).reshape(3,3)\nrnd\nprint(""random array: "", rnd.shape)\n\n\n# #### How to add two arrays together?\n# <b>numpy.concatenate((a1, a2, ...), axis=0)</b>\n# \n# Join a sequence of arrays along an existing axis. Check the docstring for complete information on the various ways it can be used.\n\n# In[57]:\n\n\narray_1 = np.array([[1, 2], [3, 4]])\narray_2 = np.array([[5, 6], [7, 8]])\narray_1\n\n\n# In[58]:\n\n\narray_2 \n\n\n# In[54]:\n\n\n# we can add array_2 as rows to the end of array_1\n# axis 0 = rows\nnp.concatenate((array_1, array_2), axis=0)\n\n\n# In[55]:\n\n\n# we can add array_2 as columns to end of array_1\n# axis 1 = columns\nnp.concatenate((array_1, array_2), axis=1)\n\n\n# #### How to delete array in numpy?\n# <b>numpy.delete(arr, obj, axis=None)</b>\n# \n# Return a new array with sub-arrays along an axis deleted. Check the docstring for complete information on the various ways it can be used.\n\n# In[78]:\n\n\ndel_arry = np.array([[1,2,3],[4,5,6]])\ndel_arry\n\n\n# In[77]:\n\n\n# column 2: [3 and 6]\n# we can delete columm on index 2 of array\ndel_arry = np.delete(del_arry, 2, axis=1)\ndel_arry\n\n\n# In[79]:\n\n\n# row 1: [4, 5, 6]\n# we can delete row on index 1 of the array\ndel_arry = np.delete(del_arry, 1, axis=0)\ndel_arry\n\n\n# #### Inserts values into arr given index?\n# <b>numpy.insert(arr, obj, values, axis=None)</b>\n# \n# Insert values along the given axis before the given indices. Check the docstring for complete information on the various ways it can be used.\n\n# In[94]:\n\n\ninsert_array = np.array([[1,2,3],[4,5,6]])\n# we can insert values into array index 6 - at the end\ninsert_array = np.insert(insert_array, 6, 10)\n# we can also insert at the begining \ninsert_array = np.insert(insert_array, 0, 100)\ninsert_array\n\n\n# In[106]:\n\n\n# we can fill up the whole column given value\ninsert_2 = np.arange(0,9).reshape(3,3)\nprint(""original array:"")\nprint(insert_2)\n\n# we can insert 0s in second column\ninsert_2 = np.insert(insert_2, 1, 0, axis=1)\nprint(""\\nafter inserting 0\'s on the first column:"")\nprint(insert_2)\n\n\n# In[119]:\n\n\n# we can also insert list as well\nlist_array = np.arange(0,9).reshape(3,3)\nlist_array = np.insert(list_array, [1], [[10],[10],[10]], axis=1)\nlist_array = np.insert(list_array, [1], 10, axis=0)\nlist_array\n\n\n# #### Split array in given index?\n# <b>numpy.split(ary, indices_or_sections, axis=0)</b>\n# \n# Split an array into multiple sub-arrays. Check the docstring for complete information on the various ways it can be used.\n\n# In[142]:\n\n\nor_array = np.array([[1,2,3],[4,5,6]])\nprint(""Orignal array:\\n "",or_array)\n#splits arr into 3 sub-arrays \nsplit_array = np.split(or_array, 2)\nprint(""\\nwe have our array splitted into two arrays"")\nsplit_array\n\n\n# In[149]:\n\n\ncopy_array = np.arange(16.0).reshape(4, 4)\n#splits arr horizontally on the 5th index\nprint(""copy array:\\n"",copy_array)\n\n# we splits our array into horizontal on the given index\nh_split = np.hsplit(copy_array, 2)\nh_split\n\n\n# In[154]:\n\n\n# we can also split array into vertical on the given index\nh_split = np.vsplit(copy_array, 2)\nh_split\n\n'"
All Python Codes/2017-23-11-so-create-combine-pandas.py,4,"b'\n# coding: utf-8\n\n# ## Creating and Combining DataFrame\n# <b>class pandas.DataFrame(data=None, index=None, columns=None, dtype=None, copy=False)</b>\n# \n# Two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects. \n# \n# <b>class pandas.Series(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False)</b>\n# One-dimensional ndarray with axis labels (including time series).\n# \n# Labels need not be unique but must be a hashable type. The object supports both integer- and label-based indexing and provides a host of methods for performing operations involving the index. Statistical methods from ndarray have been overridden to automatically exclude missing data (currently represented as NaN).\n# \n# ### Here are the main steps we will go through\n# * How to create dataframe using pandas?\n# * How to combine two data set using pandas?\n# \n# This is Just a little illustration.\n# \n# <img style=""float: left;"" src=""https://www.tutorialspoint.com/python_pandas/images/structure_table.jpg""></img>\n\n# In[5]:\n\n\nimport pandas as pd\nimport numpy as np\n\n\n# #### How to create dataframe using pandas?\n\n# In[16]:\n\n\n# working with series\n#create a series\ns = pd.Series(np.random.randn(5))\n#create a dataframe column\ndf = pd.DataFrame(s, columns=[\'Column_1\'])\ndf \n\n\n# In[8]:\n\n\n#sorting \ndf.sort_values(by=\'Column_1\')\n\n\n# In[10]:\n\n\n#boolean indexing\n#It returns all rows in column_name,\n#that are less than 10\ndf[df[\'Column_1\'] <= 1]\n\n\n# In[230]:\n\n\n# creating simple series\nobj2 = pd.Series(np.random.randn(5), index=[\'d\', \'b\', \'a\', \'c\', \'e\'])\nobj2\n\n\n# In[20]:\n\n\nobj2.index\n\n\n# In[229]:\n\n\n# returns the value in e\nobj2[\'e\']\n\n\n# In[26]:\n\n\n# returns all values that are greater than -2\nobj2[obj2 > -2]\n\n\n# In[27]:\n\n\n# we can do multiplication on dataframe\nobj2 * 2\n\n\n# In[28]:\n\n\n# we can do boolean expression\n\'b\' in obj2\n\n\n# In[228]:\n\n\n# returns false, because \'g\' is not defined in our data\n\'g\' in obj2\n\n\n# In[39]:\n\n\n#Let\'s see we have this data\nsdata = {\'Cat\': 24, \'Dog\': 11, \'Fox\': 18, \'Horse\': 1000}\nobj3 = pd.Series(sdata)\nobj3\n\n\n# In[227]:\n\n\n# defined list, and assign series to it\nsindex = [\'Lion\', \'Dog\', \'Cat\', \'Horse\']\nobj4 = pd.Series(sdata, index=sindex)\nobj4\n\n\n# In[226]:\n\n\n# checking if our data contains null\nobj4.isnull()\n\n\n# In[44]:\n\n\n#we can add two dataframe together\nobj3 + obj4\n\n\n# In[224]:\n\n\n# we can create series calling Series function on pandas\nprogramming = pd.Series([89,78,90,100,98])\nprogramming\n\n\n# In[223]:\n\n\n# assign index to names\nprogramming.index = [\'C++\', \'C\', \'R\', \'Python\', \'Java\']\nprogramming\n\n\n# In[102]:\n\n\n# let\'s create simple data\ndata = {\'Programming\': [\'C++\', \'C\', \'R\', \'Python\', \'Java\'],\n        \'Year\': [1998, 1972, 1993, 1980, 1991],\n        \'Popular\': [90, 79, 75, 99, 97]}\nframe = pd.DataFrame(data)\nframe\n\n\n# In[103]:\n\n\n# set our index \npd.DataFrame(data, columns=[\'Popular\', \'Programming\', \'Year\'])\n\n\n# In[133]:\n\n\ndata2 = pd.DataFrame(data, columns=[\'Year\', \'Programming\', \'Popular\', \'Users\'],\n                    index=[1,2,3,4,5])\ndata2\n\n\n# In[134]:\n\n\ndata2[\'Programming\']\n\n\n# In[135]:\n\n\ndata2.Popular\n\n\n# In[137]:\n\n\ndata2.Users = np.random.random(5)*104\ndata2 = np.round(data2)\ndata2\n\n\n# #### How to combine two data set using pandas?\n\n# In[169]:\n\n\n# we will do merging two dataset together \nmerg1 = {\'Edit\': 24, \'View\': 11, \'Inser\': 18, \'Cell\': 40}\nmerg1 = pd.Series(merg1)\nmerg1 = pd.DataFrame(merg1, columns=[\'Merge1\'])\n\nmerg2 = {\'Kernel\':50, \'Navigate\':27, \'Widgets\':29,\'Help\':43}\nmerg2 = pd.Series(merg2)\nmerg2 = pd.DataFrame(merg2, columns=[\'Merge2\'])\n\n\n# In[170]:\n\n\nmerg1\n\n\n# In[171]:\n\n\nmerg2\n\n\n# In[195]:\n\n\n#join matching rows from bdf to adf\n#pd.merge(merg1, merg2, left_index=True, right_index=True)\njoin = merg1.join(merg2)\njoin\n\n\n# In[199]:\n\n\n#replace all NA/null data with value\njoin = join.fillna(12)\njoin\n\n\n# In[201]:\n\n\n#compute and append one or more new columns\njoin = join.assign(Area=lambda df: join.Merge1*join.Merge2)\njoin\n\n\n# In[205]:\n\n\n#add single column\njoin[\'Volume\'] = join.Merge1*join.Merge2*join.Area\njoin\n\n\n# In[209]:\n\n\njoin.head(2)\n\n\n# In[208]:\n\n\njoin.tail(2)\n\n\n# In[210]:\n\n\n#randomly select fraction of rows\njoin.sample(frac=0.5)\n\n\n# In[211]:\n\n\n#order rows by values of a column (low to high)\njoin.sort_values(\'Volume\')\n\n\n# In[213]:\n\n\n#order row by values of a column (high to low)\njoin.sort_values(\'Volume\', ascending=False)\n\n\n# In[217]:\n\n\n#return the columns of a dataframe - by renaming\njoin = join.rename(columns={\'Merge1\':\'X\',\'Merge2\':\'Y\'})\n\n\n# In[218]:\n\n\njoin\n\n\n# In[220]:\n\n\n#count number of rows with each unique value of variable\njoin[\'Y\'].value_counts()\n\n\n# In[221]:\n\n\n#number of rows in dataframe\nlen(join)\n\n\n# In[222]:\n\n\n#descriptive statistics\njoin.describe()\n\n\n# ### Thank you, more to come soon!\n'"
All Python Codes/2017-23-11-so-groupby-pandas.py,2,"b'\n# coding: utf-8\n\n# ## Groupby\n# <b>DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False, **kwargs)</b>\n# \n# Group series using mapper (dict or key function, apply given function to group, return result as series) or by a series of columns.\n# \n# ### Any groupby operation involves one of the following operations on the original object. They are \xe2\x88\x92\n# \n# * Splitting the Object\n# \n# * Applying a function\n# \n# * Combining the results\n# \n# <img style=""float: left;"" src=""https://i.stack.imgur.com/sgCn1.jpg""></img>\n\n# In[1]:\n\n\n# import library\nimport pandas as pd\n\n\n# In[4]:\n\n\ndata = {\'Students\': [\'S1\', \'S2\', \'S3\', \'S3\', \'S1\',\n         \'S4\', \'S4\', \'S3\', \'S2\', \'S2\', \'S4\', \'S3\'],\n         \'Rank\': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2],\n         \'Year\': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017],\n         \'Grade\':[87,79,83,73,74,81,56,78,94,70,80,69]}\ndf = pd.DataFrame(data)\ndf\n\n\n# ### Split Data into Groups\n# Pandas object can be split into any of their objects. There are multiple ways to split an object like \xe2\x88\x92\n# \n# * obj.groupby(\'key\')\n# * obj.groupby([\'key1\',\'key2\'])\n# * obj.groupby(key,axis=1)\n# \n# Let us now see how the grouping objects can be applied to the DataFrame object\n\n# In[7]:\n\n\n# let\'s groupby students\ndf.groupby(\'Students\')\n\n\n# In[8]:\n\n\n# to view groups \ndf.groupby(\'Students\').groups\n\n\n# In[9]:\n\n\n# you can group by with multiple columns \ndf.groupby([\'Students\',\'Year\']).groups\n\n\n# In[11]:\n\n\n# iterating through groups\ngrouped = df.groupby(\'Students\')\nfor student, group_name in grouped:\n    print(student)\n    print(group_name)\n\n\n# In[13]:\n\n\n# select group by value\ngrouped = df.groupby(\'Year\')\nprint(grouped.get_group(2014))\n\n\n# In[16]:\n\n\n# find the mean of grouped by data\nimport numpy as np\ngrouped = df.groupby(\'Year\')\nprint(grouped[\'Grade\'].agg(np.mean))\n\n\n# In[18]:\n\n\n# find the average for all students\ngrouped = df.groupby(\'Students\')\nprint(grouped[\'Grade\'].agg(np.mean).round())\n\n\n# In[19]:\n\n\n# count \ngrouped = df.groupby(\'Year\')\nprint(grouped[\'Grade\'].value_counts())\n\n\n# In[21]:\n\n\n#Filtration filters the data on a defined criteria and returns the subset of data. \n#The filter() function is used to filter the data.\n# we are going to find the top 3 students\ndf.groupby(\'Students\').filter(lambda x: len(x) >= 3)\n\n\n# ### I\'ll be updating this notebook soon!\n# using real dataset!!\n'"
All Python Codes/2017-23-11-so-import-export-slicing-numpy.py,7,"b'\n# coding: utf-8\n\n# ## Importing, Exporting, Basic Slicing and Indexing.\n# In terms of the importing and exporting files I would not go depth on it. You can refer the docstring for complete information on the various ways it can be used. A few examples will be given here in regard to this. I would spent sometime on the slicing and indexing arrays.\n# ### Here are the main steps we will go through\n# * How to use loadtxt, genfromtxt, and savetxt?\n# * How to slice and index array?\n# \n# This is just little illustration.\n# <img src=""http://www.bogotobogo.com/python/images/python_strings/string_diagram.png"">\n\n# #### How to use loadtxt, genfromtxt, and savetxt??\n# * <b>numpy.loadtxt()</b> : Load data from a text file. This function aims to be a fast reader for simply formatted files. \n# * <b>numpy.genfromtxt()</b>: Load data from a text file, with missing values handled as specified. When spaces are used as delimiters, or when no delimiter has been given as input, there should not be any missing data between two fields. When the variables are named (either by a flexible dtype or with names, there must not be any header in the file (else a ValueError exception is raised). Individual values are not stripped of spaces by default. When using a custom converter, make sure the function does remove spaces.\n# * <b>numpy.savetxt()</b>: Save an array to a text file. Further explanation of the fmt parameter (%[flag]width[.precision]specifier):\n# \n# ##### Example \n# Here is the general idea, I\'ll come back to it.\n\n# In[2]:\n\n\nimport numpy as np \n# using numpy you can load text file\nnp.loadtxt(\'file_name.txt\')\n# load csv file\nnp.genfromtxt(\'file_name.csv\', delimiter=\',\')\n# you can write to a text file and save it\nnp.savetxt(\'file_name.txt\', arr, delimiter=\' \')\n# you can write to a csv file and save it\nnp.savetxt(\'file_name.csv\', arr, delimiter=\',\')\n\n\n# #### How to slice and index array?\n# <b>ndarrays</b> can be indexed using the standard Python x[obj] syntax, where x is the array and obj the selection. There are three kinds of indexing available: field access, basic slicing, advanced indexing. Which one occurs depends on obj.\n# \n# The basic slice syntax is i:j:k where i is the starting index, j is the stopping index, and k is the step (k\\neq0). This selects the m elements (in the corresponding dimension) with index values i, i + k, ..., i + (m - 1) k where m = q + (r\\neq0) and q and r are the quotient and remainder obtained by dividing j - i by k: j - i = q k + r, so that i + (m - 1) k < j.\n# \n# Check the docstring for complete information on the various ways it can be used. A few examples will be given here:\n\n# In[9]:\n\n\n# slicing 1 to 7 gives us: [1 through 6]\nslice_array = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nslice_array[1:7]\n\n\n# In[12]:\n\n\n# if we do this, we are giving k, which is the step function. in this case step by 2\nslice_array[1:7:2]\n\n\n# In[21]:\n\n\nslice_arrays = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])+1\n#return the element at index 5\nslice_arrays[5]\n\n\n# In[25]:\n\n\n#returns the 2D array element on index value of 2 and 5\nslice_arrays[[2,5]]\n\n\n# In[30]:\n\n\n#assign array element on index 1 the value 4\nslice_arrays[1] = 100\n#assign array element on index [1][3] the value 10\nslice_arrays[[1,3]] = 100\nslice_arrays\n\n\n# In[64]:\n\n\n#return the elements at indices 0,1,2 on a 2D array:\nslice_arrays[0:3]\n\n\n# In[65]:\n\n\n#returns the elements at indices 1,100\nslice_arrays[:2]\n\n\n# In[66]:\n\n\nslice_2d = np.arange(16).reshape(4,4)\nslice_2d\n\n\n# In[67]:\n\n\n#returns the elements on rows 0,1,2, at column 4\nslice_2d[0:3, :4]\n\n\n# In[72]:\n\n\n#returns the elements at index 1 on all columns\nslice_2d[:, 1]\n\n\n# In[84]:\n\n\n# return the last two rows\nslice_2d[-2:10]\n# returns the last three rows\nslice_2d[1:]\n\n\n# In[85]:\n\n\n# reverse all the array backword\nslice_2d[::-1]\n\n\n# In[86]:\n\n\n#returns an array with boolean values\nslice_2d < 5\n\n\n# In[87]:\n\n\n#inverts a boolearn array, if its positive arr - convert to negative, vice versa\n~slice_2d\n\n\n# In[91]:\n\n\n#returns array elements smaller than 5\nslice_2d[slice_2d < 5]\n\n'"
All Python Codes/2017-23-11-so-stats-vector-math-numpy (1).py,34,"b'\n# coding: utf-8\n\n# ## Elementwise Operations and Statistics.\n# In this section I will cover on Elementwise operations, Basic reductions, Broadcasting, and Sorting data. Arrays are important because they enable you to express batch operations on data without writing any for loops. This is usually called vectorization. Any arithmetic operations between equal-size arrays applies the operation elementwise:\n# ### Here are the main steps we will go through\n# * Element-wise operations\n#     * Elementwise operations\n#     * Basic reductions\n#     * Broadcasting\n#     * Sorting data\n# * Do some statistics on numpy\n# \n# This is just little illustration.\n# <img src=""https://s3.amazonaws.com/dq-content/6/numpy_ndimensional.svg"">\n\n# #### Basic operations\n\n# In[1]:\n\n\nimport numpy as np\narr1 = np.array([[1,2,3],[4,5,6]])\narr2 = np.array([[7,8,9]])\n\n\n# In[2]:\n\n\n#elementwise add arr2 to arr1\nnp.add(arr1, arr2)\n\n\n# In[3]:\n\n\n#elementwise subtract arr2 from arr1\nnp.subtract(arr1, arr2)\n\n\n# In[4]:\n\n\n#elementwise multiply arr1 by arr2\nnp.multiply(arr1,arr2)\n\n\n# In[5]:\n\n\n#elementwise divide arr1 by arr2\nnp.divide(arr1, arr2)\n\n\n# In[6]:\n\n\n#elementwise raise arr1 raised to the power of arr2\nnp.power(arr1,arr2)\n\n\n# In[7]:\n\n\n#returns True if the arrays have the same elements and shape\nnp.array_equal(arr1,arr2)\n\n\n# In[9]:\n\n\n#square root of each element in the array\nnp.sqrt(arr1)\n\n\n# In[26]:\n\n\n#Transcendental functions:\n#sine of each element in the array\nnp.sin(arr1)\n\n\n# In[11]:\n\n\n#natural log of each element in the array\nnp.log(arr1)\n\n\n# In[12]:\n\n\n#absolute value of each element in the array\nnp.abs(arr1)\n\n\n# In[24]:\n\n\narr = np.random.random(9)\nprint(""original array:\\n "", arr)\n#rounds up to the nearest int\nnp.ceil(arr)\n\n\n# In[22]:\n\n\n#rounds down to the nearest int\nnp.floor(arr)\n\n\n# In[23]:\n\n\n#rounds to the nearest int\nnp.round(arr)\n\n\n# In[25]:\n\n\n#Logical operations:\na = np.array([1, 1, 0, 0], dtype=bool)\nb = np.array([1, 0, 1, 0], dtype=bool)\nnp.logical_or(a, b)\n\n\n# #### Basic reductions\n\n# In[27]:\n\n\nx = np.array([1, 2, 3, 4])\nnp.sum(x)\n\n\n# In[28]:\n\n\n#Sum by rows and by columns:\nx = np.array([[1, 1], [2, 2]])\nx\n\n\n# In[29]:\n\n\nx.sum(axis=0)   # columns (first dimension)\n\n\n# In[30]:\n\n\nx[:, 0].sum(), x[:, 1].sum()\n\n\n# In[31]:\n\n\nx.sum(axis=1)   # rows (second dimension)\n\n\n# #### Basic reductions\n#  * Basic operations on numpy arrays (addition, etc.) are elementwise\n# \n#  * This works on arrays of the same size.\n#   Nevertheless, It\xe2\x80\x99s also possible to do operations on arrays of different\n#     sizes if NumPy can transform these arrays so that they all have\n#     the same size: this conversion is called broadcasting.\n#     The image below gives an example of broadcasting:\n\n# In[32]:\n\n\na = np.tile(np.arange(0, 40, 10), (3, 1)).T\na\n\n\n# In[33]:\n\n\nb = np.array([0, 1, 2])\nb\n\n\n# In[34]:\n\n\na + b\n\n\n# In[36]:\n\n\n#We have already used broadcasting without knowing it!:\na = np.ones((4, 5))\na[0] = 2 \na\n\n\n# #### Sorting data\n\n# In[37]:\n\n\na = np.array([[4, 3, 5], [1, 2, 1]])\nb = np.sort(a, axis=1)\nb\n\n\n# In[38]:\n\n\n#Sorts each row separately!\na.sort(axis=1)\na\n\n\n# In[45]:\n\n\ns = np.array([[2,4,5,8,2,0,3,7]])\nt = np.sort(s, axis=1)\nt\n\n\n# #### Do some statistics on numpy\n\n# In[49]:\n\n\nstat = np.arange(25).reshape(5,5)\nstat\n\n\n# In[50]:\n\n\n#returns mean along specific axis\nnp.mean(stat, axis=0)\n\n\n# In[51]:\n\n\n#returns sum of arr\nstat.sum()\n\n\n# In[52]:\n\n\n#returns minimum value of arr\nstat.min()\n\n\n# In[53]:\n\n\n#returns maximum value of specific axis\nstat.max(axis=0)\n\n\n# In[54]:\n\n\n#returns the variance of array\nnp.var(stat)\n\n\n# In[55]:\n\n\n#returns the standard deviation of specific axis\nnp.std(stat, axis=1)\n\n\n# In[57]:\n\n\n#returns correlation coefficient of array\nnp.corrcoef(stat)\n\n'"
All Python Codes/2017-24-11-so-barplots-matplotlib.py,1,"b""\n# coding: utf-8\n\n# # Bar plots in Matplotlib\n\n# In[5]:\n\n\nget_ipython().magic('pylab')\nget_ipython().magic('matplotlib inline')\n\n\n# In[17]:\n\n\n# input data\nmean_values = [3.3, 1, 2, 3]\nvariance = [0.3,0.2, 0.4, 0.5]\nbar_labels = ['bar 1', 'bar 2', 'bar 3','bar 4']\n\n# plot bars\nx_pos = list(range(len(bar_labels)))\nbar(x_pos, mean_values)\n\n# set height of the y-axis\nmax_y = max(zip(mean_values, variance)) # returns a tuple, here: (3, 5)\nylim([0, (max_y[0] + max_y[1]) * 1.1])\n\n# set axes labels and title\nylabel('Y lable')\nxticks(x_pos, bar_labels)\ntitle('Bar plot')\n\n\n# In[90]:\n\n\n# input data\nfigure(figsize=(10,5))\nmean_values = [3.3, 1, 2, 3]\nvariance = [0.3,0.2, 0.4, 0.5]\nbar_labels = ['bar 1', 'bar 2', 'bar 3','bar 4']\n\n# plot bars\nx_pos = list(range(len(bar_labels)))\nbarh(x_pos, mean_values,align='center', alpha=0.4, color='g')\n\n# set height of the y-axis\nmax_y = max(zip(mean_values, variance)) \nylim([len(max_y)+0.5])\n# set axes labels and title\nylabel('Y lable')\nxticks(x_pos, bar_labels)\ntitle('Bar plot')\n\n\n# In[39]:\n\n\n# Input data\ngreen_data = [1, 3, 5]\nblue_data = [4, 2, 3.4]\nred_data = [3, 4, 6]\nlabels = ['Green', 'Blue', 'Red']\n\n# Setting the positions and width for the bars\npos = list(range(len(green_data))) \nwidth = 0.2 \n    \n# Plotting the bars\nfig, ax = subplots(figsize=(8,6))\n\nbar(pos, green_data, width, alpha=0.5, color='g', label=labels[0])\nbar([p + width for p in pos], blue_data, width, alpha=0.5, color='b', label=labels[1])\nbar([p + width*2 for p in pos], red_data, width, alpha=0.5, color='r', label=labels[2])\n\n# Setting axis labels and ticks\nax.set_ylabel('y-value')\nax.set_title('Grouped bar plot')\nax.set_xticks([p + 1.5 * width for p in pos])\nax.set_xticklabels(labels)\n\n# Setting the x-axis and y-axis limits\nxlim(min(pos)-width, max(pos)+width*4)\nylim([0, max(green_data + blue_data + red_data) * 1.5])\n\n# Adding the legend and showing the plot\nlegend(['Green', 'Blue', 'Red'], loc='upper left')\n\n\n# In[86]:\n\n\ndata = range(200, 225, 5)\n\nbar_labels = ['a', 'b', 'c', 'd', 'e']\n\nfig = figure(figsize=(10,8))\n\n# plot bars\ny_pos = np.arange(len(data))\nyticks(y_pos, bar_labels, fontsize=12)\nbars = barh(y_pos, data, align='center', alpha=0.5, color='g')\n\n# annotation and labels\nfor b,d in zip(bars, data):\n    text(b.get_width() + b.get_width()*0.05, b.get_y() + b.get_height()/2,\n        '{0:.0%}'.format(d/min(data)), \n        ha='center', va='bottom', fontsize=12, color='k')\n\nxlabel('X axis label', fontsize=12)\nylabel('Y axis label', fontsize=12)\nt = title('Bar plot with plot labels/text', fontsize=14)\nylim([-1,len(data)+0.5])\nvlines(min(data), -1, len(data)+0.5, linestyles='--', color='r')\n\n"""
All Python Codes/2017-24-11-so-customization-matplotlib.py,5,"b'\n# coding: utf-8\n\n# ## Working with Matplotlib\n# Introduction:\n# \n# Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython shell, the jupyter notebook, web application servers, and four graphical user interface toolkits.\n# \n# ### Here are the main steps we will go through\n# * How to save your plot using matplotlib?\n# * How to create plot using matplotlib?\n# * How to make your graph look pretty?\n# \n# This is Just a little illustration.\n# \n# <img style=""float:left;"" src=""https://static.lwn.net/images/2015/02-matplotlib-3d.png""></img>\n\n# #### How to save your plot using matplotlib?\n# I am gonna show you how to save your graph, you can refer the docstring for complete information on the various ways it can be used.\n\n# In[1]:\n\n\n# import matplotlib, numpy\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# In[2]:\n\n\n#saves plot/figure to image\nplt.savefig(\'name_your_graph.png\')\n\n\n# In[11]:\n\n\n# we can create simple plot as follows, linear graph\n# Prepare the data\nlinear = np.linspace(0, 10, 100)\n\n# Plot the data, set label and color, b=blue, g=green\nplt.plot(linear, linear, label=\'linear\', color=\'b\')\n\n# Add a legend\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n# In[23]:\n\n\n#by calling \nplt.style.available\n\n\n# In[26]:\n\n\nx = np.linspace(0, 4 * np.pi)\ny = np.sin(x)\n\nplt.plot(x, y)\n\nplt.show()\n\n\n# In[27]:\n\n\nplt.style.use(\'ggplot\')\n\nplt.plot(x, y)\n\nplt.show()\n\n\n# In[32]:\n\n\nwith plt.style.context((\'seaborn-darkgrid\')):\n    plt.plot(x, y, \'r-o\')\n    plt.show()\n\n\n# #### Working with text inside your graph\n\n# However, you can use subplots to set up and place your Axes on a regular grid. So that means that in most cases, Axes and subplot are synonymous, they will designate the same thing. When you do call subplot to add Axes to your figure, do so with the add_subplots() function. \n\n# In[14]:\n\n\n# we can create scatter plot for this \nfig = plt.figure()\n\n# Set up Axes\nax = fig.add_subplot(111)\n\n# Scatter the data\nax.scatter(np.linspace(0, 1, 5), np.linspace(0, 5, 5), label=\'scatter\')\n\n# Add a legend\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n# #### How To Change The Size of Figures\n# \n# Now that you have seen how to initialize a Figure and Axes from scratch, you will also want to know how you can change certain small details that the package sets up for you, such as the figure size.\n# Let\xe2\x80\x99s say you don\xe2\x80\x99t to have your figure size to be default size and you want to change this. How do you set the size of your figures manually?\n\n# In[20]:\n\n\n# easy! you can just call \nfig = plt.figure(figsize=(10,8))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\n\n# In[21]:\n\n\n# let\'s plot simple bar graph\n# Plot the data\nax1.bar([1,2,3],[3,4,5])\nax2.barh([0.5,1,2.5],[0,1,2])\n\n# Show the plot\nplt.show()\n\n\n# In[3]:\n\n\nx = np.linspace(0, 2, 100)\n\nplt.plot(x, x, label=\'linear\')\nplt.plot(x, x**2, label=\'quadratic\')\nplt.plot(x, x**3, label=\'cubic\')\n\nplt.xlabel(\'x label\')\nplt.ylabel(\'y label\')\n\nplt.title(""Simple Plot"")\n\nplt.legend()\n\nplt.show()\n\n\n# In[4]:\n\n\n#plot data connected by lines\nlines = plt.plot(x,x)\nplt.show()\n\n\n# ## More to come soon!\n'"
All Python Codes/2017-24-11-so-data-to-from-string-numpy.py,3,"b'\n# coding: utf-8\n\n# ## Data to from String in Numpy\n# Numpy, short for Numerical Python, is the fundamental package required for hight performance scientific computing and its best library to learn and apply on data science career.\n# \n# This is just little illustration.\n# \n# <img style=""float: left;"" src=""http://community.datacamp.com.s3.amazonaws.com/community/production/ckeditor_assets/pictures/332/content_arrays-axes.png"" width=600 height=400>\n\n# In[1]:\n\n\nimport numpy as np\n\n\n# In[2]:\n\n\na = np.array([[1,2],\n           [3,4]], \n          dtype = np.uint8)\n\n\n# In[3]:\n\n\na.tostring()\n\n\n# In[5]:\n\n\na.tostring(order=\'F\')\n\n\n# In[8]:\n\n\ns = a.tostring()\na = np.fromstring(s, dtype=np.uint8)\na\n\n\n# In[9]:\n\n\na.shape = 2,2\na\n\n'"
All Python Codes/2017-24-11-so-matrix-object-numpy.py,7,"b'\n# coding: utf-8\n\n# ## Matrix Object in Numpy\n# Numpy, short for Numerical Python, is the fundamental package required for hight performance scientific computing and its best library to learn and apply on data science career.\n# \n# This is just little illustration.\n# \n# <img style=""float: left;"" src=""https://www.safaribooksonline.com/library/view/python-for-data/9781449323592/httpatomoreillycomsourceoreillyimages1346880.png"" width=400 height=200>\n\n# In[1]:\n\n\nimport numpy as np\n\n\n# In[2]:\n\n\na = np.array([[1,2,4],\n              [2,5,3], \n              [7,8,9]])\nA = np.mat(a)\nA\n\n\n# In[3]:\n\n\nA = np.mat(\'1,2,4;2,5,3;7,8,9\')\nA\n\n\n# In[5]:\n\n\na = np.array([[ 1, 2],\n              [ 3, 4]])\nb = np.array([[10,20], \n              [30,40]])\n\nnp.bmat(\'a,b;b,a\')\n\n\n# In[10]:\n\n\nx = np.array([[4], [2], [3]])\nx\n\n\n# In[11]:\n\n\nA * x\n\n\n# In[12]:\n\n\nprint(A * A.I)\n\n\n# In[13]:\n\n\nprint(A ** 3)\n\n'"
All Python Codes/2017-24-11-so-sorting-numpy-arrays-numpy.py,6,"b""\n# coding: utf-8\n\n# ## Sorting arrays in Numpy\n# Numpy, short for Numerical Python, is the fundamental package required for hight performance scientific computing and its best library to learn and apply on data science career.\n# \n# \n\n# In[3]:\n\n\nimport numpy as np\n\n\n# In[9]:\n\n\nnames = np.array(['F', 'C', 'A', 'G'])\nweights = np.array([20.8, 93.2, 53.4, 61.8])\n\nsort(weights)\n\n\n# In[10]:\n\n\n#argsort\nordered_indices = np.argsort(weights)\nordered_indices\n\n\n# In[11]:\n\n\nweights[ordered_indices]\n\n\n# In[13]:\n\n\nnames[ordered_indices]\n\n\n# In[15]:\n\n\ndata = np.array([20.8,  93.2,  53.4,  61.8])\ndata.argsort()\n\n\n# In[16]:\n\n\n# sort data\ndata.sort()\ndata\n\n\n# In[17]:\n\n\n# 2d array\na = np.array([\n        [.2, .1, .5], \n        [.4, .8, .3],\n        [.9, .6, .7]\n    ])\na\n\n\n# In[18]:\n\n\nsort(a)\n\n\n# In[20]:\n\n\n# sort by column\nsort(a, axis = 0)\n\n\n# In[22]:\n\n\n# search sort\nsorted_array = linspace(0,1,5)\nvalues = array([.1,.8,.3,.12,.5,.25])\n\n\n# In[24]:\n\n\nnp.searchsorted(sorted_array, values)\n\n"""
All Python Codes/2017-24-11-so-structured-arrays-numpy.py,6,"b'\n# coding: utf-8\n\n# ## Structured Arrays in Numpy\n# Numpy, short for Numerical Python, is the fundamental package required for hight performance scientific computing and its best library to learn and apply on data science career.\n# \n# This is just little illustration.\n# \n# <img style=""float: left;"" src=""http://slideplayer.com/6419067/22/images/5/NumPy+Array.jpg"" width=600 height=400>\n\n# In[1]:\n\n\nimport numpy as np\n\n\n# In[3]:\n\n\na = np.array([1.0,2.0,3.0,4.0], np.float32)\n\n\n# In[22]:\n\n\n# called the function view on our data\na.view(np.complex64)\n\n\n# In[21]:\n\n\n# assign our to data to dtype\nmy_dtype = np.dtype([(\'mass\', \'float32\'), (\'vol\', \'float32\')])\n\n\n# In[8]:\n\n\na.view(my_dtype)\n\n\n# In[9]:\n\n\nmy_data = np.array([(1,1), (1,2), (2,1), (1,3)], my_dtype)\nprint(my_data)\n\n\n# In[10]:\n\n\nmy_data[0]\n\n\n# In[11]:\n\n\nmy_data[0][\'vol\']\n\n\n# In[12]:\n\n\nmy_data[\'mass\']\n\n\n# In[13]:\n\n\nmy_data.sort(order=(\'vol\', \'mass\'))\nprint(my_data)\n\n\n# In[14]:\n\n\nperson_dtype = np.dtype([(\'name\', \'S10\'), (\'age\', \'int\'), (\'weight\', \'float\')])\n\n\n# In[15]:\n\n\nperson_dtype.itemsize\n\n\n# In[16]:\n\n\npeople = np.empty((3,4), person_dtype)\n\n\n# In[17]:\n\n\npeople[\'age\'] = [[33, 25, 47, 54],\n                 [29, 61, 32, 27],\n                 [19, 33, 18, 54]]\n\n\n# In[18]:\n\n\npeople[\'weight\'] = [[135., 105., 255., 140.],\n                    [154., 202., 137., 187.],\n                    [188., 135., 88., 145.]]\n\n\n# In[19]:\n\n\nprint(people)\n\n\n# In[20]:\n\n\npeople[-1,-1]\n\n'"
All Python Codes/2017-24-11-so-working-with-image-matplotlib.py,0,"b'\n# coding: utf-8\n\n# ## Working with image plotting\n# Introduction:\n# \n# Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython shell, the jupyter notebook, web application servers, and four graphical user interface toolkits.\n# \n# ### Here are the main steps we will go through\n# * How to add text to graph?\n# \n# This is Just a little illustration.\n# \n# <img style=""float:left;"" src=""https://i.imgur.com/bFsdlJy.png""></img>\n\n# In[23]:\n\n\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\n\n# In[24]:\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nget_ipython().magic(\'matplotlib inline\')\n\n\n# In[26]:\n\n\nimg = mpimg.imread(\'Beauty-Black-White-Wallpaper.jpg\')\n\n\n# In[27]:\n\n\nimg.shape\n\n\n# In[28]:\n\n\nimgplot = plt.imshow(img)\n\n\n# In[29]:\n\n\nlum_img = img[:,:,0]\nimgplot = plt.imshow(lum_img)\n\n\n# In[30]:\n\n\nimgplot = plt.imshow(lum_img)\nimgplot.set_cmap(\'hot\')\n\n\n# In[31]:\n\n\nimgplot = plt.imshow(lum_img)\nimgplot.set_cmap(\'spectral\')\n\n\n# In[32]:\n\n\nimgplot = plt.imshow(lum_img)\nimgplot.set_cmap(\'spectral\')\nplt.colorbar()\nplt.show()\n\n\n# In[33]:\n\n\nimgplot = plt.imshow(lum_img)\nimgplot.set_clim(0.0,0.7)\n\n'"
All Python Codes/2017-24-11-so-working-with-text-matplotlib.py,3,"b'\n# coding: utf-8\n\n# ## Working with text plotting\n# Introduction:\n# \n# Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython shell, the jupyter notebook, web application servers, and four graphical user interface toolkits.\n# \n# ### Here are the main steps we will go through\n# * How to add text to graph?\n# \n# This is Just a little illustration.\n# \n# <img style=""float:left;"" src=""https://matplotlib.org/1.3.0/_images/annotate_text_arrow.png""></img>\n\n# In[1]:\n\n\n# import matplotlib, numpy\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# In[9]:\n\n\n# set plt.figure()\nfig = plt.figure(figsize=(10, 8))\n\nax = fig.add_subplot(111)\nfig.subplots_adjust(top=0.85)\n\nax.set_title(\'axes title\')\n\nax.set_ylabel(\'Frequency\')\nax.set_xlabel(\'Data\')\n\nax.text(4, 7, \'Some text\', style=\'italic\',\n        bbox={\'facecolor\':\'red\', \'alpha\':0.5, \'pad\':10})\n\nax.plot(np.linspace(0, 1, 5), np.linspace(0, 5, 5))\nax.plot([4], [1], \'s\')\nax.annotate(\'Here is the point\', xy=(4, 1), xytext=(3, 4),\n            arrowprops=dict(facecolor=\'blue\', shrink=0.04))\n\nax.axis([0, 10, 0, 10])\n\nplt.show()\n\n\n# In[3]:\n\n\n# more advance example\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nt = np.arange(0.0, 5.0, 0.01)\ns = np.cos(2*np.pi*t)\nline, = ax.plot(t, s, lw=2)\n\nax.annotate(\'local max\', xy=(2, 1), xytext=(3, 1.5),\n            arrowprops=dict(facecolor=\'green\', shrink=0.05),\n            )\n\nax.set_ylim(-2,2)\nplt.show()\n\n'"
All Python Codes/2017-25-11-so-apply-pandas.py,1,"b'\n# coding: utf-8\n\n# ## Apply function \n# Subset rows or columns of dataframe according to labels in the specified index.\n# \n# Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.\n\n# In[105]:\n\n\nimport pandas as pd\nimport numpy as np\n\n\n# In[106]:\n\n\ndata = pd.read_csv(\'data/train.csv\')\n\n\n# In[107]:\n\n\ndata.head(4)\n\n\n# In[108]:\n\n\n# let\'s use apply function to get the length of names\ndata[""Name_length""] = data.Name.apply(len)\n\n\n# In[109]:\n\n\ndata.loc[0:5, [""Name"", ""Name_length""]]\n\n\n# In[110]:\n\n\n# let\'s get the mean price on fare column\ndata[""Fare_mean""] = data.Fare.apply(np.mean)\n\n\n# In[111]:\n\n\ndata.loc[0:5, [""Fare"", ""Fare_mean""]]\n\n\n# In[112]:\n\n\ndata.Name.str.split(\'.\')[0][0].split(\',\')[1]\n\n\n# In[113]:\n\n\n# let\'s get the name perfix, like Mr. Mrs. Mss. Ms...\ndata[\'prefix\'] = data.Name.str.split(\'.\').apply(lambda x: x[0].split(\',\')[1])\n\n\n# In[114]:\n\n\ndata.loc[0:10, [\'Name\', \'prefix\']]\n\n\n# In[115]:\n\n\ndel data[\'dummy_prefix\']\n\n\n# In[117]:\n\n\ndata.tail()\n\n\n# In[116]:\n\n\n# let\'s get the unique prefix\ndata[\'prefix\'].unique()\n\n\n# In[118]:\n\n\n# let\'s use apply function to combined prefixes, \n# male = Mr Master, Don, rev, Dr, sir, col, capt, == 0\n# female = Mrs miss, Mme, Ms, Lady, Mlle, the countess,Jonkheer  == 1\n\n\n# In[119]:\n\n\ndummy_pre = data.groupby(\'prefix\')\n\n\n# In[120]:\n\n\n#list(data.groupby(\'prefix\'))\n\n\n# In[121]:\n\n\ndummy_pre.count()\n\n\n# In[122]:\n\n\nget_dummy = data.prefix\n\n\n# In[126]:\n\n\npd.get_dummies(data[\'prefix\'])\n\n\n# In[125]:\n\n\ndata.head()\n\n'"
All Python Codes/2017-25-11-so-cross-validation-sklearn.py,0,"b'\n# coding: utf-8\n\n# ## Cross Validation\n# First, we\'ll try to build a model without cross-validation apply to it. Second, we\'ll use the same model with applying cross-validation and see if the accuracy score changed or not.\n# \n# ### Main contents:\n# * Build Support-Vector-Machine without cross-validation\n# * Apply the same model with cross-validation\n# \n# Note! We\'ll cover the metrics scores later\n# \n# <img style=""float:left;"" src=""https://cdn-images-1.medium.com/max/1600/1*J2B_bcbd1-s1kpWOu_FZrg.png"" width=700 height=300>\n\n# In[1]:\n\n\n# libraries we need\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.svm import SVC\n\n\n# In[4]:\n\n\n# load iris dataset\ndata_holder = load_iris()\nprint(data_holder.data.shape)\nprint(data_holder.target.shape)\n\n\n# In[6]:\n\n\n# set our X and y to data and target values\nX , y = data_holder.data, data_holder.target\n\n\n# In[13]:\n\n\n# split our data into train and test sets\n# let\'s split into 70/30: train=70% and test=30%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= .4, random_state = 0)\n\n\n# In[22]:\n\n\nprint(""X train shape: "", X_train.shape)\nprint(""X test shape: "", X_test.shape)\nprint()\nprint(""y train shape: "", y_train.shape)\nprint(""y test shape: "", y_test.shape)\n\n\n# In[16]:\n\n\n# let\'s fit into our model, svc\n# we\'ll set it to some parameters, but we\'ll go through depth on parameter tuning later\nmodel = SVC(kernel=\'linear\', C=1)\n# fit our training data\nmodel.fit(X_train, y_train)\n# print how our model is doing\nprint(""Score: "", model.score(X_test, y_test))\n\n\n# As you can see our model scores 96% on our training data, we\'ll try to boost that accuracy score higher.\n# \n# ### Computing cross-validated \n# The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset.\n\n# In[20]:\n\n\n# call cross-validation library\nfrom sklearn.model_selection import cross_val_score\nmodel = SVC(kernel=\'linear\', C=1)\n\n# let\'s try it using cv\nscores = cross_val_score(model, X, y, cv=5)\n\n\n# #### Evaluate Model\n# Here is the output of our 5 KFold cross validation. Each value is the accuracy score of the support vector classifier when leaving out a different fold. There are three values because there are three folds. A higher accuracy score, the better.\n\n# In[19]:\n\n\nscores\n\n\n# To get an good measure of the model\'s accuracy, we calculate the mean of the three scores. This is our measure of model accuracy.\n\n# In[18]:\n\n\n# print mean score\nprint(""Accuracy using CV: "", scores.mean())\n\n\n# ## I\'ll be updating more on this section!\n'"
All Python Codes/2017-25-11-so-evaluating-classification-metrics-sklearn.py,0,"b'\n# coding: utf-8\n\n# ## Evaluating Classification Metrics\n# First, we\'ll try to build a model without cross-validation apply to it. Second, we\'ll use the same model with applying cross-validation and see if the accuracy score changed or not.\n# \n# ### Main contents:\n# ### Training and testing on the same data\n#   * Rewards overly complex models that ""overfit"" the training data and won\'t necessarily generalize\n# #### Train/test split\n#   * Split the dataset into two pieces, so that the model can be trained and tested on different data\n#   * Better estimate of out-of-sample performance, but still a ""high variance"" estimate\n# #### K-fold cross-validation\n#   * Systematically create ""K"" train/test splits and average the results together\n#   * Even better estimate of out-of-sample performance\n#   * Runs ""K"" times slower than train/test split\n#   \n# ### Model evaluation metrics\n# ##### Classification problems: Classification accuracy\n#   * There are many more metrics, here we\'ll cover on classification metrics\n# \n# <img style=""float:left;"" src=""https://image.slidesharecdn.com/finalcustlingprofiling-160226163538/95/customer-linguistic-profiling-10-638.jpg?cb=1456504658"" width=600 height=300>\n\n# In[2]:\n\n\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\n\n# In[3]:\n\n\n# libraries we need\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.svm import SVC\n\n\n# In[4]:\n\n\n# load iris dataset\ndata_holder = load_iris()\nprint(data_holder.data.shape)\nprint(data_holder.target.shape)\n\n\n# In[5]:\n\n\n# set our X and y to data and target values\nX , y = data_holder.data, data_holder.target\n\n\n# In[6]:\n\n\n# let\'s split into 70/30: train=70% and test=30%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= .4, random_state = 0)\n\n\n# In[7]:\n\n\nprint(""X train shape: "", X_train.shape)\nprint(""X test shape: "", X_test.shape)\nprint()\nprint(""y train shape: "", y_train.shape)\nprint(""y test shape: "", y_test.shape)\n\n\n# In[8]:\n\n\n# we\'ll set it to some parameters, but we\'ll go through depth on parameter tuning later\nmodel = SVC(kernel=\'linear\', C=1)\n# fit our training data\nmodel.fit(X_train, y_train)\n#let\'s predict \npred = model.predict(X_test)\n\n\n# ### Accuracy Score\n# Calling the accuracy_score class we can get the score of our model\n\n# In[10]:\n\n\n#accuracy score\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, pred)\n\n\n# In[11]:\n\n\n# let\'s get our classification report\n\n\n# ### Classification Metrics\n# Build a text report showing the main classification metrics\n\n# In[12]:\n\n\n#classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))\n\n\n# ### Confusion Matrix\n# A confusion matrix is a table that is often used to describe the performance of a classification model (or ""classifier"") on a set of test data for which the true values are known.\n\n# In[13]:\n\n\n#confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, pred))\n\n'"
All Python Codes/2017-25-11-so-evaluating-clustering-metrics-sklearn.py,0,"b'\n# coding: utf-8\n\n# ## Evaluating Clustering Metrics\n# \n# ### Main contents:\n# * adjusted rand index\n# * homogeneity\n# * V-measure\n\n# ### Rand index adjusted for chance.\n# The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n\n# In[1]:\n\n\nfrom sklearn.metrics import adjusted_rand_score\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nadjusted_rand_score(y_true, y_pred)\n\n\n# In[2]:\n\n\nadjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])  \n\n\n# In[3]:\n\n\nadjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n\n\n# ### Homogeneity\n# Homogeneity metric of a cluster labeling given a ground truth.\n# A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class.\n\n# In[4]:\n\n\nfrom sklearn.metrics.cluster import homogeneity_score\nhomogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n\n\n# In[5]:\n\n\nprint(""%.6f"" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))\nprint(""%.6f"" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))\n\n\n# In[7]:\n\n\nprint(""%.6f"" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))                                               \nprint(""%.6f"" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\n# you can just play around testing different data points                                                \n\n\n# ### V-measure\n# V-measure cluster labeling given a ground truth.\n# This score is identical to normalized_mutual_info_score.\n# \n# The V-measure is the harmonic mean between homogeneity and completeness:\n# v = 2 * (homogeneity * completeness) / (homogeneity + completeness)\n# \n# This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won\xe2\x80\x99t change the score value in any way.\n# \n# This metric is furthermore symmetric: switching label_true with label_pred will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known.\n\n# In[8]:\n\n\nfrom sklearn.metrics.cluster import v_measure_score\nv_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\nv_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n\n\n# In[9]:\n\n\nprint(""%.6f"" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\nprint(""%.6f"" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n\n\n# In[10]:\n\n\nprint(""%.6f"" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n\n'"
All Python Codes/2017-25-11-so-evaluating-regression-metrics-sklearn.py,0,"b""\n# coding: utf-8\n\n# ## Evaluating Regression Metrics\n# \n# ### Main contents:\n# * Mean Absolute Error\n# * Mean Squared Error\n# * Rsquare Score\n# \n# \n\n# ### Mean Absolute Error\n\n# In[2]:\n\n\nfrom sklearn.metrics import mean_absolute_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nmean_absolute_error(y_true, y_pred)\n\n\n# In[3]:\n\n\ny_true = [[0.5, 1], [-1, 1], [7, -6]]\ny_pred = [[0, 2], [-1, 2], [8, -5]]\nmean_absolute_error(y_true, y_pred)\n\n\n# In[5]:\n\n\nmean_absolute_error(y_true, y_pred, multioutput='raw_values')\nmean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n\n\n# ### Mean Squared Error\n\n# In[7]:\n\n\nfrom sklearn.metrics import mean_squared_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nmean_squared_error(y_true, y_pred)\n\n\n# In[8]:\n\n\ny_true = [[0.5, 1],[-1, 1],[7, -6]]\ny_pred = [[0, 2],[-1, 2],[8, -5]]\nmean_squared_error(y_true, y_pred) \n\n\n# In[9]:\n\n\nmean_squared_error(y_true, y_pred, multioutput='raw_values')\n\nmean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n\n\n# ### R-square Score\n# R^2 (coefficient of determination) regression score function.\n# Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n\n# In[12]:\n\n\nfrom sklearn.metrics import r2_score\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nr2_score(y_true, y_pred) \n\n\n# In[13]:\n\n\ny_true = [[0.5, 1], [-1, 1], [7, -6]]\ny_pred = [[0, 2], [-1, 2], [8, -5]]\nr2_score(y_true, y_pred, multioutput='variance_weighted')\n\n\n# In[14]:\n\n\ny_true = [1,2,3]\ny_pred = [1,2,3]\nr2_score(y_true, y_pred)\n\n\n# In[15]:\n\n\ny_true = [1,2,3]\ny_pred = [2,2,2]\nr2_score(y_true, y_pred)\n\n\n# In[16]:\n\n\ny_true = [1,2,3]\ny_pred = [3,2,1]\nr2_score(y_true, y_pred)\n\n"""
All Python Codes/2017-25-11-so-grid-search-sklearn.py,1,"b""\n# coding: utf-8\n\n# ## Grid Search\n# \n# Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc.\n\n# In[2]:\n\n\n# Load libraries\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn import linear_model\nfrom sklearn.model_selection import GridSearchCV\n\n\n# In[3]:\n\n\n# Load data\niris = load_iris()\nX = iris.data\ny = iris.target\n\n\n# ### create logistic regression\n\n# In[4]:\n\n\nlogistic = linear_model.LogisticRegression()\n\n\n# ### Create regularization penalty space\n\n# In[5]:\n\n\npenalty = ['l1', 'l2']\n\n# Create regularization hyperparameter space\nC = np.logspace(0, 4, 10)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)\n\n\n# ### Create grid search using 10-fold cross validation\n\n# In[6]:\n\n\nclf = GridSearchCV(logistic, hyperparameters, cv=10)\n\n\n# ### Fit grid search\n\n# In[15]:\n\n\nbest_model = clf.fit(X, y)\n\n\n# In[9]:\n\n\n# View best hyperparameters\nprint('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', best_model.best_estimator_.get_params()['C'])\n\n\n# ### Predict target vector\n\n# In[10]:\n\n\nbest_model.predict(X)\n\n\n# In[14]:\n\n\nbest_model.score(X,y)\n\n"""
All Python Codes/2017-25-11-so-load-data-sklearn.py,2,"b'\n# coding: utf-8\n\n# ## Loading Data in Sklearn\n# You can load your data from different formats or you can use build on data from sklearn\n# \n\n# In[7]:\n\n\n# we can just create random data\nimport numpy as np \nX = np.random.random((11,5))\ny = np.array([\'M\',\'M\',\'F\',\'F\',\'M\',\'F\',\'M\',\'M\',\'F\',\'F\',\'F\'])\nX[X < 0.7] = 0\n\n\n# In[8]:\n\n\n#use model selection to split our data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n\n# In[10]:\n\n\nX_train.shape\n\n\n# In[14]:\n\n\nX_test.shape\n\n\n# In[12]:\n\n\ny_train.shape\n\n\n# In[13]:\n\n\ny_test.shape\n\n\n# ### using datasets from sklearn\n\n# In[17]:\n\n\nimport sklearn.datasets as data\n\n\n# In[22]:\n\n\nget_ipython().magic(\'pinfo2 data\')\n\n\n# In[25]:\n\n\n# print all datasets\ndata.__all__ \n\n\n# In[21]:\n\n\n# we can get all of this data\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.datasets import load_boston\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.datasets import load_digits\nfrom sklearn.datasets import load_files\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_linnerud\nfrom sklearn.datasets import load_sample_images\nfrom sklearn.datasets import load_sample_image\nfrom sklearn.datasets import load_wine\n\n\n# In[33]:\n\n\nlbc = load_breast_cancer()\nprint(""load_breast_cancer"", lbc.data.shape)\nprint(""load_breast_cancer"", lbc.target.shape)\n\n\n# In[35]:\n\n\nlb = load_boston()\nprint(""load_boston: "", lb.data.shape)\nprint(""load_boston: "", lb.target.shape)\n\n\n# In[36]:\n\n\nld = load_digits()\nprint(""load_digits: "", ld.data.shape)\nprint(""load_digits: "", ld.target.shape)\n\n\n# In[37]:\n\n\nlr = load_iris()\nprint(""load_iris: "", lr.data.shape)\nprint(""load_iris: "", lr.target.shape)\n\n\n# you can do more with it...\n'"
All Python Codes/2017-25-11-so-map-pandas.py,1,"b'\n# coding: utf-8\n\n# ## Map function \n# Map values of Series using input correspondence (which can be a dict, Series, or function)\n# \n# \n\n# In[55]:\n\n\nimport pandas as pd\nimport numpy as np\n\n\n# In[56]:\n\n\ndata = pd.read_csv(\'data/train.csv\')\n\n\n# In[57]:\n\n\ndata.head()\n\n\n# In[58]:\n\n\n# let\'s convert 1 and 0 from sex column\ndata[""Sex_Num""] = data.Sex.map({\'female\':0,\'male\':1})\n\n\n# In[59]:\n\n\ndata.head()\n\n\n# In[60]:\n\n\ndata.loc[0:4, [""Sex"", ""Sex_Num""]]\n\n\n# In[61]:\n\n\n# let\'s get the mean fare price using map function\ndata[""mean_fare""] = data.Fare.map(lambda x: np.mean(x)a)\n\n\n# In[62]:\n\n\ndata.loc[0:3, [""Fare"",""mean_fare""]]\n\n\n# In[63]:\n\n\ndata[""Embarked""].unique()\n\n\n# In[64]:\n\n\ndata[""Dummy_Embarked""] = data.Embarked.map({\'S\':0,\'C\':1,\'Q\':3,None:4})\n\n\n# In[65]:\n\n\ndata.loc[0:4, [\'Embarked\',\'Dummy_Embarked\']]\n\n'"
All Python Codes/2017-25-11-so-model-fitting-supervised-unsupervised-sklearn.py,0,"b""\n# coding: utf-8\n\n# ## Model fitting\n# * Supervised Learning\n# * Unsupervised Learning\n\n# ### supervised learning model fitting\n\n# In[1]:\n\n\nfrom sklearn.datasets import load_iris\nholder = load_iris()\nX, y = holder.data, holder.target\n\n\n# In[7]:\n\n\n#supervised learning model fitting with linearRegression\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X,y)\n\n\n# In[8]:\n\n\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state =0)\n\n\n# In[9]:\n\n\n#supervised learning model fitting with Kneighbor classifier\nfrom sklearn import neighbors\nknn = neighbors.KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\n\n# In[10]:\n\n\n#supervised learning model fitting with support vector classifier\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='linear')\nsvc.fit(X_train, y_train)\n\n\n# ### unsupervised learning model fitting\n\n# In[11]:\n\n\n#unsupervised learning model fitting with kmeans\nfrom sklearn.cluster import KMeans\nk_means = KMeans(n_clusters=3, random_state=0)\nk_means.fit(X_train)\n\n\n# In[12]:\n\n\n#unsupervised learning model fitting with PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=0.95)\npca_model = pca.fit_transform(X_train)\n\n"""
All Python Codes/2017-25-11-so-model-prediction-supervised-unsupervised-sklearn.py,0,"b""\n# coding: utf-8\n\n# ## Model Prediction\n# * Supervised Learning\n# * Unsupervised Learning\n\n# In[2]:\n\n\nfrom sklearn.datasets import load_iris\nholder = load_iris()\nX, y = holder.data, holder.target\n\n\n# In[4]:\n\n\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state =0)\n\n\n# ### supervised learning model prediction\n\n# In[18]:\n\n\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression().fit(X_train,y_train)\n# here we call the predict methond\ny_pred = lr.predict(X_test)\n\n\n# In[17]:\n\n\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='linear').fit(X, y)\n# here we call the predict methond\ny_pred = svc.predict(X_test)\n\n\n# In[16]:\n\n\nfrom sklearn import neighbors\nknn = neighbors.KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)\n# here we call the predict methond\ny_pred = knn.predict_proba(X_test)\n\n\n# ### unsupervised learning model prediction\n\n# In[15]:\n\n\nfrom sklearn.cluster import KMeans\nk_means = KMeans(n_clusters=3, random_state=0).fit(X_train, y_train)\n# here we call the predict methond\ny_pred = k_means.predict(X_test)\n\n"""
All Python Codes/2017-25-11-so-preprocessing-binarization-sklearn.py,0,"b""\n# coding: utf-8\n\n# ## Preprocessing Binarization \n\n# In[3]:\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# In[4]:\n\n\nfrom sklearn.datasets import load_iris\nholder = load_iris()\nX, y = holder.data, holder.target\n\n\n# In[5]:\n\n\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state =0)\n\n\n# ### Binarizer \n\n# In[6]:\n\n\nfrom sklearn.preprocessing import Binarizer\nbinarizer = Binarizer(threshold=0.0).fit(X)\nbinary_X = binarizer.transform(X)\n\n\n# In[13]:\n\n\nX[0:4]\n\n\n# In[12]:\n\n\nbinary_X[0:4]\n\n"""
All Python Codes/2017-25-11-so-preprocessing-encoding-categorical-features-sklearn.py,0,"b'\n# coding: utf-8\n\n# ## LabelEncoder\n# Encode labels with value between 0 and n_classes-1.\n# \n\n# In[1]:\n\n\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\n\n# In[6]:\n\n\nfrom sklearn import preprocessing\n# call our labelEncoder class\nle = preprocessing.LabelEncoder()\n# fit our data\nle.fit([1, 2, 2, 6])\n# print classes\nle.classes_\n# transform\nle.transform([1, 1, 2, 6]) \n#print inverse data\nle.inverse_transform([0, 0, 1, 2])\n\n\n# In[9]:\n\n\nle = preprocessing.LabelEncoder()\nle.fit([""paris"", ""paris"", ""tokyo"", ""amsterdam""])\n\nlist(le.classes_)\n\nle.transform([""tokyo"", ""tokyo"", ""paris""]) \n\n#list(le.inverse_transform([2, 2, 1]))\n\n'"
All Python Codes/2017-25-11-so-preprocessing-imputing-missing-values-sklearn.py,2,"b""\n# coding: utf-8\n\n# ## Imputing missing values in sklearn\n# \n# Mean imputation replaces missing values with the mean value of that feature/variable. Mean imputation is one of the most 'naive' imputation methods because unlike more complex methods like k-nearest neighbors imputation, it does not use the information we have about an observation to estimate a value for it.\n\n# In[1]:\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# In[2]:\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import Imputer\n\n\n# In[12]:\n\n\n# Create an empty dataset\ndf = pd.DataFrame()\n\n# Create two variables called x0 and x1. Make the first value of x1 a missing value\ndf['x0'] = [0.3051,0.4949,0.6974,np.nan,0.2231,np.nan,0.4436,0.5897,0.6308,0.5]\ndf['x1'] = [np.nan,0.2654,0.2615,0.5846,0.4615,np.nan,0.4962,0.3269,np.nan,0.6731]\n\n# View the dataset\ndf\n\n\n# In[18]:\n\n\n# Create an imputer object that looks for 'Nan' values\nmean_imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n\n# Train the imputor on the df dataset\nmean_imputer = mean_imputer.fit(df)\n\n\n# In[19]:\n\n\n# Apply the imputer to the df dataset\nimputed_df = mean_imputer.transform(df.values)\n\n"""
All Python Codes/2017-25-11-so-preprocessing-normalization-sklearn.py,0,"b""\n# coding: utf-8\n\n# ## Preprocessing Normalization \n\n# In[4]:\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# In[5]:\n\n\nfrom sklearn.datasets import load_iris\nholder = load_iris()\nX, y = holder.data, holder.target\n\n\n# In[6]:\n\n\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state =0)\n\n\n# ### Normalizer \n\n# In[7]:\n\n\nfrom sklearn.preprocessing import Normalizer\nscaler = Normalizer().fit(X_train)\nnormalized_X = scaler.transform(X_train)\nnormalized_X_test = scaler.transform(X_test)\n\n"""
All Python Codes/2017-25-11-so-preprocessing-standardscaler-sklearn.py,0,"b""\n# coding: utf-8\n\n# ## Preprocessing StandardScaler \n\n# In[2]:\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# In[3]:\n\n\nfrom sklearn.datasets import load_iris\nholder = load_iris()\nX, y = holder.data, holder.target\n\n\n# In[4]:\n\n\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state =0)\n\n\n# ### StandardScaler \n\n# In[5]:\n\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(X_train)\nstandardized_X = scaler.transform(X_train)\nstandardized_X_test = scaler.transform(X_test)\n\n"""
All Python Codes/2017-25-11-so-randomized-parameter-optimization-sklearn.py,0,"b'\n# coding: utf-8\n\n# ## Randomized Parameter Optimization\n# \n# While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n\n# In[2]:\n\n\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\n\n# In[3]:\n\n\n# Load libraries\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn import linear_model\nfrom sklearn.grid_search import RandomizedSearchCV\n\n\n# In[4]:\n\n\n# Load data\niris = load_iris()\nX = iris.data\ny = iris.target\n\n\n# In[11]:\n\n\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state =0)\n\n\n# ### create logistic regression\n\n# In[16]:\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n\n\n# ### params build up\n\n# In[17]:\n\n\nparams = {""n_neighbors"" : range(1,5), ""weights"": [""uniform"", ""distance""]}\n\n\n# In[18]:\n\n\nrsearch = RandomizedSearchCV(estimator=knn, \n                             param_distributions=params, \n                             cv=4, n_iter=8,\n                             random_state=5)\n\n\n# In[19]:\n\n\nrsearch.fit(X_train, y_train)\n\n\n# In[20]:\n\n\nprint(rsearch.best_score_)\n\n\n# In[21]:\n\n\nprint(rsearch.best_estimator_)\n\n'"
Data Exploring/osl_model.py,1,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Tue Mar 14 10:57:11 2017\n\n@author: Saleban\n""""""\n#Multiple Linear Regression\n\n#Importing the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#Data\ndataset = pd.read_csv(\'50_Startups.csv\')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 4]\n\n#Categorical variables\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder = LabelEncoder()\nX[:, 3] = labelencoder.fit_transform(X[:, 3])\nonehotencoder = OneHotEncoder(categorical_features=[3])\nX = onehotencoder.fit_transform(X).toarray()\n\n#Avoiding dummy variable trap\nX = X[:, 1:]\n\n#Fitting train and test sets\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/5, random_state = 0)\n\n#Fitting multiple linear regression\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)\n\n#Predict test set result\ny_pred = regressor.predict(X_test)\n\n#Building the optimal model using backward elimination\nimport statsmodels.formula.api as sm\nX = np.append(arr = np.ones((50,1)).astype(int),values = X, axis = 1) \n#Let\'s assume that our Significance leve = 0.05\n#Fit the model with all possible prediction\n\n# here you start with all features...\n# and you keep removing, least predicted feature...\nX_opt = X[:,[0,1,2,3,4,5]]\nregressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()\nregressor_OLS.summary()\n\n#Remove the predictor - P > SL\nX_opt = X[:,[0,1,3,4,5]]\nregressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()\nregressor_OLS.summary()\n\n#Remove the predictor - P > SL\nX_opt = X[:,[0,3,4,5]]\nregressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()\nregressor_OLS.summary()\n\n#Remove the predictor - P > SL\nX_opt = X[:,[0,3,5]]\nregressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()\nregressor_OLS.summary()\n\n#Remove the predictor - P > SL\n# at the end you only left, some important features that tell more about your predicted model...\nX_opt = X[:,[0,3]]\nregressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()\nregressor_OLS.summary()\n\n\n\n\n'"
snippets - machine learning sklearn/cross-validation.py,0,"b'from sklearn.cross_validation import cross_val_score\nprint(cross_val_score(knn, X_train, y_train, cv = 5))\nprint(cross_val_score(lr, X, y, cv = 4))\n'"
snippets - machine learning sklearn/evaluating-classification-metrics.py,0,"b'#accuracy score\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n\n#classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n\n#confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, y_pred))'"
snippets - machine learning sklearn/evaluating-clustering-metrics.py,0,"b'#adjusted rand index\nfrom sklearn.metrics import adjusted_rand_score\nadjusted_rand_score(y_true, y_pred)\n\n#homogeneity\nfrom sklearn.metrics import homogeneity_score\nhomogeneity_score(y_true, y_pred)\n\n#V-measure\nfrom sklearn.metrics import v_measure_score\nmetrics.v_measure_score(y_true, y_pred)\n\n'"
snippets - machine learning sklearn/evaluating-regression-metrics.py,0,"b'#Mean Absolute Error\nfrom sklearn.metrics import mean_absolute_error\ny_true = [3, -0.5, 2]\nmean_absolute_error(y_true, y_pred)\n\n#mean squared error\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, y_pred)\n\n#Rsquare Score\nfrom sklearn.metrics import r2_score\nr2_score(y_true, y_pred)\n'"
snippets - machine learning sklearn/grid-search.py,1,"b'from sklearn.grid_search import GridSearchCV\nparams = {""n_neighbors"": np.arange(1,5),\n\t\t  ""metric"": [""euclidean"", ""cityblock""]}\ngrid = GridSearchCV(estimator=knn, param_grid=params)\ngrid.fit(X_train, y_train)\nprint(grid.best_score)\nprint(grid.best_estimator_.n_neighbors)\n\n'"
snippets - machine learning sklearn/loading-data.py,2,"b""import numpy as np \nX = np.random.random((10,5))\ny = np.array(['M','M','F','F','M','F','M','M','F','F','F'])\nX[X < 0.7] = 0\n#use model selection to split our data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n"""
snippets - machine learning sklearn/model-fitting-sup-unsup.py,0,"b""#supervised learning model fitting\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X,y)\n\nfrom sklearn import neighbors\nknn = neighbors.KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='linear')\nsvc.fit(X_train, y_train)\n\n#unsupervised learning model fitting\nfrom sklearn.cluster import KMeans\nk_means = KMeans(n_clusters=3, random_state=0)\nk_means.fit(X_train)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=0.95)\npca_model = pca.fit_transform(X_train)"""
snippets - machine learning sklearn/model-prediction-sup-unsup.py,1,"b""#supervised learning model prediction\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\ny_pred = lr.predict(X_test)\n\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='linear')\ny_pred = svc.predict(np.random.random((2,5)))\n\nfrom sklearn import neighbors\nknn = neighbors.KNeighborsClassifier(n_neighbors=5)\ny_pred = knn.predict_proba(X_test)\n\n#unsupervised learning model prediction\nfrom sklearn.cluster import KMeans\nk_means = KMeans(n_clusters=3, random_state=0)\ny_pred = k_means.predict(X_test)\n"""
snippets - machine learning sklearn/preprocessing-binarization.py,0,b'from sklearn.preprocessing import Binarizer\nbinarizer = Binarizer(threshold=0.0).fit(X)\nbinary_X = binarizer.transform(X)'
snippets - machine learning sklearn/preprocessing-encoding-categorical-features.py,0,b'from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)'
snippets - machine learning sklearn/preprocessing-imputing-missing-values.py,0,"b""from sklearn.preprocessing import Imputer\nimpute = Imputer(missing_values = 0, strategy='mean', axis=0)\nimpute.fit_transform(X_train)"""
snippets - machine learning sklearn/preprocessing-normalization.py,0,b'from sklearn.preprocessing import Normalizer\nscaler = Normalizer().fit(X_train)\nnormalized_X = scaler.transform(X_train)\nnormalized_X_test = scaler.transform(X_test)'
snippets - machine learning sklearn/preprocessing-polynomial-features.py,0,b'from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(5)\npoly.fit_transform(X)'
snippets - machine learning sklearn/preprocessing-standardization.py,0,b'from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(X_train)\nstandardized_X = scaler.transform(X_train)\nstandardized_X_test = scaler.transform(X_test)\n'
snippets - machine learning sklearn/randomized-parameter-optimization.py,0,"b'from sklearn.grid_search import RandomizedSearchCV\nparams = {""n_neighbors"" : range(1,5),\n\t\t  ""weights"": [""uniform"", ""distance""]}\nrsearch = RandomizedSearchCV(estimator=knn,\n\t\t\t\t\t\t\t param_distributions=params,\n\t\t\t\t\t\t\t cv=4,\n\t\t\t\t\t\t\t n_iter=8,\n\t\t\t\t\t\t\t random_state=5)\nrsearch.fit(X_train, y_train)\nprint(rsearch.best_score_)'"
snippets - machine learning sklearn/supervised-learning-estimators.py,0,"b""#LinearRegression\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression(normalize=True)\n\n#Support Vector Machine (SVM)\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='linear')\n\n#Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\n\n#K Nearest Neighbors\nfrom sklearn import neighbors \nknn = neighbors.KNeighborsClassifier(n_neighbors=5)"""
snippets - machine learning sklearn/unsupervised-learning-estimators.py,0,"b'#Principal Component Analysis PCA\nfrom sklearn.decompostion import PCA\npca = PCA(n_components=0.95)\n\n#K Means\nfrom sklearn.cluster import KMeans\nk_means = KMeans(n_clusters=3, random_state=0)\n'"
snippets - numpy/add_remove.py,4,"b'import numpy as np \n#appends values to end of arr\nnp.append(arr, values)\n#inserts values into arr before index 2\nnp.insert(arr, 2, values)\n#deletes row on index 3 of arr\nnp.delete(arr, 3, axis=0)\n#deletes column on index 4 of arr\nnp.delete(arr, 4, axis=1)'"
snippets - numpy/combing-splitting.py,4,"b'import numpy as np \n#adds arr2 as rows to the end of arr1\nnp.concatenate((arr1, arr2), axis=0)\n#adds arr2 as columns to end of arr1\nnp.concatenate((arr1, arr2), axis=1)\n#splits arr into 3 sub-arrays \nnp.split(arr, 3)\n#splits arr horizontally on the 5th index\nnp.hsplit(arr, 5)\n\n'"
snippets - numpy/copy-sort-reshape.py,1,"b'import numpy as np \n#copies arr to new memory\nnp.copy(arr)\n#creates view of arr elements with type dtype\narr.view(dtype)\n#sorts arr\narr.sort()\n#sorts specific axis of arr\narr.sort(axis=0)\n#flattens 2D array two_d_array to 1D\ntwo_d_arr.flatten()\n#transposes arr (rows become columns and vice versa)\narr.T \n#reshapes arr to 3 rows, 4 columns without changing data\narr.reshape(3,4)\n#changes arr shape to 5x6 all fills new views with 0\narr.resize((5,6))\n'"
snippets - numpy/creating-arrays.py,11,"b'import numpy as np \n#one dimensional array\nnp.array([1,2,3])\n#two dimensional array\nnp.array([(1,2,3),(4,5,6)])\n#1D array of length 3 all values 0\nnp.zeros(3)\n#3x4 array with all values 1\nnp.ones((3,4))\n#5x5 array of 0 with 1 on diagonal\nnp.eye(5)\n#array of 6 evenly divided values from 0 to 100\nnp.linspace(0,100,6)\n#array of values from 0 to less than 10 with step 3\nnp.arange(0,10,3)\n#2x3 array with all values 8\nnp.full((2,3), 8)\n#4x5 array of random floats between 0-1\nnp.random.rand(4,5)\n#6x7 array of random floats between 0-100\nnp.random.rand(6,7)*100\n#2x3 array with random ints between 0-4\nnp.random.randint(5,size=(2,3))\n'"
snippets - numpy/importing-exporing.py,4,"b""import numpy as np \n#from a text file\nnp.loadtxt('file_name.txt')\n#from a csv file\nnp.genfromtxt('file_name.csv', delimiter=',')\n#writes to a text file\nnp.savetxt('file_name.txt', arr, delimiter=' ')\n#writes to a csv file\nnp.savetxt('file_name.csv', arr, delimiter=',')"""
snippets - numpy/index-slice-subsetting.py,0,"b'import numpy as np \n#return the element at index 5\narr[5]\n#returns the 2D array element on index \narr[2,5]\n#assign array element on index 1 the value 4\narr[1] = 4\n#assign array element on index [1][3] the value 10\narr[1,3] = 10\n#return the elements at indices 0,1,2\n# on a 2D array: returns rows 0,1,2\narr[0:3]\n#returns the elements on rows 0,1,2, at column 4\narr[0:3, 4]\n#returns the elements at indices 0,1\narr[:2]\n#returns the elements at index 1 on all rows\narr[:, 1]\n#returns an array with boolean values\narr < 5\n#inverts a boolearn array, if its positive arr - convert to negative, vice versa\n~arr\n#returns array elements smaller than 5\narr[arr < 5]\n'"
snippets - numpy/inspecting-properties.py,3,"b'import numpy as np \n#return dimensions of elements in arr\narr.size\n#return dimensions of arr (rows, columns)\narr.shape\n#return type of elements in arr\narr.dtype\n#convert arr elements to type dtype\narr.astype(dtype)\n#convert arr to a python list\narr.tolist()\n#view documentation for np.eye\nnp.info(np.eye) \n#or\nnp.eye?'"
snippets - numpy/scalar-math.py,6,"b'import numpy as np \n#add 1 to each array element\nnp.add(arr, 1)\n#subtract 2 from each array element\nnp.subtract(arr, 2)\n#multiply each array element by 3\nnp.multiply(arr, 3)\n#divide each array element by 4 (returns np.nan for division by zero)\nnp.divide(arr, 4)\n#raise each array element to 5th power\nnp.power(arr, 5)\n\n\n'"
snippets - numpy/statistics-np.py,3,"b'import numpy as np \n#returns mean along specific axis\nnp.mean(arr, axis=0)\n#returns sum of arr\narr.sum()\n#returns minimum value of arr\narr.min()\n#returns maximum value of specific axis\narr.max(axis=0)\n#returns the variance of array\nnp.var(arr)\n#returns the standard deviation of specific axis\nnp.std(arr, axis=1)\n#returns correlation coefficient of array\narr.corrcoef() '"
snippets - numpy/vector-math.py,13,"b'import numpy as np \n#elementwise add arr2 to arr1\nnp.add(arr1, arr2)\n#elementwise subtract arr2 from arr1\nnp.subtract(arr1, arr2)\n#elementwise multiply arr1 by arr2\nnp.multiply(arr1,arr2)\n#elementwise divide arr1 by arr2\nnp.divide(arr1, arr2)\n#elementwise raise arr1 raised to the power of arr2\nnp.power(arr1,arr2)\n#returns True if the arrays have the same elements and shape\nnp.array_equal(arr1,arr2)\n#square root of each element in the array\nnp.sqrt(arr)\n#sine of each element in the array\nnp.sin(arr)\n#natural log of each element in the array\nnp.log(arr)\n#absolute value of each element in the array\nnp.abs(arr)\n#rounds up to the nearest int\nnp.ceil(arr)\n#rounds down to the nearest int\nnp.floor(arr)\n#rounds to the nearest int\nnp.round(arr)'"
snippets - pandas/combine-data-set.py,0,"b'import pandas as pd \n#join matching rows from bdf to adf\npd.merge(adf, bdf,\n\t\thow=\'left\', on=\'x1\')\n#join matching rows from adf to bdf\npd.merge(adf, bdf,\n\t\thow=\'right\', on=\'x1\')\n#join data. retain only rows in both sets\npd.merge(adf, bdf,\n\t\thow=\'inner\', on=\'x1\')\n#join data. retain all values, all rows\npd.merge(adf, bdf,\n\t\thow=\'outer\', on=\'x1\')\n#all rows in adf that have a match  in bdf\nadf[adf.x1.isin(bdf.x1)]\n#all ros in adf that do not have a match in bdf\nadf[~adf.x1.isin(bdf.x1)]\n#rows tha appear in both ydf and xdf (intersection)\npd.merge(ydf,zdf)\n#rows that appear in either or both ydf and zdf (union)\npd.merge(ydf,zdf, how=\'outer\')\n#rows tha appear in ydf but not xdf (setdiff)\npd.merge(ydf,zdf, how=\'outer\',\n\t\tindicator=True)\n.query(\'_merge == ""left_only""\')\n.drop([\'_merge\'], axis=1)\n# use ? for more information'"
snippets - pandas/creating-dataframes.py,0,"b'import pandas as pd \n#specify values for each column\ndf = pd.DataFrame(\n\t\t\t\t{""a"": [4,5,6],\n\t\t\t\t ""b"": [7,8,9],\n\t\t\t\t ""c"": [10,11,12]},\n\t\t\t\t index= [1,2,3])\n#specify values for each row\ndf = pd.DataFrame(\n\t[[4,7,10],\n\t [5,8,11],\n\t [6,9,12]],\n\t index=[1,2,3],\n\t columns=[\'a\',\'b\',\'c\'])\n#create dataframe with a multiIndex\ndf = pd.DataFrame(\n\t\t\t\t{""a"": [4,5,6],\n\t\t\t\t ""b"": [7,8,9],\n\t\t\t\t ""c"": [10,11,12]},\nindex = pd.MultiIndex.from_tuples(\n\t\t[(\'d\',1),(\'d\',2),(\'e\',2)],\n\t\tnames=[\'n\',\'v\']))\npd.MultiIndex?'"
snippets - pandas/group-data.py,0,"b'import pandas as pd \n#return a groupby object, grouped by values in column named \'col\'\ndf.groupby(by=""col"")\n#return a groupby objec, grouped by values in index level named \'ind\'\ndf.groupby(level=""ind"")\n# use ? for more information'"
snippets - pandas/handling-missing-data.py,0,b'import pandas as pd \n#drop rows with any column having NA/null data.\ndf.dropna()\n#replace all NA/null data with value\ndf.fillna(value)\n# use ? for more information'
snippets - pandas/make-new-column.py,0,"b""import pandas as pd \n#compute and append one or more new columns\ndf.assign(Area=lambda df: df.Length*df.Height)\n#add single column\ndf['Volume'] = df.Length*df.Height*df.Depth\n#bin column into a buckets\npd.qcut(df.col, n, labels=False)\n# use ? for more information"""
snippets - pandas/method--chaining.py,0,"b""import pandas as pd \n#most pandas methods return a DataFrame so that\n#another pandas method can be applied to the result.\n#this improves readability of code\ndf = (pd.melt(df)\n\t  .rename(columns={\n\t  \t\t\t\t'variable':'var',\n\t  \t\t\t\t'value':'val'})\n\t  .query('val >= 200')\n\t  )\npd.melt?"""
snippets - pandas/pandas_series_sample.py,2,"b""\n#\nimport pandas as pd \nimport numpy as np \n#create a series\ns = pd.Series(np.random.randn(5))\n#create a dataframe column\ndf = pd.DataFrame(s, columns=['column_name'])\ndf \n\n#sorting \ndf.sort_values(by='column_name')\n\n#boolean indexing\n#It returns all rows in column_name,\n#that are less than 10\ndf[df['column_name'] <= 10]\n\n#Sample data loading\nimport matplotlib.pyplot as plt \n%matplotlib inline\nplt.style.use('ggplot')\n#read data\ndf = pd.read_csv('data_path_name')\n#show data\ndf.head()\n\n#ploting \nplt.figure(figsize=(X,Y))\n#scatter\nplt.scatter(x=df['column_name'].index, y=['column_name'])\n\n#convert to datetime object\ntimes = pd.datetimeIndex(df['time_column'])\n#groupe by year, month, week, day, etc.\ngrouped = df.groupeby([times.year]).mean()\n#plot\nplt.plot(grouped['column_name'])\n\n#access null values \ndf[np.isnan(df['column_name'])]\n#use forward fill gap\ndf['column_name'] = df['column_name'].fillna(method='ffill')\n"""
snippets - pandas/plotting-pandas.py,0,"b""import pandas as pd \n#histogram for each column\ndf.plot.hist()\n#scatter chart using pairs of points\ndf.plot.scatter(x='w',y='h')"""
snippets - pandas/reshaping-data.py,0,"b""import pandas as pd \n#gather columns into rows\npd.melt(df)\n#spread rows into columns\ndf.pivot(columns='var', values='val')\n#append rows of dataframe\npd.concat([df1,df2])\n#append columns of dataframe\npd.concat([df1,df2], axis=1)\n#order rows by values of a column (low to high)\ndf.sort_values('row_name')\n#order row by values of a column (high to low)\ndf.sort_values('row_name', ascending=False)\n#return the columns of a dataframe\ndf.rename(columns={'y':'year'})\n#sort the index of a dataframe\ndf.sort_index()\n#reset index of dataframe to row numbers, moving index to column\ndf.reset_index()\n#drop columns from dataframe\ndf.drop(['Length','Height'], axis=1)\n#use ? for more info\n"""
snippets - pandas/subset-observations-rows.py,0,"b""import pandas as pd \n#extract rows that meet logical criteria\ndf[df.Lenght > 7]\n#remove duplicate rows (only considers columns)\ndf.drop_duplicates()\n#select first n rows\ndf.head(n)\n#select last n rows\ndf.tail(n)\n#randomly select fraction of rows\ndf.sample(frac=0.5)\n#randomly select n rows\ndf.sample(n=10)\n#select rows by position\ndf.iloc[10:20]\n#select and order top n entries\ndf.nlargest(n, 'value')\n#select and order bottom n entries\ndf.nsmallest(n, 'value')\n#refer to logic in python (and pandas)\n# use ? for more information\n"""
snippets - pandas/subset-variables-columns.py,0,"b""import pandas as pd \n#select multiple columns with specific names\ndf[['width', 'col_name2','col_name3']]\n#select single column with specific name\ndf['width'] #or\ndf.width\n#select columns whose names matches regular expression regex\ndf.filter(regex='regex')\n#select all columns between x2 and x4 inclusive\ndf.loc[:, 'x2':'x4']\n#select columns in positions 1,2 and 5.\ndf.iloc[:,[1,2,5]]\n#select rows meeting logical condition, and only the specific columns\ndf.loc[df['a'] > 10, ['a','c']]\n# use ? for more information"""
snippets - pandas/summarize-data.py,0,"b""import pandas as pd \n#count number of rows with each unique value of variable\ndf['w'].value_counts()\n#number of rows in dataframe\nlen(df)\n#number of distinct values in a column\ndf['w'].nunique()\n#descriptive statistics\ndf.describe()\n# use ? for more information"""
snippets - time series analysis/time-series.py,0,"b""import pandas as pd \n\n#specify with start date and number of periods\nrng = pd.date_range('2017 Jul 15 10:13',\n\t\t\t\t\tperiods=10, freq='M')\n\n#add timestamp\npd.Timestamp('2017-07-10')\n#or\npd.Timestamp('2017-07-10 10:15:13.999')\n\n#convert between a DateTimeIndex and a PeriodIndex\nts_dt.to_period()\t# convert to periodIndex\nts_pd.to_timestamp()# convert to dataTimeIndex\n\n#runing time\nimport timeit\n#always use %timeit before calling command\n%timeit data = pd.read()\n\n#data resampling\n_data = pd.date_range('1/1/2011', periods=72, freq='H')\nts = pd.Series(list(range(len(_data))), index=_data)\nts.head()\n \n#only access every 45 minutes\nconverted = ts.asfreq('45Min', method='ffill')\n"""
snippets - visualization/1d-data-plotting.py,0,"b""import matplotlib.pyplot as plt \n\n#plot data connected by lines\nlines = plt.plot(x,y)\n#creats a scatterplot, unconnected data points\nplt.scatter(x,y)\n#simple vertical bar chart\nplt.bar(xvalue, data, width, color...)\n#simple horizontal bar\nplt.barh(yvalues, data, width, color...)\n#plots a histogram\nplt.hist(x,y)\n#box and whisker plot\nplt.boxplot(x,y)\n#creates violin plot\nplt.violinplot(x,y)\n#fill area under/between plots\nax.fill(x,y, color='lightblue')\nax.fill_between(x,y, color='yellow')"""
snippets - visualization/Geographical_visualizing_sample.py,0,b'import pandas as pd \n'
snippets - visualization/customization-save-plot.py,0,"b""import matplotlib.pyplot as plt \n\n#saves plot/figure to image\nplt.savefig('pic_name.png')\n#saves transparent plot/figure to image\nplt.savefig('transparentback.png')\n\n#customization:\n#colors plot to color blue\nplt.plot(x,y, color='lightblue')\nplt.plot(x,y, alpha=0.4)\n#mappable: the image, contourset etc to which colorbar applies\nplt.colorbar(mappable, orientation='horizontal')\n"""
snippets - visualization/legends-ticks.py,0,"b'import matplotlib.pyplot as plt \n\n#Legends:\n#sets title of plot\nplt.title(\'Just goes here\')\n#sets label next to x-axis\nplt.xlabel(\'x-axis\')\n#sets label next to y-axis\nplt.ylabel(\'y-axis\')\n#set title and axis labels\nax.set(title=\'axis\', ylabel=\'y-axis\', xlabel=\'x-axis\')\n#no overlapping plot elements\nax.legend(loc=\'best\')\n\n#Ticks:\n#set ticks\nplt.xticks(x, labels, rotation=\'vertical\')\n#set x-ticks\nax.xaxis.set(ticks=range(1,5), ticklabels=[3,100,-12,""foo""])\n#make y-ticks longer and it go in and out\nax.tick_params(axis=\'y\', direction=\'inout\', length=10)\n'"
snippets - visualization/markers-lines.py,0,"b""import matplotlib.pyplot as plt \n\n#add * for every data point\nplt.plot(x,y, marker='*')\n#adds dot for every data point\nplt.plot(x,y, marker='.')\n\n#lines:\n#sets line width\nplt.plot(x,y, linewidth=2)\n#sets linestyle, ls can be ommitted\nplt.plot(x,y, ls='solid')\n#sets linestyle, ls can be ommitted\nplt.plot(x,y, ls='--')\n#lines are '--' and '_',\nplt.plot(x,y, '--', x**2, y**2, '-.')\n#sets properties of plot lines\nplt.setp(lines, color='red', linewidth=2)"""
snippets - visualization/plots-figure-axis.py,0,"b'import matplotlib.pyplot as plt \n#a container that contains all plot elements\nfig = plt.figures()\n\n#Initializes subplot\nfig.add_axes()\n#A subplot is an axes on a grid system, rows-cols num\na = fig.add_subplot(222)\n#adds subplot\nfig, b = plt.subplots(nrows=3, ncols=2)\n#creates subplot\nax = plt.subplots(2,2)\n\n'"
snippets - visualization/plots-figure.py,0,b''
snippets - visualization/text-limits.py,0,"b""import matplotlib.pyplot as plt \n\n#Text:\n#places text at coordinates 1/1\nplt.text(1,1, 'Example text', style='italic')\n#annotate the point with coordinates xy with text \nax.annotate('some annotation', xy=(10,10))\n#just put math formula\nplt.title(r'$delta_i=20$',fontsize=10)\n\n#Limits:\n#sets x-axis to display 0 - 7\nplt.xlim(0,7)\n#sets y-axis to display -0.5 - -9\nplt.ylim(-0.5,-9)\n#sets limits\nax.set(xlim=[0,7], ylim[-0.5,-9])\nax.set_xlim(0,7)\n#set margins: add padding to a plot, values 0 - 1\nplt.margins(x=1.0, y=1.0)\n#set the aspect ratio of the plot to 1\nplt.axis('equal')"""
Data Exploring/Data Exploring 12-25-17/osl_model.py,0,"b""sns.countplot(x='y', data=data, palette='hls')\nplt.show()\n"""
Data Exploring/mglearn/__init__.py,0,"b""from . import plots\nfrom . import tools\nfrom .plots import cm3, cm2\nfrom .tools import discrete_scatter\nfrom .plot_helpers import ReBl\n\n__all__ = ['tools', 'plots', 'cm3', 'cm2', 'discrete_scatter', 'ReBl']\n"""
Data Exploring/mglearn/datasets.py,11,"b'import numpy as np\nimport pandas as pd\nimport os\nfrom scipy import signal\nfrom sklearn.datasets import load_boston\nfrom sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\nfrom .make_blobs import make_blobs\n\nDATA_PATH = os.path.join(os.path.dirname(__file__), "".."", ""data"")\n\n\ndef make_forge():\n    # a carefully hand-designed dataset lol\n    X, y = make_blobs(centers=2, random_state=4, n_samples=30)\n    y[np.array([7, 27])] = 0\n    mask = np.ones(len(X), dtype=np.bool)\n    mask[np.array([0, 1, 5, 26])] = 0\n    X, y = X[mask], y[mask]\n    return X, y\n\n\ndef make_wave(n_samples=100):\n    rnd = np.random.RandomState(42)\n    x = rnd.uniform(-3, 3, size=n_samples)\n    y_no_noise = (np.sin(4 * x) + x)\n    y = (y_no_noise + rnd.normal(size=len(x))) / 2\n    return x.reshape(-1, 1), y\n\n\ndef load_extended_boston():\n    boston = load_boston()\n    X = boston.data\n\n    X = MinMaxScaler().fit_transform(boston.data)\n    X = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X)\n    return X, boston.target\n\n\ndef load_citibike():\n    data_mine = pd.read_csv(os.path.join(DATA_PATH, ""citibike.csv""))\n    data_mine[\'one\'] = 1\n    data_mine[\'starttime\'] = pd.to_datetime(data_mine.starttime)\n    data_starttime = data_mine.set_index(""starttime"")\n    data_resampled = data_starttime.resample(""3h"").sum().fillna(0)\n    return data_resampled.one\n\n\ndef make_signals():\n    # fix a random state seed\n    rng = np.random.RandomState(42)\n    n_samples = 2000\n    time = np.linspace(0, 8, n_samples)\n    # create three signals\n    s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\n    s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\n    s3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal\n\n    # concatenate the signals, add noise\n    S = np.c_[s1, s2, s3]\n    S += 0.2 * rng.normal(size=S.shape)\n\n    S /= S.std(axis=0)  # Standardize data\n    S -= S.min()\n    return S\n'"
Data Exploring/mglearn/make_blobs.py,4,"b'import numbers\nimport numpy as np\n\nfrom sklearn.utils import check_array, check_random_state\nfrom sklearn.utils import shuffle as shuffle_\n\n\ndef make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=1.0,\n               center_box=(-10.0, 10.0), shuffle=True, random_state=None):\n    """"""Generate isotropic Gaussian blobs for clustering.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, or tuple, optional (default=100)\n        The total number of points equally divided among clusters.\n\n    n_features : int, optional (default=2)\n        The number of features for each sample.\n\n    centers : int or array of shape [n_centers, n_features], optional\n        (default=3)\n        The number of centers to generate, or the fixed center locations.\n\n    cluster_std: float or sequence of floats, optional (default=1.0)\n        The standard deviation of the clusters.\n\n    center_box: pair of floats (min, max), optional (default=(-10.0, 10.0))\n        The bounding box for each cluster center when centers are\n        generated at random.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The generated samples.\n\n    y : array of shape [n_samples]\n        The integer labels for cluster membership of each sample.\n\n    Examples\n    --------\n    >>> from sklearn.datasets.samples_generator import make_blobs\n    >>> X, y = make_blobs(n_samples=10, centers=3, n_features=2,\n    ...                   random_state=0)\n    >>> print(X.shape)\n    (10, 2)\n    >>> y\n    array([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])\n\n    See also\n    --------\n    make_classification: a more intricate variant\n    """"""\n    generator = check_random_state(random_state)\n\n    if isinstance(centers, numbers.Integral):\n        centers = generator.uniform(center_box[0], center_box[1],\n                                    size=(centers, n_features))\n    else:\n        centers = check_array(centers)\n        n_features = centers.shape[1]\n\n    if isinstance(cluster_std, numbers.Real):\n        cluster_std = np.ones(len(centers)) * cluster_std\n\n    X = []\n    y = []\n\n    n_centers = centers.shape[0]\n    if isinstance(n_samples, numbers.Integral):\n        n_samples_per_center = [int(n_samples // n_centers)] * n_centers\n        for i in range(n_samples % n_centers):\n            n_samples_per_center[i] += 1\n    else:\n        n_samples_per_center = n_samples\n\n    for i, (n, std) in enumerate(zip(n_samples_per_center, cluster_std)):\n        X.append(centers[i] + generator.normal(scale=std,\n                                               size=(n, n_features)))\n        y += [i] * n\n\n    X = np.concatenate(X)\n    y = np.array(y)\n\n    if shuffle:\n        X, y = shuffle_(X, y, random_state=generator)\n\n    return X, y\n'"
Data Exploring/mglearn/plot_2d_separator.py,12,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom .plot_helpers import cm2, cm3, discrete_scatter\n\n\ndef plot_2d_classification(classifier, X, fill=False, ax=None, eps=None,\n                           alpha=1, cm=cm3):\n    # multiclass\n    if eps is None:\n        eps = X.std() / 2.\n\n    if ax is None:\n        ax = plt.gca()\n\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n    xx = np.linspace(x_min, x_max, 1000)\n    yy = np.linspace(y_min, y_max, 1000)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    X_grid = np.c_[X1.ravel(), X2.ravel()]\n    decision_values = classifier.predict(X_grid)\n    ax.imshow(decision_values.reshape(X1.shape), extent=(x_min, x_max,\n                                                         y_min, y_max),\n              aspect=\'auto\', origin=\'lower\', alpha=alpha, cmap=cm)\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n\ndef plot_2d_scores(classifier, X, ax=None, eps=None, alpha=1, cm=""viridis"",\n                   function=None):\n    # binary with fill\n    if eps is None:\n        eps = X.std() / 2.\n\n    if ax is None:\n        ax = plt.gca()\n\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n    xx = np.linspace(x_min, x_max, 100)\n    yy = np.linspace(y_min, y_max, 100)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    X_grid = np.c_[X1.ravel(), X2.ravel()]\n    if function is None:\n        function = getattr(classifier, ""decision_function"",\n                           getattr(classifier, ""predict_proba""))\n    else:\n        function = getattr(classifier, function)\n    decision_values = function(X_grid)\n    if decision_values.ndim > 1 and decision_values.shape[1] > 1:\n        # predict_proba\n        decision_values = decision_values[:, 1]\n    grr = ax.imshow(decision_values.reshape(X1.shape),\n                    extent=(x_min, x_max, y_min, y_max), aspect=\'auto\',\n                    origin=\'lower\', alpha=alpha, cmap=cm)\n\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n    return grr\n\n\ndef plot_2d_separator(classifier, X, fill=False, ax=None, eps=None, alpha=1,\n                      cm=cm2, linewidth=None, threshold=None,\n                      linestyle=""solid""):\n    # binary?\n    if eps is None:\n        eps = X.std() / 2.\n\n    if ax is None:\n        ax = plt.gca()\n\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n    xx = np.linspace(x_min, x_max, 1000)\n    yy = np.linspace(y_min, y_max, 1000)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    X_grid = np.c_[X1.ravel(), X2.ravel()]\n    try:\n        decision_values = classifier.decision_function(X_grid)\n        levels = [0] if threshold is None else [threshold]\n        fill_levels = [decision_values.min()] + levels + [\n            decision_values.max()]\n    except AttributeError:\n        # no decision_function\n        decision_values = classifier.predict_proba(X_grid)[:, 1]\n        levels = [.5] if threshold is None else [threshold]\n        fill_levels = [0] + levels + [1]\n    if fill:\n        ax.contourf(X1, X2, decision_values.reshape(X1.shape),\n                    levels=fill_levels, alpha=alpha, cmap=cm)\n    else:\n        ax.contour(X1, X2, decision_values.reshape(X1.shape), levels=levels,\n                   colors=""black"", alpha=alpha, linewidths=linewidth,\n                   linestyles=linestyle, zorder=5)\n\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n\nif __name__ == \'__main__\':\n    from sklearn.datasets import make_blobs\n    from sklearn.linear_model import LogisticRegression\n    X, y = make_blobs(centers=2, random_state=42)\n    clf = LogisticRegression().fit(X, y)\n    plot_2d_separator(clf, X, fill=True)\n    discrete_scatter(X[:, 0], X[:, 1], y)\n    plt.show()\n'"
Data Exploring/mglearn/plot_agglomerative.py,10,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.neighbors import KernelDensity\n\n\ndef plot_agglomerative_algorithm():\n    # generate synthetic two-dimensional data\n    X, y = make_blobs(random_state=0, n_samples=12)\n\n    agg = AgglomerativeClustering(n_clusters=X.shape[0], compute_full_tree=True).fit(X)\n\n    fig, axes = plt.subplots(X.shape[0] // 5, 5, subplot_kw={\'xticks\': (),\n                                                             \'yticks\': ()},\n                             figsize=(20, 8))\n\n    eps = X.std() / 2\n\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n    gridpoints = np.c_[xx.ravel().reshape(-1, 1), yy.ravel().reshape(-1, 1)]\n\n    for i, ax in enumerate(axes.ravel()):\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n        agg.n_clusters = X.shape[0] - i\n        agg.fit(X)\n        ax.set_title(""Step %d"" % i)\n        ax.scatter(X[:, 0], X[:, 1], s=60, c=\'grey\')\n        bins = np.bincount(agg.labels_)\n        for cluster in range(agg.n_clusters):\n            if bins[cluster] > 1:\n                points = X[agg.labels_ == cluster]\n                other_points = X[agg.labels_ != cluster]\n\n                kde = KernelDensity(bandwidth=.5).fit(points)\n                scores = kde.score_samples(gridpoints)\n                score_inside = np.min(kde.score_samples(points))\n                score_outside = np.max(kde.score_samples(other_points))\n                levels = .8 * score_inside + .2 * score_outside\n                ax.contour(xx, yy, scores.reshape(100, 100), levels=[levels],\n                           colors=\'k\', linestyles=\'solid\', linewidths=2)\n\n    axes[0, 0].set_title(""Initialization"")\n\n\ndef plot_agglomerative():\n    X, y = make_blobs(random_state=0, n_samples=12)\n    agg = AgglomerativeClustering(n_clusters=3)\n\n    eps = X.std() / 2.\n\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n    gridpoints = np.c_[xx.ravel().reshape(-1, 1), yy.ravel().reshape(-1, 1)]\n\n    ax = plt.gca()\n    for i, x in enumerate(X):\n        ax.text(x[0] + .1, x[1], ""%d"" % i, horizontalalignment=\'left\', verticalalignment=\'center\')\n\n    ax.scatter(X[:, 0], X[:, 1], s=60, c=\'grey\')\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n    for i in range(11):\n        agg.n_clusters = X.shape[0] - i\n        agg.fit(X)\n\n        bins = np.bincount(agg.labels_)\n        for cluster in range(agg.n_clusters):\n            if bins[cluster] > 1:\n                points = X[agg.labels_ == cluster]\n                other_points = X[agg.labels_ != cluster]\n\n                kde = KernelDensity(bandwidth=.5).fit(points)\n                scores = kde.score_samples(gridpoints)\n                score_inside = np.min(kde.score_samples(points))\n                score_outside = np.max(kde.score_samples(other_points))\n                levels = .8 * score_inside + .2 * score_outside\n                ax.contour(xx, yy, scores.reshape(100, 100), levels=[levels],\n                           colors=\'k\', linestyles=\'solid\', linewidths=1)\n\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n'"
Data Exploring/mglearn/plot_animal_tree.py,0,"b'from scipy.misc import imread\nimport matplotlib.pyplot as plt\n\n\ndef plot_animal_tree(ax=None):\n    import graphviz\n    if ax is None:\n        ax = plt.gca()\n    mygraph = graphviz.Digraph(node_attr={\'shape\': \'box\'},\n                               edge_attr={\'labeldistance\': ""10.5""},\n                               format=""png"")\n    mygraph.node(""0"", ""Has feathers?"")\n    mygraph.node(""1"", ""Can fly?"")\n    mygraph.node(""2"", ""Has fins?"")\n    mygraph.node(""3"", ""Hawk"")\n    mygraph.node(""4"", ""Penguin"")\n    mygraph.node(""5"", ""Dolphin"")\n    mygraph.node(""6"", ""Bear"")\n    mygraph.edge(""0"", ""1"", label=""True"")\n    mygraph.edge(""0"", ""2"", label=""False"")\n    mygraph.edge(""1"", ""3"", label=""True"")\n    mygraph.edge(""1"", ""4"", label=""False"")\n    mygraph.edge(""2"", ""5"", label=""True"")\n    mygraph.edge(""2"", ""6"", label=""False"")\n    mygraph.render(""tmp"")\n    ax.imshow(imread(""tmp.png""))\n    ax.set_axis_off()\n'"
Data Exploring/mglearn/plot_cross_validation.py,22,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef plot_group_kfold():\n    from sklearn.model_selection import GroupKFold\n    groups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]\n\n    plt.figure(figsize=(10, 2))\n    plt.title(""GroupKFold"")\n\n    axes = plt.gca()\n    axes.set_frame_on(False)\n\n    n_folds = 12\n    n_samples = 12\n    n_iter = 3\n    n_samples_per_fold = 1\n\n    cv = GroupKFold(n_splits=3)\n    mask = np.zeros((n_iter, n_samples))\n    for i, (train, test) in enumerate(cv.split(range(12), groups=groups)):\n        mask[i, train] = 1\n        mask[i, test] = 2\n\n    for i in range(n_folds):\n        # test is grey\n        colors = [""grey"" if x == 2 else ""white"" for x in mask[:, i]]\n        # not selected has no hatch\n\n        boxes = axes.barh(y=range(n_iter), width=[1 - 0.1] * n_iter,\n                          left=i * n_samples_per_fold, height=.6, color=colors,\n                          hatch=""//"", edgecolor=""k"", align=\'edge\')\n        for j in np.where(mask[:, i] == 0)[0]:\n            boxes[j].set_hatch("""")\n\n    axes.barh(y=[n_iter] * n_folds, width=[1 - 0.1] * n_folds,\n              left=np.arange(n_folds) * n_samples_per_fold, height=.6,\n              color=""w"", edgecolor=\'k\', align=""edge"")\n\n    for i in range(12):\n        axes.text((i + .5) * n_samples_per_fold, 3.5, ""%d"" %\n                  groups[i], horizontalalignment=""center"")\n\n    axes.invert_yaxis()\n    axes.set_xlim(0, n_samples + 1)\n    axes.set_ylabel(""CV iterations"")\n    axes.set_xlabel(""Data points"")\n    axes.set_xticks(np.arange(n_samples) + .5)\n    axes.set_xticklabels(np.arange(1, n_samples + 1))\n    axes.set_yticks(np.arange(n_iter + 1) + .3)\n    axes.set_yticklabels(\n        [""Split %d"" % x for x in range(1, n_iter + 1)] + [""Group""])\n    plt.legend([boxes[0], boxes[1]], [""Training set"", ""Test set""], loc=(1, .3))\n    plt.tight_layout()\n\n\ndef plot_shuffle_split():\n    from sklearn.model_selection import ShuffleSplit\n    plt.figure(figsize=(10, 2))\n    plt.title(""ShuffleSplit with 10 points""\n              "", train_size=5, test_size=2, n_splits=4"")\n\n    axes = plt.gca()\n    axes.set_frame_on(False)\n\n    n_folds = 10\n    n_samples = 10\n    n_iter = 4\n    n_samples_per_fold = 1\n\n    ss = ShuffleSplit(n_splits=4, train_size=5, test_size=2, random_state=43)\n    mask = np.zeros((n_iter, n_samples))\n    for i, (train, test) in enumerate(ss.split(range(10))):\n        mask[i, train] = 1\n        mask[i, test] = 2\n\n    for i in range(n_folds):\n        # test is grey\n        colors = [""grey"" if x == 2 else ""white"" for x in mask[:, i]]\n        # not selected has no hatch\n\n        boxes = axes.barh(y=range(n_iter), width=[1 - 0.1] * n_iter,\n                          left=i * n_samples_per_fold, height=.6, color=colors,\n                          hatch=""//"", edgecolor=\'k\', align=\'edge\')\n        for j in np.where(mask[:, i] == 0)[0]:\n            boxes[j].set_hatch("""")\n\n    axes.invert_yaxis()\n    axes.set_xlim(0, n_samples + 1)\n    axes.set_ylabel(""CV iterations"")\n    axes.set_xlabel(""Data points"")\n    axes.set_xticks(np.arange(n_samples) + .5)\n    axes.set_xticklabels(np.arange(1, n_samples + 1))\n    axes.set_yticks(np.arange(n_iter) + .3)\n    axes.set_yticklabels([""Split %d"" % x for x in range(1, n_iter + 1)])\n    # legend hacked for this random state\n    plt.legend([boxes[1], boxes[0], boxes[2]], [\n               ""Training set"", ""Test set"", ""Not selected""], loc=(1, .3))\n    plt.tight_layout()\n\n\ndef plot_stratified_cross_validation():\n    fig, both_axes = plt.subplots(2, 1, figsize=(12, 5))\n    # plt.title(""cross_validation_not_stratified"")\n    axes = both_axes[0]\n    axes.set_title(""Standard cross-validation with sorted class labels"")\n\n    axes.set_frame_on(False)\n\n    n_folds = 3\n    n_samples = 150\n\n    n_samples_per_fold = n_samples / float(n_folds)\n\n    for i in range(n_folds):\n        colors = [""w""] * n_folds\n        colors[i] = ""grey""\n        axes.barh(y=range(n_folds), width=[n_samples_per_fold - 1] *\n                  n_folds, left=i * n_samples_per_fold, height=.6,\n                  color=colors, hatch=""//"", edgecolor=\'k\', align=\'edge\')\n\n    axes.barh(y=[n_folds] * n_folds, width=[n_samples_per_fold - 1] *\n              n_folds, left=np.arange(3) * n_samples_per_fold, height=.6,\n              color=""w"", edgecolor=\'k\', align=\'edge\')\n\n    axes.invert_yaxis()\n    axes.set_xlim(0, n_samples + 1)\n    axes.set_ylabel(""CV iterations"")\n    axes.set_xlabel(""Data points"")\n    axes.set_xticks(np.arange(n_samples_per_fold / 2.,\n                              n_samples, n_samples_per_fold))\n    axes.set_xticklabels([""Fold %d"" % x for x in range(1, n_folds + 1)])\n    axes.set_yticks(np.arange(n_folds + 1) + .3)\n    axes.set_yticklabels(\n        [""Split %d"" % x for x in range(1, n_folds + 1)] + [""Class label""])\n    for i in range(3):\n        axes.text((i + .5) * n_samples_per_fold, 3.5, ""Class %d"" %\n                  i, horizontalalignment=""center"")\n\n    ax = both_axes[1]\n    ax.set_title(""Stratified Cross-validation"")\n    ax.set_frame_on(False)\n    ax.invert_yaxis()\n    ax.set_xlim(0, n_samples + 1)\n    ax.set_ylabel(""CV iterations"")\n    ax.set_xlabel(""Data points"")\n\n    ax.set_yticks(np.arange(n_folds + 1) + .3)\n    ax.set_yticklabels(\n        [""Split %d"" % x for x in range(1, n_folds + 1)] + [""Class label""])\n\n    n_subsplit = n_samples_per_fold / 3.\n    for i in range(n_folds):\n        test_bars = ax.barh(\n            y=[i] * n_folds, width=[n_subsplit - 1] * n_folds,\n            left=np.arange(n_folds) * n_samples_per_fold + i * n_subsplit,\n            height=.6, color=""grey"", hatch=""//"", edgecolor=\'k\', align=\'edge\')\n\n    w = 2 * n_subsplit - 1\n    ax.barh(y=[0] * n_folds, width=[w] * n_folds, left=np.arange(n_folds)\n            * n_samples_per_fold + (0 + 1) * n_subsplit, height=.6, color=""w"",\n            hatch=""//"", edgecolor=\'k\', align=\'edge\')\n    ax.barh(y=[1] * (n_folds + 1), width=[w / 2., w, w, w / 2.],\n            left=np.maximum(0, np.arange(n_folds + 1) * n_samples_per_fold -\n                            n_subsplit), height=.6, color=""w"", hatch=""//"",\n            edgecolor=\'k\', align=\'edge\')\n    training_bars = ax.barh(y=[2] * n_folds, width=[w] * n_folds,\n                            left=np.arange(n_folds) * n_samples_per_fold,\n                            height=.6, color=""w"", hatch=""//"", edgecolor=\'k\',\n                            align=\'edge\')\n\n    ax.barh(y=[n_folds] * n_folds, width=[n_samples_per_fold - 1] *\n            n_folds, left=np.arange(n_folds) * n_samples_per_fold, height=.6,\n            color=""w"", edgecolor=\'k\', align=\'edge\')\n\n    for i in range(3):\n        ax.text((i + .5) * n_samples_per_fold, 3.5, ""Class %d"" %\n                i, horizontalalignment=""center"")\n    ax.set_ylim(4, -0.1)\n    plt.legend([training_bars[0], test_bars[0]], [\n               \'Training data\', \'Test data\'], loc=(1.05, 1), frameon=False)\n\n    fig.tight_layout()\n\n\ndef plot_cross_validation():\n    plt.figure(figsize=(12, 2))\n    plt.title(""cross_validation"")\n    axes = plt.gca()\n    axes.set_frame_on(False)\n\n    n_folds = 5\n    n_samples = 25\n\n    n_samples_per_fold = n_samples / float(n_folds)\n\n    for i in range(n_folds):\n        colors = [""w""] * n_folds\n        colors[i] = ""grey""\n        bars = plt.barh(\n            y=range(n_folds), width=[n_samples_per_fold - 0.1] * n_folds,\n            left=i * n_samples_per_fold, height=.6, color=colors, hatch=""//"",\n            edgecolor=\'k\', align=\'edge\')\n    axes.invert_yaxis()\n    axes.set_xlim(0, n_samples + 1)\n    plt.ylabel(""CV iterations"")\n    plt.xlabel(""Data points"")\n    plt.xticks(np.arange(n_samples_per_fold / 2., n_samples,\n                         n_samples_per_fold),\n               [""Fold %d"" % x for x in range(1, n_folds + 1)])\n    plt.yticks(np.arange(n_folds) + .3,\n               [""Split %d"" % x for x in range(1, n_folds + 1)])\n    plt.legend([bars[0], bars[4]], [\'Training data\', \'Test data\'],\n               loc=(1.05, 0.4), frameon=False)\n\n\ndef plot_threefold_split():\n    plt.figure(figsize=(15, 1))\n    axis = plt.gca()\n    bars = axis.barh([0, 0, 0], [11.9, 2.9, 4.9], left=[0, 12, 15], color=[\n                     \'white\', \'grey\', \'grey\'], hatch=""//"", edgecolor=\'k\',\n                     align=\'edge\')\n    bars[2].set_hatch(r"""")\n    axis.set_yticks(())\n    axis.set_frame_on(False)\n    axis.set_ylim(-.1, .8)\n    axis.set_xlim(-0.1, 20.1)\n    axis.set_xticks([6, 13.3, 17.5])\n    axis.set_xticklabels([""training set"", ""validation set"",\n                          ""test set""], fontdict={\'fontsize\': 20})\n    axis.tick_params(length=0, labeltop=True, labelbottom=False)\n    axis.text(6, -.3, ""Model fitting"",\n              fontdict={\'fontsize\': 13}, horizontalalignment=""center"")\n    axis.text(13.3, -.3, ""Parameter selection"",\n              fontdict={\'fontsize\': 13}, horizontalalignment=""center"")\n    axis.text(17.5, -.3, ""Evaluation"",\n              fontdict={\'fontsize\': 13}, horizontalalignment=""center"")\n'"
Data Exploring/mglearn/plot_dbscan.py,1,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\nfrom .plot_helpers import discrete_scatter, cm3\n\n\ndef plot_dbscan():\n    X, y = make_blobs(random_state=0, n_samples=12)\n\n    dbscan = DBSCAN()\n    clusters = dbscan.fit_predict(X)\n    clusters\n\n    fig, axes = plt.subplots(3, 4, figsize=(11, 8),\n                             subplot_kw={\'xticks\': (), \'yticks\': ()})\n    # Plot clusters as red, green and blue, and outliers (-1) as white\n    colors = [cm3(1), cm3(0), cm3(2)]\n    markers = [\'o\', \'^\', \'v\']\n\n    # iterate over settings of min_samples and eps\n    for i, min_samples in enumerate([2, 3, 5]):\n        for j, eps in enumerate([1, 1.5, 2, 3]):\n            # instantiate DBSCAN with a particular setting\n            dbscan = DBSCAN(min_samples=min_samples, eps=eps)\n            # get cluster assignments\n            clusters = dbscan.fit_predict(X)\n            print(""min_samples: %d eps: %f  cluster: %s""\n                  % (min_samples, eps, clusters))\n            if np.any(clusters == -1):\n                c = [\'w\'] + colors\n                m = [\'o\'] + markers\n            else:\n                c = colors\n                m = markers\n            discrete_scatter(X[:, 0], X[:, 1], clusters, ax=axes[i, j], c=c,\n                             s=8, markers=m)\n            inds = dbscan.core_sample_indices_\n            # vizualize core samples and clusters.\n            if len(inds):\n                discrete_scatter(X[inds, 0], X[inds, 1], clusters[inds],\n                                 ax=axes[i, j], s=15, c=colors,\n                                 markers=markers)\n            axes[i, j].set_title(""min_samples: %d eps: %.1f""\n                                 % (min_samples, eps))\n    fig.tight_layout()\n'"
Data Exploring/mglearn/plot_decomposition.py,0,"b'import matplotlib.pyplot as plt\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\n\n\ndef plot_decomposition(people, pca):\n    image_shape = people.images[0].shape\n    plt.figure(figsize=(20, 3))\n    ax = plt.gca()\n\n    imagebox = OffsetImage(people.images[0], zoom=1.5, cmap=""gray"")\n    ab = AnnotationBbox(imagebox, (.05, 0.4), pad=0.0, xycoords=\'data\')\n    ax.add_artist(ab)\n\n    for i in range(4):\n        imagebox = OffsetImage(pca.components_[i].reshape(image_shape), zoom=1.5, cmap=""viridis"")\n\n        ab = AnnotationBbox(imagebox, (.3 + .2 * i, 0.4),\n                            pad=0.0,\n                            xycoords=\'data\'\n                            )\n        ax.add_artist(ab)\n        if i == 0:\n            plt.text(.18, .25, \'x_%d *\' % i, fontdict={\'fontsize\': 50})\n        else:\n            plt.text(.15 + .2 * i, .25, \'+ x_%d *\' % i, fontdict={\'fontsize\': 50})\n\n    plt.text(.95, .25, \'+ ...\', fontdict={\'fontsize\': 50})\n\n    plt.rc(\'text\', usetex=True)\n    plt.text(.13, .3, r\'\\approx\', fontdict={\'fontsize\': 50})\n    plt.axis(""off"")\n'"
Data Exploring/mglearn/plot_grid_search.py,1,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.datasets import load_iris\nimport pandas as pd\n\n\ndef plot_cross_val_selection():\n    iris = load_iris()\n    X_trainval, X_test, y_trainval, y_test = train_test_split(iris.data,\n                                                              iris.target,\n                                                              random_state=0)\n\n    param_grid = {\'C\': [0.001, 0.01, 0.1, 1, 10, 100],\n                  \'gamma\': [0.001, 0.01, 0.1, 1, 10, 100]}\n    grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n    grid_search.fit(X_trainval, y_trainval)\n    results = pd.DataFrame(grid_search.cv_results_)[15:]\n\n    best = np.argmax(results.mean_test_score.values)\n    plt.figure(figsize=(10, 3))\n    plt.xlim(-1, len(results))\n    plt.ylim(0, 1.1)\n    for i, (_, row) in enumerate(results.iterrows()):\n        scores = row[[\'split%d_test_score\' % i for i in range(5)]]\n        marker_cv, = plt.plot([i] * 5, scores, \'^\', c=\'gray\', markersize=5,\n                              alpha=.5)\n        marker_mean, = plt.plot(i, row.mean_test_score, \'v\', c=\'none\', alpha=1,\n                                markersize=10, markeredgecolor=\'k\')\n        if i == best:\n            marker_best, = plt.plot(i, row.mean_test_score, \'o\', c=\'red\',\n                                    fillstyle=""none"", alpha=1, markersize=20,\n                                    markeredgewidth=3)\n\n    plt.xticks(range(len(results)), [str(x).strip(""{}"").replace(""\'"", """") for x\n                                     in grid_search.cv_results_[\'params\']],\n               rotation=90)\n    plt.ylabel(""Validation accuracy"")\n    plt.xlabel(""Parameter settings"")\n    plt.legend([marker_cv, marker_mean, marker_best],\n               [""cv accuracy"", ""mean accuracy"", ""best parameter setting""],\n               loc=(1.05, .4))\n\n\ndef plot_grid_search_overview():\n    plt.figure(figsize=(10, 3), dpi=70)\n    axes = plt.gca()\n    axes.yaxis.set_visible(False)\n    axes.xaxis.set_visible(False)\n    axes.set_frame_on(False)\n\n    def draw(ax, text, start, target=None):\n        if target is not None:\n            patchB = target.get_bbox_patch()\n            end = target.get_position()\n        else:\n            end = start\n            patchB = None\n        annotation = ax.annotate(text, end, start, xycoords=\'axes pixels\',\n                                 textcoords=\'axes pixels\', size=20,\n                                 arrowprops=dict(\n                                     arrowstyle=""-|>"", fc=""w"", ec=""k"",\n                                     patchB=patchB,\n                                     connectionstyle=""arc3,rad=0.0""),\n                                 bbox=dict(boxstyle=""round"", fc=""w""),\n                                 horizontalalignment=""center"",\n                                 verticalalignment=""center"")\n        plt.draw()\n        return annotation\n\n    step = 100\n    grr = 400\n\n    final_evaluation = draw(axes, ""final evaluation"", (5 * step, grr - 3 *\n                                                       step))\n    retrained_model = draw(axes, ""retrained model"", (3 * step, grr - 3 * step),\n                           final_evaluation)\n    best_parameters = draw(axes, ""best parameters"", (.5 * step, grr - 3 *\n                                                     step), retrained_model)\n    cross_validation = draw(axes, ""cross-validation"", (.5 * step, grr - 2 *\n                                                       step), best_parameters)\n    draw(axes, ""parameter grid"", (0.0, grr - 0), cross_validation)\n    training_data = draw(axes, ""training data"", (2 * step, grr - step),\n                         cross_validation)\n    draw(axes, ""training data"", (2 * step, grr - step), retrained_model)\n    test_data = draw(axes, ""test data"", (5 * step, grr - step),\n                     final_evaluation)\n    draw(axes, ""data set"", (3.5 * step, grr - 0.0), training_data)\n    draw(axes, ""data set"", (3.5 * step, grr - 0.0), test_data)\n    plt.ylim(0, 1)\n    plt.xlim(0, 1.5)\n'"
Data Exploring/mglearn/plot_helpers.py,3,"b'import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap, colorConverter, LinearSegmentedColormap\n\n\ncm_cycle = ListedColormap([\'#0000aa\', \'#ff5050\', \'#50ff50\', \'#9040a0\', \'#fff000\'])\ncm3 = ListedColormap([\'#0000aa\', \'#ff2020\', \'#50ff50\'])\ncm2 = ListedColormap([\'#0000aa\', \'#ff2020\'])\n\n# create a smooth transition from the first to to the second color of cm3\n# similar to RdBu but with our red and blue, also not going through white,\n# which is really bad for greyscale\n\ncdict = {\'red\': [(0.0, 0.0, cm2(0)[0]),\n                 (1.0, cm2(1)[0], 1.0)],\n\n         \'green\': [(0.0, 0.0, cm2(0)[1]),\n                   (1.0, cm2(1)[1], 1.0)],\n\n         \'blue\': [(0.0, 0.0, cm2(0)[2]),\n                  (1.0, cm2(1)[2], 1.0)]}\n\nReBl = LinearSegmentedColormap(""ReBl"", cdict)\n\n\ndef discrete_scatter(x1, x2, y=None, markers=None, s=10, ax=None,\n                     labels=None, padding=.2, alpha=1, c=None, markeredgewidth=None):\n    """"""Adaption of matplotlib.pyplot.scatter to plot classes or clusters.\n\n    Parameters\n    ----------\n\n    x1 : nd-array\n        input data, first axis\n\n    x2 : nd-array\n        input data, second axis\n\n    y : nd-array\n        input data, discrete labels\n\n    cmap : colormap\n        Colormap to use.\n\n    markers : list of string\n        List of markers to use, or None (which defaults to \'o\').\n\n    s : int or float\n        Size of the marker\n\n    padding : float\n        Fraction of the dataset range to use for padding the axes.\n\n    alpha : float\n        Alpha value for all points.\n    """"""\n    if ax is None:\n        ax = plt.gca()\n\n    if y is None:\n        y = np.zeros(len(x1))\n\n    unique_y = np.unique(y)\n\n    if markers is None:\n        markers = [\'o\', \'^\', \'v\', \'D\', \'s\', \'*\', \'p\', \'h\', \'H\', \'8\', \'<\', \'>\'] * 10\n\n    if len(markers) == 1:\n        markers = markers * len(unique_y)\n\n    if labels is None:\n        labels = unique_y\n\n    # lines in the matplotlib sense, not actual lines\n    lines = []\n\n    current_cycler = mpl.rcParams[\'axes.prop_cycle\']\n\n    for i, (yy, cycle) in enumerate(zip(unique_y, current_cycler())):\n        mask = y == yy\n        # if c is none, use color cycle\n        if c is None:\n            color = cycle[\'color\']\n        elif len(c) > 1:\n            color = c[i]\n        else:\n            color = c\n        # use light edge for dark markers\n        if np.mean(colorConverter.to_rgb(color)) < .4:\n            markeredgecolor = ""grey""\n        else:\n            markeredgecolor = ""black""\n\n        lines.append(ax.plot(x1[mask], x2[mask], markers[i], markersize=s,\n                             label=labels[i], alpha=alpha, c=color,\n                             markeredgewidth=markeredgewidth,\n                             markeredgecolor=markeredgecolor)[0])\n\n    if padding != 0:\n        pad1 = x1.std() * padding\n        pad2 = x2.std() * padding\n        xlim = ax.get_xlim()\n        ylim = ax.get_ylim()\n        ax.set_xlim(min(x1.min() - pad1, xlim[0]), max(x1.max() + pad1, xlim[1]))\n        ax.set_ylim(min(x2.min() - pad2, ylim[0]), max(x2.max() + pad2, ylim[1]))\n\n    return lines\n'"
Data Exploring/mglearn/plot_improper_preprocessing.py,0,"b'import matplotlib.pyplot as plt\n\n\ndef make_bracket(s, xy, textxy, width, ax):\n    annotation = ax.annotate(\n        s, xy, textxy, ha=""center"", va=""center"", size=20,\n        arrowprops=dict(arrowstyle=""-["", fc=""w"", ec=""k"",\n                        lw=2,), bbox=dict(boxstyle=""square"", fc=""w""))\n    annotation.arrow_patch.get_arrowstyle().widthB = width\n\n\ndef plot_improper_processing():\n    fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n\n    for axis in axes:\n        bars = axis.barh([0, 0, 0], [11.9, 2.9, 4.9], left=[0, 12, 15],\n                         color=[\'white\', \'grey\', \'grey\'], hatch=""//"",\n                         align=\'edge\', edgecolor=\'k\')\n        bars[2].set_hatch(r"""")\n        axis.set_yticks(())\n        axis.set_frame_on(False)\n        axis.set_ylim(-.1, 6)\n        axis.set_xlim(-0.1, 20.1)\n        axis.set_xticks(())\n        axis.tick_params(length=0, labeltop=True, labelbottom=False)\n        axis.text(6, -.3, ""training folds"",\n                  fontdict={\'fontsize\': 14}, horizontalalignment=""center"")\n        axis.text(13.5, -.3, ""validation fold"",\n                  fontdict={\'fontsize\': 14}, horizontalalignment=""center"")\n        axis.text(17.5, -.3, ""test set"",\n                  fontdict={\'fontsize\': 14}, horizontalalignment=""center"")\n\n    make_bracket(""scaler fit"", (7.5, 1.3), (7.5, 2.), 15, axes[0])\n    make_bracket(""SVC fit"", (6, 3), (6, 4), 12, axes[0])\n    make_bracket(""SVC predict"", (13.4, 3), (13.4, 4), 2.5, axes[0])\n\n    axes[0].set_title(""Cross validation"")\n    axes[1].set_title(""Test set prediction"")\n\n    make_bracket(""scaler fit"", (7.5, 1.3), (7.5, 2.), 15, axes[1])\n    make_bracket(""SVC fit"", (7.5, 3), (7.5, 4), 15, axes[1])\n    make_bracket(""SVC predict"", (17.5, 3), (17.5, 4), 4.8, axes[1])\n\n\ndef plot_proper_processing():\n    fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n\n    for axis in axes:\n        bars = axis.barh([0, 0, 0], [11.9, 2.9, 4.9],\n                         left=[0, 12, 15], color=[\'white\', \'grey\', \'grey\'],\n                         hatch=""//"", align=\'edge\', edgecolor=\'k\')\n        bars[2].set_hatch(r"""")\n        axis.set_yticks(())\n        axis.set_frame_on(False)\n        axis.set_ylim(-.1, 4.5)\n        axis.set_xlim(-0.1, 20.1)\n        axis.set_xticks(())\n        axis.tick_params(length=0, labeltop=True, labelbottom=False)\n        axis.text(6, -.3, ""training folds"", fontdict={\'fontsize\': 14},\n                  horizontalalignment=""center"")\n        axis.text(13.5, -.3, ""validation fold"", fontdict={\'fontsize\': 14},\n                  horizontalalignment=""center"")\n        axis.text(17.5, -.3, ""test set"", fontdict={\'fontsize\': 14},\n                  horizontalalignment=""center"")\n\n    make_bracket(""scaler fit"", (6, 1.3), (6, 2.), 12, axes[0])\n    make_bracket(""SVC fit"", (6, 3), (6, 4), 12, axes[0])\n    make_bracket(""SVC predict"", (13.4, 3), (13.4, 4), 2.5, axes[0])\n\n    axes[0].set_title(""Cross validation"")\n    axes[1].set_title(""Test set prediction"")\n\n    make_bracket(""scaler fit"", (7.5, 1.3), (7.5, 2.), 15, axes[1])\n    make_bracket(""SVC fit"", (7.5, 3), (7.5, 4), 15, axes[1])\n    make_bracket(""SVC predict"", (17.5, 3), (17.5, 4), 4.8, axes[1])\n    fig.subplots_adjust(hspace=.3)\n'"
Data Exploring/mglearn/plot_interactive_tree.py,6,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.externals.six import StringIO  # doctest: +SKIP\nfrom sklearn.tree import export_graphviz\nfrom scipy.misc import imread\nfrom scipy import ndimage\nfrom sklearn.datasets import make_moons\n\nimport re\n\nfrom .tools import discrete_scatter\nfrom .plot_helpers import cm2\n\n\ndef tree_image(tree, fout=None):\n    try:\n        import graphviz\n    except ImportError:\n        # make a hacky white plot\n        x = np.ones((10, 10))\n        x[0, 0] = 0\n        return x\n    dot_data = StringIO()\n    export_graphviz(tree, out_file=dot_data, max_depth=3, impurity=False)\n    data = dot_data.getvalue()\n    #data = re.sub(r""gini = 0\\.[0-9]+\\\\n"", """", dot_data.getvalue())\n    data = re.sub(r""samples = [0-9]+\\\\n"", """", data)\n    data = re.sub(r""\\\\nsamples = [0-9]+"", """", data)\n    data = re.sub(r""value"", ""counts"", data)\n\n    graph = graphviz.Source(data, format=""png"")\n    if fout is None:\n        fout = ""tmp""\n    graph.render(fout)\n    return imread(fout + "".png"")\n\n\ndef plot_tree_progressive():\n    X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n    plt.figure()\n    ax = plt.gca()\n    discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n    axes = []\n    for i in range(3):\n        fig, ax = plt.subplots(1, 2, figsize=(12, 4),\n                               subplot_kw={\'xticks\': (), \'yticks\': ()})\n        axes.append(ax)\n    axes = np.array(axes)\n\n    for i, max_depth in enumerate([1, 2, 9]):\n        tree = plot_tree(X, y, max_depth=max_depth, ax=axes[i, 0])\n        axes[i, 1].imshow(tree_image(tree))\n        axes[i, 1].set_axis_off()\n\n\ndef plot_tree_partition(X, y, tree, ax=None):\n    if ax is None:\n        ax = plt.gca()\n    eps = X.std() / 2.\n\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n    xx = np.linspace(x_min, x_max, 1000)\n    yy = np.linspace(y_min, y_max, 1000)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    X_grid = np.c_[X1.ravel(), X2.ravel()]\n\n    Z = tree.predict(X_grid)\n    Z = Z.reshape(X1.shape)\n    faces = tree.apply(X_grid)\n    faces = faces.reshape(X1.shape)\n    border = ndimage.laplace(faces) != 0\n    ax.contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\n    ax.scatter(X1[border], X2[border], marker=\'.\', s=1)\n\n    discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n    return ax\n\n\ndef plot_tree(X, y, max_depth=1, ax=None):\n    tree = DecisionTreeClassifier(max_depth=max_depth, random_state=0).fit(X, y)\n    ax = plot_tree_partition(X, y, tree, ax=ax)\n    ax.set_title(""depth = %d"" % max_depth)\n    return tree\n'"
Data Exploring/mglearn/plot_kmeans.py,6,"b'import numpy as np\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\n\nfrom .tools import discrete_scatter\nfrom .plot_2d_separator import plot_2d_classification\nfrom .plot_helpers import cm3\n\n\ndef plot_kmeans_algorithm():\n\n    X, y = make_blobs(random_state=1)\n    # we don\'t want cyan in there\n    with mpl.rc_context(rc={\'axes.prop_cycle\': cycler(\'color\', [\'#0000aa\',\n                                                                \'#ff2020\',\n                                                                \'#50ff50\'])}):\n        fig, axes = plt.subplots(3, 3, figsize=(10, 8), subplot_kw={\'xticks\': (), \'yticks\': ()})\n        axes = axes.ravel()\n        axes[0].set_title(""Input data"")\n        discrete_scatter(X[:, 0], X[:, 1], ax=axes[0], markers=[\'o\'], c=\'w\')\n\n        axes[1].set_title(""Initialization"")\n        init = X[:3, :]\n        discrete_scatter(X[:, 0], X[:, 1], ax=axes[1], markers=[\'o\'], c=\'w\')\n        discrete_scatter(init[:, 0], init[:, 1], [0, 1, 2], ax=axes[1],\n                         markers=[\'^\'], markeredgewidth=2)\n\n        axes[2].set_title(""Assign Points (1)"")\n        km = KMeans(n_clusters=3, init=init, max_iter=1, n_init=1).fit(X)\n        centers = km.cluster_centers_\n        # need to compute labels by hand. scikit-learn does two e-steps for max_iter=1\n        # (and it\'s totally my fault)\n        labels = np.argmin(pairwise_distances(init, X), axis=0)\n        discrete_scatter(X[:, 0], X[:, 1], labels, markers=[\'o\'],\n                         ax=axes[2])\n        discrete_scatter(init[:, 0], init[:, 1], [0, 1, 2],\n                         ax=axes[2], markers=[\'^\'], markeredgewidth=2)\n\n        axes[3].set_title(""Recompute Centers (1)"")\n        discrete_scatter(X[:, 0], X[:, 1], labels, markers=[\'o\'],\n                         ax=axes[3])\n        discrete_scatter(centers[:, 0], centers[:, 1], [0, 1, 2],\n                         ax=axes[3], markers=[\'^\'], markeredgewidth=2)\n\n        axes[4].set_title(""Reassign Points (2)"")\n        km = KMeans(n_clusters=3, init=init, max_iter=1, n_init=1).fit(X)\n        labels = km.labels_\n        discrete_scatter(X[:, 0], X[:, 1], labels, markers=[\'o\'],\n                         ax=axes[4])\n        discrete_scatter(centers[:, 0], centers[:, 1], [0, 1, 2],\n                         ax=axes[4], markers=[\'^\'], markeredgewidth=2)\n\n        km = KMeans(n_clusters=3, init=init, max_iter=2, n_init=1).fit(X)\n        axes[5].set_title(""Recompute Centers (2)"")\n        centers = km.cluster_centers_\n        discrete_scatter(X[:, 0], X[:, 1], labels, markers=[\'o\'],\n                         ax=axes[5])\n        discrete_scatter(centers[:, 0], centers[:, 1], [0, 1, 2],\n                         ax=axes[5], markers=[\'^\'], markeredgewidth=2)\n\n        axes[6].set_title(""Reassign Points (3)"")\n        labels = km.labels_\n        discrete_scatter(X[:, 0], X[:, 1], labels, markers=[\'o\'],\n                         ax=axes[6])\n        markers = discrete_scatter(centers[:, 0], centers[:, 1], [0, 1, 2],\n                                   ax=axes[6], markers=[\'^\'],\n                                   markeredgewidth=2)\n\n        axes[7].set_title(""Recompute Centers (3)"")\n        km = KMeans(n_clusters=3, init=init, max_iter=3, n_init=1).fit(X)\n        centers = km.cluster_centers_\n        discrete_scatter(X[:, 0], X[:, 1], labels, markers=[\'o\'],\n                         ax=axes[7])\n        discrete_scatter(centers[:, 0], centers[:, 1], [0, 1, 2],\n                         ax=axes[7], markers=[\'^\'], markeredgewidth=2)\n        axes[8].set_axis_off()\n        axes[8].legend(markers, [""Cluster 0"", ""Cluster 1"", ""Cluster 2""], loc=\'best\')\n\n\ndef plot_kmeans_boundaries():\n    X, y = make_blobs(random_state=1)\n    init = X[:3, :]\n    km = KMeans(n_clusters=3, init=init, max_iter=2, n_init=1).fit(X)\n    discrete_scatter(X[:, 0], X[:, 1], km.labels_, markers=[\'o\'])\n    discrete_scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n                     [0, 1, 2], markers=[\'^\'], markeredgewidth=2)\n    plot_2d_classification(km, X, cm=cm3, alpha=.4)\n\n\ndef plot_kmeans_faces(km, pca, X_pca, X_people, y_people, target_names):\n    n_clusters = 10\n    image_shape = (87, 65)\n    fig, axes = plt.subplots(n_clusters, 11, subplot_kw={\'xticks\': (), \'yticks\': ()},\n                             figsize=(10, 15), gridspec_kw={""hspace"": .3})\n\n    for cluster in range(n_clusters):\n        center = km.cluster_centers_[cluster]\n        mask = km.labels_ == cluster\n        dists = np.sum((X_pca - center) ** 2, axis=1)\n        dists[~mask] = np.inf\n        inds = np.argsort(dists)[:5]\n        dists[~mask] = -np.inf\n        inds = np.r_[inds, np.argsort(dists)[-5:]]\n        axes[cluster, 0].imshow(pca.inverse_transform(center).reshape(image_shape), vmin=0, vmax=1)\n        for image, label, asdf, ax in zip(X_people[inds], y_people[inds],\n                                          km.labels_[inds], axes[cluster, 1:]):\n            ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n            ax.set_title(""%s"" % (target_names[label].split()[-1]), fontdict={\'fontsize\': 9})\n\n    # add some boxes to illustrate which are similar and which dissimilar\n    rec = plt.Rectangle([-5, -30], 73, 1295, fill=False, lw=2)\n    rec = axes[0, 0].add_patch(rec)\n    rec.set_clip_on(False)\n    axes[0, 0].text(0, -40, ""Center"")\n\n    rec = plt.Rectangle([-5, -30], 385, 1295, fill=False, lw=2)\n    rec = axes[0, 1].add_patch(rec)\n    rec.set_clip_on(False)\n    axes[0, 1].text(0, -40, ""Close to center"")\n\n    rec = plt.Rectangle([-5, -30], 385, 1295, fill=False, lw=2)\n    rec = axes[0, 6].add_patch(rec)\n    rec.set_clip_on(False)\n    axes[0, 6].text(0, -40, ""Far from center"")\n'"
Data Exploring/mglearn/plot_kneighbors_regularization.py,6,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\n\ndef plot_regression_datasets():\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    for n_samples, ax in zip([10, 100, 1000], axes):\n        x, y = make_dataset(n_samples)\n        ax.plot(x, y, \'o\', alpha=.6)\n\n\ndef plot_kneighbors_regularization():\n    rnd = np.random.RandomState(42)\n    x = np.linspace(-3, 3, 100)\n    y_no_noise = np.sin(4 * x) + x\n    y = y_no_noise + rnd.normal(size=len(x))\n    X = x[:, np.newaxis]\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    x_test = np.linspace(-3, 3, 1000)\n\n    for n_neighbors, ax in zip([2, 5, 20], axes.ravel()):\n        kneighbor_regression = KNeighborsRegressor(n_neighbors=n_neighbors)\n        kneighbor_regression.fit(X, y)\n        ax.plot(x, y_no_noise, label=""true function"")\n        ax.plot(x, y, ""o"", label=""data"")\n        ax.plot(x_test, kneighbor_regression.predict(x_test[:, np.newaxis]),\n                label=""prediction"")\n        ax.legend()\n        ax.set_title(""n_neighbors = %d"" % n_neighbors)\n\nif __name__ == ""__main__"":\n    plot_kneighbors_regularization()\n    plt.show()\n'"
Data Exploring/mglearn/plot_knn_classification.py,2,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import euclidean_distances\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom .datasets import make_forge\nfrom .plot_helpers import discrete_scatter\n\n\ndef plot_knn_classification(n_neighbors=1):\n    X, y = make_forge()\n\n    X_test = np.array([[8.2, 3.66214339], [9.9, 3.2], [11.2, .5]])\n    dist = euclidean_distances(X, X_test)\n    closest = np.argsort(dist, axis=0)\n\n    for x, neighbors in zip(X_test, closest.T):\n        for neighbor in neighbors[:n_neighbors]:\n            plt.arrow(x[0], x[1], X[neighbor, 0] - x[0],\n                      X[neighbor, 1] - x[1], head_width=0, fc=\'k\', ec=\'k\')\n\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n    test_points = discrete_scatter(X_test[:, 0], X_test[:, 1], clf.predict(X_test), markers=""*"")\n    training_points = discrete_scatter(X[:, 0], X[:, 1], y)\n    plt.legend(training_points + test_points, [""training class 0"", ""training class 1"",\n                                               ""test pred 0"", ""test pred 1""])\n'"
Data Exploring/mglearn/plot_knn_regression.py,3,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import euclidean_distances\n\nfrom .datasets import make_wave\nfrom .plot_helpers import cm3\n\n\ndef plot_knn_regression(n_neighbors=1):\n    X, y = make_wave(n_samples=40)\n    X_test = np.array([[-1.5], [0.9], [1.5]])\n\n    dist = euclidean_distances(X, X_test)\n    closest = np.argsort(dist, axis=0)\n\n    plt.figure(figsize=(10, 6))\n\n    reg = KNeighborsRegressor(n_neighbors=n_neighbors).fit(X, y)\n    y_pred = reg.predict(X_test)\n\n    for x, y_, neighbors in zip(X_test, y_pred, closest.T):\n        for neighbor in neighbors[:n_neighbors]:\n                plt.arrow(x[0], y_, X[neighbor, 0] - x[0], y[neighbor] - y_,\n                          head_width=0, fc=\'k\', ec=\'k\')\n\n    train, = plt.plot(X, y, \'o\', c=cm3(0))\n    test, = plt.plot(X_test, -3 * np.ones(len(X_test)), \'*\', c=cm3(2),\n                     markersize=20)\n    pred, = plt.plot(X_test, y_pred, \'*\', c=cm3(0), markersize=20)\n    plt.vlines(X_test, -3.1, 3.1, linestyle=""--"")\n    plt.legend([train, test, pred],\n               [""training data/target"", ""test data"", ""test prediction""],\n               ncol=3, loc=(.1, 1.025))\n    plt.ylim(-3.1, 3.1)\n    plt.xlabel(""Feature"")\n    plt.ylabel(""Target"")\n'"
Data Exploring/mglearn/plot_linear_regression.py,1,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom .datasets import make_wave\nfrom .plot_helpers import cm2\n\n\ndef plot_linear_regression_wave():\n    X, y = make_wave(n_samples=60)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n    line = np.linspace(-3, 3, 100).reshape(-1, 1)\n\n    lr = LinearRegression().fit(X_train, y_train)\n    print(""w[0]: %f  b: %f"" % (lr.coef_[0], lr.intercept_))\n\n    plt.figure(figsize=(8, 8))\n    plt.plot(line, lr.predict(line))\n    plt.plot(X, y, \'o\', c=cm2(0))\n    ax = plt.gca()\n    ax.spines[\'left\'].set_position(\'center\')\n    ax.spines[\'right\'].set_color(\'none\')\n    ax.spines[\'bottom\'].set_position(\'center\')\n    ax.spines[\'top\'].set_color(\'none\')\n    ax.set_ylim(-3, 3)\n    #ax.set_xlabel(""Feature"")\n    #ax.set_ylabel(""Target"")\n    ax.legend([""model"", ""training data""], loc=""best"")\n    ax.grid(True)\n    ax.set_aspect(\'equal\')\n'"
Data Exploring/mglearn/plot_linear_svc_regularization.py,1,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nfrom sklearn.datasets import make_blobs\n\nfrom .plot_helpers import discrete_scatter\n\n\ndef plot_linear_svc_regularization():\n    X, y = make_blobs(centers=2, random_state=4, n_samples=30)\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\n    # a carefully hand-designed dataset lol\n    y[7] = 0\n    y[27] = 0\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n\n    for ax, C in zip(axes, [1e-2, 10, 1e3]):\n        discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n\n        svm = LinearSVC(C=C, tol=0.00001, dual=False).fit(X, y)\n        w = svm.coef_[0]\n        a = -w[0] / w[1]\n        xx = np.linspace(6, 13)\n        yy = a * xx - (svm.intercept_[0]) / w[1]\n        ax.plot(xx, yy, c=\'k\')\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.set_title(""C = %f"" % C)\n    axes[0].legend(loc=""best"")\n\nif __name__ == ""__main__"":\n    plot_linear_svc_regularization()\n    plt.show()\n'"
Data Exploring/mglearn/plot_metrics.py,11,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom .tools import plot_2d_separator, plot_2d_scores, cm, discrete_scatter\nfrom .plot_helpers import ReBl\n\n\ndef plot_confusion_matrix_illustration():\n    plt.figure(figsize=(8, 8))\n    confusion = np.array([[401, 2], [8, 39]])\n    plt.text(0.40, .7, confusion[0, 0], size=70, horizontalalignment=\'right\')\n    plt.text(0.40, .2, confusion[1, 0], size=70, horizontalalignment=\'right\')\n    plt.text(.90, .7, confusion[0, 1], size=70, horizontalalignment=\'right\')\n    plt.text(.90, 0.2, confusion[1, 1], size=70, horizontalalignment=\'right\')\n    plt.xticks([.25, .75], [""predicted \'not nine\'"", ""predicted \'nine\'""], size=20)\n    plt.yticks([.25, .75], [""true \'nine\'"", ""true \'not nine\'""], size=20)\n    plt.plot([.5, .5], [0, 1], \'--\', c=\'k\')\n    plt.plot([0, 1], [.5, .5], \'--\', c=\'k\')\n\n    plt.xlim(0, 1)\n    plt.ylim(0, 1)\n\n\ndef plot_binary_confusion_matrix():\n    plt.text(0.45, .6, ""TN"", size=100, horizontalalignment=\'right\')\n    plt.text(0.45, .1, ""FN"", size=100, horizontalalignment=\'right\')\n    plt.text(.95, .6, ""FP"", size=100, horizontalalignment=\'right\')\n    plt.text(.95, 0.1, ""TP"", size=100, horizontalalignment=\'right\')\n    plt.xticks([.25, .75], [""predicted negative"", ""predicted positive""], size=15)\n    plt.yticks([.25, .75], [""positive class"", ""negative class""], size=15)\n    plt.plot([.5, .5], [0, 1], \'--\', c=\'k\')\n    plt.plot([0, 1], [.5, .5], \'--\', c=\'k\')\n\n    plt.xlim(0, 1)\n    plt.ylim(0, 1)\n\n\ndef plot_decision_threshold():\n    from mglearn.datasets import make_blobs\n    from sklearn.svm import SVC\n    from sklearn.model_selection import train_test_split\n\n    X, y = make_blobs(n_samples=(400, 50), centers=2, cluster_std=[7.0, 2],\n                      random_state=22)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    fig, axes = plt.subplots(2, 3, figsize=(15, 8), subplot_kw={\'xticks\': (), \'yticks\': ()})\n    plt.suptitle(""decision_threshold"")\n    axes[0, 0].set_title(""training data"")\n    discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=axes[0, 0])\n\n    svc = SVC(gamma=.05).fit(X_train, y_train)\n    axes[0, 1].set_title(""decision with threshold 0"")\n    discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=axes[0, 1])\n    plot_2d_scores(svc, X_train, function=""decision_function"", alpha=.7,\n                   ax=axes[0, 1], cm=ReBl)\n    plot_2d_separator(svc, X_train, linewidth=3, ax=axes[0, 1])\n    axes[0, 2].set_title(""decision with threshold -0.8"")\n    discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=axes[0, 2])\n    plot_2d_separator(svc, X_train, linewidth=3, ax=axes[0, 2], threshold=-.8)\n    plot_2d_scores(svc, X_train, function=""decision_function"", alpha=.7,\n                   ax=axes[0, 2], cm=ReBl)\n\n    axes[1, 0].set_axis_off()\n\n    mask = np.abs(X_train[:, 1] - 7) < 5\n    bla = np.sum(mask)\n\n    line = np.linspace(X_train.min(), X_train.max(), 100)\n    axes[1, 1].set_title(""Cross-section with threshold 0"")\n    axes[1, 1].plot(line, svc.decision_function(np.c_[line, 10 * np.ones(100)]), c=\'k\')\n    dec = svc.decision_function(np.c_[line, 10 * np.ones(100)])\n    contour = (dec > 0).reshape(1, -1).repeat(10, axis=0)\n    axes[1, 1].contourf(line, np.linspace(-1.5, 1.5, 10), contour, alpha=0.4, cmap=cm)\n    discrete_scatter(X_train[mask, 0], np.zeros(bla), y_train[mask], ax=axes[1, 1])\n    axes[1, 1].set_xlim(X_train.min(), X_train.max())\n    axes[1, 1].set_ylim(-1.5, 1.5)\n    axes[1, 1].set_xticks(())\n    axes[1, 1].set_ylabel(""Decision value"")\n\n    contour2 = (dec > -.8).reshape(1, -1).repeat(10, axis=0)\n    axes[1, 2].set_title(""Cross-section with threshold -0.8"")\n    axes[1, 2].contourf(line, np.linspace(-1.5, 1.5, 10), contour2, alpha=0.4, cmap=cm)\n    discrete_scatter(X_train[mask, 0], np.zeros(bla), y_train[mask], alpha=.1, ax=axes[1, 2])\n    axes[1, 2].plot(line, svc.decision_function(np.c_[line, 10 * np.ones(100)]), c=\'k\')\n    axes[1, 2].set_xlim(X_train.min(), X_train.max())\n    axes[1, 2].set_ylim(-1.5, 1.5)\n    axes[1, 2].set_xticks(())\n    axes[1, 2].set_ylabel(""Decision value"")\n    axes[1, 0].legend([\'negative class\', \'positive class\'])\n'"
Data Exploring/mglearn/plot_nmf.py,3,"b'from sklearn.decomposition import NMF\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.externals.joblib import Memory\n\nmemory = Memory(cachedir=""cache"")\n\n\ndef plot_nmf_illustration():\n    rnd = np.random.RandomState(5)\n    X_ = rnd.normal(size=(300, 2))\n    # Add 8 to make sure every point lies in the positive part of the space\n    X_blob = np.dot(X_, rnd.normal(size=(2, 2))) + rnd.normal(size=2) + 8\n\n    nmf = NMF(random_state=0)\n    nmf.fit(X_blob)\n    X_nmf = nmf.transform(X_blob)\n\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n    axes[0].scatter(X_blob[:, 0], X_blob[:, 1], c=X_nmf[:, 0], linewidths=0,\n                    s=60, cmap=\'viridis\')\n    axes[0].set_xlabel(""feature 1"")\n    axes[0].set_ylabel(""feature 2"")\n    axes[0].set_xlim(0, 12)\n    axes[0].set_ylim(0, 12)\n    axes[0].arrow(0, 0, nmf.components_[0, 0], nmf.components_[0, 1], width=.1,\n                  head_width=.3, color=\'k\')\n    axes[0].arrow(0, 0, nmf.components_[1, 0], nmf.components_[1, 1], width=.1,\n                  head_width=.3, color=\'k\')\n    axes[0].set_aspect(\'equal\')\n    axes[0].set_title(""NMF with two components"")\n\n    # second plot\n    nmf = NMF(random_state=0, n_components=1)\n    nmf.fit(X_blob)\n\n    axes[1].scatter(X_blob[:, 0], X_blob[:, 1], c=X_nmf[:, 0], linewidths=0,\n                    s=60, cmap=\'viridis\')\n    axes[1].set_xlabel(""feature 1"")\n    axes[1].set_ylabel(""feature 2"")\n    axes[1].set_xlim(0, 12)\n    axes[1].set_ylim(0, 12)\n    axes[1].arrow(0, 0, nmf.components_[0, 0], nmf.components_[0, 1], width=.1,\n                  head_width=.3, color=\'k\')\n\n    axes[1].set_aspect(\'equal\')\n    axes[1].set_title(""NMF with one component"")\n\n\n@memory.cache\ndef nmf_faces(X_train, X_test):\n    # Build NMF models with 10, 50, 100 and 500 components\n    # this list will hold the back-transformd test-data\n    reduced_images = []\n    for n_components in [10, 50, 100, 500]:\n        # build the NMF model\n        nmf = NMF(n_components=n_components, random_state=0)\n        nmf.fit(X_train)\n        # transform the test data (afterwards has n_components many dimensions)\n        X_test_nmf = nmf.transform(X_test)\n        # back-transform the transformed test-data\n        # (afterwards it\'s in the original space again)\n        X_test_back = np.dot(X_test_nmf, nmf.components_)\n        reduced_images.append(X_test_back)\n    return reduced_images\n\n\ndef plot_nmf_faces(X_train, X_test, image_shape):\n    reduced_images = nmf_faces(X_train, X_test)\n\n    # plot the first three images in the test set:\n    fix, axes = plt.subplots(3, 5, figsize=(15, 12),\n                             subplot_kw={\'xticks\': (), \'yticks\': ()})\n    for i, ax in enumerate(axes):\n        # plot original image\n        ax[0].imshow(X_test[i].reshape(image_shape),\n                     vmin=0, vmax=1)\n        # plot the four back-transformed images\n        for a, X_test_back in zip(ax[1:], reduced_images):\n            a.imshow(X_test_back[i].reshape(image_shape), vmin=0, vmax=1)\n\n    # label the top row\n    axes[0, 0].set_title(""original image"")\n    for ax, n_components in zip(axes[0, 1:], [10, 50, 100, 500]):\n        ax.set_title(""%d components"" % n_components)\n'"
Data Exploring/mglearn/plot_nn_graphs.py,0,"b'\n\ndef plot_logistic_regression_graph():\n    import graphviz\n    lr_graph = graphviz.Digraph(node_attr={\'shape\': \'circle\', \'fixedsize\': \'True\'},\n                                graph_attr={\'rankdir\': \'LR\', \'splines\': \'line\'})\n    inputs = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_0"")\n    output = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_2"")\n\n    for i in range(4):\n        inputs.node(""x[%d]"" % i, labelloc=""c"")\n    inputs.body.append(\'label = ""inputs""\')\n    inputs.body.append(\'color = ""white""\')\n\n    lr_graph.subgraph(inputs)\n\n    output.body.append(\'label = ""output""\')\n    output.body.append(\'color = ""white""\')\n    output.node(""y"")\n\n    lr_graph.subgraph(output)\n\n    for i in range(4):\n        lr_graph.edge(""x[%d]"" % i, ""y"", label=""w[%d]"" % i)\n    return lr_graph\n\n\ndef plot_single_hidden_layer_graph():\n    import graphviz\n    nn_graph = graphviz.Digraph(node_attr={\'shape\': \'circle\', \'fixedsize\': \'True\'},\n                                graph_attr={\'rankdir\': \'LR\', \'splines\': \'line\'})\n\n    inputs = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_0"")\n    hidden = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_1"")\n    output = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_2"")\n\n    for i in range(4):\n        inputs.node(""x[%d]"" % i)\n\n    inputs.body.append(\'label = ""inputs""\')\n    inputs.body.append(\'color = ""white""\')\n\n    hidden.body.append(\'label = ""hidden layer""\')\n    hidden.body.append(\'color = ""white""\')\n\n    for i in range(3):\n        hidden.node(""h%d"" % i, label=""h[%d]"" % i)\n\n    output.node(""y"")\n    output.body.append(\'label = ""output""\')\n    output.body.append(\'color = ""white""\')\n\n    nn_graph.subgraph(inputs)\n    nn_graph.subgraph(hidden)\n    nn_graph.subgraph(output)\n\n    for i in range(4):\n        for j in range(3):\n            nn_graph.edge(""x[%d]"" % i, ""h%d"" % j)\n\n    for i in range(3):\n        nn_graph.edge(""h%d"" % i, ""y"")\n    return nn_graph\n\n\ndef plot_two_hidden_layer_graph():\n    import graphviz\n    nn_graph = graphviz.Digraph(node_attr={\'shape\': \'circle\', \'fixedsize\': \'True\'},\n                                graph_attr={\'rankdir\': \'LR\', \'splines\': \'line\'})\n\n    inputs = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_0"")\n    hidden = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_1"")\n    hidden2 = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_2"")\n\n    output = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_3"")\n\n    for i in range(4):\n        inputs.node(""x[%d]"" % i)\n\n    inputs.body.append(\'label = ""inputs""\')\n    inputs.body.append(\'color = ""white""\')\n\n    for i in range(3):\n        hidden.node(""h1[%d]"" % i)\n\n    for i in range(3):\n        hidden2.node(""h2[%d]"" % i)\n\n    hidden.body.append(\'label = ""hidden layer 1""\')\n    hidden.body.append(\'color = ""white""\')\n\n    hidden2.body.append(\'label = ""hidden layer 2""\')\n    hidden2.body.append(\'color = ""white""\')\n\n    output.node(""y"")\n    output.body.append(\'label = ""output""\')\n    output.body.append(\'color = ""white""\')\n\n    nn_graph.subgraph(inputs)\n    nn_graph.subgraph(hidden)\n    nn_graph.subgraph(hidden2)\n\n    nn_graph.subgraph(output)\n\n    for i in range(4):\n        for j in range(3):\n            nn_graph.edge(""x[%d]"" % i, ""h1[%d]"" % j, label="""")\n\n    for i in range(3):\n        for j in range(3):\n            nn_graph.edge(""h1[%d]"" % i, ""h2[%d]"" % j, label="""")\n\n    for i in range(3):\n        nn_graph.edge(""h2[%d]"" % i, ""y"", label="""")\n\n    return nn_graph\n'"
Data Exploring/mglearn/plot_pca.py,5,"b'from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.externals.joblib import Memory\n\nmemory = Memory(cachedir=""cache"")\n\n\ndef plot_pca_illustration():\n    rnd = np.random.RandomState(5)\n    X_ = rnd.normal(size=(300, 2))\n    X_blob = np.dot(X_, rnd.normal(size=(2, 2))) + rnd.normal(size=2)\n\n    pca = PCA()\n    pca.fit(X_blob)\n    X_pca = pca.transform(X_blob)\n\n    S = X_pca.std(axis=0)\n\n    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n    axes = axes.ravel()\n\n    axes[0].set_title(""Original data"")\n    axes[0].scatter(X_blob[:, 0], X_blob[:, 1], c=X_pca[:, 0], linewidths=0,\n                    s=60, cmap=\'viridis\')\n    axes[0].set_xlabel(""feature 1"")\n    axes[0].set_ylabel(""feature 2"")\n    axes[0].arrow(pca.mean_[0], pca.mean_[1], S[0] * pca.components_[0, 0],\n                  S[0] * pca.components_[0, 1], width=.1, head_width=.3,\n                  color=\'k\')\n    axes[0].arrow(pca.mean_[0], pca.mean_[1], S[1] * pca.components_[1, 0],\n                  S[1] * pca.components_[1, 1], width=.1, head_width=.3,\n                  color=\'k\')\n    axes[0].text(-1.5, -.5, ""Component 2"", size=14)\n    axes[0].text(-4, -4, ""Component 1"", size=14)\n    axes[0].set_aspect(\'equal\')\n\n    axes[1].set_title(""Transformed data"")\n    axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=X_pca[:, 0], linewidths=0,\n                    s=60, cmap=\'viridis\')\n    axes[1].set_xlabel(""First principal component"")\n    axes[1].set_ylabel(""Second principal component"")\n    axes[1].set_aspect(\'equal\')\n    axes[1].set_ylim(-8, 8)\n\n    pca = PCA(n_components=1)\n    pca.fit(X_blob)\n    X_inverse = pca.inverse_transform(pca.transform(X_blob))\n\n    axes[2].set_title(""Transformed data w/ second component dropped"")\n    axes[2].scatter(X_pca[:, 0], np.zeros(X_pca.shape[0]), c=X_pca[:, 0],\n                    linewidths=0, s=60, cmap=\'viridis\')\n    axes[2].set_xlabel(""First principal component"")\n    axes[2].set_aspect(\'equal\')\n    axes[2].set_ylim(-8, 8)\n\n    axes[3].set_title(""Back-rotation using only first component"")\n    axes[3].scatter(X_inverse[:, 0], X_inverse[:, 1], c=X_pca[:, 0],\n                    linewidths=0, s=60, cmap=\'viridis\')\n    axes[3].set_xlabel(""feature 1"")\n    axes[3].set_ylabel(""feature 2"")\n    axes[3].set_aspect(\'equal\')\n    axes[3].set_xlim(-8, 4)\n    axes[3].set_ylim(-8, 4)\n\n\ndef plot_pca_whitening():\n    rnd = np.random.RandomState(5)\n    X_ = rnd.normal(size=(300, 2))\n    X_blob = np.dot(X_, rnd.normal(size=(2, 2))) + rnd.normal(size=2)\n\n    pca = PCA(whiten=True)\n    pca.fit(X_blob)\n    X_pca = pca.transform(X_blob)\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 10))\n    axes = axes.ravel()\n\n    axes[0].set_title(""Original data"")\n    axes[0].scatter(X_blob[:, 0], X_blob[:, 1], c=X_pca[:, 0], linewidths=0, s=60, cmap=\'viridis\')\n    axes[0].set_xlabel(""feature 1"")\n    axes[0].set_ylabel(""feature 2"")\n    axes[0].set_aspect(\'equal\')\n\n    axes[1].set_title(""Whitened data"")\n    axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=X_pca[:, 0], linewidths=0, s=60, cmap=\'viridis\')\n    axes[1].set_xlabel(""First principal component"")\n    axes[1].set_ylabel(""Second principal component"")\n    axes[1].set_aspect(\'equal\')\n    axes[1].set_xlim(-3, 4)\n\n\n@memory.cache\ndef pca_faces(X_train, X_test):\n    # copy and pasted from nmf. refactor?\n    # Build NMF models with 10, 50, 100, 500 components\n    # this list will hold the back-transformd test-data\n    reduced_images = []\n    for n_components in [10, 50, 100, 500]:\n        # build the NMF model\n        pca = PCA(n_components=n_components)\n        pca.fit(X_train)\n        # transform the test data (afterwards has n_components many dimensions)\n        X_test_pca = pca.transform(X_test)\n        # back-transform the transformed test-data\n        # (afterwards it\'s in the original space again)\n        X_test_back = pca.inverse_transform(X_test_pca)\n        reduced_images.append(X_test_back)\n    return reduced_images\n\n\ndef plot_pca_faces(X_train, X_test, image_shape):\n    reduced_images = pca_faces(X_train, X_test)\n\n    # plot the first three images in the test set:\n    fix, axes = plt.subplots(3, 5, figsize=(15, 12),\n                             subplot_kw={\'xticks\': (), \'yticks\': ()})\n    for i, ax in enumerate(axes):\n        # plot original image\n        ax[0].imshow(X_test[i].reshape(image_shape),\n                     vmin=0, vmax=1)\n        # plot the four back-transformed images\n        for a, X_test_back in zip(ax[1:], reduced_images):\n            a.imshow(X_test_back[i].reshape(image_shape), vmin=0, vmax=1)\n\n    # label the top row\n    axes[0, 0].set_title(""original image"")\n    for ax, n_components in zip(axes[0, 1:], [10, 50, 100, 500]):\n        ax.set_title(""%d components"" % n_components)\n'"
Data Exploring/mglearn/plot_rbf_svm_parameters.py,0,"b'import matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom .plot_2d_separator import plot_2d_separator\nfrom .tools import make_handcrafted_dataset\nfrom .plot_helpers import discrete_scatter\n\n\ndef plot_svm(log_C, log_gamma, ax=None):\n    X, y = make_handcrafted_dataset()\n    C = 10. ** log_C\n    gamma = 10. ** log_gamma\n    svm = SVC(kernel=\'rbf\', C=C, gamma=gamma).fit(X, y)\n    if ax is None:\n        ax = plt.gca()\n    plot_2d_separator(svm, X, ax=ax, eps=.5)\n    # plot data\n    discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    # plot support vectors\n    sv = svm.support_vectors_\n    # class labels of support vectors are given by the sign of the dual coefficients\n    sv_labels = svm.dual_coef_.ravel() > 0\n    discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3, ax=ax)\n    ax.set_title(""C = %.4f gamma = %.4f"" % (C, gamma))\n\n\ndef plot_svm_interactive():\n    from IPython.html.widgets import interactive, FloatSlider\n    C_slider = FloatSlider(min=-3, max=3, step=.1, value=0, readout=False)\n    gamma_slider = FloatSlider(min=-2, max=2, step=.1, value=0, readout=False)\n    return interactive(plot_svm, log_C=C_slider, log_gamma=gamma_slider)\n'"
Data Exploring/mglearn/plot_ridge.py,1,"b'import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.model_selection import learning_curve, KFold\n\nfrom .datasets import load_extended_boston\n\n\ndef plot_learning_curve(est, X, y):\n    training_set_size, train_scores, test_scores = learning_curve(\n        est, X, y, train_sizes=np.linspace(.1, 1, 20), cv=KFold(20, shuffle=True, random_state=1))\n    estimator_name = est.__class__.__name__\n    line = plt.plot(training_set_size, train_scores.mean(axis=1), \'--\',\n                    label=""training "" + estimator_name)\n    plt.plot(training_set_size, test_scores.mean(axis=1), \'-\',\n             label=""test "" + estimator_name, c=line[0].get_color())\n    plt.xlabel(\'Training set size\')\n    plt.ylabel(\'Score (R^2)\')\n    plt.ylim(0, 1.1)\n\n\ndef plot_ridge_n_samples():\n    X, y = load_extended_boston()\n\n    plot_learning_curve(Ridge(alpha=1), X, y)\n    plot_learning_curve(LinearRegression(), X, y)\n    plt.legend(loc=(0, 1.05), ncol=2, fontsize=11)\n'"
Data Exploring/mglearn/plot_scaling.py,2,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import (StandardScaler, MinMaxScaler, Normalizer,\n                                   RobustScaler)\nfrom .plot_helpers import cm2\n\n\ndef plot_scaling():\n    X, y = make_blobs(n_samples=50, centers=2, random_state=4, cluster_std=1)\n    X += 3\n\n    plt.figure(figsize=(15, 8))\n    main_ax = plt.subplot2grid((2, 4), (0, 0), rowspan=2, colspan=2)\n\n    main_ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm2, s=60)\n    maxx = np.abs(X[:, 0]).max()\n    maxy = np.abs(X[:, 1]).max()\n\n    main_ax.set_xlim(-maxx + 1, maxx + 1)\n    main_ax.set_ylim(-maxy + 1, maxy + 1)\n    main_ax.set_title(""Original Data"")\n    other_axes = [plt.subplot2grid((2, 4), (i, j))\n                  for j in range(2, 4) for i in range(2)]\n\n    for ax, scaler in zip(other_axes, [StandardScaler(), RobustScaler(),\n                                       MinMaxScaler(), Normalizer(norm=\'l2\')]):\n        X_ = scaler.fit_transform(X)\n        ax.scatter(X_[:, 0], X_[:, 1], c=y, cmap=cm2, s=60)\n        ax.set_xlim(-2, 2)\n        ax.set_ylim(-2, 2)\n        ax.set_title(type(scaler).__name__)\n\n    other_axes.append(main_ax)\n\n    for ax in other_axes:\n        ax.spines[\'left\'].set_position(\'center\')\n        ax.spines[\'right\'].set_color(\'none\')\n        ax.spines[\'bottom\'].set_position(\'center\')\n        ax.spines[\'top\'].set_color(\'none\')\n        ax.xaxis.set_ticks_position(\'bottom\')\n        ax.yaxis.set_ticks_position(\'left\')\n'"
Data Exploring/mglearn/plot_tree_nonmonotonous.py,0,"b'import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom .tools import discrete_scatter\nfrom .plot_2d_separator import plot_2d_separator\n\n\ndef plot_tree_not_monotone():\n    import graphviz\n    # make a simple 2d dataset\n    X, y = make_blobs(centers=4, random_state=8)\n    y = y % 2\n    plt.figure()\n    discrete_scatter(X[:, 0], X[:, 1], y)\n    plt.legend([""Class 0"", ""Class 1""], loc=""best"")\n\n    # learn a decision tree model\n    tree = DecisionTreeClassifier(random_state=0).fit(X, y)\n    plot_2d_separator(tree, X, linestyle=""dashed"")\n\n    # visualize the tree\n    export_graphviz(tree, out_file=""mytree.dot"", impurity=False, filled=True)\n    with open(""mytree.dot"") as f:\n        dot_graph = f.read()\n    print(""Feature importances: %s"" % tree.feature_importances_)\n    return graphviz.Source(dot_graph)\n'"
Data Exploring/mglearn/plots.py,0,"b'from .plot_linear_svc_regularization import plot_linear_svc_regularization\nfrom .plot_interactive_tree import plot_tree_progressive, plot_tree_partition\nfrom .plot_animal_tree import plot_animal_tree\nfrom .plot_rbf_svm_parameters import plot_svm\nfrom .plot_knn_regression import plot_knn_regression\nfrom .plot_knn_classification import plot_knn_classification\nfrom .plot_2d_separator import plot_2d_classification, plot_2d_separator\nfrom .plot_nn_graphs import (plot_logistic_regression_graph,\n                             plot_single_hidden_layer_graph,\n                             plot_two_hidden_layer_graph)\nfrom .plot_linear_regression import plot_linear_regression_wave\nfrom .plot_tree_nonmonotonous import plot_tree_not_monotone\nfrom .plot_scaling import plot_scaling\nfrom .plot_pca import plot_pca_illustration, plot_pca_whitening, plot_pca_faces\nfrom .plot_decomposition import plot_decomposition\nfrom .plot_nmf import plot_nmf_illustration, plot_nmf_faces\nfrom .plot_helpers import cm2, cm3\nfrom .plot_agglomerative import plot_agglomerative, plot_agglomerative_algorithm\nfrom .plot_kmeans import plot_kmeans_algorithm, plot_kmeans_boundaries, plot_kmeans_faces\nfrom .plot_improper_preprocessing import plot_improper_processing, plot_proper_processing\nfrom .plot_cross_validation import (plot_threefold_split, plot_group_kfold,\n                                    plot_shuffle_split, plot_cross_validation,\n                                    plot_stratified_cross_validation)\n\nfrom .plot_grid_search import plot_grid_search_overview, plot_cross_val_selection\nfrom .plot_metrics import (plot_confusion_matrix_illustration,\n                           plot_binary_confusion_matrix,\n                           plot_decision_threshold)\nfrom .plot_dbscan import plot_dbscan\nfrom .plot_ridge import plot_ridge_n_samples\n\n__all__ = [\'plot_linear_svc_regularization\',\n           ""plot_animal_tree"", ""plot_tree_progressive"",\n           \'plot_tree_partition\', \'plot_svm\',\n           \'plot_knn_regression\',\n           \'plot_logistic_regression_graph\',\n           \'plot_single_hidden_layer_graph\',\n           \'plot_two_hidden_layer_graph\',\n           \'plot_2d_classification\',\n           \'plot_2d_separator\',\n           \'plot_knn_classification\',\n           \'plot_linear_regression_wave\',\n           \'plot_tree_not_monotone\',\n           \'plot_scaling\',\n           \'plot_pca_illustration\',\n           \'plot_pca_faces\',\n           \'plot_pca_whitening\',\n           \'plot_decomposition\',\n           \'plot_nmf_illustration\',\n           \'plot_nmf_faces\',\n           \'plot_agglomerative\',\n           \'plot_agglomerative_algorithm\',\n           \'plot_kmeans_boundaries\',\n           \'plot_kmeans_algorithm\',\n           \'plot_kmeans_faces\',\n           \'cm3\', \'cm2\', \'plot_improper_processing\', \'plot_proper_processing\',\n           \'plot_group_kfold\',\n           \'plot_shuffle_split\',\n           \'plot_stratified_cross_validation\',\n           \'plot_threefold_split\',\n           \'plot_cross_validation\',\n           \'plot_grid_search_overview\',\n           \'plot_cross_val_selection\',\n           \'plot_confusion_matrix_illustration\',\n           \'plot_binary_confusion_matrix\',\n           \'plot_decision_threshold\',\n           \'plot_dbscan\',\n           \'plot_ridge_n_samples\'\n           ]\n'"
Data Exploring/mglearn/tools.py,12,"b'import numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.tree import export_graphviz\nimport matplotlib.pyplot as plt\nfrom .plot_2d_separator import (plot_2d_separator, plot_2d_classification,\n                                plot_2d_scores)\nfrom .plot_helpers import cm2 as cm, discrete_scatter\n\n\ndef visualize_coefficients(coefficients, feature_names, n_top_features=25):\n    """"""Visualize coefficients of a linear model.\n\n    Parameters\n    ----------\n    coefficients : nd-array, shape (n_features,)\n        Model coefficients.\n\n    feature_names : list or nd-array of strings, shape (n_features,)\n        Feature names for labeling the coefficients.\n\n    n_top_features : int, default=25\n        How many features to show. The function will show the largest (most\n        positive) and smallest (most negative)  n_top_features coefficients,\n        for a total of 2 * n_top_features coefficients.\n    """"""\n    coefficients = coefficients.squeeze()\n    if coefficients.ndim > 1:\n        # this is not a row or column vector\n        raise ValueError(""coeffients must be 1d array or column vector, got""\n                         "" shape {}"".format(coefficients.shape))\n    coefficients = coefficients.ravel()\n\n    if len(coefficients) != len(feature_names):\n        raise ValueError(""Number of coefficients {} doesn\'t match number of""\n                         ""feature names {}."".format(len(coefficients),\n                                                    len(feature_names)))\n    # get coefficients with large absolute values\n    coef = coefficients.ravel()\n    positive_coefficients = np.argsort(coef)[-n_top_features:]\n    negative_coefficients = np.argsort(coef)[:n_top_features]\n    interesting_coefficients = np.hstack([negative_coefficients,\n                                          positive_coefficients])\n    # plot them\n    plt.figure(figsize=(15, 5))\n    colors = [cm(1) if c < 0 else cm(0)\n              for c in coef[interesting_coefficients]]\n    plt.bar(np.arange(2 * n_top_features), coef[interesting_coefficients],\n            color=colors)\n    feature_names = np.array(feature_names)\n    plt.subplots_adjust(bottom=0.3)\n    plt.xticks(np.arange(1, 1 + 2 * n_top_features),\n               feature_names[interesting_coefficients], rotation=60,\n               ha=""right"")\n    plt.ylabel(""Coefficient magnitude"")\n    plt.xlabel(""Feature"")\n\n\ndef heatmap(values, xlabel, ylabel, xticklabels, yticklabels, cmap=None,\n            vmin=None, vmax=None, ax=None, fmt=""%0.2f""):\n    if ax is None:\n        ax = plt.gca()\n    # plot the mean cross-validation scores\n    img = ax.pcolor(values, cmap=cmap, vmin=vmin, vmax=vmax)\n    img.update_scalarmappable()\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_xticks(np.arange(len(xticklabels)) + .5)\n    ax.set_yticks(np.arange(len(yticklabels)) + .5)\n    ax.set_xticklabels(xticklabels)\n    ax.set_yticklabels(yticklabels)\n    ax.set_aspect(1)\n\n    for p, color, value in zip(img.get_paths(), img.get_facecolors(),\n                               img.get_array()):\n        x, y = p.vertices[:-2, :].mean(0)\n        if np.mean(color[:3]) > 0.5:\n            c = \'k\'\n        else:\n            c = \'w\'\n        ax.text(x, y, fmt % value, color=c, ha=""center"", va=""center"")\n    return img\n\n\ndef make_handcrafted_dataset():\n    # a carefully hand-designed dataset lol\n    X, y = make_blobs(centers=2, random_state=4, n_samples=30)\n    y[np.array([7, 27])] = 0\n    mask = np.ones(len(X), dtype=np.bool)\n    mask[np.array([0, 1, 5, 26])] = 0\n    X, y = X[mask], y[mask]\n    return X, y\n\n\ndef print_topics(topics, feature_names, sorting, topics_per_chunk=6,\n                 n_words=20):\n    for i in range(0, len(topics), topics_per_chunk):\n        # for each chunk:\n        these_topics = topics[i: i + topics_per_chunk]\n        # maybe we have less than topics_per_chunk left\n        len_this_chunk = len(these_topics)\n        # print topic headers\n        print((""topic {:<8}"" * len_this_chunk).format(*these_topics))\n        print((""-------- {0:<5}"" * len_this_chunk).format(""""))\n        # print top n_words frequent words\n        for i in range(n_words):\n            try:\n                print((""{:<14}"" * len_this_chunk).format(\n                    *feature_names[sorting[these_topics, i]]))\n            except:\n                pass\n        print(""\\n"")\n\n\ndef get_tree(tree, **kwargs):\n    try:\n        # python3\n        from io import StringIO\n    except ImportError:\n        # python2\n        from StringIO import StringIO\n    f = StringIO()\n    export_graphviz(tree, f, **kwargs)\n    import graphviz\n    return graphviz.Source(f.getvalue())\n\n__all__ = [\'plot_2d_separator\', \'plot_2d_classification\', \'plot_2d_scores\',\n           \'cm\', \'visualize_coefficients\', \'print_topics\', \'heatmap\',\n           \'discrete_scatter\']\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/__init__.py,0,"b""from . import plots\nfrom . import tools\nfrom .plots import cm3, cm2\nfrom .tools import discrete_scatter\nfrom .plot_helpers import ReBl\n\n__all__ = ['tools', 'plots', 'cm3', 'cm2', 'discrete_scatter', 'ReBl']\n"""
Data Exploring/Data Exploring 12-25-17/mglearn/datasets.py,11,"b'import numpy as np\nimport pandas as pd\nimport os\nfrom scipy import signal\nfrom sklearn.datasets import load_boston\nfrom sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\nfrom .make_blobs import make_blobs\n\nDATA_PATH = os.path.join(os.path.dirname(__file__), "".."", ""data"")\n\n\ndef make_forge():\n    # a carefully hand-designed dataset lol\n    X, y = make_blobs(centers=2, random_state=4, n_samples=30)\n    y[np.array([7, 27])] = 0\n    mask = np.ones(len(X), dtype=np.bool)\n    mask[np.array([0, 1, 5, 26])] = 0\n    X, y = X[mask], y[mask]\n    return X, y\n\n\ndef make_wave(n_samples=100):\n    rnd = np.random.RandomState(42)\n    x = rnd.uniform(-3, 3, size=n_samples)\n    y_no_noise = (np.sin(4 * x) + x)\n    y = (y_no_noise + rnd.normal(size=len(x))) / 2\n    return x.reshape(-1, 1), y\n\n\ndef load_extended_boston():\n    boston = load_boston()\n    X = boston.data\n\n    X = MinMaxScaler().fit_transform(boston.data)\n    X = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X)\n    return X, boston.target\n\n\ndef load_citibike():\n    data_mine = pd.read_csv(os.path.join(DATA_PATH, ""citibike.csv""))\n    data_mine[\'one\'] = 1\n    data_mine[\'starttime\'] = pd.to_datetime(data_mine.starttime)\n    data_starttime = data_mine.set_index(""starttime"")\n    data_resampled = data_starttime.resample(""3h"").sum().fillna(0)\n    return data_resampled.one\n\n\ndef make_signals():\n    # fix a random state seed\n    rng = np.random.RandomState(42)\n    n_samples = 2000\n    time = np.linspace(0, 8, n_samples)\n    # create three signals\n    s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\n    s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\n    s3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal\n\n    # concatenate the signals, add noise\n    S = np.c_[s1, s2, s3]\n    S += 0.2 * rng.normal(size=S.shape)\n\n    S /= S.std(axis=0)  # Standardize data\n    S -= S.min()\n    return S\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/make_blobs.py,4,"b'import numbers\nimport numpy as np\n\nfrom sklearn.utils import check_array, check_random_state\nfrom sklearn.utils import shuffle as shuffle_\n\n\ndef make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=1.0,\n               center_box=(-10.0, 10.0), shuffle=True, random_state=None):\n    """"""Generate isotropic Gaussian blobs for clustering.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, or tuple, optional (default=100)\n        The total number of points equally divided among clusters.\n\n    n_features : int, optional (default=2)\n        The number of features for each sample.\n\n    centers : int or array of shape [n_centers, n_features], optional\n        (default=3)\n        The number of centers to generate, or the fixed center locations.\n\n    cluster_std: float or sequence of floats, optional (default=1.0)\n        The standard deviation of the clusters.\n\n    center_box: pair of floats (min, max), optional (default=(-10.0, 10.0))\n        The bounding box for each cluster center when centers are\n        generated at random.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The generated samples.\n\n    y : array of shape [n_samples]\n        The integer labels for cluster membership of each sample.\n\n    Examples\n    --------\n    >>> from sklearn.datasets.samples_generator import make_blobs\n    >>> X, y = make_blobs(n_samples=10, centers=3, n_features=2,\n    ...                   random_state=0)\n    >>> print(X.shape)\n    (10, 2)\n    >>> y\n    array([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])\n\n    See also\n    --------\n    make_classification: a more intricate variant\n    """"""\n    generator = check_random_state(random_state)\n\n    if isinstance(centers, numbers.Integral):\n        centers = generator.uniform(center_box[0], center_box[1],\n                                    size=(centers, n_features))\n    else:\n        centers = check_array(centers)\n        n_features = centers.shape[1]\n\n    if isinstance(cluster_std, numbers.Real):\n        cluster_std = np.ones(len(centers)) * cluster_std\n\n    X = []\n    y = []\n\n    n_centers = centers.shape[0]\n    if isinstance(n_samples, numbers.Integral):\n        n_samples_per_center = [int(n_samples // n_centers)] * n_centers\n        for i in range(n_samples % n_centers):\n            n_samples_per_center[i] += 1\n    else:\n        n_samples_per_center = n_samples\n\n    for i, (n, std) in enumerate(zip(n_samples_per_center, cluster_std)):\n        X.append(centers[i] + generator.normal(scale=std,\n                                               size=(n, n_features)))\n        y += [i] * n\n\n    X = np.concatenate(X)\n    y = np.array(y)\n\n    if shuffle:\n        X, y = shuffle_(X, y, random_state=generator)\n\n    return X, y\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_2d_separator.py,12,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom .plot_helpers import cm2, cm3, discrete_scatter\n\n\ndef plot_2d_classification(classifier, X, fill=False, ax=None, eps=None,\n                           alpha=1, cm=cm3):\n    # multiclass\n    if eps is None:\n        eps = X.std() / 2.\n\n    if ax is None:\n        ax = plt.gca()\n\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n    xx = np.linspace(x_min, x_max, 1000)\n    yy = np.linspace(y_min, y_max, 1000)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    X_grid = np.c_[X1.ravel(), X2.ravel()]\n    decision_values = classifier.predict(X_grid)\n    ax.imshow(decision_values.reshape(X1.shape), extent=(x_min, x_max,\n                                                         y_min, y_max),\n              aspect=\'auto\', origin=\'lower\', alpha=alpha, cmap=cm)\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n\ndef plot_2d_scores(classifier, X, ax=None, eps=None, alpha=1, cm=""viridis"",\n                   function=None):\n    # binary with fill\n    if eps is None:\n        eps = X.std() / 2.\n\n    if ax is None:\n        ax = plt.gca()\n\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n    xx = np.linspace(x_min, x_max, 100)\n    yy = np.linspace(y_min, y_max, 100)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    X_grid = np.c_[X1.ravel(), X2.ravel()]\n    if function is None:\n        function = getattr(classifier, ""decision_function"",\n                           getattr(classifier, ""predict_proba""))\n    else:\n        function = getattr(classifier, function)\n    decision_values = function(X_grid)\n    if decision_values.ndim > 1 and decision_values.shape[1] > 1:\n        # predict_proba\n        decision_values = decision_values[:, 1]\n    grr = ax.imshow(decision_values.reshape(X1.shape),\n                    extent=(x_min, x_max, y_min, y_max), aspect=\'auto\',\n                    origin=\'lower\', alpha=alpha, cmap=cm)\n\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n    return grr\n\n\ndef plot_2d_separator(classifier, X, fill=False, ax=None, eps=None, alpha=1,\n                      cm=cm2, linewidth=None, threshold=None,\n                      linestyle=""solid""):\n    # binary?\n    if eps is None:\n        eps = X.std() / 2.\n\n    if ax is None:\n        ax = plt.gca()\n\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n    xx = np.linspace(x_min, x_max, 1000)\n    yy = np.linspace(y_min, y_max, 1000)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    X_grid = np.c_[X1.ravel(), X2.ravel()]\n    try:\n        decision_values = classifier.decision_function(X_grid)\n        levels = [0] if threshold is None else [threshold]\n        fill_levels = [decision_values.min()] + levels + [\n            decision_values.max()]\n    except AttributeError:\n        # no decision_function\n        decision_values = classifier.predict_proba(X_grid)[:, 1]\n        levels = [.5] if threshold is None else [threshold]\n        fill_levels = [0] + levels + [1]\n    if fill:\n        ax.contourf(X1, X2, decision_values.reshape(X1.shape),\n                    levels=fill_levels, alpha=alpha, cmap=cm)\n    else:\n        ax.contour(X1, X2, decision_values.reshape(X1.shape), levels=levels,\n                   colors=""black"", alpha=alpha, linewidths=linewidth,\n                   linestyles=linestyle, zorder=5)\n\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n\nif __name__ == \'__main__\':\n    from sklearn.datasets import make_blobs\n    from sklearn.linear_model import LogisticRegression\n    X, y = make_blobs(centers=2, random_state=42)\n    clf = LogisticRegression().fit(X, y)\n    plot_2d_separator(clf, X, fill=True)\n    discrete_scatter(X[:, 0], X[:, 1], y)\n    plt.show()\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_agglomerative.py,10,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.neighbors import KernelDensity\n\n\ndef plot_agglomerative_algorithm():\n    # generate synthetic two-dimensional data\n    X, y = make_blobs(random_state=0, n_samples=12)\n\n    agg = AgglomerativeClustering(n_clusters=X.shape[0], compute_full_tree=True).fit(X)\n\n    fig, axes = plt.subplots(X.shape[0] // 5, 5, subplot_kw={\'xticks\': (),\n                                                             \'yticks\': ()},\n                             figsize=(20, 8))\n\n    eps = X.std() / 2\n\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n    gridpoints = np.c_[xx.ravel().reshape(-1, 1), yy.ravel().reshape(-1, 1)]\n\n    for i, ax in enumerate(axes.ravel()):\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n        agg.n_clusters = X.shape[0] - i\n        agg.fit(X)\n        ax.set_title(""Step %d"" % i)\n        ax.scatter(X[:, 0], X[:, 1], s=60, c=\'grey\')\n        bins = np.bincount(agg.labels_)\n        for cluster in range(agg.n_clusters):\n            if bins[cluster] > 1:\n                points = X[agg.labels_ == cluster]\n                other_points = X[agg.labels_ != cluster]\n\n                kde = KernelDensity(bandwidth=.5).fit(points)\n                scores = kde.score_samples(gridpoints)\n                score_inside = np.min(kde.score_samples(points))\n                score_outside = np.max(kde.score_samples(other_points))\n                levels = .8 * score_inside + .2 * score_outside\n                ax.contour(xx, yy, scores.reshape(100, 100), levels=[levels],\n                           colors=\'k\', linestyles=\'solid\', linewidths=2)\n\n    axes[0, 0].set_title(""Initialization"")\n\n\ndef plot_agglomerative():\n    X, y = make_blobs(random_state=0, n_samples=12)\n    agg = AgglomerativeClustering(n_clusters=3)\n\n    eps = X.std() / 2.\n\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n    gridpoints = np.c_[xx.ravel().reshape(-1, 1), yy.ravel().reshape(-1, 1)]\n\n    ax = plt.gca()\n    for i, x in enumerate(X):\n        ax.text(x[0] + .1, x[1], ""%d"" % i, horizontalalignment=\'left\', verticalalignment=\'center\')\n\n    ax.scatter(X[:, 0], X[:, 1], s=60, c=\'grey\')\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n    for i in range(11):\n        agg.n_clusters = X.shape[0] - i\n        agg.fit(X)\n\n        bins = np.bincount(agg.labels_)\n        for cluster in range(agg.n_clusters):\n            if bins[cluster] > 1:\n                points = X[agg.labels_ == cluster]\n                other_points = X[agg.labels_ != cluster]\n\n                kde = KernelDensity(bandwidth=.5).fit(points)\n                scores = kde.score_samples(gridpoints)\n                score_inside = np.min(kde.score_samples(points))\n                score_outside = np.max(kde.score_samples(other_points))\n                levels = .8 * score_inside + .2 * score_outside\n                ax.contour(xx, yy, scores.reshape(100, 100), levels=[levels],\n                           colors=\'k\', linestyles=\'solid\', linewidths=1)\n\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_animal_tree.py,0,"b'from scipy.misc import imread\nimport matplotlib.pyplot as plt\n\n\ndef plot_animal_tree(ax=None):\n    import graphviz\n    if ax is None:\n        ax = plt.gca()\n    mygraph = graphviz.Digraph(node_attr={\'shape\': \'box\'},\n                               edge_attr={\'labeldistance\': ""10.5""},\n                               format=""png"")\n    mygraph.node(""0"", ""Has feathers?"")\n    mygraph.node(""1"", ""Can fly?"")\n    mygraph.node(""2"", ""Has fins?"")\n    mygraph.node(""3"", ""Hawk"")\n    mygraph.node(""4"", ""Penguin"")\n    mygraph.node(""5"", ""Dolphin"")\n    mygraph.node(""6"", ""Bear"")\n    mygraph.edge(""0"", ""1"", label=""True"")\n    mygraph.edge(""0"", ""2"", label=""False"")\n    mygraph.edge(""1"", ""3"", label=""True"")\n    mygraph.edge(""1"", ""4"", label=""False"")\n    mygraph.edge(""2"", ""5"", label=""True"")\n    mygraph.edge(""2"", ""6"", label=""False"")\n    mygraph.render(""tmp"")\n    ax.imshow(imread(""tmp.png""))\n    ax.set_axis_off()\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_cross_validation.py,22,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef plot_group_kfold():\n    from sklearn.model_selection import GroupKFold\n    groups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]\n\n    plt.figure(figsize=(10, 2))\n    plt.title(""GroupKFold"")\n\n    axes = plt.gca()\n    axes.set_frame_on(False)\n\n    n_folds = 12\n    n_samples = 12\n    n_iter = 3\n    n_samples_per_fold = 1\n\n    cv = GroupKFold(n_splits=3)\n    mask = np.zeros((n_iter, n_samples))\n    for i, (train, test) in enumerate(cv.split(range(12), groups=groups)):\n        mask[i, train] = 1\n        mask[i, test] = 2\n\n    for i in range(n_folds):\n        # test is grey\n        colors = [""grey"" if x == 2 else ""white"" for x in mask[:, i]]\n        # not selected has no hatch\n\n        boxes = axes.barh(y=range(n_iter), width=[1 - 0.1] * n_iter,\n                          left=i * n_samples_per_fold, height=.6, color=colors,\n                          hatch=""//"", edgecolor=""k"", align=\'edge\')\n        for j in np.where(mask[:, i] == 0)[0]:\n            boxes[j].set_hatch("""")\n\n    axes.barh(y=[n_iter] * n_folds, width=[1 - 0.1] * n_folds,\n              left=np.arange(n_folds) * n_samples_per_fold, height=.6,\n              color=""w"", edgecolor=\'k\', align=""edge"")\n\n    for i in range(12):\n        axes.text((i + .5) * n_samples_per_fold, 3.5, ""%d"" %\n                  groups[i], horizontalalignment=""center"")\n\n    axes.invert_yaxis()\n    axes.set_xlim(0, n_samples + 1)\n    axes.set_ylabel(""CV iterations"")\n    axes.set_xlabel(""Data points"")\n    axes.set_xticks(np.arange(n_samples) + .5)\n    axes.set_xticklabels(np.arange(1, n_samples + 1))\n    axes.set_yticks(np.arange(n_iter + 1) + .3)\n    axes.set_yticklabels(\n        [""Split %d"" % x for x in range(1, n_iter + 1)] + [""Group""])\n    plt.legend([boxes[0], boxes[1]], [""Training set"", ""Test set""], loc=(1, .3))\n    plt.tight_layout()\n\n\ndef plot_shuffle_split():\n    from sklearn.model_selection import ShuffleSplit\n    plt.figure(figsize=(10, 2))\n    plt.title(""ShuffleSplit with 10 points""\n              "", train_size=5, test_size=2, n_splits=4"")\n\n    axes = plt.gca()\n    axes.set_frame_on(False)\n\n    n_folds = 10\n    n_samples = 10\n    n_iter = 4\n    n_samples_per_fold = 1\n\n    ss = ShuffleSplit(n_splits=4, train_size=5, test_size=2, random_state=43)\n    mask = np.zeros((n_iter, n_samples))\n    for i, (train, test) in enumerate(ss.split(range(10))):\n        mask[i, train] = 1\n        mask[i, test] = 2\n\n    for i in range(n_folds):\n        # test is grey\n        colors = [""grey"" if x == 2 else ""white"" for x in mask[:, i]]\n        # not selected has no hatch\n\n        boxes = axes.barh(y=range(n_iter), width=[1 - 0.1] * n_iter,\n                          left=i * n_samples_per_fold, height=.6, color=colors,\n                          hatch=""//"", edgecolor=\'k\', align=\'edge\')\n        for j in np.where(mask[:, i] == 0)[0]:\n            boxes[j].set_hatch("""")\n\n    axes.invert_yaxis()\n    axes.set_xlim(0, n_samples + 1)\n    axes.set_ylabel(""CV iterations"")\n    axes.set_xlabel(""Data points"")\n    axes.set_xticks(np.arange(n_samples) + .5)\n    axes.set_xticklabels(np.arange(1, n_samples + 1))\n    axes.set_yticks(np.arange(n_iter) + .3)\n    axes.set_yticklabels([""Split %d"" % x for x in range(1, n_iter + 1)])\n    # legend hacked for this random state\n    plt.legend([boxes[1], boxes[0], boxes[2]], [\n               ""Training set"", ""Test set"", ""Not selected""], loc=(1, .3))\n    plt.tight_layout()\n\n\ndef plot_stratified_cross_validation():\n    fig, both_axes = plt.subplots(2, 1, figsize=(12, 5))\n    # plt.title(""cross_validation_not_stratified"")\n    axes = both_axes[0]\n    axes.set_title(""Standard cross-validation with sorted class labels"")\n\n    axes.set_frame_on(False)\n\n    n_folds = 3\n    n_samples = 150\n\n    n_samples_per_fold = n_samples / float(n_folds)\n\n    for i in range(n_folds):\n        colors = [""w""] * n_folds\n        colors[i] = ""grey""\n        axes.barh(y=range(n_folds), width=[n_samples_per_fold - 1] *\n                  n_folds, left=i * n_samples_per_fold, height=.6,\n                  color=colors, hatch=""//"", edgecolor=\'k\', align=\'edge\')\n\n    axes.barh(y=[n_folds] * n_folds, width=[n_samples_per_fold - 1] *\n              n_folds, left=np.arange(3) * n_samples_per_fold, height=.6,\n              color=""w"", edgecolor=\'k\', align=\'edge\')\n\n    axes.invert_yaxis()\n    axes.set_xlim(0, n_samples + 1)\n    axes.set_ylabel(""CV iterations"")\n    axes.set_xlabel(""Data points"")\n    axes.set_xticks(np.arange(n_samples_per_fold / 2.,\n                              n_samples, n_samples_per_fold))\n    axes.set_xticklabels([""Fold %d"" % x for x in range(1, n_folds + 1)])\n    axes.set_yticks(np.arange(n_folds + 1) + .3)\n    axes.set_yticklabels(\n        [""Split %d"" % x for x in range(1, n_folds + 1)] + [""Class label""])\n    for i in range(3):\n        axes.text((i + .5) * n_samples_per_fold, 3.5, ""Class %d"" %\n                  i, horizontalalignment=""center"")\n\n    ax = both_axes[1]\n    ax.set_title(""Stratified Cross-validation"")\n    ax.set_frame_on(False)\n    ax.invert_yaxis()\n    ax.set_xlim(0, n_samples + 1)\n    ax.set_ylabel(""CV iterations"")\n    ax.set_xlabel(""Data points"")\n\n    ax.set_yticks(np.arange(n_folds + 1) + .3)\n    ax.set_yticklabels(\n        [""Split %d"" % x for x in range(1, n_folds + 1)] + [""Class label""])\n\n    n_subsplit = n_samples_per_fold / 3.\n    for i in range(n_folds):\n        test_bars = ax.barh(\n            y=[i] * n_folds, width=[n_subsplit - 1] * n_folds,\n            left=np.arange(n_folds) * n_samples_per_fold + i * n_subsplit,\n            height=.6, color=""grey"", hatch=""//"", edgecolor=\'k\', align=\'edge\')\n\n    w = 2 * n_subsplit - 1\n    ax.barh(y=[0] * n_folds, width=[w] * n_folds, left=np.arange(n_folds)\n            * n_samples_per_fold + (0 + 1) * n_subsplit, height=.6, color=""w"",\n            hatch=""//"", edgecolor=\'k\', align=\'edge\')\n    ax.barh(y=[1] * (n_folds + 1), width=[w / 2., w, w, w / 2.],\n            left=np.maximum(0, np.arange(n_folds + 1) * n_samples_per_fold -\n                            n_subsplit), height=.6, color=""w"", hatch=""//"",\n            edgecolor=\'k\', align=\'edge\')\n    training_bars = ax.barh(y=[2] * n_folds, width=[w] * n_folds,\n                            left=np.arange(n_folds) * n_samples_per_fold,\n                            height=.6, color=""w"", hatch=""//"", edgecolor=\'k\',\n                            align=\'edge\')\n\n    ax.barh(y=[n_folds] * n_folds, width=[n_samples_per_fold - 1] *\n            n_folds, left=np.arange(n_folds) * n_samples_per_fold, height=.6,\n            color=""w"", edgecolor=\'k\', align=\'edge\')\n\n    for i in range(3):\n        ax.text((i + .5) * n_samples_per_fold, 3.5, ""Class %d"" %\n                i, horizontalalignment=""center"")\n    ax.set_ylim(4, -0.1)\n    plt.legend([training_bars[0], test_bars[0]], [\n               \'Training data\', \'Test data\'], loc=(1.05, 1), frameon=False)\n\n    fig.tight_layout()\n\n\ndef plot_cross_validation():\n    plt.figure(figsize=(12, 2))\n    plt.title(""cross_validation"")\n    axes = plt.gca()\n    axes.set_frame_on(False)\n\n    n_folds = 5\n    n_samples = 25\n\n    n_samples_per_fold = n_samples / float(n_folds)\n\n    for i in range(n_folds):\n        colors = [""w""] * n_folds\n        colors[i] = ""grey""\n        bars = plt.barh(\n            y=range(n_folds), width=[n_samples_per_fold - 0.1] * n_folds,\n            left=i * n_samples_per_fold, height=.6, color=colors, hatch=""//"",\n            edgecolor=\'k\', align=\'edge\')\n    axes.invert_yaxis()\n    axes.set_xlim(0, n_samples + 1)\n    plt.ylabel(""CV iterations"")\n    plt.xlabel(""Data points"")\n    plt.xticks(np.arange(n_samples_per_fold / 2., n_samples,\n                         n_samples_per_fold),\n               [""Fold %d"" % x for x in range(1, n_folds + 1)])\n    plt.yticks(np.arange(n_folds) + .3,\n               [""Split %d"" % x for x in range(1, n_folds + 1)])\n    plt.legend([bars[0], bars[4]], [\'Training data\', \'Test data\'],\n               loc=(1.05, 0.4), frameon=False)\n\n\ndef plot_threefold_split():\n    plt.figure(figsize=(15, 1))\n    axis = plt.gca()\n    bars = axis.barh([0, 0, 0], [11.9, 2.9, 4.9], left=[0, 12, 15], color=[\n                     \'white\', \'grey\', \'grey\'], hatch=""//"", edgecolor=\'k\',\n                     align=\'edge\')\n    bars[2].set_hatch(r"""")\n    axis.set_yticks(())\n    axis.set_frame_on(False)\n    axis.set_ylim(-.1, .8)\n    axis.set_xlim(-0.1, 20.1)\n    axis.set_xticks([6, 13.3, 17.5])\n    axis.set_xticklabels([""training set"", ""validation set"",\n                          ""test set""], fontdict={\'fontsize\': 20})\n    axis.tick_params(length=0, labeltop=True, labelbottom=False)\n    axis.text(6, -.3, ""Model fitting"",\n              fontdict={\'fontsize\': 13}, horizontalalignment=""center"")\n    axis.text(13.3, -.3, ""Parameter selection"",\n              fontdict={\'fontsize\': 13}, horizontalalignment=""center"")\n    axis.text(17.5, -.3, ""Evaluation"",\n              fontdict={\'fontsize\': 13}, horizontalalignment=""center"")\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_dbscan.py,1,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\nfrom .plot_helpers import discrete_scatter, cm3\n\n\ndef plot_dbscan():\n    X, y = make_blobs(random_state=0, n_samples=12)\n\n    dbscan = DBSCAN()\n    clusters = dbscan.fit_predict(X)\n    clusters\n\n    fig, axes = plt.subplots(3, 4, figsize=(11, 8),\n                             subplot_kw={\'xticks\': (), \'yticks\': ()})\n    # Plot clusters as red, green and blue, and outliers (-1) as white\n    colors = [cm3(1), cm3(0), cm3(2)]\n    markers = [\'o\', \'^\', \'v\']\n\n    # iterate over settings of min_samples and eps\n    for i, min_samples in enumerate([2, 3, 5]):\n        for j, eps in enumerate([1, 1.5, 2, 3]):\n            # instantiate DBSCAN with a particular setting\n            dbscan = DBSCAN(min_samples=min_samples, eps=eps)\n            # get cluster assignments\n            clusters = dbscan.fit_predict(X)\n            print(""min_samples: %d eps: %f  cluster: %s""\n                  % (min_samples, eps, clusters))\n            if np.any(clusters == -1):\n                c = [\'w\'] + colors\n                m = [\'o\'] + markers\n            else:\n                c = colors\n                m = markers\n            discrete_scatter(X[:, 0], X[:, 1], clusters, ax=axes[i, j], c=c,\n                             s=8, markers=m)\n            inds = dbscan.core_sample_indices_\n            # vizualize core samples and clusters.\n            if len(inds):\n                discrete_scatter(X[inds, 0], X[inds, 1], clusters[inds],\n                                 ax=axes[i, j], s=15, c=colors,\n                                 markers=markers)\n            axes[i, j].set_title(""min_samples: %d eps: %.1f""\n                                 % (min_samples, eps))\n    fig.tight_layout()\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_decomposition.py,0,"b'import matplotlib.pyplot as plt\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\n\n\ndef plot_decomposition(people, pca):\n    image_shape = people.images[0].shape\n    plt.figure(figsize=(20, 3))\n    ax = plt.gca()\n\n    imagebox = OffsetImage(people.images[0], zoom=1.5, cmap=""gray"")\n    ab = AnnotationBbox(imagebox, (.05, 0.4), pad=0.0, xycoords=\'data\')\n    ax.add_artist(ab)\n\n    for i in range(4):\n        imagebox = OffsetImage(pca.components_[i].reshape(image_shape), zoom=1.5, cmap=""viridis"")\n\n        ab = AnnotationBbox(imagebox, (.3 + .2 * i, 0.4),\n                            pad=0.0,\n                            xycoords=\'data\'\n                            )\n        ax.add_artist(ab)\n        if i == 0:\n            plt.text(.18, .25, \'x_%d *\' % i, fontdict={\'fontsize\': 50})\n        else:\n            plt.text(.15 + .2 * i, .25, \'+ x_%d *\' % i, fontdict={\'fontsize\': 50})\n\n    plt.text(.95, .25, \'+ ...\', fontdict={\'fontsize\': 50})\n\n    plt.rc(\'text\', usetex=True)\n    plt.text(.13, .3, r\'\\approx\', fontdict={\'fontsize\': 50})\n    plt.axis(""off"")\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_grid_search.py,1,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.datasets import load_iris\nimport pandas as pd\n\n\ndef plot_cross_val_selection():\n    iris = load_iris()\n    X_trainval, X_test, y_trainval, y_test = train_test_split(iris.data,\n                                                              iris.target,\n                                                              random_state=0)\n\n    param_grid = {\'C\': [0.001, 0.01, 0.1, 1, 10, 100],\n                  \'gamma\': [0.001, 0.01, 0.1, 1, 10, 100]}\n    grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n    grid_search.fit(X_trainval, y_trainval)\n    results = pd.DataFrame(grid_search.cv_results_)[15:]\n\n    best = np.argmax(results.mean_test_score.values)\n    plt.figure(figsize=(10, 3))\n    plt.xlim(-1, len(results))\n    plt.ylim(0, 1.1)\n    for i, (_, row) in enumerate(results.iterrows()):\n        scores = row[[\'split%d_test_score\' % i for i in range(5)]]\n        marker_cv, = plt.plot([i] * 5, scores, \'^\', c=\'gray\', markersize=5,\n                              alpha=.5)\n        marker_mean, = plt.plot(i, row.mean_test_score, \'v\', c=\'none\', alpha=1,\n                                markersize=10, markeredgecolor=\'k\')\n        if i == best:\n            marker_best, = plt.plot(i, row.mean_test_score, \'o\', c=\'red\',\n                                    fillstyle=""none"", alpha=1, markersize=20,\n                                    markeredgewidth=3)\n\n    plt.xticks(range(len(results)), [str(x).strip(""{}"").replace(""\'"", """") for x\n                                     in grid_search.cv_results_[\'params\']],\n               rotation=90)\n    plt.ylabel(""Validation accuracy"")\n    plt.xlabel(""Parameter settings"")\n    plt.legend([marker_cv, marker_mean, marker_best],\n               [""cv accuracy"", ""mean accuracy"", ""best parameter setting""],\n               loc=(1.05, .4))\n\n\ndef plot_grid_search_overview():\n    plt.figure(figsize=(10, 3), dpi=70)\n    axes = plt.gca()\n    axes.yaxis.set_visible(False)\n    axes.xaxis.set_visible(False)\n    axes.set_frame_on(False)\n\n    def draw(ax, text, start, target=None):\n        if target is not None:\n            patchB = target.get_bbox_patch()\n            end = target.get_position()\n        else:\n            end = start\n            patchB = None\n        annotation = ax.annotate(text, end, start, xycoords=\'axes pixels\',\n                                 textcoords=\'axes pixels\', size=20,\n                                 arrowprops=dict(\n                                     arrowstyle=""-|>"", fc=""w"", ec=""k"",\n                                     patchB=patchB,\n                                     connectionstyle=""arc3,rad=0.0""),\n                                 bbox=dict(boxstyle=""round"", fc=""w""),\n                                 horizontalalignment=""center"",\n                                 verticalalignment=""center"")\n        plt.draw()\n        return annotation\n\n    step = 100\n    grr = 400\n\n    final_evaluation = draw(axes, ""final evaluation"", (5 * step, grr - 3 *\n                                                       step))\n    retrained_model = draw(axes, ""retrained model"", (3 * step, grr - 3 * step),\n                           final_evaluation)\n    best_parameters = draw(axes, ""best parameters"", (.5 * step, grr - 3 *\n                                                     step), retrained_model)\n    cross_validation = draw(axes, ""cross-validation"", (.5 * step, grr - 2 *\n                                                       step), best_parameters)\n    draw(axes, ""parameter grid"", (0.0, grr - 0), cross_validation)\n    training_data = draw(axes, ""training data"", (2 * step, grr - step),\n                         cross_validation)\n    draw(axes, ""training data"", (2 * step, grr - step), retrained_model)\n    test_data = draw(axes, ""test data"", (5 * step, grr - step),\n                     final_evaluation)\n    draw(axes, ""data set"", (3.5 * step, grr - 0.0), training_data)\n    draw(axes, ""data set"", (3.5 * step, grr - 0.0), test_data)\n    plt.ylim(0, 1)\n    plt.xlim(0, 1.5)\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_helpers.py,3,"b'import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap, colorConverter, LinearSegmentedColormap\n\n\ncm_cycle = ListedColormap([\'#0000aa\', \'#ff5050\', \'#50ff50\', \'#9040a0\', \'#fff000\'])\ncm3 = ListedColormap([\'#0000aa\', \'#ff2020\', \'#50ff50\'])\ncm2 = ListedColormap([\'#0000aa\', \'#ff2020\'])\n\n# create a smooth transition from the first to to the second color of cm3\n# similar to RdBu but with our red and blue, also not going through white,\n# which is really bad for greyscale\n\ncdict = {\'red\': [(0.0, 0.0, cm2(0)[0]),\n                 (1.0, cm2(1)[0], 1.0)],\n\n         \'green\': [(0.0, 0.0, cm2(0)[1]),\n                   (1.0, cm2(1)[1], 1.0)],\n\n         \'blue\': [(0.0, 0.0, cm2(0)[2]),\n                  (1.0, cm2(1)[2], 1.0)]}\n\nReBl = LinearSegmentedColormap(""ReBl"", cdict)\n\n\ndef discrete_scatter(x1, x2, y=None, markers=None, s=10, ax=None,\n                     labels=None, padding=.2, alpha=1, c=None, markeredgewidth=None):\n    """"""Adaption of matplotlib.pyplot.scatter to plot classes or clusters.\n\n    Parameters\n    ----------\n\n    x1 : nd-array\n        input data, first axis\n\n    x2 : nd-array\n        input data, second axis\n\n    y : nd-array\n        input data, discrete labels\n\n    cmap : colormap\n        Colormap to use.\n\n    markers : list of string\n        List of markers to use, or None (which defaults to \'o\').\n\n    s : int or float\n        Size of the marker\n\n    padding : float\n        Fraction of the dataset range to use for padding the axes.\n\n    alpha : float\n        Alpha value for all points.\n    """"""\n    if ax is None:\n        ax = plt.gca()\n\n    if y is None:\n        y = np.zeros(len(x1))\n\n    unique_y = np.unique(y)\n\n    if markers is None:\n        markers = [\'o\', \'^\', \'v\', \'D\', \'s\', \'*\', \'p\', \'h\', \'H\', \'8\', \'<\', \'>\'] * 10\n\n    if len(markers) == 1:\n        markers = markers * len(unique_y)\n\n    if labels is None:\n        labels = unique_y\n\n    # lines in the matplotlib sense, not actual lines\n    lines = []\n\n    current_cycler = mpl.rcParams[\'axes.prop_cycle\']\n\n    for i, (yy, cycle) in enumerate(zip(unique_y, current_cycler())):\n        mask = y == yy\n        # if c is none, use color cycle\n        if c is None:\n            color = cycle[\'color\']\n        elif len(c) > 1:\n            color = c[i]\n        else:\n            color = c\n        # use light edge for dark markers\n        if np.mean(colorConverter.to_rgb(color)) < .4:\n            markeredgecolor = ""grey""\n        else:\n            markeredgecolor = ""black""\n\n        lines.append(ax.plot(x1[mask], x2[mask], markers[i], markersize=s,\n                             label=labels[i], alpha=alpha, c=color,\n                             markeredgewidth=markeredgewidth,\n                             markeredgecolor=markeredgecolor)[0])\n\n    if padding != 0:\n        pad1 = x1.std() * padding\n        pad2 = x2.std() * padding\n        xlim = ax.get_xlim()\n        ylim = ax.get_ylim()\n        ax.set_xlim(min(x1.min() - pad1, xlim[0]), max(x1.max() + pad1, xlim[1]))\n        ax.set_ylim(min(x2.min() - pad2, ylim[0]), max(x2.max() + pad2, ylim[1]))\n\n    return lines\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_improper_preprocessing.py,0,"b'import matplotlib.pyplot as plt\n\n\ndef make_bracket(s, xy, textxy, width, ax):\n    annotation = ax.annotate(\n        s, xy, textxy, ha=""center"", va=""center"", size=20,\n        arrowprops=dict(arrowstyle=""-["", fc=""w"", ec=""k"",\n                        lw=2,), bbox=dict(boxstyle=""square"", fc=""w""))\n    annotation.arrow_patch.get_arrowstyle().widthB = width\n\n\ndef plot_improper_processing():\n    fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n\n    for axis in axes:\n        bars = axis.barh([0, 0, 0], [11.9, 2.9, 4.9], left=[0, 12, 15],\n                         color=[\'white\', \'grey\', \'grey\'], hatch=""//"",\n                         align=\'edge\', edgecolor=\'k\')\n        bars[2].set_hatch(r"""")\n        axis.set_yticks(())\n        axis.set_frame_on(False)\n        axis.set_ylim(-.1, 6)\n        axis.set_xlim(-0.1, 20.1)\n        axis.set_xticks(())\n        axis.tick_params(length=0, labeltop=True, labelbottom=False)\n        axis.text(6, -.3, ""training folds"",\n                  fontdict={\'fontsize\': 14}, horizontalalignment=""center"")\n        axis.text(13.5, -.3, ""validation fold"",\n                  fontdict={\'fontsize\': 14}, horizontalalignment=""center"")\n        axis.text(17.5, -.3, ""test set"",\n                  fontdict={\'fontsize\': 14}, horizontalalignment=""center"")\n\n    make_bracket(""scaler fit"", (7.5, 1.3), (7.5, 2.), 15, axes[0])\n    make_bracket(""SVC fit"", (6, 3), (6, 4), 12, axes[0])\n    make_bracket(""SVC predict"", (13.4, 3), (13.4, 4), 2.5, axes[0])\n\n    axes[0].set_title(""Cross validation"")\n    axes[1].set_title(""Test set prediction"")\n\n    make_bracket(""scaler fit"", (7.5, 1.3), (7.5, 2.), 15, axes[1])\n    make_bracket(""SVC fit"", (7.5, 3), (7.5, 4), 15, axes[1])\n    make_bracket(""SVC predict"", (17.5, 3), (17.5, 4), 4.8, axes[1])\n\n\ndef plot_proper_processing():\n    fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n\n    for axis in axes:\n        bars = axis.barh([0, 0, 0], [11.9, 2.9, 4.9],\n                         left=[0, 12, 15], color=[\'white\', \'grey\', \'grey\'],\n                         hatch=""//"", align=\'edge\', edgecolor=\'k\')\n        bars[2].set_hatch(r"""")\n        axis.set_yticks(())\n        axis.set_frame_on(False)\n        axis.set_ylim(-.1, 4.5)\n        axis.set_xlim(-0.1, 20.1)\n        axis.set_xticks(())\n        axis.tick_params(length=0, labeltop=True, labelbottom=False)\n        axis.text(6, -.3, ""training folds"", fontdict={\'fontsize\': 14},\n                  horizontalalignment=""center"")\n        axis.text(13.5, -.3, ""validation fold"", fontdict={\'fontsize\': 14},\n                  horizontalalignment=""center"")\n        axis.text(17.5, -.3, ""test set"", fontdict={\'fontsize\': 14},\n                  horizontalalignment=""center"")\n\n    make_bracket(""scaler fit"", (6, 1.3), (6, 2.), 12, axes[0])\n    make_bracket(""SVC fit"", (6, 3), (6, 4), 12, axes[0])\n    make_bracket(""SVC predict"", (13.4, 3), (13.4, 4), 2.5, axes[0])\n\n    axes[0].set_title(""Cross validation"")\n    axes[1].set_title(""Test set prediction"")\n\n    make_bracket(""scaler fit"", (7.5, 1.3), (7.5, 2.), 15, axes[1])\n    make_bracket(""SVC fit"", (7.5, 3), (7.5, 4), 15, axes[1])\n    make_bracket(""SVC predict"", (17.5, 3), (17.5, 4), 4.8, axes[1])\n    fig.subplots_adjust(hspace=.3)\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_interactive_tree.py,6,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.externals.six import StringIO  # doctest: +SKIP\nfrom sklearn.tree import export_graphviz\nfrom scipy.misc import imread\nfrom scipy import ndimage\nfrom sklearn.datasets import make_moons\n\nimport re\n\nfrom .tools import discrete_scatter\nfrom .plot_helpers import cm2\n\n\ndef tree_image(tree, fout=None):\n    try:\n        import graphviz\n    except ImportError:\n        # make a hacky white plot\n        x = np.ones((10, 10))\n        x[0, 0] = 0\n        return x\n    dot_data = StringIO()\n    export_graphviz(tree, out_file=dot_data, max_depth=3, impurity=False)\n    data = dot_data.getvalue()\n    #data = re.sub(r""gini = 0\\.[0-9]+\\\\n"", """", dot_data.getvalue())\n    data = re.sub(r""samples = [0-9]+\\\\n"", """", data)\n    data = re.sub(r""\\\\nsamples = [0-9]+"", """", data)\n    data = re.sub(r""value"", ""counts"", data)\n\n    graph = graphviz.Source(data, format=""png"")\n    if fout is None:\n        fout = ""tmp""\n    graph.render(fout)\n    return imread(fout + "".png"")\n\n\ndef plot_tree_progressive():\n    X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n    plt.figure()\n    ax = plt.gca()\n    discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n    axes = []\n    for i in range(3):\n        fig, ax = plt.subplots(1, 2, figsize=(12, 4),\n                               subplot_kw={\'xticks\': (), \'yticks\': ()})\n        axes.append(ax)\n    axes = np.array(axes)\n\n    for i, max_depth in enumerate([1, 2, 9]):\n        tree = plot_tree(X, y, max_depth=max_depth, ax=axes[i, 0])\n        axes[i, 1].imshow(tree_image(tree))\n        axes[i, 1].set_axis_off()\n\n\ndef plot_tree_partition(X, y, tree, ax=None):\n    if ax is None:\n        ax = plt.gca()\n    eps = X.std() / 2.\n\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n    xx = np.linspace(x_min, x_max, 1000)\n    yy = np.linspace(y_min, y_max, 1000)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    X_grid = np.c_[X1.ravel(), X2.ravel()]\n\n    Z = tree.predict(X_grid)\n    Z = Z.reshape(X1.shape)\n    faces = tree.apply(X_grid)\n    faces = faces.reshape(X1.shape)\n    border = ndimage.laplace(faces) != 0\n    ax.contourf(X1, X2, Z, alpha=.4, cmap=cm2, levels=[0, .5, 1])\n    ax.scatter(X1[border], X2[border], marker=\'.\', s=1)\n\n    discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n    return ax\n\n\ndef plot_tree(X, y, max_depth=1, ax=None):\n    tree = DecisionTreeClassifier(max_depth=max_depth, random_state=0).fit(X, y)\n    ax = plot_tree_partition(X, y, tree, ax=ax)\n    ax.set_title(""depth = %d"" % max_depth)\n    return tree\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_kmeans.py,6,"b'import numpy as np\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\n\nfrom .tools import discrete_scatter\nfrom .plot_2d_separator import plot_2d_classification\nfrom .plot_helpers import cm3\n\n\ndef plot_kmeans_algorithm():\n\n    X, y = make_blobs(random_state=1)\n    # we don\'t want cyan in there\n    with mpl.rc_context(rc={\'axes.prop_cycle\': cycler(\'color\', [\'#0000aa\',\n                                                                \'#ff2020\',\n                                                                \'#50ff50\'])}):\n        fig, axes = plt.subplots(3, 3, figsize=(10, 8), subplot_kw={\'xticks\': (), \'yticks\': ()})\n        axes = axes.ravel()\n        axes[0].set_title(""Input data"")\n        discrete_scatter(X[:, 0], X[:, 1], ax=axes[0], markers=[\'o\'], c=\'w\')\n\n        axes[1].set_title(""Initialization"")\n        init = X[:3, :]\n        discrete_scatter(X[:, 0], X[:, 1], ax=axes[1], markers=[\'o\'], c=\'w\')\n        discrete_scatter(init[:, 0], init[:, 1], [0, 1, 2], ax=axes[1],\n                         markers=[\'^\'], markeredgewidth=2)\n\n        axes[2].set_title(""Assign Points (1)"")\n        km = KMeans(n_clusters=3, init=init, max_iter=1, n_init=1).fit(X)\n        centers = km.cluster_centers_\n        # need to compute labels by hand. scikit-learn does two e-steps for max_iter=1\n        # (and it\'s totally my fault)\n        labels = np.argmin(pairwise_distances(init, X), axis=0)\n        discrete_scatter(X[:, 0], X[:, 1], labels, markers=[\'o\'],\n                         ax=axes[2])\n        discrete_scatter(init[:, 0], init[:, 1], [0, 1, 2],\n                         ax=axes[2], markers=[\'^\'], markeredgewidth=2)\n\n        axes[3].set_title(""Recompute Centers (1)"")\n        discrete_scatter(X[:, 0], X[:, 1], labels, markers=[\'o\'],\n                         ax=axes[3])\n        discrete_scatter(centers[:, 0], centers[:, 1], [0, 1, 2],\n                         ax=axes[3], markers=[\'^\'], markeredgewidth=2)\n\n        axes[4].set_title(""Reassign Points (2)"")\n        km = KMeans(n_clusters=3, init=init, max_iter=1, n_init=1).fit(X)\n        labels = km.labels_\n        discrete_scatter(X[:, 0], X[:, 1], labels, markers=[\'o\'],\n                         ax=axes[4])\n        discrete_scatter(centers[:, 0], centers[:, 1], [0, 1, 2],\n                         ax=axes[4], markers=[\'^\'], markeredgewidth=2)\n\n        km = KMeans(n_clusters=3, init=init, max_iter=2, n_init=1).fit(X)\n        axes[5].set_title(""Recompute Centers (2)"")\n        centers = km.cluster_centers_\n        discrete_scatter(X[:, 0], X[:, 1], labels, markers=[\'o\'],\n                         ax=axes[5])\n        discrete_scatter(centers[:, 0], centers[:, 1], [0, 1, 2],\n                         ax=axes[5], markers=[\'^\'], markeredgewidth=2)\n\n        axes[6].set_title(""Reassign Points (3)"")\n        labels = km.labels_\n        discrete_scatter(X[:, 0], X[:, 1], labels, markers=[\'o\'],\n                         ax=axes[6])\n        markers = discrete_scatter(centers[:, 0], centers[:, 1], [0, 1, 2],\n                                   ax=axes[6], markers=[\'^\'],\n                                   markeredgewidth=2)\n\n        axes[7].set_title(""Recompute Centers (3)"")\n        km = KMeans(n_clusters=3, init=init, max_iter=3, n_init=1).fit(X)\n        centers = km.cluster_centers_\n        discrete_scatter(X[:, 0], X[:, 1], labels, markers=[\'o\'],\n                         ax=axes[7])\n        discrete_scatter(centers[:, 0], centers[:, 1], [0, 1, 2],\n                         ax=axes[7], markers=[\'^\'], markeredgewidth=2)\n        axes[8].set_axis_off()\n        axes[8].legend(markers, [""Cluster 0"", ""Cluster 1"", ""Cluster 2""], loc=\'best\')\n\n\ndef plot_kmeans_boundaries():\n    X, y = make_blobs(random_state=1)\n    init = X[:3, :]\n    km = KMeans(n_clusters=3, init=init, max_iter=2, n_init=1).fit(X)\n    discrete_scatter(X[:, 0], X[:, 1], km.labels_, markers=[\'o\'])\n    discrete_scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n                     [0, 1, 2], markers=[\'^\'], markeredgewidth=2)\n    plot_2d_classification(km, X, cm=cm3, alpha=.4)\n\n\ndef plot_kmeans_faces(km, pca, X_pca, X_people, y_people, target_names):\n    n_clusters = 10\n    image_shape = (87, 65)\n    fig, axes = plt.subplots(n_clusters, 11, subplot_kw={\'xticks\': (), \'yticks\': ()},\n                             figsize=(10, 15), gridspec_kw={""hspace"": .3})\n\n    for cluster in range(n_clusters):\n        center = km.cluster_centers_[cluster]\n        mask = km.labels_ == cluster\n        dists = np.sum((X_pca - center) ** 2, axis=1)\n        dists[~mask] = np.inf\n        inds = np.argsort(dists)[:5]\n        dists[~mask] = -np.inf\n        inds = np.r_[inds, np.argsort(dists)[-5:]]\n        axes[cluster, 0].imshow(pca.inverse_transform(center).reshape(image_shape), vmin=0, vmax=1)\n        for image, label, asdf, ax in zip(X_people[inds], y_people[inds],\n                                          km.labels_[inds], axes[cluster, 1:]):\n            ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n            ax.set_title(""%s"" % (target_names[label].split()[-1]), fontdict={\'fontsize\': 9})\n\n    # add some boxes to illustrate which are similar and which dissimilar\n    rec = plt.Rectangle([-5, -30], 73, 1295, fill=False, lw=2)\n    rec = axes[0, 0].add_patch(rec)\n    rec.set_clip_on(False)\n    axes[0, 0].text(0, -40, ""Center"")\n\n    rec = plt.Rectangle([-5, -30], 385, 1295, fill=False, lw=2)\n    rec = axes[0, 1].add_patch(rec)\n    rec.set_clip_on(False)\n    axes[0, 1].text(0, -40, ""Close to center"")\n\n    rec = plt.Rectangle([-5, -30], 385, 1295, fill=False, lw=2)\n    rec = axes[0, 6].add_patch(rec)\n    rec.set_clip_on(False)\n    axes[0, 6].text(0, -40, ""Far from center"")\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_kneighbors_regularization.py,6,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\n\ndef plot_regression_datasets():\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    for n_samples, ax in zip([10, 100, 1000], axes):\n        x, y = make_dataset(n_samples)\n        ax.plot(x, y, \'o\', alpha=.6)\n\n\ndef plot_kneighbors_regularization():\n    rnd = np.random.RandomState(42)\n    x = np.linspace(-3, 3, 100)\n    y_no_noise = np.sin(4 * x) + x\n    y = y_no_noise + rnd.normal(size=len(x))\n    X = x[:, np.newaxis]\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    x_test = np.linspace(-3, 3, 1000)\n\n    for n_neighbors, ax in zip([2, 5, 20], axes.ravel()):\n        kneighbor_regression = KNeighborsRegressor(n_neighbors=n_neighbors)\n        kneighbor_regression.fit(X, y)\n        ax.plot(x, y_no_noise, label=""true function"")\n        ax.plot(x, y, ""o"", label=""data"")\n        ax.plot(x_test, kneighbor_regression.predict(x_test[:, np.newaxis]),\n                label=""prediction"")\n        ax.legend()\n        ax.set_title(""n_neighbors = %d"" % n_neighbors)\n\nif __name__ == ""__main__"":\n    plot_kneighbors_regularization()\n    plt.show()\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_knn_classification.py,2,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import euclidean_distances\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom .datasets import make_forge\nfrom .plot_helpers import discrete_scatter\n\n\ndef plot_knn_classification(n_neighbors=1):\n    X, y = make_forge()\n\n    X_test = np.array([[8.2, 3.66214339], [9.9, 3.2], [11.2, .5]])\n    dist = euclidean_distances(X, X_test)\n    closest = np.argsort(dist, axis=0)\n\n    for x, neighbors in zip(X_test, closest.T):\n        for neighbor in neighbors[:n_neighbors]:\n            plt.arrow(x[0], x[1], X[neighbor, 0] - x[0],\n                      X[neighbor, 1] - x[1], head_width=0, fc=\'k\', ec=\'k\')\n\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n    test_points = discrete_scatter(X_test[:, 0], X_test[:, 1], clf.predict(X_test), markers=""*"")\n    training_points = discrete_scatter(X[:, 0], X[:, 1], y)\n    plt.legend(training_points + test_points, [""training class 0"", ""training class 1"",\n                                               ""test pred 0"", ""test pred 1""])\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_knn_regression.py,3,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import euclidean_distances\n\nfrom .datasets import make_wave\nfrom .plot_helpers import cm3\n\n\ndef plot_knn_regression(n_neighbors=1):\n    X, y = make_wave(n_samples=40)\n    X_test = np.array([[-1.5], [0.9], [1.5]])\n\n    dist = euclidean_distances(X, X_test)\n    closest = np.argsort(dist, axis=0)\n\n    plt.figure(figsize=(10, 6))\n\n    reg = KNeighborsRegressor(n_neighbors=n_neighbors).fit(X, y)\n    y_pred = reg.predict(X_test)\n\n    for x, y_, neighbors in zip(X_test, y_pred, closest.T):\n        for neighbor in neighbors[:n_neighbors]:\n                plt.arrow(x[0], y_, X[neighbor, 0] - x[0], y[neighbor] - y_,\n                          head_width=0, fc=\'k\', ec=\'k\')\n\n    train, = plt.plot(X, y, \'o\', c=cm3(0))\n    test, = plt.plot(X_test, -3 * np.ones(len(X_test)), \'*\', c=cm3(2),\n                     markersize=20)\n    pred, = plt.plot(X_test, y_pred, \'*\', c=cm3(0), markersize=20)\n    plt.vlines(X_test, -3.1, 3.1, linestyle=""--"")\n    plt.legend([train, test, pred],\n               [""training data/target"", ""test data"", ""test prediction""],\n               ncol=3, loc=(.1, 1.025))\n    plt.ylim(-3.1, 3.1)\n    plt.xlabel(""Feature"")\n    plt.ylabel(""Target"")\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_linear_regression.py,1,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom .datasets import make_wave\nfrom .plot_helpers import cm2\n\n\ndef plot_linear_regression_wave():\n    X, y = make_wave(n_samples=60)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n    line = np.linspace(-3, 3, 100).reshape(-1, 1)\n\n    lr = LinearRegression().fit(X_train, y_train)\n    print(""w[0]: %f  b: %f"" % (lr.coef_[0], lr.intercept_))\n\n    plt.figure(figsize=(8, 8))\n    plt.plot(line, lr.predict(line))\n    plt.plot(X, y, \'o\', c=cm2(0))\n    ax = plt.gca()\n    ax.spines[\'left\'].set_position(\'center\')\n    ax.spines[\'right\'].set_color(\'none\')\n    ax.spines[\'bottom\'].set_position(\'center\')\n    ax.spines[\'top\'].set_color(\'none\')\n    ax.set_ylim(-3, 3)\n    #ax.set_xlabel(""Feature"")\n    #ax.set_ylabel(""Target"")\n    ax.legend([""model"", ""training data""], loc=""best"")\n    ax.grid(True)\n    ax.set_aspect(\'equal\')\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_linear_svc_regularization.py,1,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nfrom sklearn.datasets import make_blobs\n\nfrom .plot_helpers import discrete_scatter\n\n\ndef plot_linear_svc_regularization():\n    X, y = make_blobs(centers=2, random_state=4, n_samples=30)\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\n    # a carefully hand-designed dataset lol\n    y[7] = 0\n    y[27] = 0\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n\n    for ax, C in zip(axes, [1e-2, 10, 1e3]):\n        discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n\n        svm = LinearSVC(C=C, tol=0.00001, dual=False).fit(X, y)\n        w = svm.coef_[0]\n        a = -w[0] / w[1]\n        xx = np.linspace(6, 13)\n        yy = a * xx - (svm.intercept_[0]) / w[1]\n        ax.plot(xx, yy, c=\'k\')\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.set_title(""C = %f"" % C)\n    axes[0].legend(loc=""best"")\n\nif __name__ == ""__main__"":\n    plot_linear_svc_regularization()\n    plt.show()\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_metrics.py,11,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom .tools import plot_2d_separator, plot_2d_scores, cm, discrete_scatter\nfrom .plot_helpers import ReBl\n\n\ndef plot_confusion_matrix_illustration():\n    plt.figure(figsize=(8, 8))\n    confusion = np.array([[401, 2], [8, 39]])\n    plt.text(0.40, .7, confusion[0, 0], size=70, horizontalalignment=\'right\')\n    plt.text(0.40, .2, confusion[1, 0], size=70, horizontalalignment=\'right\')\n    plt.text(.90, .7, confusion[0, 1], size=70, horizontalalignment=\'right\')\n    plt.text(.90, 0.2, confusion[1, 1], size=70, horizontalalignment=\'right\')\n    plt.xticks([.25, .75], [""predicted \'not nine\'"", ""predicted \'nine\'""], size=20)\n    plt.yticks([.25, .75], [""true \'nine\'"", ""true \'not nine\'""], size=20)\n    plt.plot([.5, .5], [0, 1], \'--\', c=\'k\')\n    plt.plot([0, 1], [.5, .5], \'--\', c=\'k\')\n\n    plt.xlim(0, 1)\n    plt.ylim(0, 1)\n\n\ndef plot_binary_confusion_matrix():\n    plt.text(0.45, .6, ""TN"", size=100, horizontalalignment=\'right\')\n    plt.text(0.45, .1, ""FN"", size=100, horizontalalignment=\'right\')\n    plt.text(.95, .6, ""FP"", size=100, horizontalalignment=\'right\')\n    plt.text(.95, 0.1, ""TP"", size=100, horizontalalignment=\'right\')\n    plt.xticks([.25, .75], [""predicted negative"", ""predicted positive""], size=15)\n    plt.yticks([.25, .75], [""positive class"", ""negative class""], size=15)\n    plt.plot([.5, .5], [0, 1], \'--\', c=\'k\')\n    plt.plot([0, 1], [.5, .5], \'--\', c=\'k\')\n\n    plt.xlim(0, 1)\n    plt.ylim(0, 1)\n\n\ndef plot_decision_threshold():\n    from mglearn.datasets import make_blobs\n    from sklearn.svm import SVC\n    from sklearn.model_selection import train_test_split\n\n    X, y = make_blobs(n_samples=(400, 50), centers=2, cluster_std=[7.0, 2],\n                      random_state=22)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    fig, axes = plt.subplots(2, 3, figsize=(15, 8), subplot_kw={\'xticks\': (), \'yticks\': ()})\n    plt.suptitle(""decision_threshold"")\n    axes[0, 0].set_title(""training data"")\n    discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=axes[0, 0])\n\n    svc = SVC(gamma=.05).fit(X_train, y_train)\n    axes[0, 1].set_title(""decision with threshold 0"")\n    discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=axes[0, 1])\n    plot_2d_scores(svc, X_train, function=""decision_function"", alpha=.7,\n                   ax=axes[0, 1], cm=ReBl)\n    plot_2d_separator(svc, X_train, linewidth=3, ax=axes[0, 1])\n    axes[0, 2].set_title(""decision with threshold -0.8"")\n    discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=axes[0, 2])\n    plot_2d_separator(svc, X_train, linewidth=3, ax=axes[0, 2], threshold=-.8)\n    plot_2d_scores(svc, X_train, function=""decision_function"", alpha=.7,\n                   ax=axes[0, 2], cm=ReBl)\n\n    axes[1, 0].set_axis_off()\n\n    mask = np.abs(X_train[:, 1] - 7) < 5\n    bla = np.sum(mask)\n\n    line = np.linspace(X_train.min(), X_train.max(), 100)\n    axes[1, 1].set_title(""Cross-section with threshold 0"")\n    axes[1, 1].plot(line, svc.decision_function(np.c_[line, 10 * np.ones(100)]), c=\'k\')\n    dec = svc.decision_function(np.c_[line, 10 * np.ones(100)])\n    contour = (dec > 0).reshape(1, -1).repeat(10, axis=0)\n    axes[1, 1].contourf(line, np.linspace(-1.5, 1.5, 10), contour, alpha=0.4, cmap=cm)\n    discrete_scatter(X_train[mask, 0], np.zeros(bla), y_train[mask], ax=axes[1, 1])\n    axes[1, 1].set_xlim(X_train.min(), X_train.max())\n    axes[1, 1].set_ylim(-1.5, 1.5)\n    axes[1, 1].set_xticks(())\n    axes[1, 1].set_ylabel(""Decision value"")\n\n    contour2 = (dec > -.8).reshape(1, -1).repeat(10, axis=0)\n    axes[1, 2].set_title(""Cross-section with threshold -0.8"")\n    axes[1, 2].contourf(line, np.linspace(-1.5, 1.5, 10), contour2, alpha=0.4, cmap=cm)\n    discrete_scatter(X_train[mask, 0], np.zeros(bla), y_train[mask], alpha=.1, ax=axes[1, 2])\n    axes[1, 2].plot(line, svc.decision_function(np.c_[line, 10 * np.ones(100)]), c=\'k\')\n    axes[1, 2].set_xlim(X_train.min(), X_train.max())\n    axes[1, 2].set_ylim(-1.5, 1.5)\n    axes[1, 2].set_xticks(())\n    axes[1, 2].set_ylabel(""Decision value"")\n    axes[1, 0].legend([\'negative class\', \'positive class\'])\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_nmf.py,3,"b'from sklearn.decomposition import NMF\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.externals.joblib import Memory\n\nmemory = Memory(cachedir=""cache"")\n\n\ndef plot_nmf_illustration():\n    rnd = np.random.RandomState(5)\n    X_ = rnd.normal(size=(300, 2))\n    # Add 8 to make sure every point lies in the positive part of the space\n    X_blob = np.dot(X_, rnd.normal(size=(2, 2))) + rnd.normal(size=2) + 8\n\n    nmf = NMF(random_state=0)\n    nmf.fit(X_blob)\n    X_nmf = nmf.transform(X_blob)\n\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n    axes[0].scatter(X_blob[:, 0], X_blob[:, 1], c=X_nmf[:, 0], linewidths=0,\n                    s=60, cmap=\'viridis\')\n    axes[0].set_xlabel(""feature 1"")\n    axes[0].set_ylabel(""feature 2"")\n    axes[0].set_xlim(0, 12)\n    axes[0].set_ylim(0, 12)\n    axes[0].arrow(0, 0, nmf.components_[0, 0], nmf.components_[0, 1], width=.1,\n                  head_width=.3, color=\'k\')\n    axes[0].arrow(0, 0, nmf.components_[1, 0], nmf.components_[1, 1], width=.1,\n                  head_width=.3, color=\'k\')\n    axes[0].set_aspect(\'equal\')\n    axes[0].set_title(""NMF with two components"")\n\n    # second plot\n    nmf = NMF(random_state=0, n_components=1)\n    nmf.fit(X_blob)\n\n    axes[1].scatter(X_blob[:, 0], X_blob[:, 1], c=X_nmf[:, 0], linewidths=0,\n                    s=60, cmap=\'viridis\')\n    axes[1].set_xlabel(""feature 1"")\n    axes[1].set_ylabel(""feature 2"")\n    axes[1].set_xlim(0, 12)\n    axes[1].set_ylim(0, 12)\n    axes[1].arrow(0, 0, nmf.components_[0, 0], nmf.components_[0, 1], width=.1,\n                  head_width=.3, color=\'k\')\n\n    axes[1].set_aspect(\'equal\')\n    axes[1].set_title(""NMF with one component"")\n\n\n@memory.cache\ndef nmf_faces(X_train, X_test):\n    # Build NMF models with 10, 50, 100 and 500 components\n    # this list will hold the back-transformd test-data\n    reduced_images = []\n    for n_components in [10, 50, 100, 500]:\n        # build the NMF model\n        nmf = NMF(n_components=n_components, random_state=0)\n        nmf.fit(X_train)\n        # transform the test data (afterwards has n_components many dimensions)\n        X_test_nmf = nmf.transform(X_test)\n        # back-transform the transformed test-data\n        # (afterwards it\'s in the original space again)\n        X_test_back = np.dot(X_test_nmf, nmf.components_)\n        reduced_images.append(X_test_back)\n    return reduced_images\n\n\ndef plot_nmf_faces(X_train, X_test, image_shape):\n    reduced_images = nmf_faces(X_train, X_test)\n\n    # plot the first three images in the test set:\n    fix, axes = plt.subplots(3, 5, figsize=(15, 12),\n                             subplot_kw={\'xticks\': (), \'yticks\': ()})\n    for i, ax in enumerate(axes):\n        # plot original image\n        ax[0].imshow(X_test[i].reshape(image_shape),\n                     vmin=0, vmax=1)\n        # plot the four back-transformed images\n        for a, X_test_back in zip(ax[1:], reduced_images):\n            a.imshow(X_test_back[i].reshape(image_shape), vmin=0, vmax=1)\n\n    # label the top row\n    axes[0, 0].set_title(""original image"")\n    for ax, n_components in zip(axes[0, 1:], [10, 50, 100, 500]):\n        ax.set_title(""%d components"" % n_components)\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_nn_graphs.py,0,"b'\n\ndef plot_logistic_regression_graph():\n    import graphviz\n    lr_graph = graphviz.Digraph(node_attr={\'shape\': \'circle\', \'fixedsize\': \'True\'},\n                                graph_attr={\'rankdir\': \'LR\', \'splines\': \'line\'})\n    inputs = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_0"")\n    output = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_2"")\n\n    for i in range(4):\n        inputs.node(""x[%d]"" % i, labelloc=""c"")\n    inputs.body.append(\'label = ""inputs""\')\n    inputs.body.append(\'color = ""white""\')\n\n    lr_graph.subgraph(inputs)\n\n    output.body.append(\'label = ""output""\')\n    output.body.append(\'color = ""white""\')\n    output.node(""y"")\n\n    lr_graph.subgraph(output)\n\n    for i in range(4):\n        lr_graph.edge(""x[%d]"" % i, ""y"", label=""w[%d]"" % i)\n    return lr_graph\n\n\ndef plot_single_hidden_layer_graph():\n    import graphviz\n    nn_graph = graphviz.Digraph(node_attr={\'shape\': \'circle\', \'fixedsize\': \'True\'},\n                                graph_attr={\'rankdir\': \'LR\', \'splines\': \'line\'})\n\n    inputs = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_0"")\n    hidden = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_1"")\n    output = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_2"")\n\n    for i in range(4):\n        inputs.node(""x[%d]"" % i)\n\n    inputs.body.append(\'label = ""inputs""\')\n    inputs.body.append(\'color = ""white""\')\n\n    hidden.body.append(\'label = ""hidden layer""\')\n    hidden.body.append(\'color = ""white""\')\n\n    for i in range(3):\n        hidden.node(""h%d"" % i, label=""h[%d]"" % i)\n\n    output.node(""y"")\n    output.body.append(\'label = ""output""\')\n    output.body.append(\'color = ""white""\')\n\n    nn_graph.subgraph(inputs)\n    nn_graph.subgraph(hidden)\n    nn_graph.subgraph(output)\n\n    for i in range(4):\n        for j in range(3):\n            nn_graph.edge(""x[%d]"" % i, ""h%d"" % j)\n\n    for i in range(3):\n        nn_graph.edge(""h%d"" % i, ""y"")\n    return nn_graph\n\n\ndef plot_two_hidden_layer_graph():\n    import graphviz\n    nn_graph = graphviz.Digraph(node_attr={\'shape\': \'circle\', \'fixedsize\': \'True\'},\n                                graph_attr={\'rankdir\': \'LR\', \'splines\': \'line\'})\n\n    inputs = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_0"")\n    hidden = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_1"")\n    hidden2 = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_2"")\n\n    output = graphviz.Digraph(node_attr={\'shape\': \'circle\'}, name=""cluster_3"")\n\n    for i in range(4):\n        inputs.node(""x[%d]"" % i)\n\n    inputs.body.append(\'label = ""inputs""\')\n    inputs.body.append(\'color = ""white""\')\n\n    for i in range(3):\n        hidden.node(""h1[%d]"" % i)\n\n    for i in range(3):\n        hidden2.node(""h2[%d]"" % i)\n\n    hidden.body.append(\'label = ""hidden layer 1""\')\n    hidden.body.append(\'color = ""white""\')\n\n    hidden2.body.append(\'label = ""hidden layer 2""\')\n    hidden2.body.append(\'color = ""white""\')\n\n    output.node(""y"")\n    output.body.append(\'label = ""output""\')\n    output.body.append(\'color = ""white""\')\n\n    nn_graph.subgraph(inputs)\n    nn_graph.subgraph(hidden)\n    nn_graph.subgraph(hidden2)\n\n    nn_graph.subgraph(output)\n\n    for i in range(4):\n        for j in range(3):\n            nn_graph.edge(""x[%d]"" % i, ""h1[%d]"" % j, label="""")\n\n    for i in range(3):\n        for j in range(3):\n            nn_graph.edge(""h1[%d]"" % i, ""h2[%d]"" % j, label="""")\n\n    for i in range(3):\n        nn_graph.edge(""h2[%d]"" % i, ""y"", label="""")\n\n    return nn_graph\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_pca.py,5,"b'from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.externals.joblib import Memory\n\nmemory = Memory(cachedir=""cache"")\n\n\ndef plot_pca_illustration():\n    rnd = np.random.RandomState(5)\n    X_ = rnd.normal(size=(300, 2))\n    X_blob = np.dot(X_, rnd.normal(size=(2, 2))) + rnd.normal(size=2)\n\n    pca = PCA()\n    pca.fit(X_blob)\n    X_pca = pca.transform(X_blob)\n\n    S = X_pca.std(axis=0)\n\n    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n    axes = axes.ravel()\n\n    axes[0].set_title(""Original data"")\n    axes[0].scatter(X_blob[:, 0], X_blob[:, 1], c=X_pca[:, 0], linewidths=0,\n                    s=60, cmap=\'viridis\')\n    axes[0].set_xlabel(""feature 1"")\n    axes[0].set_ylabel(""feature 2"")\n    axes[0].arrow(pca.mean_[0], pca.mean_[1], S[0] * pca.components_[0, 0],\n                  S[0] * pca.components_[0, 1], width=.1, head_width=.3,\n                  color=\'k\')\n    axes[0].arrow(pca.mean_[0], pca.mean_[1], S[1] * pca.components_[1, 0],\n                  S[1] * pca.components_[1, 1], width=.1, head_width=.3,\n                  color=\'k\')\n    axes[0].text(-1.5, -.5, ""Component 2"", size=14)\n    axes[0].text(-4, -4, ""Component 1"", size=14)\n    axes[0].set_aspect(\'equal\')\n\n    axes[1].set_title(""Transformed data"")\n    axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=X_pca[:, 0], linewidths=0,\n                    s=60, cmap=\'viridis\')\n    axes[1].set_xlabel(""First principal component"")\n    axes[1].set_ylabel(""Second principal component"")\n    axes[1].set_aspect(\'equal\')\n    axes[1].set_ylim(-8, 8)\n\n    pca = PCA(n_components=1)\n    pca.fit(X_blob)\n    X_inverse = pca.inverse_transform(pca.transform(X_blob))\n\n    axes[2].set_title(""Transformed data w/ second component dropped"")\n    axes[2].scatter(X_pca[:, 0], np.zeros(X_pca.shape[0]), c=X_pca[:, 0],\n                    linewidths=0, s=60, cmap=\'viridis\')\n    axes[2].set_xlabel(""First principal component"")\n    axes[2].set_aspect(\'equal\')\n    axes[2].set_ylim(-8, 8)\n\n    axes[3].set_title(""Back-rotation using only first component"")\n    axes[3].scatter(X_inverse[:, 0], X_inverse[:, 1], c=X_pca[:, 0],\n                    linewidths=0, s=60, cmap=\'viridis\')\n    axes[3].set_xlabel(""feature 1"")\n    axes[3].set_ylabel(""feature 2"")\n    axes[3].set_aspect(\'equal\')\n    axes[3].set_xlim(-8, 4)\n    axes[3].set_ylim(-8, 4)\n\n\ndef plot_pca_whitening():\n    rnd = np.random.RandomState(5)\n    X_ = rnd.normal(size=(300, 2))\n    X_blob = np.dot(X_, rnd.normal(size=(2, 2))) + rnd.normal(size=2)\n\n    pca = PCA(whiten=True)\n    pca.fit(X_blob)\n    X_pca = pca.transform(X_blob)\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 10))\n    axes = axes.ravel()\n\n    axes[0].set_title(""Original data"")\n    axes[0].scatter(X_blob[:, 0], X_blob[:, 1], c=X_pca[:, 0], linewidths=0, s=60, cmap=\'viridis\')\n    axes[0].set_xlabel(""feature 1"")\n    axes[0].set_ylabel(""feature 2"")\n    axes[0].set_aspect(\'equal\')\n\n    axes[1].set_title(""Whitened data"")\n    axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=X_pca[:, 0], linewidths=0, s=60, cmap=\'viridis\')\n    axes[1].set_xlabel(""First principal component"")\n    axes[1].set_ylabel(""Second principal component"")\n    axes[1].set_aspect(\'equal\')\n    axes[1].set_xlim(-3, 4)\n\n\n@memory.cache\ndef pca_faces(X_train, X_test):\n    # copy and pasted from nmf. refactor?\n    # Build NMF models with 10, 50, 100, 500 components\n    # this list will hold the back-transformd test-data\n    reduced_images = []\n    for n_components in [10, 50, 100, 500]:\n        # build the NMF model\n        pca = PCA(n_components=n_components)\n        pca.fit(X_train)\n        # transform the test data (afterwards has n_components many dimensions)\n        X_test_pca = pca.transform(X_test)\n        # back-transform the transformed test-data\n        # (afterwards it\'s in the original space again)\n        X_test_back = pca.inverse_transform(X_test_pca)\n        reduced_images.append(X_test_back)\n    return reduced_images\n\n\ndef plot_pca_faces(X_train, X_test, image_shape):\n    reduced_images = pca_faces(X_train, X_test)\n\n    # plot the first three images in the test set:\n    fix, axes = plt.subplots(3, 5, figsize=(15, 12),\n                             subplot_kw={\'xticks\': (), \'yticks\': ()})\n    for i, ax in enumerate(axes):\n        # plot original image\n        ax[0].imshow(X_test[i].reshape(image_shape),\n                     vmin=0, vmax=1)\n        # plot the four back-transformed images\n        for a, X_test_back in zip(ax[1:], reduced_images):\n            a.imshow(X_test_back[i].reshape(image_shape), vmin=0, vmax=1)\n\n    # label the top row\n    axes[0, 0].set_title(""original image"")\n    for ax, n_components in zip(axes[0, 1:], [10, 50, 100, 500]):\n        ax.set_title(""%d components"" % n_components)\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_rbf_svm_parameters.py,0,"b'import matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom .plot_2d_separator import plot_2d_separator\nfrom .tools import make_handcrafted_dataset\nfrom .plot_helpers import discrete_scatter\n\n\ndef plot_svm(log_C, log_gamma, ax=None):\n    X, y = make_handcrafted_dataset()\n    C = 10. ** log_C\n    gamma = 10. ** log_gamma\n    svm = SVC(kernel=\'rbf\', C=C, gamma=gamma).fit(X, y)\n    if ax is None:\n        ax = plt.gca()\n    plot_2d_separator(svm, X, ax=ax, eps=.5)\n    # plot data\n    discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    # plot support vectors\n    sv = svm.support_vectors_\n    # class labels of support vectors are given by the sign of the dual coefficients\n    sv_labels = svm.dual_coef_.ravel() > 0\n    discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3, ax=ax)\n    ax.set_title(""C = %.4f gamma = %.4f"" % (C, gamma))\n\n\ndef plot_svm_interactive():\n    from IPython.html.widgets import interactive, FloatSlider\n    C_slider = FloatSlider(min=-3, max=3, step=.1, value=0, readout=False)\n    gamma_slider = FloatSlider(min=-2, max=2, step=.1, value=0, readout=False)\n    return interactive(plot_svm, log_C=C_slider, log_gamma=gamma_slider)\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_ridge.py,1,"b'import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.model_selection import learning_curve, KFold\n\nfrom .datasets import load_extended_boston\n\n\ndef plot_learning_curve(est, X, y):\n    training_set_size, train_scores, test_scores = learning_curve(\n        est, X, y, train_sizes=np.linspace(.1, 1, 20), cv=KFold(20, shuffle=True, random_state=1))\n    estimator_name = est.__class__.__name__\n    line = plt.plot(training_set_size, train_scores.mean(axis=1), \'--\',\n                    label=""training "" + estimator_name)\n    plt.plot(training_set_size, test_scores.mean(axis=1), \'-\',\n             label=""test "" + estimator_name, c=line[0].get_color())\n    plt.xlabel(\'Training set size\')\n    plt.ylabel(\'Score (R^2)\')\n    plt.ylim(0, 1.1)\n\n\ndef plot_ridge_n_samples():\n    X, y = load_extended_boston()\n\n    plot_learning_curve(Ridge(alpha=1), X, y)\n    plot_learning_curve(LinearRegression(), X, y)\n    plt.legend(loc=(0, 1.05), ncol=2, fontsize=11)\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_scaling.py,2,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import (StandardScaler, MinMaxScaler, Normalizer,\n                                   RobustScaler)\nfrom .plot_helpers import cm2\n\n\ndef plot_scaling():\n    X, y = make_blobs(n_samples=50, centers=2, random_state=4, cluster_std=1)\n    X += 3\n\n    plt.figure(figsize=(15, 8))\n    main_ax = plt.subplot2grid((2, 4), (0, 0), rowspan=2, colspan=2)\n\n    main_ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm2, s=60)\n    maxx = np.abs(X[:, 0]).max()\n    maxy = np.abs(X[:, 1]).max()\n\n    main_ax.set_xlim(-maxx + 1, maxx + 1)\n    main_ax.set_ylim(-maxy + 1, maxy + 1)\n    main_ax.set_title(""Original Data"")\n    other_axes = [plt.subplot2grid((2, 4), (i, j))\n                  for j in range(2, 4) for i in range(2)]\n\n    for ax, scaler in zip(other_axes, [StandardScaler(), RobustScaler(),\n                                       MinMaxScaler(), Normalizer(norm=\'l2\')]):\n        X_ = scaler.fit_transform(X)\n        ax.scatter(X_[:, 0], X_[:, 1], c=y, cmap=cm2, s=60)\n        ax.set_xlim(-2, 2)\n        ax.set_ylim(-2, 2)\n        ax.set_title(type(scaler).__name__)\n\n    other_axes.append(main_ax)\n\n    for ax in other_axes:\n        ax.spines[\'left\'].set_position(\'center\')\n        ax.spines[\'right\'].set_color(\'none\')\n        ax.spines[\'bottom\'].set_position(\'center\')\n        ax.spines[\'top\'].set_color(\'none\')\n        ax.xaxis.set_ticks_position(\'bottom\')\n        ax.yaxis.set_ticks_position(\'left\')\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plot_tree_nonmonotonous.py,0,"b'import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom .tools import discrete_scatter\nfrom .plot_2d_separator import plot_2d_separator\n\n\ndef plot_tree_not_monotone():\n    import graphviz\n    # make a simple 2d dataset\n    X, y = make_blobs(centers=4, random_state=8)\n    y = y % 2\n    plt.figure()\n    discrete_scatter(X[:, 0], X[:, 1], y)\n    plt.legend([""Class 0"", ""Class 1""], loc=""best"")\n\n    # learn a decision tree model\n    tree = DecisionTreeClassifier(random_state=0).fit(X, y)\n    plot_2d_separator(tree, X, linestyle=""dashed"")\n\n    # visualize the tree\n    export_graphviz(tree, out_file=""mytree.dot"", impurity=False, filled=True)\n    with open(""mytree.dot"") as f:\n        dot_graph = f.read()\n    print(""Feature importances: %s"" % tree.feature_importances_)\n    return graphviz.Source(dot_graph)\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/plots.py,0,"b'from .plot_linear_svc_regularization import plot_linear_svc_regularization\nfrom .plot_interactive_tree import plot_tree_progressive, plot_tree_partition\nfrom .plot_animal_tree import plot_animal_tree\nfrom .plot_rbf_svm_parameters import plot_svm\nfrom .plot_knn_regression import plot_knn_regression\nfrom .plot_knn_classification import plot_knn_classification\nfrom .plot_2d_separator import plot_2d_classification, plot_2d_separator\nfrom .plot_nn_graphs import (plot_logistic_regression_graph,\n                             plot_single_hidden_layer_graph,\n                             plot_two_hidden_layer_graph)\nfrom .plot_linear_regression import plot_linear_regression_wave\nfrom .plot_tree_nonmonotonous import plot_tree_not_monotone\nfrom .plot_scaling import plot_scaling\nfrom .plot_pca import plot_pca_illustration, plot_pca_whitening, plot_pca_faces\nfrom .plot_decomposition import plot_decomposition\nfrom .plot_nmf import plot_nmf_illustration, plot_nmf_faces\nfrom .plot_helpers import cm2, cm3\nfrom .plot_agglomerative import plot_agglomerative, plot_agglomerative_algorithm\nfrom .plot_kmeans import plot_kmeans_algorithm, plot_kmeans_boundaries, plot_kmeans_faces\nfrom .plot_improper_preprocessing import plot_improper_processing, plot_proper_processing\nfrom .plot_cross_validation import (plot_threefold_split, plot_group_kfold,\n                                    plot_shuffle_split, plot_cross_validation,\n                                    plot_stratified_cross_validation)\n\nfrom .plot_grid_search import plot_grid_search_overview, plot_cross_val_selection\nfrom .plot_metrics import (plot_confusion_matrix_illustration,\n                           plot_binary_confusion_matrix,\n                           plot_decision_threshold)\nfrom .plot_dbscan import plot_dbscan\nfrom .plot_ridge import plot_ridge_n_samples\n\n__all__ = [\'plot_linear_svc_regularization\',\n           ""plot_animal_tree"", ""plot_tree_progressive"",\n           \'plot_tree_partition\', \'plot_svm\',\n           \'plot_knn_regression\',\n           \'plot_logistic_regression_graph\',\n           \'plot_single_hidden_layer_graph\',\n           \'plot_two_hidden_layer_graph\',\n           \'plot_2d_classification\',\n           \'plot_2d_separator\',\n           \'plot_knn_classification\',\n           \'plot_linear_regression_wave\',\n           \'plot_tree_not_monotone\',\n           \'plot_scaling\',\n           \'plot_pca_illustration\',\n           \'plot_pca_faces\',\n           \'plot_pca_whitening\',\n           \'plot_decomposition\',\n           \'plot_nmf_illustration\',\n           \'plot_nmf_faces\',\n           \'plot_agglomerative\',\n           \'plot_agglomerative_algorithm\',\n           \'plot_kmeans_boundaries\',\n           \'plot_kmeans_algorithm\',\n           \'plot_kmeans_faces\',\n           \'cm3\', \'cm2\', \'plot_improper_processing\', \'plot_proper_processing\',\n           \'plot_group_kfold\',\n           \'plot_shuffle_split\',\n           \'plot_stratified_cross_validation\',\n           \'plot_threefold_split\',\n           \'plot_cross_validation\',\n           \'plot_grid_search_overview\',\n           \'plot_cross_val_selection\',\n           \'plot_confusion_matrix_illustration\',\n           \'plot_binary_confusion_matrix\',\n           \'plot_decision_threshold\',\n           \'plot_dbscan\',\n           \'plot_ridge_n_samples\'\n           ]\n'"
Data Exploring/Data Exploring 12-25-17/mglearn/tools.py,12,"b'import numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.tree import export_graphviz\nimport matplotlib.pyplot as plt\nfrom .plot_2d_separator import (plot_2d_separator, plot_2d_classification,\n                                plot_2d_scores)\nfrom .plot_helpers import cm2 as cm, discrete_scatter\n\n\ndef visualize_coefficients(coefficients, feature_names, n_top_features=25):\n    """"""Visualize coefficients of a linear model.\n\n    Parameters\n    ----------\n    coefficients : nd-array, shape (n_features,)\n        Model coefficients.\n\n    feature_names : list or nd-array of strings, shape (n_features,)\n        Feature names for labeling the coefficients.\n\n    n_top_features : int, default=25\n        How many features to show. The function will show the largest (most\n        positive) and smallest (most negative)  n_top_features coefficients,\n        for a total of 2 * n_top_features coefficients.\n    """"""\n    coefficients = coefficients.squeeze()\n    if coefficients.ndim > 1:\n        # this is not a row or column vector\n        raise ValueError(""coeffients must be 1d array or column vector, got""\n                         "" shape {}"".format(coefficients.shape))\n    coefficients = coefficients.ravel()\n\n    if len(coefficients) != len(feature_names):\n        raise ValueError(""Number of coefficients {} doesn\'t match number of""\n                         ""feature names {}."".format(len(coefficients),\n                                                    len(feature_names)))\n    # get coefficients with large absolute values\n    coef = coefficients.ravel()\n    positive_coefficients = np.argsort(coef)[-n_top_features:]\n    negative_coefficients = np.argsort(coef)[:n_top_features]\n    interesting_coefficients = np.hstack([negative_coefficients,\n                                          positive_coefficients])\n    # plot them\n    plt.figure(figsize=(15, 5))\n    colors = [cm(1) if c < 0 else cm(0)\n              for c in coef[interesting_coefficients]]\n    plt.bar(np.arange(2 * n_top_features), coef[interesting_coefficients],\n            color=colors)\n    feature_names = np.array(feature_names)\n    plt.subplots_adjust(bottom=0.3)\n    plt.xticks(np.arange(1, 1 + 2 * n_top_features),\n               feature_names[interesting_coefficients], rotation=60,\n               ha=""right"")\n    plt.ylabel(""Coefficient magnitude"")\n    plt.xlabel(""Feature"")\n\n\ndef heatmap(values, xlabel, ylabel, xticklabels, yticklabels, cmap=None,\n            vmin=None, vmax=None, ax=None, fmt=""%0.2f""):\n    if ax is None:\n        ax = plt.gca()\n    # plot the mean cross-validation scores\n    img = ax.pcolor(values, cmap=cmap, vmin=vmin, vmax=vmax)\n    img.update_scalarmappable()\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_xticks(np.arange(len(xticklabels)) + .5)\n    ax.set_yticks(np.arange(len(yticklabels)) + .5)\n    ax.set_xticklabels(xticklabels)\n    ax.set_yticklabels(yticklabels)\n    ax.set_aspect(1)\n\n    for p, color, value in zip(img.get_paths(), img.get_facecolors(),\n                               img.get_array()):\n        x, y = p.vertices[:-2, :].mean(0)\n        if np.mean(color[:3]) > 0.5:\n            c = \'k\'\n        else:\n            c = \'w\'\n        ax.text(x, y, fmt % value, color=c, ha=""center"", va=""center"")\n    return img\n\n\ndef make_handcrafted_dataset():\n    # a carefully hand-designed dataset lol\n    X, y = make_blobs(centers=2, random_state=4, n_samples=30)\n    y[np.array([7, 27])] = 0\n    mask = np.ones(len(X), dtype=np.bool)\n    mask[np.array([0, 1, 5, 26])] = 0\n    X, y = X[mask], y[mask]\n    return X, y\n\n\ndef print_topics(topics, feature_names, sorting, topics_per_chunk=6,\n                 n_words=20):\n    for i in range(0, len(topics), topics_per_chunk):\n        # for each chunk:\n        these_topics = topics[i: i + topics_per_chunk]\n        # maybe we have less than topics_per_chunk left\n        len_this_chunk = len(these_topics)\n        # print topic headers\n        print((""topic {:<8}"" * len_this_chunk).format(*these_topics))\n        print((""-------- {0:<5}"" * len_this_chunk).format(""""))\n        # print top n_words frequent words\n        for i in range(n_words):\n            try:\n                print((""{:<14}"" * len_this_chunk).format(\n                    *feature_names[sorting[these_topics, i]]))\n            except:\n                pass\n        print(""\\n"")\n\n\ndef get_tree(tree, **kwargs):\n    try:\n        # python3\n        from io import StringIO\n    except ImportError:\n        # python2\n        from StringIO import StringIO\n    f = StringIO()\n    export_graphviz(tree, f, **kwargs)\n    import graphviz\n    return graphviz.Source(f.getvalue())\n\n__all__ = [\'plot_2d_separator\', \'plot_2d_classification\', \'plot_2d_scores\',\n           \'cm\', \'visualize_coefficients\', \'print_topics\', \'heatmap\',\n           \'discrete_scatter\']\n'"
notebook - pandas/Uber Data Analysis/python/2017-24-11-so-uber-analysis-pandas.py,0,"b'\n# coding: utf-8\n\n# ## Uber Data Analysis \n# \n# I\'ll be showing you how to analyze some real data, in this case we are going to do some analysis on Uber data.\n# \n# I am gonna show you how to graph something like Location data, you\'ll be apply to produce the image below after you finished this tutorial:\n# \n# <img style=""float:left;"" src=""https://i.imgur.com/fWM9sw9.png""></img>\n\n# In[1]:\n\n\nimport pandas as pd\nimport numpy as np\n\n\n# In[30]:\n\n\ndata = pd.read_csv(""data/uber_data.csv"")\n\n\n# In[31]:\n\n\ndata.head()\n\n\n# In[32]:\n\n\ndata[\'Date/Time\'] = data[\'Date/Time\'].map(pd.to_datetime)\n\n\n# In[33]:\n\n\ndata.tail()\n\n\n# In[34]:\n\n\n# let\'s get the days of the month \ndef day_of_month(dt):\n    return dt.day\n\ndata[\'DayOM\'] = data[\'Date/Time\'].map(day_of_month)\n\n\n# In[35]:\n\n\ndata.head()\n\n\n# In[36]:\n\n\ndef week_day(dt):\n    return dt.weekday()\n\ndata[\'WeekDay\'] = data[\'Date/Time\'].map(week_day)\n\n\n# In[37]:\n\n\ndef get_hour(dt):\n    return dt.hour\n\n\n# In[38]:\n\n\ndata[\'hour\'] = data[\'Date/Time\'].map(get_hour)\n\ndata.tail()\n\n\n# In[41]:\n\n\n#let\'s do some analysis\nimport matplotlib.pyplot as plt\nplt.hist(data.DayOM, bins=30, rwidth=.8, range=(0.5, 30.5))\nplt.xlabel(\'date of the month\')\nplt.ylabel(\'frequency\')\nplt.title(\'Frequency by D-o-M - uber - Apr 2014\')\nplt.show()\n\n\n# In[43]:\n\n\ndef count_rows(rows):\n    return len(rows)\n\nby_date = data.groupby(\'DayOM\').apply(count_rows)\nby_date.head()\n\n\n# In[46]:\n\n\nplt.bar(range(1, 31), by_date)\nplt.show()\n\n\n# In[48]:\n\n\nby_date_sorted = by_date.sort_values()\nby_date_sorted.head()\n\n\n# In[49]:\n\n\nplt.bar(range(1, 31), by_date_sorted)\nplt.xticks(range(1,31), by_date_sorted.index)\nplt.xlabel(\'date of the month\')\nplt.ylabel(\'frequency\')\nplt.title(\'Frequency by DoM - uber - Apr 2014\')\nplt.show()\n("""")\n\n\n# In[52]:\n\n\n# analysis for week day\nplt.hist(data.WeekDay, bins=7, range =(-.5,6.5), rwidth=.8, color=\'green\', alpha=.4)\nplt.xticks(range(7), \'Mon Tue Wed Thu Fri Sat Sun\'.split())\nplt.show()\n\n\n# In[57]:\n\n\n# plot location\nplt.hist(data[\'Lon\'], bins=100, range = (-74.1, -73.9), color=\'purple\', alpha=.5, label = \'longitude\')\nplt.grid()\nplt.legend(loc=\'upper left\')\nplt.twiny()\nplt.hist(data[\'Lat\'], bins=100, range = (40.5, 41), color=\'g\', alpha=.5, label = \'latitude\')\nplt.legend(loc=\'best\')\nplt.show()\n("""")\n\n\n# In[56]:\n\n\nplt.figure(figsize=(20, 20))\nplt.plot(data[\'Lon\'], data[\'Lat\'], \'.\', ms=1, alpha=.5, color=\'purple\')\nplt.xlim(-74.2, -73.7)\nplt.ylim(40.7, 41)\nplt.show()\n\n\n# # more to come, soon!\n'"
