file_path,api_count,code
geolib.py,0,"b'# This provides an alias to ""gaia"", so that people can start using ""geolib""\nfrom gaia import *\nimport gaia.preprocess as preprocess\n'"
setup.py,0,"b'""""""Main builder script for Gaia.""""""\n\nimport sys\nimport re\nfrom setuptools import setup, find_packages\n\nfrom gaia import __version__\n\n\nwith open(\'README.md\') as f:\n    desc = f.read()\n\n# parse requirements file\nwith open(\'requirements.txt\') as f:\n    requires = []       # main requirements\n    extras = {}         # optional requirements\n    current = requires  # current section\n\n    comment = re.compile(\'(^#.*$|\\s+#.*$)\')\n    v26 = re.compile(r\'\\s*;\\s*python_version\\s*<\\s*[\\\'""]2.7[\\\'""]\\s*\')\n    for line in f.readlines():\n        line = line.strip()\n\n        # detect a new optional package section\n        if line.startswith(\'# optional:\'):\n            package = line.split(\':\')[1].strip()\n            extras[package] = []\n            current = extras[package]\n\n        line = comment.sub(\'\', line)\n        if not line:\n            continue\n\n        if v26.search(line):\n            # version 2.6 only\n            if sys.version_info[:2] == (2, 6):\n                line = v26.sub(\'\', line)\n                current.append(line)\n        else:\n            # all other versions\n            current.append(line)\n\nsetup(\n    name=\'gaia\',\n    version=__version__,\n    description=\'A flexible geospatial workflow framework.\',\n    long_description=desc,\n    author=\'Gaia developers\',\n    author_email=\'kitware@kitware.com\',\n    license=\'Apache 2.0\',\n    classifiers=[\n        \'Development Status :: 3 - Alpha\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Operating System :: OS Independent\',\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.6\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Topic :: Scientific/Engineering\'\n    ],\n    keywords=\'geospatial GIS workflow data\',\n    packages=find_packages(exclude=[\'tests*\', \'server*\', \'docs\']),\n    package_data={\n        \'gaia\': [\n            \'conf/*\',\n        ]\n    },\n    # require_python=\'>=2.6\',\n    url=\'https://github.com/OpenDataAnalytics/gaia\',\n    install_requires=requires,\n    # extras_require=extras\n)\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Gaia documentation build configuration file, created by\n# sphinx-quickstart on Tue Mar 10 15:29:13 2015.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n\n#Mock gdal\nfrom mock import Mock as MagicMock\n\nclass Mock(MagicMock):\n    @classmethod\n    def __getattr__(cls, name):\n            return Mock()\n\nMOCK_MODULES = [\'gdal\',  \'gdalconst\', \'osgeo\', \'ogr\', \'osr\', \'osgeo.gdal_array\',\n                \'numpy\', \'pandas\', \'geopandas\', \'psycopg2\', \'PIL\', \'PIL.Image\',\n                \'fiona\', \'sqlalchemy\', \'geoalchemy2\', \'gdalnumeric\']\nsys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.viewcode\',\n]\n\nautodoc_default_flags = [\n    \'show-inheritance\',\n    \'members\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix of source filenames.\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'Gaia\'\ncopyright = u\'2015, Gaia Developers\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'0.1.0\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'0.1.0\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\'_build\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \'default\'\nif not os.environ.get(\'READTHEDOCS\', None):\n    import sphinx_rtd_theme\n    html_theme = \'sphinx_rtd_theme\'\n    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\nhtml_extra_path = [\'examples\',]\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'Gaiadoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n  (\'index\', \'Gaia.tex\', u\'Gaia Documentation\',\n   u\'Gaia Developers\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (\'index\', \'gaia\', u\'Gaia Documentation\',\n     [u\'Gaia Developers\'], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  (\'index\', \'Gaia\', u\'Gaia Documentation\',\n   u\'Gaia Developers\', \'Gaia\', \'One line description of project.\',\n   \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n'"
gaia/__init__.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n##############################################################################\n#  Copyright Kitware Inc. and Epidemico Inc.\n#\n#  Licensed under the Apache License, Version 2.0 ( the ""License"" );\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n##############################################################################\nimport importlib\nimport traceback\n\nimport os\nimport pkg_resources as pr\nimport logging\n\nfrom .display import pygeojs_adapter\nfrom .io import writers\nfrom .util import GaiaException\n\n#: Global version number for the package\n__version__ = \'0.0.1a1\'\n\n\ntry:\n    from ConfigParser import ConfigParser\nexcept ImportError:\n    from configparser import ConfigParser\n\nlogger = logging.getLogger(__name__)\n\nbase_dir = os.path.join(\n    os.path.dirname(os.path.realpath(__file__)))\n\n#: Holder for database connection settings\nsqlengines = {}\n\n#: Holder for Gaia settings\nconfig = {}\n\n\ndef connect(\n    girder_url=\'http://localhost:8989\',\n    username=None,\n    password=None,\n    apikey=None,\n    newt_sessionid=None):\n    """"""Initialize a connection to a Girder data management system\n\n    Gaia datasets can be created from girder files and folders using\n    either of these formats:\n\n      gaia.create(\'girder://file/${id}\')\n      gaia.create(\'girder://folder/${id}\')\n\n    The datasets are stored in the girder system, and proxied to gaia.\n\n    :param girder_url: The full path to the Girder instance, for example,\n    \'http://localhost:80\' or \'https://my.girder.com\'.\n    :param username: (string) The name for logging into Girder.\n    :param password: (string) The password for logging into Girder.\n    :apikey: (string) An api key, which can be used instead of username & password.\n    :newt_sessionid: (string) Session token from NEWT web service at NERSC.\n       (Girder must be connected to NEWT service to authenicate.)\n\n    Note that applications can connect to only ONE girder instance for the\n    entire session.\n    """"""\n    from gaia.io import GirderInterface\n    gint = GirderInterface.get_instance()\n    if gint.is_initialized():\n        raise GaiaException(\'GirderInterface already initialized.\')\n    gint.initialize(girder_url, username=username, password=password,\n        apikey=apikey, newt_sessionid=newt_sessionid)\n    return gint\n\ndef create(data_source, *args, **kwargs):\n    """"""\n    Convenience method to provide a simpler API for creating\n    GaiaDataObject\n\n    :param data_source: the source data for the object. Can be one of:\n      * a path (string) on local filesystem\n      * a web url (string) that Gaia can download from\n      * a python object (numpy array, GeoPandas dataframe, etc.)\n      * TBD a tuple indicating postgis parameters\n      * a 2-tuple specifying a GirderInterface object and path(string) to the file\n    :return: Gaia data obkject\n    """"""\n    from gaia.io import readers\n    reader = readers.GaiaReader(data_source, *args, **kwargs)\n    return reader.read()\n\n\ndef get_abspath(inpath):\n    """"""\n    Get absolute path of a path string\n\n    :param inpath: file path string\n    :return: absolute path as string\n    """"""\n    if not os.path.isabs(inpath):\n        return os.path.abspath(os.path.join(base_dir, inpath))\n    else:\n        return inpath\n\n\ndef get_config(config_file=None):\n    """"""\n    Retrieve app configuration parameters\n    such as database connections\n\n    :return: configuration\n    """"""\n    global config\n    if not config_file:\n        if config:\n            return config\n        config_file = os.path.join(base_dir, \'conf/gaia.cfg\')\n    parser = ConfigParser()\n    parser.read(config_file)\n    config_dict = {}\n    for section in parser.sections():\n        config_dict[section] = {}\n        for key, val in parser.items(section):\n            config_dict[section][key] = val.strip(\'""\').strip(""\'"")\n    config = config_dict\n    return config_dict\n\n\ndef get_plugins():\n    """"""\n    Load and return a list of installed plugin modules\n\n    :return: list of plugin modules\n    """"""\n    installed_plugins = []\n    for ep in pr.iter_entry_points(group=\'gaia.plugins\'):\n        try:\n            module = ep.load()\n            importlib.import_module(module.__name__)\n            installed_plugins.append(module)\n            if hasattr(module, \'get_config\'):\n                config.update(module.get_config())\n        except ImportError:\n            logger.error(\'Could not load module: {}\'.format(\n                traceback.print_exc()))\n    return installed_plugins\n\n\ndef show(*data_objects, **options):\n    """"""\n    Displays data objects using available rendering code\n\n    :param data_objects: 1 or more Gaia data objects\n    :param options: options to pass to rendering backend\n    :return scene object from display backend\n\n    Note: gaia.show() only renders if it is the\n    last line of code in the cell input.\n    """"""\n    if not data_objects:\n        print(\'(no data objects)\')\n        return None\n\n    # Is jupyterlab_geojs available?\n    if pygeojs_adapter.is_loaded():\n        scene = pygeojs_adapter.show(*data_objects, **options)\n        return scene\n\n    # (else)\n    print(data_objects)\n    return None\n\ndef save(data_object, filename, **options):\n    """"""Writes data object to specified file.\n\n    :param data_object: GaiaDataObject instance\n    :param filename: filesystem path\n    :param options: options to pass to writing backend\n    :return boolean indicating success\n    """"""\n    return writers.write_gaia_object(data_object, filename, **options)\n\ndef submit_crop(data_object, geometry_object, nersc_repository):\n    """"""Submits processing job to NERSC HPC machine\n\n    Current support is (only) for girder-hosted datasets\n\n    :param data_object: GirderDataObject to be cropped\n    :param geometry_object: GaiaDataObject specifying the crop geometry\n    :param nersc_repository: (string) accounting repository (e.g., m1234)\n    :return job_id (string) that can be used for tracking and creating\n      new GaiaDataObject when job is complete.\n    """"""\n    # Hand off to cumulus interface\n    from gaia.io.cumulus_interface import CumulusInterface\n    cumulus_interface = CumulusInterface()\n    return cumulus_interface.submit_crop(data_object, geometry_object, nersc_repository)\n\nget_config()\n'"
gaia/core.py,0,b'import gaia\nfrom gaia import config\nfrom gaia.util import GaiaException\n\n# Leave this here for backwards compatibility\n'
gaia/filters.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n###############################################################################\n#  Copyright Kitware Inc. and Epidemico Inc.\n#\n#  Licensed under the Apache License, Version 2.0 ( the ""License"" );\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\nimport operator\n\n#: Dict of mathematical operators for equations\nops = {\n    ""="": operator.eq,\n    ""!="": operator.ne,\n    "">"": operator.gt,\n    "">="": operator.ge,\n    ""<"": operator.lt,\n    ""<="": operator.le,\n}\n\n\ndef filter_pandas(df, filters):\n    r""""""\n    Filter a GeoPandas DataFrame, return a new filtered DataFrame.\n    Currently all filters are joined by \'AND\'\n    TODO: Support for \'OR\', parentheses?\n\n    :param df: The DataFrame to filter\n    :param filters: An array of (attribute, operator, value) arrays\\\n    for example [(\'city\', \'in\', [\'Boston\', \'New York\']), (\'id\', \'>\', 10)]\n    :return: A filtered DataFrame\n    """"""\n    for filter in filters:\n        attribute = filter[0]\n        operator = filter[1]\n        values = filter[2]\n        if operator.lower() == ""in"":\n            df = df[df[attribute].isin(values)]\n        elif operator.lower() == ""not in"":\n            df = df[~df[attribute].isin(values)]\n        elif operator.lower() in [""contains"", ""startswith"", ""endswith""]:\n            str_func = getattr(df[attribute].str, operator.lower())\n            df = df[str_func(r\'{}\'.format(values))]\n        elif operator in ops.keys():\n            df = df[ops[operator](df[attribute], values)]\n    return df\n\n\ndef filter_postgis(filters):\n    r""""""\n    Generate a SQL statement to be used as a WHERE clause.\n    TODO: Support parentheses?\n\n    :param filters: list of filters in the form of\n    (attribute, operator, values [, join option (AND, OR)])\\\n    for example [(\'city\', \'in\', [\'Boston\', \'New York\']), (\'id\', \'>\', 10)]\n    :return: SQL string and list of parameters\n    """"""\n    sql_filters = None\n    sql_params = []\n    sql_joiner = \' AND \'\n    for filter in filters:\n        attribute = filter[0]\n        operator = filter[1]\n        values = filter[2]\n        if len(filter) > 3:\n            sql_joiner = filter[3]\n        if type(values) in (list, tuple):\n            sql_filter = \'""{}"" {} (\'.format(attribute, operator) + \',\'.join(\n                [\'%s\' for x in values]) + \')\'\n            sql_params.extend(values)\n        else:\n            sql_filter = \'""{}"" {} %s\'.format(attribute, operator)\n            sql_params.append(values)\n        if not sql_filters:\n            sql_filters = sql_filter\n        else:\n            sql_filters = sql_filters + sql_joiner + sql_filter\n    return sql_filters, sql_params\n'"
gaia/formats.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n###############################################################################\n#  Copyright Kitware Inc. and Epidemico Inc.\n#\n#  Licensed under the Apache License, Version 2.0 ( the ""License"" );\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\nimport itertools\n\n""""""\nLists of acceptable file extensions by format, for example:\nJSON = [\'.json\', \'.geojson\']\nRASTER = [\'.tif\', \'.tiff\', \'.geotif\', \'.geotiff\']\n""""""\n#: File extensions for GeoJSON files\nJSON = [\'.json\', \'.geojson\']\n#: File extension for shapefiles\nSHP = [\'.shp\']\n#: File extension for pandas dataframes\nPANDAS = [\'pandas\']\n#: File extensions for all vector datasets\nVECTOR = list(itertools.chain.from_iterable([JSON, SHP, PANDAS]))\n#: File extensions for raster datasets\nGEOTIFF = [\'.tif\', \'.tiff\', \'.geotif\', \'.geotiff\']\nPNG = [\'.png\']\nJPG = [\'.jpg\', \'.jpeg\']\nNITF = [\'.NTF\', \'.ntf\']\nRASTER = GEOTIFF + PNG + JPG + NITF\n#: File extensions for all datasets\nALL = VECTOR + RASTER\n#: File extensions for text-based datasets\nTEXT = list(itertools.chain.from_iterable([JSON]))\n#: File extensions for bindary datasets\nBINARY = list(itertools.chain.from_iterable([RASTER, SHP]))\n'"
gaia/gaia_data.py,0,"b'from __future__ import absolute_import, division, print_function\nfrom builtins import (\n    bytes, str, open, super, range, zip, round, input, int, pow, object\n)\n\nfrom sqlalchemy import create_engine, MetaData, Table, text\nfrom geoalchemy2 import Geometry\nimport fiona\nimport geopandas\ntry:\n    import osr\nexcept ImportError:\n    from osgeo import osr\n\nfrom gaia.filters import filter_postgis\nfrom gaia.geo.gdal_functions import gdal_reproject\nfrom gaia.util import GaiaException, sqlengines\n\n\nclass GaiaDataObject(object):\n    def __init__(self, reader=None, dataFormat=None, epsg=None, **kwargs):\n        self._data = None\n        self._metadata = None\n        self._reader = reader\n        self._datatype = None\n        self._dataformat = dataFormat\n        self._epsg = epsg\n\n    def get_metadata(self):\n        if not self._metadata:\n            self._reader.load_metadata(self)\n        return self._metadata\n\n    def set_metadata(self, metadata):\n        self._metadata = metadata\n\n    def get_data(self):\n        if self._data is None:\n            self._reader.load_data(self)\n        return self._data\n\n    def set_data(self, data):\n        self._data = data\n\n    def get_epsg(self):\n        return self._epsg\n\n    def reproject(self, epsg):\n        repro = geopandas.GeoDataFrame.copy(self.get_data())\n        repro[repro.geometry.name] = repro.geometry.to_crs(epsg=epsg)\n        repro.crs = fiona.crs.from_epsg(epsg)\n        self._data = repro\n        self._epsg = epsg\n\n        # Recompute bounds\n        geometry = repro[\'geometry\']\n        geopandas_bounds = geometry.total_bounds\n        xmin, ymin, xmax, ymax = geopandas_bounds\n        coords = [[\n            [xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]\n        ]]\n        metadata = self.get_metadata()\n        bounds = metadata.get(\'bounds\', {})\n        bounds[\'coordinates\'] = coords\n        metadata[\'bounds\'] = bounds\n        self.set_metadata(metadata)\n\n    def _getdatatype(self):\n        if not self._datatype:\n            self.get_metadata()\n            if not self._datatype:\n                self._datatype = self._metadata.get(\'type_\', \'unknown\')\n\n        return self._datatype\n\n    def _setdatatype(self, value):\n        self._datatype = value\n\n    datatype = property(_getdatatype, _setdatatype)\n\n    def _getdataformat(self):\n        if not self._dataformat:\n            self.get_metadata()\n\n        return self._dataformat\n\n    def _setdataformat(self, value):\n        self._dataformat = value\n\n    dataformat = property(_getdataformat, _setdataformat)\n\n\nclass GDALDataObject(GaiaDataObject):\n    def __init__(self, reader=None, **kwargs):\n        super(GDALDataObject, self).__init__(**kwargs)\n        self._reader = reader\n        self._epsgComputed = False\n\n    def get_epsg(self):\n        if not self._epsgComputed:\n            if not self._data:\n                self.get_data()\n\n            projection = self._data.GetProjection()\n            data_crs = osr.SpatialReference(wkt=projection)\n\n            try:\n                self.epsg = int(data_crs.GetAttrValue(\'AUTHORITY\', 1))\n                self._epsgComputed = True\n            except KeyError:\n                raise GaiaException(""EPSG code coud not be determined"")\n\n        return self.epsg\n\n    def reproject(self, epsg):\n        self._data = gdal_reproject(self._data, \'\', epsg=epsg)\n        self.epsg = epsg\n\n\nclass PostgisDataObject(GaiaDataObject):\n    def __init__(self, reader=None, **kwargs):\n        super(PostgisDataObject, self).__init__(**kwargs)\n\n        self._reader = reader\n\n        self._table = None\n        self._hostname = None\n        self._dbname = None\n        self._user = None\n        self._password = None\n        self._columns = []\n        self._filters = None\n        self._geom_column = \'the_geom\'\n        self._epsg = None\n        self._meta = None\n        self._table_obj = None\n\n    # Define table property\n    def _settable(self, table):\n        self._table = table\n\n    def _gettable(self):\n        return self._table\n\n    table = property(_gettable, _settable)\n\n    # Define hostname property\n    def _sethostname(self, hostname):\n        self._hostname = hostname\n\n    def _gethostname(self):\n        return self._hostname\n\n    hostname = property(_gethostname, _sethostname)\n\n    # Define db property\n    def _setdbname(self, dbname):\n        self._dbname = dbname\n\n    def _getdbname(self):\n        return self._dbname\n\n    dbname = property(_getdbname, _setdbname)\n\n    # Define user property\n    def _setuser(self, user):\n        self._user = user\n\n    def _getuser(self):\n        return self._user\n\n    user = property(_getuser, _setuser)\n\n    # Define password property\n    def _setpassword(self, password):\n        self._password = password\n\n    def _getpassword(self):\n        return self._password\n\n    password = property(_getpassword, _setpassword)\n\n    # Define epsg property\n    def _setepsg(self, epsg):\n        self._epsg = epsg\n\n    def _getepsg(self):\n        return self._epsg\n\n    epsg = property(_getepsg, _setepsg)\n\n    # Define filters property\n    def _setfilters(self, filters):\n        self._filters = filters\n\n    def _getfilters(self):\n        return self._filters\n\n    filters = property(_getfilters, _setfilters)\n\n    # Define geom_column property\n    def _setgeom_column(self, geom_column):\n        self._geom_column = geom_column\n\n    def _getgeom_column(self):\n        return self._geom_column\n\n    geom_column = property(_getgeom_column, _setgeom_column)\n\n    # Define engine property\n    def _setengine(self, engine):\n        self._engine = engine\n\n    def _getengine(self):\n        return self._engine\n\n    engine = property(_getengine, _setengine)\n\n    # etc...\n\n    def initialize_engine(self):\n        self._engine = self.get_engine(self.get_connection_string())\n\n        self.get_table_info()\n        self.verify()\n\n    # methods additional in PostgisIO\n\n    def get_engine(self, connection_string):\n        """"""\n        Create and return a SQLAlchemy engine object\n\n        :param connection_string: Database connection string\n        :return: SQLAlchemy Engine object\n        """"""\n        if connection_string not in sqlengines:\n            sqlengines[connection_string] = create_engine(\n                self.get_connection_string())\n        return sqlengines[connection_string]\n\n    def verify(self):\n        """"""\n        Make sure that all PostgisIO columns exist in the actual table\n        """"""\n        for col in self._columns:\n            if col not in self._table_obj.columns.keys():\n                raise GaiaException(\'{} column not found in {}\'.format(\n                    col, self._table_obj))\n\n    def get_connection_string(self):\n        """"""\n        Get connection string based on host, dbname, username, password\n\n        :return: Postgres connection string for SQLAlchemy\n        """"""\n        auth = \'\'\n        if self._user:\n            auth = self._user\n        if self._password:\n            auth = auth + \':\' + self._password\n        if auth:\n            auth += \'@\'\n        conn_string = \'postgresql://{auth}{host}/{dbname}\'.format(\n            auth=auth, host=self._hostname, dbname=self._dbname)\n\n        return conn_string\n\n    def get_epsg(self):\n        """"""\n        Get the EPSG code of the data\n\n        :return: EPSG code\n        """"""\n        return self._epsg\n\n    def get_table_info(self):\n        """"""\n        Use SQLALchemy reflection to gather data on the table, including the\n        geometry column, geometry type, and EPSG code, and assign to the\n        PostgisIO object\'s attributes.\n        """"""\n        epsg = None\n        meta = MetaData()\n        table_obj = Table(self._table, meta,\n                          autoload=True, autoload_with=self._engine)\n        if not self._columns:\n            self._columns = table_obj.columns.keys()\n        geo_cols = [(col.name, col.type) for col in table_obj.columns\n                    if hasattr(col.type, \'srid\')]\n        if geo_cols:\n            geo_col = geo_cols[0]\n            self._geom_column = geo_col[0]\n            geo_obj = geo_col[1]\n            if self._geom_column not in self._columns:\n                self._columns.append(self._geom_column)\n            if hasattr(geo_obj, \'srid\'):\n                epsg = geo_obj.srid\n                if epsg == -1:\n                    epsg = 4326\n            if hasattr(geo_obj, \'geometry_type\'):\n                self._geometry_type = geo_obj.geometry_type\n\n        self._epsg = epsg\n        self._table_obj = table_obj\n        self._meta = meta\n\n    def get_geometry_type(self):\n        """"""\n        Get the geometry type of the data\n\n        :return: Geometry type\n        """"""\n        return self._geometry_type\n\n    def get_query(self):\n        """"""\n        Formulate a query string and parameter list based on the\n        table name, columns, and filter\n\n        :return: Query string\n        """"""\n        columns = \',\'.join([\'""{}""\'.format(x) for x in self._columns])\n        query = \'SELECT {} FROM ""{}""\'.format(columns, self._table)\n        filter_params = []\n        if self._filters:\n            filter_sql, filter_params = filter_postgis(self._filters)\n            query += \' WHERE {}\'.format(filter_sql)\n        query += \';\'\n        return str(text(query)), filter_params\n'"
gaia/girder_data.py,0,"b'from __future__ import absolute_import, division, print_function\nfrom builtins import (\n    bytes, str, open, super, range, zip, round, input, int, pow, object\n)\nimport json\nimport urllib\n\nfrom gaia.gaia_data import GaiaDataObject\nfrom gaia.io.girder_interface import GirderInterface\n\n\nclass GirderDataObject(GaiaDataObject):\n    """"""Proxies either a file or a folder on girder\n\n    """"""\n    def __init__(self, reader, resource_type, resource_id, **kwargs):\n        super(GirderDataObject, self).__init__(**kwargs)\n        """"""\n\n        Optional bounds input provided because some girder assetstores\n        are not configured for geometa\n        """"""\n        self._reader = reader\n        self.resource_type = resource_type\n        self.resource_id = resource_id\n        self.mapnik_style = None\n        self.bounds = kwargs.get(\'bounds\')\n        # print(\'Created girder object, resource_id: {}\'.format(resource_id))\n\n    def get_metadata(self, force=False):\n        if force or not self._metadata:\n            gc = GirderInterface._get_girder_client()\n            metadata = gc.get(\'item/{}/geometa\'.format(self.resource_id))\n            # print(\'returned metadata: {}\'.format(metadata))\n            self._metadata = metadata\n\n            if self.bounds:\n                geom = self._bounds_to_geom(self.bounds)\n                self._metadata[\'bounds\'] = geom\n        return self._metadata\n\n    def set_mapnik_style(self, style):\n        """"""A convenience method for applying mapnik styles for large-image\n\n        Example style object:\n            {\n                \'band\': 1,\n                \'max\': 5000,\n                \'min\': 2000,\n                \'palette\': \'matplotlib.Plasma_6\',\n                \'scheme\': \'linear\'\n            }\n\n        """"""\n        self.mapnik_style = style\n\n    def _get_tiles_url(self):\n        """"""Constructs url for large_image display\n\n        Returns None for non-raster datasets\n        """"""\n        if self._getdatatype() != \'raster\':\n            return None\n\n        # (else)\n        girder_url = GirderInterface.get_instance().girder_url\n        base_url = \'{}/api/v1/item/{}/tiles/zxy/{{z}}/{{x}}/{{y}}\'.format(\n            girder_url, self.resource_id)\n        mapnik_string = \'\'\n        if self.mapnik_style:\n            if isinstance(self.mapnik_style, str):\n                style_string = self.mapnik_style\n            else:\n                style_string = json.dumps(self.mapnik_style)\n            encoded_string = urllib.parse.quote_plus(style_string)\n            mapnik_string = \'&style={}\'.format(encoded_string)\n        tiles_url = \'{}?encoding=PNG&projection=EPSG:3857{}\'.format(\n            base_url, mapnik_string)\n\n        # print(\'Using tiles_url:\', tiles_url)\n        return tiles_url\n\n    def _bounds_to_geom(self, bounds):\n        """"""Converts bounds as [xmin,ymin,xmax,ymax] to geojson polygon\n\n        :bounds: list of doubles xmin, ymin, xmax, ymax\n\n        For internal use. Needed for certain assetstores which either don\'t\n        provide bounds or provide incorrect bounds data.\n        """"""\n        xmin, ymin, xmax, ymax = bounds\n        coords = [[\n            [xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]\n        ]]\n        geom = {\n            \'coordinates\': coords,\n            \'type\': \'Polygon\'\n        }\n        return geom\n'"
gaia/parser.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n###############################################################################\n#  Copyright Kitware Inc. and Epidemico Inc.\n#\n#  Licensed under the Apache License, Version 2.0 ( the ""License"" );\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\nimport argparse\nimport importlib\nimport inspect\nimport json\nimport logging\nimport gaia.geo\nfrom gaia import get_plugins\nfrom gaia.gaia_process import GaiaProcess\nfrom gaia.inputs import GaiaIO\n\nlogger = logging.getLogger(__name__)\n\n\ndef add_to_dict(x):\n    class_name = \'{}.{}\'.format(x[1].__module__, x[1].__name__)\n    if issubclass(x[1], GaiaProcess):\n        if x[1].required_inputs:\n            valid_processes.append(\n                {class_name: {y: getattr(x[1], y) for y in (\n                    \'required_inputs\', \'required_args\',\n                    \'optional_args\', \'default_output\')}})\n    elif issubclass(x[1], GaiaIO):\n        valid_inputs.append({class_name: {y: getattr(x[1], y) for y in (\n            \'type\', \'default_output\')}})\n\n\nvalid_processes = []\nvalid_inputs = []\n\nfor mod in (gaia.geo.geo_inputs, gaia.inputs, gaia.geo):\n    for x in inspect.getmembers(mod, inspect.isclass):\n        add_to_dict(x)\nfor plugin in get_plugins():\n    for x in inspect.getmembers(plugin, inspect.isclass):\n        if x[1] in plugin.PLUGIN_CLASS_EXPORTS:\n            add_to_dict(x)\nvalid_classes = [list(x.keys())[0].split(\'.\')[-1] for x in valid_inputs] +\\\n                [list(y.keys())[0].split(\'.\')[-1] for y in valid_processes]\n\n\ndef deserialize(dct):\n    """"""\n    Convert a JSON representation of a Gaia IO or Process\n    into a Python object of the appropriate class\n    :param dct: The JSON object\n    :return: An object of class which the JSON represents\n    """"""\n    if ""_type"" in dct.keys():\n        cls_name = dct[\'_type\'].split(""."")[-1]\n        module_name = ""."".join(dct[\'_type\'].split(""."")[:-1])\n        cls = getattr(importlib.import_module(module_name), cls_name)\n        if cls_name not in valid_classes:\n            raise ImportError(\n                \'Not allowed to create class {}\'.format(cls_name))\n        del dct[\'_type\']\n        args = dct.get(\'args\') or []\n        if args:\n            del dct[\'args\']\n        return cls(*args, **dct)\n    return dct\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Run geospatial process.\')\n    parser.add_argument(\'jsonfile\', default=None,\n                        help=\'JSON file to parse\')\n    args = parser.parse_args()\n    with open(args.jsonfile) as infile:\n            process = json.load(infile, object_hook=deserialize)\n    process.compute()\n    if hasattr(process, \'output\') and hasattr(process.output, \'uri\'):\n        print(""Result saved to {}"".format(process.output.uri))\n    else:\n        print(\'Process complete.\')\n'"
gaia/process_registry.py,0,"b'from __future__ import absolute_import, division, print_function\nfrom builtins import (\n    bytes, str, open, super, range, zip, round, input, int, pow, object\n)\n\nfrom gaia import GaiaException\n\n\n""""""\nA process is just two stateless methods, \'validate\' and \'compute\',\nassociated with a process name.\n""""""\n\n""""""\nBy making this is a module-level variable, anyone who imports the module\nhas access to the single, global registry.  This avoids the need for some\nbootstrapping code that instantiates a single registry and makes it\navailable to the whole application.\n""""""\n__process_registry = {}\n\n\ndef find_processes(processName):\n    """"""\n    Return a list of registry entries that implement the named process.\n    """"""\n    if processName in __process_registry:\n        return __process_registry[processName]\n    return None\n\n\ndef register_process(processName):\n    """"""\n    Return a process registration decorator\n    """"""\n    def processRegistrationDecorator(computeMethod):\n        if processName not in __process_registry:\n            __process_registry[processName] = []\n        __process_registry[processName].append(computeMethod)\n        return computeMethod\n    return processRegistrationDecorator\n\n\ndef list_processes(processName=None):\n    """"""\n    Display a list of the processes in the registry, for debugging or\n    informational purposes.\n    """"""\n    def display_processes(name, plist):\n        print(\'%s processes:\' % name)\n        for item in plist:\n            print(item)\n\n    if processName is not None:\n        if processName in __process_registry:\n            display_processes(processName, __process_registry[processName])\n        else:\n            print(\'No processes registered for %s\' % processName)\n    else:\n        for pName in __process_registry:\n            display_processes(pName, __process_registry[pName])\n\n\ndef compute(processName, inputs, args):\n    """"""\n    Just looks up a process that can do the job and asks it to \'compute\'\n    """"""\n    processes = find_processes(processName)\n\n    if not processes:\n        list_processes(processName)\n        raise GaiaException(\'Unable to find suitable %s process\' % processName)\n\n    for p in processes:\n        # How will we choose between equally ""valid"" processes?  For now\n        # just return the first one.\n        try:\n            return p(inputs, args)\n        except GaiaException:\n            pass\n\n    raise GaiaException(\'No registered processes were able to validate inputs\')\n'"
gaia/types.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n###############################################################################\n#  Copyright Kitware Inc. and Epidemico Inc.\n#\n#  Licensed under the Apache License, Version 2.0 ( the ""License"" );\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\n\nVECTOR = ""vector""\nRASTER = ""raster""\nPROCESS = ""process""\n'"
gaia/util.py,0,"b'from __future__ import absolute_import, division, print_function\nfrom builtins import (\n    bytes, str, open, super, range, zip, round, input, int, pow, object\n)\n\nsqlengines = {}\n\n\nclass GaiaException(Exception):\n    """"""\n    Base Gaia exception class\n    """"""\n    pass\n\n\nclass MissingParameterError(GaiaException):\n    """"""Raise when a required parameter is missing""""""\n    pass\n\n\nclass MissingDataException(GaiaException):\n    """"""Raise when required data is missing""""""\n    pass\n\n\nclass UnhandledOperationException(GaiaException):\n    """"""Raise when required data is missing""""""\n    pass\n\n\nclass UnsupportedFormatException(GaiaException):\n    """"""Raise when an unsupported data format is used""""""\n    pass\n\n\ndef get_uri_extension(uripath):\n    idx = uripath.rfind(\'.\')\n    if idx >= 0:\n        return uripath[idx + 1:]\n    return None\n'"
gaia/validate_base.py,0,"b'from __future__ import absolute_import, division, print_function\nfrom builtins import (\n    bytes, str, open, super, range, zip, round, input, int, pow, object\n)\n\nfrom gaia import formats\nfrom gaia.gaia_data import GaiaDataObject\nfrom gaia.util import GaiaException\nimport gaia.types as types\n\n\ndef test_arg_type(args, arg, arg_type):\n    """"""\n    Try to cast a process argument to its required type. Raise an\n    exception if not successful.\n    :param arg: The argument property\n    :param arg_type: The required argument type (int, str, etc)\n    """"""\n    try:\n        arg_type(args[arg])\n    except Exception:\n        raise GaiaException(\'Required argument {} must be of type {}\'\n                            .format(arg, arg_type))\n\n\ndef validate_base(inputs, args, required_inputs=[], required_args=[],\n                  optional_args=[]):\n    """"""\n    Ensure that all required inputs and arguments are present.\n    """"""\n    input_types = []\n    errors = []\n\n    for procInput in inputs:\n        if not isinstance(procInput, GaiaDataObject):\n            raise GaiaException(\'Not a GaiaDataObject\')\n\n        inputDataType = procInput._getdatatype()\n        if inputDataType == types.PROCESS:\n            for t in [i for i in dir(types) if not i.startswith(""__"")]:\n                if any((True for x in procInput.default_output if x in getattr(\n                        formats, t, []))):\n                    inputDataType = getattr(types, t)\n                    break\n        input_types.append(inputDataType)\n\n    for i, req_input in enumerate(required_inputs):\n        if i >= len(input_types):\n            errors.append(""Not enough inputs for process"")\n        elif req_input[\'type\'] != input_types[i]:\n            errors.append(""Input #{} is of incorrect type."".format(i+1))\n\n    if len(input_types) > len(required_inputs):\n        if (required_inputs[-1][\'max\'] is not None and\n            len(input_types) > len(required_inputs) +\n                required_inputs[-1][\'max\']-1):\n            errors.append(""Incorrect # of inputs; expected {}"".format(\n                len(required_inputs)))\n        else:\n            for i in range(len(required_inputs)-1, len(input_types)):\n                if input_types[i] != required_inputs[-1][\'type\']:\n                    errors.append(\n                        ""Input #{} is of incorrect type."".format(i + 1))\n    if errors:\n        raise GaiaException(\'\\n\'.join(errors))\n    for item in required_args:\n        arg, arg_type = item[\'name\'], item[\'type\']\n        if arg not in args or args[arg] is None:\n            raise GaiaException(\'Missing required argument {}\'.format(arg))\n        test_arg_type(args, arg, arg_type)\n        if \'options\' in item and args[arg] not in item[\'options\']:\n            raise GaiaException(\'Invalid value for {}\'.format(item[\'name\']))\n    for item in optional_args:\n        arg, arg_type = item[\'name\'], item[\'type\']\n        if arg in optional_args and optional_args[arg] is not None:\n            test_arg_type(optional_args, arg, arg_type)\n            argval = args[arg]\n            if \'options\' in item and argval not in item[\'options\']:\n                raise GaiaException(\n                    \'Invalid value for {}\'.format(item[\'name\']))\n'"
gaia/validators.py,0,"b'from __future__ import absolute_import, division, print_function\nfrom builtins import (\n    bytes, str, open, super, range, zip, round, input, int, pow, object\n)\n\nimport gaia.types as types\nfrom gaia.validate_base import validate_base\n\n\n# """"""\n# Decorator for validating centroid process inputs.  This can be reused by both\n# the pandas and postgis centroid implementations, while a different one would\n# be required in the case of a gdal/raster centroid.\n# """"""\n# def validate_centroid(v):\n#     def centroid_validator(inputs=[], args={}):\n#         required_inputs = [{\n#             \'description\': \'Line/Polygon dataset\',\n#             \'type\': types.VECTOR,\n#             \'max\': 1\n#         }]\n\n#         optional_args = [{\n#             \'name\': \'combined\',\n#             \'title\': \'Combined\',\n#             \'description\': \'Get centroid of features (default False)\',\n#             \'type\': bool,\n#         }]\n\n#         validate_base(inputs, args, required_inputs=required_inputs,\n#                       optional_args=optional_args)\n#         return v(inputs, args)\n\n#     return centroid_validator\n\n\n# """"""\n# Decorator for validating intersection process inputs\n# """"""\n# def validate_intersection(v):\n#     def intersection_validator(inputs=[], args={}):\n#         required_inputs=[{\n#             \'description\': \'Feature dataset\',\n#             \'type\': types.VECTOR,\n#             \'max\': 1\n#         },{\n#             \'description\': \'Intersect dataset\',\n#             \'type\': types.VECTOR,\n#             \'max\': 1\n#         }]\n\n#         validate_base(inputs, args, required_inputs=required_inputs)\n#         return v(inputs, args)\n\n#     return intersection_validator\n\n\ndef validate_within(v):\n    """"""\n    Decorator for validating within process inputs\n    """"""\n    def within_validator(inputs=[], args={}):\n        required_inputs = [{\n            \'description\': \'Feature dataset\',\n            \'type\': types.VECTOR,\n            \'max\': 1\n        }, {\n            \'description\': \'Within dataset\',\n            \'type\': types.VECTOR,\n            \'max\': 1\n        }]\n\n        validate_base(inputs, args, required_inputs=required_inputs)\n        return v(inputs, args)\n\n    return within_validator\n\n\ndef validate_subset(v):\n    """"""\n    Decorator for validating subset process inputs\n    """"""\n    def subset_validator(inputs=[], args=[]):\n        required_inputs = [{\n            \'description\': \'Image to subset\',\n            \'type\': types.RASTER,\n            \'max\': 1\n        }, {\n            \'description\': \'Subset area:\',\n            \'type\': types.VECTOR,\n            \'max\': 1\n        }]\n\n        validate_base(inputs, args, required_inputs=required_inputs)\n        return v(inputs, args)\n\n    return subset_validator\n'"
tests/__init__.py,0,b''
ansible/library/girder.py,0,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n###############################################################################\n#  Copyright Kitware Inc.\n#\n#  Licensed under the Apache License, Version 2.0 ( the ""License"" );\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\n\n# Ansible\'s module magic requires this to be\n# \'from ansible.module_utils.basic import *\' otherwise it will error out. See:\n# https://github.com/ansible/ansible/blob/v1.9.4-1/lib/ansible/module_common.py#L41-L59\n# For more information on this magic. For now we noqa to prevent flake8 errors\nfrom ansible.module_utils.basic import *  # noqa\nfrom inspect import getmembers, ismethod, getargspec\n\ntry:\n    from girder_client import GirderClient, AuthenticationError, HttpError\n    HAS_GIRDER_CLIENT = True\nexcept ImportError:\n    HAS_GIRDER_CLIENT = False\n\n\n__version__ = ""0.2.0""\n\nDOCUMENTATION = \'\'\'\n---\nmodule: girder\nauthor: ""Chris Kotfila (chris.kotfila@kitware.com)\nversion_added: ""0.1""\nshort_description: A module that wraps girder_client\nrequirements: [ girder_client==1.1.0 ]\ndescription:\n   - Manage a girder instance using the RESTful API\noptions:\n    host:\n        required: false\n        default: \'localhost\'\n        description:\n            - domain or IP of the host running girder\n    port:\n        required: false\n        default: \'80\' for http, \'443\' for https\n        description:\n            - port the girder instance is running on\n\n    apiRoot:\n        required: false\n        default: \'/api/v1\'\n        description:\n            - path on server corresponding to the root of Girder REST API\n\n    scheme:\n        required: false\n        default: \'http\'\n        description:\n            - A string containing the scheme for the Girder host\n\n    dryrun:\n        required: false\n        default: None (passed through)\n        description:\n            - See GirderClient.__init__()\n\n    blacklist:\n        required: false\n        default: None (passed through)\n        description:\n            - See GirderClient.__init__()\n\n    username:\n        required: true\n        description:\n            - Valid username for the system\n            - Required with password\n            - must be specified if \'token\' is not specified\n                - (See note on \'user\')\n\n    password:\n        required: true\n        description:\n            - Valid password for the system\n            - Required with username\n            - must be specified if \'token\' is not specified\n                - (See note on \'user\')\n    token:\n        required: true\n        description:\n            - A girder client token\n            - Can be retrieved by accessing the accessing the \'token\' attribute\n              from a successfully authenticated call to girder in a previous\n              task.\n            - Required if \'username\' and \'password\' are not specified\n                - (See note on \'user\')\n    state:\n        required: false\n        default: ""present""\n        choices: [""present"", ""absent""]\n        description:\n            - Used to indicate the presence or absence of a resource\n              - e.g.,  user, plugin, assetstore\n\n    user:\n        required: false\n        description:\n            - If using the \'user\' task, you are NOT REQUIRED to pass in a\n              \'username\' & \'password\',  or a \'token\' attributes. This is because\n              the first user created on an fresh install of girder is\n              automatically made an administrative user. Once you are certain\n              you have an admin user you should use those credentials in all\n              subsequent tasks that use the \'user\' task.\n\n            - Takes a mapping of key value pairs\n              options:\n                  login:\n                      required: true\n                      description:\n                          - The login name of the user\n                  password:\n                      required: true\n                      description:\n                          - The password of the user\n\n                  firstName:\n                      required: false\n                      default: pass through to girder client\n                      description:\n                          - The first name of the user\n\n                  lastName:\n                      required: false\n                      default: pass through to girder client\n                      description:\n                          - The last name of the user\n                  email:\n                      required: false\n                      default: pass through to girder client\n                      description:\n                          - The email of the user\n                  admin:\n                      required: false\n                      default: false\n                      description:\n                          - If true,  make the user an administrator.\n\n\n    plugin:\n        required: false\n        description:\n            - Specify what plugins should be activated (state: present)\n              or deactivated (state: absent).\n            - Takes a list of plugin names,  incorrect names are silently\n              ignored\n\n    assetstore:\n        required: false\n        description:\n            - Specifies an assetstore\n            - Takes many options depending on \'type\'\n              options:\n                  name:\n                      required: true\n                      description:\n                          - Name of the assetstore\n                  type:\n                      required: true\n                      choices: [\'filesystem\', \'gridfs\', \'s3\', \'hdfs\']\n                      description:\n                          - Currently only \'filesystem\' has been tested\n                  readOnly:\n                      required: false\n                      default: false\n                      description:\n                          - Should the assetstore be read only?\n                  current:\n                      required: false\n                      default: false\n                      description:\n                          - Should the assetstore be set as the current\n                            assetstore?\n\n              options (filesystem):\n                  root:\n                      required: true\n                      description:\n                          -  Filesystem path to the assetstore\n\n              options (gridfs) (EXPERIMENTAL):\n                   db:\n                       required: true\n                       description:\n                           - database name\n                   mongohost:\n                       required: true\n                       description:\n                           - Mongo host URI\n\n                   replicaset:\n                       required: false\n                       default: \'\'\n                       description:\n                           - Replica set name\n\n              options (s3) (EXPERIMENTAL):\n                   bucket:\n                       required: true\n                       description:\n                           - The S3 bucket to store data in\n\n                   prefix:\n                       required: true\n                       description:\n                           - Optional path prefix within the bucket under which\n                             files will be stored\n\n                   accessKeyId:\n                       required: true\n                       description:\n                           - the AWS access key ID to use for authentication\n\n                   secret:\n                       required: true\n                       description:\n                           - the AWS secret key to use for authentication\n\n                   service:\n                       required: false\n                       default: s3.amazonaws.com\n                       description:\n                           - The S3 service host (for S3 type)\n                           - This can be used to specify a protocol and port\n                             -  use the form [http[s]://](host domain)[:(port)]\n                           - Do not include the bucket name here\n\n              options (hdfs) (EXPERIMENTAL):\n                   host:\n                       required: true\n                       description:\n                           - None\n                   port:\n                       required: true\n                       description:\n                           - None\n                   path:\n                       required: true\n                       description:\n                           - None\n                   user:\n                       required: true\n                       description:\n                           - None\n                   webHdfsPort\n                       required: true\n                       description:\n                           - None\n\n    group:\n        required: false\n        description:\n            - Create a group with pre-existing users\n        options:\n            name:\n                required: true\n                description:\n                    - Name of the group\n\n            description:\n                required: false\n                description:\n                    - Description of the group\n            users:\n                required: false\n                type: list\n                description:\n                    - List of dicts with users login and their level\n                options:\n                    login:\n                        required: true\n                        description:\n                            - the login name\n                    type:\n                        required: true\n                        choices: [""member"", ""moderator"", ""admin""]\n                        description:\n                            - Access level for that user in the group\n\n    collection:\n        required: false\n        description:\n            - Create a collection\n        options:\n            name:\n                required: true\n                description:\n                    - Name of the collection\n\n            description:\n                required: false\n                description:\n                    - Description of the collection\n\n            folders:\n                required: false\n                description:\n                    - A list of folder options\n                    - Specified by the \'folder\' option to the girder module\n                    - (see \'folder:\')\n            access:\n                required: false\n                description:\n                    - Set the access for the collection/folder\n                options:\n                    users:\n                        required: false\n                        description:\n                            - list of login/type arguments\n                            - login is a user login\n                            - type is one of \'admin\', \'moderator\', \'member\'\n                    groups:\n                        required: false\n                        description:\n                            - list of name/type arguments\n                            - name is a group name\n                            - type is one of \'admin\', \'moderator\', \'member\'\n\n    folder:\n        required: false\n        description:\n            - Create a folder\n        options:\n            name:\n                required: true\n                description:\n                    - Name of the folder\n\n            description:\n                required: false\n                description:\n                    - Description of the folder\n            parentType:\n                required: true\n                choices: [""user"", ""folder"", ""collection""]\n                description:\n                    - The type of the parent\n            parentId:\n                required: true\n                description:\n                    - The ID of the parent collection\n            folders:\n                required: false\n                description:\n                    - A list of folder options\n                    - Specified by the \'folder\' option to the girder module\n                    - (see \'folder:\')\n            access:\n                required: false\n                description:\n                    - Set the access for the collection/folder\n                options:\n                    users:\n                        required: false\n                        description:\n                            - list of login/type arguments\n                            - login is a user login\n                            - type is one of \'admin\', \'moderator\', \'member\'\n                    groups:\n                        required: false\n                        description:\n                            - list of name/type arguments\n                            - name is a group name\n                            - type is one of \'admin\', \'moderator\', \'member\'\n\n    item:\n        required: false\n        description:\n            - Create a item\n        options:\n            name:\n                required: true\n                description:\n                    - Name of the item\n\n            description:\n                required: false\n                description:\n                    - Description of the item\n            folderId:\n                required: true\n                description:\n                    - The ID of the parent collection\n\n     files:\n        required: false\n        description:\n            - Uploads a list of files to an item\n        options:\n            itemId:\n                required: true\n                description:\n                    - the parent item for the file\n            sources:\n                required: true\n                description:\n                    - list of local file paths\n                    - files will be uploaded to the item\n\n\'\'\'\n\nEXAMPLES = \'\'\'\n\n\n#############\n# Example using \'user\'\n###\n\n\n# Ensure ""admin"" user exists\n- name: Create \'admin\' User\n  girder:\n    user:\n      firstName: ""Chris""\n      lastName: ""Kotfila""\n      login: ""admin""\n      password: ""letmein""\n      email: ""chris.kotfila@kitware.com""\n      admin: yes\n    state: present\n\n# Ensure a \'foobar\' user exists\n- name: Create \'foobar\' User\n  girder:\n    username: ""admin""\n    password: ""letmein""\n    user:\n      firstName: ""Foo""\n      lastName: ""Bar""\n      login: ""foobar""\n      password: ""foobarbaz""\n      email: ""foo.bar@kitware.com""\n      admin: yes\n    state: present\n\n# Remove the \'foobar\' user\n- name: Remove \'foobar\' User\n  username: ""admin""\n  password: ""letmein""\n  girder:\n    user:\n      login: ""foobar""\n      password: ""foobarbaz""\n    state: absent\n\n############\n# Examples using Group\n#\n\n# Create an \'alice\' user\n- name: Create \'alice\' User\n  girder:\n    port: 8080\n    username: ""admin""\n    password: ""letmein""\n    user:\n      firstName: ""Alice""\n      lastName: ""Test""\n      login: ""alice""\n      password: ""letmein""\n      email: ""alice.test@kitware.com""\n    state: present\n\n# Create a \'bill\' user\n- name: Create \'bill\' User\n  girder:\n    port: 8080\n    username: ""admin""\n    password: ""letmein""\n    user:\n      firstName: ""Bill""\n      lastName: ""Test""\n      login: ""bill""\n      password: ""letmein""\n      email: ""bill.test@kitware.com""\n    state: present\n\n# Create a \'chris\' user\n- name: Create \'chris\' User\n  girder:\n    port: 8080\n    username: ""admin""\n    password: ""letmein""\n    user:\n      firstName: ""Chris""\n      lastName: ""Test""\n      login: ""chris""\n      password: ""letmein""\n      email: ""chris.test@kitware.com""\n    state: present\n\n- name: Create a test group with users\n  girder:\n    port: 8080\n    username: ""admin""\n    password: ""letmein""\n    group:\n      name: ""Test Group""\n      description: ""Basic test group""\n      users:\n        - login: alice\n          type: member\n        - login: bill\n          type: moderator\n        - login: chris\n          type: admin\n    state: present\n\n# Remove Bill from the group,\n# Note that \'group\' list is idempotent - it describes the desired state\n\n- name: Remove bill from group\n  girder:\n    port: 8080\n    username: ""admin""\n    password: ""letmein""\n    group:\n      name: ""Test Group""\n      description: ""Basic test group""\n      users:\n        - login: alice\n          type: member\n        - login: chris\n          type: admin\n    state: present\n\n#############\n# Example using \'plugins\'\n###\n\n# To enable or disable all plugins you may pass the ""*""\n# argument.  This does not (yet) support arbitrary regexes\n- name: Disable all plugins\n  girder:\n    username: ""admin""\n    password: ""letmein""\n    plugins: ""*""\n    state: absent\n\n- name: Enable thumbnails plugin\n  girder:\n    username: ""admin""\n    password: ""letmein""\n    port: 8080\n    plugins:\n      - thumbnails\n    state: present\n\n# Note that \'thumbnails\'  is still enabled from the previous task,\n# the \'plugins\' task ensures that plugins are enabled or disabled,\n# it does NOT define the complete list of enabled or disabled plugins.\n- name: Ensure jobs and gravatar plugins are enabled\n  girder:\n    username: ""admin""\n    password: ""letmein""\n    plugins:\n      - jobs\n      - gravatar\n    state: present\n\n\n\n############\n# Filesystem Assetstore Tests\n#\n\n- name: Create filesystem assetstore\n  girder:\n    username: ""admin""\n     password: ""letmein""\n     assetstore:\n       name: ""Temp Filesystem Assetstore""\n       type: ""filesystem""\n       root: ""/data/""\n       current: true\n     state: present\n\n- name: Delete filesystem assetstore\n  girder:\n    username: ""admin""\n    password: ""letmein""\n    assetstore:\n      name: ""Temp Filesystem Assetstore""\n      type: ""filesystem""\n      root: ""/tmp/""\n    state: absent\n\n\n############\n# Examples using collections, folders, items and files\n#\n\n# Creates a test collection called ""Test Collection""\n- name: Create collection\n  girder:\n    port: 8080\n    username: ""admin""\n    password: ""letmein""\n    collection:\n      name: ""Test Collection""\n      description: ""A test collection""\n  register: test_collection\n\n# Creates a folder called ""test folder"" under ""Test Collection""\n- name: Create folder\n  girder:\n    port: 8080\n    username: ""admin""\n    password: ""letmein""\n    folder:\n      parentType: ""collection""\n      parentId: ""{{test_collection[\'gc_return\'][\'_id\'] }}""\n      name: ""test folder""\n      description: ""A test folder""\n  register: test_folder\n\n# Creates an item called ""test item"" under ""test folder""\n- name: Create an item\n  girder:\n    port: 8080\n    username: ""admin""\n    password: ""letmein""\n    item:\n      folderId: ""{{test_folder[\'gc_return\'][\'_id\'] }}""\n      name: ""test item""\n      description: ""A test item""\n  register: test_item\n\n# Upload files on the localhost at /tmp/data/test1.txt and\n# /tmp/data/test2.txt to the girder instance under the item\n# ""test item""\n# Note:  the list is idempotent and will remove files that are\n# not listed under the item. Files are checked for both name\n# and size to determine if they should be updated.\n- name: Upload files\n  girder:\n    port: 8080\n    username: ""admin""\n    password: ""letmein""\n    files:\n      itemId: ""{{ test_item[\'gc_return\'][\'_id\'] }}""\n      sources:\n        - /tmp/data/test1.txt\n        - /tmp/data/test2.txt\n  register: retval\n\n\n############\n# Examples Using collection/folder hierarchy\n#\n\n- name: Create collection with a folder and a subfolder\n  girder:\n    port: 8080\n    username: ""admin""\n    password: ""letmein""\n    collection:\n      name: ""Test Collection""\n      description: ""A test collection""\n      folders:\n        - name: ""test folder""\n          description: ""A test folder""\n          folders:\n            - name: ""test subfolder""\n            - name: ""test subfolder 2""\n  register: test_collection\n\n\n\n############\n# Examples Setting access to files/folders\n#\n\n\n- name: Create collection with access\n  girder:\n    port: 8080\n    username: ""admin""\n    password: ""letmein""\n    collection:\n      name: ""Test Collection""\n      description: ""A test collection""\n      public: no\n      access:\n        users:\n          - login: alice\n            type: admin\n          - login: chris\n            type: member\n  register: test_collection\n\n\n- name: Add group to Test Collection\n  girder:\n    port: 8080\n    username: ""admin""\n    password: ""letmein""\n    collection:\n      name: ""Test Collection""\n      description: ""A test collection""\n      public: no\n      access:\n        users:\n          - login: alice\n            type: admin\n          - login: bill\n            type: moderator\n          - login: chris\n            type: member\n        groups:\n          - name: Test Group\n            type: member\n  register: test_collection\n\n- name: Add Test Folder with access\n  girder:\n    port: 8080\n    username: ""admin""\n    password: ""letmein""\n    folder:\n      parentType: ""collection""\n      parentId: ""{{test_collection[\'gc_return\'][\'_id\'] }}""\n      name: ""test folder""\n      description: ""A test folder""\n      access:\n        users:\n          - login: bill\n            type: admin\n        groups:\n          - name: Test Group\n            type: member\n  register: test_folder\n\n\n\n############\n# Examples using get\n#\n\n\n# Get my info\n- name: Get users from http://localhost:80/api/v1/users\n  girder:\n    username: \'admin\'\n    password: \'letmein\'\n    get:\n      path: ""users""\n    register: ret_val\n\n# Prints debugging messages with the emails of the users\n# From the last task by accessing \'gc_return\' of the registered\n# variable \'ret_val\'\n- name: print emails of users\n  debug: msg=""{{ item[\'email\'] }}""\n  with_items: ""{{ ret_val[\'gc_return\'] }}""\n\n\n#############\n# Advanced usage\n#\n\n# Supports get, post, put, delete methods,  but does\n# not guarantee idempotence on these methods!\n\n- name: Restart the server\n  girder:\n    username: ""admin""\n    password: ""letmein""\n    put:\n      path: ""system/restart""\n\n# An example of posting an item to Girder\n# Note that this is NOT idempotent. Running\n# multiple times will create ""An Item"", ""An Item (1)"",\n# ""An Item (2)"", etc..\n\n- name: Get Me\n  girder:\n    username: ""admin""\n    password: ""letmein""\n    get:\n      path: ""user/me""\n  register: ret\n\n# Show use of \'token\' for subsequent authentication\n- name: Get my public folder\n  girder:\n    token: ""{{ ret[\'token\'] }}""\n    get:\n      path: ""folder""\n      parameters:\n        parentType: ""user""\n        parentId: ""{{ ret[\'gc_return\'][\'_id\'] }}""\n        text: ""Public""\n  register: ret\n\n\n- name: Post an item to my public folder\n  girder:\n    host: ""data.kitware.com""\n    scheme: \'https\'\n    token: ""{{ ret[\'token\'] }}""\n    post:\n      path: ""item""\n      parameters:\n        folderId: ""{{ ret[\'gc_return\'][0][\'_id\'] }}""\n        name: ""An Item""\n\n\n\'\'\'\n\n\ndef class_spec(cls, include=None):\n    include = include if include is not None else []\n\n    for fn, method in getmembers(cls, predicate=ismethod):\n        if fn in include:\n            spec = getargspec(method)\n            # spec.args[1:] so we don\'t include \'self\'\n            params = spec.args[1:]\n            d = len(spec.defaults) if spec.defaults is not None else 0\n            r = len(params) - d\n\n            yield (fn, {""required"": params[:r],\n                        ""optional"": params[r:]})\n\n\nclass Resource(object):\n    known_resources = [\'collection\', \'folder\', \'item\', \'group\']\n\n    def __init__(self, client, resource_type):\n        self._resources = None\n        self._resources_by_name = None\n        self.client = client\n\n        if resource_type in self.known_resources:\n            self.resource_type = resource_type\n        else:\n            raise Exception(""{} is an unknown resource!"".format(resource_type))\n\n    @property\n    def resources(self):\n        if self._resources is None:\n            self._resources = {r[\'_id\']: r for r\n                               in self.client.get(self.resource_type)}\n        return self._resources\n\n    @property\n    def resources_by_name(self):\n        if self._resources_by_name is None:\n            self._resources_by_name = {r[\'name\']: r\n                                       for r in self.resources.values()}\n\n        return self._resources_by_name\n\n    def __apply(self, _id, func, *args, **kwargs):\n        if _id in self.resources.keys():\n            ret = func(""{}/{}"".format(self.resource_type, _id),\n                       *args, **kwargs)\n            self.client.changed = True\n        return ret\n\n    def id_exists(self, _id):\n        return _id in self.resources.keys()\n\n    def name_exists(self, _name):\n        return _name in self.resources_by_name.keys()\n\n    def create(self, body, **kwargs):\n        try:\n            ret = self.client.post(self.resource_type, body, **kwargs)\n            self.client.changed = True\n        except HttpError as htErr:\n            try:\n                # If we can\'t create the item,  try and return\n                # The item with the same name\n                ret = self.resource_by_name[args[\'name\']]  # noqa: F999\n            except KeyError:\n                raise htErr\n        return ret\n\n    def read(self, _id):\n        return self.resources[_id]\n\n    def read_by_name(self, name):\n        return self.resources_by_name[name][\'_id\']\n\n    def update(self, _id, body, **kwargs):\n        if _id in self.resources:\n            current = self.resources[_id]\n            # if body is a subset of current we don\'t actually need to update\n            if set(body.items()) <= set(current.items()):\n                return current\n            else:\n                return self.__apply(_id, self.client.put, body, **kwargs)\n        else:\n            raise Exception(""{} does not exist!"".format(_id))\n\n    def update_by_name(self, name, body, **kwargs):\n        return self.update(self.resources_by_name[name][\'_id\'],\n                           body, **kwargs)\n\n    def delete(self, _id):\n        return self.__apply(_id, self.client.delete)\n\n    def delete_by_name(self, name):\n        try:\n            return self.delete(self.resources_by_name[name][\'_id\'])\n        except KeyError:\n            return {}\n\n\nclass AccessMixin(object):\n\n    def get_access(self, _id):\n        return self.client.get(""{}/{}/access""\n                               .format(self.resource_type, _id))\n\n    def put_access(self, _id, access, public=True):\n        current_access = self.get_access(_id)\n\n        if set([tuple(u.values()) for u in access[\'users\']]) ^ \\\n           set([(u[""id""], u[\'level\']) for u in current_access[\'users\']]):\n            self.client.changed = True\n\n        if set([tuple(g.values()) for g in access[\'groups\']]) ^ \\\n           set([(u[""id""], u[\'level\']) for u in current_access[\'groups\']]):\n            self.client.changed = True\n\n        return self.client.put(""{}/{}/access""\n                               .format(self.resource_type, _id),\n                               dict(access=json.dumps(access),  # noqa: F999\n                                    public=""true"" if public else ""false""))  # noqa: F999\n\n\nclass CollectionResource(AccessMixin, Resource):\n    def __init__(self, client):\n        super(CollectionResource, self).__init__(client, ""collection"")\n\n\nclass GroupResource(Resource):\n    def __init__(self, client):\n        super(GroupResource, self).__init__(client, ""group"")\n\n\nclass FolderResource(AccessMixin, Resource):\n    def __init__(self, client, parentType, parentId):\n        super(FolderResource, self).__init__(client, ""folder"")\n        self.parentType = parentType\n        self.parentId = parentId\n\n    @property\n    def resources(self):\n        if self._resources is None:\n            self._resources = {r[\'_id\']: r for r\n                               in self.client.get(self.resource_type, {\n                                   ""parentType"": self.parentType,\n                                   ""parentId"": self.parentId\n                               })}\n            # parentType is stored as parrentCollection in database\n            # We need parentType to be available so we can do set\n            # comparison to check if we are updating parentType (e.g.\n            # Moving a subfolder from a folder to a collection)\n            for _id in self._resources.keys():\n                self._resources[_id][\'parentType\'] = \\\n                    self._resources[_id][\'parentCollection\']\n        return self._resources\n\n\nclass ItemResource(Resource):\n    def __init__(self, client, folderId):\n        super(ItemResource, self).__init__(client, ""item"")\n        self.folderId = folderId\n\n    @property\n    def resources(self):\n        if self._resources is None:\n            self._resources = {r[\'_id\']: r for r\n                               in self.client.get(self.resource_type, {\n                                   ""folderId"": self.folderId\n                               })}\n        return self._resources\n\n\nclass GirderClientModule(GirderClient):\n\n    # Exclude these methods from both \'raw\' mode\n    _include_methods = [\'get\', \'put\', \'post\', \'delete\',\n                        \'plugins\', \'user\', \'assetstore\',\n                        \'collection\', \'folder\', \'item\', \'files\',\n                        \'group\']\n\n    _debug = True\n\n    def exit(self):\n        if not self._debug:\n            del self.message[\'debug\']\n\n        self.module.exit_json(changed=self.changed, **self.message)\n\n    def fail(self, msg):\n        self.module.fail_json(msg=msg)\n\n    def __init__(self):\n        self.changed = False\n        self.message = {""msg"": ""Success!"", ""debug"": {}}\n\n        self.spec = dict(class_spec(self.__class__,\n                                    GirderClientModule._include_methods))\n        self.required_one_of = self.spec.keys()\n\n        # Note: if additional types are added o girder this will\n        # have to be updated!\n        self.access_types = {""member"": 0, ""moderator"": 1, ""admin"": 2}\n\n    def __call__(self, module):\n        self.module = module\n\n        super(GirderClientModule, self).__init__(\n            **{p: self.module.params[p] for p in\n               [\'host\', \'port\', \'apiRoot\',\n                \'scheme\', \'dryrun\', \'blacklist\']\n               if module.params[p] is not None})\n        # If a username and password are set\n        if self.module.params[\'username\'] is not None:\n            try:\n                self.authenticate(\n                    username=self.module.params[\'username\'],\n                    password=self.module.params[\'password\'])\n\n            except AuthenticationError:\n                self.fail(""Could not Authenticate!"")\n\n        # If a token is set\n        elif self.module.params[\'token\'] is not None:\n            self.token = self.module.params[\'token\']\n\n        # Else error if we\'re not trying to create a user\n        elif self.module.params[\'user\'] is None:\n            self.fail(""Must pass in either username & password, ""\n                      ""or a valid girder_client token"")\n\n        self.message[\'token\'] = self.token\n\n        for method in self.required_one_of:\n            if self.module.params[method] is not None:\n                self.__process(method)\n                self.exit()\n\n        self.fail(""Could not find executable method!"")\n\n    def __process(self, method):\n        # Parameters from the YAML file\n        params = self.module.params[method]\n        # Final list of arguments to the function\n        args = []\n        # Final list of keyword arguments to the function\n        kwargs = {}\n\n        if type(params) is dict:\n            for arg_name in self.spec[method][\'required\']:\n                if arg_name not in params.keys():\n                    self.fail(""%s is required for %s"" % (arg_name, method))\n                args.append(params[arg_name])\n\n            for kwarg_name in self.spec[method][\'optional\']:\n                if kwarg_name in params.keys():\n                    kwargs[kwarg_name] = params[kwarg_name]\n\n        elif type(params) is list:\n            args = params\n        else:\n            args = [params]\n\n        ret = getattr(self, method)(*args, **kwargs)\n\n        self.message[\'debug\'][\'method\'] = method\n        self.message[\'debug\'][\'args\'] = args\n        self.message[\'debug\'][\'kwargs\'] = kwargs\n        self.message[\'debug\'][\'params\'] = params\n\n        self.message[\'gc_return\'] = ret\n\n    def files(self, itemId, sources=None):\n        ret = {""added"": [],\n               ""removed"": []}\n\n        files = self.get(""item/{}/files"".format(itemId))\n\n        if self.module.params[\'state\'] == \'present\':\n\n            file_dict = {f[\'name\']: f for f in files}\n\n            source_dict = {os.path.basename(s): {  # noqa: F999\n                ""path"": s,\n                ""name"": os.path.basename(s),  # noqa: F999\n                ""size"": os.path.getsize(s)} for s in sources}  # noqa: F999\n\n            source_names = set([(s[\'name\'], s[\'size\'])\n                                for s in source_dict.values()])\n\n            file_names = set([(f[\'name\'], f[\'size\'])\n                              for f in file_dict.values()])\n\n            for n, _ in (file_names - source_names):\n                self.delete(""file/{}"".format(file_dict[n][\'_id\']))\n                ret[\'removed\'].append(file_dict[n])\n\n            for n, _ in (source_names - file_names):\n                self.uploadFileToItem(itemId, source_dict[n][\'path\'])\n                ret[\'added\'].append(source_dict[n])\n\n        elif self.module.params[\'state\'] == \'absent\':\n            for f in files:\n                self.delete(""file/{}"".format(f[\'_id\']))\n                ret[\'removed\'].append(f)\n\n        if len(ret[\'added\']) != 0 or len(ret[\'removed\']) != 0:\n            self.changed = True\n\n        return ret\n\n    def _get_user_by_login(self, login):\n        try:\n            user = self.get(""/resource/lookup"",\n                            {""path"": ""/user/{}"".format(login)})\n        except HttpError:\n            user = None\n        return user\n\n    def _get_group_by_name(self, name):\n        try:\n            # Could potentially fail if we have more 50 groups\n            group = {g[\'name\']: g for g in self.get(""group"")}[\'name\']\n        except (KeyError, HttpError):\n            group = None\n        return group\n\n    def group(self, name, description, users=None, debug=False):\n\n        r = GroupResource(self)\n        valid_fields = [(""name"", name),\n                        (""description"", description)]\n\n        if self.module.params[\'state\'] == \'present\':\n            if r.name_exists(name):\n                ret = r.update_by_name(name, {k: v for k, v in valid_fields\n                                              if v is not None})\n            else:\n                ret = r.create({k: v for k, v in valid_fields\n                                if v is not None})\n\n            if users is not None:\n                ret[""added""] = []\n                ret[""removed""] = []\n                ret[""updated""] = []\n\n                group_id = ret[\'_id\']\n\n                # Validate and normalize the user list\n                for user in users:\n                    assert ""login"" in user.keys(), \\\n                        ""User list must have a login attribute""\n\n                    user[\'type\'] = self.access_types.get(\n                        user.get(\'type\', \'member\'), ""member"")\n\n                # dict of passed in login -> type\n                user_levels = {u[\'login\']: u[\'type\'] for u in users}\n\n                # dict of current login -> user information for this group\n                members = {m[\'login\']: m for m in\n                           self.get(\'group/{}/member\'.format(group_id))}\n\n                # Add these users\n                for login in (set(user_levels.keys()) - set(members.keys())):\n                    user = self._get_user_by_login(login)\n                    if user is not None:\n                        # add user at level\n                        self.post(""group/{}/invitation"".format(group_id),\n                                  {""userId"": user[""_id""],\n                                   ""level"": user_levels[login],\n                                   ""quiet"": True,\n                                   ""force"": True})\n                        ret[\'added\'].append(user)\n                    else:\n                        raise Exception(\'{} is not a valid login!\'\n                                        .format(login))\n\n                # Remove these users\n                for login in (set(members.keys()) - set(user_levels.keys())):\n                    self.delete(""/group/{}/member"".format(group_id),\n                                {""userId"": members[login][\'_id\']})\n                    ret[\'removed\'].append(members[login])\n\n                # Set of users that potentially need to be updated\n                if len(set(members.keys()) & set(user_levels.keys())):\n                    group_access = self.get(\'group/{}/access\'.format(group_id))\n                    # dict of current login -> access information for this group\n                    user_access = {m[\'login\']: m\n                                   for m in group_access[\'access\'][\'users\']}\n\n                    # dict of login -> level for the current group\n                    # Note:\n                    #  Here we join members with user_access - if the member\n                    #  is not in user_access then the member has a level of 0 by\n                    #  default. This gives us  a complete list of every login,\n                    #  and its access level, including those that are IN the\n                    #  group, but have no permissions ON the group.\n                    member_levels = {m[\'login\']:\n                                     user_access.get(m[\'login\'],\n                                                     {""level"": 0})[\'level\']\n                                     for m in members.values()}\n\n                    ret = self._promote_or_demote_in_group(ret,\n                                                           member_levels,\n                                                           user_levels,\n                                                           group_id)\n\n                # Make sure \'changed\' is handled correctly if we\'ve\n                # manipulated the group\'s users in any way\n                if (len(ret[\'added\']) != 0 or len(ret[\'removed\']) != 0 or\n                        len(ret[\'updated\']) != 0):\n                    self.changed = True\n\n        elif self.module.params[\'state\'] == \'absent\':\n            ret = r.delete_by_name(name)\n\n        return ret\n\n    def _promote_or_demote_in_group(self, ret, member_levels, user_levels,\n                                    group_id):\n        """"""Promote or demote a set of users.\n\n        :param ret: the current dict of return values\n        :param members_levels: the current access levels of each member\n        :param user_levels: the desired levels of each member\n        :param types: a mapping between resource names and access levels\n        :returns: info about what has (or has not) been updated\n        :rtype: dict\n\n        """"""\n\n        reverse_type = {v: k for k, v in self.access_types.items()}\n\n        for login in (set(member_levels.keys()) &\n                      set(user_levels.keys())):\n            user = self._get_user_by_login(login)\n            _id = user[""_id""]\n\n            # We\'re promoting\n            if member_levels[login] < user_levels[login]:\n                resource = reverse_type[user_levels[login]]\n                self.post(""group/{}/{}""\n                          .format(group_id, resource),\n                          {""userId"": _id})\n\n                user[\'from_level\'] = member_levels[login]\n                user[\'to_level\'] = user_levels[login]\n                ret[\'updated\'].append(user)\n\n            # We\'re demoting\n            elif member_levels[login] > user_levels[login]:\n                resource = reverse_type[member_levels[login]]\n                self.delete(""group/{}/{}""\n                            .format(group_id, resource),\n                            {""userId"": _id})\n\n                # In case we\'re not demoting to member make sure\n                # to update to promote to whatever level we ARE\n                # demoting too now that our user is a only a member\n                if user_levels[login] != 0:\n                    resource = reverse_type[user_levels[login]]\n                    self.post(""group/{}/{}""\n                              .format(group_id, resource),\n                              {""userId"": _id})\n\n                    user[\'from_level\'] = member_levels[login]\n                    user[\'to_level\'] = user_levels[login]\n                    ret[\'updated\'].append(user)\n\n        return ret\n\n    def item(self, name, folderId, description=None, files=None,\n             access=None, debug=False):\n        ret = {}\n        r = ItemResource(self, folderId)\n        valid_fields = [(""name"", name),\n                        (""description"", description),\n                        (\'folderId\', folderId)]\n\n        if self.module.params[\'state\'] == \'present\':\n            if r.name_exists(name):\n                ret = r.update_by_name(name, {k: v for k, v in valid_fields\n                                              if v is not None})\n            else:\n                ret = r.create({k: v for k, v in valid_fields\n                                if v is not None})\n        # handle files here\n\n        elif self.module.params[\'state\'] == \'absent\':\n            ret = r.delete_by_name(name)\n\n        return ret\n\n    def folder(self, name, parentId, parentType, description=None,\n               public=True, folders=None, access=None, debug=False):\n\n        ret = {}\n\n        assert parentType in [\'collection\', \'folder\', \'user\'], \\\n            ""parentType must be collection or folder""\n\n        r = FolderResource(self, parentType, parentId)\n        valid_fields = [(""name"", name),\n                        (""description"", description),\n                        (""parentType"", parentType),\n                        (""parentId"", parentId)]\n\n        if self.module.params[\'state\'] == \'present\':\n            if r.name_exists(name):\n                ret = r.update_by_name(name, {k: v for k, v in valid_fields\n                                              if v is not None})\n            else:\n                valid_fields = valid_fields + [(""public"", public)]\n                ret = r.create({k: v for k, v in valid_fields\n                                if v is not None})\n\n            if folders is not None:\n                self._process_folders(folders, ret[""_id""], ""folder"")\n\n            # handle access here\n            if access is not None:\n                _id = ret[\'_id\']\n                ret[\'access\'] = self._access(r, access, _id, public=public)\n\n        elif self.module.params[\'state\'] == \'absent\':\n            ret = r.delete_by_name(name)\n\n        return ret\n\n    def _access(self, r, access, _id, public=True):\n        access_list = {""users"": [], ""groups"": []}\n        users = access.get(""users"", None)\n        groups = access.get(""groups"", None)\n\n        if groups is not None:\n            assert set(g[\'type\'] for g in groups) <= \\\n                set(self.access_types.keys()), ""Invalid access type!""\n\n            # Hash of name -> group information\n            # used to get user id\'s for access control lists\n            all_groups = {g[\'name\']: g for g in self.get(""group"")}\n\n            access_list[\'groups\'] = [{\'id\': all_groups[g[\'name\']][""_id""],\n                                      \'level\': self.access_types[g[\'type\']]}\n                                     for g in groups]\n\n        if users is not None:\n\n            assert set(u[\'type\'] for u in users) <= \\\n                set(self.access_types.keys()), ""Invalid access type!""\n\n            # Hash of login -> user information\n            # used to get user id\'s for access control lists\n            current_users = {u[\'login\']: self._get_user_by_login(u[\'login\'])\n                             for u in users}\n\n            access_list[\'users\'] = [{\'id\': current_users[u[\'login\']][""_id""],\n                                     ""level"": self.access_types[u[\'type\']]}\n                                    for u in users]\n\n        return r.put_access(_id, access_list, public=public)\n\n    def _process_folders(self, folders, parentId, parentType):\n        """"""Process a list of folders from a user or collection.\n\n        :param folders: List of folders passed as attribute\n                        to user or collection\n        :param parentId: ID of the user or the collection\n        :param parentType: one of \'user\' or \'collection\'\n        :returns: Nothing\n        :rtype: None\n\n        """"""\n\n        current_folders = {f[\'name\']: f for f in\n                           self.get(""folder"", {""parentType"": parentType,\n                                               ""parentId"": parentId})}\n        # Add, update or noop listed folders\n        for folder in folders:\n            # some validation of folder here would be a good idea\n            kwargs = folder.copy()\n            del kwargs[\'name\']\n            self.folder(folder[\'name\'],\n                        parentId=parentId,\n                        parentType=parentType,\n                        **kwargs)\n\n        # Make sure we remove folders not listed\n        for name in (set(current_folders.keys()) -\n                     set([f[\'name\'] for f in folders])):\n\n            original_state = self.module.params[\'state\']\n            self.module.params[\'state\'] = ""absent""\n            self.folder(name,\n                        parentId=parentId,\n                        parentType=parentType)\n            self.module.params[\'state\'] = original_state\n\n    def collection(self, name, description=None,\n                   public=True, access=None, folders=None, debug=False):\n\n        ret = {}\n        r = CollectionResource(self)\n        valid_fields = [(""name"", name),\n                        (""description"", description)]\n\n        if self.module.params[\'state\'] == \'present\':\n            if r.name_exists(name):\n                ret = r.update_by_name(name, {k: v for k, v in valid_fields\n                                              if v is not None})\n            else:\n                ret = r.create({k: v for k, v in valid_fields\n                                if v is not None})\n        if folders is not None:\n            self._process_folders(folders, ret[""_id""], ""collection"")\n\n        if access is not None:\n            _id = ret[\'_id\']\n            ret[\'access\'] = self._access(r, access, _id, public=public)\n\n        elif self.module.params[\'state\'] == \'absent\':\n            ret = r.delete_by_name(name)\n\n        return ret\n\n    def plugins(self, *plugins):\n        import json\n        ret = []\n\n        available_plugins = self.get(""system/plugins"")\n        self.message[\'debug\'][\'available_plugins\'] = available_plugins\n\n        plugins = set(plugins)\n        enabled_plugins = set(available_plugins[\'enabled\'])\n\n        # Could maybe be expanded to handle all regular expressions?\n        if ""*"" in plugins:\n            plugins = set(available_plugins[\'all\'].keys())\n\n        # Fail if plugins are passed in that are not available\n        if not plugins <= set(available_plugins[""all""].keys()):\n            self.fail(""%s, not available!"" %\n                      "","".join(list(plugins -\n                                    set(available_plugins[""all""].keys()))))\n\n        # If we\'re trying to ensure plugins are present\n        if self.module.params[\'state\'] == \'present\':\n            # If plugins is not a subset of enabled plugins:\n            if not plugins <= enabled_plugins:\n                # Put the union of enabled_plugins nad plugins\n                ret = self.put(""system/plugins"",\n                               {""plugins"":\n                                json.dumps(list(plugins | enabled_plugins))})\n                self.changed = True\n\n        # If we\'re trying to ensure plugins are absent\n        elif self.module.params[\'state\'] == \'absent\':\n            # If there are plugins in the list that are enabled\n            if len(enabled_plugins & plugins):\n\n                # Put the difference of enabled_plugins and plugins\n                ret = self.put(""system/plugins"",\n                               {""plugins"":\n                                json.dumps(list(enabled_plugins - plugins))})\n                self.changed = True\n\n        return ret\n\n    def user(self, login, password, firstName=None,\n             lastName=None, email=None, admin=False, folders=None):\n\n        if self.module.params[\'state\'] == \'present\':\n\n            # Fail if we don\'t have firstName, lastName and email\n            for var_name, var in [(\'firstName\', firstName),\n                                  (\'lastName\', lastName), (\'email\', email)]:\n                if var is None:\n                    self.fail(""%s must be set if state ""\n                              ""is \'present\'"" % var_name)\n\n            try:\n                ret = self.authenticate(username=login,\n                                        password=password)\n\n                me = self.get(""user/me"")\n\n                # List of fields that can actually be updated\n                updateable = [\'firstName\', \'lastName\', \'email\', \'admin\']\n                passed_in = [firstName, lastName, email, admin]\n\n                # If there is actually an update to be made\n                if set([(k, v) for k, v in me.items() if k in updateable]) ^ \\\n                   set(zip(updateable, passed_in)):\n\n                    self.put(""user/%s"" % me[\'_id\'],\n                             parameters={\n                                 ""login"": login,\n                                 ""firstName"": firstName,\n                                 ""lastName"": lastName,\n                                 ""password"": password,\n                                 ""email"": email,\n                                 ""admin"": ""true"" if admin else ""false""})\n                    self.changed = True\n            # User does not exist (with this login info)\n            except AuthenticationError:\n                ret = self.post(""user"", parameters={\n                    ""login"": login,\n                    ""firstName"": firstName,\n                    ""lastName"": lastName,\n                    ""password"": password,\n                    ""email"": email,\n                    ""admin"": ""true"" if admin else ""false""\n                })\n                self.changed = True\n\n            if folders is not None:\n                _id = self.get(""resource/lookup"",\n                               {""path"": ""/user/{}"".format(login)})[""_id""]\n                self._process_folders(folders, _id, ""user"")\n\n        elif self.module.params[\'state\'] == \'absent\':\n            ret = []\n            try:\n                ret = self.authenticate(username=login,\n                                        password=password)\n\n                me = self.get(""user/me"")\n\n                self.delete(\'user/%s\' % me[\'_id\'])\n                self.changed = True\n            # User does not exist (with this login info)\n            except AuthenticationError:\n                ret = []\n\n        return ret\n\n    assetstore_types = {\n        ""filesystem"": 0,\n        ""girdfs"": 1,\n        ""s3"": 2,\n        ""hdfs"": ""hdfs""\n    }\n\n    def __validate_hdfs_assetstore(self, *args, **kwargs):\n        # Check if hdfs plugin is available,  enable it if it isn\'t\n        pass\n\n    def assetstore(self, name, type, root=None, db=None, mongohost=None,\n                   replicaset=\'\', bucket=None, prefix=\'\', accessKeyId=None,\n                   secret=None, service=\'s3.amazonaws.com\', host=None,\n                   port=None, path=None, user=None, webHdfsPort=None,\n                   readOnly=False, current=False):\n\n            # Fail if somehow we have an asset type not in assetstore_types\n        if type not in self.assetstore_types.keys():\n            self.fail(""assetstore type %s is not implemented!"" % type)\n\n        argument_hash = {\n            ""filesystem"": {\'name\': name,\n                           \'type\': self.assetstore_types[type],\n                           \'root\': root},\n            ""gridfs"": {\'name\': name,\n                       \'type\': self.assetstore_types[type],\n                       \'db\': db,\n                       \'mongohost\': mongohost,\n                       \'replicaset\': replicaset},\n            ""s3"": {\'name\': name,\n                   \'type\': self.assetstore_types[type],\n                   \'bucket\': bucket,\n                   \'prefix\': prefix,\n                   \'accessKeyId\': accessKeyId,\n                   \'secret\': secret,\n                   \'service\': service},\n            \'hdfs\': {\'name\': name,\n                     \'type\': self.assetstore_types[type],\n                     \'host\': host,\n                     \'port\': port,\n                     \'path\': path,\n                     \'user\': user,\n                     \'webHdfsPort\': webHdfsPort}\n        }\n\n        # Fail if we don\'t have all the required attributes\n        # for this asset type\n        for k, v in argument_hash[type].items():\n            if v is None:\n                self.fail(""assetstores of type ""\n                          ""%s require attribute %s"" % (type, k))\n\n        # Set optional arguments in the hash\n        argument_hash[type][\'readOnly\'] = readOnly\n        argument_hash[type][\'current\'] = current\n\n        ret = []\n        # Get the current assetstores\n        assetstores = {a[\'name\']: a for a in self.get(""assetstore"")}\n\n        self.message[\'debug\'][\'assetstores\'] = assetstores\n\n        # If we want the assetstore to be present\n        if self.module.params[\'state\'] == \'present\':\n\n            # And the asset store exists\n            if name in assetstores.keys():\n\n                id = assetstores[name][\'_id\']\n\n                ####\n                # Fields that could potentially be updated\n                #\n                # This is necessary because there are fields in the assetstores\n                # that do not hash (e.g., capacity) and fields in the\n                # argument_hash that are not returned by \'GET\' assetstore (e.g.\n                # readOnly). We could be more precise about this\n                # (e.g., by only checking items that are relevant to this type)\n                # but readability suffers.\n                updateable = [""root"", ""mongohost"", ""replicaset"", ""bucket"",\n                              ""prefix"", ""db"", ""accessKeyId"", ""secret"",\n                              ""service"", ""host"", ""port"", ""path"", ""user"",\n                              ""webHdfsPort"", ""current""]\n\n                # tuples of (key,  value) for fields that can be updated\n                # in the assetstore\n                assetstore_items = set((k, assetstores[name][k])\n                                       for k in updateable\n                                       if k in assetstores[name].keys())\n\n                # tuples of (key,  value) for fields that can be updated\n                # in the argument_hash for this assetstore type\n                arg_hash_items = set((k, argument_hash[type][k])\n                                     for k in updateable\n                                     if k in argument_hash[type].keys())\n\n                # if arg_hash_items not a proper subset of assetstore_items\n                if not arg_hash_items <= assetstore_items:\n                    # Update\n                    ret = self.put(""assetstore/%s"" % id,\n                                   parameters=argument_hash[type])\n\n                    self.changed = True\n\n            # And the asset store does not exist\n            else:\n                try:\n                    # If __validate_[type]_assetstore exists then call the\n                    # function with argument_hash. E.g.,  to check if the\n                    # HDFS plugin is enabled\n                    getattr(self, ""__validate_%s_assetstore"" % type\n                            )(**argument_hash)\n                except AttributeError:\n                    pass\n\n                ret = self.post(""assetstore"",\n                                parameters=argument_hash[type])\n                self.changed = True\n        # If we want the assetstore to be gone\n        elif self.module.params[\'state\'] == \'absent\':\n            # And the assetstore exists\n            if name in assetstores.keys():\n                id = assetstores[name][\'_id\']\n                ret = self.delete(""assetstore/%s"" % id,\n                                  parameters=argument_hash[type])\n\n        return ret\n\n\ndef main():\n    """"""Entry point for ansible girder client module\n\n    :returns: Nothing\n    :rtype: NoneType\n\n    """"""\n\n    # Default spec for initalizing and authenticating\n    argument_spec = {\n        # __init__\n        \'host\': dict(),\n        \'port\': dict(),\n        \'apiRoot\': dict(),\n        \'scheme\': dict(),\n        \'dryrun\': dict(),\n        \'blacklist\': dict(),\n\n        # authenticate\n        \'username\': dict(),\n        \'password\': dict(),\n        \'token\':    dict(),\n\n        # General\n        \'state\': dict(default=""present"", choices=[\'present\', \'absent\'])\n    }\n\n    gcm = GirderClientModule()\n\n    for method in gcm.required_one_of:\n        argument_spec[method] = dict()\n\n    module = AnsibleModule(    # noqa: F999\n        argument_spec=argument_spec,\n        required_one_of=[gcm.required_one_of,\n                         [""token"", ""username"", ""user""]],\n        required_together=[[""username"", ""password""]],\n        mutually_exclusive=gcm.required_one_of,\n        supports_check_mode=False)\n\n    if not HAS_GIRDER_CLIENT:\n        module.fail_json(msg=""Could not import GirderClient!"")\n\n    try:\n        gcm(module)\n\n    except HttpError as e:\n        import traceback\n        module.fail_json(msg=""%s:%s\\n%s\\n%s"" % (e.__class__, str(e),\n                                                e.responseText,\n                                                traceback.format_exc()))\n    except Exception as e:\n        import traceback\n        # exc_type, exc_obj, exec_tb = sys.exc_info()\n        module.fail_json(msg=""%s: %s\\n\\n%s"" % (e.__class__, str(e),\n                                               traceback.format_exc()))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
docs/examples/crop.py,0,"b""from __future__ import print_function\nimport os\nfrom zipfile import ZipFile\n\nimport gaia\nfrom gaia import preprocess\n\nsrc_folder = os.path.dirname(__file__)\ndata_folder = os.path.join(src_folder, os.pardir, os.pardir, 'tests', 'data')\n\n\n## Vector crop\nprint('VECTOR')\n\n# Load 2 datasets\nhospitals = gaia.create(os.path.join(data_folder, 'iraq_hospitals.geojson'))\ndistricts = gaia.create(os.path.join(data_folder, 'baghdad_districts.geojson'))\n\n# Apply crop\nprint('Before crop (vector)', districts.get_data().shape)\nvector_crop = preprocess.crop(hospitals, districts)\nprint('After crop (vector)', vector_crop.get_data().shape)\n#print(cropped.get_data().head())\n\nvector_filename = 'cropped.geojson'\ngaia.save(vector_crop, vector_filename)\nprint('Wrote file {}'.format(vector_filename))\n\n# Readback file and print some info\nreadback = gaia.create(vector_filename)\nprint('After readback (vector)', readback.get_data().shape)\n#print(readback.get_data().head())\n\n\n## Raster Crop\nprint()\nprint('RASTER')\n\n# Load geometry file\nzipfile = ZipFile(os.path.join(data_folder, '2states.zip'), 'r')\nzipfile.extract('2states.geojson', data_folder)\n\ntry:\n    # Load raster file\n    airtemp_raster = gaia.create(os.path.join(data_folder, 'globalairtemp.tif'))\n    print('type: {}'.format(type(airtemp_raster)))\n    print('meta: {}'.format(airtemp_raster.get_metadata()))\n    two_states = gaia.create(os.path.join(data_folder, '2states.geojson'))\n    raster_crop = preprocess.crop(airtemp_raster, two_states)\n    epsg = raster_crop.get_epsg()\n    print('epsg: {}'.format(epsg))\n\n    raster_filename = 'cropped.tif'\n    gaia.save(raster_crop, raster_filename)\nfinally:\n    testfile = os.path.join(data_folder, '2states.geojson')\n    if os.path.exists(testfile):\n        os.remove(testfile)\n"""
docs/examples/girder_example.py,0,"b'import json\nimport sys\n\nimport geojson\n\nimport gaia\n\n""""""\ngirder_url=\'https://data.kitware.com\'\napikey = \'WQMfRhZn7ymUo8iMMXTej8MOx4OjXVIf3Hsvk12J\'\ngaia.connect(girder_url=girder_url, apikey=apikey)\n\n# Maybe a method to generate girder url?\nutm_tiff_id = \'5b1e99b58d777f2e622561e2\'\nny_geojs_id = \'5ad8e9678d777f0685794ac9\'\nsfbay_tiff_id = \'5babd80c8d777f06b90730d1\'\nresource_url = \'girder://file/{}\'.format(sfbay_tiff_id)\n""""""\n#girder_url = \'http://localhost:8989\'\nuser = \'admin\'\npw = \'letmein\'\ndatastore = gaia.connect(username=user, password=pw)\n\n# from gaia.io import GirderInterface\n# gint = GirderInterface.get_instance()\n# gc = gint.gc\n# user = gint.user\n# #private_folder_list = gc.listFolder(\n# private_list = gc.listFolder(\n#     user[\'_id\'], parentFolderType=\'user\', name=\'Private\')\n# try:\n#     private_folder = next(private_list)\n# except StopIteration:\n#     raise GaiaException(\'User/Private folder not found\')\n\n# gaia_list = gc.listFolder(\n#     private_folder[\'_id\'], parentFolderType=\'folder\', name=\'gaia\')\n\n# try:\n#     gaia_folder = next(gaia_list)\n# except StopIteration:\n#     # Create folder\n#     gaia_folder = TBD()\n\n\n# API option 1: create internal url\ndatastore_url = datastore.lookup_url(\'Public/DEM_bare_earth.tif\')\nprint(datastore_url)\ndataset = gaia.create(datastore_url)\n\n# API option 2: pass in tuple as the data source\n# data_source = (datastore, \'Public/DEM_bare_earth.tif\')\n# dataset = gaia.create(data_source)\n\nmeta = dataset.get_metadata()\nprint()\nprint(json.dumps(meta, sort_keys=True, indent=2))\nprint(\'Input dataset width {}, height {}\'.format(meta[\'width\'], meta[\'height\']))\n\n# Generate crop geometry (small!)\nbounds = meta.get(\'bounds\',{}).get(\'coordinates\')[0]\nassert bounds, \'Dataset bounds missing\'\n# print()\n# print(bounds)\n\n# Compute center coordinates\nx = (bounds[0][0] + bounds[2][0]) / 2.0\ny = (bounds[0][1] + bounds[2][1]) / 2.0\n\n# Use smll percentage of height & width\ndx = 0.005 * (bounds[2][0] - bounds[0][0])\ndy = 0.005 * (bounds[2][1] - bounds[0][1])\nrect = [\n    [x,y], [x, y-dy], [x-dx, y-dy], [x-dx, y], [x,y]\n]\n# print()\n# print(rect)\n\n# Must pass rectangle in as a LIST, in order to get geom formatted the way resgeodata uses\ncrop_geom = geojson.Polygon([rect])\nprint()\nprint(crop_geom)\nsys.exit(0)\n# Perform crop operation\nfrom gaia import preprocess\ncropped_dataset = preprocess.crop(dataset, crop_geom, name=\'crop100m.tif\')\nprint()\ncropped_meta = cropped_dataset.get_metadata()\nprint(\'Cropped dataset width {}, height {}\'.format(\n    cropped_meta[\'width\'], cropped_meta[\'height\']))\nprint(cropped_meta)\n\nprint(\'finis\', dataset)\n'"
gaia/display/__init__.py,0,b''
gaia/display/deprecated_labgeojs_adapter.py,0,"b'""""""\nDisplay options using jupyterlab_geojs\n""""""\n\n#from gaia.girder_data import GirderDataObject\n\n\n# Is jupyterlab_geojs available?\ntry:\n    import geojson\n    from IPython.display import display\n    import jupyterlab_geojs\n    IS_GEOJS_LOADED = True\nexcept ImportError as err:\n    IS_GEOJS_LOADED = False\n\n\ndef is_loaded():\n    """"""Returns boolean indicating if jupyterlab_geojs is loaded\n\n    :return boolean\n    """"""\n    return IS_GEOJS_LOADED\n\n\ndef show(data_objects, **options):\n    """"""Returns geojs scene for JupyterLab display\n\n    :param data_objects: list of GeoData objects to display, in\n        front-to-back rendering order.\n    :param options: options passed to jupyterlab_geojs.Scene instance.\n    :return: jupyterlab_geojs.Scene instance if running Jupyter;\n        otherwise returns data_objects for default display\n    """"""\n    if not data_objects:\n        return None\n\n    if not is_loaded():\n        return data_objects\n\n    # (else)\n    if not hasattr(data_objects, \'__iter__\'):\n        data_objects = [data_objects]\n\n    #print(data_objects)\n    scene = jupyterlab_geojs.Scene(**options)\n    scene.create_layer(\'osm\')\n    # feature_layer = scene.create_layer(\'feature\')\n    feature_layer = None\n\n    combined_bounds = None\n    # Reverse order so that first item ends on top\n    for data_object in reversed(data_objects):\n        # Create map feature\n        #print(data_object._getdatatype(), data_object._getdataformat())\n        # type is vector, format is [.json, .geojson, .shp, pandas]\n        """"""\n        data = data_object.get_data()\n\n        # Can only seem to get json *string*; so parse into json *object*\n        json_string = data.to_json()\n        json_object = json.loads(json_string)\n        feature = feature_layer.create_feature(\'geojson\', json_object)\n        #print(json_object)\n        feature.enableToolTip = True  # dont work\n\n        geometry = data[\'geometry\']\n        bounds = geometry.total_bounds\n        """"""\n        meta = data_object.get_metadata()\n        #print(meta)\n        meta_bounds = meta.get(\'bounds\').get(\'coordinates\')[0]\n        #print(meta_bounds)\n        assert meta_bounds, \'data_object missing bounds\'\n\n        # Bounds format is [xmin, ymin, xmax, ymax]\n        bounds = [\n            meta_bounds[0][0], meta_bounds[0][1],\n            meta_bounds[2][0], meta_bounds[2][1]\n        ]\n\n        #print(bounds)\n        if combined_bounds is None:\n            combined_bounds = bounds\n        else:\n            combined_bounds[0] = min(combined_bounds[0], bounds[0])\n            combined_bounds[1] = min(combined_bounds[1], bounds[1])\n            combined_bounds[2] = max(combined_bounds[2], bounds[2])\n            combined_bounds[3] = max(combined_bounds[3], bounds[3])\n\n        # print(\'options:\', options)\n        rep = options.get(\'representation\')\n        if rep == \'outline\':\n            # Create polygon object\n            rect = [\n                [bounds[0], bounds[1]],\n                [bounds[2], bounds[1]],\n                [bounds[2], bounds[3]],\n                [bounds[0], bounds[3]],\n                [bounds[0], bounds[1]],\n            ]\n            geojs_polygon = geojson.Polygon([rect])\n            properties = {\n                \'fillColor\': \'#fff\',\n                \'fillOpacity\': 0.1,\n                \'stroke\': True,\n                \'strokeColor\': \'#333\',\n                \'strokeWidth\': 2\n            }\n            geojson_feature = geojson.Feature(\n                geometry=geojs_polygon, properties=properties)\n            geojson_collection = geojson.FeatureCollection([geojson_feature])\n            # print(geojson_collection)\n\n            if feature_layer is None:\n                feature_layer = scene.create_layer(\'feature\')\n\n            feature = feature_layer.create_feature(\n                \'geojson\', geojson_collection, **options)\n        #elif isinstance(data_object, GirderDataObject) and \\\n        elif data_object.__class__.__name__ == \'GirderDataObject\' and \\\n            data_object._getdatatype() == \'raster\':\n            # Use large-image display - only admin can tell if it is installed\n            #print(data_object._getdatatype(), data_object._getdataformat())\n            tile_url = data_object._get_tile_url()\n            print(\'tile_url\', tile_url)\n            tile_layer = scene.create_layer(\'tile\', url=tile_url)\n\n    #print(combined_bounds)\n    corners = [\n        [combined_bounds[0], combined_bounds[1]],\n        [combined_bounds[2], combined_bounds[1]],\n        [combined_bounds[2], combined_bounds[3]],\n        [combined_bounds[0], combined_bounds[3]]\n    ]\n    scene.set_zoom_and_center(corners=corners)\n    #display(scene)\n    return scene\n\n\ndef _is_jupyter():\n    """"""Determines if Jupyter is loaded\n\n    return: boolean\n    """"""\n    try:\n        ipy = get_ipython()\n    except NameError as err:\n        return False\n\n    # If jupyter, ipy is zmq shell\n    return is_instance(ipy, ipkernel.zmqshell.ZMQInteractiveShell)\n'"
gaia/display/pygeojs_adapter.py,0,"b'""""""\nDisplay options using pygeojs widget for Jupyter notebooks\n""""""\n\n# Is jupyterlab_geojs available?\ntry:\n    import geojson\n    from IPython.display import display\n    import pygeojs\n    IS_PYGEOJS_LOADED = True\nexcept ImportError:\n    IS_PYGEOJS_LOADED = False\n\nimport geopandas\n\nimport gaia.types\nfrom gaia.util import GaiaException\n\n\ndef is_loaded():\n    """"""Returns boolean indicating if pygeojs is loaded\n\n    :return boolean\n    """"""\n    return IS_PYGEOJS_LOADED\n\n\ndef show(data_objects, **options):\n    """"""Returns pygeojs scene for JupyterLab display\n\n    :param data_objects: list of GeoData objects to display, in\n        front-to-back rendering order.\n    :param options: options passed to jupyterlab_geojs.Scene instance.\n    :return: pygeojs.scene instance if running Jupyter;\n        otherwise returns data_objects for default display\n    """"""\n    if not is_loaded():\n        return data_objects\n\n    # (else)\n    if not hasattr(data_objects, \'__iter__\'):\n        data_objects = [data_objects]\n\n    # print(data_objects)\n    scene = pygeojs.scene(**options)\n    scene.createLayer(\'osm\')\n\n    if not data_objects:\n        print(\'No data objects\')\n        return scene\n\n    # feature_layer = scene.createLayer(\'feature\')\n    feature_layer = None\n\n    combined_bounds = None\n    # Reverse order so that first item ends on top\n    for data_object in reversed(data_objects):\n        if data_object._getdatatype() == gaia.types.VECTOR:\n            # print(\'Adding vector object\')\n            # Special handling for vector datasets:\n            # First, make a copy of the geopandas frame\n            df = geopandas.GeoDataFrame.copy(data_object.get_data())\n\n            # Convert to lon-lat if needed\n            epsg = data_object.get_epsg()\n            if epsg and str(epsg) != \'4326\':\n                print(\'Converting crs\')\n                df[df.geometry.name] = df.geometry.to_crs(epsg=\'4326\')\n\n            # Strip any z coordinates (force to z = 1)\n            df.geometry = df.geometry.scale(zfact=0.0).translate(zoff=1.0)\n            # df.to_file(\'/home/john/temp/df.pandas\')\n            # print(df)\n            # print(df.geometry)\n\n            # Calculate bounds\n            geopandas_bounds = df.geometry.total_bounds\n            xmin, ymin, xmax, ymax = geopandas_bounds\n            meta_bounds = [\n                [xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]\n            ]\n\n            # Add map feature\n            if feature_layer is None:\n                feature_layer = scene.createLayer(\'feature\')\n\n            # Use __geo_interface__ to get the geojson\n            feature_layer.readGeoJSON(df.__geo_interface__)\n            # print(df.__geo_interface__)\n        else:\n            # Get bounds, in order to compute overall bounds\n            meta = data_object.get_metadata()\n            # print(\'meta: {}\'.format(meta))\n            # print(meta)\n            raster_bounds = meta.get(\'bounds\').get(\'coordinates\')[0]\n            # print(meta_bounds)\n            assert raster_bounds, \'data_object missing bounds\'\n\n            # meta bounds inconsistent between sources, so compute brute force\n            xvals, yvals = zip(*raster_bounds)\n            xmin, xmax = min(xvals), max(xvals)\n            ymin, ymax = min(yvals), max(yvals)\n            meta_bounds = [\n                [xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]\n            ]\n\n        # Bounds format is [xmin, ymin, xmax, ymax]\n        bounds = [\n            meta_bounds[0][0], meta_bounds[0][1],\n            meta_bounds[2][0], meta_bounds[2][1]\n        ]\n\n        # print(bounds)\n        if combined_bounds is None:\n            combined_bounds = bounds\n        else:\n            combined_bounds[0] = min(combined_bounds[0], bounds[0])\n            combined_bounds[1] = min(combined_bounds[1], bounds[1])\n            combined_bounds[2] = max(combined_bounds[2], bounds[2])\n            combined_bounds[3] = max(combined_bounds[3], bounds[3])\n\n        # print(\'options:\', options)\n        rep = options.get(\'representation\')\n        if rep == \'outline\':\n            # Create polygon object\n            rect = [\n                [bounds[0], bounds[1]],\n                [bounds[2], bounds[1]],\n                [bounds[2], bounds[3]],\n                [bounds[0], bounds[3]],\n                [bounds[0], bounds[1]],\n            ]\n            geojs_polygon = geojson.Polygon([rect])\n            properties = {\n                \'fillColor\': \'#fff\',\n                \'fillOpacity\': 0.1,\n                \'stroke\': True,\n                \'strokeColor\': \'#333\',\n                \'strokeWidth\': 2\n            }\n            geojson_feature = geojson.Feature(\n                geometry=geojs_polygon, properties=properties)\n            geojson_collection = geojson.FeatureCollection([geojson_feature])\n            # print(geojson_collection)\n\n            if feature_layer is None:\n                feature_layer = scene.createLayer(\'feature\')\n\n            feature_layer.createFeature(\n                \'geojson\', geojson_collection, **options)\n\n        elif data_object.__class__.__name__ == \'GirderDataObject\':\n            if data_object._getdatatype() == \'raster\':\n                # Use large-image display\n                # Todo - verify that it is installed\n                tiles_url = data_object._get_tiles_url()\n                # print(\'tiles_url\', tiles_url)\n                opacity = 1.0\n                if hasattr(data_object, \'opacity\'):\n                    opacity = data_object.opacity\n                scene.createLayer(\n                    \'osm\', url=tiles_url, keepLower=False, opacity=opacity)\n            else:\n                raise GaiaException(\n                    \'Cannot display GirderDataObject with data type {}\'.format(\n                        data_object._getdatatype()))\n\n        elif data_object._getdatatype() == gaia.types.VECTOR:\n            pass  # vector objects handled above\n        else:\n            msg = \'Cannot display dataobject, type {}\'.format(\n                data_object.__class__.__name__)\n            raise GaiaException(msg)\n\n    # Send custom message to (javascript) client to set zoom & center\n    rpc = {\'method\': \'set_zoom_and_center\', \'params\': combined_bounds}\n    scene.send(rpc)\n    return scene\n\n\ndef _is_jupyter():\n    """"""Determines if Jupyter is loaded\n\n    return: boolean\n    """"""\n    try:\n        ipy = get_ipython()\n    except NameError:\n        return False\n\n    # If jupyter, ipy is zmq shell\n    return ipy.__class__.__name__ == \'ZMQInteractiveShell\'\n'"
gaia/geo/__init__.py,0,b'from gaia.geo.gdal_functions import *\n'
gaia/geo/gdal_functions.py,5,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n###############################################################################\n#  Copyright Kitware Inc. and Epidemico Inc.\n#\n#  Licensed under the Apache License, Version 2.0 ( the ""License"" );\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\nimport re\nimport string\nimport os\nimport json\nimport logging\nimport gdalconst\nimport numpy\nimport gdal\nimport shapely\nimport rasterio\nimport rasterio.features\nfrom gaia.util import (\n    GaiaException,\n    UnsupportedFormatException,\n    get_uri_extension\n)\ntry:\n    import gdalnumeric\nexcept ImportError:\n    from osgeo import gdalnumeric\nimport ogr\nimport osr\nfrom PIL import Image, ImageDraw\nfrom osgeo.gdal_array import BandReadAsArray, BandWriteArray\nimport numpy as np\nfrom numpy.ma.core import MaskedConstant\n\nlogger = logging.getLogger(\'gaia.geo.gdal_functions\')\n\n# Python bindings do not raise exceptions unless you\n# explicitly call UseExceptions()\ngdal.UseExceptions()\ngdal.PushErrorHandler(\'CPLQuietErrorHandler\')\n\n#: Map of raster data types to max values\nndv_lookup = {\n    \'Byte\': 255,\n    \'UInt16\': 65535,\n    \'Int16\': -32767,\n    \'UInt32\': 4294967293,\n    \'Int32\': -2147483647,\n    \'Float32\': 1.175494351E-38,\n    \'Float64\': 1.7976931348623158E+308\n}\n\n# Map of file extensions to driver name\ndriver_lookup = {\n    \'tif\': \'GTiff\',\n    \'tiff\': \'GTiff\'\n}\n\n\ndef register_driver_name(fileExt, driverName):\n    driver_lookup[fileExt] = driverName\n\n\ndef write_dataset(raster_dataset, filepath, fileFormat=None):\n    # if no fileFormat is provide, it will be looked up from the filepath\n    driverName = fileFormat\n    if not driverName:\n        ext = get_uri_extension(filepath)\n        if ext in driver_lookup:\n            driverName = driver_lookup[ext]\n        else:\n            msg = \'No format provide, could not be guessed from %s\' % filepath\n            raise UnsupportedFormatException(msg)\n\n    output_driver = gdal.GetDriverByName(driverName)\n    outfile = output_driver.CreateCopy(filepath, raster_dataset, False)\n    logger.debug(str(outfile))\n    outfile = None\n\n\ndef raster_to_numpy_array(raster_data, as_single_band=True,\n                          old_nodata=None, new_nodata=None):\n    """"""\n    Convert raster output to numpy array output\n\n    :param raster_data: Original raster output dataset\n    :param as_single_band: Output data as 2D array of its first band\n    (default is True). If False, returns full 3D array.\n    :param old_nodata: Explicitly identify existing NoData values\n    (default None). If None, attempts to get existing NoData values stored\n    in the raster band.\n    :param new_nodata: Replace NoData values in each band with new_nodata\n    (default None). If new_nodata is not None but old_nodata is None\n    and no existing NoData value is stored in the band, uses unchanged\n    default ReadAsArray() return values.\n    :return: Converted numpy array dataset\n    """"""\n    bands = as_single_band + (1 - as_single_band) * raster_data.RasterCount\n    nrow = raster_data.RasterYSize\n    ncol = raster_data.RasterXSize\n    dims = (bands, nrow, ncol)\n\n    out_data_array = np.full(dims, np.nan)\n\n    for i in range(bands):\n        srcband = raster_data.GetRasterBand(i + 1)\n        srcband_array = np.array(srcband.ReadAsArray().astype(np.float))\n        if old_nodata is None:\n            old_nodata = srcband.GetNoDataValue()\n        if new_nodata is not None and old_nodata is not None:\n            if np.isnan(old_nodata):\n                srcband_array[np.isnan(srcband_array)] = new_nodata\n            else:\n                srcband_array[srcband_array == old_nodata] = new_nodata\n            print(\'NoData: Replaced \' + str(old_nodata) +\n                  \' with \' + str(new_nodata))\n        out_data_array[i, :, :] = srcband_array\n\n    if as_single_band:\n        return out_data_array[0, :, :]\n    else:\n        return out_data_array\n\n\ndef gdal_reproject(src, dst,\n                   epsg=3857,\n                   error_threshold=0.125,\n                   resampling=gdal.GRA_NearestNeighbour):\n    """"""\n    Reproject a raster image\n\n    :param src: The source image\n    :param dst: The filepath/name of the output image\n    :param epsg: The EPSG code to reproject to\n    :param error_threshold: Default is 0.125 (same as gdalwarp commandline)\n    :param resampling: Default method is Nearest Neighbor\n    """"""\n    # Open source dataset\n    src_ds = get_dataset(src)\n\n    # Define target SRS\n    dst_srs = osr.SpatialReference()\n    dst_srs.ImportFromEPSG(int(epsg))\n    dst_wkt = dst_srs.ExportToWkt()\n\n    # Resampling might be passed as a string\n    if not isinstance(resampling, int):\n        resampling = getattr(gdal, resampling)\n\n    # Call AutoCreateWarpedVRT() to fetch default values\n    # for target raster dimensions and geotransform\n    reprojected_ds = gdal.AutoCreateWarpedVRT(src_ds,\n                                              None,\n                                              dst_wkt,\n                                              resampling,\n                                              error_threshold)\n\n    # Create the final warped raster\n    if dst:\n        gdal.GetDriverByName(\'GTiff\').CreateCopy(dst, reprojected_ds)\n    return reprojected_ds\n\n\ndef gdal_resize(raster, dimensions, projection, transform):\n    """"""\n    Transform a dataset to the specified dimensions and projection/bounds\n\n    :param dataset: Dataset to be resized\n    :param dimensions: dimensions to resize to (X, Y)\n    :param projection: Projection of of resized dataset\n    :param transform: Geotransform of resized dataset\n    :return: Resized dataset\n    """"""\n    dataset = get_dataset(raster)\n    datatype = dataset.GetRasterBand(1).DataType\n    resized_ds = gdal.GetDriverByName(\'MEM\').Create(\n        \'\', dimensions[0], dimensions[1],  dataset.RasterCount, datatype)\n    for i in range(1, resized_ds.RasterCount+1):\n        nodatavalue = dataset.GetRasterBand(i).GetNoDataValue()\n        resized_band = resized_ds.GetRasterBand(i)\n        resized_arr = resized_band.ReadAsArray()\n        if nodatavalue:\n            resized_arr[resized_arr == 0] = nodatavalue\n            resized_band.SetNoDataValue(nodatavalue)\n        resized_band.WriteArray(resized_arr)\n    resized_ds.SetGeoTransform(transform)\n    resized_ds.SetProjection(projection)\n\n    gdal.ReprojectImage(dataset, resized_ds)\n    return resized_ds\n\n\ndef gdal_clip(raster_input, raster_output, polygon_json, nodata=0):\n    """"""\n    This function will subset a raster by a vector polygon.\n    Adapted from the GDAL/OGR Python Cookbook at\n    https://pcjericks.github.io/py-gdalogr-cookbook\n\n    :param raster_input: raster input filepath\n    :param raster_output: raster output filepath\n    :param polygon_json: polygon as geojson string\n    :param nodata: nodata value for output raster file\n    :return: GDAL Dataset\n    """"""\n\n    def image_to_array(i):\n        """"""\n        Converts a Python Imaging Library array to a\n        gdalnumeric image.\n        """"""\n        a = gdalnumeric.numpy.frombuffer(i.tobytes(), dtype=\'b\')\n        a.shape = i.im.size[1], i.im.size[0]\n        return a\n\n    def world_to_pixel(geoMatrix, x, y):\n        """"""\n        Uses a gdal geomatrix (gdal.GetGeoTransform()) to calculate\n        the pixel location of a geospatial coordinate\n        """"""\n        ulX = geoMatrix[0]\n        ulY = geoMatrix[3]\n        xDist = geoMatrix[1]\n        yDist = geoMatrix[5]\n        pixel = int((x - ulX) / xDist)\n        line = int((y - ulY) / yDist)\n        return (pixel, line)\n\n    src_image = get_dataset(raster_input)\n    # Load the source data as a gdalnumeric array\n    src_array = src_image.ReadAsArray()\n    src_dtype = src_array.dtype\n\n    # Also load as a gdal image to get geotransform\n    # (world file) info\n    geo_trans = src_image.GetGeoTransform()\n    nodata_values = []\n    for i in range(src_image.RasterCount):\n        nodata_value = src_image.GetRasterBand(i+1).GetNoDataValue()\n        if not nodata_value:\n            nodata_value = nodata\n        nodata_values.append(nodata_value)\n\n    # Create an OGR layer from a boundary GeoJSON geometry string\n    if type(polygon_json) == dict:\n        polygon_json = json.dumps(polygon_json)\n    poly = ogr.CreateGeometryFromJson(polygon_json)\n\n    # Convert the layer extent to image pixel coordinates\n    min_x, max_x, min_y, max_y = poly.GetEnvelope()\n    ul_x, ul_y = world_to_pixel(geo_trans, min_x, max_y)\n    lr_x, lr_y = world_to_pixel(geo_trans, max_x, min_y)\n\n    # Check for null intersection\n    if lr_x < 0 or lr_y < 0 \\\n            or ul_x > src_array.shape[-1] \\\n            or ul_y > src_array.shape[-2]:\n        return None\n\n    # Clip to the input image bounds\n    ul_x = max(ul_x, 0)\n    ul_y = max(ul_y, 0)\n    lr_x = min(lr_x, src_array.shape[-1])\n    lr_y = min(lr_y, src_array.shape[-2])\n\n    # Calculate the pixel size of the new image\n    px_width = int(lr_x - ul_x)\n    px_height = int(lr_y - ul_y)\n\n    if px_width < 1 or px_height < 1:\n        return None\n\n    if raster_input.RasterCount == 1:\n        clip2d = src_array[ul_y:lr_y, ul_x:lr_x]\n        clip = np.expand_dims(clip2d, axis=0)\n    else:\n        clip = src_array[:, ul_y:lr_y, ul_x:lr_x]\n\n    # create pixel offset to pass to new image Projection info\n    xoffset = ul_x\n    yoffset = ul_y\n\n    # Create a new geomatrix for the image\n    geo_trans = list(geo_trans)\n    geo_trans[0] = min_x\n    geo_trans[3] = max_y\n\n    # Map points to pixels for drawing the\n    # boundary on a blank 8-bit,\n    # black and white, mask image.\n    raster_poly = Image.new(""L"", (px_width, px_height), 1)\n    rasterize = ImageDraw.Draw(raster_poly)\n    geometry_count = poly.GetGeometryCount()\n    for i in range(0, geometry_count):\n        points = []\n        pixels = []\n        pts = poly.GetGeometryRef(i)\n        if pts.GetPointCount() == 0:\n            pts = pts.GetGeometryRef(0)\n        for p in range(pts.GetPointCount()):\n            points.append((pts.GetX(p), pts.GetY(p)))\n        for p in points:\n            pixels.append(world_to_pixel(geo_trans, p[0], p[1]))\n        rasterize.polygon(pixels, 0)\n    mask = image_to_array(raster_poly)\n\n    # Clip the image using the mask\n    for i in range(raster_input.RasterCount):\n        clip[i] = gdalnumeric.numpy.choose(\n            mask, (clip[i], nodata_value)).astype(src_dtype)\n\n    # create output raster\n    raster_band = raster_input.GetRasterBand(1)\n    output_driver = gdal.GetDriverByName(\'MEM\')\n    output_dataset = output_driver.Create(\n        \'\', clip.shape[-1], clip.shape[-2],\n        raster_input.RasterCount, raster_band.DataType)\n    output_dataset.SetGeoTransform(geo_trans)\n    output_dataset.SetProjection(raster_input.GetProjection())\n    gdalnumeric.CopyDatasetInfo(raster_input, output_dataset,\n                                xoff=xoffset, yoff=yoffset)\n    bands = raster_input.RasterCount\n    for i in range(bands):\n        inband = raster_input.GetRasterBand(i + 1)\n\n        outBand = output_dataset.GetRasterBand(i + 1)\n        outBand.SetColorInterpretation(inband.GetColorInterpretation())\n        outBand.SetNoDataValue(nodata_values[i])\n        outBand.WriteArray(clip[i])\n\n    if raster_output:\n        output_driver = gdal.GetDriverByName(\'GTiff\')\n        outfile = output_driver.CreateCopy(raster_output, output_dataset, False)\n        logger.debug(str(outfile))\n        outfile = None\n\n    return output_dataset\n\n\ndef gdal_calc(calculation, raster_output, rasters,\n              bands=None, nodata=None, allBands=False, output_type=None,\n              format=\'GTiff\'):\n    """"""\n    Adopted from GDAL 1.10 gdal_calc.py script.\n\n    :param calculation: equation to calculate, such as A + (B / 2)\n    :param raster_output: Raster file to save output as\n    :param rasters: array of rasters, should equal # of letters in calculation\n    :param bands: array of band numbers, one for each raster in rasters array\n    :param nodata: NoDataValue to use in output raster\n    :param allBands: use all bands of specified raster by index\n    :param output_type: data type for output raster (\'Float32\', \'Uint16\', etc)\n    :return: gdal Dataset\n    """"""\n\n    calculation = re.sub(r\'(logical_|bitwise_)\', r\'numpy.\\1\', calculation)\n\n    # set up some lists to store data for each band\n    datasets = [get_dataset(raster) for raster in rasters]\n    if not bands:\n        bands = [1 for raster in rasters]\n    datatypes = []\n    datatype_nums = []\n    nodata_vals = []\n    dimensions = None\n    alpha_list = string.ascii_uppercase[:len(rasters)]\n\n    # loop through input files - checking dimensions\n    for i, (raster, alpha, band) in enumerate(zip(datasets, alpha_list, bands)):\n        raster_band = raster.GetRasterBand(band)\n        datatypes.append(gdal.GetDataTypeName(raster_band.DataType))\n        datatype_nums.append(raster_band.DataType)\n        nodata_vals.append(raster_band.GetNoDataValue())\n        # check that the dimensions of each layer are the same as the first\n        if dimensions:\n            if dimensions != [datasets[i].RasterXSize, datasets[i].RasterYSize]:\n                datasets[i] = gdal_resize(raster,\n                                          dimensions,\n                                          datasets[0].GetProjection(),\n                                          datasets[0].GetGeoTransform())\n        else:\n            dimensions = [datasets[0].RasterXSize, datasets[0].RasterYSize]\n\n    # process allBands option\n    allbandsindex = None\n    allbandscount = 1\n    if allBands:\n        allbandscount = datasets[allbandsindex].RasterCount\n        if allbandscount <= 1:\n            allbandsindex = None\n\n    ################################################################\n    # set up output file\n    ################################################################\n\n    # open output file exists\n    # remove existing file and regenerate\n    if os.path.isfile(raster_output):\n        os.remove(raster_output)\n    # create a new file\n    logger.debug(""Generating output file %s"" % (raster_output))\n\n    # find data type to use\n    if not output_type:\n        # use the largest type of the input files\n        output_type = gdal.GetDataTypeName(max(datatype_nums))\n\n    # create file\n    output_driver = gdal.GetDriverByName(\'MEM\')\n    output_dataset = output_driver.Create(\n        \'\', dimensions[0], dimensions[1], allbandscount,\n        gdal.GetDataTypeByName(output_type))\n\n    # set output geo info based on first input layer\n    output_dataset.SetGeoTransform(datasets[0].GetGeoTransform())\n    output_dataset.SetProjection(datasets[0].GetProjection())\n\n    if nodata is None:\n        nodata = ndv_lookup[output_type]\n\n    for i in range(1, allbandscount+1):\n        output_band = output_dataset.GetRasterBand(i)\n        output_band.SetNoDataValue(nodata)\n        # write to band\n        output_band = None\n\n    ################################################################\n    # find block size to chop grids into bite-sized chunks\n    ################################################################\n\n    # use the block size of the first layer to read efficiently\n    block_size = datasets[0].GetRasterBand(bands[0]).GetBlockSize()\n    # store these numbers in variables that may change later\n    n_x_valid = block_size[0]\n    n_y_valid = block_size[1]\n    # find total x and y blocks to be read\n    n_x_blocks = int((dimensions[0] + block_size[0] - 1) / block_size[0])\n    n_y_blocks = int((dimensions[1] + block_size[1] - 1) / block_size[1])\n    buffer_size = block_size[0]*block_size[1]\n\n    ################################################################\n    # start looping through each band in allbandscount\n    ################################################################\n    for band_num in range(1, allbandscount+1):\n\n        ################################################################\n        # start looping through blocks of data\n        ################################################################\n        # loop through X-lines\n        for x in range(0, n_x_blocks):\n            # in the rare (impossible?) case that the blocks don\'t fit perfectly\n            # change the block size of the final piece\n            if x == n_x_blocks-1:\n                n_x_valid = dimensions[0] - x * block_size[0]\n                buffer_size = n_x_valid*n_y_valid\n\n            # find X offset\n            x_offset = x*block_size[0]\n\n            # reset buffer size for start of Y loop\n            n_y_valid = block_size[1]\n            buffer_size = n_x_valid*n_y_valid\n\n            # loop through Y lines\n            for y in range(0, n_y_blocks):\n                # change the block size of the final piece\n                if y == n_y_blocks-1:\n                    n_y_valid = dimensions[1] - y * block_size[1]\n                    buffer_size = n_x_valid*n_y_valid\n\n                # find Y offset\n                y_offset = y*block_size[1]\n\n                # create empty buffer to mark where nodata occurs\n                nodatavalues = numpy.zeros(buffer_size)\n                nodatavalues.shape = (n_y_valid, n_x_valid)\n\n                # fetch data for each input layer\n                for i, alpha in enumerate(alpha_list):\n\n                    # populate lettered arrays with values\n                    if allbandsindex is not None and allbandsindex == i:\n                        this_band = band_num\n                    else:\n                        this_band = bands[i]\n                    band_vals = BandReadAsArray(\n                        datasets[i].GetRasterBand(this_band),\n                        xoff=x_offset,\n                        yoff=y_offset,\n                        win_xsize=n_x_valid,\n                        win_ysize=n_y_valid)\n\n                    # fill in nodata values\n                    nodatavalues = 1*numpy.logical_or(\n                        nodatavalues == 1, band_vals == nodata_vals[i])\n\n                    # create an array of values for this block\n                    exec(""%s=band_vals"" % alpha)\n                    band_vals = None\n\n                # try the calculation on the array blocks\n                try:\n                    calc_result = eval(calculation)\n                except Exception as e:\n                    logger.error(""eval of calculation %s failed"" % calculation)\n                    raise e\n\n                # propogate nodata values\n                # (set nodata cells to 0 then add nodata value to these cells)\n                calc_result = ((1 * (nodatavalues == 0)) * calc_result) + \\\n                              (nodata * nodatavalues)\n\n                # write data block to the output file\n                output_band = output_dataset.GetRasterBand(band_num)\n                BandWriteArray(output_band, calc_result,\n                               xoff=x_offset, yoff=y_offset)\n\n    if raster_output:\n        output_driver = gdal.GetDriverByName(format)\n        outfile = output_driver.CreateCopy(raster_output, output_dataset, False)\n        logger.debug(str(outfile))\n    return output_dataset\n\n\ndef gdal_zonalstats(zones, raster):\n    """"""\n    Return a list of zonal statistics.\n\n    :param zones: vector dataset in JSON format representing polygons (zones)\n    :param raster: Raster file to generate statistics from in each polygon\n    :return: list of polygon features with statistics properties appended.\n    """"""\n    return list(gen_zonalstats(zones, raster))\n\n\ndef rasterio_bbox(raster_input):\n    """"""\n    This function will return bounding box information\n    for a raster layer.\n\n    :param raster_input: raster input filepath\n    :return: list of coordinates in (xmin, ymin, xmax, ymax) format\n    """"""\n\n    footprint = rasterio_footprint(raster_input)\n    shape = shapely.geometry.Polygon(footprint)\n    return list(shape.bounds)\n\n\ndef rasterio_footprint(raster_input):\n    """"""\n    This function will return the footprint of a raster\n    layer\n\n    :param raster_input: raster input filepath\n    :return: list of coordinates\n    """"""\n    with rasterio.open(raster_input) as dataset:\n        # Read the dataset\'s valid data mask as a ndarray.\n        mask = dataset.dataset_mask()\n\n        # Extract feature shapes and values from the array.\n        feature = rasterio.features.shapes(mask, transform=dataset.affine)\n        footprint = next(feature)[0]\n        shape = shapely.geometry.shape(json.loads(json.dumps(footprint)))\n        return list(shape.minimum_rotated_rectangle.exterior.coords)\n\n\ndef gen_zonalstats(zones_json, raster):\n    """"""\n    Generator function that yields the statistics of a raster dataset\n    within each polygon (zone) of a vector dataset.\n\n    :param zones_json: Polygons in GeoJSON format\n    :param raster: Raster dataset\n    :return: Polygons with additional properties for calculated raster stats.\n    """"""\n    global_transform = True\n\n    # Open data\n    raster = get_dataset(raster)\n    if type(zones_json) is str:\n        shp = ogr.Open(zones_json)\n        zones_json = json.loads(zones_json)\n    else:\n        shp = ogr.Open(json.dumps(zones_json))\n\n    lyr = shp.GetLayer()\n\n    # Get raster georeference info\n    transform = raster.GetGeoTransform()\n    xOrigin = transform[0]\n    yOrigin = transform[3]\n    pixelWidth = transform[1]\n    pixelHeight = transform[5]\n\n    # Reproject vector geometry to same projection as raster\n    sourceSR = lyr.GetSpatialRef()\n    targetSR = osr.SpatialReference()\n    targetSR.ImportFromWkt(raster.GetProjectionRef())\n    coordTrans = osr.CoordinateTransformation(sourceSR, targetSR)\n\n    # Check for matching spatial references\n    differing_SR = (sourceSR.ExportToWkt() != targetSR.ExportToWkt())\n\n    # TODO: Use a multiprocessing pool to process features more quickly\n    for feat, feature in zip(lyr, zones_json[\'features\']):\n        geom = feat.geometry()\n\n        # geotransform of the feature by global\n        if (global_transform and differing_SR):\n                    geom.Transform(coordTrans)\n\n        # Get geometry type\n        geom_type = geom.GetGeometryName()\n\n        # Get extent of feat\n        if geom_type == \'MULTIPOLYGON\':\n            pointsX = []\n            pointsY = []\n            for count, polygon in enumerate(geom):\n                ring = geom.GetGeometryRef(count).GetGeometryRef(0)\n                numpoints = ring.GetPointCount()\n                for p in range(numpoints):\n                        lon, lat, z = ring.GetPoint(p)\n                        if abs(lon) != float(\'inf\'):\n                            pointsX.append(lon)\n                        if abs(lat) != float(\'inf\'):\n                            pointsY.append(lat)\n        elif geom_type == \'POLYGON\':\n            ring = geom.GetGeometryRef(0)\n            numpoints = ring.GetPointCount()\n            pointsX = []\n            pointsY = []\n            for p in range(numpoints):\n                    lon, lat, z = ring.GetPoint(p)\n                    if abs(lon) != float(\'inf\'):\n                        pointsX.append(lon)\n                    if abs(lat) != float(\'inf\'):\n                        pointsY.append(lat)\n        else:\n            raise GaiaException(\n                ""ERROR: Geometry needs to be either Polygon or Multipolygon"")\n\n        xmin = min(pointsX)\n        xmax = max(pointsX)\n        ymin = min(pointsY)\n        ymax = max(pointsY)\n\n        # Specify offset and rows and columns to read\n        xoff = int((xmin - xOrigin)/pixelWidth)\n        yoff = int((yOrigin - ymax)/pixelWidth)\n        xcount = int((xmax - xmin)/pixelWidth)+1\n        ycount = int((ymax - ymin)/pixelWidth)+1\n\n        # Create memory target raster\n        target_ds = gdal.GetDriverByName(\'MEM\').Create(\n            \'\', xcount, ycount, 1, gdal.GDT_Byte)\n        # apply new geotransform of the feature subset\n        if global_transform is False:\n            target_ds.SetGeoTransform((\n                (xOrigin + (xoff * pixelWidth)),\n                pixelWidth,\n                0,\n                (yOrigin + (yoff * pixelHeight)),\n                0,\n                pixelHeight,\n            ))\n        else:\n            # apply new geotransform of the global set\n            target_ds.SetGeoTransform((\n                xmin, pixelWidth, 0,\n                ymax, 0, pixelHeight,\n            ))\n\n        # Create memory vector layer\n        mem_ds = ogr.GetDriverByName(\'Memory\').CreateDataSource(\'out\')\n        mem_layer = mem_ds.CreateLayer(\n            geom.GetGeometryName(),\n            None,\n            geom.GetGeometryType()\n        )\n        mem_layer.CreateFeature(feat.Clone())\n\n        # Create for target raster the same projection as for the value raster\n        raster_srs = osr.SpatialReference()\n        raster_srs.ImportFromWkt(raster.GetProjectionRef())\n        target_ds.SetProjection(raster_srs.ExportToWkt())\n\n        # Rasterize zone polygon to raster\n        gdal.RasterizeLayer(target_ds, [1], mem_layer, burn_values=[1])\n\n        # Read raster as arrays\n        banddataraster = raster.GetRasterBand(1)\n        try:\n            dataraster = banddataraster.ReadAsArray(\n                xoff, yoff, xcount, ycount).astype(numpy.float)\n        except AttributeError:\n            # Nothing within bounds, move on to next polygon\n            properties = feature[\'properties\']\n            for p in [\'count\', \'sum\', \'mean\', \'median\', \'min\', \'max\', \'stddev\']:\n                properties[p] = None\n            yield(feature)\n        else:\n            # Get no data value of array\n            noDataValue = banddataraster.GetNoDataValue()\n            if noDataValue:\n                # Updata no data value in array with new value\n                dataraster[dataraster == noDataValue] = numpy.nan\n\n            bandmask = target_ds.GetRasterBand(1)\n            datamask = bandmask.ReadAsArray(\n                0, 0, xcount, ycount).astype(numpy.float)\n\n            # Mask zone of raster\n            zoneraster = numpy.ma.masked_array(\n                dataraster,  numpy.logical_not(datamask))\n\n            properties = feature[\'properties\']\n            properties[\'count\'] = zoneraster.count()\n            properties[\'sum\'] = numpy.nansum(zoneraster)\n            if type(properties[\'sum\']) == MaskedConstant:\n                # No non-null values for raster data in polygon, skip\n                for p in [\'sum\', \'mean\', \'median\', \'min\', \'max\', \'stddev\']:\n                    properties[p] = None\n            else:\n                properties[\'mean\'] = numpy.nanmean(zoneraster)\n                properties[\'min\'] = numpy.nanmin(zoneraster)\n                properties[\'max\'] = numpy.nanmax(zoneraster)\n                properties[\'stddev\'] = numpy.nanstd(zoneraster)\n                median = numpy.ma.median(zoneraster)\n                if hasattr(median, \'data\'):\n                    try:\n                        properties[\'median\'] = median.data.item()\n                    except AttributeError:\n                        if median:\n                            properties[\'median\'] = median\n            yield(feature)\n\n\ndef get_dataset(object):\n    """"""\n    Given an object, try returning a GDAL Dataset\n\n    :param object: GDAL Dataset or file path to raster image\n    :return: GDAL Dataset\n    """"""\n    if type(object).__name__ == \'Dataset\':\n        return object\n    else:\n        return gdal.Open(object, gdalconst.GA_ReadOnly)\n'"
gaia/io/__init__.py,0,b'from gaia.io.girder_interface import GirderInterface\n'
gaia/io/cumulus_interface.py,0,"b'# =============================================================================\n#\n#  Copyright (c) Kitware, Inc.\n#  All rights reserved.\n#  See LICENSE.txt for details.\n#\n#  This software is distributed WITHOUT ANY WARRANTY; without even\n#  the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR\n#  PURPOSE.  See the above copyright notice for more information.\n#\n# =============================================================================\nimport datetime\nimport io\nimport json\nimport os\nimport time\nimport uuid\n\nfrom girder_client import HttpError\nimport requests\n\nimport gaia\nfrom gaia.girder_data import GirderDataObject\nfrom gaia.io.girder_interface import GirderInterface\nfrom gaia.util import GaiaException\n\nNERSC_URL = \'https://newt.nersc.gov/newt\'\n\n# Some hard-coded constants for now\nMACHINE = \'cori\'\nJOHNT_PATH = \'/global/homes/j/johnt\'\nCONDA_ENV_PATH = \'{}/.conda/envs/py3\'.format(JOHNT_PATH)\nPROJECT_PATH = \'{}/project\'.format(JOHNT_PATH)\nGAIA_PATH = \'{}/git/gaia\'.format(PROJECT_PATH)\n\n\nclass CumulusInterface():\n    """"""An internal class that submits jobs to NERSC via girder/cumulus\n\n    This class uses the girder client owned by GirderInterface, which must be\n    authenticated with NERSC (NEWT api). Note that this version is hard-coded\n    to the Cori machine.\n    """"""\n\n    # ---------------------------------------------------------------------\n    def __init__(self):\n        """"""Recommend using separate instance for each job submission.\n        """"""\n        self._girder_client = None\n        self._nersc_scratch_folder = None\n        self._private_folder_id = None\n\n        # Internal, job-specific ids\n        self._cluster_id = None\n        self._input_folder_id = None\n        self._job_folder_id = None\n        self._job_id = None\n        self._output_folder_id = None\n        self._script_id = None\n\n        girder_interface = GirderInterface.get_instance()\n        if girder_interface.nersc_requests is None:\n            msg = """"""GirderInterface is not configured for NERSC job submission -- \\\nmust authenticate with NEWT session id.""""""\n            raise GaiaException(msg)\n\n        # Get user\'s scratch directory\n        data = {\n            \'executable\': \'echo $SCRATCH\',\n            \'loginenv\': \'true\'\n        }\n        machine = \'cori\'\n        url = \'%s/command/%s\' % (NERSC_URL, machine)\n        r = girder_interface.nersc_requests.post(url, data=data)\n        r.raise_for_status()\n        js = r.json()\n        self._nersc_scratch_folder = js.get(\'output\')\n\n        # Get Girder client\n        self._girder_client = girder_interface.gc\n\n        # Get id for user\'s private girder folder\n        user = self._girder_client.get(\'user/me\')\n        print(\'user\', user)\n        user_id = user[\'_id\']\n        # r = self._girder_client.listFolder(user_id, \'user\', name=\'Private\')\n        r = self._girder_client.listFolder(user_id, \'user\', name=\'Public\')\n        # Getting mixed signals on what listFolder returns\n        # I *think* it is a generator\n        try:\n            self._private_folder_id = next(r)[\'_id\']\n        except Exception:\n            # But just in case\n            self._private_folder_id = r[0][\'_id\']\n        # print(\'private_folder_id\', self._private_folder_id)\n\n    # ---------------------------------------------------------------------\n    def submit_crop(\n            self,\n            input_object,\n            crop_object,\n            nersc_repository,\n            job_name=\'geolib\'):\n        """"""\n        """"""\n        # Validate inputs\n        if not isinstance(input_object, GirderDataObject):\n            print(\'input object type\', type(input_object))\n            raise GaiaException(""""""submit_crop() currently only supports \\\nGirderDataObject input"""""")\n        if not crop_object._getdatatype() == gaia.types.VECTOR:\n            raise GaiaException(\'Crop object not type VECTOR\')\n\n        # Get input object\'s filename\n        # For now (March 2019) we are storing a cache of files on cori\n        # for the ESS-DIVE dev server\n        item = self._girder_client.getItem(input_object.resource_id)\n        input_filename = item.get(\'name\')\n\n        # Call internal methods in this order\n        #   create_cluster()\n        #   create_slurm_script()\n        #   create_job()\n        #   upload_inputs()\n        #   submit_job()\n        print(\'Creating cluster on {}\'.format(MACHINE))\n        self.create_cluster(MACHINE)\n\n        # Create SLURM commands\n        print(\'Creating SLURM script {}\'.format(job_name))\n        command_list = list()\n        command_list.append(\'ulimit -s unlimited\')  # stack size\n        command_list.append(\'module load python/3.6-anaconda-4.4\')\n        command_list.append(\'source activate {}\'.format(CONDA_ENV_PATH))\n        command_list.append(\'export PYTHONPATH={}\'.format(GAIA_PATH))\n\n        # Last command is the python script itself\n        py_script = \'{}/nersc/crop.py\'.format(GAIA_PATH)\n\n        # For now, we have chache copies of input files on cori:\n        input_path = \'{}/data/{}\'.format(PROJECT_PATH, input_filename)\n        geometry_filename = \'crop_geometry.geojson\'\n        output_filename = \'output.tif\'\n        py_command = \'python {} {} {} {}\'.format(\n            py_script, input_path, geometry_filename, output_filename)\n\n        # Arguments\n        # -n number of nodes\n        # -c number of cpus per allocated process\n        # -u unbuffered (don\'t buffer terminal output - needed by cumulus)\n        command_list.append(\'srun -n 1 -c 1 -u {}\'.format(py_command))\n        self.create_slurm_script(\'metadata\', command_list)\n\n        print(\'Creating job {}\'.format(job_name))\n        self.create_job(job_name)\n\n        # Set job metadata - keywords used by smtk job panel\n        job_metadata = dict()\n        # job_metadata[\'solver\'] = solver\n        job_metadata[\'notes\'] = \'\'\n        number_of_nodes = 1\n        job_metadata[\'numberOfNodes\'] = number_of_nodes\n        # Total number of cores (1 core per task times number of nodes)\n        number_of_tasks = 1\n        job_metadata[\'numberOfCores\'] = number_of_nodes * number_of_tasks\n\n        # Time stamp (seconds since epoci)\n        job_metadata[\'startTimeStamp\'] = time.time()\n\n        # Plus one specific to our job\n        job_metadata[\'outputFilename\'] = output_filename\n        self.set_job_metadata(job_metadata)\n\n        print(\'Uploading geometry file\')\n        name = geometry_filename\n        geom_string = crop_object.get_data().to_json()\n        size = len(geom_string)\n        # print(\'geom_string:\', geom_string)\n        geom_stream = io.StringIO(geom_string)\n        self._girder_client.uploadFile(\n            self._input_folder_id, geom_stream, name, size, parentType=\'folder\')\n\n        print(\'Submitting job\')\n        datecode = datetime.datetime.now().strftime(\'%y%m%d\')\n        output_dir = \'{}/geolib/{}/{}\'.format(\n            self._nersc_scratch_folder, datecode, job_name)\n        return self.submit_job(MACHINE, nersc_repository, output_dir)\n\n    # ---------------------------------------------------------------------\n    def create_cluster(self, machine_name, cluster_name=None):\n        \'\'\'\n        \'\'\'\n        if cluster_name is None:\n            user = self._girder_client.get(\'user/me\')\n            user_name = user.get(\'firstName\', \'user\')\n            cluster_name = \'%s.%s\' % (machine_name, user_name)\n\n        cluster = None\n        cluster_list = self._girder_client.get(\'clusters\')\n        for extant_cluster in cluster_list:\n            if extant_cluster[\'name\'] == cluster_name:\n                cluster = extant_cluster\n                self._cluster_id = extant_cluster[\'_id\']\n                break\n\n        if not cluster:\n            body = {\n                \'config\': {\n                    \'host\': machine_name\n                },\n                \'name\': cluster_name,\n                \'type\': \'newt\'\n            }\n\n            r = self._girder_client.post(\'clusters\', data=json.dumps(body))\n            self._cluster_id = r[\'_id\']\n            print(\'cluster_id\', self._cluster_id)\n\n        # Reset the state of the cluster\n        body = {\n            \'status\': \'created\'\n        }\n        endpoint = \'clusters/%s\' % self._cluster_id\n        r = self._girder_client.patch(endpoint, data=json.dumps(body))\n\n        # Now test the connection\n        r = self._girder_client.put(\'clusters/%s/start\' % self._cluster_id)\n        sleeps = 0\n        while True:\n            time.sleep(1)\n            r = self._girder_client.get(\'clusters/%s/status\' % self._cluster_id)\n            # print(\'status\', r[\'status\'])\n\n            if r[\'status\'] == \'running\':\n                break\n            elif r[\'status\'] == \'error\':\n                r = self._girder_client.get(\n                    \'clusters/%s/log\' % self._cluster_id)\n                print(r)\n                raise Exception(\'ERROR creating cluster\')\n\n            if sleeps > 9:\n                raise Exception(\'Cluster never moved into running state\')\n            sleeps += 1\n\n    # ---------------------------------------------------------------------\n    def create_slurm_script(self, name, command_list):\n        \'\'\'Creates script to submit job\n        \'\'\'\n        body = {\n            \'commands\': command_list,\n            \'name\': name\n        }\n        r = self._girder_client.post(\'scripts\', data=json.dumps(body))\n        self._script_id = r[\'_id\']\n        print(\'script_id\', self._script_id)\n\n    # ---------------------------------------------------------------------\n    def create_job(self, job_name, tail=None):\n        \'\'\'\n        \'\'\'\n        # Create job folders\n        folder_name = uuid.uuid4().hex  # unique name\n        self._job_folder_id = self.get_folder(\n            self._private_folder_id, folder_name)\n        print(\'Created job folder\', folder_name)\n        self._input_folder_id = self.get_folder(\n            self._job_folder_id, \'input_files\')\n        self._output_folder_id = self.get_folder(\n            self._job_folder_id, \'output_files\')\n        # Make sure job_name isn\'t null\n        if not job_name:\n            job_name = \'CumulusJob\'\n\n        # Create job spec\n        body = {\n            \'name\': job_name,\n            \'scriptId\': self._script_id,\n            \'output\': [{\n                \'folderId\': self._output_folder_id,\n                \'path\': \'.\'\n            }],\n            \'input\': [\n                {\n                    \'folderId\': self._input_folder_id,\n                    \'path\': \'.\'\n                }\n            ]\n        }\n\n        if tail:\n            body[\'output\'].append({\n                ""path"": tail,\n                ""tail"": True\n            })\n\n        job = self._girder_client.post(\'jobs\', data=json.dumps(body))\n        self._job_id = job[\'_id\']\n        print(\'Created job_id\', self._job_id)\n\n    # ---------------------------------------------------------------------\n    def upload_inputs(self, files_to_upload, folders_to_upload=[]):\n        \'\'\'Uploads input files to (girder) input folder\n        \'\'\'\n        if not self._input_folder_id:\n            raise Exception(\'Input folder missing\')\n\n        def upload_file(path):\n            name = os.path.basename(path)\n            size = os.path.getsize(path)\n            with open(path, \'rb\') as fp:\n                self._girder_client.uploadFile(\n                    self._input_folder_id, fp, name, size, parentType=\'folder\')\n\n        for file_path in files_to_upload:\n            if not file_path or not os.path.exists(file_path):\n                raise Exception(\'Input file not found: %s\' % file_path)\n            upload_file(file_path)\n\n        for folder_path in folders_to_upload:\n            if not folder_path or not os.path.exists(folder_path):\n                raise Exception(\'Input folder not found: %s\' % folder_path)\n            # Create folder on girder\n            basename = os.path.basename(folder_path)\n            new_folder = self._girder_client.createFolder(\n                self._input_folder_id, basename)\n            # Upload files\n            pattern = \'%s/*\' % folder_path\n            self._girder_client.upload(pattern, new_folder._id)\n\n    # ---------------------------------------------------------------------\n    def submit_job(self,\n                   machine,\n                   project_account,\n                   job_output_dir,\n                   timeout_minutes=5,\n                   queue=\'debug\',\n                   qos=None,\n                   number_of_nodes=1):\n        \'\'\'\n        \'\'\'\n        body = {\n            \'machine\': machine,\n            \'account\': project_account,\n            \'numberOfNodes\': number_of_nodes,\n            \'maxWallTime\': {\n                \'hours\': 0,\n                \'minutes\': timeout_minutes,\n                \'seconds\': 0\n            },\n            \'queue\': queue,\n        }\n        if \'cori\' == machine:\n            body[\'constraint\'] = \'knl\'\n\n        if qos:\n            body[\'qualityOfService\'] = qos\n\n        body[\'jobOutputDir\'] = job_output_dir\n\n        print(\'submit_job body:\', body)\n        url = \'clusters/%s/job/%s/submit\' % (self._cluster_id, self._job_id)\n        self._girder_client.put(url, data=json.dumps(body))\n        print(\'Submitted job\', self._job_id)\n        return self._job_id\n\n    # ---------------------------------------------------------------------\n    def set_job_metadata(self, meta):\n        \'\'\'Writes metadata to job\n        \'\'\'\n        body = dict()\n        body[\'metadata\'] = meta\n        self._girder_client.patch(\n            \'jobs/%s\' % self._job_id, data=json.dumps(body))\n\n    # ---------------------------------------------------------------------\n    def download_results(self, destination_folder):\n        \'\'\'Downloads all output files to a local directory\n\n        \'\'\'\n        if not os.path.exists(destination_folder):\n            os.makedirs(destination_folder)\n\n        self._girder_client.downloadFolderRecursive(\n            self._output_folder_id, destination_folder)\n\n        print(\'Downloaded files to %s\' % destination_folder)\n\n    # ---------------------------------------------------------------------\n    def release_resources(self):\n        \'\'\'Closes/deletes any current resources\n\n        \'\'\'\n        resource_info = {\n            \'clusters\': [self._cluster_id],\n            \'jobs\': [self._job_id],\n            \'scripts\': [self._script_id],\n            \'folder\': [self._job_folder]\n        }\n        for resource_type, id_list in resource_info.items():\n            for resource_id in id_list:\n                if resource_id is not None:\n                    url = \'%s/%s\' % (resource_type, resource_id)\n                    self._girder_client.delete(url)\n\n        self._input_folder_id = None\n        self._job_folder_id = None\n        self._job_id = None\n        self._output_folder_id = None\n        self._script_id = None\n\n    # ---------------------------------------------------------------------\n    def get_folder(self, parent_id, name):\n        \'\'\'Returns folder_id, creating one if needed\n        \'\'\'\n        # Check if folder already exists\n        folder_list = list(self._girder_client.listFolder(parent_id, name=name))\n        if folder_list:\n            folder = folder_list[0]\n            return folder[\'_id\']\n\n        # (else)\n        try:\n            r = self._girder_client.createFolder(parent_id, name)\n            return r[\'_id\']\n        except HttpError as e:\n            print(e.responseText)\n\n        return None\n'"
gaia/io/gaia_reader.py,0,"b'from __future__ import absolute_import, division, print_function\nfrom builtins import (\n    bytes, str, open, super, range, zip, round, input, int, pow, object\n)\nfrom future.utils import with_metaclass\n\nfrom gaia.gaia_data import GaiaDataObject\nfrom gaia.util import (\n    GaiaException,\n    MissingParameterError,\n    MissingDataException,\n    UnsupportedFormatException,\n    get_uri_extension\n)\n\n\nclass GaiaReaderFactoryMetaclass(type):\n    """"""\n    This is the metaclass for any type deriving from GaiaReader, providing\n    us with a registry of possible readers, which we can use to look for an\n    appropriate subtype at runtime, based on constructor args.\n    """"""\n    _registry = {}\n\n    """"""\n    Make sure we include every GaiaReader subtype in our registry.\n    """"""\n    def __new__(cls, clsname, bases, dct):\n        classtoreturn = super(GaiaReaderFactoryMetaclass,\n                              cls).__new__(cls, clsname, bases, dct)\n        GaiaReaderFactoryMetaclass._registry[clsname] = classtoreturn\n        return classtoreturn\n\n    """"""\n    Intercept the constructor arguments generically generally when attempting\n    to instantiate a reader.  If we aren\'t in here via direct use of the\n    GaiaReader class itself, then the developer probably knows what specific\n    subtype she wants, so we make sure she gets that.  Otherwise, we will use\n    some heuristic to choose the subtype we construct.  If we see a\n    \'reader_class\' keyword argument, we try to choose that class.  If not, we\n    iterate through all the registered readers, looking for a subclass with a\n    static ""can_read"" method which returns true given the specific constructor\n    arguments we got.  Currently we construct the first one of these we find.\n    """"""\n    def __call__(cls, *args, **kwargs):\n        registry = GaiaReaderFactoryMetaclass._registry\n        subclass = None\n        instance = None\n\n        if id(cls) != id(GaiaReader):\n            # Allow for direct subclass instantiation\n            instance = cls.__new__(cls, args, kwargs)\n        else:\n            if \'reader_class\' in kwargs:\n                classname = kwargs[\'reader_class\']\n                if classname in registry:\n                    subclass = registry[classname]\n            else:\n                for classname, classinstance in registry.items():\n                    if hasattr(classinstance, \'can_read\'):\n                        canReadMethod = getattr(classinstance, \'can_read\')\n                        if canReadMethod(*args, **kwargs):\n                            subclass = classinstance\n                            # FIXME:\n                            break\n\n            if subclass:\n                instance = subclass.__new__(subclass, args, kwargs)\n            else:\n                argsstr = \'args: %s, kwargs: %s\' % (args, kwargs)\n                msg = \'Unable to find GaiaReader subclass for: %s\' % argsstr\n                raise GaiaException(msg)\n\n        if instance is not None:\n            instance.__init__(*args, **kwargs)\n\n        return instance\n\n\nclass GaiaReader(with_metaclass(GaiaReaderFactoryMetaclass, object)):\n    """"""\n    Abstract base class, root of the reader class hierarchy.\n    """"""\n    def __init__(self, *args, **kwargs):\n        pass\n\n    """"""\n    Return a GaiaDataObject\n    """"""\n    def read(self, data_source, format=None, epsg=None):\n        return GaiaDataObject(reader=self)\n\n    def load_metadata(self, dataObject):\n        print(\'GaiaReader _load_metadata()\')\n\n    def load_data(self, dataObject):\n        print(\'GaiaReader _load_data()\')\n'"
gaia/io/gdal_reader.py,0,"b'from __future__ import absolute_import, division, print_function\nfrom builtins import (\n    bytes, str, open, super, range, zip, round, input, int, pow, object\n)\n\nimport os\n\ntry:\n    import osr\nexcept ImportError:\n    from osgeo import osr\n\nimport gdal\nfrom gaia.geo.gdal_functions import (\n    gdal_reproject,\n    raster_to_numpy_array,\n    rasterio_bbox,\n    rasterio_footprint\n)\n\nfrom gaia.io.gaia_reader import GaiaReader\nfrom gaia.gaia_data import GDALDataObject\nfrom gaia.util import (\n    UnhandledOperationException,\n    UnsupportedFormatException,\n    get_uri_extension\n)\nimport gaia.formats as formats\nimport gaia.types as types\n\n\nclass GaiaGDALReader(GaiaReader):\n    """"""\n    A specific subclass for reading GDAL files\n    """"""\n    def __init__(self, url, *args, **kwargs):\n        super(GaiaGDALReader, self).__init__(*args, **kwargs)\n\n        self.uri = url\n        self.ext = \'.%s\' % get_uri_extension(self.uri)\n\n        self.as_numpy_array = False\n        self.as_single_band = True\n        self.old_nodata = None\n        self.new_nodata = None\n\n    @staticmethod\n    def can_read(url, *args, **kwargs):\n        # Todo update for girder-hosted files\n        if not isinstance(url, str):\n            return False\n\n        extension = get_uri_extension(url)\n        if extension == \'tif\' or extension == \'tiff\':\n            return True\n        return False\n\n    def read(self, format=formats.RASTER, epsg=None, as_numpy_array=False,\n             as_single_band=True, old_nodata=None, new_nodata=None):\n        """"""\n        Read data from a raster dataset\n\n        :param as_numpy_array: Output data as numpy\n        (default is False i.e. raster osgeo.gdal.Dataset)\n        :param as_single_band: Output data as 2D array of its first band\n        (default is True). If False, returns full 3D array.\n        :param old_nodata: Explicitly identify existing NoData values\n        (default None). If None, attempts to get existing NoData values stored\n        in the raster band.\n        :param new_nodata: Replace NoData values in each band with new_nodata\n        (default None). If new_nodata is not None but old_nodata is None\n        and no existing NoData value is stored in the band, uses unchanged\n        default ReadAsArray() return values.\n        :param epsg: EPSG code to reproject data to\n        :return: GDAL Dataset\n        """"""\n        self.format = format\n        self.epsg = epsg\n        self.as_numpy_array = as_numpy_array\n        self.as_single_band = as_single_band\n        self.old_nodata = old_nodata\n        self.new_nodata = new_nodata\n\n        # FIXME: if we got ""as_numpy_array=True"", should we return different\n        # data object type?\n        o = GDALDataObject(reader=self, dataFormat=self.format, epsg=self.epsg)\n        return o\n\n    def load_metadata(self, dataObject):\n        # self.__read_internal(dataObject)\n        data = dataObject.get_data()\n\n        # Get corner points\n        gt = data.GetGeoTransform()\n        if gt is None:\n            raise Exception(\n                \'Cannot compute corners - dataset has no geo transform\')\n        num_cols = data.RasterXSize\n        num_rows = data.RasterYSize\n        corners = list()\n        for px in [0, num_cols]:\n            for py in [0, num_rows]:\n                x = gt[0] + px*gt[1] + py*gt[2]\n                y = gt[3] + px*gt[4] + py*gt[5]\n                corners.append([x, y])\n\n        # if as_lonlat:\n        #     spatial_ref = osr.SpatialReference()\n        #     spatial_ref.ImportFromWkt(self.get_wkt_string())\n        #     corners = self._convert_to_lonlat(corners, spatial_ref)\n\n        xvals = [c[0] for c in corners]\n        yvals = [c[1] for c in corners]\n        xmin = min(xvals)\n        ymin = min(yvals)\n        xmax = max(xvals)\n        ymax = max(yvals)\n        coords = [[\n            [xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]\n        ]]\n        metadata = {\n            \'bounds\': {\n                \'coordinates\': coords\n            },\n            \'height\': data.RasterYSize,\n            \'width\': data.RasterXSize\n        }\n        # print(\'metadata: {}\'.format(metadata))\n        dataObject.set_metadata(metadata)\n\n    def load_data(self, dataObject):\n        self.__read_internal(dataObject)\n\n    def __read_internal(self, dataObject):\n        if self.ext not in formats.RASTER:\n            raise UnsupportedFormatException(\n                ""Only the following raster formats are supported: {}"".format(\n                    \',\'.join(formats.RASTER)\n                )\n            )\n        self.basename = os.path.basename(self.uri)\n\n        dataObject.set_data(gdal.Open(self.uri))\n\n        if self.epsg and dataObject.get_epsg() != self.epsg:\n            dataObject.reproject(self.epsg)\n\n        dataObject.set_metadata({})\n        dataObject._datatype = types.RASTER\n        dataObject._dataformat = formats.RASTER\n\n        if self.as_numpy_array:\n            raise UnhandledOperationException(\'Convert GDAL dataset to numpy\')\n            # np_data = raster_to_numpy_array(out_data, as_single_band,\n            #                                 old_nodata, new_nodata)\n'"
gaia/io/geojson_reader.py,0,"b'from __future__ import absolute_import, division, print_function\nfrom builtins import (\n    bytes, str, open, super, range, zip, round, input, int, pow, object\n)\n\nimport re\nfrom six import string_types\nimport geojson\nimport geopandas\n\nfrom gaia.io.readers import GaiaReader\nfrom gaia.gaia_data import GaiaDataObject\nfrom gaia import GaiaException\nfrom gaia.util import (\n    MissingParameterError,\n    MissingDataException,\n    UnsupportedFormatException,\n    get_uri_extension\n)\nimport gaia.formats as formats\nimport gaia.types as types\n\n\nclass GaiaGeoJSONReader(GaiaReader):\n    """"""\n    Another specific subclass for reading GeoJSON\n    """"""\n    epsgRegex = re.compile(\'epsg:([\\d]+)\')\n\n    def __init__(self, data_source, *args, **kwargs):\n        super(GaiaGeoJSONReader, self).__init__(*args, **kwargs)\n\n        self.geojson_object = None\n        self.uri = None\n        self.ext = None\n\n        if isinstance(data_source, string_types):\n            self.uri = data_source\n            self.ext = \'.%s\' % get_uri_extension(self.uri)\n        elif isinstance(data_source, geojson.GeoJSON):\n            self.geojson_object = data_source\n\n    @staticmethod\n    def can_read(data_source, *args, **kwargs):\n        if isinstance(data_source, string_types):\n            # Check string for a supported filename/url\n            extension = \'.{}\'.format(get_uri_extension(data_source))\n            if extension in formats.VECTOR:\n                return True\n            return False\n        elif isinstance(data_source, geojson.GeoJSON):\n            return True\n\n    def read(self, format=None, epsg=None):\n        return super().read(format, epsg)\n\n    def load_metadata(self, dataObject):\n        self.__read_internal(dataObject)\n\n    def load_data(self, dataObject):\n        self.__read_internal(dataObject)\n\n    def __read_internal(self, dataObject):\n        # FIXME: need to handle format\n        # if not self.format:\n        #     self.format = self.default_output\n\n        if self.uri:\n            if self.ext not in formats.VECTOR:\n                tpl = ""Only the following vector formats are supported: {}""\n                msg = tpl.format(\',\'.join(formats.VECTOR))\n                raise UnsupportedFormatException(msg)\n            data = geopandas.read_file(self.uri)\n\n        elif self.geojson_object:\n            if isinstance(self.geojson_object, geojson.geometry.Geometry):\n                feature = geojson.Feature(geometry=self.geojson_object)\n                features = geojson.FeatureCollection([feature])\n            elif isinstance(self.geojson_object, geojson.Feature):\n                features = geojson.FeatureCollection([self.geojson_object])\n            elif isinstance(self.geojson_object, geojson.FeatureCollection):\n                features = self.geojson_object\n            else:\n                raise UnsupportedFormatException(\n                    \'Unrecognized geojson object {}\'.self.geojson_object)\n\n            # For now, hard code crs to lat-lon\n            data = geopandas.GeoDataFrame.from_features(\n                features, crs=dict(init=\'epsg:4326\'))\n\n        # FIXME: still need to handle filtering\n        # if self.filters:\n        #     self.filter_data()\n\n        # FIXME: skipped the transformation step for now\n        # return self.transform_data(format, epsg)\n\n        # Initialize metadata\n        metadata = dict()\n\n        # Calculate bounds\n        feature_bounds = data.bounds\n        minx = feature_bounds[\'minx\'].min()\n        miny = feature_bounds[\'miny\'].min()\n        maxx = feature_bounds[\'maxx\'].max()\n        maxy = feature_bounds[\'maxy\'].max()\n\n        # Hack format to match resonant geodata (geojson polygon)\n        coords = [[\n            [minx, miny], [], [maxx, maxy], []\n        ]]\n        metadata[\'bounds\'] = dict(coordinates=coords)\n\n        dataObject.set_metadata(metadata)\n\n        dataObject.set_data(data)\n        epsgString = data.crs[\'init\']\n\n        m = self.epsgRegex.search(epsgString)\n        if m:\n            dataObject._epsg = int(m.group(1))\n        dataObject._datatype = types.VECTOR\n        dataObject._dataformat = formats.VECTOR\n'"
gaia/io/girder_interface.py,0,"b'from __future__ import print_function\n\nimport requests\n\nimport girder_client\n\nfrom gaia.util import GaiaException, MissingParameterError\n\n\nclass GirderInterface(object):\n    """"""An internal class that provides a thin encapsulation of girder_client.\n\n    This class must be used as a singleton.\n    """"""\n\n    instance = None  # singleton\n\n    def __init__(self):\n        """"""Applies crude singleton pattern (raise exception if called twice)\n        """"""\n        if GirderInterface.instance:\n            msg = """"""GirderInterface already exists \\\n            -- use get_instance() class method""""""\n            raise GaiaException(msg)\n\n        GirderInterface.instance = self\n        self.girder_url = None\n        self.gc = None  # girder client\n        self.user = None  # girder user object\n        self.gaia_folder = None\n        self.default_folder = None\n        self.nersc_requests = None  # requests session\n\n    @classmethod\n    def get_instance(cls):\n        """"""Returns singleton instance, creating if needed.\n        """"""\n        if cls.instance is None:\n            cls.instance = cls()\n\n        return cls.instance\n\n    @classmethod\n    def is_initialized(cls):\n        if cls.instance is None:\n            return False\n\n        if cls.instance.gc is None:\n            return False\n\n        # (else)\n        return True\n\n    def initialize(\n            self,\n            girder_url,\n            username=None,\n            password=None,\n            apikey=None,\n            newt_sessionid=None):\n        """"""Connect to girder server and authenticate with input credentials\n\n        :param girder_url: The full path to the Girder instance, for example,\n        \'http://localhost:80\' or \'https://my.girder.com\'.\n        :param username: The name for logging into Girder.\n        :param password: The password for logging into Girder.\n        :apikey: An api key, which can be used instead of username & password.\n        :newt_sessionid: (string) Session token from NEWT web service at NERSC.\n           (Girder must be connected to NEWT service to authenicate.)\n        """"""\n        if self.__class__.is_initialized():\n            msg = """"""GirderInterface already initialized -- \\\n                cannot initialize twice""""""\n            raise GaiaException(msg)\n\n        self.girder_url = girder_url\n        # Check that we have credentials\n\n        api_url = \'{}/api/v1\'.format(girder_url)\n        # print(\'api_url: {}\'.format(api_url))\n        self.gc = girder_client.GirderClient(apiUrl=api_url)\n\n        if username is not None and password is not None:\n            self.gc.authenticate(username=username, password=password)\n        elif apikey is not None:\n            self.gc.authenticate(apiKey=apikey)\n        elif newt_sessionid is not None:\n            self.nersc_requests = requests.Session()\n            url = \'{}/newt/authenticate/{}\'.format(api_url, newt_sessionid)\n            r = self.nersc_requests.put(url)\n            r.raise_for_status()\n            self.nersc_requests.cookies.update(dict(\n                newt_sessionid=newt_sessionid))\n            # self.nersc_requests.cookies.set(\'newt_sessionid\', newt_sessionid)\n\n            # Get scratch directory\n            data = {\n                \'executable\': \'/usr/bin/echo $SCRATCH\',\n                \'loginenv\': \'true\'\n            }\n            machine = \'cori\'\n            NERSC_URL = \'https://newt.nersc.gov/newt\'\n            url = \'%s/command/%s\' % (NERSC_URL, machine)\n            r = self.nersc_requests.post(url, data=data)\n            r.raise_for_status()\n            print(r.json())\n\n            self.gc.token = self.nersc_requests.cookies[\'girderToken\']\n        else:\n            raise MissingParameterError(\'No girder credentials provided.\')\n\n        # Get user info\n        self.user = self.gc.getUser(\'me\')\n\n        # Get or intialize Private/gaia/default folder\n        private_list = self.gc.listFolder(\n            # self.user[\'_id\'], parentFolderType=\'user\', name=\'Private\')\n            # HACK FOR DEMO - use public folder until we set up\n            # mechanism to send girder token to js client\n            self.user[\'_id\'], parentFolderType=\'user\', name=\'Public\')\n        try:\n            private_folder = next(private_list)\n        except StopIteration:\n            raise GaiaException(\'User/Private folder not found\')\n\n        gaia_list = self.gc.listFolder(\n            private_folder[\'_id\'], parentFolderType=\'folder\', name=\'gaia\')\n        try:\n            self.gaia_folder = next(gaia_list)\n        except StopIteration:\n            description = \'Created by Gaia\'\n            self.gaia_folder = self.gc.createFolder(\n                private_folder[\'_id\'], \'gaia\', description=description)\n\n        default_list = self.gc.listFolder(\n            self.gaia_folder[\'_id\'], parentFolderType=\'folder\', name=\'default\')\n        try:\n            self.default_folder = next(default_list)\n        except StopIteration:\n            description = \'Created by Gaia\'\n            self.default_folder = self.gc.createFolder(\n                self.gaia_folder[\'_id\'], \'default\', description=description)\n            print(\'Created gaia/default folder\')\n\n    def lookup_url(self, path=None, job_id=None, test=False):\n        """"""Returns internal url for resource at specified path\n\n        :param path: (string) Girder path, from user\'s root to resource\n        :param job_id: (string) Girder job id, represents processing job\n            submitted to remote machine\n        :param test: (boolean) if True, raise exception if resource not found\n\n        Either path or job_id must be specified (but not both!)\n        """"""\n        if path:\n            resource = self.lookup_resource(path, test)\n            if resource is None:\n                return None\n\n            # (else) construct ""gaia"" url\n            resource_type = resource[\'_modelType\']\n            resource_id = resource[\'_id\']\n            gaia_url = \'girder://{}/{}\'.format(resource_type, resource_id)\n            return gaia_url\n        elif job_id:\n            job_endpoint = \'jobs/{}\'.format(job_id)\n            job_info = self.gc.get(job_endpoint)\n            if not job_info:\n                raise GaiaException(\'Job not found on girder\')\n\n            status = job_info.get(\'status\')\n            if status != \'complete\':\n                print(\'job_info:\\n\', job_info)\n                raise GaiaException(\n                    \'Job status not complete ({})\'.format(status))\n\n            output_folder_id = job_info.get(\'output\', [])[0].get(\'folderId\')\n            # print(\'output_folder_id\', output_folder_id)\n\n            # Output filename is stored in metadata\n            default_filename = \'output.tif\'\n            metadata = job_info.get(\'metadata\', {})\n            output_filename = metadata.get(\'outputFilename\', default_filename)\n\n            # Get item id\n            params = dict(\n                folderId=output_folder_id, name=output_filename, limit=1)\n            output_list = self.gc.get(\'item\', parameters=params)\n            # print(output_list)\n            if output_list:\n                output_info = output_list[0]\n                output_item_id = output_info.get(\'_id\', \'missing\')\n                # print(\'Output file {} is item id {}\'.format(\n                #     output_filename, output_item_id))\n            else:\n                raise GaiaException(\n                    \'Output file {} not found\'.format(output_filename))\n\n            # Create gaia object for output\n            gaia_url = \'girder://item/{}\'.format(output_item_id)\n            return gaia_url\n        else:\n            raise MissingParameterError(\n                \'Must specify either path or job_id argument\')\n\n    def lookup_resource(self, path, test=True):\n        """"""Does lookup of resource at specified path\n\n        :param path: (string) Girder path, from user\'s root to resource\n        :param test: (boolean) if True, raise exception if resource not found\n        :return: (object) resource info including _id, and name\n        """"""\n        if path.startswith(\'/\'):\n            girder_path = path\n        else:\n            girder_path = \'user/{}/{}\'.format(self.user[\'login\'], path)\n        resource = self.gc.get(\n            \'resource/lookup\', parameters={\'path\': girder_path, \'test\': test})\n        return resource\n\n    def ls(self, path, text=None, name=None, offset=0, limit=40,\n            formatted=True):\n        """"""Returns list of files at specified girder path (folder)\n\n        :param formatted: (bool) if true, return abridged, formatted list\n\n        """"""\n        # Lookup folder id for given path\n        folder_resource = self.lookup_resource(path)\n        folder_id = folder_resource.get(\'_id\')\n\n        # Get items in that folder\n        gen = self.gc.listItem(folder_id)\n        contents = list(gen)\n        if not formatted:\n            return contents\n\n        # (else) generate a list of abridged, formatted items\n        def sizeof_fmt(num, suffix=\'B\'):\n            \'\'\'Converts number to human-readable form\'\'\'\n            for unit in [\'\', \'K\', \'M\', \'G\', \'T\', \'P\', \'E\', \'Z\']:\n                if abs(num) < 1024.0:\n                    return ""%3.1f %s%s"" % (num, unit, suffix)\n                num /= 1024.0\n            return ""%.1f %s%s"" % (num, \'Y\', suffix)\n\n        content_list = [None] * len(contents)\n        for i, item in enumerate(contents):\n            sizeof = sizeof_fmt(item.get(\'size\'))\n            content_list[i] = [sizeof, item.get(\'name\')]\n\n        return content_list\n\n    @classmethod\n    def _get_default_folder_id(cls):\n        """"""Returns id for default folder\n\n        For internal use only\n        """"""\n        instance = cls.get_instance()\n        if instance.default_folder is None:\n            return None\n        # (else)\n        return instance.default_folder.get(\'_id\')\n\n    @classmethod\n    def _get_girder_client(cls):\n        """"""Returns GirderClient instance\n\n        For internal use only\n        """"""\n        if cls.instance is None:\n            raise GaiaException(\'GirderInterface not initialized\')\n\n        if cls.instance.gc is None:\n            raise GaiaException(\'GirderClient not initialized\')\n\n        return cls.instance.gc\n'"
gaia/io/girder_reader.py,0,"b'from __future__ import absolute_import, print_function\nimport os\n\nfrom gaia import GaiaException\nfrom gaia.girder_data import GirderDataObject\nfrom gaia.io.gaia_reader import GaiaReader\nfrom gaia.io.girder_interface import GirderInterface\nimport gaia.formats as formats\n# import gaia.types as types\n\n\nclass GirderReader(GaiaReader):\n    """"""\n    A specific subclass for reading GDAL files\n    """"""\n    def __init__(self, data_source, *args, **kwargs):\n        """"""\n        """"""\n        super(GirderReader, self).__init__(*args, **kwargs)\n        self.girder_source = data_source\n        self.url = None\n        # Bounds can be pass in optionally\n        self.bounds = kwargs.get(\'bounds\')\n\n        if isinstance(data_source, str):\n            self.url = data_source\n        elif isinstance(data_source, tuple):\n            self.girder_source = data_source\n\n    @staticmethod\n    def can_read(source, *args, **kwargs):\n        # For now, support either url (string) or tuple (GirderInterface,path)\n        if isinstance(source, str):\n            girder_scheme = \'girder://\'\n            if source is not None and source.startswith(girder_scheme):\n                result = GirderReader._parse_girder_url(source)\n                if result is None:\n                    return False\n\n                # Todo Confirm that resource exists on girder?\n                return True\n\n            # (else)\n            return False\n        else:\n            if not isinstance(source, tuple) or not len(source) == 2:\n                return False\n\n            gint, path = source\n            if not isinstance(gint, GirderInterface):\n                return False\n\n            if not isinstance(path, str):\n                raise GaiaException(\'Second tuple element is not a string\')\n\n            if not gint.is_initialized():\n                msg = """"""Cannot read girder object; \\\n                    must first call gaia.connect()""""""\n                raise GaiaException(msg)\n\n        # (else)\n        return True\n\n    def read(self, **kwargs):\n        """"""Returns a GirderDataset\n\n        Doesn\'t actally load or move data; it remains on Girder\n        Todo: kwargs should probably be a union of raster and vector types,\n        that get passed to GirderDataset\n\n        :return: Girder Dataset\n        """"""\n        if self.url:\n            parsed_result = self.__class__._parse_girder_url(self.url)\n            if parsed_result is None:\n                raise GaiaException(\'Internal error - not a girder url\')\n\n            resource_type, resource_id = parsed_result\n            return GirderDataObject(\n                self, resource_type, resource_id, bounds=self.bounds)\n\n        elif self.girder_source:\n            gint, path = self.girder_source\n            resource = gint.lookup_resource(path)\n            if resource is None:\n                template = \'File not found on Girder at specified path ({})\'\n                msg = template.format(path)\n                raise GaiaException(msg)\n\n            resource_type = resource[\'_modelType\']\n            resource_id = resource[\'_id\']\n            return GirderDataObject(\n                self, resource_type, resource_id, bounds=self.bounds)\n\n        raise GaiaException(\n            \'Internal error - should never reach end of GirderReader.read()\')\n        return None\n\n    def load_metadata(self, dataObject):\n        # Todo\n        pass\n\n    @staticmethod\n    def _parse_girder_url(url):\n        """"""\n\n        Returns either None or tuple(resource_type, resource_id)\n        """"""\n        if url is None:\n            raise GaiaException(\'Internal error - url is None\')\n\n        girder_scheme = \'girder://\'\n        if not url.startswith(girder_scheme):\n            return None\n\n        # Extract resource type (file or folder) and id\n        start_index = len(girder_scheme)\n        path_string = url[start_index:]\n        path_list = path_string.split(\'/\')\n        # print(\'path_list: \', path_list)\n        if (len(path_list) != 2):\n            raise GaiaException(\'Invalid girder url; path must be length 2\')\n\n        resource_type, resource_id = path_list\n        if (resource_type not in [\'item\', \'folder\']):\n            msg = """"""Invalid girder url; path must start with either \\\n                \\""item/\\"" or \\""folder/\\""""""""\n            raise GaiaException(msg)\n\n        return resource_type, resource_id\n'"
gaia/io/postgis_reader.py,0,"b""from __future__ import absolute_import, division, print_function\nfrom builtins import (\n    bytes, str, open, super, range, zip, round, input, int, pow, object\n)\n\nfrom gaia.gaia_data import PostgisDataObject\nfrom gaia.io.readers import GaiaReader\n\nimport gaia.types as types\nimport gaia.formats as formats\n\n\nclass GaiaPostGISReader(GaiaReader):\n    required_arguments = ['table', 'dbname', 'hostname', 'user', 'password']\n\n    def __init__(self, *args, **kwargs):\n        super(GaiaPostGISReader, self).__init__(*args, **kwargs)\n        self._args = args\n        self._kwargs = kwargs\n\n    def read(self, format=None, epsg=None):\n        print('GaiaPostGISReader read()')\n        dataObject = PostgisDataObject(reader=self)\n        dataObject.format = format\n        dataObject.epsg = epsg\n        return dataObject\n\n    def load_metadata(self, dataObject):\n        self.__set_db_properties(dataObject)\n\n    def load_data(self, dataObject):\n        self.__set_db_properties(dataObject)\n\n    def __set_db_properties(self, dataObject):\n        for key in self.required_arguments:\n            if key in self._kwargs:\n                print('  Setting property %s to %s' % (key, self._kwargs[key]))\n                setattr(dataObject, key, self._kwargs[key])\n        dataObject.initialize_engine()\n        dataObject._datatype = types.VECTOR\n        dataObject._dataformat = formats.VECTOR\n\n    @staticmethod\n    def can_read(*args, **kwargs):\n        for arg in GaiaPostGISReader.required_arguments:\n            if arg not in kwargs:\n                return False\n        return True\n"""
gaia/io/readers.py,0,b'from gaia.io.gaia_reader import GaiaReader\nfrom gaia.io.geojson_reader import GaiaGeoJSONReader\nfrom gaia.io.gdal_reader import GaiaGDALReader\nfrom gaia.io.girder_reader import GirderReader\n'
gaia/io/writers.py,0,"b""from __future__ import absolute_import, division, print_function\nimport os\n\ntry:\n    import osr\nexcept ImportError:\n    from osgeo import osr\n\nimport gdal\nimport geopandas\n\n\nfrom gaia import types\nfrom gaia.gaia_data import GaiaDataObject\nfrom gaia.util import GaiaException\n\n# Map of <file-extension, driver-name> for GeoPandas\nGEOPANDAS_DRIVERS = {\n    '.geojson': 'GeoJSON',\n    '.json': 'GeoJSON',\n    '.shp': 'ESRI Shapefile'\n}\n\n# Map of <file-extension, driver-name> for GDAL\nGDAL_DRIVERS = {\n    '.geotiff': 'GTiff',\n    '.jp2': 'JP2OpenJPEG',\n    '.jp2k': 'JP2OpenJPEG',\n    '.ntf': 'NITF',\n    '.tif': 'GTiff'\n}\n\n\ndef write_gaia_object(gaia_object, filename, **options):\n    if gaia_object.__class__.__name__ == 'GirderDataObject':\n        raise GaiaException('Writing not supported for GirderDataObject')\n\n    data_type = gaia_object._getdatatype()\n    if data_type == types.VECTOR:\n        return write_vector_object(gaia_object, filename, **options)\n    elif data_type == types.RASTER:\n        return write_raster_object(gaia_object, filename, **options)\n    else:\n        raise GaiaException('Unsupported data type {}'.format(data_type))\n\n\ndef write_vector_object(gaia_object, filename, **options):\n    # Delete existing file (if any)\n    if os.path.exists(filename):\n        os.remove(filename)\n\n    data = gaia_object.get_data()\n    ext = os.path.splitext(filename)[1]\n    if ext == '':\n        ext = '.geojson'  # default\n    driver = GEOPANDAS_DRIVERS.get(ext)\n    if driver is None:\n        raise GaiaException('Unsupported file extension {}'.format(ext))\n    data.to_file(filename, driver, **options)\n\n\ndef write_raster_object(gaia_object, filename, **options):\n    # Delete existing file (if any)\n    if os.path.exists(filename):\n        os.remove(filename)\n\n    ext = os.path.splitext(filename)[1]\n    if ext == '':\n        ext = '.tif'  # default\n    driver_name = GDAL_DRIVERS.get(ext)\n    if driver_name is None:\n        raise GaiaException('Unsupported file extension {}'.format(ext))\n\n    # Have to create copy of dataset in order to write to file\n    driver = gdal.GetDriverByName(driver_name)\n    if driver is None:\n        raise GaiaException('GDAL driver {} not found'.format(driver_name))\n\n    gdal_dataset = gaia_object.get_data()\n    output_dataset = driver.CreateCopy(filename, gdal_dataset, strict=0)\n    # Setting the dataset to None causes the write to disk\n    # Add # noqa comment to ignore flake8 error that variable isn't used\n    output_dataset = None  # writes to disk  # noqa: F841\n"""
gaia/preprocess/__init__.py,0,"b'\nfrom gaia.preprocess.pandas_processes import *\nfrom gaia.preprocess.gdal_processes import *\nfrom gaia.preprocess.girder_processes import *\n\nfrom gaia.process_registry import compute\n\n\n# def crop (inputs=[], args={}):\ndef crop (*args, **kwargs):\n    """"""Crop dataset(s) to specified geometry\n\n    :param datasets: list of datasets to crop\n    :param geometry: crop geometry in GeoJSON format\n    :param name: optional name for resulting dataset\n    :return: dataset or None if no intersection\n    """"""\n    return compute(\'crop\', inputs=list(args), args=kwargs)\n\n# def centroid(inputs=[], args={}):\n#     return compute(\'centroid\', inputs=inputs, args=args)\n\n# def intersection(inputs=[], args={}):\n#     return compute(\'intersection\', inputs=inputs, args=args)\n\n# def within(inputs=[], args=[]):\n#     return compute(\'within\', inputs=inputs, args=args)\n\n# def subset(inputs=[], args=[]):\n#     return compute(\'subset\', inputs=inputs, args=args)\n'"
gaia/preprocess/gdal_processes.py,0,"b'from __future__ import absolute_import, division, print_function\nfrom builtins import (\n    bytes, str, open, super, range, zip, round, input, int, pow, object\n)\n\nfrom gaia import GaiaException\nfrom gaia.gaia_data import GDALDataObject\nfrom gaia.validators import validate_subset\nfrom gaia.process_registry import register_process\nfrom gaia.geo.gdal_functions import gdal_clip\nfrom gaia.io.gdal_reader import GaiaGDALReader\nimport gaia.types\n\n\ndef validate_gdal(v):\n    """"""\n    Rely on the base validate method for the bulk of the work, just make\n    sure the inputs are gdal-compatible.\n    """"""\n    def validator(inputs=[], args=[]):\n        # FIXME: we should check we have a specific gdal type input also\n        return v(inputs, args)\n    return validator\n\n\n@register_process(\'crop\')\n@validate_subset\n@validate_gdal\ndef compute_subset_gdal(inputs=[], args=[]):\n    """"""\n    Runs the subset computation, creating a raster dataset as output.\n    """"""\n    raster, clip = inputs[0], inputs[1]\n    raster_img = raster.get_data()\n\n    if clip.get_epsg() != raster.get_epsg():\n        clip.reproject(raster.get_epsg())\n\n    clip_json = clip.get_data().geometry.unary_union.__geo_interface__\n\n    # Passing ""None"" as second arg instead of a file path.  This tells gdal_clip\n    # not to write the output dataset to a tiff file on disk\n    output_dataset = gdal_clip(raster_img, None, clip_json)\n    if output_dataset is None:\n        return None\n\n    # Copy data to new GDALDataObject\n    outputDataObject = GDALDataObject()\n    outputDataObject.set_data(output_dataset)\n    outputDataObject._datatype = gaia.types.RASTER\n\n    # Instantiate temporary reader to (only) parse metadata\n    reader = GaiaGDALReader(\'internal.tif\')\n    reader.load_metadata(outputDataObject)\n\n    return outputDataObject\n'"
gaia/preprocess/girder_processes.py,0,"b'from __future__ import absolute_import, division, print_function\nfrom builtins import (\n    bytes, str, open, super, range, zip, round, input, int, pow, object\n)\nimport sys\nfrom urllib.parse import urlencode\n\nimport collections\nimport json\n\nimport gaia.types\nimport gaia.validators as validators\nfrom gaia.util import GaiaException\nfrom gaia.gaia_data import GaiaDataObject\nfrom gaia.girder_data import GirderDataObject\nfrom gaia.process_registry import register_process\n\n\ndef validate_girder(v):\n    """"""\n    Verify that inputs are all girder objects\n    """"""\n    def validator(inputs=[], args={}):\n        # First object must be GirderDataObject\n        if (type(inputs[0]) is not GirderDataObject):\n            raise GaiaException(\'girder process requires GirderDataObject\')\n\n        # Second object must have vector geometry\n        if (isinstance(inputs[1], GaiaDataObject) and\n                inputs[1].get_datatype() != gaia.types.VECTOR):\n            template = """"""girder process cannot use datatype \\""{}\\"""" \\\n                for crop geometry""""""\n            raise GaiaException(template.format(inputs[1].get_datatype()))\n\n        # For now, second object/geometry must be on local filesystem\n        if isinstance(inputs[1], GirderDataObject):\n            raise GaiaException(\'crop geometry on girder not supported\')\n\n        # Otherwise call up the chain to let parent do common validation\n        return v(inputs, args)\n\n    return validator\n\n\n@register_process(\'crop\')\n@validate_girder\ndef compute_girder_crop(inputs=[], args_dict={}):\n    """"""\n    Runs the subset computation on girder\n    """"""\n    datasets = inputs[0]\n    if isinstance(inputs[1], GaiaDataObject):\n        geometry = inputs[1].get_data()\n    else:\n        geometry = inputs[1]\n    # print(\'datasets: \', datasets)\n    # print(\'geometry:\', geometry)\n    # print(\'args_dict:\', args_dict)\n\n    # if not isinstance(input, collections.Iterable):\n    #     datasets = [datasets]\n\n    # Current support is single dataset\n\n    filename = args_dict.get(\'name\', \'crop2_output.tif\')\n    # print(filename)\n\n    from gaia.io.girder_interface import GirderInterface\n    gc = GirderInterface._get_girder_client()\n    results_folder_id = GirderInterface._get_default_folder_id()\n\n    # Check for existing file (and delete)\n    result = gc.listItem(results_folder_id, name=filename)\n    # print(result)\n    for item in result:\n        item_path = \'item/{}\'.format(item[\'_id\'])\n        print(\'Deleting existing (item {})\'.format(filename, item_path))\n        del_result = gc.delete(item_path)\n        print(del_result)\n\n    # Run the clip operation\n    path = \'raster/clip\'\n    params = {\n        \'itemId\': datasets.resource_id,\n        \'geometry\': json.dumps(geometry),\n        \'name\': filename,\n        \'folderId\': results_folder_id\n    }\n    job = gc.get(path, parameters=params)\n    # print()\n    # print(job)\n\n    import time\n    path = \'job/{}\'.format(job[\'_id\'])\n    status = job[\'status\']\n    for i in range(50):\n        if status >= 3:\n            break\n        # (Using sys.stdout to skip newline at end)\n        sys.stdout.write(\n            \'{:2d} Checking job {} status... \'.format(i+1, job[\'_id\']))\n        time.sleep(1.0)\n        job = gc.get(path)\n        status = job[\'status\']\n        print(status)\n\n    # Get item id of (new) output file\n    result = gc.listItem(results_folder_id, name=filename)\n    cropped_item = next(result)\n    # print(cropped_item)\n\n    outputDataObject = GirderDataObject(None, \'item\', cropped_item[\'_id\'])\n    return outputDataObject\n'"
gaia/preprocess/pandas_processes.py,0,"b'from __future__ import absolute_import, division, print_function\nfrom builtins import (\n    bytes, str, open, super, range, zip, round, input, int, pow, object\n)\n\nimport gaia.formats\nimport gaia.types\nimport gaia.validators as validators\nfrom gaia.process_registry import register_process\nfrom gaia import GaiaException\nfrom gaia.gaia_data import GaiaDataObject\n\nfrom geopandas import GeoDataFrame\nfrom geopandas import GeoSeries\n\n\ndef validate_pandas(v):\n    """"""\n    Since the base validate method we import does most of the work in a fairly\n    generic way, this function only needs to add a little bit to that: make\n    sure the inputs contain geopandas dataframe.  Additionally, all the\n    processes defined in this module can re-use the same validate method.\n    """"""\n    def validator(inputs=[], args={}):\n        # First should check if input is compatible w/ pandas computation\n        if type(inputs[0].get_data()) is not GeoDataFrame:\n            raise GaiaException(\'pandas process requires a GeoDataFrame\')\n\n        # Otherwise call up the chain to let parent do common validation\n        return v(inputs, args)\n\n    return validator\n\n\n@register_process(\'crop\')\n@validators.validate_within\n@validate_pandas\ndef crop_pandas(inputs=[], args={}):\n    """"""\n    Calculate the within process using pandas GeoDataFrames\n\n    :return: within result as a GaiaDataObject,\n             or None if no intersection\n    """"""\n    first, second = inputs[0], inputs[1]\n    if first.get_epsg() != second.get_epsg():\n        second.reproject(epsg=first.get_epsg())\n    first_df, second_df = first.get_data(), second.get_data()\n    first_within = first_df[first_df.geometry.within(\n        second_df.geometry.unary_union)]\n\n    if first_within.empty:\n        return None\n\n    # Construct GaiaDataObject manually\n    # Todo consider adding static method to GaiaDataObject\n    outputDataObject = GaiaDataObject(\n        reader=None, dataFormat=gaia.formats.PANDAS, epsg=first.get_epsg())\n    outputDataObject.set_data(first_within)\n    outputDataObject._datatype = gaia.types.VECTOR\n\n    # Construct bounds, which uses geojson format\n    geometry = first_within[\'geometry\']\n    geopandas_bounds = geometry.total_bounds\n    xmin, ymin, xmax, ymax = geopandas_bounds\n    coords = [[\n        [xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]\n    ]]\n    metadata = {\n        \'bounds\': {\n            \'coordinates\': coords\n        }\n    }\n    outputDataObject.set_metadata(metadata)\n    return outputDataObject\n\n\n# """"""\n# These methods can be very small and focused on doing only one thing: given\n# an array of inputs and possibly some arguments, do the computation and\n# return something (maybe a GaiaDataObject, or perhaps just a number)\n# """"""\n# @register_process(\'centroid\')\n# @validate_centroid\n# @validate_pandas\n# def compute_centroid_pandas(inputs=[], args={}):\n#     """"""\n#     Calculate the centroid using pandas GeoDataFrames\n\n#     :return: centroid as a GeoDataFrame\n#     """"""\n\n#     # Only worry about a pandas version of the compute method\n#     print(\'compute_centroid_pandas\')\n\n#     df_in = inputs[0].get_data()\n#     df = GeoDataFrame(df_in.copy(), geometry=df_in.geometry.name)\n#     if \'combined\' in args and args[\'combined\']:\n#         gs = GeoSeries(df.geometry.unary_union.centroid,\n#                        name=df_in.geometry.name)\n#         output_data = GeoDataFrame(gs)\n#     else:\n#         df[df.geometry.name] = df.geometry.centroid\n#         output_data = df\n\n#     # Now processes need to create and return a GaiaDataObject, whose\n#     # ""data"" member contains the actual data\n#     outputDataObject = GaiaDataObject()\n#     outputDataObject.set_data(output_data)\n\n#     return outputDataObject\n\n\n# """"""\n# Do a pandas intersection\n# """"""\n# @register_process(\'intersection\')\n# @validate_intersection\n# @validate_pandas\n# def compute_intersection_pandas(inputs=[], args={}):\n#     print(\'compute_intersection_pandas\')\n#     return None\n'"
tests/cases/__init__.py,0,b''
tests/cases/test_create.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n###############################################################################\n#  Copyright Kitware Inc. and Epidemico Inc.\n#\n#  Licensed under the Apache License, Version 2.0 ( the ""License"" );\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\nimport os\nimport json\nimport unittest\nfrom zipfile import ZipFile\nimport gaia\nfrom gaia.preprocess import crop\n\nbase_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)))\ntestfile_path = os.path.join(base_dir, \'../data\')\n\n\nclass TestCreateAPI(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        config_file = os.path.join(base_dir, \'../../gaia/conf/gaia.cfg\')\n        gaia.get_config(config_file)\n\n    def test_create_api(self):\n        """"""\n        Test cropping (within process) for vector inputs\n        """"""\n        path1 = os.path.join(testfile_path, \'iraq_hospitals.geojson\')\n        path2 = os.path.join(testfile_path, \'baghdad_districts.geojson\')\n\n        data1 = gaia.create(path1)\n        data2 = gaia.create(path2)\n\n        output = crop(data1, data2)\n\n        self.assertEqual(len(output.get_data()), 19)\n'"
tests/cases/test_processes.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n###############################################################################\n#  Copyright Kitware Inc. and Epidemico Inc.\n#\n#  Licensed under the Apache License, Version 2.0 ( the ""License"" );\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\nimport os\nimport json\nimport unittest\nfrom zipfile import ZipFile\n\nimport geojson\n\nimport gaia\nfrom gaia.preprocess import crop\nfrom gaia.io import readers\n\nbase_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)))\ntestfile_path = os.path.join(base_dir, \'../data\')\n\n\nclass TestGaiaProcesses(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        config_file = os.path.join(base_dir, \'../../gaia/conf/gaia.cfg\')\n        gaia.get_config(config_file)\n\n        zipfile = ZipFile(os.path.join(testfile_path, \'2states.zip\'), \'r\')\n        zipfile.extract(\'2states.geojson\', testfile_path)\n\n    @classmethod\n    def tearDownClass(cls):\n        twostates_path = os.path.join(testfile_path, \'2states.geojson\')\n        if os.path.exists(twostates_path):\n            os.remove(twostates_path)\n\n    def test_crop_pandas(self):\n        """"""\n        Test cropping (within process) for vector inputs\n        """"""\n        reader1 = readers.GaiaReader(\n            os.path.join(testfile_path, \'iraq_hospitals.geojson\'))\n\n        reader2 = readers.GaiaReader(\n            os.path.join(testfile_path, \'baghdad_districts.geojson\'))\n\n        output = crop(reader1.read(), reader2.read())\n\n        self.assertEqual(len(output.get_data()), 19)\n\n    def test_crop_vector_null(self):\n        """"""Test case where vector intersection is null""""""\n        source_path = os.path.join(testfile_path, \'2states.geojson\')\n        source = gaia.create(source_path)\n\n        tool_path = os.path.join(testfile_path, \'iraq_hospitals.geojson\')\n        tool = gaia.create(tool_path)\n\n        cropped = crop(source, tool)\n        self.assertIsNone(cropped)\n\n    def test_crop_gdal(self):\n        """"""\n        Test cropping (subset process) for vector & raster inputs\n        """"""\n        try:\n            reader1 = readers.GaiaReader(\n                os.path.join(testfile_path, \'globalairtemp.tif\'))\n            rasterData = reader1.read()\n\n            reader2 = readers.GaiaReader(\n                os.path.join(testfile_path, \'2states.geojson\'))\n            vectorData = reader2.read()\n\n            output = crop(rasterData, vectorData)\n\n            self.assertEqual(type(output.get_data()).__name__, \'Dataset\')\n        except Exception:\n            raise\n\n    def test_crop_rgb(self):\n        """"""\n        Test cropping raster data with RGB bands\n        """"""\n        input_path = os.path.join(testfile_path, \'simplergb.tif\')\n        input_raster = gaia.create(input_path)\n\n        # Generate crop geometry from raster bounds\n        bounds = input_raster.get_metadata().get(\'bounds\').get(\'coordinates\')\n        bounds = bounds[0]\n        x = (bounds[0][0] + bounds[2][0]) / 2.0\n        y = (bounds[0][1] + bounds[2][1]) / 2.0\n\n        dx = 0.12 * (bounds[2][0] - bounds[0][0])\n        dy = 0.16 * (bounds[2][1] - bounds[0][1])\n        poly = [[\n            [x, y], [x+dx, y+dy], [x-dx, y+dy], [x-dx, y-dy], [x+dx, y-dy]\n        ]]\n        geometry = geojson.Polygon(poly)\n        crop_geom = gaia.create(geometry)\n\n        cropped_raster = crop(input_raster, crop_geom)\n        self.assertIsNotNone(cropped_raster)\n\n    def test_crop_rgb_null(self):\n        """"""Test with geometry that does not intersect""""""\n        input_path = os.path.join(testfile_path, \'simplergb.tif\')\n        input_raster = gaia.create(input_path)\n\n        input_path = os.path.join(testfile_path, \'iraq_hospitals.geojson\')\n        input_vector = gaia.create(input_path)\n\n        cropped = crop(input_raster, input_vector)\n        self.assertIsNone(cropped)\n\n    def test_crop_rgb_bigger(self):\n        """"""Test with geometry partially outside raster bounds""""""\n        input_path = os.path.join(testfile_path, \'simplergb.tif\')\n        input_raster = gaia.create(input_path)\n\n        bounds = input_raster.get_metadata().get(\'bounds\').get(\'coordinates\')\n        bounds = bounds[0]\n        x = (bounds[0][0] + bounds[2][0]) / 2.0\n        y = (bounds[0][1] + bounds[2][1]) / 2.0\n\n        dx = 1.0 * (bounds[2][0] - bounds[0][0])\n        dy = 1.0 * (bounds[2][1] - bounds[0][1])\n        poly = [[\n            [x, y], [x, y-dy], [x+dx, y-dy], [x+dx, y]\n        ]]\n        geometry = geojson.Polygon(poly)\n        crop_geom = gaia.create(geometry)\n\n        cropped_raster = crop(input_raster, crop_geom)\n        self.assertIsNotNone(cropped_raster)\n'"
