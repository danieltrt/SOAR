file_path,api_count,code
scripts/step1.py,0,"b'""""""\nThis script reads the government report, extracts, cleans and saves\nall the text so it can be analyzed in the next scripts.\n""""""\n\nimport re\n\nimport PyPDF2\n\n\nCHARACTERS = {\n    ""\xc3\xa7"": ""\xc3\x81"",\n    ""\xe2\x81\x84"": ""\xc3\xa1"",\n    ""\xe2\x80\xa6"": ""\xc3\x89"",\n    ""\xe2\x80\x9d"": ""\xc3\xa9"",\n    ""\xc3\xaa"": ""\xc3\x8d"",\n    ""\xe2\x84\xa2"": \'\xc3\xad\',\n    ""\xc3\xae"": ""\xc3\x93"",\n    ""\xc5\xa0"": ""\xc3\xb3"",\n    ""\xc4\xb1"": ""\xc3\xb6"",\n    ""\xc3\xb2"": ""\xc3\x9a"",\n    ""\xc5\x93"": ""\xc3\xba"",\n    ""\xc5\x92"": ""\xc3\xb1"",\n    ""\xc3\x94"": ""\xe2\x80\x98"",\n    ""\xc3\x95"": ""\xe2\x80\x99"",\n    ""\xc2\xa5"": ""\xe2\x80\xa2 "",\n    ""\xc3\x91"": ""\xe2\x80\x94"",\n    ""\xc2\xa8"": ""\xc2\xae"",\n    ""\xc2\xab"": ""\xc2\xb4"",\n    ""\xc3\x92"": ""\xe2\x80\x9c""\n}\n\n\ndef extract_text():\n    """"""Read the PDF contents and extract the text that we need.""""""\n\n    reader = PyPDF2.PdfFileReader(""informe.pdf"")\n    full_text = """"\n\n    # The page numbers in the PDF are not the same as the reported\n    # number of pages, we use this variable to keep track of both.\n    pdf_page_number = 3\n\n    # We will only retrieve the first 3 sections of the government report\n    # which are between pages 14 and 326.\n    for i in range(14, 327):\n\n        # This block is used to remove the page number at the start of\n        # each page. The first if removes page numbers with one digit.\n        # The second if removes page numbers with 2 digits and the else\n        # statement removes page numbers with 3 digits.\n        if pdf_page_number <= 9:\n            page_text = reader.getPage(i).extractText().strip()[1:]\n        elif pdf_page_number >= 10 and pdf_page_number <= 99:\n            page_text = reader.getPage(i).extractText().strip()[2:]\n        else:\n            page_text = reader.getPage(i).extractText().strip()[3:]\n\n        full_text += page_text.replace(""\\n"", """")\n        pdf_page_number += 1\n\n    # There\'s a small issue when decoding the PDF file.\n    # We will manually fix all the weird characters\n    # with their correct equivalents.\n    for item, replacement in CHARACTERS.items():\n        full_text = full_text.replace(item, replacement)\n\n    # We remove all extra white spaces.\n    full_text = re.sub("" +"", "" "", full_text)\n\n    # Finally we save the cleaned text into a .txt file.\n    with open(""transcript_clean.txt"", ""w"", encoding=""utf-8"") as temp_file:\n        temp_file.write(full_text)\n\n\nif __name__ == ""__main__"":\n\n    extract_text()\n'"
scripts/step2.py,0,"b'""""""\nThis script extracts features from the transcript txt file and saves them to .csv files\nso they can be used in any toolkkit.\n""""""\n\nimport csv\nimport spacy\n\n\ndef main():\n    """"""Loads the model and processes it.\n    \n    The model used can be installed by running this command on your CMD/Terminal:\n\n    python -m spacy download es_core_news_md\n    \n    """"""\n\n    corpus = open(""transcript_clean.txt"", ""r"", encoding=""utf-8"").read()\n    nlp = spacy.load(""es_core_news_md"")\n\n    # Our corpus is bigger than the default limit, we will set\n    # a new limit equal to its length.\n    nlp.max_length = len(corpus)\n\n    doc = nlp(corpus)\n\n    get_tokens(doc)\n    get_entities(doc)\n    get_sentences(doc)\n\n\ndef get_tokens(doc):\n    """"""Get the tokens and save them to .csv\n\n    Parameters\n    ----------\n    doc : spacy.doc\n        A doc object.\n\n    """"""\n\n    data_list = [[""text"", ""text_lower"", ""lemma"", ""lemma_lower"",\n                  ""part_of_speech"", ""is_alphabet"", ""is_stopword""]]\n\n    for token in doc:\n        data_list.append([\n            token.text, token.lower_, token.lemma_, token.lemma_.lower(),\n            token.pos_, token.is_alpha, token.is_stop\n        ])\n\n    with open(""./tokens.csv"", ""w"", encoding=""utf-8"", newline="""") as tokens_file:\n        csv.writer(tokens_file).writerows(data_list)\n\n\ndef get_entities(doc):\n    """"""Get the entities and save them to .csv\n\n    Parameters\n    ----------\n    doc : spacy.doc\n        A doc object.\n\n    """"""\n\n    data_list = [[""text"", ""text_lower"", ""label""]]\n\n    for ent in doc.ents:\n        data_list.append([ent.text, ent.lower_, ent.label_])\n\n    with open(""./entities.csv"", ""w"", encoding=""utf-8"", newline="""") as entities_file:\n        csv.writer(entities_file).writerows(data_list)\n\n\ndef get_sentences(doc):\n    """"""Get the sentences, score and save them to .csv\n\n    You will require to download the dataset (zip) from the following url:\n\n    https://www.kaggle.com/rtatman/sentiment-lexicons-for-81-languages\n\n    Once downloaded you will require to extract 2 .txt files:\n\n    negative_words_es.txt\n    positive_words_es.txt\n\n    Parameters\n    ----------\n    doc : spacy.doc\n        A doc object.\n\n    """"""\n\n    # Load positive and negative words into lists.\n    with open(""positive_words_es.txt"", ""r"", encoding=""utf-8"") as temp_file:\n        positive_words = temp_file.read().splitlines()\n\n    with open(""negative_words_es.txt"", ""r"", encoding=""utf-8"") as temp_file:\n        negative_words = temp_file.read().splitlines()\n\n    data_list = [[""text"", ""score""]]\n\n    for sent in doc.sents:\n\n        # Only take into account real sentences.\n        if len(sent.text) > 10:\n\n            score = 0\n\n            # Start scoring the sentence.\n            for word in sent:\n\n                if word.lower_ in positive_words:\n                    score += 1\n\n                if word.lower_ in negative_words:\n                    score -= 1\n\n            data_list.append([sent.text, score])\n\n\n    with open(""./sentences.csv"", ""w"", encoding=""utf-8"", newline="""") as sentences_file:\n        csv.writer(sentences_file).writerows(data_list)\n\n\nif __name__ == ""__main__"":\n\n    main()\n'"
scripts/step3.py,2,"b'""""""\nThis script generates plots and insights that were used in the infographic.\nIt is advised to only run one function at a time.\n""""""\n\nimport geopandas\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nsns.set(style=""ticks"",\n        rc={\n            ""figure.figsize"": [12, 7],\n            ""text.color"": ""white"",\n            ""axes.labelcolor"": ""white"",\n            ""axes.edgecolor"": ""white"",\n            ""xtick.color"": ""white"",\n            ""ytick.color"": ""white"",\n            ""axes.facecolor"": ""#5C0E10"",\n            ""figure.facecolor"": ""#5C0E10""}\n        )\n\nACCENT_MARKS = [""\xc3\xa1"", ""\xc3\x81"", ""\xc3\xa9"", ""\xc3\x89"", ""\xc3\xad"", ""\xc3\x8d"", ""\xc3\xb3"", ""\xc3\x93"", ""\xc3\xba"", ""\xc3\x9a""]\nFRIENDLY_MARKS = [""a"", ""A"", ""e"", ""E"", ""i"", ""I"", ""o"", ""O"", ""u"", ""U""]\n\nSTATES = [\n    ""Aguascalientes"",\n    ""Baja California"",\n    ""Baja California Sur"",\n    ""Campeche"",\n    ""Chiapas"",\n    ""Chihuahua"",\n    ""Ciudad de M\xc3\xa9xico"",\n    ""Coahuila"",\n    ""Colima"",\n    ""Durango"",\n    ""Estado de M\xc3\xa9xico"",\n    ""Guanajuato"",\n    ""Guerrero"",\n    ""Hidalgo"",\n    ""Jalisco"",\n    ""Michoac\xc3\xa1n"",\n    ""Morelos"",\n    ""Nayarit"",\n    ""Nuevo Le\xc3\xb3n"",\n    ""Oaxaca"",\n    ""Puebla"",\n    ""Quer\xc3\xa9taro"",\n    ""Quintana Roo"",\n    ""San Luis Potos\xc3\xad"",\n    ""Sinaloa"",\n    ""Sonora"",\n    ""Tabasco"",\n    ""Tamaulipas"",\n    ""Tlaxcala"",\n    ""Veracruz"",\n    ""Yucat\xc3\xa1n"",\n    ""Zacatecas""\n]\n\n\ndef get_word_counts(df):\n    """"""Gets the total word count and the total unique lemmas.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The DataFrame to be analyzed.\n\n    """"""\n\n    # Small fix for programa and programar.\n    df.loc[df[\'lemma_lower\'] == ""programa"", ""lemma_lower""] = ""programar""\n\n    words = df[df[""is_alphabet""] == True][""text_lower""].count()\n    print(""Words:"", words)\n\n    unique_words = df[df[""is_alphabet""] == True][""lemma_lower""].nunique()\n    print(""Unique words:"", unique_words)\n\n\ndef plot_most_used_words(df):\n    """"""Generates a bar plot with the counts of the most used lemmas.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The DataFrame to be plotted.\n\n    """"""\n\n    # Small fix for programa and programar.\n    df.loc[df[""lemma_lower""] == ""programa"", ""lemma_lower""] = ""programar""\n\n    # Only take into account alphabet tokens that are longer than 1 character and are not stop words.\n    words = df[\n        (df[""is_alphabet""] == True) &\n        (df[""is_stopword""] == False) &\n        (df[""lemma_lower""].str.len() > 1)\n    ][""lemma_lower""].value_counts()[:20]\n\n    sns.barplot(x=words.values, y=words.index, palette=""Blues_d"", linewidth=0)\n    plt.xlabel(""Occurrences Count"")\n    plt.title(""Most Frequent Words"")\n    plt.savefig(""words_counts.png"", facecolor=""#5C0E10"")\n\n\ndef get_entity_counts(df):\n    """"""Gets the number of counts per entity.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The DataFrame to be analyzed.\n\n    """"""\n\n    entities = df[""label""].value_counts()\n    print(entities)\n\n    locations = df[df[""label""] == ""ORG""][""text""].value_counts()\n    print(locations)\n\n\ndef get_state_counts(df):\n    """"""Gets the number of counts per state.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The DataFrame to be analyzed.\n\n    """"""\n\n    total_count = 0\n    state_counts = list()\n\n    # We will get the count for each state.\n    for state in STATES:\n\n        state_count = len(df[df[""text_lower""] == state.lower()])\n\n        state_counts.append([state, state_count])\n        total_count += state_count\n\n    state_counts.sort(key=lambda x: x[1])\n\n    print(state_counts)\n    print(total_count)\n\n\ndef plot_map(df):\n    """"""Generates a map using the state counts. You will require to download\n    the following file and extract its contents to a folder named: mexicostates\n\n    https://www.arcgis.com/home/item.html?id=ac9041c51b5c49c683fbfec61dc03ba8\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The DataFrame to be plotted.\n\n    """"""\n\n    # First we read the shape file from its unzipped folder.\n    mexico_df = geopandas.read_file(""./mexicostates"")\n\n    for state in STATES:\n\n        # We remove accent marks and rename Ciudad de Mexico to its former name.\n        clean_name = clean_word(state)\n\n        if clean_name == ""Ciudad de Mexico"":\n            clean_name = ""Distrito Federal""\n        elif clean_name == ""Estado de Mexico"":\n            clean_name = ""Mexico""\n\n        # We insert the count value into the row with the matching ADMIN_NAME (state name).\n        mexico_df.loc[\n            mexico_df[""ADMIN_NAME""] == clean_name, ""count""\n        ] = len(df[df[""text_lower""] == state.lower()])\n\n    plt.rcParams[""figure.figsize""] = [12, 8]\n\n    mexico_df.plot(column=""count"", cmap=""plasma"", legend=True)\n    plt.title(""Mentions by State"")\n    plt.axis(""off"")\n    plt.tight_layout()\n    plt.savefig(""map.png"", facecolor=""#5C0E10"")\n\n\ndef clean_word(word):\n    """"""Cleans the word by replacing non-friendly characters.\n\n    Parameters\n    ----------\n    word : str\n        The word to be cleaned.\n\n    Returns\n    -------\n    str\n        The cleaned word.\n\n    """"""\n\n    for index, char in enumerate(ACCENT_MARKS):\n        word = word.replace(char, FRIENDLY_MARKS[index])\n\n    return word\n\n\ndef plot_sentiment_analysis(df):\n    """"""Generates a bar plot with the sentiment scores of each sentence.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The DataFrame to be plotted.\n\n    """"""\n\n    # Only take into account scores between -10 and 10.\n    df = df[(df[""score""] <= 10) & (df[""score""] >= -10)]\n\n    # We will make bars with a score below zero yellow and\n    # bars with a score above zero blue.\n    colors = np.array([(0.811, 0.913, 0.145)]*len(df[""score""]))\n    colors[df[""score""] >= 0] = (0.529, 0.870, 0.972)\n\n    yticks_labels = [str(i) for i in range(-12, 12, 2)]\n    plt.yticks(np.arange(-12, 12, 2), yticks_labels)\n\n    plt.bar(df.index, df[""score""], color=colors, linewidth=0)\n    plt.xlabel(""Sentence Number"")\n    plt.ylabel(""Score"")\n    plt.title(""Sentiment Analysis"")\n    plt.savefig(""sentiment_analysis.png"", facecolor=""#5C0E10"")\n\n\ndef plot_donut(df):\n    """"""Generates a donut plot with the counts of 3 categories.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The DataFrame to be plotted.\n\n    """"""\n\n    # We will only need 3 categories and 3 values.\n    labels = [""Positivo"", ""Negativo"", ""Neutro""]\n\n    positive = len(df[df[""score""] > 0])\n    negative = len(df[df[""score""] < 0])\n    neutral = len(df[df[""score""] == 0])\n\n    values = [positive, negative, neutral]\n    colors = [""green"", ""orange"", ""yellow""]\n    explode = (0, 0, 0)  # Explode a slice if required\n\n    plt.rcParams[""font.size""] = 18\n    plt.rcParams[""legend.fontsize""] = 20\n\n    plt.pie(values, explode=explode, labels=None,\n            colors=colors, autopct=\'%1.1f%%\', shadow=False)\n\n    # We draw a circle in the Pie chart to make it a donut chart.\n    centre_circle = plt.Circle(\n        (0, 0), 0.75, color=""#5C0E10"", fc=""#5C0E10"", linewidth=0)\n\n    fig = plt.gcf()\n    fig.gca().add_artist(centre_circle)\n\n    plt.axis(""equal"")\n    plt.legend(labels)\n    plt.savefig(""donut.png"",  facecolor=""#5C0E10"")\n\n\nif __name__ == ""__main__"":\n\n    tokens_df = pd.read_csv(""./data/tokens.csv"")\n    entities_df = pd.read_csv(""./data/entities.csv"")\n    sentences_df = pd.read_csv(""./data/sentences.csv"")\n\n    # get_word_counts(tokens_df)\n    # get_entity_counts(entities_df)\n    # get_state_counts(entities_df)\n\n    # plot_most_used_words(tokens_df)\n    # plot_map(entities_df)\n    # plot_sentiment_analysis(sentences_df)\n    # plot_donut(sentences_df)\n'"
