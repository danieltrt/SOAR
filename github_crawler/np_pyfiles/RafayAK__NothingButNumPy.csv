file_path,api_count,code
Understanding_and_Creating_Binary_Classification_NNs/Layers/ActivationLayer.py,2,"b'""""""\nCreated by : Rafay Khan\nTwitter: @RafayAK\n""""""\n\n\nimport numpy as np  # import numpy library\n\n\nclass SigmoidLayer:\n    """"""\n    This file implements activation layers\n    inline with a computational graph model\n\n    Args:\n        shape: shape of input to the layer\n\n    Methods:\n        forward(Z)\n        backward(upstream_grad)\n\n    """"""\n\n    def __init__(self, shape):\n        """"""\n        The consturctor of the sigmoid/logistic activation layer takes in the following arguments\n\n        Args:\n            shape: shape of input to the layer\n        """"""\n        self.A = np.zeros(shape)  # create space for the resultant activations\n\n    def forward(self, Z):\n        """"""\n        This function performs the forwards propagation step through the activation function\n\n        Args:\n            Z: input from previous (linear) layer\n        """"""\n        self.A = 1 / (1 + np.exp(-Z))  # compute activations\n\n    def backward(self, upstream_grad):\n        """"""\n        This function performs the  back propagation step through the activation function\n        Local gradient => derivative of sigmoid => A*(1-A)\n\n        Args:\n            upstream_grad: gradient coming into this layer from the layer above\n\n        """"""\n        # couple upstream gradient with local gradient, the result will be sent back to the Linear layer\n        self.dZ = upstream_grad * self.A*(1-self.A)\n'"
Understanding_and_Creating_Binary_Classification_NNs/Layers/LinearLayer.py,5,"b'""""""\nCreated by : Rafay Khan\nTwitter: @RafayAK\n""""""\n\n\nimport numpy as np  # import numpy library\nfrom util.paramInitializer import initialize_parameters  # import function to initialize weights and biases\n\n\nclass LinearLayer:\n    """"""\n        This Class implements all functions to be executed by a linear layer\n        in a computational graph\n\n        Args:\n            input_shape: input shape of Data/Activations\n            n_out: number of neurons in layer\n            ini_type: initialization type for weight parameters, default is ""plain""\n                      Opitons are: plain, xavier and he\n\n        Methods:\n            forward(A_prev)\n            backward(upstream_grad)\n            update_params(learning_rate)\n\n    """"""\n\n    def __init__(self, input_shape, n_out, ini_type=""plain""):\n        """"""\n        The constructor of the LinearLayer takes the following parameters\n\n        Args:\n            input_shape: input shape of Data/Activations\n            n_out: number of neurons in layer\n            ini_type: initialization type for weight parameters, default is ""plain""\n        """"""\n\n        self.m = input_shape[1]  # number of examples in training data\n        # `params` store weights and bias in a python dictionary\n        self.params = initialize_parameters(input_shape[0], n_out, ini_type)  # initialize weights and bias\n        self.Z = np.zeros((self.params[\'W\'].shape[0], input_shape[1]))  # create space for resultant Z output\n\n    def forward(self, A_prev):\n        """"""\n        This function performs the forwards propagation using activations from previous layer\n\n        Args:\n            A_prev:  Activations/Input Data coming into the layer from previous layer\n        """"""\n\n        self.A_prev = A_prev  # store the Activations/Training Data coming in\n        self.Z = np.dot(self.params[\'W\'], self.A_prev) + self.params[\'b\']  # compute the linear function\n\n    def backward(self, upstream_grad):\n        """"""\n        This function performs the back propagation using upstream gradients\n\n        Args:\n            upstream_grad: gradient coming in from the upper layer to couple with local gradient\n        """"""\n\n        # derivative of Cost w.r.t W\n        self.dW = np.dot(upstream_grad, self.A_prev.T)\n\n        # derivative of Cost w.r.t b, sum across rows\n        self.db = np.sum(upstream_grad, axis=1, keepdims=True)\n\n        # derivative of Cost w.r.t A_prev\n        self.dA_prev = np.dot(self.params[\'W\'].T, upstream_grad)\n\n    def update_params(self, learning_rate=0.1):\n        """"""\n        This function performs the gradient descent update\n\n        Args:\n            learning_rate: learning rate hyper-param for gradient descent, default 0.1\n        """"""\n\n        self.params[\'W\'] = self.params[\'W\'] - learning_rate * self.dW  # update weights\n        self.params[\'b\'] = self.params[\'b\'] - learning_rate * self.db  # update bias(es)\n'"
Understanding_and_Creating_Binary_Classification_NNs/util/cost_functions.py,10,"b'""""""\nCreated by : Rafay Khan\nTwitter: @RafayAK\n""""""\n\n\nimport numpy as np\n\n""""""\nContains a bunch of Cost functions.\nThis file implementations of :\n    - Binary Cross_entropy Cost function\n        * compute_binary_cost(Y, P_hat) -> ""unstable""\n        * compute_stable_bce_cost(Y, Z) -> ""stable"" \n        * computer_keras_like_bce_cost(Y, P_hat, from_logits=False) -> stable\n    - Mean Squared Error Cost function\n""""""\n\n\ndef compute_bce_cost(Y, P_hat):\n    """"""\n    This function computes Binary Cross-Entropy(bce) Cost and returns the Cost and its\n    derivative.\n    This function uses the following Binary Cross-Entropy Cost defined as:\n    => (1/m) * np.sum(-Y*np.log(P_hat) - (1-Y)*np.log(1-P_hat))\n\n    Args:\n        Y: labels of data\n        P_hat: Estimated output probabilities from the last layer, the output layer\n\n    Returns:\n        cost: The Binary Cross-Entropy Cost result\n        dP_hat: gradient of Cost w.r.t P_hat\n\n    """"""\n    m = Y.shape[1]  # m -> number of examples in the batch\n\n    cost = (1/m) * np.sum(-Y*np.log(P_hat) - (1-Y)*np.log(1-P_hat))\n    cost = np.squeeze(cost)  # remove extraneous dimensions to give just a scalar (e.g. this turns [[17]] into 17)\n\n    dP_hat = (1/m) * (-(Y/P_hat) + ((1-Y)/(1-P_hat)))\n\n    return cost, dP_hat\n\n\ndef compute_stable_bce_cost(Y, Z):\n    """"""\n    This function computes the ""Stable"" Binary Cross-Entropy(stable_bce) Cost and returns the Cost and its\n    derivative w.r.t Z_last(the last linear node) .\n    The Stable Binary Cross-Entropy Cost is defined as:\n    => (1/m) * np.sum(max(Z,0) - ZY + log(1+exp(-|Z|)))\n    Args:\n        Y: labels of data\n        Z: Values from the last linear node\n\n    Returns:\n        cost: The ""Stable"" Binary Cross-Entropy Cost result\n        dZ_last: gradient of Cost w.r.t Z_last\n    """"""\n    m = Y.shape[1]\n\n    cost = (1/m) * np.sum(np.maximum(Z, 0) - Z*Y + np.log(1+ np.exp(- np.abs(Z))))\n    dZ_last = (1/m) * ((1/(1+np.exp(- Z))) - Y)  # from Z computes the Sigmoid so P_hat - Y, where P_hat = sigma(Z)\n\n    return cost, dZ_last\n\n\ndef compute_keras_like_bce_cost(Y, P_hat, from_logits=False):\n    """"""\n    This function computes the Binary Cross-Entropy(stable_bce) Cost function the way Keras\n    implements it. Accepting either probabilities(P_hat) from the sigmoid neuron or values direct\n    from the linear node(Z)\n\n    Args:\n        Y: labels of data\n        P_hat: Probabilities from sigmoid function\n        from_logits: flag to check if logits are being provided or not(Default: False)\n\n    Returns:\n        cost: The ""Stable"" Binary Cross-Entropy Cost result\n        dZ_last: gradient of Cost w.r.t Z_last\n\n    """"""\n    if from_logits:\n        # assume that P_hat contains logits and not probabilities\n        return compute_stable_bce_cost(Y, Z=P_hat)\n\n    else:\n        # Assume P_hat contains probabilities. So make logits out of them\n\n        # First clip probabilities to stable range\n        EPSILON = 1e-07\n        P_MAX = 1- EPSILON  # 0.9999999\n\n        P_hat = np.clip(P_hat, a_min=EPSILON, a_max=P_MAX)\n\n        # Second, Convert stable probabilities to logits(Z)\n        Z = np.log(P_hat/(1-P_hat))\n\n        # now call compute_stable_bce_cost\n        return compute_stable_bce_cost(Y, Z)\n\n\n\ndef compute_mse_cost(Y, Y_hat):\n    """"""\n    This function computes Mean Squared Error(mse) Cost and returns the Cost and its derivative.\n    This function uses the Squared Error Cost defined as follows:\n    => (1/2m)*sum(Y - Y_hat)^.2\n\n    Args:\n        Y: labels of data\n        Y_hat: Predictions(activations) from a last layer, the output layer\n\n    Returns:\n        cost: The Squared Error Cost result\n        dY_hat: gradient of Cost w.r.t Y_hat\n\n    """"""\n    m = Y.shape[1]  # m -> number of examples in the batch\n\n    cost = (1 / (2 * m)) * np.sum(np.square(Y - Y_hat))\n    cost = np.squeeze(cost)  # remove extraneous dimensions to give just a scalar (e.g. this turns [[17]] into 17)\n\n    dY_hat = -1 / m * (Y - Y_hat)  # derivative of the squared error cost function\n\n    return cost, dY_hat\n'"
Understanding_and_Creating_Binary_Classification_NNs/util/paramInitializer.py,4,"b'import numpy as np\n\n\ndef initialize_parameters(n_in, n_out, ini_type=\'plain\'):\n    """"""\n    Helper function to initialize some form of random weights and Zero biases\n    Args:\n        n_in: size of input layer\n        n_out: size of output/number of neurons\n        ini_type: set initialization type for weights\n\n    Returns:\n        params: a dictionary containing W and b\n    """"""\n\n    params = dict()  # initialize empty dictionary of neural net parameters W and b\n\n    if ini_type == \'plain\':\n        params[\'W\'] = np.random.randn(n_out, n_in) *0.01  # set weights \'W\' to small random gaussian\n    elif ini_type == \'xavier\':\n        params[\'W\'] = np.random.randn(n_out, n_in) / (np.sqrt(n_in))  # set variance of W to 1/n\n    elif ini_type == \'he\':\n        # Good when ReLU used in hidden layers\n        # Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n        # Kaiming He et al. (https://arxiv.org/abs/1502.01852)\n        # http: // cs231n.github.io / neural - networks - 2 /  # init\n        params[\'W\'] = np.random.randn(n_out, n_in) * np.sqrt(2/n_in)  # set variance of W to 2/n\n\n    params[\'b\'] = np.zeros((n_out, 1))    # set bias \'b\' to zeros\n\n    return params\n\n\n'"
Understanding_and_Creating_Binary_Classification_NNs/util/utilities.py,31,"b'""""""\nCreated by : Rafay Khan\nTwitter: @RafayAK\n""""""\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt  # import matplotlib for plotting and visualization\nimport matplotlib\n\n# only required for inline labels in `plot_decision_boundary_distances` function\ntry:\n    from labellines import labelLine\nexcept ImportError:\n    print(""""""Caution! `matplotlib-label-lines` package is not available, \'plot_decision_boundary_distances`\n    will not work. Try installing the package through `pip install matplotlib-label-lines`"""""")\n\n\n""""""\nContains a bunch of helper functions\n""""""\n\n\ndef predict(X, Y, Zs, As, thresh=0.5):\n    """"""\n    helper function to predict on data using a neural net model layers\n\n    Args:\n        X: Data in shape (features x num_of_examples)\n        Y: labels in shape ( label x num_of_examples)\n        Zs: All linear layers in form of a list e.g [Z1,Z2,...,Zn]\n        As: All Activation layers in form of a list e.g [A1,A2,...,An]\n        thresh: is the classification threshold. All values >= threshold belong to positive class(1)\n                and the rest to the negative class(0).Default threshold value is 0.5\n    Returns::\n        p: predicted labels\n        probas : raw probabilities\n        accuracy: the number of correct predictions from total predictions\n    """"""\n    m = X.shape[1]\n    n = len(Zs)  # number of layers in the neural network\n    p = np.zeros((1, m))\n\n    # Forward propagation\n    Zs[0].forward(X)\n    As[0].forward(Zs[0].Z)\n    for i in range(1, n):\n        Zs[i].forward(As[i-1].A)\n        As[i].forward(Zs[i].Z)\n    probas = As[n-1].A\n\n    # convert probas to 0/1 predictions\n    for i in range(0, probas.shape[1]):\n        if probas[0, i] >= thresh:  # 0.5  the default threshold\n            p[0, i] = 1\n        else:\n            p[0, i] = 0\n\n    # print results\n    # print (""predictions: "" + str(p))\n    # print (""true labels: "" + str(y))\n    accuracy = np.sum((p == Y) / m)\n\n    return p, probas, accuracy*100\n\n\ndef plot_learning_curve(costs, learning_rate, total_epochs, save=False):\n    """"""\n    This function plots the Learning Curve of the model\n\n    Args:\n        costs: list of costs recorded during training\n        learning_rate: the learning rate during training\n        total_epochs: number of epochs the model was trained for\n        save: bool flag to save the image or not. Default False\n    """"""\n    # plot the cost\n    plt.figure()\n\n    steps = int(total_epochs / len(costs))  # the steps at with costs were recorded\n    plt.ylabel(\'Cost\')\n    plt.xlabel(\'Iterations \')\n    plt.title(""Learning rate ="" + str(learning_rate))\n    plt.plot(np.squeeze(costs))\n    locs, labels = plt.xticks()\n    plt.xticks(locs[1:-1], tuple(np.array(locs[1:-1], dtype=\'int\')*steps))  # change x labels of the plot\n    plt.xticks()\n    if save:\n        plt.savefig(\'Cost_Curve.png\', bbox_inches=\'tight\')\n    plt.show()\n\n\ndef predict_dec(Zs, As, X, thresh=0.5):\n    """"""\n    Used for plotting decision boundary.\n\n    Args:\n        Zs: All linear layers in form of a list e.g [Z1,Z2,...,Zn]\n        As: All Activation layers in form of a list e.g [A1,A2,...,An]\n        X: Data in shape (features x num_of_examples) i.e (K x m), where \'m\'=> number of examples\n           and ""K""=> number of features\n        thresh: is the classification threshold. All values >= threshold belong to positive class(1)\n                and the rest to the negative class(0).Default threshold value is 0.5\n    Returns:\n        predictions: vector of predictions of our model (red: 0 / green: 1)\n    """"""\n\n    # Predict using forward propagation and a classification threshold of 0.5\n    m = X.shape[1]\n    n = len(Zs)  # number of layers in the neural network\n\n    # Forward propagation\n    Zs[0].forward(X)\n    As[0].forward(Zs[0].Z)\n    for i in range(1, n):\n        Zs[i].forward(As[i - 1].A)\n        As[i].forward(Zs[i].Z)\n    probas = As[n - 1].A   # output probabilities\n\n    # if probability of example >= thresh => output 1, vice versa\n    predictions = (probas >= thresh)\n    return predictions\n\n\ndef plot_decision_boundary(model, X, Y, feat_crosses=None, axis_lines=False,save=False):\n    """"""\n    Plots decision boundary\n\n    Args:\n        model: neural network layer and activations in lambda function\n        X: Data in shape (num_of_examples x features)\n        feat_crosses: list of tuples showing which features to cross\n        axis_lines: Draw axis lines at x=0 and y=0(bool, default False)\n        save: flag to save plot image\n    """"""\n\n    # first plot the data to see what is the size of the plot\n    plt.scatter(X[:, 0], X[:, 1], s=200, c=np.squeeze(Y))  # s-> size of marker\n\n    # get the x and y range of the plot\n    x_ticks = plt.xticks()[0]\n    y_ticks = plt.yticks()[0]\n\n    plt.clf()  # clear figure after getting size\n\n    # Generate a grid of points between min_x_point-0.5 and max_x_point+0.5 with 1000 points in between,\n    # similarly, for y points\n    xs = np.linspace(min(x_ticks) - 0.5, max(x_ticks) + 0.5, 1000)\n    ys = np.linspace(max(y_ticks) + 0.5, min(y_ticks) - 0.5, 1000)\n\n    xx, yy = np.meshgrid(xs, ys)  # create data points\n    # Predict the function value for the whole grid\n\n    prediction_data = np.c_[xx.ravel(), yy.ravel()]\n    # add feat_crosses if provided\n    if feat_crosses:\n        for feature in feat_crosses:\n            prediction_data = np.c_[prediction_data, prediction_data[:, feature[0]]*prediction_data[:, feature[1]]]\n\n    Z = model(prediction_data)\n    Z = Z.reshape(xx.shape)\n\n    # Plot the contour and training examples\n    plt.style.use(\'seaborn-whitegrid\')\n    plt.contour(xx, yy, Z, cmap=\'Blues\')  # draw a blue colored decision boundary\n    plt.title(\'Decision Boundary\', size=18)\n    plt.xlabel(\'$x_1$\', size=20)\n    plt.ylabel(\'$x_2$\', size=20)\n    if axis_lines:\n        plt.axhline(0, color=\'black\')\n        plt.axvline(0, color=\'black\')\n\n    # color map \'cmap\' maps 0 labeled data points to red and 1 labeled points to green\n    cmap = matplotlib.colors.ListedColormap([""red"", ""green""], name=\'from_list\', N=None)\n    plt.scatter(X[:, 0], X[:, 1], s=200, c=np.squeeze(Y), marker=\'x\', cmap=cmap)  # s-> size of marker\n\n    if save:\n        plt.savefig(\'decision_boundary.png\', bbox_inches=\'tight\')\n\n    plt.show()\n\n\ndef plot_decision_boundary_shaded(model, X, Y, feat_crosses=None, axis_lines=False,save=False):\n    """"""\n        Plots shaded decision boundary\n\n        Args:\n            model: neural network layer and activations in lambda function\n            X: Data in shape (num_of_examples x features)\n            feat_crosses: list of tuples showing which features to cross\n            axis_lines: Draw axis lines at x=0 and y=0(bool, default False)\n            save: flag to save plot image\n    """"""\n\n    # first plot the data to see what is the size of the plot\n    plt.scatter(X[:, 0], X[:, 1], s=200, c=np.squeeze(Y))  # s-> size of marker\n\n    # get the x and y range of the plot\n    x_ticks = plt.xticks()[0]\n    y_ticks = plt.yticks()[0]\n\n    plt.clf()  # clear figure after getting size\n\n    # Generate a grid of points between min_x_point-0.5 and max_x_point+0.5 with 1000 points in between,\n    # similarly, for y points\n    xs = np.linspace(min(x_ticks)-0.5, max(x_ticks)+0.5, 1000)\n    ys = np.linspace(max(y_ticks)+0.5, min(y_ticks)-0.5, 1000)\n    xx, yy = np.meshgrid(xs, ys)\n\n    # Predict the function value for the whole grid\n\n    prediction_data = np.c_[xx.ravel(), yy.ravel()]\n    # add feat_crosses if provided\n    if feat_crosses:\n        for feature in feat_crosses:\n            prediction_data = np.c_[prediction_data, prediction_data[:, feature[0]] * prediction_data[:, feature[1]]]\n\n    Z = model(prediction_data)\n    Z = Z.reshape(xx.shape)\n\n    # Plot the contour and training examples\n    cmap = matplotlib.colors.ListedColormap([""red"",""green""], name=\'from_list\', N=None)\n    plt.style.use(\'seaborn-whitegrid\')\n\n    # \'contourf\'-> filled contours (red(\'#EABDBD\'): 0 / green(\'#C8EDD6\'): 1)\n    plt.contourf(xx, yy, Z, cmap=matplotlib.colors.ListedColormap([\'#EABDBD\', \'#C8EDD6\'], name=\'from_list\', N=None))\n    plt.title(\'Shaded Decision Boundary\', size=18)\n    plt.xlabel(\'$x_1$\', size=20)\n    plt.ylabel(\'$x_2$\', size=20)\n    if axis_lines:\n        plt.axhline(0, color=\'black\')\n        plt.axvline(0, color=\'black\')\n    plt.scatter(X[:, 0], X[:, 1], s=200, c=np.squeeze(Y), marker=\'x\', cmap=cmap)  # s-> size of marker\n\n    if save:\n        plt.savefig(\'decision_boundary_shaded.png\', bbox_inches=\'tight\')\n\n    plt.show()\n\n\ndef point_on_line(P1, P2, points):\n    """"""\n    Helper function for `plot_decision_boundary_distances`.\n    This function calculates the perpendicular point(closes point) on the decision boundary line from another point\n\n    Logic for finding intersection points:\n        -https://stackoverflow.com/questions/10301001/perpendicular-on-a-line-segment-from-a-given-point\n\n    Logic for finding distances:\n        -https://stackoverflow.com/questions/39840030/distance-between-point-and-a-line-from-two-points/39840218\n\n    Args:\n        P1: a point on the line\n        P2: another point on the line\n        points: list of points\n\n    Returns:\n        intersection_points, distances_to_intersection points\n\n    """"""\n    distances = np.abs(np.cross(P2 - P1, P1 - points) / np.linalg.norm(P2 - P1))\n\n    x = np.dot(points - P1, (P2 - P1).T) / np.dot(P2 - P1, (P2 - P1).T)\n    return P1 + x * (P2 - P1), distances\n\n\ndef plot_decision_boundary_distances(model, X, Y, feat_crosses=None, axis_lines=False, save=False):\n    """"""\n    Plots decision boundary\n\n    Args:\n        model: neural network layer and activations in lambda function\n        X: Data in shape (num_of_examples x features)\n        feat_crosses: list of tuples showing which features to cross\n        axis_lines: Draw axis lines at x=0 and y=0(bool, default False)\n        save: flag to save plot image\n    """"""\n\n    # first plot the data to see what is the size of the plot\n    plt.scatter(X[:, 0], X[:, 1], s=200, c=np.squeeze(Y))\n\n    # get the x and y range of the plot\n    x_ticks = plt.xticks()[0]\n    y_ticks = plt.yticks()[0]\n\n    plt.clf()  # clear figure after getting size\n\n    # Generate a grid of points between min_x_point-0.5 and max_x_point+0.5 with 1000 points in between,\n    # similarly, for y points\n    xs = np.linspace(min(x_ticks) - 0.5, max(x_ticks) + 0.5, 1000)\n    ys = np.linspace(max(y_ticks) + 0.5, min(y_ticks) - 0.5, 1000)\n    xx, yy = np.meshgrid(xs, ys)  # create data points\n    # Predict the function value for the whole grid\n\n    prediction_data = np.c_[xx.ravel(), yy.ravel()]\n    # add feat_crosses if provided\n    if feat_crosses:\n        for feature in feat_crosses:\n            prediction_data = np.c_[prediction_data, prediction_data[:, feature[0]] * prediction_data[:, feature[1]]]\n\n    Z = model(prediction_data)\n    Z = Z.reshape(xx.shape)\n\n    # Plot the contour and training examples\n    plt.style.use(\'seaborn-whitegrid\')\n    c = plt.contour(xx, yy, Z, cmap=\'Blues\')  # draw a blue colored decision boundary\n    plt.title(\'Distances from Decision Boundary\', size=18)\n    plt.xlabel(\'$x_1$\', size=20)\n    plt.ylabel(\'$x_2$\', size=20)\n    if axis_lines:\n        plt.axhline(0, color=\'black\')\n        plt.axvline(0, color=\'black\')\n\n    # color map \'cmap\' maps 0 labeled data points to red and 1 labeled points to green\n    cmap = matplotlib.colors.ListedColormap([""red"", ""green""], name=\'from_list\', N=None)\n    plt.scatter(X[:, 0], X[:, 1], s=200, c=np.squeeze(Y), marker=\'x\', cmap=cmap)  # s-> size of marker\n\n    points = X  # data points from to which perpendicular lines are drawn\n    v = c.collections[0].get_paths()[0].vertices  # returns two points from the decision line(visible start & end point)\n    P1 = np.expand_dims(np.asarray((v[0, 0], v[0, 1])), axis=0)  # the visible start point of the line\n    P2 = np.expand_dims(np.asarray((v[-1, 0], v[-1, 1])), axis=0)  # the visible end point of the line\n\n    inter_points, distances = point_on_line(P1, P2, points)\n\n    # combine the intersection points so that they\'re in the format required by `plt.plot` so\n    # each list item is:\n    # [(x_1,x_2), (y_1, y_2), len_of_line]\n    perpendicular_line_points = [list(zip(a, b))+[c] for a, b, c in zip(points, inter_points, distances)]\n\n    # plot and label perpendicular lines to the decision boundary one by one\n    # labelLine function comes from https://github.com/cphyc/matplotlib-label-lines/tree/master/labellines/baseline\n    for line in perpendicular_line_points:\n        x_points = np.clip(line[0], a_min=-0.5, a_max=1.5)  # clip lines going out of bounds of visible area\n        y_points = np.clip(line[1], a_min=-0.5, a_max=1.5)\n        len = line[2]\n        plt.plot(x_points, y_points, \'m--\', label=\'{:.2f}\'.format(len))  # print label to 2 decimal places\n        labelLine(plt.gca().get_lines()[-1], x= sum(x_points)/2)  # label of the line should be centered, so (x_1+x_2)/2\n\n    if save:\n        plt.savefig(\'decision_boundary_with_distances.png\', bbox_inches=\'tight\')\n\n    plt.tight_layout()\n    plt.show()\n'"
Understanding_and_Creating_NNs/Layers/ActivationLayer.py,2,"b'""""""\nCreated by : Rafay Khan\nTwitter: @RafayAK\n\nThis file implements activation layers\ninline with a computational graph model\n""""""\n\nimport numpy as np  # import numpy library\n\nclass SigmoidLayer:\n    """"""\n    This class implements the Sigmoid Layer\n\n    Args:\n        shape: shape of input to the layer\n\n    Methods:\n        forward(Z)\n        backward(upstream_grad)\n\n    """"""\n\n    def __init__(self, shape):\n        """"""\n        The consturctor of the sigmoid/logistic activation layer takes in the following arguments\n\n        Args:\n            shape: shape of input to the layer\n        """"""\n        self.A = np.zeros(shape)  # create space for the resultant activations\n\n    def forward(self, Z):\n        """"""\n        This function performs the forwards propagation step through the activation function\n\n        Args:\n            Z: input from previous (linear) layer\n        """"""\n        self.A = 1 / (1 + np.exp(-Z))  # compute activations\n\n    def backward(self, upstream_grad):\n        """"""\n        This function performs the  back propagation step through the activation function\n        Local gradient => derivative of sigmoid => A*(1-A)\n\n        Args:\n            upstream_grad: gradient coming into this layer from the layer above\n\n        """"""\n        # couple upstream gradient with local gradient, the result will be sent back to the Linear layer\n        self.dZ = upstream_grad * self.A*(1-self.A)\n'"
Understanding_and_Creating_NNs/Layers/LinearLayer.py,5,"b'""""""\nCreated by : Rafay Khan\nTwitter: @RafayAK\n""""""\n\nimport numpy as np  # import numpy library\nfrom util.paramInitializer import initialize_parameters  # import function to initialize weights and biases\n\n\nclass LinearLayer:\n    """"""\n        This Class implements all functions to be executed by a linear layer\n        in a computational graph\n\n        Args:\n            input_shape: input shape of Data/Activations\n            n_out: number of neurons in layer\n            ini_type: initialization type for weight parameters, default is ""plain""\n                      Opitons are: plain, xavier and he\n\n        Methods:\n            forward(A_prev)\n            backward(upstream_grad)\n            update_params(learning_rate)\n\n    """"""\n\n    def __init__(self, input_shape, n_out, ini_type=""plain""):\n        """"""\n        The constructor of the LinearLayer takes the following parameters\n\n        Args:\n            input_shape: input shape of Data/Activations\n            n_out: number of neurons in layer\n            ini_type: initialization type for weight parameters, default is ""plain""\n        """"""\n\n        self.m = input_shape[1]  # number of examples in training data\n        # `params` store weights and bias in a python dictionary\n        self.params = initialize_parameters(input_shape[0], n_out, ini_type)  # initialize weights and bias\n        self.Z = np.zeros((self.params[\'W\'].shape[0], input_shape[1]))  # create space for resultant Z output\n\n    def forward(self, A_prev):\n        """"""\n        This function performs the forwards propagation using activations from previous layer\n\n        Args:\n            A_prev:  Activations/Input Data coming into the layer from previous layer\n        """"""\n\n        self.A_prev = A_prev  # store the Activations/Training Data coming in\n        self.Z = np.dot(self.params[\'W\'], self.A_prev) + self.params[\'b\']  # compute the linear function\n\n    def backward(self, upstream_grad):\n        """"""\n        This function performs the back propagation using upstream gradients\n\n        Args:\n            upstream_grad: gradient coming in from the upper layer to couple with local gradient\n        """"""\n\n        # derivative of Cost w.r.t W\n        self.dW = np.dot(upstream_grad, self.A_prev.T)\n\n        # derivative of Cost w.r.t b, sum across rows\n        self.db = np.sum(upstream_grad, axis=1, keepdims=True)\n\n        # derivative of Cost w.r.t A_prev\n        self.dA_prev = np.dot(self.params[\'W\'].T, upstream_grad)\n\n    def update_params(self, learning_rate=0.1):\n        """"""\n        This function performs the gradient descent update\n\n        Args:\n            learning_rate: learning rate hyper-param for gradient descent, default 0.1\n        """"""\n\n        self.params[\'W\'] = self.params[\'W\'] - learning_rate * self.dW  # update weights\n        self.params[\'b\'] = self.params[\'b\'] - learning_rate * self.db  # update bias(es)\n'"
Understanding_and_Creating_NNs/util/paramInitializer.py,4,"b'""""""\nCreated by : Rafay Khan\nTwitter: @RafayAK\n""""""\n\nimport numpy as np\n\n\ndef initialize_parameters(n_in, n_out, ini_type=\'plain\'):\n    """"""\n    Helper function to initialize some form of random weights and Zero biases\n    Args:\n        n_in: size of input layer\n        n_out: size of output/number of neurons\n        ini_type: set initialization type for weights\n\n    Returns:\n        params: a dictionary containing W and b\n    """"""\n\n    params = dict()  # initialize empty dictionary of neural net parameters W and b\n\n    if ini_type == \'plain\':\n        params[\'W\'] = np.random.randn(n_out, n_in) *0.01  # set weights \'W\' to small random gaussian\n    elif ini_type == \'xavier\':\n        params[\'W\'] = np.random.randn(n_out, n_in) / (np.sqrt(n_in))  # set variance of W to 1/n\n    elif ini_type == \'he\':\n        # Good when ReLU used in hidden layers\n        # Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n        # Kaiming He et al. (https://arxiv.org/abs/1502.01852)\n        # http: // cs231n.github.io / neural - networks - 2 /  # init\n        params[\'W\'] = np.random.randn(n_out, n_in) * np.sqrt(2/n_in)  # set variance of W to 2/n\n\n    params[\'b\'] = np.zeros((n_out, 1))    # set bias \'b\' to zeros\n\n    return params\n\n\n'"
Understanding_and_Creating_NNs/util/utilities.py,20,"b'""""""\nCreated by : Rafay Khan\nTwitter: @RafayAK\n\nContains a bunch of helper functions\n""""""\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt  # import matplotlib for plotting and visualization\nimport matplotlib\n\n\ndef compute_cost(Y, Y_hat):\n    """"""\n    This function computes and returns the Cost and its derivative.\n    The is function uses the Squared Error Cost function -> (1/2m)*sum(Y - Y_hat)^.2\n\n    Args:\n        Y: labels of data\n        Y_hat: Predictions(activations) from a last layer, the output layer\n\n    Returns:\n        cost: The Squared Error Cost result\n        dY_hat: gradient of Cost w.r.t the Y_hat\n\n    """"""\n    m = Y.shape[1]\n\n    cost = (1 / (2 * m)) * np.sum(np.square(Y - Y_hat))\n    cost = np.squeeze(cost)  # remove extraneous dimensions to give just a scalar\n\n    dY_hat = -1 / m * (Y - Y_hat)  # derivative of the squared error cost function\n\n    return cost, dY_hat\n\n\ndef predict(X, Y, Zs, As):\n    """"""\n    helper function to predict on data using a neural net model layers\n\n    Args:\n        X: Data in shape (features x num_of_examples)\n        Y: labels in shape ( label x num_of_examples)\n        Zs: All linear layers in form of a list e.g [Z1,Z2,...,Zn]\n        As: All Activation layers in form of a list e.g [A1,A2,...,An]\n    Returns::\n        p: predicted labels\n        probas : raw probabilities\n        accuracy: the number of correct predictions from total predictions\n    """"""\n    m = X.shape[1]\n    n = len(Zs)  # number of layers in the neural network\n    p = np.zeros((1, m))\n\n    # Forward propagation\n    Zs[0].forward(X)\n    As[0].forward(Zs[0].Z)\n    for i in range(1, n):\n        Zs[i].forward(As[i-1].A)\n        As[i].forward(Zs[i].Z)\n    probas = As[n-1].A\n\n    # convert probas to 0/1 predictions\n    for i in range(0, probas.shape[1]):\n        if probas[0, i] > 0.5:  # 0.5 is threshold\n            p[0, i] = 1\n        else:\n            p[0, i] = 0\n\n    # print results\n    # print (""predictions: "" + str(p))\n    # print (""true labels: "" + str(y))\n    accuracy = np.sum((p == Y) / m)\n\n    return p, probas, accuracy*100\n\n\ndef plot_learning_curve(costs, learning_rate, total_epochs, save=False):\n    """"""\n    This function plots the Learning Curve of the model\n\n    Args:\n        costs: list of costs recorded during training\n        learning_rate: the learning rate during training\n        total_epochs: number of epochs the model was trained for\n        save: bool flag to save the image or not. Default False\n    """"""\n    # plot the cost\n    plt.figure()\n\n    steps = int(total_epochs / len(costs))  # the steps at with costs were recorded\n    plt.ylabel(\'Cost\')\n    plt.xlabel(\'Iterations \')\n    plt.title(""Learning rate ="" + str(learning_rate))\n    plt.plot(np.squeeze(costs))\n    locs, labels = plt.xticks()\n    plt.xticks(locs[1:-1], tuple(np.array(locs[1:-1], dtype=\'int\')*steps))  # change x labels of the plot\n    plt.xticks()\n    if save:\n        plt.savefig(\'Cost_Curve.png\', bbox_inches=\'tight\')\n    plt.show()\n\n\ndef predict_dec(Zs, As, X):\n    """"""\n    Used for plotting decision boundary.\n\n    Args:\n        Zs: All linear layers in form of a list e.g [Z1,Z2,...,Zn]\n        As: All Activation layers in form of a list e.g [A1,A2,...,An]\n        X: Data in shape (features x num_of_examples) i.e (K x m), where \'m\'=> number of examples\n           and ""K""=> number of features\n\n    Returns:\n        predictions: vector of predictions of our model (red: 0 / green: 1)\n    """"""\n\n    # Predict using forward propagation and a classification threshold of 0.5\n    m = X.shape[1]\n    n = len(Zs)  # number of layers in the neural network\n\n    # Forward propagation\n    Zs[0].forward(X)\n    As[0].forward(Zs[0].Z)\n    for i in range(1, n):\n        Zs[i].forward(As[i - 1].A)\n        As[i].forward(Zs[i].Z)\n    probas = As[n - 1].A   # output probabilities\n\n    predictions = (probas > 0.5)  # if probability of example > 0.5 => output 1, vice versa\n    return predictions\n\n\ndef plot_decision_boundary(model, X, Y, feat_crosses=None, save=False):\n    """"""\n    Plots decision boundary\n\n    Args:\n        model: neural network layer and activations in lambda function\n        X: Data in shape (num_of_examples x features)\n        feat_crosses: list of tuples showing which features to cross\n        save: flag to save plot image\n    """"""\n    # Generate a grid of points between -0.5 and 1.5 with 1000 points in between\n    xs = np.linspace(-0.5, 1.5, 1000)\n    ys = np.linspace(1.5, -0.5, 1000)\n    xx, yy = np.meshgrid(xs, ys) # create data points\n    # Predict the function value for the whole grid\n\n    # Z = model(np.c_[xx.ravel(), yy.ravel()])  # => add this for feature cross eg ""xx.ravel()*yy.ravel()""\n\n    prediction_data = np.c_[xx.ravel(), yy.ravel()]\n    # add feat_crosses if provided\n    if feat_crosses:\n        for feature in feat_crosses:\n            prediction_data = np.c_[prediction_data, prediction_data[:, feature[0]]*prediction_data[:, feature[1]]]\n\n    Z = model(prediction_data)\n    Z = Z.reshape(xx.shape)\n\n    # Plot the contour and training examples\n    plt.style.use(\'seaborn-whitegrid\')\n    plt.contour(xx, yy, Z, cmap=\'Blues\')  # draw a blue colored decision boundary\n    plt.title(\'Decision boundary\', size=18)\n    plt.xlabel(\'$x_1$\', size=20)\n    plt.ylabel(\'$x_2$\', size=20)\n    plt.axhline(0, color=\'black\')\n    plt.axvline(0, color=\'black\')\n\n    # color map \'cmap\' maps 0 labeled data points to red and 1 labeled points to green\n    cmap = matplotlib.colors.ListedColormap([""red"", ""green""], name=\'from_list\', N=None)\n    plt.scatter(X[:, 0], X[:, 1], s=200, c=np.squeeze(Y), marker=\'x\', cmap=cmap)  # s-> size of marker\n\n    if save:\n        plt.savefig(\'decision_boundary.png\', bbox_inches=\'tight\')\n\n    plt.show()\n\n\ndef plot_decision_boundary_shaded(model, X, Y, feat_crosses=None, save=False):\n    """"""\n        Plots shaded decision boundary\n\n        Args:\n            model: neural network layer and activations in lambda function\n            X: Data in shape (num_of_examples x features)\n            feat_crosses: list of tuples showing which features to cross\n            save: flag to save plot image\n    """"""\n\n    # Generate a grid of points between -0.5 and 1.5 with 1000 points in between\n    xs = np.linspace(-0.5, 1.5, 1000)\n    ys = np.linspace(1.5, -0.5, 1000)\n    xx, yy = np.meshgrid(xs, ys)\n    # Predict the function value for the whole grid\n    # Z = model(np.c_[xx.ravel(), yy.ravel()]) # => add this for feature cross eg ""xx.ravel()*yy.ravel()""\n\n    prediction_data = np.c_[xx.ravel(), yy.ravel()]\n    # add feat_crosses if provided\n    if feat_crosses:\n        for feature in feat_crosses:\n            prediction_data = np.c_[prediction_data, prediction_data[:, feature[0]] * prediction_data[:, feature[1]]]\n\n    Z = model(prediction_data)\n    Z = Z.reshape(xx.shape)\n\n    # Plot the contour and training examples\n    cmap = matplotlib.colors.ListedColormap([""red"",""green""], name=\'from_list\', N=None)\n    plt.style.use(\'seaborn-whitegrid\')\n\n    # \'contourf\'-> filled contours (red(\'#EABDBD\'): 0 / green(\'#C8EDD6\'): 1)\n    plt.contourf(xx, yy, Z, cmap=matplotlib.colors.ListedColormap([\'#EABDBD\', \'#C8EDD6\'], name=\'from_list\', N=None))\n    plt.title(\'Decision boundary\', size=18)\n    plt.xlabel(\'$x_1$\', size=20)\n    plt.ylabel(\'$x_2$\', size=20)\n    plt.axhline(0, color=\'black\')\n    plt.axvline(0, color=\'black\')\n    plt.scatter(X[:, 0], X[:, 1], s=200, c=np.squeeze(Y), marker=\'x\', cmap=cmap)  # s-> size of marker\n\n    if save:\n        plt.savefig(\'decision_boundary_shaded.png\', bbox_inches=\'tight\')\n\n    plt.show()\n\n'"
