file_path,api_count,code
setup.py,0,"b""import setuptools\n\nwith open('README.md', 'r') as f:\n    long_description = f.read()\n\nsetuptools.setup(\n    name='pykitml',\n    version='0.0.1',\n    author='RainingComputers',\n    author_email='vishnu.vish.shankar@gmail.com',\n    description='Machine Learning library written in Python and NumPy.',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    url='https://github.com/RainingComputers/pykitml',\n    packages=setuptools.find_packages(exclude=['docs', 'tests']),\n    python_requires='>=3.5',\n    install_requires=[\n        'numpy', 'matplotlib', 'tqdm', 'graphviz'\n    ],\n    classifiers=[\n        'Programming Language :: Python :: 3',\n        'License :: OSI Approved :: MIT License',\n        'Operating System :: OS Independent',\n        'Development Status :: 3 - Alpha',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence'\n    ],\n    keywords='pykitml'\n)"""
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'pykitml\'\ncopyright = \'2019, Vishnu Shankar B\'\nauthor = \'Vishnu Shankar B\'\n\n# The short X.Y version\nversion = \'0.0.1\'\n# The full version, including alpha/beta/rc tags\nrelease = \'0.0.1\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.napoleon\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n#html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'pykitmldoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'pykitml.tex\', \'pykitml Documentation\',\n     \'Vishnu Shankar B\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'pykitml\', \'pykitml Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'pykitml\', \'pykitml Documentation\',\n     author, \'pykitml\', \'Machine learninh library written in Python and NumPy.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n# -- Extension configuration -------------------------------------------------\n'"
pykitml/__init__.py,0,"b""'''\npykitml (Python Kit for Machine Learning),\n    pykitml Machine Learning library.\nCopyrights(c) Vishnu Shankar\nMIT License (See LICENSE file)\nhttps://github.com/RainingComputers\n'''\n\nfrom .network import *\nfrom .linear_regression import *\nfrom .logistic_regression import *\nfrom .svm import *\nfrom .naive_bayes import *\nfrom .decision_tree import *\nfrom .random_forest import *\nfrom .nearest_neighbor import *\nfrom .pca import *\nfrom .kmeans_clustering import *\nfrom .pklhandler import *\nfrom .normalize import *\nfrom .optimizers import *\nfrom .preprocessing import *\n\nfrom . import testing\n"""
pykitml/_classifier.py,12,"b'from abc import ABC, abstractmethod\nimport itertools\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tqdm\n\nfrom . import _heatmap\nfrom . import preprocessing\n\nclass Classifier(ABC):\n    \'\'\'\n    Mix-in class for classifier models.\n    \'\'\'\n\n    @abstractmethod\n    def get_output(self):\n        \'\'\'\n        Returns the output activations of the model.\n\n        Returns\n        -------\n        numpy.array\n            The output activations.\n        \'\'\'\n        pass\n\n    @abstractmethod\n    def feed(self, input_data):\n        \'\'\'\n        Accepts input array and feeds it to the model.\n\n        Parameters\n        ----------\n        input_data : numpy.array\n            The input to feed the model.\n\n        Raises\n        ------\n        ValueError\n            If the input data has invalid dimensions/shape.      \n\n        Note\n        ----\n        This function only feeds the input data, to get the output after calling this\n        function use :py:func:`get_output` or :py:func:`get_output_onehot`\n        \'\'\'\n        pass\n\n    @property\n    @abstractmethod\n    def _out_size(self):\n        \'\'\'\n        Returns number of nodes/neurons in the output layer.\n        \'\'\'\n        pass\n\n    def get_output_onehot(self):\n        \'\'\'\n        Returns the output layer activations of the model as a one-hot array. A one-hot array\n        is an array of bits in which only `one` of the bits is high/true. In this case, the\n        corresponding bit to the neuron/node having the highest activation will be high/true.\n        \n        Returns\n        -------\n        numpy.array\n            The one-hot output activations array. \n        \'\'\'\n        output_targets = self.get_output()\n\n        if(self._out_size == 1):\n            # For binary classification\n            return np.where(output_targets>0.5, 1, 0)\n        elif(output_targets.ndim == 1):\n            # If output is a vector/1D array, axis=1 will not work\n            index = np.argmax(output_targets)\n            output_onehot = np.zeros((self._out_size))\n            output_onehot[index] = 1\n        else:\n            # Create a onehot array from outputs\n            output_onehot = np.zeros(output_targets.shape)\n            output_onehot[np.arange(output_targets.shape[0]), np.argmax(output_targets, axis=1)] = 1\n\n        # Return one hot array\n        return output_onehot\n\n    def accuracy(self, testing_data, testing_targets):\n        \'\'\'\n        Tests the accuracy of the model on the testing data passed to the\n        function. This function should be only used for classification.\n\n        Parameters\n        ----------\n        testing_data : numpy.array\n            numpy array containing testing data.\n        testing_targets : numpy.array\n            numpy array containing testing targets, corresponding to the testing data.\n        \n        Returns\n        -------\n        accuracy : float\n           The accuracy of the model over the testing data i.e how many testing examples\n           did the model predict correctly.\n        \'\'\'\n        # Evalute over all the testing data and get outputs\n        self.feed(testing_data)\n\n        # Create a onehot array from outputs\n        output_onehot = self.get_output_onehot()\n\n        # Calculate how many examples it classified correctly\n        if(self._out_size == 1):\n            no_correct = (testing_targets == output_onehot).sum()\n        else:    \n            no_correct = (testing_targets == output_onehot).all(axis=1).sum()\n        \n        # Calculate accuracy\n        accuracy = (no_correct/testing_data.shape[0]) * 100\n\n        # return accuracy\n        return round(accuracy, 2) \n\n    def confusion_matrix(self, test_data, test_targets, gnames=[], plot=True,):   \n        \'\'\'\n        Returns and plots confusion matrix on the given test data.\n\n        Parameters\n        ----------\n        test_data : numpy.array\n            Numpy array containing test data\n        test_targets : numpy.array\n            Numpy array containing the targets corresponding to the test data.\n        plot : bool\n            If set to false, will not plot the matrix. Default is true.\n        gnames : list\n            List of string names for each class/group.\n\n        Returns\n        -------\n        confusion_matrix : numpy.array\n            The confusion matrix. \n        \'\'\'\n        print(\'Creating Confusion Matrix...\')\n\n        # feed the data\n        self.feed(test_data)\n\n        # Get output\n        outputs = self.get_output_onehot()\n\n        # Binary classification\n        if(self._out_size == 1):\n            # Number of groups\n            ngroups = 2\n            # Column/Row labels\n            labels = [\'False\', \'True\']\n            # Split outputs to two groups\n            outputs = preprocessing.onehot(outputs)\n            targets = preprocessing.onehot(test_targets)\n            # Prevent bugs that show up when outputs are all zeros\n            if(outputs.shape[1] == 1):\n                outputs = np.pad(outputs, ((0, 0), (0 ,1)), \'constant\', constant_values=0)\n\n        # Multiclass classification\n        else:\n            # Number of groups\n            ngroups = self._out_size\n            # Column/Row labels\n            if(len(gnames) != 0):\n                labels = gnames\n            else:\n                labels = [str(x) for x in range(0, ngroups)]\n            # Targets\n            targets = test_targets\n        \n        # Confusion matrix\n        conf_mat = np.zeros((ngroups, ngroups))\n\n        # Loop through every possibility and count them\n        pairs = list(itertools.product(range(0, ngroups), repeat=2))\n        for predicted, actual in tqdm.tqdm(pairs, ncols=80, unit=\'pairs\'):\n            # Make one hot vector for predicted\n            pred_vec = np.zeros((ngroups))\n            pred_vec[predicted] = 1\n\n            # Make one hot vector for actual\n            act_vec = np.zeros((ngroups))\n            act_vec[actual] = 1\n\n            # Count\n            out_count = np.all(outputs == pred_vec, axis=1)\n            target_count  = np.all(targets == act_vec, axis=1)\n            tot_count = np.logical_and(out_count, target_count).sum()\n            conf_mat[predicted][actual] = tot_count\n\n        # Plot the confusion matrix\n        if(plot):\n            # Plot confusion matrix\n            fig, ax = plt.subplots(figsize=(10, 7))\n            im, _ = _heatmap.heatmap(conf_mat, labels, labels, ax=ax,\n                            cmap=""YlGn"", cbarlabel=""Count"")\n            _heatmap.annotate_heatmap(im, valfmt=""{x:.0f}"")\n\n            # Labels\n            fig.canvas.set_window_title(\'Confusion Matrix\') \n            ax.set_xlabel(\'Actual\')\n            ax.xaxis.set_label_position(\'top\') \n            ax.set_ylabel(\'Predicted\')\n\n            # Show\n            plt.show()\n\n        # return\n        return conf_mat'"
pykitml/_exceptions.py,0,"b""class InvalidFeatureType(Exception):\n    '''\n    Raised when specified feature type is invalid for the model.\n    '''\n    pass\n\nclass InvalidDistributionType(Exception):\n    '''\n    Raised when specified distribution type is invalid for the model.\n    '''\n    pass\n\ndef _valid_list(input_list, valid_items):\n    '''\n    Used to check if items in a list are valid.\n\n    Parameters\n    ----------\n    input_list : list\n        The list to check/validate.\n    valid_items : list\n        List of valid items the list can contain.\n    '''\n    if (all(item in valid_items for item in input_list) and len(input_list)>0):\n        return True\n    else:\n        return False\n        """
pykitml/_functions.py,19,"b""import numpy as np\n\n'''\nThis module contains utility functions\n'''\n\n# =====================\n# = Utility functions =\n# =====================\n\ndef pdist(x, y):\n    '''\n    Calculate pairwise square distances between matrix x and y.\n    See: https://stackoverflow.com/a/56084419/5516481\n    '''\n    if(x.ndim==1): x = np.array([x])\n\n    nx, p = x.shape\n    x_ext = np.empty((nx, 3*p))\n    x_ext[:, :p] = 1\n    x_ext[:, p:2*p] = x\n    x_ext[:, 2*p:] = np.square(x)\n\n    ny = y.shape[0]\n    y_ext = np.empty((3*p, ny))\n    y_ext[:p] = np.square(y).T\n    y_ext[p:2*p] = -2*y.T\n    y_ext[2*p:] = 1\n\n    return x_ext.dot(y_ext)\n\n# ==============================================\n# = Activation functions and their derivatives =\n# ==============================================\n\ndef sigmoid(weighted_sum):\n    '''\n    Returns sigmoid of the weighted sum array of a layer.\n    '''\n    return 1 / (1 + np.exp(-weighted_sum))\n\ndef sigmoid_prime(weighted_sum, activations):\n    '''\n    Returns the derivative of sigmoid w.r.t layer's weighted sum.\n    '''\n    return activations * (1 - activations)\n\ndef tanh(weighted_sum):\n    '''\n    Returns tanh of the weighted sum array of a layer.\n    '''\n    return np.tanh(weighted_sum)\n\ndef tanh_prime(weighted_sum, activations):\n    '''\n    Returns the derivative of tanh w.r.t layer's weighted sum.\n    '''\n    return 1 - (activations ** 2)\n\ndef leakyrelu(weighted_sum):\n    '''\n    Returns leaky-ReLU of the weighted sum array of a layer.\n    '''\n    return np.where(weighted_sum > 0, weighted_sum, 0.01 * weighted_sum)\n\ndef leakyrelu_prime(weighted_sum, activations):\n    '''\n    Returns the derivative of leaky-ReLU w.r.t layer's weighted sum.\n    '''\n    return np.where(weighted_sum > 0, 1, 0.01)\n\ndef relu(weighted_sum):\n    '''\n    Returns ReLU of the weighted sum array of a layer.\n    '''\n    return np.where(weighted_sum > 0, weighted_sum, 0)\n\ndef relu_prime(weighted_sum, activations):\n    '''\n    Returns the derivative of ReLU w.r.t layer's weighted sum.\n    '''\n    return np.where(weighted_sum > 0, 1, 0)\n\ndef softmax(weighted_sum):\n    '''\n    Returns softmax of the weighted sum array of a layer.\n    If weighted_sum is a 2D array, then it performs softmax over each row.\n    '''\n    if(weighted_sum.ndim == 1):\n        exps = np.exp(weighted_sum - np.max(weighted_sum))\n        return exps / np.sum(exps)\n    else:\n        normalized = weighted_sum - np.expand_dims(np.max(weighted_sum, axis=1), axis=1)\n        exps = np.exp(normalized)\n        return exps / np.expand_dims(np.sum(exps, axis=1), 1)\n\ndef identity(weighted_sum):\n    '''\n    Returns identity of the weighted sum array of a layer.\n    '''\n    return weighted_sum\n\ndef identity_prime(weighted_sum, activations):\n    '''\n    Returns the derivative of identity w.r.t layer's weighted sum.\n    '''\n    return 1\n\ndef softmax_prime(weighted_sum, activations):\n    '''\n    Returns the derivative of softmax w.r.t layer's weighted sum.\n    '''\n    return activations * (1 - activations)\n\n# ========================================\n# = Cost functions and their derivatives =\n# ========================================\n\ndef mse(output, target):\n    '''\n    Returns mean squared error cost of the output.\n    '''\n    return 0.5 * ((output - target) ** 2)\n\ndef mse_prime(output, target):\n    '''\n    Returns the derivative of the mse cost.\n    '''\n    return (output-target)\n\ndef cross_entropy(output, target):\n    '''\n    Returns cross entropy cost of the output.\n    '''\n    return -(target * np.log(output)) - ((1-target) * np.log(1-output))\n\ndef cross_entropy_prime(output, target):\n    '''\n    Returns the derivative of the cross entropy cost.\n    '''\n    return (output-target) / (output * (1-output))\n\ndef hinge_loss(output, target):\n    '''\n    Returns hinge loss of the output for SVMs.\n    '''\n    return np.maximum(0, 1 - target*output)\n\ndef hinge_loss_prime(output, target):\n    '''\n    Returns derivative of hinge loss.\n    '''\n    return np.where((target*output)>1, 0, -1*target)\n    """
pykitml/_heatmap.py,5,"b""import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n'''\nThis module contains helper functions to draw heatmaps.\nREF: https://matplotlib.org/3.1.1/gallery/images_contours_and_fields/image_annotated_heatmap.html\n'''\n\ndef heatmap(data, row_labels, col_labels, ax=None, cbar_kw={}, cbarlabel='', **kwargs):\n    '''\n    Create a heatmap from a numpy array and two lists of labels.\n\n    Parameters\n    ----------\n    data\n        A 2D numpy array of shape (N, M).\n    row_labels\n        A list or array of length N with the labels for the rows.\n    col_labels\n        A list or array of length M with the labels for the columns.\n    ax\n        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n        not provided, use current axes or create a new one.  Optional.\n    cbar_kw\n        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n    cbarlabel\n        The label for the colorbar.  Optional.\n    **kwargs\n        All other arguments are forwarded to `imshow`.\n    '''\n\n    if not ax:\n        ax = plt.gca()\n\n    # Plot the heatmap\n    im = ax.imshow(data, **kwargs)\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va='bottom')\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(data.shape[1]))\n    ax.set_yticks(np.arange(data.shape[0]))\n    # ... and label them with the respective list entries.\n    ax.set_xticklabels(col_labels)\n    ax.set_yticklabels(row_labels)\n\n    # Let the horizontal axes labeling appear on top.\n    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=-30, ha='right', rotation_mode='anchor')\n\n    # Turn spines off and create white grid.\n    for _, spine in ax.spines.items(): spine.set_visible(False)\n\n    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n    ax.grid(which='minor', color='w', linestyle='-', linewidth=3)\n    ax.tick_params(which='minor', bottom=False, left=False)\n\n    return im, cbar\n\n\ndef annotate_heatmap(im, data=None, valfmt='{x:.2f}', textcolors=['black', 'white'],\n                threshold=None, **textkw):\n    '''\n    A function to annotate a heatmap.\n\n    Parameters\n    ----------\n    im\n        The AxesImage to be labeled.\n    data\n        Data used to annotate.  If None, the image's data is used.  Optional.\n    valfmt\n        The format of the annotations inside the heatmap.  This should either\n        use the string format method, e.g. '$ {x:.2f}', or be a\n        `matplotlib.ticker.Formatter`.  Optional.\n    textcolors\n        A list or array of two color specifications.  The first is used for\n        values below a threshold, the second for those above.  Optional.\n    threshold\n        Value in data units according to which the colors from textcolors are\n        applied.  If None (the default) uses the middle of the colormap as\n        separation.  Optional.\n    **kwargs\n        All other arguments are forwarded to each call to `text` used to create\n        the text labels.\n    '''\n\n    if not isinstance(data, (list, np.ndarray)):\n        data = im.get_array()\n\n    # Normalize the threshold to the images color range.\n    if threshold is not None:\n        threshold = im.norm(threshold)\n    else:\n        threshold = im.norm(data.max())/2.\n\n    # Set default alignment to center, but allow it to be\n    # overwritten by textkw.\n    kw = dict(horizontalalignment='center', verticalalignment='center')\n    kw.update(textkw)\n\n    # Get the formatter in case a string is supplied\n    if isinstance(valfmt, str):\n        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n\n    # Loop over the data and create a `Text` for each 'pixel'.\n    # Change the text's color depending on the data.\n    texts = []\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n            texts.append(text)\n\n    return texts"""
pykitml/_minimize_model.py,2,"b""from abc import ABC, abstractmethod\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tqdm\n\nclass MinimizeModel(ABC):\n    '''\n    Abstract base class for all models that use gradients and minimize a cost function.\n    '''\n\n    def train(self, training_data, targets, batch_size, epochs, optimizer,\n            testing_data=None, testing_targets=None, testing_freq=1, decay_freq=1):\n        '''\n        Trains the model on the training data, after training is complete, you can call\n        :py:func:`plot_performance` to plot performance graphs.\n\n        Parameters\n        ----------\n        training_data : numpy.array\n            numpy array containing training data.\n        targets : numpy.array\n            numpy array containing training targets, corresponding to the training data.\n        batch_size : int\n            Number of training examples to use in one epoch, or\n            number of training examples to use to estimate the gradient.\n        epochs : int\n            Number of epochs the model should be trained for.\n        optimizer : any Optimizer object\n            See :ref:`optimizers`\n        testing_data : numpy.array\n            numpy array containing testing data.\n        testing_targets : numpy.array\n            numpy array containing testing targets, corresponding to the testing data.\n        testing_freq : int\n            How frequently the model should be tested, i.e the model will be tested\n            after every :code:`testing_freq` epochs. You may want to increase this to reduce \n            training time.\n        decay_freq : int\n            How frequently the model should decay the learning rate. The learning rate\n            will decay after every :code:`decay_freq` epochs.\n\n        Raises\n        ------\n        ValueError\n            If :code:`training_data`, :code:`targets`, :code:`testing_data` or \n            :code:`testing_targets` has invalid dimensions/shape. \n        '''\n        print('Training Model...')\n\n        # Dictionary for holding performance log\n        self._performance_log = {}\n        self._performance_log['epoch'] = []\n        self._performance_log['cost_train'] = []\n        self._performance_log['learning_rate'] = []\n        if(testing_data is not None):\n            self._performance_log['cost_test'] = []   \n\n        # Loop through each epoch\n        pbar = tqdm.trange(0, epochs, ncols=80, unit='epochs')\n        for epoch in pbar:\n            total_gradient = self._get_batch_grad(epoch, batch_size, training_data, targets)\n\n            # After completing a batch, average the total sum of gradients and tweak the parameters\n            self._mparams = optimizer._optimize(self._mparams, total_gradient/batch_size)\n\n            # Decay the learning rate\n            if((epoch+1) % decay_freq == 0): optimizer._decay()\n\n            # Log and print performance\n            if((epoch+1)%testing_freq == 0):\n                # log epoch\n                self._performance_log['epoch'].append(epoch+1)\n                \n                # log learning rate\n                learning_rate = optimizer._learning_rate\n                self._performance_log['learning_rate'].append(learning_rate)\n                \n                # get cost of the model on training data\n                cost_train = self.cost(training_data, targets)\n                pbar.set_postfix(cost=cost_train)\n                self._performance_log['cost_train'].append(cost_train)\n                \n                # get cost of the model on testing data if it is provided\n                if(testing_data is None): continue\n                cost_test = self.cost(testing_data, testing_targets)\n                self._performance_log['cost_test'].append(cost_test)\n\n        # Close progress bar\n        pbar.close()\n\n    def _get_batch_grad(self, epoch, batch_size, training_data, targets):\n        '''\n        Calculates the sum of all the gradients fot the given batch\n\n        Parameters\n        ----------\n        epoch : int\n            Current epoch in the training process.\n        batch_size : int\n            Size of the batch.\n        training_data : numpy.array\n            numpy array containing training data.\n        targets : numpy.array\n            numpy array containing training targets, corresponding to the training data.\n        '''\n        # Total gradient for the batch \n        total_gradient = 0\n        \n        # feed the batch\n        start_index = (epoch*batch_size)%training_data.shape[0]\n        end_index = start_index+batch_size\n        indices = np.arange(start_index, end_index) % training_data.shape[0]\n        self.feed(training_data[indices])\n\n        # Loop through the batch\n        for example in range(0, batch_size):\n            # Add the calculated gradients to the total\n            index = ((epoch*batch_size) + example)%training_data.shape[0]\n            # Get gradient\n            total_gradient += self._backpropagate(example, targets[index])\n        \n        # return the total\n        return total_gradient\n\n    def plot_performance(self):\n        '''\n        Plots logged performance data after training. \n        Should be called after :py:func:`train`.\n\n        Raises\n        ------\n        AttributeError\n            If the model has not been trained, i.e :py:func:`train` has\n            not been called before.\n        IndexError\n            If :py:func:`train` failed.\n        '''\n        graph = self._performance_log\n\n        # Window title\n        plt.figure('Performance graph', figsize=(10, 7))\n\n        # First subplot\n        plt.subplot(2, 1, 1)\n\n        # Plot average cost vs epochs on training data\n        plt.plot(graph['epoch'], graph['cost_train'], label='Training data')\n        \n        # Plot average cost on testing data\n        if('cost_test' in graph.keys()):\n            plt.plot(graph['epoch'], graph['cost_test'], label='Test data')\n\n        # Axis labels\n        plt.ylabel('Average cost')\n        plt.xlabel('No. of epochs')\n        plt.legend()\n\n        # Second subplot\n        plt.subplot(2, 1, 2)\n\n        # Plot learning rate vs epochs\n        plt.plot(graph['epoch'], graph['learning_rate'], label='Learning Rate')\n\n        # Axis labels\n        plt.ylabel('Learning Rate')\n        plt.xlabel('No. of epochs')\n        plt.legend()        \n\n        # Show the plot\n        plt.show()\n\n    def cost(self, testing_data, testing_targets):\n        '''\n        Tests the average cost of the model on the testing data passed to the\n        function.\n\n        Parameters\n        ----------\n        testing_data : numpy.array\n            numpy array containing testing data.\n        testing_targets : numpy.array\n            numpy array containing testing targets, corresponding to the testing data.\n        \n        Returns\n        -------\n        cost : float\n            The average cost of the model over the testing data.\n\n        Raises\n        ------\n        ValueError\n            If :code:`testing_data` or :code:`testing_tagets` has invalid dimensions/shape.      \n        '''\n        # Evalute over all the testing data and get outputs\n        self.feed(testing_data)\n        output_targets = self.get_output()\n\n        # Calculate cost\n        cost = np.sum(self._cost_function(output_targets, testing_targets))\n        # Add regularization\n        cost += self._get_norm_weights()\n        \n        # Average the cost\n        cost = cost/testing_data.shape[0]\n\n        # return cost\n        return round(cost, 2)\n\n    @property\n    @abstractmethod\n    def _mparams(self):\n        '''\n        @property method to get model parameters\n        '''\n        pass\n    \n    @_mparams.setter\n    @abstractmethod\n    def _mparams(self, mparams):\n        '''\n        @params.setter method to set model parameters\n        '''\n        pass\n\n    @abstractmethod\n    def feed(self, input_data):\n        '''\n        Accepts input array and feeds it to the model.\n\n        Parameters\n        ----------\n        input_data : numpy.array\n            The input to feed the model.\n\n        Raises\n        ------\n        ValueError\n            If the input data has invalid dimensions/shape.      \n\n        Note\n        ----\n        This function only feeds the input data, to get the output after calling this\n        function use :py:func:`get_output` or :py:func:`get_output_onehot`\n        '''\n        pass\n\n    @abstractmethod\n    def get_output(self):\n        '''\n        Returns the output activations of the model.\n\n        Returns\n        -------\n        numpy.array\n            The output activations.\n        '''\n        pass\n\n    @abstractmethod\n    def _backpropagate(self, index, targets):\n        '''\n        This function calculates gradient of the cost function w.r.t all weights and \n        biases of the model by backpropagating the error through the model.\n\n        Parameters\n        ----------\n        index : int\n            Index of the example in the batch that was fed using feed.\n        target : numpy.array\n            The correct activations that the output layer should have.\n\n        Returns\n        -------\n        gradient: numpy.array\n            Numpy object array containing gradients, has same shape as self._mparams\n\n        Raises\n        ------\n        ValueError\n            If the input data has invalid dimensions/shape.\n\n        Note\n        ----\n        You have to call :py:func:`~feed` before you call this function.\n        ''' \n        pass\n\n    @property\n    @abstractmethod\n    def _cost_function(self):\n        '''\n        Return the cost function used in the model.\n        '''\n        pass\n\n    @abstractmethod\n    def _get_norm_weights(self):\n        '''\n        Return the norm of all the regularized parameters of the models\n        multiplied by the regularization parameter.\n        '''\n        pass\n\n    """
pykitml/_shared_array.py,0,"b""import numpy, ctypes, multiprocessing\n\n'''\nThis module contains helper functions to share\nnumpy arrays between python multiprocessing processes.\n\nSee: https://stackoverflow.com/a/5034106/5516481\n'''\n\n_ctypes_to_numpy = {\n    ctypes.c_char   : numpy.dtype(numpy.uint8),\n    ctypes.c_wchar  : numpy.dtype(numpy.int16),\n    ctypes.c_byte   : numpy.dtype(numpy.int8),\n    ctypes.c_ubyte  : numpy.dtype(numpy.uint8),\n    ctypes.c_short  : numpy.dtype(numpy.int16),\n    ctypes.c_ushort : numpy.dtype(numpy.uint16),\n    ctypes.c_int    : numpy.dtype(numpy.int32),\n    ctypes.c_uint   : numpy.dtype(numpy.uint32),\n    ctypes.c_long   : numpy.dtype(numpy.int64),\n    ctypes.c_ulong  : numpy.dtype(numpy.uint64),\n    ctypes.c_float  : numpy.dtype(numpy.float32),\n    ctypes.c_double : numpy.dtype(numpy.float64)\n}\n\n_numpy_to_ctypes = dict(zip(_ctypes_to_numpy.values(), _ctypes_to_numpy.keys()))\n\n\ndef shm_as_ndarray(mp_array, shape = None):\n    '''\n    Given a multiprocessing.Array, returns an ndarray pointing to\n    the same data.\n    '''\n\n    # support SynchronizedArray:\n    if not hasattr(mp_array, '_type_'):\n        mp_array = mp_array.get_obj()\n\n    dtype = _ctypes_to_numpy[mp_array._type_]\n    result = numpy.frombuffer(mp_array, dtype)\n\n    if(shape is not None):\n        result = result.reshape(shape)\n\n    return numpy.asarray(result)\n\n\ndef ndarray_to_shm(array, lock = False):\n    '''\n    Generate an 1D multiprocessing.Array containing the data from\n    the passed ndarray. The data will be *copied* into shared\n    memory.\n    '''\n\n    array1d = array.ravel(order='A')\n\n    try:\n        c_type = _numpy_to_ctypes[array1d.dtype]\n    except KeyError:\n        c_type = _numpy_to_ctypes[numpy.dtype(array1d.dtype)]\n\n    result = multiprocessing.Array(c_type, array1d.size, lock=lock)\n    shm_as_ndarray(result)[:] = array1d\n    return result"""
pykitml/_single_layer_model.py,9,"b""from abc import ABC, abstractmethod\n\nimport numpy as np\n\nfrom ._minimize_model import MinimizeModel\n\nclass SingleLayerModel(MinimizeModel, ABC):\n    '''\n    General base class for single layer models.\n    '''\n\n    def __init__(self, input_size, output_size, reg_param=0):\n        '''\n        Parameters\n        ----------\n        input_size : int\n            Size of input data or number of input features.\n        output_size: int\n            Number of categories or groups.\n        reg_param : int\n            Regularization parameter for the model, also known as 'weight decay'.\n        '''        \n        # Save sizes\n        self._input_size = input_size\n        self._output_size = output_size\n\n        # Initialize regularization parameter\n        self._reg_param = reg_param\n        self._reg_param_half = reg_param/2\n    \n        # Initialize weights and parameters\n        epsilon = np.sqrt(6)/(np.sqrt(output_size) + np.sqrt(input_size))\n        weights = np.random.rand(output_size, input_size)*2*epsilon - epsilon\n        biases = np.random.rand(output_size) * 2 * epsilon - epsilon\n        \n        # Numpy array to store activations\n        self._input_activations = np.array([])\n        self._activations = np.array([])\n        self._weighted_sum = np.array([])\n\n        # Put parameters in numpy dtype=object array\n        W = 0 # Weights\n        B = 1 # Biases\n        self._params = np.array([None, None], dtype=object)\n        self._params[W] = weights\n        self._params[B] = biases\n\n    @property\n    def _mparams(self):\n        return self._params\n    \n    @_mparams.setter\n    def _mparams(self, mparams):\n        self._params = mparams\n\n    @property\n    def _cost_function(self):\n        return self._cost_func\n\n    @property\n    def _out_size(self):\n        return self._output_size\n\n    def feed(self, input_data):\n        # Constants\n        W = 0 # Weights\n        B = 1 # Biases\n\n        # feed\n        self._input_activations = input_data\n        self._weighted_sum = (input_data @ self._params[W].T) + self._params[B]\n        self._activations = self._activ_func(self._weighted_sum)\n\n    def get_output(self):\n        return self._activations.squeeze()\n\n    def _backpropagate(self, index, target):\n        # Constants\n        W = 0 # Weights\n        B = 1 # Biases\n\n        # Gradients\n        da_dz = self._activ_func_prime(self._weighted_sum[index], self._activations[index])\n        dc_db = self._cost_func_prime(self._activations[index], target) * da_dz\n        dc_dw = np.multiply.outer(dc_db, self._input_activations[index])\n        \n        # Add regularization\n        dc_dw += self._reg_param*self._params[W]\n        \n        # Return gradient\n        gradient = np.array([None, None], dtype=object)\n        gradient[W] = dc_dw\n        gradient[B] = dc_db\n        return gradient\n\n    def _get_norm_weights(self):\n        W = 0\n        return self._reg_param_half*(self._params[W]**2).sum()\n\n    @property\n    @abstractmethod\n    def _activ_func(self):\n        pass\n\n    @property\n    @abstractmethod\n    def _activ_func_prime(self):\n        pass\n\n    @property\n    @abstractmethod\n    def _cost_func(self):\n        pass\n\n    @property\n    @abstractmethod\n    def _cost_func_prime(self):\n        pass"""
pykitml/decision_tree.py,12,"b""from itertools import combinations\n\nimport numpy as np\nfrom graphviz import Digraph \nimport tqdm\n\nfrom ._classifier import Classifier\nfrom ._exceptions import _valid_list, InvalidFeatureType\n\n\ndef condition(column, split, ftype):\n    '''\n    Returns boolean array for splitting the dataset.\n\n    Parameters\n    ----------\n    column : numpy.array\n        The column of the dataset.\n    split : int or tuple.\n        The point or categories to split the dataset with.\n    ftype : string\n        The type of feature, :code:`'continues'`, :code:`'ranked'`,\n        or :code:`'categorical'`. \n    '''\n    if(ftype == 'ranked' or ftype == 'continues'):\n        return column < split\n    elif(ftype == 'categorical'):\n        cond = np.full(column.shape, True, dtype=bool)\n        for category in split:\n            cond = np.logical_and(cond, (column==category))\n        return cond\n\n\nclass _Node:\n    def __init__(self, split, col, gini_index, nindex, feature_type):\n        '''\n        The node class.\n\n        Parameters\n        ----------\n        split : int or tuple\n            The point or categories to split the dataset with.\n        col : int\n            The features col to apply the split on.\n        gini_index : float\n            The gini index for this node.\n        nindex : int\n            ID for this node.\n        feature_type : string\n            The type of feature, :code:`'continues'`, :code:'`'ranked'`,\n            or :code:`'categorical'` \n        '''\n        # Condition that will split the data\n        self._split = split\n        self._col = col\n        self._ftype = feature_type\n        self._gini = round(gini_index, 4)\n        self._index = nindex\n\n        # Reference to two nodes\n        self.right_node = None\n        self.left_node = None\n\n    @property\n    def leaf(self):\n        return False\n\n    def decision(self, input_data):\n        '''\n        Splits the dataset and passes it to subnodes. The inputs\n        travel till the reach a leaf node and backtrack as outputs.\n        '''\n        # Make sure input data is 2d\n        if(input_data.ndim == 1): input_data = np.array([input_data])\n\n        # Condition\n        cond = condition(input_data[:, self._col], self._split, self._ftype)\n        \n        # Split data, travel to sub nodes and get output\n        left_output = self.left_node.decision(input_data[cond]) \n        right_output = self.right_node.decision(input_data[~cond])\n        \n        # Return output\n        outputs = np.zeros((input_data.shape[0], left_output.shape[1]))\n        outputs[cond] = left_output\n        outputs[~cond] = right_output\n        return outputs\n\n    def __str__(self):\n        if(self._ftype == 'ranked' or self._ftype == 'continues'):\n            if_str = 'Feature-'+str(self._col)+' < '+str(self._split)\n        elif(self._ftype == 'categorical'):\n            cat_str = str(self._split).replace(',', ' or')\n            if_str = 'Feature-'+str(self._col)+' = '+cat_str\n        \n        return if_str+'\\nNode-'+str(self._index)\n\n\nclass _Leaf:\n    def __init__(self, term_val, gini_index, nindex):\n        '''\n        Leaf node or terminal node.\n\n        Parameters\n        ----------\n        term_val : numpy.array\n            The output/probabilities for this node.\n        gini_index : float\n            Gini index for this node.\n        nindex : int\n            ID for this node.\n        '''\n        self._term_val = np.round(term_val, 2)\n        self._samples = None\n        self._gini = round(gini_index, 4)\n        self._index = nindex\n\n    @property\n    def leaf(self):\n        return True\n\n    def decision(self, input_data):\n        '''\n        Return outputs/probabilities. Starts the backtracking\n        process.\n        '''\n        self._samples = input_data.shape[0]\n        return np.tile(self._term_val, (input_data.shape[0], 1))\n\n    def __str__(self):\n        return  str(self._term_val)+'\\nNode - '+str(self._index)\n\n\nclass DecisionTree(Classifier):\n    '''\n    Implements Decision Tree model.\n    '''\n\n    def __init__(self, input_size, output_size, feature_type=[], max_depth=6):        \n        '''\n        Parameters\n        ----------\n        input_size : int\n            Size of input data or number of input features.\n        output_size : int\n            Number of categories or groups.\n        feature_type : list\n            List of string describing the type of feature for\n            each column. Can be :code:`'continues'`, \n            :code:`'ranked'`, or :code:`'categorical'`.\n        max_depth : int\n            The maximum depth the tree can grow to. Prevents from \n            overfitting (somewhat).\n\n        Raises\n        ------\n        InvalidFeatureType\n            Invalid/Unknown feature type. Can only be :code:`'continues'`, \n            :code:`'ranked'`, or :code:`'categorical'`.\n        '''\n\n        # Save values\n        self._input_size = input_size\n        self._output_size = output_size\n        self._ftype = feature_type\n        self._max_depth = max_depth\n        self._node_count = 0\n\n        # Check if given feature types are valid\n        valid_ftypes = ['continues', 'ranked', 'categorical']\n        if not _valid_list(feature_type, valid_ftypes):\n            raise InvalidFeatureType\n\n        # The columns on which the tree will train on\n        # This variable can be overridden in a child class to ignore\n        # certain columns of the input data while training\n        self._cols_train = list(range(input_size))\n\n        # CAn be overridden in child class to suppress progressbar while training\n        self._pbardis = False\n\n        # Tree nodes\n        self._root_node = None\n\n        # Outputs\n        self._output = None\n\n    @property\n    def _out_size(self):\n        return self._output_size\n\n    def train(self, inputs, outputs):\n        '''\n        Trains the model on the training data.\n\n        Parameters\n        ----------\n        training_data : numpy.array\n            numpy array containing training data.\n        targets : numpy.array\n            numpy array containing training targets, corresponding to the training data.\n\n        Raises\n        ------\n        numpy.AxisError\n            If output_size is less than two. Use :py:func:`pykitml.onehot` to change\n            0/False to [1, 0] and 1/True to [0, 1] for binary classification.\n        '''\n        print('Training Model...')\n        # Convert outputs from onehot to values\n        outputs = np.argmax(outputs, axis=1)\n        # Grow the tree\n        pbar = tqdm.tqdm(total=inputs.shape[0], ncols=80, unit='expls', disable=self._pbardis)\n        prob, gini = self._gini_index(outputs)\n        self._root_node = self._recursive_grow(pbar, inputs, outputs, prob, gini, -1)\n        # Close progress bar\n        pbar.close()\n\n    def feed(self, input_data):\n        self._output = self._root_node.decision(input_data)\n\n    def get_output(self):\n        return self._output.squeeze()\n\n    @staticmethod\n    def _get_splits(column, ftype):\n        '''\n        Given input column of a dataset and the type of feature,\n        generates all possible points/categories to split the dataset on.\n\n        Parameters\n        ----------\n        column : int\n            The column of the dataset.\n        ftype : str\n            The type of feature. Can be :code:`continues`, :code:`continues`, \n            :code:'`ranked`, or :code:`categorical`.,\n        '''\n        if(ftype == 'ranked' or ftype == 'continues'):\n            # All possible values to split the dataset\n            return np.unique(column)\n        elif(ftype == 'categorical'):\n            # All the possible 'or' combinations as a list of tuples\n            categories = np.unique(column).tolist()\n            combs = list(combinations(categories, len(categories)-1))\n            categories = [tuple([x]) for x in categories]\n            return combs+categories\n\n    def _recursive_grow(self, pbar, inputs, outputs, prob, gini, col, depth=0):\n        '''\n        This method recursively generates nodes of the tree.\n        '''\n        # Get the columns to iterate\n        cols_train = [i for i in self._cols_train if i != col]\n\n        # Keep track of least gini index\n        min_gini_index = 10\n        min_split = None\n        min_col = None\n        min_gi_col = None\n        min_inputs_left = None\n        min_inputs_right = None\n        min_outputs_left = None\n        min_outputs_right = None\n        min_p_left = None\n        min_p_right = None\n        min_gi_left = None\n        min_gi_right = None\n\n        # Generate data splits, get best split condition\n        for col in cols_train:\n            # Loop through each possible split\n            splits = self._get_splits(inputs[:, col], self._ftype[col])\n            for split in splits:\n                # Generate split condition\n                cond = condition(inputs[:, col], split, self._ftype[col])\n                \n                # Split the data based on condition\n                inputs_left, outputs_left = inputs[cond, :], outputs[cond]\n                inputs_right, outputs_right = inputs[~cond, :], outputs[~cond]\n\n                # Continue if no split\n                if(inputs_left.shape[0]==0 or inputs_right.shape[0]==0):\n                    continue\n\n                # Get gini index for this column and split\n                weight_left = inputs_left.shape[0]/inputs.shape[0]\n                weight_right = inputs_right.shape[0]/inputs.shape[0]\n                p_left, gi_left = self._gini_index(outputs_left)\n                p_right, gi_right = self._gini_index(outputs_right)\n                gi_col = weight_left*gi_left + weight_right*gi_right\n\n                # Track minimun value\n                if(gi_col < min_gini_index):\n                    min_gini_index = gi_col\n                    min_split = split\n                    min_col = col\n                    min_gi_col = gi_col\n                    min_inputs_left = inputs_left\n                    min_inputs_right = inputs_right\n                    min_outputs_left = outputs_left\n                    min_outputs_right = outputs_right\n                    min_p_left = p_left\n                    min_p_right = p_right\n                    min_gi_left = gi_left\n                    min_gi_right = gi_right\n\n        # Create node\n        self._node_count+=1\n        pbar.set_postfix(nodes=self._node_count)\n\n        # If best split is not better than current node's gini index, stop\n        # Create terminal node or leaf\n        # If maxdepth has been exceeded, create leaf and return\n        if(gini <= min_gini_index or depth == self._max_depth): \n            pbar.update(inputs.shape[0])\n            return _Leaf(prob, gini, self._node_count)\n\n        # Create node, split data further\n        node = _Node(min_split, min_col, min_gi_col, self._node_count, self._ftype[min_col])\n        node.left_node = self._recursive_grow(pbar, min_inputs_left, min_outputs_left, \n            min_p_left, min_gi_left, min_col, depth+1)\n        node.right_node = self._recursive_grow(pbar, min_inputs_right, min_outputs_right, \n            min_p_right, min_gi_right, min_col, depth+1)\n\n        # return node\n        return node\n\n    def _gini_index(self, outputs):\n        '''\n        Given the outputs of a split dataset, calculates gini index,\n        i.e. how 'pure' the dataset is. 0 for most pure and 1 for least\n        pure.\n        '''\n        # Get their probabilities P(Ci)\n        p_ci = np.bincount(outputs)/outputs.shape[0]\n        # Pad p_ci to out_size with zeros\n        pad_length = self._out_size-np.max(outputs)-1\n        p_ci = np.pad(p_ci, (0, pad_length), 'constant', constant_values=0)\n        # gini index gi = 1-sum(P(Ci)**2)\n        return p_ci, 1-(p_ci**2).sum()\n\n    def show_tree(self):\n        '''\n        Draws a visualization/graph of the tree.\n        '''\n        print('Drawing Tree...')\n\n        def walk(pbar, g, node):\n            # Exit condition\n            if(node.leaf): return\n            # Draw left node\n            g.edge(str(node), str(node.left_node), label='True')\n            # Draw right node\n            g.edge(str(node), str(node.right_node), label='False')\n            # Update progress bar\n            pbar.update(2)\n            # Draw subnodes\n            walk(pbar, g, node.left_node)\n            walk(pbar, g, node.right_node)\n\n        pbar = tqdm.tqdm(total=self._node_count, ncols=80, unit='nodes')\n        pbar.update()\n        g = Digraph('Tree', format='png')\n        walk(pbar, g, self._root_node)\n\n        g.view()\n"""
pykitml/kmeans_clustering.py,9,"b""import numpy as np\nimport tqdm\nimport matplotlib.pyplot as plt\n\nfrom . import _functions\nfrom . import normalize\n\ndef kmeans(training_data, nclusters, max_iter=1000, trials=50):\n    '''\n    Identifies cluster centres on training data using k-means.\n\n    Parameters\n    ----------\n    training_data : numpy.array\n        Numpy array containing training data.\n    nclusters : int\n        Number of cluster to find.\n    max_iter : int\n        Maximum number of iterations to run per trial.\n    trials : int\n        Number of times k-means should run, each with different\n        random initialization.\n\n    Returns\n    -------\n    clusters : numpy.array\n        Numpy array containing cluster centres.\n    cost : numpy.array\n        The cost of converged cluster centres.\n\n    '''\n    \n    # Keep track of trial with the least cost\n    min_cost = np.float('infinity')\n    distances = None\n    clusters_min_cost = None\n    clusters = None\n\n    # Keep log of maximum number of iterations for convergence\n    max_iter_log = 0\n    \n    pbar = tqdm.trange(0, trials, ncols=80, unit='trials')\n    for trial in pbar:\n        # Use kmeans++ to initialize cluster centres\n        clusters = np.zeros((nclusters, training_data.shape[1]))\n        \n        # First cluster centre is random\n        index = np.random.randint(training_data.shape[0], size=1)\n        clusters[0] = training_data[index]\n        \n        # Loop for rest of cluster centres\n        for i in range(1, nclusters):\n            # Calculate distance between every data point and previous cluster centre\n            prev_cluster_dists = _functions.pdist(clusters[i-1], training_data).squeeze()\n            # Normalize distances\n            prev_cluster_dists = prev_cluster_dists/prev_cluster_dists.sum()\n            \n            # Sample index with probability distribution proportional to distances\n            index = np.random.choice(training_data.shape[0], 1, p=prev_cluster_dists)\n            \n            # Assign next cluster centre\n            clusters[i] = training_data[index]\n        \n        # Start kmeans, Keep looping and moving the cluster points to mean\n        for iteration in range(max_iter):\n            new_clusters = np.zeros((nclusters, training_data.shape[1]))\n\n            # Calculate distances between clusters and every point in training data \n            distances = _functions.pdist(training_data, clusters)\n\n            # Assign clusters index to each data point\n            cluster_assignments = np.argmin(distances, axis=1)\n\n            # Move cluster by taking mean of all the points assigned to that cluster\n            for i in range(nclusters):\n                cluster_points = training_data[cluster_assignments==i]\n                if(cluster_points.shape[0] == 0): continue\n                new_clusters[i] = np.mean(cluster_points, axis=0)\n            \n            # Check for convergence\n            if(np.abs(new_clusters-clusters)==0).all(): break\n\n            # Assign new clusters\n            clusters = new_clusters\n\n        # Select cluster centres with least cost\n        cost = np.mean(np.min(distances, axis=1))\n        if(cost < min_cost):\n            clusters_min_cost = clusters\n            min_cost = cost\n\n        # Update maximum iterations for convergence\n        if(iteration > max_iter_log):\n            max_iter_log = iteration\n\n        # Update progress bar\n        pbar.set_postfix(cost=min_cost, max_it=max_iter_log)\n\n    return clusters_min_cost, min_cost\n"""
pykitml/linear_regression.py,0,"b""import numpy as np\n\nfrom ._single_layer_model import SingleLayerModel\nfrom . import _functions\n\nclass LinearRegression(SingleLayerModel):\n    '''\n    Implements linear regression.\n    '''\n\n    @property\n    def _activ_func(self):\n        return _functions.identity\n\n    @property\n    def _activ_func_prime(self):\n        return _functions.identity_prime\n\n    @property\n    def _cost_func(self):\n        return _functions.mse\n\n    @property\n    def _cost_func_prime(self):\n        return _functions.mse_prime"""
pykitml/logistic_regression.py,0,"b""import numpy as np\n\nfrom ._single_layer_model import SingleLayerModel\nfrom ._classifier import Classifier\nfrom . import _functions\n\nclass LogisticRegression(SingleLayerModel, Classifier):\n    '''\n    Implements logistic regression for classification.\n    '''\n    def __init__(self, input_size, output_size, reg_param=0):\n        # Initialize base class\n        super(LogisticRegression, self).__init__(input_size, output_size, reg_param)\n\n        # Choose output activation function\n        if(output_size == 1):\n            # For binary classification\n            self._afunc = _functions.sigmoid\n            self._afunc_prime = _functions.sigmoid_prime\n        else:\n            # For multiclass classification\n            self._afunc = _functions.softmax\n            self._afunc_prime = _functions.softmax_prime\n\n    @property\n    def _activ_func(self):\n        return self._afunc\n\n    @property\n    def _activ_func_prime(self):\n        return self._afunc_prime\n\n    @property\n    def _cost_func(self):\n        return _functions.cross_entropy\n\n    @property\n    def _cost_func_prime(self):\n        return _functions.cross_entropy_prime"""
pykitml/naive_bayes.py,25,"b""from copy import deepcopy\n\nimport numpy as np\nimport tqdm\n\nfrom ._classifier import Classifier\nfrom ._exceptions import _valid_list, InvalidDistributionType\n\ndef _gaussian(x, mean, std_dev):\n    sqrt_2pi = np.sqrt(2*np.pi)\n    return (1/(std_dev*sqrt_2pi))*np.exp(-0.5*(((x-mean)/std_dev)**2))\n\nclass NaiveBayes(Classifier):\n    '''\n    Implements Naive Bayes classifier.\n\n    Note\n    ----\n    Consider using :class:`.GaussianNaiveBayes` if all of your\n    features are continuous.\n    '''\n\n    def __init__(self, input_size, output_size, distributions, reg_param=1):\n        '''\n        Parameters\n        ----------\n        input_size : int\n            Size of input data or number of input features.\n        output_size: int\n            Number of categories or groups.\n        distribution : list\n            List of strings describing the distribution to use \n            for each feature. Option are :code:`'gaussian'`, \n            :code:`'binomial'`, :code:`'multinomial'`.\n        reg_param : int\n            If a given class and feature value never occur together in the training data,\n            then the frequency-based probability estimate will be zero.\n            This is problematic because it will wipe out all information in the other \n            probabilities when they are multiplied.\n            So, the probability will become :code:`log(reg_param)`.\n            This is a way to regularize Naive Bayes classifier.\n            See https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes\n\n        Raises\n        ------\n        InvalidDistributionType\n            If invalid distribution. Can only be :code:`'gaussian'`, \n            :code:`'binomial'`, :code:`'multinomial'`.\n        IndexError\n            If the input_size does not match the length of distribution length.\n        '''\n        # Save info\n        self._input_size = input_size\n        self._output_size = output_size\n        self._reg_param = reg_param\n\n        self._dists = distributions\n\n        # Check if given distributions are valid\n        valid_dists = ['gaussian', 'binomial', 'multinomial']\n        if not _valid_list(distributions, valid_dists):\n            raise InvalidDistributionType\n\n        # indices of categorical features\n        self._categorical = [i for i, x in enumerate(distributions) if x!='gaussian']\n\n        # for each class/group, p(class), std_dev, mean, freq and range\n        self._pclass = np.zeros((output_size))\n        self._mean = np.zeros((output_size, input_size))\n        self._std_dev = np.zeros((output_size, input_size))\n        self._freqp = [deepcopy([None]*input_size) for x in range(output_size)]\n        self._max = None\n\n        # Output\n        self._output = None\n\n    @property\n    def _out_size(self):\n        return self._output_size\n\n    def feed(self, input_data):\n        # Make sure array is 2D\n        if(input_data.ndim == 1): input_data = np.array([input_data])\n\n        # Set output to correct size\n        self._output = np.zeros((input_data.shape[0], self._output_size))\n        \n        # Loop through each group and find probability\n        for C in range(0, self._output_size):\n            \n            # Loop through each feature and multiply prod(p(xi|Ci))=p(x|Ci)\n            # We are doing sum(log(p(xi|Ci))) instead of prod(p(xi|Ci)) to prevent\n            # the product from going to zero due to very small numbers.\n            # So the result will be log(p(x|Ci)) instead of p(x|Ci)\n            p_xci = np.zeros((input_data.shape[0]))\n            for x in range(0, self._input_size):\n                # p(xi|Ci) if catgorical feature\n                if(self._dists[x] != 'gaussian'):\n                    p_xici = self._freqp[C][x][input_data[:, x].astype(int)]\n                # p(xi|Ci) if continues feature\n                else:    \n                    p_xici = _gaussian(input_data[:, x], self._mean[C][x], self._std_dev[C][x])\n                \n                # log(p(x|Ci)) = sum(log(p(xi|Ci)))\n                p_xci += np.log(p_xici)\n            \n            # log(p(Ci|x)) = log(p(Ci))+log(p(x|Ci))\n            self._output[:, C] = np.log(self._pclass[C])+p_xci\n\n    def get_output(self):\n        return self._output.squeeze()\n\n    def train(self, training_data, targets):\n        '''\n        Trains the model on the training data.\n\n        Parameters\n        ----------\n        training_data : numpy.array\n            numpy array containing training data.\n        targets : numpy.array\n            numpy array containing training targets, corresponding to the training data.\n\n        Raises\n        ------\n        numpy.AxisError\n            If output_size is less than two. Use :py:func:`pykitml.onehot` to change\n            0/False to [1, 0] and 1/True to [0, 1] for binary classification.\n\n        '''\n        print('Training Model...')\n\n        # Get max values for each feature\n        self._max = np.amax(training_data, axis=0)\n\n        # Loop through each group\n        for group in tqdm.trange(0, self._output_size, ncols=80, unit='groups'):\n            # Get examples for the group\n            group_examples = self._get_group_examples(training_data, targets, group)\n            # Get mean and standard deviation for the group examples\n            self._get_mean_std(group, group_examples, training_data.shape[0])\n            # Get frequency/probability for categorical \n            self._get_freq(group, group_examples)\n\n    def _get_group_examples(self, training_data, targets, group):\n        # Create one hot vector\n        gvec = np.zeros((self._output_size))\n        gvec[group] = 1\n\n        # Get all example data for this group from training data\n        indices = np.argwhere(np.all(targets == gvec, axis=1))\n        group_examples = training_data[indices].squeeze()\n\n        # return\n        return group_examples\n\n    def _get_mean_std(self, group, group_examples, tot_examples):\n        # Get mean, std_dev and p(class)\n        self._pclass[group] = group_examples.shape[0]/tot_examples\n        self._mean[group] = np.mean(group_examples, axis=0)\n        self._std_dev[group] = np.std(group_examples, axis=0)\n\n    def _get_freq(self, group, group_examples):\n        # Get frequencies\n        for feature in self._categorical:\n            # Get frequencies of each category in the feature for this\n            # group example\n            freqp = np.bincount(group_examples[:, feature].astype(int))\n            freqp = freqp/group_examples.shape[0]\n            # Replace with reg_param where p(xi|Ci) is zero\n            # AKA regularization            \n            pad_length = int(self._max[feature]-freqp.shape[0])+1\n            freqp = np.pad(freqp, (0, pad_length), 'constant', constant_values=0)\n            freqp = np.where(freqp==0, self._reg_param, freqp)\n            self._freqp[group][feature] = freqp\n\n\nclass GaussianNaiveBayes(NaiveBayes):\n    def __init__(self, input_size, output_size):\n        '''\n        Parameters\n        ----------\n        input_size : int\n            Size of input data or number of input features.\n        output_size: int\n            Number of categories or groups.\n        '''\n        # Save info\n        self._input_size = input_size\n        self._output_size = output_size\n\n        # for each class/group, p(class), std_dev, mean, freq and range\n        self._pclass = np.zeros((output_size))\n        self._mean = np.zeros((output_size, input_size))\n        self._std_dev = np.zeros((output_size, input_size))\n\n        # Output\n        self._output = None\n\n    def feed(self, input_data):\n        # Make sure array in 2D\n        if(input_data.ndim == 1): input_data = np.array([input_data])\n\n        # Set output to correct size\n        self._output = np.zeros((input_data.shape[0], self._output_size))\n        \n        # Loop through each group and find probability\n        for C in range(0, self._output_size):\n            # Calculate p(xi|Ci)\n            p_xici = _gaussian(input_data, self._mean[C], self._std_dev[C])\n            \n            # log(p(x|Ci)) = sum(log(p(xi|Ci)))\n            p_xci = np.log(p_xici).sum(axis=1)\n            \n            # log(p(Ci|x)) = log(p(Ci))+log(p(x|Ci))\n            self._output[:, C] = np.log(self._pclass[C])+p_xci\n\n    def train(self, training_data, targets):\n        print('Training Model...')\n\n        # Loop through each group\n        for group in tqdm.trange(0, self._output_size, ncols=80, unit='groups'):\n            # Get examples for the group\n            group_examples = self._get_group_examples(training_data, targets, group)\n            # Get mean and standard deviation for the group examples\n            self._get_mean_std(group, group_examples, training_data.shape[0])\n"""
pykitml/nearest_neighbor.py,3,"b""import numpy as np\n\nfrom ._classifier import Classifier\nfrom . import _functions\n\nclass NearestNeighbor(Classifier):\n    '''\n    This class implements nearest neighbor classifier.\n    '''\n\n    def __init__(self, inputs_size, output_size, no_neighbors=1):\n        '''\n        Parameters\n        ----------\n        input_size : int\n            Size of input data or number of input features.\n        output_size : int\n            Number of categories or groups.\n        no_neighbors : int\n            The number of nearest neighbors to consider.\n        '''\n        self._k = no_neighbors\n        self._output = None\n\n        self._input_size = inputs_size\n        self._output_size = output_size\n\n    @property\n    def _out_size(self):\n        return self._output_size\n\n    def train(self, training_data, targets):\n        '''\n        Trains the model on the training data.\n\n        Parameters\n        ----------\n        training_data : numpy.array\n            numpy array containing training data.\n        targets : numpy.array\n            numpy array containing training targets, corresponding to the training data.\n        '''\n        self._inputs = training_data\n        self._outputs = targets\n\n    def feed(self, input_data):\n        # Make sure array is 2D\n        if(input_data.ndim == 1): input_data = np.array([input_data])\n\n        # Get pair wise distances\n        distances = _functions.pdist(input_data, self._inputs)\n\n        # Sort the distances\n        indices = np.argsort(distances, axis=1)[:, 0:self._k]\n\n        # Get output\n        self._output = np.mean(self._outputs[indices], axis=1)\n\n    def get_output(self):\n        return self._output.squeeze()\n    \n"""
pykitml/network.py,15,"b""import numpy as np\n\nfrom ._minimize_model import MinimizeModel\nfrom ._classifier import Classifier\nfrom . import _functions\n\nclass NeuralNetwork(MinimizeModel, Classifier):\n    '''\n    This class implements Feed Neural Network.\n    '''\n\n    def __init__(self, layer_sizes, reg_param=0, config='leakyrelu-softmax-cross_entropy'):\n        '''\n        Parameters\n        ----------\n        layer_sizes : list\n            A list of integers describing the number of layers and the number of neurons in each\n            layer. For e.g. :code:`[784, 100, 100, 10]` describes a neural network with one input\n            layer having 784 neurons, two hidden layers having 100 neurons each and a output layer\n            with 10 neurons.\n        reg_param : int\n            Regularization parameter for the network, also known as 'weight decay'.\n        config : str\n            The config string describes what activation functions and cost function to use for\n            the network. The string should contain three function names seperated with '-' \n            character and should follow the order:\n            :code:`'<hidden_layer_activation_func>-<output_layer_activation_func>-<cost_function>'`.\n            For e.g. :code:`'relu-softmax-cross_entropy'` tells the class to use `relu` as the\n            activation function for input and hidden layers, `softmax` for output layer and\n            `cross entropy` for the cost function.\n\n            List of available activation functions:\n            :code:`leakyrelu`, :code:`relu`, :code:`softmax`, :code:`tanh`, :code:`sigmoid`, :code:`identity`.\n\n            List of available cost functions:\n            :code:`mse` (Mean Squared Error), :code:`cross_entropy` (Cross Entropy).\n\n        Raises\n        ------\n        AttributeError\n            If invalid config string.\n        '''\n        # Get function names from the config string, the config string contains\n        # list of functions to be used in the neural network, seperated with '-' character.\n        # syntax: 'hidden_layers-output_layer-cost function', eg: 'relu-sigmoid-cross_entropy', \n        func_names = config.split('-')\n        func_names_prime = [func_name + '_prime' for func_name in func_names]\n        \n        # Initialize regularization parameter\n        self._reg_param = reg_param\n        self._reg_param_half = reg_param/2\n\n        # Initialize functions\n        self._activ_func = getattr(_functions, func_names[0])\n        self._activ_func_prime = getattr(_functions, func_names_prime[0])\n        self._output_activ_func = getattr(_functions, func_names[1])\n        self._output_activ_func_prime = getattr(_functions, func_names_prime[1])\n        self._cost_func = getattr(_functions, func_names[2])\n        self._cost_func_prime = getattr(_functions, func_names_prime[2])\n\n        # Store activation function names\n        self._functs = func_names\n\n        # Store layer sizes\n        self._lsize = layer_sizes\n\n        # Number of layers\n        self._nlayers = len(layer_sizes)\n\n        # Intialize parameters\n        weights = [np.array([])] * self.nlayers\n        biases = [np.array([])] * self.nlayers\n        # Loop through each layer and initialize random weights and biases\n        for l in range(1, self.nlayers):\n            layer_size = layer_sizes[l]\n            input_layer_size = layer_sizes[l-1]\n            epsilon = np.sqrt(6)/(np.sqrt(layer_size) + np.sqrt(input_layer_size))\n            weights[l] = np.random.rand(layer_size, input_layer_size)*2*epsilon - epsilon\n            biases[l] = np.random.rand(layer_size) * 2 * epsilon - epsilon\n        # Put parameters in numpy dtype=object array\n        self._params = np.array(\n            [np.array(weights, dtype=object), np.array(biases, dtype=object)],\n            dtype=object\n        )  \n\n        # List of numpy arrays for storing temporary values\n        self._weighted_sums = [np.array([])] * self.nlayers\n        self._activations = [np.array([])] * self.nlayers\n\n    def __repr__(self):\n        return 'Layers: '+str(self._lsize)+' Config: '+str(self._functs)\n\n    @property\n    def _mparams(self):\n        return self._params\n    \n    @_mparams.setter\n    def _mparams(self, mparams):\n        self._params = mparams\n\n    @property\n    def _cost_function(self):\n        return self._cost_func\n\n    @property\n    def _out_size(self):\n        return self._lsize[-1]\n\n    @property\n    def nlayers(self):\n        '''\n        The number of layers in the network.\n        '''      \n        return self._nlayers\n\n    def feed(self, input_data):\n        # Constants\n        W = 0 # Weights\n        B = 1 # Biases\n\n        # Set inputs\n        self._activations[0] = input_data\n\n        # Feed through hidden layers\n        for l in range(1, self.nlayers-1):\n            self._weighted_sums[l] = (self._activations[l-1]@self._params[W][l].T) + self._params[B][l]\n            self._activations[l] = self._activ_func(self._weighted_sums[l])\n\n        # Feed thorugh output layer\n        self._weighted_sums[-1] = (self._activations[-2]@self._params[W][-1].T) + self._params[B][-1]\n        self._activations[-1] = self._output_activ_func(self._weighted_sums[-1])\n\n    def get_output(self):\n        return self._activations[-1].squeeze()\n\n    def _backpropagate(self, index, target):\n        # Constants\n        W = 0 # Weights        \n        \n        # Used to hold activation_function'(z[l]) where z[l] = w[l]*a[l-1] + b[l] \n        da_dz = [np.array([])] * self.nlayers\n\n        # Used to hold partial derivatives of cost function w.r.t parameters\n        dc_dw = [np.array([])] * self.nlayers\n        dc_db = [np.array([])] * self.nlayers\n        \n        # Calculate activation_function'(z)\n        def calc_da_dz(l):\n            da_dz[l] = self._activ_func_prime(self._weighted_sums[l][index], self._activations[l][index])\n        \n        # Calculate the partial derivatives of the cost w.r.t all the biases of layer \n        # 'l' (NOT for output layer)\n        def calc_dc_db(l):\n            dc_db[l] = (dc_db[l+1] @ self._params[W][l+1]) * da_dz[l]\n\n        # Calculate the partial derivatives of the cost w.r.t all the weights of layer 'l'\n        def calc_dc_dw(l):\n            dc_dw[l] = np.multiply.outer(dc_db[l], self._activations[l-1][index])\n            # Regularization\n            dc_dw[l] += self._reg_param*self._params[W][l]\n\n        # Calculate the partial derivatives of the cost function w.r.t the ouput layer's \n        # activations, weights, biases\n        da_dz[-1] = self._output_activ_func_prime(self._weighted_sums[-1][index], self._activations[-1][index])\n        dc_db[-1] = self._cost_func_prime(self._activations[-1][index], target) * da_dz[-1]\n        calc_dc_dw(-1)\n\n        # Calculate the partial derivatives of the cost function w.r.t the hidden layers'\n        # activations, weights, biases\n        for l in range(self.nlayers - 2, 0, -1):\n            calc_da_dz(l)\n            calc_dc_db(l)\n            calc_dc_dw(l)\n\n        # return gradient\n        return np.array(\n            [np.array(dc_dw, dtype=object), np.array(dc_db, dtype=object)],\n            dtype=object\n        )\n\n    def _get_norm_weights(self):\n        # If regularization is zero\n        if(self._reg_param == 0): return 0\n        \n        # else\n        W = 0\n        norm = 0\n        # Calculate norm of the weights\n        for l in range(self.nlayers):\n            norm += (self._reg_param_half)*(self._params[W][l]**2).sum()\n        \n        return norm\n"""
pykitml/normalize.py,2,"b""import numpy as np\n\n# ===============================================\n# = Functions for Normalization/Feature-scaling =\n# ===============================================\n\ndef get_minmax(array):\n    '''\n    Returns two row arrays, one array containing minimum values of each column\n    and another one with maximum values.\n\n    Parameters\n    ----------\n    array : numpy.array\n        The array to get minimum and maximum values for.\n\n    Returns\n    -------\n    array_min : numpy.array\n        Array containing minimum values of each column.\n    array_max : numpy.array\n        Array containing maximum values of each column.\n    '''\n    return np.amin(array, axis=0), np.amax(array, axis=0)\n\ndef normalize_minmax(array, array_min, array_max, cols=[]):\n    '''\n    Normalizes columns of the array to between 0 and 1 using min-max\n    normalization.\n\n    Parameters\n    ----------\n    array : numpy.array\n        The array to normalize.\n    array_min : numpy.array\n        Array containing minimum values of each column.\n    array_max : numpy.array\n        Array containing maximum values of each column.\n    cols : list\n        The columns to normalize. If the list is empty (default),\n        all columns will be normalized.\n\n    Returns\n    -------\n    numpy.array\n        The normalized array.\n\n    Note\n    ----\n    You can use :py:func:`~get_minmax` function to get :code:`array_min`\n    and :code:`array_max` parameters.\n    '''\n    normalized_array = array.astype(float)\n    all_normalized = (array - array_min) / (array_max - array_min)\n\n    if(len(cols) == 0):\n        # Normalize all columns\n        normalized_array = all_normalized\n    elif(array.ndim == 1):\n        # Normalize only specified columns, 1D array\n        normalized_array[cols] = all_normalized[cols]\n    else:\n        # Normalize onlt specified columns, 2D array\n        normalized_array[:, cols] = all_normalized[:, cols]\n\n    return normalized_array\n\ndef denormalize_minmax(array, array_min, array_max, cols=[]):\n    '''\n    Denormalizes columns of a min-max normalized array.\n\n    Parameters\n    ----------\n    array : numpy.array\n        The array to denormalize.\n    array_min : numpy.array\n        Array containing minimum values of each column.\n    array_max : numpy.array\n        Array containing maximum values of each column.\n    cols : list\n        The columns to normalize. If the list is empty (default),\n        all columns will be denormalized.\n\n    Returns\n    -------\n    numpy.array\n        The denormalized array.\n\n    Note\n    ----\n    You can use :py:func:`~get_minmax` function to get :code:`array_min`\n    and :code:`array_max` parameters.\n    '''\n    denormalized_array = array.astype(float)\n    all_denormalized = (array * (array_max - array_min)) + array_min\n\n    if(len(cols) == 0):\n        # Deormalize all columns\n        denormalized_array = all_denormalized\n    elif(array.ndim == 1):\n        # Denormalize only specified columns, 1D array\n        denormalized_array[cols] = all_denormalized[cols]\n    else:\n        # Denormalize onlt specified columns, 2D array\n        denormalized_array[:, cols] = all_denormalized[:, cols]\n\n    return denormalized_array\n\n\ndef get_meanstd(array):\n    '''\n    Returns two row arrays, one array containing mean of each column\n    and another one with standard deviation of each column.\n\n    Parameters\n    ----------\n    array : numpy.array\n        The array to get mean and standard deviation values for.\n\n    Returns\n    -------\n    array_mean : numpy.array\n        Array containing mean values of each column.\n    array_stddev : numpy.array\n        Array containing standard deviation values of each column.\n    '''\n    return np.mean(array, axis=0), np.std(array, axis=0)\n\ndef normalize_mean(array, array_mean, array_stddev, cols=[]):\n    '''\n    Normalizes columns of the array with mean normalization.\n\n    Parameters\n    ----------\n    array : numpy.array\n        The array to normalize.\n    array_mean : numpy.array\n        Array containing mean values of each column.\n    array_stddev : numpy.array\n        Array containing standard deviation values of each column.\n    cols : list\n        The columns to normalize. If the list is empty (default),\n        all columns will be normalized.\n\n\n    Returns\n    -------\n    numpy.array\n        The normalized array.\n\n    Note\n    ----\n    You can use :py:func:`~get_meanstd` function to get :code:`array_mean`\n    and :code:`array_stddev` parameters.\n    '''\n    normalized_array = array.astype(float)\n    all_normalized = (array-array_mean)/array_stddev\n\n    if(len(cols) == 0):\n        # Normalize all columns\n        normalized_array = all_normalized\n    elif(array.ndim == 1):\n        # Normalize only specified columns, 1D array\n        normalized_array[cols] = all_normalized[cols]\n    else:\n        # Normalize only specified columns, 2D array\n        normalized_array[:, cols] = all_normalized[:, cols]\n\n    return normalized_array\n\ndef denormalize_mean(array, array_mean, array_stddev, cols=[]):\n    '''\n    Denormalizes a mean normalized array.\n\n    Parameters\n    ----------\n    array : numpy.array\n        The array to denormalize.\n    array_mean : numpy.array\n        Array containing mean values of each column.\n    array_stddev : numpy.array\n        Array containing standard deviation values of each column.\n\n    Returns\n    -------\n    numpy.array\n        The denormalized array.\n\n    Note\n    ----\n    You can use :py:func:`~get_meanstd` function to get :code:`array_mean`\n    and :code:`array_stddev` parameters.\n    '''\n    denormalized_array = array.astype(float)\n    all_denormalized = (array*array_stddev) + array_mean\n\n    if(len(cols) == 0):\n        # Denormalize all columns\n        denormalized_array = all_denormalized\n    elif(array.ndim == 1):\n        # Denormalize only specified columns, 1D array\n        denormalized_array[cols] = all_denormalized[cols]\n    else:\n        # Denormalize onlt specified columns, 2D array\n        denormalized_array[:, cols] = all_denormalized[:, cols]\n\n    return denormalized_array"""
pykitml/optimizers.py,0,"b""from abc import ABC, abstractmethod\n\nimport numpy as np\n\n'''\nOptimizers module,\nREF: http://cs231n.github.io/neural-networks-3/\n'''\n\nclass Optimizer(ABC):\n    '''\n    This class is the base class for all optimizers\n    '''\n    @abstractmethod\n    def _optimize(self, parameter, parameter_gradient):\n        pass\n\n    @property\n    @abstractmethod\n    def _mlearning_rate(self):\n        pass\n\n    @_mlearning_rate.setter\n    @abstractmethod\n    def _mlearning_rate(self, learning_rate):\n        pass\n\n    @property\n    @abstractmethod\n    def _mdecay_rate(self):\n        pass\n\n    def _decay(self):\n        self._mlearning_rate = self._mdecay_rate*self._mlearning_rate\n\n\nclass GradientDescent(Optimizer):\n    '''\n    This class implements gradient descent optimization.\n    '''\n    def __init__(self, learning_rate, decay_rate=1):\n        '''\n        Parameters\n        ----------\n            learning_rate : float\n            decay_rate : float\n                Decay rate for leraning rate\n        '''\n        self._learning_rate = learning_rate\n        self._decay_rate = decay_rate\n\n    @property\n    def _mlearning_rate(self):\n        return self._learning_rate\n\n    @_mlearning_rate.setter\n    def _mlearning_rate(self, learning_rate):\n        self._learning_rate = learning_rate\n\n    @property\n    def _mdecay_rate(self):\n        return self._decay_rate\n\n    def _optimize(self, parameter, parameter_gradient):\n        # Update and return the parameter\n        return parameter - (self._learning_rate * parameter_gradient)\n\n\nclass Momentum(Optimizer):\n    '''\n    This class implements momentum optimization.\n    '''\n    def __init__(self, learning_rate, decay_rate=1, beta=0.9):\n        '''\n        Parameters\n        ----------\n            learning_rate : float\n            decay_rate : float\n                Decay rate for leraning rate\n            beta : float\n                Should be between 0 to 1.\n        '''\n        self._learning_rate = learning_rate\n        self._decay_rate = decay_rate\n        self._beta = beta\n        self._v = 0\n\n    @property\n    def _mlearning_rate(self):\n        return self._learning_rate\n\n    @_mlearning_rate.setter\n    def _mlearning_rate(self, learning_rate):\n        self._learning_rate = learning_rate\n\n    @property\n    def _mdecay_rate(self):\n        return self._decay_rate\n\n    def _optimize(self, parameter, parameter_gradient):\n        # Integrate v\n        self._v = (self._beta*self._v) - (self._learning_rate*parameter_gradient)\n        # Update and return the parameter\n        return parameter + self._v\n\n\nclass Nesterov(Optimizer):\n    '''\n    This class implements neterov momentum optimization.\n    '''\n    def __init__(self, learning_rate, decay_rate=1, beta=0.9):\n        '''\n        Parameters\n        ----------\n            learning_rate : float\n            decay_rate : float\n                Decay rate for leraning rate\n            beta : float\n                Should be between 0 to 1.\n        '''\n        self._learning_rate = learning_rate\n        self._decay_rate = decay_rate\n        self._beta = beta\n        self._v = 0\n        self._v_prev = 0\n\n    @property\n    def _mlearning_rate(self):\n        return self._learning_rate\n\n    @_mlearning_rate.setter\n    def _mlearning_rate(self, learning_rate):\n        self._learning_rate = learning_rate\n\n    @property\n    def _mdecay_rate(self):\n        return self._decay_rate\n\n    def _optimize(self, parameter, parameter_gradient):\n        # Back up before updating\n        self._v_prev = self._v\n        # Integrate v\n        self._v = (self._beta*self._v) - (self._learning_rate*parameter_gradient)\n        # Update and return the parameter\n        return parameter - (self._beta*self._v_prev) + ((1+self._beta)*self._v)\n\n\nclass Adagrad(Optimizer):\n    '''\n    This class implements adagrad optmization.\n    '''\n    def __init__(self, learning_rate, decay_rate=1):\n        '''\n        Parameters\n        ----------\n            learning_rate : float\n            decay_rate : float\n                Decay rate for leraning rate\n        '''\n        self._learning_rate = learning_rate\n        self._decay_rate = decay_rate\n        self._cache = 0\n\n    @property\n    def _mlearning_rate(self):\n        return self._learning_rate\n\n    @_mlearning_rate.setter\n    def _mlearning_rate(self, learning_rate):\n        self._learning_rate = learning_rate\n\n    @property\n    def _mdecay_rate(self):\n        return self._decay_rate\n\n    def _optimize(self, parameter, parameter_gradient):\n        # For numerical stability\n        eps = 10e-8\n        # Calculate cache\n        self._cache += parameter_gradient**2\n        # Update parameter and return\n        return parameter + (-self._learning_rate*parameter_gradient)/((self._cache**0.5)+eps)\n\n\nclass RMSprop(Optimizer):\n    '''\n    This class implements RMSprop optimization.\n    '''\n    def __init__(self, learning_rate, decay_rate=1, beta=0.9):\n        '''\n        Parameters\n        ----------\n            learning_rate : float\n            decay_rate : float\n                Decay rate for leraning rate\n            beta : float\n                Should be between 0 to 1.\n        '''\n        self._learning_rate = learning_rate\n        self._decay_rate = decay_rate\n        self._beta = beta\n        self._cache = 0\n\n    @property\n    def _mlearning_rate(self):\n        return self._learning_rate\n\n    @_mlearning_rate.setter\n    def _mlearning_rate(self, learning_rate):\n        self._learning_rate = learning_rate\n\n    @property\n    def _mdecay_rate(self):\n        return self._decay_rate\n\n    def _optimize(self, parameter, parameter_gradient):\n        # For numerical stability\n        eps = 10e-8\n        # Calculate cache\n        self._cache = self._beta*self._cache + (1-self._beta)*(parameter_gradient**2)\n        # Update parameter and return\n        return parameter + (-self._learning_rate*parameter_gradient)/((self._cache**0.5)+eps)\n        \n        \nclass Adam(Optimizer):\n    '''\n    This class implements adam optimization.\n    '''\n    def __init__(self, learning_rate, decay_rate=1, beta1=0.9, beta2=0.9):\n        '''\n        Parameters\n        ----------\n            learning_rate : float\n            decay_rate : float\n                Decay rate for leraning rate\n            beta1 : float\n                Should be between 0 to 1.\n            beta2 : float\n                Should be between 0 to 1.\n        '''\n        self._learning_rate = learning_rate\n        self._decay_rate = decay_rate\n        self._beta1 = beta1\n        self._beta2 = beta2\n        self._m = 0\n        self._v = 0\n\n    @property\n    def _mlearning_rate(self):\n        return self._learning_rate\n\n    @_mlearning_rate.setter\n    def _mlearning_rate(self, learning_rate):\n        self._learning_rate = learning_rate\n\n    @property\n    def _mdecay_rate(self):\n        return self._decay_rate\n\n    def _optimize(self, parameter, parameter_gradient):\n        # For numerical stability\n        eps = 10e-8\n        # Momentum\n        self._m = self._beta1*self._m + (1-self._beta1)*parameter_gradient\n        # RMS\n        self._v = self._beta2*self._v + (1-self._beta2)*(parameter_gradient**2)\n        # Update parameter\n        return parameter + (-self._learning_rate*self._m)/((self._v**0.5)+eps)\n"""
pykitml/pca.py,2,"b""import numpy as np\n\nclass PCA:\n    '''\n    This class implements Principle Component Analysis.\n    '''\n    def __init__(self, data_points, no_components):\n        '''\n        This class implements Principle Component Analysis, used for \n        dimensionality reduction.\n\n        Parameters\n        ----------\n        data_points : numpy.array\n            The dataset to perform PCA i.e. dimensionality reduction on.\n        no_components : int\n            Number of principle components to use.\n        '''\n        # Calculate covariance matrix\n        covariance_matrix = (data_points.T) @ data_points;\n        covariance_matrix = covariance_matrix/data_points.shape[0]\n\n        # Perform Singular Value Decomposition on the comvariance matrix\n        u, s, v = np.linalg.svd(covariance_matrix, full_matrices=True)\n\n        # Calculate amount of variance retained\n        self._retention = np.sum(s[0:no_components])/np.sum(s)\n\n        # The transformation matrix for PCA\n        self._transform = u[:, 0:no_components]\n\n    def transform(self, data_points):\n        '''\n        Transforms the input dataset to lower dimensions.\n        \n        Parameters\n        ----------\n        data_points : nunmpy.array\n            The input dataset.\n        '''\n        # Transform the datapoints using principle components\n        return data_points@self._transform\n\n    def inverse_transform(self, pca_points):\n        '''\n        Gets the original dataset from transformed points.\n\n        Parameters\n        ----------\n        pca_points : numpy.array\n            The trasformed points.\n\n        '''\n        # Transform from principle components back to approx feature\n        return pca_points @ (self._transform.T)\n\n    @property\n    def retention(self):\n        '''\n        Returns the amount of variance retained, between 0 and 1.\n        '''\n        return round(self._retention, 2)"""
pykitml/pklhandler.py,0,"b""import pickle\n\n'''\nThis module contains functions for saving and \nloading .pkl files\n'''\n\ndef save(object_, file_name):\n    '''\n    Saves an object into a file.\n\n    Parameters\n    ----------\n    object_ : object\n        The object to save\n    file_name : str\n        The name of the file to save the object in.\n\n    Raises\n    ------\n        OSError\n            If the file cannot be created due to a system-related error.\n    '''\n    file = open(file_name, 'wb')\n    pickle.dump(object_, file)\n    file.close()\n\n\ndef load(file_name):    \n    '''\n    Loads an object from file.\n\n    Parameters\n    ----------\n    file_name : str\n        The name of the file to load the object from.\n\n    Returns\n    -------\n    object\n        The python object stored in the file.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the file does not exist.\n    '''\n    file = open(file_name, 'rb')\n    object_ = pickle.load(file)\n    file.close()\n    return object_\n"""
pykitml/preprocessing.py,16,"b""from itertools import combinations_with_replacement\n\nimport numpy as np\n\n'''\nThis module contains helper functions for preprocessing data.\n'''\n\ndef onehot(input_array):\n    '''\n    Converts input array to one-hot array.\n\n    Parameters\n    ----------\n    input_array : numpy.array\n        The input numpy array.\n    \n    Returns\n    -------\n    one_hot : numpy.array\n        The converted onehot array.\n\n    Example\n    -------\n    >>> import numpy as np\n    >>> import pykitml as pk\n    >>> a = np.array([0, 1, 2])\n    >>> pk.onehot(a)\n    array([[1., 0., 0.],\n           [0., 1., 0.],\n           [0., 0., 1.]])\n    '''\n    array = input_array.astype(int)\n    one_hot = np.zeros((array.size, array.max()+1))\n    one_hot[np.arange(array.size), array] = 1\n    return one_hot\n\ndef onehot_cols(dataset, cols):\n    '''\n    Converts/replaces columns of dataset to one-hot values.\n\n    Parameters\n    ----------\n    dataset : numpy.array\n        The input dataset.\n    cols : list\n        The columns which has to be replaced/converted\n        to one-hot values.\n\n    Returns\n    -------\n    dataset_new : numpy.array\n        The new dataset with replaced columns.\n\n    Example\n    -------\n        \n        >>> import pykitml as pk\n        >>> import numpy as np\n        >>> a = np.array([[0, 1, 2.2], [1, 2, 3.4], [0, 0, 1.1]]) \n        >>> a\n        array([[0. , 1. , 2.2],\n               [1. , 2. , 3.4],\n               [0. , 0. , 1.1]])\n        >>> pk.onehot_cols(a, cols=[0, 1])\n        array([[1. , 0. , 0. , 1. , 0. , 2.2],\n               [0. , 1. , 0. , 0. , 1. , 3.4],\n               [1. , 0. , 1. , 0. , 0. , 1.1]])\n\n    '''\n    offset=0\n    dataset_new = dataset\n    for col in cols:\n        onehot_colmn = onehot(dataset_new[:, col+offset])\n        dataset_new = np.delete(dataset_new, col+offset, axis=1)\n        dataset_new = np.insert(dataset_new, [col+offset], onehot_colmn, axis=1)\n        offset += onehot_colmn.shape[1]-1 \n\n    return dataset_new\n\ndef onehot_cols_traintest(dataset_train, dataset_test,  cols):\n    '''\n    Converts/replaces columns of :code:`dataset_train` and \n    :code:`dataset_test` to one-hot values.\n\n    Parameters\n    ----------\n    dataset_train : numpy.array\n        The training dataset.\n    dataset_test : numpy.array\n        The testing dataset.\n    cols : list\n        The columns which has to be replaced/converted\n        to one-hot values.\n\n    Returns\n    -------\n    dataset_train_new : numpy.array\n        The new training dataset with replaced columns.\n    dataset_test_new : numpy.array\n        The new testing dataset with replaced columns.\n\n    Example\n    -------\n        \n        >>> import pykitml as pk\n        >>> import numpy as np\n        >>> a_train = np.array([[0, 1, 3.2], [1, 2, 3.5], [0, 0, 3.4]])\n        >>> a_test = np.array([[0, 3, 3.2], [1, 2, 4.5], [1, 3, 4.5]])\n        >>> a_train_onehot, a_test_onehot = pk.onehot_cols_traintest(a_train, a_test, cols=[0,1])\n        >>> a_train_onehot\n        array([[1. , 0. , 0. , 1. , 0. , 0. , 3.2],\n               [0. , 1. , 0. , 0. , 1. , 0. , 3.5],\n               [1. , 0. , 1. , 0. , 0. , 0. , 3.4]])\n        >>> a_test_onehot\n        array([[1. , 0. , 0. , 0. , 0. , 1. , 3.2],\n               [0. , 1. , 0. , 0. , 1. , 0. , 4.5],\n               [0. , 1. , 0. , 0. , 0. , 1. , 4.5]])\n\n    '''\n    # Combine the datasets\n    dataset_new = np.concatenate((dataset_train, dataset_test), axis=0)\n    \n    # Replace columns with on hot values\n    offset=0\n    for col in cols:\n        onehot_colmn = onehot(dataset_new[:, col+offset])\n        dataset_new = np.delete(dataset_new, col+offset, axis=1)\n        dataset_new = np.insert(dataset_new, [col+offset], onehot_colmn, axis=1)\n        offset += onehot_colmn.shape[1]-1 \n\n    split = dataset_train.shape[0]\n    return dataset_new[:split, :], dataset_new[split:, :]\n\ndef polynomial(dataset_inputs, degree=3, cols=[]):\n    '''\n    Generates polynomial features from the input dataset.\n    For example, if an input sample is two dimensional and of the form [a, b], \n    the degree-2 polynomial features are :code:`[a, b, a^2, ab, b^2]`, and degree-3\n    polynomial features are \n    :code:`[a, b, a^2, ab, b^2, a^3, (a^2)*b, a*(b^2), b^3]`.\n\n    Parameters\n    ----------\n    dataset_inputs : numpy.array\n        The input dataset to generate the ploynomials from.\n    degree : int\n        The dgree of the polynomial.\n    cols : list\n        The columns to use to generate polynomial features, columns\n        not in this list will be ignored. If empty (default), all columns will\n        used to generate polynomial features.\n\n    Returns\n    -------\n    numpy.array\n        The new dataset with polynomial features.\n\n    Example\n    -------\n\n        >>> import numpy as np\n        >>> import pykitml as pk\n        >>> pk.polynomial(np.array([[1, 2], [2, 3]]), degree=2)\n        array([[1., 2., 1., 2., 4.],\n               [2., 3., 4., 6., 9.]])\n        >>> pk.polynomial(np.array([[1, 2], [2, 3]]), degree=3)\n        array([[ 1.,  2.,  1.,  2.,  4.,  1.,  2.,  4.,  8.],\n               [ 2.,  3.,  4.,  6.,  9.,  8., 12., 18., 27.]])\n        >>> pk.polynomial(np.array([[1, 4, 5, 2], [2, 5, 6, 3]]), degree=2, cols=[0, 3])\n        array([[1., 4., 5., 2., 1., 2., 4.],\n               [2., 5., 6., 3., 4., 6., 9.]])\n               \n    '''\n    # Make sure 2D array\n    if(dataset_inputs.ndim == 1):\n        inputs = np.array([dataset_inputs])\n    else:\n        inputs = dataset_inputs\n\n    # Choose the columns to genrate polynomial features for\n    if(len(cols) == 0): cols = range(inputs.shape[1])\n    \n    poly_dataset = inputs\n\n    # Generate degree terms\n    for d in range(2, degree+1):\n        # Generate terms indices for degree d\n        term_indices = list(combinations_with_replacement(cols, r=d))\n        # Multiply them to form the term and concatenate\n        for indices in term_indices:\n            term = inputs[:, indices].prod(axis=1)\n            temp = np.zeros((poly_dataset.shape[0], poly_dataset.shape[1]+1))\n            temp[:, :-1] = poly_dataset\n            temp[:, -1] = term\n            poly_dataset = temp\n\n    return poly_dataset.squeeze() \n\n"""
pykitml/random_forest.py,3,"b""import os\nimport random\nimport multiprocessing as mp\nfrom math import ceil\nfrom contextlib import redirect_stdout\n\nimport numpy as np\nimport tqdm\n\nfrom . import _shared_array\nfrom ._classifier import Classifier\nfrom . import decision_tree\n\n\nclass _RandomTree(decision_tree.DecisionTree):\n    def __init__(self, input_size, output_size, num_features, feature_type=[], max_depth=6):\n        # Initialize parent class\n        super(_RandomTree, self).__init__(input_size, output_size, feature_type, max_depth)\n\n        # Select only a few random columns of the dataset for training\n        self._cols_train = np.random.choice(input_size, num_features, replace=False)\n\n        # Disable progress bar\n        self._pbardis = True\n        \n\nclass RandomForest(Classifier):\n    def __init__(self, input_size, output_size, feature_type=[], max_depth=6):\n        '''\n        Parameters\n        ----------\n        input_size : int\n            Size of input data or number of input features.\n        output_size : int\n            Number of categories or groups.\n        feature_type : list\n            List of string describing the type of feature for\n            each column. Can be :code:`'continues'`, \n            :code:`'ranked'`, or :code:`'categorical'`.\n        max_depth : int\n            The maximum depth the trees can grow to.\n\n        Raises\n        ------\n        InvalidFeatureType\n            Invalid/Unknown feature type. Can only be :code:`'continues'`, \n            :code:`'ranked'`, or :code:`'categorical'`.\n        '''\n        # Save values\n        self._input_size = input_size\n        self._output_size = output_size\n        self._ftype = feature_type\n        self._max_depth = max_depth\n\n        # List to store trees in\n        self._trees = []\n\n        # Outputs\n        self._output = None\n\n    @property\n    def _out_size(self):\n        return self._output_size\n    \n    @property\n    def trees(self):\n        '''\n        A list of decision trees used in the forest.\n        '''\n        return self._trees\n\n    def train(self, inputs, outputs, num_trees=100, num_feature_bag=None):\n        '''\n        Trains the model on the training data.\n\n        Parameters\n        ----------\n        training_data : numpy.array\n            numpy array containing training data.\n        targets : numpy.array\n            numpy array containing training targets, corresponding to the training data.\n        num_trees : int\n            Number of trees to grow.\n        num_feature_bag : int or None\n            Number of random features to select when growing\n            a tree. If :code:`None` (default), ceiling of square root of :code:`input_size`\n            is chosen.\n\n        Raises\n        ------\n        numpy.AxisError\n            If output_size is less than two. Use :py:func:`pykitml.onehot` to change\n            0/False to [1, 0] and 1/True to [0, 1] for binary classification.\n        '''\n        print('Training Model...')\n\n        # Number of features to bag/choose for each tree\n        if(num_feature_bag is None): \n            num_feature_bag = ceil(np.sqrt(self._input_size))\n\n        def train_trees(input_q, ret_q, inputs_sh, inputs_shape, outputs_sh, outputs_shape):\n            # Retrive numpy arrays from multiprocessing arrays\n            inputs = _shared_array.shm_as_ndarray(inputs_sh, inputs_shape)\n            output = _shared_array.shm_as_ndarray(outputs_sh, outputs_shape)\n            \n            # Supress print statements\n            with redirect_stdout(open(os.devnull, 'w')):\n                while(True):\n                    # Get tree from input queue\n                    try:\n                        tree = input_q.get(block=False)\n                    except mp.queues.Empty:\n                        break\n\n                    # Create bootstraped datset\n                    indices = np.random.choice(inputs.shape[0], inputs.shape[0])\n                    bootstrapped_inputs = inputs[indices]\n                    bootstrapped_outputs = outputs[indices]\n\n                    # Grow the tree\n                    tree.train(bootstrapped_inputs, bootstrapped_outputs)  \n\n                    # Put the trained tree in output queue\n                    ret_q.put(tree)  \n\n        # Create queues\n        input_q = mp.Queue()\n        ret_q = mp.Queue()\n\n        # Initialize input queue\n        for i in range(num_trees):\n            # Create tree\n            tree = _RandomTree(self._input_size, self._output_size, num_feature_bag, \n                self._ftype, self._max_depth)\n            # Put it in queue\n            input_q.put(tree)\n\n        # Create shared multiprocess array for inputs and outputs\n        inputs_sh = _shared_array.ndarray_to_shm(inputs)\n        outputs_sh = _shared_array.ndarray_to_shm(outputs)\n\n        # Start multiprocess\n        for i in range(os.cpu_count()):\n            p = mp.Process(\n                target=train_trees, args=(input_q, ret_q, \n                    inputs_sh, inputs.shape, outputs_sh, outputs.shape)\n            )\n            p.start()\n\n        # Progress bar and append trained trees to list\n        pbar = tqdm.tqdm(total=num_trees, ncols=80, unit='trees')\n        \n        while(len(self._trees) != num_trees):\n            tree = ret_q.get()\n            self._trees.append(tree)\n            pbar.update()\n        \n        # Return if done\n        pbar.close()\n\n    def feed(self, input_data):\n        # Loop through all the trees and total their outputs\n        total = 0\n        for tree in self._trees:\n            tree.feed(input_data)\n            total += tree.get_output()\n        \n        # Average\n        self._output = total/len(self._trees)\n        \n    \n    def get_output(self):\n        return self._output.squeeze()\n\n        \n\n"""
pykitml/svm.py,1,"b""import functools\n\nimport numpy as np\n\nfrom ._single_layer_model import SingleLayerModel\nfrom ._classifier import Classifier\nfrom . import _functions\n\ndef gaussian_kernel(input_data, training_inputs, sigma=1):\n    '''\n    Transforms the give input data using the gaussian kernal.\n\n    Parameters\n    ----------\n    input_data : numpy.array\n        The input data points to transform.\n    training_inputs : numpy.array\n        The training data.\n    sigma : float\n        Hyperparameter that determines the 'spread' of the kernel.\n\n    '''\n    # Calculate squared L2 norm of each data point with \n    # every other data point\n    distances = _functions.pdist(input_data, training_inputs)\n    # Apply gaussian kernel\n    transformed_inputs = np.exp((-1/(2*sigma**2))*distances)\n    # return\n    return transformed_inputs\n\nclass SVM(SingleLayerModel, Classifier):\n    '''\n    Implements Support Vector Machine with Linear Kernel.\n\n    Note\n    ----\n    The outputs/targets in the training/testing data should have :code:`-1` instead\n    of :code:`0` for training. See example for more details.\n    '''\n\n    @property\n    def _activ_func(self):\n        return _functions.identity\n\n    @property\n    def _activ_func_prime(self):\n        return _functions.identity_prime\n\n    @property\n    def _cost_func(self):\n        return _functions.hinge_loss\n\n    @property\n    def _cost_func_prime(self):\n        return _functions.hinge_loss_prime\n\n"""
pykitml/testing.py,1,"b'import os\nimport cProfile\nfrom unittest.mock import patch\nfrom functools import wraps\n\nimport matplotlib.pyplot as plt\nfrom graphviz import Digraph \n\nimport numpy as np\n\n\ndef _profile(test_func):\n    \'\'\'\n    Calls test function and profiles it.\n\n    Parameters\n    ----------\n    test_func : function\n        The function to test and profile.\n    \'\'\'\n    # Reset random seed\n    np.random.seed(0)\n    # Call the test function and profile it\n    profiler = cProfile.Profile()\n    profiler.runcall(test_func)\n    profiler.dump_stats(test_func.__name__+\'.dat\') \n\n\ndef pktest_graph(test_func):\n    \'\'\'\n    To test and profile function under pytest. Will prevent \n    :code:`matplotlib.pyplot.show()` from blocking other tests.\n\n    Parameters\n    ----------\n    test_func : function\n        The function to test and profile.\n    \'\'\'\n    # Create wrapper function for testing and profiling in pytest\n    @wraps(test_func)\n    def test_wrapper():\n        # Close any open plots\n        plt.close()\n        plt.clf()\n\n        with patch(\'matplotlib.pyplot.show\') as show_func, patch(\'graphviz.Digraph.view\') as view_func:\n            # Run the test function\n            _profile(test_func)\n        \n            # Test if graph worked\n            if ""PYTEST_CURRENT_TEST"" in os.environ:\n                assert show_func.called\n                \n\n    return test_wrapper\n\n\ndef pktest_nograph(test_func):\n    \'\'\'\n    To test and profile function under pytest.\n\n    Parameters\n    ----------\n    test_func : function\n        The function to test and profile.\n    \'\'\'\n    # Create wrapper function for testing and profiling in pytest\n    @wraps(test_func)\n    def test_wrapper():\n        _profile(test_func)\n\n    return test_wrapper'"
tests/test_adult.py,0,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_adult():\n    import os.path\n\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import adult\n\n    # Download the dataset \n    if(not os.path.exists('adult.data.pkl')): adult.get()\n\n    # Load adult data set\n    inputs_train, outputs_train, inputs_test, outputs_test = adult.load()\n\n    # Normalize dataset\n    array_min, array_max = pk.get_minmax(inputs_train)\n    inputs_train = pk.normalize_minmax(inputs_train, array_min, array_max, cols=[0, 2, 9, 10, 11])\n    inputs_test = pk.normalize_minmax(inputs_test, array_min, array_max, cols=[0, 2, 9, 10, 11])\n\n    # Convert categorical values to one-hot values\n    inputs_train, inputs_test = pk.onehot_cols_traintest(inputs_train, inputs_test, cols=[1, 3, 4, 5, 6, 7, 8, 9, 12])\n\n    # Create model\n    adult_classifier = pk.LogisticRegression(104, 1)\n\n    # Train the model\n    adult_classifier.train(\n        training_data=inputs_train,\n        targets=outputs_train, \n        batch_size=10, \n        epochs=1500, \n        optimizer=pk.Adam(learning_rate=0.015, decay_rate=0.99),\n        testing_data=inputs_test,\n        testing_targets=outputs_test, \n        testing_freq=30,\n        decay_freq=40\n    )\n\n    # Save it\n    pk.save(adult_classifier, 'adult_classifier.pkl') \n    \n    # Plot performance\n    adult_classifier.plot_performance()\n    \n    # Print accuracy\n    accuracy = adult_classifier.accuracy(inputs_train, outputs_train)\n    print('Train accuracy:', accuracy)\n    accuracy = adult_classifier.accuracy(inputs_test, outputs_test)\n    print('Test accuracy:', accuracy)\n\n    # Plot confusion matrix\n    adult_classifier.confusion_matrix(inputs_test, outputs_test)\n\n    # Assert if it has enough accuracy\n    assert adult_classifier.accuracy(inputs_test, outputs_test) >= 82\n\n\nif __name__ == '__main__':\n    try:\n        test_adult.__wrapped__()\n    except AssertionError:\n        pass"""
tests/test_adult_forest.py,0,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\nimport pytest\n\n@pytest.mark.skip(reason='Will take too long')\n@pktest_graph\ndef test_adult_forest():\n    import os\n\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import adult\n\n    # Download the dataset \n    if(not os.path.exists('adult.data.pkl')): adult.get()\n\n    # Load adult data set\n    inputs_train, outputs_train, inputs_test, outputs_test = adult.load() \n    outputs_train = pk.onehot(outputs_train)\n    outputs_test = pk.onehot(outputs_test)\n\n    # Create model\n    ftypes = [\n        'continues', 'categorical', 'continues', 'categorical',\n        'categorical', 'categorical', 'categorical', 'categorical', 'categorical',\n        'continues', 'continues', 'continues', 'categorical'\n    ]\n    forest_adult_classifier = pk.RandomForest(13, 2, max_depth=9, feature_type=ftypes)\n\n    # Train\n    forest_adult_classifier.train(inputs_train, outputs_train, num_feature_bag=13)\n\n    # Save it\n    pk.save(forest_adult_classifier, 'forest_adult_classifier.pkl')\n\n    # Print accuracy\n    accuracy = forest_adult_classifier.accuracy(inputs_train, outputs_train)\n    print('Train accuracy:', accuracy)\n    accuracy = forest_adult_classifier.accuracy(inputs_test, outputs_test)\n    print('Test accuracy:', accuracy)\n\n    # Plot confusion matrix\n    forest_adult_classifier.confusion_matrix(inputs_test, outputs_test, \n        gnames=['False', 'True'])\n\n    # Assert accuracy\n    assert (forest_adult_classifier.accuracy(inputs_test, outputs_test)) >= 84\n\nif __name__ == '__main__':\n    try:\n        test_adult_forest.__wrapped__()\n    except AssertionError:\n        pass"""
tests/test_adult_tree.py,0,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\nimport pytest\n\n@pytest.mark.skip(reason='Will take too long')\n@pktest_graph\ndef test_adult_tree():\n    import os\n\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import adult\n\n    # Download the dataset \n    if(not os.path.exists('adult.data.pkl')): adult.get()\n\n    # Load adult data set\n    inputs_train, outputs_train, inputs_test, outputs_test = adult.load() \n    outputs_train = pk.onehot(outputs_train)\n    outputs_test = pk.onehot(outputs_test)\n\n    # Create model\n    ftypes = [\n        'continues', 'categorical', 'continues', 'categorical',\n        'categorical', 'categorical', 'categorical', 'categorical', 'categorical',\n        'continues', 'continues', 'continues', 'categorical'\n    ]\n    tree_adult_classifier = pk.DecisionTree(13, 2, max_depth=6, feature_type=ftypes)\n\n    # Train\n    tree_adult_classifier.train(inputs_train, outputs_train)\n\n    # Save it\n    pk.save(tree_adult_classifier, 'tree_adult_classifier.pkl')\n\n    # Print accuracy\n    accuracy = tree_adult_classifier.accuracy(inputs_train, outputs_train)\n    print('Train accuracy:', accuracy)\n    accuracy = tree_adult_classifier.accuracy(inputs_test, outputs_test)\n    print('Test accuracy:', accuracy)\n\n    # Plot confusion matrix\n    tree_adult_classifier.confusion_matrix(inputs_test, outputs_test, \n        gnames=['False', 'True'])\n\n    # Plot descision tree\n    tree_adult_classifier.show_tree()\n\n    # Assert accuracy\n    assert (tree_adult_classifier.accuracy(inputs_test, outputs_test)) >= 84\n\nif __name__ == '__main__':\n    try:\n        test_adult_tree.__wrapped__()\n    except AssertionError:\n        pass"""
tests/test_banknote.py,1,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_banknote():\n    import os.path\n\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import banknote\n\n    # Download the dataset \n    if(not os.path.exists('banknote.pkl')): banknote.get()\n\n    # Load banknote data set\n    inputs_train, outputs_train, inputs_test, outputs_test = banknote.load()\n\n    # Normalize dataset\n    array_min, array_max = pk.get_minmax(inputs_train)\n    inputs_train = pk.normalize_minmax(inputs_train, array_min, array_max)\n    inputs_test = pk.normalize_minmax(inputs_test, array_min, array_max)\n\n    # Create polynomial features\n    inputs_train_poly = pk.polynomial(inputs_train)\n    inputs_test_poly = pk.polynomial(inputs_test)\n\n    # Create model\n    banknote_classifier = pk.LogisticRegression(inputs_train_poly.shape[1], 1)\n\n    # Train the model\n    banknote_classifier.train(\n        training_data=inputs_train_poly,\n        targets=outputs_train, \n        batch_size=10, \n        epochs=1500, \n        optimizer=pk.Adam(learning_rate=0.06, decay_rate=0.99),\n        testing_data=inputs_test_poly,\n        testing_targets=outputs_test, \n        testing_freq=30,\n        decay_freq=40\n    )\n\n    # Save it\n    pk.save(banknote_classifier, 'banknote_classifier.pkl') \n\n    # Plot performance\n    banknote_classifier.plot_performance()\n    \n    # Print accuracy\n    accuracy = banknote_classifier.accuracy(inputs_train_poly, outputs_train)\n    print('Train accuracy:', accuracy)\n    accuracy = banknote_classifier.accuracy(inputs_test_poly, outputs_test)\n    print('Test accuracy:', accuracy)\n\n    # Plot confusion matrix\n    banknote_classifier.confusion_matrix(inputs_test_poly, outputs_test)\n\n    # Assert if it has enough accuracy\n    assert banknote_classifier.accuracy(inputs_test_poly, outputs_test) >= 99\n\n@pktest_nograph\ndef test_predict_banknote():\n    import os.path\n\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import banknote\n\n    # Predict banknote validity with variance, skewness, curtosis, entropy\n    # of -2.3, -9.3, 9.37, -0.86\n\n    # Load banknote data set\n    inputs_train, outputs_train, inputs_test, outputs_test = banknote.load()\n\n    # Load the model\n    banknote_classifier = pk.load('banknote_classifier.pkl')\n\n    # Normalize the inputs\n    array_min, array_max = pk.get_minmax(inputs_train)\n    input_data = pk.normalize_minmax(np.array([-2.3, -9.3, 9.37, -0.86]), array_min, array_max)\n\n    # Create polynomial features\n    input_data_poly = pk.polynomial(input_data)\n\n    # Get output\n    banknote_classifier.feed(input_data_poly)\n    model_output = banknote_classifier.get_output()\n\n    # Print result\n    print(model_output)  \n\nif __name__ == '__main__':\n    try:\n        test_banknote.__wrapped__()\n        test_predict_banknote.__wrapped__()\n    except AssertionError:\n        pass"""
tests/test_banknote_forest.py,1,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_banknote_forest():\n    import os\n\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import banknote\n\n    # Download the dataset \n    if(not os.path.exists('banknote.pkl')): banknote.get()\n\n    # Load heart data set\n    inputs_train, outputs_train, inputs_test, outputs_test = banknote.load()\n    \n    # Change 0/False to [1, 0]\n    # Change 1/True to [0, 1]\n    outputs_train = pk.onehot(outputs_train)\n    outputs_test = pk.onehot(outputs_test)\n\n    # Create model\n    ftypes = ['continues']*4\n    forest_banknote_classifier = pk.RandomForest(4, 2, max_depth=9, feature_type=ftypes)\n\n    # Train\n    forest_banknote_classifier.train(inputs_train, outputs_train)\n\n    # Save it\n    pk.save(forest_banknote_classifier, 'forest_banknote_classifier.pkl')\n\n    # Print accuracy\n    accuracy = forest_banknote_classifier.accuracy(inputs_train, outputs_train)\n    print('Train accuracy:', accuracy)\n    accuracy = forest_banknote_classifier.accuracy(inputs_test, outputs_test)\n    print('Test accuracy:', accuracy)\n\n    # Plot confusion matrix\n    forest_banknote_classifier.confusion_matrix(inputs_test, outputs_test, \n        gnames=['False', 'True'])\n\n    # Assert accuracy\n    assert (forest_banknote_classifier.accuracy(inputs_test, outputs_test)) >= 98\n\n@pktest_nograph\ndef test_predict_banknote_forest():\n    import os.path\n\n    import numpy as np\n    import pykitml as pk\n\n    # Predict banknote validity with variance, skewness, curtosis, entropy\n    # of -2.3, -9.3, 9.37, -0.86\n    input_data = np.array([-2.3, -9.3, 9.37, -0.86])\n\n    # Load the model\n    forest_banknote_classifier = pk.load('forest_banknote_classifier.pkl')\n\n    # Get output\n    forest_banknote_classifier.feed(input_data)\n    model_output = forest_banknote_classifier.get_output()\n\n    # Print result\n    print(model_output)  \n\nif __name__ == '__main__':\n    try:\n        test_banknote_forest.__wrapped__()\n        test_predict_banknote_forest.__wrapped__()\n    except AssertionError:\n        pass\n\n"""
tests/test_banknote_tree.py,0,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_banknote_tree():\n    import os\n\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import banknote\n\n    # Download the dataset \n    if(not os.path.exists('banknote.pkl')): banknote.get()\n\n    # Load heart data set\n    inputs_train, outputs_train, inputs_test, outputs_test = banknote.load()\n    \n    # Change 0/False to [1, 0]\n    # Change 1/True to [0, 1]\n    outputs_train = pk.onehot(outputs_train)\n    outputs_test = pk.onehot(outputs_test)\n\n    # Create model\n    ftypes = ['continues']*4\n    tree_banknote_classifier = pk.DecisionTree(4, 2, max_depth=6, feature_type=ftypes)\n\n    # Train\n    tree_banknote_classifier.train(inputs_train, outputs_train)\n\n    # Save it\n    pk.save(tree_banknote_classifier, 'tree_banknote_classifier.pkl')\n\n    # Print accuracy\n    accuracy = tree_banknote_classifier.accuracy(inputs_train, outputs_train)\n    print('Train accuracy:', accuracy)\n    accuracy = tree_banknote_classifier.accuracy(inputs_test, outputs_test)\n    print('Test accuracy:', accuracy)\n\n    # Plot confusion matrix\n    tree_banknote_classifier.confusion_matrix(inputs_test, outputs_test, \n        gnames=['False', 'True'])\n\n    # Plot descision tree\n    tree_banknote_classifier.show_tree()\n\n    # Assert accuracy\n    assert (tree_banknote_classifier.accuracy(inputs_test, outputs_test)) >= 98\n\nif __name__ == '__main__':\n    try:\n        test_banknote_tree.__wrapped__()\n    except AssertionError:\n        pass\n\n"""
tests/test_fashion.py,0,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\ndef test_download():\n    from pykitml.datasets import mnist\n    # Download the mnist data set\n    mnist.get(type='fashion')\n    # Test ran successfully\n    assert True\n\n@pktest_graph\ndef test_adam_fashion():\n    import os\n\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import mnist\n    \n    # If the dataset is not available then download it\n    if(not os.path.exists('mnist.pkl')): mnist.get(type='fashion')\n\n    # Load dataset\n    training_data, training_targets, testing_data, testing_targets = mnist.load()\n    \n    # Create a new neural network\n    fashion_classifier = pk.NeuralNetwork([784, 100, 10])\n    \n    # Train it\n    fashion_classifier.train(\n        training_data=training_data,\n        targets=training_targets, \n        batch_size=50, \n        epochs=1200, \n        optimizer=pk.Adam(learning_rate=0.012, decay_rate=0.95), \n        testing_data=testing_data, \n        testing_targets=testing_targets,\n        testing_freq=30,\n        decay_freq=10\n    )\n    \n    # Save it\n    pk.save(fashion_classifier, 'fashion_classifier_network.pkl')\n\n    # Show performance\n    accuracy = fashion_classifier.accuracy(training_data, training_targets)\n    print('Train Accuracy:', accuracy)        \n    accuracy = fashion_classifier.accuracy(testing_data, testing_targets)\n    print('Test Accuracy:', accuracy)\n\n    # Plot performance\n    fashion_classifier.plot_performance()\n\n    # Show confusion matrix\n    fashion_classifier.confusion_matrix(\n        training_data, training_targets,\n        gnames = ['T-shirt/Top', 'Trouser', 'Pullover',\n                'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker',\n                'Bag', 'Ankle Boot'\n            ]\n    )\n\n    # Assert if it has enough accuracy\n    assert fashion_classifier.accuracy(training_data, training_targets) > 84\n\nif __name__ == '__main__':\n    try:\n        test_adam_fashion.__wrapped__()\n    except AssertionError:\n        pass\n    """
tests/test_fishlength.py,1,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_fishlength():\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import fishlength\n\n    # Load the dataset\n    inputs, outputs = fishlength.load()\n\n    # Normalize inputs\n    array_min, array_max = pk.get_minmax(inputs)\n    inputs = pk.normalize_minmax(inputs, array_min, array_max)\n\n    # Create polynomial features\n    inputs_poly = pk.polynomial(inputs)\n\n    # Normalize outputs\n    array_min, array_max = pk.get_minmax(outputs)\n    outputs = pk.normalize_minmax(outputs, array_min, array_max)\n\n    # Create model\n    fish_classifier = pk.LinearRegression(inputs_poly.shape[1], 1)\n\n    # Train the model\n    fish_classifier.train(\n        training_data=inputs_poly,\n        targets=outputs, \n        batch_size=22, \n        epochs=1000, \n        optimizer=pk.Adam(learning_rate=0.02, decay_rate=0.99), \n        testing_freq=1,\n        decay_freq=10\n    )\n\n    # Save model\n    pk.save(fish_classifier, 'fish_classifier.pkl')\n\n    # Plot performance\n    fish_classifier.plot_performance()\n\n    # Assert if it has enough accuracy\n    assert fish_classifier.cost(inputs_poly, outputs) <= 0\n\n@pktest_nograph\ndef test_predict_fishlength():\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import fishlength\n\n    # Predict length of fish that is 28 days old at 25C\n\n    # Load the dataset\n    inputs, outputs = fishlength.load()\n\n    # Load the model\n    fish_classifier = pk.load('fish_classifier.pkl')\n    \n    # Normalize inputs\n    array_min, array_max = pk.get_minmax(inputs)\n    input_data = pk.normalize_minmax(np.array([28, 25]), array_min, array_max)\n\n    # Create plynomial features\n    input_data_poly = pk.polynomial(input_data)\n    \n    # Get output\n    fish_classifier.feed(input_data_poly)\n    model_output = fish_classifier.get_output()\n\n    # Denormalize output\n    array_min, array_max = pk.get_minmax(outputs)\n    model_output = pk.denormalize_minmax(model_output, array_min, array_max)\n\n    # Print result\n    print(model_output)\n\n\nif __name__ == '__main__':\n    try:\n        test_fishlength.__wrapped__()\n\n        test_predict_fishlength.__wrapped__()\n    except AssertionError:\n        pass\n\n"""
tests/test_functions.py,37,"b'import numpy as np\n\nfrom pykitml import _functions\n\neg_ws = np.array([[0.1, -0.2, 0.3], [-0.4, 0.5, -0.6]])\n\n# =============================\n# = Test activation functions =\n# =============================\n\ndef test_sigmoid():\n    expected_output = np.array([[0.52497919, 0.450166  , 0.57444252],\n       [0.40131234, 0.62245933, 0.35434369]])\n\n    assert np.allclose(_functions.sigmoid(eg_ws), expected_output)\n\ndef test_tanh():\n    expected_output = np.array([[ 0.09966799, -0.19737532,  0.29131261],\n       [-0.37994896,  0.46211716, -0.53704957]])\n\n    assert np.allclose(_functions.tanh(eg_ws), expected_output)\n\ndef test_leakyrelu():\n    expected_output = np.array([[ 0.1  , -0.002,  0.3  ],\n       [-0.004,  0.5  , -0.006]])\n\n    assert np.allclose(_functions.leakyrelu(eg_ws), expected_output)\n\ndef test_relu():\n   expected_output = np.array([[0.1, 0, 0.3], [0, 0.5, 0]])\n\n   assert np.allclose(_functions.relu(eg_ws), expected_output)\n\ndef test_softmax():\n    expected_output = np.array([[0.33758454, 0.25008878, 0.41232669],\n       [0.23373585, 0.57489742, 0.19136673]])\n\n    assert np.allclose(_functions.softmax(eg_ws), expected_output)\n\n# ===========================================\n# = Test derivative of activation functions =\n# ===========================================\n\ndef test_sigmoid_prime():\n    activ = _functions.sigmoid(eg_ws)\n\n    expected_output = np.array([[0.24937604, 0.24751657, 0.24445831],\n       [0.24026075, 0.23500371, 0.22878424]])\n\n    assert np.allclose(_functions.sigmoid_prime(eg_ws, activ), expected_output)\n\ndef test_tanh_prime():\n    activ = _functions.tanh(eg_ws)\n\n    expected_output = np.array([[0.99006629, 0.96104298, 0.91513696],\n       [0.85563879, 0.78644773, 0.71157776]])\n\n    assert np.allclose(_functions.tanh_prime(eg_ws, activ), expected_output)\n\ndef test_leakyrelu_prime():\n    activ = _functions.leakyrelu(eg_ws)\n\n    expected_output = np.array([[1.  , 0.01, 1.  ],\n       [0.01, 1.  , 0.01]])\n\n    assert np.allclose(_functions.leakyrelu_prime(eg_ws, activ), expected_output)\n\ndef test_relu_prime():\n    activ = _functions.relu(eg_ws)\n\n    expected_output = np.array([[1, 0, 1], [0, 1, 0]])\n\n    assert np.allclose(_functions.relu_prime(eg_ws, activ), expected_output)\n\ndef test_softmax_prime():\n    activ = _functions.leakyrelu(eg_ws)\n\n    expected_output = np.array([[ 0.09    , -0.002004,  0.21    ],\n       [-0.004016,  0.25    , -0.006036]])\n\n    assert np.allclose(_functions.softmax_prime(eg_ws, activ), expected_output)\n\n# =======================\n# = Test cost functions =\n# =======================\n\ndef test_mse():\n   eg_output = np.array([0.1, 0.4, -0.1, 0.3])\n   eg_target = np.array([0.2, 0.3, -0.5, 0.2])\n   expected_output = np.array([0.005, 0.005, 0.08 , 0.005])\n\n   assert np.allclose(_functions.mse(eg_output, eg_target), expected_output)\n\ndef test_cross_entropy():\n   eg_output = np.array([0.3, 0.1, 0.9, 0.7])\n   eg_target = np.array([1, 0, 1, 1])\n   expected_output = np.array([1.2039728 , 0.10536052, 0.10536052, 0.35667494])\n\n   assert np.allclose(_functions.cross_entropy(eg_output, eg_target), expected_output)\n\n# =====================================\n# = Test derivative of cost functions =\n# =====================================\n\ndef test_mse_prime():\n   eg_output = np.array([0.1, 0.4, -0.1, 0.3])\n   eg_target = np.array([0.2, 0.3, -0.5, 0.2])\n   expected_output = np.array([-0.1, 0.1, 0.4, 0.1])\n\n   assert np.allclose(_functions.mse_prime(eg_output, eg_target), expected_output)\n\ndef test_cross_entropy_prime():\n   eg_output = np.array([0.3, 0.1, 0.9, 0.7])\n   eg_target = np.array([1, 0, 1, 1])\n   expected_output = np.array([-3.33333333,  1.11111111, -1.11111111, -1.42857143])\n\n   assert np.allclose(_functions.cross_entropy_prime(eg_output, eg_target), expected_output)\n\n'"
tests/test_heart.py,0,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_heart():\n    import os.path\n\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import heartdisease\n\n    # Download the dataset \n    if(not os.path.exists('heartdisease.pkl')): heartdisease.get()\n\n    # Load heartdisease data set\n    inputs, outputs = heartdisease.load()\n\n    # Normalize inputs in the dataset\n    inputs_min, inputs_max = pk.get_minmax(inputs)\n    inputs = pk.normalize_minmax(inputs, inputs_min, inputs_max, cols=[0, 3, 4, 7, 9])  \n\n    # Change categorical values to onehot values\n    inputs = pk.onehot_cols(inputs, [1, 2, 5, 6, 8, 10, 11, 12])      \n\n    # Create model\n    heart_classifier = pk.LogisticRegression(35, 1)\n\n    # Train the model\n    heart_classifier.train(\n        training_data=inputs,\n        targets=outputs, \n        batch_size=10, \n        epochs=1500, \n        optimizer=pk.Adam(learning_rate=0.015, decay_rate=0.99), \n        testing_freq=30,\n        decay_freq=40\n    )\n\n    # Save it\n    pk.save(heart_classifier, 'heart_classifier.pkl') \n\n    # Print accuracy and plot performance\n    heart_classifier.plot_performance()\n    accuracy = heart_classifier.accuracy(inputs, outputs)\n    print('Accuracy:', accuracy)\n\n    # Plot confusion matrix\n    heart_classifier.confusion_matrix(inputs, outputs)\n\n    # Assert if it has enough accuracy\n    assert heart_classifier.accuracy(inputs, outputs) >= 87\n\nif __name__ == '__main__':\n    try:\n        test_heart.__wrapped__()\n    except AssertionError:\n        pass"""
tests/test_heart_bayes.py,1,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_heart_bayes():\n    import os.path\n\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import heartdisease\n\n    # Download the dataset \n    if(not os.path.exists('heartdisease.pkl')): heartdisease.get()\n\n    # Load heart data set\n    inputs, outputs = heartdisease.load()\n\n    # Change 0/False to [1, 0]\n    # Change 1/True to [0, 1]\n    outputs = pk.onehot(outputs)\n\n    distrbutions = [\n        'gaussian', 'binomial', 'multinomial',\n        'gaussian', 'gaussian', 'binomial', 'multinomial',\n        'gaussian', 'binomial', 'gaussian', 'multinomial',\n        'multinomial', 'multinomial'\n    ]\n\n    # Create model\n    bayes_heart_classifier = pk.NaiveBayes(13, 2, distrbutions)\n\n    # Train\n    bayes_heart_classifier.train(inputs, outputs)\n\n    # Save it\n    pk.save(bayes_heart_classifier, 'bayes_heart_classifier.pkl')\n\n    # Print accuracy\n    accuracy = bayes_heart_classifier.accuracy(inputs, outputs)\n    print('Accuracy:', accuracy)\n\n    # Plot confusion matrix\n    bayes_heart_classifier.confusion_matrix(inputs, outputs, \n        gnames=['False', 'True'])\n\n    # Assert accuracy\n    assert (bayes_heart_classifier.accuracy(inputs, outputs)) > 84\n\n@pktest_nograph\ndef test_predict_heart_bayes():\n    import os.path\n\n    import numpy as np\n    import pykitml as pk\n\n    # Predict heartdisease for a person with\n    # age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal\n    # 67, 1, 4, 160, 286, 0, 2, 108, 1, 1.5, 2, 3, 3 \n    input_data = np.array([67, 1, 4, 160, 286, 0, 2, 108, 1, 1.5, 2, 3, 3], dtype=float)\n\n    # Load the model\n    bayes_heart_classifier = pk.load('bayes_heart_classifier.pkl')\n\n    # Get output\n    bayes_heart_classifier.feed(input_data)\n    model_output = bayes_heart_classifier.get_output()\n\n    # Print result (log of probabilities)\n    print(model_output)\n\nif __name__ == '__main__':\n    # Train\n    try:\n        test_heart_bayes.__wrapped__()\n        test_predict_heart_bayes.__wrapped__()\n    except AssertionError:\n        pass"""
tests/test_heart_forest.py,1,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_heart_forest():\n    import os.path\n    \n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import heartdisease\n\n    # Download the dataset \n    if(not os.path.exists('heartdisease.pkl')): heartdisease.get()\n\n    # Load heart data set\n    inputs, outputs = heartdisease.load()\n    outputs = pk.onehot(outputs)\n\n    # Create model\n    ftypes = [\n        'continues', 'categorical', 'categorical',\n        'continues', 'continues', 'categorical', 'categorical',\n        'continues', 'categorical', 'continues', 'categorical',\n        'categorical', 'categorical'\n    ]\n    forest_heart_classifier = pk.RandomForest(13, 2, max_depth=8, feature_type=ftypes)\n\n    # Train\n    forest_heart_classifier.train(inputs, outputs)\n\n    # Save it\n    pk.save(forest_heart_classifier, 'forest_heart_classifier.pkl')\n\n    # Print accuracy\n    accuracy = forest_heart_classifier.accuracy(inputs, outputs)\n    print('Accuracy:', accuracy)\n\n    # Plot confusion matrix\n    forest_heart_classifier.confusion_matrix(inputs, outputs, \n        gnames=['False', 'True'])\n\n    # Assert accuracy\n    assert (forest_heart_classifier.accuracy(inputs, outputs)) >= 94\n\n@pktest_nograph\ndef test_predict_heart_forest():\n    import os.path\n\n    import numpy as np\n    import pykitml as pk\n\n    # Predict heartdisease for a person with\n    # age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal\n    # 67, 1, 4, 160, 286, 0, 2, 108, 1, 1.5, 2, 3, 3 \n    input_data = np.array([67, 1, 4, 160, 286, 0, 2, 108, 1, 1.5, 2, 3, 3], dtype=float)\n\n    # Load the model\n    forest_heart_classifier = pk.load('forest_heart_classifier.pkl')\n\n    # Get output\n    forest_heart_classifier.feed(input_data)\n    model_output = forest_heart_classifier.get_output()\n\n    # Print result (log of probabilities)\n    print(model_output)\n\nif __name__ == '__main__':\n    try:\n        test_heart_forest.__wrapped__()\n        test_predict_heart_forest.__wrapped__()\n    except AssertionError:\n        pass"""
tests/test_heart_tree.py,0,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_heart_tree():\n    import os.path\n    \n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import heartdisease\n\n    # Download the dataset \n    if(not os.path.exists('heartdisease.pkl')): heartdisease.get()\n\n    # Load heart data set\n    inputs, outputs = heartdisease.load()\n    outputs = pk.onehot(outputs)\n\n    # Create model\n    ftypes = [\n        'continues', 'categorical', 'categorical',\n        'continues', 'continues', 'categorical', 'categorical',\n        'continues', 'categorical', 'continues', 'categorical',\n        'categorical', 'categorical'\n    ]\n    tree_heart_classifier = pk.DecisionTree(13, 2, max_depth=6, feature_type=ftypes)\n\n    # Train\n    tree_heart_classifier.train(inputs, outputs)\n\n    # Save it\n    pk.save(tree_heart_classifier, 'tree_heart_classifier.pkl')\n\n    # Print accuracy\n    accuracy = tree_heart_classifier.accuracy(inputs, outputs)\n    print('Accuracy:', accuracy)\n\n    # Plot confusion matrix\n    tree_heart_classifier.confusion_matrix(inputs, outputs, \n        gnames=['False', 'True'])\n\n    # Plot descision tree\n    tree_heart_classifier.show_tree()\n\n    # Assert accuracy\n    assert (tree_heart_classifier.accuracy(inputs, outputs)) >= 94\n\n\nif __name__ == '__main__':\n    try:\n        test_heart_tree.__wrapped__()\n    except AssertionError:\n        pass"""
tests/test_iris.py,0,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_iris():\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import iris\n\n    # Load iris data set\n    inputs_train, outputs_train, inputs_test, outputs_test = iris.load()\n\n    # Normalize inputs in the dataset\n    inputs_min, inputs_max = pk.get_minmax(inputs_train)\n    inputs_train = pk.normalize_minmax(inputs_train, inputs_min, inputs_max)\n    inputs_test = pk.normalize_minmax(inputs_test, inputs_min, inputs_max)\n\n    # Create model\n    iris_classifier = pk.LogisticRegression(4, 3)\n\n    # Train the model\n    iris_classifier.train(\n        training_data=inputs_train,\n        targets=outputs_train, \n        batch_size=10, \n        epochs=1500, \n        optimizer=pk.Adam(learning_rate=0.4, decay_rate=0.99), \n        testing_data=inputs_test,\n        testing_targets=outputs_test,\n        testing_freq=30,\n        decay_freq=20\n    )\n\n    # Save it\n    pk.save(iris_classifier, 'iris_classifier.pkl') \n\n    # Print accuracy\n    accuracy = iris_classifier.accuracy(inputs_train, outputs_train)\n    print('Train accuracy:', accuracy)\n    accuracy = iris_classifier.accuracy(inputs_test, outputs_test)\n    print('Test accuracy:', accuracy)\n\n    # Plot performance\n    iris_classifier.plot_performance()\n\n    # Plot confusion matrix\n    iris_classifier.confusion_matrix(inputs_test, outputs_test, \n        gnames=['Setosa', 'Versicolor', 'Virginica'])\n\n    # Assert if it has enough accuracy\n    assert iris_classifier.accuracy(inputs_train, outputs_train) >= 98\n\nif __name__ == '__main__':\n    try:\n        test_iris.__wrapped__()\n    except AssertionError:\n        pass"""
tests/test_iris_bayes.py,1,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_iris_bayes():\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import iris\n\n    # Load iris data set\n    inputs_train, outputs_train, inputs_test, outputs_test = iris.load()\n\n    # Create model\n    bayes_iris_classifier = pk.GaussianNaiveBayes(4, 3)\n\n    # Train\n    bayes_iris_classifier.train(inputs_train, outputs_train)\n\n    # Save it\n    pk.save(bayes_iris_classifier, 'bayes_iris_classifier.pkl')\n\n    # Print accuracy\n    accuracy = bayes_iris_classifier.accuracy(inputs_train, outputs_train)\n    print('Train accuracy:', accuracy)\n    accuracy = bayes_iris_classifier.accuracy(inputs_test, outputs_test)\n    print('Test accuracy:', accuracy)\n\n    # Plot confusion matrix\n    bayes_iris_classifier.confusion_matrix(inputs_test, outputs_test, \n        gnames=['Setosa', 'Versicolor', 'Virginica'])\n\n    # Assert accuracy\n    assert (bayes_iris_classifier.accuracy(inputs_train, outputs_train)) >= 95\n\n@pktest_nograph\ndef test_predict_iris_bayes():\n    import numpy as np\n    import pykitml as pk\n\n    # Predict type of species with \n    # sepal-length sepal-width petal-length petal-width\n    # 5.8, 2.7, 3.9, 1.2\n    input_data = np.array([5.8, 2.7, 3.9, 1.2])\n\n    # Load the model\n    bayes_iris_classifier = pk.load('bayes_iris_classifier.pkl')\n\n    # Get output\n    bayes_iris_classifier.feed(input_data)\n    model_output = bayes_iris_classifier.get_output_onehot()\n\n    # Print result\n    print(model_output)\n\nif __name__ == '__main__':\n    try:\n        test_iris_bayes.__wrapped__()\n        test_predict_iris_bayes.__wrapped__()\n    except AssertionError:\n        pass\n"""
tests/test_iris_neighbor.py,1,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_iris_neighbor():\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import iris\n\n    # Load iris data set\n    inputs_train, outputs_train, inputs_test, outputs_test = iris.load()\n\n    # Create model\n    neighbor_iris_classifier = pk.NearestNeighbor(4, 3)\n\n    # Train the model\n    neighbor_iris_classifier.train(\n        training_data=inputs_train,\n        targets=outputs_train, \n    )\n\n    # Save it\n    pk.save(neighbor_iris_classifier, 'neighbor_iris_classifier.pkl') \n\n    # Print accuracy\n    accuracy = neighbor_iris_classifier.accuracy(inputs_train, outputs_train)\n    print('Train accuracy:', accuracy)\n    accuracy = neighbor_iris_classifier.accuracy(inputs_test, outputs_test)\n    print('Test accuracy:', accuracy)\n\n    # Plot confusion matrix\n    neighbor_iris_classifier.confusion_matrix(inputs_test, outputs_test, \n        gnames=['Setosa', 'Versicolor', 'Virginica'])\n\n    # Assert if it has enough accuracy\n    assert neighbor_iris_classifier.accuracy(inputs_train, outputs_train) >= 100\n\n@pktest_nograph\ndef test_predict_iris_neighbor():\n    import numpy as np\n    import pykitml as pk\n\n    # Predict type of species with \n    # sepal-length sepal-width petal-length petal-width\n    # 5.8, 2.7, 3.9, 1.2\n    input_data = np.array([5.8, 2.7, 3.9, 1.2])\n\n    # Load the model\n    neighbor_iris_classifier = pk.load('neighbor_iris_classifier.pkl')\n\n    # Get output\n    neighbor_iris_classifier.feed(input_data)\n    model_output = neighbor_iris_classifier.get_output_onehot()\n\n    # Print result\n    print(model_output)\n\nif __name__ == '__main__':\n    try:\n        test_iris_neighbor.__wrapped__()\n        test_predict_iris_neighbor.__wrapped__()\n    except AssertionError:\n        pass"""
tests/test_iris_svm.py,3,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_iris_svm():\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import iris\n\n    # Load iris data set\n    inputs_train, outputs_train, inputs_test, outputs_test = iris.load()\n\n    # Format the outputs for svm training, zeros to -1\n    svm_outputs_train = np.where(outputs_train==0, -1, 1)\n    svm_outputs_test = np.where(outputs_test==0, -1, 1)\n\n    # Create model\n    svm_iris_classifier = pk.SVM(4, 3)\n\n    # Train the model\n    svm_iris_classifier.train(\n        training_data=inputs_train,\n        targets=svm_outputs_train, \n        batch_size=20, \n        epochs=1000, \n        optimizer=pk.Adam(learning_rate=3, decay_rate=0.95),\n        testing_data=inputs_test,\n        testing_targets=svm_outputs_test, \n        testing_freq=30,\n        decay_freq=10\n    )\n\n    # Save it\n    pk.save(svm_iris_classifier, 'svm_iris_classifier.pkl')\n\n    # Print accuracy\n    accuracy = svm_iris_classifier.accuracy(inputs_train, outputs_train)\n    print('Train accuracy:', accuracy)\n    accuracy = svm_iris_classifier.accuracy(inputs_test, outputs_test)\n    print('Test accuracy:', accuracy)\n\n    # Plot performance\n    svm_iris_classifier.plot_performance()\n\n    # Plot confusion matrix\n    svm_iris_classifier.confusion_matrix(inputs_test, outputs_test, \n        gnames=['Setosa', 'Versicolor', 'Virginica'])\n\n    # Assert if it has enough accuracy\n    assert svm_iris_classifier.accuracy(inputs_train, outputs_train) >= 97\n\n@pktest_nograph\ndef test_predict_iris_svm():\n    import numpy as np\n    import pykitml as pk\n\n    # Predict type of species with \n    # sepal-length sepal-width petal-length petal-width\n    # 5.8, 2.7, 3.9, 1.2\n    input_data = np.array([5.8, 2.7, 3.9, 1.2])\n\n    # Load the model\n    svm_iris_classifier = pk.load('svm_iris_classifier.pkl')\n\n    # Get output\n    svm_iris_classifier.feed(input_data)\n    model_output = svm_iris_classifier.get_output_onehot()\n\n    # Print result\n    print(model_output)\n\nif __name__ == '__main__':\n    try:\n        test_iris_svm.__wrapped__()\n        test_predict_iris_svm.__wrapped__()\n    except AssertionError:\n        pass"""
tests/test_iris_tree.py,1,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_iris_tree():\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import iris\n\n    # Load iris data set\n    inputs_train, outputs_train, inputs_test, outputs_test = iris.load()\n\n    # Create model\n    tree_iris_classifier = pk.DecisionTree(4, 3, max_depth=4, feature_type=['continues']*4)\n\n    # Train\n    tree_iris_classifier.train(inputs_train, outputs_train)\n\n    # Save it\n    pk.save(tree_iris_classifier, 'tree_iris_classifier.pkl')\n\n    # Print accuracy\n    accuracy = tree_iris_classifier.accuracy(inputs_train, outputs_train)\n    print('Train accuracy:', accuracy)\n    accuracy = tree_iris_classifier.accuracy(inputs_test, outputs_test)\n    print('Test accuracy:', accuracy)\n\n    # Plot confusion matrix\n    tree_iris_classifier.confusion_matrix(inputs_test, outputs_test, \n        gnames=['Setosa', 'Versicolor', 'Virginica'])\n\n    # Plot decision tree\n    tree_iris_classifier.show_tree()\n\n    # Assert accuracy\n    assert (tree_iris_classifier.accuracy(inputs_train, outputs_train)) >= 98\n\n@pktest_nograph\ndef test_predict_iris_tree():\n    import numpy as np\n    import pykitml as pk\n\n    # Predict type of species with \n    # sepal-length sepal-width petal-length petal-width\n    # 5.8, 2.7, 3.9, 1.2\n    input_data = np.array([5.8, 2.7, 3.9, 1.2])\n\n    # Load the model\n    tree_iris_classifier = pk.load('tree_iris_classifier.pkl')\n\n    # Get output\n    tree_iris_classifier.feed(input_data)\n    model_output = tree_iris_classifier.get_output_onehot()\n\n    # Print result\n    print(model_output)\n\nif __name__ == '__main__':\n    try:\n        test_iris_tree.__wrapped__()\n        test_predict_iris_tree.__wrapped__()\n    except AssertionError:\n        pass"""
tests/test_mnist.py,0,"b""import sys\nimport os.path\n\nimport numpy as np\nimport pykitml as pk\nfrom pykitml.datasets import mnist\nfrom pykitml.testing import pktest_graph, pktest_nograph\n\ndef test_download():\n    # Download the mnist data set\n    mnist.get()\n    # Test ran successfully\n    assert True\n\n@pktest_graph\ndef test_adagrad():\n    # Load dataset\n    training_data, training_targets, testing_data, testing_targets = mnist.load()\n    \n    # Create a new neural network\n    digit_classifier = pk.NeuralNetwork([784, 100, 10])\n\n    # Train it\n    digit_classifier.train(\n        training_data=training_data,\n        targets=training_targets, \n        batch_size=50, \n        epochs=1200, \n        optimizer=pk.Adagrad(learning_rate=0.07, decay_rate=0.99), \n        testing_data=testing_data, \n        testing_targets=testing_targets,\n        testing_freq=30,\n        decay_freq=10\n    )\n    \n    # Save it\n    pk.save(digit_classifier, 'digit_classifier_network.pkl')\n\n    # Show performance\n    accuracy = digit_classifier.accuracy(training_data, training_targets)\n    print('Train Accuracy:', accuracy)        \n    accuracy = digit_classifier.accuracy(testing_data, testing_targets)\n    print('Test Accuracy:', accuracy)\n    \n    # Plot performance graph\n    digit_classifier.plot_performance()\n\n    # Show confusion matrix\n    digit_classifier.confusion_matrix(training_data, training_targets)\n\n    # Assert if it has enough accuracy\n    assert digit_classifier.accuracy(training_data, training_targets) > 94\n\n@pktest_graph\ndef test_nesterov():\n    # Load dataset\n    training_data, training_targets, testing_data, testing_targets = mnist.load()\n    \n    # Create a new neural network\n    digit_classifier = pk.NeuralNetwork([784, 100, 10])\n\n    # Train it\n    digit_classifier.train(\n        training_data=training_data,\n        targets=training_targets, \n        batch_size=50, \n        epochs=1200, \n        optimizer=pk.Nesterov(learning_rate=0.1, decay_rate=0.99), \n        testing_data=testing_data, \n        testing_targets=testing_targets,\n        testing_freq=30,\n        decay_freq=10\n    )\n    \n    # Save it\n    pk.save(digit_classifier, 'digit_classifier_network.pkl')\n\n    # Show performance\n    accuracy = digit_classifier.accuracy(training_data, training_targets)\n    print('Train Accuracy:', accuracy)        \n    accuracy = digit_classifier.accuracy(testing_data, testing_targets)\n    print('Test Accuracy:', accuracy)\n    \n    # Plot performance graph\n    digit_classifier.plot_performance()\n\n    # Show confusion matrix\n    digit_classifier.confusion_matrix(training_data, training_targets)\n\n    # Assert if it has enough accuracy\n    assert digit_classifier.accuracy(training_data, training_targets) > 94\n\n@pktest_graph\ndef test_relu_nesterov():\n    # Load dataset\n    training_data, training_targets, testing_data, testing_targets = mnist.load()\n    \n    # Create a new neural network\n    digit_classifier = pk.NeuralNetwork([784, 100, 10], config='relu-softmax-cross_entropy')\n\n    # Train it\n    digit_classifier.train(\n        training_data=training_data,\n        targets=training_targets, \n        batch_size=50, \n        epochs=1200, \n        optimizer=pk.Nesterov(learning_rate=0.1, decay_rate=0.99), \n        testing_data=testing_data, \n        testing_targets=testing_targets,\n        testing_freq=30,\n        decay_freq=10\n    )\n    \n    # Save it\n    pk.save(digit_classifier, 'digit_classifier_network.pkl')\n\n    # Show performance\n    accuracy = digit_classifier.accuracy(training_data, training_targets)\n    print('Train Accuracy:', accuracy)        \n    accuracy = digit_classifier.accuracy(testing_data, testing_targets)\n    print('Test Accuracy:', accuracy)\n    \n    # Plot performance graph\n    digit_classifier.plot_performance()\n\n    # Show confusion matrix\n    digit_classifier.confusion_matrix(training_data, training_targets)\n\n    # Assert if it has enough accuracy\n    assert digit_classifier.accuracy(training_data, training_targets) > 94\n\n@pktest_graph\ndef test_momentum():\n    # Load dataset\n    training_data, training_targets, testing_data, testing_targets = mnist.load()\n    \n    # Create a new neural network\n    digit_classifier = pk.NeuralNetwork([784, 100, 10])\n    \n    # Train it\n    digit_classifier.train(\n        training_data=training_data,\n        targets=training_targets, \n        batch_size=50, \n        epochs=1200, \n        optimizer=pk.Momentum(learning_rate=0.1, decay_rate=0.95), \n        testing_data=testing_data, \n        testing_targets=testing_targets,\n        testing_freq=30,\n        decay_freq=20\n    )\n    \n    # Save it\n    pk.save(digit_classifier, 'digit_classifier_network.pkl')\n\n    # Show performance\n    accuracy = digit_classifier.accuracy(training_data, training_targets)\n    print('Train Accuracy:', accuracy)        \n    accuracy = digit_classifier.accuracy(testing_data, testing_targets)\n    print('Test Accuracy:', accuracy)\n    \n    # Plot performance graph\n    digit_classifier.plot_performance()\n\n    # Show confusion matrix\n    digit_classifier.confusion_matrix(training_data, training_targets)\n\n    # Assert if it has enough accuracy\n    assert digit_classifier.accuracy(training_data, training_targets) > 94\n\n@pktest_graph\ndef test_gradient_descent():\n    # Load dataset\n    training_data, training_targets, testing_data, testing_targets = mnist.load()\n    \n    # Create a new neural network\n    digit_classifier = pk.NeuralNetwork([784, 100, 10])\n    \n    # Train it\n    digit_classifier.train(\n        training_data=training_data,\n        targets=training_targets, \n        batch_size=50, \n        epochs=1200, \n        optimizer=pk.GradientDescent(learning_rate=0.2, decay_rate=0.99), \n        testing_data=testing_data, \n        testing_targets=testing_targets,\n        testing_freq=30,\n        decay_freq=20\n    )\n    \n    # Save it\n    pk.save(digit_classifier, 'digit_classifier_network.pkl')\n\n    # Show performance\n    accuracy = digit_classifier.accuracy(training_data, training_targets)\n    print('Train Accuracy:', accuracy)        \n    accuracy = digit_classifier.accuracy(testing_data, testing_targets)\n    print('Test Accuracy:', accuracy)\n    \n    # Plot performance graph\n    digit_classifier.plot_performance()\n\n    # Show confusion matrix\n    digit_classifier.confusion_matrix(training_data, training_targets)\n\n    # Assert if it has enough accuracy\n    assert digit_classifier.accuracy(training_data, training_targets) > 92\n\n@pktest_graph\ndef test_RMSprop():\n    # Load dataset\n    training_data, training_targets, testing_data, testing_targets = mnist.load()\n    \n    # Create a new neural network\n    digit_classifier = pk.NeuralNetwork([784, 100, 10])\n    \n    # Train it\n    digit_classifier.train(\n        training_data=training_data,\n        targets=training_targets, \n        batch_size=50, \n        epochs=1200, \n        optimizer=pk.RMSprop(learning_rate=0.012, decay_rate=0.95), \n        testing_data=testing_data, \n        testing_targets=testing_targets,\n        testing_freq=30,\n        decay_freq=15\n    )\n    \n    # Save it\n    pk.save(digit_classifier, 'digit_classifier_network.pkl')\n\n    # Show performance\n    accuracy = digit_classifier.accuracy(training_data, training_targets)\n    print('Train Accuracy:', accuracy)        \n    accuracy = digit_classifier.accuracy(testing_data, testing_targets)\n    print('Test Accuracy:', accuracy)\n    \n    # Plot performance graph\n    digit_classifier.plot_performance()\n\n    # Show confusion matrix\n    digit_classifier.confusion_matrix(training_data, training_targets)\n\n    # Assert if it has enough accuracy\n    assert digit_classifier.accuracy(training_data, training_targets) > 95\n\n@pktest_graph\ndef test_adam():\n    import os.path\n\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import mnist\n    \n    # Download dataset\n    if(not os.path.exists('mnist.pkl')): mnist.get()\n\n    # Load dataset\n    training_data, training_targets, testing_data, testing_targets = mnist.load()\n    \n    # Create a new neural network\n    digit_classifier = pk.NeuralNetwork([784, 100, 10])\n    \n    # Train it\n    digit_classifier.train(\n        training_data=training_data,\n        targets=training_targets, \n        batch_size=50, \n        epochs=1200, \n        optimizer=pk.Adam(learning_rate=0.012, decay_rate=0.95), \n        testing_data=testing_data, \n        testing_targets=testing_targets,\n        testing_freq=30,\n        decay_freq=15\n    )\n    \n    # Save it\n    pk.save(digit_classifier, 'digit_classifier_network.pkl')\n\n    # Show performance\n    accuracy = digit_classifier.accuracy(training_data, training_targets)\n    print('Train Accuracy:', accuracy)        \n    accuracy = digit_classifier.accuracy(testing_data, testing_targets)\n    print('Test Accuracy:', accuracy)\n    \n    # Plot performance graph\n    digit_classifier.plot_performance()\n\n    # Show confusion matrix\n    digit_classifier.confusion_matrix(training_data, training_targets)\n\n    # Assert if it has enough accuracy\n    assert digit_classifier.accuracy(training_data, training_targets) > 95\n\n@pktest_graph\ndef test_predict_mnist_adam():\n    import random\n\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pykitml as pk\n    from pykitml.datasets import mnist\n\n    # Load dataset\n    training_data, training_targets, testing_data, testing_targets = mnist.load()\n\n    # Load the trained network\n    digit_classifier = pk.load('digit_classifier_network.pkl')\n\n    # Pick a random example from testing data\n    index = random.randint(0, 9999)\n\n    # Show the test data and the label\n    plt.imshow(training_data[index].reshape(28, 28))\n    plt.show()\n    print('Label: ', training_targets[index])\n\n    # Show prediction\n    digit_classifier.feed(training_data[index])\n    model_output = digit_classifier.get_output_onehot()\n    print('Predicted: ', model_output)\n\nif __name__ == '__main__':\n    # List of optimizers\n    optimizers = [\n        'gradient_descent', 'momentum', 'nesterov',\n        'adagrad', 'RMSprop', 'adam' \n    ]\n    # Check if arguments passed to the script is correct\n    if(len(sys.argv) != 2 or sys.argv[1] not in optimizers):\n        print('Usage: python3 test_mnist.py OPTIMIZER')\n        print('List of available optimizers:')\n        print(str(optimizers))\n        exit()\n    \n    # If the dataset is not available then download it\n    if(not os.path.exists('mnist.pkl')): mnist.get()\n\n    # Run the requested optimizer test function\n    try:\n        locals()['test_'+sys.argv[1]].__wrapped__()\n        test_predict_mnist_adam.__wrapped__()\n    except AssertionError:\n        pass\n"""
tests/test_mnist_svm.py,2,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_mnist_svm():\n    import os.path\n\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import mnist\n    \n    # Download dataset\n    if(not os.path.exists('mnist.pkl')): mnist.get()\n\n    # Load mnist data set\n    inputs_train, outputs_train, inputs_test, outputs_test = mnist.load()\n\n    # Train on only first 10000\n    inputs_train = inputs_train[:10000]\n    outputs_train = outputs_train[:10000]\n\n    # Transform inputs using gaussian kernal\n    sigma = 3.15\n    gaussian_inputs_train = pk.gaussian_kernel(inputs_train, inputs_train, sigma)\n    gaussian_inputs_test = pk.gaussian_kernel(inputs_test, inputs_train, sigma)\n\n    # Format the outputs for svm training, zeros to -1\n    svm_outputs_train = np.where(outputs_train==0, -1, 1)\n    svm_outputs_test = np.where(outputs_test==0, -1, 1)\n\n    # Create model\n    svm_mnist_classifier = pk.SVM(gaussian_inputs_train.shape[1], 10)\n\n    # Train the model\n    svm_mnist_classifier.train(\n        training_data=gaussian_inputs_train,\n        targets=svm_outputs_train, \n        batch_size=20, \n        epochs=1000, \n        optimizer=pk.Adam(learning_rate=3.5, decay_rate=0.95),\n        testing_data=gaussian_inputs_test,\n        testing_targets=svm_outputs_test, \n        testing_freq=30,\n        decay_freq=10\n    )\n\n    # Save it\n    pk.save(svm_mnist_classifier, 'svm_mnist_classifier.pkl')\n\n    # Print accuracy\n    accuracy = svm_mnist_classifier.accuracy(gaussian_inputs_train, outputs_train)\n    print('Train accuracy:', accuracy)\n    accuracy = svm_mnist_classifier.accuracy(gaussian_inputs_test, outputs_test)\n    print('Test accuracy:', accuracy)\n\n    # Plot performance\n    svm_mnist_classifier.plot_performance()\n\n    # Plot confusion matrix\n    svm_mnist_classifier.confusion_matrix(gaussian_inputs_test, outputs_test)\n\n    # Assert if it has enough accuracy\n    assert svm_mnist_classifier.accuracy(gaussian_inputs_train, outputs_train) >= 90\n\n@pktest_graph\ndef test_predict_mnist_svm():\n    import random\n\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pykitml as pk\n    from pykitml.datasets import mnist\n\n    # Load dataset\n    inputs_train, outputs_train, inputs_test, outputs_test = mnist.load()\n\n    # Use only first 10000\n    inputs_train = inputs_train[:10000]\n    outputs_train = outputs_train[:10000]\n\n    # Load the trained network\n    svm_mnist_classifier = pk.load('svm_mnist_classifier.pkl')\n\n    # Pick a random example from testing data\n    index = random.randint(0, 9000)\n\n    # Show the test data and the label\n    plt.imshow(inputs_train[index].reshape(28, 28))\n    plt.show()\n    print('Label: ', outputs_train[index])\n\n    # Transform the input\n    input_data = pk.gaussian_kernel(inputs_train[index], inputs_train)\n\n    # Show prediction\n    svm_mnist_classifier.feed(input_data)\n    model_output = svm_mnist_classifier.get_output_onehot()\n    print('Predicted: ', model_output)\n\nif __name__ == '__main__':\n    try:\n        test_mnist_svm.__wrapped__()\n        test_predict_mnist_svm.__wrapped__()\n    except AssertionError:\n        pass"""
tests/test_normalization.py,5,"b'# ================================================\n# = Test normalization/feature-scaling functions = \n# ================================================\n\nimport numpy as np\n\nimport pykitml as pk\n\neg_array = np.array([\n    [0.1,   0.3434, 1.3434, 3],\n    [1.2,   4.54,   6.7,    3.456],\n    [5.678, 2.345,  2.453,  8.345],\n    [2.3,   6.2,    8.3,    1.2]\n])\n\ndef test_minmax():\n    expected_output = (np.array([0.1   , 0.3434, 1.3434, 1.2   ]), \n                    np.array([5.678, 6.2  , 8.3  , 8.345]))\n\n    assert np.allclose(pk.get_minmax(eg_array), expected_output)\n\ndef test_normalize():\n    array_min, array_max = pk.get_minmax(eg_array)\n\n    norm_array = pk.normalize_minmax(eg_array, array_min, array_max)\n    denorm_array = pk.denormalize_minmax(norm_array, array_min, array_max)\n\n    assert np.allclose(denorm_array, eg_array)'"
tests/test_pca.py,0,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_pca_compression():\n    import os.path\n    import random\n\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pykitml as pk\n    from pykitml.datasets import mnist\n    \n    # Download dataset\n    if(not os.path.exists('mnist.pkl')): mnist.get()\n\n    # Load dataset\n    training_data, training_targets, testing_data, testing_targets = mnist.load()\n\n    # Train PCA, reduce 784 dimensions to 250 dimensions\n    pca = pk.PCA(training_data, 250)\n    print('Variance retention:', pca.retention)\n\n    # Pick random datapoints\n    indices = random.sample(range(1, 1000), 16)\n    examples = training_data[indices]\n\n    # Show the original images\n    plt.figure('Original', figsize=(10, 7))\n    for i in range(1, 17):\n        plt.subplot(4, 4, i)\n        plt.imshow(examples[i-1].reshape((28, 28)), cmap='gray')\n\n    # Transform the example and compress\n    transformed_examples = pca.transform(examples)\n\n    # Inverse transform and recover the examples\n    recovered_examples = pca.inverse_transform(transformed_examples)\n\n    # Show the inverse transformed examples\n    plt.figure('Recovered', figsize=(10, 7))\n    for i in range(1, 17):\n        plt.subplot(4, 4, i)\n        plt.imshow(recovered_examples[i-1].reshape((28, 28)), cmap='gray')\n    \n    # Show results\n    plt.show()    \n\n\nif __name__ == '__main__':\n    test_pca_compression.__wrapped__()\n"""
tests/test_s1_kmeans.py,0,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\n@pktest_graph\ndef test_s1_kmeans():\n    import os\n    \n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import s1clustering\n    import matplotlib.pyplot as plt\n\n    # Download the dataset \n    if(not os.path.exists('s1.pkl')): s1clustering.get()\n\n    # Load the dataset\n    train_data = s1clustering.load()\n\n    # Run KMeans\n    clusters, cost = pk.kmeans(train_data, 15)\n\n    # Plot dataset, x and y\n    plt.scatter(train_data[:, 0], train_data[:, 1])    \n\n    # Plot clusters, x and y\n    plt.scatter(clusters[:, 0], clusters[:, 1], c='red')\n\n    # Show graph\n    plt.show()\n\n    # Assert cost\n    assert cost <= 1790000000\n\nif __name__ == '__main__':\n    try:\n        test_s1_kmeans.__wrapped__()\n    except AssertionError:\n        pass"""
tests/test_sonar_forest.py,0,"b""from pykitml.testing import pktest_graph, pktest_nograph\n\nimport pytest\n\n@pktest_graph\ndef test_sonar_forest():\n    import os\n\n    import numpy as np\n    import pykitml as pk\n    from pykitml.datasets import sonar\n\n    # Download the dataset\n    if(not os.path.exists('sonar.pkl')): sonar.get()\n\n    # Load the sonar dataset\n    inputs_train, outputs_train, inputs_test, outputs_test = sonar.load()\n    outputs_train = pk.onehot(outputs_train)\n    outputs_test = pk.onehot(outputs_test)\n\n    # Create model\n    forest_sonar_classifier = pk.RandomForest(60, 2, max_depth=9, feature_type=['continues']*60)\n\n    # Train the model\n    forest_sonar_classifier.train(inputs_train, outputs_train, num_feature_bag=60)\n\n    # Save it\n    pk.save(forest_sonar_classifier, 'forest_sonar_classifier.pkl')\n\n    # Print accuracy\n    accuracy = forest_sonar_classifier.accuracy(inputs_train, outputs_train)\n    print('Train accuracy:', accuracy)\n    accuracy = forest_sonar_classifier.accuracy(inputs_test, outputs_test)\n    print('Test accuracy:', accuracy)\n\n    # Plot confusion matrix\n    forest_sonar_classifier.confusion_matrix(inputs_test, outputs_test, \n        gnames=['False', 'True'])\n\nif __name__ == '__main__':\n    try:\n        test_sonar_forest.__wrapped__()\n    except AssertionError:\n        pass"""
pykitml/datasets/__init__.py,0,b''
pykitml/datasets/adult.py,1,"b""import os\nfrom urllib import request\n\nimport numpy as np\n\nfrom .. import pklhandler\n\n'''\nThis module contains helper functions to download the adult dataset.\n'''\n\ndef get():\n    '''\n    Downloads adult dataset from\n    https://archive.ics.uci.edu/ml/datasets/adult\n    and saves it as a pkl files `adult.data.pkl` and `adult.test.pkl`.\n\n    Raises\n    ------\n        urllib.error.URLError\n            If internet connection is not available or the URL is not accessible.\n        OSError\n            If the files cannot be created due to a system-related error.\n        KeyError\n            If invalid/unknown type.\n\n    Note\n    ----\n    You only need to call this method once, i.e, after the dataset has been downloaded\n    and you have the `adult.data.pkl` and `adult.test.pkl` files, you don't need to call \n    this method again.\n    '''\n    # Dictionary to store categorical values\n    workclass_dict = {\n        'Private':0, 'Self-emp-not-inc':1, 'Self-emp-inc':2, 'Federal-gov':3, \n        'Local-gov':4, 'State-gov':5, 'Without-pay':6, 'Never-worked':7\n    }\n\n    education_dict = {\n        'Bachelors':0, 'Some-college':1, '11th':2, 'HS-grad':3, 'Prof-school':4, 'Assoc-acdm':5,\n        'Assoc-voc':6, '9th':7, '7th-8th':8, '12th':9, 'Masters':10, '1st-4th':11, '10th':12,\n        'Doctorate':13, '5th-6th':14, 'Preschool':15\n    }\n\n    marital_dict = {\n        'Married-civ-spouse':0, 'Divorced':1, 'Never-married':2, 'Separated':3, \n        'Widowed':4, 'Married-spouse-absent':5, 'Married-AF-spouse':6\n    }\n\n    occupation_dict = {\n        'Tech-support':0, 'Craft-repair':1, 'Other-service':2, 'Sales':3, 'Exec-managerial':4, \n        'Prof-specialty':5, 'Handlers-cleaners':6, 'Machine-op-inspct':7, 'Adm-clerical':8, \n        'Farming-fishing':9, 'Transport-moving':10, 'Priv-house-serv':11, 'Protective-serv':12, \n        'Armed-Forces':13\n    }\n\n    relationship_dict = {\n        'Wife':0, 'Own-child':1, 'Husband':2, 'Not-in-family':3, 'Other-relative':4, \n        'Unmarried':5\n    }\n\n    race_dict = {\n        'White':0, 'Asian-Pac-Islander':1, 'Amer-Indian-Eskimo':2, 'Other':3, 'Black':4\n    }\n\n    sex_dict = {\n        'Female':0, 'Male':1\n    }\n\n    country_dict = {\n        'United-States':0, 'Cambodia':1, 'England':2, 'Puerto-Rico':3, 'Canada':4, 'Germany':5, \n        'Outlying-US(Guam-USVI-etc)':6, 'India':7, 'Japan':8, 'Greece':9, 'South':10, 'China':11, 'Cuba':12, \n        'Iran':13, 'Honduras':14, 'Philippines':15, 'Italy':16, 'Poland':17, 'Jamaica':18, 'Vietnam':19, 'Mexico':20, \n        'Portugal':21, 'Ireland':22, 'France':23, 'Dominican-Republic':24, 'Laos':25, 'Ecuador':26, 'Taiwan':27, \n        'Haiti':28, 'Columbia':29, 'Hungary':30, 'Guatemala':31, 'Nicaragua':32, 'Scotland':33, 'Thailand':34, \n        'Yugoslavia':35, 'El-Salvador':36, 'Trinadad&Tobago':37, 'Peru':38, 'Hong':39, 'Holand-Netherlands':40,\n    }\n\n    out_dict = {\n        '<=50K\\n':0, '>50K\\n':1\n    }\n\n    def download(files_name):\n        # Url to download the dataset from\n        url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/'+files_name\n\n        # Download the dataset\n        print('Downloading ' + files_name + '...')\n        request.urlretrieve(url, files_name)\n        print('Download complete.')\n\n        # Parse data and save it as a pkl files\n        data_array = []\n        # Open the files and put the values in a list\n        with open(files_name, 'r') as datafiles:\n            for line in datafiles:\n                try:\n                    values = line.replace(' ', '').replace('.', '').split(',')\n                    del values[4]\n                    values[0] = float(values[0])\n                    values[1] = workclass_dict[values[1]]\n                    values[2] = float(values[2])\n                    values[3] = education_dict[values[3]]\n                    values[4] = marital_dict[values[4]]\n                    values[5] = occupation_dict[values[5]]\n                    values[6] = relationship_dict[values[6]]\n                    values[7] = race_dict[values[7]]\n                    values[8] = sex_dict[values[8]]\n                    values[9] = float(values[9])\n                    values[10] = float(values[10])\n                    values[11] = float(values[11])\n                    values[12] = country_dict[values[12]]\n                    values[13] = out_dict[values[13]]\n                    data_array.append(values)\n                except (KeyError, ValueError, IndexError):\n                    continue\n        # Convert to numpy array\n        np_data = np.array(data_array)\n\n        # save it as a pkl files\n        pklhandler.save(np_data, files_name+'.pkl')\n\n    # Download the data set\n    download('adult.data')\n    download('adult.test')\n\n    # Clean up\n    print('Deleting unnecessary files...')\n    os.remove('adult.data')\n    os.remove('adult.test')\n\n\ndef load():\n    '''\n    Loads the adult dataset from `adult.data.pkl` and `adult.test.pkl` files.\n    The inputs have the following columns:\n\n    - age\n    - workclass : \n      Private=0, Self-emp-not-inc=1, Self-emp-inc=2, Federal-gov=3, \n      Local-gov=4, State-gov=5, Without-pay=6, Never-worked=7\n    - fnlwgt \n    - education :\n      Bachelors=0, Some-college=1, 11th=2, HS-grad=3, Prof-school=4, Assoc-acdm=5,\n      Assoc-voc=6, 9th=7, 7th-8th=8, 12th=9, Masters=10, 1st-4th=11, 10th=12,\n      Doctorate=13, 5th-6th=14, Preschool=15\n    - marital-status :\n      Married-civ-spouse=0, Divorced=1, Never-married=2, Separated=3, \n      Widowed=4, Married-spouse-absent=5, Married-AF-spouse=6\n    - occupation :\n      Tech-support=0, Craft-repair=1, Other-service=2, Sales=3, Exec-managerial=4, \n      Prof-specialty=5, Handlers-cleaners=6, Machine-op-inspct=7, Adm-clerical=8, \n      Farming-fishing=9, Transport-moving=10, Priv-house-serv=11, Protective-serv=12, \n      Armed-Forces=13\n    - relationship :\n      Wife=0, Own-child=1, Husband=2, Not-in-family=3, Other-relative=4, \n      Unmarried=5\n    - race :\n      White=0, Asian-Pac-Islander=1, Amer-Indian-Eskimo=2, Other=3, Black=4\n    - sex :\n      Female=0, Male=1\n    - capital-gain\n    - capital-loss\n    - hours-per-week\n    - native-country\n      United-States=0, Cambodia=1, England=2, Puerto-Rico=3, Canada=4, Germany=5, \n      Outlying-US(Guam-USVI-etc)=6, India=7, Japan=8, Greece=9, South=10, China=11, Cuba=12, \n      Iran=13, Honduras=14, Philippines=15, Italy=16, Poland=17, Jamaica=18, Vietnam=19, Mexico=20, \n      Portugal=21, Ireland=22, France=23, Dominican-Republic=24, Laos=25, Ecuador=26, Taiwan=27, \n      Haiti=28, Columbia=29, Hungary=30, Guatemala=31, Nicaragua=32, Scotland=33, Thailand=34, \n      Yugoslavia=35, El-Salvador=36, Trinadad&Tobago=37, Peru=38, Hong=39, Holand-Netherlands=40,\n\n    The outputs are:\n\n    - <=50K = 0/False \n    - >50K = 1/True\n\n    Returns\n    -------\n    inputs_train : numpy.array\n        392106x13 numpy array containing training inputs.\n    outputs_train : numpy.array\n        Numpy array of size 392106.\n    inputs_test : numpy.array\n        195780x13 numpy array containing testing inputs.\n    outputs_test : numpy.array\n        Numpy array of size 195780.\n\n    Raises\n    ------\n        filesNotFoundError\n            If `adult.data.pkl` or `adult.test.pkl` files does not exist, \n            i.e, if the dataset was not downloaded and saved using the \n            :py:func:`~get` method.  \n    '''\n    train = pklhandler.load('adult.data.pkl')\n    inputs_train = train[:, :-1]\n    outputs_train = train[:, -1]\n\n    test = pklhandler.load('adult.test.pkl')\n    inputs_test = test[:, :-1]\n    outputs_test = test[:, -1]\n\n    return inputs_train, outputs_train, inputs_test, outputs_test\n\n"""
pykitml/datasets/banknote.py,6,"b""import os\nfrom urllib import request\n\nimport numpy as np\nfrom numpy import genfromtxt\n\nfrom .. import pklhandler\n\n'''\nThis module contains helper functions to download and load\nthe banknote dataset.\n'''\n\ndef get():\n    '''\n    Downloads the banknote dataset from\n    http://archive.ics.uci.edu/ml/datasets/banknote+authentication\n    and saves it as a pkl file `banknote.pkl`.\n    \n    Raises\n    ------\n        urllib.error.URLError\n            If internet connection is not available or the URL is not accessible.\n        OSError\n            If the file cannot be created due to a system-related error.\n        KeyError\n            If invalid/unknown type.\n\n    Note\n    ----\n    You only need to call this method once, i.e, after the dataset has been downloaded\n    and you have the `banknote.pkl` file, you don't need to call this method again.   \n    '''\n    # Url to download the dataset from\n    url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt'\n    \n    # Download the dataset\n    print('Downloading data_banknote_authentication.txt')\n    request.urlretrieve(url, 'data_banknote_authentication.txt')\n    print('Download complete.')\n\n    # Parse the data and save it as a pkl file\n    pklhandler.save(genfromtxt('data_banknote_authentication.txt', delimiter=','), 'banknote.pkl')\n\n    # Delete unnecessary files\n    os.remove('data_banknote_authentication.txt')\n    print('Deleted unnecessary files.')\n\ndef load():\n    '''\n    Loads the banknote data from pkl file.\n    \n    The inputs have the following columns:\n\n    - Variance of Wavelet Transformed image (continuous)\n    - Skewness of Wavelet Transformed image (continuous) \n    - Curtosis of Wavelet Transformed image (continuous) \n    - Entropy of image (continuous) \n\n    The outputs are:\n\n    - 0 = Real\n    - 1 = Counterfeit\n\n    Returns\n    -------\n    inputs_train : numpy.array\n        1102x4 numpy array containing training inputs.\n    outputs_train : numpy.array\n        Numpy array of size 1102.\n    inputs_test : numpy.array\n        270x4 numpy array containing testing inputs.\n    outputs_test : numpy.array\n        Numpy array of size 270.\n    \n    '''\n    data_array = pklhandler.load('banknote.pkl')\n\n    # Separate data, positive and negative examples\n    negative_examples = data_array[:762]\n    positive_examples = data_array[762:]\n\n    # Separate into training and testing\n    negative_examples_test = negative_examples[:150]\n    negative_examples_train = negative_examples[150:]\n    positive_examples_test = positive_examples[:120]\n    positive_examples_train = positive_examples[120:]\n\n    # Join them to form training and testing dataset\n    train = np.concatenate((negative_examples_train, positive_examples_train), axis=0)\n    test = np.concatenate((negative_examples_test, positive_examples_test), axis=0)\n\n    # Shuffle the dataset\n    shuff_indices = np.arange(train.shape[0])\n    np.random.shuffle(shuff_indices)    \n    train = train[shuff_indices]\n    shuff_indices = np.arange(test.shape[0])\n    np.random.shuffle(shuff_indices)   \n    test = test[shuff_indices]\n\n    inputs_train = train[:, :-1]\n    outputs_train = train[:, -1]\n    inputs_test = test[:, :-1]\n    outputs_test = test[:, -1]\n\n    return inputs_train, outputs_train, inputs_test, outputs_test\n        """
pykitml/datasets/fishlength.py,2,"b""import numpy as np\n\n'''\nThis module contains helper functions to load the fish length dataset.\n'''\n\ninputs = np.array([\n    # Age Temperature\n    [ 14,  25],\n    [ 28,  25],\n    [ 41,  25],\n    [ 55,  25],\n    [ 69,  25],\n    [ 83,  25],\n    [ 97,  25],\n    [111,  25],\n    [125,  25],\n    [139,  25],\n    [153,  25],\n    [ 14,  27],\n    [ 28,  27],\n    [ 41,  27],\n    [ 55,  27],\n    [ 69,  27],\n    [ 83,  27],\n    [ 97,  27],\n    [111,  27],\n    [125,  27],\n    [139,  27],\n    [153,  27],\n    [ 14,  29],\n    [ 28,  29],\n    [ 41,  29],\n    [ 55,  29],\n    [ 69,  29],\n    [ 83,  29],\n    [ 97,  29],\n    [111,  29],\n    [125,  29],\n    [139,  29],\n    [153,  29],\n    [ 14,  31],\n    [ 28,  31],\n    [ 41,  31],\n    [ 55,  31],\n    [ 69,  31],\n    [ 83,  31],\n    [ 97,  31],\n    [111,  31],\n    [125,  31],\n    [139,  31],\n    [153,  31]\n])\n\noutputs = np.array([\n    # Fish-length\n     620,\n    1315,\n    2120,\n    2600,\n    3110,\n    3535,\n    3935,\n    4465,\n    4530,\n    4570,\n    4600,\n     625,\n    1215,\n    2110,\n    2805,\n    3255,\n    4015,\n    4315,\n    4495,\n    4535,\n    4600,\n    4600,\n     590,\n    1305,\n    2140,\n    2890,\n    3920,\n    3920,\n    4515,\n    4520,\n    4525,\n    4565,\n    4566,\n     590,\n    1205,\n    1915,\n    2140,\n    2710,\n    3020,\n    3030,\n    3040,\n    3180,\n    3257,\n    3214,\n])\n\ndef load():\n    '''\n    Loads the fish length dataset without any preprocessing.\n    Source: https://people.sc.fsu.edu/~jburkardt/datasets/regression/x06.txt\n\n    The length of a species of fish is to be represented as a function\n    of the age and water temperature. The fish are kept in tanks\n    at 25, 27, 29 and 31 degrees Celsius.  After birth, a test specimen\n    is chosen at random every 14 days and its length measured.\n\n    Returns\n    -------\n    inputs : numpy.array\n        44x2 numpy array, each row having 2 features,\n        :code:`age temperature`\n    outputs : numpy.array\n        Length of fish, numpy array with 44 elements.\n    '''\n    return inputs, outputs"""
pykitml/datasets/heartdisease.py,1,"b""import os\nfrom urllib import request\n\nimport numpy as np\n\nfrom .. import pklhandler\n\n'''\nThis module contains helper functions to download and load\nthe heart disease dataset. \n'''\n\ndef get():\n    '''\n    Downloads heartdisease dataset from\n    https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n    and saves it as a pkl file `heartdisease.pkl`.\n\n    Raises\n    ------\n        urllib.error.URLError\n            If internet connection is not available or the URL is not accessible.\n        OSError\n            If the file cannot be created due to a system-related error.\n        KeyError\n            If invalid/unknown type.\n\n    Note\n    ----\n    You only need to call this method once, i.e, after the dataset has been downloaded\n    and you have the `heartdisease.pkl` file, you don't need to call this method again.\n    '''\n    # Url to download the dataset from\n    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'\n\n    # Download the dataset\n    print('Downloading processed.cleveland.data...')\n    request.urlretrieve(url, 'processed.cleveland.data')\n    print('Download complete.')\n\n    # Parse data and save it as a pkl file.\n    data_array = []\n    # Open the file and put the values in a list.\n    with open('processed.cleveland.data', 'r') as datafile:\n        for line in datafile:\n            try:\n                data_array.append(list(map(float, line.split(','))))\n            except ValueError:\n                continue\n    # Convert the list into a numpy array.\n    heartdisease_data_array = np.array(data_array)\n    # Save as a pkl file.\n    pklhandler.save(heartdisease_data_array, 'heartdisease.pkl')\n\n    # Delete unnecessary files.\n    os.remove('processed.cleveland.data')\n    print('Deleted unnecessary files.')\n\ndef load():\n    '''\n    Loads heart disease dataset from saved pickle file `heartdisease.pkl` to numpy arrays.\n    Loads data without any preprocessing.\n\n    Returns\n    -------\n    inputs : numpy.array\n        297x13 numpy array. 297 training examples, each example having 13 inputs(columns).\n        The 13 columns corresponds to: \n        :code:`age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal`.\n\n        - age : Age in years\n        - sex : 1=male, 0=female\n        - cp : Chest pain type (1=typical-angina, 2=atypical-angina 3=non-anginal 4=asymptomatic)\n        - trestbps :  Resting blood pressure in mmHg\n        - chol : Serum cholesterol in mg/dl\n        - fbs : Fasting blood sugar > 120 mg/dl? (1=true, 0=false)\n        - restecg : Resting electrocardiographic results (0=normal, 1=ST-T-abnormality 2= left-ventricular-hypertrophy)\n        - thalach : Maximum heart rate achieved \n        - exang : Exercise induced angina (1=yes, 0=no) \n        - oldpeak : ST depression induced by exercise relative to rest\n        - slope: Slope of the peak exercise ST segment (1=upsloping 2=flat 3=downsloping)\n        - ca : Number of major vessels colored by flourosopy (0-3)\n        - thal: 3=normal, 6=fixed-defect, 7=reversable-defect\n\n    outputs : numpy.array\n        Numpy array with 297 elements. \n        \n        - 0: < 50% diameter narrowing \n        - 1: > 50% diameter narrowing    \n\n    Raises\n    ------\n        FileNotFoundError\n            If `heartdisease.pkl` file does not exist, i.e, if the dataset was not \n            downloaded and saved using the :py:func:`~get` method.  \n    '''\n    # Load data from pkl file.\n    heartdisease_data_array = pklhandler.load('heartdisease.pkl')   \n    inputs =  heartdisease_data_array[:,:-1]\n    outputs = (heartdisease_data_array[:,-1]>0)*1\n\n    # return data\n    return inputs, outputs\n\n"""
pykitml/datasets/iris.py,4,"b""import numpy as np\n\n'''\nThis module contains helper function to load the iris dataset.\n'''\n    # sepal-length, sepal-width, petal-length, petal-width\n    # all in cm\n\n\ninputs_train = np.array([\n    [5.8, 2.7, 5.1, 1.9], [5.5, 2.3, 4.0, 1.3], [5.8, 2.7, 3.9, 1.2],\n    [5.5, 2.5, 4.0, 1.3], [6.4, 2.8, 5.6, 2.2], [6.8, 2.8, 4.8, 1.4],\n    [5.5, 2.4, 3.7, 1.0], [7.6, 3.0, 6.6, 2.1], [5.4, 3.0, 4.5, 1.5],\n    [5.5, 2.4, 3.8, 1.1], [5.9, 3.2, 4.8, 1.8], [6.3, 3.3, 4.7, 1.6],\n    [6.4, 3.2, 5.3, 2.3], [5.5, 3.5, 1.3, 0.2], [6.1, 2.8, 4.0, 1.3],\n    [6.0, 3.4, 4.5, 1.6], [5.1, 3.3, 1.7, 0.5], [7.7, 3.8, 6.7, 2.2],\n    [5.8, 2.7, 4.1, 1.0], [6.0, 2.9, 4.5, 1.5], [5.8, 2.8, 5.1, 2.4],\n    [5.2, 3.4, 1.4, 0.2], [6.7, 2.5, 5.8, 1.8], [7.0, 3.2, 4.7, 1.4],\n    [6.3, 3.3, 6.0, 2.5], [7.7, 2.6, 6.9, 2.3], [6.7, 3.3, 5.7, 2.1],\n    [7.2, 3.6, 6.1, 2.5], [6.3, 3.4, 5.6, 2.4], [5.1, 3.5, 1.4, 0.3],\n    [4.8, 3.4, 1.6, 0.2], [6.3, 2.7, 4.9, 1.8], [6.7, 3.1, 4.4, 1.4],\n    [5.0, 3.4, 1.6, 0.4], [6.6, 3.0, 4.4, 1.4], [5.5, 4.2, 1.4, 0.2],\n    [5.0, 3.4, 1.5, 0.2], [6.4, 3.1, 5.5, 1.8], [5.1, 3.8, 1.5, 0.3],\n    [6.1, 2.8, 4.7, 1.2], [5.0, 2.0, 3.5, 1.0], [4.6, 3.4, 1.4, 0.3],\n    [6.0, 2.2, 5.0, 1.5], [6.1, 2.6, 5.6, 1.4], [4.8, 3.4, 1.9, 0.2],\n    [6.6, 2.9, 4.6, 1.3], [6.1, 2.9, 4.7, 1.4], [6.4, 2.8, 5.6, 2.1],\n    [5.4, 3.7, 1.5, 0.2], [5.0, 3.2, 1.2, 0.2], [6.2, 2.8, 4.8, 1.8],\n    [6.5, 3.0, 5.8, 2.2], [5.6, 3.0, 4.5, 1.5], [6.9, 3.1, 5.4, 2.1],\n    [7.1, 3.0, 5.9, 2.1], [4.9, 3.1, 1.5, 0.1], [6.9, 3.2, 5.7, 2.3],\n    [5.8, 4.0, 1.2, 0.2], [6.3, 2.3, 4.4, 1.3], [6.4, 2.9, 4.3, 1.3],\n    [5.2, 2.7, 3.9, 1.4], [4.6, 3.1, 1.5, 0.2], [6.0, 2.7, 5.1, 1.6],\n    [5.1, 3.5, 1.4, 0.2], [6.0, 3.0, 4.8, 1.8], [5.4, 3.9, 1.7, 0.4],\n    [5.1, 3.4, 1.5, 0.2], [5.4, 3.4, 1.7, 0.2], [5.7, 2.5, 5.0, 2.0],\n    [6.7, 3.0, 5.0, 1.7], [5.6, 2.5, 3.9, 1.1], [6.5, 2.8, 4.6, 1.5],\n    [5.4, 3.9, 1.3, 0.4], [4.9, 3.0, 1.4, 0.2], [7.4, 2.8, 6.1, 1.9],\n    [7.2, 3.2, 6.0, 1.8], [5.6, 2.9, 3.6, 1.3], [6.4, 3.2, 4.5, 1.5],\n    [4.3, 3.0, 1.1, 0.1], [5.1, 3.7, 1.5, 0.4], [5.4, 3.4, 1.5, 0.4],\n    [4.8, 3.1, 1.6, 0.2], [7.7, 3.0, 6.1, 2.3], [5.6, 3.0, 4.1, 1.3],\n    [6.3, 2.5, 4.9, 1.5], [5.7, 4.4, 1.5, 0.4], [6.5, 3.2, 5.1, 2.0],\n    [4.9, 3.1, 1.5, 0.1], [4.8, 3.0, 1.4, 0.1], [5.2, 3.5, 1.5, 0.2],\n    [7.9, 3.8, 6.4, 2.0], [5.7, 3.8, 1.7, 0.3], [5.6, 2.8, 4.9, 2.0],\n    [6.8, 3.0, 5.5, 2.1], [5.7, 2.8, 4.5, 1.3], [4.7, 3.2, 1.3, 0.2],\n    [6.0, 2.2, 4.0, 1.0], [6.1, 3.0, 4.9, 1.8], [4.7, 3.2, 1.6, 0.2],\n    [6.5, 3.0, 5.5, 1.8], [5.0, 3.6, 1.4, 0.2], [4.9, 2.4, 3.3, 1.0],\n    [5.7, 2.6, 3.5, 1.0], [6.7, 3.1, 4.7, 1.5], [6.4, 2.7, 5.3, 1.9],\n    [5.2, 4.1, 1.5, 0.1], [5.0, 3.0, 1.6, 0.2], [7.7, 2.8, 6.7, 2.0],\n    [4.9, 2.5, 4.5, 1.7], [4.9, 3.1, 1.5, 0.1], [7.2, 3.0, 5.8, 1.6],\n    [4.4, 2.9, 1.4, 0.2], [6.3, 2.9, 5.6, 1.8], [5.9, 3.0, 4.2, 1.5],\n    [6.3, 2.8, 5.1, 1.5], [4.4, 3.0, 1.3, 0.2], [7.3, 2.9, 6.3, 1.8],\n    [4.6, 3.6, 1.0, 0.2], [6.2, 2.2, 4.5, 1.5], [6.9, 3.1, 4.9, 1.5],\n])\n\noutputs_train = np.array([\n    [0, 0, 1], [0, 1, 0], [0, 1, 0],\n    [0, 1, 0], [0, 0, 1], [0, 1, 0],\n    [0, 1, 0], [0, 0, 1], [0, 1, 0],\n    [0, 1, 0], [0, 1, 0], [0, 1, 0],\n    [0, 0, 1], [1, 0, 0], [0, 1, 0],\n    [0, 1, 0], [1, 0, 0], [0, 0, 1],\n    [0, 1, 0], [0, 1, 0], [0, 0, 1],\n    [1, 0, 0], [0, 0, 1], [0, 1, 0],\n    [0, 0, 1], [0, 0, 1], [0, 0, 1],\n    [0, 0, 1], [0, 0, 1], [1, 0, 0],\n    [1, 0, 0], [0, 0, 1], [0, 1, 0],\n    [1, 0, 0], [0, 1, 0], [1, 0, 0],\n    [1, 0, 0], [0, 0, 1], [1, 0, 0],\n    [0, 1, 0], [0, 1, 0], [1, 0, 0],\n    [0, 0, 1], [0, 0, 1], [1, 0, 0],\n    [0, 1, 0], [0, 1, 0], [0, 0, 1],\n    [1, 0, 0], [1, 0, 0], [0, 0, 1],\n    [0, 0, 1], [0, 1, 0], [0, 0, 1],\n    [0, 0, 1], [1, 0, 0], [0, 0, 1],\n    [1, 0, 0], [0, 1, 0], [0, 1, 0],\n    [0, 1, 0], [1, 0, 0], [0, 1, 0],\n    [1, 0, 0], [0, 0, 1], [1, 0, 0],\n    [1, 0, 0], [1, 0, 0], [0, 0, 1],\n    [0, 1, 0], [0, 1, 0], [0, 1, 0],\n    [1, 0, 0], [1, 0, 0], [0, 0, 1],\n    [0, 0, 1], [0, 1, 0], [0, 1, 0],\n    [1, 0, 0], [1, 0, 0], [1, 0, 0],\n    [1, 0, 0], [0, 0, 1], [0, 1, 0],\n    [0, 1, 0], [1, 0, 0], [0, 0, 1],\n    [1, 0, 0], [1, 0, 0], [1, 0, 0],\n    [0, 0, 1], [1, 0, 0], [0, 0, 1],\n    [0, 0, 1], [0, 1, 0], [1, 0, 0],\n    [0, 1, 0], [0, 0, 1], [1, 0, 0],\n    [0, 0, 1], [1, 0, 0], [0, 1, 0],\n    [0, 1, 0], [0, 1, 0], [0, 0, 1],\n    [1, 0, 0], [1, 0, 0], [0, 0, 1],\n    [0, 0, 1], [1, 0, 0], [0, 0, 1],\n    [1, 0, 0], [0, 0, 1], [0, 1, 0],\n    [0, 0, 1], [1, 0, 0], [0, 0, 1],\n    [1, 0, 0], [0, 1, 0], [0, 1, 0],\n])\n\ninputs_test = np.array([\n    [5.6, 2.7, 4.2, 1.3], [6.2, 3.4, 5.4, 2.3], [4.8, 3.0, 1.4, 0.3],\n    [5.8, 2.7, 5.1, 1.9], [6.1, 3.0, 4.6, 1.4], [6.7, 3.3, 5.7, 2.5],\n    [6.7, 3.0, 5.2, 2.3], [6.8, 3.2, 5.9, 2.3], [5.7, 2.8, 4.1, 1.3],\n    [5.8, 2.6, 4.0, 1.2], [5.0, 3.5, 1.3, 0.3], [5.1, 3.8, 1.6, 0.2],\n    [4.6, 3.2, 1.4, 0.2], [6.7, 3.1, 5.6, 2.4], [5.1, 3.8, 1.9, 0.4],\n    [5.1, 2.5, 3.0, 1.1], [5.7, 2.9, 4.2, 1.3], [5.9, 3.0, 5.1, 1.8],\n    [5.3, 3.7, 1.5, 0.2], [5.7, 3.0, 4.2, 1.2], [5.0, 2.3, 3.3, 1.0],\n    [6.9, 3.1, 5.1, 2.3], [5.0, 3.3, 1.4, 0.2], [4.5, 2.3, 1.3, 0.3],\n    [5.5, 2.6, 4.4, 1.2], [6.5, 3.0, 5.2, 2.0], [5.0, 3.5, 1.6, 0.6],\n    [6.3, 2.5, 5.0, 1.9], [6.2, 2.9, 4.3, 1.3], [4.4, 3.2, 1.3, 0.2],  \n])\n\noutputs_test = np.array([\n    [0, 1, 0], [0, 0, 1], [1, 0, 0],\n    [0, 0, 1], [0, 1, 0], [0, 0, 1],\n    [0, 0, 1], [0, 0, 1], [0, 1, 0],\n    [0, 1, 0], [1, 0, 0], [1, 0, 0],\n    [1, 0, 0], [0, 0, 1], [1, 0, 0],\n    [0, 1, 0], [0, 1, 0], [0, 0, 1],\n    [1, 0, 0], [0, 1, 0], [0, 1, 0],\n    [0, 0, 1], [1, 0, 0], [1, 0, 0],\n    [0, 1, 0], [0, 0, 1], [1, 0, 0],\n    [0, 0, 1], [0, 1, 0], [1, 0, 0], \n])\n\ndef load():\n    '''\n    Loads the iris dataset without any preprocessing.\n    The data set consists of 50 samples from each of three species of Iris \n    (Iris setosa, Iris virginica and Iris versicolor). \n    Four features were measured from each sample: the length and the width \n    of the sepals and petals\n\n    Inputs have the following features/columns:\n\n        :code:`sepal-length sepal-width petal-length petal-width`\n    \n    Outputs:\n\n        :code:`[1, 0, 0]` - Iris-setosa,\n        :code:`[0, 1, 0]` - Iris-versicolor,\n        :code:`[0, 0, 1]` - Iris-virginica.\n\n    Returns\n    -------\n    inputs_train : numpy.array\n        120x4 numpy array, each row having 4 features,\n    outputs_train : numpy.array\n        120x3 numpy array, contains 150 one-hot vectors, each\n        corresponding to a category,\n    inputs_test : numpy.array\n        30x4 numpy array, each row having 4 features,\n    outputs_test : numpy.array\n        30x3 numpy array, contains 150 one-hot vectors, each\n        corresponding to a category,\n    '''\n    return inputs_train, outputs_train, inputs_test, outputs_test"""
pykitml/datasets/mnist.py,6,"b""'''\nThis module contains helper functions to download and load MNIST and MNIST like datasets.   \n'''\n\n# ============================================================  \n# = Forked from: https://github.com/hsjeong5/MNIST-for-Numpy =\n# = Modified with minor changes                              =\n# ============================================================\n\nimport gzip\nimport pickle\nimport os\nfrom urllib import request\n\nimport numpy as np\n\nfrom .. import pklhandler\n\ndef get(type = 'classic'):\n    '''\n    Downloads the MNIST dataset and saves it as a pickle file, `mnist.pkl`.\n\n    Parameters\n    ----------\n    type : str\n        The type of MNIST dataset to download.\n\n        - 'classic' : Downloads the classic hanwritten digits dataset from http://yann.lecun.com/exdb/mnist/\n        - 'fashion' : Downloads fashion MNIST from https://github.com/zalandoresearch/fashion-mnist\n\n\n    Raises\n    ------\n        urllib.error.URLError\n            If internet connection is not available or the URL is not accessible.\n        OSError\n            If the file cannot be created due to a system-related error.\n        KeyError\n            If invalid/unknown type.\n\n    Note\n    ----\n    You only need to call this method once, i.e, after the dataset has been \n    downloaded and you have the `mnist.pkl` file, you don't need to call this method again.\n    '''\n    # dict of URLs containing MNIST like datasets\n    type_URLs = {'classic':'http://yann.lecun.com/exdb/mnist/',\n            'fashion':'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/'\n    }\n\n    # MNIST files to download\n    filename = [\n        ['training_images','train-images-idx3-ubyte.gz'],\n        ['test_images','t10k-images-idx3-ubyte.gz'],\n        ['training_labels','train-labels-idx1-ubyte.gz'],\n        ['test_labels','t10k-labels-idx1-ubyte.gz']\n    ]\n\n    def download_mnist():\n        # Download .gz files\n        base_url = type_URLs[type]\n        for name in filename:\n            print('Downloading '+name[1]+'...')\n            request.urlretrieve(base_url+name[1], name[1])\n        print('Download complete.')\n\n    def save_mnist():\n        # Read .gz files and put them in a numpy array and save it as a pkl file\n        mnist = {}\n        for name in filename[:2]:\n            with gzip.open(name[1], 'rb') as f:\n                mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n        for name in filename[-2:]:\n            with gzip.open(name[1], 'rb') as f:\n                mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n        pklhandler.save(mnist, 'mnist.pkl')\n        print('Save complete.')\n\n    def clean():\n        # Remove unnecessary files\n        os.remove('train-images-idx3-ubyte.gz')\n        os.remove('t10k-images-idx3-ubyte.gz')\n        os.remove('train-labels-idx1-ubyte.gz')\n        os.remove('t10k-labels-idx1-ubyte.gz')\n        print('Deleted unnecessary files.')\n\n    download_mnist()\n    save_mnist()\n    clean()\n\n\ndef load():\n    '''\n    Loads MNIST dataset from saved pickle file `mnist.pkl` to numpy arrays.\n\n    Returns\n    -------\n        training_data : numpy.array\n            60,000x784 numpy array, each row contains flattened version of training images.\n        training_targets : numpy.array\n            60,000x10 numpy array that contains one hot target array of the corresponding \n            training images.\n        testing_data : numpy.array\n            10,000x784 numpy array, each row contains flattened version of test images.\n        testing_targets : numpy.array\n            10,000x10 numpy array that contains one hot target array of the corresponding\n            test images.\n\n    Raises\n    ------\n        FileNotFoundError\n            If `mnist.pkl` file does not exist, i.e, if the dataset was not downloaded and\n            saved using the :py:func:`~get` method. \n    '''\n    mnist = pklhandler.load('mnist.pkl')\n    # Normalize data\n    training_data = mnist['training_images']/255\n    testing_data = mnist['test_images']/255\n    # Create one-hot target array for training labels\n    training_targets = np.zeros((60000, 10))\n    training_targets[np.arange(60000), mnist['training_labels']] = 1\n    # Create one-hot target array for testing labels\n    testing_targets = np.zeros((10000, 10))\n    testing_targets[np.arange(10000), mnist['test_labels']] = 1\n    # return the data\n    return training_data, training_targets, testing_data, testing_targets\n\n\nif __name__ == '__main__':\n    get()\n    """
pykitml/datasets/s1clustering.py,1,"b""import os\nfrom urllib import request\n\nimport numpy as np\n\nfrom .. import pklhandler\n\n'''\nThis module contains helper functions to download and load\nthe S1 clustering dataset. \n'''\n\ndef get():\n    '''\n    Downloads the s1 clustering dataset from \n    http://cs.joensuu.fi/sipu/datasets/\n    and save is as a pkl file `s1.pkl`.\n\n    Raises\n    ------\n        urllib.error.URLError\n            If internet connection is not available or the URL is not accessible.\n        OSError\n            If the file cannot be created due to a system-related error.\n        KeyError\n            If invalid/unknown type.\n\n    Note\n    ----\n    You only need to call this method once, i.e, after the dataset has been downloaded\n    and you have the `s1.pkl` file, you don't need to call this method again.\n    '''\n    # Url to download the dataset from\n    url = 'http://cs.joensuu.fi/sipu/datasets/s1.txt'\n\n    # Download the dataset\n    print('Downloading s1.txt...')\n    request.urlretrieve(url, 's1.txt')\n    print('Download complete.')\n\n    # Parse the data and save it as a pkl file\n    data_array = np.loadtxt('s1.txt')\n    pklhandler.save(data_array, 's1.pkl')\n\n    # Delete unnecessary files.\n    os.remove('s1.txt')\n    print('Deleted unnecessary files.')\n\ndef load():\n    '''\n    Loads x, y points from the s1 clustering dataset from saved pickle file `s1.pkl` to \n    numpy array. S1 clustering dataset contains 15 clusters.\n\n    Returns\n    -------\n    training_data : numpy.array\n        5000x2 numpy array containing x, y points. \n\n    Raises\n    ------\n        FileNotFoundError\n            If `s1.pkl` file does not exist, i.e, if the dataset was not \n            downloaded and saved using the :py:func:`~get` method.  \n    '''\n    return pklhandler.load('s1.pkl')\n"""
pykitml/datasets/sonar.py,5,"b'import os\nfrom urllib import request\n\nimport numpy as np\n\nfrom .. import pklhandler\n\n\'\'\'\nThis module contains helper functions to download the sonar dataset.\n\'\'\'\n\ndef get():\n    \'\'\'\n    Downloads sonar dataset from\n    https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)\n    and saves it as a pkl file `sonar.pkl`.\n\n    Raises\n    ------\n        urllib.error.URLError\n            If internet connection is not available or the URL is not accessible.\n        OSError\n            If the files cannot be created due to a system-related error.\n        KeyError\n            If invalid/unknown type.\n\n    Note\n    ----\n    You only need to call this method once, i.e, after the dataset has been downloaded\n    and you have the `sonar.pkl` file, you don\'t need to call \n    this method again.\n    \'\'\'\n    # Url to download the dataset from\n    url = \'https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\'\n\n    # Download the dataset\n    print(\'Downloading sonar.all-data...\')\n    request.urlretrieve(url, \'sonar.all-data\')\n    print(\'Download complete.\')\n\n    out_dict = {\n        \'R\\n\':0, \'M\\n\':1\n    }\n\n    # Parse data and save it as pkl file\n    data_array = []\n    # Open the file and put the values in a list.\n    with open(\'sonar.all-data\', \'r\') as datafile:\n        for line in datafile:\n            values = line.split(\',\')\n            values[-1] = out_dict[values[-1]]\n            data_array.append(list(map(float, values)))\n    # Convert the list to numpy array\n    sonar_data_array = np.array(data_array)\n    # Save it as a pkl file\n    pklhandler.save(sonar_data_array, \'sonar.pkl\')\n\ndef load():\n    \'\'\'\n    Loads the adult dataset from `sonar.pkl` file.\n\n    Each pattern is a set of 60 numbers in the range 0.0 to 1.0. \n    Each number represents the energy within a particular frequency band, \n    integrated over a certain period of time. The integration aperture for \n    higher frequencies occur later in time, since these frequencies are \n    transmitted later during the chirp.\n\n    The label associated with each record contains the letter \n    ""R"" if the object is a rock and ""M"" if it is a mine (metal cylinder).\n\n    Returns\n    -------\n    inputs_train : numpy.array\n        190x60 numpy array containing training inputs.\n    outputs_train : numpy.array\n        Numpy array of size 190.\n    inputs_test : numpy.array\n        18x60 numpy array containing testing inputs.\n    outputs_test : numpy.array\n        Numpy array of size 18.\n\n    Raises\n    ------\n        filesNotFoundError\n            If `sonar.pkl` file does not exist, \n            i.e, if the dataset was not downloaded and saved using the \n            :py:func:`~get` method.     \n\n    \'\'\'\n    # Load the data from pkl file\n    sonar_data_array = pklhandler.load(\'sonar.pkl\')\n\n    # Split into train and test\n    train_neg = sonar_data_array[0:90]\n    train_pos = sonar_data_array[97:197]\n    test_neg = sonar_data_array[90:97]\n    test_pos = sonar_data_array[197:208]\n\n    # Shuffle the dataset, join neg and pos examples\n    train = np.concatenate((train_pos, train_neg), axis=0)\n    np.random.shuffle(train)\n    test = np.concatenate((test_pos, test_neg), axis=0)\n    np.random.shuffle(test)\n\n    # Split the dataset into inputs and outputs\n    inputs_train = train[:, :-1]\n    outputs_train = train[:, -1]\n    inputs_test = test[:, :-1]\n    outputs_test = test[:, -1]\n\n    # return\n    return inputs_train, outputs_train, inputs_test, outputs_test\n'"
