file_path,api_count,code
config.py,0,b'epochs = 30\nlearning_rate = 0.1\n'
main.py,0,"b'import numpy as np\nimport time\n\nimport config\nfrom core.network import Network\nfrom core.dense import Dense\nfrom core.activation_layer import Activation\nfrom activations.activation_functions import Tanh, dTanh\nfrom loss.loss_functions import MSE, dMSE\nfrom optimizers.optimizer_functions import Momentum\n\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\n\n# Load MNIST\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.reshape(x_train.shape[0], 1, 28 * 28)\nx_train = x_train.astype(""float32"")\nx_train /= 255\ny_train = np_utils.to_categorical(y_train)\n\nx_test = x_test.reshape(x_test.shape[0], 1, 28 * 28)\nx_test = x_test.astype(""float32"")\nx_test /= 255\ny_test = np_utils.to_categorical(y_test)\n\n# Model\nnn = Network()\nnn.add(Dense(28 * 28, 100))\nnn.add(Activation(Tanh, dTanh))\nnn.add(Dense(100, 50))\nnn.add(Activation(Tanh, dTanh))\nnn.add(Dense(50, 10))\nnn.add(Activation(Tanh, dTanh))\n\n# Training\n\nnn.useLoss(MSE, dMSE)\nnn.useOptimizer(Momentum(), learning_rate=config.learning_rate)\nnn.fit(x_train[0:2000], y_train[0:2000], epochs=config.epochs)\n\n\n# Prediction\nout = nn.predict(x_test[0:2])\nprint(""\\nPredicted Values: "")\nprint(out, end=""\\n"")\nprint(""True Values: "")\nprint(y_test[0:2])\n'"
activations/__init__.py,0,b''
activations/activation_functions.py,9,"b'import numpy as np\n\n\ndef Linear(X, constant=1):\n    return constant*X\n\n\ndef dLinear(constant=1):\n    return constant\n\n\ndef Sigmoid(X):\n    res = 1 / (1 + np.exp(-X))\n    return res\n\n\ndef dSigmoid(X):\n    return Sigmoid(X) * (1-Sigmoid(X))\n\n\ndef Tanh(X):\n    return np.tanh(X)\n\n\ndef dTanh(X):\n    return 1 - np.power(np.tanh(X), 2)\n\n\ndef ReLu(X):\n    return np.maximum(0, X)\n\n\ndef dReLu(X):\n    return 1 if X > 0 else 0\n\n\ndef Leaky_ReLu(X, factor=0.01):\n    return np.maximum(factor*X, X)\n\n\ndef dLeaky_ReLu(X, factor=0.01):\n    return 1 if X > 0 else factor\n\n\n# TODO: d/dX\n\ndef Softmax(X):\n    exp = np.exp(X)\n    exp_sum = np.sum(np.exp(X))\n    res = exp/exp_sum\n    return res\n\n\ndef GeLu(X):\n    res = 0.5 * X * (1 + np.tanh(np.sqrt(2 / np.pi) *\n                                 (X + 0.044715 * np.power(X, 3))))\n    return res\n'"
core/__init__.py,0,b''
core/activation_layer.py,0,"b'from .layer import Layer\n\n\nclass Activation(Layer):\n    def __init__(self, activation, dactivation):\n        self.activation = activation\n        self.dactivation = dactivation\n\n    def forward_propagation(self, input_data):\n        self.input = input_data\n        self.output = self.activation(self.input)\n        return self.output\n\n    def backward_propagation(self, output_error, optimizer_fn, learning_rate):\n        return self.dactivation(self.input) * output_error\n'"
core/dense.py,7,"b'from .layer import Layer\nimport numpy as np\nimport config\n\n\nclass Dense(Layer):\n    # input_size = Number of Input Neurons\n    # output_size = Number of Output Neurons\n    def __init__(self, input_size, output_size):\n        self.weights = np.random.rand(input_size, output_size) - 0.5\n        self.bias = np.random.rand(1, output_size) - 0.5\n        self.vW = np.zeros([input_size, output_size])\n        self.vB = np.zeros([1, output_size])\n\n    def forward_propagation(self, input_data):\n        self.input = input_data\n        self.output = np.dot(self.input, self.weights) + self.bias\n        return self.output\n\n    def backward_propagation(self, output_error, optimizer_fn, learning_rate):\n        input_error = np.dot(output_error, self.weights.T)\n        dW = np.dot(self.input.T, output_error)\n        dB = output_error\n\n        w_updated, b_updated, vW_updated, vB_updated = optimizer_fn.minimize(\n            self.weights, self.bias, dW, dB, self.vW, self.vB, learning_rate\n        )\n        self.weights = w_updated\n        self.bias = b_updated\n        self.vW = vW_updated\n        self.vB = vB_updated\n        return input_error\n'"
core/layer.py,0,"b'# Base Class\nclass Layer:\n    def __init__(self):\n        self.input = None\n        self.output = None\n\n    def forward_propagation(self, input):\n        raise NotImplementedError\n\n    def backward_propagation(self, output_error, optimizer_fn, learning_rate):\n        raise NotImplementedError\n'"
core/network.py,0,"b""class Network:\n    def __init__(self):\n        self.layers = []\n        self.loss = None\n        self.dloss = None\n\n    def add(self, layer):\n        self.layers.append(layer)\n\n    def useLoss(self, loss, dloss):\n        self.loss = loss\n        self.dloss = dloss\n\n    def useOptimizer(self, optimizer, learning_rate):\n        self.optimizer = optimizer\n        self.learning_rate = learning_rate\n\n    def predict(self, input_data):\n        samples = len(input_data)\n        result = []\n        for i in range(samples):\n            output = input_data[i]\n            for layer in self.layers:\n                output = layer.forward_propagation(output)\n            result.append(output)\n        return result\n\n    def fit(self, x_train, y_train, epochs):\n        samples = len(x_train)\n        for i in range(epochs):\n            err = 0\n            for j in range(samples):\n                output = x_train[j]\n                for layer in self.layers:\n                    output = layer.forward_propagation(output)\n                err += self.loss(y_train[j], output)\n                error = self.dloss(y_train[j], output)\n                for layer in reversed(self.layers):\n                    error = layer.backward_propagation(\n                        error, self.optimizer, self.learning_rate)\n            err /= samples\n            print('epoch %d/%d\\terror=%f' % (i+1, epochs, err))\n"""
loss/__init__.py,0,b''
loss/loss_functions.py,2,"b'import numpy as np\n\n\ndef MSE(y, yhat):\n    return np.mean(np.power(y-yhat, 2))\n\n\ndef dMSE(y, yhat):\n    return 2*(yhat-y)/y.size\n\n\ndef MAE(y, yhat):\n    return np.sum(np.abs(y-yhat))\n\n\ndef dMAE(y, yhat):\n    return 1 if y == yhat else -1\n'"
optimizers/__init__.py,0,b''
optimizers/optimizer_functions.py,0,"b'import numpy as np\n\n\nclass GradientDescent:\n    def minimize(self, w, b, dW, dB, vW, vB, learning_rate=0.01):\n        """"""Implements Gradient Descent to find minima of cost function\n\n    Parameters:\n    - w (numpy array): weights matrix\n    - b (numpy array): bias matrix\n    - dW (numpy array): gradient of weights matrix wrt cost function\n    - dB (numpy array): gradient of bias matrix wrt cost function\n    - learning_rate (double): learning rate used to update weights\n\n    Returns:\n    - w_updated (numpy array): updated weights\n    - b_updated (numpy array): updated bias\n\n    """"""\n        w_updated = w - learning_rate * dW\n        b_updated = b - learning_rate * dB\n        return w_updated, b_updated, vW, vB\n\n\nclass Momentum:\n    def minimize(self, w, b, dW, dB, vW, vB, learning_rate=0.01, beta=0.9):\n        """"""Implements Gradient Descent with Momentum to find minima of cost function\n\n        Parameters:\n        - w (numpy array): weights matrix\n        - b (numpy array): bias matrix\n        - dW (numpy array): gradient of weights matrix wrt cost function\n        - dB (numpy array): gradient of bias matrix wrt cost function\n        - learning_rate (double): learning rate used to update weights\n        - beta (double): Momentum term for smoothing\n        - vW (numpy array): holds the state of the optimizer for previous iteration (weights)\n        - vB (numpy array): holds the state of the optimizer for previous iterations (biases)\n\n        Returns:\n        - w_updated (numpy array): updated weights\n        - b_updated (numpy array): updated bias\n        - vW (numpy array): updated state of the optimizer for current iteration (weights)\n        - vB (numpy array): updated state of the optimizer for current iteration (biases)\n\n        """"""\n\n        vW = beta * vW + (1 - beta) * dW\n        vB = beta * vB + (1 - beta) * dB\n        w_updated = w - learning_rate * vW\n        b_updated = b - learning_rate * vB\n        return w_updated, b_updated, vW, vB\n\n\ndef RMSProp(w, b, dW, dB, learning_rate, beta, epsilon):\n    pass\n\n\ndef Adam(w, b, dW, dB, learning_rate, beta1, beta2, epsilon):\n    pass\n'"
