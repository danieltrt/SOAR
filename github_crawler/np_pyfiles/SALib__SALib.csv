file_path,api_count,code
setup.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n    Setup file for salib.\n    Use setup.cfg to configure your project.\n\n    This file was generated with PyScaffold 3.2.2.\n    PyScaffold helps you to put up the scaffold of your new Python project.\n    Learn more under: https://pyscaffold.org/\n""""""\nimport os\nimport sys\n\nfrom pkg_resources import VersionConflict, require\nfrom setuptools import setup\n\nscripts = [\'src/SALib/scripts/salib.py\']\nif os.name == \'nt\':\n    scripts.append(\'src/SALib/scripts/salib.bat\')\n\ntry:\n    require(\'setuptools>=38.3\')\nexcept VersionConflict:\n    print(""Error: version of setuptools is too old (<38.3)!"")\n    sys.exit(1)\n\n\nif __name__ == ""__main__"":\n    setup(use_pyscaffold=True, scripts=scripts)\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport os\nimport sys\nimport inspect\nimport shutil\n\n__location__ = os.path.join(os.getcwd(), os.path.dirname(\n    inspect.getfile(inspect.currentframe())))\n\n# Support markdown\nfrom recommonmark.parser import CommonMarkParser\n\nsource_parsers = {\n    \'.md\': CommonMarkParser,\n}\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.join(__location__, \'../src\'))\n\n# -- Run sphinx-apidoc ------------------------------------------------------\n# This hack is necessary since RTD does not issue `sphinx-apidoc` before running\n# `sphinx-build -b html . _build/html`. See Issue:\n# https://github.com/rtfd/readthedocs.org/issues/1139\n# DON\'T FORGET: Check the box ""Install your project inside a virtualenv using\n# setup.py install"" in the RTD Advanced Settings.\n# Additionally it helps us to avoid running apidoc manually\n\ntry:  # for Sphinx >= 1.7\n    from sphinx.ext import apidoc\nexcept ImportError:\n    from sphinx import apidoc\n\noutput_dir = os.path.join(__location__, ""api"")\nmodule_dir = os.path.join(__location__, ""../src/SALib"")\ntry:\n    shutil.rmtree(output_dir)\nexcept FileNotFoundError:\n    pass\n\ntry:\n    import sphinx\n    from pkg_resources import parse_version\n\n    cmd_line_template = ""sphinx-apidoc -f -o {outputdir} {moduledir}""\n    cmd_line = cmd_line_template.format(outputdir=output_dir, moduledir=module_dir)\n\n    args = cmd_line.split("" "")\n    if parse_version(sphinx.__version__) >= parse_version(\'1.7\'):\n        args = args[1:]\n\n    apidoc.main(args)\nexcept Exception as e:\n    print(""Running `sphinx-apidoc` failed!\\n{}"".format(e))\n\n# -- General configuration -----------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named \'sphinx.ext.*\') or your custom ones.\nextensions = [\'sphinx.ext.autodoc\', \'sphinx.ext.intersphinx\', \'sphinx.ext.todo\',\n              \'sphinx.ext.autosummary\', \'sphinx.ext.viewcode\', \'sphinx.ext.coverage\',\n              \'sphinx.ext.doctest\', \'sphinx.ext.ifconfig\', \'sphinx.ext.mathjax\',\n              \'sphinx.ext.napoleon\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix of source filenames.\nsource_suffix = [\'.rst\', \'.md\']\n\n# The encoding of source files.\n# source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'SALib\'\ncopyright = u\'2019, Jon Herman, Will Usher and others\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'\'  # Is set by calling `setup.py docs`\n# The full version, including alpha/beta/rc tags.\nrelease = \'\'  # Is set by calling `setup.py docs`\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n# language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\'_build\']\n\n# The reST default role (used for this markup: `text`) to use for all documents.\n# default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n# keep_warnings = False\n\n\n# -- Options for HTML output ---------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\nhtml_theme_options = {\n    \'sidebar_width\': \'300px\',\n    \'page_width\': \'1200px\'\n}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\ntry:\n    from SALib import __version__ as version\nexcept ImportError:\n    pass\nelse:\n    release = version\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n# html_logo = """"\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n# html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n# html_domain_indices = True\n\n# If false, no index is generated.\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n# html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n# html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n# html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'salib-doc\'\n\n\n# -- Options for LaTeX output --------------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n# \'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n# \'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n# \'preamble\': \'\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n  (\'index\', \'user_guide.tex\', u\'SALib Documentation\',\n   u\'Jon Herman, Will Usher and others\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = """"\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n\n# If false, no module index is generated.\n# latex_domain_indices = True\n\n# -- External mapping ------------------------------------------------------------\npython_version = \'.\'.join(map(str, sys.version_info[0:2]))\nintersphinx_mapping = {\n    \'sphinx\': (\'http://www.sphinx-doc.org/en/stable\', None),\n    \'python\': (\'https://docs.python.org/\' + python_version, None),\n    \'matplotlib\': (\'https://matplotlib.org\', None),\n    \'numpy\': (\'https://docs.scipy.org/doc/numpy\', None),\n    \'sklearn\': (\'http://scikit-learn.org/stable\', None),\n    \'pandas\': (\'http://pandas.pydata.org/pandas-docs/stable\', None),\n    \'scipy\': (\'https://docs.scipy.org/doc/scipy/reference\', None),\n}\n'"
tests/__init__.py,0,b''
tests/conftest.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import print_function, absolute_import, division\n\nimport pytest\nimport tempfile\n\ndef make_temporary_file():\n    """""" Returns a temporary file name\n\n    Returns\n    =========\n    openfile.name : str\n        Name of the temporary file\n    """"""\n    with tempfile.NamedTemporaryFile() as openfile:\n        return openfile.name\n\n\n@pytest.fixture(scope=\'function\')\ndef setup_function():\n    filename = make_temporary_file()\n    with open(filename, ""w+"") as ofile:\n        ofile.write(""Test1 0.0 100.0\\n"")\n        ofile.write(""Test2 5.0 51.0\\n"")\n    return filename\n\n\n@pytest.fixture(scope=\'function\')\ndef setup_param_file():\n    filename = make_temporary_file()\n    with open(filename, ""w"") as ofile:\n        ofile.write(""Test 1,0,1.0\\n"")\n        ofile.write(""Test 2,0,1.0\\n"")\n        ofile.write(""Test 3,0,1.0\\n"")\n    return filename\n\n\n@pytest.fixture(scope=\'function\')\ndef setup_param_file_with_groups():\n    filename = make_temporary_file()\n    with open(filename, ""w"") as ofile:\n        ofile.write(""Test 1,0,1.0,Group 1\\n"")\n        ofile.write(""Test 2,0,1.0,Group 1\\n"")\n        ofile.write(""Test 3,0,1.0,Group 2\\n"")\n    return filename\n\n\n@pytest.fixture(scope=\'function\')\ndef setup_param_groups_prime():\n    filename = make_temporary_file()\n    with open(filename, ""w"") as ofile:\n        ofile.write(""Test 1,0,1.0,Group 1\\n"")\n        ofile.write(""Test 2,0,1.0,Group 2\\n"")\n        ofile.write(""Test 3,0,1.0,Group 2\\n"")\n        ofile.write(""Test 4,0,1.0,Group 3\\n"")\n        ofile.write(""Test 5,0,1.0,Group 3\\n"")\n        ofile.write(""Test 6,0,1.0,Group 3\\n"")\n        ofile.write(""Test 7,0,1.0,Group 3\\n"")\n    return filename\n'"
tests/test_analyze_morris.py,52,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division\n\nfrom pytest import raises\nfrom numpy.testing import assert_allclose, assert_equal\n\nimport numpy as np\n\nfrom SALib.analyze.morris import analyze, \\\n    compute_mu_star_confidence, \\\n    compute_elementary_effects, \\\n    get_increased_values, \\\n    get_decreased_values, \\\n    compute_grouped_metric, \\\n    compute_grouped_sigma\n\n\ndef test_compute_mu_star_confidence():\n    \'\'\'\n    Tests that compute mu_star_confidence is computed correctly\n    \'\'\'\n\n    ee = np.array([2.52, 2.01, 2.30, 0.66, 0.93, 1.3], dtype=np.float)\n    num_trajectories = 6\n    num_resamples = 1000\n    conf_level = 0.95\n\n    actual = compute_mu_star_confidence(\n        ee, num_trajectories, num_resamples, conf_level)\n    expected = 0.5\n    assert_allclose(actual, expected, atol=1e-01)\n\n\ndef test_analysis_of_morris_results():\n    \'\'\'\n    Tests a one-dimensional vector of results\n\n    Taken from the solution to Exercise 4 (p.138) in Saltelli (2008).\n    \'\'\'\n    model_input = np.array([[0, 1. / 3], [0, 1], [2. / 3, 1],\n                            [0, 1. / 3], [2. / 3, 1. / 3], [2. / 3, 1],\n                            [2. / 3, 0], [2. / 3, 2. / 3], [0, 2. / 3],\n                            [1. / 3, 1], [1, 1], [1, 1. / 3],\n                            [1. / 3, 1], [1. / 3, 1. / 3], [1, 1. / 3],\n                            [1. / 3, 2. / 3], [1. / 3, 0], [1, 0]],\n                           dtype=np.float)\n\n    model_output = np.array([0.97, 0.71, 2.39, 0.97, 2.30, 2.39,\n                             1.87, 2.40, 0.87, 2.15, 1.71, 1.54,\n                             2.15, 2.17, 1.54, 2.20, 1.87, 1.0],\n                            dtype=np.float)\n\n    problem = {\n        \'num_vars\': 2,\n        \'names\': [\'Test 1\', \'Test 2\'],\n        \'groups\': None,\n        \'bounds\': [[0.0, 1.0], [0.0, 1.0]]\n    }\n\n    Si = analyze(problem, model_input, model_output,\n                 num_resamples=1000,\n                 conf_level=0.95,\n                 print_to_console=False)\n\n    desired_mu = np.array([0.66, 0.21])\n    assert_allclose(Si[\'mu\'], desired_mu, rtol=1e-1,\n                    err_msg=""The values for mu are incorrect"")\n    desired_mu_star = np.array([1.62, 0.35])\n    assert_allclose(Si[\'mu_star\'], desired_mu_star, rtol=1e-2,\n                    err_msg=""The values for mu star are incorrect"")\n    desired_sigma = np.array([1.79, 0.41])\n    assert_allclose(Si[\'sigma\'], desired_sigma, rtol=1e-2,\n                    err_msg=""The values for sigma are incorrect"")\n    desired_names = [\'Test 1\', \'Test 2\']\n    assert_equal(Si[\'names\'], desired_names,\n                 err_msg=""The values for names are incorrect"")\n\n\ndef test_conf_level_within_zero_one_bounds():\n    ee = [0, 0, 0]\n    N = 1\n    num_resamples = 2\n    conf_level_too_low = -1\n    with raises(ValueError):\n        compute_mu_star_confidence(ee, N, num_resamples, conf_level_too_low)\n    conf_level_too_high = 2\n    with raises(ValueError):\n        compute_mu_star_confidence(ee, N, num_resamples, conf_level_too_high)\n\n\ndef test_compute_elementary_effects():\n    \'\'\'\n    Inputs for elementary effects taken from Exercise 5 from Saltelli (2008).\n    See page 140-145.\n    `model_inputs` are from trajectory t_1 from table 3.10 on page 141.\n    `desired` is equivalent to column t_1 in table 3.12 on page 145.\n    \'\'\'\n    model_inputs = np.array([\n                            [1.64, -1.64, -1.64, 0.39, -0.39, 0.39, -1.64,\n                             -1.64, -0.39, -0.39, 1.64, 1.64, -0.39, 0.39,\n                             1.64],\n                            [1.64, -1.64, -1.64, 0.39, -0.39, -1.64, -1.64,\n                             -1.64, -0.39, -0.39, 1.64, 1.64, -0.39, 0.39,\n                             1.64],\n                            [1.64, -1.64, -1.64, 0.39, -0.39, -1.64, -1.64,\n                             -1.64, 1.64, -0.39, 1.64, 1.64, -0.39, 0.39,\n                             1.64],\n                            [1.64, -1.64, -1.64, 0.39, -0.39, -1.64, -1.64,\n                             0.39, 1.64, -0.39, 1.64, 1.64, -0.39, 0.39, 1.64],\n                            [1.64, -1.64, -1.64, -1.64, -0.39, -1.64, -1.64,\n                             0.39, 1.64, -0.39, 1.64, 1.64, -0.39, 0.39, 1.64],\n                            [1.64, -1.64, -1.64, -1.64, -0.39, -1.64, -1.64,\n                             0.39, 1.64, -0.39, 1.64, -0.39, -0.39, 0.39,\n                             1.64],\n                            [1.64, -1.64, -1.64, -1.64, -0.39, -1.64, -1.64,\n                             0.39, 1.64, -0.39, 1.64, -0.39, -0.39, -1.64,\n                             1.64],\n                            [1.64, 0.39, -1.64, -1.64, -0.39, -1.64, -1.64,\n                             0.39, 1.64, -0.39, 1.64, -0.39, -0.39, -1.64,\n                             1.64],\n                            [1.64, 0.39, -1.64, -1.64, -0.39, -1.64, -1.64,\n                             0.39, 1.64, -0.39, 1.64, -0.39, -0.39, -1.64,\n                             -0.39],\n                            [1.64, 0.39, -1.64, -1.64, -0.39, -1.64, -1.64,\n                             0.39, 1.64, 1.64, 1.64, -0.39, -0.39, -1.64,\n                             -0.39],\n                            [1.64, 0.39, -1.64, -1.64, 1.64, -1.64, -1.64,\n                             0.39,\n                                1.64, 1.64, 1.64, -0.39, -0.39, -1.64, -0.39],\n                            [1.64, 0.39, -1.64, -1.64, 1.64, -1.64, -1.64,\n                             0.39,\n                                1.64, 1.64, -0.39, -0.39, -0.39, -1.64, -0.39],\n                            [1.64, 0.39, -1.64, -1.64, 1.64, -1.64, 0.39, 0.39,\n                                1.64, 1.64, -0.39, -0.39, -0.39, -1.64, -0.39],\n                            [1.64, 0.39, 0.39, -1.64, 1.64, -1.64, 0.39, 0.39,\n                                1.64, 1.64, -0.39, -0.39, -0.39, -1.64, -0.39],\n                            [-0.39, 0.39, 0.39, -1.64, 1.64, -1.64, 0.39, 0.39,\n                                1.64, 1.64, -0.39, -0.39, -0.39, -1.64, -0.39],\n                            [-0.39, 0.39, 0.39, -1.64, 1.64, -1.64, 0.39, 0.39,\n                             1.64, 1.64, -0.39, -0.39, 1.64, -1.64, -0.39]],\n                            dtype=np.float)\n    model_outputs = np.array([24.9, 22.72, 21.04, 16.01, 10.4, 10.04, 8.6,\n                              13.39, 4.69, 8.02, 9.98, 3.75, 1.33, 2.59,\n                              6.37, 9.99],\n                             dtype=np.float)\n    delta = 2. / 3\n\n    actual = compute_elementary_effects(model_inputs, model_outputs, 16, delta)\n    desired = np.array([[-5.67], [7.18], [1.89], [8.42],\n                        [2.93], [3.28], [-3.62], [-7.55],\n                        [-2.51], [5.00], [9.34], [0.54],\n                        [5.43], [2.15], [13.05]],\n                       dtype=np.float)\n    assert_allclose(actual, desired, atol=1e-1)\n\n\ndef test_compute_grouped_elementary_effects():\n\n    model_inputs = np.array([[.39, -.39, -1.64, 0.39, -0.39, -0.39, 0.39, 0.39,\n                              -1.64, -0.39, 0.39, -1.64, 1.64, 1.64, 1.64],\n                             [-1.64, 1.64, 0.39, -1.64, 1.64, 1.64, -1.64,\n                              -1.64, -1.64, 1.64, 0.39, -1.64, 1.64, 1.64,\n                              1.64],\n                             [-1.64, 1.64, 0.39, -1.64, 1.64, 1.64, -1.64,\n                              -1.64, 0.39, 1.64, -1.64, 0.39, -.39, -.39, -.39]\n                             ])\n\n    model_results = np.array([13.85, -10.11, 1.12])\n\n    problem = {\'names\': [\'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\',\n                         \'9\', \'10\', \'11\', \'12\', \'13\', \'14\', \'15\'],\n               \'bounds\': [[]],\n               \'groups\':\n               (np.array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1],\n                          [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0]]),\n                [\'gp1\', \'gp2\']),\n               \'num_vars\': 15\n               }\n    ee = compute_elementary_effects(model_inputs, model_results, 3, 2. / 3)\n    mu_star = np.average(np.abs(ee), axis=1)\n    actual = compute_grouped_metric(mu_star, problem[\'groups\'][0].T)\n    desired = np.array([16.86, 35.95])\n    assert_allclose(actual, desired, atol=1e-1)\n\n\ndef test_compute_elementary_effects_small():\n    \'\'\'\n    Computes elementary effects for two variables,\n    over six trajectories with four levels.\n    \'\'\'\n    model_inputs = np.array([[0, 1. / 3], [0, 1], [2. / 3, 1],\n                             [0, 1. / 3], [2. / 3, 1. / 3], [2. / 3, 1],\n                             [2. / 3, 0], [2. / 3, 2. / 3], [0, 2. / 3],\n                             [1. / 3, 1], [1, 1], [1, 1. / 3],\n                             [1. / 3, 1], [1. / 3, 1. / 3], [1, 1. / 3],\n                             [1. / 3, 2. / 3], [1. / 3, 0], [1, 0]],\n                            dtype=np.float)\n\n    model_outputs = np.array([0.97, 0.71, 2.39, 0.97, 2.3, 2.39, 1.87, 2.40,\n                              0.87, 2.15, 1.71, 1.54, 2.15, 2.17, 1.54, 2.2,\n                              1.87, 1.0],\n                             dtype=np.float)\n\n    delta = 2. / 3\n    actual = compute_elementary_effects(model_inputs, model_outputs, 3, delta)\n    desired = np.array(\n        [[2.52, 2.01, 2.30, -0.66, -0.93, -1.30],\n         [-0.39, 0.13, 0.80, 0.25, -0.02, 0.51]])\n    assert_allclose(actual, desired, atol=1e-0)\n\n\ndef test_compute_increased_value_for_ee():\n    up = np.array([[[False, True], [True, False]],\n                   [[True, False], [False, True]],\n                   [[False, True], [False, False]],\n                   [[True, False], [False, False]],\n                   [[False, False], [True, False]],\n                   [[False, False], [True, False]]],\n                  dtype=bool)\n\n    lo = np.array([[[False, False], [False, False]],\n                   [[False, False], [False, False]],\n                   [[False, False], [True, False]],\n                   [[False, False], [False, True]],\n                   [[False, True], [False, False]],\n                   [[False, True], [False, False]]],\n                  dtype=bool)\n\n    model_outputs = np.array([0.97, 0.71, 2.39, 0.97, 2.3, 2.39, 1.87, 2.40,\n                              0.87, 2.15, 1.71, 1.54, 2.15, 2.17, 1.54, 2.2,\n                              1.87, 1.0],\n                             dtype=np.float)\n    op_vec = model_outputs.reshape(6, 3)\n    actual = get_increased_values(op_vec, up, lo)\n    desired = np.array([[2.39, 2.3, 2.4, 1.71, 1.54, 1.0],\n                        [0.71, 2.39, 2.40, 1.71, 2.15, 2.20]],\n                       dtype=np.float)\n    assert_allclose(actual, desired, atol=1e-1)\n\n\ndef test_compute_decreased_value_for_ee():\n    up = np.array([[[False, True], [True, False]],\n                   [[True, False], [False, True]],\n                   [[False, True], [False, False]],\n                   [[True, False], [False, False]],\n                   [[False, False], [True, False]],\n                   [[False, False], [True, False]]],\n                  dtype=bool)\n\n    lo = np.array([[[False, False], [False, False]],\n                   [[False, False], [False, False]],\n                   [[False, False], [True, False]],\n                   [[False, False], [False, True]],\n                   [[False, True], [False, False]],\n                   [[False, True], [False, False]]],\n                  dtype=bool)\n\n    model_outputs = np.array([0.97, 0.71, 2.39, 0.97, 2.3, 2.39, 1.87, 2.40,\n                              0.87, 2.15, 1.71, 1.54, 2.15, 2.17, 1.54, 2.2,\n                              1.87, 1.0],\n                             dtype=np.float)\n    op_vec = model_outputs.reshape(6, 3)\n    actual = get_decreased_values(op_vec, up, lo)\n    desired = np.array([[0.71, 0.97, 0.87, 2.15, 2.17, 1.87],\n                        [0.97, 2.30, 1.87, 1.54, 2.17, 1.87]],\n                       dtype=np.float)\n    assert_allclose(actual, desired, atol=1e-1)\n\n\ndef test_compute_grouped_mu_star():\n    \'\'\'\n    Computes mu_star for 3 variables grouped into 2 groups\n    There are six trajectories.\n    \'\'\'\n    group_matrix = np.array([[1, 0], [0, 1], [0, 1]], dtype=np.int)\n    ee = np.array([[2.52, 2.01, 2.30, -0.66, -0.93, -1.30],\n                   [-2.00, 0.13, -0.80, 0.25, -0.02, 0.51],\n                   [2.00, -0.13, 0.80, -0.25, 0.02, -0.51]])\n    mu_star = np.average(np.abs(ee), 1)\n    actual = compute_grouped_metric(mu_star, group_matrix)\n    desired = np.array([1.62, 0.62], dtype=np.float)\n    assert_allclose(actual, desired, rtol=1e-1)\n\n\ndef test_sigma_returned_for_groups_with_only_one_param():\n    \'\'\'\n    Tests that a value for sigma is returned when the group contains 1 param\n\n    Morris groups do not allow a value for sigma to be computed because it\n    requires the use of mu (as opposed to mu_star).\n\n    However, if the group consists of just one parameter, then the sampling\n    will be identical to the situation in which no groups are used,\n    but only for that parameter.\n\n    An NA should be returned for all other groups (as opposed to 0, which could\n    confuse plotting.morris)\n    \'\'\'\n    group_matrix = np.array([[1, 0], [0, 1], [0, 1]], dtype=np.int)\n    ee = np.array([[2.52, 2.01, 2.30, -0.66, -0.93, -1.30],\n                   [-2.00, 0.13, -0.80, 0.25, -0.02, 0.51],\n                   [2.00, -0.13, 0.80, -0.25, 0.02, -0.51]])\n    sigma = np.std(ee, axis=1, ddof=1)\n    actual = compute_grouped_sigma(sigma, group_matrix)\n    desired = np.array([1.79352911, np.NAN], dtype=np.float)\n    assert_allclose(actual, desired, rtol=1e-1)\n\n\ndef test_raise_error_if_not_floats():\n\n    inputs = np.array([[0, 1. / 3], [0, 1], [2. / 3, 1],\n                       [0, 1. / 3], [2. / 3, 1. / 3], [2. / 3, 1],\n                       [2. / 3, 0], [2. / 3, 2. / 3], [0, 2. / 3],\n                       [1. / 3, 1], [1, 1], [1, 1. / 3],\n                       [1. / 3, 1], [1. / 3, 1. / 3], [1, 1. / 3],\n                       [1. / 3, 2. / 3], [1. / 3, 0], [1, 0]],\n                      dtype=np.int)\n\n    outputs = np.array([0.97, 0.71, 2.39, 0.97, 2.30, 2.39,\n                        1.87, 2.40, 0.87, 2.15, 1.71, 1.54,\n                        2.15, 2.17, 1.54, 2.20, 1.87, 1.0],\n                       dtype=np.int)\n\n    problem = {\n        \'num_vars\': 2,\n        \'names\': [\'Test 1\', \'Test 2\'],\n        \'groups\': None,\n        \'bounds\': [[0.0, 1.0], [0.0, 1.0]]\n    }\n\n    with raises(ValueError):\n        analyze(problem, inputs, outputs)\n\n\ndef test_doesnot_raise_error_if_floats():\n\n    inputs = np.array([[0, 1. / 3], [0, 1], [2. / 3, 1],\n                       [0, 1. / 3], [2. / 3, 1. / 3], [2. / 3, 1],\n                       [2. / 3, 0], [2. / 3, 2. / 3], [0, 2. / 3],\n                       [1. / 3, 1], [1, 1], [1, 1. / 3],\n                       [1. / 3, 1], [1. / 3, 1. / 3], [1, 1. / 3],\n                       [1. / 3, 2. / 3], [1. / 3, 0], [1, 0]],\n                      dtype=np.float32)\n\n    outputs = np.array([0.97, 0.71, 2.39, 0.97, 2.30, 2.39,\n                        1.87, 2.40, 0.87, 2.15, 1.71, 1.54,\n                        2.15, 2.17, 1.54, 2.20, 1.87, 1.0],\n                       dtype=np.float32)\n\n    problem = {\n        \'num_vars\': 2,\n        \'names\': [\'Test 1\', \'Test 2\'],\n        \'groups\': None,\n        \'bounds\': [[0.0, 1.0], [0.0, 1.0]]\n    }\n\n    analyze(problem, inputs, outputs)\n'"
tests/test_cli.py,0,"b'import subprocess\nimport importlib\nfrom SALib.util import avail_approaches\n\n\ndef test_cli_usage():\n    cmd = [""salib""]\n    try:\n        out = subprocess.check_output(cmd,\n                                      stderr=subprocess.STDOUT,\n                                      shell=True,\n                                      universal_newlines=True)\n    except subprocess.CalledProcessError as e:\n        pass\n    else:\n        # if no error raised, check the returned string\n        assert len(out) > 0 and ""usage"" in out.lower(), \\\n            ""Incorrect message returned from utility""\n\n\ndef test_cli_avail_methods():\n    method_types = [\'sample\', \'analyze\']\n\n    for method in method_types:\n        module = importlib.import_module(\'.\'.join([\'SALib\', method]))\n        actions = avail_approaches(module)\n        for act in actions:\n            approach = importlib.import_module(\'.\'.join(\n                [\'SALib\', method, act]))\n\n            # Just try to access the functions - raises error on failure\n            approach.cli_parse\n            approach.cli_action\n\n\nif __name__ == \'__main__\':\n    test_cli_usage()\n    test_cli_avail_methods()\n'"
tests/test_cli_analyze.py,14,"b'import sys\nimport subprocess\nfrom SALib.test_functions import Ishigami\nimport numpy as np\nimport re\n\nsalib_cli = ""./src/SALib/scripts/salib.py""\nishigami_fp = ""./src/SALib/test_functions/params/Ishigami.txt""\n\nif sys.version_info[0] == 2:\n    subprocess.run = subprocess.call\n\n\ndef test_delta():\n    cmd = ""python {cli} sample saltelli -p {fn} -o model_input.txt -n 1000""\\\n          .format(cli=salib_cli, fn=ishigami_fp) +\\\n          "" --precision 8 --max-order 2 --seed=100""\n    subprocess.run(cmd.split())\n\n    # Run model and save output\n    np.savetxt(\'model_output.txt\', Ishigami.evaluate(\n        np.loadtxt(\'model_input.txt\')))\n\n    analyze_cmd = ""python {cli} analyze delta -p {fn} -X model_input.txt \\\n    -Y model_output.txt -c 0 -r 10 --seed=100"".format(cli=salib_cli,\n                                                      fn=ishigami_fp).split()\n\n    result = subprocess.check_output(analyze_cmd, universal_newlines=True)\n    result = re.sub(r\'[\\n\\t\\s]*\', \'\', result)\n\n    expected_output = \'Parameterdeltadelta_confS1S1_confx10.2104780.0060910.3113620.012291x20.3540230.0062380.4283650.017972x30.1609860.0047180.0011110.002995\'\n    assert len(result) > 0 and result in expected_output, \\\n        ""Results did not match expected values:\\n\\n Expected: \\n{} \\n\\n Got: \\n{}"".format(\n            expected_output, result)\n\n\ndef test_dgsm():\n    # Generate inputs\n    cmd = ""python {cli} sample finite_diff -p {fn} -o model_input.txt -d 0.001\\\n    --precision=8 -n 1000 --seed=100"".format(cli=salib_cli,\n                                             fn=ishigami_fp).split()\n    subprocess.run(cmd)\n\n    # Run model and save output\n    np.savetxt(\'model_output.txt\', Ishigami.evaluate(\n        np.loadtxt(\'model_input.txt\')))\n\n    analyze_cmd = ""python {cli} analyze dgsm -p {fn} -X model_input.txt\\\n    -Y model_output.txt -c 0 -r 1000 --seed=100""\\\n     .format(cli=salib_cli, fn=ishigami_fp).split()\n\n    # run analysis and use regex to strip all whitespace from result\n    result = subprocess.check_output(analyze_cmd, universal_newlines=True)\n    result = re.sub(r\'[\\n\\t\\s]*\', \'\', result)\n\n    expected = ""Parametervivi_stddgsmdgsm_confx17.62237816.1981232.2075541.034173x224.48775717.3385567.0920191.090835x311.18125824.0621273.2382591.477114""\n\n    assert len(result) > 0 and result == expected, \\\n        ""Unexpected DGSM results.\\n\\nExpected:\\n{}\\n\\nGot:{}""\\\n        .format(expected, result)\n\n\ndef test_fast():\n    # Generate inputs\n    cmd = ""python {cli} sample fast_sampler -p {fn} -o model_input.txt \\\n    --precision=8 -n 1000 -M 4 --seed=100"".format(cli=salib_cli,\n                                                  fn=ishigami_fp).split()\n    subprocess.run(cmd)\n\n    # Run model and save output\n    np.savetxt(\'model_output.txt\', Ishigami.evaluate(\n        np.loadtxt(\'model_input.txt\')))\n\n    analyze_cmd = ""python {cli} analyze fast -p {fn} \\\n    -Y model_output.txt -c 0 --seed=100""\\\n     .format(cli=salib_cli, fn=ishigami_fp).split()\n\n    # run analysis and use regex to strip all whitespace from result\n    result = subprocess.check_output(analyze_cmd, universal_newlines=True)\n    result = re.sub(r\'[\\n\\t\\s]*\', \'\', result)\n\n    expected = ""ParameterFirstTotalx10.3104030.555603x20.4425530.469546x30.0000000.239155""\n\n    assert len(result) > 0 and result == expected, \\\n        ""Unexpected FAST results.\\n\\nExpected:\\n{}\\n\\nGot:{}""\\\n        .format(expected, result)\n\n\ndef test_ff():\n    # Generate inputs\n    cmd = ""python {cli} sample ff -p {fn} -o model_input.txt \\\n    --precision=8 -n 1000 --seed=100"".format(cli=salib_cli,\n                                             fn=ishigami_fp).split()\n    subprocess.run(cmd)\n\n    # Run model and save output\n    np.savetxt(\'model_output.txt\', Ishigami.evaluate(\n        np.loadtxt(\'model_input.txt\')))\n\n    analyze_cmd = ""python {cli} analyze ff -p {fn} -X model_input.txt\\\n    -Y model_output.txt -c 0 --seed=100""\\\n     .format(cli=salib_cli, fn=ishigami_fp).split()\n\n    # run analysis and use regex to strip all whitespace from result\n    result = subprocess.check_output(analyze_cmd, universal_newlines=True)\n    result = re.sub(r\'[\\n\\t\\s]*\', \'\', result)\n\n    expected = ""ParameterMEx10.000000x20.000000x30.000000dummy_00.000000(\'x1\',\'x2\')0.000000(\'x1\',\'x3\')0.000000(\'x2\',\'x3\')0.000000(\'x1\',\'dummy_0\')0.000000(\'x2\',\'dummy_0\')0.000000(\'x3\',\'dummy_0\')0.000000""\n\n    assert len(result) > 0 and result == expected, \\\n        ""Unexpected FF results.\\n\\nExpected:\\n{}\\n\\nGot:{}""\\\n        .format(expected, result)\n\n\ndef test_morris():\n\n    # Generate inputs\n    cmd = ""python {cli} sample morris -p {fn} -o model_input.txt -n 100\\\n    --precision=8 --levels=10 --seed=100 -lo False""\\\n    .format(cli=salib_cli, fn=ishigami_fp).split()\n\n    subprocess.run(cmd)\n\n    # Run model and save output\n    np.savetxt(\'model_output.txt\', Ishigami.evaluate(\n        np.loadtxt(\'model_input.txt\')))\n\n    # run analysis\n    analyze_cmd = ""python {cli} analyze morris -p {fn} -X model_input.txt\\\n    -Y model_output.txt -c 0 -r 1000 -l 10 --seed=100""\\\n     .format(cli=salib_cli, fn=ishigami_fp).split()\n\n    result = subprocess.check_output(analyze_cmd, universal_newlines=True)\n    result = re.sub(r\'[\\n\\t\\s]*\', \'\', result)\n\n    expected_output = """"""ParameterMu_StarMuMu_Star_ConfSigmax17.4997.4991.8019.330x22.215-0.4700.3482.776x35.4240.8641.1487.862""""""\n\n    assert len(result) > 0 and result == expected_output, \\\n        ""Results did not match expected values:\\n\\n Expected: \\n{} \\n\\n Got: \\n{}"".format(\n            expected_output, result)\n\n\ndef test_rbd_fast():\n    # Generate inputs\n    cmd = ""python {cli} sample ff -p {fn} -o model_input.txt \\\n    --precision=8 --seed=100"".format(cli=salib_cli, fn=ishigami_fp).split()\n\n    subprocess.run(cmd)\n\n    # Run model and save output\n    np.savetxt(\'model_output.txt\', Ishigami.evaluate(\n        np.loadtxt(\'model_input.txt\')))\n\n    analyze_cmd = ""python {cli} analyze rbd_fast -p {fn} -X model_input.txt\\\n    -Y model_output.txt --seed=100""\\\n     .format(cli=salib_cli, fn=ishigami_fp).split()\n\n    # run analysis and use regex to strip all whitespace from result\n    result = subprocess.check_output(analyze_cmd, universal_newlines=True)\n    result = re.sub(r\'[\\n\\t\\s]*\', \'\', result)\n\n    expected = ""ParameterFirstx10.39223x20.299578x30.0342307""\n\n    assert len(result) > 0 and result == expected, \\\n        ""Unexpected RBD-FAST results.\\n\\nExpected:\\n{}\\n\\nGot:{}""\\\n        .format(expected, result)\n\n\ndef test_sobol():\n    # Generate inputs\n    cmd = ""python {cli} sample saltelli -p {fn} -o model_input.txt -n 1000\\\n    --precision 8 --max-order 2 --seed=100"".format(cli=salib_cli,\n                                                   fn=ishigami_fp)\n    cmd = cmd.split()\n\n    result = subprocess.check_output(cmd, universal_newlines=True)\n    np.savetxt(\'model_output.txt\', Ishigami.evaluate(\n        np.loadtxt(\'model_input.txt\')))\n\n    analyze_cmd = ""python {cli} analyze sobol -p {fn}\\\n    -Y model_output.txt -c 0 --max-order 2\\\n    -r 1000 --seed=100"".format(cli=salib_cli, fn=ishigami_fp).split()\n\n    result = subprocess.check_output(analyze_cmd, universal_newlines=True)\n    result = re.sub(r\'[\\n\\t\\s]*\', \'\', result)\n\n    expected_output = \'ParameterS1S1_confSTST_confx10.3079750.0630470.5601370.091908x20.4477670.0533230.4387220.040634x3-0.0042550.0596670.2428450.026578Parameter_1Parameter_2S2S2_confx1x20.0122050.086177x1x30.2515260.108147x2x3-0.0099540.065569\'\n    assert len(result) > 0 and result == expected_output, \\\n        ""Results did not match expected values:\\n\\n Expected: \\n{} \\n\\n Got: \\n{}"".format(\n            expected_output, result)\n\n\nif __name__ == \'__main__\':\n    test_delta()\n    test_dgsm()\n    test_fast()\n    test_ff()\n    test_morris()\n    test_rbd_fast()\n    test_sobol()\n'"
tests/test_cli_sample.py,0,"b'from os.path import join as pth_join\nimport subprocess\n\nsalib_cli = ""./src/SALib/scripts/salib.py""\nishigami_fp = ""./src/SALib/test_functions/params/Ishigami.txt""\ntest_data = pth_join(\'tests\', \'data\', \'test.txt\')\n\n\ndef test_cli_entry():\n    cmd = \'python {cli} -h\'.format(cli=salib_cli).split()\n    result = subprocess.check_output(cmd)\n    assert \'Errno\' not in str(result), ""Error occurred when trying to use CLI!""\n\n\ndef test_ff():\n    cmd = ""python {cli} sample ff -p {fn} -o {test_data} -n 100"".format(\n        cli=salib_cli,\n        fn=ishigami_fp,\n        test_data=test_data).split()\n    result = subprocess.check_output(cmd)\n    assert len(result) == 0, ""Error occurred!""\n\n\ndef test_fast():\n    cmd = ""python {cli} sample fast_sampler -p {fn} -o {test_data} -n 100"".format(\n        cli=salib_cli,\n        fn=ishigami_fp,\n        test_data=test_data).split()\n    result = subprocess.check_output(cmd)\n    assert len(result) == 0, ""Error occurred!""\n\n\ndef test_finite_diff():\n    cmd = ""python {cli} sample finite_diff -p {fn} -o {test_data} -n 100"".format(\n        cli=salib_cli,\n        fn=ishigami_fp,\n        test_data=test_data).split()\n    result = subprocess.check_output(cmd)\n    assert len(result) == 0, ""Error occurred!""\n\n\ndef test_latin():\n    cmd = ""python {cli} sample latin -p {fn} -o {test_data} -n 100"".format(\n        cli=salib_cli,\n        fn=ishigami_fp,\n        test_data=test_data).split()\n    result = subprocess.check_output(cmd)\n    assert len(result) == 0, ""Error occurred!""\n\n\ndef test_saltelli():\n    cmd = ""python {cli} sample latin -p {fn} -o {test_data} -n 100"".format(\n        cli=salib_cli,\n        fn=ishigami_fp,\n        test_data=test_data).split()\n    result = subprocess.check_output(cmd)\n    assert len(result) == 0, ""Error occurred!""\n\n\nif __name__ == \'__main__\':\n    test_cli_entry()\n    test_ff()\n    test_fast()\n    test_finite_diff()\n    test_latin()\n    test_saltelli()\n'"
tests/test_ff.py,25,"b'\'\'\'\nCreated on 30 Jun 2015\n\n@author: @willu47\n\'\'\'\nimport numpy as np\nfrom numpy.testing import assert_equal, assert_allclose\nfrom SALib.sample.ff import sample, find_smallest, extend_bounds\nfrom SALib.analyze.ff import analyze, interactions\n\n\ndef test_find_smallest():\n    \'\'\'\n    \'\'\'\n    num_vars = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 16, 17, 31, 32, 33]\n    expected = [0, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 6]\n    for x, y in zip(num_vars, expected):\n        actual = find_smallest(x)\n        assert_equal(actual, y)\n\n\ndef test_extend_bounds():\n    problem = {\'bounds\': np.repeat([-1, 1], 12).reshape(2, 12).T,\n           \'num_vars\': 12,\n           \'names\': [""x"" + str(x + 1) for x in range(12)]\n           }\n    actual = extend_bounds(problem)\n    expected = {\'names\': [\'x1\', \'x2\', \'x3\', \'x4\',\n                          \'x5\', \'x6\', \'x7\', \'x8\',\n                          \'x9\', \'x10\', \'x11\', \'x12\',\n                          \'dummy_0\', \'dummy_1\', \'dummy_2\', \'dummy_3\'],\n                \'bounds\': [np.array([-1,  1]), np.array([-1,  1]),\n                           np.array([-1,  1]), np.array([-1,  1]),\n                           np.array([-1,  1]), np.array([-1,  1]),\n                           np.array([-1,  1]), np.array([-1,  1]),\n                           np.array([-1,  1]), np.array([-1,  1]),\n                           np.array([-1,  1]), np.array([-1,  1]),\n                           np.array([0, 1]), np.array([0, 1]),\n                           np.array([0, 1]), np.array([0, 1])],\n                \'num_vars\': 16}\n\n    assert_equal(actual, expected)\n\ndef test_ff_sample():\n    problem = {\'bounds\': [[0., 1.], [0., 1.], [0., 1.], [0., 1.]],\n               \'num_vars\': 4,\n               \'names\': [\'x1\', \'x2\', \'x3\', \'x4\']}\n    actual = sample(problem)\n    expected = np.array([[ 1, 1, 1, 1],\n                         [ 1, 0, 1, 0],\n                         [ 1, 1, 0, 0],\n                         [ 1, 0, 0, 1],\n                         [0, 0, 0, 0],\n                         [0, 1, 0, 1],\n                         [0, 0, 1, 1],\n                         [0, 1, 1, 0]], dtype=np.float)\n    assert_equal(actual, expected)\n\n\ndef test_ff_sample_scaled():\n    \'\'\'\n    \'\'\'\n    problem = {\'bounds\': [[0., 2.5], [0., 1.], [0., 1.], [0., 1.]],\n               \'num_vars\': 4,\n               \'names\': [\'x1\', \'x2\', \'x3\', \'x4\']}\n    actual = sample(problem)\n    expected = np.array([[ 2.5, 1, 1, 1],\n                         [ 2.5, 0, 1, 0],\n                         [ 2.5, 1, 0, 0],\n                         [ 2.5, 0, 0, 1],\n                         [0, 0, 0, 0],\n                         [0, 1, 0, 1],\n                         [0, 0, 1, 1],\n                         [0, 1, 1, 0]], dtype=np.float)\n    assert_equal(actual, expected)\n\n\ndef test_ff_analyze():\n    \'\'\'\n    \'\'\'\n\n    problem = {\'bounds\': [[0., 2.5], [0., 1.], [0., 1.], [0., 1.]],\n               \'num_vars\': 4,\n               \'names\': [\'x1\', \'x2\', \'x3\', \'x4\']}\n    X = np.array([[ 1, 1, 1, 1],\n                  [ 1, 0, 1, 0],\n                  [ 1, 1, 0, 0],\n                  [ 1, 0, 0, 1],\n                  [0, 0, 0, 0],\n                  [0, 1, 0, 1],\n                  [0, 0, 1, 1],\n                  [0, 1, 1, 0]], dtype=np.float)\n    Y = np.array([1.5, 1, 1.5, 1, 2, 2.5, 2, 2.5], dtype=np.float)\n    actual = analyze(problem, X, Y)\n    expected = {\'ME\': np.array([ -0.5 ,  0.25,  0.  ,  0.  ]), \'names\': [\'x1\', \'x2\', \'x3\', \'x4\']}\n    assert_equal(actual, expected)\n\n\ndef test_ff_example():\n    \'\'\'\n    \'\'\'\n\n    problem = {\'bounds\': np.repeat([-1, 1], 12).reshape(2, 12).T,\n               \'num_vars\': 12,\n               \'names\': [""x"" + str(x + 1) for x in range(12)]\n               }\n\n    X = sample(problem)\n    Y = X[:, 0] + 2 * X[:, 1] + 3 * X[:, 2] + 4 * X[:, 6] * X[:, 11]\n\n    expected = np.array([10, -2, 4, -8, 2, 6, -4,\n                         0, 2, 6, -4, 0, 10, -2, 4, -8,\n                         - 2, -6, 4, 0, -10, 2, -4, 8,\n                          - 10, 2, -4, 8, -2, -6, 4, 0])\n\n    assert_equal(Y, expected)\n\n    Si = analyze(problem, X, Y)\n\n    expected = np.array([1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=np.float)\n    assert_equal(expected, Si[\'ME\'])\n\n\ndef test_interactions_from_saltelli():\n    \'\'\'\n    \'\'\'\n    problem = {\'bounds\': np.repeat([-1, 1], 12).reshape(2, 12).T,\n               \'num_vars\': 12,\n               \'names\': [""x"" + str(x + 1) for x in range(12)]\n               }\n\n    X = sample(problem)\n\n    Y = np.array([10, -2, 4, -8, 2, 6, -4, 0,\n                   2, 6, -4, 0, 10, -2, 4, -8,\n                  - 2, -6, 4, 0, -10, 2, -4, 8,\n                 - 10, 2, -4, 8, -2, -6, 4, 0])\n\n    Si = analyze(problem, X, Y, second_order=True)\n    actual = Si[\'IE\']\n    expected = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0,\n                0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0,\n                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0,\n                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0,\n                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n\n    assert_equal(actual, expected)\n\n\ndef test_interactions():\n    \'\'\'\n    \'\'\'\n    problem = {\'bounds\': [[0., 2.5], [0., 1.], [0., 1.], [0., 1.]],\n               \'num_vars\': 4,\n               \'names\': [\'x1\', \'x2\', \'x3\', \'x4\']}\n    X = np.array([[ 2.5, 1.0, 1.0, 1.0],\n                  [ 2.5, 0, 1.0, 0],\n                  [ 2.5, 1.0, 0, 0],\n                  [ 2.5, 0, 0, 1.0],\n                  [0, 0, 0, 0],\n                  [0, 1.0, 0, 1.0],\n                  [0, 0, 1.0, 1.0],\n                  [0, 1.0, 1.0, 0]], dtype=np.float)\n    Y = X[:, 0] + (0.1 * X[:, 1]) + ((1.2 * X[:, 2]) * (1.3 + X[:, 3]))\n#     Y = np.array([1.5, 1, 1.5, 1, 2, 2.5, 2, 2.5], dtype=np.float)\n    ie_names, ie = interactions(problem, Y, print_to_console=True)\n    actual = ie\n    assert_allclose(actual, [0.3, 0, 0, 0, 0, 0.3], rtol=1e-4, atol=1e-4)\n'"
tests/test_regression.py,7,"b'from __future__ import division\n\nfrom numpy.testing import assert_allclose\n\nfrom SALib.analyze import delta\nfrom SALib.analyze import dgsm\nfrom SALib.analyze import fast\nfrom SALib.analyze import rbd_fast\nfrom SALib.analyze import sobol\nfrom SALib.sample import fast_sampler\nfrom SALib.sample import finite_diff\nfrom SALib.sample import latin\nfrom SALib.sample import saltelli\nimport numpy as np\n\nfrom SALib.analyze import morris\nfrom SALib.sample.morris import sample\nfrom SALib.test_functions import Ishigami\nfrom SALib.util import read_param_file\n\nfrom pytest import fixture\n\n\n@fixture(scope=\'function\')\ndef set_seed():\n    """"""Sets seeds for random generators so that tests can be repeated\n\n    It is necessary to set seeds for both the numpy.random, and\n    the stdlib.random libraries.\n    """"""\n    seed = 123456\n    np.random.seed(seed)\n\n\nclass TestMorris:\n\n    def test_regression_morris_vanilla(self, set_seed):\n        """"""Note that this is a poor estimate of the Ishigami\n        function.\n        """"""\n        set_seed\n        param_file = \'src/SALib/test_functions/params/Ishigami.txt\'\n        problem = read_param_file(param_file)\n        param_values = sample(problem, 10000, 4,\n                              optimal_trajectories=None)\n\n        Y = Ishigami.evaluate(param_values)\n\n        Si = morris.analyze(problem, param_values, Y,\n                            conf_level=0.95, print_to_console=False,\n                            num_levels=4)\n\n        assert_allclose(Si[\'mu_star\'], [7.536586, 7.875, 6.308785],\n                        atol=0, rtol=1e-5)\n\n    def test_regression_morris_groups(self, set_seed):\n        set_seed\n        param_file = \'src/SALib/test_functions/params/Ishigami_groups.txt\'\n        problem = read_param_file(param_file)\n\n        param_values = sample(problem=problem, N=10000,\n                              num_levels=4,\n                              optimal_trajectories=None)\n\n        Y = Ishigami.evaluate(param_values)\n\n        Si = morris.analyze(problem, param_values, Y,\n                            conf_level=0.95, print_to_console=False,\n                            num_levels=4)\n\n        assert_allclose(Si[\'mu_star\'], [7.610322, 10.197014],\n                        atol=0, rtol=1e-5)\n\n    def test_regression_morris_groups_brute_optim(self, set_seed):\n\n        set_seed\n        param_file = \'src/SALib/test_functions/params/Ishigami_groups.txt\'\n        problem = read_param_file(param_file)\n\n        param_values = sample(problem=problem, N=50,\n                              num_levels=4,\n                              optimal_trajectories=6,\n                              local_optimization=False)\n\n        Y = Ishigami.evaluate(param_values)\n\n        Si = morris.analyze(problem, param_values, Y,\n                            conf_level=0.95, print_to_console=False,\n                            num_levels=4)\n\n        assert_allclose(Si[\'mu\'], [9.786986, np.NaN],\n                        atol=0, rtol=1e-5)\n\n        assert_allclose(Si[\'sigma\'], [6.453729, np.NaN],\n                        atol=0, rtol=1e-5)\n\n        assert_allclose(Si[\'mu_star\'], [9.786986, 7.875],\n                        atol=0, rtol=1e-5)\n\n    def test_regression_morris_groups_local_optim(self, set_seed):\n        set_seed\n        param_file = \'src/SALib/test_functions/params/Ishigami_groups.txt\'\n        problem = read_param_file(param_file)\n\n        param_values = sample(problem=problem, N=500,\n                              num_levels=4,\n                              optimal_trajectories=20,\n                              local_optimization=True)\n\n        Y = Ishigami.evaluate(param_values)\n\n        Si = morris.analyze(problem, param_values, Y,\n                            conf_level=0.95, print_to_console=False,\n                            num_levels=4)\n\n        assert_allclose(Si[\'mu_star\'],\n                        [13.95285, 7.875],\n                        rtol=1e-5)\n\n    def test_regression_morris_optimal(self, set_seed):\n        \'\'\'\n        Tests the use of optimal trajectories with Morris.\n\n        Uses brute force approach\n\n        Note that the relative tolerance is set to a very high value\n        (default is 1e-05) due to the coarse nature of the num_levels.\n        \'\'\'\n        set_seed\n        param_file = \'src/SALib/test_functions/params/Ishigami.txt\'\n        problem = read_param_file(param_file)\n        param_values = sample(problem=problem, N=20,\n                              num_levels=4,\n                              optimal_trajectories=9,\n                              local_optimization=True)\n\n        Y = Ishigami.evaluate(param_values)\n\n        Si = morris.analyze(problem, param_values, Y,\n                            conf_level=0.95, print_to_console=False,\n                            num_levels=4)\n\n        assert_allclose(Si[\'mu_star\'],\n                        [9.786986e+00, 7.875000e+00, 1.388621],\n                        atol=0,\n                        rtol=1e-5)\n\n\ndef test_regression_sobol():\n    param_file = \'src/SALib/test_functions/params/Ishigami.txt\'\n    problem = read_param_file(param_file)\n    param_values = saltelli.sample(problem, 10000, calc_second_order=True)\n\n    Y = Ishigami.evaluate(param_values)\n\n    Si = sobol.analyze(problem, Y,\n                       calc_second_order=True, conf_level=0.95,\n                       print_to_console=False)\n\n    assert_allclose(Si[\'S1\'], [0.31, 0.44, 0.00], atol=5e-2, rtol=1e-1)\n    assert_allclose(Si[\'ST\'], [0.55, 0.44, 0.24], atol=5e-2, rtol=1e-1)\n    assert_allclose([Si[\'S2\'][0][1], Si[\'S2\'][0][2], Si[\'S2\'][1][2]], [\n                    0.00, 0.25, 0.00], atol=5e-2, rtol=1e-1)\n\n\ndef test_regression_sobol_parallel():\n    param_file = \'src/SALib/test_functions/params/Ishigami.txt\'\n    problem = read_param_file(param_file)\n    param_values = saltelli.sample(problem, 10000, calc_second_order=True)\n\n    Y = Ishigami.evaluate(param_values)\n\n    Si = sobol.analyze(problem, Y,\n                       calc_second_order=True, parallel=True,\n                       conf_level=0.95, print_to_console=False)\n\n    assert_allclose(Si[\'S1\'], [0.31, 0.44, 0.00], atol=5e-2, rtol=1e-1)\n    assert_allclose(Si[\'ST\'], [0.55, 0.44, 0.24], atol=5e-2, rtol=1e-1)\n    assert_allclose([Si[\'S2\'][0][1], Si[\'S2\'][0][2], Si[\'S2\'][1][2]], [\n                    0.00, 0.25, 0.00], atol=5e-2, rtol=1e-1)\n\n\ndef test_regression_sobol_groups():\n    problem = {\n        \'num_vars\': 3,\n        \'names\': [\'x1\', \'x2\', \'x3\'],\n        \'bounds\': [[-np.pi, np.pi]] * 3,\n        \'groups\': [\'G1\', \'G2\', \'G1\']\n    }\n    param_values = saltelli.sample(problem, 10000, calc_second_order=True)\n\n    Y = Ishigami.evaluate(param_values)\n    Si = sobol.analyze(problem, Y,\n                       calc_second_order=True, parallel=True,\n                       conf_level=0.95, print_to_console=False)\n\n    assert_allclose(Si[\'S1\'], [0.55, 0.44], atol=5e-2, rtol=1e-1)\n    assert_allclose(Si[\'ST\'], [0.55, 0.44], atol=5e-2, rtol=1e-1)\n    assert_allclose(Si[\'S2\'][0][1], [0.00], atol=5e-2, rtol=1e-1)\n\n\ndef test_regression_sobol_groups_dists():\n    problem = {\n        \'num_vars\': 3,\n        \'names\': [\'x1\', \'x2\', \'x3\'],\n        \'bounds\': [[-np.pi, np.pi], [1.0, 0.2], [3, 0.5]],\n        \'groups\': [\'G1\', \'G2\', \'G1\'],\n        \'dists\': [\'unif\', \'lognorm\', \'triang\']\n    }\n    param_values = saltelli.sample(problem, 10000, calc_second_order=True)\n\n    Y = Ishigami.evaluate(param_values)\n    Si = sobol.analyze(problem, Y,\n                       calc_second_order=True, parallel=True,\n                       conf_level=0.95, print_to_console=False)\n\n    assert_allclose(Si[\'S1\'], [0.427, 0.573], atol=5e-2, rtol=1e-1)\n    assert_allclose(Si[\'ST\'], [0.428, 0.573], atol=5e-2, rtol=1e-1)\n    assert_allclose(Si[\'S2\'][0][1], [0.001], atol=5e-2, rtol=1e-1)\n\n\ndef test_regression_fast():\n    param_file = \'src/SALib/test_functions/params/Ishigami.txt\'\n    problem = read_param_file(param_file)\n    param_values = fast_sampler.sample(problem, 10000)\n\n    Y = Ishigami.evaluate(param_values)\n\n    Si = fast.analyze(problem, Y, print_to_console=False)\n    assert_allclose(Si[\'S1\'], [0.31, 0.44, 0.00], atol=5e-2, rtol=1e-1)\n    assert_allclose(Si[\'ST\'], [0.55, 0.44, 0.24], atol=5e-2, rtol=1e-1)\n\n\ndef test_regression_rbd_fast():\n    param_file = \'src/SALib/test_functions/params/Ishigami.txt\'\n    problem = read_param_file(param_file)\n    param_values = latin.sample(problem, 10000)\n\n    Y = Ishigami.evaluate(param_values)\n\n    Si = rbd_fast.analyze(problem, param_values, Y, print_to_console=False)\n    assert_allclose(Si[\'S1\'], [0.31, 0.44, 0.00], atol=5e-2, rtol=1e-1)\n\n\ndef test_regression_dgsm():\n    param_file = \'src/SALib/test_functions/params/Ishigami.txt\'\n    problem = read_param_file(param_file)\n    param_values = finite_diff.sample(problem, 10000, delta=0.001)\n\n    Y = Ishigami.evaluate(param_values)\n\n    Si = dgsm.analyze(problem, param_values, Y,\n                      conf_level=0.95, print_to_console=False)\n\n    assert_allclose(Si[\'dgsm\'], [2.229, 7.066, 3.180], atol=5e-2, rtol=1e-1)\n\n\ndef test_regression_delta():\n    param_file = \'src/SALib/test_functions/params/Ishigami.txt\'\n    problem = read_param_file(param_file)\n    param_values = latin.sample(problem, 10000)\n\n    Y = Ishigami.evaluate(param_values)\n\n    Si = delta.analyze(problem, param_values, Y, num_resamples=10,\n                       conf_level=0.95, print_to_console=True)\n\n    assert_allclose(Si[\'delta\'], [0.210, 0.358, 0.155], atol=5e-2, rtol=1e-1)\n    assert_allclose(Si[\'S1\'], [0.31, 0.44, 0.00], atol=5e-2, rtol=1e-1)\n\ndef test_regression_delta_svm():\n    xy_input_fn = \'tests/data/delta_svm_regression_data.csv\'\n    inputs = np.loadtxt(xy_input_fn, delimiter=\',\', skiprows=1)\n\n    X = inputs[:, 0]\n    Y = inputs[:, 1]\n\n    Ygrid = [0.0, 0.3030303,  0.60606061, 0.90909091, 1.21212121, 1.51515152, \n             1.81818182, 2.12121212, 2.42424242, 2.72727273, 3.03030303, 3.33333333,\n             3.63636364, 3.93939394, 4.24242424, 4.54545455, 4.84848485, 5.15151515, \n             5.45454545, 5.75757576, 6.06060606, 6.36363636, 6.66666667, 6.96969697, \n             7.27272727, 7.57575758, 7.87878788, 8.18181818, 8.48484848, 8.78787879, \n             9.09090909, 9.39393939, 9.6969697, 10.0, 10.3030303, 10.60606061,\n             10.90909091, 11.21212121, 11.51515152, 11.81818182, 12.12121212, 12.42424242,\n             12.72727273, 13.03030303, 13.33333333, 13.63636364, 13.93939394, 14.24242424,\n             14.54545455, 14.84848485, 15.15151515, 15.45454545, 15.75757576, 16.06060606,\n             16.36363636, 16.66666667, 16.96969697, 17.27272727, 17.57575758, 17.87878788,\n             18.18181818, 18.48484848, 18.78787879, 19.09090909, 19.39393939, 19.6969697,\n             20.0,  20.3030303, 20.60606061, 20.90909091, 21.21212121, 21.51515152,\n             21.81818182, 22.12121212, 22.42424242, 22.72727273, 23.03030303, 23.33333333,\n             23.63636364, 23.93939394, 24.24242424, 24.54545455, 24.84848485, 25.15151515,\n             25.45454545, 25.75757576, 26.06060606, 26.36363636, 26.66666667, 26.96969697,\n             27.27272727, 27.57575758, 27.87878788, 28.18181818, 28.48484848, 28.78787879,\n             29.09090909, 29.39393939, 29.6969697, 30.0]\n\n\n    m = [0.0, 717.82142857, 1435.64285714, 2153.46428571, 2871.28571429, \n         3589.10714286, 4306.92857143, 5024.75, 5742.57142857, 6460.39285714,\n         7178.21428571, 7896.03571429, 8613.85714286, 9331.67857143, 10049.5, \n         10767.32142857, 11485.14285714, 12202.96428571, 12920.78571429, 13638.60714286,\n         14356.42857143, 15074.25, 15792.07142857, 16509.89285714, 17227.71428571,\n         17945.53571429, 18663.35714286, 19381.17857143, 20099.0]\n\n    num_resamples = 200\n    conf_level = 0.95\n\n    test_res = delta.bias_reduced_delta(Y, Ygrid, X, m, num_resamples, conf_level)\n\n    np.testing.assert_allclose(test_res, \n        (0.6335098491949687, 0.026640611898969522), \n        atol=0.005)\n'"
tests/test_sobol.py,3,"b""from __future__ import division\n\nfrom pytest import raises\nfrom numpy.testing import assert_equal, assert_allclose\nimport numpy as np\nfrom scipy.stats import norm\n\nfrom SALib.analyze import sobol\nfrom SALib.sample import saltelli, sobol_sequence\nfrom SALib.test_functions import Ishigami, Sobol_G\nfrom SALib.util import read_param_file\n\n\ndef setup_samples(N=500, calc_second_order=True):\n    param_file = 'src/SALib/test_functions/params/Ishigami.txt'\n    problem = read_param_file(param_file)\n    param_values = saltelli.sample(\n        problem, N=N, calc_second_order=calc_second_order)\n    return problem, param_values\n\n\ndef test_sobol_sequence():\n    # example from Joe & Kuo: http://web.maths.unsw.edu.au/~fkuo/sobol/\n    S = sobol_sequence.sample(10, 3)\n    expected = [[0, 0, 0],\n                [0.5, 0.5, 0.5],\n                [0.75, 0.25, 0.25],\n                [0.25, 0.75, 0.75],\n                [0.375, 0.375, 0.625],\n                [0.875, 0.875, 0.125],\n                [0.625, 0.125, 0.875],\n                [0.125, 0.625, 0.375],\n                [0.1875, 0.3125, 0.9375],\n                [0.6875, 0.8125, 0.4375]]\n    assert_allclose(S, expected, atol=5e-2, rtol=1e-1)\n\n\ndef test_sample_size_second_order():\n    N = 500\n    D = 3\n    problem, param_values = setup_samples(N=N)\n    assert_equal(param_values.shape, [N * (2 * D + 2), D])\n\n\ndef test_sample_size_first_order():\n    N = 500\n    D = 3\n    problem, param_values = setup_samples(N=N, calc_second_order=False)\n    assert_equal(param_values.shape, [N * (D + 2), D])\n\n\ndef test_incorrect_sample_size():\n    problem, param_values = setup_samples()\n    Y = Ishigami.evaluate(param_values)\n    with raises(RuntimeError):\n        sobol.analyze(problem, Y[:-10], calc_second_order=True)\n\n\ndef test_bad_conf_level():\n    problem, param_values = setup_samples()\n    Y = Ishigami.evaluate(param_values)\n    with raises(RuntimeError):\n        sobol.analyze(problem, Y,\n                      calc_second_order=True,\n                      conf_level=1.01,\n                      print_to_console=False)\n\n\ndef test_incorrect_second_order_setting():\n    # note this will still be a problem if N(2D+2) also divides by (D+2)\n    problem, param_values = setup_samples(N=501, calc_second_order=False)\n    Y = Ishigami.evaluate(param_values)\n    with raises(RuntimeError):\n        sobol.analyze(problem, Y, calc_second_order=True)\n\n\ndef test_include_print():\n    problem, param_values = setup_samples()\n    Y = Ishigami.evaluate(param_values)\n    sobol.analyze(problem, Y,\n                  calc_second_order=True,\n                  conf_level=0.95,\n                  print_to_console=True)\n\n\ndef test_parallel_first_order():\n    c2o = False\n    N = 10000\n    problem, param_values = setup_samples(N=N, calc_second_order=c2o)\n    Y = Ishigami.evaluate(param_values)\n\n    A, B, AB, BA = sobol.separate_output_values(Y, D=3, N=N,\n                                                calc_second_order=c2o)\n    r = np.random.randint(N, size=(N, 100))\n    Z = norm.ppf(0.5 + 0.95 / 2)\n    tasks, n_processors = sobol.create_task_list(D=3,\n                                                 calc_second_order=c2o,\n                                                 n_processors=None)\n    Si_list = []\n    for t in tasks:\n        Si_list.append(sobol.sobol_parallel(Z, A, AB, BA, B, r, t))\n    Si = sobol.Si_list_to_dict(Si_list, D=3, calc_second_order=c2o)\n\n    assert_allclose(Si['S1'], [0.31, 0.44, 0.00], atol=5e-2, rtol=1e-1)\n    assert_allclose(Si['ST'], [0.55, 0.44, 0.24], atol=5e-2, rtol=1e-1)\n\n\ndef test_parallel_second_order():\n    c2o = True\n    N = 10000\n    problem, param_values = setup_samples(N=N, calc_second_order=c2o)\n    Y = Ishigami.evaluate(param_values)\n\n    A, B, AB, BA = sobol.separate_output_values(Y, D=3, N=N,\n                                                calc_second_order=c2o)\n    r = np.random.randint(N, size=(N, 100))\n    Z = norm.ppf(0.5 + 0.95 / 2)\n    tasks, n_processors = sobol.create_task_list(D=3,\n                                                 calc_second_order=c2o,\n                                                 n_processors=None)\n    Si_list = []\n    for t in tasks:\n        Si_list.append(sobol.sobol_parallel(Z, A, AB, BA, B, r, t))\n    Si = sobol.Si_list_to_dict(Si_list, D=3, calc_second_order=c2o)\n\n    assert_allclose(Si['S1'], [0.31, 0.44, 0.00], atol=5e-2, rtol=1e-1)\n    assert_allclose(Si['ST'], [0.55, 0.44, 0.24], atol=5e-2, rtol=1e-1)\n    assert_allclose([Si['S2'][0][1], Si['S2'][0][2], Si['S2'][1][2]], [\n                    0.00, 0.25, 0.00], atol=5e-2, rtol=1e-1)\n\n\ndef test_Sobol_G_using_sobol():\n    '''\n    Tests the accuracy of the Sobol/Saltelli procedure using the Sobol_G\n    test function, comparing the results from the Sobol/Saltelli analysis\n    against the analytically computed sensitivity index from the Sobol_G\n    function.\n    '''\n    problem = {'num_vars': 6,\n               'names': ['x1', 'x2', 'x3', 'x4', 'x5', 'x6'],\n               'bounds': [[0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1]]}\n    N = 5000\n    a = np.array([78, 12, 0.5, 2, 97, 33])\n    param_values = saltelli.sample(problem, N, calc_second_order=False)\n    model_results = Sobol_G.evaluate(param_values, a)\n    Si = sobol.analyze(problem, model_results, calc_second_order=False)\n#     expected = Sobol_G.total_sensitivity_index(a)\n#     assert_allclose(Si['ST'], expected)\n    expected = Sobol_G.sensitivity_index(a)\n    assert_allclose(Si['S1'], expected, atol=1e-2, rtol=1e-6)\n"""
tests/test_test_functions.py,23,"b'from pytest import raises\n\nfrom numpy.testing import assert_allclose\nimport numpy as np\n\nfrom SALib.test_functions.Sobol_G import evaluate, _total_variance, \\\n    _partial_first_order_variance, \\\n    sensitivity_index, \\\n    total_sensitivity_index\n\n\ndef test_Sobol_G():\n    \'\'\'\n    \'\'\'\n    parameter_values = np.zeros((1, 8))\n    actual = evaluate(parameter_values)\n    expected = np.array([4.0583])\n    assert_allclose(actual, expected, atol=1e-4, rtol=1e-4)\n\n\ndef test_Sobol_G_raises_error_if_values_wrong_size():\n    """"""\n    Tests that a value error is raised if the Sobol G function is called with\n    the wrong number of variables\n    """"""\n    a = [1, 2, 3, 4, 5, 6, 7, 8]\n    with raises(ValueError):\n        evaluate(np.array([1, 2, 3, 4, 5, 6, 7]), a)\n\n\ndef test_Sobol_G_raises_error_if_values_gt_one():\n    """"""\n    Tests that a value error is raised if the Sobol G function is called with\n    values greater than one\n    """"""\n    with raises(ValueError):\n        evaluate(np.array([0, 1, .02, 0.23, 1.234, 0.02848848, 0, 0.78]))\n\n\ndef test_Sobol_G_raises_error_if_values_lt_zero():\n    """"""\n    Tests that a value error is raised if the Sobol G function is called with\n    values less than zero.\n    """"""\n    with raises(ValueError):\n        evaluate(np.array([0, -1, -.02, 1, 1, -0.1, -0, -12]))\n\n\ndef test_Sobol_G_raises_error_if_values_not_numpy_array():\n    """"""\n    Tests that a type error is raised if the Sobol G function is called with\n    values argument not as a numpy array.\n    """"""\n    fixture = [list(range(8)), str(12345678)]\n    for x in fixture:\n        with raises(TypeError):\n            evaluate(x)\n\n\ndef test_total_variance():\n\n    a = np.array([78, 12, 0.5, 2, 97, 33])\n    actual = _total_variance(a)\n    expected = 0.19347\n\n    assert_allclose(actual, expected, rtol=1e-4)\n\n\ndef test_partial_first_order_variance():\n\n    a = np.array([78, 12, 0.5, 2, 97, 33])\n    actual = _partial_first_order_variance(a)\n    expected = (len(a),)\n\n    assert a.shape == expected\n\n    expected = np.array([0.000053, 0.001972, 0.148148,\n                         0.037037, 0.000035, 0.000288])\n\n    assert_allclose(actual, expected, atol=1e-4, rtol=1e-4)\n\n\ndef test_sensitivity_index():\n    a = np.array([78, 12, 0.5, 2, 97, 33])\n    actual = sensitivity_index(a)\n    expected = np.array([0.000276, 0.010195, 0.765743,\n                         0.191436, 0.000179, 0.001490])\n    assert_allclose(actual, expected, atol=1e-2, rtol=1e-6)\n\n\ndef test_total_sensitivity_index():\n    a = np.array([78, 12, 0.5, 2, 97, 33])\n\n    actual = total_sensitivity_index(a)\n\n    expected = np.array([0.030956547, 0.040875287, 0.796423551,\n                         0.222116249, 0.030859879, 0.032170899])\n\n    assert_allclose(actual, expected, atol=1e-2, rtol=1e-6)\n\n\ndef test_modified_Sobol_G():\n    parameter_values = np.zeros((1, 8))\n    delta_values = np.ones_like(parameter_values)\n    alpha_values = np.array([2]*8)\n    actual = evaluate(parameter_values, delta=delta_values, alpha=alpha_values)\n    expected = np.array([10.6275])\n    assert_allclose(actual, expected, atol=1e-4, rtol=1e-4)\n\n\ndef test_modified_Sobol_G_error_if_type_wrong():\n\n    parameter_values = np.zeros((1, 8))\n    delta_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n    expected_err = ""The argument `delta` must be given as a numpy ndarray""\n    with raises(TypeError, match=expected_err):\n        evaluate(parameter_values, delta=delta_values)\n\n    alpha_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n    expected_err = ""The argument `alpha` must be given as a numpy ndarray""\n    with raises(TypeError, match=expected_err):\n        evaluate(parameter_values, alpha=alpha_values)\n\n\ndef test_modified_Sobol_G_error_if_value_beyond_range():\n    parameter_values = np.zeros((1, 8))\n    delta_values = np.array([-0.5, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 1.8])\n\n    expected_err = ""Sobol G function called with delta values less than zero or greater than one""\n    with raises(ValueError, match=expected_err):\n        evaluate(parameter_values, delta=delta_values) \n\n    alpha_values = np.array([0, -0.2, -0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n    expected_err = ""Sobol G function called with alpha values less than or equal to zero""\n    with raises(ValueError, match=expected_err):\n        evaluate(parameter_values, alpha=alpha_values)\n\n\ndef test_modified_partial_first_order_variance():\n\n    a = np.array([78, 12, 0.5, 2, 97, 33])\n    alpha = np.array([1, 2, 15, 0.6, 8, 48])\n    actual = _partial_first_order_variance(a, alpha)\n    expected = (len(a),)\n\n    assert a.shape == expected\n\n    expected = np.array([5.34102441e-05, 4.73372781e-03, 3.22580645e+00,\n                         1.81818182e-02, 3.91993532e-04, 2.05472122e-02])\n\n    assert_allclose(actual, expected, atol=1e-4, rtol=1e-4)\n\n'"
tests/test_to_df.py,1,"b'import numpy as np\nimport pandas as pd\nfrom SALib.sample import saltelli, morris as morris_sample, \\\n                         finite_diff, fast_sampler, ff as ff_sample, latin\nfrom SALib.analyze import sobol, morris, dgsm, fast, ff, rbd_fast\nfrom SALib.test_functions import Ishigami, Sobol_G\n\n\ndef test_morris_to_df():\n    params = [\'x1\', \'x2\', \'x3\', \'x4\', \'x5\', \'x6\', \'x7\', \'x8\']\n\n    problem = {\n        \'num_vars\': 8,\n        \'names\': params,\n        \'groups\': None,\n        \'bounds\': [[0.0, 1.0],\n                   [0.0, 1.0],\n                   [0.0, 1.0],\n                   [0.0, 1.0],\n                   [0.0, 1.0],\n                   [0.0, 1.0],\n                   [0.0, 1.0],\n                   [0.0, 1.0]]\n    }\n\n    param_values = morris_sample.sample(problem, N=1000, num_levels=4,\n                                        optimal_trajectories=None)\n    Y = Sobol_G.evaluate(param_values)\n    Si = morris.analyze(problem, param_values, Y)\n    Si_df = Si.to_df()\n\n    assert isinstance(Si_df, pd.DataFrame), \\\n        ""Morris Si: Expected DataFrame, got {}"".format(type(Si_df))\n\n    expected_index = set(params)\n    assert set(Si_df.index) == expected_index, ""Incorrect index in DataFrame""\n\n    col_names = [\'mu\', \'mu_star\', \'sigma\', \'mu_star_conf\']\n    assert set(Si_df.columns) == set(col_names), \\\n        ""Unexpected column names in DataFrame. Expected {}, got {}"".format(\n            col_names, Si_df.columns)\n# End test_morris_to_df()\n\n\ndef test_sobol_to_df():\n    params = [\'x1\', \'x2\', \'x3\']\n    problem = {\n        \'num_vars\': 3,\n        \'names\': params,\n        \'bounds\': [[-np.pi, np.pi]]*3\n    }\n\n    X = saltelli.sample(problem, 1000)\n    Y = Ishigami.evaluate(X)\n    Si = sobol.analyze(problem, Y, print_to_console=False)\n    total, first, second = Si.to_df()\n\n    assert isinstance(total, pd.DataFrame), \\\n        ""Total Si: Expected DataFrame, got {}"".format(type(total))\n    assert isinstance(first, pd.DataFrame), \\\n        ""First Si: Expected DataFrame, got {}"".format(type(first))\n    assert isinstance(second, pd.DataFrame), \\\n        ""Second Si: Expected DataFrame, got {}"".format(type(second))\n\n    expected_index = set(params)\n    assert set(total.index) == expected_index, \\\n        ""Index for Total Si are incorrect""\n    assert set(first.index) == expected_index, \\\n        ""Index for first order Si are incorrect""\n    assert set(second.index) == set([(\'x1\', \'x2\'),\n                                     (\'x1\', \'x3\'),\n                                     (\'x2\', \'x3\')]), \\\n        ""Index for second order Si are incorrect""\n# End test_sobol_to_df()\n\n\ndef test_dgsm_to_df():\n    params = [\'x1\', \'x2\', \'x3\']\n    problem = {\n        \'num_vars\': 3,\n        \'names\': params,\n        \'groups\': None,\n        \'bounds\': [[-3.14159265359, 3.14159265359],\n                   [-3.14159265359, 3.14159265359],\n                   [-3.14159265359, 3.14159265359]]\n    }\n\n    param_values = finite_diff.sample(problem, 1000, delta=0.001)\n    Y = Ishigami.evaluate(param_values)\n\n    Si = dgsm.analyze(problem, param_values, Y, print_to_console=False)\n    Si_df = Si.to_df()\n\n    assert isinstance(Si_df, pd.DataFrame), \\\n        ""DGSM Si: Expected DataFrame, got {}"".format(type(Si_df))\n    assert set(Si_df.index) == set(params), ""Incorrect index in DataFrame""\n\n    col_names = [\'vi\', \'vi_std\', \'dgsm\', \'dgsm_conf\']\n    assert set(Si_df.columns) == set(col_names), \\\n        ""Unexpected column names in DataFrame""\n# End test_dgsm_to_df()\n\n\ndef test_fast_to_df():\n    params = [\'x1\', \'x2\', \'x3\']\n    problem = {\n        \'num_vars\': 3,\n        \'names\': params,\n        \'groups\': None,\n        \'bounds\': [[-3.14159265359, 3.14159265359],\n                   [-3.14159265359, 3.14159265359],\n                   [-3.14159265359, 3.14159265359]]\n    }\n\n    param_values = fast_sampler.sample(problem, 1000)\n    Y = Ishigami.evaluate(param_values)\n\n    Si = fast.analyze(problem, Y, print_to_console=False)\n    Si_df = Si.to_df()\n\n    expected_index = set(params)\n    assert isinstance(Si_df, pd.DataFrame), \\\n        ""FAST Si: Expected DataFrame, got {}"".format(type(Si_df))\n    assert set(Si_df.index) == expected_index, ""Incorrect index in DataFrame""\n\n    col_names = set([\'S1\', \'ST\'])\n    assert set(Si_df.columns) == col_names, \\\n        ""Unexpected column names in DataFrame. Expected {}, got {}"".format(\n            col_names, Si_df.columns)\n# End test_fast_to_df()\n\n\ndef test_ff_to_df():\n    params = [\'x1\', \'x2\', \'x3\']\n    main_index = params + [\'dummy_0\']\n\n    problem = {\n        \'num_vars\': 3,\n        \'names\': params,\n        \'groups\': None,\n        \'bounds\': [[-3.14159265359, 3.14159265359],\n                   [-3.14159265359, 3.14159265359],\n                   [-3.14159265359, 3.14159265359]]\n    }\n\n    X = ff_sample.sample(problem)\n    Y = X[:, 0] + (0.1 * X[:, 1]) + ((1.2 * X[:, 2]) * (0.2 + X[:, 0]))\n    Si = ff.analyze(problem, X, Y, second_order=True, print_to_console=False)\n    main_effect, inter_effect = Si.to_df()\n\n    assert isinstance(main_effect, pd.DataFrame), \\\n        ""FF ME: Expected DataFrame, got {}"".format(type(main_effect))\n    assert isinstance(main_effect, pd.DataFrame), \\\n        ""FF IE: Expected DataFrame, got {}"".format(type(inter_effect))\n    assert set(main_effect.index) == set(main_index), \\\n        ""Incorrect index in Main Effect DataFrame""\n\n    inter_index = set([(\'x1\', \'x2\'),\n                       (\'x1\', \'x3\'),\n                       (\'x2\', \'x3\'),\n                       (\'x1\', \'dummy_0\'),\n                       (\'x2\', \'dummy_0\'),\n                       (\'x3\', \'dummy_0\')])\n    assert set(inter_effect.index) == inter_index, \\\n        ""Incorrect index in Interaction Effect DataFrame""\n# End test_fast_to_df()\n\n\ndef test_rbd_to_df():\n    params = [\'x1\', \'x2\', \'x3\']\n    problem = {\n        \'num_vars\': 3,\n        \'names\': params,\n        \'groups\': None,\n        \'bounds\': [[-3.14159265359, 3.14159265359],\n                   [-3.14159265359, 3.14159265359],\n                   [-3.14159265359, 3.14159265359]]\n    }\n\n    param_values = latin.sample(problem, 1000)\n    Y = Ishigami.evaluate(param_values)\n    Si = rbd_fast.analyze(problem, param_values, Y, print_to_console=False)\n    Si_df = Si.to_df()\n\n    assert isinstance(Si_df, pd.DataFrame), \\\n        ""RBD Si: Expected DataFrame, got {}"".format(type(Si_df))\n    assert set(Si_df.index) == set(params), ""Incorrect index in DataFrame""\n\n    col_names = set([\'S1\'])\n    assert set(Si_df.columns) == col_names, \\\n        ""Unexpected column names in DataFrame. Expected {}, got {}"".format(\n            col_names, Si_df.columns)\n# End test_fast_to_df()\n'"
tests/test_util.py,10,"b'from __future__ import division\n\nfrom pytest import raises\nfrom numpy.testing import assert_equal, assert_allclose\n\nimport numpy as np\nimport pytest\n\nfrom SALib.util import read_param_file, scale_samples, unscale_samples, \\\n    compute_groups_matrix\n\nfrom . conftest import make_temporary_file\n\n\n@pytest.fixture(scope=\'function\')\ndef setup_param_file_group_dist():\n    filename = make_temporary_file()\n    with open(filename, ""w"") as ofile:\n        ofile.write(""Test1 0.0 100.0 Group1 unif\\n"")\n        ofile.write(""Test2 5.0 51.0 Group1 triang\\n"")\n        ofile.write(""Test3 10.0 1.0 Group2 norm\\n"")\n    return filename\n\n\n@pytest.fixture(scope=\'function\')\ndef setup_csv_param_file_space():\n    filename = make_temporary_file()\n    with open(filename, ""w"") as ofile:\n        ofile.write(""Test 1,0.0,100.0\\n"")\n        ofile.write(""Test 2,5.0,51.0\\n"")\n    return filename\n\n\n@pytest.fixture(scope=\'function\')\ndef setup_tab_param_file_espace_names():\n    filename = make_temporary_file()\n    with open(filename, ""w"") as ofile:\n        ofile.write(""Test 1\\t0.0\\t100.0\\n"")\n        ofile.write(""Test 2\\t5.0\\t51.0\\n"")\n    return filename\n\n\n@pytest.fixture(scope=\'function\')\ndef setup_csv_param_file_space_comments():\n    filename = make_temporary_file()\n    with open(filename, ""w"") as ofile:\n        ofile.write(""# Here is a comment\\n"")\n        ofile.write(""\'Test 1\',0.0,100.0\\n"")\n        ofile.write(""\'Test 2\',5.0,51.0\\n"")\n    return filename\n\n\ndef test_readfile(setup_function):\n    \'\'\'\n    Tests a standard parameter file is read correctly\n    \'\'\'\n\n    filename = setup_function\n    pf = read_param_file(filename)\n\n    assert_equal(pf[\'bounds\'], [[0, 100], [5, 51]])\n    assert_equal(pf[\'num_vars\'], 2)\n    assert_equal(pf[\'names\'], [\'Test1\', \'Test2\'])\n\n\ndef test_readfile_group_dist(setup_param_file_group_dist):\n    \'\'\'\n    Tests a parameter file with groups and distributions is read correctly\n    \'\'\'\n    filename = setup_param_file_group_dist\n    pf = read_param_file(filename)\n    assert_equal(pf[\'bounds\'], [[0, 100], [5, 51], [10, 1]])\n    assert_equal(pf[\'num_vars\'], 3)\n    assert_equal(pf[\'names\'], [\'Test1\', \'Test2\', \'Test3\'])\n    assert_equal(pf[\'groups\'], [\'Group1\', \'Group1\', \'Group2\'])\n    assert_equal(pf[\'dists\'], [\'unif\', \'triang\', \'norm\'])\n\n\ndef test_csv_readfile_with_whitespace(setup_csv_param_file_space):\n    \'\'\'\n    A comma delimited parameter file with whitespace in the names\n    \'\'\'\n\n    filename = setup_csv_param_file_space\n\n    pf = read_param_file(filename)\n\n    assert_equal(pf[\'bounds\'], [[0, 100], [5, 51]])\n    assert_equal(pf[\'num_vars\'], 2)\n    assert_equal(pf[\'names\'], [\'Test 1\', \'Test 2\'])\n\n\ndef test_tab_readfile_whitespace(setup_tab_param_file_espace_names):\n    \'\'\'\n    A tab delimited parameter file with whitespace in the names\n    \'\'\'\n\n    filename = setup_tab_param_file_espace_names\n    pf = read_param_file(filename)\n\n    assert_equal(pf[\'bounds\'], [[0, 100], [5, 51]])\n    assert_equal(pf[\'num_vars\'], 2)\n    assert_equal(pf[\'names\'], [\'Test 1\', \'Test 2\'])\n\n\ndef test_csv_readfile_comments(setup_csv_param_file_space_comments):\n    \'\'\'\n    \'\'\'\n\n    filename = setup_csv_param_file_space_comments\n\n    pf = read_param_file(filename)\n\n    print(pf[\'bounds\'], pf[\'num_vars\'], pf[\'names\'])\n\n    assert_equal(pf[\'bounds\'], [[0, 100], [5, 51]])\n    assert_equal(pf[\'num_vars\'], 2)\n    assert_equal(pf[\'names\'], [\'Test 1\', \'Test 2\'])\n\n\n# Test scale samples\ndef test_scale_samples():\n    \'\'\'\n    Simple test to ensure that samples are correctly scaled\n    \'\'\'\n\n    params = np.arange(0, 1.1, 0.1).repeat(2).reshape((11, 2))\n\n    bounds = [[10, 20], [-10, 10]]\n\n    desired = np.array(\n        [np.arange(10, 21, 1), np.arange(-10, 12, 2)], dtype=np.float).T\n    scale_samples(params, bounds)\n    assert_allclose(params, desired, atol=1e-03, rtol=1e-03)\n\n\ndef test_unscale_samples():\n    \'\'\'\n    Simple test to unscale samples back to [0,1] range\n    \'\'\'\n    params = np.array(\n        [np.arange(10, 21, 1), np.arange(-10, 12, 2)], dtype=np.float).T\n    bounds = [[10, 20], [-10, 10]]\n\n    desired = np.arange(0, 1.1, 0.1).repeat(2).reshape((11, 2))\n    unscale_samples(params, bounds)\n    assert_allclose(params, desired, atol=1e-03, rtol=1e-03)\n\n\ndef test_scale_samples_upper_lt_lower():\n    \'\'\'\n    Raise ValueError if upper bound lower than lower bound\n    \'\'\'\n    params = np.array([[0, 0], [0.1, 0.1], [0.2, 0.2]])\n    bounds = [[10, 9], [-10, 10]]\n    with raises(ValueError):\n        scale_samples(params, bounds)\n\n\ndef test_scale_samples_upper_eq_lower():\n    \'\'\'\n    Raise ValueError if upper bound lower equal to lower bound\n    \'\'\'\n    params = np.array([[0, 0], [0.1, 0.1], [0.2, 0.2]])\n    bounds = [[10, 10], [-10, 10]]\n    with raises(ValueError):\n        scale_samples(params, bounds)\n\n\ndef test_compute_groups_from_parameter_file():\n    \'\'\'\n    Tests that a group file is read correctly\n    \'\'\'\n    actual_matrix, actual_unique_names = \\\n        compute_groups_matrix([\'Group 1\', \'Group 2\', \'Group 2\'])\n\n    assert_equal(actual_matrix, np.array(\n        [[1, 0], [0, 1], [0, 1]], dtype=np.int))\n    assert_equal(actual_unique_names, [\'Group 1\', \'Group 2\'])\n'"
examples/delta/delta.py,2,"b'import sys\r\n\r\nfrom SALib.analyze import delta\r\nfrom SALib.util import read_param_file\r\nimport numpy as np\r\n\r\n\r\nsys.path.append(\'../..\')\r\n\r\n\r\n# Read the parameter range file and generate samples\r\n# Since this is ""given data"", the bounds in the parameter file will not be used\r\n# but the columns are still expected\r\nproblem = read_param_file(\'../../src/SALib/test_functions/params/Ishigami.txt\')\r\nX = np.loadtxt(\'model_input.txt\')\r\nY = np.loadtxt(\'model_output.txt\')\r\n\r\n# Perform the sensitivity analysis using the model output\r\n# Specify which column of the output file to analyze (zero-indexed)\r\nSi = delta.analyze(problem, X, Y, num_resamples=10, conf_level=0.95, print_to_console=False)\r\n# Returns a dictionary with keys \'delta\', \'delta_conf\', \'S1\', \'S1_conf\'\r\nprint(str(Si[\'delta\']))\r\n'"
examples/dgsm/dgsm.py,0,"b'import sys\r\n\r\nfrom SALib.analyze import dgsm\r\nfrom SALib.sample import finite_diff\r\nfrom SALib.test_functions import Ishigami\r\nfrom SALib.util import read_param_file\r\n\r\n\r\nsys.path.append(\'../..\')\r\n\r\n\r\n# Read the parameter range file and generate samples\r\nproblem = read_param_file(\'../../src/SALib/test_functions/params/Ishigami.txt\')\r\n\r\n# Generate samples\r\nparam_values = finite_diff.sample(problem, 1000, delta=0.001)\r\n\r\n# Run the ""model"" -- this will happen offline for external models\r\nY = Ishigami.evaluate(param_values)\r\n\r\n# Perform the sensitivity analysis using the model output\r\n# Specify which column of the output file to analyze (zero-indexed)\r\nSi = dgsm.analyze(problem, param_values, Y, conf_level=0.95, print_to_console=False)\r\n# Returns a dictionary with keys \'vi\', \'vi_std\', \'dgsm\', and \'dgsm_conf\'\r\n# e.g. Si[\'vi\'] contains the sensitivity measure for each parameter, in\r\n# the same order as the parameter file\r\n\r\n# For comparison, Morris mu* < sqrt(v_i)\r\n# and total order S_tot <= dgsm, following Sobol and Kucherenko (2009)\r\n'"
examples/fast/fast.py,0,"b'import sys\r\nsys.path.append(\'../..\')\r\n\r\nfrom SALib.analyze import fast\r\nfrom SALib.sample import fast_sampler\r\nfrom SALib.test_functions import Ishigami\r\nfrom SALib.util import read_param_file\r\n\r\n# Read the parameter range file and generate samples\r\nproblem = read_param_file(\'../../src/SALib/test_functions/params/Ishigami.txt\')\r\n\r\n# Generate samples\r\nparam_values = fast_sampler.sample(problem, 1000)\r\n\r\n# Run the ""model"" and save the output in a text file\r\n# This will happen offline for external models\r\nY = Ishigami.evaluate(param_values)\r\n\r\n# Perform the sensitivity analysis using the model output\r\n# Specify which column of the output file to analyze (zero-indexed)\r\nSi = fast.analyze(problem, Y, print_to_console=False)\r\n# Returns a dictionary with keys \'S1\' and \'ST\'\r\n# e.g. Si[\'S1\'] contains the first-order index for each parameter, in the\r\n# same order as the parameter file\r\n'"
examples/ff/ff.py,0,"b'import sys\r\n\r\nfrom SALib.analyze.ff import analyze\r\nfrom SALib.sample.ff import sample\r\nfrom SALib.util import read_param_file\r\n\r\nsys.path.append(\'../..\')\r\n\r\n# Read the parameter range file and generate samples\r\nproblem = read_param_file(\'../../src/SALib/test_functions/params/Ishigami.txt\')\r\n# or define manually without a parameter file:\r\n# problem = {\r\n#  \'num_vars\': 3,\r\n#  \'names\': [\'x1\', \'x2\', \'x3\'],\r\n#  \'groups\': None,\r\n#  \'bounds\': [[-3.14159265359, 3.14159265359],\r\n#             [-3.14159265359, 3.14159265359],\r\n#             [-3.14159265359, 3.14159265359]]\r\n# }\r\n\r\n# Generate samples\r\nX = sample(problem)\r\n\r\n# Run the ""model"" -- this will happen offline for external models\r\nY = X[:, 0] + (0.1 * X[:, 1]) + ((1.2 * X[:, 2]) * (0.2 + X[:, 0]))\r\n\r\n# Perform the sensitivity analysis using the model output\r\nanalyze(problem, X, Y, second_order=True, print_to_console=True)\r\n# Returns a dictionary with keys \'ME\' (main effect) and \'IE\' (interaction effect)\r\n# The techniques bulks out the number of parameters with dummy parameters to the\r\n# nearest 2**n.  Any results involving dummy parameters should be treated with\r\n# a sceptical eye.\r\n'"
examples/morris/morris.py,0,"b'import sys\n\nfrom SALib.analyze import morris\nfrom SALib.sample.morris import sample\nfrom SALib.test_functions import Sobol_G\nfrom SALib.util import read_param_file\nfrom SALib.plotting.morris import horizontal_bar_plot, covariance_plot, \\\n    sample_histograms\nimport matplotlib.pyplot as plt\n\nsys.path.append(\'../..\')\n\n# Read the parameter range file and generate samples\nproblem = read_param_file(\'../../src/SALib/test_functions/params/Sobol_G.txt\')\n# or define manually without a parameter file:\n# problem = {\n#  \'num_vars\': 8,\n#  \'names\': [\'x1\', \'x2\', \'x3\', \'x4\', \'x5\', \'x6\', \'x7\', \'x8\'],\n#  \'groups\': None,\n#  \'bounds\': [[0.0, 1.0],\n#             [0.0, 1.0],\n#             [0.0, 1.0],\n#             [0.0, 1.0],\n#             [0.0, 1.0],\n#             [0.0, 1.0],\n#             [0.0, 1.0],\n#             [0.0, 1.0]]\n# }\n# Files with a 4th column for ""group name"" will be detected automatically, e.g.\n# param_file = \'../../src/SALib/test_functions/params/Ishigami_groups.txt\'\n\n# Generate samples\nparam_values = sample(problem, N=1000, num_levels=4,\n                      optimal_trajectories=None)\n\n# To use optimized trajectories (brute force method),\n# give an integer value for optimal_trajectories\n\n# Run the ""model"" -- this will happen offline for external models\nY = Sobol_G.evaluate(param_values)\n\n# Perform the sensitivity analysis using the model output\n# Specify which column of the output file to analyze (zero-indexed)\nSi = morris.analyze(problem, param_values, Y, conf_level=0.95,\n                    print_to_console=True,\n                    num_levels=4, num_resamples=100)\n# Returns a dictionary with keys \'mu\', \'mu_star\', \'sigma\', and \'mu_star_conf\'\n# e.g. Si[\'mu_star\'] contains the mu* value for each parameter, in the\n# same order as the parameter file\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nhorizontal_bar_plot(ax1, Si, {}, sortby=\'mu_star\', unit=r""tCO$_2$/year"")\ncovariance_plot(ax2, Si, {}, unit=r""tCO$_2$/year"")\n\nfig2 = plt.figure()\nsample_histograms(fig2, param_values, problem, {\'color\': \'y\'})\nplt.show()\n'"
examples/rbd_fast/rbd_fast.py,0,"b'import sys\r\nsys.path.append(\'../..\')\r\n\r\nfrom SALib.analyze import rbd_fast\r\nfrom SALib.sample import latin\r\nfrom SALib.test_functions import Ishigami\r\nfrom SALib.util import read_param_file\r\n\r\n# Read the parameter range file and generate samples\r\nproblem = read_param_file(\'../../src/SALib/test_functions/params/Ishigami.txt\')\r\n\r\n# Generate samples\r\nparam_values = latin.sample(problem, 1000)\r\n\r\n# Run the ""model"" and save the output in a text file\r\n# This will happen offline for external models\r\nY = Ishigami.evaluate(param_values)\r\n\r\n# Perform the sensitivity analysis using the model output\r\n# Specify which column of the output file to analyze (zero-indexed)\r\nSi = rbd_fast.analyze(problem, param_values, Y, print_to_console=False)\r\n# Returns a dictionary with keys \'S1\' and \'ST\'\r\n# e.g. Si[\'S1\'] contains the first-order index for each parameter, in the\r\n# same order as the parameter file\r\n'"
examples/sobol/sobol.py,0,"b'import sys\r\nsys.path.append(\'../..\')\r\n\r\nfrom SALib.analyze import sobol\r\nfrom SALib.sample import saltelli\r\nfrom SALib.test_functions import Ishigami\r\nfrom SALib.util import read_param_file\r\n\r\n\r\n# Read the parameter range file and generate samples\r\nproblem = read_param_file(\'../../src/SALib/test_functions/params/Ishigami.txt\')\r\n\r\n# Generate samples\r\nparam_values = saltelli.sample(problem, 1000, calc_second_order=True)\r\n\r\n# Run the ""model"" and save the output in a text file\r\n# This will happen offline for external models\r\nY = Ishigami.evaluate(param_values)\r\n\r\n# Perform the sensitivity analysis using the model output\r\n# Specify which column of the output file to analyze (zero-indexed)\r\nSi = sobol.analyze(problem, Y, calc_second_order=True, conf_level=0.95, print_to_console=True)\r\n# Returns a dictionary with keys \'S1\', \'S1_conf\', \'ST\', and \'ST_conf\'\r\n# e.g. Si[\'S1\'] contains the first-order index for each parameter,\r\n# in the same order as the parameter file\r\n# The optional second-order indices are now returned in keys \'S2\', \'S2_conf\'\r\n# These are both upper triangular DxD matrices with nan\'s in the duplicate\r\n# entries.\r\n# Optional keyword arguments parallel=True and n_processors=(int) for parallel execution\r\n# using multiprocessing\r\n\r\n# First-order indices expected with Saltelli sampling:\r\n# x1: 0.3139\r\n# x2: 0.4424\r\n# x3: 0.0\r\n'"
src/SALib/__init__.py,0,"b""# -*- coding: utf-8 -*-\nfrom pkg_resources import get_distribution, DistributionNotFound\n\ntry:\n    # Change here if project is renamed and does not equal the package name\n    dist_name = 'SALib'\n    __version__ = get_distribution(dist_name).version\nexcept DistributionNotFound:\n    __version__ = 'unknown'\nfinally:\n    del get_distribution, DistributionNotFound\n"""
src/SALib/analyze/__init__.py,0,b''
src/SALib/analyze/common_args.py,0,"b""import argparse\n\n\ndef setup(parser):\n    parser = argparse.ArgumentParser(\n        description='Perform sensitivity analysis on model output')\n    parser.add_argument(\n        '-p', '--paramfile', type=str, required=True,\n        help='Parameter range file')\n    parser.add_argument(\n        '-Y', '--model-output-file', type=str, required=True,\n        help='Model output file')\n    parser.add_argument('-c', '--column', type=int, required=False,\n                        default=0, help='Column of output to analyze')\n    parser.add_argument('--delimiter', type=str, required=False, default=' ',\n                        help='Column delimiter in model output file')\n    parser.add_argument(\n        '-s', '--seed', type=int, required=False, default=None,\n        help='Random Seed')\n    return parser\n\n\ndef create(cli_parser=None):\n    parser = argparse.ArgumentParser(\n        description='Perform sensitivity analysis on model output')\n    parser = setup(parser)\n\n    if cli_parser:\n        parser = cli_parser(parser)\n\n    return parser\n\n\ndef run_cli(cli_parser, run_analysis, known_args=None):\n    parser = create(cli_parser)\n    args = parser.parse_args(known_args)\n\n    run_analysis(args)\n"""
src/SALib/analyze/delta.py,21,"b'from __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom scipy.stats import norm, gaussian_kde, rankdata\r\n\r\nimport numpy as np\r\n\r\nfrom . import common_args\r\nfrom ..util import read_param_file, ResultDict\r\n\r\n\r\ndef analyze(problem, X, Y, num_resamples=100,\r\n            conf_level=0.95, print_to_console=False, seed=None):\r\n    """"""Perform Delta Moment-Independent Analysis on model outputs.\r\n\r\n    Returns a dictionary with keys \'delta\', \'delta_conf\', \'S1\', and \'S1_conf\',\r\n    where each entry is a list of size D (the number of parameters) containing\r\n    the indices in the same order as the parameter file.\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        The problem definition\r\n    X: numpy.matrix\r\n        A NumPy matrix containing the model inputs\r\n    Y : numpy.array\r\n        A NumPy array containing the model outputs\r\n    num_resamples : int\r\n        The number of resamples when computing confidence intervals (default 10)\r\n    conf_level : float\r\n        The confidence interval level (default 0.95)\r\n    print_to_console : bool\r\n        Print results directly to console (default False)\r\n\r\n    References\r\n    ----------\r\n    .. [1] Borgonovo, E. (2007). ""A new uncertainty importance measure.""\r\n           Reliability Engineering & System Safety, 92(6):771-784,\r\n           doi:10.1016/j.ress.2006.04.015.\r\n\r\n    .. [2] Plischke, E., E. Borgonovo, and C. L. Smith (2013). ""Global\r\n           sensitivity measures from given data."" European Journal of\r\n           Operational Research, 226(3):536-550, doi:10.1016/j.ejor.2012.11.047.\r\n\r\n    Examples\r\n    --------\r\n    >>> X = latin.sample(problem, 1000)\r\n    >>> Y = Ishigami.evaluate(X)\r\n    >>> Si = delta.analyze(problem, X, Y, print_to_console=True)\r\n    """"""\r\n    if seed:\r\n        np.random.seed(seed)\r\n\r\n    D = problem[\'num_vars\']\r\n    N = Y.size\r\n\r\n    if not 0 < conf_level < 1:\r\n        raise RuntimeError(""Confidence level must be between 0-1."")\r\n\r\n    # equal frequency partition\r\n    exp = (2 / (7 + np.tanh((1500 - N) / 500)))\r\n    M = min(int(np.ceil(N**exp)), 48)\r\n    m = np.linspace(0, N, M + 1)\r\n    Ygrid = np.linspace(np.min(Y), np.max(Y), 100)\r\n\r\n    keys = (\'delta\', \'delta_conf\', \'S1\', \'S1_conf\')\r\n    S = ResultDict((k, np.zeros(D)) for k in keys)\r\n    S[\'names\'] = problem[\'names\']\r\n\r\n    if print_to_console:\r\n        print(""Parameter %s %s %s %s"" % keys)\r\n\r\n    try:\r\n        for i in range(D):\r\n            X_i = X[:, i]\r\n            S[\'delta\'][i], S[\'delta_conf\'][i] = bias_reduced_delta(\r\n                Y, Ygrid, X_i, m, num_resamples, conf_level)\r\n            S[\'S1\'][i] = sobol_first(Y, X_i, m)\r\n            S[\'S1_conf\'][i] = sobol_first_conf(\r\n                Y, X_i, m, num_resamples, conf_level)\r\n            if print_to_console:\r\n                print(""%s %f %f %f %f"" % (S[\'names\'][i], S[\'delta\'][\r\n                    i], S[\'delta_conf\'][i], S[\'S1\'][i], S[\'S1_conf\'][i]))\r\n    except np.linalg.LinAlgError as e:\r\n        msg = ""Singular matrix detected\\n""\r\n        msg += ""This may be due to the sample size ({}) being too small\\n"".format(Y.size)\r\n        msg += ""If this is not the case, check Y values or raise an issue with the\\n""\r\n        msg += ""SALib team""\r\n\r\n        raise np.linalg.LinAlgError(msg)\r\n\r\n    return S\r\n\r\n# Plischke et al. 2013 estimator (eqn 26) for d_hat\r\n\r\n\r\ndef calc_delta(Y, Ygrid, X, m):\r\n    N = len(Y)\r\n    fy = gaussian_kde(Y, bw_method=\'silverman\')(Ygrid)\r\n    abs_fy = np.abs(fy)\r\n    xr = rankdata(X, method=\'ordinal\')\r\n\r\n    d_hat = 0\r\n    for j in range(len(m) - 1):\r\n        ix = np.where((xr > m[j]) & (xr <= m[j + 1]))[0]\r\n        nm = len(ix)\r\n\r\n        Y_ix = Y[ix]\r\n        if not np.all(np.equal(Y_ix, Y_ix[0])):\r\n            fyc = gaussian_kde(Y_ix, bw_method=\'silverman\')(Ygrid)\r\n            fy_ = np.abs(fy - fyc)\r\n        else:\r\n            fy_ = abs_fy\r\n        \r\n        d_hat += (nm / (2 * N)) * np.trapz(fy_, Ygrid)\r\n\r\n    return d_hat\r\n\r\n\r\ndef bias_reduced_delta(Y, Ygrid, X, m, num_resamples, conf_level):\r\n    """"""Plischke et al. 2013 bias reduction technique (eqn 30)""""""\r\n    d = np.zeros(num_resamples)\r\n    d_hat = calc_delta(Y, Ygrid, X, m)\r\n\r\n    N = len(Y)\r\n    r = np.random.randint(N, size=(num_resamples, N))\r\n    for i in range(num_resamples):\r\n        r_i = r[i, :]\r\n        d[i] = calc_delta(Y[r_i], Ygrid, X[r_i], m)\r\n\r\n    d = 2 * d_hat - d\r\n    return (d.mean(), norm.ppf(0.5 + conf_level / 2) * d.std(ddof=1))\r\n\r\n\r\ndef sobol_first(Y, X, m):\r\n    xr = rankdata(X, method=\'ordinal\')\r\n    Vi = 0\r\n    N = len(Y)\r\n    Y_mean = Y.mean()\r\n    for j in range(len(m) - 1):\r\n        ix = np.where((xr > m[j]) & (xr <= m[j + 1]))[0]\r\n        nm = len(ix)\r\n        Vi += (nm / N) * ((Y[ix].mean() - Y_mean)**2)\r\n    return Vi / np.var(Y)\r\n\r\n\r\ndef sobol_first_conf(Y, X, m, num_resamples, conf_level):\r\n    s = np.zeros(num_resamples)\r\n\r\n    N = len(Y)\r\n    r = np.random.randint(N, size=(num_resamples, N))\r\n    for i in range(num_resamples):\r\n        r_i = r[i, :]\r\n        s[i] = sobol_first(Y[r_i], X[r_i], m)\r\n\r\n    return norm.ppf(0.5 + conf_level / 2) * s.std(ddof=1)\r\n\r\n\r\ndef cli_parse(parser):\r\n    parser.add_argument(\'-X\', \'--model-input-file\', type=str, required=True,\r\n                        default=None,\r\n                        help=\'Model input file\')\r\n    parser.add_argument(\'-r\', \'--resamples\', type=int, required=False,\r\n                        default=10,\r\n                        help=\'Number of bootstrap resamples for \\\r\n                           Sobol confidence intervals\')\r\n    return parser\r\n\r\n\r\ndef cli_action(args):\r\n    problem = read_param_file(args.paramfile)\r\n    Y = np.loadtxt(args.model_output_file,\r\n                   delimiter=args.delimiter, usecols=(args.column,))\r\n    X = np.loadtxt(args.model_input_file, delimiter=args.delimiter, ndmin=2)\r\n    if len(X.shape) == 1:\r\n        X = X.reshape((len(X), 1))\r\n\r\n    analyze(problem, X, Y, num_resamples=args.resamples, print_to_console=True,\r\n            seed=args.seed)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    common_args.run_cli(cli_parse, cli_action)\r\n'"
src/SALib/analyze/dgsm.py,13,"b'from __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom scipy.stats import norm\r\n\r\nimport numpy as np\r\n\r\nfrom . import common_args\r\nfrom ..util import read_param_file, ResultDict\r\n\r\n\r\ndef analyze(problem, X, Y, num_resamples=100,\r\n            conf_level=0.95, print_to_console=False, seed=None):\r\n    """"""Calculates Derivative-based Global Sensitivity Measure on model outputs.\r\n\r\n    Returns a dictionary with keys \'vi\', \'vi_std\', \'dgsm\', and \'dgsm_conf\',\r\n    where each entry is a list of size D (the number of parameters) containing\r\n    the indices in the same order as the parameter file.\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        The problem definition\r\n    X : numpy.matrix\r\n        The NumPy matrix containing the model inputs\r\n    Y : numpy.array\r\n        The NumPy array containing the model outputs\r\n    num_resamples : int\r\n        The number of resamples used to compute the confidence\r\n        intervals (default 1000)\r\n    conf_level : float\r\n        The confidence interval level (default 0.95)\r\n    print_to_console : bool\r\n        Print results directly to console (default False)\r\n\r\n    References\r\n    ----------\r\n    .. [1] Sobol, I. M. and S. Kucherenko (2009). ""Derivative based global\r\n           sensitivity measures and their link with global sensitivity\r\n           indices."" Mathematics and Computers in Simulation, 79(10):3009-3017,\r\n           doi:10.1016/j.matcom.2009.01.023.\r\n    """"""\r\n    if seed:\r\n        np.random.seed(seed)\r\n\r\n    D = problem[\'num_vars\']\r\n    Y_size = Y.size\r\n\r\n    if Y_size % (D + 1) == 0:\r\n        N = int(Y_size / (D + 1))\r\n    else:\r\n        raise RuntimeError(""Incorrect number of samples in model output file."")\r\n\r\n    if not 0 < conf_level < 1:\r\n        raise RuntimeError(""Confidence level must be between 0-1."")\r\n\r\n    dims = (N, D)\r\n    base = np.empty(N)\r\n    X_base = np.empty(dims)\r\n    perturbed = np.empty(dims)\r\n    X_perturbed = np.empty(dims)\r\n    step = D + 1\r\n\r\n    base = Y[0:Y_size:step]\r\n    X_base = X[0:Y_size:step, :]\r\n\r\n    # First order (+conf.) and Total order (+conf.)\r\n    keys = (\'vi\', \'vi_std\', \'dgsm\', \'dgsm_conf\')\r\n    S = ResultDict((k, np.empty(D)) for k in keys)\r\n    S[\'names\'] = problem[\'names\']\r\n\r\n    if print_to_console:\r\n        print(""Parameter %s %s %s %s"" % keys)\r\n\r\n    bounds = problem[\'bounds\']\r\n    for j in range(D):\r\n        perturbed[:, j] = Y[(j + 1):Y_size:step]\r\n        X_perturbed[:, j] = X[(j + 1):Y_size:step, j]\r\n\r\n        diff = X_perturbed[:, j] - X_base[:, j]\r\n        perturbed_j = perturbed[:, j]\r\n        S[\'vi\'][j], S[\'vi_std\'][j] = calc_vi_stats(base,\r\n                                                   perturbed_j,\r\n                                                   diff)\r\n        S[\'dgsm\'][j], S[\'dgsm_conf\'][j] = calc_dgsm(base,\r\n                                                    perturbed_j,\r\n                                                    diff,\r\n                                                    bounds[j],\r\n                                                    num_resamples,\r\n                                                    conf_level)\r\n\r\n        if print_to_console:\r\n            print(""%s %f %f %f %f"" % (\r\n                S[\'names\'][j], S[\'vi\'][j], S[\'vi_std\'][j], S[\'dgsm\'][j], S[\'dgsm_conf\'][j]))\r\n\r\n    return S\r\n\r\n\r\ndef calc_vi_stats(base, perturbed, x_delta):\r\n    """"""Calculate v_i mean and std.\r\n    \r\n    v_i sensitivity measure following Sobol and Kucherenko (2009)\r\n    For comparison, Morris mu* < sqrt(v_i)\r\n\r\n    Same as calc_vi_mean but returns standard deviation as well.\r\n    """"""\r\n    dfdx = ((perturbed - base) / x_delta)**2\r\n    return np.mean(dfdx), np.std(dfdx)\r\n\r\n\r\ndef calc_vi_mean(base, perturbed, x_delta):\r\n    """"""Calculate v_i mean.\r\n\r\n    Same as calc_vi_stats but only returns the mean.\r\n    """"""\r\n    dfdx = ((perturbed - base) / x_delta)**2\r\n    return dfdx.mean()\r\n\r\n\r\ndef calc_dgsm(base, perturbed, x_delta, bounds, num_resamples, conf_level):\r\n    """"""v_i sensitivity measure following Sobol and Kucherenko (2009).\r\n    For comparison, total order S_tot <= dgsm\r\n    """"""\r\n    D = np.var(base)\r\n    vi = calc_vi_mean(base, perturbed, x_delta)\r\n    dgsm = vi * (bounds[1] - bounds[0])**2 / (D * np.pi**2)\r\n\r\n    len_base = len(base)\r\n    s = np.empty(num_resamples)\r\n    r = np.random.randint(len_base, size=(num_resamples, len_base))\r\n    for i in range(num_resamples):\r\n        r_i = r[i]\r\n        s[i] = calc_vi_mean(base[r_i], perturbed[r_i], x_delta[r_i])\r\n\r\n    return dgsm, norm.ppf(0.5 + conf_level / 2.0) * s.std(ddof=1)\r\n\r\n\r\ndef cli_parse(parser):\r\n    parser.add_argument(\'-X\', \'--model-input-file\', type=str,\r\n                        required=True, default=None,\r\n                        help=\'Model input file\')\r\n    parser.add_argument(\'-r\', \'--resamples\', type=int, required=False,\r\n                        default=1000,\r\n                        help=\'Number of bootstrap resamples for Sobol \\\r\n                           confidence intervals\')\r\n    return parser\r\n\r\n\r\ndef cli_action(args):\r\n    problem = read_param_file(args.paramfile)\r\n\r\n    Y = np.loadtxt(args.model_output_file,\r\n                   delimiter=args.delimiter, usecols=(args.column,))\r\n    X = np.loadtxt(args.model_input_file, delimiter=args.delimiter, ndmin=2)\r\n    if len(X.shape) == 1:\r\n        X = X.reshape((len(X), 1))\r\n\r\n    analyze(problem, X, Y, num_resamples=args.resamples, print_to_console=True,\r\n            seed=args.seed)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    common_args.run_cli(cli_parse, cli_action)\r\n'"
src/SALib/analyze/fast.py,14,"b'from __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport math\r\nfrom sys import exit\r\n\r\nimport numpy as np\r\n\r\nfrom . import common_args\r\nfrom ..util import read_param_file, ResultDict\r\n\r\n\r\ndef analyze(problem, Y, M=4, print_to_console=False, seed=None):\r\n    """"""Performs the Fourier Amplitude Sensitivity Test (FAST) on model outputs.\r\n\r\n    Returns a dictionary with keys \'S1\' and \'ST\', where each entry is a list of\r\n    size D (the number of parameters) containing the indices in the same order\r\n    as the parameter file.\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        The problem definition\r\n    Y : numpy.array\r\n        A NumPy array containing the model outputs\r\n    M : int\r\n        The interference parameter, i.e., the number of harmonics to sum in\r\n        the Fourier series decomposition (default 4)\r\n    print_to_console : bool\r\n        Print results directly to console (default False)\r\n\r\n    References\r\n    ----------\r\n    .. [1] Cukier, R. I., C. M. Fortuin, K. E. Shuler, A. G. Petschek, and J. H.\r\n           Schaibly (1973).  ""Study of the sensitivity of coupled reaction\r\n           systems to uncertainties in rate coefficients.""  J. Chem. Phys.,\r\n           59(8):3873-3878, doi:10.1063/1.1680571.\r\n\r\n    .. [2] Saltelli, A., S. Tarantola, and K. P.-S. Chan (1999).  ""A\r\n          Quantitative Model-Independent Method for Global Sensitivity\r\n          Analysis of Model Output.""  Technometrics, 41(1):39-56,\r\n          doi:10.1080/00401706.1999.10485594.\r\n\r\n    Examples\r\n    --------\r\n    >>> X = fast_sampler.sample(problem, 1000)\r\n    >>> Y = Ishigami.evaluate(X)\r\n    >>> Si = fast.analyze(problem, Y, print_to_console=False)\r\n    """"""\r\n    if seed:\r\n        np.random.seed(seed)\r\n\r\n    D = problem[\'num_vars\']\r\n\r\n    if Y.size % (D) == 0:\r\n        N = int(Y.size / D)\r\n    else:\r\n        print(""""""\r\n            Error: Number of samples in model output file must be a multiple of D,\r\n            where D is the number of parameters in your parameter file.\r\n          """""")\r\n        exit()\r\n\r\n    # Recreate the vector omega used in the sampling\r\n    omega = np.zeros([D])\r\n    omega[0] = math.floor((N - 1) / (2 * M))\r\n    m = math.floor(omega[0] / (2 * M))\r\n\r\n    if m >= (D - 1):\r\n        omega[1:] = np.floor(np.linspace(1, m, D - 1))\r\n    else:\r\n        omega[1:] = np.arange(D - 1) % m + 1\r\n\r\n    # Calculate and Output the First and Total Order Values\r\n    if print_to_console:\r\n        print(""Parameter First Total"")\r\n    Si = ResultDict((k, [None] * D) for k in [\'S1\', \'ST\'])\r\n    Si[\'names\'] = problem[\'names\']\r\n\r\n    for i in range(D):\r\n        l = np.arange(i * N, (i + 1) * N)\r\n        Si[\'S1\'][i] = compute_first_order(Y[l], N, M, omega[0])\r\n        Si[\'ST\'][i] = compute_total_order(Y[l], N, omega[0])\r\n        if print_to_console:\r\n            print(""%s %f %f"" %\r\n                  (problem[\'names\'][i], Si[\'S1\'][i], Si[\'ST\'][i]))\r\n\r\n    return Si\r\n\r\n\r\ndef compute_first_order(outputs, N, M, omega):\r\n    f = np.fft.fft(outputs)\r\n    Sp = np.power(np.absolute(f[np.arange(1, int((N + 1) / 2))]) / N, 2)\r\n    V = 2 * np.sum(Sp)\r\n    D1 = 2 * np.sum(Sp[np.arange(1, M + 1) * int(omega) - 1])\r\n    return D1 / V\r\n\r\n\r\ndef compute_total_order(outputs, N, omega):\r\n    f = np.fft.fft(outputs)\r\n    Sp = np.power(np.absolute(f[np.arange(1, int((N + 1) / 2))]) / N, 2)\r\n    V = 2 * np.sum(Sp)\r\n    Dt = 2 * sum(Sp[np.arange(int(omega / 2))])\r\n    return (1 - Dt / V)\r\n\r\n\r\n# No additional arguments required for FAST\r\ncli_parse = None\r\n\r\n\r\ndef cli_action(args):\r\n    problem = read_param_file(args.paramfile)\r\n    Y = np.loadtxt(args.model_output_file,\r\n                   delimiter=args.delimiter, usecols=(args.column,))\r\n\r\n    analyze(problem, Y, print_to_console=True, seed=args.seed)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    common_args.run_cli(cli_parse, cli_action)\r\n'"
src/SALib/analyze/ff.py,5,"b'\'\'\'\nCreated on 30 Jun 2015\n\n@author: will2\n\'\'\'\n\nfrom __future__ import print_function\nimport numpy as np\nfrom . import common_args\n\nimport pandas as pd\nfrom types import MethodType\n\nfrom SALib.util import read_param_file, ResultDict\nfrom SALib.sample.ff import generate_contrast, extend_bounds\n\n\ndef analyze(problem, X, Y, second_order=False, print_to_console=False,\n            seed=None):\n    """"""Perform a fractional factorial analysis\n\n    Returns a dictionary with keys \'ME\' (main effect) and \'IE\' (interaction\n    effect). The techniques bulks out the number of parameters with dummy\n    parameters to the nearest 2**n.  Any results involving dummy parameters\n    could indicate a problem with the model runs.\n\n    Arguments\n    ---------\n    problem: dict\n        The problem definition\n    X: numpy.matrix\n        The NumPy matrix containing the model inputs\n    Y: numpy.array\n        The NumPy array containing the model outputs\n    second_order: bool, default=False\n        Include interaction effects\n    print_to_console: bool, default=False\n        Print results directly to console\n\n    Returns\n    -------\n    Si: dict\n        A dictionary of sensitivity indices, including main effects ``ME``,\n        and interaction effects ``IE`` (if ``second_order`` is True)\n\n    Examples\n    --------\n    >>> X = sample(problem)\n    >>> Y = X[:, 0] + (0.1 * X[:, 1]) + ((1.2 * X[:, 2]) * (0.2 + X[:, 0]))\n    >>> analyze(problem, X, Y, second_order=True, print_to_console=True)\n    """"""\n    if seed:\n        np.random.seed(seed)\n\n    problem = extend_bounds(problem)\n    num_vars = problem[\'num_vars\']\n\n    X = generate_contrast(problem)\n\n    main_effect = (1. / (2 * num_vars)) * np.dot(Y, X)\n\n    Si = ResultDict((k, [None] * num_vars)\n                    for k in [\'names\', \'ME\'])\n    Si[\'ME\'] = main_effect\n    Si[\'names\'] = problem[\'names\']\n\n    if print_to_console:\n        print(""Parameter ME"")\n        for j in range(num_vars):\n            print(""%s %f"" % (problem[\'names\'][j], Si[\'ME\'][j]))\n\n    if second_order:\n        interaction_names, interaction_effects = interactions(problem,\n                                                              Y,\n                                                              print_to_console)\n        Si[\'interaction_names\'] = interaction_names\n        Si[\'IE\'] = interaction_effects\n\n    Si.to_df = MethodType(to_df, Si)\n\n    return Si\n\n\ndef to_df(self):\n    \'\'\'Conversion method to Pandas DataFrame. To be attached to ResultDict.\n\n    Returns\n    -------\n    main_effect, inter_effect: tuple\n        A tuple of DataFrames for main effects and interaction effects.\n        The second element (for interactions) will be `None` if not available.\n    \'\'\'\n    names = self[\'names\']\n    main_effect = self[\'ME\']\n    interactions = self.get(\'IE\', None)\n\n    inter_effect = None\n    if interactions:\n        interaction_names = self.get(\'interaction_names\')\n        names = [name for name in names if not isinstance(name, list)]\n        inter_effect = pd.DataFrame({\'IE\': interactions},\n                                    index=interaction_names)\n\n    main_effect = pd.DataFrame({\'ME\': main_effect}, index=names)\n\n    return main_effect, inter_effect\n\n\ndef interactions(problem, Y, print_to_console=False):\n    """"""Computes the second order effects\n\n    Computes the second order effects (interactions) between\n    all combinations of pairs of input factors\n\n    Arguments\n    ---------\n    problem: dict\n        The problem definition\n    Y: numpy.array\n        The NumPy array containing the model outputs\n    print_to_console: bool, default=False\n        Print results directly to console\n\n    Returns\n    -------\n    ie_names: list\n        The names of the interaction pairs\n    IE: list\n        The sensitivity indices for the pairwise interactions\n\n    """"""\n\n    names = problem[\'names\']\n    num_vars = problem[\'num_vars\']\n\n    X = generate_contrast(problem)\n\n    ie_names = []\n    IE = []\n\n    for col in range(X.shape[1]):\n        for col_2 in range(col):\n            x = X[:, col] * X[:, col_2]\n            var_names = (names[col_2], names[col])\n            ie_names.append(var_names)\n            IE.append((1. / (2 * num_vars)) * np.dot(Y, x))\n    if print_to_console:\n        [print(\'%s %f\' % (n, i)) for (n, i) in zip(ie_names, IE)]\n\n    return ie_names, IE\n\n\ndef cli_parse(parser):\n    parser.add_argument(\'-X\', \'--model-input-file\', type=str,\n                        required=True, default=None,\n                        help=\'Model input file\')\n    parser.add_argument(\'--max-order\', type=int, required=False, default=2,\n                        choices=[1, 2], help=\'Maximum order of sensitivity \\\n                           indices to calculate\')\n    return parser\n\n\ndef cli_action(args):\n    problem = read_param_file(args.paramfile)\n    Y = np.loadtxt(args.model_output_file,\n                   delimiter=args.delimiter, usecols=(args.column,))\n    X = np.loadtxt(args.model_input_file, delimiter=args.delimiter, ndmin=2)\n    if len(X.shape) == 1:\n        X = X.reshape((len(X), 1))\n    analyze(problem, X, Y, (args.max_order == 2), print_to_console=True,\n            seed=args.seed)\n\n\nif __name__ == ""__main__"":\n    common_args.run_cli(cli_parse, cli_action)\n'"
src/SALib/analyze/morris.py,29,"b'from __future__ import division\nfrom __future__ import print_function\n\nfrom scipy.stats import norm\n\nimport numpy as np\n\nfrom . import common_args\nfrom ..util import read_param_file, compute_groups_matrix, ResultDict\nfrom ..sample.morris import compute_delta\n\n\ndef analyze(problem, X, Y,\n            num_resamples=100,\n            conf_level=0.95,\n            print_to_console=False,\n            num_levels=4,\n            seed=None):\n    """"""Perform Morris Analysis on model outputs.\n\n    Returns a dictionary with keys \'mu\', \'mu_star\', \'sigma\', and\n    \'mu_star_conf\', where each entry is a list of parameters containing\n    the indices in the same order as the parameter file.\n\n    Arguments\n    ---------\n    problem : dict\n        The problem definition\n    X : numpy.matrix\n        The NumPy matrix containing the model inputs of dtype=float\n    Y : numpy.array\n        The NumPy array containing the model outputs of dtype=float\n    num_resamples : int\n        The number of resamples used to compute the confidence\n        intervals (default 1000)\n    conf_level : float\n        The confidence interval level (default 0.95)\n    print_to_console : bool\n        Print results directly to console (default False)\n    num_levels : int\n        The number of grid levels, must be identical to the value\n        passed to SALib.sample.morris (default 4)\n\n    Returns\n    -------\n    Si : dict\n        A dictionary of sensitivity indices containing the following entries.\n\n        - `mu` - the mean elementary effect\n        - `mu_star` - the absolute of the mean elementary effect\n        - `sigma` - the standard deviation of the elementary effect\n        - `mu_star_conf` - the bootstrapped confidence interval\n        - `names` - the names of the parameters\n\n    References\n    ----------\n    .. [1] Morris, M. (1991).  ""Factorial Sampling Plans for Preliminary\n           Computational Experiments.""  Technometrics, 33(2):161-174,\n           doi:10.1080/00401706.1991.10484804.\n    .. [2] Campolongo, F., J. Cariboni, and A. Saltelli (2007).  ""An effective\n           screening design for sensitivity analysis of large models.""\n           Environmental Modelling & Software, 22(10):1509-1518,\n           doi:10.1016/j.envsoft.2006.10.004.\n\n    Examples\n    --------\n    >>> X = morris.sample(problem, 1000, num_levels=4)\n    >>> Y = Ishigami.evaluate(X)\n    >>> Si = morris.analyze(problem, X, Y, conf_level=0.95,\n    >>>                     print_to_console=True, num_levels=4)\n\n    """"""\n    if seed:\n        np.random.seed(seed)\n\n    msg = (""dtype of {} array must be \'float\', float32 or float64"")\n    if X.dtype not in [\'float\', \'float32\', \'float64\']:\n        raise ValueError(msg.format(\'X\'))\n    if Y.dtype not in [\'float\', \'float32\', \'float64\']:\n        raise ValueError(msg.format(\'Y\'))\n\n    # Assume that there are no groups\n    groups = None\n    delta = compute_delta(num_levels)\n\n    num_vars = problem[\'num_vars\']\n\n    if (problem.get(\'groups\') is None) & (Y.size % (num_vars + 1) == 0):\n        num_trajectories = int(Y.size / (num_vars + 1))\n    elif problem.get(\'groups\') is not None:\n        groups, unique_group_names = compute_groups_matrix(\n            problem[\'groups\'])\n        number_of_groups = len(unique_group_names)\n        num_trajectories = int(Y.size / (number_of_groups + 1))\n    else:\n        raise ValueError(""Number of samples in model output file must be""\n                         ""a multiple of (D+1), where D is the number of""\n                         ""parameters (or groups) in your parameter file."")\n    ee = np.zeros((num_vars, num_trajectories))\n    ee = compute_elementary_effects(\n        X, Y, int(Y.size / num_trajectories), delta)\n\n    # Output the Mu, Mu*, and Sigma Values. Also return them in case this is\n    # being called from Python\n    Si = ResultDict((k, [None] * num_vars)\n                    for k in [\'names\', \'mu\', \'mu_star\', \'sigma\', \'mu_star_conf\'])\n    Si[\'mu\'] = np.average(ee, 1)\n    Si[\'mu_star\'] = np.average(np.abs(ee), 1)\n    Si[\'sigma\'] = np.std(ee, axis=1, ddof=1)\n    Si[\'names\'] = problem[\'names\']\n\n    for j in range(num_vars):\n        Si[\'mu_star_conf\'][j] = compute_mu_star_confidence(\n            ee[j, :], num_trajectories, num_resamples, conf_level)\n\n    if groups is None:\n        if print_to_console:\n            print(""{0:<30} {1:>10} {2:>10} {3:>15} {4:>10}"".format(\n                ""Parameter"",\n                ""Mu_Star"",\n                ""Mu"",\n                ""Mu_Star_Conf"",\n                ""Sigma"")\n            )\n            for j in list(range(num_vars)):\n                print(""{0:30} {1:10.3f} {2:10.3f} {3:15.3f} {4:10.3f}"".format(\n                    Si[\'names\'][j],\n                    Si[\'mu_star\'][j],\n                    Si[\'mu\'][j],\n                    Si[\'mu_star_conf\'][j],\n                    Si[\'sigma\'][j])\n                )\n        return Si\n    elif groups is not None:\n        # if there are groups, then the elementary effects returned need to be\n        # computed over the groups of variables,\n        # rather than the individual variables\n        Si_grouped = dict((k, [None] * num_vars)\n                          for k in [\'mu_star\', \'mu_star_conf\'])\n        Si_grouped[\'mu_star\'] = compute_grouped_metric(Si[\'mu_star\'], groups)\n        Si_grouped[\'mu_star_conf\'] = compute_grouped_metric(Si[\'mu_star_conf\'],\n                                                            groups)\n        Si_grouped[\'names\'] = unique_group_names\n        Si_grouped[\'sigma\'] = compute_grouped_sigma(Si[\'sigma\'], groups)\n        Si_grouped[\'mu\'] = compute_grouped_sigma(Si[\'mu\'], groups)\n\n        if print_to_console:\n            print(""{0:<30} {1:>10} {2:>10} {3:>15} {4:>10}"".format(\n                ""Parameter"",\n                ""Mu_Star"",\n                ""Mu"",\n                ""Mu_Star_Conf"",\n                ""Sigma"")\n            )\n            for j in list(range(number_of_groups)):\n                print(""{0:30} {1:10.3f} {2:10.3f} {3:15.3f} {4:10.3f}"".format(\n                    Si_grouped[\'names\'][j],\n                    Si_grouped[\'mu_star\'][j],\n                    Si_grouped[\'mu\'][j],\n                    Si_grouped[\'mu_star_conf\'][j],\n                    Si_grouped[\'sigma\'][j])\n                )\n\n        return Si_grouped\n    else:\n        raise RuntimeError(\n            ""Could not determine which parameters should be returned"")\n\n\ndef compute_grouped_sigma(ungrouped_sigma, group_matrix):\n    \'\'\'\n    Returns sigma for the groups of parameter values in the\n    argument ungrouped_metric where the group consists of no more than\n    one parameter\n    \'\'\'\n\n    group_matrix = np.array(group_matrix, dtype=np.bool)\n\n    sigma_masked = np.ma.masked_array(ungrouped_sigma * group_matrix.T,\n                                      mask=(group_matrix ^ 1).T)\n    sigma_agg = np.ma.mean(sigma_masked, axis=1)\n    sigma = np.zeros(group_matrix.shape[1], dtype=np.float)\n    np.copyto(sigma, sigma_agg, where=group_matrix.sum(axis=0) == 1)\n    np.copyto(sigma, np.NAN, where=group_matrix.sum(axis=0) != 1)\n\n    return sigma\n\n\ndef compute_grouped_metric(ungrouped_metric, group_matrix):\n    \'\'\'\n    Computes the mean value for the groups of parameter values in the\n    argument ungrouped_metric\n    \'\'\'\n\n    group_matrix = np.array(group_matrix, dtype=np.bool)\n\n    mu_star_masked = np.ma.masked_array(ungrouped_metric * group_matrix.T,\n                                        mask=(group_matrix ^ 1).T)\n    mean_of_mu_star = np.ma.mean(mu_star_masked, axis=1)\n\n    return mean_of_mu_star\n\n\ndef get_increased_values(op_vec, up, lo):\n\n    up = np.pad(up, ((0, 0), (1, 0), (0, 0)), \'constant\')\n    lo = np.pad(lo, ((0, 0), (0, 1), (0, 0)), \'constant\')\n\n    res = np.einsum(\'ik,ikj->ij\', op_vec, up + lo)\n\n    return res.T\n\n\ndef get_decreased_values(op_vec, up, lo):\n\n    up = np.pad(up, ((0, 0), (0, 1), (0, 0)), \'constant\')\n    lo = np.pad(lo, ((0, 0), (1, 0), (0, 0)), \'constant\')\n\n    res = np.einsum(\'ik,ikj->ij\', op_vec, up + lo)\n\n    return res.T\n\n\ndef compute_elementary_effects(model_inputs, model_outputs, trajectory_size,\n                               delta):\n    \'\'\'\n    Arguments\n    ---------\n    model_inputs : matrix of inputs to the model under analysis.\n        x-by-r where x is the number of variables and\n        r is the number of rows (a function of x and num_trajectories)\n    model_outputs\n        an r-length vector of model outputs\n    trajectory_size\n        a scalar indicating the number of rows in a trajectory\n    delta : float\n        scaling factor computed from `num_levels`\n\n    Returns\n    ---------\n    ee : np.array\n        Elementary Effects for each parameter\n    \'\'\'\n    num_vars = model_inputs.shape[1]\n    num_rows = model_inputs.shape[0]\n    num_trajectories = int(num_rows / trajectory_size)\n\n    ee = np.zeros((num_trajectories, num_vars), dtype=np.float)\n\n    ip_vec = model_inputs.reshape(num_trajectories, trajectory_size, num_vars)\n    ip_cha = np.subtract(ip_vec[:, 1:, :], ip_vec[:, 0:-1, :])\n    up = (ip_cha > 0)\n    lo = (ip_cha < 0)\n\n    op_vec = model_outputs.reshape(num_trajectories, trajectory_size)\n\n    result_up = get_increased_values(op_vec, up, lo)\n    result_lo = get_decreased_values(op_vec, up, lo)\n\n    ee = np.subtract(result_up, result_lo)\n    np.divide(ee, delta, out=ee)\n\n    return ee\n\n\ndef compute_mu_star_confidence(ee, num_trajectories, num_resamples,\n                               conf_level):\n    \'\'\'\n    Uses bootstrapping where the elementary effects are resampled with\n    replacement to produce a histogram of resampled mu_star metrics.\n    This resample is used to produce a confidence interval.\n    \'\'\'\n    if not 0 < conf_level < 1:\n        raise ValueError(""Confidence level must be between 0-1."")\n\n    resample_index = np.random.randint(\n        len(ee), size=(num_resamples, num_trajectories))\n    ee_resampled = ee[resample_index]\n\n    # Compute average of the absolute values over each of the resamples\n    mu_star_resampled = np.average(np.abs(ee_resampled), axis=1)\n\n    return norm.ppf(0.5 + conf_level / 2) * mu_star_resampled.std(ddof=1)\n\n\ndef cli_parse(parser):\n    parser.add_argument(\'-X\', \'--model-input-file\', type=str,\n                        required=True, default=None,\n                        help=\'Model input file\')\n    parser.add_argument(\'-r\', \'--resamples\', type=int, required=False,\n                        default=1000,\n                        help=\'Number of bootstrap resamples for Sobol \\\n                           confidence intervals\')\n    parser.add_argument(\'-l\', \'--levels\', type=int, required=False,\n                        default=4, help=\'Number of grid levels \\\n                           (Morris only)\')\n    parser.add_argument(\'--grid-jump\', type=int, required=False,\n                        default=2, help=\'Grid jump size (Morris only)\')\n    return parser\n\n\ndef cli_action(args):\n    problem = read_param_file(args.paramfile)\n    Y = np.loadtxt(args.model_output_file,\n                   delimiter=args.delimiter, usecols=(args.column,))\n    X = np.loadtxt(args.model_input_file, delimiter=args.delimiter, ndmin=2)\n    if len(X.shape) == 1:\n        X = X.reshape((len(X), 1))\n\n    analyze(problem, X, Y, num_resamples=args.resamples, print_to_console=True,\n            num_levels=args.levels, seed=args.seed)\n\n\nif __name__ == ""__main__"":\n    common_args.run_cli(cli_parse, cli_action)\n'"
src/SALib/analyze/rbd_fast.py,7,"b'#!/usr/bin/env python\r\n# coding=utf8\r\n\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nfrom scipy.signal import periodogram\r\n\r\nfrom . import common_args\r\nfrom ..util import read_param_file, ResultDict\r\n\r\n\r\ndef analyze(problem, X, Y, M=10, print_to_console=False, seed=None):\r\n    """"""Performs the Random Balanced Design - Fourier Amplitude Sensitivity Test\r\n    (RBD-FAST) on model outputs.\r\n\r\n    Returns a dictionary with keys \'S1\', where each entry is a list of\r\n    size D (the number of parameters) containing the indices in the same order\r\n    as the parameter file.\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        The problem definition\r\n    X : numpy.array\r\n        A NumPy array containing the model inputs\r\n    Y : numpy.array\r\n        A NumPy array containing the model outputs\r\n    M : int\r\n        The interference parameter, i.e., the number of harmonics to sum in\r\n        the Fourier series decomposition (default 10)\r\n    print_to_console : bool\r\n        Print results directly to console (default False)\r\n\r\n    References\r\n    ----------\r\n    .. [1] S. Tarantola, D. Gatelli and T. Mara (2006) ""Random Balance Designs\r\n          for the Estimation of First Order Global Sensitivity Indices"",\r\n          Reliability Engineering and System Safety, 91:6, 717-727\r\n\r\n    .. [2] Elmar Plischke (2010) ""An effective algorithm for computing global\r\n          sensitivity indices (EASI) Reliability Engineering & System Safety"",\r\n          95:4, 354-360. doi:10.1016/j.ress.2009.11.005\r\n\r\n    .. [3] Jean-Yves Tissot, Cl\xc3\xa9mentine Prieur (2012) ""Bias correction for the\r\n          estimation of sensitivity indices based on random balance designs."",\r\n          Reliability Engineering and System Safety, Elsevier, 107, 205-213.\r\n          doi:10.1016/j.ress.2012.06.010\r\n\r\n    .. [4] Jeanne Goffart, Mickael Rabouille & Nathan Mendes (2015)\r\n          ""Uncertainty and sensitivity analysis applied to hygrothermal\r\n          simulation of a brick building in a hot and humid climate"",\r\n          Journal of Building Performance Simulation.\r\n          doi:10.1080/19401493.2015.1112430\r\n\r\n    Examples\r\n    --------\r\n    >>> X = latin.sample(problem, 1000)\r\n    >>> Y = Ishigami.evaluate(X)\r\n    >>> Si = rbd_fast.analyze(problem, X, Y, print_to_console=False)\r\n    """"""\r\n    if seed:\r\n        np.random.seed(seed)\r\n\r\n    D = problem[\'num_vars\']\r\n    N = Y.size\r\n\r\n    # Calculate and Output the First Order Value\r\n    if print_to_console:\r\n        print(""Parameter First"")\r\n    Si = ResultDict((k, [None] * D) for k in [\'S1\'])\r\n    Si[\'names\'] = problem[\'names\']\r\n\r\n    for i in range(D):\r\n        S1 = compute_first_order(permute_outputs(X[:, i], Y), M)\r\n        S1 = unskew_S1(S1, M, N)\r\n        Si[\'S1\'][i] = S1\r\n        if print_to_console:\r\n            print(""%s %g"" %\r\n                  (problem[\'names\'][i].ljust(9), Si[\'S1\'][i]))\r\n    return Si\r\n\r\n\r\ndef permute_outputs(X, Y):\r\n    """"""\r\n    Permute the output according to one of the inputs as in [_2]\r\n\r\n    References\r\n    ----------\r\n    .. [2] Elmar Plischke (2010) ""An effective algorithm for computing global\r\n          sensitivity indices (EASI) Reliability Engineering & System Safety"",\r\n          95:4, 354-360. doi:10.1016/j.ress.2009.11.005\r\n\r\n    """"""\r\n    permutation_index = np.argsort(X)\r\n    permutation_index = np.concatenate([permutation_index[::2],\r\n                                        permutation_index[1::2][::-1]])\r\n    return Y[permutation_index]\r\n\r\n\r\ndef compute_first_order(permuted_outputs, M):\r\n    _, Pxx = periodogram(permuted_outputs)\r\n    V = np.sum(Pxx[1:])\r\n    D1 = np.sum(Pxx[1: M + 1])\r\n    return D1 / V\r\n\r\n\r\ndef unskew_S1(S1, M, N):\r\n    """"""\r\n    Unskew the sensivity indice\r\n    (Jean-Yves Tissot, Cl\xc3\xa9mentine Prieur (2012) ""Bias correction for the\r\n    estimation of sensitivity indices based on random balance designs."",\r\n    Reliability Engineering and System Safety, Elsevier, 107, 205-213.\r\n    doi:10.1016/j.ress.2012.06.010)\r\n    """"""\r\n    lamb = (2 * M) / N\r\n    return S1 - lamb / (1 - lamb) * (1 - S1)\r\n\r\n\r\ndef cli_parse(parser):\r\n    parser.add_argument(\'-X\', \'--model-input-file\',\r\n                        type=str, required=True, help=\'Model input file\')\r\n    return parser\r\n\r\n\r\ndef cli_action(args):\r\n    problem = read_param_file(args.paramfile)\r\n    X = np.loadtxt(args.model_input_file,\r\n                   delimiter=args.delimiter)\r\n    Y = np.loadtxt(args.model_output_file,\r\n                   delimiter=args.delimiter,\r\n                   usecols=(args.column,))\r\n    analyze(problem, X, Y, print_to_console=True, seed=args.seed)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    common_args.run_cli(cli_parse, cli_action)\r\n'"
src/SALib/analyze/sobol.py,15,"b'from __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom scipy.stats import norm\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nfrom . import common_args\r\nfrom ..util import read_param_file, compute_groups_matrix, ResultDict\r\nfrom types import MethodType\r\n\r\nfrom multiprocessing import Pool, cpu_count\r\nfrom functools import partial\r\nfrom itertools import combinations\r\n\r\ntry:\r\n    from itertools import zip_longest\r\nexcept ImportError:\r\n    # Python 2\r\n    from itertools import izip_longest as zip_longest\r\n\r\n\r\ndef analyze(problem, Y, calc_second_order=True, num_resamples=100,\r\n            conf_level=0.95, print_to_console=False, parallel=False,\r\n            n_processors=None, seed=None):\r\n    """"""Perform Sobol Analysis on model outputs.\r\n\r\n    Returns a dictionary with keys \'S1\', \'S1_conf\', \'ST\', and \'ST_conf\', where\r\n    each entry is a list of size D (the number of parameters) containing the\r\n    indices in the same order as the parameter file.  If calc_second_order is\r\n    True, the dictionary also contains keys \'S2\' and \'S2_conf\'.\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        The problem definition\r\n    Y : numpy.array\r\n        A NumPy array containing the model outputs\r\n    calc_second_order : bool\r\n        Calculate second-order sensitivities (default True)\r\n    num_resamples : int\r\n        The number of resamples (default 100)\r\n    conf_level : float\r\n        The confidence interval level (default 0.95)\r\n    print_to_console : bool\r\n        Print results directly to console (default False)\r\n\r\n    References\r\n    ----------\r\n    .. [1] Sobol, I. M. (2001).  ""Global sensitivity indices for nonlinear\r\n           mathematical models and their Monte Carlo estimates.""  Mathematics\r\n           and Computers in Simulation, 55(1-3):271-280,\r\n           doi:10.1016/S0378-4754(00)00270-6.\r\n    .. [2] Saltelli, A. (2002).  ""Making best use of model evaluations to\r\n           compute sensitivity indices.""  Computer Physics Communications,\r\n           145(2):280-297, doi:10.1016/S0010-4655(02)00280-1.\r\n    .. [3] Saltelli, A., P. Annoni, I. Azzini, F. Campolongo, M. Ratto, and\r\n           S. Tarantola (2010).  ""Variance based sensitivity analysis of model\r\n           output.  Design and estimator for the total sensitivity index.""\r\n           Computer Physics Communications, 181(2):259-270,\r\n           doi:10.1016/j.cpc.2009.09.018.\r\n\r\n    Examples\r\n    --------\r\n    >>> X = saltelli.sample(problem, 1000)\r\n    >>> Y = Ishigami.evaluate(X)\r\n    >>> Si = sobol.analyze(problem, Y, print_to_console=True)\r\n\r\n    """"""\r\n    if seed:\r\n        np.random.seed(seed)\r\n    # determining if groups are defined and adjusting the number\r\n    # of rows in the cross-sampled matrix accordingly\r\n    if not problem.get(\'groups\'):\r\n        D = problem[\'num_vars\']\r\n    else:\r\n        D = len(set(problem[\'groups\']))\r\n\r\n    if calc_second_order and Y.size % (2 * D + 2) == 0:\r\n        N = int(Y.size / (2 * D + 2))\r\n    elif not calc_second_order and Y.size % (D + 2) == 0:\r\n        N = int(Y.size / (D + 2))\r\n    else:\r\n        raise RuntimeError(""""""\r\n        Incorrect number of samples in model output file.\r\n        Confirm that calc_second_order matches option used during sampling."""""")\r\n\r\n    if conf_level < 0 or conf_level > 1:\r\n        raise RuntimeError(""Confidence level must be between 0-1."")\r\n\r\n    # normalize the model output\r\n    Y = (Y - Y.mean()) / Y.std()\r\n\r\n    A, B, AB, BA = separate_output_values(Y, D, N, calc_second_order)\r\n    r = np.random.randint(N, size=(N, num_resamples))\r\n    Z = norm.ppf(0.5 + conf_level / 2)\r\n\r\n    if not parallel:\r\n        S = create_Si_dict(D, calc_second_order)\r\n\r\n        for j in range(D):\r\n            S[\'S1\'][j] = first_order(A, AB[:, j], B)\r\n            S[\'S1_conf\'][j] = Z * first_order(A[r], AB[r, j], B[r]).std(ddof=1)\r\n            S[\'ST\'][j] = total_order(A, AB[:, j], B)\r\n            S[\'ST_conf\'][j] = Z * total_order(A[r], AB[r, j], B[r]).std(ddof=1)\r\n\r\n        # Second order (+conf.)\r\n        if calc_second_order:\r\n            for j in range(D):\r\n                for k in range(j + 1, D):\r\n                    S[\'S2\'][j, k] = second_order(\r\n                        A, AB[:, j], AB[:, k], BA[:, j], B)\r\n                    S[\'S2_conf\'][j, k] = Z * second_order(A[r], AB[r, j],\r\n                                                          AB[r, k], BA[r, j], B[r]).std(ddof=1)\r\n\r\n    else:\r\n        tasks, n_processors = create_task_list(\r\n            D, calc_second_order, n_processors)\r\n\r\n        func = partial(sobol_parallel, Z, A, AB, BA, B, r)\r\n        pool = Pool(n_processors)\r\n        S_list = pool.map_async(func, tasks)\r\n        pool.close()\r\n        pool.join()\r\n\r\n        S = Si_list_to_dict(S_list.get(), D, calc_second_order)\r\n\r\n    # Print results to console\r\n    if print_to_console:\r\n        print_indices(S, problem, calc_second_order)\r\n\r\n    # Add problem context and override conversion method for special case\r\n    S.problem = problem\r\n    S.to_df = MethodType(to_df, S)\r\n    return S\r\n\r\n\r\ndef first_order(A, AB, B):\r\n    # First order estimator following Saltelli et al. 2010 CPC, normalized by\r\n    # sample variance\r\n    return np.mean(B * (AB - A), axis=0) / np.var(np.r_[A, B], axis=0)\r\n\r\n\r\ndef total_order(A, AB, B):\r\n    # Total order estimator following Saltelli et al. 2010 CPC, normalized by\r\n    # sample variance\r\n    return 0.5 * np.mean((A - AB) ** 2, axis=0) / np.var(np.r_[A, B], axis=0)\r\n\r\n\r\ndef second_order(A, ABj, ABk, BAj, B):\r\n    # Second order estimator following Saltelli 2002\r\n    Vjk = np.mean(BAj * ABk - A * B, axis=0) / np.var(np.r_[A, B], axis=0)\r\n    Sj = first_order(A, ABj, B)\r\n    Sk = first_order(A, ABk, B)\r\n\r\n    return Vjk - Sj - Sk\r\n\r\n\r\ndef create_Si_dict(D, calc_second_order):\r\n    # initialize empty dict to store sensitivity indices\r\n    S = ResultDict((k, np.zeros(D))\r\n                   for k in (\'S1\', \'S1_conf\', \'ST\', \'ST_conf\'))\r\n\r\n    if calc_second_order:\r\n        S[\'S2\'] = np.zeros((D, D))\r\n        S[\'S2\'][:] = np.nan\r\n        S[\'S2_conf\'] = np.zeros((D, D))\r\n        S[\'S2_conf\'][:] = np.nan\r\n\r\n    return S\r\n\r\n\r\ndef separate_output_values(Y, D, N, calc_second_order):\r\n    AB = np.zeros((N, D))\r\n    BA = np.zeros((N, D)) if calc_second_order else None\r\n    step = 2 * D + 2 if calc_second_order else D + 2\r\n\r\n    A = Y[0:Y.size:step]\r\n    B = Y[(step - 1):Y.size:step]\r\n    for j in range(D):\r\n        AB[:, j] = Y[(j + 1):Y.size:step]\r\n        if calc_second_order:\r\n            BA[:, j] = Y[(j + 1 + D):Y.size:step]\r\n\r\n    return A, B, AB, BA\r\n\r\n\r\ndef sobol_parallel(Z, A, AB, BA, B, r, tasks):\r\n    sobol_indices = []\r\n    for d, j, k in tasks:\r\n        if d == \'S1\':\r\n            s = first_order(A, AB[:, j], B)\r\n        elif d == \'S1_conf\':\r\n            s = Z * first_order(A[r], AB[r, j], B[r]).std(ddof=1)\r\n        elif d == \'ST\':\r\n            s = total_order(A, AB[:, j], B)\r\n        elif d == \'ST_conf\':\r\n            s = Z * total_order(A[r], AB[r, j], B[r]).std(ddof=1)\r\n        elif d == \'S2\':\r\n            s = second_order(A, AB[:, j], AB[:, k], BA[:, j], B)\r\n        elif d == \'S2_conf\':\r\n            s = Z * second_order(A[r], AB[r, j], AB[r, k],\r\n                                 BA[r, j], B[r]).std(ddof=1)\r\n        sobol_indices.append([d, j, k, s])\r\n\r\n    return sobol_indices\r\n\r\n\r\ndef create_task_list(D, calc_second_order, n_processors):\r\n    # Create list with one entry (key, parameter 1, parameter 2) per sobol\r\n    # index (+conf.). This is used to supply parallel tasks to multiprocessing.Pool\r\n    tasks_first_order = [[d, j, None] for j in range(\r\n        D) for d in (\'S1\', \'S1_conf\', \'ST\', \'ST_conf\')]\r\n\r\n    # Add second order (+conf.) to tasks\r\n    tasks_second_order = []\r\n    if calc_second_order:\r\n        tasks_second_order = [[d, j, k] for j in range(D) for k in\r\n                              range(j + 1, D) for d in (\'S2\', \'S2_conf\')]\r\n\r\n    if n_processors is None:\r\n        n_processors = min(cpu_count(), len(\r\n            tasks_first_order) + len(tasks_second_order))\r\n\r\n    if not calc_second_order:\r\n        tasks = np.array_split(tasks_first_order, n_processors)\r\n    else:\r\n        # merges both lists alternating its elements and splits the resulting list into n_processors sublists\r\n        tasks = np.array_split([v for v in sum(\r\n            zip_longest(tasks_first_order[::-1], tasks_second_order), ())\r\n            if v is not None], n_processors)\r\n\r\n    return tasks, n_processors\r\n\r\n\r\ndef Si_list_to_dict(S_list, D, calc_second_order):\r\n    # Convert the parallel output into the regular dict format for printing/returning\r\n    S = create_Si_dict(D, calc_second_order)\r\n    L = []\r\n    for l in S_list:  # first reformat to flatten\r\n        L += l\r\n\r\n    for s in L:  # First order (+conf.)\r\n        if s[2] is None:\r\n            S[s[0]][s[1]] = s[3]\r\n        else:\r\n            S[s[0]][s[1], s[2]] = s[3]\r\n\r\n    return S\r\n\r\n\r\ndef Si_to_pandas_dict(S_dict):\r\n    """"""Convert Si information into Pandas DataFrame compatible dict.\r\n\r\n    Parameters\r\n    ----------\r\n    S_dict : ResultDict\r\n        Sobol sensitivity indices\r\n\r\n    See Also\r\n    ----------\r\n    Si_list_to_dict\r\n\r\n    Returns\r\n    ----------\r\n    tuple : of total, first, and second order sensitivities.\r\n            Total and first order are dicts.\r\n            Second order sensitivities contain a tuple of parameter name\r\n            combinations for use as the DataFrame index and second order\r\n            sensitivities.\r\n            If no second order indices found, then returns tuple of (None, None)\r\n\r\n    Examples\r\n    --------\r\n    >>> X = saltelli.sample(problem, 1000)\r\n    >>> Y = Ishigami.evaluate(X)\r\n    >>> Si = sobol.analyze(problem, Y, print_to_console=True)\r\n    >>> T_Si, first_Si, (idx, second_Si) = sobol.Si_to_pandas_dict(Si, problem)\r\n    """"""\r\n    problem = S_dict.problem\r\n    total_order = {\r\n        \'ST\': S_dict[\'ST\'],\r\n        \'ST_conf\': S_dict[\'ST_conf\']\r\n    }\r\n    first_order = {\r\n        \'S1\': S_dict[\'S1\'],\r\n        \'S1_conf\': S_dict[\'S1_conf\']\r\n    }\r\n\r\n    idx = None\r\n    second_order = None\r\n    if \'S2\' in S_dict:\r\n        names = problem[\'names\']\r\n        idx = list(combinations(names, 2))\r\n        second_order = {\r\n            \'S2\': [S_dict[\'S2\'][names.index(i[0]), names.index(i[1])]\r\n                   for i in idx],\r\n            \'S2_conf\': [S_dict[\'S2_conf\'][names.index(i[0]), names.index(i[1])]\r\n                        for i in idx]\r\n        }\r\n    return total_order, first_order, (idx, second_order)\r\n\r\n\r\ndef to_df(self):\r\n    \'\'\'Conversion method to Pandas DataFrame. To be attached to ResultDict.\r\n\r\n    Returns\r\n    ========\r\n    List : of Pandas DataFrames in order of Total, First, Second\r\n    \'\'\'\r\n    total, first, (idx, second) = Si_to_pandas_dict(self)\r\n    names = self.problem[\'names\']\r\n    ret = [pd.DataFrame(total, index=names),\r\n           pd.DataFrame(first, index=names)]\r\n\r\n    if second:\r\n        ret += [pd.DataFrame(second, index=idx)]\r\n\r\n    return ret\r\n\r\n\r\ndef print_indices(S, problem, calc_second_order):\r\n    # Output to console\r\n    if not problem.get(\'groups\'):\r\n        title = \'Parameter\'\r\n        names = problem[\'names\']\r\n        D = problem[\'num_vars\']\r\n    else:\r\n        title = \'Group\'\r\n        _, names = compute_groups_matrix(problem[\'groups\'])\r\n        D = len(names)\r\n\r\n    print(\'%s S1 S1_conf ST ST_conf\' % title)\r\n\r\n    for j in range(D):\r\n        print(\'%s %f %f %f %f\' % (names[j], S[\'S1\'][\r\n            j], S[\'S1_conf\'][j], S[\'ST\'][j], S[\'ST_conf\'][j]))\r\n\r\n    if calc_second_order:\r\n        print(\'\\n%s_1 %s_2 S2 S2_conf\' % (title, title))\r\n\r\n        for j in range(D):\r\n            for k in range(j + 1, D):\r\n                print(""%s %s %f %f"" % (names[j], names[k],\r\n                                       S[\'S2\'][j, k], S[\'S2_conf\'][j, k]))\r\n\r\n\r\ndef cli_parse(parser):\r\n    parser.add_argument(\'--max-order\', type=int, required=False, default=2,\r\n                        choices=[1, 2],\r\n                        help=\'Maximum order of sensitivity indices to \'\r\n                        \'calculate\')\r\n    parser.add_argument(\'-r\', \'--resamples\', type=int, required=False,\r\n                        default=1000,\r\n                        help=\'Number of bootstrap resamples for Sobol \'\r\n                        \'confidence intervals\')\r\n    parser.add_argument(\'--parallel\', action=\'store_true\', help=\'Makes \'\r\n                        \'use of parallelization.\',\r\n                        dest=\'parallel\')\r\n    parser.add_argument(\'--processors\', type=int, required=False,\r\n                        default=None,\r\n                        help=\'Number of processors to be used with the \' +\r\n                        \'parallel option.\', dest=\'n_processors\')\r\n    return parser\r\n\r\n\r\ndef cli_action(args):\r\n    problem = read_param_file(args.paramfile)\r\n    Y = np.loadtxt(args.model_output_file, delimiter=args.delimiter,\r\n                   usecols=(args.column,))\r\n\r\n    analyze(problem, Y, (args.max_order == 2),\r\n            num_resamples=args.resamples, print_to_console=True,\r\n            parallel=args.parallel, n_processors=args.n_processors,\r\n            seed=args.seed)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    common_args.run_cli(cli_parse, cli_action)\r\n'"
src/SALib/plotting/__init__.py,0,b''
src/SALib/plotting/bar.py,0,"b""\n__all__ = ['plot']\n\n# magic string indicating DF columns holding conf bound values\nCONF_COLUMN = '_conf'\n\ndef plot(Si_df, ax=None):\n    '''Create bar chart of results\n\n    Parameters\n    ----------\n    * Si_df: pd.DataFrame, of sensitivity results\n\n    Returns\n    ----------\n    * ax : matplotlib axes object\n\n    Examples\n    ----------\n    >>> from SALib.plotting.bar import plot as barplot\n    >>> from SALib.test_functions import Ishigami\n    >>>\n    >>> X = saltelli.sample(problem, 1000)\n    >>> Y = Ishigami.evaluate(X)\n    >>> Si = sobol.analyze(problem, Y, print_to_console=False)\n    >>> Si_df = Si.to_df()\n    >>> barplot(Si_df)\n    '''\n    conf_cols = Si_df.columns.str.contains(CONF_COLUMN)\n\n    confs = Si_df.loc[:, conf_cols]\n    confs.columns = [c.replace(CONF_COLUMN, '') for c in confs.columns]\n\n    Sis = Si_df.loc[:, ~conf_cols]\n\n    ax = Sis.plot(kind='bar', yerr=confs, ax=ax)\n    return ax"""
src/SALib/plotting/morris.py,5,"b'\'\'\'\nCreated on 29 Jun 2015\n\n@author: @willu47\n\nThis module provides the basic infrastructure for plotting charts for the\nMethod of Morris results\n\nThe procedures should build upon and return an axes instance::\n\n    import matplotlib.plot as plt\n    Si = morris.analyze(problem, param_values, Y, conf_level=0.95,\n                        print_to_console=False, num_levels=10)\n    p = morris.horizontal_bar_plot(Si)\n    # set plot style etc.\n\n    fig, ax = plt.subplots(1, 1)\n    my_plotter(ax, data1, data2, {\'marker\':\'x\'})\n\n    p.show()\n\'\'\'\nimport numpy as np\n\n\ndef _sort_Si(Si, key, sortby=\'mu_star\'):\n    return np.array([Si[key][x] for x in np.argsort(Si[sortby])])\n\n\ndef _sort_Si_by_index(Si, key, index):\n    return np.array([Si[key][x] for x in index])\n\n\ndef horizontal_bar_plot(ax, Si, param_dict, sortby=\'mu_star\', unit=\'\'):\n    \'\'\'Updates a matplotlib axes instance with a horizontal bar plot\n\n    of mu_star, with error bars representing mu_star_conf\n    \'\'\'\n\n    assert sortby in [\'mu_star\', \'mu_star_conf\', \'sigma\', \'mu\']\n\n    # Sort all the plotted elements by mu_star (or optionally another\n    # metric)\n    names_sorted = _sort_Si(Si, \'names\', sortby)\n    mu_star_sorted = _sort_Si(Si, \'mu_star\', sortby)\n    mu_star_conf_sorted = _sort_Si(Si, \'mu_star_conf\', sortby)\n\n    # Plot horizontal barchart\n    y_pos = np.arange(len(mu_star_sorted))\n    plot_names = names_sorted\n\n    out = ax.barh(y_pos,\n                  mu_star_sorted,\n                  xerr=mu_star_conf_sorted,\n                  align=\'center\',\n                  ecolor=\'black\',\n                  **param_dict)\n\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(plot_names)\n    ax.set_xlabel(r\'$\\mu^\\star$\' + unit)\n\n    ax.set_ylim(min(y_pos)-1, max(y_pos)+1)\n\n    return out\n\n\ndef covariance_plot(ax, Si, param_dict, unit=""""):\n    \'\'\'Plots mu* against sigma or the 95% confidence interval\n\n    \'\'\'\n\n    if Si[\'sigma\'] is not None:\n        # sigma is not present if using morris groups\n        y = Si[\'sigma\']\n        out = ax.scatter(Si[\'mu_star\'], y, c=u\'k\', marker=u\'o\',\n                         **param_dict)\n        ax.set_ylabel(r\'$\\sigma$\')\n\n        ax.set_xlim(0,)\n        ax.set_ylim(0,)\n\n        x_axis_bounds = np.array(ax.get_xlim())\n\n        line1, = ax.plot(x_axis_bounds, x_axis_bounds, \'k-\')\n        line2, = ax.plot(x_axis_bounds, 0.5 * x_axis_bounds, \'k--\')\n        line3, = ax.plot(x_axis_bounds, 0.1 * x_axis_bounds, \'k-.\')\n\n        ax.legend((line1, line2, line3), (r\'$\\sigma / \\mu^{\\star} = 1.0$\',\n                                          r\'$\\sigma / \\mu^{\\star} = 0.5$\',\n                                          r\'$\\sigma / \\mu^{\\star} = 0.1$\'),\n                  loc=\'best\')\n\n    else:\n        y = Si[\'mu_star_conf\']\n        out = ax.scatter(Si[\'mu_star\'], y, c=u\'k\', marker=u\'o\',\n                         **param_dict)\n        ax.set_ylabel(r\'$95\\% CI$\')\n\n    ax.set_xlabel(r\'$\\mu^\\star$ \' + unit)\n    ax.set_ylim(0-(0.01 * np.array(ax.get_ylim()[1])), )\n\n    return out\n\n\ndef sample_histograms(fig, input_sample, problem, param_dict):\n    \'\'\'Plots a set of subplots of histograms of the input sample\n\n    \'\'\'\n    num_vars = problem[\'num_vars\']\n    names = problem[\'names\']\n\n    framing = 101 + (num_vars * 10)\n\n    # Find number of levels\n    num_levels = len(set(input_sample[:, 1]))\n\n    out = []\n\n    for variable in range(num_vars):\n        ax = fig.add_subplot(framing + variable)\n        out.append(ax.hist(input_sample[:, variable],\n                           bins=num_levels,\n                           density=False,\n                           label=None,\n                           **param_dict))\n\n        ax.set_title(\'%s\' % (names[variable]))\n        ax.tick_params(axis=\'x\',  # changes apply to the x-axis\n                       which=\'both\',  # both major and minor ticks are affected\n                       bottom=\'off\',  # ticks along the bottom edge are off\n                       top=\'off\',  # ticks along the top edge are off\n                       labelbottom=\'off\')  # labels along the bottom edge off)\n        if variable > 0:\n            ax.tick_params(axis=\'y\',  # changes apply to the y-axis\n                           which=\'both\',  # both major and minor ticks affected\n                           labelleft=\'off\')  # labels along the left edge off)\n\n    return out\n\n\nif __name__ == \'__main__\':\n    pass\n'"
src/SALib/sample/__init__.py,0,b''
src/SALib/sample/common_args.py,0,"b'import argparse\n\n\ndef setup(parser):\n    """"""Add common sampling options to CLI parser.\n\n    Parameters\n    ----------\n    parser : argparse object\n\n    Returns\n    ----------\n    Updated argparse object\n    """"""\n    parser.add_argument(\'-n\', \'--samples\', type=int, required=True,\n                        help=\'Number of Samples\')\n    parser.add_argument(\'-p\', \'--paramfile\', type=str, required=True,\n                        help=\'Parameter Range File\')\n    parser.add_argument(\'-o\', \'--output\', type=str, required=True, \n                        help=\'Output File\')\n    parser.add_argument(\'-s\', \'--seed\', type=int, required=False, \n                        default=None,\n                        help=\'Random Seed\')\n    parser.add_argument(\'--delimiter\', type=str, required=False,\n                        default=\' \',\n                        help=\'Column delimiter\')\n    parser.add_argument(\'--precision\', type=int, required=False,\n                        default=8,\n                        help=\'Output floating-point precision\')\n\n    return parser\n\n\ndef create(cli_parser=None):\n    """"""Create CLI parser object.\n\n    Parameters\n    ----------\n    cli_parser : function [optional]\n        Function to add method specific arguments to parser\n\n    Returns\n    ----------\n    argparse object\n    """"""\n    parser = argparse.ArgumentParser(\n        description=\'Create parameter samples for sensitivity analysis\')\n    parser = setup(parser)\n\n    if cli_parser:\n        parser = cli_parser(parser)\n\n    return parser\n\n\ndef run_cli(cli_parser, run_sample, known_args=None):\n    """"""Run sampling with CLI arguments.\n\n    Parameters\n    ----------\n    cli_parser : function\n        Function to add method specific arguments to parser\n    run_sample: function\n        Method specific function that runs the sampling\n    known_args: list [optional]\n        Additional arguments to parse\n\n    Returns\n    ----------\n    argparse object\n    """"""\n    parser = create(cli_parser)\n    args = parser.parse_args(known_args)\n\n    run_sample(args)\n'"
src/SALib/sample/fast_sampler.py,10,"b'from __future__ import division\r\n\r\nimport math\r\n\r\nimport numpy as np\r\n\r\nfrom . import common_args\r\nfrom .. util import scale_samples, read_param_file\r\n\r\n\r\ndef sample(problem, N, M=4, seed=None):\r\n    """"""Generate model inputs for the Fourier Amplitude Sensitivity Test (FAST).\r\n\r\n    Returns a NumPy matrix containing the model inputs required by the Fourier\r\n    Amplitude sensitivity test.  The resulting matrix contains N * D rows and D\r\n    columns, where D is the number of parameters.  The samples generated are\r\n    intended to be used by :func:`SALib.analyze.fast.analyze`.\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        The problem definition\r\n    N : int\r\n        The number of samples to generate\r\n    M : int\r\n        The interference parameter, i.e., the number of harmonics to sum in the\r\n        Fourier series decomposition (default 4)\r\n    """"""\r\n    if seed:\r\n        np.random.seed(seed)\r\n\r\n    if N <= 4 * M**2:\r\n        raise ValueError(""""""\r\n        Sample size N > 4M^2 is required. M=4 by default."""""")\r\n\r\n    D = problem[\'num_vars\']\r\n\r\n    omega = np.zeros([D])\r\n    omega[0] = math.floor((N - 1) / (2 * M))\r\n    m = math.floor(omega[0] / (2 * M))\r\n\r\n    if m >= (D - 1):\r\n        omega[1:] = np.floor(np.linspace(1, m, D - 1))\r\n    else:\r\n        omega[1:] = np.arange(D - 1) % m + 1\r\n\r\n    # Discretization of the frequency space, s\r\n    s = (2 * math.pi / N) * np.arange(N)\r\n\r\n    # Transformation to get points in the X space\r\n    X = np.zeros([N * D, D])\r\n    omega2 = np.zeros([D])\r\n\r\n    for i in range(D):\r\n        omega2[i] = omega[0]\r\n        idx = list(range(i)) + list(range(i + 1, D))\r\n        omega2[idx] = omega[1:]\r\n        l = range(i * N, (i + 1) * N)\r\n\r\n        # random phase shift on [0, 2pi) following Saltelli et al.\r\n        # Technometrics 1999\r\n        phi = 2 * math.pi * np.random.rand()\r\n\r\n        for j in range(D):\r\n            g = 0.5 + (1 / math.pi) * np.arcsin(np.sin(omega2[j] * s + phi))\r\n            X[l, j] = g\r\n\r\n    scale_samples(X, problem[\'bounds\'])\r\n    return X\r\n\r\n\r\ndef cli_parse(parser):\r\n    """"""Add method specific options to CLI parser.\r\n\r\n    Parameters\r\n    ----------\r\n    parser : argparse object\r\n\r\n    Returns\r\n    ----------\r\n    Updated argparse object\r\n    """"""\r\n    parser.add_argument(\'-M\', \'--m-coef\', type=int, required=False, default=4,\r\n                        help=\'M coefficient, default 4\', dest=\'M\')\r\n    return parser\r\n\r\n\r\ndef cli_action(args):\r\n    """"""Run sampling method\r\n\r\n    Parameters\r\n    ----------\r\n    args : argparse namespace\r\n    """"""\r\n    problem = read_param_file(args.paramfile)\r\n    param_values = sample(problem, N=args.samples, M=args.M, seed=args.seed)\r\n    np.savetxt(args.output, param_values, delimiter=args.delimiter,\r\n               fmt=\'%.\' + str(args.precision) + \'e\')\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    common_args.run_cli(cli_parse, cli_action)\r\n'"
src/SALib/sample/ff.py,4,"b'""""""The sampling implementation of fractional factorial method\n\nThis implementation is based on the formulation put forward in\n[`Saltelli et al. 2008 <http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470059974.html>`_]\n\n""""""\n\nfrom scipy.linalg import hadamard\nimport numpy as np\nfrom . import common_args\nfrom ..util import scale_samples, read_param_file\n\n\ndef find_smallest(num_vars):\n    """"""Find the smallest exponent of two that is greater than the number\n    of variables\n\n    Arguments\n    =========\n    num_vars : int\n        Number of variables\n\n    Returns\n    =======\n    x : int\n        Smallest exponent of two greater than `num_vars`\n    """"""\n    for x in range(10):\n        if num_vars <= 2 ** x:\n            return x\n\n\ndef extend_bounds(problem):\n    """"""Extends the problem bounds to the nearest power of two\n\n    Arguments\n    =========\n    problem : dict\n        The problem definition\n    """"""\n\n    num_vars = problem[\'num_vars\']\n    num_ff_vars = 2 ** find_smallest(num_vars)\n    num_dummy_variables = num_ff_vars - num_vars\n\n    bounds = list(problem[\'bounds\'])\n    names = problem[\'names\']\n    if num_dummy_variables > 0:\n        bounds.extend([[0, 1] for x in range(num_dummy_variables)])\n        names.extend([""dummy_"" + str(var)\n                      for var in range(num_dummy_variables)])\n        problem[\'bounds\'] = bounds\n        problem[\'names\'] = names\n        problem[\'num_vars\'] = num_ff_vars\n\n    return problem\n\n\ndef generate_contrast(problem):\n    """"""Generates the raw sample from the problem file\n\n    Arguments\n    =========\n    problem : dict\n        The problem definition\n    """"""\n\n    num_vars = problem[\'num_vars\']\n\n    # Find the smallest n, such that num_vars < k\n    k = [2 ** n for n in range(16)]\n    k_chosen = 2 ** find_smallest(num_vars)\n\n    # Generate the fractional factorial contrast\n    contrast = np.vstack([hadamard(k_chosen), -hadamard(k_chosen)])\n\n    return contrast\n\n\ndef sample(problem, seed=None):\n    """"""Generates model inputs using a fractional factorial sample\n\n    Returns a NumPy matrix containing the model inputs required for a\n    fractional factorial analysis.\n    The resulting matrix has D columns, where D is smallest power of 2 that is\n    greater than the number of parameters.\n    These model inputs are intended to be used with\n    :func:`SALib.analyze.ff.analyze`.\n\n    The problem file is padded with a number of dummy variables called\n    ``dummy_0`` required for this procedure. These dummy variables can be used\n    as a check for errors in the analyze procedure.\n\n    This algorithm is an implementation of that contained in\n    [`Saltelli et al. 2008 <http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470059974.html>`_]\n\n    Arguments\n    =========\n    problem : dict\n        The problem definition\n\n    Returns\n    =======\n    sample : :class:`numpy.array`\n\n    """"""\n    if seed:\n        np.random.seed(seed)\n    contrast = generate_contrast(problem)\n    sample = np.array((contrast + 1.) / 2, dtype=np.float)\n    problem = extend_bounds(problem)\n    scale_samples(sample, problem[\'bounds\'])\n    return sample\n\n\n# No additional CLI options\ncli_parse = None\n\n\ndef cli_action(args):\n    """"""Run sampling method\n\n    Parameters\n    ----------\n    args : argparse namespace\n    """"""\n    problem = read_param_file(args.paramfile)\n    param_values = sample(problem, seed=args.seed)\n    np.savetxt(args.output, param_values, delimiter=args.delimiter,\n               fmt=\'%.\' + str(args.precision) + \'e\')\n\n\nif __name__ == ""__main__"":\n    common_args.run_cli(cli_parse, cli_action)\n'"
src/SALib/sample/finite_diff.py,5,"b'from __future__ import division\r\n\r\nimport numpy as np\r\n\r\nfrom . import common_args\r\nfrom . import sobol_sequence\r\nfrom ..util import scale_samples, read_param_file\r\n\r\n\r\ndef sample(problem, N, delta=0.01, seed=None):\r\n    """"""Generate matrix of samples for derivative-based global sensitivity measure (dgsm).\r\n    Start from a QMC (sobol) sequence and finite difference with delta % steps\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        SALib problem specification\r\n\r\n    N : int\r\n        number of samples\r\n\r\n    delta : float\r\n        Finite difference step size (percent)\r\n    \r\n    seed : int or None\r\n        random seed value\r\n\r\n    Returns\r\n    ----------\r\n    np.array : DGSM sequence\r\n    """"""\r\n    if seed:\r\n        np.random.seed(seed)\r\n\r\n    D = problem[\'num_vars\']\r\n    bounds = problem[\'bounds\']\r\n\r\n    # How many values of the Sobol sequence to skip\r\n    skip_values = 1000\r\n\r\n    # Create base sequence - could be any type of sampling\r\n    base_sequence = sobol_sequence.sample(N + skip_values, D)\r\n    # scale before finite differencing\r\n    scale_samples(base_sequence, bounds)\r\n    dgsm_sequence = np.empty([N * (D + 1), D])\r\n\r\n    index = 0\r\n    for i in range(skip_values, N + skip_values):\r\n        # Copy the initial point\r\n        dgsm_sequence[index, :] = base_sequence[i, :]\r\n\r\n        index += 1\r\n        for j in range(D):\r\n            temp = np.zeros(D)\r\n            bnd_j = bounds[j]\r\n            temp[j] = base_sequence[i, j] * delta\r\n            dgsm_sequence[index, :] = base_sequence[i, :] + temp\r\n            dgsm_sequence[index, j] = min(\r\n                dgsm_sequence[index, j], bnd_j[1])\r\n            dgsm_sequence[index, j] = max(\r\n                dgsm_sequence[index, j], bnd_j[0])\r\n            index += 1\r\n\r\n    return dgsm_sequence\r\n\r\n\r\ndef cli_parse(parser):\r\n    """"""Add method specific options to CLI parser.\r\n\r\n    Parameters\r\n    ----------\r\n    parser : argparse object\r\n\r\n    Returns\r\n    ----------\r\n    Updated argparse object\r\n    """"""\r\n    parser.add_argument(\'-d\', \'--delta\', type=float,\r\n                        required=False, default=0.01,\r\n                        help=\'Finite difference step size (percent)\')\r\n    return parser\r\n\r\n\r\ndef cli_action(args):\r\n    """"""Run sampling method\r\n\r\n    Parameters\r\n    ----------\r\n    args : argparse namespace\r\n    """"""\r\n    problem = read_param_file(args.paramfile)\r\n    param_values = sample(problem, args.samples, args.delta, seed=args.seed)\r\n    np.savetxt(args.output, param_values, delimiter=args.delimiter,\r\n               fmt=\'%.\' + str(args.precision) + \'e\')\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    common_args.run_cli(cli_parse, cli_action)\r\n'"
src/SALib/sample/latin.py,6,"b'from __future__ import division\r\n\r\nimport numpy as np\r\n\r\nfrom . import common_args\r\nfrom ..util import scale_samples, read_param_file\r\n\r\n\r\ndef sample(problem, N, seed=None):\r\n    """"""Generate model inputs using Latin hypercube sampling (LHS).\r\n\r\n    Returns a NumPy matrix containing the model inputs generated by Latin\r\n    hypercube sampling.  The resulting matrix contains N rows and D columns,\r\n    where D is the number of parameters.\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        The problem definition\r\n    N : int\r\n        The number of samples to generate\r\n    """"""\r\n    if seed:\r\n        np.random.seed(seed)\r\n    D = problem[\'num_vars\']\r\n\r\n    result = np.empty([N, D])\r\n    temp = np.empty([N])\r\n    d = 1.0 / N\r\n\r\n    for i in range(D):\r\n        for j in range(N):\r\n            temp[j] = np.random.uniform(low=j * d, \r\n                                        high=(j + 1) * d)\r\n\r\n        np.random.shuffle(temp)\r\n\r\n        for j in range(N):\r\n            result[j, i] = temp[j]\r\n\r\n    scale_samples(result, problem[\'bounds\'])\r\n    return result\r\n\r\n\r\n# No additional CLI options\r\ncli_parse = None\r\n\r\n\r\ndef cli_action(args):\r\n    """"""Run sampling method\r\n\r\n    Parameters\r\n    ----------\r\n    args : argparse namespace\r\n    """"""\r\n    problem = read_param_file(args.paramfile)\r\n    param_values = sample(problem, args.samples, seed=args.seed)\r\n    np.savetxt(args.output, param_values, delimiter=args.delimiter,\r\n               fmt=\'%.\' + str(args.precision) + \'e\')\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    cli_parse = None  # No additional options\r\n    common_args.run_cli(cli_parse, cli_action)\r\n'"
src/SALib/sample/saltelli.py,4,"b'from __future__ import division\r\n\r\nimport numpy as np\r\n\r\nfrom . import common_args\r\nfrom . import sobol_sequence\r\nfrom ..util import scale_samples, nonuniform_scale_samples, read_param_file, compute_groups_matrix\r\n\r\n\r\ndef sample(problem, N, calc_second_order=True, seed=None):\r\n    """"""Generates model inputs using Saltelli\'s extension of the Sobol sequence.\r\n\r\n    Returns a NumPy matrix containing the model inputs using Saltelli\'s sampling\r\n    scheme.  Saltelli\'s scheme extends the Sobol sequence in a way to reduce\r\n    the error rates in the resulting sensitivity index calculations.  If\r\n    calc_second_order is False, the resulting matrix has N * (D + 2)\r\n    rows, where D is the number of parameters.  If calc_second_order is True,\r\n    the resulting matrix has N * (2D + 2) rows.  These model inputs are\r\n    intended to be used with :func:`SALib.analyze.sobol.analyze`.\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        The problem definition\r\n    N : int\r\n        The number of samples to generate\r\n    calc_second_order : bool\r\n        Calculate second-order sensitivities (default True)\r\n    """"""\r\n    if seed:\r\n        np.random.seed(seed)\r\n\r\n    D = problem[\'num_vars\']\r\n    groups = problem.get(\'groups\')\r\n\r\n    if not groups:\r\n        Dg = problem[\'num_vars\']\r\n    else:\r\n        Dg = len(set(groups))\r\n        G, group_names = compute_groups_matrix(groups)\r\n\r\n    # How many values of the Sobol sequence to skip\r\n    skip_values = 1000\r\n\r\n    # Create base sequence - could be any type of sampling\r\n    base_sequence = sobol_sequence.sample(N + skip_values, 2 * D)\r\n\r\n    if calc_second_order:\r\n        saltelli_sequence = np.zeros([(2 * Dg + 2) * N, D])\r\n    else:\r\n        saltelli_sequence = np.zeros([(Dg + 2) * N, D])\r\n    index = 0\r\n\r\n    for i in range(skip_values, N + skip_values):\r\n\r\n        # Copy matrix ""A""\r\n        for j in range(D):\r\n            saltelli_sequence[index, j] = base_sequence[i, j]\r\n\r\n        index += 1\r\n\r\n        # Cross-sample elements of ""B"" into ""A""\r\n        for k in range(Dg):\r\n            for j in range(D):\r\n                if (not groups and j == k) or (groups and group_names[k] == groups[j]):\r\n                    saltelli_sequence[index, j] = base_sequence[i, j + D]\r\n                else:\r\n                    saltelli_sequence[index, j] = base_sequence[i, j]\r\n\r\n            index += 1\r\n\r\n        # Cross-sample elements of ""A"" into ""B""\r\n        # Only needed if you\'re doing second-order indices (true by default)\r\n        if calc_second_order:\r\n            for k in range(Dg):\r\n                for j in range(D):\r\n                    if (not groups and j == k) or (groups and group_names[k] == groups[j]):\r\n                        saltelli_sequence[index, j] = base_sequence[i, j]\r\n                    else:\r\n                        saltelli_sequence[index, j] = base_sequence[i, j + D]\r\n\r\n                index += 1\r\n\r\n        # Copy matrix ""B""\r\n        for j in range(D):\r\n            saltelli_sequence[index, j] = base_sequence[i, j + D]\r\n\r\n        index += 1\r\n    if not problem.get(\'dists\'):\r\n        # scaling values out of 0-1 range with uniform distributions\r\n        scale_samples(saltelli_sequence, problem[\'bounds\'])\r\n        return saltelli_sequence\r\n    else:\r\n        # scaling values to other distributions based on inverse CDFs\r\n        scaled_saltelli = nonuniform_scale_samples(\r\n            saltelli_sequence, problem[\'bounds\'], problem[\'dists\'])\r\n        return scaled_saltelli\r\n\r\n\r\ndef cli_parse(parser):\r\n    """"""Add method specific options to CLI parser.\r\n\r\n    Parameters\r\n    ----------\r\n    parser : argparse object\r\n\r\n    Returns\r\n    ----------\r\n    Updated argparse object\r\n    """"""\r\n    parser.add_argument(\'--max-order\', type=int, required=False, default=2,\r\n                        choices=[1, 2],\r\n                        help=\'Maximum order of sensitivity indices \\\r\n                           to calculate\')\r\n    return parser\r\n\r\n\r\ndef cli_action(args):\r\n    """"""Run sampling method\r\n\r\n    Parameters\r\n    ----------\r\n    args : argparse namespace\r\n    """"""\r\n    problem = read_param_file(args.paramfile)\r\n    param_values = sample(problem, args.samples,\r\n                          calc_second_order=(args.max_order == 2),\r\n                          seed=args.seed)\r\n    np.savetxt(args.output, param_values, delimiter=args.delimiter,\r\n               fmt=\'%.\' + str(args.precision) + \'e\')\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    common_args.run_cli(cli_parse, cli_action)\r\n'"
src/SALib/sample/sobol_sequence.py,3,"b'from __future__ import division\r\n\r\nimport math\r\nimport sys\r\n\r\nimport numpy as np\r\n\r\nfrom .directions import directions\r\n\r\n\r\nif sys.version_info[0] > 2:\r\n    long = int\r\n\r\n#==============================================================================\r\n# The following code is based on the Sobol sequence generator by Frances\r\n# Y. Kuo and Stephen Joe. The license terms are provided below.\r\n#\r\n# Copyright (c) 2008, Frances Y. Kuo and Stephen Joe\r\n# All rights reserved.\r\n#\r\n# Redistribution and use in source and binary forms, with or without\r\n# modification, are permitted provided that the following conditions are\r\n# met:\r\n#\r\n# Redistributions of source code must retain the above copyright\r\n# notice, this list of conditions and the following disclaimer.\r\n#\r\n# Redistributions in binary form must reproduce the above copyright\r\n# notice, this list of conditions and the following disclaimer in the\r\n# documentation and/or other materials provided with the distribution.\r\n#\r\n# Neither the names of the copyright holders nor the names of the\r\n# University of New South Wales and the University of Waikato\r\n# and its contributors may be used to endorse or promote products derived\r\n# from this software without specific prior written permission.\r\n#\r\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS\'\' AND ANY\r\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\r\n# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\r\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE FOR ANY\r\n# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\r\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\r\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\r\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,\r\n# STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\r\n# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\r\n# POSSIBILITY OF SUCH DAMAGE.\r\n#==============================================================================\r\n\r\n\r\ndef sample(N, D):\r\n    """"""Generate (N x D) numpy array of Sobol sequence samples""""""\r\n    scale = 31\r\n    result = np.zeros([N, D])\r\n    \r\n    if D > len(directions) + 1:\r\n        raise ValueError(""Error in Sobol sequence: not enough dimensions"")\r\n\r\n    L = int(math.ceil(math.log(N) / math.log(2)))\r\n\r\n    if L > scale:\r\n        raise ValueError(""Error in Sobol sequence: not enough bits"")\r\n\r\n    for i in range(D):\r\n        V = np.zeros(L + 1, dtype=long)\r\n\r\n        if i == 0:\r\n            for j in range(1, L + 1):\r\n                V[j] = 1 << (scale - j)  # all m\'s = 1\r\n        else:\r\n            m = np.array(directions[i - 1], dtype=int)\r\n            a = m[0]\r\n            s = len(m) - 1\r\n\r\n            # The following code discards the first row of the ``m`` array\r\n            # Because it has floating point errors, e.g. values of 2.24e-314\r\n            if L <= s:\r\n                for j in range(1, L + 1):\r\n                    V[j] = m[j] << (scale - j)\r\n            else:\r\n                for j in range(1, s + 1):\r\n                    V[j] = m[j] << (scale - j)\r\n                for j in range(s + 1, L + 1):\r\n                    V[j] = V[j - s] ^ (V[j - s] >> s)\r\n                    for k in range(1, s):\r\n                        V[j] ^= ((a >> (s - 1 - k)) & 1) * V[j - k]\r\n\r\n        X = long(0)\r\n        for j in range(1, N):\r\n            X ^= V[index_of_least_significant_zero_bit(j - 1)]\r\n            result[j][i] = float(X / math.pow(2, scale))\r\n\r\n    return result\r\n\r\n\r\ndef index_of_least_significant_zero_bit(value):\r\n    index = 1\r\n    while((value & 1) != 0):\r\n        value >>= 1\r\n        index += 1\r\n\r\n    return index\r\n'"
src/SALib/scripts/__init__.py,0,b''
src/SALib/scripts/salib.py,0,"b""'''Command-line utility for SALib'''\nimport importlib\nimport argparse\nfrom SALib import (analyze, sample)\nfrom SALib.util import avail_approaches\n\n\ndef parse_subargs(module, parser, method, opts):\n    '''Attach argument parser for action specific options.\n\n    Arguments\n    ---------\n    module : module\n        name of module to extract action from\n    parser : argparser\n        argparser object to attach additional arguments to\n    method : str\n        name of method (morris, sobol, etc).\n        Must match one of the available submodules\n    opts : list\n        A list of argument options to parse\n\n    Returns\n    ---------\n    subargs : argparser namespace object\n    '''\n    module.cli_args(parser)\n    subargs = parser.parse_args(opts)\n    return subargs\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='SALib - Sensitivity Analysis Library')\n    subparsers = parser.add_subparsers(help='Sample or Analysis method',\n                                       dest='action')\n    sample_parser = subparsers.add_parser(\n        'sample', description='Select one of the sample methods')\n    analyze_parser = subparsers.add_parser('analyze')\n\n    # Get list of available samplers and analyzers\n    samplers = avail_approaches(sample)\n    analyzers = avail_approaches(analyze)\n    sample_parser.add_argument('method', choices=samplers)\n    analyze_parser.add_argument('method', choices=analyzers)\n\n    known_args, opts = parser.parse_known_args()\n\n    action = known_args.action\n    if not action:\n        parser.print_help()\n        exit()\n\n    method_name = known_args.method\n    common_args = importlib.import_module(\n        '.'.join(['SALib', action, 'common_args']))\n    module = importlib.import_module('.'.join(['SALib', action, method_name]))\n\n    if len(opts) == 0:\n        cmd_parse = common_args.create(module.cli_parse)\n        cmd_parse.print_help()\n        exit()\n\n    common_args.run_cli(module.cli_parse, module.cli_action, opts)\n\n\nif __name__ == '__main__':\n    main()\n"""
src/SALib/test_functions/Ishigami.py,1,"b'from __future__ import division\r\n\r\nimport math\r\n\r\nimport numpy as np\r\n\r\n\r\n# Non-monotonic Ishigami Function (3 parameters)\r\n# Using Saltelli sampling with a sample size of ~1000\r\n# the expected first-order indices would be:\r\n# x1: 0.3139\r\n# x2: 0.4424\r\n# x3: 0.0\r\ndef evaluate(values):\r\n    Y = np.zeros([values.shape[0]])\r\n    A = 7\r\n    B = 0.1\r\n\r\n    for i, X in enumerate(values):\r\n        Y[i] = math.sin(X[0]) + A * math.pow(math.sin(X[1]), 2) + \\\r\n            B * math.pow(X[2], 4) * math.sin(X[0])\r\n\r\n    return Y\r\n'"
src/SALib/test_functions/Sobol_G.py,20,"b'from __future__ import division\r\n\r\nimport numpy as np\r\n\r\n\r\n# Non-monotonic Sobol G Function (8 parameters)\r\n# First-order indices:\r\n# x1: 0.7165\r\n# x2: 0.1791\r\n# x3: 0.0237\r\n# x4: 0.0072\r\n# x5-x8: 0.0001\r\ndef evaluate(values, a=None, delta=None, alpha=None):\r\n    """"""Modified Sobol G-function.\r\n\r\n    Reverts to original Sobol G-function if delta and alpha are not given.\r\n\r\n    .. [1] Saltelli, A., Annoni, P., Azzini, I., Campolongo, F., Ratto, M., \r\n           Tarantola, S., 2010. Variance based sensitivity analysis of model \r\n           output. Design and estimator for the total sensitivity index. \r\n           Computer Physics Communications 181, 259\xe2\x80\x93270. \r\n           https://doi.org/10.1016/j.cpc.2009.09.018\r\n\r\n    Parameters\r\n    ----------\r\n    values : numpy.ndarray\r\n        input variables\r\n    a : numpy.ndarray\r\n        parameter values\r\n    delta : numpy.ndarray\r\n        shift parameters\r\n    alpha : numpy.ndarray\r\n        curvature parameters\r\n\r\n    Returns\r\n    -------\r\n    Y : Result of G-function\r\n    """"""\r\n    if type(values) != np.ndarray:\r\n        raise TypeError(""The argument `values` must be a numpy ndarray"")\r\n  \r\n    if a is None:\r\n        a = np.array([0, 1, 4.5, 9, 99, 99, 99, 99])\r\n\r\n    if delta is None:\r\n        delta = np.zeros_like(a)\r\n    else:\r\n        if not isinstance(delta, np.ndarray):\r\n            raise TypeError(""The argument `delta` must be given as a numpy ndarray"")\r\n\r\n        delta_inbetween = delta[(delta < 0) | (delta > 1)]\r\n        if delta_inbetween.any():\r\n            raise ValueError(""Sobol G function called with delta values less than zero or greater than one"")\r\n\r\n    if alpha is None:\r\n        alpha = np.ones_like(a)\r\n    else:\r\n        if not isinstance(alpha, np.ndarray):\r\n            raise TypeError(""The argument `alpha` must be given as a numpy ndarray"")\r\n\r\n        alpha_gto = alpha <= 0.0\r\n        if alpha_gto.any():\r\n            raise ValueError(""Sobol G function called with alpha values less than or equal to zero"")\r\n\r\n    ltz = values < 0\r\n    gto = values > 1\r\n\r\n    if ltz.any() == True:\r\n        raise ValueError(""Sobol G function called with values less than zero"")\r\n    elif gto.any() == True:\r\n        raise ValueError(""Sobol G function called with values greater than one"")\r\n\r\n    Y = np.ones([values.shape[0]])\r\n\r\n    for i, row in enumerate(values):\r\n        shift_of_x = row + delta\r\n        integral = np.modf(shift_of_x)[1]\r\n        mod_x = shift_of_x - integral\r\n        temp_y = (np.abs(2 * mod_x - 1)**alpha)\r\n        y_elements = ((1 + alpha) * temp_y + a) / (1 + a)\r\n        Y[i] = np.prod(y_elements)\r\n\r\n    return Y\r\n\r\n\r\n\r\ndef _partial_first_order_variance(a=None, alpha=None):\r\n    if a is None:\r\n        a = [0, 1, 4.5, 9, 99, 99, 99, 99]\r\n    if alpha is None:\r\n        alpha = np.ones_like(a)\r\n    a = np.array(a)\r\n    \r\n    return np.divide((alpha**2), np.multiply((1 + 2 * alpha), np.square(1 + a)))\r\n\r\n\r\ndef _total_variance(a=None, alpha=None):\r\n    if a is None:\r\n        a = [0, 1, 4.5, 9, 99, 99, 99, 99]\r\n    if alpha is None:\r\n        alpha = np.ones_like(a)\r\n\r\n    a = np.array(a)\r\n    return np.add(-1, np.product(1 + _partial_first_order_variance(a, alpha), axis=0))\r\n\r\n\r\ndef sensitivity_index(a, alpha=None):\r\n    a = np.array(a)\r\n    return np.divide(_partial_first_order_variance(a, alpha), _total_variance(a, alpha))\r\n\r\n\r\ndef total_sensitivity_index(a, alpha=None):\r\n    a = np.array(a)\r\n    \r\n    pv = _partial_first_order_variance(a, alpha)\r\n    tv = _total_variance(a, alpha)\r\n    \r\n    sum_pv = pv.sum(axis=0)\r\n    \r\n    return np.subtract(1, np.divide(np.subtract(sum_pv, pv.T), tv))\r\n'"
src/SALib/test_functions/__init__.py,0,"b'__all__ = [""Sobol_G""]\n'"
src/SALib/util/__init__.py,11,"b'""""""A set of utility functions\n\n""""""\nfrom collections import OrderedDict\nimport csv\nfrom warnings import warn\nfrom .results import ResultDict\nimport pkgutil\n\nimport numpy as np\nimport scipy as sp\n\n\n__all__ = [""scale_samples"", ""read_param_file"",\n           ""ResultDict"", ""avail_approaches""]\n\n\ndef avail_approaches(pkg):\n    \'\'\'Create list of available modules.\n\n    Arguments\n    ---------\n    pkg : module\n        module to inspect\n\n    Returns\n    ---------\n    method : list\n        A list of available submodules\n    \'\'\'\n    methods = [modname for importer, modname, ispkg in\n               pkgutil.walk_packages(path=pkg.__path__)\n               if modname not in\n               [\'common_args\', \'directions\', \'sobol_sequence\']]\n    return methods\n\n\ndef scale_samples(params, bounds):\n    \'\'\'Rescale samples in 0-to-1 range to arbitrary bounds\n\n    Arguments\n    ---------\n    params : numpy.ndarray\n        numpy array of dimensions `num_params`-by-:math:`N`,\n        where :math:`N` is the number of samples\n\n    bounds : list\n        list of lists of dimensions `num_params`-by-2\n    \'\'\'\n    # Check bounds are legal (upper bound is greater than lower bound)\n    b = np.array(bounds)\n    lower_bounds = b[:, 0]\n    upper_bounds = b[:, 1]\n\n    if np.any(lower_bounds >= upper_bounds):\n        raise ValueError(""Bounds are not legal"")\n\n    # This scales the samples in-place, by using the optional output\n    # argument for the numpy ufunctions\n    # The calculation is equivalent to:\n    #   sample * (upper_bound - lower_bound) + lower_bound\n    np.add(np.multiply(params,\n                       (upper_bounds - lower_bounds),\n                       out=params),\n           lower_bounds,\n           out=params)\n\n\ndef unscale_samples(params, bounds):\n    """"""Rescale samples from arbitrary bounds back to [0,1] range\n\n    Arguments\n    ---------\n    bounds : list\n        list of lists of dimensions num_params-by-2\n    params : numpy.ndarray\n        numpy array of dimensions num_params-by-N,\n        where N is the number of samples\n    """"""\n    # Check bounds are legal (upper bound is greater than lower bound)\n    b = np.array(bounds)\n    lower_bounds = b[:, 0]\n    upper_bounds = b[:, 1]\n\n    if np.any(lower_bounds >= upper_bounds):\n        raise ValueError(""Bounds are not legal"")\n\n    # This scales the samples in-place, by using the optional output\n    # argument for the numpy ufunctions\n    # The calculation is equivalent to:\n    #   (sample - lower_bound) / (upper_bound - lower_bound)\n    np.divide(np.subtract(params, lower_bounds, out=params),\n              np.subtract(upper_bounds, lower_bounds),\n              out=params)\n\n\ndef nonuniform_scale_samples(params, bounds, dists):\n    """"""Rescale samples in 0-to-1 range to other distributions\n\n    Arguments\n    ---------\n    problem : dict\n        problem definition including bounds\n    params : numpy.ndarray\n        numpy array of dimensions num_params-by-N,\n        where N is the number of samples\n    dists : list\n        list of distributions, one for each parameter\n            unif: uniform with lower and upper bounds\n            triang: triangular with width (scale) and location of peak\n                    location of peak is in percentage of width\n                    lower bound assumed to be zero\n            norm: normal distribution with mean and standard deviation\n            lognorm: lognormal with ln-space mean and standard deviation\n    """"""\n    b = np.array(bounds)\n\n    # initializing matrix for converted values\n    conv_params = np.empty_like(params)\n\n    # loop over the parameters\n    for i in range(conv_params.shape[1]):\n        # setting first and second arguments for distributions\n        b1 = b[i][0]\n        b2 = b[i][1]\n\n        if dists[i] == \'triang\':\n            # checking for correct parameters\n            if b1 <= 0 or b2 <= 0 or b2 >= 1:\n                raise ValueError(\'\'\'Triangular distribution: Scale must be\n                    greater than zero; peak on interval [0,1]\'\'\')\n            else:\n                conv_params[:, i] = sp.stats.triang.ppf(\n                    params[:, i], c=b2, scale=b1, loc=0)\n\n        elif dists[i] == \'unif\':\n            if b1 >= b2:\n                raise ValueError(\'\'\'Uniform distribution: lower bound\n                    must be less than upper bound\'\'\')\n            else:\n                conv_params[:, i] = params[:, i] * (b2 - b1) + b1\n\n        elif dists[i] == \'norm\':\n            if b2 <= 0:\n                raise ValueError(\'\'\'Normal distribution: stdev must be > 0\'\'\')\n            else:\n                conv_params[:, i] = sp.stats.norm.ppf(\n                    params[:, i], loc=b1, scale=b2)\n\n        # lognormal distribution (ln-space, not base-10)\n        # paramters are ln-space mean and standard deviation\n        elif dists[i] == \'lognorm\':\n            # checking for valid parameters\n            if b2 <= 0:\n                raise ValueError(\n                    \'\'\'Lognormal distribution: stdev must be > 0\'\'\')\n            else:\n                conv_params[:, i] = np.exp(\n                    sp.stats.norm.ppf(params[:, i], loc=b1, scale=b2))\n\n        else:\n            valid_dists = [\'unif\', \'triang\', \'norm\', \'lognorm\']\n            raise ValueError(\'Distributions: choose one of %s\' %\n                             "", "".join(valid_dists))\n\n    return conv_params\n\n\ndef read_param_file(filename, delimiter=None):\n    """"""Unpacks a parameter file into a dictionary\n\n    Reads a parameter file of format::\n\n        Param1,0,1,Group1,dist1\n        Param2,0,1,Group2,dist2\n        Param3,0,1,Group3,dist3\n\n    (Group and Dist columns are optional)\n\n    Returns a dictionary containing:\n        - names - the names of the parameters\n        - bounds - a list of lists of lower and upper bounds\n        - num_vars - a scalar indicating the number of variables\n                     (the length of names)\n        - groups - a list of group names (strings) for each variable\n        - dists - a list of distributions for the problem,\n                    None if not specified or all uniform\n\n    Arguments\n    ---------\n    filename : str\n        The path to the parameter file\n    delimiter : str, default=None\n        The delimiter used in the file to distinguish between columns\n\n    """"""\n    names = []\n    bounds = []\n    groups = []\n    dists = []\n    num_vars = 0\n    fieldnames = [\'name\', \'lower_bound\', \'upper_bound\', \'group\', \'dist\']\n\n    with open(filename, \'r\') as csvfile:\n        dialect = csv.Sniffer().sniff(csvfile.read(1024), delimiters=delimiter)\n        csvfile.seek(0)\n        reader = csv.DictReader(\n            csvfile, fieldnames=fieldnames, dialect=dialect)\n        for row in reader:\n            if row[\'name\'].strip().startswith(\'#\'):\n                pass\n            else:\n                num_vars += 1\n                names.append(row[\'name\'])\n                bounds.append(\n                    [float(row[\'lower_bound\']), float(row[\'upper_bound\'])])\n\n                # If the fourth column does not contain a group name, use\n                # the parameter name\n                if row[\'group\'] is None:\n                    groups.append(row[\'name\'])\n                elif row[\'group\'] is \'NA\':\n                    groups.append(row[\'name\'])\n                else:\n                    groups.append(row[\'group\'])\n\n                # If the fifth column does not contain a distribution\n                # use uniform\n                if row[\'dist\'] is None:\n                    dists.append(\'unif\')\n                else:\n                    dists.append(row[\'dist\'])\n\n    if groups == names:\n        groups = None\n    elif len(set(groups)) == 1:\n        raise ValueError(\'\'\'Only one group defined, results will not be\n            meaningful\'\'\')\n\n    # setting dists to none if all are uniform\n    # because non-uniform scaling is not needed\n    if all([d == \'unif\' for d in dists]):\n        dists = None\n\n    return {\'names\': names, \'bounds\': bounds, \'num_vars\': num_vars,\n            \'groups\': groups, \'dists\': dists}\n\n\ndef compute_groups_matrix(groups):\n    """"""Generate matrix which notes factor membership of groups\n\n    Computes a k-by-g matrix which notes factor membership of groups\n    where:\n        k is the number of variables (factors)\n        g is the number of groups\n    Also returns a g-length list of unique group_names whose positions\n    correspond to the order of groups in the k-by-g matrix\n\n    Arguments\n    ---------\n    groups : list\n        Group names corresponding to each variable\n\n    Returns\n    -------\n    tuple\n        containing group matrix assigning parameters to\n        groups and a list of unique group names\n    """"""\n    if not groups:\n        return None\n\n    num_vars = len(groups)\n\n    # Get a unique set of the group names\n    unique_group_names = list(OrderedDict.fromkeys(groups))\n    number_of_groups = len(unique_group_names)\n\n    indices = dict([(x, i) for (i, x) in enumerate(unique_group_names)])\n\n    output = np.zeros((num_vars, number_of_groups), dtype=np.int)\n\n    for parameter_row, group_membership in enumerate(groups):\n        group_index = indices[group_membership]\n        output[parameter_row, group_index] = 1\n\n    return output, unique_group_names\n\n\ndef requires_gurobipy(_has_gurobi):\n    \'\'\'\n    Decorator function which takes a boolean _has_gurobi as an argument.\n    Use decorate any functions which require gurobi.\n    Raises an import error at runtime if gurobi is not present.\n    Note that all runtime errors should be avoided in the working code,\n    using brute force options as preference.\n    \'\'\'\n    def _outer_wrapper(wrapped_function):\n        def _wrapper(*args, **kwargs):\n            if _has_gurobi:\n                result = wrapped_function(*args, **kwargs)\n            else:\n                warn(""Gurobi not available"", ImportWarning)\n                result = None\n            return result\n        return _wrapper\n    return _outer_wrapper\n'"
src/SALib/util/results.py,0,"b""import pandas as pd\nfrom SALib.plotting.bar import plot as barplot\n\nclass ResultDict(dict):\n    '''Dictionary holding analysis results.\n\n    Conversion methods (e.g. to Pandas DataFrames) to be attached as necessary\n    by each implementing method\n    '''\n    def __init__(self, *args, **kwargs):\n        super(ResultDict, self).__init__(*args, **kwargs)\n\n    def to_df(self):\n        '''Convert dict structure into Pandas DataFrame.'''\n        return pd.DataFrame({k: v for k, v in self.items() if k is not 'names'},\n                            index=self['names'])\n\n    def plot(self):\n        '''Create bar chart of results'''\n        Si_df = self.to_df()\n\n        if isinstance(Si_df, (list, tuple)):\n            import matplotlib.pyplot as plt\n\n            fig, axes = plt.subplots(1, len(Si_df))\n            for idx, f in enumerate(Si_df):\n                axes[idx] = barplot(f, ax=axes[idx])\n\n        else:\n            axes = barplot(Si_df)\n\n        return axes\n"""
tests/sample/morris/test_gurobi.py,2,"b'from unittest import skipUnless\n\nimport numpy as np\nfrom numpy.testing import assert_equal\nfrom pytest import raises\n\nfrom SALib.sample.morris.brute import BruteForce\nfrom SALib.sample.morris.gurobi import GlobalOptimisation\n\nfrom SALib.sample.morris import _sample_oat, \\\n    _compute_optimised_trajectories, \\\n    _sample_groups\n\n\nfrom SALib.util import read_param_file, compute_groups_matrix\n\ntry:\n    import gurobipy\nexcept ImportError:\n    _has_gurobi = False\nelse:\n    _has_gurobi = True\n\n\n@skipUnless(_has_gurobi, ""Gurobi is required for combinatorial optimisation"")\ndef test_optimal_sample_with_groups(setup_param_groups_prime):\n    \'\'\'\n    Tests that the combinatorial optimisation approach matches\n    that of the brute force approach\n    \'\'\'\n    param_file = setup_param_groups_prime\n    problem = read_param_file(param_file)\n\n    N = 10\n    num_levels = 8\n    k_choices = 4\n    num_params = problem[\'num_vars\']\n\n    sample = _sample_oat(problem,\n                         N,\n                         num_levels)\n\n    strategy = GlobalOptimisation()\n    actual = strategy.return_max_combo(sample,\n                                       N,\n                                       num_params,\n                                       k_choices)\n\n    brute_strategy = BruteForce()\n    desired = brute_strategy.brute_force_most_distant(sample,\n                                                      N,\n                                                      num_params,\n                                                      k_choices)\n\n    assert_equal(actual, desired)\n\n\n@skipUnless(_has_gurobi, ""Gurobi is required for combinatorial optimisation"")\ndef test_size_of_trajectories_with_groups(setup_param_groups_prime):\n    \'\'\'\n    Tests that the number of trajectories produced is computed\n    correctly (i.e. that the size of the trajectories is a function\n    of the number of groups, rather than the number of variables\n    when groups are used.\n\n    There are seven variables and three groups.\n    With N=10:\n    1. the sample ignoring groups (i.e. the call to `sample_oat\')\n    should be of size N*(D+1)-by-D.\n    2. the sample with groups should be of size N*(G+1)-by-D\n    When k=4:\n    3. the optimal sample ignoring groups should be of size k*(D+1)-by-D\n    4. the optimal sample with groups should be of size k*(G+1)-by-D\n    \'\'\'\n    param_file = setup_param_groups_prime\n    group_problem = read_param_file(param_file)\n    no_group_problem = read_param_file(param_file)\n    no_group_problem[\'groups\'] = None\n\n    N = 11\n    num_levels = 8\n    k_choices = 4\n    num_params = group_problem[\'num_vars\']\n\n    num_groups = 3\n\n    # Test 1. dimensions of sample ignoring groups\n    sample = _sample_oat(no_group_problem,\n                         N,\n                         num_levels)\n\n    size_x, size_y = sample.shape\n\n    assert_equal(size_x, N * (num_params + 1))\n    assert_equal(size_y, num_params)\n\n    # Test 2. dimensions of sample with groups\n\n    group_sample = _sample_groups(group_problem,\n                                  N,\n                                  num_levels)\n\n    size_x, size_y = group_sample.shape\n\n    assert_equal(size_x, N * (num_groups + 1))\n    assert_equal(size_y, num_params)\n\n    # Test 3. dimensions of optimal sample without groups\n\n    optimal_sample_without_groups = \\\n        _compute_optimised_trajectories(no_group_problem,\n                                        sample,\n                                        N,\n                                        k_choices)\n\n    size_x, size_y = optimal_sample_without_groups.shape\n\n    assert_equal(size_x, k_choices * (num_params + 1))\n    assert_equal(size_y, num_params)\n\n    # Test 4. dimensions of optimal sample with groups\n\n    optimal_sample_with_groups = _compute_optimised_trajectories(group_problem,\n                                                                 group_sample,\n                                                                 N,\n                                                                 k_choices)\n\n    size_x, size_y = optimal_sample_with_groups.shape\n\n    assert_equal(size_x, k_choices * (num_groups + 1))\n    assert_equal(size_y, num_params)\n\n\n@skipUnless(_has_gurobi, ""Gurobi is required for combinatorial optimisation"")\ndef test_optimal_combinations(setup_function):\n\n    N = 6\n    param_file = setup_function\n    problem = read_param_file(param_file)\n    num_params = problem[\'num_vars\']\n    num_levels = 10\n    k_choices = 4\n\n    morris_sample = _sample_oat(problem, N, num_levels)\n\n    global_strategy = GlobalOptimisation()\n    actual = global_strategy.return_max_combo(morris_sample,\n                                              N,\n                                              num_params,\n                                              k_choices)\n\n    brute_strategy = BruteForce()\n    desired = brute_strategy.brute_force_most_distant(morris_sample,\n                                                      N,\n                                                      num_params,\n                                                      k_choices)\n    assert_equal(actual, desired)\n\n\n@skipUnless(_has_gurobi, ""Gurobi is required for combinatorial optimisation"")\ndef test_optimised_trajectories_without_groups(setup_function):\n    """"""\n    Tests that the optimisation problem gives\n    the same answer as the brute force problem\n    (for small values of `k_choices` and `N`),\n    particularly when there are two or more identical\n    trajectories\n    """"""\n\n    N = 6\n    param_file = setup_function\n    problem = read_param_file(param_file)\n    k_choices = 4\n\n    num_params = problem[\'num_vars\']\n    groups = problem[\'groups\']\n\n    # 6 trajectories, with 5th and 6th identical\n    input_sample = np.array([[0.33333333,  0.66666667],\n                             [1., 0.66666667],\n                             [1., 0.],\n                             [0., 0.33333333],\n                             [0., 1.],\n                             [0.66666667, 1.],\n                             [0.66666667, 0.33333333],\n                             [0.66666667, 1.],\n                             [0., 1.],\n                             [0.66666667, 1.],\n                             [0.66666667, 0.33333333],\n                             [0., 0.33333333],\n                             [1., 1.],\n                             [1., 0.33333333],\n                             [0.33333333, 0.33333333],\n                             [1., 1.],\n                             [1., 0.33333333],\n                             [0.33333333, 0.33333333]], dtype=np.float32)\n\n    # print(input_sample)\n\n    # From gurobi optimal trajectories\n    strategy = GlobalOptimisation()\n    actual = strategy.return_max_combo(input_sample,\n                                       N,\n                                       num_params,\n                                       k_choices)\n\n    local_strategy = BruteForce()\n    desired = local_strategy.brute_force_most_distant(input_sample,\n                                                      N,\n                                                      num_params,\n                                                      k_choices,\n                                                      groups)\n\n    assert_equal(actual, desired)\n\n\n@skipUnless(_has_gurobi, ""Gurobi is required for combinatorial optimisation"")\ndef test_optimised_trajectories_groups(setup_param_groups_prime):\n    """"""\n    Tests that the optimisation problem gives\n    the same answer as the brute force problem\n    (for small values of `k_choices` and `N`)\n    with groups\n    """"""\n\n    N = 11\n    param_file = setup_param_groups_prime\n    problem = read_param_file(param_file)\n    num_levels = 4\n    k_choices = 4\n\n    num_params = problem[\'num_vars\']\n    groups = compute_groups_matrix(problem[\'groups\'], num_params)\n    input_sample = _sample_groups(problem, N, num_levels)\n\n    # From gurobi optimal trajectories\n    strategy = GlobalOptimisation()\n    actual = strategy.sample(input_sample,\n                             N,\n                             num_params,\n                             k_choices,\n                             groups)\n\n    brute_strategy = BruteForce()\n    desired = brute_strategy.sample(input_sample,\n                                    N,\n                                    num_params,\n                                    k_choices,\n                                    groups)\n    assert_equal(actual, desired)\n\n\n@skipUnless(_has_gurobi, ""Gurobi is required for combinatorial optimisation"")\ndef test_raise_error_if_k_gt_N(setup_function):\n    """"""Check that an error is raised if `k_choices` is greater than\n    (or equal to) `N`\n    """"""\n    N = 4\n    param_file = setup_function\n    problem = read_param_file(param_file)\n    num_levels = 4\n    k_choices = 6\n\n    morris_sample = _sample_oat(problem, N, num_levels)\n\n    with raises(ValueError):\n        _compute_optimised_trajectories(problem,\n                                        morris_sample,\n                                        N,\n                                        k_choices,\n                                        local_optimization=False)\n'"
tests/sample/morris/test_morris.py,19,"b'from __future__ import division\n\nfrom numpy.testing import assert_equal, assert_allclose\n\nfrom pytest import raises, fixture\n\nimport numpy as np\nimport warnings\n\nfrom SALib.sample.morris import (sample,\n                                 _compute_optimised_trajectories,\n                                 generate_p_star,\n                                 compute_b_star,\n                                 compute_delta,\n                                 generate_trajectory,\n                                 generate_x_star)\nfrom SALib.util import read_param_file, compute_groups_matrix\n\n\n@fixture(scope=\'function\')\ndef setup_input():\n    input_1 = [[0, 1 / 3.], [0, 1.], [2 / 3., 1.]]\n    input_2 = [[0, 1 / 3.], [2 / 3., 1 / 3.], [2 / 3., 1.]]\n    input_3 = [[2 / 3., 0], [2 / 3., 2 / 3.], [0, 2 / 3.]]\n    input_4 = [[1 / 3., 1.], [1., 1.], [1, 1 / 3.]]\n    input_5 = [[1 / 3., 1.], [1 / 3., 1 / 3.], [1, 1 / 3.]]\n    input_6 = [[1 / 3., 2 / 3.], [1 / 3., 0], [1., 0]]\n    input_sample = np.concatenate([input_1, input_2, input_3,\n                                   input_4, input_5, input_6])\n    return input_sample\n\n\n@fixture(scope=\'function\')\ndef expected_sample():\n    input_1 = [[0, 1 / 3.], [0, 1.], [2 / 3., 1.]]\n    input_3 = [[2 / 3., 0], [2 / 3., 2 / 3.], [0, 2 / 3.]]\n    input_4 = [[1 / 3., 1.], [1., 1.], [1, 1 / 3.]]\n    input_6 = [[1 / 3., 2 / 3.], [1 / 3., 0], [1., 0]]\n    return np.concatenate([input_1, input_3, input_4, input_6])\n\n\ndef test_odd_num_levels_raises_warning(setup_param_file_with_groups):\n\n    parameter_file = setup_param_file_with_groups\n    problem = read_param_file(parameter_file)\n    with warnings.catch_warnings(record=True) as w:\n        # Cause all warnings to always be triggered.\n        warnings.simplefilter(""always"")\n        # Trigger a warning.\n        sample(problem, 10, num_levels=3)\n        # Verify some things\n        assert len(w) == 1\n        assert issubclass(w[-1].category, UserWarning)\n        assert ""num_levels should be an even number, sample may be biased"" in str(w[-1].message)\n\n\ndef test_even_num_levels_no_warning(setup_param_file_with_groups):\n\n    parameter_file = setup_param_file_with_groups\n    problem = read_param_file(parameter_file)\n    with warnings.catch_warnings(record=True) as w:\n        # Cause all warnings to always be triggered.\n        warnings.simplefilter(""always"")\n        # Trigger a warning.\n        sample(problem, 10, num_levels=4)\n        # Verify some things\n        assert len(w) == 0\n\n\ndef test_group_in_param_file_read(setup_param_file_with_groups):\n    \'\'\'\n    Tests that groups in a parameter file are read correctly\n    \'\'\'\n    parameter_file = setup_param_file_with_groups\n    problem = read_param_file(parameter_file)\n    groups, group_names = compute_groups_matrix(\n        problem[\'groups\'])\n\n    assert_equal(problem[\'names\'], [""Test 1"", ""Test 2"", ""Test 3""])\n    assert_equal(groups, np.array([[1, 0], [1, 0], [0, 1]], dtype=np.int))\n    assert_equal(group_names, [\'Group 1\', \'Group 2\'])\n\n\ndef test_optimal_trajectories_lt_samples(setup_param_file):\n\n    parameter_file = setup_param_file\n    problem = read_param_file(parameter_file)\n\n    samples = 10\n    num_levels = 4\n\n    with raises(ValueError):\n        sample(problem, samples, num_levels,\n               optimal_trajectories=samples)\n\n\ndef test_optimal_trajectories_lt_10(setup_param_file):\n\n    parameter_file = setup_param_file\n    problem = read_param_file(parameter_file)\n\n    samples = 10\n    num_levels = 4\n\n    optimal_trajectories = 11\n    with raises(ValueError):\n        sample(problem, samples, num_levels,\n               optimal_trajectories=optimal_trajectories)\n\n\ndef test_optimal_trajectories_gte_one(setup_param_file):\n\n    parameter_file = setup_param_file\n    problem = read_param_file(parameter_file)\n\n    samples = 10\n    num_levels = 4\n    optimal_trajectories = 1\n\n    with raises(ValueError):\n        sample(problem, samples, num_levels,\n               optimal_trajectories)\n\n\ndef test_find_optimum_trajectories(setup_input, expected_sample):\n\n    N = 6\n    problem = {\'num_vars\': 2, \'groups\': None}\n    k_choices = 4\n\n    output = _compute_optimised_trajectories(\n        problem, setup_input, N, k_choices)\n    expected = expected_sample\n    np.testing.assert_equal(output, expected)\n\n\ndef test_catch_inputs_not_in_zero_one_range(setup_input):\n    problem = {\'num_vars\': 2, \'groups\': None}\n    k_choices = 4\n    N = 10\n    with raises(ValueError):\n        _compute_optimised_trajectories(problem, setup_input * 10, N,\n                                        k_choices)\n\n\ndef test_group_sample_fails_with_wrong_G_matrix():\n    N = 6\n    num_levels = 4\n\n    problem = {\'bounds\': [[0., 1.], [0., 1.], [0., 1.], [0., 1.]],\n               \'num_vars\': 4,\n               \'groups\': list([1, 2, 3])}\n\n    with raises(ValueError) as err:\n        sample(problem, N, num_levels)\n\n    assert ""Groups do not match to number of variables"" in str(err.value)\n\n\nclass TestGroupSampleGeneration:\n\n    def test_generate_p_star(self):\n        \'\'\'\n        Matrix P* - size (g * g) - describes order in which groups move\n        each row contains one element equal to 1, all others are 0\n        no two columns have 1s in the same position\n        \'\'\'\n        for i in range(1, 100):\n            output = generate_p_star(i)\n            if np.any(np.sum(output, 0) != np.ones(i)):\n                raise AssertionError(""Not legal P along axis 0"")\n            elif np.any(np.sum(output, 1) != np.ones(i)):\n                raise AssertionError(""Not legal P along axis 1"")\n\n    def test_compute_delta(self):\n        fixture = np.arange(2, 10)\n        output = [compute_delta(f) for f in fixture]\n        desired = np.array([1.0, 0.75, 0.666667, 0.625,\n                            0.6, 0.583333, 0.571429, 0.5625])\n        assert_allclose(output, desired, rtol=1e-2)\n\n    def test_generate_trajectory(self):\n        # Two groups of three factors\n        G = np.array([[1, 0], [0, 1], [0, 1]])\n        # Four levels\n        num_levels = 4\n        output = generate_trajectory(G, num_levels)\n        if np.any((output > 1) | (output < 0)):\n            raise AssertionError(""Bound not working: %s"", output)\n        assert_equal(output.shape[0], 3)\n        assert_equal(output.shape[1], 3)\n\n    def test_compute_B_star(self):\n        \'\'\'\n        Tests for expected output\n\n        Taken from example 3.2 in Saltelli et al. (2008) pg 122\n        \'\'\'\n\n        k = 3\n        g = 2\n\n        x_star = np.array([[1. / 3, 1. / 3, 0.]])\n        J = np.ones((g + 1, k))\n        G = np.array([[1, 0], [0, 1], [0, 1]])\n        D_star = np.array([[1, 0, 0], [0, -1, 0], [0, 0, 1]])\n        P_star = np.array([[1, 0], [0, 1]])\n        delta = 2. / 3\n        B = np.tril(np.ones([g + 1, g], dtype=int), -1)\n\n        desired = np.array([[1. / 3, 1, 0], [1, 1, 0], [1, 1. / 3, 2. / 3]])\n\n        output = compute_b_star(J, x_star, delta, B, G, P_star, D_star)\n        assert_allclose(output, desired)\n\n    def test_generate_x_star(self):\n        """"""\n        """"""\n        num_params = 4\n        num_levels = 4\n\n        np.random.seed(10)\n        actual = generate_x_star(num_params, num_levels)\n        print(actual)\n        expected = np.array([[0.333333, 0.333333, 0., 0.333333]])\n        assert_allclose(actual, expected, rtol=1e-05)\n'"
tests/sample/morris/test_morris_strategies.py,27,"b'""""""\n""""""\nfrom SALib.sample.morris import _sample_groups, SampleMorris\nfrom SALib.sample.morris.local import LocalOptimisation\nfrom SALib.sample.morris.brute import BruteForce\n\nfrom SALib.util import read_param_file\n\nimport numpy as np\nimport pytest\nfrom numpy.testing import assert_equal, assert_allclose\nfrom pytest import fixture, raises\nimport numpy.random as rd\n\n\n@fixture(scope=\'function\')\ndef setup_input():\n    input_1 = [[0, 1 / 3.], [0, 1.], [2 / 3., 1.]]\n    input_2 = [[0, 1 / 3.], [2 / 3., 1 / 3.], [2 / 3., 1.]]\n    input_3 = [[2 / 3., 0], [2 / 3., 2 / 3.], [0, 2 / 3.]]\n    input_4 = [[1 / 3., 1.], [1., 1.], [1, 1 / 3.]]\n    input_5 = [[1 / 3., 1.], [1 / 3., 1 / 3.], [1, 1 / 3.]]\n    input_6 = [[1 / 3., 2 / 3.], [1 / 3., 0], [1., 0]]\n    return np.concatenate([input_1, input_2, input_3, input_4, input_5,\n                           input_6])\n\n\n@fixture(scope=\'function\')\ndef setup_problem(setup_input):\n\n    input_sample = setup_input\n    num_samples = 6\n    problem = {\'num_vars\': 2, \'groups\': None}\n    k_choices = 4\n\n    groups = None\n    num_params = problem.get(\'num_vars\')\n\n    input_1 = [[0, 1 / 3.], [0, 1.], [2 / 3., 1.]]\n    input_3 = [[2 / 3., 0], [2 / 3., 2 / 3.], [0, 2 / 3.]]\n    input_4 = [[1 / 3., 1.], [1., 1.], [1, 1 / 3.]]\n    input_6 = [[1 / 3., 2 / 3.], [1 / 3., 0], [1., 0]]\n\n    expected = np.concatenate([input_1, input_3,\n                               input_4, input_6])\n\n    return (input_sample, num_samples, problem,\n            k_choices, groups, num_params, expected)\n\n\n@fixture(scope=\'function\')\ndef strategy():\n    return BruteForce()\n\n\nclass TestSharedMethods:\n\n    def test_check_input_sample_N(self, strategy, setup_input):\n        input_sample = setup_input\n        num_params = 4\n        N = 5\n        with raises(AssertionError):\n            strategy.check_input_sample(input_sample, num_params, N)\n\n    def test_check_input_sample_num_vars(self, strategy, setup_input):\n        input_sample = setup_input\n        num_params = 3\n        N = 6\n        with raises(AssertionError):\n            strategy.check_input_sample(input_sample, num_params, N)\n\n    def test_check_input_sample_range(self, strategy, setup_input):\n        input_sample = setup_input\n        input_sample *= 100\n        num_params = 4\n        N = 6\n        with raises(AssertionError):\n            strategy.check_input_sample(input_sample, num_params, N)\n\n    def test_find_maximum(self, strategy):\n        scores = np.array(range(15))\n        k_choices = 4\n        N = 6\n        output = strategy.find_maximum(scores, N, k_choices)\n        expected = [2, 3, 4, 5]\n        assert_equal(output, expected)\n\n    def test_distance(self, strategy):\n        \'\'\'\n        Tests the computation of the distance of two trajectories\n        \'\'\'\n        input_1 = np.array(\n            [[0, 1 / 3.], [0, 1.], [2 / 3., 1.]], dtype=np.float32)\n        input_3 = np.array([[2 / 3., 0], [2 / 3., 2 / 3.],\n                            [0, 2 / 3.]], dtype=np.float32)\n        output = strategy.compute_distance(input_1, input_3)\n        assert_allclose(output, 6.18, atol=1e-2)\n\n    def test_distance_of_identical_matrices_is_min(self, strategy):\n        input_1 = np.array([[1., 1.],\n                            [1., 0.33333333],\n                            [0.33333333, 0.33333333]])\n        input_2 = input_1.copy()\n        actual = strategy.compute_distance(input_1, input_2)\n        desired = 0\n        assert_allclose(actual, desired, atol=1e-2)\n\n    def test_distance_fail_with_difference_size_ip(self, strategy):\n        input_1 = np.array([[0, 1 / 3.], [0, 1.]], dtype=np.float32)\n        input_3 = np.array([[2 / 3., 0], [2 / 3., 2 / 3.],\n                            [0, 2 / 3.]], dtype=np.float32)\n        \n        expected_err = "".*Input matrices are different sizes.*""\n        with raises(ValueError, match=expected_err):\n            strategy.compute_distance(input_1, input_3)\n\n    def test_compute_distance_matrix(self, strategy, setup_input):\n        \'\'\'\n        Tests that a distance matrix is computed correctly\n\n        for an input of six trajectories and two parameters\n        \'\'\'\n        sample_inputs = setup_input\n        output = strategy.compute_distance_matrix(sample_inputs, 6, 2)\n        expected = np.zeros((6, 6), dtype=np.float32)\n        expected[1, :] = [5.50, 0, 0, 0, 0, 0]\n        expected[2, :] = [6.18, 5.31, 0, 0, 0, 0]\n        expected[3, :] = [6.89, 6.18, 6.57, 0, 0, 0]\n        expected[4, :] = [6.18, 5.31, 5.41, 5.5, 0, 0]\n        expected[5, :] = [7.52, 5.99, 5.52, 7.31, 5.77, 0]\n        assert_allclose(output, expected, rtol=1e-2)\n\n    def test_compute_distance_matrix_local(self, strategy, setup_input):\n        \'\'\'\n        Tests that a distance matrix is computed correctly for the\n        local distance optimization.\n        The only change is that the local method needs the upper triangle of\n        the distance matrix instead of the lower one.\n\n        This is for an input of six trajectories and two parameters\n        \'\'\'\n        sample_inputs = setup_input\n        output = strategy.compute_distance_matrix(\n            sample_inputs, 6, 2, local_optimization=True)\n        expected = np.zeros((6, 6), dtype=np.float32)\n        expected[0, :] = [0,    5.50, 6.18, 6.89, 6.18, 7.52]\n        expected[1, :] = [5.50, 0,    5.31, 6.18, 5.31, 5.99]\n        expected[2, :] = [6.18, 5.31, 0,    6.57, 5.41, 5.52]\n        expected[3, :] = [6.89, 6.18, 6.57, 0,    5.50, 7.31]\n        expected[4, :] = [6.18, 5.31, 5.41, 5.5,  0,    5.77]\n        expected[5, :] = [7.52, 5.99, 5.52, 7.31, 5.77, 0]\n        assert_allclose(output, expected, rtol=1e-2)\n\n\nclass TestLocallyOptimalStrategy:\n\n    def test_local(self, setup_problem):\n\n        rd.seed(12345)\n\n        (input_sample, num_samples, _,\n         k_choices, groups, num_params, expected) = setup_problem\n\n        local_strategy = LocalOptimisation()\n        context = SampleMorris(local_strategy)\n        actual = context.sample(input_sample, num_samples, num_params,\n                                k_choices, groups)\n        np.testing.assert_equal(actual, expected)\n\n    def test_find_local_maximum_distance(self, setup_input):\n        \'\'\'\n        Test whether finding the local maximum distance equals the global\n        maximum distance in a simple case for a defined random seed.\n        From Saltelli et al. 2008, in the solution to exercise 3a,\n        Chapter 3, page 134.\n\n        Note that local and brute force methods are not guaranteed to produce\n        the same results, even for simple problems,\n        hence forcing the seed here.\n\n        \'\'\'\n\n        rd.seed(12345)\n\n        local_strategy = LocalOptimisation()\n        brute_strategy = BruteForce()\n\n        sample_inputs = setup_input\n        N = 6\n        num_params = 2\n        k_choices = 4\n        output_global = brute_strategy.brute_force_most_distant(sample_inputs,\n                                                                N, num_params,\n                                                                k_choices)\n        output_local = local_strategy.find_local_maximum(sample_inputs, N,\n                                                         num_params, k_choices)\n        assert_equal(output_global, output_local)\n\n    def test_random_seed(self, setup_param_groups_prime):\n        """"""Setting the seed before generating a sample results in two\n        identical samples\n        """"""\n        N = 8\n        param_file = setup_param_groups_prime\n        problem = read_param_file(param_file)\n        num_levels = 4\n\n        np.random.seed(12345)\n        expected = _sample_groups(problem, N, num_levels)\n\n        np.random.seed(12345)\n        actual = _sample_groups(problem, N, num_levels)\n\n        assert_equal(actual, expected)\n\n    @pytest.mark.parametrize(\'execution_number\', range(1))\n    def test_local_optimised_groups(self,\n                                    setup_param_groups_prime,\n                                    execution_number):\n        """"""\n        Tests that the local optimisation problem gives\n        the same answer as the brute force problem\n        (for small values of `k_choices` and `N`)\n        with groups for a defined random seed.\n\n        Note that local and brute force methods are not guaranteed to produce\n        exact answers, even for small problems.\n        """"""\n        rd.seed(12345)\n\n        N = 8\n        param_file = setup_param_groups_prime\n        problem = read_param_file(param_file)\n        num_levels = 4\n        k_choices = 4\n\n        num_params = problem[\'num_vars\']\n\n        num_groups = len(set(problem[\'groups\']))\n\n        input_sample = _sample_groups(problem, N, num_levels)\n\n        local = LocalOptimisation()\n\n        # From local optimal trajectories\n        actual = local.find_local_maximum(input_sample, N, num_params,\n                                          k_choices, num_groups)\n\n        brute = BruteForce()\n        desired = brute.brute_force_most_distant(input_sample,\n                                                 N,\n                                                 num_params,\n                                                 k_choices,\n                                                 num_groups)\n\n        print(""Actual: {}\\nDesired: {}\\n"".format(actual, desired))\n        print(input_sample)\n        assert_equal(actual, desired)\n\n\nclass TestLocalMethods:\n\n    def test_sum_distances(self, setup_input):\n        \'\'\'\n        Tests whether the combinations are summed correctly.\n        \'\'\'\n        strategy = LocalOptimisation()\n\n        dist_matr = strategy.compute_distance_matrix(setup_input, 6, 2,\n                                                     num_groups=None,\n                                                     local_optimization=True)\n        indices = (1, 3, 2)\n        distance = strategy.sum_distances(indices, dist_matr)\n\n        expected = 10.47\n        assert_allclose(distance, expected, rtol=1e-2)\n\n    def test_get_max_sum_ind(self):\n        \'\'\'\n        Tests whether the right maximum indices are returned.\n        \'\'\'\n        strategy = LocalOptimisation()\n\n        indices = np.array([(1, 2, 4), (3, 2, 1), (4, 2, 1)])\n        distances = np.array([20, 40, 50])\n\n        output = strategy.get_max_sum_ind(indices, distances, 0, 0)\n        expected = (4, 2, 1)\n\n        assert_equal(output, expected)\n\n    def test_add_indices(self):\n        \'\'\'\n        Tests whether the right indices are added.\n        \'\'\'\n        strategy = LocalOptimisation()\n\n        indices = (1, 3, 4)\n        matr = np.zeros((6, 6), dtype=np.int16)\n        ind_extra = strategy.add_indices(indices, matr)\n\n        expected = [(1, 3, 4, 0), (1, 3, 4, 2), (1, 3, 4, 5)]\n\n        assert_equal(ind_extra, expected)\n\n    def test_get_max_sum_index_raises_error(self):\n        strategy = LocalOptimisation()\n        indices = [(1, 2, 4), (3, 2, 1), (4, 2, 1)]\n        distances_wrong = [20, 40]\n\n        with raises(ValueError):\n            strategy.get_max_sum_ind(indices, distances_wrong, 0, 0)\n\n    def test_combo_from_locally_optimal_method(self, setup_input):\n        \'\'\'\n        Tests whether the correct combination is picked from the fixture drawn\n        from Saltelli et al. 2008, in the solution to exercise 3a,\n        Chapter 3, page 134.\n        \'\'\'\n        sample_inputs = setup_input\n        N = 6\n        num_params = 2\n        k_choices = 4\n        strategy = LocalOptimisation()\n        output = strategy.find_local_maximum(sample_inputs, N,\n                                             num_params, k_choices)\n        expected = [0, 2, 3, 5]  # trajectories 1, 3, 4, 6\n        assert_equal(output, expected)\n\n\nclass TestBruteForceStrategy:\n\n    def test_brute_force(self, setup_problem):\n\n        (input_sample, num_samples, _,\n         k_choices, groups, num_params, expected) = setup_problem\n\n        strategy = BruteForce()\n        context = SampleMorris(strategy)\n        actual = context.sample(input_sample, num_samples, num_params,\n                                k_choices, groups)\n\n        np.testing.assert_equal(actual, expected)\n\n\nclass TestBruteForceMethods:\n\n    def test_combo_from_find_most_distant(self, setup_input):\n        \'\'\'\n        Tests whether the correct combination is picked from the fixture drawn\n        from Saltelli et al. 2008, in the solution to exercise 3a,\n        Chapter 3, page 134.\n        \'\'\'\n        sample_inputs = setup_input\n        N = 6\n        num_params = 2\n        k_choices = 4\n        strategy = BruteForce()\n        scores = strategy.find_most_distant(sample_inputs, N, num_params,\n                                            k_choices)\n        output = strategy.find_maximum(scores, N, k_choices)\n        expected = [0, 2, 3, 5]  # trajectories 1, 3, 4, 6\n        assert_equal(output, expected)\n\n    def test_scores_from_find_most_distant(self, setup_input):\n        \'\'\'\n        Checks whether array of scores from (6 4) is correct.\n\n        Data is derived from Saltelli et al. 2008,\n        in the solution to exercise 3a, Chapter 3, page 134.\n\n        \'\'\'\n        sample_inputs = setup_input\n        N = 6\n        num_params = 2\n        k_choices = 4\n        strategy = BruteForce()\n        output = strategy.find_most_distant(sample_inputs, N, num_params,\n                                            k_choices)\n        expected = np.array([15.022, 13.871, 14.815, 14.582, 16.178, 14.912,\n                             15.055, 16.410, 15.685, 16.098, 14.049, 15.146,\n                             14.333, 14.807, 14.825],\n                            dtype=np.float32)\n\n        assert_allclose(output, expected, rtol=1e-1, atol=1e-2)\n\n    def test_catch_combos_too_large(self):\n        N = int(1e6)\n        k_choices = 4\n        num_params = 2\n        input_sample = np.random.random_sample((N, num_params))\n        strategy = BruteForce()\n        with raises(ValueError):\n            strategy.find_most_distant(input_sample, N, num_params, k_choices)\n\n    def test_make_index_list(self):\n        N = 4\n        num_params = 2\n        groups = None\n        strategy = BruteForce()\n        actual = strategy._make_index_list(N, num_params, groups)\n        desired = [np.array([0, 1, 2]), np.array([3, 4, 5]),\n                   np.array([6, 7, 8]), np.array([9, 10, 11])]\n        assert_equal(desired, actual)\n\n    def test_make_index_list_with_groups(self):\n        N = 4\n        num_params = 3\n        groups = 2\n        strategy = BruteForce()\n        actual = strategy._make_index_list(N, num_params, groups)\n        desired = [np.array([0, 1, 2]), np.array([3, 4, 5]),\n                   np.array([6, 7, 8]), np.array([9, 10, 11])]\n        assert_equal(actual, desired)\n'"
src/SALib/sample/morris/__init__.py,20,"b'""""""\nGenerate a sample using the Method of Morris\n\nThree variants of Morris\' sampling for elementary effects is supported:\n\n- Vanilla Morris\n- Optimised trajectories when ``optimal_trajectories=True`` (using\n    Campolongo\'s enhancements from 2007 and optionally Ruano\'s enhancement\n    from 2012; ``local_optimization=True``)\n- Groups with optimised trajectories when ``optimal_trajectories=True`` and\n    the problem definition specifies groups (note that ``local_optimization``\n    must be ``False``)\n\nAt present, optimised trajectories is implemented using either a brute-force\napproach, which can be very slow, especially if you require more than four\ntrajectories, or a local method based which is much faster. Both methods now\nimplement working with groups of factors.\n\nNote that the number of factors makes little difference,\nbut the ratio between number of optimal trajectories and the sample size\nresults in an exponentially increasing number of scores that must be\ncomputed to find the optimal combination of trajectories.  We suggest going\nno higher than 4 from a pool of 100 samples with the brute force approach.\nWith local_optimization = True (which is default),\nit is possible to go higher than the previously suggested 4 from 100.\n\n""""""\nfrom __future__ import division\n\nimport numpy as np\n\nimport numpy.random as rd\nimport warnings\n\nfrom . gurobi import GlobalOptimisation\nfrom . local import LocalOptimisation\nfrom . brute import BruteForce\n\nfrom . strategy import SampleMorris\n\nfrom SALib.sample import common_args\nfrom SALib.util import scale_samples, read_param_file, compute_groups_matrix\n\ntry:\n    import gurobipy\nexcept ImportError:\n    _has_gurobi = False\nelse:\n    _has_gurobi = True\n\n__all__ = [\'sample\']\n\n\ndef sample(problem, N, num_levels=4, optimal_trajectories=None,\n           local_optimization=True, seed=None):\n    """"""Generate model inputs using the Method of Morris\n\n    Returns a NumPy matrix containing the model inputs required for Method of\n    Morris.  The resulting matrix has :math:`(G+1)*T` rows and :math:`D`\n    columns, where :math:`D` is the number of parameters, :math:`G` is the\n    number of groups (if no groups are selected, the number of parameters).\n    :math:`T` is the number of trajectories :math:`N`,\n    or `optimal_trajectories` if selected.\n    These model inputs  are intended to be used with\n    :func:`SALib.analyze.morris.analyze`.\n\n    Parameters\n    ----------\n    problem : dict\n        The problem definition\n    N : int\n        The number of trajectories to generate\n    num_levels : int, default=4\n        The number of grid levels (should be even)\n    optimal_trajectories : int\n        The number of optimal trajectories to sample (between 2 and N)\n    local_optimization : bool, default=True\n        Flag whether to use local optimization according to Ruano et al. (2012)\n        Speeds up the process tremendously for bigger N and num_levels.\n        If set to ``False`` brute force method is used, unless ``gurobipy`` is\n        available\n\n    Returns\n    -------\n    sample : numpy.ndarray\n        Returns a numpy.ndarray containing the model inputs required for Method\n        of Morris. The resulting matrix has :math:`(G/D+1)*N/T` rows and\n        :math:`D` columns, where :math:`D` is the number of parameters.\n    """"""\n    if seed:\n        np.random.seed(seed)\n\n    if not num_levels % 2 == 0:\n        warnings.warn(""num_levels should be an even number, sample may be biased"")\n    if problem.get(\'groups\'):\n        sample = _sample_groups(problem, N, num_levels)\n    else:\n        sample = _sample_oat(problem, N, num_levels)\n\n    if optimal_trajectories:\n\n        sample = _compute_optimised_trajectories(problem,\n                                                 sample,\n                                                 N,\n                                                 optimal_trajectories,\n                                                 local_optimization)\n\n    scale_samples(sample, problem[\'bounds\'])\n    return sample\n\n\ndef _sample_oat(problem, N, num_levels=4):\n    """"""Generate trajectories without groups\n\n    Arguments\n    ---------\n    problem : dict\n        The problem definition\n    N : int\n        The number of samples to generate\n    num_levels : int, default=4\n        The number of grid levels\n    """"""\n    group_membership = np.asmatrix(np.identity(problem[\'num_vars\'],\n                                               dtype=int))\n\n    num_params = group_membership.shape[0]\n    sample = np.array([generate_trajectory(group_membership,\n                                           num_levels)\n                       for n in range(N)])\n    return sample.reshape((N * (num_params + 1), num_params))\n\n\ndef _sample_groups(problem, N, num_levels=4):\n    """"""Generate trajectories for groups\n\n    Returns an :math:`N(g+1)`-by-:math:`k` array of `N` trajectories,\n    where :math:`g` is the number of groups and :math:`k` is the number\n    of factors\n\n    Arguments\n    ---------\n    problem : dict\n        The problem definition\n    N : int\n        The number of trajectories to generate\n    num_levels : int, default=4\n        The number of grid levels\n\n    Returns\n    -------\n    numpy.ndarray\n    """"""\n    if len(problem[\'groups\']) != problem[\'num_vars\']:\n        raise ValueError(""Groups do not match to number of variables"")\n\n    group_membership, _ = compute_groups_matrix(problem[\'groups\'])\n\n    if group_membership is None:\n        raise ValueError(""Please define the \'group_membership\' matrix"")\n    if not isinstance(group_membership, np.ndarray):\n        raise TypeError(""Argument \'group_membership\' should be formatted \\\n                         as a numpy ndarray"")\n\n    num_params = group_membership.shape[0]\n    num_groups = group_membership.shape[1]\n    sample = np.zeros((N * (num_groups + 1), num_params))\n    sample = np.array([generate_trajectory(group_membership,\n                                           num_levels)\n                       for n in range(N)])\n    return sample.reshape((N * (num_groups + 1), num_params))\n\n\ndef generate_trajectory(group_membership, num_levels=4):\n    """"""Return a single trajectory\n\n    Return a single trajectory of size :math:`(g+1)`-by-:math:`k`\n    where :math:`g` is the number of groups,\n    and :math:`k` is the number of factors,\n    both implied by the dimensions of `group_membership`\n\n    Arguments\n    ---------\n    group_membership : np.ndarray\n        a k-by-g matrix which notes factor membership of groups\n    num_levels : int, default=4\n        The number of levels in the grid\n\n    Returns\n    -------\n    np.ndarray\n    """"""\n\n    delta = compute_delta(num_levels)\n\n    # Infer number of groups `g` and number of params `k` from\n    # `group_membership` matrix\n    num_params = group_membership.shape[0]\n    num_groups = group_membership.shape[1]\n\n    # Matrix B - size (g + 1) * g -  lower triangular matrix\n    B = np.tril(np.ones([num_groups + 1, num_groups],\n                        dtype=int), -1)\n\n    P_star = generate_p_star(num_groups)\n\n    # Matrix J - a (g+1)-by-num_params matrix of ones\n    J = np.ones((num_groups + 1, num_params))\n\n    # Matrix D* - num_params-by-num_params matrix which decribes whether\n    # factors move up or down\n    D_star = np.diag(rd.choice([-1, 1], num_params))\n\n    x_star = generate_x_star(num_params, num_levels)\n\n    # Matrix B* - size (num_groups + 1) * num_params\n    B_star = compute_b_star(J, x_star, delta, B,\n                            group_membership, P_star, D_star)\n\n    return B_star\n\n\ndef compute_b_star(J, x_star, delta, B, G, P_star, D_star):\n    """"""\n    """"""\n    element_a = J[0, :] * x_star\n    element_b = np.matmul(G, P_star).T\n    element_c = np.matmul(2.0 * B, element_b)\n    element_d = np.matmul((element_c - J), D_star)\n\n    b_star = element_a + (delta / 2.0) * (element_d + J)\n    return b_star\n\n\ndef generate_p_star(num_groups):\n    """"""Describe the order in which groups move\n\n    Arguments\n    ---------\n    num_groups : int\n\n    Returns\n    -------\n    np.ndarray\n        Matrix P* - size (g-by-g)\n    """"""\n    p_star = np.eye(num_groups, num_groups)\n    rd.shuffle(p_star)\n    return p_star\n\n\ndef generate_x_star(num_params, num_levels):\n    """"""Generate an 1-by-num_params array to represent initial position for EE\n\n    This should be a randomly generated array in the p level grid\n    :math:`\\omega`\n\n    Arguments\n    ---------\n    num_params : int\n        The number of parameters (factors)\n    num_levels : int\n        The number of levels\n\n    Returns\n    -------\n    numpy.ndarray\n        The initial starting positions of the trajectory\n    """"""\n    x_star = np.zeros((1, num_params))\n    delta = compute_delta(num_levels)\n    bound = 1 - delta\n    grid = np.linspace(0, bound, int(num_levels / 2))\n\n    x_star[0, :] = rd.choice(grid, num_params)\n\n    return x_star\n\n\ndef compute_delta(num_levels):\n    """"""Computes the delta value from number of levels\n\n    Arguments\n    ---------\n    num_levels : int\n        The number of levels\n\n    Returns\n    -------\n    float\n    """"""\n    return num_levels / (2.0 * (num_levels - 1))\n\n\ndef _compute_optimised_trajectories(problem, input_sample, N, k_choices,\n                                    local_optimization=False):\n    \'\'\'\n    Calls the procedure to compute the optimum k_choices of trajectories\n    from the input_sample.\n    If there are groups, then this procedure allocates the groups to the\n    correct call here.\n\n    Arguments\n    ---------\n    problem : dict\n        The problem definition\n    input_sample :\n    N : int\n        The number of samples to generate\n    k_choices : int\n        The number of optimal trajectories\n    local_optimization : bool, default=False\n        If true, uses local optimisation heuristic\n    \'\'\'\n    if _has_gurobi is False \\\n            and local_optimization is False \\\n            and k_choices > 10:\n        msg = ""Running optimal trajectories greater than values of 10 \\\n                will take a long time.""\n        raise ValueError(msg)\n\n    num_params = problem[\'num_vars\']\n\n    if np.any((input_sample < 0) | (input_sample > 1)):\n        raise ValueError(""Input sample must be scaled between 0 and 1"")\n\n    if _has_gurobi and local_optimization is False:\n        # Use global optimization method\n        strategy = GlobalOptimisation()\n    elif local_optimization:\n        # Use local method\n        strategy = LocalOptimisation()\n    else:\n        # Use brute force approach\n        strategy = BruteForce()\n\n    if problem.get(\'groups\'):\n        num_groups = len(set(problem[\'groups\']))\n    else:\n        num_groups = None\n\n    context = SampleMorris(strategy)\n    output = context.sample(input_sample, N, num_params,\n                            k_choices, num_groups)\n\n    return output\n\n\ndef cli_parse(parser):\n    parser.add_argument(\'-l\', \'--levels\', type=int, required=False,\n                        default=4, help=\'Number of grid levels \\\n                        (Morris only)\')\n    parser.add_argument(\'-k\', \'--k-optimal\', type=int, required=False,\n                        default=None,\n                        help=\'Number of optimal trajectories \\\n                        (Morris only)\')\n    parser.add_argument(\'-lo\', \'--local\', type=bool, required=True,\n                        default=False,\n                        help=\'Use the local optimisation method \\\n                        (Morris with optimization only)\')\n    return parser\n\n\ndef cli_action(args):\n    rd.seed(args.seed)\n\n    problem = read_param_file(args.paramfile)\n    param_values = sample(problem, args.samples, args.levels,\n                          args.k_optimal, args.local)\n\n    np.savetxt(args.output, param_values, delimiter=args.delimiter,\n               fmt=\'%.\' + str(args.precision) + \'e\')\n\n\nif __name__ == ""__main__"":\n    common_args.run_cli(cli_parse, cli_action)\n'"
src/SALib/sample/morris/brute.py,6,"b'""""""\n""""""\nfrom SALib.sample.morris.strategy import Strategy\nfrom scipy.special import comb as nchoosek\nfrom itertools import combinations, islice\nimport sys\nimport numpy as np\n\n\nclass BruteForce(Strategy):\n    """"""Implements the brute force optimisation strategy\n    """"""\n\n    def _sample(self, input_sample, num_samples,\n                num_params, k_choices, num_groups=None):\n        return self.brute_force_most_distant(input_sample, num_samples,\n                                             num_params, k_choices, num_groups)\n\n    def brute_force_most_distant(self, input_sample, num_samples,\n                                 num_params, k_choices,\n                                 num_groups=None):\n        """"""Use brute force method to find most distant trajectories\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n        num_samples : int\n            The number of samples to generate\n        num_params : int\n            The number of parameters\n        k_choices : int\n            The number of optimal trajectories\n        num_groups : int, default=None\n            The number of groups\n\n        Returns\n        -------\n        list\n        """"""\n        scores = self.find_most_distant(input_sample,\n                                        num_samples,\n                                        num_params,\n                                        k_choices,\n                                        num_groups)\n\n        maximum_combo = self.find_maximum(scores, num_samples, k_choices)\n\n        return maximum_combo\n\n    def find_most_distant(self, input_sample, num_samples,\n                          num_params, k_choices, num_groups=None):\n        """"""\n        Finds the \'k_choices\' most distant choices from the\n        \'num_samples\' trajectories contained in \'input_sample\'\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n        num_samples : int\n            The number of samples to generate\n        num_params : int\n            The number of parameters\n        k_choices : int\n            The number of optimal trajectories\n        num_groups : int, default=None\n            The number of groups\n\n        Returns\n        -------\n        numpy.ndarray\n        """"""\n        # Now evaluate the (N choose k_choices) possible combinations\n        if nchoosek(num_samples, k_choices) >= sys.maxsize:\n            raise ValueError(""Number of combinations is too large"")\n        number_of_combinations = int(nchoosek(num_samples, k_choices))\n\n        # First compute the distance matrix for each possible pairing\n        # of trajectories and store in a shared-memory array\n        distance_matrix = self.compute_distance_matrix(input_sample,\n                                                       num_samples,\n                                                       num_params,\n                                                       num_groups)\n\n        # Initialise the output array\n        chunk = int(1e6)\n        if chunk > number_of_combinations:\n            chunk = number_of_combinations\n\n        counter = 0\n        # Generate a list of all the possible combinations\n        combo_gen = combinations(range(num_samples), k_choices)\n        scores = np.zeros(number_of_combinations, dtype=np.float32)\n        # Generate the pairwise indices once\n        pairwise = np.array(\n            [y for y in combinations(range(k_choices), 2)])\n\n        mappable = self.mappable\n        for combos in self.grouper(chunk, combo_gen):\n            scores[(counter * chunk):((counter + 1) * chunk)] \\\n                = mappable(combos, pairwise, distance_matrix)\n            counter += 1\n        return scores\n\n    @staticmethod\n    def grouper(n, iterable):\n        it = iter(iterable)\n        while True:\n            chunk = tuple(islice(it, n))\n            if not chunk:\n                return\n            yield chunk\n\n    @staticmethod\n    def mappable(combos, pairwise, distance_matrix):\n        \'\'\'\n        Obtains scores from the distance_matrix for each pairwise combination\n        held in the combos array\n\n        Arguments\n        ----------\n        combos : numpy.ndarray\n        pairwise : numpy.ndarray\n        distance_matrix : numpy.ndarray\n        \'\'\'\n        combos = np.array(combos)\n        # Create a list of all pairwise combination for each combo in combos\n        combo_list = combos[:, pairwise[:, ]]\n\n        addresses = [combo_list[:, :, 1], combo_list[:, :, 0]]\n\n        all_distances = distance_matrix[addresses]\n        new_scores = np.sqrt(\n            np.einsum(\'ij,ij->i\', all_distances, all_distances))\n        return new_scores\n\n    def find_maximum(self, scores, N, k_choices):\n        """"""Finds the `k_choices` maximum scores from `scores`\n\n        Arguments\n        ---------\n        scores : numpy.ndarray\n        N : int\n        k_choices : int\n\n        Returns\n        -------\n        list\n        """"""\n        if not isinstance(scores, np.ndarray):\n            raise TypeError(""Scores input is not a numpy array"")\n\n        index_of_maximum = int(scores.argmax())\n        maximum_combo = self.nth(combinations(\n            range(N), k_choices), index_of_maximum, None)\n        return sorted(maximum_combo)\n\n    @staticmethod\n    def nth(iterable, n, default=None):\n        """"""Returns the nth item or a default value\n\n        Arguments\n        ---------\n        iterable : iterable\n        n : int\n        default : default=None\n            The default value to return\n        """"""\n\n        if type(n) != int:\n            raise TypeError(""n is not an integer"")\n\n        return next(islice(iterable, n, None), default)\n'"
src/SALib/sample/morris/gurobi.py,2,"b'\'\'\'Finds optimal trajectories using a global optimisation method\n\nExample\n-------\nRun using\n\n>>> optimal_trajectories.py -n=10 \\\n    -p=esme_param.txt -o=test_op.txt \\\n    -s=12892 --num-levels=4 --grid-jump=2 \\\n    --k-choices=4\n\n\'\'\'\nfrom __future__ import division\n\nimport re\nfrom datetime import datetime as dt\n\nimport numpy as np\n\ntry:\n    from scipy.misc import comb as nchoosek\nexcept ImportError:\n    from scipy.special import comb as nchoosek\n\nfrom . strategy import Strategy\n\nfrom SALib.util import requires_gurobipy\n\ntry:\n    from gurobipy import Model, quicksum, GRB\nexcept ImportError:\n    _has_gurobi = False\nelse:\n    _has_gurobi = True\n\n\n@requires_gurobipy(_has_gurobi)\nclass GlobalOptimisation(Strategy):\n    """"""Implements the global optimisation algorithm\n    """"""\n\n    def _sample(self, input_sample, num_samples,\n                num_params, k_choices, num_groups=None):\n        return self.return_max_combo(input_sample, num_samples,\n                                     num_params, k_choices, num_groups)\n\n    @staticmethod\n    def global_model(N, k_choices, distance_matrix):\n\n        if k_choices >= N:\n            raise ValueError(""k_choices must be less than N"")\n\n        model = Model(""distance1"")\n        trajectories = range(N)\n\n        distance_matrix = np.array(\n            distance_matrix / distance_matrix.max(), dtype=np.float64)\n\n        dm = distance_matrix ** 2\n\n        y, x = {}, {}\n        for i in trajectories:\n            y[i] = model.addVar(vtype=""B"", obj=0, name=""y[%s]"" % i)\n            for j in range(i + 1, N):\n                x[i, j] = model.addVar(\n                    vtype=""B"", obj=1.0, name=""x[%s,%s]"" % (i, j))\n        model.update()\n\n        model.setObjective(quicksum([x[i, j] * dm[j][i]\n                                     for i in trajectories\n                                     for j in range(i + 1, N)]))\n\n        # Add constraints to the model\n        model.addConstr(quicksum([y[i]\n                                  for i in trajectories]) <= k_choices, ""27"")\n\n        for i in trajectories:\n            for j in range(i + 1, N):\n                model.addConstr(x[i, j] <= y[i], ""28-%s-%s"" % (i, j))\n                model.addConstr(x[i, j] <= y[j], ""29-%s-%s"" % (i, j))\n                model.addConstr(y[i] + y[j] <= 1 + x[i, j],\n                                ""30-%s-%s"" % (i, j))\n\n        model.addConstr(quicksum([x[i, j] for i in trajectories\n                                  for j in range(i + 1, N)])\n                        <= nchoosek(k_choices, 2), ""Cut_1"")\n        model.update()\n        return model\n\n    def return_max_combo(self, input_data, N, num_params,\n                         k_choices, num_groups=None):\n        """"""Find the optimal combination of most different trajectories\n\n        Arguments\n        ---------\n        input_data : numpy.ndarray\n        N : int\n            The number of samples\n        num_params : int\n            The number of factors\n        k_choices : int\n            The number of optimal trajectories to select\n        num_groups: int, default=None\n            The number of groups\n\n        Returns\n        -------\n        list\n        """"""\n\n        distance_matrix = self.compute_distance_matrix(\n            input_data, N, num_params, num_groups)\n\n        model = self.global_model(N, k_choices, distance_matrix)\n        model.params.Threads = 0\n\n        model.ModelSense = GRB.MAXIMIZE\n        model.optimize()\n        if model.Status == GRB.OPTIMAL:\n            print(model.objval)\n\n        variables = list(model.getVars())\n        x_vars = []\n        for v in variables:\n            if (v.X > 0) & (v.VarName[0] == \'x\'):\n                x_vars.append(v.VarName)\n        b = [re.findall(r""\\d{1,}"", str(v)) for v in x_vars]\n        maximum_combo = list(set([int(y) for z in b for y in z]))\n\n        print(maximum_combo)\n\n        return sorted(maximum_combo)\n\n\ndef timestamp(num_params, p_levels, k_choices, N):\n    """"""\n    Returns a uniform timestamp with parameter values for file identification\n    """"""\n    string = ""_v%s_l%s_gs%s_k%s_N%s_%s.txt"" % (num_params,\n                                               p_levels,\n                                               k_choices,\n                                               N,\n                                               dt.strftime(dt.now(),\n                                                           ""%d%m%y%H%M%S""))\n    return string\n'"
src/SALib/sample/morris/local.py,7,"b'""""""\n""""""\nfrom itertools import combinations\nimport numpy as np\n\nfrom . strategy import Strategy\n\n\nclass LocalOptimisation(Strategy):\n    """"""Implements the local optimisation algorithm using the Strategy interface\n    """"""\n\n    def _sample(self, input_sample, num_samples,\n                num_params, k_choices, num_groups=None):\n        return self.find_local_maximum(input_sample, num_samples, num_params,\n                                       k_choices, num_groups)\n\n    def find_local_maximum(self, input_sample, N, num_params,\n                           k_choices, num_groups=None):\n        """"""Find the most different trajectories in the input sample using a\n        local approach\n\n        An alternative by Ruano et al. (2012) for the brute force approach as\n        originally proposed by Campolongo et al. (2007). The method should\n        improve the speed with which an optimal set of trajectories is\n        found tremendously for larger sample sizes.\n\n        Arguments\n        ---------\n        input_sample : np.ndarray\n        N : int\n            The number of trajectories\n        num_params : int\n            The number of factors\n        k_choices : int\n            The number of optimal trajectories to return\n        num_groups : int, default=None\n            The number of groups\n        Returns\n        -------\n        list\n        """"""\n        distance_matrix = self.compute_distance_matrix(input_sample, N,\n                                                       num_params, num_groups,\n                                                       local_optimization=True)\n\n        tot_indices_list = []\n        tot_max_array = np.zeros(k_choices - 1)\n\n        maxima_template = np.zeros(len(distance_matrix))\n        # Loop over `k_choices`, i starts at 1\n        for i in range(1, k_choices):\n            indices_list = []\n            row_maxima_i = maxima_template.copy()\n\n            for row_nr, row in enumerate(distance_matrix):\n                indices = tuple(row.argsort()[-i:][::-1]) + (row_nr,)\n                row_maxima_i[row_nr] = self.sum_distances(\n                    indices, distance_matrix)\n                indices_list.append(indices)\n\n            # Find the indices belonging to the maximum distance\n            i_max_ind = self.get_max_sum_ind(indices_list, row_maxima_i, i, 0)\n\n            # Loop \'m\' (called loop \'k\' in Ruano)\n            m_max_ind = i_max_ind\n            # m starts at 1\n            m = 1\n\n            while m <= k_choices - i - 1:\n                m_ind = self.add_indices(m_max_ind, distance_matrix)\n\n                len_m_ind = len(m_ind)\n\n                m_maxima = np.zeros(len_m_ind)\n\n                for n in range(len_m_ind):\n                    m_maxima[n] = self.sum_distances(m_ind[n], distance_matrix)\n\n                m_max_ind = self.get_max_sum_ind(m_ind, m_maxima, i, m)\n\n                m += 1\n\n            tot_indices_list.append(m_max_ind)\n            tot_max_array[i -\n                          1] = self.sum_distances(m_max_ind, distance_matrix)\n\n        tot_max = self.get_max_sum_ind(\n            tot_indices_list, tot_max_array, ""tot"", ""tot"")\n        return sorted(tot_max)\n\n    def sum_distances(self, indices, distance_matrix):\n        """"""Calculate combinatorial distance between a select group of\n        trajectories, indicated by indices\n\n        Arguments\n        ---------\n        indices : tuple\n        distance_matrix : numpy.ndarray (M,M)\n\n        Returns\n        -------\n        numpy.ndarray\n\n        Notes\n        -----\n        This function can perhaps be quickened by calculating the sum of the\n        distances. The calculated distances, as they are right now,\n        are only used in a relative way. Purely summing distances would lead\n        to the same result, at a perhaps quicker rate.\n        """"""\n        combs_tup = combinations(indices, 2)\n\n        combs = np.array(tuple(zip(*combs_tup)))\n\n        # Calculate distance (vectorized)\n        dist = np.sqrt(\n            np.sum(np.square(distance_matrix[combs[0], combs[1]]), axis=0))\n\n        return dist\n\n    def get_max_sum_ind(self, indices_list, distances, i, m):\n        \'\'\'Get the indices that belong to the maximum distance in `distances`\n\n        Arguments\n        ---------\n        indices_list : list\n            list of tuples\n        distances : numpy.ndarray\n            size M\n        i : int\n        m : int\n\n        Returns\n        -------\n        list\n        \'\'\'\n        if len(indices_list) != len(distances):\n            msg = ""Indices and distances are lists of different length."" + \\\n                ""Length indices_list = {} and length distances = {}."" + \\\n                ""In loop i = {}  and m =  {}""\n            raise ValueError(msg.format(\n                len(indices_list), len(distances), i, m))\n\n        max_index = distances.argsort()[-1:][::-1]\n        return indices_list[max_index[0]]\n\n    def add_indices(self, indices, distance_matrix):\n        \'\'\'Adds extra indices for the combinatorial problem.\n\n        Arguments\n        ---------\n        indices : tuple\n        distance_matrix : numpy.ndarray (M,M)\n\n        Example\n        -------\n        >>> add_indices((1,2), numpy.array((5,5)))\n        [(1, 2, 3), (1, 2, 4), (1, 2, 5)]\n\n        \'\'\'\n        list_new_indices = []\n        for i in range(len(distance_matrix)):\n            if i not in indices:\n                list_new_indices.append(indices + (i,))\n        return list_new_indices\n'"
src/SALib/sample/morris/strategy.py,13,"b'""""""\nDefines a family of algorithms for generating samples\n\nThe sample a for use with :class:`SALib.analyze.morris.analyze`,\nencapsulate each one, and makes them interchangeable.\n\nExample\n-------\n>>> localoptimisation = LocalOptimisation()\n>>> context = SampleMorris(localoptimisation)\n>>> context.sample(input_sample, num_samples, num_params, k_choices, groups)\n""""""\nimport abc\n\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\n\nclass SampleMorris(object):\n    """"""Computes the optimum `k_choices` of trajectories from the input_sample.\n\n    Arguments\n    ---------\n    strategy : :class:`Strategy`\n    """"""\n\n    def __init__(self, strategy):\n        self._strategy = strategy\n\n    def sample(self, input_sample, num_samples, num_params, k_choices,\n               num_groups):\n        """"""Computes the optimum k_choices of trajectories\n        from the input_sample.\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n        num_samples : int\n            The number of samples to generate\n        num_params : int\n            The number of parameters\n        k_choices : int\n            The number of optimal trajectories\n        num_groups : int\n            The number of groups\n\n        Returns\n        -------\n        numpy.ndarray\n            An array of optimal trajectories\n        """"""\n        return self._strategy.sample(input_sample, num_samples, num_params,\n                                     k_choices, num_groups)\n\n\nclass Strategy:\n    """"""\n    Declare an interface common to all supported algorithms.\n    :class:`SampleMorris` uses this interface to call the algorithm\n    defined by a ConcreteStrategy.\n    """"""\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def _sample(self, input_sample, num_samples,\n                num_params, k_choices, num_groups):\n        """"""Implement this in your class\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n        num_samples : int\n            The number of samples to generate\n        num_params : int\n            The number of parameters\n        k_choices : int\n            The number of optimal trajectories\n        num_groups : int\n            The number of groups\n\n        Returns\n        -------\n        list\n            A list of trajectory indices\n        """"""\n        pass\n\n    def sample(self, input_sample, num_samples, num_params,\n               k_choices, num_groups=None):\n        """"""Computes the optimum k_choices of trajectories\n        from the input_sample.\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n        num_samples : int\n            The number of samples to generate\n        num_params : int\n            The number of parameters\n        k_choices : int\n            The number of optimal trajectories\n        num_groups : int, default=None\n            The number of groups\n\n        Returns\n        -------\n        numpy.ndarray\n        """"""\n        self.run_checks(num_samples, k_choices)\n        maximum_combo = self._sample(input_sample, num_samples,\n                                     num_params, k_choices, num_groups)\n\n        assert isinstance(maximum_combo, list)\n\n        output = self.compile_output(input_sample,\n                                     num_samples,\n                                     num_params,\n                                     maximum_combo,\n                                     num_groups)\n        return output\n\n    @staticmethod\n    def run_checks(number_samples, k_choices):\n        """"""Runs checks on `k_choices`\n        """"""\n        assert isinstance(k_choices, int), \\\n            ""Number of optimal trajectories should be an integer""\n\n        if k_choices < 2:\n            raise ValueError(\n                ""The number of optimal trajectories must be set to 2 or more."")\n        if k_choices >= number_samples:\n            msg = ""The number of optimal trajectories should be less than the \\\n                    number of samples""\n            raise ValueError(msg)\n\n    @staticmethod\n    def _make_index_list(num_samples, num_params, num_groups=None):\n        """"""Identify indices of input sample associated with each trajectory\n\n        For each trajectory, identifies the indexes of the input sample which\n        is a function of the number of factors/groups and the number of samples\n\n        Arguments\n        ---------\n        num_samples : int\n            The number of trajectories\n        num_params : int\n            The number of parameters\n        num_groups : int\n            The number of groups\n\n        Returns\n        -------\n        list of numpy.ndarray\n\n        Example\n        -------\n        >>> BruteForce()._make_index_list(num_samples=4, num_params=3,\n            num_groups=2)\n        [np.array([0, 1, 2]), np.array([3, 4, 5]), np.array([6, 7, 8]),\n         np.array([9, 10, 11])]\n        """"""\n        if num_groups is None:\n            num_groups = num_params\n\n        index_list = []\n        for j in range(num_samples):\n            index_list.append(np.arange(num_groups + 1) + j * (num_groups + 1))\n        return index_list\n\n    def compile_output(self, input_sample, num_samples, num_params,\n                       maximum_combo, num_groups=None):\n        """"""Picks the trajectories from the input\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n        num_samples : int\n        num_params : int\n        maximum_combo : list\n        num_groups : int\n\n        """"""\n\n        if num_groups is None:\n            num_groups = num_params\n\n        self.check_input_sample(input_sample, num_groups, num_samples)\n\n        index_list = self._make_index_list(num_samples, num_params, num_groups)\n\n        output = np.zeros(\n            (np.size(maximum_combo) * (num_groups + 1), num_params))\n        for counter, combo in enumerate(maximum_combo):\n            output[index_list[counter]] = np.array(\n                input_sample[index_list[combo]])\n        return output\n\n    @staticmethod\n    def check_input_sample(input_sample, num_params, num_samples):\n        """"""Check the `input_sample` is valid\n\n        Checks input sample is:\n            - the correct size\n            - values between 0 and 1\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n        num_params : int\n        num_samples : int\n        """"""\n        assert type(input_sample) == np.ndarray, \\\n            ""Input sample is not an numpy array""\n        assert input_sample.shape[0] == (num_params + 1) * num_samples, \\\n            ""Input sample does not match number of parameters or groups""\n        assert np.any((input_sample >= 0) | (input_sample <= 1)), \\\n            ""Input sample must be scaled between 0 and 1""\n\n    @staticmethod\n    def compute_distance(m, l):\n        \'\'\'Compute distance between two trajectories\n\n        Returns\n        -------\n        numpy.ndarray\n        \'\'\'\n\n        if np.shape(m) != np.shape(l):\n            raise ValueError(""Input matrices are different sizes"")\n        if np.array_equal(m, l):\n            # print(""Trajectory %s and %s are equal"" % (m, l))\n            distance = 0\n        else:\n            distance = np.array(np.sum(cdist(m, l)), dtype=np.float32)\n\n        return distance\n\n    def compute_distance_matrix(self, input_sample, num_samples, num_params,\n                                num_groups=None,\n                                local_optimization=False):\n        """"""Computes the distance between each and every trajectory\n\n        Each entry in the matrix represents the sum of the geometric distances\n        between all the pairs of points of the two trajectories\n\n        If the `groups` argument is filled, then the distances are still\n        calculated for each trajectory,\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n            The input sample of trajectories for which to compute\n            the distance matrix\n        num_samples : int\n            The number of trajectories\n        num_params : int\n            The number of factors\n        num_groups : int, default=None\n            The number of groups\n        local_optimization : bool, default=False\n            If True, fills the lower triangle of the distance matrix\n\n        Returns\n        -------\n        distance_matrix : numpy.ndarray\n\n        """"""\n        if num_groups:\n            self.check_input_sample(input_sample, num_groups, num_samples)\n        else:\n            self.check_input_sample(input_sample, num_params, num_samples)\n\n        index_list = self._make_index_list(num_samples, num_params, num_groups)\n        distance_matrix = np.zeros(\n            (num_samples, num_samples), dtype=np.float32)\n\n        for j in range(num_samples):\n            input_1 = input_sample[index_list[j]]\n            for k in range(j + 1, num_samples):\n                input_2 = input_sample[index_list[k]]\n\n                # Fills the lower triangle of the matrix\n                if local_optimization is True:\n                    distance_matrix[j, k] = self.compute_distance(\n                        input_1, input_2)\n\n                distance_matrix[k, j] = self.compute_distance(input_1, input_2)\n        return distance_matrix\n'"
