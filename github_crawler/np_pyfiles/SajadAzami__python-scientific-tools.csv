file_path,api_count,code
addtional_examples/bootstrap.py,4,"b'""""""CE2_Codes, 12/20/16, Sajad Azami""""""\n\nimport math\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as st\nimport seaborn as sns\nfrom statsmodels.distributions.empirical_distribution import ECDF\n\n__author__ = \'sajjadaazami@gmail.com (Sajad Azami)\'\nsns.set_style(""whitegrid"")\n\n\n# Simulation of bootsrap sampling\ndef bootstrap(samples):\n    return np.random.choice(samples, 100)\n\n\n# Computing a 95% Confidence Band for the CDF F, with n observation of distributin \'dist\'\ndef CDF_band(n_samples, dist):\n    samples = []\n    if dist == \'Normal\':\n        for i in range(0, n_samples):\n            samples.append(np.random.normal(0, 1))\n    elif dist == \'Cauchy\':\n        samples = np.random.standard_cauchy(n_samples)\n    else:\n        print(\'Wrong distribution!\')\n        return\n\n        # Now we bootstrap 100 samples 1000 times\n    bootstraped_samples = []\n    for i in range(0, 1000):\n        bootstraped_samples.extend(bootstrap(samples))\n    # Adding the original samples\n    bootstraped_samples.extend(samples)\n\n    # Estimated Empirical CDF\n    ecdf = ECDF(bootstraped_samples)\n\n    line = np.linspace(-5, 5, 1000)\n    ecdf_points = []\n    for i in line:\n        ecdf_points.append(ecdf(i))\n    real_values = []\n    if dist == \'Normal\':\n        real_values = st.norm.cdf(line)\n    elif dist == \'Cauchy\':\n        real_values = st.cauchy.cdf(line)\n    plt.subplot(211)\n    plt.title(\'Blue: ECDF | Black: CDF \' + dist)\n    plt.plot(line, ecdf_points)\n    plt.plot(line, real_values, color=\'black\')\n\n    # Creating the confidence band\n    epsilon = math.sqrt((1 / (2 * 100) * math.log10(2 / 0.05)))\n    lower_band_points = []\n    upper_band_points = []\n    for x in line:\n        lower_band_points.append(max(ecdf(x) - epsilon, 0))\n    for x in line:\n        upper_band_points.append(min(ecdf(x) + epsilon, 1))\n    plt.subplot(212)\n    plt.title(\'Red: Lower CB | Green: Upper CB\')\n    plt.plot(line, lower_band_points, color=\'red\')\n    plt.plot(line, upper_band_points, color=\'green\')\n    plt.plot(line, real_values, color=\'black\')\n    plt.show()\n\n    # Computing how many times the CB contains the true value\n    count = 0\n    for i in range(0, len(line)):\n        if lower_band_points[i] <= real_values[i] <= upper_band_points[i]:\n            pass\n        else:\n            count += 1\n    print(count, \'times Confidence Band does not contain the true value from \', len(line), \'points\')\n\n\n# Simulating confidence band\ndef main():\n    CDF_band(100, \'Normal\')\n    CDF_band(100, \'Cauchy\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
addtional_examples/cdf_pdf.py,2,"b'""""""CE1_Codes, 11/15/16, Sajad Azami""""""\n\nimport math\n\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom scipy.stats import norm\n\nsns.set_style(""whitegrid"")\n\n__author__ = \'sajjadaazami@gmail.com (Sajad Azami)\'\n\n\ndef normal_dist(x): (1 / math.sqrt(math.pi * 2 * 1)) * math.exp(-((x - 0) ** 2) / 2 * 1)\n\n\n# X = N(5,18), Finding the PDF and CDF and some probability\ndef simulate():\n    line = np.linspace(-100, 100, 201)\n    X = norm.pdf(line, loc=5, scale=18)\n\n    # Plotting the PDF and CDF of N(5,18) over the range of (-100, 100)\n    pyplot.subplot(211)\n    pyplot.plot(line, X)\n    pyplot.title(\'PDF\')\n    CDF = np.cumsum(X)\n    pyplot.subplot(212)\n    pyplot.title(\'CDF\')\n    pyplot.plot(line, CDF)\n    pyplot.show()\n\n    # 1. P(X<8)\n    print(\'P(X<8): \', norm.cdf(8, loc=5, scale=18))\n\n    # 2. P(X>-2)\n    print(\'P(X>-2): \', 1 - norm.cdf(-2, loc=5, scale=18))\n\n    # 3. x such that P(X>x) = 0.05\n    print(\'x such that P(X>x) = 0.05: \', norm.ppf(0.95, loc=5, scale=18))\n\n    # 4. P(0<=X<4)\n    print(\'P(0<=X<4: \', norm.cdf(4, loc=5, scale=18) - norm.cdf(0, loc=5, scale=18))\n\n    # 5. x such that P(abs(X) > abs(x)) = 0.05\n    print(\'x such that P(abs(X) > abs(x)) = 0.05: \', norm.ppf(0.975, loc=5, scale=18))\n\n\ndef main():\n    simulate()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
addtional_examples/simple_WLLN.py,4,"b'""""""CE1_Codes, 11/15/16, Sajad Azami""""""\n\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot\n\n__author__ = \'sajjadaazami@gmail.com (Sajad Azami)\'\n\nsns.set_style(""whitegrid"")\n\n\n# Let Xi be random variables of N(0,1) and let Xn = 1/n(sum(Xi))\n# We will plot Xn versus n for n = 1,...,10000\n# We will do this for Xi being random variables of Cauchy\ndef simulate():\n    # Xi be random variables of N(0,1)\n    n = []\n    Xn = []\n    for i in range(1, 10001):\n        X_i = np.random.normal(0, 1, i)\n        Xn.append(1 / i * (np.sum(X_i)))\n        n.append(i)\n    pyplot.subplot(211)\n    pyplot.title(\'Normal\')\n    pyplot.plot(n, Xn)\n\n    n = []\n    Xn = []\n    for i in range(1, 10001):\n        X_i = np.random.standard_cauchy()\n        Xn.append(1 / i * (np.sum(X_i)))\n        n.append(i)\n    pyplot.subplot(212)\n    pyplot.title(\'Cauchy\')\n    pyplot.plot(n, Xn)\n    pyplot.show()\n\n\ndef main():\n    simulate()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
addtional_examples/PGM/data_preparation.py,0,"b'""""""Linear Regression, 1/21/17, Sajad Azami""""""\n\nimport pandas as pd\n\n__author__ = \'sajjadaazami@gmail.com (Sajad Azami)\'\n\n\n# Reads train data from csv, returns pandas DF\ndef read_data(path):\n    data = pd.read_csv(path, header=None, index_col=False)\n    return data\n\n\n# Reads train data from csv, returns pandas DF\ndef split_label(df, label_index):\n    label = df.get(label_index)\n    data = df.drop(label_index, axis=1)\n    return data, label\n'"
addtional_examples/PGM/main.py,3,"b'""""""PGM, 2/1/17, Sajad Azami""""""\n\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import gridspec\nimport matplotlib.pyplot as plt\nimport random\nimport pandas as pd\nimport data_preparation\nfrom sklearn.naive_bayes import MultinomialNB\nfrom mpl_toolkits.mplot3d import Axes3D\nimport warnings\nfrom pgmpy.models import BayesianModel\nfrom pgmpy.factors.discrete import TabularCPD\nfrom pgmpy.estimators import MaximumLikelihoodEstimator, BayesianEstimator\nfrom pgmpy.inference import ExactInference\n\n__author__ = \'sajjadaazami@gmail.com (Sajad Azami)\'\n\nsns.set_style(""whitegrid"")\n\n\ndef warn(*args, **kwargs):\n    pass\n\n\nwarnings.warn = warn\n\n\n# Calculate Leave One Out Cross Validation Error\ndef get_LOOCV(features_to_use, train_label):\n    RSS_list = []\n    for i in range(0, features_to_use.shape[1]):\n        temp_data = features_to_use.drop(i, axis=0)\n        temp_label = np.delete(train_label.reshape(len(train_label), 1), i, 0)\n        mnnb = MultinomialNB()\n        mnnb.fit(temp_data, temp_label)\n        pred = mnnb.predict(features_to_use.iloc[[i]])\n        RSS_list.append(sum((train_label[i] - pred) ** 2))\n    LOOCV = sum(RSS_list) / len(RSS_list)\n    return LOOCV\n\n\n# Learn naive bayes model from feature set of feature_list\ndef naive_bayes_with_some_features(all_city_data, all_city_label, feature_list):\n    all_city_label = all_city_label.reshape(len(all_city_label), )\n    features_to_use = all_city_data.loc[:, feature_list]\n    mnnb = MultinomialNB()\n    mnnb.fit(features_to_use, all_city_label)\n    pred = mnnb.predict(features_to_use)\n    print(""Number of mislabeled points out of a total "" + str(features_to_use.shape[0]) + \' points: \' + (\n        str((all_city_label != pred).sum())))\n    # LOOCV risk\n    print(\'Feature set: \' + str(feature_list) + \'\\nLOOCV: \' + str(get_LOOCV(features_to_use, all_city_label)))\n    print(\'\')\n    return mnnb\n\n\n# Loading dataset\ncleveland = data_preparation.read_data(\'./data_set/processed.cleveland.data.txt\')\nhungarian = data_preparation.read_data(\'./data_set/processed.hungarian.data.txt\')\nswitzerland = data_preparation.read_data(\'./data_set/processed.switzerland.data.txt\')\nva = data_preparation.read_data(\'./data_set/processed.va.data.txt\')\nprint(\'Data set Loaded!\')\n\n# Merge datasets\nframes = [cleveland, hungarian, switzerland, va]\nall_city_data = pd.concat(frames)\n\n# Splitting label and features\nall_city_data, all_city_label = data_preparation.split_label(all_city_data, 13)\nall_city_label = all_city_label.reshape(len(all_city_label), 1)\nall_city_data = all_city_data.reset_index(drop=True)\n\n# Filling missing values with each columns mean for column [0, 3, 4, 7, 9] and mode for the rest\nall_city_data = all_city_data.replace(\'?\', -10)\nall_city_data = all_city_data.astype(np.float)\nall_city_data = all_city_data.replace(-10, np.NaN)\n\nmeans = all_city_data.mean()\nmean_indices = [0, 3, 4, 7, 9]\nmode_indices = [1, 2, 5, 6, 8, 10, 11, 12]\nfor i in mean_indices:\n    all_city_data[i] = all_city_data[i].fillna(means[i])\nfor i in mode_indices:\n    all_city_data[i] = all_city_data[i].fillna(all_city_data[i].mode()[0])\n\n# Decreasing label classes from 5 to 2(0 or 1)\nfor i in range(0, len(all_city_label)):\n    if all_city_label[i] != 0:\n        all_city_label[i] = 1\n\n# Discretizing values of continuous columns [0, 3, 4, 7, 9]\nall_city_data[0] = pd.cut(all_city_data[0].values, 5,\n                          labels=[0, 1, 2, 3, 4]).astype(list)\nall_city_data[3] = pd.cut(all_city_data[3].values, 3,\n                          labels=[0, 1, 2]).astype(list)\nall_city_data[4] = pd.cut(all_city_data[4].values, 5,\n                          labels=[0, 1, 2, 3, 4]).astype(list)\nall_city_data[7] = pd.cut(all_city_data[7].values, 5,\n                          labels=[0, 1, 2, 3, 4]).astype(list)\nall_city_data[9] = pd.cut(all_city_data[9].values, 3,\n                          labels=[0, 1, 2]).astype(list)\n\n# Bar plot each feature vs label after filling missing values\nfig = plt.figure()\n\ngs = gridspec.GridSpec(3, 3)\ncounter = 0\n# Discrete values\nfor k in range(0, 3):\n    for j in range(0, 3):\n        if counter == 8:\n            break\n        ax_temp = fig.add_subplot(gs[k, j], projection=\'3d\')\n\n        x = all_city_data[mode_indices[counter]].values.reshape(len(all_city_data[mode_indices[counter]]), 1)\n        y = all_city_label\n        d = {}\n        for i in range(0, len(x)):\n            if (x[i][0], y[i][0]) in d.keys():\n                d[(x[i][0], y[i][0])] += 1\n            else:\n                d[(x[i][0], y[i][0])] = 0\n        x = []\n        y = []\n        z = []\n        for i in d.items():\n            x.append(i[0][0])\n            y.append(i[0][1])\n            z.append(i[1])\n        ax_temp.bar(x, z, zs=y, zdir=\'y\', alpha=0.6, color=\'r\' * 4)\n        ax_temp.set_xlabel(\'X\')\n        ax_temp.set_ylabel(\'Y\')\n        ax_temp.set_zlabel(\'Z\')\n        ax_temp.title.set_text((\'Feature \' + str(mode_indices[counter])))\n        counter += 1\nplt.show()\n\n# Continuous values\nfig = plt.figure()\ngs = gridspec.GridSpec(2, 3)\ncounter = 0\nfor k in range(0, 2):\n    for j in range(0, 3):\n        if counter == 5:\n            break\n        # print(counter)\n        ax_temp = fig.add_subplot(gs[k, j], projection=\'3d\')\n\n        x = all_city_data[mean_indices[counter]].values.reshape(len(all_city_data[mean_indices[counter]]), 1)\n        y = all_city_label\n        d = {}\n        for i in range(0, len(x)):\n            if (x[i][0], y[i][0]) in d.keys():\n                d[(x[i][0], y[i][0])] += 1\n            else:\n                d[(x[i][0], y[i][0])] = 0\n        x = []\n        y = []\n        z = []\n        for i in d.items():\n            x.append(i[0][0])\n            y.append(i[0][1])\n            z.append(i[1])\n        ax_temp.bar(x, z, zs=y, zdir=\'y\', alpha=0.6, color=\'r\' * 4)\n        ax_temp.set_xlabel(\'X\')\n        ax_temp.set_ylabel(\'Y\')\n        ax_temp.set_zlabel(\'Z\')\n        ax_temp.title.set_text((\'Feature \' + str(mean_indices[counter])))\n        counter += 1\nplt.show()\n\n# Learning naive bayes model from various subsets of data\nnaive_bayes_with_some_features(all_city_data, all_city_label, feature_list=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\nnaive_bayes_with_some_features(all_city_data, all_city_label, feature_list=[0, 1, 2])\nnaive_bayes_with_some_features(all_city_data, all_city_label, feature_list=[0, 1, 2, 4])\nnaive_bayes_with_some_features(all_city_data, all_city_label, feature_list=[0, 1, 2, 3, 4, 5])\n\n# Splitting train and test data for PGM model\ntemp_data = pd.concat([all_city_data, pd.DataFrame(all_city_label, columns=[13])], axis=1)\npgm_train_set = temp_data.loc[0:700]\npgm_test_set = temp_data.loc[700:]\nprint(pgm_train_set)\n\n\n# Implementing PGM model on data\n# Using these features: 0: (age) 1: (sex) 2: (cp)\npgm_model = BayesianModel()\npgm_model.add_nodes_from([0, 1, 2, 13])\npgm_model.add_edges_from([(1, 13)])\npgm_model.fit(pgm_train_set.loc[:, [0, 1, 2, 13]])\npgm_test_set = pgm_test_set.loc[:, [0, 1, 2, 13]].drop(13, axis=1)\nprint(pgm_test_set)\nprint(pgm_model.get_cpds(13))\n'"
addtional_examples/Linear Regression/part_one/__init__.py,0,"b'""""""Linear Regression, 1/30/17, Sajad Azami""""""\n\n__author__ = \'sajjadaazami@gmail.com (Sajad Azami)\'\n'"
addtional_examples/Linear Regression/part_one/data_preparation.py,0,"b'""""""Linear Regression, 1/21/17, Sajad Azami""""""\n\nimport pandas as pd\n\n__author__ = \'sajjadaazami@gmail.com (Sajad Azami)\'\n\n\n# Reads train data from csv, returns pandas DF\ndef read_data(path, label_index):\n    data = pd.read_csv(path, header=None)\n    label = data.get(label_index)\n    data = data.drop(label_index, axis=1)\n    return data, label\n'"
addtional_examples/Linear Regression/part_one/linear_regression.py,6,"b'""""""Linear Regression, 1/21/17, Sajad Azami""""""\n\nimport math\n\nimport seaborn as sns\nimport numpy as np\n\n__author__ = \'sajjadaazami@gmail.com (Sajad Azami)\'\nsns.set_style(""darkgrid"")\n\n\n# Likelihood function calculator\ndef get_log_likelihood(response_vector, xy):\n    N = len(response_vector)\n    sigma2 = (1 / (N - 2)) * sum((response_vector - xy) ** 2)\n    l = -N * (np.log(math.sqrt(sigma2))) - (1 / (2 * sigma2)) * sum((response_vector - xy) ** 2)\n    return l\n\n\n# Fits a line that minimizes Residual Sum of Errors\n# The line is Yi = B0 + B1*Xi\n# feature_vector(Xi) and response_vector(Yi) are numpy arrays\n# returns B0, B1, sigma2_hat(estimated variance of epsilon)\n# standard errors of B0 and B1, RSS and R2 Metrics\ndef rss_regressor(feature_vector, response_vector, feature_vector_test, response_vector_test):\n    Y_bar = sum(response_vector) / len(response_vector)\n    Y_bar_test = sum(response_vector_test) / len(response_vector_test)\n    X_bar = sum(feature_vector) / len(feature_vector)\n    B1_hat = sum((feature_vector - X_bar) * (response_vector - Y_bar)) / sum((feature_vector - X_bar) ** 2)\n    B0_hat = Y_bar - B1_hat * X_bar\n\n    RSS_train = sum((response_vector - (B0_hat + B1_hat * feature_vector)) ** 2)\n    RSS_test = sum((response_vector_test - (B0_hat + B1_hat * feature_vector_test)) ** 2)\n\n    sigma2_hat = 1 / (len(feature_vector) - 2) * RSS_train\n\n    S_x = (1 / len(feature_vector)) * sum((feature_vector - X_bar) ** 2)\n    standard_error_B1 = math.sqrt(sigma2_hat) / S_x * math.sqrt(len(feature_vector))\n    standard_error_B0 = standard_error_B1 * math.sqrt(sum(feature_vector ** 2) / len(feature_vector))\n\n    TSS_train = sum((response_vector - Y_bar) ** 2)\n    R2_train = 1 - (RSS_train / TSS_train)\n    TSS_test = sum((response_vector_test - Y_bar_test) ** 2)\n    R2_test = 1 - (RSS_test / TSS_test)\n    return B0_hat, B1_hat, sigma2_hat, standard_error_B0, standard_error_B1, RSS_train, R2_train, RSS_test, R2_test\n\n\n# Fits a line that minimizes Residual Sum of Errors\n# The line is Yi = B0 + B1*Xi\n# feature_vector(Xi) and response_vector(Yi) are numpy arrays\n# feature_vector is n*k matrix where k is number of co-variates\n# returns B0, B1, sigma2_hat(estimated variance of epsilon)\n# standard errors of B0 and B1, RSS and R2 Metrics\ndef multivariate_rss_regressor(feature_vector, response_vector, feature_vector_test, response_vector_test):\n    beta_hat_vector = np.dot(np.dot(np.linalg.inv(np.dot(feature_vector.T, feature_vector)), feature_vector.T),\n                             response_vector)\n    residual_vector = np.dot(feature_vector, beta_hat_vector) - response_vector\n    S = len(beta_hat_vector)\n\n    RSS_train = sum(residual_vector ** 2)\n    TSS_train = sum((response_vector - sum(response_vector) / len(response_vector)) ** 2)\n    R2_train = 1 - (RSS_train / TSS_train)\n\n    log_likelihood = get_log_likelihood(response_vector, np.dot(feature_vector, beta_hat_vector))\n    AIC = log_likelihood - S\n\n    return AIC, RSS_train, R2_train, beta_hat_vector\n\n\n# Calculates Leave One Out Cross Validation Error\ndef get_LOOCV(features_to_use, train_label):\n    RSS_list = []\n    for i in range(0, features_to_use.shape[1]):\n        temp_data = np.delete(features_to_use.T, i, 0)\n        temp_label = np.delete(train_label.reshape(len(train_label), 1), i, 0)\n        all_feature_model = multivariate_rss_regressor(temp_data, temp_label, None, None)\n        RSS_list.append(all_feature_model[1])\n    LOOCV = sum(RSS_list) / len(RSS_list)\n    return LOOCV\n\n\n# Gets points of fitted line for plotting purpose\ndef get_points(line, B0, B1):\n    points = []\n    for i in line:\n        points.append(B0 + B1 * i)\n    return points\n'"
addtional_examples/Linear Regression/part_one/main.py,6,"b'""""""Linear Regression, 1/21/17, Sajad Azami""""""\n\nimport part_one.data_preparation as data_preparation\nimport part_one.linear_regression as linear_regression\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import gridspec\nimport matplotlib.pyplot as plt\nimport random\n\n__author__ = \'sajjadaazami@gmail.com (Sajad Azami)\'\nsns.set_style(""darkgrid"")\n\ndata_set_1, label_1 = data_preparation.read_data(\'../data_set/Dataset1.csv\', 8)\nprint(\'Data set Loaded!\')\n\n# Split train and test data\ntrain_data_1 = data_set_1[:400]\ntrain_label_1 = label_1[:400]\ntest_data_1 = data_set_1[400:]\ntest_label_1 = label_1[400:]\n\nprint(\'Train data size:\', len(train_data_1))\nprint(\'Test data size:\', len(test_data_1))\n\n# Scatter plot each feature vs label\nfig = plt.figure()\ngs = gridspec.GridSpec(3, 3)\ncounter = 0\nfor i in range(0, 3):\n    for j in range(0, 3):\n        counter += 1\n        if counter == 9:\n            break\n        ax_temp = fig.add_subplot(gs[i, j])\n        ax_temp.scatter(train_data_1.get(counter - 1), train_label_1)\n        ax_temp.title.set_text((\'Feature \' + str(counter)))\nplt.show()\n\n# Finding Simple Linear Regression models for each feature with RSS metric\nlinear_regressions = []\nfor i in range(0, 8):\n    linear_regressions.append(\n        linear_regression.rss_regressor(train_data_1.get(i).values, train_label_1,\n                                        test_data_1.get(i), test_label_1))\n\n# Plotting Lines fitted with each feature\nfig = plt.figure()\ngs = gridspec.GridSpec(3, 3)\ncounter = 0\nfor i in range(0, 3):\n    for j in range(0, 3):\n        counter += 1\n        if counter == 9:\n            break\n        line = np.linspace(min(train_data_1.get(counter - 1)) - 3,\n                           max(train_data_1.get(counter - 1) + 3), 10000)\n        ax_temp = fig.add_subplot(gs[i, j])\n        ax_temp.scatter(train_data_1.get(counter - 1), train_label_1)\n        ax_temp.plot(line, linear_regression.get_points(line, linear_regressions[counter - 1][0],\n                                                        linear_regressions[counter - 1][1]))\n        ax_temp.title.set_text((\'Line with Feature \' + str(counter)))\nplt.show()\n\n# Reporting Linear Regression Characteristics for train and test Data\nfor i in range(0, len(linear_regressions)):\n    regression_temp = linear_regressions[i]\n    b0_hat = regression_temp[0]\n    b1_hat = regression_temp[1]\n    estimated_epsilon = regression_temp[2]\n    standard_error_b0 = regression_temp[3]\n    standard_error_b1 = regression_temp[4]\n    RSS_train = regression_temp[5]\n    R2_train = regression_temp[6]\n    RSS_test = regression_temp[7]\n    R2_test = regression_temp[8]\n    print(\'Simple Linear Regression with Feature\' + str(i + 1) +\n          \'\\nEstimated (Beta0, Beta1): (\' + str(b0_hat) + \', \' + str(b1_hat) + \')\\n\' +\n          \'Standard Error of Beta0 and Beta1: (\' + str(standard_error_b0) + \', \' + str(standard_error_b1) +\n          \')\\nEstimated Variance of Epsilon: \' + str(estimated_epsilon) + \'\\n\' +\n          \'RSS_train: \' + str(RSS_train) + str(\'\\n\') +\n          \'R2_train: \' + str(R2_train) + str(\'\\n\') +\n          \'RSS_test: \' + str(RSS_test) + str(\'\\n\') +\n          \'R2_test: \' + str(R2_test) + str(\'\\n\'))\n\n# Starting with Feature4(as the best feature) and adding features, then checking AIC, RSS and R2\ncurrent_AIC = linear_regression.get_log_likelihood(train_label_1, train_data_1.get(3)) - 1\nprint(\'Current AIC only using Feature4 : \' + str(current_AIC))\nused_features_indexes = [3]\nfeatures_to_use = train_data_1.get(3).reshape(1, len(train_data_1.get(3)))\ntest_features_to_use = test_data_1.get(3).reshape(1, len(test_data_1.get(3)))\nimproving = True\nwhile improving:\n    improvements = {}\n    features_to_use_temp = []\n    test_features_to_use_temp = []\n    for i in range(0, 8):\n        if i in used_features_indexes:\n            continue\n        else:\n            features_to_use_temp = np.concatenate((features_to_use, train_data_1.get(i).\n                                                   reshape(1, len(train_data_1.get(i)))),\n                                                  axis=0).T\n            test_features_to_use_temp = np.concatenate((test_features_to_use, test_data_1.get(i).\n                                                        reshape(1,\n                                                                len(test_data_1.get(i)))), axis=0).T\n            multiple_regression_result = \\\n                linear_regression.multivariate_rss_regressor(features_to_use_temp, train_label_1,\n                                                             test_features_to_use_temp,\n                                                             test_label_1)\n            print(\'AIC after adding Feature\' + str(i + 1) + \' :\' + str(multiple_regression_result[0]))\n            improvements[i] = (multiple_regression_result[0] - current_AIC)\n    if len(improvements) > 0 and improvements[max(improvements)] > 0:\n        best_index = max(improvements.keys(), key=(lambda k: improvements[k]))\n        used_features_indexes.append(best_index)\n        features_to_use = np.concatenate((features_to_use, train_data_1.get(best_index).\n                                          reshape(1, len(train_data_1.get(best_index)))),\n                                         axis=0)\n        print_temp = \'Model Uses Feature \'\n        for j in range(0, len(used_features_indexes)):\n            print_temp = print_temp + \'(\' + str(used_features_indexes[j] + 1) + \') \'\n        print(print_temp)\n        multiple_regression_result = \\\n            linear_regression.multivariate_rss_regressor(features_to_use.T, train_label_1, None, None)\n        print(\'RSS: \' + str(multiple_regression_result[1]) + \' and R2: \' + str(multiple_regression_result[2]) + \'\\n\')\n        improving = True\n    else:\n        improving = False\n\n# Evaluating the LOOCV metric for different model\nLOOCV = linear_regression.get_LOOCV(features_to_use, train_label_1)\nprint(\'Leave One Out Cross Validation Risk for \' + str(features_to_use.shape[0]) + \' Features is: \' +\n      str(LOOCV))\ntemp_features = features_to_use\nloocvs = [LOOCV]\n\n# Performing the backward method\nfor i in range(0, 7):\n    temp_features = np.delete(temp_features, 0, 0)\n    LOOCV = linear_regression.get_LOOCV(temp_features, train_label_1)\n    loocvs.append(LOOCV)\n    print(\'Leave One Out Cross Validation Risk for \' + str(temp_features.shape[0]) + \' Features is: \' +\n          str(LOOCV))\nplt.plot(np.linspace(1, 8, 8), loocvs)\nplt.title(\'RSS vs Number of Used Features\')\nplt.show()\n\n# Testing Full-Feature model with different number of train data\nfeatures_count = [50, 100, 250, 300, 350, 400]\nRSS = []\nfor i in features_count:\n    indices = random.sample(range(0, 400), i)\n    temp_data = features_to_use.T[indices].reshape(i, 8)\n    temp_label = train_label_1[indices]\n    RSS.append(linear_regression.multivariate_rss_regressor(\n        temp_data, temp_label, None, None)[1])\nplt.plot(features_count, RSS)\nplt.title(\'Train RSS vs Number of Train\')\nplt.show()\nprint(RSS)\n'"
addtional_examples/Linear Regression/part_two/__init__.py,0,"b'""""""Linear Regression, 1/30/17, Sajad Azami""""""\n\n__author__ = \'sajjadaazami@gmail.com (Sajad Azami)\'\n'"
addtional_examples/Linear Regression/part_two/main.py,7,"b'""""""Linear Regression, 1/30/17, Sajad Azami""""""\n\nimport part_one.data_preparation as data_preparation\nimport part_one.linear_regression as linear_regression\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import gridspec\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Lasso\nimport pandas as pd\n\n__author__ = \'sajjadaazami@gmail.com (Sajad Azami)\'\nsns.set_style(""darkgrid"")\n\ndata_set_1, label_1 = data_preparation.read_data(\'../data_set/Dataset2.csv\', 6)\nprint(\'Data set Loaded!\')\n\n# Split train and test data\ntrain_data_1 = data_set_1[:200]\ntrain_label_1 = label_1[:200]\ntest_data_1 = data_set_1[200:]\ntest_label_1 = label_1[200:]\n\nprint(\'Train data size:\', len(train_data_1))\nprint(\'Test data size:\', len(test_data_1))\n\n# Scatter plot each feature vs label\nfig = plt.figure()\ngs = gridspec.GridSpec(2, 3)\ncounter = 0\nfor i in range(0, 2):\n    for j in range(0, 3):\n        counter += 1\n        ax_temp = fig.add_subplot(gs[i, j])\n        ax_temp.scatter(train_data_1.get(counter - 1), train_label_1)\n        ax_temp.title.set_text((\'Feature \' + str(counter)))\nplt.show()\n\n# Filling missing values with Gaussian Noise, N(mean_of_row, 1)\nfor i in range(0, train_data_1.shape[0]):\n    row = train_data_1.values[i]\n    for j in range(0, len(row)):\n        if row[j] == 0:\n            row[j] = abs(np.random.normal(sum(row) / len(row), scale=1))\n# Filling missing values of test data with the same way\nfor i in range(0, test_data_1.shape[0]):\n    test_row = test_data_1.values[i]\n    if test_row[j] == 0:\n        test_row[j] = abs(np.random.normal(sum(test_row) / len(test_row), scale=1))\n\n# Scatter plot each feature vs label after filling missing values\nfig = plt.figure()\ngs = gridspec.GridSpec(2, 3)\ncounter = 0\nfor i in range(0, 2):\n    for j in range(0, 3):\n        counter += 1\n        ax_temp = fig.add_subplot(gs[i, j])\n        ax_temp.scatter(train_data_1.get(counter - 1), train_label_1)\n        ax_temp.title.set_text((\'Feature \' + str(counter)))\nplt.show()\n\n# Using Lasso Regression on Data\n# Optimization Objective: (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\nalpha = 0.000001\nlasso_model = Lasso(alpha=alpha)\nlasso_line = lasso_model.fit(train_data_1, train_label_1)\ntitle = \'Alpha = \' + str(alpha) + \'\\nRed: Lasso Prediction, Blue: Real Values\'\nplt.plot(np.linspace(0, 40, 40), lasso_line.predict(test_data_1), \'r\')\nplt.plot(np.linspace(0, 40, 40), test_label_1)\nplt.title(title)\nplt.show()\n\n# Testing for different alphas\nalphas = [0.0001, 0.001, 0.01, 0.1, 1, 10]\ncounter = 0\nfig = plt.figure()\ngs = gridspec.GridSpec(2, 3)\nfor i in range(0, 2):\n    for j in range(0, 3):\n        lasso_model = Lasso(alpha=alphas[counter])\n        lasso_line = lasso_model.fit(train_data_1, train_label_1)\n        predictions = lasso_line.predict(test_data_1)\n        if counter == 0:\n            RSS = sum((test_label_1 - predictions) ** 2)\n            TSS = sum((test_label_1 - (sum(test_label_1) / len(test_label_1))) ** 2)\n            R2 = 1 - (RSS / TSS)\n            print(\'RSS: \' + str(RSS) + \', R2: \' + str(R2))\n        title = \'Alpha = \' + str(alphas[counter])\n        ax_temp = fig.add_subplot(gs[i, j])\n        ax_temp.plot(np.linspace(0, 40, 40), predictions, \'r\')\n        ax_temp.plot(np.linspace(0, 40, 40), test_label_1)\n        ax_temp.title.set_text(title)\n        counter += 1\nplt.show()\n\n# Predicting the final submission file with the best model\n# Filling missing values with Gaussian Noise, N(mean_of_row, 1)\nfinal_data = pd.read_csv(\'../data_set/Dataset2_Unlabeled.csv\', header=None)\nfor i in range(0, final_data.shape[0]):\n    row = final_data.values[i]\n    for j in range(0, len(row)):\n        if row[j] == 0:\n            row[j] = abs(np.random.normal(sum(row) / len(row), scale=1))\nlasso_model = Lasso(alpha=0.0001)\nlasso_line = lasso_model.fit(train_data_1, train_label_1)\npredictions = lasso_line.predict(final_data)\npredictions.tofile(\'predictions.csv\', sep=\',\\n\')\n'"
