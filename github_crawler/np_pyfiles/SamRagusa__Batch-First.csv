file_path,api_count,code
code_testing.py,54,"b'import tensorflow as tf\n\nimport random\nimport chess.pgn\nimport chess.uci\nimport os\nimport tempfile\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nfrom batch_first.anns.ann_creation_helper import parse_into_ann_input_inference\n\nfrom batch_first.anns.database_creator import get_data_from_pgns, create_board_eval_board_from_game_fn, \\\n    combine_pickles_and_create_tfrecords, serializer_creator\n\nfrom batch_first.classes_and_structs import *\n\nfrom batch_first.numba_board import  perft_test, is_legal_move, numpy_node_info_dtype, push_moves, set_up_move_array, \\\n    popcount, has_insufficient_material, has_legal_move, piece_type_at, scan_reversed\n\nfrom batch_first.numba_negamax_zero_window import set_up_root_node_for_struct, struct_array_to_ann_inputs, \\\n    zero_window_negamax_search\n\nfrom batch_first.transposition_table import get_empty_hash_table\n\nfrom batch_first.global_open_priority_nodes import PriorityBins\n\n\n\n#These fens come from the ChessProgramming Wiki, and can be found here: https://www.chessprogramming.org/Perft_Results\nDEFAULT_TESTING_FENS = [""rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"",\n                        ""r3k2r/p1ppqpb1/bn2pnp1/3PN3/1p2P3/2N2Q1p/PPPBBPPP/R3K2R w KQkq - 0 1"",\n                        ""8/2p5/3p4/KP5r/1R3p1k/8/4P1P1/8 w - - 0 1"",\n                        ""r3k2r/Pppp1ppp/1b3nbN/nP6/BBP1P3/q4N2/Pp1P2PP/R2Q1RK1 w kq - 0 1"", #In check\n                        ""r2q1rk1/pP1p2pp/Q4n2/bbp1p3/Np6/1B3NBn/pPPP1PPP/R3K2R b KQ - 0 1"", #Mirrored version of above\n                        ""rnbq1k1r/pp1Pbppp/2p5/8/2B5/8/PPP1NnPP/RNBQK2R w KQ - 1 8 "",\n                        ""r4rk1/1pp1qppp/p1np1n2/2b1p1B1/2B1P1b1/P1NP1N2/1PP1QPPP/R4RK1 w - - 0 10""]\n\n\n#These values come from the ChessProgramming Wiki, and can be found here: https://www.chessprogramming.org/Perft_Results\nDEFAULT_FEN_PERFT_RESULTS = [[20,400,8902,197281,4865609,119060324,3195901860,849989789566,2439530234167,693552859712417],\n                             [48,2039,97862,4085603,193690690],\n                             [14,191,2812,43238,674624,11030083, 178633661],\n                             [6,264,9467,422333,15833292,706045033],\n                             [6,264,9467,422333,15833292,706045033],\n                             [44,1486,62379,2103487,89941194],\n                             [46,2079,89890,3894594,164075551,6923051137,287188994746,11923589843526,490154852788714]]\n\n\n# This array represents moves, some illegal or impossible, for use in testing.  It contains all possible combinations\n# of from squares, to squares, and promotion pieces.  The array is of the form:\n# [..., [move_from_square, move_to_square, move_promotion], ...]\nDEFAULT_MOVES_TO_CHECK = np.array(\n    [[squares[0][0], squares[0][1], squares[1]] for squares in itertools.product(\n        itertools.permutations(chess.SQUARES,r=2),\n        [0] + [piece_type for piece_type in chess.PIECE_TYPES if not piece_type in [chess.KING, chess.PAWN]])],\n    dtype=np.uint8)\n\n\n\n\ndef full_perft_tester(perft_function_to_test, fens_to_test=DEFAULT_TESTING_FENS,\n                      perft_results=DEFAULT_FEN_PERFT_RESULTS, max_expected_boards_to_test=5000000):\n    """"""\n    Tests a given PERFT function by checking it\'s results against known results for several different boards and depths.\n\n\n    :param perft_function_to_test: A function that takes 2 arguments, the first being a board\'s FEN representation\n     as a string, and the second being the depth of the PERFT test to be done.\n    :param fens_to_test: An iterable of strings, each a FEN representation of a board.\n    :param perft_results: A list of the same length as fens_to_test, containing lists of integers, each corresponding\n     to the expected PERFT result if tested at a depth of it\'s index\n    :param max_expected_boards_to_test: The maximum expected PERFT result to test for a given board\n    :return: True if all tests were passed, False if not\n    """"""\n    for cur_fen, cur_results in zip(fens_to_test, perft_results):\n        for j, expected_result in enumerate(cur_results):\n            if expected_result <= max_expected_boards_to_test:\n                if perft_function_to_test(cur_fen, j+1) != expected_result:\n                    return False\n    return True\n\n\ndef zobrist_hash_test(hash_getter, fen_to_start=None, num_sequences_to_test=1000, max_moves_per_test=20):\n    """"""\n    This functions tests the engine\'s ability to incrementally maintain a board\'s Zobrist hash while pushing\n    moves.  It tests this by generating a set of random move sequences from a given board, and compares the computed\n    results with the values computed by the functions within the python-chess package.\n\n\n    :param hash_getter: The hashing function to test, it must accept 3 parameters, the fen to start the\n     move sequences from, the list of lists of python-chess Moves to make from the starting board, and the maximum\n     number of moves per test. The function must return an ndarray of the board hashes at each position in the random\n     move sequences (zero if the sequence terminates early), it\'s shape will be\n     [num_sequences_to_test, max_moves_per_test]\n    :param fen_to_start: The fen (as a string) representing the board to start making random moves from.  If None is\n     given, the fen for the start of a normal game is used\n    :param num_sequences_to_test: The number of random move sequences (each starting from the given initial fen) to test\n    :param max_moves_per_test: The maximum number of random moves to be made for each testing sequence.  This is a\n     maximum because some random move sequences result in a premature win/loss/draw\n    :return: True if all tests were passed, False if not\n    """"""\n    if fen_to_start is None:\n        fen_to_start = DEFAULT_TESTING_FENS[0]\n\n    correct_hashes = np.zeros((num_sequences_to_test, max_moves_per_test), dtype=np.uint64)\n    move_lists = [[] for _ in range(num_sequences_to_test)]\n    for j in range(num_sequences_to_test):\n        cur_board = chess.Board(fen_to_start)\n        for i in range(max_moves_per_test):\n            possible_next_moves = list(cur_board.generate_legal_moves())\n            if len(possible_next_moves) == 0:\n                break\n            move_lists[j].append(possible_next_moves[random.randrange(len(possible_next_moves))])\n            cur_board.push(move_lists[j][-1])\n\n            correct_hashes[j,i] = zobrist_hash(cur_board)\n\n    #Go through incorrect hashes and print relevant information about them for use during debugging\n    calculated_hashes = hash_getter(fen_to_start, move_lists, max_moves_per_test)\n    same_hashes = calculated_hashes == correct_hashes\n    if not np.all(same_hashes):\n        for j in range(len(same_hashes)):\n            if np.sum(same_hashes[j]) != 0:\n                cur_board = chess.Board(fen_to_start)\n                for i,move in enumerate(move_lists[j]):\n                    if not same_hashes[j,i]:\n                        print(""Board and move being made which caused the first incorrect hash in sequence %d:\\n%s\\n%s\\n%s\\nDifference in hash values:%d\\n"" % (\n                            j, cur_board, cur_board.fen(), move, correct_hashes[j, i] ^ calculated_hashes[j, i]))\n                        break\n\n                    cur_board.push(move)\n\n    return np.all(same_hashes)\n\n\ndef get_expected_features(boards, piece_to_filter_fn, ep_filter_index, castling_filter_indices):\n    """"""\n    Gets an array of feature indices for the given chess boards.\n\n\n    :param boards: A list of of python-chess Board objects.  These are the boards the expected features\n     will be computed for\n    :param piece_to_filter_fn: A function taking a python-chess Piece as input, and returning the index of the input\n     filter used to identify that piece (incremented by one if unoccupied is not used as a filter)\n    :param ep_filter_index: The index of the input filter used to identify if a square is an ep square,\n     this is incremented by one if unoccupied is not used as a filter\n    :param castling_filter_indices: The index of the input filter used to identify if a square contains a rook which\n     has castling rights, this is incremented by one if unoccupied is not used as a filter\n    :return: An ndarray containing the given boards features, it will have size [len(boards),8,8] and dtype np.uint8\n    """"""\n    CORNER_SQUARES = np.array([A1, H1, A8, H8], np.uint8)\n    CORNER_BBS = np.array([BB_A1, BB_H1, BB_A8, BB_H8], np.uint64)\n\n    desired_filters = np.empty([len(boards), 8, 8], dtype=np.uint8)\n\n    filter_getters = [\n        lambda p: piece_to_filter_fn(chess.Piece(p.piece_type, not p.color) if p else None),\n        piece_to_filter_fn]\n\n    for j in range(len(boards)):\n        piece_map = map(boards[j].piece_at, chess.SQUARES)\n\n        cur_filter_squares = np.array(list(map(filter_getters[boards[j].turn], piece_map)))\n\n        if not boards[j].ep_square is None:\n            cur_filter_squares[boards[j].ep_square] = ep_filter_index\n\n        corner_filter_nums = castling_filter_indices[2 * [1 ^ boards[j].turn] + 2 * [boards[j].turn]] #indices are [0,0,1,1] if white\'s turn else [1,1,0,0]\n\n        castling_mask = np.array(np.uint64(boards[j].castling_rights) & CORNER_BBS,dtype=np.bool_)\n        cur_filter_squares[CORNER_SQUARES[castling_mask]] =  corner_filter_nums[castling_mask]\n\n        desired_filters[j] = cur_filter_squares.reshape((8,8))\n\n        if not boards[j].turn:\n            desired_filters[j] = desired_filters[j,::-1]\n\n    return desired_filters\n\n\ndef inference_input_pipeline_test(inference_pipe_fn, boards_to_input_list_fn, boards, uses_unoccupied,\n                                  piece_to_filter_fn, ep_filter_index, castling_filter_indices, num_splits):\n    """"""\n    A function used to test the conversion of a board to a representation consumed by ANNs during inference.\n    It creates an array of features (one-hot inputs potentially with empty squares) by using the\n    python-chess package, and confirms that the given pipeline produces matching inputs.\n\n\n    :param inference_pipe_fn: A function with no parameters, returning a size 2 tuple, with the first element being\n     a tuple of Placeholders to feed values to, and the second being the tensor for the one-hot representation\n     of the boards to be given to the ANN for inference\n    :param boards_to_input_list_fn: A function taking as it\'s input a list of python-chess Board objects, and returning\n     an iterable of arrays to be fed to the TensorFlow part of the pipe to test\n    :param boards: A list of of python-chess Board objects\n    :param uses_unoccupied: A boolean value, indicating if the input to the ANNs should have a filter for\n     unoccupied squares\n    :param piece_to_filter_fn: A function taking a python-chess Piece as input, and returning the index of the input\n     filter used to identify that piece (incremented by one if unoccupied is not used as a filter)\n    :param ep_filter_index: The index of the input filter used to identify if a square is an ep square,\n     this is incremented by one if unoccupied is not used as a filter\n    :param castling_filter_indices:  The index of the input filter used to identify if a square contains a rook which\n     has castling rights, this is incremented by one if unoccupied is not used as a filter\n    :param num_splits: The number of times to split the array of test boards before giving it to the pipe\n     (which is done to ensure that errors which don\'t occur every run are still properly tested)\n    :return: A size 3 tuple, the first element being a boolean value signifying if the test was passed,\n     the second element being the expected filters, and the third being the filters computed by the inference pipeline\n    """"""\n\n    desired_filters = get_expected_features(boards, piece_to_filter_fn, ep_filter_index, castling_filter_indices)\n\n    #The TensorFlow code is written such that specific values can easily be pulled during debugging\n    with tf.Session() as sess:\n        pipe_placeholders, pipe_output = inference_pipe_fn()\n\n        desired_square_values = tf.placeholder(tf.uint8, [None,8,8])\n\n        # Using absolute value so that an array such as [1,0,-1,1] isn\'t perceived as a one-hot vector\n        abs_filter_sums = tf.reduce_sum(tf.abs(pipe_output), axis=3)\n\n        squares_with_incorrect_sums = tf.logical_and(tf.not_equal(abs_filter_sums,0), tf.not_equal(abs_filter_sums, 1))\n\n        board_has_square_with_incorrect_filter_sum = tf.reduce_any(squares_with_incorrect_sums, axis=[1, 2])\n\n        if uses_unoccupied:\n            onehot_data = pipe_output\n        else:\n            onehot_data = tf.concat([tf.expand_dims(1-abs_filter_sums,axis=3), pipe_output], axis=3)\n\n\n        filters_used = tf.cast(tf.argmax(onehot_data,axis=3),tf.uint8)\n\n        wrong_filter_squares = tf.not_equal(desired_square_values, filters_used)\n\n        board_has_wrong_filter = tf.reduce_any(wrong_filter_squares, axis=[1,2])\n\n        boards_with_issues =  tf.logical_or(board_has_square_with_incorrect_filter_sum, board_has_wrong_filter)\n\n        run_results = []\n        split_indices = np.r_[np.arange(0, len(boards), len(boards) // num_splits), len(boards)]\n        for start, end in zip(split_indices[:-1], split_indices[1:]):\n            run_results.append(\n                sess.run(\n                    [filters_used, boards_with_issues],\n                    {\n                        desired_square_values: desired_filters[start:end],\n                        **dict(zip(pipe_placeholders, boards_to_input_list_fn(boards[start:end]))),\n                    }))\n\n        calculated_filters, problemed_board_mask = zip(*run_results)\n        calculated_filters = np.concatenate(calculated_filters)\n        problemed_board_mask = np.concatenate(problemed_board_mask)\n\n        for j in range(len(problemed_board_mask)):\n            if problemed_board_mask[j]:\n                print(""Inference pipeline test failed for board number:"", j)\n                print(boards[j].fen())\n                print(boards[j], ""\\n-Desired filters:\\n"", desired_filters[j],\n                      ""\\n-Computed filters:\\n"", calculated_filters[j], ""\\n\\n\\n"")\n\n        return not np.any(problemed_board_mask), desired_filters, calculated_filters\n\n\ndef move_verification_tester(move_legality_tester, board_creator_fn, fens_to_test=DEFAULT_TESTING_FENS,\n                             moves_to_test=DEFAULT_MOVES_TO_CHECK):\n    """"""\n    A function to test a method of move legality verification.  This is used to confirm that the move verification done\n    for moves stored in the transposition table is correct.  It uses the python-chess package to compute the correct\n    legality of moves, and compares against that.\n\n\n    :param move_legality_tester: A function accepting two parameters, the first being a board as returned by the given\n     board_creator_fn, and the second being a size 3 ndarray representing a move. The function must return a\n     boolean value, indicating if the move is legal\n    :param board_creator_fn: A function which takes as input a Python-Chess Board object, and outputs the board in the\n     representation to be used when testing move legality.\n    :param fens_to_test: An iterable of strings, each a FEN representation of a board.\n    :param moves_to_test: A uint8 ndarray of size [num_moves_to_test, 3], representing the moves to test for each\n     testing fen\n    :return: True if all tests were passed, False if not\n    """"""\n    for cur_fen in fens_to_test:\n        cur_board = chess.Board(cur_fen)\n\n        cur_testing_board = board_creator_fn(cur_board)\n        for j in range(len(moves_to_test)):\n            if move_legality_tester(cur_testing_board, moves_to_test[j]) != cur_board.is_legal(\n                    chess.Move(*moves_to_test[j]) if moves_to_test[j, 2] != 0 else chess.Move(*moves_to_test[j, :2])):\n                return False\n\n    return True\n\n\ndef generate_boards_for_testing(sf_location, num_random_games=250, max_moves_per_game=1000, sf_move_time=1):\n    stockfish_ai = chess.uci.popen_engine(sf_location)\n\n    for j in range(num_random_games):\n        cur_board = chess.Board()\n        random_turn = True\n        for _ in range(max_moves_per_game):\n            if random_turn:\n                possible_next_moves = list(cur_board.generate_legal_moves())\n                chosen_move = possible_next_moves[random.randrange(len(possible_next_moves))]\n            else:\n                stockfish_ai.position(cur_board)\n                chosen_move = stockfish_ai.go(movetime=sf_move_time).bestmove\n\n            cur_board.push(chosen_move)\n\n            yield cur_board\n\n            if cur_board.is_game_over():\n                break\n\n            random_turn = not random_turn\n\n\ndef check_if_legal_move_exists_tester(has_legal_move_fn, existing_high_level_uci_filename, boards_to_test=None):\n    if boards_to_test is None:\n        boards_to_test = generate_boards_for_testing(existing_high_level_uci_filename)\n\n    for board in boards_to_test:\n        if has_legal_move_fn(board) != any(board.generate_legal_moves()):\n            print(board)\n            print(board.fen())\n            print(""Calculated value:"", has_legal_move_fn(board))\n            print(""Actual moves:"", list(board.generate_legal_moves()))\n            return False\n\n    return True\n\n\ndef complete_board_eval_tester(tfrecords_writer, feature_getter, inference_pipe_fn, boards_to_input_for_inference,\n                               piece_to_filter_fn, ep_filter_index, castling_filter_indices, num_splits=250,\n                               num_random_games=25, max_moves_per_game=250, uses_unoccupied=False):\n\n    """"""\n    This function can be thought of in two parts, one tests the board evaluation inference pipeline, and the other\n     tests the entire board evaluation training pipeline (from PGN to Tensor).\n\n    It works by first playing a number of chess games, choosing moves randomly and storing each board configuration.\n     It then uses inference_input_pipeline_test to both test the inference pipeline, and produce the expected\n     feature arrays for the training pipeline.  A temporary pgn file is created from the games played,\n     and sent through the pipeline used to create and consume the board evaluation database.  If every feature array\n     produced by the training pipeline is contained within the expected feature arrays,\n     then the training pipeline passes the test.\n\n\n\n    :param tfrecords_writer: A function to write a tfrecords file in the same way done when creating the board\n     evaluation ANN training data.  It must accept two parameters, the first is the filename of the pgn file,\n     and the second the desired output tfrecords filename.  The function doesn\'t need to return anything\n    :param feature_getter: A function accepting the filename of the tfrecords file created for testing,\n     and returning the tensor for the one-hot representation of the boards to be given to the ANN\n     for training (unoccupied filter may be omitted if not used)\n    :param inference_pipe_fn: See inference_input_pipeline_test\n    :param boards_to_input_for_inference: See inference_input_pipeline_test\n    :param piece_to_filter_fn: See inference_input_pipeline_test\n    :param ep_filter_index: See inference_input_pipeline_test\n    :param castling_filter_indices: See inference_input_pipeline_test\n    :param num_splits: See inference_input_pipeline_test\n    :param num_random_games: The number of chess games to be played/tested (choosing random moves).  This is used for\n     creating the PGN file to test the training pipe, or in generating the list of boards to test the inference pipe\n    :param max_moves_per_game: The maximum number of moves (halfmoves) to be made before a game should be stopped.\n     If None is given, a limit of 250 will be used\n    :param uses_unoccupied: See inference_input_pipeline_test\n    :return: A tuple of two boolean values, the first indicating if the training pipeline tests were passed,\n     and the second indicating if the inference pipeline tests were passed\n\n\n    NOTES:\n    1) Ideally a temporary file would be used for writing/reading the tfrecords database\n    """"""\n    tf_records_filename = ""THIS_COULD-SHOULD_BE_A_TEMPORARY_FILE.tfrecords""\n\n    temp_pgn, temp_filename = tempfile.mkstemp()\n\n    boards = [chess.Board()]\n    for j in range(num_random_games):\n\n        cur_board = chess.Board()\n        for _ in range(max_moves_per_game):\n            possible_next_moves = list(cur_board.generate_legal_moves())\n\n            cur_board.push(possible_next_moves[random.randrange(len(possible_next_moves))])\n\n            boards.append(cur_board.copy())\n\n            if cur_board.is_game_over():\n                break\n\n        #Writing the game to the pgn file (doing multiple times since only one board is picked from each game)\n        for j in range(5):\n            os.write(temp_pgn, str.encode(str(chess.pgn.Game.from_board(cur_board))))\n\n    #Create the tfrecords file as it would be created for training\n    tfrecords_writer(temp_filename, tf_records_filename)\n\n    os.close(temp_pgn)\n\n\n    input_features = feature_getter(tf_records_filename)\n\n    one_hot_features = np.argmax(\n        np.concatenate((np.expand_dims(1-np.sum(input_features, axis=3),axis=3), input_features), axis=3),axis=3)\n\n    inference_pipe_results, desired_filters, inference_filters = inference_input_pipeline_test(\n        inference_pipe_fn,\n        boards_to_input_for_inference,\n        boards,\n        uses_unoccupied,\n        piece_to_filter_fn,\n        ep_filter_index=ep_filter_index,\n        castling_filter_indices=castling_filter_indices,\n        num_splits=num_splits)\n\n    no_training_pipe_issues = True\n    for filter in one_hot_features:\n        num_correct_squares = np.sum(filter == desired_filters, (1,2))\n        if np.all(64 != num_correct_squares):\n            print(filter)\n            max_correct = np.max(num_correct_squares)\n            print(desired_filters[num_correct_squares==max_correct])\n            in_boards = np.array(boards)[num_correct_squares == max_correct][0]\n            print(in_boards.fen())\n            print(in_boards, ""\\n"")\n\n            no_training_pipe_issues = False\n\n    return no_training_pipe_issues, inference_pipe_results\n\n\ndef zero_window_search_tester(expected_val_fn, calculated_evaluator, hash_table_creator, boards=None, fens=None,\n                              max_depth=3, max_separator_change=1200, runs_per_depth=3):\n    """"""\n    A function to test a zero-window minimax search.  It first computes a \'correct\' minimax value based on\n    a given search function, then repeatedly does zero-window search calls (with increasing search depth)\n    above and below the \'correct\' minimax value to verify that the searches are behaving as desired.\n\n\n\n    :param expected_val_fn: A function which calculates and returns the full minimax value of a board.  It must accept\n    2 parameters, a Python-Chess board, and the desired search depth.\n    :param calculated_evaluator: The zero-window search function being tested.  It requires 4 parameters,\n    a Python-Chess board, the depth to search, the separator to test, and a hash table.  It must return\n    the value calculated\n    :param hash_table_creator: A function which takes no arguments and returns a hash table to give the search being\n    tested\n    :param boards: An iterable of Python-Chess Board objects.   If supplied, \'fens\' parameter will be ignored\n    :param fens: An iterable of strings, each a FEN representation of a board.  If \'boards\' is supplied this will not be\n    used, and if neither are supplied DEFAULT_TESTING_FENS will be used\n    :param max_depth: The maximum search depth to test.  Tests will be performed for depths 1 to max_depth\n    :param max_separator_change: The maximum distance from the expected minimax value to do a zero-window test\n    :param runs_per_depth: The number of times the zero-window search will be tested for each board/depth combination\n    (one \'run\' is testing one value above the minimax value, and one value below)\n    :return: True if all tests were passed, False if not\n    """"""\n    issue_printer = lambda b,m,s,c : print(\n        ""%s\\n%s\\nThe actual minimax value for the above board is %f, but when testing a value of %f, the zero-window search resulted in %f""%(b, b.fen(), m, s, c))\n\n    def test_board(board):\n        hash_table = hash_table_creator()\n        for depth in range(1, max_depth+1):\n            minimax_value = expected_val_fn(board, depth)\n\n            for _ in range(runs_per_depth):\n                above_val_separator = minimax_value + np.random.rand() * max_separator_change\n\n                if above_val_separator == minimax_value:\n                    above_val_separator = np.nextafter(np.nextafter(minimax_value, MAX_FLOAT32_VAL), MAX_FLOAT32_VAL)\n\n                calculated_value = calculated_evaluator(board, depth, above_val_separator, hash_table)\n\n                if calculated_value < minimax_value or calculated_value >= above_val_separator:\n                    issue_printer(board, minimax_value, above_val_separator, calculated_value)\n                    return False\n\n                below_val_separator = minimax_value - np.random.rand() * max_separator_change\n\n                if below_val_separator == minimax_value:\n                    below_val_separator = np.nextafter(np.nextafter(minimax_value, MIN_FLOAT32_VAL), MIN_FLOAT32_VAL)\n\n                calculated_value = calculated_evaluator(board, depth, below_val_separator, hash_table)\n\n                if calculated_value > minimax_value or calculated_value < below_val_separator:\n                    issue_printer(board, minimax_value, below_val_separator, calculated_value)\n                    return False\n        return True\n\n\n    if boards is None:\n        boards = map(chess.Board, fens if fens else DEFAULT_TESTING_FENS)\n\n    return all(map(test_board, boards))\n\n\n\n\n\n\n\n\ndef decompress_squares(to_decompress, desired_num):\n    to_return = np.empty(desired_num, np.uint8)\n    to_return[::2] = to_decompress >> 4\n\n    if desired_num % 2:\n        to_return[1::2] = to_decompress[:-1] & np.uint8(0x0F)\n    else:\n        to_return[1::2] = to_decompress & np.uint8(0x0F)\n    return to_return\n\n\ndef weighted_piece_sum_creator(piece_values=None):\n    if piece_values is None:\n        piece_values = np.array([0,100,300,300,500,900,0], dtype=np.int32)\n\n        values_for_white = np.array([0, 900, 500, 300, 300, 100, 500])\n        piece_filter_values = np.r_[0, values_for_white, -values_for_white]\n\n    def bf_piece_sum_eval(compressed_pieces_filters, relevant_piece_masks):\n        squares_per_mask = popcount(relevant_piece_masks)\n\n        decompressed_pieces = decompress_squares(compressed_pieces_filters, np.sum(squares_per_mask))\n        cur_piece_values = piece_filter_values[decompressed_pieces]\n\n        split_indices = np.r_[0,np.cumsum(squares_per_mask, dtype=np.int32)]\n\n        to_return = np.array([np.sum(cur_piece_values[s:e]) for s, e in zip(split_indices[:-1], split_indices[1:])])\n        return to_return\n\n\n    def simple_board_piece_sum(board):\n        total = 0\n        for square in scan_reversed(board[\'occupied\']):\n            type = piece_type_at(board, square)\n            cur_value = piece_values[type]\n            if board[\'occupied_co\'][0] & BB_SQUARES[square]:\n                cur_value *= -1\n            total += cur_value\n        return total\n\n    return bf_piece_sum_eval, simple_board_piece_sum\n\n\ndef create_negamax_function(eval_fn):\n    """"""\n    :param eval_fn: The evaluation function used must account for how the board data (in batch first) is given as if\n    it were white\'s turn (for the sake of the ANNS), and must match that behavior\n    (see \'dummy_eval_for_bf\' and \'dummy_eval_for_simple_search\')\n    """"""\n    def get_eval_score(board, depth):\n        if board[""halfmove_clock""] >= 50 or has_insufficient_material(board):\n            return TIE_RESULT_SCORE\n\n        set_up_move_array(board)\n\n        if board[\'children_left\'] == 0:\n            if board[\'best_value\'] == TIE_RESULT_SCORE:\n                return TIE_RESULT_SCORE\n            return LOSS_RESULT_SCORES[depth] if board[\'turn\'] else WIN_RESULT_SCORES[depth]\n\n        if depth == 0:\n            return eval_fn(board)\n\n        return None\n\n    def simple_struct_copy_push(board, move):\n        to_push = np.array([board.copy()])\n        push_moves(to_push, np.array([move]))\n\n        to_return = to_push[0]\n        to_return[\'children_left\'] = 0\n        to_return[\'unexplored_moves\'][:] = 255\n\n        return to_return\n\n    def negamax(board, depth, alpha, beta, color):\n        eval_score = get_eval_score(board, depth)\n\n        if not eval_score is None:\n            return color * eval_score\n\n        best_val = MIN_FLOAT32_VAL\n        for j in range(board[\'children_left\']):\n            best_val = np.maximum(\n                best_val,\n                - negamax(simple_struct_copy_push(board, board[\'unexplored_moves\'][j]), depth - 1, -beta, -alpha, -color))\n            alpha = np.maximum(alpha, best_val)\n            if alpha >= beta:\n                break\n        return best_val\n\n    def search_helper(py_board, depth, alpha=MIN_FLOAT32_VAL, beta=MAX_FLOAT32_VAL):\n        color = 1 if py_board.turn else -1\n        return negamax(create_node_info_from_python_chess_board(py_board)[0], depth, alpha, beta, color)\n\n    return search_helper\n\n\ndef negamax_zero_window_search_creator(eval_fn, move_predictor, max_batch_size=5000):\n    dummy_previous_board_map = np.zeros([2, 1], dtype=np.uint64)\n\n    def zero_window_search(board, depth, separator, hash_table):\n        priority_bins = PriorityBins(\n            np.linspace(-4000,4000,1000),\n            max_batch_size)\n\n        separator_to_use = np.nextafter(separator, MIN_FLOAT32_VAL)\n\n        root_node = set_up_root_node_for_struct(\n            move_predictor,\n            hash_table,\n            dummy_previous_board_map,\n            create_node_info_from_python_chess_board(board, depth, separator_to_use))\n\n        if root_node.struct[\'terminated\']:\n            return root_node.struct[\'best_value\']\n\n        to_return = zero_window_negamax_search(\n            root_node,\n            priority_bins,\n            eval_fn,\n            move_predictor,\n            hash_table=hash_table,\n            previous_board_map=dummy_previous_board_map)\n        return to_return\n\n    return zero_window_search\n\n\ndef pseudo_random_move_eval(*args):\n    """"""\n    NOTES:\n    1) This function uses np.linspace instead of randomly generated values to maintain the deterministic behavior of the\n    tree search (only helpful when tracking down a bug).\n    """"""\n    return lambda x: np.linspace(0, 1, np.sum(x[1]))\n\n\ndef cur_hash_getter(fen_to_start,move_lists,max_possible_moves):\n    hashes = np.zeros((len(move_lists), max_possible_moves), dtype=np.uint64)\n    initial_board = create_node_info_from_fen(fen_to_start, 255, 0)\n    for j in range(len(move_lists)):\n        board = initial_board.copy()\n\n        for i,move in enumerate(move_lists[j]):\n            moves_to_push = np.array(\n                [[move.from_square, move.to_square, move.promotion if move.promotion else 0]], dtype=np.uint8)\n\n            push_moves(board, moves_to_push)\n\n            hashes[j,i] = board[0][\'hash\']\n    return hashes\n\n\ndef cur_boards_to_input_list_fn(boards):\n    struct_array = np.concatenate(list(map(create_node_info_from_python_chess_board, boards)))\n\n    return struct_array_to_ann_inputs(\n        struct_array,\n        np.array([], dtype=numpy_node_info_dtype),\n        np.ones(len(struct_array), dtype=np.bool_),\n        np.array([], dtype=np.bool_),\n        len(struct_array))\n\n\ndef cur_piece_to_filter_fn(piece):\n    if piece is None:\n        return 0\n    if piece.color:\n        return 8-piece.piece_type\n    else:\n        return 15-piece.piece_type\n\n\ndef dummy_tfrecords_writer(pgn_filename, output_filename):\n    TEMP_FILENAME = ""THIS_SHOULD_BE_A_TEMP_FILE.pkl""\n    get_data_from_pgns(\n        [pgn_filename],\n        TEMP_FILENAME,\n        create_board_eval_board_from_game_fn(min_start_moves=0,for_testing=True),\n        print_interval=None)\n\n    combine_pickles_and_create_tfrecords(\n        [TEMP_FILENAME],\n        [output_filename],\n        [1],\n        serializer_creator())\n\n\n\n\ndef dummy_tf_records_input_data_pipe_creator(filename, include_unoccupied=False):\n    def tf_records_input_data_fn():\n        dataset = tf.data.TFRecordDataset(filename)\n\n        def parser(record):\n            keys_to_features = {""board"": tf.FixedLenFeature([8 * 8 ], tf.int64),\n                                ""score"": tf.FixedLenFeature([], tf.int64)}\n\n            parsed_example = tf.parse_single_example(record, keys_to_features)\n\n            reshaped_board = tf.reshape(parsed_example[\'board\'], [8,8])\n\n            omit_unoccupied_decrement = 0 if include_unoccupied else 1\n            one_hot_board = tf.one_hot(reshaped_board - omit_unoccupied_decrement, 16 - omit_unoccupied_decrement)\n            return one_hot_board\n\n        dataset = dataset.map(parser)\n        iterator = dataset.make_one_shot_iterator()\n        features = iterator.get_next()\n        return {""board"" : features}\n\n    return tf_records_input_data_fn\n\n\ndef cur_tfrecords_to_features(filename):\n    input_generator = dummy_tf_records_input_data_pipe_creator(filename)()\n    with tf.Session() as sess:\n        inputs = []\n        try:\n            while True:\n                inputs.append(sess.run(input_generator[\'board\']))\n        except tf.errors.OutOfRangeError:\n            return np.stack(inputs)\n\n\n\ndef full_test(existing_high_level_uci_filename):\n    """"""\n    Runs every test relevant to Batch First\'s performance or correctness (that\'s been created so far).\n\n    :return: A boolean value indicating if all tests were passed\n    """"""\n    result_str = [""Failed"", ""Passed""]\n\n    print(""Starting tests (this will likely take 1-3 minutes).\\n"")\n\n    test_results = np.zeros(7, dtype=np.bool_)\n\n\n    test_results[0] = full_perft_tester(\n        lambda fen, depth: perft_test(create_node_info_from_fen(fen, 0, 0), depth))\n\n    print(""PERFT test using NumPy structured scalar:                     %s"" % result_str[test_results[0]])\n\n    test_results[1] = move_verification_tester(\n        is_legal_move,\n        lambda b: create_node_info_from_python_chess_board(b)[0])\n\n    print(""Move legality verification test:                              %s"" % result_str[test_results[1]])\n\n    test_results[2] = zobrist_hash_test(cur_hash_getter)\n\n    print(""Incremental Zobrist hash test:                                %s"" % result_str[test_results[2]])\n\n    test_results[3:5] = complete_board_eval_tester(\n        tfrecords_writer=dummy_tfrecords_writer,\n        feature_getter=cur_tfrecords_to_features,\n        inference_pipe_fn=lambda : parse_into_ann_input_inference(5000, True),\n        boards_to_input_for_inference=cur_boards_to_input_list_fn,\n        piece_to_filter_fn=cur_piece_to_filter_fn,\n        ep_filter_index=1,\n        castling_filter_indices=np.array([8, 15], dtype=np.uint8))\n\n    print(""Board evaluation data creation and training pipeline test:    %s"" % result_str[test_results[3]])\n    print(""Board evaluation inference pipeline test:                     %s"" % result_str[test_results[4]])\n\n    test_results[5] = check_if_legal_move_exists_tester(\n        lambda b : has_legal_move(create_node_info_from_python_chess_board(b)[0]),\n        existing_high_level_uci_filename)\n\n    print(""Legal move existance test:                                    %s"" % result_str[test_results[5]])\n\n    bf_eval_fn, simple_eval_fn = weighted_piece_sum_creator()\n    test_results[6] = zero_window_search_tester(\n        expected_val_fn=create_negamax_function(simple_eval_fn),\n        calculated_evaluator=negamax_zero_window_search_creator(bf_eval_fn, pseudo_random_move_eval),\n        hash_table_creator=get_empty_hash_table)\n\n    print(""Zero-window search test:                                      %s"" % result_str[test_results[6]])\n\n\n    if all(test_results):\n        print(""\\nAll tests were passed!"")\n        return True\n    else:\n        print(""\\nSome tests failed! (see above)."")\n        return False\n\n\n\n\n\nif __name__ == ""__main__"":\n    full_test(existing_high_level_uci_filename=""stockfish-8-linux/Linux/stockfish_8_x64"")\n'"
playing_chess.py,3,"b'import numpy as np\n\nimport chess\nimport chess.uci\nimport time\n\nfrom batch_first.engine import ChessEngine, BatchFirstEngine\nfrom batch_first.chestimator import get_inference_functions\nfrom batch_first.anns.ann_creation_helper import combine_graphdefs, save_trt_graphdef, remap_inputs\n\n\ndef play_one_game(engine1, engine2, print_info=False):\n    """"""\n    Given two objects which inherit the ChessEngine class this function will officiate one game of chess between the\n    two engines.\n    """"""\n    the_board = chess.Board()\n    prev_move_time = time.time()\n    halfmove_counter = 0\n    engine1_turn = True\n    engine1.start_new_game()\n    engine2.start_new_game()\n\n    if print_info:\n        print(""%s\\n%s\\n""%(the_board, the_board.fen()))\n\n    while not the_board.is_game_over():\n        cur_engine = engine1 if engine1_turn else engine2\n\n        cur_engine.ready_engine()\n        next_move = cur_engine.pick_move(the_board)\n        cur_engine.release_resources()\n\n        if not the_board.is_legal(next_move):\n            print(""Exiting game due to player %d trying to push %s for the following board: \\n%s""%(1+int(not engine1_turn), next_move, the_board))\n            break\n\n        the_board.push(next_move)\n        engine1_turn = not engine1_turn\n        halfmove_counter += 1\n\n        if print_info:\n            print(""Player %d made move %s after %f time.""%(1+int(not engine1_turn), str(next_move), time.time() - prev_move_time))\n            print(""Total halfmove count: %d\\n%s\\n%s\\n""%(halfmove_counter, the_board, the_board.fen()))\n            prev_move_time = time.time()\n\n    if print_info:\n        print(""The game completed in %d halfmoves with result %s.""%(halfmove_counter, the_board.result()))\n\n    return the_board\n\n\nclass RandomEngine(ChessEngine):\n    def pick_move(self, board):\n        return np.random.choice(np.array(list(board.generate_legal_moves())))\n\n\nclass UCIEngine(ChessEngine):\n    def __init__(self, engine_location, num_threads=1, move_time=15, print_search_info=False):\n        """"""\n        :param move_time: Either the time in milliseconds given to choose a move, or a size 2 tuple representing the\n         range of possible time to give.  e.g. (100,1000) would randomly choose a time between 100 and 1000 ms\n        """"""\n        self.engine = chess.uci.popen_engine(engine_location)\n        self.engine.setoption({""threads"" : num_threads})\n\n        self.info_handler = chess.uci.InfoHandler()\n        self.engine.info_handlers.append(self.info_handler)\n\n        self.move_time = move_time\n\n        self.print_info = print_search_info\n\n    def pick_move(self, board):\n        self.engine.position(board)\n        if isinstance(self.move_time, tuple):\n            time_to_use = self.move_time[0] + np.random.rand(1)*(self.move_time[1] - self.move_time[0])\n        else:\n            time_to_use = self.move_time\n\n        to_return = self.engine.go(movetime=time_to_use).bestmove\n\n        if self.print_info:\n            print(self.info_handler.info)\n\n        return to_return\n\n\ndef compete(player1, player2, pairs_of_games=1, print_results=True, print_games=False):\n    outcomes = np.zeros([2,3],dtype=np.int32)\n\n    result_indices = {""1-0"": 0, ""0-1"": 1, ""1/2-1/2"": 2}\n    match_info = [(0, player1, player2),\n                  (1, player2, player1)]\n\n    for _ in range(pairs_of_games):\n        for i, p_white, p_black in match_info:\n            outcomes[i, result_indices[play_one_game(p_white, p_black, print_games).result()]] += 1\n\n    if print_results:\n        for j in range(2):\n            print(""When player %d was white there was: %d wins, %d losses, and %d ties""%(j+1, outcomes[j, 0], outcomes[j, 1], outcomes[j, 2]))\n\n    return outcomes\n\n\nif __name__ == ""__main__"":\n    MAX_SEARCH_BATCH_SIZE = 2048\n\n\n    GRAPHDEF_FILENAMES = [\n        ""/srv/tmp/encoder_evaluation_helper/no_input_dilations_inception_modules_3/no_input_dilations_inception_modules_3.8/1542731579"",\n        ""/srv/tmp/move_scoring_helper_current/no_dilations_for_trt_test_10_inception_diff_input/no_dilations_for_trt_test_10_inception_diff_input.3/1542881177""\n    ]\n    OUTPUT_MODEL_PATH = ""/srv/tmp/combining_graphs_1""\n    OUTPUT_MODEL_FILENAME = ""COMBINED_OUTPUT_TEST_314.pbtxt""\n\n    OUTPUT_NODE_NAMES = [""Squeeze"", ""requested_move_scores""]\n\n    PREFIXES = [""value_network"", ""policy_network""]\n\n    # combine_graphdefs(\n    #     GRAPHDEF_FILENAMES,\n    #     OUTPUT_MODEL_PATH,\n    #     OUTPUT_MODEL_FILENAME,\n    #     OUTPUT_NODE_NAMES,\n    #     name_prefixes=PREFIXES,\n    # )\n\n    REMAPPED_INPUT_NAME = ""remapped_input_graphdef__314""\n    # remap_inputs(OUTPUT_MODEL_PATH + ""/"" + OUTPUT_MODEL_FILENAME, OUTPUT_MODEL_PATH, REMAPPED_INPUT_NAME, int(MAX_SEARCH_BATCH_SIZE*1.25))\n\n    OUTPUT_NODE_NAMES = [""%s/%s""%(prefix,name) for name, prefix in zip(OUTPUT_NODE_NAMES, PREFIXES)]\n    TRT_OUTPUT_FILENAME = ""COMBINED_TRT_TEST_314.pbtxt""\n    # save_trt_graphdef(\n    #     OUTPUT_MODEL_PATH + ""/"" +  REMAPPED_INPUT_NAME,\n    #     OUTPUT_MODEL_PATH,\n    #     TRT_OUTPUT_FILENAME,\n    #     OUTPUT_NODE_NAMES,\n    #     trt_memory_fraction=.65,\n    #     max_batch_size=int(1.25*MAX_SEARCH_BATCH_SIZE),\n    #     write_as_text=True)\n\n\n    MOVE_SCORING_TEST_FILENAME = ""/srv/databases/lichess/lichess_db_standard_rated_2018-07_first_100k_games.npy""\n    ZERO_VALUE_BOARD_FILENAME = ""/srv/databases/has_zero_valued_board/combined_zero_boards.npy""\n\n    BOARD_PREDICTOR, MOVE_PREDICTOR, PREDICTOR_CLOSER = get_inference_functions(OUTPUT_MODEL_PATH + ""/"" + TRT_OUTPUT_FILENAME, session_gpu_memory=.2)\n\n    search_depth = 4\n    batch_first_engine = BatchFirstEngine(\n        search_depth,\n        BOARD_PREDICTOR,\n        MOVE_PREDICTOR,\n        bin_database_file=""deeper_network_1.npy"",\n        max_batch_size=MAX_SEARCH_BATCH_SIZE,\n        saved_zero_shift_file=""no_dilations_inception_1.npy"",\n    )\n\n\n\n    ethereal_engine = UCIEngine(""Ethereal-11.00/src/Ethereal"", move_time=10, num_threads=1, print_search_info=True)\n\n    competition_results = compete(batch_first_engine, ethereal_engine, print_games=True)\n\n    PREDICTOR_CLOSER()\n'"
batch_first/__init__.py,52,"b'import numpy as np\nimport numba as nb\n\nfrom numba import njit\n\nimport chess\nimport itertools\nimport functools\n\nfrom chess.polyglot import POLYGLOT_RANDOM_ARRAY, zobrist_hash\n\nfrom numba import cffi_support\nfrom cffi import FFI\n\nffi = FFI()\n\nimport khash_numba._khash_ffi as khash_ffi\n\ncffi_support.register_module(khash_ffi)\n\nkhash_init = khash_ffi.lib.khash_int2int_init\nkhash_get = khash_ffi.lib.khash_int2int_get\nkhash_set = khash_ffi.lib.khash_int2int_set\nkhash_destroy = khash_ffi.lib.khash_int2int_destroy\n\n\n@njit\ndef create_index_table(ids):\n    table = khash_init()\n    for j in range(len(ids)):\n        khash_set(table, ids[j], j)\n    return table\n\n\ndef get_table_and_array_for_set_of_dicts(dicts):\n    unique_keys = sorted(set(itertools.chain.from_iterable(dicts)))\n\n    # The sorted is so that the index of 0 will always be 0\n    index_lookup_table = create_index_table(np.array(sorted([np.uint64(key) for key in unique_keys]), dtype=np.uint64))\n\n    array = np.zeros(shape=[len(dicts), len(unique_keys)], dtype=np.uint64)\n\n    for square_num, dict in enumerate(dicts):\n        for key, value in dict.items():\n            index_to_set = khash_get(ffi.cast(""void *"", index_lookup_table), np.uint64(key), np.uint64(0))\n            array[square_num, index_to_set] = np.uint64(value)\n\n    return index_lookup_table, array\n\n\ndef generate_move_filter_table():\n    """"""\n    Generate a lookup table for the policy encoding described in the following paper:\n\n    \'Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\'\n    https://arxiv.org/pdf/1712.01815.pdf\n\n    So in the returned table, the value at index (f,t,p) is the index of the policy plane (in the move scoring ann)\n    associated with the chess move described as moving a piece from square f to square t and being promoted to piece p.\n    """"""\n    diffs = {}\n    for j in range(1, 8):\n        diffs[(0, j)] = j - 1\n        diffs[(0, -j)] = j + 6\n        diffs[(j, 0)] = j + 13\n        diffs[(-j, 0)] = j + 20\n        diffs[(j, j)] = j + 27\n        diffs[(j, -j)] = j + 34\n        diffs[(-j, j)] = j + 41\n        diffs[(-j, -j)] = j + 48\n\n    diffs[(1, 2)] = 56\n    diffs[(1, -2)] = 57\n    diffs[(-1, 2)] = 58\n    diffs[(-1, -2)] = 59\n    diffs[(2, 1)] = 60\n    diffs[(2, -1)] = 61\n    diffs[(-2, 1)] = 62\n    diffs[(-2, -1)] = 63\n\n    filter_table = np.zeros([64,64,6], dtype=np.uint8)\n\n    for square1 in chess.SQUARES:\n        for square2 in chess.SQUARES:\n            file_diff = chess.square_file(square2) - chess.square_file(square1)\n            rank_diff = chess.square_rank(square2) - chess.square_rank(square1)\n            if not diffs.get((file_diff, rank_diff)) is None:\n                filter_table[square1, square2] = diffs[(file_diff, rank_diff)]\n\n                if rank_diff == 1 and file_diff in [1,0,-1]:\n                    filter_table[square1, square2, 2:5] = 3*(1+file_diff) + np.arange(64,67)\n    return filter_table\n\n\nMOVE_FILTER_LOOKUP = generate_move_filter_table()\n\n\nnumpy_move_dtype = np.dtype([(""from_square"", np.uint8), (""to_square"", np.uint8), (""promotion"", np.uint8)])\nmove_type = nb.from_dtype(numpy_move_dtype)\n\nRANDOM_ARRAY = np.array(POLYGLOT_RANDOM_ARRAY, dtype=np.uint64)\n\nBB_DIAG_MASKS = np.array(chess.BB_DIAG_MASKS, dtype=np.uint64)\nBB_FILE_MASKS = np.array(chess.BB_FILE_MASKS, dtype=np.uint64)\nBB_RANK_MASKS = np.array(chess.BB_RANK_MASKS, dtype=np.uint64)\n\nDIAG_ATTACK_INDEX_LOOKUP_TABLE, DIAG_ATTACK_ARRAY = get_table_and_array_for_set_of_dicts(chess.BB_DIAG_ATTACKS)\nFILE_ATTACK_INDEX_LOOKUP_TABLE, FILE_ATTACK_ARRAY = get_table_and_array_for_set_of_dicts(chess.BB_FILE_ATTACKS)\nRANK_ATTACK_INDEX_LOOKUP_TABLE, RANK_ATTACK_ARRAY = get_table_and_array_for_set_of_dicts(chess.BB_RANK_ATTACKS)\n\nBB_KNIGHT_ATTACKS = np.array(chess.BB_KNIGHT_ATTACKS, dtype=np.uint64)\nBB_KING_ATTACKS = np.array(chess.BB_KING_ATTACKS, dtype=np.uint64)\n\nBB_PAWN_ATTACKS = np.array(chess.BB_PAWN_ATTACKS, dtype=np.uint64)\n\nBB_RAYS = np.array(chess.BB_RAYS, dtype=np.uint64)\nBB_BETWEEN = np.array(chess.BB_BETWEEN, dtype=np.uint64)\n\nMIN_FLOAT32_VAL = np.finfo(np.float32).min\nMAX_FLOAT32_VAL = np.finfo(np.float32).max\nALMOST_MIN_FLOAT_32_VAL = np.nextafter(MIN_FLOAT32_VAL, MAX_FLOAT32_VAL)\nALMOST_MAX_FLOAT_32_VAL = np.nextafter(MAX_FLOAT32_VAL, MIN_FLOAT32_VAL)\n\n\n# The maximum number of moves which can be stored/assessed by the engine\nMAX_MOVES_LOOKED_AT = 100\n\n# The maximum depth the engine is allowed to go\nMAX_SEARCH_DEPTH = 100\n\n# This value is used for indicating that a given node has/had a legal move in the transposition table that it will/would\n# expand prior to it\'s full move generation and scoring.\nNEXT_MOVE_IS_FROM_TT_VAL = np.uint8(254)\n\n# This value is used for indicating that the next move index of a board is actually a dummy variable, and there are no more moves left\nNO_MORE_MOVES_VALUE = np.uint8(255)\n\n# This value is used for indicating that a move in a transposition table entry is not being stored.\nNO_TT_MOVE_VALUE = np.uint8(255)\n\n# This value is used for indicating that no entry in the transposition table exists for that hash.  It is stored as the\n# entries depth.\nNO_TT_ENTRY_VALUE = np.uint8(255)\n\n# This value is the value used to assigned a node who\'s next move was found in the TT to the desired bin.\nTT_MOVE_SCORE_VALUE = ALMOST_MAX_FLOAT_32_VAL\n\n# This value is used when there is no current ep square.  It (obviously) does not indicate that square 0 is an ep square\nNO_EP_SQUARE = np.uint8(0)\n\n# The value to set a move\'s promotion to if no promotion is done\nNO_PROMOTION_VALUE = np.uint8(0)\n\nTIE_RESULT_SCORE = np.float32(0)\n\n\n# This is used in times when a value should be \'None\', but Numba won\'t let it.   To appease the compiler\n# this array is given, and it\'s first value checked against MIN_FLOAT32_VAL to see if it\'s the \'None\' situation.\n# Ideally this will be removed eventually.\nINT_ARRAY_NONE = np.array([MIN_FLOAT32_VAL])\n\n# The win/loss arrays are such that the magnitudes of the win/loss decrease as the index in the array increases.\n# This is so depth can be used to index the arrays\nWIN_RESULT_SCORES = np.full(MAX_SEARCH_DEPTH, np.nextafter(ALMOST_MAX_FLOAT_32_VAL, MIN_FLOAT32_VAL))\nLOSS_RESULT_SCORES = np.full(MAX_SEARCH_DEPTH, np.nextafter(ALMOST_MIN_FLOAT_32_VAL, MAX_FLOAT32_VAL))\n\nfor j in range(1, MAX_SEARCH_DEPTH):\n    WIN_RESULT_SCORES[j] = np.nextafter(WIN_RESULT_SCORES[j - 1], MIN_FLOAT32_VAL)\n    LOSS_RESULT_SCORES[j] = np.nextafter(LOSS_RESULT_SCORES[j - 1], MAX_FLOAT32_VAL)\n\n\nSIZE_EXPONENT_OF_TWO_FOR_TT_INDICES = np.uint8(30)\nTT_HASH_MASK = np.uint64(2 ** (SIZE_EXPONENT_OF_TWO_FOR_TT_INDICES) - 1)\n\n\nCOLORS = [WHITE, BLACK] = np.array([1, 0], dtype=np.uint8)\nTURN_COLORS = [TURN_WHITE, TURN_BLACK] = [True, False]\nPIECE_TYPES = [PAWN, KNIGHT, BISHOP, ROOK, QUEEN, KING] = np.arange(1, 7, dtype=np.uint8)\n\n\nSQUARES = [\n    A1, B1, C1, D1, E1, F1, G1, H1,\n    A2, B2, C2, D2, E2, F2, G2, H2,\n    A3, B3, C3, D3, E3, F3, G3, H3,\n    A4, B4, C4, D4, E4, F4, G4, H4,\n    A5, B5, C5, D5, E5, F5, G5, H5,\n    A6, B6, C6, D6, E6, F6, G6, H6,\n    A7, B7, C7, D7, E7, F7, G7, H7,\n    A8, B8, C8, D8, E8, F8, G8, H8] = np.arange(64, dtype=np.uint8)\n\nSQUARES_180 = SQUARES ^ 0x38\n\nBB_VOID = np.uint64(0)\nBB_ALL = np.uint64(0xffffffffffffffff)\n\nBB_SQUARES = [\n    BB_A1, BB_B1, BB_C1, BB_D1, BB_E1, BB_F1, BB_G1, BB_H1,\n    BB_A2, BB_B2, BB_C2, BB_D2, BB_E2, BB_F2, BB_G2, BB_H2,\n    BB_A3, BB_B3, BB_C3, BB_D3, BB_E3, BB_F3, BB_G3, BB_H3,\n    BB_A4, BB_B4, BB_C4, BB_D4, BB_E4, BB_F4, BB_G4, BB_H4,\n    BB_A5, BB_B5, BB_C5, BB_D5, BB_E5, BB_F5, BB_G5, BB_H5,\n    BB_A6, BB_B6, BB_C6, BB_D6, BB_E6, BB_F6, BB_G6, BB_H6,\n    BB_A7, BB_B7, BB_C7, BB_D7, BB_E7, BB_F7, BB_G7, BB_H7,\n    BB_A8, BB_B8, BB_C8, BB_D8, BB_E8, BB_F8, BB_G8, BB_H8\n] = np.array([1 << sq for sq in SQUARES], dtype=np.uint64)\n\nBB_CORNERS = BB_A1 | BB_H1 | BB_A8 | BB_H8\n\nBB_LIGHT_SQUARES = np.uint64(0x55aa55aa55aa55aa)\nBB_DARK_SQUARES = np.uint64(0xaa55aa55aa55aa55)\n\n\nBB_FILES = [\n    BB_FILE_A,\n    BB_FILE_B,\n    BB_FILE_C,\n    BB_FILE_D,\n    BB_FILE_E,\n    BB_FILE_F,\n    BB_FILE_G,\n    BB_FILE_H\n] = np.array([0x0101010101010101 << i for i in range(8)], dtype=np.uint64)\n\nBB_RANKS = [\n    BB_RANK_1,\n    BB_RANK_2,\n    BB_RANK_3,\n    BB_RANK_4,\n    BB_RANK_5,\n    BB_RANK_6,\n    BB_RANK_7,\n    BB_RANK_8\n] = np.array([0xff << (8 * i) for i in range(8)], dtype=np.uint64)\n\n\nBB_BACKRANKS = BB_RANK_1 | BB_RANK_8\n\nINITIAL_BOARD_FEN = chess.STARTING_FEN\n\ndef generate_move_to_enumeration_dict():\n    """"""\n    Generates a dictionary where the keys are (from_square, to_square) and their values are the move number\n    that move has been assigned.  It is done in a way such that for move number N from board B, if you were to flip B\n    vertically, the same move would have number 1792-N. (there are 1792 moves recognized)\n\n\n    IMPORTANT NOTES:\n    1) This ignores the fact that not all pawn promotions are the same, this effects the number of logits\n    in the move scoring ANN\n    """"""\n    possible_moves = {}\n\n    board = chess.Board(\'8/8/8/8/8/8/8/8 w - - 0 1\')\n    for square in chess.SQUARES[:len(SQUARES) // 2]:\n        for piece in [chess.Piece(chess.KNIGHT, True), chess.Piece(chess.QUEEN, True)]:\n            board.set_piece_at(square, piece)\n            for move in board.generate_legal_moves():\n                if possible_moves.get((move.from_square, move.to_square)) is None:\n                    possible_moves[move.from_square, move.to_square] = len(possible_moves)\n\n            board.remove_piece_at(square)\n\n    switch_square_fn = lambda x: x ^ 0x38\n\n    total_possible_moves = len(possible_moves) * 2 - 1\n\n    for (from_square, to_square), move_num in list(possible_moves.items()):\n        possible_moves[switch_square_fn(from_square), switch_square_fn(to_square)] = total_possible_moves - move_num\n\n    return possible_moves\n\n\nMOVE_TO_INDEX_ARRAY = np.zeros(shape=[64, 64], dtype=np.int32)\nOLD_REVERSED_MOVE_TO_INDEX_ARRAY = np.zeros(shape=[64, 64], dtype=np.int32)\nREVERSED_MOVE_TO_INDEX_ARRAY = np.zeros(shape=[64, 64], dtype=np.int32)\n\ndict_keys, dict_values = list(zip(*generate_move_to_enumeration_dict().items()))\ndict_keys = np.array(dict_keys)\n\nMOVE_TO_INDEX_ARRAY[dict_keys[:,0], dict_keys[:,1]] = dict_values\n\nreversed_keys = dict_keys ^ 0x38\nREVERSED_MOVE_TO_INDEX_ARRAY[reversed_keys[:,0], reversed_keys[:,1]] = dict_values\n\n\nREVERSED_EP_LOOKUP_ARRAY = SQUARES_180.copy()\nREVERSED_EP_LOOKUP_ARRAY[0] = NO_EP_SQUARE\n\n\ndef power_set(iterable):\n    s = list(iterable)\n    return itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s) + 1))\n\nflip_vert_const_1 = np.uint64(0x00FF00FF00FF00FF)\nflip_vert_const_2 = np.uint64(0x0000FFFF0000FFFF)\n\n@nb.vectorize([nb.uint64(nb.uint64)], nopython=True)\ndef flip_vertically(bb):\n    bb = ((bb >>  8) & flip_vert_const_1) | ((bb & flip_vert_const_1) <<  8)\n    bb = ((bb >> 16) & flip_vert_const_2) | ((bb & flip_vert_const_2) << 16)\n    bb = ( bb >> 32) | ( bb << 32)\n    return bb\n\ndef get_castling_lookup_tables():\n    possible_castling_rights = np.zeros(2 ** 4, dtype=np.uint64)\n    for j, set in enumerate(power_set([BB_A1, BB_H1, BB_A8, BB_H8])):\n        possible_castling_rights[j] = np.uint64(functools.reduce(lambda x, y: x | y, set, np.uint64(0)))\n\n    white_turn_castling_tables = create_index_table(possible_castling_rights)\n    black_turn_castling_tables = create_index_table(flip_vertically(possible_castling_rights))\n\n    return white_turn_castling_tables, black_turn_castling_tables, possible_castling_rights\n\n\nWHITE_CASTLING_RIGHTS_LOOKUP_TABLE, BLACK_CASTLING_RIGHTS_LOOKUP_TABLE, POSSIBLE_CASTLING_RIGHTS = get_castling_lookup_tables()\n'"
batch_first/chestimator.py,0,"b'import tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom tensorflow.python.platform import gfile\n\nfrom tensorflow.contrib import tensorrt as trt\n\n\n\ndef get_predictors(session, graphdef_filename, eval_output_tensor, move_output_stages_tensor_names, move_input_tensor_names, eval_input_tensor_names):\n    """"""\n    All ANN interactions should be called through C/C++ functions for speed (hopefully will be addressed soon)\n    """"""\n    if graphdef_filename[-3:] == "".pb"":\n        with gfile.FastGFile(graphdef_filename, \'rb\') as f:\n            model_graph_def = tf.GraphDef()\n            model_graph_def.ParseFromString(f.read())\n    else:\n        with open(graphdef_filename, \'r\') as f:\n            txt = f.read()\n            model_graph_def = text_format.Parse(txt, tf.GraphDef())\n\n    desired_tensors = tf.import_graph_def(\n        model_graph_def,\n        return_elements=[eval_output_tensor] + move_output_stages_tensor_names + move_input_tensor_names + eval_input_tensor_names,\n        name="""")\n\n    eval_output = desired_tensors[0]\n    move_outputs = desired_tensors[1:len(move_output_stages_tensor_names) + 1]\n    move_inputs = desired_tensors[len(move_output_stages_tensor_names) + 1:-len(eval_input_tensor_names)]\n    eval_inputs = desired_tensors[-len(eval_input_tensor_names):]\n\n    with tf.device(\'/GPU:0\'):\n        with tf.control_dependencies([move_outputs[0]]):\n            dummy_operation = tf.constant([0], dtype=tf.float32, name=""dummy_const"")\n\n\n    board_predictor = session.make_callable(eval_output_tensor, eval_input_tensor_names)\n\n    def start_move_prediction(*board_inputs):\n        handle = session.partial_run_setup([dummy_operation, move_outputs[1]], eval_inputs + move_inputs)\n        session.partial_run(handle, dummy_operation, dict(zip(eval_inputs, board_inputs)))\n\n        return lambda move_info: session.partial_run(handle,\n                                                     move_outputs[1],\n                                                     dict(zip(move_inputs[-3:], move_info)))\n\n    return board_predictor, start_move_prediction\n\n\ndef get_inference_functions(graphdef_filename, session_gpu_memory=.4):\n    sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=session_gpu_memory)))\n\n    path_adder = lambda p, l : list(map(lambda st : ""%s/%s""%(p, st), l))\n    eval_output_tensor_name = ""value_network/Squeeze:0""\n    eval_input_tensor_names = [""piece_filters:0"", ""occupied_bbs:0""]\n\n    move_scoring_stages_names = [""Reshape"", ""requested_move_scores:0""]\n\n    move_scoring_input_tensor_names = [""from_square_placeholder:0"", ""move_filter_placeholder:0"", ""moves_per_board_placeholder:0""]\n\n    eval_input_tensor_names = path_adder(""input_parser"", eval_input_tensor_names)\n\n    move_scoring_stages_names = path_adder(""policy_network"", move_scoring_stages_names)\n    move_scoring_input_tensor_names = path_adder(""policy_network"", move_scoring_input_tensor_names)\n\n    predictors = get_predictors(\n        sess,\n        graphdef_filename,\n        eval_output_tensor_name, move_scoring_stages_names, move_scoring_input_tensor_names, eval_input_tensor_names)\n\n    closer_fn = lambda: sess.close()\n\n    return predictors[0], predictors[1], closer_fn\n\n\n\n'"
batch_first/classes_and_structs.py,29,"b'from collections import OrderedDict\n\nfrom . import *\n\n\n\nnumpy_node_info_dtype = np.dtype(\n    [(""pawns"", np.uint64),\n     (""knights"", np.uint64),\n     (""bishops"", np.uint64),\n     (""rooks"", np.uint64),\n     (""queens"", np.uint64),\n     (""kings"", np.uint64),\n     (""occupied_co"", np.uint64, (2)),\n     (""occupied"", np.uint64),\n     (""turn"", np.int8),\n     (""castling_rights"", np.uint64),\n     (""ep_square"", np.uint8),\n     (""halfmove_clock"", np.uint8),\n     (""hash"", np.uint64),\n     (""terminated"", np.bool_),\n     (""separator"", np.float32),\n     (""depth"", np.uint8),\n     (""best_value"", np.float32),\n     (""unexplored_moves"", np.uint8, (MAX_MOVES_LOOKED_AT, 3)),\n     (""unexplored_move_scores"", np.float32, (MAX_MOVES_LOOKED_AT)),\n     (\'prev_move\', np.uint8, (3)),\n     (""next_move_index"", np.uint8),\n     (""children_left"", np.uint8)])\n\n\nnumba_node_info_type = nb.from_dtype(numpy_node_info_dtype)\n\n\ndef create_node_info_from_python_chess_board(board, depth=255, separator=0):\n    return np.array(\n        [(board.pawns,\n          board.knights,\n          board.bishops,\n          board.rooks,board.queens,\n          board.kings,\n          board.occupied_co,\n          board.occupied,\n          np.int8(board.turn),\n          board.castling_rights,\n          board.ep_square if not board.ep_square is None else NO_EP_SQUARE,\n          board.halfmove_clock,\n          zobrist_hash(board),\n          False,                # terminated\n          separator,\n          depth,\n          MIN_FLOAT32_VAL,      # best_value\n          np.full([MAX_MOVES_LOOKED_AT, 3], 255, dtype=np.uint8),                # unexplored moves\n          np.full([MAX_MOVES_LOOKED_AT], MIN_FLOAT32_VAL, dtype=np.float32),     # unexplored move scores\n          np.full([3], 255, dtype=np.uint8), # The move made to reach the position this board represents\n          0,        # next_move_index  (the index in the stored moves where the next move to make is)\n          0)],      # children_left (the number of children which have yet to returne a value, or be created)\n        dtype=numpy_node_info_dtype)\n\n\ndef create_node_info_from_fen(fen, depth, separator):\n    return create_node_info_from_python_chess_board(chess.Board(fen), depth, separator)\n\n\n\ngame_node_type = nb.deferred_type()\n\ngamenode_spec = OrderedDict()\n\ngamenode_spec[""board_struct""] = numba_node_info_type[:]\ngamenode_spec[""parent""] = nb.optional(game_node_type)\n\n\n\n\n@nb.jitclass(gamenode_spec)\nclass GameNode:\n    def __init__(self, board_struct, parent):\n        self.board_struct = board_struct\n\n        # Eventually this should be some sort of list, so that updating multiple parents is possible\n        # when handling transpositions which are open at the same time.\n        self.parent = parent\n\n    @property\n    def struct(self):\n        return self.board_struct[0]\n\n\ngame_node_type.define(GameNode.class_type.instance_type)\n\n\n\ngame_node_holder_type = nb.deferred_type()\n\ngame_node_holder_spec = OrderedDict()\n\ngame_node_holder_spec[""held_node""] = GameNode.class_type.instance_type\ngame_node_holder_spec[""next_holder""] = nb.optional(game_node_holder_type)\n\n@nb.jitclass(game_node_holder_spec)\nclass GameNodeHolder:\n    """"""\n    A jitclass used for representing a linked list of GameNode objects.  (this is mainly used for compilation purposes)\n\n    NOTES:\n    1) This shouldn\'t be needed at all and a \'next\' SOMETHING should just be added to the GameNode class, but\n    Numba won\'t let that happen (yet)\n    """"""\n    def __init__(self, held_node, next_holder):\n        self.held_node = held_node\n        self.next_holder = next_holder\n\n    @property\n    def struct(self):\n        return self.held_node.struct\n\ngame_node_holder_type.define(GameNodeHolder.class_type.instance_type)\n\n\n\ngame_node_holder_holder_type = nb.deferred_type()\n\ngame_node_holder_holder_spec = OrderedDict()\n\ngame_node_holder_holder_spec[""held""] = GameNodeHolder.class_type.instance_type\ngame_node_holder_holder_spec[""next""] = nb.optional(game_node_holder_holder_type)\n\n@nb.jitclass(game_node_holder_holder_spec)\nclass GameNodeHolderHolder:\n    """"""\n    This is a temporary class used to avoid the time required for Numba\'s boxing and unboxing when using Lists of\n    JitClass objects.  It will be removed when more JIT coverage allows.\n\n    """"""\n    def __init__(self, held, next):\n        self.held = held\n        self.next = next\n\ngame_node_holder_holder_type.define(GameNodeHolderHolder.class_type.instance_type)\n\n\n\n@njit\ndef get_list_from_holder_holder(holder):\n    to_return = []\n    while not holder is None:\n        to_return.append(holder.held)\n        holder = holder.next\n    return to_return\n\n@njit\ndef get_holder_holder_from_list(lst):\n    next_holder = None\n    for sub_holder in lst[::-1]:\n        next_holder = GameNodeHolderHolder(sub_holder, next_holder)\n    return next_holder\n\n@njit\ndef clear_holder_holder(holder):\n    dummy_sub_holder = create_dummy_node_holder()\n    while not holder is None:\n        holder.held = dummy_sub_holder\n        holder = holder.next\n\n\n\n@njit\ndef len_node_holder(ll):\n    count = 0\n    while not ll is None:\n        count += 1\n        ll = ll.next_holder\n    return count\n\n\n@njit\ndef create_dummy_node_holder():\n    return GameNodeHolder(GameNode(np.empty(1, numpy_node_info_dtype), None), None)\n\n\n@njit\ndef filter_holders_then_append(root, holder, mask, append_to_end):\n    if holder is None:\n        root.next_holder = append_to_end\n        return 0\n\n    root.next_holder = holder\n\n    holder = root\n    mask_index = 0\n    total_kept = 0\n    while not holder.next_holder is None:\n        if mask[mask_index]:\n            holder = holder.next_holder\n            total_kept += 1\n        else:\n            holder.next_holder = holder.next_holder.next_holder\n        mask_index += 1\n\n    holder.next_holder = append_to_end\n    return total_kept\n'"
batch_first/engine.py,24,"b'from . import *\n\nfrom .transposition_table import get_empty_hash_table, clear_hash_table\nfrom .numba_negamax_zero_window import iterative_deepening_mtd_f, start_move_scoring, start_board_evaluations\nfrom .global_open_priority_nodes import PriorityBins\n\n\n\ndef generate_bin_ranges(filename, move_eval_fn, percentiles=None, max_batch_size=1000, output_filename=None, print_info=False):\n    """"""\n    Generate values representing the boundaries for the bins in the PriorityBins class based on a given\n    move evaluation function.  It also calculates the mean (zero-shift) for the move values.\n\n    :param filename: The filename for the binary file (in NumPy .npy format) containing board structs.\n     It\'s used for computing a sample of move scores\n    :param move_eval_fn: The move evaluation function to be used when searching the tree\n    :param percentiles: The percentiles desired from the sample of move scores computed\n    :param max_batch_size: The maximum batch size to be given to the move_eval_fn\n    :param output_filename: The filename to save the computed bins to, or None if saving the bins is not desired\n    :param print_info: A boolean value indicating if info about the computations should be printed\n    :return: An ndarray of the values at the given percentiles\n    """"""\n    def bin_helper_move_scoring_fn(struct_array):\n        move_thread, move_score_getter, _, _ = start_move_scoring(\n            struct_array,\n            struct_array[:1],\n            np.ones(len(struct_array), dtype=np.bool_),\n            np.zeros(1, dtype=np.bool_),\n            move_eval_fn)\n\n        to_concat_from_squares = []\n        to_concat_filters = []\n\n        for struct in struct_array:\n            relevant_moves = struct[\'unexplored_moves\'][:struct[\'children_left\']]\n\n            if not struct[\'turn\']:\n                relevant_moves[:,:2] = SQUARES_180[relevant_moves[:,:2]]\n\n            to_concat_filters.append(MOVE_FILTER_LOOKUP[relevant_moves[:, 0], relevant_moves[:, 1], relevant_moves[:, 2]])\n            to_concat_from_squares.append(relevant_moves[:,0])\n\n        move_filters = np.concatenate(to_concat_filters)\n        from_squares = np.concatenate(to_concat_from_squares)\n\n\n        move_thread.join()\n\n        to_return = move_score_getter[0]([move_filters, from_squares, struct_array[\'children_left\']])\n\n        return to_return\n\n\n    if percentiles is None:\n        percentiles = np.arange(0, 100, .02)\n\n    if print_info:\n        print(""Loading data from file for bin calculations"")\n\n    struct_array = np.load(filename)\n\n    if print_info:\n        print(""Loaded %d BoardInfo structs""%len(struct_array))\n\n\n    increment = max_batch_size\n    struct_array = struct_array[:- (len(struct_array) % increment)]\n\n    combined_results = np.concatenate(\n        [bin_helper_move_scoring_fn(struct_array[j * increment:(j + 1) * increment]) for j in range((len(struct_array)-1)//increment)])\n\n    zero_shift = np.mean(combined_results)\n\n    combined_results -= zero_shift\n    combined_results = np.abs(combined_results)\n\n    if print_info:\n        print(""Computed %d move evaluations""%len(combined_results))\n\n    bins = np.percentile(combined_results, percentiles)\n\n    if not output_filename is None:\n        np.save(output_filename, bins)\n        np.save(output_filename + ""_shift"", zero_shift)\n\n        if print_info:\n            print(""Saved bins to file"")\n\n    return bins, zero_shift\n\n\ndef calculate_eval_zero_shift(filename, board_eval_fn, max_batch_size=5000, output_filename=""draw_board_mean"", print_info=False):\n    """"""\n    Calculates the mean evaluation value of boards which have the \'expected\' value of 0 (currently decided by StockFish).\n    The calculated mean can then be used to shift the evaluation function so that it values tie games\n    at 0 (the new evaluation is calculated: f\'(x)=f(x)-mean).\n\n    Zero-shifting the evaluation function is crucial since negamax is used rather than minimax, and because\n    it maintains the accuracy of the stored draw value.\n\n\n    :param filename: The filename for the binary file (in NumPy .npy format) containing board structs which each\n    have a \'desired\' value of 0 (according to StockFish).\n     It\'s used for computing a sample of move scores\n    :param board_eval_fn: The board evaluation function to be used when searching the tree\n    :param max_batch_size: The maximum batch size to be given to the board_eval_fn\n    :param output_filename: The filename to save the computed bins to, or None if saving the bins is not desired\n    :param print_info: A boolean value indicating if info about the computations should be printed\n    :return: The mean evaluation value\n    """"""\n    def eval_helper(struct_array):\n        thread, scores = start_board_evaluations(\n            struct_array,\n            np.ones(len(struct_array), dtype=np.bool_),\n            board_eval_fn)\n\n        thread.join()\n\n        return scores\n\n    if print_info:\n        print(""Loading data from file for zero-shift calculations"")\n\n    struct_array = np.load(filename)\n\n    if print_info:\n        print(""Loaded %d BoardInfo structs""%len(struct_array))\n\n    struct_array = struct_array[:- (len(struct_array) % max_batch_size)]\n\n    num_batches = (len(struct_array)-1)//max_batch_size\n\n    combined_results = np.concatenate(\n        [eval_helper(\n            struct_array[j * max_batch_size:(j + 1) * max_batch_size]) for j in range(num_batches)])\n\n    if print_info:\n        print(""Computed %d board evaluations""%len(combined_results))\n\n    mean = np.float32(np.mean(combined_results))\n\n    if print_info:\n        print(""The mean calculated board evaluation value is: %f""%mean)\n\n    if not output_filename is None:\n        np.save(output_filename, mean)\n\n        if print_info:\n            print(""Saved mean to file"")\n\n    return mean\n\n\ndef get_previous_board_map_from_py_board(board):\n    """"""\n    SPEED IMPROVEMENTS TO MAKE:\n    1) Stop going backward if castling rights are changed\n    2) Stop using the python-chess board implementation\n    3) Stop using a dictionary so it can be JIT compiled\n    """"""\n    board = board.copy()  #Just in case the board shouldn\'t be modified\n\n    hash_dict = {zobrist_hash(board):1}\n\n    while board.move_stack and board.halfmove_clock:\n        board.pop()\n\n        cur_hash = np.int64(np.uint64(zobrist_hash(board)))\n        if cur_hash in hash_dict:\n            hash_dict[cur_hash] += 1\n        else:\n            hash_dict[cur_hash] = 1\n\n    to_pass_on = np.array(list(zip(*hash_dict.items())), dtype=np.uint64)\n\n    return to_pass_on[:, np.argsort(to_pass_on[0])]\n\n\n\n\nclass ChessEngine(object):\n\n    def pick_move(self, Board):\n        """"""\n        Given a Python-Chess Board object, return a Python-Chess Move object representing the move\n        the engine would like to make.\n        """"""\n        raise NotImplementedError(""This method must be implemented!"")\n\n    def start_new_game(self):\n        """"""\n        Run at the start of each new game\n        """"""\n        pass\n\n    def ready_engine(self):\n        """"""\n        Set up whatever is needed to choose a move (used if resources must be released after each move).\n        """"""\n        pass\n\n    def release_resources(self):\n        """"""\n        Release the resources currently used by the engine (like GPU memory or large chunks of RAM).\n        """"""\n        pass\n\n\n\n\nclass BatchFirstEngine(ChessEngine):\n\n    def __init__(self, search_depth, board_eval_fn, move_eval_fn, bin_database_file=None, bin_output_filename=None,\n                 first_guess_fn=None, max_batch_size=5000, zero_valued_boards_file=None, saved_zero_shift_file=None):\n        """"""\n        :param bin_database_file: If bin_output_filename is not None, then this is the NumPy database of boards to have\n        bins be created from.  If bin_output_filename is None, then this is the NumPy file containing an array of bins.\n        :param bin_output_filename: The name of the (NumPy) file which will be saved containing the bins computed, or None\n        if the bins should not be saved.\n        """"""\n        if saved_zero_shift_file is None and zero_valued_boards_file is None:\n            raise ValueError(""Either saved_zero_shift_file or zero_valued_boards_file must be specified, but both are None!"")\n\n        if bin_database_file is None and bin_output_filename is None:\n            raise ValueError(""Either bin_database_file or bin_output_filename must be specified but both are None!"")\n\n\n        if first_guess_fn is None:\n            self.first_guess_fn = lambda x : 0\n        else:\n            self.first_guess_fn = first_guess_fn\n\n        self.search_depth = search_depth\n\n        self.board_evaluator = board_eval_fn\n        self.move_evaluator = move_eval_fn\n\n        if bin_output_filename is None:\n            self.bins = np.load(bin_database_file)\n            move_zero_shift = np.load(bin_database_file[:-4] + ""_shift.npy"")\n        else:\n            self.bins, move_zero_shift = generate_bin_ranges(\n                bin_database_file,\n                self.move_evaluator,\n                max_batch_size=int(1.25*max_batch_size),\n                output_filename=bin_output_filename,\n                print_info=True)\n\n\n        if zero_valued_boards_file is None:\n            zero_shift = np.load(saved_zero_shift_file)\n        else:\n            zero_shift = calculate_eval_zero_shift(\n                zero_valued_boards_file,\n                self.board_evaluator,\n                max_batch_size=int(1.25*max_batch_size),\n                output_filename=saved_zero_shift_file,\n                print_info=True)\n\n        self.board_evaluator = lambda *args : board_eval_fn(*args) - zero_shift\n\n        self.open_node_holder = PriorityBins(\n            self.bins,\n            max_batch_size,\n            zero_shift=move_zero_shift,\n            # save_info=True, #Must be set to True if printing info about the searches!\n        )\n\n        self.hash_table = get_empty_hash_table()\n\n    def start_new_game(self):\n        clear_hash_table(self.hash_table)\n\n    def pick_move(self, board):\n        returned_score, move_to_return, self.hash_table = iterative_deepening_mtd_f(\n            fen=board.fen(),\n            depths_to_search=np.arange(1,self.search_depth+1),\n            open_node_holder=self.open_node_holder,\n            board_eval_fn=self.board_evaluator,\n            move_eval_fn=self.move_evaluator,\n            hash_table=self.hash_table,\n\n            previous_board_map=get_previous_board_map_from_py_board(board),\n\n            # print_info=True,       #If this is True, the save_info parameter for the PriorityBins must be True (in the __init__ function)  (better connection of these values to come)!\n            )\n\n        return move_to_return\n\n'"
batch_first/global_open_priority_nodes.py,11,"b'from .classes_and_structs import *\n\n\n@njit\ndef should_not_terminate(game_node):\n    cur_node = game_node\n    while cur_node is not None:\n        if cur_node.struct.terminated:\n            return False\n        cur_node = cur_node.parent\n    return True\n\n\n@njit\ndef append_non_terminating(to_check, root):\n    while not to_check is None:\n        if should_not_terminate(to_check.held_node):\n            root.next_holder = to_check\n            root = root.next_holder\n\n        to_check = to_check.next_holder\n\n    root.next_holder = None\n    return root\n\n\n@njit\ndef append_non_terminating_with_counting(to_check, root, max_to_get):\n    num_found = 0\n    while not to_check is None:\n        if should_not_terminate(to_check.held_node):\n            root.next_holder = to_check\n            root = root.next_holder\n\n            num_found += 1\n            if num_found == max_to_get:\n                to_check = to_check.next_holder\n                break\n\n        to_check = to_check.next_holder\n\n    root.next_holder = None\n    return root, to_check, num_found\n\n\n\nclass GlobalNodeList(object):\n    def is_empty(self):\n        """"""\n        Checks if the node list is empty.\n\n        :return: A boolean value indicating if the list is empty.\n        """"""\n        raise NotImplementedError(""This method must be implemented!"")\n\n    def insert_nodes_and_get_next_batch(self, to_insert, scores):\n        """"""\n        Inserts the given nodes into the stored list, and gets the next batch of nodes to be computed.\n\n        :param to_insert: An ndarray of the nodes to be inserted into the list\n        :param scores: An ndarray of the values used for prioritization, corresponding to the given nodes to insert\n        :return: An ndarray of the nodes to be computed in the next batch\n        """"""\n        raise NotImplementedError(""This method must be implemented!"")\n\n    def clear_list(self):\n        """"""\n        Clears the list so that it is empty.\n        """"""\n        raise NotImplementedError(""This method must be implemented!"")\n\n\n\n@njit\ndef insert_nodes(bins, bin_lls, bin_lengths, non_empty_mask, to_insert, scores, zero_shift):\n    scores -= zero_shift\n    scores = np.abs(scores)\n\n    bin_indices = np.digitize(scores, bins)\n    for j in range(len(scores)):\n        temp_next = to_insert.next_holder\n\n        if non_empty_mask[bin_indices[j]]:\n            to_insert.next_holder = bin_lls[bin_indices[j]]\n        else:\n            to_insert.next_holder = None\n            non_empty_mask[bin_indices[j]] = True\n\n        bin_lengths[bin_indices[j]] += 1\n        bin_lls[bin_indices[j]] = to_insert\n        to_insert = temp_next\n\n\n@njit\ndef get_batch(bin_lls, bin_lengths, non_empty_mask, max_batch_size_to_accept):\n    dummy_root = create_dummy_node_holder()\n    end_node = dummy_root\n    num_found = 0\n\n    temp_dummy_node = create_dummy_node_holder()\n    for bin_index in np.where(non_empty_mask)[0]:\n        end_node, bin_leftover, just_found = append_non_terminating_with_counting(\n            bin_lls[bin_index], end_node, max_batch_size_to_accept - num_found)\n\n        num_found += just_found\n\n        if not bin_leftover is None:\n            bin_lls[bin_index] = bin_leftover\n            non_empty_mask[bin_index] = True\n            bin_lengths[bin_index] = len_node_holder(bin_leftover)\n            break\n\n        bin_lls[bin_index] = temp_dummy_node\n        non_empty_mask[bin_index] = False\n        bin_lengths[bin_index] = 0\n\n        if num_found == max_batch_size_to_accept:\n            break\n\n    return dummy_root.next_holder\n\n\n@njit\ndef pop_all_non_terminating(bin_lls, bin_lengths, non_empty_mask):\n    """"""\n    Set all the bin arrays to empty (by use of a mask), and return an array of all the nodes currently\n    in a bin array that should not terminate.\n    """"""\n    dummy_root = create_dummy_node_holder()\n    end_ll_node = dummy_root\n\n    temp_dummy_node = create_dummy_node_holder()\n    for bin_index in np.where(non_empty_mask)[0]:\n        end_ll_node = append_non_terminating(bin_lls[bin_index], end_ll_node)\n        bin_lls[bin_index] = temp_dummy_node\n\n    bin_lengths[non_empty_mask] = 0\n    non_empty_mask[non_empty_mask] = False\n\n    return dummy_root.next_holder, end_ll_node\n\n\n@njit\ndef insert_and_get_batch(to_insert, scores, bins, bin_ll_holder, bin_lengths, non_empty_mask, max_batch_size_to_accept, zero_shift):\n    own_len = np.sum(bin_lengths)\n\n    # This should not be using self.max_batch_size_to_accept for the initial check (here), instead should probably\n    # be using a value greater than that, because even if 0 nodes are terminating, the time saved will likely\n    # be more than the time spent computing the extra nodes (though it\'s unlikely that 0 nodes will be terminated\n    # in actual play)\n    if len(scores) + own_len < max_batch_size_to_accept:\n        if own_len == 0:\n            dummy_root = create_dummy_node_holder()\n            append_non_terminating(to_insert, dummy_root)\n            return dummy_root.next_holder\n        elif len(scores) == 0:\n            bin_lls = get_list_from_holder_holder(bin_ll_holder)\n            to_return = pop_all_non_terminating(bin_lls, bin_lengths, non_empty_mask)[0]\n        else:\n            bin_lls = get_list_from_holder_holder(bin_ll_holder)\n            to_return, end_node = pop_all_non_terminating(bin_lls, bin_lengths, non_empty_mask)\n            end_node.next_holder = to_insert\n\n        clear_holder_holder(bin_ll_holder)\n        return to_return\n\n    bin_lls = get_list_from_holder_holder(bin_ll_holder)\n\n    insert_nodes(bins, bin_lls, bin_lengths, non_empty_mask, to_insert, scores, zero_shift)\n\n    batch_to_return = get_batch(bin_lls, bin_lengths, non_empty_mask, max_batch_size_to_accept)\n\n    new_holder_holder = get_holder_holder_from_list(bin_lls)\n    bin_ll_holder.held = new_holder_holder.held\n    bin_ll_holder.next = new_holder_holder.next\n\n    return batch_to_return\n\n\n\nclass PriorityBins(GlobalNodeList):\n    def __init__(self, bins, max_batch_size_to_accept, zero_shift=0, save_info=False):\n        num_bins = (len(bins) + 1)\n\n        self.bins = bins[::-1]\n\n        self.bin_lengths = np.zeros(num_bins, dtype=np.int32)\n        self.non_empty_mask = np.zeros(num_bins, dtype=np.bool_)\n\n        temp_dummy_node = create_dummy_node_holder()\n        self.holder_holder = get_holder_holder_from_list([temp_dummy_node for _ in range(num_bins)])\n\n        self.max_batch_size_to_accept = max_batch_size_to_accept\n        self.zero_shift = zero_shift\n\n        self.save_info = save_info\n\n        if save_info:\n            self.reset_logs()\n\n    def reset_logs(self):\n        self.total_in = 0\n        self.total_out = 0\n\n    def __len__(self):\n        return np.sum(self.bin_lengths)\n\n    def is_empty(self):\n        return not np.any(self.non_empty_mask)\n\n    def num_non_empty(self):\n        return np.sum(self.non_empty_mask)\n\n    def largest_bin(self):\n        return np.max(self.bin_lengths)\n\n    def clear_list(self):\n        clear_holder_holder(self.holder_holder)\n\n        self.bin_lengths[self.non_empty_mask] = 0\n        self.non_empty_mask[self.non_empty_mask] = False\n\n    def insert_nodes_and_get_next_batch(self, to_insert, scores):\n        if self.save_info:\n            self.total_in += len(scores)\n\n        to_return = insert_and_get_batch(\n            to_insert,\n            scores,\n            self.bins,\n            self.holder_holder,\n            self.bin_lengths,\n            self.non_empty_mask,\n            self.max_batch_size_to_accept,\n            self.zero_shift)\n\n        if self.save_info and to_return:\n            self.total_out += len_node_holder(to_return)\n\n        return to_return\n\n'"
batch_first/numba_board.py,40,"b'from .classes_and_structs import *\n\n\n\ndef convert_board_to_whites_perspective(ary):\n    """"""\n    NOTES:\n    1) This doesn\'t change any moves which are stored\n    """"""\n    if ary[0][\'turn\']:\n        return ary\n\n    struct = ary[0]\n\n    struct[\'occupied_co\'] = flip_vertically(struct[\'occupied_co\'][::-1])\n    struct[\'occupied\'] = flip_vertically(struct[\'occupied\'])\n\n    struct[\'kings\'] = flip_vertically(struct[\'kings\'])\n    struct[\'queens\'] = flip_vertically(struct[\'queens\'])\n    struct[\'rooks\'] = flip_vertically(struct[\'rooks\'])\n    struct[\'bishops\'] = flip_vertically(struct[\'bishops\'])\n    struct[\'knights\'] = flip_vertically(struct[\'knights\'])\n    struct[\'pawns\'] = flip_vertically(struct[\'pawns\'])\n\n    struct[\'castling_rights\'] = flip_vertically(struct[\'castling_rights\'])\n    struct[\'ep_square\'] = square_mirror(struct[\'ep_square\']) if struct[\'ep_square\'] != NO_EP_SQUARE else NO_EP_SQUARE\n    struct[\'turn\'] = True\n\n    return ary\n\n\n# Obviously needs to be refactored\n@nb.vectorize([nb.uint8(nb.uint64)], nopython=True)\ndef msb(n):\n    r = 0\n    n = n >> 1\n    while n:\n        r += 1\n        n = n >> 1\n    return r\n\n# Obviously needs to be refactored\n@njit\ndef scan_reversed(bb):\n    for index in range(BB_SQUARES.shape[0]):\n        if bb & BB_SQUARES[index]:\n            yield np.uint8(index)\n            bb ^= BB_SQUARES[index]\n    return\n\n\n\npopcount_const_1 = np.uint64(0x5555555555555555)\npopcount_const_2 = np.uint64(0x3333333333333333)\npopcount_const_3 = np.uint64(0x0f0f0f0f0f0f0f0f)\npopcount_const_4 = np.uint64(0x0101010101010101)\n\n@nb.vectorize([nb.uint8(nb.uint64)], nopython=True)\ndef popcount(n):\n    n = n - ((n >> 1) & popcount_const_1)\n    n = (n & popcount_const_2) + ((n >> 2) & popcount_const_2)\n    n = (n + (n >> 4)) & popcount_const_3\n    n = (n * popcount_const_4) >> 56\n    return n\n\n@njit(nb.uint8(nb.uint8))\ndef square_file(square):\n    return square & 7\n\n@njit(nb.uint8(nb.uint8))\ndef square_rank(square):\n    return square >> 3\n\n@njit(nb.uint64(nb.uint64))\ndef shift_down(b):\n    return b >> 8\n\n@njit(nb.uint64(nb.uint64))\ndef shift_up(b):\n    return b << 8\n\nNOT_BB_FILE_A = ~BB_FILE_A\nNOT_BB_FILE_H = ~BB_FILE_H\n\n@njit(nb.uint64(nb.uint64))\ndef shift_right(b):\n    return (b << 1) & NOT_BB_FILE_A\n\n@njit(nb.uint64(nb.uint64))\ndef shift_left(b):\n    return (b >> 1) & NOT_BB_FILE_H\n\n@nb.vectorize([nb.uint8(nb.uint8)], nopython=True)\ndef square_mirror(square):\n    """"""Mirrors the square vertically.""""""\n    return square ^ 0x38\n\n\n@njit\ndef any(iterable):\n    for _ in iterable:\n        return True\n    return False\n\n\nCASTLING_DIFF_SQUARES = np.array([\n    [[H8, G8, F8],\n     [H1, G1, F1]],\n    [[A8, C8, D8],\n     [A1, C1, D1]]], dtype=np.uint64)\n\n\nCASTLING_ZORBRIST_HASH_CHANGES = np.zeros([2, 2, 2], dtype=np.uint64)\nfor j in range(2):\n    for color in COLORS:\n        for pivot in range(2):\n            CASTLING_ZORBRIST_HASH_CHANGES[j, color, pivot] = np.bitwise_xor.reduce(\n                RANDOM_ARRAY[((np.array([ROOK, KING, ROOK], dtype=np.uint64) - 1) * 2 + pivot) * 64 + CASTLING_DIFF_SQUARES[j, color]])\n\n\n\n@njit\ndef piece_type_at(board_state, square):\n    """"""\n    Gets the piece type at the given square.\n    """"""\n    mask = BB_SQUARES[square]\n\n    if not board_state.occupied & mask:\n        return 0\n    elif board_state.pawns & mask:\n        return PAWN\n    elif board_state.knights & mask:\n        return KNIGHT\n    elif board_state.bishops & mask:\n        return BISHOP\n    elif board_state.rooks & mask:\n        return ROOK\n    elif board_state.queens & mask:\n        return QUEEN\n    else:\n        return KING\n\n\n@njit\ndef _remove_piece_at(board_state, square):\n    piece_type = piece_type_at(board_state, square)\n    mask = BB_SQUARES[square]\n\n    if piece_type == PAWN:\n        board_state.pawns ^= mask\n    elif piece_type == KNIGHT:\n        board_state.knights ^= mask\n    elif piece_type == BISHOP:\n        board_state.bishops ^= mask\n    elif piece_type == ROOK:\n        board_state.rooks ^= mask\n    elif piece_type == QUEEN:\n        board_state.queens ^= mask\n    elif piece_type == KING:\n        board_state.kings ^= mask\n    else:\n        return 0\n\n    board_state.occupied ^= mask\n\n    board_state.occupied_co[:] &= ~mask\n\n    return piece_type\n\n\n@njit\ndef _set_piece_at(board_state, square, piece_type, color):\n    _remove_piece_at(board_state, square)\n\n    mask = BB_SQUARES[square]\n\n    if piece_type == PAWN:\n        board_state.pawns |= mask\n    elif piece_type == KNIGHT:\n        board_state.knights |= mask\n    elif piece_type == BISHOP:\n        board_state.bishops |= mask\n    elif piece_type == ROOK:\n        board_state.rooks |= mask\n    elif piece_type == QUEEN:\n        board_state.queens |= mask\n    elif piece_type == KING:\n        board_state.kings |= mask\n\n    board_state.occupied ^= mask\n\n    board_state.occupied_co[color] ^= mask\n\n\n@njit\ndef has_insufficient_material(board_state):\n    # Enough material to mate.\n    if board_state[\'pawns\'] or board_state[\'rooks\'] or board_state[\'queens\']:\n        return False\n\n    # A single knight or a single bishop.\n    elif popcount(board_state[\'occupied\']) <= 3:\n        return True\n\n    # More than a single knight.\n    elif board_state[\'knights\']:\n        return False\n\n    # All bishops on the same color.\n    elif board_state[\'bishops\'] & BB_DARK_SQUARES == 0:\n        return True\n    elif board_state[\'bishops\'] & BB_LIGHT_SQUARES == 0:\n        return True\n\n    return False\n\n\n@njit\ndef is_zeroing(board_state, move_from_square, move_to_square):\n    """"""\n    Checks if the given pseudo-legal move is a capture or pawn move.\n    """"""\n    return np.bool_(BB_SQUARES[move_from_square] & board_state.pawns or BB_SQUARES[move_to_square] & board_state.occupied_co[1 ^ board_state.turn])\n\n\n@njit\ndef _to_chess960_tuple(board_state, move):\n    if move[0] == E1 and board_state.kings & BB_E1:\n        if move[1] == G1 and not board_state.rooks & BB_G1:\n            return E1, H1, 0\n        elif move[1] == C1 and not board_state.rooks & BB_C1:\n            return E1, A1, 0\n    elif move[0] == E8 and board_state.kings & BB_E8:\n        if move[1] == G8 and not board_state.rooks & BB_G8:\n            return E8, H8, 0\n        elif move[1] == C8 and not board_state.rooks & BB_C8:\n            return E8, A8, 0\n    return move[0], move[1], move[2]\n\n\n@njit\ndef push_moves(struct_array, move_array):\n    """"""\n    Pushes the given moves for the given boards (makes the moves), while doing this it also incrementally updates\n    the structs internally stored Zobrist hassh.\n\n\n    :param struct_array: An ndarray with dtype numpy_node_info_dtype.\n    :param move_array: The moves to be pushed, one for each of the structs in struct_array.  It is given as\n    an ndarray with dtype np.uint8 and shape of [len(struct_array), 3] (dimention 2 has size 3 for\n    from_square, to_square, and promotion).\n\n    NOTES:\n    1) While this function doesn\'t take up very much time, speed improvements should be considered a very\n    high priority.  This is because unlike most other functions, the GPU will be idle (or at least very underutilized)\n    during it\'s execution (This is due to it creating data for the GPU to consume)\n        -I have a plan for a staged implementation in TensorFlow to avoid this entirely, but it would require the use\n        of the C++ API, so it may take some time (but then it will also be able to be used from compiled functions).\n         I plan to propose this idea somewhere in the GitHub repository (like in the wiki or issues sections) within\n         the next few days.\n    2) At this time I don\'t believe the big loop in this function is being vectorized by the LLVM compiler.  I think\n    when some refactoring is done this may happen automatically (things like storing occupied_w and occupied_b\n    as an array so it can be indexed with turn).\n    """"""\n    for j in range(len(struct_array)):\n        move_from_square, move_to_square, move_promotion = _to_chess960_tuple(struct_array[j], move_array[j])\n\n        # Reset ep square.\n        ep_square = struct_array[j].ep_square\n        struct_array[j].ep_square = 0\n\n        # reset the ep square in the hash\n        if ep_square:\n            if struct_array[j].turn:\n                ep_mask = shift_down(BB_SQUARES[ep_square])\n            else:\n                ep_mask = shift_up(BB_SQUARES[ep_square])\n\n            if (shift_left(ep_mask) | shift_right(ep_mask)) & struct_array[j].pawns & struct_array[j].occupied_co[struct_array[j].turn]:\n                struct_array[j].hash ^= RANDOM_ARRAY[772 + square_file(ep_square)]\n\n        # Increment move counters.\n        struct_array[j].halfmove_clock += 1\n\n        pivot = 1 if struct_array[j].turn else 0\n\n        # Zero the half move clock.\n        if is_zeroing(struct_array[j], move_from_square, move_to_square):\n            struct_array[j].halfmove_clock = 0\n\n        from_bb = BB_SQUARES[move_from_square]\n        to_bb = BB_SQUARES[move_to_square]\n\n        piece_type = _remove_piece_at(struct_array[j], move_from_square)\n\n        # Remove the piece that\'s being moved from the hash\n        struct_array[j].hash ^= RANDOM_ARRAY[((piece_type - 1) * 2 + pivot) * 64 + move_from_square]\n\n        capture_square = move_to_square\n\n        captured_piece_type = piece_type_at(struct_array[j], capture_square)\n\n        castle_deltas = struct_array[j].castling_rights\n\n        struct_array[j].castling_rights = struct_array[j].castling_rights & ~to_bb & ~from_bb\n\n        castle_deltas ^= struct_array[j].castling_rights\n\n        if castle_deltas:\n            if castle_deltas & BB_A1:\n                struct_array[j].hash ^= RANDOM_ARRAY[768 + 1]\n            if castle_deltas & BB_H1:\n                struct_array[j].hash ^= RANDOM_ARRAY[768]\n            if castle_deltas & BB_A8:\n                struct_array[j].hash ^= RANDOM_ARRAY[768 + 3]\n            if castle_deltas & BB_H8:\n                struct_array[j].hash ^= RANDOM_ARRAY[768 + 2]\n\n        if piece_type == KING:\n            castle_deltas = struct_array[j].castling_rights\n            if struct_array[j].turn:\n                struct_array[j].castling_rights &= ~BB_RANK_1\n                castle_deltas ^= struct_array[j].castling_rights\n                if castle_deltas:\n                    if castle_deltas & BB_A1:\n                        struct_array[j].hash ^= RANDOM_ARRAY[768 + 1]\n                    if castle_deltas & BB_H1:\n                        struct_array[j].hash ^= RANDOM_ARRAY[768]\n            else:\n                struct_array[j].castling_rights &= ~BB_RANK_8\n                castle_deltas ^= struct_array[j].castling_rights\n                if castle_deltas:\n                    if castle_deltas & BB_A8:\n                        struct_array[j].hash ^= RANDOM_ARRAY[768 + 3]\n                    if castle_deltas & BB_H8:\n                        struct_array[j].hash ^= RANDOM_ARRAY[768 + 2]\n\n        if piece_type == PAWN:\n            if move_to_square >= move_from_square:\n                diff = move_to_square - move_from_square\n                if diff == 16:\n                    struct_array[j].ep_square = move_from_square + 8\n                elif ep_square:\n                    if move_to_square == ep_square and diff in [7, 9] and not captured_piece_type:\n                        # Remove pawns captured en passant.\n                        capture_square = ep_square - 8\n                        remove_piece_mask = BB_SQUARES[capture_square]\n\n                        struct_array[j].pawns ^= remove_piece_mask\n                        struct_array[j].occupied ^= remove_piece_mask\n                        struct_array[j].occupied_co[BLACK] &= ~remove_piece_mask\n\n                        # Remove the captured pawn from the Zobrist hash\n                        struct_array[j].hash ^= RANDOM_ARRAY[capture_square]\n            else:\n                diff = move_from_square - move_to_square\n                if diff == 16:\n                    struct_array[j].ep_square = move_from_square - 8\n                elif ep_square:\n                    if move_to_square == ep_square and diff in [7, 9] and not captured_piece_type:\n                        # Remove pawns captured en passant.\n                        capture_square = ep_square + 8\n                        remove_piece_mask = BB_SQUARES[capture_square]\n\n                        struct_array[j].pawns ^= remove_piece_mask\n                        struct_array[j].occupied ^= remove_piece_mask\n                        struct_array[j].occupied_co[WHITE] &= ~remove_piece_mask\n\n                        # Remove the captured pawn from the Zobrist hash\n                        struct_array[j].hash ^= RANDOM_ARRAY[64 + capture_square]\n\n        # Promotion.\n        if move_promotion:\n            piece_type = move_promotion\n\n        # Castling.\n        castling = piece_type == KING and struct_array[j].occupied_co[struct_array[j].turn] & to_bb\n\n        if castling:\n            # This could be using a special implementation since the types of pieces are known\n            # (look up a few lines to pawn removal, for reference)\n            _remove_piece_at(struct_array[j], move_from_square)\n            _remove_piece_at(struct_array[j], move_to_square)\n\n            temp_index1 = np.int8(square_file(move_to_square) < square_file(move_from_square))\n            temp_index2 = struct_array[j].turn\n            for the_square, the_piece in zip(CASTLING_DIFF_SQUARES[temp_index1, temp_index2, 1:], [KING, ROOK]):\n                _set_piece_at(struct_array[j], the_square, the_piece, struct_array[j].turn)\n\n            struct_array[j].hash ^= CASTLING_ZORBRIST_HASH_CHANGES[temp_index1, temp_index2, pivot]\n\n\n        # Put piece on target square.\n        if not castling and piece_type:\n            _set_piece_at(struct_array[j], move_to_square, piece_type, struct_array[j].turn)\n\n            # Put the moving piece in the new location in the hash\n            struct_array[j].hash ^= RANDOM_ARRAY[((piece_type - 1) * 2 + pivot) * 64 + move_to_square]\n\n            if captured_piece_type:\n                struct_array[j].hash ^= RANDOM_ARRAY[\n                    ((captured_piece_type - 1) * 2 + (pivot + 1) % 2) * 64 + move_to_square]\n\n\n        # Swap turn.\n        struct_array[j].turn ^= 1\n        struct_array[j].hash ^= RANDOM_ARRAY[780]\n\n        # set the ep square in the hash\n        if struct_array[j].ep_square:\n            if struct_array[j].turn:\n                ep_mask = shift_down(BB_SQUARES[struct_array[j].ep_square])\n            else:\n                ep_mask = shift_up(BB_SQUARES[struct_array[j].ep_square])\n            if (shift_left(ep_mask) | shift_right(ep_mask)) & struct_array[j].pawns & struct_array[j].occupied_co[struct_array[j].turn]:\n                struct_array[j].hash ^= RANDOM_ARRAY[772 + square_file(struct_array[j].ep_square)]\n\n\n@njit\ndef _attackers_mask(board_state, color, square, occupied):\n    queens_and_rooks = board_state.queens | board_state.rooks\n    queens_and_bishops = board_state.queens | board_state.bishops\n\n    attackers = (\n        (BB_KING_ATTACKS[square] & board_state.kings) |\n        (BB_KNIGHT_ATTACKS[square] & board_state.knights) |\n        (RANK_ATTACK_ARRAY[\n             square,\n             khash_get(RANK_ATTACK_INDEX_LOOKUP_TABLE, BB_RANK_MASKS[square] & occupied, 0)] & queens_and_rooks) |\n        (FILE_ATTACK_ARRAY[\n             square,\n             khash_get(FILE_ATTACK_INDEX_LOOKUP_TABLE, BB_FILE_MASKS[square] & occupied, 0)] & queens_and_rooks) |\n        (DIAG_ATTACK_ARRAY[\n             square,\n             khash_get(DIAG_ATTACK_INDEX_LOOKUP_TABLE, BB_DIAG_MASKS[square] & occupied, 0)] & queens_and_bishops) |\n        (BB_PAWN_ATTACKS[1 ^ color, square] & board_state.pawns))\n\n    return attackers & board_state.occupied_co[color]\n\n\n@njit\ndef attacks_mask(board_state, square):\n    bb_square = BB_SQUARES[square]\n\n    if bb_square & board_state.pawns:\n        return BB_PAWN_ATTACKS[np.int8(bb_square & board_state.occupied_co[WHITE]), square]\n    elif bb_square & board_state.knights:\n        return BB_KNIGHT_ATTACKS[square]\n    elif bb_square & board_state.kings:\n        return BB_KING_ATTACKS[square]\n    else:\n        attacks = np.uint64(0)\n        if bb_square & board_state.bishops or bb_square & board_state.queens:\n            attacks = DIAG_ATTACK_ARRAY[square,\n                khash_get(DIAG_ATTACK_INDEX_LOOKUP_TABLE, BB_DIAG_MASKS[square] & board_state.occupied, 0)]\n        if bb_square & board_state.rooks or bb_square & board_state.queens:\n\n            attacks |= (RANK_ATTACK_ARRAY[square,\n                            khash_get(RANK_ATTACK_INDEX_LOOKUP_TABLE, BB_RANK_MASKS[square] & board_state.occupied,0)] |\n                        FILE_ATTACK_ARRAY[square,\n                            khash_get(FILE_ATTACK_INDEX_LOOKUP_TABLE, BB_FILE_MASKS[square] & board_state.occupied, 0)])\n        return attacks\n\n\n@njit\ndef pin_mask(board_state, color, square):\n    king = msb(board_state.occupied_co[color] & board_state.kings)\n    square_mask = BB_SQUARES[square]\n    for attacks, sliders in zip((FILE_ATTACK_ARRAY, RANK_ATTACK_ARRAY, DIAG_ATTACK_ARRAY), (\n        board_state.rooks | board_state.queens, board_state.rooks | board_state.queens,\n        board_state.bishops | board_state.queens)):\n        rays = attacks[king][0]\n        if rays & square_mask:\n            snipers = rays & sliders & board_state.occupied_co[1 ^ color]\n\n            for sniper in scan_reversed(snipers):\n                if BB_BETWEEN[sniper, king] & (board_state.occupied | square_mask) == square_mask:\n                    return BB_RAYS[king, sniper]\n            break\n\n    return BB_ALL\n\n\n@njit\ndef is_en_passant(board_state, from_square, to_square):\n    """"""\n    Checks if the given pseudo-legal move is an en passant capture.\n    """"""\n    return board_state.ep_square and (\n            board_state.ep_square == to_square and board_state.pawns & BB_SQUARES[from_square] and np.abs(np.int8(\n        to_square - from_square)) in [7, 9] and not board_state.occupied & BB_SQUARES[to_square])\n\n\n@njit\ndef is_castling(board_state, from_square, to_square):\n    """"""\n    Checks if the given pseudo-legal move is a castling move.\n    """"""\n    if board_state.kings & BB_SQUARES[from_square]:\n        from_file = square_file(from_square)\n        to_file = square_file(to_square)\n        if from_file > to_file:\n            diff = from_file - to_file\n        else:\n            diff = to_file - from_file\n\n        return diff > 1 or bool(board_state.rooks & board_state.occupied_co[board_state.turn] & BB_SQUARES[to_square])\n    return False\n\n\n@njit\ndef _ep_skewered(board_state, king, capturer):\n    """"""\n    Handle the special case where the king would be in check, if the pawn and its capturer disappear from the rank.\n\n    Vertical skewers of the captured pawn are not possible. (Pins on the capturer are not handled here.)\n    """"""\n    # only using as workaround, won\'t do this long term\n    if board_state.turn:\n        last_double = board_state.ep_square - 8\n    else:\n        last_double = board_state.ep_square + 8\n\n    occupancy = (board_state.occupied & ~BB_SQUARES[last_double] & ~BB_SQUARES[capturer] | BB_SQUARES[\n        board_state.ep_square])\n\n    # Horizontal attack on the fifth or fourth rank.\n    horizontal_attackers = board_state.occupied_co[1 ^ board_state.turn] & (\n                board_state.rooks | board_state.queens)\n    if RANK_ATTACK_ARRAY[\n        king, khash_get(RANK_ATTACK_INDEX_LOOKUP_TABLE, BB_RANK_MASKS[king] & occupancy, 0)] & horizontal_attackers:\n        return True\n\n    return False\n\n@njit\ndef is_safe(board_state, king, blockers, from_square, to_square):\n    if from_square == king:\n        if is_castling(board_state, from_square, to_square):\n            return True\n        else:\n            return not bool(_attackers_mask(board_state, 1 ^ board_state.turn, to_square, board_state.occupied))\n\n    elif is_en_passant(board_state, from_square, to_square):\n        return (pin_mask(board_state, board_state.turn, from_square) & BB_SQUARES[to_square] and\n                not _ep_skewered(board_state, king, from_square))\n    else:\n        return not blockers & BB_SQUARES[from_square] or BB_RAYS[from_square, to_square] & BB_SQUARES[king]\n\n\n@njit\ndef _slider_blockers(board_state, king):\n    snipers = (((board_state.rooks | board_state.queens) &\n               (RANK_ATTACK_ARRAY[king, 0] | FILE_ATTACK_ARRAY[king, 0])) |\n               (DIAG_ATTACK_ARRAY[king, 0] & (board_state.bishops | board_state.queens)))\n\n    blockers = 0\n    for sniper in scan_reversed(snipers & board_state.occupied_co[1 ^ board_state.turn]):\n        b = BB_BETWEEN[king, sniper] & board_state.occupied\n\n        # Add to blockers if exactly one piece in between.\n        if b and BB_SQUARES[msb(b)] == b:\n            blockers |= b\n\n    return blockers & board_state.occupied_co[board_state.turn]\n\n\n@njit\ndef _attacked_for_king(board_state, path, occupied):\n    for sq in scan_reversed(path):\n        if _attackers_mask(board_state, 1 ^ board_state.turn, sq, occupied):\n            return True\n    return False\n\n\n@njit\ndef _castling_uncovers_rank_attack(board_state, rook_bb, king_to):\n    """"""\n    Test the special case where we castle and our rook shielded us from\n    an attack, so castling would be into check.\n    """"""\n    rank_pieces = BB_RANK_MASKS[king_to] & (board_state.occupied ^ rook_bb)\n    sliders = (board_state.queens | board_state.rooks) & board_state.occupied_co[1 ^ board_state.turn]\n    return RANK_ATTACK_ARRAY[king_to, khash_get(RANK_ATTACK_INDEX_LOOKUP_TABLE, rank_pieces, 0)] & sliders\n\n\n@njit\ndef _from_chess960_tuple(board_state, from_square, to_square):\n    if from_square == E1 and board_state.kings & BB_E1:\n        if to_square == H1:\n            return E1, G1, NO_PROMOTION_VALUE\n        elif to_square == A1:\n            return E1, C1, NO_PROMOTION_VALUE\n    elif from_square == E8 and board_state.kings & BB_E8:\n        if to_square == H8:\n            return E8, G8, NO_PROMOTION_VALUE\n        elif to_square == A8:\n            return E8, C8, NO_PROMOTION_VALUE\n\n    # promotion is set to 0 because this function only gets called when looking for castling moves,\n    # which can\'t have promotions\n    return from_square, to_square, NO_PROMOTION_VALUE\n\n\ndef pseudo_legal_ep_fn_creator(has_legal_move_checker=False):\n    def set_pseudo_legal_ep(board_state, from_mask=BB_ALL, to_mask=BB_ALL, king=0, blockers=0):\n        if not board_state[\'ep_square\']:\n            return False\n\n        if not BB_SQUARES[board_state[\'ep_square\']] & to_mask:\n            return False\n\n        if BB_SQUARES[board_state[\'ep_square\']] & board_state[\'occupied\']:\n            return False\n\n        capturers = (board_state[\'pawns\'] & board_state[\'occupied_co\'][board_state[\'turn\']] & from_mask &\n                     BB_PAWN_ATTACKS[1 ^  board_state[\'turn\'], board_state[\'ep_square\']] &\n                     BB_RANKS[3 + board_state[\'turn\']])\n\n        if has_legal_move_checker:\n            for capturer in scan_reversed(capturers):\n                if is_safe(board_state, king, blockers, capturer, board_state[\'ep_square\']):\n                    return True\n            return False\n        else:\n            for capturer in scan_reversed(capturers):\n                board_state[\'unexplored_moves\'][board_state.children_left] = np.array(\n                    [capturer, board_state[\'ep_square\'], 0],\n                    dtype=np.uint8)\n                board_state[\'children_left\'] += 1\n\n    return njit(set_pseudo_legal_ep)\n\n\nset_pseudo_legal_ep = pseudo_legal_ep_fn_creator()\nhas_pseudo_legal_ep = pseudo_legal_ep_fn_creator(True)\n\n\ndef castling_fn_creator(has_legal_move_checker=False):\n    def set_castling_moves(board_state, from_mask=BB_ALL, to_mask=BB_ALL, blockers=0):\n        backrank = BB_RANK_1 if board_state[\'turn\'] else BB_RANK_8\n\n        king = board_state[\'occupied_co\'][board_state[\'turn\']] & board_state[\'kings\'] & backrank & from_mask\n\n        king &= -king #I think this can be removed\n\n        if not king or _attacked_for_king(board_state, king, board_state[\'occupied\']):\n            return False\n\n        for candidate in scan_reversed(board_state[\'castling_rights\'] & backrank & to_mask):\n            rook = BB_SQUARES[candidate]\n\n            empty_for_rook = np.uint64(0)\n            empty_for_king = np.uint64(0)\n\n            if rook < king:\n                bb_c = BB_FILE_C & backrank\n                bb_d = BB_FILE_D & backrank\n\n                king_to = msb(bb_c)\n                if not rook & bb_d:\n                    empty_for_rook = BB_BETWEEN[candidate, msb(bb_d)] | bb_d\n                if not king & bb_c:\n                    empty_for_king = BB_BETWEEN[msb(king), king_to] | bb_c\n            else:\n                bb_f = BB_FILE_F & backrank\n                bb_g = BB_FILE_G & backrank\n\n                king_to = msb(bb_g)\n                if not rook & bb_f:\n                    empty_for_rook = BB_BETWEEN[candidate, msb(bb_f)] | bb_f\n                if not king & bb_g:\n                    empty_for_king = BB_BETWEEN[msb(king), king_to] | bb_g\n\n            if has_legal_move_checker:\n                if not ((board_state[\'occupied\'] ^ king ^ rook) & (empty_for_king | empty_for_rook) or\n                        _attacked_for_king(board_state, empty_for_king, board_state[\'occupied\'] ^ king) or\n                        _castling_uncovers_rank_attack(board_state, rook, king_to)):\n                    temp_tup = _from_chess960_tuple(board_state, msb(king), candidate)\n                    if is_safe(board_state, king, blockers, temp_tup[0], temp_tup[1]):\n                        return True\n            else:\n                if not ((board_state[\'occupied\'] ^ king ^ rook) & (empty_for_king | empty_for_rook) or\n                        _attacked_for_king(board_state, empty_for_king, board_state[\'occupied\'] ^ king) or\n                        _castling_uncovers_rank_attack(board_state, rook, king_to)):\n                    board_state[\'unexplored_moves\'][board_state.children_left, :] = _from_chess960_tuple(board_state,\n                                                                                                         msb(king),\n                                                                                                         candidate)\n                    board_state[\'children_left\'] += 1\n        return False\n\n    return njit(set_castling_moves)\n\n\nset_castling_moves = castling_fn_creator()\nhas_castling_move = castling_fn_creator(True)\n\n\ndef pseudo_legal_move_fn_creator(has_legal_move_checker=False):\n    def set_pseudo_legal_moves(board_state, from_mask=BB_ALL, to_mask=BB_ALL, king=0, blockers=0):\n        """"""\n        NOTES:\n        1) All of the NumPy array creation when setting the unexplored_moves may or may not have speed penalties,\n        but either way it would be much better to somehow just convince Numba that the values have\n        dtype np.uint8 (which they do!).\n        """"""\n        cur_turn_occupied = board_state[\'occupied_co\'][board_state[\'turn\']]\n        opponent_occupied = board_state[\'occupied_co\'][1 ^ board_state[\'turn\']]\n\n        # Generate piece moves.\n        non_pawns = cur_turn_occupied & ~board_state[\'pawns\'] & from_mask\n\n        for from_square in scan_reversed(non_pawns):\n\n            moves = attacks_mask(board_state, from_square) & ~cur_turn_occupied & to_mask\n            if has_legal_move_checker:\n                for to_square in scan_reversed(moves):\n                    if is_safe(board_state, king, blockers, from_square, to_square):\n                        return True\n            else:\n                for to_square in scan_reversed(moves):\n                    board_state[\'unexplored_moves\'][board_state.children_left, :] = np.array([from_square, to_square, 0],\n                                                                                             dtype=np.uint8)\n                    board_state[\'children_left\'] += 1\n\n        # Generate castling moves.\n        if from_mask & board_state[\'kings\']:\n            if has_legal_move_checker:\n                if has_castling_move(board_state, from_mask, to_mask, blockers=blockers):\n                    return True\n            else:\n                set_castling_moves(board_state, from_mask, to_mask)\n\n        # The remaining moves are all pawn moves.\n        pawns = board_state[\'pawns\'] & cur_turn_occupied & from_mask\n        if not pawns:\n            return False\n\n        # Generate pawn captures.\n        capturers = pawns\n\n        if has_legal_move_checker:\n            for from_square in scan_reversed(capturers):\n                targets = BB_PAWN_ATTACKS[board_state[\'turn\'], from_square] & opponent_occupied & to_mask\n\n                for to_square in scan_reversed(targets):\n                    if is_safe(board_state, king, blockers, from_square, to_square):\n                        return True\n        else:\n            for from_square in scan_reversed(capturers):\n                targets = BB_PAWN_ATTACKS[board_state[\'turn\'], from_square] & opponent_occupied & to_mask\n\n                for to_square in scan_reversed(targets):\n                    if square_rank(to_square) in [0, 7]:\n                        board_state[\'unexplored_moves\'][board_state.children_left:board_state.children_left + 4, 0] = from_square\n                        board_state[\'unexplored_moves\'][board_state.children_left:board_state.children_left + 4, 1] = to_square\n                        board_state[\'unexplored_moves\'][board_state.children_left:board_state.children_left + 4, 2] = (QUEEN, ROOK, BISHOP, KNIGHT)\n                        board_state[\'children_left\'] += 4\n                    else:\n                        board_state[\'unexplored_moves\'][board_state.children_left, :] = np.array(\n                            [from_square, to_square, 0],\n                            dtype=np.uint8)\n                        board_state[\'children_left\'] += 1\n\n        # Prepare pawn advance generation.\n        if board_state[\'turn\']:\n            single_moves = pawns << 8 & ~board_state[\'occupied\']\n            double_moves = single_moves << 8 & ~board_state[\'occupied\'] & (BB_RANK_3 | BB_RANK_4)\n        else:\n            single_moves = pawns >> 8 & ~board_state[\'occupied\']\n            double_moves = single_moves >> 8 & ~board_state[\'occupied\'] & (BB_RANK_6 | BB_RANK_5)\n\n        single_moves &= to_mask\n        double_moves &= to_mask\n\n        if has_legal_move_checker:\n            for to_square in scan_reversed(single_moves):\n                if not board_state[\'turn\']:\n                    from_square = to_square + 8\n                else:\n                    from_square = to_square - 8\n                if is_safe(board_state, king, blockers, from_square, to_square):\n                    return True\n\n            for to_square in scan_reversed(double_moves):\n                if not board_state[\'turn\']:\n                    from_square = to_square + 16\n                else:\n                    from_square = to_square - 16\n\n                if is_safe(board_state, king, blockers, from_square, to_square):\n                    return True\n\n        else:\n            # Generate single pawn moves.\n            for to_square in scan_reversed(single_moves):\n                if not board_state[\'turn\']:\n                    from_square = to_square + 8\n                else:\n                    from_square = to_square - 8\n\n                if square_rank(to_square) in [0, 7]:\n                    board_state[\'unexplored_moves\'][board_state.children_left:board_state.children_left + 4, 0] = from_square\n                    board_state[\'unexplored_moves\'][board_state.children_left:board_state.children_left + 4, 1] = to_square\n                    board_state[\'unexplored_moves\'][board_state.children_left:board_state.children_left + 4, 2] = (QUEEN, ROOK, BISHOP, KNIGHT)\n                    board_state[\'children_left\'] += 4\n                else:\n                    board_state[\'unexplored_moves\'][board_state.children_left, :] = np.array([from_square, to_square, 0],\n                                                                                             dtype=np.uint8)\n                    board_state[\'children_left\'] += 1\n\n            # Generate double pawn moves.\n            for to_square in scan_reversed(double_moves):\n                if not board_state[\'turn\']:\n                    from_square = to_square + 16\n                else:\n                    from_square = to_square - 16\n\n                board_state[\'unexplored_moves\'][board_state.children_left, :] = np.array([from_square, to_square, 0],\n                                                                                         dtype=np.uint8)\n                board_state[\'children_left\'] += 1\n\n\n        if has_legal_move_checker:\n            # Generate en passant captures.\n            if board_state[\'ep_square\']:\n                return has_pseudo_legal_ep(board_state, from_mask, to_mask, king=king, blockers=blockers)\n            return False\n        else:\n            if board_state[\'ep_square\']:\n                set_pseudo_legal_ep(board_state, from_mask, to_mask)\n\n\n    return njit(set_pseudo_legal_moves)\n\nset_pseudo_legal_moves = pseudo_legal_move_fn_creator()\nhas_pseudo_legal_move = pseudo_legal_move_fn_creator(True)\n\n\n@njit\ndef is_pseudo_legal_ep(board_state, from_mask, to_mask):\n    if not board_state[\'ep_square\']:\n        return False\n\n    if not BB_SQUARES[board_state[\'ep_square\']] & to_mask:\n        return False\n\n    if BB_SQUARES[board_state[\'ep_square\']] & board_state[\'occupied\']:\n        return False\n\n    if board_state[\'pawns\'] & board_state[\'occupied_co\'][board_state[\'turn\']] & from_mask & BB_PAWN_ATTACKS[\n        1 ^ board_state[\'turn\'], board_state[\'ep_square\']] & BB_RANKS[3 + board_state[\'turn\']]:\n\n        return True\n    return False\n\n\n@njit\ndef is_pseudo_legal_castling_move(board_state, from_mask, to_mask):\n    backrank = BB_RANK_1 if board_state[\'turn\'] else BB_RANK_8\n\n    king = board_state[\'occupied_co\'][board_state[\'turn\']] & board_state[\'kings\'] & backrank & from_mask\n\n    king &= -king\n\n    if not king or _attacked_for_king(board_state, king, board_state[\'occupied\']):\n        return False\n\n\n    candidates = board_state[\'castling_rights\'] & backrank & to_mask\n    if candidates:\n        candidate = msb(candidates)\n        rook = BB_SQUARES[candidate]\n\n        empty_for_rook = np.uint64(0)\n        empty_for_king = np.uint64(0)\n\n        if rook < king:\n            bb_c = BB_FILE_C & backrank\n            bb_d = BB_FILE_D & backrank\n\n\n            king_to = msb(bb_c)\n            if not rook & bb_d:\n                empty_for_rook = BB_BETWEEN[candidate, msb(bb_d)] | bb_d\n            if not king & bb_c:\n                empty_for_king = BB_BETWEEN[msb(king), king_to] | bb_c\n        else:\n            bb_f = BB_FILE_F & backrank\n            bb_g = BB_FILE_G & backrank\n\n\n            king_to = msb(bb_g)\n            if not rook & bb_f:\n                empty_for_rook = BB_BETWEEN[candidate, msb(bb_f)] | bb_f\n            if not king & bb_g:\n                empty_for_king = BB_BETWEEN[msb(king), king_to] | bb_g\n\n        if not ((board_state[\'occupied\'] ^ king ^ rook) & (empty_for_king | empty_for_rook) or\n                _attacked_for_king(board_state, empty_for_king, board_state[\'occupied\'] ^ king) or\n                _castling_uncovers_rank_attack(board_state, rook, king_to)):\n            return True\n\n    return False\n\n\n@njit\ndef is_pseudo_legal_move(board_state, move):\n    cur_turn_occupied = board_state[\'occupied_co\'][board_state[\'turn\']]\n    opponent_occupied = board_state[\'occupied_co\'][1 ^ board_state[\'turn\']]\n\n    from_mask = BB_SQUARES[move[0]]\n    to_mask = BB_SQUARES[move[1]]\n\n    # Generate piece moves.\n    non_pawns = cur_turn_occupied & ~board_state[\'pawns\'] & from_mask\n    if non_pawns:\n        if attacks_mask(board_state, msb(non_pawns)) & ~cur_turn_occupied & to_mask:\n            if not move[2]:\n                return True\n\n    # Generate castling moves.\n    if from_mask & board_state[\'kings\']:\n        chess960_from_square, chess960_to_square, _ = _to_chess960_tuple(board_state, move)\n        if chess960_to_square != move[1]:\n            if is_pseudo_legal_castling_move(board_state, BB_SQUARES[chess960_from_square], BB_SQUARES[chess960_to_square]):\n                if not move[2]:\n                    return True\n\n    # The remaining possible moves are all pawn moves.\n    pawns = board_state[\'pawns\'] & cur_turn_occupied & from_mask\n    if not pawns:\n        return False\n\n    # Check pawn captures.\n    if board_state[\'turn\']:\n        targets = BB_PAWN_ATTACKS[WHITE, msb(pawns)] & opponent_occupied & to_mask\n        if targets:\n            if square_rank(msb(targets)) in [0,7]:\n                return not move[2] in [0, PAWN, KING]\n            else:\n                return move[2] == 0\n    else:\n        if BB_PAWN_ATTACKS[BLACK, msb(pawns)] & opponent_occupied & to_mask:\n            return True\n\n\n    # Check pawn advance generation.\n    if board_state[\'turn\']:\n        single_moves = pawns << 8 & ~board_state[\'occupied\']\n        if single_moves & to_mask:\n            if square_rank(move[1]) in [0,7]:\n                if not move[2] in [0, PAWN, KING]:\n                    return True\n            else:\n                if not move[2]:\n                    return True\n        if (single_moves << 8 & ~board_state[\'occupied\'] & (BB_RANK_3 | BB_RANK_4)) & to_mask:\n            if not move[2]:\n                return True\n    else:\n        single_moves = pawns >> 8 & ~board_state[\'occupied\']\n        if single_moves & to_mask:\n            if square_rank(move[1]) in [0,7]:\n                if not move[2] in [0, PAWN, KING]:\n                    return True\n            else:\n                if not move[2]:\n                    return True\n        if (single_moves >> 8 & ~board_state[\'occupied\'] & (BB_RANK_6 | BB_RANK_5)) & to_mask:\n            if not move[2]:\n                return True\n\n    # Generate en passant captures.\n    if board_state[\'ep_square\'] and not move[2]:\n        return is_pseudo_legal_ep(board_state, from_mask, to_mask)\n\n    return False\n\n\n@njit\ndef is_evasion(board_state, king, checkers, from_mask, to_mask):\n    """"""\n    NOTES:\n    1) This does NOT check if the move is legal\n    """"""\n    sliders = checkers & (board_state.bishops | board_state.rooks | board_state.queens)\n\n    attacked = np.uint64(0)\n    for checker in scan_reversed(sliders):\n        attacked |= BB_RAYS[king, checker] & ~BB_SQUARES[checker]\n\n    if BB_SQUARES[king] & from_mask:\n        if BB_KING_ATTACKS[king] & ~board_state.occupied_co[board_state.turn] & ~attacked & to_mask:\n            return True\n\n    checker = msb(checkers)\n    if BB_SQUARES[checker] == checkers:\n        # If it captures or blocks a single checker.\n        return ~board_state.kings & from_mask and (BB_BETWEEN[king, checker] | checkers) & to_mask\n\n    return False\n\n\ndef evasions_creator(has_legal_move_checker=False):\n    def set_evasions(board_state, king, checkers, from_mask=BB_ALL, to_mask=BB_ALL, blockers=0):\n        sliders = checkers & (board_state.bishops | board_state.rooks | board_state.queens)\n\n        attacked = np.uint64(0)\n        for checker in scan_reversed(sliders):\n            attacked |= BB_RAYS[king, checker] & ~BB_SQUARES[checker]\n\n        if BB_SQUARES[king] & from_mask:\n            if has_legal_move_checker:\n                for to_square in scan_reversed(BB_KING_ATTACKS[king] & ~board_state.occupied_co[board_state.turn] & ~attacked & to_mask):\n                    if is_safe(board_state, king, blockers, king, to_square):\n                        return True\n            else:\n                for to_square in scan_reversed(BB_KING_ATTACKS[king] & ~board_state.occupied_co[board_state.turn] & ~attacked & to_mask):\n                    board_state[\'unexplored_moves\'][board_state.children_left, 0] = king\n                    board_state[\'unexplored_moves\'][board_state.children_left, 1] = to_square\n                    board_state[\'unexplored_moves\'][board_state.children_left, 2] = 0\n                    board_state[\'children_left\'] += 1\n\n        checker = msb(checkers)\n        if BB_SQUARES[checker] == checkers:\n            # Capture or block a single checker.\n            target = BB_BETWEEN[king][checker] | checkers\n            if has_legal_move_checker:\n                if has_pseudo_legal_move(board_state, ~board_state.kings & from_mask, target & to_mask, king, blockers):\n                    return True\n            else:\n                set_pseudo_legal_moves(board_state, ~board_state.kings & from_mask, target & to_mask)\n\n            # Capture the checking pawn en passant (but avoid yielding duplicate moves).\n            if board_state.ep_square:\n                if not BB_SQUARES[board_state.ep_square] & target:\n                    if not board_state.turn:\n                        last_double = board_state.ep_square + 8\n                    else:\n                        last_double = board_state.ep_square - 8\n                    if last_double == checker:\n                        if has_legal_move_checker:\n                            return has_pseudo_legal_ep(board_state, from_mask, to_mask, king, blockers)\n                        else:\n                            set_pseudo_legal_ep(board_state, from_mask, to_mask)\n        return False\n\n    return njit(set_evasions)\n\n\nset_evasions = evasions_creator()\nhas_evasion = evasions_creator(True)\n\n\n@njit\ndef set_up_move_array(board_struct):\n    king = msb(board_struct[\'kings\'] & board_struct[\'occupied_co\'][board_struct.turn])\n\n    blockers = _slider_blockers(board_struct, king)\n    checkers = _attackers_mask(board_struct, 1 ^ board_struct[\'turn\'], king, board_struct[\'occupied\'])\n\n    # If in check\n    if checkers:\n        set_evasions(board_struct, king, checkers, BB_ALL, BB_ALL)\n    else:\n        set_pseudo_legal_moves(board_struct, BB_ALL, BB_ALL)\n\n    legal_move_index = 0\n    for j in range(board_struct[\'children_left\']):\n        if is_safe(board_struct, king, blockers, board_struct[\'unexplored_moves\'][j, 0], board_struct[\'unexplored_moves\'][j, 1]):\n            board_struct[\'unexplored_moves\'][legal_move_index] = board_struct[\'unexplored_moves\'][j]\n            legal_move_index += 1\n\n    board_struct[\'unexplored_moves\'][legal_move_index:board_struct[\'children_left\'],:] = 255\n    board_struct[\'children_left\'] = legal_move_index\n\n    if not board_struct[\'children_left\']:\n        board_struct[\'terminated\'] = True\n        board_struct[\'best_value\'] = LOSS_RESULT_SCORES[board_struct[\'depth\']] if checkers else TIE_RESULT_SCORE\n\n@njit\ndef set_up_move_arrays(structs):\n    for j in range(len(structs)):\n        set_up_move_array(structs[j])\n\n@njit\ndef has_legal_move(board_struct):\n    king = msb(board_struct[\'kings\'] & board_struct[\'occupied_co\'][board_struct.turn])\n\n    blockers = _slider_blockers(board_struct, king)\n    checkers = _attackers_mask(board_struct, 1 ^ board_struct[\'turn\'], king, board_struct[\'occupied\'])\n\n    # If in check\n    if checkers:\n        if has_evasion(board_struct, king, checkers, BB_ALL, BB_ALL, blockers):\n            return True\n        board_struct[\'best_value\'] = LOSS_RESULT_SCORES[board_struct[\'depth\']]\n\n    else:\n        if has_pseudo_legal_move(board_struct, BB_ALL, BB_ALL, king, blockers):\n            return True\n        board_struct[\'best_value\'] = TIE_RESULT_SCORE\n\n    return False\n\n\n@njit\ndef set_up_move_array_except_move(board_struct, move_to_avoid):\n    king = msb(board_struct[\'kings\'] & board_struct[\'occupied_co\'][board_struct.turn])\n\n    blockers = _slider_blockers(board_struct, king)\n    checkers = _attackers_mask(board_struct, 1 ^ board_struct[\'turn\'], king, board_struct[\'occupied\'])\n\n    # If in check\n    if checkers:\n        set_evasions(board_struct, king, checkers, BB_ALL, BB_ALL)\n    else:\n        set_pseudo_legal_moves(board_struct, BB_ALL, BB_ALL)\n\n    legal_move_index = 0\n    for j in range(board_struct[\'children_left\']):\n        if np.any(move_to_avoid != board_struct[\'unexplored_moves\'][j]):\n            if is_safe(board_struct, king, blockers, board_struct[\'unexplored_moves\'][j, 0], board_struct[\'unexplored_moves\'][j, 1]):\n                board_struct[\'unexplored_moves\'][legal_move_index] = board_struct[\'unexplored_moves\'][j]\n                legal_move_index += 1\n\n    board_struct[\'unexplored_moves\'][legal_move_index:board_struct[\'children_left\'],:] = 255\n    board_struct[\'children_left\'] = legal_move_index\n\n\n@njit\ndef is_into_check(board_scalar, from_square, to_square):\n    """"""\n    Checks if the given move would leave the king in check or put it into\n    check. The move must be at least pseudo legal.  This function was adapted from the Python-Chess version of it.\n    """"""\n    king = msb(board_scalar.occupied_co[board_scalar.turn] & board_scalar.kings)\n\n    checkers = _attackers_mask(board_scalar, 1 ^ board_scalar.turn, king, board_scalar.occupied)\n    if checkers:\n        return not is_evasion(board_scalar, king, checkers, BB_SQUARES[from_square], BB_SQUARES[to_square])\n\n    return not is_safe(board_scalar, king, _slider_blockers(board_scalar, king), from_square, to_square)\n\n\n@njit\ndef is_legal_move(board_scalar, move):\n    return is_pseudo_legal_move(board_scalar, move) and not is_into_check(board_scalar, move[0], move[1])\n\n\n@njit\ndef perft_test_move_gen_helper(struct_array):\n    for j in range(len(struct_array)):\n        king = msb(struct_array[j][\'kings\'] & struct_array[j][\'occupied_co\'][struct_array[j][\'turn\']])\n\n        blockers = _slider_blockers(struct_array[j], king)\n        checkers = _attackers_mask(struct_array[j], 1 ^ struct_array[j][\'turn\'], king, struct_array[j][\'occupied\'])\n\n        if checkers:\n            set_evasions(struct_array[j], king, checkers, BB_ALL, BB_ALL)\n        else:\n            set_pseudo_legal_moves(struct_array[j], BB_ALL, BB_ALL)\n\n        legal_move_index = 0\n        for i in range(struct_array[j][\'children_left\']):\n            if is_safe(struct_array[j], king, blockers, struct_array[j][\'unexplored_moves\'][i, 0], struct_array[j][\'unexplored_moves\'][i, 1]):\n                struct_array[j][\'unexplored_moves\'][legal_move_index] = struct_array[j][\'unexplored_moves\'][i]\n                legal_move_index += 1\n\n        struct_array[j][\'unexplored_moves\'][legal_move_index:struct_array[j][\'children_left\'], :] = 255\n        struct_array[j][\'children_left\'] = legal_move_index\n\n\ndef perft_test(struct_array, depth, print_info=False):\n    if print_info:\n        print(""Starting depth %d with %d nodes.""%(depth, len(struct_array)))\n\n    if not depth:\n        return len(struct_array)\n\n    struct_array[\'unexplored_moves\'] = np.full_like(struct_array[\'unexplored_moves\'], 255)\n    struct_array[\'children_left\'] = np.zeros_like(struct_array[\'children_left\'])\n\n    perft_test_move_gen_helper(struct_array)\n\n\n    if depth == 1:\n        return np.sum(struct_array[\'children_left\'])\n\n    legal_moves = struct_array[\'unexplored_moves\'][struct_array[\'unexplored_moves\'][..., 0] != 255]\n\n    repeated_struct_array = np.repeat(struct_array, struct_array[\'children_left\'])\n\n    #Not sure if these actually provide a speed increase\n    repeated_struct_array = np.ascontiguousarray(repeated_struct_array)\n    legal_moves = np.ascontiguousarray(legal_moves)\n\n    push_moves(repeated_struct_array, legal_moves)\n\n    return perft_test(repeated_struct_array, depth - 1)'"
batch_first/numba_negamax_zero_window.py,43,"b'import threading\nimport time\n\nfrom .numba_board import *\nfrom . import transposition_table as tt\n\nfrom .classes_and_structs import *\n\n\n\n@njit\ndef compress_square_array(to_compress):\n    """"""\n    Given an array of uint8s which all have values less than 16, compress them by storing\n    two values per uint8 (as opposed to one).\n    """"""\n    to_compress[::2] <<= np.uint8(4)\n    to_compress[::2] |= to_compress[1::2]\n    return to_compress[::2]\n\n\n@njit\ndef square_scanner_helper(bb, mirror_squares=False):\n    if mirror_squares:\n        for square in scan_reversed(bb):\n            yield square_mirror(square)\n    else:\n        for square in scan_reversed(bb):\n            yield square\n\n\n@njit\ndef get_square_ary(struct, relevent_square_mask):\n    """"""\n    Creates and returns an array of the ann filters indices corresponding to the given boards \'relevant\' squares.\n    A \'relevant\' square is a square that\'s either occupied or an ep-capture square.\n    """"""\n    squares = np.empty(popcount(relevent_square_mask), np.uint8)\n    for j, square in enumerate(square_scanner_helper(relevent_square_mask, not struct[\'turn\'])):\n        type = piece_type_at(struct, square)\n\n        if type:\n            bb_square = BB_SQUARES[square]\n            squares[j] = 7 if struct[\'occupied_co\'][struct[\'turn\']] & bb_square else 14\n            if struct[\'castling_rights\'] & bb_square == 0:\n                squares[j] -= type\n        else:\n            squares[j] = 0\n\n    return squares\n\n\n@njit\ndef own_concat(lst, total_size):\n    """"""\n    A customized implementation of np.concatenate (mainly used because Numba wouldn\'t let it use np.concatenate for some reason)\n    """"""\n    to_return = np.empty(total_size + (total_size % 2), np.uint8)\n    start_index = 0\n    for to_place in lst:\n        end_index = start_index + len(to_place)\n        to_return[start_index:end_index] = to_place\n        start_index = end_index\n\n    if total_size % 2: #to fix the issues caused by part of the array staying \'empty\'\n        to_return[-1] = 0\n\n    return to_return\n\n\n@njit\ndef struct_array_to_ann_inputs(child_structs, not_child_structs, to_score_child_mask, to_score_not_child_mask, total_num_to_score):\n    """"""\n    Uses ceil((popcnt(occupied)+int(has_ep))/2)*8+64=[80, 200] bits per board transferred.\n    """"""\n    occupied_bbs = np.empty(total_num_to_score, dtype=np.uint64)\n\n    total_squares = 0\n    store_index = 0\n    to_concat = []\n    for j in range(len(child_structs) + len(not_child_structs)):\n        if j < len(child_structs):\n            should_score = to_score_child_mask[j]\n            struct = child_structs[j]\n        else:\n            should_score = to_score_not_child_mask[j - len(child_structs)]\n            struct = not_child_structs[j - len(child_structs)]\n\n        if should_score:\n            occupied_bbs[store_index] = struct[\'occupied\']\n            if struct[\'ep_square\']:\n                occupied_bbs[store_index] |= BB_SQUARES[struct[\'ep_square\']]\n\n            if not struct[\'turn\']:\n                occupied_bbs[store_index] = flip_vertically(occupied_bbs[store_index])\n\n            cur_squares = get_square_ary(struct, occupied_bbs[store_index])\n\n            total_squares += len(cur_squares)\n            to_concat.append(cur_squares)\n\n            store_index += 1\n    compressed_squares = compress_square_array(own_concat(to_concat, total_squares))\n\n    return compressed_squares, occupied_bbs\n\n\n@njit\ndef can_draw_from_repetition(board_struct, parent_node, previous_board_map):\n    """"""\n    NOTES:\n     1) Look into if hash collisions are something thing I need to be checking for (type 1 collisions)\n    """"""\n    if board_struct[\'halfmove_clock\'] < 4:\n        return False\n\n    #Checks if the board is a repitition of a board which was made in the actual game (meaning prior to the current search)\n    hash_index = np.searchsorted(previous_board_map[0], board_struct[\'hash\'])\n    if board_struct[\'hash\'] == previous_board_map[hash_index, 0]:\n        num_found_so_far = 1 + previous_board_map[hash_index, 1]\n    else:\n        num_found_so_far = 1\n\n\n    if parent_node.parent is None:\n        return False\n\n    node = parent_node.parent\n\n    if board_struct[\'hash\'] == node.struct[\'hash\']:\n        num_found_so_far += 1\n\n    if num_found_so_far >= 3:\n        return True\n\n    for num in range(node.struct[\'halfmove_clock\'], 1, -2):\n        if num < 3 and num_found_so_far == 1:\n            return False\n        elif node.parent is None or node.parent.parent is None:\n            return False\n\n        node = node.parent.parent\n\n        if board_struct[\'hash\'] == node.struct[\'hash\']:\n            num_found_so_far += 1\n\n            if num_found_so_far >= 3:\n                return True\n\n    return False\n\n\n@njit\ndef set_up_next_best_move(board_struct):\n    board_struct[\'next_move_index\'] = np.argmax(board_struct[\'unexplored_move_scores\'])\n    best_move_score = board_struct[\'unexplored_move_scores\'][board_struct[\'next_move_index\']]\n    if best_move_score == MIN_FLOAT32_VAL:\n        board_struct[\'next_move_index\'] = NO_MORE_MOVES_VALUE\n    else:\n        board_struct[\'unexplored_move_scores\'][board_struct[\'next_move_index\']] = MIN_FLOAT32_VAL\n    return best_move_score\n\n\ndef start_move_scoring(children, not_children, child_score_mask, not_child_score_mask, move_eval_fn):\n    num_children_to_score = np.sum(child_score_mask)\n    num_not_child_to_score = np.sum(not_child_score_mask)\n\n    result_getter = [None]\n    def set_move_scores():\n        result_getter[0] = move_eval_fn(\n            *struct_array_to_ann_inputs(\n                children,\n                not_children,\n                child_score_mask,\n                not_child_score_mask,\n                num_children_to_score + num_not_child_to_score))\n\n    t = threading.Thread(target=set_move_scores)\n    t.start()\n    return t, result_getter, num_children_to_score, num_not_child_to_score\n\n\ndef start_board_evaluations(struct_array, to_score_mask, board_eval_fn):\n    """"""\n    Start the evaluation of the depth zero nodes which were not previously terminated.\n    """"""\n    num_to_score = np.sum(to_score_mask)\n    evaluation_scores = np.empty(num_to_score, dtype=np.float32)\n\n    def evaluate_and_set():\n        evaluation_scores[:] = board_eval_fn(\n            *struct_array_to_ann_inputs(\n                struct_array,\n                np.array([], dtype=numpy_node_info_dtype),\n                to_score_mask,\n                np.array([], dtype=np.bool_),\n                num_to_score))\n\n    t = threading.Thread(target=evaluate_and_set)\n    t.start()\n\n    return t, evaluation_scores\n\n\n@njit\ndef should_terminate_from_tt(board_struct, hash_table):\n    """"""\n    Checks if the node should be terminated from the information contained in the given hash_table.  It also\n    updates the values in the given node when applicable.\n    """"""\n    hash_entry = hash_table[board_struct[\'hash\'] & TT_HASH_MASK]\n    if hash_entry[\'depth\'] != NO_TT_ENTRY_VALUE:\n        if hash_entry[\'entry_hash\'] == board_struct[\'hash\']:\n            if hash_entry[\'depth\'] >= board_struct[\'depth\']:\n                if hash_entry[\'lower_bound\'] >= board_struct[\'separator\']:\n                    board_struct[\'best_value\'] = hash_entry[\'lower_bound\']\n                    return True\n                else:\n                    if hash_entry[\'upper_bound\'] < board_struct[\'separator\']:\n                        board_struct[\'best_value\'] = hash_entry[\'upper_bound\']\n                        return True\n                    if hash_entry[\'lower_bound\'] > board_struct[\'best_value\']:\n                        board_struct[\'best_value\'] = hash_entry[\'lower_bound\']\n    return False\n\n\n@njit\ndef depth_zero_should_terminate_array(struct_array, hash_table, previous_board_map, node_holder):\n    """"""\n    This function goes through the given struct_array and looks for depth zero nodes which should terminate\n    for reasons other than scoring by the evaluation function.  This is separate from\n    the child_termination_check_and_move_gen function so that it can give the GPU the boards for evaluation as quickly\n    as possible.\n\n\n    Things being checked:\n    1) Draw by the 50-move rule\n    2) Draw by insufficient material\n    3) Draw by stalemate\n    4) Win/loss by checkmate\n    5) Termination by information contained in the TT\n    6) Draw by threefold repetition\n    """"""\n    for j in range(len(struct_array)):\n        if struct_array[j][\'depth\'] == 0:\n            if struct_array[j][\'halfmove_clock\'] >= 50 or has_insufficient_material(struct_array[j]):\n                struct_array[j][\'terminated\'] = True\n                struct_array[j][\'best_value\'] = TIE_RESULT_SCORE\n            elif can_draw_from_repetition(struct_array[j], node_holder.held_node, previous_board_map):\n                # This is just assigning a draw value, though it doesn\'t necessarily imply a draw,\n                # just that one can be claimed.   Not sure if this needs to be handled, and if yes how to handle it\n                struct_array[j][\'terminated\'] = True\n                struct_array[j][\'best_value\'] = TIE_RESULT_SCORE\n            elif should_terminate_from_tt(struct_array[j], hash_table):\n                struct_array[j][\'terminated\'] = True\n            elif not has_legal_move(struct_array[j]):\n                struct_array[j][\'terminated\'] = True\n\n        node_holder = node_holder.next_holder\n\n\n@njit\ndef has_legal_tt_move(board_struct, hash_table):\n    """"""\n    Checks if a move is being stored in the transposition table for the given board struct, and if there is, that the\n    move is legal.  If it does find a legal move, it stores the move in the struct\'s first move index, sets the\n    struct\'s next move score to a constant value, and sets it\'s children_left to a specified constant to indicate there\n    was a legal move found in the tt.\n\n    :return: True if a move is found, or False if not.\n    """"""\n    node_entry = hash_table[board_struct[\'hash\'] & TT_HASH_MASK]\n    if node_entry[\'depth\'] != NO_TT_ENTRY_VALUE:\n        if node_entry[\'entry_hash\'] == board_struct[\'hash\']:\n            if node_entry[\'stored_move\'][0] != NO_TT_MOVE_VALUE:\n                if is_legal_move(board_struct, node_entry[\'stored_move\']):\n                    board_struct[\'unexplored_moves\'][0] = node_entry[\'stored_move\']\n                    board_struct[\'next_move_index\'] = 0\n                    board_struct[\'children_left\'] = NEXT_MOVE_IS_FROM_TT_VAL\n                    return True\n    return False\n\n\n@njit\ndef child_termination_check_and_move_gen(struct_array, hash_table, node_holder, previous_board_map):\n    """"""\n    Things being checked:\n    1) Draw by the 50-move rule\n    2) Draw by insufficient material\n    3) Draw by stalemate\n    4) Win/loss by checkmate\n    5) Termination by information contained in the TT\n    6) Draw by threefold repetition\n    """"""\n    for j in range(len(struct_array)):\n        if struct_array[j][\'depth\'] != 0:\n            if struct_array[j][""halfmove_clock""] >= 50 or has_insufficient_material(struct_array[j]):\n                struct_array[j][\'best_value\'] = TIE_RESULT_SCORE\n                struct_array[j][\'terminated\'] = True\n            elif can_draw_from_repetition(struct_array[j], node_holder.held_node, previous_board_map):\n                # This is just assigning a draw value, though it doesn\'t necessarily imply a draw,\n                # just that one can be claimed.   Not sure if this needs to be handled, and if yes how to handle it\n                struct_array[j][\'terminated\'] = True\n                struct_array[j][\'best_value\'] = TIE_RESULT_SCORE\n            elif should_terminate_from_tt(struct_array[j], hash_table):\n                struct_array[j][\'terminated\'] = True\n            elif has_legal_tt_move(struct_array[j], hash_table):\n                pass\n            else:\n                set_up_move_array(struct_array[j])\n\n        node_holder = node_holder.next_holder\n\n\n@njit\ndef create_child_structs(struct_array):\n    #This should not copy the entire array, instead only the fields which are not directly written over\n    #Also this should not be creating full structs for depth zero nodes, a new dtype will likely need to be created\n    #which has a subset of the current ones fields.\n    child_array = struct_array.copy()\n\n    new_next_move_values = np.empty_like(struct_array[\'best_value\'])\n\n    #This should be removed, and struct_array[\'prev_move\'] should be used instead.  Numba has been very stuborn in resisting this change\n    moves_to_push = np.empty((len(struct_array), 3), dtype=np.uint8)\n\n    for j in range(len(struct_array)):\n        child_array[j][\'unexplored_moves\'][:] = 255\n        child_array[j][\'unexplored_move_scores\'][:] = MIN_FLOAT32_VAL\n        child_array[j][\'prev_move\'][:] = struct_array[j][\'unexplored_moves\'][struct_array[j][\'next_move_index\']]\n        child_array[j][\'depth\'] = struct_array[j][\'depth\'] - 1\n\n        moves_to_push[j] = struct_array[j][\'unexplored_moves\'][struct_array[j][\'next_move_index\']]\n\n        if struct_array[j][\'children_left\'] != NEXT_MOVE_IS_FROM_TT_VAL:\n            new_next_move_values[j] = set_up_next_best_move(struct_array[j])\n        else:\n            new_next_move_values[j] = TT_MOVE_SCORE_VALUE\n\n    push_moves(child_array, moves_to_push)\n\n    child_array[\'best_value\'][:] = MIN_FLOAT32_VAL\n    child_array[\'children_left\'][:] = 0\n    child_array[\'separator\'][:] *= -1\n    child_array[\'next_move_index\'][:] = 255\n\n    return child_array, new_next_move_values\n\n\n@njit\ndef generate_moves_for_tt_move_nodes(struct_array, to_check_mask):\n    """"""\n    Generates the legal moves for the array of board structs when a legal move for the node was found in\n    the transposition table (TT), and then was expanded in the same iteration as this function is being run.\n    It generates all of the legal moves except for the move which was already found in the TT.\n    It then sets each struct\'s children_left to the actual number of children left,\n    as opposed to the indicator value NEXT_MOVE_IS_FROM_TT_VAL which it previously was.\n    """"""\n    for j in range(len(struct_array)):\n        if to_check_mask[j]:\n            struct_array[j][\'children_left\'] = 0\n            set_up_move_array_except_move(struct_array[j], struct_array[j][\'unexplored_moves\'][0].copy())\n            struct_array[j][\'children_left\'] += 1\n\n\n@njit\ndef create_holder_for_structs(struct_array, parent_holder, to_create_mask, starting_holder=None):\n    found = 0\n    for j in range(len(struct_array)):\n        if to_create_mask[j]:\n            starting_holder = GameNodeHolder(GameNode(struct_array[j:j + 1], parent_holder.held_node), starting_holder)\n            found += 1\n        parent_holder = parent_holder.next_holder\n\n    return starting_holder, found\n\n\n@njit\ndef create_new_holders_and_filter_old(root, node_linked_list, have_children_left_mask, child_struct, create_holder_mask):\n    """"""\n    Creates new GameNodes and GameNodeHolders for the given new children, and filters the parents which don\'t need\n    to be given to the open node holder for re-insertion.  Their node holder are connected, and appended to the given root.\n    """"""\n    new_child_nodes, num_new_children = create_holder_for_structs(child_struct, node_linked_list, create_holder_mask)\n\n    num_not_filtered = filter_holders_then_append(root, node_linked_list, have_children_left_mask, new_child_nodes)\n\n    return num_new_children, num_new_children + num_not_filtered\n\n@njit\ndef get_struct_array_from_node_holder(node_holder, length):\n    to_return = np.empty(length, dtype=numpy_node_info_dtype)\n    for j in range(length):\n        to_return[j] = node_holder.struct\n        node_holder = node_holder.next_holder\n    return to_return\n\n\n@njit\ndef update_node_from_value(node, value, following_move, hash_table, new_termination=True):\n    if not node is None:\n        if node.struct[\'terminated\']:\n            if value > node.struct[\'best_value\']:\n                node.struct[\'best_value\'] = value\n\n                tt.add_board_and_move_to_tt(node.struct, following_move, hash_table)\n                update_node_from_value(node.parent, - value, node.struct[\'prev_move\'], hash_table, False)\n        else:\n            if new_termination:\n                node.struct[\'children_left\'] -= 1\n\n            node.struct[\'best_value\'] = np.maximum(value, node.struct[\'best_value\'])\n\n            if node.struct[\'best_value\'] >= node.struct[\'separator\'] or node.struct[\'children_left\'] == 0:\n                node.struct[\'terminated\'] = True\n\n                tt.add_board_and_move_to_tt(node.struct, following_move, hash_table)\n\n                update_node_from_value(node.parent, - node.struct[\'best_value\'], node.struct[\'prev_move\'], hash_table)\n\n\n@njit\ndef update_tree_from_terminating_nodes(parent_node_holder, struct_array, hash_table, was_evaluated_mask, eval_results):\n    """"""\n    Updates the search tree from the nodes in the current batch which are terminating, this includes all nodes which\n    have been marked terminated, or are depth zero.  It also updates the transposition table as needed.\n    """"""\n    should_update_mask = np.logical_or(struct_array[\'depth\'] == 0, struct_array[\'terminated\'])\n\n    if eval_results[0] != MAX_FLOAT32_VAL:  #This would be a None parameter but Numba won\'t let it compile so this is used instead\n        tt.add_evaluated_boards_to_tt(struct_array, was_evaluated_mask, eval_results, hash_table)\n\n        eval_results_for_parents = - eval_results\n\n\n    index_in_evaluations = 0\n    for j in range(len(struct_array)):\n        if was_evaluated_mask[j]:\n            update_node_from_value(\n                parent_node_holder.held_node,\n                eval_results_for_parents[index_in_evaluations],\n                struct_array[j][\'prev_move\'],\n                hash_table)\n            index_in_evaluations += 1\n\n        elif should_update_mask[j]:\n            update_node_from_value(\n                parent_node_holder.held_node,\n                - struct_array[j][\'best_value\'],\n                struct_array[j][\'prev_move\'],\n                hash_table)\n\n        parent_node_holder = parent_node_holder.next_holder\n\n\n@njit\ndef set_nodes_to_altered_structs(node_holder, struct_array, to_do_mask):\n    for j in range(len(struct_array)):\n        if to_do_mask[j]:\n            node_holder.struct[\'unexplored_moves\'][:] = struct_array[j][\'unexplored_moves\'][:]\n            node_holder.struct[\'unexplored_move_scores\'][:] = struct_array[j][\'unexplored_move_scores\'][:]\n\n            node_holder.struct[\'children_left\'] = struct_array[j][\'children_left\']\n            node_holder.struct[\'next_move_index\'] = struct_array[j][\'next_move_index\']\n\n        node_holder = node_holder.next_holder\n\n\n@njit\ndef set_child_move_scores(child_structs, scored_child_mask, scores, score_size_array, cum_sum_sizes):\n    next_move_scores = np.empty(len(score_size_array),dtype=np.float32)\n    num_completed = 0\n    for j in range(len(child_structs)):\n        if scored_child_mask[j]:\n            cur_score_size = score_size_array[num_completed]\n            cur_cum_sum_size = cum_sum_sizes[num_completed]\n\n            child_structs[j][\'unexplored_move_scores\'][:cur_score_size] = scores[cur_cum_sum_size - cur_score_size:cur_cum_sum_size]\n\n            next_move_scores[num_completed] = set_up_next_best_move(child_structs[j])\n            num_completed += 1\n    return next_move_scores\n\n\n@njit\ndef get_move_from_and_filter_squares_and_sizes(child_structs, not_child_structs, child_mask, not_child_mask, num_scored, num_children):\n    size_array = np.empty(num_scored, np.uint8)\n    total_num_children = len(child_structs)\n\n    size_array[:num_children] = child_structs[\'children_left\'][child_mask]\n    size_array[num_children:] = not_child_structs[\'children_left\'][not_child_mask] - 1  #subtracting 1 here to account for the TT move which doesn\'t need to be scored\n\n    move_indices = np.empty((np.sum(size_array), 2), dtype=np.uint8)\n\n    cur_start_index = 0\n    scored_so_far = 0\n    for j in range(len(child_structs) + len(not_child_structs)):\n        if j < len(child_structs):\n            was_scored = child_mask[j]\n            struct = child_structs[j]\n        else:\n            was_scored = not_child_mask[j - total_num_children]\n            struct = not_child_structs[j - total_num_children]\n\n        if was_scored:\n            cur_size = size_array[scored_so_far]\n\n            if struct[\'turn\']:\n                relevant_moves = struct[\'unexplored_moves\'][:cur_size,:2]\n            else:\n                relevant_moves = SQUARES_180[struct[\'unexplored_moves\'][:cur_size,:2].ravel()].reshape((-1, 2))\n\n            move_indices[cur_start_index:cur_start_index + cur_size, 0] = relevant_moves[:, 0]\n\n            #The following loop is needed since Numba can\'t handle more than 1 advanced index\n            for i in range(cur_size):\n                move_indices[cur_start_index + i, 1] = MOVE_FILTER_LOOKUP[relevant_moves[i, 0], relevant_moves[i, 1], struct[\'unexplored_moves\'][i, 2]]\n\n            cur_start_index += cur_size\n            scored_so_far += 1\n\n    return size_array, move_indices\n\n\n@njit\ndef prepare_to_finish_move_scoring(child_structs, adult_structs, scored_child_mask, scored_adult_mask,\n                                   num_scored_children, num_scored_adults):\n    size_array, from_to_squares = get_move_from_and_filter_squares_and_sizes(\n        child_structs,\n        adult_structs,\n        scored_child_mask,\n        scored_adult_mask,\n        num_scored_children + num_scored_adults,\n        num_scored_children)\n\n    cum_sum_sizes = np.cumsum(size_array)\n\n    return size_array, from_to_squares, cum_sum_sizes\n\n\n@njit\ndef complete_move_evaluation(scores, child_structs, adult_nodes, scored_child_mask, scored_adult_mask,\n                             num_children, size_array, cum_sum_sizes):\n    adult_next_move_scores = np.empty(len(size_array) - num_children, dtype=np.float32)\n\n    child_next_move_scores = set_child_move_scores(\n        child_structs,\n        scored_child_mask,\n        scores,\n        size_array[:num_children],\n        cum_sum_sizes[:num_children])\n\n    cur_adult_index = 0\n    for j in range(len(child_structs)):\n        if scored_adult_mask[j]:\n            cur_index = cur_adult_index + num_children\n            cur_size = size_array[cur_index]\n            cur_cum_sum_size = cum_sum_sizes[cur_index]\n            cur_node = adult_nodes.held_node\n\n            cur_node.struct[\'unexplored_move_scores\'][:cur_size] = scores[\n                                                                            cur_cum_sum_size - cur_size: cur_cum_sum_size]\n            adult_next_move_scores[cur_adult_index] = set_up_next_best_move(cur_node.struct)\n\n            cur_adult_index += 1\n\n        adult_nodes = adult_nodes.next_holder\n\n    return child_next_move_scores, adult_next_move_scores\n\n\ndef do_iteration(node_linked_list, hash_table, previous_board_map, board_eval_fn, move_eval_fn):\n    length_of_batch = len_node_holder(node_linked_list)  #this can and should be given to this function\n    struct_batch = get_struct_array_from_node_holder(node_linked_list, length_of_batch)\n\n    child_struct, struct_batch_next_move_scores = create_child_structs(struct_batch)\n\n    child_was_from_tt_move_mask = struct_batch[\'children_left\'] == NEXT_MOVE_IS_FROM_TT_VAL\n\n    depth_zero_children_mask = child_struct[\'depth\'] == 0\n    depth_not_zero_mask = np.logical_not(depth_zero_children_mask)\n\n    depth_zero_should_terminate_array(child_struct, hash_table, previous_board_map, node_linked_list)\n\n    depth_zero_not_scored_mask = np.logical_and(depth_zero_children_mask, np.logical_not(child_struct[\'terminated\']))\n\n    if np.any(depth_zero_not_scored_mask):\n        evaluation_thread, evaluation_scores = start_board_evaluations(\n            child_struct,\n            depth_zero_not_scored_mask,\n            board_eval_fn)\n    else:\n        evaluation_thread = None\n        evaluation_scores = None\n\n\n    generate_moves_for_tt_move_nodes(struct_batch, child_was_from_tt_move_mask)\n\n    not_one_child_left_mask = struct_batch[\'children_left\'] != 1\n\n    not_only_move_was_tt_move_mask = np.logical_or(not_one_child_left_mask, np.logical_not(child_was_from_tt_move_mask))\n    tt_move_nodes_with_more_kids_mask = np.logical_and(not_one_child_left_mask, child_was_from_tt_move_mask)\n\n    child_termination_check_and_move_gen(child_struct, hash_table, node_linked_list, previous_board_map)\n\n    non_zerod_child_not_term_mask = np.logical_and(\n        depth_not_zero_mask,\n        np.logical_not(child_struct[\'terminated\']))\n\n    non_zerod_kids_for_move_scoring_mask = np.logical_and(\n        non_zerod_child_not_term_mask,\n        child_struct[\'children_left\'] != NEXT_MOVE_IS_FROM_TT_VAL)\n\n    # Now that staging has been implemented for move scoring, this must be started as soon as it knows exactly which\n    # nodes have moves to be scored.  This likely involves stopping the move generation when the first move for each\n    # board is discovered, and resuming after the boards which have moves to score have been given to TensorFlow\n    # (or after a thread with that task has been started)\n    if np.any(non_zerod_kids_for_move_scoring_mask) or np.any(tt_move_nodes_with_more_kids_mask):\n        move_thread, move_score_getter, num_children_move_scoring, num_adult_move_scoring = start_move_scoring(\n            child_struct,\n            struct_batch,\n            non_zerod_kids_for_move_scoring_mask,\n            tt_move_nodes_with_more_kids_mask,\n            move_eval_fn)\n    else:\n        move_thread = None\n        child_next_move_scores = None\n        not_child_next_move_scores = None\n\n    # A mask of the given batch which have more unexplored children left\n    have_children_left_mask = np.logical_and(\n        struct_batch[""next_move_index""] != NO_MORE_MOVES_VALUE,\n        not_only_move_was_tt_move_mask)\n\n\n    set_nodes_to_altered_structs(\n        node_linked_list,\n        struct_batch,\n        have_children_left_mask)\n\n    if not evaluation_thread is None:\n        evaluation_thread.join()\n\n    if not move_thread is None:\n        move_completion_info = prepare_to_finish_move_scoring(\n            child_struct,\n            struct_batch,\n            non_zerod_kids_for_move_scoring_mask,\n            tt_move_nodes_with_more_kids_mask,\n            num_children_move_scoring,\n            num_adult_move_scoring)\n\n    update_tree_from_terminating_nodes(\n        node_linked_list,\n        child_struct,\n        hash_table,\n        depth_zero_not_scored_mask,\n        evaluation_scores if not evaluation_scores is None else INT_ARRAY_NONE)\n\n    if not move_thread is None:\n        move_thread.join()\n\n        move_scores = move_score_getter[0](\n            [move_completion_info[1][:, 0],\n             move_completion_info[1][:, 1],\n             move_completion_info[0]])\n\n        child_next_move_scores, not_child_next_move_scores = complete_move_evaluation(\n            scores=move_scores,\n            child_structs=child_struct,\n            adult_nodes=node_linked_list,\n            scored_child_mask=non_zerod_kids_for_move_scoring_mask,\n            scored_adult_mask=tt_move_nodes_with_more_kids_mask,\n            num_children=num_children_move_scoring,\n            size_array=move_completion_info[0],\n            cum_sum_sizes=move_completion_info[2])\n\n    dummy_root = create_dummy_node_holder()\n    num_new_children, num_returning = create_new_holders_and_filter_old(dummy_root, node_linked_list, have_children_left_mask, child_struct, non_zerod_child_not_term_mask)\n    to_return = dummy_root.next_holder\n\n    #Set up the array of scores used to place the returned nodes into their proper bins\n    scores_to_return = np.full(num_returning, TT_MOVE_SCORE_VALUE, dtype=np.float32)\n    num_not_child_scores = num_returning - num_new_children\n    if num_not_child_scores != 0:\n        scores_to_return[:num_not_child_scores] = struct_batch_next_move_scores[have_children_left_mask]\n        if not not_child_next_move_scores is None and len(not_child_next_move_scores) != 0:\n            scores_to_return[:num_not_child_scores][scores_to_return[:num_not_child_scores] == TT_MOVE_SCORE_VALUE] = not_child_next_move_scores\n    if not child_next_move_scores is None and len(child_next_move_scores) != 0:\n            scores_to_return[num_not_child_scores:][child_struct[non_zerod_child_not_term_mask][\'children_left\'] != NEXT_MOVE_IS_FROM_TT_VAL] = child_next_move_scores\n\n\n    return to_return, scores_to_return\n\n\ndef zero_window_negamax_search(root_game_node, open_node_holder, board_eval_fn, move_eval_fn, hash_table,\n                               previous_board_map):\n    next_batch = GameNodeHolder(root_game_node, None)\n    while next_batch:\n        to_insert, to_insert_scores = do_iteration(\n            next_batch, hash_table, previous_board_map, board_eval_fn, move_eval_fn)\n\n        if root_game_node.struct[\'terminated\']:\n            open_node_holder.clear_list()\n            break\n\n        if not to_insert and open_node_holder.is_empty():\n            break\n\n        next_batch = open_node_holder.insert_nodes_and_get_next_batch(to_insert, to_insert_scores)\n\n    return root_game_node.struct[\'best_value\']\n\n\ndef set_up_root_node_for_struct(move_eval_fn, hash_table, previous_board_map, root_struct):\n    if not root_struct[\'turn\']:\n        root_struct = convert_board_to_whites_perspective(root_struct)\n\n    root_node = GameNode(root_struct, None)\n\n    temp_game_node_holder = GameNodeHolder(root_node, None)\n\n    struct_array = root_node.board_struct\n\n    child_termination_check_and_move_gen(\n        struct_array,\n        hash_table,\n        temp_game_node_holder,\n        previous_board_map)\n\n    num_moves_to_score = struct_array[0][\'children_left\']\n    num_moves_to_score_as_array = np.array([num_moves_to_score])\n\n    if struct_array[0][\'terminated\'] or num_moves_to_score == NEXT_MOVE_IS_FROM_TT_VAL:\n        return root_node\n\n    move_thread, move_score_getter, _, _ = start_move_scoring(\n        struct_array,\n        struct_array,\n        np.zeros(1, dtype=np.bool_),\n        np.ones(1, dtype=np.bool_),\n        move_eval_fn)\n\n\n    move_thread.join()\n\n    relevant_moves = struct_array[0][\'unexplored_moves\'][:num_moves_to_score]\n\n    if not struct_array[0][\'turn\']:\n        relevant_moves[:, :2] = SQUARES_180[relevant_moves[:, :2]]\n\n    move_filters = MOVE_FILTER_LOOKUP[relevant_moves[:, 0], relevant_moves[:, 1], relevant_moves[:, 2]]\n    move_from_squares = relevant_moves[:, 0]\n\n    scores = move_score_getter[0]([move_from_squares, move_filters, num_moves_to_score_as_array])\n\n    complete_move_evaluation(\n        scores,\n        struct_array,\n        temp_game_node_holder,\n        np.zeros(1, dtype=np.bool_),\n        np.ones(1, dtype=np.bool_),\n        0,\n        num_moves_to_score_as_array,\n        num_moves_to_score_as_array)\n\n    return root_node\n\n\ndef set_up_root_node_from_fen(move_eval_fn, hash_table, previous_board_map, fen, depth=255, separator=0):\n    return set_up_root_node_for_struct(\n        move_eval_fn,\n        hash_table,\n        previous_board_map,\n        create_node_info_from_fen(fen, depth, separator))\n\n\ndef mtd_f(fen, depth, first_guess, open_node_holder, board_eval_fn, move_eval_fn, hash_table, previous_board_map,\n          guess_increment=.05, print_info=False):\n    """"""\n    Does an mtd(f) search modified to a binary search (this is done to address the granularity of the evaluation network).\n    """"""\n    cur_guess = first_guess\n\n    upper_bound = WIN_RESULT_SCORES[0]\n    lower_bound = LOSS_RESULT_SCORES[0]\n\n    if print_info:\n        counter = 0\n\n    while lower_bound < upper_bound:\n\n        if lower_bound == LOSS_RESULT_SCORES[0]:\n            if upper_bound != WIN_RESULT_SCORES[0]:\n                beta = np.minimum(upper_bound - guess_increment, np.nextafter(upper_bound, MIN_FLOAT32_VAL))\n            else:\n                beta = cur_guess\n        elif upper_bound == WIN_RESULT_SCORES[0]:\n            beta = np.maximum(lower_bound + guess_increment, np.nextafter(lower_bound, MAX_FLOAT32_VAL))\n        else:\n            beta = np.maximum(lower_bound + (upper_bound - lower_bound) / 2, np.nextafter(lower_bound, MAX_FLOAT32_VAL))\n\n        seperator_to_use = np.nextafter(beta, MIN_FLOAT32_VAL)\n\n        # This would ideally share the same tree, but updated for the new separation value\n        cur_root_node = set_up_root_node_from_fen(move_eval_fn, hash_table, previous_board_map, fen, depth, seperator_to_use)\n\n        if cur_root_node.struct[\'terminated\']:\n            cur_guess = cur_root_node.struct[\'best_value\']\n        else:\n            cur_guess = zero_window_negamax_search(\n                cur_root_node,\n                open_node_holder,\n                board_eval_fn,\n                move_eval_fn,\n                hash_table=hash_table,\n                previous_board_map=previous_board_map)\n\n        if cur_guess < beta:\n            upper_bound = cur_guess\n        else:\n            lower_bound = cur_guess\n\n        if print_info:\n            counter += 1\n            print(""Finished iteration %d with lower and upper bounds (%f,%f) after search returned %f"" % (counter, lower_bound, upper_bound, cur_guess))\n\n    tt_move = tt.choose_move(hash_table, cur_root_node, fen.split()[1]==\'b\')\n\n    return cur_guess, tt_move, hash_table\n\n\ndef iterative_deepening_mtd_f(fen, depths_to_search, open_node_holder, board_eval_fn, move_eval_fn, hash_table,\n                              previous_board_map, first_guess=0, guess_increments=None, print_info=False):\n    if guess_increments is None:\n        guess_increments = [.05]*len(depths_to_search)\n\n    if print_info:\n        start_time = time.time()\n\n\n    for depth, increment in zip(depths_to_search, guess_increments):\n        if print_info:\n            print(""Starting depth %d search, with first guess %f""%(depth, first_guess))\n            started_search = time.time()\n\n        first_guess, tt_move, hash_table = mtd_f(\n            fen,\n            depth,\n            first_guess,\n            open_node_holder,\n            board_eval_fn,\n            move_eval_fn,\n            hash_table=hash_table,\n            previous_board_map=previous_board_map,\n            guess_increment=increment,\n            print_info=print_info)\n\n\n        if print_info:\n            print(""Completed depth %d in time %f with value %f\\n""%(depth, time.time() - started_search, first_guess))\n\n\n    if print_info:\n        print(""The nodes processed per second (including repeats) is:"", open_node_holder.total_out/(time.time() - start_time))\n        print(""The number of nodes inserted into the node list (including repeats) was %d, and %d were retrieved.\\n"" % (open_node_holder.total_in, open_node_holder.total_out))\n        open_node_holder.reset_logs()\n\n\n    return first_guess, tt_move, hash_table\n'"
batch_first/transposition_table.py,12,"b'from . import *\n\nfrom .numba_board import square_mirror\n\n\n\nhash_table_numpy_dtype = np.dtype([(""entry_hash"", np.uint64),  #Total of 160 bits per entry\n                                   (""depth"", np.uint8),\n                                   (""upper_bound"", np.float32),\n                                   (""lower_bound"", np.float32),\n                                   (""stored_move"", np.uint8, (3))])\n\nhash_table_numba_dtype = nb.from_dtype(hash_table_numpy_dtype)\n\n\nblank_tt_entry = np.array([(\n        0,\n        NO_TT_ENTRY_VALUE,\n        MAX_FLOAT32_VAL,\n        MIN_FLOAT32_VAL,\n        np.full(3, NO_TT_MOVE_VALUE, dtype=np.uint8))], dtype=hash_table_numpy_dtype)[0]\n\n\n\ndef get_empty_hash_table():\n    return np.full(2**SIZE_EXPONENT_OF_TWO_FOR_TT_INDICES, blank_tt_entry)\n\n\n@njit\ndef clear_hash_table(table):\n    table[table[\'depth\'] != NO_TT_ENTRY_VALUE] = blank_tt_entry\n    return table\n\n\ndef choose_move(hash_table, node, flip_move=False):\n    """"""\n    Chooses the desired move to be made from the given node.  This is done by use of the given hash table.\n\n    :return: A python-chess Move object representing the desired move to be made\n    """"""\n    root_tt_entry = hash_table[np.uint64(node.struct[\'hash\']) & TT_HASH_MASK]\n    move_array = root_tt_entry[\'stored_move\']\n\n    if flip_move:\n        move_array[:-1] = square_mirror(move_array[:-1])\n\n    return chess.Move(\n        move_array[0].view(np.int8),\n        move_array[1].view(np.int8),\n        None if move_array[2]==0 else move_array[2].view(np.int8))\n\n\n@nb.njit\ndef set_tt_node(hash_entry, board_hash, depth, overwrite_hash=True, overwrite_bounds=False,\n                upper_bound=MAX_FLOAT32_VAL, lower_bound=MIN_FLOAT32_VAL):\n    """"""\n    Puts the given information about a node into the hash table.\n    """"""\n    hash_entry[\'depth\'] = depth\n\n    if overwrite_hash:\n        hash_entry[\'entry_hash\'] = board_hash\n\n    if upper_bound != MAX_FLOAT32_VAL or overwrite_bounds:\n        hash_entry[\'upper_bound\'] = upper_bound\n    if lower_bound != MIN_FLOAT32_VAL or overwrite_bounds:\n        hash_entry[\'lower_bound\'] = lower_bound\n\n\n@nb.njit\ndef set_tt_move(hash_entry, following_move):\n    """"""\n    Write the given move in the given hash table, at the given index.\n    """"""\n    hash_entry[\'stored_move\'][:] = following_move\n\n\n@nb.njit\ndef wipe_tt_move(hash_entry):\n    """"""\n    Writes over the move in the given hash_table at the given index.  It sets all the move values to NO_TT_MOVE_VALUE.\n    """"""\n    hash_entry[\'stored_move\'][:] = NO_TT_MOVE_VALUE\n\n\n@nb.njit\ndef add_board_and_move_to_tt(board_struct, following_move, hash_table):\n    """"""\n    Adds the information about a current board and the move which was made previously, to the\n    transposition table.\n\n    NOTES:\n    1) While this currently does work, it represents one of the most crucial components of the negamax search,\n    and has not been given the thought and effort it needs.  This is an extremely high priority\n    """"""\n    node_entry = hash_table[board_struct[\'hash\'] & TT_HASH_MASK]\n    if node_entry[\'depth\'] != NO_TT_ENTRY_VALUE:\n        if node_entry[\'entry_hash\'] == board_struct[\'hash\']:\n            if node_entry[\'depth\'] == board_struct[\'depth\']:\n                if board_struct[\'best_value\'] >= board_struct[\'separator\']:\n                    if board_struct[\'best_value\'] > node_entry[\'lower_bound\']:\n                        node_entry[\'lower_bound\'] = board_struct[\'best_value\']\n                        set_tt_move(node_entry, following_move)\n\n                elif board_struct[\'best_value\'] < node_entry[\'upper_bound\']:\n                    node_entry[\'upper_bound\'] = board_struct[\'best_value\']\n\n            elif node_entry[\'depth\'] < board_struct[\'depth\']:\n                # Overwrite the data currently stored in the hash table\n                if board_struct[\'best_value\'] >= board_struct[\'separator\']:\n                    set_tt_move(node_entry, following_move)\n                    set_tt_node(node_entry, board_struct[\'hash\'], board_struct[\'depth\'],\n                                lower_bound=board_struct[\'best_value\'], overwrite_hash=False, overwrite_bounds=True)\n                else:\n                    set_tt_node(node_entry, board_struct[\'hash\'], board_struct[\'depth\'],\n                                upper_bound=board_struct[\'best_value\'], overwrite_hash=False, overwrite_bounds=True)\n            # Don\'t change anything if it\'s depth is less than the depth in the TT\n        else:\n            # Using the always replace scheme for simplicity and easy implementation (likely only for now)\n            if board_struct[\'best_value\'] >= board_struct[\'separator\']:\n                set_tt_move(node_entry, following_move)\n                set_tt_node(node_entry, board_struct[\'hash\'], board_struct[\'depth\'],\n                            lower_bound=board_struct[\'best_value\'], overwrite_bounds=True)\n            else:\n                wipe_tt_move(node_entry)\n                set_tt_node(node_entry, board_struct[\'hash\'], board_struct[\'depth\'],\n                            upper_bound=board_struct[\'best_value\'], overwrite_bounds=True)\n    else:\n        if board_struct[\'best_value\'] >= board_struct[\'separator\']:\n            set_tt_move(node_entry, following_move)\n            set_tt_node(node_entry, board_struct[\'hash\'], board_struct[\'depth\'], lower_bound=board_struct[\'best_value\'])\n        else:\n            set_tt_node(node_entry, board_struct[\'hash\'], board_struct[\'depth\'], upper_bound=board_struct[\'best_value\'])\n\n\n\n\n@nb.njit\ndef add_evaluated_boards_to_tt(struct_array, was_evaluated_mask, eval_results, hash_table):\n    num_done = 0\n    for j in range(len(struct_array)):\n        if was_evaluated_mask[j]:\n            node_entry = hash_table[struct_array[j][\'hash\'] & TT_HASH_MASK]\n            if node_entry[\'depth\'] == NO_TT_ENTRY_VALUE:\n                cur_result = eval_results[num_done]\n                set_tt_node(node_entry, struct_array[j][\'hash\'], 0, lower_bound=cur_result, upper_bound=cur_result)\n\n            num_done += 1'"
batch_first/anns/ann_creation_helper.py,12,"b'import tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.contrib import layers\nfrom tensorflow.python import training\n\nimport chess\n\nfrom functools import reduce\n\nfrom google.protobuf import text_format\n\nfrom batch_first.numba_board import popcount\n\n\nfrom tensorflow.contrib import tensorrt as trt\n\n\n\n\ndef parse_into_ann_input_inference(max_boards, convert_to_nhwc=False):\n    """"""\n    NOTES:\n    1) If a constant/operation is typed in a confusing manor, it\'s so the entirely of this can be done on GPU\n    """"""\n    possible_lookup_nums = np.arange(2 ** 16, dtype=np.uint16)\n    num_bits = popcount(possible_lookup_nums.astype(np.uint64))\n\n    location_lookup_ary = np.array([[[chess.square_rank(loc), chess.square_file(loc)] for loc in chess.SQUARES_180]], np.int32)\n    location_lookup_ary = np.ones([max_boards, 1, 1], np.int32) * location_lookup_ary\n\n    location_lookup_ary = location_lookup_ary.reshape([max_boards, 8, 8, 2])[:, ::-1]\n    location_lookup_ary = location_lookup_ary.reshape([max_boards, 4, 16, 2])\n\n    mask_getter = lambda n: np.unpackbits(np.frombuffer(n, dtype=np.uint8)[::-1])[::-1]\n    masks_to_gather_ary = np.array(list(map(mask_getter, possible_lookup_nums)), dtype=np.bool_)\n\n    pieces_from_nums = lambda n: [n >> 4, (n & np.uint8(0x0F))]\n    piece_lookup_ary = np.array(list(map(pieces_from_nums, possible_lookup_nums)), dtype=np.int32)\n\n    range_repeater = numpy_style_repeat_1d_creator(max_multiple=33, max_to_repeat=max_boards, out_type=tf.int64)\n\n    popcount_lookup = tf.constant(num_bits, tf.int64)\n    locations_for_masking = tf.constant(location_lookup_ary, tf.int64)\n    occupancy_mask_table = tf.constant(masks_to_gather_ary, tf.half)\n    piece_lookup_table = tf.constant(piece_lookup_ary, tf.int64)\n\n    ones_to_slice = tf.constant(np.ones(33 * max_boards), dtype=tf.float32)  # This is used since there seems to be no simple/efficient way to broadcast for scatter_nd\n\n    piece_indicators = tf.placeholder(tf.int32, shape=[None], name=""piece_filters"")  #Given as an array of uint8s\n    occupied_bbs = tf.placeholder(tf.int64, shape=[None], name=""occupied_bbs"")       #Given as an array of uint64s\n\n    # The code below this comment defines ops which are run during inference\n\n    occupied_bitcasted = tf.cast(tf.bitcast(occupied_bbs, tf.uint16), dtype=tf.int32)\n\n    partial_popcounts = tf.gather(popcount_lookup, occupied_bitcasted, ""byte_popcount_loopkup"")\n    partial_popcounts = tf.cast(partial_popcounts, tf.int32)\n    occupied_popcounts = tf.reduce_sum(partial_popcounts, axis=-1, name=""popcount_lookup_sum"")\n\n    location_mask = tf.gather(occupancy_mask_table, occupied_bitcasted, ""gather_location_mask"")\n    location_mask = tf.cast(location_mask, tf.bool)\n    piece_coords = tf.boolean_mask(locations_for_masking, location_mask, ""mask_desired_locations"")\n\n    gathered_pieces = tf.gather(piece_lookup_table, piece_indicators, ""gather_pieces"")\n    piece_filter_indices = tf.reshape(gathered_pieces, [-1, 1])\n\n    repeated_board_numbers = range_repeater(occupied_popcounts)\n    board_numbers_for_concat = tf.expand_dims(repeated_board_numbers, -1)\n\n    # Removes either the last piece filter, or no filters (based on if the number of filters was odd and half of the final uint8 was padding)\n    piece_filter_indices = piece_filter_indices[:tf.shape(board_numbers_for_concat)[0]]\n\n    one_indices = tf.concat([board_numbers_for_concat, piece_filter_indices, piece_coords], axis=-1) #Should figure out how this can be done with (or similarly to) tf.parallel_stack\n\n    boards = tf.scatter_nd(\n        indices=one_indices,\n        updates=ones_to_slice[:tf.shape(one_indices)[0]],\n        shape=[tf.shape(occupied_bbs, out_type=tf.int64)[0], 15, 8, 8])\n\n    if convert_to_nhwc:\n        boards = tf.transpose(boards, [0,2,3,1])\n\n    return (piece_indicators, occupied_bbs), boards\n\n\ndef vec_and_transpose_op(vector, operation, output_type=None):\n    """"""\n    Equivalent to running tf.cast(operation(tf.expand_dims(vector, 1), tf.expand_dims(vector, 0)), output_type)\n\n    :param output_type: An optional parameter, if given the tensor will be cast to the given type before being returned\n    """"""\n    to_return = operation(tf.expand_dims(vector, 1), tf.expand_dims(vector, 0))\n    if not output_type is None:\n        return tf.cast(to_return, output_type)\n    return to_return\n\n\ndef kendall_rank_correlation_coefficient(logits, labels):\n    """"""\n    A function to calculate Kendall\'s Tau-a rank correlation coefficient.\n\n    NOTES:\n    1) This TensorFlow implementation is extremely fast for small quantities, but scales poorly (It\'s O[N^2]).\n     The intended use is during training, to avoid running non-graph operations and keep computations on GPU.\n    """"""\n    quantity = tf.shape(logits, out_type=tf.float32)[0]\n\n    diffs_sign_helper = lambda t : tf.sign(vec_and_transpose_op(t, tf.subtract, tf.float32))\n\n    sign_product = diffs_sign_helper(logits) * diffs_sign_helper(labels)\n    concordant_minus_discordant = tf.reduce_sum(tf.matrix_band_part(sign_product, -1, 0))\n\n    return concordant_minus_discordant/(quantity*(quantity-1)/2)\n\n\ndef py_func_scipy_rank_helper_creator(logits, labels):\n    """"""\n    A function to make it easier to use SciPy rank correlation functions from within a TensorFlow graph.\n    """"""\n    def helper_to_return(function):\n        return tf.py_func(\n            function,\n            [logits, labels],\n            [tf.float64, tf.float64],\n            stateful=False)\n    return helper_to_return\n\n\ndef combine_graphdefs(graphdef_filenames, output_model_path, output_filename, output_node_names, name_prefixes=None):\n    if name_prefixes is None:\n        name_prefixes = len(graphdef_filenames) * [""""]\n\n    with tf.Session() as sess:\n        for filename, prefix in zip(graphdef_filenames, name_prefixes):\n            tf.saved_model.loader.load(sess, [\'serve\'], filename, import_scope=prefix)\n\n        constant_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess,\n            tf.get_default_graph().as_graph_def(),\n            [""%s/%s""%(prefix,name) for name, prefix in zip(output_node_names, name_prefixes)])\n\n        tf.train.write_graph(\n            constant_graph_def,\n            output_model_path,\n            output_filename)\n\n\ndef remap_inputs(model_path, output_model_path, output_filename, max_batch_size=None):\n    with tf.Session() as sess:\n        with tf.device(\'/GPU:0\'):\n            with tf.name_scope(""input_parser""):\n                placeholders, formatted_data = parse_into_ann_input_inference(max_batch_size)\n\n        with open(model_path, \'r\') as f:\n            graph_def = text_format.Parse(f.read(), tf.GraphDef())\n\n        tf.import_graph_def(\n            graph_def,\n            input_map={""policy_network/FOR_INPUT_MAPPING_transpose"" : formatted_data,\n                       ""value_network/FOR_INPUT_MAPPING_transpose"" : formatted_data},\n            name="""")\n\n        tf.train.write_graph(\n            sess.graph_def,\n            output_model_path,\n            output_filename,\n            as_text=True)\n\n\ndef save_trt_graphdef(model_path, output_model_path, output_filename, output_node_names,\n                      trt_memory_fraction=.5, total_video_memory=1.1e10,\n                      max_batch_size=1000, write_as_text=True):\n\n    #This would ideally be 1 instead of .85, but the GPU that this is running on is responsible for things like graphics\n    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=.85 - trt_memory_fraction))) as sess:\n\n        with open(model_path, \'r\') as f:\n            txt = f.read()\n            model_graph_def = text_format.Parse(txt, tf.GraphDef())\n\n        trt_graph = trt.create_inference_graph(\n            model_graph_def,\n            output_node_names,\n            max_batch_size=max_batch_size,\n            precision_mode=""FP32"",\n            max_workspace_size_bytes=int(trt_memory_fraction*total_video_memory))\n\n        tf.train.write_graph(trt_graph, output_model_path, output_filename, as_text=write_as_text)\n\n\ndef create_input_convolutions_shared_weights(the_input, kernel_initializer, data_format, mode,\n                                             undilated_kernels=64, num_unique_filters=[32,32,24,24,20,20,24,24]):\n    """"""\n    This function creates a set of 9 convolutional layers which together model the movement of chess pieces (more\n    information about this is in the README).  After concatenation of the convolutions, batch normalization\n    and ReLu are applied.\n\n\n    :param data_format: A string of either ""NHWC"" or ""NCHW""\n    :param undilated_kernels: The number of filters to use in the one undilated 3x3 convolution (including those\n     shared with the dilated filters)\n    :param num_unique_filters: An iterable of the number of filters to be used for the 8 dilated convolutions (dilation 2-7, knight filters)\n    :return: The concatenated output of the convolutions (after bach normalization and ReLu)\n    """"""\n    with tf.variable_scope(""input_module""):\n        channel_axis = 3 if data_format == ""NHWC"" else 1\n        layer_channel_str = ""channels_last"" if data_format == ""NHWC"" else ""channels_first""\n\n        path_outputs = [\n            tf.layers.conv2d(the_input,\n                             undilated_kernels,\n                             kernel_size=3,\n                             padding=\'same\',\n                             data_format=layer_channel_str,\n                             use_bias=False,\n                             kernel_initializer=kernel_initializer(),\n                             name=""3x3_adjacency_filter"")]\n\n\n        dilations = [[d,d] for d in range(2,8)] + [(2,4),(4,2)]\n        filter_sizes = (len(dilations) - 2) * [3] + 2 * [2]\n\n        for j, (rate, filter_size, num_filters) in enumerate(zip(dilations, filter_sizes, num_unique_filters)):\n            path_outputs.append(\n                tf.layers.conv2d(the_input,\n                                 num_filters,\n                                 filter_size,\n                                 padding=\'same\',\n                                 data_format=layer_channel_str,\n                                 dilation_rate=rate,\n                                 use_bias=False,\n                                 kernel_initializer=kernel_initializer(),\n                                 name=""input_%dx%d_with_%dx%d_dilation"" % (filter_size, filter_size, rate[0], rate[1])\n                                 )\n            )\n\n        all_convs = tf.concat(path_outputs, axis=channel_axis)\n\n        batch_normalized = tf.layers.batch_normalization(\n            all_convs,\n            axis=channel_axis,\n            scale=False,\n            training=(mode == tf.estimator.ModeKeys.TRAIN),\n            trainable=True,\n            fused=True)\n\n        convolutional_module_outputs = tf.nn.relu(batch_normalized, name=\'first_layers_relu\')\n\n    tf.contrib.layers.summarize_activation(convolutional_module_outputs)\n\n    return convolutional_module_outputs\n\n\n\ndef build_convolutional_module_with_batch_norm(the_input, module, kernel_initializer, mode,\n                                               num_previously_built_inception_modules=0, make_trainable=True,\n                                               weight_regularizer=None, data_format=""NHWC""):\n    """"""\n    Builds a convolutional module based on a given design using batch normalization and the rectifier activation.\n    It returns the final layer/layers in the module.\n\n    The following are a few examples of what can be used in the \'module\' parameter (explanation follows):\n\n    example_1_module = [[[35,1], (1024, 8)]]\n\n    example_2_module = [[[30, 1]],\n                        [[15, 1], [30, 3]],\n                        [[15, 1], [30, 3, 2]],\n                        [[15, 1], [30, 3, 3]],        #  <-- This particular path does a 1x1 convolution on the module\'s\n                        [[10, 1], [20, 3, 4]],        #      input with 15 filters, followed by a 3x3 \'same\' padded\n                        [[8, 1],  [16, 3, 5]],        #      convolution with dilation factor of 3.  It is concatenated\n                        [[8, 1],  [16, 2, (2, 4)]],   #      with the output of the other paths, and then returned\n                        [[8, 1],  [16, 2, (4, 2)]]]\n\n\n    :param module: A list (representing the module\'s shape), of lists (each representing the shape of a \'path\' from the\n    input to the output of the module), of either tuples or lists of size 2 or 3 (representing individual layers).\n    If a tuple is used, it indicates that the layer should use \'valid\' padding, and if a list is used it will use a\n    padding of \'same\'.  The contents of the innermost list or tuple will be the number of filters to create for a layer,\n    followed by the information to pass to conv2d as kernel_size, and then optionally, a third element which is\n    to be passed to conv2d as a dilation factor.\n    """"""\n    if weight_regularizer is None:\n        weight_regularizer = lambda:None\n\n    path_outputs = [None for _ in range(len(module))]\n    to_summarize = []\n    cur_input = None\n    for j, path in enumerate(module):\n        with tf.variable_scope(""module_"" + str(num_previously_built_inception_modules + 1) + ""/path_"" + str(j + 1)):\n            for i, section in enumerate(path):\n                if i == 0:\n                    if j != 0:\n                        path_outputs[j - 1] = cur_input\n\n                    cur_input = the_input\n\n                cur_conv_output = tf.layers.conv2d(\n                    inputs=cur_input,\n                    filters=section[0],\n                    kernel_size=section[1],\n                    padding=\'valid\' if isinstance(section, tuple) else \'same\',\n                    dilation_rate = 1 if len(section) < 3 else section[-1],\n                    use_bias=False,\n                    kernel_initializer=kernel_initializer(),\n                    kernel_regularizer=weight_regularizer(),\n                    trainable=make_trainable,\n                    data_format=""channels_last"" if data_format == ""NHWC"" else ""channels_first"",\n                    name=""layer_"" + str(i + 1))\n\n                cur_batch_normalized = tf.layers.batch_normalization(cur_conv_output,\n                                                                     axis=-1 if data_format == ""NHWC"" else 1,\n                                                                     scale=False,\n                                                                     training=(mode == tf.estimator.ModeKeys.TRAIN),\n                                                                     trainable=make_trainable,\n                                                                     fused=True)\n\n                cur_input = tf.nn.relu(cur_batch_normalized)\n\n                to_summarize.append(cur_input)\n\n    path_outputs[-1] = cur_input\n\n    list(layers.summarize_activation(layer) for layer in to_summarize)\n\n    with tf.variable_scope(""module_"" + str(num_previously_built_inception_modules + 1)):\n        if len(path_outputs) == 1:\n            return path_outputs[0]\n\n        return tf.concat([temp_input for temp_input in path_outputs], -1 if data_format == ""NHWC"" else 1)\n\n\ndef build_convolutional_modules(input, modules, mode, kernel_initializer, kernel_regularizer, make_trainable=True,\n                                num_previous_modules=0, data_format=""NHWC""):\n    """"""\n    Creates a desired set of convolutional modules.  Primarily the modules are created through the\n    build_convolutional_module_with_batch_norm function, but if it\'s functionality proves insufficient, a function can be\n    given in place of a module shape, which will accept the previous modules output as input, and who\'s output will\n    be given to the next module for input (or returned). (A detailed example is given below, and more information about the\n    shape of a module can be found in the comment for the build_convolutional_module_with_batch_norm function)\n\n\n    Below is an example of what may be given for the \'modules\' parameter, along with a detailed explanation of what\n    each component does.\n\n\n    #Assume the input is such that the following holds true\n    tf.shape(input) == [-1, 8, 8, 15]\n\n\n    modules = [\n        [[[20, 3]],           # The following 7 layers with 3x3 kernels and increasing dilation factors are such that\n         [[20, 3, 2]],        # their combined filters centered at any given square will consider only the squares in\n         [[20, 3, 3]],        # which a queen could possibly attack from, and the central square itself (and padding)\n         [[20, 3, 4]],\n         [[10, 3, 5]],\n         [[10, 3, 6]],\n         [[10, 3, 7]],\n         [[8, 2, (2, 4)]],     # For any given square, this and the following layer collectively consider all possible\n         [[8, 2, (4, 2)]]],    # squares in which a knight could attack from (and padding when needed),\n\n        # After the module\'s 9 input layers are created, their outputs are concatenated together, and the module\'s\n        # output will have the shape : [-1, 8, 8, 126]\n\n\n        # Now a module is created which applies a 1x1 convolution, followed by a an 8x8 convolution with valid padding\n        [[[32,1], (1024, 8)]],\n\n\n        # To prepare the outputs from the convolutional modules for the fully connected layers, they are reshaped from\n        # rank 4 and shape  [-1, 1, 1, 1024], to rank 2 with shape [-1, 1024]\n        lambda x: tf.reshape(x, [-1, 1024]),\n    ]\n\n    #The shape [-1, 1024] tensor would then be returned by this function.\n\n\n\n    :param modules: A list which sequentially represents the graph operations to be created.  It may contain\n     any combination of either functions which accept the previous module\'s output and return the desired input for\n     the next module, or shapes of modules which can be given to the build_convolutional_module_with_batch_norm method\n    """"""\n    if isinstance(make_trainable, bool):\n        make_trainable = [make_trainable]*len(modules)\n\n    cur_inception_module = input\n\n    for module_num, (module_shape, trainable) in enumerate(zip(modules, make_trainable)):\n        if callable(module_shape):\n            cur_inception_module = module_shape(cur_inception_module)\n        else:\n            cur_inception_module = build_convolutional_module_with_batch_norm(\n                cur_inception_module,\n                module_shape,\n                kernel_initializer,\n                mode,\n                make_trainable=trainable,\n                num_previously_built_inception_modules=module_num + num_previous_modules,\n                weight_regularizer=kernel_regularizer,\n                data_format=data_format)\n\n    if isinstance(cur_inception_module, list):\n        inception_module_paths_flattened = [\n            tf.reshape(\n                path,\n                [-1, reduce(lambda a, b: a * b, path.get_shape().as_list()[1:])]\n            ) for path in cur_inception_module]\n        return tf.concat(inception_module_paths_flattened, 1)\n\n    return cur_inception_module\n\n\ndef build_fully_connected_layers_with_batch_norm(the_input, shape, kernel_initializer, mode, scope_prefix=""""):\n    """"""\n    Builds fully connected layers with batch normalization onto the computational graph of the desired shape.\n\n\n    The following are a few examples of the shapes that can be given, and how the layers are built\n\n    shape = [512, 256, 128]\n    result: input --> 512 --> 256 --> 128\n\n    shape = [[25, 75], [200], 100, 75]\n    result: concat((input --> 25 --> 75), (input --> 200)) --> 100 --> 75\n\n    More complicated shapes can be used by defining modules recursively like the following\n    shape = [[[100], [50, 50]], [200, 50], [75], 100, 20]\n    """"""\n    if len(shape) == 0:\n        return the_input\n\n    module_outputs = []\n    for j, inner_modules in enumerate(shape):\n        if isinstance(inner_modules, list):\n            output = build_fully_connected_layers_with_batch_norm(\n                the_input,\n                inner_modules,\n                kernel_initializer,\n                mode,\n                scope_prefix=""%sfc_module_%d/"" % (scope_prefix, j + 1))\n            module_outputs.append(output)\n        else:\n            if len(module_outputs) == 1:\n                the_input = module_outputs[0]\n            elif len(module_outputs) > 1:\n                the_input = tf.concat(module_outputs, axis=1)\n\n            for i, layer_shape in enumerate(shape[j:]):\n\n                with tf.variable_scope(scope_prefix + ""FC_"" + str(i + 1)):\n                    pre_activation = tf.layers.dense(\n                        inputs=the_input,\n                        units=layer_shape,\n                        use_bias=False,\n                        kernel_initializer=kernel_initializer(),\n                        name=""layer"")\n\n                    batch_normalized = tf.layers.batch_normalization(pre_activation,\n                                                                     scale=False,\n                                                                     training=(mode == tf.estimator.ModeKeys.TRAIN),\n                                                                     fused=True)\n\n                    the_input = tf.nn.relu(batch_normalized)\n                layers.summarize_activation(the_input)\n\n            module_outputs = [the_input]\n            break\n\n    if len(module_outputs) == 1:\n        return module_outputs[0]\n\n    return tf.concat(module_outputs, axis=1)\n\n\ndef metric_dict_creator(the_dict):\n    metric_dict = {}\n    for key, value in the_dict.items():\n        if isinstance(value, tuple): # Given a tuple (tensor, summary)\n            metric_dict[key] = (tf.reduce_mean(value[0]), value[1])\n        else: #Given a tensor\n            mean_value = tf.reduce_mean(value)\n            metric_dict[key] = (mean_value, tf.summary.scalar(key, mean_value))\n\n    return metric_dict\n\n\ndef numpy_style_repeat_1d_creator(max_multiple=100, max_to_repeat=10000, out_type=tf.int32):\n    board_num_lookup_ary = np.repeat(\n        np.arange(max_to_repeat),\n        np.full([max_to_repeat], max_multiple))\n    board_num_lookup_ary = board_num_lookup_ary.reshape(max_to_repeat, max_multiple)\n\n    def fn_to_return(multiples):\n        board_num_lookup_tensor = tf.constant(board_num_lookup_ary, dtype=out_type)\n\n        if multiples.dtype != tf.int32:\n            multiples = tf.cast(multiples, dtype=tf.int32)\n\n        padded_multiples = tf.pad(\n            multiples,\n            [[0, max_to_repeat - tf.shape(multiples)[0]]])\n\n        padded_multiples = tf.cast(padded_multiples, tf.int32)\n        to_return =  tf.boolean_mask(\n            board_num_lookup_tensor,\n            tf.sequence_mask(padded_multiples, maxlen=max_multiple))\n        return to_return\n\n    return fn_to_return\n\n\ndef count_tfrecords(filename):\n    return sum(1 for _ in tf.python_io.tf_record_iterator(filename))\n\n\nclass ValidationRunHook(tf.train.SessionRunHook):\n    """"""\n    A subclass of tf.train.SessionRunHook to be used to evaluate validation data\n    efficiently during an Estimator\'s training run.\n\n\n    TO DO:\n    1) Figure out how to handle steps to do one complete epoch\n    2) Have this not call the evaluate function because it has to restore from a\n    checkpoint, it will likely be faster if I evaluate it on the current training graph\n    3) Implement an epoch counter to be printed along with the validation results\n    """"""\n    def __init__(self, step_increment, estimator, input_fn_creator, temp_num_steps_in_epoch=None,\n                 recall_input_fn_creator_after_evaluate=False):\n        self.step_increment = step_increment\n        self.estimator = estimator\n        self.input_fn_creator = input_fn_creator\n        self.recall_input_fn_creator_after_evaluate = recall_input_fn_creator_after_evaluate\n        self.temp_num_steps_in_epoch = temp_num_steps_in_epoch\n\n    def begin(self):\n        self._global_step_tensor = tf.train.get_global_step()\n\n        if self._global_step_tensor is None:\n            raise RuntimeError(""Global step should be created to use ValidationRunHook."")\n\n        self._input_fn = self.input_fn_creator()\n\n    def after_create_session(self, session, coord):\n        self._step_started = session.run(self._global_step_tensor)\n\n    def before_run(self, run_context):\n        return training.session_run_hook.SessionRunArgs(self._global_step_tensor)\n\n    def after_run(self, run_context, run_values):\n        if (run_values.results - self._step_started) % self.step_increment == 0 and run_values.results != 0:\n            print(self.estimator.evaluate(\n                input_fn=self._input_fn,\n                steps=self.temp_num_steps_in_epoch))\n\n            if self.recall_input_fn_creator_after_evaluate:\n                self._input_fn =  self.input_fn_creator()'"
batch_first/anns/database_creator.py,27,"b'import tensorflow as tf\n\nimport re\nimport pickle\nimport time\nimport chess.pgn\n\nfrom batch_first.numba_board import *\n\n\n\n\ndef game_iterator(pgn_filename):\n    """"""\n    Iterates through the games stored on the given pgn file (as python-chess Game objects).\n    """"""\n    with open(pgn_filename) as pgn_file:\n        while True:\n            game = chess.pgn.read_game(pgn_file)\n            if game is None:\n                break\n\n            yield game\n\n\ndef get_gamenode_after_moves(game, num_moves):\n    """"""\n    Get the node from a given game after a specified number of moves has been made.\n    """"""\n    for _ in range(num_moves):\n        game = game.variations[0]\n    return game\n\n\ndef get_py_board_info_tuple(board):\n    return np.array([board.kings, board.queens, board.rooks, board.bishops, board.knights, board.pawns,\n                     board.castling_rights, 0 if board.ep_square is None else BB_SQUARES[np.int32(board.ep_square)],\n                     board.occupied_co[board.turn], board.occupied_co[not board.turn], board.occupied], dtype=np.uint64)\n\n\ndef get_feature_array(white_info):\n    """"""\n    NOTES:\n    1) For the method used here to work (iterating over the masks), rook indices must always be set before\n     castling_rights, and unoccupied must be set before the ep_square.\n    """"""\n    answer = np.zeros(64,dtype=np.uint8)\n\n    occupied_colors = np.array([[white_info[8]], [white_info[9]]])\n    piece_info = np.array([white_info[:7]])\n\n    masks = np.unpackbits(np.bitwise_and(occupied_colors, piece_info).reshape(14).view(np.uint8)).reshape(\n        [14, 8, 8]).view(np.bool_)[..., ::-1].reshape(14, 64)\n\n    for j, mask in enumerate(masks):\n        answer[mask] = j + 2\n\n    #Set the ep square\n    if white_info[7] !=0:\n        answer[msb(white_info[7])] = 1\n\n    return answer\n\n\ndef additional_move_features(board_features):\n    return {""move_from_square"": tf.train.Feature(int64_list=tf.train.Int64List(value=[board_features[1]])),\n            ""move_to_square"": tf.train.Feature(int64_list=tf.train.Int64List(value=[board_features[2]])),\n            ""move_filter"": tf.train.Feature(int64_list=tf.train.Int64List(value=[board_features[3]]))}\n\n\ndef serializer_creator(additional_feature_dict_fn=None):\n    """"""\n    A convenience function to aid in the use of the combine_pickles_and_create_tfrecords function.\n\n    :param additional_feature_dict_fn: A function which returns a dictionary mapping feature names (strings)\n     to tf.train.Feature objects.  The features given in this dict will be serialized along with the ""board"" and\n     ""score"" features\n    :return: A function to be given to the combine_pickles_and_create_tfrecords function as\n     the serializer parameter\n    """"""\n    def serializer_to_return(board_features, score):\n        feature_input = board_features[0] if isinstance(board_features[0], tuple) else board_features\n        feature_dict = {\n            ""board"": tf.train.Feature(int64_list=tf.train.Int64List(value=get_feature_array(feature_input))),\n            ""score"": tf.train.Feature(int64_list=tf.train.Int64List(value=[score]))}\n\n        if additional_feature_dict_fn:\n            feature_dict.update(additional_feature_dict_fn(board_features))\n\n        example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n        return example.SerializeToString()\n\n    return serializer_to_return\n\n\ndef get_nodes_value(node, board_turn, max_win_value=1000000):\n    """"""\n    Gets the value of the given node, or None if no value is stored.\n\n    :param node: The node who\'s value is to be returned\n    :param board_turn: If this parameter is False, it will multiply the score by -1 (changing the score to be with\n     respect to the other player)\n    :param win_value: The value for a depth-0 winning board.  The value of a depth-1 win would thus be one less,\n     and depth-n would be n less\n    """"""\n    re_search_results = re.search(r\'\\[\\%eval (.*?)\\]\', node.comment)\n    if re_search_results is None:\n        return None\n\n    sf_score_str = re_search_results.group(1)\n\n    # If a mate is found, store the win/loss value,\n    # scaled such that the magnitude of the win/loss value will decrease as the depth from the mate\n    # increases.\n    if sf_score_str[0] == \'#\':\n        mate_depth = np.int64(int(sf_score_str[1:]))\n\n        mate_value = max_win_value if mate_depth > 0 else -max_win_value\n\n        sf_score = mate_value - mate_depth\n    else:\n        sf_score = np.int64(float(sf_score_str) * 100)  # converts the score to the traditional centi-pawn scores used by StockFish\n\n    if not board_turn:\n        sf_score *= -1\n\n    return sf_score\n\n\ndef get_data_from_pgns(pgn_filenames, output_filename, get_board_for_game_fn, print_interval=10000):\n    """"""\n    Creates a pickle database from the data in the given pgn files.  One example is produced per game, and is done so\n    by a given function.\n\n\n    :param pgn_filenames: An iterable of the names/paths of pgn files to have data gathered from\n    :param output_filename: A string used as the name of the output pickle database\n    :param get_board_for_game_fn: A function that accepts a python-chess Game, and returns a length two tuple.  That tuple\'s\n     first element is either the board\'s representation (a tuple), or a tuple with it\'s representation as it\'s first\n     value (followed by any other desired information) (All of this information is used to ensure that each example is unique).\n     The second element of the returned tuple is the score associated with the example\n    :param print_interval: The number of games checked between printing the progress\n    """"""\n    eval_boards = {}\n    num_checked = 0\n    for j, filename in enumerate(pgn_filenames):\n        if print_interval:\n            print(""Starting file %d""%j)\n        last_print = time.time()\n\n        for returns in map(get_board_for_game_fn, game_iterator(filename)):\n            if not returns is None:\n                if eval_boards.get(returns[0]) is None:\n                    eval_boards[returns[0]] = returns[1]\n\n            num_checked += 1\n\n            if print_interval and num_checked % print_interval == 0:\n                print(""%d boards have been checked, generating %d usable boards, with %f time since the last print""%(num_checked, len(eval_boards), time.time() - last_print))\n                last_print = time.time()\n\n    with open(output_filename, \'wb\') as writer:\n        pickle.dump(eval_boards, writer, pickle.HIGHEST_PROTOCOL)\n\n\ndef combine_pickles_and_create_tfrecords(filenames, output_filenames, output_ratios, serializer):\n    """"""\n    Combines the information in the given pickle files (maintaining uniqueness), serializes the data, then saves it\n    as a set of tfrecords files (in the desired ratios).\n\n    :param filenames: An iterable of pickled filenames (produced by get_data_from_pgns) to serialize and combine\n    :param output_filenames: An iterable of filenames to save the serialized tfrecords to\n    :param output_ratios: The ratios of the combined data to be saved to each of the files in output_filenames\n    :param serializer: A function which returns a serialized TensorFlow Example, the details of it\'s two parameters\n     are described in the get_data_from_pgns\'s comments as the return of the get_board_for_game_fn parameter\n     (the serializer_creator function can be used to generate this parameter)\n    """"""\n    combined_dict = {}\n    for name in filenames:\n        with open(name, ""rb"") as to_read:\n            for key, value in pickle.load(to_read).items():\n                if combined_dict.get(key) is None:\n                    combined_dict[key] = value\n\n    break_indices = np.r_[0, (len(combined_dict) * np.cumsum(np.array(output_ratios))[:-1]).astype(np.int32),len(combined_dict)]\n    dict_iterator = iter(combined_dict)\n\n    key_arrays = ([next(dict_iterator) for _ in range(j-i)] for i,j in zip(break_indices[:-1],break_indices[1:]))\n    serialized_examples = (map(serializer, keys, (combined_dict[k] for k in keys)) for keys in key_arrays)\n\n    for filename, examples in zip(output_filenames, serialized_examples):\n        with tf.python_io.TFRecordWriter(filename) as writer:\n            for example in examples:\n                writer.write(example)\n\n\ndef create_board_eval_board_from_game_fn(min_start_moves=6, for_testing=False):\n    random_number_starter = min_start_moves - 1\n    min_boards_in_game = min_start_moves + 1\n    def get_board_for_game(game):\n        num_boards_in_game = len(list(game.main_line()))\n\n        if num_boards_in_game <= min_boards_in_game:\n            return None\n\n        desired_game_node = get_gamenode_after_moves(game, np.random.randint(random_number_starter, num_boards_in_game-1))\n        py_board = desired_game_node.board()\n\n        if for_testing:\n            sf_score = 0\n        else:\n            sf_score = get_nodes_value(desired_game_node,  py_board.turn)\n            if sf_score is None:\n                return None\n\n        if py_board.turn:\n            white_info = tuple(get_py_board_info_tuple(py_board))\n        else:\n            white_info = tuple(flip_vertically(get_py_board_info_tuple(py_board)))\n\n        return white_info, sf_score\n    return get_board_for_game\n\n\ndef create_move_scoring_board_from_game_fn(min_start_moves=6):\n    random_number_starter = min_start_moves - 1\n    min_boards_in_game = min_start_moves + 1\n    def get_board_for_game(game):\n        num_boards_in_game = len(list(game.main_line()))\n\n        if num_boards_in_game <= min_boards_in_game:\n            return None\n\n        desired_game_node = get_gamenode_after_moves(game, np.random.randint(random_number_starter, num_boards_in_game-1))\n\n        py_board = desired_game_node.board()\n\n        next_node = desired_game_node.variations[0]\n        next_value = get_nodes_value(next_node, py_board.turn)\n\n        if next_value is None:\n            return None\n\n        # Store the move and flip it if the board was converted from black\'s perspective\n        move_made = next_node.move\n        if not py_board.turn:\n            move_made.from_square = chess.square_mirror(move_made.from_square)\n            move_made.to_square = chess.square_mirror(move_made.to_square)\n\n        if move_made.promotion is None or move_made.promotion == chess.QUEEN:\n            move_promotion = NO_PROMOTION_VALUE\n        else:\n            move_promotion = move_made.promotion\n\n        move_filter = MOVE_FILTER_LOOKUP[\n            move_made.from_square,\n            move_made.to_square,\n            move_promotion]\n\n        if py_board.turn:\n            white_info = tuple(get_py_board_info_tuple(py_board))\n        else:\n            white_info = tuple(flip_vertically(get_py_board_info_tuple(py_board)))\n\n        return (white_info, move_made.from_square, move_made.to_square, move_filter), next_value\n\n    return get_board_for_game\n\n\ndef save_all_boards_from_game_as_npy(pgn_file, output_filename, max_games=100000, print_interval=5000):\n    """"""\n    Goes through the games in a pgn file and saves the unique boards in NumPy file format\n    (with dtype numpy_node_info_dtype).  Prior to being saved the legal move arrays are set up.\n\n    :param pgn_file: The pgn file to gather boards from\n    :param output_filename: The name for the database file to be created\n    :param max_games: The maximum number of games to go through\n    :param print_interval: The number of games between each progress update\n\n    NOTES:\n    1) This function uses a large amount of memory (mainly caused by np.unique)\n    """"""\n    prev_time = time.time()\n    root_struct = create_node_info_from_python_chess_board(chess.Board())\n\n    collected = []\n    for j, game in enumerate(game_iterator(pgn_file)):\n\n        if j == max_games:\n            break\n\n        if j % print_interval == 0:\n            print(""%d games complete with %d boards collected (not unique) with %f time since last print.""%(j, len(collected), time.time() - prev_time))\n            prev_time = time.time()\n\n        struct = root_struct.copy()\n\n        move_iterator = (np.array([[move.from_square, move.to_square, 0 if move.promotion is None else move.promotion]]) for move in game.main_line())\n        for move_ary in move_iterator:\n            push_moves(struct, move_ary)\n            collected.append(struct.copy())\n\n    print(""Completed board acquisition"")\n\n    unique_structs = np.unique(np.array(collected))\n\n    print(""%d unique boards produced."" % len(unique_structs))\n\n    set_up_move_arrays(unique_structs)\n\n    structs_with_less_than_max_moves = unique_structs[unique_structs[\'children_left\'] <= MAX_MOVES_LOOKED_AT]\n\n    print(""Moves have now been set up."")\n\n    np.save(output_filename, structs_with_less_than_max_moves)\n\n\ndef get_zero_valued_boards(filename, output_filename, print_interval=25000):\n    def iterate_zero_value_nodes():\n        num_done = 0\n        for game in game_iterator(filename):\n            while len(game.variations) == 1:\n                game = game.variations[0]\n                if get_nodes_value(game, True) == 0:\n                    num_done += 1\n                    if num_done % print_interval == 0:\n                        print(""Zero valued boards gathered:"", num_done)\n                    yield game.board()\n\n\n    to_return = np.array([create_node_info_from_python_chess_board(b) for b in iterate_zero_value_nodes()])\n    unique_boards = np.unique(to_return)\n\n    np.save(output_filename, unique_boards)\n\n\ndef get_locations_of_lines_that_pass_filters(filename, filters, print_interval=1e7):\n    """"""\n    Gets the line numbers of the lines in a given file that pass a set of filters.\n    """"""\n    def passes(line):\n        for filter in filters:\n            if filter(line):\n                return False\n        return True\n\n    with open(filename, \'r\') as f:\n        prev_time = time.time()\n        line_nums = []\n        for j,line in enumerate(iter(f.readline, \'\')):\n            if passes(line):\n                line_nums.append(f.tell())\n\n            if j % print_interval == 0:\n                print(""%d lines completed so far with %d lines found in %f time since last print""%(j, len(line_nums), time.time() - prev_time))\n                prev_time = time.time()\n\n        return line_nums\n\n\ndef clean_pgn_file(pgn_to_filter, output_filename, line_filters=[], header_filters=[]):\n    def passes_header_filters(headers):\n        for filter in header_filters:\n            if filter(headers):\n                return False\n        return True\n\n    line_nums = get_locations_of_lines_that_pass_filters(\n        pgn_to_filter,\n        line_filters)\n    line_nums = line_nums[: -1]  #the last game isn\'t used because if it\'s the last game in the file the array indexing to follow will cause an error\n\n    line_nums = np.array(line_nums)\n    print(""Desired line numbers have been found."")\n    with open(output_filename, \'w\') as writer:\n        with open(pgn_to_filter, \'r\') as f:\n            temp_stuff = [(offset, passes_header_filters(header)) for offset, header in chess.pgn.scan_headers(f)]\n\n            offsets, should_write = zip(*temp_stuff)\n\n            should_write = np.array(list(should_write), dtype=np.bool_)\n\n            game_offsets = np.array(list(offsets))\n\n            print(""Game offsets calculated and headers collected"")\n\n            temp_indices = np.searchsorted(game_offsets, line_nums)\n\n\n            actual_offset_indices = temp_indices - 1\n\n            should_write = should_write[actual_offset_indices]\n\n            amount_to_write = game_offsets[temp_indices] - game_offsets[actual_offset_indices]\n            filtered_start_lines = game_offsets[actual_offset_indices]\n\n            print(""Starting to write the new file."")\n\n            for offset, amount in zip(filtered_start_lines[should_write], amount_to_write[should_write]):\n                f.seek(offset)\n                writer.write(f.read(amount))\n\n\ndef combine_numpy_files_and_make_unique(filenames, output_name):\n    combined = np.concatenate([np.load(file) for file in filenames])\n    unique_boards = np.unique(combined)\n    np.save(output_name, unique_boards)\n\n\nif __name__ == ""__main__"":\n    """"""\n    The commented out code throughout the rest of this file is how the ANN training databases are created,\n    as well as any other database used in Batch First. \n    """"""\n\n\n    def add_path_fn_creator(path):\n        return lambda x : [path + y for y in x]\n\n    PGN_FILENAMES_WITHOUT_PATHS = [\n        ""lichess_db_standard_rated_2018-06.pgn"",\n        ""lichess_db_standard_rated_2018-05.pgn"",\n        ""lichess_db_standard_rated_2018-04.pgn"",\n        ""lichess_db_standard_rated_2018-03.pgn"",\n        ""lichess_db_standard_rated_2018-02.pgn"",\n        ""lichess_db_standard_rated_2018-01.pgn"",\n        ""lichess_db_standard_rated_2017-12.pgn"",\n        ""lichess_db_standard_rated_2017-11.pgn"",\n        ""lichess_db_standard_rated_2017-10.pgn"",\n        ""lichess_db_standard_rated_2017-09.pgn"",\n        ""lichess_db_standard_rated_2017-08.pgn"",\n        ""lichess_db_standard_rated_2017-07.pgn"",\n        ""lichess_db_standard_rated_2017-06.pgn"",\n        ""lichess_db_standard_rated_2017-05.pgn"",\n        ""lichess_db_standard_rated_2017-04.pgn"",\n        ""lichess_db_standard_rated_2017-03.pgn"",\n        ""lichess_db_standard_rated_2017-02.pgn"",\n        ""lichess_db_standard_rated_2017-01.pgn"",\n        ""lichess_db_standard_rated_2016-12.pgn"",\n        ""lichess_db_standard_rated_2016-11.pgn"",\n        ""lichess_db_standard_rated_2016-10.pgn"",\n        ""lichess_db_standard_rated_2016-09.pgn"",\n        ""lichess_db_standard_rated_2016-08.pgn"",\n        ""lichess_db_standard_rated_2016-07.pgn"",\n        ""lichess_db_standard_rated_2016-06.pgn"",\n        ""lichess_db_standard_rated_2016-05.pgn"",\n        ""lichess_db_standard_rated_2016-04.pgn"",\n        ""lichess_db_standard_rated_2016-03.pgn"",\n        ""lichess_db_standard_rated_2016-02.pgn"",\n        ""lichess_db_standard_rated_2016-01.pgn"",][::-1]\n\n    PICKLE_FILENAMES = list(map(lambda s : s[:-3] + ""pkl"", PGN_FILENAMES_WITHOUT_PATHS))\n\n    PGN_FILENAMES = add_path_fn_creator(""/srv/databases/lichess/original_pgns/"")(PGN_FILENAMES_WITHOUT_PATHS)\n\n    FILTERED_FILENAMES = add_path_fn_creator(""/srv/databases/lichess/filtered_pgns/"")(PGN_FILENAMES_WITHOUT_PATHS)\n    FILTERED_FILENAMES = list(map(lambda f: ""%s_filtered.pgn"" % f[:-4], FILTERED_FILENAMES))\n\n\n    TFRECORDS_FILENAMES = [\n        ""lichess_training.tfrecords"",\n        ""lichess_validation.tfrecords"",\n        ""lichess_testing.tfrecords""]\n\n    TFRECORDS_OUTPUT_RATIOS = [.85, .1, .05]\n\n\n\n\n\n\n    pgn_not_used_for_ann_training = ""/srv/databases/lichess/lichess_db_standard_rated_2018-07.pgn""\n    npy_output_filename = ""/srv/databases/lichess/lichess_db_standard_rated_2018-07_first_100k_games""\n    # save_all_boards_from_game_as_npy(pgn_not_used_for_ann_training, npy_output_filename)\n\n\n    file_index = -6\n    OUTPUT_FILTERED_FILENAME = add_path_fn_creator(""/srv/databases/has_zero_valued_board/"")(PGN_FILENAMES_WITHOUT_PATHS)[file_index]\n    # clean_pgn_file(\n    #     FILTERED_FILENAMES[file_index],\n    #     OUTPUT_FILTERED_FILENAME,\n    #     [lambda s: s[0] != \'1\',\n    #      lambda s: not ""%eval 0.0"" in s],\n    #     [lambda h: h[\'Termination\'] != ""Normal""])\n\n\n    file_index = -2\n    INPUT_FILENAME = FILTERED_FILENAMES[file_index]\n    OUTPUT_FILENAME = INPUT_FILENAME[:-4] + ""_zero_boards""\n    # get_zero_valued_boards(INPUT_FILENAME, OUTPUT_FILENAME)\n\n\n    ########################The commented out code below is used for the move scoring database###################\n    file_index = 6\n    OUTPUT_FILENAME = add_path_fn_creator(""/srv/databases/lichess_just_move_scoring_fixed_ag_promotion/"")(PGN_FILENAMES_WITHOUT_PATHS)[file_index][:-3] + ""pkl""\n    OUTPUT_FILENAME = OUTPUT_FILENAME[:-4] + ""_second_pass.pkl""\n\n    CUR_MOVE_PGN_FILENAME = FILTERED_FILENAMES[file_index]\n    # get_data_from_pgns(\n    #     [CUR_MOVE_PGN_FILENAME],\n    #     OUTPUT_FILENAME,\n    #     create_move_scoring_board_from_game_fn(5))\n\n\n    #########################The commented out code below is used for the board evaluation database###################\n    file_index = 6\n    CUR_EVAL_PGN_FILENAME = FILTERED_FILENAMES[file_index]\n    OUTPUT_FILENAME = add_path_fn_creator(""/srv/databases/lichess_combined_methods_eval_databases/"")(PGN_FILENAMES_WITHOUT_PATHS)[file_index][:-3] + ""pkl""\n    # OUTPUT_FILENAME = OUTPUT_FILENAME[:-4] + ""_second_pass.pkl""\n    # get_data_from_pgns(\n    #     [CUR_EVAL_PGN_FILENAME],\n    #     OUTPUT_FILENAME,\n    #     create_board_eval_board_from_game_fn())\n\n\n\n    #########################The commented out code below is used for the board evaluation database#########################\n    eval_path_adder = add_path_fn_creator(""/srv/databases/lichess_combined_methods_eval_databases/"")\n\n    EVAL_PICKLE_FILES = eval_path_adder(PICKLE_FILENAMES)\n    # EVAL_PICKLE_FILES += list(map(lambda s: s[:-4] + ""_second_pass.pkl"", EVAL_PICKLE_FILES))\n\n    EVAL_OUTPUT_TFRECORDS_FILES = eval_path_adder(TFRECORDS_FILENAMES)\n\n    # combine_pickles_and_create_tfrecords(\n    #     EVAL_PICKLE_FILES,\n    #     EVAL_OUTPUT_TFRECORDS_FILES,\n    #     TFRECORDS_OUTPUT_RATIOS,\n    #     serializer_creator())\n\n\n    #########################The commented out code below is used for the move scoring database###################\n    policy_path_adder = add_path_fn_creator(""/srv/databases/lichess_just_move_scoring_fixed_ag_promotion/"")\n    MOVE_SCORING_PICKLE_FILES = policy_path_adder(PICKLE_FILENAMES)\n    # MOVE_SCORING_PICKLE_FILES += list(map(lambda s: s[:-4] + ""_second_pass.pkl"", MOVE_SCORING_PICKLE_FILES))\n    POLICY_OUTPUT_TFRECORDS_FILES = policy_path_adder(TFRECORDS_FILENAMES)\n\n    # combine_pickles_and_create_tfrecords(\n    #     MOVE_SCORING_PICKLE_FILES,\n    #     POLICY_OUTPUT_TFRECORDS_FILES,\n    #     TFRECORDS_OUTPUT_RATIOS,\n    #     serializer_creator(additional_move_features))\n\n'"
batch_first/anns/evaluation_ann.py,0,"b'import tensorflow as tf\n\nfrom scipy.stats import kendalltau, weightedtau, spearmanr\n\nimport batch_first.anns.ann_creation_helper as ann_h\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\n\ndef diag_comparison_model_fn(features, labels, mode, params):\n    """"""\n    Generates an EstimatorSpec for a model which scores chess boards.  It learns by maximizing the difference between\n    board evaluation values, where one is intended to be greater than the other based on some pre-calculated\n    scoring system (e.g. StockFish evaluations).\n    """"""\n    convolutional_module_outputs=ann_h.create_input_convolutions_shared_weights(\n        features[\'board\'],\n        params[\'kernel_initializer\'],\n        params[\'data_format\'],\n        mode,\n        num_unique_filters=[32, 20, 20, 18, 18, 16, 24, 24]) #236 with the 64 undilated included\n\n    if len(params[\'convolutional_modules\']):\n        convolutional_module_outputs = ann_h.build_convolutional_modules(\n            convolutional_module_outputs,\n            params[\'convolutional_modules\'],\n            mode,\n            params[\'kernel_initializer\'],\n            params[\'kernel_regularizer\'],\n            params[\'trainable_cnn_modules\'],\n            num_previous_modules=1,\n            data_format=params[\'data_format\'])\n\n    logits = tf.layers.conv2d(\n        inputs=convolutional_module_outputs,\n        filters=1,\n        kernel_size=[1,1],\n        padding=""valid"",\n        data_format=""channels_last"" if params[\'data_format\']==""NHWC"" else ""channels_first"",\n        use_bias=False,\n        kernel_initializer=params[\'kernel_initializer\'](),\n        kernel_regularizer=params[\'kernel_regularizer\'](),\n        name=""logit_layer"")\n\n    logits = tf.squeeze(logits, axis=[1,2,3])\n\n    loss = None\n    train_op = None\n\n\n    # Calculate loss\n    if mode != tf.estimator.ModeKeys.PREDICT:\n        with tf.variable_scope(""loss""):\n            calculated_diff_matrix = ann_h.vec_and_transpose_op(logits, tf.subtract)\n\n            label_matrix = features[\'label_matrix\']\n            weight_matrix = features[\'weight_matrix\']\n\n            loss = tf.losses.sigmoid_cross_entropy(label_matrix, calculated_diff_matrix, weights=weight_matrix)\n            loss_summary = tf.summary.scalar(""loss"", loss)\n\n\n    # Configure the Training Op\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        global_step = tf.train.get_global_step()\n        learning_rate = params[\'learning_decay_function\'](global_step)\n        tf.summary.scalar(""learning_rate"", learning_rate)\n        train_op = tf.contrib.layers.optimize_loss(\n            loss=loss,\n            global_step=global_step,\n            learning_rate=learning_rate,\n            optimizer=params[\'optimizer\'],\n            summaries=params[\'train_summaries\'])\n\n    predictions = {""scores"" : logits}\n    the_export_outputs = {""serving_default"" : tf.estimator.export.RegressionOutput(value=logits)}\n\n    validation_metrics = None\n    training_hooks = []\n\n    # Create the metrics\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        tau_a = ann_h.kendall_rank_correlation_coefficient(logits, features[\'score\'])\n\n        to_create_metric_dict = {\n            ""loss/loss"": (loss, loss_summary),\n            ""metrics/mean_evaluation_value"" : logits,\n            ""metrics/mean_abs_evaluation_value"": tf.abs(logits),\n            ""metrics/kendall_tau-a"" : tau_a,\n        }\n\n        validation_metrics = ann_h.metric_dict_creator(to_create_metric_dict)\n\n        tf.contrib.layers.summarize_tensors(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n        training_hooks.append(\n            tf.train.SummarySaverHook(\n                save_steps=params[\'log_interval\'],\n                output_dir=params[\'model_dir\'],\n                summary_op=tf.summary.merge_all()))\n\n    elif mode == tf.estimator.ModeKeys.EVAL:\n        the_logits, update1 = tf.contrib.metrics.streaming_concat(logits)\n        the_labels, update2 = tf.contrib.metrics.streaming_concat(features[\'score\'])\n\n        BIGGER_THAN_CP_LESS_THAN_WIN_VAL = tf.constant(100000, dtype=features[\'score\'].dtype)\n\n        mate_mask = tf.greater(tf.abs(the_labels), BIGGER_THAN_CP_LESS_THAN_WIN_VAL)\n        mate_logits = tf.boolean_mask(the_logits, mate_mask)\n        mate_labels = tf.boolean_mask(the_labels, mate_mask)\n\n        non_mate_mask = tf.logical_not(mate_mask)\n        non_mate_logits = tf.boolean_mask(the_logits, non_mate_mask)\n        non_mate_labels = tf.boolean_mask(the_labels, non_mate_mask)\n\n        mate_tau_b, mate_tau_b_p_value = ann_h.py_func_scipy_rank_helper_creator(mate_logits, mate_labels)(kendalltau)\n\n        non_mate_coef_creator = ann_h.py_func_scipy_rank_helper_creator(non_mate_logits, non_mate_labels)\n        non_mate_tau_b, non_mate_tau_b_p_value = non_mate_coef_creator(kendalltau)\n        non_mate_weighted_tau_b, non_mate_weighted_tau_b_p_value= non_mate_coef_creator(weightedtau)\n\n        update = tf.group(update1, update2)\n\n        scipy_rank_coef_creator = ann_h.py_func_scipy_rank_helper_creator(the_logits, the_labels)\n\n        tau_b, tau_b_p_value = scipy_rank_coef_creator(kendalltau)\n        rho, rho_p_value = scipy_rank_coef_creator(spearmanr)\n\n        validation_metrics = {\n            ""metrics/mean_evaluation_value"" : tf.metrics.mean(logits),\n            ""metrics/mean_abs_evaluation_value"" : tf.metrics.mean(tf.abs(logits)),\n            ""metrics/kendall_tau-b"": (tau_b, update),\n            ""metrics/kendall_tau-b_p_value"": (tau_b_p_value, update),\n            ""metrics/kendall_tau-b_mate_only"" : (mate_tau_b, update),\n            ""metrics/non_mate_kendall_tau-b"" : (non_mate_tau_b, update),\n            ""metrics/non_mate_weighted_tau-b"": (non_mate_weighted_tau_b, update),\n            ""metrics/non_mate_weighted_kendall_tau-b_p_value"": (non_mate_weighted_tau_b_p_value, update),\n            ""metrics/spearman_rho"": (rho, update),\n            ""metrics/spearman_rho_p_value"": (rho_p_value, update),\n        }\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=predictions,\n        loss=loss,\n        train_op=train_op,\n        training_hooks=training_hooks,\n        export_outputs=the_export_outputs,\n        eval_metric_ops=validation_metrics)\n\n\ndef lower_diag_score_comparison_input_fn(filename_pattern, batch_size, include_unoccupied=True, shuffle_buffer_size=None,\n                                         num_things_in_parallel=None, num_things_to_prefetch=None, shuffle_seed=None,\n                                         data_format=""NHWC""):\n    if num_things_to_prefetch is None:\n        num_things_to_prefetch = tf.contrib.data.AUTOTUNE  #IMPORTANT NOTE: This seems to make the program crash after a few iterations (at least it does on my computer)\n\n    dataset = tf.data.TFRecordDataset(filename_pattern)\n\n    if shuffle_buffer_size:\n        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(shuffle_buffer_size,seed=shuffle_seed))\n\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n\n    def process_batch(records):\n        keys_to_features = {""board"": tf.FixedLenFeature([8 * 8], tf.int64),\n                            ""score"": tf.FixedLenFeature([], tf.int64)}\n\n        parsed_examples = tf.parse_example(records, keys_to_features)\n\n        reshaped_boards = tf.reshape(parsed_examples[\'board\'], [-1, 8, 8])\n\n        omit_unoccupied_decrement = 0 if include_unoccupied else 1\n        boards = tf.one_hot(\n            reshaped_boards - omit_unoccupied_decrement,\n            16 - omit_unoccupied_decrement,\n            axis=-1 if data_format==""NHWC"" else 1)\n\n        desired_diff_matrix = ann_h.vec_and_transpose_op(parsed_examples[\'score\'], tf.subtract, tf.float32)\n\n        lower_diag_diff_matrix = tf.matrix_band_part(desired_diff_matrix, -1, 0)\n\n        lower_diag_sign = tf.sign(lower_diag_diff_matrix)\n        weight_mask = tf.abs(lower_diag_sign)\n        bool_weight_mask = tf.cast(weight_mask, tf.bool)\n\n        comparison_indices = tf.where(bool_weight_mask)\n\n        # value_larger_than_centipawn_less_than_mate = 100000\n        # desired_found_mate = tf.greater(tf.abs(parsed_examples[\'score\']), value_larger_than_centipawn_less_than_mate)\n\n        # both_found_mate = ann_h.vec_and_transpose_op(desired_found_mate, tf.logical_and)\n\n        # desired_signs = tf.sign(parsed_examples[\'score\'])\n\n        # same_sign_matrix = ann_h.vec_and_transpose_op(desired_signs, tf.equal)\n\n        # both_same_player_mates = tf.logical_and(both_found_mate, same_sign_matrix)\n\n        # both_same_mate_and_nonzero_weight = tf.logical_and(both_same_player_mates, bool_weight_mask)\n\n        # same_mate_depth_diff_decrement = .95\n\n        # weight_helper = same_mate_depth_diff_decrement * tf.cast(both_same_mate_and_nonzero_weight, tf.float32)\n\n        # mate_adjusted_weight_mask = weight_mask - weight_helper\n\n        label_matrix = (lower_diag_sign + weight_mask)/2\n\n        # return boards, parsed_examples[\'score\'], label_matrix, mate_adjusted_weight_mask, comparison_indices\n        return boards, parsed_examples[\'score\'], label_matrix, weight_mask, comparison_indices\n\n\n    dataset = dataset.map(process_batch, num_parallel_calls=num_things_in_parallel)\n\n    dataset = dataset.apply(tf.contrib.data.prefetch_to_device(\'/gpu:0\', buffer_size=num_things_to_prefetch))\n\n    iterator = dataset.make_one_shot_iterator()\n\n    features = iterator.get_next()\n\n    feature_dict = {""board"": features[0], ""score"": features[1], ""label_matrix"": features[2], ""weight_matrix"": features[3]}\n\n    return feature_dict, None\n\n\ndef board_eval_serving_input_receiver(data_format=""NCHW""):\n    def fn_to_return():\n        placeholder_shape = [None, 15, 8, 8] if data_format==""NCHW"" else [None, 8, 8, 15]\n\n        for_remapping = tf.placeholder(tf.float32, placeholder_shape, ""FOR_INPUT_MAPPING_transpose"")\n\n        receiver_tensors = {""board"": for_remapping}\n        return tf.estimator.export.ServingInputReceiver(receiver_tensors, receiver_tensors)\n    return fn_to_return\n\n\ndef main(unused_par):\n\n    SAVE_MODEL_DIR = ""/srv/tmp/diag_loss_3/pre_commit_test_2534111""\n    TRAINING_FILENAME_PATTERN = ""/srv/databases/lichess_combined_methods_eval_databases/lichess_training.tfrecords""\n    VALIDATION_FILENAME_PATTERN = ""/srv/databases/lichess_combined_methods_eval_databases/lichess_validation.tfrecords""\n    TRAIN_OP_SUMMARIES = [""gradient_norm"", ""gradients""]\n    NUM_INPUT_FILTERS = 15\n    OPTIMIZER = \'Adam\'\n    TRAINING_SHUFFLE_BUFFER_SIZE = 16800000\n    TRAINING_BATCH_SIZE = 512      #The effective batch size used for the loss = n(n-1)/2  (where n is the number of boards in the batch)\n    VALIDATION_BATCH_SIZE = 1000\n    LOG_ITERATION_INTERVAL = 2500\n    LEARNING_RATE = 2.5e-3\n    KERNEL_REGULARIZER = lambda: None\n    KERNEL_INITIALIZER = lambda: tf.contrib.layers.variance_scaling_initializer()\n    TRAINABLE_CNN_MODULES = True\n    DATA_FORMAT = ""NCHW""\n    SAME_MATE_DEPTH_DIFF_LOSS_WEIGHT_DECREMENT = .95\n    VALUE_LARGER_THAN_CENTIPAWN_LESS_THAN_MATE = 100000\n\n    num_examples_in_training_file = 16851682\n    num_examples_in_validation_file = 1982551\n\n    BATCHES_IN_TRAINING_EPOCH = num_examples_in_training_file // TRAINING_BATCH_SIZE\n    BATCHES_IN_VALIDATION_EPOCH =  num_examples_in_validation_file // VALIDATION_BATCH_SIZE\n\n    # learning_decay_function = lambda gs: LEARNING_RATE\n    learning_decay_function = lambda gs : tf.train.exponential_decay(LEARNING_RATE, gs,\n                                                                     BATCHES_IN_TRAINING_EPOCH, 0.96, staircase=True)\n\n    CONVOLUTIONAL_MODULES = [[[[512, 1], [128, 1]] + 6 * [[32, 3]] + [(16, 8)]]]\n\n\n    # Create the Estimator\n    the_estimator = tf.estimator.Estimator(\n        model_fn=diag_comparison_model_fn,\n        model_dir=SAVE_MODEL_DIR,\n        config=tf.estimator.RunConfig().replace(\n            save_checkpoints_steps=LOG_ITERATION_INTERVAL,\n            save_summary_steps=LOG_ITERATION_INTERVAL),\n        params={\n            ""optimizer"": OPTIMIZER,\n            ""log_interval"": LOG_ITERATION_INTERVAL,\n            ""model_dir"": SAVE_MODEL_DIR,\n            ""convolutional_modules"" : CONVOLUTIONAL_MODULES,\n            ""learning_rate"": LEARNING_RATE,\n            ""train_summaries"": TRAIN_OP_SUMMARIES,\n            ""learning_decay_function"" : learning_decay_function,\n            ""num_input_filters"" : NUM_INPUT_FILTERS,\n            ""kernel_initializer"" : KERNEL_INITIALIZER,\n            ""kernel_regularizer"" : KERNEL_REGULARIZER,\n            ""trainable_cnn_modules"" : TRAINABLE_CNN_MODULES,\n            ""same_mate_depth_diff_decrement"" : SAME_MATE_DEPTH_DIFF_LOSS_WEIGHT_DECREMENT,\n            ""value_larger_than_centipawn_less_than_mate"" : VALUE_LARGER_THAN_CENTIPAWN_LESS_THAN_MATE,\n            ""data_format"" : DATA_FORMAT,\n            })\n\n\n    validation_hook = ann_h.ValidationRunHook(\n        step_increment=BATCHES_IN_TRAINING_EPOCH,\n        estimator=the_estimator,\n        input_fn_creator=lambda: lambda : lower_diag_score_comparison_input_fn(\n            VALIDATION_FILENAME_PATTERN,\n            VALIDATION_BATCH_SIZE,\n            include_unoccupied=NUM_INPUT_FILTERS == 16,\n            num_things_to_prefetch=5,\n            num_things_in_parallel=12,\n            data_format=DATA_FORMAT,\n        ),\n        temp_num_steps_in_epoch=BATCHES_IN_VALIDATION_EPOCH)\n\n    the_estimator.train(\n        input_fn=lambda : lower_diag_score_comparison_input_fn(\n            TRAINING_FILENAME_PATTERN,\n            TRAINING_BATCH_SIZE,\n            shuffle_buffer_size=TRAINING_SHUFFLE_BUFFER_SIZE,\n            include_unoccupied=NUM_INPUT_FILTERS == 16,\n            num_things_in_parallel=12,\n            num_things_to_prefetch=36,\n            data_format=DATA_FORMAT,\n            shuffle_seed=12312312,\n        ),\n        hooks=[validation_hook],\n        # max_steps=1,\n    )\n\n    # Save the model for inference\n    the_estimator.export_savedmodel(SAVE_MODEL_DIR, board_eval_serving_input_receiver())\n\n\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
batch_first/anns/move_evaluation_ann.py,0,"b'import tensorflow as tf\n\nfrom scipy.stats import kendalltau, weightedtau, spearmanr\n\nimport batch_first.anns.ann_creation_helper as ann_h\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef lower_diag_policy_comparison_model_fn(features, labels, mode, params):\n    """"""\n    Generates an EstimatorSpec for a model which scores pairs of chess boards and moves.  It learns by maximizing the\n    difference between the board/move paris, where one is intended to be greater than the other based on\n    some pre-calculated scoring system (e.g. StockFish evaluations).\n\n    The value should be the value of the board after the given move has been made.\n    """"""\n    convolutional_module_outputs = ann_h.create_input_convolutions_shared_weights(\n        features[\'board\'],\n        params[\'kernel_initializer\'],\n        params[\'data_format\'],\n        mode,\n        num_unique_filters=[32, 20, 20, 18, 18, 16, 24, 24])  #236 with the 64 undilated included\n\n    if len(params[\'convolutional_modules\']):\n        convolutional_module_outputs = ann_h.build_convolutional_modules(\n            convolutional_module_outputs,\n            params[\'convolutional_modules\'],\n            mode,\n            params[\'kernel_initializer\'],\n            params[\'kernel_regularizer\'],\n            params[\'trainable_cnn_modules\'],\n            num_previous_modules=1,\n            data_format=params[\'data_format\'])\n\n    original_logits = tf.layers.conv2d(\n        inputs=convolutional_module_outputs,\n        filters=params[\'num_logit_filters\'],\n        kernel_size=1,\n        padding=""valid"",\n        data_format=""channels_last"" if params[\'data_format\']==""NHWC"" else ""channels_first"",\n        use_bias=False,\n        kernel_initializer=params[\'kernel_initializer\'](),\n        kernel_regularizer=params[\'kernel_regularizer\'](),\n        name=""logit_layer"")\n\n\n    lookup_str = ""move_to_square"" if params[\'num_logit_filters\']==64 else ""move_filter""\n    if params[\'data_format\'] == ""NHWC"":\n        new_logit_shape = [-1, 64, params[\'num_logit_filters\']]\n        first_square = ""move_from_square""\n        second_square = lookup_str\n    else:\n        new_logit_shape = [-1, params[\'num_logit_filters\'], 64]\n        first_square = lookup_str\n        second_square = ""move_from_square""\n\n\n    move_reshaped_logits = tf.reshape(original_logits, new_logit_shape)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        range_repeater = ann_h.numpy_style_repeat_1d_creator(out_type=tf.int64)\n        board_indices = range_repeater(features[\'moves_per_board\'])\n    else:\n        num_moves = tf.shape(features[\'move_from_square\'])[0]\n        board_indices = tf.range(num_moves, dtype=tf.int32)\n\n    indices_to_gather = tf.stack([\n        board_indices,\n        tf.cast(features[first_square], dtype=board_indices.dtype),\n        tf.cast(features[second_square], dtype=board_indices.dtype)], axis=1)\n\n    logits = tf.gather_nd(move_reshaped_logits, indices_to_gather, name=""requested_move_scores"")\n\n\n    loss = None\n    train_op = None\n\n    # Calculate loss\n    if mode != tf.estimator.ModeKeys.PREDICT:\n        with tf.variable_scope(""loss""):\n            calculated_diff_matrix = ann_h.vec_and_transpose_op(logits, tf.subtract)\n            label_matrix = features[\'label_matrix\']\n            weight_matrix = features[\'weight_matrix\']\n\n            loss = tf.losses.sigmoid_cross_entropy(label_matrix, calculated_diff_matrix, weights=weight_matrix)\n            loss_summary = tf.summary.scalar(""loss"", loss)\n\n\n    # Configure the Training Op\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        global_step = tf.train.get_global_step()\n        learning_rate = params[\'learning_decay_function\'](global_step)\n        tf.summary.scalar(""learning_rate"", learning_rate)\n        train_op = tf.contrib.layers.optimize_loss(\n            loss=loss,\n            global_step=global_step,\n            learning_rate=learning_rate,\n            optimizer=params[\'optimizer\'],\n            summaries=params[\'train_summaries\'])\n\n    predictions = {""scores"" : logits}\n    the_export_outputs = {""serving_default"" : tf.estimator.export.RegressionOutput(value=logits)}\n\n    validation_metrics = None\n    training_hooks = []\n\n    # Create the metrics\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        tau_a = ann_h.kendall_rank_correlation_coefficient(logits, features[\'score\'])\n\n        to_create_metric_dict = {\n            ""loss/loss"": (loss, loss_summary),\n            ""metrics/mean_evaluation_value"" : logits,\n            ""metrics/mean_abs_evaluation_value"": tf.abs(logits),\n            ""metrics/kendall_tau-a"" : tau_a,\n        }\n\n        validation_metrics = ann_h.metric_dict_creator(to_create_metric_dict)\n\n        tf.contrib.layers.summarize_tensors(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n        training_hooks.append(\n            tf.train.SummarySaverHook(\n                save_steps=params[\'log_interval\'],\n                output_dir=params[\'model_dir\'],\n                summary_op=tf.summary.merge_all()))\n\n    elif mode == tf.estimator.ModeKeys.EVAL:\n        the_logits, update1 = tf.contrib.metrics.streaming_concat(logits)\n        the_labels, update2 = tf.contrib.metrics.streaming_concat(features[\'score\'])\n\n        update = tf.group(update1, update2)\n\n        scipy_rank_coef_creator = ann_h.py_func_scipy_rank_helper_creator(the_logits, the_labels)\n\n        tau_b, tau_b_p_value = scipy_rank_coef_creator(kendalltau)\n        weighted_tau_b, weighted_tau_b_p_value = scipy_rank_coef_creator(weightedtau)\n        rho, rho_p_value = scipy_rank_coef_creator(spearmanr)\n\n        validation_metrics = {\n            ""metrics/mean_evaluation_value"" : tf.metrics.mean(logits),\n            ""metrics/mean_abs_evaluation_value"" : tf.metrics.mean(tf.abs(logits)),\n            ""metrics/kendall_tau-b"": (tau_b, update),\n            ""metrics/kendall_tau-b_p_value"": (tau_b_p_value, update),\n            ""metrics/weighted_kendall_tau-b"": (weighted_tau_b, update),\n            ""metrics/weighted_kendall_tau-b_p_value"": (weighted_tau_b_p_value, update),\n            ""metrics/spearman_rho"": (rho, update),\n            ""metrics/spearman_rho_p_value"": (rho_p_value, update),\n        }\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=predictions,\n        loss=loss,\n        train_op=train_op,\n        training_hooks=training_hooks,\n        export_outputs=the_export_outputs,\n        eval_metric_ops=validation_metrics)\n\n\ndef simpler_lower_diag_score_comparison_input_fn(filename_pattern, batch_size, include_unoccupied=True, shuffle_buffer_size=None,\n                                             num_things_in_parallel=None, num_things_to_prefetch=None, shuffle_seed=None,\n                                             data_format=""NHWC""):\n    if num_things_to_prefetch is None:\n        num_things_to_prefetch = tf.contrib.data.AUTOTUNE  #IMPORTANT NOTE: This seems to make the program crash after a few iterations (at least it does on my computer)\n\n    dataset = tf.data.TFRecordDataset(filename_pattern)\n\n    if shuffle_buffer_size:\n        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(shuffle_buffer_size,seed=shuffle_seed))\n\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n\n    def process_batch(records):\n        keys_to_features = {""board"": tf.FixedLenFeature([8 * 8], tf.int64),\n                            ""score"": tf.FixedLenFeature([], tf.int64),\n                            ""move_from_square"": tf.FixedLenFeature([], tf.int64),\n                            ""move_to_square"": tf.FixedLenFeature([], tf.int64),\n                            ""move_filter"": tf.FixedLenFeature([], tf.int64)}\n\n        parsed_examples = tf.parse_example(records, keys_to_features)\n\n        reshaped_boards = tf.reshape(parsed_examples[\'board\'], [-1, 8, 8])\n\n        omit_unoccupied_decrement = 0 if include_unoccupied else 1\n        boards = tf.one_hot(\n            reshaped_boards - omit_unoccupied_decrement,\n            16 - omit_unoccupied_decrement,\n            axis=-1 if data_format==""NHWC"" else 1)\n\n        desired_diff_matrix = ann_h.vec_and_transpose_op(parsed_examples[\'score\'], tf.subtract, tf.float32)\n\n        lower_diag_diff_matrix = tf.matrix_band_part(desired_diff_matrix, -1, 0)\n\n        lower_diag_sign = tf.sign(lower_diag_diff_matrix)\n        weight_mask = tf.abs(lower_diag_sign)\n        # bool_weight_mask = tf.cast(weight_mask, tf.bool)\n        #\n        # value_larger_than_centipawn_less_than_mate = 100000\n        # desired_found_mate = tf.greater(tf.abs(parsed_examples[\'score\']), value_larger_than_centipawn_less_than_mate)\n        #\n        # both_found_mate = ann_h.vec_and_transpose_op(desired_found_mate, tf.logical_and)\n        #\n        # desired_signs = tf.sign(parsed_examples[\'score\'])\n        #\n        # same_sign_matrix = ann_h.vec_and_transpose_op(desired_signs, tf.equal)\n        #\n        # both_same_player_mates = tf.logical_and(both_found_mate, same_sign_matrix)\n        #\n        # both_same_mate_and_nonzero_weight = tf.logical_and(both_same_player_mates, bool_weight_mask)\n        #\n        # same_mate_depth_diff_decrement = .95\n        # weight_helper = same_mate_depth_diff_decrement * tf.cast(both_same_mate_and_nonzero_weight, tf.float32)\n        #\n        # mate_adjusted_weight_mask = weight_mask - weight_helper\n\n        label_matrix = (lower_diag_sign + weight_mask)/2\n\n        # return (boards, parsed_examples[\'score\'], label_matrix, mate_adjusted_weight_mask,\n        #         parsed_examples[\'move_filter\'], parsed_examples[\'move_to_square\'], parsed_examples[\'move_filter\'])\n\n        return (boards, parsed_examples[\'score\'], label_matrix, weight_mask,\n                parsed_examples[\'move_filter\'], parsed_examples[\'move_to_square\'], parsed_examples[\'move_filter\'])\n\n\n    dataset = dataset.map(process_batch, num_parallel_calls=num_things_in_parallel)\n\n    dataset = dataset.apply(tf.contrib.data.prefetch_to_device(\'/gpu:0\', buffer_size=num_things_to_prefetch))\n\n    iterator = dataset.make_one_shot_iterator()\n\n    features = iterator.get_next()\n\n    feature_names = [""board"", ""score"", ""label_matrix"", ""weight_matrix"", ""move_from_square"", ""move_to_square"", ""move_filter""]\n\n    feature_dict = dict(zip(feature_names, features))\n    return feature_dict, None\n\n\ndef move_scoring_serving_input_receiver(data_format=""NCHW""):\n    def fn_to_return():\n        placeholder_shape = [None, 15, 8, 8] if data_format == ""NCHW"" else [None, 8, 8, 15]\n        for_remapping = tf.placeholder(tf.float32, placeholder_shape, ""FOR_INPUT_MAPPING_transpose"")\n\n        moves_per_board = tf.placeholder(tf.uint8, shape=[None], name=""moves_per_board_placeholder"")\n        from_squares = tf.placeholder(tf.uint8, shape=[None], name=""from_square_placeholder"")\n        move_filters = tf.placeholder(tf.uint8, shape=[None], name=""move_filter_placeholder"")\n\n        receiver_tensors = {\n            ""board"": for_remapping,\n            ""move_from_square"": from_squares,\n            ""move_filter"": move_filters,\n            ""moves_per_board"": moves_per_board,\n        }\n\n        return tf.estimator.export.ServingInputReceiver(receiver_tensors, receiver_tensors)\n    return fn_to_return\n\n\n\n\ndef main(unused_par):\n    SAVE_MODEL_DIR = ""/srv/tmp/diag_move_loss_314/pre_commit_test_1""\n    TRAINING_FILENAME_PATTERN = ""/srv/databases/lichess_just_move_scoring_fixed_ag_promotion/lichess_training.tfrecords""\n    VALIDATION_FILENAME_PATTERN = ""/srv/databases/lichess_just_move_scoring_fixed_ag_promotion/lichess_validation.tfrecords""\n    TRAIN_OP_SUMMARIES = [""gradient_norm"", ""gradients""]\n    NUM_INPUT_FILTERS = 15\n    OPTIMIZER = \'Adam\'\n    TRAINING_SHUFFLE_BUFFER_SIZE = 17100000\n    TRAINING_BATCH_SIZE = 512     #The effective batch size used for the loss = n(n-1)/2  (where n is the number of boards in the batch)\n    VALIDATION_BATCH_SIZE = 1024\n    LOG_ITERATION_INTERVAL = 2500\n    LEARNING_RATE = 5e-3\n    KERNEL_REGULARIZER = lambda: None\n    KERNEL_INITIALIZER = lambda: tf.contrib.layers.variance_scaling_initializer()\n    TRAINABLE_CNN_MODULES = True\n    DATA_FORMAT = ""NCHW""\n\n\n    NUM_LOGIT_FILTERS = 73 #64\n\n    num_examples_in_training_file = 17106078\n    num_examples_in_validation_file = 2012480\n\n    BATCHES_IN_TRAINING_EPOCH = num_examples_in_training_file // TRAINING_BATCH_SIZE\n    BATCHES_IN_VALIDATION_EPOCH =  num_examples_in_validation_file // VALIDATION_BATCH_SIZE\n\n    learning_decay_function = lambda gs: LEARNING_RATE\n\n\n    CONVOLUTIONAL_MODULES = [[[[512, 1], [128, 1]] + 6 * [[32, 3]]]]\n\n    # Create the Estimator\n    the_estimator = tf.estimator.Estimator(\n        model_fn=lower_diag_policy_comparison_model_fn,\n        model_dir=SAVE_MODEL_DIR,\n        config=tf.estimator.RunConfig().replace(\n            save_checkpoints_steps=LOG_ITERATION_INTERVAL,\n            save_summary_steps=LOG_ITERATION_INTERVAL),\n        params={\n            ""optimizer"": OPTIMIZER,\n            ""log_interval"": LOG_ITERATION_INTERVAL,\n            ""model_dir"": SAVE_MODEL_DIR,\n            ""convolutional_modules"" : CONVOLUTIONAL_MODULES,\n            ""learning_rate"": LEARNING_RATE,\n            ""train_summaries"": TRAIN_OP_SUMMARIES,\n            ""learning_decay_function"" : learning_decay_function,\n            ""num_input_filters"" : NUM_INPUT_FILTERS,\n            ""kernel_initializer"" : KERNEL_INITIALIZER,\n            ""kernel_regularizer"" : KERNEL_REGULARIZER,\n            ""trainable_cnn_modules"" : TRAINABLE_CNN_MODULES,\n            ""data_format"" : DATA_FORMAT,\n            ""num_logit_filters"" : NUM_LOGIT_FILTERS,\n            })\n\n\n    validation_hook = ann_h.ValidationRunHook(\n        step_increment=BATCHES_IN_TRAINING_EPOCH//2,\n        estimator=the_estimator,\n        input_fn_creator=lambda: lambda : simpler_lower_diag_score_comparison_input_fn(\n            VALIDATION_FILENAME_PATTERN,\n            VALIDATION_BATCH_SIZE,\n            include_unoccupied=NUM_INPUT_FILTERS == 16,\n            num_things_to_prefetch=1,\n            num_things_in_parallel=12,\n            data_format=DATA_FORMAT),\n        temp_num_steps_in_epoch=BATCHES_IN_VALIDATION_EPOCH)\n\n    the_estimator.train(\n        input_fn=lambda : simpler_lower_diag_score_comparison_input_fn(\n            TRAINING_FILENAME_PATTERN,\n            TRAINING_BATCH_SIZE,\n            shuffle_buffer_size=TRAINING_SHUFFLE_BUFFER_SIZE,\n            include_unoccupied=NUM_INPUT_FILTERS == 16,\n            num_things_in_parallel=12,\n            num_things_to_prefetch=1,\n            data_format=DATA_FORMAT),\n        hooks=[validation_hook],\n        # max_steps=1,\n    )\n\n    # Save the model for inference\n    the_estimator.export_savedmodel(SAVE_MODEL_DIR, move_scoring_serving_input_receiver())\n\n\n\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n\n\n\n'"
