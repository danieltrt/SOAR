file_path,api_count,code
setup.py,0,"b'""""""A setuptools based setup module.\n\nSee:\nhttps://packaging.python.org/en/latest/distributing.html\nhttps://github.com/pypa/sampleproject\n""""""\n\nfrom setuptools import setup, find_packages\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nsetup(\n    name=\'seipy\',\n    version=\'1.3.2\',\n    description=\'Helper functions for data science\',\n    long_description=long_description,\n    url=\'https://github.com/Seiji-Armstrong/seipy\',\n    author_email=\'seiji.armstrong@gmail.com\',\n    license=\'MIT\',\n    classifiers=[\n        \'Development Status :: 5 - Production/Stable\',\n        \'Intended Audience :: Developers\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n    ],\n    keywords=\'pandas numpy spark jupyter data-science machine-learning s3\',\n    packages=find_packages(exclude=[]),\n    install_requires=[\'pandas\',\n                      \'requests\',\n                      \'scipy\',\n                      \'scikit-learn\',\n                      \'numpy\',\n                      \'pandas\',\n                      \'matplotlib\',\n                      \'scapy-python3\',\n                      \'seaborn\',\n                      \'ipython\',\n                      \'boto3\',\n                      \'pyspark\'],\n    python_requires=\'>=3\',\n)\n'"
seipy/__init__.py,0,b'from .pandas_ import *\nfrom .numpy_ import *\nfrom .ml import *\nfrom .base import *\nfrom .jupyter_ import *\nfrom .spark_ import *\nfrom .s3_ import *\n'
seipy/base.py,4,"b'import subprocess\nfrom os import walk\nfrom os.path import join\nfrom functools import reduce\nfrom collections import Counter, namedtuple\nimport ast\nimport mmap\nimport re\nimport json\nimport pandas as pd\n\n\nclass Stack:\n    """"""\n    simple stack class\n    """"""\n    def __init__(self):\n        self.items = []\n\n    def isEmpty(self):\n        return self.items == []\n\n    def push(self, item):\n        self.items.insert(0, item)\n\n    def pop(self):\n        return self.items.pop(0)\n\n    def peek(self):\n        return self.items[0]\n\n    def size(self):\n        return len(self.items)\n\n\nclass LabeledData(dict):\n    """"""Container object for datasets\n\n    Dictionary-like object that exposes its keys as attributes.\n\n    (copied from sklearn.datasets.base.Bunch)\n    """"""\n\n    def __init__(self, **kwargs):\n        # type: (object) -> object\n        super(LabeledData, self).__init__(kwargs)\n\n    def __setattr__(self, key, value):\n        self[key] = value\n\n    def __dir__(self):\n        return self.keys()\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(key)\n\n    def __setstate__(self, state):\n        # https://github.com/scikit-learn/scikit-learn/issues/6196\n        pass\n\n\ndef issue_shell_command(cmd: str, my_env=None):\n    """"""\n    Issues a command in a shell and returns the result as str.\n\n    Parameters:\n    cmd - command to be issued (str)\n\n    In python3.x, stdout,stderr are both b\'\' (byte string literal: bytes object)\n        and must be decoded to UTF-8 for string concatenation etc\n    Example usage (simple):\n    >> issue_shell_command(cmd=""ls"")\n    Example usage (more involved):\n    >> s3dir = ""s3://...""; issue_shell_command(""aws s3 ls --recursive {}"".format(s3dir))\n    """"""\n    pipe = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE,\n                          stderr=subprocess.PIPE, env=my_env)\n\n    return pipe.stdout.strip().decode(\'UTF-8\') + \'\\n\' + pipe.stderr.strip().decode(\'UTF-8\')\n\n\ndef merge_dicts(*dict_args):\n    """"""\n    Given any number of dicts, shallow copy and merge into a new dict,\n    precedence goes to key value pairs in latter dicts.\n    """"""\n    result = {}\n    for dictionary in dict_args:\n        result.update(dictionary)\n    return result\n\n\ndef merge_two_dicts(dict_1, dict_2):\n    """"""\n    Given two dicts, return one merged dict.\n    """"""\n    return {**dict_1, **dict_2}\n\n\ndef merge_multi_dicts(*dict_args):\n    """"""\n    Given multiple dicts, return one merge dict by calling `reduce` on `merge_two_dicts`.\n    ALternative: reduce(lambda d1,d2: {**d1,**d2}, dict_args[0])\n    """"""\n    return reduce(merge_two_dicts, dict_args[0])\n\n\ndef swap_key_val_dict(a_dict):\n    """"""\n    swap the keys and values of a dict\n    """"""\n    return {val: key for key, val in a_dict.items()}\n\n\ndef enumerate_with_prefix(a_list, prefix=\'pre_\'):\n    """"""\n    given a list, return a list enumerated with prefix.\n    """"""\n    num_digits = len(str(len(a_list)))  # eg 5 -> 1, 15 -> 2, 150 -> 3 etc.\n    enum_list = [prefix + str(idx).zfill(num_digits)\n                 for idx, el in enumerate(a_list)]\n    return enum_list\n\n\ndef relevant_files(root_dir, include_regex=\'\', exclude=""*****""):\n    """"""Return list of files with inclusion regex and exclusion regex.\n\n    inputs:\n    ""root_dir"" is the root directory\n    ""include_regex"" is the string that is searched for within filenames\n    ""exclude_regex"" is the string that will exclude files if found in name.\n\n    returns:\n    list of filenames in all subsequent folders, matched with include_regex.\n    """"""\n    f_list = []\n    for (dir_path, _, file_names) in walk(root_dir):\n        f_list.extend(join(dir_path, filename)\n                      for filename in file_names\n                      if (include_regex in filename))\n    f_list = [el for el in f_list if exclude not in el]\n    return f_list\n\n\ndef relevant_files_list(root_dir, include_list=[], exclude=""*****""):\n    """"""Return list of files with inclusion regex in list and exclusion regex.\n\n    inputs:\n    ""root_dir"" is the root directory\n    ""include_list"" is list of strings that is searched for within filenames\n    ""exclude_regex"" is the string that will exclude files if found in name.\n\n    returns:\n    list of filenames in all subsequent folders, matched with include_regex.\n    """"""\n    f_list = []\n    for (dir_path, _, file_names) in walk(root_dir):\n        f_list.extend(join(dir_path, filename)\n                      for filename in file_names\n                      if any(el in filename for el in include_list))\n    f_list = [el for el in f_list if exclude not in el]\n    return f_list\n\n\ndef files_containing_str(str_, file_list):\n    """"""\n    return all files in provided file_list containing str_.\n    """"""\n    return [el for el in file_list if str_in_file(str_, el)]\n\n\ndef date_range_array(start=\'2016-12-1\', end=\'2016-12-5\', strf=\'%Y-%m-%d\'):\n    """"""\n    Return a numpy.array of dates in range between start and stop.\n    Format of date string is given by strf.\n    Wrapper around pandas.date_range\n    """"""\n    return pd.date_range(start=start, end=end).map(lambda x: x.strftime(strf))\n\n\ndef str_in_file(str_, file_):\n    """"""\n    Returns Boolean: True if str_ in file_, otherwise False.\n    First converts string to Bytes.\n    Uses mmap in order not to read entire file each time.\n    """"""\n    b_ = str_.encode()\n    with open(file_, \'rb\', 0) as file, \\\n            mmap.mmap(file.fileno(), 0, access=mmap.ACCESS_READ) as s:\n        if re.search(b_, s):\n            return True\n        return False\n\n\ndef str_from_collec(collection, no_space=False):\n    """"""\n    Given a collection, return a string of values\n        Collection can be: column of DataFrame (Series), list, etc.\n\n    Useful in creating fv_str strings for example, by feeding row of DataFrame\n    containing only feature columns\n    """"""\n    if no_space:\n        return \',\'.join(map(str, collection))\n    else:\n        return \', \'.join(map(str, collection))\n\n\ndef uniq_tokens_in_nested_col(col_series):\n    """"""\n    Given a column in a dataframe containing lists of tokens,\n        return unique tokens.\n    Can also receive a list of lists, pd.Series of lists, np.array of lists.\n    """"""\n    return set([el for sublist in col_series for el in sublist])\n\n\ndef uniq_tokens_css(thresh, *collection_vals):\n    """"""\n    Extract unique tokens in comma separated strings, contained in a collection,\n        return list of unique tokens that occur more than a threshold value.\n    Apply sorted to output for reproducible output.\n    """"""\n    temp_lists = []\n    for vals in collection_vals:\n        temp_lists += [el.split(\',\') for el in vals]\n    temp_fulllist = [el.strip() for sublist in temp_lists for el in sublist]\n    temp_count = Counter(temp_fulllist)\n    return sorted([k for k, v in list(temp_count.items()) if v > thresh])\n\n\ndef str_list_to_list_str(str_list, regex_pattern=\'[A-Z]\\d+\'):\n    """"""\n    Turn a string of a list into a list of string tokens.\n    Tokens determined by regex_pattern\n    """"""\n    p = re.compile(regex_pattern)\n    return p.findall(str_list)\n\n\ndef save_json(dict_to_json, save_path):\n    """"""\n    Saves dict to json file\n    """"""\n    out_file = open(save_path, ""w"")\n    # save the dictionary to this file\n    json.dump(dict_to_json, out_file, indent=4)\n    # close the file\n    out_file.close()\n    print(""json file saved at: {}"".format(save_path))\n\n\ndef load_json(path_to_load):\n    """"""\n    Loads json from file\n    """"""\n    with open(path_to_load, \'r\') as f:\n        return json.load(f)\n\n\ndef vals_sortby_key(dict_to_sort):\n    """"""\n    sort dict by keys alphanumerically, then return vals.\n    Keys should be ""feat_00, feat_01"", or ""stage_00, stage_01"" etc.\n    """"""\n    return [val for (key, val) in sorted(dict_to_sort.items())]\n\n\ndef diff_2_lists(list_1, list_2):\n    """"""\n    return elements not common to both lists\n    Symmetric difference operator ^ is used instead of -.\n    """"""\n    return list(set(list_1) ^ set(list_2))\n\n\ndef file_len(fname):\n    """"""\n    count number of lines in file\n    copy-pasted from http://stackoverflow.com/questions/845058/how-to-get-line-count-cheaply-in-python\n    """"""\n    with open(fname) as f:\n        for i, l in enumerate(f):\n            pass\n    return i + 1\n\n\ndef literal(str_field: str):\n    """"""\n    converts string of object back to object\n    example:\n    $ str_literal(\'[\'a\',\'b\',\'c\']\')\n    [\'a\', \'b\', \'c\'] # type is list\n    """"""\n    return ast.literal_eval(str_field)\n\n\ndef cumsum(collec, tuple_list_id=None, thresh=None):\n    """"""\n    Return index at which the cumulative sum of the collection exceeds the threshold.\n    If the collection is a list of tuples, then a new list is created from only the\n        indexed element of the tuple.\n    """"""\n    if tuple_list_id is not None:\n        collec = [el[tuple_list_id] for el in collec]\n    i = 0\n    cumsum = 0\n    if sum(collec) < thresh:\n        print(""threshold higher than sum of collection."")\n        return len(collec)\n    while cumsum < thresh:\n        cumsum += collec[i]\n        i += 1\n    return i\n\n\ndef named_two_tuples(list_1, list_2, tuple_name=\'ntup\', tuple_el_names=\'x y\'):\n    """"""\n    Given two ordered lists of x and y\n        return a list of namedtuples\n\n    Example usage:\n    >> named_two_tuples(feat_importance, feat_names,\n                        tuple_name=\'fimp\', tuple_el_names=\'importance name\')\n    """"""\n    ntup = namedtuple(tuple_name, tuple_el_names)\n    return [ntup(el[0], el[1]) for el in zip(list_1, list_2)]\n\n\ndef replace_rn(strobj):\n    """"""\n    There are characters such as ""\\r"" and ""\\n"" that perform line breaks (or returns),\n    and these can create parsing dramas when reading/writing to disk.\n    This helper function mitigates this by replacing it with a benign alternative.\n    """"""\n    return strobj.replace(\'\\r\', \'esc-r-\').replace(\'\\n\', \'esc-n-\')\n\n\ndef get_nested_item(data_dict, key_list):\n    """"""\n    obtain the deepest nested item in a nested dict given keys in a list.\n    """"""\n    item = data_dict.copy()\n    for k in key_list:\n        item = item[k]\n    return item\n\n\ndef infix_to_postfix(infixexpr: str, token_regex: str, precedences: dict):\n    """"""\n    convert infix to postfix\n    adapted from http://interactivepython.org/runestone/static/pythonds/BasicDS/InfixPrefixandPostfixExpressions.html\n\n    Example usage:\n\n    >> token_regex = ""c[0-9]""\n    >> prec = {""("": 1,\n        ""MINUS"": 2,\n        ""OR"": 3,\n        ""AND"": 4}\n    >> expr = \'( c1 AND c2 ) MINUS c3\'\n    >> infix_to_postfix(expr, token_regex, prec)\n    [out]:  \'c1 c2 AND c3 MINUS\'\n\n    """"""\n    op_stack = Stack()\n    postfix_list = []\n    token_list = infixexpr.split()\n    for token in token_list:\n        if re.search(token_regex, token):\n            postfix_list.append(token)\n        elif token == \'(\':\n            op_stack.push(token)\n        elif token == \')\':\n            top_token = op_stack.pop()\n            while top_token != \'(\':\n                postfix_list.append(top_token)\n                top_token = op_stack.pop()\n        else:\n            while (not op_stack.isEmpty()) and \\\n               (precedences[op_stack.peek()] >= precedences[token]):\n                  postfix_list.append(op_stack.pop())\n            op_stack.push(token)\n\n    while not op_stack.isEmpty():\n        postfix_list.append(op_stack.pop())\n    return "" "".join(postfix_list)\n\n\ndef postfix_computer(tokens: list, logic: dict, operations: dict):\n    """"""\n    compute postfix expression given list of tokens.\n     Each token in `tokens` must either be a key in `logic` or `operations`\n     Example:\n          op_dict = {\'AND\': lambda x,y: np.logical_and(x,y),\n           \'OR\': lambda x,y: np.logical_or(x,y),\n           \'MINUS\': lambda x,y: np.logical_and(x, np.logical_not(y))\n          }\n    """"""\n\n    stack = []\n    for token in tokens:\n        if token in logic:\n            stack.append(logic[token])\n            continue\n\n        op2, op1 = stack.pop(), stack.pop()\n        if token in operations:\n            stack.append(operations[token](op1, op2))\n\n    return stack.pop()\n\n\ndef list_import_modules(library_root):\n    """"""\n    given the root dir of python library, return unique list of import modules.\n      Useful for `install_requires` flag in setup.py, for example.\n    """"""\n    py_files = relevant_files(library_root, include_regex="".py"", exclude="".pyc"")\n    import_lines = []\n    for one_file in py_files:\n        with open(one_file, ""r"") as f:\n            lines = f.readlines()\n        for i, line in enumerate(lines):\n            if ""import "" in line:\n                if ""from"" in line:\n                    root = line.strip(\'\\n\').split(\'from \')[1]\n                else:\n                    root = line.strip(\'\\n\').split(\'import \')[1]\n                root = root.split(\' \')[0].split(\'.\')[0]\n                if root:\n                    import_lines.append(root)\n    return list(set(import_lines))\n'"
seipy/jupyter_.py,1,"b'from os.path import basename\nfrom IPython.display import display, HTML\nimport numpy as np\nimport pandas as pd\nimport sys\nfrom .base import date_range_array, relevant_files_list, files_containing_str\n\n\ndef show_mem_usage():\n    """"""\n    Displays memory usage from inspection\n    of global variables in this notebook\n\n    taken from\n    http://practicalpython.blogspot.com/2017/03/monitoring-memory-usage-in-jupyter.html\n    """"""\n    gl = sys._getframe(1).f_globals\n    vars = {}\n    for k, v in list(gl.items()):\n        # for pandas dataframes\n        if hasattr(v, \'memory_usage\'):\n            mem = v.memory_usage(deep=True)\n            if not np.isscalar(mem):\n                mem = mem.sum()\n            vars.setdefault(id(v), [mem]).append(k)\n        # work around for a bug\n        elif isinstance(v, pd.Panel):\n            v = v.values\n        vars.setdefault(id(v), [sys.getsizeof(v)]).append(k)\n    total = 0\n    for k, (value, *names) in vars.items():\n        if value > 1e6:\n            print(names, ""%.3fMB"" % (value / 1e6))\n        total += value\n    print(""%.3fMB"" % (total / 1e6))\n\n\ndef disp(df):\n    """"""\n    display a pandas DataFrame without the index\n     (wrapper of IPython.display.display)\n    designed for use within Jupyter notebook\n    """"""\n    display(HTML(df.to_html(index=False)))\n\n\ndef notebook_contains(search_str=\'\',\n                      on_docker=True,\n                      git_dir=\'~/git/experiments/\',\n                      start_date=\'2015-01-01\', end_date=\'2018-12-31\',\n                      exclude_str=\'checkpoint\',\n                      include_prefix=False,\n                      prefix=\'notebooks/\'):\n    """"""\n    return all jupyter notebooks containing search_str in specified time window.\n    TO DO: extend to list of search_strings, extend to list of exclude_strings.\n    """"""\n    if on_docker:\n        base_dir = ""/home/jovyan/work/""\n    else:\n        base_dir = git_dir[:]\n    dates = date_range_array(start=start_date, end=end_date)\n    rel_files = relevant_files_list(base_dir, dates, exclude_str)\n    files = files_containing_str(search_str, rel_files)\n    if prefix[-1] == \'/\':\n        prefix = prefix[:-1]\n    if include_prefix:\n        return [prefix+el.split(basename(prefix))[-1] for el in files]\n    else:\n        return [el.split(basename(prefix))[-1] for el in files]'"
seipy/s3_.py,0,"b'import zipfile\nimport boto3\nimport io\nimport datetime\nimport requests\nimport pandas as pd\n\n\ndef get_creds(cred_fpath=None, api_path=None):\n    """"""helper function to obtain aws keys\n    """"""\n    if cred_fpath is not None:\n        print(""reading keys from credentials file"")\n        keys = pd.read_csv(cred_fpath, sep=""="")\n        myAccessKey = keys.loc[\'aws_access_key_id \'][\'[default]\'].strip()\n        mySecretKey = keys.loc[\'aws_secret_access_key \'][\'[default]\'].strip()\n        myToken = """"\n    else:\n        r = requests.get(api_path)\n        creds = r.json()\n        myAccessKey = creds[""AccessKeyId""]\n        mySecretKey = creds[""SecretAccessKey""]\n        myToken = creds[""Token""]\n    return myAccessKey, mySecretKey, myToken\n\n\ndef s3zip_func(s3zip_path, _func=None, cred_fpath=None, api_path=None, num_files=-1, verbose=False, include=None, **kwargs):\n    """"""\n    unzip a zip file on s3 and perform func with kwargs.\n     func must accept `fpath` and `fname` as key word arguments.\n    fpath: pointer to unzipped subfile in zip file\n    fname: str of subfile in zip file\n    num_files: int: if -1 include all files\n\n    adapted from https://stackoverflow.com/questions/23376816/python-s3-download-zip-file\n    """"""\n\n    def operate(subfile, _func, verbose, **kwargs):\n        if verbose:\n            print(""current time is {}"".format(datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\')))\n            print(""{} opened."".format(subfile))\n        if _func is None:\n            return subfile\n        else:\n            return _func(fpath=zipf.open(subfile), fname=subfile, **kwargs)\n\n    s3bucket, s3zip = s3zip_path.split(""s3://"")[-1].split(\'/\', 1)\n    myAccessKey, mySecretKey, myToken = get_creds(cred_fpath=cred_fpath, api_path=api_path)\n\n    session = boto3.session.Session(\n        aws_access_key_id=myAccessKey,\n        aws_secret_access_key=mySecretKey,\n        aws_session_token=myToken\n    )\n\n    s3 = session.resource(""s3"")\n    bucket = s3.Bucket(s3bucket)\n    obj = bucket.Object(s3zip)\n\n    with io.BytesIO(obj.get()[""Body""].read()) as tf:\n\n        # rewind the file\n        tf.seek(0)\n\n        # Read the file as a zipfile and process the members\n        with zipfile.ZipFile(tf, mode=\'r\') as zipf:\n            zipfiles = zipf.namelist()\n            if include is None:\n                include = zipfiles\n            if num_files == -1:\n                num_files = len(zipfiles)\n            results = [operate(subfile, _func, verbose, **kwargs)\n                       for subfile in zipfiles[:num_files] if subfile in include]\n    return results\n'"
seipy/ml/__init__.py,0,b''
seipy/ml/linear_algebra.py,0,"b'from scipy.spatial import distance\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\ndef cos_sim_sk(a, b):\n    """"""\n    Wrapper around sklearn.metrics.pairwise.cosine_similarity\n    Useful for bypassing DeprecationWarning by reshaping.\n\n    Inputs are of type numpy.ndarray\n    """"""\n    return cosine_similarity(a.reshape(1, -1), b.reshape(1, -1))[0][0]\n\n\ndef cos_sim_sci(vec1, vec2):\n    """"""\n    Returns cosine similarity of 2 vectors.\n    Uses scipy.spatial.distance.cosine\n    """"""\n    return 1 - distance.cosine(vec1, vec2)\n\n\ndef distmat(fframe=None, metric=None):\n    """"""\n    Generate a distance matrix from a DataFrame containing only feature columns.\n        The distance metric is specified with `metric`.\n    If called with no args, return list of possible metrics.\n\n    """"""\n    if fframe is None:\n        d_metric = [""braycurtis"", ""canberra"", ""chebyshev"", ""cityblock"", ""correlation"",\n                    ""cosine"", ""dice"", ""euclidean"", ""hamming"", ""jaccard"", ""kulsinski"",\n                    ""mahalanobis"", ""matching"", ""minkowski"", ""rogerstanimoto"",\n                    ""russellrao"", ""seuclidean"", ""sokalmichener"", ""sokalsneath"",\n                    ""sqeuclidean"", ""wminkowski"", ""yule""]\n        return d_metric\n    return distance.cdist(fframe.as_matrix(), fframe.as_matrix(), metric)'"
seipy/numpy_/__init__.py,0,b'from .base import *'
seipy/numpy_/base.py,4,"b'import numpy as np\n\n\ndef matlab3d_style(np3_array):\n    """"""\n    Return MATLAB style display of 3d Numpy array\n    :param np3_array: np.array with shape (x,y,3)\n    :return:\n    """"""\n    a = np3_array.copy()\n    print(np.array([a[:, :, 0], a[:, :, 1], a[:, :, 2]]))\n\n    return np3_array\n\n\ndef identity(x):\n    return x\n\n\ndef num_cols(arr: np.array):\n    """"""\n    returns number of columns in numpy array\n    Note, this work around is necessary as the shape tuple for a one column array\n        returns (x, ), so arr.shape[1] doesn\'t always work.\n    """"""\n    return arr[:, np.newaxis].shape[-1]\n'"
seipy/pandas_/__init__.py,0,b'from .base import *'
seipy/pandas_/base.py,7,"b'""""""\nhelper functions to use with pandas library.\nConventions:\ndf -> pd.DataFrame\n\nIdeology:\n- Write functions that do one thing\n- Try write functions that are idempotent\n""""""\n\nfrom functools import reduce\nfrom collections import namedtuple\nimport numpy as np\nimport pandas as pd\nimport re\n\n\ndef drop_uniq_cols(df):\n    """"""\n    Returns DataFrame with columns that have more than one value (not unique)\n    """"""\n    return df.loc[:, df.apply(pd.Series.nunique) > 1]\n\n\ndef non_x_cols(df, x):\n    """"""\n    Returns DataFrame with columns that contain values other than x.\n    Drops any column that only contains x.\n    """"""\n    cond_1 = (df.apply(pd.Series.nunique) == 1)\n    cond_2 = (df.iloc[0] == x)\n    return df.loc[:, ~(cond_1 & cond_2)]\n\n\ndef df_duplicate_row(df, row, row_cols):\n    """"""\n    (Check logic)\n    Creates numpy array from df (pd.DataFrame),\n    returns non_duplicate rows.\n    """"""\n    X = df[row_cols].as_matrix().astype(np.float)\n    fv_ix = [ix for ix, el in enumerate(X) if (el == row.values).all()]\n    df_rows = df.ix[fv_ix]\n    return df_rows\n\n\ndef filt(orig_df, **params):\n    """"""\n    Filter DataFrame on any number of equality conditions.\n    Example usage:\n    >> filt(df,\n               season=""summer"",\n               age=("">"", 18),\n               sport=(""isin"", [""Basketball"", ""Soccer""]),\n               name=(""contains"", ""Armstrong"")\n               )\n    >> a = { \'season\': ""summer"", \'age\': ("">"", 18)}\n    >> filt(df, **a) # can feed in dict with **dict notation\n    notes:\n        any input with single value is assumed to use ""equivalent"" operation and is modified.\n        numpy.all is used to apply AND operation element-wise across 0th axis.\n        NA values are filled to False for conditions.\n    """"""\n    input_df = orig_df.copy()\n\n    if not params.items():\n        return input_df\n\n    def equivalent(a, b): return a == b\n\n    def greater_than(a, b): return a > b\n\n    def greater_or_equal(a, b): return a >= b\n\n    def less_than(a, b): return a < b\n\n    def less_or_equal(a, b): return a <= b\n\n    def isin(a, b): return a.isin(b)\n\n    def notin(a, b): return ~(a.isin(b))\n\n    def contains(a, b): return a.str.contains(b)\n\n    def notcontains(a, b): return ~(a.str.contains(b))\n\n    def not_equivalent(a, b): return a != b\n\n    operation = {""=="": equivalent,\n                 ""!="": not_equivalent,\n                 "">"": greater_than,\n                 "">="": greater_or_equal,\n                 ""<"": less_than,\n                 ""<="": less_or_equal,\n                 ""isin"": isin,\n                 ""notin"": notin,\n                 ""contains"": contains,\n                 ""notcontains"": notcontains}\n\n    cond = namedtuple(\'cond\', \'key operator val\')\n\n    filt_on = [(el[0], el[1]) if isinstance(el[1], tuple) else (el[0], (""=="", el[1]))\n               for el in params.items()]  # enforcing equivalence operation on single vals.\n    conds = [cond(el[0], el[1][0], el[1][1]) for el in filt_on]\n    logic = [operation[x.operator](input_df[x.key], x.val).fillna(False) for x in conds]\n    return input_df[np.all(logic, axis=0)]\n\n\ndef filt_read_csv(file_path, chunksize=100000, **filt_conds):\n    """"""\n    read a csv in chunks while filtering on conditions\n    wrapper of pd.read_csv and uses pandas_.base.filt\n    """"""\n    iter_csv = pd.read_csv(file_path, iterator=True, chunksize=chunksize)\n    return pd.concat([filt(chunk, **filt_conds) for chunk in iter_csv], ignore_index=True)\n\n\ndef display_max_rows(max_num=500):\n    """"""sets number of rows to display (useful in Jupyter environment)\n    """"""\n    pd.set_option(\'display.max_rows\', max_num)\n\n\ndef assign_index(input_df, index_col=\'idx\'):\n    """"""Return input DataFrame with id col.\n    """"""\n    out_df = input_df.reset_index(drop=True)\n    out_df.loc[:, index_col] = out_df.index\n    return out_df\n\n\ndef enumerate_col(input_df, enumerate_over, new_col=""newCol""):\n    """"""\n    Enumerate unique entries in enumerate_over_col and add new column newCol.\n    """"""\n    new_df = input_df.copy()\n    unique_vals = new_df[enumerate_over].unique()\n    enumerated_dict = {vals: ix for ix, vals in enumerate(unique_vals)}\n    new_df.loc[:, new_col] = new_df[enumerate_over].map(lambda x: enumerated_dict[x])\n    return new_df\n\n\ndef concat_df_list(list_csv, comment=\'#\', header=\'infer\', sep=\',\', nrows=None):\n    """"""Concatenates list of dataframes first created by list comprehension.\n\n    """"""\n    return pd.concat([pd.read_csv(el, comment=comment, header=header, sep=sep, nrows=nrows)\n                      for el in list_csv], ignore_index=True)\n\n\ndef merge_dfs(dataframe_list, join_meth=\'outer\', on=None, left_index=False, right_index=False):\n    """"""\n    Merge together a list of DataFrames.\n    """"""\n\n    def simple_merge(df1, df2):\n        return pd.merge(left=df1, right=df2, on=on,\n                        left_index=left_index, right_index=right_index, how=join_meth)\n\n    merged_frame = reduce(simple_merge, dataframe_list)\n    return merged_frame\n\n\ndef uniq_x_y(X, y, output_index=False):\n    """"""\n    Given a X,y pair of numpy arrays (X:2D, y:1D),\n    return unique rows (arrays) of X, with corresponding y values.\n    """"""\n    uniq_df = pd.DataFrame(X).drop_duplicates()\n    X_uniq = uniq_df.values\n    y_uniq = y[uniq_df.index]\n    if output_index:\n        return X_uniq, y_uniq, uniq_df.index\n    return X_uniq, y_uniq\n\n\ndef gby_count(df, col, col_full_name=False):\n    """"""\n    Perform groupby count aggregation, rename column to count,\n    and keep flat structure of DataFrame.\n    Wrapper on pd.DataFrame.groupby\n    """"""\n    if col_full_name:\n        col_name = ""count_"" + col\n    else:\n        col_name = ""count""\n    return df.groupby(col).size().reset_index(name=col_name).sort_values(by=col_name, ascending=False)\n\n\ndef distribute_rows(df, key_col):\n    """"""\n    Distributes rows according to the series found in key_col.\n    Example:\n        >> df = pd.DataFrame([(el+3, el) for el in range(4)], columns=[\'x\', \'n\'])\n        >> df.T\n           0  1  2  3\n        x  3  4  5  6\n        n  0  1  2  3\n        >> df.loc[np.repeat(df.index.values,df.n)].T\n           1  2  2  3  3  3\n        x  4  5  5  6  6  6\n        n  1  2  2  3  3  3\n    """"""\n    distribution_index = np.repeat(df.index.values, df[key_col])\n    return df.loc[distribution_index]\n\n\ndef resample_by_col(df, gby_col=\'Cluster ID\', scale_by=\'identity\'):\n    """"""\n    Resample the rows of DataFrame.\n    Groupby is first performed on gby_col, then counts are rescaled by\n    function defined in scale_by (identity, log, sqrt).\n    Groupby DataFrame is then resampled by new distribution, and an outer\n    join is performed with original.drop_duplicates(gby_col).\n\n    Note: inputting `df` and `df.drop_duplicates(gby_col)` lead to different\n    results in `gby_col_count` col. Always input original.\n\n    Improvements: create all_n func and do pass function curried like in scala\n    """"""\n    gby_df = df.groupby(gby_col).size().reset_index(name=gby_col + ""_count"")\n\n    def identity(x): return x\n\n    def all_one(x): return 1\n\n    def all_two(x): return 2\n\n    def all_three(x): return 3\n\n    scale_funcs = {\'log\': np.log, \'sqrt\': np.sqrt, \'identity\': identity,\n                   \'all_one\': all_one, \'all_two\': all_two, \'all_three\': all_three}\n    distribution = gby_df[gby_col + ""_count""].map(scale_funcs[scale_by]).map(np.ceil).map(np.int)\n    df_new_dist = gby_df.loc[np.repeat(gby_df.index.values, distribution)]\n    return pd.merge(df_new_dist, df.drop_duplicates(gby_col), on=gby_col, how=\'outer\')\n\n\ndef convert_all_elements_tuple(input_df):\n    """"""\n    If all elements are lists or numpy arrays, these are not hashable.\n    So we can convert to tuples, in order to be able to perform groupbys.\n    """"""\n    return input_df.applymap(tuple)\n\n\ndef explode_rows_vertically(df):\n    """"""\n    For use on DataFrames containing lists in each field.\n    Note: per row, lists in each column must be equal in length.\n        They can, however, differ in length from row to row.\n    Explode lists in each column of input DataFrame vertically,\n        one row at a time.\n    Return DataFrame of exploded elements.\n    Note we could also use `+= zip(*[df[col][ix] for col in df])`\n    """"""\n    exploded_list = []\n    for ix, row in df.iterrows():\n        exploded_list += zip(*row.values)\n    return pd.DataFrame(exploded_list)\n\n\ndef str_list_to_tuple_str_series(col, regex_pattern=\'[A-Z]\\d+\'):\n    """"""\n    Convert string of lists into tuples of strings,\n    for each row in column.\n    regex_pattern determines string tokens.\n    """"""\n    if not isinstance(col[0], str):\n        print(""error: str expected, instead {} found."".format(type(col[0])))\n        return col\n    else:\n        p = re.compile(regex_pattern)\n        return col.apply(p.findall).apply(tuple)\n\n\ndef df_to_namedtups(df, name=""Pandas""):\n    """"""\n    Given a DataFrame, df, return a list of namedtuples\n    """"""\n    return list(df.itertuples(index=False, name=name))\n\n\ndef gby_uniqlists(in_df, pivot_col=None, col=None) -> pd.DataFrame:\n    """"""\n    Computes aggregates for each unique value in `pivot_col` and returns\n        new DataFrame with list of unique values in col + nunique in col.\n\n    :param in_df: pd.DataFrame; input DataFrame\n    :param pivot_col: str; column to groupby (pivot) off\n    :param col: str; column to aggregate values on.\n    :return: pd.DataFrame; groupby aggregates DataFrame with unique vals in list + length.\n    """"""\n\n    gby_temp = in_df.groupby(pivot_col)[col].apply(set).apply(list).reset_index()\n    return gby_temp.assign(len_=gby_temp[col].apply(len)).rename(\n        columns={\'len_\': col + \'_uniq_len\', col: col + \'_uniq\'})\n\n\ndef gby_multiframe(in_df, pivot_col=None, cols=None) -> pd.DataFrame:\n    """"""\n    wrapper of velpy.helper.pandas_.gby_uniqlists performed over list of cols,\n        then joined on pivot_col.\n    :param in_df: pd.DataFrame; input DataFrame\n    :param pivot_col: str; column to groupby (pivot) off\n    :param cols: list[str]; list of columns to aggregate values on.\n    :return: pd.DataFrame; merged groupby aggregates DataFrame\n    """"""\n    cols = [col for col in cols if col in in_df.columns]\n    return merge_dfs([gby_uniqlists(in_df, pivot_col=pivot_col, col=col)\n                      for col in cols], on=pivot_col)\n\n\ndef nonuniq(orig_df):\n    """"""\n    return DataFrame of sorted non-uniq counts of fields (greater than 1).\n    """"""\n    return (orig_df.apply(pd.Series.nunique)\n            .sort_values(ascending=False)\n            .reset_index(name=\'uniq_count\')\n            .pipe(lambda x: x[x[\'uniq_count\'] > 1])\n            )\n\n\ndef update_sub(orig_df, pivot_col=None, pivot_val=None, new_col=None, new_val=None):\n    """"""\n    update a subset of DataFrame based on fixed boolean matching.\n    """"""\n    out_df = orig_df.copy()\n    out_df.loc[out_df[pivot_col] == pivot_val, new_col] = new_val\n    return out_df\n\n\ndef sorted_cols(orig_df):\n    """"""\n    sort columns by name and return df\n    """"""\n    cols = sorted(orig_df.columns)\n    return orig_df[cols]\n\n\ndef ddupe_with_precision(orig_df, precision=10):\n    """"""\n    round original DataFrame to a given precision, drop_duplicates,\n        return original dataframe with those rows indexed.\n    """"""\n    return orig_df.loc[orig_df.round(precision).drop_duplicates().index]\n\n\ndef mapping_2cols(orig_df, colx, coly):\n    """"""\n    return dict mapping vals in colx to coly. Order is important.\n    """"""\n    return dict(orig_df[[colx, coly]].drop_duplicates().values)\n\n\ndef newcol_mapped(orig_df, orig_col, new_col, mapping):\n    """"""\n    map a new column from an old column and a mapping dictionary\n    """"""\n    new_vals = orig_df[orig_col].map(mapping).values\n    return orig_df.assign(**{new_col: new_vals})\n\n\ndef prepend_comment(orig_df):\n    """"""\n    prepend the first column name with a \'#\'\n    """"""\n    input_df = orig_df.copy()\n    first_col = input_df.columns[0]\n    return input_df.rename(columns={first_col: \'#\' + first_col})\n\n\ndef remove_prepended_comment(orig_df):\n    """"""\n    remove the \'#\' prepend to the first column name\n    Note: if there is no comment, this won\'t do anything (idempotency).\n    """"""\n    input_df = orig_df.copy()\n    first_col = input_df.columns[0]\n    return input_df.rename(columns={first_col: first_col.replace(\'#\', """")})\n\n\ndef remove_double_quotes(orig_df: pd.DataFrame, quote_cols, all_cols=False) -> pd.DataFrame:\n    """"""\n    Replace double quotes found in fields with two single quotes.\n     This must be done for SQL queries to correctly parse fields in Hive (and others)\n    """"""\n\n    def replace_quotes(x):\n        if isinstance(x, str):\n            return x.replace(""\\"""", ""\'\'"")\n        return x\n\n    out_df = orig_df.copy()\n    if all_cols:\n        return out_df.applymap(replace_quotes)\n    else:\n        out_df[quote_cols] = out_df[quote_cols].applymap(replace_quotes)\n        return out_df\n\n\ndef apply_uniq(df, orig_col, new_col, _func):\n    """"""\n    Apply func to only unique entries and join with original to fill.\n    Answered for:\n    https://stackoverflow.com/questions/46798532/how-do-you-effectively-use-pd-dataframe-apply-on-rows-with-duplicate-values/\n\n    """"""\n    out_df = df.copy()\n    if new_col in out_df:\n        out_df = out_df.drop(new_col, axis=\'columns\')\n    return out_df.merge(out_df[[orig_col]]\n                        .drop_duplicates()\n                        .assign(**{new_col: lambda x: x[orig_col].apply(_func)}\n                                ), how=\'inner\', on=orig_col)\n\n\ndef mult_types(df):\n    """"""\n    Return series of columns with unique number of types per col\n\n    Can be useful to see which cols have more than 1 type, for example:\n    >> counts = mult_types(df)\n    >> counts[counts != 1]\n    """"""\n    return df.applymap(type).drop_duplicates().apply(pd.Series.nunique)\n\n\ndef collapse_hierarchy(orig_df: pd.DataFrame):\n    """"""\n    this collapses multilevel pandas columns into a single level by joining tokens with underscore.\n    Example:\n        orig_cols = [(\'a\', \'max\'), (\'a\', \'size\'), (\'b\', \'\')]\n         new_cols = [\'a_max\', \'a_size\', \'b\']\n    """"""\n    gby = orig_df.copy()\n    if isinstance(gby.columns, pd.core.indexes.multi.MultiIndex):\n        gby.columns = [\'_\'.join(col).rstrip(\'_\') for col in gby.columns.values]\n    else:\n        print(""columns not MultiIndex, returning original."")\n    return gby\n\n'"
seipy/pcap/__init__.py,0,b''
seipy/pcap/base.py,0,"b'import scapy\n\n\n# import scapy_ssl_tls as scl # python 2\n\n\ndef read_pcap(pcap_file):\n    """"""\n    light wrapper over scapy\'s pcap reader.\n    """"""\n    return scapy.utils.rdpcap(pcap_file)\n'"
seipy/plots_/__init__.py,0,b''
seipy/plots_/base.py,0,"b'from matplotlib import pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom ..ml.linear_algebra import distmat\n\n\ndef scatter_2d(orig_df: pd.DataFrame, colx, coly, label_col,\n               xmin=None, xmax=None, ymin=None, ymax=None):\n    """"""\n    Return scatter plot of 2 columns in a DataFrame, taking labels as colours.\n    """"""\n    plt.scatter(orig_df[colx], orig_df[coly],\n                c=orig_df[label_col].values, cmap=\'viridis\')\n    plt.colorbar()\n    plt.xlim(xmin, xmax)\n    plt.ylim(ymin, ymax)\n    plt.show()\n\n\ndef visualise_dist(fframe=None, metric=\'euclidean\'):\n    """"""\n    Plot a distance matrix from a DataFrame containing only feature columns.\n        The plot is a heatmap, and the distance metric is specified with `metric`\n    """"""\n\n    plt.figure(figsize=(14, 10))\n#     ax = plt.gca()\n    sns.heatmap(distmat(fframe, metric=metric))\n    plt.show()'"
seipy/spark_/__init__.py,0,b'from .base import *'
seipy/spark_/base.py,0,"b'from ..s3_ import get_creds\n\n\ndef s3spark_init(cred_fpath=""~/.aws/credentials"", api_path=None, use_token=False):\n    """"""\n    initialise SparkSession for use with Jupyter and s3 SQL queries\n    Includes support for s3a\n    Returns spark session\n\n    aws credentials file default path: ""~/.aws/credentials""\n    """"""\n    import os\n    os.environ[\'PYSPARK_SUBMIT_ARGS\'] = \\\n        \'--packages com.amazonaws:aws-java-sdk:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0 pyspark-shell\'\n\n    from pyspark.sql import SparkSession\n\n    spark = SparkSession \\\n        .builder \\\n        .appName(""using_s3"") \\\n        .getOrCreate()\n\n    hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\n    myAccessKey, mySecretKey, myToken = get_creds(cred_fpath=cred_fpath, api_path=api_path)\n\n    hadoopConf.set(""fs.s3.impl"", ""org.apache.hadoop.fs.s3native.NativeS3FileSystem"")\n    hadoopConf.set(""fs.s3.awsAccessKeyId"", myAccessKey)\n    hadoopConf.set(""fs.s3.awsSecretAccessKey"", mySecretKey)\n    hadoopConf.set(""fs.s3a.access.key"", myAccessKey)\n    hadoopConf.set(""fs.s3a.secret.key"", mySecretKey)\n    hadoopConf.set(""fs.s3a.awsAccessKeyId"", myAccessKey)\n    hadoopConf.set(""fs.s3a.awsSecretAccessKey"", mySecretKey)\n    if use_token:\n        hadoopConf.set(""fs.s3a.session.token"", myToken)\n        hadoopConf.set(""fs.s3.awsSessionToken"", myToken)\n        hadoopConf.set(""fs.s3a.awsSessionToken"", myToken)\n    return spark\n\n\ndef register_sql(spark, files, schema=None, sep=None, table_name=""table"", return_count=False):\n    """"""\n    Register a list of files as a SQL temporary view.\n\n    parameters:\n    - files is overloaded: can be one file path or list of file paths.\n    - spark: pyspark.sql.SparkSession\n    - table_name: this is how we will refer to table in SQL query\n    Schema of files must be the same for the table\n\n    Example usage after registering table:\n    >> DF = spark.sql(""SELECT * FROM table"")\n    """"""\n    tempFiles = spark.read.csv(files, schema=schema, sep=sep, header=True)\n    tempFiles.createOrReplaceTempView(table_name)\n    if return_count:\n        return spark.sql(""SELECT * FROM {}"".format(table_name)).count()\n'"
