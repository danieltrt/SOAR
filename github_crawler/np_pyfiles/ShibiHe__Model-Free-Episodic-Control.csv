file_path,api_count,code
dqn_ep/EC_functions.py,23,"b'__author__ = \'frankhe\'\nimport numpy as np\nimport cPickle\nimport heapq\n""""""\ndue to the KNN_runtime_test.py  we use BallTree\n""""""\nfrom sklearn.neighbors import BallTree\nimport image_preprocessing as ip\nimport logging\nfrom collections import OrderedDict\n\n\nclass Buffer(object):\n    def __init__(self, size, state_dimension, frequency):\n        self.size = size\n        self.state_dimension = state_dimension\n        self.lru = np.zeros(size, np.float32)\n        self.state = np.zeros((size, state_dimension), np.float32)\n        self.q_return = np.zeros(size, np.float32)\n        self.items = 0\n        self.tree = None\n        self.last_tree_built_time = None\n        self.insert_times = 0\n        self.update_frequency = frequency\n        self.mini_frequency = 1000\n\n    def update_tree(self, time):\n        print \'rebuild tree\'\n        self.tree = BallTree(self.state[:self.items, :], leaf_size=self.size)\n        self.last_tree_built_time = time\n        print \'rebuild done\'\n\n\nclass DistanceNode(object):\n    def __init__(self, dist, index):\n        self.distance = dist\n        self.index = index\n\n\nclass QECTable(object):\n    def __init__(self, knn, state_dimension, projection_type, observation_dimension, buffer_size, num_actions, rng,\n                 rebuild_frequency):\n        self.knn = knn\n        self.time = 0.0  # time stamp for LRU\n        self.ec_buffer = []\n        self.buffer_maximum_size = buffer_size\n        self.rng = rng\n        for i in range(num_actions):\n            self.ec_buffer.append(Buffer(buffer_size, state_dimension, rebuild_frequency))\n\n        # projection\n        """"""\n        I tried to make a function self.projection(state)\n        but cPickle can not pickle an object that has an function attribute\n        """"""\n        self._initialize_projection_function(state_dimension, observation_dimension, projection_type)\n\n    def _initialize_projection_function(self, dimension_result, dimension_observation, p_type):\n        if p_type == \'random\':\n            self.matrix_projection = self.rng.normal(loc=0.0, scale=1.0/np.sqrt(dimension_result),\n                                                     size=(dimension_result, dimension_observation)).astype(np.float32)\n        elif p_type == \'VAE\':\n            pass\n        else:\n            raise ValueError(\'unrecognized projection type\')\n\n    """"""estimate the value of Q_EC(s,a)  O(N*logK*D)  check existence: O(N) -> KNN: O(D*N*logK)""""""\n    def estimate(self, s, a, verbose=False):\n        if type(a) == np.ndarray:\n            a = a[0]\n        state = np.dot(self.matrix_projection, s.flatten())\n        self.time += 0.001\n\n        buffer_a = self.ec_buffer[a]\n\n        knn_distance_list = []\n        knn_return_list = []\n\n        #  first check if we already have this state\n\n        if buffer_a.tree is None:\n            # there is no knn tree now\n            for i in xrange(buffer_a.items):\n                if self._similar_state(buffer_a.state[i], state):\n                    buffer_a.lru[i] = self.time\n                    if verbose:\n                        return buffer_a.q_return[i], [0]*self.knn, [0]*self.knn\n                    return buffer_a.q_return[i]\n        else:\n            nearest_item = buffer_a.tree.query(state.reshape((1, -1)), return_distance=False)[0, 0]\n            if self._similar_state(buffer_a.state[nearest_item], state):\n                buffer_a.lru[nearest_item] = self.time\n                if verbose:\n                    return buffer_a.q_return[nearest_item], [0]*self.knn, [0]*self.knn\n                return buffer_a.q_return[nearest_item]\n\n        #  second KNN to evaluate the novel state\n        threshold = 300.0\n        if buffer_a.tree is None:\n            # there is no approximate tree now\n            distance_list = []\n            for i in xrange(buffer_a.items):\n                distance_list.append(DistanceNode(self._calc_distance(state, buffer_a.state[i]), i))\n\n            smallest = heapq.nsmallest(self.knn, distance_list, key=lambda x: x.distance)\n            value = 0.0\n            for d_node in smallest:\n                if d_node.distance > threshold:\n                    break\n                index = d_node.index\n                value += buffer_a.q_return[index]\n                buffer_a.lru[index] = self.time\n                knn_distance_list.append(d_node.distance)\n                knn_return_list.append(buffer_a.q_return[index])\n            if verbose:\n                return value / (0.0001 + len(knn_distance_list)), knn_distance_list, knn_return_list\n            return value / (0.0001 + len(knn_distance_list))\n        else:\n            dist, smallest = buffer_a.tree.query(state.reshape((1, -1)), k=self.knn, return_distance=True)\n            dist = dist[0]\n            smallest = smallest[0]\n            value = 0.0\n            for pos in range(self.knn):\n                if dist[pos] > threshold:\n                    break\n                i = smallest[pos]\n                value += buffer_a.q_return[i]\n                #  if this node does not change after last annoy\n                if buffer_a.lru[i] <= buffer_a.last_tree_built_time:\n                    buffer_a.lru[i] = self.time\n                knn_distance_list.append(dist[pos])\n                knn_return_list.append(buffer_a.q_return[i])\n            if verbose:\n                return value / (0.0001 + len(knn_distance_list)), knn_distance_list, knn_return_list\n            return value / (0.0001 + len(knn_distance_list))\n\n    @staticmethod\n    def _calc_distance(a, b):\n        return np.sum(np.absolute(a-b)**2)\n\n    @staticmethod\n    def _similar_state(a, b, threshold=200.0):\n        dist = QECTable._calc_distance(a, b)\n        if dist < threshold:\n            return True\n        else:\n            return False\n\n    """"""update Q_EC(s,a)""""""\n    def update(self, s, a, r):  # s is 84*84;  a is 0 to num_actions; r is reward\n        if type(a) == np.ndarray:\n            a = a[0]\n        state = np.dot(self.matrix_projection, s.flatten())\n        self.time += 0.001\n\n        buffer_a = self.ec_buffer[a]\n\n        #  first check if we already have this state\n\n        if buffer_a.tree is None:\n            # there is no approximate tree now\n            for i in xrange(buffer_a.items):\n                if self._similar_state(buffer_a.state[i], state):\n                    buffer_a.q_return[i] = np.maximum(buffer_a.q_return[i], r)\n                    buffer_a.lru[i] = self.time\n                    return\n        else:\n            nearest_item = buffer_a.tree.query(state.reshape((1, -1)), return_distance=False)[0, 0]\n            if self._similar_state(buffer_a.state[nearest_item], state):\n                buffer_a.q_return[nearest_item] = np.maximum(buffer_a.q_return[nearest_item], r)\n                buffer_a.lru[nearest_item] = self.time\n                return\n\n        # second insert a new node\n        if buffer_a.items < self.buffer_maximum_size:\n            i = buffer_a.items\n            buffer_a.items += 1\n        else:\n            i = np.argmin(buffer_a.lru)\n        # logging.info(\'insert at {}\'.format(buffer_a.insert_times))\n        buffer_a.insert_times += 1\n        buffer_a.state[i] = state\n        buffer_a.q_return[i] = r\n        buffer_a.lru[i] = self.time\n\n        if buffer_a.tree is None:\n            if buffer_a.insert_times == np.minimum(buffer_a.mini_frequency, self.buffer_maximum_size):\n                buffer_a.update_tree(self.time)\n                buffer_a.insert_times = 0\n        else:\n            if buffer_a.insert_times % buffer_a.mini_frequency == 0:\n                buffer_a.update_tree(self.time)\n                buffer_a.mini_frequency = np.minimum(buffer_a.mini_frequency+1, buffer_a.update_frequency)\n\n\nclass TraceNode(object):\n    def __init__(self, observation, action, reward, terminal):\n        self.image = observation\n        self.action = action\n        self.reward = reward\n        self.terminal = terminal\n\n\nclass TraceRecorder(object):\n    def __init__(self):\n        self.trace_list = []\n\n    def add_trace(self, observation, action, reward, terminal):\n        node = TraceNode(observation, action, reward, terminal)\n        self.trace_list.append(node)\n\n\ndef print_table(table):\n    a = 0\n    for action_buffer in table.ec_buffer:\n        print \'action buffer of \', a, \'length=\', action_buffer.items\n        a += 1\n        for i in range(action_buffer.items):\n            print \'(\', action_buffer.lru[i], action_buffer.q_return[i], \')\',\n        print\n    print\n\n\nclass LshHash(object):\n    def __init__(self, out_dimension, in_dimension, size, rng):\n        self.d = OrderedDict()\n        self.size = size\n        self.projection = rng.randn(out_dimension, in_dimension)\n\n    def get_hash(self, s, a):\n        code = np.dot(self.projection, s.flatten())\n        code = \'\'.join([\'1\' if x > 0 else \'0\' for x in code])\n        code += bin(a)[2:]\n        code = hash(code)\n        return code\n\n    def update(self, s, a, r):\n        if type(a) == np.ndarray:\n            a = a[0]\n        key = self.get_hash(s, a)\n        if key in self.d:\n            value = self.d[key]\n            self.d.pop(key)\n            self.d[key] = np.maximum(value, r)\n        else:\n            self.d[key] = r\n            if len(self.d) > self.size:\n                self.d.popitem(False)\n\n    def evaluate(self, s, a):\n        if type(a) == np.ndarray:\n            a = a[0]\n        key = self.get_hash(s, a)\n        return self.d.get(key, 0.0)\n\nif __name__ == \'__main__\':\n    images = cPickle.load(open(\'game_images\', mode=\'rb\'))\n    images = [x.astype(np.float32)/255.0 for x in images]\n    # from images2gif import writeGif\n    # writeGif(\'see.gif\', images)\n\n    table = QECTable(2, 64, \'random\', images[0].size, 10, 2, np.random.RandomState(12345), 10, 1, 10)\n    # distance = np.zeros((100, 100), np.float32)\n    # for i in range(100):\n    #     for j in range(i+1, 100):\n    #         distance[i, j] = distance[j, i] = QECTable._calc_distance(images[i], images[j])\n    # np.set_printoptions(threshold=np.inf)\n    # print distance\n    #\n    # raw_input()\n    table.update(images[0], 0, 1)\n    table.update(images[1], 0, 2)\n    table.update(images[2], 0, 3)\n    table.update(images[3], 0, 4)\n    table.update(images[4], 0, 5)\n    table.update(images[5], 0, 6)\n    table.update(images[6], 0, 7)\n    table.update(images[33], 0, 8)\n    table.update(images[34], 0, 9)\n    table.update(images[35], 0, 10)\n    table.update(images[36], 0, 11)\n    print_table(table)\n    table.update(images[37], 0, 12)\n    table.update(images[38], 0, 13)\n    print_table(table)\n\n\n'"
dqn_ep/ale_agents.py,44,"b'__author__ = \'frankhe\'\n""""""\nepisodic control and DQN agents\n""""""\nimport time\nimport os\nimport logging\nimport numpy as np\nimport cPickle\nimport EC_functions\nfrom images2gif import writeGif\n\nimport ale_data_set\nimport sys\nsys.setrecursionlimit(10000)\n\n\nclass EpisodicControl(object):\n    def __init__(self, qec_table, ec_discount, num_actions, epsilon_start,\n                 epsilon_min, epsilon_decay, exp_pref, ec_testing, rng):\n        self.qec_table = qec_table\n        self.ec_discount = ec_discount\n        self.num_actions = num_actions\n        self.epsilon_start = epsilon_start\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.exp_pref = exp_pref\n        self.rng = rng\n\n        self.trace_list = EC_functions.TraceRecorder()\n\n        self.epsilon = self.epsilon_start\n        if self.epsilon_decay != 0:\n            self.epsilon_rate = ((self.epsilon_start - self.epsilon_min) /\n                                 self.epsilon_decay)\n        else:\n            self.epsilon_rate = 0\n\n        # CREATE A FOLDER TO HOLD RESULTS\n        time_str = time.strftime(""_%m-%d-%H-%M_"", time.gmtime())\n        self.exp_dir = self.exp_pref + time_str + \\\n                       ""{}"".format(self.ec_discount).replace(""."", ""p"")\n\n        try:\n            os.stat(self.exp_dir)\n        except OSError:\n            os.makedirs(self.exp_dir)\n\n        self._open_results_file()\n\n        self.step_counter = None\n        self.episode_reward = None\n\n        self.total_reward = 0.\n        self.total_episodes = 0\n\n        self.start_time = None\n\n        self.last_img = None\n        self.last_action = None\n\n        self.steps_sec_ema = 0.\n\n        self.play_images = []\n        self.testing = ec_testing\n\n        self.program_start_time = None\n        self.last_count_time = None\n\n    def time_count_start(self):\n        self.last_count_time = self.program_start_time = time.time()\n\n    def _open_results_file(self):\n        logging.info(""OPENING "" + self.exp_dir + \'/results.csv\')\n        self.results_file = open(self.exp_dir + \'/results.csv\', \'w\', 0)\n        self.results_file.write(\'epoch, episode_nums, total_reward, avg_reward, epoch time, total time\\n\')\n        self.results_file.flush()\n\n    def _update_results_file(self, epoch, total_episodes, total_reward):\n        this_time = time.time()\n        total_time = this_time-self.program_start_time\n        epoch_time = this_time-self.last_count_time\n        out = ""{},{},{},{},{},{}\\n"".format(epoch, total_episodes, total_reward, total_reward/total_episodes,\n                                           epoch_time, total_time)\n        self.last_count_time = this_time\n        self.results_file.write(out)\n        self.results_file.flush()\n\n    def start_episode(self, observation):\n        """"""\n        This method is called once at the beginning of each episode.\n        No reward is provided, because reward is only available after\n        an action has been taken.\n\n        Arguments:\n           observation - height x width numpy array\n\n        Returns:\n           An integer action\n        """"""\n\n        self.step_counter = 0\n        self.episode_reward = 0\n\n        self.trace_list.trace_list = []\n        self.start_time = time.time()\n        return_action = self.rng.randint(0, self.num_actions)\n\n        self.last_action = return_action\n\n        self.last_img = observation\n\n        return return_action\n\n    def _choose_action(self, trace_list, qec_table, epsilon, observation, reward):\n        trace_list.add_trace(self.last_img, self.last_action, reward, False)\n\n        # epsilon greedy\n        if self.rng.rand() < epsilon:\n            return self.rng.randint(0, self.num_actions)\n\n        value = -100\n        maximum_action = 0\n        # argmax(Q(s,a))\n        for action in range(self.num_actions):\n            value_t = qec_table.estimate(observation, action)\n            if value_t > value:\n                value = value_t\n                maximum_action = action\n\n        return maximum_action\n\n    def step(self, reward, observation):\n        """"""\n        This method is called each time step.\n\n        Arguments:\n           reward      - Real valued reward.\n           observation - A height x width numpy array\n\n        Returns:\n           An integer action.\n\n        """"""\n\n        self.step_counter += 1\n        self.episode_reward += reward\n\n        self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_rate)\n\n        action = self._choose_action(self.trace_list, self.qec_table, self.epsilon, observation, np.clip(reward, -1, 1))\n\n        self.last_action = action\n        self.last_img = observation\n\n        return action\n\n    def end_episode(self, reward, terminal=True):\n        """"""\n        This function is called once at the end of an episode.\n\n        Arguments:\n           reward      - Real valued reward.\n           terminal    - Whether the episode ended intrinsically\n                         (ie we didn\'t run out of steps)\n        Returns:\n            None\n        """"""\n\n        self.episode_reward += reward\n        self.total_reward += self.episode_reward\n        self.total_episodes += 1\n        self.step_counter += 1\n        total_time = time.time() - self.start_time\n\n        # Store the latest sample.\n        self.trace_list.add_trace(self.last_img, self.last_action, np.clip(reward, -1, 1), True)\n\n        # calculate time\n        rho = 0.98\n        self.steps_sec_ema *= rho\n        self.steps_sec_ema += (1. - rho) * (self.step_counter/total_time)\n        logging.info(""steps/second: {:.2f}, avg: {:.2f}"".format(\n            self.step_counter/total_time, self.steps_sec_ema))\n        logging.info(\'episode {} reward: {:.2f}\'.format(self.total_episodes, self.episode_reward))\n\n        if self.testing:\n            for node in self.trace_list.trace_list:\n                self.play_images.append(node.image)\n            # skip the update process\n            return\n\n        """"""\n        do update\n        """"""\n        q_return = 0.\n        # last_q_return = -1.0\n        for i in range(len(self.trace_list.trace_list)-1, -1, -1):\n            node = self.trace_list.trace_list[i]\n            q_return = q_return * self.ec_discount + node.reward\n            self.qec_table.update(node.image, node.action, q_return)\n            # if not np.isclose(q_return, last_q_return):\n            #     self.qec_table.update(node.image, node.action, q_return)\n            #     last_q_return = q_return\n\n    def finish_epoch(self, epoch):\n        # so large that i only keep one\n        qec_file = open(self.exp_dir + \'/qec_table_file_\' + \\\n                        \'.pkl\', \'w\')\n        cPickle.dump(self.qec_table, qec_file, 2)\n        qec_file.close()\n\n        self._update_results_file(epoch, self.total_episodes, self.total_reward)\n        self.total_episodes = 0\n        self.total_reward = 0\n\n        # EC_functions.print_table(self.qec_table)\n\n        if self.testing:\n            writeGif(self.exp_dir + \'/played.gif\', self.play_images)\n\n            # handle = open(\'played_images\', \'w+\')\n            # cPickle.dump(self.play_images, handle, 2)\n\n\nclass NeuralAgent(object):\n\n    def __init__(self, q_network, epsilon_start, epsilon_min,\n                 epsilon_decay, replay_memory_size, exp_pref,\n                 replay_start_size, update_frequency, rng):\n\n        self.network = q_network\n        self.epsilon_start = epsilon_start\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.replay_memory_size = replay_memory_size\n        self.exp_pref = exp_pref\n        self.replay_start_size = replay_start_size\n        self.update_frequency = update_frequency\n        self.rng = rng\n\n        self.phi_length = self.network.num_frames\n        self.image_width = self.network.input_width\n        self.image_height = self.network.input_height\n\n        # CREATE A FOLDER TO HOLD RESULTS\n        time_str = time.strftime(""_%m-%d-%H-%M_"", time.gmtime())\n        self.exp_dir = self.exp_pref + time_str + \\\n                       ""{}"".format(self.network.lr).replace(""."", ""p"") + ""_"" \\\n                       + ""{}"".format(self.network.discount).replace(""."", ""p"")\n\n        try:\n            os.stat(self.exp_dir)\n        except OSError:\n            os.makedirs(self.exp_dir)\n\n        self.num_actions = self.network.num_actions\n\n        self.data_set = ale_data_set.DataSet(width=self.image_width,\n                                             height=self.image_height,\n                                             rng=rng,\n                                             max_steps=self.replay_memory_size,\n                                             phi_length=self.phi_length)\n\n        # just needs to be big enough to create phi\'s\n        self.test_data_set = ale_data_set.DataSet(width=self.image_width,\n                                                  height=self.image_height,\n                                                  rng=rng,\n                                                  max_steps=self.phi_length * 2,\n                                                  phi_length=self.phi_length)\n        self.epsilon = self.epsilon_start\n        if self.epsilon_decay != 0:\n            self.epsilon_rate = ((self.epsilon_start - self.epsilon_min) /\n                                 self.epsilon_decay)\n        else:\n            self.epsilon_rate = 0\n\n        self.testing = False\n\n        self._open_results_file()\n        self._open_learning_file()\n\n        self.episode_counter = 0\n        self.batch_counter = 0\n\n        self.holdout_data = None\n\n        # In order to add an element to the data set we need the\n        # previous state and action and the current reward.  These\n        # will be used to store states and actions.\n        self.last_img = None\n        self.last_action = None\n\n        # Exponential moving average of runtime performance.\n        self.steps_sec_ema = 0.\n\n        self.program_start_time = None\n        self.last_count_time = None\n        self.epoch_time = None\n        self.total_time = None\n\n    def time_count_start(self):\n        self.last_count_time = self.program_start_time = time.time()\n\n    def _open_results_file(self):\n        logging.info(""OPENING "" + self.exp_dir + \'/results.csv\')\n        self.results_file = open(self.exp_dir + \'/results.csv\', \'w\', 0)\n        self.results_file.write(\\\n            \'epoch,num_episodes,total_reward,reward_per_epoch,mean_q, epoch time, total time\\n\')\n        self.results_file.flush()\n\n    def _open_learning_file(self):\n        self.learning_file = open(self.exp_dir + \'/learning.csv\', \'w\', 0)\n        self.learning_file.write(\'mean_loss,epsilon\\n\')\n        self.learning_file.flush()\n\n    def _update_results_file(self, epoch, num_episodes, holdout_sum):\n        out = ""{},{},{},{},{},{},{}\\n"".format(epoch, num_episodes, self.total_reward,\n                                        self.total_reward / float(num_episodes),\n                                        holdout_sum, self.epoch_time, self.total_time)\n        self.last_count_time = time.time()\n        self.results_file.write(out)\n        self.results_file.flush()\n\n    def _update_learning_file(self):\n        out = ""{},{}\\n"".format(np.mean(self.loss_averages),\n                               self.epsilon)\n        self.learning_file.write(out)\n        self.learning_file.flush()\n\n    def start_episode(self, observation):\n        """"""\n        This method is called once at the beginning of each episode.\n        No reward is provided, because reward is only available after\n        an action has been taken.\n\n        Arguments:\n           observation - height x width numpy array\n\n        Returns:\n           An integer action\n        """"""\n\n        self.step_counter = 0\n        self.batch_counter = 0\n        self.episode_reward = 0\n\n        # We report the mean loss for every epoch.\n        self.loss_averages = []\n\n        self.start_time = time.time()\n        return_action = self.rng.randint(0, self.num_actions)\n\n        self.last_action = return_action\n\n        self.last_img = observation\n\n        return return_action\n\n    def _show_phis(self, phi1, phi2):\n        import matplotlib.pyplot as plt\n        for p in range(self.phi_length):\n            plt.subplot(2, self.phi_length, p+1)\n            plt.imshow(phi1[p, :, :], interpolation=\'none\', cmap=""gray"")\n            plt.grid(color=\'r\', linestyle=\'-\', linewidth=1)\n        for p in range(self.phi_length):\n            plt.subplot(2, self.phi_length, p+5)\n            plt.imshow(phi2[p, :, :], interpolation=\'none\', cmap=""gray"")\n            plt.grid(color=\'r\', linestyle=\'-\', linewidth=1)\n        plt.show()\n\n    def step(self, reward, observation):\n        """"""\n        This method is called each time step.\n\n        Arguments:\n           reward      - Real valued reward.\n           observation - A height x width numpy array\n\n        Returns:\n           An integer action.\n\n        """"""\n\n        self.step_counter += 1\n\n        # TESTING---------------------------\n        if self.testing:\n            self.episode_reward += reward\n            action = self._choose_action(self.test_data_set, .0,\n                                         observation, np.clip(reward, -1, 1))\n\n        # NOT TESTING---------------------------\n        else:\n\n            if len(self.data_set) > self.replay_start_size:\n                self.epsilon = max(self.epsilon_min,\n                                   self.epsilon - self.epsilon_rate)\n\n                action = self._choose_action(self.data_set, self.epsilon,\n                                             observation,\n                                             np.clip(reward, -1, 1))\n\n                if self.step_counter % self.update_frequency == 0:\n                    loss = self._do_training()\n                    self.batch_counter += 1\n                    self.loss_averages.append(loss)\n\n            else:  # Still gathering initial random data...\n                action = self._choose_action(self.data_set, self.epsilon,\n                                             observation,\n                                             np.clip(reward, -1, 1))\n\n        self.last_action = action\n        self.last_img = observation\n\n        return action\n\n    def _choose_action(self, data_set, epsilon, cur_img, reward):\n        """"""\n        Add the most recent data to the data set and choose\n        an action based on the current policy.\n        """"""\n\n        data_set.add_sample(self.last_img, self.last_action, reward, False)\n        if self.step_counter >= self.phi_length:\n            phi = data_set.phi(cur_img)\n            action = self.network.choose_action(phi, epsilon)\n        else:\n            action = self.rng.randint(0, self.num_actions)\n\n        return action\n\n    def _do_training(self):\n        """"""\n        Returns the average loss for the current batch.\n        May be overridden if a subclass needs to train the network\n        differently.\n        """"""\n        imgs, actions, rewards, terminals = \\\n                                self.data_set.random_batch(\n                                    self.network.batch_size)\n        return self.network.train(imgs, actions, rewards, terminals)\n\n    def end_episode(self, reward, terminal=True):\n        """"""\n        This function is called once at the end of an episode.\n\n        Arguments:\n           reward      - Real valued reward.\n           terminal    - Whether the episode ended intrinsically\n                         (ie we didn\'t run out of steps)\n        Returns:\n            None\n        """"""\n\n        self.episode_reward += reward\n        self.step_counter += 1\n        total_time = time.time() - self.start_time\n\n        if self.testing:\n            # If we run out of time, only count the last episode if\n            # it was the only episode.\n            if terminal or self.episode_counter == 0:\n                self.episode_counter += 1\n                self.total_reward += self.episode_reward\n        else:\n\n            # Store the latest sample.\n            self.data_set.add_sample(self.last_img,\n                                     self.last_action,\n                                     np.clip(reward, -1, 1),\n                                     True)\n\n            rho = 0.98\n            self.steps_sec_ema *= rho\n            self.steps_sec_ema += (1. - rho) * (self.step_counter/total_time)\n\n            logging.info(""steps/second: {:.2f}, avg: {:.2f}"".format(\n                self.step_counter/total_time, self.steps_sec_ema))\n\n            if self.batch_counter > 0:\n                self._update_learning_file()\n                logging.info(""average loss: {:.4f}"".format(\\\n                                np.mean(self.loss_averages)))\n\n    def finish_epoch(self, epoch):\n        net_file = open(self.exp_dir + \'/network_file_\' + str(epoch) + \\\n                        \'.pkl\', \'w\')\n        cPickle.dump(self.network, net_file, -1)\n        net_file.close()\n        this_time = time.time()\n        self.total_time = this_time-self.program_start_time\n        self.epoch_time = this_time-self.last_count_time\n\n    def start_testing(self):\n        self.testing = True\n        self.total_reward = 0\n        self.episode_counter = 0\n\n    def finish_testing(self, epoch):\n        self.testing = False\n        holdout_size = 3200\n\n        if self.holdout_data is None and len(self.data_set) > holdout_size:\n            imgs, _, _, _ = self.data_set.random_batch(holdout_size)\n            self.holdout_data = imgs[:, :self.phi_length]\n\n        holdout_sum = 0\n        if self.holdout_data is not None:\n            for i in range(holdout_size):\n                holdout_sum += np.max(\n                    self.network.q_vals(self.holdout_data[i]))\n\n        self._update_results_file(epoch, self.episode_counter,\n                                  holdout_sum / holdout_size)\n\n\nclass EC_DQN(object):\n    def __init__(self, q_network, qec_table, epsilon_start, epsilon_min,\n                 epsilon_decay, replay_memory_size, exp_pref,\n                 replay_start_size, update_frequency, ec_discount, num_actions, ec_testing, rng):\n        self.network = q_network\n        self.qec_table = qec_table\n        self.ec_testing = ec_testing\n        self.ec_discount = ec_discount\n        self.num_actions = num_actions\n        self.epsilon_start = epsilon_start\n\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.replay_memory_size = replay_memory_size\n        self.exp_pref = exp_pref\n        self.replay_start_size = replay_start_size\n        self.update_frequency = update_frequency\n        self.rng = rng\n\n        self.phi_length = self.network.num_frames\n        self.image_width = self.network.input_width\n        self.image_height = self.network.input_height\n\n        # CREATE A FOLDER TO HOLD RESULTS\n        time_str = time.strftime(""_%m-%d-%H-%M_"", time.gmtime())\n        self.exp_dir = self.exp_pref + time_str + \\\n                       ""{}"".format(self.network.lr).replace(""."", ""p"") + ""_"" \\\n                       + ""{}"".format(self.network.discount).replace(""."", ""p"")\n\n        try:\n            os.stat(self.exp_dir)\n        except OSError:\n            os.makedirs(self.exp_dir)\n\n        self.data_set = ale_data_set.DataSet(width=self.image_width,\n                                             height=self.image_height,\n                                             rng=rng,\n                                             max_steps=self.replay_memory_size,\n                                             phi_length=self.phi_length)\n\n        # just needs to be big enough to create phi\'s\n        self.test_data_set = ale_data_set.DataSet(width=self.image_width,\n                                                  height=self.image_height,\n                                                  rng=rng,\n                                                  max_steps=self.phi_length * 2,\n                                                  phi_length=self.phi_length)\n        self.epsilon = self.epsilon_start\n        if self.epsilon_decay != 0:\n            self.epsilon_rate = ((self.epsilon_start - self.epsilon_min) /\n                                 self.epsilon_decay)\n        else:\n            self.epsilon_rate = 0\n\n        self.testing = False\n\n        self._open_results_file()\n        self._open_learning_file()\n        self._open_recording_file()\n\n        self.step_counter = None\n        self.episode_reward = None\n        self.start_time = None\n        self.loss_averages = None\n        self.total_reward = None\n\n        self.episode_counter = 0\n        self.batch_counter = 0\n\n        self.holdout_data = None\n\n        # In order to add an element to the data set we need the\n        # previous state and action and the current reward.  These\n        # will be used to store states and actions.\n        self.last_img = None\n        self.last_action = None\n\n        # Exponential moving average of runtime performance.\n        self.steps_sec_ema = 0.\n        self.program_start_time = None\n        self.last_count_time = None\n        self.epoch_time = None\n        self.total_time = None\n\n    def time_count_start(self):\n        self.last_count_time = self.program_start_time = time.time()\n\n    def _open_results_file(self):\n        logging.info(""OPENING "" + self.exp_dir + \'/results.csv\')\n        self.results_file = open(self.exp_dir + \'/results.csv\', \'w\', 0)\n        self.results_file.write(\\\n            \'epoch,num_episodes,total_reward,reward_per_epoch,mean_q, epoch time, total time\\n\')\n        self.results_file.flush()\n\n    def _open_learning_file(self):\n        self.learning_file = open(self.exp_dir + \'/learning.csv\', \'w\', 0)\n        self.learning_file.write(\'mean_loss,epsilon\\n\')\n        self.learning_file.flush()\n\n    def _update_results_file(self, epoch, num_episodes, holdout_sum):\n        out = ""{},{},{},{},{},{},{}\\n"".format(epoch, num_episodes, self.total_reward,\n                                        self.total_reward / float(num_episodes),\n                                        holdout_sum, self.epoch_time, self.total_time)\n        self.last_count_time = time.time()\n        self.results_file.write(out)\n        self.results_file.flush()\n\n    def _update_learning_file(self):\n        out = ""{},{}\\n"".format(np.mean(self.loss_averages),\n                               self.epsilon)\n        self.learning_file.write(out)\n        self.learning_file.flush()\n\n    def _open_recording_file(self):\n        self.recording_tot = 0\n        self.recording_file = open(self.exp_dir + \'/recording.csv\', \'w\', 0)\n        self.recording_file.write(\'q-learning,table,replay,\')\n        for i in range(self.qec_table.knn):\n            self.recording_file.write(\'knn_value\'+str(i)+\',\')\n        self.recording_file.write(\'\\n\')\n        self.recording_file.flush()\n\n    def _update_recording_file(self, q_return, table_return, replay_return, knn_return_list, knn_distance_list):\n        if self.recording_tot > 10000000:\n            return\n        self.recording_tot += 1\n        out = ""{},{},{},"".format(q_return, table_return, replay_return)\n        self.recording_file.write(out)\n        for i in range(len(knn_return_list)):\n            self.recording_file.write(str(knn_return_list[i])+\',\')\n        for i in range(len(knn_distance_list)):\n            self.recording_file.write(str(knn_distance_list[i])+\',\')\n        self.recording_file.write(\'\\n\')\n        self.recording_file.flush()\n\n    def start_episode(self, observation):\n        """"""\n        This method is called once at the beginning of each episode.\n        No reward is provided, because reward is only available after\n        an action has been taken.\n\n        Arguments:\n           observation - height x width numpy array\n\n        Returns:\n           An integer action\n        """"""\n\n        self.step_counter = 0\n        self.batch_counter = 0\n        self.episode_reward = 0\n\n        # We report the mean loss for every epoch.\n        self.loss_averages = []\n\n        self.start_time = time.time()\n        return_action = self.rng.randint(0, self.num_actions)\n\n        self.last_action = return_action\n\n        self.last_img = observation\n\n        return return_action\n\n    def _choose_action(self, data_set, epsilon, cur_img, reward):\n        """"""\n        Add the most recent data to the data set and choose\n        an action based on the current policy.\n        """"""\n\n        data_set.add_sample(self.last_img, self.last_action, reward, False)\n        if self.step_counter >= self.phi_length:\n            phi = data_set.phi(cur_img)\n            action = self.network.choose_action(phi, epsilon)\n        else:\n            action = self.rng.randint(0, self.num_actions)\n\n        return action\n\n    def _do_training(self):\n        """"""\n        Returns the average loss for the current batch.\n        May be overridden if a subclass needs to train the network\n        differently.\n        """"""\n        imgs, actions, rewards, terminals, return_value = self.data_set.random_batch(self.network.batch_size, True)\n        evaluation = np.zeros((self.network.batch_size, 1), np.float32)\n        return_table_list = []\n        knn_distance_list_list = []\n        knn_return_list_list = []\n        for i in range(self.network.batch_size):\n            state = imgs[i][self.data_set.phi_length-1]\n            return_table, knn_distance_list, knn_return_list = self.qec_table.estimate(state, actions[i], verbose=True)\n            return_table_list.append(return_table)\n            knn_distance_list_list.append(knn_distance_list)\n            knn_return_list_list.append(knn_return_list)\n            evaluation[i] = np.maximum(return_table, return_value[i])\n            # if len(knn_distance_list) == 0 or return_table < 1.0:\n            #     evaluation[i] = 0.0\n            # else:\n            #     evaluation[i] = return_table\n\n        loss, target = self.network.train(imgs, actions, rewards, terminals, evaluation=evaluation, get_target=True)\n        for i in range(self.network.batch_size):\n            self._update_recording_file(target[i], return_table_list[i],\n                                        return_value[i], knn_return_list_list[i], knn_distance_list_list[i])\n        return loss\n\n    def step(self, reward, observation):\n        """"""\n        This method is called each time step.\n\n        Arguments:\n           reward      - Real valued reward.\n           observation - A height x width numpy array\n\n        Returns:\n           An integer action.\n\n        """"""\n\n        self.step_counter += 1\n        self.episode_reward += reward\n\n        # TESTING---------------------------\n        if self.testing:\n            action = self._choose_action(self.test_data_set, 0.0,\n                                         observation, np.clip(reward, -1, 1))\n\n        # NOT TESTING---------------------------\n        else:\n            if len(self.data_set) > self.replay_start_size:\n                self.epsilon = max(self.epsilon_min,\n                                   self.epsilon - self.epsilon_rate)\n\n                action = self._choose_action(self.data_set, self.epsilon,\n                                             observation,\n                                             np.clip(reward, -1, 1))\n\n                if self.step_counter % self.update_frequency == 0:\n                    loss = self._do_training()\n                    self.batch_counter += 1\n                    self.loss_averages.append(loss)\n\n            else:  # Still gathering initial random data...\n                action = self._choose_action(self.data_set, self.epsilon,\n                                             observation,\n                                             np.clip(reward, -1, 1))\n\n        self.last_action = action\n        self.last_img = observation\n\n        return action\n\n    def end_episode(self, reward, terminal=True):\n        """"""\n        This function is called once at the end of an episode.\n\n        Arguments:\n           reward      - Real valued reward.\n           terminal    - Whether the episode ended intrinsically\n                         (ie we didn\'t run out of steps)\n        Returns:\n            None\n        """"""\n\n        self.episode_reward += reward\n        self.step_counter += 1\n        total_time = time.time() - self.start_time\n\n        if self.testing:\n            # If we run out of time, only count the last episode if\n            # it was the only episode.\n            if terminal or self.episode_counter == 0:\n                self.episode_counter += 1\n                self.total_reward += self.episode_reward\n        else:\n            # Store the latest sample.\n            self.data_set.add_sample(self.last_img,\n                                     self.last_action,\n                                     np.clip(reward, -1, 1),\n                                     True)\n            """"""update""""""\n            q_return = 0.\n            # last_q_return = -1.0\n            index = (self.data_set.top-1) % self.data_set.max_steps\n            while True:\n                q_return = q_return * self.network.discount + self.data_set.rewards[index]\n                # if not np.isclose(q_return, last_q_return):\n                #     self.qec_table.update(self.data_set.imgs[index], self.data_set.actions[index], q_return)\n                #     last_q_return = q_return\n                self.qec_table.update(self.data_set.imgs[index], self.data_set.actions[index], q_return)\n                self.data_set.return_value[index] = q_return\n                index = (index-1) % self.data_set.max_steps\n                if self.data_set.terminal[index] or index == self.data_set.bottom:\n                    break\n\n            rho = 0.98\n            self.steps_sec_ema *= rho\n            self.steps_sec_ema += (1. - rho) * (self.step_counter/total_time)\n\n            logging.info(""steps/second: {:.2f}, avg: {:.2f}"".format(\n                self.step_counter/total_time, self.steps_sec_ema))\n\n            if self.batch_counter > 0:\n                self._update_learning_file()\n                logging.info(""average loss: {:.4f}"".format(\\\n                                np.mean(self.loss_averages)))\n\n    def finish_epoch(self, epoch):\n        # so large that i only keep one\n        qec_file = open(self.exp_dir + \'/qec_table_file_\' + \\\n                        \'.pkl\', \'w\')\n        cPickle.dump(self.qec_table, qec_file, 2)\n        qec_file.close()\n\n        net_file = open(self.exp_dir + \'/network_file_\' + str(epoch) + \\\n                        \'.pkl\', \'w\')\n        cPickle.dump(self.network, net_file, -1)\n        net_file.close()\n        this_time = time.time()\n        self.total_time = this_time-self.program_start_time\n        self.epoch_time = this_time-self.last_count_time\n\n    def start_testing(self):\n        self.testing = True\n        self.total_reward = 0\n        self.episode_counter = 0\n\n    def finish_testing(self, epoch):\n        self.testing = False\n        holdout_size = 3200\n\n        if self.holdout_data is None and len(self.data_set) > holdout_size:\n            imgs, _, _, _ = self.data_set.random_batch(holdout_size)\n            self.holdout_data = imgs[:, :self.phi_length]\n\n        holdout_sum = 0\n        if self.holdout_data is not None:\n            for i in range(holdout_size):\n                holdout_sum += np.max(\n                    self.network.q_vals(self.holdout_data[i]))\n\n        self._update_results_file(epoch, self.episode_counter,\n                                  holdout_sum / holdout_size)\n\n\nclass NeuralNetworkEpisodicMemory1(object):\n    def __init__(self, q_network, qec_table, epsilon_start, epsilon_min,\n                 epsilon_decay, replay_memory_size, exp_pref,\n                 replay_start_size, update_frequency, ec_discount, num_actions, ec_testing, rng):\n        self.network = q_network\n        self.qec_table = qec_table\n        self.ec_testing = ec_testing\n        self.ec_discount = ec_discount\n        self.num_actions = num_actions\n        self.epsilon_start = epsilon_start\n\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.replay_memory_size = replay_memory_size\n        self.exp_pref = exp_pref\n        self.replay_start_size = replay_start_size\n        self.update_frequency = update_frequency\n        self.rng = rng\n\n        self.phi_length = self.network.num_frames\n        self.image_width = self.network.input_width\n        self.image_height = self.network.input_height\n\n        # CREATE A FOLDER TO HOLD RESULTS\n        time_str = time.strftime(""_%m-%d-%H-%M_"", time.gmtime())\n        self.exp_dir = self.exp_pref + time_str + \\\n                       ""{}"".format(self.network.lr).replace(""."", ""p"") + ""_"" \\\n                       + ""{}"".format(self.network.discount).replace(""."", ""p"")\n\n        try:\n            os.stat(self.exp_dir)\n        except OSError:\n            os.makedirs(self.exp_dir)\n\n        self.data_set = ale_data_set.DataSet(width=self.image_width,\n                                             height=self.image_height,\n                                             rng=rng,\n                                             max_steps=self.replay_memory_size,\n                                             phi_length=self.phi_length)\n\n        # just needs to be big enough to create phi\'s\n        self.test_data_set = ale_data_set.DataSet(width=self.image_width,\n                                                  height=self.image_height,\n                                                  rng=rng,\n                                                  max_steps=self.phi_length * 2,\n                                                  phi_length=self.phi_length)\n        self.epsilon = self.epsilon_start\n        if self.epsilon_decay != 0:\n            self.epsilon_rate = ((self.epsilon_start - self.epsilon_min) /\n                                 self.epsilon_decay)\n        else:\n            self.epsilon_rate = 0\n\n        self.testing = False\n\n        self._open_results_file()\n        self._open_learning_file()\n\n        self.step_counter = None\n        self.episode_reward = None\n        self.start_time = None\n        self.loss_averages = None\n        self.total_reward = None\n\n        self.episode_counter = 0\n        self.batch_counter = 0\n\n        self.holdout_data = None\n\n        # In order to add an element to the data set we need the\n        # previous state and action and the current reward.  These\n        # will be used to store states and actions.\n        self.last_img = None\n        self.last_action = None\n\n        # Exponential moving average of runtime performance.\n        self.steps_sec_ema = 0.\n        self.program_start_time = None\n        self.last_count_time = None\n        self.epoch_time = None\n        self.total_time = None\n\n    def time_count_start(self):\n        self.last_count_time = self.program_start_time = time.time()\n\n    def _open_results_file(self):\n        logging.info(""OPENING "" + self.exp_dir + \'/results.csv\')\n        self.results_file = open(self.exp_dir + \'/results.csv\', \'w\', 0)\n        self.results_file.write(\\\n            \'epoch,num_episodes,total_reward,reward_per_epoch,mean_q, epoch time, total time\\n\')\n        self.results_file.flush()\n\n    def _open_learning_file(self):\n        self.learning_file = open(self.exp_dir + \'/learning.csv\', \'w\', 0)\n        self.learning_file.write(\'mean_loss,epsilon\\n\')\n        self.learning_file.flush()\n\n    def _update_results_file(self, epoch, num_episodes, holdout_sum):\n        out = ""{},{},{},{},{},{},{}\\n"".format(epoch, num_episodes, self.total_reward,\n                                        self.total_reward / float(num_episodes),\n                                        holdout_sum, self.epoch_time, self.total_time)\n        self.last_count_time = time.time()\n        self.results_file.write(out)\n        self.results_file.flush()\n\n    def _update_learning_file(self):\n        out = ""{},{}\\n"".format(np.mean(self.loss_averages),\n                               self.epsilon)\n        self.learning_file.write(out)\n        self.learning_file.flush()\n\n    def start_episode(self, observation):\n        """"""\n        This method is called once at the beginning of each episode.\n        No reward is provided, because reward is only available after\n        an action has been taken.\n\n        Arguments:\n           observation - height x width numpy array\n\n        Returns:\n           An integer action\n        """"""\n\n        self.step_counter = 0\n        self.batch_counter = 0\n        self.episode_reward = 0\n        self.training_times = 0\n\n        # We report the mean loss for every epoch.\n        self.loss_averages = []\n\n        self.start_time = time.time()\n        return_action = self.rng.randint(0, self.num_actions)\n\n        self.last_action = return_action\n\n        self.last_img = observation\n\n        return return_action\n\n    def _choose_action(self, data_set, epsilon, cur_img, reward):\n        """"""\n        Add the most recent data to the data set and choose\n        an action based on the current policy.\n        """"""\n\n        data_set.add_sample(self.last_img, self.last_action, reward, False)\n        if self.step_counter >= self.phi_length:\n            phi = data_set.phi(cur_img)\n            action = self.network.choose_action(phi, epsilon)\n        else:\n            action = self.rng.randint(0, self.num_actions)\n\n        return action\n\n    def _do_training(self):\n        """"""\n        Returns the average loss for the current batch.\n        May be overridden if a subclass needs to train the network\n        differently.\n        """"""\n        imgs, actions, rewards, terminals = self.data_set.random_batch(self.network.batch_size)\n        # evaluation = np.zeros((self.network.batch_size, 1), np.float32)\n        # for i in range(self.network.batch_size):\n        #     state = imgs[i][self.data_set.phi_length-1]\n        #     evaluation[i] = self.qec_table.estimate(state, actions[i])\n\n        return self.network.train(imgs, actions, rewards, terminals, rewards)\n\n    def step(self, reward, observation):\n        """"""\n        This method is called each time step.\n\n        Arguments:\n           reward      - Real valued reward.\n           observation - A height x width numpy array\n\n        Returns:\n           An integer action.\n\n        """"""\n\n        self.step_counter += 1\n        self.episode_reward += reward\n\n        # TESTING---------------------------\n        if self.testing:\n            action = self._choose_action(self.test_data_set, 0.0,\n                                         observation, np.clip(reward, -1, 1))\n\n        # NOT TESTING---------------------------\n        else:\n            if len(self.data_set) > self.replay_start_size:\n                self.epsilon = max(self.epsilon_min,\n                                   self.epsilon - self.epsilon_rate)\n\n                action = self._choose_action(self.data_set, self.epsilon,\n                                             observation,\n                                             np.clip(reward, -1, 1))\n\n                if self.step_counter % self.update_frequency == 0:\n                    self.training_times += 1\n                    # loss = self._do_training()\n                    # self.batch_counter += 1\n                    # self.loss_averages.append(loss)\n\n            else:  # Still gathering initial random data...\n                action = self._choose_action(self.data_set, self.epsilon,\n                                             observation,\n                                             np.clip(reward, -1, 1))\n\n        self.last_action = action\n        self.last_img = observation\n\n        return action\n\n    def end_episode(self, reward, terminal=True):\n        """"""\n        This function is called once at the end of an episode.\n\n        Arguments:\n           reward      - Real valued reward.\n           terminal    - Whether the episode ended intrinsically\n                         (ie we didn\'t run out of steps)\n        Returns:\n            None\n        """"""\n\n        self.episode_reward += reward\n        self.step_counter += 1\n        total_time = time.time() - self.start_time\n\n        if self.testing:\n            # If we run out of time, only count the last episode if\n            # it was the only episode.\n            if terminal or self.episode_counter == 0:\n                self.episode_counter += 1\n                self.total_reward += self.episode_reward\n        else:\n            # Store the latest sample.\n            self.data_set.add_sample(self.last_img,\n                                     self.last_action,\n                                     np.clip(reward, -1, 1),\n                                     True)\n            """"""update""""""\n            q_return = 0.\n            # last_q_return = -1.0\n            index = (self.data_set.top-1) % self.data_set.max_steps\n            while True:\n                q_return = q_return * self.network.discount + self.data_set.rewards[index]\n                # if not np.isclose(q_return, last_q_return):\n                #     self.qec_table.update(self.data_set.imgs[index], self.data_set.actions[index], q_return)\n                #     last_q_return = q_return\n                # self.qec_table.update(self.data_set.imgs[index], self.data_set.actions[index], q_return)\n                self.data_set.rewards[index] = q_return\n                index = (index-1) % self.data_set.max_steps\n                if self.data_set.terminal[index] or index == self.data_set.bottom:\n                    break\n\n            for i in range(self.training_times):\n                loss = self._do_training()\n                self.batch_counter += 1\n                self.loss_averages.append(loss)\n\n            rho = 0.98\n            self.steps_sec_ema *= rho\n            self.steps_sec_ema += (1. - rho) * (self.step_counter/total_time)\n\n            logging.info(""steps/second: {:.2f}, avg: {:.2f}"".format(\n                self.step_counter/total_time, self.steps_sec_ema))\n\n            if self.batch_counter > 0:\n                self._update_learning_file()\n                logging.info(""average loss: {:.4f}"".format(\\\n                                np.mean(self.loss_averages)))\n\n    def finish_epoch(self, epoch):\n        # so large that i only keep one\n        qec_file = open(self.exp_dir + \'/qec_table_file_\' + \\\n                        \'.pkl\', \'w\')\n        cPickle.dump(self.qec_table, qec_file, 2)\n        qec_file.close()\n\n        net_file = open(self.exp_dir + \'/network_file_\' + str(epoch) + \\\n                        \'.pkl\', \'w\')\n        cPickle.dump(self.network, net_file, -1)\n        net_file.close()\n        this_time = time.time()\n        self.total_time = this_time-self.program_start_time\n        self.epoch_time = this_time-self.last_count_time\n\n    def start_testing(self):\n        self.testing = True\n        self.total_reward = 0\n        self.episode_counter = 0\n\n    def finish_testing(self, epoch):\n        self.testing = False\n        holdout_size = 3200\n\n        if self.holdout_data is None and len(self.data_set) > holdout_size:\n            imgs, _, _, _ = self.data_set.random_batch(holdout_size)\n            self.holdout_data = imgs[:, :self.phi_length]\n\n        holdout_sum = 0\n        if self.holdout_data is not None:\n            for i in range(holdout_size):\n                holdout_sum += np.max(\n                    self.network.q_vals(self.holdout_data[i]))\n\n        self._update_results_file(epoch, self.episode_counter,\n                                  holdout_sum / holdout_size)\n\n\nclass NeuralNetworkEpisodicMemory2(object):  # MAX with discounted return in history\n    def __init__(self, q_network, qec_table, epsilon_start, epsilon_min,\n                 epsilon_decay, replay_memory_size, exp_pref,\n                 replay_start_size, update_frequency, ec_discount, num_actions, ec_testing, rng):\n        self.network = q_network\n        self.qec_table = qec_table\n        self.ec_testing = ec_testing\n        self.ec_discount = ec_discount\n        self.num_actions = num_actions\n        self.epsilon_start = epsilon_start\n\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.replay_memory_size = replay_memory_size\n        self.exp_pref = exp_pref\n        self.replay_start_size = replay_start_size\n        self.update_frequency = update_frequency\n        self.rng = rng\n\n        self.phi_length = self.network.num_frames\n        self.image_width = self.network.input_width\n        self.image_height = self.network.input_height\n\n        # CREATE A FOLDER TO HOLD RESULTS\n        time_str = time.strftime(""_%m-%d-%H-%M_"", time.gmtime())\n        self.exp_dir = self.exp_pref + time_str + \\\n                       ""{}"".format(self.network.lr).replace(""."", ""p"") + ""_"" \\\n                       + ""{}"".format(self.network.discount).replace(""."", ""p"")\n\n        try:\n            os.stat(self.exp_dir)\n        except OSError:\n            os.makedirs(self.exp_dir)\n\n        self.data_set = ale_data_set.DataSet(width=self.image_width,\n                                             height=self.image_height,\n                                             rng=rng,\n                                             max_steps=self.replay_memory_size,\n                                             phi_length=self.phi_length)\n\n        # just needs to be big enough to create phi\'s\n        self.test_data_set = ale_data_set.DataSet(width=self.image_width,\n                                                  height=self.image_height,\n                                                  rng=rng,\n                                                  max_steps=self.phi_length * 2,\n                                                  phi_length=self.phi_length)\n        self.epsilon = self.epsilon_start\n        if self.epsilon_decay != 0:\n            self.epsilon_rate = ((self.epsilon_start - self.epsilon_min) /\n                                 self.epsilon_decay)\n        else:\n            self.epsilon_rate = 0\n\n        self.testing = False\n\n        self._open_results_file()\n        self._open_learning_file()\n\n        self.step_counter = None\n        self.episode_reward = None\n        self.start_time = None\n        self.loss_averages = None\n        self.total_reward = None\n\n        self.episode_counter = 0\n        self.batch_counter = 0\n\n        self.holdout_data = None\n\n        # In order to add an element to the data set we need the\n        # previous state and action and the current reward.  These\n        # will be used to store states and actions.\n        self.last_img = None\n        self.last_action = None\n\n        # Exponential moving average of runtime performance.\n        self.steps_sec_ema = 0.\n        self.program_start_time = None\n        self.last_count_time = None\n        self.epoch_time = None\n        self.total_time = None\n\n    def time_count_start(self):\n        self.last_count_time = self.program_start_time = time.time()\n\n    def _open_results_file(self):\n        logging.info(""OPENING "" + self.exp_dir + \'/results.csv\')\n        self.results_file = open(self.exp_dir + \'/results.csv\', \'w\', 0)\n        self.results_file.write(\\\n            \'epoch,num_episodes,total_reward,reward_per_epoch,mean_q, epoch time, total time\\n\')\n        self.results_file.flush()\n\n    def _open_learning_file(self):\n        self.learning_file = open(self.exp_dir + \'/learning.csv\', \'w\', 0)\n        self.learning_file.write(\'mean_loss,epsilon\\n\')\n        self.learning_file.flush()\n\n    def _update_results_file(self, epoch, num_episodes, holdout_sum):\n        out = ""{},{},{},{},{},{},{}\\n"".format(epoch, num_episodes, self.total_reward,\n                                        self.total_reward / float(num_episodes),\n                                        holdout_sum, self.epoch_time, self.total_time)\n        self.last_count_time = time.time()\n        self.results_file.write(out)\n        self.results_file.flush()\n\n    def _update_learning_file(self):\n        out = ""{},{}\\n"".format(np.mean(self.loss_averages),\n                               self.epsilon)\n        self.learning_file.write(out)\n        self.learning_file.flush()\n\n    def _open_recording_file(self):\n        self.recording_tot = 0\n        self.recording_file = open(self.exp_dir + \'/recording.csv\', \'w\', 0)\n        self.recording_file.write(\'q-learning,table,replay,\')\n        for i in range(self.qec_table.knn):\n            self.recording_file.write(\'knn_value\'+str(i)+\',\')\n        self.recording_file.write(\'\\n\')\n        self.recording_file.flush()\n\n    def _update_recording_file(self, q_return, table_return, replay_return, knn_return_list, knn_distance_list):\n        if self.recording_tot > 10000000:\n            return\n        self.recording_tot += 1\n        out = ""{},{},{},"".format(q_return, table_return, replay_return)\n        self.recording_file.write(out)\n        for i in range(len(knn_return_list)):\n            self.recording_file.write(str(knn_return_list[i])+\',\')\n        for i in range(len(knn_distance_list)):\n            self.recording_file.write(str(knn_distance_list[i])+\',\')\n        self.recording_file.write(\'\\n\')\n        self.recording_file.flush()\n\n    def start_episode(self, observation):\n        """"""\n        This method is called once at the beginning of each episode.\n        No reward is provided, because reward is only available after\n        an action has been taken.\n\n        Arguments:\n           observation - height x width numpy array\n\n        Returns:\n           An integer action\n        """"""\n\n        self.step_counter = 0\n        self.batch_counter = 0\n        self.episode_reward = 0\n\n        # We report the mean loss for every epoch.\n        self.loss_averages = []\n\n        self.start_time = time.time()\n        return_action = self.rng.randint(0, self.num_actions)\n\n        self.last_action = return_action\n\n        self.last_img = observation\n\n        return return_action\n\n    def _choose_action(self, data_set, epsilon, cur_img, reward):\n        """"""\n        Add the most recent data to the data set and choose\n        an action based on the current policy.\n        """"""\n\n        data_set.add_sample(self.last_img, self.last_action, reward, False)\n        if self.step_counter >= self.phi_length:\n            phi = data_set.phi(cur_img)\n            action = self.network.choose_action(phi, epsilon)\n        else:\n            action = self.rng.randint(0, self.num_actions)\n\n        return action\n\n    def _do_training(self):\n        """"""\n        Returns the average loss for the current batch.\n        May be overridden if a subclass needs to train the network\n        differently.\n        """"""\n        imgs, actions, rewards, terminals, return_value = self.data_set.random_batch(self.network.batch_size, True)\n        loss = self.network.train(imgs, actions, rewards, terminals, evaluation=return_value)\n        return loss\n\n    def step(self, reward, observation):\n        """"""\n        This method is called each time step.\n\n        Arguments:\n           reward      - Real valued reward.\n           observation - A height x width numpy array\n\n        Returns:\n           An integer action.\n\n        """"""\n\n        self.step_counter += 1\n        self.episode_reward += reward\n\n        # TESTING---------------------------\n        if self.testing:\n            action = self._choose_action(self.test_data_set, 0.0,\n                                         observation, np.clip(reward, -1, 1))\n\n        # NOT TESTING---------------------------\n        else:\n            if len(self.data_set) > self.replay_start_size:\n                self.epsilon = max(self.epsilon_min,\n                                   self.epsilon - self.epsilon_rate)\n\n                action = self._choose_action(self.data_set, self.epsilon,\n                                             observation,\n                                             np.clip(reward, -1, 1))\n\n                if self.step_counter % self.update_frequency == 0:\n                    loss = self._do_training()\n                    self.batch_counter += 1\n                    self.loss_averages.append(loss)\n\n            else:  # Still gathering initial random data...\n                action = self._choose_action(self.data_set, self.epsilon,\n                                             observation,\n                                             np.clip(reward, -1, 1))\n\n        self.last_action = action\n        self.last_img = observation\n\n        return action\n\n    def end_episode(self, reward, terminal=True):\n        """"""\n        This function is called once at the end of an episode.\n\n        Arguments:\n           reward      - Real valued reward.\n           terminal    - Whether the episode ended intrinsically\n                         (ie we didn\'t run out of steps)\n        Returns:\n            None\n        """"""\n\n        self.episode_reward += reward\n        self.step_counter += 1\n        total_time = time.time() - self.start_time\n\n        if self.testing:\n            # If we run out of time, only count the last episode if\n            # it was the only episode.\n            if terminal or self.episode_counter == 0:\n                self.episode_counter += 1\n                self.total_reward += self.episode_reward\n        else:\n            # Store the latest sample.\n            self.data_set.add_sample(self.last_img,\n                                     self.last_action,\n                                     np.clip(reward, -1, 1),\n                                     True)\n            """"""update""""""\n            q_return = 0.\n            # last_q_return = -1.0\n            index = (self.data_set.top-1) % self.data_set.max_steps\n            while True:\n                q_return = q_return * self.network.discount + self.data_set.rewards[index]\n                self.data_set.return_value[index] = q_return\n                index = (index-1) % self.data_set.max_steps\n                if self.data_set.terminal[index] or index == self.data_set.bottom:\n                    break\n\n            rho = 0.98\n            self.steps_sec_ema *= rho\n            self.steps_sec_ema += (1. - rho) * (self.step_counter/total_time)\n\n            logging.info(""steps/second: {:.2f}, avg: {:.2f}"".format(\n                self.step_counter/total_time, self.steps_sec_ema))\n\n            if self.batch_counter > 0:\n                self._update_learning_file()\n                logging.info(""average loss: {:.4f}"".format(\\\n                                np.mean(self.loss_averages)))\n\n    def finish_epoch(self, epoch):\n        # so large that i only keep one\n        qec_file = open(self.exp_dir + \'/qec_table_file_\' + \\\n                        \'.pkl\', \'w\')\n        cPickle.dump(self.qec_table, qec_file, 2)\n        qec_file.close()\n\n        net_file = open(self.exp_dir + \'/network_file_\' + str(epoch) + \\\n                        \'.pkl\', \'w\')\n        cPickle.dump(self.network, net_file, -1)\n        net_file.close()\n        this_time = time.time()\n        self.total_time = this_time-self.program_start_time\n        self.epoch_time = this_time-self.last_count_time\n\n    def start_testing(self):\n        self.testing = True\n        self.total_reward = 0\n        self.episode_counter = 0\n\n    def finish_testing(self, epoch):\n        self.testing = False\n        holdout_size = 3200\n\n        if self.holdout_data is None and len(self.data_set) > holdout_size:\n            imgs, _, _, _ = self.data_set.random_batch(holdout_size)\n            self.holdout_data = imgs[:, :self.phi_length]\n\n        holdout_sum = 0\n        if self.holdout_data is not None:\n            for i in range(holdout_size):\n                holdout_sum += np.max(\n                    self.network.q_vals(self.holdout_data[i]))\n\n        self._update_results_file(epoch, self.episode_counter,\n                                  holdout_sum / holdout_size)\n\n\nclass NeuralNetworkEpisodicMemory3(object):  # lsh\n    def __init__(self, q_network, lsh, epsilon_start, epsilon_min,\n                 epsilon_decay, replay_memory_size, exp_pref,\n                 replay_start_size, update_frequency, ec_discount, num_actions, ec_testing, rng):\n        self.network = q_network\n        self.lsh = lsh\n        self.ec_testing = ec_testing\n        self.ec_discount = ec_discount\n        self.num_actions = num_actions\n        self.epsilon_start = epsilon_start\n\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.replay_memory_size = replay_memory_size\n        self.exp_pref = exp_pref\n        self.replay_start_size = replay_start_size\n        self.update_frequency = update_frequency\n        self.rng = rng\n\n        self.phi_length = self.network.num_frames\n        self.image_width = self.network.input_width\n        self.image_height = self.network.input_height\n\n        # CREATE A FOLDER TO HOLD RESULTS\n        time_str = time.strftime(""_%m-%d-%H-%M_"", time.gmtime())\n        self.exp_dir = self.exp_pref + time_str + \\\n                       ""{}"".format(self.network.lr).replace(""."", ""p"") + ""_"" \\\n                       + ""{}"".format(self.network.discount).replace(""."", ""p"")\n\n        try:\n            os.stat(self.exp_dir)\n        except OSError:\n            os.makedirs(self.exp_dir)\n\n        self.data_set = ale_data_set.DataSet(width=self.image_width,\n                                             height=self.image_height,\n                                             rng=rng,\n                                             max_steps=self.replay_memory_size,\n                                             phi_length=self.phi_length)\n\n        # just needs to be big enough to create phi\'s\n        self.test_data_set = ale_data_set.DataSet(width=self.image_width,\n                                                  height=self.image_height,\n                                                  rng=rng,\n                                                  max_steps=self.phi_length * 2,\n                                                  phi_length=self.phi_length)\n        self.epsilon = self.epsilon_start\n        if self.epsilon_decay != 0:\n            self.epsilon_rate = ((self.epsilon_start - self.epsilon_min) /\n                                 self.epsilon_decay)\n        else:\n            self.epsilon_rate = 0\n\n        self.testing = False\n\n        self._open_results_file()\n        self._open_learning_file()\n\n        self.step_counter = None\n        self.episode_reward = None\n        self.start_time = None\n        self.loss_averages = None\n        self.total_reward = None\n\n        self.episode_counter = 0\n        self.batch_counter = 0\n\n        self.holdout_data = None\n\n        # In order to add an element to the data set we need the\n        # previous state and action and the current reward.  These\n        # will be used to store states and actions.\n        self.last_img = None\n        self.last_action = None\n\n        # Exponential moving average of runtime performance.\n        self.steps_sec_ema = 0.\n        self.program_start_time = None\n        self.last_count_time = None\n        self.epoch_time = None\n        self.total_time = None\n\n    def time_count_start(self):\n        self.last_count_time = self.program_start_time = time.time()\n\n    def _open_results_file(self):\n        logging.info(""OPENING "" + self.exp_dir + \'/results.csv\')\n        self.results_file = open(self.exp_dir + \'/results.csv\', \'w\', 0)\n        self.results_file.write(\\\n            \'epoch,num_episodes,total_reward,reward_per_epoch,mean_q, epoch time, total time\\n\')\n        self.results_file.flush()\n\n    def _open_learning_file(self):\n        self.learning_file = open(self.exp_dir + \'/learning.csv\', \'w\', 0)\n        self.learning_file.write(\'mean_loss,epsilon\\n\')\n        self.learning_file.flush()\n\n    def _update_results_file(self, epoch, num_episodes, holdout_sum):\n        out = ""{},{},{},{},{},{},{}\\n"".format(epoch, num_episodes, self.total_reward,\n                                        self.total_reward / float(num_episodes),\n                                        holdout_sum, self.epoch_time, self.total_time)\n        self.last_count_time = time.time()\n        self.results_file.write(out)\n        self.results_file.flush()\n\n    def _update_learning_file(self):\n        out = ""{},{}\\n"".format(np.mean(self.loss_averages),\n                               self.epsilon)\n        self.learning_file.write(out)\n        self.learning_file.flush()\n\n    def start_episode(self, observation):\n        """"""\n        This method is called once at the beginning of each episode.\n        No reward is provided, because reward is only available after\n        an action has been taken.\n\n        Arguments:\n           observation - height x width numpy array\n\n        Returns:\n           An integer action\n        """"""\n\n        self.step_counter = 0\n        self.batch_counter = 0\n        self.episode_reward = 0\n\n        # We report the mean loss for every epoch.\n        self.loss_averages = []\n\n        self.start_time = time.time()\n        return_action = self.rng.randint(0, self.num_actions)\n\n        self.last_action = return_action\n\n        self.last_img = observation\n\n        return return_action\n\n    def _choose_action(self, data_set, epsilon, cur_img, reward):\n        """"""\n        Add the most recent data to the data set and choose\n        an action based on the current policy.\n        """"""\n\n        data_set.add_sample(self.last_img, self.last_action, reward, False)\n        if self.step_counter >= self.phi_length:\n            phi = data_set.phi(cur_img)\n            action = self.network.choose_action(phi, epsilon)\n        else:\n            action = self.rng.randint(0, self.num_actions)\n\n        return action\n\n    def _do_training(self):\n        """"""\n        Returns the average loss for the current batch.\n        May be overridden if a subclass needs to train the network\n        differently.\n        """"""\n        imgs, actions, rewards, terminals = self.data_set.random_batch(self.network.batch_size)\n        return_value = np.zeros((self.network.batch_size, 1), dtype=np.float32)\n        for i in range(self.network.batch_size):\n            state = imgs[i][self.data_set.phi_length-1]\n            return_value[i] = self.lsh.evaluate(state, actions[i])\n        loss = self.network.train(imgs, actions, rewards, terminals, evaluation=return_value)\n        return loss\n\n    def step(self, reward, observation):\n        """"""\n        This method is called each time step.\n\n        Arguments:\n           reward      - Real valued reward.\n           observation - A height x width numpy array\n\n        Returns:\n           An integer action.\n\n        """"""\n\n        self.step_counter += 1\n        self.episode_reward += reward\n\n        # TESTING---------------------------\n        if self.testing:\n            action = self._choose_action(self.test_data_set, 0.0,\n                                         observation, np.clip(reward, -1, 1))\n\n        # NOT TESTING---------------------------\n        else:\n            if len(self.data_set) > self.replay_start_size:\n                self.epsilon = max(self.epsilon_min,\n                                   self.epsilon - self.epsilon_rate)\n\n                action = self._choose_action(self.data_set, self.epsilon,\n                                             observation,\n                                             np.clip(reward, -1, 1))\n\n                if self.step_counter % self.update_frequency == 0:\n                    loss = self._do_training()\n                    self.batch_counter += 1\n                    self.loss_averages.append(loss)\n\n            else:  # Still gathering initial random data...\n                action = self._choose_action(self.data_set, self.epsilon,\n                                             observation,\n                                             np.clip(reward, -1, 1))\n\n        self.last_action = action\n        self.last_img = observation\n\n        return action\n\n    def end_episode(self, reward, terminal=True):\n        """"""\n        This function is called once at the end of an episode.\n\n        Arguments:\n           reward      - Real valued reward.\n           terminal    - Whether the episode ended intrinsically\n                         (ie we didn\'t run out of steps)\n        Returns:\n            None\n        """"""\n\n        self.episode_reward += reward\n        self.step_counter += 1\n        total_time = time.time() - self.start_time\n\n        if self.testing:\n            # If we run out of time, only count the last episode if\n            # it was the only episode.\n            if terminal or self.episode_counter == 0:\n                self.episode_counter += 1\n                self.total_reward += self.episode_reward\n        else:\n            # Store the latest sample.\n            self.data_set.add_sample(self.last_img,\n                                     self.last_action,\n                                     np.clip(reward, -1, 1),\n                                     True)\n            """"""update""""""\n            q_return = 0.\n            # last_q_return = -1.0\n            index = (self.data_set.top-1) % self.data_set.max_steps\n            while True:\n                q_return = q_return * self.network.discount + self.data_set.rewards[index]\n                # self.data_set.return_value[index] = q_return\n                self.lsh.update(self.data_set.imgs[index], self.data_set.actions[index], q_return)\n                index = (index-1) % self.data_set.max_steps\n                if self.data_set.terminal[index] or index == self.data_set.bottom:\n                    break\n\n            rho = 0.98\n            self.steps_sec_ema *= rho\n            self.steps_sec_ema += (1. - rho) * (self.step_counter/total_time)\n\n            logging.info(""steps/second: {:.2f}, avg: {:.2f}"".format(\n                self.step_counter/total_time, self.steps_sec_ema))\n\n            if self.batch_counter > 0:\n                self._update_learning_file()\n                logging.info(""average loss: {:.4f}"".format(\\\n                                np.mean(self.loss_averages)))\n\n    def finish_epoch(self, epoch):\n        # so large that i only keep one\n        qec_file = open(self.exp_dir + \'/lsh_file_\' + \\\n                        \'.pkl\', \'w\')\n        cPickle.dump(self.lsh, qec_file, 2)\n        qec_file.close()\n\n        net_file = open(self.exp_dir + \'/network_file_\' + str(epoch) + \\\n                        \'.pkl\', \'w\')\n        cPickle.dump(self.network, net_file, -1)\n        net_file.close()\n        this_time = time.time()\n        self.total_time = this_time-self.program_start_time\n        self.epoch_time = this_time-self.last_count_time\n\n    def start_testing(self):\n        self.testing = True\n        self.total_reward = 0\n        self.episode_counter = 0\n\n    def finish_testing(self, epoch):\n        self.testing = False\n        holdout_size = 3200\n\n        if self.holdout_data is None and len(self.data_set) > holdout_size:\n            imgs, _, _, _ = self.data_set.random_batch(holdout_size)\n            self.holdout_data = imgs[:, :self.phi_length]\n\n        holdout_sum = 0\n        if self.holdout_data is not None:\n            for i in range(holdout_size):\n                holdout_sum += np.max(\n                    self.network.q_vals(self.holdout_data[i]))\n\n        self._update_results_file(epoch, self.episode_counter,\n                                  holdout_sum / holdout_size)\n'"
dqn_ep/ale_data_set.py,39,"b'""""""This class stores all of the samples for training.  It is able to\nconstruct randomly selected batches of phi\'s from the stored history.\n""""""\n\nimport numpy as np\nimport time\nimport theano\n\nfloatX = theano.config.floatX\n\n\nclass DataSet(object):\n    """"""A replay memory consisting of circular buffers for observed images,\nactions, and rewards.\n\n    """"""\n    def __init__(self, width, height, rng, max_steps=1000, phi_length=4):\n        """"""Construct a DataSet.\n\n        Arguments:\n            width, height - image size\n            max_steps - the number of time steps to store\n            phi_length - number of images to concatenate into a state\n            rng - initialized numpy random number generator, used to\n            choose random minibatches\n\n        """"""\n        # TODO: Specify capacity in number of state transitions, not\n        # number of saved time steps.\n\n        # Store arguments.\n        self.width = width\n        self.height = height\n        self.max_steps = max_steps\n        self.phi_length = phi_length\n        self.rng = rng\n\n        # Allocate the circular buffers and indices.\n        self.imgs = np.zeros((max_steps, height, width), dtype=\'uint8\')\n        self.actions = np.zeros(max_steps, dtype=\'int32\')\n        self.rewards = np.zeros(max_steps, dtype=floatX)\n        self.return_value = np.zeros(max_steps, dtype=floatX)\n        self.terminal = np.zeros(max_steps, dtype=\'bool\')\n\n        self.bottom = 0\n        self.top = 0\n        self.size = 0\n\n    def add_sample(self, img, action, reward, terminal):\n        """"""Add a time step record.\n\n        Arguments:\n            img -- observed image\n            action -- action chosen by the agent\n            reward -- reward received after taking the action\n            terminal -- boolean indicating whether the episode ended\n            after this time step\n        """"""\n        self.imgs[self.top] = img\n        self.actions[self.top] = action\n        self.rewards[self.top] = reward\n        self.terminal[self.top] = terminal\n\n        if self.size == self.max_steps:\n            self.bottom = (self.bottom + 1) % self.max_steps\n        else:\n            self.size += 1\n        self.top = (self.top + 1) % self.max_steps\n\n    def __len__(self):\n        """"""Return an approximate count of stored state transitions.""""""\n        # TODO: Properly account for indices which can\'t be used, as in\n        # random_batch\'s check.\n        return max(0, self.size - self.phi_length)\n\n    def last_phi(self):\n        """"""Return the most recent phi (sequence of image frames).""""""\n        indexes = np.arange(self.top - self.phi_length, self.top)\n        return self.imgs.take(indexes, axis=0, mode=\'wrap\')\n\n    def phi(self, img):\n        """"""Return a phi (sequence of image frames), using the last phi_length -\n        1, plus img.\n\n        """"""\n        indexes = np.arange(self.top - self.phi_length + 1, self.top)\n\n        phi = np.empty((self.phi_length, self.height, self.width), dtype=floatX)\n        phi[0:self.phi_length - 1] = self.imgs.take(indexes,\n                                                    axis=0,\n                                                    mode=\'wrap\')\n        phi[-1] = img\n        return phi\n\n    def random_batch(self, batch_size, get_return_value=False):\n        """"""Return corresponding imgs, actions, rewards, and terminal status for\nbatch_size randomly chosen state transitions.\n\n        """"""\n        # Allocate the response.\n        imgs = np.zeros((batch_size,\n                         self.phi_length + 1,\n                         self.height,\n                         self.width),\n                        dtype=\'uint8\')\n        actions = np.zeros((batch_size, 1), dtype=\'int32\')\n        rewards = np.zeros((batch_size, 1), dtype=floatX)\n        terminal = np.zeros((batch_size, 1), dtype=\'bool\')\n        return_value = np.zeros((batch_size, 1), dtype=floatX)\n\n        count = 0\n        while count < batch_size:\n            # Randomly choose a time step from the replay memory.\n            index = self.rng.randint(self.bottom,\n                                     self.bottom + self.size - self.phi_length)\n\n            # Both the before and after states contain phi_length\n            # frames, overlapping except for the first and last.\n            all_indices = np.arange(index, index + self.phi_length + 1)\n            end_index = index + self.phi_length - 1\n            \n            # Check that the initial state corresponds entirely to a\n            # single episode, meaning none but its last frame (the\n            # second-to-last frame in imgs) may be terminal. If the last\n            # frame of the initial state is terminal, then the last\n            # frame of the transitioned state will actually be the first\n            # frame of a new episode, which the Q learner recognizes and\n            # handles correctly during training by zeroing the\n            # discounted future reward estimate.\n            if np.any(self.terminal.take(all_indices[0:-2], mode=\'wrap\')):\n                continue\n\n            # Add the state transition to the response.\n            imgs[count] = self.imgs.take(all_indices, axis=0, mode=\'wrap\')\n            actions[count] = self.actions.take(end_index, mode=\'wrap\')\n            rewards[count] = self.rewards.take(end_index, mode=\'wrap\')\n            terminal[count] = self.terminal.take(end_index, mode=\'wrap\')\n            return_value[count] = self.return_value.take(end_index, mode=\'wrap\')\n            count += 1\n\n        if get_return_value:\n            return imgs, actions, rewards, terminal, return_value\n        return imgs, actions, rewards, terminal\n\n\n# TESTING CODE BELOW THIS POINT...\n\ndef simple_tests():\n    np.random.seed(222)\n    dataset = DataSet(width=2, height=3,\n                      rng=np.random.RandomState(42),\n                      max_steps=6, phi_length=4)\n    for i in range(10):\n        img = np.random.randint(0, 256, size=(3, 2))\n        action = np.random.randint(16)\n        reward = np.random.random()\n        terminal = False\n        if np.random.random() < .05:\n            terminal = True\n        print \'img\', img\n        dataset.add_sample(img, action, reward, terminal)\n        print ""I"", dataset.imgs\n        print ""A"", dataset.actions\n        print ""R"", dataset.rewards\n        print ""T"", dataset.terminal\n        print ""SIZE"", dataset.size\n        print\n    print ""LAST PHI"", dataset.last_phi()\n    print\n    print \'BATCH\', dataset.random_batch(2)\n\n\ndef speed_tests():\n\n    dataset = DataSet(width=80, height=80,\n                      rng=np.random.RandomState(42),\n                      max_steps=20000, phi_length=4)\n\n    img = np.random.randint(0, 256, size=(80, 80))\n    action = np.random.randint(16)\n    reward = np.random.random()\n    start = time.time()\n    for i in range(100000):\n        terminal = False\n        if np.random.random() < .05:\n            terminal = True\n        dataset.add_sample(img, action, reward, terminal)\n    print ""samples per second: "", 100000 / (time.time() - start)\n\n    start = time.time()\n    for i in range(200):\n        a = dataset.random_batch(32)\n    print ""batches per second: "", 200 / (time.time() - start)\n\n    print dataset.last_phi()\n\n\ndef trivial_tests():\n\n    dataset = DataSet(width=2, height=1,\n                      rng=np.random.RandomState(42),\n                      max_steps=3, phi_length=2)\n\n    img1 = np.array([[1, 1]], dtype=\'uint8\')\n    img2 = np.array([[2, 2]], dtype=\'uint8\')\n    img3 = np.array([[3, 3]], dtype=\'uint8\')\n\n    dataset.add_sample(img1, 1, 1, False)\n    dataset.add_sample(img2, 2, 2, False)\n    dataset.add_sample(img3, 2, 2, True)\n    print ""last"", dataset.last_phi()\n    print ""random"", dataset.random_batch(1)\n\n\ndef max_size_tests():\n    dataset1 = DataSet(width=3, height=4,\n                      rng=np.random.RandomState(42),\n                      max_steps=10, phi_length=4)\n    dataset2 = DataSet(width=3, height=4,\n                      rng=np.random.RandomState(42),\n                      max_steps=1000, phi_length=4)\n    for i in range(100):\n        img = np.random.randint(0, 256, size=(4, 3))\n        action = np.random.randint(16)\n        reward = np.random.random()\n        terminal = False\n        if np.random.random() < .05:\n            terminal = True\n        dataset1.add_sample(img, action, reward, terminal)\n        dataset2.add_sample(img, action, reward, terminal)\n        np.testing.assert_array_almost_equal(dataset1.last_phi(),\n                                             dataset2.last_phi())\n        print ""passed""\n\n\ndef test_memory_usage_ok():\n    import memory_profiler\n    dataset = DataSet(width=80, height=80,\n                      rng=np.random.RandomState(42),\n                      max_steps=100000, phi_length=4)\n    last = time.time()\n\n    for i in xrange(1000000000):\n        if (i % 100000) == 0:\n            print i\n        dataset.add_sample(np.random.random((80, 80)), 1, 1, False)\n        if i > 200000:\n            imgs, actions, rewards, terminals = \\\n                                        dataset.random_batch(32)\n        if (i % 10007) == 0:\n            print time.time() - last\n            mem_usage = memory_profiler.memory_usage(-1)\n            print len(dataset), mem_usage\n        last = time.time()\n\n\ndef main():\n    speed_tests()\n    test_memory_usage_ok()\n    max_size_tests()\n    simple_tests()\n\nif __name__ == ""__main__"":\n    main()\n'"
dqn_ep/ale_experiment.py,4,"b'""""""The ALEExperiment class handles the logic for training a deep\nQ-learning agent in the Arcade Learning Environment.\n\nAuthor: Nathan Sprague\n\n""""""\nimport logging\nimport numpy as np\nimport image_preprocessing\n\n# Number of rows to crop off the bottom of the (downsampled) screen.\n# This is appropriate for breakout, but it may need to be modified\n# for other games.\nCROP_OFFSET = 8\n\n\nclass ALEExperiment(object):\n    def __init__(self, ale, agent, resized_width, resized_height,\n                 resize_method, num_epochs, epoch_length, test_length,\n                 frame_skip, death_ends_episode, max_start_nullops, rng):\n        self.ale = ale\n        self.agent = agent\n        self.num_epochs = num_epochs\n        self.epoch_length = epoch_length\n        self.test_length = test_length\n        self.frame_skip = frame_skip\n        self.death_ends_episode = death_ends_episode\n        self.min_action_set = ale.getMinimalActionSet()\n        self.resized_width = resized_width\n        self.resized_height = resized_height\n        self.resize_method = resize_method\n        self.width, self.height = ale.getScreenDims()\n\n        self.buffer_length = 10\n        self.buffer_count = 0\n        self.screen_buffer = np.empty((self.buffer_length,\n                                       self.height, self.width),\n                                      dtype=np.uint8)\n\n        self.terminal_lol = False # Most recent episode ended on a loss of life\n        self.max_start_nullops = max_start_nullops\n        self.rng = rng\n\n    def run(self):\n        """"""\n        Run the desired number of training epochs, a testing epoch\n        is conducted after each training epoch.\n        """"""\n        self.agent.time_count_start()\n        for epoch in range(1, self.num_epochs + 1):\n            self.run_epoch(epoch, self.epoch_length)\n            self.agent.finish_epoch(epoch)\n\n            if self.test_length > 0:\n                self.agent.start_testing()\n                self.run_epoch(epoch, self.test_length, True)\n                self.agent.finish_testing(epoch)\n\n    def run_epoch(self, epoch, num_steps, testing=False):\n        """""" Run one \'epoch\' of training or testing, where an epoch is defined\n        by the number of steps executed.  Prints a progress report after\n        every trial\n\n        Arguments:\n        epoch - the current epoch number\n        num_steps - steps per epoch\n        testing - True if this Epoch is used for testing and not training\n\n        """"""\n        self.terminal_lol = False  # Make sure each epoch starts with a reset.\n        steps_left = num_steps\n        while steps_left > 0:\n            prefix = ""testing"" if testing else ""training""\n            logging.info(prefix + "" epoch: "" + str(epoch) + "" steps_left: "" +\n                         str(steps_left))\n            _, num_steps = self.run_episode(steps_left, testing)\n\n            steps_left -= num_steps\n\n    def _init_episode(self):\n        """""" This method resets the game if needed, performs enough null\n        actions to ensure that the screen buffer is ready and optionally\n        performs a randomly determined number of null action to randomize\n        the initial game state.""""""\n\n        if not self.terminal_lol or self.ale.game_over():\n            self.ale.reset_game()\n\n            if self.max_start_nullops > 0:\n                random_actions = self.rng.randint(8, self.max_start_nullops+1)\n                for _ in range(random_actions):\n                    self._act(0)  # Null action\n\n        # Make sure the screen buffer is filled at the beginning of\n        # each episode...\n        self._act(0)\n        self._act(0)\n\n    def _act(self, action):\n        """"""Perform the indicated action for a single frame, return the\n        resulting reward and store the resulting screen image in the\n        buffer\n\n        """"""\n        reward = self.ale.act(action)\n        index = self.buffer_count % self.buffer_length\n\n        self.ale.getScreenGrayscale(self.screen_buffer[index, ...])\n\n        self.buffer_count += 1\n        return reward\n\n    def _step(self, action):\n        """""" Repeat one action the appopriate number of times and return\n        the summed reward. """"""\n        reward = 0\n        for _ in range(self.frame_skip):\n            reward += self._act(action)\n\n        return reward\n\n    def run_episode(self, max_steps, testing):\n        """"""Run a single training episode.\n\n        The boolean terminal value returned indicates whether the\n        episode ended because the game ended or the agent died (True)\n        or because the maximum number of steps was reached (False).\n        Currently this value will be ignored.\n\n        Return: (terminal, num_steps)\n\n        """"""\n\n        self._init_episode()\n\n        start_lives = self.ale.lives()\n\n        action = self.agent.start_episode(self.get_observation())\n        num_steps = 0\n        while True:\n            reward = self._step(self.min_action_set[action])\n            self.terminal_lol = (self.death_ends_episode and not testing and\n                                 self.ale.lives() < start_lives)\n            terminal = self.ale.game_over() or self.terminal_lol\n            num_steps += 1\n\n            if terminal or num_steps >= max_steps:\n                self.agent.end_episode(reward, terminal)\n                break\n\n            action = self.agent.step(reward, self.get_observation())\n        return terminal, num_steps\n\n    def get_observation(self):\n        """""" Resize and merge the previous two screen images """"""\n\n        assert self.buffer_count >= self.buffer_length\n        index = self.buffer_count % self.buffer_length - 1\n        # max_image = np.maximum(self.screen_buffer[index, ...],\n        #                        self.screen_buffer[index - 1, ...])\n        max_image = self.screen_buffer[index]\n        for i in range(self.buffer_length):\n            max_image = np.maximum(max_image, self.screen_buffer[index-i, ...])\n        return self.resize_image(max_image)\n\n    def resize_image(self, image):\n        """""" Appropriately resize a single image """"""\n\n        if self.resize_method == \'crop\':\n            # resize keeping aspect ratio\n            resize_height = int(round(\n                float(self.height) * self.resized_width / self.width))\n\n            resized = image_preprocessing.resize(image, (self.resized_width, resize_height))\n\n            # Crop the part we want\n            crop_y_cutoff = resize_height - CROP_OFFSET - self.resized_height\n            cropped = resized[crop_y_cutoff:\n                              crop_y_cutoff + self.resized_height, :]\n\n            return cropped\n        elif self.resize_method == \'scale\':\n            return image_preprocessing.resize(image, (self.resized_width, self.resized_height))\n        else:\n            raise ValueError(\'Unrecognized image resize method.\')\n\n'"
dqn_ep/image_preprocessing.py,2,"b""__author__ = 'frankhe'\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport scipy.misc\nimport cPickle\n\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[..., :3], [0.299, 0.587, 0.114])\n\n\ndef resize(image, size):\n    return scipy.misc.imresize(image, size=size)\n\n\ndef imshow(photo, gray=False):\n    if gray:\n        plt.imshow(photo, cmap = plt.get_cmap('gray'))\n    else:\n        plt.imshow(photo)\n    plt.show()\n\n\ndef show_wall_paper():\n    img = mpimg.imread('wallpaper.jpg')\n    gray = rgb2gray(img)\n    gray = resize(gray, (1000, 1000))\n    imshow(gray, True)\n\nif __name__ == '__main__':\n    f1 = open('game_images', mode='rb')\n    images = cPickle.load(f1)\n    print images[0].size, images[0].shape\n    for i in range(1, len(images)):\n        # imshow(images[i])\n        # print np.sum(images[i]-images[i-1])\n        raw_input()\n\n"""
dqn_ep/images2gif.py,7,"b'#   This file is part of VISVIS. This file may be distributed\n#   seperately, but under the same license as VISVIS (LGPL).\n#\n#   images2gif is free software: you can redistribute it and/or modify\n#   it under the terms of the GNU Lesser General Public License as\n#   published by the Free Software Foundation, either version 3 of\n#   the License, or (at your option) any later version.\n#\n#   images2gif is distributed in the hope that it will be useful,\n#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#   GNU Lesser General Public License for more details.\n#\n#   You should have received a copy of the GNU Lesser General Public\n#   License along with this program.  If not, see\n#   <http://www.gnu.org/licenses/>.\n#\n#   Copyright (C) 2009 Almar Klein\n\n"""""" Module images2gif\n\nProvides functionality for reading and writing animated GIF images.\nUse writeGif to write a series of numpy arrays or PIL images as an\nanimated GIF. Use readGif to read an animated gif as a series of numpy\narrays.\n\nMany thanks to Ant1 for:\n* noting the use of ""palette=PIL.Image.ADAPTIVE"", which significantly\n  improves the results.\n* the modifications to save each image with its own palette, or optionally\n  the global palette (if its the same).\n\n- based on gifmaker (in the scripts folder of the source distribution of PIL)\n- based on gif file structure as provided by wikipedia\n\n""""""\n\ntry:\n    import PIL\n    from PIL import Image, ImageChops\n    from PIL.GifImagePlugin import getheader, getdata\nexcept ImportError:\n    PIL = None\n\ntry:\n    import numpy as np\nexcept ImportError:\n    np = None\n\n# getheader gives a 87a header and a color palette (two elements in a list).\n# getdata()[0] gives the Image Descriptor up to (including) ""LZW min code size"".\n# getdatas()[1:] is the image data itself in chuncks of 256 bytes (well\n# technically the first byte says how many bytes follow, after which that\n# amount (max 255) follows).\n\n\ndef intToBin(i):\n    """""" Integer to two bytes """"""\n    # devide in two parts (bytes)\n    i1 = i % 256\n    i2 = int( i/256)\n    # make string (little endian)\n    return chr(i1) + chr(i2)\n\n\ndef getheaderAnim(im):\n    """""" Animation header. To replace the getheader()[0] """"""\n    bb = ""GIF89a""\n    bb += intToBin(im.size[0])\n    bb += intToBin(im.size[1])\n    bb += ""\\x87\\x00\\x00""\n    return bb\n\n\ndef getImageDescriptor(im):\n    """""" Used for the local color table properties per image.\n    Otherwise global color table applies to all frames irrespective of\n    wether additional colours comes in play that require a redefined palette\n    Still a maximum of 256 color per frame, obviously.\n\n    Written by Ant1 on 2010-08-22\n    """"""\n    bb = \'\\x2C\' # Image separator,\n    bb += intToBin( 0 ) # Left position\n    bb += intToBin( 0 ) # Top position\n    bb += intToBin( im.size[0] ) # image width\n    bb += intToBin( im.size[1] ) # image height\n    bb += \'\\x87\' # packed field : local color table flag1, interlace0, sorted table0, reserved00, lct size111=7=2^(7+1)=256.\n    # LZW minimum size code now comes later, begining of [image data] blocks\n    return bb\n\n\ndef getAppExt(loops=float(\'inf\')):\n    """""" Application extention. Part that specifies amount of loops.\n    If loops is inf, it goes on infinitely.\n    """"""\n    if loops == 0:\n        bb = """" # application extension should not be used\n                # (the extension interprets zero loops\n                # to mean an infinite number of loops)\n    else:\n        bb = ""\\x21\\xFF\\x0B""  # application extension\n        bb += ""NETSCAPE2.0""\n        bb += ""\\x03\\x01""\n        if loops == float(\'inf\'):\n            loops = 2**16-1\n        bb += intToBin(loops)\n        bb += \'\\x00\'  # end\n    return bb\n\n\ndef getGraphicsControlExt(duration=0.1):\n    """""" Graphics Control Extension. A sort of header at the start of\n    each image. Specifies transparancy and duration. """"""\n    bb = \'\\x21\\xF9\\x04\'\n    bb += \'\\x08\'  # no transparancy\n    bb += intToBin( int(duration*100) ) # in 100th of seconds\n    bb += \'\\x00\'  # no transparant color\n    bb += \'\\x00\'  # end\n    return bb\n\n\ndef _writeGifToFile(fp, images, durations, loops):\n    """""" Given a set of images writes the bytes to the specified stream.\n    """"""\n\n    # Obtain palette for all images and count each occurance\n    palettes, occur = [], []\n    for im in images:\n        palettes.append(im.palette.getdata()[1])\n    for palette in palettes:\n        occur.append( palettes.count( palette ) )\n\n    # Select most-used palette as the global one (or first in case no max)\n    globalPalette = palettes[ occur.index(max(occur)) ]\n\n    # Init\n    frames = 0\n    firstFrame = True\n\n\n    for im, palette in zip(images, palettes):\n\n        if firstFrame:\n            # Write header\n\n            # Gather info\n            header = getheaderAnim(im)\n            appext = getAppExt(loops)\n\n            # Write\n            fp.write(header)\n            fp.write(globalPalette)\n            fp.write(appext)\n\n            # Next frame is not the first\n            firstFrame = False\n\n        if True:\n            # Write palette and image data\n\n            # Gather info\n            data = getdata(im)\n            imdes, data = data[0], data[1:]\n            graphext = getGraphicsControlExt(durations[frames])\n            # Make image descriptor suitable for using 256 local color palette\n            lid = getImageDescriptor(im)\n\n            # Write local header\n            if palette != globalPalette:\n                # Use local color palette\n                fp.write(graphext)\n                fp.write(lid) # write suitable image descriptor\n                fp.write(palette) # write local color table\n                fp.write(\'\\x08\') # LZW minimum size code\n            else:\n                # Use global color palette\n                fp.write(graphext)\n                fp.write(imdes) # write suitable image descriptor\n\n            # Write image data\n            for d in data:\n                fp.write(d)\n\n        # Prepare for next round\n        frames = frames + 1\n\n    fp.write("";"")  # end gif\n    return frames\n\n\n## Exposed functions\n\ndef writeGif(filename, images, duration=0.1, loops=0, dither=1):\n    """""" writeGif(filename, images, duration=0.1, loops=0, dither=1)\n    Write an animated gif from the specified images.\n    images should be a list of numpy arrays of PIL images.\n    Numpy images of type float should have pixels between 0 and 1.\n    Numpy images of other types are expected to have values between 0 and 255.\n    """"""\n\n    if PIL is None:\n        raise RuntimeError(""Need PIL to write animated gif files."")\n\n    AD = Image.ADAPTIVE\n    images2 = []\n\n    # convert to PIL\n    for im in images:\n\n        if isinstance(im,Image.Image):\n            images2.append( im.convert(\'P\', palette=AD, dither=dither) )\n\n        elif np and isinstance(im, np.ndarray):\n            if im.dtype == np.uint8:\n                pass\n            elif im.dtype in [np.float32, np.float64]:\n                im = (im*255).astype(np.uint8)\n            else:\n                im = im.astype(np.uint8)\n            # convert\n            if len(im.shape)==3 and im.shape[2]==3:\n                im = Image.fromarray(im,\'RGB\').convert(\'P\', palette=AD, dither=dither)\n            elif len(im.shape)==2:\n                im = Image.fromarray(im,\'L\').convert(\'P\', palette=AD, dither=dither)\n            else:\n                raise ValueError(""Array has invalid shape to be an image."")\n            images2.append(im)\n\n        else:\n            raise ValueError(""Unknown image type."")\n\n    # check duration\n    if hasattr(duration, \'__len__\'):\n        if len(duration) == len(images2):\n            durations = [d for d in duration]\n        else:\n            raise ValueError(""len(duration) doesn\'t match amount of images."")\n    else:\n        durations = [duration for im in images2]\n\n\n    # open file\n    fp = open(filename, \'wb\')\n\n    # write\n    try:\n        n = _writeGifToFile(fp, images2, durations, loops)\n        print n, \'frames written\'\n    finally:\n        fp.close()\n\n\ndef readGif(filename):\n    """"""\n    """"""\n\n    # Check PIL\n    if PIL is None:\n        raise RuntimeError(""Need PIL to read animated gif files."")\n\n    # Check whether it exists\n    if not os.path.isfile(filename):\n        raise IOError(\'File not found: \'+str(filename))\n\n    # Load file using PIL\n    pilIm = PIL.Image.open(filename)\n    pilIm.seek(0)\n\n    # Read all images inside\n    ims = []\n    try:\n        while True:\n            # Get image as numpy array\n            tmp = pilIm.convert() # Make without palette\n            a = np.asarray(tmp)\n            if len(a.shape)==0:\n                raise MemoryError(""Too little memory to convert PIL image to array"")\n            # Store, and next\n            ims.append(a)\n            pilIm.seek(pilIm.tell()+1)\n    except EOFError:\n        pass\n\n    # Done\n    return ims\n\n\nif __name__ == \'__main__\':\n    im = np.zeros((200,200), dtype=np.uint8)\n    im[10:30,:] = 100\n    im[:,80:120] = 255\n    im[-50:-40,:] = 50\n\n    images = [im*1.0, im*0.8, im*0.6, im*0.4, im*0]\n    writeGif(\'lala3.gif\',images)\n'"
dqn_ep/launcher.py,2,"b'#! /usr/bin/env python\n""""""This script handles reading command line arguments and starting the\ntraining process.  It shouldn\'t be executed directly; it is used by\nrunning script.\n\n""""""\nimport os\nimport argparse\nimport logging\ntry:\n    import ale_python_interface\nexcept ImportError:\n    import atari_py.ale_python_interface as ale_python_interface\nimport cPickle\nimport numpy as np\nimport theano\n\nimport ale_experiment\nimport q_network\nimport ale_agents\nimport EC_functions\n\n\ndef process_args(args, defaults, description):\n    """"""\n    Handle the command line.\n\n    args     - list of command line arguments (not including executable name)\n    defaults - a name space with variables corresponding to each of\n               the required default command line values.\n    description - a string to display at the top of the help message.\n    """"""\n    parser = argparse.ArgumentParser(description=description)\n    parser.add_argument(\'-r\', \'--rom\', dest=""rom"", default=defaults.ROM,\n                        help=\'ROM to run (default: %(default)s)\')\n    parser.add_argument(\'-e\', \'--epochs\', dest=""epochs"", type=int,\n                        default=defaults.EPOCHS,\n                        help=\'Number of training epochs (default: %(default)s)\')\n    parser.add_argument(\'-s\', \'--steps-per-epoch\', dest=""steps_per_epoch"",\n                        type=int, default=defaults.STEPS_PER_EPOCH,\n                        help=\'Number of steps per epoch (default: %(default)s)\')\n    parser.add_argument(\'-t\', \'--test-length\', dest=""steps_per_test"",\n                        type=int, default=defaults.STEPS_PER_TEST,\n                        help=\'Number of steps per test (default: %(default)s)\')\n    parser.add_argument(\'--display-screen\', dest=""display_screen"",\n                        action=\'store_true\', default=False,\n                        help=\'Show the game screen.\')\n    parser.add_argument(\'--double-dqn\', dest=""double_dqn"",\n                        action=\'store_true\', default=False,\n                        help=\'enable double DQN\')\n    parser.add_argument(\'--experiment-prefix\', dest=""experiment_prefix"",\n                        default=None,\n                        help=\'Experiment name prefix \'\n                        \'(default is the name of the game)\')\n    parser.add_argument(\'--frame-skip\', dest=""frame_skip"",\n                        default=defaults.FRAME_SKIP, type=int,\n                        help=\'Every how many frames to process \'\n                        \'(default: %(default)s)\')\n    parser.add_argument(\'--repeat-action-probability\',\n                        dest=""repeat_action_probability"",\n                        default=defaults.REPEAT_ACTION_PROBABILITY, type=float,\n                        help=(\'Probability that action choice will be \' +\n                              \'ignored (default: %(default)s)\'))\n    parser.add_argument(\'--update-rule\', dest=""update_rule"",\n                        type=str, default=defaults.UPDATE_RULE,\n                        help=(\'deepmind_rmsprop|rmsprop|sgd \' +\n                              \'(default: %(default)s)\'))\n    parser.add_argument(\'--batch-accumulator\', dest=""batch_accumulator"",\n                        type=str, default=defaults.BATCH_ACCUMULATOR,\n                        help=(\'sum|mean (default: %(default)s)\'))\n    parser.add_argument(\'--learning-rate\', dest=""learning_rate"",\n                        type=float, default=defaults.LEARNING_RATE,\n                        help=\'Learning rate (default: %(default)s)\')\n    parser.add_argument(\'--rms-decay\', dest=""rms_decay"",\n                        type=float, default=defaults.RMS_DECAY,\n                        help=\'Decay rate for rms_prop (default: %(default)s)\')\n    parser.add_argument(\'--rms-epsilon\', dest=""rms_epsilon"",\n                        type=float, default=defaults.RMS_EPSILON,\n                        help=\'Denominator epsilson for rms_prop \' +\n                        \'(default: %(default)s)\')\n    parser.add_argument(\'--momentum\', type=float, default=defaults.MOMENTUM,\n                        help=(\'Momentum term for Nesterov momentum. \'+\n                              \'(default: %(default)s)\'))\n    parser.add_argument(\'--clip-delta\', dest=""clip_delta"", type=float,\n                        default=defaults.CLIP_DELTA,\n                        help=(\'Max absolute value for Q-update delta value. \' +\n                              \'(default: %(default)s)\'))\n    parser.add_argument(\'--discount\', type=float, default=defaults.DISCOUNT,\n                        help=\'Discount rate\')\n    parser.add_argument(\'--epsilon-start\', dest=""epsilon_start"",\n                        type=float, default=defaults.EPSILON_START,\n                        help=(\'Starting value for epsilon. \' +\n                              \'(default: %(default)s)\'))\n    parser.add_argument(\'--epsilon-min\', dest=""epsilon_min"",\n                        type=float, default=defaults.EPSILON_MIN,\n                        help=\'Minimum epsilon. (default: %(default)s)\')\n    parser.add_argument(\'--epsilon-decay\', dest=""epsilon_decay"",\n                        type=float, default=defaults.EPSILON_DECAY,\n                        help=(\'Number of steps to minimum epsilon. \' +\n                              \'(default: %(default)s)\'))\n    parser.add_argument(\'--phi-length\', dest=""phi_length"",\n                        type=int, default=defaults.PHI_LENGTH,\n                        help=(\'Number of recent frames used to represent \' +\n                              \'state. (default: %(default)s)\'))\n    parser.add_argument(\'--max-history\', dest=""replay_memory_size"",\n                        type=int, default=defaults.REPLAY_MEMORY_SIZE,\n                        help=(\'Maximum number of steps stored in replay \' +\n                              \'memory. (default: %(default)s)\'))\n    parser.add_argument(\'--batch-size\', dest=""batch_size"",\n                        type=int, default=defaults.BATCH_SIZE,\n                        help=\'Batch size. (default: %(default)s)\')\n    parser.add_argument(\'--network-type\', dest=""network_type"",\n                        type=str, default=defaults.NETWORK_TYPE,\n                        help=(\'nips_cuda|nips_dnn|nature_cuda|nature_dnn\' +\n                              \'|linear (default: %(default)s)\'))\n    parser.add_argument(\'--freeze-interval\', dest=""freeze_interval"",\n                        type=int, default=defaults.FREEZE_INTERVAL,\n                        help=(\'Interval between target freezes. \' +\n                              \'(default: %(default)s)\'))\n    parser.add_argument(\'--update-frequency\', dest=""update_frequency"",\n                        type=int, default=defaults.UPDATE_FREQUENCY,\n                        help=(\'Number of actions before each SGD update. \'+\n                              \'(default: %(default)s)\'))\n    parser.add_argument(\'--replay-start-size\', dest=""replay_start_size"",\n                        type=int, default=defaults.REPLAY_START_SIZE,\n                        help=(\'Number of random steps before training. \' +\n                              \'(default: %(default)s)\'))\n    parser.add_argument(\'--resize-method\', dest=""resize_method"",\n                        type=str, default=defaults.RESIZE_METHOD,\n                        help=(\'crop|scale (default: %(default)s)\'))\n    parser.add_argument(\'--nn-file\', dest=""nn_file"", type=str, default=None,\n                        help=\'Pickle file containing trained net.\')\n    parser.add_argument(\'--death-ends-episode\', dest=""death_ends_episode"",\n                        type=str, default=defaults.DEATH_ENDS_EPISODE,\n                        help=(\'true|false (default: %(default)s)\'))\n    parser.add_argument(\'--max-start-nullops\', dest=""max_start_nullops"",\n                        type=int, default=defaults.MAX_START_NULLOPS,\n                        help=(\'Maximum number of null-ops at the start \' +\n                              \'of games. (default: %(default)s)\'))\n    parser.add_argument(\'--deterministic\', dest=""deterministic"",\n                        type=bool, default=defaults.DETERMINISTIC,\n                        help=(\'Whether to use deterministic parameters \' +\n                              \'for learning. (default: %(default)s)\'))\n    parser.add_argument(\'--cudnn_deterministic\', dest=""cudnn_deterministic"",\n                        type=bool, default=defaults.CUDNN_DETERMINISTIC,\n                        help=(\'Whether to use deterministic backprop. \' +\n                              \'(default: %(default)s)\'))\n\n    # for episodic control\n    parser.add_argument(\'--method\', dest=""method"",\n                        type=str, default=defaults.METHOD,\n                        help=\'choose different learning algorithms\')\n    parser.add_argument(\'--knn\', dest=\'knn\',\n                        type=int, default=defaults.K_NEAREST_NEIGHBOR,\n                        help=\'k nearest neighbor\')\n    parser.add_argument(\'--ec-discount\', dest=\'ec_discount\',\n                        type=float, default=defaults.EC_DISCOUNT,\n                        help=\'episodic control discount\')\n    parser.add_argument(\'--buffer-size\', dest=\'buffer_size\',\n                        type=int, default=defaults.BUFFER_SIZE,\n                        help=\'buffer size for each action in episodic control\')\n    parser.add_argument(\'--state-dimension\', dest=\'state_dimension\',\n                        type=int, default=defaults.DIMENSION_OF_STATE,\n                        help=\'the dimension of the stored state\')\n    parser.add_argument(\'--projection-type\', dest=\'projection_type\',\n                        type=str, default=defaults.PROJECTION_TYPE,\n                        help=\'the type of the ec projection\')\n    parser.add_argument(\'--qec-table\', dest=\'qec_table\',\n                        type=str, default=None,\n                        help=\'Qec table file for episodic control\')\n    parser.add_argument(\'--ec-testing\', dest=\'ec_testing\',\n                        type=bool, default=defaults.TESTING,\n                        help=\'used for episodic_control_run.py to test a qec_table\')\n    parser.add_argument(\'--rebuild-knn-frequency\', dest=\'rebuild_knn_frequency\',\n                        type=int, default=defaults.REBUILD_KNN_FREQUENCY,\n                        help=\'rebuild frequency for knn\')\n\n    parameters = parser.parse_args(args)\n    if parameters.experiment_prefix is None:\n        name = os.path.splitext(os.path.basename(parameters.rom))[0]\n        parameters.experiment_prefix = name\n\n    if parameters.death_ends_episode == \'true\':\n        parameters.death_ends_episode = True\n    elif parameters.death_ends_episode == \'false\':\n        parameters.death_ends_episode = False\n    else:\n        raise ValueError(""--death-ends-episode must be true or false"")\n\n    if parameters.freeze_interval > 0:\n        # This addresses an inconsistency between the Nature paper and\n        # the Deepmind code.  The paper states that the target network\n        # update frequency is ""measured in the number of parameter\n        # updates"".  In the code it is actually measured in the number\n        # of action choices.\n        parameters.freeze_interval = (parameters.freeze_interval //\n                                      parameters.update_frequency)\n\n    return parameters\n\n\ndef launch(args, defaults, description):\n    """"""\n    Execute a complete training run.\n    """"""\n\n    logging.basicConfig(level=logging.INFO)\n    parameters = process_args(args, defaults, description)\n\n    if parameters.rom.endswith(\'.bin\'):\n        rom = parameters.rom\n    else:\n        rom = ""%s.bin"" % parameters.rom\n    full_rom_path = os.path.join(defaults.BASE_ROM_PATH, rom)\n\n    if parameters.deterministic:\n        rng = np.random.RandomState(123456)\n    else:\n        rng = np.random.RandomState()\n\n    if parameters.cudnn_deterministic:\n        theano.config.dnn.conv.algo_bwd = \'deterministic\'\n\n    ale = ale_python_interface.ALEInterface()\n    ale.setInt(\'random_seed\', rng.randint(1000))\n\n    if parameters.display_screen:\n        import sys\n        if sys.platform == \'darwin\':\n            import pygame\n            pygame.init()\n            ale.setBool(\'sound\', False) # Sound doesn\'t work on OSX\n\n    ale.setBool(\'display_screen\', parameters.display_screen)\n    ale.setFloat(\'repeat_action_probability\',\n                 parameters.repeat_action_probability)\n\n    ale.loadROM(full_rom_path)\n\n    num_actions = len(ale.getMinimalActionSet())\n\n    agent = None\n\n    if parameters.method == \'ec_dqn\':\n        if parameters.nn_file is None:\n            network = q_network.DeepQLearner(defaults.RESIZED_WIDTH,\n                                             defaults.RESIZED_HEIGHT,\n                                             num_actions,\n                                             parameters.phi_length,\n                                             parameters.discount,\n                                             parameters.learning_rate,\n                                             parameters.rms_decay,\n                                             parameters.rms_epsilon,\n                                             parameters.momentum,\n                                             parameters.clip_delta,\n                                             parameters.freeze_interval,\n                                             parameters.batch_size,\n                                             parameters.network_type,\n                                             parameters.update_rule,\n                                             parameters.batch_accumulator,\n                                             rng, use_ec=True, double=parameters.double_dqn)\n        else:\n            handle = open(parameters.nn_file, \'r\')\n            network = cPickle.load(handle)\n\n        if parameters.qec_table is None:\n            qec_table = EC_functions.QECTable(parameters.knn,\n                                              parameters.state_dimension,\n                                              parameters.projection_type,\n                                              defaults.RESIZED_WIDTH*defaults.RESIZED_HEIGHT,\n                                              parameters.buffer_size,\n                                              num_actions,\n                                              rng,\n                                              parameters.rebuild_knn_frequency)\n        else:\n            handle = open(parameters.qec_table, \'r\')\n            qec_table = cPickle.load(handle)\n\n        agent = ale_agents.EC_DQN(network,\n                                  qec_table,\n                                  parameters.epsilon_start,\n                                  parameters.epsilon_min,\n                                  parameters.epsilon_decay,\n                                  parameters.replay_memory_size,\n                                  parameters.experiment_prefix,\n                                  parameters.replay_start_size,\n                                  parameters.update_frequency,\n                                  parameters.ec_discount,\n                                  num_actions,\n                                  parameters.ec_testing,\n                                  rng)\n\n    if parameters.method == \'dqn_episodic_memory1\':\n        if parameters.nn_file is None:\n            network = q_network.DeepQLearner(defaults.RESIZED_WIDTH,\n                                             defaults.RESIZED_HEIGHT,\n                                             num_actions,\n                                             parameters.phi_length,\n                                             parameters.discount,\n                                             parameters.learning_rate,\n                                             parameters.rms_decay,\n                                             parameters.rms_epsilon,\n                                             parameters.momentum,\n                                             parameters.clip_delta,\n                                             parameters.freeze_interval,\n                                             parameters.batch_size,\n                                             parameters.network_type,\n                                             parameters.update_rule,\n                                             parameters.batch_accumulator,\n                                             rng, use_episodic_mem=True, double=parameters.double_dqn)\n        else:\n            handle = open(parameters.nn_file, \'r\')\n            network = cPickle.load(handle)\n\n        if parameters.qec_table is None:\n            qec_table = EC_functions.QECTable(parameters.knn,\n                                              parameters.state_dimension,\n                                              parameters.projection_type,\n                                              defaults.RESIZED_WIDTH*defaults.RESIZED_HEIGHT,\n                                              parameters.buffer_size,\n                                              num_actions,\n                                              rng,\n                                              parameters.rebuild_knn_frequency)\n        else:\n            handle = open(parameters.qec_table, \'r\')\n            qec_table = cPickle.load(handle)\n\n        agent = ale_agents.NeuralNetworkEpisodicMemory1(network,\n                                                        qec_table,\n                                                        parameters.epsilon_start,\n                                                        parameters.epsilon_min,\n                                                        parameters.epsilon_decay,\n                                                        parameters.replay_memory_size,\n                                                        parameters.experiment_prefix,\n                                                        parameters.replay_start_size,\n                                                        parameters.update_frequency,\n                                                        parameters.ec_discount,\n                                                        num_actions,\n                                                        parameters.ec_testing,\n                                                        rng)\n    if parameters.method == \'dqn_episodic_memory2\':\n        if parameters.nn_file is None:\n            network = q_network.DeepQLearner(defaults.RESIZED_WIDTH,\n                                             defaults.RESIZED_HEIGHT,\n                                             num_actions,\n                                             parameters.phi_length,\n                                             parameters.discount,\n                                             parameters.learning_rate,\n                                             parameters.rms_decay,\n                                             parameters.rms_epsilon,\n                                             parameters.momentum,\n                                             parameters.clip_delta,\n                                             parameters.freeze_interval,\n                                             parameters.batch_size,\n                                             parameters.network_type,\n                                             parameters.update_rule,\n                                             parameters.batch_accumulator,\n                                             rng, use_episodic_mem=True, double=parameters.double_dqn)\n        else:\n            handle = open(parameters.nn_file, \'r\')\n            network = cPickle.load(handle)\n\n        if parameters.qec_table is None:\n            qec_table = EC_functions.QECTable(parameters.knn,\n                                              parameters.state_dimension,\n                                              parameters.projection_type,\n                                              defaults.RESIZED_WIDTH*defaults.RESIZED_HEIGHT,\n                                              parameters.buffer_size,\n                                              num_actions,\n                                              rng,\n                                              parameters.rebuild_knn_frequency)\n        else:\n            handle = open(parameters.qec_table, \'r\')\n            qec_table = cPickle.load(handle)\n\n    if parameters.method == \'dqn_episodic_memory3\':\n        if parameters.nn_file is None:\n            network = q_network.DeepQLearner(defaults.RESIZED_WIDTH,\n                                             defaults.RESIZED_HEIGHT,\n                                             num_actions,\n                                             parameters.phi_length,\n                                             parameters.discount,\n                                             parameters.learning_rate,\n                                             parameters.rms_decay,\n                                             parameters.rms_epsilon,\n                                             parameters.momentum,\n                                             parameters.clip_delta,\n                                             parameters.freeze_interval,\n                                             parameters.batch_size,\n                                             parameters.network_type,\n                                             parameters.update_rule,\n                                             parameters.batch_accumulator,\n                                             rng, use_episodic_mem=True, double=parameters.double_dqn)\n        else:\n            handle = open(parameters.nn_file, \'r\')\n            network = cPickle.load(handle)\n\n        if parameters.qec_table is None:\n            qec_table = EC_functions.LshHash(parameters.state_dimension,\n                                             defaults.RESIZED_WIDTH*defaults.RESIZED_HEIGHT,\n                                             parameters.buffer_size,\n                                             rng)\n        else:\n            handle = open(parameters.qec_table, \'r\')\n            qec_table = cPickle.load(handle)\n\n        agent = ale_agents.NeuralNetworkEpisodicMemory3(network,\n                                                        qec_table,\n                                                        parameters.epsilon_start,\n                                                        parameters.epsilon_min,\n                                                        parameters.epsilon_decay,\n                                                        parameters.replay_memory_size,\n                                                        parameters.experiment_prefix,\n                                                        parameters.replay_start_size,\n                                                        parameters.update_frequency,\n                                                        parameters.ec_discount,\n                                                        num_actions,\n                                                        parameters.ec_testing,\n                                                        rng)\n\n    if parameters.method == \'dqn\':\n        if parameters.nn_file is None:\n            network = q_network.DeepQLearner(defaults.RESIZED_WIDTH,\n                                             defaults.RESIZED_HEIGHT,\n                                             num_actions,\n                                             parameters.phi_length,\n                                             parameters.discount,\n                                             parameters.learning_rate,\n                                             parameters.rms_decay,\n                                             parameters.rms_epsilon,\n                                             parameters.momentum,\n                                             parameters.clip_delta,\n                                             parameters.freeze_interval,\n                                             parameters.batch_size,\n                                             parameters.network_type,\n                                             parameters.update_rule,\n                                             parameters.batch_accumulator,\n                                             rng, double=parameters.double_dqn)\n        else:\n            handle = open(parameters.nn_file, \'r\')\n            network = cPickle.load(handle)\n\n        agent = ale_agents.NeuralAgent(network,\n                                       parameters.epsilon_start,\n                                       parameters.epsilon_min,\n                                       parameters.epsilon_decay,\n                                       parameters.replay_memory_size,\n                                       parameters.experiment_prefix,\n                                       parameters.replay_start_size,\n                                       parameters.update_frequency,\n                                       rng)\n\n    if parameters.method == \'episodic_control\':\n            if parameters.qec_table is None:\n                qec_table = EC_functions.QECTable(parameters.knn,\n                                                  parameters.state_dimension,\n                                                  parameters.projection_type,\n                                                  defaults.RESIZED_WIDTH*defaults.RESIZED_HEIGHT,\n                                                  parameters.buffer_size,\n                                                  num_actions,\n                                                  rng,\n                                                  parameters.rebuild_knn_frequency)\n            else:\n                handle = open(parameters.qec_table, \'r\')\n                qec_table = cPickle.load(handle)\n\n            agent = ale_agents.EpisodicControl(qec_table,\n                                               parameters.ec_discount,\n                                               num_actions,\n                                               parameters.epsilon_start,\n                                               parameters.epsilon_min,\n                                               parameters.epsilon_decay,\n                                               parameters.experiment_prefix,\n                                               parameters.ec_testing,\n                                               rng)\n\n    experiment = ale_experiment.ALEExperiment(ale, agent,\n                                              defaults.RESIZED_WIDTH,\n                                              defaults.RESIZED_HEIGHT,\n                                              parameters.resize_method,\n                                              parameters.epochs,\n                                              parameters.steps_per_epoch,\n                                              parameters.steps_per_test,\n                                              parameters.frame_skip,\n                                              parameters.death_ends_episode,\n                                              parameters.max_start_nullops,\n                                              rng)\n\n    experiment.run()\n\nif __name__ == \'__main__\':\n    pass\n'"
dqn_ep/q_network.py,9,"b'""""""\nCode for deep Q-learning as described in:\n\nPlaying Atari with Deep Reinforcement Learning\nNIPS Deep Learning Workshop 2013\n\nand\n\nHuman-level control through deep reinforcement learning.\nNature, 518(7540):529-533, February 2015\n\n\nAuthor of Lasagne port: Nissan Pow\nModifications: Nathan Sprague\n""""""\nimport lasagne\nimport numpy as np\nimport theano\nimport theano.tensor as T\nfrom updates import deepmind_rmsprop\n\n\nclass DeepQLearner:\n    """"""\n    Deep Q-learning network using Lasagne.\n    """"""\n\n    def __init__(self, input_width, input_height, num_actions,\n                 num_frames, discount, learning_rate, rho,\n                 rms_epsilon, momentum, clip_delta, freeze_interval,\n                 batch_size, network_type, update_rule,\n                 batch_accumulator, rng, input_scale=255.0, use_ec=False, use_episodic_mem=False, double=False):\n\n        self.input_width = input_width\n        self.input_height = input_height\n        self.num_actions = num_actions\n        self.num_frames = num_frames\n        self.batch_size = batch_size\n        self.discount = discount\n        self.rho = rho\n        self.lr = learning_rate\n        self.rms_epsilon = rms_epsilon\n        self.momentum = momentum\n        self.clip_delta = clip_delta\n        self.freeze_interval = freeze_interval\n        self.rng = rng\n\n        lasagne.random.set_rng(self.rng)\n\n        self.update_counter = 0\n\n        self.l_out = self.build_network(network_type, input_width, input_height,\n                                        num_actions, num_frames, batch_size)\n        if self.freeze_interval > 0:\n            self.next_l_out = self.build_network(network_type, input_width,\n                                                 input_height, num_actions,\n                                                 num_frames, batch_size)\n            self.reset_q_hat()\n\n        states = T.tensor4(\'states\')\n        next_states = T.tensor4(\'next_states\')\n        rewards = T.col(\'rewards\')\n        actions = T.icol(\'actions\')\n        terminals = T.icol(\'terminals\')\n\n        evaluation = T.col(\'evaluation\')\n\n        # Shared variables for training from a minibatch of replayed\n        # state transitions, each consisting of num_frames + 1 (due to\n        # overlap) images, along with the chosen action and resulting\n        # reward and terminal status.\n        self.imgs_shared = theano.shared(\n            np.zeros((batch_size, num_frames + 1, input_height, input_width),\n                     dtype=theano.config.floatX))\n        self.rewards_shared = theano.shared(\n            np.zeros((batch_size, 1), dtype=theano.config.floatX),\n            broadcastable=(False, True))\n        self.actions_shared = theano.shared(\n            np.zeros((batch_size, 1), dtype=\'int32\'),\n            broadcastable=(False, True))\n        self.terminals_shared = theano.shared(\n            np.zeros((batch_size, 1), dtype=\'int32\'),\n            broadcastable=(False, True))\n\n        # shared variable for ec_dqn train\n        self.evaluation_shared = theano.shared(\n            np.zeros((batch_size, 1), dtype=theano.config.floatX),\n            broadcastable=(False, True))\n\n        # Shared variable for a single state, to calculate q_vals.\n        self.state_shared = theano.shared(\n            np.zeros((num_frames, input_height, input_width),\n                     dtype=theano.config.floatX))\n\n        q_vals = lasagne.layers.get_output(self.l_out, states / input_scale)\n        \n        if self.freeze_interval > 0:\n            next_q_vals = lasagne.layers.get_output(self.next_l_out,\n                                                    next_states / input_scale)\n        else:\n            next_q_vals = lasagne.layers.get_output(self.l_out,\n                                                    next_states / input_scale)\n            next_q_vals = theano.gradient.disconnected_grad(next_q_vals)\n\n        terminalsX = terminals.astype(theano.config.floatX)\n        actionmask = T.eq(T.arange(num_actions).reshape((1, -1)),\n                          actions.reshape((-1, 1))).astype(theano.config.floatX)\n        if not double:\n            target = (rewards + (T.ones_like(terminalsX) - terminalsX) *\n                      self.discount * T.max(next_q_vals, axis=1, keepdims=True))\n        else:\n            next_actions = T.argmax(next_q_vals, axis=1)  # batch*1\n            next_actionmask = T.eq(T.arange(num_actions).reshape((1, -1)),\n                                   next_actions.reshape((-1, 1))).astype(theano.config.floatX)\n            target = rewards + (T.ones_like(terminalsX) - terminalsX) * self.discount * \\\n                               (next_q_vals * next_actionmask).sum(axis=1).reshape((-1, 1))\n        target2 = target\n\n        if use_ec:\n            target2 = T.maximum(target, evaluation)\n            # mask1 = T.eq(evaluation, 0.0)\n            # mask2 = T.invert(mask1)\n            # target2 = target*mask1 + evaluation*mask2\n        if use_episodic_mem:\n            target2 = evaluation\n\n        output = (q_vals * actionmask).sum(axis=1).reshape((-1, 1))\n        diff = target2 - output\n\n        if self.clip_delta > 0:\n            # If we simply take the squared clipped diff as our loss,\n            # then the gradient will be zero whenever the diff exceeds\n            # the clip bounds. To avoid this, we extend the loss\n            # linearly past the clip point to keep the gradient constant\n            # in that regime.\n            # \n            # This is equivalent to declaring d loss/d q_vals to be\n            # equal to the clipped diff, then backpropagating from\n            # there, which is what the DeepMind implementation does.\n            quadratic_part = T.minimum(abs(diff), self.clip_delta)\n            linear_part = abs(diff) - quadratic_part\n            loss = 0.5 * quadratic_part ** 2 + self.clip_delta * linear_part\n        else:\n            loss = 0.5 * diff ** 2\n\n        if batch_accumulator == \'sum\':\n            loss = T.sum(loss)\n        elif batch_accumulator == \'mean\':\n            loss = T.mean(loss)\n        else:\n            raise ValueError(""Bad accumulator: {}"".format(batch_accumulator))\n\n        params = lasagne.layers.helper.get_all_params(self.l_out)  \n        train_givens = {\n            states: self.imgs_shared[:, :-1],\n            next_states: self.imgs_shared[:, 1:],\n            rewards: self.rewards_shared,\n            actions: self.actions_shared,\n            terminals: self.terminals_shared,\n            evaluation: self.evaluation_shared\n        }\n        if update_rule == \'deepmind_rmsprop\':\n            updates = deepmind_rmsprop(loss, params, self.lr, self.rho,\n                                       self.rms_epsilon)\n        elif update_rule == \'rmsprop\':\n            updates = lasagne.updates.rmsprop(loss, params, self.lr, self.rho,\n                                              self.rms_epsilon)\n        elif update_rule == \'sgd\':\n            updates = lasagne.updates.sgd(loss, params, self.lr)\n        else:\n            raise ValueError(""Unrecognized update: {}"".format(update_rule))\n\n        if self.momentum > 0:\n            updates = lasagne.updates.apply_momentum(updates, None,\n                                                     self.momentum)\n\n        self._train = theano.function([], [loss, target], updates=updates,\n                                      givens=train_givens, on_unused_input=\'warn\')\n        q_givens = {\n            states: self.state_shared.reshape((1,\n                                               self.num_frames,\n                                               self.input_height,\n                                               self.input_width))\n        }\n        self._q_vals = theano.function([], q_vals[0], givens=q_givens)\n\n    def build_network(self, network_type, input_width, input_height,\n                      output_dim, num_frames, batch_size):\n        if network_type == ""nature_cuda"":\n            return self.build_nature_network(input_width, input_height,\n                                             output_dim, num_frames, batch_size)\n        if network_type == ""nature_dnn"":\n            return self.build_nature_network_dnn(input_width, input_height,\n                                                 output_dim, num_frames,\n                                                 batch_size)\n        elif network_type == ""linear"":\n            return self.build_linear_network(input_width, input_height,\n                                             output_dim, num_frames, batch_size)\n        else:\n            raise ValueError(""Unrecognized network: {}"".format(network_type))\n\n    def train(self, imgs, actions, rewards, terminals, evaluation=None, get_target=False):\n        """"""\n        Train one batch.\n\n        Arguments:\n\n        imgs - b x (f + 1) x h x w numpy array, where b is batch size,\n               f is num frames, h is height and w is width.\n        actions - b x 1 numpy array of integers\n        rewards - b x 1 numpy array\n        terminals - b x 1 numpy boolean array (currently ignored)\n\n        Returns: average loss\n        """"""\n\n        self.imgs_shared.set_value(imgs)\n        self.actions_shared.set_value(actions)\n        self.rewards_shared.set_value(rewards)\n        self.terminals_shared.set_value(terminals)\n        if evaluation is not None:\n            self.evaluation_shared.set_value(evaluation)\n        if self.freeze_interval > 0 and self.update_counter % self.freeze_interval == 0:\n            self.reset_q_hat()\n        loss, target = self._train()\n        self.update_counter += 1\n        if get_target:\n            return np.sqrt(loss), target\n        return np.sqrt(loss)\n\n    def q_vals(self, state):\n        self.state_shared.set_value(state)\n        return self._q_vals()\n\n    def choose_action(self, state, epsilon):\n        if self.rng.rand() < epsilon:\n            return self.rng.randint(0, self.num_actions)\n        q_vals = self.q_vals(state)\n        return np.argmax(q_vals)\n\n    def reset_q_hat(self):\n        all_params = lasagne.layers.helper.get_all_param_values(self.l_out)\n        lasagne.layers.helper.set_all_param_values(self.next_l_out, all_params)\n\n    def build_nature_network(self, input_width, input_height, output_dim,\n                             num_frames, batch_size):\n        """"""\n        Build a large network consistent with the DeepMind Nature paper.\n        """"""\n        from lasagne.layers import cuda_convnet\n\n        l_in = lasagne.layers.InputLayer(\n            shape=(None, num_frames, input_width, input_height)\n        )\n\n        l_conv1 = cuda_convnet.Conv2DCCLayer(\n            l_in,\n            num_filters=32,\n            filter_size=(8, 8),\n            stride=(4, 4),\n            nonlinearity=lasagne.nonlinearities.rectify,\n            W=lasagne.init.HeUniform(), # Defaults to Glorot\n            b=lasagne.init.Constant(.1),\n            dimshuffle=True\n        )\n\n        l_conv2 = cuda_convnet.Conv2DCCLayer(\n            l_conv1,\n            num_filters=64,\n            filter_size=(4, 4),\n            stride=(2, 2),\n            nonlinearity=lasagne.nonlinearities.rectify,\n            W=lasagne.init.HeUniform(),\n            b=lasagne.init.Constant(.1),\n            dimshuffle=True\n        )\n\n        l_conv3 = cuda_convnet.Conv2DCCLayer(\n            l_conv2,\n            num_filters=64,\n            filter_size=(3, 3),\n            stride=(1, 1),\n            nonlinearity=lasagne.nonlinearities.rectify,\n            W=lasagne.init.HeUniform(),\n            b=lasagne.init.Constant(.1),\n            dimshuffle=True\n        )\n\n        l_hidden1 = lasagne.layers.DenseLayer(\n            l_conv3,\n            num_units=512,\n            nonlinearity=lasagne.nonlinearities.rectify,\n            W=lasagne.init.HeUniform(),\n            b=lasagne.init.Constant(.1)\n        )\n\n        l_out = lasagne.layers.DenseLayer(\n            l_hidden1,\n            num_units=output_dim,\n            nonlinearity=None,\n            W=lasagne.init.HeUniform(),\n            b=lasagne.init.Constant(.1)\n        )\n\n        return l_out\n\n\n    def build_nature_network_dnn(self, input_width, input_height, output_dim,\n                                 num_frames, batch_size):\n        """"""\n        Build a large network consistent with the DeepMind Nature paper.\n        """"""\n        from lasagne.layers import dnn\n\n        l_in = lasagne.layers.InputLayer(\n            shape=(None, num_frames, input_width, input_height)\n        )\n\n        l_conv1 = dnn.Conv2DDNNLayer(\n            l_in,\n            num_filters=32,\n            filter_size=(8, 8),\n            stride=(4, 4),\n            nonlinearity=lasagne.nonlinearities.rectify,\n            W=lasagne.init.HeUniform(),\n            b=lasagne.init.Constant(.1)\n        )\n\n        l_conv2 = dnn.Conv2DDNNLayer(\n            l_conv1,\n            num_filters=64,\n            filter_size=(4, 4),\n            stride=(2, 2),\n            nonlinearity=lasagne.nonlinearities.rectify,\n            W=lasagne.init.HeUniform(),\n            b=lasagne.init.Constant(.1)\n        )\n\n        l_conv3 = dnn.Conv2DDNNLayer(\n            l_conv2,\n            num_filters=64,\n            filter_size=(3, 3),\n            stride=(1, 1),\n            nonlinearity=lasagne.nonlinearities.rectify,\n            W=lasagne.init.HeUniform(),\n            b=lasagne.init.Constant(.1)\n        )\n\n        l_hidden1 = lasagne.layers.DenseLayer(\n            l_conv3,\n            num_units=512,\n            nonlinearity=lasagne.nonlinearities.rectify,\n            W=lasagne.init.HeUniform(),\n            b=lasagne.init.Constant(.1)\n        )\n\n        l_out = lasagne.layers.DenseLayer(\n            l_hidden1,\n            num_units=output_dim,\n            nonlinearity=None,\n            W=lasagne.init.HeUniform(),\n            b=lasagne.init.Constant(.1)\n        )\n\n        return l_out\n\n    def build_linear_network(self, input_width, input_height, output_dim,\n                             num_frames, batch_size):\n        """"""\n        Build a simple linear learner.  Useful for creating\n        tests that sanity-check the weight update code.\n        """"""\n\n        l_in = lasagne.layers.InputLayer(\n            shape=(None, num_frames, input_width, input_height)\n        )\n\n        l_out = lasagne.layers.DenseLayer(\n            l_in,\n            num_units=output_dim,\n            nonlinearity=None,\n            W=lasagne.init.Constant(0.0),\n            b=None\n        )\n\n        return l_out\n\ndef main():\n    net = DeepQLearner(84, 84, 16, 4, .99, .00025, .95, .95, 10000,\n                       32, \'nature_cuda\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
dqn_ep/run_DQN_episodic_memory.py,0,"b'#! /usr/bin/env python\n__author__ = \'frankhe\'\n""""""\nDQN episodic memory\n\n""""""\n\nimport launcher\nimport sys\n\n\nclass Defaults:\n    # ----------------------\n    # Experiment Parameters\n    # ----------------------\n    STEPS_PER_EPOCH = 250000\n    EPOCHS = 200\n    STEPS_PER_TEST = 125000\n\n    # ----------------------\n    # ALE Parameters\n    # ----------------------\n    BASE_ROM_PATH = ""../roms/""\n    ROM = \'ms_pacman.bin\'\n    FRAME_SKIP = 4\n    REPEAT_ACTION_PROBABILITY = 0\n\n    # ----------------------\n    # Agent/Network parameters:\n    # ----------------------\n    UPDATE_RULE = \'deepmind_rmsprop\'\n    BATCH_ACCUMULATOR = \'sum\'\n    LEARNING_RATE = .00025\n    DISCOUNT = .99\n    RMS_DECAY = .95 # (Rho)\n    RMS_EPSILON = .01\n    MOMENTUM = 0 # Note that the ""momentum"" value mentioned in the Nature\n                 # paper is not used in the same way as a traditional momentum\n                 # term.  It is used to track gradient for the purpose of\n                 # estimating the standard deviation. This package uses\n                 # rho/RMS_DECAY to track both the history of the gradient\n                 # and the squared gradient.\n    CLIP_DELTA = 1.0\n    EPSILON_START = 1.0\n    EPSILON_MIN = .005\n    EPSILON_DECAY = 100000\n    PHI_LENGTH = 4\n    UPDATE_FREQUENCY = 4\n    REPLAY_MEMORY_SIZE = 1000000\n    BATCH_SIZE = 32\n    NETWORK_TYPE = ""nature_dnn""\n    FREEZE_INTERVAL = 10000\n    REPLAY_START_SIZE = 50000\n    RESIZE_METHOD = \'scale\'\n    RESIZED_WIDTH = 84\n    RESIZED_HEIGHT = 84\n    DEATH_ENDS_EPISODE = \'true\'\n    MAX_START_NULLOPS = 30\n    DETERMINISTIC = True\n    CUDNN_DETERMINISTIC = False\n\n    METHOD = \'dqn_episodic_memory1\'\n    K_NEAREST_NEIGHBOR = 5\n    EC_DISCOUNT = 1.0\n    BUFFER_SIZE = 10000\n    REBUILD_KNN_FREQUENCY = 1000\n    DIMENSION_OF_STATE = 64\n    PROJECTION_TYPE = \'random\'  # or VAE\n    TESTING = False\n\nif __name__ == ""__main__"":\n    launcher.launch(sys.argv[1:], Defaults, __doc__)\n'"
dqn_ep/run_DQN_episodic_memory2.py,0,"b'#! /usr/bin/env python\n__author__ = \'frankhe\'\n""""""\nDQN episodic memory\n\n""""""\n\nimport launcher\nimport sys\n\n\nclass Defaults:\n    # ----------------------\n    # Experiment Parameters\n    # ----------------------\n    STEPS_PER_EPOCH = 250000\n    EPOCHS = 200\n    STEPS_PER_TEST = 125000\n\n    # ----------------------\n    # ALE Parameters\n    # ----------------------\n    BASE_ROM_PATH = ""../roms/""\n    ROM = \'ms_pacman.bin\'\n    FRAME_SKIP = 4\n    REPEAT_ACTION_PROBABILITY = 0\n\n    # ----------------------\n    # Agent/Network parameters:\n    # ----------------------\n    UPDATE_RULE = \'deepmind_rmsprop\'\n    BATCH_ACCUMULATOR = \'sum\'\n    LEARNING_RATE = .00025\n    DISCOUNT = .99\n    RMS_DECAY = .95 # (Rho)\n    RMS_EPSILON = .01\n    MOMENTUM = 0 # Note that the ""momentum"" value mentioned in the Nature\n                 # paper is not used in the same way as a traditional momentum\n                 # term.  It is used to track gradient for the purpose of\n                 # estimating the standard deviation. This package uses\n                 # rho/RMS_DECAY to track both the history of the gradient\n                 # and the squared gradient.\n    CLIP_DELTA = 1.0\n    EPSILON_START = 1.0\n    EPSILON_MIN = .005\n    EPSILON_DECAY = 100000\n    PHI_LENGTH = 4\n    UPDATE_FREQUENCY = 4\n    REPLAY_MEMORY_SIZE = 1000000\n    BATCH_SIZE = 32\n    NETWORK_TYPE = ""nature_dnn""\n    FREEZE_INTERVAL = 10000\n    REPLAY_START_SIZE = 50000\n    RESIZE_METHOD = \'scale\'\n    RESIZED_WIDTH = 84\n    RESIZED_HEIGHT = 84\n    DEATH_ENDS_EPISODE = \'true\'\n    MAX_START_NULLOPS = 30\n    DETERMINISTIC = True\n    CUDNN_DETERMINISTIC = False\n\n    METHOD = \'dqn_episodic_memory2\'\n    K_NEAREST_NEIGHBOR = 5\n    EC_DISCOUNT = 1.0\n    BUFFER_SIZE = 10000\n    REBUILD_KNN_FREQUENCY = 1000\n    DIMENSION_OF_STATE = 64\n    PROJECTION_TYPE = \'random\'  # or VAE\n    TESTING = False\n\nif __name__ == ""__main__"":\n    launcher.launch(sys.argv[1:], Defaults, __doc__)\n'"
dqn_ep/run_DQN_lsh.py,0,"b'#! /usr/bin/env python\n__author__ = \'frankhe\'\n""""""\nDQN episodic memory\n\n""""""\n\nimport launcher\nimport sys\n\n\nclass Defaults:\n    # ----------------------\n    # Experiment Parameters\n    # ----------------------\n    STEPS_PER_EPOCH = 250000\n    EPOCHS = 200\n    STEPS_PER_TEST = 125000\n\n    # ----------------------\n    # ALE Parameters\n    # ----------------------\n    BASE_ROM_PATH = ""../roms/""\n    ROM = \'ms_pacman.bin\'\n    FRAME_SKIP = 4\n    REPEAT_ACTION_PROBABILITY = 0\n\n    # ----------------------\n    # Agent/Network parameters:\n    # ----------------------\n    UPDATE_RULE = \'deepmind_rmsprop\'\n    BATCH_ACCUMULATOR = \'sum\'\n    LEARNING_RATE = .00025\n    DISCOUNT = .99\n    RMS_DECAY = .95 # (Rho)\n    RMS_EPSILON = .01\n    MOMENTUM = 0 # Note that the ""momentum"" value mentioned in the Nature\n                 # paper is not used in the same way as a traditional momentum\n                 # term.  It is used to track gradient for the purpose of\n                 # estimating the standard deviation. This package uses\n                 # rho/RMS_DECAY to track both the history of the gradient\n                 # and the squared gradient.\n    CLIP_DELTA = 1.0\n    EPSILON_START = 1.0\n    EPSILON_MIN = .005\n    EPSILON_DECAY = 100000\n    PHI_LENGTH = 4\n    UPDATE_FREQUENCY = 4\n    REPLAY_MEMORY_SIZE = 1000000\n    BATCH_SIZE = 32\n    NETWORK_TYPE = ""nature_dnn""\n    FREEZE_INTERVAL = 10000\n    REPLAY_START_SIZE = 50000\n    RESIZE_METHOD = \'scale\'\n    RESIZED_WIDTH = 84\n    RESIZED_HEIGHT = 84\n    DEATH_ENDS_EPISODE = \'true\'\n    MAX_START_NULLOPS = 30\n    DETERMINISTIC = True\n    CUDNN_DETERMINISTIC = False\n\n    METHOD = \'dqn_episodic_memory3\'\n    K_NEAREST_NEIGHBOR = 5\n    EC_DISCOUNT = 1.0\n    BUFFER_SIZE = 1000000\n    REBUILD_KNN_FREQUENCY = 1000\n    DIMENSION_OF_STATE = 64\n    PROJECTION_TYPE = \'random\'  # or VAE\n    TESTING = False\n\nif __name__ == ""__main__"":\n    launcher.launch(sys.argv[1:], Defaults, __doc__)\n'"
dqn_ep/run_ECDQN.py,0,"b'#! /usr/bin/env python\n__author__ = \'frankhe\'\n""""""\nExecute DQN or Episodic control\n\n""""""\n\nimport launcher\nimport sys\n\n\nclass Defaults:\n    # ----------------------\n    # Experiment Parameters\n    # ----------------------\n    STEPS_PER_EPOCH = 250000\n    EPOCHS = 200\n    STEPS_PER_TEST = 125000\n\n    # ----------------------\n    # ALE Parameters\n    # ----------------------\n    BASE_ROM_PATH = ""../roms/""\n    ROM = \'ms_pacman.bin\'\n    FRAME_SKIP = 4\n    REPEAT_ACTION_PROBABILITY = 0\n\n    # ----------------------\n    # Agent/Network parameters:\n    # ----------------------\n    UPDATE_RULE = \'deepmind_rmsprop\'\n    BATCH_ACCUMULATOR = \'sum\'\n    LEARNING_RATE = .00025\n    DISCOUNT = .99\n    RMS_DECAY = .95 # (Rho)\n    RMS_EPSILON = .01\n    MOMENTUM = 0 # Note that the ""momentum"" value mentioned in the Nature\n                 # paper is not used in the same way as a traditional momentum\n                 # term.  It is used to track gradient for the purpose of\n                 # estimating the standard deviation. This package uses\n                 # rho/RMS_DECAY to track both the history of the gradient\n                 # and the squared gradient.\n    CLIP_DELTA = 1.0\n    EPSILON_START = 1.0\n    EPSILON_MIN = .005\n    EPSILON_DECAY = 100000\n    PHI_LENGTH = 4\n    UPDATE_FREQUENCY = 4\n    REPLAY_MEMORY_SIZE = 1000000\n    BATCH_SIZE = 32\n    NETWORK_TYPE = ""nature_dnn""\n    FREEZE_INTERVAL = 10000\n    REPLAY_START_SIZE = 50000\n    RESIZE_METHOD = \'scale\'\n    RESIZED_WIDTH = 84\n    RESIZED_HEIGHT = 84\n    DEATH_ENDS_EPISODE = \'true\'\n    MAX_START_NULLOPS = 30\n    DETERMINISTIC = True\n    CUDNN_DETERMINISTIC = False\n\n    METHOD = \'ec_dqn\'\n    K_NEAREST_NEIGHBOR = 5\n    EC_DISCOUNT = 1.0\n    BUFFER_SIZE = 10000\n    REBUILD_KNN_FREQUENCY = 1000\n    DIMENSION_OF_STATE = 64\n    PROJECTION_TYPE = \'random\'  # or VAE\n    TESTING = False\n\nif __name__ == ""__main__"":\n    launcher.launch(sys.argv[1:], Defaults, __doc__)\n'"
dqn_ep/run_episodic_control.py,0,"b'#! /usr/bin/env python\n__author__ = \'frankhe\'\n\n""""""\nExecute DQN or Episodic control\n\n""""""\n\nimport launcher\nimport sys\n\n\nclass Defaults:\n    # ----------------------\n    # Experiment Parameters\n    # ----------------------\n    STEPS_PER_EPOCH = 10000\n    EPOCHS = 5000\n    STEPS_PER_TEST = 0\n\n    # ----------------------\n    # ALE Parameters\n    # ----------------------\n    BASE_ROM_PATH = ""../roms/""\n    ROM = \'ms_pacman.bin\'\n    FRAME_SKIP = 4\n    REPEAT_ACTION_PROBABILITY = 0\n\n    # ----------------------\n    # Agent/Network parameters:\n    # ----------------------\n    UPDATE_RULE = \'deepmind_rmsprop\'\n    BATCH_ACCUMULATOR = \'sum\'\n    LEARNING_RATE = .00025\n    DISCOUNT = .99\n    RMS_DECAY = .95 # (Rho)\n    RMS_EPSILON = .01\n    MOMENTUM = 0 # Note that the ""momentum"" value mentioned in the Nature\n                 # paper is not used in the same way as a traditional momentum\n                 # term.  It is used to track gradient for the purpose of\n                 # estimating the standard deviation. This package uses\n                 # rho/RMS_DECAY to track both the history of the gradient\n                 # and the squared gradient.\n    CLIP_DELTA = 1.0\n    EPSILON_START = 1.0\n    EPSILON_MIN = .005\n    EPSILON_DECAY = 10000\n    PHI_LENGTH = 4\n    UPDATE_FREQUENCY = 4\n    REPLAY_MEMORY_SIZE = 1000000\n    BATCH_SIZE = 32\n    NETWORK_TYPE = ""nature_dnn""\n    FREEZE_INTERVAL = 10000\n    REPLAY_START_SIZE = 50000\n    RESIZE_METHOD = \'scale\'\n    RESIZED_WIDTH = 84\n    RESIZED_HEIGHT = 84\n    DEATH_ENDS_EPISODE = \'true\'\n    MAX_START_NULLOPS = 30\n    DETERMINISTIC = True\n    CUDNN_DETERMINISTIC = False\n\n    METHOD = \'episodic_control\'\n    K_NEAREST_NEIGHBOR = 11\n    EC_DISCOUNT = 1.0\n    BUFFER_SIZE = 1000000\n    REBUILD_KNN_FREQUENCY = 10000\n    DIMENSION_OF_STATE = 64\n    PROJECTION_TYPE = \'random\'  # or VAE\n    TESTING = False\n\nif __name__ == ""__main__"":\n    launcher.launch(sys.argv[1:], Defaults, __doc__)\n'"
dqn_ep/run_nature.py,0,"b'#! /usr/bin/env python\n""""""\nExecute DQN or Episodic control\n\n""""""\n\nimport launcher\nimport sys\n\n\nclass Defaults:\n    # ----------------------\n    # Experiment Parameters\n    # ----------------------\n    STEPS_PER_EPOCH = 250000\n    EPOCHS = 200\n    STEPS_PER_TEST = 125000\n\n    # ----------------------\n    # ALE Parameters\n    # ----------------------\n    BASE_ROM_PATH = ""../roms/""\n    ROM = \'ms_pacman.bin\'\n    FRAME_SKIP = 4\n    REPEAT_ACTION_PROBABILITY = 0\n\n    # ----------------------\n    # Agent/Network parameters:\n    # ----------------------\n    UPDATE_RULE = \'deepmind_rmsprop\'\n    BATCH_ACCUMULATOR = \'sum\'\n    LEARNING_RATE = .00025\n    DISCOUNT = .99\n    RMS_DECAY = .95 # (Rho)\n    RMS_EPSILON = .01\n    MOMENTUM = 0 # Note that the ""momentum"" value mentioned in the Nature\n                 # paper is not used in the same way as a traditional momentum\n                 # term.  It is used to track gradient for the purpose of\n                 # estimating the standard deviation. This package uses\n                 # rho/RMS_DECAY to track both the history of the gradient\n                 # and the squared gradient.\n    CLIP_DELTA = 1.0\n    EPSILON_START = 1.0\n    EPSILON_MIN = .1\n    EPSILON_DECAY = 1000000\n    PHI_LENGTH = 4\n    UPDATE_FREQUENCY = 4\n    REPLAY_MEMORY_SIZE = 1000000\n    BATCH_SIZE = 32\n    NETWORK_TYPE = ""nature_dnn""\n    FREEZE_INTERVAL = 10000\n    REPLAY_START_SIZE = 50000\n    RESIZE_METHOD = \'scale\'\n    RESIZED_WIDTH = 84\n    RESIZED_HEIGHT = 84\n    DEATH_ENDS_EPISODE = \'true\'\n    MAX_START_NULLOPS = 30\n    DETERMINISTIC = True\n    CUDNN_DETERMINISTIC = False\n\n    METHOD = \'dqn\'\n    K_NEAREST_NEIGHBOR = 11\n    REBUILD_KNN_FREQUENCY = 1000\n    EC_DISCOUNT = 1.0\n    BUFFER_SIZE = 1000000\n    DIMENSION_OF_STATE = 64\n    PROJECTION_TYPE = \'random\'  # or VAE\n    TESTING = False\n\nif __name__ == ""__main__"":\n    launcher.launch(sys.argv[1:], Defaults, __doc__)\n'"
dqn_ep/updates.py,2,"b'""""""\nGradient update rules for the deep_q_rl package. \n\nSome code here is modified from the Lasagne package:\n \nhttps://github.com/Lasagne/Lasagne/blob/master/LICENSE\n\n""""""\n\nimport theano\nimport theano.tensor as T\nfrom lasagne.updates import get_or_compute_grads\nfrom collections import OrderedDict\nimport numpy as np\n\n# The MIT License (MIT)\n\n# Copyright (c) 2014 Sander Dieleman\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n# The MIT License (MIT)\n\n# Copyright (c) 2014 Sander Dieleman\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\ndef deepmind_rmsprop(loss_or_grads, params, learning_rate, \n                     rho, epsilon):\n    """"""RMSProp updates [1]_.\n\n    Scale learning rates by dividing with the moving average of the root mean\n    squared (RMS) gradients.\n\n    Parameters\n    ----------\n    loss_or_grads : symbolic expression or list of expressions\n        A scalar loss expression, or a list of gradient expressions\n    params : list of shared variables\n        The variables to generate update expressions for\n    learning_rate : float or symbolic scalar\n        The learning rate controlling the size of update steps\n    rho : float or symbolic scalar\n        Gradient moving average decay factor\n    epsilon : float or symbolic scalar\n        Small value added for numerical stability\n\n    Returns\n    -------\n    OrderedDict\n        A dictionary mapping each parameter to its update expression\n\n    Notes\n    -----\n    `rho` should be between 0 and 1. A value of `rho` close to 1 will decay the\n    moving average slowly and a value close to 0 will decay the moving average\n    fast.\n\n    Using the step size :math:`\\\\eta` and a decay factor :math:`\\\\rho` the\n    learning rate :math:`\\\\eta_t` is calculated as:\n\n    .. math::\n       r_t &= \\\\rho r_{t-1} + (1-\\\\rho)*g^2\\\\\\\\\n       \\\\eta_t &= \\\\frac{\\\\eta}{\\\\sqrt{r_t + \\\\epsilon}}\n\n    References\n    ----------\n    .. [1] Tieleman, T. and Hinton, G. (2012):\n           Neural Networks for Machine Learning, Lecture 6.5 - rmsprop.\n           Coursera. http://www.youtube.com/watch?v=O3sxAc4hxZU (formula @5:20)\n    """"""\n\n    grads = get_or_compute_grads(loss_or_grads, params)\n    updates = OrderedDict()\n\n    for param, grad in zip(params, grads):\n        value = param.get_value(borrow=True)\n\n        acc_grad = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n                             broadcastable=param.broadcastable)\n        acc_grad_new = rho * acc_grad + (1 - rho) * grad\n\n        acc_rms = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n                             broadcastable=param.broadcastable)\n        acc_rms_new = rho * acc_rms + (1 - rho) * grad ** 2\n\n\n        updates[acc_grad] = acc_grad_new\n        updates[acc_rms] = acc_rms_new\n\n        updates[param] = (param - learning_rate * \n                          (grad / \n                           T.sqrt(acc_rms_new - acc_grad_new **2 + epsilon)))\n\n    return updates\n'"
dqn_ep/watch_episodic_control.py,0,"b'#! /usr/bin/env python\n__author__ = \'frankhe\'\n\n""""""\nwatch Episodic control\nrunning:\npython watch_episodic_control.py --qec-table qec_table_file.pkl\n\n""""""\n\nimport launcher\nimport sys\n\n\nclass Defaults:\n    # ----------------------\n    # Experiment Parameters\n    # ----------------------\n    STEPS_PER_EPOCH = 50\n    EPOCHS = 1\n    STEPS_PER_TEST = 0\n\n    # ----------------------\n    # ALE Parameters\n    # ----------------------\n    BASE_ROM_PATH = ""../roms/""\n    ROM = \'ms_pacman.bin\'\n    FRAME_SKIP = 4\n    REPEAT_ACTION_PROBABILITY = 0\n\n    # ----------------------\n    # Agent/Network parameters:\n    # ----------------------\n    UPDATE_RULE = \'deepmind_rmsprop\'\n    BATCH_ACCUMULATOR = \'sum\'\n    LEARNING_RATE = .00025\n    DISCOUNT = .99\n    RMS_DECAY = .95 # (Rho)\n    RMS_EPSILON = .01\n    MOMENTUM = 0 # Note that the ""momentum"" value mentioned in the Nature\n                 # paper is not used in the same way as a traditional momentum\n                 # term.  It is used to track gradient for the purpose of\n                 # estimating the standard deviation. This package uses\n                 # rho/RMS_DECAY to track both the history of the gradient\n                 # and the squared gradient.\n    CLIP_DELTA = 1.0\n    EPSILON_START = .005\n    EPSILON_MIN = .005\n    EPSILON_DECAY = 0\n    PHI_LENGTH = 4\n    UPDATE_FREQUENCY = 4\n    REPLAY_MEMORY_SIZE = 1000000\n    BATCH_SIZE = 32\n    NETWORK_TYPE = ""nature_dnn""\n    FREEZE_INTERVAL = 10000\n    REPLAY_START_SIZE = 50000\n    RESIZE_METHOD = \'scale\'\n    RESIZED_WIDTH = 84\n    RESIZED_HEIGHT = 84\n    DEATH_ENDS_EPISODE = \'true\'\n    MAX_START_NULLOPS = 30\n    DETERMINISTIC = True\n    CUDNN_DETERMINISTIC = False\n\n    DQN = False\n    EPISODIC_CONTROL = True\n    K_NEAREST_NEIGHBOR = 11\n    EC_DISCOUNT = 1.0\n    BUFFER_SIZE = 1000000\n    DIMENSION_OF_STATE = 64\n    PROJECTION_TYPE = \'random\'  # or VAE\n    TESTING = True\n\nif __name__ == ""__main__"":\n    launcher.launch(sys.argv[1:], Defaults, __doc__)\n'"
expermients/KNN_runtime_test.py,10,"b'__author__ = \'frankhe\'\nimport numpy as np\nimport cPickle\nimport heapq\nimport time\nfrom sklearn.neighbors import KDTree, BallTree\nimport kdtree\nfrom annoy import AnnoyIndex\nfrom pyflann import FLANN\ndata_num = 100000\ntest_num_for_each = 20\n\ndimension_result = 64\ndimension_observation = 84*84\nK = 11\n\n\ndef create_data():\n    f1 = open(\'states.pkl\', \'w\')\n    f2 = open(\'states_for_test.pkl\', \'w\')\n    matrix_projection = np.random.normal(\n        loc=0.0, scale=1.0/np.sqrt(dimension_result), size=(dimension_result, dimension_observation))\n    num = data_num\n    data = np.zeros((num, dimension_result))\n    for i in xrange(num):\n        print ""creating data"", i\n        data[i] = np.dot(matrix_projection, np.random.randint(0, 255, dimension_observation))\n    data_states = data\n\n    test = np.zeros((num, test_num_for_each, dimension_result))\n    for i in xrange(num):\n        print ""creating test"", i\n        for j in xrange(test_num_for_each):\n            test[i, j] = np.dot(matrix_projection, np.random.randint(0, 255, dimension_observation))\n    test_states = test\n\n    cPickle.dump(data_states, f1, 2)\n    cPickle.dump(test_states, f2, 2)\n    f1.close()\n    f2.close()\n\n\nclass DistanceNode(object):\n    def __init__(self, distance, index):\n        self.distance = distance\n        self.index = index\n\n\ndef make_test(test_start=1000, test_end=1050):\n    f1 = open(\'states.pkl\', \'r\')\n    f2 = open(\'states_for_test.pkl\', \'r\')\n    data_states = cPickle.load(f1)\n    test_states = cPickle.load(f2)\n    f1.close()\n    f2.close()\n\n    time_brute = []\n    time_sk_kd = []\n    time_sk_ball = []\n    time_kdtree = []\n    time_annoy = []\n    time_flann = []\n    time_brute_tot = time_sk_kd_tot = time_sk_ball_tot = time_kdtree_tot = time_annoy_tot = time_flann_tot = 0\n\n    kdtree_tree = None\n    for items in xrange(test_start, test_end):\n        print ""item:"", items\n\n        ground_truth = np.zeros((test_num_for_each, K), dtype=np.int32)\n        time_brute_start = time.time()\n        for no_test in xrange(test_num_for_each):\n            distance_list = []\n            current_state = test_states[items, no_test]\n            for target in xrange(items):\n                target_state = data_states[target]\n                distance_list.append(DistanceNode(np.sum(np.absolute(current_state - target_state)**2), target))\n            smallest = heapq.nsmallest(K, distance_list, key=lambda x: x.distance)\n            ground_truth[no_test] = [x.index for x in smallest]\n        time_brute_end = time.time()\n        time_brute.append(time_brute_end - time_brute_start)\n        time_brute_tot += time_brute[-1]\n        # print ground_truth\n\n        time_sk_kd_start = time.time()\n        tree = KDTree(data_states[:items, :])\n        dist, indices = tree.query(test_states[items], K)\n        time_sk_kd_end = time.time()\n        time_sk_kd.append(time_sk_kd_end - time_sk_kd_start)\n        time_sk_kd_tot += time_sk_kd[-1]\n        # print indices\n\n        time_sk_ball_start = time.time()\n        tree = BallTree(data_states[:items, :], 10000)\n        dist, indices = tree.query(test_states[items], K)\n        time_sk_ball_end = time.time()\n        time_sk_ball.append(time_sk_ball_end - time_sk_ball_start)\n        time_sk_ball_tot += time_sk_ball[-1]\n        # print indices\n\n        """"""\n        annoy is absolutely disappointing for its low speed and poor accuracy.\n        """"""\n        time_annoy_start = time.time()\n        annoy_result = np.zeros((test_num_for_each, K), dtype=np.int32)\n        tree = AnnoyIndex(dimension_result)\n        for i in xrange(items):\n            tree.add_item(i, data_states[i, :])\n        tree.build(10)\n        for no_test in xrange(test_num_for_each):\n            current_state = test_states[items, no_test]\n            annoy_result[no_test] = tree.get_nns_by_vector(current_state, K)\n        time_annoy_end = time.time()\n        time_annoy.append(time_annoy_end - time_annoy_start)\n        time_annoy_tot += time_annoy[-1]\n        # print annoy_result\n        # print annoy_result - indices\n\n        """"""\n        flann is still not very ideal\n        """"""\n\n        time_flann_start = time.time()\n        flann = FLANN()\n        result, dist = flann.nn(data_states[:items, :], test_states[items], K, algorithm=\'kdtree\', trees=10, checks=16)\n        time_flann_end = time.time()\n        time_flann.append(time_flann_end - time_flann_start)\n        time_flann_tot += time_flann[-1]\n        # print result-indices\n\n        """"""\n        This kdtree module is so disappointing!!!! It is 100 times slower than Sklearn and even slower than brute force,\n        more over it even makes mistakes.\n\n        This kdtree module supports online insertion and deletion. I thought it would be much faster than Sklearn\n         KdTree which rebuilds the tree every time. But the truth is the opposite.\n        """"""\n\n        # time_kdtree_start = time.time()\n        # if kdtree_tree is None:\n        #     point_list = [MyTuple(data_states[i, :], i) for i in xrange(items)]\n        #     kdtree_tree = kdtree.create(point_list)\n        # else:\n        #     point = MyTuple(data_states[items, :], items)\n        #     kdtree_tree.add(point)\n        # kdtree_result = np.zeros((test_num_for_each, K), dtype=np.int32)\n        # for no_test in xrange(test_num_for_each):\n        #     current_state = test_states[items, no_test]\n        #     smallest = kdtree_tree.search_knn(MyTuple(current_state, -1), K)\n        #     kdtree_result[no_test] = [x[0].data.pos for x in smallest]\n        # time_kdtree_end = time.time()\n        # time_kdtree.append(time_kdtree_end - time_kdtree_start)\n        # time_kdtree_tot += time_kdtree[-1]\n        # print kdtree_result\n        # print kdtree_result-indices\n\n    print \'brute force:\', time_brute_tot\n    print \'sklearn KDTree\', time_sk_kd_tot\n    print \'sklearn BallTree\', time_sk_ball_tot\n    print \'approximate annoy\', time_annoy_tot\n    print \'approximate flann\', time_flann_tot\n    print \'kdtree (deprecated)\', time_kdtree_tot\n\n\nclass MyTuple(tuple):\n    def __new__(cls, data, pos):\n        return super(MyTuple, cls).__new__(cls, data)\n\n    def __init__(self, data, pos):\n        self.pos = pos\n\nif __name__ == \'__main__\':\n    # create_data()\n    make_test()\n\n\n\n\n\n\n'"
expermients/__init__.py,0,"b""__author__ = 'frankhe'\n"""
