file_path,api_count,code
rnn.py,19,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\n\nclass RNN(object):\n\n    def __init__(self, input_dim, hidden_dim, output_dim, depth, lr=0.002):\n        self.lr = lr\n        self.depth = depth\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.U = xavier_init(input_dim, hidden_dim, fc=True)\n        self.W = xavier_init(hidden_dim, hidden_dim, fc=True)\n        self.V = xavier_init(hidden_dim, output_dim, fc=True)\n        # tmp variable\n        self.x = None\n        self.H = None\n        self.Alpha = None\n\n    def forward_prop(self, x):\n        batch_size = x.shape[1]\n        self.x = x\n        self.H = np.zeros((self.depth, batch_size, self.hidden_dim))\n        self.Alpha = np.zeros((self.depth, batch_size, self.hidden_dim))\n        h_prev = np.zeros((batch_size, self.hidden_dim))\n        sigmoid_output = np.zeros((self.depth, batch_size, self.output_dim))\n        for t in range(self.depth):\n            self.Alpha[t] = self.x[t]@self.U + h_prev@self.W\n            self.H[t] = self.relu(self.Alpha[t])\n            o_t = self.H[t]@self.V\n            y_t = self.sigmoid(o_t)\n            sigmoid_output[t] = y_t\n            h_prev = self.H[t]\n        return sigmoid_output\n\n    def backward_prop(self, sigmoid_output, output_label):\n        batch_size = output_label.shape[1]\n        dU = np.zeros(self.U.shape)\n        dW = np.zeros(self.W.shape)\n        dV = np.zeros(self.V.shape)\n        dH_t_front = np.zeros((batch_size, self.hidden_dim))\n        for t in range(self.depth-1, 0, -1):\n            dY_t = sigmoid_output[t] - output_label[t]\n            dO_t = dY_t * sigmoid_output[t] * (1 - sigmoid_output[t])\n            dV += self.H[t].T @ dO_t\n            dH_t = dO_t @ self.V.T + dH_t_front\n            dAlpha_t = self.relu(self.Alpha[t], dH_t, deriv=True)\n            dU += self.x[t].T @ dAlpha_t\n            if t > 0:\n                dW += self.H[t-1].T @ dAlpha_t\n            dH_t_front = dAlpha_t @ self.W.T\n        self.U -= self.lr * dU\n        self.W -= self.lr * dW\n        self.V -= self.lr * dV\n\n    def relu(self, x, front_delta=None, deriv=False):\n        if deriv == False:\n            return x * (x > 0)\n        else:\n            back_delta = front_delta * 1. * (x > 0)\n            return back_delta\n\n    def sigmoid(self, x):\n        return np.where(x >= 0,\n                        1 / (1 + np.exp(-x)),\n                        np.exp(x) / (1 + np.exp(x)))\n\n\ndef xavier_init(c1, c2, w=1, h=1, fc=False):\n    fan_1 = c2 * w * h\n    fan_2 = c1 * w * h\n    ratio = np.sqrt(6.0 / (fan_1 + fan_2))\n    params = ratio * (2 * np.random.random((c1, c2, w, h)) - 1)\n    if fc:\n        params = params.reshape(c1, c2)\n    return params\n\n\n# The X of this dataset is @data_size number of floats lies within (0, 1)\n# The Y of this dataset is whether the current prefix sum of X has exceed the value @data_size/2.\n# X (length, data_size, 1)\n# Y (length, data_size, 1)\ndef generate_dataset(data_size, length, split_ratio):\n    X = np.random.uniform(0, 1, (data_size, length, 1))\n    Y = np.zeros((data_size, length, 1))\n    threshold = length / 2.\n    for i in range(data_size):\n        prefix_sum = 0\n        for j in range(length):\n            prefix_sum += X[i][j][0]\n            Y[i][j][0] = int(prefix_sum > threshold)\n    split_point = int(data_size * split_ratio)\n    train_x, test_x = X[:split_point], X[split_point:]\n    train_y, test_y = Y[:split_point], Y[split_point:]\n    return np.swapaxes(train_x, 0, 1), np.swapaxes(test_x, 0, 1), \\\n           np.swapaxes(train_y, 0, 1), np.swapaxes(test_y, 0, 1)\n\n\ndef main():\n    length = 12\n    data_size = 1000\n    split_ratio = 0.9\n    max_iter = 100\n    iters_before_test = 10\n    batch_size = 25\n    train_x, test_x, train_y, test_y = generate_dataset(data_size, length, split_ratio)\n    rnn = RNN(1, 10, 1, length)\n    for iters in range(max_iter+1):\n        st_idx = int(iters % ((split_ratio * length) / batch_size))\n        ed_idx = int(st_idx + batch_size)\n        sigmoid_output = rnn.forward_prop(train_x[:, st_idx:ed_idx, :])\n        rnn.backward_prop(sigmoid_output, train_y[:, st_idx:ed_idx, :])\n        loss = np.sum((sigmoid_output - train_y[:, st_idx:ed_idx, :]) ** 2)\n        print(""The loss on training data is %f"" % loss)\n        if iters % iters_before_test == 0:\n            sigmoid_output = rnn.forward_prop(test_x)\n            predict_label = sigmoid_output > 0.5\n            accuracy = float(np.sum(predict_label == test_y.astype(bool))) / test_y.size\n            print(""The accuracy on testing data is %f"" % accuracy)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
