file_path,api_count,code
01_mysteries_of_neural_networks/04_optimizers/fully_connected_nn.py,14,"b'# File:         fully_connected_nn.py\n# Version:      1.0\n# Author:       SkalskiP https://github.com/SkalskiP\n# Date:         31.10.2018\n# Description:  The file contains a simple implementation of a fully connected neural network.\n#               The original implementation can be found in the Medium article:\n#               https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n\nimport numpy as np\n\ndef sigmoid(Z):\n    return 1/(1+np.exp(-Z))\n\ndef relu(Z):\n    return np.maximum(0,Z)\n\ndef sigmoid_backward(dA, Z):\n    sig = sigmoid(Z)\n    return dA * sig * (1 - sig)\n\ndef relu_backward(dA, Z):\n    dZ = np.array(dA, copy = True)\n    dZ[Z <= 0] = 0\n    return dZ\n\ndef init_layers(nn_architecture, seed = 99):\n    np.random.seed(seed)\n    number_of_layers = len(nn_architecture)\n    params_values = {}\n    \n    for idx, layer in enumerate(nn_architecture):\n        layer_idx = idx + 1\n        \n        layer_input_size = layer[""input_dim""]\n        layer_output_size = layer[""output_dim""]\n        \n        params_values[\'W\' + str(layer_idx)] = np.random.randn(\n            layer_output_size, layer_input_size) * 0.1\n        params_values[\'b\' + str(layer_idx)] = np.random.randn(\n            layer_output_size, 1) * 0.1\n        \n    return params_values\n\n\ndef single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=""relu""):\n    Z_curr = np.dot(W_curr, A_prev) + b_curr\n    \n    if activation is ""relu"":\n        activation_func = relu\n    elif activation is ""sigmoid"":\n        activation_func = sigmoid\n    else:\n        raise Exception(\'Non-supported activation function\')\n        \n    return activation_func(Z_curr), Z_curr\n\n\ndef full_forward_propagation(X, params_values, nn_architecture):\n    memory = {}\n    A_curr = X\n    \n    for idx, layer in enumerate(nn_architecture):\n        layer_idx = idx + 1\n        A_prev = A_curr\n        \n        activ_function_curr = layer[""activation""]\n        W_curr = params_values[""W"" + str(layer_idx)]\n        b_curr = params_values[""b"" + str(layer_idx)]\n        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n        \n        memory[""A"" + str(idx)] = A_prev\n        memory[""Z"" + str(layer_idx)] = Z_curr\n       \n    return A_curr, memory\n\n\ndef get_cost_value(Y_hat, Y, eps = 0.001):\n    m = Y_hat.shape[1]\n    cost = -1 / m * (np.dot(Y, np.log(Y_hat + eps).T) + np.dot(1 - Y, np.log(1 - Y_hat  + eps).T))\n    return np.squeeze(cost)\n\n\ndef convert_prob_into_class(probs):\n    probs_ = np.copy(probs)\n    probs_[probs_ > 0.5] = 1\n    probs_[probs_ <= 0.5] = 0\n    return probs_\n\n\ndef get_accuracy_value(Y_hat, Y):\n    Y_hat_ = convert_prob_into_class(Y_hat)\n    return (Y_hat_ == Y).all(axis=0).mean()\n\n\ndef single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=""relu""):\n    m = A_prev.shape[1]\n    \n    if activation is ""relu"":\n        backward_activation_func = relu_backward\n    elif activation is ""sigmoid"":\n        backward_activation_func = sigmoid_backward\n    else:\n        raise Exception(\'Non-supported activation function\')\n    \n    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n    dA_prev = np.dot(W_curr.T, dZ_curr)\n\n    return dA_prev, dW_curr, db_curr\n\n\ndef full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture, eps = 0.000000000001):\n    grads_values = {}\n    m = Y.shape[1]\n    Y = Y.reshape(Y_hat.shape)\n    \n    dA_prev = - (np.divide(Y, Y_hat + eps) - np.divide(1 - Y, 1 - Y_hat + eps))\n    \n    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n        layer_idx_curr = layer_idx_prev + 1\n        activ_function_curr = layer[""activation""]\n        \n        dA_curr = dA_prev\n        \n        A_prev = memory[""A"" + str(layer_idx_prev)]\n        Z_curr = memory[""Z"" + str(layer_idx_curr)]\n        \n        W_curr = params_values[""W"" + str(layer_idx_curr)]\n        b_curr = params_values[""b"" + str(layer_idx_curr)]\n        \n        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n        \n        grads_values[""dW"" + str(layer_idx_curr)] = dW_curr\n        grads_values[""db"" + str(layer_idx_curr)] = db_curr\n    \n    return grads_values\n\n\ndef update(params_values, grads_values, nn_architecture, learning_rate):\n\n    for layer_idx, layer in enumerate(nn_architecture, 1):\n        params_values[""W"" + str(layer_idx)] -= learning_rate * grads_values[""dW"" + str(layer_idx)]        \n        params_values[""b"" + str(layer_idx)] -= learning_rate * grads_values[""db"" + str(layer_idx)]\n\n    return params_values\n\n\ndef train(X, Y, nn_architecture, epochs, learning_rate, verbose=False, callback=None):\n    params_values = init_layers(nn_architecture, 2)\n    cost_history = []\n    accuracy_history = []\n    \n    for i in range(epochs):\n        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n        \n        cost = get_cost_value(Y_hat, Y)\n        cost_history.append(cost)\n        accuracy = get_accuracy_value(Y_hat, Y)\n        accuracy_history.append(accuracy)\n        \n        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n        \n        if(i % 50 == 0):\n            if(verbose):\n                print(""Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}"".format(i, cost, accuracy))\n            if(callback is not None):\n                callback(i, params_values)\n            \n    return params_values, cost_history, accuracy_history'"
02_data_science_toolkit/03_explaining_image_classifier_predictions/plot_utils.py,0,"b'import matplotlib.pyplot as plt\n\nclass PlotUtil:\n\n    @staticmethod\n    def plot_grid(images, img_shape, grid_size, cls_true=None, cls_pred=None, save_path=None):\n        fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(15, 15))\n        fig.subplots_adjust(hspace= 0.9 / grid_size[0], wspace= 0.9 / grid_size[1])\n\n        for i, ax in enumerate(axes.flat):\n            normalized_image = (images[i] - images[i].min())/(images[i].max() - images[i].min())\n            ax.imshow(normalized_image.reshape(img_shape), interpolation=\'nearest\')\n            \n            titles = []\n            if cls_true is not None:\n                titles.append(""True: {0}"".format(cls_true[i]))\n            if cls_pred is not None:\n                titles.append(""Pred: {0}"".format(cls_pred[i]))\n\n            ax.set_title("" "".join(titles))\n            ax.axis(\'off\')\n            \n        if save_path:\n            plt.savefig(save_path, bbox_inches=\'tight\')\n            \n        plt.show()\n\n    @staticmethod\n    def plot_3x3_grid(images, img_shape, cls_true=None, cls_pred=None, save_path=None):\n        PlotUtil.plot_grid(images, img_shape, (3, 3), cls_true=cls_true, cls_pred=cls_pred, save_path=save_path)\n        \n    @staticmethod\n    def plot_5x5_grid(images, img_shape, cls_true=None, cls_pred=None, save_path=None):\n        PlotUtil.plot_grid(images, img_shape, (5, 5), cls_true=cls_true, cls_pred=cls_pred, save_path=save_path)\n        \n    @staticmethod\n    def plot_7x7_grid(images, img_shape, cls_true=None, cls_pred=None, save_path=None):\n        PlotUtil.plot_grid(images, img_shape, (7, 7), cls_true=cls_true, cls_pred=cls_pred, save_path=save_path)'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/__init__.py,0,b''
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/base.py,5,"b'from abc import ABC, abstractmethod\nfrom typing import Optional, Tuple, List\n\nimport numpy as np\n\n\nclass Layer(ABC):\n\n    @property\n    def weights(self) -> Optional[Tuple[np.array, np.array]]:\n        """"""\n        Returns weights tensor if layer is trainable.\n        Returns None for non-trainable layers.\n        """"""\n        return None\n\n    @property\n    def gradients(self) -> Optional[Tuple[np.array, np.array]]:\n        """"""\n        Returns bias tensor if layer is trainable.\n        Returns None for non-trainable layers.\n        """"""\n        return None\n\n    @abstractmethod\n    def forward_pass(self, a_prev: np.array, training: bool) -> np.array:\n        """"""\n        Perform layer forward propagation logic.\n        """"""\n        pass\n\n    @abstractmethod\n    def backward_pass(self, da_curr: np.array) -> np.array:\n        pass\n\n    def set_wights(self, w: np.array, b: np.array) -> None:\n        """"""\n        Perform layer backward propagation logic.\n        """"""\n        pass\n\n\nclass Optimizer(ABC):\n\n    @abstractmethod\n    def update(self, layers: List[Layer]) -> None:\n        """"""\n        Updates value of weights and bias tensors in trainable layers.\n        """"""\n        pass\n\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/errors.py,0,b'class InvalidPaddingModeError(Exception):\n    pass\n'
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/tests/__init__.py,0,b''
02_data_science_toolkit/03_explaining_image_classifier_predictions/01_coco_res_net/coco_loader.py,1,"b'from skimage.io import imread\nfrom skimage.transform import resize\nimport numpy as np\n\nclass CocoLoader:\n\n    IMG_WIDTH = 224\n    IMG_HEIGHT = 224\n    IMG_CHANNEL = 3\n\n    def preprocess_image(self, path: str):\n        x = imread(path)\n        x = resize(x, (CocoLoader.IMG_WIDTH, CocoLoader.IMG_HEIGHT)) * 255\n        return x\n\n    def load_sample(self, size: int):\n        with open(""./dataset/5k.txt"", ""r"") as f:\n            images_paths = f.read().split(\'\\n\')\n            dataset = np.ndarray(shape=(size, CocoLoader.IMG_WIDTH, CocoLoader.IMG_HEIGHT, CocoLoader.IMG_CHANNEL), dtype=np.float32)\n            for index, images_path in enumerate(images_paths[:size]):\n                dataset[index] = self.preprocess_image(images_path)\n        return dataset'"
02_data_science_toolkit/04_random_forest__IN_PROGRESS/src/__init__.py,0,b''
02_data_science_toolkit/04_random_forest__IN_PROGRESS/src/decision_tree.py,7,"b'import numpy as np\n\nfrom src.utils import find_split\n\n\nclass Node:\n    def __init__(\n        self, assigned_label: int = None,\n        split_feature: int = None,\n        split_value: float = None\n    ) -> None:\n        self.assigned_label = assigned_label\n        self.split_feature = split_feature\n        self.split_value = split_value\n        self.left_child = None\n        self.right_child = None\n\n\nclass TreeClassifier:\n    def __init__(self, max_depth: int, min_samples_split: int) -> None:\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.root = Node()\n\n    def fit(self, X: np.array, y: np.array) -> None:\n        self.__split(X, y, self.root, 1)\n\n    def predict(self, X: np.array) -> np.array:\n        predictions = np.empty(X.shape[0])\n        for idx, x in enumerate(X):\n            predictions[idx] = self.__single_example_prediction(x)\n        return predictions\n\n    def __split(self, X: np.array, y: np.array, node: Node, depth: int) -> None:\n        n_samples = y.shape[0]\n\n        if n_samples <= self.min_samples_split or depth >= self.max_depth:\n            unique, counts = np.unique(y, return_counts=True)\n            node.assigned_label = unique[np.argmax(counts)]\n            return\n\n        node.split_feature, node.split_value, X_left, X_right, y_left, y_right = \\\n            find_split(X, y)\n        node.left_child, node.right_child = Node(), Node()\n\n        self.__split(X_left, y_left, node.left_child, depth + 1)\n        self.__split(X_right, y_right, node.right_child, depth + 1)\n\n    def __single_example_prediction(self, x: np.array) -> int:\n        node = self.root\n        while node.assigned_label is None:\n            if x[node.split_feature] > node.split_value:\n                node = node.right_child\n            else:\n                node = node.left_child\n        return node.assigned_label\n'"
02_data_science_toolkit/04_random_forest__IN_PROGRESS/src/plot_utils.py,4,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef display_2d_data_set(x: np.array, y: np.array, output_path: str = None) -> None:\n    plt.style.use('dark_background')\n    plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    plt.scatter(x[:, 0], x[:, 1], c=y.ravel(), s=50,\n                cmap=plt.cm.Spectral, edgecolors='white')\n\n    if output_path:\n        plt.savefig(output_path, bbox_inches='tight')\n\n    plt.show()\n\n\ndef display_classification_areas(\n        model,\n        cords: np.array,\n        labels: np.array,\n        output_path: str = None\n\n) -> None:\n    x_start = cords[:, 0].min() - 0.5\n    x_end = cords[:, 0].max() + 0.5\n    y_start = cords[:, 1].min() - 0.5\n    y_end = cords[:, 1].max() + 0.5\n\n    grid = np.mgrid[x_start:x_end:100j, y_start:y_end:100j]\n    grid_2d = grid.reshape(2, -1).T\n    x_grid, y_grid = grid\n\n    prediction = model.predict(grid_2d)\n\n    plt.style.use('dark_background')\n    plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    plt.contourf(x_grid, y_grid, prediction.reshape(100, 100),\n                 alpha=0.7, cmap=plt.cm.Spectral)\n    plt.scatter(cords[:, 0], cords[:, 1], c=labels.ravel(),\n                s=50, cmap=plt.cm.Spectral, edgecolors='white')\n\n    if output_path:\n        plt.savefig(output_path, bbox_inches='tight')\n\n    plt.show()\n"""
02_data_science_toolkit/04_random_forest__IN_PROGRESS/src/random_forest.py,4,"b'import numpy as np\n\nfrom src.decision_tree import TreeClassifier\nfrom src.utils import get_most_frequent_element_value, create_bags\n\n\nclass RandomForestClassifier:\n    def __init__(self, n_estimators: int, fraction: float, max_depth: int,\n                 min_samples_split: int):\n        self.n_estimators = n_estimators\n        self.fraction = fraction\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n\n        self.estimators = [\n            TreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split)\n            for _ in range(self.n_estimators)\n        ]\n\n    def fit(self, X: np.array, y: np.array) -> None:\n        bags = create_bags(X, y, self.n_estimators, self.fraction)\n        for estimator, (X_bag, y_bag) in zip(self.estimators, bags):\n            estimator.fit(X_bag, y_bag)\n\n    def predict(self, X: np.array) -> np.array:\n        n_samples = X.shape[0]\n        predictions_raw = np.empty((self.n_estimators, n_samples))\n        predictions = np.empty(n_samples)\n\n        for i in range(self.n_estimators):\n            pred = self.estimators[i].predict(X)\n            predictions_raw[i] = pred\n\n        for i in range(n_samples):\n            predictions[i] = get_most_frequent_element_value(predictions_raw[:, i])\n        return predictions\n'"
02_data_science_toolkit/04_random_forest__IN_PROGRESS/src/utils.py,12,"b'from typing import List, Tuple, Union\n\nimport numpy as np\n\n\ndef group_gini_index(samples: np.array) -> float:\n    n_samples = samples.shape[0]\n    _, n_groups = np.unique(samples, return_counts=True)\n    return 1 - sum(map(lambda x: (x/n_samples)**2, n_groups))\n\n\ndef groups_gini_index(first: np.array, second: np.array) -> float:\n    n_first = first.shape[0]\n    n_second = second.shape[0]\n    n_total = n_first + n_second\n    return group_gini_index(first) * n_first / n_total + \\\n           group_gini_index(second) * n_second / n_total\n\n\ndef find_split(X: np.array, y: np.array):\n    n_samples, n_features = X.shape\n    split_feature, split_value, best_gini, X_left, X_right, y_left, y_right = \\\n        None, None, None, None, None, None, None\n\n    for feature_idx in range(n_features):\n        order = X[:, feature_idx].argsort()\n        X_sorted = X[order]\n        y_sorted = y[order]\n\n        for sample_idx in range(1, n_samples):\n            y_left_, y_right_ = np.split(y_sorted, [sample_idx])\n            gini = groups_gini_index(y_left_, y_right_)\n\n            if best_gini is None or gini < best_gini:\n                best_gini = gini\n                split_feature = feature_idx\n                split_value = (X_sorted[sample_idx, feature_idx] + X_sorted[\n                    sample_idx - 1, feature_idx]) / 2\n                y_left, y_right = y_left_, y_right_\n                X_left, X_right = np.split(X_sorted, [sample_idx])\n\n    return split_feature, split_value, X_left, X_right, y_left, y_right\n\n\ndef create_bags(X: np.array, y: np.array, n_bags: int, fraction: float\n                ) -> List[Tuple[np.array, np.array]]:\n    n_samples = X.shape[0]\n    bags = []\n    for _ in range(n_bags):\n        indexes = np.random.choice(n_samples, int(n_samples * fraction),\n                                   replace=True)\n        bags.append((X[indexes], y[indexes]))\n    return bags\n\n\ndef get_most_frequent_element_value(predictions: np.array) -> Union[int, float]:\n    values, counts = np.unique(predictions, return_counts=True)\n    idx = np.argmax(counts)\n    return values[idx]\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/activation/__init__.py,0,b''
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/activation/relu.py,4,"b'import numpy as np\n\nfrom src.base import Layer\n\n\nclass ReluLayer(Layer):\n    def __init__(self):\n        self._z = None\n\n    def forward_pass(self, a_prev: np.array, training: bool) -> np.array:\n        """"""\n        :param a_prev - ND tensor with shape (n, ..., channels)\n        :output ND tensor with shape (n, ..., channels)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        """"""\n        self._z = np.maximum(0, a_prev)\n        return self._z\n\n    def backward_pass(self, da_curr: np.array) -> np.array:\n        """"""\n        :param da_curr - ND tensor with shape (n, ..., channels)\n        :output ND tensor with shape (n, ..., channels)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        """"""\n        dz = np.array(da_curr, copy=True)\n        dz[self._z <= 0] = 0\n        return dz\n\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/activation/softmax.py,4,"b'import numpy as np\n\nfrom src.base import Layer\n\n\nclass SoftmaxLayer(Layer):\n    def __init__(self):\n        self._z = None\n\n    def forward_pass(self, a_prev: np.array, training: bool) -> np.array:\n        """"""\n        :param a_prev - 2D tensor with shape (n, k)\n        :output 2D tensor with shape (n, k)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        k - number of classes\n        """"""\n        e = np.exp(a_prev - a_prev.max(axis=1, keepdims=True))\n        self._z = e / np.sum(e, axis=1, keepdims=True)\n        return self._z\n\n    def backward_pass(self, da_curr: np.array) -> np.array:\n        """"""\n        :param da_curr - 2D tensor with shape (n, k)\n        :output 2D tensor with shape (n, k)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        k - number of classes\n        """"""\n        return da_curr\n\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/cs231n/__init__.py,0,b''
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/cs231n/fast_conv.py,14,"b'# ------------------------------------------------------------------------------\n# File was originally part of Stamford CS231N course: https://cs231n.github.io/\n# ------------------------------------------------------------------------------\n\nfrom typing import Tuple\n\nimport numpy as np\n\n\ndef get_im2col_idx(\n    array_shape: Tuple[int, int, int, int],\n    filter_dim: Tuple[int, int] = (3, 3),\n    pad: int = 0,\n    stride: int = 1\n) -> Tuple[np.array, np.array, np.array]:\n    """"""\n    :param array_shape - 4 element tuple (n, c, h_in, w_in)\n    :param filter_dim - 2 element tuple (h_f, w_f)\n    :param pad - padding along width and height of input volume\n    :param stride - stride along width and height of input volume\n    :output 2D tensor\n    ------------------------------------------------------------------------\n    n - number of examples in batch\n    w_in - width of input volume\n    h_in - width of input volume\n    c - number of channels of the input volume\n    """"""\n    n, c, h_in, w_in = array_shape\n    h_f, w_f = filter_dim\n\n    h_out = (h_in + 2 * pad - h_f) // stride + 1\n    w_out = (w_in + 2 * pad - w_f) // stride + 1\n\n    i0 = np.repeat(np.arange(h_f), w_f)\n    i0 = np.tile(i0, c)\n    i1 = stride * np.repeat(np.arange(h_out), w_out)\n    j0 = np.tile(np.arange(w_f), h_f * c)\n    j1 = stride * np.tile(np.arange(w_out), h_out)\n    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n    k = np.repeat(np.arange(c), h_f * w_f).reshape(-1, 1)\n    return k, i, j\n\n\ndef im2col(\n    array: np.array,\n    filter_dim: Tuple[int, int] = (3, 3),\n    pad: int = 0,\n    stride: int = 1\n) -> np.array:\n    """"""\n    :param array - 4D tensor with shape (n, c, h_in, w_in)\n    :param filter_dim - 2 element tuple (h_f, w_f)\n    :param pad - padding along width and height of input volume\n    :param stride - stride along width and height of input volume\n    :output 2D tensor with shape (h_f * w_f * c, h_out, w_out * n)\n    ------------------------------------------------------------------------\n    n - number of examples in batch\n    w_in - width of input volume\n    h_in - width of input volume\n    w_in - width of output volume\n    h_in - width of output volume\n    w_f - width of filter volume\n    h_f - height of filter volume\n    c - number of channels of the input volume\n    """"""\n    _, c, _, _ = array.shape\n    h_f, w_f = filter_dim\n    array_pad = np.pad(\n        array=array,\n        pad_width=((0, 0), (0, 0), (pad, pad), (pad, pad)),\n        mode=\'constant\'\n    )\n    k, i, j = get_im2col_idx(\n        array_shape=array.shape,\n        filter_dim=filter_dim,\n        pad=pad,\n        stride=stride\n    )\n    cols = array_pad[:, k, i, j]\n    return cols.transpose(1, 2, 0).reshape(h_f * w_f * c, -1)\n\n\ndef col2im(\n    cols: np.array,\n    array_shape: Tuple[int, int, int, int],\n    filter_dim: Tuple[int, int] = (3, 3),\n    pad: int = 0,\n    stride: int = 1\n) -> np.array:\n    """"""\n    :param cols - 2D tensor with shape (h_f * w_f * c, h_out, w_out * n)\n    :param array_shape - 4 element tuple (n, c, h_in, w_in)\n    :param filter_dim - 2 element tuple (h_f, w_f)\n    :param pad - padding along width and height of input volume\n    :param stride - stride along width and height of input volume\n    :output 4D tensor with shape (n, c, h_in, w_in)\n    ------------------------------------------------------------------------\n    n - number of examples in batch\n    w_in - width of input volume\n    h_in - width of input volume\n    w_in - width of output volume\n    h_in - width of output volume\n    w_f - width of filter volume\n    h_f - height of filter volume\n    c - number of channels of the input volume\n    """"""\n    n, c, h_in, w_in = array_shape\n    h_f, w_f = filter_dim\n    h_pad, w_pad = h_in + 2 * pad, w_in + 2 * pad\n    array_pad = np.zeros((n, c, h_pad, w_pad), dtype=cols.dtype)\n    k, i, j = get_im2col_idx(\n        array_shape=array_shape,\n        filter_dim=filter_dim,\n        pad=pad,\n        stride=stride\n    )\n    cols_reshaped = cols.reshape(c * h_f * w_f, -1, n)\n    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n    np.add.at(array_pad, (slice(None), k, i, j), cols_reshaped)\n    return array_pad[:, :, pad:pad+h_in, pad:pad+w_in]\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/cs231n/setup.py,0,"b""# ------------------------------------------------------------------------------\n# File was originally part of Stamford CS231N course: https://cs231n.github.io/\n# ------------------------------------------------------------------------------\n\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Build import cythonize\nimport numpy\n\nextensions = [\n  Extension('fast_conv_cython', ['fast_conv_cython.pyx'],\n    include_dirs = [numpy.get_include()]),\n]\n\nsetup(\n    ext_modules = cythonize(extensions),\n)"""
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/__init__.py,0,b''
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/convolutional.py,46,"b'from __future__ import annotations\n\ntry:\n    from src.cs231n.fast_conv_cython import col2im_cython, im2col_cython\nexcept ImportError:\n    print(\'run the following from the cs231n directory and try again:\')\n    print(\'python setup.py build_ext --inplace\')\n    print(\'You may also need to restart your iPython kernel\')\n\n\nfrom typing import Tuple, Optional\n\nimport numpy as np\n\nfrom src.base import Layer\nfrom src.errors import InvalidPaddingModeError\nfrom src.cs231n.fast_conv import im2col, col2im\n\n\nclass ConvLayer2D(Layer):\n\n    def __init__(\n        self, w: np.array,\n        b: np.array,\n        padding: str = \'valid\',\n        stride: int = 1\n    ):\n        """"""\n        :param w -  4D tensor with shape (h_f, w_f, c_f, n_f)\n        :param b - 1D tensor with shape (n_f, )\n        :param padding - flag describing type of activation padding valid/same\n        :param stride - stride along width and height of input volume\n        ------------------------------------------------------------------------\n        h_f - height of filter volume\n        w_f - width of filter volume\n        c_f - number of channels of filter volume\n        n_f - number of filters in filter volume\n        """"""\n        self._w, self._b = w, b\n        self._padding = padding\n        self._stride = stride\n        self._dw, self._db = None, None\n        self._a_prev = None\n\n    @classmethod\n    def initialize(\n        cls, filters: int,\n        kernel_shape: Tuple[int, int, int],\n        padding: str = \'valid\',\n        stride: int = 1\n    ) -> ConvLayer2D:\n        w = np.random.randn(*kernel_shape, filters) * 0.1\n        b = np.random.randn(filters) * 0.1\n        return cls(w=w, b=b, padding=padding, stride=stride)\n\n    @property\n    def weights(self) -> Optional[Tuple[np.array, np.array]]:\n        return self._w, self._b\n\n    @property\n    def gradients(self) -> Optional[Tuple[np.array, np.array]]:\n        if self._dw is None or self._db is None:\n            return None\n        return self._dw, self._db\n\n    def forward_pass(self, a_prev: np.array, training: bool) -> np.array:\n        """"""\n        :param a_prev - 4D tensor with shape (n, h_in, w_in, c)\n        :output 4D tensor with shape (n, h_out, w_out, n_f)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        w_in - width of input volume\n        h_in - width of input volume\n        w_out - width of input volume\n        h_out - width of input volume\n        c - number of channels of the input volume\n        n_f - number of filters in filter volume\n        """"""\n        self._a_prev = np.array(a_prev, copy=True)\n        output_shape = self.calculate_output_dims(input_dims=a_prev.shape)\n        n, h_in, w_in, _ = a_prev.shape\n        _, h_out, w_out, _ = output_shape\n        h_f, w_f, _, n_f = self._w.shape\n        pad = self.calculate_pad_dims()\n        a_prev_pad = self.pad(array=a_prev, pad=pad)\n        output = np.zeros(output_shape)\n\n        for i in range(h_out):\n            for j in range(w_out):\n                h_start = i * self._stride\n                h_end = h_start + h_f\n                w_start = j * self._stride\n                w_end = w_start + w_f\n\n                output[:, i, j, :] = np.sum(\n                    a_prev_pad[:, h_start:h_end, w_start:w_end, :, np.newaxis] *\n                    self._w[np.newaxis, :, :, :],\n                    axis=(1, 2, 3)\n                )\n\n        return output + self._b\n\n    def backward_pass(self, da_curr: np.array) -> np.array:\n        """"""\n        :param da_curr - 4D tensor with shape (n, h_out, w_out, n_f)\n        :output 4D tensor with shape (n, h_in, w_in, c)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        w_in - width of input volume\n        h_in - width of input volume\n        w_out - width of input volume\n        h_out - width of input volume\n        c - number of channels of the input volume\n        n_f - number of filters in filter volume\n        """"""\n        _, h_out, w_out, _ = da_curr.shape\n        n, h_in, w_in, _ = self._a_prev.shape\n        h_f, w_f, _, _ = self._w.shape\n        pad = self.calculate_pad_dims()\n        a_prev_pad = self.pad(array=self._a_prev, pad=pad)\n        output = np.zeros_like(a_prev_pad)\n\n        self._db = da_curr.sum(axis=(0, 1, 2)) / n\n        self._dw = np.zeros_like(self._w)\n\n        for i in range(h_out):\n            for j in range(w_out):\n                h_start = i * self._stride\n                h_end = h_start + h_f\n                w_start = j * self._stride\n                w_end = w_start + w_f\n                output[:, h_start:h_end, w_start:w_end, :] += np.sum(\n                    self._w[np.newaxis, :, :, :, :] *\n                    da_curr[:, i:i+1, j:j+1, np.newaxis, :],\n                    axis=4\n                )\n                self._dw += np.sum(\n                    a_prev_pad[:, h_start:h_end, w_start:w_end, :, np.newaxis] *\n                    da_curr[:, i:i+1, j:j+1, np.newaxis, :],\n                    axis=0\n                )\n\n        self._dw /= n\n        return output[:, pad[0]:pad[0]+h_in, pad[1]:pad[1]+w_in, :]\n\n    def set_wights(self, w: np.array, b: np.array) -> None:\n        """"""\n        :param w -  4D tensor with shape (h_f, w_f, c_f, n_f)\n        :param b - 1D tensor with shape (n_f, )\n        ------------------------------------------------------------------------\n        h_f - height of filter volume\n        w_f - width of filter volume\n        c_f - number of channels of filter volume\n        n_f - number of filters in filter volume\n        """"""\n        self._w = w\n        self._b = b\n\n    def calculate_output_dims(\n        self, input_dims: Tuple[int, int, int, int]\n    ) -> Tuple[int, int, int, int]:\n        """"""\n        :param input_dims - 4 element tuple (n, h_in, w_in, c)\n        :output 4 element tuple (n, h_out, w_out, n_f)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        w_in - width of input volume\n        h_in - width of input volume\n        w_out - width of input volume\n        h_out - width of input volume\n        c - number of channels of the input volume\n        n_f - number of filters in filter volume\n        """"""\n        n, h_in, w_in, _ = input_dims\n        h_f, w_f, _, n_f = self._w.shape\n        if self._padding == \'same\':\n            return n, h_in, w_in, n_f\n        elif self._padding == \'valid\':\n            h_out = (h_in - h_f) // self._stride + 1\n            w_out = (w_in - w_f) // self._stride + 1\n            return n, h_out, w_out, n_f\n        else:\n            raise InvalidPaddingModeError(\n                f""Unsupported padding value: {self._padding}""\n            )\n\n    def calculate_pad_dims(self) -> Tuple[int, int]:\n        """"""\n        :output - 2 element tuple (h_pad, w_pad)\n        ------------------------------------------------------------------------\n        h_pad - single side padding on height of the volume\n        w_pad - single side padding on width of the volume\n        """"""\n        if self._padding == \'same\':\n            h_f, w_f, _, _ = self._w.shape\n            return (h_f - 1) // 2, (w_f - 1) // 2\n        elif self._padding == \'valid\':\n            return 0, 0\n        else:\n            raise InvalidPaddingModeError(\n                f""Unsupported padding value: {self._padding}""\n            )\n\n    @staticmethod\n    def pad(array: np.array, pad: Tuple[int, int]) -> np.array:\n        """"""\n        :param array -  4D tensor with shape (n, h_in, w_in, c)\n        :param pad - 2 element tuple (h_pad, w_pad)\n        :output 4D tensor with shape (n, h_out, w_out, n_f)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        w_in - width of input volume\n        h_in - width of input volume\n        w_out - width of input volume\n        h_out - width of input volume\n        c - number of channels of the input volume\n        n_f - number of filters in filter volume\n        h_pad - single side padding on height of the volume\n        w_pad - single side padding on width of the volume\n        """"""\n        return np.pad(\n            array=array,\n            pad_width=((0, 0), (pad[0], pad[0]), (pad[1], pad[1]), (0, 0)),\n            mode=\'constant\'\n        )\n\n\nclass FastConvLayer2D(ConvLayer2D):\n\n    def __init__(\n        self, w: np.array,\n        b: np.array,\n        padding: str = \'valid\',\n        stride: int = 1\n    ):\n        """"""\n        :param w -  4D tensor with shape (h_f, w_f, c_f, n_f)\n        :param b - 1D tensor with shape (n_f, )\n        :param padding - flag describing type of activation padding valid/same\n        :param stride - stride along width and height of input volume\n        ------------------------------------------------------------------------\n        h_f - height of filter volume\n        w_f - width of filter volume\n        c_f - number of channels of filter volume\n        n_f - number of filters in filter volume\n        """"""\n        super(FastConvLayer2D, self).__init__(\n            w=w, b=b, padding=padding, stride=stride\n        )\n        self._cols = None\n\n    def forward_pass(self, a_prev: np.array, training: bool) -> np.array:\n        """"""\n        :param a_prev - 4D tensor with shape (n, h_in, w_in, c)\n        :output 4D tensor with shape (n, h_out, w_out, n_f)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        w_in - width of input volume\n        h_in - width of input volume\n        w_out - width of input volume\n        h_out - width of input volume\n        c - number of channels of the input volume\n        n_f - number of filters in filter volume\n        """"""\n        self._a_prev = np.array(a_prev, copy=True)\n        n, h_out, w_out, _ = self.calculate_output_dims(input_dims=a_prev.shape)\n        h_f, w_f, _, n_f = self._w.shape\n        pad = self.calculate_pad_dims()\n        w = np.transpose(self._w, (3, 2, 0, 1))\n\n        self._cols = im2col(\n            array=np.moveaxis(a_prev, -1, 1),\n            filter_dim=(h_f, w_f),\n            pad=pad[0],\n            stride=self._stride\n        )\n\n        result = w.reshape((n_f, -1)).dot(self._cols)\n        output = result.reshape(n_f, h_out, w_out, n)\n\n        return output.transpose(3, 1, 2, 0) + self._b\n\n    def backward_pass(self, da_curr: np.array) -> np.array:\n        """"""\n        :param da_curr - 4D tensor with shape (n, h_out, w_out, n_f)\n        :output 4D tensor with shape (n, h_in, w_in, c)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        w_in - width of input volume\n        h_in - width of input volume\n        w_out - width of input volume\n        h_out - width of input volume\n        c - number of channels of the input volume\n        n_f - number of filters in filter volume\n        """"""\n        n, h_out, w_out, _ = self.calculate_output_dims(\n            input_dims=self._a_prev.shape)\n        h_f, w_f, _, n_f = self._w.shape\n        pad = self.calculate_pad_dims()\n\n        self._db = da_curr.sum(axis=(0, 1, 2)) / n\n        da_curr_reshaped = da_curr.transpose(3, 1, 2, 0).reshape(n_f, -1)\n\n        w = np.transpose(self._w, (3, 2, 0, 1))\n        dw = da_curr_reshaped.dot(self._cols.T).reshape(w.shape)\n        self._dw = np.transpose(dw, (2, 3, 1, 0))\n\n        output_cols = w.reshape(n_f, -1).T.dot(da_curr_reshaped)\n\n        output = col2im(\n            cols=output_cols,\n            array_shape=np.moveaxis(self._a_prev, -1, 1).shape,\n            filter_dim=(h_f, w_f),\n            pad=pad[0],\n            stride=self._stride\n        )\n        return np.transpose(output, (0, 2, 3, 1))\n\n\nclass SuperFastConvLayer2D(ConvLayer2D):\n\n    def __init__(\n            self, w: np.array,\n            b: np.array,\n            padding: str = \'valid\',\n            stride: int = 1\n    ):\n        """"""\n        :param w -  4D tensor with shape (h_f, w_f, c_f, n_f)\n        :param b - 1D tensor with shape (n_f, )\n        :param padding - flag describing type of activation padding valid/same\n        :param stride - stride along width and height of input volume\n        ------------------------------------------------------------------------\n        h_f - height of filter volume\n        w_f - width of filter volume\n        c_f - number of channels of filter volume\n        n_f - number of filters in filter volume\n        """"""\n        super(SuperFastConvLayer2D, self).__init__(\n            w=w, b=b, padding=padding, stride=stride\n        )\n        self._cols = None\n\n    def forward_pass(self, a_prev: np.array, training: bool) -> np.array:\n        """"""\n        :param a_prev - 4D tensor with shape (n, h_in, w_in, c)\n        :output 4D tensor with shape (n, h_out, w_out, n_f)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        w_in - width of input volume\n        h_in - width of input volume\n        w_out - width of input volume\n        h_out - width of input volume\n        c - number of channels of the input volume\n        n_f - number of filters in filter volume\n        """"""\n        self._a_prev = np.array(a_prev, copy=True)\n        n, h_out, w_out, _ = self.calculate_output_dims(input_dims=a_prev.shape)\n        h_f, w_f, _, n_f = self._w.shape\n        pad = self.calculate_pad_dims()\n        w = np.transpose(self._w, (3, 2, 0, 1))\n\n        self._cols = im2col_cython(\n            np.moveaxis(a_prev, -1, 1),\n            h_f,\n            w_f,\n            pad[0],\n            self._stride\n        )\n\n        result = w.reshape((n_f, -1)).dot(self._cols)\n        output = result.reshape(n_f, h_out, w_out, n)\n\n        return output.transpose(3, 1, 2, 0) + self._b\n\n    def backward_pass(self, da_curr: np.array) -> np.array:\n        """"""\n        :param da_curr - 4D tensor with shape (n, h_out, w_out, n_f)\n        :output 4D tensor with shape (n, h_in, w_in, c)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        w_in - width of input volume\n        h_in - width of input volume\n        w_out - width of input volume\n        h_out - width of input volume\n        c - number of channels of the input volume\n        n_f - number of filters in filter volume\n        """"""\n        n, h_out, w_out, _ = self.calculate_output_dims(\n            input_dims=self._a_prev.shape)\n        h_f, w_f, _, n_f = self._w.shape\n        pad = self.calculate_pad_dims()\n\n        self._db = da_curr.sum(axis=(0, 1, 2)) / n\n        da_curr_reshaped = da_curr.transpose(3, 1, 2, 0).reshape(n_f, -1)\n\n        w = np.transpose(self._w, (3, 2, 0, 1))\n        dw = da_curr_reshaped.dot(self._cols.T).reshape(w.shape)\n        self._dw = np.transpose(dw, (2, 3, 1, 0))\n\n        output_cols = w.reshape(n_f, -1).T.dot(da_curr_reshaped)\n\n        a_prev = np.moveaxis(self._a_prev, -1, 1)\n        output = col2im_cython(\n            output_cols,\n            a_prev.shape[0],\n            a_prev.shape[1],\n            a_prev.shape[2],\n            a_prev.shape[3],\n            h_f,\n            w_f,\n            pad[0],\n            self._stride\n        )\n        return np.transpose(output, (0, 2, 3, 1))\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/dense.py,13,"b'from __future__ import annotations\n\nfrom typing import Optional, Tuple\n\nimport numpy as np\n\nfrom src.base import Layer\n\n\nclass DenseLayer(Layer):\n\n    def __init__(self, w: np.array, b: np.array):\n        """"""\n        :param w - 2D weights tensor with shape (units_curr, units_prev)\n        :param b - 1D bias tensor with shape (1, units_curr)\n        ------------------------------------------------------------------------\n        units_prev - number of units in previous layer\n        units_curr -  number of units in current layer\n        """"""\n        self._w, self._b = w, b\n        self._dw, self._db = None, None\n        self._a_prev = None\n\n    @classmethod\n    def initialize(cls, units_prev: int, units_curr: int) -> DenseLayer:\n        """"""\n        :param units_prev - positive integer, number of units in previous layer\n        :param units_curr - positive integer, number of units in current layer\n        """"""\n        w = np.random.randn(units_curr, units_prev) * 0.1\n        b = np.random.randn(1, units_curr) * 0.1\n        return cls(w=w, b=b)\n\n    @property\n    def weights(self) -> Optional[Tuple[np.array, np.array]]:\n        return self._w, self._b\n\n    @property\n    def gradients(self) -> Optional[Tuple[np.array, np.array]]:\n        if self._dw is None or self._db is None:\n            return None\n        return self._dw, self._db\n\n    def forward_pass(self, a_prev: np.array, training: bool) -> np.array:\n        """"""\n        :param a_prev - 2D tensor with shape (n, units_prev)\n        :output - 2D tensor with shape (n, units_curr)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        units_prev - number of units in previous layer\n        units_curr -  number of units in current layer\n        """"""\n        self._a_prev = np.array(a_prev, copy=True)\n        return np.dot(a_prev, self._w.T) + self._b\n\n    def backward_pass(self, da_curr: np.array) -> np.array:\n        """"""\n        :param da_curr - 2D tensor with shape (n, units_curr)\n        :output - 2D tensor with shape (n, units_prev)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        units_prev - number of units in previous layer\n        units_curr -  number of units in current layer\n        """"""\n        n = self._a_prev.shape[0]\n        self._dw = np.dot(da_curr.T, self._a_prev) / n\n        self._db = np.sum(da_curr, axis=0, keepdims=True) / n\n        return np.dot(da_curr, self._w)\n\n    def set_wights(self, w: np.array, b: np.array) -> None:\n        """"""\n        :param w - 2D weights tensor with shape (units_curr, units_prev)\n        :param b - 1D bias tensor with shape (1, units_curr)\n        ------------------------------------------------------------------------\n        units_prev - number of units in previous layer\n        units_curr -  number of units in current layer\n        """"""\n        self._w = w\n        self._b = b\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/dropout.py,4,"b'from src.base import Layer\n\nimport numpy as np\n\n\nclass DropoutLayer(Layer):\n\n    def __init__(self, keep_prob):\n        """"""\n        :param keep_prob - probability that given unit will not be dropped out\n        """"""\n        self._keep_prob = keep_prob\n        self._mask = None\n\n    def forward_pass(self, a_prev: np.array, training: bool) -> np.array:\n        if training:\n            self._mask = (np.random.rand(*a_prev.shape) < self._keep_prob)\n            return self._apply_mask(a_prev, self._mask)\n        else:\n            return a_prev\n\n    def backward_pass(self, da_curr: np.array) -> np.array:\n        return self._apply_mask(da_curr, self._mask)\n\n    def _apply_mask(self, array: np.array, mask: np.array) -> np.array:\n        array *= mask\n        array /= self._keep_prob\n        return array\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/flatten.py,3,"b'import numpy as np\n\nfrom src.base import Layer\n\n\nclass FlattenLayer(Layer):\n\n    def __init__(self):\n        self._shape = ()\n\n    def forward_pass(self, a_prev: np.array, training: bool) -> np.array:\n        """"""\n        :param a_prev - ND tensor with shape (n, ..., channels)\n        :output - 1D tensor with shape (n, 1)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        """"""\n        self._shape = a_prev.shape\n        return np.ravel(a_prev).reshape(a_prev.shape[0], -1)\n\n    def backward_pass(self, da_curr: np.array) -> np.array:\n        """"""\n        :param da_curr - 1D tensor with shape (n, 1)\n        :output - ND tensor with shape (n, ..., channels)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        """"""\n        return da_curr.reshape(self._shape)\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/pooling.py,10,"b'from __future__ import annotations\n\nfrom typing import Tuple\n\nimport numpy as np\n\nfrom src.base import Layer\n\n\nclass MaxPoolLayer(Layer):\n\n    def __init__(self, pool_size: Tuple[int, int], stride: int = 2):\n        """"""\n        :param pool_size - tuple holding shape of 2D pooling window\n        :param stride - stride along width and height of input volume used to\n        apply pooling operation\n        """"""\n        self._pool_size = pool_size\n        self._stride = stride\n        self._a = None\n        self._cache = {}\n\n    def forward_pass(self, a_prev: np.array, training: bool) -> np.array:\n        """"""\n        :param a_prev - 4D tensor with shape(n, h_in, w_in, c)\n        :output 4D tensor with shape(n, h_out, w_out, c)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        w_in - width of input volume\n        h_in - width of input volume\n        c - number of channels of the input/output volume\n        w_out - width of output volume\n        h_out - width of output volume\n        """"""\n        self._a = np.array(a_prev, copy=True)\n        n, h_in, w_in, c = a_prev.shape\n        h_pool, w_pool = self._pool_size\n        h_out = 1 + (h_in - h_pool) // self._stride\n        w_out = 1 + (w_in - w_pool) // self._stride\n        output = np.zeros((n, h_out, w_out, c))\n\n        for i in range(h_out):\n            for j in range(w_out):\n                h_start = i * self._stride\n                h_end = h_start + h_pool\n                w_start = j * self._stride\n                w_end = w_start + w_pool\n                a_prev_slice = a_prev[:, h_start:h_end, w_start:w_end, :]\n                self._save_mask(x=a_prev_slice, cords=(i, j))\n                output[:, i, j, :] = np.max(a_prev_slice, axis=(1, 2))\n        return output\n\n    def backward_pass(self, da_curr: np.array) -> np.array:\n        """"""\n        :param da_curr - 4D tensor with shape(n, h_out, w_out, c)\n        :output 4D tensor with shape(n, h_in, w_in, c)\n        ------------------------------------------------------------------------\n        n - number of examples in batch\n        w_in - width of input volume\n        h_in - width of input volume\n        c - number of channels of the input/output volume\n        w_out - width of output volume\n        h_out - width of output volume\n        """"""\n        output = np.zeros_like(self._a)\n        _, h_out, w_out, _ = da_curr.shape\n        h_pool, w_pool = self._pool_size\n\n        for i in range(h_out):\n            for j in range(w_out):\n                h_start = i * self._stride\n                h_end = h_start + h_pool\n                w_start = j * self._stride\n                w_end = w_start + w_pool\n                output[:, h_start:h_end, w_start:w_end, :] += \\\n                    da_curr[:, i:i + 1, j:j + 1, :] * self._cache[(i, j)]\n        return output\n\n    def _save_mask(self, x: np.array, cords: Tuple[int, int]) -> None:\n        mask = np.zeros_like(x)\n        n, h, w, c = x.shape\n        x = x.reshape(n, h * w, c)\n        idx = np.argmax(x, axis=1)\n\n        n_idx, c_idx = np.indices((n, c))\n        mask.reshape(n, h * w, c)[n_idx, idx, c_idx] = 1\n        self._cache[cords] = mask\n\n\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/model/__init__.py,0,b''
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/model/sequential.py,8,"b'from __future__ import annotations\nfrom typing import List, Dict, Callable, Optional\nimport time\n\nimport numpy as np\n\nfrom src.base import Layer, Optimizer\nfrom src.utils.core import generate_batches, format_time\nfrom src.utils.metrics import softmax_accuracy, softmax_cross_entropy\n\n\nclass SequentialModel:\n    def __init__(self, layers: List[Layer], optimizer: Optimizer):\n        self._layers = layers\n        self._optimizer = optimizer\n\n        self._train_acc = []\n        self._test_acc = []\n        self._train_loss = []\n        self._test_loss = []\n\n    def train(\n        self,\n        x_train: np.array,\n        y_train: np.array,\n        x_test: np.array,\n        y_test: np.array,\n        epochs: int,\n        bs: int = 64,\n        verbose: bool = False,\n        callback: Optional[Callable[[SequentialModel], None]] = None\n    ) -> None:\n        """"""\n        :param x_train - ND feature tensor with shape (n_train, ...)\n        :param y_train - 2D one-hot labels tensor with shape (n_train, k)\n        :param x_test - ND feature tensor with shape (n_test, ...)\n        :param y_test - 2D one-hot labels tensor with shape (n_test, k)\n        :param epochs - number of epochs used during model training\n        :param bs - size of batch used during model training\n        :param verbose - if set to True, model will produce logs during training\n        :param callback - function that will be executed at the end of each epoch\n        ------------------------------------------------------------------------\n        n_train - number of examples in train data set\n        n_test - number of examples in test data set\n        k - number of classes\n        """"""\n\n        for epoch in range(epochs):\n            epoch_start = time.time()\n            y_hat = np.zeros_like(y_train)\n            for idx, (x_batch, y_batch) in \\\n                    enumerate(generate_batches(x_train, y_train, bs)):\n\n                y_hat_batch = self._forward(x_batch, training=True)\n                activation = y_hat_batch - y_batch\n                self._backward(activation)\n                self._update()\n                n_start = idx * bs\n                n_end = n_start + y_hat_batch.shape[0]\n                y_hat[n_start:n_end, :] = y_hat_batch\n\n            self._train_acc.append(softmax_accuracy(y_hat, y_train))\n            self._train_loss.append(softmax_cross_entropy(y_hat, y_train))\n\n            y_hat = self._forward(x_test, training=False)\n            test_acc = softmax_accuracy(y_hat, y_test)\n            self._test_acc.append(test_acc)\n            test_loss = softmax_cross_entropy(y_hat, y_test)\n            self._test_loss.append(test_loss)\n\n            if verbose:\n                epoch_time = format_time(start_time=epoch_start, end_time=time.time())\n                print(""iter: {:05} | test loss: {:.5f} | test accuracy: {:.5f} | time: {}""\n                      .format(epoch+1, test_loss, test_acc, epoch_time))\n\n    def predict(self, x: np.array) -> np.array:\n        """"""\n        :param x - ND feature tensor with shape (n, ...)\n        :output 2D one-hot labels tensor with shape (n, k)\n        ------------------------------------------------------------------------\n        n - number of examples in data set\n        k - number of classes\n        """"""\n        return self._forward(x, training=False)\n\n    @property\n    def history(self) -> Dict[str, List[float]]:\n        return {\n            ""train_acc"": self._train_acc,\n            ""test_acc"": self._test_acc,\n            ""train_loss"": self._train_loss,\n            ""test_loss"": self._test_loss\n        }\n\n    def _forward(self, x: np.array, training: bool) -> np.array:\n        activation = x\n        for idx, layer in enumerate(self._layers):\n            activation = layer.forward_pass(a_prev=activation, training=training)\n        return activation\n\n    def _backward(self, x: np.array) -> None:\n        activation = x\n        for layer in reversed(self._layers):\n            activation = layer.backward_pass(da_curr=activation)\n\n    def _update(self) -> None:\n        self._optimizer.update(layers=self._layers)\n\n\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/optimizers/__init__.py,0,b''
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/optimizers/adam.py,8,"b'from typing import Tuple, List\n\nimport numpy as np\n\nfrom src.base import Optimizer, Layer\n\n\nclass Adam(Optimizer):\n    def __init__(\n        self, lr: float,\n        beta1: float = 0.9,\n        beta2: float = 0.999,\n        eps: float = 1e-8\n    ):\n        """"""\n        :param lr - learning rate\n        :param beta1 -\n        :param beta2 -\n        :param eps - small value to avoid zero denominator\n        """"""\n        self._cache_v = {}\n        self._cache_s = {}\n        self._lr = lr\n        self._beta1 = beta1\n        self._beta2 = beta2\n        self._eps = eps\n\n    def update(self, layers: List[Layer]) -> None:\n        if len(self._cache_s) == 0 or len(self._cache_v) == 0:\n            self._init_cache(layers)\n\n        for idx, layer in enumerate(layers):\n            weights, gradients = layer.weights, layer.gradients\n            if weights is None or gradients is None:\n                continue\n\n            (w, b), (dw, db) = weights, gradients\n            dw_key, db_key = Adam._get_cache_keys(idx)\n\n            self._cache_v[dw_key] = self._beta1 * self._cache_v[dw_key] + \\\n                (1 - self._beta1) * dw\n            self._cache_v[db_key] = self._beta1 * self._cache_v[db_key] + \\\n                (1 - self._beta1) * db\n\n            self._cache_s[dw_key] = self._beta2 * self._cache_s[dw_key] + \\\n                (1 - self._beta2) * np.square(dw)\n            self._cache_s[db_key] = self._beta2 * self._cache_s[db_key] + \\\n                (1 - self._beta2) * np.square(db)\n\n            dw = self._cache_v[dw_key] / (np.sqrt(self._cache_s[dw_key]) + self._eps)\n            db = self._cache_v[db_key] / (np.sqrt(self._cache_s[db_key]) + self._eps)\n\n            layer.set_wights(\n                w=w - self._lr * dw,\n                b=b - self._lr * db\n            )\n\n    def _init_cache(self, layers: List[Layer]) -> None:\n        for idx, layer in enumerate(layers):\n            gradients = layer.gradients\n            if gradients is None:\n                continue\n\n            dw, db = gradients\n            dw_key, db_key = Adam._get_cache_keys(idx)\n\n            self._cache_v[dw_key] = np.zeros_like(dw)\n            self._cache_v[db_key] = np.zeros_like(db)\n            self._cache_s[dw_key] = np.zeros_like(dw)\n            self._cache_s[db_key] = np.zeros_like(db)\n\n    @staticmethod\n    def _get_cache_keys(idx: int) -> Tuple[str, str]:\n        """"""\n        :param idx - index of layer\n        """"""\n        return f""dw{idx}"", f""db{idx}""\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/optimizers/gradient_descent.py,0,"b'from typing import List\n\nfrom src.base import Optimizer, Layer\n\n\nclass GradientDescent(Optimizer):\n    def __init__(self, lr: float):\n        """"""\n        :param lr - learning rate\n        """"""\n        self._lr = lr\n\n    def update(self, layers: List[Layer]) -> None:\n        for layer in layers:\n            weights, gradients = layer.weights, layer.gradients\n            if weights is None or gradients is None:\n                continue\n\n            (w, b), (dw, db) = weights, gradients\n            layer.set_wights(\n                w = w - self._lr * dw,\n                b = b - self._lr * db\n            )'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/optimizers/rms_prop.py,6,"b'from typing import List, Tuple\n\nimport numpy as np\n\nfrom src.base import Optimizer, Layer\n\n\nclass RMSProp(Optimizer):\n    def __init__(self, lr: float, beta: float = 0.9, eps: float = 1e-8):\n        """"""\n        :param lr - learning rate\n        :param beta - discounting factor for the history/coming gradient\n        :param eps - small value to avoid zero denominator\n        """"""\n        self._cache = {}\n        self._lr = lr\n        self._beta = beta\n        self._eps = eps\n\n    def update(self, layers: List[Layer]) -> None:\n        if len(self._cache) == 0:\n            self._init_cache(layers)\n\n        for idx, layer in enumerate(layers):\n            weights, gradients = layer.weights, layer.gradients\n            if weights is None or gradients is None:\n                continue\n\n            (w, b), (dw, db) = weights, gradients\n            dw_key, db_key = RMSProp._get_cache_keys(idx)\n\n            self._cache[dw_key] = self._beta * self._cache[dw_key] + \\\n                (1 - self._beta) * np.square(dw)\n            self._cache[db_key] = self._beta * self._cache[db_key] + \\\n                (1 - self._beta) * np.square(db)\n\n            dw = dw / (np.sqrt(self._cache[dw_key]) + self._eps)\n            db = db / (np.sqrt(self._cache[db_key]) + self._eps)\n\n            layer.set_wights(\n                w=w - self._lr * dw,\n                b=b - self._lr * db\n            )\n\n    def _init_cache(self, layers: List[Layer]) -> None:\n        for idx, layer in enumerate(layers):\n            gradients = layer.gradients\n            if gradients is None:\n                continue\n\n            dw, db = gradients\n            dw_key, db_key = RMSProp._get_cache_keys(idx)\n\n            self._cache[dw_key] = np.zeros_like(dw)\n            self._cache[db_key] = np.zeros_like(db)\n\n    @staticmethod\n    def _get_cache_keys(idx: int) -> Tuple[str, str]:\n        """"""\n        :param idx - index of layer\n        """"""\n        return f""dw{idx}"", f""db{idx}""\n\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/utils/__init__.py,0,b''
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/utils/core.py,9,"b'import numpy as np\nimport time\n\n\ndef convert_categorical2one_hot(y: np.array) -> np.array:\n    """"""\n    :param y - categorical array with (n, 1) shape\n    :return one hot array with (n, k) shape\n    ----------------------------------------------------------------------------\n    n - number of examples\n    k - number of classes\n    """"""\n    one_hot_matrix = np.zeros((y.size, y.max() + 1))\n    one_hot_matrix[np.arange(y.size), y] = 1\n    return one_hot_matrix\n\n\ndef convert_prob2categorical(probs: np.array) -> np.array:\n    """"""\n    :param probs - softmax output array with (n, k) shape\n    :return categorical array with (n, ) shape\n    ----------------------------------------------------------------------------\n    n - number of examples\n    k - number of classes\n    """"""\n    return np.argmax(probs, axis=1)\n\n\ndef convert_prob2one_hot(probs: np.array) -> np.array:\n    """"""\n    :param probs - softmax output array with (n, k) shape\n    :return one hot array with (n, k) shape\n    ----------------------------------------------------------------------------\n    n - number of examples\n    k - number of classes\n    """"""\n    class_idx = convert_prob2categorical(probs)\n    one_hot_matrix = np.zeros_like(probs)\n    one_hot_matrix[np.arange(probs.shape[0]), class_idx] = 1\n    return one_hot_matrix\n\n\ndef generate_batches(x: np.array, y: np.array, batch_size: int):\n    """"""\n    :param x - features array with (n, ...) shape\n    :param y - one hot ground truth array with (n, k) shape\n    :batch_size - number of elements in single batch\n    ----------------------------------------------------------------------------\n    n - number of examples in data set\n    k - number of classes\n    """"""\n    for i in range(0, x.shape[0], batch_size):\n        yield (\n            x.take(indices=range(\n                i, min(i + batch_size, x.shape[0])), axis=0),\n            y.take(indices=range(\n                i, min(i + batch_size, y.shape[0])), axis=0)\n        )\n\n\ndef format_time(start_time: time.time, end_time: time.time) -> str:\n    """"""\n    :param start_time - beginning of time period\n    :param end_time - ending of time period\n    :output - string in HH:MM:SS.ss format\n    ----------------------------------------------------------------------------\n    HH - hours\n    MM - minutes\n    SS - seconds\n    ss - hundredths of a second\n    """"""\n    hours, rem = divmod(end_time - start_time, 3600)\n    minutes, seconds = divmod(rem, 60)\n    return ""{:0>2}:{:0>2}:{:05.2f}"".format(int(hours), int(minutes), seconds)\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/utils/metrics.py,2,"b'import numpy as np\n\nfrom src.utils.core import convert_prob2one_hot\n\n\ndef softmax_accuracy(y_hat: np.array, y: np.array) -> float:\n    """"""\n    :param y_hat - 2D one-hot prediction tensor with shape (n, k)\n    :param y - 2D one-hot ground truth labels tensor with shape (n, k)\n    ----------------------------------------------------------------------------\n    n - number of examples in batch\n    k - number of classes\n    """"""\n    y_hat = convert_prob2one_hot(y_hat)\n    return (y_hat == y).all(axis=1).mean()\n\n\ndef softmax_cross_entropy(y_hat, y, eps=1e-20) -> float:\n    """"""\n    :param y_hat - 2D one-hot prediction tensor with shape (n, k)\n    :param y - 2D one-hot ground truth labels tensor with shape (n, k)\n    ----------------------------------------------------------------------------\n    n - number of examples in batch\n    k - number of classes\n    """"""\n    n = y_hat.shape[0]\n    return - np.sum(y * np.log(np.clip(y_hat, eps, 1.))) / n\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/utils/plots.py,3,"b'from typing import Tuple, Optional\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom matplotlib import rcParams\n\nrcParams[\'font.family\'] = \'sans-serif\'\nrcParams[\'font.sans-serif\'] = [\'Verdana\']\nrcParams[\'font.size\'] = 16\n\nBG_COLOR = (240 / 255, 238 / 255, 223 / 255)\n\n\ndef lines(\n    y_1: np.array,\n    y_2: np.array,\n    label_1: str,\n    label_2: str,\n    title: str,\n    fig_size: Tuple[int, int],\n    path: Optional[str] = None\n) -> None:\n\n    assert len(y_1) == len(y_2)\n    x = np.arange(len(y_1))\n    fig, ax = plt.subplots(figsize=fig_size, constrained_layout=True, facecolor=BG_COLOR)\n\n    # plot\n    ax.plot(x, y_1, label=label_1, color=\'crimson\', linewidth=2)\n    ax.plot(x, y_2, label=label_2, linewidth=2)\n    ax.set_title(title, loc=\'left\', pad=20)\n\n    # axis\n    for s in [\'top\', \'right\']:\n        ax.spines[s].set_visible(False)\n\n    ax.xaxis.set_ticks_position(\'none\')\n    ax.set_xlabel(\'epoch\', fontsize=12)\n    ax.xaxis.set_label_coords(0.97, 0.04)\n\n    # grid\n    ax.grid(b=True, color=\'grey\', linestyle=\'-.\', linewidth=0.5, alpha=0.5)\n\n    # legend\n    fig.legend(bbox_to_anchor=(0.90, 0.15), loc=""lower left"", frameon=False,\n               prop={\'size\': 14})\n\n    ax.set_facecolor(BG_COLOR)\n\n    if path:\n        fig.savefig(path, facecolor=fig.get_facecolor(), dpi=100)\n    plt.show()\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/tests/layers/__init__.py,0,b''
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/tests/utils/__init__.py,0,b''
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/tests/layers/unit_tests/__init__.py,0,b''
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/tests/layers/unit_tests/test_convolutional.py,71,"b""import numpy as np\nimport pytest\n\nfrom src.errors import InvalidPaddingModeError\nfrom src.layers.convolutional import ConvLayer2D, FastConvLayer2D, \\\n    SuperFastConvLayer2D\n\n\nclass TestConvLayer2D:\n\n    def test_pad_symmetrical(self):\n        # given\n        array = np.random.rand(100, 28, 28, 3)\n        pad = 3, 3\n\n        # when\n        result = ConvLayer2D.pad(array=array, pad=pad)\n\n        # then\n        print(result.sum())\n        print(array.sum())\n        assert result.shape == (100, 34, 34, 3)\n        assert abs(result.sum() - array.sum()) < 1e-8\n\n    def test_pad_asymmetrical(self):\n        # given\n        array = np.random.rand(100, 28, 28, 3)\n        pad = 3, 5\n\n        # when\n        result = ConvLayer2D.pad(array=array, pad=pad)\n\n        # then\n        print(result.sum())\n        print(array.sum())\n        assert result.shape == (100, 34, 38, 3)\n        assert abs(result.sum() - array.sum()) < 1e-8\n\n    def test_calculate_pad_width_with_valid_padding(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        padding = 'valid'\n\n        # when\n        layer = ConvLayer2D(w=w, b=b, padding=padding)\n        result = layer.calculate_pad_dims()\n\n        # then\n        assert result == (0, 0)\n\n    def test_calculate_pad_width_with_same_padding_symmetrical(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        padding = 'same'\n\n        # when\n        layer = ConvLayer2D(w=w, b=b, padding=padding)\n        result = layer.calculate_pad_dims()\n\n        # then\n        assert result == (2, 2)\n\n    def test_calculate_pad_width_with_same_padding_asymmetrical(self):\n        # given\n        w = np.random.rand(5, 7, 3, 16)\n        b = np.random.rand(16)\n        padding = 'same'\n\n        # when\n        layer = ConvLayer2D(w=w, b=b, padding=padding)\n        result = layer.calculate_pad_dims()\n\n        # then\n        assert result == (2, 3)\n\n    def test_calculate_pad_width_with_invalid_padding_value(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        padding = 'lorem ipsum'\n\n        # when\n        layer = ConvLayer2D(w=w, b=b, padding=padding)\n        with pytest.raises(InvalidPaddingModeError):\n            _ = layer.calculate_pad_dims()\n\n    def test_calculate_output_dims_with_same_padding_symmetrical(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        padding = 'same'\n\n        # when\n        layer = ConvLayer2D(w=w, b=b, padding=padding)\n        result = layer.calculate_output_dims((32, 11, 11, 3))\n\n        # then\n        assert result == (32, 11, 11, 16)\n\n    def test_calculate_output_dims_with_same_padding_asymmetrical(self):\n        # given\n        w = np.random.rand(3, 5, 3, 16)\n        b = np.random.rand(16)\n        padding = 'same'\n\n        # when\n        layer = ConvLayer2D(w=w, b=b, padding=padding)\n        result = layer.calculate_output_dims((32, 11, 11, 3))\n\n        # then\n        assert result == (32, 11, 11, 16)\n\n    def test_calculate_output_dims_with_valid_padding_symmetrical(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        padding = 'valid'\n\n        # when\n        layer = ConvLayer2D(w=w, b=b, padding=padding)\n        result = layer.calculate_output_dims((32, 11, 11, 3))\n\n        # then\n        assert result == (32, 7, 7, 16)\n\n    def test_calculate_output_dims_with_valid_padding_asymmetrical(self):\n        # given\n        w = np.random.rand(3, 5, 3, 16)\n        b = np.random.rand(16)\n        padding = 'valid'\n\n        # when\n        layer = ConvLayer2D(w=w, b=b, padding=padding)\n        result = layer.calculate_output_dims((32, 11, 11, 3))\n\n        # then\n        assert result == (32, 9, 7, 16)\n\n    def test_calculate_output_dims_with_invalid_padding_value(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        padding = 'lorem ipsum'\n\n        # when\n        layer = ConvLayer2D(w=w, b=b, padding=padding)\n        with pytest.raises(InvalidPaddingModeError):\n            _ = layer.calculate_output_dims((32, 11, 11, 3))\n\n    def test_forward_pass_with_same_padding(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        activation = np.random.rand(16, 11, 11, 3)\n        padding = 'same'\n\n        # when\n        layer = ConvLayer2D(w=w, b=b, padding=padding)\n        result = layer.forward_pass(activation, training=True)\n\n        assert result.shape == (16, 11, 11, 16)\n        expected_val = np.sum(w[:, :, :, 0] * activation[0, 0:5, 0:5, :]) + b[0]\n        assert abs(expected_val - result[0, 2, 2, 0]) < 1e-8\n\n    def test_forward_pass_with_valid_padding(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        activation = np.random.rand(16, 11, 11, 3)\n        padding = 'valid'\n\n        # when\n        layer = ConvLayer2D(w=w, b=b, padding=padding)\n        result = layer.forward_pass(activation, training=True)\n\n        assert result.shape == (16, 7, 7, 16)\n        expected_val = np.sum(w[:, :, :, 0] * activation[0, 0:5, 0:5, :]) + b[0]\n        assert abs(expected_val - result[0, 0, 0, 0]) < 1e-8\n\n    def test_forward_pass_with_invalid_padding_value(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        activation = np.random.rand(16, 11, 11, 3)\n        padding = 'lorem ipsum'\n\n        # when\n        layer = ConvLayer2D(w=w, b=b, padding=padding)\n        with pytest.raises(InvalidPaddingModeError):\n            _ = layer.forward_pass(activation, training=True)\n\n    def test_backward_pass_only_size_same_padding(self):\n        # given\n        activation = np.random.rand(64, 11, 11, 3)\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        layer = ConvLayer2D(w=w, b=b, padding='same')\n\n        # when\n        forward_result = layer.forward_pass(activation, training=True)\n        backward_result = layer.backward_pass(forward_result)\n\n        # then\n        assert backward_result.shape == activation.shape\n\n    def test_backward_pass_only_size_valid_padding(self):\n        # given\n        activation = np.random.rand(64, 11, 11, 3)\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        layer = ConvLayer2D(w=w, b=b, padding='valid')\n\n        # when\n        forward_result = layer.forward_pass(activation, training=True)\n        backward_result = layer.backward_pass(forward_result)\n\n        # then\n        assert backward_result.shape == activation.shape\n\n\nclass TestFastConvLayer2D:\n\n    def test_forward_pass_with_same_padding(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        activation = np.random.rand(16, 11, 11, 3)\n        padding = 'same'\n\n        # when\n        layer = FastConvLayer2D(w=w, b=b, padding=padding)\n        result = layer.forward_pass(activation, training=True)\n\n        assert result.shape == (16, 11, 11, 16)\n        expected_val = np.sum(w[:, :, :, 0] * activation[0, 0:5, 0:5, :]) + b[0]\n        assert abs(expected_val - result[0, 2, 2, 0]) < 1e-8\n\n    def test_forward_pass_with_valid_padding(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        activation = np.random.rand(16, 11, 11, 3)\n        padding = 'valid'\n\n        # when\n        layer = FastConvLayer2D(w=w, b=b, padding=padding)\n        result = layer.forward_pass(activation, training=True)\n\n        assert result.shape == (16, 7, 7, 16)\n        expected_val = np.sum(w[:, :, :, 0] * activation[0, 0:5, 0:5, :]) + b[0]\n        assert abs(expected_val - result[0, 0, 0, 0]) < 1e-8\n\n    def test_forward_pass_with_invalid_padding_value(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        activation = np.random.rand(16, 11, 11, 3)\n        padding = 'lorem ipsum'\n\n        # when\n        layer = FastConvLayer2D(w=w, b=b, padding=padding)\n        with pytest.raises(InvalidPaddingModeError):\n            _ = layer.forward_pass(activation, training=True)\n\n    def test_backward_pass_only_size_same_padding(self):\n        # given\n        activation = np.random.rand(64, 11, 11, 3)\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        layer = FastConvLayer2D(w=w, b=b, padding='same')\n\n        # when\n        forward_result = layer.forward_pass(activation, training=True)\n        backward_result = layer.backward_pass(forward_result)\n\n        # then\n        assert backward_result.shape == activation.shape\n\n    def test_backward_pass_only_size_valid_padding(self):\n        # given\n        activation = np.random.rand(64, 11, 11, 3)\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        layer = FastConvLayer2D(w=w, b=b, padding='valid')\n\n        # when\n        forward_result = layer.forward_pass(activation, training=True)\n        backward_result = layer.backward_pass(forward_result)\n\n        # then\n        assert backward_result.shape == activation.shape\n\n\nclass TestSuperFastConvLayer2D:\n\n    def test_forward_pass_with_same_padding(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        activation = np.random.rand(16, 11, 11, 3)\n        padding = 'same'\n\n        # when\n        layer = SuperFastConvLayer2D(w=w, b=b, padding=padding)\n        result = layer.forward_pass(activation, training=True)\n\n        assert result.shape == (16, 11, 11, 16)\n        expected_val = np.sum(w[:, :, :, 0] * activation[0, 0:5, 0:5, :]) + b[0]\n        assert abs(expected_val - result[0, 2, 2, 0]) < 1e-8\n\n    def test_forward_pass_with_valid_padding(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        activation = np.random.rand(16, 11, 11, 3)\n        padding = 'valid'\n\n        # when\n        layer = SuperFastConvLayer2D(w=w, b=b, padding=padding)\n        result = layer.forward_pass(activation, training=True)\n\n        assert result.shape == (16, 7, 7, 16)\n        expected_val = np.sum(w[:, :, :, 0] * activation[0, 0:5, 0:5, :]) + b[0]\n        assert abs(expected_val - result[0, 0, 0, 0]) < 1e-8\n\n    def test_forward_pass_with_invalid_padding_value(self):\n        # given\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        activation = np.random.rand(16, 11, 11, 3)\n        padding = 'lorem ipsum'\n\n        # when\n        layer = SuperFastConvLayer2D(w=w, b=b, padding=padding)\n        with pytest.raises(InvalidPaddingModeError):\n            _ = layer.forward_pass(activation, training=True)\n\n    def test_backward_pass_only_size_same_padding(self):\n        # given\n        activation = np.random.rand(64, 11, 11, 3)\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        layer = SuperFastConvLayer2D(w=w, b=b, padding='same')\n\n        # when\n        forward_result = layer.forward_pass(activation, training=True)\n        backward_result = layer.backward_pass(forward_result)\n\n        # then\n        assert backward_result.shape == activation.shape\n\n    def test_backward_pass_only_size_valid_padding(self):\n        # given\n        activation = np.random.rand(64, 11, 11, 3)\n        w = np.random.rand(5, 5, 3, 16)\n        b = np.random.rand(16)\n        layer = SuperFastConvLayer2D(w=w, b=b, padding='valid')\n\n        # when\n        forward_result = layer.forward_pass(activation, training=True)\n        backward_result = layer.backward_pass(forward_result)\n\n        # then\n        assert backward_result.shape == activation.shape\n"""
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/tests/layers/unit_tests/test_flatten.py,6,"b'import numpy as np\n\nfrom src.layers.flatten import FlattenLayer\n\n\nclass TestFlattenLayer:\n\n    def test_flatten_forward_pass_with_tree_axis(self):\n        # given\n        activation = np.random.rand(100, 28, 28)\n\n        # when\n        flatten_layer = FlattenLayer()\n        result = flatten_layer.forward_pass(activation, training=True)\n\n        # then\n        assert result.shape == (100, 28 * 28)\n\n    def test_flatten_forward_pass_with_four_axis(self):\n        # given\n        activation = np.random.rand(100, 28, 28, 3)\n\n        # when\n        flatten_layer = FlattenLayer()\n        result = flatten_layer.forward_pass(activation, training=True)\n\n        # then\n        assert result.shape == (100, 28 * 28 * 3)\n\n    def test_flatten_backward_pass_with_tree_axis(self):\n        # given\n        activation = np.random.rand(100, 28, 28)\n\n        # when\n        flatten_layer = FlattenLayer()\n        forward_result = flatten_layer.forward_pass(activation, training=True)\n        backward_result = flatten_layer.backward_pass(forward_result)\n\n        # then\n        assert np.alltrue(activation == backward_result)\n\n    def test_flatten_backward_pass_with_four_axis(self):\n        # given\n        activation = np.random.rand(100, 28, 28, 3)\n\n        # when\n        flatten_layer = FlattenLayer()\n        forward_result = flatten_layer.forward_pass(activation, training=True)\n        backward_result = flatten_layer.backward_pass(forward_result)\n\n        # then\n        assert np.alltrue(activation == backward_result)\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/tests/layers/unit_tests/test_pooling.py,21,"b'import numpy as np\n\nfrom src.layers.pooling import MaxPoolLayer\n\n\nclass TestMaxPoolLayer:\n\n    def test_forward_pass_single_channel_single_item(self):\n        # given\n        pool_size = (2, 2)\n        stride = 2\n        activation = np.array([[\n            [[1], [2], [2], [1]],\n            [[3], [4], [0], [0]],\n            [[5], [2], [1], [1]],\n            [[3], [4], [0], [3]]\n        ]])\n\n        expected_result = np.array([[\n            [[4], [2]],\n            [[5], [3]],\n        ]])\n\n        # when\n        layer = MaxPoolLayer(pool_size=pool_size, stride=stride)\n        result = layer.forward_pass(activation, training=True)\n\n        # then\n        assert result.shape == (1, 2, 2, 1)\n        assert np.alltrue(expected_result == result)\n\n    def test_forward_pass_two_channels_single_item(self):\n        # given\n        pool_size = (2, 2)\n        stride = 2\n        activation = np.array([[\n            [\n                [1, 5],\n                [2, 2],\n                [2, 2],\n                [1, 1]\n            ],\n            [\n                [3, 3],\n                [4, 4],\n                [0, 3],\n                [0, 0]\n            ],\n            [\n                [5, 2],\n                [2, 2],\n                [1, 1],\n                [1, 1]\n            ],\n            [\n                [3, 3],\n                [4, 4],\n                [0, 2],\n                [3, 0]\n            ]\n        ]])\n\n        expected_result = np.array([[\n            [\n                [4, 5],\n                [2, 3]\n            ],\n            [\n                [5, 4],\n                [3, 2]\n            ]\n        ]])\n\n        # when\n        layer = MaxPoolLayer(pool_size=pool_size, stride=stride)\n        result = layer.forward_pass(activation, training=True)\n\n        # then\n        assert result.shape == (1, 2, 2, 2)\n        assert np.alltrue(expected_result == result)\n\n    def test_forward_pass_single_channel_two_items(self):\n        # given\n        pool_size = (2, 2)\n        stride = 2\n        activation = np.array([\n            [\n                [[1], [2], [2], [1]],\n                [[3], [4], [0], [0]],\n                [[5], [2], [1], [1]],\n                [[3], [4], [0], [3]]\n            ],\n            [\n                [[5], [2], [2], [1]],\n                [[3], [4], [3], [0]],\n                [[2], [2], [1], [1]],\n                [[3], [4], [2], [0]]\n            ]\n        ])\n\n        expected_result = np.array([\n            [\n                [[4], [2]],\n                [[5], [3]]\n            ],\n            [\n                [[5], [3]],\n                [[4], [2]]\n            ]\n        ])\n\n        # when\n        layer = MaxPoolLayer(pool_size=pool_size, stride=stride)\n        result = layer.forward_pass(activation, training=True)\n\n        # then\n        assert result.shape == (2, 2, 2, 1)\n        assert np.alltrue(expected_result == result)\n\n    def test_backward_pass_single_channel_single_item(self):\n        # given\n        pool_size = (2, 2)\n        stride = 2\n        forward_activation = np.array([[\n            [[1], [2], [2], [1]],\n            [[3], [4], [0], [0]],\n            [[5], [2], [1], [1]],\n            [[3], [4], [0], [3]]\n        ]])\n\n        backward_activation = np.array([[\n            [[3], [1]],\n            [[8], [2]],\n        ]])\n\n        expected_backward_result = np.array([[\n            [[0], [0], [1], [0]],\n            [[0], [3], [0], [0]],\n            [[8], [0], [0], [0]],\n            [[0], [0], [0], [2]]\n        ]])\n\n        # when\n        layer = MaxPoolLayer(pool_size=pool_size, stride=stride)\n        _ = layer.forward_pass(forward_activation, training=True)\n        backward_result = layer.backward_pass(backward_activation)\n\n        # then\n        assert np.alltrue(expected_backward_result == backward_result)\n\n    def test_backward_pass_two_channels_single_item(self):\n        # given\n        pool_size = (2, 2)\n        stride = 2\n        forward_activation = np.array([[\n            [\n                [1, 5],\n                [2, 2],\n                [2, 2],\n                [1, 1]\n            ],\n            [\n                [3, 3],\n                [4, 4],\n                [0, 3],\n                [0, 0]\n            ],\n            [\n                [5, 2],\n                [2, 2],\n                [1, 1],\n                [1, 1]\n            ],\n            [\n                [3, 3],\n                [4, 4],\n                [0, 2],\n                [3, 0]\n            ]\n        ]])\n\n        backward_activation = np.array([[\n            [\n                [7, 2],\n                [4, 3]\n            ],\n            [\n                [1, 5],\n                [2, 2]\n            ]\n        ]])\n\n        expected_backward_result = np.array([[\n            [\n                [0, 2],\n                [0, 0],\n                [4, 0],\n                [0, 0]\n            ],\n            [\n                [0, 0],\n                [7, 0],\n                [0, 3],\n                [0, 0]\n            ],\n            [\n                [1, 0],\n                [0, 0],\n                [0, 0],\n                [0, 0]\n            ],\n            [\n                [0, 0],\n                [0, 5],\n                [0, 2],\n                [2, 0]\n            ]\n        ]])\n\n        # when\n        layer = MaxPoolLayer(pool_size=pool_size, stride=stride)\n        _ = layer.forward_pass(forward_activation, training=True)\n        backward_result = layer.backward_pass(backward_activation)\n\n        # then\n        assert np.alltrue(expected_backward_result == backward_result)\n\n    def test_backward_pass_single_channel_two_items(self):\n        # given\n        pool_size = (2, 2)\n        stride = 2\n        forward_activation = np.array([\n            [\n                [[1], [2], [2], [1]],\n                [[3], [4], [0], [0]],\n                [[5], [2], [1], [1]],\n                [[3], [4], [0], [3]]\n            ],\n            [\n                [[5], [2], [2], [1]],\n                [[3], [4], [3], [0]],\n                [[2], [2], [1], [1]],\n                [[3], [4], [2], [0]]\n            ]\n        ])\n\n        backward_activation = np.array([\n            [\n                [[7], [2]],\n                [[4], [3]]\n            ],\n            [\n                [[1], [5]],\n                [[2], [2]]\n            ]\n        ])\n\n        expected_backward_result = np.array([\n            [\n                [[0], [0], [2], [0]],\n                [[0], [7], [0], [0]],\n                [[4], [0], [0], [0]],\n                [[0], [0], [0], [3]]\n            ],\n            [\n                [[1], [0], [0], [0]],\n                [[0], [0], [5], [0]],\n                [[0], [0], [0], [0]],\n                [[0], [2], [2], [0]]\n            ]\n        ])\n\n        # when\n        layer = MaxPoolLayer(pool_size=pool_size, stride=stride)\n        _ = layer.forward_pass(forward_activation, training=True)\n        backward_result = layer.backward_pass(backward_activation)\n\n        # then\n        assert np.alltrue(expected_backward_result == backward_result)\n'"
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/tests/utils/unit_tests/__init__.py,0,b''
01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/tests/utils/unit_tests/test_core.py,12,"b'import numpy as np\n\nfrom src.utils.core import convert_prob2one_hot, convert_categorical2one_hot\n\n\nclass TestConvertProb2OneHot:\n\n    def test_conversion_with_2_classes(self):\n        # given\n        prob = np.array([\n            [0.1, 0.9],\n            [0.7, 0.3],\n            [0.8, 0.2],\n            [0.2, 0.8],\n        ])\n        expected_one_hot = np.array([\n            [0, 1],\n            [1, 0],\n            [1, 0],\n            [0, 1],\n        ])\n\n        # when\n        result_one_hot = convert_prob2one_hot(prob)\n\n        # then\n        assert np.alltrue(result_one_hot == expected_one_hot)\n\n    def test_conversion_with_3_classes(self):\n        # given\n        prob = np.array([\n            [0.3, 0.3, 0.4],\n            [0.7, 0.1, 0.2],\n            [0.8, 0.1, 0.1],\n            [0.2, 0.8, 0.0]\n        ])\n        expected_one_hot = np.array([\n            [0, 0, 1],\n            [1, 0, 0],\n            [1, 0, 0],\n            [0, 1, 0]\n        ])\n\n        # when\n        result_one_hot = convert_prob2one_hot(prob)\n\n        # then\n        assert np.alltrue(result_one_hot == expected_one_hot)\n\n\nclass TestConvertCategoricalToOneHot:\n\n    def test_conversion_with_2_classes(self):\n        # given\n        categories = np.array([0, 1, 1, 0, 1])\n        expected_one_hot = np.array([\n            [1, 0],\n            [0, 1],\n            [0, 1],\n            [1, 0],\n            [0, 1]\n        ])\n\n        # when\n        result_one_hot = convert_categorical2one_hot(categories)\n\n        # then\n        assert np.alltrue(result_one_hot == expected_one_hot)\n\n    def test_conversion_with_3_classes(self):\n        # given\n        categories = np.array([0, 2, 1, 0, 1])\n        expected_one_hot = np.array([\n            [1, 0, 0],\n            [0, 0, 1],\n            [0, 1, 0],\n            [1, 0, 0],\n            [0, 1, 0]\n        ])\n\n        # when\n        result_one_hot = convert_categorical2one_hot(categories)\n\n        # then\n        assert np.alltrue(result_one_hot == expected_one_hot)\n'"
