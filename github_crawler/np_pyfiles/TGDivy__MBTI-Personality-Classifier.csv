file_path,api_count,code
MBTI personality classifier (1).py,9,"b'\n# coding: utf-8\n\n# # MBTI Personality Classifier\n# \n# This programme will classify people into mbti personality types based on their past 50 posts on social media using the basic naivebayesclassifier\n\n# In[1]:\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk\nimport string\nfrom nltk.classify import NaiveBayesClassifier\n\n\n# ### Importing the dataset\n\n# In[2]:\n\n\ndata_set = pd.read_csv(""mbti_1.csv"")\ndata_set.tail()\n\n\n# ### Checking the dataset for missing values\n\n# In[3]:\n\n\ndata_set.isnull().any()\n\n\n# ## Exploring the dataset\n\n# The size of the dataset\n\n# In[4]:\n\n\ndata_set.shape\n\n\n# Explroing the posts in posts field\n\n# In[5]:\n\n\ndata_set.iloc[0,1].split(\'|||\')\n\n\n# Finding the number of posts\n\n# In[6]:\n\n\nlen(data_set.iloc[1,1].split(\'|||\'))\n\n\n# Finding the unique vales from type of personality column\n\n# In[7]:\n\n\ntypes = np.unique(np.array(data_set[\'type\']))\ntypes\n\n\n# The total number of posts for each type\n\n# In[8]:\n\n\ntotal = data_set.groupby([\'type\']).count()*50\ntotal\n\n\n# Graphing it for better visualization\n\n# In[9]:\n\n\nplt.figure(figsize = (12,6))\n\nplt.bar(np.array(total.index), height = total[\'posts\'],)\nplt.xlabel(\'Personality types\', size = 14)\nplt.ylabel(\'Number of posts available\', size = 14)\nplt.title(\'Total posts for each personality type\')\n\n\n# ## Organising the data to create a bag words model\n\n# Segrating all the posts by their personality types and ***creating a new dataframe to store all this in***\n\n# In[10]:\n\n\nall_posts= pd.DataFrame()\nfor j in types:\n    temp1 = data_set[data_set[\'type\']==j][\'posts\']\n    temp2 = []\n    for i in temp1:\n        temp2+=i.split(\'|||\')\n    temp3 = pd.Series(temp2)\n    all_posts[j] = temp3\n\n\n# In[11]:\n\n\nall_posts.tail()\n\n\n# ### Creating a function to tokenize the words\n\n# In[12]:\n\n\nuseless_words = nltk.corpus.stopwords.words(""english"") + list(string.punctuation)\ndef build_bag_of_words_features_filtered(words):\n    words = nltk.word_tokenize(words)\n    return {\n        word:1 for word in words \\\n        if not word in useless_words}\n\n\n# A random check of the function\n\n# In[13]:\n\n\nbuild_bag_of_words_features_filtered(all_posts[\'INTJ\'].iloc[1])\n\n\n# ## Creating an array of features\n\n# In[14]:\n\n\nfeatures=[]\nfor j in types:\n    temp1 = all_posts[j]\n    temp1 = temp1.dropna() #not all the personality types have same number of files\n    features += [[(build_bag_of_words_features_filtered(i), j)     for i in temp1]]\n\n\n# Because each number of personality types have different number of posts they must be splitted accordingle. Taking 80% for training and 20% for testing\n\n# In[15]:\n\n\nsplit=[]\nfor i in range(16):\n    split += [len(features[i]) * 0.8]\nsplit = np.array(split,dtype = int)\n\n\n# In[16]:\n\n\nsplit\n\n\n# Data for training\n\n# In[17]:\n\n\ntrain=[]\nfor i in range(16):\n    train += features[i][:split[i]] \n\n\n# Training the model\n\n# In[18]:\n\n\nsentiment_classifier = NaiveBayesClassifier.train(train)\n\n\n# Testing the model on the dataset it was trained for accuracy\n\n# In[19]:\n\n\nnltk.classify.util.accuracy(sentiment_classifier, train)*100\n\n\n# Creating the test data\n\n# In[20]:\n\n\ntest=[]\nfor i in range(16):\n    test += features[i][split[i]:]\n\n\n# Testing the model on the test dataset which it has never seen before\n\n# In[21]:\n\n\nnltk.classify.util.accuracy(sentiment_classifier, test)*100\n\n\n# # The model performs at efficieny of only 10% which is pretty bad.\n# \n# ## Hence, instead of selecting all 16 types of personalitys as a unique feature I explored the dataset further and decided to simplify it.\n# \n# The Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides everyone into 16 distinct personality types across 4 axis:\n# \n# - Introversion (I) \xe2\x80\x93 Extroversion (E)\n# - Intuition (N) \xe2\x80\x93 Sensing (S)\n# - Thinking (T) \xe2\x80\x93 Feeling (F)\n# - Judging (J) \xe2\x80\x93 Perceiving (P)\n# <br><br>\n# We will use this and create 4 classifyers to classify the person \n\n# ## Creating a classifyer for Introversion (I) and Extroversion (E)\n# \n# **Note:** The details for the steps over here are same as the ones while creating the model above, hence I will only explain the changes\n\n# In[22]:\n\n\n# Features for the bag of words model\nfeatures=[]\nfor j in types:\n    temp1 = all_posts[j]\n    temp1 = temp1.dropna() #not all the personality types have same number of files\n    if(\'I\' in j):\n        features += [[(build_bag_of_words_features_filtered(i), \'introvert\')         for i in temp1]]\n    if(\'E\' in j):\n        features += [[(build_bag_of_words_features_filtered(i), \'extrovert\')         for i in temp1]]\n\n\n# Data for training\n\n# In[23]:\n\n\ntrain=[]\nfor i in range(16):\n    train += features[i][:split[i]] \n\n\n# Training the model\n\n# In[24]:\n\n\nIntroExtro = NaiveBayesClassifier.train(train)\n\n\n# Testing the model on the dataset it was trained for accuracy\n\n# In[25]:\n\n\nnltk.classify.util.accuracy(IntroExtro, train)*100\n\n\n# Creating the test data\n\n# In[26]:\n\n\ntest=[]\nfor i in range(16):\n    test += features[i][split[i]:]\n\n\n# Testing the model on the test dataset which it has never seen before\n\n# In[27]:\n\n\nnltk.classify.util.accuracy(IntroExtro, test)*100\n\n\n# Seeing that this model has good somewhat good results, I shall repeat the same with the rest of the traits\n\n# ## Creating a classifyer for Intuition (N) and Sensing (S)\n\n# In[28]:\n\n\n# Features for the bag of words model\nfeatures=[]\nfor j in types:\n    temp1 = all_posts[j]\n    temp1 = temp1.dropna() #not all the personality types have same number of files\n    if(\'N\' in j):\n        features += [[(build_bag_of_words_features_filtered(i), \'Intuition\')         for i in temp1]]\n    if(\'E\' in j):\n        features += [[(build_bag_of_words_features_filtered(i), \'Sensing\')         for i in temp1]]\n\n\n# Data for training\n\n# In[29]:\n\n\ntrain=[]\nfor i in range(16):\n    train += features[i][:split[i]] \n\n\n# Training the model\n\n# In[30]:\n\n\nIntuitionSensing = NaiveBayesClassifier.train(train)\n\n\n# Testing the model on the dataset it was trained for accuracy\n\n# In[31]:\n\n\nnltk.classify.util.accuracy(IntuitionSensing, train)*100\n\n\n# Creating the test data\n\n# In[32]:\n\n\ntest=[]\nfor i in range(16):\n    test += features[i][split[i]:]\n\n\n# Testing the model on the test dataset which it has never seen before\n\n# In[33]:\n\n\nnltk.classify.util.accuracy(IntuitionSensing, test)*100\n\n\n# ## Creating a classifyer for Thinking (T) and Feeling (F)\n\n# In[34]:\n\n\n# Features for the bag of words model\nfeatures=[]\nfor j in types:\n    temp1 = all_posts[j]\n    temp1 = temp1.dropna() #not all the personality types have same number of files\n    if(\'T\' in j):\n        features += [[(build_bag_of_words_features_filtered(i), \'Thinking\')         for i in temp1]]\n    if(\'F\' in j):\n        features += [[(build_bag_of_words_features_filtered(i), \'Feeling\')         for i in temp1]]\n\n\n# Data for training\n\n# In[35]:\n\n\ntrain=[]\nfor i in range(16):\n    train += features[i][:split[i]] \n\n\n# Training the model\n\n# In[36]:\n\n\nThinkingFeeling = NaiveBayesClassifier.train(train)\n\n\n# Testing the model on the dataset it was trained for accuracy\n\n# In[37]:\n\n\nnltk.classify.util.accuracy(ThinkingFeeling, train)*100\n\n\n# Creating the test data\n\n# In[38]:\n\n\ntest=[]\nfor i in range(16):\n    test += features[i][split[i]:]\n\n\n# Testing the model on the test dataset which it has never seen before\n\n# In[39]:\n\n\nnltk.classify.util.accuracy(ThinkingFeeling, test)*100\n\n\n# ## Creating a classifyer for Judging (J) and Percieving (P)\n\n# In[40]:\n\n\n# Features for the bag of words model\nfeatures=[]\nfor j in types:\n    temp1 = all_posts[j]\n    temp1 = temp1.dropna() #not all the personality types have same number of files\n    if(\'J\' in j):\n        features += [[(build_bag_of_words_features_filtered(i), \'Judging\')         for i in temp1]]\n    if(\'P\' in j):\n        features += [[(build_bag_of_words_features_filtered(i), \'Percieving\')         for i in temp1]]\n\n\n# Data for training\n\n# In[41]:\n\n\ntrain=[]\nfor i in range(16):\n    train += features[i][:split[i]] \n\n\n# Training the model\n\n# In[42]:\n\n\nJudgingPercieiving = NaiveBayesClassifier.train(train)\n\n\n# Testing the model on the dataset it was trained for accuracy\n\n# In[43]:\n\n\nnltk.classify.util.accuracy(JudgingPercieiving, train)*100\n\n\n# Creating the test data\n\n# In[44]:\n\n\ntest=[]\nfor i in range(16):\n    test += features[i][split[i]:]\n\n\n# Testing the model on the test dataset which it has never seen before\n\n# In[45]:\n\n\nnltk.classify.util.accuracy(JudgingPercieiving, test)*100\n\n\n# # Summarizing the results of the models\n# ***\n# \n\n# In[46]:\n\n\ntemp = {\'train\' : [81.12443979837917,70.14524215640667,80.03456948570128,79.79341109742592], \'test\' : [58.20469312585358,54.46262259027357,59.41315234035509,54.40549600629061]}\nresults = pd.DataFrame.from_dict(temp, orient=\'index\', columns=[\'Introvert - Extrovert\', \'Intuition - Sensing\', \'Thinking - Feeling\', \'Judging - Percieiving\'])\nresults\n\n\n# Plotting the results for better appeal\n\n# In[47]:\n\n\nplt.figure(figsize = (12,6))\n\nplt.bar(np.array(results.columns), height = results.loc[\'train\'],)\nplt.xlabel(\'Personality types\', size = 14)\nplt.ylabel(\'Number of posts available\', size = 14)\nplt.title(\'Total posts for each personality type\')\n\n\n# In[48]:\n\n\nlabels = np.array(results.columns)\n\ntraining = results.loc[\'train\']\nind = np.arange(4)\nwidth = 0.4\nfig = plt.figure()\nax = fig.add_subplot(111)\nrects1 = ax.bar(ind, training, width, color=\'royalblue\')\n\ntesting = results.loc[\'test\']\nrects2 = ax.bar(ind+width, testing, width, color=\'seagreen\')\n\nfig.set_size_inches(12, 6)\nfig.savefig(\'Results.png\', dpi=200)\n\nax.set_xlabel(\'Model Classifying Trait\', size = 18)\nax.set_ylabel(\'Accuracy Percent (%)\', size = 18)\nax.set_xticks(ind + width / 2)\nax.set_xticklabels(labels)\nax.legend((rects1[0], rects2[0]), (\'Tested on a known dataframe\', \'Tested on an unknown dataframe\'))\nplt.show()\n\n\n# # Testing the models to predict my trait my feeding few of my quora writings\n# \n# link to my quora answers feed: https://www.quora.com/profile/Divya-Bramhecha\n\n# Defining a functions that inputs the writings, tokenizes them and then predicts the output based on our earlier classifiers\n\n# In[192]:\n\n\ndef MBTI(input):\n    tokenize = build_bag_of_words_features_filtered(input)\n    ie = IntroExtro.classify(tokenize)\n    Is = IntuitionSensing.classify(tokenize)\n    tf = ThinkingFeeling.classify(tokenize)\n    jp = JudgingPercieiving.classify(tokenize)\n    \n    mbt = \'\'\n    \n    if(ie == \'introvert\'):\n        mbt+=\'I\'\n    if(ie == \'extrovert\'):\n        mbt+=\'E\'\n    if(Is == \'Intuition\'):\n        mbt+=\'N\'\n    if(Is == \'Sensing\'):\n        mbt+=\'S\'\n    if(tf == \'Thinking\'):\n        mbt+=\'T\'\n    if(tf == \'Feeling\'):\n        mbt+=\'F\'\n    if(jp == \'Judging\'):\n        mbt+=\'J\'\n    if(jp == \'Percieving\'):\n        mbt+=\'P\'\n    return(mbt)\n    \n\n\n# ### Building another functions that takes all of my posts as input and outputs the graph showing percentage of each trait seen in each posts and sums up displaying your personality as the graph title\n# \n# **Note:** The input should be an array of your posts\n\n# In[243]:\n\n\ndef tellmemyMBTI(input, name, traasits=[]):\n    a = []\n    trait1 = pd.DataFrame([0,0,0,0],[\'I\',\'N\',\'T\',\'J\'],[\'count\'])\n    trait2 = pd.DataFrame([0,0,0,0],[\'E\',\'S\',\'F\',\'P\'],[\'count\'])\n    for i in input:\n        a += [MBTI(i)]\n    for i in a:\n        for j in [\'I\',\'N\',\'T\',\'J\']:\n            if(j in i):\n                trait1.loc[j]+=1                \n        for j in [\'E\',\'S\',\'F\',\'P\']:\n            if(j in i):\n                trait2.loc[j]+=1 \n    trait1 = trait1.T\n    trait1 = trait1*100/len(input)\n    trait2 = trait2.T\n    trait2 = trait2*100/len(input)\n    \n    \n    #Finding the personality\n    YourTrait = \'\'\n    for i,j in zip(trait1,trait2):\n        temp = max(trait1[i][0],trait2[j][0])\n        if(trait1[i][0]==temp):\n            YourTrait += i  \n        if(trait2[j][0]==temp):\n            YourTrait += j\n    traasits +=[YourTrait] \n    \n    #Plotting\n    \n    labels = np.array(results.columns)\n\n    intj = trait1.loc[\'count\']\n    ind = np.arange(4)\n    width = 0.4\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    rects1 = ax.bar(ind, intj, width, color=\'royalblue\')\n\n    esfp = trait2.loc[\'count\']\n    rects2 = ax.bar(ind+width, esfp, width, color=\'seagreen\')\n\n    fig.set_size_inches(10, 7)\n    \n    \n\n    ax.set_xlabel(\'Finding the MBTI Trait\', size = 18)\n    ax.set_ylabel(\'Trait Percent (%)\', size = 18)\n    ax.set_xticks(ind + width / 2)\n    ax.set_xticklabels(labels)\n    ax.set_yticks(np.arange(0,105, step= 10))\n    ax.set_title(\'Your Personality is \'+YourTrait,size = 20)\n    plt.grid(True)\n    \n    \n    fig.savefig(name+\'.png\', dpi=200)\n    \n    plt.show()\n    return(traasits)\n        \n\n\n# # Importing my quora answers from a text file\n# \n# I copied all my answer from the link i provided before (i broke down the paragraphs as separte posts)\n\n# In[244]:\n\n\nMy_writings = open(""Myquora.txt"")\nmy_writing = My_writings.readlines()\n#my_writing\n\n\n# In[245]:\n\n\nmy_posts = my_writing[0].split(\'|||\')\nlen(my_posts)\n#my_posts\n\n\n# # Using the classifier to predict my personality type\n\n# In[246]:\n\n\ntrait=tellmemyMBTI(my_posts, \'Divy\')\n\n\n# # Concluding note\n# \n# My profile according to https://www.16personalities.com/ is INTJ.\n# \n# I am pretty happy that using such a basic model it was pretty close to my real profile, only 1 different. And even that difference was very close, between 10% inaccuary which pretty good.\n# \n# Although, I am not sure how the classifier will perform on all test cases in general. Specially, the data for some profiles was very less.\n\n# # Sanaya profile\n\n# In[247]:\n\n\nMy_writings = open(""Sanayapoem.txt"")\nmy_writing = My_writings.readlines()\n#my_writing\n\n\n# In[248]:\n\n\nmy_posts = my_writing[0].split(\'|||\')\nlen(my_posts)\n#my_posts\n\n\n# In[249]:\n\n\ntrait = tellmemyMBTI(my_posts,\'sanaya\')\n\n\n# # Valentin Pyataev\n\n# In[250]:\n\n\nMy_writings = open(""Valentin pyatev.txt"")\nmy_writing = My_writings.readlines()\n#my_writing\n\n\n# In[251]:\n\n\nmy_posts = my_writing[0].split(\'|||\')\nlen(my_posts)\n#my_posts\n\n\n# In[252]:\n\n\ntrait=tellmemyMBTI(my_posts,\'Valentin\')\n\n\n# # MIT gurukul people\n\n# In[253]:\n\n\nMy_writings = open(""All texts.txt"")\nmy_writing = My_writings.readlines()\na =[\'\'];\nfor i in my_writing:\n    a[0]=a[0]+i\nlen(a)\n\n\n# In[254]:\n\n\nmy_posts = a[0].split(\'&&&\')\nlen(my_posts)\n#my_posts\n\n\n# Posts for each person\n\n# In[255]:\n\n\nalls = [None]*len(my_posts)\nfor i in range(len(my_posts)):\n    alls[i] = my_posts[i].split(\'|||\') \n\n\n# Email ID connection\n\n# In[256]:\n\n\nNames = open(""Names.txt"")\nnames = Names.readlines()\n#names\n\n\n# In[257]:\n\n\nfor i in range(len(names)):\n    names[i] = names[i].replace(\'@gmail.com\\n\',\'\')\n    print(names[i])\nnames[len(names)-1]=names[len(names)-1].replace(\'@gmail.com\',\'\')\n\n\n# In[258]:\n\n\nfor i in range(len(alls)):\n    trait=tellmemyMBTI(alls[i],names[i])\n\n\n# In[260]:\n\n\ntrait\n\n'"
