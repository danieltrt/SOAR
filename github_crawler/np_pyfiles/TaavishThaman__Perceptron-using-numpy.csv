file_path,api_count,code
perceptron.py,13,"b'import numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn import datasets\r\nnp.random.seed(0)\r\n\r\n#Linearly Seperable dataset using sklearn with 2 features\r\nX, y = datasets.make_blobs(n_samples = 1000, centers = 2, cluster_std=1.5)\r\nplt.scatter(X[:,0],X[:,1], c=y)\r\n\r\n#Split dataset into train and test sets\r\nfrom sklearn.model_selection import train_test_split\r\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size =0.3, random_state=1)\r\n\r\n#Flattening\r\nX_train_flatten = X_train.reshape(X_train.shape[0], -1).T\r\nX_test_flatten = X_test.reshape(X_test.shape[0], -1).T\r\n\r\n#Standardization\r\nX_train_flatten = (X_train_flatten)/(X_train_flatten.max())\r\nX_test_flatten = (X_test_flatten)/(X_test_flatten.max())\r\n\r\n#Sigmoid function\r\ndef sigmoid(z):\r\n    s = 1/(1 + np.exp(-z))\r\n    return s\r\n\r\n#initialize_weights randomly\r\ndef initialize_weights(dim):\r\n\r\n    W = np.random.normal((dim,1))\r\n    b = 0\r\n    return W, b\r\n    \r\n    assert(W.shape == (dim, 1))\r\n    assert(isinstance(b, float) or isinstance(b, int))\r\n\r\n#Propagation Function\r\ndef propagate_func(W, b, X, Y):\r\n    \r\n    #Forward Prop\r\n    m = X.shape[1]\r\n    A = sigmoid(np.dot(W.T, X) + b)\r\n    cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))\r\n    #Forward Prop\r\n    \r\n    #Backward Prop\r\n    dW = (1/m)*np.dot(X, (A-Y).T)\r\n    db = (1/m)*np.sum(A-Y)\r\n    #Backward Prop\r\n    \r\n    assert(dW.shape == W.shape)\r\n    assert(db.dtype == float)\r\n    cost = np.squeeze(cost)\r\n    assert(cost.shape == ())\r\n    \r\n    grad_dict = {""dW"":dW , ""db"":db}\r\n    return grad_dict, cost\r\n\r\n#Optimization using gradient descent\r\ndef optimize(W, b, X, Y, num_iterations, learning_rate, print_cost = False):\r\n    \r\n    costs = []\r\n    \r\n    for i in range(num_iterations):\r\n        \r\n        grad_dict, cost = propagate_func(W,b,X,Y)\r\n        \r\n        dW = grad_dict[""dW""]\r\n        db = grad_dict[""db""]\r\n        \r\n        #Gradient Descent\r\n        W = W - learning_rate * dW\r\n        b = b - learning_rate * db\r\n        #Gradient Descent\r\n        \r\n        if i%1000 == 0:\r\n            costs.append(cost)\r\n            \r\n        if print_cost and i % 1000 == 0:\r\n            print (""Cost after iteration %i: %f"" % (i, cost))\r\n            \r\n    param_dict = {""W"":W, ""b"":b}\r\n    grad_dict = {""dW"":dW, ""db"":db}\r\n    \r\n    return param_dict, grad_dict, costs\r\n\r\n#Prediction function\r\ndef predict(W, b, X):\r\n    m = X.shape[1]\r\n    Y_pred = np.zeros((1,m))\r\n    W = W.reshape(X.shape[0], 1)\r\n    \r\n    A  = sigmoid(np.dot(W.T, X)+ b)\r\n    \r\n    for i in range(m):\r\n        Y_pred[0,i] = 1 if A[0,i] > 0.5 else 0\r\n        \r\n    assert(Y_pred.shape == (1, m))\r\n        \r\n    return Y_pred\r\n\r\n#Perceptron Model\r\ndef model(X_train, Y_train , X_test, Y_test, num_iterations, learning_rate, print_cost = False):\r\n    \r\n    W, b = initialize_weights(X_train.shape[0])\r\n    param_dict, grad_dict, costs = optimize(W, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\r\n    \r\n    W = param_dict[""W""]\r\n    b = param_dict[""b""]\r\n    \r\n    Y_pred_test = predict(W, b ,X_test)\r\n    Y_pred_train = predict(W, b, X_train)\r\n    \r\n    print(""train accuracy: {} %"".format(100 - np.mean(np.abs(Y_pred_train - Y_train)) * 100))\r\n    print(""test accuracy: {} %"".format(100 - np.mean(np.abs(Y_pred_test - Y_test)) * 100))\r\n    \r\n    dict = {""costs"": costs,\r\n            ""Y_pred_test"": Y_pred_test, \r\n            ""Y_pred_train"" : Y_pred_train, \r\n            ""W"" : W, \r\n            ""b"" : b,\r\n            ""learning_rate"" : learning_rate,\r\n            ""num_iterations"": num_iterations}\r\n    \r\n    return dict\r\n\r\ndict = model(X_train_flatten, Y_train, X_test_flatten, Y_test, num_iterations = 100000, learning_rate =0.001, print_cost = True)\r\n\r\nW = dict[""W""]\r\nb = dict[""b""] \r\n\r\n#Linearly seperated values x_vals\r\nx_vals = np.linspace(-3,6,10).reshape((10,1))\r\n#y_vals corresponding to x_vals using eqn W[0]*X1 + W[1]*X2 + b = 0\r\ny_vals = -1*((W[0]*x_vals + b)/(W[1])) \r\n\r\nplt.xlabel(\'X1\')\r\nplt.ylabel(\'X2\')\r\nplt.scatter(X[:,0], X[:,1], c=y, s=0.003)\r\nplt.plot(x_vals, y_vals)\r\n\r\n        \r\n    \r\n    \r\n    \r\n    \r\n'"
