file_path,api_count,code
setup.py,0,"b'# -*- coding: utf-8 -*-\n#\n""""""Setuptools-based setup script for pydgq.""""""\n\nfrom __future__ import division, print_function, absolute_import\n\nimport os\n\n# remove file, ignore error if it did not exist\n#\n# portable version for Python 2.7 and 3.x\n#\ndef myremove(filename):\n    try:\n        exc = FileNotFoundError  # this does not exist in Python 2.7\n    except:\n        exc = OSError\n\n    try:\n        os.remove( redirect_name )\n    except exc:\n        pass\n\n\n#########################################################\n# Config\n#########################################################\n\n# choose build type here\n#\nbuild_type=""optimized""\n#build_type=""debug""\n\n\n#########################################################\n# Init\n#########################################################\n\n# check for Python 2.7 or later\n# http://stackoverflow.com/questions/19534896/enforcing-python-version-in-setup-py\nimport sys\nif sys.version_info < (2,7):\n    sys.exit(\'Sorry, Python < 2.7 is not supported\')\n\nfrom setuptools import setup\nfrom setuptools.extension import Extension\n\ntry:\n    from Cython.Build import cythonize\nexcept ImportError:\n    sys.exit(""Cython not found. Cython is needed to build the extension modules for pydgq."")\n\n\n#########################################################\n# HACK: Attempt to package the correct data file on POSIX\n#########################################################\n\nredirect_name = os.path.join(""pydgq"", ""pydgq_data.bin"")\nfile_27_name  = os.path.join(""pydgq"", ""pydgq_data_27.bin"")\nfile_34_name  = os.path.join(""pydgq"", ""pydgq_data_34.bin"")\n\n# remove existing symlink or file if any\n#\nmyremove( redirect_name )\n\n# symlink correct file depending on Python version\n#\n# (FIXME/TODO: for now, we assume that setup.py is running under the same Python version the packaged library will run under)\n#\nif sys.version_info < (3,0):\n    print(""Packaging %s as %s"" % (file_27_name, redirect_name))\n    os.symlink( file_27_name, redirect_name )\nelse:\n    print(""Packaging %s as %s"" % (file_34_name, redirect_name))\n    os.symlink( file_34_name, redirect_name )\n\n\n#########################################################\n# Definitions\n#########################################################\n\nextra_compile_args_math_optimized    = [\'-march=native\', \'-O2\', \'-msse\', \'-msse2\', \'-mfma\', \'-mfpmath=sse\']\nextra_compile_args_math_debug        = [\'-march=native\', \'-O0\', \'-g\']\n\nextra_compile_args_nonmath_optimized = [\'-O2\']\nextra_compile_args_nonmath_debug     = [\'-O0\', \'-g\']\n\nextra_link_args_optimized    = []\nextra_link_args_debug        = []\n\n\nif build_type == \'optimized\':\n    my_extra_compile_args_math    = extra_compile_args_math_optimized\n    my_extra_compile_args_nonmath = extra_compile_args_nonmath_optimized\n    my_extra_link_args            = extra_link_args_optimized\n    debug = False\n    print( ""build configuration selected: optimized"" )\nelse: # build_type == \'debug\':\n    my_extra_compile_args_math    = extra_compile_args_math_debug\n    my_extra_compile_args_nonmath = extra_compile_args_nonmath_debug\n    my_extra_link_args            = extra_link_args_debug\n    debug = True\n    print( ""build configuration selected: debug"" )\n\n\n#########################################################\n# Long description\n#########################################################\n\nDESC=""""""Integrate first-order ODE systems  u\'(t) = f(u, t).\n\nThe main feature of this library is dG(q), i.e. the\ntime-discontinuous Galerkin method using a Lobatto basis\n(a.k.a. hierarchical polynomial basis).\n\ndG(q) is a very accurate implicit method that often allows\nusing a rather large timestep. Due to its Galerkin nature,\nit also allows inspecting the behavior of the solution\ninside the timestep.\n\nArbitrary q is supported, but often best results are\nobtained for q=1 or q=2.\n\nSome classical integrators (RK2, RK3, RK4, IMR, BE)\nare also provided for convenience.\n\nThe focus is on arbitrary nonlinear problems; all implicit\nmethods are implemented using fixed-point (Banach/Picard)\niteration.\n""""""\n\n\n#########################################################\n# Helpers\n#########################################################\n\nmy_include_dirs = ["".""]  # IMPORTANT, see https://github.com/cython/cython/wiki/PackageHierarchy\n\ndef ext(extName):\n    extPath = extName.replace(""."", os.path.sep)+"".pyx""\n    return Extension( extName,\n                      [extPath],\n                      extra_compile_args=my_extra_compile_args_nonmath\n                    )\ndef ext_math(extName):\n    extPath = extName.replace(""."", os.path.sep)+"".pyx""\n    return Extension( extName,\n                      [extPath],\n                      extra_compile_args=my_extra_compile_args_math,\n                      extra_link_args=my_extra_link_args,\n                      libraries=[""m""]  # ""m"" links libm, the math library on unix-likes; see http://docs.cython.org/src/tutorial/external.html\n                    )\n\n# http://stackoverflow.com/questions/13628979/setuptools-how-to-make-package-contain-extra-data-folder-and-all-folders-inside\ndatadirs  = (""doc"", ""test"")\ndataexts  = ("".py"", "".pyx"", "".pxd"", "".c"", "".sh"", "".lyx"", "".pdf"")\ndatafiles = []\ngetext = lambda filename: os.path.splitext(filename)[1]\nfor datadir in datadirs:\n    datafiles.extend( [(root, [os.path.join(root, f) for f in files if getext(f) in dataexts])\n                       for root, dirs, files in os.walk(datadir)] )\n\ndatafiles.append( (\'.\', [""README.md"", ""LICENSE.md"", ""TODO.md"", ""CHANGELOG.md""]) )\n\n#########################################################\n# Modules\n#########################################################\n\next_module_types      = ext(      ""pydgq.solver.types""                )\n\next_module_compsum    = ext_math( ""pydgq.solver.compsum""              )\n\next_module_discontify = ext_math( ""pydgq.utils.discontify""            )\n\next_module_kernintf   = ext_math( ""pydgq.solver.kernel_interface""     )\next_module_bkernels   = ext_math( ""pydgq.solver.builtin_kernels""      )\n\next_module_intgintf   = ext_math( ""pydgq.solver.integrator_interface"" )\next_module_explicit   = ext_math( ""pydgq.solver.explicit""             )\next_module_implicit   = ext_math( ""pydgq.solver.implicit""             )\next_module_galerkin   = ext_math( ""pydgq.solver.galerkin""             )\n\next_module_odesolve   = ext_math( ""pydgq.solver.odesolve""             )\n\n#########################################################\n\n# Extract __version__ from the package __init__.py\n# (since it\'s not a good idea to actually run __init__.py during the build process).\n#\n# http://stackoverflow.com/questions/2058802/how-can-i-get-the-version-defined-in-setup-py-setuptools-in-my-package\n#\nimport ast\nwith open(\'pydgq/__init__.py\') as f:\n    for line in f:\n        if line.startswith(\'__version__\'):\n            version = ast.parse(line).body[0].value.s\n            break\n    else:\n        version = \'0.0.unknown\'\n        print( ""WARNING: Version information not found, using placeholder \'%s\'"" % (version) )\n\n\nsetup(\n    name = ""pydgq"",\n    version = version,\n    author = ""Juha Jeronen"",\n    author_email = ""juha.jeronen@jyu.fi"",\n    url = ""https://github.com/Technologicat/pydgq"",\n\n    description = ""ODE system solver using dG(q) (time-discontinuous Galerkin w/ Lobatto basis)"",\n    long_description = DESC,\n\n    license = ""BSD"",\n    platforms = [""Linux""],  # free-form text field; http://stackoverflow.com/questions/34994130/what-platforms-argument-to-setup-in-setup-py-does\n\n    classifiers = [ ""Development Status :: 4 - Beta"",\n                    ""Environment :: Console"",\n                    ""Intended Audience :: Developers"",\n                    ""Intended Audience :: Science/Research"",\n                    ""License :: OSI Approved :: BSD License"",\n                    ""Operating System :: POSIX :: Linux"",\n                    ""Programming Language :: Cython"",\n                    ""Programming Language :: Python"",\n                    ""Programming Language :: Python :: 2"",\n                    ""Programming Language :: Python :: 2.7"",\n                    ""Programming Language :: Python :: 3"",\n                    ""Programming Language :: Python :: 3.4"",\n                    ""Topic :: Scientific/Engineering"",\n                    ""Topic :: Scientific/Engineering :: Mathematics"",\n                    ""Topic :: Software Development :: Libraries"",\n                    ""Topic :: Software Development :: Libraries :: Python Modules""\n                  ],\n\n    setup_requires = [""cython"", ""numpy""],\n    # pydgq.utils.precalc can optionally use mpi4py, but it is not mandatory.\n    # Also, thre no need to run precalc in common use cases, since we include\n    # the data file pydgq_data.bin (computed using default options) in the package.\n    # Thus, we just leave out mpi4py.\n    install_requires = [""numpy"", ""pylu""],\n    provides = [""pydgq""],\n\n    # same keywords as used as topics on GitHub\n    keywords = [""numerical integration ordinary-differential-equations ode ivp ode-solver solver galerkin discontinuous-galerkin cython numpy""],\n\n    ext_modules = cythonize( [ ext_module_types,\n                               ext_module_compsum,\n                               ext_module_discontify,\n                               ext_module_kernintf, ext_module_bkernels,\n                               ext_module_intgintf, ext_module_explicit, ext_module_implicit, ext_module_galerkin,\n                               ext_module_odesolve,  ],\n                             include_path = my_include_dirs,\n                             gdb_debug = debug ),\n\n    # Declare packages so that  python -m setup build  will copy .py files (especially __init__.py).\n    packages = [""pydgq"", ""pydgq.solver"", ""pydgq.utils""],\n\n    # Install also Cython headers so that other Cython modules can cimport ours\n    # FIXME: force sdist, but sdist only, to keep the .pyx files (this puts them also in the bdist)\n    package_data={\'pydgq\':        [\'*.pxd\', \'*.pyx\', \'*.bin\'],  # note: paths relative to each package\n                  \'pydgq.solver\': [\'*.pxd\', \'*.pyx\'],\n                  \'pydgq.utils\':  [\'*.pxd\', \'*.pyx\']},\n\n    # Disable zip_safe, because:\n    #   - Cython won\'t find .pxd files inside installed .egg, hard to compile libs depending on this one\n    #   - dynamic loader may need to have the library unzipped to a temporary folder anyway (at import time)\n    zip_safe = False,\n\n    # Usage examples; not in a package\n    data_files = datafiles\n)\n\n\n# remove symlink created earlier\n#\nmyremove( redirect_name )\n\n'"
doc/legtest.py,5,"b""# -*- coding: utf-8 -*-\n#\n# Trying out the NumPy API for Legendre polynomials and Gauss--Legendre quadrature.\n#\n# JJ 2016-02-16\n\nfrom __future__ import division, print_function, absolute_import\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef get_legendre_polynomials(max_degree):\n    polynomials = []\n\n    # For each degree d, get the polynomial coefficients of a Legendre series\n    # that has only the dth degree term. Construct the corresponding Polynomial object,\n    #\n    # The coefficients are listed from the lowest order to highest.\n    #\n    for d in range(max_degree):\n        # d zeroes followed by a one\n        #\n        series_coeffs = [ 0. ] * d\n        series_coeffs.append( 1. )\n\n        # coefficients for a standard power series 1, x, x**2, ...\n        #\n        c = np.polynomial.legendre.leg2poly( series_coeffs )\n\n        # create the Polynomial object, remapping the input range to [0,1] for convenience\n        #\n        polynomials.append( np.polynomial.Polynomial( c, domain=[0., 1.], window=[-1., 1.] ) )\n\n    return polynomials\n\n\ndef main():\n    # Set the maximum degree.\n    #\n    # For our purposes, for accurate results (all polynomials staying within [-1,1] for the whole interval)\n    # it seems d = 30 is about the upper limit of what this implementation can do.\n    #\n    d = 30\n\n    # From the API docs for numpy.polynomial.legendre.leggauss:\n    #    Computes the sample points and weights for Gauss-Legendre quadrature.\n    #    These sample points and weights will correctly integrate polynomials of degree 2*deg - 1 or less over the interval [-1, 1] with the weight function f(x) = 1.\n    #\n    # Hence, in Galerkin methods, to exactly handle a mass matrix where neither of the terms is differentiated, using affine mapping to the reference element [0,1]\n    # (implying piecewise constant Jacobian), we need to have\n    #\n    #   2*deg - 1 = 2*d\n    #\n    # i.e.\n    #\n    #   deg = (2*d + 1) / 2\n    #\n#    deg = int(np.ceil( (2*d + 1)/2. ))\n#    q,w = np.polynomial.legendre.leggauss( deg )\n#    print( deg,(2*deg-1),q,w )\n\n    P = get_legendre_polynomials(d)\n    xx = np.linspace(0., 1., 501)\n\n    plt.figure(1)\n    plt.clf()\n    for p in P:\n        plt.plot( xx, p(xx) )\n\n    plt.axis('tight')\n    a = plt.axis()\n    plt.axis( [ a[0], a[1], a[2]*1.05, a[3]*1.05 ] )\n\n    plt.grid(b=True, which='both')\n    plt.title('Legendre polynomials')\n\n\n    # Try some operations\n\n    # As long as we keep the Polynomial objects, we can multiply them the intuitive way, producing a new Polynomial:\n    #\n    print( P[2]*P[3] )  # => poly([ 0.    0.75  0.   -3.5   0.    3.75])\n\n    # We can also differentiate them, which is useful for constructing the mass matrix:\n    #\n    print( P[2].deriv(1)*P[3] )  # => poly([  0.   0.  -9.   0.  15.])\n\n    # Also integration is supported.\n    #\n    # p.integ() returns the definite integral, as a Polynomial object, from lbnd to an unspecified upper limit x, adding the integration constant k.\n    # The value of x is chosen when calling the resulting object.\n    #\n    # Legendre polynomials are L2-orthogonal on [-1,1]; the scaled ones are orthogonal on [0,1]:\n    print( ( (P[2]*P[2]).integ(lbnd=0, k=0) )(1.0) )  # 1/(2 n + 1);  here n = 2, so this = 1/5 = 0.2\n    print( ( (P[2]*P[3]).integ(lbnd=0, k=0) )(1.0) )  # zero\n\n    # The integral of  dPn/dx * Pm  over the interval is zero if:\n    #\n    #  - n + m is even\n    #  - n < m  (and by the previous condition, also  n <= m)\n    #\n    # These observations are based on the L2-orthogonality and the relation\n    #\n    #   (2 n + 1) P_n = (d/dx)( P_{n+1} - P_{n-1} )         (*)\n    #\n    # which can be used to get rid of the derivative. The relation (*) follows from Bonnet\xe2\x80\x99s recursion formula,\n    #\n    #   (n + 1) P_{n+1} = (2 n + 1) P_n - n P_{n-1}\n    #\n    # By recursive application, (*) leads to the representation\n    #\n    #   (d/dx) P_{n+1} = (2 n + 1) P_n + ( 2 (n - 2) + 1 ) P_{n-2} + ( 2 (n - 4) + 1 ) P_{n-4} + ...\n    #\n    # which is guaranteed to bottom out at P_1 and P_0 (by using  P_0 = 1  and  P_1 = x  in (*)).\n    #\n    # See\n    #  https://en.wikipedia.org/wiki/Legendre_polynomials#Additional_properties_of_Legendre_polynomials\n    #\n    print( ( (P[3].deriv(1)*P[3]).integ(lbnd=0, k=0) )(1.0) )  # zero, n + m even\n    print( ( (P[3].deriv(1)*P[1]).integ(lbnd=0, k=0) )(1.0) )  # zero, n + m even\n    print( ( (P[2].deriv(1)*P[3]).integ(lbnd=0, k=0) )(1.0) )  # zero, n < m\n    print( ( (P[3].deriv(1)*P[2]).integ(lbnd=0, k=0) )(1.0) )  # nonzero (derivative of p3 contains p2, p0)\n\n\nif __name__ == '__main__':\n    main()\n    plt.show()\n\n"""
doc/legtest2.py,23,"b'# -*- coding: utf-8 -*-\n#\n# Trying out the NumPy API for Legendre polynomials and Gauss--Legendre quadrature,\n# with an eye toward the modern hierarchical (Lobatto) basis functions for Galerkin methods\n# (B. Szab\xc3\xb3, I. Babu\xc5\xa1ka, Finite element analysis, John Wiley & Sons, 1991).\n#\n# JJ 2016-02-16\n\nfrom __future__ import division, print_function, absolute_import\n\nimport time\n\nimport numpy as np\nimport scipy.integrate\nimport matplotlib.pyplot as plt\n\nimport pylu.dgesv as dgesv\n\n\nclass RandomPileOfTestStuff:\n    def __init__(self, q=15, tol=1e-8):\n        assert( q >= 2 )  # we don\'t have special case handling for q=1 in build_hierarchical_basis()\n\n        self.q   = q     # max polynomial degree for Legendre polynomials; number of basis functions for hierarchical basis (as in ""dG(q)"")\n        self.tol = tol   # tolerance for nonzero check\n\n        self.P   = None  # Legendre polynomials\n        self.N   = None  # hierarchical basis functions (FEM, dG)\n\n        self.C   = None  # dG mass matrix for the first-order problem u\' = f(u, t)\n\n        self.get_legendre_polynomials()\n        self.build_hierarchical_basis()\n        self.dgmass()\n\n    def get_legendre_polynomials(self):\n        q = self.q\n\n        P = []\n\n        # For each degree d, get the polynomial coefficients of a Legendre series\n        # that has only the dth degree term. Construct the corresponding Polynomial object.\n        #\n        # The coefficients are listed from the lowest order to highest.\n        #\n        for d in range(q):\n            # d zeroes followed by a one\n            #\n            series_coeffs = [ 0. ] * d\n            series_coeffs.append( 1. )\n\n            # coefficients for a standard power series 1, x, x**2, ...\n            #\n            c = np.polynomial.legendre.leg2poly( series_coeffs )\n\n            P.append( np.polynomial.Polynomial( c ) )\n\n        self.P = P\n\n    def build_hierarchical_basis(self):\n        assert( self.P is not None )\n\n        q = self.q\n        P = self.P\n\n        N = []\n        N.append( np.polynomial.Polynomial( [0.5, -0.5] ) )  # N_1, will become N[0] in the code, using Polynomial instead of explicit lambda gets us support for .deriv()\n        N.append( np.polynomial.Polynomial( [0.5,  0.5] ) )  # N_2\n        for j in range(2,q):\n            #N.append( np.sqrt( (2.*j - 1.)/2.) * P[j-1].integ(lbnd=-1, k=0) )  # surely this approach makes no numerical sense\n\n            # Explicit solution, using NumPy to evaluate the sum of Legendre polynomials.\n            #\n            # Much better (and still fast), but not nearly as accurate as evaluating using higher precision internally. See legtest3.py.\n            #\n            series_coeffs = [ 0. ] * (j-2)\n            series_coeffs.extend( [-1., 0., 1.] )  # -P_{j-2} + P_{j}\n            c = np.polynomial.legendre.leg2poly( series_coeffs )\n            Nj = np.polynomial.Polynomial(c)  /  np.sqrt( 2. * (2.*j - 1.) )\n            N.append( Nj )\n\n        self.N = N\n\n    # This numerical approach for generating the matrix is prone to roundoff and obsolete (not to mention stupid\n    # since we know that most of the matrix entries should be zero); see the analytical solution in legtest3.py.\n    #\n    def dgmass(self):\n        assert( self.N is not None )\n\n        q = self.q\n        N = self.N\n\n        C = np.empty( (q,q), dtype=np.float64 )\n        for i in range(q):\n            for j in range(q):\n                C[i,j] = scipy.integrate.quad( N[j].deriv(1)*N[i], -1., 1. )[0]\n        C[ np.abs(C) < self.tol ] = 0.0\n        C[0,0] += 1.0  # simulate the effect of the jump term (N_1 is the only function that is nonzero at xi=-1)\n\n        self.C = C\n\n\ndef main():\n    # Up to q=24, the full script works despite warnings from quad() in dgmass().\n    #\n    # For evaluating the hierarchical basis functions only (no dgmass()):\n    #\n    #   q = 30, still sort of works, small deviations (1e-7) can be seen in the endpoint values of the few highest-order Nj\n    #   q = 40, almost works, high-order Nj start getting wobbly\n    #   q = 50, completely broken, out of precision\n    #\n    # By comparison, legtest3.py, which uses SymPy\'s mpmath (arbitrary precision floating point), works at least up to q=300, but is very slow.\n    #\n    stuff = RandomPileOfTestStuff(q=24, tol=1e-3)\n\n    # From the API docs for numpy.polynomial.legendre.leggauss:\n    #    Computes the sample points and weights for Gauss-Legendre quadrature.\n    #    These sample points and weights will correctly integrate polynomials of degree 2*deg - 1 or less over the interval [-1, 1] with the weight function f(x) = 1.\n    #\n    # Hence, in Galerkin methods, to exactly handle a mass matrix where neither of the terms is differentiated, using affine mapping to the reference element [-1,1]\n    # (implying piecewise constant Jacobian), we need to have\n    #\n    #   2*deg - 1 = 2*d\n    #\n    # i.e.\n    #\n    #   deg = (2*d + 1) / 2\n    #\n#    deg = int(np.ceil( (2*d + 1)/2. ))\n#    q,w = np.polynomial.legendre.leggauss( deg )\n#    print( deg,(2*deg-1),q,w )\n\n    print( stuff.C )\n    print( np.linalg.matrix_rank(stuff.C) )  # should be full rank\n    plt.figure(2)\n    plt.spy(stuff.C)\n    plt.plot( [0,stuff.q-1], [0,stuff.q-1], \'r--\' )\n#    plt.imshow(M, interpolation=""nearest"", cmap=""Oranges"")\n#    plt.colorbar()\n    plt.title(r""$\\mathbf{M}$"")\n\n\n##    L,U,p = dgesv.lup(stuff.C)\n##    print( np.transpose(np.nonzero(L)) )\n##    print( np.transpose(np.nonzero(U)) )\n##    print( p )\n##    plt.figure(3)\n##    plt.subplot(1,2, 1)\n##    plt.spy(L)\n##    plt.plot( [0,stuff.q-1], [0,stuff.q-1], \'r--\' )\n###    plt.imshow(L, interpolation=""nearest"", cmap=""Oranges"")\n###    plt.colorbar(orientation=""horizontal"")\n##    plt.title(r""$\\mathbf{L}$"")\n##    plt.subplot(1,2, 2)\n##    plt.spy(U)\n##    plt.plot( [0,stuff.q-1], [0,stuff.q-1], \'r--\' )\n###    plt.imshow(U, interpolation=""nearest"", cmap=""Oranges"")\n###    plt.colorbar(orientation=""horizontal"")\n##    plt.title(r""$\\mathbf{U}$"")\n\n\n    LU,p = dgesv.lup_packed(stuff.C)\n    plt.figure(4)\n    plt.spy(LU)\n    plt.plot( [0,stuff.q-1], [0,stuff.q-1], \'r--\' )\n    plt.title(r""$\\mathbf{LU}$ (packed format)"")\n\n    mincols,maxcols = dgesv.find_bands(LU, 1e-15)\n    print( mincols, maxcols )\n\n\n##    # old Python-based mincols, maxcols finding code\n##\n##    # Find the smallest column index with nonzero data on each row in L.\n##    #\n##    # We can use this to ""sparsify"" the backsolve even though the data structure is dense.\n##    #\n##    # This assumes that each row has at least one nonzero entry (which is always the case for an invertible matrix).\n##    #\n##    Lnz = np.nonzero(L)\n##    mincols = []\n##    rowprev = -1\n##    n = len(Lnz[0])\n##    i = 0\n##    while i < n:\n##        if Lnz[0][i] != rowprev:\n##            mincols.append(Lnz[1][i])\n##            rowprev = Lnz[0][i]\n##        i += 1\n##    mincols = np.array( mincols, dtype=np.intc, order=""C"" )\n##    print( L )\n##    print( mincols )\n\n##    # Find the largest column index with nonzero data on each row in U.\n##    #\n##    # We can use this to ""sparsify"" the backsolve even though the data structure is dense.\n##    #\n##    # This assumes that each row has at least one nonzero entry (which is always the case for an invertible matrix).\n##    #\n##    Unz = np.nonzero(U)\n##    maxcols = []\n##    rowprev = -1\n##    n = len(Unz[0])\n##    i = n - 1\n##    while i >= 0:\n##        if Unz[0][i] != rowprev:\n##            maxcols.append(Unz[1][i])\n##            rowprev = Unz[0][i]\n##        i -= 1\n##    maxcols.reverse()\n##    maxcols = np.array( maxcols, dtype=np.intc, order=""C"" )\n##    print( U )\n##    print( maxcols )\n\n\n    # Visualize\n    #\n    xx = np.linspace(-1., 1., 100001)  # the good thing about the fast approach... smooth curves!\n    plt.figure(1)\n    plt.clf()\n    for func in stuff.N:\n        plt.plot( xx, func(xx) )\n\n    plt.axis(\'tight\')\n    a = plt.axis()\n    plt.axis( [ a[0], a[1], a[2]*1.05, a[3]*1.05 ] )\n\n    plt.grid(b=True, which=\'both\')\n    plt.title(\'Hierarchical basis functions\')\n\n\n    # Try some operations on the original Legendre polynomials\n    #\n    # As long as we keep the Polynomial objects, we can multiply them the intuitive way, producing a new Polynomial:\n    #\n    print( stuff.P[2]*stuff.P[3] )  # => poly([ 0.    0.75  0.   -3.5   0.    3.75])\n\n    # We can also differentiate them, which is useful for constructing the mass matrix:\n    #\n    print( stuff.P[2].deriv(1)*stuff.P[3] )  # => poly([  0.   0.  -9.   0.  15.])\n\n    # Also integration is supported.\n    #\n    # p.integ() returns the definite integral, as a Polynomial object, from lbnd to an unspecified upper limit x, adding the integration constant k.\n    # The value of x is chosen when calling the resulting object.\n    #\n    # Legendre polynomials are L2-orthogonal on [-1,1]:\n    print( ( (stuff.P[2]*stuff.P[2]).integ(lbnd=-1, k=0) )(1.0) )  # 2/(2 n + 1);  here n = 2, so this = 2/5 = 0.4\n    print( ( (stuff.P[2]*stuff.P[3]).integ(lbnd=-1, k=0) )(1.0) )  # zero\n\n    # The integral of  dPn/dx * Pm  over the interval is zero if:\n    #\n    #  - n + m is even\n    #  - n < m  (and by the previous condition, also  n <= m)\n    #\n    # These observations are based on the L2-orthogonality and the relation\n    #\n    #   (2 n + 1) P_n = (d/dx)( P_{n+1} - P_{n-1} )         (*)\n    #\n    # which can be used to get rid of the derivative. The relation (*) follows from Bonnet\xe2\x80\x99s recursion formula,\n    #\n    #   (n + 1) P_{n+1} = (2 n + 1) P_n - n P_{n-1}\n    #\n    # By recursive application, (*) leads to the representation\n    #\n    #   (d/dx) P_{n+1} = (2 n + 1) P_n + ( 2 (n - 2) + 1 ) P_{n-2} + ( 2 (n - 4) + 1 ) P_{n-4} + ...\n    #\n    # which is guaranteed to bottom out at P_1 and P_0 (by using  P_0 = 1  and  P_1 = x  in (*)).\n    #\n    # See\n    #  https://en.wikipedia.org/wiki/Legendre_polynomials#Additional_properties_of_Legendre_polynomials\n    #\n    print( ( (stuff.P[3].deriv(1)*stuff.P[3]).integ(lbnd=-1, k=0) )(1.0) )  # zero, n + m even\n    print( ( (stuff.P[3].deriv(1)*stuff.P[1]).integ(lbnd=-1, k=0) )(1.0) )  # zero, n + m even\n    print( ( (stuff.P[2].deriv(1)*stuff.P[3]).integ(lbnd=-1, k=0) )(1.0) )  # zero, n < m\n    print( ( (stuff.P[3].deriv(1)*stuff.P[2]).integ(lbnd=-1, k=0) )(1.0) )  # nonzero (derivative of p3 contains p2, p0)\n\n\n# naive solve (repeat the LU decomposition process each time)\n#\ndef method1(reps, A, b, x):\n    for j in range(reps):\n#        dgesv.solve( A, b[j,:], x )\n        dgesv.solve( A, b, x )\n\n# decompose once, then solve\n#\ndef method2(reps, A, b, x):\n    LU,p = dgesv.lup_packed(A)\n    for j in range(reps):\n#        dgesv.solve_decomposed( LU, p, b[j,:], x )\n        dgesv.solve_decomposed( LU, p, b, x )\n\n# decompose once, then solve, utilize banded structure\n#\ndef method3(reps, A, b, x):\n    LU,p = dgesv.lup_packed(A)\n    mincols,maxcols = dgesv.find_bands(LU, 1e-15)\n    for j in range(reps):\n#        dgesv.solve_decomposed_banded( LU, p, mincols, maxcols, b[j,:], x )\n        dgesv.solve_decomposed_banded( LU, p, mincols, maxcols, b, x )\n\n\nclass MyTimer:\n    t0 = None\n    l  = None\n\n    def __init__(self, label=""""):\n        self.label = label\n\n    def __enter__(self):\n        self.t0 = time.time()\n\n    def __exit__(self, type, value, traceback):\n        dt = time.time() - self.t0\n        l  = (""%s: "" % self.label) if len(self.label) else ""time taken: ""\n        print( ""%s%gs"" % (l, dt) )\n\n\nif __name__ == \'__main__\':\n    main()\n    plt.show()\n\n#    # Running the benchmark loop at the Python end makes the banded version look slower (for our matrix M, the C code is actually ~3x faster than the generic non-banded version),\n#    # because a large majority of the execution time is taken up by data conversion from Python to C and back (and Python asserts, if enabled).\n#    #\n#    # To get reliable results on the C code only (which is a realistic use case if used from inside a Cython-accelerated solver, which is the whole point of dgesv.pyx),\n#    # the looping must be done inside dgesv.pyx.\n#    #\n#    reps = 100000\n#    for q in range(3, 16):\n#        stuff = RandomPileOfTestStuff(q)\n#        n = np.shape(stuff.C)[0]\n##        b = np.random.uniform(0.0, 1.0, size=(reps,n))  # this makes slicing part of the performance measurement - not good\n#        b = np.random.uniform(0.0, 1.0, size=(n,))\n#        x = np.empty( [n], dtype=np.float64, order=""C"" )\n\n#        print( ""Timings for %d runs"" % reps )\n#        with MyTimer(""%dx%d naive"" % (n,n)) as mt:\n#            method1(reps, stuff.C, b, x)\n#        with MyTimer(""%dx%d decompose-once"" % (n,n)) as mt:\n#            method2(reps, stuff.C, b, x)\n#        with MyTimer(""%dx%d decompose-once-banded"" % (n,n)) as mt:\n#            method3(reps, stuff.C, b, x)\n\n'"
doc/legtest3.py,29,"b'# -*- coding: utf-8 -*-\n#\n# Trying out the NumPy API for Legendre polynomials and Gauss--Legendre quadrature,\n# with an eye toward the modern hierarchical (Lobatto) basis functions for Galerkin methods\n# (B. Szab\xc3\xb3, I. Babu\xc5\xa1ka, Finite element analysis, John Wiley & Sons, 1991).\n#\n# JJ 2016-02-16\n\nfrom __future__ import division, print_function, absolute_import\n\nimport time\n\nimport numpy as np\n\ntry:\n    import mpmath  # Python 3.x\nexcept ImportError:\n    import sympy.mpmath as mpmath  # Python 2.7\n\nimport matplotlib.pyplot as plt\n\nimport pylu.dgesv as dgesv\n\n\nclass RandomPileOfTestStuff:\n    # Create a NumPy wrapper for high-precision Legendre polynomials from mpmath.\n    #\n    # Note that vectorize() is just a convenience wrapper; the implementation is\n    # essentially a Python for loop, so it won\'t increase performance over manually\n    # looping over the items.\n    #\n    # See\n    #   http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.vectorize.html\n    #\n    #\n    # Static method!\n    #\n    # (Actually, technically speaking this is a function object bound to the class scope;\n    #  see the rules for creating static members and the discussion on Python\'s class system,\n    #   http://stackoverflow.com/questions/3506150/static-class-members-python\n    #   https://docs.python.org/2/tutorial/classes.html#class-definition-syntax\n    # )\n    #\n    _P = np.vectorize( mpmath.legendre )\n\n    def __init__(self, q=15):\n        assert( q >= 1 )\n\n        # max polynomial degree\n        self.q = q\n\n        self.P   = None  # Legendre polynomials\n        self.N   = None  # hierarchical basis functions (FEM, dG)\n\n        # Galerkin matrices\n        self.K   = None  # N\' * N\'\n        self.C   = None  # N\' * N\n        self.M   = None  # N  * N\n\n        self.get_legendre_polynomials()\n        self.build_hierarchical_basis()\n        self.build_K()\n        self.build_C()\n        self.build_M()\n\n    def get_legendre_polynomials(self):\n        q = self.q\n\n        # We use a function factory to freeze j to the given value (i.e. bind it at define time instead of at call time).\n        #\n        # See\n        #   http://stackoverflow.com/questions/1107210/python-lambda-problems\n        #\n        # Note that in Python, it is valid to call a static method by self.f() (as well as the traditional ClassName.f()).\n        #\n        P = []\n        for j in range(q):\n            P.append(  ( lambda j: lambda x : self._P(j, x) )(j)  )\n\n        self.P = P\n\n    def build_hierarchical_basis(self):\n        q = self.q\n\n        N = []\n        N.append( lambda x: (1./2.) * (1. - x) )\n        N.append( lambda x: (1./2.) * (1. + x) )\n        # explicit solution, using mpmath\'s high-precision routines (slow, but works fine at least to q=300)\n        for j in range(2,q+1):\n            # HACK: Python 3 compatibility: we must float(j), because some part of the toolchain here wants to convert all arguments to mpf, which does not work for int.\n            N.append(  (  lambda j: lambda x : ( self._P(j, x) - self._P(j-2, x) ) / np.sqrt( 2. * (2.*j - 1.) )  )(float(j))  )\n\n        self.N = N\n\n    # stiffness matrix, integrand N\' * N\'\n    def build_K(self):\n        q = self.q\n        n = q+1\n\n        K = np.eye( n, dtype=np.float64 )\n        K[0,0] =  1./2.\n        K[0,1] = -1./2.\n        K[1,0] = -1./2.\n        K[1,1] =  1./2.\n\n        self.K = K\n\n    # damping or gyroscopic matrix, integrand N\' * N\n    def build_C(self):\n        q = self.q\n        n = q+1\n\n        C = np.zeros( (n,n), dtype=np.float64 )\n        C[0,0] = -1./2.\n        C[0,1] =  1./2.\n        C[1,0] = -1./2.\n        C[1,1] =  1./2.\n\n        if q >= 2:\n            t = 1./np.sqrt(6.)\n            C[0,2] = -t\n            C[1,2] =  t\n            C[2,0] =  t\n            C[2,1] = -t\n\n        # General formula for C_ji for j,i >= 2.\n        for j in range(2,n):\n            i = j + 1  # i-1 = j  <=>  i = j+1\n            if i >= 2 and i < n:\n                C[j,i] =  2. * np.sqrt( 1. / ( ( 2.*j - 1. ) * ( 2.*j + 1. ) ) )\n            i = j - 1  # i+1 = j  <=>  i = j-1\n            if i >= 2 and i < n:\n                C[j,j-1] = -2. * np.sqrt( 1. / ( ( 2.*j - 1. ) * ( 2.*j - 3. ) ) )\n\n        self.C = C\n\n    # mass matrix, integrand N * N\n    def build_M(self):\n        q = self.q\n        n = q+1\n\n        M = np.zeros( (n,n), dtype=np.float64 )\n        M[0,0] = 2./3.\n        M[0,1] = 1./3.\n        M[1,0] = 1./3.\n        M[1,1] = 2./3.\n        if q >= 2:\n            t = 1./np.sqrt(2.)\n            M[0,2] = -t\n            M[1,2] = -t\n            M[2,0] = -t\n            M[2,1] = -t\n        if q >= 3:\n            t = 1. / (3. * np.sqrt(10) )\n            M[0,3] = -t\n            M[1,3] =  t\n            M[3,0] = -t\n            M[3,1] =  t\n\n        # General formula for M_ji for j,i >= 2.\n        for j in range(2,n):\n            M[j,j]   = 1. / (2.*j - 1.) * ( 1. / (2.*j + 1.)  +  1. / (2.*j - 3.) )\n            i = j - 2  # i+2 = j  <=>  i = j-2\n            if i >= 2 and i < n:\n                M[j,i] = 1. / ( np.sqrt(2.*j - 5.) * (2.*j - 3.) * np.sqrt(2.*j - 1.) )\n            i = j + 2  # i-2 = j  <=>  i = j+2\n            if i >= 2 and i < n:\n                M[j,i] = 1. / ( np.sqrt(2.*j - 1.) * (2.*j + 1.) * np.sqrt(2.*j + 3.) )\n\n        self.M = M\n\n\ndef main():\n    stuff = RandomPileOfTestStuff(q=100)\n\n    # From the API docs for numpy.polynomial.legendre.leggauss:\n    #    Computes the sample points and weights for Gauss-Legendre quadrature.\n    #    These sample points and weights will correctly integrate polynomials of degree 2*deg - 1 or less over the interval [-1, 1] with the weight function f(x) = 1.\n    #\n    # Hence, in Galerkin methods, to exactly handle a mass matrix where neither of the terms is differentiated, using affine mapping to the reference element [-1,1]\n    # (implying piecewise constant Jacobian), we need to have\n    #\n    #   2*deg - 1 = 2*d\n    #\n    # i.e.\n    #\n    #   deg = (2*d + 1) / 2\n    #\n    # where d is the degree of the highest-degree polynomial present in the Galerkin basis, and deg is the order of the Gauss-Legendre rule.\n    # Obviously, since only integer deg are available, we must round up (if rounded down, 2*deg-1 is less than the smallest needed, 2*d).\n    # Thus the actual practical result is\n    #\n    #   deg = ceil( (2*d + 1) / 2 ) = d + ceil( 1/2 ) = d+1\n    #\n    # (Observe that a rule of this order can do one degree more than the matrix M (integrand N*N) needs. With this, we could exactly integrate x*N*N, if needed.)\n    #\n    #\n    # For the purposes of solving the first-order problem  u\' = f(u, t)  by dG, the matrix is not our M, but instead our C (N\'*N = degree d-1 plus degree d), so\n    #\n    #   2*deg - 1 = 2*d - 1\n    #\n    # i.e.\n    #\n    #   deg = d\n    #\n    # Thus, we can solve this problem with a Gauss-Legendre rule of one order lower than in the case where the matrix M is needed.\n    #\n    #\n#    deg = d+1\n#    q,w = np.polynomial.legendre.leggauss( deg )\n#    print( deg,(2*deg-1),q,w )\n\n    # matrix, name, figure number to plot, bugcheck (max(abs(bugcheck)) should evaluate to 0 for mat[2:,2:])\n    data = ( (stuff.K, ""K"", 2, lambda v: v - np.transpose(v)),\n             (stuff.C, ""C"", 3, lambda v: v + np.transpose(v)),\n             (stuff.M, ""M"", 4, lambda v: v - np.transpose(v)) )\n\n    for mat,name,figno,bugcheck in data:\n        print( mat )\n        plt.figure(figno)\n\n        plt.subplot(1,2, 1)\n\n        plt.spy(mat)  # spy() doesn\'t work for a full matrix without any zero entries! (try stuff.M with q=2)\n\n#        plt.imshow(mat, interpolation=""nearest"", cmap=""Oranges"")\n#        plt.colorbar()\n\n        plt.plot( [0,stuff.q], [0,stuff.q], \'r--\' )  # mark diagonal\n        plt.title(r""$\\mathbf{%s}$"" % name)\n\n        if stuff.q >= 2:\n            v = mat[2:,2:]\n            b = np.max( np.abs( bugcheck(v) ) )\n            assert b == 0.0, ""bugcheck fail for matrix %s; should be 0, got %g"" % (name, b)\n\n\n        # LU decomposition (sort of)\n        #\n        plt.subplot(1,2, 2)\n        A = mat.copy()\n        A[0,0] += 1.0  # K and C are rank-deficient by one; simulate effect of boundary conditions (or dG jump term)\n        LU,p = dgesv.lup_packed(A)\n        plt.spy(LU)\n        plt.plot( [0,stuff.q], [0,stuff.q], \'r--\' )\n        plt.title(r""$\\mathbf{LU}$ (packed format)"")\n\n\n##    L,U,p = dgesv.lup(stuff.M)\n##    print( np.transpose(np.nonzero(L)) )\n##    print( np.transpose(np.nonzero(U)) )\n##    print( p )\n##    plt.figure(3)\n##    plt.subplot(1,2, 1)\n##    plt.spy(L)\n##    plt.plot( [0,stuff.q-1], [0,stuff.q-1], \'r--\' )\n###    plt.imshow(L, interpolation=""nearest"", cmap=""Oranges"")\n###    plt.colorbar(orientation=""horizontal"")\n##    plt.title(r""$\\mathbf{L}$"")\n##    plt.subplot(1,2, 2)\n##    plt.spy(U)\n##    plt.plot( [0,stuff.q-1], [0,stuff.q-1], \'r--\' )\n###    plt.imshow(U, interpolation=""nearest"", cmap=""Oranges"")\n###    plt.colorbar(orientation=""horizontal"")\n##    plt.title(r""$\\mathbf{U}$"")\n\n\n##    LU,p = dgesv.lup_packed(stuff.M)\n##    plt.figure(4)\n##    plt.spy(LU)\n##    plt.plot( [0,stuff.q-1], [0,stuff.q-1], \'r--\' )\n##    plt.title(r""$\\mathbf{LU}$ (packed format)"")\n\n##    mincols,maxcols = dgesv.find_bands(LU, 1e-15)\n##    print( mincols, maxcols )\n\n\n##    # old Python-based mincols, maxcols finding code\n##\n##    # Find the smallest column index with nonzero data on each row in L.\n##    #\n##    # We can use this to ""sparsify"" the backsolve even though the data structure is dense.\n##    #\n##    # This assumes that each row has at least one nonzero entry (which is always the case for an invertible matrix).\n##    #\n##    Lnz = np.nonzero(L)\n##    mincols = []\n##    rowprev = -1\n##    n = len(Lnz[0])\n##    i = 0\n##    while i < n:\n##        if Lnz[0][i] != rowprev:\n##            mincols.append(Lnz[1][i])\n##            rowprev = Lnz[0][i]\n##        i += 1\n##    mincols = np.array( mincols, dtype=np.intc, order=""C"" )\n##    print( L )\n##    print( mincols )\n\n##    # Find the largest column index with nonzero data on each row in U.\n##    #\n##    # We can use this to ""sparsify"" the backsolve even though the data structure is dense.\n##    #\n##    # This assumes that each row has at least one nonzero entry (which is always the case for an invertible matrix).\n##    #\n##    Unz = np.nonzero(U)\n##    maxcols = []\n##    rowprev = -1\n##    n = len(Unz[0])\n##    i = n - 1\n##    while i >= 0:\n##        if Unz[0][i] != rowprev:\n##            maxcols.append(Unz[1][i])\n##            rowprev = Unz[0][i]\n##        i -= 1\n##    maxcols.reverse()\n##    maxcols = np.array( maxcols, dtype=np.intc, order=""C"" )\n##    print( U )\n##    print( maxcols )\n\n\n    # Visualize\n    #\n    xx = np.linspace(-1., 1., 101)\n    plt.figure(1)\n    plt.clf()\n    for func in stuff.N:\n        plt.plot( xx, func(xx) )\n\n    plt.axis(\'tight\')\n    a = plt.axis()\n    plt.axis( [ a[0], a[1], a[2]*1.05, a[3]*1.05 ] )\n\n    plt.grid(b=True, which=\'both\')\n    plt.title(\'Hierarchical basis functions\')\n\n\n# naive solve (repeat the LU decomposition process each time)\n#\ndef method1(reps, A, b, x):\n    for j in range(reps):\n#        dgesv.solve( A, b[j,:], x )\n        dgesv.solve( A, b, x )\n\n# decompose once, then solve\n#\ndef method2(reps, A, b, x):\n    LU,p = dgesv.lup_packed(A)\n    for j in range(reps):\n#        dgesv.solve_decomposed( LU, p, b[j,:], x )\n        dgesv.solve_decomposed( LU, p, b, x )\n\n# decompose once, then solve, utilize banded structure\n#\ndef method3(reps, A, b, x):\n    LU,p = dgesv.lup_packed(A)\n    mincols,maxcols = dgesv.find_bands(LU, tol=1e-15)\n    for j in range(reps):\n#        dgesv.solve_decomposed_banded( LU, p, mincols, maxcols, b[j,:], x )\n        dgesv.solve_decomposed_banded( LU, p, mincols, maxcols, b, x )\n\n\nclass MyTimer:\n    t0 = None\n    l  = None\n    n  = None  # number of runs\n\n    def __init__(self, label="""", n=None):\n        self.label = label\n        self.n     = n\n\n    def __enter__(self):\n        self.t0 = time.time()\n        return self\n\n    def __exit__(self, type, value, traceback):\n        dt  = time.time() - self.t0\n        l   = (""%s: "" % self.label) if len(self.label) else ""time taken: ""\n        avg = ("", avg. %gs per run"" % (dt/self.n)) if self.n is not None else """"\n        print( ""%s%gs%s"" % (l, dt, avg) )\n\n\nif __name__ == \'__main__\':\n    main()\n    plt.show()\n\n#    # Running the benchmark loop at the Python end makes the banded version look slower (for our matrix ""C"", the C code is actually ~3x faster than the generic non-banded version),\n#    # because a large majority of the execution time is taken up by data conversion from Python to C and back (and Python asserts, if enabled).\n#    #\n#    # For small matrices (q = 15 or so), to get reliable results on the C code only (which is a realistic use case if used from inside a Cython-accelerated solver, which is the whole point of dgesv.pyx),\n#    # the benchmark looping must be done inside dgesv.pyx.\n#    #\n#    # Naive benchmarking results start becoming reliable around q >= 100.\n#    #\n#    reps   = 10000\n##    qstart = 3\n##    qend   = 16  # actually one-past-end\n#    qstart = 300\n#    qend   = 301\n\n#    for q in range(qstart, qend):\n#        stuff = RandomPileOfTestStuff(q)\n#        A = stuff.M\n\n#        n = np.shape(A)[0]\n##        A[0,0] += 1.0  # simulate effect of dG jump term (this is the only basis function which is nonzero at x = -1)\n##        b = np.random.uniform(0.0, 1.0, size=(reps,n))  # this makes slicing part of the performance measurement - not good\n#        b = np.random.uniform(0.0, 1.0, size=(n,))\n#        x = np.empty( [n], dtype=np.float64, order=""C"" )\n\n#        print( ""Timings for %d runs"" % reps )\n##        with MyTimer(""%dx%d naive"" % (n,n), reps) as mt:\n##            print( mt.label )\n##            method1(reps, A, b, x)\n#        with MyTimer(""%dx%d decompose-once"" % (n,n), reps) as mt:\n#            method2(reps, A, b, x)\n#        with MyTimer(""%dx%d decompose-once-banded"" % (n,n), reps) as mt:\n#            method3(reps, A, b, x)\n#        print( ""Residual from last run %g"" % np.max(np.abs( np.dot(A,x) - b )) )\n\n'"
pydgq/__init__.py,0,"b'# -*- coding: utf-8 -*-\n#\n""""""Integrate first-order ODE system  u\'(t) = f(u, t).\n\nThe main point of interest in this library is dG(q), i.e. the\ntime-discontinuous Galerkin method using a Lobatto basis\n(a.k.a. hierarchical polynomial basis). See ivp().\n\nFor preparing the data file used by the integrator (pydgq_data.bin),\nrun the module pydgq.utils.precalc as the main program.\n\nNote also that since the precalc module is not needed once the data file\nhas been generated, it is not automatically imported.\n\nWhen this module is imported, it imports all symbols from pydgq.solver.odesolve\ninto the local namespace.\n""""""\n\nfrom __future__ import absolute_import  # https://www.python.org/dev/peps/pep-0328/\n\n__version__ = \'0.1.2\'\n\nfrom .solver.odesolve import *\n\n'"
test/builtin_1storder_kernel_test.py,9,"b'# -*- coding: utf-8 -*-\n#\n# Tests/usage examples for kernels built-in to the solver.\n\nfrom __future__ import division, print_function\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom pydgq.solver.types import DTYPE\nfrom pydgq.solver.galerkin import init\nimport pydgq.solver.builtin_kernels\nimport pydgq.solver.odesolve\nfrom pydgq.utils.discontify import discontify  # for plotting dG results\n\n\n#####################\n# config for testing\n#####################\n\nq = 2  # degree of basis for dG and cG\n\n# How many visualization (interpolation) points to use within each timestep for Galerkin methods.\n#\n# Note that the dG solution has the best accuracy at the endpoint of the timestep;\n# to compare apples-to-apples with classical integrators, this should be set to 1.\n#\n# Larger values (e.g. 11) are useful for visualizing the behavior of the dG solution inside\n# the timestep (something the classical integrators do not model at all).\n#\nnt_vis_galerkin = 1\n\nnt = 100   # number of timesteps\ndt = 0.02  # timestep size\n\nsave_from = 0  # see pydgq.solver.odesolve.ivp()\n\n\n# known analytical solution, for testing the integrators\n#\n# # see http://docs.sympy.org/dev/modules/solvers/ode.html\n# from sympy import Function, dsolve, Eq, Derivative, sin, cos, symbols\n# from sympy.abc import t\n# w = Function(\'w\')\n# dsolve(Derivative(w(t), t) - w(t), w(t))   # w\' = w\n#\n# ==>  w(t) == C1 exp(t)\n#\n# where C1 = w(0) accounts for the initial condition.\n#\ndef reference_solution(tt, n, w0):  # n = number of DOFs\n    tt = np.atleast_1d(tt)\n    ww = np.empty( (tt.shape[0],n), dtype=DTYPE, order=""C"" )\n    for j in range(n):\n        ww[:,j] = w0[j] * np.exp(tt)\n    return ww\n\n\n#####################\n# main program\n#####################\n\n# rel_tol : how close the numerical solution must be to the exact analytical one, in l-infinity norm (max abs)\ndef test(integrator, nt_vis, rel_tol=1e-2, vis=False):\n    n_saved_timesteps = pydgq.solver.odesolve.n_saved_timesteps( nt, save_from )\n    result_len        = pydgq.solver.odesolve.result_len( nt, save_from, interp=nt_vis )\n    startj,endj       = pydgq.solver.odesolve.timestep_boundaries( nt, save_from, interp=nt_vis )\n\n    n   = 3  # number of DOFs in the 1st-order system\n    w0  = np.arange(1, n+1, dtype=DTYPE)  # a trivial IC\n    M   = np.eye(n, dtype=DTYPE)          # a trivial ""M"" matrix\n\n    # instantiate kernel\n    rhs = pydgq.solver.builtin_kernels.Linear1stOrderKernel(n, M)\n\n    # create output arrays\n    ww   = None #np.empty( (result_len,n), dtype=DTYPE, order=""C"" )    # result array for w; if None, will be created by ivp()\n    ff   = np.empty( (result_len,n), dtype=DTYPE, order=""C"" )          # optional,  result array for w\', could be None\n    fail = np.empty( (n_saved_timesteps,), dtype=np.intc, order=""C"" )  # optional,  fail flag for each timestep, could be None\n\n    # solve problem\n    ww,tt = pydgq.solver.odesolve.ivp( integrator=integrator, allow_denormals=False,\n                                       w0=w0, dt=dt, nt=nt,\n                                       save_from=save_from, interp=nt_vis,\n                                       rhs=rhs,\n                                       ww=ww, ff=ff, fail=fail,\n                                       maxit=100 )\n\n    # check result\n    ww_ref = reference_solution(tt, n, w0)\n    relerr_linfty = np.linalg.norm(ww - ww_ref, ord=np.inf) / np.linalg.norm(ww_ref, ord=np.inf)\n    if (relerr_linfty < rel_tol).all():\n        passed = 1\n        if not vis:\n            print(""PASS, %03s, tol=% 7g, relerr=%g"" % (integrator, rel_tol, relerr_linfty))\n    else:\n        passed = 0\n        if not vis:\n            print(""FAIL, %03s, tol=% 7g, relerr=%g"" % (integrator, rel_tol, relerr_linfty))\n\n    # visualize if requested\n    if vis:\n        plt.figure(1)\n        plt.clf()\n\n        # show the discontinuities at timestep boundaries if using dG (and actually have something to draw within each timestep)\n        if integrator == ""dG"" and nt_vis > 1:\n            tt = discontify( tt, endj - 1, fill=""nan"" )\n            for j in range(n):\n                # we need the copy() to get memory-contiguous data for discontify() to process\n                wtmp = discontify( ww_ref[:,j].copy(), endj - 1, fill=""nan"" )\n                plt.plot( tt, wtmp, \'r--\' )  # known exact solution\n\n                wtmp = discontify( ww[:,j].copy(), endj - 1, fill=""nan"" )\n                plt.plot( tt, wtmp, \'k-\'  )  # numerical solution\n        else:\n            plt.plot( tt, ww_ref, \'r--\' )  # known exact solution\n            plt.plot( tt, ww,     \'k-\'  )  # numerical solution\n\n        plt.grid(b=True, which=""both"")\n        plt.axis(""tight"")\n        plt.xlabel(r""$t$"")\n        plt.ylabel(r""$w(t)$"")\n\n    return passed\n\n\nif __name__ == \'__main__\':\n    print(""** Testing integrators with 1st-order linear kernel **"")\n\n    # ""SE"" is not applicable, since we are testing a 1st-order problem\n    stuff_to_test = ( (""IMR"", 1e-3,  False),\n                      (""BE"",  1e-1,  False),\n                      (""RK4"", 1e-8,  False),\n                      (""RK3"", 1e-6,  False),\n                      (""RK2"", 1e-3,  False),\n                      (""FE"",  1e-1,  False),\n                      (""dG"",  1e-12, True ),\n                      (""cG"",  1e-4,  True )\n                    )\n    n_passed = 0\n\n    for integrator,rel_tol,is_galerkin in stuff_to_test:\n        if is_galerkin:\n            nt_vis = nt_vis_galerkin  # visualization points per timestep in Galerkin methods\n            init(q=q, method=integrator, nt_vis=nt_vis, rule=None)\n        else:\n            nt_vis = 1   # other methods compute only the end value for each timestep\n\n        n_passed += test(integrator, nt_vis, rel_tol)\n    print(""** %d/%d tests passed **"" % (n_passed, len(stuff_to_test)))\n\n#    # DEBUG: draw something\n#    #\n#    nt_vis = nt_vis_galerkin\n#    init(q=q, method=""dG"", nt_vis=nt_vis, rule=None)\n#    test(integrator=""dG"", nt_vis=nt_vis, vis=True)\n#    plt.show()\n\n'"
test/builtin_1storder_withmass_kernel_test.py,8,"b'# -*- coding: utf-8 -*-\n#\n# Tests/usage examples for kernels built-in to the solver.\n\nfrom __future__ import division, print_function\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom pydgq.solver.types import DTYPE\nfrom pydgq.solver.galerkin import init\nimport pydgq.solver.builtin_kernels\nimport pydgq.solver.odesolve\nfrom pydgq.utils.discontify import discontify  # for plotting dG results\n\n\n#####################\n# config for testing\n#####################\n\nq = 2  # degree of basis for dG and cG\n\n# How many visualization (interpolation) points to use within each timestep for Galerkin methods.\n#\n# Note that the dG solution has the best accuracy at the endpoint of the timestep;\n# to compare apples-to-apples with classical integrators, this should be set to 1.\n#\n# Larger values (e.g. 11) are useful for visualizing the behavior of the dG solution inside\n# the timestep (something the classical integrators do not model at all).\n#\nnt_vis_galerkin = 1\n\nnt = 100   # number of timesteps\ndt = 0.02  # timestep size\n\nsave_from = 0  # see pydgq.solver.odesolve.ivp()\n\n\n#####################\n# main program\n#####################\n\n# rel_tol : how close the numerical solution must be to the exact analytical one, in l-infinity norm (max abs)\ndef test(integrator, nt_vis, rel_tol=1e-2, vis=False):\n    n_saved_timesteps = pydgq.solver.odesolve.n_saved_timesteps( nt, save_from )\n    result_len        = pydgq.solver.odesolve.result_len( nt, save_from, interp=nt_vis )\n    startj,endj       = pydgq.solver.odesolve.timestep_boundaries( nt, save_from, interp=nt_vis )\n\n    n   = 2  # number of DOFs in the 1st-order system\n    w0  = np.arange(1, n+1, dtype=DTYPE)  # a trivial IC\n\n    # set up ""M"" matrix (arbitrary example)\n    M   = (np.arange(n*n, dtype=DTYPE) + 1.).reshape( (n,n) )\n\n    # set up ""A"" matrix\n    #\n    # let\'s make it swap the components (so that u1\' = u2,  u2\' = u1)\n    #\n    A   = np.zeros( (n,n), dtype=DTYPE, order=""C"" )\n    A[0,1] = 1.\n    A[1,0] = 1.\n\n    # for checking the result\n    invA_times_M = (np.linalg.inv(A)).dot(M)\n\n    # instantiate kernels\n    rhs1 = pydgq.solver.builtin_kernels.Linear1stOrderKernelWithMassMatrix(n, M, A)\n    rhs2 = pydgq.solver.builtin_kernels.Linear1stOrderKernel(n, invA_times_M)  # different algorithm, but should give the same result\n\n    # create output arrays\n    ww   = None #np.empty( (result_len,n), dtype=DTYPE, order=""C"" )    # result array for w; if None, will be created by ivp()\n    ff   = np.empty( (result_len,n), dtype=DTYPE, order=""C"" )          # optional,  result array for w\', could be None\n    fail = np.empty( (n_saved_timesteps,), dtype=np.intc, order=""C"" )  # optional,  fail flag for each timestep, could be None\n\n    # solve problem\n    ww,tt = pydgq.solver.odesolve.ivp( integrator=integrator, allow_denormals=False,\n                                       w0=w0, dt=dt, nt=nt,\n                                       save_from=save_from, interp=nt_vis,\n                                       rhs=rhs1,\n                                       ww=ww, ff=ff, fail=fail,\n                                       maxit=100 )\n\n    # check result\n    ww_ref,dummy = pydgq.solver.odesolve.ivp( integrator=integrator, allow_denormals=False,\n                                       w0=w0, dt=dt, nt=nt,\n                                       save_from=save_from, interp=nt_vis,\n                                       rhs=rhs2,\n                                       ww=ww, ff=ff, fail=fail,\n                                       maxit=100 )\n\n    relerr_linfty = np.linalg.norm(ww - ww_ref, ord=np.inf) / np.linalg.norm(ww_ref, ord=np.inf)\n    if (relerr_linfty < rel_tol).all():\n        passed = 1\n        if not vis:\n            print(""PASS, %03s, tol=% 7g, relerr=%g"" % (integrator, rel_tol, relerr_linfty))\n    else:\n        passed = 0\n        if not vis:\n            print(""FAIL, %03s, tol=% 7g, relerr=%g"" % (integrator, rel_tol, relerr_linfty))\n\n    # visualize if requested\n    if vis:\n        plt.figure(1)\n        plt.clf()\n\n        # show the discontinuities at timestep boundaries if using dG (and actually have something to draw within each timestep)\n        if integrator == ""dG"" and nt_vis > 1:\n            tt = discontify( tt, endj - 1, fill=""nan"" )\n            for j in range(n):\n                # we need the copy() to get memory-contiguous data for discontify() to process\n                wtmp = discontify( ww_ref[:,j].copy(), endj - 1, fill=""nan"" )\n                plt.plot( tt, wtmp, \'r--\' )  # known exact solution\n\n                wtmp = discontify( ww[:,j].copy(), endj - 1, fill=""nan"" )\n                plt.plot( tt, wtmp, \'k-\'  )  # numerical solution\n        else:\n            plt.plot( tt, ww_ref, \'r--\' )  # known exact solution\n            plt.plot( tt, ww,     \'k-\'  )  # numerical solution\n\n        plt.grid(b=True, which=""both"")\n        plt.axis(""tight"")\n        plt.xlabel(r""$t$"")\n        plt.ylabel(r""$w(t)$"")\n\n    return passed\n\n\nif __name__ == \'__main__\':\n    print(""** Testing integrators with 1st-order linear kernel with mass matrix **"")\n\n    # ""SE"" is not applicable, since we are testing a 1st-order problem\n    stuff_to_test = ( (""IMR"", 1e-3,  False),\n                      (""BE"",  1e-1,  False),\n                      (""RK4"", 1e-8,  False),\n                      (""RK3"", 1e-6,  False),\n                      (""RK2"", 1e-3,  False),\n                      (""FE"",  1e-1,  False),\n                      (""dG"",  1e-12, True ),\n                      (""cG"",  1e-4,  True )\n                    )\n    n_passed = 0\n\n    for integrator,rel_tol,is_galerkin in stuff_to_test:\n        if is_galerkin:\n            nt_vis = nt_vis_galerkin  # visualization points per timestep in Galerkin methods\n            init(q=q, method=integrator, nt_vis=nt_vis, rule=None)\n        else:\n            nt_vis = 1   # other methods compute only the end value for each timestep\n\n        n_passed += test(integrator, nt_vis, rel_tol)\n    print(""** %d/%d tests passed **"" % (n_passed, len(stuff_to_test)))\n\n#    # DEBUG: draw something\n#    #\n#    nt_vis = nt_vis_galerkin\n#    init(q=q, method=""dG"", nt_vis=nt_vis, rule=None)\n#    test(integrator=""dG"", nt_vis=nt_vis, vis=True)\n#    plt.show()\n\n'"
test/builtin_2ndorder_kernel_test.py,11,"b'# -*- coding: utf-8 -*-\n#\n# Tests/usage examples for kernels built-in to the solver.\n\nfrom __future__ import division, print_function\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom pydgq.solver.types import DTYPE\nfrom pydgq.solver.galerkin import init\nimport pydgq.solver.builtin_kernels\nimport pydgq.solver.odesolve\nfrom pydgq.utils.discontify import discontify  # for plotting dG results\n\n\n#####################\n# config for testing\n#####################\n\nq = 2  # degree of basis for dG and cG\n\n# How many visualization (interpolation) points to use within each timestep for Galerkin methods.\n#\n# Note that the dG solution has the best accuracy at the endpoint of the timestep;\n# to compare apples-to-apples with classical integrators, this should be set to 1.\n#\n# Larger values (e.g. 11) are useful for visualizing the behavior of the dG solution inside\n# the timestep (something the classical integrators do not model at all).\n#\nnt_vis_galerkin = 1\n\nnt = 100   # number of timesteps\ndt = 0.02  # timestep size\n\nsave_from = 0  # see pydgq.solver.odesolve.ivp()\n\n\n# known analytical solution, for testing the integrators\n#\n# # see http://docs.sympy.org/dev/modules/solvers/ode.html\n# from sympy import Function, dsolve, Eq, Derivative, sin, cos, symbols\n# from sympy.abc import t\n# w = Function(\'w\')\n# dsolve(Derivative(w(t), t, t) + w(t), w(t))   # w\'\' = -w\n#\n# ==>  w(t) == C1*sin(t) + C2*cos(t)\n#\n# where  C1 = w\'(0), C2 = w(0)  account for the initial condition.\n#\n# Hence we have also\n#\n#      w\'(t) == C1*cos(t) - C2*sin(t)\n#\ndef reference_solution(tt, n, w0):  # n = number of DOFs\n    tt = np.atleast_1d(tt)\n    ww = np.empty( (tt.shape[0],n), dtype=DTYPE, order=""C"" )\n    m  = n//2\n    for j in range(m):\n        # u\n        ww[:,2*j]   = w0[2*j+1] * np.sin(tt) + w0[2*j] * np.cos(tt)\n        # v\n        ww[:,2*j+1] = w0[2*j+1] * np.cos(tt) - w0[2*j] * np.sin(tt)\n    return ww\n\n\n#####################\n# main program\n#####################\n\n# rel_tol : how close the numerical solution must be to the exact analytical one, in l-infinity norm (max abs)\ndef test(integrator, nt_vis, rel_tol=1e-2, vis=False):\n    n_saved_timesteps = pydgq.solver.odesolve.n_saved_timesteps( nt, save_from )\n    result_len        = pydgq.solver.odesolve.result_len( nt, save_from, interp=nt_vis )\n    startj,endj       = pydgq.solver.odesolve.timestep_boundaries( nt, save_from, interp=nt_vis )\n\n    n   = 4  # number of DOFs in the **1st-order** system\n\n    # set IC\n    w0  = np.empty( (n,), dtype=DTYPE, order=""C"" )\n    w0[0] =  0.  # u1\n    w0[1] =  1.  # v1\n    w0[2] =  2.  # u2\n    w0[3] = -1.  # v2\n\n    # set up the M0 and M1 matrices for  u\'\' = M0 u + M1 u\'\n    #\n    m  = n//2\n    M0 = -np.eye(   m,     dtype=DTYPE)\n    M1 =  np.zeros( (m,m), dtype=DTYPE)\n\n    # instantiate kernel\n    rhs = pydgq.solver.builtin_kernels.Linear2ndOrderKernel(n, M0, M1)  # note n, not m\n\n    # create output arrays\n    ww   = None #np.empty( (result_len,n), dtype=DTYPE, order=""C"" )    # result array for w; if None, will be created by ivp()\n    ff   = np.empty( (result_len,n), dtype=DTYPE, order=""C"" )          # optional,  result array for w\', could be None\n    fail = np.empty( (n_saved_timesteps,), dtype=np.intc, order=""C"" )  # optional,  fail flag for each timestep, could be None\n\n    # solve problem\n    ww,tt = pydgq.solver.odesolve.ivp( integrator=integrator, allow_denormals=False,\n                                       w0=w0, dt=dt, nt=nt,\n                                       save_from=save_from, interp=nt_vis,\n                                       rhs=rhs,\n                                       ww=ww, ff=ff, fail=fail,\n                                       maxit=100 )\n\n    # check result\n    ww_ref = reference_solution(tt, n, w0)\n    relerr_linfty = np.linalg.norm(ww - ww_ref, ord=np.inf) / np.linalg.norm(ww_ref, ord=np.inf)\n    if (relerr_linfty < rel_tol).all():\n        passed = 1\n        if not vis:\n            print(""PASS, %03s, tol=% 7g, relerr=%g"" % (integrator, rel_tol, relerr_linfty))\n    else:\n        passed = 0\n        if not vis:\n            print(""FAIL, %03s, tol=% 7g, relerr=%g"" % (integrator, rel_tol, relerr_linfty))\n\n    # visualize if requested\n    if vis:\n        plt.figure(1)\n        plt.clf()\n\n        # show the discontinuities at timestep boundaries if using dG (and actually have something to draw within each timestep)\n        if integrator == ""dG"" and nt_vis > 1:\n            tt = discontify( tt, endj - 1, fill=""nan"" )\n            for j in range(n):\n                # we need the copy() to get memory-contiguous data for discontify() to process\n                wtmp = discontify( ww_ref[:,j].copy(), endj - 1, fill=""nan"" )\n                plt.plot( tt, wtmp, \'r--\' )  # known exact solution\n\n                wtmp = discontify( ww[:,j].copy(), endj - 1, fill=""nan"" )\n                plt.plot( tt, wtmp, \'k-\'  )  # numerical solution\n        else:\n            plt.plot( tt, ww_ref, \'r--\' )  # known exact solution\n            plt.plot( tt, ww,     \'k-\'  )  # numerical solution\n\n        plt.grid(b=True, which=""both"")\n        plt.axis(""tight"")\n        plt.xlabel(r""$t$"")\n        plt.ylabel(r""$w(t)$"")\n\n    return passed\n\n\nif __name__ == \'__main__\':\n    print(""** Testing integrators with 2nd-order linear kernel **"")\n\n    stuff_to_test = ( (""SE"",  1e-1,  False),  # 2nd-order problem, also ""SE"" is applicable\n                      (""IMR"", 1e-3,  False),\n                      (""BE"",  1e-1,  False),\n                      (""RK4"", 1e-8,  False),\n                      (""RK3"", 1e-6,  False),\n                      (""RK2"", 1e-3,  False),\n                      (""FE"",  1e-1,  False),\n                      (""dG"",  1e-12, True ),\n                      (""cG"",  1e-4,  True )\n                    )\n    n_passed = 0\n\n    for integrator,rel_tol,is_galerkin in stuff_to_test:\n        if is_galerkin:\n            nt_vis = nt_vis_galerkin  # visualization points per timestep in Galerkin methods\n            init(q=q, method=integrator, nt_vis=nt_vis, rule=None)\n        else:\n            nt_vis = 1   # other methods compute only the end value for each timestep\n\n        n_passed += test(integrator, nt_vis, rel_tol)\n    print(""** %d/%d tests passed **"" % (n_passed, len(stuff_to_test)))\n\n#    # DEBUG: draw something\n#    #\n#    nt_vis = nt_vis_galerkin\n#    init(q=q, method=""dG"", nt_vis=nt_vis, rule=None)\n#    test(integrator=""dG"", nt_vis=nt_vis, vis=True)\n#    plt.show()\n\n'"
test/builtin_2ndorder_withmass_kernel_test.py,10,"b'# -*- coding: utf-8 -*-\n#\n# Tests/usage examples for kernels built-in to the solver.\n\nfrom __future__ import division, print_function\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom pydgq.solver.types import DTYPE\nfrom pydgq.solver.galerkin import init\nimport pydgq.solver.builtin_kernels\nimport pydgq.solver.odesolve\nfrom pydgq.utils.discontify import discontify  # for plotting dG results\n\n\n#####################\n# config for testing\n#####################\n\nq = 2  # degree of basis for dG and cG\n\n# How many visualization (interpolation) points to use within each timestep for Galerkin methods.\n#\n# Note that the dG solution has the best accuracy at the endpoint of the timestep;\n# to compare apples-to-apples with classical integrators, this should be set to 1.\n#\n# Larger values (e.g. 11) are useful for visualizing the behavior of the dG solution inside\n# the timestep (something the classical integrators do not model at all).\n#\nnt_vis_galerkin = 1\n\nnt = 100   # number of timesteps\ndt = 0.02  # timestep size\n\nsave_from = 0  # see pydgq.solver.odesolve.ivp()\n\n\n#####################\n# main program\n#####################\n\n# rel_tol : how close the numerical solution must be to the exact analytical one, in l-infinity norm (max abs)\ndef test(integrator, nt_vis, rel_tol=1e-2, vis=False):\n    n_saved_timesteps = pydgq.solver.odesolve.n_saved_timesteps( nt, save_from )\n    result_len        = pydgq.solver.odesolve.result_len( nt, save_from, interp=nt_vis )\n    startj,endj       = pydgq.solver.odesolve.timestep_boundaries( nt, save_from, interp=nt_vis )\n\n    n   = 4  # number of DOFs in the **1st-order** system\n\n    # set IC\n    w0  = np.empty( (n,), dtype=DTYPE, order=""C"" )\n    w0[0] =  0.  # u1\n    w0[1] =  1.  # v1\n    w0[2] =  2.  # u2\n    w0[3] = -1.  # v2\n\n    # set up the M0, M1 and M2 matrices for  M2 u\'\' = M0 u + M1 u\'\n    #\n    m  = n//2\n    M0 = -np.eye(   m,     dtype=DTYPE)\n    M1 =  np.zeros( (m,m), dtype=DTYPE)\n\n    M2 = np.zeros( (m,m), dtype=DTYPE, order=""C"" )\n    M2[0,1] = 1.\n    M2[1,0] = 1.\n\n    # for checking the result\n    invM2_times_M0 = (np.linalg.inv(M2)).dot(M0)\n    invM2_times_M1 = (np.linalg.inv(M2)).dot(M1)\n\n    # instantiate kernel\n    rhs1 = pydgq.solver.builtin_kernels.Linear2ndOrderKernelWithMassMatrix(n, M0, M1, M2)        # note n, not m\n    rhs2 = pydgq.solver.builtin_kernels.Linear2ndOrderKernel(n, invM2_times_M0, invM2_times_M1)  # different algorithm, but should give the same result\n\n    # create output arrays\n    ww   = None #np.empty( (result_len,n), dtype=DTYPE, order=""C"" )    # result array for w; if None, will be created by ivp()\n    ff   = np.empty( (result_len,n), dtype=DTYPE, order=""C"" )          # optional,  result array for w\', could be None\n    fail = np.empty( (n_saved_timesteps,), dtype=np.intc, order=""C"" )  # optional,  fail flag for each timestep, could be None\n\n    # solve problem\n    ww,tt = pydgq.solver.odesolve.ivp( integrator=integrator, allow_denormals=False,\n                                       w0=w0, dt=dt, nt=nt,\n                                       save_from=save_from, interp=nt_vis,\n                                       rhs=rhs1,\n                                       ww=ww, ff=ff, fail=fail,\n                                       maxit=100 )\n\n    # check result\n    ww_ref,dummy = pydgq.solver.odesolve.ivp( integrator=integrator, allow_denormals=False,\n                                       w0=w0, dt=dt, nt=nt,\n                                       save_from=save_from, interp=nt_vis,\n                                       rhs=rhs2,\n                                       ww=ww, ff=ff, fail=fail,\n                                       maxit=100 )\n\n    relerr_linfty = np.linalg.norm(ww - ww_ref, ord=np.inf) / np.linalg.norm(ww_ref, ord=np.inf)\n    if (relerr_linfty < rel_tol).all():\n        passed = 1\n        if not vis:\n            print(""PASS, %03s, tol=% 7g, relerr=%g"" % (integrator, rel_tol, relerr_linfty))\n    else:\n        passed = 0\n        if not vis:\n            print(""FAIL, %03s, tol=% 7g, relerr=%g"" % (integrator, rel_tol, relerr_linfty))\n\n    # visualize if requested\n    if vis:\n        plt.figure(1)\n        plt.clf()\n\n        # show the discontinuities at timestep boundaries if using dG (and actually have something to draw within each timestep)\n        if integrator == ""dG"" and nt_vis > 1:\n            tt = discontify( tt, endj - 1, fill=""nan"" )\n            for j in range(n):\n                # we need the copy() to get memory-contiguous data for discontify() to process\n                wtmp = discontify( ww_ref[:,j].copy(), endj - 1, fill=""nan"" )\n                plt.plot( tt, wtmp, \'r--\' )  # known exact solution\n\n                wtmp = discontify( ww[:,j].copy(), endj - 1, fill=""nan"" )\n                plt.plot( tt, wtmp, \'k-\'  )  # numerical solution\n        else:\n            plt.plot( tt, ww_ref, \'r--\' )  # known exact solution\n            plt.plot( tt, ww,     \'k-\'  )  # numerical solution\n\n        plt.grid(b=True, which=""both"")\n        plt.axis(""tight"")\n        plt.xlabel(r""$t$"")\n        plt.ylabel(r""$w(t)$"")\n\n    return passed\n\n\nif __name__ == \'__main__\':\n    print(""** Testing integrators with 2nd-order linear kernel with mass matrix **"")\n\n    stuff_to_test = ( (""SE"",  1e-1,  False),  # 2nd-order problem, also ""SE"" is applicable\n                      (""IMR"", 1e-3,  False),\n                      (""BE"",  1e-1,  False),\n                      (""RK4"", 1e-8,  False),\n                      (""RK3"", 1e-6,  False),\n                      (""RK2"", 1e-3,  False),\n                      (""FE"",  1e-1,  False),\n                      (""dG"",  1e-12, True ),\n                      (""cG"",  1e-4,  True )\n                    )\n    n_passed = 0\n\n    for integrator,rel_tol,is_galerkin in stuff_to_test:\n        if is_galerkin:\n            nt_vis = nt_vis_galerkin  # visualization points per timestep in Galerkin methods\n            init(q=q, method=integrator, nt_vis=nt_vis, rule=None)\n        else:\n            nt_vis = 1   # other methods compute only the end value for each timestep\n\n        n_passed += test(integrator, nt_vis, rel_tol)\n    print(""** %d/%d tests passed **"" % (n_passed, len(stuff_to_test)))\n\n#    # DEBUG: draw something\n#    #\n#    nt_vis = nt_vis_galerkin\n#    init(q=q, method=""dG"", nt_vis=nt_vis, rule=None)\n#    test(integrator=""dG"", nt_vis=nt_vis, vis=True)\n#    plt.show()\n\n'"
test/cython_kernel_test.py,6,"b'# -*- coding: utf-8 -*-\n#\n# Tests/usage examples for a custom Cython-based kernel.\n\nfrom __future__ import division, print_function\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom pydgq.solver.types import DTYPE\nfrom pydgq.solver.galerkin import init\nimport pydgq.solver.odesolve\nfrom pydgq.utils.discontify import discontify  # for plotting dG results\n\n# import our extension module that contains the custom kernel\n#\ntry:\n    from cython_kernel import MyKernel\nexcept ImportError:\n    print( ""ERROR: cython_kernel.pyx must be compiled first; run  \'python -m setup build_ext --inplace\'  to do this"" )\n    raise\n\n#####################\n# config for testing\n#####################\n\nq = 2  # degree of basis for dG and cG\n\n# How many visualization (interpolation) points to use within each timestep for Galerkin methods.\n#\n# Note that the dG solution has the best accuracy at the endpoint of the timestep;\n# to compare apples-to-apples with classical integrators, this should be set to 1.\n#\n# Larger values (e.g. 11) are useful for visualizing the behavior of the dG solution inside\n# the timestep (something the classical integrators do not model at all).\n#\nnt_vis_galerkin = 1\n\nnt = 100  # number of timesteps\ndt = 0.1  # timestep size\n\nsave_from = 0  # see pydgq.solver.odesolve.ivp()\n\n\n#####################\n# main program\n#####################\n\n# rel_tol : how close the numerical solution must be to the exact analytical one, in l-infinity norm (max abs)\ndef test(integrator, nt_vis, rel_tol=1e-2, vis=False):\n    n_saved_timesteps = pydgq.solver.odesolve.n_saved_timesteps( nt, save_from )\n    result_len        = pydgq.solver.odesolve.result_len( nt, save_from, interp=nt_vis )\n    startj,endj       = pydgq.solver.odesolve.timestep_boundaries( nt, save_from, interp=nt_vis )\n\n    n   = 3  # number of DOFs in the 1st-order system\n    w0  = np.zeros( (n,), dtype=DTYPE, order=""C"" )  # a trivial IC\n\n    # instantiate kernel\n    rhs = MyKernel(n, omega=0.1 * (2. * np.pi))\n\n    # create output arrays\n    ww   = None #np.empty( (result_len,n), dtype=DTYPE, order=""C"" )    # result array for w; if None, will be created by ivp()\n    ff   = np.empty( (result_len,n), dtype=DTYPE, order=""C"" )          # optional,  result array for w\', could be None\n    fail = np.empty( (n_saved_timesteps,), dtype=np.intc, order=""C"" )  # optional,  fail flag for each timestep, could be None\n\n    # solve problem\n    ww,tt = pydgq.solver.odesolve.ivp( integrator=integrator, allow_denormals=False,\n                                       w0=w0, dt=dt, nt=nt,\n                                       save_from=save_from, interp=nt_vis,\n                                       rhs=rhs,\n                                       ww=ww, ff=ff, fail=fail,\n                                       maxit=100 )\n\n    # check result\n    ww_ref = rhs.reference_solution(tt)\n    relerr_linfty = np.linalg.norm(ww - ww_ref, ord=np.inf) / np.linalg.norm(ww_ref, ord=np.inf)\n    if (relerr_linfty < rel_tol).all():\n        passed = 1\n        if not vis:\n            print(""PASS, %03s, tol=% 7g, relerr=%g"" % (integrator, rel_tol, relerr_linfty))\n    else:\n        passed = 0\n        if not vis:\n            print(""FAIL, %03s, tol=% 7g, relerr=%g"" % (integrator, rel_tol, relerr_linfty))\n\n    # visualize if requested\n    if vis:\n        plt.figure(1)\n        plt.clf()\n\n        # show the discontinuities at timestep boundaries if using dG (and actually have something to draw within each timestep)\n        if integrator == ""dG"" and nt_vis > 1:\n            tt = discontify( tt, endj - 1, fill=""nan"" )\n            for j in range(n):\n                # we need the copy() to get memory-contiguous data for discontify() to process\n                wtmp = discontify( ww_ref[:,j].copy(), endj - 1, fill=""nan"" )\n                plt.plot( tt, wtmp, \'r--\' )  # known exact solution\n\n                wtmp = discontify( ww[:,j].copy(), endj - 1, fill=""nan"" )\n                plt.plot( tt, wtmp, \'k-\'  )  # numerical solution\n        else:\n            plt.plot( tt, ww_ref, \'r--\' )  # known exact solution\n            plt.plot( tt, ww,     \'k-\'  )  # numerical solution\n\n        plt.grid(b=True, which=""both"")\n        plt.axis(""tight"")\n        plt.xlabel(r""$t$"")\n        plt.ylabel(r""$w(t)$"")\n\n    return passed\n\n\nif __name__ == \'__main__\':\n    print(""** Testing integrators with custom Cython kernel **"")\n\n    # ""SE"" is not applicable, since we are testing a 1st-order problem\n    stuff_to_test = ( (""IMR"", 1e-3,  False),\n                      (""BE"",  1e-1,  False),\n                      (""RK4"", 1e-8,  False),\n                      (""RK3"", 1e-6,  False),\n                      (""RK2"", 1e-3,  False),\n                      (""FE"",  1e-1,  False),\n                      (""dG"",  1e-12, True ),\n                      (""cG"",  1e-4,  True )\n                    )\n    n_passed = 0\n\n    for integrator,rel_tol,is_galerkin in stuff_to_test:\n        if is_galerkin:\n            nt_vis = nt_vis_galerkin  # visualization points per timestep in Galerkin methods\n            init(q=q, method=integrator, nt_vis=nt_vis, rule=None)\n        else:\n            nt_vis = 1   # other methods compute only the end value for each timestep\n\n        n_passed += test(integrator, nt_vis, rel_tol)\n    print(""** %d/%d tests passed **"" % (n_passed, len(stuff_to_test)))\n\n#    # DEBUG: draw something\n#    #\n#    nt_vis = nt_vis_galerkin\n#    init(q=q, method=""dG"", nt_vis=nt_vis, rule=None)\n#    test(integrator=""dG"", nt_vis=nt_vis, vis=True)\n#    plt.show()\n\n'"
test/lorenz_example.py,5,"b'# -*- coding: utf-8 -*-\n#\n# Usage example: solve the Lorenz system using a custom Python kernel.\n\nfrom __future__ import division, print_function\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d.axes3d import Axes3D\n\nfrom pydgq.solver.types import DTYPE\nfrom pydgq.solver.galerkin import init\nfrom pydgq.solver.kernel_interface import PythonKernel\nimport pydgq.solver.odesolve\nfrom pydgq.utils.discontify import discontify  # for plotting dG results\n\n\n#####################\n# config\n#####################\n\nq = 2  # degree of basis for dG and cG\n\n# How many visualization (interpolation) points to use within each timestep for Galerkin methods.\n#\n# Note that the dG solution has the best accuracy at the endpoint of the timestep;\n# to compare apples-to-apples with classical integrators, this should be set to 1.\n#\n# Larger values (e.g. 11) are useful for visualizing the behavior of the dG solution inside\n# the timestep (something the classical integrators do not model at all).\n#\nnt_vis_galerkin = 11\n\nnt = 3500  # number of timesteps\ndt = 0.1   # timestep size\n\nsave_from = 0  # see pydgq.solver.odesolve.ivp()\n\n\n#####################\n# custom kernel\n#####################\n\n# A kernel for the Lorenz system.\n#\n# The custom kernel only needs to override callback(); even __init__ is not strictly needed,\n# unless adding some custom parameters (like here).\n#\nclass LorenzKernel(PythonKernel):\n    def __init__(self, rho, sigma, beta):\n        # super\n        PythonKernel.__init__(self, n=3)\n\n        # custom init\n        self.rho   = rho\n        self.sigma = sigma\n        self.beta  = beta\n\n    def callback(self, t):\n        # dxdt = sigma (y - x)\n        # dydt = x (rho - z) - y\n        # dzdt = x y - beta z\n        self.out[0] = self.sigma * (self.w[1] - self.w[0])\n        self.out[1] = self.w[0]  * (self.rho  - self.w[2]) - self.w[1]\n        self.out[2] = self.w[0]  * self.w[1] - self.beta * self.w[2]  # this is nonlinear, so we can\'t use a built-in linear kernel\n\n\n#####################\n# main program\n#####################\n\ndef test(integrator, nt_vis):\n    n_saved_timesteps = pydgq.solver.odesolve.n_saved_timesteps( nt, save_from )\n    result_len        = pydgq.solver.odesolve.result_len( nt, save_from, interp=nt_vis )\n    startj,endj       = pydgq.solver.odesolve.timestep_boundaries( nt, save_from, interp=nt_vis )\n\n    n  = 3  # the Lorenz system has 3 DOFs\n\n    # we use the same values as in the example at https://en.wikipedia.org/wiki/Lorenz_system\n    #\n    rho   = 28.\n    sigma = 10.\n    beta  = 8./3.\n\n    # set IC\n    #\n    w0 = np.empty( (n,), dtype=DTYPE, order=""C"" )\n    w0[0] = 0.\n    w0[1] = 2.\n    w0[2] = 20.\n\n    # instantiate kernel\n    rhs = LorenzKernel(rho=rho, sigma=sigma, beta=beta)\n\n    # create output arrays\n    ww   = None #np.empty( (result_len,n), dtype=DTYPE, order=""C"" )    # result array for w; if None, will be created by ivp()\n    ff   = np.empty( (result_len,n), dtype=DTYPE, order=""C"" )          # optional,  result array for w\', could be None\n    fail = np.empty( (n_saved_timesteps,), dtype=np.intc, order=""C"" )  # optional,  fail flag for each timestep, could be None\n\n    # solve problem\n    ww,tt = pydgq.solver.odesolve.ivp( integrator=integrator, allow_denormals=False,\n                                       w0=w0, dt=dt, nt=nt,\n                                       save_from=save_from, interp=nt_vis,\n                                       rhs=rhs,\n                                       ww=ww, ff=ff, fail=fail,\n                                       maxit=10 )\n\n    # visualize\n    #\n    # http://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html\n    #\n    print( ""** Plotting solution **"" )\n    fig = plt.figure(1)\n    plt.clf()\n\n    # Axes3D has a tendency to underestimate how much space it needs; it draws its labels\n    # outside the window area in certain orientations.\n    #\n    # This causes the labels to be clipped, which looks bad. We prevent this by creating the axes\n    # in a slightly smaller rect (leaving a margin). This way the labels will show - outside the Axes3D,\n    # but still inside the figure window.\n    #\n    # The final touch is to set the window background to a matching white, so that the\n    # background of the figure appears uniform.\n    #\n    fig.patch.set_color( (1,1,1) )\n    fig.patch.set_alpha( 1.0 )\n    x0y0wh = [ 0.02, 0.02, 0.96, 0.96 ]  # left, bottom, width, height      (here as fraction of subplot area)\n\n    ax = Axes3D(fig, rect=x0y0wh)\n\n    # show the discontinuities at timestep boundaries if using dG (and actually have something to draw within each timestep)\n    if integrator == ""dG"" and nt_vis > 1:\n        tt = discontify( tt, endj - 1, fill=""nan"" )\n\n        wtmp = np.empty( (tt.shape[0],n), dtype=DTYPE, order=""C"" )\n        for j in range(n):\n            # we need the copy() to get memory-contiguous data for discontify() to process\n            wtmp[:,j] = discontify( ww[:,j].copy(), endj - 1, fill=""nan"" )\n\n        ax.plot( wtmp[:,0], wtmp[:,1], wtmp[:,2], linewidth=0.5  )\n    else:\n        ax.plot( ww[:,0], ww[:,1], ww[:,2], linewidth=0.5  )\n\n    plt.grid(b=True, which=""both"")\n    plt.axis(""tight"")\n    ax.set_xlabel(r""$x$"")\n    ax.set_ylabel(r""$y$"")\n    ax.set_zlabel(r""$z$"")\n    plt.suptitle(r""Lorenz system: $\\rho = %g$, $\\sigma = %g$, $\\beta = %g$, $x_0 = %g$, $y_0 = %g$, $z_0 = %g$"" % (rho, sigma, beta, w0[0], w0[1], w0[2]))\n\nif __name__ == \'__main__\':\n    print(""** Solving the Lorenz system **"")\n\n    nt_vis = nt_vis_galerkin\n    init(q=q, method=""dG"", nt_vis=nt_vis, rule=None)\n    test(integrator=""dG"", nt_vis=nt_vis)\n    plt.show()\n\n'"
test/python_kernel_test.py,12,"b'# -*- coding: utf-8 -*-\n#\n# Tests/usage examples for a custom Python-based kernel.\n\nfrom __future__ import division, print_function\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom pydgq.solver.types import DTYPE\nfrom pydgq.solver.galerkin import init\nfrom pydgq.solver.kernel_interface import PythonKernel\nimport pydgq.solver.odesolve\nfrom pydgq.utils.discontify import discontify  # for plotting dG results\n\n\n#####################\n# config for testing\n#####################\n\nq = 2  # degree of basis for dG and cG\n\n# How many visualization (interpolation) points to use within each timestep for Galerkin methods.\n#\n# Note that the dG solution has the best accuracy at the endpoint of the timestep;\n# to compare apples-to-apples with classical integrators, this should be set to 1.\n#\n# Larger values (e.g. 11) are useful for visualizing the behavior of the dG solution inside\n# the timestep (something the classical integrators do not model at all).\n#\nnt_vis_galerkin = 1\n\nnt = 100  # number of timesteps\ndt = 0.1  # timestep size\n\nsave_from = 0  # see pydgq.solver.odesolve.ivp()\n\n\n#####################\n# custom kernel\n#####################\n\n# A simple cosine kernel with phase-shifted components.\n#\n# See cython_kernel.pyx for a Cython-accelerated version of this.\n#\n# The custom kernel only needs to override callback(); even __init__ is not strictly needed,\n# unless adding some custom parameters (like here).\n#\nclass MyKernel(PythonKernel):\n    def __init__(self, n, omega):  # omega : rad/s\n        # super\n        PythonKernel.__init__(self, n)\n\n        # custom init\n        self.omega = omega\n\n    def callback(self, t):\n        for j in range(self.n):\n            phi0_j = (float(j+1) / self.n) * 2. * np.pi\n            self.out[j] = np.cos(phi0_j + self.omega*t)\n\n    # known analytical solution, for testing the integrators\n    #\n    def reference_solution(self, tt):\n        tt  = np.atleast_1d(tt)\n        ww  = np.empty( (tt.shape[0],self.n), dtype=DTYPE, order=""C"" )\n        sol = lambda t,phi0 : 1./self.omega * np.sin(phi0 + self.omega*t)\n        for j in range(self.n):\n            phi0_j  = (float(j+1) / self.n) * 2. * np.pi\n            ww[:,j] = sol(tt,phi0_j) - sol(0.,phi0_j)  # shift to account for the initial condition (all solution components start at zero)\n        return ww\n\n\n#####################\n# main program\n#####################\n\n# rel_tol : how close the numerical solution must be to the exact analytical one, in l-infinity norm (max abs)\ndef test(integrator, nt_vis, rel_tol=1e-2, vis=False):\n    n_saved_timesteps = pydgq.solver.odesolve.n_saved_timesteps( nt, save_from )\n    result_len        = pydgq.solver.odesolve.result_len( nt, save_from, interp=nt_vis )\n    startj,endj       = pydgq.solver.odesolve.timestep_boundaries( nt, save_from, interp=nt_vis )\n\n    n   = 3  # number of DOFs in the 1st-order system\n    w0  = np.zeros( (n,), dtype=DTYPE, order=""C"" )  # a trivial IC\n\n    # instantiate kernel\n    rhs = MyKernel(n, omega=0.1 * (2. * np.pi))\n\n    # create output arrays\n    ww   = None #np.empty( (result_len,n), dtype=DTYPE, order=""C"" )    # result array for w; if None, will be created by ivp()\n    ff   = np.empty( (result_len,n), dtype=DTYPE, order=""C"" )          # optional,  result array for w\', could be None\n    fail = np.empty( (n_saved_timesteps,), dtype=np.intc, order=""C"" )  # optional,  fail flag for each timestep, could be None\n\n    # solve problem\n    ww,tt = pydgq.solver.odesolve.ivp( integrator=integrator, allow_denormals=False,\n                                       w0=w0, dt=dt, nt=nt,\n                                       save_from=save_from, interp=nt_vis,\n                                       rhs=rhs,\n                                       ww=ww, ff=ff, fail=fail,\n                                       maxit=100 )\n\n    # check result\n    ww_ref = rhs.reference_solution(tt)\n    relerr_linfty = np.linalg.norm(ww - ww_ref, ord=np.inf) / np.linalg.norm(ww_ref, ord=np.inf)\n    if (relerr_linfty < rel_tol).all():\n        passed = 1\n        if not vis:\n            print(""PASS, %03s, tol=% 7g, relerr=%g"" % (integrator, rel_tol, relerr_linfty))\n    else:\n        passed = 0\n        if not vis:\n            print(""FAIL, %03s, tol=% 7g, relerr=%g"" % (integrator, rel_tol, relerr_linfty))\n\n    # visualize if requested\n    if vis:\n        plt.figure(1)\n        plt.clf()\n\n        # show the discontinuities at timestep boundaries if using dG (and actually have something to draw within each timestep)\n        if integrator == ""dG"" and nt_vis > 1:\n            tt = discontify( tt, endj - 1, fill=""nan"" )\n            for j in range(n):\n                # we need the copy() to get memory-contiguous data for discontify() to process\n                wtmp = discontify( ww_ref[:,j].copy(), endj - 1, fill=""nan"" )\n                plt.plot( tt, wtmp, \'r--\' )  # known exact solution\n\n                wtmp = discontify( ww[:,j].copy(), endj - 1, fill=""nan"" )\n                plt.plot( tt, wtmp, \'k-\'  )  # numerical solution\n        else:\n            plt.plot( tt, ww_ref, \'r--\' )  # known exact solution\n            plt.plot( tt, ww,     \'k-\'  )  # numerical solution\n\n        plt.grid(b=True, which=""both"")\n        plt.axis(""tight"")\n        plt.xlabel(r""$t$"")\n        plt.ylabel(r""$w(t)$"")\n\n    return passed\n\n\nif __name__ == \'__main__\':\n    print(""** Testing integrators with custom Python kernel **"")\n\n    # ""SE"" is not applicable, since we are testing a 1st-order problem\n    stuff_to_test = ( (""IMR"", 1e-3,  False),\n                      (""BE"",  1e-1,  False),\n                      (""RK4"", 1e-8,  False),\n                      (""RK3"", 1e-6,  False),\n                      (""RK2"", 1e-3,  False),\n                      (""FE"",  1e-1,  False),\n                      (""dG"",  1e-12, True ),\n                      (""cG"",  1e-4,  True )\n                    )\n    n_passed = 0\n\n    for integrator,rel_tol,is_galerkin in stuff_to_test:\n        if is_galerkin:\n            nt_vis = nt_vis_galerkin  # visualization points per timestep in Galerkin methods\n            init(q=q, method=integrator, nt_vis=nt_vis, rule=None)\n        else:\n            nt_vis = 1   # other methods compute only the end value for each timestep\n\n        n_passed += test(integrator, nt_vis, rel_tol)\n    print(""** %d/%d tests passed **"" % (n_passed, len(stuff_to_test)))\n\n#    # DEBUG: draw something\n#    #\n#    nt_vis = nt_vis_galerkin\n#    init(q=q, method=""dG"", nt_vis=nt_vis, rule=None)\n#    test(integrator=""dG"", nt_vis=nt_vis, vis=True)\n#    plt.show()\n\n'"
test/setup.py,0,"b'# -*- coding: utf-8 -*-\n#\n""""""Setup script for compiling the Cython example.\n\nUsage:\n    python -m setup build_ext --inplace\n""""""\n\nfrom __future__ import division, print_function, absolute_import\n\n#########################################################\n# Config\n#########################################################\n\n# choose build type here\n#\nbuild_type=""optimized""\n#build_type=""debug""\n\n\n#########################################################\n# Init\n#########################################################\n\n# check for Python 2.7 or later\n# http://stackoverflow.com/questions/19534896/enforcing-python-version-in-setup-py\nimport sys\nif sys.version_info < (2,7):\n    sys.exit(\'Sorry, Python < 2.7 is not supported\')\n\nimport os\n\nfrom setuptools import setup\nfrom setuptools.extension import Extension\n\ntry:\n    from Cython.Build import cythonize\nexcept ImportError:\n    sys.exit(""Cython not found. Cython is needed to build the extension modules for pydgq."")\n\n\n#########################################################\n# Definitions\n#########################################################\n\nextra_compile_args_math_optimized    = [\'-fopenmp\', \'-march=native\', \'-O2\', \'-msse\', \'-msse2\', \'-mfma\', \'-mfpmath=sse\']\nextra_compile_args_math_debug        = [\'-fopenmp\', \'-march=native\', \'-O0\', \'-g\']\n\nextra_compile_args_nonmath_optimized = [\'-O2\']\nextra_compile_args_nonmath_debug     = [\'-O0\', \'-g\']\n\nextra_link_args_optimized    = [\'-fopenmp\']\nextra_link_args_debug        = [\'-fopenmp\']\n\n\nif build_type == \'optimized\':\n    my_extra_compile_args_math    = extra_compile_args_math_optimized\n    my_extra_compile_args_nonmath = extra_compile_args_nonmath_optimized\n    my_extra_link_args            = extra_link_args_optimized\n    debug = False\n    print( ""build configuration selected: optimized"" )\nelse: # build_type == \'debug\':\n    my_extra_compile_args_math    = extra_compile_args_math_debug\n    my_extra_compile_args_nonmath = extra_compile_args_nonmath_debug\n    my_extra_link_args            = extra_link_args_debug\n    debug = True\n    print( ""build configuration selected: debug"" )\n\n\n#########################################################\n# Helpers\n#########################################################\n\nmy_include_dirs = ["".""]  # IMPORTANT, see https://github.com/cython/cython/wiki/PackageHierarchy\n\ndef ext(extName):\n    extPath = extName.replace(""."", os.path.sep)+"".pyx""\n    return Extension( extName,\n                      [extPath],\n                      extra_compile_args=my_extra_compile_args_nonmath\n                    )\ndef ext_math(extName):\n    extPath = extName.replace(""."", os.path.sep)+"".pyx""\n    return Extension( extName,\n                      [extPath],\n                      extra_compile_args=my_extra_compile_args_math,\n                      extra_link_args=my_extra_link_args,\n                      libraries=[""m""]  # ""m"" links libm, the math library on unix-likes; see http://docs.cython.org/src/tutorial/external.html\n                    )\n\n\n#########################################################\n# Modules\n#########################################################\n\next_module_ckernel  = ext_math( ""cython_kernel"" )\n\n#########################################################\n\nsetup(\n    ext_modules = cythonize( [ ext_module_ckernel ],\n                             include_path = my_include_dirs,\n                             gdb_debug = debug )\n)\n\n'"
pydgq/solver/__init__.py,0,b''
pydgq/utils/__init__.py,0,b''
pydgq/utils/listutils.py,0,"b'# -*- coding: utf-8 -*-\n""""""List-handling utilities.""""""\n\nfrom __future__ import division, print_function, absolute_import\n\n\ndef load_balance_list(L, n):\n    """"""Given a list of arbitrary items, split it to n roughly equal-sized parts.\n\n    This is useful for dividing a list of work items in MPI parallelization.\n    It is assumed that each work item takes the same amount of time; hence the\n    initial distribution is generated by naive integer division.\n\n    If len(L) does not divide evenly with n, the remaining items are distributed\n    on an item-by-item basis to the first (len(L) mod n) parts.\n\n    If n > len(L), the items will be distributed on an item-by-item basis to the\n    first len(L) parts, and the rest of the parts will get an empty list.\n    \n    Examples:\n        load_balance_list(range(14), 2)\n            =>  [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12, 13]]\n        In this case, the division is even (no remainder).\n\n        load_balance_list(range(15), 2)\n            =>  [[0, 1, 2, 3, 4, 5, 6, 14], [7, 8, 9, 10, 11, 12, 13]]\n        In this case, the item ""14"" is left over after the integer division.\n        The leftover item is placed in the first part.\n\n        load_balance_list(range(4), 8)\n            =>  [[0], [1], [2], [3], [], [], [], []]\n        In this case, n is so large that there are not enough items to place\n        even one item in each part. The empty list is generated for those parts\n        for which no item is available.\n\n    Parameters:\n        L = any Python list.\n\n    Return value:\n        List of lists: L_out = [L1, L2, L3, ..., Ln]\n        where L1, L2, ... are sublists of L.\n\n        It always holds that len(L_out) == n. Note that the lengths of the\n        Lj (individual lists inside L_out) may differ by one item, depending on\n        whether the division was even.\n\n        If n == 1, the return value is [L] for compatibility\n        of the output format with the n > 1 case.\n\n    """"""\n    if n < 1:\n        raise ValueError(""n must be >= 1; got %d"" % (n))\n\n    # If we\'re splitting L to one part, this is a no-op.\n    # But wrap it - the caller is expecting a list of lists.\n    #\n    if n == 1:\n        return [L]\n\n    nitems = len(L)  # number of items to be distributed\n\n    out = []\n    if n <= nitems:\n        # Items per process.\n        blocklen  = nitems // n  # integer division!\n\n        # Leftover items.\n        remainder = nitems - blocklen*n  # this is always < n\n\n        # Distribute the items that divided evenly.\n        for m in range(n):\n            offs = m*blocklen\n            out.append( L[offs:offs+blocklen] )\n\n        # Distribute the leftovers (if any).\n        if remainder > 0:\n            offs = nitems - remainder\n            for m in range(remainder):\n                out[m].append( L[offs] )\n                offs += 1\n    else:\n        # Distribute one item to each part as long as items are available.\n        for m in range(nitems):\n            out.append( [L[m]] )  # wrap the item to make a single-item list.\n\n        # Give an empty list to the rest of the parts.\n        nempties = n - nitems  # this many empty lists are needed\n        for m in range(nempties):\n            out.append( [] )\n\n    assert( len(out) == n )\n    return out\n\n'"
pydgq/utils/mpi_shim.py,15,"b'# -*- coding: utf-8 -*-\n""""""MPI support wrapper using mpi4py as backend.\n\nLicense: 2-clause BSD; copyright 2012-2017 Juha Jeronen and University of Jyv\xc3\xa4skyl\xc3\xa4.\n\nTo load MPI, import this module. The module is designed so that it can\nalways be imported, regardless of whether mpi4py is installed.\n\nThe availability of the mpi4py library can then be queried at runtime\nusing has_mpi().\n\nSome rudimentary wrappers (get_size(), get_rank(), get_comm_world())\nare provided; all the rest should be done manually using mpi4py,\nand only if has_mpi() returns True. This way, the same code can\nrun both with and without MPI.\n\nSee also gather_varlength_array(), a buffer-based fast gather\nfor rank-1 NumPy arrays of varying lengths (like gatherv,\nbut determines the sizes automatically).\n\nExample:\n\nimport mpi_shim\n\nMPI    = mpi_shim.get_mpi()         # either a reference to mpi4py.MPI, or None if n/a\nnprocs = mpi_shim.get_size()        # actual value from mpi4py, or 1    if n/a\nmy_id  = mpi_shim.get_rank()        # actual value from mpi4py, or 0    if n/a\ncomm   = mpi_shim.get_comm_world()  # actual value from mpi4py, or None if n/a\n\n# The rest works as usual ( adapted from http://mpi4py.scipy.org/docs/usrman/tutorial.html ):\n\ndata = (my_id+1)**2  # <insert meaningful calculation here>\nif MPI is not None:\n   data = comm.gather(data, root=0)\n\nif my_id == 0:\n   for i in range(nprocs):\n       assert data[i] == (i+1)**2\nelse:\n   assert data is None\n""""""\n\nfrom __future__ import division, print_function, absolute_import\n\nimport numpy as np  # used in gather_varlength_array()\n\ntry:\n    import mpi4py.MPI as MPI\n\n    __comm_world = MPI.COMM_WORLD\n    __nproc = MPI.Comm.Get_size(__comm_world)\n    __my_id = MPI.Comm.Get_rank(__comm_world)\n\n    __library_ok = True\n\n    # Considering the code that uses this module,\n    # this is maybe more sensible than always True.\n    #\n    __mpi_available = (__nproc > 1)\nexcept ImportError:\n    __comm_world = None\n    __nproc = 1\n    __my_id = 0\n\n    __library_ok = False\n    __mpi_available = False\n\n# version of mpi_shim\n#\n# Especially important, since this module is to be copied to each project that uses it.\n#\n__version__ = ""1.0.0""\n\n\n##########################################\n# Wrapper functions\n##########################################\n\ndef has_mpi4py_library():\n    """"""Return whether the mpi4py library is available (bool).\n\n    This is just a simple library loadedness check.\n\n    If you want to also see whether there are at least 2 MPI processes\n    in the current group (and hence parallel processing makes sense),\n    use has_mpi() instead.\n\n    """"""\n    return __library_ok\n\ndef has_mpi():\n    """"""Return True if the mpi4py library is available and there are at least\n    2 MPI processes in the current group. Otherwise return False.\n\n    This is a ""does parallel processing make sense?"" check.\n\n    See has_mpi4py_library() for just checking library availability.\n\n    """"""\n    return __mpi_available\n\ndef get_mpi():\n    """"""Return a reference to the loaded mpi4py.MPI instance.\n\n    If mpi4py is not available, return None.\n    """"""\n    if __library_ok:\n        return MPI\n    else:\n        return None\n\ndef get_size():\n    """"""Return the size (int) of the MPI group.\n\n    See also:\n        get_rank()    (get the ID of the currently running instance)\n    """"""\n    return __nproc\n\ndef get_rank():\n    """"""Return the MPI rank (int) of the running instance.\n\n    The rank is the identifier of the process, numbered 0, 1, ..., get_size()-1.\n\n    See also:\n        get_size()    (get number of running instances)\n    """"""\n    return __my_id\n\ndef get_comm_world():\n    """"""Return mpi4py.MPI.COMM_WORLD (reference) if mpi4py is available, or None if n/a.""""""\n    return __comm_world\n\n\n###################################\n# MPI-aware printing\n###################################\n\ndef mpi_print(*args):\n    """"""Print args, but only if running in the root process.\n\n    See also:\n        mpi_allprint()    (gather args from all MPI ranks, print in root process)\n    """"""\n    if __my_id == 0:  # also true if no MPI\n        print(*args)\n\ndef mpi_allprint(*args):\n    """"""Gather args from all MPI ranks, print them all in the root process.\n\n    The root process makes a separate call to built-in print for args from each rank.\n\n    MPI rank of the messages is not reported; add this data to the message yourself if desired.\n\n    See also:\n        mpi_print()    (print only in the root process)\n    """"""\n    comm = get_comm_world()\n\n    if comm is None:  # no MPI\n        print(*args)\n        return\n\n    allargs = comm.gather(args, root=0)\n    if get_rank() == 0:\n        for args in allargs:\n            print(*args)\n    # else no-op\n\n\n###################################\n# Helpers\n###################################\n\ndef gather_varlength_array(data, datatype, do_allgather=False):\n    """"""Fast (buffer-based) MPI gather for variable-sized data using NumPy arrays.\n\n    Data type of all items must be the same. It must be a primitive datatype\n    that mpi4py.MPI.Gather() supports.\n\n    Parameters:\n        data : Python list or rank-1 array\n            Local data items in each process, to be gathered at the root process.\n        datatype : NumPy dtype\n            Datatype specification (e.g. np.int, np.float64, np.complex128, ...).\n        do_allgather : bool, optional\n            If True, do an Allgather() instead of a Gather().\n\n    In different processes, `data` is allowed to have a different number of elements,\n    but `datatype` must be the same.\n\n    `datatype` is mandatory so that zero-length input can be handled properly.\n\n    Return value:\n        In the root process:\n            tuple (data, I), where\n                data : rank-1 np.array\n                    flattened data array, with data from all processes\n                    concatenated in order of increasing MPI rank.\n                I : rank-1 np.array of length get_size()+1\n                    data from process j is located in data[ I[j] : I[j+1] ].\n                    The end fencepost for the last process is provided to avoid\n                    the need for special-casing in calling code.\n\n        In all other processes:\n            If do_allgather is True, same as in root process, otherwise None.\n    """"""\n    # Use an efficient buffer transfer for gathering the data.\n    #\n    # Because the amount of data per process varies, we must first allgather the data lengths\n    # to determine a suitable buffer size. We will use the maximum size in all processes.\n    #\n    nprocs = get_size()\n    data_lengths = np.empty( [nprocs], dtype=int )\n    n_local_items = np.size(data)\n    n_local_items_array = np.array( n_local_items, dtype=int )  # rank-1 array of length 1\n\n    def lengths_to_offsets(lengths):\n        return np.concatenate( ( [0], np.cumsum(lengths) ) )\n\n    comm = get_comm_world()\n    comm.Allgather( n_local_items_array, data_lengths )  # order of parms: sendbuf, recvbuf\n\n    # Any leftover array elements are simply unused; we use data_lengths to determine\n    # which elements to read.\n    #\n    max_entries_per_process = np.max(data_lengths)\n    nentries = nprocs*max_entries_per_process\n\n    # Check special case: nothing to do if no data\n    #\n    if max_entries_per_process == 0:\n        if get_rank() == 0  or  do_allgather:\n            return (np.empty( [0], dtype=datatype ), lengths_to_offsets(data_lengths))\n        else:\n            return None\n\n    # Prepare receive buffer at root process\n    # (or at all processes if allgathering)\n    #\n    if get_rank() == 0  or  do_allgather:\n        data_recvbuf = np.empty( [nentries], dtype=datatype )\n    else:\n        data_recvbuf = None\n\n    # Prepare and fill send buffers in all processes\n    #\n    data_sendbuf = np.empty( [max_entries_per_process], dtype=datatype )\n    data_sendbuf[0:n_local_items] = np.array( data, dtype=datatype )\n\n    if do_allgather:\n        comm.Allgather( data_sendbuf, data_recvbuf )\n    else:\n        comm.Gather( data_sendbuf, data_recvbuf, root=0 )\n\n    # In the root process, extract the data.\n    # (or in all processes if we are allgathering)\n    #\n    if get_rank() == 0  or  do_allgather:\n        result = np.empty( [np.sum(data_lengths)], dtype=datatype )\n\n        # This is similar to util.flatten_list_of_arrays(), but from a single np.array buffer.\n        offs_out = 0\n        for nproc in range(nprocs):\n            offs_in = nproc*max_entries_per_process\n            datalen = data_lengths[nproc]\n            if datalen > 0:\n                result[offs_out:(offs_out+datalen)] = data_recvbuf[offs_in:(offs_in+datalen)]\n                offs_out += datalen\n        assert( offs_out == np.sum(data_lengths) )\n        return (result, lengths_to_offsets(data_lengths))\n    else:\n        return None\n\ndef allgather_varlength_array(data, datatype):\n    """"""Same as gather_varlength_array(data, datatype, do_allgather=True).""""""\n    return gather_varlength_array(data, datatype, do_allgather=True)\n\n'"
pydgq/utils/precalc.py,21,"b'# -*- coding: utf-8 -*-\n""""""Generate data file (pydgq_data.bin) for pydgq.\n\nThe generation is very slow; MPI parallelization is supported to boost performance.\n\nRun this module as the main program (with or without mpiexec) to perform the precalculation.\nCommand-line options are available; pass the standard --help flag to see them.\n""""""\n\nfrom __future__ import division, print_function, absolute_import\n\nimport time\n\ntry:\n    import cPickle as pickle  # Python 2.7\nexcept ImportError:\n    import pickle  # Python 3.x\n\nimport functools  # reduce (Python 3 compatibility)\n\nimport numpy as np\n\ntry:\n    import mpmath  # Python 3.x\nexcept ImportError:\n    import sympy.mpmath as mpmath  # Python 2.7\n\nfrom pydgq.solver.types import RTYPE  # the precalc data is always real-valued regardless of DTYPE\nimport pydgq.utils.mpi_shim as mpi_shim\nimport pydgq.utils.listutils as listutils\n\n# version of precalc.py (this is NOT the version of the pydgq package)\n__version__ = ""1.0.2""\n\n\n############################################################################################################\n# Worker classes\n############################################################################################################\n\n# Memoization\nclass Cache:\n    """"""Cache for values that have been computed so far. Useful if precomputing for a large number of different divisions of [-1,1] some of which contain identical points.\n\n    The cache is meant to be shared across all instances of Precalc in the current process.\n\n    """"""\n    def __init__(self, q):\n        """"""q = maximum degree (see Precalc)""""""\n\n        self.q     = q\n        self._data = {}\n\n        # create empty caches for all polynomial degrees\n        for j in range(q+1):\n            self._data[j] = {}\n\n    # syntactic sugar\n\n    # e.g. ""if cache contains 3"" --> a sub-cache exists for 3rd degree polynomials\n    def __contains__(self, key):\n        return key in self._data\n\n    # e.g. ""cache[3]"" --> the sub-cache for 3rd degree polynomials\n    def __getitem__(self, key):\n        return self._data[key]\n\n    # the caller is supposed to write only to the individual sub-caches, not directly into the top level one\n    def __setitem__(self, key, value):\n        raise NotImplementedError(""The top-level cache in Cache does not support __setitem__(), maybe you meant to write into cache[j]?"")\n\n\nclass Precalc:\n    """"""Precalculate hierarchical (Lobatto) basis functions up to the given degree q (>= 1) at the given points (rank-1 np.array of length >= 1) on the reference element [-1,1].""""""\n\n    # Legendre polynomials, high precision from sympy.mpmath to avoid cancellation in hierarchical basis functions\n    _P = mpmath.legendre  # lambda j, x: ...\n\n    def __init__(self, q, xx, cache=None):\n        """"""q = maximum degree, xx = vector of points, cache = Cache instance or None\n\n        The cache is used to memoize results for each unique value in xx. This is mainly useful\n        for sharing evaluations to future instances, which may get some of the same elements in their xx.\n        """"""\n\n        self.x  = None\n        self.y  = None  # ""y = f(x)""\n\n        assert(q >= 1)\n        assert(np.size(xx) >= 1)\n\n        self.q     = q\n        self.xx    = xx\n        self.cache = cache\n\n        self._build_hierarchical_basis()\n\n    def _build_hierarchical_basis(self):\n        """"""Build a list of Python functions that evaluate the hierarchical basis functions.\n\n        After running this, self.N[j] is a one-argument lambda that evaluates N_j at x. (Not vectorized!)\n\n        """"""\n        q = self.q\n\n        N = []\n        N.append( lambda x: (1./2.) * (1. - x) )  # linear, left endpoint\n        N.append( lambda x: (1./2.) * (1. + x) )  # linear, right endpoint\n\n        # bubble functions (see user manual)\n        for j in range(2,q+1):\n            # HACK: Python 3 compatibility: we must float(j), because some part of the toolchain here wants to convert all arguments to mpf, which does not work for int.\n            N.append(  (  lambda j: lambda x : ( self._P(j, x) - self._P(j-2, x) ) / np.sqrt( 2. * (2.*j - 1.) )  )(float(j))  )  # use factory to bind j at define time\n\n        self.N = N\n\n    def run(self):\n        """"""Do the precomputation. MPI-fied.""""""\n\n        # Create indices and corresponding x values.\n        #\n        nx    = np.size(self.xx)\n        all_i = range(nx)\n        all_x = self.xx\n\n        # Divide the work among the tasks.\n        #\n        split_i = listutils.load_balance_list( all_i, mpi_shim.get_size() )\n        my_i    = split_i[ mpi_shim.get_rank() ]\n\n        # Do the local work for task-local items.\n        #\n        N = self.N\n        ly = np.empty( [self.q+1, len(my_i)], dtype=RTYPE )  # local y (basis function values)\n        # use caching (if available) to avoid slow re-evaluation of the polynomials for x values already seen.\n        if self.cache is not None:\n            for li,gi in enumerate(my_i):  # local i, corresponding global i\n                x = all_x[gi]\n                for j in range(self.q+1):\n                    # yes, we use floats as keys, and we do really want floating-point equality down to the last ulp in order to accept the entry as matched.\n                    if x in self.cache[j]:\n                        ly[j,li] = self.cache[j][x]\n                    else:\n                        ly[j,li] = N[j]( x )  # Compute. Writing into the ly array forces conversion to RTYPE, as we want.\n                        self.cache[j][x] = ly[j,li]\n\n        else:\n            for li,gi in enumerate(my_i):  # local i, corresponding global i\n                x = all_x[gi]\n                for j in range(self.q+1):\n                    ly[j,li] = N[j]( x )\n\n        # Gather results.\n        #\n        if mpi_shim.get_size() > 1:\n            # Allocate an array to receive the global data.\n            #\n            # This will need to be reordered after receiving, because any leftover items (modulo nprocs) are processed by some of the first tasks.\n            # We have the correct x indices in split_i; at the first step we just glue together the data arrays in sequence using MPI\'s Gatherv().\n            #\n            gshape  = [nx, self.q+1]  # transposed global shape (see below)\n            recv    = np.empty( np.prod(gshape), dtype=RTYPE )  # linear buffer for global data (we\'ll reshape this after receiving)\n\n            counts  = [len(lis)*(self.q+1) for lis in split_i]  # lis contains the task-local item indices; all tasks handle q+1 basis functions\n            disps   = [0] + np.cumsum( np.array(counts, dtype=int)[:-1] ).tolist()   #  e.g. [3,3,3,2] -> [0,3,6,9]\n\n            assert( np.prod(np.size(ly)) == counts[mpi_shim.get_rank()] )  # in each task, computed local data size must match actual size of local data\n\n            # The number of basis functions is constant (q+1), while the length of my_i may vary across the tasks.\n            #\n            # Thus, we transpose the local data array before linearizing it for sending, so that the ""slices"" are of constant length.\n            # This makes it simpler to de-linearize the global array after the gather.\n            #\n            # For Gatherv(), Allgatherv(), sendbuf, recvbuf, see:\n            #    https://wiki.gwdg.de/index.php/Mpi4py\n            #\n            sendbuf = [ np.reshape(np.transpose(ly),-1), counts[mpi_shim.get_rank()] ]  # local_data, local_count (must be counts[mpi_rank])\n\n            # TODO: other dtypes?\n            if RTYPE == np.float64:\n                MPI_datatype = mpi_shim.get_mpi().DOUBLE\n            elif RTYPE == np.float32:\n                MPI_datatype = mpi_shim.get_mpi().SINGLE\n            else:\n                raise NotImplementedError(""Unknown RTYPE %s, cannot transmit data buffer"" % (RTYPE))\n            recvbuf = [ recv, counts, disps, MPI_datatype ]  # data, counts, displacements, mpi_datatype\n            comm = mpi_shim.get_comm_world()\n            comm.Allgatherv( sendbuf, recvbuf )\n\n            # De-linearize the received array.\n            #\n            # This utilizes the fact that in the transposed data, each ""item"" (one x value for all basis functions) has the same length.\n            #\n            recv = np.reshape(recv, gshape)\n\n            # Get the permutation of rows that was applied by splitting all_i -> split_i and then gluing together the split_i in sequence.\n            #\n            # E.g. with nx=11, nprocs=4, we could have\n            #\n            # MPI rank 0: [0,1,8]\n            # MPI rank 1: [2,3,9]\n            # MPI rank 2: [4,5,10]\n            # MPI rank 3: [6,7]\n            #\n            # After gather this becomes:\n            #\n            # [0,1,8,2,3,9,4,5,10,6,7]\n            #\n            # These are the indices in all_x corresponding to each row in the transposed data array.\n            #\n            perm = np.array( functools.reduce( lambda x,y: x+y,  split_i ), dtype=int )\n\n            # Get the inverse permutation.\n            #\n            invperm       = np.empty_like( perm )\n            invperm[perm] = np.arange( np.size(perm), dtype=int )  # inverse permutation of range(N): invperm[perm] = range(N)\n\n            # Remap by inverse permutation. This orders the data correctly, so that the ith row of the transposed data corresponds to basis function values at all_x[i].\n            #\n            # Undo the transpose (applying it again) to obtain the final result. Then self.y[j,i] is N[j]( all_x[i] ).\n            #\n            self.y = np.transpose( recv[invperm] )\n            self.x = all_x\n\n        else:  # only one task, which processes everything\n            self.y = ly\n            self.x = all_x\n\n\n############################################################################################################\n# Main program\n############################################################################################################\n\ndef main(q, nx, **kwargs):\n    """"""Create precomputed arrays of basis function values.\n\n    Parameters are the maximum degree q (>= 1) and the maximum number of visualization points nx (>= 1).\n\n    Since the basis is hierarchical, any lower degree is obtained by simply chopping off the extraneous rows, as y[:(desired_q + 1),:].\n\n    Thus, for visualization, different arrays are needed only for different numbers of points, which are taken to be equally spaced\n    on the reference element [-1,1]. Arrays are generated for 1, 2, ..., nx points. Caching (memoization) is used to accelerate\n    the computation so that only unique points are actually evaluated (e.g. arrays with 3 and 5 points share the point at 0.0)\n\n    For integration, separate arrays are created, of function values at Gauss-Legendre points for integration rules of order 1,2,...,q+1.\n    The q+1 rule is sufficient for evaluation of mass matrices of the form N(x)*N(x) (where N has at most degree q).\n    The q rule is sufficient for N\'(x)*N(x) (needed in dG(q)) and lower orders. (This is assuming an affine coordinate mapping\n    from the reference element to the actual timestep, so that the Jacobian of the coordinate mapping is constant on each element.)\n    """"""\n\n    if mpi_shim.get_size() < 2:\n        msg = "" This script can benefit from MPI parallelization; consider running with \'mpiexec -n <some reasonable number> python -m precalc\\n""\n        sep = ( ""="" * len(msg) ) + ""\\n""\n        mpi_shim.mpi_print(""\\n\\n%s%s%s\\n"" % (sep, msg, sep))\n\n    data = {}\n\n    # common metadata\n    data[""maxq""] = q  # note: one less than number of rows in y items\n\n    # memoize for faster computation\n    cache = Cache(q)\n\n    #########################################################\n    # Create array for visualization (equally spaced points)\n    #########################################################\n\n    mpi_shim.mpi_print( ""Maximum degree %d (%d basis functions)"" % (q, q+1) )\n\n    mpi_shim.mpi_print( ""\\nGenerating visualization data"" )\n    mpi_shim.mpi_print( ""Equally spaced points: max points = %d"" % nx )\n\n    data[""vis""] = {}  # data at visualization points\n    data[""vis""][""maxnx""] = nx\n\n    for k in range(1,nx+1):\n        if k == 1:\n            # Considering the use of this data in ODE system integration (odesolve.pyx),\n            # the end of the timestep is the most reasonable choice in the case of a single point.\n            #\n            xx = np.array( [1.], dtype=RTYPE )\n        else:\n            xx = np.linspace( -1., 1., k, dtype=RTYPE )\n\n        mpi_shim.mpi_print( ""    Computing for %d points"" % k )\n\n        # The cache avoids the need to re-compute the basis functions for any values of x\n        # already seen at an earlier iteration of this loop, making this run much faster.\n        #\n        precalc = Precalc(q, xx, cache)\n        precalc.run()\n\n        data[""vis""][k] = { ""x"" : precalc.x, ""y"" : precalc.y }\n\n#    # NOTE: the data is accessed by indexing by [""vis""][how many points], like this:\n#    print data[""vis""][3][""x""]  # points (x values)\n#    print data[""vis""][3][""y""]  # corresponding function values (y values)\n\n\n    #########################################################\n    # Create array for integration (Gauss-Legendre points)\n    #########################################################\n\n    mpi_shim.mpi_print( ""\\nGenerating integrator data (Gauss-Legendre rule)"" )\n\n    data[""integ""] = {}  # data at Gauss-Legendre integration points\n    data[""integ""][""maxrule""] = q+1\n\n    for d in range(1,q+2):\n        mpi_shim.mpi_print( ""    Computing for GL rule of order %d"" % d )\n\n        xx,ww = np.polynomial.legendre.leggauss( d )\n\n        # We run this from order 1 up to order q+1, even though the lower-degree rules may not be high enough for exact integration\n        # of the highest-degree basis functions, in order to allow for under-integration by the user later.\n        #\n        # Note that if a lower-degree basis is being used (by chopping off rows of the data array, see above),\n        # one can then chop this data, too, using a lower-degree (desired_q + 1) rule to still obtain exact integration.\n        #\n        # E.g.\n        #   o = data[""integ""][3]  # Gauss-Legendre data for rule order 3, with basis functions up to degree data[""maxq""].\n        #                         # This under-integrates most of the higher-degree functions.\n        #   b = o[""y""][:3,:]  # basis functions 0, 1, 2 only - using the order 3 rule, for these the integration of the mass matrix is exact\n        #   x = o[""x""]  # the integration points (for information only, not actually needed for computing the integral)\n        #   w = o[""w""]  # the integration weights\n        #\n        # Recall that b[j,i] = N_j at x_i.\n        #\n        # For computing the matrices in Galerkin methods, it is much better to use the analytical results (see legtest3.py) than to use Gauss-Legendre numerically,\n        # but the b array is very useful for e.g. evaluating Galerkin series in this basis. Let c be a rank-1 np.array with Galerkin coefficients (in this example, of length 3).\n        # Then:\n        #\n        #   u = np.dot( b, c )\n        #\n        # gives the values of u at the integration points, where u is a function expressed as a linear combination of the basis functions.\n        #\n        precalc = Precalc(q, xx, cache)  # the cache likely doesn\'t help here, but it doesn\'t hurt to use it, either.\n        precalc.run()\n\n        # We save a copy of the Gauss-Legendre weights as ""w"".\n        data[""integ""][d] = { ""x"" : precalc.x, ""y"" : precalc.y, ""w"" : ww }\n\n#    # NOTE: the data is accessed by the order of the Gauss-Legendre rule, e.g.:\n#    print data[""integ""][2]\n\n    #########################################################\n    # Save results\n    #########################################################\n\n    # In the root process: save results to disk\n    #\n    if mpi_shim.get_rank() == 0:\n        # http://stackoverflow.com/questions/10075661/how-to-save-dictionaries-and-arrays-in-the-same-archive-with-numpy-savez\n        with open(\'pydgq_data.bin\', \'wb\') as outfile:\n            pickle.dump( data, outfile, protocol=pickle.HIGHEST_PROTOCOL )\n        print( ""Wrote pydgq_data.bin"" )\n\n# TODO: how to find the data file in an actual installation? (need to save it relative to the package directory)\n# TODO: change to .mat format to make the data file more self-documenting?\n\n\n############################################################################################################\n# Command line parser\n############################################################################################################\n\nif __name__ == \'__main__\':\n\n    # MPI support: only the root process parses command-line arguments\n    #\n    kwargs = None\n    comm = mpi_shim.get_comm_world()\n    if mpi_shim.get_rank() == 0:\n        # We wrap the parsing in try/finally and always broadcast *something*\n        # so that the non-root processes can silently exit if the parsing fails.\n        #\n        # http://stackoverflow.com/questions/25087360/parsing-arguments-using-argparse-and-mpi4py\n        #\n        try:\n            import argparse\n            parser = argparse.ArgumentParser(description=""""""Precalculate hierarchical (Lobatto) basis functions for Galerkin integrators.\n\nFor high degrees, the definition of the bubble functions exhibits numerical cancellation, and must thus be computed at increased precision before casting the result to target precision. This is done using arbitrary-precision floating point math, which relies on a pure software implementation and is thus very slow. This script performs the required precomputation and saves the result to disk.\n\nThis script supports MPI for parallelization."""""", formatter_class=argparse.RawDescriptionHelpFormatter)\n\n            parser.add_argument( \'-v\', \'--version\', action=\'version\', version=(\'%(prog)s \' + __version__) )\n\n            group_behavior = parser.add_argument_group(\'behavior\', \'Precalculator behavior options.\')\n\n            group_behavior.add_argument( \'-q\', \'--degree\',\n                                         dest=\'q\',\n                                         default=10,\n                                         type=int,\n                                         metavar=\'n\',\n                                         help=\'Sets the highest degree (the ""q"" in ""dG(q)"") to precompute. Must be >= 1. (Very high degrees, such as 50, are supported by this software, but usually dG(q) gives the best results for q=1 or q=2.) Default %(default)s.\' )\n\n            group_behavior.add_argument( \'-nx\', \'--points\',\n                                         dest=\'nx\',\n                                         default=101,\n                                         type=int,\n                                         metavar=\'n\',\n                                         help=\'For visualization use: sets how many evenly spaced points (on the reference element [-1,1]) each basis function will be evaluated at. Must be >= 1. Default %(default)s.\' )\n\n            # http://parezcoydigo.wordpress.com/2012/08/04/from-argparse-to-dictionary-in-python-2-7/\n            kwargs = vars( parser.parse_args() )\n\n        finally:\n            # Broadcast to all ranks (even if parsing failed, so that the other ranks won\'t hang)\n            #\n            if mpi_shim.get_size() > 1:\n                kwargs = comm.bcast(kwargs, root=0)\n\n    else:\n        # Other ranks: receive broadcast, and exit if empty\n        #\n        kwargs = comm.bcast(kwargs, root=0)\n\n        if kwargs is None:\n            exit(0)\n\n    main(**kwargs)\n\n'"
