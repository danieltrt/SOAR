file_path,api_count,code
Week 11/External_Libraries/optimizers_viz.py,21,"b'#@title Optim_viz Library\n############\n### Adaptado de wassname/viz_torch_optim para compatibilidad con Pytorch  1.1 y modularizaci\xc3\xb3n.\n############\n\nimport torch\nimport numpy as np\nfrom torch import optim\nfrom torch.optim import Optimizer\nfrom torch.optim import lr_scheduler\nimport numpy\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import LogNorm\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\nfrom collections import defaultdict\nfrom itertools import zip_longest\nfrom functools import partial\nfrom matplotlib import rcParams\nimport datetime\n\ntorch.set_default_tensor_type(\'torch.DoubleTensor\')\ndtype=np.float64\nrcParams[\'figure.figsize\']=(10,10)\n\n\nclass Problem(object):\n    def __init__(self, f, df, minima, x0, bounds=[[-5,5],[-5,5]], lr=1e-3, steps=3000, noise=dict(m=0,c=0)):\n        """"""\n        Problem setup\n        \n        Params:\n        - f: function [x1,x2] => z\n        - df: derivative function ([x1,x2]=>[dx1,dx2])\n        - minima: where the function has a minima\n        - self: bounds\n        \n        - x0: suggested start\n        - lr: suggested learning rate\n        - steps: suggested steps\n        """"""\n        def f_noise(t):\n            """"""Add some noise""""""\n            t = to_tensor(t)\n            c = torch.rand(t[0].size()) * noise[\'c\']\n            m = 1 + torch.rand(t[0].size()) * noise[\'m\']\n            z = f(t)\n            return  m * z + c \n        self.f = f_noise\n        self._f = f\n        self.df = df\n        self.x0 = x0\n        self.bounds = bounds\n        self.minima = minima\n        self.lr = lr\n        self.steps = steps\n        \n        self.xmin = bounds[0][0]\n        self.xmax = bounds[0][1]\n        self.ymin = bounds[1][0]\n        self.ymax = bounds[1][1]\n        \n        \n""""""Valley""""""\n\n\ndef beales(tensor):\n    """"""Beales function, like a valley""""""\n    x, y = tensor\n    x = to_tensor(x)\n    y = to_tensor(y)\n    # + noise(x,y)\n    return (1.5 - x + x * y)**2 + (2.25 - x + x * y**2)**2 + (2.625 - x + x * y**3)**2\n  \ndef dbeales(tensor):\n    x, y = tensor\n    x = to_tensor(x)\n    y = to_tensor(y)\n    dx = 2 * (x * y**3 - x + 2.625) * (y**3 - 1) + 2 * (x * y**2 -\n                                                        x + 2.25) * (y**2 - 1) + 2 * (x * y - x + 1.5) * (y - 1)\n    dy = 6 * (x * y**3 - x + 2.625) * x * y**2 + 4 * \\\n        (x * y**2 - x + 2.25) * x * y + 2 * (x * y - x + 1.5) * x\n    return torch.stack([dx, dy], 1)[0]\n  \ndef to_tensor(x):\n    # TODO: I\'m sure there\'s a proper way to do this\n    if isinstance(x, np.ndarray):\n        return torch.Tensor(x.astype(dtype))\n    if isinstance(x, list):\n        return torch.Tensor(x)\n    elif isinstance(x, (float, int, numpy.generic)):\n        return torch.Tensor([float(x)])\n    else:\n        return x\n\n\ndef test_f(f, df, constructor, steps=150, x0=[-4,-1], solution=[-2,0], scheduler=None, exact=False):\n    """"""\n    modified from https://github.com/pytorch/pytorch/blob/master/test/test_optim.py\n\n    params:\n    scheduler: e.g. scheduler = torch.optim.CyclicLR(optimizer)\n    \n    """"""\n    state = {}\n\n    # start\n    params = torch.tensor(x0, requires_grad=True)\n    optimizer = constructor([params])\n    initial_lr = optimizer.param_groups[0][\'lr\']\n    if scheduler:\n        _scheduler = scheduler(optimizer, initial_lr)\n\n    solution = torch.Tensor(solution)\n    initial_dist = params.data.dist(solution)\n\n    def eval():\n        optimizer.zero_grad()\n        loss = f(params)\n        loss.backward()\n        return loss\n\n    data=[]\n    dist=[]\n    lrs=[]\n    for i in range(steps):\n        \n        loss = optimizer.step(eval)\n        if scheduler:\n            _scheduler.batch_step()\n        \n        # record\n        dist.append(loss.squeeze().data.numpy()) # loss\n        data.append(params.data.numpy().copy())\n        lrs.append(optimizer.param_groups[0][\'lr\'])\n    return np.array(data), np.array(dist), lrs\n\n\nclass CyclicLR(object):\n    """"""\n    from https://github.com/thomasjpfan/pytorch/blob/401ec389db2c9d2978917a6e4d1101b20340d7e7/torch/optim/lr_scheduler.py\n    not merged into pytorch yet https://github.com/pytorch/pytorch/pull/2016\n\n    Sets the learning rate of each parameter group according to\n    cyclical learning rate policy (CLR). The policy cycles the learning\n    rate between two boundaries with a constant frequency, as detailed in\n    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n    The distance between the two boundaries can be scaled on a per-iteration\n    or per-cycle basis.\n    Cyclical learning rate policy changes the learning rate after every batch.\n    `batch_step` should be called after a batch has been used for training.\n    To resume training, save `last_batch_iteration` and use it to instantiate `CycleLR`.\n    This class has three built-in policies, as put forth in the paper:\n    ""triangular"":\n        A basic triangular cycle w/ no amplitude scaling.\n    ""triangular2"":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    ""exp_range"":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n        cycle iteration.\n    This implementation was adapted from the github repo: `bckenstler/CLR`_\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        base_lr (float or list): Initial learning rate which is the\n            lower boundary in the cycle for eachparam groups.\n            Default: 0.001\n        max_lr (float or list): Upper boundaries in the cycle for\n            each parameter group. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore\n            max_lr may not actually be reached depending on\n            scaling function. Default: 0.006\n        step_size (int): Number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch. Default: 2000\n        mode (str): One of {triangular, triangular2, exp_range}.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n            Default: \'triangular\'\n        gamma (float): Constant in \'exp_range\' scaling function:\n            gamma**(cycle iterations)\n            Default: 1.0\n        scale_fn (function): Custom scaling policy defined by a single\n            argument lambda function, where\n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored\n            Default: None\n        scale_mode (str): {\'cycle\', \'iterations\'}.\n            Defines whether scale_fn is evaluated on\n            cycle number or cycle iterations (training\n            iterations since start of cycle).\n            Default: \'cycle\'\n        last_batch_iteration (int): The index of the last batch. Default: -1\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = torch.optim.CyclicLR(optimizer)\n        >>> data_loader = torch.utils.data.DataLoader(...)\n        >>> for epoch in range(10):\n        >>>     for batch in data_loader:\n        >>>         scheduler.batch_step()\n        >>>         train_batch(...)\n    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n    """"""\n\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode=\'triangular\', gamma=1.,\n                 scale_fn=None, scale_mode=\'cycle\', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError(\'{} is not an Optimizer\'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(""expected {} base_lr, got {}"".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(""expected {} max_lr, got {}"".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in [\'triangular\', \'triangular2\', \'exp_range\'] \\\n                and scale_fn is None:\n            raise ValueError(\'mode is invalid and scale_fn is None\')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == \'triangular\':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'triangular2\':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'exp_range\':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = \'iterations\'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group[\'lr\'] = lr\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 / (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == \'cycle\':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs\n\ndef build_beales_problem():\n    return Problem(\n        f=beales,\n        df=dbeales,\n        minima=np.array([3., 0.5]),\n        bounds=[[-4.5,4.5],[-4.5,4.5]],\n        x0=[2,1.7],\n        steps=3400,\n        lr=3e-3,\n        noise=dict(m=0.13, c=7),\n    )\n\ndef build_optimizers(lr):   \n    return dict(\n        # Good high dimensional optimizers sometimes do poorly in low D spaces, so we will lower the LR on simple optimisers\n        # need smaller lr\'s sometimes\n        SGD= lambda params: optim.SGD(params, lr=lr/80),\n        momentum = lambda params: optim.SGD(params, lr=lr/80, momentum=0.9, nesterov=False, dampening=0),\n        momentum_dampen = lambda params: optim.SGD(params, lr=lr/80, momentum=0.9, nesterov=False, dampening=0.3),\n        nesterov = lambda params: optim.SGD(params, lr=lr/80, momentum=0.9, nesterov=True, dampening=0),\n        nesterov_decay = lambda params: optim.SGD(params, lr=lr/80, momentum=0.9, nesterov=True, weight_decay=1e-4, dampening=0),\n        \n        # need larger lr\'s sometimes\n        Adadelta = lambda params: optim.Adadelta(params),\n        Adagrad = lambda params: optim.Adagrad(params, lr=lr*20),\n        \n        # \n        Adamax = lambda params: optim.Adamax(params, lr=lr*20),\n        RMSprop = lambda params: optim.RMSprop(params, lr=lr*10),\n        Adam = lambda params: optim.Adam(params, lr=lr*10),\n    #     Adam_decay = lambda params:  optim.Adam(params, lr=lr*10, weight_decay=1e-9),\n        \n        # need to read about these, might not be comparable\n    #     ASGD = lambda params: optim.ASGD(params, lr=lr),\n    #     Rprop = lambda params: optim.Rprop(params, lr=lr),\n    #     LBFGS = lambda params: optim.LBFGS(params),\n    )\n\ndef build_params(problem):\n    xmin = problem.xmin\n    xmax = problem.xmax\n    ymin = problem.ymin\n    ymax = problem.ymax\n    ystep = xstep= (xmax-xmin)/200.0\n    zeps = 1.1e-0 # we don\'t want the minima to be actual zero or we wont get any lines shown on a log scale\n    z_min = problem.f(problem.minima).data.numpy().reshape(1)\n    minima_ = problem.minima.reshape(-1, 1)\n    _x0 = np.array([problem.x0]).T\n    # and x, y, z\n    x, y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))\n    z = problem.f([x, y]).data.numpy() \n    if z.min()<z_min[0]:\n      print(\'WARN: your minima is not the true minima\', z_min[0], z.min())\n      z_min[0]=z.min()  \n    z += -z_min[0] + zeps  # we shift everything up so the min is 1, so we can show on log scale\n    logzmax=np.log(z.max()-z.min()+zeps)            \n    return dict(\n        xmin=xmin,\n        xmax=xmax,\n        ymin=ymin,\n        ymax=ymax,\n        ystep=ystep,\n        zeps=zeps,\n        z_min=z_min,\n        minima_=minima_,\n        _x0=_x0,\n        x=x,\n        y=y,\n        z=z,\n        logzmax=logzmax  \n    )\n\n\n\ndef plot_minimized_function(params):\n    x = params[""x""]\n    y = params[""y""]\n    z = params[""z""]\n    logzmax = params[""logzmax""]\n    minima = params[""minima_""]\n    x0 = params[\'_x0\']\n    ax = plt.gca()\n    cm=ax.contour(x, y, z, levels=np.logspace(0, logzmax//2, 55), norm=LogNorm(), cmap=plt.cm.jet, alpha=0.15)\n    plt.colorbar(cm)\n    ax.plot(*minima, \'r*\', markersize=10)\n    ax.plot(*x0, \'r+\', markersize=10)\n    plt.title(\'debug: grid\')\n\n    plt.show()\n\n\n\n\n\ndef save_optim_animation():\n    ts = datetime.datetime.utcnow().strftime(\'%Y%m%d_%H-%M-%S\')\n\n    rcParams[\'figure.figsize\']=(10,10)\n    rcParams[\'figure.dpi\']=100\n    rcParams[\'animation.writer\']=\'ffmpeg\' # faster than fmmpeg\n    rcParams[\'savefig.dpi\']=180\n    rcParams[\'animation.codec\']=\'h264\'\n\n    # rcParams[\'savefig.bbox\'] = \'tight\'\n    seconds = 20\n    fps = 30 # too low and you will miss the fast moving ones\n    rcParams[\'animation.bitrate\']=-1 #fps*1000//25 # ~1mb/s\n    cuttoff=problem.steps//1 # if we want to crop the data to X steps, judging by the loss plot\n\n    decimation = int(np.round(cuttoff/(seconds*fps))) or 1 # don\'t need to plot every step\n    # assert cuttoff>seconds*fps\n    decimation, cuttoff,seconds*fps, problem.steps\n    if scheduler:\n        params = torch.tensor([0.0,1.0], requires_grad=True)\n        optimizer = optim.Adam([params])\n        scheduler_name = type(scheduler(optimizer, 0.1)).__name__\n    else:\n        scheduler_name=\'None\'\n\n    title=f\'function: f{problem._f.__name__}\'\n    if scheduler:\n        title += f\', scheduler: {scheduler_name}\'\n    save_file = course_path + \'videos/{name:}_{scheduler}_{ts:}\'.format(name=problem._f.__name__, ts=ts, scheduler=scheduler_name)\n\n    # from http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/\n    class TrajectoryAnimation(animation.FuncAnimation):\n        \n        def __init__(self, *paths, labels=[], fig=None, ax=None, frames=None, \n                     interval=60, repeat_delay=5, blit=True, **kwargs):\n\n            if fig is None:\n                if ax is None:\n                    fig, ax = plt.subplots()\n                else:\n                    fig = ax.get_figure()\n            else:\n                if ax is None:\n                    ax = fig.gca()\n\n            self.fig = fig\n            self.ax = ax\n            \n            self.paths = paths\n\n            if frames is None:\n                frames = max(path.shape[1] for path in paths)\n      \n            self.lines = [ax.plot([], [], label=label, lw=2)[0] \n                          for _, label in zip_longest(paths, labels)]\n            self.points = [ax.plot([], [], \'o\', color=line.get_color())[0] \n                           for line in self.lines]\n\n            super(TrajectoryAnimation, self).__init__(fig, self.animate, init_func=self.init_anim,\n                                                      frames=frames, interval=interval, blit=blit,\n                                                      repeat_delay=repeat_delay, **kwargs)\n\n        def init_anim(self):\n            for line, point in zip(self.lines, self.points):\n                line.set_data([], [])\n                point.set_data([], [])\n            return self.lines + self.points\n\n        def animate(self, i):\n            for line, point, path in zip(self.lines, self.points, self.paths):\n                line.set_data(*path[::,:i])\n                point.set_data(*path[::,i-1:i])\n            return self.lines + self.points\n\n    fig, ax = plt.subplots(figsize=(12, 8))\n    # fig.set_tight_layout(True)\n    ax.contour(x, y, z, levels=np.logspace(0, logzmax//2, 35), norm=LogNorm(), cmap=plt.cm.jet, alpha=0.5)\n    ax.plot(*minima_, \'r*\', markersize=10)\n    ax.plot(*problem.x0, \'r+\', markersize=10)\n\n    ax.set_title(\'{} (github.com/wassname/viz_torch_optim)\'.format(title))\n    ax.set_xlabel(\'$x$\')\n    ax.set_ylabel(\'$y$\')\n\n    ax.set_xlim((xmin, xmax))\n    ax.set_ylim((ymin, ymax))\n\n    anim = TrajectoryAnimation(*paths[:,:,:cuttoff:decimation], labels=methods, ax=ax, interval=1000//fps)\n\n    ax.legend(loc=\'upper left\')\n\n    # Query the figure\'s on-screen size and DPI. Note that when saving the figure to\n    # a file, we need to provide a DPI for that separately.\n    print(\'fig size: {0} DPI, size in inches {1}\'.format(\n        fig.get_dpi(), fig.get_size_inches()))\n\n    import matplotlib.animation\n    import logging\n    _log = logging.getLogger(matplotlib.animation.__file__)\n    _log.setLevel(logging.DEBUG)\n\n    anim.save(save_file+\'_2d\'+\'.mp4\', fps=fps, writer=""ffmpeg"", codec=\'h264\')\n    #anim.save(save_file+\'2d\'+\'.gif\', fps=fps, codec=\'gif\', writer=""imagemagick"")\n\ndef run_optimizers(problem, constructors, params):\n    lr = problem.lr\n    xmin = params[""xmin""]\n    xmax = params[""xmax""]\n    ymin = params[""ymin""]\n    ymax = params[""ymax""]\n    ystep = params[""ystep""]\n    zeps = params[""zeps""]\n    z_min = params[""z_min""]\n    minima_ = params[""minima_""]\n    _x0 = params[""_x0""]\n    x = params[""x""]\n    y = params[""y""]\n    z = params[""z""]\n    logzmax = params[""logzmax""]\n\n    results = {}\n    distance = {}\n    scheduler = lambda optimizer, initial_lr: CyclicLR(optimizer, base_lr=1e-5, max_lr=initial_lr, step_size=100, mode=\'exp_range\', gamma=0.9983)\n    for name, constructor in tqdm(constructors.items()):\n        data, dist, lrs = test_f(problem.f, problem.df, constructor, x0=problem.x0, steps=problem.steps, scheduler=scheduler)\n        results[name] = data\n        distance[name] = dist\n    methods = constructors.keys()\n    paths = np.array([path.T for path in results.values()]) # should be (2,N) each\n    zpaths = np.array([distance[name] - z_min[0] + zeps for name in methods])\n    \n    # Log z\'s\n    for i, name in enumerate(results):\n        zmax = zpaths[i][np.isfinite(zpaths[i])].max()\n        print(name, zmax, \'\\t\', np.isfinite(zmax).all(), \'\\t\', zmax.max()>z[:,0].max())     \n    # clip zpaths\n    zmax = z.max()\n    zpaths[np.isfinite(zpaths)==False]=zmax\n    zpaths = np.clip(zpaths, 0, zmax)\n    return zpaths, results, distance, lrs\n\ndef plot_optim_journeys(zpaths, results, distance, lrs, params):\n    xmin = params[""xmin""]\n    xmax = params[""xmax""]\n    ymin = params[""ymin""]\n    ymax = params[""ymax""]\n    ystep = params[""ystep""]\n    zeps = params[""zeps""]\n    z_min = params[""z_min""]\n    minima_ = params[""minima_""]\n    _x0 = params[""_x0""]\n    x = params[""x""]\n    y = params[""y""]\n    z = params[""z""]\n    logzmax = params[""logzmax""]\n    \n    # Preview plots\n    # static preview 2d to let you debug your steps and learning rate\n\n    # loss\n    for i, name in enumerate(results):\n        plt.plot(np.abs(distance[name]), label=name)\n    plt.legend()\n    plt.title(\'loss (mae)\')\n    plt.ylim(0,zpaths[:,0].mean()*1.3)\n    plt.show()\n\n\n    # Position\n    plt.figure(figsize=(12,12))\n    ax = plt.gca()\n    for name in results:\n        plt.scatter(*results[name].T, label=name, s=1)\n    plt.legend()\n    plt.xlim(xmin,xmax)\n    plt.ylim(ymin,ymax)\n\n    cm=ax.contour(x, y, z, levels=np.logspace(0, logzmax//2, 35), norm=LogNorm(), cmap=plt.cm.jet, alpha=0.15)\n    # plt.colorbar(cm)\n    ax.plot(*minima_, \'r*\', markersize=10)\n    ax.plot(*_x0, \'r+\', markersize=10)\n    plt.title(\'debug: paths\')\n\n    plt.show()\n\n    # lr\n    plt.plot(lrs)\n    # plt.yscale(\'log\')\n    plt.title(\'learning rate\')\n    plt.show()\n\ndef plot_3D_journey(zpaths, results, problem, params):\n    xmin = params[""xmin""]\n    xmax = params[""xmax""]\n    ymin = params[""ymin""]\n    ymax = params[""ymax""]\n    ystep = params[""ystep""]\n    zeps = params[""zeps""]\n    z_min = params[""z_min""]\n    minima_ = params[""minima_""]\n    _x0 = params[""_x0""]\n    x = params[""x""]\n    y = params[""y""]\n    z = params[""z""]\n    logzmax = params[""logzmax""]\n    # static preview 3d\n    fig = plt.figure(figsize=(8, 5))\n    ax = plt.axes(projection=\'3d\', elev=50,azim=-95)\n\n    ax.plot_surface(x, y, z, norm=LogNorm(), rstride=1, cstride=1, edgecolor=\'none\', alpha=.25, cmap=plt.cm.jet)\n    ax.plot(*minima_, problem.f(minima_).data.numpy(), \'r*\', markersize=10)\n    ax.plot(*_x0, problem.f(_x0).data.numpy(), \'r+\', markersize=10)\n\n    ax.set_xlabel(\'$x$\')\n    ax.set_ylabel(\'$y$\')\n    ax.set_zlabel(\'$z$\')\n\n    ax.set_xlim((xmin, xmax))\n    ax.set_ylim((ymin, ymax))\n\n    # anim = TrajectoryAnimation3D(*paths, zpaths=zpaths, labels=methods, ax=ax)\n    # quick plot to let you debug your steps and learning rate\n    ax = plt.gca()\n    for i, name in enumerate(results):\n        ax.scatter3D(*results[name].T, zpaths[i], label=name, s=1)\n    plt.legend()\n    plt.xlim(xmin,xmax)\n    plt.ylim(ymin,ymax)\n\n    ax.legend(loc=\'upper right\')'"
Week 3/02 - Modules main/Modules.py,0,"b'\ndef welcome():\n    print(""this is an intro for modules"")\n\n\ndef info_about(module):\n    print(dir(module))\n\n\ndef help_about(module):\n    help(module)\n    \n'"
Week 3/02 - Modules main/mod_main.py,0,"b""# Main Example\nfrom time import sleep\n\n\ndef wait():\n    sleep(4)\n\n\nif __name__ == '__main__':\n    print('Run Directly')\n    wait()\n    print('Initial Config done')\nelse:\n    print('Run from import')\n    wait()\n    print('Import Config done')\n\n\nprint(f'{__name__} Loaded')\n\n'''\ndef main():\n    print('Run Directly')\n    \n    \nif __name__ == '__main__':\n    main()\n'''\n"""
Week 3/02 - Modules main/mod_main2.py,0,"b""# Main Example 2\n\ndef main():\n    print('Run Directly')\n\n \nif __name__ == '__main__':\n    main()"""
Week 3/02 - Modules main/randomize.py,0,"b'import random as rn\n\n\ndef rand():\n    return rn.random()\n\n\ndef rand10():\n    return rn.randint(0,10)\n\n\ndef rand100():\n    return rn.randrange(0,100)\n\n\ndef choice(data):\n    return rn.choice(data)'"
Week 3/02 - Modules main/mate/__init__.py,0,"b""print(f'{__name__} loaded')\n\n__all__ = [\n    'graph'\n]"""
Week 3/02 - Modules main/mate/graph.py,11,"b'## GRAPHS\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy import ma\nfrom matplotlib import ticker, cm\n\n\n\ndef graph(array, **kwargs):\n    plt.plot(array)\n    print(kwargs)\n    if kwargs.keys():\n        try:\n            plt.xlabel(kwargs[\'x\'])\n            plt.ylabel(kwargs[\'y\'])\n        except:\n            print()\n    plt.show()\n    \n\n    \n\n    \ndef koch_snowflake(order, scale=10):\n    """"""\n    Return two lists x, y of point coordinates of the Koch snowflake.\n\n    Arguments\n    ---------\n    order : int\n        The recursion depth.\n    scale : float\n        The extent of the snowflake (edge length of the base triangle).\n    """"""\n    def _koch_snowflake_complex(order):\n        if order == 0:\n            # initial triangle\n            angles = np.array([0, 120, 240]) + 90\n            return scale / np.sqrt(3) * np.exp(np.deg2rad(angles) * 1j)\n        else:\n            ZR = 0.5 - 0.5j * np.sqrt(3) / 3\n\n            p1 = _koch_snowflake_complex(order - 1)  # start points\n            p2 = np.roll(p1, shift=-1)  # end points\n            dp = p2 - p1  # connection vectors\n\n            new_points = np.empty(len(p1) * 4, dtype=np.complex128)\n            new_points[::4] = p1\n            new_points[1::4] = p1 + dp / 3\n            new_points[2::4] = p1 + dp * ZR\n            new_points[3::4] = p1 + dp / 3 * 2\n            return new_points\n\n    points = _koch_snowflake_complex(order)\n    x, y = points.real, points.imag\n    return x, y \n\n    \n    \ndef graph_snowflake(order,size):\n    x, y = koch_snowflake(order)\n    plt.figure(figsize=size)\n    plt.axis(\'equal\')\n    plt.fill(x, y)\n    plt.show()\n    \n    \n    \n    \n    \n    \n    \ndef warm_map(N,x,y,colormap):\n    \n    X, Y = np.meshgrid(x, y)\n\n    # A low hump with a spike coming out.\n    # Needs to have z/colour axis on a log scale so we see both hump and spike.\n    # linear scale only shows the spike.\n    Z1 = np.exp(-(X)**2 - (Y)**2)\n    Z2 = np.exp(-(X * 10)**2 - (Y * 10)**2)\n    z = Z1 + 50 * Z2\n\n    # Put in some negative values (lower left corner) to cause trouble with logs:\n    z[:5, :5] = -1\n\n    # The following is not strictly essential, but it will eliminate\n    # a warning.  Comment it out to see the warning.\n    z = ma.masked_where(z <= 0, z)\n\n\n    # Automatic selection of levels works; setting the\n    # log locator tells contourf to use a log scale:\n    fig, ax = plt.subplots()\n    cs = ax.contourf(X, Y, z, locator=ticker.LogLocator(), cmap=colormap)\n\n    # Alternatively, you can manually set the levels\n    # and the norm:\n    # lev_exp = np.arange(np.floor(np.log10(z.min())-1),\n    #                    np.ceil(np.log10(z.max())+1))\n    # levs = np.power(10, lev_exp)\n    # cs = ax.contourf(X, Y, z, levs, norm=colors.LogNorm())\n\n    cbar = fig.colorbar(cs)\n\n    plt.show()\n\n'"
Week 3/02 - Modules main/mate/op.py,0,b'## OPERATIONS\n\n\ndef prom(array):\n    result = 0\n    for i in array:\n        result += i\n        \n    return result / len(array)\n\n\ndef sumatoria(array):\n    result = 0\n    for i in array:\n        result += i\n    \n    return result\n\n\ndef potencias_cuadrada(array):\n    result = []\n    for i in array:\n        result.append(i**2)\n    \n    return result'
Week 3/02 - Modules main/personal_module/__init__.py,0,"b""print(f'{__name__} Loaded')\n\n\n__all__ = [\n    'greetings'\n]"""
Week 3/02 - Modules main/personal_module/greetings.py,0,"b'\ndef eng():\n    return ""Hi"";\n\n\ndef ger():\n    return \'Hallo\'\n\n\ndef kor():\n    return \'\xec\x95\x88\xeb\x85\x95\xed\x95\x98\xec\x84\xb8\xec\x9a\x94\'\n\n\ndef esp():\n    return \'Hola\'\n\n\ndef fre():\n    return \'Salut\'\n\n'"
Week 3/02 - Modules main/mate/op2/op2.py,0,b'\ndef prom(array):\n    result = 0\n    for i in array:\n        result += i\n        print(result)\n        \n    return result / len(array)'
Week 3/02 - Modules main/personal_m2/sub1/mod1.py,0,"b""\ndef foo():\n    print('Hello from subpkg1')"""
Week 3/02 - Modules main/personal_m2/sub2/mod2.py,0,"b""def foo():\n    print('Hello from subpkg2')"""
