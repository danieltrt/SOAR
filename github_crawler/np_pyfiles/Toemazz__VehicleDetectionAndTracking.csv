file_path,api_count,code
OneCameraFront.py,0,"b""import cv2\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom VehicleDetectionAndTrackingProject import VehicleDetectionAndTrackingProject\n\n# Set up video capture\nvideo = cv2.VideoCapture('videos/front_right_2.mp4')\n\n# Get information about the videos\nn_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = int(video.get(cv2.CAP_PROP_FPS))\n\n# Create instances for vehicle detection\nvdt = VehicleDetectionAndTrackingProject(front=True, left=False)\n\nfor n in tqdm(range(n_frames)):\n    # Grab the frames from their respective video streams\n    ok, frame = video.read()\n\n    if ok:\n        # Process images\n        out = vdt.pipeline(frame)\n\n        # Get warnings\n        warning = vdt.warning\n\n        # Add 'SAFE' to image when no warnings were issued\n        if not warning:\n            dims = out.shape[:2]\n            cv2.putText(out, 'SAFE', (int(dims[1]/2)-80, 50), cv2.FONT_HERSHEY_DUPLEX, 2.0, (0, 255, 0), 2, cv2.LINE_AA)\n\n        # Save frame\n        cv2.imwrite('output/front_right_2_out/frame{}.png'.format(n+1), out)\n"""
OneCameraRear.py,0,"b""import cv2\nfrom tqdm import tqdm\nfrom VehicleDetectionAndTrackingProject import VehicleDetectionAndTrackingProject\n\n# Set up video capture\nvideo = cv2.VideoCapture('videos/rear_right_2.mp4')\n\n# Get information about the videos\nn_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = int(video.get(cv2.CAP_PROP_FPS))\n\n# Create instances for vehicle detection\nvdt = VehicleDetectionAndTrackingProject(front=False)\n\nfor _ in tqdm(range(n_frames)):\n    # Grab the frames from their respective video streams\n    ok, frame_in = video.read()\n\n    if ok:\n        # Process images\n        frame_out = vdt.pipeline(frame_in)\n\n        # Get warning\n        frame_warning = vdt.warning\n\n        # Add 'SAFE' to image when no warnings were issued\n        if not frame_warning:\n            dims = frame_out.shape[:2]\n            cv2.putText(frame_out, 'SAFE', (int(dims[1]/2)-80, 50), cv2.FONT_HERSHEY_DUPLEX, 2.0, (0, 255, 0), 2,\n                        cv2.LINE_AA)\n\n        # Save frame\n        cv2.imwrite('output/rear_right_2_out_test/frame{}.png'.format(vdt.count), frame_out)\n"""
TwoCamerasFront.py,1,"b""import cv2\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom VehicleDetectionAndTrackingProject import VehicleDetectionAndTrackingProject\n\n# Set up video capture\nleft_video = cv2.VideoCapture('videos/front_left_1.mp4')\nright_video = cv2.VideoCapture('videos/front_right_1.mp4')\n\n# Get information about the videos\nn_frames = min(int(left_video.get(cv2.CAP_PROP_FRAME_COUNT)), int(right_video.get(cv2.CAP_PROP_FRAME_COUNT)))\nfps = int(left_video.get(cv2.CAP_PROP_FPS))\n\n# Create instances for vehicle detection\nleft_vdt = VehicleDetectionAndTrackingProject(left=True)\nright_vdt = VehicleDetectionAndTrackingProject(left=False)\n\nfor n in tqdm(range(n_frames)):\n    # Grab the frames from their respective video streams\n    ok, left_in = left_video.read()\n    _, right_in = right_video.read()\n\n    if ok:\n        # Process images\n        left_out = left_vdt.pipeline(left_in)\n        right_out = right_vdt.pipeline(right_in)\n\n        # Get warnings\n        left_warning = left_vdt.warning\n        right_warning = right_vdt.warning\n\n        # Horizontally concatenate images and resize\n        out = np.hstack([left_out, right_out])\n        out = cv2.resize(out, (0, 0), fx=0.5, fy=0.5)\n\n        # Add 'SAFE' to image when no warnings were issued\n        if (not left_warning) and (not right_warning):\n            dims = out.shape[:2]\n            cv2.putText(out, 'SAFE', (int(dims[1]/2)-40, 25), cv2.FONT_HERSHEY_DUPLEX, 1.0, (0, 255, 0), 2, cv2.LINE_AA)\n\n        # Save frame\n        cv2.imwrite('output/front_both_1_out/frame{}.png'.format(n+1), out)\n"""
TwoVideoTrackingTest.py,2,"b""import cv2\nimport numpy as np\nfrom random import randint\n\n\ndef add_random_points_to_image(dimensions, n_points, colour):\n    copy = np.zeros(dimensions, dtype=np.uint8)\n    copy[:] = 255\n    points = []\n\n    for i in range(n_points):\n        rand_x = randint(0, dimensions[1])\n        rand_y = randint(0, dimensions[0])\n\n        cv2.circle(copy, (rand_x, rand_y), 4, colour, -1)\n        points.append((rand_y, rand_x))\n\n    return copy, points\n\n\n# Initialize parameters\ndims = (270, 480, 3)\nx_distance = 50\np_distance = 5\nnum_points = 100\n\n# Add random points to image\nleft, l_pts = add_random_points_to_image(dims, num_points, (255, 0, 0))\nright, r_pts = add_random_points_to_image(dims, num_points, (0, 255, 0))\nprint(l_pts)\n# Find points to be removed\nl_pts_del, r_pts_del = [], []\nfor i in range(num_points):\n    if l_pts[i][1] >= dims[1]-x_distance:\n        l_pts_del.append(l_pts[i])\n\n    if r_pts[i][1] <= x_distance:\n        r_pts_del.append(r_pts[i])\n\n# Create output image\noutput = np.hstack((left, right))\n\n# Find points with similar y-coordinates\nfor l_pt in l_pts_del:\n    for r_pt in r_pts_del:\n        if abs(l_pt[0]-r_pt[0]) <= p_distance:\n            cv2.line(output, (l_pt[1], l_pt[0]), (r_pt[1]+dims[1], r_pt[0]), (0, 0, 0), 2)\n            print('MATCHED {} and {}'.format(l_pt, r_pt))\n\n# Draw horizontal lines\ncv2.line(output, (dims[1]-x_distance, 0), (dims[1]-x_distance, 2*dims[0]), (0, 0, 0), 2)\ncv2.line(output, (dims[1]+x_distance, 0), (dims[1]+x_distance, 2*dims[0]), (0, 0, 0), 2)\n\n# Display output\ncv2.imshow('Output', output)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n"""
VehicleDetectionAndTracking.py,9,"b""import sys\nimport numpy as np\nfrom collections import deque\nfrom moviepy.editor import VideoFileClip\nfrom sklearn.utils.linear_assignment_ import linear_assignment\n\nfrom utilities.VehicleDetector import VehicleDetector\nfrom utilities.VehicleTracker import VehicleTracker\nfrom utilities.BoundingBox import *\n\n\nclass VehicleDetectionAndTracking:\n    def __init__(self, min_conf=0.6, max_age=2, max_hits=8):\n        # Initialize constants\n        self.max_age = max_age                   # no. of consecutive unmatched detection before a track is deleted\n        self.min_hits = max_hits                 # no. of consecutive matches needed to establish a track\n        self.tracker_list = []\n        self.track_id_list = deque(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'])\n        self.count = 0\n\n        # Set up 'Vehicle Detector'\n        self.detector = VehicleDetector(kitti=False, min_conf=min_conf)\n\n    # Method: Used to match detections to trackers\n    @staticmethod\n    def match_detections_to_trackers(trackers, detections, min_iou=0.25):\n        # Initialize 'iou_matrix'\n        iou_matrix = np.zeros((len(trackers), len(detections)), dtype=np.float32)\n\n        # Populate 'iou_matrix'\n        for t, tracker in enumerate(trackers):\n            for d, detection in enumerate(detections):\n                iou_matrix[t, d] = box_iou_ratio(tracker, detection)\n\n        # Produce matches by using the Hungarian algorithm to maximize the sum of IOU\n        matched_index = linear_assignment(-iou_matrix)\n\n        # Populate 'unmatched_trackers'\n        unmatched_trackers = []\n        for t in np.arange(len(trackers)):\n            if t not in matched_index[:, 0]:\n                unmatched_trackers.append(t)\n\n        # Populate 'unmatched_detections'\n        unmatched_detections = []\n        for d in np.arange(len(detections)):\n            if d not in matched_index[:, 1]:\n                unmatched_detections.append(d)\n\n        # Populate 'matches'\n        matches = []\n        for m in matched_index:\n            # Create tracker if IOU is greater than 'min_iou'\n            if iou_matrix[m[0], m[1]] > min_iou:\n                matches.append(m.reshape(1, 2))\n            else:\n                unmatched_trackers.append(m[0])\n                unmatched_detections.append(m[1])\n\n        if matches:\n            # Concatenate arrays on the same axis\n            matches = np.concatenate(matches, axis=0)\n        else:\n            matches = np.empty((0, 2), dtype=int)\n\n        # Return matches, unmatched detection and unmatched trackers\n        return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n\n    # Method: Used as a 'pipeline' function for detection and tracking\n    def pipeline(self, image):\n        # Get bounding boxes for located vehicles\n        det_boxes = self.detector.get_bounding_box_locations(image)\n\n        # Get list of tracker bounding boxes\n        trk_boxes = []\n        if self.tracker_list:\n            for tracker in self.tracker_list:\n                trk_boxes.append(tracker.box)\n\n        # Match detected vehicles to trackers\n        matched, unmatched_dets, unmatched_trks = self.match_detections_to_trackers(trk_boxes, det_boxes)\n\n        # Deal with matched detections\n        if len(matched) > 0:\n            for trk_idx, det_idx in matched:\n                z = det_boxes[det_idx]\n                z = np.expand_dims(z, axis=0).T\n                temp_trk = self.tracker_list[trk_idx]\n                temp_trk.predict_and_update(z)\n                xx = temp_trk.x_state.T[0].tolist()\n                xx = [xx[0], xx[2], xx[4], xx[6]]\n                trk_boxes[trk_idx] = xx\n                temp_trk.box = xx\n                temp_trk.num_hits += 1\n\n        # Deal with unmatched detections\n        if len(unmatched_dets) > 0:\n            for i in unmatched_dets:\n                z = det_boxes[i]\n                z = np.expand_dims(z, axis=0).T\n                temp_trk = VehicleTracker()  # Create a new tracker\n                x = np.array([[z[0], 0, z[1], 0, z[2], 0, z[3], 0]]).T\n                temp_trk.x_state = x\n                temp_trk.predict()\n                xx = temp_trk.x_state\n                xx = xx.T[0].tolist()\n                xx = [xx[0], xx[2], xx[4], xx[6]]\n                temp_trk.box = xx\n                temp_trk.id = self.track_id_list.popleft()  # assign an ID for the tracker\n                self.tracker_list.append(temp_trk)\n                trk_boxes.append(xx)\n\n        # Deal with unmatched tracks\n        if len(unmatched_trks) > 0:\n            for i in unmatched_trks:\n                temp_trk = self.tracker_list[i]\n                temp_trk.num_unmatched += 1\n                temp_trk.predict()\n                xx = temp_trk.x_state\n                xx = xx.T[0].tolist()\n                xx = [xx[0], xx[2], xx[4], xx[6]]\n                temp_trk.box = xx\n                trk_boxes[i] = xx\n\n        # Populate the list of trackers to be displayed on the image\n        good_tracker_list = []\n\n        for tracker in self.tracker_list:\n            if tracker.num_hits >= self.min_hits and tracker.num_unmatched <= self.max_age:\n                good_tracker_list.append(tracker)\n                tracker_bb = tracker.box\n\n                # Draw bounding box on the image\n                image = draw_box_label(image, tracker_bb)\n\n                self.count += 1\n\n        # Find list of trackers to be deleted\n        deleted_trackers = filter(lambda x: x.num_unmatched > self.max_age, self.tracker_list)\n\n        for tracker in deleted_trackers:\n            self.track_id_list.append(tracker.id)\n\n        # Update list of active trackers\n        self.tracker_list = [x for x in self.tracker_list if x.num_unmatched <= self.max_age]\n\n        return image\n\n    # Method: Used to end VideoFileClip processes\n    @staticmethod\n    def close_clip(clip):\n        try:\n            clip.reader.close()\n            del clip.reader\n\n            if clip.audio is not None:\n                clip.audio.reader.close_proc()\n                del clip.audio\n\n            del clip\n        except Exception:\n            sys.exc_clear()\n\n\nvdt = VehicleDetectionAndTracking(min_conf=0.8, max_age=2, max_hits=8)\noutput = 'video1_short_out_80.mp4'\ninput_vid = VideoFileClip('videos/video1_short.mp4')\noutput_vid = input_vid.fl_image(vdt.pipeline)\noutput_vid.write_videofile(output, threads=4, audio=False)\nvdt.close_clip(output_vid)\nprint(vdt.count)\n\n"""
VehicleDetectionAndTrackingProject.py,11,"b""import sys\nimport numpy as np\nfrom collections import deque\nfrom sklearn.utils.linear_assignment_ import linear_assignment\n\nfrom utilities.VehicleDetector import VehicleDetector\nfrom utilities.VehicleTracker import VehicleTracker\nfrom utilities.BoundingBox import *\n\n\nclass VehicleDetectionAndTrackingProject:\n    def __init__(self, min_conf=0.6, max_age=4, max_hits=10, front=True, left=False):\n        # Initialize constants\n        self.max_age = max_age                   # no. of consecutive unmatched detection before a track is deleted\n        self.min_hits = max_hits                 # no. of consecutive matches needed to establish a track\n        self.tracker_list = []\n        self.track_id_list = deque(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'])\n        self.left = left\n        self.front = front\n        self.vehicle_detected = False\n        self.count = 0\n\n        # Set up 'Vehicle Detector'\n        self.detector = VehicleDetector(kitti=False, min_conf=min_conf)\n\n    # Method: Used to match detections to trackers\n    @staticmethod\n    def match_detections_to_trackers(trackers, detections, min_iou=0.25):\n        # Initialize 'iou_matrix'\n        iou_matrix = np.zeros((len(trackers), len(detections)), dtype=np.float32)\n\n        # Populate 'iou_matrix'\n        for t, tracker in enumerate(trackers):\n            for d, detection in enumerate(detections):\n                iou_matrix[t, d] = box_iou_ratio(tracker, detection)\n\n        # Produce matches by using the Hungarian algorithm to maximize the sum of IOU\n        matched_index = linear_assignment(-iou_matrix)\n\n        # Populate 'unmatched_trackers'\n        unmatched_trackers = []\n        for t in np.arange(len(trackers)):\n            if t not in matched_index[:, 0]:\n                unmatched_trackers.append(t)\n\n        # Populate 'unmatched_detections'\n        unmatched_detections = []\n        for d in np.arange(len(detections)):\n            if d not in matched_index[:, 1]:\n                unmatched_detections.append(d)\n\n        # Populate 'matches'\n        matches = []\n        for m in matched_index:\n            # Create tracker if IOU is greater than 'min_iou'\n            if iou_matrix[m[0], m[1]] > min_iou:\n                matches.append(m.reshape(1, 2))\n            else:\n                unmatched_trackers.append(m[0])\n                unmatched_detections.append(m[1])\n\n        if matches:\n            # Concatenate arrays on the same axis\n            matches = np.concatenate(matches, axis=0)\n        else:\n            matches = np.empty((0, 2), dtype=int)\n\n        # Return matches, unmatched detection and unmatched trackers\n        return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n\n    # Method: Used as a 'pipeline' function for detection and tracking\n    def pipeline(self, image):\n        dims = image.shape[:2]\n        self.count += 1\n\n        # Get bounding boxes for located vehicles\n        det_boxes = self.detector.get_bounding_box_locations(image)\n\n        # Get list of tracker bounding boxes\n        trk_boxes = []\n        if self.tracker_list:\n            for tracker in self.tracker_list:\n                trk_boxes.append(tracker.box)\n\n        # Match detected vehicles to trackers\n        matched, unmatched_dets, unmatched_trks = self.match_detections_to_trackers(trk_boxes, det_boxes)\n\n        # Deal with matched detections\n        if len(matched) > 0:\n            for trk_idx, det_idx in matched:\n                z = det_boxes[det_idx]\n                z = np.expand_dims(z, axis=0).T\n                temp_trk = self.tracker_list[trk_idx]\n                temp_trk.predict_and_update(z)\n                xx = temp_trk.x_state.T[0].tolist()\n                xx = [xx[0], xx[2], xx[4], xx[6]]\n                trk_boxes[trk_idx] = xx\n                temp_trk.box = xx\n                temp_trk.num_hits += 1\n\n        # Deal with unmatched detections\n        if len(unmatched_dets) > 0:\n            for i in unmatched_dets:\n                z = det_boxes[i]\n                z = np.expand_dims(z, axis=0).T\n                temp_trk = VehicleTracker()  # Create a new tracker\n                x = np.array([[z[0], 0, z[1], 0, z[2], 0, z[3], 0]]).T\n                temp_trk.x_state = x\n                temp_trk.predict()\n                xx = temp_trk.x_state\n                xx = xx.T[0].tolist()\n                xx = [xx[0], xx[2], xx[4], xx[6]]\n                temp_trk.box = xx\n                temp_trk.id = self.track_id_list.popleft()  # assign an ID for the tracker\n                self.tracker_list.append(temp_trk)\n                trk_boxes.append(xx)\n\n        # Deal with unmatched tracks\n        if len(unmatched_trks) > 0:\n            for i in unmatched_trks:\n                temp_trk = self.tracker_list[i]\n                temp_trk.num_unmatched += 1\n                temp_trk.predict()\n                xx = temp_trk.x_state\n                xx = xx.T[0].tolist()\n                xx = [xx[0], xx[2], xx[4], xx[6]]\n                temp_trk.box = xx\n                trk_boxes[i] = xx\n\n        # Populate the list of trackers to be displayed on the image\n        good_tracker_list = []\n        warning_count = 0\n        area_count = 0\n\n        for tracker in self.tracker_list:\n            if tracker.num_hits >= self.min_hits and tracker.num_unmatched <= self.max_age:\n                good_tracker_list.append(tracker)\n                tracker_bb = tracker.box\n\n                # Draw bounding box on the image\n                image = draw_box_label(image, tracker_bb)\n\n                if self.front:\n                    center = (int(np.average([tracker_bb[0], tracker_bb[2]])),\n                              int(np.average([tracker_bb[1], tracker_bb[3]])))\n\n                    if self.left:\n                        if center[1] <= dims[1] // 2:\n                            cv2.putText(image, 'WARNING', (20, 50), cv2.FONT_HERSHEY_DUPLEX, 2.0, (0, 0, 255), 2,\n                                        cv2.LINE_AA)\n                            warning_count += 1\n                    else:\n                        if center[1] >= dims[1] // 2:\n                            cv2.putText(image, 'WARNING', (dims[1]-300, 50), cv2.FONT_HERSHEY_DUPLEX, 2.0, (0, 0, 255), 2,\n                                        cv2.LINE_AA)\n                            warning_count += 1\n                else:\n                    h = tracker_bb[2] - tracker_bb[0]\n                    w = tracker_bb[3] - tracker_bb[1]\n                    bb_area = h * w\n                    bb_area_percent = 100 * (bb_area/(dims[0]*dims[1]))\n\n                    if bb_area_percent >= 2:\n                        cv2.putText(image, 'WARNING', (int(dims[1]/2)-120, 50), cv2.FONT_HERSHEY_DUPLEX, 2.0,\n                                    (0, 0, 255), 2, cv2.LINE_AA)\n                        area_count += 1\n\n        # Find list of trackers to be deleted\n        deleted_trackers = filter(lambda x: x.num_unmatched > self.max_age, self.tracker_list)\n\n        for tracker in deleted_trackers:\n            self.track_id_list.append(tracker.id)\n\n        # Update list of active trackers\n        self.tracker_list = [x for x in self.tracker_list if x.num_unmatched <= self.max_age]\n\n        # True if vehicle was detected in a 'danger zone'\n        if self.front:\n            self.warning = True if warning_count > 0 else False\n        else:\n            self.warning = True if area_count > 0 else False\n\n        return image\n\n    # Method: Used to end VideoFileClip processes\n    @staticmethod\n    def close_clip(clip):\n        try:\n            clip.reader.close()\n            del clip.reader\n\n            if clip.audio is not None:\n                clip.audio.reader.close_proc()\n                del clip.audio\n\n            del clip\n        except Exception:\n            sys.exc_clear()\n"""
utilities/BoundingBox.py,0,"b'import cv2\n\n\n# Method: Used to calculate the ratio between intersection and union of 2 bounding boxes\ndef box_iou_ratio(a, b):\n    """"""\n    :param a: Box A\n    :param b: Box B\n    :return: Ratio = AnB / AuB\n    """"""\n    w_intersection = max(0, (min(a[2], b[2]) - max(a[0], b[0])))\n    h_intersection = max(0, (min(a[3], b[3]) - max(a[1], b[1])))\n    s_intersection = w_intersection * h_intersection\n\n    s_a = (a[2] - a[0]) * (a[3] - a[1])\n    s_b = (b[2] - b[0]) * (b[3] - b[1])\n\n    return float(s_intersection) / (s_a + s_b - s_intersection)\n\n\n# Method: Used to draw the bounding boxes and labels on an image\ndef draw_box_label(image, box, colour=(0, 255, 255)):\n    """"""\n    :param image: Image\n    :param box: Bounding Box\n    :param colour: Colour for the Bounding Box\n    :return: Image with bounding box (and label)\n    """"""\n    left, top, right, bottom = box[1], box[0], box[3], box[2]\n\n    # Draw the bounding box\n    cv2.rectangle(image, (left, top), (right, bottom), colour, 4)\n\n    return image\n\n'"
utilities/LabelMap.py,0,"b'import logging\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom object_detection.protos import string_int_label_map_pb2\n\n\n# Method: Used to checks if a label map is valid\ndef validate_label_map(label_map):\n    """"""\n    :param label_map: StringIntLabelMap to validate\n    :return: ValueError: if label map is invalid\n    """"""\n    for item in label_map.item:\n        if item.id < 1:\n            raise ValueError(\'Label map ids should be >= 1.\')\n\n\n# Method: Creates a dictionary of COCO compatible categories keyed by category id\ndef create_category_index(categories):\n    """"""\n    :param categories: A list of dicts, each of which has the following keys: \'id\', \'name\'\n    :return: A dict containing the same entries as categories, but keyed by the \'id\' field of each category\n    """"""\n    category_index = {}\n\n    for cat in categories:\n        category_index[cat[\'id\']] = cat\n\n    return category_index\n\n\n# Method: Loads label map proto and returns categories list compatible with eval\ndef convert_label_map_to_categories(label_map, max_num_classes, use_display_name=True):\n    """"""\n    :param label_map: A StringIntLabelMapProto or None.  If None, a default categories list is created with\n        max_num_classes categories.\n    :param max_num_classes: Maximum number of (consecutive) label indices to include\n    :param use_display_name: Choose whether to load \'display_name\' field as category name\n    :return: A list of dictionaries representing all possible categories\n    """"""\n    categories, list_of_ids_already_added = [], []\n\n    if not label_map:\n        label_id_offset = 1\n\n        for class_id in range(max_num_classes):\n            categories.append({\'id\': class_id + label_id_offset,\n                               \'name\': \'category_{}\'.format(class_id + label_id_offset)})\n        return categories\n\n    for item in label_map.item:\n        if not 0 < item.id <= max_num_classes:\n            logging.info(\'Ignore item %d since it falls outside of requested label range.\', item.id)\n            continue\n\n        if use_display_name and item.HasField(\'display_name\'):\n            name = item.display_name\n        else:\n            name = item.name\n\n        if item.id not in list_of_ids_already_added:\n            list_of_ids_already_added.append(item.id)\n            categories.append({\'id\': item.id, \'name\': name})\n\n    return categories\n\n\n# Method: Used to load label map proto\ndef load_label_map(path):\n    """"""\n    :param path: Path to StringIntLabelMap proto text file\n    :return: A StringIntLabelMapProto\n    """"""\n    with tf.gfile.GFile(path, \'r\') as fid:\n        label_map_string = fid.read()\n        label_map = string_int_label_map_pb2.StringIntLabelMap()\n\n        try:\n            text_format.Merge(label_map_string, label_map)\n        except text_format.ParseError:\n            label_map.ParseFromString(label_map_string)\n\n    validate_label_map(label_map)\n\n    return label_map\n\n\n# Method: Used to read a label map and returns a dictionary of label names to id\ndef get_label_map_dict(label_map_path, use_display_name=False):\n    """"""\n    :param label_map_path: Path to label_map\n    :param use_display_name: Whether to use the label map items\' display names as keys\n    :return: A dictionary mapping label names to id\n    """"""\n    label_map = load_label_map(label_map_path)\n    label_map_dict = {}\n\n    for item in label_map.item:\n        if use_display_name:\n            label_map_dict[item.display_name] = item.id\n        else:\n            label_map_dict[item.name] = item.id\n\n    return label_map_dict\n\n\n# Method: Used to read a label map and returns a category index\ndef create_category_index_from_label_map(label_map_path):\n    """"""\n    :param label_map_path: Path to `StringIntLabelMap` proto text file\n    :return: A category index, which is a dictionary that maps integer ids to dicts containing categories\n    """"""\n    label_map = load_label_map(label_map_path)\n    max_num_classes = max(item.id for item in label_map.item)\n    categories = convert_label_map_to_categories(label_map, max_num_classes)\n\n    return create_category_index(categories)\n'"
utilities/VehicleDetector.py,6,"b'import os\nimport numpy as np\nimport tensorflow as tf\n\nfrom utilities import LabelMap\n\n\nclass VehicleDetector:\n    # Method: Constructor\n    def __init__(self, kitti=False, min_conf=0.7, info=False):\n        """"""\n        :param kitti: If True, use the Kitti model\n        :param min_conf: Minimum acceptable confidence level\n        :param info: If True, display all visualisations\n        """"""\n        # Change to current working directory\n        os.chdir(os.getcwd())\n\n        self.bounding_boxes = []\n        self.min_conf = min_conf\n        self.info = info\n        self.kitti = kitti\n\n        path_to_model = \'data/frozen_model.pb\'\n        if self.kitti:\n            path_to_label_map = \'data/kitti_label_map.pbtxt\'\n            num_classes = 2\n        else:\n            path_to_label_map = \'data/mscoco_label_map.pbtxt\'\n            num_classes = 7\n\n        # Define TensorFlow graph\n        self.detection_graph = tf.Graph()\n\n        # Load model and initialize the TensorFlow graph\n        with self.detection_graph.as_default():\n            od_graph_def = tf.GraphDef()\n            with tf.gfile.GFile(path_to_model, \'rb\') as fid:\n                serialized_graph = fid.read()\n                od_graph_def.ParseFromString(serialized_graph)\n                tf.import_graph_def(od_graph_def, name=\'\')\n\n            # Setup session and image tensor\n            self.session = tf.Session(graph=self.detection_graph)\n            self.image_tensor = self.detection_graph.get_tensor_by_name(\'image_tensor:0\')\n\n            # Each box represents a part of the image where a particular object was detected\n            self.boxes = self.detection_graph.get_tensor_by_name(\'detection_boxes:0\')\n\n            # Each score represents the classification confidence for each of the objects\n            self.scores = self.detection_graph.get_tensor_by_name(\'detection_scores:0\')\n            self.classes = self.detection_graph.get_tensor_by_name(\'detection_classes:0\')\n            self.num_detections = self.detection_graph.get_tensor_by_name(\'num_detections:0\')\n\n        # Load label map and convert into categories\n        loaded_label_map = LabelMap.load_label_map(path_to_label_map)\n        categories = LabelMap.convert_label_map_to_categories(label_map=loaded_label_map,\n                                                              max_num_classes=num_classes,\n                                                              use_display_name=True)\n\n        # Assign an index to each category\n        self.category_index = LabelMap.create_category_index(categories)\n\n    # Method: Used to convert image into numpy array\n    @staticmethod\n    def load_image_into_numpy_array(image):\n        """"""\n        :param image: Image\n        :return: Image as NumPy array\n        """"""\n        (im_width, im_height) = image.size\n\n        return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n\n    # Method: Used to convert normalized coordinates to pixel coordinates\n    @staticmethod\n    def normalized_to_pixel_coordinates(box, dims):\n        """"""\n        :param box: Box with normalized coordinates\n        :param dims: Image dimensions\n        :return: Box with pixel coordinates\n        """"""\n        height, width = dims[0], dims[1]\n        pixel_coords = [int(box[0] * height), int(box[1] * width), int(box[2] * height), int(box[3] * width)]\n\n        return np.array(pixel_coords)\n\n    # Method: Used to detect the locations of the vehicles in the image\n    def get_bounding_box_locations(self, image):\n        """"""\n        :param image: Image\n        :return: Bounding box locations surrounding detected vehicles\n        """"""\n        with self.detection_graph.as_default():\n            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n            image_expanded = np.expand_dims(image, axis=0)\n\n            # Actual detection\n            (boxes, scores, classes, num_detections) = self.session.run([self.boxes,\n                                                                         self.scores,\n                                                                         self.classes,\n                                                                         self.num_detections],\n                                                                        feed_dict={self.image_tensor: image_expanded})\n\n            # Remove 1D entries from the shape of the array\n            boxes = np.squeeze(boxes)\n            classes = np.squeeze(classes)\n            scores = np.squeeze(scores)\n\n            # Convert to a list\n            classes_list = classes.tolist()\n\n            # Find cars detected in the image\n            if self.kitti:\n                index_vector = [i for i, id in enumerate(classes_list) if ((id == 1) and (scores[i] > self.min_conf))]\n            else:\n                index_vector = [i for i, id in enumerate(classes_list) if ((id == 3) and (scores[i] > self.min_conf))]\n\n            if len(index_vector) > 0:\n                temp_boxes = []\n\n                for index in index_vector:\n                    # Get image dimensions\n                    dims = image.shape[0:2]\n\n                    # Convert normalized coordinates to pixel coordinates\n                    box = self.normalized_to_pixel_coordinates(boxes[index], dims)\n\n                    # Calculate height, width and ratio to filter out boxes that not the right shape or size\n                    box_h = box[2] - box[0]\n                    box_w = box[3] - box[1]\n                    ratio = box_h / box_w\n\n                    # Filter out boxes that are not the right shape or size\n                    if ratio < 0.8 and box_h > 20 and box_w > 20:\n                        temp_boxes.append(box)\n\n                        if self.info:\n                            print(\'[INFO]: Vehicle Detected at {} with {:.2f}% confidence\'.format(box,\n                                                                                                  scores[index]*100.0))\n\n                self.bounding_boxes = temp_boxes\n\n        return self.bounding_boxes\n'"
utilities/VehicleTracker.py,4,"b'import numpy as np\nfrom filterpy.kalman import KalmanFilter\nfrom scipy.linalg import block_diag\n\n\nclass VehicleTracker:\n    # Method: Constructor\n    def __init__(self):\n        # Initialize parameters for tracker\n        self.id = 0\n        self.num_hits = 0\n        self.num_unmatched = 0\n        self.box = []\n\n        # Initialize parameters for the Kalman filter\n        self.kf = KalmanFilter(dim_x=8, dim_z=8)\n        self.dt = 1.0\n        self.x_state = []\n\n        # State transition matrix (assuming constant velocity model)\n        self.kf.F = np.array([[1, self.dt, 0, 0, 0, 0, 0, 0],\n                              [0, 1, 0, 0, 0, 0, 0, 0],\n                              [0, 0, 1, self.dt, 0, 0, 0, 0],\n                              [0, 0, 0, 1, 0, 0, 0, 0],\n                              [0, 0, 0, 0, 1, self.dt, 0, 0],\n                              [0, 0, 0, 0, 0, 1, 0, 0],\n                              [0, 0, 0, 0, 0, 0, 1, self.dt],\n                              [0, 0, 0, 0, 0, 0, 0, 1]])\n\n        # Measurement matrix (assuming we can only measure the coordinates)\n        self.kf.H = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n                              [0, 0, 1, 0, 0, 0, 0, 0],\n                              [0, 0, 0, 0, 1, 0, 0, 0],\n                              [0, 0, 0, 0, 0, 0, 1, 0]])\n\n        # State covariance matrix\n        self.kf.P *= 100.0\n\n        # Process uncertainty\n        self.Q_comp_mat = np.array([[self.dt ** 4 / 2., self.dt ** 3 / 2.],\n                                    [self.dt ** 3 / 2., self.dt ** 2]])\n        self.kf.Q = block_diag(self.Q_comp_mat, self.Q_comp_mat,\n                               self.Q_comp_mat, self.Q_comp_mat)\n\n        # State uncertainty\n        self.kf.R = np.eye(4)*6.25\n\n    # Method: Used to predict and update the next state for a bounding box\n    def predict_and_update(self, z):\n        """"""\n        :param z: Box\n        :return: Box with updated location\n        """"""\n        self.kf.x = self.x_state\n\n        # Predict\n        self.kf.predict()\n\n        # Update\n        self.kf.update(z)\n\n        # Get current state and convert to integers for pixel coordinates\n        self.x_state = self.kf.x.astype(int)\n\n    # Method: Used to only predict the next state for the bounding box\n    def predict(self):\n        """"""\n        :return: Box with predicted location\n        """"""\n        self.kf.x = self.x_state\n\n        # Predict\n        self.kf.predict()\n\n        # Get current state and convert to integers for pixel coordinates\n        self.x_state = self.kf.x.astype(int)\n'"
utilities/VideoConversion.py,0,"b'import os\nimport cv2\nfrom tqdm import tqdm as tq\nimport subprocess\n\n\n# Method: Used to convert a video into frames\ndef video_to_frames(video_path, output_path):\n    """"""\n    :param video_path: Path to original video\n    :param output_path: Path to output frames\n    """"""\n    count, frame_num = 1, 1\n\n    # Get video\n    video = cv2.VideoCapture(video_path)\n    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    for _ in tq(range(1, total_frames+1)):\n        # Read frame\n        ok, img = video.read()\n\n        if ok:\n            # Save frame\n            frame_name = \'frame{}.png\'.format(frame_num)\n            frame_path = os.path.join(output_path, frame_name)\n            cv2.imwrite(frame_path, img)\n            frame_num += 1\n            count += 1\n            # if 1200 <= count < 2550:\n            #     # Save frame\n            #     frame_name = \'frame{}.png\'.format(frame_num)\n            #     frame_path = os.path.join(output_path, frame_name)\n            #     cv2.imwrite(frame_path, img)\n            #     frame_num += 1\n            # count += 1\n\n\n# Method: used to create a video from a list of frames\ndef frames_to_video(input_dir, output_path, frame_format=\'frame\', file_type=\'.png\', codec=\'libx264\', fps=30):\n    """"""\n    :param input_dir: Path to image directory\n    :param output_path: Path to video\n    :param frame_format: Frame Format\n    :param file_type: Image file extension\n    :param codec: Video codec\n    :param fps: Frames/second\n    """"""\n    input_image_format = \'{}/{}%d{}\'.format(input_dir, frame_format, file_type)\n    subprocess.call(\'ffmpeg -y -r {} -i {} -vcodec {} {}\'.format(fps, input_image_format, codec, output_path))\n\n\n# video_to_frames(\'C:/PythonProjects/VehicleDetectionAndTracking/front_left_vlc_2.mp4\',\n#                 \'C:/PythonProjects/VehicleDetectionAndTracking/output/front_left_2_out/\')\n\nframes_to_video(\'C:/PythonProjects/VehicleDetectionAndTracking/output/front_both_1_out/\',\n                \'C:/PythonProjects/VehicleDetectionAndTracking/videos/front_both_1_out.mp4\', fps=30)\n'"
utilities/Visualization.py,15,"b'import six\nimport collections\nimport numpy as np\nimport tensorflow as tf\nimport PIL.Image as Image\nimport PIL.ImageColor as ImageColor\nimport PIL.ImageDraw as ImageDraw\nimport PIL.ImageFont as ImageFont\n\nstandard_colors = [\n    \'AliceBlue\', \'Chartreuse\', \'Aqua\', \'Aquamarine\', \'Azure\', \'Beige\', \'Bisque\',\n    \'BlanchedAlmond\', \'BlueViolet\', \'BurlyWood\', \'CadetBlue\', \'AntiqueWhite\',\n    \'Chocolate\', \'Coral\', \'CornflowerBlue\', \'Cornsilk\', \'Crimson\', \'Cyan\',\n    \'DarkCyan\', \'DarkGoldenRod\', \'DarkGrey\', \'DarkKhaki\', \'DarkOrange\',\n    \'DarkOrchid\', \'DarkSalmon\', \'DarkSeaGreen\', \'DarkTurquoise\', \'DarkViolet\',\n    \'DeepPink\', \'DeepSkyBlue\', \'DodgerBlue\', \'FireBrick\', \'FloralWhite\',\n    \'ForestGreen\', \'Fuchsia\', \'Gainsboro\', \'GhostWhite\', \'Gold\', \'GoldenRod\',\n    \'Salmon\', \'Tan\', \'HoneyDew\', \'HotPink\', \'IndianRed\', \'Ivory\', \'Khaki\',\n    \'Lavender\', \'LavenderBlush\', \'LawnGreen\', \'LemonChiffon\', \'LightBlue\',\n    \'LightCoral\', \'LightCyan\', \'LightGoldenRodYellow\', \'LightGray\', \'LightGrey\',\n    \'LightGreen\', \'LightPink\', \'LightSalmon\', \'LightSeaGreen\', \'LightSkyBlue\',\n    \'LightSlateGray\', \'LightSlateGrey\', \'LightSteelBlue\', \'LightYellow\', \'Lime\',\n    \'LimeGreen\', \'Linen\', \'Magenta\', \'MediumAquaMarine\', \'MediumOrchid\',\n    \'MediumPurple\', \'MediumSeaGreen\', \'MediumSlateBlue\', \'MediumSpringGreen\',\n    \'MediumTurquoise\', \'MediumVioletRed\', \'MintCream\', \'MistyRose\', \'Moccasin\',\n    \'NavajoWhite\', \'OldLace\', \'Olive\', \'OliveDrab\', \'Orange\', \'OrangeRed\',\n    \'Orchid\', \'PaleGoldenRod\', \'PaleGreen\', \'PaleTurquoise\', \'PaleVioletRed\',\n    \'PapayaWhip\', \'PeachPuff\', \'Peru\', \'Pink\', \'Plum\', \'PowderBlue\', \'Purple\',\n    \'Red\', \'RosyBrown\', \'RoyalBlue\', \'SaddleBrown\', \'Green\', \'SandyBrown\',\n    \'SeaGreen\', \'SeaShell\', \'Sienna\', \'Silver\', \'SkyBlue\', \'SlateBlue\',\n    \'SlateGray\', \'SlateGrey\', \'Snow\', \'SpringGreen\', \'SteelBlue\', \'GreenYellow\',\n    \'Teal\', \'Thistle\', \'Tomato\', \'Turquoise\', \'Violet\', \'Wheat\', \'White\',\n    \'WhiteSmoke\', \'Yellow\', \'YellowGreen\'\n]\n\n\n# Method: Used to add a bounding box to an image (numpy array)\ndef draw_bounding_box_on_image_array(image, ymin, xmin, ymax, xmax, point, color=\'red\', thickness=4, display_str_list=(),\n                                     use_normalized_coordinates=True):\n    """"""\n    :param image: A numpy array with shape [height, width, 3]\n    :param ymin: ymin of bounding box\n    :param xmin: xmin of bounding box\n    :param ymax: ymax of bounding box\n    :param xmax: xmax of bounding box\n    :param color: Color to draw bounding box\n    :param thickness: Line thickness\n    :param display_str_list: List of strings to display in box (each to be shown on its own line)\n    :param use_normalized_coordinates: If True, treat coordinates ymin, xmin, ymax, xmax as relative to the image.\n        Otherwise treat coordinates as absolute\n    """"""\n    image_pil = Image.fromarray(np.uint8(image)).convert(\'RGB\')\n    draw_bounding_box_on_image(image_pil, ymin, xmin, ymax, xmax, point, color, thickness, display_str_list,\n                               use_normalized_coordinates)\n    np.copyto(image, np.array(image_pil))\n\n\n# Method: Used to add a bounding box to an image\ndef draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax, center, color=\'red\', thickness=4, display_str_list=(),\n                               use_normalized_coordinates=True):\n    """"""\n    :param image: A numpy array with shape [height, width, 3]\n    :param ymin: ymin of bounding box\n    :param xmin: xmin of bounding box\n    :param ymax: ymax of bounding box\n    :param xmax: xmax of bounding box\n    :param color: Color to draw bounding box\n    :param thickness: Line thickness\n    :param display_str_list: List of strings to display in box (each to be shown on its own line)\n    :param use_normalized_coordinates: If True, treat coordinates ymin, xmin, ymax, xmax as relative to the image.\n        Otherwise treat coordinates as absolute\n    """"""\n    draw = ImageDraw.Draw(image)\n    im_width, im_height = image.size\n\n    if use_normalized_coordinates:\n        (left, right, top, bottom) = (xmin * im_width, xmax * im_width, ymin * im_height, ymax * im_height)\n    else:\n        (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n    draw.line([(left, top), (left, bottom), (right, bottom), (right, top), (left, top)], width=thickness, fill=color)\n\n    try:\n        font = ImageFont.truetype(\'arial.ttf\', 24)\n    except IOError:\n        font = ImageFont.load_default()\n\n    # If the total height of the display strings added to the top of the bounding box exceeds the top of the image,\n    # stack the strings below the bounding box instead of above.\n    display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n    # Each display_str has a top and bottom margin of 0.05x.\n    total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n\n    if top > total_display_str_height:\n        text_bottom = top\n    else:\n        text_bottom = bottom + total_display_str_height\n\n    # Reverse list and print from bottom to top.\n    for display_str in display_str_list[::-1]:\n        text_width, text_height = font.getsize(display_str)\n        margin = np.ceil(0.05 * text_height)\n        draw.rectangle([(left, text_bottom - text_height - 2 * margin), (left + text_width, text_bottom)], fill=color)\n        draw.text((left + margin, text_bottom - text_height - margin), display_str, fill=\'black\', font=font)\n        text_bottom -= text_height - 2 * margin\n\n\n# Method: Used to draw keypoints on an image (numpy array)\ndef draw_keypoints_on_image_array(image, keypoints, color=\'red\', radius=2, use_normalized_coordinates=True):\n    """"""\n    :param image: A numpy array with shape [height, width, 3]\n    :param keypoints: A numpy array with shape [num_keypoints, 2]\n    :param color: Color to draw the keypoints with\n    :param radius: Keypoint radius\n    :param use_normalized_coordinates: If True, treat keypoint values as relative to the image.\n        Otherwise treat them as absolute\n    """"""\n    image_pil = Image.fromarray(np.uint8(image)).convert(\'RGB\')\n    draw_keypoints_on_image(image_pil, keypoints, color, radius, use_normalized_coordinates)\n    np.copyto(image, np.array(image_pil))\n\n\n# Method: Used to draw keypoints on an image\ndef draw_keypoints_on_image(image, keypoints, color=\'red\', radius=2, use_normalized_coordinates=True):\n    """"""\n    :param image: A numpy array with shape [height, width, 3]\n    :param keypoints: A numpy array with shape [num_keypoints, 2]\n    :param color: Color to draw the keypoints with\n    :param radius: Keypoint radius\n    :param use_normalized_coordinates: If True, treat keypoint values as relative to the image.\n        Otherwise treat them as absolute\n    """"""\n    draw = ImageDraw.Draw(image)\n    im_width, im_height = image.size\n    keypoints_x = [k[1] for k in keypoints]\n    keypoints_y = [k[0] for k in keypoints]\n\n    if use_normalized_coordinates:\n        keypoints_x = tuple([im_width * x for x in keypoints_x])\n        keypoints_y = tuple([im_height * y for y in keypoints_y])\n\n    for keypoint_x, keypoint_y in zip(keypoints_x, keypoints_y):\n        draw.ellipse([(keypoint_x - radius, keypoint_y - radius), (keypoint_x + radius, keypoint_y + radius)],\n                     outline=color, fill=color)\n\n\n# Method: Used to draw mask on an image (numpy array)\ndef draw_mask_on_image_array(image, mask, color=\'red\', alpha=0.7):\n    """"""\n    :param image: uint8 numpy array with shape (img_height, img_height, 3)\n    :param mask: uint8 numpy array of shape (img_height, img_height) with values between either 0 or 1\n    :param color: Color to draw the keypoints with\n    :param alpha: Transparency value between 0 and 1\n    """"""\n    if image.dtype != np.uint8:\n        raise ValueError(\'`image` not of type np.uint8\')\n\n    if mask.dtype != np.uint8:\n        raise ValueError(\'`mask` not of type np.uint8\')\n\n    if np.any(np.logical_and(mask != 1, mask != 0)):\n        raise ValueError(\'`mask` elements should be in [0, 1]\')\n\n    rgb = ImageColor.getrgb(color)\n    pil_image = Image.fromarray(image)\n    solid_color = np.expand_dims(np.ones_like(mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n    pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert(\'RGBA\')\n    pil_mask = Image.fromarray(np.uint8(255.0 * alpha * mask)).convert(\'L\')\n    pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n    np.copyto(image, np.array(pil_image.convert(\'RGB\')))\n\n\n# Method: Used to overlay labeled boxes on an image with formatted scores and label names\ndef visualize_boxes_and_labels_on_image_array(image, boxes, classes, scores, category_index, instance_masks=None,\n                                              keypoints=None, use_normalized_coordinates=False, max_boxes_to_draw=10,\n                                              min_score_thresh=0.5, line_thickness=4):\n    """"""\n    :param image: A numpy array with shape (img_height, img_width, 3)\n    :param boxes: A numpy array of shape [N, 4]\n    :param classes: A numpy array of shape [N]. (Note: Class indices are 1-based, and match the keys in the label map)\n    :param scores: A numpy array of shape [N] or None.  (Note: If scores=None, then this function assumes that the\n        boxes to be plotted are groundtruth boxes and plot all boxes as black with no classes or scores)\n    :param category_index: A dict containing category index `id` and category name `name`) keyed by category indices\n    :param instance_masks: A numpy array of shape [N, image_height, image_width]\n    :param keypoints: A numpy array of shape [N, num_keypoints, 2]\n    :param use_normalized_coordinates: Whether boxes is to be interpreted as normalized coordinates or not\n    :param max_boxes_to_draw: Maximum number of boxes to visualize.  If None, draw all boxes\n    :param min_score_thresh: Minimum score threshold for a box to be visualized\n    :param line_thickness: Integer controlling line width of the boxes\n    """"""\n    # Create a display string (and color) for every box location, group any boxes that correspond to the same location\n    box_to_display_str_map = collections.defaultdict(list)\n    box_to_color_map = collections.defaultdict(str)\n    box_to_instance_masks_map = {}\n    box_to_keypoints_map = collections.defaultdict(list)\n\n    if not max_boxes_to_draw:\n        max_boxes_to_draw = boxes.shape[0]\n\n    for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n        if scores is None or scores[i] > min_score_thresh:\n            box = tuple(boxes[i].tolist())\n\n            if instance_masks is not None:\n                box_to_instance_masks_map[box] = instance_masks[i]\n\n            if keypoints is not None:\n                box_to_keypoints_map[box].extend(keypoints[i])\n\n            if scores is None:\n                box_to_color_map[box] = \'black\'\n            else:\n                if classes[i] in category_index.keys():\n                    class_name = category_index[classes[i]][\'name\']\n                else:\n                    class_name = \'N/A\'\n\n                display_str = \'{}: {}%\'.format(class_name, int(100 * scores[i]))\n            box_to_display_str_map[box].append(display_str)\n            box_to_color_map[box] = standard_colors[classes[i] % len(standard_colors)]\n\n    # Draw all boxes onto image.\n    for box, color in box_to_color_map.items():\n        ymin, xmin, ymax, xmax = box\n\n        box_center = (np.average(box[1::2]), np.average(box[0::2]))\n\n        if instance_masks is not None:\n            draw_mask_on_image_array(image, box_to_instance_masks_map[box], color=color)\n\n        draw_bounding_box_on_image_array(image, ymin, xmin, ymax, xmax, box_center, color=color, thickness=line_thickness,\n                                         display_str_list=box_to_display_str_map[box],\n                                         use_normalized_coordinates=use_normalized_coordinates)\n\n        if keypoints is not None:\n            draw_keypoints_on_image_array(image, box_to_keypoints_map[box], color=color, radius=line_thickness // 2,\n                                          use_normalized_coordinates=use_normalized_coordinates)\n\n    return image\n'"
utilities/__init__.py,0,b''
