file_path,api_count,code
video_stitcher.py,5,"b'import cv2\nimport numpy as np\nimport imutils\nimport tqdm\nimport os\nfrom moviepy.editor import ImageSequenceClip\n\n\nclass VideoStitcher:\n    def __init__(self, left_video_in_path, right_video_in_path, video_out_path, video_out_width=800, display=False):\n        # Initialize arguments\n        self.left_video_in_path = left_video_in_path\n        self.right_video_in_path = right_video_in_path\n        self.video_out_path = video_out_path\n        self.video_out_width = video_out_width\n        self.display = display\n\n        # Initialize the saved homography matrix\n        self.saved_homo_matrix = None\n\n    def stitch(self, images, ratio=0.75, reproj_thresh=4.0):\n        # Unpack the images\n        (image_b, image_a) = images\n\n        # If the saved homography matrix is None, then we need to apply keypoint matching to construct it\n        if self.saved_homo_matrix is None:\n            # Detect keypoints and extract\n            (keypoints_a, features_a) = self.detect_and_extract(image_a)\n            (keypoints_b, features_b) = self.detect_and_extract(image_b)\n\n            # Match features between the two images\n            matched_keypoints = self.match_keypoints(keypoints_a, keypoints_b, features_a, features_b, ratio, reproj_thresh)\n\n            # If the match is None, then there aren\'t enough matched keypoints to create a panorama\n            if matched_keypoints is None:\n                return None\n\n            # Save the homography matrix\n            self.saved_homo_matrix = matched_keypoints[1]\n\n        # Apply a perspective transform to stitch the images together using the saved homography matrix\n        output_shape = (image_a.shape[1] + image_b.shape[1], image_a.shape[0])\n        result = cv2.warpPerspective(image_a, self.saved_homo_matrix, output_shape)\n        result[0:image_b.shape[0], 0:image_b.shape[1]] = image_b\n\n        # Return the stitched image\n        return result\n\n    @staticmethod\n    def detect_and_extract(image):\n        # Detect and extract features from the image (DoG keypoint detector and SIFT feature extractor)\n        descriptor = cv2.xfeatures2d.SIFT_create()\n        (keypoints, features) = descriptor.detectAndCompute(image, None)\n\n        # Convert the keypoints from KeyPoint objects to numpy arrays\n        keypoints = np.float32([keypoint.pt for keypoint in keypoints])\n\n        # Return a tuple of keypoints and features\n        return (keypoints, features)\n\n    @staticmethod\n    def match_keypoints(keypoints_a, keypoints_b, features_a, features_b, ratio, reproj_thresh):\n        # Compute the raw matches and initialize the list of actual matches\n        matcher = cv2.DescriptorMatcher_create(""BruteForce"")\n        raw_matches = matcher.knnMatch(features_a, features_b, k=2)\n        matches = []\n\n        for raw_match in raw_matches:\n            # Ensure the distance is within a certain ratio of each other (i.e. Lowe\'s ratio test)\n            if len(raw_match) == 2 and raw_match[0].distance < raw_match[1].distance * ratio:\n                matches.append((raw_match[0].trainIdx, raw_match[0].queryIdx))\n\n        # Computing a homography requires at least 4 matches\n        if len(matches) > 4:\n            # Construct the two sets of points\n            points_a = np.float32([keypoints_a[i] for (_, i) in matches])\n            points_b = np.float32([keypoints_b[i] for (i, _) in matches])\n\n            # Compute the homography between the two sets of points\n            (homography_matrix, status) = cv2.findHomography(points_a, points_b, cv2.RANSAC, reproj_thresh)\n\n            # Return the matches, homography matrix and status of each matched point\n            return (matches, homography_matrix, status)\n\n        # No homography could be computed\n        return None\n\n    @staticmethod\n    def draw_matches(image_a, image_b, keypoints_a, keypoints_b, matches, status):\n        # Initialize the output visualization image\n        (height_a, width_a) = image_a.shape[:2]\n        (height_b, width_b) = image_b.shape[:2]\n        visualisation = np.zeros((max(height_a, height_b), width_a + width_b, 3), dtype=""uint8"")\n        visualisation[0:height_a, 0:width_a] = image_a\n        visualisation[0:height_b, width_a:] = image_b\n\n        for ((train_index, query_index), s) in zip(matches, status):\n            # Only process the match if the keypoint was successfully matched\n            if s == 1:\n                # Draw the match\n                point_a = (int(keypoints_a[query_index][0]), int(keypoints_a[query_index][1]))\n                point_b = (int(keypoints_b[train_index][0]) + width_a, int(keypoints_b[train_index][1]))\n                cv2.line(visualisation, point_a, point_b, (0, 255, 0), 1)\n\n        # return the visualization\n        return visualisation\n\n    def run(self):\n        # Set up video capture\n        left_video = cv2.VideoCapture(self.left_video_in_path)\n        right_video = cv2.VideoCapture(self.right_video_in_path)\n        print(\'[INFO]: {} and {} loaded\'.format(self.left_video_in_path.split(\'/\')[-1],\n                                                self.right_video_in_path.split(\'/\')[-1]))\n        print(\'[INFO]: Video stitching starting....\')\n\n        # Get information about the videos\n        n_frames = min(int(left_video.get(cv2.CAP_PROP_FRAME_COUNT)),\n                       int(right_video.get(cv2.CAP_PROP_FRAME_COUNT)))\n        fps = int(left_video.get(cv2.CAP_PROP_FPS))\n        frames = []\n\n        for _ in tqdm.tqdm(np.arange(n_frames)):\n            # Grab the frames from their respective video streams\n            ok, left = left_video.read()\n            _, right = right_video.read()\n\n            if ok:\n                # Stitch the frames together to form the panorama\n                stitched_frame = self.stitch([left, right])\n\n                # No homography could not be computed\n                if stitched_frame is None:\n                    print(""[INFO]: Homography could not be computed!"")\n                    break\n\n                # Add frame to video\n                stitched_frame = imutils.resize(stitched_frame, width=self.video_out_width)\n                frames.append(stitched_frame)\n\n                if self.display:\n                    # Show the output images\n                    cv2.imshow(""Result"", stitched_frame)\n\n                # If the \'q\' key was pressed, break from the loop\n                if cv2.waitKey(1) & 0xFF == ord(""q""):\n                    break\n\n        cv2.destroyAllWindows()\n        print(\'[INFO]: Video stitching finished\')\n\n        # Save video\n        print(\'[INFO]: Saving {} in {}\'.format(self.video_out_path.split(\'/\')[-1],\n                                               os.path.dirname(self.video_out_path)))\n        clip = ImageSequenceClip(frames, fps=fps)\n        clip.write_videofile(self.video_out_path, codec=\'mpeg4\', audio=False, progress_bar=True, verbose=False)\n        print(\'[INFO]: {} saved\'.format(self.video_out_path.split(\'/\')[-1]))\n\n\n# Example call to \'VideoStitcher\'\nstitcher = VideoStitcher(left_video_in_path=\'videos/bike_left_01.mp4\',\n                         right_video_in_path=\'videos/bike_right_01.mp4\',\n                         video_out_path=\'output/bike_01_stitched.mp4\')\nstitcher.run()\n'"
