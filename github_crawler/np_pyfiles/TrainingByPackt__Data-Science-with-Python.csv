file_path,api_count,code
Chapter05/ex-46-income-script.py,0,"b'import xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport pickle\nimport pandas as pd\n\ndef load_obj(file):\n\twith open(file + \'.pkl\', \'rb\') as f:\n\t\treturn pickle.load(f)\n\t\t\nloaded_model = xgb.Booster({\'nthread\': 8})\nloaded_model.load_model(\'income-model.model\')\n\nlabel_dict = load_obj(\'income_labels\')\n\n\nage = input(""Please enter age: "")\nworkclass = input(""Please enter workclass: "")\neducation_num = input(""Please enter education_num: "")\noccupation = input(""Please enter occupation: "")\ncapital_gain = input(""Please enter capital_gain: "")\ncapital_loss = input(""Please enter capital_loss: "")\nhours_per_week = input(""Please enter hours_per_week: "")\n\ndata_list = [age, workclass, education_num, occupation, capital_gain, capital_loss, hours_per_week]\n\n\ndata = pd.DataFrame([data_list])\n\n\ndata.columns = [\'age\', \'workclass\', \'education-num\',\n       \'occupation\', \'capital-gain\', \'capital-loss\', \'hours-per-week\']\n\ndata[[\'workclass\', \'occupation\']] = data[[\'workclass\', \'occupation\']].apply(lambda x: label_dict[x.name].transform(x))\n\ndata = data.astype(int)\n\ndata_xgb = xgb.DMatrix(data)\n\npred = loaded_model.predict(data_xgb)\n\nincome = label_dict[\'income\'].inverse_transform([int(pred[0])])\n\nprint(""Predicted income is "" + str(income[0]))'"
Chapter02/Activities/Activity_02.py,0,"b""# Activity 1: Line Plot\n\n# Create x\nx = ['January','February','March','April','May','June']\nprint(x)\n\n# Create y\ny = [1000, 1200, 1400, 1600, 1800, 2000]\nprint(y)\n\n# Create the plot\nimport matplotlib.pyplot as plt # import matplotlib\nplt.plot(x, y, '*:b') # plot items sold (y) by month (x)\nplt.xlabel('Month') # label x-axis\nplt.ylabel('Items Sold') # label y-axis\nplt.title('Items Sold has been Increasing Linearly') # add plot title\nplt.show() # print plot"""
Chapter02/Activities/Activity_03.py,0,"b""# Activity 2: Bar plot\n\n# Create a list for x\nx = ['Boston Celtics','Los Angeles Lakers', 'Chicago Bulls', 'Golden State Warriors', 'San Antonio Spurs']\nprint(x)\n\n# Create a list for y\ny = [17, 16, 6, 6, 5]\nprint(y)\n\n# Put into a data frame so we can sort them\nimport pandas as pd\ndf = pd.DataFrame({'Team': x,\n                   'Titles': y})\n\n# Sort df by titles\ndf_sorted = df.sort_values(by=('Titles'), ascending=False)\n\n# Make a programmatic title\nteam_with_most_titles = df_sorted['Team'][0] # get team with most titles\nmost_titles = df_sorted['Titles'][0] # get the number of max titles\ntitle = 'The {} have the most titles with {}'.format(team_with_most_titles, most_titles) # create title\nprint(title)\n\n# Plot it\nimport matplotlib.pyplot as plt # import matplotlib\nplt.bar(df_sorted['Team'], df_sorted['Titles'], color='red') # plot titles by team and make bars red\nplt.xlabel('Team') # create x label\nplt.ylabel('Number of Championships') # create y label\nplt.xticks(rotation=45) # rotate x tick labels 45 degrees\nplt.title(title) # title\nplt.savefig('Titles_by_Team') # save figure to present working directory\nplt.show() # print plot\n\n# Fix the cropping\nimport matplotlib.pyplot as plt\nplt.bar(df_sorted['Team'], df_sorted['Titles'], color='red')\nplt.xlabel('Team')\nplt.ylabel('Number of Championships')\nplt.xticks(rotation=45)\nplt.title(title)\nplt.savefig('Titles_by_Team', bbox_inches='tight') # fix the cropping issue\nplt.show()"""
Chapter02/Activities/Activity_04.py,1,"b""# Activity 3: Multiple Plot Types using Subplots \n\n# import Items_Sold_by_Week.csv\nimport pandas as pd\nItems_by_Week = pd.read_csv('Items_Sold_by_Week.csv')\n\n# For scatterplot\n# import Height_by_Weight.csv\nimport pandas as pd\nWeight_by_Height = pd.read_csv('Weight_by_Height.csv')\n\n# For histogram and Box-and-Whisker\n# Create an array of 100 normally distributed numbers\nimport numpy as np\ny = np.random.normal(loc=0, scale=0.1, size=100) # 100 numbers with mean of 0 and standard deviation of 0.1\n\n# generate figure with 6 subplots organized in 3 rows and 2 columns that do not overlap\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows=3, ncols=2)\nplt.tight_layout() # prevent plot overlap\n\n# Name the titles\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows=3, ncols=2)\n# line plot (top left)\naxes[0,0].set_title('Line')\n# Bar plot (top right)\naxes[0,1].set_title('Bar')\n# Horizontal bar plot (middle left)\naxes[1,0].set_title('Horizontal Bar')\n# Histogram (middle right)\naxes[1,1].set_title('Histogram')\n# Scatterplot (bottom left)\naxes[2,0].set_title('Scatter')\n# Box-and-Whisker\naxes[2,1].set_title('Box-and-Whisker')\nplt.tight_layout() # prevent plot overlap\n\n# in the \xe2\x80\x98Line\xe2\x80\x99, \xe2\x80\x98Bar\xe2\x80\x99, and \xe2\x80\x98Horizontal Bar\xe2\x80\x99 axes, plot \xe2\x80\x98Items_Sold\xe2\x80\x99 by \xe2\x80\x98Week\xe2\x80\x99 from the \xe2\x80\x98Items_by_Week\xe2\x80\x99 \n# Horizontal bar\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows=3, ncols=2)\n# line plot (top left)\naxes[0,0].plot(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for line plot\naxes[0,0].set_title('Line')\n# Bar plot (top right)\naxes[0,1].bar(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for bar plot\naxes[0,1].set_title('Bar')\n# Horizontal bar plot (middle left)\naxes[1,0].barh(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for horizontal bar plot\naxes[1,0].set_title('Horizontal Bar')\n# Histogram (middle right)\naxes[1,1].set_title('Histogram')\n# Scatterplot (bottom left)\naxes[2,0].set_title('Scatter')\n# Box-and-Whisker\naxes[2,1].set_title('Box-and-Whisker')\nplt.tight_layout() # prevent plot overlap\n\n# in the 'Histogram' and 'Box-and-Whisker axes, plot \xe2\x80\x98Items_Sold\xe2\x80\x99 by \xe2\x80\x98Week\xe2\x80\x99 from the \xe2\x80\x98Items_by_Week\xe2\x80\x99 \n# Horizontal bar\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows=3, ncols=2)\n# line plot (top left)\naxes[0,0].plot(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for line plot\naxes[0,0].set_title('Line')\n# Bar plot (top right)\naxes[0,1].bar(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for bar plot\naxes[0,1].set_title('Bar')\n# Horizontal bar plot (middle left)\naxes[1,0].barh(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for horizontal bar plot\naxes[1,0].set_title('Horizontal Bar')\n# Histogram (middle right)\naxes[1,1].hist(y, bins=20)\naxes[1,1].set_title('Histogram')\n# Scatterplot (bottom left)\naxes[2,1].boxplot(y)\naxes[2,0].set_title('Scatter')\n# Box-and-Whisker\naxes[2,1].set_title('Box-and-Whisker')\nplt.tight_layout() # prevent plot overlap\n\n# add scatterplot\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows=3, ncols=2)\n# line plot (top left)\naxes[0,0].plot(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for line plot\naxes[0,0].set_title('Line')\n# Bar plot (top right)\naxes[0,1].bar(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for bar plot\naxes[0,1].set_title('Bar')\n# Horizontal bar plot (middle left)\naxes[1,0].barh(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for horizontal bar plot\naxes[1,0].set_title('Horizontal Bar')\n# Histogram (middle right)\naxes[1,1].hist(y, bins=20) # for histogram\naxes[1,1].set_title('Histogram') \n# Scatterplot (bottom left)\naxes[2,0].scatter(Weight_by_Height['Height'], Weight_by_Height['Weight']) # for scatterplot\naxes[2,0].set_title('Scatter')\n# Box-and-Whisker\naxes[2,1].boxplot(y) # for Box-and-Whisker\naxes[2,1].set_title('Box-and-Whisker')\nplt.tight_layout() # prevent plot overlap\n\n# Set x- and y-axis for each subplot\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows=3, ncols=2)\n# line plot (top left)\naxes[0,0].plot(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for line plot\naxes[0,0].set_xlabel('Week')\naxes[0,0].set_ylabel('Items Sold')\naxes[0,0].set_title('Line')\n# Bar plot (top right)\naxes[0,1].bar(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for bar plot\naxes[0,1].set_xlabel('Week')\naxes[0,1].set_ylabel('Items Sold')\naxes[0,1].set_title('Bar')\n# Horizontal bar plot (middle left)\naxes[1,0].barh(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for horizontal bar plot\naxes[1,0].set_xlabel('Items Sold')\naxes[1,0].set_ylabel('Week')\naxes[1,0].set_title('Horizontal Bar')\n# Histogram (middle right)\naxes[1,1].hist(y, bins=20) # for histogram\naxes[1,1].set_xlabel('y')\naxes[1,1].set_ylabel('Frequency')\naxes[1,1].set_title('Histogram') \n# Scatterplot (bottom left)\naxes[2,0].scatter(Weight_by_Height['Height'], Weight_by_Height['Weight']) # for scatterplot\naxes[2,0].set_xlabel('Height')\naxes[2,0].set_ylabel('Weight')\naxes[2,0].set_title('Scatter')\n# Box-and-Whisker\naxes[2,1].boxplot(y) # for Box-and-Whisker\naxes[2,1].set_title('Box-and-Whisker')\nplt.tight_layout() # prevent plot overlap\n\n# Enlarge the figure size and Save the figure\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(8,8)) # for figure size\n# line plot (top left)\naxes[0,0].plot(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for line plot\naxes[0,0].set_xlabel('Week')\naxes[0,0].set_ylabel('Items Sold')\naxes[0,0].set_title('Line')\n# Bar plot (top right)\naxes[0,1].bar(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for bar plot\naxes[0,1].set_xlabel('Week')\naxes[0,1].set_ylabel('Items Sold')\naxes[0,1].set_title('Bar')\n# Horizontal bar plot (middle left)\naxes[1,0].barh(Items_by_Week['Week'], Items_by_Week['Items_Sold']) # for horizontal bar plot\naxes[1,0].set_xlabel('Items Sold')\naxes[1,0].set_ylabel('Week')\naxes[1,0].set_title('Horizontal Bar')\n# Histogram (middle right)\naxes[1,1].hist(y, bins=20) # for histogram\naxes[1,1].set_xlabel('y')\naxes[1,1].set_ylabel('Frequency')\naxes[1,1].set_title('Histogram') \n# Scatterplot (bottom left)\naxes[2,0].scatter(Weight_by_Height['Height'], Weight_by_Height['Weight']) # for scatterplot\naxes[2,0].set_xlabel('Height')\naxes[2,0].set_ylabel('Weight')\naxes[2,0].set_title('Scatter')\n# Box-and-Whisker\naxes[2,1].boxplot(y) # for Box-and-Whisker\naxes[2,1].set_title('Box-and-Whisker')\nplt.tight_layout() # prevent plot overlap\nfig.savefig('Six_Subplots') # save figure\n"""
Chapter02/Exercises/Exercise_13.py,1,"b""# Exercise 1: Line Plot\n\n# create an array of numbers for x\nimport numpy as np\nx = np.linspace(0, 10, 20)\nprint(x)\n\n# create y\ny = x**3\nprint(y)\n\n# create the plot\nimport matplotlib.pyplot as plt\nplt.plot(x, y)\nplt.show()\n\n# add x-axis label\nimport matplotlib.pyplot as plt\nplt.plot(x, y)\nplt.xlabel('Linearly Spaced Numbers') # add x axis label\nplt.show()\n\n# add y-label\nimport matplotlib.pyplot as plt\nplt.plot(x, y)\nplt.xlabel('Linearly Spaced Numbers') # add x axis label\nplt.ylabel('y Value') # add y axis label\nplt.show()\n\n# add title\nimport matplotlib.pyplot as plt\nplt.plot(x, y)\nplt.xlabel('Linearly Spaced Numbers') # add x axis label\nplt.ylabel('y Value') # add y axis label\nplt.title('x by x Cubed') # add title\nplt.show()\n\n# change line color\nimport matplotlib.pyplot as plt\nplt.plot(x, y, 'k') # change color to black\nplt.xlabel('Linearly Spaced Numbers') # add x axis label\nplt.ylabel('y Value') # add y axis label\nplt.title('x by x Cubed') # add title\nplt.show()\n\n# make markers into diamonds\nimport matplotlib.pyplot as plt\nplt.plot(x, y, 'Dk') # make markers into diamonds\nplt.xlabel('Linearly Spaced Numbers') # add x axis label\nplt.ylabel('y Value') # add y axis label\nplt.title('x by x Cubed') # add title\nplt.show()\n\n# connect markers with a solid line\nimport matplotlib.pyplot as plt\nplt.plot(x, y, 'D-k') # connect markers with a solid line\nplt.xlabel('Linearly Spaced Numbers') # add x axis label\nplt.ylabel('y Value') # add y axis label\nplt.title('x by x Cubed') # add title\nplt.show()\n\n# increase title font size\nimport matplotlib.pyplot as plt\nplt.plot(x, y, 'D-k') # connect markers with a solid line\nplt.xlabel('Linearly Spaced Numbers') # add x axis label\nplt.ylabel('y Value') # add y axis label\nplt.title('x by x Cubed', fontsize=22) # increase font size\nplt.show()\n\n\n"""
Chapter02/Exercises/Exercise_14.py,0,"b""# Exercise 2: Add a Second Line to Line Plot\n\n# continuing from exercise 1\n\n# create another line for which to plot\ny2 = x**2\nprint(y2)\n\n# add a line for y2\nimport matplotlib.pyplot as plt\nplt.plot(x, y, 'D-k') # connect markers with a solid line\nplt.plot(x, y2) # add a line for y2\nplt.xlabel('Linearly Spaced Numbers') # add x axis label\nplt.ylabel('y Value') # add y axis label\nplt.title('x by x Cubed', fontsize=22) # increase font size\nplt.show()\n\n# change color and style of y2\nimport matplotlib.pyplot as plt\nplt.plot(x, y, 'D-k') # connect markers with a solid line\nplt.plot(x, y2, '--r') # make y2 a red, dotted line\nplt.xlabel('Linearly Spaced Numbers') # add x axis label\nplt.ylabel('y Value') # add y axis label\nplt.title('x by x Cubed', fontsize=22) # increase font size\nplt.show()\n\n# create a legend\nimport matplotlib.pyplot as plt\nplt.plot(x, y, 'D-k', label='x cubed') # label as x cubed\nplt.plot(x, y2, '--r', label='x squared') # label as x squared\nplt.xlabel('Linearly Spaced Numbers') # add x axis label\nplt.ylabel('y Value') # add y axis label\nplt.title('x by x Cubed', fontsize=22) # increase font size\nplt.legend(loc='upper left') # create a plot legend and place it in the upper left\nplt.show()\n\n# add multi-line, descriptive title\nimport matplotlib.pyplot as plt\nplt.plot(x, y, 'D-k', label='x cubed') # label as x cubed\nplt.plot(x, y2, '--r', label='x squared') # label as x squared\nplt.xlabel('Linearly Spaced Numbers') # add x axis label\nplt.ylabel('y Value') # add y axis label\nplt.title('As x increases, \\nx Cubed (black) increases \\nat a Greater Rate than \\nx Squared (red)', fontsize=22) # make a multi-line title\nplt.legend(loc='upper left') # create a plot legend and place it in the upper left\nplt.show()\n\n# change dimensions of plot\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,5)) # increase plot size\nplt.plot(x, y, 'D-k', label='x cubed') # label as x cubed\nplt.plot(x, y2, '--r', label='x squared') # label as x squared\nplt.xlabel('Linearly Spaced Numbers') # add x axis label\nplt.ylabel('y Value')\nplt.title('As x increases, \\nx Cubed (black) increases \\nat a Greater Rate than \\nx Squared (red)', fontsize=22) # make a multi-line title\nplt.legend(loc='upper left') # create a plot legend and place it in the upper left\nplt.show()"""
Chapter02/Exercises/Exercise_15.py,0,"b""# Exercise 3: Bar Plot\n\n# create a list of groups\nx = ['Shirts', 'Pants','Shorts','Shoes']\nprint(x)\n\n# create a list of revenue\ny = [1000, 1200, 800, 1800]\nprint(y)\n\n# create bar plot\nimport matplotlib.pyplot as plt\nplt.bar(x, y) # plot revenue by group\nplt.show()\n\n# style the plot\nimport matplotlib.pyplot as plt\nplt.bar(x, y) # plot revenue by group\nplt.xlabel('Item Type') # x-axis label\nplt.ylabel('Sales Revenue ($)') # y-axis label\nplt.title('Sales Revenue by Item Type')\nplt.show() # print the plot\n\n# find the index of the greatest value in list y\nindex_of_max_y = y.index(max(y))\nprint(index_of_max_y)\n\n# determine the most sold item\nmost_sold_item = x[index_of_max_y]\nprint(most_sold_item)\n\n# make the title programmatic\nimport matplotlib.pyplot as plt\nplt.bar(x, y) # plot revenue by group\nplt.xlabel('Item Type') # x-axis label\nplt.ylabel('Sales Revenue ($)') # y-axis label\nplt.title('{} Produce the Most Sales Revenue'.format(most_sold_item)) # create programmatic title\nplt.show() # print the plot\n\n# make a horizontal bar plot\nimport matplotlib.pyplot as plt\nplt.barh(x, y) # turn the plot horizontal\nplt.xlabel('Item Type') # x-axis label\nplt.ylabel('Sales Revenue ($)') # y-axis label\nplt.title('{} Produce the Most Sales Revenue'.format(most_sold_item)) # create programmatic title\nplt.show() # print the plot\n\n# switch the axes labels\nimport matplotlib.pyplot as plt\nplt.barh(x, y) # turn the plot horizontal\nplt.xlabel('Sales Revenue ($)') # x-axis label\nplt.ylabel('Item Type') # y-axis label\nplt.title('{} Produce the Most Sales Revenue'.format(most_sold_item)) # create programmatic title\nplt.show() # print the plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
Chapter02/Exercises/Exercise_16.py,1,"b""# Exercise 4: Histogram\n\n# generate list of normally distributed numbers\nimport numpy as np\ny = np.random.normal(loc=0, scale=0.1, size=100) # 100 numbers with mean of 0 and standard deviation of 0.1\nprint(y)\n\n# create histogram\nimport matplotlib.pyplot as plt\nplt.hist(y, bins=20)\nplt.show()\n\n# label the axes\nimport matplotlib.pyplot as plt\nplt.hist(y, bins=20)\nplt.xlabel('y Value')\nplt.ylabel('Frequency')\nplt.show()\n\n# run the shapiro wilk test\nfrom scipy.stats import shapiro\nshap_w, shap_p = shapiro(y)\nprint(shap_p)\n\n# set up some logic\nif shap_p > 0.05:\n    normal_YN = 'Fail to reject the null hypothesis. Data is normally distributed.'\nelse:\n    normal_YN = 'Null hypothesis is rejected. Data is not normally distributed.'\nprint(normal_YN)\n\n# re-create histogram\nimport matplotlib.pyplot as plt\nplt.hist(y, bins=20)\nplt.xlabel('y Value')\nplt.ylabel('Frequency')\nplt.title(normal_YN) # programmatic plot title\nplt.show()"""
Chapter02/Exercises/Exercise_17.py,1,"b""# Exercise 5: Box-and-Whisker plot\n\n# Generate a list of normally distributed numbers\nimport numpy as np\ny = np.random.normal(loc=0, scale=0.1, size=100) # 100 numbers with mean of 0 and standard deviation of 0.1\n\n# Generate boxplot\nimport matplotlib.pyplot as plt # import matplotlib\nplt.boxplot(y) # create boxplot of y\nplt.show() # print plot\n\n# Calculate shapiro wilk p-value\nfrom scipy.stats import shapiro\nshap_w, shap_p = shapiro(y)\nprint(shap_p)\n\n# Get outliers\n# convert to z-scores\nfrom scipy.stats import zscore\ny_z_scores = zscore(y) # convert y into z scores\n\n# get the number of scores with absolute value of 3 or more\ntotal_outliers = 0\nfor i in range(len(y_z_scores)):\n    if abs(y_z_scores[i]) >= 3:\n        total_outliers += 1\nprint(total_outliers)\n           \n# set up some logic for the title\nif shap_p > 0.05:\n    title = 'Normally distributed with {} outlier(s).'.format(total_outliers)\nelse:\n    title = 'Not normally distributed with {} outlier(s).'.format(total_outliers)\nprint(title)\n\n# Generate boxplot with programmatic tile\nimport matplotlib.pyplot as plt # import matplotlib\nplt.boxplot(y) # generate boxplot\nplt.title(title) # programmatic title\nplt.show() # print plot"""
Chapter02/Exercises/Exercise_18.py,0,"b""# Exercise 6: Scatterplot\n\n# generate list of numbers for height\ny = [5, 5.5, 5, 5.5, 6, 6.5, 6, 6.5, 7, 5.5, 5.25, 6, 5.25]\nprint(y)\n\n# create a list of numbers for weight\nx = [100, 150, 110, 140, 140, 170, 168, 165, 180, 125, 115, 155, 135]\nprint(x)\n\n# create histogram\nimport matplotlib.pyplot as plt\nplt.scatter(x, y) # generate scatterplot\nplt.xlabel('Weight') # label x-axis\nplt.ylabel('Height') # label y-axis\nplt.show() # print plot\n\n# calculate pearson correlations\nfrom scipy.stats import pearsonr\ncorrelation_coeff, p_value = pearsonr(x, y)\nprint(correlation_coeff)\n\n# Set up some logic\nif correlation_coeff == 1.00:\n    title = 'There is a perfect positive linear relationship (r = {0:0.2f}).'.format(correlation_coeff)\nelif correlation_coeff >= 0.8:\n    title = 'There is a very strong, positive linear relationship (r = {0:0.2f}).'.format(correlation_coeff)\nelif correlation_coeff >= 0.6:\n    title = 'There is a strong, positive linear relationship (r = {0:0.2f}).'.format(correlation_coeff)\nelif correlation_coeff >= 0.4:\n    title = 'There is a moderate, positive linear relationship (r = {0:0.2f}).'.format(correlation_coeff)\nelif correlation_coeff >= 0.2:\n    title = 'There is a weak, positive linear relationship (r = {0:0.2f}).'.format(correlation_coeff)\nelif correlation_coeff > 0:\n    title = 'There is a very weak, positive linear relationship (r = {0:0.2f}).'.format(correlation_coeff)\nelif correlation_coeff == 0:\n    title = 'There is no linear relationship (r = {0:0.2f}).'.format(correlation_coeff)\nelif correlation_coeff <= -0.8:\n    title = 'There is a very strong, negative linear relationship (r = {0:0.2f}).'.format(correlation_coeff)\nelif correlation_coeff <= -0.6:\n    title = 'There is a strong, negative linear relationship (r = {0:0.2f}).'.format(correlation_coeff)\nelif correlation_coeff <= -0.4:\n    title = 'There is a moderate, negative linear relationship (r = {0:0.2f}).'.format(correlation_coeff)\nelif correlation_coeff <= -0.2:\n    title = 'There is a weak, negative linear relationship (r = {0:0.2f}).'.format(correlation_coeff)\nelse: \n    title = 'There is a very weak, negative linear relationship (r = {0:0.2f}).'.format(correlation_coeff)\nprint(title)\n\n# Use title as title\nimport matplotlib.pyplot as plt\nplt.scatter(x, y) # generate scatterplot\nplt.xlabel('Weight') # label x-axis\nplt.ylabel('Height') # label y-axis\nplt.title(title) # set programmatic title\nplt.show() # print plot\n"""
Chapter02/Exercises/Exercise_19.py,1,"b""# Exercise 7: Single Line Plot Using Subplots\n\nimport numpy as np\nx = np.linspace(0, 10, 20) # create x\ny = x**3 # create y\n\n# create figure and a set of axes\nimport matplotlib.pyplot as plt # import dependencies\nfig, axes = plt.subplots() # create figure and set of axes\nplt.show() # print plot\n\n# call the plot object\nfig\n\n# create a line plot in the axes\nimport matplotlib.pyplot as plt # import dependencies\nfig, axes = plt.subplots() # create figure and set of axes\naxes.plot(x, y) # generate line\nplt.show() # print plot\n\n# create styled plot\nimport matplotlib.pyplot as plt # import matplotlib\nfig, axes = plt.subplots() # create figure and axes\naxes.plot(x, y, 'D-k') # create black line with diamond markers\naxes.set_xlabel('Linearly Spaced Numbers') # x label\naxes.set_ylabel('y Value') # y label\naxes.set_title('As x increases, y increases by x cubed') # set title\nplt.show() # print plot\n\n# call the plot object\nfig"""
Chapter02/Exercises/Exercise_20.py,1,"b""# Exercise 8: Multiple Line Plots Using Subplots\n\nimport numpy as np\nx = np.linspace(0, 10, 20) # create x\ny = x**3 # create y\ny2 = x**2 # create y2\n\n# create figure object with two subplots that are side-by-side\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows=1, ncols=2)\n\n# plot x on left axes\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows=1, ncols=2)\naxes[0].plot(x, y) # plot x squared by y\n\n# add title and axis labels for left axes\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows=1, ncols=2)\naxes[0].plot(x, y) # plot x squared by y\naxes[0].set_title('x by x Cubed') # set title\naxes[0].set_xlabel('Linearly Spaced Numbers') # set x axis label\naxes[0].set_ylabel('y Value') # set y axis label\n\n# style the right axes\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows=1, ncols=2)\naxes[0].plot(x, y) # plot x-cubed by x\naxes[0].set_title('x by x Cubed') # set title\naxes[0].set_xlabel('Linearly Spaced Numbers') # set x axis label\naxes[0].set_ylabel('y Value') # set y axis label\n# plot on the right axes\naxes[1].plot(x, y2) # plot x-squared by x\naxes[1].set_title('x by x Squared') # set title\naxes[1].set_xlabel('Linearly Spaced Numbers') # set x axis label\naxes[1].set_ylabel('y Value') # set y axis label\n\n# prevent plot overlap\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows=1, ncols=2)\naxes[0].plot(x, y) # plot x-cubed by x\naxes[0].set_title('x by x Cubed') # set title\naxes[0].set_xlabel('Linearly Spaced Numbers') # set x axis label\naxes[0].set_ylabel('y Value') # set y axis label\n# plot on the right axes\naxes[1].plot(x, y2) # plot x-squared by x\naxes[1].set_title('x by x Squared') # set title\naxes[1].set_xlabel('Linearly Spaced Numbers') # set x axis label\naxes[1].set_ylabel('y Value') # set y axis label\nplt.tight_layout() # prevent plot overlap\n\n# call the object\nfig"""
Chapter03/Activities/Activity_05.py,1,"b""# Activity 1: Generating predictions and evaluating performance of multiple linear regression model\n\n# continuing from Exercise 4:\n\n# generate predictions on the test data\npredictions = model.predict(X_test)\n\n# plot correlation of predicted and actual values\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nplt.scatter(y_test, predictions)\nplt.xlabel('Y Test (True Values)')\nplt.ylabel('Predicted Values')\nplt.title('Predicted vs. Actual Values (r = {0:0.2f})'.format(pearsonr(y_test, predictions)[0], 2))\nplt.show()\n\n# plot distribution of residuals\nimport seaborn as sns\nfrom scipy.stats import shapiro\nsns.distplot((y_test - predictions), bins = 50)\nplt.xlabel('Residuals')\nplt.ylabel('Density')\nplt.title('Histogram of Residuals (Shapiro W p-value = {0:0.3f})'.format(shapiro(y_test - predictions)[1]))\nplt.show()\n\n# compute metrics and put into a dataframe\nfrom sklearn import metrics\nimport numpy as np\nmetrics_df = pd.DataFrame({'Metric': ['MAE', \n                                      'MSE', \n                                      'RMSE', \n                                      'R-Squared'],\n                          'Value': [metrics.mean_absolute_error(y_test, predictions),\n                                    metrics.mean_squared_error(y_test, predictions),\n                                    np.sqrt(metrics.mean_squared_error(y_test, predictions)),\n                                    metrics.explained_variance_score(y_test, predictions)]}).round(3)\nprint(metrics_df)\n"""
Chapter03/Activities/Activity_06.py,2,"b""# Activity 2: Evaluating tuned model performance\n\n# continuing from Exercise 7:\n\n# generate predicted probabilities of yes\npredicted_prob = model.predict_proba(X_test)[:,1]\n\n# generate predicted classes\npredicted_class = model.predict(X_test)\n\n# evaluate performance with confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\ncm = pd.DataFrame(confusion_matrix(y_test, predicted_class))\ncm['Total'] = np.sum(cm, axis=1)\ncm = cm.append(np.sum(cm, axis=0), ignore_index=True)\ncm.columns = ['Predicted No', 'Predicted Yes', 'Total']\ncm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])\nprint(cm)\n\n# generate a classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, predicted_class))"""
Chapter03/Activities/Activity_07.py,2,"b""# Activity 3: Generating predictions and evaluating performance of grid search SVC model\n\n# continuing from Exercise 9:\n\n# generate predicted classes\npredicted_class = model.predict(X_test_scaled)\n\n# evaluate performance with confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\ncm = pd.DataFrame(confusion_matrix(y_test, predicted_class))\ncm['Total'] = np.sum(cm, axis=1)\ncm = cm.append(np.sum(cm, axis=0), ignore_index=True)\ncm.columns = ['Predicted No', 'Predicted Yes', 'Total']\ncm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])\nprint(cm)\n\n# generate a classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, predicted_class))"""
Chapter03/Activities/Activity_08.py,0,"b""# Activity 4: Prepare data for decision tree classifier\n\n# clear environment prior to running this code\n\n# import data\nimport pandas as pd\ndf = pd.read_csv('weather.csv')\n\n# dummy code 'Summary'\nimport pandas as pd\ndf_dummies = pd.get_dummies(df, drop_first=True)\n\n# shuffle df_dummies\nfrom sklearn.utils import shuffle\ndf_shuffled = shuffle(df_dummies, random_state=42)\n\n# split df_shuffled into X and y\nDV = 'Rain' # Save the DV as DV\nX = df_shuffled.drop(DV, axis=1) # get features (X)\ny = df_shuffled[DV] # get DV (y)\n\n# split X and y into testing and training data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# scale X_train and X_test\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n"""
Chapter03/Activities/Activity_09.py,2,"b""# Activity 5: Generating predictions and evaluating performance of decision tree classifier model\n\n# continuing from Exercise 11:\n\n# generate predicted probabilities of rain\npredicted_prob = model.predict_proba(X_test_scaled)[:,1]\n\n# generate predicted classes\npredicted_class = model.predict(X_test_scaled)\n\n# evaluate performance with confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\ncm = pd.DataFrame(confusion_matrix(y_test, predicted_class))\ncm['Total'] = np.sum(cm, axis=1)\ncm = cm.append(np.sum(cm, axis=0), ignore_index=True)\ncm.columns = ['Predicted No', 'Predicted Yes', 'Total']\ncm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])\nprint(cm)\n\n# generate a classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, predicted_class))\n\n\n"""
Chapter03/Activities/Activity_10.py,1,"b""# Activity 6: Tuning a random forest regressor\n\n# continuing from Exercise 12\n\n# Specify the hyperparameter space\nimport numpy as np\ngrid = {'criterion': ['mse','mae'],\n        'max_features': ['auto', 'sqrt', 'log2', None],\n        'min_impurity_decrease': np.linspace(0.0, 1.0, 10),\n        'bootstrap': [True, False],\n        'warm_start': [True, False]}\n\n# Instantiate the GridSearchCV model\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = GridSearchCV(RandomForestRegressor(), grid, scoring='explained_variance', cv=5)\n\n# Fit to the training set\nmodel.fit(X_train_scaled, y_train)\n\n# Print the tuned parameters\nbest_parameters = model.best_params_\nprint(best_parameters)\n\n\n"""
Chapter03/Activities/Activity_11.py,1,"b""# Activity 7: Generating predictions and evaluating performance of tuned random forest regressor model\n\n# continuing from Exercise 13:\n\n# generate predictions on the test data\npredictions = model.predict(X_test)\n\n# plot correlation of predicted and actual values\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nplt.scatter(y_test, predictions)\nplt.xlabel('Y Test (True Values)')\nplt.ylabel('Predicted Values')\nplt.title('Predicted vs. Actual Values (r = {0:0.2f})'.format(pearsonr(y_test, predictions)[0], 2))\nplt.show()\n\n# plot distribution of residuals\nimport seaborn as sns\nfrom scipy.stats import shapiro\nsns.distplot((y_test - predictions), bins = 50)\nplt.xlabel('Residuals')\nplt.ylabel('Density')\nplt.title('Histogram of Residuals (Shapiro W p-value = {0:0.3f})'.format(shapiro(y_test - predictions)[1]))\nplt.show()\n\n# compute metrics and put into a dataframe\nfrom sklearn import metrics\nimport numpy as np\nmetrics_df = pd.DataFrame({'Metric': ['MAE', \n                                      'MSE', \n                                      'RMSE', \n                                      'R-Squared'],\n                          'Value': [metrics.mean_absolute_error(y_test, predictions),\n                                    metrics.mean_squared_error(y_test, predictions),\n                                    np.sqrt(metrics.mean_squared_error(y_test, predictions)),\n                                    metrics.explained_variance_score(y_test, predictions)]}).round(3)\nprint(metrics_df)"""
Chapter03/Exercises/Exercise_21.py,0,"b""# Exercise 1: Preparing data for linear regression model\n\n# import data\nimport pandas as pd\ndf = pd.read_csv('weather.csv')\n\n# check info\ndf.info() \n\n# get number of levels in 'Summary'\nlevels = len(pd.value_counts(df['Description']))\nprint('There are {} levels in the Description column'.format(levels))\n\n# dummy code 'Summary'\nimport pandas as pd\ndf_dummies = pd.get_dummies(df, drop_first=True)\n\n# check info\nprint('There are {} columns in df_dummies'.format(df_dummies.shape[1]))\n\n# shuffle df_dummies\nfrom sklearn.utils import shuffle\ndf_shuffled = shuffle(df_dummies, random_state=42)\n\n# split df_shuffled into X and y\nDV = 'Temperature_c' # Save the DV as DV\nX = df_shuffled.drop(DV, axis=1) # get features (X)\ny = df_shuffled[DV] # get DV (y)\n\n# split X and y into testing and training data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"""
Chapter03/Exercises/Exercise_22.py,0,"b""# Exercise 2: Fitting a simple linear regression model and determining the intercept and coefficient\n\n# continuing from Exercise 1:\n\n# instantiate linear regression model\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\n\n# fit model to training data\nmodel.fit(X_train[['Humidity']], y_train)\n\n# extract the intercept\nintercept = model.intercept_\n\n# extract the coefficient\ncoefficient = model.coef_\n\n# print the formula\nprint('Temperature = {0:0.2f} + ({1:0.2f} x Humidity)'.format(intercept, coefficient[0]))\n"""
Chapter03/Exercises/Exercise_23.py,1,"b""# Exercise 3: Generating predictions and evaluating performance of simple linear regression model\n\n# continuing from Exercise 2: \n\n# generate predictions on the test data\npredictions = model.predict(X_test[['Humidity']])\n\n# plot correlation of predicted and actual values\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nplt.scatter(y_test, predictions)\nplt.xlabel('Y Test (True Values)')\nplt.ylabel('Predicted Values')\nplt.title('Predicted vs. Actual Values (r = {0:0.2f})'.format(pearsonr(y_test, predictions)[0], 2))\nplt.show()\n\n# plot distribution of residuals\nimport seaborn as sns\nfrom scipy.stats import shapiro\nsns.distplot((y_test - predictions), bins = 50)\nplt.xlabel('Residuals')\nplt.ylabel('Density')\nplt.title('Histogram of Residuals (Shapiro W p-value = {0:0.3f})'.format(shapiro(y_test - predictions)[1]))\nplt.show()\n\n# compute metrics and put into a dataframe\nfrom sklearn import metrics\nimport numpy as np\nmetrics_df = pd.DataFrame({'Metric': ['MAE', \n                                      'MSE', \n                                      'RMSE', \n                                      'R-Squared'],\n                          'Value': [metrics.mean_absolute_error(y_test, predictions),\n                                    metrics.mean_squared_error(y_test, predictions),\n                                    np.sqrt(metrics.mean_squared_error(y_test, predictions)),\n                                    metrics.explained_variance_score(y_test, predictions)]}).round(3)\nprint(metrics_df)\n\n\n\n"""
Chapter03/Exercises/Exercise_24.py,0,"b""# Exercise 4: Fitting a multiple linear regression model and determining the intercept and coefficient\n\n# continuing from Exercise 3:\n\n# instantiate linear regression model\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\n\n# fit model to training data\nmodel.fit(X_train, y_train)\n\n# extract the intercept\nintercept = model.intercept_\n\n# extract the coefficients\ncoefficients = model.coef_\n\n# print the formula\nprint('Temperature = {0:0.2f} + ({1:0.2f} x Humidity) + ({2:0.2f} x Wind Speed) + ({3:0.2f} x Wind Bearing Degrees) + ({4:0.2f} x Visibility) + ({5:0.2f} x Pressure) + ({6:0.2f} x Rain) + ({7:0.2f} x Normal Weather) + ({8:0.2f} x Warm Weather)'.format(intercept, \n                                                                                                                                                                                                                                                            coefficients[0],\n                                                                                                                                                                                                                                                            coefficients[1],\n                                                                                                                                                                                                                                                            coefficients[2],\n                                                                                                                                                                                                                                                            coefficients[3],\n                                                                                                                                                                                                                                                            coefficients[4],\n                                                                                                                                                                                                                                                            coefficients[5],\n                                                                                                                                                                                                                                                            coefficients[6],\n                                                                                                                                                                                                                                                            coefficients[7]))\n\n \n                                                                  """
Chapter03/Exercises/Exercise_25.py,0,"b""# Exercise 5: Fitting a logistic regression model and determining the intercept and coefficient\n\n# clear environment prior to running this code\n\n# import data\nimport pandas as pd\ndf = pd.read_csv('weather.csv')\n\n# dummy code 'Summary'\nimport pandas as pd\ndf_dummies = pd.get_dummies(df, drop_first=True)\n\n# shuffle df_dummies\nfrom sklearn.utils import shuffle\ndf_shuffled = shuffle(df_dummies, random_state=42)\n\n# split df_shuffled into X and y\nDV = 'Rain' # Save the DV as DV\nX = df_shuffled.drop(DV, axis=1) # get features (X)\ny = df_shuffled[DV] # get DV (y)\n\n# split X and y into testing and training data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) \n\n# instantiate logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\n\n# fit the model to the training data\nmodel.fit(X_train, y_train)\n\n# extract the intercept\nintercept = model.intercept_\n\n# extract the coefficients\ncoefficients = model.coef_\n\n# place coefficients in a list\ncoef_list = list(coefficients[0,:])\n \n# put coefficients in a df with feature name\ncoef_df = pd.DataFrame({'Feature': list(X_train.columns),\n                        'Coefficient': coef_list})\nprint(coef_df)\n\n\n\n"""
Chapter03/Exercises/Exercise_26.py,2,"b""# Exercise 6: Generating predictions and evaluating performance of logistic regression model\n\n# continuing from Exercise 5:\n\n# generate predicted probabilities of yes\npredicted_prob = model.predict_proba(X_test)[:,1]\n\n# generate predicted classes\npredicted_class = model.predict(X_test)\n\n# evaluate performance with confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\ncm = pd.DataFrame(confusion_matrix(y_test, predicted_class))\ncm['Total'] = np.sum(cm, axis=1)\ncm = cm.append(np.sum(cm, axis=0), ignore_index=True)\ncm.columns = ['Predicted No', 'Predicted Yes', 'Total']\ncm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])\nprint(cm)\n\n# generate a classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, predicted_class))\n\n"""
Chapter03/Exercises/Exercise_27.py,1,"b""# Exercise 7: Tuning hyperparameters of logistic regression model\n\n# continuing from Exercise 6:\n\n# instantiate a grid with the possible values for hyperparamters (see documentation)\nimport numpy as np\ngrid = {'penalty': ['l1', 'l2'],\n        'C': np.linspace(1, 10, 10)}\n\n# instantiate GridSearchCV model\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nmodel = GridSearchCV(LogisticRegression(solver='liblinear'), grid, scoring='f1', cv=5)\n\n# fit the gridsearch model\nmodel.fit(X_train, y_train)\n\n# print the best parameters\nbest_parameters = model.best_params_\nprint(best_parameters)\n\n"""
Chapter03/Exercises/Exercise_28.py,0,"b""# Exercise 8: Preparing data for support vector classifier\n\n# clear environment prior to running this code\n\n# import data\nimport pandas as pd\ndf = pd.read_csv('weather.csv')\n\n# dummy code 'Summary'\nimport pandas as pd\ndf_dummies = pd.get_dummies(df, drop_first=True)\n\n# shuffle df_dummies\nfrom sklearn.utils import shuffle\ndf_shuffled = shuffle(df_dummies, random_state=42)\n\n# split df_shuffled into X and y\nDV = 'Rain' # Save the DV as DV\nX = df_shuffled.drop(DV, axis=1) # get features (X)\ny = df_shuffled[DV] # get DV (y)\n\n# split X and y into testing and training data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# scale X_train and X_test\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler() # instantiate StandardScaler model\nX_train_scaled = scaler.fit_transform(X_train) # transform X_train to z-scores\nX_test_scaled = scaler.transform(X_test) # transform X_test to z-scores\n\n\n\n\n\n"""
Chapter03/Exercises/Exercise_29.py,1,"b""# Exercise 9: Tuning support vector classifier using grid search\n\n# continuing from exercise 8:\n\n# instantiate grid\nimport numpy as np\ngrid = {'C': np.linspace(1, 10, 10),\n        'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n\n# instantiate GridSearchCV model\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nmodel = GridSearchCV(SVC(gamma='auto'), grid, scoring='f1', cv=5)\n\n# fit the gridsearch model\nmodel.fit(X_train_scaled, y_train)\n\n# print the best parameters\nbest_parameters = model.best_params_\nprint(best_parameters)"""
Chapter03/Exercises/Exercise_30.py,2,"b""# Exercise 10: Tuning decision tree classifier using grid search in pipeline\n\n# continuing from Activity 4:\n\n# Specify the hyperparameter space\nimport numpy as np\ngrid = {'criterion': ['gini', 'entropy'],\n        'min_weight_fraction_leaf': np.linspace(0.0, 0.5, 10),\n        'min_impurity_decrease': np.linspace(0.0, 1.0, 10),\n        'class_weight': [None, 'balanced'],\n        'presort': [True, False]}\n\n# Instantiate the GridSearchCV model\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = GridSearchCV(DecisionTreeClassifier(), grid, scoring='f1', cv=5)\n\n# Fit to the training set\nmodel.fit(X_train_scaled, y_train)\n\n# Print the tuned parameters\nbest_parameters = model.best_params_\nprint(best_parameters)"""
Chapter03/Exercises/Exercise_31.py,0,"b""# Exercise 11: Programmatically extracting tuned hyperparameters from decision tree classifier grid search model\n\n# continuing from Exercise 10:\n\n# access the 'Tree__criterion' value\nprint(best_parameters['criterion'])\n\n# instantiate model\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(class_weight=best_parameters['class_weight'],\n                               criterion=best_parameters['criterion'],\n                               min_impurity_decrease=best_parameters['min_impurity_decrease'],\n                               min_weight_fraction_leaf=best_parameters['min_weight_fraction_leaf'],\n                               presort=best_parameters['presort'])\n\n# scale X_train and fit model\nmodel.fit(X_train_scaled, y_train)\n\n# extract feature_importances attribute\nprint(model.feature_importances_)\n\n# plot feature importance in descending order\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf_imp = pd.DataFrame({'Importance': list(model.feature_importances_)}, index=X.columns)\n# sort dataframe\ndf_imp_sorted = df_imp.sort_values(by=('Importance'), ascending=True)\n# plot these\ndf_imp_sorted.plot.barh(figsize=(5,5))\nplt.title('Relative Feature Importance')\nplt.xlabel('Relative Importance')\nplt.ylabel('Variable')\nplt.legend(loc=4)\nplt.show()\n\n\n\n\n\n\n"""
Chapter03/Exercises/Exercise_32.py,0,"b""# Exercise 12: Preparing data for random forest regressor\n\n# clear environment prior to running this code\n\n# import data\nimport pandas as pd\ndf = pd.read_csv('weather.csv')\n\n# dummy code 'Summary'\nimport pandas as pd\ndf_dummies = pd.get_dummies(df, drop_first=True)\n\n# shuffle df_dummies\nfrom sklearn.utils import shuffle\ndf_shuffled = shuffle(df_dummies, random_state=42)\n\n# split df_shuffled into X and y\nDV = 'Temperature_c' # Save the DV as DV\nX = df_shuffled.drop(DV, axis=1) # get features (X)\ny = df_shuffled[DV] # get DV (y)\n\n# split X and y into testing and training data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# scale X_train and X_test\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)"""
Chapter03/Exercises/Exercise_33.py,0,"b""# Exercise 13: Programmatically extracting tuned hyperparameters and determining feature importance from random forest regressor grid search model\n\n# continuing from Activity 6\n\n# instantiate model\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(criterion=best_parameters['criterion'],\n                              max_features=best_parameters['max_features'],\n                              min_impurity_decrease=best_parameters['min_impurity_decrease'],\n                              bootstrap=best_parameters['bootstrap'],\n                              warm_start=best_parameters['warm_start'])\n\n# fit model\nmodel.fit(X_train_scaled, y_train)\n\n# plot feature importance in descending order\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf_imp = pd.DataFrame({'Importance': list(model.feature_importances_)}, index=X.columns)\n# sort dataframe\ndf_imp_sorted = df_imp.sort_values(by=('Importance'), ascending=True)\n# plot these\ndf_imp_sorted.plot.barh(figsize=(5,5))\nplt.title('Relative Feature Importance')\nplt.xlabel('Relative Importance')\nplt.ylabel('Variable')\nplt.legend(loc=4)\nplt.show()\n\n\n\n\n\n"""
Chapter04/Activities/Activity_12.py,0,"b""# Activity 1: Ensemble k-Means Clustering \n\n# import data\nimport pandas as pd\ndf = pd.read_csv('glass.csv')\n\n# shuffle df\nfrom sklearn.utils import shuffle\ndf_shuffled = shuffle(df, random_state=42)\n\n# standardize\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler() # create StandardScaler() object\nscaled_features = scaler.fit_transform(df_shuffled) # fit scaler model and transform df_shuffled\n\n# instantiate an empty dataframe\nimport pandas as pd\nlabels_df = pd.DataFrame()\n\n# Build 100 models\nfrom sklearn.cluster import KMeans\nfor i in range(100):\n    model = KMeans(n_clusters=2)\n    model.fit(scaled_features) # fit model\n    labels = model.labels_ # get predicted labels\n    labels_df['Model_{}_Labels'.format(i+1)] = labels # put the labels into the empty df\n\n# calculate mode for each row\nrow_mode = labels_df.mode(axis=1)\n\n# assign the row_mode array as a column in labels_df\nlabels_df['row_mode'] = row_mode\n\n# preview the data\nprint(labels_df.head(5))"""
Chapter04/Activities/Activity_13.py,1,"b'# Activity 2: Evaluating Mean Inertia by Cluster After PCA Transformation\n\n# Continuing from Exercise 7:\n\nfrom sklearn.decomposition import PCA\nmodel = PCA(n_components=best_n_components) # remember, best_n_components = 6\n\n# fit model and transform scaled_features into best_n_components\ndf_pca = model.fit_transform(scaled_features)\n\n# fit 100 models for each n_clusters 1-10\nfrom sklearn.cluster import KMeans\nimport numpy as np\nmean_inertia_list_PCA = [] # create a list for the average inertia at each n_clusters\nfor x in range(1, 11): # loop through n_clusters 1-10\n    inertia_list = [] # create a list for each individual inertia value at n_cluster\n    for i in range(100):\n        model = KMeans(n_clusters=x) # instantiate model\n        model.fit(df_pca) # fit model\n        inertia = model.inertia_ # get inertia\n        inertia_list.append(inertia) # append inertia to inertia_list\n    # moving to the outside loop\n    mean_inertia = np.mean(inertia_list) # get mean of inertia list\n    mean_inertia_list_PCA.append(mean_inertia) # append mean_inertia to mean_inertia_list\n    \n# print mean_inertia_list_PCA\nprint(mean_inertia_list_PCA)  '"
Chapter04/Exercises/Exercise_34.py,0,"b""# Exercise 1: Building HCA Model \n\n# import data\nimport pandas as pd\ndf = pd.read_csv('glass.csv')\n\n# get df info\nprint(df.info())\n\n# shuffle df\nfrom sklearn.utils import shuffle\ndf_shuffled = shuffle(df, random_state=42)\n\n# standardize\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler() # instantiate scaler object\nscaled_features = scaler.fit_transform(df_shuffled) # fit and transform df_shuffled\n\n# create linkage model\nfrom scipy.cluster.hierarchy import linkage \nmodel = linkage(scaled_features, method='complete')\n\n\n\n"""
Chapter04/Exercises/Exercise_35.py,0,"b""# Exercise 2: Plotting HCA Model and Assigning Predictions\n\n# continuing from exercise 1:\n\n# plot dendrogram\nimport matplotlib.pyplot as plt \nfrom scipy.cluster.hierarchy import dendrogram\nplt.figure(figsize=(10,5))\nplt.title('Dendrogram for Glass Data')\ndendrogram(model,\n           leaf_rotation=90,\n           leaf_font_size=6)\nplt.show()\n\n# get labels\nfrom scipy.cluster.hierarchy import fcluster \nlabels = fcluster(model, t=9, criterion='distance')\nprint(labels)\n\n# assign labels array as a column in df_shuffled\ndf_shuffled['Predicted_Cluster'] = labels\n\n# preview data\nprint(df_shuffled.head(5))"""
Chapter04/Exercises/Exercise_36.py,0,"b""# Exercise 3: Fitting k-Means Model and Assigning Predictions\n\n# import data\nimport pandas as pd\ndf = pd.read_csv('glass.csv')\n\n# shuffle df\nfrom sklearn.utils import shuffle\ndf_shuffled = shuffle(df, random_state=42)\n\n# standardize\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler() # create StandardScaler() object\nscaled_features = scaler.fit_transform(df_shuffled) # fit scaler model and transform df_shuffled\n\n# instantiate kmeans model\nfrom sklearn.cluster import KMeans\nmodel = KMeans(n_clusters=2)\n\n# fit model\nmodel.fit(scaled_features)\n\n# get predicted labels\nlabels = model.labels_\n\n# see how many of each label we have\nimport pandas as pd\npd.value_counts(labels)\n\n# add label to df_shuffled\ndf_shuffled['Predicted_Cluster'] = labels\nprint(df_shuffled.head(5))\n\n"""
Chapter04/Exercises/Exercise_37.py,1,"b""# Exercise 4: Calculating Mean Inertia by n_clusters\n\n# import data\nimport pandas as pd\ndf = pd.read_csv('glass.csv')\n\n# shuffle df\nfrom sklearn.utils import shuffle\ndf_shuffled = shuffle(df, random_state=42)\n\n# standardize\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler() # create StandardScaler() object\nscaled_features = scaler.fit_transform(df_shuffled) # fit scaler model and transform df_shuffled\n\n# calculate mean inertia by n_clusters\nfrom sklearn.cluster import KMeans\nimport numpy as np\nmean_inertia_list = [] # create a list for the average inertia at each n_clusters\nfor x in range(1, 11): # loop through n_clusters 1-10\n    inertia_list = [] # create a list for each individual inertia value at n_cluster\n    for i in range(100):\n        model = KMeans(n_clusters=x) # instantiate model\n        model.fit(scaled_features) # fit model\n        inertia = model.inertia_ # get inertia\n        inertia_list.append(inertia) # append inertia to inertia_list\n    # moving to the outside loop\n    mean_inertia = np.mean(inertia_list) # get mean of inertia list\n    mean_inertia_list.append(mean_inertia) # append mean_inertia to mean_inertia_list\nprint(mean_inertia_list) """
Chapter04/Exercises/Exercise_38.py,0,"b""# Exercise 5: Plotting Mean Inertia by n_clusters\n\n# plot inertia by n_clusters\nimport matplotlib.pyplot as plt\nx = list(range(1, len(mean_inertia_list)+1))\ny = mean_inertia_list\nplt.plot(x, y)\nplt.title('Mean Inertia by n_clusters')\nplt.xlabel('n_clusters')\nplt.xticks(x)\nplt.ylabel('Mean Inertia')\nplt.show()\n\n"""
Chapter04/Exercises/Exercise_39.py,0,"b""# Exercise 6: Fitting PCA Model\n\n# import data\nimport pandas as pd\ndf = pd.read_csv('glass.csv')\n\n# shuffle df\nfrom sklearn.utils import shuffle\ndf_shuffled = shuffle(df, random_state=42)\n\n# standardize\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler() # instantiate scaler object\nscaled_features = scaler.fit_transform(df_shuffled) # fit and transform df_shuffled\n\n# instantiate PCA model\nfrom sklearn.decomposition import PCA\nmodel = PCA()\n\n# fit model\nmodel.fit(scaled_features)\n\n# get proportion of explained variance in each component\nexplained_var_ratio = model.explained_variance_ratio_\n\n# print the explained variance ratio\nprint(explained_var_ratio)\n"""
Chapter04/Exercises/Exercise_40.py,1,"b""# Exercise 7: Choosing n_components using Threshold\n\n# Continuing from Exercise 6:\n\n# get the cumulative sum of explained variance by each component\nimport numpy as np\ncum_sum_explained_var = np.cumsum(model.explained_variance_ratio_)\nprint(cum_sum_explained_var)\n\n# set a threshold for % of variance in the data to preserve\nthreshold = .95\nfor i in range(len(cum_sum_explained_var)):\n    if cum_sum_explained_var[i] >= threshold:\n        best_n_components = i+1\n        break\n    else:\n        pass\n    \n# print the best number of n_components\nprint('The best n_components is {}'.format(best_n_components))"""
Chapter04/Exercises/Exercise_41.py,0,"b""# Exercise 8: Visual Comparison of Inertia by n_clusters\n\n# Continuing from Activity 2\n\n# note: for this visualization to work properly, mean_inertia_list from exercise 5 must still be in the environment\n\n# plot inertia by n_clusters with both lines\nimport matplotlib.pyplot as plt\nx = list(range(1,len(mean_inertia_list_PCA)+1))\ny = mean_inertia_list_PCA\ny2 = mean_inertia_list \nplt.plot(x, y, label='PCA')\nplt.plot(x, y2, label='No PCA')\nplt.title('Mean Inertia by n_clusters for Original Features and PCA Transformed Features')\nplt.xlabel('n_clusters')\nplt.xticks(x)\nplt.ylabel('Inertia')\nplt.legend()\nplt.show()"""
Chapter04/Exercises/Exercise_42.py,0,"b""# Exercise 9: Fitting LDA Model\n\n# import data\nimport pandas as pd\ndf = pd.read_csv('glass_w_outcome.csv')\n\n# shuffle df\nfrom sklearn.utils import shuffle\ndf_shuffled = shuffle(df, random_state=42)\n\n# Save the DV as DV\nDV = 'Type'\n\n# Get X's and y\nX = df_shuffled.drop(DV, axis=1)\ny = df_shuffled[DV]\n\n# split into testing and training before transforming into its components\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# standardize X_train and X_test\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler() # create StandardScaler() object\nX_train_scaled = scaler.fit_transform(X_train) # fit scaler model and transform X_train\nX_test_scaled = scaler.transform(X_test) # transform X_test\n\n# instantiate LDA model\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nmodel = LinearDiscriminantAnalysis()\n\n# fit the model on the training data\nmodel.fit(X_train_scaled, y_train)\n\n# compute explained ratio by component\nmodel.explained_variance_ratio_"""
Chapter04/Exercises/Exercise_43.py,2,"b""# Exercise 10: Using LDA Transformed Components in Classification Model\n\n# Continuing from Exercise 9:\n\n# transform the training features to the training components\nX_train_LDA = model.transform(X_train_scaled) \n\n# transform the testing features to the testing components\nX_test_LDA = model.transform(X_test_scaled) \n\n# create a random forest model\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier() \n\n# fit the model on the training components\nmodel.fit(X_train_LDA, y_train) \n\n# generate predictions on the testing components\npredictions = model.predict(X_test_LDA) \n\n# style the confusion matrix\nfrom sklearn.metrics import confusion_matrix \nimport pandas as pd\nimport numpy as np\ncm = pd.DataFrame(confusion_matrix(y_test, predictions))\ncm['Total'] = np.sum(cm, axis=1)\ncm = cm.append(np.sum(cm, axis=0), ignore_index=True)\ncm.columns = ['Predicted 1', 'Predicted 2', 'Predicted 3', 'Total']\ncm = cm.set_index([['Actual 1', 'Actual 2', 'Actual 3', 'Total']])\nprint(cm)\n\n# to get the accuracy score\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, predictions)\n"""
