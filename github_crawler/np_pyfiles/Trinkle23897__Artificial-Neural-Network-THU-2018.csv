file_path,api_count,code
hw3/draw.py,34,"b'import numpy as np\nfrom pylab import *\n\ndef plot_loss_each_iter():\n\t# loss value each iter\n\tD = 10\n\ttrain_loss_cnn = np.load(\'cnn/result/d5/train_loss_iter.npy\').reshape(D, -1).mean(axis=0)\n\tvalid_loss_cnn = np.load(\'cnn/result/d5/valid_loss_iter.npy\').reshape(D, -1).mean(axis=0)\n\ttrain_loss_mlp = np.load(\'mlp/result/d5/train_loss_iter.npy\').reshape(D, -1).mean(axis=0)\n\tvalid_loss_mlp = np.load(\'mlp/result/d5/valid_loss_iter.npy\').reshape(D, -1).mean(axis=0)\n\titer_ = np.arange(train_loss_cnn.shape[0] * D)\n\tfigure()\n\tp = subplot(111)\n\tp.plot(iter_[::D], train_loss_cnn, \'-\', label=\'CNN train loss\')\n\tp.plot(iter_[::5*D], valid_loss_cnn, \'--\', label=\'CNN valid loss\')\n\tp.plot(iter_[::D], train_loss_mlp, \'-\', label=\'MLP train loss\')\n\tp.plot(iter_[::5*D], valid_loss_mlp, \'--\', label=\'MLP valid loss\')\n\tp.set_ylim((0, .6))\n\tp.set_xlabel(r\'# of Iterations\')\n\tp.set_ylabel(r\'Loss\')\n\tp.legend(loc=\'upper right\')\n\ttight_layout()\n\tsavefig(""loss_iter.pdf"")\n\ndef plot_comp(name, loss_lim, acc_lim):\n\t# full version comparison of MLP and CNN\n\ttrain_loss_cnn = np.load(\'cnn/result/%s/train_loss.npy\' % name)\n\ttrain_loss_mlp = np.load(\'mlp/result/%s/train_loss.npy\' % name)\n\tvalid_loss_cnn = np.load(\'cnn/result/%s/val_loss.npy\' % name)\n\tvalid_loss_mlp = np.load(\'mlp/result/%s/val_loss.npy\' % name)\n\titer_ = np.arange(train_loss_cnn.shape[0])+1\n\tfigure()\n\tp = subplot(111)\n\tp.plot(iter_, train_loss_cnn, \'-\', label=\'CNN train loss\')\n\tp.plot(iter_, valid_loss_cnn, \'--\', label=\'CNN valid loss\')\n\tp.plot(iter_, train_loss_mlp, \'-\', label=\'MLP train loss\')\n\tp.plot(iter_, valid_loss_mlp, \'--\', label=\'MLP valid loss\')\n\tp.set_xlim((1, 20))\n\tp.set_ylim((0, loss_lim))\n\tp.set_xlabel(r\'# of Epochs\')\n\tp.set_ylabel(r\'Loss\')\n\tp.legend(loc=\'upper right\')\n\ttight_layout()\n\tsavefig(""loss_%s.pdf"" % name)\n\ttrain_acc_cnn = np.load(\'cnn/result/%s/train_acc.npy\' % name)\n\ttrain_acc_mlp = np.load(\'mlp/result/%s/train_acc.npy\' % name)\n\tvalid_acc_cnn = np.load(\'cnn/result/%s/val_acc.npy\' % name)\n\tvalid_acc_mlp = np.load(\'mlp/result/%s/val_acc.npy\' % name)\n\tfigure()\n\tp = subplot(111)\n\tp.plot(iter_, train_acc_cnn, \'-\', label=\'CNN train acc\')\n\tp.plot(iter_, valid_acc_cnn, \'--\', label=\'CNN valid acc\')\n\tp.plot(iter_, train_acc_mlp, \'-\', label=\'MLP train acc\')\n\tp.plot(iter_, valid_acc_mlp, \'--\', label=\'MLP valid acc\')\n\tp.set_xlim((1, 20))\n\tp.set_ylim((acc_lim, 1))\n\tp.set_xlabel(r\'# of Epochs\')\n\tp.set_ylabel(r\'Accuracy\')\n\tp.legend(loc=\'lower right\')\n\ttight_layout()\n\tsavefig(""acc_%s.pdf"" % name)\n\ndef plot_comp2(name=\'d5\', name2=\'nobn\'):\n\t# full version comparison of MLP and CNN\n\tfigure()\n\tp = subplot(111)\n\ttrain_loss_cnn = np.load(\'cnn/result/%s/train_loss.npy\' % name)\n\tvalid_loss_cnn = np.load(\'cnn/result/%s/val_loss.npy\' % name)\n\titer_ = np.arange(train_loss_cnn.shape[0])+1\n\tp.plot(iter_, train_loss_cnn, \'-\', label=\'%s train loss\' % \'BN\')\n\tp.plot(iter_, valid_loss_cnn, \'--\', label=\'%s valid loss\' % \'BN\')\n\ttrain_loss_cnn = np.load(\'cnn/result/%s/train_loss.npy\' % name2)\n\tvalid_loss_cnn = np.load(\'cnn/result/%s/val_loss.npy\' % name2)\n\tp.plot(iter_, train_loss_cnn, \'-\', label=\'%s train loss\' % \'no BN\')\n\tp.plot(iter_, valid_loss_cnn, \'--\', label=\'%s valid loss\' % \'no BN\')\n\tp.set_xlim((1, 20))\n\tp.set_ylim((0, .1))\n\tp.set_xlabel(r\'# of Epochs\')\n\tp.set_ylabel(r\'Loss\')\n\tp.legend(loc=\'upper right\')\n\ttight_layout()\n\tsavefig(""loss_comp_CNN_%s_%s.pdf"" % (name, name2))\n\n\tfigure()\n\tp = subplot(111)\n\ttrain_loss_mlp = np.load(\'mlp/result/%s/train_loss.npy\' % name)\n\tvalid_loss_mlp = np.load(\'mlp/result/%s/val_loss.npy\' % name)\n\tp.plot(iter_, train_loss_mlp, \'-\', label=\'BN train loss\')\n\tp.plot(iter_, valid_loss_mlp, \'--\', label=\'BN valid loss\')\n\ttrain_loss_mlp = np.load(\'mlp/result/%s/train_loss.npy\' % name2)\n\tvalid_loss_mlp = np.load(\'mlp/result/%s/val_loss.npy\' % name2)\n\tp.plot(iter_, train_loss_mlp, \'-\', label=\'no BN train loss\')\n\tp.plot(iter_, valid_loss_mlp, \'--\', label=\'no BN valid loss\')\n\tp.set_xlim((1, 20))\n\tp.set_ylim((0, .8))\n\tp.set_xlabel(r\'# of Epochs\')\n\tp.set_ylabel(r\'Loss\')\n\tp.legend(loc=\'upper right\')\n\ttight_layout()\n\tsavefig(""loss_comp_MLP_%s_%s.pdf"" % (name, name2))\n\n\n\n\tfigure()\n\tp = subplot(111)\n\ttrain_acc_cnn = np.load(\'cnn/result/%s/train_acc.npy\' % name)\n\tvalid_acc_cnn = np.load(\'cnn/result/%s/val_acc.npy\' % name)\n\tp.plot(iter_, train_acc_cnn, \'-\', label=\'BN train acc\')\n\tp.plot(iter_, valid_acc_cnn, \'--\', label=\'BN valid acc\')\n\ttrain_acc_cnn = np.load(\'cnn/result/%s/train_acc.npy\' % name2)\n\tvalid_acc_cnn = np.load(\'cnn/result/%s/val_acc.npy\' % name2)\n\tp.plot(iter_, train_acc_cnn, \'-\', label=\'no BN train acc\')\n\tp.plot(iter_, valid_acc_cnn, \'--\', label=\'no BN valid acc\')\n\tp.set_xlim((1, 20))\n\tp.set_ylim((.97, 1))\n\tp.set_xlabel(r\'# of Epochs\')\n\tp.set_ylabel(r\'Accuracy\')\n\tp.legend(loc=\'lower right\')\n\ttight_layout()\n\tsavefig(""acc_comp_CNN_%s_%s.pdf"" % (name, name2))\n\n\tfigure()\n\tp = subplot(111)\n\ttrain_acc_mlp = np.load(\'mlp/result/%s/train_acc.npy\' % name)\n\tvalid_acc_mlp = np.load(\'mlp/result/%s/val_acc.npy\' % name)\n\tp.plot(iter_, train_acc_mlp, \'-\', label=\'BN train acc\')\n\tp.plot(iter_, valid_acc_mlp, \'--\', label=\'BN valid acc\')\n\ttrain_acc_mlp = np.load(\'mlp/result/%s/train_acc.npy\' % name2)\n\tvalid_acc_mlp = np.load(\'mlp/result/%s/val_acc.npy\' % name2)\n\tp.plot(iter_, train_acc_mlp, \'-\', label=\'no BN train acc\')\n\tp.plot(iter_, valid_acc_mlp, \'--\', label=\'no BN valid acc\')\n\tp.set_xlim((1, 20))\n\tp.set_ylim((.75, 1))\n\tp.set_xlabel(r\'# of Epochs\')\n\tp.set_ylabel(r\'Accuracy\')\n\tp.legend(loc=\'lower right\')\n\ttight_layout()\n\tsavefig(""acc_comp_MLP_%s_%s.pdf"" % (name, name2))\n\ndef plot_drop():\n\ttrain_loss_cnn = np.load(\'cnn/result/d5/train_loss.npy\')\n\titer_ = np.arange(train_loss_cnn.shape[0])+1\n\tfor model in [\'mlp\', \'cnn\']:\n\t\tfor metric in [\'acc\', \'loss\']:\n\t\t\tfigure()\n\t\t\tp = subplot(111)\n\t\t\tfor name in [\'d1\', \'d3\', \'d5\', \'d7\', \'d9\']:\n\t\t\t\tfor face in [\'train\', \'val\']:\n\t\t\t\t\tresult = np.load(\'%s/result/%s/%s_%s.npy\' % (model, name, face, metric))\n\t\t\t\t\tp.plot(iter_, result, \'-\' if face == \'train\' else \'--\', label=\'%s %s %s\' % (name, face, metric))\n\n\t\t\tp.set_xlim((1, 20))\n\t\t\tif metric == \'acc\':\n\t\t\t\tp.set_ylim((.4 if model == \'mlp\' else .93, 1))\n\t\t\t\tp.set_ylabel(r\'Accuracy\')\n\t\t\t\tp.legend(loc=\'lower right\')\n\t\t\telse:\n\t\t\t\tp.set_ylim((0, 1.35 if model == \'mlp\' else .5))\n\t\t\t\tp.set_ylabel(r\'Loss\')\n\t\t\t\tp.legend(loc=\'upper right\')\n\t\t\tp.set_xlabel(r\'# of Epochs\')\n\t\t\ttight_layout()\n\t\t\tsavefig(""drop_%s_%s.pdf"" % (model, metric))\n\t\n\nif __name__ == \'__main__\':\n \tplot_loss_each_iter()\n \tplot_comp(\'d5\', .6, .87)\n \tplot_comp2()\n \t# plot_drop()'"
hw1/codes/layers.py,7,"b""import numpy as np\n\n\nclass Layer(object):\n    def __init__(self, name, trainable=False):\n        self.name = name\n        self.trainable = trainable\n        self._saved_tensor = None\n\n    def forward(self, input):\n        pass\n\n    def backward(self, grad_output):\n        pass\n\n    def update(self, config):\n        pass\n\n    def _saved_for_backward(self, tensor):\n        '''The intermediate results computed during forward stage\n        can be saved and reused for backward, for saving computation'''\n\n        self._saved_tensor = tensor\n\n\nclass Relu(Layer):\n    def __init__(self, name):\n        super(Relu, self).__init__(name)\n\n    def forward(self, input):\n        '''Your codes here'''\n        self._saved_for_backward(input)\n        input[input <= 0] = 0\n        return input\n\n    def backward(self, grad_output):\n        '''Your codes here'''\n        grad_output[self._saved_tensor <= 0] = 0\n        return grad_output\n\n\nclass Sigmoid(Layer):\n    def __init__(self, name):\n        super(Sigmoid, self).__init__(name)\n\n    def forward(self, input):\n        '''Your codes here'''\n        self._saved_for_backward(1. / (1. + np.exp(-input)))\n        return self._saved_tensor\n\n    def backward(self, grad_output):\n        '''Your codes here'''\n        return self._saved_tensor * (1. - self._saved_tensor) * grad_output\n\n\nclass Linear(Layer):\n    def __init__(self, name, in_num, out_num, init_std):\n        super(Linear, self).__init__(name, trainable=True)\n        self.in_num = in_num\n        self.out_num = out_num\n        self.W = np.random.randn(in_num, out_num) * init_std\n        self.b = np.zeros(out_num)\n\n        self.grad_W = np.zeros((in_num, out_num))\n        self.grad_b = np.zeros(out_num)\n\n        self.diff_W = np.zeros((in_num, out_num))\n        self.diff_b = np.zeros(out_num)\n\n    def forward(self, input):\n        '''Your codes here'''\n        self._saved_for_backward(input)\n        return input.dot(self.W) + self.b\n\n    def backward(self, grad_output):\n        '''Your codes here'''\n        self.grad_W = -self._saved_tensor.T.dot(grad_output) # 784x10 = 784x100 * 100x10\n        self.grad_b = -grad_output.sum(axis=0)\n        return grad_output.dot(self.W.T)\n\n    def update(self, config):\n        mm = config['momentum']\n        lr = config['learning_rate']\n        wd = config['weight_decay']\n\n        # self.diff_W = mm * self.diff_W - lr * (self.grad_W + wd * self.W)\n        # self.W = self.W + self.diff_W\n\n        # self.diff_b = mm * self.diff_b - lr * (self.grad_b + wd * self.b)\n        # self.b = self.b + self.diff_b\n        self.diff_W = mm * self.diff_W + (self.grad_W + wd * self.W)\n        self.W = self.W - lr * self.diff_W\n\n        self.diff_b = mm * self.diff_b + (self.grad_b + wd * self.b)\n        self.b = self.b - lr * self.diff_b\n"""
hw1/codes/load_data.py,4,"b""import numpy as np\nimport os\n\n\ndef load_mnist_2d(data_dir):\n    fd = open(os.path.join(data_dir, 'train-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    trX = loaded[16:].reshape((60000, 28 * 28)).astype(float)\n\n    fd = open(os.path.join(data_dir, 'train-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    trY = loaded[8:].reshape((60000))\n\n    fd = open(os.path.join(data_dir, 't10k-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    teX = loaded[16:].reshape((10000, 28 * 28)).astype(float)\n\n    fd = open(os.path.join(data_dir, 't10k-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    teY = loaded[8:].reshape((10000))\n\n    trX = (trX - 128.0) / 255.0\n    teX = (teX - 128.0) / 255.0\n\n    return trX, teX, trY, teY"""
hw1/codes/loss.py,0,"b""from __future__ import division\nimport numpy as np\n\n\nclass EuclideanLoss(object):\n    def __init__(self, name):\n        self.name = name\n\n    def forward(self, input, target):\n        '''Your codes here'''\n        return ((target - input) ** 2).mean(axis=0).sum() / 2.\n\n    def backward(self, input, target):\n        '''Your codes here'''\n        return target - input\n"""
hw1/codes/network.py,0,"b'class Network(object):\n    def __init__(self):\n        self.layer_list = []\n        self.params = []\n        self.num_layers = 0\n\n    def add(self, layer):\n        self.layer_list.append(layer)\n        self.num_layers += 1\n\n    def forward(self, input):\n        output = input\n        for i in range(self.num_layers):\n            output = self.layer_list[i].forward(output)\n\n        return output\n\n    def backward(self, grad_output):\n        grad_input = grad_output\n        for i in range(self.num_layers - 1, -1, -1):\n            grad_input = self.layer_list[i].backward(grad_input)\n\n    def update(self, config):\n        for i in range(self.num_layers):\n            if self.layer_list[i].trainable:\n                self.layer_list[i].update(config)\n'"
hw1/codes/plot.py,11,"b'import numpy as np\nfrom pylab import *\n\nacc1 = np.load(\'acc1.npy\')\nloss1 = np.load(\'loss1.npy\')*10\nacc2s = np.load(\'acc2s.npy\')\nloss2s = np.load(\'loss2s.npy\')*10\nacc2r = np.load(\'acc2r.npy\')\nloss2r = np.load(\'loss2r.npy\')*10\nacc3s = np.load(\'acc3s.npy\')\nloss3s = np.load(\'loss3s.npy\')*10\nacc3r = np.load(\'acc3r.npy\')\nloss3r = np.load(\'loss3r.npy\')*10\nprint(\' 1: %.2f %.6f\'%(100*acc1.max(), loss1.min()))\nprint(\'2s: %.2f %.6f\'%(100*acc2s.max(), loss2s.min()))\nprint(\'2r: %.2f %.6f\'%(100*acc2r.max(), loss2r.min()))\nprint(\'3s: %.2f %.6f\'%(100*acc3s.max(), loss3s.min()))\nprint(\'3r: %.2f %.6f\'%(100*acc3r.max(), loss3r.min()))\niter_ = np.arange(acc1.shape[0]) * 50\nprint(acc1.shape[0])\nfigure()\np = subplot(111)\np.plot(iter_, loss1, \'-\', label=\'0-layer\')\np.plot(iter_, loss2s, \'-\', label=\'1-layer + Sigmoid\')\np.plot(iter_, loss2r, \'-\', label=\'1-layer + ReLU\')\np.plot(iter_, loss3s, \'-\', label=\'2-layer + Sigmoid\')\np.plot(iter_, loss3r, \'-\', label=\'2-layer + ReLU\')\np.set_xlabel(r\'# of Iterations\')\np.set_ylabel(r\'Loss\')\np.legend(loc=\'upper right\')\ntight_layout()\nsavefig(""loss.pdf"")\nfigure()\np = subplot(111)\np.plot(iter_, acc1, \'-\', label=\'0-layer\')\np.plot(iter_, acc2s, \'-\', label=\'1-layer + Sigmoid\')\np.plot(iter_, acc2r, \'-\', label=\'1-layer + ReLU\')\np.plot(iter_, acc3s, \'-\', label=\'2-layer + Sigmoid\')\np.plot(iter_, acc3r, \'-\', label=\'2-layer + ReLU\')\np.set_xlabel(r\'# of Iterations\')\np.set_ylabel(r\'Accuracy\')\np.legend(loc=\'lower right\')\ntight_layout()\nsavefig(""acc.pdf"")\n\n#  1: 23:24:44.414     Testing, total mean loss 0.019417, total acc 0.863300 - 23:24:33.131\n# 2s: 20:20:39.807     Testing, total mean loss 0.003224, total acc 0.967700 - 20:18:21.597\n# 2r: 20:48:01.448     Testing, total mean loss 0.002306, total acc 0.981300 - 20:45:16.709\n#-2r: 20:38:47.940     Testing, total mean loss 0.002271, total acc 0.981500 - 20:35:59.910\n# 3s: 00:38:10.865     Testing, total mean loss 0.001759, total acc 0.980098 - 00:33:01.622\n# 3r: 21:24:04.253     Testing, total mean loss 0.001675, total acc 0.980588 - 21:19:28.262'"
hw1/codes/run_mlp.py,2,"b""from network import Network\nfrom utils import LOG_INFO\nfrom layers import Relu, Sigmoid, Linear\nfrom loss import EuclideanLoss\nfrom solve_net import train_net, test_net\nfrom load_data import load_mnist_2d\n\n\ntrain_data, test_data, train_label, test_label = load_mnist_2d('data')\n\n# Your model defintion here\n# You should explore different model architecture\n# model = Network()\n# model.add(Linear('fc1', 784, 10, 0.001))\n\nloss = EuclideanLoss(name='loss')\n\n# Training configuration\n# You should adjust these hyperparameters\n# NOTE: one iteration means model forward-backwards one batch of samples.\n#       one epoch means model has gone through all the training samples.\n#       'disp_freq' denotes number of iterations in one epoch to display information.\n\n# config = {\n#     'learning_rate': 0.000001,\n#     'weight_decay': 0.005,\n#     'momentum': 0.9,\n#     'batch_size': 100,\n#     'max_epoch': 100,\n#     'disp_freq': 50,\n#     'test_epoch': 5\n# }\n\ndef one_layer_net():\n    model = Network()\n    model.add(Linear('fc1', 784, 10, 0.001))\n    config = {\n        'learning_rate': 0.00001,\n        'weight_decay': 0.005,\n        'momentum': 0.9,\n        'batch_size': 50,\n        'max_epoch': 10,\n        'disp_freq': 50,\n        'test_epoch': 5\n    }\n    return model, config\n\ndef two_layer_sigmoid():\n    model = Network()\n    model.add(Linear('fc1', 784, 256, 0.001))\n    model.add(Sigmoid('sg1'))\n    model.add(Linear('fc2', 256, 10, 0.001))\n    model.add(Sigmoid('sg2'))\n    config = {\n        'learning_rate': 0.01,\n        'weight_decay': 0.005,\n        'momentum': 0.9,\n        'batch_size': 100,\n        'max_epoch': 20,\n        'disp_freq': 50,\n        'test_epoch': 5\n    }\n    return model, config\n\ndef two_layer_relu():\n    model = Network()\n    model.add(Linear('fc1', 784, 256, 0.001))\n    model.add(Relu('rl1'))\n    model.add(Linear('fc2', 256, 10, 0.001))\n    model.add(Relu('rl2'))\n    config = {\n        'learning_rate': 0.0001,\n        'weight_decay': 0.005,\n        'momentum': 0.9,\n        'batch_size': 200,\n        'max_epoch': 40,\n        'disp_freq': 50,\n        'test_epoch': 5\n    }\n    return model, config\n\ndef three_layer_sigmoid():\n    model = Network()\n    model.add(Linear('fc1', 784, 256, 0.001))\n    model.add(Sigmoid('sg1'))\n    model.add(Linear('fc2', 256, 128, 0.001))\n    model.add(Sigmoid('sg2'))\n    model.add(Linear('fc3', 128, 10, 0.001))\n    model.add(Sigmoid('sg3'))\n    config = {\n        'learning_rate': 0.01,\n        'weight_decay': 0.005,\n        'momentum': 0.9,\n        'batch_size': 300,\n        'max_epoch': 60,\n        'disp_freq': 50,\n        'test_epoch': 5\n    }\n    return model, config\n\ndef three_layer_relu():\n    model = Network()\n    model.add(Linear('fc1', 784, 256, 0.001))\n    model.add(Relu('rl1'))\n    model.add(Linear('fc2', 256, 128, 0.001))\n    model.add(Relu('rl2'))\n    model.add(Linear('fc3', 128, 10, 0.001))\n    model.add(Relu('rl3'))\n    config = {\n        'learning_rate': 0.0001,\n        'weight_decay': 0.005,\n        'momentum': 0.9,\n        'batch_size': 300,\n        'max_epoch': 60,\n        'disp_freq': 50,\n        'test_epoch': 5\n    }\n    return model, config\n\nmodel, config = one_layer_net()\n\nloss_, acc_ = [], []\n\nfor epoch in range(config['max_epoch']):\n    LOG_INFO('Training @ %d epoch...' % (epoch))\n    a, b = train_net(model, loss, config, train_data, train_label, config['batch_size'], config['disp_freq'])\n    loss_ += a\n    acc_ += b\n    if epoch % config['test_epoch'] == 0:\n        LOG_INFO('Testing @ %d epoch...' % (epoch))\n        test_net(model, loss, test_data, test_label, config['batch_size'])\ntest_net(model, loss, test_data, test_label, config['batch_size'])\n\nimport numpy as np\nnp.save('loss', loss_)\nnp.save('acc', acc_)"""
hw1/codes/solve_net.py,7,"b""from utils import LOG_INFO, onehot_encoding, calculate_acc\nimport numpy as np\n\n\ndef data_iterator(x, y, batch_size, shuffle=True):\n    indx = list(range(len(x)))\n    if shuffle:\n        np.random.shuffle(indx)\n\n    for start_idx in range(0, len(x), batch_size):\n        end_idx = min(start_idx + batch_size, len(x))\n        yield x[indx[start_idx: end_idx]], y[indx[start_idx: end_idx]]\n\n\ndef train_net(model, loss, config, inputs, labels, batch_size, disp_freq):\n\n    iter_counter = 0\n    loss_list = []\n    acc_list = []\n    loss_ = []\n    acc_ = []\n    for input, label in data_iterator(inputs, labels, batch_size):\n        target = onehot_encoding(label, 10)\n        iter_counter += 1\n\n        # forward net\n        output = model.forward(input)\n        # calculate loss\n        loss_value = loss.forward(output, target)\n        # generate gradient w.r.t loss\n        grad = loss.backward(output, target)\n        # backward gradient\n\n        model.backward(grad)\n        # update layers' weights\n        model.update(config)\n\n        acc_value = calculate_acc(output, label)\n        loss_list.append(loss_value)\n        acc_list.append(acc_value)\n\n        if iter_counter % disp_freq == 0:\n            msg = '  Training iter %d, batch loss %.6f, batch acc %.6f' % (iter_counter, np.mean(loss_list), np.mean(acc_list))\n            loss_.append(np.mean(loss_list))\n            acc_.append(np.mean(acc_list))\n            loss_list = []\n            acc_list = []\n            LOG_INFO(msg)\n    return loss_, acc_\n    # np.save('loss', loss_)\n    # np.save('acc', acc_)\n\ndef test_net(model, loss, inputs, labels, batch_size):\n    loss_list = []\n    acc_list = []\n\n    for input, label in data_iterator(inputs, labels, batch_size, shuffle=False):\n        target = onehot_encoding(label, 10)\n        output = model.forward(input)\n        loss_value = loss.forward(output, target)\n        acc_value = calculate_acc(output, label)\n        loss_list.append(loss_value)\n        acc_list.append(acc_value)\n\n    msg = '    Testing, total mean loss %.6f, total acc %.6f' % (np.mean(loss_list), np.mean(acc_list))\n    LOG_INFO(msg)\n"""
hw1/codes/utils.py,2,"b""from __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom datetime import datetime\n\n\ndef onehot_encoding(label, max_num_class):\n    encoding = np.eye(max_num_class)\n    encoding = encoding[label]\n    return encoding\n\n\ndef calculate_acc(output, label):\n    correct = np.sum(np.argmax(output, axis=1) == label)\n    return correct / len(label)\n\n\ndef LOG_INFO(msg):\n    now = datetime.now()\n    display_now = str(now).split(' ')[1][:-3]\n    print(display_now + ' ' + msg)\n"""
hw2/codes/functions.py,8,"b""import numpy as np\n\ndef im2col(input, kh, kw, stride=1):\n    '''\n    input: N * C * H * W with pad\n    out: C*kh*kw * N*hout*wout\n    '''\n    N, C, H, W = input.shape\n    hout = (H - kh) / stride + 1\n    wout = (W - kw) / stride + 1\n    shapes = (C, kh, kw, N, hout, wout)\n    strides = input.itemsize * np.array([H * W, W, 1, C * H * W, W * stride, stride])\n    output = np.lib.stride_tricks.as_strided(input, shape=shapes, strides=strides)\n    return output.reshape(C * kh * kw, N * hout * wout)\n\ndef im2col_conv(input, w, b=None): # with pad\n    N, C, H, W = input.shape\n    cout, _, kh, kw = w.shape\n    col_input = im2col(input, kh, kw)\n    col_out = w.reshape(cout, -1).dot(col_input)\n    if b is not None:\n        col_out += b.reshape(-1, 1)\n    hout = H - kh + 1\n    wout = W - kw + 1\n    return col_out.reshape(cout, N, hout, wout).transpose((1, 0, 2, 3))\n\ndef conv2d_forward(input, w, b, kernel_size, pad):\n    '''\n    Args:\n        input: shape = n (#sample) x c_in (#input channel) x h_in (#height) x w_in (#width)\n        W: weight, shape = c_out (#output channel) x c_in (#input channel) x k (#kernel_size) x k (#kernel_size)\n        b: bias, shape = c_out\n        kernel_size: size of the convolving kernel (or filter)\n        pad: number of zero added to both sides of input\n\n    Returns:\n        output: shape = n (#sample) x c_out (#output channel) x h_out x w_out,\n            where h_out, w_out is the height and width of output, after convolution\n    '''\n    input = np.pad(input, ((0, 0), (0, 0), (pad, pad), (pad, pad)), 'constant')\n    return im2col_conv(input, w, b)\n\ndef conv2d_backward(input, grad_output, w, b, kernel_size, pad):\n    '''\n    Args:\n        input: shape = n (#sample) x c_in (#input channel) x h_in (#height) x w_in (#width)\n        grad_output: shape = n (#sample) x c_out (#output channel) x h_out x w_out\n        W: weight, shape = c_out (#output channel) x c_in (#input channel) x k (#kernel_size) x k (#kernel_size)\n        b: bias, shape = c_out\n        kernel_size: size of the convolving kernel (or filter)\n        pad: number of zero added to both sides of input\n\n    Returns:\n        grad_input: gradient of input, shape = n (#sample) x c_in (#input channel) x h_in (#height) x w_in (#width)\n        grad_W: gradient of W, shape = c_out (#output channel) x c_in (#input channel) x k (#kernel_size) x k (#kernel_size)\n        grad_b: gradient of b, shape = c_out\n    '''\n    input = np.pad(input, ((0, 0), (0, 0), (pad, pad), (pad, pad)), 'constant')\n    grad_b = grad_output.sum(axis=(0, 2, 3))\n    grad_w = im2col_conv(input.transpose((1, 0, 2, 3)), grad_output.transpose((1, 0, 2, 3))).transpose((1, 0, 2, 3))\n    grad_out = np.pad(grad_output, ((0, 0), (0, 0), (kernel_size - 1, kernel_size - 1), (kernel_size - 1, kernel_size - 1)), 'constant')\n    grad_w_loc = np.rot90(w.transpose((1, 0, 2, 3)), 2, axes=(2, 3))\n    grad_input = im2col_conv(grad_out, grad_w_loc)\n    if pad != 0:\n        grad_input = grad_input[:, :, pad:-pad, pad:-pad]\n    return grad_input, grad_w, grad_b\n\ndef avgpool2d_forward(input, kernel_size, pad):\n    '''\n    Args:\n        input: shape = n (#sample) x c_in (#input channel) x h_in (#height) x w_in (#width)\n        kernel_size: size of the window to take average over\n        pad: number of zero added to both sides of input\n\n    Returns:\n        output: shape = n (#sample) x c_in (#input channel) x h_out x w_out,\n            where h_out, w_out is the height and width of output, after average pooling over input\n    '''\n    if kernel_size == 2 and pad == 0:\n        out = (input[..., ::2, ::2] + input[..., ::2, 1::2] + input[..., 1::2, ::2] + input[..., 1::2, 1::2]) * .25\n        return out\n    input = np.pad(input, ((0, 0), (0, 0), (pad, pad), (pad, pad)), 'constant')\n    N, C, H, W = input.shape\n    hout = H / kernel_size\n    wout = W / kernel_size\n    input = input.reshape([N * C, 1, H, W])\n    col_input = im2col(input, kernel_size, kernel_size, stride=kernel_size).reshape(kernel_size*kernel_size, -1)\n    output = col_input.mean(axis=0).reshape(N, C, hout, wout)\n    return output\n\ndef avgpool2d_backward(input, grad_output, kernel_size, pad):\n    '''\n    Args:\n        input: shape = n (#sample) x c_in (#input channel) x h_in (#height) x w_in (#width)\n        grad_output: shape = n (#sample) x c_in (#input channel) x h_out x w_out\n        kernel_size: size of the window to take average over\n        pad: number of zero added to both sides of input\n\n    Returns:\n        grad_input: gradient of input, shape = n (#sample) x c_in (#input channel) x h_in (#height) x w_in (#width)\n    '''\n    grad = np.kron(grad_output, np.ones((kernel_size, kernel_size))) / (kernel_size * kernel_size)\n    if pad == 0:\n        return grad\n    else:\n        return grad[:, :, pad:-pad, pad:-pad]"""
hw2/codes/layers.py,11,"b""import numpy as np\nfrom functions import conv2d_forward, conv2d_backward, avgpool2d_forward, avgpool2d_backward\n\n\nclass Layer(object):\n    def __init__(self, name, trainable=False):\n        self.name = name\n        self.trainable = trainable\n        self._saved_tensor = None\n\n    def forward(self, input):\n        pass\n\n    def backward(self, grad_output):\n        pass\n\n    def update(self, config):\n        pass\n\n    def _saved_for_backward(self, tensor):\n        self._saved_tensor = tensor\n\n\nclass Relu(Layer):\n    def __init__(self, name):\n        super(Relu, self).__init__(name)\n\n    def forward(self, input):\n        self._saved_for_backward(input)\n        input[input <= 0] = 0\n        return input\n\n    def backward(self, grad_output):\n        grad_output[self._saved_tensor <= 0] = 0\n        return grad_output\n\n\nclass Sigmoid(Layer):\n    def __init__(self, name):\n        super(Sigmoid, self).__init__(name)\n\n    def forward(self, input):\n        self._saved_for_backward(1. / (1. + np.exp(-input)))\n        return self._saved_tensor\n\n    def backward(self, grad_output):\n        '''Your codes here'''\n        return self._saved_tensor * (1. - self._saved_tensor) * grad_output\n\n\nclass Linear(Layer):\n    def __init__(self, name, in_num, out_num, init_std):\n        super(Linear, self).__init__(name, trainable=True)\n        self.in_num = in_num\n        self.out_num = out_num\n        self.W = np.random.randn(in_num, out_num) * init_std\n        self.b = np.zeros(out_num)\n\n        self.grad_W = np.zeros((in_num, out_num))\n        self.grad_b = np.zeros(out_num)\n\n        self.diff_W = np.zeros((in_num, out_num))\n        self.diff_b = np.zeros(out_num)\n\n    def forward(self, input):\n        self._saved_for_backward(input)\n        return input.dot(self.W) + self.b\n\n    def backward(self, grad_output):\n        self.grad_W = self._saved_tensor.T.dot(grad_output) # 784x10 = 784x100 * 100x10\n        self.grad_b = grad_output.sum(axis=0)\n        return grad_output.dot(self.W.T)\n\n    def update(self, config):\n        mm = config['momentum']\n        lr = config['learning_rate']\n        wd = config['weight_decay']\n\n        self.diff_W = mm * self.diff_W + (self.grad_W + wd * self.W)\n        self.W = self.W - lr * self.diff_W\n\n        self.diff_b = mm * self.diff_b + (self.grad_b + wd * self.b)\n        self.b = self.b - lr * self.diff_b\n\n\nclass Reshape(Layer):\n    def __init__(self, name, new_shape):\n        super(Reshape, self).__init__(name)\n        self.new_shape = new_shape\n\n    def forward(self, input):\n        self._saved_for_backward(input)\n        return input.reshape(*self.new_shape)\n\n    def backward(self, grad_output):\n        input = self._saved_tensor\n        return grad_output.reshape(*input.shape)\n\n\nclass Conv2D(Layer):\n    def __init__(self, name, in_channel, out_channel, kernel_size, pad, init_std):\n        super(Conv2D, self).__init__(name, trainable=True)\n        self.kernel_size = kernel_size\n        self.pad = pad\n        self.W = np.random.randn(out_channel, in_channel, kernel_size, kernel_size) * init_std\n        self.b = np.zeros(out_channel)\n\n        self.diff_W = np.zeros(self.W.shape)\n        self.diff_b = np.zeros(out_channel)\n\n    def forward(self, input):\n        self._saved_for_backward(input)\n        output = conv2d_forward(input, self.W, self.b, self.kernel_size, self.pad)\n        return output\n\n    def backward(self, grad_output):\n        input = self._saved_tensor\n        grad_input, self.grad_W, self.grad_b = conv2d_backward(input, grad_output, self.W, self.b, self.kernel_size, self.pad)\n        return grad_input\n\n    def update(self, config):\n        mm = config['momentum']\n        lr = config['learning_rate']\n        wd = config['weight_decay']\n\n        self.diff_W = mm * self.diff_W + (self.grad_W + wd * self.W)\n        self.W = self.W - lr * self.diff_W\n\n        self.diff_b = mm * self.diff_b + (self.grad_b + wd * self.b)\n        self.b = self.b - lr * self.diff_b\n\n\nclass AvgPool2D(Layer):\n    def __init__(self, name, kernel_size, pad):\n        super(AvgPool2D, self).__init__(name)\n        self.kernel_size = kernel_size\n        self.pad = pad\n\n    def forward(self, input):\n        self._saved_for_backward(input)\n        output = avgpool2d_forward(input, self.kernel_size, self.pad)\n        return output\n\n    def backward(self, grad_output):\n        input = self._saved_tensor\n        grad_input = avgpool2d_backward(input, grad_output, self.kernel_size, self.pad)\n        return grad_input\n"""
hw2/codes/load_data.py,4,"b""import numpy as np\nimport os\n\n\ndef load_mnist_4d(data_dir):\n    fd = open(os.path.join(data_dir, 'train-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    trX = loaded[16:].reshape((60000, 1, 28, 28)).astype(float)\n\n    fd = open(os.path.join(data_dir, 'train-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    trY = loaded[8:].reshape((60000))\n\n    fd = open(os.path.join(data_dir, 't10k-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    teX = loaded[16:].reshape((10000, 1, 28, 28)).astype(float)\n\n    fd = open(os.path.join(data_dir, 't10k-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    teY = loaded[8:].reshape((10000))\n\n    trX = (trX - 128.0) / 255.0\n    teX = (teX - 128.0) / 255.0\n\n    return trX, teX, trY, teY\n"""
hw2/codes/loss.py,4,"b'from __future__ import division\nimport numpy as np\n\n\nclass EuclideanLoss(object):\n    def __init__(self, name):\n        self.name = name\n\n    def forward(self, input, target):\n        return ((target - input) ** 2).mean(axis=0).sum() / 2.\n\n    def backward(self, input, target):\n        return target - input\n\n\nclass SoftmaxCrossEntropyLoss(object):\n    def __init__(self, name):\n        self.name = name\n\n    def forward(self, input, target): # B * C\n        B, C = input.shape\n        shin = input - np.max(input, axis=1, keepdims=True)\n        loss = -np.sum((shin - np.log(np.sum(np.exp(shin), axis=1, keepdims=True))) * target) / B\n        return loss\n\n    def backward(self, input, target):\n        B, C = input.shape\n        shin = input - np.max(input, axis=1, keepdims=True)\n        prob = np.exp(shin - np.log(np.sum(np.exp(shin), axis=1, keepdims=True)))\n        return (prob - target) / B\n\n'"
hw2/codes/network.py,0,"b'class Network(object):\n    def __init__(self):\n        self.layer_list = []\n        self.params = []\n        self.num_layers = 0\n\n    def add(self, layer):\n        self.layer_list.append(layer)\n        self.num_layers += 1\n\n    def forward(self, input):\n        output = input\n        for i in range(self.num_layers):\n            output = self.layer_list[i].forward(output)\n\n        return output\n\n    def backward(self, grad_output):\n        grad_input = grad_output\n        for i in range(self.num_layers - 1, -1, -1):\n            grad_input = self.layer_list[i].backward(grad_input)\n\n    def update(self, config):\n        for i in range(self.num_layers):\n            if self.layer_list[i].trainable:\n                self.layer_list[i].update(config)\n'"
hw2/codes/plot.py,5,"b'import numpy as np\nfrom pylab import *\n\nD = 10\nacc1 = np.load(\'res/small/acc.npy\').reshape(D, -1).mean(axis=0)\nloss1 = np.load(\'res/small/loss.npy\').reshape(D, -1).mean(axis=0)\nacc2 = np.load(\'res/large/acc.npy\').reshape(D, -1).mean(axis=0)\nloss2 = np.load(\'res/large/loss.npy\').reshape(D, -1).mean(axis=0)\ncut = int(acc1.shape[0] / 10 * 4)\nprint(\' 1: %.2f %.6f\'%(100*acc1[:cut].max(), loss1[:cut].min()))\nprint(\' 2: %.2f %.6f\'%(100*acc2[:cut].max(), loss2[:cut].min()))\niter_ = np.arange(acc1.shape[0]) * D\nprint(acc1.shape, iter_.shape[0])\nfigure()\np = subplot(111)\np.plot(iter_[:cut], loss1[:cut], \'-\', label=\'Original CNN\')\np.plot(iter_[:cut], loss2[:cut], \'-\', label=\'Designed CNN\')\np.set_ylim((0, .4))\np.set_xlabel(r\'# of Iterations\')\np.set_ylabel(r\'Loss\')\np.legend(loc=\'upper right\')\ntight_layout()\nsavefig(""loss.pdf"")\nfigure()\np = subplot(111)\np.plot(iter_[:cut], acc1[:cut], \'-\', label=\'Original CNN\')\np.plot(iter_[:cut], acc2[:cut], \'-\', label=\'Designed CNN\')\np.set_ylim((.9, 1))\np.set_xlabel(r\'# of Iterations\')\np.set_ylabel(r\'Accuracy\')\np.legend(loc=\'lower right\')\ntight_layout()\nsavefig(""acc.pdf"")\n\n#  1: 23:24:44.414     Testing, total mean loss 0.019417, total acc 0.863300 - 23:24:33.131\n# 2s: 20:20:39.807     Testing, total mean loss 0.003224, total acc 0.967700 - 20:18:21.597\n# 2r: 20:48:01.448     Testing, total mean loss 0.002306, total acc 0.981300 - 20:45:16.709\n#-2r: 20:38:47.940     Testing, total mean loss 0.002271, total acc 0.981500 - 20:35:59.910\n# 3s: 00:38:10.865     Testing, total mean loss 0.001759, total acc 0.980098 - 00:33:01.622\n# 3r: 21:24:04.253     Testing, total mean loss 0.001675, total acc 0.980588 - 21:19:28.262'"
hw2/codes/run_cnn.py,6,"b'from network import Network\nfrom layers import Relu, Linear, Conv2D, AvgPool2D, Reshape\nfrom utils import LOG_INFO\nfrom loss import EuclideanLoss, SoftmaxCrossEntropyLoss\nfrom solve_net import train_net, test_net, getvis\nfrom load_data import load_mnist_4d\nimport numpy as np\nimport cv2\n# import matplotlib.pyplot as plt\ndef vis_square(data, id):\n    """"""Take an array of shape (n, height, width) or (n, height, width, 3)\n       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)""""""\n    \n    # normalize data for display\n    data = (data - data.min()) / (data.max() - data.min())\n    # print(data)\n    # force the number of filters to be square\n    n = int(np.ceil(np.sqrt(data.shape[0])))\n    padding = (((0, n ** 2 - data.shape[0]),\n               (0, 1), (0, 1))                 # add some space between filters\n               + ((0, 0),) * (data.ndim - 3))  # don\'t pad the last dimension (if there is one)\n    data = np.pad(data, padding, mode=\'constant\', constant_values=1)  # pad with ones (white)\n    \n    # tile the filters into an image\n    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n    cv2.imwrite(\'try%d.png\'%id, data*255)\n    # print(data.shape)\n    # print(data)\n    # plt.savefig(\'try%d.pdf\'%id)\n    # plt.imshow(data)\n    # plt.axis(\'off\')\n\ntrain_data, test_data, train_label, test_label = load_mnist_4d(\'data\')\n\n# Your model defintion here\n# You should explore different model architecture\nmodel = Network()\nmodel.add(Conv2D(\'conv1\', 1, 24, 5, 2, 1))\nmodel.add(Relu(\'relu1\'))\nmodel.add(AvgPool2D(\'pool1\', 2, 0))  # output shape: N x 4 x 14 x 14\nmodel.add(Conv2D(\'conv2\', 24, 10, 5, 2, 1))\nmodel.add(Relu(\'relu2\'))\nmodel.add(AvgPool2D(\'pool2\', 2, 0))  # output shape: N x 4 x 7 x 7\nmodel.add(Reshape(\'flatten\', (-1, 49*10)))\nmodel.add(Linear(\'fc3\', 49*10, 10, 0.1))\n\n# loss = EuclideanLoss(name=\'loss\')\nloss = SoftmaxCrossEntropyLoss(name=\'loss\')\n\n# Training configuration\n# You should adjust these hyperparameters\n# NOTE: one iteration means model forward-backwards one batch of samples.\n#       one epoch means model has gone through all the training samples.\n#       \'disp_freq\' denotes number of iterations in one epoch to display information.\n\n# np.random.seed(1626)\nconfig = {\n    \'learning_rate\': 0.01,\n    \'weight_decay\': 0.000,\n    \'momentum\': 0.9,\n    \'batch_size\': 100,\n    \'max_epoch\': 100,\n    \'disp_freq\': 50,\n    \'test_epoch\': 1\n}\nloss_ = []\nacc_ = []\ntest_loss = np.zeros(2) + 100\nfor epoch in range(config[\'max_epoch\']):\n    vis = getvis(model, test_data, test_label)\n    for i in range(4):\n        vis_square(vis[i], i)\n    vis_square(model.layer_list[0].W.reshape(-1, model.layer_list[0].W.shape[2], model.layer_list[0].W.shape[3]), -1)\n    LOG_INFO(\'Training @ %d epoch...\' % (epoch))\n    a, b = train_net(model, loss, config, train_data, train_label, config[\'batch_size\'], config[\'disp_freq\'], 600)\n    loss_.append(a)\n    acc_.append(b)\n    if epoch % config[\'test_epoch\'] == 0:\n        LOG_INFO(\'Testing @ %d epoch...\' % (epoch))\n        test_ = test_net(model, loss, test_data, test_label, 100)\n        if epoch % 4 == 0 and epoch > 0:\n        # if test_loss.min() < test_ and abs(test_loss.max() - test_loss.min()) / test_loss.min() < 0.01 or epoch % 7 == 0:\n            config[\'learning_rate\'] /= 2\n            config[\'weight_decay\'] = 0\n            print(\'lr: \', config[\'learning_rate\'])\n        test_loss[1:] = test_loss[:-1].copy()\n        test_loss[0] = test_\nvis = getvis(model, test_data, test_label)\nfor i in range(4):\n    vis_square(vis[i], i)\nnp.save(\'loss\', loss_)\nnp.save(\'acc\', acc_)'"
hw2/codes/solve_net.py,4,"b""from utils import LOG_INFO, onehot_encoding, calculate_acc\nimport numpy as np\n\n\ndef data_iterator(x, y, batch_size, shuffle=True):\n    indx = list(range(len(x)))\n    if shuffle:\n        np.random.shuffle(indx)\n\n    for start_idx in range(0, len(x), batch_size):\n        end_idx = min(start_idx + batch_size, len(x))\n        yield x[indx[start_idx: end_idx]], y[indx[start_idx: end_idx]]\n\n\ndef train_net(model, loss, config, inputs, labels, batch_size, disp_freq, max_iter):\n    loss_ = []\n    acc_ = []\n    iter_counter = 0\n    loss_list = []\n    acc_list = []\n\n    for input, label in data_iterator(inputs, labels, batch_size):\n        target = onehot_encoding(label, 10)\n        iter_counter += 1\n\n        # forward net\n        output = model.forward(input)\n        # calculate loss\n        loss_value = loss.forward(output, target)\n        # generate gradient w.r.t loss\n        grad = loss.backward(output, target)\n        # backward gradient\n\n        model.backward(grad)\n        # update layers' weights\n        model.update(config)\n\n        acc_value = calculate_acc(output, label)\n        loss_list.append(loss_value)\n        acc_list.append(acc_value)\n        loss_.append(loss_value)\n        acc_.append(acc_value)\n        if iter_counter % disp_freq == 0:\n            msg = '  Training iter %d, batch loss %.4f, batch acc %.4f' % (iter_counter, np.mean(loss_list), np.mean(acc_list))\n            loss_list = []\n            acc_list = []\n            LOG_INFO(msg)\n        if iter_counter > max_iter:\n            break\n    return loss_, acc_\n\ndef test_net(model, loss, inputs, labels, batch_size):\n    loss_list = []\n    acc_list = []\n\n    for input, label in data_iterator(inputs, labels, batch_size, shuffle=False):\n        target = onehot_encoding(label, 10)\n        output = model.forward(input)\n        loss_value = loss.forward(output, target)\n        acc_value = calculate_acc(output, label)\n        loss_list.append(loss_value)\n        acc_list.append(acc_value)\n\n    msg = '    Testing, total mean loss %.5f, total acc %.5f' % (np.mean(loss_list), np.mean(acc_list))\n    LOG_INFO(msg)\n    return np.mean(loss_list)\n\nimport cv2\ndef getvis(model, inputs, labels, batch_size=4):\n    for input, label in data_iterator(inputs, labels, batch_size, shuffle=False):\n        print(input.shape)\n        for i in range(batch_size):\n            cv2.imwrite('%d.png'%i, input[i, 0]*255+128)\n        output = model.layer_list[0].forward(input)\n        return output\n"""
hw2/codes/utils.py,2,"b""from __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom datetime import datetime\n\n\ndef onehot_encoding(label, max_num_class):\n    encoding = np.eye(max_num_class)\n    encoding = encoding[label]\n    return encoding\n\n\ndef calculate_acc(output, label):\n    correct = np.sum(np.argmax(output, axis=1) == label)\n    return correct / len(label)\n\n\ndef LOG_INFO(msg):\n    now = datetime.now()\n    display_now = str(now).split(' ')[1][:-3]\n    print(display_now + ' ' + msg)\n"""
hw3/cnn/load_data.py,4,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport os\n\n\ndef load_mnist_4d(data_dir):\n    fd = open(os.path.join(data_dir, 'train-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    trX = loaded[16:].reshape((60000, 28, 28, 1)).astype(float)\n\n    fd = open(os.path.join(data_dir, 'train-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    trY = loaded[8:].reshape((60000))\n\n    fd = open(os.path.join(data_dir, 't10k-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    teX = loaded[16:].reshape((10000, 28, 28, 1)).astype(float)\n\n    fd = open(os.path.join(data_dir, 't10k-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    teY = loaded[8:].reshape((10000))\n\n    trX = (trX - 128.0) / 255.0\n    teX = (teX - 128.0) / 255.0\n\n    return trX, teX, trY, teY\n"""
hw3/cnn/main.py,3,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport time\nfrom model import Model\nfrom load_data import load_mnist_4d\n\ntf.app.flags.DEFINE_integer(""batch_size"", 100, ""batch size for training"")\ntf.app.flags.DEFINE_integer(""h1"", 256, ""first hidden layer size for training"")\ntf.app.flags.DEFINE_integer(""h2"", 128, ""second hidden layer size for training"")\ntf.app.flags.DEFINE_integer(""num_epochs"", 20, ""number of epochs"")\ntf.app.flags.DEFINE_float(""drop_rate"", 0.5, ""drop out rate"")\ntf.app.flags.DEFINE_boolean(""is_train"", True, ""False to inference"")\ntf.app.flags.DEFINE_string(""data_dir"", ""../MNIST_data"", ""data dir"")\ntf.app.flags.DEFINE_string(""gpu"", ""-1"", ""gpu config"")\ntf.app.flags.DEFINE_string(""train_dir"", ""./train"", ""training dir"")\ntf.app.flags.DEFINE_integer(""inference_version"", 0, ""the version for inference"")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef shuffle(X, y, shuffle_parts):\n    chunk_size = int(len(X) / shuffle_parts)\n    shuffled_range = list(range(chunk_size))\n\n    X_buffer = np.copy(X[0:chunk_size])\n    y_buffer = np.copy(y[0:chunk_size])\n\n    for k in range(shuffle_parts):\n        np.random.shuffle(shuffled_range)\n        for i in range(chunk_size):\n            X_buffer[i] = X[k * chunk_size + shuffled_range[i]]\n            y_buffer[i] = y[k * chunk_size + shuffled_range[i]]\n\n        X[k * chunk_size:(k + 1) * chunk_size] = X_buffer\n        y[k * chunk_size:(k + 1) * chunk_size] = y_buffer\n\n    return X, y\n\n\ndef train_epoch(model, sess, X, y): # Training Process\n    loss, acc = 0.0, 0.0\n    st, ed, times = 0, FLAGS.batch_size, 0\n    while st < len(X) and ed <= len(X):\n        X_batch, y_batch = X[st:ed], y[st:ed]\n        feed = {model.x_: X_batch, model.y_: y_batch}\n        loss_, acc_, _ = sess.run([model.loss, model.acc, model.train_op], feed)\n        loss += loss_\n        acc += acc_\n        st, ed = ed, ed+FLAGS.batch_size\n        times += 1\n    loss /= times\n    acc /= times\n    return acc, loss\n\n\ndef valid_epoch(model, sess, X, y): # Valid Process\n    loss, acc = 0.0, 0.0\n    st, ed, times = 0, FLAGS.batch_size, 0\n    while st < len(X) and ed <= len(X):\n        X_batch, y_batch = X[st:ed], y[st:ed]\n        feed = {model.x_: X_batch, model.y_: y_batch}\n        loss_, acc_ = sess.run([model.loss_val, model.acc_val], feed)\n        loss += loss_\n        acc += acc_\n        st, ed = ed, ed+FLAGS.batch_size\n        times += 1\n    loss /= times\n    acc /= times\n    return acc, loss\n\n\ndef inference(model, sess, X): # Test Process\n    return sess.run([model.pred_val], {model.x_: X})[0]\n\nconfig = tf.ConfigProto(allow_soft_placement=True)\nif FLAGS.gpu != \'-1\':\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = FLAGS.gpu\n    config.gpu_options.allow_growth = True\n\nwith tf.Session() as sess:\n    if not os.path.exists(FLAGS.train_dir):\n        os.mkdir(FLAGS.train_dir)\n    if FLAGS.is_train:\n        X_train, X_test, y_train, y_test = load_mnist_4d(FLAGS.data_dir)\n        X_val, y_val = X_train[50000:], y_train[50000:]\n        X_train, y_train = X_train[:50000], y_train[:50000]\n        cnn_model = Model()\n        if tf.train.get_checkpoint_state(FLAGS.train_dir):\n            cnn_model.saver.restore(sess, tf.train.latest_checkpoint(FLAGS.train_dir))\n        else:\n            tf.global_variables_initializer().run()\n\n        pre_losses = [1e18] * 3\n        best_val_acc = 0.0\n        for epoch in range(FLAGS.num_epochs):\n            start_time = time.time()\n            train_acc, train_loss = train_epoch(cnn_model, sess, X_train, y_train)\n            X_train, y_train = shuffle(X_train, y_train, 1)\n\n            val_acc, val_loss = valid_epoch(cnn_model, sess, X_val, y_val)\n\n            if val_acc >= best_val_acc:\n                best_val_acc = val_acc\n                best_epoch = epoch + 1\n                test_acc, test_loss = valid_epoch(cnn_model, sess, X_test, y_test)\n                cnn_model.saver.save(sess, \'%s/checkpoint\' % FLAGS.train_dir, global_step=cnn_model.global_step)\n\n            epoch_time = time.time() - start_time\n            print(""Epoch "" + str(epoch + 1) + "" of "" + str(FLAGS.num_epochs) + "" took "" + str(epoch_time) + ""s"")\n            print(""  learning rate:                 "" + str(cnn_model.learning_rate.eval()))\n            print(""  training loss:                 "" + str(train_loss))\n            print(""  training accuracy:             "" + str(train_acc))\n            print(""  validation loss:               "" + str(val_loss))\n            print(""  validation accuracy:           "" + str(val_acc))\n            print(""  best epoch:                    "" + str(best_epoch))\n            print(""  best validation accuracy:      "" + str(best_val_acc))\n            print(""  test loss:                     "" + str(test_loss))\n            print(""  test accuracy:                 "" + str(test_acc))\n\n            if train_loss > max(pre_losses):\n                sess.run(cnn_model.learning_rate_decay_op)\n            pre_losses = pre_losses[1:] + [train_loss]\n\n    else:\n        cnn_model = Model()\n        if FLAGS.inference_version == 0:\n            model_path = tf.train.latest_checkpoint(FLAGS.train_dir)\n        else:\n            model_path = \'%s/checkpoint-%08d\' % (FLAGS.train_dir, FLAGS.inference_version)\n        cnn_model.saver.restore(sess, model_path)\n        X_train, X_test, y_train, y_test = load_mnist_4d(FLAGS.data_dir)\n\n        count = 0\n        for i in range(len(X_test)):\n            test_image = X_test[i].reshape((1, 28, 28, 1))\n            result = inference(cnn_model, sess, test_image)[0]\n            if result == y_test[i]:\n                count += 1\n        print(""test accuracy: {}"".format(float(count) / len(X_test)))\n'"
hw3/cnn/model.py,0,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nFLAGS = tf.app.flags.FLAGS\n\nclass Model:\n    def __init__(self,\n                 learning_rate=0.001,\n                 learning_rate_decay_factor=0.9995):\n        self.x_ = tf.placeholder(tf.float32, [None, 28, 28, 1])\n        self.y_ = tf.placeholder(tf.int32, [None])\n        self.init = tf.contrib.layers.xavier_initializer()\n\n        # TODO:  fill the blank of the arguments\n        self.loss, self.pred, self.acc = self.forward(is_train=True, reuse=False)\n        self.loss_val, self.pred_val, self.acc_val = self.forward(is_train=False, reuse=True)\n\n        self.learning_rate = tf.Variable(float(learning_rate), trainable=False, dtype=tf.float32)\n        self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * learning_rate_decay_factor)\n\n        self.global_step = tf.Variable(0, trainable=False)\n        self.params = tf.trainable_variables()\n        \n        # TODO:  maybe you need to update the parameter of batch_normalization?\n        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n            self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step,\n                                                                                var_list=self.params)\n\n        self.saver = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2,\n                                    max_to_keep=3, pad_step_number=True, keep_checkpoint_every_n_hours=1.0)\n\n    def forward(self, is_train, reuse=None):\n    \n        with tf.variable_scope(""model"", reuse=reuse):\n            # TODO: implement input -- Conv -- BN -- ReLU -- Dropout -- MaxPool -- Conv -- BN -- ReLU -- Dropout -- MaxPool -- Linear -- loss\n            #        the 10-class prediction output is named as ""logits""\n            # Your Conv Layer\n            conv1 = tf.layers.conv2d(self.x_, filters=FLAGS.h1, kernel_size=[3, 3], strides=[1, 1], padding=""SAME"", kernel_initializer=self.init)\n            # Your BN Layer: use batch_normalization_layer function\n            bn1 = batch_normalization_layer(conv1, is_train)\n            # Your Relu Layer\n            relu1 = tf.nn.relu(bn1)\n            # Your Dropout Layer: use dropout_layer function\n            drop1 = dropout_layer(relu1, FLAGS.drop_rate, is_train)\n            # Your MaxPool\n            maxpool1 = tf.layers.max_pooling2d(drop1, pool_size=[2, 2], strides=2)\n            # Your Conv Layer\n            conv2 = tf.layers.conv2d(maxpool1, filters=FLAGS.h2, kernel_size=[3, 3], strides=[1, 1], padding=""SAME"", kernel_initializer=self.init)\n            # Your BN Layer: use batch_normalization_layer function\n            bn2 = batch_normalization_layer(conv2, is_train)\n            # Your Relu Layer\n            relu2 = tf.nn.relu(bn2)\n            # Your Dropout Layer: use dropout_layer function\n            drop2 = dropout_layer(relu2, FLAGS.drop_rate, is_train)\n            # Your MaxPool\n            maxpool2 = tf.layers.max_pooling2d(drop2, pool_size=[2, 2], strides=2)\n            # Your Linear Layer\n            reshape2 = tf.reshape(maxpool2, [-1, 7*7*FLAGS.h2])\n            # logits = tf.Variable(tf.constant(0.0, shape=[100, 10]))  # deleted this line after you implement above layers\n            logits = tf.layers.dense(reshape2, units=10)\n\n        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y_, logits=logits))\n        pred = tf.argmax(logits, 1)  # Calculate the prediction result\n        correct_pred = tf.equal(tf.cast(pred, tf.int32), self.y_)\n        acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))  # Calculate the accuracy in this mini-batch\n        \n        return loss, pred, acc\n\ndef batch_normalization_layer(incoming, is_train=True):\n    # TODO: implement the batch normalization function and applied it on fully-connected layers\n    # NOTE:  If isTrain is True, you should return calculate mu and sigma by mini-batch\n    #       If isTrain is False, you must estimate mu and sigma from training data\n    return tf.layers.batch_normalization(incoming, training=is_train)\n    \ndef dropout_layer(incoming, drop_rate, is_train=True):\n    # TODO: implement the dropout function and applied it on fully-connected layers\n    # Note: When drop_rate=0, it means drop no values\n    #       If isTrain is True, you should randomly drop some values, and scale the others by 1 / (1 - drop_rate)\n    #       If isTrain is False, remain all values not changed\n    return tf.nn.dropout(incoming, drop_rate) if is_train else incoming\n'"
hw3/mlp/load_data.py,4,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport os\n\n\ndef load_mnist_2d(data_dir):\n    fd = open(os.path.join(data_dir, 'train-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    trX = loaded[16:].reshape((60000, 28 * 28)).astype(float)\n\n    fd = open(os.path.join(data_dir, 'train-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    trY = loaded[8:].reshape((60000))\n\n    fd = open(os.path.join(data_dir, 't10k-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    teX = loaded[16:].reshape((10000, 28 * 28)).astype(float)\n\n    fd = open(os.path.join(data_dir, 't10k-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    teY = loaded[8:].reshape((10000))\n\n    trX = (trX - 128.0) / 255.0\n    teX = (teX - 128.0) / 255.0\n\n    return trX, teX, trY, teY"""
hw3/mlp/main.py,3,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport time\nfrom model import Model\nfrom load_data import load_mnist_2d\n\ntf.app.flags.DEFINE_integer(""batch_size"", 100, ""batch size for training"")\ntf.app.flags.DEFINE_integer(""hidden_size"", 50, ""hidden layer size for training"")\ntf.app.flags.DEFINE_integer(""num_epochs"", 20, ""number of epochs"")\ntf.app.flags.DEFINE_float(""drop_rate"", 0.5, ""drop out rate"")\ntf.app.flags.DEFINE_boolean(""is_train"", True, ""False to inference"")\ntf.app.flags.DEFINE_string(""data_dir"", ""../MNIST_data"", ""data dir"")\ntf.app.flags.DEFINE_string(""gpu"", ""-1"", ""gpu config"")\ntf.app.flags.DEFINE_string(""train_dir"", ""./train"", ""training dir"")\ntf.app.flags.DEFINE_integer(""inference_version"", 0, ""the version for inference"")\nFLAGS = tf.app.flags.FLAGS\n\ndef shuffle(X, y, shuffle_parts):  # Shuffle the X and y\n    chunk_size = int(len(X) / shuffle_parts)\n    shuffled_range = list(range(chunk_size))\n\n    X_buffer = np.copy(X[0:chunk_size])\n    y_buffer = np.copy(y[0:chunk_size])\n\n    for k in range(shuffle_parts):\n        np.random.shuffle(shuffled_range)\n        for i in range(chunk_size):\n            X_buffer[i] = X[k * chunk_size + shuffled_range[i]]\n            y_buffer[i] = y[k * chunk_size + shuffled_range[i]]\n\n        X[k * chunk_size:(k + 1) * chunk_size] = X_buffer\n        y[k * chunk_size:(k + 1) * chunk_size] = y_buffer\n\n    return X, y\n\n\ndef train_epoch(model, sess, X, y): # Training Process\n    loss, acc = 0.0, 0.0\n    st, ed, times = 0, FLAGS.batch_size, 0\n    while st < len(X) and ed <= len(X):\n        X_batch, y_batch = X[st:ed], y[st:ed]\n        feed = {model.x_: X_batch, model.y_: y_batch}\n        loss_, acc_, _ = sess.run([model.loss, model.acc, model.train_op], feed)\n        loss += loss_\n        acc += acc_\n        st, ed = ed, ed+FLAGS.batch_size\n        times += 1\n    loss /= times\n    acc /= times\n    return acc, loss\n\n\ndef valid_epoch(model, sess, X, y): # Valid Process\n    loss, acc = 0.0, 0.0\n    st, ed, times = 0, FLAGS.batch_size, 0\n    while st < len(X) and ed <= len(X):\n        X_batch, y_batch = X[st:ed], y[st:ed]\n        feed = {model.x_: X_batch, model.y_: y_batch}\n        loss_, acc_ = sess.run([model.loss_val, model.acc_val], feed)\n        loss += loss_\n        acc += acc_\n        st, ed = ed, ed+FLAGS.batch_size\n        times += 1\n    loss /= times\n    acc /= times\n    return acc, loss\n\n\ndef inference(model, sess, X): # Test Process\n    return sess.run([model.pred_val], {model.x_: X})[0]\n\nconfig = tf.ConfigProto(allow_soft_placement=True)\nif FLAGS.gpu != \'-1\':\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = FLAGS.gpu\n    config.gpu_options.allow_growth = True\n\nwith tf.Session(config=config) as sess:\n    if not os.path.exists(FLAGS.train_dir):\n        os.mkdir(FLAGS.train_dir)\n    if FLAGS.is_train:\n        X_train, X_test, y_train, y_test = load_mnist_2d(FLAGS.data_dir)\n        X_val, y_val = X_train[50000:], y_train[50000:]\n        X_train, y_train = X_train[:50000], y_train[:50000]\n        mlp_model = Model()\n        if tf.train.get_checkpoint_state(FLAGS.train_dir):\n            mlp_model.saver.restore(sess, tf.train.latest_checkpoint(FLAGS.train_dir))\n        else:\n            tf.global_variables_initializer().run()\n\n        pre_losses = [1e18] * 3\n        best_val_acc = 0.0\n        for epoch in range(FLAGS.num_epochs):\n            start_time = time.time()\n            train_acc, train_loss = train_epoch(mlp_model, sess, X_train, y_train)  # Complete the training process\n            X_train, y_train = shuffle(X_train, y_train, 1)\n\n            val_acc, val_loss = valid_epoch(mlp_model, sess, X_val, y_val)  # Complete the valid process\n\n            if val_acc >= best_val_acc:  # when valid_accuracy > best_valid_accuracy, save the model\n                best_val_acc = val_acc\n                best_epoch = epoch + 1\n                test_acc, test_loss = valid_epoch(mlp_model, sess, X_test, y_test)  # Complete the test process\n                mlp_model.saver.save(sess, \'%s/checkpoint\' % FLAGS.train_dir, global_step=mlp_model.global_step)\n\n            epoch_time = time.time() - start_time\n            print(""Epoch "" + str(epoch + 1) + "" of "" + str(FLAGS.num_epochs) + "" took "" + str(epoch_time) + ""s"")\n            print(""  learning rate:                 "" + str(mlp_model.learning_rate.eval()))\n            print(""  training loss:                 "" + str(train_loss))\n            print(""  training accuracy:             "" + str(train_acc))\n            print(""  validation loss:               "" + str(val_loss))\n            print(""  validation accuracy:           "" + str(val_acc))\n            print(""  best epoch:                    "" + str(best_epoch))\n            print(""  best validation accuracy:      "" + str(best_val_acc))\n            print(""  test loss:                     "" + str(test_loss))\n            print(""  test accuracy:                 "" + str(test_acc))\n\n            if train_loss > max(pre_losses):  # Learning rate decay\n                sess.run(mlp_model.learning_rate_decay_op)\n            pre_losses = pre_losses[1:] + [train_loss]\n\n    else:\n        mlp_model = Model()\n        if FLAGS.inference_version == 0:  # Load the checkpoint\n            model_path = tf.train.latest_checkpoint(FLAGS.train_dir)\n        else:\n            model_path = \'%s/checkpoint-%08d\' % (FLAGS.train_dir, FLAGS.inference_version)\n        mlp_model.saver.restore(sess, model_path)\n        X_train, X_test, y_train, y_test = load_mnist_2d(FLAGS.data_dir)  # load_mnist_2d when implementing MLP\n\n        count = 0\n        for i in range(len(X_test)):\n            test_image = X_test[i].reshape((1, 784))  # May be different in MLP model\n            result = inference(mlp_model, sess, test_image)[0]\n            if result == y_test[i]:\n                count += 1\n        print(""test accuracy: {}"".format(float(count) / len(X_test)))\n'"
hw3/mlp/model.py,0,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nfrom tensorflow.python.training import moving_averages\nFLAGS = tf.app.flags.FLAGS\n\nclass Model:\n    def __init__(self,\n                 learning_rate=0.001,\n                 learning_rate_decay_factor=0.9995):\n        self.x_ = tf.placeholder(tf.float32, [None, 28*28])\n        self.y_ = tf.placeholder(tf.int32, [None])\n        self.hidden_size = FLAGS.hidden_size\n        self.init = tf.contrib.layers.xavier_initializer()\n\t\t# TODO:  fill the blank of the arguments\n        self.loss, self.pred, self.acc = self.forward(is_train=True, reuse=False)\n        self.loss_val, self.pred_val, self.acc_val = self.forward(is_train=False, reuse=True)\n        \n        self.learning_rate = tf.Variable(float(learning_rate), trainable=False, dtype=tf.float32)\n        self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * learning_rate_decay_factor)  # Learning rate decay\n\n        self.global_step = tf.Variable(0, trainable=False)\n        self.params = tf.trainable_variables()\n\n        # TODO:  maybe you need to update the parameter of batch_normalization?\n        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n            self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step,\n                                                                                var_list=self.params)  # Use Adam Optimizer\n\n        self.saver = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2,\n                                    max_to_keep=3, pad_step_number=True, keep_checkpoint_every_n_hours=1.0)\n                                    \n    def forward(self, is_train, reuse=None):\n    \n        with tf.variable_scope(""model"", reuse=reuse):\n            # TODO:  implement input -- Linear -- BN -- ReLU -- Dropout -- Linear -- loss\n            #        the 10-class prediction output is named as ""logits""\n            # Your Linear Layer\n            fc1 = tf.layers.dense(inputs=self.x_, units=self.hidden_size, kernel_initializer=self.init)\n            # Your BN Layer: use batch_normalization_layer function\n            bn1 = batch_normalization_layer(fc1, is_train)\n            # Your Relu Layer\n            relu1 = tf.nn.relu(bn1)\n            # Your Dropout Layer: use dropout_layer function\n            drop1 = dropout_layer(relu1, FLAGS.drop_rate, is_train)\n            # Your Linear Layer\n            logits = tf.layers.dense(inputs=drop1, units=10, kernel_initializer=self.init)\n            # logits = tf.Variable(tf.constant(0.0, shape=[100, 10]))  # deleted this line after you implement above layers\n\n        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y_, logits=logits))\n        pred = tf.argmax(logits, 1)  # Calculate the prediction result\n        correct_pred = tf.equal(tf.cast(pred, tf.int32), self.y_)\n        acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))  # Calculate the accuracy in this mini-batch\n        \n        return loss, pred, acc\n\ndef batch_normalization_layer(incoming, is_train=True):\n    # TODO: implement the batch normalization function and applied it on fully-connected layers\n    # NOTE:  If isTrain is True, you should return calculate mu and sigma by mini-batch\n    #       If isTrain is False, you must estimate mu and sigma from training data\n    return tf.layers.batch_normalization(incoming, training=is_train)\n    \ndef dropout_layer(incoming, drop_rate, is_train=True):\n    # TODO: implement the dropout function and applied it on fully-connected layers\n    # Note: When drop_rate=0, it means drop no values\n    #       If isTrain is True, you should randomly drop some values, and scale the others by 1 / (1 - drop_rate)\n    #       If isTrain is False, remain all values not changed\n    return tf.nn.dropout(incoming, drop_rate) if is_train else incoming\n'"
hw4/codes/main.py,3,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import constant_op\nimport sys\nimport json\nimport time\nimport random\nrandom.seed(1229)\n\nfrom model import RNN, _START_VOCAB\n\ntf.app.flags.DEFINE_boolean(""is_train"", True, ""Set to False to inference."")\ntf.app.flags.DEFINE_integer(""symbols"", 18430, ""vocabulary size."")\ntf.app.flags.DEFINE_integer(""labels"", 5, ""Number of labels."")\ntf.app.flags.DEFINE_float(""learning_rate"", 0.005, ""Number of labels."")\ntf.app.flags.DEFINE_integer(""epoch"", 20, ""Number of epoch."")\ntf.app.flags.DEFINE_integer(""embed_units"", 300, ""Size of word embedding."")\ntf.app.flags.DEFINE_integer(""units"", 512, ""Size of each model layer."")\ntf.app.flags.DEFINE_integer(""layers"", 1, ""Number of layers in the model."")\ntf.app.flags.DEFINE_integer(""batch_size"", 16, ""Batch size to use during training."")\ntf.app.flags.DEFINE_string(""data_dir"", ""./data"", ""Data directory"")\ntf.app.flags.DEFINE_string(""train_dir"", ""./train"", ""Training directory."")\ntf.app.flags.DEFINE_string(""model"", ""lstm"", ""Choose model: [rnn, lstm, gru]"")\ntf.app.flags.DEFINE_integer(""per_checkpoint"", 1000, ""How many steps to do per checkpoint."")\ntf.app.flags.DEFINE_integer(""inference_version"", 0, ""The version for inferencing."")\ntf.app.flags.DEFINE_boolean(""log_parameters"", True, ""Set to True to show the parameters"")\n\nFLAGS = tf.app.flags.FLAGS\n\ndef load_data(path, fname):\n    print(\'Creating %s dataset...\' % fname)\n    data = []\n    with open(\'%s/%s\' % (path, fname)) as f:\n        for idx, line in enumerate(f):\n            tokens = line.split(\' \')\n            data.append({\'label\':tokens[0], \'text\':tokens[1:]})\n    return data\n\ndef build_vocab(path, data):\n    print(""Creating vocabulary..."")\n    vocab = {}\n    for i, pair in enumerate(data):\n        for token in pair[\'text\']:\n            if token in vocab:\n                vocab[token] += 1\n            else:\n                vocab[token] = 1\n    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n    if len(vocab_list) > FLAGS.symbols:\n        vocab_list = vocab_list[:FLAGS.symbols]\n    else:\n        FLAGS.symbols = len(vocab_list)\n\n    print(""Loading word vectors..."")\n    vectors = {}\n    with open(\'%s/vector.txt\' % path) as f:\n        for i, line in enumerate(f):\n            if i % 1000 == 0:\n                print(""    processing line %d"" % i)\n            s = line.strip()\n            word = s[:s.find(\' \')]\n            vector = s[s.find(\' \')+1:]\n            vectors[word] = vector\n    \n    embed = []\n    for word in vocab_list:\n        if word in vectors:\n            vector = list(map(float, vectors[word].split()))\n        else:\n            vector = np.zeros((FLAGS.embed_units), dtype=np.float32)\n        embed.append(vector)\n    embed = np.array(embed, dtype=np.float32)\n    return vocab_list, embed\n\ndef gen_batch_data(data):\n    def padding(sent, l):\n        return sent + [\'_PAD\'] * (l-len(sent))\n\n    max_len = max([len(item[\'text\']) for item in data])\n    texts, texts_length, labels = [], [], []\n        \n    for item in data:\n        texts.append(padding(item[\'text\'], max_len))\n        texts_length.append(len(item[\'text\']))\n        labels.append(int(item[\'label\']))\n\n    batched_data = {\'texts\': np.array(texts), \'texts_length\':texts_length, \'labels\':labels}\n\n    return batched_data\n\ndef train(model, sess, dataset):\n    st, ed, loss, accuracy = 0, 0, .0, .0\n    while ed < len(dataset):\n        st, ed = ed, ed+FLAGS.batch_size if ed+FLAGS.batch_size < len(dataset) else len(dataset)\n        batch_data = gen_batch_data(dataset[st:ed])\n        outputs = model.train_step(sess, batch_data)\n        loss += outputs[0]\n        accuracy += outputs[1]\n\n    return loss / len(dataset), accuracy / len(dataset)\n\ndef evaluate(model, sess, dataset):\n    st, ed, loss, accuracy = 0, 0, .0, .0\n    while ed < len(dataset):\n        st, ed = ed, ed+FLAGS.batch_size if ed+FLAGS.batch_size < len(dataset) else len(dataset)\n        batch_data = gen_batch_data(dataset[st:ed])\n        outputs = sess.run([\'loss:0\', \'accuracy:0\'], {\'texts:0\':batch_data[\'texts\'], \'texts_length:0\':batch_data[\'texts_length\'], \'labels:0\':batch_data[\'labels\']})\n        loss += outputs[0]\n        accuracy += outputs[1]\n    return loss / len(dataset), accuracy / len(dataset)\n\ndef inference(model, sess, dataset):\n    st, ed, loss, accuracy = 0, 0, .0, .0\n    result = []\n    while ed < len(dataset):\n        st, ed = ed, ed+FLAGS.batch_size if ed+FLAGS.batch_size < len(dataset) else len(dataset)\n        batch_data = gen_batch_data(dataset[st:ed])\n        outputs = sess.run([\'predict_labels:0\'], {\'texts:0\':batch_data[\'texts\'], \'texts_length:0\':batch_data[\'texts_length\']})\n        result += outputs[0].tolist()\n\n    with open(\'result.txt\', \'w\') as f:\n        for label in result:\n            f.write(\'%d\\n\' % label)\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nwith tf.Session(config=config) as sess:\n    if FLAGS.is_train:\n        print(FLAGS.__flags)\n        data_train = load_data(FLAGS.data_dir, \'train.txt\')\n        data_dev = load_data(FLAGS.data_dir, \'dev.txt\')\n        vocab, embed = build_vocab(FLAGS.data_dir, data_train)\n        \n        model = RNN(\n                FLAGS.symbols, \n                FLAGS.embed_units,\n                FLAGS.units, \n                FLAGS.layers,\n                FLAGS.labels,\n                embed,\n                learning_rate=FLAGS.learning_rate,\n                model_choose=FLAGS.model)\n        if FLAGS.log_parameters:\n            model.print_parameters()\n        \n        if tf.train.get_checkpoint_state(FLAGS.train_dir):\n            print(""Reading model parameters from %s"" % FLAGS.train_dir)\n            model.saver.restore(sess, tf.train.latest_checkpoint(FLAGS.train_dir))\n        else:\n            print(""Created model with fresh parameters."")\n            tf.global_variables_initializer().run()\n            op_in = model.symbol2index.insert(constant_op.constant(vocab),\n                constant_op.constant(list(range(FLAGS.symbols)), dtype=tf.int64))\n            sess.run(op_in)\n\n        for epoch in list(range(FLAGS.epoch)):\n            random.shuffle(data_train)\n            start_time = time.time()\n            loss, accuracy = train(model, sess, data_train)\n            print(""epoch %d learning rate %.4f epoch-time %.4f loss %.8f accuracy [%.8f]"" % (epoch, model.learning_rate.eval(), time.time()-start_time, loss, accuracy))\n            model.saver.save(sess, \'%s/checkpoint\' % FLAGS.train_dir, global_step=epoch)\n            loss, accuracy = evaluate(model, sess, data_dev)\n            print(""        dev_set, loss %.8f, accuracy [%.8f]"" % (loss, accuracy))\n    else:\n        data_dev = load_data(FLAGS.data_dir, \'dev.txt\')\n        data_test = load_data(FLAGS.data_dir, \'test.txt\')\n\n        model = RNN(\n                FLAGS.symbols, \n                FLAGS.embed_units,\n                FLAGS.units, \n                FLAGS.layers,\n                FLAGS.labels,\n                embed=None,\n                model_choose=FLAGS.model)\n\n        if FLAGS.inference_version == 0:\n            model_path = tf.train.latest_checkpoint(FLAGS.train_dir)\n        else:\n            model_path = \'%s/checkpoint-%08d\' % (FLAGS.train_dir, FLAGS.inference_version)\n        print(\'restore from %s\' % model_path)\n        model.saver.restore(sess, model_path)\n\n        loss, accuracy = evaluate(model, sess, data_dev)\n        print(""        dev_set, loss %.8f, accuracy [%.8f]"" % (loss, accuracy))\n\n        inference(model, sess, data_test)\n        print(""        test_set, write inference results to result.txt"")\n\n\n'"
hw4/codes/model.py,0,"b'import numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops.nn import dynamic_rnn\nfrom tensorflow.contrib.seq2seq.python.ops.loss import sequence_loss\nfrom tensorflow.contrib.lookup.lookup_ops import MutableHashTable\nfrom tensorflow.contrib.layers.python.layers import layers\nfrom tensorflow.contrib.session_bundle import exporter\n\nfrom rnn_cell import GRUCell, BasicLSTMCell, MultiRNNCell, BasicRNNCell\n\nPAD_ID = 0\nUNK_ID = 1\n_START_VOCAB = [\'_PAD\', \'_UNK\']\n\nclass RNN(object):\n    def __init__(self,\n            num_symbols,\n            num_embed_units,\n            num_units,\n            num_layers,\n            num_labels,\n            embed,\n            learning_rate=0.005,\n            max_gradient_norm=5.0,\n\t\t\tparam_da=150,\n\t\t\tparam_r=10,\n            model_choose=\'lstm\'):\n        \n        self.texts = tf.placeholder(tf.string, (None, None), \'texts\')  # shape: [batch, length]\n\n        #todo: implement placeholders\n        self.texts_length = tf.placeholder(tf.int32, (None, ), \'texts_length\')  # shape: [batch]\n        self.labels = tf.placeholder(tf.int64, (None, ), \'labels\')  # shape: [batch]\n        \n        self.symbol2index = MutableHashTable(\n                key_dtype=tf.string,\n                value_dtype=tf.int64,\n                default_value=UNK_ID,\n                shared_name=""in_table"",\n                name=""in_table"",\n                checkpoint=True)\n\t\t\n        batch_size = tf.shape(self.texts)[0]\n        # build the vocab table (string to index)\n        # initialize the training process\n        self.learning_rate = tf.Variable(float(learning_rate), \n                trainable=False, dtype=tf.float32)\n        self.global_step = tf.Variable(0, trainable=False)\n\n\n        self.index_input = self.symbol2index.lookup(self.texts)   # shape: [batch, length]\n        \n        # build the embedding table (index to vector)\n        if embed is None:\n            # initialize the embedding randomly\n            self.embed = tf.get_variable(\'embed\', [num_symbols, num_embed_units], tf.float32)\n        else:\n            # initialize the embedding by pre-trained word vectors\n            self.embed = tf.get_variable(\'embed\', dtype=tf.float32, initializer=embed)\n\n        #todo: implement embedding inputs\n        self.embed_input = tf.nn.embedding_lookup(self.embed, self.index_input) #shape: [batch, length, num_embed_units]\n\n        #todo: implement 3 RNNCells (BasicRNNCell, GRUCell, BasicLSTMCell) in a multi-layer setting with #num_units neurons and #num_layers layers\n        if model_choose not in [\'rnn\',\'lstm\', \'gru\']:\n            model_choose = \'lstm\'\n        cell_type = {\'rnn\': BasicRNNCell, \'lstm\': BasicLSTMCell, \'gru\': GRUCell}[model_choose]\n        cell_fw = MultiRNNCell([cell_type(num_units) for x in range(num_layers)])\n        cell_bw = MultiRNNCell([cell_type(num_units) for x in range(num_layers)])\n\n        #todo: implement bidirectional RNN\n        outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, self.embed_input, self.texts_length, dtype=tf.float32, scope=""rnn"")\n        H = tf.concat(outputs, 2) # shape: (batch, length, 2*num_units)\n        \n        \n        with tf.variable_scope(\'logits\'):\n            #todo: implement self-attention mechanism, feel free to add codes to calculate temporary results\n            Ws1 = tf.get_variable(""Ws1"", [2 * num_units, param_da])\n            Ws2 = tf.get_variable(""Ws2"", [param_da, param_r])\n            A = tf.nn.softmax(tf.einsum(""ijk,kl->ijl"", tf.nn.tanh(tf.einsum(""ijk,kl->ijl"", H, Ws1)), Ws2))\n            M = tf.matmul(A, H, transpose_a=True) # shape: [batch, param_r, 2*num_units]\n            flatten_M = tf.reshape(M, shape=[batch_size, param_r*2*num_units]) # shape: [batch, param_r*2*num_units]\n\n            logits = tf.layers.dense(flatten_M, num_labels, activation=None, name=\'projection\') # shape: [batch, num_labels]\n\t\t\n        #todo: calculate additional loss, feel free to add codes to calculate temporary results\n        identity = tf.reshape(tf.tile(tf.diag(tf.ones([param_r])), [batch_size, 1]), [batch_size, param_r, param_r])\n        self.penalized_term = tf.norm(tf.matmul(A, A, transpose_a=True) - identity)\n        \n        self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels, logits=logits), name=\'loss\') + 0.0001*self.penalized_term\n        predict_labels = tf.argmax(logits, 1, \'predict_labels\')\n        self.accuracy = tf.reduce_sum(tf.cast(tf.equal(self.labels, predict_labels), tf.int32), name=\'accuracy\')\n\n        self.params = tf.trainable_variables()\n            \n        # calculate the gradient of parameters\n        opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n        gradients = tf.gradients(self.loss, self.params)\n        clipped_gradients, self.gradient_norm = tf.clip_by_global_norm(gradients, \n                max_gradient_norm)\n        self.update = opt.apply_gradients(zip(clipped_gradients, self.params), \n                global_step=self.global_step)\n        \n        self.saver = tf.train.Saver(write_version=tf.train.SaverDef.V2, \n                max_to_keep=5, pad_step_number=True)\n\n    def print_parameters(self):\n        for item in self.params:\n            print(\'%s: %s\' % (item.name, item.get_shape()))\n    \n    def train_step(self, session, data):\n        input_feed = {self.texts: data[\'texts\'],\n                self.texts_length: data[\'texts_length\'],\n                self.labels: data[\'labels\']}\n        output_feed = [self.loss, self.accuracy, self.gradient_norm, self.update]\n        return session.run(output_feed, input_feed)\n'"
hw4/codes/rnn_cell.py,0,"b'""""""Module implementing RNN Cells.\nThis module provides a number of basic commonly used RNN cells, such as LSTM\n(Long Short Term Memory) or GRU (Gated Recurrent Unit), and a number of\noperators that allow adding dropouts, projections, or embeddings for inputs.\nConstructing multi-layer cells is supported by the class `MultiRNNCell`, or by\ncalling the `rnn` ops several times.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport hashlib\nimport numbers\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.layers import base as base_layer\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops import variables as tf_variables\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import nest\n\n\n_BIAS_VARIABLE_NAME = ""bias""\n_WEIGHTS_VARIABLE_NAME = ""kernel""\n\n\ndef _like_rnncell(cell):\n    """"""Checks that a given object is an RNNCell by using duck typing.""""""\n    conditions = [hasattr(cell, ""output_size""), hasattr(cell, ""state_size""),\n                                hasattr(cell, ""zero_state""), callable(cell)]\n    return all(conditions)\n\n\ndef _concat(prefix, suffix, static=False):\n    """"""Concat that enables int, Tensor, or TensorShape values.\n    This function takes a size specification, which can be an integer, a\n    TensorShape, or a Tensor, and converts it into a concatenated Tensor\n    (if static = False) or a list of integers (if static = True).\n    Args:\n        prefix: The prefix; usually the batch size (and/or time step size).\n            (TensorShape, int, or Tensor.)\n        suffix: TensorShape, int, or Tensor.\n        static: If `True`, return a python list with possibly unknown dimensions.\n            Otherwise return a `Tensor`.\n    Returns:\n        shape: the concatenation of prefix and suffix.\n    Raises:\n        ValueError: if `suffix` is not a scalar or vector (or TensorShape).\n        ValueError: if prefix or suffix was `None` and asked for dynamic\n            Tensors out.\n    """"""\n    if isinstance(prefix, ops.Tensor):\n        p = prefix\n        p_static = tensor_util.constant_value(prefix)\n        if p.shape.ndims == 0:\n            p = array_ops.expand_dims(p, 0)\n        elif p.shape.ndims != 1:\n            raise ValueError(""prefix tensor must be either a scalar or vector, ""\n                                             ""but saw tensor: %s"" % p)\n    else:\n        p = tensor_shape.as_shape(prefix)\n        p_static = p.as_list() if p.ndims is not None else None\n        p = (constant_op.constant(p.as_list(), dtype=dtypes.int32)\n                 if p.is_fully_defined() else None)\n    if isinstance(suffix, ops.Tensor):\n        s = suffix\n        s_static = tensor_util.constant_value(suffix)\n        if s.shape.ndims == 0:\n            s = array_ops.expand_dims(s, 0)\n        elif s.shape.ndims != 1:\n            raise ValueError(""suffix tensor must be either a scalar or vector, ""\n                                             ""but saw tensor: %s"" % s)\n    else:\n        s = tensor_shape.as_shape(suffix)\n        s_static = s.as_list() if s.ndims is not None else None\n        s = (constant_op.constant(s.as_list(), dtype=dtypes.int32)\n                 if s.is_fully_defined() else None)\n\n    if static:\n        shape = tensor_shape.as_shape(p_static).concatenate(s_static)\n        shape = shape.as_list() if shape.ndims is not None else None\n    else:\n        if p is None or s is None:\n            raise ValueError(""Provided a prefix or suffix of None: %s and %s""\n                                             % (prefix, suffix))\n        shape = array_ops.concat((p, s), 0)\n    return shape\n\n\ndef _linear(args,\n                        output_size,\n                        bias,\n                        bias_initializer=None,\n                        kernel_initializer=None):\n    """"""Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n    Args:\n        args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n        output_size: int, second dimension of W[i].\n        bias: boolean, whether to add a bias term or not.\n        bias_initializer: starting value to initialize the bias\n            (default is all zeros).\n        kernel_initializer: starting value to initialize the weight.\n    Returns:\n        A 2D Tensor with shape [batch x output_size] equal to\n        sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n    Raises:\n        ValueError: if some of the arguments has unspecified or wrong shape.\n    """"""\n    if args is None or (nest.is_sequence(args) and not args):\n        raise ValueError(""`args` must be specified"")\n    if not nest.is_sequence(args):\n        args = [args]\n\n    # Calculate the total size of arguments on dimension 1.\n    total_arg_size = 0\n    shapes = [a.get_shape() for a in args]\n    for shape in shapes:\n        if shape.ndims != 2:\n            raise ValueError(""linear is expecting 2D arguments: %s"" % shapes)\n        if shape[1].value is None:\n            raise ValueError(""linear expects shape[1] to be provided for shape %s, ""\n                                             ""but saw %s"" % (shape, shape[1]))\n        else:\n            total_arg_size += shape[1].value\n\n    dtype = [a.dtype for a in args][0]\n\n    # Now the computation.\n    scope = vs.get_variable_scope()\n    with vs.variable_scope(scope) as outer_scope:\n        weights = vs.get_variable(\n                _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\n                dtype=dtype,\n                initializer=kernel_initializer)\n        if len(args) == 1:\n            res = math_ops.matmul(args[0], weights)\n        else:\n            res = math_ops.matmul(array_ops.concat(args, 1), weights)\n        if not bias:\n            return res\n        with vs.variable_scope(outer_scope) as inner_scope:\n            inner_scope.set_partitioner(None)\n            if bias_initializer is None:\n                bias_initializer = init_ops.constant_initializer(0.0, dtype=dtype)\n            biases = vs.get_variable(\n                    _BIAS_VARIABLE_NAME, [output_size],\n                    dtype=dtype,\n                    initializer=bias_initializer)\n        return nn_ops.bias_add(res, biases)\n\n\ndef _zero_state_tensors(state_size, batch_size, dtype):\n    """"""Create tensors of zeros based on state_size, batch_size, and dtype.""""""\n    def get_state_shape(s):\n        """"""Combine s with batch_size to get a proper tensor shape.""""""\n        c = _concat(batch_size, s)\n        c_static = _concat(batch_size, s, static=True)\n        size = array_ops.zeros(c, dtype=dtype)\n        size.set_shape(c_static)\n        return size\n    return nest.map_structure(get_state_shape, state_size)\n\n\nclass RNNCell(base_layer.Layer):\n    """"""Abstract object representing an RNN cell.\n    Every `RNNCell` must have the properties below and implement `call` with\n    the signature `(output, next_state) = call(input, state)`.    The optional\n    third input argument, `scope`, is allowed for backwards compatibility\n    purposes; but should be left off for new subclasses.\n    This definition of cell differs from the definition used in the literature.\n    In the literature, \'cell\' refers to an object with a single scalar output.\n    This definition refers to a horizontal array of such units.\n    An RNN cell, in the most abstract setting, is anything that has\n    a state and performs some operation that takes a matrix of inputs.\n    This operation results in an output matrix with `self.output_size` columns.\n    If `self.state_size` is an integer, this operation also results in a new\n    state matrix with `self.state_size` columns.    If `self.state_size` is a\n    (possibly nested tuple of) TensorShape object(s), then it should return a\n    matching structure of Tensors having shape `[batch_size].concatenate(s)`\n    for each `s` in `self.batch_size`.\n    """"""\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Run this RNN cell on inputs, starting from the given state.\n        Args:\n            inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n            state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n                with shape `[batch_size x self.state_size]`.    Otherwise, if\n                `self.state_size` is a tuple of integers, this should be a tuple\n                with shapes `[batch_size x s] for s in self.state_size`.\n            scope: VariableScope for the created subgraph; defaults to class name.\n        Returns:\n            A pair containing:\n            - Output: A `2-D` tensor with shape `[batch_size x self.output_size]`.\n            - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n                the arity and shapes of `state`.\n        """"""\n        if scope is not None:\n            with vs.variable_scope(scope,\n                                                         custom_getter=self._rnn_get_variable) as scope:\n                return super(RNNCell, self).__call__(inputs, state, scope=scope)\n        else:\n            with vs.variable_scope(vs.get_variable_scope(),\n                                                         custom_getter=self._rnn_get_variable):\n                return super(RNNCell, self).__call__(inputs, state)\n\n    def _rnn_get_variable(self, getter, *args, **kwargs):\n        variable = getter(*args, **kwargs)\n        trainable = (variable in tf_variables.trainable_variables() or\n                                 (isinstance(variable, tf_variables.PartitionedVariable) and\n                                    list(variable)[0] in tf_variables.trainable_variables()))\n        if trainable and variable not in self._trainable_weights:\n            self._trainable_weights.append(variable)\n        elif not trainable and variable not in self._non_trainable_weights:\n            self._non_trainable_weights.append(variable)\n        return variable\n\n    @property\n    def state_size(self):\n        """"""size(s) of state(s) used by this cell.\n        It can be represented by an Integer, a TensorShape or a tuple of Integers\n        or TensorShapes.\n        """"""\n        raise NotImplementedError(""Abstract method"")\n\n    @property\n    def output_size(self):\n        """"""Integer or TensorShape: size of outputs produced by this cell.""""""\n        raise NotImplementedError(""Abstract method"")\n\n    def build(self, _):\n        # This tells the parent Layer object that it\'s OK to call\n        # self.add_variable() inside the call() method.\n        pass\n\n    def zero_state(self, batch_size, dtype):\n        """"""Return zero-filled state tensor(s).\n        Args:\n            batch_size: int, float, or unit Tensor representing the batch size.\n            dtype: the data type to use for the state.\n        Returns:\n            If `state_size` is an int or TensorShape, then the return value is a\n            `N-D` tensor of shape `[batch_size x state_size]` filled with zeros.\n            If `state_size` is a nested list or tuple, then the return value is\n            a nested list or tuple (of the same structure) of `2-D` tensors with\n            the shapes `[batch_size x s]` for each s in `state_size`.\n        """"""\n        with ops.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n            state_size = self.state_size\n            return _zero_state_tensors(state_size, batch_size, dtype)\n\n\nclass BasicRNNCell(RNNCell):\n    """"""The most basic RNN cell.\n    Args:\n        num_units: int, The number of units in the LSTM cell.\n        activation: Nonlinearity to use.    Default: `tanh`.\n        reuse: (optional) Python boolean describing whether to reuse variables\n         in an existing scope.    If not `True`, and the existing scope already has\n         the given variables, an error is raised.\n    """"""\n\n    def __init__(self, num_units, activation=None, reuse=None):\n        super(BasicRNNCell, self).__init__(_reuse=reuse)\n        self._num_units = num_units\n        self._activation = activation or math_ops.tanh\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def call(self, inputs, state):\n        """"""basic RNN: output = new_state = activation(W * input + U * state + B).""""""\n        output = self._activation(_linear([inputs, state], self._num_units, True))\n        return output, output\n\n\nclass GRUCell(RNNCell):\n    """"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).""""""\n\n    def __init__(self,\n                             num_units,\n                             activation=None,\n                             reuse=None,\n                             kernel_initializer=None,\n                             bias_initializer=None):\n        super(GRUCell, self).__init__(_reuse=reuse)\n        self._num_units = num_units\n        self._activation = activation or math_ops.tanh\n        self._kernel_initializer = kernel_initializer\n        self._bias_initializer = bias_initializer\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def call(self, inputs, state):\n        """"""Gated recurrent unit (GRU) with nunits cells.""""""\n        with vs.variable_scope(""gates""):    # Reset gate and update gate.\n            # We start with bias of 1.0 to not reset and not update.\n            bias_ones = self._bias_initializer\n            if self._bias_initializer is None:\n                dtype = [a.dtype for a in [inputs, state]][0]\n                bias_ones = init_ops.constant_initializer(1.0, dtype=dtype)\n            value = math_ops.sigmoid(\n                    _linear([inputs, state], 2 * self._num_units, True, bias_ones,\n                                    self._kernel_initializer))\n            r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\n        with vs.variable_scope(""candidate""):\n            #todo: calculate c and new_h according to GRU\n            # Ref to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py\n            c = self._activation(_linear([inputs, r * state], self._num_units, False))\n        new_h = u * state + (1 - u) * c\n        return new_h, new_h\n\n\n_LSTMStateTuple = collections.namedtuple(""LSTMStateTuple"", (""c"", ""h""))\n\n\nclass LSTMStateTuple(_LSTMStateTuple):\n    """"""Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.\n    Stores two elements: `(c, h)`, in that order.\n    Only used when `state_is_tuple=True`.\n    """"""\n    __slots__ = ()\n\n    @property\n    def dtype(self):\n        (c, h) = self\n        if c.dtype != h.dtype:\n            raise TypeError(""Inconsistent internal state: %s vs %s"" %\n                                            (str(c.dtype), str(h.dtype)))\n        return c.dtype\n\n\nclass BasicLSTMCell(RNNCell):\n    """"""Basic LSTM recurrent network cell.\n    The implementation is based on: http://arxiv.org/abs/1409.2329.\n    We add forget_bias (default: 1) to the biases of the forget gate in order to\n    reduce the scale of forgetting in the beginning of the training.\n    It does not allow cell clipping, a projection layer, and does not\n    use peep-hole connections: it is the basic baseline.\n    For advanced models, please use the full @{tf.nn.rnn_cell.LSTMCell}\n    that follows.\n    """"""\n\n    def __init__(self, num_units, forget_bias=1.0,\n                             state_is_tuple=True, activation=None, reuse=None):\n        """"""Initialize the basic LSTM cell.\n        Args:\n            num_units: int, The number of units in the LSTM cell.\n            forget_bias: float, The bias added to forget gates (see above).\n            state_is_tuple: If True, accepted and returned states are 2-tuples of\n                the `c_state` and `m_state`.    If False, they are concatenated\n                along the column axis.    The latter behavior will soon be deprecated.\n            activation: Activation function of the inner states.    Default: `tanh`.\n            reuse: (optional) Python boolean describing whether to reuse variables\n                in an existing scope.    If not `True`, and the existing scope already has\n                the given variables, an error is raised.\n        """"""\n        super(BasicLSTMCell, self).__init__(_reuse=reuse)\n        if not state_is_tuple:\n            logging.warn(""%s: Using a concatenated state is slower and will soon be ""\n                                     ""deprecated.    Use state_is_tuple=True."", self)\n        self._num_units = num_units\n        self._forget_bias = forget_bias\n        self._state_is_tuple = state_is_tuple\n        self._activation = activation or math_ops.tanh\n\n    @property\n    def state_size(self):\n        return (LSTMStateTuple(self._num_units, self._num_units)\n                        if self._state_is_tuple else 2 * self._num_units)\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def call(self, inputs, state):\n        """"""Long short-term memory cell (LSTM).""""""\n        sigmoid = math_ops.sigmoid\n        # Parameters of gates are concatenated into one multiply for efficiency.\n        if self._state_is_tuple:\n            c, h = state\n        else:\n            c, h = array_ops.split(value=state, num_or_size_splits=2, axis=1)\n\n        #todo: calculate new_c and new_h according to LSTM\n        # Ref to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py\n        gate_input = _linear([inputs, h], 4 * self._num_units, bias=True)\n        i, f, o, c_ = array_ops.split(value=gate_input, num_or_size_splits=4, axis=1)\n        new_c = sigmoid(f + self._forget_bias) * c + sigmoid(i) * self._activation(c_)\n        new_h = sigmoid(o) * self._activation(new_c)\n\n        if self._state_is_tuple:\n            new_state = LSTMStateTuple(new_c, new_h)\n        else:\n            new_state = array_ops.concat([new_c, new_h], 1)\n        return new_h, new_state\n\n\nclass MultiRNNCell(RNNCell):\n    """"""RNN cell composed sequentially of multiple simple cells.""""""\n\n    def __init__(self, cells, state_is_tuple=True):\n        """"""Create a RNN cell composed sequentially of a number of RNNCells.\n        Args:\n            cells: list of RNNCells that will be composed in this order.\n            state_is_tuple: If True, accepted and returned states are n-tuples, where\n                `n = len(cells)`.    If False, the states are all\n                concatenated along the column axis.    This latter behavior will soon be\n                deprecated.\n        Raises:\n            ValueError: if cells is empty (not allowed), or at least one of the cells\n                returns a state tuple but the flag `state_is_tuple` is `False`.\n        """"""\n        super(MultiRNNCell, self).__init__()\n        if not cells:\n            raise ValueError(""Must specify at least one cell for MultiRNNCell."")\n        if not nest.is_sequence(cells):\n            raise TypeError(\n                    ""cells must be a list or tuple, but saw: %s."" % cells)\n\n        self._cells = cells\n        self._state_is_tuple = state_is_tuple\n        if not state_is_tuple:\n            if any(nest.is_sequence(c.state_size) for c in self._cells):\n                raise ValueError(""Some cells return tuples of states, but the flag ""\n                                                 ""state_is_tuple is not set.    State sizes are: %s""\n                                                 % str([c.state_size for c in self._cells]))\n\n    @property\n    def state_size(self):\n        if self._state_is_tuple:\n            return tuple(cell.state_size for cell in self._cells)\n        else:\n            return sum([cell.state_size for cell in self._cells])\n\n    @property\n    def output_size(self):\n        return self._cells[-1].output_size\n\n    def zero_state(self, batch_size, dtype):\n        with ops.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n            if self._state_is_tuple:\n                return tuple(cell.zero_state(batch_size, dtype) for cell in self._cells)\n            else:\n                # We know here that state_size of each cell is not a tuple and\n                # presumably does not contain TensorArrays or anything else fancy\n                return super(MultiRNNCell, self).zero_state(batch_size, dtype)\n\n    def call(self, inputs, state):\n        """"""Run this multi-layer cell on inputs, starting from state.""""""\n        cur_state_pos = 0\n        cur_inp = inputs\n        new_states = []\n        for i, cell in enumerate(self._cells):\n            with vs.variable_scope(""cell_%d"" % i):\n                if self._state_is_tuple:\n                    if not nest.is_sequence(state):\n                        raise ValueError(\n                                ""Expected state to be a tuple of length %d, but received: %s"" %\n                                (len(self.state_size), state))\n                    cur_state = state[i]\n                else:\n                    cur_state = array_ops.slice(state, [0, cur_state_pos],\n                                                                            [-1, cell.state_size])\n                    cur_state_pos += cell.state_size\n                cur_inp, new_state = cell(cur_inp, cur_state)\n                new_states.append(new_state)\n\n        new_states = (tuple(new_states) if self._state_is_tuple else\n                                    array_ops.concat(new_states, 1))\n\n        return cur_inp, new_states\n\n\n'"
hw4/hw4/extract.py,10,"b""import numpy as np\nname = ['rnn', 'lstm', 'gru']\nfor i in name:\n    for j in [1,2]:\n        info = open('%s%d.log'%(i, j)).read().split('Created model with fresh parameters.\\n')[-1].split('\\n')[:-1]\n        train_loss = []\n        train_acc = []\n        dev_loss = []\n        dev_acc = []\n        test_acc = []\n        for train in info[::3]:\n            train_loss.append(float(train.split('loss ')[-1].split('acc')[0]))\n            train_acc.append(float(train.split('[')[-1].replace(']','')))\n        train_loss = np.array(train_loss)\n        train_acc = np.array(train_acc)\n        print('%s%d & %f & %f '%(i.upper(),j, train_loss.min(), train_acc.max()))\n        np.save('%s%d_train_loss'%(i,j), train_loss)\n        np.save('%s%d_train_acc'%(i,j), train_acc)\n        for dev in info[1::3]:\n            dev_loss.append(float(dev.split('loss ')[-1].split(', ')[0]))\n            dev_acc.append(float(dev.split('[')[-1].replace(']','')))\n        dev_loss = np.array(dev_loss)\n        dev_acc = np.array(dev_acc)\n        print('& %f & %f \\\\\\\\' % (dev_loss.min(), dev_acc.max()))\n        np.save('%s%d_dev_loss'%(i,j), dev_loss)\n        np.save('%s%d_dev_acc'%(i,j), dev_acc)\n        for test in info[2::3]:\n            test_acc.append(float(test.split('test ')[-1].split(' ')[0]))\n        test_acc = np.array(test_acc)\n        #print('%s%d test: '%(i,j), test_acc.max())\n        np.save('%s%d_test_acc' %(i,j), test_acc)\n"""
hw4/hw4/view.py,7,"b'import os\nimport numpy as np\nfrom pylab import *\nfrom matplotlib.legend_handler import HandlerTuple\n\nlength = np.load(\'gru1_dev_loss.npy\').shape[0]\nmodels = [\'rnn1\', \'rnn2\', \'lstm1\', \'lstm2\', \'gru1\', \'gru2\']\niter_ = np.arange(length)\n# train_loss + dev_loss\ntrain_loss = []\ndev_loss = []\nfor model in models:\n\ttrain_loss.append(np.load(model + \'_train_loss.npy\'))\n\tdev_loss.append(np.load(model + \'_dev_loss.npy\'))\n\nfigure()\np = subplot(111)\nfor i, model in enumerate(models):\n\tl1 = p.plot(iter_, train_loss[i], \'-\', label=\'%s train loss\' % model.upper())\nfor i, model in enumerate(models):\n\tl2 = p.plot(iter_, dev_loss[i], \'--\', label=\'%s dev loss\' % model.upper())\np.set_ylim((0, .5))\np.set_xlabel(r\'# of Epochs\')\np.set_ylabel(r\'Loss\')\np.legend(loc=""upper left"", ncol=2)\ntight_layout()\nsavefig(""loss.pdf"")\n\n# train_acc + dev_acc\n\ntrain_acc = []\ndev_acc = []\nfor model in models:\n\ttrain_acc.append(np.load(model + \'_train_acc.npy\'))\n\tdev_acc.append(np.load(model + \'_dev_acc.npy\'))\n\nfigure()\np = subplot(111)\nfor i, model in enumerate(models):\n\tp.plot(iter_, train_acc[i], \'-\', label=\'%s train acc\' % model.upper())\nfor i, model in enumerate(models):\n\tp.plot(iter_, dev_acc[i], \'--\', label=\'%s dev acc\' % model.upper())\np.set_ylim((-.1, 1.1))\np.set_xlabel(r\'# of Epochs\')\np.set_ylabel(r\'Accuracy\')\np.legend(loc=""lower right"", ncol=2)\ntight_layout()\nsavefig(""acc.pdf"")\n\n# test_acc\n\ntest_acc = []\nfor model in models:\n\ttest_acc.append(np.load(model + \'_test_acc.npy\'))\n\nfigure()\np = subplot(111)\nfor i, model in enumerate(models):\n\tp.plot(iter_, test_acc[i], \'-\', label=\'%s test acc\' % model.upper())\np.set_ylim((.0, 1))\np.set_xlabel(r\'# of Epochs\')\np.set_ylabel(r\'Accuracy\')\np.legend(loc=\'lower right\')\ntight_layout()\nsavefig(""test.pdf"")\n'"
hw4/hw4/gd/rnn_cell.py,0,"b'""""""Module implementing RNN Cells.\nThis module provides a number of basic commonly used RNN cells, such as LSTM\n(Long Short Term Memory) or GRU (Gated Recurrent Unit), and a number of\noperators that allow adding dropouts, projections, or embeddings for inputs.\nConstructing multi-layer cells is supported by the class `MultiRNNCell`, or by\ncalling the `rnn` ops several times.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport hashlib\nimport numbers\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.layers import base as base_layer\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops import variables as tf_variables\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import nest\n\n\n_BIAS_VARIABLE_NAME = ""bias""\n_WEIGHTS_VARIABLE_NAME = ""kernel""\n\n\ndef _like_rnncell(cell):\n    """"""Checks that a given object is an RNNCell by using duck typing.""""""\n    conditions = [hasattr(cell, ""output_size""), hasattr(cell, ""state_size""),\n                                hasattr(cell, ""zero_state""), callable(cell)]\n    return all(conditions)\n\n\ndef _concat(prefix, suffix, static=False):\n    """"""Concat that enables int, Tensor, or TensorShape values.\n    This function takes a size specification, which can be an integer, a\n    TensorShape, or a Tensor, and converts it into a concatenated Tensor\n    (if static = False) or a list of integers (if static = True).\n    Args:\n        prefix: The prefix; usually the batch size (and/or time step size).\n            (TensorShape, int, or Tensor.)\n        suffix: TensorShape, int, or Tensor.\n        static: If `True`, return a python list with possibly unknown dimensions.\n            Otherwise return a `Tensor`.\n    Returns:\n        shape: the concatenation of prefix and suffix.\n    Raises:\n        ValueError: if `suffix` is not a scalar or vector (or TensorShape).\n        ValueError: if prefix or suffix was `None` and asked for dynamic\n            Tensors out.\n    """"""\n    if isinstance(prefix, ops.Tensor):\n        p = prefix\n        p_static = tensor_util.constant_value(prefix)\n        if p.shape.ndims == 0:\n            p = array_ops.expand_dims(p, 0)\n        elif p.shape.ndims != 1:\n            raise ValueError(""prefix tensor must be either a scalar or vector, ""\n                                             ""but saw tensor: %s"" % p)\n    else:\n        p = tensor_shape.as_shape(prefix)\n        p_static = p.as_list() if p.ndims is not None else None\n        p = (constant_op.constant(p.as_list(), dtype=dtypes.int32)\n                 if p.is_fully_defined() else None)\n    if isinstance(suffix, ops.Tensor):\n        s = suffix\n        s_static = tensor_util.constant_value(suffix)\n        if s.shape.ndims == 0:\n            s = array_ops.expand_dims(s, 0)\n        elif s.shape.ndims != 1:\n            raise ValueError(""suffix tensor must be either a scalar or vector, ""\n                                             ""but saw tensor: %s"" % s)\n    else:\n        s = tensor_shape.as_shape(suffix)\n        s_static = s.as_list() if s.ndims is not None else None\n        s = (constant_op.constant(s.as_list(), dtype=dtypes.int32)\n                 if s.is_fully_defined() else None)\n\n    if static:\n        shape = tensor_shape.as_shape(p_static).concatenate(s_static)\n        shape = shape.as_list() if shape.ndims is not None else None\n    else:\n        if p is None or s is None:\n            raise ValueError(""Provided a prefix or suffix of None: %s and %s""\n                                             % (prefix, suffix))\n        shape = array_ops.concat((p, s), 0)\n    return shape\n\n\ndef _linear(args,\n                        output_size,\n                        bias,\n                        bias_initializer=None,\n                        kernel_initializer=None):\n    """"""Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n    Args:\n        args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n        output_size: int, second dimension of W[i].\n        bias: boolean, whether to add a bias term or not.\n        bias_initializer: starting value to initialize the bias\n            (default is all zeros).\n        kernel_initializer: starting value to initialize the weight.\n    Returns:\n        A 2D Tensor with shape [batch x output_size] equal to\n        sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n    Raises:\n        ValueError: if some of the arguments has unspecified or wrong shape.\n    """"""\n    if args is None or (nest.is_sequence(args) and not args):\n        raise ValueError(""`args` must be specified"")\n    if not nest.is_sequence(args):\n        args = [args]\n\n    # Calculate the total size of arguments on dimension 1.\n    total_arg_size = 0\n    shapes = [a.get_shape() for a in args]\n    for shape in shapes:\n        if shape.ndims != 2:\n            raise ValueError(""linear is expecting 2D arguments: %s"" % shapes)\n        if shape[1].value is None:\n            raise ValueError(""linear expects shape[1] to be provided for shape %s, ""\n                                             ""but saw %s"" % (shape, shape[1]))\n        else:\n            total_arg_size += shape[1].value\n\n    dtype = [a.dtype for a in args][0]\n\n    # Now the computation.\n    scope = vs.get_variable_scope()\n    with vs.variable_scope(scope) as outer_scope:\n        weights = vs.get_variable(\n                _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\n                dtype=dtype,\n                initializer=kernel_initializer)\n        if len(args) == 1:\n            res = math_ops.matmul(args[0], weights)\n        else:\n            res = math_ops.matmul(array_ops.concat(args, 1), weights)\n        if not bias:\n            return res\n        with vs.variable_scope(outer_scope) as inner_scope:\n            inner_scope.set_partitioner(None)\n            if bias_initializer is None:\n                bias_initializer = init_ops.constant_initializer(0.0, dtype=dtype)\n            biases = vs.get_variable(\n                    _BIAS_VARIABLE_NAME, [output_size],\n                    dtype=dtype,\n                    initializer=bias_initializer)\n        return nn_ops.bias_add(res, biases)\n\n\ndef _zero_state_tensors(state_size, batch_size, dtype):\n    """"""Create tensors of zeros based on state_size, batch_size, and dtype.""""""\n    def get_state_shape(s):\n        """"""Combine s with batch_size to get a proper tensor shape.""""""\n        c = _concat(batch_size, s)\n        c_static = _concat(batch_size, s, static=True)\n        size = array_ops.zeros(c, dtype=dtype)\n        size.set_shape(c_static)\n        return size\n    return nest.map_structure(get_state_shape, state_size)\n\n\nclass RNNCell(base_layer.Layer):\n    """"""Abstract object representing an RNN cell.\n    Every `RNNCell` must have the properties below and implement `call` with\n    the signature `(output, next_state) = call(input, state)`.    The optional\n    third input argument, `scope`, is allowed for backwards compatibility\n    purposes; but should be left off for new subclasses.\n    This definition of cell differs from the definition used in the literature.\n    In the literature, \'cell\' refers to an object with a single scalar output.\n    This definition refers to a horizontal array of such units.\n    An RNN cell, in the most abstract setting, is anything that has\n    a state and performs some operation that takes a matrix of inputs.\n    This operation results in an output matrix with `self.output_size` columns.\n    If `self.state_size` is an integer, this operation also results in a new\n    state matrix with `self.state_size` columns.    If `self.state_size` is a\n    (possibly nested tuple of) TensorShape object(s), then it should return a\n    matching structure of Tensors having shape `[batch_size].concatenate(s)`\n    for each `s` in `self.batch_size`.\n    """"""\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Run this RNN cell on inputs, starting from the given state.\n        Args:\n            inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n            state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n                with shape `[batch_size x self.state_size]`.    Otherwise, if\n                `self.state_size` is a tuple of integers, this should be a tuple\n                with shapes `[batch_size x s] for s in self.state_size`.\n            scope: VariableScope for the created subgraph; defaults to class name.\n        Returns:\n            A pair containing:\n            - Output: A `2-D` tensor with shape `[batch_size x self.output_size]`.\n            - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n                the arity and shapes of `state`.\n        """"""\n        if scope is not None:\n            with vs.variable_scope(scope,\n                                                         custom_getter=self._rnn_get_variable) as scope:\n                return super(RNNCell, self).__call__(inputs, state, scope=scope)\n        else:\n            with vs.variable_scope(vs.get_variable_scope(),\n                                                         custom_getter=self._rnn_get_variable):\n                return super(RNNCell, self).__call__(inputs, state)\n\n    def _rnn_get_variable(self, getter, *args, **kwargs):\n        variable = getter(*args, **kwargs)\n        trainable = (variable in tf_variables.trainable_variables() or\n                                 (isinstance(variable, tf_variables.PartitionedVariable) and\n                                    list(variable)[0] in tf_variables.trainable_variables()))\n        if trainable and variable not in self._trainable_weights:\n            self._trainable_weights.append(variable)\n        elif not trainable and variable not in self._non_trainable_weights:\n            self._non_trainable_weights.append(variable)\n        return variable\n\n    @property\n    def state_size(self):\n        """"""size(s) of state(s) used by this cell.\n        It can be represented by an Integer, a TensorShape or a tuple of Integers\n        or TensorShapes.\n        """"""\n        raise NotImplementedError(""Abstract method"")\n\n    @property\n    def output_size(self):\n        """"""Integer or TensorShape: size of outputs produced by this cell.""""""\n        raise NotImplementedError(""Abstract method"")\n\n    def build(self, _):\n        # This tells the parent Layer object that it\'s OK to call\n        # self.add_variable() inside the call() method.\n        pass\n\n    def zero_state(self, batch_size, dtype):\n        """"""Return zero-filled state tensor(s).\n        Args:\n            batch_size: int, float, or unit Tensor representing the batch size.\n            dtype: the data type to use for the state.\n        Returns:\n            If `state_size` is an int or TensorShape, then the return value is a\n            `N-D` tensor of shape `[batch_size x state_size]` filled with zeros.\n            If `state_size` is a nested list or tuple, then the return value is\n            a nested list or tuple (of the same structure) of `2-D` tensors with\n            the shapes `[batch_size x s]` for each s in `state_size`.\n        """"""\n        with ops.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n            state_size = self.state_size\n            return _zero_state_tensors(state_size, batch_size, dtype)\n\n\nclass BasicRNNCell(RNNCell):\n    """"""The most basic RNN cell.\n    Args:\n        num_units: int, The number of units in the LSTM cell.\n        activation: Nonlinearity to use.    Default: `tanh`.\n        reuse: (optional) Python boolean describing whether to reuse variables\n         in an existing scope.    If not `True`, and the existing scope already has\n         the given variables, an error is raised.\n    """"""\n\n    def __init__(self, num_units, activation=None, reuse=None):\n        super(BasicRNNCell, self).__init__(_reuse=reuse)\n        self._num_units = num_units\n        self._activation = activation or math_ops.tanh\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def call(self, inputs, state):\n        """"""basic RNN: output = new_state = activation(W * input + U * state + B).""""""\n        output = self._activation(_linear([inputs, state], self._num_units, True))\n        return output, output\n\n\nclass GRUCell(RNNCell):\n    """"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).""""""\n\n    def __init__(self,\n                             num_units,\n                             activation=None,\n                             reuse=None,\n                             kernel_initializer=None,\n                             bias_initializer=None):\n        super(GRUCell, self).__init__(_reuse=reuse)\n        self._num_units = num_units\n        self._activation = activation or math_ops.tanh\n        self._kernel_initializer = kernel_initializer\n        self._bias_initializer = bias_initializer\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def call(self, inputs, state):\n        """"""Gated recurrent unit (GRU) with nunits cells.""""""\n        with vs.variable_scope(""gates""):    # Reset gate and update gate.\n            # We start with bias of 1.0 to not reset and not update.\n            bias_ones = self._bias_initializer\n            if self._bias_initializer is None:\n                dtype = [a.dtype for a in [inputs, state]][0]\n                bias_ones = init_ops.constant_initializer(1.0, dtype=dtype)\n            value = math_ops.sigmoid(\n                    _linear([inputs, state], 2 * self._num_units, True, bias_ones,\n                                    self._kernel_initializer))\n            r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\n        with vs.variable_scope(""candidate""):\n            #todo: calculate c and new_h according to GRU\n            # Ref to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py\n            c = self._activation(_linear([inputs, r * state], self._num_units, False))\n        new_h = u * state + (1 - u) * c\n        return new_h, new_h\n\n\n_LSTMStateTuple = collections.namedtuple(""LSTMStateTuple"", (""c"", ""h""))\n\n\nclass LSTMStateTuple(_LSTMStateTuple):\n    """"""Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.\n    Stores two elements: `(c, h)`, in that order.\n    Only used when `state_is_tuple=True`.\n    """"""\n    __slots__ = ()\n\n    @property\n    def dtype(self):\n        (c, h) = self\n        if c.dtype != h.dtype:\n            raise TypeError(""Inconsistent internal state: %s vs %s"" %\n                                            (str(c.dtype), str(h.dtype)))\n        return c.dtype\n\n\nclass BasicLSTMCell(RNNCell):\n    """"""Basic LSTM recurrent network cell.\n    The implementation is based on: http://arxiv.org/abs/1409.2329.\n    We add forget_bias (default: 1) to the biases of the forget gate in order to\n    reduce the scale of forgetting in the beginning of the training.\n    It does not allow cell clipping, a projection layer, and does not\n    use peep-hole connections: it is the basic baseline.\n    For advanced models, please use the full @{tf.nn.rnn_cell.LSTMCell}\n    that follows.\n    """"""\n\n    def __init__(self, num_units, forget_bias=1.0,\n                             state_is_tuple=True, activation=None, reuse=None):\n        """"""Initialize the basic LSTM cell.\n        Args:\n            num_units: int, The number of units in the LSTM cell.\n            forget_bias: float, The bias added to forget gates (see above).\n            state_is_tuple: If True, accepted and returned states are 2-tuples of\n                the `c_state` and `m_state`.    If False, they are concatenated\n                along the column axis.    The latter behavior will soon be deprecated.\n            activation: Activation function of the inner states.    Default: `tanh`.\n            reuse: (optional) Python boolean describing whether to reuse variables\n                in an existing scope.    If not `True`, and the existing scope already has\n                the given variables, an error is raised.\n        """"""\n        super(BasicLSTMCell, self).__init__(_reuse=reuse)\n        if not state_is_tuple:\n            logging.warn(""%s: Using a concatenated state is slower and will soon be ""\n                                     ""deprecated.    Use state_is_tuple=True."", self)\n        self._num_units = num_units\n        self._forget_bias = forget_bias\n        self._state_is_tuple = state_is_tuple\n        self._activation = activation or math_ops.tanh\n\n    @property\n    def state_size(self):\n        return (LSTMStateTuple(self._num_units, self._num_units)\n                        if self._state_is_tuple else 2 * self._num_units)\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def call(self, inputs, state):\n        """"""Long short-term memory cell (LSTM).""""""\n        sigmoid = math_ops.sigmoid\n        # Parameters of gates are concatenated into one multiply for efficiency.\n        if self._state_is_tuple:\n            c, h = state\n        else:\n            c, h = array_ops.split(value=state, num_or_size_splits=2, axis=1)\n\n        #todo: calculate new_c and new_h according to LSTM\n        # Ref to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py\n        gate_input = _linear([inputs, h], 4 * self._num_units, bias=True)\n        i, f, o, c_ = array_ops.split(value=gate_input, num_or_size_splits=4, axis=1)\n        new_c = sigmoid(f + self._forget_bias) * c + sigmoid(i) * self._activation(c_)\n        new_h = sigmoid(o) * self._activation(new_c)\n\n        if self._state_is_tuple:\n            new_state = LSTMStateTuple(new_c, new_h)\n        else:\n            new_state = array_ops.concat([new_c, new_h], 1)\n        return new_h, new_state\n\n\nclass MultiRNNCell(RNNCell):\n    """"""RNN cell composed sequentially of multiple simple cells.""""""\n\n    def __init__(self, cells, state_is_tuple=True):\n        """"""Create a RNN cell composed sequentially of a number of RNNCells.\n        Args:\n            cells: list of RNNCells that will be composed in this order.\n            state_is_tuple: If True, accepted and returned states are n-tuples, where\n                `n = len(cells)`.    If False, the states are all\n                concatenated along the column axis.    This latter behavior will soon be\n                deprecated.\n        Raises:\n            ValueError: if cells is empty (not allowed), or at least one of the cells\n                returns a state tuple but the flag `state_is_tuple` is `False`.\n        """"""\n        super(MultiRNNCell, self).__init__()\n        if not cells:\n            raise ValueError(""Must specify at least one cell for MultiRNNCell."")\n        if not nest.is_sequence(cells):\n            raise TypeError(\n                    ""cells must be a list or tuple, but saw: %s."" % cells)\n\n        self._cells = cells\n        self._state_is_tuple = state_is_tuple\n        if not state_is_tuple:\n            if any(nest.is_sequence(c.state_size) for c in self._cells):\n                raise ValueError(""Some cells return tuples of states, but the flag ""\n                                                 ""state_is_tuple is not set.    State sizes are: %s""\n                                                 % str([c.state_size for c in self._cells]))\n\n    @property\n    def state_size(self):\n        if self._state_is_tuple:\n            return tuple(cell.state_size for cell in self._cells)\n        else:\n            return sum([cell.state_size for cell in self._cells])\n\n    @property\n    def output_size(self):\n        return self._cells[-1].output_size\n\n    def zero_state(self, batch_size, dtype):\n        with ops.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n            if self._state_is_tuple:\n                return tuple(cell.zero_state(batch_size, dtype) for cell in self._cells)\n            else:\n                # We know here that state_size of each cell is not a tuple and\n                # presumably does not contain TensorArrays or anything else fancy\n                return super(MultiRNNCell, self).zero_state(batch_size, dtype)\n\n    def call(self, inputs, state):\n        """"""Run this multi-layer cell on inputs, starting from state.""""""\n        cur_state_pos = 0\n        cur_inp = inputs\n        new_states = []\n        for i, cell in enumerate(self._cells):\n            with vs.variable_scope(""cell_%d"" % i):\n                if self._state_is_tuple:\n                    if not nest.is_sequence(state):\n                        raise ValueError(\n                                ""Expected state to be a tuple of length %d, but received: %s"" %\n                                (len(self.state_size), state))\n                    cur_state = state[i]\n                else:\n                    cur_state = array_ops.slice(state, [0, cur_state_pos],\n                                                                            [-1, cell.state_size])\n                    cur_state_pos += cell.state_size\n                cur_inp, new_state = cell(cur_inp, cur_state)\n                new_states.append(new_state)\n\n        new_states = (tuple(new_states) if self._state_is_tuple else\n                                    array_ops.concat(new_states, 1))\n\n        return cur_inp, new_states\n\n\n'"
hw4/hw4/mm9/rnn_cell.py,0,"b'""""""Module implementing RNN Cells.\nThis module provides a number of basic commonly used RNN cells, such as LSTM\n(Long Short Term Memory) or GRU (Gated Recurrent Unit), and a number of\noperators that allow adding dropouts, projections, or embeddings for inputs.\nConstructing multi-layer cells is supported by the class `MultiRNNCell`, or by\ncalling the `rnn` ops several times.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport hashlib\nimport numbers\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.layers import base as base_layer\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops import variables as tf_variables\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import nest\n\n\n_BIAS_VARIABLE_NAME = ""bias""\n_WEIGHTS_VARIABLE_NAME = ""kernel""\n\n\ndef _like_rnncell(cell):\n    """"""Checks that a given object is an RNNCell by using duck typing.""""""\n    conditions = [hasattr(cell, ""output_size""), hasattr(cell, ""state_size""),\n                                hasattr(cell, ""zero_state""), callable(cell)]\n    return all(conditions)\n\n\ndef _concat(prefix, suffix, static=False):\n    """"""Concat that enables int, Tensor, or TensorShape values.\n    This function takes a size specification, which can be an integer, a\n    TensorShape, or a Tensor, and converts it into a concatenated Tensor\n    (if static = False) or a list of integers (if static = True).\n    Args:\n        prefix: The prefix; usually the batch size (and/or time step size).\n            (TensorShape, int, or Tensor.)\n        suffix: TensorShape, int, or Tensor.\n        static: If `True`, return a python list with possibly unknown dimensions.\n            Otherwise return a `Tensor`.\n    Returns:\n        shape: the concatenation of prefix and suffix.\n    Raises:\n        ValueError: if `suffix` is not a scalar or vector (or TensorShape).\n        ValueError: if prefix or suffix was `None` and asked for dynamic\n            Tensors out.\n    """"""\n    if isinstance(prefix, ops.Tensor):\n        p = prefix\n        p_static = tensor_util.constant_value(prefix)\n        if p.shape.ndims == 0:\n            p = array_ops.expand_dims(p, 0)\n        elif p.shape.ndims != 1:\n            raise ValueError(""prefix tensor must be either a scalar or vector, ""\n                                             ""but saw tensor: %s"" % p)\n    else:\n        p = tensor_shape.as_shape(prefix)\n        p_static = p.as_list() if p.ndims is not None else None\n        p = (constant_op.constant(p.as_list(), dtype=dtypes.int32)\n                 if p.is_fully_defined() else None)\n    if isinstance(suffix, ops.Tensor):\n        s = suffix\n        s_static = tensor_util.constant_value(suffix)\n        if s.shape.ndims == 0:\n            s = array_ops.expand_dims(s, 0)\n        elif s.shape.ndims != 1:\n            raise ValueError(""suffix tensor must be either a scalar or vector, ""\n                                             ""but saw tensor: %s"" % s)\n    else:\n        s = tensor_shape.as_shape(suffix)\n        s_static = s.as_list() if s.ndims is not None else None\n        s = (constant_op.constant(s.as_list(), dtype=dtypes.int32)\n                 if s.is_fully_defined() else None)\n\n    if static:\n        shape = tensor_shape.as_shape(p_static).concatenate(s_static)\n        shape = shape.as_list() if shape.ndims is not None else None\n    else:\n        if p is None or s is None:\n            raise ValueError(""Provided a prefix or suffix of None: %s and %s""\n                                             % (prefix, suffix))\n        shape = array_ops.concat((p, s), 0)\n    return shape\n\n\ndef _linear(args,\n                        output_size,\n                        bias,\n                        bias_initializer=None,\n                        kernel_initializer=None):\n    """"""Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n    Args:\n        args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n        output_size: int, second dimension of W[i].\n        bias: boolean, whether to add a bias term or not.\n        bias_initializer: starting value to initialize the bias\n            (default is all zeros).\n        kernel_initializer: starting value to initialize the weight.\n    Returns:\n        A 2D Tensor with shape [batch x output_size] equal to\n        sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n    Raises:\n        ValueError: if some of the arguments has unspecified or wrong shape.\n    """"""\n    if args is None or (nest.is_sequence(args) and not args):\n        raise ValueError(""`args` must be specified"")\n    if not nest.is_sequence(args):\n        args = [args]\n\n    # Calculate the total size of arguments on dimension 1.\n    total_arg_size = 0\n    shapes = [a.get_shape() for a in args]\n    for shape in shapes:\n        if shape.ndims != 2:\n            raise ValueError(""linear is expecting 2D arguments: %s"" % shapes)\n        if shape[1].value is None:\n            raise ValueError(""linear expects shape[1] to be provided for shape %s, ""\n                                             ""but saw %s"" % (shape, shape[1]))\n        else:\n            total_arg_size += shape[1].value\n\n    dtype = [a.dtype for a in args][0]\n\n    # Now the computation.\n    scope = vs.get_variable_scope()\n    with vs.variable_scope(scope) as outer_scope:\n        weights = vs.get_variable(\n                _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\n                dtype=dtype,\n                initializer=kernel_initializer)\n        if len(args) == 1:\n            res = math_ops.matmul(args[0], weights)\n        else:\n            res = math_ops.matmul(array_ops.concat(args, 1), weights)\n        if not bias:\n            return res\n        with vs.variable_scope(outer_scope) as inner_scope:\n            inner_scope.set_partitioner(None)\n            if bias_initializer is None:\n                bias_initializer = init_ops.constant_initializer(0.0, dtype=dtype)\n            biases = vs.get_variable(\n                    _BIAS_VARIABLE_NAME, [output_size],\n                    dtype=dtype,\n                    initializer=bias_initializer)\n        return nn_ops.bias_add(res, biases)\n\n\ndef _zero_state_tensors(state_size, batch_size, dtype):\n    """"""Create tensors of zeros based on state_size, batch_size, and dtype.""""""\n    def get_state_shape(s):\n        """"""Combine s with batch_size to get a proper tensor shape.""""""\n        c = _concat(batch_size, s)\n        c_static = _concat(batch_size, s, static=True)\n        size = array_ops.zeros(c, dtype=dtype)\n        size.set_shape(c_static)\n        return size\n    return nest.map_structure(get_state_shape, state_size)\n\n\nclass RNNCell(base_layer.Layer):\n    """"""Abstract object representing an RNN cell.\n    Every `RNNCell` must have the properties below and implement `call` with\n    the signature `(output, next_state) = call(input, state)`.    The optional\n    third input argument, `scope`, is allowed for backwards compatibility\n    purposes; but should be left off for new subclasses.\n    This definition of cell differs from the definition used in the literature.\n    In the literature, \'cell\' refers to an object with a single scalar output.\n    This definition refers to a horizontal array of such units.\n    An RNN cell, in the most abstract setting, is anything that has\n    a state and performs some operation that takes a matrix of inputs.\n    This operation results in an output matrix with `self.output_size` columns.\n    If `self.state_size` is an integer, this operation also results in a new\n    state matrix with `self.state_size` columns.    If `self.state_size` is a\n    (possibly nested tuple of) TensorShape object(s), then it should return a\n    matching structure of Tensors having shape `[batch_size].concatenate(s)`\n    for each `s` in `self.batch_size`.\n    """"""\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Run this RNN cell on inputs, starting from the given state.\n        Args:\n            inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n            state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n                with shape `[batch_size x self.state_size]`.    Otherwise, if\n                `self.state_size` is a tuple of integers, this should be a tuple\n                with shapes `[batch_size x s] for s in self.state_size`.\n            scope: VariableScope for the created subgraph; defaults to class name.\n        Returns:\n            A pair containing:\n            - Output: A `2-D` tensor with shape `[batch_size x self.output_size]`.\n            - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n                the arity and shapes of `state`.\n        """"""\n        if scope is not None:\n            with vs.variable_scope(scope,\n                                                         custom_getter=self._rnn_get_variable) as scope:\n                return super(RNNCell, self).__call__(inputs, state, scope=scope)\n        else:\n            with vs.variable_scope(vs.get_variable_scope(),\n                                                         custom_getter=self._rnn_get_variable):\n                return super(RNNCell, self).__call__(inputs, state)\n\n    def _rnn_get_variable(self, getter, *args, **kwargs):\n        variable = getter(*args, **kwargs)\n        trainable = (variable in tf_variables.trainable_variables() or\n                                 (isinstance(variable, tf_variables.PartitionedVariable) and\n                                    list(variable)[0] in tf_variables.trainable_variables()))\n        if trainable and variable not in self._trainable_weights:\n            self._trainable_weights.append(variable)\n        elif not trainable and variable not in self._non_trainable_weights:\n            self._non_trainable_weights.append(variable)\n        return variable\n\n    @property\n    def state_size(self):\n        """"""size(s) of state(s) used by this cell.\n        It can be represented by an Integer, a TensorShape or a tuple of Integers\n        or TensorShapes.\n        """"""\n        raise NotImplementedError(""Abstract method"")\n\n    @property\n    def output_size(self):\n        """"""Integer or TensorShape: size of outputs produced by this cell.""""""\n        raise NotImplementedError(""Abstract method"")\n\n    def build(self, _):\n        # This tells the parent Layer object that it\'s OK to call\n        # self.add_variable() inside the call() method.\n        pass\n\n    def zero_state(self, batch_size, dtype):\n        """"""Return zero-filled state tensor(s).\n        Args:\n            batch_size: int, float, or unit Tensor representing the batch size.\n            dtype: the data type to use for the state.\n        Returns:\n            If `state_size` is an int or TensorShape, then the return value is a\n            `N-D` tensor of shape `[batch_size x state_size]` filled with zeros.\n            If `state_size` is a nested list or tuple, then the return value is\n            a nested list or tuple (of the same structure) of `2-D` tensors with\n            the shapes `[batch_size x s]` for each s in `state_size`.\n        """"""\n        with ops.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n            state_size = self.state_size\n            return _zero_state_tensors(state_size, batch_size, dtype)\n\n\nclass BasicRNNCell(RNNCell):\n    """"""The most basic RNN cell.\n    Args:\n        num_units: int, The number of units in the LSTM cell.\n        activation: Nonlinearity to use.    Default: `tanh`.\n        reuse: (optional) Python boolean describing whether to reuse variables\n         in an existing scope.    If not `True`, and the existing scope already has\n         the given variables, an error is raised.\n    """"""\n\n    def __init__(self, num_units, activation=None, reuse=None):\n        super(BasicRNNCell, self).__init__(_reuse=reuse)\n        self._num_units = num_units\n        self._activation = activation or math_ops.tanh\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def call(self, inputs, state):\n        """"""basic RNN: output = new_state = activation(W * input + U * state + B).""""""\n        output = self._activation(_linear([inputs, state], self._num_units, True))\n        return output, output\n\n\nclass GRUCell(RNNCell):\n    """"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).""""""\n\n    def __init__(self,\n                             num_units,\n                             activation=None,\n                             reuse=None,\n                             kernel_initializer=None,\n                             bias_initializer=None):\n        super(GRUCell, self).__init__(_reuse=reuse)\n        self._num_units = num_units\n        self._activation = activation or math_ops.tanh\n        self._kernel_initializer = kernel_initializer\n        self._bias_initializer = bias_initializer\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def call(self, inputs, state):\n        """"""Gated recurrent unit (GRU) with nunits cells.""""""\n        with vs.variable_scope(""gates""):    # Reset gate and update gate.\n            # We start with bias of 1.0 to not reset and not update.\n            bias_ones = self._bias_initializer\n            if self._bias_initializer is None:\n                dtype = [a.dtype for a in [inputs, state]][0]\n                bias_ones = init_ops.constant_initializer(1.0, dtype=dtype)\n            value = math_ops.sigmoid(\n                    _linear([inputs, state], 2 * self._num_units, True, bias_ones,\n                                    self._kernel_initializer))\n            r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\n        with vs.variable_scope(""candidate""):\n            #todo: calculate c and new_h according to GRU\n            # Ref to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py\n            c = self._activation(_linear([inputs, r * state], self._num_units, False))\n        new_h = u * state + (1 - u) * c\n        return new_h, new_h\n\n\n_LSTMStateTuple = collections.namedtuple(""LSTMStateTuple"", (""c"", ""h""))\n\n\nclass LSTMStateTuple(_LSTMStateTuple):\n    """"""Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.\n    Stores two elements: `(c, h)`, in that order.\n    Only used when `state_is_tuple=True`.\n    """"""\n    __slots__ = ()\n\n    @property\n    def dtype(self):\n        (c, h) = self\n        if c.dtype != h.dtype:\n            raise TypeError(""Inconsistent internal state: %s vs %s"" %\n                                            (str(c.dtype), str(h.dtype)))\n        return c.dtype\n\n\nclass BasicLSTMCell(RNNCell):\n    """"""Basic LSTM recurrent network cell.\n    The implementation is based on: http://arxiv.org/abs/1409.2329.\n    We add forget_bias (default: 1) to the biases of the forget gate in order to\n    reduce the scale of forgetting in the beginning of the training.\n    It does not allow cell clipping, a projection layer, and does not\n    use peep-hole connections: it is the basic baseline.\n    For advanced models, please use the full @{tf.nn.rnn_cell.LSTMCell}\n    that follows.\n    """"""\n\n    def __init__(self, num_units, forget_bias=1.0,\n                             state_is_tuple=True, activation=None, reuse=None):\n        """"""Initialize the basic LSTM cell.\n        Args:\n            num_units: int, The number of units in the LSTM cell.\n            forget_bias: float, The bias added to forget gates (see above).\n            state_is_tuple: If True, accepted and returned states are 2-tuples of\n                the `c_state` and `m_state`.    If False, they are concatenated\n                along the column axis.    The latter behavior will soon be deprecated.\n            activation: Activation function of the inner states.    Default: `tanh`.\n            reuse: (optional) Python boolean describing whether to reuse variables\n                in an existing scope.    If not `True`, and the existing scope already has\n                the given variables, an error is raised.\n        """"""\n        super(BasicLSTMCell, self).__init__(_reuse=reuse)\n        if not state_is_tuple:\n            logging.warn(""%s: Using a concatenated state is slower and will soon be ""\n                                     ""deprecated.    Use state_is_tuple=True."", self)\n        self._num_units = num_units\n        self._forget_bias = forget_bias\n        self._state_is_tuple = state_is_tuple\n        self._activation = activation or math_ops.tanh\n\n    @property\n    def state_size(self):\n        return (LSTMStateTuple(self._num_units, self._num_units)\n                        if self._state_is_tuple else 2 * self._num_units)\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def call(self, inputs, state):\n        """"""Long short-term memory cell (LSTM).""""""\n        sigmoid = math_ops.sigmoid\n        # Parameters of gates are concatenated into one multiply for efficiency.\n        if self._state_is_tuple:\n            c, h = state\n        else:\n            c, h = array_ops.split(value=state, num_or_size_splits=2, axis=1)\n\n        #todo: calculate new_c and new_h according to LSTM\n        # Ref to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py\n        gate_input = _linear([inputs, h], 4 * self._num_units, bias=True)\n        i, f, o, c_ = array_ops.split(value=gate_input, num_or_size_splits=4, axis=1)\n        new_c = sigmoid(f + self._forget_bias) * c + sigmoid(i) * self._activation(c_)\n        new_h = sigmoid(o) * self._activation(new_c)\n\n        if self._state_is_tuple:\n            new_state = LSTMStateTuple(new_c, new_h)\n        else:\n            new_state = array_ops.concat([new_c, new_h], 1)\n        return new_h, new_state\n\n\nclass MultiRNNCell(RNNCell):\n    """"""RNN cell composed sequentially of multiple simple cells.""""""\n\n    def __init__(self, cells, state_is_tuple=True):\n        """"""Create a RNN cell composed sequentially of a number of RNNCells.\n        Args:\n            cells: list of RNNCells that will be composed in this order.\n            state_is_tuple: If True, accepted and returned states are n-tuples, where\n                `n = len(cells)`.    If False, the states are all\n                concatenated along the column axis.    This latter behavior will soon be\n                deprecated.\n        Raises:\n            ValueError: if cells is empty (not allowed), or at least one of the cells\n                returns a state tuple but the flag `state_is_tuple` is `False`.\n        """"""\n        super(MultiRNNCell, self).__init__()\n        if not cells:\n            raise ValueError(""Must specify at least one cell for MultiRNNCell."")\n        if not nest.is_sequence(cells):\n            raise TypeError(\n                    ""cells must be a list or tuple, but saw: %s."" % cells)\n\n        self._cells = cells\n        self._state_is_tuple = state_is_tuple\n        if not state_is_tuple:\n            if any(nest.is_sequence(c.state_size) for c in self._cells):\n                raise ValueError(""Some cells return tuples of states, but the flag ""\n                                                 ""state_is_tuple is not set.    State sizes are: %s""\n                                                 % str([c.state_size for c in self._cells]))\n\n    @property\n    def state_size(self):\n        if self._state_is_tuple:\n            return tuple(cell.state_size for cell in self._cells)\n        else:\n            return sum([cell.state_size for cell in self._cells])\n\n    @property\n    def output_size(self):\n        return self._cells[-1].output_size\n\n    def zero_state(self, batch_size, dtype):\n        with ops.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n            if self._state_is_tuple:\n                return tuple(cell.zero_state(batch_size, dtype) for cell in self._cells)\n            else:\n                # We know here that state_size of each cell is not a tuple and\n                # presumably does not contain TensorArrays or anything else fancy\n                return super(MultiRNNCell, self).zero_state(batch_size, dtype)\n\n    def call(self, inputs, state):\n        """"""Run this multi-layer cell on inputs, starting from state.""""""\n        cur_state_pos = 0\n        cur_inp = inputs\n        new_states = []\n        for i, cell in enumerate(self._cells):\n            with vs.variable_scope(""cell_%d"" % i):\n                if self._state_is_tuple:\n                    if not nest.is_sequence(state):\n                        raise ValueError(\n                                ""Expected state to be a tuple of length %d, but received: %s"" %\n                                (len(self.state_size), state))\n                    cur_state = state[i]\n                else:\n                    cur_state = array_ops.slice(state, [0, cur_state_pos],\n                                                                            [-1, cell.state_size])\n                    cur_state_pos += cell.state_size\n                cur_inp, new_state = cell(cur_inp, cur_state)\n                new_states.append(new_state)\n\n        new_states = (tuple(new_states) if self._state_is_tuple else\n                                    array_ops.concat(new_states, 1))\n\n        return cur_inp, new_states\n\n\n'"
hw2/codes/res/large/run_cnn.py,6,"b'from network import Network\nfrom layers import Relu, Linear, Conv2D, AvgPool2D, Reshape\nfrom utils import LOG_INFO\nfrom loss import EuclideanLoss, SoftmaxCrossEntropyLoss\nfrom solve_net import train_net, test_net, getvis\nfrom load_data import load_mnist_4d\nimport numpy as np\nimport cv2\n# import matplotlib.pyplot as plt\ndef vis_square(data, id):\n    """"""Take an array of shape (n, height, width) or (n, height, width, 3)\n       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)""""""\n    \n    # normalize data for display\n    data = (data - data.min()) / (data.max() - data.min())\n    # print(data)\n    # force the number of filters to be square\n    n = int(np.ceil(np.sqrt(data.shape[0])))\n    padding = (((0, n ** 2 - data.shape[0]),\n               (0, 1), (0, 1))                 # add some space between filters\n               + ((0, 0),) * (data.ndim - 3))  # don\'t pad the last dimension (if there is one)\n    data = np.pad(data, padding, mode=\'constant\', constant_values=1)  # pad with ones (white)\n    \n    # tile the filters into an image\n    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n    cv2.imwrite(\'try%d.png\'%id, data*255)\n    # print(data.shape)\n    # print(data)\n    # plt.savefig(\'try%d.pdf\'%id)\n    # plt.imshow(data)\n    # plt.axis(\'off\')\n\ntrain_data, test_data, train_label, test_label = load_mnist_4d(\'data\')\n\n# Your model defintion here\n# You should explore different model architecture\nmodel = Network()\nmodel.add(Conv2D(\'conv1\', 1, 12, 3, 1, 1))\nmodel.add(Relu(\'relu1\'))\nmodel.add(AvgPool2D(\'pool1\', 2, 0))  # output shape: N x 4 x 14 x 14\nmodel.add(Conv2D(\'conv2\', 12, 10, 3, 1, 1))\nmodel.add(Relu(\'relu2\'))\nmodel.add(AvgPool2D(\'pool2\', 2, 0))  # output shape: N x 4 x 7 x 7\nmodel.add(Reshape(\'flatten\', (-1, 49*10)))\nmodel.add(Linear(\'fc3\', 49*10, 10, 0.1))\n\n# loss = EuclideanLoss(name=\'loss\')\nloss = SoftmaxCrossEntropyLoss(name=\'loss\')\n\n# Training configuration\n# You should adjust these hyperparameters\n# NOTE: one iteration means model forward-backwards one batch of samples.\n#       one epoch means model has gone through all the training samples.\n#       \'disp_freq\' denotes number of iterations in one epoch to display information.\n\n# np.random.seed(1626)\nconfig = {\n    \'learning_rate\': 0.01,\n    \'weight_decay\': 0.000,\n    \'momentum\': 0.9,\n    \'batch_size\': 100,\n    \'max_epoch\': 100,\n    \'disp_freq\': 50,\n    \'test_epoch\': 1\n}\nloss_ = []\nacc_ = []\ntest_loss = np.zeros(2) + 100\nfor epoch in range(config[\'max_epoch\']):\n    vis = getvis(model, test_data, test_label)\n    for i in range(4):\n        vis_square(vis[i], i)\n    vis_square(model.layer_list[0].W.reshape(-1, model.layer_list[0].W.shape[2], model.layer_list[0].W.shape[3]), -1)\n    LOG_INFO(\'Training @ %d epoch...\' % (epoch))\n    a, b = train_net(model, loss, config, train_data, train_label, config[\'batch_size\'], config[\'disp_freq\'], 600000)\n    loss_.append(a)\n    acc_.append(b)\n    if epoch % config[\'test_epoch\'] == 0:\n        LOG_INFO(\'Testing @ %d epoch...\' % (epoch))\n        test_ = test_net(model, loss, test_data, test_label, 100)\n        if epoch % 10 == 0 and epoch > 0:\n        # if test_loss.min() < test_ and abs(test_loss.max() - test_loss.min()) / test_loss.min() < 0.01 or epoch % 7 == 0:\n            config[\'learning_rate\'] = max(config[\'learning_rate\']/2, 1e-6)\n            print(\'lr: \', config[\'learning_rate\'])\n        test_loss[1:] = test_loss[:-1].copy()\n        test_loss[0] = test_\nvis = getvis(model, test_data, test_label)\nfor i in range(4):\n    vis_square(vis[i], i)\nnp.save(\'loss\', loss_)\nnp.save(\'acc\', acc_)\n'"
