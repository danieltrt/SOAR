file_path,api_count,code
Machine-Learning-Advanced/coursework/check_test.py,14,"b'import unittest\nimport copy\nfrom IPython.display import Markdown, display\nimport numpy as np\nfrom frozenlake import FrozenLakeEnv\n\ndef printmd(string):\n    display(Markdown(string))\n\ndef policy_evaluation_soln(env, policy, gamma=1, theta=1e-8):\n    V = np.zeros(env.nS)\n    while True:\n        delta = 0\n        for s in range(env.nS):\n            Vs = 0\n            for a, action_prob in enumerate(policy[s]):\n                for prob, next_state, reward, done in env.P[s][a]:\n                    Vs += action_prob * prob * (reward + gamma * V[next_state])\n            delta = max(delta, np.abs(V[s]-Vs))\n            V[s] = Vs\n        if delta < theta:\n            break\n    return V\n\ndef q_from_v_soln(env, V, s, gamma=1):\n    q = np.zeros(env.nA)\n    for a in range(env.nA):\n        for prob, next_state, reward, done in env.P[s][a]:\n            q[a] += prob * (reward + gamma * V[next_state])\n    return q\n\ndef policy_improvement_soln(env, V, gamma=1):\n    policy = np.zeros([env.nS, env.nA]) / env.nA\n    for s in range(env.nS):\n        q = q_from_v_soln(env, V, s, gamma)\n        best_a = np.argwhere(q==np.max(q)).flatten()\n        policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a)\n    return policy\n\ndef policy_iteration_soln(env, gamma=1, theta=1e-8):\n    policy = np.ones([env.nS, env.nA]) / env.nA\n    while True:\n        V = policy_evaluation_soln(env, policy, gamma, theta)\n        new_policy = policy_improvement_soln(env, V)\n        if (new_policy == policy).all():\n            break;\n        policy = copy.copy(new_policy)\n    return policy, V\n\nenv = FrozenLakeEnv()\nrandom_policy = np.ones([env.nS, env.nA]) / env.nA\n\nclass Tests(unittest.TestCase):\n\n    def policy_evaluation_check(self, policy_evaluation):\n        soln = policy_evaluation_soln(env, random_policy)\n        to_check = policy_evaluation(env, random_policy)\n        np.testing.assert_array_almost_equal(soln, to_check)\n\n    def q_from_v_check(self, q_from_v):\n        V = policy_evaluation_soln(env, random_policy)\n        soln = np.zeros([env.nS, env.nA])\n        to_check = np.zeros([env.nS, env.nA])\n        for s in range(env.nS):\n            soln[s] = q_from_v_soln(env, V, s)\n            to_check[s] = q_from_v(env, V, s)\n        np.testing.assert_array_almost_equal(soln, to_check)\n\n    def policy_improvement_check(self, policy_improvement):\n        V = policy_evaluation_soln(env, random_policy)\n        new_policy = policy_improvement(env, V)\n        new_V = policy_evaluation_soln(env, new_policy)\n        self.assertTrue(np.all(new_V >= V))\n\n    def policy_iteration_check(self, policy_iteration):\n        policy_soln, _ = policy_iteration_soln(env)\n        policy_to_check, _ = policy_iteration(env)\n        soln = policy_evaluation_soln(env, policy_soln)\n        to_check = policy_evaluation_soln(env, policy_to_check)\n        np.testing.assert_array_almost_equal(soln, to_check)\n\n    def truncated_policy_iteration_check(self, truncated_policy_iteration):\n        self.policy_iteration_check(truncated_policy_iteration)\n\n    def value_iteration_check(self, value_iteration):\n        self.policy_iteration_check(value_iteration)\n\ncheck = Tests()\n\ndef run_check(check_name, func):\n    try:\n        getattr(check, check_name)(func)\n    except check.failureException as e:\n        printmd(\'**<span style=""color: red;"">PLEASE TRY AGAIN</span>**\')\n        return\n    printmd(\'**<span style=""color: green;"">PASSED</span>**\')'"
Machine-Learning-Advanced/coursework/frozenlake.py,2,"b'import numpy as np\nimport sys\nfrom six import StringIO, b\n\nfrom gym import utils\nfrom gym.envs.toy_text import discrete\n\nLEFT = 0\nDOWN = 1\nRIGHT = 2\nUP = 3\n\nMAPS = {\n    ""4x4"": [\n        ""SFFF"",\n        ""FHFH"",\n        ""FFFH"",\n        ""HFFG""\n    ],\n    ""8x8"": [\n        ""SFFFFFFF"",\n        ""FFFFFFFF"",\n        ""FFFHFFFF"",\n        ""FFFFFHFF"",\n        ""FFFHFFFF"",\n        ""FHHFFFHF"",\n        ""FHFFHFHF"",\n        ""FFFHFFFG""\n    ],\n}\n\nclass FrozenLakeEnv(discrete.DiscreteEnv):\n    """"""\n    Winter is here. You and your friends were tossing around a frisbee at the park\n    when you made a wild throw that left the frisbee out in the middle of the lake.\n    The water is mostly frozen, but there are a few holes where the ice has melted.\n    If you step into one of those holes, you\'ll fall into the freezing water.\n    At this time, there\'s an international frisbee shortage, so it\'s absolutely imperative that\n    you navigate across the lake and retrieve the disc.\n    However, the ice is slippery, so you won\'t always move in the direction you intend.\n    The surface is described using a grid like the following\n\n        SFFF\n        FHFH\n        FFFH\n        HFFG\n\n    S : starting point, safe\n    F : frozen surface, safe\n    H : hole, fall to your doom\n    G : goal, where the frisbee is located\n\n    The episode ends when you reach the goal or fall in a hole.\n    You receive a reward of 1 if you reach the goal, and zero otherwise.\n\n    """"""\n\n    metadata = {\'render.modes\': [\'human\', \'ansi\']}\n\n    def __init__(self, desc=None, map_name=""4x4"",is_slippery=True):\n        if desc is None and map_name is None:\n            raise ValueError(\'Must provide either desc or map_name\')\n        elif desc is None:\n            desc = MAPS[map_name]\n        self.desc = desc = np.asarray(desc,dtype=\'c\')\n        self.nrow, self.ncol = nrow, ncol = desc.shape\n\n        nA = 4\n        nS = nrow * ncol\n\n        isd = np.array(desc == b\'S\').astype(\'float64\').ravel()\n        isd /= isd.sum()\n\n        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n\n        def to_s(row, col):\n            return row*ncol + col\n        def inc(row, col, a):\n            if a==0: # left\n                col = max(col-1,0)\n            elif a==1: # down\n                row = min(row+1,nrow-1)\n            elif a==2: # right\n                col = min(col+1,ncol-1)\n            elif a==3: # up\n                row = max(row-1,0)\n            return (row, col)\n\n        for row in range(nrow):\n            for col in range(ncol):\n                s = to_s(row, col)\n                for a in range(4):\n                    li = P[s][a]\n                    letter = desc[row, col]\n                    if letter in b\'GH\':\n                        li.append((1.0, s, 0, True))\n                    else:\n                        if is_slippery:\n                            for b in [(a-1)%4, a, (a+1)%4]:\n                                newrow, newcol = inc(row, col, b)\n                                newstate = to_s(newrow, newcol)\n                                newletter = desc[newrow, newcol]\n                                done = bytes(newletter) in b\'GH\'\n                                rew = float(newletter == b\'G\')\n                                li.append((1.0/3.0, newstate, rew, done))\n                        else:\n                            newrow, newcol = inc(row, col, a)\n                            newstate = to_s(newrow, newcol)\n                            newletter = desc[newrow, newcol]\n                            done = bytes(newletter) in b\'GH\'\n                            rew = float(newletter == b\'G\')\n                            li.append((1.0, newstate, rew, done))\n        \n        # obtain one-step dynamics for dynamic programming setting\n        self.P = P\n\n        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n\n    def _render(self, mode=\'human\', close=False):\n        if close:\n            return\n        outfile = StringIO() if mode == \'ansi\' else sys.stdout\n\n        row, col = self.s // self.ncol, self.s % self.ncol\n        desc = self.desc.tolist()\n        desc = [[c.decode(\'utf-8\') for c in line] for line in desc]\n        desc[row][col] = utils.colorize(desc[row][col], ""red"", highlight=True)\n        if self.lastaction is not None:\n            outfile.write(""  ({})\\n"".format([""Left"",""Down"",""Right"",""Up""][self.lastaction]))\n        else:\n            outfile.write(""\\n"")\n        outfile.write(""\\n"".join(\'\'.join(line) for line in desc)+""\\n"")\n\n        if mode != \'human\':\n            return outfile\n'"
Machine-Learning-Advanced/coursework/plot_utils.py,3,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_values(V):\n\t# reshape value function\n\tV_sq = np.reshape(V, (4,4))\n\n\t# plot the state-value function\n\tfig = plt.figure(figsize=(6, 6))\n\tax = fig.add_subplot(111)\n\tim = ax.imshow(V_sq, cmap='cool')\n\tfor (j,i),label in np.ndenumerate(V_sq):\n\t    ax.text(i, j, np.round(label, 5), ha='center', va='center', fontsize=14)\n\tplt.tick_params(bottom='off', left='off', labelbottom='off', labelleft='off')\n\tplt.title('State-Value Function')\n\tplt.show()"""
Machine-Learning-Basics/coursework/dbscan_lab_helper.py,4,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom itertools import cycle, islice\nfrom sklearn import cluster\n\nfigsize = (10,10)\npoint_size=150\npoint_border=0.8\n\n\ndef plot_dataset(dataset, xlim=(-15, 15), ylim=(-15, 15)):\n    plt.figure(figsize=figsize)\n    plt.scatter(dataset[:,0], dataset[:,1], s=point_size, color=""#00B3E9"", edgecolor=\'black\', lw=point_border)\n    plt.xlim(xlim)\n    plt.ylim(ylim)\n    plt.show()\n    \ndef plot_clustered_dataset(dataset, y_pred, xlim=(-15, 15), ylim=(-15, 15), neighborhood=False, epsilon=0.5):\n\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    colors = np.array(list(islice(cycle([\'#df8efd\', \'#78c465\', \'#ff8e34\',\n                                     \'#f65e97\', \'#a65628\', \'#984ea3\',\n                                     \'#999999\', \'#e41a1c\', \'#dede00\']),\n                              int(max(y_pred) + 1))))\n    colors = np.append(colors, \'#BECBD6\')\n    \n    \n    if neighborhood:\n        for point in dataset:\n            circle1 = plt.Circle(point, epsilon, color=\'#666666\', fill=False, zorder=0, alpha=0.3)\n            ax.add_artist(circle1)\n\n    ax.scatter(dataset[:, 0], dataset[:, 1], s=point_size, color=colors[y_pred], zorder=10, edgecolor=\'black\', lw=point_border)\n    plt.xlim(xlim)\n    plt.ylim(ylim)\n    plt.show()\n    \ndef plot_dbscan_grid(dataset, eps_values, min_samples_values):\n    \n    fig = plt.figure(figsize=(16, 20))\n    plt.subplots_adjust(left=.02, right=.98, bottom=0.001, top=.96, wspace=.05,\n                        hspace=0.25)\n\n\n    plot_num = 1\n\n    for i, min_samples in enumerate(min_samples_values):\n        for j, eps in enumerate(eps_values):\n            ax = fig.add_subplot( len(min_samples_values) , len(eps_values), plot_num)\n\n            dbscan = cluster.DBSCAN(eps=eps, min_samples=min_samples)\n            y_pred_2 = dbscan.fit_predict(dataset)\n\n            colors = np.array(list(islice(cycle([\'#df8efd\', \'#78c465\', \'#ff8e34\',\n                                                 \'#f65e97\', \'#a65628\', \'#984ea3\',\n                                                 \'#999999\', \'#e41a1c\', \'#dede00\']),\n                                          int(max(y_pred_2) + 1))))\n            colors = np.append(colors, \'#BECBD6\')\n\n\n            for point in dataset:\n                circle1 = plt.Circle(point, eps, color=\'#666666\', fill=False, zorder=0, alpha=0.3)\n                ax.add_artist(circle1)\n\n            ax.text(0, -0.03, \'Epsilon: {} \\nMin_samples: {}\'.format(eps, min_samples), transform=ax.transAxes, fontsize=16, va=\'top\')\n            ax.scatter(dataset[:, 0], dataset[:, 1], s=50, color=colors[y_pred_2], zorder=10, edgecolor=\'black\', lw=0.5)\n\n\n            plt.xticks(())\n            plt.yticks(())\n            plt.xlim(-14, 5)\n            plt.ylim(-12, 7)\n\n            plot_num = plot_num + 1\n\n    plt.show()'"
Machine-Learning-Basics/coursework/helper.py,4,"b'import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import mean_squared_error\nimport itertools\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\ndef draw_scatterplot(x_data, x_label, y_data, y_label):\n    fig = plt.figure(figsize=(8,8))\n    ax = fig.add_subplot(111)\n\n    plt.xlim(0, 5)\n    plt.ylim(0, 5)\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(y_label)\n    ax.scatter(x_data, y_data, s=30)\n\n\ndef draw_clusters(biased_dataset, predictions, cmap=\'viridis\'):\n    fig = plt.figure(figsize=(8,8))\n    ax = fig.add_subplot(111)\n    plt.xlim(0, 5)\n    plt.ylim(0, 5)\n    ax.set_xlabel(\'Avg scifi rating\')\n    ax.set_ylabel(\'Avg romance rating\')\n    \n\n    clustered = pd.concat([biased_dataset.reset_index(), pd.DataFrame({\'group\':predictions})], axis=1)\n    plt.scatter(clustered[\'avg_scifi_rating\'], clustered[\'avg_romance_rating\'], c=clustered[\'group\'], s=20, cmap=cmap)\n\n        \ndef clustering_errors(k, data):\n    kmeans = KMeans(n_clusters=k).fit(data)\n    predictions = kmeans.predict(data)\n    #cluster_centers = kmeans.cluster_centers_\n    # errors = [mean_squared_error(row, cluster_centers[cluster]) for row, cluster in zip(data.values, predictions)]\n    # return sum(errors)\n    silhouette_avg = silhouette_score(data, predictions)\n    return silhouette_avg\n\ndef sparse_clustering_errors(k, data):\n    kmeans = KMeans(n_clusters=k).fit(data)\n    predictions = kmeans.predict(data)\n    cluster_centers = kmeans.cluster_centers_\n    errors = [mean_squared_error(row, cluster_centers[cluster]) for row, cluster in zip(data, predictions)]\n    return sum(errors)\n\n\ndef get_genre_ratings(ratings, movies, genres, column_names):\n    genre_ratings = pd.DataFrame()\n    for genre in genres:        \n        genre_movies = movies[movies[\'genres\'].str.contains(genre) ]\n        avg_genre_votes_per_user = ratings[ratings[\'movieId\'].isin(genre_movies[\'movieId\'])].loc[:, [\'userId\', \'rating\']].groupby([\'userId\'])[\'rating\'].mean().round(2)\n        \n        genre_ratings = pd.concat([genre_ratings, avg_genre_votes_per_user], axis=1)\n        \n    genre_ratings.columns = column_names\n    return genre_ratings\n    \ndef get_dataset_3(movies, ratings, genre_ratings):    \n    # Extract action ratings from dataset\n    action_movies = movies[movies[\'genres\'].str.contains(\'Action\') ]\n    # Get average vote on action movies per user\n    avg_action_votes_per_user = ratings[ratings[\'movieId\'].isin(action_movies[\'movieId\'])].loc[:, [\'userId\', \'rating\']].groupby([\'userId\'])[\'rating\'].mean().round(2)\n    # Add action ratings to romance and scifi in dataframe\n    genre_ratings_3 = pd.concat([genre_ratings, avg_action_votes_per_user], axis=1)\n    genre_ratings_3.columns = [\'avg_romance_rating\', \'avg_scifi_rating\', \'avg_action_rating\']\n    \n    # Let\'s bias the dataset a little so our clusters can separate scifi vs romance more easily\n    b1 = 3.2\n    b2 = 2.5\n    biased_dataset_3 = genre_ratings_3[((genre_ratings_3[\'avg_romance_rating\'] < b1 - 0.2) & (genre_ratings_3[\'avg_scifi_rating\'] > b2)) | ((genre_ratings_3[\'avg_scifi_rating\'] < b1) & (genre_ratings_3[\'avg_romance_rating\'] > b2))]\n    biased_dataset_3 = pd.concat([biased_dataset_3[:300], genre_ratings_3[:2]])\n    biased_dataset_3 = pd.DataFrame(biased_dataset_3.to_records())\n    \n    return biased_dataset_3\n\ndef draw_clusters_3d(biased_dataset_3, predictions):\n    fig = plt.figure(figsize=(8,8))\n    ax = fig.add_subplot(111)\n\n    plt.xlim(0, 5)\n    plt.ylim(0, 5)\n    ax.set_xlabel(\'Avg scifi rating\')\n    ax.set_ylabel(\'Avg romance rating\')\n\n    clustered = pd.concat([biased_dataset_3.reset_index(), pd.DataFrame({\'group\':predictions})], axis=1)\n\n    colors = itertools.cycle(plt.rcParams[""axes.prop_cycle""].by_key()[""color""])\n\n    for g in clustered.group.unique():\n        color = next(colors)\n        for index, point in clustered[clustered.group == g].iterrows():\n            if point[\'avg_action_rating\'].astype(float) > 3: \n                size = 50\n            else:\n                size = 15\n            plt.scatter(point[\'avg_scifi_rating\'], \n                        point[\'avg_romance_rating\'], \n                        s=size, \n                        color=color)\n    \ndef draw_movie_clusters(clustered, max_users, max_movies):\n    c=1\n    for cluster_id in clustered.group.unique():\n        # To improve visibility, we\'re showing at most max_users users and max_movies movies per cluster.\n        # You can change these values to see more users & movies per cluster\n        d = clustered[clustered.group == cluster_id].drop([\'index\', \'group\'], axis=1)\n        n_users_in_cluster = d.shape[0]\n        \n        d = sort_by_rating_density(d, max_movies, max_users)\n        \n        d = d.reindex_axis(d.mean().sort_values(ascending=False).index, axis=1)\n        d = d.reindex_axis(d.count(axis=1).sort_values(ascending=False).index)\n        d = d.iloc[:max_users, :max_movies]\n        n_users_in_plot = d.shape[0]\n        \n        # We\'re only selecting to show clusters that have more than 9 users, otherwise, they\'re less interesting\n        if len(d) > 9:\n            print(\'cluster # {}\'.format(cluster_id))\n            print(\'# of users in cluster: {}.\'.format(n_users_in_cluster), \'# of users in plot: {}\'.format(n_users_in_plot))\n            fig = plt.figure(figsize=(15,4))\n            ax = plt.gca()\n\n            ax.invert_yaxis()\n            ax.xaxis.tick_top()\n            labels = d.columns.str[:40]\n\n            ax.set_yticks(np.arange(d.shape[0]) , minor=False)\n            ax.set_xticks(np.arange(d.shape[1]) , minor=False)\n\n            ax.set_xticklabels(labels, minor=False)\n                        \n            ax.get_yaxis().set_visible(False)\n\n            # Heatmap\n            heatmap = plt.imshow(d, vmin=0, vmax=5, aspect=\'auto\')\n\n            ax.set_xlabel(\'movies\')\n            ax.set_ylabel(\'User id\')\n\n            divider = make_axes_locatable(ax)\n            cax = divider.append_axes(""right"", size=""5%"", pad=0.05)\n\n            # Color bar\n            cbar = fig.colorbar(heatmap, ticks=[5, 4, 3, 2, 1, 0], cax=cax)\n            cbar.ax.set_yticklabels([\'5 stars\', \'4 stars\',\'3 stars\',\'2 stars\',\'1 stars\',\'0 stars\'])\n\n            plt.setp(ax.get_xticklabels(), rotation=90, fontsize=9)\n            plt.tick_params(axis=\'both\', which=\'both\', bottom=\'off\', top=\'off\', left=\'off\', labelbottom=\'off\', labelleft=\'off\') \n            #print(\'cluster # {} \\n(Showing at most {} users and {} movies)\'.format(cluster_id, max_users, max_movies))\n\n            plt.show()\n\n\n            # Let\'s only show 5 clusters\n            # Remove the next three lines if you want to see all the clusters\n            # Contribution welcomed: Pythonic way of achieving this\n            # c = c+1\n            # if c > 6:\n            #    break\n                \ndef get_most_rated_movies(user_movie_ratings, max_number_of_movies):\n    # 1- Count\n    user_movie_ratings = user_movie_ratings.append(user_movie_ratings.count(), ignore_index=True)\n    # 2- sort\n    user_movie_ratings_sorted = user_movie_ratings.sort_values(len(user_movie_ratings)-1, axis=1, ascending=False)\n    user_movie_ratings_sorted = user_movie_ratings_sorted.drop(user_movie_ratings_sorted.tail(1).index)\n    # 3- slice\n    most_rated_movies = user_movie_ratings_sorted.iloc[:, :max_number_of_movies]\n    return most_rated_movies\n\ndef get_users_who_rate_the_most(most_rated_movies, max_number_of_movies):\n    # Get most voting users\n    # 1- Count\n    most_rated_movies[\'counts\'] = pd.Series(most_rated_movies.count(axis=1))\n    # 2- Sort\n    most_rated_movies_users = most_rated_movies.sort_values(\'counts\', ascending=False)\n    # 3- Slice\n    most_rated_movies_users_selection = most_rated_movies_users.iloc[:max_number_of_movies, :]\n    most_rated_movies_users_selection = most_rated_movies_users_selection.drop([\'counts\'], axis=1)\n    \n    return most_rated_movies_users_selection\n\ndef sort_by_rating_density(user_movie_ratings, n_movies, n_users):\n    most_rated_movies = get_most_rated_movies(user_movie_ratings, n_movies)\n    most_rated_movies = get_users_who_rate_the_most(most_rated_movies, n_users)\n    return most_rated_movies\n    \ndef draw_movies_heatmap(most_rated_movies_users_selection, axis_labels=True):\n    \n    # Reverse to match the order of the printed dataframe\n    #most_rated_movies_users_selection = most_rated_movies_users_selection.iloc[::-1]\n    \n    fig = plt.figure(figsize=(15,4))\n    ax = plt.gca()\n    \n    # Draw heatmap\n    heatmap = ax.imshow(most_rated_movies_users_selection,  interpolation=\'nearest\', vmin=0, vmax=5, aspect=\'auto\')\n\n    if axis_labels:\n        ax.set_yticks(np.arange(most_rated_movies_users_selection.shape[0]) , minor=False)\n        ax.set_xticks(np.arange(most_rated_movies_users_selection.shape[1]) , minor=False)\n        ax.invert_yaxis()\n        ax.xaxis.tick_top()\n        labels = most_rated_movies_users_selection.columns.str[:40]\n        ax.set_xticklabels(labels, minor=False)\n        ax.set_yticklabels(most_rated_movies_users_selection.index, minor=False)\n        plt.setp(ax.get_xticklabels(), rotation=90)\n    else:\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n    \n    ax.grid(False)\n    ax.set_ylabel(\'User id\')\n\n    # Separate heatmap from color bar\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(""right"", size=""5%"", pad=0.05)\n\n    # Color bar\n    cbar = fig.colorbar(heatmap, ticks=[5, 4, 3, 2, 1, 0], cax=cax)\n    cbar.ax.set_yticklabels([\'5 stars\', \'4 stars\',\'3 stars\',\'2 stars\',\'1 stars\',\'0 stars\'])\n\n\n\n    plt.show()\n    \ndef bias_genre_rating_dataset(genre_ratings, score_limit_1, score_limit_2):\n    biased_dataset = genre_ratings[((genre_ratings[\'avg_romance_rating\'] < score_limit_1 - 0.2) & (genre_ratings[\'avg_scifi_rating\'] > score_limit_2)) | ((genre_ratings[\'avg_scifi_rating\'] < score_limit_1) & (genre_ratings[\'avg_romance_rating\'] > score_limit_2))]\n    biased_dataset = pd.concat([biased_dataset[:300], genre_ratings[:2]])\n    biased_dataset = pd.DataFrame(biased_dataset.to_records())\n    return biased_dataset'"
Machine-Learning-Advanced/coursework/dermatologist-ai/get_results.py,4,"b'import itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sys\n\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\n\ndef plot_roc_auc(y_true, y_pred):\n    """"""\n    This function plots the ROC curves and provides the scores.\n    """"""\n\n    # initialize dictionaries and array\n    fpr = dict()\n    tpr = dict()\n    roc_auc = np.zeros(3)\n    \n    # prepare for figure\n    plt.figure()\n    colors = [\'aqua\', \'cornflowerblue\']\n\n    # for both classification tasks (categories 1 and 2)\n    for i in range(2):\n        # obtain ROC curve\n        fpr[i], tpr[i], _ = roc_curve(y_true[:,i], y_pred[:,i])\n        # obtain ROC AUC\n        roc_auc[i] = auc(fpr[i], tpr[i])\n        # plot ROC curve\n        plt.plot(fpr[i], tpr[i], color=colors[i], lw=2,\n                 label=\'ROC curve for task {d} (area = {f:.2f})\'.format(d=i+1, f=roc_auc[i]))\n    # get score for category 3\n    roc_auc[2] = np.average(roc_auc[:2])\n    \n    # format figure\n    plt.plot([0, 1], [0, 1], \'k--\', lw=2)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel(\'False Positive Rate\')\n    plt.ylabel(\'True Positive Rate\')\n    plt.title(\'ROC curves\')\n    plt.legend(loc=""lower right"")\n    plt.show()\n    \n    # print scores\n    for i in range(3):\n        print(\'Category {d} Score: {f:.3f}\'. format(d=i+1, f=roc_auc[i]))\n\ndef plot_confusion_matrix(y_true, y_pred, thresh, classes):\n    """"""\n    This function plots the (normalized) confusion matrix.\n    """"""\n\n    # obtain class predictions from probabilities\n    y_pred = (y_pred>=thresh)*1\n    # obtain (unnormalized) confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # normalize confusion matrix\n    cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure()\n    plt.imshow(cm, interpolation=\'nearest\', cmap=plt.cm.Blues)\n    plt.title(\'Confusion matrix\')\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], \'.2f\'),\n                 horizontalalignment=""center"",\n                 color=""white"" if cm[i, j] > thresh else ""black"")\n\n    plt.tight_layout()\n    plt.ylabel(\'True label\')\n    plt.xlabel(\'Predicted label\')\n    plt.show()\n\nif __name__ == ""__main__"":\n\n    preds_path = sys.argv[1]\n    if len(sys.argv)==3:\n        thresh = float(sys.argv[2])\n    else:\n        thresh = 0.5\n\n    # get ground truth labels for test dataset\n    truth = pd.read_csv(\'ground_truth.csv\')\n    y_true = truth.as_matrix(columns=[""task_1"", ""task_2""])\n\n    # get model predictions for test dataset\n    y_pred = pd.read_csv(preds_path)\n    y_pred = y_pred.as_matrix(columns=[""task_1"", ""task_2""])\n\n    # plot ROC curves and print scores\n    plot_roc_auc(y_true, y_pred)\n    # plot confusion matrix\n    classes = [\'benign\', \'malignant\']\n    plot_confusion_matrix(y_true[:,0], y_pred[:,0], thresh, classes)\n'"
Machine-Learning-Advanced/projects/dog-project/extract_bottleneck_features.py,0,"b""def extract_VGG16(tensor):\n\tfrom keras.applications.vgg16 import VGG16, preprocess_input\n\treturn VGG16(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n\ndef extract_VGG19(tensor):\n\tfrom keras.applications.vgg19 import VGG19, preprocess_input\n\treturn VGG19(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n\ndef extract_Resnet50(tensor):\n\tfrom keras.applications.resnet50 import ResNet50, preprocess_input\n\treturn ResNet50(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n\ndef extract_Xception(tensor):\n\tfrom keras.applications.xception import Xception, preprocess_input\n\treturn Xception(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n\ndef extract_InceptionV3(tensor):\n\tfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\n\treturn InceptionV3(weights='imagenet', include_top=False).predict(preprocess_input(tensor))"""
Machine-Learning-Basics/projects/customer_segments/visuals.py,2,"b'###########################################\n# Suppress matplotlib user warnings\n# Necessary for newer version of matplotlib\nimport warnings\nwarnings.filterwarnings(""ignore"", category = UserWarning, module = ""matplotlib"")\n#\n# Display inline matplotlib plots with IPython\nfrom IPython import get_ipython\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\n###########################################\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport pandas as pd\nimport numpy as np\n\ndef pca_results(good_data, pca):\n\t\'\'\'\n\tCreate a DataFrame of the PCA results\n\tIncludes dimension feature weights and explained variance\n\tVisualizes the PCA results\n\t\'\'\'\n\n\t# Dimension indexing\n\tdimensions = dimensions = [\'Dimension {}\'.format(i) for i in range(1,len(pca.components_)+1)]\n\n\t# PCA components\n\tcomponents = pd.DataFrame(np.round(pca.components_, 4), columns = list(good_data.keys()))\n\tcomponents.index = dimensions\n\n\t# PCA explained variance\n\tratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n\tvariance_ratios = pd.DataFrame(np.round(ratios, 4), columns = [\'Explained Variance\'])\n\tvariance_ratios.index = dimensions\n\n\t# Create a bar plot visualization\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Plot the feature weights as a function of the components\n\tcomponents.plot(ax = ax, kind = \'bar\');\n\tax.set_ylabel(""Feature Weights"")\n\tax.set_xticklabels(dimensions, rotation=0)\n\n\n\t# Display the explained variance ratios\n\tfor i, ev in enumerate(pca.explained_variance_ratio_):\n\t\tax.text(i-0.40, ax.get_ylim()[1] + 0.05, ""Explained Variance\\n          %.4f""%(ev))\n\n\t# Return a concatenated DataFrame\n\treturn pd.concat([variance_ratios, components], axis = 1)\n\ndef cluster_results(reduced_data, preds, centers, pca_samples):\n\t\'\'\'\n\tVisualizes the PCA-reduced cluster data in two dimensions\n\tAdds cues for cluster centers and student-selected sample data\n\t\'\'\'\n\n\tpredictions = pd.DataFrame(preds, columns = [\'Cluster\'])\n\tplot_data = pd.concat([predictions, reduced_data], axis = 1)\n\n\t# Generate the cluster plot\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Color map\n\tcmap = cm.get_cmap(\'gist_rainbow\')\n\n\t# Color the points based on assigned cluster\n\tfor i, cluster in plot_data.groupby(\'Cluster\'):   \n\t    cluster.plot(ax = ax, kind = \'scatter\', x = \'Dimension 1\', y = \'Dimension 2\', \\\n\t                 color = cmap((i)*1.0/(len(centers)-1)), label = \'Cluster %i\'%(i), s=30);\n\n\t# Plot centers with indicators\n\tfor i, c in enumerate(centers):\n\t    ax.scatter(x = c[0], y = c[1], color = \'white\', edgecolors = \'black\', \\\n\t               alpha = 1, linewidth = 2, marker = \'o\', s=200);\n\t    ax.scatter(x = c[0], y = c[1], marker=\'$%d$\'%(i), alpha = 1, s=100);\n\n\t# Plot transformed sample points \n\tax.scatter(x = pca_samples[:,0], y = pca_samples[:,1], \\\n\t           s = 150, linewidth = 4, color = \'black\', marker = \'x\');\n\n\t# Set plot title\n\tax.set_title(""Cluster Learning on PCA-Reduced Data - Centroids Marked by Number\\nTransformed Sample Data Marked by Black Cross"");\n\n\ndef biplot(good_data, reduced_data, pca):\n    \'\'\'\n    Produce a biplot that shows a scatterplot of the reduced\n    data and the projections of the original features.\n    \n    good_data: original data, before transformation.\n               Needs to be a pandas dataframe with valid column names\n    reduced_data: the reduced data (the first two dimensions are plotted)\n    pca: pca object that contains the components_ attribute\n\n    return: a matplotlib AxesSubplot object (for any additional customization)\n    \n    This procedure is inspired by the script:\n    https://github.com/teddyroland/python-biplot\n    \'\'\'\n\n    fig, ax = plt.subplots(figsize = (14,8))\n    # scatterplot of the reduced data    \n    ax.scatter(x=reduced_data.loc[:, \'Dimension 1\'], y=reduced_data.loc[:, \'Dimension 2\'], \n        facecolors=\'b\', edgecolors=\'b\', s=70, alpha=0.5)\n    \n    feature_vectors = pca.components_.T\n\n    # we use scaling factors to make the arrows easier to see\n    arrow_size, text_pos = 7.0, 8.0,\n\n    # projections of the original features\n    for i, v in enumerate(feature_vectors):\n        ax.arrow(0, 0, arrow_size*v[0], arrow_size*v[1], \n                  head_width=0.2, head_length=0.2, linewidth=2, color=\'red\')\n        ax.text(v[0]*text_pos, v[1]*text_pos, good_data.columns[i], color=\'black\', \n                 ha=\'center\', va=\'center\', fontsize=18)\n\n    ax.set_xlabel(""Dimension 1"", fontsize=14)\n    ax.set_ylabel(""Dimension 2"", fontsize=14)\n    ax.set_title(""PC plane with original feature projections."", fontsize=16);\n    return ax\n    \n\ndef channel_results(reduced_data, outliers, pca_samples):\n\t\'\'\'\n\tVisualizes the PCA-reduced cluster data in two dimensions using the full dataset\n\tData is labeled by ""Channel"" and cues added for student-selected sample data\n\t\'\'\'\n\n\t# Check that the dataset is loadable\n\ttry:\n\t    full_data = pd.read_csv(""customers.csv"")\n\texcept:\n\t    print(""Dataset could not be loaded. Is the file missing?"")       \n\t    return False\n\n\t# Create the Channel DataFrame\n\tchannel = pd.DataFrame(full_data[\'Channel\'], columns = [\'Channel\'])\n\tchannel = channel.drop(channel.index[outliers]).reset_index(drop = True)\n\tlabeled = pd.concat([reduced_data, channel], axis = 1)\n\t\n\t# Generate the cluster plot\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Color map\n\tcmap = cm.get_cmap(\'gist_rainbow\')\n\n\t# Color the points based on assigned Channel\n\tlabels = [\'Hotel/Restaurant/Cafe\', \'Retailer\']\n\tgrouped = labeled.groupby(\'Channel\')\n\tfor i, channel in grouped:   \n\t    channel.plot(ax = ax, kind = \'scatter\', x = \'Dimension 1\', y = \'Dimension 2\', \\\n\t                 color = cmap((i-1)*1.0/2), label = labels[i-1], s=30);\n\t    \n\t# Plot transformed sample points   \n\tfor i, sample in enumerate(pca_samples):\n\t\tax.scatter(x = sample[0], y = sample[1], \\\n\t           s = 200, linewidth = 3, color = \'black\', marker = \'o\', facecolors = \'none\');\n\t\tax.scatter(x = sample[0]+0.25, y = sample[1]+0.3, marker=\'$%d$\'%(i), alpha = 1, s=125);\n\n\t# Set plot title\n\tax.set_title(""PCA-Reduced Data Labeled by \'Channel\'\\nTransformed Sample Data Circled"");'"
Machine-Learning-Basics/projects/finding_donors/visuals.py,5,"b'###########################################\n# Suppress matplotlib user warnings\n# Necessary for newer version of matplotlib\nimport warnings\nwarnings.filterwarnings(""ignore"", category = UserWarning, module = ""matplotlib"")\n#\n# Display inline matplotlib plots with IPython\nfrom IPython import get_ipython\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\n###########################################\n\nimport matplotlib.pyplot as pl\nimport matplotlib.patches as mpatches\nimport numpy as np\nimport pandas as pd\nfrom time import time\nfrom sklearn.metrics import f1_score, accuracy_score\n\n\ndef distribution(data, transformed = False):\n    """"""\n    Visualization code for displaying skewed distributions of features\n    """"""\n    \n    # Create figure\n    fig = pl.figure(figsize = (11,5));\n\n    # Skewed feature plotting\n    for i, feature in enumerate([\'capital-gain\',\'capital-loss\']):\n        ax = fig.add_subplot(1, 2, i+1)\n        ax.hist(data[feature], bins = 25, color = \'#00A0A0\')\n        ax.set_title(""\'%s\' Feature Distribution""%(feature), fontsize = 14)\n        ax.set_xlabel(""Value"")\n        ax.set_ylabel(""Number of Records"")\n        ax.set_ylim((0, 2000))\n        ax.set_yticks([0, 500, 1000, 1500, 2000])\n        ax.set_yticklabels([0, 500, 1000, 1500, "">2000""])\n\n    # Plot aesthetics\n    if transformed:\n        fig.suptitle(""Log-transformed Distributions of Continuous Census Data Features"", \\\n            fontsize = 16, y = 1.03)\n    else:\n        fig.suptitle(""Skewed Distributions of Continuous Census Data Features"", \\\n            fontsize = 16, y = 1.03)\n\n    fig.tight_layout()\n    fig.show()\n\n\ndef evaluate(results, accuracy, f1):\n    """"""\n    Visualization code to display results of various learners.\n    \n    inputs:\n      - learners: a list of supervised learners\n      - stats: a list of dictionaries of the statistic results from \'train_predict()\'\n      - accuracy: The score for the naive predictor\n      - f1: The score for the naive predictor\n    """"""\n  \n    # Create figure\n    fig, ax = pl.subplots(2, 3, figsize = (11,7))\n\n    # Constants\n    bar_width = 0.3\n    colors = [\'#A00000\',\'#00A0A0\',\'#00A000\']\n    \n    # Super loop to plot four panels of data\n    for k, learner in enumerate(results.keys()):\n        for j, metric in enumerate([\'train_time\', \'acc_train\', \'f_train\', \'pred_time\', \'acc_test\', \'f_test\']):\n            for i in np.arange(3):\n                \n                # Creative plot code\n                ax[j//3, j%3].bar(i+k*bar_width, results[learner][i][metric], width = bar_width, color = colors[k])\n                ax[j//3, j%3].set_xticks([0.45, 1.45, 2.45])\n                ax[j//3, j%3].set_xticklabels([""1%"", ""10%"", ""100%""])\n                ax[j//3, j%3].set_xlabel(""Training Set Size"")\n                ax[j//3, j%3].set_xlim((-0.1, 3.0))\n    \n    # Add unique y-labels\n    ax[0, 0].set_ylabel(""Time (in seconds)"")\n    ax[0, 1].set_ylabel(""Accuracy Score"")\n    ax[0, 2].set_ylabel(""F-score"")\n    ax[1, 0].set_ylabel(""Time (in seconds)"")\n    ax[1, 1].set_ylabel(""Accuracy Score"")\n    ax[1, 2].set_ylabel(""F-score"")\n    \n    # Add titles\n    ax[0, 0].set_title(""Model Training"")\n    ax[0, 1].set_title(""Accuracy Score on Training Subset"")\n    ax[0, 2].set_title(""F-score on Training Subset"")\n    ax[1, 0].set_title(""Model Predicting"")\n    ax[1, 1].set_title(""Accuracy Score on Testing Set"")\n    ax[1, 2].set_title(""F-score on Testing Set"")\n    \n    # Add horizontal lines for naive predictors\n    ax[0, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = \'k\', linestyle = \'dashed\')\n    ax[1, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = \'k\', linestyle = \'dashed\')\n    ax[0, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = \'k\', linestyle = \'dashed\')\n    ax[1, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = \'k\', linestyle = \'dashed\')\n    \n    # Set y-limits for score panels\n    ax[0, 1].set_ylim((0, 1))\n    ax[0, 2].set_ylim((0, 1))\n    ax[1, 1].set_ylim((0, 1))\n    ax[1, 2].set_ylim((0, 1))\n\n    # Create patches for the legend\n    patches = []\n    for i, learner in enumerate(results.keys()):\n        patches.append(mpatches.Patch(color = colors[i], label = learner))\n    pl.legend(handles = patches, bbox_to_anchor = (-.80, 2.53), \\\n               loc = \'upper center\', borderaxespad = 0., ncol = 3, fontsize = \'x-large\')\n    \n    # Aesthetics\n    pl.suptitle(""Performance Metrics for Three Supervised Learning Models"", fontsize = 16, y = 1.10)\n    pl.tight_layout()\n    pl.show()\n    \n\ndef feature_plot(importances, X_train, y_train):\n    \n    # Display the five most important features\n    indices = np.argsort(importances)[::-1]\n    columns = X_train.columns.values[indices[:5]]\n    values = importances[indices][:5]\n\n    # Creat the plot\n    fig = pl.figure(figsize = (9,5))\n    pl.title(""Normalized Weights for First Five Most Predictive Features"", fontsize = 16)\n    pl.bar(np.arange(5), values, width = 0.6, align=""center"", color = \'#00A000\', \\\n          label = ""Feature Weight"")\n    pl.bar(np.arange(5) - 0.3, np.cumsum(values), width = 0.2, align = ""center"", color = \'#00A0A0\', \\\n          label = ""Cumulative Feature Weight"")\n    pl.xticks(np.arange(5), columns)\n    pl.xlim((-0.5, 4.5))\n    pl.ylabel(""Weight"", fontsize = 12)\n    pl.xlabel(""Feature"", fontsize = 12)\n    \n    pl.legend(loc = \'upper center\')\n    pl.tight_layout()\n    pl.show()  \n'"
