file_path,api_count,code
Data Preparation/taxiData_esLoad_bulk.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat May 12 2018\n@author: Vivek M Agrawal\n\nThis module leverages Python Elasticsearch Client to create/update and bulk index\ndata from PKDD 2015 Prediction Challenge.\n\nLink to source data: www.ecmlpkdd2015.org/ \nLink to Python Elasticsearch Client API documentation: \n\thttps://elasticsearch-py.readthedocs.io/en/master/\nLink to Python Elasticsearch Client Bulk helpers documentation: \n\thttps://elasticsearch-py.readthedocs.io/en/master/helpers.html\n\nHelper options is used to batch process JSON formatted data from a directory and\nload it to local Elasticsearch index. \n\nTODO:\n  * Test if reloading of existing index correctly creates new versions under the\n    same id or creates duplicates as index is not explicitly specified\n""""""\n\n#!/usr/bin/env python\n\n__author__ = ""Vivek M Agrawal""\n__version__ = ""1.0""\n\nES_INDEX = \'taxi_data\'\nES_HOST = {""host"": ""localhost"", ""port"": 9200}\nDATA_DIR = \'jsonData\'\n\nimport os\nimport json\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch import helpers\n\n# instantiate Elasticsearch object\nes = Elasticsearch(hosts=[ES_HOST])\n\n# check if the index already exists\nif es.indices.exists(ES_INDEX):\n    # confirm with the user if the existing index can be reused or should be deleted\n    # and recreated\n    if (input(""Press \'Y\' to delete and recreate existing index..."") == \'Y\' or \'y\'):\n        # delete the existing index based on user input\n        res = es.indices.delete(index=ES_INDEX, ignore=[400, 404])\n        print(""Deleted index {}. ES Response:\\n{}"".format(ES_INDEX, res))\n        # define the index structure to be recreated\n        request_body = {\n            ""settings"": {\n                ""number_of_shards"": 2,\n                ""number_of_replicas"": 0\n            },\n            ""mappings"": { \n                ""_doc"": { \n                    ""properties"": {\n                        ""call_type"": {""type"": ""text""},\n                        ""customer_id"": {""type"": ""text""},\n                        ""partial_location_flag"": {""type"": ""boolean""},\n                        ""surge_rate"": { ""type"": ""text""},\n                        ""taxi_id"": {""type"": ""text""},\n                        ""taxi_stand_id"": {""type"": ""text""},\n                        ""trip_id"": {""type"": ""text""},\n                        ""trip_instance_id"": {""type"": ""text""},\n                        ""trip_instance_location"": {""type"": ""geo_point""},\n                        ""trip_start_time"": {""type"": ""date"",\n                            ""format"": ""strict_date_optional_time||epoch_millis""}\n                    }\n                }\n            }\n        }\n        # recreated the index\n        res = es.indices.create(index=ES_INDEX, body=request_body, ignore=400)\n        print(""Recreated index {}. ES Response:\\n{}"".format(ES_INDEX, res))\n\n# indicator for directory with json data to load into the index\njsonDir = os.getcwd() + \'//\' + DATA_DIR + \'//\'\n\n# if the directory exists, use bulk api to index the json files contained in\n# the directory to index from above step\nif os.path.exists(jsonDir):\n    # read the content of the directory\n    for path, subdirs, files in os.walk(jsonDir):\n        # for the different files in the directory\n        for fn in files:\n            print(""Processing file {}...\\n"".format(fn))\n            # open the json file\n            with open(os.path.join(jsonDir, fn), \'r\') as f:\n                # ... and load into a local variable\n                rowChunk = json.load(f)\n            # load the chunk data to elasticsearch using helper function\n            # leverages a generator for creating bulk ingest message by \n            # collating the concatenated document and source data\n            bulkIngest = ({\n                ""_index"": ES_INDEX, \n                ""_type"": ""_doc"", \n                ""_source"": chunk}\n                    for chunk in rowChunk) # for each chunk\n            # at end of operation, load the data to elastic search\n            helpers.bulk(es, bulkIngest)\n'"
Data Preparation/taxiData_esLoad_test.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat May 12 2018\n@author: Vivek M Agrawal\n\nThis module leverages Python Elasticsearch Client to create and index taxi\ndata from PKDD 2015 Prediction Challenge.\n\nLink to source data: www.ecmlpkdd2015.org/ \nLink to API documentation: https://elasticsearch-py.readthedocs.io/en/master/\n\nBoth the bulk API and helper options are explored in the code. Reader can further\nevaluate efficiencies of either of the approaches with loaded volumes. \n""""""\n\n#!/usr/bin/env python\n\n__author__ = ""Vivek M Agrawal""\n__version__ = ""1.0""\n\nES_INDEX = \'taxi_data\'\nES_HOST = {""host"": ""localhost"", ""port"": 9200}\n\nimport json\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch import helpers\n\n# instantiate Elasticsearch object\nes = Elasticsearch(hosts=[ES_HOST])\n\n#check if the index already exists\nif es.indices.exists(ES_INDEX):\n    # if exists, delete the existing index\n    res = es.indices.delete(index=ES_INDEX, ignore=[400, 404])\n    print(""ES Response: {}"".format(res))\n\n# define the index structure\nrequest_body = {\n    ""settings"": {\n        ""number_of_shards"": 1,\n        ""number_of_replicas"": 0\n    },\n    ""mappings"": { \n        ""_doc"": { \n            ""properties"": {\n                ""call_type"": {""type"": ""text""},\n                ""customer_id"": {""type"": ""text""},\n                ""partial_location_flag"": {""type"": ""boolean""},\n                ""surge_rate"": { ""type"": ""text""},\n                ""taxi_id"": {""type"": ""text""},\n                ""taxi_stand_id"": {""type"": ""text""},\n                ""trip_id"": {""type"": ""text""},\n                ""trip_instance_id"": {""type"": ""text""},\n                ""trip_instance_location"": {""type"": ""geo_point""},\n                ""trip_start_time"": {""type"": ""date"",\n                    ""format"": ""strict_date_optional_time||epoch_millis""}\n            }\n        }\n    }\n}\n\n# ignore 400 cause by IndexAlreadyExistsException when creating an index\nres = es.indices.create(index=ES_INDEX, body=request_body, ignore=400)\nprint(""ES Response: {}"".format(res))\n\nwith open("".//jsonData//rowChunk_0.json"", \'r\') as f:\n    rowChunk = json.load(f)\n\n## without helper function\n## initialize the document data to be applied to all records\n#doc = [{\n#        ""index"":{\n#            ""_index"": ES_INDEX, \n#            ""_type"": ""_doc""}\n#        }]\n## create a placeholder to collate concatenated document and source data\n#bulkIngest = str()\n## for all the json chunks in the source data\n#for chunk in rowChunk:\n#    # append the chunk to the bulk ingest payload\n#    bulkIngest = bulkIngest + \\\n#    str(json.dumps(doc[0]) + \'\\n\' + json.dumps(chunk) + \'\\n\')\n## at end of operation, load the data to elastic search\n#es.bulk(body=bulkIngest)\n\n# using helper function\n# leverages a generator for creating bulk ingest message by collating the\n# concatenated document and source data\nbulkIngest = ({\n    ""_index"": ES_INDEX, \n    ""_type"": ""_doc"", \n    ""_source"": chunk}\n        for chunk in rowChunk) # for each chunk\n# at end of operation, load the data to elastic search\nhelpers.bulk(es, bulkIngest)\n'"
Data Preparation/taxiData_prep.py,4,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on May 7 2018\nRevised on May 12 2018\n@author: Vivek M Agrawal\n\nThis module converts the csv formatted Taxi Service Trajectory data from ECML \nPKDD 2015 Prediction Challenge to format suitable for ElasticSearch ingestion.\n\nLink to source data: www.ecmlpkdd2015.org/ \n\nGiven large volume of data, chunking option in pandas is utilized while reading\nthe csv file to reduce memory load. Polyline trajectory in the input data (list \nof lists of point in time coordinates) stacked into individual location vectors\nand written out to JSON format for bulk ingestion into ElasticSearch. \n""""""\n\n#!/usr/bin/env python\n\n__author__ = ""Vivek M Agrawal""\n__version__ = ""1.0""\n\nimport sys\nimport json\nimport pandas as pd\nimport numpy as np\nfrom pprint import pprint\n\n# define data column names to use as json tags\ncolNames = [""trip_id"", ""call_type"", ""customer_id"", ""taxi_stand_id"", ""taxi_id"", \\\n""trip_start_time"", ""surge_rate"", ""partial_location_flag"", ""trip_location"", ]\n\n# enforce column data types\n# note: np.str allows ""NA"" value to be read as NaN, using np.uint8 results in \n# exception for such values\n# http://pandas.pydata.org/pandas-docs/stable/io.html#io-navaluesconst\ncolDtype = {""trip_id"": np.str, ""call_type"": np.str, ""customer_id"": np.str, \\\n""taxi_stand_id"": np.str, ""taxi_id"": np.str, ""trip_start_time"":np.uint64, \\\n""surge_rate"": np.str, ""partial_location_flag"": np.bool, ""trip_location"":np.object}\n\n# set file chunkCounter to store individual chunks as json formatted output\nchunkCounter = 0\nSKIPROWS = 0 # use this if restarting after failure\nCHUNKSIZE = 1000\n\n# http://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking\n# process the data file in chunks of 5000 records at a time to avoid memory issues\nchunkReader = pd.read_table("".//csvData//train.csv"", sep=\',\', header=0, \\\nnames=colNames, dtype=colDtype, error_bad_lines=False, chunksize=CHUNKSIZE, \\\nskiprows=SKIPROWS)\n\nfor chunk in chunkReader:\n    # # review data in the source chunk\n    # pprint(chunk)\n    # input()\n    try:\n        # convert epoch to epch_millis expected at ES\n        chunk[""trip_start_time""] *= 1000\n    \n        # split the list of geo tags for each trip from polyline map trace [list] \n        # to individual row items for each location co-ordinate\n        # https://stackoverflow.com/questions/27263805/pandas-when-cell-contents-are-lists-create-a-row-for-each-element-in-the-list\n        rowChunk = chunk.set_index([""trip_id"", ""call_type"", ""customer_id"", \\\n        ""taxi_stand_id"", ""taxi_id"", ""trip_start_time"", ""surge_rate"", \\\n        ""partial_location_flag""])[""trip_location""]\n    \n        # convert the ingested string object in the polyplot ""trip_location"" to \n        # python list proper for stacking the series\n        for i in range(0, len(rowChunk.values)):\n            #pprint(type(rowChunk.values[i]))\n            rowChunk.values[i] = json.loads(rowChunk.values[i])\n            #pprint(type(rowChunk.values[i]))\n            #pprint(rowChunk.values[i])\n    \n        # split the list of lists containing polyplot ""trip_location"" to individual\n        # location items (by applying pd.Series operation to the rowChunk items)\n        # and then stack those horizontally to create new column with chunkCounter for\n        # values corresponding to the different trip locations (using the .stack()\n        # method) and then reset indexes to update names for the two newly created\n        # columns (using .reset_index() method)\n        rowChunk = rowChunk.apply(pd.Series).stack().reset_index()\n    \n        # add column names back as the final step\n        rowChunk.columns = [""trip_id"", ""call_type"", ""customer_id"", ""taxi_stand_id"", \\\n        ""taxi_id"", ""trip_start_time"", ""surge_rate"", ""partial_location_flag"", \\\n        ""trip_instance_id"", ""trip_instance_location""]\n        \n        # write the updated data frame to json object after orienting by records to\n        # get output formatted list like [{column -> value}, ... , {column -> value}]\n        with open(str("".//jsonData//rowChunk_""+str(chunkCounter)+"".json""), \'w\') as f:\n            f.write(rowChunk.to_json(orient=\'records\'))\n        # to view contents of the output file, use: \n        # cat rowChunk_0.json | python -m json.tool > rowChunk_0-pretty.json\n    \n        # increment chunk chunkCounter and continue until end of file\n        chunkCounter += 1\n        \n        if (chunkCounter%100) == 0:\n            print(""Processed chunk # {}...\\n"".format(chunkCounter))\n            \n    except Exception as e:\n        print(""\\n\\nTerminated after processing chunk # {}..."".format(chunkCounter))\n        sys.exit()\n'"
