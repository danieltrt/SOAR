file_path,api_count,code
main.py,13,"b'import numpy as np\nimport os\nimport model\n\ndef load_mnist():  # train 60k / test 10k\n    data_dir = \'./\'\n\n    fd = open(os.path.join(data_dir, \'train-images-idx3-ubyte\'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    trX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float)\n\n    fd = open(os.path.join(data_dir, \'train-labels-idx1-ubyte\'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    trY = loaded[8:].reshape((60000)).astype(np.int)\n\n    fd = open(os.path.join(data_dir, \'t10k-images-idx3-ubyte\'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    teX = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n\n    fd = open(os.path.join(data_dir, \'t10k-labels-idx1-ubyte\'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    teY = loaded[8:].reshape((10000)).astype(np.int)\n\n    trY = np.asarray(trY)\n    teY = np.asarray(teY)\n\n    perm = np.random.permutation(trY.shape[0])\n    trX = trX[perm]\n    trY = trY[perm]\n\n    perm = np.random.permutation(teY.shape[0])\n    teX = teX[perm]\n    teY = teY[perm]\n\n    return trX, trY, teX, teY  # TrainX, TrainY, TestX, TestY\n\ndef main():\n    trainX, trainY, testX, testY = load_mnist()\n    print(""Shapes: "", trainX.shape, trainY.shape, testX.shape, testY.shape)\n    \n    epochs = 25\n    num_hidden_units = 300 \n    minibatch_size = 100  \n    regularization_rate = 0.01 \n    learning_rate = 0.001 \n\n    model = model.MLP(num_hidden_units, minibatch_size, regularization_rate, learning_rate)\n\n    print(""Starting training.........."")\n    model.train(trainX, trainY, epochs)\n    print(""Training complete"")\n\n    print(""Starting testing.........."")\n    labels = model.test(testX)\n    accuracy = np.mean((labels == testY)) * 100.0\n    print(""\\nTest accuracy: %lf%%"" % accuracy)\n\nif __name__ == \'__main__\':\n    main()'"
model.py,25,"b'import numpy as np\n\nclass MLP:\n    # Hyperparameters initialize\n    def __init__(self, hidden_units, minibatch_size, regularization_rate, learning_rate):\n        self.hidden_units = hidden_units\n        self.minibatch_size = minibatch_size\n        self.regularization_rate = regularization_rate\n        self.learning_rate = learning_rate\n\n    # ReLu (Rectified Linear Unit) function\n    def relu_function(self, matrix_content, matrix_dim_x, matrix_dim_y):\n        ret_vector = np.zeros((matrix_dim_x, matrix_dim_y))\n\n        for i in range(matrix_dim_x):\n            for j in range(matrix_dim_y):\n                ret_vector[i, j] = max(0, matrix_content[i,j])\n\n        return ret_vector\n\n    # the gradient of ReLu (Rectified Linear Unit) function\n    def grad_relu(self, matrix_content, matrix_dim_x, matrix_dim_y):\n        ret_vector = np.zeros((matrix_dim_x, matrix_dim_y))\n\n        for i in range(matrix_dim_x):\n            for j in range(matrix_dim_y):\n                if matrix_content[i, j] > 0:\n                    ret_vector[i, j] = 1\n                else:\n                    ret_vector[i, j] = 0\n\n        return ret_vector\n\n    # Softmax fucntion\n    def softmax_function(self, vector_content):\n        return np.exp(vector_content - np.max(vector_content)) / np.sum(np.exp(vector_content - np.max(vector_content)), axis=0)\n\n    # generator for mini-batch\n    def iterate_minibatches(self, inputs, targets, batch_size, shuffle=False):\n        assert inputs.shape[0] == targets.shape[0]  # \xeb\xa7\x8c\xec\x95\xbd input / output shape \xec\xb2\xb4\xed\x81\xac\n\n        if shuffle: # batch shuffle \xec\xa0\x81\xec\x9a\xa9\n            indices = np.arange(inputs.shape[0])\n            np.random.shuffle(indices)\n\n        for start_idx in range(0, inputs.shape[0] - batch_size + 1, batch_size):\n            if shuffle:\n                excerpt = indices[start_idx:start_idx + batch_size]\n            else:\n                excerpt = slice(start_idx, start_idx + batch_size)\n\n            yield inputs[excerpt], targets[excerpt]\n\n    #function to train the MLP model\n    def train(self, trainX, trainY, epochs):\n\n        # parameter initialize\n        w1_mat = np.random.randn(self.hidden_units, 28*28) *np.sqrt(2./(self.hidden_units+28*28))\n        w2_mat = np.random.randn(10, self.hidden_units) *np.sqrt(2./(10+self.hidden_units))\n        b1_vec = np.zeros((self.hidden_units, 1))\n        b2_vec = np.zeros((10, 1))\n\n        # input / output size reshape\n        trainX = np.reshape(trainX, (trainX.shape[0], 28*28))\n        trainY = np.reshape(trainY, (trainY.shape[0], 1))\n\n        for num_epochs in range(epochs) :\n            if num_epochs % 2 == 0:\n                print(""Current epoch number : "", num_epochs)\n\n            for batch in self.iterate_minibatches(trainX, trainY, self.minibatch_size, shuffle=True):\n                x_batch, y_batch = batch\n                x_batch = x_batch.T\n                y_batch = y_batch.T\n\n                # logit calc and apply ReLU\n                z1 = np.dot(w1_mat, x_batch) + b1_vec\n                a1 = self.relu_function(z1, self.hidden_units, self.minibatch_size)\n\n                # logit calc and apply Softmax\n                z2 = np.dot(w2_mat, a1) + b2_vec\n                a2_softmax = self.softmax_function(z2)\n\n                # cross-entropy for loss function\n                gt_vector = np.zeros((10, self.minibatch_size))\n                for example_num in range(self.minibatch_size):\n                    gt_vector[y_batch[0, example_num], example_num] = 1\n\n                # regularization for weights\n                d_w2_mat = self.regularization_rate*w2_mat\n                d_w1_mat = self.regularization_rate*w1_mat\n\n                # backpropagation\n                delta_2 = np.array(a2_softmax - gt_vector)\n                d_w2_mat = d_w2_mat + np.dot(delta_2, (np.matrix(a1)).T)\n                d_b2_vec = np.sum(delta_2, axis=1, keepdims=True)\n\n                delta_1 = np.array(np.multiply((np.dot(w2_mat.T, delta_2)), self.grad_relu(z1, self.hidden_units, self.minibatch_size)))\n                d_w1_mat = d_w1_mat + np.dot(delta_1, np.matrix(x_batch).T)\n                d_b1_vec = np.sum(delta_1, axis=1, keepdims=True)\n\n                d_w2_mat = d_w2_mat/self.minibatch_size\n                d_w1_mat = d_w1_mat/self.minibatch_size\n                d_b2_vec = d_b2_vec/self.minibatch_size\n                d_b1_vec = d_b1_vec/self.minibatch_size\n\n                # update weights\n                w2_mat = w2_mat - self.learning_rate*d_w2_mat\n                b2_vec = b2_vec - self.learning_rate*d_b2_vec\n\n                w1_mat = w1_mat - self.learning_rate*d_w1_mat\n                b1_vec = b1_vec - self.learning_rate*d_b1_vec\n\n        self.w1_mat, self.b1_vec, self.w2_mat, self.b2_vec = w1_mat, b1_vec, w2_mat, b2_vec\n\n    # function to test the MLP model\n    def test(self, testX):\n        output_labels = np.zeros(testX.shape[0])\n\n        num_examples = testX.shape[0]\n\n        testX = np.reshape(testX, (num_examples, 28*28))\n        testX = testX.T\n\n        # test with trained model\n        z1 = np.dot(self.w1_mat, testX) + self.b1_vec    \n        a1 = self.relu_function(z1, self.hidden_units, num_examples)\n\n        z2 = np.dot(self.w2_mat, a1) + self.b2_vec\n        a2_softmax = self.softmax_function(z2)\n\n        for i in range(num_examples):\n            pred_col = a2_softmax[:, [i]]\n            output_labels[i] = np.argmax(pred_col)\n\n        return output_labels'"
