file_path,api_count,code
basic_creating_knn/basic_creating_knn.py,1,"b'\'\'\'Python36\nDesigning a KNN algorithm from scratch\'\'\'\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom matplotlib import style\nfrom collections import Counter\nstyle.use(\'fivethirtyeight\')\n\ndef main():\n    # making two classes k and r; with their 3 features\n    dataset = {\'k\':[[1,2],[2,3],[4,5],[3,6],[3,8],[4,1]], \'r\':[[5,2],[4,9],[5,6],[6,5],[7,7],[8,6]]}\n    new_features = [4.2,7]\n    \n    for i in dataset:\n        for ii in dataset[i]:\n            plt.scatter(ii[0],ii[1], s=100, color=i)\n        #endfor\n    #endfor\n    \n    plt.scatter(new_features[0], new_features[1], s=100)\n    plt.title(""Predict the blue dot!"")\n    plt.show()\n    \n    result = k_nearest_neighbors(dataset, new_features, k=5)\n    print(""result: "")\n    print(result)\n    \n    return\n#enddef\n\ndef k_nearest_neighbors(data, predict, k=3):\n    if len(data) >= k:\n        warnings.warn(\'K is set to a value less than total voting groups\')\n    #endif\n    \n    # knn algorithm\n    distances = []\n    for group in data:\n        for features in data[group]:\n            # applying the euclidean distance from predict point to the features of the group\n            #    and then comparing the distances with each point to find which is the shortest\n            #    distance in order to classify our predict point as that group\n            #        WILL TAKE A LOT OF TIME AND IT\'S INEFFICIENT WITH BIGGER DATASETS!\n            # and what if the feautures are not 2 dimensional but 3 or more dimensions? Euclidean distance cannot be calculated that easily!\n            \n            # faster way\n            euclidean_distance = np.linalg.norm(np.array(features)-np.array(predict))\n            distances.append([euclidean_distance, group])\n        #endfor\n    #endfor\n    \n    # getting votes from each group just 3 times as k=3 (we don\'t care what distances are so just use second column which is the group\n    votes = []\n    for i in sorted(distances)[:k]:\n        votes.append(i[1])\n    #endfor\n    print(""votes: "")\n    print(votes)\n    print(""vote count: "")\n    print(Counter(votes).most_common(2))\n    vote_result = Counter(votes).most_common(2)[0][0]\n    \n    return vote_result\n\nif __name__ == \'__main__\':\n    main()\n#endif'"
breast_cancer_knn/breast_cancer_knn.py,1,"b'\'\'\'Python36\nDesigning a KNN algorithm from scratch and using breast_cancer.txt\'\'\'\n\nimport numpy as np\nimport warnings\nfrom collections import Counter\nimport pandas as pd\nimport random\n\ndef main():\n    df = pd.read_csv(\'breast_cancer.txt\')\n    df.replace(\'?\', -99999, inplace=True)\n    df.drop(\'id\', 1, inplace=True)\n    \n    # the reason for doing this is because df.head() won\'t display the complete data in correct format. Plus everything has to be an int or float so it was needed\n    # uncomment below line to see the error in displayed data\n    #print(df.head())\n    full_data = df.astype(int).values.tolist()\n    random.shuffle(full_data)\n    \n    test_size = 0.2\n    train_set = {2:[], 4:[]}\n    test_set = {2:[], 4:[]}\n    # 80 percent train data\n    train_data = full_data[:-int(test_size*len(full_data))]\n    # 20 percent test data\n    test_data = full_data[-int(test_size*len(full_data)):]\n    \n    # populate the dictionary\n    # key is either 2 or 4 in each i\n    # and column for the class in the dataset is the last one so address it using -1\n    # append data to that key i excluding the last value (class) \n    for i in train_data:\n        train_set[i[-1]].append(i[:-1])\n    #endfor\n    for i in test_data:\n        test_set[i[-1]].append(i[:-1])\n    #endfor\n    \n    correct = 0\n    total = 0\n    for group in test_set:\n        for data in test_set[group]:\n            vote, confidence = k_nearest_neighbors(train_set, data, k=3)\n            # increase the correct count if group in test_set matches the vote from train_set\n            if group == vote:\n                correct+=1\n                print(\'Vote result: \', vote, \'Match Confidence: \', confidence)\n            else:\n                print(\'Vote result: \', vote, \'Not matched confidence: \', confidence)\n            #endif\n            total+=1 \n        #endfor\n    #endfor\n    \n    print(\'Accuracy: \', correct/total)\n    return\n#enddef\n\ndef k_nearest_neighbors(data, predict, k=3):\n    if len(data) >= k:\n        warnings.warn(\'K is set to a value less than total voting groups\')\n    #endif\n    \n    # knn algorithm\n    distances = []\n    for group in data:\n        for features in data[group]:\n            # applying the euclidean distance from predict point to the features of the group\n            #    and then comparing the distances with each point to find which is the shortest\n            #    distance in order to classify our predict point as that group\n            #        WILL TAKE A LOT OF TIME AND IT\'S INEFFICIENT WITH BIGGER DATASETS!\n            # and what if the feautures are not 2 dimensional but 3 or more dimensions? Euclidean distance cannot be calculated that easily!\n            \n            # faster way\n            euclidean_distance = np.linalg.norm(np.array(features)-np.array(predict))\n            distances.append([euclidean_distance, group])\n        #endfor\n    #endfor\n    \n    # getting votes from each group just 3 times as k=3 (we don\'t care what distances are so just use second column which is the group\n    votes = []\n    for i in sorted(distances)[:k]:\n        votes.append(i[1])\n    #endfor\n    \n    #print(""votes: "")\n    #print(votes)\n    #print(""vote count: "")\n    print(Counter(votes).most_common(2))\n    vote_result = Counter(votes).most_common(1)[0][0]\n    \n    # confidence is the most common votes divided by the test points k\n    confidence = Counter(votes).most_common(1)[0][0]/k\n    \n    return vote_result, confidence\n\nif __name__ == \'__main__\':\n    main()\n#endif'"
breast_cancer_sklearn/breast_cancer_sklearn.py,3,"b""'''\nPython 36\nDataset: \nhttps://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/\n'''\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing, neighbors, cross_validation\n\ndef main():\n    df = pd.read_csv('breast_cancer.txt')\n    # missing attribute ? mentioned in the dataset\n    df.replace('?', -99999, inplace=True)\n    # taking out useless attribute as it can effect the KNN algorithm\n    df.drop(['id'], 1, inplace=True)\n    \n    # x is the feature array so taking features only where class is not needed\n    X = np.array(df.drop(['class'], 1))\n    print(X)\n    # y is the label\n    Y = np.array(df['class'])\n    print(Y)\n    \n    # splitting x and y into test and train dataset\n    X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=0.2)\n    \n    # defining the classifier clf\n    clf = neighbors.KNeighborsClassifier()\n    clf.fit(X_train, Y_train)\n    \n    # checking the accuracy of test data in the classifier\n    accuracy = clf.score(X_test, Y_test)\n    print(accuracy)\n    \n    # testing created data\n    example_measures = np.array([[6,2,1,6,6,10,3,9,1], [2,2,1,6,6,3,3,9,1]])\n    example_measures = example_measures.reshape(len(example_measures),-1)\n    prediction = clf.predict(example_measures)\n    # returns either 2 or 4 (2 for benign, 4 for malignant; mentioned in dataset link)\n    print(prediction)\n    \n    return\n#enddef\n\nif __name__ == '__main__':\n    main()\n#endif """
iris_flower_datset/iris_flower_dataset.py,4,"b""'''Python36\nDataset:\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.names\n'''\n\nimport numpy as np\nfrom sklearn import preprocessing, cross_validation, neighbors\nimport pandas as pd\nfrom numpy import sum\n\ndef main():\n    df = pd.read_csv('iris_flower.txt')\n    #df.replace('?', -99999, inplace=True)\n        \n    X = np.array(df.drop(['class'], 1))\n    Y = np.array(df['class'])\n    \n    count = 0\n    accuracy = []\n    while count<=10:\n        \n        X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=0.2)\n        clf = neighbors.KNeighborsClassifier()\n        clf.fit(X_train, Y_train)\n        \n        accuracy.append(clf.score(X_test, Y_test))\n        \n        count+=1\n    #endwhile\n    avg_accuracy = sum(np.array(accuracy))/len(accuracy)\n    print(avg_accuracy)\n    \n    # testing the flower class type with our custom data\n    flower_test = np.array([[6.5,2.5,2.9,0.7], [4.2,3.1,5.3,2.6], [5.1,4.6,9.5,1.4]])\n    flower_test = flower_test.reshape(len(flower_test), -1)\n    predict = clf.predict(flower_test)\n    print(predict)\n    \n    return\n#enddef\n\nif __name__ == '__main__':\n    main()\n#endif"""
wine_recognition/wine_recognition.py,1,"b'\'\'\'Python36\nDataset:\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names\n\nDesigning a KNN algorithm from scratch and using wine.txt\n\'\'\'\n\nimport numpy as np\nimport warnings\nfrom collections import Counter\nimport pandas as pd\nimport random\n\ndef main():\n    df = pd.read_csv(\'wine.txt\')\n    #df.replace(\'?\', -99999, inplace=True)\n    #df.drop(\'id\', 1, inplace=True)\n    \n    # the reason for doing this is because df.head() won\'t display the complete data in correct format. Plus everything has to be an int or float so it was needed\n    # uncomment below line to see the error in displayed data\n    #print(df.head())\n    full_data = df.astype(int).values.tolist()\n    random.shuffle(full_data)\n    \n    test_size = 0.2\n    train_set = {1:[], 2:[], 3:[]}\n    test_set = {1:[], 2:[], 3:[]}\n    # 80 percent train data\n    train_data = full_data[:-int(test_size*len(full_data))]\n    # 20 percent test data\n    test_data = full_data[-int(test_size*len(full_data)):]\n    \n    # populate the dictionary\n    # key is either 1 or 2 or 3 in each i\n    # and column for the class in the dataset is the first one so address it using 1\n    # append data to that key i excluding the last value (class) \n    for i in train_data:\n        train_set[i[0]].append(i[1:])\n    #endfor\n    for i in test_data:\n        test_set[i[0]].append(i[1:])\n    #endfor\n    \n    correct = 0\n    total = 0\n    for group in test_set:\n        for data in test_set[group]:\n            vote, confidence = k_nearest_neighbors(train_set, data, k=3)\n            # increase the correct count if group in test_set matches the vote from train_set\n            if group == vote:\n                correct+=1\n                print(\'Vote result: \', vote, \'Match Confidence: \', confidence)\n            else:\n                print(\'Vote result: \', vote, \'Not matched confidence: \', confidence)\n            #endif\n            total+=1 \n        #endfor\n    #endfor\n    \n    print(\'Accuracy: \', correct/total)\n    return\n#enddef\n\ndef k_nearest_neighbors(data, predict, k=3):\n    if len(data) >= k:\n        warnings.warn(\'K is set to a value less than total voting groups\')\n    #endif\n    \n    # knn algorithm\n    distances = []\n    for group in data:\n        for features in data[group]:\n            # applying the euclidean distance from predict point to the features of the group\n            #    and then comparing the distances with each point to find which is the shortest\n            #    distance in order to classify our predict point as that group\n            #        WILL TAKE A LOT OF TIME AND IT\'S INEFFICIENT WITH BIGGER DATASETS!\n            # and what if the feautures are not 2 dimensional but 3 or more dimensions? Euclidean distance cannot be calculated that easily!\n            \n            # faster way\n            euclidean_distance = np.linalg.norm(np.array(features)-np.array(predict))\n            distances.append([euclidean_distance, group])\n        #endfor\n    #endfor\n    \n    # getting votes from each group just 3 times as k=3 (we don\'t care what distances are so just use second column which is the group\n    votes = []\n    for i in sorted(distances)[:k]:\n        votes.append(i[1])\n    #endfor\n    \n    #print(""votes: "")\n    #print(votes)\n    #print(""vote count: "")\n    print(Counter(votes).most_common(3))\n    vote_result = Counter(votes).most_common(1)[0][0]\n    \n    # confidence is the most common votes divided by the test points k\n    confidence = Counter(votes).most_common(1)[0][0]/k\n    \n    return vote_result, confidence\n\nif __name__ == \'__main__\':\n    main()\n#endif'"
wine_recognition/wine_recognition_sklearn.py,4,"b""'''Python36\nDataset:\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names\n'''\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing, cross_validation, neighbors\nimport random\n\ndef main():\n    df = pd.read_csv('wine.txt')\n    X = np.array(df.drop(['class'], 1))\n    Y = np.array(df['class'])\n    \n    count = 0\n    accuracy = []\n    while count<=10:\n        \n        X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=0.2)\n        \n        clf = neighbors.KNeighborsClassifier()\n        clf.fit(X_train, Y_train)\n        \n        accuracy.append(clf.score(X_test, Y_test))\n        \n        count+=1\n    #endwhile\n    \n    avg_accuracy = sum(np.array(accuracy))/len(accuracy)\n    print(avg_accuracy)\n    \n    # testing the wine class type with our custom data\n    wine_test = np.array([[10.6,2,2.61,17.6,11,2.6,5.51,.31,1.25,5.05,1.06,3.58,1095], [11.82,1.72,1.88,19.5,86,2.5,1.64,.37,1.42,2.06,.94,2.44,415]])\n    wine_test = wine_test.reshape(len(wine_test), -1)\n    predict = clf.predict(wine_test)\n    print(predict)\n    \n    return\n#enddef\n\nif __name__ == '__main__':\n    main()\n#endif"""
