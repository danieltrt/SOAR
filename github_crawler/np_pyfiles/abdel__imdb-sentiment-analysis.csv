file_path,api_count,code
notebooks/utils.py,3,"b'import os\nimport re\nimport bcolz\nimport keras\nimport itertools\nimport numpy as np\nimport _pickle as pickle\n\nfrom itertools import chain\nfrom matplotlib import pyplot as plt\nfrom keras.utils.data_utils import get_file\nfrom numpy.random import normal\n\ndef plot_confusion_matrix(cm, classes, normalize=False, title=\'Confusion matrix\', cmap=plt.cm.Blues):\n    """"""\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    (This function is copied from the scikit docs.)\n    """"""\n    plt.figure()\n    plt.imshow(cm, interpolation=\'nearest\', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n    print(cm)\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j], horizontalalignment=""center"", color=""white"" if cm[i, j] > thresh else ""black"")\n\n    plt.tight_layout()\n    plt.ylabel(\'True label\')\n    plt.xlabel(\'Predicted label\')\n    \ndef load_array(fname):\n    return bcolz.open(fname)[:]\n\ndef get_glove_dataset(dataset):\n    """"""Download the requested glove dataset from files.fast.ai\n    and return a location that can be passed to load_vectors.\n    """"""\n    md5sums = {\'6B.50d\': \'8e1557d1228decbda7db6dfd81cd9909\',\n               \'6B.100d\': \'c92dbbeacde2b0384a43014885a60b2c\',\n               \'6B.200d\': \'af271b46c04b0b2e41a84d8cd806178d\',\n               \'6B.300d\': \'30290210376887dcc6d0a5a6374d8255\'}\n    glove_path = os.path.abspath(\'data/glove/results\')\n    return get_file(dataset,\n                    \'http://files.fast.ai/models/glove/\' + dataset + \'.tgz\',\n                    cache_subdir=glove_path,\n                    md5_hash=md5sums.get(dataset, None),\n                    untar=True)\n\ndef load_vectors(loc):\n    return (load_array(loc+\'.dat\'),\n        pickle.load(open(loc+\'_words.pkl\',\'rb\'), encoding=\'latin1\'),\n        pickle.load(open(loc+\'_idx.pkl\',\'rb\'), encoding=\'latin1\'))\n\ndef create_embeddings(max_features, vecs, wordidx, idx2word):\n    n_fact = vecs.shape[1]\n    emb = np.zeros((max_features, n_fact))\n\n    for i in range(1,len(emb)):\n        word = idx2word[i]\n        if word and re.match(r""^[a-zA-Z0-9\\-]*$"", word):\n            src_idx = wordidx[word]\n            emb[i] = vecs[src_idx]\n        else:\n            # If we can\'t find the word in glove, randomly initialize\n            emb[i] = normal(scale=0.6, size=(n_fact,))\n\n    # This is our ""rare word"" id - we want to randomly initialize\n    emb[-1] = normal(scale=0.6, size=(n_fact,))\n    emb/=3\n    return emb'"
webapp/app.py,0,"b'import utils\nimport numpy as np\nimport io, traceback\nimport tensorflow as tf\n\nfrom flask import jsonify\nfrom flask import Flask, request, g\nfrom flask_mako import MakoTemplates, render_template\nfrom plim import preprocessor\n\nfrom keras.models import load_model\n\napp = Flask(__name__, instance_relative_config=True)\n\n# For Plim templates\nmako = MakoTemplates(app)\napp.config[\'MAKO_PREPROCESSOR\'] = preprocessor\napp.config.from_object(\'config.DevelopmentConfig\')\n\n@app.route(\'/\')\ndef homepage():\n    return render_template(\'index.html.slim\', name=\'mako\')\n\n@app.route(\'/predict\', methods=[\'POST\'])\ndef predict():\n    # Get input text and convert to lower case\n    text = request.form.get(\'text\', type=str)\n    text = text.lower()\n\n    # Get predictions\n    prob, pred = utils.predict(text)\n    print(""Prediction Probability:"", prob)\n    print(""Prediction Class:"", pred)\n\n    if prob is not None:\n        return jsonify(\'{""prob"": ""%s"", ""pred"": ""%s""}\' %(prob, pred))\n    else:\n        return jsonify(\'{""error"": true}\')\n\nif __name__ == \'__main__\':\n    app.run(host=\'0.0.0.0\')\n'"
webapp/config.py,0,"b""class Config(object):\n    SITE_NAME = 'Sentiment Analysis on Movie Reviews'\n    SITE_SLUG_NAME = 'imdb-sentiment-analysis'\n    TAGLINES = ['Sentiment Analysis using Deep Learning']\n    SITE_DESCRIPTION = 'Sentiment analysis on movie reviews'\n    SITE_KEYWORDS = 'sentiment analysis, imdb, movie, review, nlp, deep learning'\n    ADMINS = ['abdelm.is@gmail.com']\n\nclass DevelopmentConfig(Config):\n    DOMAIN = 'localhost=5000'\n    ASSET_DOMAIN = 'localhost=5000'\n\nclass ProductionConfig(Config):\n    DOMAIN = 'imdb-sentiment-analysis.herokuapp.com'\n    ASSET_DOMAIN = 'imdb-sentiment-analysis.herokuapp.com'\n"""
webapp/utils.py,1,"b'import nltk\nimport numpy as np\nimport tensorflow as tf\n\nfrom nltk.corpus import stopwords\nfrom keras.models import load_model\nfrom keras.preprocessing import sequence, text\n\n# Download NLTK stopwords\nnltk.download(\'stopwords\')\n\n# Params\nmaxlen = 400\nmax_features = 5000\n\n# Load model\nprint(""Loading model"")\nmodel = load_model(\'./model/cnn_model.h5\', compile=False)\ngraph = tf.get_default_graph()\n\ndef preprocess(data):\n    # Prepare the stopwords\n    stopwords_nltk = set(stopwords.words(\'english\'))\n    relevant_words = set([\'not\', \'nor\', \'no\', \'wasn\', \'ain\', \'aren\', \'very\',\n                          \'only\', \'but\', \'don\', \'isn\', \'weren\'])\n    stopwords_filtered = list(stopwords_nltk.difference(relevant_words))\n\n    # Remove the stop words from input text\n    data = \' \'.join([word for word in data.split() if word not in stopwords_filtered])\n\n    # One-hot the input text\n    data = text.one_hot(data, max_features)\n    data = np.array(data)\n\n    # Pad the sequences\n    data = sequence.pad_sequences([data], maxlen=maxlen)\n\n    return data\n\ndef predict(text):\n    text = preprocess(text)\n\n    with graph.as_default():\n        pred_prob = model.predict(text)[0][0]\n        pred_class = model.predict_classes(text)[0][0]\n\n    return pred_prob, pred_class\n'"
webapp/wsgi.py,0,b'from app import app as application\n'
