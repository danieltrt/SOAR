file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport os\nimport re\nimport codecs\n\nfrom setuptools import setup, find_packages\n\ncwd = os.path.abspath(os.path.dirname(__file__))\n\ndef read(filename):\n    with codecs.open(os.path.join(cwd, filename), \'rb\', \'utf-8\') as h:\n        return h.read()\n\nmetadata = read(os.path.join(cwd, \'spampy\', \'__init__.py\'))\n\ndef extract_metaitem(meta):\n    meta_match = re.search(r""""""^__{meta}__\\s+=\\s+[\'\\""]([^\'\\""]*)[\'\\""]"""""".format(meta=meta),\n                           metadata, re.MULTILINE)\n    if meta_match:\n        return meta_match.group(1)\n    raise RuntimeError(\'Unable to find __{meta}__ string.\'.format(meta=meta))\n\nwith open(\'requirements.txt\') as f:\n    requirements = f.read().splitlines()\n\nwith open(\'requirements.testing.txt\') as f:\n    requirements_testing = f.read().splitlines()\n\nsetup(\n    name=\'spampy\',\n    version=extract_metaitem(\'version\'),\n    license=extract_metaitem(\'license\'),\n    description=extract_metaitem(\'description\'),\n    long_description=(read(\'README.rst\')),\n    long_description_content_type=\'text/x-rst\',\n    author=extract_metaitem(\'author\'),\n    author_email=extract_metaitem(\'email\'),\n    maintainer=extract_metaitem(\'author\'),\n    maintainer_email=extract_metaitem(\'email\'),\n    url=extract_metaitem(\'url\'),\n    download_url=extract_metaitem(\'download_url\'),\n    packages=find_packages(exclude=(\'tests\', \'docs\')),\n    include_package_data=True,\n    platforms=[\'Any\'],\n    install_requires=requirements,\n    setup_requires=[\'pytest-runner\'],\n    tests_require=requirements_testing,\n    python_requires=\'!=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, <4\',\n    keywords=\'machine learning, spam filter, support vector machine, spam, svm\',\n    classifiers=[\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Operating System :: OS Independent\',\n        \'Topic :: Software Development\',\n        \'Topic :: Scientific/Engineering\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Python :: 3.8\',\n    ],\n)\n'"
spampy/__init__.py,0,"b""#!/usr/bin/env python\n\n'''Spam filtering module with Machine Learning using SVM.'''\n\nfrom __future__ import absolute_import\n\n__author__       = 'Abdullah Selek'\n__email__        = 'abdullahselek@gmail.com'\n__copyright__    = 'Copyright (c) 2018 Abdullah Selek'\n__license__      = 'MIT License'\n__version__      = '0.2.0'\n__url__          = 'https://github.com/abdullahselek/spampy'\n__download_url__ = 'https://pypi.org/project/spampy/'\n__description__  = 'Spam filtering module with Machine Learning using SVM.'\n\nfrom spampy import (\n    email_processor,\n    spam_classifier,\n    dataset_downloader\n)\n"""
spampy/__main__.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport click\n\nfrom spampy import (\n    __version__,\n    dataset_downloader,\n    spam_classifier\n)\n\n\nhelp_message = '''\n  Spam filtering module with Machine Learning using SVM.\n  Usage\n    $ python spampy [<options>]\n  Options\n    --help, -h              Display help message\n    --download, -d          Download enron dataset\n    --eclassify, -ec        Classify given raw email with enron dataset, prompts for raw email\n    --classify, -c          Classify given raw email, prompts for raw email\n    --version, -v           Display installed version\n  Examples\n    $ python spampy --help\n    $ python spampy --download\n    $ python spampy --eclassify\n    $ python spampy --classify\n'''\n\nspampy_version = __version__\n\n@click.command(add_help_option=False)\n@click.option('-d', '--download', is_flag=True, default=False, help='Use this command to download enron dataset')\n@click.option('-ec', '--eclassify', is_flag=True, default=False, help='Raw email string to classify, run -d (--download) before use')\n@click.option('-c', '--classify', is_flag=True, default=False, help='Raw email string to classify')\n@click.option('-v', '--version', is_flag=True, default=False, help='Display installed version')\n@click.option('-h', '--help', is_flag=True, default=False, help='Display help message')\n\n\ndef main(download, eclassify, classify, version, help):\n    if (help):\n        print(help_message)\n        sys.exit(0)\n    else:\n        if (version):\n            print('koolsla' +  ' ' + spampy_version)\n        else:\n            if (download):\n                dataset_downloader.download_enron_dataset()\n            elif (eclassify):\n                email = click.prompt('Raw email', type=str)\n                is_spam = spam_classifier.classify_email_with_enron(email)\n                print(True if is_spam == 1 else False)\n            elif (classify):\n                email = click.prompt('Raw email', type=str)\n                is_spam = spam_classifier.classify_email(email)\n                print(True if is_spam == 1 else False)\n\n\nif __name__ == '__main__':\n    main()\n"""
spampy/dataset_downloader.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\n\n\ndef download_enron_dataset():\n    """"""\n    Downloads Enron dataset.\n    """"""\n\n    script_path = os.path.join(\'spampy\', \'dataset_downloader.sh\')\n    os.system(script_path)\n'"
spampy/email_processor.py,5,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport re\nimport nltk\nimport os\nimport numpy as np\nimport codecs\nimport multiprocessing as mp\n\nfrom collections import Counter\nfrom typing import Dict, List, Tuple\n\n\ndef preprocess(email: str) -> str:\n    """"""\n    Preprocess (simplifies) raw email.\n    Args:\n      email (str):\n        Raw e-mail\n    Returns:\n      Processed (simplified) email\n    """"""\n\n    # Make e-mail lower case\n    email = email.lower()\n    # Strip html tags\n    email = re.sub(\'<[^<>]+>\', \' \', email)\n    # Any numbers get replaced with the string \'number\'\n    email = re.sub(\'[0-9]+\', \'number\', email)\n    # Any word starting with http or https:// replaced with \'httpaddr\'\n    email = re.sub(r\'(http|https)://[^\\s]*\', \'httpaddr\', email)\n    # Strings with ""@"" in the middle are considered emails --> \'emailaddr\'\n    email = re.sub(r\'[^\\s]+@[^\\s]+\', \'emailaddr\', email)\n    # The \'$\' sign gets replaced with \'dollar\'\n    email = re.sub(\'[$]+\', \'dollar\', email)\n    return email\n\n\ndef create_tokenlist(email: str) -> List:\n    """"""\n    Tokenizes it, creates a list of tokens in the e-mail.\n    Args:\n      email (str):\n        Raw e-mail\n    Returns:\n      Ordered list of tokens in the e-mail.\n    """"""\n    \n    # use NLTK porter stemmer\n    stemmer = nltk.stem.porter.PorterStemmer()\n    email = preprocess(email)\n    # Split the e-mail into single words by \' \', \'@\', \'$\', \'/\', ...\n    tokens = re.split(r\'[ \\@\\$\\/\\#\\.\\-\\:\\&\\*\\+\\=\\[\\]\\?\\!\\(\\)\\{\\}\\,\\\'\\""\\>\\_\\<\\;\\%]\', email)\n    # Loop over each word and use a stemmer to shorten it,\n    tokenlist = []\n    for token in tokens:\n        # Remove any non alphanumeric characters\n        token = re.sub(\'[^a-zA-Z0-9]\', \'\', token)\n        # Use the Porter stemmer to stem the word\n        stemmed = stemmer.stem(token)\n        # Pass empty tokens\n        if not len(token): continue\n        # Save a list of all unique stemmed words\n        tokenlist.append(stemmed)\n    return tokenlist    \n\n\ndef get_vocablary_dict(path: str = \'spampy/datasets\', filename: str = \'vocablary.txt\') -> Dict:\n    """"""\n    Add vocablary text file content into a dictionary.\n    Args:\n      path (str):\n        Vocablary file folder path.\n      filename (str):\n        Vocablary file name.\n    Returns:\n      Vocablary dict.\n    """"""\n\n    vocablary_dict = {}\n    with open(os.path.join(path, filename), \'r\') as f:\n        for line in f:\n            (val, key) = line.split()\n            vocablary_dict[int(val)] = key\n    return vocablary_dict\n\n\ndef get_vocablary_indices(email: str, vocablary_dict: Dict) -> List:\n    """"""\n    Returns a list of indices (location) of each stemmed word in email.\n    Args:\n      email (str):\n        E-mail.\n      vocablary_dict (dict):\n        Vocablary dictionary created by `get_vocablary_dict`.\n    Returns:\n      Indices list.\n    """"""\n\n    tokenlist = create_tokenlist(email)\n    index_list = [vocablary_dict[token] for token in tokenlist if token in vocablary_dict]\n    return index_list\n\n\ndef feature_vector_from_email(email: str, vocablary_dict: Dict) -> Dict:\n    """"""\n    Returns a vector of shape (n,1) with a size of the vocablary_dict.\n    If the vocab word with index == 1 is in the email, first element in\n    this vector is 1, 0 otherwise.\n    Args:\n      email (str):\n        E-mail.\n      vocablary_dict (dict):\n        Vocablary dictionary created by `get_vocablary_dict`.\n    """"""\n\n    n = len(vocablary_dict)\n    result = np.zeros((n,1))\n    vocablary_indices = get_vocablary_indices(email, vocablary_dict)\n    for index in vocablary_indices:\n        result[index] = 1\n    return result\n\n\ndef listdir(directory: str):\n    """"""\n    A specialized version of os.listdir() that ignores files that\n    start with a leading period.\n    \n    Especially dismissing .DS_STORE s.\n    """"""\n    filelist = os.listdir(directory)\n    return [x for x in filelist if not (x.startswith(\'.\'))]\n\n\ndef enron_processor(emails_dir: str, return_dict: Dict):\n    """"""\n    A function which processes .txt email files into lists\n    and returns in a dictionary.\n    \n    Args:\n      emails_dir (str):\n        Root folders for emails.\n      return_dict (dict):\n        Shared dict for processed datas.\n    """"""\n\n    all_words = []\n    dirs = [os.path.join(emails_dir, f) for f in listdir(emails_dir)]\n    for d in dirs:\n        emails = [os.path.join(d, f) for f in listdir(d)]\n        for mail in emails:\n            with codecs.open(mail, ""r"", encoding=\'utf-8\', errors=\'ignore\') as m:\n                for line in m:\n                    words = line.split()\n                    all_words += words\n    dictionary = Counter(all_words)\n    list_to_remove = list(dictionary.keys())\n    return_dict[\'all_words\'] = dictionary\n    return_dict[\'list_to_remove\'] = list_to_remove\n\n\ndef create_enron_dictionary(root_dir: str = \'spampy/datasets/enron\') -> Dict:\n    """"""\n    A function which create a dictionary from enron dataset.\n    Uses multiple process.\n    \n    Args:\n      root_dir (str):\n        Root folders for enron dataset.\n    """"""\n\n    manager = mp.Manager()\n    return_dict = manager.dict()\n    jobs = []\n    emails_dirs = [os.path.join(root_dir, f) for f in listdir(root_dir)]\n    for emails_dir in emails_dirs:\n        p = mp.Process(target=enron_processor, args=(emails_dir, return_dict))\n        jobs.append(p)\n        p.start()\n\n    for proc in jobs:\n        proc.join()\n\n    dictionary = return_dict[\'all_words\']\n    list_to_remove = return_dict[\'list_to_remove\']\n\n    for item in list_to_remove:\n        if item.isalpha() == False: \n            del dictionary[item]\n        elif len(item) == 1:\n            del dictionary[item]\n    dictionary = dictionary.most_common(3000)\n    np.save(\'dict_enron.npy\', dictionary)\n    return dictionary\n\n\ndef features_processor(emails_dir: str, return_dict: Dict):\n    """"""\n    A function which processes data features into lists\n    and returns in a dictionary.\n    \n    Args:\n      emails_dir (str):\n        Root folders for emails.\n      return_dict (dict):\n        Shared dict for processed datas.\n    """"""\n\n    features_matrix = return_dict[\'features_matrix\']\n    train_labels = return_dict[\'train_labels\']\n    docID = 0\n    enron_dict = return_dict[\'enron_dict\']\n    dirs = [os.path.join(emails_dir, f) for f in os.listdir(emails_dir)]\n    for d in dirs:\n        emails = [os.path.join(d, f) for f in os.listdir(d)]\n        for mail in emails:\n            with open(mail) as m:\n                all_words = []\n                for line in m:\n                    words = line.split()\n                    all_words += words\n                for word in all_words:\n                    wordID = 0\n                    for i, d in enumerate(enron_dict):\n                        if d[0] == u\'word\':\n                            wordID = i\n                            features_matrix[docID, wordID] = all_words.count(word)\n            train_labels[docID] = int(mail.split(""."")[-2] == \'spam\')\n            docID = docID + 1\n    return_dict[\'features_matrix\'] = features_matrix\n    return_dict[\'train_labels\'] = train_labels\n\n\ndef extract_enron_features(root_dir: str = \'spampy/datasets/enron\') -> Tuple:\n    """"""\n    A function creates features and labels from enron dataset.\n    Uses multiple process and returns in a tuple.\n    \n    Args:\n      root_dir (str):\n        Root folders for enron dataset.\n    """"""\n\n    enron_dict = create_enron_dictionary(root_dir)\n    manager = mp.Manager()\n    return_dict = manager.dict()\n    return_dict[\'enron_dict\'] = enron_dict\n    features_matrix = np.zeros((33716, 3000))\n    train_labels = np.zeros(33716)\n    return_dict[\'features_matrix\'] = features_matrix\n    return_dict[\'train_labels\'] = train_labels\n    jobs = []\n    emails_dirs = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n    for emails_dir in emails_dirs:\n        p = mp.Process(target=features_processor, args=(emails_dir, return_dict))\n        jobs.append(p)\n        p.start()\n\n    for proc in jobs:\n        proc.join()\n\n    features_matrix = return_dict[\'features_matrix\']\n    train_labels = return_dict[\'train_labels\']\n    return np.array(features_matrix), np.array(train_labels)\n'"
spampy/spam_classifier.py,6,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os.path\nimport scipy.io as sio\nimport numpy as np\n\nfrom os.path import join, dirname\nfrom typing import List, Tuple\n\nfrom sklearn import svm\nfrom spampy import email_processor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\n\n# Parent directory\nparent_directory_path = dirname(__file__)\n# Support Vector Machine\nlinear_svm = svm.SVC(C=0.1, kernel=\'linear\')\nlinear_svc = LinearSVC()\n\n\ndef load_training_set() -> Tuple[List, List]:\n    """"""\n    Load training set and return features and labels.\n    Returns:\n      Training features and labels.\n    """"""\n\n    # Training set\n    training_set = join(parent_directory_path, \'datasets/spamTrain.mat\')\n    dataset = sio.loadmat(training_set)\n    X, y = dataset[\'X\'], dataset[\'y\']\n    return X, y\n\n\ndef load_test_set() -> Tuple[List, List]:\n    """"""\n    Load test set and return features and labels.\n    Returns:\n      Test features and labels.\n    """"""\n\n    training_set = join(parent_directory_path, \'datasets/spamTest.mat\')\n    dataset = sio.loadmat(training_set)\n    Xtest, ytest = dataset[\'Xtest\'], dataset[\'ytest\']\n    return Xtest, ytest\n\n\ndef train_svm():\n    """"""\n    Fit SVM with features and labels.\n    """"""\n\n    X, y = load_training_set()\n    linear_svm.fit(X, y.flatten())\n\n\ndef classify_email(email: str) -> int:\n    """"""\n    Classify spam possibility of given email.\n    Args:\n      email (str):\n        Raw e-mail.\n    Returns:\n      Spam or not.\n    """"""\n\n    train_svm()\n    vocablary_dict = email_processor.get_vocablary_dict()\n    feature_vector = email_processor.feature_vector_from_email(email, vocablary_dict)\n    double_dimesion_email = np.reshape(feature_vector, (-1, 1899))\n    spam_prediction = linear_svm.predict(double_dimesion_email)\n    return spam_prediction\n\n\ndef classify_email_with_enron(email: str) -> int:\n    """"""\n    Classify spam possibility of given email with enron dataset.\n    Args:\n      email (str):\n        Raw e-mail.\n    Returns:\n      Spam or not.\n    """"""\n\n    vocablary_dict = email_processor.create_enron_dictionary()\n    feature_vector = email_processor.feature_vector_from_email(email, vocablary_dict)\n    double_dimesion_email = np.reshape(feature_vector, (-1, 3000))\n    if os.path.exists(\'enron_features_matrix.npy\') == False & os.path.exists(\'enron_labels.npy\') == False:\n        features_matrix, labels = email_processor.extract_enron_features()\n        np.save(\'enron_features_matrix.npy\', features_matrix)\n        np.save(\'enron_labels.npy\', labels)\n    else:\n        features_matrix = np.load(\'enron_features_matrix.npy\')\n        labels = np.load(\'enron_labels.npy\')\n    X_train, _, y_train, _ = train_test_split(features_matrix, labels, test_size=0.40)\n    linear_svc.fit(X_train, y_train)\n    return linear_svc.predict(double_dimesion_email)\n'"
tests/test_dataset_downloader.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\nfrom spampy import dataset_downloader\n\n\nclass DatasetDownloaderTests(unittest.TestCase):\n\n    def test_download_enron_dataset(self):\n        dataset_downloader.download_enron_dataset()\n        self.assertTrue(os.path.exists('spampy/datasets/enron'))\n"""
tests/test_email_processor.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport unittest\nfrom spampy import email_processor\n\nclass EmailProcessorTests(unittest.TestCase):\n\n    def test_preprocess(self):\n        processed_email = email_processor.preprocess('<xyz@hotmail.com> Do You Want To Make $1000 Or More Per Week? https://github.com')\n        self.assertEqual(processed_email, '  do you want to make dollarnumber or more per week? httpaddr')\n\n\n    def test_create_tokenlist(self):\n        processed_email = email_processor.preprocess('<xyz@hotmail.com> Do You Want To Make $1000 Or More Per Week? https://github.com')\n        tokens = email_processor.create_tokenlist(processed_email)\n        self.assertEqual(len(tokens), 11)\n\n\n    def test_get_vocablary_dict(self):\n        vocablary_dict = email_processor.get_vocablary_dict()\n        self.assertEqual(len(vocablary_dict), 1899)\n\n\n    def test_get_vocablary_indices(self):\n        email = '<xyz@hotmail.com> Do You Want To Make $1000 Or More Per Week? https://github.com'\n        vocablary_dict = email_processor.get_vocablary_dict()\n        index_list = email_processor.get_vocablary_indices(email, vocablary_dict)\n        self.assertEqual(len(index_list), 0)\n\n\n    def test_feature_vector_from_email(self):\n        email = '<xyz@hotmail.com> Do You Want To Make $1000 Or More Per Week? https://github.com'\n        vocablary_dict = email_processor.get_vocablary_dict()\n        feature_vector = email_processor.feature_vector_from_email(email, vocablary_dict)\n        self.assertEqual(len(feature_vector), 1899)\n\n\n    def test_create_enron_dictionary(self):\n        enron_dictionary = email_processor.create_enron_dictionary()\n        self.assertEqual(len(enron_dictionary), 3000)\n"""
tests/test_spam_classifier.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport unittest\nimport os\n\nfrom spampy import spam_classifier\n\nclass SpamClassifierTests(unittest.TestCase):\n\n    def test_load_training_set(self):\n        X, y = spam_classifier.load_training_set()\n        self.assertIsNotNone(X)\n        self.assertIsNotNone(y)\n\n\n    def test_load_test_set(self):\n        Xtest, ytest = spam_classifier.load_test_set()\n        self.assertIsNotNone(Xtest)\n        self.assertIsNotNone(ytest)\n\n\n    def test_classify_email(self):\n        with open(os.path.join('tests/data', 'spam_sample.txt'), 'r') as f:\n            prediction = spam_classifier.classify_email(f.read())\n            self.assertEqual(prediction, 1)\n"""
