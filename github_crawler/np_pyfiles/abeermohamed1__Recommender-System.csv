file_path,api_count,code
compsup_logit.py,0,"b'# For Python examples, use spark-submit directly:\r\n#spark-submit path_to_python_file/python_file.py\r\n\r\nfrom pyspark import SparkContext\r\nfrom pyspark.sql import SQLContext\r\nfrom pyspark.mllib.classification import LogisticRegressionWithSGD\r\nfrom pyspark.mllib.regression import LabeledPoint\r\nfrom numpy import array, product\r\n######################################################\r\n\r\ntopicsNumber=9\r\n\r\nmainPath = ""file:////home/cloudera/BigDataCourseProject/baby/"";\r\n\r\nintermediatePath = mainPath+""intermediate/"";\r\n\r\nundirected_comp_sub_json_file = intermediatePath + ""undirected_relation_type_json.json"";\r\ndirected_comp_sub_json_file = intermediatePath + ""directed_relation_type_json.json"";\r\n\t\r\ndef parsePoint(point):\r\n\tpoint_dict=point.asDict()\r\n\tvalues = [float(point_dict.get(""f_""+str(x),0)) for x in range(topicsNumber)]\r\n\tlabel = float(point_dict.get(\'relation\',0))\r\n\treturn LabeledPoint(label,values)\r\n\r\ndef parseDirectedPoint(point):\r\n\tpoint_dict=point.asDict()\r\n\t# 2 ids, topic features, 2 -> price, brand, label\r\n\tvalues = [float(point_dict.get(""f_""+str(x),0)) for x in range(topicsNumber)]\r\n\tvalues.extend([float(point_dict.get(""brand"",0)),float(point_dict.get(\'price\',0))])\r\n\tlabel = float(point_dict.get(\'relation\',0))\r\n\treturn LabeledPoint(label,values)\r\n\t\r\ndef getIds_likelyhood(point):\r\n\tpoint_dict=point.asDict()\r\n\tvalues = [float(point_dict.get(""f_""+str(x),0)) for x in range(topicsNumber)]\r\n\treturn (point_dict.get(""asin_1"",\'\'),point_dict.get(""asin_2"",\'\'),product(values))\r\n\r\n\r\n######################################################\r\nsc = SparkContext(appName=""Complementary and Supplementary Logistic Regression"")\r\nsc.setLogLevel(""ERROR"")\r\nsqlContext = SQLContext(sc)\r\n\r\n############################# Undirected #########################\r\n### Prepare Undirected Data\r\n\r\nundirected_comp_sub_df = sqlContext.read.load(undirected_comp_sub_json_file, format=""json"").limit(100)\r\nundirectedParsedData = undirected_comp_sub_df.map(lambda point: parsePoint(point))\r\n\r\n### Build ModeL\r\nundirectedModel = LogisticRegressionWithSGD.train(undirectedParsedData)\r\nundirectedModel.clearThreshold()\r\nundirectedModel.save(sc,mainPath+""comp_sub_undireced_model"")\r\nundirectedLabelsAndPreds = undirectedParsedData.map(lambda point: (point.label,undirectedModel.predict(point.features)))\r\nundirectedLabelsAndPredsIndexed=undirectedLabelsAndPreds.zipWithIndex().map(lambda (x,y): (y,x));\r\n\r\nundirectedLabelsAndPreds.unpersist()\r\nundirectedParsedData.unpersist()\r\n\r\n#extract productsIds \r\nproductsIds = undirected_comp_sub_df.map(lambda point: getIds_likelyhood(point))\r\nproductsIdsIndexed = productsIds.zipWithIndex().map(lambda (x,y): (y,x));\r\n\r\nproductsIds_undirectedLabelsAndPredsIndexed = productsIdsIndexed.join(undirectedLabelsAndPredsIndexed)\r\n\r\nproductsIdsIndexed.unpersist()\r\nundirectedLabelsAndPredsIndexed.unpersist()\r\n\r\n######################################################\r\n### Prepare Directed Data\r\n\r\ndirected_comp_sub_df = sqlContext.read.load(directed_comp_sub_json_file, format=""json"").limit(100)\r\ndirectedParsedData = directed_comp_sub_df.map(lambda point: parseDirectedPoint(point))\r\n\r\n######################################################\r\n\r\ndirectedModel = LogisticRegressionWithSGD.train(directedParsedData)\r\ndirectedModel.clearThreshold()\r\ndirectedModel.save(sc,mainPath+""comp_sub_direced_model"")\r\n\r\ndirectedLabelsAndPreds = directedParsedData.map(lambda point: (point.label,directedModel.predict(point.features)))\r\ndirectedLabelsAndPredsIndexed=directedLabelsAndPreds.zipWithIndex().map(lambda (x,y): (y,x));\r\n\r\n#print(""@@@@@@@@@@@@@@@@@@@@@ 0 @@@@@@@@@@@@@@@@@@@@@@"")\r\n#print((undirectedLabelsAndPreds.join(directedLabelsAndPreds)).take(15))\r\n#print(""@@@@@@@@@@@@@@@@@@@@@ END0 @@@@@@@@@@@@@@@@@@@@@@"")\r\n\r\n######################################################\r\n### Prepare Data for Graph\r\ngraphCompSupData = productsIds_undirectedLabelsAndPredsIndexed.join(directedLabelsAndPredsIndexed)\r\n\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ 1 @@@@@@@@@@@@@@@@@@@@@@"")\r\nprint(graphCompSupData.take(20))\r\n## (idx, (((id1, id2,likeliHood), (lbl1, prd1)), (lbl2, prd2)))\r\nprint(""@@@@@@@@@@@@@@@@@@@@@@ END1 @@@@@@@@@@@@@@@@@@@@@"")\r\n\r\ntrainThreshold = 0.2\r\n\r\ndef getCompSub(prd1,prd2) :\r\n\treturn 1 if (float(prd1)*float(prd2))> trainThreshold else 0\r\n\t\r\n\r\ngraphCompSupData = graphCompSupData.map(lambda (idx, (((id1, id2,likeliHood), (lbl1, prd1)), (lbl2, prd2))):(id1,id2, getCompSub(prd1,prd2) ,lbl1))\r\n\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ 1 @@@@@@@@@@@@@@@@@@@@@@"")\r\nfor i in graphCompSupData.filter(lambda (id1,id2,pred,lbl): 0 ==  lbl ).take(20):\r\n\tprint(i)\r\n## (idx, (((id1, id2), (lbl1, prd1)), (lbl2, prd2)))\r\nprint(""@@@@@@@@@@@@@@@@@@@@@@ END1 @@@@@@@@@@@@@@@@@@@@@"")\r\n\r\n######################################################\r\n### Calculate Model Accuracy\r\n\r\ntesting=graphCompSupData.map(lambda (id1,id2,pred,lbl): (id1,id2,pred,lbl, pred ==  lbl) )\r\n\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ 1 @@@@@@@@@@@@@@@@@@@@@@"")\r\nfor i in testing.take(100):\r\n\tprint(i)\r\n## (idx, (((id1, id2), (lbl1, prd1)), (lbl2, prd2)))\r\nprint(""@@@@@@@@@@@@@@@@@@@@@@ END1 @@@@@@@@@@@@@@@@@@@@@"")\r\n\r\ntrainErr = testing.filter(lambda (id1,id2,pred,lbl,success): not success ).count()\r\n\r\ntrainErr = float(trainErr) / float(graphCompSupData.count())\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ 2 @@@@@@@@@@@@@@@@@@@@@@"")\r\nprint(""Training Error = "" + str(trainErr*100)+""%"")\r\nprint(""@@@@@@@@@@@@@@@@@@@@@@ END2 @@@@@@@@@@@@@@@@@@@@@"")\r\n\r\n######################################################\r\n### Write Graph Data on Hadoop\r\nfinal_graphCompSupData=graphCompSupData.map(lambda (id1,id2,pred,lbl):(id1,id2,pred))\r\nfinal_graphCompSupData.saveAsTextFile(intermediatePath+""graph_compsub"")\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ 3 @@@@@@@@@@@@@@@@@@@@@@"")\r\nprint(final_graphCompSupData.take(5))\r\nprint(""@@@@@@@@@@@@@@@@@@@@@@ END3 @@@@@@@@@@@@@@@@@@@@@"")\r\n\r\n######################################################\r\nsc.stop()\r\n'"
prepare _data_files_json.py,0,"b'\r\nimport json\r\nimport ast\r\n\r\n\r\nmeta_file=\'baby/meta_Baby.json\'\r\nmeta_json_file=\'baby/meta_json_Baby.json\'\r\nprod_text_json_file=\'baby/prod_text_json_Baby.json\'\r\nprod_no_relation_json_file=\'baby/prod_no_relation_json_Baby.json\'\r\n\r\n# preparing meta data - title - description\r\nprint(""preparing meta data - title - description"")\r\nmeta_json_file_target = open(meta_json_file, \'w\')\r\nprod_text_json_file_target = open(prod_text_json_file, \'w\')\r\nprod_no_relation_json_file_target = open(prod_no_relation_json_file, \'w\')\r\nwith open(meta_file) as f:\r\n    for line in f:\r\n        prod_meta_data = {}\r\n        json_data=ast.literal_eval(line)\r\n        prod_meta_data[""asin""]=json_data[""asin""]\r\n        prod_meta_data[""brand""]=json_data.get(""brand"",\'\')\r\n        prod_meta_data[""price""]=json_data.get(""price"",0)\r\n        # add title and description to reviews\r\n        prod_text_data = {}\r\n        product_title=json_data.get(""title"",\'\')\r\n        product_desc=json_data.get(""description"",\'\')\r\n        prod_meta_data_title_len = len(product_title)\r\n        prod_meta_data_desc_len = len(product_desc)\r\n        prod_meta_data_title_desc = (\' \' if prod_meta_data_title_len > 0 else \'\') + product_title + (\' \' if prod_meta_data_desc_len > 0 else \'\') + product_desc\r\n        if len(prod_meta_data_title_desc) > 0:\r\n            prod_text_data[""asin""]=json_data[""asin""]\r\n            prod_text_data[""text""] = "" "".join([prod_meta_data_title_desc])\r\n\r\n        ## also viewed also bought\r\n        has_also_viewed = False\r\n        has_also_bought = False\r\n\r\n        related = json_data.get(""related"", [])\r\n        if len(related) > 0:\r\n            also_viewed = related.get(""also_viewed"", [])\r\n            has_also_viewed = len(also_viewed) > 0\r\n            if has_also_viewed:\r\n                prod_meta_data[""also_viewed""] = also_viewed\r\n\r\n            also_bought = related.get(""also_bought"", [])\r\n            has_also_bought = len(also_bought) > 0\r\n            if has_also_bought:\r\n                prod_meta_data[""also_bought""] = also_bought\r\n            # if has no also viewed or also bought ==> has no relation\r\n            if not has_also_viewed and not has_also_bought:\r\n                prod_no_relation_data = {}\r\n                prod_no_relation_data[""no_relation""] = prod_meta_data[""asin""]\r\n                prod_no_relation_json_file_target.write(json.dumps(prod_no_relation_data))\r\n                prod_no_relation_json_file_target.write(\'\\n\')\r\n\r\n            #writing to files\r\n            #meta data\r\n            meta_json_file_target.write(json.dumps(prod_meta_data))\r\n            meta_json_file_target.write(\'\\n\')\r\n            #prod text\r\n            prod_text_json_file_target.write(json.dumps(prod_text_data))\r\n            prod_text_json_file_target.write(\'\\n\')\r\n\r\n        ## has no relation\r\n        else:\r\n            prod_no_relation_data = {}\r\n            prod_no_relation_data[""no_relation""]=prod_meta_data[""asin""]\r\n            prod_no_relation_json_file_target.write(json.dumps(prod_no_relation_data))\r\n            prod_no_relation_json_file_target.write(\'\\n\')\r\n            # writing to files\r\n            # meta data\r\n            meta_json_file_target.write(json.dumps(prod_meta_data))\r\n            meta_json_file_target.write(\'\\n\')\r\n            # prod text\r\n            prod_text_json_file_target.write(json.dumps(prod_text_data))\r\n            prod_text_json_file_target.write(\'\\n\')\r\n\r\n            continue\r\n\r\nmeta_json_file_target.close()\r\nprod_no_relation_json_file_target.close()\r\n\r\n# preparing reviews\r\nprint(""preparing reviews"")\r\nreviews_file=\'baby/reviews_Baby_5.json\'\r\n\r\nwith open(reviews_file) as f:\r\n    for line in f:\r\n        prod_reviews={}\r\n        json_data = ast.literal_eval(line)\r\n        prod_reviews[""asin""]=json_data[""asin""]\r\n        prod_reviews[""text""]=json_data[""reviewText""]\r\n        prod_text_json_file_target.write(json.dumps(prod_reviews))\r\n        prod_text_json_file_target.write(\'\\n\')\r\n\r\n\r\nqa_file=\'baby/qa_Baby.json\'\r\n\r\n# preparing QAs\r\nprint(""preparing QAs"")\r\nwith open(qa_file) as f:\r\n    for line in f:\r\n        prod_qa = {}\r\n        json_data = ast.literal_eval(line)\r\n        prod_qa[""asin""]=json_data[""asin""]\r\n        prod_qa[""text""]="" "".join([json_data.get(""question"",\'\'),json_data.get(""answer"",\'\')])\r\n        prod_text_json_file_target.write(json.dumps(prod_qa))\r\n        prod_text_json_file_target.write(\'\\n\')\r\n\r\n\r\nprod_text_json_file_target.close()\r\n\r\n'"
relation_logit.py,0,"b'from pyspark import SparkContext\r\nfrom pyspark.sql import SQLContext\r\nfrom pyspark.mllib.classification import LogisticRegressionWithSGD\r\nfrom pyspark.mllib.regression import LabeledPoint\r\nfrom numpy import array\r\n######################################################\r\ntopicsNumber=9\r\n\r\nmainPath = ""file:////home/cloudera/BigDataCourseProject/baby/"";\r\n\r\nintermediatePath = mainPath+""intermediate/"";\r\n\r\nundirected_relation_json_file = intermediatePath + ""undirected_relation_json.json"";\r\ndirected_relation_json_file = intermediatePath + ""directed_relation_json.json"";\r\n\r\n\t\r\ndef parsePoint(point):\r\n\tpoint_dict=point.asDict()\r\n\tvalues = [float(point_dict.get(""f_""+str(x),0)) for x in range(topicsNumber)]\r\n\tlabel = float(point_dict.get(\'relation\',0))\r\n\treturn LabeledPoint(label,values)\r\n\r\ndef parseDirectedPoint(point):\r\n\tpoint_dict=point.asDict()\r\n\t# 2 ids, topic features, 2 -> price, brand, label\r\n\tvalues = [float(point_dict.get(""f_""+str(x),0)) for x in range(topicsNumber)]\r\n\tvalues.extend([float(point_dict.get(""brand"",0)),float(point_dict.get(\'price\',0))])\r\n\tlabel = float(point_dict.get(\'relation\',0))\r\n\treturn LabeledPoint(label,values)\r\n\t\r\ndef getIds(point):\r\n\tpoint_dict=point.asDict()\r\n\treturn (point_dict.get(""asin_1"",\'\'),point_dict.get(""asin_2"",\'\'))\r\n\t\r\n######################################################\r\nsc = SparkContext(appName=""Relation Logistic Regression"")\r\nsc.setLogLevel(""ERROR"")\r\nsqlContext = SQLContext(sc)\r\n\r\n### Prepare Undirected Data\r\n\r\nundirected_relation_df = sqlContext.read.load(undirected_relation_json_file, format=""json"")\r\n#.limit(100)\r\nundirectedParsedData = undirected_relation_df.map(lambda point: parsePoint(point))\r\n\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ 0 @@@@@@@@@@@@@@@@@@@@@@"")\r\nprint(""undirectedParsedData == ""+str(undirectedParsedData.take(10)))\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ END0 @@@@@@@@@@@@@@@@@@@@@@"")\r\n\r\n\r\n### Build ModeL undirected\r\nundirectedModel = LogisticRegressionWithSGD.train(undirectedParsedData)\r\nundirectedModel.clearThreshold()\r\nundirectedModel.save(sc,mainPath+""undireced_relation_model"")\r\nundirectedLabelsAndPreds = undirectedParsedData.map(lambda point: (point.label,float(undirectedModel.predict(point.features))))\r\nundirectedLabelsAndPredsIndexed = undirectedLabelsAndPreds.zipWithIndex().map(lambda (x,y): (y,x));\r\n\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ 0 @@@@@@@@@@@@@@@@@@@@@@"")\r\nprint(""undirectedLabelsAndPredsIndexed == ""+str(undirectedLabelsAndPredsIndexed.take(10)))\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ END0 @@@@@@@@@@@@@@@@@@@@@@"")\r\n\r\n######################################################\r\n#join with productsIds \r\nproductsIds = undirected_relation_df.map(lambda point: getIds(point))\r\nproductsIdsIndexed = productsIds.zipWithIndex().map(lambda (x,y): (y,x));\r\n\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ 3 @@@@@@@@@@@@@@@@@@@@@@"")\r\nprint(""productsIds == ""+str(productsIds.take(10)))\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ END3 @@@@@@@@@@@@@@@@@@@@@@"")\r\n\r\nproductsIds_undirectedLabelsAndPredsIndexed = productsIdsIndexed.join(undirectedLabelsAndPredsIndexed)\r\nproductsIdsIndexed.unpersist()\r\nundirectedLabelsAndPredsIndexed.unpersist()\r\nundirectedLabelsAndPreds.unpersist()\r\n######################################################\r\n### Prepare Directed Data\r\n\r\ndirected_relation_df = sqlContext.read.load(directed_relation_json_file, format=""json"")\r\n#.limit(100)\r\ndirectedParsedData = directed_relation_df.map(lambda point: parseDirectedPoint(point))\r\n\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ 0 @@@@@@@@@@@@@@@@@@@@@@"")\r\nprint(""directedParsedData   == ""+str(directedParsedData.take(10)))\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ END0 @@@@@@@@@@@@@@@@@@@@@@"")\r\n######################################################\r\n### Build ModeL directed\r\n\r\ndirectedModel = LogisticRegressionWithSGD.train(directedParsedData)\r\ndirectedModel.clearThreshold()\r\ndirectedModel.save(sc,mainPath+""direced_relation_model"")\r\ndirectedLabelsAndPreds = directedParsedData.map(lambda point: (point.label,float(directedModel.predict(point.features))))\r\ndirectedLabelsAndPredsIndexed = directedLabelsAndPreds.zipWithIndex().map(lambda (x,y): (y,x));\r\n\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ 0 @@@@@@@@@@@@@@@@@@@@@@"")\r\nprint(""directedLabelsAndPredsIndexed   == ""+str(directedLabelsAndPredsIndexed.take(10)))\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ END0 @@@@@@@@@@@@@@@@@@@@@@"")\r\n\r\n\r\ngraphRelationData = productsIds_undirectedLabelsAndPredsIndexed.join(directedLabelsAndPredsIndexed)\r\n\r\nproductsIds_undirectedLabelsAndPredsIndexed.unpersist();\r\ndirectedLabelsAndPredsIndexed.unpersist();\r\ndirectedLabelsAndPreds.unpersist();\r\n\n######################################################\r\n### Prepare Data for Graph\nprint(""@@@@@@@@@@@@@@@@@@@@@ 4 @@@@@@@@@@@@@@@@@@@@@@"")\r\nprint(graphRelationData.take(10))\r\n#(idx, (((id1, id2), (lbl1, prd1)), (lbl2, prd2)))\r\nprint(""@@@@@@@@@@@@@@@@@@@@@@ END4 @@@@@@@@@@@@@@@@@@@@@"")\r\n#graphRelationData = graphRelationData.map(lambda (((id1, id2), (lbl1, prd1)), (lbl2, prd2)):(id1,id2,float(prd1)*(1-float(prd1))*float(prd2)*(1-float(prd2))))\r\n#  ==> (id1,id2,pred,lbl)\r\ngraphRelationData = graphRelationData.map(lambda (idx, (((id1, id2), (lbl1, prd1)), (lbl2, prd2))):(id1,id2,float(prd1)*float(prd2),lbl1))\r\n\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ 1 @@@@@@@@@@@@@@@@@@@@@@"")\r\nfor i in graphRelationData.filter(lambda (id1,id2,pred,lbl): 0 ==  lbl ).take(20):\r\n\tprint(i)\r\n## (idx, (((id1, id2), (lbl1, prd1)), (lbl2, prd2)))\r\nprint(""@@@@@@@@@@@@@@@@@@@@@@ END1 @@@@@@@@@@@@@@@@@@@@@"")\r\n\n######################################################\r\n### Calculate Model Accuracy\r\ntrainThreshold = 0.2\r\ntesting=   graphRelationData.map(lambda (id1,id2,pred,lbl): (id1,id2,pred,lbl,(1 if float(pred)>float(trainThreshold) else 0) ==  lbl) )\r\n\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ 1 @@@@@@@@@@@@@@@@@@@@@@"")\r\nfor i in testing.take(100):\r\n\tprint(i)\r\n## (idx, (((id1, id2), (lbl1, prd1)), (lbl2, prd2)))\r\nprint(""@@@@@@@@@@@@@@@@@@@@@@ END1 @@@@@@@@@@@@@@@@@@@@@"")\r\n\r\ntrainErr = testing.filter(lambda (id1,id2,pred,lbl,success): not success).count()\r\n\ntrainErr = float(trainErr) / float(graphRelationData.count())\r\n\nprint(""@@@@@@@@@@@@@@@@@@@@@ 5 @@@@@@@@@@@@@@@@@@@@@@"")\nprint(""Training Error = "" + str(trainErr*100)+""%"")\r\nprint(""@@@@@@@@@@@@@@@@@@@@@@ END5 @@@@@@@@@@@@@@@@@@@@@"")\n\n######################################################\r\n### Write Graph Data on Hadoop\ngraphRelationData.filter(lambda (id1,id2,pred,lbl): float(pred)>float(trainThreshold) ).map(lambda (id1,id2,pred,lbl):(id1,id2,pred)).saveAsTextFile(intermediatePath+""graph_relation"")\r\nprint(""@@@@@@@@@@@@@@@@@@@@@ 6 @@@@@@@@@@@@@@@@@@@@@@"")\r\nprint(graphRelationData.take(5))\r\nprint(""@@@@@@@@@@@@@@@@@@@@@@ END6 @@@@@@@@@@@@@@@@@@@@@"")\r\n\r\n######################################################\r\nsc.stop()\n'"
