file_path,api_count,code
pragmaticml/__init__.py,0,b''
pragmaticml/confusion_matrix.py,0,"b'""""""@author: Abhijit Kar""""""\n\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef tp(y_test, y_pred, id):\n    """"""Intersection of the class\'s row and column""""""\n    conf_mat = metrics.confusion_matrix(y_test, y_pred)\n    return conf_mat[id, id]\n\ndef tn(y_test, y_pred, id):\n    """"""Sum of all rows and columns excluding that class\'s row and column""""""\n    conf_mat = metrics.confusion_matrix(y_test, y_pred)\n    return conf_mat.sum() - conf_mat[id].sum() - conf_mat[:,id].sum() + conf_mat[id, id]\n\ndef fp(y_test, y_pred, id):\n    """"""Sum of values in the class\'s column""""""\n    conf_mat = metrics.confusion_matrix(y_test, y_pred)\n    return conf_mat[:, id].sum() - conf_mat[id, id]\n\ndef fn(y_test, y_pred, id):\n    """"""Sum of values in the class\'s row""""""\n    conf_mat = metrics.confusion_matrix(y_test, y_pred)\n    return conf_mat[id].sum() - conf_mat[id, id]\n\ndef recall(y_test, y_pred, id):\n    """"""TP / (TP + FN) - When actual value is positive, How often is the prediction correct?""""""\n    return tp(y_test, y_pred, id) / float(tp(y_test, y_pred, id) + fn(y_test, y_pred, id))\n\ndef specificity(y_test, y_pred, id):\n    """"""TN / (TN + FP) - When the actual value is negative, How often is the prediction correct?""""""\n    return tn(y_test, y_pred, id) / float(tn(y_test, y_pred, id) + fp(y_test, y_pred, id))\n\ndef precision(y_test, y_pred, id):\n    """"""TP / (TP + FP) - When a positive value is predicted, how often is the prediction correct?""""""\n    return tp(y_test, y_pred, id) / float(tp(y_test, y_pred, id) + fp(y_test, y_pred, id))\n\nfunc_keys = [\'TP\', \'TN\', \'FN\', \'FP\', \'Recall\', \'Specificity\', \'Precision\']\nfunc_vals = [tp, tn, fn, fp, recall, specificity, precision]\n\ndef show(y_test, y_pred):\n    """"""Returns a Data Frame showcasing the confusion matrix""""""\n    conf_mat = metrics.confusion_matrix(y_test, y_pred)\n    categories = y_test.astype(\'category\').cat.categories\n    return pd.DataFrame(conf_mat, columns = categories, index = categories)\n\ndef visualize(y_test, y_pred):\n    """"""Draws a HeatMap of the confusion matrix""""""\n    conf_mat = metrics.confusion_matrix(y_test, y_pred)\n    categories = y_test.astype(\'category\').cat.categories\n    \n    sns.set_context(\'talk\')\n    ax = plt.subplot()\n    \n    sns.heatmap(conf_mat, cmap = \'Blues_r\', xticklabels = categories, yticklabels = categories)\n    \n    plt.title(\'Confusion Matrix\')\n    plt.xlabel(\'Predicted\\n\')\n    plt.ylabel(\'Actual\\n\')\n    \n    ax.title.set_position([0.5, -0.18])\n    ax.xaxis.set_ticks_position(\'top\')\n    ax.xaxis.set_label_position(\'top\')\n    \n    plt.show()\n\ndef describe(y_test, y_pred):\n    """"""Returns a DataFrame with all the metrics calculated from Confusion Matrix at once""""""\n    categories = y_test.astype(\'category\').cat.categories\n    return pd.DataFrame([[func(y_test, y_pred, i) for func in func_vals] for i in range(len(categories))],\n                          columns = func_keys, index = categories)\n\nif __name__ == ""__main__"":\n    pima_df = pd.read_csv(\'../data/pima.csv\', dtype = {\'diabetes\': \'int8\'})\n    feature_cols = [\'num_preg\', \'insulin\', \'bmi\', \'age\']\n    X = pima_df[feature_cols]\n    y = pima_df.diabetes\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n\n    logreg = LogisticRegression()\n    logreg.fit(X_train, y_train)\n    y_pred = logreg.predict(X_test)\n\n    print(show(y_test, y_pred))\n    visualize(y_test, y_pred)\n    print(describe(y_test, y_pred))\n\n    assert (metrics.recall_score(y_test, y_pred) == recall(y_test, y_pred, 1)), ""Recall Doesn\'t Match""\n    assert (metrics.precision_score(y_test, y_pred) == precision(y_test, y_pred, 1)), \'Precision Failed\'\n'"
