file_path,api_count,code
Utils.py,17,"b'""""""\nLoad this script to your python by running the following code\nimport requests\nurl = ""https://raw.githubusercontent.com/abulbasar/machine-learning/master/Utils.py""\nexec(requests.get(url).text)\n   \n""""""\nimport math\nimport pandas as pd\nimport numpy as np\nimport scipy\nimport matplotlib.pyplot as plt\nfrom sklearn import decomposition, preprocessing\n\n      \nclass Batchable(object):\n    """"""\n    Create a batchable object that would return batched X and y values.\n    \n    Usage:\n    ....\n    init = tf.global_variables_initializer()\n    batchable = Batchable(X_train, y_train)\n    with tf.Session() as sess:\n    init.run()\n    for i, progress, X_batch, y_batch in batchable.next():\n        sess.run(opt, feed_dict={X: X_batch, y: y_batch})\n        if i % (bachable.max_iters // 20) == 0:\n            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n            print(""Progress:%3d%%"" % progress, \n                  ""Train accuracy: %.4f"" % acc_train, \n                  ""Test accuracy: %.4f"" % acc_test)\n    \n    \n    """"""\n    \n    def __shuffle(self):\n        X = self.X\n        y = self.y\n        idx = np.arange(X.shape[0])\n        np.random.shuffle(idx)\n        self.X = X[idx]\n        self.y = y[idx]\n        self.require_shuffle = False\n        return\n    \n    def __init__(self, X, y, batch_size = 32, epochs = 10, seed = 1):\n        \n        if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n            raise ValueError(\'Both X and y must be np.ndarray\')\n        \n        if X.shape[0] != y.shape[0]:\n            raise ValueError(""X and y must be of same size of axis=0"")\n        \n        from math import ceil\n        np.random.seed(seed)\n        self.X = X\n        self.y = y\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.num_batches = ceil(X.shape[0] / batch_size)\n        self.max_iters = self.epochs * self.num_batches\n        self.__shuffle()\n        \n    def next(self):\n        iteration, progress = 0, 0\n        for i in range(self.epochs):\n            self.current_epoch = i\n            for j in range(self.num_batches):\n                self.current_batch = j\n                start = j * self.batch_size\n                end = start + self.batch_size\n                iteration = iteration + 1\n                progress = int(100 * iteration / self.max_iters) + 1\n                yield iteration, progress, self.X[start:end], self.y[start:end]\n            self.__shuffle()\n\n\ndef data_generator(X, y, batch_size = 32, epochs = 1):\n    from collections import namedtuple\n    from math import ceil\n    Batch = namedtuple(""batch"", [""epoch"", ""global_step"", ""progress"", ""X_batch"", ""y_batch""])\n    global_step = 0\n    for epoch in range(epochs):\n        m = X.shape[0]\n        indices = np.arange(m)\n        np.random.shuffle(indices)\n        X = X[indices]\n        y = None if y is None else y[indices]\n        num_batches = ceil(m/batch_size)\n        for j in range(num_batches):\n            start = j * batch_size\n            end = start + batch_size\n            X_batch = X[start:end]\n            y_batch = None if y is None else y[start:end]\n            progress = (j + 1) * 100 / num_batches\n            yield Batch(epoch, global_step, progress, X_batch, y_batch)\n            global_step = global_step + 1\n            \n\ndef plot_scores(scores, window = 10):\n\n   """"""\n   Parameters: \n   scores: dict containing iteration index as key and the cost value as value.\n   window: length of the rolling window\n   plt: matplotlib.pyplot module. Import it as plt if necessary.\n   \n   Output:\n   Displays cost decay curve with rolling mean. \n   """"""\n   s = pd.Series(scores)\n   plt.plot(s, label = ""original"", alpha = 0.3, color = ""steelblue"")\n   plt.plot(s.rolling(window).mean(), label = ""rolling mean"", color = ""steelblue"")\n   plt.legend()\n   plt.xlabel(""Iterations"")\n   plt.ylabel(""Cost"")\n   plt.title(""Cost decay over iterations"")\n   \n   \nclass CifarLoader(object):\n    """"""\n    Loads CIFAR10 dataset\n    \n    """"""\n   \n    def load_data(self, files):\n        import pickle\n        import numpy as np\n        X = np.empty([0, 3072])\n        y = np.empty([0])\n        for path in files:\n            print(path)\n            with open(path, ""rb"") as f:\n                d = pickle.load(f, encoding=\'bytes\')\n                X = np.vstack([X, d[b""data""]]).astype(""uint8"")\n                y = np.hstack([y, d[b""labels""]]).astype(""uint8"")\n        return X, y\n        \n    def __init__(self, data_path):\n        import os, pickle\n        \n        training_files = [os.path.join(data_path, ""data_batch_{0}"".format(i))  for i in range(1, 6)]\n        test_files = [os.path.join(data_path, ""test_batch"")]\n        labels_file = os.path.join(data_path, ""batches.meta"")\n\n        X_train, y_train = self.load_data(training_files)\n        X_test, y_test = self.load_data(test_files)\n        \n        self.X_train = X_train.reshape([-1, 3, 32, 32]).transpose([0, 2, 3, 1])/255\n        self.X_test = X_test.reshape([-1, 3, 32, 32]).transpose([0, 2, 3, 1])/255\n        self.y_train = y_train\n        self.y_test = y_test\n        \n        with open(labels_file, ""rb"") as f:\n            labels = pickle.load(f, encoding=""bytes"")\n        labels = [s.decode(""utf-8"")  for s in labels[b\'label_names\']]\n        \n        self.labels = labels\n    \n    def __repr__(self):\n        row_format =""{:<15}"" * 2\n        lines = [\n            row_format.format(""X_train"", str(self.X_train.shape)),\n            row_format.format(""X_test"", str(self.X_test.shape)),\n            row_format.format(""y_train"", str(self.y_train.shape)),\n            row_format.format(""y_test"", str(self.y_test.shape)),\n            row_format.format(""labels"", str(self.labels))\n        ]\n        return ""\\n"".join(lines)\n\n   \n   \nimport numpy as np\n\ndef outliers(y):\n    q1, q3 = np.percentile(y, [25, 75])\n    iqr = q3 - q1\n    lower_bound = max(np.min(y), q1 - (iqr * 1.5))\n    upper_bound = min(np.max(y), q3 + (iqr * 1.5))\n    return (y > upper_bound) | (y < lower_bound)\n\n\ndef load_mnist_csv(path = ""/data/MNIST/"", one_hot = False, shape = None):\n    df_train = pd.read_csv(path + ""mnist_train.csv"", header=None)\n    df_test = pd.read_csv(path + ""mnist_test.csv"", header=None)\n    \n    X_train = df_train.iloc[:, 1:].values/255\n    X_test = df_test.iloc[:, 1:].values/255\n    y_train = df_train.iloc[:, 0].values\n    y_test = df_test.iloc[:, 0].values\n    \n    if shape == ""2D"":\n        X_train = X_train.reshape(-1, 28, 28)\n        X_test = X_test.reshape(-1, 28, 28)\n        \n    if shape == ""3D"":\n        X_train = X_train.reshape(-1, 28, 28, 1)\n        X_test = X_test.reshape(-1, 28, 28, 1)\n    \n    if one_hot:\n        eye = np.eye(len(np.unique(y_train)))\n        y_train, y_test = eye[y_train], eye[y_test]\n        \n    return X_train, X_test, y_train, y_test\n\n\n\n\ndef to_categorical(y):\n    y = y.flatten()\n    depth = len(np.unique(y))\n    eye = np.depth(depth)\n    return eye[y]\n'"
compare.py,3,"b'#!/usr/bin/python3\n\nimport sys\n\nimport random\nimport math \nimport numpy as np\nimport tensorflow as tf\nfrom time import time\n\n\ndef python_(x1, x2):\n    m = len(x1)\n    total = 0\n    for i in range(m):\n        total += math.sqrt(x1[i] ** 2 + x2[i] ** 2)\n    return total/m\n    \n    \ndef numpy_(x1, x2):\n    avg = np.mean(np.sqrt(x1 ** 2 + x2 ** 2))\n    return avg\n\ndef tensorflow_(x1, x2):\n  m = len(x1)\n  x1_ = tf.placeholder(""float64"", [m])\n  x2_ = tf.placeholder(""float64"", [m])\n  distances = tf.sqrt(tf.square(x1_) + tf.square(x2_))\n  avg = tf.reduce_mean(distances)\n  \n  with tf.Session() as sess:\n    avg_ = sess.run(avg, feed_dict={x1_: x1, x2_: x2})\n    return avg_\n    \n    \nif __name__ == ""__main__"":\n    N = 10 ** 6\n    if len(sys.argv) > 1:\n      N = int(sys.argv[1])\n    print(""Total nunber of records: "", N)\n    \n    random.seed(1)\n    x1 = [random.random() for i in range(N)]\n    x2 = [random.random() for i in range(N)]\n    #print(""x1"", x1)\n    #print(""x2"", x2)\n\n    start = time()\n    result = python_(x1, x2)\n    print(""Python result"", result, ""time: "", time() - start, "" ms"")\n\n\n    x1_np = np.array(x1, dtype = np.float64)\n    x2_np = np.array(x2)\n    \n    start = time()\n    result = numpy_(x1_np, x2_np)\n    print(""Numpy result"", result, ""time: "", time() - start, "" ms"")\n    \n    \n    start = time()\n    result = tensorflow_(x1, x2)\n    print(""Tensorflow result"", result, ""time: "", time() - start, "" ms"")\n    '"
Flask-ML/app.py,0,"b'#!/usr/bin/env python\n# coding: utf-8\n\n\nfrom flask import Flask, render_template, request, redirect\nimport pandas as pd\nimport pickle\n\napp = Flask(__name__)\n\n# Load the model\nwith open(""/tmp/model.pickle"", ""rb"") as f:\n    model = pickle.load(f)\n\n# Function to predict single input record\ndef predict(record):\n    df_input = pd.DataFrame.from_dict([record])\n    prediction = 10 ** model.predict(df_input)\n    return prediction[0]\n\n\n@app.route(\'/\')\ndef home():\n    return render_template(\'input.html\')\n    \n\n@app.route(\'/predict_price\',methods = [""GET"", ""POST""])\ndef result():\n    request_data = None\n    prediction = None\n    if request.method == ""POST"":\n        request_data = dict(request.form)\n        print(request_data)\n        prediction = predict(request_data)\n        return render_template(""output.html"", record = request_data, prediction = prediction)\n    return redirect(""/"", code=302)\n    \nif __name__ == \'__main__\':\n    app.run(debug=True)\n    \n    \n    '"
Flask-ML/rest-app.py,0,"b'from flask import Flask\nfrom flask_restful import reqparse, abort, Api, Resource\n\nimport pandas as pd\nimport pickle\n\nimport os.path\nimport sys\n\napp = Flask(__name__)\napi = Api(app)\n\n\nclass InsuranceCalculator(Resource):\n    \n    def __init__(self):\n        \n        # Load the model. If the model does not exist, run train.py to build the model.\n        model_path = ""/tmp/model.pickle""\n\n        if not os.path.isfile(model_path):\n            print(""""""\n            [Error] Saved model is not found {}.\n            First run train.py to build the model and relaunch flask server.\\n\\n\n            """""".format(model_path))\n            sys.exit(1)\n            \n        with open(model_path, ""rb"") as f:\n            self.model = pickle.load(f)\n    \n    # Function to predict single input record\n    def predict(self, record):\n        df_input = pd.DataFrame.from_dict([record])\n        prediction = 10 ** self.model.predict(df_input)\n        return prediction[0]\n    \n    post_parser = (reqparse\n        .RequestParser(bundle_errors=True)\n        .add_argument(\'age\', type = int)\n        .add_argument(\'bmi\', type = float)\n        .add_argument(\'children\', type = int)\n        .add_argument(\'smoker\', type = str)\n        .add_argument(\'gender\', type = str)\n        .add_argument(\'region\', type = str))\n        \n    def post(self):\n        args = self.post_parser.parse_args(strict = True)\n        print(args)\n        args[""prediction""] = self.predict(args)\n        return args\n        \n\napi.add_resource(InsuranceCalculator, \'/\')\n\nif __name__ == \'__main__\':\n    app.run(debug=True, port = 5001)\n    \n    \n    '"
Flask-ML/train.py,3,"b'#!/usr/bin/env python\n# coding: utf-8\n\n\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport matplotlib.pyplot as plt\nfrom sklearn import pipeline, preprocessing, compose, linear_model, impute, model_selection\n\n\n# Load data\nprint(""Loading the training observations"")\ndf = pd.read_csv(""https://raw.githubusercontent.com/abulbasar/data/master/insurance.csv"")\nprint(""Training data: \\n"", df.head())\n\ntarget = ""charges""\ny = np.log10(df[target])\nX = df.drop(columns=[target])\n\n\n# Categorical features\ncat_columns = [""gender"", ""smoker"", ""region""]\n\n# Numeric features\nnum_columns = [""age"", ""bmi"", ""children""]\n\n\n# Build pipelines for categorical data and numeric data\ncat_pipe = pipeline.Pipeline([\n    (\'imputer\', impute.SimpleImputer(strategy=\'constant\', fill_value=\'missing\')),\n    (\'onehot\', preprocessing.OneHotEncoder(handle_unknown=\'ignore\'))\n]) \n\nnum_pipe = pipeline.Pipeline([\n    (\'imputer\', impute.SimpleImputer(strategy=\'median\')),\n    (\'poly\', preprocessing.PolynomialFeatures(degree=2, include_bias=False)),\n    (\'scaler\', preprocessing.StandardScaler()),\n])\n\npreprocessing_pipe = compose.ColumnTransformer([\n    (""cat"", cat_pipe, cat_columns),\n    (""num"", num_pipe, num_columns)\n])\n\n\n# Build estimator pipeline\nestimator_pipe = pipeline.Pipeline([\n    (""preprocessing"", preprocessing_pipe),\n    (""est"", linear_model.ElasticNet(random_state=1))\n])\n\n\n# Parameter grid to tune hyper parameters\nparam_grid = {\n    ""est__alpha"": 0.0 + np.random.random(10) * 0.02,\n    ""est__l1_ratio"": np.linspace(0.0001, 1, 20),\n}\n\n# Grid Search estimator\ngsearch = model_selection.GridSearchCV(estimator_pipe, param_grid, cv = 5, verbose=1, n_jobs=8)\n\n# Fit grid search estimator\nprint(""Fitting the model"")\ngsearch.fit(X, y)\n\nprint(""Gridsearch best score: "", gsearch.best_score_, ""best params: "", gsearch.best_params_)\n\n# Sanity test the quality of model\ny_pred = gsearch.predict(X)\nplt.scatter(y, y_pred - y)\n\nprint(""Sample predictions: "", pd.DataFrame({""actual"": y, ""predict"": y_pred}).sample(10))\n\n\n# Save the tuned model\npath = ""/tmp/model.pickle""\nwith open(path, ""wb"") as f:\n    pickle.dump(gsearch, f)\n\nprint(""Saved the model: "" + path, ""Now start or restart the flask web application."")'"
RL/frozen-lake.py,0,"b'""""""\nSFFF       (S: starting point, safe)\nFHFH       (F: frozen surface, safe)\nFFFH       (H: hole, fall to your doom)\nHFFG       (G: goal, where the frisbee is located)\n\n""""""\n\nimport gym\nenv = gym.make(\'FrozenLake8x8-v0\')  # try for different environments\nprint(env.get_action_meanings())\nenv.reset()\nenv.render()\n\ninput(""Press enter to exit"")\nenv.close()\n'"
RL/frozenlake8x8_valueiteration.py,7,"b'""""""\nSolving FrozenLake8x8 environment using Value-Iteration\n""""""\nimport numpy as np\nimport gym\nfrom gym import wrappers\n\n\ndef run_episode(env, policy, gamma = 1.0, render = False):\n    """""" Evaluates policy by using it to run an episode and finding its\n    total reward.\n    args:\n    env: gym environment.\n    policy: the policy to be used.\n    gamma: discount factor.\n    render: boolean to turn rendering on/off.\n    returns:\n    total reward: real value of the total reward recieved by agent under policy.\n    """"""\n    obs = env.reset()\n    total_reward = 0\n    step_idx = 0\n    while True:\n        if render:\n            env.render()\n        obs, reward, done , _ = env.step(int(policy[obs]))\n        total_reward += (gamma ** step_idx * reward)\n        step_idx += 1\n        if done:\n            break\n    return total_reward\n\n\ndef evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n    """""" Evaluates a policy by running it n times.\n    returns:\n    average total reward\n    """"""\n    scores = [\n            run_episode(env, policy, gamma = gamma, render = False)\n            for _ in range(n)]\n    return np.mean(scores)\n\n\ndef extract_policy(v, gamma = 1.0):\n    """""" Extract the policy given a value-function """"""\n    policy = np.zeros(env.nS)\n    for s in range(env.nS):\n        q_sa = np.zeros(env.action_space.n)\n        for a in range(env.action_space.n):\n            for next_sr in env.P[s][a]:\n                # next_sr is a tuple of (probability, next state, reward, done)\n                p, s_, r, _ = next_sr\n                q_sa[a] += (p * (r + gamma * v[s_]))\n        policy[s] = np.argmax(q_sa)\n    return policy\n\n\ndef value_iteration(env, gamma = 1.0):\n    """""" Value-iteration algorithm """"""\n    v = np.zeros(env.nS)  # initialize value-function\n    max_iterations = 100000\n    eps = 1e-20\n    for i in range(max_iterations):\n        prev_v = np.copy(v)\n        for s in range(env.nS):\n            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n            v[s] = max(q_sa)\n\n        if np.sum(np.fabs(prev_v - v)) <= eps:\n            print (\'Value-iteration converged at iteration# %d.\' %(i+1))\n            break\n    return v\n\n\nif __name__ == \'__main__\':\n    env_name  = \'FrozenLake8x8-v0\'\n    gamma = 1.0\n    env = gym.make(env_name)\n    optimal_v = value_iteration(env, gamma);\n    policy = extract_policy(optimal_v, gamma)\n    policy_score = evaluate_policy(env, policy, gamma, n=1000)\n    print(\'Policy average score = \', policy_score)'"
RL/mountain-car.py,0,"b'import gym\nenv = gym.make(\'MountainCarContinuous-v0\') # try for different environments\nobservation = env.reset()\nfor t in range(100):\n    env.render()\n    print(observation)\n    action = env.action_space.sample()\n    observation, reward, done, info = env.step(action)\n    print(observation, reward, done, info)\n    if done:\n        print(""Finished after {} timesteps"".format(t+1))\n        break\nenv.close()\n'"
Text2Speech/app.py,0,"b'#!/usr/bin/env python\n# coding: utf-8\n\n\nfrom flask import Flask, render_template, request, redirect\nimport pandas as pd\nimport pickle\nfrom awsclient import polly_large_text\n\napp = Flask(__name__)\n\n\n@app.route(\'/\')\ndef home():\n    return render_template(\'input.html\', filename = None)\n    \n\n@app.route(\'/convert-text-to-speech\',methods = [""POST""])\ndef result():\n    request_data = dict(request.form)\n    content = request_data.get(""content"")\n    VoiceId = request_data.get(""VoiceId"")\n    print(""Content: "", content, ""VoiceId: "", VoiceId)\n    mp3_file_name = polly_large_text(content, VoiceId)\n    print(""Output: "", mp3_file_name)\n    return render_template(""input.html"", filename = mp3_file_name, content = content)    \n    \nif __name__ == \'__main__\':\n    app.run(debug=True, host=""0.0.0.0"")\n    \n    \n    '"
Text2Speech/awsclient.py,0,"b'#!/usr/bin/env python\n# coding: utf-8\n\n\nimport boto3\nimport uuid\nimport json\nimport shutil\nimport uuid\nimport os\n\n\npolly = boto3.client(""polly"")\ns3 = boto3.resource(""s3"")\n\nclass PollyService:\n    \n    polly = boto3.client(""polly"")\n    s3 = boto3.resource(""s3"")\n    \n    # Voices: Aditi,Raveena,Ivy,Joanna,Kendra,Kimberly,Salli,Joey,Justin,Matthew\n    # LanguageCode: \'en-IN\'|\'en-US\'\n    args = {\n        ""OutputFormat"": ""mp3"",\n        ""VoiceId"": ""Aditi"",\n        ""Text"": None,\n        ""Engine"": ""standard"",\n        ""LanguageCode"": \'en-IN\'\n\n    }\n    \n    def __init__(self, output_location:str, save_to_s3:bool, VoiceId:str = ""Aditi""):\n        self.save_to_s3 = save_to_s3\n        self.output_location = output_location\n        self.VoiceId = VoiceId\n        \n        if output_location is None:\n            raise Exception(""Specify output location"")\n\n        if save_to_s3:\n            print(""Initializing bucket"")\n            self.bucket = s3.Bucket(output_location)\n        else:\n            self.local_path = output_location\n    \n    def _save_to_local(self, filename, data):\n        with open(filename, ""wb"") as f:\n            f.write(data)\n            print(""Written the output to a file - "" + filename)\n    \n    def _save_to_s3(self, filename, data):\n        bucket.put_object(Key = ""audios/"" + filename, Body = data)\n    \n    def synthensize(self, text):   \n        message_id = str(uuid.uuid1())\n        self.args[""Text""] = text\n        self.args[""VoiceId""] = self.VoiceId\n\n        response = polly.synthesize_speech(**self.args)\n        data = response[""AudioStream""].read()\n        \n        if self.save_to_s3:\n            self.bucket.put_object(Key = ""audios/{0}.mp3"".format(message_id), Body = data)\n            self.bucket.put_object(Key = ""inputs/{0}.txt"".format(message_id), Body = text.encode(""utf-8""))\n            print(""Saved the output to "" + self.output_location)\n        else:\n            filename = self.output_location + ""/"" + message_id + "".mp3""\n            self._save_to_local(filename, data)\n            print(""Saved the output to "" + self.output_location)\n        return message_id\n\ndef split_into_chunks(content, limit = 3000):\n    import spacy\n    nlp = spacy.load(\'en_core_web_sm\')\n    text_sentences = nlp(content)\n    sentences = list(text_sentences.sents)\n    sentences = [sentence.text for sentence in sentences]\n    """"""\n    chunks = []\n    chunk = """"\n    for sentence in sentences:\n        if len(chunk) + len(sentence) > limit:\n            chunks.append(chunk)\n            chunk = sentence\n        else:\n            chunk += sentence\n    if len(chunk)>0:\n        chunks.append(chunk)\n    """"""\n    print(""Input content size (chars): "", len(content))\n    print(""Number of sentences: "", len(sentences))\n    #print(""Number of chunks: "", len(chunks))\n    return sentences\n\ndef concat_mp3(files, destination_file, del_source = False):\n    destination = open(destination_file, \'wb\')\n    for filename in files:\n        print(""Reading file: "", filename)\n        shutil.copyfileobj(open(filename, \'rb\'), destination)\n        if del_source:\n            os.remove(filename)\n    destination.close()\n\n\ndef polly_large_text(content, VoiceId):\n    if not os.path.exists(""static""):\n        os.makedirs(""static"")\n\n    polly_dir = ""/tmp""\n    polly_service = PollyService(polly_dir, False, VoiceId)\n    chunks = split_into_chunks(content)\n    mp3_file_ids = [polly_service.synthensize(chunk) for chunk in chunks]\n    mp3s = [f""{polly_dir}/{file_id}.mp3"" for file_id in mp3_file_ids]\n    destination_file = str(uuid.uuid1()) + \'.mp3\'\n    concat_mp3(mp3s, \'static/\' + destination_file, True)\n    with open(""static/"" + destination_file + "".txt"", ""w"") as f:\n        f.write(content)\n    return destination_file\n\n\n        \n        \n'"
