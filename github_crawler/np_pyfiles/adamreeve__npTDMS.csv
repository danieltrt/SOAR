file_path,api_count,code
setup.py,0,"b'import os\n\nfrom setuptools import setup\n\n\ndef read_version():\n    here = os.path.abspath(os.path.dirname(__file__))\n    version_path = os.path.sep.join((here, ""nptdms"", ""version.py""))\n    v_globals = {}\n    v_locals = {}\n    exec(open(version_path).read(), v_globals, v_locals)\n    return v_locals[\'__version__\']\n\n\nsetup(\n  name = \'npTDMS\',\n  version = read_version(),\n  description = (""Cross-platform, NumPy based module for reading ""\n    ""TDMS files produced by LabView.""),\n  author = \'Adam Reeve\',\n  author_email = \'adreeve@gmail.com\',\n  url = \'https://github.com/adamreeve/npTDMS\',\n  packages = [\'nptdms\', \'nptdms.export\', \'nptdms.test\'],\n  long_description=open(\'README.rst\').read(),\n  license = \'LGPL\',\n  classifiers = [\n    \'Development Status :: 4 - Beta\',\n    \'Operating System :: OS Independent\',\n    \'Programming Language :: Python\',\n    \'Programming Language :: Python :: 2\',\n    \'Programming Language :: Python :: 3\',\n    \'Programming Language :: Python :: 2.7\',\n    \'Programming Language :: Python :: 3.5\',\n    \'Programming Language :: Python :: 3.6\',\n    \'Programming Language :: Python :: 3.7\',\n    \'Programming Language :: Python :: 3.8\',\n    \'Topic :: Scientific/Engineering\',\n    \'License :: OSI Approved :: GNU Library or Lesser General Public License (LGPL)\',\n    \'Intended Audience :: Science/Research\',\n    \'Natural Language :: English\',\n  ],\n  install_requires = [\'numpy\'],\n  extras_require = {\n      \'test\': [\'pytest>=3.1.0\', \'hypothesis\', \'pytest-benchmark\', \'mock<4.0;python_version<""3.4""\'],\n      \'pandas\': [\'pandas\'],\n      \'hdf\': [\'h5py>=2.10.0\'],\n      \'thermocouple_scaling\': [\'thermocouples_reference\', \'scipy\'],\n  },\n  entry_points = """"""\n  [console_scripts]\n  tdmsinfo=nptdms.tdmsinfo:main\n  """"""\n)\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# npTDMS documentation build configuration file, created by\n# sphinx-quickstart on Mon Feb 27 17:36:23 2012.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys, os\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n# -- General configuration -----------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named \'sphinx.ext.*\') or your custom ones.\nextensions = [""sphinx.ext.autodoc""]\n\n# Autodoc options\nautoclass_content = ""doc""\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = []\n\n# The suffix of source filenames.\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'npTDMS\'\ncopyright = u\'2012, Adam Reeve\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\nv_globals = {}\nv_locals = {}\nexec(open(\'../nptdms/version.py\').read(), v_globals, v_locals)\n# The short X.Y version.\nversion = \'.\'.join(\'%d\' % d for d in v_locals[\'__version_info__\'][:2])\n# The full version, including alpha/beta/rc tags.\nrelease = v_locals[\'__version__\']\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\'_build\']\n\n# The reST default role (used for this markup: `text`) to use for all documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\nautodoc_member_order = \'bysource\'\n\n\n# -- Options for HTML output ---------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \'default\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'npTDMSdoc\'\n\n\n# -- Options for LaTeX output --------------------------------------------------\n\n# The paper size (\'letter\' or \'a4\').\n#latex_paper_size = \'letter\'\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#latex_font_size = \'10pt\'\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n  (\'index\', \'npTDMS.tex\', u\'npTDMS Documentation\',\n   u\'Adam Reeve\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Additional stuff for the LaTeX preamble.\n#latex_preamble = \'\'\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output --------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (\'index\', \'nptdms\', u\'npTDMS Documentation\',\n     [u\'Adam Reeve\'], 1)\n]\n\n\nautodoc_mock_imports = [\'numpy\', \'np\', \'numpy.polynomial.polynomial\']\n'"
nptdms/__init__.py,0,"b'""""""Module for reading binary TDMS files produced by LabView""""""\n\nfrom __future__ import absolute_import\n\n\n# Make version number available\nfrom .version import __version_info__, __version__\n\n# Export public objects\nfrom .tdms import TdmsFile, TdmsGroup, TdmsChannel, DataChunk, GroupDataChunk, ChannelDataChunk\nfrom .writer import TdmsWriter, RootObject, GroupObject, ChannelObject\n'"
nptdms/base_segment.py,5,"b'from copy import copy\nfrom io import UnsupportedOperation\nimport os\nimport struct\nimport numpy as np\n\nfrom nptdms import types\nfrom nptdms.common import toc_properties\nfrom nptdms.log import log_manager\n\n\nlog = log_manager.get_logger(__name__)\n_struct_unpack = struct.unpack\n\nRAW_DATA_INDEX_NO_DATA = 0xFFFFFFFF\nRAW_DATA_INDEX_MATCHES_PREVIOUS = 0x00000000\n\n\nclass BaseSegment(object):\n    """""" Abstract base class for TDMS segments\n    """"""\n\n    __slots__ = [\n        \'position\', \'num_chunks\', \'ordered_objects\', \'toc_mask\',\n        \'next_segment_offset\', \'next_segment_pos\',\n        \'raw_data_offset\', \'data_position\', \'final_chunk_proportion\',\n        \'endianness\', \'object_properties\']\n\n    def __init__(\n            self, position, toc_mask, endianness, next_segment_offset,\n            next_segment_pos, raw_data_offset, data_position):\n        self.position = position\n        self.toc_mask = toc_mask\n        self.endianness = endianness\n        self.next_segment_offset = next_segment_offset\n        self.next_segment_pos = next_segment_pos\n        self.raw_data_offset = raw_data_offset\n        self.data_position = data_position\n        self.num_chunks = 0\n        self.final_chunk_proportion = 1.0\n        self.ordered_objects = []\n        self.object_properties = None\n\n    def __repr__(self):\n        return ""<TdmsSegment at position %d>"" % self.position\n\n    def read_segment_objects(self, file, previous_segment_objects, previous_segment=None):\n        """"""Read segment metadata section and update object information\n\n        :param file: Open TDMS file\n        :param previous_segment_objects: Dictionary of path to the most\n            recently read segment object for a TDMS object.\n        :param previous_segment: Previous segment in the file.\n        """"""\n\n        if not self.toc_mask & toc_properties[\'kTocMetaData\']:\n            self._reuse_previous_segment_metadata(previous_segment)\n            return\n\n        endianness = self.endianness\n\n        new_obj_list = self.toc_mask & toc_properties[\'kTocNewObjList\']\n        if not new_obj_list:\n            # In this case, there can be a list of new objects that\n            # are appended, or previous objects can also be repeated\n            # if their properties change.\n            # Copy the list of objects for now, but any objects that have\n            # metadata changed will need to be copied before being modified.\n            self.ordered_objects = [\n                o for o in previous_segment.ordered_objects]\n            existing_objects = {o.path: (i, o) for (i, o) in enumerate(self.ordered_objects)}\n        else:\n            existing_objects = None\n\n        log.debug(""Reading segment object metadata at %d"", file.tell())\n\n        # First four bytes have number of objects in metadata\n        num_objects_bytes = file.read(4)\n        num_objects = _struct_unpack(endianness + \'L\', num_objects_bytes)[0]\n\n        for _ in range(num_objects):\n            # Read the object path\n            object_path = types.String.read(file, endianness)\n            raw_data_index_header_bytes = file.read(4)\n            raw_data_index_header = _struct_unpack(endianness + \'L\', raw_data_index_header_bytes)[0]\n            log.debug(""Reading metadata for object %s with index header 0x%08x"", object_path, raw_data_index_header)\n\n            # Check whether we already have this object in our list from\n            # the last segment\n            (existing_object_index, existing_object) = (\n                self._get_existing_object(existing_objects, object_path)\n                if existing_objects is not None\n                else (None, None))\n            if existing_object_index is not None:\n                self._update_existing_object(\n                    object_path, existing_object_index, existing_object, raw_data_index_header, file)\n            elif object_path in previous_segment_objects:\n                previous_segment_obj = previous_segment_objects[object_path]\n                self._reuse_previous_object(\n                    object_path, previous_segment_obj, raw_data_index_header, file)\n            else:\n                segment_obj = self._new_segment_object(object_path)\n                self.ordered_objects.append(segment_obj)\n                if raw_data_index_header == RAW_DATA_INDEX_MATCHES_PREVIOUS:\n                    raise ValueError(""Raw data index for %s says to reuse previous structure, ""\n                                     ""but we have not seen this object before"" % object_path)\n                elif raw_data_index_header != RAW_DATA_INDEX_NO_DATA:\n                    segment_obj.has_data = True\n                    segment_obj.read_raw_data_index(file, raw_data_index_header)\n\n            self._read_object_properties(file, object_path)\n        self._calculate_chunks()\n\n    def _update_existing_object(\n            self, object_path, existing_object_index, existing_object, raw_data_index_header, file):\n        """""" Update raw data index information for an object already in the list of segment objects\n        """"""\n        if raw_data_index_header == RAW_DATA_INDEX_NO_DATA:\n            # Re-use object but leave data index information as set previously\n            if existing_object.has_data:\n                new_obj = copy(existing_object)\n                new_obj.has_data = False\n                self.ordered_objects[existing_object_index] = new_obj\n        elif raw_data_index_header == RAW_DATA_INDEX_MATCHES_PREVIOUS:\n            # Re-use object and ensure we set has data to true for this segment\n            if not existing_object.has_data:\n                new_obj = copy(existing_object)\n                new_obj.has_data = True\n                self.ordered_objects[existing_object_index] = new_obj\n        else:\n            # New segment metadata, or updates to existing data\n            segment_obj = self._new_segment_object(object_path)\n            segment_obj.has_data = True\n            segment_obj.read_raw_data_index(file, raw_data_index_header)\n            self.ordered_objects[existing_object_index] = segment_obj\n\n    def _reuse_previous_object(\n            self, object_path, previous_segment_obj, raw_data_index_header, file):\n        """""" Attempt to reuse raw data index information from a previous segment\n        """"""\n        if raw_data_index_header == RAW_DATA_INDEX_NO_DATA:\n            # Re-use object but leave data index information as set previously\n            if previous_segment_obj.has_data:\n                segment_obj = copy(previous_segment_obj)\n                segment_obj.has_data = False\n            else:\n                segment_obj = previous_segment_obj\n        elif raw_data_index_header == RAW_DATA_INDEX_MATCHES_PREVIOUS:\n            # Re-use previous object and ensure we set has data to true for this segment\n            if not previous_segment_obj.has_data:\n                segment_obj = copy(previous_segment_obj)\n                segment_obj.has_data = True\n            else:\n                segment_obj = previous_segment_obj\n        else:\n            # Changed metadata in this segment\n            segment_obj = self._new_segment_object(object_path)\n            segment_obj.has_data = True\n            segment_obj.read_raw_data_index(file, raw_data_index_header)\n        self.ordered_objects.append(segment_obj)\n\n    def _reuse_previous_segment_metadata(self, previous_segment):\n        try:\n            self.ordered_objects = previous_segment.ordered_objects\n            self._calculate_chunks()\n        except AttributeError:\n            raise ValueError(\n                ""kTocMetaData is not set for segment but ""\n                ""there is no previous segment"")\n\n    def _get_existing_object(self, existing_objects, object_path):\n        """""" Find an object already in the list of objects that are reused from the previous segment\n        """"""\n        try:\n            return existing_objects[object_path]\n        except KeyError:\n            return None, None\n\n    def _read_object_properties(self, file, object_path):\n        """"""Read properties for an object in the segment\n        """"""\n        num_properties_bytes = file.read(4)\n        num_properties = _struct_unpack(self.endianness + \'L\', num_properties_bytes)[0]\n        if num_properties > 0:\n            log.debug(""Reading %d properties"", num_properties)\n            if self.object_properties is None:\n                self.object_properties = {}\n            self.object_properties[object_path] = [\n                read_property(file, self.endianness)\n                for _ in range(num_properties)]\n\n    def read_raw_data(self, f):\n        """"""Read raw data from a TDMS segment\n\n        :returns: A generator of RawDataChunk objects with raw channel data for\n            objects in this segment.\n        """"""\n\n        if not self.toc_mask & toc_properties[\'kTocRawData\']:\n            yield RawDataChunk.empty()\n\n        f.seek(self.data_position)\n\n        total_data_size = self.next_segment_offset - self.raw_data_offset\n        log.debug(\n            ""Reading %d bytes of data at %d in %d chunks"",\n            total_data_size, f.tell(), self.num_chunks)\n\n        data_objects = [o for o in self.ordered_objects if o.has_data]\n        for chunk in range(self.num_chunks):\n            yield self._read_data_chunk(f, data_objects, chunk)\n\n    def read_raw_data_for_channel(self, f, channel_path, chunk_offset=0, num_chunks=None):\n        """"""Read raw data from a TDMS segment\n\n        :param f: Open TDMS file object\n        :param channel_path: Path of channel to read data for\n        :param chunk_offset: Index of chunk to begin reading from\n        :param num_chunks: Number of chunks to read, or None to read to the end\n        :returns: A generator of RawChannelDataChunk objects with raw channel data for\n            a single channel in this segment.\n        """"""\n\n        if not self.toc_mask & toc_properties[\'kTocRawData\']:\n            yield RawChannelDataChunk.empty()\n\n        f.seek(self.data_position)\n\n        data_objects = [o for o in self.ordered_objects if o.has_data]\n        chunk_size = self._get_chunk_size()\n\n        if chunk_offset > 0:\n            f.seek(chunk_size * chunk_offset, os.SEEK_CUR)\n        stop_chunk = self.num_chunks if num_chunks is None else num_chunks + chunk_offset\n        for chunk_index in range(chunk_offset, stop_chunk):\n            yield self._read_channel_data_chunk(f, data_objects, chunk_index, channel_path)\n\n    def _calculate_chunks(self):\n        """"""\n        Work out the number of chunks the data is in, for cases\n        where the meta data doesn\'t change at all so there is no\n        lead in.\n        """"""\n\n        data_size = self._get_chunk_size()\n\n        total_data_size = self.next_segment_offset - self.raw_data_offset\n        if data_size < 0 or total_data_size < 0:\n            raise ValueError(""Negative data size"")\n        elif data_size == 0:\n            # Sometimes kTocRawData is set, but there isn\'t actually any data\n            if total_data_size != data_size:\n                raise ValueError(\n                    ""Zero channel data size but data length based on ""\n                    ""segment offset is %d."" % total_data_size)\n            self.num_chunks = 0\n            return\n        chunk_remainder = total_data_size % data_size\n        if chunk_remainder == 0:\n            self.num_chunks = int(total_data_size // data_size)\n        else:\n            log.warning(\n                ""Data size %d is not a multiple of the ""\n                ""chunk size %d. Will attempt to read last chunk"",\n                total_data_size, data_size)\n            self.num_chunks = 1 + int(total_data_size // data_size)\n            self.final_chunk_proportion = (\n                    float(chunk_remainder) / float(data_size))\n\n    def _get_chunk_size(self):\n        return sum([\n            o.data_size\n            for o in self.ordered_objects if o.has_data])\n\n    def _read_data_chunk(self, file, data_objects, chunk_index):\n        """""" Read data from a chunk for all channels\n        """"""\n        raise NotImplementedError(""Data chunk reading must be implemented in base classes"")\n\n    def _read_channel_data_chunk(self, file, data_objects, chunk_index, channel_path):\n        """""" Read data from a chunk for a single channel\n        """"""\n        # In the base case we can read data for all channels\n        # and then select only the requested channel.\n        # Derived classes can implement more optimised reading.\n        data_chunk = self._read_data_chunk(file, data_objects, chunk_index)\n        try:\n            return data_chunk.channel_data[channel_path]\n        except KeyError:\n            return RawChannelDataChunk.empty()\n\n    def _new_segment_object(self, object_path):\n        """""" Create a new segment object for a segment\n\n        :param object_path: Path for the object\n        """"""\n\n        raise NotImplementedError(""New segment object creation must be implemented in base classes"")\n\n\nclass BaseSegmentObject(object):\n    """""" Abstract base class for an object in a TDMS segment\n    """"""\n\n    __slots__ = [\n        \'path\', \'number_values\', \'data_size\',\n        \'has_data\', \'data_type\', \'endianness\']\n\n    def __init__(self, path, endianness):\n        self.path = path\n        self.number_values = 0\n        self.data_size = 0\n        self.has_data = False\n        self.data_type = None\n        self.endianness = endianness\n\n    def read_raw_data_index(self, file, raw_data_index_header):\n        """""" Read the raw data index for a single object in a segment\n        """"""\n        raise NotImplementedError(""Segment metadata reading must be implemented in base classes"")\n\n    @property\n    def scaler_data_types(self):\n        return None\n\n\nclass RawDataChunk(object):\n    """"""Data read from a single chunk in a TDMS segment\n\n    :ivar channel_data: A dictionary of channel data chunks.\n        Keys are object paths and values are RawChannelDataChunk instances.\n    """"""\n\n    def __init__(self, channel_data):\n        self.channel_data = channel_data\n\n    @staticmethod\n    def empty():\n        return RawDataChunk({})\n\n    @staticmethod\n    def channel_data(data):\n        channel_chunks = {\n            path: RawChannelDataChunk.channel_data(d)\n            for (path, d) in data.items()\n        }\n        return RawDataChunk(channel_chunks)\n\n    @staticmethod\n    def scaler_data(data):\n        channel_chunks = {\n            path: RawChannelDataChunk.scaler_data(d)\n            for (path, d) in data.items()\n        }\n        return RawDataChunk(channel_chunks)\n\n\nclass RawChannelDataChunk(object):\n    """"""Data read for a single channel from a single chunk in a TDMS segment\n\n    :ivar data: Raw data in this chunk for a standard TDMS channel.\n    :ivar scaler_data: A dictionary of scaler data in this segment for\n        DAQmx raw data. Keys are the scaler id and values are data arrays.\n    """"""\n\n    def __init__(self, data, scaler_data):\n        self.data = data\n        self.scaler_data = scaler_data\n\n    def __len__(self):\n        if self.data is not None:\n            return len(self.data)\n        elif self.scaler_data is not None:\n            return next(len(d) for d in self.scaler_data.values())\n        return 0\n\n    @staticmethod\n    def empty():\n        return RawChannelDataChunk(None, None)\n\n    @staticmethod\n    def channel_data(data):\n        return RawChannelDataChunk(data, None)\n\n    @staticmethod\n    def scaler_data(data):\n        return RawChannelDataChunk(None, data)\n\n\ndef read_property(f, endianness=""<""):\n    """""" Read a property from a segment\'s metadata """"""\n\n    prop_name = types.String.read(f, endianness)\n    prop_data_type = types.tds_data_types[types.Uint32.read(f, endianness)]\n    value = prop_data_type.read(f, endianness)\n    log.debug(""Property \'%s\' = %r"", prop_name, value)\n    return prop_name, value\n\n\ndef fromfile(file, dtype, count, *args, **kwargs):\n    """"""Wrapper around np.fromfile to support any file-like object""""""\n\n    try:\n        return np.fromfile(file, dtype=dtype, count=count, *args, **kwargs)\n    except (TypeError, IOError, UnsupportedOperation):\n        return np.frombuffer(\n            file.read(count * np.dtype(dtype).itemsize),\n            dtype=dtype, count=count, *args, **kwargs)\n\n\ndef read_interleaved_segment_bytes(f, bytes_per_row, num_values):\n    """""" Read a segment of interleaved data as rows of bytes\n    """"""\n    number_bytes = bytes_per_row * num_values\n    combined_data = fromfile(f, dtype=np.uint8, count=number_bytes)\n\n    try:\n        # Reshape, so that one row is all bytes for all objects\n        combined_data = combined_data.reshape(-1, bytes_per_row)\n    except ValueError:\n        # Probably incomplete segment at the end => try to clip data\n        crop_len = (combined_data.shape[0] // bytes_per_row)\n        crop_len *= bytes_per_row\n        log.warning(""Cropping data from %d to %d bytes to match segment ""\n                    ""size derived from channels"",\n                    combined_data.shape[0], crop_len)\n        combined_data = combined_data[:crop_len].reshape(-1, bytes_per_row)\n    return combined_data\n'"
nptdms/channel_data.py,7,"b'"""""" Responsible for storing data read from TDMS files\n""""""\n\nimport tempfile\nimport numpy as np\n\nfrom nptdms import types\nfrom nptdms.timestamp import TimestampArray\nfrom nptdms.log import log_manager\n\nlog = log_manager.get_logger(__name__)\n\n\ndef get_data_receiver(obj, num_values, raw_timestamps, memmap_dir=None):\n    """"""Return a new channel data receiver to use for the given TDMS object\n\n    :param obj: TDMS channel object to receive data for\n    :param num_values: Number of values to be stored\n    :param raw_timestamps: Whether to store timestamp data as raw TDMS timestamps or a numpy datetime64 array\n    :param memmap_dir: Optional directory to store memory map files,\n        or None to not use memory map files\n    """"""\n    if obj.data_type is None:\n        return None\n\n    if obj.data_type == types.DaqMxRawData:\n        return DaqmxDataReceiver(obj, num_values, memmap_dir)\n\n    if obj.data_type == types.TimeStamp:\n        return TimestampDataReceiver(obj, num_values, raw_timestamps, memmap_dir)\n\n    if obj.data_type.nptype is None:\n        return ListDataReceiver(obj)\n\n    return NumpyDataReceiver(obj, num_values, memmap_dir)\n\n\nclass ListDataReceiver(object):\n    """"""Simple list based data receiver for objects that don\'t have a\n        corresponding numpy data type\n\n       :ivar data: List of data points\n    """"""\n\n    def __init__(self, channel):\n        """"""Initialise new data receiver for a TDMS object\n        """"""\n        if channel.data_type == types.String:\n            self._dtype = np.dtype(\'O\')\n        else:\n            self._dtype = None\n        self._data = []\n        self.scaler_data = {}\n\n    def append_data(self, data):\n        """"""Append data from a segment\n        """"""\n        self._data.extend(data)\n\n    @property\n    def data(self):\n        return np.array(self._data, dtype=self._dtype)\n\n\nclass NumpyDataReceiver(object):\n    """"""Receives data for a TDMS object and stores it in a numpy array\n\n    :ivar data: Data that has been read for the object\n    """"""\n\n    def __init__(self, obj, num_values, memmap_dir=None):\n        """"""Initialise data receiver backed by a numpy array\n\n        :param obj: Object to store data for\n        :param num_values: Number of values to be stored\n        :param memmap_dir: Optional directory to store memory map files in.\n        """"""\n\n        self.path = obj.path\n        self.data = _new_numpy_array(\n            obj.data_type.nptype, num_values, memmap_dir)\n        self.scaler_data = {}\n        self._data_insert_position = 0\n        log.debug(""Allocated %d sample slots for %s"", len(self.data), obj.path)\n\n    def append_data(self, new_data):\n        """"""Update the object data with a new array of data""""""\n\n        log.debug(""Adding %d data points to data for %s"", len(new_data), self.path)\n        start_pos = self._data_insert_position\n        end_pos = self._data_insert_position + len(new_data)\n        self.data[start_pos:end_pos] = new_data\n        self._data_insert_position += len(new_data)\n\n\nclass DaqmxDataReceiver(object):\n    """"""Receives raw scaler data for a DAQmx object and stores it in numpy\n    arrays\n\n    :ivar scaler_data: Dictionary mapping from scaler id to data for a scaler\n    """"""\n\n    def __init__(self, obj, num_values, memmap_dir=None):\n        """"""Initialise data receiver for DAQmx backed by a numpy array\n\n        :param obj: Object to store data for\n        :param memmap_dir: Optional directory to store memory mmap files in.\n        """"""\n\n        self.path = obj.path\n        self.data = None\n        self.scaler_data = {}\n        self._scaler_insert_positions = {}\n        for scaler_id, scaler_type in obj.scaler_data_types.items():\n            self.scaler_data[scaler_id] = _new_numpy_array(\n                scaler_type.nptype, num_values, memmap_dir)\n            self._scaler_insert_positions[scaler_id] = 0\n\n    def append_scaler_data(self, scale_id, new_data):\n        """"""Append new DAQmx scaler data read from a segment\n        """"""\n\n        log.debug(""Adding %d data points for object %s, scaler %d"",\n                  len(new_data), self.path, scale_id)\n        data_array = self.scaler_data[scale_id]\n        start_pos = self._scaler_insert_positions[scale_id]\n        end_pos = start_pos + len(new_data)\n        data_array[start_pos:end_pos] = new_data\n        self._scaler_insert_positions[scale_id] += len(new_data)\n\n\nclass TimestampDataReceiver(object):\n    """"""Receives timestamp data for a TDMS object and stores it in a numpy array\n\n    :ivar data: Data that has been read for the object as either a TimestampArray\n        or datetime64 array depending on whether raw_timestamps is True or False\n    """"""\n\n    def __init__(self, obj, num_values, raw_timestamps=False, memmap_dir=None):\n        """"""Initialise timestamp data receiver backed by a numpy array\n\n        :param obj: Object to store data for\n        :param num_values: Number of values to be stored\n        :param raw_timestamps: Whether to store data as raw TDMS timestamps or a numpy datetime64 array\n        :param memmap_dir: Optional directory to store memory map files in.\n        """"""\n\n        self.path = obj.path\n        self._raw_timestamps = raw_timestamps\n        if raw_timestamps:\n            byte_array = _new_numpy_array(np.dtype(\'uint8\'), num_values * 16, memmap_dir)\n            dtype = np.dtype([(\'second_fractions\', \'uint64\'), (\'seconds\', \'int64\')])\n            self.data = TimestampArray(byte_array.view(dtype))\n        else:\n            self.data = _new_numpy_array(np.dtype(\'datetime64[us]\'), num_values, memmap_dir)\n        self.scaler_data = {}\n        self._data_insert_position = 0\n        log.debug(""Allocated %d sample slots for %s"", len(self.data), obj.path)\n\n    def append_data(self, new_data):\n        """"""Update the object data with a new array of data""""""\n\n        log.debug(""Adding %d data points to data for %s"", len(new_data), self.path)\n        start_pos = self._data_insert_position\n        end_pos = self._data_insert_position + len(new_data)\n        if self._raw_timestamps:\n            # Need to be careful about potential endianness mismatch, so order of fields can differ\n            self.data[\'seconds\'][start_pos:end_pos] = new_data[\'seconds\']\n            self.data[\'second_fractions\'][start_pos:end_pos] = new_data[\'second_fractions\']\n        else:\n            self.data[start_pos:end_pos] = new_data.as_datetime64()\n        self._data_insert_position += len(new_data)\n\n\ndef _new_numpy_array(dtype, num_values, memmap_dir=None):\n    """"""Initialise a new numpy array for data\n\n    :param dtype: Numpy data type for array\n    :param num_values: Capacity required\n    :param memmap_dir: Optional directory to store memory mmap files\n    """"""\n    if memmap_dir:\n        memmap_file = tempfile.NamedTemporaryFile(\n            mode=\'w+b\', prefix=""nptdms_"", dir=memmap_dir)\n        return np.memmap(\n            memmap_file.file,\n            mode=\'w+\',\n            shape=(num_values,),\n            dtype=dtype)\n\n    return np.zeros(num_values, dtype=dtype)\n'"
nptdms/common.py,0,"b'import itertools\ntry:\n    long\nexcept NameError:\n    # Python 3\n    long = int\ntry:\n    zip_longest = itertools.izip_longest\nexcept AttributeError:\n    # Python 3\n    zip_longest = itertools.zip_longest\n\n\ntoc_properties = {\n    \'kTocMetaData\': (long(1) << 1),\n    \'kTocRawData\': (long(1) << 3),\n    \'kTocDAQmxRawData\': (long(1) << 7),\n    \'kTocInterleavedData\': (long(1) << 5),\n    \'kTocBigEndian\': (long(1) << 6),\n    \'kTocNewObjList\': (long(1) << 2)\n}\n\n\nclass ObjectPath(object):\n    """""" Represents the path of an object in a TDMS file\n\n        :ivar group: Group name or None for the root object\n        :ivar channel: Channel name or None for the root object or a group objecct\n    """"""\n    def __init__(self, *path_components):\n        self.group = None\n        self.channel = None\n        if len(path_components) > 0:\n            self.group = path_components[0]\n        if len(path_components) > 1:\n            self.channel = path_components[1]\n        if len(path_components) > 2:\n            raise ValueError(""Object path may only have up to two components"")\n        self._path = _components_to_path(self.group, self.channel)\n\n    @property\n    def is_root(self):\n        return self.group is None\n\n    @property\n    def is_group(self):\n        return self.group is not None and self.channel is None\n\n    @property\n    def is_channel(self):\n        return self.channel is not None\n\n    def group_path(self):\n        """""" For channel paths, returns the path of the channel\'s group as a string\n        """"""\n        return _components_to_path(self.group, None)\n\n    @staticmethod\n    def from_string(path_string):\n        components = list(_path_components(path_string))\n        return ObjectPath(*components)\n\n    def __str__(self):\n        """""" String representation of the object path\n        """"""\n        return self._path\n\n\ndef _path_components(path):\n    """""" Generator that yields components within an object path\n    """"""\n    # Iterate over each character and the next character\n    chars = zip_longest(path, path[1:])\n    try:\n        # Iterate over components\n        while True:\n            char, next_char = next(chars)\n            if char != \'/\':\n                raise ValueError(""Invalid path, expected \\""/\\"""")\n            elif next_char is not None and next_char != ""\'"":\n                raise ValueError(""Invalid path, expected \\""\'\\"""")\n            else:\n                # Consume ""\'"" or raise StopIteration if at the end\n                next(chars)\n            component = []\n            # Iterate over characters in component name\n            while True:\n                char, next_char = next(chars)\n                if char == ""\'"" and next_char == ""\'"":\n                    component += ""\'""\n                    # Consume second ""\'""\n                    next(chars)\n                elif char == ""\'"":\n                    yield """".join(component)\n                    break\n                else:\n                    component += char\n    except StopIteration:\n        return\n\n\ndef _components_to_path(group, channel):\n    components = []\n    if group is not None:\n        components.append(group)\n    if channel is not None:\n        components.append(channel)\n    return (\'/\' + \'/\'.join(\n        [""\'"" + c.replace(""\'"", ""\'\'"") + ""\'"" for c in components]))\n'"
nptdms/daqmx.py,2,"b'from collections import defaultdict\nimport numpy as np\n\nfrom nptdms import types\nfrom nptdms.base_segment import (\n    BaseSegment, BaseSegmentObject, RawDataChunk, read_interleaved_segment_bytes)\nfrom nptdms.log import log_manager\n\n\nlog = log_manager.get_logger(__name__)\n\n\nFORMAT_CHANGING_SCALER = 0x00001269\nDIGITAL_LINE_SCALER = 0x0000126A\n\n\nclass DaqmxSegment(BaseSegment):\n    """""" A TDMS segment with DAQmx data\n    """"""\n\n    def _new_segment_object(self, object_path):\n        return DaqmxSegmentObject(object_path, self.endianness)\n\n    def _get_chunk_size(self):\n        # For DAQmxRawData, each channel in a segment has the same number\n        # of values and contains the same raw data widths, so use\n        # the first valid channel metadata to calculate the data size.\n        try:\n            return next(\n                o.number_values * o.total_raw_data_width\n                for o in self.ordered_objects\n                if o.has_data and\n                o.number_values * o.total_raw_data_width > 0)\n        except StopIteration:\n            return 0\n\n    def _read_data_chunk(self, file, data_objects, chunk_index):\n        """"""Read data from DAQmx data segment""""""\n\n        log.debug(""Reading DAQmx data segment"")\n\n        all_daqmx = all(\n            o.data_type == types.DaqMxRawData for o in data_objects)\n        if not all_daqmx:\n            raise Exception(""Cannot read a mix of DAQmx and ""\n                            ""non-DAQmx interleaved data"")\n\n        # If we have DAQmx data, we expect all objects to have matching\n        # raw data widths, so just use the first object:\n        raw_data_widths = data_objects[0].daqmx_metadata.raw_data_widths\n        chunk_size = data_objects[0].number_values\n        scaler_data = defaultdict(dict)\n\n        # Data for each set of raw data (corresponding to one card) is\n        # interleaved separately, so read one after another\n        for (raw_buffer_index, raw_data_width) in enumerate(raw_data_widths):\n            # Read all data into 1 byte unsigned ints first\n            combined_data = read_interleaved_segment_bytes(\n                file, raw_data_width, chunk_size)\n\n            # Now set arrays for each scaler of each channel where the scaler\n            # data comes from this set of raw data\n            for (i, obj) in enumerate(data_objects):\n                scalers_for_raw_buffer_index = [\n                    scaler for scaler in obj.daqmx_metadata.scalers\n                    if scaler.raw_buffer_index == raw_buffer_index]\n                for scaler in scalers_for_raw_buffer_index:\n                    offset = scaler.raw_byte_offset\n                    scaler_size = scaler.data_type.size\n                    byte_columns = tuple(\n                        range(offset, offset + scaler_size))\n                    # Select columns for this scaler, so that number of values\n                    # will be number of bytes per point * number of data\n                    # points. Then use ravel to flatten the results into a\n                    # vector.\n                    this_scaler_data = combined_data[:, byte_columns].ravel()\n                    # Now set correct data type, so that the array length\n                    # should be correct\n                    this_scaler_data.dtype = (\n                        scaler.data_type.nptype.newbyteorder(self.endianness))\n                    if obj.daqmx_metadata.scaler_type == DIGITAL_LINE_SCALER:\n                        this_scaler_data = np.bitwise_and(this_scaler_data, 1)\n                    scaler_data[obj.path][scaler.scale_id] = this_scaler_data\n\n        return RawDataChunk.scaler_data(scaler_data)\n\n\nclass DaqmxSegmentObject(BaseSegmentObject):\n    """""" A DAQmx TDMS segment object\n    """"""\n\n    __slots__ = [\'daqmx_metadata\']\n\n    def __init__(self, path, endianness):\n        super(DaqmxSegmentObject, self).__init__(path, endianness)\n        self.daqmx_metadata = None\n\n    def read_raw_data_index(self, f, raw_data_index_header):\n        if raw_data_index_header not in (FORMAT_CHANGING_SCALER, DIGITAL_LINE_SCALER):\n            raise ValueError(\n                ""Unexpected raw data index for DAQmx data: 0x%08X"" %\n                raw_data_index_header)\n        # This is a DAQmx raw data segment.\n        #    0x00001269 for segment containing Format Changing scaler.\n        #    0x0000126A for segment containing Digital Line scaler.\n        # Note that the NI docs on the TDMS format state that digital line scaler data\n        # has 0x00001369, which appears to be incorrect\n\n        # Read the data type\n        data_type_val = types.Uint32.read(f, self.endianness)\n        try:\n            self.data_type = types.tds_data_types[data_type_val]\n        except KeyError:\n            raise KeyError(""Unrecognised data type: %s"" % data_type_val)\n\n        daqmx_metadata = DaqMxMetadata(f, self.endianness, raw_data_index_header)\n        log.debug(""DAQmx metadata: %r"", daqmx_metadata)\n\n        self.data_type = daqmx_metadata.data_type\n        # DAQmx format has special chunking\n        self.data_size = daqmx_metadata.chunk_size * sum(daqmx_metadata.raw_data_widths)\n        self.number_values = daqmx_metadata.chunk_size\n        self.daqmx_metadata = daqmx_metadata\n\n    @property\n    def total_raw_data_width(self):\n        return sum(self.daqmx_metadata.raw_data_widths)\n\n    @property\n    def scaler_data_types(self):\n        if self.daqmx_metadata is None:\n            return None\n        return dict(\n            (s.scale_id, s.data_type)\n            for s in self.daqmx_metadata.scalers)\n\n\nclass DaqMxMetadata(object):\n    """""" Describes DAQmx data for a single channel\n    """"""\n\n    __slots__ = [\n        \'data_type\',\n        \'scaler_type\',\n        \'dimension\',\n        \'chunk_size\',\n        \'raw_data_widths\',\n        \'scalers\',\n        ]\n\n    def __init__(self, f, endianness, scaler_type):\n        """"""\n        Read the metadata for a DAQmx raw segment.  This is the raw\n        DAQmx-specific portion of the raw data index.\n        """"""\n        self.scaler_type = scaler_type\n        self.data_type = types.tds_data_types[0xFFFFFFFF]\n        self.dimension = types.Uint32.read(f, endianness)\n        # In TDMS format version 2.0, 1 is the only valid value for dimension\n        if self.dimension != 1:\n            raise ValueError(""Data dimension is not 1"")\n        self.chunk_size = types.Uint64.read(f, endianness)\n\n        # size of vector of format changing scalers\n        scaler_vector_length = types.Uint32.read(f, endianness)\n        self.scalers = [\n            DaqMxScaler(f, endianness, scaler_type)\n            for _ in range(scaler_vector_length)]\n\n        # Read raw data widths.\n        # This is an array of widths in bytes, which should be the same\n        # for all channels that have DAQmx data in a segment.\n        # There is one element per acquisition card, as data is interleaved\n        # separately for each card.\n        raw_data_widths_length = types.Uint32.read(f, endianness)\n        self.raw_data_widths = np.zeros(raw_data_widths_length, dtype=np.int32)\n        for width_idx in range(raw_data_widths_length):\n            self.raw_data_widths[width_idx] = types.Uint32.read(f, endianness)\n\n    def __repr__(self):\n        """""" Return string representation of DAQmx metadata\n        """"""\n        properties = (\n            ""%s=%s"" % (name, _get_attr_repr(self, name))\n            for name in self.__slots__)\n\n        properties_list = "", "".join(properties)\n        return ""%s(%s)"" % (self.__class__.__name__, properties_list)\n\n\nclass DaqMxScaler(object):\n    """""" Details of a DAQmx raw data scaler read from a TDMS file\n    """"""\n\n    __slots__ = [\n        \'scale_id\',\n        \'data_type\',\n        \'raw_buffer_index\',\n        \'raw_byte_offset\',\n        \'sample_format_bitmap\',\n        ]\n\n    def __init__(self, open_file, endianness, scaler_type):\n        data_type_code = types.Uint32.read(open_file, endianness)\n        self.data_type = DAQMX_TYPES[data_type_code]\n\n        # more info for format changing scaler\n        self.raw_buffer_index = types.Uint32.read(open_file, endianness)\n        self.raw_byte_offset = types.Uint32.read(open_file, endianness)\n        if scaler_type == DIGITAL_LINE_SCALER:\n            self.sample_format_bitmap = types.Uint8.read(open_file, endianness)\n        else:\n            self.sample_format_bitmap = types.Uint32.read(open_file, endianness)\n        self.scale_id = types.Uint32.read(open_file, endianness)\n\n    def __repr__(self):\n        properties = (\n            ""%s=%s"" % (name, _get_attr_repr(self, name))\n            for name in self.__slots__)\n\n        properties_list = "", "".join(properties)\n        return ""%s(%s)"" % (self.__class__.__name__, properties_list)\n\n\ndef _get_attr_repr(obj, attr_name):\n    val = getattr(obj, attr_name)\n    if isinstance(val, type):\n        return val.__name__\n    return repr(val)\n\n\n# Type codes for DAQmx scalers don\'t match the normal TDMS type codes:\nDAQMX_TYPES = {\n    0: types.Uint8,\n    1: types.Int8,\n    2: types.Uint16,\n    3: types.Int16,\n    4: types.Uint32,\n    5: types.Int32,\n}\n'"
nptdms/log.py,0,"b'import logging\n\n\nclass LogManager(object):\n    """""" Manages multiple loggers from different modules\n    """"""\n\n    def __init__(self):\n        self.log_level = logging.WARNING\n\n        self.console_handler = logging.StreamHandler()\n        self.console_handler.setLevel(self.log_level)\n\n        self.formatter = logging.Formatter(\n            \'[%(name)s %(levelname)s] %(message)s\')\n        self.console_handler.setFormatter(self.formatter)\n\n        self.loggers = {}\n\n    def get_logger(self, module_name):\n        """""" Return a logger for a module\n        """"""\n        log = logging.getLogger(module_name)\n        log.setLevel(self.log_level)\n        log.addHandler(self.console_handler)\n        self.loggers[module_name] = log\n        return log\n\n    def set_level(self, level):\n        """""" Set the log level for all loggers that have been created\n        """"""\n        self.log_level = level\n        self.console_handler.setLevel(level)\n        for log in self.loggers.values():\n            log.setLevel(level)\n\n\nlog_manager = LogManager()\n'"
nptdms/reader.py,6,"b'"""""" Lower level TDMS reader API that allows finer grained reading of data\n""""""\n\nimport logging\nimport os\nimport numpy as np\n\nfrom nptdms import types\nfrom nptdms.common import ObjectPath, toc_properties\nfrom nptdms.utils import Timer, OrderedDict\nfrom nptdms.base_segment import RawChannelDataChunk\nfrom nptdms.tdms_segment import ContiguousDataSegment, InterleavedDataSegment\nfrom nptdms.daqmx import DaqmxSegment\nfrom nptdms.log import log_manager\n\n\nlog = log_manager.get_logger(__name__)\n\n\nclass TdmsReader(object):\n    """""" Reads metadata and data from a TDMS file.\n\n    :ivar object_metadata: Dictionary of object path to ObjectMetadata\n    """"""\n\n    def __init__(self, tdms_file):\n        """""" Initialise a new TdmsReader\n\n        :param tdms_file: Either the path to the tdms file to read\n            as a string or pathlib.Path, or an already opened file.\n        """"""\n        self._segments = None\n        self._prev_segment_objects = {}\n        self.object_metadata = OrderedDict()\n        self._file_path = None\n        self._index_file_path = None\n\n        self._segment_channel_offsets = None\n        self._segment_chunk_sizes = None\n\n        if hasattr(tdms_file, ""read""):\n            # Is a file\n            self._file = tdms_file\n        else:\n            # Is path to a file\n            self._file_path = str(tdms_file)\n            self._file = open(self._file_path, \'rb\')\n            index_file_path = self._file_path + \'_index\'\n            if os.path.isfile(index_file_path):\n                self._index_file_path = index_file_path\n\n    def close(self):\n        if self._file is None:\n            # Already closed\n            return\n\n        if self._file_path is not None:\n            # File path was provided so we opened the file and\n            # should close it.\n            self._file.close()\n        # Otherwise always remove reference to the file\n        self._file = None\n\n    def read_metadata(self):\n        """""" Read all metadata and structure information from a TdmsFile\n        """"""\n        self._ensure_open()\n\n        if self._index_file_path is not None:\n            reading_index_file = True\n            file = open(self._index_file_path, \'rb\')\n        else:\n            reading_index_file = False\n            file = self._file\n\n        self._segments = []\n        segment_position = 0\n        try:\n            with Timer(log, ""Read metadata""):\n                # Read metadata first to work out how much space we need\n                previous_segment = None\n                while True:\n                    start_position = file.tell()\n                    try:\n                        segment = self._read_segment_metadata(\n                            file, segment_position, previous_segment, reading_index_file)\n                    except EOFError:\n                        # We\'ve finished reading the file\n                        break\n\n                    self._update_object_metadata(segment)\n                    self._update_object_properties(segment)\n                    self._segments.append(segment)\n                    previous_segment = segment\n\n                    segment_position = segment.next_segment_pos\n                    if reading_index_file:\n                        lead_size = 7 * 4\n                        file.seek(start_position + lead_size + segment.raw_data_offset, os.SEEK_SET)\n                    else:\n                        file.seek(segment.next_segment_pos, os.SEEK_SET)\n        finally:\n            if reading_index_file:\n                file.close()\n\n    def read_raw_data(self):\n        """""" Read raw data from all segments, chunk by chunk\n\n        :returns: A generator that yields RawDataChunk objects\n        """"""\n        self._ensure_open()\n        if self._segments is None:\n            raise RuntimeError(\n                ""Cannot read data unless metadata has first been read"")\n        for segment in self._segments:\n            self._verify_segment_start(segment)\n            for chunk in segment.read_raw_data(self._file):\n                yield chunk\n\n    def read_raw_data_for_channel(self, channel_path, offset=0, length=None):\n        """""" Read raw data for a single channel, chunk by chunk\n\n        :param channel_path: The path of the channel object to read data for\n        :param offset: Initial position to read data from.\n        :param length: Number of values to attempt to read.\n            If None, then all values starting from the offset will be read.\n            Fewer values will be returned if attempting to read beyond the end of the available data.\n        :returns: A generator that yields RawChannelDataChunk objects\n        """"""\n        self._ensure_open()\n        if self._segments is None:\n            raise RuntimeError(""Cannot read data unless metadata has first been read"")\n\n        if self._segment_channel_offsets is None:\n            with Timer(log, ""Build data index""):\n                self._build_index()\n        segment_offsets = self._segment_channel_offsets[channel_path]\n        chunk_sizes = self._segment_chunk_sizes[channel_path]\n\n        object_metadata = self.object_metadata[channel_path]\n        if length is None:\n            length = object_metadata.num_values - offset\n        end_index = offset + length\n\n        # Binary search to find first and last segments to read\n        start_segment = np.searchsorted(segment_offsets, offset, side=\'right\')\n        end_segment = np.searchsorted(segment_offsets, end_index, side=\'left\')\n\n        segment_index = start_segment\n        for segment in self._segments[start_segment:end_segment + 1]:\n            self._verify_segment_start(segment)\n            # By default, read all chunks in a segment\n            chunk_offset = 0\n            num_chunks = segment.num_chunks\n            chunk_size = chunk_sizes[segment_index]\n            segment_start_index = 0 if segment_index == 0 else segment_offsets[segment_index - 1]\n            remaining_values_to_skip = 0\n            remaining_values_to_trim = 0\n\n            # For the first and last segments, we may not need to read all chunks,\n            # and may need to trim some data from the beginning or end of the chunk.\n            if segment_index == start_segment:\n                num_values_to_skip = offset - segment_start_index\n                chunk_offset = num_values_to_skip // chunk_size\n                remaining_values_to_skip = num_values_to_skip % chunk_size\n                num_chunks -= chunk_offset\n            if segment_index == end_segment:\n                # Note: segment_index may be both start and end\n                segment_end_index = segment_offsets[segment_index]\n                num_values_to_trim = segment_end_index - end_index\n\n                # Account for segments where the final chunk is truncated\n                final_chunk_size = (segment_end_index - segment_start_index) % chunk_size\n                final_chunk_size = chunk_size if final_chunk_size == 0 else final_chunk_size\n                if num_values_to_trim >= final_chunk_size:\n                    num_chunks -= 1\n                    num_values_to_trim -= final_chunk_size\n\n                num_chunks -= num_values_to_trim // chunk_size\n                remaining_values_to_trim = num_values_to_trim % chunk_size\n\n            for i, chunk in enumerate(\n                    segment.read_raw_data_for_channel(self._file, channel_path, chunk_offset, num_chunks)):\n                skip = remaining_values_to_skip if i == 0 else 0\n                trim = remaining_values_to_trim if i == (num_chunks - 1) else 0\n                yield _trim_channel_chunk(chunk, skip, trim)\n\n            segment_index += 1\n\n    def read_channel_chunk_for_index(self, channel_path, index):\n        """""" Read the chunk containing the given index\n\n        :returns: Tuple of raw channel data chunk and the integer offset to the beginning of the chunk\n        :rtype: (RawChannelDataChunk, int)\n        """"""\n        self._ensure_open()\n        if self._segments is None:\n            raise RuntimeError(""Cannot read data unless metadata has first been read"")\n\n        if self._segment_channel_offsets is None:\n            with Timer(log, ""Build data index""):\n                self._build_index()\n        segment_offsets = self._segment_channel_offsets[channel_path]\n\n        # Binary search to find the segment to read\n        segment_index = np.searchsorted(segment_offsets, index, side=\'right\')\n        segment = self._segments[segment_index]\n        chunk_size = self._segment_chunk_sizes[channel_path][segment_index]\n        segment_start_index = segment_offsets[segment_index - 1] if segment_index > 0 else 0\n\n        index_in_segment = index - segment_start_index\n        chunk_index = index_in_segment // chunk_size\n\n        self._verify_segment_start(segment)\n        chunk_data = next(segment.read_raw_data_for_channel(self._file, channel_path, chunk_index, 1))\n        chunk_offset = segment_start_index + chunk_index * chunk_size\n        return chunk_data, chunk_offset\n\n    def _read_segment_metadata(\n            self, file, segment_position, previous_segment=None, is_index_file=False):\n        (position, toc_mask, endianness, data_position, raw_data_offset,\n         next_segment_offset, next_segment_pos) = self._read_lead_in(file, segment_position, is_index_file)\n\n        segment_args = (\n            position, toc_mask, endianness, next_segment_offset,\n            next_segment_pos, raw_data_offset, data_position)\n        if toc_mask & toc_properties[\'kTocDAQmxRawData\']:\n            segment = DaqmxSegment(*segment_args)\n        elif toc_mask & toc_properties[\'kTocInterleavedData\']:\n            segment = InterleavedDataSegment(*segment_args)\n        else:\n            segment = ContiguousDataSegment(*segment_args)\n\n        segment.read_segment_objects(\n            file, self._prev_segment_objects, previous_segment)\n        return segment\n\n    def _read_lead_in(self, file, segment_position, is_index_file=False):\n        expected_tag = b\'TDSh\' if is_index_file else b\'TDSm\'\n        tag = file.read(4)\n        if tag == b\'\':\n            raise EOFError\n        if tag != expected_tag:\n            raise ValueError(\n                ""Segment does not start with %r, but with %r"" % (expected_tag, tag))\n\n        log.debug(""Reading segment at %d"", segment_position)\n\n        # Next four bytes are table of contents mask\n        toc_mask = types.Int32.read(file)\n\n        if log.isEnabledFor(logging.DEBUG):\n            for prop_name, prop_mask in toc_properties.items():\n                prop_is_set = (toc_mask & prop_mask) != 0\n                log.debug(""Property %s is %s"", prop_name, prop_is_set)\n\n        endianness = \'>\' if (toc_mask & toc_properties[\'kTocBigEndian\']) else \'<\'\n\n        # Next four bytes are version number\n        version = types.Int32.read(file, endianness)\n        if version not in (4712, 4713):\n            log.warning(""Unrecognised version number."")\n\n        # Now 8 bytes each for the offset values\n        next_segment_offset = types.Uint64.read(file, endianness)\n        raw_data_offset = types.Uint64.read(file, endianness)\n\n        # Calculate data and next segment position\n        lead_size = 7 * 4\n        data_position = segment_position + lead_size + raw_data_offset\n        if next_segment_offset == 0xFFFFFFFFFFFFFFFF:\n            # Segment size is unknown. This can happen if LabVIEW crashes.\n            # Try to read until the end of the file.\n            log.warning(\n                ""Last segment of file has unknown size, ""\n                ""will attempt to read to the end of the file"")\n            next_segment_pos = self._get_data_file_size()\n            next_segment_offset = next_segment_pos - segment_position - lead_size\n        else:\n            log.debug(""Next segment offset = %d, raw data offset = %d"",\n                      next_segment_offset, raw_data_offset)\n            log.debug(""Data size = %d b"",\n                      next_segment_offset - raw_data_offset)\n            next_segment_pos = (\n                    segment_position + next_segment_offset + lead_size)\n\n        return (segment_position, toc_mask, endianness, data_position, raw_data_offset,\n                next_segment_offset, next_segment_pos)\n\n    def _verify_segment_start(self, segment):\n        """""" When reading data for a segment, check for the TDSm tag at the start of the segment in an attempt\n            to detect any mismatch between tdms and tdms_index files.\n        """"""\n        position = segment.position\n        self._file.seek(segment.position)\n        expected_tag = b\'TDSm\'\n        tag = self._file.read(4)\n        if tag != expected_tag:\n            raise ValueError(\n                ""Attempted to read data segment at position {0} but did not find segment start header. "".format(\n                    position) +\n                ""Check that the tdms_index file matches the tdms data file."")\n\n    def _get_data_file_size(self):\n        current_pos = self._file.tell()\n        self._file.seek(0, os.SEEK_END)\n        end_pos = self._file.tell()\n        self._file.seek(current_pos, os.SEEK_SET)\n        return end_pos\n\n    def _update_object_metadata(self, segment):\n        """""" Update object metadata using the metadata read from a single segment\n        """"""\n        for segment_object in segment.ordered_objects:\n            path = segment_object.path\n            self._prev_segment_objects[path] = segment_object\n\n            object_metadata = self._get_or_create_object(path)\n            object_metadata.num_values += _number_of_segment_values(segment_object, segment)\n            _update_object_data_type(path, object_metadata, segment_object)\n            _update_object_scaler_data_types(path, object_metadata, segment_object)\n\n    def _update_object_properties(self, segment):\n        """""" Update object properties using any properties in a segment\n        """"""\n        if segment.object_properties is not None:\n            for path, properties in segment.object_properties.items():\n                object_metadata = self._get_or_create_object(path)\n                for prop, val in properties:\n                    object_metadata.properties[prop] = val\n\n    def _get_or_create_object(self, path):\n        """""" Get existing object metadata or create metadata for a new object\n        """"""\n        try:\n            return self.object_metadata[path]\n        except KeyError:\n            obj = ObjectMetadata()\n            self.object_metadata[path] = obj\n            return obj\n\n    def _build_index(self):\n        """""" Builds an index into the segment data for faster lookup of values\n\n            _segment_channel_offsets provides data offset at the end of each segment per channel\n            _segment_chunk_sizes provides chunk sizes in each segment per channel\n        """"""\n        data_objects = [\n            path\n            for (path, obj) in self.object_metadata.items()\n            if ObjectPath.from_string(path).is_channel]\n        num_segments = len(self._segments)\n\n        segment_num_values = {\n            path: np.zeros(num_segments, dtype=np.int64) for path in data_objects}\n        segment_chunk_sizes = {\n            path: np.zeros(num_segments, dtype=np.int64) for path in data_objects}\n\n        for i, segment in enumerate(self._segments):\n            for obj in segment.ordered_objects:\n                if not obj.has_data:\n                    continue\n                segment_chunk_sizes[obj.path][i] = obj.number_values if obj.has_data else 0\n                segment_num_values[obj.path][i] = _number_of_segment_values(obj, segment)\n\n        self._segment_chunk_sizes = segment_chunk_sizes\n        self._segment_channel_offsets = {\n            path: np.cumsum(segment_count) for (path, segment_count) in segment_num_values.items()}\n\n    def _ensure_open(self):\n        if self._file is None:\n            raise RuntimeError(\n                ""Cannot read data after the underlying TDMS reader is closed"")\n\n\ndef _number_of_segment_values(segment_object, segment):\n    """""" Compute the number of values an object has in a segment\n    """"""\n    if not segment_object.has_data:\n        return 0\n    num_chunks = segment.num_chunks\n    final_chunk_proportion = segment.final_chunk_proportion\n    if final_chunk_proportion == 1.0:\n        return segment_object.number_values * num_chunks\n    else:\n        return (segment_object.number_values * (num_chunks - 1) +\n                int(segment_object.number_values * final_chunk_proportion))\n\n\ndef _update_object_data_type(path, obj, segment_object):\n    """""" Update the data type for an object using its segment metadata\n    """"""\n    if obj.data_type is not None and obj.data_type != segment_object.data_type:\n        raise ValueError(\n            ""Segment data doesn\'t have the same type as previous ""\n            ""segments for objects %s. Expected type %s but got %s"" %\n            (path, obj.data_type, segment_object.data_type))\n    obj.data_type = segment_object.data_type\n\n\ndef _update_object_scaler_data_types(path, obj, segment_object):\n    """""" Update the DAQmx scaler data types for an object using its segment metadata\n    """"""\n    if segment_object.scaler_data_types is not None:\n        if obj.scaler_data_types is not None and obj.scaler_data_types != segment_object.scaler_data_types:\n            raise ValueError(\n                ""Segment data doesn\'t have the same scaler data types as previous ""\n                ""segments for objects %s. Expected types %s but got %s"" %\n                (path, obj.scaler_data_types, segment_object.scaler_data_types))\n        obj.scaler_data_types = segment_object.scaler_data_types\n\n\nclass ObjectMetadata(object):\n    """""" Stores information about an object in a TDMS file\n    """"""\n    def __init__(self):\n        self.properties = OrderedDict()\n        self.data_type = None\n        self.scaler_data_types = None\n        self.num_values = 0\n\n\ndef _trim_channel_chunk(chunk, skip=0, trim=0):\n    if skip == 0 and trim == 0:\n        return chunk\n    data = None\n    scaler_data = None\n    if chunk.data is not None:\n        data = chunk.data[skip:len(chunk.data) - trim]\n    if chunk.scaler_data is not None:\n        scaler_data = {\n            scale_id: d[skip:len(d) - trim]\n            for (scale_id, d) in chunk.scaler_data.items()}\n    return RawChannelDataChunk(data, scaler_data)\n'"
nptdms/scaling.py,22,"b'import numpy as np\nimport numpy.polynomial.polynomial as poly\nimport re\n\nfrom nptdms.log import log_manager\n\n\nlog = log_manager.get_logger(__name__)\n\nRAW_DATA_INPUT_SOURCE = 0xFFFFFFFF\nVOLTAGE_EXCITATION = 10322\nCURRENT_EXCITATION = 10134\n\n\nclass LinearScaling(object):\n    """""" Linear scaling with slope and intercept\n    """"""\n    def __init__(self, intercept, slope, input_source):\n        self.intercept = intercept\n        self.slope = slope\n        self.input_source = input_source\n\n    @staticmethod\n    def from_properties(properties, scale_index):\n        try:\n            input_source = properties[\n                ""NI_Scale[%d]_Linear_Input_Source"" % scale_index]\n        except KeyError:\n            input_source = RAW_DATA_INPUT_SOURCE\n        return LinearScaling(\n            properties[""NI_Scale[%d]_Linear_Y_Intercept"" % scale_index],\n            properties[""NI_Scale[%d]_Linear_Slope"" % scale_index],\n            input_source)\n\n    def scale(self, data):\n        return data * self.slope + self.intercept\n\n\nclass PolynomialScaling(object):\n    """""" Polynomial scaling with an arbitrary number of coefficients\n    """"""\n    def __init__(self, coefficients, input_source):\n        self.coefficients = coefficients\n        self.input_source = input_source\n\n    @staticmethod\n    def from_properties(properties, scale_index):\n        try:\n            number_of_coefficients = properties[\n                \'NI_Scale[%d]_Polynomial_Coefficients_Size\' % scale_index]\n        except KeyError:\n            number_of_coefficients = 4\n        try:\n            input_source = properties[\n                ""NI_Scale[%d]_Polynomial_Input_Source"" % scale_index]\n        except KeyError:\n            input_source = RAW_DATA_INPUT_SOURCE\n        coefficients = [\n            properties[\n                \'NI_Scale[%d]_Polynomial_Coefficients[%d]\' % (scale_index, i)]\n            for i in range(number_of_coefficients)]\n        return PolynomialScaling(coefficients, input_source)\n\n    def scale(self, data):\n        if len(self.coefficients) == 0:\n            return np.zeros(len(data), dtype=np.dtype(\'float64\'))\n\n        # Ensure data is double type before scaling\n        data = data.astype(np.dtype(\'float64\'), copy=False)\n        return np.polynomial.polynomial.polyval(data, self.coefficients)\n\n\nclass RtdScaling(object):\n    """""" Converts a signal from a resistance temperature detector into\n        degrees celcius using the Callendar-Van Dusen equation\n    """"""\n    def __init__(\n            self, current_excitation, r0_nominal_resistance,\n            a, b, c,\n            lead_wire_resistance, resistance_configuration, input_source):\n        self.current_excitation = current_excitation\n        self.r0_nominal_resistance = r0_nominal_resistance\n        self.a = a\n        self.b = b\n        self.c = c\n        self.lead_wire_resistance = lead_wire_resistance\n        self.resistance_configuration = resistance_configuration\n        self.input_source = input_source\n\n    @staticmethod\n    def from_properties(properties, scale_index):\n        prefix = ""NI_Scale[%d]"" % scale_index\n        current_excitation = properties[\n            ""%s_RTD_Current_Excitation"" % prefix]\n        r0_nominal_resistance = properties[\n            ""%s_RTD_R0_Nominal_Resistance"" % prefix]\n        a = properties[""%s_RTD_A"" % prefix]\n        b = properties[""%s_RTD_B"" % prefix]\n        c = properties[""%s_RTD_C"" % prefix]\n        lead_wire_resistance = properties[\n            ""%s_RTD_Lead_Wire_Resistance"" % prefix]\n        resistance_configuration = properties[\n            ""%s_RTD_Resistance_Configuration"" % prefix]\n        input_source = properties[\n            ""%s_RTD_Input_Source"" % prefix]\n        return RtdScaling(\n            current_excitation, r0_nominal_resistance, a, b, c,\n            lead_wire_resistance, resistance_configuration, input_source)\n\n    def scale(self, data):\n        """""" Convert voltage data to temperature\n        """"""\n        (a, b, r_0) = (self.a, self.b, self.r0_nominal_resistance)\n\n        # R(T) = R(0)[1 + A*T + B*T^2 + (T - 100)*C*T^3]\n        # R(T) = V/I\n\n        r_t = data / self.current_excitation\n        r_t = _adjust_for_lead_resistance(\n            r_t, CURRENT_EXCITATION, self.resistance_configuration, self.lead_wire_resistance)\n\n        positive_temperature = r_t >= r_0\n        # First solve for positive temperatures using the quadratic form\n        temperature = (-a + np.sqrt(a ** 2 - 4.0 * b * (1.0 - r_t / r_0), where=positive_temperature)) / (2.0 * b)\n        if not np.all(positive_temperature):\n            # Use full quartic for any negative temperatures\n            for i in np.where(np.logical_not(positive_temperature))[0]:\n                temperature[i] = self._solve_quartic_form(r_t[i])\n        return temperature\n\n    def _solve_quartic_form(self, r_t):\n        (a, b, c, r_0) = (self.a, self.b, self.c, self.r0_nominal_resistance)\n        poly_coefficients = [r_0 - r_t, r_0 * a, r_0 * b, -100.0 * r_0 * c, r_0 * c]\n        roots = poly.polyroots(poly_coefficients)\n        return RtdScaling._get_negative_real_root(roots)\n\n    @staticmethod\n    def _get_negative_real_root(roots):\n        filtered = [r for r in roots if not np.iscomplex(r) and r.real < 0.0]\n        if len(filtered) != 1:\n            raise ValueError(""Expected single real valued negative root for RTD equation"")\n        return filtered[0].real\n\n\nclass TableScaling(object):\n    """""" Scales data using a map from input to output values with\n        linear interpolation for points in between inputs.\n    """"""\n    def __init__(\n            self, pre_scaled_values, scaled_values, input_source):\n\n        # This is a bit counterintuitive but the scaled values are the input\n        # values and the pre-scaled values are the output values for\n        # interpolation.\n\n        # Ensure values are monotonically increasing for interpolation to work\n        if not np.all(np.diff(scaled_values) > 0):\n            scaled_values = np.flip(scaled_values)\n            pre_scaled_values = np.flip(pre_scaled_values)\n        if not np.all(np.diff(scaled_values) > 0):\n            # Reversing didn\'t help\n            raise ValueError(\n                ""Table scaled values must be monotonically ""\n                ""increasing or decreasing"")\n\n        self.input_values = scaled_values\n        self.output_values = pre_scaled_values\n        self.input_source = input_source\n\n    @staticmethod\n    def from_properties(properties, scale_index):\n        prefix = ""NI_Scale[%d]_Table_"" % scale_index\n        try:\n            input_source = properties[prefix + ""Input_Source""]\n        except KeyError:\n            input_source = RAW_DATA_INPUT_SOURCE\n        num_pre_scaled_values = properties[\n            prefix + ""Pre_Scaled_Values_Size""]\n        num_scaled_values = properties[\n            prefix + ""Scaled_Values_Size""]\n        if num_pre_scaled_values != num_scaled_values:\n            raise ValueError(\n                ""Number of pre-scaled values does not match ""\n                ""number of scaled values"")\n        pre_scaled_values = np.array([\n            properties[prefix + ""Pre_Scaled_Values[%d]"" % i]\n            for i in range(num_pre_scaled_values)])\n        scaled_values = np.array([\n            properties[prefix + ""Scaled_Values[%d]"" % i]\n            for i in range(num_scaled_values)])\n        return TableScaling(pre_scaled_values, scaled_values, input_source)\n\n    def scale(self, data):\n        """""" Calculate scaled data\n        """"""\n        return np.interp(data, self.input_values, self.output_values)\n\n\nclass ThermistorScaling(object):\n    """""" Converts a voltage measurement from a Thermistor into temperature in Kelvin\n    """"""\n    def __init__(\n            self,\n            excitation_type,\n            excitation_value,\n            resistance_configuration,\n            r1_reference_resistance,\n            lead_wire_resistance,\n            a, b, c,\n            temperature_offset,\n            input_source):\n        self.excitation_type = excitation_type\n        self.excitation_value = excitation_value\n        self.resistance_configuration = resistance_configuration\n        self.r1_reference_resistance = r1_reference_resistance\n        self.lead_wire_resistance = lead_wire_resistance\n        self.a = a\n        self.b = b\n        self.c = c\n        self.temperature_offset = temperature_offset\n        self.input_source = input_source\n\n    @staticmethod\n    def from_properties(properties, scale_index):\n        prefix = ""NI_Scale[%d]_Thermistor"" % scale_index\n        excitation_type = properties[""%s_Excitation_Type"" % prefix]\n        excitation_value = properties[""%s_Excitation_Value"" % prefix]\n        resistance_configuration = properties[""%s_Resistance_Configuration"" % prefix]\n        r1_reference_resistance = properties[""%s_R1_Reference_Resistance"" % prefix]\n        lead_wire_resistance = properties[""%s_Lead_Wire_Resistance"" % prefix]\n        a = properties[""%s_A"" % prefix]\n        b = properties[""%s_B"" % prefix]\n        c = properties[""%s_C"" % prefix]\n        temperature_offset = properties[""%s_Temperature_Offset"" % prefix]\n        input_source = properties[""%s_Input_Source"" % prefix]\n        return ThermistorScaling(\n            excitation_type, excitation_value,\n            resistance_configuration, r1_reference_resistance, lead_wire_resistance,\n            a, b, c, temperature_offset, input_source)\n\n    def scale(self, data):\n        """""" Convert voltage data to temperature in Kelvin\n        """"""\n        # Ensure data is double precision\n        data = data.astype(np.dtype(\'float64\'), copy=False)\n        if self.excitation_type == CURRENT_EXCITATION:\n            r_t = data / self.excitation_value\n        elif self.excitation_type == VOLTAGE_EXCITATION:\n            # Calculate resistance based on voltage divider circuit\n            # R_t = R1 / ((V_excitation / V_out) - 1)\n            r_t = self.r1_reference_resistance * np.reciprocal(self.excitation_value * np.reciprocal(data) - 1.0)\n        else:\n            raise ValueError(""Invalid excitation type: %s"" % self.excitation_type)\n\n        r_t = _adjust_for_lead_resistance(\n            r_t, self.excitation_type, self.resistance_configuration, self.lead_wire_resistance)\n\n        coefficients = [self.a, self.b, 0.0, self.c]\n        return np.reciprocal(\n            np.polynomial.polynomial.polyval(np.log(r_t), coefficients)) - self.temperature_offset\n\n\nclass ThermocoupleScaling(object):\n    """""" Convert between voltages in uV and degrees celcius for a Thermocouple.\n        Can convert in either direction depending on the scaling direction\n        parameter.\n    """"""\n    def __init__(self, type_code, scaling_direction, input_source):\n        from thermocouples_reference import thermocouples\n\n        # Thermocouple types from\n        # http://zone.ni.com/reference/en-XX/help/371361R-01/glang/tdms_create_scalinginfo/#instance2\n        thermocouple_type = {\n            10047: \'B\',\n            10055: \'E\',\n            10072: \'J\',\n            10073: \'K\',\n            10077: \'N\',\n            10082: \'R\',\n            10085: \'S\',\n            10086: \'T\',\n        }[type_code]\n        self.thermocouple = thermocouples[thermocouple_type]\n\n        self.scaling_direction = scaling_direction\n        self.input_source = input_source\n\n    @staticmethod\n    def from_properties(properties, scale_index):\n        prefix = ""NI_Scale[%d]_Thermocouple"" % scale_index\n        input_source = properties.get(\n            ""%s_Input_Source"" % prefix, RAW_DATA_INPUT_SOURCE)\n        type_code = properties.get(\n            ""%s_Thermocouple_Type"" % prefix, 10072)\n        scaling_direction = properties.get(\n            ""%s_Scaling_Direction"" % prefix, 0)\n        return ThermocoupleScaling(type_code, scaling_direction, input_source)\n\n    def scale(self, data):\n        """""" Apply thermocouple scaling\n        """"""\n\n        # Note that the thermocouples_reference package uses mV for voltages,\n        # but TDMS uses uV.\n        nan = float(\'nan\')\n\n        def scale_uv_to_c(micro_volts):\n            """"""Convert micro volts to degrees celcius""""""\n            milli_volts = micro_volts / 1000.0\n            try:\n                return self.thermocouple.inverse_CmV(milli_volts, Tref=0.0)\n            except ValueError:\n                return nan\n\n        def scale_c_to_uv(temp):\n            """"""Convert degrees celcius to micro volts""""""\n            try:\n                return 1000.0 * self.thermocouple.emf_mVC(temp, Tref=0.0)\n            except ValueError:\n                return nan\n\n        if self.scaling_direction == 1:\n            scaled = np.vectorize(scale_c_to_uv)(data)\n        else:\n            scaled = np.vectorize(scale_uv_to_c)(data)\n        return scaled\n\n\nclass AddScaling(object):\n    """""" Adds two scalings\n    """"""\n    def __init__(self, left_input_source, right_input_source):\n        self.left_input_source = left_input_source\n        self.right_input_source = right_input_source\n\n    @staticmethod\n    def from_properties(properties, scale_index):\n        left_input_source = properties[\n            ""NI_Scale[%d]_Add_Left_Operand_Input_Source"" % scale_index]\n        right_input_source = properties[\n            ""NI_Scale[%d]_Add_Right_Operand_Input_Source"" % scale_index]\n        return AddScaling(left_input_source, right_input_source)\n\n    def scale(self, left_data, right_data):\n        return left_data + right_data\n\n\nclass SubtractScaling(object):\n    """""" Subtracts one scaling from another\n    """"""\n    def __init__(self, left_input_source, right_input_source):\n        self.left_input_source = left_input_source\n        self.right_input_source = right_input_source\n\n    @staticmethod\n    def from_properties(properties, scale_index):\n        left_input_source = properties[\n            ""NI_Scale[%d]_Subtract_Left_Operand_Input_Source"" % scale_index]\n        right_input_source = properties[\n            ""NI_Scale[%d]_Subtract_Right_Operand_Input_Source"" % scale_index]\n        return SubtractScaling(left_input_source, right_input_source)\n\n    def scale(self, left_data, right_data):\n        """""" Calculate scaled data\n        """"""\n\n        # Subtracting the left operand from the right doesn\'t make much sense,\n        # but this does match the Excel TDMS plugin behaviour.\n        return right_data - left_data\n\n\nclass DaqMxScalerScaling(object):\n    """""" Reads scaler from DAQmx data\n    """"""\n    def __init__(self, scale_id):\n        self.scale_id = scale_id\n\n    def scale_daqmx(self, scaler_data):\n        return scaler_data[self.scale_id]\n\n\nclass MultiScaling(object):\n    """""" Computes scaled data from multiple scalings\n    """"""\n    def __init__(self, scalings):\n        self.scalings = scalings\n\n    def scale(self, raw_channel_data):\n        final_scale = len(self.scalings) - 1\n        return self._compute_scaled_data(final_scale, raw_channel_data)\n\n    def get_dtype(self, raw_data_type, scaler_data_types):\n        """""" Get the numpy dtype for scaled data\n        """"""\n        final_scale = len(self.scalings) - 1\n        return self._compute_scale_dtype(final_scale, raw_data_type, scaler_data_types)\n\n    def _compute_scale_dtype(self, scale_index, raw_data_type, scaler_data_types):\n        if scale_index == RAW_DATA_INPUT_SOURCE:\n            return raw_data_type.nptype\n        scaling = self.scalings[scale_index]\n        if isinstance(scaling, DaqMxScalerScaling):\n            return scaler_data_types[scaling.scale_id].nptype\n        elif isinstance(scaling, AddScaling) or isinstance(scaling, SubtractScaling):\n            return np.result_type(\n                self._compute_scale_dtype(scaling.left_input_source, raw_data_type, scaler_data_types),\n                self._compute_scale_dtype(scaling.right_input_source, raw_data_type, scaler_data_types))\n        else:\n            # Any other scaling type should produce double data\n            return np.dtype(\'float64\')\n\n    def _compute_scaled_data(self, scale_index, raw_channel_data):\n        """""" Compute output data from a single scale in the set of all scalings,\n            computing any required input scales recursively.\n        """"""\n        if scale_index == RAW_DATA_INPUT_SOURCE:\n            if raw_channel_data.data is None:\n                raise Exception(""Invalid scaling input source for DAQmx data"")\n            return raw_channel_data.data\n\n        scaling = self.scalings[scale_index]\n        if scaling is None:\n            raise Exception(\n                ""Cannot compute data for scale %d"" % scale_index)\n\n        if isinstance(scaling, DaqMxScalerScaling):\n            return scaling.scale_daqmx(raw_channel_data.scaler_data)\n        elif hasattr(scaling, \'input_source\'):\n            input_data = self._compute_scaled_data(\n                scaling.input_source, raw_channel_data)\n            return scaling.scale(input_data)\n        elif (hasattr(scaling, \'left_input_source\') and\n              hasattr(scaling, \'right_input_source\')):\n            left_input_data = self._compute_scaled_data(\n                scaling.left_input_source, raw_channel_data)\n            right_input_data = self._compute_scaled_data(\n                scaling.right_input_source, raw_channel_data)\n            return scaling.scale(left_input_data, right_input_data)\n        else:\n            raise ValueError(""Cannot compute scaled data for %r"" % scaling)\n\n\ndef get_scaling(channel_properties, group_properties, file_properties):\n    """""" Get scaling for a channel from either the channel itself,\n        its group, or the whole TDMS file\n    """"""\n    scalings = (\n        _get_channel_scaling(p)\n        for p in [channel_properties, group_properties, file_properties])\n    try:\n        return next(s for s in scalings if s is not None)\n    except StopIteration:\n        return None\n\n\ndef _get_channel_scaling(properties):\n    num_scalings = _get_number_of_scalings(properties)\n    if num_scalings is None or num_scalings == 0:\n        return None\n    scaling_status = properties.get(""NI_Scaling_Status"", ""unscaled"")\n    if scaling_status == ""scaled"":\n        # Data is written with scaling already applied\n        return None\n\n    scalings = [None] * num_scalings\n    for scale_index in range(num_scalings):\n        type_property = \'NI_Scale[%d]_Scale_Type\' % scale_index\n        try:\n            scale_type = properties[type_property]\n        except KeyError:\n            # Scalings are not in properties if they come from DAQmx scalers\n            scalings[scale_index] = DaqMxScalerScaling(scale_index)\n            continue\n        if scale_type == \'Polynomial\':\n            scalings[scale_index] = PolynomialScaling.from_properties(\n                properties, scale_index)\n        elif scale_type == \'Linear\':\n            scalings[scale_index] = LinearScaling.from_properties(\n                properties, scale_index)\n        elif scale_type == \'RTD\':\n            scalings[scale_index] = RtdScaling.from_properties(\n                properties, scale_index)\n        elif scale_type == \'Table\':\n            scalings[scale_index] = TableScaling.from_properties(\n                properties, scale_index)\n        elif scale_type == \'Thermistor\':\n            scalings[scale_index] = ThermistorScaling.from_properties(\n                properties, scale_index)\n        elif scale_type == \'Thermocouple\':\n            scalings[scale_index] = ThermocoupleScaling.from_properties(\n                properties, scale_index)\n        elif scale_type == \'Add\':\n            scalings[scale_index] = AddScaling.from_properties(\n                properties, scale_index)\n        elif scale_type == \'Subtract\':\n            scalings[scale_index] = SubtractScaling.from_properties(\n                properties, scale_index)\n        else:\n            log.warning(""Unsupported scale type: %s"", scale_type)\n            return None\n\n    if not scalings:\n        return None\n    return MultiScaling(scalings)\n\n\n_scale_regex = re.compile(r""NI_Scale\\[(\\d+)\\]_Scale_Type"")\n\n\ndef _get_number_of_scalings(properties):\n    num_scalings_property = ""NI_Number_Of_Scales""\n    if num_scalings_property in properties:\n        return int(properties[num_scalings_property])\n\n    matches = (_scale_regex.match(key) for key in properties.keys())\n    try:\n        return max(int(m.group(1)) for m in matches if m is not None) + 1\n    except ValueError:\n        return None\n\n\ndef _adjust_for_lead_resistance(\n        measured_resistance, excitation_type, resistance_configuration, lead_wire_resistance):\n    """""""" Adjust a measured resistance to account for lead wire resistance\n    """"""\n    if resistance_configuration == 3:\n        return measured_resistance - lead_wire_resistance\n    if excitation_type == CURRENT_EXCITATION and resistance_configuration == 2:\n        return measured_resistance - 2.0 * lead_wire_resistance\n    return measured_resistance\n'"
nptdms/tdms.py,12,"b'"""""" Python module for reading TDMS files produced by LabView\n\n    This module contains the public facing API for reading TDMS files\n""""""\n\nfrom collections import defaultdict\nimport warnings\nimport numpy as np\n\nfrom nptdms import scaling, types\nfrom nptdms.utils import Timer, OrderedDict, cached_property\nfrom nptdms.log import log_manager\nfrom nptdms.common import ObjectPath\nfrom nptdms.reader import TdmsReader\nfrom nptdms.channel_data import get_data_receiver\nfrom nptdms.export import hdf_export, pandas_export\nfrom nptdms.base_segment import RawChannelDataChunk\nfrom nptdms.timestamp import TdmsTimestamp, TimestampArray\n\n\nlog = log_manager.get_logger(__name__)\n\n\n# Have to get a reference to the builtin property decorator\n# so we can use it in TdmsObject, which has a property method.\n_property_builtin = property\n\n\nclass TdmsFile(object):\n    """""" Reads and stores data from a TDMS file.\n\n    There are two main ways to create a new TdmsFile object.\n    TdmsFile.read will read all data into memory::\n\n        tdms_file = TdmsFile.read(tdms_file_path)\n\n    or you can use TdmsFile.open to read file metadata but not immediately read all data,\n    for cases where a file is too large to easily fit in memory or you don\'t need to\n    read data for all channels::\n\n        with TdmsFile.open(tdms_file_path) as tdms_file:\n            # Use tdms_file\n            ...\n\n    This class acts like a dictionary, where the keys are names of groups in the TDMS\n    files and the values are TdmsGroup objects.\n    A TdmsFile can be indexed by group name to access a group within the TDMS file, for example::\n\n        tdms_file = TdmsFile.read(tdms_file_path)\n        group = tdms_file[group_name]\n\n    Iterating over a TdmsFile produces the names of groups in this file,\n    or you can use the groups method to directly access all groups::\n\n        for group in tdms_file.groups():\n            # Use group\n            ...\n    """"""\n\n    @staticmethod\n    def read(file, raw_timestamps=False, memmap_dir=None):\n        """""" Creates a new TdmsFile object and reads all data in the file\n\n        :param file: Either the path to the tdms file to read\n            as a string or pathlib.Path, or an already opened file.\n        :param raw_timestamps: By default TDMS timestamps are read as numpy datetime64\n            but this loses some precision.\n            Setting this to true will read timestamps as a custom TdmsTimestamp type.\n        :param memmap_dir: The directory to store memory mapped data files in,\n            or None to read data into memory. The data files are created\n            as temporary files and are deleted when the channel data is no\n            longer used. tempfile.gettempdir() can be used to get the default\n            temporary file directory.\n        """"""\n        return TdmsFile(file, raw_timestamps=raw_timestamps, memmap_dir=memmap_dir)\n\n    @staticmethod\n    def open(file, raw_timestamps=False, memmap_dir=None):\n        """""" Creates a new TdmsFile object and reads metadata, leaving the file open\n            to allow reading channel data\n\n        :param file: Either the path to the tdms file to read\n            as a string or pathlib.Path, or an already opened file.\n        :param raw_timestamps: By default TDMS timestamps are read as numpy datetime64\n            but this loses some precision.\n            Setting this to true will read timestamps as a custom TdmsTimestamp type.\n        :param memmap_dir: The directory to store memory mapped data files in,\n            or None to read data into memory. The data files are created\n            as temporary files and are deleted when the channel data is no\n            longer used. tempfile.gettempdir() can be used to get the default\n            temporary file directory.\n        """"""\n        return TdmsFile(\n            file, raw_timestamps=raw_timestamps, memmap_dir=memmap_dir, read_metadata_only=True, keep_open=True)\n\n    @staticmethod\n    def read_metadata(file, raw_timestamps=False):\n        """""" Creates a new TdmsFile object and only reads the metadata\n\n        :param file: Either the path to the tdms file to read\n            as a string or pathlib.Path, or an already opened file.\n        :param raw_timestamps: By default TDMS timestamps are read as numpy datetime64\n            but this loses some precision.\n            Setting this to true will read timestamps as a custom TdmsTimestamp type.\n        """"""\n        return TdmsFile(file, raw_timestamps=raw_timestamps, read_metadata_only=True)\n\n    def __init__(self, file, raw_timestamps=False, memmap_dir=None, read_metadata_only=False, keep_open=False):\n        """"""Initialise a new TdmsFile object\n\n        :param file: Either the path to the tdms file to read\n            as a string or pathlib.Path, or an already opened file.\n        :param raw_timestamps: By default TDMS timestamps are read as numpy datetime64\n            but this loses some precision.\n            Setting this to true will read timestamps as a custom TdmsTimestamp type.\n        :param memmap_dir: The directory to store memory mapped data files in,\n            or None to read data into memory. The data files are created\n            as temporary files and are deleted when the channel data is no\n            longer used. tempfile.gettempdir() can be used to get the default\n            temporary file directory.\n        :param read_metadata_only: If this parameter is enabled then only the\n            metadata of the TDMS file will read.\n        :param keep_open: Keeps the file open so data can be read if only metadata\n            is read initially.\n        """"""\n\n        self._memmap_dir = memmap_dir\n        self._raw_timestamps = raw_timestamps\n        self._groups = OrderedDict()\n        self._properties = OrderedDict()\n        self._channel_data = {}\n        self.data_read = False\n\n        self._reader = TdmsReader(file)\n        try:\n            self._read_file(self._reader, read_metadata_only)\n        finally:\n            if not keep_open:\n                self._reader.close()\n\n    def groups(self):\n        """"""Returns a list of the groups in this file\n\n        :rtype: List of TdmsGroup.\n        """"""\n\n        return list(self._groups.values())\n\n    @_property_builtin\n    def properties(self):\n        """""" Return the properties of this file as a dictionary\n\n        These are the properties associated with the root TDMS object.\n        """"""\n\n        return self._properties\n\n    def as_dataframe(self, time_index=False, absolute_time=False, scaled_data=True):\n        """"""\n        Converts the TDMS file to a DataFrame. DataFrame columns are named using the TDMS object paths.\n\n        :param time_index: Whether to include a time index for the dataframe.\n        :param absolute_time: If time_index is true, whether the time index\n            values are absolute times or relative to the start time.\n        :param scaled_data: By default the scaled data will be used.\n            Set to False to use raw unscaled data.\n            For DAQmx data, there will be one column per DAQmx raw scaler and column names will include the scale id.\n        :return: The full TDMS file data.\n        :rtype: pandas.DataFrame\n        """"""\n\n        return pandas_export.from_tdms_file(self, time_index, absolute_time, scaled_data)\n\n    def as_hdf(self, filepath, mode=\'w\', group=\'/\'):\n        """"""\n        Converts the TDMS file into an HDF5 file\n\n        :param filepath: The path of the HDF5 file you want to write to.\n        :param mode: The write mode of the HDF5 file. This can be \'w\' or \'a\'\n        :param group: A group in the HDF5 file that will contain the TDMS data.\n        """"""\n        return hdf_export.from_tdms_file(self, filepath, mode, group)\n\n    def data_chunks(self):\n        """""" A generator that streams chunks of data from disk.\n        This method may only be used when the TDMS file was opened without reading all data immediately.\n\n        :rtype: Generator that yields :class:`DataChunk` objects\n        """"""\n        channel_offsets = defaultdict(int)\n        for chunk in self._reader.read_raw_data():\n            _convert_data_chunk(chunk, self._raw_timestamps)\n            yield DataChunk(self, chunk, channel_offsets)\n            for path, data in chunk.channel_data.items():\n                channel_offsets[path] += len(data)\n\n    def close(self):\n        """""" Close the underlying file if it was opened by this TdmsFile\n\n            If this TdmsFile was initialised with an already open file\n            then the reference to it is released but the file is not closed.\n        """"""\n        if self._reader is not None:\n            self._reader.close()\n            self._reader = None\n\n    def __len__(self):\n        """""" Returns the number of groups in this file\n        """"""\n        return len(self._groups)\n\n    def __iter__(self):\n        """""" Returns an iterator over the names of groups in this file\n        """"""\n        return iter(self._groups)\n\n    def __getitem__(self, group_name):\n        """""" Retrieve a TDMS group from the file by name\n        """"""\n        try:\n            return self._groups[group_name]\n        except KeyError:\n            raise KeyError(""There is no group named \'%s\' in the TDMS file"" % group_name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def _read_file(self, tdms_reader, read_metadata_only):\n        tdms_reader.read_metadata()\n\n        # Use object metadata to build group and channel objects\n        group_properties = OrderedDict()\n        group_channels = OrderedDict()\n        object_properties = {\n            path_string: self._convert_properties(obj.properties)\n            for path_string, obj in tdms_reader.object_metadata.items()}\n        try:\n            self._properties = object_properties[\'/\']\n        except KeyError:\n            pass\n\n        for (path_string, obj) in tdms_reader.object_metadata.items():\n            properties = object_properties[path_string]\n            path = ObjectPath.from_string(path_string)\n            if path.is_root:\n                pass\n            elif path.is_group:\n                group_properties[path.group] = properties\n            else:\n                # Object is a channel\n                try:\n                    channel_group_properties = object_properties[path.group_path()]\n                except KeyError:\n                    channel_group_properties = OrderedDict()\n                channel = TdmsChannel(\n                    path, obj.data_type, obj.scaler_data_types, obj.num_values,\n                    properties, channel_group_properties, self._properties,\n                    tdms_reader, self._raw_timestamps, self._memmap_dir)\n                if path.group in group_channels:\n                    group_channels[path.group].append(channel)\n                else:\n                    group_channels[path.group] = [channel]\n\n        # Create group objects containing channels and properties\n        for group_name, properties in group_properties.items():\n            try:\n                channels = group_channels[group_name]\n            except KeyError:\n                channels = []\n            group_path = ObjectPath(group_name)\n            self._groups[group_name] = TdmsGroup(group_path, properties, channels)\n        for group_name, channels in group_channels.items():\n            if group_name not in self._groups:\n                # Group with channels but without any corresponding object metadata in the file:\n                group_path = ObjectPath(group_name)\n                self._groups[group_name] = TdmsGroup(group_path, {}, channels)\n\n        if not read_metadata_only:\n            self._read_data(tdms_reader)\n\n    def _read_data(self, tdms_reader):\n        with Timer(log, ""Allocate space""):\n            # Allocate space for data\n            for group in self.groups():\n                for channel in group.channels():\n                    self._channel_data[channel.path] = get_data_receiver(\n                        channel, len(channel), self._raw_timestamps, self._memmap_dir)\n\n        with Timer(log, ""Read data""):\n            # Now actually read all the data\n            for chunk in tdms_reader.read_raw_data():\n                for (path, data) in chunk.channel_data.items():\n                    channel_data = self._channel_data[path]\n                    if data.data is not None:\n                        channel_data.append_data(data.data)\n                    elif data.scaler_data is not None:\n                        for scaler_id, scaler_data in data.scaler_data.items():\n                            channel_data.append_scaler_data(scaler_id, scaler_data)\n\n            for group in self.groups():\n                for channel in group.channels():\n                    channel_data = self._channel_data[channel.path]\n                    if channel_data is not None:\n                        channel._set_raw_data(channel_data)\n\n        self.data_read = True\n\n    def _convert_properties(self, properties):\n        def convert_prop(val):\n            if isinstance(val, TdmsTimestamp) and not self._raw_timestamps:\n                # Convert timestamps to numpy datetime64 if raw timestamps are not requested\n                return val.as_datetime64()\n            return val\n        return OrderedDict((k, convert_prop(v)) for (k, v) in properties.items())\n\n    def object(self, *path):\n        """"""(Deprecated) Get a TDMS object from the file\n\n        :param path: The object group and channel names. Providing no channel\n            returns a group object, and providing no channel or group\n            will return the root object.\n        :rtype: One of :class:`TdmsGroup`, :class:`TdmsChannel`, :class:`RootObject`\n\n        For example, to get the root object::\n\n            object()\n\n        To get a group::\n\n            object(""group_name"")\n\n        To get a channel::\n\n            object(""group_name"", ""channel_name"")\n        """"""\n\n        _deprecated(""TdmsFile.object"",\n                    ""Use TdmsFile.properties to access properties of the root object, "" +\n                    ""TdmsFile[group_name] to access a group object and "" +\n                    ""TdmsFile[group_name][channel_name] to access a channel object."")\n\n        def get_name(component):\n            try:\n                return component.name\n            except AttributeError:\n                return component\n\n        path = [get_name(c) for c in path]\n        object_path = ObjectPath(*path)\n        try:\n            return self.objects[str(object_path)]\n        except KeyError:\n            raise KeyError(""Invalid object path: %s"" % object_path)\n\n    def group_channels(self, group):\n        """"""(Deprecated) Returns a list of channel objects for the given group\n\n        :param group: Group or name of the group to get channels for.\n        :rtype: List of :class:`TdmsObject` objects.\n        """"""\n\n        _deprecated(""TdmsFile.group_channels"", ""Use TdmsFile[group_name].channels()."")\n\n        if isinstance(group, TdmsGroup):\n            return group.channels()\n\n        return self._groups[group].channels()\n\n    def channel_data(self, group, channel):\n        """"""(Deprecated)  Get the data for a channel\n\n        :param group: The name of the group the channel is in.\n        :param channel: The name of the channel to get data for.\n        :returns: The channel data.\n        :rtype: NumPy array.\n        """"""\n\n        _deprecated(""TdmsFile.channel_data"", ""Use TdmsFile[group_name][channel_name].data."")\n\n        if self._reader is None:\n            # Data should have already been loaded\n            return self[group][channel].data\n        else:\n            # Data must be lazily loaded\n            return self[group][channel].read_data()\n\n    @_property_builtin\n    def objects(self):\n        """""" (Deprecated) A dictionary of objects in the TDMS file, where the keys are the object paths.\n        """"""\n\n        _deprecated(""TdmsFile.objects"", ""Use TdmsFile.groups() to access all groups in the file, "" +\n                    ""and group.channels() to access all channels in a group."")\n\n        objects = OrderedDict()\n        root_path = ObjectPath()\n        objects[str(root_path)] = RootObject(self._properties)\n\n        for group in self.groups():\n            objects[group.path] = group\n            for channel in group.channels():\n                objects[channel.path] = channel\n\n        return objects\n\n\nclass TdmsGroup(object):\n    """""" Represents a group of channels in a TDMS file.\n\n    This class acts like a dictionary, where the keys are names of channels in the group\n    and the values are TdmsChannel objects.\n    A TdmsGroup can be indexed by channel name to access a channel in this group, for example::\n\n        channel = group[channel_name]\n\n    Iterating over a TdmsGroup produces the names of channels in this group,\n    or you can use the channels method to directly access all channels::\n\n        for channel in group.channels():\n            # Use channel\n            ...\n\n    :ivar ~.properties: Dictionary of TDMS properties defined for this group.\n    """"""\n\n    def __init__(self, path, properties, channels):\n        self._path = path\n        self.properties = properties\n        self._channels = {c.name: c for c in channels}\n\n    def __repr__(self):\n        return ""<TdmsGroup with path %s>"" % self.path\n\n    @_property_builtin\n    def path(self):\n        """""" Path to the TDMS object for this group\n        """"""\n        return str(self._path)\n\n    @_property_builtin\n    def name(self):\n        """""" The name of this group\n        """"""\n        return self._path.group\n\n    def channels(self):\n        """""" The list of channels in this group\n\n        :rtype: A list of TdmsChannel\n        """"""\n        return list(self._channels.values())\n\n    def as_dataframe(self, time_index=False, absolute_time=False, scaled_data=True):\n        """"""\n        Converts the TDMS group to a DataFrame. DataFrame columns are named using the channel names.\n\n        :param time_index: Whether to include a time index for the dataframe.\n        :param absolute_time: If time_index is true, whether the time index\n            values are absolute times or relative to the start time.\n        :param scaled_data: By default the scaled data will be used.\n            Set to False to use raw unscaled data.\n            For DAQmx data, there will be one column per DAQmx raw scaler and column names will include the scale id.\n        :return: The TDMS object data.\n        :rtype: pandas.DataFrame\n        """"""\n\n        return pandas_export.from_group(self, time_index, absolute_time, scaled_data)\n\n    def __len__(self):\n        """""" Returns the number of channels in this group\n        """"""\n        return len(self._channels)\n\n    def __iter__(self):\n        """""" Returns an iterator over the names of channels in this group\n        """"""\n        return iter(self._channels)\n\n    def __getitem__(self, channel_name):\n        """""" Retrieve a TDMS channel from this group by name\n        """"""\n        try:\n            return self._channels[channel_name]\n        except KeyError:\n            raise KeyError(\n                ""There is no channel named \'%s\' in group \'%s\' of the TDMS file"" %\n                (channel_name, self.name))\n\n    def property(self, property_name):\n        """"""(Deprecated) Returns the value of a TDMS property\n\n        :param property_name: The name of the property to get.\n        :returns: The value of the requested property.\n        :raises: KeyError if the property isn\'t found.\n        """"""\n\n        _deprecated(""TdmsGroup.property"", ""Use TdmsGroup.properties[property_name]."")\n\n        try:\n            return self.properties[property_name]\n        except KeyError:\n            raise KeyError(\n                ""Object does not have property \'%s\'"" % property_name)\n\n    @_property_builtin\n    def group(self):\n        """""" (Deprecated) Returns the name of the group for this object,\n            or None if it is the root object.\n        """"""\n        _deprecated(""TdmsGroup.group"", ""Use TdmsGroup.name."")\n        return self._path.group\n\n    @_property_builtin\n    def channel(self):\n        """""" (Deprecated) Returns the name of the channel for this object,\n            or None if it is a group or the root object.\n        """"""\n        _deprecated(""TdmsGroup.channel"", ""This always returns None."")\n        return None\n\n    @_property_builtin\n    def has_data(self):\n        """"""(Deprecated)""""""\n        _deprecated(""TdmsGroup.has_data"", ""This always returns False."")\n        return False\n\n\nclass TdmsChannel(object):\n    """""" Represents a data channel in a TDMS file.\n\n    This class acts like an array, you can get the length of a channel using :code:`len(channel)`,\n    and can iterate over values in the channel using a for loop,\n    or index into a channel using an integer index to get a single value::\n\n        for value in channel:\n            # Use value\n            ...\n        first_value = channel[0]\n\n    Or you can index using a slice to retrieve a range of data as a numpy array.\n    To get all data in this channel as a numpy array::\n\n        all_data = channel[:]\n\n    Or to retrieve a subset of data::\n\n        data_subset = channel[start:stop]\n\n    :ivar ~.properties: Dictionary of TDMS properties defined for this channel,\n                      for example the start time and time increment for waveforms.\n    """"""\n\n    def __init__(\n            self, path, data_type, scaler_data_types, number_values,\n            properties, group_properties, file_properties,\n            tdms_reader, raw_timestamps, memmap_dir):\n        self._path = path\n        self.properties = properties\n        self._length = number_values\n        self.data_type = data_type\n        self.scaler_data_types = scaler_data_types\n        self._group_properties = group_properties\n        self._file_properties = file_properties\n        self._reader = tdms_reader\n        self._raw_timestamps = raw_timestamps\n        self._memmap_dir = memmap_dir\n\n        self._raw_data = None\n        self._cached_chunk = None\n        self._cached_chunk_bounds = None\n\n    def __repr__(self):\n        return ""<TdmsChannel with path %s>"" % self.path\n\n    def __len__(self):\n        """""" Returns the number of values in this channel\n        """"""\n        return self._length\n\n    def __iter__(self):\n        """""" Returns an iterator over the values in this channel\n        """"""\n        if self._raw_data is not None:\n            return iter(self.data)\n        else:\n            return self._read_data_values()\n\n    def __getitem__(self, index):\n        if self._raw_data is not None:\n            return self.data[index]\n        elif index is Ellipsis:\n            return self.read_data()\n        elif isinstance(index, slice):\n            return self._read_slice(index.start, index.stop, index.step)\n        elif isinstance(index, int):\n            return self._read_at_index(index)\n        else:\n            raise TypeError(""Invalid index type \'%s\', expected int, slice or Ellipsis"" % type(index).__name__)\n\n    @_property_builtin\n    def path(self):\n        """""" Path to the TDMS object for this channel\n        """"""\n        return str(self._path)\n\n    @_property_builtin\n    def name(self):\n        """""" The name of this channel\n        """"""\n        return self._path.channel\n\n    @cached_property\n    def dtype(self):\n        """""" NumPy data type of the channel data\n\n        For data with a scaling this is the data type of the scaled data\n\n        :rtype: numpy.dtype\n        """"""\n        channel_scaling = self._scaling\n        if channel_scaling is not None:\n            return channel_scaling.get_dtype(self.data_type, self.scaler_data_types)\n        return self._raw_data_dtype()\n\n    def _raw_data_dtype(self):\n        if self.data_type is types.String:\n            return np.dtype(\'O\')\n        elif self.data_type is types.TimeStamp:\n            return np.dtype(\'<M8[us]\')\n        if self.data_type is not None and self.data_type.nptype is not None:\n            return self.data_type.nptype\n        return np.dtype(\'V8\')\n\n    @cached_property\n    def data(self):\n        """""" If the TdmsFile was created by reading all data, this property\n        provides direct access to the numpy array containing the data for this channel.\n\n        Indexing into the channel with a slice should be preferred to using this property, for example::\n\n            channel_data = channel[:]\n        """"""\n        if len(self) > 0 and self._raw_data is None:\n            raise RuntimeError(""Channel data has not been read"")\n\n        if self._raw_data is None:\n            return np.empty((0, ), dtype=self.dtype)\n        return self._scale_data(self._raw_data)\n\n    @_property_builtin\n    def raw_data(self):\n        """""" If the TdmsFile was created by reading all data, this property\n        provides direct access to the numpy array of raw, unscaled data.\n        For unscaled objects this is the same as the data property.\n        """"""\n        if len(self) > 0 and self._raw_data is None:\n            raise RuntimeError(""Channel data has not been read"")\n\n        if self._raw_data is None:\n            return np.empty((0, ), dtype=self._raw_data_dtype())\n        if self._raw_data.scaler_data:\n            if len(self._raw_data.scaler_data) == 1:\n                return next(v for v in self._raw_data.scaler_data.values())\n            else:\n                raise Exception(\n                    ""This object has data for multiple DAQmx scalers, ""\n                    ""use the raw_scaler_data property to get raw data ""\n                    ""for a scale_id"")\n        return self._raw_data.data\n\n    @_property_builtin\n    def raw_scaler_data(self):\n        """""" If the TdmsFile was created by reading all data, this property\n        provides direct access to the numpy array of raw DAQmx scaler data\n        as a dictionary mapping from scale id to raw data arrays.\n        """"""\n        if len(self) > 0 and self._raw_data is None:\n            raise RuntimeError(""Channel data has not been read"")\n\n        return self._raw_data.scaler_data\n\n    def data_chunks(self):\n        """""" A generator that streams chunks data for this channel from disk.\n        This method may only be used when the TDMS file was opened without reading all data immediately.\n\n        :rtype: Generator that yields :class:`ChannelDataChunk` objects\n        """"""\n        channel_offset = 0\n        for raw_data_chunk in self._read_channel_data_chunks():\n            yield ChannelDataChunk(self, raw_data_chunk, channel_offset)\n            channel_offset += len(raw_data_chunk)\n\n    def read_data(self, offset=0, length=None, scaled=True):\n        """""" Reads data for this channel from the TDMS file and returns it as a numpy array\n\n        Indexing into the channel with a slice should be preferred over using\n        this method, but this method is needed if you want to read raw, unscaled data.\n\n        :param offset: Initial position to read data from.\n        :param length: Number of values to attempt to read.\n            Fewer values will be returned if attempting to read beyond the end of the available data.\n        :param scaled: By default scaling will be applied to the returned data.\n            Set this parameter to False to return raw unscaled data.\n            For DAQmx data a dictionary of scaler id to raw scaler data will be returned.\n        """"""\n        raw_data = self._read_channel_data(offset, length)\n        if raw_data is None:\n            dtype = self.dtype if scaled else self._raw_data_dtype()\n            return np.empty((0,), dtype=dtype)\n        if scaled:\n            return self._scale_data(raw_data)\n        else:\n            if raw_data.scaler_data:\n                return raw_data.scaler_data\n            return raw_data.data\n\n    def time_track(self, absolute_time=False, accuracy=\'ns\'):\n        """"""Return an array of time or the independent variable for this channel\n\n        This depends on the object having the wf_increment\n        and wf_start_offset properties defined.\n        Note that wf_start_offset is usually zero for time-series data.\n        If you have time-series data channels with different start times,\n        you should use the absolute time or calculate the time offsets using\n        the wf_start_time property.\n\n        For larger timespans, the accuracy setting should be set lower.\n        The default setting is \'ns\', which has a timespan of\n        [1678 AD, 2262 AD]. For the exact ranges, refer to\n        http://docs.scipy.org/doc/numpy/reference/arrays.datetime.html\n        section ""Datetime Units"".\n\n        :param absolute_time: Whether the returned time values are absolute\n            times rather than relative to the start time. If true, the\n            wf_start_time property must be set.\n        :param accuracy: The accuracy of the returned datetime64 array.\n        :rtype: NumPy array.\n        :raises: KeyError if required properties aren\'t found\n\n        """"""\n\n        try:\n            increment = self.properties[\'wf_increment\']\n            offset = self.properties[\'wf_start_offset\']\n        except KeyError:\n            raise KeyError(""Object does not have time properties available."")\n\n        relative_time = np.linspace(\n            offset,\n            offset + (len(self) - 1) * increment,\n            len(self))\n\n        if not absolute_time:\n            return relative_time\n\n        try:\n            start_time = self.properties[\'wf_start_time\']\n        except KeyError:\n            raise KeyError(\n                ""Object does not have start time property available."")\n\n        try:\n            unit_correction = {\n                \'s\': 1e0,\n                \'ms\': 1e3,\n                \'us\': 1e6,\n                \'ns\': 1e9,\n            }[accuracy]\n        except KeyError:\n            raise KeyError(""Invalid accuracy: {0}"".format(accuracy))\n\n        # Because numpy only knows ints as its date datatype,\n        # convert to accuracy.\n        time_type = ""timedelta64[{0}]"".format(accuracy)\n        return (np.datetime64(start_time) +\n                (relative_time * unit_correction).astype(time_type))\n\n    def as_dataframe(self, time_index=False, absolute_time=False, scaled_data=True):\n        """"""\n        Converts the TDMS channel to a DataFrame. The DataFrame column is named using the channel path.\n\n        :param time_index: Whether to include a time index for the dataframe.\n        :param absolute_time: If time_index is true, whether the time index\n            values are absolute times or relative to the start time.\n        :param scaled_data: By default the scaled data will be used.\n            Set to False to use raw unscaled data.\n            For DAQmx data, there will be one column per DAQmx raw scaler and column names will include the scale id.\n        :return: The TDMS object data.\n        :rtype: pandas.DataFrame\n        """"""\n\n        return pandas_export.from_channel(self, time_index, absolute_time, scaled_data)\n\n    def _read_data_values(self):\n        for chunk in self.data_chunks():\n            for value in chunk:\n                yield value\n\n    def _read_slice(self, start, stop, step):\n        if step == 0:\n            raise ValueError(""Step size cannot be zero"")\n\n        # Replace None values with defaults\n        step = 1 if step is None else step\n        if start is None:\n            start = 0 if step > 0 else -1\n        if stop is None:\n            stop = self._length if step > 0 else -1 - self._length\n\n        # Handle negative indices\n        if start < 0:\n            start = self._length + start\n        if stop < 0:\n            stop = self._length + stop\n\n        # Check for empty ranges\n        if stop == start:\n            return np.empty((0, ), dtype=self.dtype)\n        if step > 0 and (stop < start or start >= self._length or stop < 0):\n            return np.empty((0,), dtype=self.dtype)\n        if step < 0 and (stop > start or stop >= self._length or start < 0):\n            return np.empty((0,), dtype=self.dtype)\n\n        # Trim values outside bounds\n        if start < 0:\n            start = 0\n        if start >= self._length:\n            start = self._length - 1\n        if stop > self._length:\n            stop = self._length\n        if stop < -1:\n            stop = -1\n\n        # Read data and handle step size\n        if step > 0:\n            read_data = self.read_data(start, stop - start)\n            return read_data[::step] if step > 1 else read_data\n        else:\n            read_data = self.read_data(stop + 1, start - stop)\n            return read_data[::step]\n\n    def _read_at_index(self, index):\n        if index < 0 or index >= self._length:\n            raise IndexError(""Index {0} is outside of the channel bounds [0, {1}]"".format(index, self._length - 1))\n\n        if self._cached_chunk is not None:\n            # Check if we\'ve already read and cached the chunk containing this index\n            bounds = self._cached_chunk_bounds\n            if bounds[0] <= index < bounds[1]:\n                return self._cached_chunk[index - bounds[0]]\n\n        chunk, chunk_offset = self._read_channel_data_chunk_for_index(index)\n        scaled_chunk = self._scale_data(chunk)\n        self._cached_chunk = scaled_chunk\n        self._cached_chunk_bounds = (chunk_offset, chunk_offset + len(scaled_chunk))\n\n        return scaled_chunk[index - chunk_offset]\n\n    def _scale_data(self, raw_data):\n        scale = self._scaling\n        if scale is not None:\n            return scale.scale(raw_data)\n        elif raw_data.scaler_data:\n            raise ValueError(""Missing scaling information for DAQmx data"")\n        else:\n            return raw_data.data\n\n    @cached_property\n    def _scaling(self):\n        return scaling.get_scaling(\n            self.properties, self._group_properties, self._file_properties)\n\n    def _read_channel_data_chunks(self):\n        for chunk in self._reader.read_raw_data_for_channel(self.path):\n            _convert_channel_data_chunk(chunk, self._raw_timestamps)\n            yield chunk\n\n    def _read_channel_data_chunk_for_index(self, index):\n        (chunk, offset) = self._reader.read_channel_chunk_for_index(self.path, index)\n        _convert_channel_data_chunk(chunk, self._raw_timestamps)\n        return chunk, offset\n\n    def _read_channel_data(self, offset=0, length=None):\n        if offset < 0:\n            raise ValueError(""offset must be non-negative"")\n        if length is not None and length < 0:\n            raise ValueError(""length must be non-negative"")\n\n        with Timer(log, ""Allocate space for channel""):\n            # Allocate space for data\n            if length is None:\n                num_values = len(self) - offset\n            else:\n                num_values = min(length, len(self) - offset)\n            num_values = max(0, num_values)\n            channel_data = get_data_receiver(self, num_values, self._raw_timestamps, self._memmap_dir)\n\n        with Timer(log, ""Read data for channel""):\n            # Now actually read all the data\n            for chunk in self._reader.read_raw_data_for_channel(self.path, offset, length):\n                if chunk.data is not None:\n                    channel_data.append_data(chunk.data)\n                if chunk.scaler_data is not None:\n                    for scaler_id, scaler_data in chunk.scaler_data.items():\n                        channel_data.append_scaler_data(scaler_id, scaler_data)\n\n        return channel_data\n\n    def _set_raw_data(self, data):\n        self._raw_data = data\n\n    def property(self, property_name):\n        """"""(Deprecated) Returns the value of a TDMS property\n\n        :param property_name: The name of the property to get.\n        :returns: The value of the requested property.\n        :raises: KeyError if the property isn\'t found.\n        """"""\n        _deprecated(""TdmsChannel.property"", ""Use TdmsChannel.properties[property_name]"")\n\n        try:\n            return self.properties[property_name]\n        except KeyError:\n            raise KeyError(\n                ""Object does not have property \'%s\'"" % property_name)\n\n    @_property_builtin\n    def group(self):\n        """""" (Deprecated) Returns the name of the group for this object,\n            or None if it is the root object.\n        """"""\n        _deprecated(""TdmsChannel.group"")\n        return self._path.group\n\n    @_property_builtin\n    def channel(self):\n        """""" (Deprecated) Returns the name of the channel for this object,\n            or None if it is a group or the root object.\n        """"""\n        _deprecated(""TdmsChannel.channel"", ""Use TdmsChannel.name"")\n        return self._path.channel\n\n    @_property_builtin\n    def has_data(self):\n        """"""(Deprecated)""""""\n        _deprecated(""TdmsChannel.has_data"", ""This always returns True"")\n        return True\n\n    @_property_builtin\n    def number_values(self):\n        """"""(Deprecated)""""""\n        _deprecated(""TdmsChannel.number_values"", ""Use len(channel)"")\n        return self._length\n\n\nclass DataChunk(object):\n    """""" A chunk of data in a TDMS file\n\n    Can be indexed by group name to get the data for a group in this channel,\n    which can then be indexed by channel name to get the data for a channel in this chunk.\n    For example::\n\n        group_chunk = data_chunk[group_name]\n        channel_chunk = group_chunk[channel_name]\n    """"""\n    def __init__(self, tdms_file, raw_data_chunk, channel_offsets):\n        self._groups = OrderedDict(\n            (group.name, GroupDataChunk(tdms_file, group, raw_data_chunk, channel_offsets))\n            for group in tdms_file.groups())\n\n    def __getitem__(self, group_name):\n        """""" Get a chunk of data for a group\n        """"""\n        return self._groups[group_name]\n\n    def groups(self):\n        """""" Returns chunks of data for all groups\n\n        :rtype: List of :class:`GroupDataChunk`\n        """"""\n        return list(self._groups.values())\n\n\nclass GroupDataChunk(object):\n    """""" A chunk of data for a group in a TDMS file\n\n    Can be indexed by channel name to get the data for a channel in this chunk.\n    For example::\n\n        channel_chunk = group_chunk[channel_name]\n\n    :ivar ~.name: Name of the group\n    """"""\n    def __init__(self, tdms_file, group, raw_data_chunk, channel_offsets):\n        self.name = group.name\n        self._channels = OrderedDict(\n            (channel.name, ChannelDataChunk(\n                channel,\n                raw_data_chunk.channel_data.get(channel.path, RawChannelDataChunk.empty()),\n                channel_offsets[channel.path]))\n            for channel in group.channels())\n\n    def __getitem__(self, channel_name):\n        """""" Get a chunk of data for a channel in this group\n        """"""\n        return self._channels[channel_name]\n\n    def channels(self):\n        """""" Returns chunks of channel data for all channels in this group\n\n        :rtype: List of :class:`ChannelDataChunk`\n        """"""\n        return list(self._channels.values())\n\n\nclass ChannelDataChunk(object):\n    """""" A chunk of data for a channel in a TDMS file\n\n    Is an array-like object that supports indexing to access data, for example::\n\n        chunk_length = len(channel_data_chunk)\n        chunk_data = channel_data_chunk[:]\n\n    :ivar ~.name: Name of the channel\n    :ivar ~.offset: Starting index of this chunk of data in the entire channel\n    """"""\n    def __init__(self, channel, raw_data_chunk, offset):\n        self._path = channel._path\n        self._channel = channel\n        self.name = channel.name\n        self.offset = offset\n        self._raw_data = raw_data_chunk\n\n    def __len__(self):\n        """""" Returns the number of values in this chunk\n        """"""\n        return len(self._raw_data)\n\n    def __getitem__(self, index):\n        """""" Get a value or slice of values from this chunk\n        """"""\n        return self._data()[index]\n\n    def __iter__(self):\n        """""" Iterate over values in this chunk\n        """"""\n        return iter(self._data())\n\n    def _data(self):\n        if self._raw_data.data is None and self._raw_data.scaler_data is None:\n            return np.empty((0, ), dtype=self._channel.dtype)\n\n        scale = self._channel._scaling\n        if scale is not None:\n            return scale.scale(self._raw_data)\n        elif self._raw_data.scaler_data:\n            raise ValueError(""Missing scaling information for DAQmx data"")\n        else:\n            return self._raw_data.data\n\n\nclass RootObject(object):\n    def __init__(self, properties):\n        self.properties = properties\n\n    def property(self, property_name):\n        _deprecated(""RootObject"", ""Use TdmsFile.properties to access properties from the root object"")\n        try:\n            return self.properties[property_name]\n        except KeyError:\n            raise KeyError(\n                ""Object does not have property \'%s\'"" % property_name)\n\n    @_property_builtin\n    def group(self):\n        _deprecated(""RootObject"", ""Use TdmsFile.properties to access properties from the root object"")\n        return None\n\n    @_property_builtin\n    def channel(self):\n        _deprecated(""RootObject"", ""Use TdmsFile.properties to access properties from the root object"")\n        return None\n\n    @_property_builtin\n    def has_data(self):\n        _deprecated(""RootObject"", ""Use TdmsFile.properties to access properties from the root object"")\n        return False\n\n\ndef _deprecated(name, detail=None):\n    message = ""\'{0}\' is deprecated and will be removed in a future release."".format(name)\n    if detail is not None:\n        message += "" {0}"".format(detail)\n    warnings.warn(message)\n\n\ndef _convert_data_chunk(chunk, raw_timestamps):\n    for channel_chunk in chunk.channel_data.values():\n        _convert_channel_data_chunk(channel_chunk, raw_timestamps)\n\n\ndef _convert_channel_data_chunk(channel_chunk, raw_timestamps):\n    if not raw_timestamps and isinstance(channel_chunk.data, TimestampArray):\n        channel_chunk.data = channel_chunk.data.as_datetime64()\n'"
nptdms/tdms_segment.py,2,"b'import os\nimport numpy as np\n\nfrom nptdms import types\nfrom nptdms.base_segment import (\n    BaseSegment,\n    BaseSegmentObject,\n    RawChannelDataChunk,\n    RawDataChunk,\n    read_interleaved_segment_bytes,\n    fromfile)\nfrom nptdms.log import log_manager\n\n\nlog = log_manager.get_logger(__name__)\n\n\nclass InterleavedDataSegment(BaseSegment):\n    """""" A TDMS segment with interleaved data\n    """"""\n\n    __slots__ = []\n\n    def _new_segment_object(self, object_path):\n        return TdmsSegmentObject(object_path, self.endianness)\n\n    def _read_data_chunk(self, file, data_objects, chunk_index):\n        # If all data types are sized and all the lengths are\n        # the same, then we can read all data at once with numpy,\n        # which is much faster\n        all_sized = all(\n            o.data_type.size is not None for o in data_objects)\n        same_length = (len(\n            set((o.number_values for o in data_objects))) == 1)\n        if all_sized and same_length:\n            return self._read_interleaved_sized(file, data_objects)\n        else:\n            return self._read_interleaved(file, data_objects)\n\n    def _read_interleaved_sized(self, file, data_objects):\n        """"""Read interleaved data where all channels have a sized data type and the same length\n        """"""\n        log.debug(""Reading interleaved data all at once"")\n\n        total_data_width = sum(o.data_type.size for o in data_objects)\n        log.debug(""total_data_width: %d"", total_data_width)\n\n        # Read all data into 1 byte unsigned ints first\n        combined_data = read_interleaved_segment_bytes(\n            file, total_data_width, data_objects[0].number_values)\n\n        # Now get arrays for each channel\n        channel_data = {}\n        data_pos = 0\n        for (i, obj) in enumerate(data_objects):\n            byte_columns = tuple(\n                range(data_pos, obj.data_type.size + data_pos))\n            log.debug(""Byte columns for channel %d: %s"", i, byte_columns)\n            # Select columns for this channel, so that number of values will\n            # be number of bytes per point * number of data points.\n            # Then use ravel to flatten the results into a vector.\n            object_data = combined_data[:, byte_columns].ravel()\n            if obj.data_type.nptype is not None:\n                # Set correct data type, so that the array length should be correct\n                object_data.dtype = (\n                    obj.data_type.nptype.newbyteorder(self.endianness))\n            else:\n                object_data = obj.data_type.from_bytes(object_data, self.endianness)\n            channel_data[obj.path] = object_data\n            data_pos += obj.data_type.size\n\n        return RawDataChunk.channel_data(channel_data)\n\n    def _read_interleaved(self, file, data_objects):\n        """"""Read interleaved data that doesn\'t have a numpy type""""""\n\n        log.debug(""Reading interleaved data point by point"")\n        object_data = {}\n        points_added = {}\n        for obj in data_objects:\n            object_data[obj.path] = obj.new_segment_data()\n            points_added[obj.path] = 0\n        while any([points_added[o.path] < o.number_values\n                   for o in data_objects]):\n            for obj in data_objects:\n                if points_added[obj.path] < obj.number_values:\n                    object_data[obj.path][points_added[obj.path]] = (\n                        obj.read_value(file))\n                    points_added[obj.path] += 1\n\n        return RawDataChunk.channel_data(object_data)\n\n\nclass ContiguousDataSegment(BaseSegment):\n    """""" A TDMS segment with contiguous (non-interleaved) data\n    """"""\n\n    __slots__ = []\n\n    def _new_segment_object(self, object_path):\n        return TdmsSegmentObject(object_path, self.endianness)\n\n    def _read_data_chunk(self, file, data_objects, chunk_index):\n        log.debug(""Reading contiguous data chunk"")\n        object_data = {}\n        for obj in data_objects:\n            number_values = self._get_channel_number_values(obj, chunk_index)\n            object_data[obj.path] = obj.read_values(file, number_values)\n        return RawDataChunk.channel_data(object_data)\n\n    def _read_channel_data_chunk(self, file, data_objects, chunk_index, channel_path):\n        """""" Read data from a chunk for a single channel\n        """"""\n        channel_data = RawChannelDataChunk.empty()\n        for obj in data_objects:\n            number_values = self._get_channel_number_values(obj, chunk_index)\n            if obj.path == channel_path:\n                channel_data = RawChannelDataChunk.channel_data(obj.read_values(file, number_values))\n            elif number_values == obj.number_values:\n                # Seek over data for other channel data\n                file.seek(obj.data_size, os.SEEK_CUR)\n            else:\n                # In last chunk with reduced chunk size\n                if obj.data_type.size is None:\n                    # Type is unsized (eg. string), try reading number of values\n                    obj.read_values(file, number_values)\n                else:\n                    file.seek(obj.data_type.size * number_values, os.SEEK_CUR)\n        return channel_data\n\n    def _get_channel_number_values(self, obj, chunk_index):\n        if (chunk_index == (self.num_chunks - 1) and\n                self.final_chunk_proportion != 1.0):\n            return int(obj.number_values * self.final_chunk_proportion)\n        else:\n            return obj.number_values\n\n\nclass TdmsSegmentObject(BaseSegmentObject):\n    """""" A standard (non DAQmx) TDMS segment object\n    """"""\n\n    __slots__ = []\n\n    def read_raw_data_index(self, f, raw_data_index_header):\n        # Metadata format is standard (non-DAQmx) TDMS format.\n        # raw_data_index_header gives the length of the index information.\n\n        # Read the data type\n        try:\n            self.data_type = types.tds_data_types[\n                types.Uint32.read(f, self.endianness)]\n        except KeyError:\n            raise KeyError(""Unrecognised data type"")\n        log.debug(""Object data type: %s"", self.data_type.__name__)\n\n        if (self.data_type.size is None and\n                self.data_type != types.String):\n            raise ValueError(\n                ""Unsupported data type: %r"" % self.data_type)\n\n        # Read data dimension\n        dimension = types.Uint32.read(f, self.endianness)\n        # In TDMS version 2.0, 1 is the only valid value for dimension\n        if dimension != 1:\n            raise ValueError(""Data dimension is not 1"")\n\n        # Read number of values\n        self.number_values = types.Uint64.read(f, self.endianness)\n\n        # Variable length data types have total size\n        if self.data_type in (types.String,):\n            self.data_size = types.Uint64.read(f, self.endianness)\n        else:\n            self.data_size = self.number_values * self.data_type.size\n\n        log.debug(\n            ""Object number of values in segment: %d"", self.number_values)\n\n    def read_value(self, file):\n        """"""Read a single value from the given file""""""\n\n        if self.data_type.nptype is not None:\n            dtype = self.data_type.nptype.newbyteorder(self.endianness)\n            return fromfile(file, dtype=dtype, count=1)[0]\n        return self.data_type.read(file, self.endianness)\n\n    def read_values(self, file, number_values):\n        """"""Read all values for this object from a contiguous segment""""""\n\n        if self.data_type.nptype is not None:\n            dtype = self.data_type.nptype.newbyteorder(self.endianness)\n            return fromfile(file, dtype=dtype, count=number_values)\n        elif self.data_type.size is not None:\n            byte_data = fromfile(file, dtype=np.dtype(\'uint8\'), count=number_values * self.data_type.size)\n            return self.data_type.from_bytes(byte_data, self.endianness)\n        else:\n            return self.data_type.read_values(file, number_values, self.endianness)\n\n    def new_segment_data(self):\n        """"""Return a new array to read the data of the current section into""""""\n\n        if self.data_type.nptype is not None:\n            return np.zeros(self.number_values, dtype=self.data_type.nptype)\n        else:\n            return [None] * self.number_values\n'"
nptdms/tdmsinfo.py,0,"b'from __future__ import print_function\n\nfrom argparse import ArgumentParser\nimport logging\n\nfrom nptdms import TdmsFile\nfrom nptdms.log import log_manager\n\n\ndef main():\n    parser = ArgumentParser(\n        description=""List the contents of a LabView TDMS file."")\n    parser.add_argument(\n        \'-p\', \'--properties\', action=""store_true"",\n        help=""Include channel properties."")\n    parser.add_argument(\n        \'-d\', \'--debug\', action=""store_true"",\n        help=""Print debugging information to stderr."")\n    parser.add_argument(\n        \'tdms_file\',\n        help=""TDMS file to read."")\n    args = parser.parse_args()\n\n    if args.debug:\n        log_manager.set_level(logging.DEBUG)\n\n    tdmsinfo(args.tdms_file, args.properties)\n\n\ndef tdmsinfo(file, show_properties=False):\n    tdms_file = TdmsFile.read_metadata(file)\n\n    level = 0\n    display(\'/\', level)\n    if show_properties:\n        display_properties(tdms_file, level + 1)\n    for group in tdms_file.groups():\n        level = 1\n        display(""%s"" % group.path, level)\n        if show_properties:\n            display_properties(group, level + 1)\n        for channel in group.channels():\n            level = 2\n            display(""%s"" % channel.path, level)\n            if show_properties:\n                level = 3\n                if channel.data_type is not None:\n                    display(""data type: %s"" % channel.data_type.__name__, level)\n                display(""length: %d"" % len(channel), level)\n                display_properties(channel, level)\n\n\ndef display_properties(tdms_object, level):\n    if tdms_object.properties:\n        display(""properties:"", level)\n        for prop, val in tdms_object.properties.items():\n            display(""%s: %s"" % (prop, val), level + 1)\n\n\ndef display(s, level):\n    print(""%s%s"" % ("" "" * 2 * level, s))\n'"
nptdms/timestamp.py,10,"b'from datetime import datetime, timedelta\nimport numpy as np\n\n\nEPOCH = np.datetime64(\'1904-01-01 00:00:00\', \'s\')\n\n\nclass TdmsTimestamp(object):\n    """""" A Timestamp from a TDMS file\n\n        The TDMS format stores timestamps as a signed number of seconds since the epoch 1904-01-01 00:00:00 UTC\n        and number of positive fractions (2^-64) of a second.\n\n        :ivar ~.seconds: Seconds since the epoch as a signed integer\n        :ivar ~.second_fractions: A positive number of 2^-64 fractions of a second\n    """"""\n\n    def __init__(self, seconds, second_fractions):\n        self.seconds = seconds\n        self.second_fractions = second_fractions\n\n    def __repr__(self):\n        return ""TdmsTimestamp({0}, {1})"".format(self.seconds, self.second_fractions)\n\n    def __str__(self):\n        dt = EPOCH + np.timedelta64(self.seconds, \'s\')\n        fraction_string = ""{0:.6f}"".format(self.second_fractions * 2.0 ** -64).split(\'.\')[1]\n        return ""{0}.{1}"".format(dt, fraction_string)\n\n    def as_datetime64(self, resolution=\'us\'):\n        """""" Convert this timestamp to a numpy datetime64 object\n\n            :param resolution: The resolution of the datetime64 object to create as a numpy unit code.\n                Must be one of \'s\', \'ms\', \'us\', \'ns\' or \'ps\'\n        """"""\n        try:\n            fractions_per_step = _fractions_per_step[resolution]\n        except KeyError:\n            raise ValueError(""Unsupported resolution for converting to numpy datetime64: \'{0}\'"".format(resolution))\n        return (\n                EPOCH +\n                np.timedelta64(self.seconds, \'s\') +\n                ((self.second_fractions / fractions_per_step) * np.timedelta64(1, resolution)))\n\n    def as_datetime(self):\n        """""" Convert this timestamp to a Python datetime.datetime object\n        """"""\n        fractions_per_us = _fractions_per_step[\'us\']\n        microseconds = (self.second_fractions / fractions_per_us)\n        return datetime(1904, 1, 1, 0, 0, 0) + timedelta(seconds=self.seconds) + timedelta(microseconds=microseconds)\n\n\nclass TimestampArray(np.ndarray):\n    """""" A numpy array of TDMS timestamps\n\n        Indexing into a TimestampArray returns TdmsTimestamp objects.\n    """"""\n\n    def __new__(cls, input_array):\n        """""" Create a new TimestampArray\n\n            The input array must be a structured numpy array with \'seconds\' and \'second_fractions\' fields.\n        """"""\n        obj = np.asarray(input_array).view(cls)\n        field_names = input_array.dtype.names\n        if field_names == (\'second_fractions\', \'seconds\'):\n            obj._field_indices = (1, 0)\n        elif field_names == (\'seconds\', \'second_fractions\'):\n            obj._field_indices = (0, 1)\n        else:\n            raise ValueError(""Input array must have a dtype with \'seconds\' and \'second_fractions\' fields"")\n        return obj\n\n    def __array_finalize__(self, obj):\n        if obj is None:\n            return\n        self._field_indices = getattr(obj, \'_field_indices\', \'<\')\n\n    def __getitem__(self, item):\n        val = super(TimestampArray, self).__getitem__(item)\n        if isinstance(item, str):\n            # Getting a field, we don\'t want to return this as a TimestampArray\n            # but as a normal numpy ndarray\n            return val.view(np.ndarray)\n        if isinstance(item, (int, np.number)):\n            # Getting a single item\n            return TdmsTimestamp(val[self._field_indices[0]], val[self._field_indices[1]])\n        # else getting a slice returns a new TimestampArray\n        return val\n\n    @property\n    def seconds(self):\n        """""" The number of seconds since the TDMS epoch (1904-01-01 00:00:00 UTC) as a numpy array\n        """"""\n        return self[\'seconds\']\n\n    @property\n    def second_fractions(self):\n        """""" The number of 2**-64 fractions of a second as a numpy array\n        """"""\n        return self[\'second_fractions\']\n\n    def as_datetime64(self, resolution=\'us\'):\n        """""" Convert to an array of numpy datetime64 objects\n\n            :param resolution: The resolution of the datetime64 objects to create as a numpy unit code.\n                Must be one of \'s\', \'ms\', \'us\', \'ns\' or \'ps\'\n        """"""\n        try:\n            fractions_per_step = _fractions_per_step[resolution]\n        except KeyError:\n            raise ValueError(""Unsupported resolution for converting to numpy datetime64: \'{0}\'"".format(resolution))\n        return (\n                EPOCH +\n                self[\'seconds\'] * np.timedelta64(1, \'s\') +\n                (self[\'second_fractions\'] / fractions_per_step) * np.timedelta64(1, resolution))\n\n\n_fractions_per_step = {\n    \'s\': 1.0 / 2 ** -64,\n    \'ms\': (10 ** -3) / 2 ** -64,\n    \'us\': (10 ** -6) / 2 ** -64,\n    \'ns\': (10 ** -9) / 2 ** -64,\n    \'ps\': (10 ** -12) / 2 ** -64,\n}\n'"
nptdms/types.py,27,"b'""""""Conversions to and from bytes representation of values in TDMS files""""""\n\nimport numpy as np\nimport struct\nfrom nptdms.timestamp import TdmsTimestamp, TimestampArray\n\n\n__all__ = [\n    \'numpy_data_types\',\n    \'tds_data_types\',\n    \'TdmsType\',\n    \'Bytes\',\n    \'Void\',\n    \'Int8\',\n    \'Int16\',\n    \'Int32\',\n    \'Int64\',\n    \'Uint8\',\n    \'Uint16\',\n    \'Uint32\',\n    \'Uint64\',\n    \'SingleFloat\',\n    \'DoubleFloat\',\n    \'ExtendedFloat\',\n    \'SingleFloatWithUnit\',\n    \'DoubleFloatWithUnit\',\n    \'ExtendedFloatWithUnit\',\n    \'String\',\n    \'Boolean\',\n    \'TimeStamp\',\n    \'ComplexSingleFloat\',\n    \'ComplexDoubleFloat\',\n    \'DaqMxRawData\',\n]\n\n\n_struct_pack = struct.pack\n_struct_unpack = struct.unpack\n\n\ntds_data_types = {}\nnumpy_data_types = {}\n\n\ndef tds_data_type(enum_value, np_type):\n    def decorator(cls):\n        cls.enum_value = enum_value\n        cls.nptype = None if np_type is None else np.dtype(np_type)\n        if enum_value is not None:\n            tds_data_types[enum_value] = cls\n        if np_type is not None:\n            numpy_data_types[np.dtype(np_type)] = cls\n        return cls\n    return decorator\n\n\nclass TdmsType(object):\n    size = None\n\n    def __init__(self):\n        self.value = None\n        self.bytes = None\n\n    def __eq__(self, other):\n        return self.bytes == other.bytes and self.value == other.value\n\n    def __repr__(self):\n        if self.value is None:\n            return ""%s"" % self.__class__.__name__\n        return ""%s(%r)"" % (self.__class__.__name__, self.value)\n\n    @classmethod\n    def read(cls, file, endianness=""<""):\n        raise NotImplementedError(""Unsupported data type to read: %r"" % cls)\n\n    @classmethod\n    def read_values(cls, file, number_values, endianness=""<""):\n        raise NotImplementedError(""Unsupported data type to read: %r"" % cls)\n\n\nclass Bytes(TdmsType):\n    def __init__(self, value):\n        self.value = value\n        self.bytes = value\n\n\nclass StructType(TdmsType):\n    struct_declaration = None\n\n    def __init__(self, value):\n        self.value = value\n        self.bytes = _struct_pack(\'<\' + self.struct_declaration, value)\n\n    @classmethod\n    def read(cls, file, endianness=""<""):\n        read_bytes = file.read(cls.size)\n        return _struct_unpack(endianness + cls.struct_declaration, read_bytes)[0]\n\n\n@tds_data_type(0, None)\nclass Void(TdmsType):\n    pass\n\n\n@tds_data_type(1, np.int8)\nclass Int8(StructType):\n    size = 1\n    struct_declaration = ""b""\n\n\n@tds_data_type(2, np.int16)\nclass Int16(StructType):\n    size = 2\n    struct_declaration = ""h""\n\n\n@tds_data_type(3, np.int32)\nclass Int32(StructType):\n    size = 4\n    struct_declaration = ""l""\n\n\n@tds_data_type(4, np.int64)\nclass Int64(StructType):\n    size = 8\n    struct_declaration = ""q""\n\n\n@tds_data_type(5, np.uint8)\nclass Uint8(StructType):\n    size = 1\n    struct_declaration = ""B""\n\n\n@tds_data_type(6, np.uint16)\nclass Uint16(StructType):\n    size = 2\n    struct_declaration = ""H""\n\n\n@tds_data_type(7, np.uint32)\nclass Uint32(StructType):\n    size = 4\n    struct_declaration = ""L""\n\n\n@tds_data_type(8, np.uint64)\nclass Uint64(StructType):\n    size = 8\n    struct_declaration = ""Q""\n\n\n@tds_data_type(9, np.single)\nclass SingleFloat(StructType):\n    size = 4\n    struct_declaration = ""f""\n\n\n@tds_data_type(10, np.double)\nclass DoubleFloat(StructType):\n    size = 8\n    struct_declaration = ""d""\n\n\n@tds_data_type(11, None)\nclass ExtendedFloat(TdmsType):\n    pass\n\n\n@tds_data_type(0x19, np.single)\nclass SingleFloatWithUnit(StructType):\n    size = 4\n    struct_declaration = ""f""\n\n\n@tds_data_type(0x1A, np.double)\nclass DoubleFloatWithUnit(StructType):\n    size = 8\n    struct_declaration = ""d""\n\n\n@tds_data_type(0x1B, None)\nclass ExtendedFloatWithUnit(TdmsType):\n    pass\n\n\n@tds_data_type(0x20, None)\nclass String(TdmsType):\n    def __init__(self, value):\n        self.value = value\n        content = value.encode(\'utf-8\')\n        length = _struct_pack(\'<L\', len(content))\n        self.bytes = length + content\n\n    @staticmethod\n    def read(file, endianness=""<""):\n        size_bytes = file.read(4)\n        size = _struct_unpack(endianness + \'L\', size_bytes)[0]\n        return file.read(size).decode(\'utf-8\')\n\n    @classmethod\n    def read_values(cls, file, number_values, endianness=""<""):\n        """""" Read string raw data\n\n            This is stored as an array of offsets\n            followed by the contiguous string data.\n        """"""\n        offsets = [0]\n        for i in range(number_values):\n            offsets.append(Uint32.read(file, endianness))\n        strings = []\n        for i in range(number_values):\n            s = file.read(offsets[i + 1] - offsets[i])\n            strings.append(s.decode(\'utf-8\'))\n        return strings\n\n\n@tds_data_type(0x21, np.bool8)\nclass Boolean(StructType):\n    size = 1\n    struct_declaration = ""b""\n\n    @classmethod\n    def read(cls, file, endianness=""<""):\n        return bool(super(Boolean, cls).read(file, endianness))\n\n\n@tds_data_type(0x44, None)\nclass TimeStamp(TdmsType):\n    # Time stamps are stored as number of seconds since\n    # 01/01/1904 00:00:00.00 UTC, ignoring leap seconds,\n    # and number of 2^-64 fractions of a second.\n    # Note that the TDMS epoch is not the Unix epoch.\n    _tdms_epoch = np.datetime64(\'1904-01-01 00:00:00\', \'us\')\n    _fractions_per_microsecond = float(10**-6) / 2**-64\n\n    size = 16\n\n    def __init__(self, value):\n        if not isinstance(value, np.datetime64):\n            value = np.datetime64(value, \'us\')\n        self.value = value\n        epoch_delta = value - self._tdms_epoch\n\n        seconds = int(epoch_delta / np.timedelta64(1, \'s\'))\n        remainder = epoch_delta - np.timedelta64(seconds, \'s\')\n        zero_delta = np.timedelta64(0, \'s\')\n        if remainder < zero_delta:\n            remainder = np.timedelta64(1, \'s\') + remainder\n            seconds = seconds - 1\n        microseconds = int(remainder / np.timedelta64(1, \'us\'))\n        second_fractions = int(microseconds * self._fractions_per_microsecond)\n        self.bytes = _struct_pack(\'<Qq\', second_fractions, seconds)\n\n    @classmethod\n    def read(cls, file, endianness=""<""):\n        data = file.read(16)\n        if endianness == ""<"":\n            (second_fractions, seconds) = _struct_unpack(\n                endianness + \'Qq\', data)\n        else:\n            (seconds, second_fractions) = _struct_unpack(\n                 endianness + \'qQ\', data)\n        return TdmsTimestamp(seconds, second_fractions)\n\n    @classmethod\n    def from_bytes(cls, byte_array, endianness=""<""):\n        """""" Convert an array of bytes to an array of timestamps\n        """"""\n        byte_array = byte_array.reshape((-1, 16))\n        if endianness == ""<"":\n            dtype = np.dtype([(\'second_fractions\', \'<u8\'), (\'seconds\', \'<i8\')])\n        else:\n            dtype = np.dtype([(\'seconds\', \'>i8\'), (\'second_fractions\', \'>u8\')])\n        return TimestampArray(byte_array.view(dtype).reshape(-1))\n\n\n@tds_data_type(0x08000c, np.complex64)\nclass ComplexSingleFloat(TdmsType):\n    size = 8\n\n\n@tds_data_type(0x10000d, np.complex128)\nclass ComplexDoubleFloat(TdmsType):\n    size = 16\n\n\n@tds_data_type(0xFFFFFFFF, None)\nclass DaqMxRawData(TdmsType):\n    pass\n'"
nptdms/utils.py,0,"b'from functools import wraps\nimport logging\nimport time\n\ntry:\n    from collections import OrderedDict\nexcept ImportError:\n    try:\n        # ordereddict available on pypi for Python < 2.7\n        from ordereddict import OrderedDict\n    except ImportError:\n        # Otherwise fall back on normal dict\n        OrderedDict = dict\n\n\ndef cached_property(func):\n    """""" Wraps a method on a class to make it a property and caches the result the first time it is evaluated\n    """"""\n    attr_name = \'_cached_prop_\' + func.__name__\n\n    @property\n    @wraps(func)\n    def get(self):\n        try:\n            return getattr(self, attr_name)\n        except AttributeError:\n            value = func(self)\n            setattr(self, attr_name, value)\n            return value\n\n    return get\n\n\nclass Timer(object):\n    """""" Context manager for logging the  time taken  for an operation\n    """"""\n\n    def __init__(self, log, description):\n        self._enabled = log.isEnabledFor(logging.INFO)\n        self._log = log\n        self._description = description\n        self._start_time = None\n\n    def __enter__(self):\n        if not self._enabled:\n            return self\n        try:\n            self._start_time = time.perf_counter()\n        except AttributeError:\n            # Python < 3.3\n            self._start_time = time.clock()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if not self._enabled:\n            return\n        try:\n            end_time = time.perf_counter()\n        except AttributeError:\n            # Python < 3.3\n            end_time = time.clock()\n\n        elapsed_time = (end_time - self._start_time) * 1.0e3\n        self._log.info(""{0}: Took {1} ms"".format(self._description, elapsed_time))\n'"
nptdms/version.py,0,"b""__version_info__ = (0, 27, 0)\n__version__ = '.'.join('%d' % d for d in __version_info__)\n"""
nptdms/writer.py,13,"b'""""""Module for writing TDMS files""""""\n\ntry:\n    from collections import OrderedDict\nexcept ImportError:\n    OrderedDict = dict\nfrom datetime import datetime\nfrom io import UnsupportedOperation\nimport numpy as np\nfrom nptdms.common import toc_properties, ObjectPath\nfrom nptdms.types import *\n\n\ntry:\n    long\nexcept NameError:\n    # Python 3\n    long = int\n    unicode = str\n\n\nclass TdmsWriter(object):\n    """"""Writes to a TDMS file.\n\n    A TdmsWriter should be used as a context manager, for example::\n\n        with TdmsWriter(path) as tdms_writer:\n            tdms_writer.write_segment(segment_data)\n    """"""\n\n    def __init__(self, file, mode=\'w\'):\n        """"""Initialise a new TDMS writer\n\n        :param file: Either the path to the tdms file to open or an already\n            opened file.\n        :param mode: Either \'w\' to open a new file or \'a\' to append to an\n            existing TDMS file.\n        """"""\n        self._file = None\n        self._file_path = None\n        self._file_mode = mode\n\n        if hasattr(file, ""read""):\n            # Is a file\n            self._file = file\n        else:\n            self._file_path = file\n\n    def open(self):\n        if self._file_path is not None:\n            self._file = open(self._file_path, self._file_mode + \'b\')\n\n    def close(self):\n        if self._file_path is not None:\n            self._file.close()\n        self._file = None\n\n    def write_segment(self, objects):\n        """""" Write a segment of data to a TDMS file\n\n        :param objects: A list of TdmsObject instances to write\n        """"""\n        segment = TdmsSegment(objects)\n        segment.write(self._file)\n\n    def __enter__(self):\n        self.open()\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        self.close()\n\n\nclass TdmsSegment(object):\n    """"""A segment of data to be written to a file\n    """"""\n\n    def __init__(self, objects):\n        """"""Initialise a new segment of TDMS data\n\n        :param objects: A list of TdmsObject instances.\n        """"""\n        paths = set(obj.path for obj in objects)\n        if len(paths) != len(objects):\n            raise ValueError(""Duplicate object paths found"")\n\n        self.objects = objects\n\n    def write(self, file):\n        metadata = self.metadata()\n        metadata_size = sum(len(val.bytes) for val in metadata)\n\n        toc = [\'kTocMetaData\', \'kTocRawData\', \'kTocNewObjList\']\n        leadin = self.leadin(toc, metadata_size)\n\n        file.write(b\'\'.join(val.bytes for val in leadin))\n        file.write(b\'\'.join(val.bytes for val in metadata))\n        self._write_data(file)\n\n    def metadata(self):\n        metadata = []\n        metadata.append(Uint32(len(self.objects)))\n        for obj in self.objects:\n            metadata.append(String(obj.path))\n            metadata.extend(self.raw_data_index(obj))\n            properties = read_properties_dict(obj.properties)\n            num_properties = len(properties)\n            metadata.append(Uint32(num_properties))\n            for prop_name, prop_value in properties.items():\n                metadata.append(String(prop_name))\n                metadata.append(Int32(prop_value.enum_value))\n                metadata.append(prop_value)\n        return metadata\n\n    def raw_data_index(self, obj):\n        if hasattr(obj, \'data\'):\n            data_type = Int32(obj.data_type.enum_value)\n            dimension = Uint32(1)\n            num_values = Uint64(len(obj.data))\n\n            data_index = [Uint32(20), data_type, dimension, num_values]\n            # For strings, we also need to write the total data size in bytes\n            if obj.data_type == String:\n                total_size = object_data_size(obj.data_type, obj.data)\n                data_index.append(Uint64(total_size))\n\n            return data_index\n        else:\n            return [Bytes(b\'\\xFF\\xFF\\xFF\\xFF\')]\n\n    def leadin(self, toc, metadata_size):\n        leadin = []\n        leadin.append(Bytes(b\'TDSm\'))\n\n        toc_mask = long(0)\n        for toc_flag in toc:\n            toc_mask = toc_mask | toc_properties[toc_flag]\n        leadin.append(Int32(toc_mask))\n\n        tdms_version = 4712\n        leadin.append(Int32(tdms_version))\n\n        next_segment_offset = metadata_size + self._data_size()\n        raw_data_offset = metadata_size\n        leadin.append(Uint64(next_segment_offset))\n        leadin.append(Uint64(raw_data_offset))\n\n        return leadin\n\n    def _data_size(self):\n        data_size = 0\n        for obj in self.objects:\n            if hasattr(obj, \'data\'):\n                data_size += object_data_size(obj.data_type, obj.data)\n        return data_size\n\n    def _write_data(self, file):\n        for obj in self.objects:\n            if hasattr(obj, \'data\'):\n                write_data(file, obj)\n\n\nclass TdmsObject(object):\n    @property\n    def has_data(self):\n        return False\n\n    @property\n    def data_type(self):\n        return None\n\n    @property\n    def path(self):\n        return None\n\n\nclass RootObject(TdmsObject):\n    """"""The root TDMS object containing properties for the TDMS file\n    """"""\n    def __init__(self, properties=None):\n        """"""Initialise a new GroupObject\n\n        :param properties: A dictionary mapping property names to\n            their value.\n        """"""\n        self.properties = properties\n\n    @property\n    def path(self):\n        """"""The string representation of the root path\n        """"""\n        return ""/""\n\n\nclass GroupObject(TdmsObject):\n    """"""A TDMS object for a group\n    """"""\n\n    def __init__(self, group, properties=None):\n        """"""Initialise a new GroupObject\n\n        :param group: The name of this group.\n        :param properties: A dictionary mapping property names to\n            their value.\n        """"""\n        self.group = group\n        self.properties = properties\n\n    @property\n    def path(self):\n        """"""The string representation of this group\'s path\n        """"""\n        return str(ObjectPath(self.group))\n\n\nclass ChannelObject(TdmsObject):\n    """"""A TDMS object for a channel with data\n    """"""\n\n    def __init__(self, group, channel, data, properties=None):\n        """"""Initialise a new ChannelObject\n\n        :param group: The name of the group this channel is in.\n        :param channel: The name of this channel.\n        :param data: 1-D Numpy array of data to be written.\n        :param properties: A dictionary mapping property names to\n            their value.\n        """"""\n        self.group = group\n        self.channel = channel\n        self.data = _to_np_array(data)\n        self.properties = properties\n\n    @property\n    def has_data(self):\n        return True\n\n    @property\n    def data_type(self):\n        try:\n            return numpy_data_types[self.data.dtype]\n        except (AttributeError, KeyError):\n            try:\n                return _to_tdms_value(self.data[0]).__class__\n            except IndexError:\n                return Void\n\n    @property\n    def path(self):\n        """"""The string representation of this channel\'s path\n        """"""\n        return str(ObjectPath(self.group, self.channel))\n\n\ndef read_properties_dict(properties_dict):\n    if properties_dict is None:\n        return {}\n\n    return OrderedDict(\n        (key, _to_tdms_value(val))\n        for key, val in properties_dict.items())\n\n\ndef _to_tdms_value(value):\n    if isinstance(value, np.number):\n        return numpy_data_types[value.dtype](value)\n    if isinstance(value, TdmsType):\n        return value\n    if isinstance(value, bool) or isinstance(value, np.bool_):\n        return Boolean(value)\n    if isinstance(value, (int, long)):\n        return to_int_property_value(value)\n    if isinstance(value, float):\n        return DoubleFloat(value)\n    if isinstance(value, datetime):\n        return TimeStamp(value)\n    if isinstance(value, np.datetime64):\n        return TimeStamp(value)\n    if isinstance(value, str):\n        return String(value)\n    if isinstance(value, unicode):\n        return String(value)\n    if isinstance(value, bytes):\n        return String(value)\n    raise TypeError(""Unsupported property type for %r"" % value)\n\n\ndef to_int_property_value(value):\n    if value >= 2 ** 63:\n        return Uint64(value)\n    if value >= 2 ** 31 or value < -2 ** 31:\n        return Int64(value)\n    return Int32(value)\n\n\ndef write_data(file, tdms_object):\n    if tdms_object.data_type == TimeStamp:\n        # Numpy\'s datetime format isn\'t compatible with TDMS,\n        # so can\'t use data.tofile\n        write_values(file, tdms_object.data)\n    elif tdms_object.data_type == String:\n        # Strings are variable size so need to be treated specially\n        write_string_values(file, tdms_object.data)\n    else:\n        try:\n            to_file(file, tdms_object.data)\n        except (AttributeError):\n            # Need to also handle lists of data,\n            # to handle timestamp data for example.\n            write_values(file, tdms_object.data)\n\n\ndef to_file(file, array):\n    """"""Wrapper around ndarray.tofile to support any file-like object""""""\n\n    try:\n        array.tofile(file)\n    except (TypeError, IOError, UnsupportedOperation):\n        # tostring actually returns bytes\n        file.write(array.tostring())\n\n\ndef write_values(file, array):\n    file.write(b\'\'.join(_to_tdms_value(val).bytes for val in array))\n\n\ndef write_string_values(file, strings):\n    try:\n        encoded_strings = [s.encode(""utf-8"") for s in strings]\n    except AttributeError:\n        # Assume if we can\'t encode then we already have bytes\n        encoded_strings = strings\n    offset = 0\n    for s in encoded_strings:\n        offset += len(s)\n        file.write(Uint32(offset).bytes)\n    for s in encoded_strings:\n        file.write(s)\n\n\ndef object_data_size(data_type, data_values):\n    if data_type == String:\n        # For string data, the total size is 8 bytes per string for the\n        # offsets to the start of each string, plus the length of each string.\n        try:\n            encoded_strings = [s.encode(""utf-8"") for s in data_values]\n        except AttributeError:\n            encoded_strings = data_values\n        return sum(4 + len(s) for s in encoded_strings)\n\n    return data_type.size * len(data_values)\n\n\ndef _to_np_array(data):\n    if isinstance(data, np.ndarray):\n        return data\n\n    dtype = _infer_dtype(data)\n    return np.array(data, dtype=dtype)\n\n\ndef _infer_dtype(data):\n    if data and isinstance(data[0], (int, long)):\n        max_value = max(data)\n        min_value = min(data)\n        if max_value >= 2**63 and min_value >= 0:\n            return np.dtype(\'uint64\')\n        elif max_value >= 2**32 or min_value < -1 * 2**31:\n            return np.dtype(\'int64\')\n        elif max_value >= 2**31 and min_value >= 0:\n            return np.dtype(\'uint32\')\n        elif max_value >= 2**16 or min_value < -1 * 2**15:\n            return np.dtype(\'int32\')\n        elif max_value >= 2**15 and min_value >= 0:\n            return np.dtype(\'uint16\')\n        elif max_value >= 2**8 or min_value < -1 * 2**7:\n            return np.dtype(\'int16\')\n        elif max_value >= 2**7 and min_value >= 0:\n            return np.dtype(\'uint8\')\n        else:\n            return np.dtype(\'int8\')\n    return None\n'"
nptdms/export/__init__.py,0,b''
nptdms/export/hdf_export.py,3,"b'import numpy as np\nfrom nptdms import types\n\n\ndef from_tdms_file(tdms_file, filepath, mode=\'w\', group=\'/\'):\n    """"""\n    Converts the TDMS file into an HDF5 file\n\n    :param tdms_file: The TDMS file object to convert.\n    :param filepath: The path of the HDF5 file you want to write to.\n    :param mode: The write mode of the HDF5 file. This can be \'w\' or \'a\'\n    :param group: A group in the HDF5 file that will contain the TDMS data.\n    """"""\n    import h5py\n\n    # Groups in TDMS are mapped to the first level of the HDF5 hierarchy\n\n    # Channels in TDMS are then mapped to the second level of the HDF5\n    # hierarchy, under the appropriate groups.\n\n    # Properties in TDMS are mapped to attributes in HDF5.\n    # These all exist under the appropriate, channel group etc.\n\n    h5file = h5py.File(filepath, mode)\n\n    if group in h5file:\n        container_group = h5file[group]\n    else:\n        container_group = h5file.create_group(group)\n\n    # First write the properties at the root level\n    for property_name, property_value in tdms_file.properties.items():\n        container_group.attrs[property_name] = _hdf_attr_value(property_value)\n\n    # Now iterate through groups and channels,\n    # writing the properties and creating data sets\n    datasets = {}\n    for group in tdms_file.groups():\n        # Write the group\'s properties\n        container_group.create_group(group.name)\n        for prop_name, prop_value in group.properties.items():\n            container_group[group.name].attrs[prop_name] = _hdf_attr_value(prop_value)\n\n        # Write properties and data for each channel\n        for channel in group.channels():\n            channel_key = group.name + \'/\' + channel.name\n\n            if channel.data_type is types.String:\n                # Encode as variable length UTF-8 strings\n                datasets[channel.path] = container_group.create_dataset(\n                    channel_key, (len(channel),), dtype=h5py.string_dtype())\n            elif channel.data_type is types.TimeStamp:\n                # Timestamps are represented as fixed length ASCII strings\n                # because HDF doesn\'t natively support timestamps\n                datasets[channel.path] = container_group.create_dataset(\n                    channel_key, (len(channel),), dtype=\'S27\')\n            else:\n                datasets[channel.path] = container_group.create_dataset(\n                    channel_key, (len(channel),), dtype=channel.dtype)\n\n            for prop_name, prop_value in channel.properties.items():\n                container_group[channel_key].attrs[prop_name] = _hdf_attr_value(prop_value)\n\n    # Set data\n    if tdms_file.data_read:\n        for group in tdms_file.groups():\n            for channel in group.channels():\n                datasets[channel.path][...] = _hdf_array(channel, channel.data)\n    else:\n        # Data hasn\'t been read into memory, stream it from disk\n        for chunk in tdms_file.data_chunks():\n            for group in chunk.groups():\n                for channel_chunk in group.channels():\n                    channel = tdms_file[group.name][channel_chunk.name]\n                    offset = channel_chunk.offset\n                    end = offset + len(channel_chunk)\n                    datasets[channel.path][offset:end] = _hdf_array(channel, channel_chunk[:])\n\n    return h5file\n\n\ndef _hdf_array(channel, data):\n    """""" Convert data array into a format suitable for initialising HDF data\n    """"""\n    if channel.data_type is types.TimeStamp:\n        string_data = np.datetime_as_string(data, unit=\'us\', timezone=\'UTC\')\n        return [s.encode(\'ascii\') for s in string_data]\n    return data\n\n\ndef _hdf_attr_value(value):\n    """""" Convert a value into a format suitable for an HDF attribute\n    """"""\n    if isinstance(value, np.datetime64):\n        return np.string_(np.datetime_as_string(value, unit=\'us\', timezone=\'UTC\'))\n    return value\n'"
nptdms/export/pandas_export.py,2,"b'import numpy as np\nfrom nptdms.utils import OrderedDict\n\n\ndef from_tdms_file(tdms_file, time_index=False, absolute_time=False, scaled_data=True):\n    """"""\n    Converts the TDMS file to a DataFrame. DataFrame columns are named using the TDMS object paths.\n\n    :param tdms_file: TDMS file object to convert.\n    :param time_index: Whether to include a time index for the dataframe.\n    :param absolute_time: If time_index is true, whether the time index\n        values are absolute times or relative to the start time.\n    :param scaled_data: By default the scaled data will be used.\n        Set to False to use raw unscaled data.\n    :return: The full TDMS file data.\n    :rtype: pandas.DataFrame\n    """"""\n\n    channels_to_export = OrderedDict()\n    for group in tdms_file.groups():\n        for channel in group.channels():\n            channels_to_export[channel.path] = channel\n    return _channels_to_dataframe(channels_to_export, time_index, absolute_time, scaled_data)\n\n\ndef from_group(group, time_index=False, absolute_time=False, scaled_data=True):\n    """"""\n    Converts a TDMS group object to a DataFrame. DataFrame columns are named using the channel names.\n\n    :param group: Group object to convert.\n    :param time_index: Whether to include a time index for the dataframe.\n    :param absolute_time: If time_index is true, whether the time index\n        values are absolute times or relative to the start time.\n    :param scaled_data: By default the scaled data will be used.\n        Set to False to use raw unscaled data.\n    :return: The TDMS object data.\n    :rtype: pandas.DataFrame\n    """"""\n\n    channels_to_export = OrderedDict((ch.name, ch) for ch in group.channels())\n    return _channels_to_dataframe(channels_to_export, time_index, absolute_time, scaled_data)\n\n\ndef from_channel(channel, time_index=False, absolute_time=False, scaled_data=True):\n    """"""\n    Converts the TDMS channel to a DataFrame\n\n    :param channel: Channel object to convert.\n    :param time_index: Whether to include a time index for the dataframe.\n    :param absolute_time: If time_index is true, whether the time index\n        values are absolute times or relative to the start time.\n    :param scaled_data: By default the scaled data will be used.\n        Set to False to use raw unscaled data.\n    :return: The TDMS object data.\n    :rtype: pandas.DataFrame\n    """"""\n\n    channels_to_export = {channel.path: channel}\n    return _channels_to_dataframe(channels_to_export, time_index, absolute_time, scaled_data)\n\n\ndef _channels_to_dataframe(channels_to_export, time_index=False, absolute_time=False, scaled_data=True):\n    import pandas as pd\n\n    dataframe_dict = OrderedDict()\n    for column_name, channel in channels_to_export.items():\n        index = channel.time_track(absolute_time) if time_index else None\n        if scaled_data:\n            dataframe_dict[column_name] = pd.Series(data=_array_for_pd(channel.data), index=index)\n        elif channel.scaler_data_types:\n            # Channel has DAQmx raw data\n            for scale_id, raw_data in channel.raw_scaler_data.items():\n                scaler_column_name = column_name + ""[{0:d}]"".format(scale_id)\n                dataframe_dict[scaler_column_name] = pd.Series(data=raw_data, index=index)\n        else:\n            # Raw data for normal TDMS file\n            dataframe_dict[column_name] = pd.Series(data=_array_for_pd(channel.raw_data), index=index)\n    return pd.DataFrame.from_dict(dataframe_dict)\n\n\ndef _array_for_pd(array):\n    """""" Convert data array to a format suitable for a Pandas dataframe\n    """"""\n    if np.issubdtype(array.dtype, np.dtype(\'void\')):\n        # If dtype is void then the array must also be empty.\n        # Pandas doesn\'t like void data types, so these are converted to empty float64 arrays\n        # and Pandas will fill values with NaN\n        return np.empty(0, dtype=\'float64\')\n    return array\n'"
nptdms/test/__init__.py,0,b''
nptdms/test/scenarios.py,72,"b'"""""" Contains different test cases for tests for reading TDMS files\n""""""\nimport numpy as np\nimport pytest\n\nfrom nptdms.test.util import (\n    GeneratedFile,\n    hexlify_value,\n    string_hexlify,\n    segment_objects_metadata,\n    channel_metadata,\n    channel_metadata_with_no_data,\n    channel_metadata_with_repeated_structure)\n\n\nTDS_TYPE_INT8 = 1\nTDS_TYPE_INT16 = 2\nTDS_TYPE_INT32 = 3\nTDS_TYPE_BOOL = 0x21\nTDS_TYPE_COMPLEX64 = 0x08000c\nTDS_TYPE_COMPLEX128 = 0x10000d\nTDS_TYPE_FLOAT32 = 9\nTDS_TYPE_FLOAT64 = 10\nTDS_TYPE_FLOAT32_WITH_UNIT = 0x19\nTDS_TYPE_FLOAT64_WITH_UNIT = 0x1A\n\n\n_scenarios = []\n\n\ndef scenario(func):\n    def as_param():\n        result = func()\n        return pytest.param(*result, id=func.__name__)\n    _scenarios.append(as_param)\n    return as_param\n\n\ndef get_scenarios():\n    return [f() for f in _scenarios]\n\n\n@scenario\ndef single_segment_with_one_channel():\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef single_segment_with_two_channels():\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([3, 4], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef single_segment_with_two_channels_interleaved():\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"", ""kTocInterleavedData""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 3], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([2, 4], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef single_segment_with_interleaved_data_of_different_width():\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"", ""kTocInterleavedData""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT8, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT16, 2),\n            channel_metadata(""/\'group\'/\'channel3\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01"" ""01 00"" ""01 00 00 00""\n        ""02"" ""02 00"" ""02 00 00 00""\n        ""03"" ""03 00"" ""03 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2, 3], dtype=np.int8),\n        (\'group\', \'channel2\'): np.array([1, 2, 3], dtype=np.int16),\n        (\'group\', \'channel3\'): np.array([1, 2, 3], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef no_metadata_segment():\n    """""" Add a segment with two channels, then a second\n        segment with the same metadata as before,\n        so there is only the lead in and binary data\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocRawData"", ),\n        """",\n        ""05 00 00 00"" ""06 00 00 00""\n        ""07 00 00 00"" ""08 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2, 5, 6], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([3, 4, 7, 8], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef object_with_no_metadata_in_segment():\n    """""" Re-use an object without setting any new metadata and\n        re-using the data structure\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata_with_repeated_structure(""/\'group\'/\'channel1\'""),\n            channel_metadata_with_repeated_structure(""/\'group\'/\'channel2\'""),\n        ),\n        ""05 00 00 00"" ""06 00 00 00""\n        ""07 00 00 00"" ""08 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2, 5, 6], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([3, 4, 7, 8], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef add_new_channel():\n    """""" Add a new voltage channel, with the other two channels\n        remaining unchanged, so only the new channel is in metadata section\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel3\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""05 00 00 00"" ""06 00 00 00""\n        ""07 00 00 00"" ""08 00 00 00""\n        ""09 00 00 00"" ""0A 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2, 5, 6], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([3, 4, 7, 8], dtype=np.int32),\n        (\'group\', \'channel3\'): np.array([9, 10], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef repeated_objects_without_data_in_segment_and_added_object():\n    """""" Repeated objects with no data in new segment as well as a new channel with data\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata_with_no_data(""/\'group\'/\'channel1\'""),\n            channel_metadata_with_no_data(""/\'group\'/\'channel2\'""),\n            channel_metadata(""/\'group\'/\'channel3\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""05 00 00 00"" ""06 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([3, 4], dtype=np.int32),\n        (\'group\', \'channel3\'): np.array([5, 6], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef increase_channel_size():\n    """""" In the second segment, increase the channel size of one channel\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 4),\n        ),\n        ""05 00 00 00"" ""06 00 00 00""\n        ""07 00 00 00"" ""08 00 00 00""\n        ""09 00 00 00"" ""0A 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2, 5, 6], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([3, 4, 7, 8, 9, 10], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef remove_a_channel():\n    """""" In the second segment, remove a channel.\n        We need to write a new object list in this case\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""05 00 00 00"" ""06 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2, 5, 6], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([3, 4], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef alternating_data_objects_with_new_obj_list():\n    """""" Alternating segments with different objects with data\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata_with_no_data(""/\'group\'/\'channel2\'""),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata_with_no_data(""/\'group\'/\'channel1\'""),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata_with_repeated_structure(""/\'group\'/\'channel1\'""),\n            channel_metadata_with_no_data(""/\'group\'/\'channel2\'""),\n        ),\n        ""05 00 00 00"" ""06 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata_with_no_data(""/\'group\'/\'channel1\'""),\n            channel_metadata_with_repeated_structure(""/\'group\'/\'channel2\'""),\n        ),\n        ""07 00 00 00"" ""08 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2, 5, 6], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([3, 4, 7, 8], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef alternating_data_objects_reusing_obj_list():\n    """""" Alternating segments with different objects with data,\n        reusing the object list from the last segment.\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata_with_no_data(""/\'group\'/\'channel2\'""),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData""),\n        segment_objects_metadata(\n            channel_metadata_with_no_data(""/\'group\'/\'channel1\'""),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData""),\n        segment_objects_metadata(\n            channel_metadata_with_repeated_structure(""/\'group\'/\'channel1\'""),\n            channel_metadata_with_no_data(""/\'group\'/\'channel2\'""),\n        ),\n        ""05 00 00 00"" ""06 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData""),\n        segment_objects_metadata(\n            channel_metadata_with_no_data(""/\'group\'/\'channel1\'""),\n            channel_metadata_with_repeated_structure(""/\'group\'/\'channel2\'""),\n        ),\n        ""07 00 00 00"" ""08 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2, 5, 6], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([3, 4, 7, 8], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef chunked_segment():\n    """""" Add segment and then a repeated segment without\n        any lead in or metadata, so data is read in chunks\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""00 00 00 00"" ""01 00 00 00""\n        ""0A 00 00 00"" ""0B 00 00 00""\n        ""02 00 00 00"" ""03 00 00 00""\n        ""0C 00 00 00"" ""0D 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocRawData"", ),\n        """",\n        ""04 00 00 00"" ""05 00 00 00""\n        ""0E 00 00 00"" ""0F 00 00 00""\n        ""06 00 00 00"" ""07 00 00 00""\n        ""10 00 00 00"" ""11 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([0, 1, 2, 3, 4, 5, 6, 7], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([10, 11, 12, 13, 14, 15, 16, 17], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef chunked_interleaved_segment():\n    """""" Add interleaved segment and then a repeated segment without\n        any lead in or metadata, so data is read in chunks\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"", ""kTocInterleavedData""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n        ""05 00 00 00"" ""06 00 00 00""\n        ""07 00 00 00"" ""08 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocRawData"", ""kTocInterleavedData""),\n        """",\n        ""07 00 00 00"" ""08 00 00 00""\n        ""05 00 00 00"" ""06 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n        ""01 00 00 00"" ""02 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 3, 5, 7, 7, 5, 3, 1], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([2, 4, 6, 8, 8, 6, 4, 2], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef less_data_than_expected():\n    """""" Add segment and then a repeated segment without\n        any lead in or metadata, so data is read in chunks,\n        but the extra chunk does not have as much data as expected.\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n        ""05 00 00 00"" ""06 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2, 5], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([3, 4, 6], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef less_data_than_expected_interleaved():\n    """""" Add interleaved data segment and then a repeated segment without\n        any lead in or metadata, so data is read in chunks,\n        but the extra chunk does not have as much data as expected.\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"", ""kTocInterleavedData""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 3),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 3),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n        ""05 00 00 00"" ""06 00 00 00""\n        ""07 00 00 00"" ""08 00 00 00""\n        ""09 00 00 00"" ""0A 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 3, 5, 7, 9], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([2, 4, 6, 8, 10], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef segment_with_zero_data_length():\n    """""" kTocRawData is set but data length is zero\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 0),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 0),\n        ),\n        """"\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([3, 4], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef incomplete_last_segment():\n    """""" Test incomplete last segment, eg. if LabView crashed\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n        ""05 00 00 00"" ""06 00 00 00""\n        ""07 00 00 00"" ""08 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocRawData"", ),\n        """",\n        ""09 00 00 00"" ""0A 00 00 00""\n        ""0B 00 00 00"" ""0C 00 00 00"",\n        incomplete=True\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2, 5, 6, 9, 10], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([3, 4, 7, 8, 11, 12], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef incomplete_last_row_of_interleaved_data():\n    """""" Test incomplete last row of interleaved data\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"", ""kTocInterleavedData""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n        ""05 00 00 00"" ""06 00 00 00""\n        ""07 00 00 00"" ""08 00 00 00""\n        ""09 00 00 00"" ""0A 00 00 00""\n        ""0B 00 00 00"",\n        incomplete=True\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 3, 5, 7, 9], dtype=np.int32),\n        (\'group\', \'channel2\'): np.array([2, 4, 6, 8, 10], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef bool_data():\n    """""" Test reading a file with boolean valued data\n    """"""\n    expected_channel_data = np.array([False, True, False, True], dtype=np.dtype(\'bool8\'))\n\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'bool_channel\'"", TDS_TYPE_BOOL, 2),\n        ),\n        ""00 01 00 01""\n    )\n    expected_data = {\n        (\'group\', \'bool_channel\'): expected_channel_data,\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef float_data():\n    """""" Test reading a file with float valued data\n    """"""\n    single_arr = np.array([0.123, 0.234, 0.345, 0.456], dtype=np.float32)\n    double_arr = np.array([0.987, 0.876, 0.765, 0.654], dtype=np.double)\n    data = """"\n    for num in single_arr[0:2]:\n        data += hexlify_value(""<f"", num)\n    for num in double_arr[0:2]:\n        data += hexlify_value(""<d"", num)\n    for num in single_arr[2:4]:\n        data += hexlify_value(""<f"", num)\n    for num in double_arr[2:4]:\n        data += hexlify_value(""<d"", num)\n\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'single_channel\'"", TDS_TYPE_FLOAT32, 2),\n            channel_metadata(""/\'group\'/\'double_channel\'"", TDS_TYPE_FLOAT64, 2),\n        ),\n        data\n    )\n    expected_data = {\n        (\'group\', \'single_channel\'): single_arr,\n        (\'group\', \'double_channel\'): double_arr,\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef float_data_with_unit():\n    """""" Test reading a file with float valued data with units\n\n        These are the same as normal floating point data but have a \'unit_string\' property\n    """"""\n    single_arr = np.array([0.123, 0.234, 0.345, 0.456], dtype=np.float32)\n    double_arr = np.array([0.987, 0.876, 0.765, 0.654], dtype=np.double)\n    data = """"\n    for num in single_arr[0:2]:\n        data += hexlify_value(""<f"", num)\n    for num in double_arr[0:2]:\n        data += hexlify_value(""<d"", num)\n    for num in single_arr[2:4]:\n        data += hexlify_value(""<f"", num)\n    for num in double_arr[2:4]:\n        data += hexlify_value(""<d"", num)\n\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'single_channel\'"", TDS_TYPE_FLOAT32_WITH_UNIT, 2),\n            channel_metadata(""/\'group\'/\'double_channel\'"", TDS_TYPE_FLOAT64_WITH_UNIT, 2),\n        ),\n        data\n    )\n    expected_data = {\n        (\'group\', \'single_channel\'): single_arr,\n        (\'group\', \'double_channel\'): double_arr,\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef complex_data():\n    """""" Test reading a file with complex valued data\n    """"""\n    complex_single_arr = np.array([1+2j, 3+4j], dtype=np.complex64)\n    complex_double_arr = np.array([5+6j, 7+8j], dtype=np.complex128)\n    data = """"\n    for num in complex_single_arr:\n        data += hexlify_value(""<f"", num.real)\n        data += hexlify_value(""<f"", num.imag)\n    for num in complex_double_arr:\n        data += hexlify_value(""<d"", num.real)\n        data += hexlify_value(""<d"", num.imag)\n\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'complex_single_channel\'"", TDS_TYPE_COMPLEX64, 2),\n            channel_metadata(""/\'group\'/\'complex_double_channel\'"", TDS_TYPE_COMPLEX128, 2),\n        ),\n        data\n    )\n    expected_data = {\n        (\'group\', \'complex_single_channel\'): complex_single_arr,\n        (\'group\', \'complex_double_channel\'): complex_double_arr,\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef scaled_data():\n    properties = {\n        ""NI_Number_Of_Scales"":\n            (3, ""01 00 00 00""),\n        ""NI_Scale[0]_Scale_Type"":\n            (0x20, hexlify_value(""<I"", len(""Linear"")) + string_hexlify(""Linear"")),\n        ""NI_Scale[0]_Linear_Slope"":\n            (10, hexlify_value(""<d"", 2.0)),\n        ""NI_Scale[0]_Linear_Y_Intercept"":\n            (10, hexlify_value(""<d"", 10.0))\n    }\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2, properties),\n        ),\n        ""01 00 00 00"" ""02 00 00 00"" ""03 00 00 00"" ""04 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([12, 14, 16, 18], dtype=np.float64),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef timestamp_data():\n    """"""Test reading contiguous timestamp data\n    """"""\n\n    times = [\n        np.datetime64(\'2012-08-23T00:00:00.123\', \'us\'),\n        np.datetime64(\'2012-08-23T01:02:03.456\', \'us\'),\n        np.datetime64(\'2012-08-23T12:00:00.0\', \'us\'),\n        np.datetime64(\'2012-08-23T12:02:03.9999\', \'us\'),\n    ]\n\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'Group\'/\'TimeChannel1\'"", 0x44, 2),\n            channel_metadata(""/\'Group\'/\'TimeChannel2\'"", 0x44, 2),\n        ),\n        timestamp_data_chunk(times)\n    )\n\n    expected_data = {\n        (\'Group\', \'TimeChannel1\'): np.array([times[0], times[1]]),\n        (\'Group\', \'TimeChannel2\'): np.array([times[2], times[3]]),\n    }\n\n    return test_file, expected_data\n\n\n@scenario\ndef interleaved_timestamp_data():\n    """"""Test reading interleaved timestamp data\n    """"""\n\n    times = [\n        np.datetime64(\'2012-08-23T00:00:00.123\', \'us\'),\n        np.datetime64(\'2012-08-23T01:02:03.456\', \'us\'),\n        np.datetime64(\'2012-08-23T12:00:00.0\', \'us\'),\n        np.datetime64(\'2012-08-23T12:02:03.9999\', \'us\'),\n    ]\n\n    metadata = (\n        # Number of objects\n        ""02 00 00 00""\n        # Length of the object path\n        ""17 00 00 00"")\n    metadata += string_hexlify(""/\'Group\'/\'TimeChannel1\'"")\n    metadata += (\n        # Length of index information\n        ""14 00 00 00""\n        # Raw data data type\n        ""44 00 00 00""\n        # Dimension\n        ""01 00 00 00""\n        # Number of raw data values\n        ""02 00 00 00""\n        ""00 00 00 00""\n        # Number of properties (0)\n        ""00 00 00 00"")\n    metadata += (\n        ""17 00 00 00"")\n    metadata += string_hexlify(""/\'Group\'/\'TimeChannel2\'"")\n    metadata += (\n        # Length of index information\n        ""14 00 00 00""\n        # Raw data data type\n        ""44 00 00 00""\n        # Dimension\n        ""01 00 00 00""\n        # Number of raw data values\n        ""02 00 00 00""\n        ""00 00 00 00""\n        # Number of properties (0)\n        ""00 00 00 00"")\n\n    test_file = GeneratedFile()\n    toc = (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"", ""kTocInterleavedData"")\n    test_file.add_segment(toc, metadata, timestamp_data_chunk(times))\n\n    expected_data = {\n        (\'Group\', \'TimeChannel1\'): np.array([times[0], times[2]]),\n        (\'Group\', \'TimeChannel2\'): np.array([times[1], times[3]]),\n    }\n\n    return test_file, expected_data\n\n\n@scenario\ndef interleaved_timestamp_and_numpy_data():\n    """"""Test reading timestamp data interleaved with a standard numpy data type\n    """"""\n\n    times = [\n        np.datetime64(\'2012-08-23T00:00:00.123\', \'us\'),\n        np.datetime64(\'2012-08-23T01:02:03.456\', \'us\'),\n    ]\n\n    metadata = (\n        # Number of objects\n        ""02 00 00 00""\n        # Length of the object path\n        ""16 00 00 00"")\n    metadata += string_hexlify(""/\'Group\'/\'TimeChannel\'"")\n    metadata += (\n        # Length of index information\n        ""14 00 00 00""\n        # Raw data data type\n        ""44 00 00 00""\n        # Dimension\n        ""01 00 00 00""\n        # Number of raw data values\n        ""02 00 00 00""\n        ""00 00 00 00""\n        # Number of properties (0)\n        ""00 00 00 00"")\n    metadata += (\n        ""15 00 00 00"")\n    metadata += string_hexlify(""/\'Group\'/\'IntChannel\'"")\n    metadata += (\n        # Length of index information\n        ""14 00 00 00""\n        # Raw data data type\n        ""03 00 00 00""\n        # Dimension\n        ""01 00 00 00""\n        # Number of raw data values\n        ""02 00 00 00""\n        ""00 00 00 00""\n        # Number of properties (0)\n        ""00 00 00 00"")\n\n    data = (timestamp_data_chunk([times[0]]) +\n            ""01 00 00 00"" +\n            timestamp_data_chunk([times[1]]) +\n            ""02 00 00 00"")\n\n    test_file = GeneratedFile()\n    toc = (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"", ""kTocInterleavedData"")\n    test_file.add_segment(toc, metadata, data)\n\n    expected_data = {\n        (\'Group\', \'TimeChannel\'): np.array([times[0], times[1]]),\n        (\'Group\', \'IntChannel\'): np.array([1, 2], dtype=np.dtype(\'int32\')),\n    }\n\n    return test_file, expected_data\n\n\n@scenario\ndef segment_without_data():\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n        ),\n        """"\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocNewObjList"", ""kTocRawData""),\n        segment_objects_metadata(\n            channel_metadata_with_repeated_structure(""/\'group\'/\'channel1\'""),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata_with_repeated_structure(""/\'group\'/\'channel1\'""),\n        ),\n        """"\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocNewObjList"", ""kTocRawData""),\n        segment_objects_metadata(\n            channel_metadata_with_repeated_structure(""/\'group\'/\'channel1\'""),\n        ),\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2, 3, 4], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef channel_without_data():\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n        ),\n        """"\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([], dtype=np.dtype(\'int32\')),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef channel_without_data_or_data_type():\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata_with_no_data(""/\'group\'/\'channel1\'""),\n        ),\n        """"\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([], dtype=np.dtype(\'void\')),\n    }\n    return test_file, expected_data\n\n\n@scenario\ndef extra_padding_after_metadata():\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n        ) + ""00 00 00 00 00 00 00 00"",\n        ""01 00 00 00"" ""02 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 2),\n        ) + ""00 00 00 00 00 00 00 00"",\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n    expected_data = {\n        (\'group\', \'channel1\'): np.array([1, 2, 3, 4], dtype=np.int32),\n    }\n    return test_file, expected_data\n\n\ndef timestamp_data_chunk(times):\n    epoch = np.datetime64(\'1904-01-01T00:00:00\')\n\n    def total_seconds(td):\n        return int(td / np.timedelta64(1, \'s\'))\n\n    def microseconds(dt):\n        diff = dt - epoch\n        secs = total_seconds(diff)\n        remainder = diff - np.timedelta64(secs, \'s\')\n        return int(remainder / np.timedelta64(1, \'us\'))\n\n    seconds = [total_seconds(t - epoch) for t in times]\n    fractions = [\n        int(float(microseconds(t)) * 2 ** 58 / 5 ** 6)\n        for t in times]\n\n    data = """"\n    for f, s in zip(fractions, seconds):\n        data += hexlify_value(""<Q"", f)\n        data += hexlify_value(""<q"", s)\n    return data\n'"
nptdms/test/test_benchmarks.py,56,"b'import os\nimport numpy as np\nimport pytest\n\nfrom nptdms import TdmsFile\nfrom nptdms.test.util import (\n    GeneratedFile,\n    hexlify_value,\n    string_hexlify,\n    segment_objects_metadata,\n    channel_metadata,\n    channel_metadata_with_no_data,\n    channel_metadata_with_repeated_structure)\nfrom nptdms.test.scenarios import TDS_TYPE_INT32, timestamp_data_chunk\n\n\n@pytest.mark.benchmark(group=\'read-all-data\')\ndef test_read_contiguous_data(benchmark):\n    """""" Benchmark reading a file with multiple channels of contiguous data\n    """"""\n    tdms_file = benchmark(read_from_start, get_contiguous_file().get_bytes_io_file())\n\n    np.testing.assert_equal(tdms_file[\'group\'][\'channel1\'][:], np.repeat([1], 10000))\n    np.testing.assert_equal(tdms_file[\'group\'][\'channel2\'][:], np.repeat([2], 10000))\n    np.testing.assert_equal(tdms_file[\'group\'][\'channel3\'][:], np.repeat([3], 10000))\n    np.testing.assert_equal(tdms_file[\'group\'][\'channel4\'][:], np.repeat([4], 10000))\n\n\n@pytest.mark.benchmark(group=\'read-all-data\')\ndef test_read_interleaved_data(benchmark):\n    """""" Benchmark reading a file with interleaved data\n    """"""\n    tdms_file = benchmark(read_from_start, get_interleaved_file().get_bytes_io_file())\n\n    np.testing.assert_equal(tdms_file[\'group\'][\'channel1\'][:], np.repeat([1], 10000))\n    np.testing.assert_equal(tdms_file[\'group\'][\'channel2\'][:], np.repeat([2], 10000))\n    np.testing.assert_equal(tdms_file[\'group\'][\'channel3\'][:], np.repeat([3], 10000))\n    np.testing.assert_equal(tdms_file[\'group\'][\'channel4\'][:], np.repeat([4], 10000))\n\n\n@pytest.mark.benchmark(group=\'read-all-channel\')\ndef test_read_contiguous_data_channel(benchmark):\n    """""" Benchmark reading a single channel\'s data from a contiguous data file\n    """"""\n    with TdmsFile.open(get_contiguous_file().get_bytes_io_file()) as tdms_file:\n        channel = tdms_file[\'group\'][\'channel3\']\n        channel_data = benchmark(read_channel_data, channel)\n\n        expected_data = np.repeat([3], 10000)\n        np.testing.assert_equal(channel_data, expected_data)\n\n\n@pytest.mark.benchmark(group=\'read-all-channel\')\ndef test_read_interleaved_data_channel(benchmark):\n    """""" Benchmark reading a single channel\'s data from an interleaved data file\n    """"""\n    with TdmsFile.open(get_interleaved_file().get_bytes_io_file()) as tdms_file:\n        channel = tdms_file[\'group\'][\'channel3\']\n        channel_data = benchmark(read_channel_data, channel)\n\n        expected_data = np.repeat([3], 10000)\n        np.testing.assert_equal(channel_data, expected_data)\n\n\n@pytest.mark.benchmark(group=\'read-all-channel\')\ndef test_stream_contiguous_data_channel(benchmark):\n    """""" Benchmark streaming channel data from a contiguous data file\n    """"""\n    with TdmsFile.open(get_contiguous_file().get_bytes_io_file()) as tdms_file:\n        channel = tdms_file[\'group\'][\'channel3\']\n        channel_data = benchmark(stream_chunks, channel)\n\n        channel_data = np.concatenate(channel_data)\n        expected_data = np.repeat([3], 10000)\n        np.testing.assert_equal(channel_data, expected_data)\n\n\n@pytest.mark.benchmark(group=\'read-all-channel\')\ndef test_stream_interleaved_data_channel(benchmark):\n    """""" Benchmark streaming channel data from an interleaved data file\n    """"""\n    with TdmsFile.open(get_interleaved_file().get_bytes_io_file()) as tdms_file:\n        channel = tdms_file[\'group\'][\'channel3\']\n        channel_data = benchmark(stream_chunks, channel)\n\n        channel_data = np.concatenate(channel_data)\n        expected_data = np.repeat([3], 10000)\n        np.testing.assert_equal(channel_data, expected_data)\n\n\n@pytest.mark.benchmark(group=\'slice-channel\')\ndef test_slice_contiguous_data_channel(benchmark):\n    """""" Benchmark reading a slice of data from a contiguous data file\n    """"""\n    with TdmsFile.open(get_contiguous_file().get_bytes_io_file()) as tdms_file:\n        channel = tdms_file[\'group\'][\'channel3\']\n        channel_data = benchmark(get_slice, channel, 5555, 6555)\n\n        expected_data = np.repeat([3], 1000)\n        np.testing.assert_equal(channel_data, expected_data)\n\n\n@pytest.mark.benchmark(group=\'slice-channel\')\ndef test_slice_interleaved_data_channel(benchmark):\n    """""" Benchmark reading a slice of data from an interleaved data file\n    """"""\n    with TdmsFile.open(get_interleaved_file().get_bytes_io_file()) as tdms_file:\n        channel = tdms_file[\'group\'][\'channel3\']\n        channel_data = benchmark(get_slice, channel, 5555, 6555)\n\n        expected_data = np.repeat([3], 1000)\n        np.testing.assert_equal(channel_data, expected_data)\n\n\n@pytest.mark.benchmark(group=\'read-all-channel\')\ndef test_index_contiguous_data_channel(benchmark):\n    """""" Benchmark reading a data from a contiguous data file using integer indices\n    """"""\n    with TdmsFile.open(get_contiguous_file().get_bytes_io_file()) as tdms_file:\n        channel = tdms_file[\'group\'][\'channel3\']\n        channel_data = np.zeros(10000, dtype=channel.dtype)\n        benchmark(index_values, channel, channel_data)\n\n        expected_data = np.repeat([3], 10000)\n        np.testing.assert_equal(channel_data, expected_data)\n\n\n@pytest.mark.benchmark(group=\'read-all-channel\')\ndef test_index_interleaved_data_channel(benchmark):\n    """""" Benchmark reading a data from a interleaved data file using integer indices\n    """"""\n    with TdmsFile.open(get_interleaved_file().get_bytes_io_file()) as tdms_file:\n        channel = tdms_file[\'group\'][\'channel3\']\n        channel_data = np.zeros(10000, dtype=channel.dtype)\n        benchmark(index_values, channel, channel_data)\n\n        expected_data = np.repeat([3], 10000)\n        np.testing.assert_equal(channel_data, expected_data)\n\n\n@pytest.mark.benchmark(group=\'read-scaled-channel\')\ndef test_stream_scaled_data_chunks(benchmark):\n    """""" Benchmark streaming channel data when the data is scaled\n    """"""\n    properties = {\n        ""NI_Number_Of_Scales"":\n            (3, ""01 00 00 00""),\n        ""NI_Scale[0]_Scale_Type"":\n            (0x20, hexlify_value(""<I"", len(""Linear"")) + string_hexlify(""Linear"")),\n        ""NI_Scale[0]_Linear_Slope"":\n            (10, hexlify_value(""<d"", 2.0)),\n        ""NI_Scale[0]_Linear_Y_Intercept"":\n            (10, hexlify_value(""<d"", 10.0))\n    }\n    test_file = GeneratedFile()\n    data_array = np.arange(0, 1000, dtype=np.dtype(\'int32\'))\n    data = data_array.tobytes()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 100, properties),\n        ),\n        data, binary_data=True\n    )\n    for _ in range(0, 9):\n        test_file.add_segment(\n            (""kTocRawData"", ), """", data, binary_data=True)\n\n    with TdmsFile.open(test_file.get_bytes_io_file()) as tdms_file:\n        channel = tdms_file[\'group\'][\'channel1\']\n        channel_data = benchmark(stream_chunks, channel)\n\n        channel_data = np.concatenate(channel_data)\n        expected_data = np.tile(10.0 + 2.0 * data_array, 10)\n        np.testing.assert_equal(channel_data, expected_data)\n\n\n@pytest.mark.benchmark(group=\'read-timestamp-data\')\ndef test_read_timestamp_data(benchmark):\n    """""" Benchmark reading a file with timestamp data\n    """"""\n    timestamps = np.tile(np.array([\n        np.datetime64(\'2012-08-23T00:00:00.123\', \'us\'),\n        np.datetime64(\'2012-08-23T01:02:03.456\', \'us\'),\n        np.datetime64(\'2012-08-23T12:00:00.0\', \'us\'),\n        np.datetime64(\'2012-08-23T12:02:03.9999\', \'us\'),\n        np.datetime64(\'2012-08-23T12:02:03.9999\', \'us\'),\n    ]), 200)\n    data = timestamp_data_chunk(timestamps)\n\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", 0x44, 200, {}),\n        ),\n        data\n    )\n\n    tdms_file = benchmark(read_from_start, test_file.get_bytes_io_file())\n\n    np.testing.assert_equal(tdms_file[\'group\'][\'channel1\'][:], timestamps)\n\n\n@pytest.mark.benchmark(group=\'read-timestamp-data\')\ndef test_read_interleaved_timestamp_data(benchmark):\n    """""" Benchmark reading a file with interleaved timestamp data\n    """"""\n    timestamps = np.tile(np.array([\n        np.datetime64(\'2012-08-23T00:00:00.123\', \'us\'),\n        np.datetime64(\'2012-08-23T01:02:03.456\', \'us\'),\n        np.datetime64(\'2012-08-23T12:00:00.0\', \'us\'),\n        np.datetime64(\'2012-08-23T12:02:03.9999\', \'us\'),\n        np.datetime64(\'2012-08-23T12:02:03.9999\', \'us\'),\n        np.datetime64(\'2012-08-23T00:00:00.123\', \'us\'),\n        np.datetime64(\'2012-08-23T01:02:03.456\', \'us\'),\n        np.datetime64(\'2012-08-23T12:00:00.0\', \'us\'),\n        np.datetime64(\'2012-08-23T12:02:03.9999\', \'us\'),\n        np.datetime64(\'2012-08-23T12:02:03.9999\', \'us\'),\n    ]), 100)\n    data = timestamp_data_chunk(timestamps)\n\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"", ""kTocInterleavedData""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", 0x44, 100, {}),\n            channel_metadata(""/\'group\'/\'channel2\'"", 0x44, 100, {}),\n        ),\n        data\n    )\n\n    tdms_file = benchmark(read_from_start, test_file.get_bytes_io_file())\n\n    np.testing.assert_equal(tdms_file[\'group\'][\'channel1\'][:], timestamps[0::2])\n    np.testing.assert_equal(tdms_file[\'group\'][\'channel2\'][:], timestamps[1::2])\n\n\n@pytest.mark.benchmark(group=\'read-metadata\')\ndef test_complex_metadata_reading(benchmark):\n    """""" Benchmark reading metadata for a file with many channels and segments with alternating sets of objects\n    """"""\n    test_file = GeneratedFile()\n    data = np.array([0] * 5, dtype=np.dtype(\'int32\')).tobytes()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel0\'"", TDS_TYPE_INT32, 1),\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 1),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 1),\n            channel_metadata(""/\'group\'/\'channel3\'"", TDS_TYPE_INT32, 1),\n            channel_metadata(""/\'group\'/\'channel4\'"", TDS_TYPE_INT32, 1),\n        ),\n        data, binary_data=True\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel5\'"", TDS_TYPE_INT32, 1),\n            channel_metadata(""/\'group\'/\'channel6\'"", TDS_TYPE_INT32, 1),\n            channel_metadata(""/\'group\'/\'channel7\'"", TDS_TYPE_INT32, 1),\n            channel_metadata(""/\'group\'/\'channel8\'"", TDS_TYPE_INT32, 1),\n            channel_metadata(""/\'group\'/\'channel9\'"", TDS_TYPE_INT32, 1),\n        ),\n        data, binary_data=True\n    )\n    for _ in range(9):\n        test_file.add_segment(\n            (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n            segment_objects_metadata(\n                channel_metadata_with_no_data(""/\'group\'/\'channel0\'""),\n                channel_metadata_with_no_data(""/\'group\'/\'channel1\'""),\n                channel_metadata_with_no_data(""/\'group\'/\'channel2\'""),\n                channel_metadata_with_no_data(""/\'group\'/\'channel3\'""),\n                channel_metadata_with_no_data(""/\'group\'/\'channel4\'""),\n                channel_metadata_with_repeated_structure(""/\'group\'/\'channel5\'""),\n                channel_metadata_with_repeated_structure(""/\'group\'/\'channel6\'""),\n                channel_metadata_with_repeated_structure(""/\'group\'/\'channel7\'""),\n                channel_metadata_with_repeated_structure(""/\'group\'/\'channel8\'""),\n                channel_metadata_with_repeated_structure(""/\'group\'/\'channel9\'""),\n            ),\n            data, binary_data=True\n        )\n        test_file.add_segment(\n            (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n            segment_objects_metadata(\n                channel_metadata_with_repeated_structure(""/\'group\'/\'channel0\'""),\n                channel_metadata_with_repeated_structure(""/\'group\'/\'channel1\'""),\n                channel_metadata_with_repeated_structure(""/\'group\'/\'channel2\'""),\n                channel_metadata_with_repeated_structure(""/\'group\'/\'channel3\'""),\n                channel_metadata_with_repeated_structure(""/\'group\'/\'channel4\'""),\n                channel_metadata_with_no_data(""/\'group\'/\'channel5\'""),\n                channel_metadata_with_no_data(""/\'group\'/\'channel6\'""),\n                channel_metadata_with_no_data(""/\'group\'/\'channel7\'""),\n                channel_metadata_with_no_data(""/\'group\'/\'channel8\'""),\n                channel_metadata_with_no_data(""/\'group\'/\'channel9\'""),\n            ),\n            data, binary_data=True\n        )\n\n    tdms_file = benchmark(read_metadata_from_start, test_file.get_bytes_io_file())\n\n    assert len(tdms_file) == 1\n    assert len(tdms_file[\'group\']) == 10\n    for channel_num in range(10):\n        assert len(tdms_file[\'group\'][\'channel{0}\'.format(channel_num)]) == 10\n\n\ndef get_contiguous_file():\n    test_file = GeneratedFile()\n    data_chunk = np.repeat(np.array([1, 2, 3, 4], dtype=np.dtype(\'int32\')), 100)\n    data_array = np.tile(data_chunk, 10)\n    data = data_array.tobytes()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 100),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 100),\n            channel_metadata(""/\'group\'/\'channel3\'"", TDS_TYPE_INT32, 100),\n            channel_metadata(""/\'group\'/\'channel4\'"", TDS_TYPE_INT32, 100),\n        ),\n        data, binary_data=True\n    )\n    for _ in range(0, 9):\n        test_file.add_segment(\n            (""kTocRawData"", ), """", data, binary_data=True)\n    return test_file\n\n\ndef get_interleaved_file():\n    test_file = GeneratedFile()\n    data_array = np.tile(np.array([1, 2, 3, 4], dtype=np.dtype(\'int32\')), 1000)\n    data = data_array.tobytes()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"", ""kTocInterleavedData""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", TDS_TYPE_INT32, 100),\n            channel_metadata(""/\'group\'/\'channel2\'"", TDS_TYPE_INT32, 100),\n            channel_metadata(""/\'group\'/\'channel3\'"", TDS_TYPE_INT32, 100),\n            channel_metadata(""/\'group\'/\'channel4\'"", TDS_TYPE_INT32, 100),\n        ),\n        data, binary_data=True\n    )\n    for _ in range(0, 9):\n        test_file.add_segment(\n            (""kTocRawData"", ""kTocInterleavedData""), """", data, binary_data=True)\n    return test_file\n\n\ndef read_from_start(file):\n    file.seek(0, os.SEEK_SET)\n    return TdmsFile.read(file)\n\n\ndef read_metadata_from_start(file):\n    file.seek(0, os.SEEK_SET)\n    return TdmsFile.read_metadata(file)\n\n\ndef read_channel_data(chan):\n    return chan[:]\n\n\ndef stream_chunks(chan):\n    all_data = []\n    for chunk in chan.data_chunks():\n        all_data.append(chunk[:])\n    return all_data\n\n\ndef get_slice(chan, start, stop):\n    return chan[start:stop]\n\n\ndef index_values(chan, target):\n    for i in range(len(chan)):\n        target[i] = chan[i]\n'"
nptdms/test/test_daqmx.py,62,"b'""""""Test reading of TDMS files with DAQmx data\n""""""\n\nfrom collections import defaultdict\nimport logging\nimport numpy as np\n\nfrom nptdms import TdmsFile\nfrom nptdms.log import log_manager\nfrom nptdms.test.util import (\n    GeneratedFile, hexlify_value, string_hexlify, segment_objects_metadata, hex_properties)\n\n\ndef test_single_channel_i16():\n    """""" Test loading a DAQmx file with a single channel of I16 data\n    """"""\n\n    scaler_metadata = daqmx_scaler_metadata(0, 3, 0)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [2], [scaler_metadata]))\n    data = (\n        ""01 00""\n        ""02 00""\n        ""FF FF""\n        ""FE FF""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n    tdms_data = test_file.load()\n\n    data = tdms_data[""Group""][""Channel1""].raw_data\n\n    assert data.dtype == np.int16\n    np.testing.assert_array_equal(data, [1, 2, -1, -2])\n\n\ndef test_single_channel_u16():\n    """""" Test loading a DAQmx file with a single channel of U16 data\n    """"""\n\n    scaler_metadata = daqmx_scaler_metadata(0, 2, 0)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [2], [scaler_metadata]))\n    data = (\n        # Data for segment\n        ""01 00""\n        ""02 00""\n        ""FF FF""\n        ""FE FF""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n    tdms_data = test_file.load()\n\n    data = tdms_data[""Group""][""Channel1""].raw_data\n\n    assert data.dtype == np.uint16\n    np.testing.assert_array_equal(data, [1, 2, 2**16 - 1, 2**16 - 2])\n\n\ndef test_single_channel_i32():\n    """""" Test loading a DAQmx file with a single channel of I32 data\n    """"""\n\n    scaler_metadata = daqmx_scaler_metadata(0, 5, 0)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [4], [scaler_metadata]))\n    data = (\n        # Data for segment\n        ""01 00 00 00""\n        ""02 00 00 00""\n        ""FF FF FF FF""\n        ""FE FF FF FF""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n    tdms_data = test_file.load()\n\n    data = tdms_data[""Group""][""Channel1""].raw_data\n\n    assert data.dtype == np.int32\n    np.testing.assert_array_equal(data, [1, 2, -1, -2])\n\n\ndef test_single_channel_u32():\n    """""" Test loading a DAQmx file with a single channel of U32 data\n    """"""\n\n    scaler_metadata = daqmx_scaler_metadata(0, 4, 0)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [4], [scaler_metadata]))\n    data = (\n        # Data for segment\n        ""01 00 00 00""\n        ""02 00 00 00""\n        ""FF FF FF FF""\n        ""FE FF FF FF""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n    tdms_data = test_file.load()\n\n    data = tdms_data[""Group""][""Channel1""].raw_data\n\n    assert data.dtype == np.uint32\n    np.testing.assert_array_equal(data, [1, 2, 2**32 - 1, 2**32 - 2])\n\n\ndef test_two_channel_i16():\n    """""" Test loading a DAQmx file with two channels of I16 data\n    """"""\n\n    scaler_1 = daqmx_scaler_metadata(0, 3, 0)\n    scaler_2 = daqmx_scaler_metadata(0, 3, 2)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [4], [scaler_1]),\n        daqmx_channel_metadata(""Channel2"", 4, [4], [scaler_2]))\n    data = (\n        # Data for segment\n        ""01 00""\n        ""11 00""\n        ""02 00""\n        ""12 00""\n        ""03 00""\n        ""13 00""\n        ""04 00""\n        ""14 00""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n    tdms_data = test_file.load()\n\n    data_1 = tdms_data[""Group""][""Channel1""].raw_data\n    assert data_1.dtype == np.int16\n    np.testing.assert_array_equal(data_1, [1, 2, 3, 4])\n\n    data_2 = tdms_data[""Group""][""Channel2""].raw_data\n    assert data_2.dtype == np.int16\n    np.testing.assert_array_equal(data_2, [17, 18, 19, 20])\n\n\ndef test_mixed_channel_widths():\n    """""" Test loading a DAQmx file with channels with different widths\n    """"""\n\n    scaler_1 = daqmx_scaler_metadata(0, 1, 0)\n    scaler_2 = daqmx_scaler_metadata(0, 3, 1)\n    scaler_3 = daqmx_scaler_metadata(0, 5, 3)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [7], [scaler_1]),\n        daqmx_channel_metadata(""Channel2"", 4, [7], [scaler_2]),\n        daqmx_channel_metadata(""Channel3"", 4, [7], [scaler_3]))\n    data = (\n        # Data for segment\n        ""01 11 00 21 00 00 00""\n        ""02 12 00 22 00 00 00""\n        ""03 13 00 23 00 00 00""\n        ""04 14 00 24 00 00 00""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n    tdms_data = test_file.load()\n\n    data_1 = tdms_data[""Group""][""Channel1""].raw_data\n    assert data_1.dtype == np.int8\n    np.testing.assert_array_equal(data_1, [1, 2, 3, 4])\n\n    data_2 = tdms_data[""Group""][""Channel2""].raw_data\n    assert data_2.dtype == np.int16\n    np.testing.assert_array_equal(data_2, [17, 18, 19, 20])\n\n    data_3 = tdms_data[""Group""][""Channel3""].raw_data\n    assert data_3.dtype == np.int32\n    np.testing.assert_array_equal(data_3, [33, 34, 35, 36])\n\n\ndef test_multiple_scalers_with_same_type():\n    """""" Test loading a DAQmx file with one channel containing multiple\n        format changing scalers of the same type\n    """"""\n\n    scaler_metadata = [\n        daqmx_scaler_metadata(0, 3, 0),\n        daqmx_scaler_metadata(1, 3, 2)]\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [4], scaler_metadata))\n    data = (\n        # Data for segment\n        ""01 00""\n        ""11 00""\n        ""02 00""\n        ""12 00""\n        ""03 00""\n        ""13 00""\n        ""04 00""\n        ""14 00""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n    tdms_data = test_file.load()\n    channel = tdms_data[""Group""][""Channel1""]\n\n    scaler_0_data = channel.raw_scaler_data[0]\n    assert scaler_0_data.dtype == np.int16\n    np.testing.assert_array_equal(scaler_0_data, [1, 2, 3, 4])\n\n    scaler_1_data = channel.raw_scaler_data[1]\n    assert scaler_1_data.dtype == np.int16\n    np.testing.assert_array_equal(scaler_1_data, [17, 18, 19, 20])\n\n\ndef test_multiple_scalers_with_different_types():\n    """""" Test loading a DAQmx file with one channel containing multiple\n        format changing scalers of different types\n    """"""\n\n    scaler_metadata = [\n        daqmx_scaler_metadata(0, 1, 0),\n        daqmx_scaler_metadata(1, 3, 1),\n        daqmx_scaler_metadata(2, 5, 3)]\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [7], scaler_metadata))\n    data = (\n        # Data for segment\n        ""01 11 00 21 00 00 00""\n        ""02 12 00 22 00 00 00""\n        ""03 13 00 23 00 00 00""\n        ""04 14 00 24 00 00 00""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n    tdms_data = test_file.load()\n    channel = tdms_data[""Group""][""Channel1""]\n\n    scaler_0_data = channel.raw_scaler_data[0]\n    assert scaler_0_data.dtype == np.int8\n    np.testing.assert_array_equal(scaler_0_data, [1, 2, 3, 4])\n\n    scaler_1_data = channel.raw_scaler_data[1]\n    assert scaler_1_data.dtype == np.int16\n    np.testing.assert_array_equal(scaler_1_data, [17, 18, 19, 20])\n\n    scaler_2_data = channel.raw_scaler_data[2]\n    assert scaler_2_data.dtype == np.int32\n    np.testing.assert_array_equal(scaler_2_data, [33, 34, 35, 36])\n\n\ndef test_multiple_raw_data_buffers():\n    """""" Test loading a DAQmx file with multiple raw data buffers\n    """"""\n\n    scaler_1 = daqmx_scaler_metadata(0, 3, 0, 0)\n    scaler_2 = daqmx_scaler_metadata(0, 3, 2, 0)\n    scaler_3 = daqmx_scaler_metadata(0, 3, 0, 1)\n    scaler_4 = daqmx_scaler_metadata(0, 3, 2, 1)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [4, 4], [scaler_1]),\n        daqmx_channel_metadata(""Channel2"", 4, [4, 4], [scaler_2]),\n        daqmx_channel_metadata(""Channel3"", 4, [4, 4], [scaler_3]),\n        daqmx_channel_metadata(""Channel4"", 4, [4, 4], [scaler_4]))\n    data = (\n        ""01 00"" ""02 00"" ""03 00"" ""04 00""\n        ""05 00"" ""06 00"" ""07 00"" ""08 00""\n        ""09 00"" ""0A 00"" ""0B 00"" ""0C 00""\n        ""0D 00"" ""0E 00"" ""0F 00"" ""10 00""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n    tdms_data = test_file.load()\n\n    data_1 = tdms_data[""Group""][""Channel1""].raw_data\n    data_2 = tdms_data[""Group""][""Channel2""].raw_data\n    data_3 = tdms_data[""Group""][""Channel3""].raw_data\n    data_4 = tdms_data[""Group""][""Channel4""].raw_data\n\n    for data in [data_1, data_2, data_3, data_4]:\n        assert data.dtype == np.int16\n\n    np.testing.assert_array_equal(data_1, [1, 3, 5, 7])\n    np.testing.assert_array_equal(data_2, [2, 4, 6, 8])\n    np.testing.assert_array_equal(data_3, [9, 11, 13, 15])\n    np.testing.assert_array_equal(data_4, [10, 12, 14, 16])\n\n\ndef test_multiple_raw_data_buffers_with_different_widths():\n    """""" DAQmx with raw data buffers with different widths\n    """"""\n\n    scaler_1 = daqmx_scaler_metadata(0, 3, 0, 0)\n    scaler_2 = daqmx_scaler_metadata(0, 3, 2, 0)\n    scaler_3 = daqmx_scaler_metadata(0, 3, 4, 0)\n    scaler_4 = daqmx_scaler_metadata(0, 5, 0, 1)\n    scaler_5 = daqmx_scaler_metadata(0, 5, 4, 1)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [6, 8], [scaler_1]),\n        daqmx_channel_metadata(""Channel2"", 4, [6, 8], [scaler_2]),\n        daqmx_channel_metadata(""Channel3"", 4, [6, 8], [scaler_3]),\n        daqmx_channel_metadata(""Channel4"", 4, [6, 8], [scaler_4]),\n        daqmx_channel_metadata(""Channel5"", 4, [6, 8], [scaler_5]))\n    data = (\n        ""01 00"" ""02 00"" ""03 00""\n        ""04 00"" ""05 00"" ""06 00""\n        ""07 00"" ""08 00"" ""09 00""\n        ""0A 00"" ""0B 00"" ""0C 00""\n        ""0D 00 00 00"" ""0E 00 00 00""\n        ""0F 00 00 00"" ""10 00 00 00""\n        ""11 00 00 00"" ""12 00 00 00""\n        ""13 00 00 00"" ""14 00 00 00""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n    tdms_data = test_file.load()\n\n    data_1 = tdms_data[""Group""][""Channel1""].raw_data\n    data_2 = tdms_data[""Group""][""Channel2""].raw_data\n    data_3 = tdms_data[""Group""][""Channel3""].raw_data\n    data_4 = tdms_data[""Group""][""Channel4""].raw_data\n    data_5 = tdms_data[""Group""][""Channel5""].raw_data\n\n    for data in [data_1, data_2, data_3]:\n        assert data.dtype == np.int16\n    for data in [data_4, data_5]:\n        assert data.dtype == np.int32\n\n    np.testing.assert_array_equal(data_1, [1, 4, 7, 10])\n    np.testing.assert_array_equal(data_2, [2, 5, 8, 11])\n    np.testing.assert_array_equal(data_3, [3, 6, 9, 12])\n    np.testing.assert_array_equal(data_4, [13, 15, 17, 19])\n    np.testing.assert_array_equal(data_5, [14, 16, 18, 20])\n\n\ndef test_multiple_raw_data_buffers_with_scalers_split_across_buffers():\n    """""" DAQmx with scalers split across different raw data buffers\n    """"""\n\n    scaler_1 = daqmx_scaler_metadata(0, 3, 0, 0)\n    scaler_2 = daqmx_scaler_metadata(1, 3, 0, 1)\n    scaler_3 = daqmx_scaler_metadata(0, 3, 2, 0)\n    scaler_4 = daqmx_scaler_metadata(1, 3, 2, 1)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(\n            ""Channel1"", 4, [4, 4], [scaler_1, scaler_2]),\n        daqmx_channel_metadata(\n            ""Channel2"", 4, [4, 4], [scaler_3, scaler_4]))\n    data = (\n        ""01 00"" ""02 00"" ""03 00"" ""04 00""\n        ""05 00"" ""06 00"" ""07 00"" ""08 00""\n        ""09 00"" ""0A 00"" ""0B 00"" ""0C 00""\n        ""0D 00"" ""0E 00"" ""0F 00"" ""10 00""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n    tdms_data = test_file.load()\n\n    channel_1 = tdms_data[""Group""][""Channel1""]\n    channel_2 = tdms_data[""Group""][""Channel2""]\n\n    scaler_data_1 = channel_1.raw_scaler_data[0]\n    scaler_data_2 = channel_1.raw_scaler_data[1]\n    scaler_data_3 = channel_2.raw_scaler_data[0]\n    scaler_data_4 = channel_2.raw_scaler_data[1]\n\n    for data in [\n            scaler_data_1, scaler_data_2, scaler_data_3, scaler_data_4]:\n        assert data.dtype == np.int16\n\n    np.testing.assert_array_equal(scaler_data_1, [1, 3, 5, 7])\n    np.testing.assert_array_equal(scaler_data_2, [9, 11, 13, 15])\n    np.testing.assert_array_equal(scaler_data_3, [2, 4, 6, 8])\n    np.testing.assert_array_equal(scaler_data_4, [10, 12, 14, 16])\n\n\ndef test_digital_line_scaler_data():\n    """""" Test loading a DAQmx file with a single channel of U8 digital line scaler data\n    """"""\n\n    scaler_metadata = daqmx_scaler_metadata(0, 0, 2, digital_line_scaler=True)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [4], [scaler_metadata], digital_line_scaler=True))\n    data = (\n        ""00 00 00 00""\n        ""00 00 01 00""\n        ""00 00 00 00""\n        ""00 00 01 00""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n    tdms_data = test_file.load()\n\n    data = tdms_data[""Group""][""Channel1""].raw_data\n\n    assert data.dtype == np.uint8\n    np.testing.assert_array_equal(data, [0, 1, 0, 1])\n\n\ndef test_digital_line_scaler_data_uses_first_bit_of_bytes():\n    """""" Test DAQmx digital line scaler data only uses the first bit in each byte to represent a 1 or 0 value\n    """"""\n\n    scaler_metadata = daqmx_scaler_metadata(0, 0, 2, digital_line_scaler=True)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [4], [scaler_metadata], digital_line_scaler=True))\n    data = (\n        ""00 00 00 00""\n        ""00 00 01 00""\n        ""00 00 02 00""\n        ""00 00 03 00""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n    tdms_data = test_file.load()\n\n    data = tdms_data[""Group""][""Channel1""].raw_data\n\n    assert data.dtype == np.uint8\n    np.testing.assert_array_equal(data, [0, 1, 0, 1])\n\n\ndef test_lazily_reading_channel():\n    """""" Test loading channels individually from a DAQmx file\n    """"""\n\n    # Single scale which is just the raw DAQmx scaler data\n    properties = {\n        ""NI_Number_Of_Scales"": (3, ""01 00 00 00""),\n    }\n    scaler_1 = daqmx_scaler_metadata(0, 3, 0)\n    scaler_2 = daqmx_scaler_metadata(0, 3, 2)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [4], [scaler_1], properties),\n        daqmx_channel_metadata(""Channel2"", 4, [4], [scaler_2], properties))\n    data = (\n        # Data for segment\n        ""01 00""\n        ""11 00""\n        ""02 00""\n        ""12 00""\n        ""03 00""\n        ""13 00""\n        ""04 00""\n        ""14 00""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            data_1 = tdms_file[""Group""][""Channel1""].read_data()\n            assert data_1.dtype == np.int16\n            np.testing.assert_array_equal(data_1, [1, 2, 3, 4])\n\n            data_2 = tdms_file[""Group""][""Channel2""].read_data()\n            assert data_2.dtype == np.int16\n            np.testing.assert_array_equal(data_2, [17, 18, 19, 20])\n\n\ndef test_lazily_reading_a_subset_of_channel_data():\n    """""" Test loading a subset of channel data from a DAQmx file\n    """"""\n\n    # Single scale which is just the raw DAQmx scaler data\n    properties = {\n        ""NI_Number_Of_Scales"": (3, ""01 00 00 00""),\n    }\n    scaler_1 = daqmx_scaler_metadata(0, 3, 0)\n    scaler_2 = daqmx_scaler_metadata(0, 3, 2)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [4], [scaler_1], properties),\n        daqmx_channel_metadata(""Channel2"", 4, [4], [scaler_2], properties))\n    data = (\n        # Data for segment\n        ""01 00""\n        ""11 00""\n        ""02 00""\n        ""12 00""\n        ""03 00""\n        ""13 00""\n        ""04 00""\n        ""14 00""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            data_1 = tdms_file[""Group""][""Channel1""].read_data(1, 2)\n            assert data_1.dtype == np.int16\n            np.testing.assert_array_equal(data_1, [2, 3])\n\n            data_2 = tdms_file[""Group""][""Channel2""].read_data(1, 2)\n            assert data_2.dtype == np.int16\n            np.testing.assert_array_equal(data_2, [18, 19])\n\n\ndef test_lazily_reading_a_subset_of_raw_channel_data():\n    """""" Test loading a subset of raw scaler channel data from a DAQmx file\n    """"""\n\n    # Single scale which is just the raw DAQmx scaler data\n    properties = {\n        ""NI_Number_Of_Scales"": (3, ""01 00 00 00""),\n    }\n    scaler_1 = daqmx_scaler_metadata(0, 3, 0)\n    scaler_2 = daqmx_scaler_metadata(0, 3, 2)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [4], [scaler_1], properties),\n        daqmx_channel_metadata(""Channel2"", 4, [4], [scaler_2], properties))\n    data = (\n        # Data for segment\n        ""01 00""\n        ""11 00""\n        ""02 00""\n        ""12 00""\n        ""03 00""\n        ""13 00""\n        ""04 00""\n        ""14 00""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            data_1 = tdms_file[""Group""][""Channel1""].read_data(1, 2, scaled=False)\n            assert len(data_1) == 1\n            assert data_1[0].dtype == np.int16\n            np.testing.assert_array_equal(data_1[0], [2, 3])\n\n            data_2 = tdms_file[""Group""][""Channel2""].read_data(1, 2, scaled=False)\n            assert len(data_2) == 1\n            assert data_2[0].dtype == np.int16\n            np.testing.assert_array_equal(data_2[0], [18, 19])\n\n\ndef test_stream_data_chunks():\n    """"""Test streaming chunks of DAQmx data from a TDMS file\n    """"""\n    properties = {\n        ""NI_Number_Of_Scales"": (3, ""01 00 00 00""),\n    }\n    scaler_1 = daqmx_scaler_metadata(0, 3, 0)\n    scaler_2 = daqmx_scaler_metadata(0, 3, 2)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [4], [scaler_1], properties),\n        daqmx_channel_metadata(""Channel2"", 4, [4], [scaler_2], properties))\n    data = (\n        # Data for segment\n        ""01 00"" ""11 00""\n        ""02 00"" ""12 00""\n        ""03 00"" ""13 00""\n        ""04 00"" ""14 00""\n        ""05 00"" ""15 00""\n        ""06 00"" ""16 00""\n        ""07 00"" ""17 00""\n        ""08 00"" ""18 00""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n    data_arrays = defaultdict(list)\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for chunk in tdms_file.data_chunks():\n                for group in chunk.groups():\n                    for channel in group.channels():\n                        key = (group.name, channel.name)\n                        assert channel.offset == len(data_arrays[key])\n                        data_arrays[key].extend(channel[:])\n\n    expected_channel_data = {\n        (""Group"", ""Channel1""): [1, 2, 3, 4, 5, 6, 7, 8],\n        (""Group"", ""Channel2""): [17, 18, 19, 20, 21, 22, 23, 24],\n    }\n    for ((group, channel), expected_data) in expected_channel_data.items():\n        actual_data = data_arrays[(group, channel)]\n        np.testing.assert_equal(actual_data, expected_data)\n\n\ndef test_daqmx_debug_logging(caplog):\n    """""" Test loading a DAQmx file with debug logging enabled\n    """"""\n    scaler_metadata = daqmx_scaler_metadata(0, 3, 0)\n    metadata = segment_objects_metadata(\n        root_metadata(),\n        group_metadata(),\n        daqmx_channel_metadata(""Channel1"", 4, [2], [scaler_metadata]))\n    data = (\n        ""01 00""\n        ""02 00""\n        ""FF FF""\n        ""FE FF""\n    )\n\n    test_file = GeneratedFile()\n    test_file.add_segment(segment_toc(), metadata, data)\n\n    log_manager.set_level(logging.DEBUG)\n    _ = test_file.load()\n\n    assert ""Reading metadata for object /\'Group\'/\'Channel1\' with index header 0x00001269"" in caplog.text\n    assert ""scaler_type=4713"" in caplog.text\n    assert ""scale_id=0"" in caplog.text\n    assert ""data_type=Int16"" in caplog.text\n\n\ndef segment_toc():\n    return (\n        ""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"", ""kTocDAQmxRawData"")\n\n\ndef root_metadata():\n    return (\n        # Length of the object path\n        ""01 00 00 00""\n        # Object path (/)\n        ""2F""\n        # Raw data index\n        ""FF FF FF FF""\n        # Num properties\n        ""00 00 00 00"")\n\n\ndef group_metadata():\n    return (\n        # Length of the object path\n        ""08 00 00 00""\n        # Object path (/\'Group\')\n        ""2F 27 47 72""\n        ""6F 75 70 27""\n        # Raw data index\n        ""FF FF FF FF""\n        # Num properties\n        ""00 00 00 00"")\n\n\ndef daqmx_scaler_metadata(scale_id, type_id, byte_offset, raw_buffer_index=0, digital_line_scaler=False):\n    return (\n        # DAQmx data type (type ids don\'t match TDMS types)\n        hexlify_value(""<I"", type_id) +\n        # Raw buffer index\n        hexlify_value(""<I"", raw_buffer_index) +\n        # Raw byte offset\n        hexlify_value(""<I"", byte_offset) +\n        # Sample format bitmap (don\'t know what this is for...)\n        (""00"" if digital_line_scaler else ""00 00 00 00"") +\n        # Scale ID\n        hexlify_value(""<I"", scale_id))\n\n\ndef daqmx_channel_metadata(\n        channel_name, num_values,\n        raw_data_widths, scaler_metadata, properties=None, digital_line_scaler=False):\n    path = ""/\'Group\'/\'"" + channel_name + ""\'""\n    return (\n        # Length of the object path\n        hexlify_value(""<I"", len(path)) +\n        # Object path\n        string_hexlify(path) +\n        # Raw data index (DAQmx)\n        (""6A 12 00 00"" if digital_line_scaler else ""69 12 00 00"") +\n        # Data type (DAQmx)\n        ""FF FF FF FF""\n        # Array  dimension\n        ""01 00 00 00"" +\n        # Number of values (chunk size)\n        hexlify_value(""<Q"", num_values) +\n        # Scaler metadata\n        hexlify_value(""<I"", len(scaler_metadata)) +\n        """".join(scaler_metadata) +\n        # Raw data width vector size\n        hexlify_value(""<I"", len(raw_data_widths)) +\n        # Raw data width values\n        """".join(hexlify_value(""<I"", v) for v in raw_data_widths) +\n        hex_properties(properties))\n'"
nptdms/test/test_example_files.py,4,"b'"""""" Test reading example TDMS files\n""""""\n\nimport os\nimport numpy as np\nfrom nptdms import tdms\n\n\nDATA_DIR = os.path.dirname(os.path.realpath(__file__)) + \'/data\'\n\n\ndef test_labview_file():\n    """"""Test reading a file that was created by LabVIEW""""""\n    test_file = tdms.TdmsFile(DATA_DIR + \'/Digital_Input.tdms\')\n    group = (""07/09/2012 06:58:23 PM - "" +\n             ""Digital Input - Decimated Data_Level1"")\n    channel = ""Dev1_port3_line7 - line 0""\n    expected = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1], dtype=np.uint8)\n\n    data = test_file[group][channel].data\n    np.testing.assert_almost_equal(data[:10], expected)\n\n\ndef test_raw_format():\n    """"""Test reading a file with DAQmx raw data""""""\n    test_file = tdms.TdmsFile(DATA_DIR + \'/raw1.tdms\')\n    group = test_file.groups()[0]\n    data = group[\'First  Channel\'].data\n    np.testing.assert_almost_equal(data[:10],\n                                   [-0.18402661, 0.14801477, -0.24506363,\n                                    -0.29725028, -0.20020142, 0.18158513,\n                                    0.02380444, 0.20661031, 0.20447401,\n                                    0.2517777])\n\n\ndef test_big_endian_format():\n    """"""Test reading a file that encodes data in big endian mode""""""\n    test_file = tdms.TdmsFile(DATA_DIR + \'/big_endian.tdms\')\n    data = test_file[\'Measured Data\'][\'Phase sweep\'].data\n    np.testing.assert_almost_equal(data[:10],\n                                   [0.0000000, 0.0634176, 0.1265799,\n                                    0.1892325, 0.2511234, 0.3120033,\n                                    0.3716271, 0.4297548, 0.4861524,\n                                    0.5405928])\n'"
nptdms/test/test_hdf.py,18,"b'"""""" Test exporting TDMS data to HDF\n""""""\nimport pytest\nimport numpy as np\ntry:\n    import h5py\nexcept ImportError:\n    pytest.skip(""Skipping HDF tests as h5py is not installed"", allow_module_level=True)\n\nfrom nptdms import TdmsFile\nfrom nptdms.test.util import (\n    GeneratedFile,\n    basic_segment,\n    channel_metadata,\n    compare_arrays,\n    hexlify_value,\n    segment_objects_metadata,\n    string_hexlify,\n)\nfrom nptdms.test import scenarios\n\n\ndef test_hdf_channel_data(tmp_path):\n    """""" Test basic conversion of channel data to HDF\n    """"""\n    test_file, expected_data = scenarios.single_segment_with_two_channels().values\n\n    tdms_data = test_file.load()\n    h5_path = tmp_path / \'h5_data_test.h5\'\n    h5 = tdms_data.as_hdf(h5_path)\n\n    for ((group, channel), expected_data) in expected_data.items():\n        h5_channel = h5[group][channel]\n        assert h5_channel.dtype.kind == \'i\'\n        np.testing.assert_almost_equal(h5_channel[...], expected_data)\n    h5.close()\n\n\ndef test_streaming_to_hdf(tmp_path):\n    """""" Test conversion of channel data to HDF when streaming data from disk\n    """"""\n    test_file, expected_data = scenarios.chunked_segment().values\n\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            h5_path = tmp_path / \'h5_streaming_data_test.h5\'\n            h5 = tdms_file.as_hdf(h5_path)\n\n    for ((group, channel), expected_data) in expected_data.items():\n        h5_channel = h5[group][channel]\n        np.testing.assert_almost_equal(h5_channel[...], expected_data)\n    h5.close()\n\n\ndef test_int_data_types(tmp_path):\n    """""" Test conversion of signed and unsigned integer types to HDF\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'i8\'"", 1, 4),\n            channel_metadata(""/\'group\'/\'u8\'"", 5, 4),\n            channel_metadata(""/\'group\'/\'i16\'"", 2, 4),\n            channel_metadata(""/\'group\'/\'u16\'"", 6, 4),\n            channel_metadata(""/\'group\'/\'i32\'"", 3, 4),\n            channel_metadata(""/\'group\'/\'u32\'"", 7, 4),\n            channel_metadata(""/\'group\'/\'i64\'"", 4, 4),\n            channel_metadata(""/\'group\'/\'u64\'"", 8, 4),\n        ),\n        ""01 02 03 04""\n        ""01 02 03 04""\n        ""01 00 02 00 03 00 04 00""\n        ""01 00 02 00 03 00 04 00""\n        ""01 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00""\n        ""01 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00""\n        ""01 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 03 00 00 00 00 00 00 00 04 00 00 00 00 00 00 00""\n        ""01 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 03 00 00 00 00 00 00 00 04 00 00 00 00 00 00 00""\n    )\n\n    tdms_data = test_file.load()\n    h5_path = tmp_path / \'h5_data_test.h5\'\n    h5 = tdms_data.as_hdf(h5_path)\n\n    for chan, expected_dtype in [\n            (\'i8\', np.dtype(\'int8\')),\n            (\'u8\', np.dtype(\'uint8\')),\n            (\'i16\', np.dtype(\'int16\')),\n            (\'u16\', np.dtype(\'uint16\')),\n            (\'i32\', np.dtype(\'int32\')),\n            (\'u32\', np.dtype(\'uint32\')),\n            (\'i64\', np.dtype(\'int64\')),\n            (\'u64\', np.dtype(\'uint64\'))]:\n        h5_channel = h5[\'group\'][chan]\n        assert h5_channel.dtype == expected_dtype\n        np.testing.assert_almost_equal(h5_channel[...], [1, 2, 3, 4])\n    h5.close()\n\n\ndef test_floating_point_data_types(tmp_path):\n    """""" Test conversion of f32 and f64 types to HDF\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'f32\'"", 9, 4),\n            channel_metadata(""/\'group\'/\'f64\'"", 10, 4),\n        ),\n        hexlify_value(\'<f\', 1) +\n        hexlify_value(\'<f\', 2) +\n        hexlify_value(\'<f\', 3) +\n        hexlify_value(\'<f\', 4) +\n        hexlify_value(\'<d\', 1) +\n        hexlify_value(\'<d\', 2) +\n        hexlify_value(\'<d\', 3) +\n        hexlify_value(\'<d\', 4)\n    )\n\n    tdms_data = test_file.load()\n    h5_path = tmp_path / \'h5_data_test.h5\'\n    h5 = tdms_data.as_hdf(h5_path)\n\n    for chan, expected_dtype in [\n            (\'f32\', np.dtype(\'float32\')),\n            (\'f64\', np.dtype(\'float64\'))]:\n        h5_channel = h5[\'group\'][chan]\n        assert h5_channel.dtype == expected_dtype\n        np.testing.assert_almost_equal(h5_channel[...], [1.0, 2.0, 3.0, 4.0])\n    h5.close()\n\n\ndef test_timestamp_data(tmp_path):\n    """""" Test conversion of timestamp channel data to HDF\n        HDF doesn\'t support timestamps natively, so these are converted to strings\n    """"""\n\n    test_file, expected_data = scenarios.timestamp_data().values\n    tdms_data = test_file.load()\n    h5_path = tmp_path / \'h5_timestamp_test.h5\'\n    h5 = tdms_data.as_hdf(h5_path)\n\n    for (group, channel), expected_values in expected_data.items():\n        h5_channel = h5[group][channel]\n        assert h5_channel.dtype.kind == \'S\'\n        expected_strings = np.datetime_as_string(expected_values, unit=\'us\', timezone=\'UTC\')\n        expected_ascii = [s.encode(\'ascii\') for s in expected_strings]\n        compare_arrays(h5_channel[...], expected_ascii)\n    h5.close()\n\n\ndef test_hdf_properties(tmp_path):\n    """""" Test properties are converted to attributes in HDF files\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_data = test_file.load()\n\n    h5_path = tmp_path / \'h5_properties_test.h5\'\n    h5 = tdms_data.as_hdf(h5_path)\n\n    # File level properties\n    assert h5.attrs[\'num\'] == 15\n\n    # Group properties\n    assert h5[\'Group\'].attrs[\'prop\'] == \'value\'\n    assert h5[\'Group\'].attrs[\'num\'] == 10\n\n    # Channel properties\n    assert h5[\'Group\'][\'Channel2\'].attrs[\'wf_start_offset\'] == 0.0\n    assert h5[\'Group\'][\'Channel2\'].attrs[\'wf_increment\'] == 0.1\n\n\ndef test_timestamp_property(tmp_path):\n    """""" Test a timestamp property is converted to an attribute in an HDF file\n        HDF doesn\'t support timestamps natively, so these are converted to strings\n    """"""\n    test_file = GeneratedFile()\n    properties = {\n        ""wf_start_time"": (0x44, hexlify_value(""<Q"", 0) + hexlify_value(""<q"", 3524551547))\n    }\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", 3, 2, properties),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n    )\n    tdms_data = test_file.load()\n\n    h5_path = tmp_path / \'h5_properties_test.h5\'\n    h5 = tdms_data.as_hdf(h5_path)\n\n    assert h5[\'group\'][\'channel1\'].attrs[\'wf_start_time\'] == b\'2015-09-08T10:05:47.000000Z\'\n\n\ndef test_as_hdf_string(tmp_path):\n    """""" Test HDF5 conversion for string datatype\n    """"""\n    strings = [""abc123"", ""?<>~`!@#$%^&*()-=_+,.;\'[]:{}|""]\n\n    test_file = GeneratedFile()\n    toc = (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"")\n    metadata = (\n        # Number of objects\n        ""01 00 00 00""\n        # Length of the object path\n        ""11 00 00 00"")\n    metadata += string_hexlify(""/\'Group\'/\'String\'"")\n    metadata += (\n        # Length of index information\n        ""1C 00 00 00""\n        # Raw data data type\n        ""20 00 00 00""\n        # Dimension\n        ""01 00 00 00""\n        # Number of raw data values\n        ""02 00 00 00""\n        ""00 00 00 00""\n        # Number of bytes in data\n        ""2B 00 00 00""\n        ""00 00 00 00""\n        # Number of properties (0)\n        ""00 00 00 00"")\n    data = (\n        ""06 00 00 00""  # index to after first string\n        ""24 00 00 00""  # index to after second string\n    )\n    for string in strings:\n        data += string_hexlify(string)\n    test_file.add_segment(toc, metadata, data)\n    tdms_data = test_file.load()\n\n    data = tdms_data[""Group""][""String""].data\n    assert len(data) == len(strings)\n    for expected, read in zip(strings, data):\n        assert expected == read\n\n    h5_path = tmp_path / \'h5_strings_test.h5\'\n    h5 = tdms_data.as_hdf(h5_path)\n    h5_strings = h5[\'Group\'][\'String\']\n    assert h5_strings.dtype.kind == \'O\'\n    assert h5_strings.shape[0] == len(strings)\n    for expected, read in zip(strings, h5_strings[...]):\n        assert expected == read\n    h5.close()\n\n\ndef test_unicode_string_data(tmp_path):\n    """""" Test HDF5 conversion for string datatype with non-ASCII data\n    """"""\n    strings = [""Hello, \\u4E16\\u754C"", ""\\U0001F600""]\n    sizes = [len(s.encode(\'utf-8\')) for s in strings]\n\n    test_file = GeneratedFile()\n    toc = (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"")\n    metadata = (\n        # Number of objects\n        ""01 00 00 00""\n        # Length of the object path\n        ""11 00 00 00"")\n    metadata += string_hexlify(""/\'Group\'/\'String\'"")\n    metadata += (\n        # Length of index information\n        ""1C 00 00 00""\n        # Raw data data type\n        ""20 00 00 00""\n        # Dimension\n        ""01 00 00 00""\n        # Number of raw data values\n        ""02 00 00 00""\n        ""00 00 00 00"" +\n        # Number of bytes in data, including index\n        hexlify_value(\'q\', sum(sizes) + 4 * len(sizes)) +\n        # Number of properties (0)\n        ""00 00 00 00"")\n    data = """"\n    offset = 0\n    for size in sizes:\n        # Index gives end positions of strings:\n        offset += size\n        data += hexlify_value(\'i\', offset)\n    for string in strings:\n        data += string_hexlify(string)\n    test_file.add_segment(toc, metadata, data)\n    tdms_data = test_file.load()\n\n    data = tdms_data[""Group""][""String""].data\n    assert len(data) == len(strings)\n    for expected, read in zip(strings, data):\n        assert expected == read\n\n    h5_path = tmp_path / \'h5_unicode_strings_test.h5\'\n    h5 = tdms_data.as_hdf(h5_path)\n    h5_strings = h5[\'Group\'][\'String\']\n    assert h5_strings.dtype.kind == \'O\'\n    assert h5_strings.shape[0] == len(strings)\n    for expected, read in zip(strings, h5_strings[...]):\n        assert expected == read\n    h5.close()\n\n\ndef test_add_to_file_under_group(tmp_path):\n    """""" Test adding TDMS data to an HDF file under a group\n    """"""\n    test_file, expected_data = scenarios.single_segment_with_two_channels().values\n    preexisting_data = np.array([1.0, 2.0, 3.0])\n\n    tdms_data = test_file.load()\n    h5_path = tmp_path / \'h5_data_test.h5\'\n    h5 = h5py.File(h5_path, \'w\')\n    h5[\'preexisting_data\'] = preexisting_data\n    h5.close()\n\n    h5 = tdms_data.as_hdf(h5_path, mode=\'a\', group=\'tdms_data\')\n\n    for ((group, channel), expected_data) in expected_data.items():\n        h5_channel = h5[\'tdms_data\'][group][channel]\n        np.testing.assert_almost_equal(h5_channel[...], expected_data)\n    np.testing.assert_almost_equal(h5[\'preexisting_data\'], preexisting_data)\n    h5.close()\n'"
nptdms/test/test_pandas.py,9,"b'""""""Test exporting TDMS data to Pandas""""""\n\nfrom datetime import datetime\nimport numpy as np\nimport pytest\ntry:\n    import pandas\nexcept ImportError:\n    pytest.skip(""Skipping Pandas tests as Pandas is not installed"", allow_module_level=True)\n\nfrom nptdms.test import scenarios\nfrom nptdms.test.test_daqmx import daqmx_channel_metadata, daqmx_scaler_metadata\nfrom nptdms.test.util import (\n    GeneratedFile,\n    basic_segment,\n    channel_metadata,\n    channel_metadata_with_no_data,\n    string_hexlify,\n    segment_objects_metadata,\n    hexlify_value\n)\n\n\ndef assert_within_tol(a, b, tol=1.0e-10):\n    assert abs(a - b) < tol\n\n\ndef timed_segment():\n    """"""TDMS segment with one group and two channels,\n    each with time properties""""""\n\n    toc = (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"")\n    metadata = (\n        # Number of objects\n        ""03 00 00 00""\n        # Length of the first object path\n        ""08 00 00 00""\n        # Object path (/\'Group\')\n        ""2F 27 47 72""\n        ""6F 75 70 27""\n        # Raw data index\n        ""FF FF FF FF""\n        # Num properties\n        ""02 00 00 00""\n        # Name length\n        ""04 00 00 00""\n        # Property name (prop)\n        ""70 72 6F 70""\n        # Property data type (string)\n        ""20 00 00 00""\n        # Length of string value\n        ""05 00 00 00""\n        # Value\n        ""76 61 6C 75 65""\n        # Length of second property name\n        ""03 00 00 00""\n        # Property name (num)\n        ""6E 75 6D""\n        # Data type of property\n        ""03 00 00 00""\n        # Value\n        ""0A 00 00 00""\n        # Length of the second object path\n        ""13 00 00 00""\n        # Second object path (/\'Group\'/\'Channel1\')\n        ""2F 27 47 72""\n        ""6F 75 70 27""\n        ""2F 27 43 68""\n        ""61 6E 6E 65""\n        ""6C 31 27""\n        # Length of index information\n        ""14 00 00 00""\n        # Raw data data type\n        ""03 00 00 00""\n        # Dimension\n        ""01 00 00 00""\n        # Number of raw data values\n        ""02 00 00 00""\n        ""00 00 00 00""\n        # Number of properties\n        ""03 00 00 00""\n        # Set time properties for the first channel\n        ""0F 00 00 00"" +\n        string_hexlify(\'wf_start_offset\') +\n        ""0A 00 00 00"" +\n        hexlify_value(""<d"", 2.0) +\n        ""0C 00 00 00"" +\n        string_hexlify(\'wf_increment\') +\n        ""0A 00 00 00"" +\n        hexlify_value(""<d"", 0.1) +\n        ""0D 00 00 00"" +\n        string_hexlify(\'wf_start_time\') +\n        ""44 00 00 00"" +\n        hexlify_value(""<Q"", 0) +\n        hexlify_value(""<q"", 3524551547) +\n        # Length of the third object path\n        ""13 00 00 00""\n        # Third object path (/\'Group\'/\'Channel2\')\n        ""2F 27 47 72""\n        ""6F 75 70 27""\n        ""2F 27 43 68""\n        ""61 6E 6E 65""\n        ""6C 32 27""\n        # Length of index information\n        ""14 00 00 00""\n        # Raw data data type\n        ""03 00 00 00""\n        # Dimension\n        ""01 00 00 00""\n        # Number of data values\n        ""02 00 00 00""\n        ""00 00 00 00""\n        # Number of properties\n        ""03 00 00 00""\n        # Set time properties for the second channel\n        ""0F 00 00 00"" +\n        string_hexlify(\'wf_start_offset\') +\n        ""0A 00 00 00"" +\n        hexlify_value(""<d"", 2.0) +\n        ""0C 00 00 00"" +\n        string_hexlify(\'wf_increment\') +\n        ""0A 00 00 00"" +\n        hexlify_value(""<d"", 0.1) +\n        ""0D 00 00 00"" +\n        string_hexlify(\'wf_start_time\') +\n        ""44 00 00 00"" +\n        hexlify_value(""<Q"", 0) +\n        hexlify_value(""<q"", 3524551547))\n    data = (\n        # Data for segment\n        ""01 00 00 00""\n        ""02 00 00 00""\n        ""03 00 00 00""\n        ""04 00 00 00""\n    )\n    return toc, metadata, data\n\n\ndef test_file_as_dataframe():\n    """"""Test converting file to Pandas dataframe""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(*timed_segment())\n    tdms_data = test_file.load()\n\n    df = tdms_data.as_dataframe()\n\n    assert len(df) == 2\n    assert ""/\'Group\'/\'Channel1\'"" in df.keys()\n    assert ""/\'Group\'/\'Channel2\'"" in df.keys()\n\n    assert (df[""/\'Group\'/\'Channel1\'""] == [1, 2]).all()\n\n\ndef test_file_as_dataframe_without_time():\n    """"""Converting file to dataframe with time index should raise when\n    time properties aren\'t present""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_data = test_file.load()\n\n    with pytest.raises(KeyError):\n        tdms_data.as_dataframe(time_index=True)\n\n\ndef test_file_as_dataframe_with_time():\n    """"""Test converting file to Pandas dataframe with a time index""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(*timed_segment())\n    tdms_data = test_file.load()\n\n    df = tdms_data.as_dataframe(time_index=True)\n\n    assert len(df.index) == 2\n    assert_within_tol(df.index[0], 2.0)\n    assert_within_tol(df.index[1], 2.1)\n\n\ndef test_file_as_dataframe_with_absolute_time():\n    """"""Convert file to Pandas dataframe with absolute time index""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(*timed_segment())\n    tdms_data = test_file.load()\n\n    df = tdms_data.as_dataframe(time_index=True, absolute_time=True)\n\n    expected_start = datetime(2015, 9, 8, 10, 5, 49)\n    assert (df.index == expected_start)[0]\n\n\ndef test_group_as_dataframe():\n    """"""Convert a group to dataframe""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(*timed_segment())\n    tdms_data = test_file.load()\n\n    df = tdms_data[""Group""].as_dataframe()\n    assert len(df) == 2\n    assert len(df.keys()) == 2\n    assert ""Channel1"" in df.keys()\n    assert ""Channel2"" in df.keys()\n    assert (df[""Channel1""] == [1, 2]).all()\n    assert (df[""Channel2""] == [3, 4]).all()\n\n\ndef test_channel_as_dataframe():\n    """"""Convert a channel to dataframe""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(*timed_segment())\n    tdms_data = test_file.load()\n\n    df = tdms_data[""Group""][""Channel2""].as_dataframe()\n    assert len(df) == 2\n    assert len(df.keys()) == 1\n    assert ""/\'Group\'/\'Channel2\'"" in df.keys()\n    assert (df[""/\'Group\'/\'Channel2\'""] == [3, 4]).all()\n\n\ndef test_channel_as_dataframe_with_time():\n    """"""Convert a channel to dataframe with a time index""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(*timed_segment())\n    tdms_data = test_file.load()\n\n    df = tdms_data[""Group""][""Channel2""].as_dataframe(time_index=True)\n\n    assert len(df.index) == 2\n    assert_within_tol(df.index[0], 2.0)\n    assert_within_tol(df.index[1], 2.1)\n\n\ndef test_channel_as_dataframe_without_time():\n    """"""Converting channel to dataframe should work correctly""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_data = test_file.load()\n\n    df = tdms_data[""Group""][""Channel2""].as_dataframe()\n\n    assert len(df.index) == 2\n    assert len(df.values) == 2\n    assert_within_tol(df.index[0], 0)\n    assert_within_tol(df.index[1], 1)\n    assert_within_tol(df.values[0], 3.0)\n    assert_within_tol(df.values[1], 4.0)\n\n\ndef test_channel_as_dataframe_with_absolute_time():\n    """"""Convert channel to Pandas dataframe with absolute time index""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(*timed_segment())\n    tdms_data = test_file.load()\n\n    df = tdms_data[""Group""][""Channel1""].as_dataframe(time_index=True, absolute_time=True)\n\n    expected_start = datetime(2015, 9, 8, 10, 5, 49)\n    assert (df.index == expected_start)[0]\n\n\ndef test_channel_as_dataframe_with_raw_data():\n    """"""Convert channel to Pandas dataframe with absolute time index""""""\n\n    test_file, _ = scenarios.scaled_data().values\n    expected_raw_data = np.array([1, 2, 3, 4], dtype=np.int32)\n    tdms_data = test_file.load()\n\n    df = tdms_data[""group""][""channel1""].as_dataframe(scaled_data=False)\n\n    np.testing.assert_equal(df[""/\'group\'/\'channel1\'""], expected_raw_data)\n\n\ndef test_raw_daqmx_channel_export():\n    """""" Test exporting raw daqmx data for a channel\n    """"""\n\n    scaler_metadata = [\n        daqmx_scaler_metadata(0, 3, 0),\n        daqmx_scaler_metadata(1, 3, 2)]\n    metadata = segment_objects_metadata(\n        daqmx_channel_metadata(""Channel1"", 4, [4], scaler_metadata))\n    data = (\n        # Data for segment\n        ""01 00""\n        ""11 00""\n        ""02 00""\n        ""12 00""\n        ""03 00""\n        ""13 00""\n        ""04 00""\n        ""14 00""\n    )\n\n    test_file = GeneratedFile()\n    segment_toc = (\n        ""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"", ""kTocDAQmxRawData"")\n    test_file.add_segment(segment_toc, metadata, data)\n    tdms_data = test_file.load()\n    channel = tdms_data[""Group""][""Channel1""]\n\n    dataframe = channel.as_dataframe(scaled_data=False)\n    expected_data = {\n        0: np.array([1, 2, 3, 4], dtype=np.int16),\n        1: np.array([17, 18, 19, 20], dtype=np.int16),\n    }\n    assert dataframe[""/\'Group\'/\'Channel1\'[0]""].dtype == np.int16\n    assert dataframe[""/\'Group\'/\'Channel1\'[1]""].dtype == np.int16\n    np.testing.assert_equal(dataframe[""/\'Group\'/\'Channel1\'[0]""], expected_data[0])\n    np.testing.assert_equal(dataframe[""/\'Group\'/\'Channel1\'[1]""], expected_data[1])\n\n\ndef test_export_with_empty_channels():\n    """"""Convert a group to dataframe when a channel has empty data and void data type""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", 3, 2),\n            channel_metadata_with_no_data(""/\'group\'/\'channel2\'""),\n        ),\n        ""01 00 00 00 02 00 00 00""\n    )\n\n    tdms_data = test_file.load()\n\n    df = tdms_data[""group""].as_dataframe()\n    assert len(df) == 2\n    assert len(df.keys()) == 2\n    assert ""channel1"" in df.keys()\n    assert ""channel2"" in df.keys()\n    assert (df[""channel1""] == [1, 2]).all()\n    assert len(df[""channel2""]) == 2\n    assert np.isnan(df[""channel2""]).all()\n'"
nptdms/test/test_scaling.py,68,"b'""""""Test scaling data""""""\n\nimport numpy as np\nimport pytest\n\nfrom nptdms import types\nfrom nptdms.scaling import get_scaling\n\ntry:\n    import thermocouples_reference\nexcept ImportError:\n    thermocouples_reference = None\ntry:\n    import scipy\nexcept ImportError:\n    scipy = None\n\n\ndef test_unsupported_scaling_type():\n    """"""Raw data is returned unscaled when the scaling type is unsupported.\n    """"""\n\n    properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""UnknownScaling""\n    }\n    scaling = get_scaling(properties, {}, {})\n\n    assert scaling is None\n\n\ndef test_linear_scaling():\n    """"""Test linear scaling""""""\n\n    data = StubTdmsData(np.array([1, 2, 3], dtype=np.dtype(\'int32\')))\n    expected_scaled_data = np.array([12.0, 14.0, 16.0])\n\n    properties = {\n        ""NI_Scaling_Status"": ""unscaled"",\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""Linear"",\n        ""NI_Scale[0]_Linear_Slope"": 2.0,\n        ""NI_Scale[0]_Linear_Y_Intercept"": 10.0\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(data)\n\n    assert scaling.get_dtype(types.Int32, None) == np.dtype(\'float64\')\n    assert scaled_data.dtype == np.dtype(\'float64\')\n    np.testing.assert_almost_equal(scaled_data, expected_scaled_data)\n\n\ndef test_polynomial_scaling():\n    """"""Test polynomial scaling""""""\n\n    data = StubTdmsData(np.array([1, 2, 3], dtype=np.dtype(\'int32\')))\n    expected_scaled_data = np.array([16.0, 44.0, 112.0])\n\n    properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""Polynomial"",\n        ""NI_Scale[0]_Polynomial_Coefficients[0]"": 10.0,\n        ""NI_Scale[0]_Polynomial_Coefficients[1]"": 1.0,\n        ""NI_Scale[0]_Polynomial_Coefficients[2]"": 2.0,\n        ""NI_Scale[0]_Polynomial_Coefficients[3]"": 3.0,\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(data)\n\n    assert scaling.get_dtype(types.Int32, None) == np.dtype(\'float64\')\n    assert scaled_data.dtype == np.dtype(\'float64\')\n    np.testing.assert_almost_equal(scaled_data, expected_scaled_data)\n\n\ndef test_polynomial_scaling_with_no_coefficients():\n    """"""Test polynomial scaling when there are no coefficients, so data should be all zero\n    """"""\n    data = StubTdmsData(np.array([1.0, 2.0, 3.0]))\n    expected_scaled_data = np.array([0.0, 0.0, 0.0])\n\n    properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""Polynomial"",\n        ""NI_Scale[0]_Polynomial_Coefficients_Size"": 0\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(data)\n\n    np.testing.assert_almost_equal(scaled_data, expected_scaled_data)\n\n\ndef test_polynomial_scaling_with_3_coefficients():\n    """"""Test polynomial scaling""""""\n\n    data = StubTdmsData(np.array([1, 2, 3], dtype=np.dtype(\'int32\')))\n    expected_scaled_data = np.array([13.0, 20.0, 31.0])\n\n    properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""Polynomial"",\n        ""NI_Scale[0]_Polynomial_Coefficients_Size"": 3,\n        ""NI_Scale[0]_Polynomial_Coefficients[0]"": 10.0,\n        ""NI_Scale[0]_Polynomial_Coefficients[1]"": 1.0,\n        ""NI_Scale[0]_Polynomial_Coefficients[2]"": 2.0,\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(data)\n\n    np.testing.assert_almost_equal(scaled_data, expected_scaled_data)\n\n\n@pytest.mark.parametrize(\n    ""resistance_configuration,lead_resistance,expected_data"",\n    [\n        (2, 0.0, [1256.89628, 1712.83429]),\n        (2, 100.0, [557.6879004146, 882.7374139697]),\n        (3, 0.0, [1256.89628, 1712.83429]),\n        (3, 100.0, [882.7374139697, 1256.896275222]),\n        (4, 0.0, [1256.89628, 1712.83429]),\n        (4, 100.0, [1256.89628, 1712.83429]),\n    ]\n)\ndef test_rtd_scaling(resistance_configuration, lead_resistance, expected_data):\n    """"""Test RTD scaling""""""\n\n    data = StubTdmsData(np.array([0.5, 0.6]))\n\n    properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""RTD"",\n        ""NI_Scale[0]_RTD_Current_Excitation"": 0.001,\n        ""NI_Scale[0]_RTD_R0_Nominal_Resistance"": 100.0,\n        ""NI_Scale[0]_RTD_A"": 0.0039083,\n        ""NI_Scale[0]_RTD_B"": -5.775e-07,\n        ""NI_Scale[0]_RTD_C"": -4.183e-12,\n        ""NI_Scale[0]_RTD_Lead_Wire_Resistance"": lead_resistance,\n        ""NI_Scale[0]_RTD_Resistance_Configuration"": resistance_configuration,\n        ""NI_Scale[0]_RTD_Input_Source"": 0xFFFFFFFF,\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(data)\n\n    assert scaling.get_dtype(types.DoubleFloat, None) == np.dtype(\'float64\')\n    np.testing.assert_almost_equal(scaled_data, expected_data, decimal=3)\n\n\ndef test_rtd_scaling_with_negative_temperature():\n    """""" Test RTD scaling with negative temperature values, which requires\n        solving the full quartic Callendar-Van Dusen equation\n    """"""\n    data = StubTdmsData(np.array([\n        0.08, 0.09, 0.095, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.22, 0.24]))\n    expected_data = np.array([\n        -50.77114, -25.48835, -12.76894, -0., 51.56605, 103.94273,\n        157.1695, 211.28915, 266.34819, 322.3973, 379.49189])\n\n    properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""RTD"",\n        ""NI_Scale[0]_RTD_Current_Excitation"": 0.001,\n        ""NI_Scale[0]_RTD_R0_Nominal_Resistance"": 100.0,\n        ""NI_Scale[0]_RTD_A"": 0.0039083,\n        ""NI_Scale[0]_RTD_B"": -5.775e-07,\n        ""NI_Scale[0]_RTD_C"": -4.183e-12,\n        ""NI_Scale[0]_RTD_Lead_Wire_Resistance"": 0,\n        ""NI_Scale[0]_RTD_Resistance_Configuration"": 2,\n        ""NI_Scale[0]_RTD_Input_Source"": 0xFFFFFFFF,\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(data)\n\n    assert scaling.get_dtype(types.DoubleFloat, None) == np.dtype(\'float64\')\n    assert scaled_data.dtype == np.dtype(\'float64\')\n    np.testing.assert_almost_equal(scaled_data, expected_data, decimal=5)\n\n\ndef test_table_scaling():\n    """"""Test table scaling""""""\n\n    data = StubTdmsData(np.array([0.5, 1.0, 1.5, 2.5, 3.0, 3.5]))\n    expected_scaled_data = np.array([2.0, 2.0, 3.0, 6.0, 8.0, 8.0])\n\n    # The scaled values are actually the range of inputs into the scaling,\n    # which are mapped to the pre-scaled values. This makes no sense but\n    # matches the behaviour of the Excel TDMS plugin.\n\n    properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""Table"",\n        ""NI_Scale[0]_Table_Scaled_Values_Size"": 3,\n        ""NI_Scale[0]_Table_Scaled_Values[0]"": 1.0,\n        ""NI_Scale[0]_Table_Scaled_Values[1]"": 2.0,\n        ""NI_Scale[0]_Table_Scaled_Values[2]"": 3.0,\n        ""NI_Scale[0]_Table_Pre_Scaled_Values_Size"": 3,\n        ""NI_Scale[0]_Table_Pre_Scaled_Values[0]"": 2.0,\n        ""NI_Scale[0]_Table_Pre_Scaled_Values[1]"": 4.0,\n        ""NI_Scale[0]_Table_Pre_Scaled_Values[2]"": 8.0,\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(data)\n\n    assert scaling.get_dtype(types.DoubleFloat, None) == np.dtype(\'float64\')\n    np.testing.assert_almost_equal(scaled_data, expected_scaled_data)\n\n\ndef test_add_scaling():\n    """""" Test scaling that adds two input scalings""""""\n\n    scaler_data = StubDaqmxData({\n        0: np.array([1, 2, 3], dtype=np.dtype(\'int32\')),\n        1: np.array([2, 4, 6], dtype=np.dtype(\'uint32\')),\n    })\n    expected_scaled_data = np.array([3.0, 6.0, 9.0])\n\n    properties = {\n        ""NI_Number_Of_Scales"": 3,\n        ""NI_Scale[2]_Scale_Type"": ""Add"",\n        ""NI_Scale[2]_Add_Left_Operand_Input_Source"": 0,\n        ""NI_Scale[2]_Add_Right_Operand_Input_Source"": 1,\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(scaler_data)\n\n    assert scaling.get_dtype(None, {0: types.Int32, 1: types.Uint32}) == np.dtype(\'int64\')\n    assert scaled_data.dtype == np.dtype(\'int64\')\n    np.testing.assert_almost_equal(scaled_data, expected_scaled_data)\n\n\ndef test_add_scaling_with_default_inputs():\n    """""" Test scaling that adds two input scalings""""""\n\n    data = StubTdmsData(np.array([1, 2, 3], dtype=np.dtype(\'int32\')))\n    expected_scaled_data = np.array([2, 4, 6])\n\n    properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""Add"",\n        ""NI_Scale[0]_Add_Left_Operand_Input_Source"": 0xFFFFFFFF,\n        ""NI_Scale[0]_Add_Right_Operand_Input_Source"": 0xFFFFFFFF,\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(data)\n\n    assert scaling.get_dtype(types.Int32, None) == np.dtype(\'int32\')\n    assert scaled_data.dtype == np.dtype(\'int32\')\n    np.testing.assert_equal(expected_scaled_data, scaled_data)\n\n\ndef test_subtract_scaling():\n    """""" Test scaling that subtracts an input scaling from another""""""\n\n    # This behaves the opposite to what you\'d expect, the left operand\n    # is subtracted from the right operand.\n    scaler_data = StubDaqmxData({\n        0: np.array([1, 2, 3], dtype=np.dtype(\'int32\')),\n        1: np.array([2, 4, 6], dtype=np.dtype(\'uint32\')),\n    })\n    expected_scaled_data = np.array([1.0, 2.0, 3.0])\n\n    properties = {\n        ""NI_Number_Of_Scales"": 3,\n        ""NI_Scale[2]_Scale_Type"": ""Subtract"",\n        ""NI_Scale[2]_Subtract_Left_Operand_Input_Source"": 0,\n        ""NI_Scale[2]_Subtract_Right_Operand_Input_Source"": 1,\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(scaler_data)\n\n    assert scaling.get_dtype(None, {0: types.Int32, 1: types.Uint32}) == np.dtype(\'int64\')\n    assert scaled_data.dtype == np.dtype(\'int64\')\n    np.testing.assert_almost_equal(scaled_data, expected_scaled_data)\n\n\n@pytest.mark.skipif(thermocouples_reference is None, reason=""thermocouples_reference is not installed"")\n@pytest.mark.skipif(scipy is None, reason=""scipy is not installed"")\ndef test_thermocouple_scaling_voltage_to_temperature():\n    """"""Test thermocouple scaling from a voltage in uV to temperature""""""\n\n    data = StubTdmsData(np.array([0.0, 10.0, 100.0, 1000.0]))\n    expected_scaled_data = np.array([0.0, 0.2534448,  2.5309141, 24.9940185])\n\n    properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""Thermocouple"",\n        ""NI_Scale[0]_Thermocouple_Thermocouple_Type"": 10073,\n        ""NI_Scale[0]_Thermocouple_Scaling_Direction"": 0,\n        ""NI_Scale[0]_Thermocouple_Input_Source"": 0xFFFFFFFF,\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(data)\n\n    np.testing.assert_almost_equal(\n        scaled_data, expected_scaled_data, decimal=3)\n\n\n@pytest.mark.skipif(thermocouples_reference is None, reason=""thermocouples_reference is not installed"")\ndef test_thermocouple_scaling_temperature_to_voltage():\n    """"""Test thermocouple scaling from a temperature to voltage in uV""""""\n\n    data = StubTdmsData(np.array([0.0, 10.0, 50.0, 100.0]))\n    expected_scaled_data = np.array([\n        0.0, 396.8619078, 2023.0778862, 4096.2302187])\n\n    properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""Thermocouple"",\n        ""NI_Scale[0]_Thermocouple_Thermocouple_Type"": 10073,\n        ""NI_Scale[0]_Thermocouple_Scaling_Direction"": 1,\n        ""NI_Scale[0]_Thermocouple_Input_Source"": 0xFFFFFFFF,\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(data)\n\n    np.testing.assert_almost_equal(\n        scaled_data, expected_scaled_data, decimal=3)\n\n\n@pytest.mark.parametrize(\n    ""resistance_configuration,lead_resistance,expected_data"",\n    [\n        (2, 0.0, [287.1495569816, 290.71633623, 294.4862276706]),\n        (2, 100.0, [287.1495569816, 290.71633623, 294.4862276706]),\n        (3, 0.0, [287.1495569816, 290.71633623, 294.4862276706]),\n        (3, 100.0, [287.4248927942, 291.0482875767, 294.8892119392]),\n        (4, 0.0, [287.1495569816, 290.71633623, 294.4862276706]),\n        (4, 100.0, [287.1495569816, 290.71633623, 294.4862276706]),\n    ]\n)\ndef test_thermistor_scaling_with_voltage_excitation(\n        resistance_configuration, lead_resistance, expected_data):\n    data = StubTdmsData(np.array([1.1, 1.0, 0.9]))\n\n    properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""Thermistor"",\n        ""NI_Scale[0]_Thermistor_Resistance_Configuration"": resistance_configuration,\n        ""NI_Scale[0]_Thermistor_Excitation_Type"": 10322,\n        ""NI_Scale[0]_Thermistor_Excitation_Value"": 2.5,\n        ""NI_Scale[0]_Thermistor_R1_Reference_Resistance"": 10000.0,\n        ""NI_Scale[0]_Thermistor_Lead_Wire_Resistance"": lead_resistance,\n        ""NI_Scale[0]_Thermistor_A"": 0.0012873851,\n        ""NI_Scale[0]_Thermistor_B"": 0.00023575235,\n        ""NI_Scale[0]_Thermistor_C"": 9.497806e-8,\n        ""NI_Scale[0]_Thermistor_Temperature_Offset"": 1.0,\n        ""NI_Scale[0]_Thermistor_Input_Source"": 0xFFFFFFFF,\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(data)\n\n    np.testing.assert_almost_equal(scaled_data, expected_data)\n\n\n@pytest.mark.parametrize(\n    ""resistance_configuration,lead_resistance,expected_data"",\n    [\n        (2, 0.0, [335.5876272527, 338.303823856, 341.3530400858]),\n        (2, 100.0, [341.3530400858, 344.8212218133, 348.831282405]),\n        (3, 0.0, [335.5876272527, 338.303823856, 341.3530400858]),\n        (3, 100.0, [338.303823856, 341.3530400858, 344.8212218133]),\n        (4, 0.0, [335.5876272527, 338.303823856, 341.3530400858]),\n        (4, 100.0, [335.5876272527, 338.303823856, 341.3530400858]),\n    ]\n)\ndef test_thermistor_scaling_with_current_excitation(\n        resistance_configuration, lead_resistance, expected_data):\n    data = StubTdmsData(np.array([1.1, 1.0, 0.9]))\n\n    properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""Thermistor"",\n        ""NI_Scale[0]_Thermistor_Resistance_Configuration"": resistance_configuration,\n        ""NI_Scale[0]_Thermistor_Excitation_Type"": 10134,\n        ""NI_Scale[0]_Thermistor_Excitation_Value"": 1.0e-3,\n        ""NI_Scale[0]_Thermistor_R1_Reference_Resistance"": 0.0,\n        ""NI_Scale[0]_Thermistor_Lead_Wire_Resistance"": lead_resistance,\n        ""NI_Scale[0]_Thermistor_A"": 0.0012873851,\n        ""NI_Scale[0]_Thermistor_B"": 0.00023575235,\n        ""NI_Scale[0]_Thermistor_C"": 9.497806e-8,\n        ""NI_Scale[0]_Thermistor_Temperature_Offset"": 1.0,\n        ""NI_Scale[0]_Thermistor_Input_Source"": 0xFFFFFFFF,\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(data)\n\n    np.testing.assert_almost_equal(scaled_data, expected_data)\n\n\ndef test_thermistor_scaling_with_invalid_excitation_type():\n    data = StubTdmsData(np.array([1.1, 1.0, 0.9]))\n\n    properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""Thermistor"",\n        ""NI_Scale[0]_Thermistor_Resistance_Configuration"": 3,\n        ""NI_Scale[0]_Thermistor_Excitation_Type"": 12345,\n        ""NI_Scale[0]_Thermistor_Excitation_Value"": 2.5,\n        ""NI_Scale[0]_Thermistor_R1_Reference_Resistance"": 10000.0,\n        ""NI_Scale[0]_Thermistor_Lead_Wire_Resistance"": 0.0,\n        ""NI_Scale[0]_Thermistor_A"": 0.0012873851,\n        ""NI_Scale[0]_Thermistor_B"": 0.00023575235,\n        ""NI_Scale[0]_Thermistor_C"": 9.497806e-8,\n        ""NI_Scale[0]_Thermistor_Temperature_Offset"": 1.0,\n        ""NI_Scale[0]_Thermistor_Input_Source"": 0xFFFFFFFF,\n    }\n    scaling = get_scaling(properties, {}, {})\n    with pytest.raises(ValueError) as exc_info:\n        _ = scaling.scale(data)\n    assert ""Invalid excitation type: 12345"" in str(exc_info.value)\n\n\ndef test_multiple_scalings_applied_in_order():\n    """"""Test all scalings applied from multiple scalings\n    """"""\n\n    data = StubTdmsData(np.array([1.0, 2.0, 3.0]))\n    expected_scaled_data = np.array([21.0, 27.0, 33.0])\n\n    properties = {\n        ""NI_Number_Of_Scales"": 3,\n        ""NI_Scaling_Status"": ""unscaled"",\n        ""NI_Scale[0]_Scale_Type"": ""Linear"",\n        ""NI_Scale[0]_Linear_Slope"": 1.0,\n        ""NI_Scale[0]_Linear_Y_Intercept"": 1.0,\n        ""NI_Scale[0]_Linear_Input_Source"": 0xFFFFFFFF,\n        ""NI_Scale[1]_Scale_Type"": ""Linear"",\n        ""NI_Scale[1]_Linear_Slope"": 2.0,\n        ""NI_Scale[1]_Linear_Y_Intercept"": 2.0,\n        ""NI_Scale[1]_Linear_Input_Source"": 0,\n        ""NI_Scale[2]_Scale_Type"": ""Linear"",\n        ""NI_Scale[2]_Linear_Slope"": 3.0,\n        ""NI_Scale[2]_Linear_Y_Intercept"": 3.0,\n        ""NI_Scale[2]_Linear_Input_Source"": 1,\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(data)\n\n    np.testing.assert_almost_equal(scaled_data, expected_scaled_data)\n\n\ndef test_multiple_scalings_but_all_with_raw_data_input():\n    """"""Test that only the last scaling is applied from multiple scalings\n       when it has the raw data as the input source\n    """"""\n\n    data = StubTdmsData(np.array([1.0, 2.0, 3.0]))\n    expected_scaled_data = np.array([6.0, 9.0, 12.0])\n\n    properties = {\n        ""NI_Number_Of_Scales"": 3,\n        ""NI_Scale[0]_Scale_Type"": ""Linear"",\n        ""NI_Scale[0]_Linear_Slope"": 1.0,\n        ""NI_Scale[0]_Linear_Y_Intercept"": 1.0,\n        ""NI_Scale[0]_Linear_Input_Source"": 0xFFFFFFFF,\n        ""NI_Scale[1]_Scale_Type"": ""Linear"",\n        ""NI_Scale[1]_Linear_Slope"": 2.0,\n        ""NI_Scale[1]_Linear_Y_Intercept"": 2.0,\n        ""NI_Scale[1]_Linear_Input_Source"": 0xFFFFFFFF,\n        ""NI_Scale[2]_Scale_Type"": ""Linear"",\n        ""NI_Scale[2]_Linear_Slope"": 3.0,\n        ""NI_Scale[2]_Linear_Y_Intercept"": 3.0,\n        ""NI_Scale[2]_Linear_Input_Source"": 0xFFFFFFFF,\n    }\n    scaling = get_scaling(properties, {}, {})\n    scaled_data = scaling.scale(data)\n\n    np.testing.assert_almost_equal(scaled_data, expected_scaled_data)\n\n\ndef test_scaling_from_group():\n    """"""Test linear scaling in a group""""""\n\n    data = StubTdmsData(np.array([1.0, 2.0, 3.0]))\n    expected_scaled_data = np.array([12.0, 14.0, 16.0])\n\n    group_properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""Linear"",\n        ""NI_Scale[0]_Linear_Slope"": 2.0,\n        ""NI_Scale[0]_Linear_Y_Intercept"": 10.0,\n    }\n    scaling = get_scaling({}, group_properties, {})\n    scaled_data = scaling.scale(data)\n\n    np.testing.assert_almost_equal(scaled_data, expected_scaled_data)\n\n\ndef test_scaling_from_root():\n    """"""Test linear scaling in the root object""""""\n\n    data = StubTdmsData(np.array([1.0, 2.0, 3.0]))\n    expected_scaled_data = np.array([12.0, 14.0, 16.0])\n\n    root_properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""Linear"",\n        ""NI_Scale[0]_Linear_Slope"": 2.0,\n        ""NI_Scale[0]_Linear_Y_Intercept"": 10.0,\n    }\n    scaling = get_scaling({}, {}, root_properties)\n    scaled_data = scaling.scale(data)\n\n    np.testing.assert_almost_equal(scaled_data, expected_scaled_data)\n\n\ndef test_scaling_status_scaled():\n    """""" When the scaling status is scaled, data is already scaled so scaling should not be applied\n    """"""\n    properties = {\n        ""NI_Number_Of_Scales"": 1,\n        ""NI_Scale[0]_Scale_Type"": ""Linear"",\n        ""NI_Scale[0]_Linear_Slope"": 2.0,\n        ""NI_Scale[0]_Linear_Y_Intercept"": 10.0,\n        ""NI_Scaling_Status"": ""scaled"",\n    }\n    scaling = get_scaling(properties, {}, {})\n    assert scaling is None\n\n\nclass StubTdmsData(object):\n    def __init__(self, data):\n        self.data = data\n        self.scaler_data = None\n\n\nclass StubDaqmxData(object):\n    def __init__(self, scaler_data):\n        self.data = None\n        self.scaler_data = scaler_data\n'"
nptdms/test/test_tdms_file.py,3,"b'""""""Test reading of example TDMS files""""""\n\nfrom collections import defaultdict\nimport logging\nimport os\nimport sys\nfrom shutil import copyfile\nimport tempfile\nimport weakref\nfrom hypothesis import (assume, given, example, settings, strategies)\nimport numpy as np\nimport pytest\nfrom nptdms import TdmsFile\nfrom nptdms.log import log_manager\nfrom nptdms.test.util import (\n    BytesIoTestFile,\n    GeneratedFile,\n    basic_segment,\n    channel_metadata,\n    compare_arrays,\n    hexlify_value,\n    segment_objects_metadata,\n    string_hexlify,\n)\nfrom nptdms.test import scenarios\n\n\n@pytest.mark.parametrize(""test_file,expected_data"", scenarios.get_scenarios())\ndef test_read_channel_data(test_file, expected_data):\n    """"""Test reading data""""""\n\n    with test_file.get_tempfile() as temp_file:\n        tdms_data = TdmsFile.read(temp_file.file)\n\n    for ((group, channel), expected_data) in expected_data.items():\n        channel_obj = tdms_data[group][channel]\n        actual_data = channel_obj.data\n        assert actual_data.dtype == expected_data.dtype\n        assert channel_obj.dtype == expected_data.dtype\n        compare_arrays(actual_data, expected_data)\n\n\n@pytest.mark.parametrize(""test_file,expected_data"", scenarios.get_scenarios())\ndef test_lazily_read_channel_data(test_file, expected_data):\n    """"""Test reading channel data lazily""""""\n\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for ((group, channel), expected_data) in expected_data.items():\n                actual_data = tdms_file[group][channel].read_data()\n                assert actual_data.dtype == expected_data.dtype\n                compare_arrays(actual_data, expected_data)\n\n\ndef test_lazily_read_raw_channel_data():\n    """"""Test reading raw channel data lazily""""""\n\n    test_file, expected_data = scenarios.single_segment_with_one_channel().values\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for ((group, channel), expected_data) in expected_data.items():\n                actual_data = tdms_file[group][channel].read_data(scaled=False)\n                assert actual_data.dtype == expected_data.dtype\n                compare_arrays(actual_data, expected_data)\n\n\ndef test_lazily_read_channel_data_with_file_path():\n    """"""Test reading channel data lazily after initialising with a file path\n    """"""\n    test_file, expected_data = scenarios.single_segment_with_one_channel().values\n    temp_file = test_file.get_tempfile(delete=False)\n    try:\n        temp_file.file.close()\n        with TdmsFile.open(temp_file.name) as tdms_file:\n            for ((group, channel), expected_data) in expected_data.items():\n                actual_data = tdms_file[group][channel].read_data()\n                assert actual_data.dtype == expected_data.dtype\n                compare_arrays(actual_data, expected_data)\n    finally:\n        os.remove(temp_file.name)\n\n\n@pytest.mark.filterwarnings(\'ignore:.* is deprecated\')\ndef test_lazily_read_channel_data_with_channel_data_method():\n    """"""Test reading channel data lazily using the channel_data method of TdmsFile\n    """"""\n    test_file, expected_data = scenarios.single_segment_with_two_channels().values\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for ((group, channel), expected_data) in expected_data.items():\n                actual_data = tdms_file.channel_data(group, channel)\n                assert actual_data.dtype == expected_data.dtype\n                np.testing.assert_almost_equal(actual_data, expected_data)\n\n\n@given(offset=strategies.integers(0, 100), length=strategies.integers(0, 100))\n@example(offset=0, length=0)\n@example(offset=0, length=100)\n@example(offset=0, length=5)\n@example(offset=0, length=10)\ndef test_reading_subset_of_data(offset, length):\n    channel_data = np.arange(0, 100, 1, dtype=np.int32)\n    # Split data into different sized segments\n    segment_data = [\n        channel_data[0:10],\n        channel_data[10:20],\n        channel_data[20:60],\n        channel_data[60:80],\n        channel_data[80:90],\n        channel_data[90:100],\n    ]\n    hex_segment_data = [\n        """".join(hexlify_value(\'<i\', x) for x in data) for data in segment_data]\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", 3, 5),\n        ),\n        hex_segment_data[0]\n    )\n    for hex_data in hex_segment_data[1:]:\n        test_file.add_segment((""kTocRawData"", ), """", hex_data)\n\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            channel_subset = tdms_file[\'group\'][\'channel1\'].read_data(offset, length)\n            expected_data = channel_data[offset:offset + length]\n            assert len(channel_subset) == len(expected_data)\n            np.testing.assert_equal(channel_subset, expected_data)\n\n\n@pytest.mark.parametrize(""test_file,expected_data"", scenarios.get_scenarios())\n@given(offset=strategies.integers(0, 10), length=strategies.integers(0, 10))\ndef test_reading_subset_of_data_for_scenario(test_file, expected_data, offset, length):\n    """"""Test reading a subset of a channel\'s data\n    """"""\n    assume(any(offset <= len(d) for d in expected_data.values()))\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for ((group, channel), expected_data) in expected_data.items():\n                actual_data = tdms_file[group][channel].read_data(offset, length)\n                compare_arrays(actual_data, expected_data[offset:offset + length])\n\n\n@pytest.mark.parametrize(""test_file,expected_data"", scenarios.get_scenarios())\ndef test_stream_data_chunks(test_file, expected_data):\n    """"""Test streaming chunks of data from a TDMS file\n    """"""\n    data_arrays = defaultdict(list)\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for chunk in tdms_file.data_chunks():\n                for group in chunk.groups():\n                    for channel in group.channels():\n                        key = (group.name, channel.name)\n                        assert channel.offset == len(data_arrays[key])\n                        data_arrays[key].extend(channel[:])\n\n    for ((group, channel), expected_data) in expected_data.items():\n        actual_data = data_arrays[(group, channel)]\n        compare_arrays(actual_data, expected_data)\n\n\ndef test_indexing_and_iterating_data_chunks():\n    """"""Test streaming chunks of data from a TDMS file and indexing into chunks\n    """"""\n    test_file, expected_data = scenarios.single_segment_with_two_channels().values\n    data_arrays = defaultdict(list)\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for chunk in tdms_file.data_chunks():\n                for (group, channel) in expected_data.keys():\n                    key = (group, channel)\n                    channel_chunk = chunk[group][channel]\n                    data_arrays[key].extend(list(channel_chunk))\n\n    for ((group, channel), expected_data) in expected_data.items():\n        actual_data = data_arrays[(group, channel)]\n        compare_arrays(actual_data, expected_data)\n\n\n@pytest.mark.parametrize(""test_file,expected_data"", scenarios.get_scenarios())\ndef test_stream_channel_data_chunks(test_file, expected_data):\n    """"""Test streaming chunks of data for a single channel from a TDMS file\n    """"""\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for ((group, channel), expected_channel_data) in expected_data.items():\n                actual_data = []\n                for chunk in tdms_file[group][channel].data_chunks():\n                    assert chunk.offset == len(actual_data)\n                    actual_data.extend(chunk[:])\n                compare_arrays(actual_data, expected_channel_data)\n\n\ndef test_iterate_channel_data_in_open_mode():\n    """"""Test iterating over channel data after opening a file without reading data\n    """"""\n    test_file, expected_data = scenarios.chunked_segment().values\n\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for ((group, channel), expected_channel_data) in expected_data.items():\n                actual_data = []\n                for value in tdms_file[group][channel]:\n                    actual_data.append(value)\n                compare_arrays(actual_data, expected_channel_data)\n\n\ndef test_iterate_channel_data_in_read_mode():\n    """"""Test iterating over channel data after reading all data\n    """"""\n    test_file, expected_data = scenarios.chunked_segment().values\n\n    with test_file.get_tempfile() as temp_file:\n        tdms_file = TdmsFile.read(temp_file.file)\n        for ((group, channel), expected_channel_data) in expected_data.items():\n            actual_data = []\n            for value in tdms_file[group][channel]:\n                actual_data.append(value)\n            compare_arrays(actual_data, expected_channel_data)\n\n\ndef test_iterate_file_and_groups():\n    """""" Test iterating over TdmsFile and TdmsGroup uses key values\n    """"""\n    test_file, expected_data = scenarios.chunked_segment().values\n\n    with test_file.get_tempfile() as temp_file:\n        tdms_file = TdmsFile.read(temp_file.file)\n        for group_name in tdms_file:\n            group = tdms_file[group_name]\n            for channel_name in group:\n                channel = group[channel_name]\n                expected_channel_data = expected_data[(group_name, channel_name)]\n                compare_arrays(channel.data, expected_channel_data)\n\n\ndef test_indexing_channel_after_read_data():\n    """""" Test indexing into a channel after reading all data\n    """"""\n    test_file, expected_data = scenarios.chunked_segment().values\n    with test_file.get_tempfile() as temp_file:\n        tdms_file = TdmsFile.read(temp_file.file)\n    for ((group, channel), expected_channel_data) in expected_data.items():\n        channel_object = tdms_file[group][channel]\n        assert channel_object[0] == expected_channel_data[0]\n        compare_arrays(channel_object[:], expected_channel_data)\n\n\n@given(index=strategies.integers(0, 7))\ndef test_indexing_channel_with_integer(index):\n    """""" Test indexing into a channel with an integer index\n    """"""\n    test_file, expected_data = scenarios.chunked_segment().values\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for ((group, channel), expected_channel_data) in expected_data.items():\n                channel_object = tdms_file[group][channel]\n                assert channel_object[index] == expected_channel_data[index]\n\n\ndef test_indexing_channel_with_integer_and_caching():\n    """""" Test indexing into a channel with an integer index, reusing the same file to test caching\n    """"""\n    test_file, expected_data = scenarios.chunked_segment().values\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for ((group, channel), expected_channel_data) in expected_data.items():\n                channel_object = tdms_file[group][channel]\n                values = []\n                for i in range(len(channel_object)):\n                    values.append(channel_object[i])\n                compare_arrays(values, expected_channel_data)\n\n\n@given(index=strategies.integers(0, 1))\ndef test_indexing_timestamp_channel_with_integer(index):\n    """""" Test indexing into a timestamp data channel with an integer index\n    """"""\n    test_file, expected_data = scenarios.timestamp_data().values\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for ((group, channel), expected_channel_data) in expected_data.items():\n                channel_object = tdms_file[group][channel]\n                assert channel_object[index] == expected_channel_data[index]\n\n\ndef test_indexing_scaled_channel_with_integer():\n    """""" Test indexing into a channel with an integer index when the channel is scaled\n    """"""\n    test_file, expected_data = scenarios.scaled_data().values\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for ((group, channel), expected_channel_data) in expected_data.items():\n                channel_object = tdms_file[group][channel]\n                values = []\n                for i in range(len(channel_object)):\n                    values.append(channel_object[i])\n                compare_arrays(values, expected_channel_data)\n\n\ndef test_indexing_channel_with_ellipsis():\n    """""" Test indexing into a channel with ellipsis returns all data\n    """"""\n    test_file, expected_data = scenarios.chunked_segment().values\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for ((group, channel), expected_channel_data) in expected_data.items():\n                channel_object = tdms_file[group][channel]\n                compare_arrays(channel_object[...], expected_channel_data)\n\n\n@pytest.fixture(scope=""module"")\ndef opened_tdms_file():\n    """""" Allow re-use of an opened TDMS file\n    """"""\n    test_file, expected_data = scenarios.chunked_segment().values\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            yield tdms_file, expected_data\n\n\n@given(\n    start=strategies.integers(-10, 10) | strategies.none(),\n    stop=strategies.integers(-10, 10) | strategies.none(),\n    step=strategies.integers(-5, 5).filter(lambda i: i != 0) | strategies.none(),\n)\n@settings(max_examples=1000)\ndef test_indexing_channel_with_slice(opened_tdms_file, start, stop, step):\n    """""" Test indexing into a channel with a slice\n    """"""\n    tdms_file, expected_data = opened_tdms_file\n    for ((group, channel), expected_channel_data) in expected_data.items():\n        channel_object = tdms_file[group][channel]\n        compare_arrays(channel_object[start:stop:step], expected_channel_data[start:stop:step])\n\n\n@pytest.mark.parametrize(\'index\', [-9, 8])\ndef test_indexing_channel_with_invalid_integer_raises_error(index):\n    """""" Test indexing into a channel with an invalid integer index\n    """"""\n    test_file, expected_data = scenarios.chunked_segment().values\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for ((group, channel), expected_channel_data) in expected_data.items():\n                channel_object = tdms_file[group][channel]\n                with pytest.raises(IndexError):\n                    _ = channel_object[index]\n\n\ndef test_indexing_channel_with_zero_step_raises_error():\n    """""" Test indexing into a channel with a slice with zero step size raises an error\n    """"""\n    test_file, expected_data = scenarios.chunked_segment().values\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for ((group, channel), expected_channel_data) in expected_data.items():\n                channel_object = tdms_file[group][channel]\n                with pytest.raises(ValueError) as exc_info:\n                    _ = channel_object[::0]\n                assert str(exc_info.value) == ""Step size cannot be zero""\n\n\n@pytest.mark.parametrize(\'index\', [""test"", None])\ndef test_indexing_channel_with_invalid_type_raises_error(index):\n    """""" Test indexing into a channel with an invalid index type\n    """"""\n    test_file, expected_data = scenarios.chunked_segment().values\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            for ((group, channel), expected_channel_data) in expected_data.items():\n                channel_object = tdms_file[group][channel]\n                with pytest.raises(TypeError) as exc_info:\n                    _ = channel_object[index]\n                assert ""Invalid index type"" in str(exc_info.value)\n\n\ndef test_invalid_offset_in_read_data_throws():\n    """""" Exception is thrown when reading a subset of data with an invalid offset\n    """"""\n    test_file, expected_data = scenarios.single_segment_with_one_channel().values\n    group, channel = list(expected_data.keys())[0]\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            with pytest.raises(ValueError) as exc_info:\n                tdms_file[group][channel].read_data(-1, 5)\n            assert ""offset must be non-negative"" in str(exc_info.value)\n\n\ndef test_invalid_length_in_read_data_throws():\n    """""" Exception is thrown when reading a subset of data with an invalid length\n    """"""\n    test_file, expected_data = scenarios.single_segment_with_one_channel().values\n    group, channel = list(expected_data.keys())[0]\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            with pytest.raises(ValueError) as exc_info:\n                tdms_file[group][channel].read_data(0, -5)\n            assert ""length must be non-negative"" in str(exc_info.value)\n\n\ndef test_read_data_after_close_throws():\n    """""" Trying to read after opening and closing without reading data should throw\n    """"""\n    test_file, expected_data = scenarios.single_segment_with_one_channel().values\n    group, channel = list(expected_data.keys())[0]\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            pass\n        with pytest.raises(RuntimeError) as exc_info:\n            tdms_file[group][channel].read_data()\n        assert ""Cannot read data after the underlying TDMS reader is closed"" in str(exc_info.value)\n\n\ndef test_read_data_after_open_in_read_mode_throws():\n    """""" Trying to read channel data after reading all data initially should throw\n    """"""\n    test_file, expected_data = scenarios.single_segment_with_one_channel().values\n    group, channel = list(expected_data.keys())[0]\n    with test_file.get_tempfile() as temp_file:\n        tdms_file = TdmsFile.read(temp_file.file)\n        with pytest.raises(RuntimeError) as exc_info:\n            tdms_file[group][channel].read_data()\n        assert ""Cannot read data after the underlying TDMS reader is closed"" in str(exc_info.value)\n\n\ndef test_access_data_property_after_opening_throws():\n    """""" Accessing the data property after opening without reading data should throw\n    """"""\n    test_file, expected_data = scenarios.single_segment_with_one_channel().values\n    group, channel = list(expected_data.keys())[0]\n    with test_file.get_tempfile() as temp_file:\n        with TdmsFile.open(temp_file.file) as tdms_file:\n            with pytest.raises(RuntimeError) as exc_info:\n                _ = tdms_file[group][channel].data\n            assert ""Channel data has not been read"" in str(exc_info.value)\n\n            with pytest.raises(RuntimeError) as exc_info:\n                _ = tdms_file[group][channel].raw_data\n            assert ""Channel data has not been read"" in str(exc_info.value)\n\n            with pytest.raises(RuntimeError) as exc_info:\n                _ = tdms_file[group][channel].raw_scaler_data\n            assert ""Channel data has not been read"" in str(exc_info.value)\n\n\n@pytest.mark.parametrize(""test_file,expected_data"", scenarios.get_scenarios())\ndef test_read_with_index_file(test_file, expected_data):\n    """""" Test reading a file with an associated tdms_index file\n    """"""\n    with test_file.get_tempfile_with_index() as tdms_file_path:\n        tdms_file = TdmsFile.read(tdms_file_path)\n\n    for ((group, channel), expected_channel_data) in expected_data.items():\n        channel_obj = tdms_file[group][channel]\n        compare_arrays(channel_obj.data, expected_channel_data)\n\n\n@pytest.mark.skipif(sys.version_info < (3, 4), reason=""pathlib only available in stdlib since 3.4"")\ndef test_read_file_passed_as_pathlib_path():\n    """""" Test reading a file when using a pathlib Path object\n    """"""\n    import pathlib\n\n    test_file, expected_data = scenarios.single_segment_with_one_channel().values\n\n    with test_file.get_tempfile_with_index() as tdms_file_path_str:\n        tdms_file_path = pathlib.Path(tdms_file_path_str)\n        tdms_file = TdmsFile.read(tdms_file_path)\n\n    for ((group, channel), expected_channel_data) in expected_data.items():\n        channel_obj = tdms_file[group][channel]\n        compare_arrays(channel_obj.data, expected_channel_data)\n\n\ndef test_read_with_mismatching_index_file():\n    """""" Test that reading data when the index file doesn\'t match the data file raises an error\n    """"""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", 3, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", 3, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", 3, 2),\n            channel_metadata(""/\'group\'/\'channel2\'"", 3, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n\n    test_file_with_index = GeneratedFile()\n    test_file_with_index.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", 3, 3),\n            channel_metadata(""/\'group\'/\'channel2\'"", 3, 3),\n        ),\n        ""01 00 00 00"" ""02 00 00 00"" ""03 00 00 00""\n        ""04 00 00 00"" ""05 00 00 00"" ""06 00 00 00""\n    )\n    test_file_with_index.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", 3, 3),\n            channel_metadata(""/\'group\'/\'channel2\'"", 3, 3),\n        ),\n        ""01 00 00 00"" ""02 00 00 00"" ""03 00 00 00""\n        ""04 00 00 00"" ""05 00 00 00"" ""06 00 00 00""\n    )\n\n    with test_file.get_tempfile(delete=False) as tdms_file:\n        with test_file_with_index.get_tempfile_with_index() as tdms_file_with_index_path:\n            # Move index file from second file to match the name of the first file\n            new_index_file = tdms_file.name + \'_index\'\n            copyfile(tdms_file_with_index_path + \'_index\', new_index_file)\n            try:\n                tdms_file.file.close()\n                with pytest.raises(ValueError) as exc_info:\n                    _ = TdmsFile.read(tdms_file.name)\n                assert \'Check that the tdms_index file matches the tdms data file\' in str(exc_info.value)\n            finally:\n                os.remove(new_index_file)\n                os.remove(tdms_file.name)\n\n\n@pytest.mark.filterwarnings(\'ignore:.* is deprecated\')\ndef test_get_objects():\n    """"""Test reading data""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_file = test_file.load()\n\n    objects = tdms_file.objects\n    assert len(objects) == 4\n    assert ""/"" in objects.keys()\n    assert ""/\'Group\'"" in objects.keys()\n    assert ""/\'Group\'/\'Channel1\'"" in objects.keys()\n    assert ""/\'Group\'/\'Channel2\'"" in objects.keys()\n\n\n@pytest.mark.filterwarnings(\'ignore:.* is deprecated\')\ndef test_get_object_from_group():\n    """"""Test passing a TdmsGroup to object returns the group""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_file = test_file.load()\n\n    groups = tdms_file.groups()\n    assert tdms_file.object(groups[0]) is groups[0]\n    assert tdms_file.object(groups[0].name) is groups[0]\n\n\ndef test_get_len_of_file():\n    """"""Test getting the length of a TdmsFile\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_data = test_file.load()\n\n    assert len(tdms_data) == 1\n\n\ndef test_get_len_of_group():\n    """"""Test getting the length of a TdmsGroup\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_data = test_file.load()\n\n    assert len(tdms_data[\'Group\']) == 2\n\n\ndef test_key_error_getting_invalid_group():\n    """"""Test getting a group that doesn\'t exist raises a KeyError\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_data = test_file.load()\n\n    with pytest.raises(KeyError) as exc_info:\n        _ = tdms_data[\'non-existent group\']\n    assert \'non-existent group\' in str(exc_info.value)\n\n\ndef test_key_error_getting_invalid_channel():\n    """"""Test getting a channel that doesn\'t exist raises a KeyError\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_data = test_file.load()\n\n    group = tdms_data[\'Group\']\n    with pytest.raises(KeyError) as exc_info:\n        _ = group[\'non-existent channel\']\n    assert \'non-existent channel\' in str(exc_info.value)\n    assert \'Group\' in str(exc_info.value)\n\n\ndef test_group_property_read():\n    """"""Test reading property of a group""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_data = test_file.load()\n\n    group = tdms_data[""Group""]\n    assert group.properties[""num""] == 10\n\n\ndef test_time_track():\n    """"""Add a time track to waveform data""""""\n\n    test_file = GeneratedFile()\n    (toc, metadata, data) = basic_segment()\n    test_file.add_segment(toc, metadata, data)\n    tdms_data = test_file.load()\n\n    channel = tdms_data[""Group""][""Channel2""]\n    time = channel.time_track()\n    assert len(time) == len(channel.data)\n    epsilon = 1.0E-15\n    assert abs(time[0]) < epsilon\n    assert abs(time[1] - 0.1) < epsilon\n\n\ndef test_memmapped_read():\n    """"""Test reading data into memmapped arrays""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_data = test_file.load(memmap_dir=tempfile.gettempdir())\n\n    data = tdms_data[""Group""][""Channel1""].data\n    assert len(data) == 2\n    assert data[0] == 1\n    assert data[1] == 2\n    data = tdms_data[""Group""][""Channel2""].data\n    assert len(data) == 2\n    assert data[0] == 3\n    assert data[1] == 4\n\n\ndef test_string_data():\n    """"""Test reading a file with string data""""""\n\n    strings = [""abcdefg"", ""qwertyuiop""]\n\n    test_file = GeneratedFile()\n    toc = (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"")\n    metadata = (\n        # Number of objects\n        ""01 00 00 00""\n        # Length of the object path\n        ""18 00 00 00"")\n    metadata += string_hexlify(""/\'Group\'/\'StringChannel\'"")\n    metadata += (\n        # Length of index information\n        ""1C 00 00 00""\n        # Raw data data type\n        ""20 00 00 00""\n        # Dimension\n        ""01 00 00 00""\n        # Number of raw data values\n        ""02 00 00 00""\n        ""00 00 00 00""\n        # Number of bytes in data\n        ""19 00 00 00""\n        ""00 00 00 00""\n        # Number of properties (0)\n        ""00 00 00 00"")\n    data = (\n        ""07 00 00 00""  # index to after first string\n        ""11 00 00 00""  # index to after second string\n    )\n    for string in strings:\n        data += string_hexlify(string)\n    test_file.add_segment(toc, metadata, data)\n    tdms_data = test_file.load()\n\n    channel = tdms_data[""Group""][""StringChannel""]\n    assert len(channel.data) == len(strings)\n    assert channel.data.dtype == channel.dtype\n    for expected, read in zip(strings, channel.data):\n        assert expected == read\n\n\ndef test_slash_and_space_in_name():\n    """"""Test name like \'01/02/03 something\'""""""\n\n    group_1 = ""01/02/03 something""\n    channel_1 = ""04/05/06 another thing""\n    group_2 = ""01/02/03 a""\n    channel_2 = ""04/05/06 b""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'{0}\'/\'{1}\'"".format(group_1, channel_1), 3, 2),\n            channel_metadata(""/\'{0}\'/\'{1}\'"".format(group_2, channel_2), 3, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n        ""03 00 00 00"" ""04 00 00 00""\n    )\n\n    tdms_data = test_file.load()\n\n    assert len(tdms_data.groups()) == 2\n    assert len(tdms_data[group_1].channels()) == 1\n    assert len(tdms_data[group_2].channels()) == 1\n    data_1 = tdms_data[group_1][channel_1].data\n    assert len(data_1) == 2\n    data_2 = tdms_data[group_2][channel_2].data\n    assert len(data_2) == 2\n\n\ndef test_single_quote_in_name():\n    group_1 = ""group\'\'s name""\n    channel_1 = ""channel\'\'s name""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'{0}\'/\'{1}\'"".format(group_1, channel_1), 3, 2),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n    )\n\n    tdms_data = test_file.load()\n\n    assert len(tdms_data.groups()) == 1\n    assert len(tdms_data[""group\'s name""].channels()) == 1\n    data_1 = tdms_data[""group\'s name""][""channel\'s name""].data\n    assert len(data_1) == 2\n\n\n@pytest.mark.filterwarnings(\'ignore:.* is deprecated\')\ndef test_root_object_paths():\n    """"""Test the group and channel properties for the root object""""""\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_data = test_file.load()\n\n    obj = tdms_data.object()\n    assert obj.group is None\n    assert obj.channel is None\n\n\n@pytest.mark.filterwarnings(\'ignore:.* is deprecated\')\ndef test_group_object_paths():\n    """"""Test the group and channel properties for a group""""""\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_data = test_file.load()\n\n    obj = tdms_data[""Group""]\n    assert obj.path == ""/\'Group\'""\n    assert obj.name == ""Group""\n    assert obj.group == ""Group""\n    assert obj.channel is None\n\n\n@pytest.mark.filterwarnings(\'ignore:.* is deprecated\')\ndef test_channel_object_paths():\n    """"""Test the group and channel properties for a group""""""\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_data = test_file.load()\n\n    obj = tdms_data[""Group""][""Channel1""]\n    assert obj.path == ""/\'Group\'/\'Channel1\'""\n    assert obj.name == ""Channel1""\n    assert obj.group == ""Group""\n    assert obj.channel == ""Channel1""\n\n\ndef test_object_repr():\n    """"""Test getting object representations of groups and channels\n    """"""\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    tdms_data = test_file.load()\n\n    group = tdms_data[""Group""]\n    assert repr(group) == ""<TdmsGroup with path /\'Group\'>""\n\n    channel = group[""Channel1""]\n    assert repr(channel) == ""<TdmsChannel with path /\'Group\'/\'Channel1\'>""\n\n\ndef test_data_read_from_bytes_io():\n    """"""Test reading data""""""\n\n    test_file = BytesIoTestFile()\n    test_file.add_segment(*basic_segment())\n    tdms_data = test_file.load()\n\n    data = tdms_data[""Group""][""Channel1""].data\n    assert len(data) == 2\n    assert data[0] == 1\n    assert data[1] == 2\n    data = tdms_data[""Group""][""Channel2""].data\n    assert len(data) == 2\n    assert data[0] == 3\n    assert data[1] == 4\n\n\ndef test_file_properties():\n    """"""Test reading properties of the file (root object)""""""\n\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n\n    tdms_file = test_file.load()\n\n    file_props = tdms_file.properties\n    assert file_props[\'num\'] == 15\n\n\ndef test_debug_logging(caplog):\n    """""" Test loading a file with debug logging enabled\n    """"""\n    test_file, expected_data = scenarios.single_segment_with_one_channel().values\n\n    log_manager.set_level(logging.DEBUG)\n    _ = test_file.load()\n\n    assert ""Reading metadata for object /\'group\'/\'channel1\' with index header 0x00000014"" in caplog.text\n    assert ""Object data type: Int32"" in caplog.text\n\n\ndef test_memory_released_when_tdms_file_out_of_scope():\n    """""" Tests that when a TDMS file object goes out of scope,\n        TDMS channels and their data are also freed.\n        This ensures there are no circular references between a TDMS file\n        and its channels, which would mean the GC is needed to free these objects.\n    """"""\n\n    test_file, expected_data = scenarios.single_segment_with_one_channel().values\n    with test_file.get_tempfile() as temp_file:\n        tdms_data = TdmsFile.read(temp_file.file)\n        chan = tdms_data[\'group\'][\'channel1\']\n        chan_ref = weakref.ref(chan)\n        data_ref = weakref.ref(chan.data)\n        raw_data_ref = weakref.ref(chan.raw_data)\n    del tdms_data\n    del chan\n\n    assert raw_data_ref() is None\n    assert data_ref() is None\n    assert chan_ref() is None\n\n\ndef test_close_after_read():\n    test_file, _ = scenarios.single_segment_with_one_channel().values\n    temp_file = test_file.get_tempfile(delete=False)\n    try:\n        temp_file.file.close()\n        tdms_data = TdmsFile.read(temp_file.name)\n        tdms_data.close()\n    finally:\n        os.remove(temp_file.name)\n\n\ndef test_multiple_close_after_open():\n    test_file, _ = scenarios.single_segment_with_one_channel().values\n    temp_file = test_file.get_tempfile(delete=False)\n    try:\n        temp_file.file.close()\n        with TdmsFile.open(temp_file.name) as tdms_data:\n            tdms_data.close()\n        tdms_data.close()\n    finally:\n        os.remove(temp_file.name)\n'"
nptdms/test/test_tdmsinfo.py,0,"b'import os\nimport sys\nfrom nptdms import tdmsinfo\nfrom nptdms.test.util import GeneratedFile, basic_segment\ntry:\n    from unittest.mock import patch\nexcept ImportError:\n    from mock import patch\n\n\ndef test_tdmsinfo(capsys):\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    temp_file = test_file.get_tempfile(delete=False)\n    try:\n        temp_file.file.close()\n        with patch.object(sys, \'argv\', [\'tdmsinfo.py\', temp_file.name]):\n            tdmsinfo.main()\n            captured = capsys.readouterr()\n            assert ""/\'Group\'/\'Channel1\'"" in captured.out\n            assert ""wf_start_offset"" not in captured.out\n    finally:\n        os.remove(temp_file.name)\n\n\ndef test_tdmsinfo_with_properties(capsys):\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    temp_file = test_file.get_tempfile(delete=False)\n    try:\n        temp_file.file.close()\n        with patch.object(sys, \'argv\', [\'tdmsinfo.py\', temp_file.name, \'--properties\']):\n            tdmsinfo.main()\n            captured = capsys.readouterr()\n            assert ""/\'Group\'/\'Channel1\'"" in captured.out\n            assert ""wf_start_offset: 0.0"" in captured.out\n            assert ""length: 2"" in captured.out\n    finally:\n        os.remove(temp_file.name)\n\n\ndef test_tdmsinfo_with_debug_output(caplog):\n    test_file = GeneratedFile()\n    test_file.add_segment(*basic_segment())\n    temp_file = test_file.get_tempfile(delete=False)\n    try:\n        temp_file.file.close()\n        with patch.object(sys, \'argv\', [\'tdmsinfo.py\', temp_file.name, \'--debug\']):\n            tdmsinfo.main()\n            assert ""Reading metadata for object /\'Group\'/\'Channel1\'"" in caplog.text\n    finally:\n        os.remove(temp_file.name)\n'"
nptdms/test/test_timestamps.py,35,"b'"""""" Test reading timestamp properties and data\n""""""\n\nfrom datetime import datetime\nimport numpy as np\nimport struct\nimport pytest\nfrom nptdms import TdmsFile\nfrom nptdms.timestamp import TdmsTimestamp, TimestampArray\nfrom nptdms.types import TimeStamp\nfrom nptdms.test.util import (\n    GeneratedFile,\n    channel_metadata,\n    hexlify_value,\n    segment_objects_metadata,\n)\n\n\ndef test_read_raw_timestamp_properties():\n    """""" Test reading timestamp properties as a raw TDMS timestamp\n    """"""\n    test_file = GeneratedFile()\n    second_fractions = 1234567890 * 10 ** 10\n    properties = {\n        ""wf_start_time"": (0x44, hexlify_value(""<Q"", second_fractions) + hexlify_value(""<q"", 3524551547))\n    }\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", 3, 2, properties),\n        ),\n        ""01 00 00 00"" ""02 00 00 00""\n    )\n\n    with test_file.get_tempfile() as temp_file:\n        tdms_data = TdmsFile.read(temp_file.file, raw_timestamps=True)\n        start_time = tdms_data[\'group\'][\'channel1\'].properties[\'wf_start_time\']\n        assert start_time.seconds == 3524551547\n        assert start_time.second_fractions == second_fractions\n\n\ndef test_timestamp_as_datetime64():\n    """""" Test converting a timestamp to a numpy datetime64\n    """"""\n    second_fractions = 1234567890 * 10 ** 10\n    seconds = 3524551547\n    timestamp = TdmsTimestamp(seconds, second_fractions)\n\n    assert timestamp.as_datetime64() == np.datetime64(\'2015-09-08T10:05:47.669260\', \'us\')\n    assert timestamp.as_datetime64().dtype == np.dtype(\'datetime64[us]\')\n    assert timestamp.as_datetime64(\'ns\') == np.datetime64(\'2015-09-08T10:05:47.669260594\', \'ns\')\n    assert timestamp.as_datetime64(\'ns\').dtype == np.dtype(\'datetime64[ns]\')\n\n\ndef test_timestamp_as_datetime():\n    """""" Test converting a timestamp to a datetime.datetime\n    """"""\n    second_fractions = 1234567890 * 10 ** 10\n    seconds = 3524551547\n    timestamp = TdmsTimestamp(seconds, second_fractions)\n\n    assert timestamp.as_datetime() == datetime(2015, 9, 8, 10, 5, 47, 669261)\n\n\ndef test_read_raw_timestamp_data():\n    """""" Test reading timestamp data as a raw TDMS timestamps\n    """"""\n    test_file = GeneratedFile()\n    seconds = 3672033330\n    second_fractions = 1234567890 * 10 ** 10\n    test_file.add_segment(\n        (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""),\n        segment_objects_metadata(\n            channel_metadata(""/\'group\'/\'channel1\'"", 0x44, 4),\n        ),\n        hexlify_value(""<Q"", 0) + hexlify_value(""<q"", seconds) +\n        hexlify_value(""<Q"", second_fractions) + hexlify_value(""<q"", seconds) +\n        hexlify_value(""<Q"", 0) + hexlify_value(""<q"", seconds + 1) +\n        hexlify_value(""<Q"", second_fractions) + hexlify_value(""<q"", seconds + 1)\n    )\n\n    expected_seconds = np.array([seconds, seconds, seconds + 1, seconds + 1], np.dtype(\'int64\'))\n    expected_second_fractions = np.array([0, second_fractions, 0, second_fractions], np.dtype(\'uint64\'))\n\n    with test_file.get_tempfile() as temp_file:\n        tdms_data = TdmsFile.read(temp_file.file, raw_timestamps=True)\n        data = tdms_data[\'group\'][\'channel1\'][:]\n        assert isinstance(data, TimestampArray)\n        np.testing.assert_equal(data.seconds, expected_seconds)\n        np.testing.assert_equal(data.second_fractions, expected_second_fractions)\n\n\ndef test_read_big_endian_timestamp_data():\n    seconds = 3672033330\n    second_fractions = 1234567890 * 10 ** 10\n    data = (\n        struct.pack("">q"", seconds) + struct.pack("">Q"", 0) +\n        struct.pack("">q"", seconds) + struct.pack("">Q"", second_fractions) +\n        struct.pack("">q"", seconds + 1) + struct.pack("">Q"", 0) +\n        struct.pack("">q"", seconds + 1) + struct.pack("">Q"", second_fractions))\n    expected_seconds = np.array([seconds, seconds, seconds + 1, seconds + 1], np.dtype(\'int64\'))\n    expected_second_fractions = np.array([0, second_fractions, 0, second_fractions], np.dtype(\'uint64\'))\n\n    timestamp_array = TimeStamp.from_bytes(np.frombuffer(data, dtype=np.dtype(\'uint8\')), \'>\')\n\n    np.testing.assert_equal(timestamp_array.seconds, expected_seconds)\n    np.testing.assert_equal(timestamp_array.second_fractions, expected_second_fractions)\n\n\ndef test_timestamp_repr():\n    timestamp = TdmsTimestamp(3672033330, 12345678900000000000)\n    assert repr(timestamp) == \'TdmsTimestamp(3672033330, 12345678900000000000)\'\n\n\ndef test_timestamp_str():\n    timestamp = TdmsTimestamp(3672033330, 12345678900000000000)\n    assert str(timestamp) == \'2020-05-11T09:15:30.669261\'\n\n\ndef test_timestamp_array_slicing():\n    timestamp_array = _get_test_timestamp_array()\n    array_slice = timestamp_array[0:2]\n    assert isinstance(array_slice, TimestampArray)\n\n\ndef test_timestamp_array_get_single_item():\n    timestamp_array = _get_test_timestamp_array()\n    array_item = timestamp_array[3]\n    assert isinstance(array_item, TdmsTimestamp)\n    assert array_item.seconds == 3672033331\n    assert array_item.second_fractions == 1234567890 * 10 ** 10\n\n\ndef test_timestamp_array_field_access():\n    timestamp_array = _get_test_timestamp_array()\n    seconds = timestamp_array.seconds\n    second_fractions = timestamp_array.second_fractions\n    assert isinstance(seconds, np.ndarray)\n    assert not isinstance(seconds, TimestampArray)\n    assert seconds.dtype == np.dtype(\'<i8\')\n    assert isinstance(second_fractions, np.ndarray)\n    assert not isinstance(second_fractions, TimestampArray)\n    assert second_fractions.dtype == np.dtype(\'<u8\')\n\n\ndef test_timestamp_array_to_datetime64():\n    timestamp_array = _get_test_timestamp_array()\n    expected_timestamps = np.array([\n        np.datetime64(\'2020-05-11 09:15:30\'),\n        np.datetime64(\'2020-05-11 09:15:30.669260\'),\n        np.datetime64(\'2020-05-11 09:15:31\'),\n        np.datetime64(\'2020-05-11 09:15:31.669260\'),\n    ])\n\n    us_array = timestamp_array.as_datetime64()\n\n    np.testing.assert_equal(us_array, expected_timestamps)\n\n\ndef test_timestamp_array_to_datetime64_with_ns_precision():\n    timestamp_array = _get_test_timestamp_array()\n    expected_timestamps = np.array([\n        np.datetime64(\'2020-05-11 09:15:30\'),\n        np.datetime64(\'2020-05-11 09:15:30.669260594\'),\n        np.datetime64(\'2020-05-11 09:15:31\'),\n        np.datetime64(\'2020-05-11 09:15:31.669260594\'),\n    ])\n\n    ns_array = timestamp_array.as_datetime64(\'ns\')\n\n    np.testing.assert_equal(ns_array, expected_timestamps)\n\n\n@pytest.mark.parametrize(\n    ""input_array"",\n    [\n        np.array([1, 2, 3, 4]),\n        np.array([(1, 2), (3, 4)], dtype=[(\'a\', \'<i8\'), (\'b\', \'<u8\')]),\n        np.array([(1, 2), (3, 4)], dtype=[(\'seconds\', \'<i8\'), (\'b\', \'<u8\')]),\n        np.array([(1, 2), (3, 4)], dtype=[(\'a\', \'<i8\'), (\'second_fractions\', \'<u8\')]),\n    ]\n)\ndef test_error_raised_with_creating_timestamp_array_with_invalid_input_type(input_array):\n    with pytest.raises(ValueError) as exc_info:\n        _ = TimestampArray(input_array)\n    assert str(exc_info.value) == ""Input array must have a dtype with \'seconds\' and \'second_fractions\' fields""\n\n\ndef test_error_raised_converting_timestamp_with_invalid_resolution():\n    timestamp_array = _get_test_timestamp_array()\n    timestamp = timestamp_array[0]\n    with pytest.raises(ValueError) as exc_info:\n        _ = timestamp.as_datetime64(\'invalid_res\')\n    assert str(exc_info.value) == ""Unsupported resolution for converting to numpy datetime64: \'invalid_res\'""\n\n\ndef test_error_raised_converting_timestamp_array_with_invalid_resolution():\n    timestamp_array = _get_test_timestamp_array()\n    with pytest.raises(ValueError) as exc_info:\n        _ = timestamp_array.as_datetime64(\'invalid_res\')\n    assert str(exc_info.value) == ""Unsupported resolution for converting to numpy datetime64: \'invalid_res\'""\n\n\ndef _get_test_timestamp_array():\n    dtype = np.dtype([(\'seconds\', \'<i8\'), (\'second_fractions\', \'<u8\')])\n    seconds = 3672033330\n    second_fractions = 1234567890 * 10 ** 10\n    array = np.array([\n        (seconds, 0),\n        (seconds, second_fractions),\n        (seconds + 1, 0),\n        (seconds + 1, second_fractions),\n    ], dtype=dtype)\n    return TimestampArray(array)\n'"
nptdms/test/test_types.py,3,"b'""""""Test type reading and writing""""""\n\nfrom datetime import date, datetime\nimport io\nimport numpy as np\nimport pytest\n\nfrom nptdms import types\n\n\n@pytest.mark.parametrize(\n    ""time_string"",\n    [\n        pytest.param(\'2019-11-08T18:47:00\', id=""standard timestamp""),\n        pytest.param(\'0000-01-01T05:00:00\', id=""timestamp before TDMS epoch""),\n        pytest.param(\'2019-11-08T18:47:00.123456\', id=""timestamp with microseconds""),\n        pytest.param(\'1903-12-31T23:59:59.500\', id=""timestamp before TDMS epoch with microseconds""),\n    ]\n)\ndef test_timestamp_round_trip(time_string):\n    expected_datetime = np.datetime64(time_string)\n\n    timestamp = types.TimeStamp(expected_datetime)\n    data_file = io.BytesIO(timestamp.bytes)\n\n    read_datetime = types.TimeStamp.read(data_file).as_datetime64()\n\n    assert expected_datetime == read_datetime\n\n\ndef test_timestamp_from_datetime():\n    """"""Test timestamp from built in datetime value""""""\n\n    input_datetime = datetime(2019, 11, 8, 18, 47, 0)\n    expected_datetime = np.datetime64(\'2019-11-08T18:47:00\')\n\n    timestamp = types.TimeStamp(input_datetime)\n    data_file = io.BytesIO(timestamp.bytes)\n\n    read_datetime = types.TimeStamp.read(data_file)\n\n    assert expected_datetime == read_datetime.as_datetime64()\n\n\ndef test_timestamp_from_date():\n    """"""Test timestamp from built in date value""""""\n\n    input_datetime = date(2019, 11, 8)\n    expected_datetime = np.datetime64(\'2019-11-08T00:00:00\')\n\n    timestamp = types.TimeStamp(input_datetime)\n    data_file = io.BytesIO(timestamp.bytes)\n\n    read_datetime = types.TimeStamp.read(data_file)\n\n    assert expected_datetime == read_datetime.as_datetime64()\n'"
nptdms/test/util.py,1,"b'"""""" Utilities for testing TDMS reading\n""""""\n\nimport binascii\nfrom contextlib import contextmanager\nimport os\nfrom io import BytesIO\nimport struct\nimport tempfile\nimport numpy as np\n\nfrom nptdms import tdms\n\n\ntry:\n    long\nexcept NameError:\n    # Python 3\n    long = int\n\n\ndef string_hexlify(input_string):\n    """"""Return hex string representation of string""""""\n    return binascii.hexlify(input_string.encode(\'utf-8\')).decode(\'utf-8\')\n\n\ndef hexlify_value(struct_type, value):\n    """"""Return hex string representation of a value""""""\n    return binascii.hexlify(struct.pack(struct_type, value)).decode(\'utf-8\')\n\n\ndef segment_objects_metadata(*args):\n    """""" Metadata for multiple objects in a segment\n    """"""\n    num_objects_hex = hexlify_value(""<I"", len(args))\n    return num_objects_hex + """".join(args)\n\n\ndef channel_metadata(channel_name, data_type, num_values, properties=None):\n    return (\n        # Length of the object path\n        hexlify_value(\'<I\', len(channel_name)) +\n        # Object path\n        string_hexlify(channel_name) +\n        # Length of index information\n        ""14 00 00 00"" +\n        # Raw data data type\n        hexlify_value(\'<I\', data_type) +\n        # Dimension\n        ""01 00 00 00"" +\n        # Number of raw data values\n        hexlify_value(\'<Q\', num_values) +\n        hex_properties(properties)\n    )\n\n\ndef hex_properties(properties):\n    if properties is None:\n        properties = {}\n    props_hex = hexlify_value(\'<I\', len(properties))\n    for (prop_name, (prop_type, prop_value)) in properties.items():\n        props_hex += hexlify_value(\'<I\', len(prop_name))\n        props_hex += string_hexlify(prop_name)\n        props_hex += hexlify_value(\'<I\', prop_type)\n        props_hex += prop_value\n    return props_hex\n\n\ndef channel_metadata_with_repeated_structure(channel_name):\n    return (\n        # Length of the object path\n        hexlify_value(\'<I\', len(channel_name)) +\n        # Object path\n        string_hexlify(channel_name) +\n        # Raw data index header meaning repeat previous data structure\n        ""00 00 00 00"" +\n        # Number of properties (0)\n        ""00 00 00 00""\n    )\n\n\ndef channel_metadata_with_no_data(channel_name):\n    return (\n        # Length of the object path\n        hexlify_value(\'<I\', len(channel_name)) +\n        # Object path\n        string_hexlify(channel_name) +\n        # Raw data index header meaning no data in this segment\n        ""FF FF FF FF"" +\n        # Number of properties (0)\n        ""00 00 00 00""\n    )\n\n\ndef basic_segment():\n    """"""Basic TDMS segment with one group and two channels""""""\n\n    toc = (""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList"")\n    metadata = (\n        # Number of objects\n        ""04 00 00 00""\n        # Length of the first object path\n        ""08 00 00 00""\n        # Object path (/\'Group\')\n        ""2F 27 47 72""\n        ""6F 75 70 27""\n        # Raw data index\n        ""FF FF FF FF""\n        # Num properties\n        ""02 00 00 00""\n        # Name length\n        ""04 00 00 00""\n        # Property name (prop)\n        ""70 72 6F 70""\n        # Property data type (string)\n        ""20 00 00 00""\n        # Length of string value\n        ""05 00 00 00""\n        # Value\n        ""76 61 6C 75 65""\n        # Length of second property name\n        ""03 00 00 00""\n        # Property name (num)\n        ""6E 75 6D""\n        # Data type of property\n        ""03 00 00 00""\n        # Value\n        ""0A 00 00 00""\n        # Length of the second object path\n        ""13 00 00 00""\n        # Second object path (/\'Group\'/\'Channel1\')\n        ""2F 27 47 72""\n        ""6F 75 70 27""\n        ""2F 27 43 68""\n        ""61 6E 6E 65""\n        ""6C 31 27""\n        # Length of index information\n        ""14 00 00 00""\n        # Raw data data type\n        ""03 00 00 00""\n        # Dimension\n        ""01 00 00 00""\n        # Number of raw data values\n        ""02 00 00 00""\n        ""00 00 00 00""\n        # Number of properties (0)\n        ""00 00 00 00""\n        # Length of the third object path\n        ""13 00 00 00""\n        # Third object path (/\'Group\'/\'Channel2\')\n        ""2F 27 47 72""\n        ""6F 75 70 27""\n        ""2F 27 43 68""\n        ""61 6E 6E 65""\n        ""6C 32 27""\n        # Length of index information\n        ""14 00 00 00""\n        # Raw data data type\n        ""03 00 00 00""\n        # Dimension\n        ""01 00 00 00""\n        # Number of data values\n        ""02 00 00 00""\n        ""00 00 00 00""\n        # Set time properties for the second channel\n        ""02 00 00 00""\n        ""0F 00 00 00"" +\n        string_hexlify(\'wf_start_offset\') +\n        ""0A 00 00 00"" +\n        hexlify_value(""<d"", 0.0) +\n        ""0C 00 00 00"" +\n        string_hexlify(\'wf_increment\') +\n        ""0A 00 00 00"" +\n        hexlify_value(""<d"", 0.1) +\n        # Length of the object path\n        ""01 00 00 00""\n        # Object path (/)\n        ""2F""\n        # Raw data index\n        ""FF FF FF FF""\n        # Num properties\n        ""01 00 00 00""\n        # Length of property name\n        ""03 00 00 00""\n        # Property name (num)\n        ""6E 75 6D""\n        # Data type of property\n        ""03 00 00 00""\n        # Value\n        ""0F 00 00 00""\n    )\n    data = (\n        # Data for segment\n        ""01 00 00 00""\n        ""02 00 00 00""\n        ""03 00 00 00""\n        ""04 00 00 00""\n    )\n    return toc, metadata, data\n\n\nclass GeneratedFile(object):\n    """"""Generate a TDMS file for testing""""""\n\n    def __init__(self):\n        self._content = []\n\n    def add_segment(self, toc, metadata, data, incomplete=False, binary_data=False):\n        metadata_bytes = _hex_to_bytes(metadata)\n        data_bytes = data if binary_data else _hex_to_bytes(data)\n        if toc is not None:\n            lead_in = b\'TDSm\'\n            toc_mask = long(0)\n            for toc_item in toc:\n                if toc_item == ""kTocMetaData"":\n                    toc_mask = toc_mask | long(1) << 1\n                elif toc_item == ""kTocRawData"":\n                    toc_mask = toc_mask | long(1) << 3\n                elif toc_item == ""kTocDAQmxRawData"":\n                    toc_mask = toc_mask | long(1) << 7\n                elif toc_item == ""kTocInterleavedData"":\n                    toc_mask = toc_mask | long(1) << 5\n                elif toc_item == ""kTocBigEndian"":\n                    toc_mask = toc_mask | long(1) << 6\n                elif toc_item == ""kTocNewObjList"":\n                    toc_mask = toc_mask | long(1) << 2\n                else:\n                    raise ValueError(""Unrecognised TOC value: %s"" % toc_item)\n            lead_in += struct.pack(\'<i\', toc_mask)\n            lead_in += _hex_to_bytes(""69 12 00 00"")\n            next_segment_offset = len(metadata_bytes) + len(data_bytes)\n            raw_data_offset = len(metadata_bytes)\n            if incomplete:\n                lead_in += _hex_to_bytes(\'FF\' * 8)\n            else:\n                lead_in += struct.pack(\'<Q\', next_segment_offset)\n            lead_in += struct.pack(\'<Q\', raw_data_offset)\n        else:\n            lead_in = b\'\'\n        self._content.append((lead_in, metadata_bytes, data_bytes))\n\n    def get_tempfile(self, **kwargs):\n        named_file = tempfile.NamedTemporaryFile(suffix="".tdms"", **kwargs)\n        file = named_file.file\n        file.write(self._get_contents())\n        file.seek(0)\n        return named_file\n\n    @contextmanager\n    def get_tempfile_with_index(self):\n        directory = tempfile.mkdtemp()\n        tdms_path = os.path.join(directory, \'test_file.tdms\')\n        tdms_index_path = os.path.join(directory, \'test_file.tdms_index\')\n        with open(tdms_path, \'wb\') as file:\n            file.write(self._get_contents())\n        with open(tdms_index_path, \'wb\') as file:\n            file.write(self._get_index_contents())\n        try:\n            yield tdms_path\n        finally:\n            os.unlink(tdms_path)\n            os.unlink(tdms_index_path)\n            os.rmdir(directory)\n\n    def load(self, *args, **kwargs):\n        with tempfile.NamedTemporaryFile(suffix="".tdms"") as named_file:\n            file = named_file.file\n            file.write(self._get_contents())\n            file.seek(0)\n            return tdms.TdmsFile(file, *args, **kwargs)\n\n    def get_bytes_io_file(self):\n        file = BytesIO()\n        file.write(self._get_contents())\n        file.seek(0)\n        return file\n\n    def _get_contents(self):\n        contents = b\'\'\n        for segment in self._content:\n            contents += segment[0]\n            contents += segment[1]\n            contents += segment[2]\n        return contents\n\n    def _get_index_contents(self):\n        contents = b\'\'\n        for segment in self._content:\n            lead_in = segment[0]\n            if len(lead_in) >= 4:\n                lead_in = b\'TDSh\' + lead_in[4:]\n            contents += lead_in\n            contents += segment[1]\n        return contents\n\n\nclass BytesIoTestFile(GeneratedFile):\n    def load(self, *args, **kwargs):\n        file = self.get_bytes_io_file()\n        return tdms.TdmsFile(file, *args, **kwargs)\n\n\ndef _hex_to_bytes(hex_data):\n    """""" Converts a string of hex to a byte array\n    """"""\n    return binascii.unhexlify(\n        hex_data.replace("" "", """").replace(""\\n"", """").encode(\'utf-8\'))\n\n\ndef compare_arrays(actual_data, expected_data):\n    try:\n        np.testing.assert_almost_equal(actual_data, expected_data)\n    except TypeError:\n        # Cannot compare given types\n        assert len(actual_data) == len(expected_data)\n        for (actual, expected) in zip(actual_data, expected_data):\n            assert actual == expected\n'"
nptdms/test/writer/__init__.py,0,b''
nptdms/test/writer/test_acceptance_tests.py,41,"b'""""""Test writing TDMS files""""""\n\nfrom datetime import datetime\nfrom io import BytesIO\nimport numpy as np\nimport os\nimport tempfile\n\nfrom nptdms import TdmsFile, TdmsWriter, RootObject, GroupObject, ChannelObject\n\n\ndef test_can_read_tdms_file_after_writing():\n    a_input = np.linspace(0.0, 1.0, 100)\n    b_input = np.linspace(0.0, 100.0, 100)\n\n    a_segment = ChannelObject(""group"", ""a"", a_input)\n    b_segment = ChannelObject(""group"", ""b"", b_input)\n\n    output_file = BytesIO()\n    with TdmsWriter(output_file) as tdms_writer:\n        tdms_writer.write_segment([a_segment, b_segment])\n\n    output_file.seek(0)\n    tdms_file = TdmsFile(output_file)\n\n    a_output = tdms_file[""group""][""a""].data\n    b_output = tdms_file[""group""][""b""].data\n\n    assert len(a_output) == len(a_input)\n    assert len(b_output) == len(b_input)\n    assert (a_output == a_input).all()\n    assert (b_output == b_input).all()\n\n\ndef test_can_read_tdms_file_properties_after_writing():\n    test_time = np.datetime64(\'2019-11-19T15:30:00\')\n\n    a_segment = RootObject(properties={\n        ""prop1"": ""foo"",\n        ""prop2"": 3,\n    })\n    b_segment = GroupObject(""group_name"", properties={\n        ""prop3"": 1.2345,\n        ""prop4"": test_time,\n    })\n\n    output_file = BytesIO()\n    with TdmsWriter(output_file) as tdms_writer:\n        tdms_writer.write_segment([a_segment, b_segment])\n\n    output_file.seek(0)\n    tdms_file = TdmsFile(output_file)\n\n    file_properties = tdms_file.properties\n    b_output = tdms_file[""group_name""]\n\n    assert ""prop1"" in file_properties, ""prop1 not found""\n    assert ""prop2"" in file_properties, ""prop2 not found""\n    assert ""prop3"" in b_output.properties, ""prop3 not found""\n    assert ""prop4"" in b_output.properties, ""prop4 not found""\n    assert file_properties[""prop1""] == ""foo""\n    assert file_properties[""prop2""] == 3\n    assert b_output.properties[""prop3""] == 1.2345\n    assert b_output.properties[""prop4""] == test_time\n\n\ndef test_can_write_multiple_segments():\n    input_1 = np.linspace(0.0, 1.0, 10)\n    input_2 = np.linspace(2.0, 3.0, 10)\n\n    segment_1 = ChannelObject(""group"", ""a"", input_1)\n    segment_2 = ChannelObject(""group"", ""a"", input_2)\n\n    output_file = BytesIO()\n    with TdmsWriter(output_file) as tdms_writer:\n        tdms_writer.write_segment([segment_1])\n        tdms_writer.write_segment([segment_2])\n\n    output_file.seek(0)\n    tdms_file = TdmsFile(output_file)\n\n    output_data = tdms_file[""group""][""a""].data\n\n    expected_data = np.append(input_1, input_2)\n    assert len(output_data) == len(expected_data)\n    assert (output_data == expected_data).all()\n\n\ndef test_can_write_to_file_using_path():\n    input_1 = np.linspace(0.0, 1.0, 10)\n    segment = ChannelObject(""group"", ""a"", input_1)\n\n    tempdir = tempfile.mkdtemp()\n    temppath = ""%s/test_file.tdms"" % tempdir\n    try:\n        with TdmsWriter(temppath) as tdms_writer:\n            tdms_writer.write_segment([segment])\n    finally:\n        if os.path.exists(temppath):\n            os.remove(temppath)\n        os.rmdir(tempdir)\n\n\ndef test_can_append_to_file_using_path():\n    input_1 = np.linspace(0.0, 1.0, 10)\n    input_2 = np.linspace(1.0, 2.0, 10)\n    segment_1 = ChannelObject(""group"", ""a"", input_1)\n    segment_2 = ChannelObject(""group"", ""a"", input_2)\n\n    tempdir = tempfile.mkdtemp()\n    temppath = ""%s/test_file.tdms"" % tempdir\n    try:\n        with TdmsWriter(temppath) as tdms_writer:\n            tdms_writer.write_segment([segment_1])\n        with TdmsWriter(temppath, \'a\') as tdms_writer:\n            tdms_writer.write_segment([segment_2])\n\n        tdms_file = TdmsFile(temppath)\n\n        output = tdms_file[""group""][""a""].data\n\n        assert len(output) == 20\n        np.testing.assert_almost_equal(\n            output, np.concatenate([input_1, input_2]))\n\n    finally:\n        if os.path.exists(temppath):\n            os.remove(temppath)\n        os.rmdir(tempdir)\n\n\ndef test_can_write_to_file_using_open_file():\n    input_1 = np.linspace(0.0, 1.0, 10)\n    segment = ChannelObject(""group"", ""a"", input_1)\n\n    with tempfile.NamedTemporaryFile(delete=True) as output_file:\n        with TdmsWriter(output_file.file) as tdms_writer:\n            tdms_writer.write_segment([segment])\n\n\ndef test_can_write_tdms_objects_read_from_file():\n    group_segment = GroupObject(""group"", properties={\n        ""prop1"": ""bar""\n    })\n    input_data = np.linspace(0.0, 1.0, 10)\n    channel_segment = ChannelObject(""group"", ""a"", input_data, properties={\n        ""prop1"": ""foo"",\n        ""prop2"": 3,\n    })\n\n    tempdir = tempfile.mkdtemp()\n    temppath = ""%s/test_file.tdms"" % tempdir\n    try:\n        with TdmsWriter(temppath) as tdms_writer:\n            tdms_writer.write_segment([group_segment, channel_segment])\n\n        tdms_file = TdmsFile(temppath)\n        read_group = tdms_file[""group""]\n        read_channel = tdms_file[""group""][""a""]\n\n        with TdmsWriter(temppath) as tdms_writer:\n            tdms_writer.write_segment([read_group, read_channel])\n\n        tdms_file = TdmsFile(temppath)\n        read_group = tdms_file[""group""]\n        read_channel = tdms_file[""group""][""a""]\n\n        assert read_group.properties[""prop1""] == ""bar""\n\n        assert len(read_channel.data) == 10\n        np.testing.assert_almost_equal(read_channel.data, input_data)\n        assert read_channel.properties[""prop1""] == ""foo""\n        assert read_channel.properties[""prop2""] == 3\n\n    finally:\n        if os.path.exists(temppath):\n            os.remove(temppath)\n        os.rmdir(tempdir)\n\n\ndef test_can_write_timestamp_data():\n    input_data = [\n        np.datetime64(\'2017-07-09T12:35:00.00\'),\n        np.datetime64(\'2017-07-09T12:36:00.00\'),\n        np.datetime64(\'2017-07-09T12:37:00.00\'),\n        ]\n\n    segment = ChannelObject(""group"", ""timedata"", input_data)\n\n    output_file = BytesIO()\n    with TdmsWriter(output_file) as tdms_writer:\n        tdms_writer.write_segment([segment])\n\n    output_file.seek(0)\n    tdms_file = TdmsFile(output_file)\n\n    output_data = tdms_file[""group""][""timedata""].data\n\n    assert len(output_data) == 3\n    assert output_data[0] == input_data[0]\n    assert output_data[1] == input_data[1]\n    assert output_data[2] == input_data[2]\n\n\ndef test_can_write_timestamp_data_with_datetimes():\n    input_data = [\n        datetime(2017, 7, 9, 12, 35, 0),\n        datetime(2017, 7, 9, 12, 36, 0),\n        datetime(2017, 7, 9, 12, 37, 0)]\n    expected_data = np.array([\n        \'2017-07-09T12:35:00\',\n        \'2017-07-09T12:36:00\',\n        \'2017-07-09T12:37:00\'], dtype=\'datetime64\')\n\n    segment = ChannelObject(""group"", ""timedata"", input_data)\n\n    output_file = BytesIO()\n    with TdmsWriter(output_file) as tdms_writer:\n        tdms_writer.write_segment([segment])\n\n    output_file.seek(0)\n    tdms_file = TdmsFile(output_file)\n\n    output_data = tdms_file[""group""][""timedata""].data\n\n    assert len(output_data) == 3\n    assert output_data[0] == expected_data[0]\n    assert output_data[1] == expected_data[1]\n    assert output_data[2] == expected_data[2]\n\n\ndef test_can_write_numpy_timestamp_data_with_dates():\n    input_data = np.array([\n        \'2017-07-09\',\n        \'2017-07-09\',\n        \'2017-07-09\'], dtype=\'datetime64\')\n\n    segment = ChannelObject(""group"", ""timedata"", input_data)\n\n    output_file = BytesIO()\n    with TdmsWriter(output_file) as tdms_writer:\n        tdms_writer.write_segment([segment])\n\n    output_file.seek(0)\n    tdms_file = TdmsFile(output_file)\n\n    output_data = tdms_file[""group""][""timedata""].data\n\n    assert len(output_data) == 3\n    assert output_data[0] == input_data[0]\n    assert output_data[1] == input_data[1]\n    assert output_data[2] == input_data[2]\n\n\ndef test_can_write_string_data():\n    input_data = [\n        ""hello world"",\n        u""\\u3053\\u3093\\u306b\\u3061\\u306f\\u4e16\\u754c""]\n\n    segment = ChannelObject(""group"", ""string_data"", input_data)\n\n    output_file = BytesIO()\n    with TdmsWriter(output_file) as tdms_writer:\n        tdms_writer.write_segment([segment])\n\n    output_file.seek(0)\n    tdms_file = TdmsFile(output_file)\n\n    output_data = tdms_file[""group""][""string_data""].data\n\n    assert len(output_data) == 2\n    assert output_data[0] == input_data[0]\n    assert output_data[1] == input_data[1]\n\n\ndef test_can_write_floats_from_list():\n    input_data = [1.0, 2.0, 3.0]\n\n    segment = ChannelObject(""group"", ""data"", input_data)\n\n    output_file = BytesIO()\n    with TdmsWriter(output_file) as tdms_writer:\n        tdms_writer.write_segment([segment])\n\n    output_file.seek(0)\n    tdms_file = TdmsFile(output_file)\n\n    output_data = tdms_file[""group""][""data""].data\n\n    assert output_data.dtype == np.float64\n    assert len(output_data) == 3\n    assert output_data[0] == input_data[0]\n    assert output_data[1] == input_data[1]\n    assert output_data[2] == input_data[2]\n\n\ndef test_can_write_ints_from_list():\n    test_cases = [\n        (np.int8, [0, 1]),\n        (np.int8, [-2 ** 7, 0]),\n        (np.int8, [0, 2 ** 7 - 1]),\n\n        (np.uint8, [0, 2 ** 7]),\n        (np.uint8, [0, 2 ** 8 - 1]),\n\n        (np.int16, [-2 ** 15, 0]),\n        (np.int16, [0, 2 ** 15 - 1]),\n\n        (np.uint16, [0, 2 ** 15]),\n        (np.uint16, [0, 2 ** 16 - 1]),\n\n        (np.int32, [-2 ** 31, 0]),\n        (np.int32, [0, 2 ** 31 - 1]),\n\n        (np.uint32, [0, 2 ** 31]),\n        (np.uint32, [0, 2 ** 32 - 1]),\n\n        (np.int64, [-2 ** 63, 0]),\n        (np.int64, [0, 2 ** 63 - 1]),\n\n        (np.uint64, [0, 2 ** 63]),\n        (np.uint64, [0, 2 ** 64 - 1]),\n    ]\n\n    for expected_type, input_data in test_cases:\n        test_case = ""data = %s, expected_type = %s"" % (\n            input_data, expected_type)\n        segment = ChannelObject(""group"", ""data"", input_data)\n\n        output_file = BytesIO()\n        with TdmsWriter(output_file) as tdms_writer:\n            tdms_writer.write_segment([segment])\n\n        output_file.seek(0)\n        tdms_file = TdmsFile(output_file)\n\n        output_data = tdms_file[""group""][""data""].data\n\n        assert output_data.dtype == expected_type, test_case\n        assert len(output_data) == len(input_data), test_case\n        for (input_val, output_val) in zip(input_data, output_data):\n            assert output_val == input_val, test_case\n\n\ndef test_can_write_complex():\n    input_complex64_data = np.array([1+2j, 3+4j], np.complex64)\n    input_complex128_data = np.array([5+6j, 7+8j], np.complex128)\n\n    complex64_segment = ChannelObject(\n            ""group"", ""complex64_data"", input_complex64_data)\n    complex128_segment = ChannelObject(\n            ""group"", ""complex128_data"", input_complex128_data)\n\n    output_file = BytesIO()\n    with TdmsWriter(output_file) as tdms_writer:\n        tdms_writer.write_segment([complex64_segment])\n        tdms_writer.write_segment([complex128_segment])\n\n    output_file.seek(0)\n    tdms_file = TdmsFile(output_file)\n\n    output_data = tdms_file[""group""][""complex64_data""].data\n    assert output_data.dtype == np.complex64\n    assert len(output_data) == 2\n    assert output_data[0] == input_complex64_data[0]\n    assert output_data[1] == input_complex64_data[1]\n\n    output_data = tdms_file[""group""][""complex128_data""].data\n    assert output_data.dtype == np.complex128\n    assert len(output_data) == 2\n    assert output_data[0] == input_complex128_data[0]\n    assert output_data[1] == input_complex128_data[1]\n'"
nptdms/test/writer/test_tdms_segment.py,6,"b'""""""Test TdmsSegment""""""\n\nfrom datetime import datetime\nimport pytest\ntry:\n    from collections import OrderedDict\nexcept ImportError:\n    OrderedDict = dict\nimport numpy as np\n\nfrom nptdms.writer import TdmsSegment, read_properties_dict\nfrom nptdms.types import *\n\n\nclass StubObject(object):\n    def __init__(self, **kwds):\n        self.__dict__.update(kwds)\n\n\ndef test_write_leadin_with_one_channel():\n    data_type = StubObject(size=4)\n\n    channel = StubObject(\n        path="""",\n        has_data=True,\n        data=[0] * 10,\n        data_type=data_type)\n\n    toc = [""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""]\n    metadata_size = 12\n\n    segment = TdmsSegment([channel])\n    leadin = segment.leadin(toc, metadata_size)\n\n    expected_values = [\n        Bytes(b\'TDSm\'),\n        Int32(14),  # TOC bitmask\n        Int32(4712),  # TDMS version\n        Uint64(52),  # Next segment offset\n        Uint64(12),  # Raw data offset\n        ]\n\n    _assert_sequence_equal(leadin, expected_values)\n\n\ndef test_write_leadin_with_object_without_data():\n    channel = StubObject(\n        path="""",\n        has_data=False)\n\n    toc = [""kTocMetaData"", ""kTocRawData"", ""kTocNewObjList""]\n    metadata_size = 12\n\n    segment = TdmsSegment([channel])\n    leadin = segment.leadin(toc, metadata_size)\n\n    expected_values = [\n        Bytes(b\'TDSm\'),\n        Int32(14),  # TOC bitmask\n        Int32(4712),  # TDMS version\n        Uint64(12),  # Next segment offset\n        Uint64(12),  # Raw data offset\n        ]\n\n    _assert_sequence_equal(leadin, expected_values)\n\n\ndef test_write_metadata_with_properties():\n    data_type = StubObject(enum_value=3)\n\n    # Use an ordered dict for properties so that\n    # the order of properties in metadata is guaranteed\n    properties = OrderedDict()\n    properties[""prop1""] = String(""foo"")\n    properties[""prop2""] = Int32(42)\n\n    channel = StubObject(\n        path=""channel_path"",\n        has_data=True,\n        data=[1] * 10,\n        data_type=data_type,\n        properties=properties)\n\n    segment = TdmsSegment([channel])\n    metadata = segment.metadata()\n\n    expected_values = [\n        Uint32(1),  # Number of objects\n        String(""channel_path""),\n        Uint32(20),  # Length of raw data index in bytes\n        Int32(3),  # Data type\n        Uint32(1),  # Array dimension\n        Uint64(10),  # Number of values\n        Uint32(2),  # Number of properties\n        String(""prop1""),  # Property name\n        Int32(0x20),\n        String(""foo""),\n        String(""prop2""),\n        Int32(3),\n        Int32(42),\n        ]\n\n    _assert_sequence_equal(metadata, expected_values)\n\n\ndef test_write_metadata_with_no_data():\n    obj = StubObject(\n        path=""object_path"",\n        has_data=False,\n        properties={})\n\n    segment = TdmsSegment([obj])\n    metadata = segment.metadata()\n\n    expected_values = [\n        Uint32(1),  # Number of objects\n        String(""object_path""),\n        Bytes(b\'\\xFF\\xFF\\xFF\\xFF\'),  # Raw data index\n        Uint32(0),  # Number of properties\n        ]\n\n    _assert_sequence_equal(metadata, expected_values)\n\n\ndef test_properties_are_converted_to_tdms_types():\n    test_time = datetime.utcnow()\n\n    properties = {\n        ""prop1"": Int32(1),\n        ""prop2"": 2,\n        ""prop3"": ""foo"",\n        ""prop4"": True,\n        ""prop5"": 3.142,\n        ""prop6"": test_time,\n    }\n\n    tdms_properties = read_properties_dict(properties)\n\n    assert len(tdms_properties) == len(properties)\n    assert tdms_properties[""prop1""] == Int32(1)\n    assert tdms_properties[""prop2""] == Int32(2)\n    assert tdms_properties[""prop3""] == String(""foo"")\n    assert tdms_properties[""prop4""] == Boolean(True)\n    assert tdms_properties[""prop5""] == DoubleFloat(3.142)\n    assert tdms_properties[""prop6""] == TimeStamp(test_time)\n\n\ndef test_datetime_converted_when_it_only_has_date_part():\n    test_time = np.datetime64(\'2017-11-19\')\n\n    properties = {\n        ""time_prop"": test_time,\n    }\n\n    tdms_properties = read_properties_dict(properties)\n\n    assert tdms_properties[""time_prop""] == TimeStamp(test_time)\n\n\ndef test_writing_long_integer_properties():\n    properties = {\n        ""prop1"": 2147483647,\n        ""prop2"": 2147483648,\n    }\n\n    tdms_properties = read_properties_dict(properties)\n\n    assert len(tdms_properties) == len(properties)\n    assert tdms_properties[""prop1""] == Int32(2147483647)\n    assert tdms_properties[""prop2""] == Int64(2147483648)\n\n\ndef test_writing_properties_with_numpy_typed_values():\n    properties = {\n        ""int32prop"": np.int32(32),\n        ""int64prop"": np.int64(64),\n        ""float32prop"": np.float32(32.0),\n        ""float64prop"": np.float64(64.0),\n        ""boolprop"": np.bool_(True),\n    }\n\n    tdms_properties = read_properties_dict(properties)\n\n    assert len(tdms_properties) == len(properties)\n    assert tdms_properties[""int32prop""] == Int32(32)\n    assert tdms_properties[""int64prop""] == Int64(64)\n    assert tdms_properties[""float32prop""] == SingleFloat(32.0)\n    assert tdms_properties[""float64prop""] == DoubleFloat(64.0)\n    assert tdms_properties[""boolprop""] == Boolean(True)\n\n\ndef test_error_raised_when_cannot_convert_property_value():\n    properties = {\n        ""prop1"": None\n    }\n\n    with pytest.raises(TypeError):\n        read_properties_dict(properties)\n\n\ndef _assert_sequence_equal(values, expected_values):\n    position = 1\n    expected_values = iter(expected_values)\n    for val in values:\n        try:\n            expected = next(expected_values)\n        except StopIteration:\n            raise ValueError(\n                ""Expected end of sequence at position %d but found: %r"" %\n                (position, val))\n        assert val == expected, ""Expected %r to equal %r at position %d"" % (val, expected, position)\n        position += 1\n    try:\n        expected = next(expected_values)\n        raise ValueError(\n            ""Expected %r at position %d but found end of sequence"" %\n            (expected, position))\n    except StopIteration:\n        pass\n'"
