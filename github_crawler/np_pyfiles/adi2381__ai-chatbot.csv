file_path,api_count,code
chatbot.py,4,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Oct 7 23:48:33 2019\n\n@author: Aditya\n""""""\n\n\n########### Building a ChatBot with Deep NLP ###########\n \n\n# Importing the libraries\nimport numpy as np\nimport tensorflow as tf\nimport re\nimport time\n \n\n\n### Phase 1: Data Preprocessing ###\n \n\n\n# Importing the dataset\n\'\'\'\nAdding encoding = \'utf-8\', errors = \'ignore\' to open file commands below are to make sure that any files\nsuch as dataset files, csv etc added to the program are encoded properly by which we mean that they are\nset to a language that can be understood by the machine since we have various file formats\n\'\'\'\nlines = open(\'movie_lines.txt\', encoding = \'utf-8\', errors = \'ignore\').read().split(\'\\n\')\nconversations = open(\'movie_conversations.txt\', encoding = \'utf-8\', errors = \'ignore\').read().split(\'\\n\')\n \n# Creating a dictionary that maps each line with its id\n# _line is a temp variable created that is used only for the loop and cannot be used later\nid_to_line = {}\nfor line in lines:\n    _line = line.split(\' +++$+++ \')\n    if len(_line) == 5:\n        id_to_line[_line[0]] = _line[4]\n \n# Creating a list of all of the conversations\n\'\'\'\nWe use an indexing of [:-1] in conversations because -1 is the last row of \nconversations and by taking : in [:-1], we take an index ranging from beginning\nto the last row but since the upper bound is excluded, so, it will also exclude\nthe last row which is the empty row\n\nTo simplify conversations dataset, we used split() to remove brackets, spaces and quotes\nfor the purpose of which we introduced another temp variable _conversation\n\'\'\'\nconversations_ids = []\nfor conversation in conversations[:-1]:\n    _conversation = conversation.split(\' +++$+++ \')[-1][1:-1].replace(""\'"", """").replace("" "", """")\n    conversations_ids.append(_conversation.split(\',\'))\n \n# Getting questions and answers seperately\n\'\'\'\nWe will get questions(which are input) and answers(which are target) seperately\nbefore clean up of questions and answers to simplify the dataset as much as possible\n\nconversation[i] refers to the index of the first element, so index for answer\nwe want to append should be the next index, so, it will be [i+1]\n\'\'\'\nquestions = []\nanswers = []\nfor conversation in conversations_ids:\n    for i in range(len(conversation) - 1):\n        questions.append(id_to_line[conversation[i]])\n        answers.append(id_to_line[conversation[i+1]])\n \n# Simplifying and cleaning the text using Regular Expressions\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r""i\'m"", ""i am"", text)\n    text = re.sub(r""he\'s"", ""he is"", text)\n    text = re.sub(r""she\'s"", ""she is"", text)\n    text = re.sub(r""that\'s"", ""that is"", text)\n    text = re.sub(r""what\'s"", ""what is"", text)\n    text = re.sub(r""where\'s"", ""where is"", text)\n    text = re.sub(r""how\'s"", ""how is"", text)\n    text = re.sub(r""\\\'ll"", "" will"", text)\n    text = re.sub(r""\\\'ve"", "" have"", text)\n    text = re.sub(r""\\\'re"", "" are"", text)\n    text = re.sub(r""\\\'d"", "" would"", text)\n    text = re.sub(r""n\'t"", "" not"", text)\n    text = re.sub(r""won\'t"", ""will not"", text)\n    text = re.sub(r""can\'t"", ""cannot"", text)\n    text = re.sub(r""[-()\\""#/@;:<>{}`+=~|.!?,]"", """", text)\n    return text\n \n# Cleaning questions\nclean_questions = []\nfor question in questions:\n    clean_questions.append(clean_text(question))\n \n# Cleaning answers\nclean_answers = []\nfor answer in answers:\n    clean_answers.append(clean_text(answer))\n \n# Filtering out the questions and answers that are too short or too long\nshort_questions = []\nshort_answers = []\ni = 0\nfor question in clean_questions:\n    if 2 <= len(question.split()) <= 25:\n        short_questions.append(question)\n        short_answers.append(clean_answers[i])\n    i += 1\nclean_questions = []\nclean_answers = []\ni = 0\nfor answer in short_answers:\n    if 2 <= len(answer.split()) <= 25:\n        clean_answers.append(answer)\n        clean_questions.append(short_questions[i])\n    i += 1\n \n# Creating a dictionary that maps each word to its number of occurrences\n\'\'\'\nWe are shifting the sequences of questions and answers by 1, therefore the\nprevious answer beccomes the next question after the shift due to which we\nget an overlap in conversation when it goes back and forth for several lines\n\'\'\'\nword_to_count = {}\nfor question in clean_questions:\n    for word in question.split():\n        if word not in word_to_count:\n            word_to_count[word] = 1\n        else:\n            word_to_count[word] += 1\nfor answer in clean_answers:\n    for word in answer.split():\n        if word not in word_to_count:\n            word_to_count[word] = 1\n        else:\n            word_to_count[word] += 1\n \n# Creating two dictionaries that map the words in the questions and the answers to a unique integer\n\'\'\'\nWe created two different dictionaries so we can apply different thresholds in order to\nfilter out non-frequent words in both the dictionaries. Also, both the dictionaries\nended up being identical because answers to previous questions become next questions\nto the next answers.\n\'\'\'\nthreshold_questions = 15\nquestions_words_to_int = {}\nword_number = 0\nfor word, count in word_to_count.items():\n    if count >= threshold_questions:\n        questions_words_to_int[word] = word_number\n        word_number += 1\n        \nthreshold_answers = 15\nanswers_words_to_int = {}\nword_number = 0\nfor word, count in word_to_count.items():\n    if count >= threshold_answers:\n        answers_words_to_int[word] = word_number\n        word_number += 1\n \n# Adding the last tokens to above two dictionaries\n\'\'\' \n<SOS> -> Appended to Start of sequence\n<EOS> -> Appended to End of sequence\n<OUT> -> Filtering words out\n<PAD> -> Replaces empty cells using <PAD> token\n\'\'\'\ntokens = [\'<PAD>\', \'<EOS>\', \'<OUT>\', \'<SOS>\']\nfor token in tokens:\n    questions_words_to_int[token] = len(questions_words_to_int) + 1\nfor token in tokens:\n    answers_words_to_int[token] = len(answers_words_to_int) + 1\n \n# Creating the inverse dictionary of the answers_words_to_int dictionary\n\'\'\'\nWe create an inverse dictionary because of the need to do inverse mapping\nfrom int to answer_words  in implementation of seq2seq model, also, we need\nto do this only for answer_words_to_int dictionary\n\'\'\'\nanswers_int_to_words = {w_i: w for w, w_i in answers_words_to_int.items()}\n \n# Adding the <EOS> token to the end of every answer\n\'\'\'\nWe only do this for answers because the first element required by the\ndecoder is the <SOS>  token and last element is <EOS> token\n\'\'\'\nfor i in range(len(clean_answers)):\n    clean_answers[i] += \' <EOS>\'\n \n# Translating all the questions and the answers into int & replacing all the words that were filtered out by <OUT> token\nquestions_into_int = []\nfor question in clean_questions:\n    ints = []\n    for word in question.split():\n        if word not in questions_words_to_int:\n            ints.append(questions_words_to_int[\'<OUT>\'])\n        else:\n            ints.append(questions_words_to_int[word])\n    questions_into_int.append(ints)\nanswers_into_int = []\nfor answer in clean_answers:\n    ints = []\n    for word in answer.split():\n        if word not in answers_words_to_int:\n            ints.append(answers_words_to_int[\'<OUT>\'])\n        else:\n            ints.append(answers_words_to_int[word])\n    answers_into_int.append(ints)\n \n# Sorting questions and answers by the length of questions\n\'\'\'\nSorting by length helps us to reduce loss through reducing the use of <PAD> tokens\nused to replace empty cells and thus helps to speed up the training\n\'\'\'\nsorted_clean_questions = []\nsorted_clean_answers = []\nfor length in range(1, 25 + 1):\n    for i in enumerate(questions_into_int):\n        if len(i[1]) == length:\n            sorted_clean_questions.append(questions_into_int[i[0]])\n            sorted_clean_answers.append(answers_into_int[i[0]])\n \n    \n    \n### Phase 2: Building SEQ2SEQ Model ###\n \n    \n    \n# Creating placeholders for the inputs and the targets\n\'\'\'\nThe tf.placeholder accepts 3 parameters (type of data, dimensions of input data, name for input)\n\nDimensions of the input data must be 2 dimensional(in tf.placeholder) because neural networks\ncan only accept inputs that are in batches, as opposed to single input. Thus, we add 1 dimension\ncorresponding to the batch.\n\nkeep_prob is a hyperparameter used to control the dropout rate. Dropout rate is the rate of the\nneurons you choose to overwrite during the one iteration of the training.\n\'\'\'\ndef model_inputs():\n    inputs = tf.placeholder(tf.int32, [None, None], name = \'input\')\n    targets = tf.placeholder(tf.int32, [None, None], name = \'target\')\n    lr = tf.placeholder(tf.float32, name = \'learning_rate\')\n    keep_prob = tf.placeholder(tf.float32, name = \'keep_prob\')\n    return inputs, targets, lr, keep_prob\n \n# Preprocessing the targets\n\'\'\'\nWe have to delete last column of answer dict before adding <SOS> to process target fuctions to\npreserve the max sequence length since after that we do a concat to add <SOS> token at the\nbeginning of the sequence, thus, we must remove the last token befor so that the sequence\nlength doesn\'t go over the max sequence length\n\nNeed for preprocessing? -> Decoder only accepts a certain format of targets(targets must be in batches) \nso preprocessing is necessary \n\n# Explanation: \n    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1])\n    : [0,0] > is the upper left side of tensor (from where you want to begin the extraction)\n    : [batch_size, -1] > taking all the lines except the last column (end, up to where we want to make the extraction)\n    : Last argument tells by how many cells we want to slice when doing the extraction and \n    since we want to get all the lines except last column, we will use a slice of [1,1]\n\'\'\'\ndef preprocess_targets(targets, word_to_int, batch_size):\n    left_side = tf.fill([batch_size, 1], word_to_int[\'<SOS>\'])\n    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n    preprocessed_targets = tf.concat([left_side, right_side], 1)\n    return preprocessed_targets\n \n# Creating the Encoder RNN\n\'\'\'\nLSTM Dropout is a technique used for regularization in neural nets\nand it helps in learning and preventing overfitting with the data.\n\nOverfitting > happens when a model learns the detail and noise in the training data \nto the extent that it negatively impacts the performance of the model on new data\n\nEncoder_cell >  the cell inside the encoder RNN that contains the stacked LSTM layers on which\n                 we apply dropout to improve accuracy\nEncoder_state > output returned by the encoder RNN, right after the last fully connected layer\n\n# Explanation:\n    def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length)\n    : rnn_inputs = corresponds to the model inputs\n    : rnn_size = number of input tensors in encoder rnn layer\n    : num_layers = number of layers\n    : keep_prob = used to apply dropout to improve accuracy of model\n    : sequence_length = length of list of each question in the batch\n\'\'\'\ndef encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n                                                                    cell_bw = encoder_cell,\n                                                                    sequence_length = sequence_length,\n                                                                    inputs = rnn_inputs,\n                                                                    dtype = tf.float32)\n    return encoder_state\n \n# Decoding the training set\n\'\'\'\n> We need the decode training set to decode the encoded questions and answers of the training set (the second part of the Seq2Seq model)\n\n> This function thus, decodes the observations of the training set and also returns output of the decoder at the same time which\nwas for the observation of the training set that is some observation going bak in the neural network to update the weights \nand thus, update the ability of the chatbot to talk like a human.\n\n# Explanation \n    def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n    : encoder_state = decoder gets encoder_state as input to to decoding\n    : decoder_cell = cell in the RNN of the decoder. It contains stacked LSTM layers with dropout applied\n    : decoder_embedded_input = embedding means mapping from discreet objects, such as words, to vectors of real numbers\n    : sequence_length = length of list of each question in the batch\n    : decoding_scope = similar to variable scope, decoding_scope is an object of the var_scope. It is an advanced Data structure\n                        that wraps our tensorflow variables\n    : output_function = used to return output of the decoding\n    : keep_prob = used to apply dropout to improve accuracy of model\n    : batch_size = size of batch (we work with batches in seq2seq)\n\nThe purpose of the embeddings matrix is to compute more efficiently the embedding input.\nBasically, you multiply your vector of inputs by the embeddings matrix to get your embedded inputs. \n\'\'\'\ndef decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n    \'\'\'\n    attention_states; First, we need to get the attention states.\n    To get them, we will have to initialize them as 3-D matrices containing only 0\xe2\x80\x99s.\n    Batch_size is the number of lines, number of columns is going to be 1 and axis is \n    going to be decoder_Cell.output_size\n    \'\'\'\n    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n    \'\'\'\n    attention keys -> keys to be compared with target states\n    attention values -> values that we use to construct context vectors, it is returned by encoder and it should be used by decoder as first element of decoding\n    attention score function -> used to compute similarity between keys and the target state\n    attention_construct_function -> used to build attention states\n\n    \'\'\'\n    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = ""bahdanau"", num_units = decoder_cell.output_size)\n    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n                                                                              attention_keys,\n                                                                              attention_values,\n                                                                              attention_score_function,\n                                                                              attention_construct_function,\n                                                                              name = ""attn_dec_train"")\n    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n                                                                                                              training_decoder_function,\n                                                                                                              decoder_embedded_input,\n                                                                                                              sequence_length,\n                                                                                                              scope = decoding_scope)\n    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n    return output_function(decoder_output_dropout)\n \n# Decoding the test/validation set\n\'\'\'\n-> we need the decode test set to decode the encoded questions answers of either the validation set, or simply new predictions that are not used anyway in the training.\n-> Now, we make same function as above but for new type of observations, these observations are of test set and validation set and\n    won\'t be used for training.\n-> This function is used to predict the oversation of questions we ask the chatbot in the test phase as well as validate them\n    Validation set is the set we make during training\n-> Key difference between this and previous function is that previous function used "".attention_decoder_fn_train"" and in this we use "".attention_decoder_fn_inference"",\n    inference term in this means to deduce logic. The chatbot thus deduce answers logically to answers the questions we ask. So, it understands its own logic and on new\n    observations it will infer for the questions asked.\n-> This function returns the Test_predictions\n\'\'\'\ndef decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = ""bahdanau"", num_units = decoder_cell.output_size)\n    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n                                                                              encoder_state[0],\n                                                                              attention_keys,\n                                                                              attention_values,\n                                                                              attention_score_function,\n                                                                              attention_construct_function,\n                                                                              decoder_embeddings_matrix,\n                                                                              sos_id,\n                                                                              eos_id,\n                                                                              maximum_length,\n                                                                              num_words,\n                                                                              name = ""attn_dec_inf"")\n    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n                                                                                                                test_decoder_function,\n                                                                                                                scope = decoding_scope)\n    return test_predictions\n \n# Creating the Decoder RNN\n\'\'\'\n-> the Lambda function creates fully connected layers which will get features from stacked LSTM and return the final scores\n\n# Explanation : weights = tf.truncated_normal_initializer(stddev = 0.1)\n             -> We need to initialize some weights which will be associated with the neurons of the\n                fully connected layer in our decoder. With above line of code, we have our fully connected weights initialized.\n\'\'\'\ndef decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n    with tf.variable_scope(""decoding"") as decoding_scope:\n        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n        weights = tf.truncated_normal_initializer(stddev = 0.1)\n        biases = tf.zeros_initializer()\n        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n                                                                      num_words,\n                                                                      None,\n                                                                      scope = decoding_scope,\n                                                                      weights_initializer = weights,\n                                                                      biases_initializer = biases)\n        training_predictions = decode_training_set(encoder_state,\n                                                   decoder_cell,\n                                                   decoder_embedded_input,\n                                                   sequence_length,\n                                                   decoding_scope,\n                                                   output_function,\n                                                   keep_prob,\n                                                   batch_size)\n        decoding_scope.reuse_variables()\n        test_predictions = decode_test_set(encoder_state,\n                                           decoder_cell,\n                                           decoder_embeddings_matrix,\n                                           word2int[\'<SOS>\'],\n                                           word2int[\'<EOS>\'],\n                                           sequence_length - 1,\n                                           num_words,\n                                           decoding_scope,\n                                           output_function,\n                                           keep_prob,\n                                           batch_size)\n    return training_predictions, test_predictions\n \n# Building the seq2seq model\n\'\'\'\n-> This function returns the training predictions and the test predictions but we want them to be returned at the same time. \n    Thus, we are going to assemble encoder that returns encoder state and decoder that returns training and test predictions.\n-> But we also need encoder because in order for decoder to return test and training predictions, it needs to take encoder state returned by \n    encoder and that is why we need those 2 networks in this function.\n\'\'\'\ndef seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questions_words_to_int):\n    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n                                                              answers_num_words + 1,\n                                                              encoder_embedding_size,\n                                                              initializer = tf.random_uniform_initializer(0, 1))\n    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n    preprocessed_targets = preprocess_targets(targets, questions_words_to_int, batch_size)\n    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n                                                         decoder_embeddings_matrix,\n                                                         encoder_state,\n                                                         questions_num_words,\n                                                         sequence_length,\n                                                         rnn_size,\n                                                         num_layers,\n                                                         questions_words_to_int,\n                                                         keep_prob,\n                                                         batch_size)\n    return training_predictions, test_predictions\n \n    \n\n### Phase 3: Training the SEQ2SEQ Model ###\n\n\n\n# Setting the Hyperparameters\n\'\'\'\n> Epochs : an epoch is a whole process of getting the batches of inputs into the neural network and then for propagating them inside the encoders \n    to get the encoder state and for propagating the encoder states with the targets inside the decoder RNN to get the final output. First, final output scores \n    then final answers predicted by chatbot and then back propagating the loss generated by the output and the target back into the Neural network and updating the weight \n    of the better ability of the chatbot to work like a human. An epoch is basically one whole iteration of the training.\n> Num_layers : number of layers you have in encoder and decoder RNN\n> Encoding_embedding_size : it Is the number of columns in embeddings matrix that is the number of columns you want to have for \n    embeddings value where in matrix each line corresponds to each token in the whole question of the corpus\n> Learning_rate : it must be not be too high or too low. If it is too high the model will learn too fast and will not therefore not learn how to speak properly and \n    if it is slow it will take a long for model to learn properly.\n> Learning rate decay :  it means by what percentage the learning rate is reduced over the iterations of the training because we want to start with some learning rate \n    but we want to apply decay over the iterations of the training to reduce the learning rate so it can learn more in depth the logic of human conversations and in our case, \n    or general the correlations found in the dataset.\n> keep_prob : used to apply dropout to improve accuracy of model\n\'\'\'\nepochs = 15\nbatch_size = 32\nrnn_size = 1024\nnum_layers = 3\nencoding_embedding_size = 1024\ndecoding_embedding_size = 1024\nlearning_rate = 0.001\nlearning_rate_decay = 0.9\nmin_learning_rate = 0.0001\nkeep_probability = 0.5\n \n# Defining a session\ntf.reset_default_graph()\nsession = tf.InteractiveSession()\n \n# Loading the model inputs\ninputs, targets, lr, keep_prob = model_inputs()\n \n# Setting the sequence length\nsequence_length = tf.placeholder_with_default(25, None, name = \'sequence_length\')\n \n# Getting the shape of the inputs tensor\n\'\'\'\nWhat is shape of a tensor?\nAll values in a tensor hold identical data type with a known (or partially known) shape. \nThe shape of the data is the dimensionality of the matrix or array.\n\'\'\'\ninput_shape = tf.shape(inputs)\n \n# Getting the training and test predictions\ntraining_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n                                                       targets,\n                                                       keep_prob,\n                                                       batch_size,\n                                                       sequence_length,\n                                                       len(answers_words_to_int),\n                                                       len(questions_words_to_int),\n                                                       encoding_embedding_size,\n                                                       decoding_embedding_size,\n                                                       rnn_size,\n                                                       num_layers,\n                                                       questions_words_to_int)\n \n# Setting up the Loss Error, the Optimizer and Gradient Clipping\n\'\'\'\nWe are using two learning rates:\n    : The first learning rate in model inputs was used in the function to create placeholders for inputs and targets since we are working with TensorFlow (essentially going from arrays to tensors for TF) \n        we have to create the placeholder for it. When we are using the learning rate for training, we are preparing the optimizer but we are using the learning rate that we already prepared in the hyper parameters(0.001)\n    : Gradient Clipping > it is a technique that will cap the gradients in the graph between a minimum value and maximum value and that is to avoid exploding or vanishing gradients issue.\n\n\'\'\'\nwith tf.name_scope(""optimization""):\n    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n                                                  targets,\n                                                  tf.ones([input_shape[0], sequence_length]))\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n    gradients = optimizer.compute_gradients(loss_error)\n    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)\n \n# Padding the sequences with the <PAD> token\ndef apply_padding(batch_of_sequences, word2int):\n    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n    return [sequence + [word2int[\'<PAD>\']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]\n \n# Splitting the data into batches of questions and answers\n\'\'\'\n    We need the start_index to set the first index of the question we are adding in the batch because we are dealing with a specific batch. \n    So, it\'s the first question / answer we are adding to the batch. To get it we are using batch index * batch size due to starting at index 0. \n    The first question added will have the start index of 0, overall in the loop of split into batches it\'s being used for that special indexing.\n\'\'\'\ndef split_into_batches(questions, answers, batch_size):\n    for batch_index in range(0, len(questions) // batch_size):\n        start_index = batch_index * batch_size\n        questions_in_batch = questions[start_index : start_index + batch_size]\n        answers_in_batch = answers[start_index : start_index + batch_size]\n        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questions_words_to_int))\n        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answers_words_to_int))\n        yield padded_questions_in_batch, padded_answers_in_batch\n \n# Splitting the questions and answers into training and validation sets\ntraining_validation_split = int(len(sorted_clean_questions) * 0.15)\ntraining_questions = sorted_clean_questions[training_validation_split:]\ntraining_answers = sorted_clean_answers[training_validation_split:]\nvalidation_questions = sorted_clean_questions[:training_validation_split]\nvalidation_answers = sorted_clean_answers[:training_validation_split]\n \n# Training\n\'\'\'\n> Total_training_loss_error: will be used to compute training losses on 100 batches\n> List_validation_loss_error: we have to make a list, we have to use the early stepping technique which consists of checking if we reached a loss that is \n    below the minimum of all the losses we got and all the losses we get, we put it in this list.\n> Early_stopping_check : corresponds to the number of checks each time there is no improvement of validation loss, so each time we don\xe2\x80\x99t reduce the validation loss, \n    early stepping check is going to be incremented by one and one it reaches a certain number which is going to be our next variable -> early_stopping_stop, we will stop the training.\n    we choose 1000 because we want to make the training last until the last epoch which is 100, but if you want to apply early stepping then use a value of 100.\n> Checkpoint : this variable will be used to save the weights which we will be able to load whenever we want to chat with our trained chatbot.\n> Session.run(tf.global_variable_initializer()) : used to run a session\n> Batch_time = ending_time \xe2\x80\x93 starting_time : used to compute the training time of each batch\n\n\'\'\'\nbatch_index_check_training_loss = 100\nbatch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\ntotal_training_loss_error = 0\nlist_validation_loss_error = []\nearly_stopping_check = 0\nearly_stopping_stop = 100\ncheckpoint = ""./chatbot_weights.ckpt""\nsession.run(tf.global_variables_initializer())\nfor epoch in range(1, epochs + 1):\n    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n        starting_time = time.time()\n        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n                                                                                               targets: padded_answers_in_batch,\n                                                                                               lr: learning_rate,\n                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n                                                                                               keep_prob: keep_probability})\n        total_training_loss_error += batch_training_loss_error\n        ending_time = time.time()\n        batch_time = ending_time - starting_time\n        if batch_index % batch_index_check_training_loss == 0:\n            print(\'Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds\'.format(epoch,\n                                                                                                                                       epochs,\n                                                                                                                                       batch_index,\n                                                                                                                                       len(training_questions) // batch_size,\n                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n            total_training_loss_error = 0\n        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n            total_validation_loss_error = 0\n            starting_time = time.time()\n            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n                                                                       targets: padded_answers_in_batch,\n                                                                       lr: learning_rate,\n                                                                       sequence_length: padded_answers_in_batch.shape[1],\n                                                                       keep_prob: 1})\n                total_validation_loss_error += batch_validation_loss_error\n            ending_time = time.time()\n            batch_time = ending_time - starting_time\n            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n            print(\'Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds\'.format(average_validation_loss_error, int(batch_time)))\n            learning_rate *= learning_rate_decay\n            if learning_rate < min_learning_rate:\n                learning_rate = min_learning_rate\n            list_validation_loss_error.append(average_validation_loss_error)\n            if average_validation_loss_error <= min(list_validation_loss_error):\n                print(\'I can speak better now!\')\n                early_stopping_check = 0\n                saver = tf.train.Saver()\n                saver.save(session, checkpoint)\n            else:\n                print(""Sorry, I need to practice more to speak better."")\n                early_stopping_check += 1\n                if early_stopping_check == early_stopping_stop:\n                    break\n    if early_stopping_check == early_stopping_stop:\n        print(""My apologies, I cannot speak better than this. This is the best I can do."")\n        break\nprint(""Over"")\n \n\n\n### Phase 4: Testing The Seq2Seq Model ###\n\n\n \n# Loading the weights and Running the session\ncheckpoint = ""./chatbot_weights.ckpt""\nsession = tf.InteractiveSession()\nsession.run(tf.global_variables_initializer())\nsaver = tf.train.Saver()\nsaver.restore(session, checkpoint)\n \n# Converting the questions from strings to lists of encoding integers\ndef convert_string2int(question, word2int):\n    question = clean_text(question)\n    return [word2int.get(word, word2int[\'<OUT>\']) for word in question.split()]\n \n# Setting up the chat\nwhile(True):\n    question = input(""You: "")\n    if question == \'goodbye\':\n        break\n    question = convert_string2int(question, questions_words_to_int)\n    question = question + [questions_words_to_int[\'<PAD>\']] * (25 - len(question))\n    fake_batch = np.zeros((batch_size, 25))\n    fake_batch[0] = question\n    predicted_answer = session.run(test_predictions, {inputs: fake_batch, keep_prob: 0.5})[0]\n    answer = \'\'\n    for i in np.argmax(predicted_answer, 1):\n        if answers_int_to_words[i] == \'i\':\n            token = \' I\'\n        elif answers_int_to_words[i] == \'<EOS>\':\n            token = \'.\'\n        elif answers_int_to_words[i] == \'<OUT>\':\n            token = \'out\'\n        else:\n            token = \' \' + answers_int_to_words[i]\n        answer += token\n        if token == \'.\':\n            break\n    print(\'ChatBot: \' + answer)'"
