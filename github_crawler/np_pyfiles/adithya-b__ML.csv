file_path,api_count,code
ML_with_Numpy/GradientNNewtonOpti_Illustration.py,9,"b'import numpy as np\nfrom sympy import *\nfrom sympy.parsing import sympy_parser as spp\n\nweights_matrix = Matrix(symbols(\'w1 w2\'))\n#Tolerable Error while approximating function\nerror = 0.2\n\n#Computing the value of (nx1) Matrix at any given point \ndef computexprmatrix(exprmatrix,x):\n    return [float(exprmatrix[i].subs(weights_matrix[0],x[0]).subs(weights_matrix[1],x[1])) for i in range(len(exprmatrix))]\n\n#Compute the error function at a particular value of weights.This can be extended to compute the value based on training set given \n#Currently it just substitutues the weights and returns the same\ndef computeerror(obj,x):\n    return float(obj.subs(weights_matrix[0],x[0]).subs(weights_matrix[1],x[1]))\n\n#Computes the values of weights by gradient descent\ndef steepestdescent(alpha=0.005):\n    print ""Applying Gradient descent:""\n    gradient=[diff(obj,weights_matrix[i]) for i in range(len(weights_matrix))]\n    weights=weights_start\n    print ""The gradient matrix is {0}"".format(gradient)\n#Here we are trying to minimize the distance between the known root of function and weights vector\n#Instead the below compute error can also be choosen for actual problems\n    #while np.linalg.norm(weights-weights_result)>error:\n    while abs(computeerror(obj,weights))>error:\n        gradient_value=computexprmatrix(gradient,weights)\n        weights=weights-np.dot(alpha,gradient_value)\n    print ""The resultant weight vector is {0}"".format(weights)\n    print ""The value of error function for above weights is{0}"".format(computeerror(obj,weights))\n    print ""The value of distance between two weight vectors is{0}"".format(np.linalg.norm(weights-weights_result))\n\n#Computes the values of weights by newton method of optimization\n#second order optimization\ndef newtonoptimization():\n    print ""Applying Newton optimization:""\n    gradient=Matrix([diff(obj,weights_matrix[i]) for i in range(len(weights_matrix))])\n    hessian=Matrix([[diff(gradient[i],weights_matrix[j]) for j in range(len(weights_matrix))] for i in range(len(gradient))])\n    hessian_inv=hessian.inv() \n    weights=weights_start\n    print ""The gradient matrix is {0}"".format(gradient)\n    print ""The hessian matrix is {0}"".format(gradient)\n#Here we are trying to minimize the distance between the known root of function and weights vector\n#Instead the below compute error can also be choosen for actual problems\n    #while np.linalg.norm(weights-weights_result)>error:\n    while abs(computeerror(obj,weights))>error:\n        gradient_value=Matrix(computexprmatrix(gradient,weights))\n        newton=hessian_inv*gradient\n        newton_value=computexprmatrix(newton,weights)\n        weights=weights-newton_value\n    print ""For Newton optimization the results are as follows:""\n    print ""The resultant weight vector is {0}"".format(weights)\n    print ""The value of error function for above weights is{0}"".format(computeerror(obj,weights))\n    print ""The value of distance between two weight vectors is{0}"".format(np.linalg.norm(weights-weights_result))\n    \n    \nif __name__==\'__main__\':\n    #Below is the Error function which is quadratic\n    obj=spp.parse_expr(\'w1**2 - 2 * w1 * w2 + 4 * w2**2\')\n    #Start weights\n    weights_start=np.array([-4.0, 6.0])\n    #The roots of above quadratic function\n    weights_result= np.array([0.0,0.0])\n    print ""The error function is {0}"".format(obj)\n    #Call for Steepest descent or in other words gradient(first order optimization)\n    steepestdescent()\n    #Call for Newton Optimization(second order optimization)\n    newtonoptimization()\n    #Below is Rosenbrock function\n    obj=spp.parse_expr(\'(1 - w1)**2 + 100 * (w2 - w1**2)**2\')\n    #Start weights\n    weights_start=np.array([-4.0, -5.0])\n    #The roots of above quadratic function\n    weights_result= np.array([1.0,1.0])\n    #Call for Steepest descent or in other words gradient(first order optimization)\n    steepestdescent()\n    #Call for Newton Optimization(second order optimization)\n    newtonoptimization()\n'"
ML_with_Numpy/Gradient_LinearModel.py,0,"b'from numpy import *\nimport sys\ndef run():\n    print ""The data file being trained on is:{0}"".format(sys.argv[1])\n    points=genfromtxt(sys.argv[1], delimiter="","")\n    learning_rate=0.0001\n    initial_m=0\n    initial_b=0\n    num_iterations=1000\n    print ""starting gradient at m={0},b={1} and error={2}"".format(initial_m,initial_b,compute_error(initial_m,initial_b,points))\n    print ""running""\n    [m,b]=optimize_error(initial_m,initial_b,points,num_iterations,learning_rate)\n    print ""After {0} iterations of gradient descent the m={0},b={1} and error={2}"".format(num_iterations,m,b,compute_error(m,b,points))\n\ndef compute_error(m,b,points):\n    totalError=0\n    for i in range(0,len(points)):\n        x=points[i,0]\n        y=points[i,1]\n        expectedvalue=m*x+b\n        totalError=totalError+(y-expectedvalue)**2\n    return totalError/float(len(points))\n\ndef optimize_error(m,b,points,num_iterations,learning_rate):\n    N=float(len(points))\n    m_gradient=0\n    b_gradient=0\n    for i in range(0,num_iterations):\n        for j in range(0,len(points)):\n            x=points[j,0]\n            y=points[j,1]\n            m_gradient += -(2/N)*x*(y-((m*x)+b))\n            b_gradient += -(2/N)*(y-((m*x)+b))\n        m=m-(learning_rate*m_gradient)\n        b=b-(learning_rate*b_gradient)\n    return [m,b]\nif __name__==\'__main__\':\n    run()\n'"
