file_path,api_count,code
1_regression/solutions.py,13,"b'import numpy as np\nfrom typing import Tuple, List\n\ndef my_loss(w_0: float, w_1: float, X: np.ndarray, Y: np.ndarray) -> float:\n    total_error = 0.0\n    N = len(X)\n    for curr_x, curr_y in zip(X, Y):\n        y_pred = w_0 + w_1 * curr_x\n        error = y_pred - curr_y\n        total_error += error ** 2\n    loss = total_error / N\n    return loss\n\ndef my_loss_vectorized(w_0: float, w_1: float, X: np.ndarray, Y: np.ndarray) -> float:\n    Y_pred = w_0 + w_1 * X\n    errors = (Y_pred - Y) ** 2\n    loss = errors.mean()\n    return loss    \n\ndef dLdw_0(w_0: float, w_1: float, X: np.ndarray, Y: np.ndarray) -> float:\n    Y_pred = w_0 + w_1 * X\n    errors = Y_pred - Y\n    return errors.mean() * 2\n\ndef dLdw_1(w_0: float, w_1: float, X: np.ndarray, Y: np.ndarray) -> float:\n    Y_pred = w_0 + w_1 * X\n    errors = Y_pred - Y\n    errors_mul = errors * X\n    return errors_mul.mean() * 2\n\ndef gradient_step_naive(\n    w_0: float, \n    w_1: float, \n    X: np.ndarray, \n    Y: np.ndarray, \n    learning_rate: float\n) -> Tuple[float, float, float]:\n    loss = my_loss(w_0, w_1, X, Y)\n    N = len(X)\n    \n    delta_w_1 = 0.0\n    delta_w_0 = 0.0\n    \n    for curr_x, curr_y in zip(X, Y):\n        y_pred = w_1 * curr_x + w_0 \n        error =  y_pred - curr_y\n        delta_w_1 += error * curr_x\n        delta_w_0 += error\n        \n    delta_w_1 *= (2/N)\n    delta_w_0 *= (2/N)\n    w_1 = w_1 - learning_rate * delta_w_1\n    w_0 = w_0 - learning_rate * delta_w_0\n    return w_0, w_1, loss\n\ndef gradient_step_vectorized(\n    w_0: float, \n    w_1: float, \n    X: np.ndarray, \n    Y: np.ndarray, \n    learning_rate: float\n) -> Tuple[float, float, float]:\n    loss = my_loss_vectorized(w_0, w_1, X, Y)\n    delta_w_0 = dLdw_0(w_0, w_1, X, Y) * learning_rate\n    delta_w_1 = dLdw_1(w_0, w_1, X, Y) * learning_rate\n    w_0 = w_0 - delta_w_0\n    w_1 = w_1 - delta_w_1\n    return w_0, w_1, loss\n\ndef train_model(\n    init_w_0: float,\n    init_w_1: float,\n    X: np.ndarray,\n    Y: np.ndarray,\n    learning_rate: float,\n    num_iterations: int\n) -> Tuple[float, float, List[float]]:\n    w_0 = init_w_0\n    w_1 = init_w_1\n    loss_history = [] \n    for i in range(num_iterations):\n        w_0, w_1, loss = gradient_step_vectorized(w_0, w_1, X, Y, learning_rate)\n        loss_history.append(loss)\n    return w_0, w_1, loss_history\n\nX_hard = np.linspace(1, 10)\nnoise = np.random.normal(size=X_hard.shape)\nY_hard = 10 * np.sin(X_hard) + 5 * X_hard - 10 + 3 * noise\n\n'"
2017_12_04_lab7/solutions.py,11,"b""import numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\nnp.random.seed(int(time.time()))\n\n\ndef generate_mess(clusters_num=3, closeness=None, point_range=100, points_in_cluster=50):\n    closeness = 2 / clusters_num if closeness is None else closeness\n    clusters = np.random.rand(clusters_num, 2) * point_range\n    points = ((np.random.rand(points_in_cluster, clusters_num, 2) * point_range - (\n        point_range / 2)) * closeness + clusters).reshape(-1, 2)\n\n    return points\n\n\ndef k_means_iteration(centroids, datapoints):\n    c_t = np.array([c.reshape(-1, 1) for c in centroids.T])\n    d_t = np.array([d.reshape(-1, 1) for d in datapoints.T])\n\n    diffs = np.array([\n        (c_t[i].reshape(-1) ** 2 - 2 * (d_t[i] @ c_t[i].T)).T + d_t[i].reshape(-1) ** 2\n        for i in range(len(c_t))])\n\n    diffs = diffs.sum(axis=0)\n\n    cluster_assignments = np.argmin(diffs, axis=0)\n    clusters = np.array([np.argwhere(cluster_assignments == i) for i in range(len(centroids))])\n    c_t = np.array(\n        [\n            [\n                d_t[i][c].mean() if d_t[i][c].size != 0 else d_t[i].mean() for c in clusters\n            ]\n            for i in range(len(c_t))\n        ])\n\n    return c_t.T, clusters\n\n#\n# def demonstrate_k_means(k_means_iter, num_iterations, centroids, datapoints):\n#     _, clusters = k_means_iter(centroids, datapoints)\n#\n#     centroids_diffs = np.zeros(centroids.shape)\n#     for i in range(num_iterations):\n#         prev_centroids = centroids\n#         centroids, clusters = k_means_iter(centroids, datapoints)\n#         centroids_diffs = centroids - prev_centroids\n#\n#     for c in clusters:\n#         plt.scatter(datapoints[:, 0][c], datapoints[:, 1][c])\n#\n#     plt.scatter(centroids[:, 0], centroids[:, 1], color='black', marker='.')\n#     print(centroids_diffs)\n#     plt.show()\n#\n#\n# def k_means_img_compression(k_means_iter, num_iterations, centroids, image):\n#     datapoints = np.copy(image.reshape(-1, 3))\n#     _, clusters = k_means_iter(centroids, datapoints)\n#     for i in range(num_iterations):\n#         centroids, clusters = k_means_iter(centroids, datapoints)\n#\n#     for i in range(len(clusters)):\n#         datapoints[clusters[i]] = centroids[i]\n#     img_reconstructed = datapoints.reshape(image.shape)\n#     print(num_iterations)\n#     plt.imshow(image[:, :, [2, 1, 0]])\n#     plt.show()\n#     plt.imshow(img_reconstructed[:, :, [2, 1, 0]])\n#     plt.show()\n"""
2_multivariate_regression/solutions.py,7,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, List\n\ndef hypotheses(W, X):\n    return X @ W\n\ndef loss(W, X, Y):\n    h = hypotheses(W, X)\n    errors = h - Y\n    errors_squared = errors ** 2\n    return np.mean(errors_squared) / 2\n\n\ndef gradient_step(W, X, Y, learning_rate=0.01):\n    H = hypotheses(W, X)\n    errors = H - Y\n    epsilons = X.transpose() @ errors / len(errors)\n    return W - epsilons * learning_rate\n\ndef train_model(\n    init_W: np.ndarray,\n    X: np.ndarray,\n    Y: np.ndarray,\n    learning_rate: float,\n    num_iterations: int\n) -> Tuple[np.ndarray, List[float]]:\n    W = init_W\n    loss_history = []\n    for i in range(num_iterations):\n        loss_history.append(loss(W, X, Y))\n        W = gradient_step(W, X, Y, learning_rate)\n    return W, loss_history\n        \n\ndef mean_normalization(feature_matrix, means=None, ranges=None):\n    mins = feature_matrix.min(axis=0)\n    maxs = feature_matrix.max(axis=0)\n    means = feature_matrix.mean(axis=0) if means is None else means\n    ranges = maxs - mins if ranges is None else ranges\n    # we alter ranges and means vector so that x_0 remains unaffected\n    ranges[0] = 1\n    means[0] = 0\n    return (feature_matrix - means) / ranges, means, ranges\n\nX_1 = np.linspace(1, 10) \nnoise = np.random.randn(X_1.shape[0]) \nY_1 = 2.1 * X_1 + 3.7 + noise'"
3_logistic_regression/solutions.py,14,"b'import numpy as np\n\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    return 1 / (1 + np.exp(-x))\n\ndef _hypotheses(W: np.ndarray, X: np.ndarray) -> np.ndarray:\n    result = X @ W\n    return sigmoid(result)\n\ndef loss(\n    W: np.ndarray, \n    X: np.ndarray, \n    Y: np.ndarray, \n    eps: float = 0.01\n) -> float:\n    hypotheses = _hypotheses(W, X)\n    result = Y * np.log(hypotheses + eps) + (1 - Y) * np.log(1 - hypotheses + eps)\n    result = result.mean()\n    result *= -1\n    return result\n\n\ndef gradient_step(\n    W, \n    X, \n    Y,\n    learning_rate=0.01\n) -> np.ndarray:\n    H = _hypotheses(W, X)\n    errors = H - Y\n    epsilons = (X.T.dot(errors)) / len(errors)\n    return W - epsilons * learning_rate\n\n\ndef mean_normalization(\n    feature_matrix: np.ndarray\n) -> np.ndarray:\n    means = feature_matrix.mean(axis=0)\n    mins = feature_matrix.min(axis=0)\n    maxs = feature_matrix.max(axis=0)\n    ranges = maxs - mins\n    # we alter ranges and means vector so that x_0 remains unaffected\n    ranges[0] = 1\n    means[0] = 0\n    return (feature_matrix - means) / ranges\n\n\ndef std_normalization(\n    feature_matrix: np.ndarray, \n    means: np.ndarray = None, \n    ranges: np.ndarray = None\n) -> np.ndarray:\n    means = feature_matrix.mean(axis=0) if means is None else means\n    mins = feature_matrix.min(axis=0)\n    maxs = feature_matrix.max(axis=0)\n    \n    stds = feature_matrix.std(axis=0) if ranges is None else ranges\n    # we alter ranges and means vector so that x_0 remains unaffected\n    stds[0] = 1\n    means[0] = 0\n    return (feature_matrix - means) / stds, means, stds\n'"
2016/2016_12_12_Tytanic/simple.py,0,"b'import pandas as pd\nfrom pandas import Series,DataFrame\n\ntitanic_df = pd.read_csv(""train.csv"")\ntest_df    = pd.read_csv(""test.csv"")\n\ntitanic_df = titanic_df.drop([\'PassengerId\',\'Name\',\'Ticket\',\'Cabin\'], axis=1)\ntest_df    = test_df.drop([\'Name\',\'Ticket\',\'Cabin\'], axis=1)\n\ntitanic_df[""Embarked""] = titanic_df[""Embarked""].fillna(""S"")\n\nembark_dummies_titanic  = pd.get_dummies(titanic_df[\'Embarked\'])\nembark_dummies_titanic.drop([\'S\'], axis=1, inplace=True)\n\nembark_dummies_test  = pd.get_dummies(test_df[\'Embarked\'])\nembark_dummies_test.drop([\'S\'], axis=1, inplace=True)\n\ntitanic_df = titanic_df.join(embark_dummies_titanic)\ntest_df    = test_df.join(embark_dummies_test)\n\ntitanic_df.drop([\'Embarked\'], axis=1,inplace=True)\ntest_df.drop([\'Embarked\'], axis=1,inplace=True)\n\ntest_df[""Fare""].fillna(test_df[""Fare""].median(), inplace=True)\n\n# convert from float to int\ntitanic_df[\'Fare\'] = titanic_df[\'Fare\'].astype(int)\ntest_df[\'Fare\']    = test_df[\'Fare\'].astype(int)\n\ntest_df[""Age""].fillna(test_df[""Age""].median(), inplace=True)\ntitanic_df[""Age""].fillna(titanic_df[""Age""].median(), inplace=True)\n\n\nperson_dummies_titanic  = pd.get_dummies(titanic_df[\'Sex\'])\nperson_dummies_titanic.columns = [\'Female\',\'Male\']\n\nperson_dummies_test  = pd.get_dummies(test_df[\'Sex\'])\nperson_dummies_test.columns = [\'Female\',\'Male\']\n\ntitanic_df = titanic_df.join(person_dummies_titanic)\ntest_df    = test_df.join(person_dummies_test)\n\ntitanic_df.drop([\'Sex\'],axis=1,inplace=True)\ntest_df.drop([\'Sex\'],axis=1,inplace=True)\n\npclass_dummies_titanic  = pd.get_dummies(titanic_df[\'Pclass\'])\npclass_dummies_titanic.columns = [\'Class_1\',\'Class_2\',\'Class_3\']\n\npclass_dummies_test  = pd.get_dummies(test_df[\'Pclass\'])\npclass_dummies_test.columns = [\'Class_1\',\'Class_2\',\'Class_3\']\n\ntitanic_df.drop([\'Pclass\'],axis=1,inplace=True)\ntest_df.drop([\'Pclass\'],axis=1,inplace=True)\n\ntitanic_df = titanic_df.join(pclass_dummies_titanic)\ntest_df    = test_df.join(pclass_dummies_test)\n\n\nX_train = titanic_df.drop(""Survived"",axis=1)[:600]\nY_train = titanic_df[""Survived""][:600]\n\nX_test = titanic_df.drop(""Survived"",axis=1)[600:]\nY_test = titanic_df[""Survived""][600:]\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train, Y_train)\nprint(""{0:.2f}%"".format(logreg.score(X_test, Y_test)*100))\n'"
