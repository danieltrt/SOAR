file_path,api_count,code
basic_image.py,8,"b""#!/usr/bin/env python\n\n__author__ = 'Aleksandar Gyorev'\n__email__  = 'a.gyorev@jacobs-university.de'\n\nimport cv2\nimport numpy as np\n\nclass BasicImage(object):\n    def __init__(self, _image_path):\n        if isinstance(_image_path, basestring) == True:\n            self.image = cv2.imread(_image_path)\n        else:\n            self.image = _image_path\n\n    def get(self):\n        return self.image\n\n    def get_size(self):\n        return self.image.shape\n\n    def show(self):\n        cv2.imshow('Basic Image', self.image)\n        cv2.waitKey(0)\n\n    def save(self, _image_path):\n        cv2.imwrite(_image_path, self.image)\n\n    def resize(self, _type, _size):\n        if _type == 'w' or _type == 'W':\n            ratio = float(_size) / self.image.shape[1]\n            dim   = (_size, int(self.image.shape[0] * ratio))\n        elif _type == 'h' or _type == 'H':\n            ratio = float(_size) / self.image.shape[0]\n            dim   = (int(self.image.shape[1] * ratio), _size)\n        else:\n            return self.image\n\n        result = cv2.resize(self.image, dim, interpolation = cv2.INTER_AREA)\n        return result\n\n    def rotate(self, _angle):\n        (height, width) = self.image.shape[:2]\n        center          = (width / 2, height / 2)\n\n        # make it a 3x3 rotation matrix\n        M = np.vstack([cv2.getRotationMatrix2D(center, _angle, 1.0), [0, 0, 1]])\n        R = np.matrix(M[0:2, 0:2])\n\n        half_height = height / 2.0\n        half_width  = width / 2.0\n\n        # coordinates of the rotated corners\n        rotated_corners = [\n                (np.array([-half_width, half_height]) * R).A[0],\n                (np.array([ half_width, half_height]) * R).A[0],\n                (np.array([-half_width,-half_height]) * R).A[0],\n                (np.array([ half_width,-half_height]) * R).A[0]]\n\n        # new image size\n        x_coords = [point[0] for point in rotated_corners]\n        y_coords = [point[1] for point in rotated_corners]\n\n        right_bound = max(x_coords)\n        left_bound  = min(x_coords)\n        top_bound   = max(y_coords)\n        bot_bound   = min(y_coords)\n\n        new_height = int(abs(top_bound - bot_bound))\n        new_width  = int(abs(right_bound - left_bound))\n\n        # translation matrix to keep it centered\n        T = np.matrix([\n            [1, 0, int(new_width / 2.0 - half_width)],\n            [0, 1, int(new_height / 2.0 - half_height)],\n            [0, 0, 1]])\n\n        # combining rotation and translation\n        M = (np.matrix(T) * np.matrix(M))[0:2, :]\n\n        result = cv2.warpAffine(self.image, M, (new_width, new_height), flags = cv2.INTER_LINEAR)\n        return result\n\n    def crop(self, _top, _bot, _left, _right):\n        result = self.image[_top:_bot + 1, _left:_right + 1]\n        return result\n\n"""
combine_images.py,1,"b""#!/usr/bin/env python\n\n__author__ = 'Aleksandar Gyorev'\n__email__  = 'a.gyorev@jacobs-university.de'\n\nimport cv2\nimport numpy as np\n\nfrom basic_image import BasicImage\n\ndef CombineImages(_height, *args):\n    result = ()\n\n    for img in args:\n        img = BasicImage(img).resize('H', _height)\n\n        if len(img.shape) == 2:\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n\n        result = result + (img,)\n\n    result = np.hstack(result)\n\n    return result\n"""
scan.py,2,"b'#!/usr/bin/env python\n\n__author__ = \'Aleksandar Gyorev\'\n__email__  = \'a.gyorev@jacobs-university.de\'\n\nimport cv2\nimport numpy as np\nimport argparse\n\nfrom transform import Transform\nfrom basic_image import BasicImage\nfrom combine_images import CombineImages\n\n"""""" Arugment Parser """"""\nap = argparse.ArgumentParser()\nap.add_argument(\'-i\',\n    \'--image\',\n    required = True,\n    help     = \'path to the image\')\n\nap.add_argument(\'-H\',\n    \'--height\',\n    required = False,\n    default  = 300,\n    help     = \'height of the image image we will process and use for finding the contours (default: 300)\')\n\nap.add_argument(\'-n\',\n    \'--noise\',\n    required = False,\n    default  = 0,\n    help     = \'the level to which we remove noise and smaller details from the scan (default: 0, i.e. preserve everything\')\n\nap.add_argument(\'-c\',\n    \'--closing\',\n    required = False,\n    default  = 3,\n    help     = \'the size of the closing element after applying the Canny edge detector\')\n\nap.add_argument(\'-a\',\n    \'--auto\',\n    required = False,\n    action   = \'store_true\',\n    default  = False,\n    help     = \'if we want to have automatically set values for the height and closing when looking for objects\')\n\nap.add_argument(\'-s\',\n    \'--save\',\n    action   = \'store_true\',\n    default  = False,\n    help     = \'set the flag in order to save the extracted images to the current folder\')\nargs         = vars(ap.parse_args())\n\n# Getting the user input\nHEIGHT              = int(args[\'height\'])\nNOISE_REMOVAL_LEVEL = max(int(args[\'noise\']) * 2 - 1, 0)\nCLOSING_SIZE        = int(args[\'closing\'])\nbi                  = BasicImage(args[\'image\'])\n\ndef scan():\n    """""" Step 1: Edge Detection """"""\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # get the grayscale image\n    gray = cv2.bilateralFilter(gray, 11, 17, 17)\n    #gray = cv2.GaussianBlur(gray, (3, 3), 0) # with a bit of blurring\n    #BasicImage(gray).show()\n\n    # automatic Canny edge detection thredhold computation\n    high_thresh, thresh_im = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    low_thresh = high_thresh / 2.0\n\n    # zero-parameter automatic Canny edge detection (method 2)\n    # Vary the percentage thresholds that are determined (in practice 0.33 tends to give good approx. results)\n    # A lower value of sigma  indicates a tighter threshold, whereas a larger value of sigma  gives a wider threshold.\n    #sigma       = 0.33\n    #v           = np.median(gray)\n    #low_thresh  = int(max(0, (1.0 - sigma) * v))\n    #high_thresh = int(min(255, (1.0 + sigma) * v))\n\n    edged = cv2.Canny(gray, low_thresh, high_thresh) # detect edges (outlines) of the objects\n    #BasicImage(edged).show()\n\n    # since some of the outlines are not exactly clear, we construct\n    # and apply a closing kernel to close the gaps b/w white pixels\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (CLOSING_SIZE, CLOSING_SIZE))\n    closed = cv2.morphologyEx(edged, cv2.MORPH_CLOSE, kernel)\n    #BasicImage(closed).show()\n\n    """""" Step 2: Finding Contours """"""\n    (contours, _) = cv2.findContours(closed.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    total = 0\n\n    # looping over the contours found\n    approx_all = []\n    for contour in contours:\n        # approximating the contour\n        contour = cv2.convexHull(contour)\n        peri    = cv2.arcLength(contour, True)\n        approx  = cv2.approxPolyDP(contour, 0.02 * peri, True)\n        area    = cv2.contourArea(contour)\n\n        # we don\'t consider anything less than 5% of the whole image\n        if area < 0.05 * total_area:\n            continue\n\n        # if the approximated contour has 4 points, then assumer it is a book\n        # a book is a rectangle and thus it has 4 vertices\n        if len(approx) == 4:\n            cv2.drawContours(image, [approx], -1, (0, 255, 0), 4)\n            approx_all.append(approx)\n            total += 1\n\n    print \'Found %d books/papers in the image.\' % total\n    #BasicImage(image).show()\n\n    # no point of displaying anything if we couldn\'t find any books\n    if total != 0:\n        """""" Displaying all intermediate steps into one image """"""\n        top_row = CombineImages(300, original, gray)\n        bot_row = CombineImages(300, closed, image)\n        BasicImage(top_row).show()\n        BasicImage(bot_row).show()\n        #com_img = np.vstack((top_row, bot_row))\n        #BasicImage(com_img).show()\n\n        """""" Step 3: Apply a Perspective Transform and Threshold """"""\n        total = 0\n        for approx in approx_all:\n            total += 1\n            warped = Transform.get_box_transform(original, approx.reshape(4, 2) * ratio)\n            #BasicImage(warped).show()\n\n            scan_warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n            scan_warped = cv2.medianBlur(scan_warped, NOISE_REMOVAL_LEVEL)\n            scan_warped = cv2.adaptiveThreshold(scan_warped, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n            #BasicImage(scan_warped).show()\n            BasicImage(CombineImages(400, warped, scan_warped)).show()\n\n            # save the image\n            if args[\'save\'] == True:\n                filename_color = \'scan%03d_color.jpg\' % total\n                filename_scan  = \'scan%03d_scan.jpg\' % total\n                BasicImage(warped).save(filename_color)\n                BasicImage(scan_warped).save(filename_scan)\n\n    return total\n\nif args[\'auto\'] == False:\n    original   = bi.get().copy()\n    ratio      = original.shape[0] / float(HEIGHT)\n    image      = bi.resize(\'H\', HEIGHT)\n    total_area = image.shape[0] * image.shape[1]\n\n    #BasicImage(image).show()\n\n    scan()\nelse:\n    for auto_height in xrange(min(650, bi.get().shape[0]), 299, -50):\n        for auto_closing in xrange(6, 1, -1):\n            HEIGHT       = auto_height\n            CLOSING_SIZE = auto_closing\n\n            original     = bi.get().copy()\n            ratio        = original.shape[0] / float(HEIGHT)\n            image        = bi.resize(\'H\', HEIGHT)\n            total_area   = image.shape[0] * image.shape[1]\n\n            print \'auto_height = \', auto_height\n            print \'auto_closing= \', auto_closing\n            if scan() != 0:\n                exit(0)\n\n'"
transform.py,11,"b""#!/usr/bin/env python\n\n__author__ = 'Aleksandar Gyorev'\n__email__  = 'a.gyorev@jacobs-university.de'\n\nimport cv2\nimport numpy as np\n\nclass Transform(object):\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def get_points_order(_points):\n        box        = np.zeros((4, 2), dtype='float32') # order will be: TL, TR, BR, BL\n\n        coord_sum  = _points.sum(axis = 1)\n        box[0]     = _points[np.argmin(coord_sum)]     # TL - has the min sum\n        box[2]     = _points[np.argmax(coord_sum)]     # BR - has the max sum\n\n        coord_diff = np.diff(_points, axis = 1)\n        box[1]     = _points[np.argmin(coord_diff)]    # TR - has the min diff\n        box[3]     = _points[np.argmax(coord_diff)]    # BL - has the max diff\n\n        return box                                     # return the ordered coordinates\n\n    @staticmethod\n    def get_box_transform(_image, _points):\n        init_box         = Transform.get_points_order(_points)\n        (tl, tr, br, bl) = init_box # get the correct order\n\n        width_top    = np.sqrt(((tl[0] - tr[0]) ** 2) + ((tl[1] - tr[1]) ** 2)) # distance b/w TL and TR\n        width_bot    = np.sqrt(((bl[0] - br[0]) ** 2) + ((bl[1] - br[1]) ** 2)) # distance b/w BL and BR\n        max_width    = max(int(width_top), int(width_bot))                      # the width of the new image\n\n        height_left  = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2)) # distance b/w TL and BL\n        height_right = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2)) # distance b/w TR and BR\n        max_height   = max(int(height_left), int(height_right))                 # the height of the new image\n\n        dest_box = np.array([ # the resulting edge points after the transform\n            [            0, 0              ], # TL\n            [max_width - 1, 0              ], # TR\n            [max_width - 1, max_height - 1 ], # BR\n            [            0, max_height - 1]], # BL\n            dtype='float32')\n\n        M     = cv2.getPerspectiveTransform(init_box, dest_box)         # transformation matrix\n        image = cv2.warpPerspective(_image, M, (max_width, max_height)) # apply the transform\n\n        return image # return the warped image\n"""
