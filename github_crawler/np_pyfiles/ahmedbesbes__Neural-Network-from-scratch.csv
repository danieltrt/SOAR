file_path,api_count,code
nn.py,29,"b'# -*- coding: utf-8 -*-\n# Author: Ahmed BESBES \n# <ahmed.besbes@hotmail.com>\n#\n\n# matplotlib for plotting\nimport matplotlib\nmatplotlib.rcParams[\'figure.figsize\'] = (10.0, 10.0)\nfrom matplotlib import pyplot as plt\n\n# numpy for vector and matrix manipulations\nimport numpy as np\n\n# we won\'t use scikit per se but we\'ll use some help functions in it\n# such as accuracy_score, shuffle, and train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\n# tqdm is progress-bar. make sure it\'s installed: pip install tqdm\nfrom tqdm import tqdm\nfrom IPython import display\n\n\ndef activation(z, derivative=False):\n    """"""\n    Sigmoid activation function:\n    It handles two modes: normal and derivative mode.\n    Applies a pointwize operation on vectors\n    \n    Parameters:\n    ---\n    z: pre-activation vector at layer l\n        shape (n[l], batch_size)\n\n    Returns: \n    pontwize activation on each element of the input z\n    """"""\n    if derivative:\n        return activation(z) * (1 - activation(z))\n    else:\n        return 1 / (1 + np.exp(-z))\n\ndef cost_function(y_true, y_pred):\n    """"""\n    Computes the Mean Square Error between a ground truth vector and a prediction vector\n    Parameters:\n    ---\n    y_true: ground-truth vector\n    y_pred: prediction vector\n    Returns:\n    ---\n    cost: a scalar value representing the loss\n    """"""\n    n = y_pred.shape[1]\n    cost = (1./(2*n)) * np.sum((y_true - y_pred) ** 2)\n    return cost\n\ndef cost_function_prime(y_true, y_pred):\n    """"""\n    Computes the derivative of the loss function w.r.t the activation of the output layer\n    Parameters:\n    ---\n    y_true: ground-truth vector\n    y_pred: prediction vector\n    Returns:\n    ---\n    cost_prime: derivative of the loss w.r.t. the activation of the output\n    shape: (n[L], batch_size)    \n    """"""\n    cost_prime = y_pred - y_true\n    return cost_prime\n\n\n\nclass NeuralNetwork(object):     \n    \'\'\'\n    This is a custom neural netwok package built from scratch with numpy. \n    It allows training using SGD, inference and live plotting of the decision boundary.\n    This code is not optimized and should not be used with real-world examples.\n    It\'s written for educational purposes only.\n\n    The Neural Network as well as its parameters and training method and procedure will \n    reside in this class.\n\n    Parameters\n    ---\n    size: list of number of neurons per layer\n\n    Examples\n    ---\n    >>> import NeuralNetwork\n    >>> nn = NeuralNetword([2, 3, 4, 1])\n    \n    This means :\n    1 input layer with 2 neurons\n    1 hidden layer with 3 neurons\n    1 hidden layer with 4 neurons\n    1 output layer with 1 neuron\n    \n    \'\'\'\n\n    def __init__(self, size, seed=42):\n        \'\'\'\n        Instantiate the weights and biases of the network\n        weights and biases are attributes of the NeuralNetwork class\n        They are updated during the training\n        \'\'\'\n        self.seed = seed\n        np.random.seed(self.seed)\n        self.size = size\n        self.weights = [np.random.randn(self.size[i], self.size[i-1]) * np.sqrt(1 / self.size[i-1]) for i in range(1, len(self.size))]\n        self.biases = [np.random.rand(n, 1) for n in self.size[1:]]\n\n    def forward(self, input):\n        \'\'\'\n        Perform a feed forward computation \n\n        Parameters\n        ---\n        input: data to be fed to the network with\n        shape: (input_shape, batch_size)\n\n        Returns\n        ---\n        a: ouptut activation (output_shape, batch_size)\n        pre_activations: list of pre-activations per layer\n        each of shape (n[l], batch_size), where n[l] is the number \n        of neuron at layer l\n        activations: list of activations per layer\n        each of shape (n[l], batch_size), where n[l] is the number \n        of neuron at layer l\n\n        \'\'\'\n        a = input\n        pre_activations = []\n        activations = [a]\n        for w, b in zip(self.weights, self.biases):\n            z = np.dot(w, a) + b\n            a  = activation(z)\n            pre_activations.append(z)\n            activations.append(a)\n        return a, pre_activations, activations\n\n    def compute_deltas(self, pre_activations, y_true, y_pred):\n        """"""\n        Computes a list containing the values of delta for each layer using \n        a recursion\n        Parameters:\n        ---\n        pre_activations: list of of pre-activations. each corresponding to a layer\n        y_true: ground truth values of the labels\n        y_pred: prediction values of the labels\n        Returns:\n        ---\n        deltas: a list of deltas per layer\n        \n        """"""\n        delta_L = cost_function_prime(y_true, y_pred) * activation(pre_activations[-1], derivative=True)\n        deltas = [0] * (len(self.size) - 1)\n        deltas[-1] = delta_L\n        for l in range(len(deltas) - 2, -1, -1):\n            delta = np.dot(self.weights[l + 1].transpose(), deltas[l + 1]) * activation(pre_activations[l], derivative=True) \n            deltas[l] = delta\n        return deltas\n\n    def backpropagate(self, deltas, pre_activations, activations):\n        """"""\n        Applies back-propagation and computes the gradient of the loss\n        w.r.t the weights and biases of the network\n\n        Parameters:\n        ---\n        deltas: list of deltas computed by compute_deltas\n        pre_activations: a list of pre-activations per layer\n        activations: a list of activations per layer\n        Returns:\n        ---\n        dW: list of gradients w.r.t. the weight matrices of the network\n        db: list of gradients w.r.t. the biases (vectors) of the network\n    \n        """"""\n        dW = []\n        db = []\n        deltas = [0] + deltas\n        for l in range(1, len(self.size)):\n            dW_l = np.dot(deltas[l], activations[l-1].transpose()) \n            db_l = deltas[l]\n            dW.append(dW_l)\n            db.append(np.expand_dims(db_l.mean(axis=1), 1))\n        return dW, db\n\n    def plot_decision_regions(self, X, y, iteration, train_loss, val_loss, train_acc, val_acc, res=0.01):\n        """"""\n        Plots the decision boundary at each iteration (i.e. epoch) in order to inspect the performance\n        of the model\n\n        Parameters:\n        ---\n        X: the input data\n        y: the labels\n        iteration: the epoch number\n        train_loss: value of the training loss\n        val_loss: value of the validation loss\n        train_acc: value of the training accuracy\n        val_acc: value of the validation accuracy\n        res: resolution of the plot\n        Returns:\n        ---\n        None: this function plots the decision boundary\n        """"""\n        X, y = X.T, y.T \n        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, res),\n                            np.arange(y_min, y_max, res))\n        \n        Z = self.predict(np.c_[xx.ravel(), yy.ravel()].T)\n        Z = Z.reshape(xx.shape)\n        plt.contourf(xx, yy, Z, alpha=0.5)\n        plt.xlim(xx.min(), xx.max())\n        plt.ylim(yy.min(), yy.max())\n        plt.scatter(X[:, 0], X[:, 1], c=y.reshape(-1),  alpha=0.2)\n        message = \'iteration: {} | train loss: {} | val loss: {} | train acc: {} | val acc: {}\'.format(iteration,\n                                                                                                     train_loss, \n                                                                                                     val_loss, \n                                                                                                     train_acc, \n                                                                                                     val_acc)\n        plt.title(message)\n\n    \n\n    def train(self, X, y, batch_size, epochs, learning_rate, validation_split=0.2, print_every=10, tqdm_=True, plot_every=None):\n        """"""\n        Trains the network using the gradients computed by back-propagation\n        Splits the data in train and validation splits\n        Processes the training data by batches and trains the network using batch gradient descent\n\n        Parameters:\n        ---\n        X: input data\n        y: input labels\n        batch_size: number of data points to process in each batch\n        epochs: number of epochs for the training\n        learning_rate: value of the learning rate\n        validation_split: percentage of the data for validation\n        print_every: the number of epochs by which the network logs the loss and accuracy metrics for train and validations splits\n        tqdm_: use tqdm progress-bar\n        plot_every: the number of epochs by which the network plots the decision boundary\n    \n        Returns:\n        ---\n        history: dictionary of train and validation metrics per epoch\n            train_acc: train accuracy\n            test_acc: validation accuracy\n            train_loss: train loss\n            test_loss: validation loss\n\n        This history is used to plot the performance of the model\n        """"""\n        history_train_losses = []\n        history_train_accuracies = []\n        history_test_losses = []\n        history_test_accuracies = []\n\n        x_train, x_test, y_train, y_test = train_test_split(X.T, y.T, test_size=validation_split, )\n        x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T \n\n        if tqdm_:\n            epoch_iterator = tqdm(range(epochs))\n        else:\n            epoch_iterator = range(epochs)\n\n        for e in epoch_iterator:\n            if x_train.shape[1] % batch_size == 0:\n                n_batches = int(x_train.shape[1] / batch_size)\n            else:\n                n_batches = int(x_train.shape[1] / batch_size ) - 1\n\n            x_train, y_train = shuffle(x_train.T, y_train.T)\n            x_train, y_train = x_train.T, y_train.T\n\n            batches_x = [x_train[:, batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)]\n            batches_y = [y_train[:, batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)]\n\n            train_losses = []\n            train_accuracies = []\n            \n            test_losses = []\n            test_accuracies = []\n\n            dw_per_epoch = [np.zeros(w.shape) for w in self.weights]\n            db_per_epoch = [np.zeros(b.shape) for b in self.biases] \n            \n            for batch_x, batch_y in zip(batches_x, batches_y):\n                batch_y_pred, pre_activations, activations = self.forward(batch_x)\n                deltas = self.compute_deltas(pre_activations, batch_y, batch_y_pred)\n                dW, db = self.backpropagate(deltas, pre_activations, activations)\n                for i, (dw_i, db_i) in enumerate(zip(dW, db)):\n                    dw_per_epoch[i] += dw_i / batch_size\n                    db_per_epoch[i] += db_i / batch_size\n\n                batch_y_train_pred = self.predict(batch_x)\n\n                train_loss = cost_function(batch_y, batch_y_train_pred)\n                train_losses.append(train_loss)\n                train_accuracy = accuracy_score(batch_y.T, batch_y_train_pred.T)\n                train_accuracies.append(train_accuracy)\n\n                batch_y_test_pred = self.predict(x_test)\n\n                test_loss = cost_function(y_test, batch_y_test_pred)\n                test_losses.append(test_loss)\n                test_accuracy = accuracy_score(y_test.T, batch_y_test_pred.T)\n                test_accuracies.append(test_accuracy)\n\n\n            # weight update\n            for i, (dw_epoch, db_epoch) in enumerate(zip(dw_per_epoch, db_per_epoch)):\n                self.weights[i] = self.weights[i] - learning_rate * dw_epoch\n                self.biases[i] = self.biases[i] - learning_rate * db_epoch\n\n            history_train_losses.append(np.mean(train_losses))\n            history_train_accuracies.append(np.mean(train_accuracies))\n            \n            history_test_losses.append(np.mean(test_losses))\n            history_test_accuracies.append(np.mean(test_accuracies))\n\n\n            if not plot_every:\n                if e % print_every == 0:    \n                    print(\'Epoch {} / {} | train loss: {} | train accuracy: {} | val loss : {} | val accuracy : {} \'.format(\n                        e, epochs, np.round(np.mean(train_losses), 3), np.round(np.mean(train_accuracies), 3), \n                        np.round(np.mean(test_losses), 3),  np.round(np.mean(test_accuracies), 3)))\n            else:\n                if e % plot_every == 0:\n                    self.plot_decision_regions(x_train, y_train, e, \n                                                np.round(np.mean(train_losses), 4), \n                                                np.round(np.mean(test_losses), 4),\n                                                np.round(np.mean(train_accuracies), 4), \n                                                np.round(np.mean(test_accuracies), 4), \n                                                )\n                    plt.show()                    \n                    display.display(plt.gcf())\n                    display.clear_output(wait=True)\n\n        self.plot_decision_regions(X, y, e, \n                                    np.round(np.mean(train_losses), 4), \n                                    np.round(np.mean(test_losses), 4),\n                                    np.round(np.mean(train_accuracies), 4), \n                                    np.round(np.mean(test_accuracies), 4), \n                                    )\n\n        history = {\'epochs\': epochs,\n                   \'train_loss\': history_train_losses, \n                   \'train_acc\': history_train_accuracies,\n                   \'test_loss\': history_test_losses,\n                   \'test_acc\': history_test_accuracies\n                   }\n        return history\n\n    def predict(self, a):\n        \'\'\'\n        Use the current state of the network to make predictions\n\n        Parameters:\n        ---\n        a: input data, shape: (input_shape, batch_size)\n\n        Returns:\n        ---\n        predictions: vector of output predictions\n        \'\'\'\n        for w, b in zip(self.weights, self.biases):\n            z = np.dot(w, a) + b\n            a = activation(z)\n        predictions = (a > 0.5).astype(int)\n        return predictions\n'"
