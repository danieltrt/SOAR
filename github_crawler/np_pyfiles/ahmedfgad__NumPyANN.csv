file_path,api_count,code
example.py,0,"b'import numpy\r\nimport pygad.nn\r\n\r\n""""""\r\nThis project creates a neural network where the architecture has input and dense layers only. More layers will be added in the future. \r\nThe project only implements the forward pass of a neural network and no training algorithm is used.\r\nFor training a neural network using the genetic algorithm, check this project (https://github.com/ahmedfgad/NeuralGenetic) in which the genetic algorithm is used for training the network.\r\nFeel free to leave an issue in this project (https://github.com/ahmedfgad/NumPyANN) in case something is not working properly or to ask for questions. I am also available for e-mails at ahmed.f.gad@gmail.com\r\n""""""\r\n\r\n# Reading the data features. Check the \'extract_features.py\' script for extracting the features & preparing the outputs of the dataset.\r\ndata_inputs = numpy.load(""dataset_features.npy"") # Download from https://github.com/ahmedfgad/NumPyANN/blob/master/dataset_features.npy\r\n\r\n# Optional step for filtering the features using the standard deviation.\r\nfeatures_STDs = numpy.std(a=data_inputs, axis=0)\r\ndata_inputs = data_inputs[:, features_STDs > 50]\r\n\r\n# Reading the data outputs. Check the \'extract_features.py\' script for extracting the features & preparing the outputs of the dataset.\r\ndata_outputs = numpy.load(""outputs.npy"") # Download from https://github.com/ahmedfgad/NumPyANN/blob/master/outputs.npy\r\n\r\n# The number of inputs (i.e. feature vector length) per sample\r\nnum_inputs = data_inputs.shape[1]\r\n# Number of outputs per sample\r\nnum_outputs = 4\r\n\r\nHL1_neurons = 150\r\nHL2_neurons = 60\r\n\r\n# Building the network architecture.\r\ninput_layer = pygad.nn.InputLayer(num_inputs)\r\nhidden_layer1 = pygad.nn.DenseLayer(num_neurons=HL1_neurons, previous_layer=input_layer, activation_function=""relu"")\r\nhidden_layer2 = pygad.nn.DenseLayer(num_neurons=HL2_neurons, previous_layer=hidden_layer1, activation_function=""relu"")\r\noutput_layer = pygad.nn.DenseLayer(num_neurons=num_outputs, previous_layer=hidden_layer2, activation_function=""softmax"")\r\n\r\n# Training the network.\r\npygad.nn.train(num_epochs=10,\r\n               last_layer=output_layer,\r\n               data_inputs=data_inputs,\r\n               data_outputs=data_outputs,\r\n               learning_rate=0.01)\r\n\r\n# Using the trained network for predictions.\r\npredictions = pygad.nn.predict(last_layer=output_layer, data_inputs=data_inputs)\r\n\r\n# Calculating some statistics\r\nnum_wrong = numpy.where(predictions != data_outputs)[0]\r\nnum_correct = data_outputs.size - num_wrong.size\r\naccuracy = 100 * (num_correct/data_outputs.size)\r\nprint(""Number of correct classifications : {num_correct}."".format(num_correct=num_correct))\r\nprint(""Number of wrong classifications : {num_wrong}."".format(num_wrong=num_wrong.size))\r\nprint(""Classification accuracy : {accuracy}."".format(accuracy=accuracy))\r\n'"
example_XOR.py,0,"b'import numpy\r\nimport pygad.nn\r\n\r\n""""""\r\nThis project creates a neural network where the architecture has input and dense layers only. More layers will be added in the future. \r\nThe project only implements the forward pass of a neural network and no training algorithm is used.\r\nFor training a neural network using the genetic algorithm, check this project (https://github.com/ahmedfgad/NeuralGenetic) in which the genetic algorithm is used for training the network.\r\nFeel free to leave an issue in this project (https://github.com/ahmedfgad/NumPyANN) in case something is not working properly or to ask for questions. I am also available for e-mails at ahmed.f.gad@gmail.com\r\n""""""\r\n\r\n# Preparing the NumPy array of the inputs.\r\ndata_inputs = numpy.array([[1, 1],\r\n                           [1, 0],\r\n                           [0, 1],\r\n                           [0, 0]])\r\n\r\n# Preparing the NumPy array of the outputs.\r\ndata_outputs = numpy.array([0, \r\n                            1, \r\n                            1, \r\n                            0])\r\n\r\n# The number of inputs (i.e. feature vector length) per sample\r\nnum_inputs = data_inputs.shape[1]\r\n# Number of outputs per sample\r\nnum_outputs = 2\r\n\r\nHL1_neurons = 2\r\n\r\n# Building the network architecture.\r\ninput_layer = pygad.nn.InputLayer(num_inputs)\r\nhidden_layer1 = pygad.nn.DenseLayer(num_neurons=HL1_neurons, previous_layer=input_layer, activation_function=""relu"")\r\noutput_layer = pygad.nn.DenseLayer(num_neurons=num_outputs, previous_layer=hidden_layer1, activation_function=""softmax"")\r\n\r\n# Training the network.\r\npygad.nn.train(num_epochs=10,\r\n               last_layer=output_layer,\r\n               data_inputs=data_inputs,\r\n               data_outputs=data_outputs,\r\n               learning_rate=0.01)\r\n\r\n# Using the trained network for predictions.\r\npredictions = pygad.nn.predict(last_layer=output_layer, data_inputs=data_inputs)\r\n\r\n# Calculating some statistics\r\nnum_wrong = numpy.where(predictions != data_outputs)[0]\r\nnum_correct = data_outputs.size - num_wrong.size\r\naccuracy = 100 * (num_correct/data_outputs.size)\r\nprint(""Number of correct classifications : {num_correct}."".format(num_correct=num_correct))\r\nprint(""Number of wrong classifications : {num_wrong}."".format(num_wrong=num_wrong.size))\r\nprint(""Classification accuracy : {accuracy}."".format(accuracy=accuracy))\r\n'"
extract_features.py,0,"b'import numpy\r\nimport skimage.io, skimage.color, skimage.feature\r\nimport os\r\n\r\nfruits = [""apple"", ""raspberry"", ""mango"", ""lemon""]\r\n# Number of samples in the datset used = 492+490+490+490=1,962\r\n# 360 is the length of the feature vector.\r\ndataset_features = numpy.zeros(shape=(1962, 360))\r\noutputs = numpy.zeros(shape=(1962))\r\n\r\nidx = 0\r\nclass_label = 0\r\nfor fruit_dir in fruits:\r\n    curr_dir = os.path.join(os.path.sep, fruit_dir)\r\n    all_imgs = os.listdir(os.getcwd()+curr_dir)\r\n    for img_file in all_imgs:\r\n        if img_file.endswith("".jpg""): # Ensures reading only JPG files.\r\n            fruit_data = skimage.io.imread(fname=os.path.sep.join([os.getcwd(), curr_dir, img_file]), as_gray=False)\r\n            fruit_data_hsv = skimage.color.rgb2hsv(rgb=fruit_data)\r\n            hist = numpy.histogram(a=fruit_data_hsv[:, :, 0], bins=360)\r\n            dataset_features[idx, :] = hist[0]\r\n            outputs[idx] = class_label\r\n            idx = idx + 1\r\n    class_label = class_label + 1\r\n\r\n# Saving the extracted features and the outputs as NumPy files.\r\nnumpy.save(""dataset_features.npy"", dataset_features)\r\nnumpy.save(""outputs.npy"", outputs)\r\n'"
nn.py,0,"b'import numpy\r\nimport functools\r\n\r\n""""""\r\nThis project creates a neural network where the architecture has input and dense layers only. More layers will be added in the future. \r\nThe project only implements the forward pass of a neural network and no training algorithm is used.\r\nFor training a neural network using the genetic algorithm, check this project (https://github.com/ahmedfgad/NeuralGenetic) in which the genetic algorithm is used for training the network.\r\nFeel free to leave an issue in this project (https://github.com/ahmedfgad/NumPyANN) in case something is not working properly or to ask for questions. I am also available for e-mails at ahmed.f.gad@gmail.com\r\n""""""\r\n\r\ndef layers_weights(last_layer, initial=True):\r\n    """"""\r\n    Creates a list holding the weights of all layers in the neural network.\r\n\r\n    last_layer: A reference to the last (output) layer in the network architecture.\r\n    initial: When True, the function returns the initial weights of the layers. When False, the trained weights of the layers are returned. The initial weights are only needed before network training starts. The trained weights are needed to predict the network outputs.\r\n\r\n    Returns a list (network_weights) holding the weights of the layers.\r\n    """"""\r\n    network_weights = []\r\n\r\n    layer = last_layer\r\n    while ""previous_layer"" in layer.__init__.__code__.co_varnames:\r\n        # If the \'initial\' parameter is True, append the initial weights. Otherwise, append the trained weights.\r\n        if initial == True:\r\n            network_weights.append(layer.initial_weights)\r\n        elif initial == False:\r\n            network_weights.append(layer.trained_weights)\r\n        else:\r\n            raise ValueError(""Unexpected value to the \'initial\' parameter: {initial}."".format(initial=initial))\r\n\r\n        # Go to the previous layer.\r\n        layer = layer.previous_layer\r\n\r\n    # If the first layer in the network is not an input layer (i.e. an instance of the InputLayer class), raise an error.\r\n    if not (type(layer) is InputLayer):\r\n        raise TypeError(""The first layer in the network architecture must be an input layer."")\r\n\r\n    # Currently, the weights of the layers are in the reverse order. In other words, the weights of the first layer are at the last index of the \'network_weights\' list while the weights of the last layer are at the first index.\r\n    # Reversing the \'network_weights\' list to order the layers\' weights according to their location in the network architecture (i.e. the weights of the first layer appears at index 0 of the list).\r\n    network_weights.reverse()\r\n    return numpy.array(network_weights)\r\n\r\ndef layers_weights_as_vector(last_layer, initial=True):\r\n    """"""\r\n    Creates a list holding the weights of each layer in the network as a vector.\r\n\r\n    last_layer: A reference to the last (output) layer in the network architecture.\r\n    initial: When True, the function returns the initial weights of the layers. When False, the trained weights of the layers are returned. The initial weights are only needed before network training starts. The trained weights are needed to predict the network outputs.\r\n    \r\n    Returns a list (network_weights) holding the weights of the layers as a vector.\r\n    """"""\r\n    network_weights = []\r\n\r\n    layer = last_layer\r\n    while ""previous_layer"" in layer.__init__.__code__.co_varnames:\r\n        # If the \'initial\' parameter is True, append the initial weights. Otherwise, append the trained weights.\r\n        if initial == True:\r\n            vector = numpy.reshape(layer.initial_weights, newshape=(layer.initial_weights.size))\r\n#            vector = DenseLayer.to_vector(matrix=layer.initial_weights)\r\n            network_weights.extend(vector)\r\n        elif initial == False:\r\n            vector = numpy.reshape(layer.trained_weights, newshape=(layer.trained_weights.size))\r\n#            vector = DenseLayer.to_vector(array=layer.trained_weights)\r\n            network_weights.extend(vector)\r\n        else:\r\n            raise ValueError(""Unexpected value to the \'initial\' parameter: {initial}."".format(initial=initial))\r\n\r\n        # Go to the previous layer.\r\n        layer = layer.previous_layer\r\n\r\n    # If the first layer in the network is not an input layer (i.e. an instance of the InputLayer class), raise an error.\r\n    if not (type(layer) is InputLayer):\r\n        raise TypeError(""The first layer in the network architecture must be an input layer."")\r\n\r\n    # Currently, the weights of the layers are in the reverse order. In other words, the weights of the first layer are at the last index of the \'network_weights\' list while the weights of the last layer are at the first index.\r\n    # Reversing the \'network_weights\' list to order the layers\' weights according to their location in the network architecture (i.e. the weights of the first layer appears at index 0 of the list).\r\n    network_weights.reverse()\r\n    return numpy.array(network_weights)\r\n\r\ndef layers_weights_as_matrix(last_layer, vector_weights):\r\n    """"""\r\n    Converts the network weights from vectors to matrices.\r\n\r\n    last_layer: A reference to the last (output) layer in the network architecture.\r\n    vector_weights: The network weights as vectors where the weights of each layer form a single vector.\r\n\r\n    Returns a list (network_weights) holding the weights of the layers as matrices.\r\n    """"""\r\n    network_weights = []\r\n\r\n    start = 0\r\n    layer = last_layer\r\n    vector_weights = vector_weights[::-1]\r\n    while ""previous_layer"" in layer.__init__.__code__.co_varnames:\r\n        layer_weights_shape = layer.initial_weights.shape\r\n        layer_weights_size = layer.initial_weights.size\r\n\r\n        weights_vector=vector_weights[start:start + layer_weights_size]\r\n#        matrix = DenseLayer.to_array(vector=weights_vector, shape=layer_weights_shape)\r\n        matrix = numpy.reshape(weights_vector, newshape=(layer_weights_shape))\r\n        network_weights.append(matrix)\r\n\r\n        start = start + layer_weights_size\r\n\r\n        # Go to the previous layer.\r\n        layer = layer.previous_layer\r\n\r\n    # If the first layer in the network is not an input layer (i.e. an instance of the InputLayer class), raise an error.\r\n    if not (type(layer) is InputLayer):\r\n        raise TypeError(""The first layer in the network architecture must be an input layer."")\r\n\r\n    # Currently, the weights of the layers are in the reverse order. In other words, the weights of the first layer are at the last index of the \'network_weights\' list while the weights of the last layer are at the first index.\r\n    # Reversing the \'network_weights\' list to order the layers\' weights according to their location in the network architecture (i.e. the weights of the first layer appears at index 0 of the list).\r\n    network_weights.reverse()\r\n    return numpy.array(network_weights)\r\n\r\ndef layers_activations(last_layer):\r\n    """"""\r\n    Creates a list holding the activation functions of all layers in the network.\r\n    \r\n    last_layer: A reference to the last (output) layer in the network architecture.\r\n    \r\n    Returns a list (activations) holding the activation functions of the layers.\r\n    """"""\r\n    activations = []\r\n\r\n    layer = last_layer\r\n    while ""previous_layer"" in layer.__init__.__code__.co_varnames:\r\n        activations.append(layer.activation_function)\r\n\r\n        # Go to the previous layer.\r\n        layer = layer.previous_layer\r\n\r\n    if not (type(layer) is InputLayer):\r\n        raise TypeError(""The first layer in the network architecture must be an input layer."")\r\n\r\n    # Currently, the activations of layers are in the reverse order. In other words, the activation function of the first layer are at the last index of the \'activations\' list while the activation function of the last layer are at the first index.\r\n    # Reversing the \'activations\' list to order the layers\' weights according to their location in the network architecture (i.e. the activation function of the first layer appears at index 0 of the list).\r\n    activations.reverse()\r\n    return activations\r\n\r\ndef sigmoid(sop):\r\n\r\n    """"""\r\n    Applies the sigmoid function.\r\n\r\n    sop: The input to which the sigmoid function is applied.\r\n\r\n    Returns the result of the sigmoid function.\r\n    """"""\r\n\r\n    if type(sop) in [list, tuple]:\r\n        sop = numpy.array(sop)\r\n\r\n    return 1.0 / (1 + numpy.exp(-1 * sop))\r\n\r\ndef relu(sop):\r\n\r\n    """"""\r\n    Applies the rectified linear unit (ReLU) function.\r\n\r\n    sop: The input to which the relu function is applied.\r\n\r\n    Returns the result of the ReLU function.\r\n    """"""\r\n\r\n    if not (type(sop) in [list, tuple, numpy.ndarray]):\r\n        if sop < 0:\r\n            return 0\r\n        else:\r\n            return sop\r\n    elif type(sop) in [list, tuple]:\r\n        sop = numpy.array(sop)\r\n\r\n    result = sop\r\n    result[sop < 0] = 0\r\n\r\n    return result\r\n\r\ndef softmax(layer_outputs):\r\n\r\n    """"""\r\n    Applies the sotmax function.\r\n\r\n    sop: The input to which the softmax function is applied.\r\n\r\n    Returns the result of the softmax function.\r\n    """"""\r\n    return layer_outputs / (numpy.sum(layer_outputs) + 0.000001)\r\n\r\ndef train(num_epochs, \r\n          last_layer, \r\n          data_inputs, \r\n          data_outputs, \r\n          learning_rate):\r\n    """"""\r\n    Trains the neural network.\r\n    \r\n    num_epochs: Number of epochs.\r\n    last_layer: Reference to the last (output) layer in the network architecture.\r\n    data_inputs: Data features.\r\n    data_outputs: Data outputs.\r\n    learning_rate: Learning rate.\r\n    """"""\r\n    # To fetch the initial weights of the layer, the \'initial\' argument is set to True.\r\n    weights = layers_weights(last_layer, initial=True)\r\n    activations = layers_activations(last_layer)\r\n    \r\n    network_error = 0\r\n    for epoch in range(num_epochs):\r\n        print(""Epoch "", epoch)\r\n        for sample_idx in range(data_inputs.shape[0]):\r\n            r1 = data_inputs[sample_idx, :]\r\n            for idx in range(len(weights) - 1):\r\n                curr_weights = weights[idx]\r\n                r1 = numpy.matmul(r1, curr_weights)\r\n                if activations[idx] == ""relu"":\r\n                    r1 = relu(r1)\r\n                elif activations[idx] == ""sigmoid"":\r\n                    r1 = sigmoid(r1)\r\n                elif activations[idx] == ""softmax"":\r\n                    r1 = softmax(r1)\r\n            curr_weights = weights[-1]\r\n            r1 = numpy.matmul(r1, curr_weights)\r\n            predicted_label = numpy.where(r1 == numpy.max(r1))[0][0]\r\n            network_error = network_error + abs(predicted_label - data_outputs[sample_idx])\r\n\r\n        # Updating the network weights once after completing an epoch (i.e. passing through all the samples).\r\n        weights = update_weights(weights=weights,\r\n                                 network_error=network_error,\r\n                                 learning_rate=learning_rate)\r\n\r\n    # Initially, the \'trained_weights\' attribute of the layers are set to None. After the is trained, the \'trained_weights\' attribute is updated by the trained weights using the update_layers_trained_weights() function.\r\n    update_layers_trained_weights(last_layer, weights)\r\n\r\ndef update_weights(weights, network_error, learning_rate):\r\n    """"""\r\n    Updates the network weights using the learning rate only. \r\n    The purpose of this project is to only apply the forward pass of training a neural network. Thus, there is no optimization algorithm is used like the gradient descent.\r\n    For optimizing the neural network, check this project (https://github.com/ahmedfgad/NeuralGenetic) in which the genetic algorithm is used for training the network.\r\n    \r\n    weights: The current weights of the network.\r\n    network_error: The network error.\r\n    learning_rate: The learning rate.\r\n\r\n    It returns the new weights.\r\n    """"""\r\n    weights = numpy.array(weights)\r\n    new_weights = weights - network_error * learning_rate * weights\r\n    return new_weights\r\n\r\ndef update_layers_trained_weights(last_layer, final_weights):\r\n    """"""\r\n    After the network weights are trained, the \'trained_weights\' attribute of each layer is updated by the weights calculated after passing all the epochs (such weights are passed in the \'final_weights\' parameter).\r\n    By just passing a reference to the last layer in the network (i.e. output layer) in addition to the final weights, this function updates the \'trained_weights\' attribute of all layers.\r\n\r\n    last_layer: A reference to the last (output) layer in the network architecture.\r\n    final_weights: An array of weights of all layers in the network after passing through all the epochs.\r\n    """"""\r\n    layer = last_layer\r\n    layer_idx = len(final_weights) - 1\r\n    while ""previous_layer"" in layer.__init__.__code__.co_varnames:\r\n        layer.trained_weights = final_weights[layer_idx]\r\n\r\n        layer_idx = layer_idx - 1\r\n        # Go to the previous layer.\r\n        layer = layer.previous_layer\r\n\r\ndef predict(last_layer, data_inputs):\r\n    """"""\r\n    Uses the trained weights for predicting the samples\' outputs.\r\n\r\n    last_layer: A reference to the last (output) layer in the network architecture.\r\n    data_inputs: Data features.\r\n\r\n    Returns the predictions of all samples.\r\n    """"""\r\n    # To fetch the trained weights of the layer, the \'initial\' argument is set to False.\r\n    weights = layers_weights(last_layer, initial=False)\r\n    activations = layers_activations(last_layer)\r\n\r\n    if len(weights) != len(activations):\r\n        raise TypeError(""The length of layers {num_layers} is not equal to the number of activations functions {num_activations} and they must be equal."".format(num_layers=len(weights), num_activations=len(activations)))\r\n\r\n    predictions = numpy.zeros(shape=(data_inputs.shape[0]))\r\n    for sample_idx in range(data_inputs.shape[0]):\r\n        r1 = data_inputs[sample_idx, :]\r\n        for curr_weights, activation in zip(weights, activations):\r\n            r1 = numpy.matmul(r1, curr_weights)\r\n            if activation == ""relu"":\r\n                r1 = relu(r1)\r\n            elif activation == ""sigmoid"":\r\n                r1 = sigmoid(r1)\r\n            elif activation == ""softmax"":\r\n                r1 = softmax(r1)\r\n        predicted_label = numpy.where(r1 == numpy.max(r1))[0][0]\r\n        predictions[sample_idx] = predicted_label\r\n    return predictions\r\n\r\ndef to_vector(array):\r\n    """"""\r\n    Converts a passed NumPy array (of any dimensionality) to its `array`  parameter into a 1D vector and returns the vector.\r\n    \r\n    array: The NumPy array to be converted into a 1D vector.\r\n\r\n    Returns the array after being reshaped into a NumPy 1D vector.\r\n\r\n    Example: weights_vector = nn.DenseLayer.to_vector(array=array)\r\n    """"""\r\n    if not (type(array) is numpy.ndarray):\r\n        raise TypeError(""An input of type numpy.ndarray is expected but an input of type {in_type} found."".format(in_type=type(array)))\r\n    return numpy.reshape(array, newshape=(array.size))\r\n\r\ndef to_array(vector, shape):\r\n    """"""\r\n    Converts a passed vector to its `vector`  parameter into a NumPy array and returns the array.\r\n\r\n    vector: The 1D vector to be converted into an array.\r\n    shape: The target shape of the array.\r\n\r\n    Returns the NumPy 1D vector after being reshaped into an array.\r\n\r\n    Example: weights_matrix = nn.DenseLayer.to_array(vector=vector, shape=shape)\r\n    """"""\r\n    if not (type(vector) is numpy.ndarray):\r\n        raise TypeError(""An input of type numpy.ndarray is expected but an input of type {in_type} found."".format(in_type=type(vector)))\r\n    if vector.ndim > 1:\r\n        raise ValueError(""A 1D NumPy array is expected but an array of {ndim} dimensions found."".format(ndim=vector.ndim))\r\n    if vector.size != functools.reduce(lambda x,y:x*y, shape, 1): # (operator.mul == lambda x,y:x*y\r\n        raise ValueError(""Mismatch between the vector length and the array shape. A vector of length {vector_length} cannot be converted into a array of shape ({array_shape})."".format(vector_length=vector.size, array_shape=shape))\r\n    return numpy.reshape(vector, newshape=shape)\r\n\r\nclass InputLayer:\r\n    """"""\r\n    Implementing the input layer of a neural network.\r\n    """"""\r\n    def __init__(self, num_inputs):\r\n        if num_inputs <= 0:\r\n            raise ValueError(""Number of input neurons cannot be <= 0. Please pass a valid value to the \'num_inputs\' parameter."")\r\n        # The number of neurons in the input layer.\r\n        self.num_neurons = num_inputs\r\n\r\nclass DenseLayer:\r\n    """"""\r\n    Implementing the input dense (fully connected) layer of a neural network.\r\n    """"""\r\n    def __init__(self, num_neurons, previous_layer, activation_function=""sigmoid""):\r\n        if num_neurons <= 0:\r\n            raise ValueError(""Number of neurons cannot be <= 0. Please pass a valid value to the \'num_neurons\' parameter."")\r\n        # Number of neurons in the dense layer.\r\n        self.num_neurons = num_neurons\r\n\r\n        supported_activation_functions = (""sigmoid"", ""relu"", ""softmax"")\r\n        if not (activation_function in supported_activation_functions):\r\n            raise ValueError(""The specified activation function \'{activation_function}\' is not among the supported activation functions {supported_activation_functions}. Please use one of the supported functions."".format(activation_function=activation_function, supported_activation_functions=supported_activation_functions))\r\n        self.activation_function = activation_function\r\n\r\n        if previous_layer is None:\r\n            raise TypeError(""The previous layer cannot be of Type \'None\'. Please pass a valid layer to the \'previous_layer\' parameter."")\r\n        # A reference to the layer that preceeds the current layer in the network architecture.\r\n        self.previous_layer = previous_layer\r\n\r\n        # Initializing the weights of the layer.\r\n        self.initial_weights = numpy.random.uniform(low=-0.1,\r\n                                                    high=0.1,\r\n                                                    size=(previous_layer.num_neurons, num_neurons))\r\n\r\n        # The trained weights of the layer. Only assigned a value after the network is trained (i.e. the train() function completes).\r\n        # Just initialized to be equal to the initial weights\r\n        self.trained_weights = self.initial_weights.copy()'"
TutorialProject/ann_numpy.py,0,"b'import numpy\r\nimport pickle\r\n\r\ndef sigmoid(inpt):\r\n    return 1.0 / (1 + numpy.exp(-1 * inpt))\r\n\r\ndef relu(inpt):\r\n    result = inpt\r\n    result[inpt < 0] = 0\r\n    return result\r\n\r\ndef update_weights(weights, learning_rate):\r\n    new_weights = weights - learning_rate * weights\r\n    return new_weights\r\n\r\ndef train_network(num_iterations, weights, data_inputs, data_outputs, learning_rate, activation=""relu""):\r\n    for iteration in range(num_iterations):\r\n        print(""Itreation "", iteration)\r\n        for sample_idx in range(data_inputs.shape[0]):\r\n            r1 = data_inputs[sample_idx, :]\r\n            for idx in range(len(weights) - 1):\r\n                curr_weights = weights[idx]\r\n                r1 = numpy.matmul(r1, curr_weights)\r\n                if activation == ""relu"":\r\n                    r1 = relu(r1)\r\n                elif activation == ""sigmoid"":\r\n                    r1 = sigmoid(r1)\r\n            curr_weights = weights[-1]\r\n            r1 = numpy.matmul(r1, curr_weights)\r\n            predicted_label = numpy.where(r1 == numpy.max(r1))[0][0]\r\n            desired_label = data_outputs[sample_idx]\r\n            if predicted_label != desired_label:\r\n                weights = update_weights(weights,\r\n                                         learning_rate=0.001)\r\n    return weights\r\n\r\ndef predict_outputs(weights, data_inputs, activation=""relu""):\r\n    predictions = numpy.zeros(shape=(data_inputs.shape[0]))\r\n    for sample_idx in range(data_inputs.shape[0]):\r\n        r1 = data_inputs[sample_idx, :]\r\n        for curr_weights in weights:\r\n            r1 = numpy.matmul(r1, curr_weights)\r\n            if activation == ""relu"":\r\n                r1 = relu(r1)\r\n            elif activation == ""sigmoid"":\r\n                r1 = sigmoid(r1)\r\n        predicted_label = numpy.where(r1 == numpy.max(r1))[0][0]\r\n        predictions[sample_idx] = predicted_label\r\n    return predictions\r\n\r\nf = open(""dataset_features.pkl"", ""rb"")\r\ndata_inputs2 = pickle.load(f)\r\nf.close()\r\n\r\nfeatures_STDs = numpy.std(a=data_inputs2, axis=0)\r\ndata_inputs = data_inputs2[:, features_STDs > 50]\r\n\r\nf = open(""outputs.pkl"", ""rb"")\r\ndata_outputs = pickle.load(f)\r\nf.close()\r\n\r\nHL1_neurons = 150\r\ninput_HL1_weights = numpy.random.uniform(low=-0.1, high=0.1,\r\n                                         size=(data_inputs.shape[1], HL1_neurons))\r\nHL2_neurons = 60\r\nHL1_HL2_weights = numpy.random.uniform(low=-0.1, high=0.1,\r\n                                       size=(HL1_neurons, HL2_neurons))\r\noutput_neurons = 4\r\nHL2_output_weights = numpy.random.uniform(low=-0.1, high=0.1,\r\n                                          size=(HL2_neurons, output_neurons))\r\n\r\nweights = numpy.array([input_HL1_weights,\r\n                       HL1_HL2_weights,\r\n                       HL2_output_weights])\r\n\r\nweights = train_network(num_iterations=10,\r\n                        weights=weights,\r\n                        data_inputs=data_inputs,\r\n                        data_outputs=data_outputs,\r\n                        learning_rate=0.01,\r\n                        activation=""relu"")\r\n\r\npredictions = predict_outputs(weights, data_inputs)\r\nnum_flase = numpy.where(predictions != data_outputs)[0]\r\nprint(""num_flase "", num_flase.size)\r\n'"
TutorialProject/extract_features.py,0,"b'import numpy\r\nimport skimage.io, skimage.color, skimage.feature\r\nimport os\r\nimport pickle\r\n\r\nfruits = [""apple"", ""raspberry"", ""mango"", ""lemon""]\r\n#492+490+490+490=1,962\r\ndataset_features = numpy.zeros(shape=(1962, 360))\r\noutputs = numpy.zeros(shape=(1962))\r\n\r\nidx = 0\r\nclass_label = 0\r\nfor fruit_dir in fruits:\r\n    curr_dir = os.path.join(os.path.sep, fruit_dir)\r\n    all_imgs = os.listdir(os.getcwd()+curr_dir)\r\n    for img_file in all_imgs:\r\n        if img_file.endswith("".jpg""): # Ensures reading only JPG files.\r\n            fruit_data = skimage.io.imread(fname=os.path.sep.join([os.getcwd(), curr_dir, img_file], as_grey=False)\r\n            fruit_data_hsv = skimage.color.rgb2hsv(rgb=fruit_data)\r\n            hist = numpy.histogram(a=fruit_data_hsv[:, :, 0], bins=360)\r\n            dataset_features[idx, :] = hist[0]\r\n            outputs[idx] = class_label\r\n            idx = idx + 1\r\n    class_label = class_label + 1\r\n\r\nwith open(""dataset_features.pkl"", ""wb"") as f:\r\n    pickle.dump(dataset_features, f)\r\n\r\nwith open(""outputs.pkl"", ""wb"") as f:\r\n    pickle.dump(outputs, f)\r\n'"
