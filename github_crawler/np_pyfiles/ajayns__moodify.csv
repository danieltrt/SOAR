file_path,api_count,code
app.py,0,"b'# Third-party imports\nfrom flask import Flask, jsonify, request, render_template, redirect\nfrom flask_pymongo import PyMongo\nfrom werkzeug import secure_filename\nimport base64\nfrom mood import main_func\n\n\n# Initialize app and database\napp = Flask(__name__)\n\napp.config[\'MONGO_DBNAME\'] = \'moodmusic\'\napp.config[\'MONGO_URI\'] = \'mongodb://localhost:27017/moodmusic\'\n\nmongo = PyMongo(app)\n\n# Setup routes\n@app.route(\'/\')\ndef index():\n  return redirect(""/cam"")\n\n@app.route(\'/player\')\ndef player():\n  return render_template(""player.html"")\n\n@app.route(\'/cam\')\ndef cam():\n  return render_template(""cam.html"")\n\n@app.route(\'/music\', methods=[\'GET\'])\ndef music():\n  songs = mongo.db.songs\n  output = []\n  for s in songs.find({\'mood\' : request.args.get(""mood"")}):\n    output.append({\'id\' : s[\'id\'],\'title\' : s[\'title\'], \'album\': s[\'album\'], \'artist\' : s[\'artist\'], \'albumart\' : s[\'albumart\'], \'url\' : s[\'url\']})\n  return jsonify(output)\n\n@app.route(\'/emotion\', methods=[\'GET\', \'POST\'])\ndef emotion():\n  # Get img data\n  img_data = request.form[\'img\']\n  img_data = img_data[23:]\n\n  # Decode the img and save as snap.jpg\n  f = open(""snap.jpg"", ""wb"")\n  f.write(base64.b64decode(img_data.encode(\'ascii\')))\n  f.close()\n\n  # Run script for emotion recognition\n  mood = main_func()\n\n  return redirect(""/player?mood="" + mood)\n\n# Main\nif __name__ == \'__main__\':\n  app.run(debug=True)\n'"
mood.py,6,"b'\n# EMOTION RECOGNITION PYTHON SCRIPT.\n\nimport cv2\nimport numpy as np\nimport dlib\nfrom sklearn.svm import SVC\nimport glob\nimport random\nimport math\nimport itertools\nfrom sklearn.externals import joblib\n\n\n\n\n\n\n\n\n\ndef crop_face(i_path):\n\n    image=cv2.imread(i_path)\n    gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n    dest_i = \'test.png\'\n    # Loading all the HAAR Cascade classifiers\n    face1 = cv2.CascadeClassifier(""data/HAARCascades/haarcascade_frontalface_default.xml"")\n    face2 = cv2.CascadeClassifier(""data/HAARCascades/haarcascade_frontalface_alt2.xml"")\n    face3 = cv2.CascadeClassifier(""data/HAARCascades/haarcascade_frontalface_alt.xml"")\n    face4 = cv2.CascadeClassifier(""data/HAARCascades/haarcascade_frontalface_alt_tree.xml"")\n\n    # Detecting faces\n    face_1 = face1.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5),flags=cv2.CASCADE_SCALE_IMAGE)\n    face_2 = face2.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5),flags=cv2.CASCADE_SCALE_IMAGE)\n    face_3 = face3.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5),flags=cv2.CASCADE_SCALE_IMAGE)\n    face_4 = face4.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n\n    # ensuring that no other object in the image has been wrongly classified as a face and at least one face is\n    # detected.\n    if len(face_1)==1:\n        req_face=face_1\n    elif len(face_2) == 1:\n        req_face = face_2\n    elif len(face_3) == 1:\n        req_face = face_3\n    elif len(face_4) == 1:\n        req_face = face_4\n    else:\n        req_face=""""\n    if len(req_face)==1:\n        #print(""\\n Face Cropped Using HAAR Cascade\\n"")\n        for (x, y, w, h) in req_face:\n            roi_gray = gray[y:y + h, x:x + w]\n        # Writing the final cropped image to the required directory\n\n        cv2.imwrite(dest_i, cv2.resize(roi_gray, (350, 350)))\n    else:\n        #print(""\\n Face Not Cropped Using HAAR Cascade\\n"")\n\n        cv2.imwrite(dest_i,gray)\n    return dest_i\n\n\ndef get_landmarks(image_p,face_det,land_pred):\n    face_detections=face_det(image_p,1)\n    for k,d in enumerate(face_detections):\n        shape=land_pred(image_p,d)\n        x_cords=[]\n        y_cords=[]\n        for i in range(1,68):\n            x_cords.append(float(shape.part(i).x))\n            y_cords.append(float(shape.part(i).y))\n\n        xmean=np.mean(x_cords)\n        ymean=np.mean(y_cords)\n        x_central=[(x-xmean) for x in x_cords] # To compensate for variation in location of face in the frame.\n        y_central=[(y-ymean) for y in y_cords]\n\n        if x_cords[26]==x_cords[29]: # 26 i   s the top of the top of the  nose-bridge, 29 is the tip of the nose\n            anglenose=0\n        else:\n            anglenose_rad=int(math.atan((y_central[26]-y_central[29])/(x_central[26]-x_central[29])))\n            # Tan Inverse of slope\n            anglenose=int(math.degrees(anglenose_rad))\n            # print(y_central[26]-y_central[29])\n            # print(y_cords[26]-y_cords[29])\n\n        if anglenose<0:\n            anglenose+=90 # Because anglenose computed above is the angle wrt to vertical\n\n        else:\n            anglenose-=90      # Because anglenose computed above is the angle wrt to vertical\n\n        landmarks_v=[]\n        for x,y,w,z in zip(x_central,y_central,x_cords,y_cords):\n            landmarks_v.append(x) # Co-ordinates are added relative to the Centre of gravity of face to accompany for\n            landmarks_v.append(y) # variation of location of face in the image.\n\n            # Euclidean distance between each point and the centre point (length of vector)\n            np_mean_co=np.asarray((ymean,xmean))\n            np_coor=np.asarray((z,w))\n            euclid_d=np.linalg.norm(np_coor-np_mean_co)\n            landmarks_v.append(euclid_d)\n\n            # Angle of the vector(used to compensate for the offset caused due to tilt of face wrt horizontal)\n            angle_rad=(math.atan((z-ymean)/(w-xmean)))\n            angle_degree=math.degrees(angle_rad)\n            angle_req=int(angle_degree-anglenose)\n            landmarks_v.append(angle_req)\n\n    if len(face_detections)<1:\n        landmarks_v=""error""\n\n    return  landmarks_v\n\n# Loading the Support Vector Machine model from storage after training\n# with linear kernel in EmotionClassifer2.py\n# SVM_emo_model_4.pkl has been trained with the cropped face dataset(using HAAR cascades)\n# SVM_emo_model_5.pkl has been trained with Original Organized Dataset\n# SVM_emo_model_4.pkl is giving better results on new data.\n# SVM_emo_model_7.pkl is giving the best accuracy.\n\ndef main_func():\n    img_path=\'snap.jpg\' # THE PATH OF THE IMAGE TO BE ANALYZED\n\n    font=cv2.FONT_HERSHEY_DUPLEX\n    emotions = [""anger"", ""happy"", ""sadness""] #Emotion list\n    clahe=cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8)) # Histogram equalization object\n    face_det=dlib.get_frontal_face_detector()\n    land_pred=dlib.shape_predictor(""data/DlibPredictor/shape_predictor_68_face_landmarks.dat"")\n\n\n\n    SUPPORT_VECTOR_MACHINE_clf2 = joblib.load(\'data/Trained_ML_Models/SVM_emo_model_7.pkl\')\n    # Loading the SVM model trained earlier in the path mentioned above.\n\n\n\n    pred_data=[]\n    pred_labels=[]\n\n    a=crop_face(img_path)\n    img=cv2.imread(a)\n    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    clahe_gray=clahe.apply(gray)\n    landmarks_vec = get_landmarks(clahe_gray,face_det,land_pred)\n\n    #print(len(landmarks_vec))\n    #print(landmarks_vec)\n\n    if landmarks_vec == ""error"":\n        pass\n    else:\n        pred_data.append(landmarks_vec)\n    np_test_data = np.array(pred_data)\n    a=SUPPORT_VECTOR_MACHINE_clf2.predict(pred_data)\n    #cv2.putText(img,\'DETECTED FACIAL EXPRESSION : \',(8,30),font,0.7,(0,0,255),2,cv2.LINE_AA)\n    #l=len(\'Facial Expression Detected : \')\n    #cv2.putText(img,emotions[a[0]].upper(),(150,60),font,1,(255,0,0),2,cv2.LINE_AA)\n    #cv2.imshow(\'test_image\',img)\n    #print(emotions[a[0]])\n\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n    return emotions[a[0]]\n\n\n'"
components/PyEmotionRecognition/DataPreprocess.py,0,"b'import glob\nimport cv2\n\nemotions={1:\'happy\',2:\'sadness\',3:\'anger\'}\n# ONLY THREE EMOTIONS HAVE BEEN USED. SO COPY FOLDERS NAMED \'HAPPY\', \'SADNESS\' AND \'ANGRY\' AFTER USING OrganizeData.py\n# to the folder OrganizedData2, MANUALLY.\n\nm_dir=\'data/FaceData/OrganizedData2/\'\ndest_m_dir=\'data/FaceData/Crop_Organized_Data4/\'\n\ndef crop_face(i_path,e_path):\n\n    image=cv2.imread(i_path)\n    gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n\n    # Loading all the HAAR Cascade classifiers\n    face1 = cv2.CascadeClassifier(""data/HAARCascades/haarcascade_frontalface_default.xml"")\n    face2 = cv2.CascadeClassifier(""data/HAARCascades/haarcascade_frontalface_alt2.xml"")\n    face3 = cv2.CascadeClassifier(""data/HAARCascades/haarcascade_frontalface_alt.xml"")\n    face4 = cv2.CascadeClassifier(""data/HAARCascades/haarcascade_frontalface_alt_tree.xml"")\n\n    # Detecting faces\n    face_1 = face1.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5),flags=cv2.CASCADE_SCALE_IMAGE)\n    face_2 = face2.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5),flags=cv2.CASCADE_SCALE_IMAGE)\n    face_3 = face3.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5),flags=cv2.CASCADE_SCALE_IMAGE)\n    face_4 = face4.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n\n    # ensuring that no other object in the image has been wrongly classified as a face and at least one face is\n    # detected.\n    if len(face_1)==1:\n        req_face=face_1\n    elif len(face_2) == 1:\n        req_face = face_2\n    elif len(face_3) == 1:\n        req_face = face_3\n    elif len(face_4) == 1:\n        req_face = face_4\n    else:\n        req_face=""""\n\n    for (x, y, w, h) in req_face:\n        roi_gray = gray[y:y + h, x:x + w]\n    # Writing the final cropped image to the required directory\n    temp_1=i_path\n    temp_split=temp_1.split(\'/\')\n    final_name=temp_split[len(temp_split)-1]\n    cv2.imwrite(dest_m_dir+e_path[48:]+\'/\'+final_name, cv2.resize(roi_gray, (350, 350)))\n\n\nfor emo_path in glob.glob(m_dir + \'*\'):\n    for image_path in glob.glob(emo_path+\'/*\'):\n        print (image_path)\n        crop_face(image_path,emo_path)\n\n\n\n\n'"
components/PyEmotionRecognition/EmotionClassifier2.py,9,"b'import cv2\nimport numpy as np\nimport dlib\nfrom sklearn.svm import SVC\nimport glob\nimport random\nimport math\nfrom sklearn.externals import joblib\n\nemotions = [""anger"", ""happy"", ""sadness""] #Emotion list\nclahe=cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8)) # Histogram equalization object\nface_det=dlib.get_frontal_face_detector()\nland_pred=dlib.shape_predictor(""data/DlibPredictor/shape_predictor_68_face_landmarks.dat"")\n\nsvm_clf2=SVC(kernel=\'linear\',probability=True,tol=1e-3) # SVM classifier object\n\ndef get_images(emotion):\n    m_path=\'INCLUDE THE MAIN PATH TO YOUR IMAGES AFTER RUNNING ORGANIZE DATA\'\n    i_path=glob.glob(m_path+emotion+\'/*\')\n    # print(i_path)\n    random.shuffle(i_path) # Shuffle the list of image paths\n    train_paths=i_path[:int(len(i_path)*0.90)] # For validation, 90 percent of training data is used\n    predict_paths=i_path[-int(len(i_path)*0.10):] # and 10 percent as test data.\n    return train_paths,predict_paths\n\ndef get_landmarks(image_p):\n    face_detections=face_det(image_p,1)\n    for k,d in enumerate(face_detections):\n        shape=land_pred(image_p,d)\n        x_cords=[]\n        y_cords=[]\n        for i in range(1,68):\n            x_cords.append(float(shape.part(i).x))\n            y_cords.append(float(shape.part(i).y))\n\n        xmean=np.mean(x_cords)\n        ymean=np.mean(y_cords)\n        x_central=[(x-xmean) for x in x_cords] # To compensate for variation in location of face in the frame.\n        y_central=[(y-ymean) for y in y_cords]\n\n        if x_cords[26]==x_cords[29]: # 26 is the top of the bridge, 29 is the tip of the nose\n            anglenose=0\n        else:\n            anglenose_rad=int(math.atan((y_central[26] - y_central[29]) / (x_central[26] - x_central[29])))\n            # Tan Inverse of slope\n            anglenose=int(math.degrees(anglenose_rad))\n            # print(y_central[26]-y_central[29])\n            # print(y_cords[26]-y_cords[29])\n\n        if anglenose<0:\n            anglenose+=90 # Because anglenose computed above is the angle wrt to vertical\n\n        else:\n            anglenose-=90      # Because anglenose computed above is the angle wrt to vertical\n\n        landmarks_v=[]\n        for x,y,w,z in zip(x_central,y_central,x_cords,y_cords):\n            landmarks_v.append(x) # Co-ordinates are added relative to the Centre of gravity of face to accompany for\n            landmarks_v.append(y) # variation of location of face in the image.\n\n            # Euclidean distance between each point and the centre point (length of vector)\n            np_mean_coor=np.asarray((ymean,xmean))\n            np_coor=np.asarray((z,w))\n            euclid_d=np.linalg.norm(np_coor-np_mean_coor)\n            landmarks_v.append(euclid_d)\n\n            # Angle of the vector, which is used to correct for the offset caused due to tilt of image wrt horizontal\n            angle_rad = (math.atan((z - ymean) / (w - xmean)))\n            angle_degree = math.degrees(angle_rad)\n            angle_req = int(angle_degree - anglenose)\n            landmarks_v.append(angle_req)\n\n    if len(face_detections)<1:\n        landmarks_v=""error""\n\n    return  landmarks_v\n\ndef org_data():\n    train_data=[]\n    train_labels=[]\n    pred_data=[]\n    pred_labels=[]\n\n    for emo in emotions:\n        train_p,pred_p=get_images(emo)\n        for im in train_p:\n            img=cv2.imread(im)\n            img_gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n            clahe_gray=clahe.apply(img_gray)\n            landmarks_vec=get_landmarks(clahe_gray)\n            if landmarks_vec == ""error"":\n                pass\n            else:\n                train_data.append(landmarks_vec)\n                train_labels.append(emotions.index(emo))\n\n        for im in pred_p:\n            img=cv2.imread(im)\n            img_gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n            clahe_gray=clahe.apply(img_gray)\n            landmarks_vec=get_landmarks(clahe_gray)\n            if landmarks_vec == ""error"":\n                pass\n            else:\n                pred_data.append(landmarks_vec)\n                pred_labels.append(emotions.index(emo))\n\n    return train_data,train_labels,pred_data,pred_labels\n\naccuracy=[]\n\nfor i in range(0,10):\n    print(""\\nSetting Random sample sets %d "" %i)\n    train_data, train_labels, pred_data, pred_labels=org_data()\n\n    np_train_data=np.array(train_data)\n    np_train_labels=np.array(train_labels)\n    print(np_train_data)\n    print(""\\nTraining SVM model with Linear Kernel : %d "" %i)\n    svm_clf2.fit(np_train_data,np_train_labels)\n\n    print(""\\nComputing Accuracy of : %d "" %i)\n    np_test_data=np.array(pred_data)\n    final_pred=svm_clf2.score(np_test_data,pred_labels)\n    print(""\\nAccuracy : %f"" %final_pred)\n    accuracy.append(final_pred)\njoblib.dump(svm_clf2,\'data/Trained_ML_Models/SVM_new_model.pkl\') # MODIFY THE NAME OF THE MODEL TO BE SAVED.\nprint(""\\n Mean Accuracy %f "" %np.mean(accuracy))\n\n\n\n\n\n\n'"
components/PyEmotionRecognition/Model_test2.py,6,"b'\n# EMOTION RECOGNITION PYTHON SCRIPT.\n\nimport cv2\nimport numpy as np\nimport dlib\nfrom sklearn.svm import SVC\nimport glob\nimport random\nimport math\nimport itertools\nfrom sklearn.externals import joblib\n\n\n\nimg_path=\'data/test_images/test1.jpg\' # THE PATH OF THE IMAGE TO BE ANALYZED\n\nfont=cv2.FONT_HERSHEY_DUPLEX\nemotions = [""anger"", ""happy"", ""sadness""] #Emotion list\nclahe=cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8)) # Histogram equalization object\nface_det=dlib.get_frontal_face_detector()\nland_pred=dlib.shape_predictor(""data/DlibPredictor/shape_predictor_68_face_landmarks.dat"")\n\n\n\n\n\n\n\ndef crop_face(i_path):\n\n    image=cv2.imread(i_path)\n    gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n    dest_i = \'data/cap_image/test.png\'\n    # Loading all the HAAR Cascade classifiers\n    face1 = cv2.CascadeClassifier(""data/HAARCascades/haarcascade_frontalface_default.xml"")\n    face2 = cv2.CascadeClassifier(""data/HAARCascades/haarcascade_frontalface_alt2.xml"")\n    face3 = cv2.CascadeClassifier(""data/HAARCascades/haarcascade_frontalface_alt.xml"")\n    face4 = cv2.CascadeClassifier(""data/HAARCascades/haarcascade_frontalface_alt_tree.xml"")\n\n    # Detecting faces\n    face_1 = face1.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5),flags=cv2.CASCADE_SCALE_IMAGE)\n    face_2 = face2.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5),flags=cv2.CASCADE_SCALE_IMAGE)\n    face_3 = face3.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5),flags=cv2.CASCADE_SCALE_IMAGE)\n    face_4 = face4.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n\n    # ensuring that no other object in the image has been wrongly classified as a face and at least one face is\n    # detected.\n    if len(face_1)==1:\n        req_face=face_1\n    elif len(face_2) == 1:\n        req_face = face_2\n    elif len(face_3) == 1:\n        req_face = face_3\n    elif len(face_4) == 1:\n        req_face = face_4\n    else:\n        req_face=""""\n    if len(req_face)==1:\n        print(""\\n Face Cropped Using HAAR Cascade\\n"")\n        for (x, y, w, h) in req_face:\n            roi_gray = gray[y:y + h, x:x + w]\n        # Writing the final cropped image to the required directory\n\n        cv2.imwrite(dest_i, cv2.resize(roi_gray, (350, 350)))\n    else:\n        print(""\\n Face Not Cropped Using HAAR Cascade\\n"")\n\n        cv2.imwrite(dest_i,gray)\n    return dest_i\n\n\ndef get_landmarks(image_p):\n    face_detections=face_det(image_p,1)\n    for k,d in enumerate(face_detections):\n        shape=land_pred(image_p,d)\n        x_cords=[]\n        y_cords=[]\n        for i in range(1,68):\n            x_cords.append(float(shape.part(i).x))\n            y_cords.append(float(shape.part(i).y))\n\n        xmean=np.mean(x_cords)\n        ymean=np.mean(y_cords)\n        x_central=[(x-xmean) for x in x_cords] # To compensate for variation in location of face in the frame.\n        y_central=[(y-ymean) for y in y_cords]\n\n        if x_cords[26]==x_cords[29]: # 26 is the top of the top of the  nose-bridge, 29 is the tip of the nose\n            anglenose=0\n        else:\n            anglenose_rad=int(math.atan((y_central[26]-y_central[29])/(x_central[26]-x_central[29])))\n            # Tan Inverse of slope\n            anglenose=int(math.degrees(anglenose_rad))\n            # print(y_central[26]-y_central[29])\n            # print(y_cords[26]-y_cords[29])\n\n        if anglenose<0:\n            anglenose+=90 # Because anglenose computed above is the angle wrt to vertical\n\n        else:\n            anglenose-=90      # Because anglenose computed above is the angle wrt to vertical\n\n        landmarks_v=[]\n        for x,y,w,z in zip(x_central,y_central,x_cords,y_cords):\n            landmarks_v.append(x) # Co-ordinates are added relative to the Centre of gravity of face to accompany for\n            landmarks_v.append(y) # variation of location of face in the image.\n\n            # Euclidean distance between each point and the centre point (length of vector)\n            np_mean_co=np.asarray((ymean,xmean))\n            np_coor=np.asarray((z,w))\n            euclid_d=np.linalg.norm(np_coor-np_mean_co)\n            landmarks_v.append(euclid_d)\n\n            # Angle of the vector(used to compensate for the offset caused due to tilt of face wrt horizontal)\n            angle_rad=(math.atan((z-ymean)/(w-xmean)))\n            angle_degree=math.degrees(angle_rad)\n            angle_req=int(angle_degree-anglenose)\n            landmarks_v.append(angle_req)\n\n    if len(face_detections)<1:\n        landmarks_v=""error""\n\n    return  landmarks_v\n\n# Loading the Support Vector Machine model from storage after training\n# with linear kernel in EmotionClassifer2.py\n# SVM_emo_model_4.pkl has been trained with the cropped face dataset(using HAAR cascades)\n# SVM_emo_model_5.pkl has been trained with Original Organized Dataset\n# SVM_emo_model_4.pkl is giving better results on new data.\n# SVM_emo_model_7.pkl is giving the best accuracy.\n\n\nSUPPORT_VECTOR_MACHINE_clf2 = joblib.load(\'data/Trained_ML_Models/SVM_emo_model_7.pkl\')\n# Loading the SVM model trained earlier in the path mentioned above.\n\n\n\npred_data=[]\npred_labels=[]\n\na=crop_face(img_path)\nimg=cv2.imread(a)\ngray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\nclahe_gray=clahe.apply(gray)\nlandmarks_vec = get_landmarks(clahe_gray)\n\nprint(len(landmarks_vec))\nprint(landmarks_vec)\n\nif landmarks_vec == ""error"":\n    pass\nelse:\n    pred_data.append(landmarks_vec)\nnp_test_data = np.array(pred_data)\na=SUPPORT_VECTOR_MACHINE_clf2.predict(pred_data)\ncv2.putText(img,\'DETECTED FACIAL EXPRESSION : \',(8,30),font,0.7,(0,0,255),2,cv2.LINE_AA)\nl=len(\'Facial Expression Detected : \')\ncv2.putText(img,emotions[a[0]].upper(),(150,60),font,1,(255,0,0),2,cv2.LINE_AA)\ncv2.imshow(\'test_image\',img)\nprint(emotions[a[0]])\n\n\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n'"
components/PyEmotionRecognition/OrganizeData.py,0,"b'import glob\nfrom shutil import copyfile\n\nemotions = [""neutral"",""anger"",""contempt"",""disgust"",""fear"",""happy"",""sadness"",""surprise""]\ncandidates_paths=glob.glob(""<INCLUDE THE PATH OF THE CK+ DATASET>"")\n\nfor x in candidates_paths:\n    serial=x[-4:]\n    for sessions in glob.glob(""data/FaceData/Emotion/%s/*"" %serial):\n        for files in glob.glob(""%s/*"" %sessions):\n            file=open(files,\'r\')\n            current_session=files[46:-30]\n            current_emotion=int(float(file.readline()))\n\n            a=glob.glob(""data/FaceData/Images/%s/%s/*"" %(serial, current_session))\n            b=sorted(a)\n            image_emotion_s=b[-1]\n            image_neutral_s=b[0]\n\n            neutral_d=""data/FaceData/OrganizedData/neutral/%s"" %image_neutral_s[50:]\n            emo_d = ""data/FaceData/OrganizedData/%s/%s"" %(emotions[current_emotion],image_neutral_s[50:])\n\n            copyfile(image_neutral_s,neutral_d)\n            copyfile(image_emotion_s,emo_d)\n\n\n\n'"
components/PyMusicMood/MusicTrain.py,1,"b'import pandas as pd\nimport glob\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.externals import joblib\nfrom sklearn.utils import shuffle\n\n\nsvm_clf=SVC(kernel=\'linear\',probability=True,tol=1e-3)\n\n\ndef pre_process():\n    final_df = pd.read_csv(\'csv_files/Final_train_DF.csv\')\n    preprocess_df = final_df[final_df.key != 9999]\n    preprocess_df.to_csv(\'csv_files/data_preprocess.csv\')\n    return\n\ndef get_train_data(rand_st):\n    preprocess_df=pd.read_csv(\'csv_files/data_preprocess.csv\')\n    column_list=[\'acousticness\',\'danceability\',\'energy\',\'instrumentalness\',\'key\',\'livenepathsss\',\'loudness\',\'speechiness\',\'tempo\']\n    train_data=preprocess_df.as_matrix(column_list)\n    train_labels=preprocess_df.as_matrix([\'emotion\'])\n    t_d=shuffle(train_data, random_state=rand_st)\n    t_l=shuffle(train_labels, random_state=rand_st)\n    return t_d,t_l\n\ndef train_svm_model(rand_s=1):\n\n    train_d,train_l=get_train_data(rand_s)\n    svm_clf.fit(train_d[:int(len(train_d))], train_l[:int(len(train_l))])\n    joblib.dump(svm_clf, \'trained_models/SVM_Music_model.pkl\')\n    #print(""\\nComputing Accuracy of : "")\n    #final_pred = svm_clf.score(train_d[-int(len(train_d)*0.10):], train_l[-int(len(train_l)*0.10):])\n    #print(""\\nAccuracy : %f"" % final_pred)\n    return\n\ndef test():\n    pre_process()\n    Acc_list=[]\n    for i in range(1,30):\n        accu=train_and_verify(i)\n        Acc_list.append(accu)\n    print(""\\n Mean Accuracy %f "" %np.mean(Acc_list))\n    return\n\ntrain_svm_model()\n\n\n\n'"
components/PyMusicMood/Music_Test.py,0,"b'import spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\nimport pandas as pd\nimport glob\nimport Spotify_generate as sg\nimport lib_csv_generator as lib_csv\nfrom sklearn.externals import joblib\nfrom sklearn.svm import SVC\n\ndef gen_id_df(lib_path):\n    emotions = [""anger"", ""happy"", ""sadness""]\n\n    music_paths = glob.glob(lib_path+\'*.mp3\')\n    title_list,artist_list,paths = lib_csv.get_batch_metadeta(music_paths)\n\n    id_list = []\n\n    for i in range(len(title_list)):\n        artist = artist_list[i]\n        title = title_list[i]\n        id = sg.get_id(artist[0], title[0])\n        id_list.append(id)\n\n    init_df = pd.DataFrame(\n        {\'title\': title_list,\n         \'artist\': artist_list,\n         \'paths\': paths,\n         \'id\':id_list\n         }, )\n\n    return init_df\n\n\n\ndef pred_emotion(result):\n    column_list = [\'acousticness\', \'danceability\', \'energy\', \'instrumentalness\', \'key\', \'livenepathsss\', \'loudness\',\n                   \'speechiness\', \'tempo\']\n    test_data = result.as_matrix(column_list)\n    emotions = [""anger"", ""happy"", ""sadness""]\n    svm_clf = joblib.load(\'trained_models/SVM_Music_model.pkl\')\n    emot_i=svm_clf.predict(test_data)\n    emo=[]\n    for i in emot_i:\n        emo.append(emotions[i])\n\n    emo_df=pd.DataFrame({\'emotion\':emo},)\n    return emo_df\n\ndef main_func2():\n    lib_path = \'/home/dhanush/Music/TestClass/\'\n    id_df=gen_id_df(lib_path)\n    audio_f_df=sg.batch_audio_features(id_df[\'id\'].tolist())\n    audio_test = pd.concat([id_df, audio_f_df], axis=1)\n    audio_test.to_csv(\'test_data/test.csv\')\n    emo=pred_emotion(audio_test)\n    audio_result=pd.DataFrame({\'title\':audio_test[\'title\'].tolist(),\n                               \'artist\':audio_test[\'artist\'].tolist(),\n                               \'path\':audio_test[\'path\'].tolist(),\n                               \'emotion\':emo},)\n\n    audio_result.to_csv(\'test_data/result.csv\')\n\nmain_func2()\n\n\n\n\n\n'"
components/PyMusicMood/Spotify_generate.py,0,"b'import spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\nimport pandas as pd\nimport glob\n\nSPOTIPY_CLIENT_ID=\'3a41de28eab44ecfa6cd15eadf0c7980\'\nSPOTIPY_CLIENT_SECRET=\'15a02aacd5f8411e896a502df02ca141\'\nSPOTIPY_REDIRECT_URI=\'http://localhost/\'\n\nemotions = [""anger"", ""happy"", ""sadness""] #Emotion list\ncsv_dir=\'csv_files/\' # Destination of the CSV files (happy, sadness and angry)\n\nclient_credentials_manager = SpotifyClientCredentials(client_id=\'3a41de28eab44ecfa6cd15eadf0c7980\',client_secret=\'15a02aacd5f8411e896a502df02ca141\')\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\nspotify = spotipy.Spotify()\n\n\ndef get_id(art,name):\n    results = sp.search(q=\'track:\' + name + \' artist:\' + art, type=\'track\',limit=1)\n    if results[\'tracks\'][\'items\']:\n        track_id =results[\'tracks\'][\'items\'][0][\'id\']\n        return track_id\n    else:\n        return 9999\n\ndef batch_audio_features(track_ids):\n\n\n\n    key = []\n    acousticness = []\n    time_signature = []\n    liveness = []\n    speechiness = []\n    mode = []\n    loudness = []\n    instrumentalness = []\n    energy = []\n    danceability = []\n    tempo = []\n\n    for id in track_ids:\n        if id==\'9999\':\n            key.append(9999)\n            acousticness.append(9999)\n            time_signature.append(9999)\n            liveness.append(9999)\n            speechiness.append(9/home/dhanush/PycharmProjects/PyMusicMood999)\n            mode.append(9999)\n            loudness.append(9999)\n            instrumentalness.append(9999)\n            energy.append(9999)\n            danceability.append(9999)\n            tempo.append(9999)\n        else :\n            audiofeatures = sp.audio_features(id)\n            if audiofeatures:\n                key.append(audiofeatures[0][\'key\'])\n                acousticness.append(audiofeatures[0][\'acousticness\'])\n                time_signature.append(audiofeatures[0][\'time_signature\'])\n                liveness.append(audiofeatures[0][\'liveness\'])\n                speechiness.append(audiofeatures[0][\'speechiness\'])\n                mode.append(audiofeatures[0][\'mode\'])\n                loudness.append(audiofeatures[0][\'loudness\'])\n                instrumentalness.append(audiofeatures[0][\'instrumentalness\'])\n                energy.append(audiofeatures[0][\'energy\'])\n                danceability.append(audiofeatures[0][\'danceability\'])\n                tempo.append(audiofeatures[0][\'tempo\'])\n            else :\n                key.append(9999)\n                acousticness.append(9999)\n                time_signature.append(9999)\n                liveness.append(9999)\n                speechiness.append(9999)\n                mode.append(9999)\n                loudness.append(9999)\n                instrumentalness.append(9999)\n                energy.append(9999)\n                danceability.append(9999)\n                tempo.append(9999)\n\n    combined_df = pd.DataFrame(\n        {\'key\': key,\n         \'acousticness\': acousticness,\n         \'time_signature\': time_signature,\n         \'livenepathsss\': liveness,\n         \'speechiness\': speechiness,\n         \'mode\': mode,\n         \'loudness\': loudness,\n         \'instrumentalness\': instrumentalness,\n         \'energy\': energy,\n         \'danceability\': danceability,\n         \'tempo\': tempo\n         })\n    #combined_df.to_csv(""final_data.csv"")\n    return combined_df\n\ndef gen_id_list() :\n    id_list=[]\n    df = pd.read_csv(csv_dir+\'final.csv\')\n    title_list=df[\'title\'].tolist()\n    artist_list=df[\'artist\'].tolist()\n\n    for i in range(len(title_list)):\n        artist = artist_list[i]\n        title = title_list[i]\n        id = get_id(artist, title)\n        id_list.append(id)\n\n    return id_list\n\n\ndef main_run2():\n    #ids=gen_id_list()\n    #id_df=pd.DataFrame({\'id\':ids})\n    #id_df.to_csv(\'id_df.csv\')\n    id_load=pd.read_csv(\'id_df.csv\')\n    id_list=id_load[\'id\'].tolist()\n    batch_audio_features(id_list)\n    return\n\n\n\ndef combine_dfs():\n    df1 = pd.read_csv(\'final_data.csv\')\n    df2 = pd.read_csv(\'csv_files/final.csv\')\n    track_paths=df2[\'paths\']\n    frame=[df1,df2]\n    df3=pd.concat(frame,axis=1)\n    df3.to_csv(""csv_files/Final_train_DF.csv"")\n    return\n\n\n#main_run2() # Running the python script\n#combine_dfs() # Combine Audiofeatures with metadata\n\n\n\n\'\'\'\naudiofeatures=sp.audio_features(""1J8gdzCFh1bzrl0p0kv5LI"")\nprint (audiofeatures[0])\nprint (\'\\n\')\n\nAudiofeatures is a list of dictionaries.\nas we are searching for only one trackID, access that dictionary using\naudiofeatures[0][\'tempo\'],audiofeatures[0][\'key\'] etc\n\'\'\'\n'"
components/PyMusicMood/lib_csv_generator.py,0,"b'from mutagen.easyid3 import EasyID3\nfrom mutagen.mp3 import MP3\nimport glob\nimport pandas as pd\nimport random\nfrom mutagen.easyid3 import EasyID3\nimport pandas as pd\n\n\n\n\ndef get_song_paths(emotion):  # Function to return the path of all songs belonging to \'emotion\'\n    m_path=\'/home/dhanush/Music/MusicClassify/\'\n    i_path=glob.glob(m_path+emotion+\'/*\')\n    print(i_path)\n    return i_path\ncsv_dest=\'csv_files/\' # Destination of the CSV files (happy, sadness and angry)_path\n\ndef get_batch_metadeta(paths):  # Function to get the metadata of a batch of mp3 files in the locations - paths\n    titles=[]\n    artists=[]\n    for path in paths:\n        audio = EasyID3(path)\n        audio_title=audio[\'title\']\n        audio_artist=audio[\'artist\']\n        titles.append(audio_title)\n        artists.append(audio_artist)\n    return titles,artists,paths\n\ndef get_single_metadata(path):  # Function to get metadata of a single file at - path\n    audio = EasyID3(path)\n    audio_title = audio[\'title\']\n    audio_artist = audio[\'artist\']\n    return audio_title,audio_artist\n\ndef conv_to_dataframe(titles,artists,emot,paths,ind_beg):\n    combined_df = pd.DataFrame(\n        {\'title\': titles,\n         \'artist\': artists,\n         \'paths\':paths\n         },)\n\n    combined_df[\'emotion\'] = emot\n    return combined_df.index(emo)\n\ndef main_run():\n    emotions = [""anger"", ""happy"", ""sadness""]  # Emotion list\n    csv_dest = \'csv_files/\'  # Destination of the CSV files (happy, sadness and angry)\n    ind=0\n    for emo in emotions:\n        paths = get_song_paths(emo)\n        ind=ind+(len(paths)-1)\n        titles,artists,_ = get_batch_metadeta(paths)\n        final_df=conv_to_dataframe(titles, artists, emotions.index(emo)+1, paths,ind)\n        final_df.to_csv(csv_dest+emo+\'.csv\')\n\n    happy=pd.read_csv(csv_dest+\'happy.csv\')\n    happy=happy[[\'title\',\'artist\',\'paths\',\'emotion\']]\n    sadness = pd.read_csv(csv_dest + \'sadness.csv\')\n    sadness = sadness[[\'title\', \'artist\', \'paths\', \'emotion\']]\n    anger = pd.read_csv(csv_dest + \'anger.csv\')\n    anger = anger[[\'title\', \'artist\', \'paths\', \'emotion\']]\n    frames = [happy, sadness, anger]\n    final_conc=pd.concat(frames)\n    final_conc.to_csv(csv_dest+""final.csv"")\n# RUNNING THE SCRIPT\n\n#main_run()\n\n\n\n\n\n'"
components/py-flask-wa/app.py,0,"b'from flask import Flask, jsonify, request, render_template, redirect\nfrom flask_pymongo import PyMongo\nfrom werkzeug import secure_filename\nimport base64\n\napp = Flask(__name__)\n\napp.config[\'MONGO_DBNAME\'] = \'restdb\'\napp.config[\'MONGO_URI\'] = \'mongodb://localhost:27017/restdb\'\n\nmongo = PyMongo(app)\n\n@app.route(\'/\')\ndef index():\n  return render_template(""index.html"")\n\n@app.route(\'/w\')\ndef webcam():\n  return render_template(""webcam.html"")\n\n@app.route(\'/img\')\ndef img():\n  i = request.query_string\n  f = open(\'a.png\',\'wb\')\n  f.write(i.decode(\'base64\'))\n  return ""success <img src=\'"" + i + ""\'>""\n\n@app.route(\'/hello\')\ndef hello():\n  return ""hello world""\n\n@app.route(\'/star\', methods=[\'GET\'])\ndef get_all_stars():\n  star = mongo.db.stars\n  output = []\n  for s in star.find():\n    output.append({\'name\' : s[\'name\'], \'distance\' : s[\'distance\']})\n  return jsonify(output)\n\n@app.route(\'/star/\', methods=[\'GET\'])\ndef get_one_star(name):\n  star = mongo.db.stars\n  s = star.find_one({\'name\' : name})\n  if s:\n    output = {\'name\': s[\'name\'], \'distance\': s[\'distance\']}\n  else:\n    output = ""No such name""\n  return jsonify(output)\n\n@app.route(\'/star\', methods=[\'POST\'])\ndef add_star():\n  star = mongo.db.stars\n  name = request.json[\'name\']\n  distance = request.json[\'distance\']\n  star_id = star.insert({\'name\': name, \'distance\': distance})\n  new_star = star.find_one({\'_id\': star_id})\n  output = {\'name\' : new_star[\'name\'], \'distance\' : new_star[\'distance\']}\n  return jsonify(output)\n\n@app.route(\'/uploader\', methods=[\'POST\'])\ndef upload_file():\n  f = request.files[\'file\']\n  f.save(secure_filename(\'1\'))\n  return ""uploaded""\n\n\nif __name__ == \'__main__\':\n  app.run(debug=True)\n'"
