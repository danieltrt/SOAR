file_path,api_count,code
Iris_claasifier.py,12,"b'import numpy as np\nimport pandas as pd\nimport theano\nimport theano.tensor as T\nfrom sklearn import datasets\nfrom sklearn.cross_validation import train_test_split\n#import matplotlib.pyplot as plt\n\niris = datasets.load_iris()\ndata = iris.data\ntarget = iris.target\n\n# Convert labels into one-hot vectors\nnum_labels = len(np.unique(target))\nlabels = np.eye(num_labels)[target]\n\n# Keeping 20% of data samples as test set\ntrain_X, test_X, train_y, test_y = train_test_split(data, labels, test_size = 0.20)\n\nx_size = train_X[0].shape[0] # size of input layer - ""4""\nh_size = 100 # size of hidden layers(100 nodes)\ny_size = train_y[0].shape[0] # size of output layer - ""3""\n\nalpha = 0.02 # Learning rate\n\n\n# Floating type symbolic expression for training features\nX = T.fmatrix(name=""X"")\n\n# Floating type symbolic expression for training targets\ny = T.fmatrix(name=""y"")\n\nW1_rand = 1/float(np.sqrt(x_size)) # Initialization limit for W1\n\nW2_rand = 1/float(np.sqrt(h_size)) # Initialization limit for W2\n\n# Theano Shared variables for neural network parameters \n\n# Weight for connections between input and hidden layer\nW1 = theano.shared(np.random.uniform(low = -W1_rand, high = W1_rand, size = (x_size, h_size)), name = ""W1"")\n\n# Bias weights for hidden layer\nb1 = theano.shared(np.zeros(h_size), name=\'b1\')\n\n# Weight for connections between input and hidden layer\nW2 = theano.shared(np.random.uniform(low = -W2_rand, high = W2_rand, size = (h_size, y_size)), name = ""W2"")\n\nb2 = theano.shared(np.zeros(y_size), name=\'b2\')\n\n\n\n# Forward Propagation\nz1 = T.dot(X, W1) + b1\na1 = T.nnet.sigmoid(z1)\nz2 = T.dot(a1, W2) + b2\ny_hat = T.nnet.softmax(z2)\n\n# the loss function we want to optimize\nloss = T.nnet.categorical_crossentropy(y_hat, y).mean()\n\n# Returns a target prediction\nprediction = T.argmax(y_hat, axis=1)\n\n# Theano functions that can be called from our Python code\nforward_prop = theano.function([X], y_hat)\ncalculate_loss = theano.function([X, y], loss)\npredict = theano.function([X], prediction)\n\n# Defines automatic differentiation of all weight w.r.t loss\ndW2 = T.grad(loss, W2)\ndb2 = T.grad(loss, b2)\ndW1 = T.grad(loss, W1)\ndb1 = T.grad(loss, b1)\n\n# Gradient step\ngradient_step = theano.function(\n    [X, y],\n    updates=((W2, W2 - alpha * dW2),\n             (W1, W1 - alpha * dW1),\n             (b2, b2 - alpha * db2),\n             (b1, b1 - alpha * db1)))\n\n\nepochs = 500\n\nfor epoch in np.arange(epochs):\n    \n    # One gradient step with complete training set\n    gradient_step(np.array(train_X, \'float32\'), np.array(train_y, \'float32\'))\n    \n    if epoch % 10 == 0 or epoch < 10:\n        \n        # Get the loss\n        current_loss = calculate_loss(np.array(train_X, \'float32\'), np.array(train_y, \'float32\'))\n        \n        # Get the accuracy between predicted and real target values\n        accuracy = np.mean(np.argmax(test_y, axis=1) == predict(np.array(test_X, \'float32\')))\n        \n        print(""Epoch -"", epoch, "" |\\t Loss: "", current_loss, "" |\\t Accuracy: "", accuracy)\n       '"
iris_classifier.py,0,"b""import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.utils import np_utils\n\ndf = pd.read_csv('../input/Iris.csv')\ndf = df.drop(['Id'], axis=1)\ndf['Species'] = df['Species'].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\ntrain, test = train_test_split(df, test_size=.2)\nX_train = train.drop(['Species'], 1).values\ny_train = train['Species'].values\nX_test = test.drop(['Species'], 1).values\ny_test = test['Species'].values\n\ny_train = np_utils.to_categorical(y_train)\ny_test = np_utils.to_categorical(y_test)\n\nnb_epoch = 100\nnb_classes = 3\nbatch_size = 10\n\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=4))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(.2))\n\n\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(.25))\n\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(.5))\n\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, nb_epoch=nb_epoch, verbose=2)\n\nscore = model.evaluate(X_test, y_test)\nprint('Score: ', score[1]*100)"""
