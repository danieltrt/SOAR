file_path,api_count,code
attention_graphs.py,1,"b""import gzip\nimport os\nimport pickle\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmatplotlib.use('TKagg')\n\n\ndef show_attention():\n  # Load attentions\n  print('Loading attentions to pickle file')\n  with gzip.open(\n      os.path.join('training_results', 'torch_train', 'attentions.pkl.gz'),\n      'r') as att_file:\n    attentions = pickle.load(att_file)\n\n  # Set up figure with colorbar\n  fig = plt.figure()\n  ax = fig.add_subplot(111)\n  cax = ax.matshow(np.mean(np.array(attentions),axis=(0,1)), cmap='bone')\n  fig.colorbar(cax)\n\n  # # Set up axes\n  # ax.set_xticklabels([''] + input_sentence.split(' ') +\n  #                    ['<EOS>'], rotation=90)\n  # ax.set_yticklabels([''] + output_words)\n  #\n  # # Show label at every tick\n  # ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n  # ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n  plt.show()\n\n\nshow_attention()\n"""
data_utils.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities for downloading data from WMT, tokenizing, vocabularies.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport re\nimport tarfile\n\nimport tensorflow as tf\nfrom six.moves import urllib\nfrom tensorflow.python.platform import gfile\n\n# Special vocabulary symbols - we always put them at the start.\n_PAD = b""_PAD""\n_GO = b""_GO""\n_EOS = b""_EOS""\n_UNK = b""_UNK""\n_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n\nPAD_ID = 0\nGO_ID = 1\nEOS_ID = 2\nUNK_ID = 3\n\n# Regular expressions used to tokenize.\n_WORD_SPLIT = re.compile(b""([.,!?\\""\':;)(])"")\n_DIGIT_RE = re.compile(br""\\d"")\n\n# URLs for WMT data.\n_WMT_ENFR_TRAIN_URL = ""http://www.statmt.org/wmt10/training-giga-fren.tar""\n_WMT_ENFR_DEV_URL = ""http://www.statmt.org/wmt15/dev-v2.tgz""\n\n\ndef maybe_download(directory, filename, url):\n  """"""Download filename from url unless it\'s already in directory.""""""\n  if not os.path.exists(directory):\n    print(""Creating directory %s"" % directory)\n    os.mkdir(directory)\n  filepath = os.path.join(directory, filename)\n  if not os.path.exists(filepath):\n    print(""Downloading %s to %s"" % (url, filepath))\n    filepath, _ = urllib.request.urlretrieve(url, filepath)\n    statinfo = os.stat(filepath)\n    print(""Successfully downloaded"", filename, statinfo.st_size, ""bytes"")\n  return filepath\n\n\ndef gunzip_file(gz_path, new_path):\n  """"""Unzips from gz_path into new_path.""""""\n  print(""Unpacking %s to %s"" % (gz_path, new_path))\n  with gzip.open(gz_path, ""rb"") as gz_file:\n    with open(new_path, ""wb"") as new_file:\n      for line in gz_file:\n        new_file.write(line)\n\n\ndef get_wmt_enfr_train_set(directory):\n  """"""Download the WMT en-fr training corpus to directory unless it\'s there.""""""\n  train_path = os.path.join(directory, ""giga-fren.release2.fixed"")\n  if not (\n    gfile.Exists(train_path + "".fr"") and gfile.Exists(train_path + "".en"")):\n    corpus_file = maybe_download(directory, ""training-giga-fren.tar"",\n                                 _WMT_ENFR_TRAIN_URL)\n    print(""Extracting tar file %s"" % corpus_file)\n    with tarfile.open(corpus_file, ""r"") as corpus_tar:\n      corpus_tar.extractall(directory)\n    gunzip_file(train_path + "".fr.gz"", train_path + "".fr"")\n    gunzip_file(train_path + "".en.gz"", train_path + "".en"")\n  return train_path\n\n\ndef get_wmt_enfr_dev_set(directory):\n  """"""Download the WMT en-fr training corpus to directory unless it\'s there.""""""\n  dev_name = ""newstest2013""\n  dev_path = os.path.join(directory, dev_name)\n  if not (gfile.Exists(dev_path + "".fr"") and gfile.Exists(dev_path + "".en"")):\n    dev_file = maybe_download(directory, ""dev-v2.tgz"", _WMT_ENFR_DEV_URL)\n    print(""Extracting tgz file %s"" % dev_file)\n    with tarfile.open(dev_file, ""r:gz"") as dev_tar:\n      fr_dev_file = dev_tar.getmember(""dev/"" + dev_name + "".fr"")\n      en_dev_file = dev_tar.getmember(""dev/"" + dev_name + "".en"")\n      fr_dev_file.name = dev_name + "".fr""  # Extract without ""dev/"" prefix.\n      en_dev_file.name = dev_name + "".en""\n      dev_tar.extract(fr_dev_file, directory)\n      dev_tar.extract(en_dev_file, directory)\n  return dev_path\n\n\ndef basic_tokenizer(sentence):\n  """"""Very basic tokenizer: split the sentence into a list of tokens.""""""\n  words = []\n  for space_separated_fragment in sentence.strip().split():\n    words.extend(_WORD_SPLIT.split(space_separated_fragment))\n  return [w for w in words if w]\n\n\ndef create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,\n                      tokenizer=None, normalize_digits=True):\n  """"""Create vocabulary file (if it does not exist yet) from data file.\n\n  Data file is assumed to contain one sentence per line. Each sentence is\n  tokenized and digits are normalized (if normalize_digits is set).\n  Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n  We write it to vocabulary_path in a one-token-per-line format, so that later\n  token in the first line gets id=0, second line gets id=1, and so on.\n\n  Args:\n    vocabulary_path: path where the vocabulary will be created.\n    data_path: data file that will be used to create vocabulary.\n    max_vocabulary_size: limit on the size of the created vocabulary.\n    tokenizer: a function to use to tokenize each data sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(vocabulary_path):\n    print(""Creating vocabulary %s from data %s"" % (vocabulary_path, data_path))\n    vocab = {}\n    with gfile.GFile(data_path, mode=""rb"") as f:\n      counter = 0\n      for line in f:\n        counter += 1\n        if counter % 100000 == 0:\n          print(""  processing line %d"" % counter)\n        line = tf.compat.as_bytes(line)\n        tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n        for w in tokens:\n          word = _DIGIT_RE.sub(b""0"", w) if normalize_digits else w\n          if word in vocab:\n            vocab[word] += 1\n          else:\n            vocab[word] = 1\n      vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n      if len(vocab_list) > max_vocabulary_size:\n        vocab_list = vocab_list[:max_vocabulary_size]\n      with gfile.GFile(vocabulary_path, mode=""wb"") as vocab_file:\n        for w in vocab_list:\n          vocab_file.write(w + b""\\n"")\n\n\ndef initialize_vocabulary(vocabulary_path):\n  """"""Initialize vocabulary from file.\n\n  We assume the vocabulary is stored one-item-per-line, so a file:\n    dog\n    cat\n  will result in a vocabulary {""dog"": 0, ""cat"": 1}, and this function will\n  also return the reversed-vocabulary [""dog"", ""cat""].\n\n  Args:\n    vocabulary_path: path to the file containing the vocabulary.\n\n  Returns:\n    a pair: the vocabulary (a dictionary mapping string to integers), and\n    the reversed vocabulary (a list, which reverses the vocabulary mapping).\n\n  Raises:\n    ValueError: if the provided vocabulary_path does not exist.\n  """"""\n  if gfile.Exists(vocabulary_path):\n    rev_vocab = []\n    rev_vocab = []\n    with gfile.GFile(vocabulary_path, mode=""rb"") as f:\n      rev_vocab.extend(f.readlines())\n    rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\n    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n    return vocab, rev_vocab\n  else:\n    raise ValueError(""Vocabulary file %s not found."", vocabulary_path)\n\n\ndef sentence_to_token_ids(sentence, vocabulary,\n                          tokenizer=None, normalize_digits=True):\n  """"""Convert a string to list of integers representing token-ids.\n\n  For example, a sentence ""I have a dog"" may become tokenized into\n  [""I"", ""have"", ""a"", ""dog""] and with vocabulary {""I"": 1, ""have"": 2,\n  ""a"": 4, ""dog"": 7""} this function will return [1, 2, 4, 7].\n\n  Args:\n    sentence: the sentence in bytes format to convert to token-ids.\n    vocabulary: a dictionary mapping tokens to integers.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n\n  Returns:\n    a list of integers, the token-ids for the sentence.\n  """"""\n\n  if tokenizer:\n    words = tokenizer(sentence)\n  else:\n    words = basic_tokenizer(sentence)\n  if not normalize_digits:\n    return [vocabulary.get(w, UNK_ID) for w in words]\n  # Normalize digits by 0 before looking words up in the vocabulary.\n  return [vocabulary.get(_DIGIT_RE.sub(b""0"", w), UNK_ID) for w in words]\n\n\ndef data_to_token_ids(data_path, target_path, vocabulary_path,\n                      tokenizer=None, normalize_digits=True):\n  """"""Tokenize data file and turn into token-ids using given vocabulary file.\n\n  This function loads data line-by-line from data_path, calls the above\n  sentence_to_token_ids, and saves the result to target_path. See comment\n  for sentence_to_token_ids on the details of token-ids format.\n\n  Args:\n    data_path: path to the data file in one-sentence-per-line format.\n    target_path: path where the file with token-ids will be created.\n    vocabulary_path: path to the vocabulary file.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(target_path):\n    print(""Tokenizing data in %s"" % data_path)\n    vocab, _ = initialize_vocabulary(vocabulary_path)\n    with gfile.GFile(data_path, mode=""rb"") as data_file:\n      with gfile.GFile(target_path, mode=""w"") as tokens_file:\n        counter = 0\n        for line in data_file:\n          counter += 1\n          if counter % 100000 == 0:\n            print(""  tokenizing line %d"" % counter)\n          token_ids = sentence_to_token_ids(tf.compat.as_bytes(line), vocab,\n                                            tokenizer, normalize_digits)\n          tokens_file.write("" "".join([str(tok) for tok in token_ids]) + ""\\n"")\n\n\ndef prepare_wmt_data(data_dir, en_vocabulary_size, fr_vocabulary_size,\n                     tokenizer=None):\n  """"""Get WMT data into data_dir, create vocabularies and tokenize data.\n\n  Args:\n    data_dir: directory in which the data sets will be stored.\n    en_vocabulary_size: size of the English vocabulary to create and use.\n    fr_vocabulary_size: size of the French vocabulary to create and use.\n    tokenizer: a function to use to tokenize each data sentence;\n      if None, basic_tokenizer will be used.\n\n  Returns:\n    A tuple of 6 elements:\n      (1) path to the token-ids for English training data-set,\n      (2) path to the token-ids for French training data-set,\n      (3) path to the token-ids for English development data-set,\n      (4) path to the token-ids for French development data-set,\n      (5) path to the English vocabulary file,\n      (6) path to the French vocabulary file.\n  """"""\n  # Get wmt data to the specified directory.\n  train_path = get_wmt_enfr_train_set(data_dir)\n  dev_path = get_wmt_enfr_dev_set(data_dir)\n\n  from_train_path = train_path + "".en""\n  to_train_path = train_path + "".fr""\n  from_dev_path = dev_path + "".en""\n  to_dev_path = dev_path + "".fr""\n  return prepare_data(data_dir, from_train_path, to_train_path, from_dev_path,\n                      to_dev_path, en_vocabulary_size,\n                      fr_vocabulary_size, tokenizer)\n\n\ndef prepare_data(data_dir, from_train_path, to_train_path, from_dev_path,\n                 to_dev_path, from_vocabulary_size, to_vocabulary_size,\n                 tokenizer=None):\n  """"""Preapre all necessary files that are required for the training.\n\n    Args:\n      data_dir: directory in which the data sets will be stored.\n      from_train_path: path to the file that includes ""from"" training samples.\n      to_train_path: path to the file that includes ""to"" training samples.\n      from_dev_path: path to the file that includes ""from"" dev samples.\n      to_dev_path: path to the file that includes ""to"" dev samples.\n      from_vocabulary_size: size of the ""from language"" vocabulary to create and use.\n      to_vocabulary_size: size of the ""to language"" vocabulary to create and use.\n      tokenizer: a function to use to tokenize each data sentence;\n        if None, basic_tokenizer will be used.\n\n    Returns:\n      A tuple of 6 elements:\n        (1) path to the token-ids for ""from language"" training data-set,\n        (2) path to the token-ids for ""to language"" training data-set,\n        (3) path to the token-ids for ""from language"" development data-set,\n        (4) path to the token-ids for ""to language"" development data-set,\n        (5) path to the ""from language"" vocabulary file,\n        (6) path to the ""to language"" vocabulary file.\n    """"""\n  # Create vocabularies of the appropriate sizes.\n  to_vocab_path = os.path.join(data_dir, ""vocab%d.to"" % to_vocabulary_size)\n  from_vocab_path = os.path.join(data_dir,\n                                 ""vocab%d.from"" % from_vocabulary_size)\n  create_vocabulary(to_vocab_path, to_train_path, to_vocabulary_size, tokenizer)\n  create_vocabulary(from_vocab_path, from_train_path, from_vocabulary_size,\n                    tokenizer)\n\n  # Create token ids for the training data.\n  to_train_ids_path = to_train_path + ("".ids%d"" % to_vocabulary_size)\n  from_train_ids_path = from_train_path + ("".ids%d"" % from_vocabulary_size)\n  data_to_token_ids(to_train_path, to_train_ids_path, to_vocab_path, tokenizer)\n  data_to_token_ids(from_train_path, from_train_ids_path, from_vocab_path,\n                    tokenizer)\n\n  # Create token ids for the development data.\n  to_dev_ids_path = to_dev_path + ("".ids%d"" % to_vocabulary_size)\n  from_dev_ids_path = from_dev_path + ("".ids%d"" % from_vocabulary_size)\n  data_to_token_ids(to_dev_path, to_dev_ids_path, to_vocab_path, tokenizer)\n  data_to_token_ids(from_dev_path, from_dev_ids_path, from_vocab_path,\n                    tokenizer)\n\n  return (from_train_ids_path, to_train_ids_path,\n          from_dev_ids_path, to_dev_ids_path,\n          from_vocab_path, to_vocab_path)\n'"
decode_prediction.py,8,"b'# Created by Albert Aparicio on 21/10/16\n# coding: utf-8\n\n# This import makes Python use \'print\' as in Python 3.x\nfrom __future__ import print_function\n\nimport h5py\nimport numpy as np\nfrom keras.models import model_from_json\nfrom keras.optimizers import RMSprop\nfrom tfglib import utils\nfrom tfglib.construct_table import parse_file\n\n###############\n# Load models #\n###############\n# mvf model\n###########\nmvf_lr = 0.001\ncontext_size = 1\n\nwith open(\'models/mvf_model.json\', \'r\') as model_json:\n    mvf_model = model_from_json(model_json.read())\n\nmvf_model.load_weights(\'models/mvf_weights.h5\')\n\nmvf_rmsprop = RMSprop(lr=mvf_lr)\nmvf_model.compile(loss=\'mae\', optimizer=mvf_rmsprop)\n\n# Load training statistics\nwith h5py.File(\'models/mvf_train_stats.h5\', \'r\') as train_stats:\n    src_mvf_mean = train_stats[\'src_train_mean\'].value\n    src_mvf_std = train_stats[\'src_train_std\'].value\n    trg_mvf_mean = train_stats[\'trg_train_mean\'].value\n    trg_mvf_std = train_stats[\'trg_train_std\'].value\n\n    train_stats.close()\n\n# log(f0) model\n###############\n# Batch shape\nlf0_batch_size = 1\nlf0_tsteps = 50\nlf0_data_dim = 2\n\nwith open(\'models/lf0_model.json\', \'r\') as model_json:\n    lf0_model = model_from_json(model_json.read())\n\nlf0_model.load_weights(\'models/lf0_weights.h5\')\nlf0_model.compile(loss=\'mse\', optimizer=\'rmsprop\')\n\n# Load training statistics\nwith h5py.File(\'models/lf0_train_stats.h5\', \'r\') as train_stats:\n    src_lf0_mean = train_stats[\'src_train_mean\'].value\n    src_lf0_std = train_stats[\'src_train_std\'].value\n    trg_lf0_mean = train_stats[\'trg_train_mean\'].value\n    trg_lf0_std = train_stats[\'trg_train_std\'].value\n\n    train_stats.close()\n\n# cepstrum parameters model\n############################\nmcp_lr = 0.0001\n# Batch shape\nmcp_batch_size = 1\nmcp_tsteps = 50\nmcp_data_dim = 40\n\nwith open(\'models/mcp_model.json\', \'r\') as model_json:\n    mcp_model = model_from_json(model_json.read())\n\nmcp_model.load_weights(\'models/mcp_weights.h5\')\n\nmcp_rmsprop = RMSprop(lr=mcp_lr)\nmcp_model.compile(loss=\'mse\', optimizer=mcp_rmsprop)\n\n# Load training statistics\nwith h5py.File(\'models/mcp_train_stats.h5\', \'r\') as train_stats:\n    src_mcp_mean = train_stats[\'src_train_mean\'][:]\n    src_mcp_std = train_stats[\'src_train_std\'][:]\n    trg_mcp_mean = train_stats[\'trg_train_mean\'][:]\n    trg_mcp_std = train_stats[\'trg_train_std\'][:]\n\n    train_stats.close()\n\n##################\n# Load basenames #\n##################\nbasenames_file = open(\'data/test/basenames.list\', \'r\')\nbasenames_lines = basenames_file.readlines()\n\n# Strip \'\\n\' characters\nbasenames = [line.split(\'\\n\')[0] for line in basenames_lines]\n\n###################\n# Loop over files #\n###################\nfor basename in basenames:\n    ###################\n    # Load parameters #\n    ###################\n    mcp_params = parse_file(40,\n                            \'data/test/vocoded/SF1/\' + basename + \'.mcp.dat\'\n                            )\n    lf0_params = parse_file(1,\n                            \'data/test/vocoded/SF1/\' + basename + \'.lf0.i.dat\'\n                            )\n    mvf_params = parse_file(1,\n                            \'data/test/vocoded/SF1/\' + basename + \'.vf.i.dat\'\n                            )\n\n    # Compute U/V flags\n    assert mvf_params.shape == lf0_params.shape\n    uv_flags = np.empty(mvf_params.shape)\n    for index, vf in enumerate(uv_flags):\n        uv_flags[index] = 1 - utils.kronecker_delta(mvf_params[index])\n\n    # Prepare data for prediction\n    mcp_params = (mcp_params - src_mcp_mean) / src_mcp_std\n    mcp_params = utils.reshape_lstm(mcp_params, mcp_tsteps, mcp_data_dim)\n\n    lf0_params = (lf0_params - src_lf0_mean) / src_lf0_std\n    lf0_params = utils.reshape_lstm(np.column_stack((lf0_params, uv_flags)), lf0_tsteps, lf0_data_dim)\n\n    mvf_params = (mvf_params - src_mvf_mean) / src_mvf_std\n    mvf_params = utils.apply_context(mvf_params, context_size)  # Apply context\n\n    ######################\n    # Predict parameters #\n    ######################\n    mvf_prediction = mvf_model.predict(np.column_stack((mvf_params, uv_flags)))\n    mvf_prediction[:, 0] = (mvf_prediction[:, 0] * trg_mvf_std) + trg_mvf_mean\n\n    lf0_prediction = lf0_model.predict(lf0_params, batch_size=lf0_batch_size)\n    lf0_prediction = lf0_prediction.reshape(-1, 2)\n    # Undo the zero-padding\n    lf0_prediction = lf0_prediction[0:mvf_prediction.shape[0], 0:lf0_prediction.shape[1]]\n    lf0_prediction[:, 0] = (lf0_prediction[:, 0] * trg_lf0_std) + trg_lf0_mean\n\n    mcp_prediction = mcp_model.predict(mcp_params, batch_size=mcp_batch_size)\n    mcp_prediction = mcp_prediction.reshape(-1, mcp_data_dim)\n    # Undo the zero-padding\n    mcp_prediction = mcp_prediction[0:mvf_prediction.shape[0], 0:mcp_prediction.shape[1]]\n    mcp_prediction = (mcp_prediction * trg_mcp_std) + trg_mcp_mean\n\n    # Round U/V predictions\n    lf0_prediction[:, 1] = np.round(lf0_prediction[:, 1])\n    mvf_prediction[:, 1] = np.round(mvf_prediction[:, 1])\n\n    # Apply U/V flag to mvf and lf0 data\n    for index, entry in enumerate(lf0_prediction):\n        if entry[1] == 0:\n            lf0_prediction[index, 0] = -1e+10\n\n    for index, entry in enumerate(mvf_prediction):\n        if entry[1] == 0:\n            mvf_prediction[index, 0] = 0\n\n    ###########################\n    # Save parameters to file #\n    ###########################\n    # TODO Make the code save these files\n    np.savetxt(\n        \'data/test/predicted/SF1-TF1/\' + basename + \'.vf.dat\',\n        mvf_prediction[:, 0]\n    )\n    np.savetxt(\n        \'data/test/predicted/SF1-TF1/\' + basename + \'.lf0.dat\',\n        lf0_prediction[:, 0]\n    )\n    np.savetxt(\n        \'data/test/predicted/SF1-TF1/\' + basename + \'.mcp.dat\',\n        mcp_prediction,\n        delimiter=\'\\t\'\n    )\n\n    # #####################\n    # # Decode parameters #\n    # #####################\n    # os.popen(\'mkdir -p data/test/predicted/SF1-TF1\')\n    #\n    # # TODO Make the execution use the system\'s $PATH\n    # # f = os.popen(\'echo $PATH\')\n    # # print(f.read())\n    # subprocess.check_output([\'echo\', \'$PATH\'])\n    #\n    # # f = os.popen(\n    # #     ""bash decode_aho.sh \'data/test/predicted/SF1-TF1/\' "" + basename\n    # # )\n'"
dtw_probabilities.py,6,"b'# Created by albert aparicio on 04/02/2017\n# coding: utf-8\n\n# This script reads DTW files and counts the repetitions of each frame, for\n# later computing the statistic distribution of the repetitions\n\n# This import makes Python use \'print\' as in Python 3.x\nfrom __future__ import print_function\n\nfrom os import walk, path\nfrom time import time\n\nimport numpy as np\nfrom h5py import File as h5_File\nfrom tfglib.utils import Progbar\nfrom tfglib.utils import display_time\n\n# Save processing start time\nstart_time = time()\n\nprint(\'Reading DTW alignment files\' + \'\\n\' +\n      \'---------------------------\' + \'\\n\')\n\ndtw_path = \'data/training/dtw/beam2\'\nseen = set()\nn_files = 150\n\n# (number of frames, number of files, 2)\ndistribution = np.empty((2000, n_files, 2))\n\nfor root, dirs, files in walk(dtw_path):\n    for index, file in enumerate(files):\n        # print(file)\n\n        dtw_data = np.loadtxt(\n            path.join(dtw_path, file),\n            delimiter=\'\\t\',\n            dtype=int\n        )\n\n        for i in range(dtw_data.shape[1]):\n\n            for x in dtw_data[:, i]:\n                if x not in seen:\n                    seen.add(x)\n                    distribution[x, index, i] = 1\n                else:\n                    distribution[x, index, i] += 1\n\n            seen = set()\n\ndistribution = distribution.reshape((-1, 2))\nmask = []\n\nprint(\'Removing 0 - 0 alignments\' + \'\\n\' +\n      \'-------------------------\')\n\nprogress_bar = Progbar(target=distribution.shape[0])\nprogress_bar.update(0)\n\nfor row in range(0, distribution.shape[0]):\n    if np.array_equal(distribution[row, :], [0, 0]):\n        mask.append([True, True])\n    else:\n        mask.append([False, False])\n\n    progress_bar.update(row + 1)\n\ndistribution = np.ma.compress_rows(np.ma.array(distribution, mask=mask))\n\n# plt.hist(\n#     distribution[:, 0] - distribution[:, 1],\n#     bins=50,\n#     rwidth=.5,\n#     align=\'left\'\n# )\n# plt.show()\n\n# Count repetitions of each entry in distribution\ndist_list = (distribution[:, 0] - distribution[:, 1]).tolist()\n\nvalues = []\nprobabilities = []\n\nprint(\'\\n\' + \'\\n\' + \'Computing probabilities of each repetition\' + \'\\n\' +\n             \'------------------------------------------\' + \'\\n\')\n\nfor index, item in enumerate(dist_list):\n    if item not in values:\n        # probabilities[str(item)] = dist_list.count(item) / len(dist_list)\n        values.append(item)\n        probabilities.append(dist_list.count(item) / len(dist_list))\n\nprint(\'Saving probabilities to .h5 file\' + \'\\n\' +\n      \'--------------------------------\' + \'\\n\')\n\nwith h5_File(\'pretrain_data/dtw_probabilities.h5\', \'w\') as f:\n    # Save numbers and probabilities\n    f.create_dataset(\n        \'values\',\n        data=np.array(values, dtype=int),\n        compression=""gzip"",\n        compression_opts=9\n    )\n    f.create_dataset(\n        \'probabilities\',\n        data=np.array(probabilities),\n        compression=""gzip"",\n        compression_opts=9\n    )\n\n    f.close()\n\nprint(\'========================\' + \'\\n\' +\n      \'======= FINISHED =======\' + \'\\n\' +\n      \'========================\')\n\nprint(\'Elapsed time: \' + display_time(time() - start_time))\n\nexit()\n'"
lf0_lstm.py,10,"b'# Created by albert aparicio on 31/10/16\n# coding: utf-8\n\n# This script initializes and trains an LSTM-based RNN for log(f0) mapping\n\n# This import makes Python use \'print\' as in Python 3.x\nfrom __future__ import print_function\n\nimport os\n\nimport h5py\nimport numpy as np\nfrom keras.layers import Dense, LSTM\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop\nfrom tfglib import construct_table as ct, utils\n\n#######################\n# Sizes and constants #\n#######################\n# Batch shape\nbatch_size = 1\ntsteps = 50\ndata_dim = 2\n\n# Other constants\nepochs = 50\n\n#############\n# Load data #\n#############\n# Switch to decide if datatable must be build or can be loaded from a file\nbuild_datatable = False\n\nprint(\'Starting...\')\n\nif build_datatable:\n  # Build datatable of training and test data\n  # (data is already encoded with Ahocoder)\n  print(\'Saving training datatable...\', end=\'\')\n  train_data = ct.save_datatable(\n      \'data/training/\',\n      \'train_data\',\n      \'data/train_datatable\'\n      )\n  print(\'done\')\n\n  print(\'Saving test datatable...\', end=\'\')\n  test_data = ct.save_datatable(\n      \'data/test/\',\n      \'test_data\',\n      \'data/test_datatable\'\n      )\n  print(\'done\')\n\nelse:\n  # Retrieve datatables from .h5 files\n  print(\'Loading training datatable...\', end=\'\')\n  train_data = ct.load_datatable(\n      \'data/train_datatable.h5\',\n      \'train_data\'\n      )\n  print(\'done\')\n\n  print(\'Loading test datatable...\', end=\'\')\n  test_data = ct.load_datatable(\n      \'data/test_datatable.h5\',\n      \'test_data\'\n      )\n  print(\'done\')\n\n################\n# Prepare data #\n################\n# Number of training samples\nnb_samples = 14500\n# Take lfo and U/V flag columns\nsrc_train_data = np.column_stack(\n    (train_data[0:nb_samples, 40],\n     train_data[0:nb_samples, 42])\n    )  # Source data\n\ntrg_train_data = np.column_stack(\n    (train_data[0:nb_samples, 83],\n     train_data[0:nb_samples, 85])\n    )  # Target data\n\nsrc_valid_data = np.column_stack(\n    (train_data[nb_samples:train_data.shape[0], 40],\n     train_data[nb_samples:train_data.shape[0], 42])\n    )  # Source data\ntrg_valid_data = np.column_stack(\n    (train_data[nb_samples:train_data.shape[0], 83],\n     train_data[nb_samples:train_data.shape[0], 85])\n    )  # Target data\n\nsrc_test_data = np.column_stack((test_data[:, 40], test_data[:, 42]))\ntrg_test_data = np.column_stack((test_data[:, 83], test_data[:, 85]))\n\n# Remove means and normalize\nsrc_train_mean = np.mean(src_train_data[:, 0], axis=0)\nsrc_train_std = np.std(src_train_data[:, 0], axis=0)\n\nsrc_train_data[:, 0] = (src_train_data[:, 0] - src_train_mean) / src_train_std\nsrc_valid_data[:, 0] = (src_valid_data[:, 0] - src_train_mean) / src_train_std\nsrc_test_data[:, 0] = (src_test_data[:, 0] - src_train_mean) / src_train_std\n\ntrg_train_mean = np.mean(trg_train_data[:, 0], axis=0)\ntrg_train_std = np.std(trg_train_data[:, 0], axis=0)\n\ntrg_train_data[:, 0] = (trg_train_data[:, 0] - trg_train_mean) / trg_train_std\ntrg_valid_data[:, 0] = (trg_valid_data[:, 0] - trg_train_mean) / trg_train_std\n# trg_test_data[:, 0] = (trg_test_data[:, 0] - trg_train_mean) / trg_train_std\n\n# Zero-pad and reshape data\nsrc_train_data = utils.reshape_lstm(src_train_data, tsteps, data_dim)\nsrc_valid_data = utils.reshape_lstm(src_valid_data, tsteps, data_dim)\nsrc_test_data = utils.reshape_lstm(src_test_data, tsteps, data_dim)\ntrg_train_data = utils.reshape_lstm(trg_train_data, tsteps, data_dim)\ntrg_valid_data = utils.reshape_lstm(trg_valid_data, tsteps, data_dim)\ntrg_test_data = utils.reshape_lstm(trg_test_data, tsteps, data_dim)\n\n# Save training statistics\nwith h5py.File(\'models/lf0_train_stats.h5\', \'w\') as f:\n  h5_src_train_mean = f.create_dataset(""src_train_mean"", data=src_train_mean)\n  h5_src_train_std = f.create_dataset(""src_train_std"", data=src_train_std)\n  h5_trg_train_mean = f.create_dataset(""trg_train_mean"", data=trg_train_mean)\n  h5_trg_train_std = f.create_dataset(""trg_train_std"", data=trg_train_std)\n\n  f.close()\n\n################\n# Define Model #\n################\n# Define an LSTM-based RNN\nprint(\'Creating Model\')\nmodel = Sequential()\n\nmodel.add(LSTM(units=100,\n               batch_input_shape=(batch_size, tsteps, data_dim),\n               return_sequences=True,\n               stateful=True))\nmodel.add(TimeDistributed(Dense(2)))\n\nrmsprop = RMSprop(lr=0.0001)\nmodel.compile(loss=\'mse\', optimizer=rmsprop)\n\n###############\n# Train model #\n###############\nprint(\'Training\')\nepoch = list(range(epochs))\nloss = []\nval_loss = []\n\nfor i in range(epochs):\n  print(\'Epoch\', i, \'/\', epochs)\n  history = model.fit(src_train_data,\n                      trg_train_data,\n                      batch_size=batch_size,\n                      verbose=1,\n                      epochs=1,\n                      shuffle=False,\n                      validation_data=(src_valid_data, trg_valid_data))\n\n  loss.append(history.history[\'loss\'])\n  val_loss.append(history.history[\'val_loss\'])\n\n  model.reset_states()\n\nprint(\'Saving model\')\nmodel.save_weights(\'models/lf0_weights.h5\')\n\nwith open(\'models/lf0_model.json\', \'w\') as model_json:\n  model_json.write(model.to_json())\n\nprint(\'Saving training results\')\nwith h5py.File(os.path.join(\'training_results\', \'baseline\', \'lf0_history.h5\'),\n               \'w\') as hist_file:\n  hist_file.create_dataset(\'loss\', data=loss,\n                           compression=\'gzip\', compression_opts=9)\n  hist_file.create_dataset(\'val_loss\', data=val_loss,\n                           compression=\'gzip\', compression_opts=9)\n  hist_file.create_dataset(\'epoch\', data=epoch, compression=\'gzip\',\n                           compression_opts=9)\n\n  hist_file.close()\n\nprint(\'========================\' + \'\\n\' +\n      \'======= FINISHED =======\' + \'\\n\' +\n      \'========================\')\n\nexit()\n'"
lf0_post_training.py,7,"b""# Created by albert aparicio on 04/11/16\n# coding: utf-8\n\n# This script computes the error metrics of the LSTM-RNN model for lf0 mapping\n\n# This import makes Python use 'print' as in Python 3.x\nfrom __future__ import print_function\n\nimport os\n\nimport h5py\nimport matplotlib\nimport numpy as np\nfrom ahoproc_tools.error_metrics import AFPR, RMSE\nfrom keras.models import model_from_json\nfrom scipy.stats import pearsonr\nfrom tfglib import utils\n\nmatplotlib.use('TKagg')\nfrom matplotlib import pyplot as plt\n\n#######################\n# Sizes and constants #\n#######################\n# Batch shape\nbatch_size = 1\ntsteps = 50\ndata_dim = 2\n\n##############\n# Load model #\n##############\n# Load already trained LSTM-RNN model\nwith open('models/lf0_model.json', 'r') as model_json:\n  model = model_from_json(model_json.read())\n\nmodel.load_weights('models/lf0_weights.h5')\nmodel.compile(loss='mse', optimizer='rmsprop')\n\n#############\n# Load data #\n#############\n# Load training statistics\nwith h5py.File('models/lf0_train_stats.h5', 'r') as train_stats:\n  src_train_mean = train_stats['src_train_mean'].value\n  src_train_std = train_stats['src_train_std'].value\n  trg_train_mean = train_stats['trg_train_mean'].value\n  trg_train_std = train_stats['trg_train_std'].value\n\n  train_stats.close()\n\n# Load test data\nprint('Loading test data...', end='')\nwith h5py.File('data/test_datatable.h5', 'r') as test_datatable:\n  test_data = test_datatable['test_data'][:, :]\n\n  test_datatable.close()\n\nsrc_test_frames = np.column_stack((test_data[:, 40], test_data[:, 42]))\nsrc_test_frames[:, 0] = (src_test_frames[:, 0] - src_train_mean) / src_train_std\nsrc_test_frames = utils.reshape_lstm(src_test_frames, tsteps, data_dim)\n\n# Zero-pad and reshape target test data\ntrg_test_frames = utils.reshape_lstm(\n    np.column_stack((test_data[:, 83], test_data[:, 85])), tsteps,\n    data_dim).reshape(-1, 2)\n\nprint('done')\n\n################\n# Predict data #\n################\nprint('Predicting')\nprediction_test = model.predict(src_test_frames, batch_size=batch_size)\nprediction_test = prediction_test.reshape(-1, 2)\n\n# De-normalize predicted output\nprediction_test[:, 0] = (prediction_test[:, 0] * trg_train_std) + trg_train_mean\n\n#################\n# Error metrics #\n#################\n# Compute and print RMSE of test data\nrmse_test = RMSE(\n    np.exp(trg_test_frames[:, 0]),\n    np.exp(prediction_test[:, 0]),\n    mask=trg_test_frames[:, 1]\n    )\n\nprint('Test RMSE: ', rmse_test)\n\n# Compute and print Pearson correlation between target and prediction\npearson = pearsonr(\n    np.exp(trg_test_frames[:, 0]),\n    np.exp(prediction_test[:, 0])\n    )\n\nprint('Pearson correlation: ', pearson[0])\n\n# Round the predicted flags to get binary values\nprediction_test[:, 1] = np.round(prediction_test[:, 1])\n\n# Compute Accuracy of U/V flag prediction\naccuracy, _, _, _ = AFPR(trg_test_frames[:, 1], prediction_test[:, 1])\n\nprint('U/V Accuracy: {}%'.format(accuracy * 100))\n# print('F-measure: ', accuracy[1] * 100, '%')\n# print('Precision: ', accuracy[2] * 100, '%')\n# print('Recall: ', accuracy[3] * 100, '%')\n\n# Load training parameters and save loss curves\nwith h5py.File('training_results/baseline/lf0_history.h5', 'r') as hist_file:\n  loss = hist_file['loss'][:]\n  val_loss = hist_file['val_loss'][:]\n  epoch = hist_file['epoch'][:]\n\n  hist_file.close()\n\nprint('Saving loss curves')\n\nplt.plot(epoch, loss, epoch, val_loss)\nplt.legend(['loss', 'val_loss'], loc='best')\nplt.grid(b=True)\nplt.suptitle('Baseline log(f0) Loss curves')\nplt.savefig(os.path.join('training_results', 'baseline', 'lf0_loss_curves.eps'),\n            bbox_inches='tight')\n\nprint('========================' + '\\n' +\n      '======= FINISHED =======' + '\\n' +\n      '========================')\n\nexit()\n"""
mcp_gru.py,4,"b'# Created by albert aparicio on 06/11/16\n# coding: utf-8\n\n# This script defines a GRU-RNN to map the cepstral components of the signal\n\n# This import makes Python use \'print\' as in Python 3.x\nfrom __future__ import print_function\n\nimport os\n\nimport h5py\nimport numpy as np\nfrom keras.layers import Dense, Dropout, GRU\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop\nfrom tfglib import construct_table as ct, utils\n\n#######################\n# Sizes and constants #\n#######################\n# Batch shape\nbatch_size = 1\ntsteps = 50\ndata_dim = 40\n\n# Other constants\nepochs = 50\n# epochs = 25\n\n#############\n# Load data #\n#############\n#  Switch to decide if datatable must be build or can be loaded from a file\nbuild_datatable = False\n\nprint(\'Starting...\')\n\nif build_datatable:\n  # Build datatable of training and test data\n  # (data is already encoded with Ahocoder)\n  print(\'Saving training datatable...\', end=\'\')\n  train_data = ct.save_datatable(\n      \'data/training/\',\n      \'train_data\',\n      \'data/train_datatable\'\n      )\n  print(\'done\')\n\n  print(\'Saving test datatable...\', end=\'\')\n  test_data = ct.save_datatable(\n      \'data/test/\',\n      \'test_data\',\n      \'data/test_datatable\'\n      )\n  print(\'done\')\n\nelse:\n  # Retrieve datatables from .h5 files\n  print(\'Loading training datatable...\', end=\'\')\n  train_data = ct.load_datatable(\n      \'data/train_datatable.h5\',\n      \'train_data\'\n      )\n  print(\'done\')\n\n  print(\'Loading test datatable...\', end=\'\')\n  test_data = ct.load_datatable(\n      \'data/test_datatable.h5\',\n      \'test_data\'\n      )\n  print(\'done\')\n\n################\n# Prepare data #\n################\n# Take MCP parameter columns\nsrc_train_data = train_data[0:17500, 0:40]  # Source data\ntrg_train_data = train_data[0:17500, 43:83]  # Target data\n\nsrc_valid_data = train_data[17500:train_data.shape[0], 0:40]  # Source data\ntrg_valid_data = train_data[17500:train_data.shape[0], 43:83]  # Target data\n\nsrc_test_data = test_data[:, 0:40]  # Source data\ntrg_test_data = test_data[:, 43:83]  # Target data\n\n# Remove means and normalize\nsrc_train_mean = np.mean(src_train_data, axis=0)\nsrc_train_std = np.std(src_train_data, axis=0)\n\nsrc_train_data = (src_train_data - src_train_mean) / src_train_std\nsrc_valid_data = (src_valid_data - src_train_mean) / src_train_std\nsrc_test_data = (src_test_data - src_train_mean) / src_train_std\n\ntrg_train_mean = np.mean(trg_train_data, axis=0)\ntrg_train_std = np.std(trg_train_data, axis=0)\n\ntrg_train_data = (trg_train_data - trg_train_mean) / trg_train_std\ntrg_valid_data = (trg_valid_data - trg_train_mean) / trg_train_std\n\n# Zero-pad and reshape data\nsrc_train_data = utils.reshape_lstm(src_train_data, tsteps, data_dim)\nsrc_valid_data = utils.reshape_lstm(src_valid_data, tsteps, data_dim)\nsrc_test_data = utils.reshape_lstm(src_test_data, tsteps, data_dim)\n\ntrg_train_data = utils.reshape_lstm(trg_train_data, tsteps, data_dim)\ntrg_valid_data = utils.reshape_lstm(trg_valid_data, tsteps, data_dim)\n\n# Save training statistics\nwith h5py.File(\'models/mcp_train_stats.h5\', \'w\') as f:\n  h5_src_train_mean = f.create_dataset(""src_train_mean"", data=src_train_mean)\n  h5_src_train_std = f.create_dataset(""src_train_std"", data=src_train_std)\n  h5_trg_train_mean = f.create_dataset(""trg_train_mean"", data=trg_train_mean)\n  h5_trg_train_std = f.create_dataset(""trg_train_std"", data=trg_train_std)\n\n  f.close()\n\n################\n# Define Model #\n################\n# Define an GRU-based RNN\nprint(\'Creating Model\')\nmodel = Sequential()\n\nmodel.add(GRU(units=70,\n              batch_input_shape=(batch_size, tsteps, data_dim),\n              return_sequences=True,\n              stateful=True))\nmodel.add(Dropout(0.5))\nmodel.add(TimeDistributed(Dense(data_dim)))\n\nrmsprop = RMSprop(lr=0.0001)\nmodel.compile(loss=\'mse\', optimizer=rmsprop)\n\n###############\n# Train model #\n###############\nprint(\'Training\')\nepoch = list(range(epochs))\nloss = []\nval_loss = []\n\nfor i in range(epochs):\n  print(\'Epoch\', i, \'/\', epochs)\n  history = model.fit(src_train_data,\n                      trg_train_data,\n                      batch_size=batch_size,\n                      verbose=1,\n                      epochs=1,\n                      shuffle=False,\n                      validation_data=(src_valid_data, trg_valid_data))\n\n  loss.append(history.history[\'loss\'])\n  val_loss.append(history.history[\'val_loss\'])\n\n  model.reset_states()\n\nprint(\'Saving model\')\nmodel.save_weights(\'models/mcp_weights.h5\')\n\nwith open(\'models/mcp_model.json\', \'w\') as model_json:\n  model_json.write(model.to_json())\n\nprint(\'Saving training results\')\nwith h5py.File(os.path.join(\'training_results\', \'baseline\', \'mcp_history.h5\'),\n               \'w\') as hist_file:\n  hist_file.create_dataset(\'loss\', data=loss,\n                           compression=\'gzip\', compression_opts=9)\n  hist_file.create_dataset(\'val_loss\', data=val_loss,\n                           compression=\'gzip\', compression_opts=9)\n  hist_file.create_dataset(\'epoch\', data=epoch, compression=\'gzip\',\n                           compression_opts=9)\n\n  hist_file.close()\n\nprint(\'========================\' + \'\\n\' +\n      \'======= FINISHED =======\' + \'\\n\' +\n      \'========================\')\n\nexit()\n'"
mcp_post_training.py,0,"b""# Created by albert aparicio on 08/11/16\n# coding: utf-8\n\n# This script computes the error metrics of the GRU-RNN model for mcp mapping\n\n# This import makes Python use 'print' as in Python 3.x\nfrom __future__ import print_function\n\nimport os\n\nimport h5py\nimport matplotlib\nfrom ahoproc_tools.error_metrics import MCD\nfrom keras.models import model_from_json\nfrom keras.optimizers import RMSprop\nfrom tfglib import utils\n\nmatplotlib.use('TKagg')\nfrom matplotlib import pyplot as plt\n\n#######################\n# Sizes and constants #\n#######################\n# Batch shape\nbatch_size = 1\ntsteps = 50\ndata_dim = 40\n\n##############\n# Load model #\n##############\n# Load already trained LSTM-RNN model\nprint('Loading model...', end='')\nwith open('models/mcp_model.json', 'r') as model_json:\n  model = model_from_json(model_json.read())\n\nmodel.load_weights('models/mcp_weights.h5')\n\nrmsprop = RMSprop(lr=0.0001)\nmodel.compile(loss='mse', optimizer=rmsprop)\nprint('done')\n\n#############\n# Load data #\n#############\n# Load training statistics\nwith h5py.File('models/mcp_train_stats.h5', 'r') as train_stats:\n  src_train_mean = train_stats['src_train_mean'][:]\n  src_train_std = train_stats['src_train_std'][:]\n  trg_train_mean = train_stats['trg_train_mean'][:]\n  trg_train_std = train_stats['trg_train_std'][:]\n\n  train_stats.close()\n\n# Load test data\nprint('Loading test data...', end='')\nwith h5py.File('data/test_datatable.h5', 'r') as test_datatable:\n  test_data = test_datatable['test_data'][:, :]\n\n  test_datatable.close()\n\nsrc_test_data = test_data[:, 0:40]  # Source data\nsrc_test_data = utils.reshape_lstm(src_test_data, tsteps, data_dim)\nsrc_test_data = (src_test_data - src_train_mean) / src_train_std\n\ntrg_test_data = test_data[:, 43:83]  # Target data\nprint('done')\n\n################\n# Predict data #\n################\nprint('Predicting')\nprediction_test = model.predict(src_test_data, batch_size=batch_size)\nprediction_test = prediction_test.reshape(-1, data_dim)\n\n# De-normalize predicted output\nprediction_test = (prediction_test * trg_train_std) + trg_train_mean\n\n#################\n# Error metrics #\n#################\n# Compute MCD of test data\nmcd_test = MCD(\n    trg_test_data,\n    prediction_test\n    )\n\n# Print resulting MCD\nprint('Test MCD: ', mcd_test)\n\n# Load training parameters and save loss curves\nwith h5py.File('training_results/baseline/mcp_history.h5', 'r') as hist_file:\n  loss = hist_file['loss'][:]\n  val_loss = hist_file['val_loss'][:]\n  epoch = hist_file['epoch'][:]\n\n  hist_file.close()\n\nprint('Saving loss curves')\n\nplt.plot(epoch, loss, epoch, val_loss)\nplt.legend(['loss', 'val_loss'], loc='best')\nplt.grid(b=True)\nplt.suptitle('Baseline MCP Loss curves')\nplt.savefig(os.path.join('training_results', 'baseline', 'mcp_loss_curves.eps'),\n            bbox_inches='tight')\n\nprint('========================' + '\\n' +\n      '======= FINISHED =======' + '\\n' +\n      '========================')\n\nexit()\n"""
mvf_dnn.py,7,"b'# Created by albert aparicio on 18/10/16\n# coding: utf-8\n\n# This is a script for initializing and training a fully-connected DNN\n\n# This import makes Python use \'print\' as in Python 3.x\nfrom __future__ import print_function\n\nimport os\n\nimport h5py\nimport numpy as np\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers import Dense, Dropout\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop\nfrom tfglib import construct_table as ct\nfrom tfglib.utils import apply_context\n\n#############\n# Load data #\n#############\n#  Switch to decide if datatable must be build or can be loaded from a file\nbuild_datatable = False\n\nprint(\'Starting...\')\n\nif build_datatable:\n  # Build datatable of training and test data\n  # (data is already encoded with Ahocoder)\n  print(\'Saving training datatable...\', end=\'\')\n  train_data = ct.save_datatable(\n      \'data/training/\',\n      \'train_data\',\n      \'data/train_datatable\'\n      )\n  print(\'done\')\n\n  print(\'Saving test datatable...\', end=\'\')\n  test_data = ct.save_datatable(\n      \'data/test/\',\n      \'test_data\',\n      \'data/test_datatable\'\n      )\n  print(\'done\')\n\nelse:\n  # Retrieve datatables from .h5 files\n  print(\'Loading training datatable...\', end=\'\')\n  train_data = ct.load_datatable(\n      \'data/train_datatable.h5\',\n      \'train_data\'\n      )\n  print(\'done\')\n\n  print(\'Loading test datatable...\', end=\'\')\n  test_data = ct.load_datatable(\n      \'data/test_datatable.h5\',\n      \'test_data\'\n      )\n  print(\'done\')\n\n#######################\n# Sizes and constants #\n#######################\nbatch_size = 300\nnb_epochs = 700\nlearning_rate = 0.00000055\ncontext_size = 1\n\n################\n# Prepare data #\n################\n# Randomize frames\n# np.random.shuffle(train_data)\n\n# Split into train and validation (17500 train, 2500 validation)\nsrc_train_frames = train_data[0:17500, 41:43]  # Source data\ntrg_train_frames = train_data[0:17500, 84:86]  # Target data\n\nsrc_valid_frames = train_data[17500:train_data.shape[0], 41:43]  # Source data\ntrg_valid_frames = train_data[17500:train_data.shape[0], 84:86]  # Target data\n\n# Normalize data\nsrc_train_mean = np.mean(src_train_frames[:, 0], axis=0)\nsrc_train_std = np.std(src_train_frames[:, 0], axis=0)\ntrg_train_mean = np.mean(trg_train_frames[:, 0], axis=0)\ntrg_train_std = np.std(trg_train_frames[:, 0], axis=0)\n\nsrc_train_frames[:, 0] = (src_train_frames[:, 0] -\n                          src_train_mean) / src_train_std\nsrc_valid_frames[:, 0] = (src_valid_frames[:, 0] -\n                          src_train_mean) / src_train_std\n\ntrg_train_frames[:, 0] = (trg_train_frames[:, 0] -\n                          trg_train_mean) / trg_train_std\ntrg_valid_frames[:, 0] = (trg_valid_frames[:, 0] -\n                          trg_train_mean) / trg_train_std\n\n# Save training statistics\nwith h5py.File(\'models/mvf_train_stats.h5\', \'w\') as f:\n  h5_src_train_mean = f.create_dataset(""src_train_mean"", data=src_train_mean)\n  h5_src_train_std = f.create_dataset(""src_train_std"", data=src_train_std)\n  h5_trg_train_mean = f.create_dataset(""trg_train_mean"", data=trg_train_mean)\n  h5_trg_train_std = f.create_dataset(""trg_train_std"", data=trg_train_std)\n\n  f.close()\n\n# Apply context\nsrc_train_frames_context = np.column_stack((\n  apply_context(src_train_frames[:, 0], context_size), src_train_frames[:, 1]\n  ))\nsrc_valid_frames_context = np.column_stack((\n  apply_context(src_valid_frames[:, 0], context_size), src_valid_frames[:, 1]\n  ))\n\n# exit()\n\n################\n# Define Model #\n################\n# Adjust DNN sizes to implement context\nprint(\'Evaluate DNN...\')\nmodel = Sequential()\n\n# model.add(Dense(100, input_dim=2))\nmodel.add(Dense(100, input_dim=(2 * context_size + 1) + 1))\nmodel.add(LeakyReLU(alpha=0.3))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(100))\nmodel.add(LeakyReLU(alpha=0.3))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(2, activation=\'linear\'))\n\n# sgd = SGD(lr=learning_rate, decay=1e-6, momentum=0.9, clipnorm=10)\nrmsprop = RMSprop(lr=learning_rate)\n\n# model.compile(loss=\'mse\', optimizer=sgd)\nmodel.compile(loss=\'mae\', optimizer=rmsprop)\n\n###############\n# Train model #\n###############\nreduce_lr = ReduceLROnPlateau(\n    monitor=\'val_loss\',\n    factor=0.1,\n    patience=5,\n    mode=\'min\')\n\nhistory = model.fit(\n    src_train_frames_context,\n    trg_train_frames,\n    batch_size=batch_size,\n    nb_epoch=nb_epochs,\n    verbose=1,\n    validation_data=(src_valid_frames_context, trg_valid_frames),\n    callbacks=[reduce_lr]\n    )\n\nprint(\'Saving model\')\nmodel.save_weights(\'models/mvf_weights.h5\')\n\nwith open(\'models/mvf_model.json\', \'w\') as model_json:\n  model_json.write(model.to_json())\n\nprint(\'Saving training results\')\nwith h5py.File(os.path.join(\'training_results\', \'baseline\', \'mvf_history.h5\'),\n               \'w\') as hist_file:\n  hist_file.create_dataset(\'loss\', data=history.history[\'loss\'],\n                           compression=\'gzip\', compression_opts=9)\n  hist_file.create_dataset(\'val_loss\', data=history.history[\'val_loss\'],\n                           compression=\'gzip\', compression_opts=9)\n  hist_file.create_dataset(\'epoch\', data=history.epoch, compression=\'gzip\',\n                           compression_opts=9)\n\n  hist_file.close()\n\nprint(\'========================\' + \'\\n\' +\n      \'======= FINISHED =======\' + \'\\n\' +\n      \'========================\')\n\nexit()\n'"
mvf_post_training.py,1,"b""# Created by albert aparicio on 28/10/16\n# coding: utf-8\n\n# This import makes Python use 'print' as in Python 3.x\nfrom __future__ import print_function\n\nimport os\n\nimport h5py\nimport matplotlib\nimport numpy as np\nfrom ahoproc_tools.error_metrics import RMSE\nfrom keras.models import model_from_json\nfrom keras.optimizers import RMSprop\nfrom tfglib.utils import apply_context\n\nmatplotlib.use('TKagg')\nfrom matplotlib import pyplot as plt\n\n#######################\n# Sizes and constants #\n#######################\nbatch_size = 300\nnb_epochs = 700\nlearning_rate = 0.00000055\ncontext_size = 1\n\n##############\n# Load model #\n##############\nprint('Loading model...', end='')\nwith open('models/mvf_model.json', 'r') as model_json:\n  model = model_from_json(model_json.read())\n\nmodel.load_weights('models/mvf_weights.h5')\n\nrmsprop = RMSprop(lr=learning_rate)\nmodel.compile(loss='mae', optimizer=rmsprop)\n\n#############\n# Load data #\n#############\n# Load training statistics\nwith h5py.File('models/mvf_train_stats.h5', 'r') as train_stats:\n  src_train_mean = train_stats['src_train_mean'].value\n  src_train_std = train_stats['src_train_std'].value\n  trg_train_mean = train_stats['trg_train_mean'].value\n  trg_train_std = train_stats['trg_train_std'].value\n\n  train_stats.close()\n\n# Load test data\nprint('Loading test data...', end='')\nwith h5py.File('data/test_datatable.h5', 'r') as test_datatable:\n  test_data = test_datatable['test_data'][:, :]\n\n  test_datatable.close()\n\nsrc_test_data = test_data[:, 41:43]  # Source data\nsrc_test_data[:, 0] = (src_test_data[:, 0] - src_train_mean) / src_train_std\n\ntrg_test_data = test_data[:, 84:86]  # Target data\n\n# Apply context\nsrc_test_data_context = np.column_stack((\n  apply_context(src_test_data[:, 0], context_size), src_test_data[:, 1]\n  ))\nprint('done')\n\n################\n# Predict data #\n################\nprint('Predicting')\nprediction = model.predict(src_test_data_context)\n\n# De-normalize predicted output\nprediction[:, 0] = (prediction[:, 0] * trg_train_std) + trg_train_mean\n\n#################\n# Error metrics #\n#################\n# Compute and print RMSE of test data\nrmse_test = RMSE(\n    trg_test_data[:, 0],\n    prediction[:, 0],\n    mask=trg_test_data[:, 1]\n    )\n\nprint('Test RMSE: ', rmse_test)\n\n# Load training parameters and save loss curves\nwith h5py.File('training_results/baseline/mvf_history.h5', 'r') as hist_file:\n  loss = hist_file['loss'][:]\n  val_loss = hist_file['val_loss'][:]\n  epoch = hist_file['epoch'][:]\n\n  hist_file.close()\n\nprint('Saving loss curves')\n\nplt.plot(epoch, loss, epoch, val_loss)\nplt.legend(['loss', 'val_loss'], loc='best')\nplt.grid(b=True)\nplt.suptitle('Baseline MVF Loss curves')\nplt.savefig(os.path.join('training_results', 'baseline', 'mvf_loss_curves.eps'),\n            bbox_inches='tight')\n\n# # Histogram of predicted training data and training data itself\n# plt.hist(prediction[:, 0], bins=100)\n# plt.title('Prediction frames')\n# plt.savefig('prediction_hist.png', bbox_inches='tight')\n# plt.show()\n\n# # Histogram of training samples\n# plt.figure()\n# plt.hist(vf_gtruth, bins=100)\n# plt.title('Training target frames')\n# plt.savefig('gtruth_hist.png', bbox_inches='tight')\n# plt.show()\n\nprint('========================' + '\\n' +\n      '======= FINISHED =======' + '\\n' +\n      '========================')\n\nexit()\n"""
pretrain_echo_files_list.py,0,"b'# Created by Albert Aparicio on 7/12/16\n# coding: utf-8\n\n# This import makes Python use \'print\' as in Python 3.x\nfrom __future__ import print_function\n\nfrom os.path import join\n\nfrom h5py import File\n\ndata_path = \'pretrain_data/training_chop\'\nparams_file = \'pretrain_params.h5\'\n\nwith File(join(data_path, params_file), \'r\') as file:\n    files_list_encoded = file[\'files_list\'][:]\n    test_files_list = [n[0] for n in files_list_encoded]\n\nwith open(join(data_path, \'pretrain_basenames.list\'), \'w\') as thefile:\n    for item in test_files_list:\n        thefile.write(""%s\\n"" % item.decode(\'utf-8\'))\n'"
seq2seq.py,36,"b'# Created by albert aparicio on 21/11/16\n# coding: utf-8\n\n# This script defines an encoder-decoder GRU-RNN model\n# to map the source\'s sequence parameters to the target\'s sequence parameters\n\n# TODO Document and explain steps\n# TODO substitute print calls for logging.info calls when applicable\n# https://docs.python.org/2/howto/logging.html#logging-basic-tutorial\n\n# This import makes Python use \'print\' as in Python 3.x\nfrom __future__ import print_function\n\nfrom sys import version_info as py_version\nfrom time import time\n\nimport h5py\nimport numpy as np\nimport tfglib.seq2seq_datatable as s2s\nfrom ahoproc_tools import error_metrics\nfrom keras.layers import Embedding\nfrom keras.layers import Input, Dropout, Dense, merge, TimeDistributed\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.utils.generic_utils import Progbar\nfrom tfglib.pretrain_data_params import pretrain_load_data_parameters\nfrom tfglib.pretrain_data_params import pretrain_save_data_parameters\nfrom tfglib.pretrain_data_params import pretrain_train_generator\nfrom tfglib.seq2seq_normalize import maxmin_scaling\nfrom tfglib.utils import display_time\n\n# Save training start time\nstart_time = time()\n\n############\n# Switches #\n############\npretrain = True\n\n# Decide if datatable/parameters must be built or can be loaded from a file\nbuild_datatable = False\n\n# Decide if the model must use weights from a previous training\nload_weights = False\n\n#######################\n# Sizes and constants #\n#######################\nmodel_description = \'seq2seq_pretrain_no-frame-noise\'\n\n# Batch shape\nbatch_size = 115\noutput_dim = 44\ndata_dim = output_dim + 10 + 10\nemb_size = 256\n\n# Other constants\nnb_epochs = 20\nlearning_rate = 0.001\nvalidation_fraction = 0.25\n\nif load_weights:\n  start_epoch = 5\nelse:\n  start_epoch = 0\n\n#############\n# Load data #\n#############\nif pretrain:\n  data_path = \'pretrain_data/training_chop\'\n\n  if build_datatable:\n    print(\'Saving pretraining parameters\')\n    (\n      max_train_length,\n      train_speakers_max,\n      train_speakers_min,\n      files_list\n    ) = pretrain_save_data_parameters(data_path)\n\n  else:\n    print(\'Load pretraining parameters\')\n    (\n      max_train_length,\n      train_speakers_max,\n      train_speakers_min,\n      files_list\n    ) = pretrain_load_data_parameters(data_path)\n\n  train_speakers = train_speakers_max.shape[0]\n\n  # #################################\n  # # TODO COMMENT AFTER DEVELOPING #\n  # #################################\n  # batch_size = 2\n  # nb_epochs = 1\n  #\n  # files_list = files_list[:5]\n  #\n  # model_description = \'DEV_\' + model_description\n  # #################################\n\nelse:\n  print(\'Preparing data\\n\' + \'=\' * 8 * 5)\n  if build_datatable:\n    # Build datatable of training and test data\n    # (data is already encoded with Ahocoder)\n    print(\'Saving training datatable...\', end=\'\')\n    (src_train_datatable,\n     src_train_masks,\n     trg_train_datatable,\n     trg_train_masks,\n     max_train_length,\n     train_speakers_max,\n     train_speakers_min\n     ) = s2s.seq2seq_save_datatable(\n      \'data/training/\',\n      \'data/seq2seq_train_datatable\'\n    )\n    print(\'done\')\n\n    print(\'Saving test datatable...\', end=\'\')\n    (src_test_datatable,\n     src_test_masks,\n     trg_test_datatable,\n     trg_test_masks,\n     max_test_length,\n     test_speakers_max,\n     test_speakers_min\n     ) = s2s.seq2seq_save_datatable(\n      \'data/test/\',\n      \'data/seq2seq_test_datatable\'\n    )\n    print(\'done\')\n\n  else:\n    # Retrieve datatables from .h5 files\n    print(\'Loading training datatable...\', end=\'\')\n    (src_train_datatable,\n     src_train_masks,\n     trg_train_datatable,\n     trg_train_masks,\n     max_train_length,\n     train_speakers_max,\n     train_speakers_min\n     ) = s2s.seq2seq2_load_datatable(\n      \'data/seq2seq_train_datatable.h5\'\n    )\n    print(\'done\')\n\n  train_speakers = train_speakers_max.shape[0]\n\n  ##################\n  # Normalize data #\n  ##################\n  # Iterate over sequence \'slices\'\n  assert src_train_datatable.shape[0] == trg_train_datatable.shape[0]\n\n  for i in range(src_train_datatable.shape[0]):\n    (\n      src_train_datatable[i, :, 0:42],\n      trg_train_datatable[i, :, 0:42]\n    ) = maxmin_scaling(\n      src_train_datatable[i, :, :],\n      src_train_masks[i, :],\n      trg_train_datatable[i, :, :],\n      trg_train_masks[i, :],\n      train_speakers_max,\n      train_speakers_min\n    )\n\n  ################################################\n  # Split data into training and validation sets #\n  ################################################\n  # #################################\n  # # TODO COMMENT AFTER DEVELOPING #\n  # #################################\n  # batch_size = 2\n  # nb_epochs = 2\n  #\n  # num = 10\n  # src_train_datatable = src_train_datatable[0:num]\n  # src_train_masks = src_train_masks[0:num]\n  # trg_train_datatable = trg_train_datatable[0:num]\n  # trg_train_masks = trg_train_masks[0:num]\n  #\n  # model_description = \'DEV_\' + model_description\n  # #################################################\n\n  src_train_data = src_train_datatable[0:int(np.floor(\n    src_train_datatable.shape[0] * (1 - validation_fraction)))]\n  src_valid_data = src_train_datatable[int(np.floor(\n    src_train_datatable.shape[0] * (1 - validation_fraction))):]\n\n  trg_train_data = trg_train_datatable[0:int(np.floor(\n    trg_train_datatable.shape[0] * (1 - validation_fraction)))]\n  trg_valid_data = trg_train_datatable[int(np.floor(\n    trg_train_datatable.shape[0] * (1 - validation_fraction))):]\n\n  trg_train_masks_f = trg_train_masks[0:int(np.floor(\n    trg_train_masks.shape[0] * (1 - validation_fraction)))]\n  trg_valid_masks_f = trg_train_masks[int(np.floor(\n    trg_train_masks.shape[0] * (1 - validation_fraction))):]\n\n################\n# Define Model #\n################\nprint(\'Initializing model\\n\' + \'=\' * 8 * 5)\nmain_input = Input(shape=(max_train_length, output_dim),\n                   dtype=\'float32\',\n                   name=\'main_input\')\n\nsrc_spk_input = Input(\n  shape=(max_train_length,),\n  dtype=\'int32\',\n  name=\'src_spk_in\'\n)\ntrg_spk_input = Input(\n  shape=(max_train_length,),\n  dtype=\'int32\',\n  name=\'trg_spk_in\'\n)\n\nembedded_spk_indexes = Embedding(\n  input_dim=train_speakers,\n  output_dim=5,\n  input_length=max_train_length,\n  name=\'spk_index_embedding\'\n)\n\nmerged_parameters = merge(\n  [main_input,\n   embedded_spk_indexes(src_spk_input),\n   embedded_spk_indexes(trg_spk_input)\n   ],\n  mode=\'concat\',\n  name=\'inputs_merge\'\n)\n\n# Bidirectional encoder LSTM\nencoder_LSTM_forwards = LSTM(\n  go_backwards=False,\n  output_dim=emb_size,\n  return_sequences=True,\n  consume_less=\'gpu\',\n  name=\'encoder_LSTM_forwards\'\n)(merged_parameters)\n\nencoder_LSTM_backwards = LSTM(\n  go_backwards=True,\n  output_dim=emb_size,\n  return_sequences=True,\n  consume_less=\'gpu\',\n  name=\'encoder_LSTM_backwards\'\n)(merged_parameters)\n\nencoder_LSTM_merged = merge(\n  [encoder_LSTM_forwards, encoder_LSTM_backwards],\n  mode=\'sum\',\n  name=\'encoder_bidirectional_merge\'\n)\n\n# Feedback input\nfeedback_in = Input(shape=(max_train_length, output_dim), name=\'feedback_in\')\ndec_in = merge([encoder_LSTM_merged, feedback_in], mode=\'concat\')\n\ndecoder_LSTM = LSTM(\n  emb_size,\n  return_sequences=True,\n  consume_less=\'gpu\',\n  name=\'decoder_LSTM\'\n)(dec_in)\n\ndropout_layer = Dropout(0.5)(decoder_LSTM)\n\nparameters_LSTM = LSTM(\n  output_dim - 2,\n  return_sequences=True,\n  consume_less=\'gpu\',\n  activation=\'linear\',\n  name=\'params_output\'\n)(dropout_layer)\n\nflags_Dense = TimeDistributed(Dense(\n  2,\n  activation=\'sigmoid\',\n), name=\'flags_output\')(dropout_layer)\n\nmodel = Model(input=[main_input, src_spk_input, trg_spk_input, feedback_in],\n              output=[parameters_LSTM, flags_Dense])\n\noptimizer_name = \'adam\'\nadam = Adam(clipnorm=5)\nparams_loss = \'mse\'\nflags_loss = \'binary_crossentropy\'\n\nif load_weights:\n  # Load weights from previous training\n  model.load_weights(\'models/\' + model_description + \'_\' + params_loss + \'_\' +\n                     flags_loss + \'_\' + optimizer_name + \'_epoch_\' +\n                     str(start_epoch) + \'_lr_\' + str(learning_rate) +\n                     \'_weights.h5\')\n\nmodel.compile(\n  optimizer=adam, sample_weight_mode=""temporal"",\n  loss={\'params_output\': params_loss, \'flags_output\': flags_loss}\n)\n\n###############\n# Train model #\n###############\nif pretrain:\n  print(\'Pretraining\' + \'\\n\' + \'-----------\')\n\n  training_history = []\n  validation_history = []\n  val_mcd = []\n  val_pitch_rmse = []\n  val_uv_accuracy = []\n\n  val_samples = int(len(files_list) * validation_fraction)\n  sampl_epoch = int(len(files_list) - val_samples)\n\n  batch_generator = pretrain_train_generator(\n    data_path,\n    batch_size=batch_size,\n    basename_len=14,\n    replicate=False\n  )\n  val_generator = pretrain_train_generator(\n    data_path,\n    batch_size=batch_size,\n    validation=True,\n    basename_len=14,\n    replicate=False\n  )\n\n  for epoch in range(start_epoch, nb_epochs):\n    print(\'Epoch {} of {}\'.format(epoch + 1, nb_epochs))\n\n    nb_batches = int(np.floor(sampl_epoch / batch_size))\n    progress_bar = Progbar(target=nb_batches)\n\n    epoch_train_partial_loss = []\n\n    try:\n      progress_bar.update(0)\n    except OverflowError as err:\n      raise Exception(\'nb_batches is 0. Please check the training data\')\n\n    for index in range(nb_batches):\n      train_in, train_out, train_mask = next(batch_generator)\n      epoch_train_partial_loss.append(\n        model.train_on_batch(\n          train_in,\n          train_out,\n          sample_weight=train_mask\n        )\n      )\n\n      progress_bar.update(index + 1)\n\n    # Obtain validation losses and metrics\n    (val_input, val_target, val_mask) = next(val_generator)\n\n    epoch_val_loss = model.evaluate(\n      val_input,\n      val_target,\n      sample_weight=val_mask,\n      batch_size=batch_size\n    )\n\n    # Mask data\n    val_pred = model.predict_on_batch(val_input)\n\n    for sequence in range(len(val_pred)):\n      val_pred[sequence] = np.ma.array(\n        val_pred[sequence],\n        mask=np.logical_not(np.repeat(\n          val_mask[\'sample_weights\'],\n          val_pred[sequence].shape[2],\n          axis=2\n        ))\n      )\n\n    if py_version.major == 3:\n      dict_iter = val_target.items()\n    else:\n      dict_iter = val_target.iteritems()\n\n    for key, value in dict_iter:\n      val_target[key] = np.ma.array(\n        value,\n        mask=np.logical_not(np.repeat(\n          val_mask[\'sample_weights\'],\n          value.shape[2],\n          axis=2\n        ))\n      )\n\n    # Compute validation metrics\n    val_mcd.append(error_metrics.MCD(\n      np.ma.compress_rows(\n        val_target[\'params_output\'][:, :, 0:40].reshape((-1, 40))),\n      np.ma.compress_rows(val_pred[0][:, :, 0:40].reshape((-1, 40)))\n    ))\n\n    val_pitch_rmse.append(error_metrics.RMSE(\n      np.ma.compress_rows(\n        val_target[\'params_output\'][:, :, 40].reshape((-1, 1))),\n      np.ma.compress_rows(val_pred[0][:, :, 40].reshape((-1, 1)))\n    )[0])\n\n    print(np.ma.compress_rows(\n      np.round(val_target[\'flags_output\'][:, :, 0]).reshape((-1, 1))))\n    print(np.ma.compress_rows(np.round(val_pred[1][:, :, 0]).reshape((-1, 1))))\n\n    uv_accuracy, _, _, _ = error_metrics.AFPR(\n      np.ma.compress_rows(\n        np.round(val_target[\'flags_output\'][:, :, 0]).reshape((-1, 1))),\n      np.ma.compress_rows(np.round(val_pred[1][:, :, 0]).reshape((-1, 1)))\n    )\n    val_uv_accuracy.append(uv_accuracy)\n\n    epoch_train_loss = np.mean(np.array(epoch_train_partial_loss), axis=0)\n\n    training_history.append(epoch_train_loss)\n    validation_history.append(epoch_val_loss)\n\n    # Generate epoch report\n    print(\'loss: \' + str(training_history[-1]) +\n          \' - val_loss: \' + str(validation_history[-1]) + \'\\n\')\n    print(\'Cepstrum MCD: \' + str(val_mcd[-1]) + \' dB\')\n    print(\'Pitch RMSE: \' + str(val_pitch_rmse[-1]))\n    print(\'U/V accuracy: \' + str(val_uv_accuracy[-1]))\n\n    ###########################\n    # Save model after each epoch #\n    ###########################\n    print(\'Saving model\\n\' + \'=\' * 8 * 5)\n\n    model.save_weights(\n      \'models/\' + model_description + \'_\' + params_loss + \'_\' +flags_loss +\n      \'_\' + optimizer_name + \'_epoch_\' + str(epoch) + \'_lr_\' +\n      str(learning_rate) + \'_weights.h5\')\n\n    with open(\'models/\' + model_description + \'_\' + params_loss + \'_\' +\n              flags_loss + \'_\' + optimizer_name + \'_epoch_\' + str(epoch) +\n              \'_lr_\' + str(learning_rate) + \'_model.json\', \'w\'\n              ) as model_json:\n      model_json.write(model.to_json())\n\n  # Save metrics\n  np.savetxt(\n    \'training_results/\' + model_description + \'_\' + params_loss + \'_\' +\n    flags_loss + \'_\' + optimizer_name + \'_epochs_\' + str(nb_epochs) + \'_lr_\' +\n    str(learning_rate) + \'_mcd.csv\', val_mcd, delimiter=\',\'\n  )\n  np.savetxt(\n    \'training_results/\' + model_description + \'_\' + params_loss + \'_\' +\n    flags_loss + \'_\' + optimizer_name + \'_epochs_\' + str(nb_epochs) + \'_lr_\' +\n    str(learning_rate) + \'_rmse.csv\', val_pitch_rmse, delimiter=\',\'\n  )\n  np.savetxt(\n    \'training_results/\' + model_description + \'_\' + params_loss + \'_\' +\n    flags_loss + \'_\' + optimizer_name + \'_epochs_\' + str(nb_epochs) + \'_lr_\' +\n    str(learning_rate) + \'_acc.csv\', val_uv_accuracy, delimiter=\',\'\n  )\n\nelse:\n  print(\'Training\\n\' + \'=\' * 8 * 5)\n\n  training_history = []\n  validation_history = []\n\n  for epoch in range(nb_epochs):\n    print(\'Epoch {} of {}\'.format(epoch + 1, nb_epochs))\n\n    nb_batches = int(src_train_data.shape[0] / batch_size)\n    progress_bar = Progbar(target=nb_batches)\n\n    epoch_train_partial_loss = []\n\n    try:\n      progress_bar.update(0)\n    except OverflowError as err:\n      raise Exception(\'nb_batches is 0. Please check the training data\')\n\n    for index in range(nb_batches):\n      # Get batch of sequences and masks\n      src_batch = src_train_data[index * batch_size:(index + 1) * batch_size]\n      trg_batch = trg_train_data[index * batch_size:(index + 1) * batch_size]\n      batch_masks = trg_train_masks_f[\n                    index * batch_size:(index + 1) * batch_size]\n\n      # Prepare feedback data\n      feedback_data = np.roll(trg_batch, 1, axis=2)\n      feedback_data[:, 0, :] = 0\n\n      epoch_train_partial_loss.append(\n        model.train_on_batch(\n          {\'main_input\' : src_batch,\n           \'feedback_in\': feedback_data},\n          {\'params_output\': trg_batch[:, :, 0:42],\n           \'flags_output\' : trg_batch[:, :, 42:44]},\n          sample_weight={\'params_output\': batch_masks,\n                         \'flags_output\' : batch_masks}\n        )\n      )\n\n      progress_bar.update(index + 1)\n\n    # Prepare validation feedback data\n    feedback_valid_data = np.roll(trg_valid_data, 1, axis=2)\n    feedback_valid_data[:, 0, :] = 0\n\n    epoch_val_loss = model.evaluate(\n      {\'main_input\' : src_valid_data,\n       \'feedback_in\': feedback_valid_data},\n      {\'params_output\': trg_valid_data[:, :, 0:42],\n       \'flags_output\' : trg_valid_data[:, :, 42:44]},\n      batch_size=batch_size,\n      sample_weight={\'params_output\': trg_valid_masks_f,\n                     \'flags_output\' : trg_valid_masks_f},\n      verbose=0\n    )\n\n    epoch_train_loss = np.mean(np.array(epoch_train_partial_loss), axis=0)\n\n    training_history.append(epoch_train_loss)\n    validation_history.append(epoch_val_loss)\n\n    # Generate epoch report\n    print(\'loss: \' + str(training_history[-1]) +\n          \' - val_loss: \' + str(validation_history[-1]) + \'\\n\')\n    print()\n\n    ###########################\n    # Saving after each epoch #\n    ###########################\n    print(\'Saving model\\n\' + \'=\' * 8 * 5)\n\n    model.save_weights(\n      \'models/\' + model_description + \'_\' + params_loss + \'_\' + flags_loss +\n      \'_\' + optimizer_name + \'_epoch_\' + str(epoch) + \'_lr_\' +\n      str(learning_rate) + \'_weights.h5\')\n\n    with open(\'models/\' + model_description + \'_\' + params_loss + \'_\' +\n              flags_loss + \'_\' + optimizer_name + \'_epoch_\' + str(epoch) +\n              \'_lr_\' + str(learning_rate) + \'_model.json\', \'w\'\n              ) as model_json:\n      model_json.write(model.to_json())\n\nprint(\'Saving training parameters\\n\' + \'=\' * 8 * 5)\nwith h5py.File(\'training_results/\' + model_description +\n                   \'_training_params.h5\', \'w\') as f:\n  f.attrs.create(\'params_loss\', np.string_(params_loss))\n  f.attrs.create(\'flags_loss\', np.string_(flags_loss))\n  f.attrs.create(\'optimizer\', np.string_(optimizer_name))\n  f.attrs.create(\'epochs\', nb_epochs, dtype=int)\n  f.attrs.create(\'learning_rate\', learning_rate)\n  f.attrs.create(\'train_speakers_max\', train_speakers_max)\n  f.attrs.create(\'train_speakers_min\', train_speakers_min)\n  f.attrs.create(\'metrics_names\',\n                 [np.string_(name) for name in model.metrics_names]\n                 )\n\nprint(\'Saving training results\')\nnp.savetxt(\n  \'training_results/\' + model_description + \'_\' + params_loss + \'_\' +\n  flags_loss + \'_\' + optimizer_name + \'_epochs_\' + str(nb_epochs) + \'_lr_\' +\n  str(learning_rate) + \'_epochs.csv\', np.arange(nb_epochs), delimiter=\',\'\n)\n\nnp.savetxt(\n  \'training_results/\' + model_description + \'_\' + params_loss + \'_\' +\n  flags_loss + \'_\' + optimizer_name + \'_epochs_\' + str(nb_epochs) + \'_lr_\' +\n  str(learning_rate) + \'_loss.csv\', training_history, delimiter=\',\'\n)\nnp.savetxt(\n  \'training_results/\' + model_description + \'_\' + params_loss + \'_\' +\n  flags_loss + \'_\' + optimizer_name + \'_epochs_\' + str(nb_epochs) + \'_lr_\' +\n  str(learning_rate) + \'_val_loss.csv\', validation_history, delimiter=\',\'\n)\n\nprint(\'========================\' + \'\\n\' +\n      \'======= FINISHED =======\' + \'\\n\' +\n      \'========================\')\n\nend_time = time()\nprint(\'Elapsed time: \' + display_time(end_time - start_time))\n\nexit()\n'"
seq2seq_dataloader.py,9,"b""# Created by albert aparicio on 31/03/17\n# coding: utf-8\n\n# This script defines a data loader for the Seq2Seq model\n\n# TODO Document and explain steps\n# TODO Move this model to tfglib\n\n# This import makes Python use 'print' as in Python 3.x\nfrom __future__ import print_function\n\nimport numpy as np\nimport tfglib.seq2seq_datatable as s2s\nfrom tfglib.seq2seq_normalize import maxmin_scaling\nfrom tfglib.utils import init_logger\n\n\nclass DataLoader(object):\n  # TODO Finish this class and move it to a new file\n  def __init__(self, args, test=False, max_seq_length=None, shortseq=True,\n               logger_level='INFO'):\n    self.logger = init_logger(name=__name__, level=logger_level)\n\n    self.logger.debug('DataLoader init')\n    self.batch_size = args.batch_size\n\n    if test:\n      self.s2s_datatable = s2s.Seq2SeqDatatable(\n          args.test_data_path, args.test_out_file,\n          basenames_file='tcstar_basenames.list', shortseq=shortseq,\n          max_seq_length=int(max_seq_length), vocoded_dir='tcstar_vocoded')\n\n      (self.src_test_data, self.src_seq_len, self.trg_test_data,\n       self.trg_test_masks_f, self.trg_seq_len, self.train_src_speakers,\n       self.train_src_speakers_max, self.train_src_speakers_min,\n       self.train_trg_speakers, self.train_trg_speakers_max,\n       self.train_trg_speakers_min, dataset_max_seq_length) = self.load_dataset(\n          args.train_out_file,\n          args.save_h5,\n          test=test\n          )\n\n      self.test_batches_per_epoch = int(\n          np.floor(self.src_test_data.shape[0] / self.batch_size)\n          )\n\n    else:\n      self.s2s_datatable = s2s.Seq2SeqDatatable(\n          args.train_data_path, args.train_out_file,\n          basenames_file='tcstar_basenames.list', shortseq=shortseq,\n          max_seq_length=int(max_seq_length), vocoded_dir='tcstar_vocoded')\n\n      (src_datatable, self.src_seq_len, trg_datatable, trg_masks,\n       self.trg_seq_len, self.train_src_speakers, self.train_src_speakers_max,\n       self.train_src_speakers_min, self.train_trg_speakers,\n       self.train_trg_speakers_max, self.train_trg_speakers_min,\n       dataset_max_seq_length) = self.load_dataset(\n          args.train_out_file,\n          args.save_h5\n          )\n\n      self.logger.debug('Split into training and validation')\n      ################################################\n      # Split data into training and validation sets #\n      ################################################\n      # ############################\n      # # COMMENT AFTER DEVELOPING #\n      # ############################\n      # batch_size = 2\n      # nb_epochs = 2\n      #\n      # num = 10\n      # src_datatable = src_datatable[0:num]\n      # src_masks = src_masks[0:num]\n      # trg_datatable = trg_datatable[0:num]\n      # trg_masks = trg_masks[0:num]\n      #\n      # model_description = 'DEV_' + model_description\n      # #################################################\n\n      self.src_train_data = src_datatable[0:int(np.floor(\n          src_datatable.shape[0] * (1 - args.val_fraction)))]\n      self.src_valid_data = src_datatable[int(np.floor(\n          src_datatable.shape[0] * (1 - args.val_fraction))):]\n\n      self.trg_train_data = trg_datatable[0:int(np.floor(\n          trg_datatable.shape[0] * (1 - args.val_fraction)))]\n      self.trg_valid_data = trg_datatable[int(np.floor(\n          trg_datatable.shape[0] * (1 - args.val_fraction))):]\n\n      self.trg_train_masks_f = trg_masks[0:int(np.floor(\n          trg_masks.shape[0] * (1 - args.val_fraction)))]\n      self.trg_valid_masks_f = trg_masks[int(np.floor(\n          trg_masks.shape[0] * (1 - args.val_fraction))):]\n\n      self.train_batches_per_epoch = int(\n          np.floor(self.src_train_data.shape[0] / self.batch_size)\n          )\n      self.valid_batches_per_epoch = int(\n          np.floor(self.src_valid_data.shape[0] / self.batch_size)\n          )\n\n    if shortseq:\n      self.max_seq_length = max_seq_length\n    else:\n      self.max_seq_length = dataset_max_seq_length\n\n  def load_dataset(self, train_out_file, save_h5, test=False):\n    import h5py\n\n    self.logger.debug('Load test dataset')\n    if save_h5:\n      self.logger.info('Saving datatable')\n\n      (src_datatable,\n       src_masks,\n       src_seq_len,\n       trg_datatable,\n       trg_masks,\n       trg_seq_len,\n       train_src_speakers_max,\n       train_src_speakers_min,\n       train_trg_speakers_max,\n       train_trg_speakers_min\n       ) = self.s2s_datatable.seq2seq_save_datatable()\n\n      self.logger.info('DONE - Saving datatable')\n\n    else:\n      self.logger.info('Load parameters')\n      (src_datatable,\n       src_masks,\n       src_seq_len,\n       trg_datatable,\n       trg_masks,\n       trg_seq_len,\n       train_src_speakers_max,\n       train_src_speakers_min,\n       train_trg_speakers_max,\n       train_trg_speakers_min\n       ) = self.s2s_datatable.seq2seq_load_datatable()\n      self.logger.info('DONE - Loaded parameters')\n\n    if test:\n      # Load training speakers data\n      with h5py.File(train_out_file + '.h5', 'r') as file:\n        # Load datasets\n        train_src_speakers_max = file.attrs.get('src_speakers_max')\n        train_src_speakers_min = file.attrs.get('src_speakers_min')\n        train_trg_speakers_max = file.attrs.get('trg_speakers_max')\n        train_trg_speakers_min = file.attrs.get('trg_speakers_min')\n\n        file.close()\n\n    train_src_speakers = train_src_speakers_max.shape[0]\n    train_trg_speakers = train_trg_speakers_max.shape[0]\n\n    # Normalize data\n    self.logger.debug('Normalize data')\n\n    # Iterate over sequence 'slices'\n    assert src_datatable.shape[0] == trg_datatable.shape[0]\n\n    for i in range(src_datatable.shape[0]):\n      (\n        src_datatable[i, :, 0:42],\n        trg_datatable[i, :, 0:42]\n        ) = maxmin_scaling(\n          src_datatable[i, :, :],\n          src_masks[i, :],\n          trg_datatable[i, :, :],\n          trg_masks[i, :],\n          train_src_speakers_max,\n          train_src_speakers_min,\n          train_trg_speakers_max,\n          train_trg_speakers_min\n          )\n\n    return (src_datatable, src_seq_len, trg_datatable, trg_masks, trg_seq_len,\n            train_src_speakers, train_src_speakers_max, train_src_speakers_min,\n            train_trg_speakers, train_trg_speakers_max, train_trg_speakers_min,\n            self.s2s_datatable.max_seq_length)\n\n  def next_batch(self, test=False, validation=False):\n    self.logger.debug('Choice between training data or validation data')\n    if test:\n      data_dict = {'src_data'         : self.src_test_data,\n                   'trg_data'         : self.trg_test_data,\n                   'trg_mask'         : self.trg_test_masks_f,\n                   'batches_per_epoch': self.test_batches_per_epoch}\n    else:\n      if validation:\n        data_dict = {'src_data'         : self.src_valid_data,\n                     'trg_data'         : self.trg_valid_data,\n                     'trg_mask'         : self.trg_valid_masks_f,\n                     'batches_per_epoch': self.valid_batches_per_epoch}\n      else:\n        # Training\n        data_dict = {'src_data'         : self.src_train_data,\n                     'trg_data'         : self.trg_train_data,\n                     'trg_mask'         : self.trg_train_masks_f,\n                     'batches_per_epoch': self.train_batches_per_epoch}\n\n    self.logger.debug('Initialize next batch generator')\n    batch_id = 0\n\n    while True:\n      self.logger.debug('--> Next batch - Prepare <--')\n      src_batch = data_dict['src_data'][\n                  batch_id * self.batch_size:(batch_id + 1) * self.batch_size,\n                  :, :]\n      src_batch_seq_len = self.src_seq_len[\n                          batch_id * self.batch_size:\n                          (batch_id + 1) * self.batch_size]\n      trg_batch = data_dict['trg_data'][\n                  batch_id * self.batch_size:(batch_id + 1) * self.batch_size,\n                  :, :]\n      trg_mask = data_dict['trg_mask'][\n                 batch_id * self.batch_size:(batch_id + 1) * self.batch_size, :]\n\n      batch_id = (batch_id + 1) % data_dict['batches_per_epoch']\n\n      self.logger.debug('--> Next batch - Yield <--')\n      yield (src_batch, src_batch_seq_len, trg_batch, trg_mask)\n"""
seq2seq_decode_prediction.py,28,"b'# Created by Albert Aparicio on 7/12/16\n# coding: utf-8\n\n# This script takes a trained model and predicts the test data\n\n# TODO subsitute print calls for logging.info calls when applicable\n# https://docs.python.org/2/howto/logging.html#logging-basic-tutorial\n\n# This import makes Python use \'print\' as in Python 3.x\nfrom __future__ import print_function\n\nimport subprocess\n\nimport h5py\nimport numpy as np\nimport tfglib.seq2seq_datatable as s2s\nimport tfglib.seq2seq_normalize as s2s_norm\nfrom ahoproc_tools import error_metrics\nfrom keras.layers import (Dense, Dropout, Embedding, Input, TimeDistributed,\n                          merge)\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.utils.generic_utils import Progbar\nfrom tfglib.pretrain_data_params import (prepare_pretrain_slice,\n                                         pretrain_load_data_parameters,\n                                         pretrain_save_data_parameters)\n\n############\n# Switches #\n############\npretrain = True\n\n# Decide if datatable/parameters must be built or can be loaded from a file\nbuild_datatable = False\n\n#############\n# Load data #\n#############\nif pretrain:\n  data_path = \'pretrain_data/training_chop\'\n\n  if build_datatable:\n    print(\'Saving pretraining parameters\')\n    (max_test_length,\n     test_speakers_max,\n     test_speakers_min,\n     test_files_list\n     ) = pretrain_save_data_parameters(data_path)\n\n  else:\n    print(\'Loading pretraining parameters\')\n    (max_test_length,\n     test_speakers_max,\n     test_speakers_min,\n     test_files_list\n     ) = pretrain_load_data_parameters(data_path)\n\n  test_speakers = test_speakers_max.shape[0]\n\nelse:\n  print(\'Loading test datatable...\', end=\'\')\n  (src_test_datatable,\n   src_test_masks,\n   trg_test_datatable,\n   trg_test_masks,\n   max_test_length,\n   test_speakers_max,\n   test_speakers_min\n   ) = s2s.seq2seq2_load_datatable(\n      \'data/seq2seq_test_datatable.h5\'\n      )\n\n  test_speakers = test_speakers_max.shape[0]\n  print(\'done\')\n\n#############################\n# Load model and parameters #\n#############################\nmodel_description = \'seq2seq_pretrain_no-frame-noise\'\n\nprint(\'Loading parameters\')\nwith h5py.File(\'training_results/\' + model_description + \'_training_params.h5\',\n               \'r\') as f:\n  params_loss = f.attrs.get(\'params_loss\').decode(\'utf-8\')\n  flags_loss = f.attrs.get(\'flags_loss\').decode(\'utf-8\')\n  optimizer_name = f.attrs.get(\'optimizer\').decode(\'utf-8\')\n  nb_epochs = f.attrs.get(\'epochs\')\n  learning_rate = f.attrs.get(\'learning_rate\')\n  train_speakers_max = f.attrs.get(\'train_speakers_max\')\n  train_speakers_min = f.attrs.get(\'train_speakers_min\')\n\nprint(\'Re-initializing model\')\noutput_dim = 44\ndata_dim = output_dim + 10 + 10\nemb_size = 256\nbatch_size = 1\n\nprediction_epoch = 18\n\n#################\n# Define models #\n#################\n# Encoder model\nmain_input = Input(batch_shape=(batch_size, max_test_length, output_dim),\n                   dtype=\'float32\',\n                   name=\'main_input\')\n\nsrc_spk_input = Input(\n    batch_shape=(batch_size, max_test_length,),\n    dtype=\'int32\',\n    name=\'src_spk_in\'\n    )\ntrg_spk_input = Input(\n    batch_shape=(batch_size, max_test_length,),\n    dtype=\'int32\',\n    name=\'trg_spk_in\'\n    )\n\nembedded_spk_indexes = Embedding(\n    input_dim=test_speakers,\n    output_dim=5,\n    input_length=max_test_length,\n    name=\'spk_index_embedding\'\n    )\n\nmerged_parameters = merge(\n    [main_input,\n     embedded_spk_indexes(src_spk_input),\n     embedded_spk_indexes(trg_spk_input)\n     ],\n    mode=\'concat\',\n    name=\'inputs_merge\'\n    )\n\n# Bidirectional encoder LSTM\nencoder_LSTM_forwards = LSTM(\n    go_backwards=False,\n    output_dim=emb_size,\n    return_sequences=True,\n    consume_less=\'gpu\',\n    name=\'encoder_LSTM_forwards\'\n    )(merged_parameters)\n\nencoder_LSTM_backwards = LSTM(\n    go_backwards=True,\n    output_dim=emb_size,\n    return_sequences=True,\n    consume_less=\'gpu\',\n    name=\'encoder_LSTM_backwards\'\n    )(merged_parameters)\n\nencoder_LSTM_merged = merge(\n    [encoder_LSTM_forwards, encoder_LSTM_backwards],\n    mode=\'sum\',\n    name=\'encoder_bidirectional_merge\'\n    )\n\n# Decoder model\n# decoder_input = Input(batch_shape=(batch_size, max_test_length, emb_size),\ndecoder_input = Input(batch_shape=(batch_size, 1, emb_size),\n                      dtype=\'float32\',\n                      name=\'decoder_input\')\nfeedback_input = Input(batch_shape=(batch_size, 1, output_dim),\n                       name=\'feedback_in\')\ndec_in = merge([decoder_input, feedback_input],\n               mode=\'concat\',\n               name=\'decoder_merge\'\n               )\n\ndecoder_LSTM = LSTM(\n    emb_size,\n    return_sequences=True,\n    consume_less=\'gpu\',\n    stateful=True,\n    name=\'decoder_LSTM\'\n    )(dec_in)\n\ndropout_layer = Dropout(0.5)(decoder_LSTM)\n\nparams_output = LSTM(\n    output_dim - 2,\n    return_sequences=True,\n    consume_less=\'gpu\',\n    activation=\'linear\',\n    stateful=True,\n    name=\'params_output\'\n    )(dropout_layer)\n\nflags_output = TimeDistributed(Dense(\n    2,\n    activation=\'sigmoid\',\n    ), name=\'flags_output\')(dropout_layer)\n\n######################\n# Instantiate models #\n######################\nencoder_model = Model(input=[main_input, src_spk_input, trg_spk_input],\n                      output=encoder_LSTM_merged)\ndecoder_model = Model(input=[decoder_input, feedback_input],\n                      output=[params_output, flags_output])\n\n# Load weights and compile models\nencoder_model.load_weights(\'models/\' + model_description + \'_\' + params_loss +\n                           \'_\' + flags_loss + \'_\' + optimizer_name + \'_epoch_\' +\n                           str(prediction_epoch) + \'_lr_\' + str(learning_rate) +\n                           \'_weights.h5\', by_name=True)\ndecoder_model.load_weights(\'models/\' + model_description + \'_\' + params_loss +\n                           \'_\' + flags_loss + \'_\' + optimizer_name + \'_epoch_\' +\n                           str(prediction_epoch) + \'_lr_\' + str(learning_rate) +\n                           \'_weights.h5\', by_name=True)\n\nadam = Adam(clipnorm=5)\n\nencoder_model.compile(optimizer=adam,\n                      loss={\'encoder_bidirectional_merge\': params_loss},\n                      sample_weight_mode=""temporal""\n                      )\n\ndecoder_model.compile(optimizer=adam,\n                      loss={\'params_output\': params_loss,\n                            \'flags_output\' : flags_loss},\n                      sample_weight_mode=""temporal""\n                      )\n\n##################\n# Load basenames #\n##################\nif pretrain:\n  # Initialize slices generator\n  pretrain_slice = prepare_pretrain_slice(\n      test_files_list,\n      data_path,\n      max_test_length,\n      train_speakers_max,\n      train_speakers_min,\n      shuffle_files=False,\n      basename_len=14,\n      replicate=False\n      )\n\n  # Initialize batch\n  main_input = np.empty((batch_size, max_test_length, 44))\n  src_spk_in = np.empty((batch_size, max_test_length))\n  trg_spk_in = np.empty((batch_size, max_test_length))\n\n  for sequence in test_files_list:\n    print(\'\\n\' + \'Processing \' + sequence)\n\n    # Get sequence parameters (only input parameters \'cos we have test data)\n    (\n      main_input[0, :, :],\n      src_spk_in[0, :],\n      trg_spk_in[0, :],\n      _,\n      _,\n      _,\n      _\n      ) = next(pretrain_slice)\n\n    # Predict parameters\n    # ==================\n\n    # Encoder prediction\n    encoder_prediction = encoder_model.predict_on_batch({\n      \'main_input\': main_input,\n      \'src_spk_in\': src_spk_in,\n      \'trg_spk_in\': trg_spk_in\n      })\n\n    # Prepare data for decoder predictions\n    decoder_prediction = np.empty((batch_size, 0, output_dim))\n    partial_prediction = np.empty((batch_size, 1, output_dim))\n    raw_uv_flags = np.empty((0, 1))\n\n    # Feedback data for first decoder iteration\n    feedback_data = np.zeros((batch_size, 1, output_dim))\n\n    # Loop parameters\n    loop_timesteps = 0\n    EOS = 0\n    max_loop = 1.5 * max_test_length\n\n    # Decoder predictions\n    progress_bar = Progbar(target=max_test_length)\n    progress_bar.update(0)\n\n    # TODO Fix EOS prediction\n    while loop_timesteps < max_test_length:\n      # while EOS < 0.5 or loop_timesteps < max_test_length:\n\n      # Predict each frame separately\n      # for index in range(encoder_prediction.shape[1]):\n      [partial_prediction[:, :, 0:42],\n       partial_prediction[:, :, 42:44]\n       ] = decoder_model.predict_on_batch(\n          {\'decoder_input\': encoder_prediction[:, loop_timesteps, :].\n            reshape(1, -1, emb_size),\n           \'feedback_in\'  : feedback_data}\n          )\n\n      # Unscale partial prediction\n      partial_prediction[\n      :, :, 0:42\n      ] = partial_prediction[:, :, 0:42].reshape(-1, 42) * (\n        train_speakers_max[int(src_spk_in[0, loop_timesteps]), :] -\n        train_speakers_min[int(src_spk_in[0, loop_timesteps]), :]\n      ) + train_speakers_min[int(src_spk_in[0, loop_timesteps]), :]\n\n      # Round U/V flag\n      raw_uv_flags = np.append(raw_uv_flags, partial_prediction[0, 0, 42])\n\n      partial_prediction[:, :, 42] = np.round(\n          partial_prediction[:, :, 42])\n\n      # Apply u/v flags to lf0 and mvf\n      if partial_prediction[:, :, 42] == 0:\n        partial_prediction[:, :, 40] = -1e+10  # lf0\n        partial_prediction[:, :, 41] = 1000  # mvf\n\n      decoder_prediction = np.concatenate(\n          (decoder_prediction, partial_prediction), axis=1)\n\n      # feedback_data = partial_prediction\n      feedback_data = main_input[0, loop_timesteps, :].reshape(1, 1, 44)\n      # feedback_data = np.concatenate((\n      # partial_prediction[:, :, 0:42],\n      # main_input[0, loop_timesteps, 42:44].reshape(1, 1, 2)), axis=2)\n      # feedback_data = np.concatenate((\n      # main_input[0, loop_timesteps, 0:42].reshape(1, 1, 42),\n      # partial_prediction[:, :, 42:44]), axis=2)\n\n      EOS = decoder_prediction[:, loop_timesteps, 43]\n      loop_timesteps += 1\n\n      progress_bar.update(loop_timesteps)\n\n    # There is no need to un-zero-pad, since the while loop stops\n    # when an EOS flag is found\n\n    # Reshape prediction into 2D matrix\n    decoder_prediction = decoder_prediction.reshape(-1, output_dim)\n    raw_uv_flags = raw_uv_flags.reshape(-1, 1)\n\n    # Save parameters to separate files #\n    # Create destination directory before saving data\n    predicted_path = \'predicted_\' + sequence\n\n    bashCommand = (\'mkdir -p \' + predicted_path[:-14])\n    process = subprocess.Popen(\n        bashCommand.split(),\n        stdout=subprocess.PIPE\n        )\n    output, error = process.communicate()\n\n    np.savetxt(\n        predicted_path + \'.vf.dat\',\n        decoder_prediction[:, 41]\n        )\n    np.savetxt(\n        predicted_path + \'.lf0.dat\',\n        decoder_prediction[:, 40]\n        )\n    np.savetxt(\n        predicted_path + \'.mcp.dat\',\n        decoder_prediction[:, 0:40],\n        delimiter=\'\\t\'\n        )\n    np.savetxt(\n        predicted_path + \'.uv.dat\',\n        raw_uv_flags\n        )\n\n    # # Display MCD\n    # print(\'\\n\'+\'MCD = \' + str(error_metrics.MCD(\n    # main_input[0, :, 0:40] *\n    # (train_speakers_max[int(src_spk_in[0, 0]), 0:40] -\n    # train_speakers_min[int(src_spk_in[0, 0]), 0:40]\n    # ) + (train_speakers_min[int(src_spk_in[0, 0]), 0:40]),\n    # decoder_prediction[:, 0:40]\n    # )))\n\nelse:\n  basenames_file = open(\'data/test/seq2seq_basenames.list\', \'r\')\n  basenames_lines = basenames_file.readlines()\n  # Strip \'\\n\' characters\n  basenames = [line.split(\'\\n\')[0] for line in basenames_lines]\n\n  # Load speakers\n  speakers_file = open(\'data/test/speakers.list\', \'r\')\n  speakers_lines = speakers_file.readlines()\n  # Strip \'\\n\' characters\n  speakers = [line.split(\'\\n\')[0] for line in speakers_lines]\n\n  #######################\n  # Loop over sequences #\n  #######################\n  print(\'Predicting sequences\')\n  assert len(basenames) == src_test_datatable.shape[0] / np.square(\n      len(speakers))\n\n  src_spk_ind = 0\n  trg_spk_ind = 0\n\n  for src_spk in speakers:\n    for trg_spk in speakers:\n      # for i in range(src_test_datatable.shape[0]):\n      for i in range(len(basenames)):\n        print(src_spk + \'->\' + trg_spk + \' \' + basenames[i])\n\n        ##################\n        # Normalize data #\n        ##################\n        src_test_datatable[i, :, 0:42] = s2s_norm.maxmin_scaling(\n            src_test_datatable[i, :, :],\n            src_test_masks[i, :],\n            trg_test_datatable[i, :, :],\n            trg_test_masks[i, :],\n            train_speakers_max,\n            train_speakers_min\n            )[0]\n\n        ######################\n        # Predict parameters #\n        ######################\n        # Initialize encoder prediction data\n        # ==================================\n        it_sequence = src_test_datatable[i, :, :]\n        src_batch = it_sequence.reshape(batch_size, -1,\n                                        it_sequence.shape[1])\n\n        # Encoder prediction\n        # ==================\n        encoder_prediction = encoder_model.predict_on_batch(src_batch)\n\n        # Prepare data for decoder predictions\n        # ====================================\n        decoder_prediction = np.empty((batch_size, 0, output_dim))\n        partial_prediction = np.empty((batch_size, 1, output_dim))\n        raw_uv_flags = np.empty((0, 1))\n\n        # Feedback data for first decoder iteration\n        feedback_data = np.zeros((batch_size, 1, output_dim))\n\n        # Loop parameters\n        loop_timesteps = 0\n        EOS = 0\n        max_loop = 1.5 * max_test_length\n\n        # Decoder predictions\n        # TODO Fix EOS prediction\n        while loop_timesteps < max_test_length:\n          # while EOS < 0.5 and loop_timesteps < max_loop:\n          # print(loop_timesteps)\n\n          [partial_prediction[:, :, 0:42],\n           partial_prediction[:, :, 42:44]\n           ] = decoder_model.predict_on_batch(\n              {\'decoder_input\': encoder_prediction.reshape(1, -1,\n                                                           emb_size),\n               \'feedback_in\'  : feedback_data}\n              )\n\n          decoder_prediction = np.concatenate(\n              (decoder_prediction, partial_prediction),\n              axis=1\n              )\n\n          # Unscale parameters\n          decoder_prediction[\n          :, loop_timesteps, 0:42\n          ] = s2s_norm.unscale_prediction(\n              src_test_datatable[i, :, :],\n              src_test_masks[i, :],\n              decoder_prediction[:, loop_timesteps, 0:42].reshape(-1,\n                                                                  42),\n              train_speakers_max,\n              train_speakers_min\n              )\n\n          ###################\n          # Round u/v flags #\n          ###################\n          raw_uv_flags = np.append(\n              raw_uv_flags,\n              [decoder_prediction[:, loop_timesteps, 42]],\n              axis=0\n              )\n\n          decoder_prediction[:, loop_timesteps, 42] = np.round(\n              decoder_prediction[:, loop_timesteps, 42])\n\n          # Apply u/v flags to lf0 and mvf\n          # for index, entry in enumerate(prediction[:, 42]):\n          #     if entry == 0:\n          if decoder_prediction[:, loop_timesteps, 42] == 0:\n            decoder_prediction[:, loop_timesteps, 40] = -1e10  # lf0\n            decoder_prediction[:, loop_timesteps, 41] = 1000  # mvf\n\n          #############################################\n          # Concatenate prediction with feedback data #\n          #############################################\n          feedback_data = decoder_prediction[\n                          :, loop_timesteps, :\n                          ].reshape(1, -1, output_dim)\n\n          EOS = decoder_prediction[:, loop_timesteps, 43]\n          loop_timesteps += 1\n\n        # There is no need to un-zero-pad, since the while loop stops\n        # when an EOS flag is found\n\n        # Reshape prediction into 2D matrix\n        decoder_prediction = decoder_prediction.reshape(-1, output_dim)\n        raw_uv_flags = raw_uv_flags.reshape(-1, 1)\n\n        #####################################\n        # Save parameters to separate files #\n        #####################################\n        # Create destination directory before saving data\n        bashCommand = (\'mkdir -p data/test/s2s_predicted/\' +\n                       src_spk + \'-\' + trg_spk + \'/\')\n        process = subprocess.Popen(\n            bashCommand.split(),\n            stdout=subprocess.PIPE\n            )\n        output, error = process.communicate()\n\n        np.savetxt(\n            \'data/test/s2s_predicted/\' + src_spk + \'-\' + trg_spk + \'/\' +\n            basenames[i] + \'.vf.dat\',\n            decoder_prediction[:, 41]\n            )\n        np.savetxt(\n            \'data/test/s2s_predicted/\' + src_spk + \'-\' + trg_spk + \'/\' +\n            basenames[i] + \'.lf0.dat\',\n            decoder_prediction[:, 40]\n            )\n        np.savetxt(\n            \'data/test/s2s_predicted/\' + src_spk + \'-\' + trg_spk + \'/\' +\n            basenames[i] + \'.mcp.dat\',\n            decoder_prediction[:, 0:40],\n            delimiter=\'\\t\'\n            )\n        np.savetxt(\n            \'data/test/s2s_predicted/\' + src_spk + \'-\' + trg_spk + \'/\' +\n            basenames[i] + \'.uv.dat\',\n            raw_uv_flags\n            )\n\n        # Display MCD\n        print(\'MCD = \' + str(error_metrics.MCD(\n            trg_test_datatable[\n            i + (src_spk_ind + trg_spk_ind) * len(basenames),\n            0:int(sum(trg_test_masks[\n                      i + (src_spk_ind + trg_spk_ind) * len(basenames),\n                      :\n                      ])),\n            0:40\n            ],\n            decoder_prediction[\n            0:int(sum(trg_test_masks[\n                      i + (src_spk_ind + trg_spk_ind) * len(basenames),\n                      :\n                      ])),\n            0:40\n            ])))\n\n      trg_spk_ind += 1\n    src_spk_ind += 1\n'"
seq2seq_histograms.py,8,"b'# Created by albert aparicio on 12/12/16\n# coding: utf-8\n\n# This script computes histograms of predicted parameters to assess the\n# performance of the model\'s training\n\n# This import makes Python use \'print\' as in Python 3.x\nfrom __future__ import print_function\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tfglib.seq2seq_datatable as s2s\nimport tfglib.seq2seq_normalize as s2s_norm\nfrom keras.layers import GRU, Dropout\nfrom keras.layers.core import RepeatVector\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.utils.generic_utils import Progbar\n\n#######################\n# Sizes and constants #\n#######################\nnb_sequences = 32\n\n######################\n# Load test database #\n######################\nprint(\'Loading test datatable...\', end=\'\')\n(src_test_datatable,\n src_test_masks,\n trg_test_datatable,\n trg_test_masks,\n max_test_length,\n test_speakers_max,\n test_speakers_min\n ) = s2s.seq2seq2_load_datatable(\n    \'data/seq2seq_test_datatable.h5\'\n)\nprint(\'done\')\n\n#############################\n# Load model and parameters #\n#############################\nwith h5py.File(\'training_results/seq2seq_training_params.h5\', \'r\') as f:\n    epochs = f.attrs.get(\'epochs\')\n    learning_rate = f.attrs.get(\'learning_rate\')\n    optimizer = f.attrs.get(\'optimizer\')\n    loss = f.attrs.get(\'loss\')\n    train_speakers_max = f.attrs.get(\'train_speakers_max\')\n    train_speakers_min = f.attrs.get(\'train_speakers_min\')\n\nprint(\'Re-initializing model\')\nseq2seq_model = Sequential()\n\n# Encoder Layer\nseq2seq_model.add(GRU(100,\n                      input_dim=44 + 10 + 10,\n                      return_sequences=False,\n                      consume_less=\'gpu\'\n                      ))\nseq2seq_model.add(RepeatVector(max_test_length))\n\n# Decoder layer\nseq2seq_model.add(GRU(100, return_sequences=True, consume_less=\'gpu\'))\nseq2seq_model.add(Dropout(0.5))\nseq2seq_model.add(GRU(\n    44,\n    return_sequences=True,\n    consume_less=\'gpu\',\n    activation=\'linear\'\n))\n\nadam = Adam(clipnorm=10)\nseq2seq_model.compile(loss=loss.decode(\'utf-8\'), optimizer=adam,\n                      sample_weight_mode=""temporal"")\nseq2seq_model.load_weights(\'models/seq2seq_\' + loss.decode(\'utf-8\') + \'_\' +\n                           optimizer.decode(\'utf-8\') + \'_epochs_\' +\n                           str(epochs) + \'_lr_\' + str(learning_rate) +\n                           \'_weights.h5\')\n\n##############################\n# Predict sequences in batch #\n##############################\n# Pre-allocate prediction results\npredictions = np.zeros(\n    (nb_sequences, trg_test_datatable.shape[1], trg_test_datatable.shape[2]))\n\nfor i in range(nb_sequences):\n    # Normalize sequence\n    src_test_datatable[i, :, 0:42] = s2s_norm.maxmin_scaling(\n        src_test_datatable[i, :, :],\n        src_test_masks[i, :],\n        trg_test_datatable[i, :, :],\n        trg_test_masks[i, :],\n        train_speakers_max,\n        train_speakers_min\n    )[0]\n\n    # Mask sequence\n    masked_sequence = s2s_norm.mask_data(\n        src_test_datatable[i, :, :],\n        src_test_masks[i, :]\n    )\n\n    # Get only valid data\n    valid_sequence = masked_sequence[~masked_sequence.mask].reshape(\n        (1,\n         -1,\n         masked_sequence.shape[1])\n    )\n    # Predict parameters\n    prediction = seq2seq_model.predict(valid_sequence)\n\n    # Unscale parameters\n    prediction[:, :, 0:42] = s2s_norm.unscale_prediction(\n        src_test_datatable[i, :, :],\n        src_test_masks[i, :],\n        prediction[:, :, 0:42].reshape(-1, 42),\n        train_speakers_max,\n        train_speakers_min\n    )\n\n    # Reshape prediction into 2D matrix\n    prediction = prediction.reshape(-1, 44)\n\n    # Round u/v flags #\n    prediction[:, 42] = np.round(prediction[:, 42])\n\n    # Apply u/v flags to lf0 and mvf #\n    for index, entry in enumerate(prediction[:, 42]):\n        if entry == 0:\n            prediction[index, 40] = -1e+10  # lf0\n            prediction[index, 41] = 0  # mvf\n\n    # Save prediction\n    predictions[i] = prediction\n\n###################\n# Plot histograms #\n###################\nprint(\'Computing prediction histograms...\')\nprogress_bar = Progbar(target=trg_test_datatable.shape[2])\n\nprogress_bar.update(0)\n# Predictions histograms\nfor param_index in range(trg_test_datatable.shape[2]):\n    # Compute histogram\n    hist, bins = np.histogram(predictions[:, :, param_index], bins=20)\n    width = 0.7 * (bins[1] - bins[0])\n    center = (bins[:-1] + bins[1:]) / 2\n    plt.bar(center, hist, align=\'center\', width=width)\n\n    # Plot vertical line at mean\n    mean = np.mean(predictions[:, :, param_index])\n    plt.plot((mean, mean), (0, 1.1 * np.max(hist)), \'r\', linewidth=2)\n\n    # Save histogram\n    plt.savefig(\'training_results/hist/seq2seq_\' + loss.decode(\'utf-8\') + \'_\' +\n                optimizer.decode(\'utf-8\') + \'_epochs_\' + str(epochs) + \'_lr_\' +\n                str(learning_rate) + \'_pred_param_\' + str(\n        param_index) + \'_hist.png\',\n                bbox_inches=\'tight\')\n    plt.close()\n\n    progress_bar.update(param_index + 1)\n\nprint(\'\\n\' + \'Computing ground truth histograms...\')\nprogress_bar = Progbar(target=trg_test_datatable.shape[2])\n\nprogress_bar.update(0)\n# Groundtruth histograms\nfor param_index in range(trg_test_datatable.shape[2]):\n    # Compute histogram\n    hist, bins = np.histogram(trg_test_datatable[:, :, param_index], bins=20)\n    width = 0.7 * (bins[1] - bins[0])\n    center = (bins[:-1] + bins[1:]) / 2\n    plt.bar(center, hist, align=\'center\', width=width)\n\n    # Plot vertical line at mean\n    mean = np.mean(trg_test_datatable[:, :, param_index])\n    plt.plot((mean, mean), (0, 1.1 * np.max(hist)), \'r\', linewidth=2)\n\n    # Save histogram\n    plt.savefig(\'training_results/hist/seq2seq_\' + loss.decode(\'utf-8\') + \'_\' +\n                optimizer.decode(\'utf-8\') + \'_epochs_\' + str(epochs) + \'_lr_\' +\n                str(learning_rate) + \'_gtrth_param_\' + str(\n        param_index) + \'_hist.png\',\n                bbox_inches=\'tight\')\n    plt.close()\n\n    progress_bar.update(param_index + 1)\n\nprint(\'\\n\' + \'========================\' +\n      \'\\n\' + \'======= FINISHED =======\' +\n      \'\\n\' + \'========================\')\n\nexit()\n'"
seq2seq_plot_curves.py,14,"b'# Created by Albert Aparicio on 6/12/16\n# coding: utf-8\n\n# This script takes the results of a training and plots its loss curves\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmodel_description = \'seq2seq_pretrain\'\n\nwith h5py.File(\'training_results/\' + model_description + \'_training_params.h5\',\n               \'r\') as f:\n    params_loss = f.attrs.get(\'params_loss\').decode(\'utf-8\')\n    flags_loss = f.attrs.get(\'flags_loss\').decode(\'utf-8\')\n    optimizer_name = f.attrs.get(\'optimizer\').decode(\'utf-8\')\n    nb_epochs = f.attrs.get(\'epochs\')\n    learning_rate = f.attrs.get(\'learning_rate\')\n    metrics_names = [name.decode(\'utf-8\') for name in\n                     f.attrs.get(\'metrics_names\')]\n\n    f.close()\n\nepoch = np.loadtxt(\'training_results/\' + model_description + \'_\' + params_loss\n                   + \'_\' + flags_loss + \'_\' + optimizer_name + \'_epochs_\' +\n                   str(nb_epochs) + \'_lr_\' + str(learning_rate) + \'_epochs.csv\',\n                   delimiter=\',\')\nlosses = np.loadtxt(\'training_results/\' + model_description + \'_\' + params_loss\n                    + \'_\' + flags_loss + \'_\' + optimizer_name + \'_epochs_\' +\n                    str(nb_epochs) + \'_lr_\' + str(learning_rate) +\n                    \'_loss.csv\', delimiter=\',\')\nval_losses = np.loadtxt(\n    \'training_results/\' + model_description + \'_\' + params_loss + \'_\' +\n    flags_loss + \'_\' + optimizer_name + \'_epochs_\' + str(nb_epochs) + \'_lr_\' +\n    str(learning_rate) + \'_val_loss.csv\', delimiter=\',\')\nmcd = np.loadtxt(\n    \'training_results/\' + model_description + \'_\' + params_loss + \'_\' +\n    flags_loss + \'_\' + optimizer_name + \'_epochs_\' + str(nb_epochs) + \'_lr_\' +\n    str(learning_rate) + \'_mcd.csv\', delimiter=\',\')\nrmse = np.loadtxt(\n    \'training_results/\' + model_description + \'_\' + params_loss + \'_\' +\n    flags_loss + \'_\' + optimizer_name + \'_epochs_\' + str(nb_epochs) + \'_lr_\' +\n    str(learning_rate) + \'_rmse.csv\', delimiter=\',\')\nacc = np.loadtxt(\n    \'training_results/\' + model_description + \'_\' + params_loss + \'_\' +\n    flags_loss + \'_\' + optimizer_name + \'_epochs_\' + str(nb_epochs) + \'_lr_\' +\n    str(learning_rate) + \'_acc.csv\', delimiter=\',\')\n\nassert (val_losses.size == losses.size)\n\n# ##############################################\n# # TODO Comment after dev\n# metrics_names = [\'loss\', \'params_output_loss\', \'flags_output_loss\']\n#\n# ##############################################\n\n# Losses plot\nh1 = plt.figure(figsize=(14, 8))\nax1 = h1.add_subplot(111)\n\nplt.plot(epoch, losses, epoch, val_losses, \'--\', linewidth=2)\n\n# Prepare legend\nlegend_list = list(metrics_names)  # We use list() to make a copy\n\nfor name in metrics_names:\n    legend_list.append(\'val_\' + name)\n\nplt.legend(legend_list, loc=\'best\')\n\nplt.suptitle(\'Parameters loss: \' + params_loss + \', Flags loss: \' + flags_loss +\n             \', Optimizer: \' + optimizer_name + \', Epochs: \' + str(nb_epochs) +\n             \', Learning rate: \' + str(learning_rate))\n\nax1.set_xlabel(\'Epochs\')\nax1.set_ylabel(\'Loss values\')\n\nax1.set_xlim(0, 19)\nmajor_xticks = np.arange(0, 20, 1)\nax1.set_xticks(major_xticks)\nax1.tick_params(which=\'both\', direction =\'out\')\n\nax1.grid(which=\'both\', ls=\'-\')\n\nplt.savefig(\'training_results/\' + model_description + \'_\' + params_loss + \'_\' +\n            flags_loss + \'_\' + optimizer_name + \'_epochs_\' +\n            str(nb_epochs) + \'_lr_\' + str(learning_rate) + \'_graph.eps\',\n            bbox_inches=\'tight\')\n# plt.show()\nplt.close(h1)\n\n# Metrics plot\nh2 = plt.figure(figsize=(10, 5))\nax2 = h2.add_subplot(111)\n\nplt.plot(epoch, mcd)  # , epoch, rmse, epoch, acc)\nplt.legend([\'MCD (dB)\'], loc=\'best\')\n# , \'RMSE\', \'Accuracy\'\nplt.suptitle(""Cepstral features\' MCD"", fontsize = 12)\n# , RMSE and ACC\nax2.set_xlabel(\'Epochs\')\nax2.set_ylabel(\'MCD (dB)\')\n\nax2.set_xlim(0, 19)\nmajor_xticks = np.arange(0, 20, 1)\n\nmajor_yticks = np.arange(np.floor(np.min(mcd)), np.ceil(np.max(mcd)), 0.2)\n\nax2.set_xticks(major_xticks)\nax2.set_yticks(major_yticks)\n\nax2.tick_params(which=\'both\', direction =\'out\')\n\nax2.grid(which=\'both\', ls=\'-\')\nplt.savefig(\'training_results/\' + model_description + \'_\' + params_loss + \'_\' +\n            flags_loss + \'_\' + optimizer_name + \'_epochs_\' +\n            str(nb_epochs) + \'_lr_\' + str(learning_rate) + \'_mcd.eps\',\n            bbox_inches=\'tight\')\n\nplt.close(h2)\n\nh2 = plt.figure(figsize=(10, 5))\nax2 = h2.add_subplot(111)\n\nplt.plot(epoch, rmse)\nplt.legend([\'RMSE\'], loc=\'best\')\n# , \'RMSE\', \'Accuracy\'\nplt.suptitle(""Pitch Root Mean Square Error (RMSE)"", fontsize=12)\n# , RMSE and ACC\nax2.set_xlabel(\'Epochs\')\nax2.set_ylabel(\'Root Mean Square Error (RMSE)\')\n\nax2.set_xlim(0, 19)\n\nmajor_xticks = np.arange(0, 20, 1)\n\nmajor_yticks = np.arange(0, np.ceil(np.max(rmse*100))/100, 0.01)\n\nax2.set_xticks(major_xticks)\nax2.set_yticks(major_yticks)\n\nax2.tick_params(which=\'both\', direction =\'out\')\n\nax2.grid(which=\'both\', ls=\'-\')\nplt.savefig(\'training_results/\' + model_description + \'_\' + params_loss + \'_\' +\n            flags_loss + \'_\' + optimizer_name + \'_epochs_\' +\n            str(nb_epochs) + \'_lr_\' + str(learning_rate) + \'_rmse.eps\',\n            bbox_inches=\'tight\')\nplt.close(h2)\n\nh2 = plt.figure(figsize=(10, 5))\nax2 = h2.add_subplot(111)\n\nplt.plot(epoch, acc)\nplt.legend([\'Accuracy\'], loc=\'best\')\nplt.suptitle(""U/V Flag Accuracy"", fontsize=12)\n# , RMSE and ACC\nax2.set_xlabel(\'Epochs\')\nax2.set_ylabel(\'Accuracy\')\n\nax2.set_xlim(0, 19)\n\nmajor_xticks = np.arange(0, 20, 1)\n\nmajor_yticks = np.arange(\n    np.floor(np.min(acc*100))/100,\n    1.005,\n    0.005\n)\n\nax2.set_xticks(major_xticks)\nax2.set_yticks(major_yticks)\n\nax2.tick_params(which=\'both\', direction =\'out\')\n\nax2.grid(which=\'both\', ls=\'-\')\nplt.savefig(\'training_results/\' + model_description + \'_\' + params_loss + \'_\' +\n            flags_loss + \'_\' + optimizer_name + \'_epochs_\' +\n            str(nb_epochs) + \'_lr_\' + str(learning_rate) + \'_acc.eps\',\n            bbox_inches=\'tight\')\nplt.close(h2)\nexit()\n'"
seq2seq_pytorch_main.py,13,"b'# -*- coding: utf-8 -*-\n# TODO Add argparser\n""""""\nTranslation with a Sequence to Sequence Network and Attention\n*************************************************************\n**Author**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n\nIn this project we will be teaching a neural network to translate from\nFrench to English.\n\n::\n\n    [KEY: > input, = target, < output]\n\n    > il est en train de peindre un tableau .\n    = he is painting a picture .\n    < he is painting a picture .\n\n    > pourquoi ne pas essayer ce vin delicieux ?\n    = why not try that delicious wine ?\n    < why not try that delicious wine ?\n\n    > elle n est pas poete mais romanciere .\n    = she is not a poet but a novelist .\n    < she not not a poet but a novelist .\n\n    > vous etes trop maigre .\n    = you re too skinny .\n    < you re all alone .\n\n... to varying degrees of success.\n\nThis is made possible by the simple but powerful idea of the `sequence\nto sequence network <http://arxiv.org/abs/1409.3215>`__, in which two\nrecurrent neural networks work together to transform one sequence to\nanother. An encoder network condenses an input sequence into a vector,\nand a decoder network unfolds that vector into a new sequence.\n\n.. figure:: /_static/img/seq-seq-images/seq2seq.png\n   :alt: \n\nTo improve upon this model we\'ll use an `attention\nmechanism <https://arxiv.org/abs/1409.0473>`__, which lets the decoder\nlearn to focus over a specific range of the input sequence.\n\n**Recommended Reading:**\n\nI assume you have at least installed PyTorch, know Python, and\nunderstand Tensors:\n\n-  http://pytorch.org/ For installation instructions\n-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in \ngeneral\n-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview\n-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user\n\n\nIt would also be useful to know about Sequence to Sequence networks and\nhow they work:\n\n-  `Learning Phrase Representations using RNN Encoder-Decoder for\n   Statistical Machine Translation <http://arxiv.org/abs/1406.1078>`__\n-  `Sequence to Sequence Learning with Neural\n   Networks <http://arxiv.org/abs/1409.3215>`__\n-  `Neural Machine Translation by Jointly Learning to Align and\n   Translate <https://arxiv.org/abs/1409.0473>`__\n-  `A Neural Conversational Model <http://arxiv.org/abs/1506.05869>`__\n\nYou will also find the previous tutorials on\n:doc:`/intermediate/char_rnn_classification_tutorial`\nand :doc:`/intermediate/char_rnn_generation_tutorial`\nhelpful as those concepts are very similar to the Encoder and Decoder\nmodels, respectively.\n\nAnd for more, read the papers that introduced these topics:\n\n-  `Learning Phrase Representations using RNN Encoder-Decoder for\n   Statistical Machine Translation <http://arxiv.org/abs/1406.1078>`__\n-  `Sequence to Sequence Learning with Neural\n   Networks <http://arxiv.org/abs/1409.3215>`__\n-  `Neural Machine Translation by Jointly Learning to Align and\n   Translate <https://arxiv.org/abs/1409.0473>`__\n-  `A Neural Conversational Model <http://arxiv.org/abs/1506.05869>`__\n\n\n**Requirements**\n""""""\nfrom __future__ import division, print_function, unicode_literals\n\nimport argparse\nimport glob\nimport gzip\nimport os\nimport random\nfrom sys import version_info\n\nimport h5py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom ahoproc_tools import error_metrics\nfrom tfglib.seq2seq_normalize import mask_data\nfrom tfglib.utils import init_logger\nfrom torch import optim\nfrom torch.autograd import Variable\n\nfrom seq2seq_dataloader import DataLoader\nfrom seq2seq_pytorch_model import AttnDecoderRNN, EncoderRNN\n\nuse_cuda = torch.cuda.is_available()\n\n# Conditional imports\nif version_info.major > 2:\n  import pickle\nelse:\n  import cPickle as pickle\n\nlogger, opts = None, None\n\nif __name__ == \'__main__\':\n  # logger.debug(\'Before parsing args\')\n  parser = argparse.ArgumentParser(\n      description=""Convert voice signal with seq2seq model"")\n  parser.add_argument(\'--train_data_path\', type=str,\n                      default=""tcstar_data_trim/training/"")\n  parser.add_argument(\'--train_out_file\', type=str,\n                      default=""tcstar_data_trim/seq2seq_train_datatable"")\n  parser.add_argument(\'--test_data_path\', type=str,\n                      default=""tcstar_data_trim/test/"")\n  parser.add_argument(\'--test_out_file\', type=str,\n                      default=""tcstar_data_trim/seq2seq_test_datatable"")\n  parser.add_argument(\'--val_fraction\', type=float, default=0.25)\n  parser.add_argument(\'--save-h5\', dest=\'save_h5\', action=\'store_true\',\n                      help=\'Save dataset to .h5 file\')\n  parser.add_argument(\'--max_seq_length\', type=int, default=500)\n  parser.add_argument(\'--params_len\', type=int, default=44)\n  # parser.add_argument(\'--patience\', type=int, default=4,\n  #                     help=""Patience epochs to do validation, if validation ""\n  #                          ""score is worse than train for patience epochs ""\n  #                          "", quit training. (Def: 4)."")\n  # parser.add_argument(\'--enc_rnn_layers\', type=int, default=1)\n  # parser.add_argument(\'--dec_rnn_layers\', type=int, default=1)\n  parser.add_argument(\'--hidden_size\', type=int, default=256)\n  # parser.add_argument(\'--cell_type\', type=str, default=""lstm"")\n  parser.add_argument(\'--batch_size\', type=int, default=10)\n  parser.add_argument(\'--epoch\', type=int, default=50)\n  parser.add_argument(\'--learning_rate\', type=float, default=0.0005)\n  # parser.add_argument(\'--dropout\', type=float, default=0)\n  parser.add_argument(\'--teacher_forcing_ratio\', type=float, default=1)\n  parser.add_argument(\'--SOS_token\', type=int, default=0)\n\n  # parser.add_argument(\'--optimizer\', type=str, default=""adam"")\n  # parser.add_argument(\'--clip_norm\', type=float, default=5)\n  # parser.add_argument(\'--attn_length\', type=int, default=500)\n  # parser.add_argument(\'--attn_size\', type=int, default=256)\n  # parser.add_argument(\'--save_every\', type=int, default=100)\n  parser.add_argument(\'--no-train\', dest=\'do_train\',\n                      action=\'store_false\', help=\'Flag to train or not.\')\n  parser.add_argument(\'--no-test\', dest=\'do_test\',\n                      action=\'store_false\', help=\'Flag to test or not.\')\n  parser.add_argument(\'--save_path\', type=str, default=""training_results"")\n  parser.add_argument(\'--pred_path\', type=str, default=""torch_predicted"")\n  # parser.add_argument(\'--tb_path\', type=str, default="""")\n  parser.add_argument(\'--log\', type=str, default=""INFO"")\n  parser.add_argument(\'--load_model\', dest=\'load_model\', action=\'store_true\',\n                      help=\'Load previous model before training\')\n  parser.add_argument(\'--server\', dest=\'server\', action=\'store_true\',\n                      help=\'Commands to be run or not run if we are running \'\n                           \'on server\')\n\n  parser.set_defaults(do_train=True, do_test=True, save_h5=False,\n                      server=False)  # ,\n  # load_model=False)\n  opts = parser.parse_args()\n\n  # Initialize logger\n  logger_level = opts.log\n  logger = init_logger(name=__name__, level=opts.log)\n\n  logger.debug(\'Parsed arguments\')\n  if not os.path.exists(os.path.join(opts.save_path, \'torch_train\')):\n    os.makedirs(os.path.join(opts.save_path, \'torch_train\'))\n  # save config\n  with gzip.open(os.path.join(opts.save_path, \'torch_train\', \'config.pkl.gz\'),\n                 \'wb\') as cf:\n    pickle.dump(opts, cf)\n\n\ndef main(args):\n  logger.debug(\'Main\')\n\n  # If-else for training and testing\n  if args.do_train:\n    dl = DataLoader(args, logger_level=args.log,\n                    max_seq_length=args.max_seq_length)\n\n    encoder1 = EncoderRNN(args.params_len, args.hidden_size, args.batch_size)\n    attn_decoder1 = AttnDecoderRNN(args.hidden_size, args.params_len,\n                                   batch_size=args.batch_size, n_layers=1,\n                                   max_length=args.max_seq_length,\n                                   dropout_p=0.1)\n\n    if use_cuda:\n      encoder1 = encoder1.cuda()\n      attn_decoder1 = attn_decoder1.cuda()\n\n    trained_encoder, trained_decoder = train_epochs(dl, encoder1, attn_decoder1)\n\n  if args.do_test:\n    # TODO What do we do for testing?\n    # pass\n    dl = DataLoader(args, logger_level=args.log, test=True,\n                    max_seq_length=args.max_seq_length)\n\n    if args.load_model:\n      encoder = EncoderRNN(args.params_len, args.hidden_size, args.batch_size)\n      decoder = AttnDecoderRNN(args.hidden_size, args.params_len,\n                               batch_size=args.batch_size, n_layers=1,\n                               max_length=args.max_seq_length,\n                               dropout_p=0.1)\n      if use_cuda:\n        encoder = encoder.cuda()\n        decoder = decoder.cuda()\n\n    else:\n      encoder = trained_encoder\n      decoder = trained_decoder\n\n    test(encoder, decoder, dl)\n\n\n######################################################################\n# Loading data files\n# ==================\n#\n# The data for this project is a set of many thousands of English to\n# French translation pairs.\n#\n# `This question on Open Data Stack\n# Exchange <http://opendata.stackexchange.com/questions/3888/dataset-of\n# -sentences-translated-into-many-languages>`__\n# pointed me to the open translation site http://tatoeba.org/ which has\n# downloads available at http://tatoeba.org/eng/downloads - and better\n# yet, someone did the extra work of splitting language pairs into\n# individual text files here: http://www.manythings.org/anki/\n#\n# The English to French pairs are too big to include in the repo, so\n# download to ``data/eng-fra.txt`` before continuing. The file is a tab\n# separated list of translation pairs:\n#\n# ::\n#\n#     I am cold.    Je suis froid.\n#\n# .. Note::\n#    Download the data from\n#    `here <https://download.pytorch.org/tutorial/data.zip>`_\n#    and extract it to the current directory.\n\n######################################################################\n# Similar to the character encoding used in the character-level RNN\n# tutorials, we will be representing each word in a language as a one-hot\n# vector, or giant vector of zeros except for a single one (at the index\n# of the word). Compared to the dozens of characters that might exist in a\n# language, there are many many more words, so the encoding vector is much\n# larger. We will however cheat a bit and trim the data to only use a few\n# thousand words per language.\n#\n# .. figure:: /_static/img/seq-seq-images/word-encoding.png\n#    :alt:\n#\n#\n\n\n######################################################################\n# We\'ll need a unique index per word to use as the inputs and targets of\n# the networks later. To keep track of all this we will use a helper class\n# called ``Lang`` which has word \xe2\x86\x92 index (``word2index``) and index \xe2\x86\x92 word\n# (``index2word``) dictionaries, as well as a count of each word\n# ``word2count`` to use to later replace rare words.\n#\n\n# EOS_token = 1\n#\n#\n# class Lang:\n#   def __init__(self, name):\n#     self.name = name\n#     self.word2index = {}\n#     self.word2count = {}\n#     self.index2word = {0: ""SOS"", 1: ""EOS""}\n#     self.n_words = 2  # Count SOS and EOS\n#\n#   def add_sentence(self, sentence):\n#     for word in sentence.split(\' \'):\n#       self.add_word(word)\n#\n#   def add_word(self, word):\n#     if word not in self.word2index:\n#       self.word2index[word] = self.n_words\n#       self.word2count[word] = 1\n#       self.index2word[self.n_words] = word\n#       self.n_words += 1\n#     else:\n#       self.word2count[word] += 1\n#\n#\n# ######################################################################\n# # The files are all in Unicode, to simplify we will turn Unicode\n# # characters to ASCII, make everything lowercase, and trim most\n# # punctuation.\n# #\n#\n# # Turn a Unicode string to plain ASCII, thanks to\n# # http://stackoverflow.com/a/518232/2809427\n# def unicode_to_ascii(s):\n#   return \'\'.join(\n#       c for c in unicodedata.normalize(\'NFD\', s)\n#       if unicodedata.category(c) != \'Mn\'\n#       )\n#\n#\n# # Lowercase, trim, and remove non-letter characters\n# def normalize_string(s):\n#   s = unicode_to_ascii(s.lower().strip())\n#   s = re.sub(r""([.!?])"", r"" \\1"", s)\n#   s = re.sub(r""[^a-zA-Z.!?]+"", r"" "", s)\n#   return s\n#\n#\n# ######################################################################\n# # To read the data file we will split the file into lines, and then split\n# # lines into pairs. The files are all English \xe2\x86\x92 Other Language, so if we\n# # want to translate from Other Language \xe2\x86\x92 English I added the ``reverse``\n# # flag to reverse the pairs.\n# #\n#\n# def read_langs(lang1, lang2, reverse=False):\n#   print(""Reading lines..."")\n#\n#   # Read the file and split into lines\n#   lines = open(\'data/%s-%s.txt\' % (lang1, lang2), encoding=\'utf-8\'). \\\n#     read().strip().split(\'\\n\')\n#\n#   # Split every line into pairs and normalize\n#   pairs = [[normalize_string(s) for s in l.split(\'\\t\')] for l in lines]\n#\n#   # Reverse pairs, make Lang instances\n#   if reverse:\n#     pairs = [list(reversed(p)) for p in pairs]\n#     input_lang = Lang(lang2)\n#     output_lang = Lang(lang1)\n#   else:\n#     input_lang = Lang(lang1)\n#     output_lang = Lang(lang2)\n#\n#   return input_lang, output_lang, pairs\n\n\n######################################################################\n# Since there are a *lot* of example sentences and we want to train\n# something quickly, we\'ll trim the data set to only relatively short and\n# simple sentences. Here the maximum length is 10 words (that includes\n# ending punctuation) and we\'re filtering to sentences that translate to\n# the form ""I am"" or ""He is"" etc. (accounting for apostrophes replaced\n# earlier).\n#\n#\n# eng_prefixes = (\n#   ""i am "", ""i m "",\n#   ""he is"", ""he s "",\n#   ""she is"", ""she s"",\n#   ""you are"", ""you re "",\n#   ""we are"", ""we re "",\n#   ""they are"", ""they re ""\n#   )\n#\n#\n# def filter_pair(p):\n#   return len(p[0].split(\' \')) < opts.max_seq_length and \\\n#          len(p[1].split(\' \')) < opts.max_seq_length and \\\n#          p[1].startswith(eng_prefixes)\n#\n#\n# def filter_pairs(pairs):\n#   return [pair for pair in pairs if filter_pair(pair)]\n\n\n######################################################################\n# The full process for preparing the data is:\n#\n# -  Read text file and split into lines, split lines into pairs\n# -  Normalize text, filter by length and content\n# -  Make word lists from sentences in pairs\n#\n#\n# def prepare_data(lang1, lang2, reverse=False):\n#   input_lang, output_lang, pairs = read_langs(lang1, lang2, reverse)\n#   print(""Read %s sentence pairs"" % len(pairs))\n#   pairs = filter_pairs(pairs)\n#   print(""Trimmed to %s sentence pairs"" % len(pairs))\n#   print(""Counting words..."")\n#   for pair in pairs:\n#     input_lang.add_sentence(pair[0])\n#     output_lang.add_sentence(pair[1])\n#   print(""Counted words:"")\n#   print(input_lang.name, input_lang.n_words)\n#   print(output_lang.name, output_lang.n_words)\n#   return input_lang, output_lang, pairs\n#\n#\n# input_lang, output_lang, pairs = prepare_data(\'eng\', \'fra\', True)\n# print(random.choice(pairs))\n\n\n######################################################################\n# .. note:: There are other forms of attention that work around the length\n#   limitation by using a relative position approach. Read about ""local\n#   attention"" in `Effective Approaches to Attention-based Neural Machine\n#   Translation <https://arxiv.org/abs/1508.04025>`__.\n#\n# Training\n# ========\n#\n# Preparing Training Data\n# -----------------------\n#\n# To train, for each pair we will need an input tensor (indexes of the\n# words in the input sentence) and target tensor (indexes of the words in\n# the target sentence). While creating these vectors we will append the\n# EOS token to both sequences.\n#\n#\n# def indexes_from_sentence(lang, sentence):\n#   return [lang.word2index[word] for word in sentence.split(\' \')]\n#\n#\n# def variable_from_sentence(lang, sentence):\n#   indexes = indexes_from_sentence(lang, sentence)\n#   indexes.append(EOS_token)\n#   result = Variable(torch.LongTensor(indexes).view(-1, 1))\n#   if use_cuda:\n#     return result.cuda()\n#   else:\n#     return result\n#\n#\n# def variables_from_pair(pair):\n#   input_variable = variable_from_sentence(input_lang, pair[0])\n#   target_variable = variable_from_sentence(output_lang, pair[1])\n#   return input_variable, target_variable\n\n\n######################################################################\n# Training the Model\n# ------------------\n#\n# To train we run the input sentence through the encoder, and keep track\n# of every output and the latest hidden state. Then the decoder is given\n# the ``<SOS>`` token as its first input, and the last hidden state of the\n# decoder as its first hidden state.\n#\n# ""Teacher forcing"" is the concept of using the real target outputs as\n# each next input, instead of using the decoder\'s guess as the next input.\n# Using teacher forcing causes it to converge faster but `when the trained\n# network is exploited, it may exhibit\n# instability <http://minds.jacobs-university.de/sites/default/files/uploads\n# /papers/ESNTutorialRev.pdf>`__.\n#\n# You can observe outputs of teacher-forced networks that read with\n# coherent grammar but wander far from the correct translation -\n# intuitively it has learned to represent the output grammar and can ""pick\n# up"" the meaning once the teacher tells it the first few words, but it\n# has not properly learned how to create the sentence from the translation\n# in the first place.\n#\n# Because of the freedom PyTorch\'s autograd gives us, we can randomly\n# choose to use teacher forcing or not with a simple if statement. Turn\n# ``teacher_forcing_ratio`` up to use more of it.\n#\n\n\n\ndef train(input_variable, target_variable, encoder, decoder,\n          encoder_optimizer,\n          decoder_optimizer, criterion, max_length):\n  encoder_hidden = encoder.init_hidden()\n\n  encoder_optimizer.zero_grad()\n  decoder_optimizer.zero_grad()\n\n  input_length = input_variable.size()[0]\n  target_length = target_variable.size()[0]\n\n  encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n  encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n  loss = 0\n\n  for ei in range(input_length):\n    encoder_output, encoder_hidden = encoder(\n        input_variable[ei], encoder_hidden)\n    encoder_outputs[ei] = encoder_output[0][0]\n\n  # decoder_input = Variable(torch.LongTensor([[opts.SOS_token]]))\n  decoder_input = Variable(torch.zeros(opts.batch_size, opts.params_len))\n  decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n  decoder_hidden = encoder_hidden\n\n  use_teacher_forcing = True if random.random() < opts.teacher_forcing_ratio \\\n    else False\n\n  if use_teacher_forcing:\n    # Teacher forcing: Feed the target as the next input\n    for di in range(target_length):\n      decoder_output, decoder_hidden, decoder_attention = decoder(\n          decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n      loss += criterion(decoder_output, target_variable[di])\n      decoder_input = target_variable[di]  # Teacher forcing\n\n  else:\n    # Without teacher forcing: use its own predictions as the next input\n    for di in range(target_length):\n      decoder_output, decoder_hidden, decoder_attention = decoder(\n          decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n      topv, topi = decoder_output.data.topk(1)\n      ni = topi[0][0]\n\n      decoder_input = Variable(torch.LongTensor([[ni]]))\n      decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n      loss += criterion(decoder_output[0], target_variable[di])\n      # if ni == EOS_token:\n      #   break\n\n  loss.backward()\n\n  encoder_optimizer.step()\n  decoder_optimizer.step()\n\n  return loss.data[0] / target_length\n\n\n######################################################################\n# This is a helper function to print time elapsed and estimated time\n# remaining given the current time and progress %.\n#\n\nimport time\nimport math\n\n\ndef as_minutes(s):\n  m = math.floor(s / 60)\n  s -= m * 60\n  return \'%dm %ds\' % (m, s)\n\n\ndef time_since(since, percent):\n  now = time.time()\n  s = now - since\n  es = s / percent\n  rs = es - s\n  return \'%s (ETA: %s)\' % (as_minutes(s), as_minutes(rs))\n\n\n######################################################################\n# The whole training process looks like this:\n#\n# -  Start a timer\n# -  Initialize optimizers and criterion\n# -  Create set of training pairs\n# -  Start empty losses array for plotting\n#\n# Then we call ``train`` many times and occasionally print the progress (%\n# of epochs, time so far, estimated time) and average loss.\n#\n\ndef train_epochs(dataloader, encoder, decoder):\n  start = time.time()\n  plot_losses = []\n  print_loss_total = 0  # Reset every print_every\n  plot_loss_total = 0  # Reset every plot_every\n  batch_idx = 0\n  total_batch_idx = 0\n  curr_epoch = 0\n  b_epoch = dataloader.train_batches_per_epoch\n\n  # encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n  # decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n  encoder_optimizer = optim.Adam(encoder.parameters(), lr=opts.learning_rate)\n  decoder_optimizer = optim.Adam(decoder.parameters(), lr=opts.learning_rate)\n  # training_pairs = [variables_from_pair(random.choice(pairs))\n  #                   for _ in range(n_epochs)]\n  # criterion = nn.NLLLoss()\n  criterion = nn.MSELoss()\n\n  # Train on dataset batches\n  for src_batch, src_batch_seq_len, trg_batch, trg_mask in \\\n      dataloader.next_batch():\n    if curr_epoch == 0 and batch_idx == 0:\n      logger.info(\n          \'Batches per epoch: {}\'.format(b_epoch))\n      logger.info(\n          \'Total batches: {}\'.format(b_epoch * opts.epoch))\n    # beg_t = timeit.default_timer()\n\n    # for epoch in range(1, n_epochs + 1):\n    # training_pair = training_pairs[epoch - 1]\n    # input_variable = training_pair[0]\n    # target_variable = training_pair[1]\n\n    # Transpose data to be shaped (max_seq_length, num_sequences, params_len)\n    input_variable = Variable(\n        torch.from_numpy(src_batch[:, :, 0:44]).float()\n        ).transpose(1, 0).contiguous()\n    target_variable = Variable(\n        torch.from_numpy(trg_batch).float()\n        ).transpose(1, 0).contiguous()\n\n    input_variable = input_variable.cuda() if use_cuda else input_variable\n    target_variable = target_variable.cuda() if use_cuda else target_variable\n\n    loss = train(input_variable, target_variable, encoder, decoder,\n                 encoder_optimizer, decoder_optimizer, criterion,\n                 opts.max_seq_length)\n    print_loss_total += loss\n    # plot_loss_total += loss\n\n    print_loss_avg = print_loss_total / (total_batch_idx + 1)\n    plot_losses.append(print_loss_avg)\n\n    logger.info(\n        \'Batch {:2.0f}/{:2.0f} - Epoch {:2.0f}/{:2.0f} ({:3.2%}) - Loss={\'\n        \':.8f} - Time: {\'\n        \'!s}\'.format(\n            batch_idx,\n            b_epoch,\n            curr_epoch + 1,\n            opts.epoch,\n            ((batch_idx % b_epoch) + 1) / b_epoch,\n            print_loss_avg,\n            time_since(start,\n                       (total_batch_idx + 1) / (b_epoch * opts.epoch))))\n\n    if batch_idx >= b_epoch:\n      curr_epoch += 1\n      batch_idx = 0\n      print_loss_total = 0\n\n      # Save model\n      # Instructions for saving and loading a model:\n      # http://pytorch.org/docs/notes/serialization.html\n      # with gzip.open(\n      enc_file = os.path.join(opts.save_path, \'torch_train\',\n                              \'encoder_{}.pkl\'.format(\n                                curr_epoch))  # , \'wb\') as enc:\n      torch.save(encoder.state_dict(), enc_file)\n      # with gzip.open(\n      dec_file = os.path.join(opts.save_path, \'torch_train\',\n                              \'decoder_{}.pkl\'.format(\n                                curr_epoch))  # , \'wb\') as dec:\n      torch.save(decoder.state_dict(), dec_file)\n\n      # TODO Validation?\n\n    batch_idx += 1\n    total_batch_idx += 1\n\n    if curr_epoch >= opts.epoch:\n      logger.info(\'Finished epochs -> BREAK\')\n      break\n\n  if not opts.server:\n    show_plot(plot_losses)\n\n  else:\n    save_path = os.path.join(opts.save_path, \'torch_train\', \'graphs\')\n\n    if not os.path.exists(save_path):\n      os.makedirs(save_path)\n\n    np.savetxt(os.path.join(save_path, \'train_losses\' + \'.csv\'), plot_losses)\n\n  return encoder, decoder\n\n\n######################################################################\n# Plotting results\n# ----------------\n#\n# Plotting is done with matplotlib, using the array of loss values\n# ``plot_losses`` saved while training.\n#\n\nif not opts.server:\n  import matplotlib\n\n  matplotlib.use(\'TKagg\')\n  import matplotlib.pyplot as plt\n  import matplotlib.ticker as ticker\n\n\n  def show_plot(points, filename=\'train_loss\'):\n    if not os.path.exists(\n        os.path.join(opts.save_path, \'torch_train\', \'graphs\')):\n      os.makedirs(os.path.join(opts.save_path, \'torch_train\', \'graphs\'))\n\n    plt.figure()\n    # fig, ax = plt.subplots()\n    # this locator puts ticks at regular intervals\n    # loc = ticker.MultipleLocator(base=0.2)\n    # ax.yaxis.set_major_locator(loc)\n    plt.plot(points)\n    plt.grid(b=True)\n    plt.savefig(\n        os.path.join(opts.save_path, \'torch_train\', \'graphs\',\n                     filename + \'.eps\'),\n        bbox_inches=\'tight\')\n\n\n######################################################################\n# Evaluation\n# ==========\n#\n# Evaluation is mostly the same as training, but there are no targets so\n# we simply feed the decoder\'s predictions back to itself for each step.\n# Every time it predicts a word we add it to the output string, and if it\n# predicts the EOS token we stop there. We also store the decoder\'s\n# attention outputs for display later.\n#\n\ndef test(encoder, decoder, dl):\n  if opts.load_model:\n    # Get filenames of last epoch files\n    enc_file = sorted(glob.glob(\n        os.path.join(opts.save_path, \'torch_train\', \'encoder*.pkl\')))[-1]\n    dec_file = sorted(glob.glob(\n        os.path.join(opts.save_path, \'torch_train\', \'decoder*.pkl\')))[-1]\n\n    # Open model files and load\n    # with gzip.open(enc_file, \'r\') as enc:\n    #   enc_f = pickle.load(enc)\n    encoder.load_state_dict(torch.load(enc_file))\n    #\n    # with gzip.open(dec_file, \'wb\') as dec:\n    decoder.load_state_dict(torch.load(dec_file))\n\n  batch_idx = 0\n  n_batch = 0\n  attentions = []\n\n  for (src_batch_padded, src_batch_seq_len, trg_batch, trg_mask) in dl.next_batch(\n      test=True):\n    src_batch = []\n\n    # Take the last `seq_len` timesteps of each sequence to remove padding\n    for i in range(src_batch_padded.shape[0]):\n      src_batch.append(src_batch_padded[i,-src_batch_seq_len[i]:,:])\n\n    # TODO Get filename from datatable\n    f_name = format(n_batch + 1,\n                    \'0\' + str(max(5, len(str(dl.src_test_data.shape[0])))))\n\n    input_variable = Variable(\n        torch.from_numpy(src_batch[:, :, 0:44]).float()\n        ).transpose(1, 0).contiguous()\n    input_variable = input_variable.cuda() if use_cuda else input_variable\n\n    input_length = input_variable.size()[0]\n    encoder_hidden = encoder.init_hidden()\n\n    encoder_outputs = Variable(\n        torch.zeros(opts.max_seq_length, encoder.hidden_size))\n    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n    for ei in range(input_length):\n      encoder_output, encoder_hidden = encoder(input_variable[ei],\n                                               encoder_hidden)\n      encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n\n    # decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS\n    decoder_input = Variable(torch.zeros(opts.batch_size, opts.params_len))\n    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n    decoder_hidden = encoder_hidden\n\n    decoded_frames = []\n    decoder_attentions = torch.zeros(opts.max_seq_length, opts.batch_size,\n                                     opts.max_seq_length)\n\n    for di in range(opts.max_seq_length):\n      decoder_output, decoder_hidden, decoder_attention = decoder(\n          decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n      decoder_attentions[di] = decoder_attention.data\n      # topv, topi = decoder_output.data.topk(1)\n      # ni = topi[0][0]\n      # if ni == EOS_token:\n      #   decoded_frames.append(\'<EOS>\')\n      #   break\n      # else:\n      decoded_frames.append(decoder_output.data.cpu().numpy())\n\n      # decoder_input = Variable(torch.LongTensor([[ni]]))\n      decoder_input = decoder_output\n      decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n    # Decode output frames\n    predictions = np.array(decoded_frames).transpose((1, 0, 2))\n    attentions.append(decoder_attentions[:di + 1].numpy().transpose((1, 0, 2)))\n\n    # TODO Decode speech data and display attentions\n    # Save original U/V flags to save them to file\n    raw_uv_flags = predictions[:, :, 42]\n\n    # Unscale target and predicted parameters\n    for i in range(predictions.shape[0]):\n\n      src_spk_index = int(src_batch[i, 0, 44])\n      trg_spk_index = int(src_batch[i, 0, 45])\n\n      # Prepare filename\n      # Get speakers names\n      src_spk_name = dl.s2s_datatable.src_speakers[src_spk_index]\n      trg_spk_name = dl.s2s_datatable.trg_speakers[trg_spk_index]\n\n      # Make sure the save directory exists\n      tf_pred_path = os.path.join(opts.test_data_path, opts.pred_path)\n\n      if not os.path.exists(\n          os.path.join(tf_pred_path, src_spk_name + \'-\' + trg_spk_name)):\n        os.makedirs(\n            os.path.join(tf_pred_path, src_spk_name + \'-\' + trg_spk_name))\n\n        # # TODO Get filename from datatable\n        # f_name = format(i + 1, \'0\' + str(\n        # max(5, len(str(dl.src_test_data.shape[0])))))\n\n      with h5py.File(\n          os.path.join(tf_pred_path, src_spk_name + \'-\' + trg_spk_name,\n                       f_name + \'_\' + str(i) + \'.h5\'), \'w\') as file:\n        file.create_dataset(\'predictions\', data=predictions[i],\n                            compression=""gzip"",\n                            compression_opts=9)\n        file.create_dataset(\'target\', data=trg_batch[i],\n                            compression=""gzip"",\n                            compression_opts=9)\n        file.create_dataset(\'mask\', data=trg_mask[i],\n                            compression=""gzip"",\n                            compression_opts=9)\n\n        trg_spk_max = dl.train_trg_speakers_max[trg_spk_index, :]\n        trg_spk_min = dl.train_trg_speakers_min[trg_spk_index, :]\n\n        trg_batch[i, :, 0:42] = (trg_batch[i, :, 0:42] * (\n          trg_spk_max - trg_spk_min)) + trg_spk_min\n\n        predictions[i, :, 0:42] = (predictions[i, :, 0:42] * (\n          trg_spk_min - trg_spk_min)) + trg_spk_min\n\n        # Round U/V flags\n        predictions[i, :, 42] = np.round(predictions[i, :, 42])\n\n        # Remove padding in prediction and target parameters\n        masked_trg = mask_data(trg_batch[i], trg_mask[i])\n        trg_batch[i] = np.ma.filled(masked_trg, fill_value=0.0)\n        unmasked_trg = np.ma.compress_rows(masked_trg)\n\n        masked_pred = mask_data(predictions[i], trg_mask[i])\n        predictions[i] = np.ma.filled(masked_pred, fill_value=0.0)\n        unmasked_prd = np.ma.compress_rows(masked_pred)\n\n        # # Apply U/V flag to lf0 and mvf params\n        # unmasked_prd[:, 40][unmasked_prd[:, 42] == 0] = -1e10\n        # unmasked_prd[:, 41][unmasked_prd[:, 42] == 0] = 1000\n\n        # Apply ground truth flags to prediction\n        unmasked_prd[:, 40][unmasked_trg[:, 42] == 0] = -1e10\n        unmasked_prd[:, 41][unmasked_trg[:, 42] == 0] = 1000\n\n        file.create_dataset(\'unmasked_prd\', data=unmasked_prd,\n                            compression=""gzip"",\n                            compression_opts=9)\n        file.create_dataset(\'unmasked_trg\', data=unmasked_trg,\n                            compression=""gzip"",\n                            compression_opts=9)\n        file.create_dataset(\'trg_max\', data=trg_spk_max,\n                            compression=""gzip"",\n                            compression_opts=9)\n        file.create_dataset(\'trg_min\', data=trg_spk_min,\n                            compression=""gzip"",\n                            compression_opts=9)\n        file.close()\n\n      # Save predictions to files\n      np.savetxt(\n          os.path.join(tf_pred_path, src_spk_name + \'-\' + trg_spk_name,\n                       f_name + \'_\' + str(i) + \'.vf.dat\'),\n          unmasked_prd[:, 41]\n          )\n      np.savetxt(\n          os.path.join(tf_pred_path, src_spk_name + \'-\' + trg_spk_name,\n                       f_name + \'_\' + str(i) + \'.lf0.dat\'),\n          unmasked_prd[:, 40]\n          )\n      np.savetxt(\n          os.path.join(tf_pred_path, src_spk_name + \'-\' + trg_spk_name,\n                       f_name + \'_\' + str(i) + \'.mcp.dat\'),\n          unmasked_prd[:, 0:40],\n          delimiter=\'\\t\'\n          )\n      np.savetxt(\n          os.path.join(tf_pred_path, src_spk_name + \'-\' + trg_spk_name,\n                       f_name + \'_\' + str(i) + \'.uv.dat\'),\n          raw_uv_flags[i, :]\n          )\n\n    # Display metrics\n    print(\'Num - {}\'.format(n_batch))\n\n    print(\'MCD = {} dB\'.format(\n        error_metrics.MCD(unmasked_trg[:, 0:40].reshape(-1, 40),\n                          unmasked_prd[:, 0:40].reshape(-1, 40))))\n    acc, _, _, _ = error_metrics.AFPR(unmasked_trg[:, 42].reshape(-1, 1),\n                                      unmasked_prd[:, 42].reshape(-1, 1))\n    print(\'U/V accuracy = {}\'.format(acc))\n\n    pitch_rmse = error_metrics.RMSE(\n        np.exp(unmasked_trg[:, 40].reshape(-1, 1)),\n        np.exp(unmasked_prd[:, 40].reshape(-1, 1)))\n    print(\'Pitch RMSE = {}\'.format(pitch_rmse))\n\n    # Increase batch index\n    if batch_idx >= dl.test_batches_per_epoch:\n      break\n    batch_idx += 1\n\n    n_batch += 1\n\n  # Dump attentions to pickle file\n  logger.info(\'Saving attentions to pickle file\')\n  with gzip.open(\n      os.path.join(opts.save_path, \'torch_train\', \'attentions.pkl.gz\'),\n      \'wb\') as att_file:\n    pickle.dump(attentions, att_file)\n\n\n######################################################################\n# We can evaluate random sentences from the training set and print out the\n# input, target, and output to make some subjective quality judgements:\n#\n\n# def evaluate_randomly(encoder, decoder, n=10):\n#   for i in range(n):\n#     pair = random.choice(pairs)\n#     print(\'>\', pair[0])\n#     print(\'=\', pair[1])\n#     output_words, attentions = evaluate(encoder, decoder, pair[0])\n#     output_sentence = \' \'.join(output_words)\n#     print(\'<\', output_sentence)\n#     print(\'\')\n\n\n######################################################################\n# Training and Evaluating\n# =======================\n#\n# With all these helper functions in place (it looks like extra work, but\n# it\'s easier to run multiple experiments easier) we can actually\n# initialize a network and start training.\n#\n# Remember that the input sentences were heavily filtered. For this small\n# dataset we can use relatively small networks of 256 hidden nodes and a\n# single GRU layer. After about 40 minutes on a MacBook CPU we\'ll get some\n# reasonable results.\n#\n# .. Note::\n#    If you run this notebook you can train, interrupt the kernel,\n#    evaluate, and continue training later. Comment out the lines where the\n#    encoder and decoder are initialized and run ``trainEpochs`` again.\n#\n\n\n\n######################################################################\n#\n\n# evaluate_randomly(encoder1, attn_decoder1)\n\n######################################################################\n# Visualizing Attention\n# ---------------------\n#\n# A useful property of the attention mechanism is its highly interpretable\n# outputs. Because it is used to weight specific encoder outputs of the\n# input sequence, we can imagine looking where the network is focused most\n# at each time step.\n#\n# You could simply run ``plt.matshow(attentions)`` to see attention output\n# displayed as a matrix, with the columns being input steps and rows being\n# output steps:\n#\n#\n# output_words, attentions = evaluate(\n#     encoder1, attn_decoder1, ""je suis trop froid ."")\n# plt.matshow(attentions.numpy())\n\n\n######################################################################\n# For a better viewing experience we will do the extra work of adding axes\n# and labels:\n#\n\ndef show_attention():\n  # Load attentions\n  logger.info(\'Loading attentions to pickle file\')\n  with gzip.open(\n      os.path.join(opts.save_path, \'torch_train\', \'attentions.pkl.gz\'),\n      \'r\') as att_file:\n    attentions = pickle.load(att_file)\n\n  # Set up figure with colorbar\n  fig = plt.figure()\n  ax = fig.add_subplot(111)\n  cax = ax.matshow(attentions.numpy(), cmap=\'bone\')\n  fig.colorbar(cax)\n\n  # # Set up axes\n  # ax.set_xticklabels([\'\'] + input_sentence.split(\' \') +\n  #                    [\'<EOS>\'], rotation=90)\n  # ax.set_yticklabels([\'\'] + output_words)\n  #\n  # # Show label at every tick\n  # ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n  # ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n  plt.show()\n\n\n# def evaluate_and_show_attention(input_sentence):\n#   output_words, attentions = evaluate(\n#       encoder1, attn_decoder1, input_sentence)\n#   print(\'input =\', input_sentence)\n#   print(\'output =\', \' \'.join(output_words))\n#   show_attention(input_sentence, output_words, attentions)\n#\n#\n# evaluate_and_show_attention(""elle a cinq ans de moins que moi ."")\n#\n# evaluate_and_show_attention(""elle est trop petit ."")\n#\n# evaluate_and_show_attention(""je ne crains pas de mourir ."")\n#\n# evaluate_and_show_attention(""c est un jeune directeur plein de talent ."")\n\n######################################################################\n# Exercises\n# =========\n#\n# -  Try with a different dataset\n#\n#    -  Another language pair\n#    -  Human \xe2\x86\x92 Machine (e.g. IOT commands)\n#    -  Chat \xe2\x86\x92 Response\n#    -  Question \xe2\x86\x92 Answer\n#\n# -  Replace the embedding pre-trained word embeddings such as word2vec or\n#    GloVe\n# -  Try with more layers, more hidden units, and more sentences. Compare\n#    the training time and results.\n# -  If you use a translation file where pairs have two of the same phrase\n#    (``I am test \\t I am test``), you can use this as an autoencoder. Try\n#    this:\n#\n#    -  Train as an autoencoder\n#    -  Save only the Encoder network\n#    -  Train a new Decoder for translation from there\n#\n\nif __name__ == \'__main__\':\n  logger.debug(\'Before calling main\')\n  main(opts)\n'"
seq2seq_pytorch_model.py,0,"b'# Created by albert aparicio on 31/03/17\n# coding: utf-8\n#\n# Code from PyTorch\'s seq2seq tutorial\n# http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nuse_cuda = torch.cuda.is_available()\n\n\n######################################################################\n# The Seq2Seq Model\n# =================\n#\n# A Recurrent Neural Network, or RNN, is a network that operates on a\n# sequence and uses its own output as input for subsequent steps.\n#\n# A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n# seq2seq network, or `Encoder Decoder\n# network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n# consisting of two RNNs called the encoder and decoder. The encoder reads\n# an input sequence and outputs a single vector, and the decoder reads\n# that vector to produce an output sequence.\n#\n# .. figure:: /_static/img/seq-seq-images/seq2seq.png\n#    :alt:\n#\n# Unlike sequence prediction with a single RNN, where every input\n# corresponds to an output, the seq2seq model frees us from sequence\n# length and order, which makes it ideal for translation between two\n# languages.\n#\n# Consider the sentence ""Je ne suis pas le chat noir"" \xe2\x86\x92 ""I am not the\n# black cat"". Most of the words in the input sentence have a direct\n# translation in the output sentence, but are in slightly different\n# orders, e.g. ""chat noir"" and ""black cat"". Because of the ""ne/pas""\n# construction there is also one more word in the input sentence. It would\n# be difficult to produce a correct translation directly from the sequence\n# of input words.\n#\n# With a seq2seq model the encoder creates a single vector which, in the\n# ideal case, encodes the ""meaning"" of the input sequence into a single\n# vector \xe2\x80\x94 a single point in some N dimensional space of sentences.\n#\n\n\n######################################################################\n# The Encoder\n# -----------\n#\n# The encoder of a seq2seq network is a RNN that outputs some value for\n# every word from the input sentence. For every input word the encoder\n# outputs a vector and a hidden state, and uses the hidden state for the\n# next input word.\n#\n# .. figure:: /_static/img/seq-seq-images/encoder-network.png\n#    :alt:\n#\n#\n\nclass EncoderRNN(nn.Module):\n  def __init__(self, input_size, hidden_size,batch_size, n_layers=1):\n    super(EncoderRNN, self).__init__()\n    self.n_layers = n_layers\n    self.hidden_size = hidden_size\n    self.batch_size = batch_size\n\n    # self.embedding = nn.Embedding(input_size, hidden_size)\n    # self.gru = nn.GRU(hidden_size, hidden_size)\n    self.gru = nn.GRU(input_size, hidden_size,  n_layers)\n\n  def forward(self, f_input, hidden):\n    # embedded = self.embedding(f_input).view(1, 1, -1)\n    # output = embedded\n    output = f_input.view(1,f_input.size()[0],f_input.size()[1])\n\n    # for i in range(self.n_layers):\n    output, hidden = self.gru(output, hidden)\n    return output, hidden\n\n  def init_hidden(self):\n    result = Variable(torch.zeros(1, self.batch_size, self.hidden_size))\n    if use_cuda:\n      return result.cuda()\n    else:\n      return result\n\n\n######################################################################\n# The Decoder\n# -----------\n#\n# The decoder is another RNN that takes the encoder output vector(s) and\n# outputs a sequence of words to create the translation.\n#\n\n\n######################################################################\n# Simple Decoder\n# ^^^^^^^^^^^^^^\n#\n# In the simplest seq2seq decoder we use only last output of the encoder.\n# This last output is sometimes called the *context vector* as it encodes\n# context from the entire sequence. This context vector is used as the\n# initial hidden state of the decoder.\n#\n# At every step of decoding, the decoder is given an input token and\n# hidden state. The initial input token is the start-of-string ``<SOS>``\n# token, and the first hidden state is the context vector (the encoder\'s\n# last hidden state).\n#\n# .. figure:: /_static/img/seq-seq-images/decoder-network.png\n#    :alt:\n#\n#\n\nclass DecoderRNN(nn.Module):\n  def __init__(self, hidden_size, output_size,batch_size, n_layers=1):\n    super(DecoderRNN, self).__init__()\n    self.n_layers = n_layers\n    self.hidden_size = hidden_size\n    self.batch_size = batch_size\n\n    # self.embedding = nn.Embedding(output_size, hidden_size)\n    # self.gru = nn.GRU(hidden_size, hidden_size)\n    self.gru = nn.GRU(output_size, hidden_size)\n    self.out = nn.Linear(hidden_size, output_size)\n    self.softmax = nn.LogSoftmax()\n\n  def forward(self, f_input, hidden):\n    # output = self.embedding(f_input).view(1, 1, -1)\n    output = f_input.view(1,f_input.size()[0],f_input.size()[1])\n\n    for i in range(self.n_layers):\n      output = F.relu(output)\n      output, hidden = self.gru(output, hidden)\n    # TODO Maybe the softmax must be removed\n    output = self.softmax(self.out(output[0]))\n    return output, hidden\n\n  def init_hidden(self):\n    result = Variable(torch.zeros(1, 1, self.hidden_size))\n    if use_cuda:\n      return result.cuda()\n    else:\n      return result\n\n\n######################################################################\n# I encourage you to train and observe the results of this model, but to\n# save space we\'ll be going straight for the gold and introducing the\n# Attention Mechanism.\n#\n\n\n######################################################################\n# Attention Decoder\n# ^^^^^^^^^^^^^^^^^\n#\n# If only the context vector is passed betweeen the encoder and decoder,\n# that single vector carries the burden of encoding the entire sentence.\n#\n# Attention allows the decoder network to ""focus"" on a different part of\n# the encoder\'s outputs for every step of the decoder\'s own outputs. First\n# we calculate a set of *attention weights*. These will be multiplied by\n# the encoder output vectors to create a weighted combination. The result\n# (called ``attn_applied`` in the code) should contain information about\n# that specific part of the input sequence, and thus help the decoder\n# choose the right output words.\n#\n# .. figure:: https://i.imgur.com/1152PYf.png\n#    :alt:\n#\n# Calculating the attention weights is done with another feed-forward\n# layer ``attn``, using the decoder\'s input and hidden state as inputs.\n# Because there are sentences of all sizes in the training data, to\n# actually create and train this layer we have to choose a maximum\n# sentence length (input length, for encoder outputs) that it can apply\n# to. Sentences of the maximum length will use all the attention weights,\n# while shorter sentences will only use the first few.\n#\n# .. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n#    :alt:\n#\n#\n\nclass AttnDecoderRNN(nn.Module):\n  def __init__(self, hidden_size, output_size, max_length,batch_size, n_layers=1,\n               dropout_p=0.1):\n    super(AttnDecoderRNN, self).__init__()\n    self.hidden_size = hidden_size\n    self.output_size = output_size\n    self.n_layers = n_layers\n    self.dropout_p = dropout_p\n    self.max_length = max_length\n    self.batch_size = batch_size\n\n    # self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n\n    self.attn = nn.Linear(self.hidden_size + self.output_size, self.max_length)\n    self.attn_combine = nn.Linear(self.hidden_size + self.output_size, self.hidden_size)\n\n    # self.attn = nn.Linear(300, self.max_length)\n    # self.attn_combine = nn.Linear(300, self.hidden_size)\n\n    self.dropout = nn.Dropout(self.dropout_p)\n    self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n    self.out = nn.Linear(self.hidden_size, self.output_size)\n\n  def forward(self, f_input, hidden, encoder_output, encoder_outputs):\n    # embedded = self.embedding(f_input).view(1, 1, -1)\n    # embedded = self.dropout(embedded)\n    embedded = f_input.view(1,f_input.size()[0],f_input.size()[1])\n\n    attn_weights = F.softmax(\n        self.attn(torch.cat((embedded[0], hidden[0]), 1)))\n    attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n                             encoder_outputs.unsqueeze(0))\n\n    output = torch.cat((embedded[0], attn_applied[0]), 1)\n    output = self.attn_combine(output).unsqueeze(0)\n\n    for i in range(self.n_layers):\n      output = F.relu(output)\n      output, hidden = self.gru(output, hidden)\n\n    # Project GRU output to our data\'s output size\n    output = self.out(output[0])\n    return output, hidden, attn_weights\n\n  def init_hidden(self):\n    result = Variable(torch.zeros(1, 1, self.hidden_size))\n    if use_cuda:\n      return result.cuda()\n    else:\n      return result\n'"
seq2seq_roc_curves.py,2,"b'# Created by Albert Aparicio on 9/1/17\n# coding: utf-8\n\n# This script takes the predictions of the U/V flag and plots its ROC curves\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tfglib.seq2seq_datatable as s2s\nfrom sklearn.metrics import roc_curve, auc\nfrom tfglib.construct_table import parse_file\n\n# Load test database\nprint(\'Loading test datatable...\', end=\'\')\n(src_test_datatable,\n src_test_masks,\n trg_test_datatable,\n trg_test_masks,\n max_test_length,\n test_speakers_max,\n test_speakers_min\n ) = s2s.seq2seq2_load_datatable(\n    \'data/seq2seq_test_datatable.h5\'\n)\nprint(\'done\')\n\n##################\n# Load basenames #\n##################\nbasenames_file = open(\'data/test/seq2seq_basenames.list\', \'r\')\nbasenames_lines = basenames_file.readlines()\n# Strip \'\\n\' characters\nbasenames = [line.split(\'\\n\')[0] for line in basenames_lines]\n\n# Load speakers\nspeakers_file = open(\'data/test/speakers.list\', \'r\')\nspeakers_lines = speakers_file.readlines()\n# Strip \'\\n\' characters\nspeakers = [line.split(\'\\n\')[0] for line in speakers_lines]\n\n#######################\n# Loop over sequences #\n#######################\nassert len(basenames) == src_test_datatable.shape[0] / np.square(len(speakers))\n\n# Preallocate False-Positive and True-Positive matrices\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nsrc_spk_ind = 0\ntrg_spk_ind = 0\n\nfor src_spk in speakers:\n    fpr[src_spk] = dict()\n    tpr[src_spk] = dict()\n    roc_auc[src_spk] = dict()\n\n    for trg_spk in speakers:\n        fpr[src_spk][trg_spk] = dict()\n        tpr[src_spk][trg_spk] = dict()\n        roc_auc[src_spk][trg_spk] = dict()\n\n        # for i in range(src_test_datatable.shape[0]):\n        for i in range(len(basenames)):\n            # TODO Consider plotting an averaged ROC for each spk combination\n            print(src_spk + \'->\' + trg_spk + \' \' + basenames[i])\n            # TODO figure out if this is necessary\n            # fpr[src_spk][trg_spk][basenames[i]] = dict()\n            # tpr[src_spk][trg_spk][basenames[i]] = dict()\n            # roc_auc[src_spk][trg_spk][basenames[i]] = dict()\n\n            # Load raw U/V flags\n            raw_uv = parse_file(1,\n                                \'data/test/s2s_predicted/\' + src_spk + \'-\' +\n                                trg_spk + \'/\' + basenames[i] + \'.uv.dat\')\n\n            # Round U/V flags\n            rounded_uv = np.round(raw_uv)\n\n            # Compute ROC curve and the area under it\n            (\n                fpr[src_spk][trg_spk][basenames[i]],\n                tpr[src_spk][trg_spk][basenames[i]],\n                _\n            ) = roc_curve(\n                trg_test_datatable[\n                    i + (src_spk_ind + trg_spk_ind) * len(basenames), :, 42\n                ],\n                rounded_uv\n            )\n\n            roc_auc[src_spk][trg_spk][basenames[i]] = auc(\n                fpr[src_spk][trg_spk][basenames[i]],\n                tpr[src_spk][trg_spk][basenames[i]]\n            )\n\n            # Plot and save ROC curve\n            fig = plt.figure()\n            lw = 2\n            plt.plot(fpr[src_spk][trg_spk][basenames[i]],\n                     tpr[src_spk][trg_spk][basenames[i]],\n                     color=\'darkorange\',\n                     lw=lw,\n                     label=\'ROC curve (area = %0.2f)\' %\n                           roc_auc[src_spk][trg_spk][basenames[i]])\n            plt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\n            plt.xlim([0.0, 1.0])\n            plt.ylim([0.0, 1.05])\n            plt.xlabel(\'False Positive Rate\')\n            plt.ylabel(\'True Positive Rate\')\n            plt.title(\'Receiver operating characteristic example\')\n            plt.legend(loc=""lower right"")\n            plt.savefig(\n                \'training_results/uv_roc_\' + src_spk + \'-\' + trg_spk +\n                \'-\' + basenames[i] + \'.png\',\n                bbox_inches=\'tight\')\n            # plt.show()\n            # plt.close(""all"")\n\n        trg_spk_ind += 1\n    src_spk_ind += 1\n'"
seq2seq_tf_main.py,25,"b'# Created by albert aparicio on 02/04/17\n# coding: utf-8\n\n# This script defines an Sequence-to-Sequence model, using the implemented\n# encoders and decoders from TensorFlow\n\n# TODO Document and explain steps\n\n# This import makes Python use \'print\' as in Python 3.x\nfrom __future__ import print_function\n\nimport argparse\nimport gzip\nimport os\nimport timeit\nfrom datetime import datetime\nfrom sys import version_info\n\nimport h5py\nimport numpy as np\nimport tensorflow as tf\nfrom ahoproc_tools import error_metrics\nfrom tfglib.seq2seq_normalize import mask_data\nfrom tfglib.utils import init_logger\n\nfrom seq2seq_dataloader import DataLoader\nfrom seq2seq_tf_model import Seq2Seq\n\n# Conditional imports\nif version_info.major > 2:\n  import pickle\nelse:\n  import cPickle as pickle\n\nlogger, opts = None, None\n\nif __name__ == \'__main__\':\n  # logger.debug(\'Before parsing args\')\n  parser = argparse.ArgumentParser(\n      description=""Convert voice signal with seq2seq model"")\n  parser.add_argument(\'--train_data_path\', type=str,\n                      default=""tcstar_data_trim/training/"")\n  parser.add_argument(\'--train_out_file\', type=str,\n                      default=""tcstar_data_trim/seq2seq_train_datatable"")\n  parser.add_argument(\'--test_data_path\', type=str,\n                      default=""tcstar_data_trim/test/"")\n  parser.add_argument(\'--test_out_file\', type=str,\n                      default=""tcstar_data_trim/seq2seq_test_datatable"")\n  parser.add_argument(\'--val_fraction\', type=float, default=0.25)\n  parser.add_argument(\'--save-h5\', dest=\'save_h5\', action=\'store_true\',\n                      help=\'Save dataset to .h5 file\')\n  parser.add_argument(\'--max_seq_length\', type=int, default=500)\n  parser.add_argument(\'--params_len\', type=int, default=44)\n  parser.add_argument(\'--patience\', type=int, default=4,\n                      help=""Patience epochs to do validation, if validation ""\n                           ""score is worse than train for patience epochs ""\n                           "", quit training. (Def: 4)."")\n  parser.add_argument(\'--enc_rnn_layers\', type=int, default=1)\n  parser.add_argument(\'--dec_rnn_layers\', type=int, default=1)\n  parser.add_argument(\'--rnn_size\', type=int, default=512)\n  parser.add_argument(\'--cell_type\', type=str, default=""lstm"")\n  parser.add_argument(\'--batch_size\', type=int, default=10)\n  parser.add_argument(\'--epoch\', type=int, default=50)\n  parser.add_argument(\'--learning_rate\', type=float, default=0.00005)\n  parser.add_argument(\'--dropout\', type=float, default=0)\n  parser.add_argument(\'--optimizer\', type=str, default=""adam"")\n  parser.add_argument(\'--clip_norm\', type=float, default=5)\n  parser.add_argument(\'--attn_length\', type=int, default=500)\n  parser.add_argument(\'--attn_size\', type=int, default=256)\n  parser.add_argument(\'--save_every\', type=int, default=100)\n  parser.add_argument(\'--no-train\', dest=\'do_train\',\n                      action=\'store_false\', help=\'Flag to train or not.\')\n  parser.add_argument(\'--no-test\', dest=\'do_test\',\n                      action=\'store_false\', help=\'Flag to test or not.\')\n  parser.add_argument(\'--save_path\', type=str, default=""training_results"")\n  parser.add_argument(\'--pred_path\', type=str, default=""tf_predicted"")\n  parser.add_argument(\'--tb_path\', type=str, default="""")\n  parser.add_argument(\'--log\', type=str, default=""INFO"")\n  parser.add_argument(\'--load_model\', dest=\'load_model\', action=\'store_true\',\n                      help=\'Load previous model before training\')\n\n  parser.set_defaults(do_train=True, do_test=True, save_h5=False,\n                      load_model=False)\n  opts = parser.parse_args()\n\n  # Initialize logger\n  # logger_level = opts.log\n  logger = init_logger(name=__name__, level=opts.log)\n\n  logger.debug(\'Parsed arguments\')\n  if not os.path.exists(os.path.join(opts.save_path, \'tf_train\')):\n    os.makedirs(os.path.join(opts.save_path, \'tf_train\'))\n  # save config\n  with gzip.open(os.path.join(opts.save_path, \'tf_train\', \'config.pkl.gz\'),\n                 \'wb\') as cf:\n    pickle.dump(opts, cf)\n\nlogger.debug(\'Before defining main\')\n\n\ndef main(args):\n  logger.debug(\'Main\')\n\n  # If-else for training and testing\n  if args.do_train:\n    logger.info(\'Training\')\n\n    logger.debug(\'Initialize training DataLoader\')\n    dl = DataLoader(args, logger_level=args.log,\n                    max_seq_length=args.max_seq_length)\n\n    logger.debug(\'Initialize model\')\n    seq2seq_model = Seq2Seq(args.enc_rnn_layers, args.dec_rnn_layers,\n                            args.rnn_size, dl.max_seq_length, args.params_len,\n                            batch_size=args.batch_size, logger_level=args.log,\n                            dropout=args.dropout,\n                            learning_rate=args.learning_rate)\n\n    logger.info(\'Start training\')\n    train(seq2seq_model, dl, args)\n\n  if args.do_test:\n    tf.reset_default_graph()\n\n    logger.debug(\'Initialize test DataLoader\')\n    dl = DataLoader(args, test=True, logger_level=args.log,\n                    max_seq_length=args.max_seq_length)\n\n    logger.debug(\'Initialize model\')\n    seq2seq_model = Seq2Seq(args.enc_rnn_layers, args.dec_rnn_layers,\n                            args.rnn_size, dl.max_seq_length, args.params_len,\n                            batch_size=args.batch_size, logger_level=args.log,\n                            infer=True, dropout=args.dropout)\n\n    test(seq2seq_model, dl)\n\n\nlogger.debug(\'Defined main\')\n\nlogger.debug(\'Before define evaluate\')\n\n\ndef evaluate(sess, model, data_loader, curr_epoch):\n  """""" Evaluate an epoch over the given data_loader batches """"""\n  batch_idx = 0\n  batch_timings = []\n  eval_losses = []\n  for (\n      src_batch, src_batch_seq_len, trg_batch,\n      trg_mask) in data_loader.next_batch(\n      validation=True):\n    # if batch_idx == 0:\n    #   batches_per_epoch = data_loader.valid_batches_per_epoch\n    beg_t = timeit.default_timer()\n\n    # Model\'s placeholders\n    logger.debug(\'Fill feed_dict with gtruth and seq_length\')\n    feed_dict = {\n      model.gtruth      : trg_batch[:, :int(model.gtruth.get_shape()[1]), :],\n      model.gtruth_masks: trg_mask[:, :int(model.gtruth.get_shape()[1])],\n      model.seq_length  : src_batch_seq_len}\n\n    logger.debug(\'feed_dict - encoder_inputs\')\n    for i, enc_in in enumerate(model.encoder_inputs):\n      # TODO Fix encoder_inputs so that it takes 64 parameters as input\n      # feed_dict[enc_in] = src_batch[:,i,:]\n      feed_dict[enc_in] = src_batch[:, i, 0:44]\n\n    # Decoder inputs are trg_batch with the first timestep set to 0 (GO)\n    logger.debug(\'Roll decoder inputs and add GO symbol\')\n    trg_batch = np.roll(trg_batch, 1, axis=1)\n    trg_batch[:, 0, :] = np.zeros((model.batch_size, model.parameters_length))\n\n    logger.debug(\'feed_dict - decoder_inputs\')\n    for i, dec_in in enumerate(model.decoder_inputs):\n      feed_dict[dec_in] = trg_batch[:, i, :]\n\n    eval_losses.append(sess.run(model.val_loss, feed_dict=feed_dict))\n    batch_timings.append(timeit.default_timer() - beg_t)\n\n    # print(\'{}/{} (epoch {}) loss {}, time/batch {} s{}\'.format(\n    #   batch_idx, valid_batches_per_epoch, curr_epoch, eval_loss,\n    #   np.mean(batch_timings), \' \' * 8), end=\'\\r\',flush=True)\n\n    if batch_idx >= data_loader.valid_batches_per_epoch:\n      break\n\n    batch_idx += 1\n    logger.info(\'** Mean eval loss for epoch {}: {} **\'.format(curr_epoch,\n                                                               np.mean(\n                                                                   eval_losses)))\n\n  return eval_losses\n\n\nlogger.debug(\'Defined evaluate\')\n\nlogger.debug(\'Before define train\')\n\n\ndef train(model, dl, args):\n  logger.debug(\'Inside train\')\n  with tf.Session() as sess:\n    if args.load_model:\n      # Load model\n      model.load(sess, os.path.join(opts.save_path, \'tf_train\'))\n      logger.info(\'Loaded model successfully!\')\n    logger.debug(\'Define constants\')\n    batch_idx = 0\n    curr_epoch = 0\n    count = 0\n    batch_timings = []\n    tr_losses = []\n    val_losses = []\n\n    logger.debug(\'Initialize TF variables\')\n    try:\n      tf.global_variables_initializer().run()\n      merged = tf.summary.merge_all()\n    except AttributeError:\n      # Backward compatibility\n      tf.initialize_all_variables().run()\n      merged = tf.merge_all_summaries()\n\n    results_dir = os.path.join(opts.save_path, \'tf_train\')\n    log_dir = os.path.join(results_dir, \'tensorboard\', opts.tb_path)\n\n    if not os.path.exists(log_dir):\n      os.makedirs(log_dir)\n\n    train_writer = tf.summary.FileWriter(log_dir, sess.graph)\n\n    best_val_loss = 10e6\n    curr_patience = opts.patience\n    logger.debug(\'Defined constants\')\n\n    logger.debug(\'Start batch loop\')\n    for src_batch, src_batch_seq_len, trg_batch, trg_mask in dl.next_batch():\n      if curr_epoch == 0 and batch_idx == 0:\n        logger.info(\'Batches per epoch: {}\'.format(dl.train_batches_per_epoch))\n        logger.info(\n            \'Total batches: {}\'.format(dl.train_batches_per_epoch * opts.epoch))\n      beg_t = timeit.default_timer()\n\n      # Model\'s placeholders\n      logger.debug(\'Fill feed_dict with gtruth and seq_length\')\n      feed_dict = {\n        model.gtruth      : trg_batch[:, :int(model.gtruth.get_shape()[1]), :],\n        model.gtruth_masks: trg_mask[:, :int(model.gtruth.get_shape()[1])],\n        model.seq_length  : src_batch_seq_len}\n\n      logger.debug(\'feed_dict - encoder_inputs\')\n      for i, enc_in in enumerate(model.encoder_inputs):\n        # TODO Fix encoder_inputs so that it takes 64 parameters as input\n        # feed_dict[enc_in] = src_batch[:,i,:]\n        feed_dict[enc_in] = src_batch[:, i, 0:44]\n\n      # Decoder inputs are trg_batch with the first timestep set to 0 (GO)\n      logger.debug(\'Roll decoder inputs and add GO symbol\')\n      trg_batch = np.roll(trg_batch, 1, axis=1)\n      trg_batch[:, 0, :] = np.zeros((model.batch_size, model.parameters_length))\n\n      logger.debug(\'feed_dict - decoder_inputs\')\n      for i, dec_in in enumerate(model.decoder_inputs):\n        feed_dict[dec_in] = trg_batch[:, i, :]\n\n      logger.debug(\'sess.run\')\n      tr_loss, _, enc_state_fw, summary = sess.run(\n          [model.loss, model.train_op, model.enc_state_fw,\n           merged],\n          feed_dict=feed_dict)\n\n      logger.debug(\'Append batch timings\')\n      batch_timings.append(timeit.default_timer() - beg_t)\n\n      # Print batch info\n      logger.info(\n          \'Batch {}/{} (epoch {}) loss {}, time/batch (s) {}\\r\'.format(\n              count,\n              opts.epoch * dl.train_batches_per_epoch,  # Total num. of batches\n              curr_epoch + 1,\n              tr_loss,\n              np.mean(\n                  batch_timings)))\n\n      tr_losses.append(tr_loss)\n\n      if batch_idx % opts.save_every == 0:\n        train_writer.add_summary(summary, count)\n        logger.info(\'Save checkpoint\')\n        checkpoint_file = os.path.join(results_dir, \'model.ckpt\')\n        model.save(sess, checkpoint_file, count)\n      if batch_idx >= dl.train_batches_per_epoch:\n        curr_epoch += 1\n        batch_idx = 0\n\n        logger.debug(\'Evaluate epoch\')\n        va_loss = np.mean(evaluate(sess, model, dl, curr_epoch))\n        val_losses.append(va_loss)\n\n        if va_loss < best_val_loss:\n          logger.info(\n              \'Val loss improved {} --> {}\'.format(best_val_loss, va_loss))\n          logger.debug(\'Update the best score\')\n          best_val_loss = va_loss\n          curr_patience = opts.patience\n          best_checkpoint_file = os.path.join(results_dir, \'best_model.ckpt\')\n          model.save(sess, best_checkpoint_file)\n\n          # else:\n          #   logger.info(\n          #       \'Val loss did not improve, patience: {}\'.format(\n          # curr_patience))\n          #\n          #   curr_patience -= 1\n          #   if model.optimizer == \'sgd\':\n          #     # if we have SGD optimizer, half the learning rate\n          #     curr_lr = sess.run(model.curr_lr)\n          #     logger.info(\n          #         \'Halving lr {} --> {} in SGD\'.format(curr_lr,\n          # 0.5 * curr_lr))\n          #     sess.run(tf.assign(model.curr_lr, curr_lr * .5))\n          #   if curr_patience == 0:\n          #     logger.info(\n          #         \'Out of patience ({}) at epoch {} with tr_loss {} and \'\n          #         \'best_val_loss {}\'.format(\n          #             opts.patience, curr_epoch, tr_loss, best_val_loss))\n          #     break\n\n      batch_idx += 1\n      count += 1\n      if curr_epoch >= opts.epoch:\n        logger.info(\'Finished epochs -> BREAK\')\n        break\n\n    # Save loss values\n    np.savetxt(\n        os.path.join(\n            results_dir,\n            datetime.now().strftime(""%Y-%m-%d_%H:%M:%S"") + \'_tr_losses.csv\'),\n        tr_losses)\n    np.savetxt(\n        os.path.join(\n            results_dir,\n            datetime.now().strftime(""%Y-%m-%d_%H:%M:%S"") + \'_val_losses.csv\'),\n        val_losses)\n\n\nlogger.debug(\'Defined train\')\n\nlogger.debug(\'Before define test\')\n\n\ndef test(model, dl):\n  """""" Evaluate an epoch over the given data_loader batches """"""\n  # Initialize indexes and constants\n  batch_idx = 0\n  batch_timings = []\n  te_losses = []\n  # m_test_loss = None\n\n  # Start TF session\n  with tf.Session() as sess:\n    # Load model\n    if model.load(sess, os.path.join(opts.save_path, \'tf_train\')):\n      logger.info(\'Loaded model successfully!\')\n\n      n_batch = 0\n\n      # DataLoader.next_batch\n      for (src_batch, src_batch_seq_len, trg_batch, trg_mask) in dl.next_batch(\n          test=True):\n\n        # TODO Get filename from datatable\n        f_name = format(n_batch + 1, \'0\' + str(\n            max(5, len(str(dl.src_test_data.shape[0])))))\n\n        beg_t = timeit.default_timer()\n\n        # Fill feed_dict with test data\n        logger.debug(\'Fill feed_dict with gtruth and seq_length\')\n        feed_dict = {\n          model.gtruth      : trg_batch[:, :int(model.gtruth.get_shape()[1]),\n                              :],\n          model.gtruth_masks: trg_mask[:, :int(model.gtruth.get_shape()[1])],\n          model.seq_length  : src_batch_seq_len}\n\n        logger.debug(\'feed_dict - encoder_inputs\')\n        for i, enc_in in enumerate(model.encoder_inputs):\n          # TODO Fix encoder_inputs so that it takes 64 parameters as input\n          # feed_dict[enc_in] = src_batch[:,i,:]\n          feed_dict[enc_in] = src_batch[:, i, 0:44]\n\n        # Decoder inputs are trg_batch with the first timestep set to 0 (GO)\n        logger.debug(\'Roll decoder inputs and add GO symbol\')\n        trg_batch = np.roll(trg_batch, 1, axis=1)\n        trg_batch[:, 0, :] = np.zeros(\n            (model.batch_size, model.parameters_length))\n\n        logger.debug(\'feed_dict - decoder_inputs\')\n        for i, dec_in in enumerate(model.decoder_inputs):\n          feed_dict[dec_in] = trg_batch[:, i, :]\n\n        # sess.run with loss and predictions\n        te_loss, predictions = sess.run([model.loss, model.prediction],\n                                        feed_dict=feed_dict)\n        te_losses.append(te_loss)\n\n        # Append times\n        batch_timings.append(timeit.default_timer() - beg_t)\n\n        # Decode predictions\n        # predictions.shape -> (batch_size, max_seq_length, params_len)\n        # Save original U/V flags to save them to file\n        raw_uv_flags = predictions[:, :, 42]\n\n        # Unscale target and predicted parameters\n        for i in range(predictions.shape[0]):\n\n          src_spk_index = int(src_batch[i, 0, 44])\n          trg_spk_index = int(src_batch[i, 0, 45])\n\n          # Prepare filename\n          # Get speakers names\n          src_spk_name = dl.s2s_datatable.src_speakers[src_spk_index]\n          trg_spk_name = dl.s2s_datatable.trg_speakers[trg_spk_index]\n\n          # Make sure the save directory exists\n          tf_pred_path = os.path.join(opts.test_data_path, opts.pred_path)\n\n          if not os.path.exists(\n              os.path.join(tf_pred_path, src_spk_name + \'-\' + trg_spk_name)):\n            os.makedirs(\n                os.path.join(tf_pred_path, src_spk_name + \'-\' + trg_spk_name))\n\n            # # TODO Get filename from datatable\n            # f_name = format(i + 1, \'0\' + str(\n            # max(5, len(str(dl.src_test_data.shape[0])))))\n\n          with h5py.File(\n              os.path.join(tf_pred_path, src_spk_name + \'-\' + trg_spk_name,\n                           f_name + \'_\' + str(i) + \'.h5\'), \'w\') as file:\n            file.create_dataset(\'predictions\', data=predictions[i],\n                                compression=""gzip"",\n                                compression_opts=9)\n            file.create_dataset(\'target\', data=trg_batch[i],\n                                compression=""gzip"",\n                                compression_opts=9)\n            file.create_dataset(\'mask\', data=trg_mask[i],\n                                compression=""gzip"",\n                                compression_opts=9)\n\n            trg_spk_max = dl.train_trg_speakers_max[trg_spk_index, :]\n            trg_spk_min = dl.train_trg_speakers_min[trg_spk_index, :]\n\n            trg_batch[i, :, 0:42] = (trg_batch[i, :, 0:42] * (\n              trg_spk_max - trg_spk_min)) + trg_spk_min\n\n            predictions[i, :, 0:42] = (predictions[i, :, 0:42] * (\n              trg_spk_min - trg_spk_min)) + trg_spk_min\n\n            # Round U/V flags\n            predictions[i, :, 42] = np.round(predictions[i, :, 42])\n\n            # Remove padding in prediction and target parameters\n            masked_trg = mask_data(trg_batch[i], trg_mask[i])\n            trg_batch[i] = np.ma.filled(masked_trg, fill_value=0.0)\n            unmasked_trg = np.ma.compress_rows(masked_trg)\n\n            masked_pred = mask_data(predictions[i], trg_mask[i])\n            predictions[i] = np.ma.filled(masked_pred, fill_value=0.0)\n            unmasked_prd = np.ma.compress_rows(masked_pred)\n\n            # # Apply U/V flag to lf0 and mvf params\n            # unmasked_prd[:, 40][unmasked_prd[:, 42] == 0] = -1e10\n            # unmasked_prd[:, 41][unmasked_prd[:, 42] == 0] = 1000\n\n            # Apply ground truth flags to prediction\n            unmasked_prd[:, 40][unmasked_trg[:, 42] == 0] = -1e10\n            unmasked_prd[:, 41][unmasked_trg[:, 42] == 0] = 1000\n\n            file.create_dataset(\'unmasked_prd\', data=unmasked_prd,\n                                compression=""gzip"",\n                                compression_opts=9)\n            file.create_dataset(\'unmasked_trg\', data=unmasked_trg,\n                                compression=""gzip"",\n                                compression_opts=9)\n            file.create_dataset(\'trg_max\', data=trg_spk_max,\n                                compression=""gzip"",\n                                compression_opts=9)\n            file.create_dataset(\'trg_min\', data=trg_spk_min,\n                                compression=""gzip"",\n                                compression_opts=9)\n            file.close()\n\n          # Save predictions to files\n          np.savetxt(\n              os.path.join(tf_pred_path, src_spk_name + \'-\' + trg_spk_name,\n                           f_name + \'_\' + str(i) + \'.vf.dat\'),\n              unmasked_prd[:, 41]\n              )\n          np.savetxt(\n              os.path.join(tf_pred_path, src_spk_name + \'-\' + trg_spk_name,\n                           f_name + \'_\' + str(i) + \'.lf0.dat\'),\n              unmasked_prd[:, 40]\n              )\n          np.savetxt(\n              os.path.join(tf_pred_path, src_spk_name + \'-\' + trg_spk_name,\n                           f_name + \'_\' + str(i) + \'.mcp.dat\'),\n              unmasked_prd[:, 0:40],\n              delimiter=\'\\t\'\n              )\n          np.savetxt(\n              os.path.join(tf_pred_path, src_spk_name + \'-\' + trg_spk_name,\n                           f_name + \'_\' + str(i) + \'.uv.dat\'),\n              raw_uv_flags[i, :]\n              )\n\n        # Display metrics\n        print(\'Num - {}\'.format(n_batch))\n\n        print(\'MCD = {} dB\'.format(\n            error_metrics.MCD(unmasked_trg[:, 0:40].reshape(-1, 40),\n                              unmasked_prd[:, 0:40].reshape(-1, 40))))\n        acc, _, _, _ = error_metrics.AFPR(unmasked_trg[:, 42].reshape(-1, 1),\n                                          unmasked_prd[:, 42].reshape(-1, 1))\n        print(\'U/V accuracy = {}\'.format(acc))\n\n        pitch_rmse = error_metrics.RMSE(\n            np.exp(unmasked_trg[:, 40].reshape(-1, 1)),\n            np.exp(unmasked_prd[:, 40].reshape(-1, 1)))\n        print(\'Pitch RMSE = {}\'.format(pitch_rmse))\n\n        # Increase batch index\n        if batch_idx >= dl.test_batches_per_epoch:\n          break\n        batch_idx += 1\n\n        n_batch += 1\n\n      # Print test results\n      m_test_loss = np.mean(te_losses)\n      print(\'** Mean test loss {} **\'.format(m_test_loss))\n\n      # Save loss values\n      np.savetxt(\n          os.path.join(\n              os.path.join(opts.save_path, \'tf_train\'),\n              datetime.now().strftime(""%Y-%m-%d_%H:%M:%S"") + \'_te_losses.csv\'),\n          te_losses)\n\n      return m_test_loss\n\n\nlogger.debug(\'Defined test\')\n\nif __name__ == \'__main__\':\n  logger.debug(\'Before calling main\')\n  main(opts)\n'"
seq2seq_tf_model.py,0,"b'# Created by albert aparicio on 31/03/17\n# coding: utf-8\n\n# This script defines an Sequence-to-Sequence model, using the implemented\n# encoders and decoders from TensorFlow\n\n# TODO Document and explain steps\n# TODO Move this model to tfglib\n\n# This import makes Python use \'print\' as in Python 3.x\nfrom __future__ import print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nimport tfglib.seq2seq_datatable as s2s\nfrom tensorflow.contrib.legacy_seq2seq.python.ops.seq2seq import \\\n  attention_decoder\nfrom tensorflow.contrib.rnn.python.ops.core_rnn import static_rnn\nfrom tfglib.seq2seq_normalize import maxmin_scaling\nfrom tfglib.utils import init_logger\n\n\n################################################################################\n# Code from santi-pdp @ GitHub\n# https://github.com/santi-pdp/word2phone/blob/master/model.py\n\ndef scalar_summary(name, x):\n  try:\n    summ = tf.summary.scalar(name, x)\n  except AttributeError:\n    summ = tf.scalar_summary(name, x)\n  return summ\n\n\ndef histogram_summary(name, x):\n  try:\n    summ = tf.summary.histogram(name, x)\n  except AttributeError:\n    summ = tf.histogram_summary(name, x)\n  return summ\n\n\n################################################################################\n\nclass Seq2Seq(object):\n  # TODO Figure out suitable values for attention length and size\n  def __init__(self, enc_rnn_layers, dec_rnn_layers, rnn_size,\n               seq_length, params_length, cell_type=\'lstm\', batch_size=20,\n               learning_rate=0.001, dropout=0.5, optimizer=\'adam\', clip_norm=5,\n               attn_length=500, attn_size=100, infer=False,\n               logger_level=\'INFO\'):\n    """"""\n    infer: only True if used for test or predictions. False to train.\n    """"""\n    self.logger = init_logger(name=__name__, level=logger_level)\n\n    self.logger.debug(\'Seq2Seq init\')\n    self.rnn_size = rnn_size\n    self.attn_length = attn_length\n    self.attn_size = attn_size\n\n    # Number of layers in encoder and decoder\n    self.enc_rnn_layers = enc_rnn_layers\n    self.dec_rnn_layers = dec_rnn_layers\n\n    self.infer = infer\n    if infer:\n      self.keep_prob = tf.Variable(1., trainable=False)\n    else:\n      self.keep_prob = tf.Variable((1. - dropout), trainable=False)\n\n    self.dropout = dropout\n    self.cell_type = cell_type\n    self.batch_size = batch_size\n    self.clip_norm = clip_norm\n    self.learning_rate = learning_rate\n\n    self.seq_length = tf.placeholder(tf.int32, [self.batch_size])\n\n    self.parameters_length = params_length\n\n    self.gtruth = tf.placeholder(tf.float32,\n                                 [self.batch_size,\n                                  seq_length,\n                                  self.parameters_length])\n\n    # Ground truth summaries\n    split_gtruth = tf.split(self.gtruth, self.parameters_length, axis=2,\n                            name=\'gtruth_parameter\')\n\n    self.gtruth_summaries = []\n    [self.gtruth_summaries.append(\n        histogram_summary(split_tensor.name, split_tensor)) for split_tensor in\n      split_gtruth]\n\n    self.gtruth_masks = tf.placeholder(tf.float32,\n                                       [self.batch_size, seq_length])\n\n    self.encoder_inputs = [\n      tf.placeholder(tf.float32, [batch_size, self.parameters_length]\n                     ) for _ in range(seq_length)]\n\n    # Encoder inputs summaries\n    split_enc_inputs = tf.split(tf.stack(self.encoder_inputs, axis=1),\n                                self.parameters_length, axis=2,\n                                name=\'encoder_parameter\')\n    self.enc_inputs_summaries = []\n    [self.enc_inputs_summaries.append(\n        histogram_summary(split_tensor.name, split_tensor)) for split_tensor in\n      split_enc_inputs]\n\n    self.decoder_inputs = [\n      tf.placeholder(tf.float32, [batch_size, self.parameters_length]\n                     ) for _ in range(seq_length)]\n\n    # To be assigned a value later\n    self.enc_state_fw = None\n    self.enc_state_bw = None\n    self.encoder_vars = None\n    self.enc_zero_fw = None\n    self.enc_zero_bw = None\n    self.encoder_state_summaries_fw = None\n    self.encoder_state_summaries_bw = None\n    self.decoder_outputs_summaries = []\n\n    self.prediction = self.inference()\n\n    self.loss = self.mse_loss(self.gtruth, self.gtruth_masks, self.prediction)\n    self.val_loss = self.mse_loss(self.gtruth, self.gtruth_masks,\n                                  self.prediction)\n\n    self.loss_summary = scalar_summary(\'loss\', self.loss)\n    self.val_loss_summary = scalar_summary(\'val_loss\', self.val_loss)\n\n    tvars = tf.trainable_variables()\n    grads = []\n    for grad in tf.gradients(self.loss, tvars):\n      # if grad is not None:\n      #   grads.append(tf.clip_by_norm(grad, self.clip_norm))\n      # else:\n      grads.append(grad)\n\n    self.optimizer = optimizer\n    # set up a variable to make the learning rate evolve during training\n    self.curr_lr = tf.Variable(self.learning_rate, trainable=False)\n    self.opt = tf.train.AdamOptimizer(self.curr_lr)\n\n    self.train_op = self.opt.apply_gradients(zip(grads, tvars))\n\n  def build_multirnn_block(self, rnn_size, rnn_layers, cell_type,\n                           activation=tf.tanh):\n    self.logger.debug(\'Build RNN block\')\n    if cell_type == \'gru\':\n      cell = tf.contrib.rnn.GRUCell(rnn_size, activation=activation)\n    elif cell_type == \'lstm\':\n      cell = tf.contrib.rnn.BasicLSTMCell(rnn_size, state_is_tuple=True,\n                                          activation=activation)\n    else:\n      raise ValueError(""The selected cell type \'%s\' is not supported""\n                       % cell_type)\n\n    if rnn_layers > 1:\n      cell = tf.contrib.rnn.MultiRNNCell([cell] * rnn_layers,\n                                         state_is_tuple=True)\n    return cell\n\n  def mse_loss(self, gtruth, gtruth_masks, prediction):\n    """"""Mean squared error loss""""""\n    # Previous to the computation, the predictions are masked\n    self.logger.debug(\'Compute loss\')\n    return tf.reduce_mean(tf.squared_difference(gtruth,\n                                                prediction * tf.expand_dims(\n                                                    gtruth_masks, -1)))\n\n  def mae_loss(self, gtruth, gtruth_masks, prediction):\n    """"""Mean absolute error loss""""""\n    # Previous to the computation, the predictions are masked\n    self.logger.debug(\'Compute loss\')\n    return tf.reduce_mean(\n        tf.abs((prediction * tf.expand_dims(gtruth_masks, -1)) - gtruth))\n\n  def inference(self):\n    self.logger.debug(\'Inference\')\n    self.logger.debug(\'Imported seq2seq model from TF\')\n\n    with tf.variable_scope(""encoder""):\n      enc_cell_fw = self.build_multirnn_block(self.rnn_size,\n                                              self.enc_rnn_layers,\n                                              self.cell_type)\n      # enc_cell_bw = self.build_multirnn_block(self.rnn_size,\n      #                                         self.enc_rnn_layers,\n      #                                         self.cell_type)\n      self.enc_zero_fw = enc_cell_fw.zero_state(self.batch_size, tf.float32)\n      # self.enc_zero_bw = enc_cell_bw.zero_state(self.batch_size, tf.float32)\n\n      self.logger.debug(\'Initialize encoder\')\n\n      # inputs = batch_norm(self.encoder_inputs, is_training=self.infer)\n      # inputs = []\n      # for tensor in self.encoder_inputs:\n      #   inputs.append(batch_norm(tensor, is_training=self.infer))\n\n      # enc_out_orig, enc_state_fw, enc_state_bw = static_bidirectional_rnn(\n      #     cell_fw=enc_cell_fw, cell_bw=enc_cell_bw, inputs=inputs,\n      #     initial_state_fw=self.enc_zero_fw,\n      # initial_state_bw=self.enc_zero_bw,\n      #     sequence_length=self.seq_length\n      #     )\n      enc_out, enc_state_fw = static_rnn(cell=enc_cell_fw,\n                                         inputs=self.encoder_inputs,\n                                         initial_state=self.enc_zero_fw,\n                                         sequence_length=self.seq_length)\n\n      # enc_out = []\n      # for tensor in enc_out_orig:\n      #   enc_out.append(batch_norm(tensor, is_training=self.infer))\n\n    # This op is created to visualize the thought vectors\n    self.enc_state_fw = enc_state_fw\n    # self.enc_state_bw = enc_state_bw\n\n    self.logger.info(\n        \'enc out (len {}) tensors shape: {}\'.format(\n            len(enc_out), enc_out[0].get_shape()\n            ))\n    # print(\'enc out tensor shape: \', enc_out.get_shape())\n\n    self.encoder_state_summaries_fw = histogram_summary(\n        \'encoder_state_fw\', enc_state_fw)\n    # self.encoder_state_summaries_bw = histogram_summary(\n    #     \'encoder_state_bw\', enc_state_bw)\n\n    dec_cell = self.build_multirnn_block(self.rnn_size,\n                                         self.dec_rnn_layers,\n                                         self.cell_type)\n    if self.dropout > 0:\n      # print(\'Applying dropout {} to decoder\'.format(self.dropout))\n      self.logger.info(\'Applying dropout {} to decoder\'.format(self.dropout))\n      dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell,\n                                               input_keep_prob=self.keep_prob)\n\n    dec_cell = tf.contrib.rnn.OutputProjectionWrapper(dec_cell,\n                                                      self.parameters_length)\n\n    if self.infer:\n      def loop_function(prev, _):\n        return prev\n    else:\n      loop_function = None\n\n    # First calculate a concatenation of encoder outputs to put attention on.\n    # assert enc_cell_fw.output_size == enc_cell_bw.output_size\n    # top_states = [\n    #   tf.reshape(e, [-1, 1, enc_cell_fw.output_size +\n    # enc_cell_bw.output_size])\n    #   for e in enc_out\n    #   ]\n    top_states = [\n      tf.reshape(e, [-1, 1, enc_cell_fw.output_size])\n      for e in enc_out\n      ]\n    attention_states = tf.concat(top_states, 1)\n\n    self.logger.debug(\'Initialize decoder\')\n    # ############################################################################\n    # # Code from renzhe0009 @ StackOverflow\n    # # http://stackoverflow.com/q/42703140/7390416\n    # # License: MIT\n    # # Because published after March 2016 - meta.stackexchange.com/q/272956\n    #\n    # # enc_state_c = tf.concat(values=(enc_state_fw.c, enc_state_bw.c), axis=1)\n    # # enc_state_h = tf.concat(values=(enc_state_fw.h, enc_state_bw.h), axis=1)\n    # enc_state_c = enc_state_fw.c + enc_state_bw.c\n    # enc_state_h = enc_state_fw.h + enc_state_bw.h\n    #\n    # enc_state = LSTMStateTuple(c=enc_state_c, h=enc_state_h)\n    # ############################################################################\n\n    dec_out, dec_state = attention_decoder(\n        self.decoder_inputs, enc_state_fw, cell=dec_cell,\n        attention_states=attention_states, loop_function=loop_function\n        )\n\n    # Apply sigmoid activation to decoder outputs\n    dec_out = tf.sigmoid(dec_out)\n\n    # print(\'dec_state shape: \', dec_state[0].get_shape())\n    # merge outputs into a tensor and transpose to be [B, seq_length, out_dim]\n    dec_outputs = tf.transpose(tf.stack(dec_out), [1, 0, 2])\n    # print(\'dec outputs shape: \', dec_outputs.get_shape())\n    self.logger.info(\'dec outputs shape: {}\'.format(dec_outputs.get_shape()))\n\n    # Decoder outputs summaries\n    split_dec_out = tf.split(dec_outputs, self.parameters_length, axis=2,\n                             name=\'decoder_parameter\')\n    [self.decoder_outputs_summaries.append(\n        histogram_summary(split_tensor.name, split_tensor)) for split_tensor in\n      split_dec_out]\n\n    # Separate decoder output into parameters and flags\n    # params_in, flags_in = tf.split(dec_outputs, [42, 2], axis=2)\n\n    self.encoder_vars = {}\n    for tvar in tf.trainable_variables():\n      if \'char_embedding\' in tvar.name or \'encoder\' in tvar.name:\n        self.encoder_vars[tvar.name] = tvar\n      print(\'tvar: \', tvar.name)\n\n    return dec_outputs\n\n  def save(self, sess, save_filename, global_step=None):\n    if not hasattr(self, \'saver\'):\n      self.saver = tf.train.Saver()\n    if not hasattr(self, \'encoder_saver\'):\n      self.encoder_saver = tf.train.Saver(var_list=self.encoder_vars)\n    print(\'Saving checkpoint...\')\n    if global_step is not None:\n      self.encoder_saver.save(sess, save_filename + \'.encoder\',\n                              global_step)\n      self.saver.save(sess, save_filename, global_step)\n    else:\n      self.encoder_saver.save(sess, save_filename + \'.encoder\')\n      self.saver.save(sess, save_filename)\n\n  def load(self, sess, save_path):\n    if not hasattr(self, \'saver\'):\n      self.saver = tf.train.Saver()\n    if os.path.exists(os.path.join(save_path, \'best_model.ckpt\')):\n      ckpt_name = os.path.join(save_path, \'best_model.ckpt\')\n      print(\'Loading checkpoint {}...\'.format(ckpt_name))\n      self.saver.restore(sess, os.path.join(ckpt_name))\n    else:\n      ckpt = tf.train.get_checkpoint_state(save_path)\n      if ckpt and ckpt.model_checkpoint_path:\n        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n        print(\'Loading checkpoint {}...\'.format(ckpt_name))\n        self.saver.restore(sess, os.path.join(save_path, ckpt_name))\n        return True\n      else:\n        return False\n'"
seq2seq_tf_plot_curves.py,9,"b""# Created by Albert Aparicio on 6/12/16\n# coding: utf-8\n\n# This script takes the results of a training and plots its loss curves\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nresults_dir = 'training_results/tf_train'\nbatch_size = 20\nepoch = 30\nbatches_per_epoch = 1573\ntotal_batches = 47190\n\nfor root, dirs, files in os.walk(results_dir):\n  tr_files = [file for file in sorted(files) if 'tr_losses.csv' in file]\n  va_files = [file for file in sorted(files) if 'val_losses.csv' in file]\n  te_files = [file for file in sorted(files) if 'te_losses.csv' in file]\n\n  tr_batches = np.loadtxt(os.path.join(results_dir, tr_files[-1]))\n  val_losses = np.loadtxt(os.path.join(results_dir, va_files[-1]))\n  te_losses = np.loadtxt(os.path.join(results_dir, te_files[-1]))\n\ntrained_epochs = int(np.floor(tr_batches.shape[0] / batches_per_epoch))\ntrained_batches = trained_epochs * batches_per_epoch\nwhole_epochs = np.split(tr_batches[:trained_batches], trained_epochs)\nleftover = tr_batches[trained_batches:]\n\ntr_losses = np.mean(np.array(whole_epochs), axis=1)\n\ntr_val_epochs = np.arange(trained_epochs) + 1\n\nh1 = plt.figure(figsize=(14, 8))\nax1 = h1.add_subplot(111)\nplt.plot(tr_val_epochs, tr_losses, tr_val_epochs, val_losses, '--', linewidth=2)\nplt.legend(['loss', 'val_loss'], loc='best')\n\nplt.suptitle(\n    'Training and validation losses. Test loss = {}'.format(np.mean(te_losses)))\n\nax1.set_xlabel('Epochs')\nax1.set_ylabel('Loss values')\n\nax1.set_xlim(tr_val_epochs[0], tr_val_epochs[-1])\nmajor_xticks = np.arange(tr_val_epochs[0], tr_val_epochs[-1] + 1, 1)\nax1.set_xticks(major_xticks)\nax1.tick_params(which='both', direction='out')\n\nax1.grid(which='both', ls='-')\n\n# TODO Save model parameters in TF training\nmodel_description = 'tf_seq2seq'\nparams_loss = 'mse'\noptimizer_name = 'adam'\nlearning_rate = 0.001\n\nfig_filename = os.path.join(results_dir,\n                            model_description + '_' + params_loss + '_' +\n                            optimizer_name + '_epochs_' + str(epoch) + '_lr_' +\n                            str(learning_rate) + '_graph')\n\nplt.savefig(fig_filename + '.eps', bbox_inches='tight')\n# plt.savefig(fig_filename + '.png', bbox_inches='tight')\n\n# plt.show()\nplt.close(h1)\n"""
tf_seq2seq_example.py,2,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Binary for training translation models and decoding from them.\n\nRunning this program without --decode will download the WMT corpus into\nthe directory specified as --data_dir and tokenize it in a very basic way,\nand then start training a model saving checkpoints to --train_dir.\n\nRunning with --decode starts an interactive loop so you can see how\nthe current checkpoint translates English sentences into French.\n\nSee the following papers for more information on neural translation models.\n * http://arxiv.org/abs/1409.3215\n * http://arxiv.org/abs/1409.0473\n * http://arxiv.org/abs/1412.2007\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport math\nimport os\nimport random\nimport sys\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nimport data_utils\nimport tf_seq2seq_model_example\n\ntf.app.flags.DEFINE_float(""learning_rate"", 0.5, ""Learning rate."")\ntf.app.flags.DEFINE_float(""learning_rate_decay_factor"", 0.99,\n                          ""Learning rate decays by this much."")\ntf.app.flags.DEFINE_float(""max_gradient_norm"", 5.0,\n                          ""Clip gradients to this norm."")\ntf.app.flags.DEFINE_integer(""batch_size"", 64,\n                            ""Batch size to use during training."")\ntf.app.flags.DEFINE_integer(""size"", 1024, ""Size of each model layer."")\ntf.app.flags.DEFINE_integer(""num_layers"", 3, ""Number of layers in the model."")\ntf.app.flags.DEFINE_integer(""from_vocab_size"", 40000,\n                            ""English vocabulary size."")\ntf.app.flags.DEFINE_integer(""to_vocab_size"", 40000, ""French vocabulary size."")\ntf.app.flags.DEFINE_string(""data_dir"", ""/tmp"", ""Data directory"")\ntf.app.flags.DEFINE_string(""train_dir"", ""/tmp"", ""Training directory."")\ntf.app.flags.DEFINE_string(""from_train_data"", None, ""Training data."")\ntf.app.flags.DEFINE_string(""to_train_data"", None, ""Training data."")\ntf.app.flags.DEFINE_string(""from_dev_data"", None, ""Training data."")\ntf.app.flags.DEFINE_string(""to_dev_data"", None, ""Training data."")\ntf.app.flags.DEFINE_integer(""max_train_data_size"", 0,\n                            ""Limit on the size of training data (0: no limit)."")\ntf.app.flags.DEFINE_integer(""steps_per_checkpoint"", 200,\n                            ""How many training steps to do per checkpoint."")\ntf.app.flags.DEFINE_boolean(""decode"", False,\n                            ""Set to True for interactive decoding."")\ntf.app.flags.DEFINE_boolean(""self_test"", False,\n                            ""Run a self-test if this is set to True."")\ntf.app.flags.DEFINE_boolean(""use_fp16"", False,\n                            ""Train using fp16 instead of fp32."")\n\nFLAGS = tf.app.flags.FLAGS\n\n# We use a number of buckets and pad to the closest one for efficiency.\n# See seq2seq_model.Seq2SeqModel for details of how they work.\n_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n\n\ndef read_data(source_path, target_path, max_size=None):\n  """"""Read data from source and target files and put into buckets.\n\n  Args:\n    source_path: path to the files with token-ids for the source language.\n    target_path: path to the file with token-ids for the target language;\n      it must be aligned with the source file: n-th line contains the desired\n      output for n-th line from the source_path.\n    max_size: maximum number of lines to read, all other will be ignored;\n      if 0 or None, data files will be read completely (no limit).\n\n  Returns:\n    data_set: a list of length len(_buckets); data_set[n] contains a list of\n      (source, target) pairs read from the provided data files that fit\n      into the n-th bucket, i.e., such that len(source) < _buckets[n][0] and\n      len(target) < _buckets[n][1]; source and target are lists of token-ids.\n  """"""\n  data_set = [[] for _ in _buckets]\n  with tf.gfile.GFile(source_path, mode=""r"") as source_file:\n    with tf.gfile.GFile(target_path, mode=""r"") as target_file:\n      source, target = source_file.readline(), target_file.readline()\n      counter = 0\n      while source and target and (not max_size or counter < max_size):\n        counter += 1\n        if counter % 100000 == 0:\n          print(""  reading data line %d"" % counter)\n          sys.stdout.flush()\n        source_ids = [int(x) for x in source.split()]\n        target_ids = [int(x) for x in target.split()]\n        # The EOS flag will be added when a batch is prepared\n        # target_ids.append(data_utils.EOS_ID)\n        for bucket_id, (source_size, target_size) in enumerate(_buckets):\n          if len(source_ids) < source_size and len(target_ids) < target_size:\n            data_set[bucket_id].append([source_ids, target_ids])\n            break\n        source, target = source_file.readline(), target_file.readline()\n  return data_set\n\n\ndef create_model(session, forward_only):\n  """"""Create translation model and initialize or load parameters in session.""""""\n  dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n  model = tf_seq2seq_model_example.Seq2SeqModel(\n    FLAGS.from_vocab_size,\n    FLAGS.to_vocab_size,\n    _buckets,\n    FLAGS.size,\n    FLAGS.num_layers,\n    FLAGS.max_gradient_norm,\n    FLAGS.batch_size,\n    FLAGS.learning_rate,\n    FLAGS.learning_rate_decay_factor,\n    forward_only=forward_only,\n    dtype=dtype)\n  ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n  if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n    print(""Reading model parameters from %s"" % ckpt.model_checkpoint_path)\n    model.saver.restore(session, ckpt.model_checkpoint_path)\n  else:\n    print(""Created model with fresh parameters."")\n    session.run(tf.global_variables_initializer())\n  return model\n\n\ndef train():\n  """"""Train a en->fr translation model using WMT data.""""""\n  # from_train = None\n  # to_train = None\n  # from_dev = None\n  # to_dev = None\n  if FLAGS.from_train_data and FLAGS.to_train_data:\n    from_train_data = FLAGS.from_train_data\n    to_train_data = FLAGS.to_train_data\n    from_dev_data = from_train_data\n    to_dev_data = to_train_data\n    if FLAGS.from_dev_data and FLAGS.to_dev_data:\n      from_dev_data = FLAGS.from_dev_data\n      to_dev_data = FLAGS.to_dev_data\n    from_train, to_train, from_dev, to_dev, _, _ = data_utils.prepare_data(\n      FLAGS.data_dir,\n      from_train_data,\n      to_train_data,\n      from_dev_data,\n      to_dev_data,\n      FLAGS.from_vocab_size,\n      FLAGS.to_vocab_size)\n  else:\n    # Prepare WMT data.\n    print(""Preparing WMT data in %s"" % FLAGS.data_dir)\n    from_train, to_train, from_dev, to_dev, _, _ = data_utils.prepare_wmt_data(\n      FLAGS.data_dir, FLAGS.from_vocab_size, FLAGS.to_vocab_size)\n\n  with tf.Session() as sess:\n    # Create model.\n    print(""Creating %d layers of %d units."" % (FLAGS.num_layers, FLAGS.size))\n    model = create_model(sess, False)\n\n    # Read data into buckets and compute their sizes.\n    print(""Reading development and training data (limit: %d).""\n          % FLAGS.max_train_data_size)\n    dev_set = read_data(from_dev, to_dev)\n    train_set = read_data(from_train, to_train, FLAGS.max_train_data_size)\n    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n    train_total_size = float(sum(train_bucket_sizes))\n\n    # A bucket scale is a list of increasing numbers from 0 to 1 that we\'ll use\n    # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n    # the size if i-th training bucket, as used later.\n    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n                           for i in xrange(len(train_bucket_sizes))]\n\n    # This is the training loop.\n    step_time, loss = 0.0, 0.0\n    current_step = 0\n    previous_losses = []\n    while True:\n      # Choose a bucket according to data distribution. We pick a random number\n      # in [0, 1] and use the corresponding interval in train_buckets_scale.\n      random_number_01 = np.random.random_sample()\n      bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                       if train_buckets_scale[i] > random_number_01])\n\n      # Get a batch and make a step.\n      start_time = time.time()\n      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n        train_set, bucket_id)\n      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n                                   target_weights, bucket_id, False)\n      step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n      loss += step_loss / FLAGS.steps_per_checkpoint\n      current_step += 1\n\n      # Once in a while, we save checkpoint, print statistics, and run evals.\n      if current_step % FLAGS.steps_per_checkpoint == 0:\n        # Print statistics for the previous epoch.\n        perplexity = math.exp(float(loss)) if loss < 300 else float(""inf"")\n        print(""global step %d learning rate %.4f step-time %.2f perplexity ""\n              ""%.2f"" % (model.global_step.eval(), model.learning_rate.eval(),\n                        step_time, perplexity))\n        # Decrease learning rate if no improvement was seen over last 3 times.\n        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n          sess.run(model.learning_rate_decay_op)\n        previous_losses.append(loss)\n        # Save checkpoint and zero timer and loss.\n        checkpoint_path = os.path.join(FLAGS.train_dir, ""translate.ckpt"")\n        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n        step_time, loss = 0.0, 0.0\n        # Run evals on development set and print their perplexity.\n        for bucket_id in xrange(len(_buckets)):\n          if len(dev_set[bucket_id]) == 0:\n            print(""  eval: empty bucket %d"" % bucket_id)\n            continue\n          encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n            dev_set, bucket_id)\n          _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n                                       target_weights, bucket_id, True)\n          eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\n            ""inf"")\n          print(""  eval: bucket %d perplexity %.2f"" % (bucket_id, eval_ppx))\n        sys.stdout.flush()\n\n\ndef decode():\n  with tf.Session() as sess:\n    # Create model and load parameters.\n    model = create_model(sess, True)\n    model.batch_size = 1  # We decode one sentence at a time.\n\n    # Load vocabularies.\n    en_vocab_path = os.path.join(FLAGS.data_dir,\n                                 ""vocab%d.from"" % FLAGS.from_vocab_size)\n    fr_vocab_path = os.path.join(FLAGS.data_dir,\n                                 ""vocab%d.to"" % FLAGS.to_vocab_size)\n    en_vocab, _ = data_utils.initialize_vocabulary(en_vocab_path)\n    _, rev_fr_vocab = data_utils.initialize_vocabulary(fr_vocab_path)\n\n    # Decode from standard input.\n    sys.stdout.write(""> "")\n    sys.stdout.flush()\n    sentence = sys.stdin.readline()\n    while sentence:\n      # Get token-ids for the input sentence.\n      token_ids = data_utils.sentence_to_token_ids(tf.compat.as_bytes(sentence),\n                                                   en_vocab)\n      # Which bucket does it belong to?\n      bucket_id = len(_buckets) - 1\n      for i, bucket in enumerate(_buckets):\n        if bucket[0] >= len(token_ids):\n          bucket_id = i\n          break\n      else:\n        logging.warning(""Sentence truncated: %s"", sentence)\n\n      # Get a 1-element batch to feed the sentence to the model.\n      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n        {bucket_id: [(token_ids, [])]}, bucket_id)\n      # Get output logits for the sentence.\n      _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n                                       target_weights, bucket_id, True)\n      # This is a greedy decoder - outputs are just argmaxes of output_logits.\n      outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n      # If there is an EOS symbol in outputs, cut them at that point.\n      if data_utils.EOS_ID in outputs:\n        outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n      # Print out French sentence corresponding to outputs.\n      print("" "".join(\n        [tf.compat.as_str(rev_fr_vocab[output]) for output in outputs]))\n      print(""> "", end="""")\n      sys.stdout.flush()\n      sentence = sys.stdin.readline()\n\n\ndef self_test():\n  """"""Test the translation model.""""""\n  with tf.Session() as sess:\n    print(""Self-test for neural translation model."")\n    # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n    model = tf_seq2seq_model_example.Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2,\n                                                  5.0, 32, 0.3, 0.99, num_samples=8,\n                                                  use_lstm=True)\n    sess.run(tf.global_variables_initializer())\n\n    # Fake data set for both the (3, 3) and (6, 6) bucket.\n    data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n                [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n    for _ in xrange(5):  # Train the fake model for 5 steps.\n      bucket_id = random.choice([0, 1])\n      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n        data_set, bucket_id)\n      model.step(sess, encoder_inputs, decoder_inputs, target_weights,\n                 bucket_id, False)\n\n\ndef main(_):\n  if FLAGS.self_test:\n    self_test()\n  elif FLAGS.decode:\n    decode()\n  else:\n    train()\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
tf_seq2seq_model_example.py,6,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Sequence-to-sequence model with an attention mechanism.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\n\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nimport data_utils\n\n\nclass Seq2SeqModel(object):\n  """"""Sequence-to-sequence model with attention and for multiple buckets.\n\n  This class implements a multi-layer recurrent neural network as encoder,\n  and an attention-based decoder. This is the same as the model described in\n  this paper: http://arxiv.org/abs/1412.7449 - please look there for details,\n  or into the seq2seq library for complete model implementation.\n  This class also allows to use GRU cells in addition to LSTM cells, and\n  sampled softmax to handle large output vocabulary size. A single-layer\n  version of this model, but with bi-directional encoder, was presented in\n    http://arxiv.org/abs/1409.0473\n  and sampled softmax is described in Section 3 of the following paper.\n    http://arxiv.org/abs/1412.2007\n  """"""\n\n  def __init__(self,\n               source_vocab_size,\n               target_vocab_size,\n               buckets,\n               size,\n               num_layers,\n               max_gradient_norm,\n               batch_size,\n               learning_rate,\n               learning_rate_decay_factor,\n               use_lstm=False,\n               num_samples=512,\n               forward_only=False,\n               dtype=tf.float32):\n    """"""Create the model.\n\n    Args:\n      source_vocab_size: size of the source vocabulary.\n      target_vocab_size: size of the target vocabulary.\n      buckets: a list of pairs (I, O), where I specifies maximum input length\n        that will be processed in that bucket, and O specifies maximum output\n        length. Training instances that have inputs longer than I or outputs\n        longer than O will be pushed to the next bucket and padded accordingly.\n        We assume that the list is sorted, e.g., [(2, 4), (8, 16)].\n      size: number of units in each layer of the model.\n      num_layers: number of layers in the model.\n      max_gradient_norm: gradients will be clipped to maximally this norm.\n      batch_size: the size of the batches used during training;\n        the model construction is independent of batch_size, so it can be\n        changed after initialization if this is convenient, e.g., for decoding.\n      learning_rate: learning rate to start with.\n      learning_rate_decay_factor: decay learning rate by this much when needed.\n      use_lstm: if true, we use LSTM cells instead of GRU cells.\n      num_samples: number of samples for sampled softmax.\n      forward_only: if set, we do not construct the backward pass in the model.\n      dtype: the data type to use to store internal variables.\n    """"""\n    self.source_vocab_size = source_vocab_size\n    self.target_vocab_size = target_vocab_size\n    self.buckets = buckets\n    self.batch_size = batch_size\n    self.learning_rate = tf.Variable(\n      float(learning_rate), trainable=False, dtype=dtype)\n    self.learning_rate_decay_op = self.learning_rate.assign(\n      self.learning_rate * learning_rate_decay_factor)\n    self.global_step = tf.Variable(0, trainable=False)\n\n    # # TODO Find out if we need this piece of code\n    # # If we use sampled softmax, we need an output projection.\n    # output_projection = None\n    # softmax_loss_function = None\n    # # Sampled softmax only makes sense if we sample less than vocabulary size.\n    # # if num_samples > 0 and num_samples < self.target_vocab_size:\n    # if 0 < num_samples < self.target_vocab_size:\n    #   w_t = tf.get_variable(""proj_w"", [self.target_vocab_size, size],\n    #                         dtype=dtype)\n    #   w = tf.transpose(w_t)\n    #   b = tf.get_variable(""proj_b"", [self.target_vocab_size], dtype=dtype)\n    #   output_projection = (w, b)\n    #\n    #   def sampled_loss(labels, inputs):\n    #     labels = tf.reshape(labels, [-1, 1])\n    #     # We need to compute the sampled_softmax_loss using 32bit floats to\n    #     # avoid numerical instabilities.\n    #     local_w_t = tf.cast(w_t, tf.float32)\n    #     local_b = tf.cast(b, tf.float32)\n    #     local_inputs = tf.cast(inputs, tf.float32)\n    #     return tf.cast(\n    #       tf.nn.sampled_softmax_loss(\n    #         weights=local_w_t,\n    #         biases=local_b,\n    #         labels=labels,\n    #         inputs=local_inputs,\n    #         num_sampled=num_samples,\n    #         num_classes=self.target_vocab_size),\n    #       dtype)\n    #\n    #   softmax_loss_function = sampled_loss\n    def mse_loss(labels, inputs):\n      # labels = tf.reshape(labels, [-1, 2])\n      print(labels.get_shape())\n      print(inputs.get_shape())\n      return tf.ones([1])\n\n    # Create the internal multi-layer cell for our RNN.\n    def single_cell():\n      return tf.contrib.rnn.GRUCell(size)\n\n    if use_lstm:\n      def single_cell():\n        return tf.contrib.rnn.BasicLSTMCell(size)\n    cell = single_cell()\n    if num_layers > 1:\n      cell = tf.contrib.rnn.MultiRNNCell(\n        [single_cell() for _ in range(num_layers)])\n\n    # The seq2seq function: we use embedding for the input and attention.\n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n      from tensorflow.contrib.legacy_seq2seq.python.ops.seq2seq import \\\n        basic_rnn_seq2seq\n      # return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n      # return tf.contrib.seq2seq.python.ops.seq2seq.basic_rnn_seq2seq(\n      return basic_rnn_seq2seq(\n        encoder_inputs,\n        decoder_inputs,\n        cell,\n        # num_encoder_symbols=source_vocab_size,\n        # num_decoder_symbols=target_vocab_size,\n        # embedding_size=size,\n        # output_projection=output_projection,\n        # feed_previous=do_decode,\n        dtype=dtype)\n\n    # Feeds for inputs.\n    self.encoder_inputs = []\n    self.decoder_inputs = []\n    self.target_weights = []\n    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n      self.encoder_inputs.append(tf.placeholder(tf.float32, shape=[None, 1],\n                                                name=""encoder{0}"".format(i)))\n    for i in xrange(buckets[-1][1] + 1):\n      self.decoder_inputs.append(tf.placeholder(tf.float32, shape=[None, 2],\n                                                name=""decoder{0}"".format(i)))\n      # TODO Maybe the weights ought to be (None, 2), too\n      self.target_weights.append(tf.placeholder(dtype, shape=[None, 1],\n                                                name=""weight{0}"".format(i)))\n\n    # Our targets are decoder inputs shifted by one.\n    targets = [self.decoder_inputs[i + 1]\n               for i in xrange(len(self.decoder_inputs) - 1)]\n\n    # Training outputs and losses.\n    if forward_only:\n      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n        self.encoder_inputs, self.decoder_inputs, targets,\n        self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n        softmax_loss_function=softmax_loss_function, dtype=tf.float32)\n      # If we use output projection, we need to project outputs for decoding.\n      if output_projection is not None:\n        for b in xrange(len(buckets)):\n          self.outputs[b] = [\n            tf.matmul(output, output_projection[0]) + output_projection[1]\n            for output in self.outputs[b]\n          ]\n    else:\n      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n        self.encoder_inputs, self.decoder_inputs, targets,\n        self.target_weights, buckets,\n        lambda x, y: seq2seq_f(x, y, False),\n        softmax_loss_function=mse_loss)\n\n    # Gradients and SGD update operation for training the model.\n    params = tf.trainable_variables()\n    if not forward_only:\n      self.gradient_norms = []\n      self.updates = []\n      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n      for b in xrange(len(buckets)):\n        gradients = tf.gradients(self.losses[b], params)\n        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n                                                         max_gradient_norm)\n        self.gradient_norms.append(norm)\n        self.updates.append(opt.apply_gradients(\n          zip(clipped_gradients, params), global_step=self.global_step))\n\n    self.saver = tf.train.Saver(tf.global_variables())\n\n  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n           bucket_id, forward_only):\n    """"""Run a step of the model feeding the given inputs.\n\n    Args:\n      session: tensorflow session to use.\n      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n      decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n      target_weights: list of numpy float vectors to feed as target weights.\n      bucket_id: which bucket of the model to use.\n      forward_only: whether to do the backward step or only forward.\n\n    Returns:\n      A triple consisting of gradient norm (or None if we did not do backward),\n      average perplexity, and the outputs.\n\n    Raises:\n      ValueError: if length of encoder_inputs, decoder_inputs, or\n        target_weights disagrees with bucket size for the specified bucket_id.\n    """"""\n    # Check if the sizes match.\n    encoder_size, decoder_size = self.buckets[bucket_id]\n    if len(encoder_inputs) != encoder_size:\n      raise ValueError(""Encoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(encoder_inputs), encoder_size))\n    if len(decoder_inputs) != decoder_size:\n      raise ValueError(""Decoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(decoder_inputs), decoder_size))\n    if len(target_weights) != decoder_size:\n      raise ValueError(""Weights length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(target_weights), decoder_size))\n\n    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n    input_feed = {}\n    for l in xrange(encoder_size):\n      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n    for l in xrange(decoder_size):\n      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n      input_feed[self.target_weights[l].name] = target_weights[l]\n\n    # Since our targets are decoder inputs shifted by one, we need one more.\n    last_target = self.decoder_inputs[decoder_size].name\n    input_feed[last_target] = np.zeros([self.batch_size, 2], dtype=np.float32)\n\n    # Output feed: depends on whether we do a backward step or not.\n    if not forward_only:\n      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                     self.gradient_norms[bucket_id],  # Gradient norm.\n                     self.losses[bucket_id]]  # Loss for this batch.\n    else:\n      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n      for l in xrange(decoder_size):  # Output logits.\n        output_feed.append(self.outputs[bucket_id][l])\n\n    outputs = session.run(output_feed, input_feed)\n    if not forward_only:\n      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n    else:\n      return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n\n  def get_batch(self, data, bucket_id):\n    """"""Get a random batch of data from the specified bucket, prepare for step.\n\n    To feed data in step(..) it must be a list of batch-major vectors, while\n    data here contains single length-major cases. So the main logic of this\n    function is to re-index data cases to be in the proper format for feeding.\n\n    Args:\n      data: a tuple of size len(self.buckets) in which each element contains\n        lists of pairs of input and output data that we use to create a batch.\n      bucket_id: integer, which bucket to get the batch for.\n\n    Returns:\n      The triple (encoder_inputs, decoder_inputs, target_weights) for\n      the constructed batch that has the proper format to call step(...) later.\n    """"""\n    encoder_size, decoder_size = self.buckets[bucket_id]\n    encoder_inputs, decoder_inputs = [], []\n\n    # Get a random batch of encoder and decoder inputs from data,\n    # pad them if needed, reverse encoder inputs and add GO to decoder.\n    for i in xrange(self.batch_size):\n      encoder_input, decoder_input = random.choice(data[bucket_id])\n\n      # Encoder inputs are padded and then reversed.\n      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n\n      # Decoder inputs get an extra ""GO"" symbol, and are padded then.\n      decoder_pad_size = decoder_size - len(decoder_input)\n      # decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n      decoder_inputs.append(\n        # TODO Comprovar si es pot fer [data_utils.GO_ID, in_val]\n        [[data_utils.GO_ID] + [tf.cast(in_val, tf.int32)] for in_val in\n         decoder_input] +\n        [[data_utils.PAD_ID] * 2] * decoder_pad_size)\n\n      # Set last decoder input\'s flag to EOS\n      decoder_inputs[i][len(decoder_input) - 1][0] = data_utils.EOS_ID\n\n    # Now we create batch-major vectors from the data selected above.\n    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n\n    # Batch encoder inputs are just re-indexed encoder_inputs.\n    for length_idx in xrange(encoder_size):\n      batch_encoder_inputs.append(\n        np.array([encoder_inputs[batch_idx][length_idx]\n                  for batch_idx in xrange(self.batch_size)], dtype=np.float32))\n      batch_encoder_inputs[-1] = batch_encoder_inputs[-1].reshape(-1, 1)\n\n    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n    for length_idx in xrange(decoder_size):\n      batch_decoder_inputs.append(\n        np.array([decoder_inputs[batch_idx][length_idx]\n                  for batch_idx in xrange(self.batch_size)], dtype=np.float32))\n\n      # Create target_weights to be 0 for targets that are padding.\n      # TODO Fix shape of weights\n      batch_weight = np.ones(self.batch_size, dtype=np.float32)\n      for batch_idx in xrange(self.batch_size):\n        # We set weight to 0 if the corresponding target is a PAD symbol.\n        # The corresponding target is decoder_input shifted by 1 forward.\n        if length_idx < decoder_size - 1:\n          target = decoder_inputs[batch_idx][length_idx + 1]\n        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n          batch_weight[batch_idx] = 0.0\n      batch_weights.append(batch_weight.reshape(-1, 1))\n    return batch_encoder_inputs, batch_decoder_inputs, batch_weights\n'"
tf_test.py,7,"b'# import numpy as np\n# import tensorflow.contrib.legacy_seq2seq as tf_seq2seq\n# from tensorflow.contrib.rnn.python.ops import core_rnn, core_rnn_cell\n#\n#\n# def decoder_loop_function(prev, i):\n#   # TODO Change Placeholder call for actual function\n#   return prev\n#\n#\n# cell = core_rnn_cell.LSTMCell(256)\n#\n#\n# # Try encapsulating encoder & decoder in functions, to be run in a Lambda layer\n# def tf_seq2seq_encoder(encoder_inputs):\n#   _, enc_state = core_rnn.static_rnn(\n#     cell,\n#     encoder_inputs,\n#     dtype=float\n#   )\n#\n#   return enc_state\n#\n#\n# # Initialize Keras input\n# # Initialize decoder input (loop_function?)\n# enc_state_f = tf_seq2seq_encoder(np.zeros((1, 5, 5)))\n#\n#\n# def tf_seq2seq_attention_decoder(decoder_inputs):\n#   (outputs, state) = tf_seq2seq.attention_decoder(\n#     decoder_inputs,\n#     enc_state_f,\n#     cell,\n#     loop_function=decoder_loop_function\n#   )\n#\n#   return outputs\n#\n#\n# (dec_output, dec_state) = tf_seq2seq_attention_decoder(np.zeros((1, 5)))\n#\n################################################################################\n\n# import tensorflow as tf\n#\n# # Model parameters\n# W = tf.Variable([.3], tf.float32)\n# b = tf.Variable([-.3], tf.float32)\n# # Model input and output\n# x = tf.placeholder(tf.float32)\n# linear_model = W * x + b\n# y = tf.placeholder(tf.float32)\n# # loss\n# loss = tf.reduce_sum(tf.square(linear_model - y))  # sum of the squares\n# # optimizer\n# optimizer = tf.train.GradientDescentOptimizer(0.01)\n# train = optimizer.minimize(loss)\n# # training data\n# x_train = [1, 2, 3, 4]\n# y_train = [0, -1, -2, -3]\n# # training loop\n# init = tf.global_variables_initializer()\n# sess = tf.Session()\n# sess.run(init)  # reset values to wrong\n# for i in range(1000):\n#   sess.run(train, {x: x_train, y: y_train})\n#\n# # evaluate training accuracy\n# curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\n# print(""W: %s b: %s loss: %s"" % (curr_W, curr_b, curr_loss))\n#\n# # Build a dataflow graph.\n# c = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n# d = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n# e = tf.matmul(c, d)\n#\n# # Construct a `Session` to execute the graph.\n# sess = tf.Session()\n#\n# # Execute the graph and store the value that `e` represents in `result`.\n# result = sess.run(e)\n#\n# ################################################################################\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.legacy_seq2seq.python.ops import seq2seq as tf_s2s\nfrom tensorflow.contrib.rnn.python import ops as tf_rnn\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\n# TODO Input these variables as TF Tensors\nbatch_size = 2\ntimesteps = 5\n\n# tensor_enc = tf.placeholder(\n#   dtype=tf.float32,\n#   shape=(batch_size, timesteps, 4),\n#   name=\'encoder_in\')\n# tensor_dec = tf.placeholder(\n#   dtype=tf.float32,\n#   shape=(batch_size, timesteps, 3),\n#   name=\'decoder_in\')\ntensor_enc = [tf.placeholder(\n  dtype=tf.float32,shape=(timesteps, 4)) for _ in xrange(batch_size)]\ntensor_dec = [tf.placeholder(\n  dtype=tf.float32,shape=(timesteps, 3)) for _ in xrange(batch_size)]\n\n# print(\'logging\')\nimport logging\nlogging.basicConfig(format=\'%(levelname)s:%(message)s\', level=logging.INFO)\n\n# logger = logging.getLogger()\n# logger.setLevel(logging.INFO)\nlogging.info(""test"")\n# logger = logging.getLogger()\n# logger.setLevel(logging.INFO)\nlogging.info(\'printing shape of tensor_enc\')\nlogging.info(tensor_enc[0].get_shape())\n# print(tensor_enc[0].get_shape())\n# print(\'logging\')\n\n# params_enc = np.zeros((4, 1))  # [0] * 4\n# params_dec = np.zeros((3, 1))  # [0] * 3\n#\n# frames_enc = [params_enc] * timesteps\n# frames_dec = [params_dec] * timesteps\n#\n# seqs_enc = [frames_enc] * batch_size\n# seqs_dec = [frames_dec] * batch_size\nseqs_enc = [np.random.rand(timesteps, 4) for _ in xrange(batch_size)]\nseqs_dec = [np.random.rand(timesteps, 3) for _ in xrange(batch_size)]\n\ns2s = tf_s2s.basic_rnn_seq2seq(\n  tensor_enc,\n  tensor_dec,\n  tf_rnn.core_rnn_cell.LSTMCell(256)\n)\nwith tf.Session() as sess:\n  # feed_dict = {tensor_enc:seqs_enc, tensor_dec: seqs_dec}\n  feed_dict = {}\n  for i, d in zip(tensor_enc, seqs_enc):\n    feed_dict[i] = d\n  for i, d in zip(tensor_dec, seqs_dec):\n    feed_dict[i] = d\n\n  init = tf.global_variables_initializer()\n  sess.run(init)\n  basic_seq2seq = sess.run(s2s, feed_dict=feed_dict) # tensor_enc: seqs_enc, tensor_dec: seqs_dec})\n\n  # print(basic_seq2seq)\n\n################################################################################\n# # Test decoder inputs\n# import numpy as np\n# from six.moves import xrange\n#\n# batch_size = 3\n# decoder_size = 7\n# GO_ID = \'GO\'\n# PAD_ID = \'PAD\'\n# EOS_ID = \'EOS\'\n#\n# decoder_batch = [[\'a\', \'b\', \'c\', \'d\'], [\'e\', \'f\', \'g\', \'h\'],\n#                  [\'i\', \'j\', \'k\', \'l\']]\n# decoder_inputs = []\n#\n# for i in xrange(batch_size):\n#   # Decoder inputs get an extra ""GO"" symbol, and are padded then.\n#   decoder_input = decoder_batch[i]\n#   decoder_pad_size = decoder_size - len(decoder_input)\n#   # decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n#   decoder_inputs.append(\n#     [[GO_ID] + [in_val] for in_val in decoder_input] +\n#     [[PAD_ID] * 2] * decoder_pad_size)\n#\n#   # Set last decoder input\'s flag to EOS\n#   decoder_inputs[i][len(decoder_input) - 1][0] = EOS_ID\n#\n# # print(decoder_inputs)\n#\n# batch_decoder_inputs = []\n#\n# for length_idx in xrange(decoder_size):\n#   batch_decoder_inputs.append(\n#     np.array([decoder_inputs[batch_idx][length_idx]\n#               for batch_idx in xrange(batch_size)]))\n#\n# print(batch_decoder_inputs)'"
