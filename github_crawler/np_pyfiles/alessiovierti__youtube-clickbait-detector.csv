file_path,api_count,code
predict.py,7,"b'import argparse\nimport re\nimport emoji\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom gensim.parsing.preprocessing import *\n\n\ndef tokenize(string):\n\n    """""" Tokenizes a string.\n\n    Adds a space between numbers and letters, removes punctuation, repeated whitespaces, words\n    shorter than 2 characters, and stop-words. Returns a list of stems and, eventually, emojis.\n\n    @param string: String to tokenize.\n    @return: A list of stems and emojis.\n    """"""\n\n    # Based on the Ranks NL (Google) stopwords list, but ""how"" and ""will"" are not stripped, and\n    # words shorter than 2 characters are not checked (since they are stripped):\n    stop_words = [\n        ""about"", ""an"", ""are"", ""as"", ""at"", ""be"", ""by"", ""com"", ""for"", ""from"", ""in"", ""is"", ""it"", ""of"",\n        ""on"", ""or"", ""that"", ""the"", ""this"", ""to"", ""was"", ""what"", ""when"", ""where"", ""who"", ""with"",\n        ""the"", ""www""\n    ]\n\n    string = strip_short(\n        strip_multiple_whitespaces(\n            strip_punctuation(\n                split_alphanum(string))),\n        minsize=2)\n    # Parse emojis:\n    emojis = [c for c in string if c in emoji.UNICODE_EMOJI]\n    # Remove every non-word character and stem each word:\n    string = stem_text(re.sub(r""[^\\w\\s,]"", """", string))\n    # List of stems and emojis:\n    tokens = string.split() + emojis\n\n    for stop_word in stop_words:\n        try:\n            tokens.remove(stop_word)\n        except:\n            pass\n\n    return tokens\n\n\ndef average_embedding(tokens, word2vec, na_vector=None):\n\n    """""" Embeds a title with the average representation of its tokens.\n\n    Returns the mean vector representation of the tokens representations. When no token is in the\n    Word2Vec model, it can be provided a vector to use instead (for example the mean vector\n    representation of the train set titles).\n\n    @param tokens: List of tokens to embed.\n    @param word2vec: Word2Vec model.\n    @param na_vector: Vector representation to use when no token is in the Word2Vec model.\n    @return: A vector representation for the token list.\n    """"""\n\n    vectors = list()\n\n    for token in tokens:\n        if token in word2vec:\n            vectors.append(word2vec[token])\n\n    if len(vectors) == 0 and na_vector is not None:\n        vectors.append(na_vector)\n\n    return np.mean(np.array(vectors), axis=0)\n\n\nparser = argparse.ArgumentParser(description=""Predict if a Youtube video is clickbait or not."")\nparser.add_argument(\n    ""--title"", ""-t"",\n    type=str, help=""Title."", required=True)\nparser.add_argument(\n    ""--views"", ""-v"",\n    type=int, help=""Number of views."", required=False)\nparser.add_argument(\n    ""--likes"", ""-l"",\n    type=int, help=""Number of likes."", required=False)\nparser.add_argument(\n    ""--dislikes"", ""-d"",\n    type=int, help=""Number of dislikes."", required=False)\nparser.add_argument(\n    ""--comments"", ""-c"",\n    type=int, help=""Number of comments."", required=False)\nargs = parser.parse_args()\n\n\n# Import the Word2Vec model and the mean vector representation computed on the train set:\nword2vec = pickle.load(open(""word2vec"", ""rb""))\nmean_title_embedding = pickle.load(open(""mean-title-embedding"", ""rb""))\n\ninput = {\n    ""video_title"": args.title,\n    ""video_views"": args.views if args.views is not None else np.NaN,\n    ""video_likes"": args.likes if args.likes is not None else np.NaN,\n    ""video_dislikes"": args.dislikes if args.dislikes is not None else np.NaN,\n    ""video_comments"": args.comments if args.comments is not None else np.NaN,\n}\nsample = pd.DataFrame([ input ])\n\n# Tokenize the title and then compute its embedding:\nsample[""video_title""] = sample[""video_title""].apply(tokenize)\nsample[""video_title""] = sample[""video_title""].apply(\n    average_embedding, word2vec=word2vec, na_vector=mean_title_embedding)\nsample = pd.concat(\n    [\n        sample[[""video_views"", ""video_likes"", ""video_dislikes"", ""video_comments""]],\n        sample[""video_title""].apply(pd.Series)\n    ], axis=1)\n\n# Compute the log of the video metadata or replace the missing values with the mean values obtained\n# from the train set:\nmean_log_video_views = pickle.load(open(""mean-log-video-views"", ""rb""))\nmean_log_video_likes = pickle.load(open(""mean-log-video-likes"", ""rb""))\nmean_log_video_dislikes = pickle.load(open(""mean-log-video-dislikes"", ""rb""))\nmean_log_video_comments = pickle.load(open(""mean-log-video-comments"", ""rb""))\n\nsample[[""video_views"", ""video_likes"", ""video_dislikes"", ""video_comments""]] = \\\n    sample[[""video_views"", ""video_likes"", ""video_dislikes"", ""video_comments""]].apply(np.log)\n\nif sample[""video_views""].isnull().any():\n    sample[""video_views""].fillna(mean_log_video_views, inplace=True)\nif sample[""video_likes""].isnull().any():\n    sample[""video_likes""].fillna(mean_log_video_likes, inplace=True)\nif sample[""video_dislikes""].isnull().any():\n    sample[""video_dislikes""].fillna(mean_log_video_dislikes, inplace=True)\nif sample[""video_comments""].isnull().any():\n    sample[""video_comments""].fillna(mean_log_video_comments, inplace=True)\n\n# Replace any -Inf value with 0:\nsample = sample.replace(-np.inf, 0)\n\n# Import the min-max scaler and apply it to the sample:\nmin_max_scaler = pickle.load(open(""min-max-scaler"", ""rb""))\nsample = pd.DataFrame(min_max_scaler.transform(sample), columns=sample.columns)\n\n# Import the SVM model:\nsvm = pickle.load(open(""svm"", ""rb""))\n\n# Print its prediction:\nprint(svm.predict(sample)[0])\n'"
