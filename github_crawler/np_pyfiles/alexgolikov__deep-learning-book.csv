file_path,api_count,code
2_6_linear_regression.py,4,"b'"""""" \xd0\xa7\xd0\xb0\xd1\x81\xd1\x82\xd1\x8c I. \xd0\x93\xd0\xbb\xd0\xb0\xd0\xb2\xd0\xb0 2. \xd0\xa0\xd0\xb0\xd0\xb7\xd0\xb4\xd0\xb5\xd0\xbb 2.6. \xd0\x9b\xd0\xb8\xd0\xbd\xd0\xb5\xd0\xb9\xd0\xbd\xd0\xb0\xd1\x8f \xd1\x80\xd0\xb5\xd0\xb3\xd1\x80\xd0\xb5\xd1\x81\xd1\x81\xd0\xb8\xd1\x8f""""""\n\nimport numpy as np\nimport tensorflow as tf\n\n# \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xba\xd0\xbe\xd0\xbd\xd1\x81\xd1\x82\xd0\xb0\xd0\xbd\xd1\x82\xd1\x8b\nn_samples, batch_size, num_steps = 1000, 100, 20000\n\n# \xd0\xbd\xd0\xb0\xd0\xb1\xd1\x80\xd0\xb0\xd1\x81\xd1\x8b\xd0\xb2\xd0\xb0\xd0\xb5\xd0\xbc n_samples \xd1\x81\xd0\xbb\xd1\x83\xd1\x87\xd0\xb0\xd0\xb9\xd0\xbd\xd1\x8b\xd1\x85 \xd1\x82\xd0\xbe\xd1\x87\xd0\xb5\xd0\xba \xd1\x80\xd0\xb0\xd0\xb2\xd0\xbd\xd0\xbe\xd0\xbc\xd0\xb5\xd1\x80\xd0\xbd\xd0\xbe \xd0\xbd\xd0\xb0 \xd0\xb8\xd0\xbd\xd1\x82\xd0\xb5\xd1\x80\xd0\xb2\xd0\xb0\xd0\xbb\xd0\xb5 [0; 1]\nX_data = np.random.uniform(0, 1, (n_samples, 1))\n# \xd0\xbf\xd0\xbe\xd0\xb4\xd1\x81\xd1\x87\xd0\xb8\xd1\x82\xd1\x8b\xd0\xb2\xd0\xb0\xd0\xb5\xd0\xbc ""\xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b""\xd0\xbf\xd0\xbe \xd1\x84\xd0\xbe\xd1\x80\xd0\xbc\xd1\x83\xd0\xbb\xd0\xb5 y = 2x+1+e, \xd0\xb3\xd0\xb4\xd0\xb5 \xd0\xb5 - \xd1\x81\xd0\xbb\xd1\x83\xd1\x87\xd0\xb0\xd0\xb9\xd0\xbd\xd0\xbe \xd1\x80\xd0\xb0\xd1\x81\xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x88\xd1\x83\xd0\xbc \xd1\x81 \xd0\xb4\xd0\xb8\xd1\x81\xd0\xbf\xd0\xb5\xd1\x80\xd1\x81\xd0\xb8\xd0\xb5\xd0\xb9 0.2\ny_data = 2 * X_data + 1 + np.random.normal(0, 0.2, (n_samples, 1))\n\n# \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xb7\xd0\xb0\xd0\xb3\xd0\xbb\xd1\x83\xd1\x88\xd0\xba\xd0\xb8, \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd1\x80\xd0\xb0\xd0\xb7\xd0\xbc\xd0\xb5\xd1\x80\xd0\xbd\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c\nX = tf.placeholder(tf.float32, shape=(batch_size, 1))\ny = tf.placeholder(tf.float32, shape=(batch_size, 1))\n\n# \xd0\xb8\xd0\xbd\xd0\xb8\xd1\x86\xd0\xb8\xd0\xb0\xd0\xbb\xd0\xb8\xd0\xb7\xd0\xb8\xd1\x80\xd1\x83\xd0\xb5\xd0\xbc \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb5 k \xd0\xb8 b\nwith tf.variable_scope(\'linear-regression\'):\n    k = tf.Variable(tf.random_normal((1, 1), stddev=0.01), name=\'slope\')\n    b = tf.Variable(tf.zeros(1,), name=\'bias\')\n\n# \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xbc\xd0\xbe\xd0\xb4\xd0\xb5\xd0\xbb\xd1\x8c\ny_pred = tf.matmul(X, k) + b\n\n# \xd1\x81\xd1\x82\xd1\x80\xd0\xbe\xd0\xb8\xd0\xbc \xd1\x84\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd1\x8e \xd0\xbe\xd1\x88\xd0\xb8\xd0\xb1\xd0\xba\xd0\xb8\nloss = tf.reduce_sum(np.power(y - y_pred, 2))\n\n# \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xbe\xd0\xbf\xd1\x82\xd0\xb8\xd0\xbc\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n\n# \xd0\xb7\xd0\xb0\xd0\xbf\xd1\x83\xd1\x81\xd0\xba\xd0\xb0\xd0\xb5\xd0\xbc \xd1\x81\xd0\xb5\xd1\x81\xd1\x81\xd0\xb8\xd1\x8e\ndisplay_step = 100\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(num_steps):\n        # \xd0\xb1\xd0\xb5\xd1\x80\xd0\xb5\xd0\xbc \xd1\x81\xd0\xbb\xd1\x83\xd1\x87\xd0\xb0\xd0\xb9\xd0\xbd\xd0\xbe\xd0\xb5 \xd0\xbf\xd0\xbe\xd0\xb4\xd0\xbc\xd0\xbd\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x81\xd1\x82\xd0\xb2\xd0\xbe \xd0\xb8\xd0\xb7 batch_size \xd0\xb8\xd0\xbd\xd0\xb4\xd0\xb5\xd0\xba\xd1\x81\xd0\xbe\xd0\xb2 \xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd1\x85\n        indices = np.random.choice(n_samples, batch_size)\n\n        # \xd0\xb1\xd0\xb5\xd1\x80\xd0\xb5\xd0\xbc \xd0\xbd\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x80 \xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd1\x85 \xd0\xbf\xd0\xbe \xd0\xb2\xd1\x8b\xd0\xb1\xd1\x80\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xbc \xd0\xb8\xd0\xbd\xd0\xb4\xd0\xb5\xd0\xba\xd1\x81\xd0\xb0\xd0\xbc\n        X_batch, y_batch = X_data[indices], y_data[indices]\n        \n        # \xd0\xbf\xd0\xbe\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xb2 \xd1\x84\xd1\x83\xd0\xbd\xd0\xba\xd1\x86\xd0\xb8\xd1\x8e sess.run \xd1\x81\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbe\xd0\xba \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xbd\xd1\x8b\xd1\x85, \xd0\xba\xd0\xbe\xd1\x82\xd0\xbe\xd1\x80\xd1\x8b\xd0\xb5 \xd0\xbd\xd1\x83\xd0\xb6\xd0\xbd\xd0\xbe \xd0\xbf\xd0\xbe\xd0\xb4\xd1\x81\xd1\x87\xd0\xb8\xd1\x82\xd0\xb0\xd1\x82\xd1\x8c\n        _, loss_val, k_val, b_val = sess.run([optimizer, loss, k, b ], feed_dict = { X: X_batch, y : y_batch })\n\n        # \xd0\xb2\xd1\x8b\xd0\xb2\xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbc \xd1\x80\xd0\xb5\xd0\xb7\xd1\x83\xd0\xbb\xd1\x8c\xd1\x82\xd0\xb0\xd1\x82\n        if (i+1) % display_step == 0:\n            print(\'Epoch %d: %.8f, k=%.4f, b=%.4f\' % (i+1, loss_val, k_val, b_val))\n'"
3_6_mnist_logistic_regression.py,0,"b'"""""" \xd0\xa7\xd0\xb0\xd1\x81\xd1\x82\xd1\x8c I. \xd0\x93\xd0\xbb\xd0\xb0\xd0\xb2\xd0\xb0 3. \xd0\xa0\xd0\xb0\xd0\xb7\xd0\xb4\xd0\xb5\xd0\xbb 3.6. \xd0\xa0\xd0\xb0\xd1\x81\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xbd\xd0\xb0\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x80\xd1\x83\xd0\xba\xd0\xbe\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbd\xd1\x8b\xd1\x85 \xd1\x86\xd0\xb8\xd1\x84\xd1\x80 \xd0\xbd\xd0\xb0 TensorFlow""""""\n\nimport tensorflow as tf\n\n# \xd0\xb7\xd0\xb0\xd0\xb3\xd1\x80\xd1\x83\xd0\xb6\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb5 MNIST\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n\n# \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xb7\xd0\xb0\xd0\xb3\xd0\xbb\xd1\x83\xd1\x88\xd0\xba\xd0\xb8, [None, 784] \xd0\xbe\xd0\xb7\xd0\xbd\xd0\xb0\xd1\x87\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb8\xd0\xb7\xd0\xb2\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd0\xb5 \xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xbe 784-\xd0\xbc\xd0\xb5\xd1\x80\xd0\xbd\xd1\x8b\xd1\x85 \xd0\xb2\xd0\xb5\xd0\xba\xd1\x82\xd0\xbe\xd1\x80\xd0\xbe\xd0\xb2\nx = tf.placeholder(tf.float32, [None, 784])\n\n# \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xbe\xd0\xb1\xd1\x83\xd1\x87\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n\n# \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xbc\xd0\xbe\xd0\xb4\xd0\xb5\xd0\xbb\xd1\x8c\ny = tf.nn.softmax(tf.matmul(x, W) + b)\n\n# \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd1\x84\xd1\x83\xd0\xbd\xd0\xba\xd1\x86\xd0\xb8\xd1\x8e \xd0\xbf\xd0\xbe\xd1\x82\xd0\xb5\xd1\x80\xd1\x8c\ny_ = tf.placeholder(tf.float32, [None, 10])\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), axis=[1]))\n\n# \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xbe\xd0\xbf\xd1\x82\xd0\xb8\xd0\xbc\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80\ntrain_step = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cross_entropy)\n\n# \xd0\xb7\xd0\xb0\xd0\xbf\xd1\x83\xd1\x81\xd0\xba\xd0\xb0\xd0\xb5\xd0\xbc \xd1\x81\xd0\xb5\xd1\x81\xd1\x81\xd0\xb8\xd1\x8e\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(1000):\n        batch_xs, batch_ys = mnist.train.next_batch(100)\n        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(""Accuracy: %s"" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n'"
3_6_mnist_relu.py,0,"b'"""""" \xd0\xa7\xd0\xb0\xd1\x81\xd1\x82\xd1\x8c I. \xd0\x93\xd0\xbb\xd0\xb0\xd0\xb2\xd0\xb0 3. \xd0\xa0\xd0\xb0\xd0\xb7\xd0\xb4\xd0\xb5\xd0\xbb 3.6. \xd0\xa0\xd0\xb0\xd1\x81\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xbd\xd0\xb0\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x80\xd1\x83\xd0\xba\xd0\xbe\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbd\xd1\x8b\xd1\x85 \xd1\x86\xd0\xb8\xd1\x84\xd1\x80 \xd0\xbd\xd0\xb0 TensorFlow""""""\n\nimport tensorflow as tf\n\n# \xd0\xb7\xd0\xb0\xd0\xb3\xd1\x80\xd1\x83\xd0\xb6\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb5 MNIST\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n\n# \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xb7\xd0\xb0\xd0\xb3\xd0\xbb\xd1\x83\xd1\x88\xd0\xba\xd0\xb8, [None, 784] \xd0\xbe\xd0\xb7\xd0\xbd\xd0\xb0\xd1\x87\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb8\xd0\xb7\xd0\xb2\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd0\xb5 \xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xbe 784-\xd0\xbc\xd0\xb5\xd1\x80\xd0\xbd\xd1\x8b\xd1\x85 \xd0\xb2\xd0\xb5\xd0\xba\xd1\x82\xd0\xbe\xd1\x80\xd0\xbe\xd0\xb2\nx = tf.placeholder(tf.float32, [None, 784])\n\n# \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb9 ReLU\nW_relu = tf.Variable(tf.truncated_normal([784, 100], stddev=0.1))\nb_relu = tf.Variable(tf.truncated_normal([100], stddev=0.1))\n\n# \xd1\x81\xd0\xba\xd1\x80\xd1\x8b\xd1\x82\xd1\x8b\xd0\xb9 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb9\nh = tf.nn.relu(tf.matmul(x, W_relu) + b_relu)\n\n# \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb9 \xd0\xb4\xd1\x80\xd0\xbe\xd0\xbf\xd0\xb0\xd1\x83\xd1\x82\xd0\xb0\nkeep_probability = tf.placeholder(tf.float32)\nh_drop = tf.nn.dropout(h, keep_probability)\n\n# softmax-\xd1\x81\xd0\xbb\xd0\xbe\xd0\xb9\nW = tf.Variable(tf.zeros([100, 10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.nn.softmax(tf.matmul(h_drop, W) + b)\n\n# \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd1\x84\xd1\x83\xd0\xbd\xd0\xba\xd1\x86\xd0\xb8\xd1\x8e \xd0\xbf\xd0\xbe\xd1\x82\xd0\xb5\xd1\x80\xd1\x8c\ny_ = tf.placeholder(tf.float32, [None, 10])\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), axis=[1]))\n\n# \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xbe\xd0\xbf\xd1\x82\xd0\xb8\xd0\xbc\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80\ntrain_step = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cross_entropy)\n\n# \xd0\xb7\xd0\xb0\xd0\xbf\xd1\x83\xd1\x81\xd0\xba\xd0\xb0\xd0\xb5\xd0\xbc \xd1\x81\xd0\xb5\xd1\x81\xd1\x81\xd0\xb8\xd1\x8e\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(2000):\n        batch_xs, batch_ys = mnist.train.next_batch(100)\n        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_probability: 0.5})\n\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(""Accuracy: %s"" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_probability: 1.}))\n'"
5_3_keras.py,0,"b'"""""" \xd0\xa7\xd0\xb0\xd1\x81\xd1\x82\xd1\x8c II. \xd0\x93\xd0\xbb\xd0\xb0\xd0\xb2\xd0\xb0 3. \xd0\xa0\xd0\xb0\xd0\xb7\xd0\xb4\xd0\xb5\xd0\xbb 5.3. \xd0\xa1\xd0\xb2\xd0\xb5\xd1\x80\xd1\x82\xd0\xba\xd0\xb8 \xd0\xb4\xd0\xbb\xd1\x8f \xd1\x80\xd0\xb0\xd1\x81\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xbd\xd0\xb0\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd1\x86\xd0\xb8\xd1\x84\xd1\x80""""""\n\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.utils import np_utils\n\n# \xd0\xb7\xd0\xb0\xd0\xb3\xd1\x80\xd1\x83\xd0\xb6\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb5 MNIST\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xba\xd0\xb0\xd0\xb6\xd0\xb4\xd1\x83\xd1\x8e \xd0\xba\xd0\xb0\xd1\x80\xd1\x82\xd0\xb8\xd0\xbd\xd0\xba\xd1\x83 \xd0\xb2 \xd0\xb4\xd0\xb2\xd1\x83\xd0\xbc\xd0\xb5\xd1\x80\xd0\xbd\xd1\x8b\xd0\xb9 \xd0\xbc\xd0\xb0\xd1\x81\xd1\x81\xd0\xb8\xd0\xb2\nbatch_size, img_rows, img_cols = 64, 28, 28\nX_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\nX_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\ninput_shape = (img_rows, img_cols, 1)\n\n# \xd0\xbf\xd1\x80\xd0\xb8\xd0\xb2\xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbc \xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xba \xd1\x82\xd0\xb8\xd0\xbf\xd1\x83 float32 \xd0\xb8 \xd0\xbd\xd0\xbe\xd1\x80\xd0\xbc\xd0\xb0\xd0\xbb\xd0\xb8\xd0\xb7\xd1\x83\xd0\xb5\xd0\xbc \xd0\xb8\xd1\x85 \xd0\xbe\xd1\x82 0 \xd0\xb4\xd0\xbe 1\nX_train = X_train.astype(""float32"")\nX_test = X_test.astype(""float32"")\nX_train /= 255\nX_test /= 255\n\n# \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbc \xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b \xd0\xb2 one-hot \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd1\x81\xd1\x82\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\nY_train = np_utils.to_categorical(y_train, 10)\nY_test = np_utils.to_categorical(y_test, 10)\n\n# \xd0\xb8\xd0\xbd\xd0\xb8\xd1\x86\xd0\xb8\xd0\xb0\xd0\xbb\xd0\xb8\xd0\xb7\xd0\xb8\xd1\x80\xd1\x83\xd0\xb5\xd0\xbc \xd0\xbc\xd0\xbe\xd0\xb4\xd0\xb5\xd0\xbb\xd1\x8c\nmodel = Sequential()\n\n# \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8f\xd0\xb5\xd0\xbc \xd1\x81\xd0\xb2\xd0\xb5\xd1\x80\xd1\x82\xd0\xbe\xd1\x87\xd0\xbd\xd1\x8b\xd0\xb5 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb8\nmodel.add(Convolution2D(32, 5, 5, border_mode=""same"", input_shape=input_shape))\nmodel.add(Activation(""relu""))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode=""same""))\nmodel.add(Convolution2D(64, 5, 5, border_mode=""same"", input_shape=input_shape))\nmodel.add(Activation(""relu""))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode=""same""))\n\n# \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8f\xd0\xb5\xd0\xbc \xd0\xbf\xd0\xbe\xd0\xbb\xd0\xbd\xd0\xbe\xd1\x81\xd0\xb2\xd1\x8f\xd0\xb7\xd0\xbd\xd1\x8b\xd0\xb5 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb8\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(Activation(""relu""))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10))\nmodel.add(Activation(""softmax""))\n\n# \xd0\xba\xd0\xbe\xd0\xbc\xd0\xbf\xd0\xb8\xd0\xbb\xd0\xb8\xd1\x80\xd1\x83\xd0\xb5\xd0\xbc \xd0\xb8 \xd0\xb7\xd0\xb0\xd0\xbf\xd1\x83\xd1\x81\xd0\xba\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xbe\xd0\xb1\xd1\x83\xd1\x87\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\nmodel.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])\nmodel.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=10, verbose=1, validation_data=(X_test, Y_test))\nscore = model.evaluate(X_test, Y_test, verbose=0)\n\n# \xd0\xb2\xd1\x8b\xd0\xb2\xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbc \xd1\x80\xd0\xb5\xd0\xb7\xd1\x83\xd0\xbb\xd1\x8c\xd1\x82\xd0\xb0\xd1\x82\xd1\x8b\nprint(""Test score: %f"" % score[0])\nprint(""Test accuracy: %f"" % score[1])\n'"
5_3_tensorflow.py,0,"b'"""""" \xd0\xa7\xd0\xb0\xd1\x81\xd1\x82\xd1\x8c II. \xd0\x93\xd0\xbb\xd0\xb0\xd0\xb2\xd0\xb0 3. \xd0\xa0\xd0\xb0\xd0\xb7\xd0\xb4\xd0\xb5\xd0\xbb 5.3. \xd0\xa1\xd0\xb2\xd0\xb5\xd1\x80\xd1\x82\xd0\xba\xd0\xb8 \xd0\xb4\xd0\xbb\xd1\x8f \xd1\x80\xd0\xb0\xd1\x81\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xbd\xd0\xb0\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd1\x86\xd0\xb8\xd1\x84\xd1\x80""""""\n\nimport tensorflow as tf\n\n# \xd0\xb7\xd0\xb0\xd0\xb3\xd1\x80\xd1\x83\xd0\xb6\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb5 MNIST\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n\n# \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xb7\xd0\xb0\xd0\xb3\xd0\xbb\xd1\x83\xd1\x88\xd0\xba\xd0\xb8, [None, 784] \xd0\xbe\xd0\xb7\xd0\xbd\xd0\xb0\xd1\x87\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb8\xd0\xb7\xd0\xb2\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd0\xb5 \xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xbe 784-\xd0\xbc\xd0\xb5\xd1\x80\xd0\xbd\xd1\x8b\xd1\x85 \xd0\xb2\xd0\xb5\xd0\xba\xd1\x82\xd0\xbe\xd1\x80\xd0\xbe\xd0\xb2\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\n\n# \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd1\x84\xd0\xbe\xd1\x80\xd0\xbc\xd0\xb0\xd1\x82\xd0\xb8\xd1\x80\xd1\x83\xd0\xb5\xd0\xbc \xd0\xb2\xd1\x85\xd0\xbe\xd0\xb4\xd0\xbd\xd0\xbe\xd0\xb9 \xd0\xb2\xd0\xb5\xd0\xba\xd1\x82\xd0\xbe\xd1\x80 \xd0\xb2 \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xb5 \xd0\xb4\xd0\xb2\xd1\x83\xd0\xbc\xd0\xb5\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xbc\xd0\xb0\xd1\x81\xd1\x81\xd0\xb8\xd0\xb2\xd0\xb0 28\xd1\x8528 (-1 - \xd0\xbd\xd0\xb5\xd0\xb8\xd0\xb7\xd0\xb2\xd0\xb5\xd1\x81\xd1\x82\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x80\xd0\xb0\xd0\xb7\xd0\xbc\xd0\xb5\xd1\x80 \xd0\xbc\xd0\xb8\xd0\xbd\xd0\xb8-\xd0\xb1\xd0\xb0\xd1\x82\xd1\x87\xd0\xb0, 1 - \xd0\xbe\xd0\xb4\xd0\xbd\xd0\xbe \xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xbe \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xba\xd0\xb0\xd0\xb6\xd0\xb4\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xbf\xd0\xb8\xd0\xba\xd1\x81\xd0\xb5\xd0\xbb\xd1\x8f)\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\n# \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd1\x81\xd0\xb2\xd0\xb5\xd1\x80\xd1\x82\xd0\xbe\xd1\x87\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb9 (5\xd1\x855 - \xd1\x80\xd0\xb0\xd0\xb7\xd0\xbc\xd0\xb5\xd1\x80 \xd1\x8f\xd0\xb4\xd1\x80\xd0\xb0 \xd1\x81\xd0\xb2\xd0\xb5\xd1\x80\xd1\x82\xd0\xba\xd0\xb8, 32 - \xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xbe \xd1\x84\xd0\xb8\xd0\xbb\xd1\x8c\xd1\x82\xd1\x80\xd0\xbe\xd0\xb2, 1 - \xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xbe \xd1\x86\xd0\xb2\xd0\xb5\xd1\x82\xd0\xbe\xd0\xb2\xd1\x8b\xd1\x85 \xd0\xba\xd0\xb0\xd0\xbd\xd0\xb0\xd0\xbb\xd0\xbe\xd0\xb2)\nW_conv_1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\nb_conv_1 = tf.Variable(tf.constant(0.1, shape=[32]))\nconv_1 = tf.nn.conv2d(x_image, W_conv_1, strides=[1, 1, 1, 1], padding=""SAME"") + b_conv_1\n\n# \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd1\x84\xd1\x83\xd0\xbd\xd0\xba\xd1\x86\xd0\xb8\xd1\x8e \xd0\xb0\xd0\xba\xd1\x82\xd0\xb8\xd0\xb2\xd0\xb0\xd1\x86\xd0\xb8\xd0\xb8\nh_conv_1 = tf.nn.relu(conv_1)\n\n# \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb9 \xd1\x81\xd1\x83\xd0\xb1\xd0\xb4\xd0\xb8\xd1\x81\xd0\xba\xd1\x80\xd0\xb5\xd1\x82\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x86\xd0\xb8\nh_pool_1 = tf.nn.max_pool(h_conv_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""SAME"")\n\n# \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8f\xd0\xb5\xd0\xbc \xd0\xb5\xd1\x89\xd0\xb5 \xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbd \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb9 \xd0\xbf\xd0\xbe \xd0\xb0\xd0\xbd\xd0\xb0\xd0\xbb\xd0\xbe\xd0\xb3\xd0\xb8\xd0\xb8 \xd1\x81 \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb2\xd1\x8b\xd0\xbc, \xd0\xbd\xd0\xbe \xd1\x81 64 \xd1\x84\xd0\xb8\xd0\xbb\xd1\x8c\xd1\x82\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\nW_conv_2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))\nb_conv_2 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv_2 = tf.nn.conv2d(h_pool_1, W_conv_2, strides=[1, 1, 1, 1], padding=""SAME"") + b_conv_2\nh_conv_2 = tf.nn.relu(conv_2)\nh_pool_2 = tf.nn.max_pool(h_conv_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""SAME"")\n\n# \xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xbf\xd0\xbb\xd0\xbe\xd1\x81\xd0\xba\xd0\xb8\xd0\xb9 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb9 \xd0\xb8\xd0\xb7 \xd0\xb4\xd0\xb2\xd1\x83\xd0\xbc\xd0\xb5\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe\nh_pool_2_flat = tf.reshape(h_pool_2, [-1, 7*7*64])\n\n# \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8f\xd0\xb5\xd0\xbc \xd0\xbf\xd0\xbe\xd0\xbb\xd0\xbd\xd0\xbe\xd1\x81\xd0\xb2\xd1\x8f\xd0\xb7\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb9 \xd0\xbd\xd0\xb0 1024 \xd0\xbd\xd0\xb5\xd0\xb9\xd1\x80\xd0\xbe\xd0\xbd\xd0\xb0\nW_fc_1 = tf.Variable(tf.truncated_normal([7*7*64, 1024], stddev=0.1))\nb_fc_1 = tf.Variable(tf.constant(0.1, shape=[1024]))\nh_fc_1 = tf.nn.relu(tf.matmul(h_pool_2_flat, W_fc_1) + b_fc_1)\n\n# \xd1\x80\xd0\xb5\xd0\xb3\xd1\x83\xd0\xbb\xd1\x8f\xd1\x80\xd0\xb8\xd0\xb7\xd1\x83\xd0\xb5\xd0\xbc \xd0\xb4\xd1\x80\xd0\xbe\xd0\xbf\xd0\xb0\xd1\x83\xd1\x82\xd0\xbe\xd0\xbc\nkeep_probability = tf.placeholder(tf.float32)\nh_fc_1_drop = tf.nn.dropout(h_fc_1, keep_probability)\n\n# \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8f\xd0\xb5\xd0\xbc \xd0\xb2\xd1\x82\xd0\xbe\xd1\x80\xd0\xbe\xd0\xb9 \xd0\xbf\xd0\xbe\xd0\xbb\xd0\xbd\xd0\xbe\xd1\x81\xd0\xb2\xd1\x8f\xd0\xb7\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb9 \xd0\xbd\xd0\xb0 10 \xd0\xb2\xd1\x8b\xd1\x85\xd0\xbe\xd0\xb4\xd0\xbe\xd0\xb2\nW_fc_2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1))\nb_fc_2 = tf.Variable(tf.constant(0.1, shape=[10]))\nlogit_conv = tf.matmul(h_fc_1_drop, W_fc_2) + b_fc_2\ny_conv = tf.nn.softmax(logit_conv)\n\n# \xd0\xbe\xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd0\xb5\xd0\xbb\xd1\x8f\xd0\xb5\xd0\xbc \xd0\xbe\xd1\x88\xd0\xb8\xd0\xb1\xd0\xba\xd1\x83 \xd0\xb8 \xd0\xbe\xd0\xbf\xd1\x82\xd0\xb8\xd0\xbc\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit_conv, labels=y))\ntrain_step = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)\n\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n# \xd0\xb7\xd0\xb0\xd0\xbf\xd1\x83\xd1\x81\xd0\xba\xd0\xb0\xd0\xb5\xd0\xbc \xd1\x81\xd0\xb5\xd1\x81\xd1\x81\xd0\xb8\xd1\x8e\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\nfor i in range(10000):\n    batch_xs, batch_ys = mnist.train.next_batch(64)\n    sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_probability: 0.5})\n\n# \xd0\xb2\xd1\x8b\xd0\xb2\xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbc \xd1\x80\xd0\xb5\xd0\xb7\xd1\x83\xd0\xbb\xd1\x8c\xd1\x82\xd0\xb0\xd1\x82\nprint(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_probability: 1.}))\n'"
