file_path,api_count,code
genetics.py,5,"b'import numpy as np\nfrom random import randint\nimport copy\n\nclass GAKill(Exception):\n\tdef __init__(self, message):\n\t\tself.message = message\n\n# Gene Super Class\nclass Gene:\n\tfitness = 0\n\tscore = 0\n\tgenotype = []\n\tcursor = 0\n\n\tdef __init__(self):\n\t\tpass\n\n\tdef encode(self):\n\t\tpass\n\n\tdef decode(self):\n\t\tpass\n\n\tdef evaluate(self):\n\t\tpass\n\n\tdef mutate(self, rate):\n\t\tgen_len = len(self.genotype)\n\n\t\t# Select some random chromosomes\n\t\tidx = np.random.random_integers(0, gen_len-1, size=(1, int(round(rate*gen_len))))\n\n\t\t# Add a small -/+ number\n\t\tself.genotype[idx] += 0.1 * (2 * np.random.random_sample(1) - 1)\n\n\tdef read_genotype(self, delta):\n\t\tchunk = self.genotype[self.cursor:self.cursor + delta]\n\t\tself.cursor += delta\n\t\treturn chunk\n\n\nclass GeneticAlgorithm:\n\tpopsize = 0\n\terror = 1\n\tepoch = 0\n\tarmageddon = 0\n\n\tdef __init__(self, epochs, mutation_rate, data, targets, obj, args):\n\t\t""""""\n\t\tThis contructor takes multiple parameters as well as the constructor\n\t\tfor the population and the arguments of the contructor. It is assumed\n\t\tthe contructor knows how to use this.\n\t\t""""""\n\t\tself.obj = obj\n\t\tself.args = args\n\t\tself.mutation_rate = mutation_rate\n\t\tself.training_data = data\n\t\tself.targets = targets\n\t\tself.armageddon = epochs\n\n\tdef populate(self, size):\n\t\t# Use the object constructor to create the population\n\t\tself.population = [self.obj(self.args) for _ in range(size)]\n\t\tself.popsize = size\n\n\tdef singleton(self):\n\t\treturn self.obj(self.args, build=False)\n\n\tdef evaluate(self):\n\t\tfor gene in self.population:\n\t\t\tgene.evaluate(self.training_data, self.targets)\n\n\t\tself.population = sorted(self.population, key=lambda gene: gene.fitness)\n\t\tself.error = 1 - self.fittest().fitness  # Set the global error\n\n\tdef crossover(self):\n\t\t# Create new population using roulette selection and breeding\n\t\tpopulation = [self.breed(self.roulette(2)) for _ in range(self.popsize)]\n\t\tself.population = population\n\n\tdef breed(self, parents):\n\t\t# Make a new gene\n\t\toffspring = self.singleton()\n\n\t\t# Determine crossover points\n\t\tlength = parents[0].genotype.size - 1\n\t\tcuts = [randint(0, round(length/2)), randint(round(length/2), length)]\n\n\t\t# Perform 2-point crossover\n\t\toffspring.genotype = np.concatenate((parents[0].genotype[:cuts[0]],\n\t\tparents[1].genotype[cuts[0]:cuts[1]], parents[0].genotype[cuts[1]:]))\n\n\t\toffspring.mutate(self.mutation_rate)\n\t\toffspring.decode()\n\n\t\treturn offspring\n\n\tdef roulette(self, n):\n\t\tchoice = self.population[-self.popsize/2:]\n\n\t\t# Gather the fitnesses from the half best population\n\t\tfitnesses = map(lambda x: x.fitness, choice)\n\t\tfitnesses /= np.sum(fitnesses) # Normalise\n\n\t\treturn np.random.choice(choice, n, p=fitnesses)\n\n\tdef fittest(self):\n\t\t# Clone the fittest gene\n\t\treturn copy.deepcopy(self.population[-1])\n\n\tdef evolve(self):\n\t\treturn True if self.epoch < self.armageddon else False\n'"
neuralnet.py,15,"b'import cPickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom genetics import Gene\nfrom nnmath import *\n\nclass NeuralNet(Gene):\n\terrors = []\n\ttest_accuracies = []\n\ttrain_accuracies = []\n\talpha_max = 0.8\n\talpha_min = 0.1\n\tdecay_speed = 100\n\n\tdef __init__(self, args, build=True):\n\t\tself.biases = []\n\t\tself.weights = []\n\t\tself.skeleton = args\n\t\tif build:\n\t\t\tself.build(self.skeleton)\n\t\t\tself.encode()\n\n\tdef build(self, skeleton):\n\t\tfor i, width in enumerate(skeleton[1:], start=1):\n\t\t\t# Intialise the parameters with random values\n\t\t\tweights = (2 * np.random.sample((width, skeleton[i-1])) - 1)\n\t\t\tbiases = (2 * np.random.sample(width) - 1)\n\t\t\tself.weights.append(weights)\n\t\t\tself.biases.append(biases)\n\n\t\tself.n = len(self.weights) + 1\n\n\tdef feed_forward(self, activation):\n\t\tzs = []\n\t\tactivations = [activation]\n\t\tz = activation\n\n\t\t# Propagate through hidden layers\n\t\tfor w, b in zip(self.weights[:-1], self.biases[:-1]):\n\t\t\tz = np.dot(w, activation) + b\n\t\t\tzs.append(z)\n\t\t\tactivation = sigmoid(z)\n\t\t\tactivations.append(activation)\n\n\t\t# Use softmax for output layer\n\t\tz = np.dot(self.weights[-1], activation) + self.biases[-1]\n\t\tzs.append(z)\n\t\tactivations.append(softmax(z))\n\t\treturn (activations, zs)\n\n\tdef backpropagate(self, activation, target):\n\t\t# Initialise the deltas\n\t\tnabla_b = [np.zeros(b.shape) for b in self.biases]\n\t\tnabla_w = [np.zeros(w.shape) for w in self.weights]\n\n\t\tactivations, zs = self.feed_forward(activation)\n\t\tself.errors[-1] += square_error(target, activations[-1])\n\t\tif np.argmax(target) == np.argmax(activations[-1]):\n\t\t\tself.train_accuracies[-1] += 1\n\n\t\t# Determine the output deltas using the derivative of the sigmoid\n\t\tdelta = softmax_prime(zs[-1]) * (activations[-1] - target)\n\n\t\tnabla_w[-1] = delta[:, None] * activations[-2][None, :]\n\t\tnabla_b[-1] = delta\n\n\t\t# Propagate error to the hidden layers\n\t\tfor i in xrange(2, self.n):\n\t\t\t# Calculate the delta for this layer\n\t\t\tdelta = np.dot(self.weights[-i+1].T, delta) * sig_prime(zs[-i])\n\n\t\t\tnabla_w[-i] = delta[:, None] * activations[-i-1][None, :]\n\t\t\tnabla_b[-i] = delta\n\n\t\treturn (nabla_w, nabla_b)\n\n\tdef gradient_descent(self, training_data, targets, epochs, test_data=None,\n\tvis=False):\n\t\tm = len(training_data)\n\n\t\tfor i in range(epochs):\n\t\t\tnabla_b = [np.zeros(b.shape) for b in self.biases]\n\t\t\tnabla_w = [np.zeros(w.shape) for w in self.weights]\n\t\t\tself.errors.append(0) # Reset the error\n\t\t\tself.train_accuracies.append(0)\n\n\t\t\tfor tag, img in training_data:\n\t\t\t\ttarget = map(lambda x: int(x in tag), targets)\n\t\t\t\tdelta_nabla_w, delta_nabla_b = self.backpropagate(img, target)\n\n\t\t\t\t# Accumulate the partial derivatives\n\t\t\t\tfor j in range(self.n - 1):\n\t\t\t\t\tnabla_w[j] += delta_nabla_w[j]\n\t\t\t\t\tnabla_b[j] += delta_nabla_b[j]\n\n\t\t\t# Update the weights and biases\n\t\t\tself.weights = [w-(self.learning_rate(i)/m)*nw for w, nw in zip(self.weights, nabla_w)]\n\n\t\t\tself.biases = [b-(self.learning_rate(i)/m)*nb for b, nb in zip(self.biases, nabla_b)]\n\n\t\t\t# Validate the neural net\n\t\t\tif test_data:\n\t\t\t\tself.test_accuracies.append(self.validate(targets, test_data))\n\n\t\t\tself.errors[-1] /= m # Normalize the error\n\t\t\tself.train_accuracies[-1] /= float(m)\n\t\t\tprint ""Epoch: "" + str(i) + "" error: "" + str(self.errors[-1]) + "" accuracy: "" + str(self.test_accuracies[-1]) + "" train_accuracy: "" + str(self.train_accuracies[-1])\n\n\t\tif vis:\n\t\t\tplt.figure(1)\n\t\t\tplt.plot(range(epochs), self.errors)\n\t\t\tplt.xlabel(\'Time (Epochs)\')\n\t\t\tplt.ylabel(\'Error\')\n\n\t\t\tplt.figure(2)\n\t\t\tplt.plot(range(epochs), self.train_accuracies, \'g\')\n\t\t\tplt.plot(range(epochs), self.test_accuracies, \'r\')\n\t\t\tplt.xlabel(\'Time (Epochs)\')\n\t\t\tplt.ylabel(\'Accuracy\')\n\n\t\t\tplt.show()\n\n\tdef validate(self, targets, test_data):\n\t\taccuracy = 0.0\n\t\tfor tag, img in test_data:\n\t\t\ttarget = map(lambda x: int(x in tag), targets)\n\t\t\tactivations, zs = self.feed_forward(img)\n\n\t\t\tif np.argmax(target) == np.argmax(activations[-1]):\n\t\t\t\taccuracy += 1\n\n\t\treturn accuracy/len(test_data)\n\n\tdef learning_rate(self, i):\n\t\treturn self.alpha_min + (self.alpha_max - self.alpha_min) * np.exp(-i/self.decay_speed)\n\n\tdef load(self, filename):\n\t\t\'\'\'This method sets the parameters of the neural net from file.\'\'\'\n\t\twith open(filename, ""r"") as f:\n\t\t\tself.weights, self.biases = cPickle.loads(f.read())\n\n\tdef save(self, filename):\n\t\t\'\'\'This method saves the parameters of the neural net to file.\'\'\'\n\t\twith open(filename, ""w+"") as f:\n\t\t\tf.write(cPickle.dumps((self.weights,self.biases)))\n\n\n\t\'\'\'*********************** Overload Gene methods ************************\'\'\'\n\n\tdef encode(self):\n\t\t\'\'\'Encode the network parameters into a series of real values\'\'\'\n\t\tgenotype = np.array([]) # Initialise a new genotype\n\t\tfor w, b in zip(self.weights, self.biases):\n\t\t\tgenotype = np.concatenate((genotype, w.flatten(), b))\n\n\t\tself.genotype = genotype\n\n\tdef decode(self):\n\t\t\'\'\'Decode genotype into layers of weights and biases\'\'\'\n\t\tself.weights = []\n\t\tself.biases = []\n\t\tfor i, width in enumerate(self.skeleton[1:], start=1):\n\t\t\td = (self.skeleton[i-1]) * width\n\t\t\t# Read the weights for layer and reshape them to 2D\n\t\t\tweights = self.read_genotype(d).reshape(width, self.skeleton[i-1])\n\t\t\tbiases = self.read_genotype(width)\n\n\t\t\tself.weights.append(weights)\n\t\t\tself.biases.append(biases)\n\n\t\tself.cursor = 0 # Reset the cursor\n\t\tself.n = len(self.weights) + 1\n\n\tdef evaluate(self, training_data, targets):\n\t\terror = 0\n\n\t\tfor tag, img in training_data:\n\t\t\ttarget = np.array(map(lambda x: int(x in tag), targets))\n\t\t\tactivations, zs = self.feed_forward(img)\n\t\t\terror += square_error(activations[-1], target)\n\n\t\tself.fitness = 1 - error/len(training_data)\n'"
nnmath.py,6,"b'import numpy as np\n\ntansig = lambda n: 2 / (1 + np.exp(-2 * n)) - 1\n\nsigmoid = lambda n: 1 / (1 + np.exp(-n))\n\nhardlim = lambda n: 1 if n >= 0 else 0\n\npurelin = lambda n: n\n\nrelu = lambda n: np.fmax(0, n)\n\nsquare_error = lambda x, y: np.sum(0.5 * (x - y)**2)\n\nsig_prime = lambda z: sigmoid(z) * (1 - sigmoid(z))\n\nrelu_prime = lambda z: relu(z) * (1 - relu(z))\n\nsoftmax = lambda n: np.exp(n)/np.sum(np.exp(n))\n\nsoftmax_prime = lambda n: softmax(n) * (1 - softmax(n))\n\ncross_entropy = lambda x, y: -np.dot(x, np.log(y))\n'"
shape.py,5,"b'import os\nimport sys\nimport random\nimport signal\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import misc\nfrom neuralnet import *\nfrom nnmath import *\nfrom genetics import GeneticAlgorithm, GAKill\n\ndef read_data(path):\n\tdata = []\n\tfor (dirpath, dirnames, filenames) in os.walk(path):\n\t\tfor dirname in dirnames:\n\t\t\tfor f in os.listdir(dirpath + dirname):\n\t\t\t\ttry:\n\t\t\t\t    img = np.ravel(misc.imread(dirpath + dirname + \'/\' + f, flatten=True))/255\n\t\t\t\t    data.append((dirname, img))\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\treturn data\n\ndef main(argv):\n\t# Set Numpy warning level\n\tnp.seterr(over=\'ignore\')\n\n\t# Define target shapes\n\ttargets = np.array([\'rectangle\', \'circle\', \'triangle\'])\n\n\tif argv[1] == \'train\':\n\t\t# Check the input arguments\n\t\tif len(argv) < 5:\n\t\t\tprint ""Usage: python shape.py train <GA-epochs> <SGD-epochs> <visFlag>""\n\t\t\tsys.exit()\n\n\t\t# Load the training data\n\t\ttraining_data = read_data(\'training_data/\')\n\t\ttest_data = read_data(\'test_data/\')\n\n\t\t# Shuffle for more randomness\n\t\trandom.shuffle(training_data)\n\n\t\t# Create a GA of neural nets\n\t\timg_len = len(training_data[0][1])\n\t\tga = GeneticAlgorithm(epochs = int(argv[2]),\n\t\t\t\t\t\t\t\tmutation_rate = 0.01,\n\t\t\t\t\t\t\t\tdata = training_data,\n\t\t\t\t\t\t\t\ttargets = targets,\n\t\t\t\t\t\t\t\tobj = NeuralNet,\n\t\t\t\t\t\t\t\targs = [img_len, 10, 4, 3])\n\n\t\t# Create the 1st generation\n\t\tprint ""Creating population...""\n\t\tga.populate(200)\n\n\t\tprint ""Initiating GA heuristic approach...""\n\n\t\t# Start evolution\n\t\terrors = []\n\t\twhile ga.evolve():\n\t\t\ttry:\n\t\t\t\tga.evaluate()\n\t\t\t\tga.crossover()\n\t\t\t\tga.epoch += 1\n\n\t\t\t\t# Store error\n\t\t\t\terrors.append(ga.error)\n\t\t\t\tprint ""error: "" + str(ga.error)\n\t\t\texcept GAKill as e:\n\t\t\t\tprint e.message\n\t\t\t\tbreak\n\n\t\tvis = bool(int(argv[4]))\n\t\tif vis:\n\t\t\t# Plot error over time\n\t\t\tfig = plt.figure()\n\t\t\tplt.plot(range(ga.epoch), errors)\n\t\t\tplt.xlabel(\'Time (Epochs)\')\n\t\t\tplt.ylabel(\'Error\')\n\t\t\tplt.show()\n\n\t\tprint ""--------------------------------------------------------------\\n""\n\n\t\tnn = ga.fittest()\n\t\tepochs = int(argv[3])\n\t\tif epochs:\n\t\t\tprint ""Initiating Gradient Descent optimization...""\n\t\t\ttry:\n\t\t\t\tnn.gradient_descent(training_data, targets, epochs, test_data=test_data, vis=vis)\n\t\t\texcept GAKill as e:\n\t\t\t\tprint e.message\n\n\t\tnn.save(""neuralnet.pkt"")\n\t\tprint ""Done!""\n\n\telif argv[1] == ""validate"":\n\t\ttest_data = read_data(\'test_data/\')\n\n\t\tnn = NeuralNet([], build=False)\n\t\tnn.load(""neuralnet.pkt"")\n\n\t\taccuracy = nn.validate(targets, test_data)\n\t\tprint ""Accuracy: "" + str(accuracy)\n\n\telif argv[1] == ""predict"":\n\t\t# Check the arguments\n\t\tif len(argv) < 3:\n\t\t    print ""Usage: python shape.py test <image>""\n\t\t    sys.exit()\n\n\t\t# Read the test image\n\t\timg = np.ravel(misc.imread(argv[2], flatten=True))/255\n\n\t\t# Build the neural net from file\n\t\tnn = NeuralNet([], build=False)\n\t\tnn.load(""neuralnet.pkt"")\n\n\t\t# Predict\n\t\tactivations, zs = nn.feed_forward(img)\n\t\tprint targets[np.argmax(activations[-1])]\n\n\telse:\n\t\tprint ""ERROR: Unknown command "" + argv[1]\n\n\ndef signal_handler(signal, frame):\n\traise(GAKill(""\\nAborting Search...""))\n\nif __name__ == ""__main__"":\n\tsignal.signal(signal.SIGINT, signal_handler)\n\tmain(sys.argv)\n'"
