file_path,api_count,code
camera.py,15,"b'import numpy as np\nimport processor\nimport transformers\n\n\nclass Camera(object):\n    """""" Class for representing pin-hole camera """"""\n\n    def __init__(self, P=None, K=None, R=None, t=None):\n        """""" P = K[R|t] camera model. (3 x 4)\n         Must either supply P or K, R, t """"""\n        if P is None:\n            try:\n                self.extrinsic = np.hstack([R, t])\n                P = np.dot(K, self.extrinsic)\n            except TypeError as e:\n                print(\'Invalid parameters to Camera. Must either supply P or K, R, t\')\n                raise\n\n        self.P = P     # camera matrix\n        self.K = K     # intrinsic matrix\n        self.R = R     # rotation\n        self.t = t     # translation\n        self.c = None  # camera center\n\n    def project(self, X):\n        """""" Project 3D homogenous points X (4 * n) and normalize coordinates.\n            Return projected 2D points (2 x n coordinates) """"""\n        x = np.dot(self.P, X)\n        x[0, :] /= x[2, :]\n        x[1, :] /= x[2, :]\n\n        return x[:2, :]\n\n    def qr_to_rq_decomposition(self):\n        """""" Convert QR to RQ decomposition with numpy.\n        Note that this could be done by passing in a square matrix with scipy:\n        K, R = scipy.linalg.rq(self.P[:, :3]) """"""\n        Q, R = np.linalg.qr(np.flipud(self.P).T)\n        R = np.flipud(R.T)\n        return R[:, ::-1], Q.T[::-1, :]\n\n    def factor(self):\n        """""" Factorize the camera matrix P into K,R,t with P = K[R|t]\n          using RQ-factorization """"""\n        if self.K is not None and self.R is not None:\n            return self.K, self.R, self.t  # Already been factorized or supplied\n\n        K, R = self.qr_to_rq_decomposition()\n        # make diagonal of K positive\n        T = np.diag(np.sign(np.diag(K)))\n        if np.linalg.det(T) < 0:\n            T[1, 1] *= -1\n\n        self.K = np.dot(K, T)\n        self.R = np.dot(T, R)  # T is its own inverse\n        self.t = np.dot(np.linalg.inv(self.K), self.P[:, 3])\n\n        return self.K, self.R, self.t\n\n    def center(self):\n        """"""  Compute and return the camera center. """"""\n        if self.c is not None:\n            return self.c\n        elif self.R:\n            # compute c by factoring\n            self.c = -np.dot(self.R.T, self.t)\n        else:\n            # P = [M|\xe2\x88\x92MC]\n            self.c = np.dot(-np.linalg.inv(self.c[:, :3]), self.c[:, -1])\n        return self.c\n\n\ndef test():\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.mplot3d import Axes3D\n    import cv2\n\n    # load points\n    points = processor.read_matrix(\'testsets/house.p3d\').T  # 3 x n\n    points = processor.cart2hom(points)  # 4 x n\n\n    img = cv2.imread(\'testsets/house1.jpg\')\n    height, width, ch = img.shape\n\n    K = np.array([\n        [width * 30, 0, width / 2],\n        [0, height * 30, height / 2],\n        [0, 0, 1]])\n    R = np.array([  # No rotation\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ])\n    t = np.array([[0], [0], [100]])  # t != 0 to be away from image plane\n\n    # Setup cameras\n    cam = Camera(K=K, R=R, t=t)\n    x = cam.project(points)\n\n    rotation_angle = 20\n    rotation_mat = transformers.rotation_3d_from_angles(rotation_angle)\n    cam = Camera(K=K, R=rotation_mat, t=t)\n    x2 = cam.project(points)\n\n    # Plot actual 3d points\n    fig = plt.figure()\n    ax = fig.gca(projection=\'3d\')\n    ax.set_aspect(\'equal\')\n    ax.plot(points[0], points[1], points[2], \'b.\')\n    ax.set_xlabel(\'x axis\')\n    ax.set_ylabel(\'y axis\')\n    ax.set_zlabel(\'z axis\')\n    ax.view_init(elev=140, azim=0)\n\n    # Plot 3d to 2d projection\n    f, ax = plt.subplots(2, sharex=True, sharey=True)\n    plt.subplots_adjust(left=0.08, bottom=0.08, right=0.99,\n                        top=0.95, wspace=0, hspace=0.01)\n    ax[0].set_aspect(\'equal\')\n    ax[0].set_title(\n        \'3D to 2D projection. Bottom x-axis rotated by {0}\xc2\xb0\'.format(rotation_angle))\n    ax[0].plot(x[0], x[1], \'k.\')\n    ax[1].plot(x2[0], x2[1], \'k.\')\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    test()\n'"
cube_reconstruction.py,20,"b""import numpy as np\nimport matplotlib.pylab as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport camera\nimport processor\nimport structure\nimport transformers\n\n\ndef plot_projections(points):\n    num_images = len(points)\n\n    plt.figure()\n    plt.suptitle('3D to 2D Projections', fontsize=16)\n    for i in range(num_images):\n        plt.subplot(1, num_images, i+1)\n        ax = plt.gca()\n        ax.set_aspect('equal')\n        ax.plot(points[i][0], points[i][1], 'r.')\n\n\ndef plot_cube(points3d, title=''):\n    fig = plt.figure()\n    fig.suptitle(title, fontsize=16)\n    ax = fig.gca(projection='3d')\n    ax.set_aspect('equal')\n    ax.plot(points3d[0], points3d[1], points3d[2], 'b.')\n    ax.set_xlabel('x axis')\n    ax.set_ylabel('y axis')\n    ax.set_zlabel('z axis')\n    ax.view_init(elev=135, azim=90)\n    return ax\n\ndef extrinsic_from_camera_pose(m_c1_wrt_c2):\n    # Inverse to get extrinsic matrix from camera to world view\n    # http://ksimek.github.io/2012/08/22/extrinsic/\n    # Returns homogenous 4x4 extrinsic camera matrix\n    # Alternatively, R = R^t, T = -RC\n    # Rct = m_c1_wrt_c2[:3, :3].T\n    # t = -np.dot(Rct, m_c1_wrt_c2[:3, 3])\n    H_m = np.vstack([m_c1_wrt_c2, [0, 0, 0, 1]])\n    ext = np.linalg.inv(H_m)\n    return ext\n\n\ndef camera_corners(camera, dist=0.25):\n    d = dist\n    x, y, z = np.ravel(camera.t)\n    corners = np.array([\n        [x-d, y+d, z],\n        [x+d, y+d, z],\n        [x+d, y-d, z],\n        [x-d, y-d, z],\n        [x-d, y+d, z]\n    ]).T\n\n    return np.asarray(np.dot(camera.R, corners))\n\n\nsize = 300  # size of image in pixels\ncenter = size / 2\nintrinsic = np.array([\n    [size, 0, center],\n    [0, size, center],\n    [0, 0, 1]\n])\n\n# Points of on the surface of the cube\npoints3d = np.array([\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 1, 2],\n    [2, 1, 0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, -1, -1, -1, -2, -2, -2, 0, 0, -1, -1, -2, -2],\n    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n])\n\n# Define pose of cube with respect to camera1 in world view\nrotation_mat = transformers.rotation_3d_from_angles(120, 0, 60)\ntranslation_mat = np.matrix([0, 0, 5]).T\nc = camera.Camera(K=intrinsic, R=rotation_mat, t=translation_mat)\n\n# Project 3d points to camera1 on the left\npoints1 = c.project(points3d)\npoints1 = processor.cart2hom(points1)\n\n# Get 4x4 homogenous extrinsic parameters of camera 1\nH_c1 = np.vstack([c.extrinsic, [0, 0, 0, 1]])\n\n# Define rotation of camera1 wrt camera2 and\n# translation of camera2 wrt camera1\nrotation_mat_wrt_c1 = transformers.rotation_3d_from_angles(0, -25, 0)\ntranslation_mat_wrt_c1 = np.matrix([3, 0, 1]).T\nH_c2_c1 = np.hstack([rotation_mat_wrt_c1, translation_mat_wrt_c1])\nH_c1_c2 = extrinsic_from_camera_pose(H_c2_c1)\n\n# Calculate pose of model wrt to camera2 in world view\nH_c2 = np.dot(H_c1_c2, H_c1)\n\n# Project 3d points to camera 2 on the right\nc2 = camera.Camera(K=intrinsic, R=H_c2[:3, :3], t=H_c2[:3, 3])\npoints2 = c2.project(points3d)\npoints2 = processor.cart2hom(points2[:2])\n\n# True essential matrix E = [t]R\ntrue_E = np.dot(structure.skew(translation_mat_wrt_c1), rotation_mat_wrt_c1)\nprint('True essential matrix:', true_E)\n\n# Calculate essential matrix with 2d points.\n# Result will be up to a scale\n# First, normalize points\npoints1n = np.dot(np.linalg.inv(intrinsic), points1)\npoints2n = np.dot(np.linalg.inv(intrinsic), points2)\nE = structure.compute_essential_normalized(points1n, points2n)\nprint('Computed essential matrix:', (-E / E[0][1]))\n\n# True fundamental matrix F = K^-t E K^-1\ntrue_F = np.dot(np.dot(np.linalg.inv(intrinsic).T, true_E), np.linalg.inv(intrinsic))\nF = structure.compute_fundamental_normalized(points1, points2)\nprint('True fundamental matrix:', true_F)\nprint('Computed fundamental matrix:', (F * true_F[2][2]))\n\n# Given we are at camera 1, calculate the parameters for camera 2\n# Using the essential matrix returns 4 possible camera paramters\nP1 = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]])\nP2s = structure.compute_P_from_essential(E)\n\nind = -1\nfor i, P2 in enumerate(P2s):\n    # Find the correct camera parameters\n    d1 = structure.reconstruct_one_point(points1n[:, 0], points2n[:, 0], P1, P2)\n    P2_homogenous = extrinsic_from_camera_pose(P2)\n    d2 = np.dot(P2_homogenous[:3, :4], d1)\n\n    if d1[2] > 0 and d2[2] > 0:\n        ind = i\n\nprint('True pose of c2 wrt c1: ', H_c1_c2)\nP2 = np.linalg.inv(np.vstack([P2s[ind], [0, 0, 0, 1]]))[:3, :4]\nP2f = structure.compute_P_from_fundamental(F)\nprint('Calculated camera 2 parameters:', P2, P2f)\n\ntripoints3d = structure.reconstruct_points(points1n, points2n, P1, P2)\ntripoints3d = structure.linear_triangulation(points1n, points2n, P1, P2)\n\nstructure.plot_epipolar_lines(points1n, points2n, E)\nplot_projections([points1, points2])\n\nax = plot_cube(points3d, 'Original')\ncam_corners1 = camera_corners(c)\ncam_corners2 = camera_corners(c2)\nax.plot(cam_corners1[0], cam_corners1[1], cam_corners1[2], 'g-')\nax.plot(cam_corners2[0], cam_corners2[1], cam_corners2[2], 'r-')\n\nplot_cube(tripoints3d, '3D reconstructed')\nplt.show()\n"""
example.py,12,"b""import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport cv2\n\nfrom camera import Camera\nimport structure\nimport processor\nimport features\n\n# Download images from http://www.robots.ox.ac.uk/~vgg/data/data-mview.html\n\ndef house():\n    input_path = 'imgs/house/'\n    camera_filepath = 'imgs/house/3D/house.00{0}.P'\n\n    cameras = [Camera(processor.read_matrix(camera_filepath.format(i)))\n            for i in range(9)]\n    [c.factor() for c in cameras]\n\n    points3d = processor.read_matrix(input_path + '3D/house.p3d').T  # 3 x n\n    points4d = np.vstack((points3d, np.ones(points3d.shape[1])))  # 4 x n\n    points2d = [processor.read_matrix(\n        input_path + '2D/house.00' + str(i) + '.corners') for i in range(9)]\n\n    index1 = 2\n    index2 = 4\n    img1 = cv2.imread(input_path + 'house.00' + str(index1) + '.pgm')  # left image\n    img2 = cv2.imread(input_path + 'house.00' + str(index2) + '.pgm')\n\n    # fig = plt.figure()\n    # ax = fig.gca(projection='3d')\n    # ax.plot(points3d[0], points3d[1], points3d[2], 'b.')\n    # ax.set_xlabel('x axis')\n    # ax.set_ylabel('y axis')\n    # ax.set_zlabel('z axis')\n    # ax.view_init(elev=135, azim=90)\n\n    # x = cameras[index2].project(points4d)\n    # plt.figure()\n    # plt.plot(x[0], x[1], 'b.')\n    # plt.show()\n\n    corner_indexes = processor.read_matrix(\n        input_path + '2D/house.nview-corners', np.int)\n    corner_indexes1 = corner_indexes[:, index1]\n    corner_indexes2 = corner_indexes[:, index2]\n    intersect_indexes = np.intersect1d(np.nonzero(\n        [corner_indexes1 != -1]), np.nonzero([corner_indexes2 != -1]))\n    corner_indexes1 = corner_indexes1[intersect_indexes]\n    corner_indexes2 = corner_indexes2[intersect_indexes]\n    points1 = processor.cart2hom(points2d[index1][corner_indexes1].T)\n    points2 = processor.cart2hom(points2d[index2][corner_indexes2].T)\n\n    height, width, ch = img1.shape\n    intrinsic = np.array([  # for imgs/house\n        [2362.12, 0, width / 2],\n        [0, 2366.12, height / 2],\n        [0, 0, 1]])\n\n    return points1, points2, intrinsic\n\n\ndef dino():\n    # Dino\n    img1 = cv2.imread('imgs/dinos/viff.003.ppm')\n    img2 = cv2.imread('imgs/dinos/viff.001.ppm')\n    pts1, pts2 = features.find_correspondence_points(img1, img2)\n    points1 = processor.cart2hom(pts1)\n    points2 = processor.cart2hom(pts2)\n\n    fig, ax = plt.subplots(1, 2)\n    ax[0].autoscale_view('tight')\n    ax[0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n    ax[0].plot(points1[0], points1[1], 'r.')\n    ax[1].autoscale_view('tight')\n    ax[1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n    ax[1].plot(points2[0], points2[1], 'r.')\n    fig.show()\n\n    height, width, ch = img1.shape\n    intrinsic = np.array([  # for dino\n        [2360, 0, width / 2],\n        [0, 2360, height / 2],\n        [0, 0, 1]])\n\n    return points1, points2, intrinsic\n\n\npoints1, points2, intrinsic = dino()\n\n# Calculate essential matrix with 2d points.\n# Result will be up to a scale\n# First, normalize points\npoints1n = np.dot(np.linalg.inv(intrinsic), points1)\npoints2n = np.dot(np.linalg.inv(intrinsic), points2)\nE = structure.compute_essential_normalized(points1n, points2n)\nprint('Computed essential matrix:', (-E / E[0][1]))\n\n# Given we are at camera 1, calculate the parameters for camera 2\n# Using the essential matrix returns 4 possible camera paramters\nP1 = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]])\nP2s = structure.compute_P_from_essential(E)\n\nind = -1\nfor i, P2 in enumerate(P2s):\n    # Find the correct camera parameters\n    d1 = structure.reconstruct_one_point(\n        points1n[:, 0], points2n[:, 0], P1, P2)\n\n    # Convert P2 from camera view to world view\n    P2_homogenous = np.linalg.inv(np.vstack([P2, [0, 0, 0, 1]]))\n    d2 = np.dot(P2_homogenous[:3, :4], d1)\n\n    if d1[2] > 0 and d2[2] > 0:\n        ind = i\n\nP2 = np.linalg.inv(np.vstack([P2s[ind], [0, 0, 0, 1]]))[:3, :4]\n#tripoints3d = structure.reconstruct_points(points1n, points2n, P1, P2)\ntripoints3d = structure.linear_triangulation(points1n, points2n, P1, P2)\n\nfig = plt.figure()\nfig.suptitle('3D reconstructed', fontsize=16)\nax = fig.gca(projection='3d')\nax.plot(tripoints3d[0], tripoints3d[1], tripoints3d[2], 'b.')\nax.set_xlabel('x axis')\nax.set_ylabel('y axis')\nax.set_zlabel('z axis')\nax.view_init(elev=135, azim=90)\nplt.show()\n"""
features.py,2,"b""import cv2\nimport numpy as np\n\n\ndef find_correspondence_points(img1, img2):\n    sift = cv2.xfeatures2d.SIFT_create()\n\n    # find the keypoints and descriptors with SIFT\n    kp1, des1 = sift.detectAndCompute(\n        cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY), None)\n    kp2, des2 = sift.detectAndCompute(\n        cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY), None)\n\n    # Find point matches\n    FLANN_INDEX_KDTREE = 0\n    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n    search_params = dict(checks=50)\n    flann = cv2.FlannBasedMatcher(index_params, search_params)\n    matches = flann.knnMatch(des1, des2, k=2)\n\n    # Apply Lowe's SIFT matching ratio test\n    good = []\n    for m, n in matches:\n        if m.distance < 0.8 * n.distance:\n            good.append(m)\n\n    src_pts = np.asarray([kp1[m.queryIdx].pt for m in good])\n    dst_pts = np.asarray([kp2[m.trainIdx].pt for m in good])\n\n    # Constrain matches to fit homography\n    retval, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 100.0)\n    mask = mask.ravel()\n\n    # We select only inlier points\n    pts1 = src_pts[mask == 1]\n    pts2 = dst_pts[mask == 1]\n\n    return pts1.T, pts2.T\n"""
processor.py,6,"b'import numpy as np\n\n\ndef read_matrix(path, astype=np.float64):\n    """""" Reads a file containing a matrix where each line represents a point\n        and each point is tab or space separated. * are replaced with -1.\n    :param path: path to the file\n    :parama astype: type to cast the numbers. Default: np.float64\n    :returns: array of array of numbers\n    """"""\n    with open(path, \'r\') as f:\n        arr = []\n        for line in f:\n            arr.append([(token if token != \'*\' else -1)\n                        for token in line.strip().split()])\n        return np.asarray(arr).astype(astype)\n\n\ndef cart2hom(arr):\n    """""" Convert catesian to homogenous points by appending a row of 1s\n    :param arr: array of shape (num_dimension x num_points)\n    :returns: array of shape ((num_dimension+1) x num_points) \n    """"""\n    if arr.ndim == 1:\n        return np.hstack([arr, 1])\n    return np.asarray(np.vstack([arr, np.ones(arr.shape[1])]))\n\n\ndef hom2cart(arr):\n    """""" Convert homogenous to catesian by dividing each row by the last row\n    :param arr: array of shape (num_dimension x num_points)\n    :returns: array of shape ((num_dimension-1) x num_points) iff d > 1 \n    """"""\n    # arr has shape: dimensions x num_points\n    num_rows = len(arr)\n    if num_rows == 1 or arr.ndim == 1:\n        return arr\n\n    return np.asarray(arr[:num_rows - 1] / arr[num_rows - 1])\n'"
structure.py,40,"b'import matplotlib.pyplot as plt\nimport numpy as np\n\ndef reconstruct_points(p1, p2, m1, m2):\n    num_points = p1.shape[1]\n    res = np.ones((4, num_points))\n\n    for i in range(num_points):\n        res[:, i] = reconstruct_one_point(p1[:, i], p2[:, i], m1, m2)\n\n    return res\n\n\ndef reconstruct_one_point(pt1, pt2, m1, m2):\n    """"""\n        pt1 and m1 * X are parallel and cross product = 0\n        pt1 x m1 * X  =  pt2 x m2 * X  =  0\n    """"""\n    A = np.vstack([\n        np.dot(skew(pt1), m1),\n        np.dot(skew(pt2), m2)\n    ])\n    U, S, V = np.linalg.svd(A)\n    P = np.ravel(V[-1, :4])\n\n    return P / P[3]\n\n\ndef linear_triangulation(p1, p2, m1, m2):\n    """"""\n    Linear triangulation (Hartley ch 12.2 pg 312) to find the 3D point X\n    where p1 = m1 * X and p2 = m2 * X. Solve AX = 0.\n    :param p1, p2: 2D points in homo. or catesian coordinates. Shape (3 x n)\n    :param m1, m2: Camera matrices associated with p1 and p2. Shape (3 x 4)\n    :returns: 4 x n homogenous 3d triangulated points\n    """"""\n    num_points = p1.shape[1]\n    res = np.ones((4, num_points))\n\n    for i in range(num_points):\n        A = np.asarray([\n            (p1[0, i] * m1[2, :] - m1[0, :]),\n            (p1[1, i] * m1[2, :] - m1[1, :]),\n            (p2[0, i] * m2[2, :] - m2[0, :]),\n            (p2[1, i] * m2[2, :] - m2[1, :])\n        ])\n\n        _, _, V = np.linalg.svd(A)\n        X = V[-1, :4]\n        res[:, i] = X / X[3]\n\n    return res\n\n\ndef compute_epipole(F):\n    """""" Computes the (right) epipole from a\n        fundamental matrix F.\n        (Use with F.T for left epipole.)\n    """"""\n    # return null space of F (Fx=0)\n    U, S, V = np.linalg.svd(F)\n    e = V[-1]\n    return e / e[2]\n\n\ndef plot_epipolar_lines(p1, p2, F, show_epipole=False):\n    """""" Plot the points and epipolar lines. P1\' F P2 = 0 """"""\n    plt.figure()\n    plt.suptitle(\'Epipolar lines\', fontsize=16)\n\n    plt.subplot(1, 2, 1, aspect=\'equal\')\n    # Plot the epipolar lines on img1 with points p2 from the right side\n    # L1 = F * p2\n    plot_epipolar_line(p1, p2, F, show_epipole)\n    plt.subplot(1, 2, 2, aspect=\'equal\')\n    # Plot the epipolar lines on img2 with points p1 from the left side\n    # L2 = F\' * p1\n    plot_epipolar_line(p2, p1, F.T, show_epipole)\n\n\ndef plot_epipolar_line(p1, p2, F, show_epipole=False):\n    """""" Plot the epipole and epipolar line F*x=0\n        in an image given the corresponding points.\n        F is the fundamental matrix and p2 are the point in the other image.\n    """"""\n    lines = np.dot(F, p2)\n    pad = np.ptp(p1, 1) * 0.01\n    mins = np.min(p1, 1)\n    maxes = np.max(p1, 1)\n\n    # epipolar line parameter and values\n    xpts = np.linspace(mins[0] - pad[0], maxes[0] + pad[0], 100)\n    for line in lines.T:\n        ypts = np.asarray([(line[2] + line[0] * p) / (-line[1]) for p in xpts])\n        valid_idx = ((ypts >= mins[1] - pad[1]) & (ypts <= maxes[1] + pad[1]))\n        plt.plot(xpts[valid_idx], ypts[valid_idx], linewidth=1)\n        plt.plot(p1[0], p1[1], \'ro\')\n\n    if show_epipole:\n        epipole = compute_epipole(F)\n        plt.plot(epipole[0] / epipole[2], epipole[1] / epipole[2], \'r*\')\n\n\ndef skew(x):\n    """""" Create a skew symmetric matrix *A* from a 3d vector *x*.\n        Property: np.cross(A, v) == np.dot(x, v)\n    :param x: 3d vector\n    :returns: 3 x 3 skew symmetric matrix from *x*\n    """"""\n    return np.array([\n        [0, -x[2], x[1]],\n        [x[2], 0, -x[0]],\n        [-x[1], x[0], 0]\n    ])\n\n\ndef compute_P(p2d, p3d):\n    """""" Compute camera matrix from pairs of\n        2D-3D correspondences in homog. coordinates.\n    """"""\n    n = p2d.shape[1]\n    if p3d.shape[1] != n:\n        raise ValueError(\'Number of points do not match.\')\n\n    # create matrix for DLT solution\n    M = np.zeros((3 * n, 12 + n))\n    for i in range(n):\n        M[3 * i, 0:4] = p3d[:, i]\n        M[3 * i + 1, 4:8] = p3d[:, i]\n        M[3 * i + 2, 8:12] = p3d[:, i]\n        M[3 * i:3 * i + 3, i + 12] = -p2d[:, i]\n\n    U, S, V = np.linalg.svd(M)\n    return V[-1, :12].reshape((3, 4))\n\n\ndef compute_P_from_fundamental(F):\n    """""" Compute the second camera matrix (assuming P1 = [I 0])\n        from a fundamental matrix.\n    """"""\n    e = compute_epipole(F.T)  # left epipole\n    Te = skew(e)\n    return np.vstack((np.dot(Te, F.T).T, e)).T\n\n\ndef compute_P_from_essential(E):\n    """""" Compute the second camera matrix (assuming P1 = [I 0])\n        from an essential matrix. E = [t]R\n    :returns: list of 4 possible camera matrices.\n    """"""\n    U, S, V = np.linalg.svd(E)\n\n    # Ensure rotation matrix are right-handed with positive determinant\n    if np.linalg.det(np.dot(U, V)) < 0:\n        V = -V\n\n    # create 4 possible camera matrices (Hartley p 258)\n    W = np.array([[0, -1, 0], [1, 0, 0], [0, 0, 1]])\n    P2s = [np.vstack((np.dot(U, np.dot(W, V)).T, U[:, 2])).T,\n          np.vstack((np.dot(U, np.dot(W, V)).T, -U[:, 2])).T,\n          np.vstack((np.dot(U, np.dot(W.T, V)).T, U[:, 2])).T,\n          np.vstack((np.dot(U, np.dot(W.T, V)).T, -U[:, 2])).T]\n\n    return P2s\n\n\ndef correspondence_matrix(p1, p2):\n    p1x, p1y = p1[:2]\n    p2x, p2y = p2[:2]\n\n    return np.array([\n        p1x * p2x, p1x * p2y, p1x,\n        p1y * p2x, p1y * p2y, p1y,\n        p2x, p2y, np.ones(len(p1x))\n    ]).T\n\n    return np.array([\n        p2x * p1x, p2x * p1y, p2x,\n        p2y * p1x, p2y * p1y, p2y,\n        p1x, p1y, np.ones(len(p1x))\n    ]).T\n\n\ndef compute_image_to_image_matrix(x1, x2, compute_essential=False):\n    """""" Compute the fundamental or essential matrix from corresponding points\n        (x1, x2 3*n arrays) using the 8 point algorithm.\n        Each row in the A matrix below is constructed as\n        [x\'*x, x\'*y, x\', y\'*x, y\'*y, y\', x, y, 1]\n    """"""\n    A = correspondence_matrix(x1, x2)\n    # compute linear least square solution\n    U, S, V = np.linalg.svd(A)\n    F = V[-1].reshape(3, 3)\n\n    # constrain F. Make rank 2 by zeroing out last singular value\n    U, S, V = np.linalg.svd(F)\n    S[-1] = 0\n    if compute_essential:\n        S = [1, 1, 0] # Force rank 2 and equal eigenvalues\n    F = np.dot(U, np.dot(np.diag(S), V))\n\n    return F\n\n\ndef scale_and_translate_points(points):\n    """""" Scale and translate image points so that centroid of the points\n        are at the origin and avg distance to the origin is equal to sqrt(2).\n    :param points: array of homogenous point (3 x n)\n    :returns: array of same input shape and its normalization matrix\n    """"""\n    x = points[0]\n    y = points[1]\n    center = points.mean(axis=1)  # mean of each row\n    cx = x - center[0] # center the points\n    cy = y - center[1]\n    dist = np.sqrt(np.power(cx, 2) + np.power(cy, 2))\n    scale = np.sqrt(2) / dist.mean()\n    norm3d = np.array([\n        [scale, 0, -scale * center[0]],\n        [0, scale, -scale * center[1]],\n        [0, 0, 1]\n    ])\n\n    return np.dot(norm3d, points), norm3d\n\n\ndef compute_normalized_image_to_image_matrix(p1, p2, compute_essential=False):\n    """""" Computes the fundamental or essential matrix from corresponding points\n        using the normalized 8 point algorithm.\n    :input p1, p2: corresponding points with shape 3 x n\n    :returns: fundamental or essential matrix with shape 3 x 3\n    """"""\n    n = p1.shape[1]\n    if p2.shape[1] != n:\n        raise ValueError(\'Number of points do not match.\')\n\n    # preprocess image coordinates\n    p1n, T1 = scale_and_translate_points(p1)\n    p2n, T2 = scale_and_translate_points(p2)\n\n    # compute F or E with the coordinates\n    F = compute_image_to_image_matrix(p1n, p2n, compute_essential)\n\n    # reverse preprocessing of coordinates\n    # We know that P1\' E P2 = 0\n    F = np.dot(T1.T, np.dot(F, T2))\n\n    return F / F[2, 2]\n\n\ndef compute_fundamental_normalized(p1, p2):\n    return compute_normalized_image_to_image_matrix(p1, p2)\n\n\ndef compute_essential_normalized(p1, p2):\n    return compute_normalized_image_to_image_matrix(p1, p2, compute_essential=True)\n'"
transformers.py,13,"b'import numpy as np\n\n""""""\nHelper functions to create various matrices\n""""""\n\ndef rotation_3d_from_angles(x_angle, y_angle=0, z_angle=0):\n    """""" Creates a 3D rotation matrix given angles in degrees.\n        Positive angles rotates anti-clockwise.\n    :params x_angle, y_angle, z_angle: x, y, z angles between 0 to 360\n    :returns: 3x3 rotation matrix """"""\n    ax = np.deg2rad(x_angle)\n    ay = np.deg2rad(y_angle)\n    az = np.deg2rad(z_angle)\n\n    # Rotation matrix around x-axis\n    rx = np.array([\n        [1, 0, 0],\n        [0, np.cos(ax), -np.sin(ax)],\n        [0, np.sin(ax), np.cos(ax)]\n    ])\n    # Rotation matrix around y-axis\n    ry = np.array([\n        [np.cos(ay), 0, np.sin(ay)],\n        [0, 1, 0],\n        [-np.sin(ay), 0, np.cos(ay)]\n    ])\n    # Rotation matrix around z-axis\n    rz = np.array([\n        [np.cos(az), -np.sin(az), 0],\n        [np.sin(az), np.cos(az), 0],\n        [0, 0, 1]\n    ])\n\n    return np.dot(np.dot(rx, ry), rz)\n'"
