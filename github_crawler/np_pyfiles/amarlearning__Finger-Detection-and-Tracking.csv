file_path,api_count,code
Finger Detection and Tracking/FingerDetection.py,8,"b'import cv2\nimport numpy as np\n\nhand_hist = None\ntraverse_point = []\ntotal_rectangle = 9\nhand_rect_one_x = None\nhand_rect_one_y = None\n\nhand_rect_two_x = None\nhand_rect_two_y = None\n\n\ndef rescale_frame(frame, wpercent=130, hpercent=130):\n    width = int(frame.shape[1] * wpercent / 100)\n    height = int(frame.shape[0] * hpercent / 100)\n    return cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)\n\n\ndef contours(hist_mask_image):\n    gray_hist_mask_image = cv2.cvtColor(hist_mask_image, cv2.COLOR_BGR2GRAY)\n    ret, thresh = cv2.threshold(gray_hist_mask_image, 0, 255, 0)\n    _, cont, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return cont\n\ndef draw_rect(frame):\n    rows, cols, _ = frame.shape\n    global total_rectangle, hand_rect_one_x, hand_rect_one_y, hand_rect_two_x, hand_rect_two_y\n\n    hand_rect_one_x = np.array(\n        [6 * rows / 20, 6 * rows / 20, 6 * rows / 20, 9 * rows / 20, 9 * rows / 20, 9 * rows / 20, 12 * rows / 20,\n         12 * rows / 20, 12 * rows / 20], dtype=np.uint32)\n\n    hand_rect_one_y = np.array(\n        [9 * cols / 20, 10 * cols / 20, 11 * cols / 20, 9 * cols / 20, 10 * cols / 20, 11 * cols / 20, 9 * cols / 20,\n         10 * cols / 20, 11 * cols / 20], dtype=np.uint32)\n\n    hand_rect_two_x = hand_rect_one_x + 10\n    hand_rect_two_y = hand_rect_one_y + 10\n\n    for i in range(total_rectangle):\n        cv2.rectangle(frame, (hand_rect_one_y[i], hand_rect_one_x[i]),\n                      (hand_rect_two_y[i], hand_rect_two_x[i]),\n                      (0, 255, 0), 1)\n\n    return frame\n\n\ndef hand_histogram(frame):\n    global hand_rect_one_x, hand_rect_one_y\n\n    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n    roi = np.zeros([90, 10, 3], dtype=hsv_frame.dtype)\n\n    for i in range(total_rectangle):\n        roi[i * 10: i * 10 + 10, 0: 10] = hsv_frame[hand_rect_one_x[i]:hand_rect_one_x[i] + 10,\n                                          hand_rect_one_y[i]:hand_rect_one_y[i] + 10]\n\n    hand_hist = cv2.calcHist([roi], [0, 1], None, [180, 256], [0, 180, 0, 256])\n    return cv2.normalize(hand_hist, hand_hist, 0, 255, cv2.NORM_MINMAX)\n\n\ndef hist_masking(frame, hist):\n    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n\n    dst = cv2.calcBackProject([hsv], [0, 1], hist, [0, 180, 0, 256], 1)\n\n    disc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (31, 31))\n    cv2.filter2D(dst, -1, disc, dst)\n\n    ret, thresh = cv2.threshold(dst, 150, 255, cv2.THRESH_BINARY)\n\n    # thresh = cv2.dilate(thresh, None, iterations=5)\n\n    thresh = cv2.merge((thresh, thresh, thresh))\n\n    return cv2.bitwise_and(frame, thresh)\n\n\ndef centroid(max_contour):\n    moment = cv2.moments(max_contour)\n    if moment[\'m00\'] != 0:\n        cx = int(moment[\'m10\'] / moment[\'m00\'])\n        cy = int(moment[\'m01\'] / moment[\'m00\'])\n        return cx, cy\n    else:\n        return None\n\n\ndef farthest_point(defects, contour, centroid):\n    if defects is not None and centroid is not None:\n        s = defects[:, 0][:, 0]\n        cx, cy = centroid\n\n        x = np.array(contour[s][:, 0][:, 0], dtype=np.float)\n        y = np.array(contour[s][:, 0][:, 1], dtype=np.float)\n\n        xp = cv2.pow(cv2.subtract(x, cx), 2)\n        yp = cv2.pow(cv2.subtract(y, cy), 2)\n        dist = cv2.sqrt(cv2.add(xp, yp))\n\n        dist_max_i = np.argmax(dist)\n\n        if dist_max_i < len(s):\n            farthest_defect = s[dist_max_i]\n            farthest_point = tuple(contour[farthest_defect][0])\n            return farthest_point\n        else:\n            return None\n\n\ndef draw_circles(frame, traverse_point):\n    if traverse_point is not None:\n        for i in range(len(traverse_point)):\n            cv2.circle(frame, traverse_point[i], int(5 - (5 * i * 3) / 100), [0, 255, 255], -1)\n\n\ndef manage_image_opr(frame, hand_hist):\n    hist_mask_image = hist_masking(frame, hand_hist)\n\n    hist_mask_image = cv2.erode(hist_mask_image, None, iterations=2)\n    hist_mask_image = cv2.dilate(hist_mask_image, None, iterations=2)\n\n    contour_list = contours(hist_mask_image)\n    max_cont = max(contour_list, key=cv2.contourArea)\n\n    cnt_centroid = centroid(max_cont)\n    cv2.circle(frame, cnt_centroid, 5, [255, 0, 255], -1)\n\n    if max_cont is not None:\n        hull = cv2.convexHull(max_cont, returnPoints=False)\n        defects = cv2.convexityDefects(max_cont, hull)\n        far_point = farthest_point(defects, max_cont, cnt_centroid)\n        print(""Centroid : "" + str(cnt_centroid) + "", farthest Point : "" + str(far_point))\n        cv2.circle(frame, far_point, 5, [0, 0, 255], -1)\n        if len(traverse_point) < 20:\n            traverse_point.append(far_point)\n        else:\n            traverse_point.pop(0)\n            traverse_point.append(far_point)\n\n        draw_circles(frame, traverse_point)\n\n\ndef main():\n    global hand_hist\n    is_hand_hist_created = False\n    capture = cv2.VideoCapture(0)\n\n    while capture.isOpened():\n        pressed_key = cv2.waitKey(1)\n        _, frame = capture.read()\n        frame = cv2.flip(frame, 1)\n\n        if pressed_key & 0xFF == ord(\'z\'):\n            is_hand_hist_created = True\n            hand_hist = hand_histogram(frame)\n\n        if is_hand_hist_created:\n            manage_image_opr(frame, hand_hist)\n\n        else:\n            frame = draw_rect(frame)\n\n        cv2.imshow(""Live Feed"", rescale_frame(frame))\n\n        if pressed_key == 27:\n            break\n\n    cv2.destroyAllWindows()\n    capture.release()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/AdaptiveThresholding.py,0,"b'import cv2\nimport matplotlib.pyplot as plt\n\n\ndef main():\n    const = 2\n    block_size = 51\n    imagePath = ""../data/sudoku.png""\n\n    image = cv2.imread(imagePath, 0)\n\n    cv2.imshow(""Orignal Image"", image)\n\n    mean_thresh = \\\n        cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, block_size, const)\n    gaussian_thresh = \\\n        cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, block_size, const)\n\n    images = [image, mean_thresh, gaussian_thresh]\n    titles = [""Orignals"", ""Mean"", ""Gaussian""]\n\n    cv2.imshow(""Mean Image"", mean_thresh)\n    cv2.imshow(""Gaussian Image"", gaussian_thresh)\n\n    for i in range(3):\n        plt.subplot(3, 1, i + 1)\n        plt.imshow(images[i], cmap=\'gray\')\n        plt.title(titles[i])\n\n    plt.show()\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/BackgroundSubtractorMOG.py,0,"b'import cv2\n\ncapture = cv2.VideoCapture(0)\nfgbg = cv2.createBackgroundSubtractorMOG2()\n\nwhile True:\n    _, frame = capture.read()\n    fmask = fgbg.apply(frame)\n\n    cv2.imshow(""Orignal Frame"", frame)\n    cv2.imshow(""F Mask"", fmask)\n\n    if cv2.waitKey(30) & 0xff == 27:\n        break\n\ncapture.release()\ncv2.destroyAllWindows()\n'"
OpenCV models/BlurDialationErosion.py,1,"b'import cv2\nimport numpy as np\n\n\ndef main():\n    kernal = np.ones((5, 5), np.uint8)\n\n    image = cv2.imread(""../data/4.2.05.tiff"", 1)\n\n    blurimage = cv2.GaussianBlur(image, (15, 15), 0)\n\n    dialate = cv2.dilate(image, kernal, iterations=1)\n    erosion = cv2.erode(image, kernal, iterations=1)\n\n    cv2.imshow(""Dialated Image"", dialate)\n    cv2.imshow(""Eroded Image"", erosion)\n    cv2.imshow(""Blur Image"", blurimage)\n    cv2.imshow(""Orignal Image"", image)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/CannyEdgeDetection.py,0,"b'import cv2\n\n\ndef main():\n    image = cv2.imread(""../data/4.2.07.tiff"", 1)\n    cv2.imshow(""Orignal Image"", image)\n\n    output = cv2.Canny(image, 100, 151, apertureSize=3, L2gradient=True)\n    cv2.imshow(""Edge Detected Image"", output)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/ColorPalette.py,1,"b'import cv2\nimport numpy as np\n\n\ndef passFunction(x):\n    pass\n\n\ndef main():\n    windowName = ""OpenCV BGR Color Palette""\n    imageData = np.zeros((512, 512, 3), np.uint8)\n\n    cv2.namedWindow(windowName)\n\n    cv2.createTrackbar(\'Blue\', windowName, 0, 255, passFunction)\n    cv2.createTrackbar(\'Green\', windowName, 0, 255, passFunction)\n    cv2.createTrackbar(\'Red\', windowName, 0, 255, passFunction)\n\n    while (True):\n        cv2.imshow(windowName, imageData)\n\n        if cv2.waitKey(1) & 0xFF == 27:\n            break\n\n        blue = cv2.getTrackbarPos(\'Blue\', windowName)\n        green = cv2.getTrackbarPos(\'Green\', windowName)\n        red = cv2.getTrackbarPos(\'Red\', windowName)\n\n        imageData[:] = [blue, green, red]\n        print(blue, green, red)\n\n    cv2.destroyWindow(windowName)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/Contours.py,1,"b'import cv2\nimport numpy as  np\n\n\ndef main():\n    image = cv2.imread(""../data/detect_blob.png"", 1)\n\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    binay_thresh = cv2.adaptiveThreshold(gray_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 115, 1)\n\n    _, contours, _ = cv2.findContours(binay_thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    cv2.drawContours(image, contours, -1, (0, 255, 0), 3)\n\n    new_image = np.zeros((image.shape[0], image.shape[1], 3), np.uint8)\n\n    for cnt in contours:\n        cv2.drawContours(new_image, [cnt], -1, (255, 0, 255), -1)\n\n        # get contour area using \'contourArea\' method\n        area_cnt = cv2.contourArea(cnt)\n\n        # get the perimeter of any contour using \'arcLength\'\n        perimeter_cnt = cv2.arcLength(cnt, True)\n\n        # get centroid oy contour using moments\n        M = cv2.moments(cnt)\n        cx = int(M[\'m10\'] / M[\'m00\'])\n        cy = int(M[\'m01\'] / M[\'m00\'])\n\n        cv2.circle(new_image, (cx, cy), 3, (0, 255, 0), -1)\n\n        print(""Area : {}, Perimeter : {}"".format(area_cnt, perimeter_cnt))\n\n    cv2.imshow(""Contoured Image"", new_image)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/DrawShape.py,1,"b'import cv2\nimport numpy as np\n\nmode = True\nxi, yi = -1, -1  # type: (int, int)\ndrawing = False\n\nwindowName = ""Drawing Shapes""\nimage = np.zeros((512, 800, 3), np.uint8)\ncv2.namedWindow(windowName)\n\n\ndef drawShape(event, x, y, flags, params):\n    global mode, drawing, xi, yi\n\n    if event == cv2.EVENT_LBUTTONDOWN:\n        drawing = True\n        xi, yi = x, y\n\n    elif event == cv2.EVENT_MOUSEMOVE:\n        if drawing == True:\n            if mode == True:\n                cv2.rectangle(image, (xi, yi), (x, y), (0, 255, 0), -1)\n            else:\n                cv2.circle(image, (x, y), 5, (255, 0, 0), -1)\n\n    elif event == cv2.EVENT_LBUTTONUP:\n        drawing = False\n        if mode == True:\n            cv2.rectangle(image, (xi, yi), (x, y), (0, 255, 0), -1)\n        else:\n            cv2.circle(image, (x, y), 5, (255, 0, 0), -1)\n\n\ncv2.setMouseCallback(windowName, drawShape)\n\n\ndef main():\n    global mode\n\n    while True:\n        preseedKey = cv2.waitKey(1)\n        cv2.imshow(windowName, image)\n\n        if preseedKey & 0xFF == ord(\'m\') or preseedKey & 0xFF == ord(\'M\'):\n            mode = not mode\n        elif preseedKey & 0xFF == 27:\n            break\n\n    cv2.destroyWindow(windowName)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/FuzzyContour.py,2,"b'import random\n\nimport cv2\nimport numpy as np\n\n\ndef main():\n    kernal = np.ones((5, 5), np.uint8)\n    image = cv2.imread(""../data/fuzzy.png"", 1)\n\n    # converting the image into gray scale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # threshold the image to diff object for background\n    _, thresh = cv2.threshold(gray_image, 50, 255, cv2.THRESH_BINARY_INV)\n\n    # dilating the image to remove noise from objects\n    dilated_image = cv2.dilate(thresh, kernal, iterations=2)\n\n    # finding all contours in fuzzy image\n    _, contours, _ = cv2.findContours(dilated_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    # new image to draw contour objects\n    sample = np.zeros((image.shape[0], image.shape[1], 3), np.uint8)\n\n    for cnt in contours:\n\n        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n\n        # get contour area using \'contourArea\' method\n        area_cnt = cv2.contourArea(cnt)\n\n        # get the perimeter of any contour using \'arcLength\'\n        perimeter_cnt = cv2.arcLength(cnt, True)\n\n        if int(area_cnt) > 1000:\n            cv2.drawContours(sample, [cnt], -1, color, -1)\n\n        print(""Area : {}, Perimeter : {}"".format(area_cnt, perimeter_cnt))\n\n    cv2.imshow(""Contoured Image"", sample)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/Haarcascade.py,0,"b'import cv2\n\n\ndef main():\n    capture = cv2.VideoCapture(0)\n    eye_path = ""../classifier/haarcascade_eye.xml""\n    face_path = ""../classifier/haarcascade_frontalface_default.xml""\n\n    eye_cascade = cv2.CascadeClassifier(eye_path)\n    face_cascade = cv2.CascadeClassifier(face_path)\n\n    while (True):\n        _, frame = capture.read()\n\n        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n        eyes = eye_cascade.detectMultiScale(gray_frame, scaleFactor=1.05, minNeighbors=5, minSize=(10,10))\n        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.05, minNeighbors=5, minSize=(40, 40))\n\n        print(""Number of eyes : "" + str(len(eyes)))\n        print(""Number of faces : "" + str(len(faces)))\n\n        for (x, y, w, h) in eyes:\n            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n\n        for (x, y, w, h) in faces:\n            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\n        cv2.imshow(""Live Capture"", frame)\n\n        if cv2.waitKey(1) == 27:\n            break\n\n    cv2.destroyAllWindows()\n    capture.release()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/HighPassFilter.py,0,"b'import cv2\n\n\ndef main():\n    image = cv2.imread(""../data/5.1.11.tiff"", 0)\n    cv2.imshow(""Orignal Image"", image)\n\n    # Laplacian High Pass Filter\n    lap_filter = cv2.Laplacian(image, ddepth=-1, ksize=7, scale=1, borderType=cv2.BORDER_DEFAULT)\n    cv2.imshow(""Laplacian Filter"", lap_filter)\n\n    # Sobel High Pass Filter\n    sobelx_filter = cv2.Sobel(image, ddepth=-1, dx=2, dy=0, ksize=7, scale=1, borderType=cv2.BORDER_DEFAULT)\n    cv2.imshow(""Sobel X Filter"", sobelx_filter)\n\n    sobely_filter = cv2.Sobel(image, ddepth=-1, dx=0, dy=2, ksize=7, scale=1, borderType=cv2.BORDER_DEFAULT)\n    cv2.imshow(""Sobel Y Filter"", sobely_filter)\n\n    sobel_filter = sobelx_filter + sobely_filter\n    cv2.imshow(""Sobel Filter"", sobel_filter)\n\n    # Scharr High Pass Filter Implementation\n    scharrx_filter = cv2.Scharr(image, ddepth=-1, dx=1, dy=0, scale=1, borderType=cv2.BORDER_DEFAULT)\n    cv2.imshow(""Scharr X Filter"", scharrx_filter)\n\n    scharry_filter = cv2.Scharr(image, ddepth=-1, dx=0, dy=1, scale=1, borderType=cv2.BORDER_DEFAULT)\n    cv2.imshow(""Scharr Y Filter"", scharry_filter)\n\n    scharr_filter = scharrx_filter + scharry_filter\n    cv2.imshow(""Scharr Filter"", scharr_filter)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/HistogramEqualization.py,0,"b'import cv2\nimport matplotlib.pyplot as plt\n\n\ndef main():\n    image = cv2.imread(""../data/4.1.03.tiff"", 1)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    red, green, blue = cv2.split(image_rgb)\n\n    eq_red_image = cv2.equalizeHist(red)\n    eq_green_image = cv2.equalizeHist(green)\n    eq_blue_image = cv2.equalizeHist(blue)\n\n    red_hist = cv2.calcHist([red], [0], None, [256], [0, 255])\n    green_hist = cv2.calcHist([green], [0], None, [256], [0, 255])\n    blue_hist = cv2.calcHist([blue], [0], None, [256], [0, 255])\n\n    eq_red_hist = cv2.calcHist([eq_red_image], [0], None, [256], [0, 255])\n    eq_green_hist = cv2.calcHist([eq_green_image], [0], None, [256], [0, 255])\n    eq_blue_hist = cv2.calcHist([eq_blue_image], [0], None, [256], [0, 255])\n\n    channels_images = [red_hist, green_hist, blue_hist]\n    equalized_images = [eq_red_hist, eq_green_hist, eq_blue_hist]\n\n    # Channels Histogram\n    for i in range(3):\n        plt.subplot(4, 1, i + 1)\n        plt.plot(channels_images[i], color=\'g\')\n        plt.xlim([0, 255])\n\n    plt.show()\n\n    # Channels Equalized Histogram\n    for i in range(3):\n        plt.subplot(3, 1, i + 1)\n        plt.plot(equalized_images[i], color=\'b\')\n        plt.xlim([0, 255])\n\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/HoughLine.py,5,"b'import cv2\nimport numpy as np\n\n\ndef main():\n    capture = cv2.VideoCapture(0)\n\n    while True:\n        ret, frame = capture.read()\n\n        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n        edges_detec = cv2.Canny(gray_frame, 50, 250, apertureSize=5, L2gradient=True)\n\n        hough_lines = cv2.HoughLines(edges_detec, 1, np.pi / 180, 200)\n\n        if hough_lines is not None:\n            for rho, theta in hough_lines[0]:\n                x0 = rho * np.cos(theta)\n                y0 = rho * np.sin(theta)\n\n                ptsX = (int(x0 + 1000 * (-np.sin(theta))), int(y0 + 1000 * (np.cos(theta))))\n                ptsY = (int(x0 - 1000 * (-np.sin(theta))), int(y0 - 1000 * (np.cos(theta))))\n                cv2.line(frame, ptsX, ptsY, (0, 255, 0), 2)\n\n        cv2.imshow(""Capture Frame"", frame)\n\n        if cv2.waitKey(1) == 27:\n            break\n\n    cv2.destroyAllWindows()\n    capture.release()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/ImageHistogram.py,1,"b'import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef main():\n    image = cv2.imread(""../data/4.1.03.tiff"", 1)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    red_hist = cv2.calcHist([image_rgb], [0], None, [256], [0, 255])\n    green_hist = cv2.calcHist([image_rgb], [1], None, [256], [0, 255])\n    blue_hist = cv2.calcHist([image_rgb], [2], None, [256], [0, 255])\n\n    # Histogram using Matplotlib\n    plt.subplot(3, 1, 1)\n    plt.hist(image.ravel(), 256, [0, 255])\n    plt.xlim([0, 255])\n    plt.title(""Image Histogram using Matplotlib"")\n\n    # Histogram using Numpy\n    plt.subplot(3, 1, 2)\n    histogram, _ = np.histogram(image.ravel(), 256, [0, 255])\n    plt.plot(histogram, color=\'r\')\n    plt.xlim([0, 255])\n    plt.title(""Image Histogram using Numpy"")\n\n    # Histogram using Numpy\n    plt.subplot(3, 1, 3)\n    plt.plot(red_hist, color=\'r\')\n    plt.xlim([0, 255])\n    plt.title(""Image Histogram using OpenCV"")\n\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/ImageRestoration.py,0,"b'import cv2\n\n\ndef main():\n    image = cv2.imread(""../data/Damaged Image.tiff"", 1)\n    mask_image = cv2.imread(""../data/Mask.tiff"", 0)\n\n    telea_image = cv2.inpaint(image, mask_image, 5, cv2.INPAINT_TELEA)\n    ns_image = cv2.inpaint(image, mask_image, 5, cv2.INPAINT_NS)\n\n    cv2.imshow(""Orignal Image"", image)\n    cv2.imshow(""Mask Image"", mask_image)\n\n    cv2.imshow(""TELEA Restored Image"", telea_image)\n    cv2.imshow(""NS Restored Image"", ns_image)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/ImageShifting.py,1,"b'import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef main():\n    imagePath = ""../data/4.1.04.tiff""\n    shifting = np.float32([[1, 0, 50], [0, 1, 50]])\n\n    image = cv2.imread(imagePath, 1)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    rows, columns, _ = image.shape\n\n    shiftedImage = cv2.warpAffine(image, shifting, (rows, columns))\n\n    plt.imshow(shiftedImage)\n    plt.title(""Shifted Image"")\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/KernalFiltering.py,5,"b'import cv2\nimport numpy as np\n\n\ndef main():\n    image = cv2.imread(""../data/7.1.01.tiff"", 1)\n\n    \'\'\'\n    # Kernal or Convolution matrix for Identity Filter\n\n    kernal = np.array(([0, 0, 0],\n                       [0, 1, 0],\n                       [0, 0, 0]), np.float32)\n\n    # Kernal or Convolution matrix for Edge Detection\n\n    kernal = np.array(([-1, -1, -1],\n                       [-1, 8, -1],\n                       [-1, -1, -1]), np.float32)\n\n    \'\'\'\n\n    # Kernal or Convolution matrix for Box BLue Filter\n\n    kernal = np.ones((5, 5), np.uint8) / 25\n    output = cv2.filter2D(image, -1, kernal)\n\n    # Low pass filters implementation\n    box_blur = cv2.boxFilter(image, -1, (31, 31))\n    simple_blur = cv2.blur(image, (21, 21))\n    gaussian_blur = cv2.GaussianBlur(image, (51, 51), 0)\n\n    cv2.imshow(""Orignal Image"", image)\n    cv2.imshow(""Filtered Image"", output)\n\n    cv2.imshow(""Box Blur"", box_blur)\n    cv2.imshow(""Simple Blur"", simple_blur)\n    cv2.imshow(""Gaussian Blur"", gaussian_blur)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/LogicalOperation.py,0,"b'import cv2\nimport matplotlib.pyplot as plt\n\n\ndef main():\n    basePath = ""../data/""\n\n    imageFileOne = basePath + ""4.1.04.tiff""\n    imageFileTwo = basePath + ""4.1.05.tiff""\n\n    imageOne = cv2.imread(imageFileOne, 1)\n    imageTwo = cv2.imread(imageFileTwo, 1)\n\n    imageOneRGB = cv2.cvtColor(imageOne, cv2.COLOR_BGR2RGB)\n    imageTwoRGB = cv2.cvtColor(imageTwo, cv2.COLOR_BGR2RGB)\n\n    negativeImage = cv2.bitwise_not(imageOneRGB)\n    andImage = cv2.bitwise_and(imageOneRGB, imageTwoRGB)\n    orImage = cv2.bitwise_or(imageOneRGB, imageTwoRGB)\n    xorImage = cv2.bitwise_xor(imageOneRGB, imageTwoRGB)\n\n    imageNames = [imageOneRGB, imageTwoRGB, negativeImage, andImage, orImage, xorImage]\n    imageTitles = [""Image One"", ""Image Two"", ""Negative"", ""AND"", ""OR"", ""XOR""]\n\n    for i in range(6):\n        plt.subplot(2, 3, i + 1)\n        plt.imshow(imageNames[i])\n        plt.title(imageTitles[i])\n        plt.xticks([])\n        plt.yticks([])\n\n    plt.show()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
OpenCV models/MediaPlayer.py,0,"b'import cv2\n\n\ndef passFunction(x):\n    pass\n\n\ndef main():\n    windowname = ""OpenCV Media Player""\n    cv2.namedWindow(windowname)\n\n    videoFilePath = ""/media/amarpandey/Media Files/Movies/Game Of Thrones/Season Seven/Game.of.Thrones.S07E03.720p.WEB.h264-TBS[eztv].mkv""\n\n    capture = cv2.VideoCapture(videoFilePath)\n    cv2.createTrackbar(\'FrameSpeed\', windowname, 10, 600, passFunction)\n\n    while (capture.isOpened()):\n\n        FrameSpeed = cv2.getTrackbarPos(\'FrameSpeed\', windowname)\n        flag, frame = capture.read()\n\n        if FrameSpeed <= 0: FrameSpeed = 1\n\n        if flag:\n            cv2.imshow(windowname, frame)\n            if cv2.waitKey(FrameSpeed) & 0xFF == 27:  # because 33 * FPS == 1 second\n                break\n        else:\n            break\n\n    cv2.destroyWindow(windowname)\n    capture.release()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/NegativeFrame.py,0,"b'import cv2\n\n\ndef main():\n    capture = cv2.VideoCapture(0)\n\n    if capture.isOpened():\n        flag, frame = capture.read()\n    else:\n        flag = False\n\n    while flag:\n\n        flag, frame = capture.read()\n\n        frame = abs(255 - frame)\n\n        cv2.imshow(""Video Camera"", frame)\n\n        if cv2.waitKey(1) & 0xFF == 27:\n            break\n\n    cv2.destroyAllWindows()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
OpenCV models/OTSUThresholding.py,0,"b'import cv2\nimport matplotlib.pyplot as plt\n\n\ndef main():\n    threshold = 0\n    max_value = 255\n\n    image = cv2.imread(""../data/7.1.08.tiff"", 0)\n\n    # when applying OTSU threshold, set threshold to 0.\n\n    _, output1 = cv2.threshold(image, threshold, max_value, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    _, output2 = cv2.threshold(image, threshold, max_value, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n    _, output3 = cv2.threshold(image, threshold, max_value, cv2.THRESH_TOZERO + cv2.THRESH_OTSU)\n    _, output4 = cv2.threshold(image, threshold, max_value, cv2.THRESH_TOZERO_INV + cv2.THRESH_OTSU)\n    _, output5 = cv2.threshold(image, threshold, max_value, cv2.THRESH_TRUNC + cv2.THRESH_OTSU)\n\n    images = [image, output1, output2, output3, output4, output5]\n    titles = [""Orignals"", ""Binary"", ""Binary Inverse"", ""TOZERO"", ""TOZERO INV"", ""TRUNC""]\n\n    for i in range(6):\n        plt.subplot(3, 2, i + 1)\n        plt.imshow(images[i], cmap=\'gray\')\n        plt.title(titles[i])\n\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/ObjectDetection.py,6,"b'import cv2\nimport numpy as np\n\n\ndef main():\n    # Blue Color Object Range\n    blue_low = np.array([110, 100, 100])\n    blue_high = np.array([130, 255, 255])\n\n    # Green Color Object Range\n    green_low = np.array([50, 100, 100])\n    green_high = np.array([70, 255, 255])\n\n    # Red Color Object Range\n    red_low = np.array([0, 100, 100])\n    red_high = np.array([10, 255, 255])\n\n    windowOrignal = ""Orignal Live Feed""\n    windowMasked = ""Masked Window Feed""\n    windowColorObject = ""Color Object Tracked""\n\n    capture = cv2.VideoCapture(0)\n\n    if capture.isOpened():\n        flag, frame_bgr = capture.read()\n    else:\n        flag = False\n\n    while flag:\n\n        flag, frame_bgr = capture.read()\n\n        frame_hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)\n\n        masker_red = cv2.inRange(frame_hsv, red_low, red_high)\n        masked_green = cv2.inRange(frame_hsv, green_low, green_high)\n\n        red_green_masked = cv2.bitwise_or(masker_red, masked_green)\n\n        target = cv2.bitwise_and(frame_bgr, frame_bgr, mask=red_green_masked)\n\n        cv2.imshow(windowColorObject, target)\n        cv2.imshow(windowMasked, red_green_masked)\n        cv2.imshow(windowOrignal, frame_bgr)\n\n        if cv2.waitKey(1) & 0xFF == 27:\n            break\n\n    cv2.destroyAllWindows()\n    capture.release()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/ObjectDetectionUpdated.py,7,"b'import cv2\nimport numpy as np\nfrom collections import deque\n\ndef rescale_frame(frame, wpercent=130, hpercent=130):\n    width = int(frame.shape[1] * wpercent / 100)\n    height = int(frame.shape[0] * hpercent / 100)\n    return cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)\n\ndef main():\n\n    buffer_size = 64\n\n    # Blue Color Object Range\n    blue_low = np.array([110, 100, 100])\n    blue_high = np.array([130, 255, 255])\n\n    # Green Color Object Range\n    green_low = np.array([50, 100, 100])\n    green_high = np.array([70, 255, 255])\n\n    # Red Color Object Range\n    red_low = np.array([0, 100, 100])\n    red_high = np.array([10, 255, 255])\n\n    windowOrignal = ""Orignal Live Feed""\n    windowMasked = ""Masked Window Feed""\n    windowColorObject = ""Color Object Tracked""\n\n    pts = deque(maxlen=buffer_size)\n    capture = cv2.VideoCapture(0)\n\n    if capture.isOpened():\n        flag, frame_bgr = capture.read()\n    else:\n        flag = False\n\n    while flag:\n\n        center = None\n        flag, frame_bgr = capture.read()\n        frame_bgr = cv2.flip(frame_bgr, 1)\n\n        blurred = cv2.GaussianBlur(frame_bgr, (11, 11), 0)\n        frame_hsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)\n\n        mask = cv2.inRange(frame_hsv, blue_low, blue_high)\n        mask = cv2.erode(mask, None, iterations=2)\n        mask = cv2.dilate(mask, None, iterations=2)\n\n        image, contours, hierarchy = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        if len(contours) > 0:\n\n            c = max(contours, key=cv2.contourArea)\n\n            moment = cv2.moments(c)\n            ((x, y), radius) = cv2.minEnclosingCircle(c)\n            center = (int(moment[""m10""] / moment[""m00""]), int(moment[""m01""] / moment[""m00""]))\n\n            if radius > 0.5:\n                cv2.circle(frame_bgr, (int(x), int(y)), int(radius), (0, 255, 255), 2)\n                cv2.circle(frame_bgr, center, 5, (0, 0, 255), -1)\n\n        pts.append(center)\n\n        for i in range(1, len(pts)):\n            if pts[i - 1] is None or pts[i] is None:\n                continue\n            thickness = int(np.sqrt(buffer_size / float(buffer_size - i + 1)) * 2.5)\n            cv2.line(frame_bgr, pts[i - 1], pts[i], (0, 0, 255), thickness)\n\n        cv2.imshow(windowOrignal, rescale_frame(frame_bgr))\n\n        if cv2.waitKey(1) & 0xFF == 27:\n            break\n\n    cv2.destroyAllWindows()\n    capture.release()\n\nif __name__ == \'__main__\':\n    main()'"
OpenCV models/PerspectiveTransformation.py,4,"b'import cv2\nimport numpy as np\n\ncountClicks = 0\ncoordinates = np.float32([])\n\nwindowname = ""Sample Image""\nimagePath = ""../data/dummy-new.jpeg""\n\nimage = cv2.imread(imagePath)\ncopyimage = image.copy()\n\ncv2.namedWindow(windowname, cv2.WINDOW_NORMAL)\n\n\ndef captures(event, x, y, flags, params):\n    global countClicks, coordinates, image\n\n    if countClicks < 4:\n        if event == cv2.EVENT_LBUTTONDBLCLK:\n            countClicks = countClicks + 1\n            coordinates = np.append(coordinates, np.float32([x, y]), axis=0)\n            cv2.circle(image, (x, y), 5, (0, 0, 255), -1)\n\n\ncv2.setMouseCallback(windowname, captures)\n\n\ndef main():\n    global countClicks, coordinates, copyimage\n\n    cv2.resizeWindow(windowname, 700, 700)\n\n    while (countClicks < 4):\n        preseedKey = cv2.waitKey(1)\n        cv2.imshow(windowname, image)\n\n        if preseedKey & 0xFF == 27:\n            break\n\n    pointone = np.float32(\n        [[coordinates[0], coordinates[1]],\n         [coordinates[2], coordinates[3]],\n         [coordinates[4], coordinates[5]],\n         [coordinates[6], coordinates[7]]])\n    pointtwo = np.float32([[0, 0], [300, 0], [0, 300], [300, 300]])\n\n    perspective = cv2.getPerspectiveTransform(pointone, pointtwo)\n    output = cv2.warpPerspective(copyimage, perspective, (310, 310))\n\n    cv2.imshow(""Output Image"", output)\n    cv2.waitKey(0)\n\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/Pyramids.py,0,"b'import cv2\n\n\ndef main():\n    image = cv2.imread(""../data/4.2.03.tiff"", 1)\n\n    first_layer_down = cv2.pyrDown(image)\n    first_layer_up = cv2.pyrUp(first_layer_down)\n\n    laplasian = cv2.subtract(image, first_layer_up)\n\n    cv2.imshow(""Orignal Image"", image)\n    cv2.imshow(""Laplasian Image"", laplasian)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/SaltPepperNoise.py,0,"b'import random\n\nimport cv2\nimport matplotlib.pyplot as plt\n\n\ndef main():\n    probability = 0.05  # only 5% of the total image will have noise\n    imagepath = ""../data/4.1.08.tiff""\n    imagebgr = cv2.imread(imagepath, 1)\n    imagergb = cv2.cvtColor(imagebgr, cv2.COLOR_BGR2RGB)\n\n    rows, columns, channels = imagergb.shape\n\n    for i in range(rows):\n        for j in range(columns):\n            rand = random.random()\n            if rand < (probability / 2):\n                # pepper noise\n                imagergb[i][j] = [0, 0, 0]\n            elif rand < probability:\n                # salt noise\n                imagergb[i][j] = [255, 255, 255]\n\n    # Median filter to remove noise from image\n    median_filter = cv2.medianBlur(imagebgr, 5)\n    median_filter = cv2.cvtColor(median_filter, cv2.COLOR_BGR2RGB)\n\n    plt.subplot(1, 2, 1)\n    plt.imshow(imagergb)\n    plt.title(""Salt and Pepper Noise Image"")\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(median_filter)\n    plt.title(""Median Filter De-Noising (Salt and Pepper)"")\n\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/Scaling.py,0,"b'import cv2\n\n\ndef main():\n    imageOne = cv2.imread(""../data/4.1.04.tiff"", 1)\n\n    areaInter = cv2.resize(imageOne, None, fx=3, fy=3, interpolation=cv2.INTER_AREA)\n    cubicInter = cv2.resize(imageOne, None, fx=3, fy=3, interpolation=cv2.INTER_CUBIC)\n    linearInter = cv2.resize(imageOne, None, fx=3, fy=3, interpolation=cv2.INTER_LINEAR)\n    nearestInter = cv2.resize(imageOne, None, fx=3, fy=3, interpolation=cv2.INTER_NEAREST)\n    lancz0s4Inter = cv2.resize(imageOne, None, fx=3, fy=3, interpolation=cv2.INTER_LANCZOS4)\n\n    cv2.imshow(""Area Interpolation Image"", areaInter)\n    cv2.imshow(""Cubic Interpolation Image"", cubicInter)\n    cv2.imshow(""Linear Interpolation Image"", linearInter)\n    cv2.imshow(""Nearest Interpolation Image"", nearestInter)\n    cv2.imshow(""LANCZ0S4 Interpolation Image"", lancz0s4Inter)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/SkinDetection.py,1,"b'import cv2\nimport numpy as np\n\n\ndef main():\n    const = 1\n    max_value = 255\n    faces_image = cv2.imread(""../data/faces.jpeg"", 1)\n\n    faces_image = cv2.cvtColor(faces_image, cv2.COLOR_BGR2HSV)\n    faces_image_700 = cv2.resize(faces_image, (700, 700))\n\n    hue = faces_image_700[:, :, 0]\n    satr = faces_image_700[:, :, 1]\n    value = faces_image_700[:, :, 2]\n\n    hsv_images = np.concatenate((hue, satr, value), axis=1)\n\n    _, hue_thresh = cv2.threshold(hue, 10, max_value, cv2.THRESH_BINARY_INV)\n    _, satr_thresh = cv2.threshold(satr, 40, max_value, cv2.THRESH_BINARY)\n\n    skin_image = cv2.bitwise_and(hue_thresh, satr_thresh)\n\n    cv2.imshow(""Hue Image"", hue_thresh)\n    cv2.imshow(""Saturation Image"", satr_thresh)\n\n    cv2.imshow(""SKin Detected Image"", skin_image)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/SplitMerge.py,0,"b'import cv2\nimport matplotlib.pyplot as plt\n\n\ndef main():\n    imageOne = cv2.imread(""../data/house.tiff"", 1)\n    imageOne = cv2.cvtColor(imageOne, cv2.COLOR_BGR2RGB)\n\n    red, green, blue = cv2.split(imageOne)\n\n    images = [cv2.merge((red, green, blue)), red, green, blue]\n    titles = [""Default RGB Image"", ""Only Red"", ""Only Blue"", ""Only Green""]\n    cmaps = [""gray"", ""Reds"", ""Greens"", ""Blues""]\n\n    for i in range(4):\n        plt.subplot(2, 2, i + 1)\n\n        plt.imshow(images[i], cmap=cmaps[i])\n        plt.title(titles[i])\n        plt.xticks([])\n        plt.yticks([])\n\n    plt.show()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
OpenCV models/Subplot.py,0,"b'import cv2\nimport matplotlib.pyplot as plt\n\n\ndef main():\n    imageOne = cv2.imread(""../data/4.1.01.tiff"", 1)\n    imageTwo = cv2.imread(""../data/4.1.02.tiff"", 1)\n\n    imageNames = [imageOne, imageTwo]\n    imageTitles = [""First Image"", ""Second Image""]\n\n    for i in range(2):\n        plt.subplot(1, 2, i + 1)\n        plt.imshow(imageNames[i])\n        plt.title(imageTitles[i])\n        plt.xticks([])\n        plt.yticks([])\n\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/TemplateMatching.py,0,"b'import cv2\n\n\ndef main():\n    frame = cv2.imread(""../data/players.jpg"", 0)\n    template = cv2.imread(""../data/template.jpg"", 0)\n\n    result = cv2.matchTemplate(frame, template, cv2.TM_CCOEFF_NORMED)\n\n    min_value, max_value, min_loc, max_loc = cv2.minMaxLoc(result)\n\n    cv2.circle(result, max_loc, 20, 255, 1)\n\n    cv2.imshow(""Frame Image"", frame)\n    cv2.imshow(""Result Image"", result)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/Transition.py,0,"b'import cv2\n\n\ndef passFunction(x):\n    pass\n\n\ndef main():\n    windowName = ""Transition Effect""\n\n    cv2.namedWindow(""Transition Effect"")\n\n    imageOne = cv2.imread(""../data/lena_color_512.tif"", 1)\n    imageTwo = cv2.imread(""../data/mandril_color.tif"", 1)\n\n    cv2.createTrackbar(""Alpha"", windowName, 0, 1000, passFunction)\n\n    while True:\n\n        alpha = cv2.getTrackbarPos(""Alpha"", windowName) / 1000\n        beta = 1 - alpha\n\n        output = cv2.addWeighted(imageOne, alpha, imageTwo, beta, 0)\n\n        cv2.imshow(windowName, output)\n\n        if cv2.waitKey(1) & 0xFF == 27:\n            break\n\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/Transparency.py,0,"b'import cv2\n\n\ndef main():\n    image = cv2.imread(""../data/house.tiff"", 1)\n\n    # Instead of split sue this way, more fast\n    blue = image[:, :, 0]\n    green = image[:, :, 1]\n    red = image[:, :, 2]\n\n    # combine each color spacing with alpha channel\n    rgba = cv2.merge((blue, green, red, red))\n\n    # cv2 gui does not support alpha value, so use default OS viewer\n    # use png format, because jpeg does not support alpha channel\n    print(cv2.imwrite(""../data/rgba_red.png"", rgba))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/VideoCamera.py,0,"b'import cv2\n\n\ndef main():\n    window_name = ""Live Video Feed""\n    cv2.namedWindow(window_name)\n\n    capture = cv2.VideoCapture(0)\n\n    capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n    capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n\n    if capture.isOpened():\n        flag, frame = capture.read()\n    else:\n        flag = False\n\n    while flag:\n\n        flag, frame = capture.read()\n\n        cv2.imshow(window_name, frame)\n        if cv2.waitKey(1) & 0xFF == 27:\n            break\n\n    cv2.destroyWindow(window_name)\n    capture.release()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/VideoCapture.py,0,"b'import cv2\n\n\ndef main():\n    framerate = 30\n    resolution = (640, 480)\n    window_name = ""Live Video Feed""\n    videoCapturePath = ""../Output/Output.avi""\n\n    cv2.namedWindow(window_name)\n    capture = cv2.VideoCapture(0)\n\n    codec = cv2.VideoWriter_fourcc(\'W\', \'M\', \'V\', \'2\')\n\n    videoCapture = cv2.VideoWriter(videoCapturePath, codec, framerate, resolution)\n\n    if capture.isOpened():\n        flag, frame = capture.read()\n    else:\n        flag = False\n\n    while flag:\n\n        flag, frame = capture.read()\n        videoCapture.write(frame)\n        cv2.imshow(window_name, frame)\n        if cv2.waitKey(1) & 0xFF == 27:\n            break\n\n    cv2.destroyWindow(window_name)\n    videoCapture.release()\n    capture.release()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/VideoRotation.py,0,"b'import time\n\nimport cv2\n\n\ndef main():\n    angle = 0\n    scale = 0.1\n    windowName = ""Video Rotation""\n    cv2.namedWindow(windowName, cv2.WINDOW_FULLSCREEN)\n\n    capture = cv2.VideoCapture(0)\n\n    if capture.isOpened():\n        flag, frame = capture.read()\n    else:\n        flag = False\n\n    rows, cols, channels = frame.shape\n\n    while flag:\n\n        if angle > 360:\n            angle = 0\n\n        if scale < 2:\n            scale = scale + 0.1\n        else:\n            scale = 0.1\n\n        flag, frame = capture.read()\n\n        rotationmatrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, scale)\n        transformedoutput = cv2.warpAffine(frame, rotationmatrix, (cols, rows))\n\n        cv2.imshow(windowName, transformedoutput)\n\n        angle = angle + 1\n        time.sleep(0.01)\n\n        if cv2.waitKey(1) & 0xFF == 27:\n            break\n\n    cv2.destroyAllWindows()\n    capture.release()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/featureDetection.py,0,"b'import cv2\n\n\ndef main():\n    org_image = cv2.imread(""../data/house.tiff"", 1)\n    \'\'\'\n    SURF is better than SIFT and computes and detects feature fast, \n    but unfortunately both are paid.\n\n    Alternative, we have ORB by OpenCV. Free. OSS.\n    PARAM: nfeatures : Number of features to be detected.\n                       Default value is around 100.\n    \'\'\'\n\n    sift = cv2.xfeatures2d.SIFT_create()\n    surf = cv2.xfeatures2d.SURF_create()\n    orb = cv2.ORB_create(nfeatures=1000)\n\n    kp_sift, decep_sift = sift.detectAndCompute(org_image, None)\n    kp_surf, decep_sift = surf.detectAndCompute(org_image, None)\n    kp_orb, decep_sift = orb.detectAndCompute(org_image, None)\n\n    org_image_sift = cv2.drawKeypoints(org_image, kp_sift, None)\n    org_image_surf = cv2.drawKeypoints(org_image, kp_surf, None)\n    org_image_orb = cv2.drawKeypoints(org_image, kp_orb, None)\n\n    cv2.imshow(""SIFT Features Detected"", org_image_sift)\n    cv2.imshow(""SURF Features Detected"", org_image_surf)\n    cv2.imshow(""ORB Features Detected"", org_image_orb)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
OpenCV models/playground.py,2,"b'import cv2\nimport numpy as np\n\n\ndef main():\n    image = cv2.imread(""../data/house.tiff"", 1)\n\n    blue, green, red = cv2.split(image)\n    rows, columns, channels = image.shape\n\n    output = np.empty((rows, columns * 3, 3), np.uint8)\n\n    output[:, 0:columns] = cv2.merge([blue, blue, blue])\n    output[:, columns:columns * 2] = cv2.merge([green, green, green])\n    output[:, columns * 2:columns * 3] = cv2.merge([red, red, red])\n\n    hsvimage = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    hue, satr, vlue = cv2.split(hsvimage)\n    hsvoutput = np.concatenate((hue, satr, vlue), axis=1)\n\n    cv2.imshow(""Sample Image"", image)\n    cv2.imshow(""Output Image"", output)\n    cv2.imshow(""HSV Image"", hsvoutput)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
