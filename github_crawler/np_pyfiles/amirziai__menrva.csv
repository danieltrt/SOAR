file_path,api_count,code
app.py,0,"b'from flask import Flask, request, jsonify, render_template, Response\nimport redis\n# from sklearn.externals import joblib as pickle\nimport cPickle as pickle  # \nimport uuid\nimport os\nimport pandas as pd\nimport uuid as uuid_\nimport sqlite3\nimport json\nfrom datetime import datetime\nfrom functools import wraps\n\n\n# TODO\n# need to get data as JSON and not form data in upload\n\n# sqlite database\n# create table models (id text primary key, name text, description text, input text, output text, link text, timestamp datetime);\n# redis flushall removes all keys\n\napp = Flask(__name__)\ndb_path = \'db/models.sqlite\'\n\n\ndef redis_server_connection():\n\treturn redis.StrictRedis(host=\'localhost\', port=6379, db=0)\n\n\n@app.route(\'/\', methods=[\'GET\'])\ndef root():\n\treturn render_template(\'index.html\')\n\n\n@app.route(\'/models\', methods=[\'GET\'])\ndef models():\n\tconn = sqlite3.connect(db_path)\n\tmodels = pd.read_sql_query(\'select * from models\', conn).sort_values(by=\'timestamp\', ascending=False)\n\treturn jsonify({\'status\': \'ok\', \'data\': json.loads(models.to_json(orient=\'records\'))})\n\n\ndef check_auth(username, password):\n    """"""This function is called to check if a username /\n    password combination is valid.\n    """"""\n    return username == \'menrva\' and password == \'menrva\'\n\n\ndef authenticate():\n    """"""Sends a 401 response that enables basic auth""""""\n    return Response(\n            \'Could not verify your access level for that URL.\\n\'\n            \'You have to login with proper credentials\', 401,\n            {\'WWW-Authenticate\': \'Basic realm=""Login Required""\'})\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        auth = request.authorization\n        if not auth or not check_auth(auth.username, auth.password):\n            return authenticate()\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\'/upload\', methods=[\'POST\', \'GET\'])\n@requires_auth\ndef upload():\n\tif request.method == \'GET\':\n\t\treturn render_template(\'upload.html\')\n\n\tfile_uploaded = request.files[\'file\']\n\n\t# find a unique uuid\n\tpath = None\n\twhile True:\n\t\tuuid = uuid_.uuid4()\n\t\tpath = \'models/%s\' % uuid\n\n\t\tif not os.path.exists(path):\n\t\t\tbreak\n\n\n\tfile_uploaded.save(path)  # save to disk\n\tprint \'%s: saved to disk\' % uuid\n\tredis_server = redis_server_connection()\n\n\t# save to database\n\tjson_ = dict(request.form)\n\tprint json_\n\tconn = sqlite3.connect(db_path)\n\tcursor = conn.cursor()\n\trow = str(uuid), json_[\'name\'][1], json_[\'description\'][1], json_[\'input\'][1], json_[\'output\'][1], json_[\'link\'][1], datetime.now()\n\tprint row\n\tcursor.execute(""insert into models values (?, ?, ?, ?, ?, ?, ?)"", row)\n\tconn.commit()\n\n\t# print file_uploaded.read()\n\t# TODO: reading back from disk, couldn\'t get file_uploaded.read() working, produced \'\'\n\tredis_server.set(uuid, open(path).read())\n\tprint \'%s: saved to memory (redis)\' % uuid\n\n\treturn jsonify({\'status\': \'ok\', \'message\': \'model was uploaded\', \'uuid\': uuid})\n\n\n@app.route(\'/predict/<uuid>\', methods=[\'POST\'])\ndef predict(uuid):\n\tredis_server = redis_server_connection()\n\t\n\t# load the model into memory\n\tif redis_server.exists(uuid):\n\t\t# print redis_server.get(uuid)\n\t\tprint \'Loading from redis\'\n\t\tmodel = pickle.loads(redis_server.get(uuid))\n\n\telse:\n\t\tprint \'%s: not found in memory\' % uuid\n\t\tpath = \'models/%s\' % uuid\n\n\t\tif not os.path.exists(path):\n\t\t\tprint \'%s: does not exist on disk (ERROR)\' % uuid\n\t\t\treturn jsonify({\'status\': \'error\', \'message\': \'model does not exist\'})\n\t\telse:\n\t\t\tprint \'%s: serialized to memory (redis)\' % uuid \n\t\t\tmodel = pickle.loads(path)\n\t\t\tredis_server.set(uuid, open(path))  # serialize to memory\n\n\n\ttry:\n\t\tjson_ = request.json\n\t\tquery = pd.DataFrame([json_])\n\t\treturn jsonify({\'prediction\': list(model.predict(query))})\n\n\texcept Exception, e:\n\t\tprint \'error\'\n\t\tprint e\n\t\treturn jsonify({\'status\': \'error\', \'message\': \'model prediction failed\', \'error\': e})\n\n\nif __name__ == \'__main__\':\n\tapp.run(host=\'0.0.0.0\', debug=True, port=80)'"
insights.py,9,"b'from time import time\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score\n\nrandom_state = 0\ncv = 5  # number of folds for cross validation\n\n\ndef decision_tree(X, y, regression, max_depth=3):\n    from sklearn.tree import export_graphviz\n    from sklearn.externals.six import StringIO  \n    from IPython.core.pylabtools import figsize\n    from IPython.display import Image\n    figsize(12.5, 6)\n    import pydot\n    \n    if regression:\n        clf = DecisionTreeRegressor(max_depth=max_depth)\n    else:\n        clf = DecisionTreeClassifier(max_depth=max_depth)\n        \n    clf.fit(X, y)\n    dot_data = StringIO()  \n    export_graphviz(clf, out_file=dot_data, feature_names=list(X.columns),\n                    filled=True, rounded=True,)\n    graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n    return Image(graph.create_png())\n\n\ndef viz(clf, clf_raw):\n    from sklearn.metrics import classification_report\n    from sklearn.metrics import confusion_matrix\n    \n    print \'Accuracy: %s\' % clf.score(x_test, y_test)\n    \n    try:\n        featureImportance(x_train, clf, 0.01)\n    except:\n        pass\n        \n    plot_learning_curve(clf_raw,\n                        \'Learning curves\', x_train, y_train, cv=5)\n    y_pred = clf.predict(x_test)\n    print classification_report(y_test, y_pred)\n\n    plot_roc_curve(clf, (x_train, y_train, x_test, y_test), False)\n    precision_recall_curve(clf, x_test, y_test)\n\n    statistics(clf, x_train, y_train, x_test, y_test, False)\n\n    plot_confusion_matrix(clf, x_test, y_test)\n    print confusion_matrix(y_test, y_pred)\n\n    # instances\n    instances= {\'good\': [], \'bad\': []}\n    preds_proba = clf.predict_proba(x_test)\n    preds = clf.predict(x_test)\n\n    l = []\n    for i in range(len(y_test)):\n        correct_index = list(y_test)[i]\n        pred = preds[i]\n        pred_proba = preds_proba[i][pred]\n\n        instances[(\'good\' if pred == correct_index else \'bad\')].append(pred_proba)\n\n        l.append({\'Type\': (\'Correct\' if pred == correct_index else \'Incorrect\'),\n                 \'Outcome\': le.inverse_transform(correct_index),\n                 \'Probability\': pred_proba})\n\n    df_sns_proba = pd.DataFrame(l)\n\n    pd.Series(instances[\'good\']).hist()\n    pd.Series(instances[\'bad\']).hist()\n    plt.show()\n    l = [{\'Type\': \'Correct\', \'Probability\': x} for x in instances[\'good\']]\n    l += [{\'Type\': \'Incorrect\', \'Probability\': x} for x in instances[\'bad\']]\n    df_sns = pd.DataFrame(l)\n    sns.violinplot(x=\'Type\', y=\'Probability\', data=df_sns)\n    plt.show()\n    sns.boxplot(x=\'Type\', y=\'Probability\', data=df_sns)\n    plt.show()\n    sns.boxplot(x=\'Outcome\', y=\'Probability\', hue=\'Type\', data=df_sns_proba)\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n    plt.show()\n    probabilities_to_check = np.linspace(0, 1, num=11)\n\n    l = []\n    for probability_threshold in probabilities_to_check:\n        a = [list(y_test)[i] == preds[i] for i, p in enumerate(preds_proba) if np.max(p) >= probability_threshold]\n        accuracy = np.nan\n        if len(a) > 0:\n            accuracy = sum(a) / float(len(a))\n            \n        l.append({\'Probability threshold\': probability_threshold,\n                 \'Percentage of cases\': len(a) / float(len(y_test)),\n                  \'Accuracy\': accuracy\n                 })\n\n    a = pd.DataFrame(l)[[\'Probability threshold\', \'Percentage of cases\', \'Accuracy\']]\n    print a\n\n    plt.plot(a[\'Probability threshold\'], a[\'Percentage of cases\'])\n    plt.plot(a[\'Probability threshold\'], a[\'Accuracy\'], c=\'g\')\n    plt.ylim([0, 1])\n    plt.show()\n    plt.plot(a[\'Probability threshold\'], a[\'Percentage of cases\'].diff().cumsum())\n    plt.plot(a[\'Probability threshold\'], a[\'Accuracy\'].diff().cumsum())\n    plt.show()\n\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    from sklearn.learning_curve import learning_curve\n    \n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(""Training examples"")\n    plt.ylabel(""Score"")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=""r"")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=""g"")\n    plt.plot(train_sizes, train_scores_mean, \'o-\', color=""r"",\n             label=""Training score"")\n    plt.plot(train_sizes, test_scores_mean, \'o-\', color=""g"",\n             label=""Cross-validation score"")\n\n    plt.legend(loc=""best"")\n    plt.show()\n\n\ndef plot_roc_curve(clf, data, regression):\n    from sklearn.metrics import roc_curve, auc\n    \n    x_train, y_train, x_test, y_test = data\n    \n    if regression:\n        """"""\n        plots actual vs. predicted\n        plots error\n        """"""\n        pred_train = clf.predict(x_train)\n        pred_test = clf.predict(x_test) \n        plt.scatter(y_test, pred_test, color=\'r\', label=\'X\')\n        plt.scatter(y_train, pred_train)\n        plt.plot([min(min(pred_train), min(pred_test)), max(max(pred_train), max(pred_test))],\n                 [min(min(y_train), min(y_test)), max(max(y_train), max(y_test))], \'k--\')\n        plt.title(\'Predicted vs. actual for train and test\')\n        plt.legend()\n        plt.show()\n        \n        print \'\\n\'\n        \n        plt.plot([x - y for x, y in zip(y_test, pred_test)], color=\'r\')\n        plt.plot([x - y for x, y in zip(y_train, pred_train)])\n        plt.title(\'Actual - prediction (error)\')\n        plt.show()\n    else:\n        """"""\n        plots the ROC curve\n        """"""\n        pred_train_prob = [x[1] for x in clf.predict_proba(x_train)]\n        pred_test_prob = [x[1] for x in clf.predict_proba(x_test)]\n        fpr, tpr, thresholds = roc_curve(y_train, pred_train_prob)\n        roc_auc = auc(fpr, tpr)\n        plt.title(\'Receiver Operating Characteristic\')\n        plt.plot(fpr, tpr, \'b\',\n        label=\'AUC = %0.2f\'% roc_auc)\n        fpr, tpr, thresholds = roc_curve(y_test, pred_test_prob)\n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, \'r\',\n                label=\'AUC = %0.2f\'% roc_auc)\n        plt.legend(loc=\'lower right\')\n        plt.plot([0,1],[0,1],\'k--\')\n        plt.xlim([-0.1,1.2])\n        plt.ylim([-0.1,1.2])\n        plt.ylabel(\'True Positive Rate\')\n        plt.xlabel(\'False Positive Rate\')\n        plt.show()\n        \n        \ndef plot_confusion_matrix(clf, x_test, y_test, title=\'Confusion matrix\', cmap=plt.cm.Blues):\n    from sklearn.metrics import confusion_matrix\n    \n    y_pred = clf.predict(x_test)\n    cm = confusion_matrix(y_test, y_pred)\n    \n    plt.imshow(cm, interpolation=\'nearest\', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    # tick_marks = np.arange(len(iris.target_names))\n    # plt.xticks(tick_marks, iris.target_names, rotation=45)\n    # plt.yticks(tick_marks, iris.target_names)\n    plt.tight_layout()\n    plt.ylabel(\'True label\')\n    plt.xlabel(\'Predicted label\')\n    plt.show()\n    \n    \ndef precision_recall_curve(clf, x_test, y_test):\n    from sklearn.metrics import precision_recall_curve\n    \n    for i in range(2):\n        y_probabilities = [x[i] for x in clf.predict_proba(x_test)]\n        precision, recall, thresholds = precision_recall_curve(y_test, y_probabilities)\n\n        plt.title(\'Precision Recall Curve\')\n        plt.plot(recall, precision, \'b\')\n\n    plt.show()\n\n\ndef barh_dic(f, title=None):\n    import operator\n    \n    y = sorted(f.items(), key=operator.itemgetter(1))\n    keys = [a[0] for a in y]\n    vals = [a[1] for a in y]\n    plt.barh(range(len(y)), vals, align=\'center\')\n    plt.yticks(range(len(y)), keys)\n    \n    if title:\n        plt.title(title)\n        \n    plt.show()\n\n\ndef feature_importance(X, clf, threshold=0.03, return_=False, show=True):\n    item = clf.feature_importances_\n        \n    val = dict((x, y) for x, y in zip(X.columns, item))\n    \n    val_ = dict({k:val[k] for k in val if val[k] >= threshold})\n        \n    if show:\n        barh_dic(val_, \'Feature importance\')\n\n    if return_:\n        return val\n    \n    \ndef statistics(clf, x_train, y_train, x_test, y_test, regression):\n    from sklearn.metrics import precision_score\n    from sklearn.metrics import f1_score\n    from sklearn.metrics import recall_score\n    from sklearn.metrics import roc_auc_score\n    \n    # if regression:\n    #     r2_train = clf.score(x_train, y_train)\n    #     r2_test = clf.score(x_test, y_test)\n    #     mse_train = mse(y_train, clf.predict(x_train))\n    #     mse_test = mse(y_test, clf.predict(x_test))\n    #     mae_train = mae(y_train, clf.predict(x_train))\n    #     mae_test = mae(y_test, clf.predict(x_test))\n    #     # mean relative error\n    #     mre_train = mean_relative_error(y_train, clf.predict(x_train))\n    #     mre_test = mean_relative_error(y_test, clf.predict(x_test))\n    #\n    #     return pd.DataFrame({\n    #             \'R2 train\': [r2_train],\n    #             \'R2 test\': [r2_test],\n    #             \'R2 %\': [r2_test / r2_train - 1],\n    #             \'MSE train\': [mse_train],\n    #             \'MSE test\': [mse_test],\n    #             \'MSE %\': mse_test / mse_train - 1,\n    #             \'MAE train\': [mae_train],\n    #             \'MAE test\': [mae_test],\n    #             \'MAE %\': mae_test / mae_train - 1,\n    #             \'MRE train\': [mre_train],\n    #             \'MRE test\': [mre_test],\n    #             \'MRE %\': mre_test / mre_train - 1\n    #         }).transpose()\n    #\n    # else:\n    accuracy_train = clf.score(x_train, y_train)\n    accuracy_test = clf.score(x_test, y_test)\n    precision_train = precision_score(y_train, clf.predict(x_train))\n    precision_test = precision_score(y_test, clf.predict(x_test))\n    recall_train = recall_score(y_train, clf.predict(x_train))\n    recall_test = recall_score(y_test, clf.predict(x_test))\n    f1_train = f1_score(y_train, clf.predict(x_train))\n    f1_test = f1_score(y_test, clf.predict(x_test))\n\n    roc_train = -1\n    roc_test = -1\n    if hasattr(clf, \'predict_proba\'):\n        roc_train = roc_auc_score(y_train, [x[1] for x in clf.predict_proba(x_train)])\n        roc_test = roc_auc_score(y_test, [x[1] for x in clf.predict_proba(x_test)])\n\n    val = {\n            \'Accuracy train\': [accuracy_train],\n            \'Accuracy test\': [accuracy_test],\n            \'Accuracy %\': accuracy_test / accuracy_train - 1,\n            \'Precision train\': [precision_train],\n            \'Precision test\': [precision_test],\n            \'Precision %\': precision_test / precision_train - 1,\n            \'Recall train\': [recall_train],\n            \'Recall test\': [recall_test],\n            \'Recall %\': recall_test / recall_train - 1,\n            \'F1 train\': [f1_train],\n            \'F1 test\': [f1_test],\n            \'F1 %\': f1_test / f1_train - 1,\n    }\n\n    if roc_train != -1:\n        val[\'ROC train\'] = [roc_train]\n        val[\'ROC test\'] = [roc_test]\n        val[\'ROC %\'] = roc_test / roc_train - 1\n\n    return pd.DataFrame(val).transpose()\n\n\ndef clf_predict_proba(clf, x):\n    return [_[1] for _ in clf.predict_proba(x)]\n\n\ndef clf_scores(clf, x_train, y_train, x_test, y_test):\n    info = dict()\n\n    # TODO: extend this to a confusion matrix per fold for more flexibility downstream (tuning)\n    # TODO: calculate a set of ROC curves per fold instead of running it on test, currently introducing bias\n    scores = cross_val_score(clf, x_train, y_train, cv=cv, n_jobs=-1)\n    runtime = time()\n    clf.fit(x_train, y_train)\n    runtime = time() - runtime\n    y_test_predicted = clf.predict(x_test)\n    info[\'runtime\'] = runtime\n    info[\'accuracy\'] = min(scores)\n    info[\'accuracy_test\'] = accuracy_score(y_test, y_test_predicted)\n    info[\'accuracy_folds\'] = scores\n    info[\'confusion_matrix\'] = confusion_matrix(y_test, y_test_predicted)\n    clf.fit(x_train, y_train)\n    fpr, tpr, _ = roc_curve(y_test, clf_predict_proba(clf, x_test))\n    info[\'fpr\'] = fpr\n    info[\'tpr\'] = tpr\n    info[\'auc\'] = auc(fpr, tpr)\n\n    return info\n\n\ndef generate_insights(scores):\n    print \'AUC curves\'\n\n    for model in scores:\n        fpr = scores[model][\'fpr\']\n        tpr = scores[model][\'tpr\']\n        auc_score = scores[model][\'auc\']\n        plt.plot(fpr, tpr, label=\'AUC %s = %0.2f\' % (model, auc_score))\n\n    plt.legend(loc=\'lower right\')\n    plt.plot([0, 1], [0, 1], \'k--\')\n    plt.xlim([-0.1, 1.2])\n    plt.ylim([-0.1, 1.2])\n    plt.ylabel(\'True Positive Rate\')\n    plt.xlabel(\'False Positive Rate\')\n    plt.show()\n\n    print \'Metric over fold distribution\'\n\n    metric_distribution = {k: scores[k][\'accuracy_folds\'] for k in scores.keys()}\n    pd.DataFrame(metric_distribution).boxplot(return_type=\'dict\');\n    plt.show()\n\n    print \'Training time\'\n    pd.Series({k:scores[k][\'runtime\'] for k in scores}).plot(kind=\'bar\');\n\ndef fit_vanilla(x_train, x_test, y_train, y_test):\n    scores = dict()\n\n    # Decision tree\n    dt = DecisionTreeClassifier(random_state=random_state)\n    scores[\'dt\'] = clf_scores(dt, x_train, y_train, x_test, y_test)\n\n    # Logistic Regression\n    lr = LogisticRegression(random_state=random_state, n_jobs=-1)\n    scores[\'lr\'] = clf_scores(lr, x_train, y_train, x_test, y_test)\n\n    # Random Forest\n    rf = RandomForestClassifier(random_state=random_state, n_jobs=-1)\n    scores[\'rf\'] = clf_scores(rf, x_train, y_train, x_test, y_test)\n\n    return scores\n'"
preprocessing.py,0,"b""import pandas as pd\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\n\n\ndef remove_columns(df, exclusions):\n    if len(exclusions) > 0:\n        return df[df.columns.difference(exclusions)]\n    else:\n        return df\n    \n    \ndef report_column_alteration(column, action, notes):\n    if type(notes) == float:\n        notes = '{:0.1f}%'.format(notes)\n        \n    print '%15s %25s %15s' % (column, action, notes)\n\n    \ndef prepare_df(df, target, target_label_encoder=None, report=True):\n    columns_to_ohe = []  # columns to be one-hot-encoded\n    columns_to_remove = []\n    target_label_encoder = None\n    \n    for col, col_type in zip(df.dtypes.index, df.dtypes.values):\n        if col_type == 'O':\n            if col == target:\n                if report:\n                    print('Encoding the target variable')\n                    \n                if not target_label_encoder:\n                    target_label_encoder = LabelEncoder()\n                    target_label_encoder.fit(df[col])\n                \n                df[col] = target_label_encoder.transform(df[col])\n\n            else:\n                ratio = df[col].nunique() / float(len(df))\n                \n                if ratio < 0.1:\n                    columns_to_ohe.append(col)\n                else:\n                    columns_to_remove.append(col)\n                    \n                    if report:\n                        report_column_alteration(col, 'Removed- overly unique', ratio * 100)\n\n        else:\n            count_na = int(len(df) - df[col].count())\n            if count_na > 0:\n                df[col] = df[col].fillna(-999)\n                \n                if report:\n                    report_column_alteration(col, 'Fill NA', (100 * count_na / float(len(df))))\n            \n    return df, columns_to_remove, columns_to_ohe, target_label_encoder\n\n\ndef prepare_df_pipeline(df, target, exclusions, target_label_encoder=None, report=True):\n    df = remove_columns(df, exclusions)\n    df, columns_to_remove, columns_to_ohe, target_label_encoder = prepare_df(df,\n                                                                             target,\n                                                                             target_label_encoder,\n                                                                             report)\n    df = remove_columns(df, columns_to_remove)\n    df = pd.get_dummies(df, columns=columns_to_ohe)\n    X, y = get_xy(df, target)\n    \n    return X, y, target_label_encoder\n\n\ndef get_xy(df, target):\n    X = df[df.columns.difference([target])]\n    y = df[target]\n    return X, y\n\n\ndef train_test_xy(df, target, exclusions, file_name_test, test_set_percentage):\n    X, y, target_label_encoder = prepare_df_pipeline(df, target, exclusions)\n\n    if file_name_test:\n        X_train, y_train = X, y\n        df_test = pd.read_csv(file_name_test)\n        X_test, y_test, _ = prepare_df_pipeline(df_test, target, exclusions, target_label_encoder, False)\n\n    else:\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_set_percentage)\n        \n    print '=' * 30\n    print 'Features  : %s' % X_train.shape[1]\n    print 'Train set : %s' % X_train.shape[0]\n    print 'Test set  : %s' % X_test.shape[0]\n    print '=' * 30\n        \n    return X_train, X_test, y_train, y_test, target_label_encoder"""
tuning.py,3,"b'from time import time\n\nimport numpy as np\nimport pandas as pd\n\n# utilities\nfrom sklearn.grid_search import RandomizedSearchCV\n\n# models\nfrom sklearn.linear_model import Perceptron, LogisticRegression, BayesianRidge, SGDClassifier, \\\n    PassiveAggressiveClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n# more information about sklearn\'s incremental models\n# http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_comparison.html\n# http://scikit-learn.org/stable/modules/scaling_strategies.html\n# TODO add xgboost, keras\nmodels_linear = [Perceptron, LogisticRegression, BayesianRidge, LinearSVC]\nmodels_online = [Perceptron, MultinomialNB, SGDClassifier, PassiveAggressiveClassifier]\nmodels_nonlinear_cheap = [DecisionTreeClassifier]\nmodels_nonlinear_expensive = [RandomForestClassifier, SVC, GradientBoostingClassifier]\n\nhyperparameters = {\n    LogisticRegression: {\'C\': [1, 10, 50, 100, 1000, 10000]},\n  # [{\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']},\n  # {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']},]\n    RandomForestClassifier: {""max_depth"": [3, None],\n              ""max_features"": [1, 3, 10],\n              ""min_samples_split"": [1, 3, 10],\n              ""min_samples_leaf"": [1, 3, 10],\n              ""bootstrap"": [True, False],\n              ""criterion"": [""gini"", ""entropy""]},\n}\n\n# parameters\nrandom_state = 0\nrandom_search_iterations_max = 10\n\n\ndef requirements_bare_minimum(y_train):\n    # by default model has to beat:\n    # 1- random guessing (AUC must be higher than 0.5), and\n    # 2- majority vote classification, important for imbalanced datasets, and\n    # 3- cross-validation variation should not be too high (less than 0.1)\n\n    return {\'auc\': 0.5,\n            \'accuracy\': pd.Series(y_train).value_counts(normalize=True)[0],\n            \'accuracy_std\': 0.1}\n\n\ndef check_requirements(model, requirements):\n    if model_insights[\'accuracy\'] > requirements[\'accuracy\'] and \\\n                        model_variation < requirements[\'accuracy_std\'] and \\\n                        model_insights[\'auc\'] > requirements[\'auc\'] and \\\n                        model_insights[\'accuracy_test\'] > requirements[\'accuracy\']:\n        return True\n    else:\n        return False\n\n\ndef tune(insights, x_train, y_train, x_test, y_test, models=\'all\', requirements=None, maximize=False):\n    if requirements is None:\n        requirements = requirements_bare_minimum(y_train)\n\n    # do vanilla models satisfy the requirements?\n    # assuming decision tree is the most intuitive, then logistic regression and then random forest\n    # TODO: extend this to metrics other than accuracy using the confusion matrix\n    for model_name in [\'dt\', \'lr\', \'rf\']:\n        model_insights = insights[model_name]\n        model_variation = np.std(model_insights[\'accuracy_folds\'])\n\n        if check_requirements(model_insights, requirements) and not maximize:\n            pass\n            # TODO: turn this back on\n            # return model_name\n\n    # model selection and tuning loop\n    models_to_train = []\n\n    if models == \'all\':\n        models_to_train += models_linear + models_nonlinear_cheap + models_nonlinear_expensive\n    elif models == \'linear\':\n        models_to_train += models_online\n    elif models_to_train == \'cheap\':\n        models_to_train += models_linear + models_nonlinear_cheap\n\n    # TODO: using all of the training data, need to use less data if runtime for insights models is large (how large?)\n    for model in models_to_train:\n        # TODO: add the looping logic\n        if model == LogisticRegression:\n            number_configurations = np.prod(np.array([len(_) for _ in hyperparameters[model]]))\n            random_search_iterations = np.min([random_search_iterations_max, number_configurations])\n            random_search = RandomizedSearchCV(model(n_jobs=-1, random_state=random_state),\n                param_distributions=hyperparameters[model], n_iter=random_search_iterations, n_jobs=-1, random_state=0)\n            runtime = time()\n            random_search.fit(x_train, y_train)\n            runtime = time() - runtime\n\n            info = dict()\n            info[\'runtime\'] = runtime\n            # info[\'accuracy\'] = min(scores)\n            # info[\'accuracy_test\'] = accuracy_score(y_test, y_test_predicted)\n            # info[\'accuracy_folds\'] = scores\n            # info[\'confusion_matrix\'] = confusion_matrix(y_test, y_test_predicted)\n            # clf.fit(x_train, y_train)\n            # fpr, tpr, _ = roc_curve(y_test, clf_predict_proba(clf, x_test))\n            # info[\'fpr\'] = fpr\n            # info[\'tpr\'] = tpr\n            # info[\'auc\'] = auc(fpr, tpr)\n\n            return random_search\n\n    return None'"
