file_path,api_count,code
model.py,2,"b'import tensorflow as tf\nfrom tensorflow.models.rnn import rnn_cell\nfrom tensorflow.models.rnn import seq2seq\nfrom tensorflow.python.ops import rnn\n\nimport numpy as np\n\nclass Model():\n    def __init__(self, args, infer=False):\n        self.args = args\n        self.args.data_dim = 1\n        if infer:\n            args.batch_size = 1\n            args.seq_length = 1\n\n        if args.model == \'rnn\':\n            cell_fn = rnn_cell.BasicRNNCell\n        elif args.model == \'gru\':\n            cell_fn = rnn_cell.GRUCell\n        elif args.model == \'lstm\':\n            cell_fn = rnn_cell.BasicLSTMCell\n        else:\n            raise Exception(""model type not supported: {}"".format(args.model))\n\n        one_cell = cell_fn(args.rnn_size)\n\n        self.cell = cell = rnn_cell.MultiRNNCell([one_cell] * args.num_layers)\n\n        self.input_data = tf.placeholder(tf.float32, \\\n                [args.batch_size, args.seq_length])\n        self.targets = tf.placeholder(tf.float32, \\\n                [args.batch_size, args.seq_length])\n        self.initial_state = cell.zero_state(args.batch_size, tf.float32)\n\n        inputs = tf.split(1, args.seq_length, self.input_data)\n\n        outputs, last_state = \\\n                rnn.rnn(cell, inputs, self.initial_state,\n                        dtype=tf.float32)\n        self.final_state = last_state\n\n        output = tf.reshape(tf.concat(1, outputs), [-1, args.rnn_size])\n\n        softmax_w = tf.get_variable(""softmax_w"", \\\n                [args.rnn_size, self.args.data_dim])\n        softmax_b = tf.get_variable(""softmax_b"", \\\n                [self.args.data_dim])\n\n        self.logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n\n        flat_targets = tf.reshape(self.targets, [-1])\n        print(self.logits)\n        print(self.targets)\n        self.cost = tf.reduce_sum(tf.pow(self.logits-flat_targets, 2))/ \\\n                (2*(args.batch_size*args.seq_length)) #L2 loss\n        print(self.cost)\n        self.lr = tf.Variable(args.learning_rate, trainable=False)\n        tvars = tf.trainable_variables()\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n                args.grad_clip)\n        #optimizer = tf.train.AdamOptimizer(self.lr)\n        #optimizer = tf.train.GradientDescentOptimizer(self.lr)\n        optimizer = tf.train.AdagradOptimizer(self.lr)\n\n        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n\n    def sample(self, sess, num=200, prime=[0.0]):\n        state = self.cell.zero_state(1, tf.float32).eval()\n        for price in prime[:-1]:\n            x = np.zeros((1, 1))\n            x[0, 0] = price\n            feed = {self.input_data: x, self.initial_state:state}\n            [state, logits] = \\\n                    sess.run([self.final_state, self.logits], feed)\n            #print price, logits\n\n        ret = prime\n        price = prime[-1]\n        for n in xrange(num):\n            x = np.zeros((1, 1))\n            x[0, 0] = price\n            feed = {self.input_data: x, self.initial_state:state}\n            [logits, state] = \\\n                    sess.run([self.logits, self.final_state], feed)\n            print logits\n            import sys; sys.exit();\n            pred = logits\n            ret += [pred]\n            price = pred\nreturn ret\n'"
reader.py,0,"b""from shove import Shove\nimport csv\n\nmem_store = Shove()\nroot = Shove('file://shovestore')\n\nprint sorted(root.keys())\nprint root['2009-01-03']\nprint root['2016-01-03']\n"""
sample.py,0,"b""import numpy as np\nimport tensorflow as tf\n\nimport argparse\nimport time\nimport os\nimport cPickle\n\nfrom utils import DataLoader\nfrom model import Model\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--save_dir', type=str, default='save',\n                       help='model directory to store checkpointed models')\n    parser.add_argument('-n', type=int, default=500,\n                       help='number of characters to sample')\n    parser.add_argument('--prime', nargs='+', default='0.0',\n                       help='prime seq')\n    args = parser.parse_args()\n    args.prime = map(float, args.prime)\n    sample(args)\n\ndef sample(args):\n    with open(os.path.join(args.save_dir, 'config.pkl')) as f:\n        saved_args = cPickle.load(f)\n    model = Model(saved_args, True)\n    with tf.Session() as sess:\n        tf.initialize_all_variables().run()\n        saver = tf.train.Saver(tf.all_variables())\n        ckpt = tf.train.get_checkpoint_state(args.save_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            saver.restore(sess, ckpt.model_checkpoint_path)\n            print model.sample(sess, args.n, args.prime)\n\nif __name__ == '__main__':\nmain()\n"""
sanitize.py,0,"b""from shove import Shove\nimport csv\n\nmem_store = Shove()\nroot = Shove('file://shovestore')\n\n\ndef parse(filename):\n  print 'Parsing', filename\n  n, e = filename.split('.')\n  with open(filename, 'rb') as csvfile:\n    reader = csv.reader(csvfile)\n    for row in reader:\n      date = row[0]\n      if date == 'unixtime':\n        names = row\n        continue\n\n      o = root[date] = root.get(date) or {}\n      for index, name in enumerate(names):\n        key = name if index is 0 else (n+'.'+name)\n        o[key] = row[index]\n      print date, len(o.keys())\n\n\nparse('bitfinexUSD.csv')\n\n\nroot.sync()\n"""
train.py,0,"b'import numpy as np\nimport tensorflow as tf\n\nimport argparse\nimport time\nimport os\nimport cPickle\n\nfrom utils import DataLoader\nfrom model import Model\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_dir\', type=str, default=\'/home/maraoz/shovestore\',\n                       help=\'data directory containing input.txt\')\n    parser.add_argument(\'--save_dir\', type=str, default=\'save\',\n                       help=\'directory to store checkpointed models\')\n    parser.add_argument(\'--rnn_size\', type=int, default=128,\n                       help=\'size of RNN hidden state\')\n    parser.add_argument(\'--num_layers\', type=int, default=4,\n                       help=\'number of layers in the RNN\')\n    parser.add_argument(\'--model\', type=str, default=\'lstm\',\n                       help=\'rnn, gru, or lstm\')\n    parser.add_argument(\'--batch_size\', type=int, default=100,\n                       help=\'minibatch size\')\n    parser.add_argument(\'--seq_length\', type=int, default=100,\n                       help=\'RNN sequence length\')\n    parser.add_argument(\'--num_epochs\', type=int, default=200,\n                       help=\'number of epochs\')\n    parser.add_argument(\'--save_every\', type=int, default=100,\n                       help=\'save frequency\')\n    parser.add_argument(\'--grad_clip\', type=float, default=5.,\n                       help=\'clip gradients at this value\')\n    parser.add_argument(\'--learning_rate\', type=float, default=0.001,\n                       help=\'learning rate\')\n    parser.add_argument(\'--decay_rate\', type=float, default=0.97,\n                       help=\'decay rate for rmsprop\')\n    args = parser.parse_args()\n    train(args)\n\ndef train(args):\n    data_loader = DataLoader(args.data_dir, args.batch_size, args.seq_length)\n\n    with open(os.path.join(args.save_dir, \'config.pkl\'), \'w\') as f:\n        cPickle.dump(args, f)\n\n    model = Model(args)\n\n    with tf.Session() as sess:\n        tf.initialize_all_variables().run()\n        saver = tf.train.Saver(tf.all_variables())\n        for e in xrange(args.num_epochs):\n            sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** e)))\n            data_loader.reset_batch_pointer()\n            state = model.initial_state.eval()\n            for b in xrange(data_loader.num_batches):\n                start = time.time()\n                x, y = data_loader.next_batch()\n                #print(x, \'->\', y)\n                #import sys; sys.exit();\n                feed = {\n                    model.input_data: x, \n                    model.targets: y,\n                    model.initial_state: state\n                }\n                train_loss, state, _ = sess.run(\\\n                        [model.cost, model.final_state, model.train_op], feed)\n                end = time.time()\n                print ""{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}"" \\\n                    .format(e * data_loader.num_batches + b,\n                            args.num_epochs * data_loader.num_batches,\n                            e, train_loss, end - start)\n                if (e * data_loader.num_batches + b) % args.save_every == 0:\n                    checkpoint_path = os.path.join(args.save_dir, \'model.ckpt\')\n                    saver.save(sess, checkpoint_path, global_step = e * data_loader.num_batches + b)\n                    print ""model saved to {}"".format(checkpoint_path)\n\nif __name__ == \'__main__\':\nmain()\n'"
utils.py,6,"b'import os\nimport collections\nimport cPickle\nimport numpy as np\nfrom shove import Shove\n\n\nclass DataLoader():\n    def __init__(self, data_dir, batch_size, seq_length):\n        self.data_dir = data_dir\n        root = Shove(\'file://\'+data_dir)\n        self.batch_size = batch_size\n        self.seq_length = seq_length\n\n        tensor_file = os.path.join(data_dir, ""data.npy"")\n\n        if not os.path.exists(tensor_file):\n            print ""reading text file""\n            self.preprocess(root, tensor_file)\n        else:\n            print ""loading preprocessed files""\n            self.load_preprocessed(tensor_file)\n        self.create_batches()\n        self.reset_batch_pointer()\n\n    def preprocess(self, root, tensor_file):\n        print \'preprocessing...\'\n        dates = sorted(root.keys())\n        prices = []\n        for date in dates:\n          sp = root[date].get(\'bitfinexUSD.price\')\n          price = float(sp)\n          prices.append(price)\n        self.tensor = np.array(prices)\n        print \'prices shape\', len(self.tensor), self.tensor.size\n        np.save(tensor_file, self.tensor)\n\n    def load_preprocessed(self, tensor_file):\n        self.tensor = np.load(tensor_file)\n\n\n    def create_batches(self):\n        self.num_batches = self.tensor.size / (self.batch_size * self.seq_length)\n        print(\'num_batches =\', self.num_batches)\n        print(\'self.tensor.size =\', self.tensor.size)\n        print(\'self.batch_size =\', self.batch_size)\n        print(\'self.seq_length =\', self.seq_length)\n        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n        xdata = self.tensor\n        ydata = np.copy(self.tensor)\n        ydata[:-1] = xdata[1:]\n        ydata[-1] = xdata[0]\n        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n\n\n    def next_batch(self):\n        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n        self.pointer += 1\n        return x, y\n\n    def reset_batch_pointer(self):\nself.pointer = 0\n'"
