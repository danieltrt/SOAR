file_path,api_count,code
classes/__init__.py,0,b''
classes/conv_layer.py,6,"b""import numpy as np\nfrom classes.neural_layer import NeuralLayer\nimport classes.utils as u\n\n\nclass ConvLayer(NeuralLayer):\n\n    def __init__(self, input_size, k, f=3, s=1, p=1, u_type='adam', a_type='relu', dropout=1):\n        self.image_size = 0\n        self.w = input_size[2]\n        self.h = input_size[1]\n        self.d = input_size[0]\n\n        self.k = k\n        self.f = f\n        self.s = s\n        self.p = p\n\n        self.w2 = int((self.w - self.f + 2 * self.p) / self.s + 1)\n        self.h2 = int((self.h - self.f + 2 * self.p) / self.s + 1)\n        self.d2 = k\n\n        super(ConvLayer, self).__init__(f*f*self.d, k, u_type=u_type, a_type=a_type, dropout=dropout)\n\n    def predict(self, batch):\n        self.image_size = batch.shape[0]\n        cols = u.im2col_indices(batch, self.f, self.f, self.p, self.s)\n        sum_weights = []\n        bias = []\n        for n in self.neurons:\n            bias.append(n.b)\n            sum_weights.append(n.weights)\n\n        sum_weights = np.array(sum_weights)\n        strength = (sum_weights.dot(cols) + np.array(bias).reshape(sum_weights.shape[0], 1)).reshape(self.k, self.h2, self.w2, -1).transpose(3, 0, 1, 2)\n\n        if self.activation:\n            if self.a_type == 'sigmoid':\n                return u.sigmoid(strength)\n            else:\n                return u.relu(strength)\n        else:\n            return strength\n\n    def forward(self, batch):\n        self.image_size = batch.shape[0]\n        cols = u.im2col_indices(batch, self.f, self.f, self.p, self.s)\n        l2 = 0\n        sum_weights = []\n        bias = []\n        for n in self.neurons:\n            n.last_input = cols\n            sum_weights.append(n.weights)\n            bias.append(n.b)\n            l2 += n.regularization()\n\n        sum_weights = np.array(sum_weights)\n        strength = (sum_weights.dot(cols) + np.array(bias).reshape(sum_weights.shape[0], 1))\n        strength = strength.reshape(self.k, self.h2, self.w2, -1).transpose(3, 0, 1, 2)\n\n        if self.activation:\n            if self.a_type == 'sigmoid':\n                self.forward_result = u.sigmoid(strength)\n            else:\n                self.forward_result = u.relu(strength)\n        else:\n            self.forward_result = strength\n\n        return self.forward_result, l2\n\n    def backward(self, d, need_d=True):\n        if d.ndim < 4:\n            d = d.reshape(self.w2, self.h2, self.k, -1).T\n\n        if self.activation:\n            if self.a_type == 'sigmoid':\n                delta = d * u.sigmoid_d(self.forward_result)\n            else:\n                delta = d * u.relu_d(self.forward_result)\n        else:\n            delta = d\n\n        sum_weights = []\n        for index, n in enumerate(self.neurons):\n            n.delta = delta[:, index, :, :].transpose(1, 2, 0).flatten()\n            if need_d:\n                rot = np.rot90(n.weights.reshape(self.d, self.f, self.f), k=2, axes=(1, 2))\n                sum_weights.append(rot)\n\n        if not need_d:\n            return\n\n        padding = ((self.w - 1) * self.s + self.f - self.w2) // 2\n        cols = u.im2col_indices(delta, self.f, self.f, padding=padding, stride=self.s)\n\n        sum_weights = np.array(sum_weights).transpose(1, 0, 2, 3).reshape(self.d, -1)\n\n        result = sum_weights.dot(cols)\n        im = result.reshape(self.d, self.h, self.w, -1).transpose(3, 0, 1, 2)\n\n        return im\n\n    def output_size(self):\n        return (self.d2, self.h2, self.w2)\n\n    def update(self, lr, l2_reg, t=0):\n        super(ConvLayer, self).update(lr, l2_reg, t)\n"""
classes/layer.py,0,"b""from abc import ABCMeta, abstractmethod\n\n\nclass Layer(object):\n    __metaclass__ = ABCMeta\n\n    is_output = False\n    activation = True\n    u_type = 'adam'\n    dropout = 1\n\n    @abstractmethod\n    def forward(self, batch):\n        pass\n\n    @abstractmethod\n    def backward(self, d, need_d=True):\n        pass\n\n    @abstractmethod\n    def output_size(self):\n        pass\n\n    @abstractmethod\n    def update(self, lr, l2_reg, t=0):\n        pass\n\n    @abstractmethod\n    def predict(self, batch):\n        pass\n"""
classes/neural_layer.py,4,"b""from classes.layer import Layer\nfrom classes.neuron import Neuron\nimport numpy as np\nimport classes.utils as u\n\n\nclass NeuralLayer(Layer):\n\n    def __init__(self, input_size, k, u_type='adam', a_type='relu', dropout=1):\n        self.neurons = []\n        self.forward_result = None\n        self.k = k\n        self.dropout = dropout\n\n        self.u_type = u_type\n        self.a_type= a_type\n\n        if isinstance(input_size, tuple):\n            input_size = np.prod(input_size)\n\n        for n in range(k):\n            n = Neuron(input_size)\n\n            if u_type == 'adam':\n                n.m, n.v = 0, 0\n            elif u_type == 'm':\n                n.v = 0, 0\n            elif u_type == 'nag':\n                n.v, n.v_prev = 0, 0\n            elif u_type == 'rmsprop':\n                n.cache, n.v = 0, 0\n\n            self.neurons.append(n)\n\n    def predict(self, batch):\n\n        if batch.ndim > 2:\n            batch = batch.reshape(batch.shape[0], -1).T\n\n        forward_result = []\n        for n in self.neurons:\n            if self.activation:\n                if self.a_type == 'relu':\n                    forward_result.append(u.relu(n.strength(batch)))\n                elif self.a_type == 'sigmoid':\n                    forward_result.append(u.sigmoid(n.strength(batch)))\n            else:\n                forward_result.append(n.strength(batch))\n\n        self.forward_result = np.array(forward_result)\n        return self.forward_result\n\n    def forward(self, batch):\n\n        if batch.ndim > 2:\n            batch = batch.reshape(batch.shape[0], -1).T\n        forward_result = []\n        l2 = 0\n        for n in self.neurons:\n            if self.activation:\n                if self.a_type == 'relu':\n                    forward_result.append(u.relu(n.strength(batch)))\n                elif self.a_type == 'sigmoid':\n                    forward_result.append(u.sigmoid(n.strength(batch)))\n            else:\n                forward_result.append(n.strength(batch))\n\n            n.last_input = batch\n            l2 += n.regularization()\n\n        self.forward_result = np.array(forward_result)\n        return self.forward_result, l2\n\n    def backward(self, d, need_d=True):\n        weights = []\n\n        if self.activation:\n            if self.a_type == 'sigmoid':\n                delta = d * u.sigmoid_d(self.forward_result)\n            else:\n                delta = d * u.relu_d(self.forward_result)\n        else:\n            delta = d\n\n        for index, n in enumerate(self.neurons):\n            n.delta = delta[index]\n            if need_d:\n                weights.append(n.weights)\n\n        if not need_d:\n            return\n        else:\n            weights = np.array(weights)\n            return weights.T.dot(delta)\n\n    def output_size(self):\n        if self.forward_result:\n            return self.forward_result.shape[1:]\n        else:\n            return self.k\n\n    def update(self, lr, l2_reg, t=0):\n        if self.u_type == 'adam':\n            u.adam_update(self.neurons, lr, t=t, l2_reg=l2_reg)\n        elif self.u_type == 'rmsprop':\n            u.rmsprop(self.neurons, lr, l2_reg=l2_reg)\n        elif self.u_type == 'm':\n            u.momentum_update(self.neurons, lr, l2_reg=l2_reg)\n        elif self.u_type == 'nag':\n            u.nag_update(self.neurons, lr, l2_reg=l2_reg)\n        elif self.u_type == 'v':\n            u.vanila_update(self.neurons, lr, l2_reg=l2_reg)\n"""
classes/neural_net.py,9,"b""from classes.neural_layer import NeuralLayer\nfrom classes.conv_layer import ConvLayer\nfrom classes.pool_layer import PoolLayer\nimport classes.utils as utils\nimport numpy as np\n\nclass NeuralNetwork(object):\n    def __init__(self, input_shape, layer_list, lr, l2_reg=0, loss='softmax'):\n        self.layers = []\n        self.lr = np.float32(lr)\n        self.l2_reg = np.float32(l2_reg)\n        self.loss = loss\n        self.epoch_count = 0\n\n        self.dropout_masks = []\n        self.t = 0\n\n        next_input_size = input_shape\n        for l in layer_list:\n            if l['type'] == 'conv':\n                l.pop('type')\n                conv = ConvLayer(next_input_size, **l)\n                self.layers.append(conv)\n                next_input_size = conv.output_size()\n\n            elif l['type'] == 'pool':\n                l.pop('type')\n                pool = PoolLayer(next_input_size, **l)\n                self.layers.append(pool)\n                next_input_size = pool.output_size()\n\n            elif l['type'] == 'fc':\n                l.pop('type')\n                fc = NeuralLayer(next_input_size, **l)\n                self.layers.append(fc)\n                next_input_size = fc.output_size()\n\n            elif l['type'] == 'output':\n                l.pop('type')\n                fc = NeuralLayer(next_input_size, **l)\n                fc.is_output = True\n                fc.activation = False\n                self.layers.append(fc)\n                next_input_size = fc.output_size()\n\n    def predict(self, batch, label):\n        next_input = batch\n        for index, layer in enumerate(self.layers):\n            next_input = layer.predict(next_input)\n\n        result = np.array(next_input)\n        if self.loss == 'softmax':\n            loss, delta = utils.softmax_loss(result, label)\n        elif self.loss == 'logistic':\n            loss, delta = utils.logistic_loss(result, label)\n\n        max_result = np.argmax(result, axis=0)\n        correct_count = np.sum(max_result == label)\n\n        return loss, correct_count / float(len(max_result)) * 100\n\n    def epoch(self, batch, label):\n        # forward\n        l2 = 0\n        next_input = batch\n        for index, layer in enumerate(self.layers):\n            layer_result = layer.forward(next_input)\n            next_input = layer_result[0]\n            l2 += layer_result[1]\n            if layer.dropout < 1 and not layer.is_output:\n                dropout_mask = np.random.rand(*next_input.shape) < layer.dropout\n                next_input *= dropout_mask / layer.dropout\n                self.dropout_masks.append(dropout_mask)\n\n\n        result = np.array(next_input)\n        if self.loss == 'softmax':\n            loss, delta = utils.softmax_loss(result, label)\n        elif self.loss == 'logistic':\n            loss, delta = utils.logistic_loss(result, label)\n\n        loss += 0.5 * self.l2_reg * l2\n        max_result = np.argmax(result, axis=0)\n        correct_count = np.sum(max_result == label)\n\n        # backprop\n        back_input = delta.T\n        for index, layer in enumerate(reversed(self.layers)):\n            is_input_layer = index < len(self.layers) - 1\n\n            if layer.dropout < 1 and not layer.is_output and self.dropout_masks:\n                dropout_mask = self.dropout_masks.pop()\n                if dropout_mask.ndim > 2 and back_input.ndim == 2:\n                    back_input *= dropout_mask.T.reshape(-1, back_input.shape[1])\n                else:\n                    back_input *= dropout_mask\n\n            back_input = layer.backward(back_input, is_input_layer)\n\n        # update\n        for index, layer in enumerate(self.layers):\n            layer.update(self.lr, l2_reg=self.l2_reg, t=self.t)\n\n        return loss + self.l2_reg * l2 / 2, correct_count / float(len(max_result)) * 100\n"""
classes/neuron.py,4,"b'import numpy as np\n\n\nclass Neuron(object):\n    def __init__(self, input_size, bias=0.0):\n        self.weights = (np.random.randn(input_size) * np.sqrt(2.0 / input_size)).astype(np.float32)\n        self.b = np.float32(bias)\n        self.last_input = None\n        self.delta = None\n\n    def strength(self, values):\n        return np.dot(self.weights, values) + self.b\n\n    def regularization(self):\n        return np.sum(np.square(self.weights))'"
classes/pool_layer.py,4,"b""import numpy as np\nfrom classes.layer import Layer\nimport classes.utils as u\n\n\nclass PoolLayer(Layer):\n    def __init__(self, input_size, f=2, s=2, method='max', dropout=1):\n        # assert f == s\n\n        self.image_size = 0\n        self.d = input_size[0]\n        self.h = input_size[1]\n        self.w = input_size[2]\n        self.method = method\n        self.argmax = None\n        self.dropout = dropout\n\n        self.f = f\n        self.s = s\n\n        assert self.h % self.f == 0\n        assert self.w % self.f == 0\n\n        self.w2 = int((self.w - self.f) / self.s + 1)\n        self.h2 = int((self.h - self.f) / self.s + 1)\n\n\n    def predict(self, batch):\n        self.image_size = batch.shape[0]\n        return self.forward(batch)[0]\n\n    def backward(self, d, need_d=True):\n        if d.ndim < 4:\n            d = d.reshape(self.image_size, self.d, self.h2, self.w2)\n\n        dout_flat = d.transpose(2, 3, 0, 1).ravel()\n\n        if self.method == 'max':\n            dX_col = np.zeros((self.f**2, self.h2 * self.w2 * self.d * self.image_size)).astype(np.float32)\n            dX_col[self.argmax, range(self.argmax.size)] = dout_flat\n        elif self.method == 'average':\n            field_size = self.f**2\n            dout_flat /= field_size\n            dX_col = np.zeros((self.f**2, self.h2 * self.w2 * self.d * self.image_size)).astype(np.float32)\n            dX_col[range(field_size), :] = dout_flat\n        else:\n            raise ValueError('pool layer type error!')\n\n        dX = u.col2im_indices(dX_col, (self.image_size * self.d, 1, self.h, self.w), self.f, self.f, padding=0, stride=self.s)\n        return dX.reshape(self.image_size, self.d, self.h, self.w)\n\n    def forward(self, batch):\n        self.image_size = batch.shape[0]\n        batch_reshaped = batch.reshape(self.image_size * self.d, 1, self.h, self.w)\n        X_col = u.im2col_indices(batch_reshaped, self.f, self.f, padding=0, stride=self.s)\n\n        if self.method == 'max':\n            self.argmax = np.argmax(X_col, axis=0)\n            out = X_col[self.argmax, range(self.argmax.size)]\n        elif self.method == 'average':\n            out = np.average(X_col, axis=0)\n        else:\n            raise ValueError('pool layer type error!')\n\n        return out.reshape(self.h2, self.w2, self.image_size, self.d).transpose(2, 3, 0, 1), 0\n\n    def update(self, lr, l2_reg, t=0):\n        pass\n\n    def output_size(self):\n        return (self.d, self.h2, self.w2)\n"""
classes/utils.py,27,"b""import numpy as np\n\n\ndef softmax_loss(x, y):\n    x = x.T\n    probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n    probs /= np.sum(probs, axis=1, keepdims=True)\n    N = x.shape[0]\n    loss = -np.sum(np.log(probs[range(N), y])) / N\n    dx = probs\n    dx[range(N), y] -= 1\n    dx /= N\n    return loss, dx\n\n\ndef logistic_loss(x, y):\n    N = x.shape[0]\n    loss = np.sum(np.square(y - x) / 2) / N\n    dx = -(y - x)\n    return loss, dx.T\n\n\ndef get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n    N, C, H, W = x_shape\n    assert (H + 2 * padding - field_height) % stride == 0\n    assert (W + 2 * padding - field_height) % stride == 0\n    out_height = (H + 2 * padding - field_height) // stride + 1\n    out_width = (W + 2 * padding - field_width) // stride + 1\n\n    i0 = np.repeat(np.arange(field_height), field_width)\n    i0 = np.tile(i0, C)\n    i1 = stride * np.repeat(np.arange(out_height), out_width)\n    j0 = np.tile(np.arange(field_width), field_height * C)\n    j1 = stride * np.tile(np.arange(out_width), out_height)\n    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n\n    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n\n    return (k, i, j)\n\n\ndef im2col_indices(x, field_height, field_width, padding=1, stride=1):\n    p = padding\n    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n\n    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n\n    cols = x_padded[:, k, i, j]\n    C = x.shape[1]\n    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n    return cols\n\n\ndef col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1, stride=1):\n    N, C, H, W = x_shape\n    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n    if padding == 0:\n        return x_padded\n    return x_padded[:, :, padding:-padding, padding:-padding]\n\n\ndef rmsprop(neurons, lr, l2_reg=0, decay_rate=0.9, eps=1e-8):\n    for n in neurons:\n        l2 = l2_reg * n.weights\n        dx = (n.last_input.dot(n.delta)).T + l2\n        d_bias = np.sum(n.delta)\n\n        n.cache = decay_rate * n.cache + (1 - decay_rate) * (dx ** 2)\n        n.weights += - lr * dx / (np.sqrt(n.cache) + eps)\n        n.b -= lr * d_bias\n\ndef adam_update(neurons, lr, t, l2_reg=0, beta1=np.float32(0.9), beta2=np.float32(0.999), eps=1e-8):\n    for n in neurons:\n        l2 = l2_reg * n.weights\n        dx = (n.last_input.dot(n.delta)).T + l2\n        d_bias = np.sum(n.delta)\n\n        n.m = beta1 * n.m + (1 - beta1) * dx\n        n.v = beta2 * n.v + (1 - beta2) * (dx**2)\n\n        m = n.m / np.float32(1-beta1**t)\n        v = n.v / np.float32(1-beta2**t)\n\n        n.weights -= lr * m / (np.sqrt(v) + eps)\n        n.b -= lr * d_bias\n\n\ndef nag_update(neurons, lr, l2_reg=0, mu=np.float32(0.9)):\n    for n in neurons:\n        l2 = l2_reg * n.weights\n        dx = (n.last_input.dot(n.delta)).T + l2\n        d_bias = np.sum(n.delta)\n\n        n.v_prev = n.v\n        n.v = mu * n.v - lr * dx\n\n        n.weights += -mu * n.v_prev + (1 + mu) * n.v\n        n.b -= lr * d_bias\n\n\ndef momentum_update(neurons, lr, l2_reg=0, mu=np.float32(0.9)):\n    for n in neurons:\n        l2 = l2_reg * n.weights\n        dx = (n.last_input.dot(n.delta)).T + l2\n        d_bias = np.sum(n.delta)\n\n        n.v = mu * n.v - lr * dx\n        n.weights += n.v\n\n        n.v_bias = mu * n.v_bias - lr * d_bias\n        n.b += n.v_bias\n\n\ndef vanila_update(neurons, lr, l2_reg=0):\n    for n in neurons:\n        l2 = l2_reg * n.weights\n        dx = (n.last_input.dot(n.delta)).T + l2\n        d_bias = np.sum(n.delta)\n\n        n.weights -= lr * dx + l2\n        n.b -= lr * d_bias\n\n\ndef sigmoid(input):\n    return 1/(1+np.exp(-input))\n\n\ndef relu(input):\n    return np.maximum(0, input)\n\n\ndef sigmoid_d(input):\n    return input * (1 - input)\n\n\ndef relu_d(input):\n    return input > 0\n"""
example/and_gate.py,1,"b""import sys, os\nsys.path.insert(1, os.path.split(sys.path[0])[0])\nimport numpy as np\nfrom classes.neural_net import NeuralNetwork\n\n\ncnn = NeuralNetwork(2,\n                    [\n                        {'type': 'output', 'k': 1, 'u_type': 'v', 'a_type': 'sigmoid'},\n                    ]\n                    , 0.001, loss='logistic')\n\ninput = np.array([[0,0], [0,1], [1,0], [1,1]]).reshape(4,1,2)\noutput = [0,1,1,1]\n\nfor i in range(9999999):\n    loss, acc = cnn.epoch(input, output)\n    print loss\n    print acc"""
example/cifar-10-batches-py/main.py,8,"b""import sys, os\nsys.path.insert(1, os.path.split(os.path.split(sys.path[0])[0])[0])\nimport pickle as pkl\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.utils import shuffle\nfrom classes.neural_net import NeuralNetwork\n\n\ndef unpickle(file):\n    fo = open(file, 'rb')\n    dict = pkl.load(fo, encoding='latin1')\n    fo.close()\n    return dict\n\nle = preprocessing.LabelEncoder()\nle.classes_ = unpickle(sys.path[0] + '/batches.meta')['label_names']\n\ntrain_images = None\ntrain_labels = []\n\ntest_images = None\ntest_labels = []\nfor i in range(1, 6):\n    data = unpickle(sys.path[0] + '/data_batch_'+str(i))\n    if train_images is None:\n        train_images = data['data']\n    else:\n        train_images = np.vstack((train_images, data['data']))\n\n    train_labels += data['labels']\n\ntrain_images = train_images.reshape(-1, 3, 32, 32)\ntrain_images = train_images.astype(np.float128)\n\nmean_image= np.mean(train_images, axis=0)\ntrain_images -= mean_image\nstd = np.std(train_images, axis=0)\ntrain_images /= std\n\ntrain_images = train_images.astype(np.float32)\n\ndata = unpickle(sys.path[0] + '/test_batch')\ntest_images = data['data'].reshape(-1, 3, 32, 32)\ntest_images = test_images.astype(np.float128)\n\ntest_images -= mean_image\ntest_images /= std\n\ntest_images = test_images.astype(np.float32)\n\ntest_labels = data['labels']\n\nlr = 1e-4\nl2_reg = 8e-6\nlearning_rate_decay = np.float32(96e-2)\nbatch_size = 1\n\ncnn = NeuralNetwork(train_images.shape[1:],\n                    [\n                        {'type': 'conv', 'k': 16, 'u_type': 'nag', 'f': 5, 's': 1, 'p': 2},\n                        {'type': 'pool', 'method': 'average'},\n                        {'type': 'conv', 'k': 20, 'u_type': 'nag', 'f': 5, 's': 1, 'p': 2},\n                        {'type': 'pool', 'method': 'average'},\n                        {'type': 'conv', 'k': 20, 'u_type': 'nag', 'f': 5, 's': 1, 'p': 2},\n                        {'type': 'pool', 'method': 'average'},\n                        {'type': 'output', 'k': len(le.classes_), 'u_type': 'adam'}\n                    ]\n                    , lr, l2_reg=l2_reg)\n\ntrain_images, train_labels = shuffle(train_images, train_labels)\nfor i in range(60000000):\n    start = i * batch_size % len(train_images)\n    end = start + batch_size\n\n    if start == 0 and i != 0:\n        cnn.epoch_count += 1\n        train_images, train_labels = shuffle(train_images, train_labels)\n        print('{} epoch finish. learning rate is {}'.format(str(cnn.epoch_count), str(cnn.lr)))\n        cnn.lr *= learning_rate_decay\n\n        loss, acc = cnn.predict(train_images[:4000], train_labels[:4000])\n        print('training acc:{}'.format(acc))\n        print('training loss:{}'.format(loss))\n\n        test_loss, test_acc = cnn.predict(test_images[:10000], test_labels[:10000])\n        print('test acc:{}'.format(test_acc))\n        print('test loss:{}'.format(test_loss))\n\n    cnn.t += 1\n    loss, acc = cnn.epoch(train_images[start:end], train_labels[start:end])\n\n    # print(loss)\n    # print(acc)\n\n"""
