file_path,api_count,code
example.py,6,"b""from classes.rnn import RNN\nimport numpy as np\n\ndata = open('anna.txt', 'r').read().lower().replace('\\n', ' ')\nchars = list(set(data))\ndata_size, vocab_size = len(data), len(chars)\nprint('data has %d characters, %d unique.' % (data_size, vocab_size))\nchar_to_ix = {ch: i for i, ch in enumerate(chars)}\nix_to_char = {i: ch for i, ch in enumerate(chars)}\n\nseq_length = 128\nlearning_rate = 1e-3\nrnn = RNN(\n    [\n        {'type': 'lstm', 'hidden_size': 256},\n        {'type': 'lstm', 'hidden_size': 256},\n        {'type': 'lstm', 'hidden_size': 256},\n        # {'type': 'lstm', 'hidden_size': vocab_size, 'dropout': 0.5, 'bi': True, 'u_type': 'adagrad'}\n    ],\n    vocab_size, learning_rate)\n\nprint(rnn.archi)\nprint('with seq_length {}'.format(seq_length))\n\nn, p = 0, 0\n\n\ndef sample(seed_ix, n):\n    x = np.zeros((1, vocab_size, 1))\n    x[0][seed_ix][0] = 1\n    ixes = []\n    rnn.reset_h_predict_to_h()\n    for t in range(n):\n        h, y = rnn.predict(x)\n        p = np.exp(y) / np.sum(np.exp(y))\n        ix = np.random.choice(range(vocab_size), p=p.ravel())\n        x = np.zeros((1, vocab_size, 1))\n        x[0][ix][0] = 1\n        ixes.append(ix)\n    return ixes\n\n\nsmooth_loss = -np.log(1.0 / vocab_size)  # loss at iteration 0\nwhile True:\n    if p + seq_length + 1 >= len(data) or n == 0:\n        p = 0\n        rnn.reset_h()\n\n    inputs = [char_to_ix[ch] for ch in data[p:p + seq_length]]\n    targets = [char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]]\n\n    x, y = np.zeros((seq_length, vocab_size, 1), np.float32), np.zeros((seq_length, vocab_size, 1), np.float32)\n    x[range(len(x)), inputs] = 1\n    y[:, targets] = 1\n\n    if n % 1000 == 0:\n        sample_ix = sample(inputs[0], 300)\n        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n        print('----\\n %s \\n----' % (txt,))\n\n    loss = rnn.epoch(x, targets)\n    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n\n    if n % 100 == 0:\n        print('iter %d, loss: %f' % (n, smooth_loss))\n\n    p += seq_length\n    n += 1\n"""
classes/functions.py,9,"b""import numpy as np\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\ndef d_sigmoid(x):\n    return x * (1 - x)\n\n\ndef relu(x):\n    return np.maximum(0, x)\n\n\ndef d_relu(x):\n    return x > 0\n\n\ndef tanh(x):\n    return np.tanh(x)\n\n\ndef d_tanh(x):\n    return 1 - (x ** 2)\n\n\ndef softmax_loss(y_prime, y):\n    probs = np.exp(y_prime - np.max(y_prime, axis=1, keepdims=True))\n    probs /= np.sum(probs, axis=1, keepdims=True)\n    N = y_prime.shape[0]\n    loss = -np.sum(np.log(probs[range(N), y])) / N\n    dx = probs.copy()\n    dx[range(N), y] -= 1\n    return loss, dx\n\n\ndef logistic_loss(y_prime, y):\n    N = y_prime.shape[0]\n    loss = np.sum(np.square(y - y_prime) / 2) / N\n    dx = -(y - y_prime)\n    return loss, dx\n\n\ndef adam_update(w, d, m, lr, beta1=0.9, beta2=0.999, eps=1e-8):\n    m['t'] += 1\n\n    m['m'] = beta1 * m['m'] + (1 - beta1) * d\n    m['v'] = beta2 * m['v'] + (1 - beta2) * (d ** 2)\n\n    m_ = m['m'] / (1 - beta1 ** m['t'])\n    v_ = m['v'] / (1 - beta2 ** m['t'])\n\n    return w - lr * m_ / (np.sqrt(v_) + eps)\n\n\ndef adagrad_update(w, d, m, lr, eps=1e-8):\n    m['m'] += d * d\n    return w - lr * d / np.sqrt(m['m'] + eps)\n"""
classes/rnn.py,9,"b'from classes.layers.vanilla_layer import VanillaLayer\nfrom classes.layers.lstm_layer import LstmLayer\nfrom classes.layers.gru_layer import GruLayer\nfrom classes.layers.fc_layer import FcLayer\nfrom classes.layers.layer import Layer\nfrom classes import functions as funcs\n\nimport numpy as np\n\n\nclass RNN(object):\n    key_u_type = (\'adam\', \'adagrad\')\n    key_layer_type = (\'vanilla\', \'lstm\', \'gru\', \'fc\')\n\n    def __init__(self, archi, d_size, lr=1e-3):\n        layers = []\n\n        self.archi = archi\n        self.dropout_masks = []\n        input_size = d_size\n        for layer in archi:\n            assert \'type\' in layer and \'hidden_size\' in layer, \'type and hidden_size has to be defined\'\n            assert layer[\'type\'] in self.key_layer_type, \'wrong layer type\'\n\n            is_output = True if layer is archi[-1] else False\n            is_input = True if layer is archi[0] else False\n            is_hidden = not is_output and not is_input\n            bi = layer[\'bi\'] if \'bi\' in layer else False\n            if is_input and bi:\n                input_size *= 2\n\n            layer[\'is_output\'], layer[\'is_input\'], layer[\'is_hidden\'], layer[\'bi\'] = is_output, is_input, is_hidden, bi\n\n            if layer[\'type\'] == \'vanilla\':\n                if bi:\n                    input_size //= 2\n                hidden_size = layer[\'hidden_size\']\n                output_size = d_size if is_output else hidden_size\n                lay = VanillaLayer(hidden_size, input_size, output_size, **layer)\n                if bi:\n                    lay = lay, VanillaLayer(hidden_size, input_size, output_size, **layer)\n                    output_size *= 2\n\n            elif layer[\'type\'] == \'lstm\':\n                if bi:\n                    input_size //= 2\n                hidden_size = layer[\'hidden_size\']\n                output_size = d_size if is_output else hidden_size\n                lay = LstmLayer(hidden_size, input_size, output_size, **layer)\n                if bi:\n                    lay = lay, LstmLayer(hidden_size, input_size, output_size, **layer)\n                    output_size *= 2\n\n            elif layer[\'type\'] == \'gru\':\n                if bi:\n                    input_size //= 2\n                hidden_size = layer[\'hidden_size\']\n                output_size = d_size if is_output else hidden_size\n                lay = GruLayer(hidden_size, input_size, output_size, **layer)\n                if bi:\n                    lay = lay, GruLayer(hidden_size, input_size, output_size, **layer)\n                    output_size *= 2\n\n            elif layer[\'type\'] == \'fc\':\n                hidden_size = layer[\'hidden_size\']\n                output_size = hidden_size\n                need_activation = not is_output\n                lay = FcLayer(hidden_size, input_size, output_size, need_activation=need_activation, **layer)\n\n            layers.append(lay)\n            input_size = output_size\n\n        assert d_size == output_size, \'input & ouput dimension is not same. use ""fc"" for alternate\'\n\n        self.layers = layers\n        self.lr = lr\n\n    def epoch(self, x, y):\n        assert self.layers, \'layers must be made\'\n        next_input = x\n        for layer, archi in zip(self.layers, self.archi):\n\n            is_bi = archi[\'bi\']\n            is_next_input_tuple = isinstance(next_input, tuple)\n            if is_bi and not is_next_input_tuple:\n                next_input = next_input, np.flip(next_input, 0)\n            elif not is_bi and is_next_input_tuple:\n                next_input = np.concatenate((next_input[0], next_input[1]), 1)\n\n            if is_bi:\n                # if isinstance(layer, tuple):\n                l1: Layer = layer[0]\n                l2: Layer = layer[1]\n\n                output_1, _ = l1.forward(next_input[0])\n                output_2, _ = l2.forward(next_input[1])\n\n                next_input, y_prime = (output_1, output_2), None\n                layer = l1\n                if layer.is_output:\n                    y_prime = np.concatenate((next_input[0], next_input[1]), 1)\n            else:\n                layer: Layer\n\n                next_input, y_prime = layer.forward(next_input)\n\n            if layer.dropout < 1 and not layer.is_output:\n                shape = next_input[0].shape if is_bi else next_input.shape\n\n                dropout_mask = np.random.rand(*shape[1:]) < layer.dropout\n                dropout_mask = np.tile(dropout_mask, shape[0]).T.reshape(shape)\n                self.dropout_masks.append(dropout_mask)\n\n                # next_input will be tuple\n                if is_bi:\n                    next_input = tuple(i * dropout_mask / layer.dropout for i in next_input)\n                else:\n                    next_input *= dropout_mask / layer.dropout\n\n        loss, next_d = funcs.softmax_loss(y_prime, y)\n\n        for layer, archi in zip(reversed(self.layers), reversed(self.archi)):\n            is_bi = archi[\'bi\']\n            is_next_d_tuple = isinstance(next_d, tuple)\n\n            if is_bi and not is_next_d_tuple:\n                half = next_d.shape[1] // 2\n                next_d = next_d[:, :half, :], next_d[:, half:, :]\n            elif not is_bi and is_next_d_tuple:\n                next_input = np.concatenate((next_d[0], next_d[1]), 1)\n\n            l = layer[0] if is_bi else layer\n            if l.dropout < 1 and not l.is_output and self.dropout_masks:\n                dropout_mask = self.dropout_masks.pop()\n                if is_bi:\n                    next_d = tuple(i * dropout_mask for i in next_d)\n                else:\n                    next_d *= dropout_mask\n\n            if is_bi:\n                l1: Layer = layer[0]\n                l2: Layer = layer[1]\n\n                next_d = l1.backward(next_d[0]), l2.backward(next_d[1])\n            else:\n                next_d = layer.backward(next_d)\n\n        for layer in reversed(self.layers):\n            if isinstance(layer, tuple):\n                l1: Layer = layer[0]\n                l2: Layer = layer[1]\n\n                l1.update(self.lr)\n                l2.update(self.lr)\n            else:\n                layer.update(self.lr)\n\n        return loss\n\n    def predict(self, x):\n        assert self.layers, \'layers must be made\'\n        next_input = x\n        for layer, archi in zip(self.layers, self.archi):\n\n            is_bi = archi[\'bi\']\n            is_next_input_tuple = isinstance(next_input, tuple)\n            if is_bi and not is_next_input_tuple:\n                next_input = next_input, np.flip(next_input, 0)\n            elif not is_bi and is_next_input_tuple:\n                next_input = np.concatenate((next_input[0], next_input[1]), 1)\n\n            if is_bi:\n                l1: Layer = layer[0]\n                l2: Layer = layer[1]\n\n                output_1, _ = l1.forward(next_input[0])\n                output_2, _ = l2.forward(next_input[1])\n\n                next_input, y_prime = (output_1, output_2), None\n                layer = l1\n                if layer.is_output:\n                    y_prime = np.concatenate((next_input[0], next_input[1]), 1)\n            else:\n                layer: Layer\n\n                next_input, y_prime = layer.forward(next_input)\n\n        return next_input, y_prime\n\n    def reset_h(self):\n        for layer in self.layers:\n            if isinstance(layer, VanillaLayer):\n                layer.h = layer.first_h.copy()\n            elif isinstance(layer, LstmLayer):\n                layer.h = layer.first_h.copy()\n                layer.c = layer.first_c.copy()\n            elif isinstance(layer, GruLayer):\n                layer.h = layer.first_h.copy()\n\n    def reset_h_predict(self):\n        for layer in self.layers:\n            if isinstance(layer, VanillaLayer):\n                layer.h_predict = layer.first_h[0].copy()\n            elif isinstance(layer, LstmLayer):\n                layer.h_predict = layer.first_h[0].copy()\n                layer.c_predict = layer.first_c[0].copy()\n            elif isinstance(layer, GruLayer):\n                layer.h_predict = layer.first_h[0].copy()\n\n    def reset_h_predict_to_h(self):\n        for layer in self.layers:\n            if isinstance(layer, VanillaLayer):\n                layer.h_predict_prev = layer.h[-1]\n            elif isinstance(layer, LstmLayer):\n                layer.h_predict_prev = layer.h[-1]\n                layer.c_predict_prev = layer.c[-1]\n            elif isinstance(layer, GruLayer):\n                layer.h_predict_prev = layer.h[-1]\n'"
classes/layers/__init__.py,0,b''
classes/layers/fc_layer.py,7,"b'from classes.layers.layer import Layer\nfrom classes import functions as funcs\n\nimport numpy as np\n\n\nclass FcLayer(Layer):\n    def __init__(self, h_size, i_size, o_size,\n                 is_input, is_output, is_hidden,\n                 u_type=\'adam\', dropout=1.0, a_type=""relu"", need_activation=True, **kwargs):\n\n        self._w, self._d, self._m = {}, {}, {}\n\n        assert h_size == o_size\n\n        self.h_size = h_size\n        self.i_size = i_size\n        self.o_size = o_size\n        self.is_ouput = is_output\n        self.is_input = is_input\n        self.is_hidden = is_hidden\n        self.u_type = u_type\n        self.dropout = dropout\n        self.a_type = a_type\n        self.need_activation = need_activation\n\n        # for cache\n        self.x = []\n\n        self._w_keys = [\'w\']\n        self._b_keys = [\'b\']\n\n        self._w, self._d, self._m = {}, {}, {}\n\n        for k in self._w_keys:\n            self._w[k] = np.random.randn(o_size, i_size).astype(np.float32) * np.sqrt(2.0 / i_size)\n\n        for k in self._b_keys:\n            self._w[k] = np.zeros((self.o_size, 1), np.float32)\n\n        for k in self._w_keys + self._b_keys:\n            self._m[k] = {\'m\': 0, \'v\': 0, \'t\': 0}\n            self._d[k] = np.zeros_like(self._w[k], np.float32)\n\n    def forward(self, x):\n        self.x = x\n        y_prime = []\n\n        for t in range(len(x)):\n            y = self._w[\'w\'].dot(x[t]) + self._w[\'b\']\n\n            if self.need_activation:\n                if self.a_type == \'relu\':\n                    y = funcs.relu(y)\n                elif self.a_type == \'sigmoid\':\n                    y = funcs.sigmoid(y)\n                elif self.a_type == \'tanh\':\n                    y = funcs.sigmoid(y)\n\n            y_prime.append(y)\n\n        y_prime = np.array(y_prime)\n\n        return y_prime, y_prime\n\n    def backward(self, dy):\n        output_d = []\n\n        for t in range(len(self.x)):\n\n            if self.need_activation:\n                if self.a_type == \'relu\':\n                    dy = funcs.d_relu(dy)\n                elif self.a_type == \'sigmoid\':\n                    dy = funcs.d_sigmoid(dy)\n                elif self.a_type == \'tanh\':\n                    dy = funcs.d_tanh(dy)\n\n            self._d[\'w\'] += dy[t].dot(self.x[t].T)\n            self._d[\'b\'] += dy[t]\n            dx = self._w[\'w\'].T.dot(dy[t])\n\n            output_d.append(dx)\n\n        return np.array(output_d)\n\n    def update(self, lr):\n        for k in self._w_keys + self._b_keys:\n            if self.u_type == \'adam\':\n                self._w[k] = funcs.adam_update(self._w[k], self._d[k], self._m[k], lr)\n            elif self.u_type == \'adagrad\':\n                self._w[k] = funcs.adagrad_update(self._w[k], self._d[k], self._m[k], lr)\n\n        for k in self._w_keys + self._b_keys:\n            self._d[k] = np.zeros_like(self._w[k], np.float32)\n\n    def predict(self, x):\n        y_prime = []\n\n        for t in range(len(x)):\n            y = self._w[\'w\'].dot(x[t]) + self._w[\'b\']\n\n            if self.need_activation:\n                if self.a_type == \'relu\':\n                    y = funcs.relu(y)\n                elif self.a_type == \'sigmoid\':\n                    y = funcs.sigmoid(y)\n                elif self.a_type == \'tanh\':\n                    y = funcs.sigmoid(y)\n\n            y_prime.append(y)\n\n        y_prime = np.array(y_prime)\n\n        return y_prime, y_prime\n'"
classes/layers/gru_layer.py,27,"b""from classes.layers.layer import Layer\nfrom classes import functions as funcs\n\nimport numpy as np\n\n\nclass GruLayer(Layer):\n    def __init__(self, h_size, i_size, o_size,\n                 is_input, is_output, is_hidden,\n                 u_type='adam', dropout=1.0, **kwargs):\n\n        self._w, self._d, self._m = {}, {}, {}\n\n        self.h_size = h_size\n        self.i_size = i_size\n        self.o_size = o_size\n        self.is_ouput = is_output\n        self.is_input = is_input\n        self.is_hidden = is_hidden\n        self.u_type = u_type\n        self.dropout = dropout\n\n        self.clip = (-2, 2)\n\n        self.first_h = [np.zeros((self.h_size, 1), np.float32)]\n        self.h = self.first_h.copy()\n        self.h_predict_prev = self.first_h[0].copy()\n\n        # for backward\n        self._h_prev = self.h[-1]\n\n        # for cache\n        self.stacked_x, self.stacked_x_r, self.z, self.r, self.h_hat = [], [], [], [], []\n\n        self._w_keys = ['w_r', 'w_z', 'w_h', 'w_y']\n        self._b_keys = ['b_r', 'b_z', 'b_h', 'b_y']\n\n        if not self.is_ouput:\n            self._w_keys.remove('w_y')\n            self._b_keys.remove('b_y')\n\n        self._w, self._d, self._m = {}, {}, {}\n\n        total_input = h_size + i_size\n        for k in self._w_keys:\n            if k == 'w_y':\n                # dev = np.sqrt(6.0 / (o_size + h_size))\n                # self._w[k] = np.random.uniform(-dev, dev, (o_size, h_size)).astype(np.float32)\n                self._w[k] = np.random.randn(o_size, h_size).astype(np.float32) * np.sqrt(2.0 / (h_size))\n                # self._w[k] = np.random.randn(o_size, h_size).astype(np.float32) * 0.01\n                # self._w[k] = np.random.randn(o_size, h_size).astype(np.float32) * np.sqrt(2.0 / (h_size + o_size))\n            else:\n                # dev = np.sqrt(6.0 / (total_input + h_size))\n                # self._w[k] = np.random.uniform(-dev, dev, (h_size, total_input)).astype(np.float32)\n                self._w[k] = np.random.randn(h_size, total_input).astype(np.float32) * np.sqrt(2.0 / (total_input))\n                # self._w[k] = np.random.randn(h_size, total_input).astype(np.float32) * 0.01\n                # self._w[k] = np.random.randn(h_size, total_input).astype(np.float32) * np.sqrt(2.0 / (total_input + h_size))\n\n        for k in self._b_keys:\n            if k == 'b_y':\n                self._w[k] = np.zeros((self.o_size, 1), np.float32)\n            else:\n                self._w[k] = np.zeros((self.h_size, 1), np.float32)\n\n        for k in self._w_keys + self._b_keys:\n            self._m[k] = {'m': 0, 'v': 0, 't': 0}\n            self._d[k] = np.zeros_like(self._w[k], np.float32)\n\n    def forward(self, x):\n        self.stacked_x, self.stacked_x_r, self.z, self.r, self.h_hat = [], [], [], [], []\n\n        h_prev = self.h[-1]\n        h_t, y_prime = [], []\n\n        # for backward.\n        self._h_prev = h_prev\n\n        for t in range(len(x)):\n            s_x = np.row_stack((h_prev, x[t]))\n            self.stacked_x.append(s_x)\n\n            r = funcs.sigmoid(self._w['w_r'].dot(s_x) + self._w['b_r'])\n            z = funcs.sigmoid(self._w['w_z'].dot(s_x) + self._w['b_z'])\n            s_x_r = np.row_stack((h_prev * r, x[t]))\n            h_hat = funcs.tanh(self._w['w_h'].dot(s_x_r) + self._w['b_h'])\n            h = z * h_prev + (1 - z) * h_hat\n\n            h_prev = h\n\n            self.stacked_x_r.append(s_x_r)\n            self.r.append(r)\n            self.z.append(z)\n            self.h_hat.append(h_hat)\n            h_t.append(h)\n\n            if self.is_ouput:\n                y_prime.append(self._w['w_y'].dot(h) + self._w['b_y'])\n\n        self.h = np.array(h_t)\n        y_prime = np.array(y_prime)\n\n        return self.h, y_prime\n\n    def backward(self, dy):\n        dh_next = np.zeros_like(self.h[0])\n        output_d = []\n\n        for t in reversed(range(len(self.stacked_x))):\n            if self.is_ouput:\n                # dL/dy * dy/dWy\n                self._d['w_y'] += np.dot(dy[t], self.h[t].T)\n                # dL/dy * dy/dby\n                self._d['b_y'] += dy[t]\n                # dL/dy * dy/dh\n                dh = self._w['w_y'].T.dot(dy[t]) + dh_next\n            else:\n                dh = dy[t] + dh_next\n\n            # dL/dh * dh/dh_prev\n            dh_prev = self.z[t] * dh\n            # dL/dh * dh/dh_hat * dh_hat/dtanh\n            dh_hat = (1.0 - self.z[t]) * dh * funcs.d_tanh(self.h_hat[t])\n\n            # dL/dh * dh/dz\n            dz = ((self.h[t - 1] if t - 1 >= 0 else self._h_prev) - self.h_hat[t]) * dh * funcs.d_sigmoid(self.z[t])\n\n            # dL/dh_hat * dh_hat/dstacked_x_r\n            self._d['w_h'] += dh_hat.dot(self.stacked_x_r[t].T)\n            self._d['b_h'] += dh_hat\n            dstacked_x_r = self._w['w_h'].T.dot(dh_hat)\n            dx = dstacked_x_r\n\n            # dL/dh_hat * dh_hat/dstacked_x_r & p to h\n            dstacked_x_r_h = dstacked_x_r[:self.h_size, :]\n            # dL/dstacked_x * dh_hat.shape dh_hat/dstacked_x\n            dr = dstacked_x_r_h * (self.h[t - 1] if t - 1 >= 0 else self._h_prev) * funcs.d_sigmoid(self.r[t])\n            dh_prev += dstacked_x_r_h * self.r[t]\n\n            self._d['w_z'] += dz.dot(self.stacked_x[t].T)\n            self._d['b_z'] += dz\n            dstacked_x = self._w['w_z'].T.dot(dz)\n            dh_prev += dstacked_x[:self.h_size, :]\n            dx += dstacked_x\n\n            self._d['w_r'] += dr.dot(self.stacked_x[t].T)\n            self._d['b_r'] += dr\n            dstacked_x = self._w['w_r'].T.dot(dz)\n            dh_prev += dstacked_x[:self.h_size, :]\n            dx += dstacked_x\n\n            dh_next = dh_prev\n\n            output_d.insert(0, dx[self.h_size:, :])\n\n        for k in self._w_keys + self._b_keys:\n            np.clip(self._d[k], self.clip[0], self.clip[1], out=self._d[k])\n\n        return np.array(output_d)\n\n    def update(self, lr):\n        for k in self._w_keys + self._b_keys:\n            if self.u_type == 'adam':\n                self._w[k] = funcs.adam_update(self._w[k], self._d[k], self._m[k], lr)\n            elif self.u_type == 'adagrad':\n                self._w[k] = funcs.adagrad_update(self._w[k], self._d[k], self._m[k], lr)\n\n        for k in self._w_keys + self._b_keys:\n            self._d[k] = np.zeros_like(self._w[k], np.float32)\n\n    def predict(self, x):\n        h_prev = self.h_predict_prev\n        h_t, y_prime = [], []\n\n        for t in range(len(x)):\n            s_x = np.row_stack((h_prev, x[t]))\n\n            r = funcs.sigmoid(self._w['w_r'].dot(s_x) + self._w['b_r'])\n            z = funcs.sigmoid(self._w['w_z'].dot(s_x) + self._w['b_z'])\n            s_x_r = np.row_stack((h_prev * r, x[t]))\n            h_hat = np.tanh(self._w['w_h'].dot(s_x_r) + self._w['b_h'])\n            h = z * h_prev + (1 - z) * h_hat\n\n            h_prev = h\n            h_t.append(h)\n\n            if self.is_ouput:\n                y_prime.append(self._w['w_y'].dot(h) + self._w['b_y'])\n\n        self.h_predict_prev = h_prev\n\n        return h_t, np.array(y_prime)\n"""
classes/layers/layer.py,0,"b'from abc import ABC, abstractmethod\nimport numpy as np\n\n\nclass Layer(ABC):\n    dropout = 1.0\n    is_input = False\n    is_output = False\n    is_hidden = False\n\n    @abstractmethod\n    def forward(self, x):\n        pass\n\n    @abstractmethod\n    def backward(self, dy):\n        pass\n\n    @abstractmethod\n    def update(self, lr):\n        pass\n\n    @abstractmethod\n    def predict(self, x):\n        pass\n'"
classes/layers/lstm_layer.py,27,"b""from classes.layers.layer import Layer\nfrom classes import functions as funcs\n\nimport numpy as np\n\n\nclass LstmLayer(Layer):\n    def __init__(self, h_size, i_size, o_size,\n                 is_input, is_output, is_hidden,\n                 u_type='adam', dropout=1.0, **kwargs):\n\n        self._w, self._d, self._m = {}, {}, {}\n        self.h_size = h_size\n        self.i_size = i_size\n        self.o_size = o_size\n        self.is_ouput = is_output\n        self.is_input = is_input\n        self.is_hidden = is_hidden\n        self.u_type = u_type\n        self.dropout = dropout\n\n        self.clip = (-2, 2)\n\n        self.first_h = [np.zeros((self.h_size, 1), np.float32)]\n        self.h = self.first_h.copy()\n        self.h_predict_prev = self.first_h[0].copy()\n\n        self.first_c = [np.zeros((self.h_size, 1), np.float32)]\n        self.c = self.first_c.copy()\n        self.c_predict_prev = self.first_c[0].copy()\n\n        # for backward\n        self._c_prev = self.c[0]\n\n        # for cache\n        self.stacked_x, self.tanh_c, self.hf, self.hi, self.ho, self.hc = [], [], [], [], [], []\n\n        self._w_keys = ['w_f', 'w_i', 'w_c', 'w_o', 'w_y']\n        self._b_keys = ['b_f', 'b_i', 'b_c', 'b_o', 'b_y']\n\n        if not self.is_ouput:\n            self._w_keys.remove('w_y')\n            self._b_keys.remove('b_y')\n\n        total_input = h_size + i_size\n        for k in self._w_keys:\n            if k == 'w_y':\n                # dev = np.sqrt(6.0 / (o_size + h_size))\n                # self._w[k] = np.random.uniform(-dev, dev, (o_size, h_size)).astype(np.float32)\n                self._w[k] = np.random.randn(o_size, h_size).astype(np.float32) * np.sqrt(2.0 / (h_size))\n                # self._w[k] = np.random.randn(o_size, h_size).astype(np.float32) * 0.01\n                # self._w[k] = np.random.randn(o_size, h_size).astype(np.float32) * np.sqrt(2.0 / (h_size + o_size))\n            else:\n                # dev = np.sqrt(6.0 / (total_input + h_size))\n                # self._w[k] = np.random.uniform(-dev, dev, (h_size, total_input)).astype(np.float32)\n                self._w[k] = np.random.randn(h_size, total_input).astype(np.float32) * np.sqrt(2.0 / (total_input))\n                # self._w[k] = np.random.randn(h_size, total_input).astype(np.float32) * 0.01\n\n                # self._w[k] = np.random.randn(h_size, total_input).astype(np.float32) * np.sqrt(2.0 / (total_input + h_size))\n\n        for k in self._b_keys:\n            if k == 'b_y':\n                self._w[k] = np.zeros((self.o_size, 1), np.float32)\n            else:\n                self._w[k] = np.zeros((self.h_size, 1), np.float32)\n\n        for k in self._w_keys + self._b_keys:\n            self._m[k] = {'m': 0, 'v': 0, 't': 0}\n            self._d[k] = np.zeros_like(self._w[k], np.float32)\n\n    def forward(self, x):\n        self.stacked_x, self.tanh_c, self.hf, self.hi, self.ho, self.hc = [], [], [], [], [], []\n\n        h_t, c_t, y_prime = [], [], []\n        h_prev = self.h[-1]\n        c_prev = self.c[-1]\n\n        # for backward. h is on stacked_x\n        self._c_prev = c_prev\n\n        for t in range(len(x)):\n            s_x = np.row_stack((h_prev, x[t]))\n            self.stacked_x.append(s_x)\n\n            output_f = funcs.sigmoid(self._w['w_f'].dot(s_x) + self._w['b_f'])\n            output_i = funcs.sigmoid(self._w['w_i'].dot(s_x) + self._w['b_i'])\n            output_o = funcs.sigmoid(self._w['w_o'].dot(s_x) + self._w['b_o'])\n            output_c = np.tanh(self._w['w_c'].dot(s_x) + self._w['b_c'])\n\n            c = (output_f * c_prev) + (output_i * output_c)\n            c_t.append(c)\n            tanh_c = np.tanh(c)\n\n            h = output_o * tanh_c\n            h_t.append(h)\n\n            h_prev = h\n            c_prev = c\n\n            self.hf.append(output_f)\n            self.hi.append(output_i)\n            self.ho.append(output_o)\n            self.hc.append(output_c)\n\n            self.tanh_c.append(tanh_c)\n\n            if self.is_ouput:\n                y_prime.append(self._w['w_y'].dot(h) + self._w['b_y'])\n\n        self.h = h_t\n        self.c = c_t\n\n        return np.array(self.h), np.array(y_prime)\n\n    def backward(self, dy):\n        dh_next = np.zeros_like(self.h[0])\n        dc_next = np.zeros_like(self.c[0])\n\n        output_d = []\n\n        for t in reversed(range(len(self.stacked_x))):\n            if self.is_ouput:\n                # dL/dy * dy/dWy\n                self._d['w_y'] += np.dot(dy[t], self.h[t].T)\n                # dL/dy * dy/dby\n                self._d['b_y'] += dy[t]\n                # dL/dy * dy/dh\n                dh = self._w['w_y'].T.dot(dy[t]) + dh_next\n            else:\n                dh = dy[t] + dh_next\n\n            # dL/dh * dh/dho * dho/dsigmoid\n            dho = self.tanh_c[t] * dh * funcs.d_sigmoid(self.ho[t])\n\n            # dL/dh * dh/dc\n            dc = self.ho[t] * dh * funcs.d_tanh(self.tanh_c[t]) + dc_next\n\n            # dL/dc * dc/dhf * dhf/dsigmoid\n            dhf = (self.c[t - 1] if t - 1 >= 0 else self._c_prev) * dc * funcs.d_sigmoid(self.hf[t])\n\n            # dL/dc * dc/dhi * dhi/dsigmoid\n            dhi = self.hc[t] * dc * funcs.d_sigmoid(self.hi[t])\n\n            # dL/dc * dc/dhc * dhc/dtanh\n            dhc = self.hi[t] * dc * funcs.d_tanh(self.hc[t])\n\n            # dL/dsigmoid * dsigmoid/dwf\n            self._d['w_f'] += dhf.dot(self.stacked_x[t].T)\n            self._d['b_f'] += dhf\n\n            # dL/dsigmoid * dsigmoid/dwi\n            self._d['w_i'] += dhi.dot(self.stacked_x[t].T)\n            self._d['b_i'] += dhi\n\n            # dL/dsigmoid * dsigmoid/dwo\n            self._d['w_o'] += dho.dot(self.stacked_x[t].T)\n            self._d['b_o'] += dho\n\n            # dL/dtanh * dtanh/dwc\n            self._d['w_c'] += dhc.dot(self.stacked_x[t].T)\n            self._d['b_c'] += dhc\n\n            dx = self._w['w_f'].T.dot(dhf) + self._w['w_i'].T.dot(dhi) + self._w['w_c'].T.dot(dhc) + self._w['w_o'].T.dot(\n                dho)\n\n            dh_next = dx[:self.h_size, :]\n            dc_next = self.hf[t] * dc\n\n            output_d.insert(0, dx[self.h_size:, :])\n\n        for k in self._w_keys + self._b_keys:\n            np.clip(self._d[k], self.clip[0], self.clip[1], out=self._d[k])\n\n        return np.array(output_d)\n\n    def update(self, lr):\n        for k in self._w_keys + self._b_keys:\n            if self.u_type == 'adam':\n                self._w[k] = funcs.adam_update(self._w[k], self._d[k], self._m[k], lr)\n            elif self.u_type == 'adagrad':\n                self._w[k] = funcs.adagrad_update(self._w[k], self._d[k], self._m[k], lr)\n\n        for k in self._w_keys + self._b_keys:\n            self._d[k] = np.zeros_like(self._w[k], np.float32)\n\n    def predict(self, x):\n        h_t, y_prime = [], []\n        h_prev = self.h_predict_prev\n        c_prev = self.c_predict_prev\n\n        for t in range(len(x)):\n            s_x = np.row_stack((h_prev, x[t]))\n\n            output_f = funcs.sigmoid(self._w['w_f'].dot(s_x) + self._w['b_f'])\n            output_i = funcs.sigmoid(self._w['w_i'].dot(s_x) + self._w['b_i'])\n            output_o = funcs.sigmoid(self._w['w_o'].dot(s_x) + self._w['b_o'])\n            output_c = funcs.tanh(self._w['w_c'].dot(s_x) + self._w['b_c'])\n\n            c = output_f * c_prev + output_i * output_c\n            h = output_o * funcs.tanh(c)\n\n            if self.is_ouput:\n                y_prime.append(self._w['w_y'].dot(h) + self._w['b_y'])\n\n            h_t.append(h)\n\n            h_prev = h\n            c_prev = c\n\n        self.h_predict_prev = h_prev\n        self.c_predict_prev = c_prev\n\n        return h_t, np.array(y_prime)\n"""
classes/layers/vanilla_layer.py,23,"b""from .layer import Layer\nimport classes.functions as funcs\nimport numpy as np\n\n\nclass VanillaLayer(Layer):\n    def __init__(self, h_size, i_size, o_size,\n                 is_input, is_output, is_hidden,\n                 u_type='adam', dropout=1.0, **kwargs):\n\n        self._w, self._d, self._m = {}, {}, {}\n\n        self.h_size = h_size\n        self.i_size = i_size\n        self.o_size = o_size\n        self.is_ouput = is_output\n        self.is_input = is_input\n        self.is_hidden = is_hidden\n        self.u_type = u_type\n        self.dropout = dropout\n\n        self.clip = (-3, 3)\n\n        self.first_h = [np.zeros((self.h_size, 1), np.float32)]\n        self.h = self.first_h.copy()\n        self.h_predict_prev = self.first_h[0].copy()\n\n        # for backward\n        self._h_prev = self.h[-1]\n\n        self.x = []\n\n        self._w_keys = ['w_xh', 'w_hh', 'w_hy']\n        self._b_keys = ['b_h', 'b_y']\n\n        if not self.is_ouput:\n            self._w_keys.remove('w_hy')\n            self._b_keys.remove('b_y')\n\n        for k in self._w_keys + self._b_keys:\n            if k == 'w_xh':\n                self._w[k] = np.random.randn(self.h_size, self.i_size).astype(np.float32) * np.sqrt(2.0 / (self.i_size))\n            elif k == 'w_hh':\n                self._w[k] = np.random.randn(self.h_size, self.h_size).astype(np.float32) * np.sqrt(2.0 / (self.h_size))\n            elif k == 'b_h':\n                self._w[k] = np.zeros((self.h_size, 1), np.float32)\n            elif k == 'w_hy':\n                self._w[k] = np.random.randn(self.o_size, self.h_size).astype(np.float32) * np.sqrt(2.0 / (self.h_size))\n            elif k == 'b_y':\n                self._w[k] = np.zeros((self.o_size, 1), np.float32)\n            else:\n                raise ValueError('{} key has to be initiated'.format(k))\n\n        for k in self._w_keys + self._b_keys:\n            self._m[k] = {'m': 0, 'v': 0, 't': 0}\n            self._d[k] = np.zeros_like(self._w[k], np.float32)\n\n    def forward(self, x):\n        self.x = x\n        h, y_prime = [], []\n\n        h_prev = self.h[-1]\n        self._h_prev = h_prev\n\n        for t in range(len(x)):\n            h.append(funcs.tanh(self._w['w_xh'].dot(x[t]) + self._w['w_hh'].dot(h_prev) + self._w['b_h']))\n            if self.is_ouput:\n                y_prime.append(np.dot(self._w['w_hy'], h[t]) + self._w['b_y'])\n            h_prev = h[t]\n\n        h = np.array(h)\n        y_prime = np.array(y_prime)\n\n        self.h = h\n        return h, y_prime\n\n    def backward(self, dy):\n        dh_next = np.zeros_like(self.h[0], np.float32)\n        output_d = []\n        for t in reversed(range(len(self.x))):\n            if self.is_ouput:\n                # dL/dy * dy/dWxy\n                self._d['w_hy'] += np.dot(dy[t], self.h[t].T)\n                # dL/dy * dy/dby\n                self._d['b_y'] += dy[t]\n\n                # dL/dy * dL/dh\n                # h was split so add dhnext\n                dh = np.dot(self._w['w_hy'].T, dy[t]) + dh_next\n            else:\n                # dL/dy * dL/dh\n                dh = dy[t] + dh_next\n\n            # dL/dh * dh/dtanh\n            dtanh = funcs.d_tanh(self.h[t]) * dh\n\n            output_d.insert(0, np.dot(self._w['w_xh'].T, dtanh))\n\n            # dL/dtanh * dtanh/dbh\n            self._d['b_h'] += dtanh\n            # dL/dtanh * dtanh/dWxh\n            self._d['w_xh'] += np.dot(dtanh, self.x[t].T)\n            # dL/dtanh * dtanh/dWhh\n            self._d['w_hh'] += np.dot(dtanh, self.h[t - 1].T if t - 1 >= 0 else self._h_prev.T)\n            # dL/dtanh * dtanh/dht-1\n            dh_next = np.dot(self._w['w_hh'].T, dtanh)\n\n        for k in self._w_keys + self._b_keys:\n            np.clip(self._d[k], self.clip[0], self.clip[1], out=self._d[k])\n\n        return np.array(output_d)\n\n    def update(self, lr):\n        for k in self._w_keys + self._b_keys:\n            if self.u_type == 'adam':\n                self._w[k] = funcs.adam_update(self._w[k], self._d[k], self._m[k], lr)\n            elif self.u_type == 'adagrad':\n                self._w[k] = funcs.adagrad_update(self._w[k], self._d[k], self._m[k], lr)\n\n        for k in self._w_keys + self._b_keys:\n            self._d[k] = np.zeros_like(self._w[k], np.float32)\n\n    def predict(self, x):\n        h, y_prime = [], []\n\n        h_prev = self.h_predict_prev\n\n        for t in range(len(x)):\n            h.append(funcs.tanh(self._w['w_xh'].dot(x[t]) + self._w['w_hh'].dot(h_prev) + self._w['b_h']))\n            if self.is_ouput:\n                y_prime.append(np.dot(self._w['w_hy'], h[t]) + self._w['b_y'])\n            h_prev = h[t]\n\n        self.h_predict_prev = h_prev\n        h = np.array(h)\n        y_prime = np.array(y_prime)\n\n        return h, y_prime\n"""
