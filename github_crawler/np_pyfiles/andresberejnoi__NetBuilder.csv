file_path,api_count,code
setup.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Mar  8 17:12:11 2017\n\n@author: andresberejnoi\n""""""\n\nfrom setuptools import setup, find_packages\nimport os\nfrom netbuilder import __version__\n\ndef package_files(directory):\n    paths = []\n    for (path, directories, filenames) in os.walk(directory):\n        for filename in filenames:\n            paths.append(os.path.join(\'..\', path, filename))\n    return paths\n\nlong_desc = """"""Neural Network Builder\nThe neural network class is in NeuralNet.py. It allows to easily create fully connected feedforward networks of any size allowed by available memory.\nIt uses numpy arrays as the primary data structure for the weight matrices.\nWith this class you can create deep neural networks very quickly (see some of the example files to see how to use it).\n""""""\ndesc = """"""Allows to create and train fully connected feedforward deep neural networks in a simple way.""""""\n#with open(\'VERSION\', \'r\') as ver:\n#    version = ver.read().rstrip()\nsetup(name=\'netbuilder\',\n      version=__version__,\n      #packages=[\'NetBuilder\'],\n      packages=find_packages(),\n      #package_data={\'\':paths},\n      #package_data={\'NetBuilder\':find_packages(\'.\')},\n      include_package_data=True,\n      author=\'Andres Berejnoi\',\n      author_email=\'andresberejnoi@gmail.com\',\n      url=\'https://github.com/andresberejnoi/NetBuilder\',\n      download_url = \'https://github.com/andresberejnoi/NetBuilder/archive/v0.2.3.tar.gz\',\n      license=\'MIT\',\n      description=desc,\n      long_description=long_desc,\n      keywords=[\'Neural Network\', \'machine learning\', \'AI\', \'artificial intelligence\', \'MLP\'],\n      install_requires=[\'numpy>=1.12\',\'PyYAML>=3.12\'],#,\'sphinx>=1.5.5\'],\n      classifiers=[\'Programming Language :: Python\',\n                   \'Programming Language :: Python :: 3\',\n                   \'Development Status :: 3 - Alpha\',\n                   \'Operating System :: Unix\',\n                   \'Intended Audience :: Education\',\n                   \'Intended Audience :: Developers\',\n                   \'Topic :: Scientific/Engineering :: Artificial Intelligence\']\n      )\n'"
netbuilder/NeuralNet.py,17,"b'# -*- coding: utf-8 -*-\n""""""@author: andresberejnoi\n\nTODO: During training, if the network gets stuck in a local minima for several epochs,\nthen randomly modify certain weights in the matrix. This might allow the network to get out\nof that minima and converge.\n\n""""""\n\n#import numpy as np\nfrom netbuilder import np\nfrom . import _param_keys as keys #import keys for saving and loading network from file\nfrom .activations import *  #import activation functions defined in the file, such as tanh and sigmoid\nfrom .loss import *\n#import tools         # this is a python file where I will put some functions before I decide to include them here directly\n\n#---------------------------------------------------------------------------------------------\nclass NetworkError(Exception):\n    """""" An exception object that can be raised to handle different situations.\n\n    It is currently very simple.\n\n    """"""\n    def __init__(self, msg):\n        """"""Sets value for error message.\n\n        Parameters\n        ----------\n        msg : string\n            Message to be displayed when this exception is raised.\n\n        """"""\n\n        self.msg = msg\n\n    def __str__(self):\n        return self.msg\n\n#----------------------------------------------------------------------------------------------\n\nclass Network(object):\n    """"""Implements the methods and attributes of an Artificial Neural Network (Multilayer).\n\n    The network implemented is a feedforward one, with backpropagation as the training algorithm.\n    This implementation uses the numpy library, so it needs to be available to be able to run.\n\n    """"""\n    #np.random.seed()                       # start the random seed before using it\n    #np.random.random()\n    #Set up a dictionary of activation functions to access them more easily\n\n    def __init__(self):\n        """"""Setting names for instance variables.\n\n        """"""\n        self.topology = None\n        self.learningRate = None\n        self.momentum = None\n        self.name = None\n        self.size = None\n        #self._hiddenActiv_fun_key = None\n        #self._outActiv_fun_key = None\n        #self.output_activation = None\n        #self.hidden_activation = None\n\n    #----------------------------------------------------------------------------------\n    # Initializers\n    #\n    def init(self,topology,learningRate=0.01,momentum=0.1,name=\'Network\',add_bias=True):\n        """"""Initializes the network with specified shape and parameters.\n\n        Parameters\n        ----------\n        topology : python iterable\n            A Python list with integers indicating the shape of the network.\n            i.e: [5,10,1]: this encodes a network of 3 layers (one input, 1 hidden, and 1 output).\n            The input layer will have 5 neurons, the hidden layer will have 10, and the output layer will have only one neuron.\n        learningRate : float, optional\n            It helps with the speed and convergence of the network. It is usually small.\n            A very small number will cause the network to converge very slowly. A high rate will make\n            the network oscillate during training and prevent it from ""learning"" patterns.\n        momentum : float, optional\n            It is also used during the training process. It is related to how much the previous changes\n            affect the new ones.\n        name : string, optional\n            A name to identify the network more easily if it is saved to a file.\n\n        """"""\n        self.topology = topology\n        self.learningRate = learningRate\n        self.momentum = momentum\n        self.name = name\n        self.size = len(self.topology)-1           #The size of the network will be the number of weeight matrices between layers, instead of the number of layers itself\n        #self._hiddenActiv_fun_key = \'tanh\'\n        #self._outActiv_fun_key = \'tanh\'\n        #self.output_activation = self.set_outActivation_fun(func=self._outActiv_fun_key)\n        #self.hidden_activation = self.set_hiddenactivation_fun(func=self._hiddenActiv_fun_key)\n\n        # Initialize random weights, and create empty matrices to store the previous changes in weight (for momentum):\n        if add_bias:\n            #self.weights = [np.random.normal(loc=0,scale=0.6,size=(topology[i]+1, topology[i+1])) for i in range(self.size)]\n            self.weights = [np.random.normal(loc=0,\n                                             scale=0.6,\n                                             size=(topology[i]+1, topology[i+1]+1)) for i in range(self.size-1)] #we are only generating matrices for inital and hidden layers\n            #Create matrix for output layer\n            f_idx = self.size-1     #use this index for the final layer matrix below\n            self.weights.append(np.random.normal(loc=0,\n                                                 scale=0.6,\n                                                 size=(topology[f_idx]+1,topology[f_idx+1])))\n        else:\n            raise NotImplemented(""Currently the network only works when bias nodes are used"")\n            self.weights = [np.random.normal(loc=0,scale=0.6,size=(topology[i], topology[i])) for i in range(self.size)]\n\n        self.Gradients = [None] * self.size\n\n    def _init_from_file(self,params,weights_dict):\n        """"""Initializes network with values saved on files.\n\n        Parameters\n        ----------\n        params : dictionary\n            Maps valid keys that can be obtained from reading a configuration file (or using _get_model()) with parameters that the network needs.\n        weights_dict : dictionary\n            The dictionary generated from reading an npz numpy file with numpy.load.\n\n        """"""\n\n        self.name = params[keys._name]\n        self.topology = params[keys._topology]\n        self.learningRate = params[keys._learning_rate]\n        self.momentum = params[keys._momentum]\n        #self._outActiv_fun_key = params[keys._output_activation]\n        #self._hiddenActiv_fun_key = params[keys._hidden_activation]\n        #self.output_activation = self.set_outActivation_fun(func=self._outActiv_fun_key)\n        #self.hidden_activation = self.set_hiddenactivation_fun(func=self._hiddenActiv_fun_key)\n\n        #unpack weights\n        self.weights = [weights_dict[layer_mat] for layer_mat in weights_dict]\n        self.size = len(self.weights)\n        self.Gradients = [None]*self.size\n\n    #-------------------------------------------------------------------------------------------\n    # Overloading Operators:\n    #\n    def __str__(self):\n        """"""For now, the string method simply returns the topology of the network.\n\n        """"""\n\n        return ""Network: {0}"".format(self.topology)\n\n    __repr__ = __str__\n\n\n    #---------------------------------------------------------------------------\n    # Getters\n    #\n    def get_num_connections(self):\n        """"""Returns the number of features or synapses (connections) present in the network.\n\n        """"""\n\n        synapses = 0\n        for mat in self.weights:\n            synapses += mat.size\n        return synapses\n\n    def get_num_nodes(self):\n        """"""Returns the number of nodes in the network (includes input and output nodes).\n\n        """"""\n\n        return sum(self.topology)\n\n    def get_connection_mat(self, idx):\n        """"""Gets the matrix weight at position idx in self.weights.\n\n        Parameters\n        ----------\n        idx : int\n            Index corresponding to a layer in self.weights.\n\n        Returns\n        -------\n        numpy array\n            The connection weights for the layer requested (self.weights[idx])\n\n        """"""\n\n        try:\n            return self.weights[idx]\n        except:\n            print(""""""Could not find layer {0} in network.\\nNetwork has {1} layers."""""".format(idx, self.size))\n\n    def _get_model(self):\n        """"""Returns a dictionary of network parameters that can be used for a configuration file. This function is used when saving the network.\n\n        """"""\n\n        parameters = {keys._topology:self.topology,\n                      keys._size:self.size,\n                      keys._name:self.name,\n                      #keys._output_activation:self._outActiv_fun_key,\n                      #keys._hidden_activation:self._hiddenActiv_fun_key,\n                      keys._learning_rate:self.learningRate,\n                      keys._momentum:self.momentum}\n\n        return parameters\n    #--------------------------------------------------------------------------\n    # Functionality of the network\n    #\n\n    def feedforward(self,inputs,hidden_activation=tanh,output_activation=tanh):\n        """"""Performs the feedforward propagation of the inputs through the layers.\n\n        Parameters\n        ----------\n        inputs : numpy array\n            Shape of array should be [number of samples x number of features per sample].\n            This array contains inputs to the first layer.\n        hidden_activation : function object, optional\n            It is the activation function for hidden layers. It must be able to accept numpy arrays.\n        output_activation : function object, optional\n            It is the activation function for final layer. It must be able to accept numpy arrays.\n\n        Returns\n        -------\n        numpy array\n            Shape will be [number of samples x number of output features].\n\n        """"""\n\n        # These two lists will contain the inputs and the outputs for each layer, respectively\n        self.netIns = []\n        self.netOuts = []\n\n        input_samples=inputs.shape[0]\n\n        #Currently, this will cause a crash when the network was created without bias nodes\n        I = np.concatenate((inputs,np.ones((input_samples,1))),axis=1)                # adds the bias input of 1\n        self.netOuts.append(I)                                              # keeping track of the outputs of every layer\n\n        #The input is propagated through the layers\n        for idx in range(self.size):\n            W = self.weights[idx]\n\n            I = np.dot(I,W)                                                 #performs the dot product between the input vector and the weight matrix\n            self.netIns.append(I)                                           # keeping track of the inputs to each layer\n\n            #if we are on the last layer, we use the output activation function\n            if idx == self.size -1:\n                I = output_activation(I)\n            #otherwise, we use the activation for the hidden layers\n            else:\n                I = hidden_activation(I)\n                #I = np.concatenate((I,np.ones((I.shape[0],1))), axis=1)\n                self.netOuts.append(I)\n\n        #self.out = I\n        return I\n\n    def predict(self,inputs,hidden_activation=tanh,output_activation=tanh):\n        """"""This function is very similar to feedforward, but makes sure that the input is in the correct format. It is intended for testing the final network without adding additional if statements into the feedfoward function which will be used during training.\n\n        Parameters\n        ----------\n        inputs : numpy array\n            Shape of array should be [number of samples x number of features per sample].\n            Inputs to the first layer.\n        hidden_activation : function object, optional\n            It is the activation function for hidden layers. It must be able to accept numpy arrays.\n        output_activation : function object, optional\n            It is the activation function for final layer. It must be able to accept numpy arrays.\n\n        Returns\n        -------\n        numpy array\n            Shape will be [number of samples x number of output features].\n\n        """"""\n\n        I = inputs\n        #if the input is a list and not a numpy array:\n        if not isinstance(I,np.ndarray):   #if imput is not numpy array\n            I = np.array(I)\n\n        #now we arrange the inputs to be organized in rows if it is only one column.\n        #for example, if we have an array: array ([0,1,2,3,4,5,6,7,8,9]), its shape will be (10,) but we need it to be (1,10) as: array ([[0,1,2,3,4,5,6,7,8,9]])\n        if len(I.shape) == 1:\n            I = I.reshape((1,I.shape[0]))\n\n        output = self.feedforward(I,hidden_activation=hidden_activation,output_activation=output_activation)\n        return output\n\n\n    def reversed_feed(self, outIn):\n        """""" Like the feedforward function but reversed. It takes an output or target vector, and returns the corresponding input vector. Nothing is stored by this function.\n\n        Parameters\n        ----------\n        outIn : numpy array\n            The target vector that will be the input for this function. It would be the output of the normal feedforward fucntion.\n\n        Returns\n        -------\n        numpy array\n            output of running the network backwards.\n\n        """"""\n\n        I = np.array(outIn)\n        for W in self.weights[::-1]:                # We traverse backwards through the weight matrices\n            I = np.dot(W,I)[:-1]                #The dot product of the two numpy arrays will have one extra element, corresponding to the bias node, but we do not need it, so we slice it off\n        return I\n\n    def _compute_error(self,expected_out,actual_out,error_func):\n        """"""Computes the error for the network output.\n\n        Parameters\n        ----------\n        expected_out : numpy array\n            Shape of array should be [batch_size (or number of samples) x number of output features].\n            This array should contain the target values that we want the network to produce once trained.\n        actual_out : numpy array\n            Shape should be equal to `expected_out`\n        error_func : function object\n            A function to compute the error (difference between the two inputs).\n\n        Returns\n        -------\n        float\n            The sum of all the differences between the two inputs.\n\n        """"""\n\n        error = error_func(expected_out,actual_out)\n        return error\n\n    def optimize(self,gradients):\n        """"""Uses the gradients computed by the backpropagation method to update network weights.\n\n        performs stochastic gradient descent and adjusts the weights\n\n\n        Parameters\n        ----------\n        gradients : python iterable\n            This iterable {list, tuple, etc.} contains numpy arrays.\n            Each numpy array is the gradient matrix computed by backpropagation for each layer matrix.\n\n        """"""\n\n        for k in range(self.size):\n            delta_weight = self.learningRate * gradients[k]\n            full_change = delta_weight + self.momentum*self.last_change[k]\n            self.weights[k] -= full_change\n            self.last_change[k] = 1*gradients[k] #copy gradient mat\n\n\n    def backprop(self, input_samples,target,output, error_func, hidden_activation=tanh,output_activation=tanh):\n        """"""Backpropagation.\n\n        Parameters\n        ----------\n        input_samples : numpy array\n            Contains all samples in a batch.\n        target_outputs : numpy array\n            Matching targets for each sample in `input_samples`.\n        output : numpy array\n            Actual output from feedforward propagation. It will be used to check the network\'s error.\n        batch_mode : bool, Don\'t use for now.\n            Indicates whether to use batch or online training.\n        error_func : function object\n            This is the function that computes the error of the epoch and used during backpropagation.\n            It must accept parameters as: error_func(target={target numpy array},actual={actual output from network},derivative={boolean to indicate operation mode})\n        hidden_activation : function object, optional\n            It is the activation function for hidden layers. It must be able to accept numpy arrays.\n            It must also provide a parameter to indicate operation in derivative or normal mode.\n        output_activation : function object, optional\n            It is the activation function for final layer. It must be able to accept numpy arrays.\n            It must also provide a parameter to indicate operation in derivative or normal mode.\n\n        """"""\n\n        #placeholder variables\n        #delta = None\n        #gradient_mat = None\n\n        #Compute gradients and deltas\n        for i in range(self.size):\n            back_index =self.size-1 -i                  # This will be used for the items to be accessed backwards\n            if i!=0:\n                W_trans = self.weights[back_index+1].T        #we use the transpose of the weights in the current layer\n                d_activ = hidden_activation(self.netIns[back_index],derivative=True)\n                d_error = np.dot(delta, W_trans)\n                delta = d_error * d_activ\n                gradient_mat = np.dot(self.netOuts[back_index].T , delta)\n                self.Gradients[back_index] = gradient_mat\n            else:\n                #Herewe calculate gradients for final layer\n                d_activ = output_activation(self.netIns[back_index],derivative=True)\n                d_error = error_func(target,output,derivative=True)\n                delta = d_error * d_activ\n                gradient_mat = np.dot(self.netOuts[back_index].T , delta)\n                self.Gradients[back_index] = gradient_mat\n        # Update weights using the computed gradients\n        self.optimize(gradients=self.Gradients)\n\n    def train(self,input_set,\n              target_set,\n              epochs=5000,\n              threshold_error=1E-10,\n              batch_size=0,\n              error_func=mean_squared_error,\n              hidden_activation=tanh,\n              output_activation=tanh,\n              print_rate=100):\n        """"""Trains the network for the specified number of epochs.\n\n        Parameters\n        ----------\n        input_set : numpy array\n            Shape of array should be [number of samples x number of features per sample].\n        target_set : numpy array\n            Shape of array should be [number of samples x number of features per output].\n        epochs : int, optional\n            The number of iterations of the training process. One epoch is completed when\n            all the training samples in a batch have been presented to the network once.\n        threshold_error : float, optional\n            The maximum error that the network should have. After completing one epoch,\n            if the error of the network is below `threshold_error`, the training stops,\n            otherwise, it must keep going until the error is lower, or the specified number\n            of epochs has been reached.\n        batch_size : int, optional\n            How many samples will make one mini batch. It is 0 by default, which means that one batch will contain all samples. Set to 1 for online training.\n        error_func : function object, optional\n            This is the function that computes the error of the epoch and used during backpropagation.\n            It must accept parameters as: error_func(target={target numpy array},actual={actual output from network},derivative={boolean to indicate the operation mode})\n        print_rate : int, optional\n            Controls the frequency of printing. It tells the trainer to print the error every certain number of epochs: print if current epoch is a multiple of print_rate.\n            Increase this number to print less often, or reduce it to print more often.\n\n        """"""\n\n        #TODO: Read the mini batch data from some file or generator. The current implementation loads the whole batch in memory and then\n        # takes mini batches from there, but this makes the mini batch method pointless (sort of)\n\n        #initialize placeholders:\n        self.last_change = [np.zeros(Mat.shape) for Mat in self.weights]\n\n        #Check if it should do batch training, mini batch, or full batch\n        num_samples = input_set.shape[0]\n\n        try:    #check that batch_size makes sense\n                assert(batch_size <= num_samples)\n        except AssertionError:\n                print (""""""Batch size \'{0}\' is bigger than number of samples available: \'{1}\'"""""".format(batch_size,num_samples))\n                raise\n        #----------------------------------------------------------------------\n        if 0 < batch_size < num_samples:    #here we do mini batch or online\n\n            #Define number of iterations per epoch:\n            num_iterations = num_samples // batch_size + (1 if num_samples%batch_size > 0 else 0)\n\n            for epoch in range(epochs+1):\n                #define start and end index through the data\n                start_idx = 0\n                end_idx = batch_size\n                error = 0\n                for i in range(num_iterations):\n                    #Prepare mini batch\n                    mini_inputs = input_set[start_idx:end_idx]\n                    mini_targets = target_set[start_idx:end_idx]\n\n                    #Feed Network with inputs to compute error\n                    output = self.feedforward(mini_inputs,hidden_activation=hidden_activation,output_activation=output_activation)\n                    error += error_func(target=mini_targets,actual=output)\n                    #print(\'Error:\',error,\'Epoch:\',epoch,\'iter:\',i)\n                    #compute the error\n                    self.backprop(input_samples=mini_inputs,\n                                          target=mini_targets,\n                                          error_func=error_func,\n                                          hidden_activation=hidden_activation,\n                                          output_activation=output_activation,\n                                          output=output)\n\n                    #Update indexes\n                    if end_idx < num_samples:       #increase indexes while there is more data\n                        start_idx = end_idx\n                        if (num_samples-end_idx) < batch_size:\n                            end_idx = num_samples\n                        else:\n                            end_idx += batch_size\n                    #else:\n                    #    raise NetworkError(""""""End index for mini batches went out of range: end index:{0} / number of samples:{1}"""""".format(end_idx,num_samples))\n\n                #print information about training\n                if epoch % print_rate == 0:                                            # Every certain number of iterations, information about the network will be printed. Increase the denominator for more printing points, or reduce it to print less frequently\n                    self.print_training_state(epoch,error)\n                if error <= threshold_error:                                        # when the error of the network is less than the threshold, the traning can stop\n                    self.print_training_state(epoch,error, finished=True)\n                    break\n\n        else:       #here we do full batch\n            mini_inputs = input_set\n            mini_targets = target_set\n            for epoch in range(epochs+1):\n                #Feed Network with inputs can compute error\n                output = self.feedforward(mini_inputs,hidden_activation=hidden_activation,output_activation=output_activation)\n                error = error_func(target=mini_targets,actual=output)\n                #print(\'Error:\',error,\'Epoch:\',i)\n\n                #compute the error\n                self.backprop(input_samples=mini_inputs,\n                                      target=mini_targets,\n                                      error_func=error_func,\n                                      hidden_activation=hidden_activation,\n                                      output_activation=output_activation,\n                                      output=output)\n\n                #if epoch % (epochs/print_rate) == 0:                                            # Every certain number of iterations, information about the network will be printed. Increase the denominator for more printing points, or reduce it to print less frequently\n                if epoch % print_rate == 0:\n                    self.print_training_state(epoch,error)\n                if error <= threshold_error:                                        # when the error of the network is less than the threshold, the traning can stop\n                    self.print_training_state(epoch,error, finished=True)\n                    break\n\n    # Information printers\n    def print_training_state(self,epoch,error,finished=False):\n        """"""Prints the current state of the training process, such as the epoch, current error.\n\n        Parameters\n        ----------\n        epoch : int\n            Current training epoch.\n        error: float\n            Network error for current epoch.\n        finished : bool, optional\n            If true, then a message indicating training is complete is printed. Otherwise\n            just print epoch and error normally.\n\n        """"""\n\n        #print(""Epoch:"",iterCount)\n        if finished:\n            print(""Network has reached a state of minimum error."")\n        #print(""Error: {0}\\tEpoch {1}"".format(error,iterCount))\n        print(""""""Epoch {0} completed"""""".format(epoch),\'Error:\',error)\n\n    def _cleanup(self):\n        """"""Sets containers back to their original state. It is a test function for now.\n\n        """"""\n\n        self.netIns = []\n        self.netOuts = []\n        self.Gradients = [None]*self.size\n\n################################################################################################\n'"
netbuilder/__init__.py,0,"b'""""""Create fully connected neural networks.\n\nExamples\n-------\n    To use the package, it has to be imported first like:\n\n    >>> import netbuilder\n    or\n    >>> import netbuilder as nb\n\n    With the following lines, you can create a neural network for\n    a binary gate:\n\n    >>> net = nb.Network()\n    >>> net.init(topology=[2,1])\n\n    The  first line above will create a `Network` object. The parameters\n    of the network are not defined yet. The second line tells it to initialize\n    weights for a shape of two input nodes for the first layer and one output\n    node at the final layer.\n    To create hidden layers, just add them to the topology parameter when\n    initializing the network:\n\n    >>> net = nb.Network()\n    >>> net.init(topology=[2,5,5,1])\n\n    The above lines will create a `Network` object with 4 layers: one input layer\n    with 2 nodes, two hidden layers with 5 nodes each, and an output layer with\n    one node.\n    To perform a feedforward propagation an input array is needed. If the array\n    is a numpy array with shape [number of samples x number of features], then\n    the `feedfoward` method can be used:\n\n    >>> x = numpy.array([[0,1]])\n    >>> net.feedforward(x)\n    array([[ 0.82683518]])\n\n    Note above that the following format for x will cause an error because the\n    shape is (,2) when it should be (1,2):\n\n    >>> x = numpy.array([0,1])\n\n    The method `predict` is available for quick testing without worry about the\n    format of the input array:\n\n    >>> x = [0,1]\n    >>> net.predict(x)\n    array([[ 0.82683518]])\n\n""""""\nimport numpy as np\n#from . import examples\n#from . import tests\n#from . import __version__\n#from .__version__ import __version__\n#from . import activations\n#import _param_keys as keys\nfrom . import _param_keys as keys\nfrom .activations import *\n#from . import loss\nfrom .loss import *\n#from . import debug_test\n#from .debug_test import *\n#from . import file_operations\nfrom .file_operations import load_model, save_model\n#from . import _param_keys as keys\n#import _param_keys as keys\n\n#from . import NeuralNet\n#from .NeuralNet import Network,NetworkError #import this at the end\nfrom .NeuralNet import Network, NetworkError\n\nname         = ""netbuilder""\nversion_info = (0,2,3)\n__version__  = \'.\'.join(str(c) for c in version_info)\n'"
netbuilder/__version__.py,0,"b'""""""\n__version__ contains the current project version.\n""""""\n__version__=\'0.2.2\'\n'"
netbuilder/_param_keys.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Mar 25 03:54:17 2017\n\n@author: andres\n""""""\n\n_weights_file = \'weights_file\'\n_config_file = \'load.cfg\'\n_topology = \'topology\'\n_size = \'size\'\n_output_activation = \'output_activation\'\n_hidden_activation = \'hidden_activation\'\n_learning_rate = \'learning_rate\'\n_momentum = \'momentum\'\n_name = \'name\'\n'"
netbuilder/activations.py,2,"b'# -*- coding: utf-8 -*-\n""""""Created on Sat Mar 25 21:18:59 2017 @author: andres.\n\nThis module contains activation functions that are used during propagation\nand training of the network. Each one has its normal operating mode and a\nderivative mode to be used during training.\n""""""\n__all__ = [\'sigmoid\',\'tanh\']\n#import numpy as np\nfrom netbuilder import np\n\ndef sigmoid(x, derivative = False):\n    """"""Implements the sigmoid function, applying it element wise on an array x.\n\n    Parameters\n    ----------\n    x : numpy array\n        This array contains arguments for the sigmoid function.\n    derivative : bool\n        Indicates whether to use the sigmoid function or its derivative.\n\n    Returns\n    -------\n    numpy array\n        An array of equal shape to `x`.\n\n    """"""\n\n    if derivative:\n        sgm = sigmoid(x)               #Computes the output of the sigmoid function because it is used in its own derivative\n        return sgm*(1-sgm)\n    else:\n        return 1/(1+np.exp(-x))\n\ndef tanh(x, derivative=False):\n    """"""Implements the hyperbolic tangent function element wise over an array x.\n\n    Parameters\n    ----------\n    x : numpy array\n        This array contains arguments for the hyperbolic tangent function.\n    derivative : bool\n        Indicates whether to use the hyperbolic tangent function or its derivative.\n\n    Returns\n    -------\n    numpy array\n        An array of equal shape to `x`.\n\n    """"""\n\n    if derivative:\n        tanh_not_derivative = tanh(x)\n        return 1.0 - tanh_not_derivative**2\n        #return 1.0 - x**2\n    else:\n        return np.tanh(x)\n'"
netbuilder/file_operations.py,4,"b'# -*- coding: utf-8 -*-\n""""""Created on Mon Mar 20 11:20:11 2017\n\n@author: Andres Berejnoi\n""""""\n#from . import NeuralNet\n#from netbuilder import Network\n#from . import keys\n#from . import Network, keys\nfrom netbuilder import keys\n#import numpy as np\nfrom netbuilder import np\nimport yaml\nimport os\n\n\ndef load_model(directory,is_csv=False):\n    """"""Loads a network model that is saved in the specified directory.\n\n    Parameters\n    ----------\n    directory : str\n        Folder path where network save files are stored.\n    is_csv : bool, not implemented\n        A boolean flag to know if the model to load uses csv or numpy format for the weights.\n    """"""\n\n    #remember current directory and move to desired directory\n    start_dir = os.getcwd()\n    os.chdir(directory)\n\n    #look for configuration file\n    config_file = keys._config_file    #I will look for a better way to automate this file name or make it accessible across the package\n    with open(config_file,\'r\') as f:\n        parameters = yaml.load(f)\n    #name = parameters[\'name\']\n    #topology = parameters[\'topology\']\n    #learningRate = parameters[\'learningRate\']\n    #momentum = parameters[\'momentum\']\n    #hidden_activation = parameters[\'hiddenActivation\']\n    #output_activation = parameters[\'outputActivation\']\n    #size = parameters[\'size\']\n    weights_file = parameters[keys._weights_file]\n\n    #open network weights\n    weights_dict = None\n    if is_csv:\n        pass\n    else:\n        weights_dict = np.load(weights_file)\n\n    #Go back to starting directory\n    os.chdir(start_dir)\n\n    #Create and initialize network\n    #net = Network()\n    net = netbuilder.Network()\n    net._init_from_file(params=parameters,weights_dict=weights_dict)\n\n    print(""Model {0} loaded correctly"".format(net.name))\n\n    return net\n\n\ndef save_model(net,directory=\'.\',csv_mode=False):\n    """"""Creates a directory and saves the network model in it.\n\n    Parameters\n    ----------\n    directory : str\n        Directory where network save folder will be created.\n    model : Network\n        Network object to save to a file.\n    csv_mode : bool, not implemented\n        if True then save network weights as a csv file. Otherwise, weights are saved as numpy format *.npz.\n\n    Returns\n    -------\n    str\n        The path to the output folder so that it can be loaded later.\n    """"""\n\n    folder_name_base = ""{0}_Model"".format(net.name)\n    fold_index = _get_next_foldername_index(folder_name_base,directory)\n    net_folder_name = ""{0}.{1}"".format(folder_name_base,fold_index)\n    #pass\n    initial_working_dir = os.getcwd()\n    print(""Working directory when calling save:"",initial_working_dir)\n\n    #move to specified directory and create output folder\n    os.chdir(directory)\n\n    try:\n        os.mkdir(net_folder_name)\n    except FileExistsError:\n        raise\n    os.chdir(net_folder_name)\n    output_path = os.getcwd()\n\n    #Save weight\n    try:\n        if csv_mode:\n            #save weights in .csv format\n            file_to_save = net.name + \'_weights.csv\'\n            with open(file_to_save,\'w\') as f:\n                for mat in net.weights:\n                    #np.savetxt(f,mat.shape,delimiter=\',\')\n                    np.savetxt(f,mat,delimiter=\',\')\n\n        else:\n            #generate array names to save:\n            names = [str(i) for i in range(net.size)]\n            file_to_save = net.name + \'_weights.npz\'\n            mapped_names = {key:mat for key,mat in zip(names,net.weights)}\n            np.savez(file_to_save, **mapped_names)\n\n        print(""""""Weights saved successfully in file {0}"""""".format(file_to_save))\n    except:\n        print(""Something went wrong when saving weights"")\n        raise\n\n    #Extract other network parameters:\n        #name\n        #topology\n        #learning rate\n        #momentum\n        #size\n    parameters = net._get_model()\n    parameters[keys._weights_file] = file_to_save    #adding the filename to the dictionary\n\n    with open(keys._config_file, \'w\') as f:\n        yaml.dump(data=parameters,stream=f)\n\n\n    print(""Files saved successfully at location:"",output_path)\n\n    #When everthing is done, go back to original working directory\n    os.chdir(initial_working_dir)\n\n    #return the path of output folder in case it is needed later\n    return output_path\n\ndef _get_next_foldername_index(name_to_check,dir_path):\n    """"""Finds folders with name_to_check in them in dir_path and extracts which one has the hgihest index.\n\n    Parameters\n    ----------\n    name_to_check : str\n        The name of the network folder that we want to look repetitions for.\n    dir_path : str\n        The folder where we want to look for network model repetitions.\n\n    Returns\n    -------\n    str\n        If there are no name matches, it returns the string \'1\'. Otherwise, it returns str(highest index found + 1)\n    """"""\n\n    dir_content = os.listdir(dir_path)\n    dir_name_indexes = [int(item.split(\'.\')[-1]) for item in dir_content if os.path.isdir(item) and name_to_check in item]    #extracting the counter in the folder name and then we find the maximum\n\n    if len(dir_name_indexes) == 0:\n        return \'1\'\n    else:\n        highest_idx = max(dir_name_indexes)\n        return str(highest_idx + 1)\n    #find all folders that have name_to_check in them:\n'"
netbuilder/loss.py,2,"b'# -*- coding: utf-8 -*-\n""""""Created on Sat Mar 25 21:24:00 2017 @author: andres\n\nThis module contains error/loss functions that can be used to train the network\n\n""""""\n__all__ = [\'mean_squared_error\']\n#import numpy as np\nfrom netbuilder import np\n\ndef mean_squared_error(target,actual,derivative=False):\n    """"""A simple loss function. It computes the difference between target and actual and\n    raises the value to the power of 2, and everything is divided by 2. The computed value is the error.\n\n    Parameters\n    ----------\n    target : numpy array\n        Contains the values we want the network to approximate.\n    actual : numpy array\n        Same shape as target. It is the output of the network after feedforward propagation.\n\n    Returns\n    -------\n    float/numpy array\n        If function is called in normal mode, then a float (the error) is returned. When derivative mode\n        is used, a numpy array is returned (same shape as input arrays).\n    """"""\n\n    try:\n        assert(target.shape==actual.shape)\n    except AssertionError:\n        print(""""""Shape of target array \'{0}\' does not match shape of actual \'{1}\'"""""".format(target.shape,actual.shape))\n        raise\n    if not derivative:\n        #compute the error and return it\n        #print(\'=\'*80)\n        #print(\'Error Function: MSE\\nTarget:\\tActual:\')\n        #for i in range(len(target)):\n        #    print(target[i],actual[i])\n\n        #print()\n        #print(\'Summing over rows and squaring:\')\n        #for i in range(len(target)):\n        #    print(np.sum((target[i]-actual[i])**2))\n        error = np.sum(0.5 * np.sum((target-actual)**2, axis=1, keepdims=True))\n        return error\n    else:\n        return (actual - target)\n'"
test_scripts/debug_test.py,7,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Fri Mar 17 00:25:09 2017\n\n@author: andresberejnoi\n""""""\n\nfrom . import *    #this will import everything necessary\n#from . import Network, mean_squared_error\n#from . import tanh,sigmoid\n#from . import save_model,load_model\n\nimport numpy as np\n\n#Some tests\ndef random_training_set():\n    \n    #Define input and output layer neurans\n    numIn = 2\n    numOut = 3\n    #create random inputs and outputs\n    np.random.seed(50)\n    input_set = np.random.rand(1000,numIn)   #1000 samples where each sample has numIn features\n    target_set = np.random.rand(1000,numOut)   \n    \n    net = Network(topology=[numIn,3,numOut])\n    net.train(input_set=input_set,\n              target_set=target_set,\n              batch_size=0,\n              epochs=1000)\n    \ndef test_AND():\n    \n    print(\'=\'*80)\n    print(""TEST AND"")\n    #Define input and output layer neurans\n    numIn = 2\n    numOut = 1\n    \n    #num_samples = 4\n    \n    #Create training sets\n    T,F = 1.,-1.\n    input_set = np.array([[F,F],\n                          [F,T],\n                          [T,F],\n                          [T,T]])\n    \n    target_set = np.array([[F],\n                           [F],\n                           [F],\n                           [T]])\n\n    \n    net_name = \'AND\'\n    net = Network()\n    net.init(topology=[numIn,numOut],name=net_name)\n    #net.set_outActivation_fun(func=\'sigmoid\')\n    net.train(input_set=input_set,\n              target_set=target_set,\n              batch_size=0,\n              epochs=1000,\n              print_rate=100)\n    \n    x = input_set[0:1]\n    y = target_set[0:1]\n    print(x.shape)\n    test_out = net.feedforward(x)\n    print(\'TEST OUTPUT:\')\n    print(test_out)\n    \n    error = mean_squared_error(target=y,actual=test_out)\n    print(\'ERROR:\',error)\n    print(\'=\'*80)\n    \n    return net\n\ndef test_XOR():\n    \n    print(\'=\'*80)\n    print(""TEST XOR"")\n    #Define input and output layer neurans\n    numIn = 2\n    numOut = 1\n    \n    #num_samples = 4\n    \n    #Create training sets\n    T,F = 1.,-1.\n    input_set = np.array([[F,F],\n                          [F,T],\n                          [T,F],\n                          [T,T]])\n    \n    target_set = np.array([[F],\n                           [T],\n                           [T],\n                           [F]])\n\n    \n    net_name = \'XOR\'\n    net = Network()\n    net.init(topology=[numIn,5,numOut],name=net_name)\n    net.train(input_set=input_set,\n              target_set=target_set,\n              batch_size=4,\n              epochs=1000,\n              print_rate=100)\n    \n    \n    test_out = net.feedforward(input_set)\n    print(\'\\nTEST OUTPUT:\')\n    print(test_out)\n    \n    error = mean_squared_error(target=target_set,actual=test_out)\n    print(\'ERROR:\',error)\n    print(\'=\'*80)\n    \n    return net\n    \nif __name__==\'__main__\':\n    #random_training_set()\n    net = test_AND()\n    #net = test_XOR()\n    \n    #Test saving method\n    output_path = save_model(net=net)\n    net_loaded = load_model(output_path)\n    \n'"
test_scripts/plot_test.py,4,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Fri May  5 02:25:03 2017\n\n@author: andres\n""""""\n\nimport netbuilder as nb\nimport numpy as np\nimport sys\n\ndef create_dummy_data():\n    #Create training sets\n    T,F = 1.,-1.\n    input_set = np.array([[F,F],\n                          [F,T],\n                          [T,F],\n                          [T,T]])\n    \n    target_set = np.array([[F],\n                           [F],\n                           [F],\n                           [T]])\n    \n\nif __name__==\'__main__\':\n    top = [2,5,1]\n    net = nb.Network()\n    net.init(top)\n    \n    #get data:\n    try:\n        data_file = sys.argv[1]\n        \n    except IndexError:\n        data_file = \n    #Create training sets\n    T,F = 1.,-1.\n    input_set = np.array([[F,F],\n                          [F,T],\n                          [T,F],\n                          [T,T]])\n    \n    target_set = np.array([[F],\n                           [F],\n                           [F],\n                           [T]])\n    \n    net.train(input_set=input_set,\n              target_set=target_set,\n              batch_size=0,\n              epochs=1000,\n              print_rate=100,\n              plot=True)\n\n\n\n'"
netbuilder/docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# NetBuilder documentation build configuration file, created by\n# sphinx-quickstart on Fri Apr 21 11:23:43 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n#sys.path.insert(0,os.path.abspath(\'.\'))\nsys.path.insert(0, os.path.abspath(\'../../\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n#------------------------------------------------------------------------\n#include python packages that depend on C extensions\nfrom unittest.mock import MagicMock\n\nclass Mock(MagicMock):\n    @classmethod\n    def __getattr__(cls, name):\n            return MagicMock()\n\nMOCK_MODULES = [\'numpy\']\nsys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n#------------------------------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n    \'sphinx.ext.napoleon\',\n    #\'numpydoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\']\n\n# Napoleon settings\nnapoleon_google_docstring = False\nnapoleon_numpy_docstring = True\nnapoleon_include_init_with_doc = False\nnapoleon_include_private_with_doc = False\nnapoleon_include_special_with_doc = True\nnapoleon_use_admonition_for_examples = False\nnapoleon_use_admonition_for_notes = False\nnapoleon_use_admonition_for_references = False\nnapoleon_use_ivar = False\nnapoleon_use_param = True\nnapoleon_use_rtype = True\n\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'NetBuilder\'\ncopyright = \'2017, Andres Berejnoi Bejarano\'\nauthor = \'Andres Berejnoi Bejarano\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'0.2\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'0.2.2\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'NetBuilderdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'NetBuilder.tex\', \'NetBuilder Documentation\',\n     \'Andres Berejnoi Bejarano\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'netbuilder\', \'NetBuilder Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'NetBuilder\', \'NetBuilder Documentation\',\n     author, \'NetBuilder\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n'"
netbuilder/examples/__init__.py,0,b'#from . import *\n'
netbuilder/examples/image_number.py,30,"b'""""""Created on Fri Feb  5 16:32:41 2016\n\n@author: andresberejnoi\n""""""\nimport numpy as np\nfrom .. import Network\nfrom .trainingShapes import shapes2\n\n\nshapes = {0: np.array([ [-0.5,-0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,-0.5,0.5,0.5,-0.5,-0.5]]),\n\n          1: np.array([ [-0.5,-0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,-0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,-0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,-0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,-0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,-0.5,0.5,0.5,-0.5,-0.5]]),\n\n          2: np.array([ [-0.5,-0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,0.5,-0.5,-0.5],\n                        [-0.5,-0.5,0.5,-0.5,-0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5]]),\n\n          3: np.array([ [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5]]),\n\n          4: np.array([ [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5]]),\n\n          5: np.array([ [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,-0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5]]),\n\n          6: np.array([ [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5]]),\n\n          7: np.array([ [-0.5,0.5,0.5,0.5,0.5,0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,0.5,-0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,-0.5,-0.5],\n                        [0.5,-0.5,-0.5,-0.5,-0.5,-0.5]]),\n\n          8: np.array([ [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5]]),\n\n          9: np.array([ [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5]]),\n}\n\nclass number(object):\n    """"""This class provides methods for creating simulated image inputs for a neural network.\n\n    The purpose is that it can generate inputs to train a neural network in pattern recognition of numbers.    \n\n    """"""\n\n    def __init__(self,number = 0, resolution = 36, noise = 0.0):\n        """"""Initializer.\n\n        """"""\n\n        self.shape = number\n        self.res = resolution\n        self.noise = noise\n\n\n\n\ndef main():\n    inputs1 = [shape.flatten() for shape in shapes.values()]            #creates a list of the values in the dictionary\n    inputs2 = [shape.flatten() for shape in shapes2.values()]\n    \n    inputs = inputs1 + inputs2\n    #inputs = inputs1\n    \n    \'\'\'\n    targets = [np.array([1,0,0,0,0,0,0,0,0,0]),             #we want a zero\n               np.array([0,1,0,0,0,0,0,0,0,0]),             #we want a 1\n               np.array([0,0,1,0,0,0,0,0,0,0]),\n               np.array([0,0,0,1,0,0,0,0,0,0]),\n               np.array([0,0,0,0,1,0,0,0,0,0]),\n               np.array([0,0,0,0,0,1,0,0,0,0]),\n               np.array([0,0,0,0,0,0,1,0,0,0]),\n               np.array([0,0,0,0,0,0,0,1,0,0]),\n               np.array([0,0,0,0,0,0,0,0,1,0]),\n               np.array([0,0,0,0,0,0,0,0,0,1])]\n    \'\'\'\n    \n    \n    targets = [np.array([0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5]),             #we want a zero\n               np.array([-0.5,0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5]),             #we want a 1\n               np.array([-0.5,-0.5,0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5]),\n               np.array([-0.5,-0.5,-0.5,0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5]),\n               np.array([-0.5,-0.5,-0.5,-0.5,0.5,-0.5,-0.5,-0.5,-0.5,-0.5]),\n               np.array([-0.5,-0.5,-0.5,-0.5,-0.5,0.5,-0.5,-0.5,-0.5,-0.5]),\n               np.array([-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,0.5,-0.5,-0.5,-0.5]),\n               np.array([-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,0.5,-0.5,-0.5]),\n               np.array([-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,0.5,-0.5]),\n               np.array([-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,0.5])]\n    \n    \n    trainingSet = list(zip(inputs,targets+targets))\n    #trainingSet = list(zip(inputs,targets))\n    \n    topology = [36,50,50,10]\n    net = Network(topology,0.01,0.01)\n    #net.save(""recog_number_weights.csv"", transpose=True, keep_bias=False)\n    \n    #random_out = net.feedforward(inputs[1])\n    \n    epochs = 10000\n    tolerance = 1E-5\n    \n    net.train(trainingSet,epochs,tolerance)\n    #print()\n    #print(random_out)\n    \nif __name__==\'__main__\':\n    main()\n    \n'"
netbuilder/examples/simple_sample.py,12,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Feb 24 -0.52:-0.52:50.5 2-0.50.56\n\n@author: andresberejnoi\n""""""\n\nfrom ..NeuralNet import Network, save_outputs\n#from tools import save_outputs\nimport numpy as np\n\n\'\'\'\nBecause of the tanh functions used by default for the activations of the network,\nbetter results are obtained by using -0.5 instead of 0 and 0.5 instead of 1. So, for example,\nthe truth table for the AND gate will be as:\n    -0.5 -0.5 | -0.5\n    -0.5  0.5 | -0.5\n     0.5 -0.5 | -0.5\n     0.5  0.5 |  0.5\n    \nTherefore it is important to remember that when interpreting the result of the network\n\'\'\'\n\n\n# Teach network XOR function\n#training set for XOR\nxor = [\n        [np.array([-0.5,-0.5]), np.array([-0.5])],\n        [np.array([-0.5,0.5]), np.array([0.5])],\n        [np.array([0.5,-0.5]), np.array([0.5])],\n        [np.array([0.5,0.5]), np.array([-0.5])]\n      ]\n\n#training set for logic AND \naand = [\n         [np.array([-0.5,-0.5]), np.array([-0.5])],\n         [np.array([-0.5,0.5]), np.array([-0.5])],\n         [np.array([0.5,-0.5]), np.array([-0.5])],\n         [np.array([0.5,0.5]), np.array([0.5])]\n       ]\n    \n#training set for logic OR \noor = [\n        [np.array([-0.5,-0.5]), np.array([-0.5])],\n        [np.array([-0.5,0.5]), np.array([0.5])],\n        [np.array([0.5,-0.5]), np.array([0.5])],\n        [np.array([0.5,0.5]), np.array([0.5])]\n      ]\n\n# The above training sets are not exactly in the correct format that the network expects, but I do not want to rewrite them again,\n# so below they will be ""fixed""\nxor = [(array[0],array[1]) for array in xor]\naan = [(array[0],array[1]) for array in aand]\noor = [(array[0],array[1]) for array in oor]\n\n#Setting up the network\ntopology = [2,10,10,1]\nepochs = 1000\ntolerance = 1E-10\ntrainingSet = xor                           #change this to any of the training sets above: trainingSet = aand, etc\n\nprint(""=""*80)\nprint(""Training...\\n"")\nnet = Network(topology, learningRate=0.1, momentum=0.1)\nnet.train(trainingSet,epochs,tolerance, batch=False)         #training begins\n\n#Now, show the results of training\n#It would be better to create a function to display this information in a better way\n#print(""=""*80)      #will 80 \'=\' signs to separate the line\nprint()\nprint(""Testing network:"")\nprint(""INPUTS    |\\tPREDICTION\\t   | EXPECTED"")\nfor inputs,target in trainingSet:\n    out = net.feedforward(inputs)\n\n    print(""{0} {1} \\t {2} \\t\\t\\t {3}   "".format(inputs[0],inputs[1],out[0],target[0]))              #for some reason, the last line is not tabbed in\n    \nprint(""=""*80)\n\n\n#extracting the value of the training pattern:\nxor_patterns = [pat[0] for pat in xor]\nsave_outputs(""xor_outs.csv"", xor_patterns, net)\n'"
netbuilder/examples/test_script.py,22,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Fri Feb  5 17:25:57 2016\n\n@author: andresberejnoi\n""""""\nimport numpy as np\nfrom NeuralNet import Network, sigmoid\n\n\n        \n#Testing the network\n#np.random.random()\n#np.random.normal(scale=0.1,size=(4,4))                                          #attempt to change the random numbers, or something\n        \n# Training exmaples for XOR gate\n\'\'\'\nx1 = np.array([0,0])\nx2 = np.array([0,1])\nx3 = np.array([1,0])\nx4 = np.array([1,1])\n\ntrainingTargets = [np.array([0]),\n                    np.array([1]),\n                    np.array([1]),\n                    np.array([0])]\n\'\'\'\n\nx1 = np.array([-0.5,-0.5])\nx2 = np.array([-0.5,0.5])\nx3 = np.array([0.5,-0.5])\nx4 = np.array([0.5,0.5])\n\ntrainingInputs = [x1,x2,x3,x4]     #                                            # an array of the inputs                             \n# Targets for training the network forn XOR gate\n\n       \ntrainingTargets = [np.array([-0.5]),\n                    np.array([0.5]),\n                    np.array([0.5]),\n                    np.array([-0.5])]\n\n\'\'\'               \ntrainingTargets = [np.array([0]),\n                    np.array([1]),\n                    np.array([1]),\n                    np.array([0])]\n\'\'\'\n                    \n        \n        \ntopology = [2,10,10,1]\nnet = Network(topology,0.1,0.1)\nnet.save(""recog_number.csv"", transpose=True, keep_bias = False)                 # saving a file with the iniitial weights\n#net.outActiv_fun = sigmoid\n\n\ntrainingSet = list(zip(trainingInputs,trainingTargets))\nepochs = 10000\ntolerance = 1E-10\n\nprint(""Initial Weights:"")\nfor W in net.weights:\n    print(W)\n    print()\n\nnet.train(trainingSet,epochs,tolerance)\n\n\ndef test(rep=10):\n    \'\'\'A small test function\'\'\'\n    global trainingSet\n    topology = [2,5,5,1]\n    net = Network(topology)\n    net.Gradients = [None,None]\n    for i in range(rep+1):\n        error=0.0\n        for I,P in trainingSet:\n            print(""Weights:"")\n            print(net.weights[0])\n            print(""Gradients:"")\n            print(net.Gradients[0])\n            #print(""Previous change:"")\n            #print(net.last_change[0])\n            print()\n            print(net.weights[1])\n            print(""Gradients:"")\n            print(net.Gradients[1])\n            #print(""Previous Change:"")\n            #print(net.last_change[1])\n            print()\n    \n            error += net.backprop(I,P)\n            \n            print(""Activations:"")\n            print(""L=0 : "",net.netOuts[0])\n            print(""L=1 : "",net.netOuts[1])\n            print(""L=2 : "",net.out)\n            print()\n        \n        print(""-----------------------------------"")\n        print(""ERROR: "",error, ""EPOCH: "",i)\n        print(""-----------------------------------"")\n        print()\n'"
netbuilder/examples/trainingShapes.py,10,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Fri Feb  5 0.59:-0.53:35 2-0.50.56\n\n@author: andresberejnoi\n""""""\nimport numpy as np\n\nshapes2 = {0: np.array([ [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5]]),\n\n          1: np.array([ [-0.5,-0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,-0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,-0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,-0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5]]),\n\n          2: np.array([ [-0.5,-0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,0.5,-0.5,-0.5],\n                        [-0.5,-0.5,0.5,-0.5,-0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5]]),\n\n          3: np.array([ [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5]]),\n\n          4: np.array([ [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5]]),\n\n          5: np.array([ [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,-0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5]]),\n\n          6: np.array([ [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5]]),\n\n          7: np.array([ [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,0.5,-0.5,-0.5],\n                        [-0.5,-0.5,-0.5,0.5,-0.5,-0.5],\n                        [-0.5,-0.5,0.5,-0.5,-0.5,-0.5],\n                        [-0.5,-0.5,0.5,-0.5,-0.5,-0.5]]),\n\n          8: np.array([ [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,0.5,-0.5]]),\n\n          9: np.array([ [-0.5,-0.5,0.5,0.5,-0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,-0.5,0.5,0.5,0.5,-0.5],\n                        [-0.5,-0.5,-0.5,-0.5,0.5,-0.5],\n                        [-0.5,0.5,0.5,0.5,-0.5,-0.5]]),\n}'"
netbuilder/tests/__init__.py,0,b'#from . import *\n'
netbuilder/tests/training_random.py,3,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Mar 16 02:12:46 2017\n\n@author: andresberejnoi\n""""""\n\nimport numpy as np\nfrom .. import Network\n\ndef random_training_set():\n    \n    #Define input and output layer neurans\n    numIn = 5\n    numOut = 3\n    #create random inputs and outputs\n    np.random.seed(50)\n    input_set = np.random.rand(1000,numIn)   #1000 samples where each sample has numIn features\n    target_set = np.random.rand(1000,numOut)   \n    \n    net = Network(topology=[numIn,15,15,numOut])\n    net.train(input_set=input_set,\n              target_set=target_set,\n              batch_size=100)\n    '"
netbuilder/docs/source/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# netbuilder documentation build configuration file, created by\n# sphinx-quickstart on Fri Apr 21 23:29:11 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, \'/home/andres/GitHub/NetBuilder/netbuilder\')\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.viewcode\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'netbuilder\'\ncopyright = \'2017, Author\'\nauthor = \'Author\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = \'en\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'netbuilderdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'netbuilder.tex\', \'netbuilder Documentation\',\n     \'Author\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'netbuilder\', \'netbuilder Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'netbuilder\', \'netbuilder Documentation\',\n     author, \'netbuilder\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n# -- Options for Epub output ----------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\nepub_author = author\nepub_publisher = author\nepub_copyright = copyright\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n'"
