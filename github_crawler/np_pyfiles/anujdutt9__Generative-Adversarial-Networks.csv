file_path,api_count,code
GAN.py,24,"b'# Generative Adversarial Networks using 1-D input signal\n\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nfrom scipy.stats import norm\n\nstyle.use(\'ggplot\')\n\n# Input Data Sample (P_data)\nmu, sigma = -1,1\nxs=np.linspace(-5,5,1000)\n# plt.plot(xs, norm.pdf(xs,loc=mu,scale=sigma))\n\nnumTrainIters = 10000\nM = 200\n\n# ------------------------------ Multi Layer Perceptron -------------------------\n# MultiLayer Perceptron Layers Weights and Bias Value Initialization\n# 4-layer ANN with 1 Hidden Layer (6 neurons) and 1 Deeply Connected Layer (5 neurons)\n# Total Number of Neurons = 11\n# Input => Hidden Layer => Deeply Connected Layer => Output\n# Weights are between the layers.\n# Biases are on the neurons of the Hidden Layers.\ndef MultiLayerPerceptron(input, output_dim):\n    # Initialize the weights with ""empty""\n    # Initialize the biases with ""0.0""\n    init_const = tf.constant_initializer(0.0)\n    init_normal = tf.random_normal_initializer()\n    # Format: tf.get_variable(name, shape, datatype, initializer, regularizer...)\n    # Weights from Input layer to Hidden Layer => W1\n    # W1 matrix is of the form: [Input data samples, number of neurons in Hidden Layer]\n    w1 = tf.get_variable(\'w1\', [input.get_shape()[1], 6], initializer=init_normal)\n\n    # Bias is added on the Hidden Layer Neuron.\n    # Dimension of Bias Matrix: [number of neurons in hidden layer]\n    b1 = tf.get_variable(\'b1\', [6], initializer=init_const)\n\n    # Deeply Connected Layer / Second Hidden Layer\n    # Weight Dimension: [num neurons in 1st Hidden Layer, num neurons in 2nd Hidden Layer]\n    w2 = tf.get_variable(\'w2\', [6, 5], initializer=init_normal)\n\n    # Bias is added on the Hidden Layer Neuron.\n    # Dimension of Bias Matrix: [number of neurons in 2nd Hidden Layer]\n    b2 = tf.get_variable(\'b2\', [5], initializer=init_const)\n\n    # Weights from Deeply Connected Layer to Output Neuron / Layer\n    # W3 matrix has dimensions: [num neurons in hiddenlayer - 1, num neurons in output layer]\n    w3 = tf.get_variable(\'w3\', [5, output_dim], initializer=init_normal)\n\n    # Bias is added on the Output Layer.\n    # Dimension of Bias Matrix: [number of neurons in Output Layer]\n    b3 = tf.get_variable(\'b3\', [output_dim], initializer=init_const)\n\n    # Activation Function (tanh()) for 1st Hidden Layer\n    a1 = tf.nn.tanh(tf.matmul(input, w1) + b1)\n\n    # Activation Function (tanh()) for 2nd Hidden Layer\n    a2 = tf.nn.tanh(tf.matmul(a1, w2) + b2)\n\n    # Activation Function (tanh()) for Output Layer\n    y_Hat = tf.nn.tanh(tf.matmul(a2, w3) + b3)\n    return y_Hat, [w1, b1, w2, b2, w3, b3]\n\n\n\n# --------------------------- Train the Neural Network ---------------------------\n# Using Momentum Optimizer to decrease the Learning Rate as\n# we progress so as to reach the minima and not miss it.\n# Useful in case when the input is not a Convex function.\ndef momentumOptimizer(loss, var_list):\n    # Initialize the learning rate to a reasonable value\n    baseLearningRate = 0.001\n    # Set the decay Rate\n    decayRate = 0.95\n    # Set the step size for decay\n    decaySteps = numTrainIters // 4\n    batch = tf.Variable(0)\n    learning_rate = tf.train.exponential_decay(\n        baseLearningRate,\n        batch,\n        decaySteps,\n        decayRate,\n        staircase=True\n    )\n    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.6).minimize(\n        loss,\n        global_step=batch,\n        var_list=var_list\n    )\n    return optimizer\n\n\n# ------------------------ Generative Adversarial Network --------------------------\n# Pre-Train the Discriminator.\n# Doing this saves time at a later stage and only tuning of Discriminator is required.\nwith tf.variable_scope(""D_pre""):\n    # Input Data to the Discriminator\n    # N sample values in matrix form\n    input_node = tf.placeholder(tf.float32, shape=(M, 1))\n\n    # Training Labels = Number of input samples\n    train_labels = tf.placeholder(tf.float32, shape=(M, 1))\n\n    # Input the data to Neural Network and get the Output\n    # Discriminator sample (D) = Output (y_Hat)\n    # Neural Network[input_node, hidden_dim = 6, output_dim = 1]\n    D, theta = MultiLayerPerceptron(input_node, 1)\n\n    # Calculate the loss using Mean Squared Error (MSE)\n    loss = tf.reduce_mean(tf.square(D - train_labels))\n\n# Optimize the Discriminator for Minimizing Loss / Cost Function.\noptimizer = momentumOptimizer(loss, None)\n\n\n\nsess = tf.InteractiveSession()\ntf.global_variables_initializer().run()\n\n\n# ------------------------ Plot Initial Decision Surface -----------------------------\ndef plot_d0(D, input_node):\n    f, ax = plt.subplots(1)\n    # p_data\n    xs = np.linspace(-5, 5, 1000)\n    ax.plot(xs, norm.pdf(xs, loc=mu, scale=sigma), label=\'p_data\')\n    # decision boundary\n    r = 1000  # resolution (number of points)\n    xs = np.linspace(-5, 5, r)\n    ds = np.zeros((r, 1))  # decision surface\n    # process multiple points in parallel in a minibatch\n    for i in range(int(r/M)):\n        x = np.reshape(xs[M*i:M*(i + 1)],(M,1))\n        ds[M*i:M*(i + 1)] = sess.run(D, {input_node: x})\n\n    ax.plot(xs, ds, label=\'decision boundary\')\n    ax.set_ylim(0, 1.1)\n    plt.legend()\n\n# plot_d0(D, input_node)\n# plt.title(\'Initial Decision Boundary\')\n# plt.show()\n# sess.close()\n\n\n# ------------------------------------ Plot Training Loss -----------------------------\nlh=np.zeros(1000)\nfor i in range(1000):\n    #d=np.random.normal(mu,sigma,M)\n    d=(np.random.random(M)-0.5) * 10.0 # instead of sampling only from gaussian, want the domain to be covered as uniformly as possible\n    labels=norm.pdf(d,loc=mu,scale=sigma)\n    lh[i],_=sess.run([loss,optimizer], {input_node: np.reshape(d,(M,1)), train_labels: np.reshape(labels,(M,1))})\n\n# plt.plot(lh)\n# plt.title(\'Training Loss\')\n# plt.show()\n\n# plot_d0(D,input_node)\n\n# Theta gets the weights and biases with lowest cost function for Pre Trained Network D.\n# Save it.\nlearnedWeights = sess.run(theta)\nsess.close()\n\n\n# Generator Network\nwith tf.variable_scope(""G""):\n    # M: Input Samples generated (Noise Signal)\n    z_node=tf.placeholder(tf.float32, shape=(M,1))\n    # Get the output and weights of the Generator using Neural Network\n    G,theta_g = MultiLayerPerceptron(z_node,1)\n    # Scale by 5 to match with range\n    G=tf.multiply(5.0,G)\n\n\n# Discriminator Network:\n# D1 => actual Data input Discriminator\n# D2 => Output of Generator is Input to Discriminator\nwith tf.variable_scope(""D"") as scope:\n    # D1 => Trained on Input Data\n    x_node = tf.placeholder(tf.float32, shape=(M,1))\n    fc,theta_d = MultiLayerPerceptron(x_node,1)\n    D1=tf.maximum(tf.minimum(fc,.99), 0.01)\n    # make a copy of D that uses the same variables, but takes in G as input\n    scope.reuse_variables()\n\n    # D2 => Takes output of Generator as Input\n    # Output of Generator (G) input to Discriminator (D2)\n    fc,theta_d = MultiLayerPerceptron(G,1)\n    D2 = tf.maximum(tf.minimum(fc,.99), 0.01)\n\n# Calculating the Value of Discriminator ""D""\n# log(D1(x)) + log(1-D2(x))\n# We need to Maximize D1 and Minimize value of D2.\n# obj_d: Value receieved after processing Input Data\nobj_d = tf.reduce_mean(tf.log(D1)+tf.log(1-D2))\n\n# Calculating value of Generator Function ""G""\n# log(D2(x))\n# We need to maximize log(D2(x)) to successfully fool Discriminator\n# obj_g: Value received after processing Generator Data\nobj_g = tf.reduce_mean(tf.log(D2))\n\n# set up optimizer for G and D\nopt_d = momentumOptimizer(1-obj_d, theta_d)\nopt_g = momentumOptimizer(1-obj_g, theta_g)\n\n\n\nsess=tf.InteractiveSession()\ntf.global_variables_initializer().run()\n\n\n\n# Copy the Weights saved in Pre-Training over to New D Network\nfor i,v in enumerate(theta_d):\n    sess.run(v.assign(learnedWeights[i]))\n\n\n\ndef plot_fig():\n    # plots pg, pdata, decision boundary\n    f,ax=plt.subplots(1)\n    # p_data\n    xs=np.linspace(-5,5,1000)\n    ax.plot(xs, norm.pdf(xs,loc=mu,scale=sigma), label=\'p_data\')\n\n    # decision boundary\n    r=5000 # resolution (number of points)\n    xs=np.linspace(-5,5,r)\n    ds=np.zeros((r,1)) # decision surface\n    # process multiple points in parallel in same minibatch\n    for i in range(int(r/M)):\n        x=np.reshape(xs[M*i:M*(i+1)],(M,1))\n        ds[M*i:M*(i+1)]=sess.run(D1,{x_node: x})\n\n    ax.plot(xs, ds, label=\'decision boundary\')\n\n    # distribution of inverse-mapped points\n    zs=np.linspace(-5,5,r)\n    gs=np.zeros((r,1)) # generator function\n    for i in range(int(r/M)):\n        z=np.reshape(zs[M*i:M*(i+1)],(M,1))\n        gs[M*i:M*(i+1)]=sess.run(G,{z_node: z})\n    histc, edges = np.histogram(gs, bins = 10)\n    ax.plot(np.linspace(-5,5,10), histc/float(r), label=\'p_g\')\n\n    ax.set_ylim(0,1.1)\n    plt.legend()\n\nplot_fig()\nplt.title(\'Before Training\')\nplt.show()\n\n\n\n# Algorithm 1 of Goodfellow et al 2014\n# Instead of optimizing with one pair (x,z) at a time, we update the gradient\n# based on the average of M loss gradients computed for M different (x,z) pairs.\n# The stochastic gradient estimated from a minibatch is closer to the true gradient\n# across the training data.\nk=1\nhistd, histg= np.zeros(numTrainIters), np.zeros(numTrainIters)\nfor i in range(numTrainIters):\n    for j in range(k):\n        x= np.random.normal(mu,sigma,M) # sampled m-batch from p_data\n        x.sort()\n        # Sample Batch Noise Input of Dimension ""M""\n        # Input Noise signal ""Z"" streached from ""-1 to 1"" to ""-5 to 5""\n        z= np.linspace(-5.0,5.0,M)+np.random.random(M)*0.01\n        histd[i],_ = sess.run([obj_d,opt_d], {x_node: np.reshape(x,(M,1)), z_node: np.reshape(z,(M,1))})\n    # Sample Noise Prior Signal Input\n    z= np.linspace(-5.0,5.0,M)+np.random.random(M)*0.01\n    # Update Generator\n    histg[i],_ = sess.run([obj_g,opt_g], {z_node: np.reshape(z,(M,1))})\n    if i % (numTrainIters//10) == 0:\n        print((float(i)/float(numTrainIters))*100)\n\n\nplt.plot(range(numTrainIters),histd, label=\'obj_d\')\nplt.plot(range(numTrainIters), 1-histg, label=\'obj_g\')\nplt.legend()\n\nplot_fig()\nplt.show()\n\nsess.close()\n\n# ----------------------------------- EOC ----------------------------------------------\n'"
