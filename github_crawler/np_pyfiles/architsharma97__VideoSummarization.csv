file_path,api_count,code
Scripts/LSTM+Skeleton.py,0,"b""\n# coding: utf-8\n\n# In[ ]:\n\nfrom keras.models import Sequential, Graph\nfrom keras.layers import LSTM\nfrom keras.layers import Merge, Convolution1D\nfrom keras.layers import Input, Dense\nimport numpy as np, h5py \n\n\n# In[ ]:\n\nnum_frames = 100\nnum_features = 500\nnum_videos = 40\n\ngraph = Graph()\ngraph.add_input(name='input1', input_shape=(None,num_features))\n\ngraph.add_node(LSTM(256, input_shape=(None, num_features), return_sequences=True), name='lstm1', input='input1')\ngraph.add_node(LSTM(256, input_shape=(None, num_features),go_backwards = True,return_sequences=True), name='lstm2', input='input1')\ngraph.add_node(Convolution1D(nb_filter=256, filter_length=1, border_mode='same', input_shape=(None, 256+256+num_features)),inputs=['lstm1','lstm2','input1'],merge_mode='concat',name='conv1')\n\ngraph.add_output(name='output', input='conv1')\n\n\n# In[ ]:\n\nimport scipy.io as sio\n#trainX = videos_features\nmat_contents = sio.loadmat('Air_Force_One.mat')\ntrainY = mat_contents['gt_score'].T\n\nprint trainY.shape\n\n\n# In[ ]:\n\n#model.fit(trainX, trainY, nb_epoch=15, batch_size=1, verbose=2)\n\n\n# In[ ]:\n\n\n\n"""
Scripts/Evaluation/evaluate.py,1,"b'import sys\nsys.path.append(""../../Data/SumMe/python"")\nimport os\nfrom summe import *\nimport imageio\nimport cv2\n# System Arguments\n# Argument 1: Location of the video\n# Argument 2: Sampling rate\n# Argument 3: Percentage of video as summary\n# Argument 4: Results folder\n\n\n# Argument 5: File where the results will be written\n# Argument 6: Name of the features used\n# Argument 7: Skimming (Put 1)\n\ndef main():\n\tvideo=sys.argv[1]\n\tdirectory=sys.argv[4]\n\tsampling_rate=int(sys.argv[2])\n\tpercent=int(sys.argv[3])\n\tvideo_length=len(imageio.get_reader(sys.argv[1]))\n\tn_clusters=int(percent*video_length/100)\n\tif video_length/sampling_rate < n_clusters:\n\t\tn_clusters=video_length/sampling_rate\n\n\tif len(sys.argv)>7 and sys.argv[7]==""1"":\n\t\tvideo_cv=cv2.VideoCapture(os.path.abspath(os.path.expanduser(sys.argv[1])))\n\t\tfps=int(video_cv.get(cv2.CAP_PROP_FPS))\n\t\tframe_count=int(video_cv.get(cv2.CAP_PROP_FRAME_COUNT))\n\t\tskim_frames_length=fps*1.8\n\t\tn_clusters=int(percent*frame_count/skim_frames_length/100)+1\n\n\tprint ""Getting frames of summary!""\n\tframe_indices=[int(idx) for idx in open(directory+\'frame_indices_\'+ sys.argv[6]+\'_\'+str(n_clusters)+\'_\'+str(sampling_rate)+\'.txt\',\'r\').read().splitlines()]\n\tprint ""Got the frames\'""\n\n\tvideo=video.split(\'/\')\n\tvideoName=video[len(video)-1].split(\'.\')[0]\n\tprint videoName\n\t\n\tvideo[len(video)-2]=""GT""\n\tHOMEDATA=\'/\'.join(video[0:len(video)-1])\n\tprint HOMEDATA\n\n\t# OPTIONAL: Recreating summary\n\t# video=imageio.get_reader(sys.argv[1])\n\t# summary=np.array([video.get_data(idx) for idx in frame_indices])\n\t\n\tf_measure, summary_length=evaluateSummary(frame_indices,videoName,HOMEDATA)\n\tprint ""F-measure %.3f at length %.2f"" %(f_measure, summary_length)\n\n\tif len(sys.argv)>5:\n\t\tif os.path.exists(sys.argv[5])==False:\n\t\t\tout_file=open(sys.argv[5],\'a\')\n\t\t\tout_file.write(""Sampling rate, Number of Clusters, F-measure, Summary Length\\n"")\n\t\telse:\n\t\t\tout_file=open(sys.argv[5],\'a\')\n\t\tout_file.write(""%d,%d,%f,%f\\n""%(sampling_rate,n_clusters,f_measure,summary_length))\n\t\n\t# optional plotting of results\n\t# methodNames={\'VSUMM using Color Histrograms\'}\n\t# summaries={}\n\t# summaries[0]=frame_indices\n\t# plotAllResults(summaries,methodNames,videoName,HOMEDATA)\n\nif __name__ == \'__main__\':\n\tmain()'"
Scripts/SIFT/evaluate.py,1,"b'import sys\nsys.path.append(""../../Data/SumMe/python"")\nimport os\nfrom summe import *\nimport imageio\n# System Arguments\n# Argument 1: Location of the video\n# Argument 2: Sampling rate\n# Argument 3: Percentage of video as summary\n# Argument 4: Results folder\n\n\n# Argument 5: File where the results will be written\n# Argument 6: Name of the features used\n\ndef main():\n\tvideo=sys.argv[1]\n\tdirectory=sys.argv[3]\n\tsampling_rate=int(sys.argv[2])\n\t#percent=int(sys.argv[3])\n\t#video_length=len(imageio.get_reader(sys.argv[1]))\n\t#n_clusters=int(percent*video_length/100)\n\t#if video_length/sampling_rate < n_clusters:\n\t#\tn_clusters=video_length/sampling_rate\n\n\tprint ""Getting frames of summary!""\n\tframe_indices=[int(idx) for idx in open(directory+\'frame_indices_\'+str(sampling_rate)+\'.txt\',\'r\').read().splitlines()]\n\tprint ""Got the frames\'""\n\n\tvideo=video.split(\'/\')\n\tvideoName=video[len(video)-1].split(\'.\')[0]\n\tprint videoName\n\t\n\tvideo[len(video)-2]=""GT""\n\tHOMEDATA=\'/\'.join(video[0:len(video)-1])\n\tprint HOMEDATA\n\n\t# OPTIONAL: Recreating summary\n\t# video=imageio.get_reader(sys.argv[1])\n\t# summary=np.array([video.get_data(idx) for idx in frame_indices])\n\t\n\tf_measure, summary_length=evaluateSummary(frame_indices,videoName,HOMEDATA)\n\tprint ""F-measure %.3f at length %.2f"" %(f_measure, summary_length)\n\n\tout_file=open(sys.argv[4],\'a\')\n\tout_file.write(""Sampling rate, F-measure, Summary Length\\n"")\n\tout_file.write(""%d,%f,%f\\n""%(sampling_rate,f_measure,summary_length))\n\t# if len(sys.argv)>4:\n\t# \tif os.path.exists(sys.argv[4])==False:\n\t# \t\tout_file=open(sys.argv[4],\'a\')\n\t# \t\tout_file.write(""Sampling rate, F-measure, Summary Length\\n"")\n\t# \telse:\n\t# \t\tout_file=open(sys.argv[4],\'a\')\n\t# \tout_file.write(""%d,%f,%f\\n""%(sampling_rate,f_measure,summary_length))\n\t\n\t# optional plotting of results\n\t# methodNames={\'VSUMM using Color Histrograms\'}\n\t# summaries={}\n\t# summaries[0]=frame_indices\n\t# plotAllResults(summaries,methodNames,videoName,HOMEDATA)\n\nif __name__ == \'__main__\':\n\tmain()'"
Scripts/SIFT/videoSumSIFT.py,1,"b'import cv2\nimport sys\nimport os\nimport Image\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nfrom collections import namedtuple\nimport time\n\nfileName = sys.argv[1]\nfolderName = sys.argv[2]\n\nskipFrames = 1\n\nmotionThreshold = 10\n\nColorMoments = namedtuple(\'ColorMoments\', [\'mean\', \'stdDeviation\', \'skewness\'], verbose=False)\n\nShot = namedtuple(\'Shot\', [\'shotNumber\', \'startingFrame\', \'endingFrame\', \'keyFrames\', \'avgEntropyDiff\', \'avgMotion\'])\n\nshotSifts = []\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n\ndef getEntropy(histogram, totalPixels):\n    entropy = 0\n    for pixels in histogram:\n        if pixels != 0:\n            prob = float (pixels / totalPixels)\n            entropy -= prob * math.log(prob, 2)\n            # print prob\n    return entropy\n\ndef getColorMoments(histogram, totalPixels):\n    sum = 0\n    for pixels in histogram:\n        sum += pixels\n    mean = float (sum / totalPixels)\n    sumOfSquares = 0\n    sumOfCubes = 0\n    for pixels in histogram:\n        sumOfSquares += math.pow(pixels-mean, 2)\n        sumOfCubes += math.pow(pixels-mean, 3)\n    variance = float (sumOfSquares / totalPixels)\n    stdDeviation = math.sqrt(variance)\n    avgSumOfCubes = float (sumOfCubes / totalPixels)\n    skewness = float (avgSumOfCubes**(1./3.))\n    return ColorMoments(mean, stdDeviation, skewness)\n\ndef getHistogramDiff(currHistogram, prevHistogram):\n    diff = 0\n    for i in range(len(currHistogram)):\n        diff += math.pow(currHistogram[i]-prevHistogram[i], 2)\n    if diff==0:\n        return 1\n    else:\n        return diff \n\ndef getHistogramRatio(currHistogramDiff, prevHistogramDiff):\n    ratio = float (currHistogramDiff / prevHistogramDiff)\n    if ratio<1:\n        ratio = 1/ratio\n    return ratio\n\ndef getEuclideanDistance(currColorMoments, prevColorMoments):\n    distance = math.pow(currColorMoments.mean - prevColorMoments.mean, 2) + math.pow(currColorMoments.stdDeviation - prevColorMoments.stdDeviation, 2) + math.pow(currColorMoments.skewness - prevColorMoments.skewness, 2)\n    return distance\n\ndef getMotion(currImage, prevImage):\n    motion = 0\n    for i in range(len(currImage)):\n        for j in range(len(currImage[i])):\n            if int (currImage[i][j]) - int (prevImage[i][j]) > motionThreshold:\n                motion += 1\n    motion = float (motion / (i+1)*(j+1))\n    return motion\n\ndef getSift(img):\n    detector = cv2.SIFT(200)\n    kp, des = detector.detectAndCompute(img, None)\n    return des\n\ndef getMotionSift(img1, img2, ind):\n    if ind==1:\n        des1 = getSift(img1)\n        shotSifts.append(des1)\n    else:\n        des1 = shotSifts[ind-1]\n    des2 = getSift(img2)\n    shotSifts.append(des2)\n    FLANN_INDEX_KDTREE = 0\n    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n    search_params = dict(checks=50)\n    flann = cv2.FlannBasedMatcher(index_params, search_params)\n    matches = flann.knnMatch(des1, des2, k=2)\n    # Need to draw only good matches, so create a mask\n    matchesMask = [[0,0] for i in xrange(len(matches))]\n    # ratio test as per Lowe\'s paper\n    sumd = 0\n    for i,(m,n) in enumerate(matches):\n        if m.distance < 0.7*n.distance:\n            sumd += m.distance\n            matchesMask[i]=[1,0]\n    return sumd\n\ndef sortShots(shots):\n    maxKeyFrames = -1\n    maxAvgEntropyDiff = -1\n    maxAvgMotion = -1\n    for shot in shots:\n        if shot.keyFrames > maxKeyFrames :\n            maxKeyFrames = shot.keyFrames\n        if shot.avgEntropyDiff > maxAvgEntropyDiff :\n            maxAvgEntropyDif = shot.avgEntropyDiff\n        if shot.avgMotion > maxAvgMotion :\n            maxAvgMotion = shot.avgMotion\n\n    weights = []\n    for shot in shots:\n        weight = shot.keyFrames / float(maxKeyFrames) + shot.avgEntropyDiff / float(maxAvgEntropyDiff) + shot.avgMotion / float(maxAvgMotion)\n        weights.append((shot.shotNumber, weight))\n\n    print \'Unsorted Weights -\', weights\n    weights.sort(reverse=True, key=lambda x: x[1])\n    print \'Sorted Weights -\', weights\n    order = [int(weight[0]) for weight in weights]\n    print \'Order -\', order\n    return order\n\ndef save_keyframes(frame_indices, summary_frames):\n    global skipFrames\n    print ""Saving frame indices""\n    \n    out_file=open(folderName+""/frame_indices_""+str(skipFrames)+"".txt"",\'w\')\n    for idx in frame_indices:\n        out_file.write(str(idx*skipFrames)+\'\\n\')\n    print ""Saved indices""\n\n    print ""Saving frames""\n    cmd=""rm -r ""+folderName+""/_keyframes_/""\n    os.system(cmd)\n    cmd=""mkdir ""+folderName+""_keyframes_""\n    for i,frame in enumerate(summary_frames):\n        cv2.imwrite(str(folderName)+""/_keyframes_/frame%d.jpg""%i, frame)\n    print ""Frames saved""\n\n\ndef main():\n    videoCap = cv2.VideoCapture(fileName)\n    #fps = videoCap.get(cv2.CAP_PROP_FPS)\n    fps=25\n    print ""Frames per second: "", fps\n\n    entropy = []\n    histogramDiff = []\n    histogramRatio = []\n    entropyDiff = []\n    euclideanDistance = []\n    motion = []\n\n    t0 = time.clock()\n\n    i = 0\n    #for a in range(skipFrames):\n    #    success, image = videoCap.read()\n    success, image = videoCap.read()\n    height = len(image)\n    #height = 1080\n    width = len(image[0])\n    #width = 1920\n    # image = image[int(0.25*height):int(0.75*height)]\n    totalPixels = width * height\n    while success:\n        # print width, height\n        grayImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        # plt.imshow(grayImage, cmap = plt.get_cmap(\'gray\'))\n        # plt.show()\n        # print grayImage.shape\n        # print grayImage\n        histogram = cv2.calcHist([grayImage],[0],None,[256],[0,256])\n        # print len(histogram)\n        entropy.append( getEntropy(histogram, totalPixels) )\n        # print entropy[i]\n        colorMoments = getColorMoments(histogram, totalPixels)\n        # print colorMoments\n        if i==0:\n            histogramDiff.append(4000000)\n            histogramRatio.append(200)\n            entropyDiff.append(0)\n            euclideanDistance.append(0)\n            motion.append(0)\n        else:\n            histogramDiff.append( getHistogramDiff(histogram, prevHistogram) )\n            if i==1:\n                histogramRatio.append(1)\n            else:\n                histogramRatio.append( getHistogramRatio(histogramDiff[i], histogramDiff[i-1]) )\n            entropyDiff.append( abs(entropy[i] - entropy[i-1]))\n            euclideanDistance.append( getEuclideanDistance(colorMoments, prevColorMoments) )\n            motion.append( getMotionSift(grayImage, prevGrayImage, i) )\n\n        prevHistogram = histogram\n        prevGrayImage = grayImage\n        prevColorMoments = colorMoments\n\n        i += 1\n        #for a in range(skipFrames):\n        #    success, image = videoCap.read()\n        success, image = videoCap.read()\n        # image = image[int(0.25*height):int(0.75*height)]\n        print i\n        # Uncomment this for breaking early i.e. 100 frames\n        # if i==100:            \n        #     break\n\n    meanEntropyDiff = sum(entropyDiff[1:]) / float(len(entropyDiff)-1)\n    meanHistogramRatio = sum(histogramRatio[1:]) / float(len(histogramRatio)-1)\n    meanEuclideanDistance = sum(euclideanDistance[1:]) / float(len(euclideanDistance)-1)\n\n    thresholdEntropyDiff = meanEntropyDiff\n    thresholdHistogramRatio = meanHistogramRatio\n    thresholdEuclideanDistance = meanEuclideanDistance\n\n    totalFrames = i\n    \n    motionSum = 0\n    entropyDiffSum = 0\n    \n    shots = []\n    shotNumber = 0\n    prevFrame = 0\n    keyFrames = 0\n    keyFramesArray = [0] * totalFrames\n    for i in range(totalFrames):\n        if euclideanDistance[i] > thresholdEuclideanDistance:\n            keyFrames += 1\n            keyFramesArray[i] = 1\n\n        entropyDiffSum += entropyDiff[i]\n        motionSum += motion[i]\n        \n        if entropyDiff[i] > thresholdEntropyDiff and histogramRatio[i] > thresholdHistogramRatio:\n            if i<= prevFrame+25 and shotNumber!=0:\n                currShot = shots[shotNumber-1]\n                numberOfFrames = currShot.endingFrame - currShot.startingFrame + 1\n                newAvgEntropyDiff = ((currShot.avgEntropyDiff * numberOfFrames) + entropyDiffSum) / (i - currShot.startingFrame)\n                newAvgMotion = ((currShot.avgMotion * numberOfFrames) + motionSum) / (i - currShot.startingFrame)\n                shots[shotNumber-1] = Shot(currShot.shotNumber, currShot.startingFrame, i-1, currShot.keyFrames + keyFrames, newAvgEntropyDiff, newAvgMotion)\n            else:\n                avgEntropyDiff = entropyDiffSum / float(i - prevFrame)\n                avgMotion = motionSum / float(i - prevFrame)\n                shots.append(Shot(shotNumber, prevFrame, i-1, keyFrames, avgEntropyDiff, avgMotion))\n                shotNumber += 1\n            \n            keyFrames = 0\n            motionSum = 0\n            entropyDiffSum = 0\n            prevFrame = i\n\n    # Adding the last shot\n    if i!=prevFrame:\n        avgEntropyDiff = entropyDiffSum / float(i - prevFrame)\n        avgMotion = motionSum / float(i - prevFrame)\n        shots.append(Shot(shotNumber, prevFrame, i-1, keyFrames, avgEntropyDiff, avgMotion))\n        shotNumber += 1\n\n    shotsOrder = sortShots(shots)\n\n    trailerFrames = totalFrames/2\n    writeBit = [0] * len(shots)\n    for shotNo in shotsOrder:\n        shot = shots[shotNo]\n        shotFrames = shot.endingFrame - shot.startingFrame + 1\n        if shotFrames < trailerFrames:\n            writeBit[shotNo] = 1\n            trailerFrames -= shotFrames\n        if trailerFrames<=0:\n            break\n\n    print width, height\n    fourcc = cv2.cv.CV_FOURCC(*\'XVID\')\n    outputVideo = cv2.VideoWriter(folderName+\'/summary_sift.avi\',fourcc,fps,(width,height))\n    inputVideo = cv2.VideoCapture(fileName)\n\n    for i in range(len(shots)):\n        if writeBit[i]==1:\n            j = shots[i].startingFrame * skipFrames\n            while j<= shots[i].endingFrame * skipFrames:\n                success, image = inputVideo.read()\n                if success:\n                    print \'writing\'\n                    # plt.imshow(image)\n                    # plt.show()\n                    outputVideo.write(image)\n                else:\n                    print ""Error in reading video""\n                j += 1\n        else:\n            j = shots[i].startingFrame * skipFrames\n            while j<= shots[i].endingFrame * skipFrames:\n                success, image = inputVideo.read()\n                if not success:\n                    print ""Error in reading video""\n                j += 1\n\n    cv2.destroyAllWindows()\n    outputVideo.release()\n\n    print ""Generating summary frames""\n    summary_frames=[]\n    keyFramesIndices=[]\n\n    for i in range(len(keyFramesArray)):\n        if(keyFramesArray[i]==1):\n            cmd=""cp ""+folderName+""/allFrames/image""+str(i)+"".jpg ""+folderName+""/keyFrames/""\n            os.system(cmd)\n            keyFramesIndices.append(i)\n            summary_frames.append(cv2.imread(folderName+""/allFrames/image""+str(i)+"".jpg"",-1))\n    print ""Generated Summary !""\n    save_keyframes(keyFramesIndices,summary_frames)\n\n\n\n    print \'Write Bit -\', writeBit\n    \n    print \'Time taken to run =\', time.clock() - t0, \'seconds\' \n\n    print \'len(Shots) -\' , len(shots), \'\\n\'\n    print \'Shots -\' , shots, \'\\n\'\n    print \'len(Entropy) -\', len(entropy), \'\\n\'\n    #print \'Entropy -\', entropy, \'\\n\'\n    print \'len(HistogramDiff) -\', len(histogramDiff), \'\\n\'\n    #print \'HistogramDiff -\', histogramDiff, \'\\n\'\n    print \'len(HistogramRatio) -\', len(histogramRatio), \'\\n\'\n    #print \'HistogramRatio -\', histogramRatio, \'\\n\'\n    print \'len(EntropyDiff) -\', len(entropyDiff), \'\\n\'\n    #print \'EntropyDiff -\', entropyDiff, \'\\n\'\n    print \'len(EuclideanDistance) -\', len(euclideanDistance), \'\\n\'\n    #print \'EuclideanDistance -\', euclideanDistance, \'\\n\'\n    print \'len(Motion) -\', len(motion), \'\\n\'\n    #print \'Motion -\', motion, \'\\n\'\n    #print \'len(MeanEntropyDiff) -\', len(meanEntropyDiff), \'\\n\'\n    print \'MeanEntropyDiff -\', meanEntropyDiff, \'\\n\'\n    #print \'len(MeanHistogramRatio) -\', len(meanHistogramRatio), \'\\n\'\n    print \'MeanHistogramRatio -\', meanHistogramRatio, \'\\n\'\n    #print \'len(MeanEuclideanDistance) -\', len(meanEuclideanDistance), \'\\n\'\n    print \'MeanEuclideanDistance -\', meanEuclideanDistance, \'\\n\'\n    print \'len(keyFramesArray) - \', len(keyFramesArray), \'\\n\'\n    print \'# of keyFrames - \', sum(keyFramesArray), \'\\n\'\n    print \'keyFramesArray - \', keyFramesArray, \'\\n\'\n\nif __name__ == \'__main__\':\n\t    main()'"
Scripts/Shot_Boundary/evaluate.py,0,"b'import sys\nsys.path.append(""/home/shibhansh/ml/cs771/VideoSummarization/Data/python"")\nimport os\nfrom summe import *\nimport imageio\n# System Arguments\n# Argument 1: Location of the video\n# Argument 2: Location of predicted summary\n# Argument 3: Location to store results : typically of the form \'result_""video_name"".txt\'\n\ndef main():\n\tvideo=sys.argv[1]\n\tvideo_name = sys.argv[1]\n\tvideo_name = video_name.split(\'/\')\n\tvideo_name = video_name[-1].split(\'.\')[0]\n\tprint ""video_name "", video_name\n\tdirectory=sys.argv[2]\n\n\tprint ""Getting frames of summary!""\n\tframe_indices=[int(idx) for idx in open(directory+\'/frame_indices_\'+video_name+\'.txt\',\'r\').read().splitlines()]\n\tprint ""Got the frames\'""\n\n\tvideo=video.split(\'/\')\n\tvideoName=video[len(video)-1].split(\'.\')[0]\n\tprint videoName\n\t\n\tvideo[len(video)-2]=""GT""\n\tHOMEDATA=\'/\'.join(video[0:len(video)-1])\n\tprint HOMEDATA\n\t\n\tf_measure, summary_length=evaluateSummary(frame_indices,videoName,HOMEDATA)\n\tprint ""F-measure %.3f at length %.2f"" %(f_measure, summary_length)\n\n\tout_file=open(sys.argv[3],\'a\')\n\tout_file.write(""F-measure, Summary Length\\n"")\n\tout_file.write(""%f,%f\\n""%(f_measure,summary_length))\n\nif __name__ == \'__main__\':\n\tmain()'"
Scripts/Shot_Boundary/key_frame_extraction.py,3,"b'import sys\nimport imageio\nimport numpy as np\nimport cv2\nimport scipy.io\nimport pywt\nimport os, sys, glob\nfrom scc import strongly_connected_components_tree\n\n# System Arguments\n# Argument 1: Location of the video\n# Argument 2: Sampling rate (k where every kth frame is chosed)\n\n# defines the number of bins for pixel values of each type as used the original work\nnum_bins_H=32\nnum_bins_S=4\nnum_bins_V=2\n\n# manual function to generate histogram on HSV values\ndef generate_histogram_hsv(frame):\n\thsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n\thsv_frame = hsv_frame\n\tglobal num_bins_H, num_bins_S, num_bins_V\n\thist = cv2.calcHist([frame], [0, 1, 2], None, [256/num_bins_H, 256/num_bins_S, 256/num_bins_V],\n\t\t[0, 256, 0, 256, 0, 256])\n\thist = cv2.normalize(hist).flatten()\n\treturn hist;\n\n# function to calculate the distance matrix for bhattacharyya_distance\ndef bhattacharyya_distance(color_histogram):\n\tdistance_matrix=np.zeros((len(color_histogram),len(color_histogram)))\n\tfor i in range(len(color_histogram)):\n\t\ttemp_list = []\n\t\tfor j in range(len(color_histogram)):\n\t\t\tif i != j:\n\t\t\t\tdistance_matrix[i][j] = cv2.compareHist(color_histogram[i],color_histogram[j],cv2.cv.CV_COMP_BHATTACHARYYA)\n\t\t\telse:\n\t\t\t\tdistance_matrix[i][j] = float(""inf"")\n\treturn distance_matrix\n\ndef save_keyframes(frame_indices, summary_frames):\n\tprint ""Saving frame indices""\n\tvideo_name = sys.argv[1]\n\tvideo_name = video_name.split(\'.\')\n\tvideo_name = video_name[0].split(\'/\')\n\tout_file=open(""frame_indices_""+video_name[1]+"".txt"",\'w\')\n\tfor idx in frame_indices:\n\t\tout_file.write(str(idx)+\'\\n\')\n\tprint ""Saved indices""\n\n\t# print ""Saving frames""\n\t# for i,frame in enumerate(summary_frames):\n\t# \tcv2.imwrite(str(sys.argv[7])+""keyframes/frame%d.jpg""%i, frame)\n\t# print ""Frames saved""\n\n\ndef main():\n\tif len(sys.argv) < 2:\n\t\tprint ""Incorrect no. of arguments, Halting !!!!""\n\t\treturn\n\tprint ""Opening video!""\n\n\tvideo=imageio.get_reader(sys.argv[1]);\n\tprint ""Video opened\\nChoosing frames""\n\n\tif len(sys.argv) >=3:\n\t#frame chosen every k frames\n\t\tprint ""choosing frames uniformly""\n\t\tsampling_rate=int(sys.argv[2])\n\t\tframes=[np.array(video.get_data(i*sampling_rate)) for i in range(len(video)/sampling_rate)]\n\n\telse:\n\t\t# delete scenes.txt if it already exists\n\t\tprint ""Detecting different shots""\n\t\tif os.path.exists(\'scenes.txt\'):\n\t\t\tos.remove(\'scenes.txt\')\n\t\t# use the parameter currently set as ""0.4"" to control the no. of frames to be selected\n\t\tvideo_name = sys.argv[1]\n\t\tvideo_name = video_name.split(\'/\')[-1]\n\t\tprint video_name\n\t\tos.chdir(\'input\')\n\t\tcmd = \'ffprobe -show_frames -of compact=p=0 -f lavfi ""movie=\'+str(video_name)+\',select=gt(scene\\,0.1)"">> ../scenes.txt\'\n\t\tos.system(cmd)\n\t\tos.chdir(\'..\')\n\t\tseginfo = \'scenes.txt\'\n\t\tframe_index_list = []\n\t\tfor line in open(seginfo,\'r\'):\n\t\t\tline = line.replace(""|"","" "")\n\t\t\tline = line.replace(""="","" "")\n\t\t\tparts = line.split()\n\t\t\tframe_index_list.append(int(parts[11])) #appending the frame no. in the list of selected frames\n\t\tprint frame_index_list, len(video)\n\t\tframes = []\n\t\tfor i in range(len(frame_index_list)):\n\t\t\tif frame_index_list[i] >= 0 and frame_index_list[i] < len(video):\n\t\t\t\tframes.append(np.array(video.get_data(frame_index_list[i])))\n\n\tif len(frames) <= 0:\n\t\tprint ""unable to detect any shot, Halting !!!!""\n\t\treturn\n\tprint ""Frames chosen: "",len(frame_index_list)\n\t#extracting color features from each representative frame\n\tprint ""Generating Histrograms""\n\tcolor_histogram=[generate_histogram_hsv(frame) for frame in frames]\n\tprint ""Color Histograms generated""\n\n\t#to-do (optional): extract texture features for each frame\n\n\t#calculate distance between each pair of feature histograms\n\tprint ""Evaluating the distance matirix for feature hitograms""\n\tdistance_matrix = bhattacharyya_distance(color_histogram)\n\tprint ""Done Evalualting distance matrix""\n\n\t#constructing NNG (nearest neighbour graph) based of distance_matrix\n\tprint ""Constructing NNG""\n\teps_texture_NN = [None]*len(distance_matrix[0])\n\tfor i in range(0,len(distance_matrix[0])):\n\t\ttemp = float(0)\n\t\tfor j in range(len(distance_matrix[i])):\n\t\t\tif distance_matrix[i][j] >= temp:\n\t\t\t\teps_texture_NN[i] = j\n\t\t\t\ttemp = distance_matrix[i][j]\n\n\t#constructing RNNG(reverse nearest neighbour graph) for the above NNG\n\tprint ""Constructing RNNG""\n\teps_texture_RNN = {}\n\tfor i in range(len(eps_texture_NN)):\n\t\tif eps_texture_NN[i] in eps_texture_RNN.keys():\n\t\t\teps_texture_RNN[eps_texture_NN[i]].append(i)\n\t\telse:\n\t\t\teps_texture_RNN[eps_texture_NN[i]] = [i]\n\t\tif i not in eps_texture_RNN.keys():\n\t\t\teps_texture_RNN[i] = []\n\n\t#calculating the SCCs(strongly connected components) for RNNG\n\tprint ""Finiding the strongly connected components of RNNG""\n\tvertices = [i for i in range(0,len(frames))]\n\tscc_graph = strongly_connected_components_tree(vertices, eps_texture_RNN)\n\n\t#choosing one frame per SCC in summary\n\tprint ""Evaluating final summary""\n\tsummary = []\n\tsummary_frames = []\n\tskim_length = 40\n\tfor scc in scc_graph:\n\t\tframe_to_add = frame_index_list[next(iter(scc))]\n\t\tfor i in range(-skim_length,skim_length):\n\t\t\tif frame_to_add + i > 0 and frame_to_add + i < len(video):\n\t\t\t\tif frame_to_add+i not in summary:\n\t\t\t\t\tsummary.append(frame_to_add+i)\n\t\t\t\t# summary_frames.append(video.get_data(frame_to_add + i))\n\n\t# writing the summary in a file \n\tos.chdir(\'summary\')\n\tsave_keyframes(summary, summary_frames)\n\nif __name__ == \'__main__\':\n\tmain()'"
Scripts/Shot_Boundary/scc.py,0,"b'\ndef strongly_connected_components_tree(vertices, edges):\n    """"""\n    Find the strongly connected components of a directed graph.\n\n    Uses a recursive linear-time algorithm described by Tarjan [2]_ to find all\n    strongly connected components of a directed graph.\n\n    Parameters\n    ----------\n    vertices : iterable\n        A sequence or other iterable of vertices.  Each vertex should be\n        hashable.\n\n    edges : mapping\n        Dictionary (or mapping) that maps each vertex v to an iterable of the\n        vertices w that are linked to v by a directed edge (v, w).\n\n    Returns\n    -------\n    components : iterator\n        An iterator that yields sets of vertices.  Each set produced gives the\n        vertices of one strongly connected component.\n\n    Raises\n    ------\n    RuntimeError\n        If the graph is deep enough that the algorithm exceeds Python\'s\n        recursion limit.\n\n    Notes\n    -----\n    The algorithm has running time proportional to the total number of vertices\n    and edges.  It\'s practical to use this algorithm on graphs with hundreds of\n    thousands of vertices and edges.\n\n    The algorithm is recursive.  Deep graphs may cause Python to exceed its\n    recursion limit.\n\n    `vertices` will be iterated over exactly once, and `edges[v]` will be\n    iterated over exactly once for each vertex `v`.  `edges[v]` is permitted to\n    specify the same vertex multiple times, and it\'s permissible for `edges[v]`\n    to include `v` itself.  (In graph-theoretic terms, loops and multiple edges\n    are permitted.)\n\n    References\n    ----------\n    .. [1] Harold N. Gabow, ""Path-based depth-first search for strong and\n       biconnected components,"" Inf. Process. Lett. 74 (2000) 107--114.\n\n    .. [2] Robert E. Tarjan, ""Depth-first search and linear graph algorithms,""\n       SIAM J.Comput. 1 (2) (1972) 146--160.\n\n    Examples\n    --------\n    Example from Gabow\'s paper [1]_.\n\n    >>> vertices = [1, 2, 3, 4, 5, 6]\n    >>> edges = {1: [2, 3], 2: [3, 4], 3: [], 4: [3, 5], 5: [2, 6], 6: [3, 4]}\n    >>> for scc in strongly_connected_components_tree(vertices, edges):\n    ...     print(scc)\n    ...\n    set([3])\n    set([2, 4, 5, 6])\n    set([1])\n\n    Example from Tarjan\'s paper [2]_.\n\n    >>> vertices = [1, 2, 3, 4, 5, 6, 7, 8]\n    >>> edges = {1: [2], 2: [3, 8], 3: [4, 7], 4: [5],\n    ...          5: [3, 6], 6: [], 7: [4, 6], 8: [1, 7]}\n    >>> for scc in  strongly_connected_components_tree(vertices, edges):\n    ...     print(scc)\n    ...\n    set([6])\n    set([3, 4, 5, 7])\n    set([8, 1, 2])\n\n    """"""\n    identified = set()\n    stack = []\n    index = {}\n    lowlink = {}\n\n    def dfs(v):\n        index[v] = len(stack)\n        stack.append(v)\n        lowlink[v] = index[v]\n\n        for w in edges[v]:\n            if w not in index:\n                # For Python >= 3.3, replace with ""yield from dfs(w)""\n                for scc in dfs(w):\n                    yield scc\n                lowlink[v] = min(lowlink[v], lowlink[w])\n            elif w not in identified:\n                lowlink[v] = min(lowlink[v], lowlink[w])\n\n        if lowlink[v] == index[v]:\n            scc = set(stack[index[v]:])\n            del stack[index[v]:]\n            identified.update(scc)\n            yield scc\n\n    for v in vertices:\n        if v not in index:\n            # For Python >= 3.3, replace with ""yield from dfs(v)""\n            for scc in dfs(v):\n                yield scc'"
Scripts/Shot_Boundary/shot_detect.py,0,"b'#!/usr/bin/python\n""""""\nUsage:\nPut shot_detect.py and input.avi (input avi file) in the same folder and execute shot_detect.py\n\nOutput:\na scene/ subfolder containing clips, \neach taken as a scene\n""""""\n\nimport os, sys, glob\n\n# Get duration of the video file\nif os.path.exists(\'duration.txt\'):\n    os.remove(\'duration.txt\')\n\ncmd = \'ffmpeg -i input.avi 2>&1|grep ""Duration"">> duration.txt\'\nos.system(cmd)\n\nprint \' ================= \'\nprint \'Detecting shot boundaries...\'\nprint \' ================= \'\nif os.path.exists(\'scenes.txt\'):\n\tos.remove(\'scenes.txt\')\n\ncmd = \'ffprobe -show_frames -of compact=p=0 -f lavfi ""movie=input.avi,select=gt(scene\\,0.3)"">> scenes.txt\'\nos.system(cmd)\n\n\n# uncompress the movie file for accurate partition\n# skip this if you do not need high accuracy\nprint \' ================= \'\nprint \'computing raw video for accurate segmentation...\'\nprint \' ================= \'\ncmd = \'ffmpeg -i input.avi -vcodec rawvideo -acodec copy uncompressed.avi\' #pcm_s16le\nos.system(cmd)\n\n# read time stamps for keyframes\nprint \' ================= \'\nprint \'Partitioning video...\'\nprint \' ================= \'\nseginfo = \'scenes.txt\'\nif not os.path.exists(\'scenes\'):\n\tos.mkdir(\'scenes\')\n\ntb = \'0\'\nte = \'0\'\ncount = 1\nf = open(\'scenesFinal.txt\', \'w\')\nfor line in open(seginfo,\'r\'):\n    line = line.replace(""|"","" "")\n    line = line.replace(""="","" "")\n    parts = line.split()\n    te = parts[11] # timestamp\n    te = float(te)\n    fstr = str(count) + \' \' + str(te) + \'\\n\'\n    f.write(fstr)\n    cmd = \'ffmpeg -ss \'\n    tb = float(tb)\n    # start time\n    if (count == 1):\n        tbw = \'00:00:00\' # first shot\n    else:\n        tbh = int(tb/3600)\n        tbm = int((tb%3600)/60)\n        tbs = ((tb%3600)%60)%60\n        tbw = str(tbh) + \':\' + str(tbm) + \':\' + str(tbs)\n    cmd += tbw + \' -i uncompressed.avi -vcodec mpeg4 -acodec copy -t \' # change codecs if necessary\n    # duration\n    td = te - tb\n    tdh = int(td/3600)\n    tdm = int((td%3600)/60)\n    tds = ((td%3600)%60)%60\n    tdw = str(tdh) + \':\' + str(tdm) + \':\' + str(tds)\n    cmd += tdw + \' scenes/\' + \'%(#)04d.avi\' % {""#"":count}\n    os.system(cmd)\n    tb = te\n    count += 1\n\n# last shot\n# cmd = \'ffmpeg -ss \'\n# tbh = int(tb/3600)\n# tbm = int((tb%3600)/60)\n# tbs = ((tb%3600)%60)%60\n# tbw = str(tbh) + \':\' + str(tbm) + \':\' + str(tbs)\n# cmd += tbw + \' -i uncompressed.avi -vcodec mpeg4 -acodec copy -t \' # change codecs if necessary\n\n# dur = \'duration.txt\';\n# for line in open(dur,\'r\'):\n#     line = line.replace("":"","" "")\n#     line = line.replace("","","" "")\n#     parts = line.split()\n#     te = float(parts[1])*3600 + float(parts[2])*60 + float(parts[3])\n\n# td = te-tb\n# tdh = int(td/3600)\n# tdm = int((td%3600)/60)\n# tds = int(((td%3600)%60)%60)\n# tds = ((td%3600)%60)%60\n# tdw = str(tdh) + \':\' + str(tdm) + \':\' + str(tds)\n# cmd += tdw + \' scenes/\' + \'%(#)04d.avi\' % {""#"":count}\n# os.system(cmd)\n# fstr = str(count) + \' \' + str(te) + \'\\n\'\n# f.write(fstr)\n\nos.remove(\'scenes.txt\')\nos.remove(\'uncompressed.avi\')\n'"
Scripts/Uniform_Sampling/uniform.py,0,"b'import sys\nimport cv2\nimport os\n# System Arguments\n# Argument 1: Location of the video\n# Argument 2: Percent of summary required\n# Argument 3: Directory where indices will be saved\n\n#percent of video for summary \npercent=int(sys.argv[2])\n\ndef main():\n\tglobal percent\n\tprint ""Opening Video!""\n\tcapture=cv2.VideoCapture(os.path.abspath(os.path.expanduser(sys.argv[1])))\n\tprint ""Video opened\\nGenerating uniformly sampled summary""\n\tframe_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n\tprint frame_count\n\tsampling_rate=100/percent\n\tif 100%percent:\n\t\tsampling_rate=sampling_rate+1\n\tframe_indices=[i for i in range(frame_count) if i%sampling_rate==0]\n\n\tprint ""Saving frame indices""\n\tout_file=open(sys.argv[3]+""frame_indices_uniform_""+str(percent*frame_count/100)+""_1.txt"",\'w\')\n\tfor idx in frame_indices:\n\t\tout_file.write(str(idx)+\'\\n\')\n\tprint ""Saved indices""\n\nif __name__ == \'__main__\':\n\tmain()'"
Scripts/VSUMM/get_video_feat.py,7,"b'import numpy as np\nimport os \nimport cv2\nimport random\n\'\'\' PATHS \'\'\' \nHOMEVIDEOS=\'videos/\'\nfrom vgg16 import VGG16\nfrom keras.preprocessing import image\nfrom imagenet_utils import preprocess_input\nfrom keras.models import Model\nfrom sklearn.decomposition import PCA\n\ndef get_cnn_feat(frames_raw):\n    frames=[]\n    pca=PCA(n_components=500)\n    for im in frames_raw:\n        print im.shape\n        im = cv2.resize(im, (224, 224)).astype(np.float32)\n        im[:,:,0] -= 103.939\n        im[:,:,1] -= 116.779\n        im[:,:,2] -= 123.68\n        # print im.shape\n        im = np.expand_dims(im, axis=0)\n        print im.shape\n        frames.append(np.asarray(im))\n    frames = np.array(frames)\n    print frames.shape\n\n    base_model = VGG16(weights=\'imagenet\', include_top=True)\n    model = Model(input=base_model.input, output=base_model.get_layer(\'fc2\').output)\n\n    i = 0\n    features = np.ndarray((frames.shape[0],4096),dtype=np.float32)\n    for x in frames:\n        print x.shape\n        features[i,:] = model.predict(x)\n        i+=1\n    return pca.fit_transform(features)\n\ndef get_color_hist(frames_raw, num_bins):\n    print ""Generating linear Histrograms using OpenCV""\n    channels=[\'b\',\'g\',\'r\']\n    \n    hist=[]\n    for frame in frames_raw:\n        feature_value=[cv2.calcHist([frame],[i],None,[int(num_bins)],[0,256]) for i,col in enumerate(channels)]\n        hist.append(np.asarray(feature_value).flatten())\n    \n    hist=np.asarray(hist)\n    print ""Done generating!""\n    print ""Shape of histogram: "" + str(hist.shape)\n    \n    return hist'"
Scripts/VSUMM/imagenet_utils.py,0,"b""import numpy as np\nimport json\n\nfrom keras.utils.data_utils import get_file\nfrom keras import backend as K\n\nCLASS_INDEX = None\nCLASS_INDEX_PATH = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n\n\ndef preprocess_input(x, dim_ordering='default'):\n    if dim_ordering == 'default':\n        dim_ordering = K.image_dim_ordering()\n    assert dim_ordering in {'tf', 'th'}\n\n    if dim_ordering == 'th':\n        x[:, 0, :, :] -= 103.939\n        x[:, 1, :, :] -= 116.779\n        x[:, 2, :, :] -= 123.68\n        # 'RGB'->'BGR'\n        x = x[:, ::-1, :, :]\n    else:\n        x[:, :, :, 0] -= 103.939\n        x[:, :, :, 1] -= 116.779\n        x[:, :, :, 2] -= 123.68\n        # 'RGB'->'BGR'\n        x = x[:, :, :, ::-1]\n    return x\n\n\ndef decode_predictions(preds, top=5):\n    global CLASS_INDEX\n    if len(preds.shape) != 2 or preds.shape[1] != 1000:\n        raise ValueError('`decode_predictions` expects '\n                         'a batch of predictions '\n                         '(i.e. a 2D array of shape (samples, 1000)). '\n                         'Found array with shape: ' + str(preds.shape))\n    if CLASS_INDEX is None:\n        fpath = get_file('imagenet_class_index.json',\n                         CLASS_INDEX_PATH,\n                         cache_subdir='models')\n        CLASS_INDEX = json.load(open(fpath))\n    results = []\n    for pred in preds:\n        top_indices = pred.argsort()[-top:][::-1]\n        result = [tuple(CLASS_INDEX[str(i)]) + (pred[i],) for i in top_indices]\n        results.append(result)\n    return results\n"""
Scripts/VSUMM/vgg16.py,1,"b'# -*- coding: utf-8 -*-\n\'\'\'VGG16 model for Keras.\n\n# Reference:\n\n- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n\n\'\'\'\nfrom __future__ import print_function\n\nimport numpy as np\nimport warnings\n\nfrom keras.models import Model\nfrom keras.layers import Flatten, Dense, Input\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.preprocessing import image\nfrom keras.utils.layer_utils import convert_all_kernels_in_model\nfrom keras.utils.data_utils import get_file\nfrom keras import backend as K\nfrom imagenet_utils import decode_predictions, preprocess_input\n\n\nTH_WEIGHTS_PATH = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels.h5\'\nTF_WEIGHTS_PATH = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\'\nTH_WEIGHTS_PATH_NO_TOP = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels_notop.h5\'\nTF_WEIGHTS_PATH_NO_TOP = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\'\n\n\ndef VGG16(include_top=True, weights=\'imagenet\',\n          input_tensor=None):\n    \'\'\'Instantiate the VGG16 architecture,\n    optionally loading weights pre-trained\n    on ImageNet. Note that when using TensorFlow,\n    for best performance you should set\n    `image_dim_ordering=""tf""` in your Keras config\n    at ~/.keras/keras.json.\n\n    The model and the weights are compatible with both\n    TensorFlow and Theano. The dimension ordering\n    convention used by the model is the one\n    specified in your Keras config file.\n\n    # Arguments\n        include_top: whether to include the 3 fully-connected\n            layers at the top of the network.\n        weights: one of `None` (random initialization)\n            or ""imagenet"" (pre-training on ImageNet).\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n\n    # Returns\n        A Keras model instance.\n    \'\'\'\n    if weights not in {\'imagenet\', None}:\n        raise ValueError(\'The `weights` argument should be either \'\n                         \'`None` (random initialization) or `imagenet` \'\n                         \'(pre-training on ImageNet).\')\n    # Determine proper input shape\n    if K.image_dim_ordering() == \'th\':\n        if include_top:\n            input_shape = (3, 224, 224)\n        else:\n            input_shape = (3, None, None)\n    else:\n        if include_top:\n            input_shape = (224, 224, 3)\n        else:\n            input_shape = (None, None, 3)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor)\n        else:\n            img_input = input_tensor\n    # Block 1\n    x = Convolution2D(64, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block1_conv1\')(img_input)\n    x = Convolution2D(64, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block1_conv2\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block1_pool\')(x)\n\n    # Block 2\n    x = Convolution2D(128, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block2_conv1\')(x)\n    x = Convolution2D(128, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block2_conv2\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block2_pool\')(x)\n\n    # Block 3\n    x = Convolution2D(256, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block3_conv1\')(x)\n    x = Convolution2D(256, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block3_conv2\')(x)\n    x = Convolution2D(256, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block3_conv3\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block3_pool\')(x)\n\n    # Block 4\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block4_conv1\')(x)\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block4_conv2\')(x)\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block4_conv3\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block4_pool\')(x)\n\n    # Block 5\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block5_conv1\')(x)\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block5_conv2\')(x)\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block5_conv3\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block5_pool\')(x)\n\n    if include_top:\n        # Classification block\n        x = Flatten(name=\'flatten\')(x)\n        x = Dense(4096, activation=\'relu\', name=\'fc1\')(x)\n        x = Dense(4096, activation=\'relu\', name=\'fc2\')(x)\n        x = Dense(1000, activation=\'softmax\', name=\'predictions\')(x)\n\n    # Create model\n    model = Model(img_input, x)\n\n    # load weights\n    if weights == \'imagenet\':\n        print(\'K.image_dim_ordering:\', K.image_dim_ordering())\n        if K.image_dim_ordering() == \'th\':\n            if include_top:\n                weights_path = get_file(\'vgg16_weights_th_dim_ordering_th_kernels.h5\',\n                                        TH_WEIGHTS_PATH,\n                                        cache_subdir=\'models\')\n            else:\n                weights_path = get_file(\'vgg16_weights_th_dim_ordering_th_kernels_notop.h5\',\n                                        TH_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir=\'models\')\n            model.load_weights(weights_path)\n            if K.backend() == \'tensorflow\':\n                warnings.warn(\'You are using the TensorFlow backend, yet you \'\n                              \'are using the Theano \'\n                              \'image dimension ordering convention \'\n                              \'(`image_dim_ordering=""th""`). \'\n                              \'For best performance, set \'\n                              \'`image_dim_ordering=""tf""` in \'\n                              \'your Keras config \'\n                              \'at ~/.keras/keras.json.\')\n                convert_all_kernels_in_model(model)\n        else:\n            if include_top:\n                weights_path = get_file(\'vgg16_weights_tf_dim_ordering_tf_kernels.h5\',\n                                        TF_WEIGHTS_PATH,\n                                        cache_subdir=\'models\')\n            else:\n                weights_path = get_file(\'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\',\n                                        TF_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir=\'models\')\n            model.load_weights(weights_path)\n            if K.backend() == \'theano\':\n                convert_all_kernels_in_model(model)\n    return model\n\n\nif __name__ == \'__main__\':\n    model = VGG16(include_top=True, weights=\'imagenet\')\n\n    img_path = \'elephant.jpg\'\n    img = image.load_img(img_path, target_size=(224, 224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    print(\'Input image shape:\', x.shape)\n\n    preds = model.predict(x)\n    print(\'Predicted:\', decode_predictions(preds))\n'"
Scripts/VSUMM/vsumm.py,5,"b'# k means clustering to generate video summary\nimport sys\nimport imageio\nimport numpy as np\nimport cv2\nimport scipy.io\n\n# k-means\nfrom sklearn.cluster import KMeans\n\n# System Arguments\n# Argument 1: Location of the video\n# Argument 2: Sampling rate (k where every kth frame is chosed)\n# Argument 3: Percentage of frames in the keyframe summany (Hence the number of cluster)\n# NOTE: pass the number of clusters as -1 to choose 1/50 the number of frames in original video\n# Only valid for SumMe dataset\n\n# optional arguments \n# Argument 4: 1: if 3D Histograms need to be generated and clustered, else 0\n# Argument 5: 1: if want to save keyframes \n# Argument 6: 1: if want to save the frame indices\n# Argument 7: directory where keyframes will be saved\n\n# defines the number of bins for pixel values of each type {r,g,b}\nnum_bins=16\n\n# size of values in each bin\nrange_per_bin=256/num_bins\n\n#frame chosen every k frames\nsampling_rate=int(sys.argv[2])\n\n# number of centroids\npercent=int(sys.argv[3])\n\n# globalizing\nnum_centroids=0\n\n# manual function to generate a 3D tensor representing histogram\n# extremely slow\ndef generate_histogram(frame):\n\tprint ""Received frame""\n\tglobal num_bins, sampling_rate, num_centroids\n\thistogram=np.zeros((num_bins,num_bins,num_bins))\n\tfor row in range(len(frame)):\n\t\tfor col in range(len(frame[row])):\n\t\t\tr,g,b=frame[row][col]\n\t\t\thistogram[r/num_bins][g/num_bins][b/num_bins]+=1;\n\treturn histogram\n\tprint ""Generated Histogram""\n\ndef save_keyframes(frame_indices, summary_frames):\n\tglobal sampling_rate, num_centroids\n\tif int(sys.argv[6])==1:\n\t\tprint ""Saving frame indices""\n\t\tout_file=open(sys.argv[7]+""frame_indices_""+str(num_centroids)+""_""+str(sampling_rate)+"".txt"",\'w\')\n\t\tfor idx in frame_indices:\n\t\t\tout_file.write(str(idx*sampling_rate)+\'\\n\')\n\t\tprint ""Saved indices""\n\n\tif int(sys.argv[5])==1:\n\t\tprint ""Saving frames""\n\t\tfor i,frame in enumerate(summary_frames):\n\t\t\tcv2.imwrite(str(sys.argv[7])+""keyframes/frame%d.jpg""%i, frame)\n\t\tprint ""Frames saved""\n\ndef main():\n\tglobal num_bins, sampling_rate, percent, num_centroids\n\tprint ""Opening video!""\n\tvideo=imageio.get_reader(sys.argv[1]);\n\tprint ""Video opened\\nChoosing frames""\n\t#choosing the subset of frames from which video summary will be generateed\n\tframes=[video.get_data(i*sampling_rate) for i in range(len(video)/sampling_rate)]\n\tprint ""Frames chosen""\n\tprint ""Length of video %d"" % len(video)\n\n\t# converting percentage to actual number\n\tnum_centroids=int(percent*len(video)/100)\t\n\tif (len(video)/sampling_rate) < num_centroids:\n\t\tprint ""Samples too less to generate such a large summary""\n\t\tprint ""Changing to maximum possible centroids""\n\t\tnum_centroids=len(video)/sampling_rate\n\t\t\n\tif len(sys.argv)>4 and int(sys.argv[4])==1:\n\t\tprint ""Generating 3D Tensor Histrograms""\n\t\t#manually generated histogram\n\t\tcolor_histogram=[generate_histogram(frame) for frame in frames]\n\t\tprint ""Color Histograms generated""\n\n\t#opencv: generates 3 histograms corresponding to each channel for each frame\n\tprint ""Generating linear Histrograms using OpenCV""\n\tchannels=[\'b\',\'g\',\'r\']\n\thist=[]\n\tfor frame in frames:\n\t\tfeature_value=[cv2.calcHist([frame],[i],None,[num_bins],[0,256]) for i,col in enumerate(channels)]\n\t\thist.append(np.asarray(feature_value).flatten())\n\thist=np.asarray(hist)\n\tprint ""Done generating!""\n\tprint ""Shape of histogram: "" + str(hist.shape)\n\n\t# clustering: defaults to using the histogram generated by OpenCV\n\tprint ""Clustering""\n\n\t# choose number of centroids for clustering from user required frames (specified in GT folder for each video)\n\tif percent==-1:\n\t\tvideo_address=sys.argv[1].split(\'/\')\n\t\tgt_file=video_address[len(video_address)-1].split(\'.\')[0]+\'.mat\'\n\t\tvideo_address[len(video_address)-1]=gt_file\n\t\tvideo_address[len(video_address)-2]=\'GT\'\n\t\tgt_file=\'/\'.join(video_address)\n\t\tnum_frames=int(scipy.io.loadmat(gt_file).get(\'user_score\').shape[0])\n\t\t# automatic summary sizing: summary assumed to be 1/100 of original video\n\t\tnum_centroids=int(0.1*num_frames)\n\n\tkmeans=KMeans(n_clusters=num_centroids).fit(hist)\n\tprint ""Done Clustering!""\n\n\tprint ""Generating summary frames""\n\tsummary_frames=[]\n\n\t# transforms into cluster-distance space (n_cluster dimensional)\n\thist_transform=kmeans.transform(hist)\n\tframe_indices=[]\n\tfor cluster in range(hist_transform.shape[1]):\n\t\tprint ""Frame number: %d"" % (np.argmin(hist_transform.T[cluster])*sampling_rate)\n\t\tframe_indices.append(np.argmin(hist_transform.T[cluster]))\n\t\n\t# frames generated in sequence from original video\n\tframe_indices=sorted(frame_indices)\n\tsummary_frames=[frames[i] for i in frame_indices]\n\tprint ""Generated summary""\n\n\tif len(sys.argv)>5 and (int(sys.argv[5])==1 or int(sys.argv[6])==1):\n\t\tsave_keyframes(frame_indices, summary_frames)\n\t\t\nif __name__ == \'__main__\':\n\tmain()'"
Scripts/VSUMM/vsumm_feat.py,5,"b'# generic VSUMM to test with different features\n# k means clustering to generate video summary\nimport sys\n\nimport numpy as np\nimport cv2\nimport scipy.io\nimport os\n\n# k-means\nfrom sklearn.cluster import KMeans\nfrom get_video_feat import *\n\n# System Arguments\n# Argument 1: Location of the video\n# Argument 2: Sampling rate (k where every kth frame is chosed)\n# Argument 3: Percentage of frames in the keyframe summany (Hence the number of cluster)\n# NOTE: pass the number of clusters as -1 to choose 1/50 the number of frames in original video\n# Only valid for SumMe dataset\n\n# optional arguments \n# Argument 4: 1: if 3D Histograms need to be generated and clustered, else 0\n# Argument 5: 1: if want to save keyframes \n# Argument 6: 1: if want to save the frame indices\n# Argument 7: directory where keyframes will be saved\n# Argument 8: Name of the features used\n\n# frame chosen every k frames\nsampling_rate=int(sys.argv[2])\n\n# percent of video for summary\npercent=int(sys.argv[3])\n\n# globalizing\nnum_centroids=0\n\ndef save_keyframes(frame_indices, summary_frames):\n\tglobal sampling_rate, num_centroids\n\tif int(sys.argv[6])==1:\n\t\tprint ""Saving frame indices""\n\t\tout_file=open(sys.argv[7]+""frame_indices_""+sys.argv[8]+\'_\'+str(num_centroids)+""_""+str(sampling_rate)+"".txt"",\'w\')\n\t\tfor idx in frame_indices:\n\t\t\tout_file.write(str(idx*sampling_rate)+\'\\n\')\n\t\tprint ""Saved indices""\n\n\tif int(sys.argv[5])==1:\n\t\tprint ""Saving frames""\n\t\tfor i,frame in enumerate(summary_frames):\n\t\t\tcv2.imwrite(str(sys.argv[7])+""keyframes/frame%d.jpg""%i, frame)\n        print ""Frames saved""\n\ndef main():\n    global num_bins, sampling_rate, num_centroids, percent\n    print ""Opening video!""\n    capture = cv2.VideoCapture(os.path.abspath(os.path.expanduser(sys.argv[1])))\n    print ""Video opened\\nChoosing frames""\n\t\n    #choosing the subset of frames from which video summary will be generateed\n    frames = []\n    i=0\n    while(capture.isOpened()):\n        if i%sampling_rate==0:\n            capture.set(1,i)\n            # print i\n            ret, frame = capture.read()\n            if frame is None :\n                break\n            #im = np.expand_dims(im, axis=0) #convert to (1, width, height, depth)\n            # print frame.shape\n            frames.append(np.asarray(frame))\n        i+=1\n    frames = np.array(frames)#convert to (num_frames, width, height, depth)\n\n    print ""Frames chosen""\n    print ""Length of video %d"" % frames.shape[0]\n    \n    # REPLACE WITH APPROPRIATE FEATURES\n    features = get_cnn_feat(frames)\n    print ""Shape of features "" + str(features.shape)\n    \n    # clustering: defaults to using the features\n    print ""Clustering""\n\n    # converting percentage to actual number\n    num_centroids=int(percent*frames.shape[0]*sampling_rate/100)   \n    \n\t# choose number of centroids for clustering from user required frames (specified in GT folder for each video)\n    if percent==-1:\n    \tvideo_address=sys.argv[1].split(\'/\')\n    \tgt_file=video_address[len(video_address)-1].split(\'.\')[0]+\'.mat\'\n    \tvideo_address[len(video_address)-1]=gt_file\n    \tvideo_address[len(video_address)-2]=\'GT\'\n    \tgt_file=\'/\'.join(video_address)\n    \tnum_frames=int(scipy.io.loadmat(gt_file).get(\'user_score\').shape[0])\n    \t# automatic summary sizing: summary assumed to be 1/10 of original video\n    \tnum_centroids=int(0.1*num_frames)\n\n    if len(frames) < num_centroids:\n        print ""Samples too less to generate such a large summary""\n        print ""Changing to maximum possible centroids""\n        num_centroids=frames.shape[0]\n\n    kmeans=KMeans(n_clusters=num_centroids).fit(features)\n    print ""Done Clustering!""\n\n    print ""Generating summary frames""\n    summary_frames=[]\n\n    # transforms into cluster-distance space (n_cluster dimensional)\n    features_transform=kmeans.transform(features)\n    frame_indices=[]\n    for cluster in range(features_transform.shape[1]):\n\t   print ""Frame number: %d"" % (np.argmin(features_transform.T[cluster])*sampling_rate)\n\t   frame_indices.append(np.argmin(features_transform.T[cluster]))\n\t\n\t# frames generated in sequence from original video\n    frame_indices=sorted(frame_indices)\n    summary_frames=[frames[i] for i in frame_indices]\n    print ""Generated summary""\n    \n    if len(sys.argv)>5 and (int(sys.argv[5])==1 or int(sys.argv[6])==1):\n        save_keyframes(frame_indices, summary_frames)\n\nif __name__ == \'__main__\':\n    main()'"
Scripts/VSUMM_Skims/vsumm_skim.py,4,"b'import sys\nsys.path.append(""../VSUMM"")\nimport os\nimport scipy.io\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom get_video_feat import *\n\n# System Arguments\n# Argument 1: Location of the video\n# Argument 2: Sampling rate (every kth frame is chosen)\n# Argument 3: Percentage length of video summary\n# Argument 4: Directory where frame indices will be saved\n# Argument 5: Name of the features used\n\n# frame chosen every k frames\nsampling_rate=int(sys.argv[2])\n\n# percentage of video for summary\npercent=int(sys.argv[3])\n\n# skim length per chosen frames in second\n# which will be adjusted according to the fps of the video\nskim_length=1.8\n\ndef main():\n\tglobal sampling_rate, percent, skim_length\n\tprint ""Opening Video!""\n\tvideo=cv2.VideoCapture(os.path.abspath(os.path.expanduser(sys.argv[1])))\n\tprint ""Video opened\\nChoosing frames""\n\t\n\tfps=int(video.get(cv2.CAP_PROP_FPS))\n\tframe_count=int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n\tskim_frames_length=fps*skim_length\n\t\n\tframes = []\n\ti=0\n\twhile(video.isOpened()):\n\t\tif i%sampling_rate==0:\n\t\t\tvideo.set(1,i)\n\t\t\t# print i\n\t\t\tret, frame = video.read()\n\t\t\tif frame is None :\n\t\t\t\tbreak\n\t\t\t#im = np.expand_dims(im, axis=0) #convert to (1, width, height, depth)\n\t\t\t# print frame.shape\n\t\t\tframes.append(np.asarray(frame))\n\t\ti+=1\n\tframes = np.array(frames)#convert to (num_frames, width, height, depth)\n\t\n\tprint ""Frames chosen""\n\tprint ""Length of video %d"" % frames.shape[0]\n\t\n\t# REPLACE WITH APPROPRIATE FEATURES\n\tfeatures=get_color_hist(frames,16)\n\tprint ""Shape of features: "" + str(features.shape)\n\n\t# clustering: defaults to using the features\n\tprint ""Clustering""\n\n\t# Choosing number of centers for clustering\n\tnum_centroids=int(percent*frame_count/skim_frames_length/100)+1\n\tprint ""Number of clusters: ""+str(num_centroids)\n\n\tkmeans=KMeans(n_clusters=num_centroids).fit(features)\n\tprint ""Done Clustering!""\n\t\n\tcentres=[]\n\tfeatures_transform=kmeans.transform(features)\n\tfor cluster in range(features_transform.shape[1]):\n\t\tcentres.append(np.argmin(features_transform.T[cluster]))\n\n\tcentres=sorted(centres)\n\tframes_indices=[]\n\tfor centre in centres:\n\t\tprint centre\n\t\tfor idx in range(max(int(centre*sampling_rate-skim_frames_length/2),0),min(int(centre*sampling_rate+skim_frames_length/2)+1,frame_count)):\n\t\t\tframes_indices.append(idx)\n\tframes_indices=sorted(set(frames_indices))\n\n\tprint ""Saving frame indices""\n\tout_file=open(sys.argv[4]+""frame_indices_""+sys.argv[5]+\'_\'+str(num_centroids)+""_""+str(sampling_rate)+"".txt"",\'w\')\n\tfor idx in frames_indices:\n\t\tout_file.write(str(idx)+\'\\n\')\n\tprint ""Saved indices""\n\nif __name__ == \'__main__\':\n\tmain()'"
