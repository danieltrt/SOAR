file_path,api_count,code
tfscripts/Tforcedp.py,2,"b'import numpy as np\nimport random\nimport datetime\nfrom tfutils import *\nfrom tensorforce.environments.environment import Environment\n\n###############################################################################################################\n##  In reinforcement learning, there is an agent - who interacts with\n##  a given Environment. Environment accepts current and future step and\n##  returns reward value back to the agent\n##\n##  Env object has the following instance variables\n##          -- entity2id_ a dictionary populated from entity2id.txt file\n##          -- relation2id_ a dictionary populated from relation2id.txt file\n##          -- relations a vector populated from the relations in relation2id.txt file\n##          -- entity2vec populated from entity2vec.bern file\n##          -- relation2vec populated from relation2vec.bern file\n##          -- kb knowledge graph for path finding. This stores all relations from\n##                  kb_env_rl.txt expect one corresponding to input task and its inverse.\n##                  Relation from state to A->B as R and the relation from B->A as R_inverse\n##          -- die number of times the agent chose the wrong path\n##\n##  Env is defined as class containing following methods\n##  -- Initialize init()\n##          Reads the entity2id.txt and relation2id.txt files and populates the 2 dictionaries self.entity2id_\n##          and self.relation2id_variables, stores the relations from relation2id.txt in self.relation,\n##          populates entity2vec and relation2vec from corresponding .bern files. Populates the kb object with\n##          relations (corresponding to input task) from kb_env_rl.txt. Sets variable die to 0.\n##\n##  -- execute()\n##          Called during learning reinforcement phases\n##          state: is [current_position, target_position]\n##\t\t    action: an integer\n##          return: (reward, [new_postion, target_position], done)\n##\n##  -- states()\n##          takes as input current or next_state\n##          returns the current and (target-current) positions as an array\n##\n##  -- actions()\n##          takes the entity id as input\n##          Returns set of valid actions for a given state from self.kb\n##          in form of an array\n##\n##  -- reset()\n##          Called from Runner before begining training\n##\n##  -- close()\n##          Set environment to None\n\n##\n##\n##\n############################################################################################################\n\n\nclass DPEnv(Environment):\n    def __init__(self, relationPath, graphPath, task=None):\n        logger.info(""Initalizing DeepPath TensorForce Env"")\n        self.task = task\n        self.graphPath = graphPath\n        self.relationPath = relationPath\n        f1 = open(dataPath + \'entity2id.txt\')\n        f2 = open(dataPath + \'relation2id.txt\')\n        self.entity2id = f1.readlines()\n        self.relation2id = f2.readlines()\n        f1.close()\n        f2.close()\n        self.entity2id_ = {}\n        self.relation2id_ = {}\n        self.relations = []\n        for line in self.entity2id:\n            self.entity2id_[line.split()[0]] = int(line.split()[1])\n        for line in self.relation2id:\n            self.relation2id_[line.split()[0]] = int(line.split()[1])\n            self.relations.append(line.split()[0])\n        self.entity2vec = np.loadtxt(dataPath + \'entity2vec.bern\')\n        self.relation2vec = np.loadtxt(dataPath + \'relation2vec.bern\')\n\n        self.path = []\n        self.path_relations = []\n\n        self.action = dict(num_actions=action_space, type=\'int\')\n\n        # Knowledge Graph for path finding\n        f = open(dataPath + \'kb_env_rl.txt\')\n        kb_all = f.readlines()\n        f.close()\n\n        self.kb = []\n        for i in range(len(task)):\n            if task[i] != None:\n                relation = task[i].split()[2]\n                for line in kb_all:\n                    rel = line.split()[2]\n                    if rel != relation and rel != relation + \'_inv\':\n                        self.kb.append(line)\n\n        logger.info(""KG loaded"")\n        print(""Knowledge Graph loaded"")\n        self.reset()\n\n\n\n    def __str__(self):\n        return \'DeepPath Env({}:{})\'.format(self.relationPath, self.graphPath)\n\n    def close(self):\n        self.env = None\n\n\n    def reset(self):\n        logger.info (""In reset: %s"", str(datetime.datetime.now()))\n        sample = self.task[random.randint(0, len(self.task)-1)].split()\n        self.localstate = [0, 0]\n        self.localstate[0] = self.entity2id_[sample[0]]\n        self.localstate[1] = self.entity2id_[sample[1]]\n        self.state = self.localstate\n        return self.state\n\n    def execute(self, actions):\n        \'\'\'\n        This function process the interact from the agent\n        state: is [current_position, target_position]\n        action: an integer from\n        return: ([new_postion, target_position], done. reward)\n        \'\'\'\n        state = self.state\n        logger.info(""In execute(): {ss}"".format(ss=self.state))\n        print(""Agent proessing. In execute(): states - "", self.state)\n\n        reward = 0\n        done = 0  # Whether the episode has finished\n        curr_pos = state[0]\n        target_pos = state[1]\n        chosen_relation = self.relations[actions]\n        choices = []\n        for line in self.kb:\n            triple = line.rsplit()\n            e1_idx = self.entity2id_[triple[0]]\n\n            if curr_pos == e1_idx and triple[2] == chosen_relation and triple[1] in self.entity2id_:\n                choices.append(triple)\n\n        if len(choices) == 0:\n            logger.info(""Incorrect Path. End episode"")\n            reward = -1\n            next_state = state  # stay in the initial state\n            done = 1\n            return (next_state, done,reward)\n        else:  # find a valid step\n            logger.info(""Found a valid step, check action"")\n            path = random.choice(choices)\n            self.path.append(path[2] + \' -> \' + path[1])\n            self.path_relations.append(path[2])\n\n            new_pos = self.entity2id_[path[1]]\n            new_state = [new_pos, target_pos]\n            self.state = new_state\n\n            if new_pos == target_pos:\n                print \'Yay, Found a path:\', self.path\n                logger.info(""Yay, Found a path: {p}"".format(p=self.path))\n                done = 1\n                reward = 1\n                new_state = None\n            return (new_state, done, reward)\n\n\n    def states(self, idx_list=None):\n        return dict(shape=state_dim, type=\'float\')\n\n\n\n    def actions(self, entityID=0):\n        return dict(num_actions=action_space, type=\'int\')\n\n'"
tfscripts/deepPath_main.py,0,"b'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nfrom tfutils import *\nimport os\n\nfrom tensorforce.agents import VPGAgent\nfrom tensorforce.agents import DQNAgent\nfrom tensorforce.execution import Runner\nfrom Tforcedp import DPEnv\n\n\n###############################################################################################################\n##  This is the main function of the tensorforce\n##  implementation of the Deep Path program\n##\n##  The program takes following argument as parameters\n##\n##  -r  or --relation = relation name\n##  -e  or --episodes = Number of episodes\n##                      default : 500\n##  -a  or --agent    = Agent Name\n##                      default : vpg\n##                      allowed values : vpg or dqn (lowercase)\n##  -D  or --debug    = Show Debug Logs\n##                      default : False\n##\n##\n##\n##\n##\n##\n\n############################################################################################################\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-r\', \'--relation\', help=""Number of episodes"")\n    parser.add_argument(\'-e\', \'--episodes\', type=int, default=500, help=""Number of episodes"")\n    parser.add_argument(\'-a\', \'--agent\', type=str, default=\'vpg\', help=""VPG or DQN Agent"")\n    parser.add_argument(\'-D\', \'--debug\', action=\'store_true\', default=False, help=""Show debug outputs"")\n\n    args = parser.parse_args()\n    print(""Running DeepPath-TensorForce"")\n\n\n    if args.relation:  # relation is defined\n        relation = args.relation\n        logger.info(\'Relation set to %s\', relation)\n    else:\n        logger.error(""Error : No Relation name provided!"")\n        return\n\n    graphPath = dataPath + \'tasks/\' + relation + \'/\' + \'graph.txt\'\n    relationPath = dataPath + \'tasks/\' + relation + \'/\' + \'train_pos\'\n    if not os.path.exists(relationPath):\n        logger.info(\'Incorrect relation specified  %s\', relation)\n        print(\'Incorrect relation specified \', relation)\n    f = open(relationPath)\n    data = f.readlines()\n    f.close()\n\n\n    # Initialize the DeePath Environment class\n    environment = DPEnv(graphPath, relationPath, task=data)\n\n    network_spec = [\n        dict(type=\'dense\', size=512, activation=\'relu\'),\n        dict(type=\'dense\', size=1024, activation=\'relu\')\n    ]\n\n    step_optimizer = dict(type=\'adam\', learning_rate=1e-3)\n    agent = None\n\n    if args.agent == \'vpg\':\n        logger.info(\'Initializing VPGAgent\')\n        agent = VPGAgent(states_spec=dict(shape=state_dim, type=\'float\'),\n                         actions_spec=dict(num_actions=action_space, type=\'int\'),\n                         network_spec=network_spec, optimizer=step_optimizer,\n                         discount=0.99, batch_size=1000)\n    elif args.agent == \'dqn\':\n        logger.info(\'Initializing DQNAgent\')\n        agent = DQNAgent(states_spec=dict(shape=state_dim, type=\'float\'),\n                         actions_spec=dict(num_actions=action_space, type=\'int\'),\n                         network_spec=network_spec, optimizer=step_optimizer,\n                         discount=0.99, batch_size=1000)\n\n    logger.info(\'Initializing Runner\')\n    runner = Runner(agent=agent, environment=environment)\n\n\n    report_episodes = args.episodes / 50  # default episodes = 500\n\n    def episode_finished(r):\n        if r.episode % report_episodes == 0:\n            logger.info(\n                ""Finished episode {ep} after {ts} timesteps. Steps Per Second "".format(ep=r.episode, ts=r.timestep))\n            logger.info(""Episode reward: {}"".format(r.episode_rewards[-1]))\n            logger.info(""Average of last 50 rewards: {}"".format(sum(r.episode_rewards[-50:]) / 50))\n            logger.info(""Average of last 100 rewards: {}"".format(sum(r.episode_rewards[-100:]) / 100))\n        return True\n\n    logger.info(""Starting {agent} for Environment \'{env}\'"".format(agent=agent, env=environment))\n    print(""Starting {agent} for Environment"".format(agent=agent))\n    runner.run(episodes=args.episodes, max_episode_timesteps=1, episode_finished=episode_finished)\n    logger.info(""Learning finished. Total episodes: {ep}"".format(ep=runner.episode))\n    print(""Learning finished. Total episodes: {ep}"".format(ep=runner.episode))\n    environment.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tfscripts/tfutils.py,0,"b""import logging\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n# create a file handler\nhandler = logging.FileHandler('deepPath_tensorforce.log')\nhandler.setLevel(logging.DEBUG)\n\n# create a logging format\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nhandler.setFormatter(formatter)\n\n# add the handlers to the logger\nlogger.addHandler(handler)\n\naction_space = 400\nstate_dim = 2\ndataPath = '../NELL-995/'"""
