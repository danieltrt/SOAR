file_path,api_count,code
ars.py,5,"b'## Augmented random Search #####\n\n#### This is a basic Psuedo-code to Implementation of  Random Search AI#####\n#### This does NOT include Contributions to it for improvements using Normalization ###\n\n###Pull Requests for improvements on the code and further contributions is welcomed ###\n\n\n# Importing the required libraries\nimport os\nimport numpy as np\nimport gym\nfrom gym import wrappers\nimport pybullet_envs           ##custom pybullet Simulation library\n\n# Setting up the Hyper Parameters\n\nclass Hyperparameters():\n\n    def __init__(self):\n        self.nb_steps = 1000           ##number of steps\n        self.episode_length = 1000     ## maximum length o feach possible episode\n        self.learning_rate = 0.02      ## basic learning rate\n        self.nb_directions = 16        ##total possible perturbations\n        self.nb_best_directions = 16   ##total best possible perturbations\n        ### chekcing for the number of best directions to be less than total directions\n        assert self.nb_best_directions <= self.nb_directions\n        ## hyperparameters to be considered according to the research paper\n        self.noise = 0.03\n        self.seed = 1\n        self.env_name = \'HalfCheetahBulletEnv-v0\'\n\n\n\n# Building the policy which will decide the locomotion of half-Cheetah\n\nclass Policy():\n\n    def __init__(self, input_size, output_size):\n        self.theta = np.zeros((output_size, input_size))\n\n    def evaluate(self, input, delta = None, direction = None):        ###Perceptrons\n        if direction is None:\n            return self.theta.dot(input)\n        elif direction == ""positive"":\n            return (self.theta + hp.noise*delta).dot(input)\n        else:\n            return (self.theta - hp.noise*delta).dot(input)\n\n    def sample_deltas(self):\n        return [np.random.randn(*self.theta.shape) for _ in range(hp.nb_directions)]\n\n    def update(self, rollouts, sigma_r):\n        step = np.zeros(self.theta.shape)\n        for r_pos, r_neg, d in rollouts:\n            step += (r_pos - r_neg) * d\n        self.theta += hp.learning_rate / (hp.nb_best_directions * sigma_r) * step\n\n## Building function for exploring the policy on one specific direction and over one episode\n   # - Reset the state\n   # - Boolean variables to identify the end of each episode\n   # - variable for number of action played (Ini=0)\n   # - variable for accumulated reward (Ini=0)\n   # - Loop untill number of action played reaches end of Loop\n   # - return a normlaized state\n   # - feed the above result to the Perceptrons\n   # - Update the pyBullet Env object , Step() method\n\n   ## To remove the bias and outliers of very high positive and negative rewards\n   ### we use the classic trick of reinforcement learning\n       # setting +1 for very high positive reward\n       # setting -1 for very high negative rewards\n\n\n# Exploring the policy on one specific direction and over one episode\n\ndef explore(env, normalizer, policy, direction = None, delta = None):\n    state = env.reset()\n    done = False\n    num_plays = 0.\n    sum_rewards = 0\n    while not done and num_plays < hp.episode_length:\n        normalizer.observe(state)\n        state = normalizer.normalize(state)\n        action = policy.evaluate(state, delta, direction)\n        state, reward, done, _ = env.step(action)\n        reward = max(min(reward, 1), -1)\n        sum_rewards += reward\n        num_plays += 1\n    return sum_rewards       \n\n\n# Training the model\n\ndef train(env, policy, normalizer, hp):\n\n    for step in range(hp.nb_steps):\n\n        # Initializing the perturbations deltas and the positive/negative rewards\n        deltas = policy.sample_deltas()\n        positive_rewards = [0] * hp.nb_directions\n        negative_rewards = [0] * hp.nb_directions\n\n        # Getting the positive rewards in the positive directions\n        for k in range(hp.nb_directions):\n            positive_rewards[k] = #explore(env, normalizer, policy, direction = ""positive"", delta = deltas[k])\n\n        # Getting the negative rewards in the negative/opposite directions\n        for k in range(hp.nb_directions):\n            negative_rewards[k] = # explore(env, normalizer, policy, direction = ""negative"", delta = deltas[k])\n\n        # Gathering all the positive/negative rewards to compute the standard deviation of these rewards\n        all_rewards = np.array(positive_rewards + negative_rewards)\n        sigma_r = all_rewards.std()\n\n        # Sorting the rollouts by the max(r_pos, r_neg) and selecting the best directions\n        scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n        order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:hp.nb_best_directions]\n        rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n\n        # Updating our policy\n        policy.update(rollouts, sigma_r)\n\n        # Printing the final reward of the policy after the update\n        reward_evaluation = #explore(env, normalizer, policy)\n        print(\'Step:\', step, \'Reward:\', reward_evaluation)\n\n# Running the main code\n\ndef mkdir(base, name):\n    path = os.path.join(base, name)\n    if not os.path.exists(path):\n        os.makedirs(path)\n    return path\nwork_dir = mkdir(\'result\', \'ars\')\nsimulation_dir = mkdir(work_dir, \'simulation\')\n\nhp = Hyperparameters()\nnp.random.seed(hp.seed)\nenv = gym.make(hp.env_name)\nenv = wrappers.Monitor(env, monitor_dir, force = True)\nnb_inputs = env.observation_space.shape[0]\nnb_outputs = env.action_space.shape[0]\npolicy = Policy(nb_inputs, nb_outputs)\nnormalizer = Normalizer(nb_inputs)\ntrain(env, policy, normalizer, hp)\n'"
