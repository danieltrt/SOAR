file_path,api_count,code
data_analysis_and_forecasting/dataset_analysis.py,18,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport datetime\nfrom numpy import nan\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nimport collections\nimport os\nimport shutil\nfrom sklearn.model_selection import cross_val_score\nfrom scipy import stats\nfrom scipy.stats import norm\n# get_ipython().run_line_magic(\'matplotlib\', \'inline\')\n#from matplotlib.pylab import rcParams\n\n\n# function to read the csv file\n\ndef read_dataframe(filepath):\n    dataframe = pd.read_csv(filepath)  # here the given csv file is reading\n\n    return dataframe\n\n\n# create datetime and drop rowID column if exists\n\ndef create_dateTime(dataframe, col_a, col_b):\n    dataframe = dataframe.sort_index()\n    dataframe[\'dateTime\'] = pd.to_datetime(dataframe[\'longTime\'], unit=\'ms\')\n    dataframe = dataframe.drop([\'longTime\'], axis=1)\n    try:\n        dataframe = dataframe.drop([col_b], axis=1)\n    except:\n        None\n\n    try:\n        dataframe = dataframe.drop([col_a], axis=1)\n    except:\n        None\n\n    return dataframe\n\n# function for converting timestamp to unixtime and return the ready dataframe\n\ndef conversion_timestamp_to_unixtime(initial_dataframe):\n    \'\'\' now conversion of timestamp to unixtime will start. In the csv file the column name of\n    timestamp is longtime.\'\'\'\n\n    longTime = initial_dataframe.loc[0:, [\'longTime\']]\n    longTime = longTime.as_matrix()\n    a = []\n    date_time_array = []\n    for k in longTime:\n        a = np.append(a, k)\n    str_time = []\n    correct_longtime = []\n    datetime_time = []\n    count = 0\n\n    for b in a:\n        b = int(b)  # make plain integer\n        str_b = str(b)\n        c = str_b[-3:]\n        new_str_b = str_b.replace(c, \'\', 1)\n        new_str_b_time = int(new_str_b)\n        correct_longtime.append(new_str_b_time)\n        now_time = datetime.datetime.fromtimestamp(new_str_b_time)\n        convert_time = now_time.strftime(\'%Y-%m-%d %H:%M\')\n        str_time.append(convert_time)\n    test_new = initial_dataframe.assign(stringTime=str_time,\n                                        correct_longtime=correct_longtime)  # here new column in the panda dataframe for string_time has added\n    test_new[\'dateTime\'] = pd.to_datetime(test_new[\'stringTime\'], format=\'%Y-%m-%d %H:%M\')\n    test_new = test_new.drop([\'longTime\', \'stringTime\', \'correct_longtime\'], axis=1)\n\n    return test_new\n\n\n# # be careful here , when perform on a dataframe reset_index then a new column will appear and it is \'index\'. No need of it so immediately drop it. for better view please take a look in the previous cell\n\n\n# create month, year column to observe dataset from a different point of view.\n\ndef distinct_month_1(dataframe, target_column, month_key):\n    array_df = []\n    for i in month_key:\n        req_data_1 = dataframe.loc[(dataframe[target_column] == i)]\n        req_frame_1 = pd.DataFrame(req_data_1, columns=dataframe.columns)\n\n        array_df.append(req_frame_1)\n\n    return array_df\n\n\ndef draw_month(month_key_value, dict_of_month, target_column):\n    for i in month_key_value:\n        value = dict_of_month[i]\n        value.iloc[:].plot(y=[target_column])\n        plt.title(\'visualization of signal \' + str(target_column) + \' in time of \' + str(i))\n        plt.xlabel(\'range\')\n        plt.ylabel(\'value\')\n\n        plt.rcParams[\'figure.figsize\'] = (12, 5)\n        plt.savefig(str(i) + \'.jpg\')\n        plt.show()\n\n\ndef create_month(dataframe, target_column_month,target_column):\n    dataframe = dataframe.set_index(\'dateTime\')\n    dataframe[\'year\'] = pd.DatetimeIndex(dataframe.index).year\n    dataframe[\'month\'] = pd.DatetimeIndex(dataframe.index).month\n\n    dict_of_month = {k: v for k, v in dataframe.groupby(\'month\')}\n    month_key_value = collections.OrderedDict(dict_of_month)\n\n    month_array_df = distinct_month_1(dataframe, target_column_month, month_key_value)\n\n#     draw_month_fig = draw_month(month_key_value, dict_of_month, target_column)\n\n    return dataframe, month_array_df\n\n\ndef choose_month(dataframe, target_column_month):\n    req_data_month = dataframe.loc[(dataframe[target_column_month] == 2) | (dataframe[target_column_month] == 3)]\n    #     req_data_month=dataframe.loc[(dataframe[target_column_month]==2|3) ]\n    req_frame_month = pd.DataFrame(req_data_month, columns=dataframe.columns)\n\n    return req_frame_month\n\n\ndef drop_month_year(dataframe):\n    #     dataframe = dataframe.drop([\'year\',\'month\'], axis=1)\n    dataframe = dataframe.reset_index()\n    return dataframe\n\n\n# def remove_rw_column(dataframe):\n#     new_variable = []\n#     for i in dataframe:\n#         x = i[:2]\n#         if x != \'RW\':\n#             new_variable = np.append(new_variable, i)\n#     return new_variable\n#\n#\n# def remove_rw_column_1(dataframe,req_string):\n#     new_variable = []\n#     for i in dataframe:\n#         x = i[:2]\n#         if x != req_string:\n#             new_variable = np.append(new_variable, i)\n#\n#     dataframe = dataframe.iloc[:][new_variable]\n#     return dataframe\n\n\ndef ascending_dataframe(dataframe, start_pos, end_pos):\n    #     multivariate_data=test_new.iloc[start_pos:end_pos][multivariate_column_label] # comment out this line if you pass column label\n    dataframe = dataframe.iloc[start_pos:end_pos][:]\n    dataframe = dataframe.loc[::-1]\n\n    return dataframe\n\n\n# # Now target column and dateTime colum will be arranged as a given column index. Here target column is the output of turbine 9\'s output\n\n\ndef rearrange_dataframe(dataframe, colname, col_pos):\n    list_col = dataframe.columns.to_list()\n    temp_list = list_col\n    for idx, i in enumerate(colname):\n        sacrifice_val = temp_list[col_pos[idx]]\n        indx = dataframe.columns.get_loc(i)\n        temp_list[col_pos[idx]] = i\n        temp_list[indx] = sacrifice_val\n\n    return dataframe.iloc[:][temp_list]\n\n\n# # Now take in consideration the signal DEWIHOBT9_I0. When the value of it\'s will be 100 only then target column will work otherwise not. So, choose this signal and drop all of the rows where it\'s value != 100 and then drop the whole colum as after dropping this column will only contain value 100 and it will affect negatively in the correlation with target signal\n\n# the function will do the following task\n# if the blast furnace signal for turbine 9 is zero then no work will be happened.\n# so, remove all the rows where this value will be zero\n\ndef check_blast_furnace(dataframe,furnace_signal_column_a,value_A, furnace_signal_column_b,value_B):\n    req_data=dataframe.loc[(dataframe[furnace_signal_column_a]>=value_A) | (dataframe[furnace_signal_column_b]>=value_B)]\n    req_frame=pd.DataFrame(req_data,columns=dataframe.columns)\n    dataframe = req_frame.reset_index()\n    dataframe = dataframe.drop([\'index\'], axis=1)\n    \n    return dataframe\n\n\n# def check_A_B_blast_furnace(dataframe,furnace_signal_column_a,value_A, furnace_signal_column_b,value_B):\n#     req_data=dataframe.loc[(dataframe[furnace_signal_column_a]>=value_A) | (dataframe[furnace_signal_column_b]>=value_B)].values\n#     req_frame=pd.DataFrame(req_data,columns=dataframe.columns)\n#\n#     return req_frame\n\n\n\n# def drop_zero_value_row_of_blast_furnace_signal(dataframe, blast_furnace_signal):\n#     #     dataframe = dataframe.reset_index()\n#     count = []\n#     print(blast_furnace_signal)\n#     for idx_blast_furnace, val_blast_furnace in enumerate(dataframe[blast_furnace_signal]):\n#         if val_blast_furnace != 100:\n#             count = np.append(count, idx_blast_furnace)\n#     print(\'size of count array here: \', count.size)\n#\n#     if count.size > 0:\n#         dataframe_1 = dataframe.drop(count, axis=0)  # axis= 0 means row indiated. 1 means column indicated\n#     else:\n#         dataframe_1 = dataframe\n#     dataframe_1 = dataframe_1.drop([blast_furnace_signal], axis=1)  # dropping the column. because all value are same\n#     return dataframe_1\n\n\n# # Now choose the target colum  and check if any value is zero or not. If zero then drop those rows. here taret column is T9\'s output, signal name is AEWIHO_T9AV2\n\n\ndef check_target_column(dataframe, target_column, req_drop_value_target):\n#     req_data_1=dataframe.loc[(dataframe[target_column]!=req_drop_value_target)]\n    dataframe = dataframe.loc[(dataframe[target_column]>=60) & (dataframe[target_column]<=90)]\n    dataframe = pd.DataFrame(dataframe,columns=dataframe.columns)\n    \n    return dataframe\n    \n\n\n# def no_zero_value_in_target(dataframe, target_column, req_drop_value_target):\n#     req_data_1=dataframe.loc[(dataframe[target_column]!=req_drop_value_target)].values\n#     req_frame_1=pd.DataFrame(req_data_1,columns=dataframe.columns)\n#\n#     return req_frame_1\n\n\n\n\n# def drop_zero_value_row_of_target_signal(dataframe, target_signal):\n#     count = []\n#     for idx_blast_furnace, val_blast_furnace in enumerate(dataframe[target_signal]):\n#         if val_blast_furnace == 0:\n#             count = np.append(count, idx_blast_furnace)\n#     print(type(count))\n#     for i in count:\n#         if i > 24222:\n#             print(i)\n#     print(\'size of count array: \', len(count))\n#\n#     if len(count) > 0:\n#         dataframe_1 = dataframe.drop(count, axis=0)  # axis= 0 means row indiated. 1 means column indicated\n#     else:\n#         dataframe_1 = dataframe\n#     dataframe_1 = dataframe_1.drop(dataframe_1.columns[0], axis=1)  # generally after resetting index the former index\n#     # take place the first place of the column. so removing it.\n#     return dataframe_1\n\ndef dataframe_reset_index(dataframe):\n    dataframe = dataframe.reset_index()\n    dataframe = dataframe.drop([\'index\'], axis=1)\n\n    return dataframe\n\n\ndef drop_column_with_same_value(dataframe):\n    cols = dataframe.select_dtypes([np.number]).columns\n    diff = dataframe[cols].diff().sum()\n    dataframe_drop_column_with_same_value = dataframe.drop(diff[diff == 0].index, axis=1)\n\n    return dataframe_drop_column_with_same_value\n\n\n# # check on the whole dataframe if there is any NAN value or not. If YES, replace it with zero and drop\n# Think twice before using this function\n# checking if any column has nan value or not. If YES then replace nan with zero and drop the row\n\n# a = dataframe_no_zero_value_blast_furnace[blast_furnace_signal].isnull().sum()\n# print(a)\n\ndef drop_nan_value(dataframe):\n    for index, column in enumerate(dataframe):\n        nan_catcher = dataframe[column].isnull().sum()\n        if nan_catcher != 0:\n            dataframe_1 = dataframe[column].replace(0, nan)\n            dataframe_1 = dataframe.dropna(how=\'any\', axis=0)\n        #             print(column,\' has total\',nan_catcher, \'nan valu\')\n        else:\n            dataframe_1 = dataframe\n    #             print(column,\' is free from nan value. look it has: \', nan_catcher,\' value\')\n\n    return dataframe_1\n\n# def drop_row(dataframe):\n#     for i in dataframe:\n#         #        print(i)\n#         dataframe_drop_row_consecutive_same_value = dataframe.loc[dataframe[i].shift() != dataframe[i]]\n#\n#     return dataframe_drop_row_consecutive_same_value\n\ndef drop_unique_valued_columns(dataframe):\n    nunique = dataframe.apply(pd.Series.nunique)\n    cols_to_drop = nunique[nunique == 1].index\n    dataframe = dataframe.drop(cols_to_drop, axis=1)\n\n    return dataframe\n\n\n\ndef drop_string_column(dataframe):\n    drop_object = dataframe.select_dtypes(exclude=[\'object\'])\n\n    return drop_object\n\ndef dataframe_datetime(dataframe):\n    dataframe_datetime = dataframe.set_index(\'dateTime\')\n    dataframe_datetime.describe()\n    return dataframe_datetime\n\n\n# free target column from outlier\ndef free_target_column_from_outlier(dataframe,target_column):\n    dataframe = dataframe[(np.abs(stats.zscore(dataframe[target_column])) < 3)]\n    return dataframe\n\n\n# function to remove outlier from all column\ndef free_dataframe_from_outlier(dataframe):\n    dataframe = dataframe[(np.abs(stats.zscore(dataframe)) < 3).all(axis=1)]\n    \n    return dataframe\n\n\n# # All data cleaning process has done. Now feature selection process will come. Before doing this just make a copy of dataframe and set the index as dateTime\n\n\ndef feature_selection_with_selectKbest(dataframe, max_best_number):\n    train_input = dataframe.iloc[:, :-1]\n    train_output = dataframe.iloc[:, -1]\n    train_output = train_output.to_frame()\n    #     train_output = pd.DataFrame(train_output)\n\n    X, y = train_input, train_output\n    X = X.astype(int)\n    y = y.astype(int)\n\n    bestfeatures = SelectKBest(score_func=chi2, k=2)\n    fit = bestfeatures.fit(X, y)\n    dfscores = pd.DataFrame(fit.scores_)\n    dfcolumns = pd.DataFrame(X.columns)\n\n    featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n    featureScores.columns = [\'Specs\', \'Score\']  # naming the dataframe columns\n    #     print(featureScores.nlargest(20,\'Score\'))  #print 10 best features\n    d = featureScores.nlargest(max_best_number, \'Score\')\n\n    e = []\n    for i, v in enumerate(d[\'Specs\']):\n        e = np.append(e, v)\n\n    e = np.append(e, dataframe.columns[-1])\n    final_dataframe = dataframe.iloc[:][e]\n\n    return final_dataframe\n\n\n# # feature selection with correlation\n\n# find correlated matrix for dataframe which came from sklearn feature selection and the datafarem which has passed\n# to sklearn feature selection function\n\ndef pearson_correlation(dataframe):\n    correlation = dataframe.corr()\n    return correlation\n\n\n# # use the correlation matrix to make the new dataframe where the feature will be the column who has a correlation value with the target in a given range.\n\n# function to make dataframe with high correlated valued column\ndef make_dataframe_with_high_correlated_value(main_dataframe, correlated_dataframe,\n                                              correlation_threshold_value, max_value):\n    target_column = main_dataframe.columns[-1]\n\n    dataframe = correlated_dataframe.reset_index()\n\n    high_correlated_array_with_target = []\n    for index_corr_reset, val_corr_reset in enumerate(dataframe[target_column]):\n        if val_corr_reset > correlation_threshold_value and val_corr_reset < max_value:\n            required_column = dataframe.loc[index_corr_reset][\'index\']\n            if required_column != target_column:\n                high_correlated_array_with_target = np.append(high_correlated_array_with_target, required_column)\n            else:\n                print(required_column)\n                pass\n\n    final_array = np.append(high_correlated_array_with_target, target_column)\n    new_dataframe = main_dataframe.iloc[:][final_array]\n\n    return new_dataframe\n\n\ndef dataframe_date_time_type(dataframe):\n    df = pd.DataFrame(index=dataframe.index)\n    target_df = dataframe.loc[:, dataframe.columns[-1]]\n    df[\'dateTime_column\'] = pd.to_datetime(dataframe.index, format=\'%Y-%m-%d %H:%M\')\n    df[\'day_name\'] = df.index.weekday_name\n    df[\'TypeofDAY\'] = np.where(df[\'dateTime_column\'].dt.dayofweek < 5, \'Weekday\',\n                               \'Weekend\')  # if the associated number less than 5 then weekend, otherwise weekday\n    df[\'TypeofDAY_number\'] = np.where(df[\'dateTime_column\'].dt.dayofweek < 5, 1, 0)  # 1 for weekday, 0 for weekend\n    df[\'Date\'] = df[\'dateTime_column\'].dt.strftime(\'%Y-%m-%d\')\n\n    df = pd.concat([df, target_df], axis=1)\n\n    return df\n\n\ndef draw_gaussian_curve(dataframe,target_column,graph_name):\n    mean_with_outlier = dataframe.describe()[target_column][\'mean\']\n    std_with_outlier = dataframe.describe()[target_column][\'std\']\n    var_with_outlier = (std_with_outlier)**2\n    print(var_with_outlier, std_with_outlier)\n    min_value_with_outlier = dataframe.describe()[target_column][\'min\']\n    max_value_with_outlier = dataframe.describe()[target_column][\'max\']\n    \n    # calculate the z-transform\n    z1 = ( min_value_with_outlier - mean_with_outlier ) / std_with_outlier\n    z2 = ( max_value_with_outlier - mean_with_outlier ) / std_with_outlier\n\n    x = np.arange(z1, z2, 0.001) # range of x in spec\n    x_all = np.arange(-10, 10, 0.001) # entire range of x, both in and out of spec\n    # mean = 0, stddev = 1, since Z-transform was calculated\n    y = norm.pdf(x,0,1)\n    y2 = norm.pdf(x_all,0,1)\n    \n    fig, ax = plt.subplots(figsize=(9,6))\n    plt.style.use(\'fivethirtyeight\')\n    ax.plot(x_all,y2)\n    \n    ax.fill_between(x,y,0, alpha=0.3, color=\'b\')\n    ax.fill_between(x_all,y2,0, alpha=0.1)\n    ax.set_xlim([-4,4])\n    ax.set_xlabel(\'# of Standard Deviations Outside the Mean\')\n    ax.set_yticklabels([])\n    ax.set_title(\'Normal Gaussian Curve\')\n    plt.savefig(graph_name+\'_normal_curve.png\', dpi=72, bbox_inches=\'tight\')\n    plt.show()\n    \ndef gaussian_curve(dataframe, target_column,name):\n    mean = dataframe.describe()[target_column][\'mean\']\n    std = dataframe.describe()[target_column][\'std\']\n    x = np.linspace(mean - 3*std, mean + 3*std, 100)\n    plt.plot(x, stats.norm.pdf(x, mean, std))\n    plt.savefig(name+\'gaussian_normal_curve.png\',bbox_inches=\'tight\')\n    plt.rcParams[\'figure.figsize\'] = (12, 5)\n    plt.show()\n\n\ndef my_sum(x,y):\n    s = x+y\n    return s\n\n\ndef draw_graph(dictionary_value, dictionary, target, path, subfolder_name):\n    fig_location = path + \'/\' + str(subfolder_name)\n\n    if not os.path.exists(fig_location):\n        os.makedirs(fig_location)\n    else:\n        shutil.rmtree(fig_location, ignore_errors=True)\n        os.makedirs(fig_location)\n    for i in dictionary_value:\n        value = dictionary[i]\n        value.iloc[:].plot(y=[target])\n\n        plt.title(\'visualization of signal \' + str(target) + \' in time of \' + str(i))\n        plt.xlabel(\'range\')\n        plt.ylabel(\'value\')\n\n        plt.rcParams[\'figure.figsize\'] = (20, 10)\n        plt.savefig(fig_location + \'/\' + str(i) + \'.jpg\',bbox_inches=\'tight\')\n        plt.show()\n\n\ndef draw_feature_vs_target(dataframe,final_directory,subfolder):\n    fig_location = final_directory + \'/\' + str(subfolder)\n\n    if not os.path.exists(fig_location):\n        os.makedirs(fig_location)\n    else:\n        shutil.rmtree(fig_location, ignore_errors=True)\n        os.makedirs(fig_location)\n\n    for now_num in range(len(dataframe.columns) - 1):\n        col_name = dataframe.columns[now_num]\n        dataframe.iloc[0:100].plot(dataframe.columns[now_num],dataframe.columns[-1])\n        x_axis = dataframe.columns[now_num]\n        y_axis = dataframe.columns[-1]\n        plt.xlabel(x_axis)\n        plt.ylabel(y_axis)\n        plt.title(\'title is \' + str(col_name)+\' vs \'+str(y_axis))\n        plt.rcParams[\'figure.figsize\'] = (20, 10)\n        plt.savefig(fig_location + \'/\' +str(col_name)+\' vs \'+str(y_axis) + \'.jpg\',bbox_inches=\'tight\')\n        plt.show()\n\n\ndef score_checking_with_cross_validation(model_list, train_input, train_output, evaluation_metrics_file_path,name):\n    f = open(evaluation_metrics_file_path, \'a\')\n    f.write(\'\\n\'+\'Score checking with Cross Validation\')\n    f.write(\'\\n\')\n    f.close()\n    for index, value in enumerate(model_list):\n        scores_r2 = cross_val_score(value, train_input, train_output, cv=10, scoring=\'r2\')\n        scores = cross_val_score(value, train_input, train_output, cv=10, scoring=\'neg_mean_squared_error\')\n        mse_scores = -scores\n        rmse_scores = np.sqrt(mse_scores)\n        print(name[index], \'--\' * 5, scores_r2.mean())\n        print(name[index], \'--\' * 5, rmse_scores.mean())\n        f = open(evaluation_metrics_file_path, \'a\')\n        f.write(str(name[index]) + \'\\t\' + \'RMSE: \' + str(rmse_scores.mean()) + \'\\n\')\n        f.write(str(name[index]) + \'\\t\' + \'r_2 square: \' + str(scores_r2.mean()) + \'\\n\')\n        f.write(\'\\n\')\n    f.write(\'Score checking finish with Cross validation\'+\'\\n\')\n    f.close()\n\nfrom statsmodels.tsa.stattools import adfuller\n\ndef test_stationarity(timeseries):\n    # Determing rolling statistics\n    rolmean = timeseries.rolling(12).mean()\n    rolstd = timeseries.rolling(12).std()\n\n    # Plot rolling statistics:\n    fig = plt.figure(figsize=(12, 8))\n    orig = plt.plot(timeseries, color=\'blue\', label=\'Original\')\n    mean = plt.plot(rolmean, color=\'red\', label=\'Rolling Mean\')\n    std = plt.plot(rolstd, color=\'black\', label=\'Rolling Std\')\n    plt.legend(loc=\'best\')\n    plt.title(\'Rolling Mean & Standard Deviation\')\n    #     plt.savefig(\'check_stationarity.jpg\')\n    plt.show()\n\n    # Perform Dickey-Fuller test:\n    print(\'Results of Dickey-Fuller Test:\')\n    dftest = adfuller(timeseries, autolag=\'AIC\', regression=\'c\')\n    dfoutput = pd.Series(dftest[0:4], index=[\'Test Statistic\', \'p-value\', \'#Lags Used\', \'Number of Observations Used\'])\n    p_value = dfoutput[\'p-value\']\n    for key, value in dftest[4].items():\n        dfoutput[\'Critical Value (%s-------)\' % key] = value\n    print(dfoutput)\n\n    if p_value <= 0.05:\n        print (p_value,"": Rejecting Null Hypothesis."")\n        print(""Series is Stationary."")\n    else:\n        print(p_value,"": Weak evidence to reject the Null Hypothesis."")\n        print(""Series is Non-Stationary."")\n\n\ndef tsplot_dataset(df, target_column):\n    n_sample = df.shape[0]\n    print(n_sample)\n    n_train = int(0.995 * n_sample) + 1\n    n_forecast = n_sample - n_train\n\n    ts_train = df.iloc[:n_train][target_column]\n    ts_test = df.iloc[n_train:][target_column]\n    print(ts_train.shape)\n    print(ts_test.shape)\n    print(""Training Series:"", ""\\n"", ts_train.head(), ""\\n"")\n    print(""Testing Series:"", ""\\n"", ts_test.head())\n\n    return n_sample, ts_train, ts_test\nimport statsmodels.tsa.api as smt\nimport seaborn as sns\n\ndef tsplot(y, lags=None, title=\'\', figsize=(14, 8)):\n    fig = plt.figure(figsize=figsize)\n    layout = (2, 2)\n    ts_ax = plt.subplot2grid(layout, (0, 0))\n    hist_ax = plt.subplot2grid(layout, (0, 1))\n    acf_ax = plt.subplot2grid(layout, (1, 0))\n    pacf_ax = plt.subplot2grid(layout, (1, 1))\n\n    y.plot(ax=ts_ax)\n    ts_ax.set_title(title)\n    y.plot(ax=hist_ax, kind=\'hist\', bins=25)\n    hist_ax.set_title(\'Histogram\')\n    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n    [ax.set_xlim(0) for ax in [acf_ax, pacf_ax]]\n    sns.despine()\n    fig.tight_layout()\n    plt.savefig(\'tsplot.png\',bbox_inches=\'tight\')\n    return ts_ax, acf_ax, pacf_ax'"
data_analysis_and_forecasting/main.py,16,"b'# import numpy as np\n# a = np.zeros((156816, 36, 53806), dtype=\'uint8\')\nimport json\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n#import pandas as pd\n#import collections\n\nfrom scipy import stats\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nfrom sklearn import linear_model\nfrom sklearn import tree\n\nfrom dataset_analysis import read_dataframe, create_dateTime, create_month\nfrom dataset_analysis import choose_month, drop_month_year, ascending_dataframe, rearrange_dataframe\nfrom dataset_analysis import check_blast_furnace, check_target_column, dataframe_reset_index\nfrom dataset_analysis import drop_nan_value, drop_unique_valued_columns, drop_string_column, dataframe_datetime\nfrom dataset_analysis import free_dataframe_from_outlier, free_target_column_from_outlier, gaussian_curve, draw_gaussian_curve\n\n#from dataset_analysis import feature_selection_with_selectKbest\nfrom dataset_analysis import pearson_correlation\nfrom dataset_analysis import make_dataframe_with_high_correlated_value\nfrom dataset_analysis import score_checking_with_cross_validation\n# from dataset_analysis import dataframe_date_time_type\nfrom dataset_analysis import draw_graph\nfrom dataset_analysis import draw_feature_vs_target\n\nfrom model_file import make_dataset, scikit_learn_model\nfrom model_file import plot_graph, evaluation_metrices\nfrom model_file import NN_model\nfrom model_file import make_dataset_LSTM, split_sequence, LSTM_model, plot_history, vanilla_lstm\nfrom keras.models import load_model\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\n\nfrom dataset_analysis import test_stationarity, tsplot_dataset, tsplot\nfrom model_file import make_dataset_arima, StartARIMAForecasting\n\nwith open(\'variable_config.json\', \'r\') as f:\n    config = json.load(f)\n\nfilepath = config[\'DEFAULT\'][\'file_path\']\nfilepath_ubuntu = config[\'DEFAULT\'][\'file_path_ubuntu\']\nfilepath_ubuntu_1 = config[\'DEFAULT\'][\'file_path_ubuntu_1\']\nfeb_march_file = config[\'DEFAULT\'][\'feb_march_file\']\n\nfilepath_server_feb_march = config[\'DEFAULT\'][\'file_path_server_feb_march\']\nfilepath_server_april_may = config[\'DEFAULT\'][\'file_path_server_april_may\']\nfilepath_server_june_july = config[\'DEFAULT\'][\'file_path_server_june_july\']\n\nfilepath_ubuntu_feb_march = config[\'DEFAULT\'][\'file_ubuntu_feb_march\']\nfilepath_ubuntu_april_may = config[\'DEFAULT\'][\'file_ubuntu_april_may\']\nfilepath_ubuntu_june_july = config[\'DEFAULT\'][\'file_ubuntu_june_july\']\n\nfilepath_windows_feb_march = config[\'DEFAULT\'][\'file_windows_feb_march\']\nfilepath_windows_april_may = config[\'DEFAULT\'][\'file_windows_april_may\']\nfilepath_windows_june_july = config[\'DEFAULT\'][\'file_windows_june_july\']\n\n\nstart_pos = config[\'DEFAULT\'][\'start_point_dataframe\']\nend_pos = config[\'DEFAULT\'][\'end_point_dataframe\']\ndate_column = config[\'DEFAULT\'][\'date_column\']\ntarget_column = config[\'DEFAULT\'][\'target_column\']\nreq_drop_value_target = config[\'DEFAULT\'][\'req_drop_value_target\']\nfurnace_signal_column_a = config[\'DEFAULT\'][\'blast_furnace_signal_a\']\nfurnace_signal_column_b = config[\'DEFAULT\'][\'blast_furnace_signal_b\']\nvalue_A = config[\'DEFAULT\'][\'req_value_of_blast_furnace_A\']\nvalue_B = config[\'DEFAULT\'][\'req_value_of_blast_furnace_B\']\nmax_best_number = config[\'DEFAULT\'][\'max_best_number\']\ncorrelation_threshold_min_value = config[\'DEFAULT\'][\'correlation_threshold_min_value\']\ncorrelation_threshold_max_value = config[\'DEFAULT\'][\'correlation_threshold_max_value\']\nrequired_number_of_test_data = config[\'DEFAULT\'][\'required_number_of_test_data\']\nsubfolder_feature_vs_target = config[\'DEFAULT\'][\'subfolder_feature_vs_target\']\nevaluation_metrics_file_name = config[\'DEFAULT\'][\'evaluation_metrics_file\']\nnumber_of_step_lstm = config[\'DEFAULT\'][\'n_steps_lstm\']\nepochs = config[\'DEFAULT\'][\'epochs\']\nbatch_size = config[\'DEFAULT\'][\'batch_size\']\n\n\nprint(filepath_ubuntu_feb_march)\n\nfrom datetime import date\ntoday = date.today()\nprint(""Today\'s date:"", today)\n\n\ncurrent_directory = os.getcwd()\nprint(\'current_directory is: \',current_directory)\naddress = \'image_folder_local_outlier\'\nfinal_directory = current_directory+\'/\'+str(address)\nif not os.path.exists(final_directory):\n    os.makedirs(final_directory)\n    print(\'created : \', final_directory)\nelse:\n    print(\' has already created\',final_directory)\n    pass\n\n\n\nevaluation_metrics_file_path = final_directory+\'/\'+evaluation_metrics_file_name\nif not os.path.isfile(evaluation_metrics_file_path):\n    f = open(evaluation_metrics_file_path,\'a\')\n    f.close()\n    print(\'metrics file now created\')\nelse:\n    os.remove(evaluation_metrics_file_path)\n    f = open(evaluation_metrics_file_path,\'a\')\n    f.close()\n    print(\'metrics file removed and created\')\n    \nimport pandas as pd\ndataframe_read = read_dataframe(filepath_ubuntu_feb_march)\n#dataframe_read = pd.read_csv(filepath_server_feb_march, nrows = 40000)\ncols_list = [\'longTime\',furnace_signal_column_a,furnace_signal_column_b,target_column,\'RWWIHOBG9_V0\',\'AEWIGHG9__P0\',\'AEWIGHG9__T0\']\ndataframe_sliced = dataframe_read.iloc[:][cols_list]  #this is done here due to overcome the huge time of processing on a big data. cols_list array includes the column which are\n                                                        #highly correlated with the target variable. This highly correlatd info got from the training or first stage of coding.\n                                                        # finding high correlation part is also given later. just uncomment that part to use.\n\ndataframe_with_date = create_dateTime(dataframe_read,\'row_ID\',\'Unnamed: 0\') # if you don\'t want to use cols_list and have much computational power\n                                                                            # and time then uncomment this line and comment out previous and the following line\n\ndataframe_with_date = create_dateTime(dataframe_sliced,\'row_ID\',\'Unnamed: 0\') # here row ID and unnamed have written beacuse slicing the main datfarame got from the IAT department generated\n                                                                # this columns.\ndataframe_include_month, month_array_df = create_month(dataframe_with_date, \'month\', target_column) #this code generate month value in numeric order by taing value from dateTime column\n                                                                                                    # january -- 1, february --2 etc.\ndataframe_with_specific_month = choose_month(dataframe_include_month,\'month\') # this line take value from the ""month"" column and keep only specific month.\n                                                                                    # please take a look in the body of the function.\ndataframe_with_specific_month_reset = drop_month_year(dataframe_with_specific_month) # here, drop of the month column is possible though it it omitted here. No need.\n\nprint(dataframe_read.shape)\nplt.plot(dataframe_read[target_column], color = \'blue\')\nplt.plot(dataframe_read[furnace_signal_column_a], color = \'red\')\nplt.plot(dataframe_read[furnace_signal_column_b], color = \'green\')\nplt.legend([target_column, furnace_signal_column_a, furnace_signal_column_b],(\'my target\',\'furnace A\',\'furnace B\'))\n#plt.xlim(0,initial_dataframe.shape[0]+10)\nplt.xticks(np.arange(0,dataframe_read.shape[0],5000),rotation=\'vertical\')\nplt.xlabel(\'Numebr of observation\')\nplt.ylabel(\'Value\')\nplt.savefig(\'only_blast_furnace.png\',bbox_inches=\'tight\')\nplt.rcParams[\'figure.figsize\'] = (12,5)\n\n# =============================================================================\n# Below code is used for draw graph with using own label name for each variable. It is same as the above graph.\n# =============================================================================\n\n#plt.plot(dataframe_read[target_column], color = \'blue\', label=\'Target(Amount of wind in Turbine 9)\')\nplt.plot(dataframe_read[furnace_signal_column_a], color = \'red\', label = \'Blast Furnace A\')\nplt.plot(dataframe_read[furnace_signal_column_b], color = \'green\', label = \'Blast Furnace B\')\nplt.legend(loc=\'best\')\n#plt.xlim(0,initial_dataframe.shape[0]+10)\nplt.xticks(np.arange(0,dataframe_read.shape[0],5000),rotation=\'vertical\')\nplt.xlabel(\'Numebr of observation\')\nplt.ylabel(\'Value\')\nplt.savefig(\'only_blast_furnace.png\',bbox_inches=\'tight\')\nplt.rcParams[\'figure.figsize\'] = (12,5)\n\n\n\ndataframe_ascending = ascending_dataframe(dataframe_with_specific_month_reset, start_pos,dataframe_with_specific_month_reset.shape[0]) # this code arrange the dataset in ascending order\n                                                                                                                        # with respect to time (smallest to largest)\nindex_array=[0,-1]\nreq_column_name = [date_column, target_column]\n# req_column_name = [date_column, furnace_signal_column]\ndataframe_rearranged = rearrange_dataframe(dataframe_ascending,req_column_name,index_array)\n\ndataframe_clean_furnace_column = check_blast_furnace(dataframe_rearranged, furnace_signal_column_a, value_A,\n                                                               furnace_signal_column_b, value_B) # this line check is there any anomaly in the blast furnace A and B\'s column\n                                                                                                # and remove that rows\nprint(dataframe_clean_furnace_column.shape)\n\n\n#dataframe_clean_target_column = dataframe_clean_furnace_column\n\n\n# =============================================================================\n# dataframe_clean_target_column = check_target_column(dataframe_clean_furnace_column,\n#                                                                   target_column, req_drop_value_target) # It checks the target column and remove the rows whos value is less than\n#                                                                                                         # a minimum value. in this task that was chosen as 60.\n# =============================================================================\n\n# =============================================================================\n# dataframe_free_from_furnace_target_column_anomaly = dataframe_reset_index(dataframe_clean_target_column)\n# print(dataframe_free_from_furnace_target_column_anomaly.shape)\n# \n# plt.plot(dataframe_free_from_furnace_target_column_anomaly [target_column], color = \'blue\')\n# plt.plot(dataframe_free_from_furnace_target_column_anomaly [furnace_signal_column_a], color = \'red\')\n# plt.plot(dataframe_free_from_furnace_target_column_anomaly [furnace_signal_column_b], color = \'green\')\n# plt.legend([target_column, furnace_signal_column_a, furnace_signal_column_b], loc=\'center left\')\n# # plt.xlim(0,initial_dataframe.shape[0]+10)\n# plt.xticks(np.arange(0,dataframe_free_from_furnace_target_column_anomaly .shape[0],5000),rotation=\'vertical\')\n# plt.xlabel(\'Numebr of observation\')\n# plt.ylabel(\'Value\')\n# #plt.savefig(\'blast_vs_target_post.png\',bbox_inches=\'tight\')\n# plt.rcParams[\'figure.figsize\'] = (12, 5)\n# =============================================================================\n\nplt.plot(dataframe_clean_furnace_column[furnace_signal_column_a], color = \'red\')\nplt.plot(dataframe_clean_furnace_column[furnace_signal_column_b],\'green\')\nplt.plot(dataframe_clean_furnace_column[target_column],\'blue\')\nplt.legend([furnace_signal_column_a, furnace_signal_column_b, target_column], loc=\'center left\')\n#plt.xlim(0,dataframe_clean_furnace_column.shape[0]+1000)\nplt.xticks(np.arange(0,dataframe_clean_furnace_column.shape[0],5000),rotation=\'vertical\')\nplt.xlabel(\'Numebr of observation\')\nplt.ylabel(\'Value\')\nplt.savefig(\'furnace_column_cleaned.png\',bbox_inches=\'tight\')\nplt.rcParams[\'figure.figsize\'] = (12, 5)\nplt.show()\n\nprint(dataframe_clean_furnace_column.shape)\n\n\n\n#dataframe_drop_nan = drop_nan_value(dataframe_free_from_furnace_target_column_anomaly)\ndataframe_drop_nan = drop_nan_value(dataframe_clean_furnace_column)\ndataframe_drop_unique_valued_column = drop_unique_valued_columns(dataframe_drop_nan)\ndataframe_drop_string = drop_string_column(dataframe_drop_unique_valued_column)\nprint(dataframe_drop_string.shape)\ndataframe_drop_string.dtypes\n\n# =============================================================================\n# dataframe_read = None\n# dataframe_with_date = None\n# dataframe_with_specific_month = None\n# dataframe_with_specific_month_reset = None\n# dataframe_ascending = None\n# dataframe_rearranged = None\n# dataframe_clean_furnace_column = None\n# dataframe_clean_target_column = None\n# dataframe_drop_nan = None\n# dataframe_drop_unique_valued_column = None\n# =============================================================================\n\nplt.plot(dataframe_drop_string[target_column], color = \'blue\')\nplt.plot(dataframe_drop_string[furnace_signal_column_a], color = \'red\')\nplt.plot(dataframe_drop_string[furnace_signal_column_b], color = \'green\')\nplt.legend([target_column, furnace_signal_column_a, furnace_signal_column_b], loc=\'upper left\')\nplt.legend([target_column], loc=\'best\')\nplt.xticks(np.arange(0,dataframe_drop_string.shape[0],4000),rotation=\'vertical\')\nplt.xlabel(\'Numebr of observation\')\nplt.ylabel(\'Value\')\nplt.savefig(\'final_target_column_before_removing_outlier.png\',bbox_inches=\'tight\')\n# plt.xlim(0,initial_dataframe.shape[0]+10)\n# plt.xticks(np.arange(0,initial_dataframe.shape[0],))\nplt.rcParams[\'figure.figsize\'] = (12, 5)\n\n\ndataframe_drop_string.describe()\ndataframe_drop_string.dtypes\n\n\ndataframe_datetime = dataframe_datetime(dataframe_drop_string) # make dateTime as index\n#print(dataframe_datetime_1.shape)\n#dataframe_datetime = dataframe_datetime_1.drop([furnace_signal_column_a, furnace_signal_column_b, \'month\'], axis=1)\n\narr =[]\nfor i in dataframe_datetime.columns:\n    arr.append(i)\n\n# function to remove outlier from a single column\ndataframe_target_column_free_from_outlier = free_target_column_from_outlier(dataframe_datetime, target_column)\n\n\nprint(dataframe_target_column_free_from_outlier.shape)\n#plt.plot(dataframe_target_column_free_from_outlier[target_column])\ndataframe_target_column_free_from_outlier.describe()\nprint(dataframe_target_column_free_from_outlier.max())\n\n\n# =============================================================================\n# plot dataframe where target column free from outlier by resetting index as it is easy to interpret\n# =============================================================================\ndataframe_target_column_free_from_outlier_copy = dataframe_target_column_free_from_outlier.copy()\ndataframe_reset_target_column_free_from_outlier = dataframe_target_column_free_from_outlier_copy.reset_index()\nprint(dataframe_reset_target_column_free_from_outlier.shape)\nplt.plot(dataframe_reset_target_column_free_from_outlier[target_column], color = \'blue\')\nplt.legend([target_column], loc=\'best\')\nplt.xticks(np.arange(0,dataframe_reset_target_column_free_from_outlier.shape[0],5000),rotation=\'vertical\')\nplt.xlabel(\'Numebr of observation\')\nplt.ylabel(\'Value\')\nplt.savefig(\'final_target_column_after_removing_outlier.png\',bbox_inches=\'tight\')\n#plt.xlim(0,initial_dataframe.shape[0]+10)\n#plt.xticks(np.arange(0,initial_dataframe.shape[0],))\nplt.rcParams[\'figure.figsize\'] = (12, 5)\n\n\n\n\n\n# function to remove outlier from a whole dataframe\ndataframe_free_from_outlier = free_dataframe_from_outlier(dataframe_datetime) \nprint(dataframe_free_from_outlier.shape)\n\nplt.plot(dataframe_datetime[target_column], color =\'red\')\nplt.plot(dataframe_free_from_outlier[target_column], color = \'green\')\nplt.show()\n\ndataframe_free_from_outlier.describe()\ndataframe_target_column_free_from_outlier.describe()\ncount =0\nfor i in dataframe_datetime[target_column]:\n    if i < 62.48:\n        count +=1\nprint(count)\n\narr =[]\nfor i in dataframe_datetime.columns:\n    arr.append(i)\n    \n#plt.plot(dataframe_datetime[furnace_signal_column_a])\n#plt.plot(dataframe_datetime[furnace_signal_column_b])\n\nboxplot_dateTime = dataframe_datetime.boxplot(column = arr)\nplt.xlabel(\'Numebr of observation\')\nplt.ylabel(\'Value\')\n#plt.savefig(\'whole_frame_boxplot.png\',bbox_inches=\'tight\')\n\n# =============================================================================\n# sd = pd.melt(dataframe_datetime, value_vars=[arr[0], arr[1]])\n# sd\n# import seaborn as sns\n# sns.swarmplot(x=\'variable\', y=\'value\', data=sd)\n# =============================================================================\n\n\n\nboxplot_target_column_free_from_outlier = dataframe_target_column_free_from_outlier.boxplot(column=arr)\nboxplot_dataframe_free_from_outlier = dataframe_free_from_outlier.boxplot(column=arr)\n\nplt.boxplot(dataframe_free_from_outlier[target_column])\n\n# =============================================================================\n# plotting the gausian curve\n# =============================================================================\n#total_mean_target_column_with_outlier, total_variance_target_column_with_outlier = dataframe_datetime[target_column].mean(),dataframe_datetime[target_column].std()\n#draw_gaussian_curve(dataframe_target_column_free_from_outlier, target_column, graph_name = \'without_outlier\') # draw gaussian curve, please change graph name \n#draw_gaussian_curve(dataframe_datetime, target_column, graph_name = \'with_outlier\') #  with outliuer\n    \ngaussian_curve(dataframe_target_column_free_from_outlier,target_column,name = \'1_free_from_outlier_1_\') # gaussian curve without outlier\n\ngaussian_curve(dataframe_datetime,target_column,name = \'1_with_from_outlier_1_\') # gaussian curve with outlier\n\n\n\n\n#sklearn_feature_best_dataframe = feature_selection_with_selectKbest(dataframe_datetime,max_best_number)\n#sklearn_correlation = pearson_correlation(sklearn_feature_best_dataframe)\n\nmain_correlation = pearson_correlation(dataframe_target_column_free_from_outlier)\n\nmain_frame = dataframe_target_column_free_from_outlier\ncorrelated_frame = main_correlation\nprint(main_frame.shape)\nprint(correlated_frame.shape)\nprint(correlated_frame)\n# main_frame = sklearn_feature_best_dataframe\n# correlated_frame = sklearn_correlation\n\ndataframe_high_correlation = make_dataframe_with_high_correlated_value(main_frame,correlated_frame,\n                                                             correlation_threshold_min_value, correlation_threshold_max_value)\n\nprint(dataframe_high_correlation.shape)\ndf_res = dataframe_high_correlation.reset_index()\ndf_res_1 = df_res.drop(df_res.columns[0], axis=1)\ndataframe_high_correlation.describe()\n\n\n# =============================================================================\n# graph target colum vs all feature before resampling\n# =============================================================================\nfor i in range (dataframe_high_correlation.shape[1]):\n    if i == 3:\n        break\n    else:\n        plt.scatter(dataframe_high_correlation[dataframe_high_correlation.columns[i]], dataframe_high_correlation[target_column])\n        plt.show()\n        plt.figure()\n\n\n\ndataframe_resample = dataframe_high_correlation.resample(\'1min\').mean()\ndataframe_resample_copy = dataframe_resample.copy()\ndataframe_resample_copy = dataframe_resample_copy.reset_index()\n\ndataframe_interpolate = dataframe_resample.interpolate(\'linear\')\ndataframe_interpolate_copy = dataframe_interpolate.copy()\ndataframe_interpolate_copy = dataframe_interpolate_copy.reset_index()\n\nplt.plot(dataframe_interpolate_copy[target_column], label=\'Target(Amount of wind in Turbine 9)\')\n#plt.legend([target_column], loc=\'best\')\nplt.legend(loc=\'best\')\nplt.xticks(np.arange(0,dataframe_interpolate_copy.shape[0],5000),rotation=\'vertical\')\nplt.xlabel(\'Numebr of observation\')\nplt.ylabel(\'Value\')\nplt.savefig(\'interpolation.png\',bbox_inches=\'tight\')\n# plt.xlim(0,initial_dataframe.shape[0]+10)\n# plt.xticks(np.arange(0,initial_dataframe.shape[0],))\nplt.rcParams[\'figure.figsize\'] = (12, 5)\n\nprint(dataframe_resample.shape)\nprint(dataframe_interpolate.shape)\n\n# =============================================================================\n# the following code stands for plotting correlation plot\n# =============================================================================\n\nimport seaborn as sns\nsns.pairplot(dataframe_interpolate_copy)\n\n\n# =============================================================================\n# The following code stands for plotting feature VS target graph (scatter plot)\n# =============================================================================\nfor i in range (dataframe_interpolate.shape[1]):\n    if i == 3:\n        break\n    else:\n        plt.scatter(dataframe_interpolate[dataframe_interpolate.columns[i]], dataframe_interpolate[target_column])\n        plt.xlabel(dataframe_interpolate.columns[i])\n        plt.ylabel(target_column)\n#        plt.savefig(dataframe_interpolate.columns[i]+\'_vs_\'+target_column+\'_correlation.jpg\')\n        plt.show()\n        plt.figure()\n        \n\n# =============================================================================\n# The following code stands for plotting feature VS target graph \n# =============================================================================\n\nfor now_num in range(3):\n    col_name = dataframe_high_correlation.columns[now_num]\n    dataframe_high_correlation.iloc[0:100].plot(dataframe_high_correlation.columns[now_num],dataframe_high_correlation.columns[-1])\n    plt.title(\'title is \'+str(col_name))\n\nmain_frame.head(2)\nmain_correlation.head(2)\nlen(dataframe_high_correlation.columns)\nprint(type(dataframe_high_correlation))\n\n# =============================================================================\n# The following code stands for observing partial correlation. In this type of plotting NEVER EVER target column will present.\n# To know the number of plot you have to know the number of feature(number of column of the dataframe except target column)\n# To know how many plot could be drawn can be calculated by doing the combination operation between two variable\n# Consider dataframe has total 10 column where 1 column is target. Then number of feature column is 9. Do, 9C2(combination).\n# It is the number of total plot  \n# =============================================================================\n\np=0\nq=0\ncolumn_number=3\ncount = 0\nq = p+1\nfor h in range(4):\n#     print(\'loop num: \',h,\'\\n\')\n    if p !=column_number and q!=column_number:\n        print(dataframe_high_correlation.columns[p],\'--\'*5,dataframe_high_correlation.columns[q])\n#        plt.plot(dataframe_high_correlation[dataframe_high_correlation.columns[p]],dataframe_high_correlation[dataframe_high_correlation.columns[q]])\n        dataframe_high_correlation.plot(dataframe_high_correlation.columns[p],dataframe_high_correlation.columns[q], kind=\'scatter\')\n        plt.xlabel(dataframe_high_correlation.columns[p])\n        plt.ylabel(dataframe_high_correlation.columns[q])\n        plt.title(dataframe_high_correlation.columns[p]+\'_vs_\'+dataframe_high_correlation.columns[q])\n        plt.savefig(dataframe_high_correlation.columns[p]+\'_vs_\'+dataframe_high_correlation.columns[q]+\'_partial_correlation_check.jpg\')\n        plt.show()\n        plt.figure()\n#         print(\'\\n\')\n#         print(\'------------count: \',count,\' p: \',p,\'\\t q: \',q)\n        q+=1\n        count+=1\n        if q == column_number:\n            print(\'*\'*20)\n            p+=1\n            q=p+1\n#             print(\'now val of p: \',p,\' and q: \',q)\n            if p == column_number-1:\n#                 print(\'val of p is: \',p)\n                print(\'finish\')\n\n\n\n# =============================================================================\n# following two lines anyone can use to plot feature vs target graph.\n# =============================================================================\n\n# subfolder_1 = \'feature_vs_target\'+\'_\'+str(today)\n# draw_feature_vs_target = draw_feature_vs_target(dataframe_interpolate,final_directory,subfolder_1)\n\n\n# =============================================================================\n# Here, train and test set are going to be made\n# =============================================================================\n\ntrain_input, train_output, test_input, test_output = make_dataset(dataframe_interpolate,required_number_of_test_data)\n\nmodel_list = [LinearRegression(fit_intercept=True),linear_model.Lasso(alpha=0.1),linear_model.Ridge(alpha=.5),\n              linear_model.BayesianRidge(), tree.DecisionTreeRegressor(max_depth=2),ExtraTreesRegressor(),\n              BaggingRegressor(ExtraTreesRegressor()),GBR()]\nname = [\'LinearRegression\',\'Lasso\',\'Ridge\',\'BayesianRidge\',\'tree\',\'ExtraTreesRegressor\',\'BaggingRegressor\',\'GBR\']\n\n    \n\nmodel = scikit_learn_model(model_list, name, train_input, train_output, test_input, test_output,\n                           final_directory, evaluation_metrics_file_path)    # this function will take the model_list array and perform training and testing operation and finally\n                                                                            # save result in a text file and graph in a specific fiolder named by the model name.\n\n# import tkinter\n# # import matplotlib\n# # matplotlib.get_backend()\n# import matplotlib\n# matplotlib.use(\'TkAgg\')\n# plt.plot(dataframe_no_zero_value_target_column[target_column], color = \'blue\')\n# # plt.show()\n# # plt.interactive(False)\n# plt.show(block=False)\n# plt.ioff()\n# # plt.interactive(False)\n\n\n# print(len(dataframe_high_correlation.columns))\n# subfolder_1 = \'feature_vs_target\'+\'_\'+str(today)\n# draw_feature_vs_target = draw_feature_vs_target(dataframe_high_correlation,final_directory,subfolder_1)\n\n# df = dataframe_date_time_type(dataframe_high_correlation)\n# # df = dataframe_date_time_type(temp_frame_1)\n#\n# dict_of_dates = {k: v for k, v in df.groupby(\'Date\')}\n# dict_of_day_type = {k:v for k,v in df.groupby(\'TypeofDAY\')}\n# dict_of_day_name = {k:v for k,v in df.groupby(\'day_name\')}\n#\n#\n# date_key_value = collections.OrderedDict(dict_of_dates)\n# day_type_key_value = collections.OrderedDict(dict_of_day_type)\n# day_name_key_value = collections.OrderedDict(dict_of_day_name)\n#\n# draw_graph_date = draw_graph(date_key_value,dict_of_dates, target_column,final_directory, subfolder_name = \'3_date_fig_target\')\n# draw_graph_week = draw_graph(day_type_key_value,dict_of_day_type, target_column,final_directory, subfolder_name = \'3_week_fig_target\')\n# draw_graph_day = draw_graph(day_name_key_value,dict_of_day_name, target_column,final_directory, subfolder_name = \'3_day_fig_target\')\n\n#train_output_cv = np.reshape(train_output,(-1,1))\n\n# =============================================================================\n# cross validation scores checking\n# =============================================================================\n\nscore_checking_with_cross_validation(model_list, train_input, train_output, evaluation_metrics_file_path,name) # cross validation process used to check the accuracy\n\n\n\n#from sklearn.ensemble import RandomForestRegressor\n#from sklearn.model_selection import KFold\n#kf = KFold(n_splits = 5, shuffle = True)\n##rf_reg = RandomForestRegressor()\n#scores = []\n#for i in range(5):\n#    result = next(kf.split(train_input), None)\n#    print(result)\n#    x_train = train_input[result[0]]\n#    print(x_train)\n#    x_test = train_input[result[1]]\n#    y_train = train_output[result[0]]\n#    y_test = train_output[result[1]]\n#    model = my_model.fit(x_train,y_train)\n#    predictions = my_model.predict(x_test)\n#    np.append(scores,model.score(x_test,y_test))\n#print(\'Scores from each Iteration: \', scores)\n#print(\'Average K-Fold Score :\' , np.mean(scores))\n\n\n#my_model.fit(train_input, train_output)\n#my_pred = my_model.predict(test_input)\n#plt.plot((min(test_output), max(test_output)), (min(my_pred), max(my_pred)), color=\'red\')\n#plt.scatter(test_output, my_pred, color=\'blue\')\n\n# from sklearn import linear_model\n#from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n#import math\n#print(\'r_2 statistic: %.2f\' % r2_score(test_output,my_pred))\n#print(""Mean_absolute_error: %.2f"" % mean_absolute_error(test_output,my_pred))\n#print(""Mean squared error: %.2f"" % mean_squared_error(test_output,my_pred))\n#RMSE=math.sqrt(mean_squared_error(test_output,my_pred))\n#print(\'RMSE: \',RMSE)\n    \n    \n\n# =============================================================================\n# Check learning curve \n# =============================================================================\n\n# import sklearn\n# from sklearn.model_selection import learning_curve\n# extratreereg = sklearn.tree.ExtraTreeRegressor()\n# extratreereg.fit(train_input, train_output)\n#\n# train_sizes, train_scores, validation_scores = learning_curve(extratreereg,train_input,train_output,\n#                                                               cv = 5,scoring = \'neg_mean_squared_error\',n_jobs=10)\n#\n# train_sizes\n#\n# print(\'Training scores:\\n\\n\', train_scores)\n# print(\'\\n\', \'-\' * 70) # separator to make the output easy to read\n# print(\'\\nValidation scores:\\n\\n\', validation_scores)\n#\n# import pandas as pd\n# train_scores_mean = -train_scores.mean(axis = 1)\n# validation_scores_mean = -validation_scores.mean(axis = 1)\n# print(\'Mean training scores\\n\\n\', pd.Series(train_scores_mean, index = train_sizes))\n# print(\'\\n\', \'-\' * 20) # separator\n# print(\'\\nMean validation scores\\n\\n\',pd.Series(validation_scores_mean, index = train_sizes))\n#\n# import matplotlib.pyplot as plt\n#\n# plt.style.use(\'seaborn\')\n# plt.plot(train_sizes, train_scores_mean, label = \'Training error\')\n# plt.plot(train_sizes, validation_scores_mean, label = \'Validation error\')\n# plt.ylabel(\'MSE\', fontsize = 14)\n# plt.xlabel(\'Training set size\', fontsize = 14)\n# plt.title(\'Learning curves for a linear regression model\', fontsize = 18, y = 1.03)\n# plt.legend()\n# plt.ylim(0,50)\n# plt.show()\n\n# =============================================================================\n# Neural Network\n# =============================================================================\n\n# from keras.models import Sequential\n# from keras.layers import Dense, Activation, Flatten\n# from sklearn.metrics import mean_absolute_error\n\nlr = 0.01\ndef lr_schedule(epoch):\n    print(\'epoch is: \', epoch)\n    return lr * (0.1 ** int(epoch / 10))\n\nNN_model_1 = NN_model(train_input)\n# NN_model.compile(loss=\'mean_absolute_error\', optimizer=\'adam\', metrics=[\'mean_absolute_error\',\'accuracy\'])\nNN_model_1.compile(optimizer=\'adam\', loss=\'mse\',metrics=[\'mse\',\'accuracy\'])\nNN_model_1.summary()\n\ntrain_model_NN = NN_model_1.fit(train_input, train_output, batch_size=batch_size, epochs=epochs, verbose=1,\n                              validation_split=0.2,shuffle=True,callbacks=[LearningRateScheduler(lr_schedule)])\n\n\n# observe learning curve with Neural Network\nprint(train_model_NN.history.keys())\nhistory_graph_NN = ""NN""\nplot_history(train_model_NN,history_graph_NN)\nNN_model_1.save(final_directory+\'/\'+""Neural_Network.h5"") # save the trained model to use later\nload_trained_NN = load_model(final_directory+\'/\'+""Neural_Network.h5"") # load the saved model\n# predicted_output_NN = load_trained_NN.predict(test_input) # uncomment this line if saved model needs to load for prediction\npredicted_output_NN = NN_model_1.predict(test_input)\ntest_output_NN = np.reshape(test_output,(-1,1)) # reshaping to 2D array is necessary to plot as the predicted output of NN is 2 dimensional\nprint(test_output_NN.shape)\nplot_graph(test_output_NN, predicted_output_NN, final_directory,\'Neural_Network\') # plotting result curve\nevaluation_metrices(test_output_NN,predicted_output_NN,final_directory,\'Neural Netowrk\', evaluation_metrics_file_path) # storing evaluation result\n\n\n\n\n# =============================================================================\n# Vanilla LSTM\n# =============================================================================\nmultiple_ip_train_data, multiple_ip_test_set = make_dataset_LSTM(dataframe_interpolate, required_number_of_test_data) # A separate procedure of making dataset for vanilla LSTM\n\nX_train_vanilla, y_train_vanilla = split_sequence(multiple_ip_train_data, number_of_step_lstm) # dividing in sequence for LSTM\nprint(\'X_train_vanilla shape: \',X_train_vanilla.shape,\'\\t dimension: \',X_train_vanilla.ndim,\'\\t size: \',X_train_vanilla.size)\nprint(\'y_train_vanilla shape: \',y_train_vanilla.shape,\'\\t dimension: \',y_train_vanilla.ndim,\'\\t size: \',y_train_vanilla.size)\n\nX_train_vanilla = X_train_vanilla.reshape((X_train_vanilla.shape[0], X_train_vanilla.shape[1], X_train_vanilla.shape[-1]))\nprint(X_train_vanilla.shape)\n\nn_steps_vanilla = number_of_step_lstm\nn_features_vanilla = X_train_vanilla.shape[-1]\nvanilla_model = vanilla_lstm(n_steps_vanilla, n_features_vanilla)\nvanilla_model.compile(optimizer=\'adam\', loss=\'mse\',metrics=[\'accuracy\'])\nvanilla_model.summary()\ntrain_model_vanilla = vanilla_model.fit(X_train_vanilla, y_train_vanilla, batch_size=batch_size, epochs=epochs, verbose=1,validation_split=0.2,\n                                        shuffle=True,callbacks=[LearningRateScheduler(lr_schedule)])\n\n\nprint(train_model_vanilla.history.keys())\nplot_history(train_model_vanilla,history_graph_NN=""vanilla_lstm"")\n\nvanilla_model.save(final_directory+\'/\'+""VANILLA_LSTM.h5"")\nload_trained_VANILLA_LSTM=load_model(""VANILLA_LSTM.h5"")\ntest_ip_vanilla,test_op_vanilla=split_sequence(multiple_ip_test_set,number_of_step_lstm)\nn_features_test = test_ip_vanilla.shape[-1]\nX_test_ip_vanilla=test_ip_vanilla.reshape((test_ip_vanilla.shape[0], test_ip_vanilla.shape[1], n_features_test))\nyhat_vanilla_loaded = vanilla_model.predict(X_test_ip_vanilla, verbose=1)\nprint(yhat_vanilla_loaded.shape)\n# evaluate the model\n_, train_mse = vanilla_model.evaluate(X_train_vanilla, y_train_vanilla, verbose=0)\n_, test_mse = vanilla_model.evaluate(X_test_ip_vanilla, test_op_vanilla, verbose=0)\nprint(\'Train: %.3f, Test: %.3f\' % (train_mse, test_mse))\n\nevaluation_metrices(test_op_vanilla, yhat_vanilla_loaded, final_directory, \'VANILLA_LSTM\',evaluation_metrics_file_path)\ntest_op_vanilla_reshape = np.reshape(test_op_vanilla,(-1,1))\nprint(test_op_vanilla_reshape.shape)\nprint(yhat_vanilla_loaded.shape)\nplot_graph(test_op_vanilla_reshape, yhat_vanilla_loaded, final_directory,\'vanilla_LSTM\')\n\n\n\n\n# =============================================================================\n# ARIMA\n# =============================================================================\n#import re\n#import seaborn as sns\n#from scipy import stats\nimport matplotlib.pyplot as plt\n#import statsmodels.tsa.api as smt\n#import plotly.graph_objects as go\n#from plotly.subplots import make_subplots\n#from dateutil.relativedelta import relativedelta\n#from statsmodels.tsa.stattools import acf, pacf\n#from statsmodels.tsa.seasonal import seasonal_decompose\n#from statsmodels.tsa.stattools import adfuller\n# from sklearn.model_selection import cross_val_predict\nimport warnings\nwarnings.filterwarnings(""ignore"")\nfrom matplotlib import pyplot\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n\ndf = dataframe_interpolate[[target_column]]\nprint(type(df))\nprint(df.shape)\n\nstat, p = stats.normaltest(df.iloc[:,-1])\nprint(\'Statistics=%.3f, p=%.3f\' % (stat, p))\nalpha = 0.05\nif p > alpha:\n    print(\'Data looks Gaussian (fail to reject H0)\')\nelse:\n    print(\'Data does not look Gaussian (reject H0)\')\n# from dataset_analysis import test_stationarity\n\n\ntest_stationarity(df.iloc[:,-1]) # It checks the stationarity of the dataset\n\nn_sample, ts_train, ts_test = tsplot_dataset(df, target_column) # make array for drawing ts_plot\n\ntsplot(ts_train, title=\'Output of the Turbine 9; Signal: \'+str(target_column), lags=62) # draw plot for time series, acf, pacf\n\nplot_acf(df.iloc[:,-1], lags=62)\nplt.xlabel(\'lags\')\nplt.ylabel(\'correlation\')\n# plt.savefig(\'acf plot code.png\',bbox_inches=\'tight\')\npyplot.show()\nplt.figure()\nplot_pacf(df.iloc[:,-1], lags=62)\nplt.xlabel(\'lags\')\nplt.ylabel(\'correlation\')\n# plt.savefig(\'pacf plot code.png\',bbox_inches=\'tight\')\npyplot.show()\n\n\ntrain_set_arima, test_set_arima = make_dataset_arima(df, required_number_of_test_data = 5)\nprint(\'train_set_arima shape: \', train_set_arima.shape)\nprint(\'test_set_arima shape: \', test_set_arima.shape)\n\nActual = [x for x in train_set_arima]\nPredictions_ARIMA = list()\n\n\ncount_1=1\nfor timepoint in range(len(test_set_arima)):\n#     print(\'I am in for loop\')\n    ActualValue =  test_set_arima[timepoint]\n    #forcast value\n    Prediction = StartARIMAForecasting(Actual, 2,0,0)\n    print(\'count=%d, Actual=%f, Predicted=%f\' % (count_1 ,ActualValue, Prediction))\n    count_1+=1\n    #add it in the list\n    Predictions_ARIMA.append(Prediction)\n    Actual.append(ActualValue)\n\nplot_graph(test_set_arima, Predictions_ARIMA, final_directory, \'ARIMA_200\')\nevaluation_metrices(test_set_arima, Predictions_ARIMA,final_directory,\'ARIMA_200\', evaluation_metrics_file_path)\n\n# =============================================================================\n# Auto ARIMA\n# =============================================================================\n\n\nfrom pmdarima.arima import auto_arima\n#training model\nmodel_auto_arima = auto_arima(train_set_arima, trace=True,start_p=0, start_q=0, start_P=0, start_Q=0,\n                  max_p=2, max_q=2, max_P=2, max_Q=2, seasonal=True,\n                  stepwise=False, suppress_warnings=True, D=1, max_D=2,\n                  error_action=\'ignore\',approximation = False)\n#fitting model\nmodel_auto_arima.fit(train_set_arima)\npredicted_auto_arima = model_auto_arima.predict(100)\n\nprint(type(predicted_auto_arima))\nprint(predicted_auto_arima.shape)\nplt.plot(train_set_arima)\nplt.plot(predicted_auto_arima)\nplt.show()\nplot_graph(test_set_arima, Predictions_ARIMA, final_directory, \'Auto_ARIMA\')\nevaluation_metrices(test_set_arima, Predictions_ARIMA,final_directory,\'Auto_ARIMA\', evaluation_metrics_file_path)\nprint(test_set_arima.shape)\n'"
data_analysis_and_forecasting/model_file.py,6,"b'import numpy as np\nfrom numpy import array\nimport matplotlib.pyplot as plt\nimport os\nimport shutil\n# from sklearn import metrics\n# from sklearn.linear_model import LinearRegression\n# from sklearn.ensemble import *\n# from sklearn.svm import SVR\n# from sklearn.ensemble.forest import RandomForestRegressor\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.neural_network import MLPRegressor\n#\n# from sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport math\n\n# from keras.models import Sequential\n# from keras.layers import Dense, Activation, Flatten\n# from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import Flatten\nfrom keras.layers import ConvLSTM2D\nfrom statsmodels.tsa.arima_model import ARIMA\n\n\ndef make_dataset(dataframe, required_number_of_test_data):\n    dataset = np.array(dataframe)\n    \n    go_for_training = int(len(dataset)-required_number_of_test_data)\n    print(\'go_for_training: \', go_for_training)\n    print(\'required_number_of_test_data: \', required_number_of_test_data)\n    percentage = go_for_training/int(len(dataset))\n    print(\'percentage: \',percentage)\n    \n    NumberOfElements = int(len(dataset) * percentage)\n    print(\'Number of Elements for training: \', NumberOfElements)\n    print(\'dataset length: \', len(dataset))\n\n    train_input = dataset[0:NumberOfElements, 0:-1]\n    print(\'train_input shape: \', train_input.shape)\n    train_output = dataset[0:NumberOfElements, -1]\n    print(\'train_output shape: \', train_output.shape)\n\n    test_input = dataset[NumberOfElements:len(dataset), 0:-1]\n    test_output = dataset[NumberOfElements:len(dataset), -1]\n    \n\n    #test_input = test_input[0:200]\n    #test_output = test_output[0:200]\n    print(\'test_input shape: \', test_input.shape)\n    print(\'test_output shape: \', test_output.shape)\n\n    return train_input, train_output, test_input, test_output\n\n\ndef make_dataset_LSTM(PandaDataframe, required_number_of_test_data):\n    dataset = np.array(PandaDataframe)\n\n    go_for_training = int(len(dataset) - required_number_of_test_data)\n    print(\'go_for_training: \', go_for_training)\n    print(\'required_number_of_test_data: \', required_number_of_test_data)\n    percentage = go_for_training / int(len(dataset))\n    print(\'percentage: \', percentage)\n\n    NumberOfElements = int(len(dataset) * percentage)\n    print(\'dataset length: \', len(dataset))\n    print(\'Number of Elements for training: \', NumberOfElements)\n\n    multiple_ip_train_data = dataset[0:NumberOfElements]\n    multiple_ip_test_set = dataset[NumberOfElements:len(dataset)]\n\n#    multiple_ip_test_set = multiple_ip_test_set[0:200]\n    \n    print(\'LSTM train set: \', multiple_ip_train_data.shape)\n    print(\'LSTM test set: \', multiple_ip_test_set.shape)\n\n    return multiple_ip_train_data, multiple_ip_test_set\n\n\n\ndef make_dataset_arima(PandaDataframe, required_number_of_test_data):\n    dataset = np.array(PandaDataframe)\n\n    go_for_training = int(len(dataset) - required_number_of_test_data)\n    print(go_for_training)\n\n    train_data = dataset[0:go_for_training]\n    test_data = dataset[go_for_training:]\n\n    return train_data, test_data\n\n#Function that calls ARIMA model to fit and forecast the data\ndef StartARIMAForecasting(Actual, P, D, Q):\n#     print(\'from function screaming\')\n    model = ARIMA(Actual, order=(P, D, Q))\n    model_fit = model.fit(disp=0)\n    prediction = model_fit.forecast()[0]\n    return prediction\n\n\n\n#(model_list: object, name: object, train_input: object, train_output: object, test_input: object, test_output: object) -> object\ndef scikit_learn_model(model_list, name, train_input, train_output, test_input, test_output, final_directory,evaluation_metrics_file_path):\n    for idx, i in enumerate(model_list):\n        train_model_1 = i\n        print(\'-------\', name[idx])\n        train_model_1.fit(train_input, train_output)\n        predicted_output = train_model_1.predict(test_input)\n        \n        graph = plot_graph(test_output, predicted_output,final_directory,name[idx])\n        evaluate_model = evaluation_metrices(test_output, predicted_output,final_directory,name[idx],evaluation_metrics_file_path)\n\n\n\ndef plot_graph(test_output, predicted_output, final_directory,subfolder):\n    fig_location = final_directory + \'/\' + str(subfolder)\n\n    if not os.path.exists(fig_location):\n        os.makedirs(fig_location)\n    else:\n        shutil.rmtree(fig_location, ignore_errors=True)\n        os.makedirs(fig_location)\n\n    plt.plot((min(test_output), max(test_output)), (min(predicted_output), max(predicted_output)), color=\'red\')\n    plt.scatter(test_output, predicted_output, color=\'blue\')\n    # plt.savefig(model+\'_\'+\'figure_actual_vs_predicted_with_best_fit_line.jpg\')\n    plt.xlabel(\'Actual output\')\n    plt.ylabel(\'Predicted output\')\n#     plt.title(\'scatter plotting of predicted_output alongside with the average line of test and predicted output\')\n    plt.rcParams[\'figure.figsize\'] =(12,5)\n    plt.savefig(fig_location + \'/\' + ""scatter_test_pred"" + \'.jpg\',bbox_inches=\'tight\')\n#    plt.show()\n    plt.figure()\n\n\n    difference_of_value = predicted_output - test_output\n    print(type(difference_of_value))\n    plt.plot(difference_of_value[:])\n#     plt.title(\'observation of the difference of actual and predicted value\')\n\n    # plt.rcParams[\'xtick.labelsize\']=2\n    # plt.rcParams[\'ytick.labelsize\']=2\n    # plt.tick_params(labelsize=20)\n    plt.ylabel(\'Difference between actual and predicted output\')\n    plt.xlabel(\'Sample number\')\n    plt.grid(b=None, which=\'both\', axis=\'both\')\n    plt.rcParams[\'figure.figsize\'] =(12,5)\n    plt.savefig(fig_location + \'/\' + ""difference_test_pred"" + \'.jpg\',bbox_inches=\'tight\')\n#    plt.show()\n    plt.figure()\n    \n    plt.hist(difference_of_value, bins=20)\n    # plt.xlim(-10,10,1)\n    # plt.savefig(model+\'_\'+\'histogram_of_difference_value.jpg\')\n    plt.xlabel(\'Value\')\n    plt.ylabel(\'Frequency\')\n#     plt.title(\'histogram of value of difference\')\n    plt.rcParams[\'figure.figsize\'] =(12,5)\n    plt.savefig(fig_location + \'/\' + ""error_histogram"" + \'.jpg\',bbox_inches=\'tight\')\n#    plt.show()\n    plt.figure()\n    \n    plt.plot(predicted_output[0:len(predicted_output[0:])], color=\'blue\')\n    plt.plot(test_output[0:], color=\'red\')\n    # plt.xlim(0,40,1)\n    # plt.ylim(50,70,1)\n    # plt.savefig(model+\'_\'+\'figure_difference_between_actual_and_predicted_value.jpg\')\n    plt.xlabel(\'Sample number\')\n    plt.ylabel(\'Actual and Predicted output\')\n#     plt.title(\'Visualization of test and predicted output in the same timestamp\')\n    plt.legend([\'predicted_output\',\'actual_output\'], loc=\'best\')\n    plt.rcParams[\'figure.figsize\'] =(12,5)\n    plt.savefig(fig_location + \'/\' + ""test_and_pred"" + \'.jpg\',bbox_inches=\'tight\')\n#    plt.show()\n    plt.figure()\n    \n    \n    # plot graph by doing 5 minute deiation between actual and predicted output\n    actual_data = range(len(test_output)+5)\n\n#     yhat = predicted_output\n#     y_Conv_Lstm_test = test_output\n\n    plt.plot(predicted_output[0:len(test_output)],color=\'blue\',marker=\'s\', linestyle=\':\')\n    plt.plot(actual_data[5:],test_output[0:len(test_output)],color=\'red\',marker=\'^\', linestyle=\'-.\')\n# plt.plot(yhat[51:99],color=\'black\',marker=\'s\', linestyle=\':\')\n    plt.title(\'deviation between predicted and target value\')\n    plt.ylabel(\'value\')\n    plt.xlabel(\'interval of minutes\')\n    plt.legend([\'predicted\',\'actual_output\'], loc=\'best\')\n    plt.grid(b=None, which=\'both\', axis=\'both\')\n    plt.xticks(np.arange(0,len(test_output)+5,10))\n    plt.xticks( rotation=25)\n    plt.rcParams[\'figure.figsize\'] =(12,5)\n    plt.savefig(fig_location + \'/\' + ""5_min_deviation_between_predicted_and_actual_output"" + \'.jpg\',bbox_inches=\'tight\')\n# plt.show()\n    plt.figure()\n    \n    \n    # plot sactter plot along with actual and predicted output\n    \n    plt.subplot(3,1,1)\n    plt.plot(predicted_output[0:len(predicted_output)], test_output[0:len(test_output)], \'bo\')\n    plt.ylabel(\'value\')\n    plt.xticks(np.arange(min(test_output[0:len(test_output)]),max(test_output[0:len(test_output)]),1))\n# plt.yticks(np.arange(min(yhat[0:60]),max(yhat[0:60]),1))\n    plt.grid(b=None, which=\'both\', axis=\'both\')\n    plt.title(\'actual vs predicted\')\n# plt.xlim(min(y_conv_LSTM_test), max(y_conv_LSTM_test))\n# plt.ylim(min(yhat), max(yhat))\n# plt.tick_params(labelsize=10)\n\n    plt.subplot(3,1,2)\n    plt.plot(predicted_output[0:len(predicted_output)])\n    plt.ylabel(\'value\')\n    plt.grid(b=None, which=\'both\', axis=\'both\')\n    plt.title(\'predicted curve\')\n# plt.tick_params(labelsize=10)\n\n    plt.subplot(3,1,3)\n    plt.plot(test_output[0:len(test_output)])\n    plt.ylabel(\'value\')\n    plt.xlabel(\'range\')\n    plt.grid(b=None, which=\'both\', axis=\'both\')\n    plt.title(\'actual curve\')\n# plt.tick_params(labelsize=10)\n    plt.rcParams[\'figure.figsize\'] =(12,5)\n    plt.savefig(fig_location + \'/\' + ""data_concentration_plot_along_predicted_and_actual_output"" + \'.jpg\',bbox_inches=\'tight\')\n    \n    plt.figure()\n# plt.show()\n\n\ndef evaluation_metrices(test_output,predicted_output,final_directory,model_name,evaluation_metrics_file_path):\n    print(\'r_2 statistic: %.2f\' % r2_score(test_output, predicted_output))\n    print(""Mean_absolute_error: %.2f"" % mean_absolute_error(test_output, predicted_output))\n    print(""Mean squared error: %.2f"" % mean_squared_error(test_output, predicted_output))\n    RMSE = math.sqrt(mean_squared_error(test_output, predicted_output))\n    print(\'RMSE: \', RMSE)\n\n    f = open(evaluation_metrics_file_path, \'a\')\n    f.write(str(model_name)+\'\\n\')\n    f.write(\'r_2 square: \'+str(r2_score(test_output, predicted_output))+\'\\n\')\n    f.write(\'MAE: \'+str(mean_absolute_error(test_output, predicted_output))+\'\\n\')\n    f.write(\'MSE: \'+str(mean_squared_error(test_output, predicted_output))+\'\\n\')\n    f.write(\'RMSE: \'+str(RMSE)+\'\\n\')\n    f.write(\'\\n\')\n    f.close()\n    print(\'!!!!---------------!!!!----------------!!!!\')\n\n\n\ndef NN_model(train_input):\n    NN_model = Sequential()\n    NN_model.add(Dense(128, kernel_initializer=\'normal\',input_dim = train_input.shape[1], activation=\'relu\'))\n    NN_model.add(Dense(256, kernel_initializer=\'normal\',activation=\'relu\'))\n    NN_model.add(Dense(256, kernel_initializer=\'normal\',activation=\'relu\'))\n    NN_model.add(Dense(256, kernel_initializer=\'normal\',activation=\'relu\'))\n#     NN_model.add(Dense(1, kernel_initializer=\'normal\',activation=\'relu\'))\n    NN_model.add(Dense(1))\n    return NN_model\n\ndef LSTM_model(activation_function,time, rows, cols, channels):\n    model = Sequential()\n    # n_seq, 1, n_steps_2, n_features\n    model.add(ConvLSTM2D(filters=64, data_format=\'channels_last\', kernel_size=(1, 2), activation=str(activation_function),\n                   input_shape=(time, rows, cols, channels), return_sequences=False))\n    # model.add(ConvLSTM2D(filters=64,data_format=\'channels_last\', kernel_size=(1,2), activation=str(activation_function)))\n    model.add(Flatten())\n    model.add(Dense(1))\n    \n    return model\n\n\ndef vanilla_lstm(n_steps_vanilla, n_features_vanilla):\n    model = Sequential()\n#     model.add(LSTM(units=100, activation=\'relu\', batch_input_shape=(8,n_steps_vanilla,n_features_vanilla)))\n    model.add(LSTM(units=100, activation=\'relu\', input_shape=(n_steps_vanilla, n_features_vanilla),return_sequences=False)) # make False if use only 1 layer.\n                                                                                                                            # make True if need multi layer\n#     model.add(LSTM(100,return_sequences=True))\n#     model.add(LSTM(100))\n#     model.add(Dropout(0.5))\n    model.add(Dense(1))\n    return model\n\n\ndef split_sequence(sequence, n_steps):\n   X, y = list(), list()\n   for i in range(len(sequence)):\n       # find the end of this pattern\n       end_ix = i + n_steps\n       # check if we are beyond the sequence\n       if end_ix > len(sequence)-1:\n           break\n       # gather input and output parts of the pattern\n       seq_x, seq_y = sequence[i:end_ix,:-1], sequence[end_ix,-1]\n       X.append(seq_x)\n       y.append(seq_y)\n   return array(X), array(y)\n\n\ndef plot_history(train_model_NN, history_graph_NN):\n    # summarize history for accuracy\n    plt.plot(train_model_NN.history[\'loss\'],\'-^\', color = \'green\')\n    plt.plot(train_model_NN.history[\'val_loss\'],\'-o\', color = \'red\')\n\n    # plt.plot(train_model_NN.history[\'mean_squared_error\'],\'-^\', color = \'green\')\n    # plt.plot(train_model_NN.history[\'val_mean_squared_error\'],\'-o\', color = \'red\')\n    # plt.title(\'model accuracy\')\n    plt.ylabel(\'loss\')\n    plt.xlabel(\'epoch\')\n    plt.legend([\'loss\',\'val_loss\'], loc=\'best\')\n    plt.savefig(history_graph_NN+\'_loss_vs_epoch.png\',bbox_inches=\'tight\')\n    plt.rcParams[\'figure.figsize\'] =(12,5)\n    plt.figure()\n'"
