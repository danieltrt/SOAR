file_path,api_count,code
Bayesian/NaiveBayes.py,7,"b'import numpy as np\nfrom collections import defaultdict\n\nclass NaiveBayes:\n\n    def __init__(self):\n        self.class_count = defaultdict(int)\n        self.class_attribute_count = defaultdict(int)\n        self.unique_class = None\n        self.unique_attribute_class = {}\n        self.n_feature = None\n        self.n_sample = None\n        pass\n\n    def fit(self, X, Y):\n        n_sample,n_feature = X.shape\n        self.n_feature = n_feature\n        self.n_sample = n_sample\n        classes = np.unique(Y)\n        self.unique_class = len(classes)\n        for f in range(n_feature):\n            self.unique_attribute_class[f] = len(np.unique(X[:,f]))\n        for c in classes:\n            index = np.where(Y == c)\n            self.class_count[c] = len(index)\n            partial_X = X[index[0]]\n            for f_index in range(n_feature):\n                feature_value = np.unique(partial_X[:,f_index])\n                for f in feature_value:\n                    self.class_attribute_count[(c,f_index,f)] = len(np.where(partial_X[:,f_index]==f))\n\n    def cal_pro(self, sample, c):\n        p = 1.0\n        for i in range(self.n_feature):\n            p *= 1.0*(self.class_attribute_count[(c,i,sample[i])]+1)/(self.class_count[c]+self.unique_attribute_class[i])\n        p *= 1.0*(self.class_count[c]+1)/(self.n_sample+self.unique_class)\n        return p\n\n    def predict(self, X, Y):\n        res = []\n        for sample in X:\n            res.append(np.argmax([self.cal_pro(sample,c) for c in self.class_count.keys()]))\n        print(np.sum([res[i]==Y[i] for i in range(len(res))])/len(res))\n        pass'"
Bayesian/main.py,0,"b""from Bayesian.NaiveBayes import NaiveBayes\nfrom sklearn import datasets\nimport random\n\ndef main():\n    iris = datasets.load_iris()\n    x = iris.data\n    y = iris.target\n    index = [random.randint(0,149) for i in range(100)]\n    test_index = [i for i in range(150) if i not in index]\n    clf = NaiveBayes()\n    clf.fit(x[index],y[index])\n    clf.predict(x[test_index],y[test_index])\n    pass\n\n\nif __name__ == '__main__':\n    main()"""
Cluster/Kmeans.py,6,"b'import numpy as np\n\nclass Kmeans:\n\n    def __init__(self):\n        self.center = None\n        pass\n\n    def distance_func(self, x1, x2):\n        return np.sum([abs(x1[i]-x2[i]) for i in range(x1.shape[-1])])\n\n    def fit(self, X, cluster_number, iteration, threshold):\n        n_sample,n_feature = X.shape\n        index = [np.random.randint(0,n_sample) for i in range(cluster_number)]\n        self.center = X[index]\n\n        for epoch in range(iteration):\n            classes = np.zeros(shape=(n_sample,1))\n            for sample_index in range(n_sample):\n                sample = X[sample_index]\n                distance = float(""inf"")\n                for center_index in range(cluster_number):\n                    center = self.center[center_index]\n                    d = self.distance_func(sample,center)\n                    if d<distance:\n                        distance = d\n                        classes[sample_index] = center_index\n            error = 0.0\n            temp_centers = np.zeros(shape=(cluster_number,n_feature))\n            for c in range(cluster_number):\n                temp_centers[c] = np.mean(X[np.where(classes==c)[0]],axis=0)\n                error += self.distance_func(temp_centers[c],self.center[c])\n            if error>threshold:\n                self.center = temp_centers\n            # print(self.center)\n\n    def predict(self, X, Y):\n        n_sample = X.shape[0]\n        res = np.zeros(shape=(n_sample,1))\n        for index in range(n_sample):\n            sample = X[index]\n            distance = float(""inf"")\n            for c_index in range(len(self.center)):\n                d = self.distance_func(sample,self.center[c_index])\n                if d<distance:\n                    distance = d\n                    res[index] = c_index\n        return res\n\n'"
Cluster/MixtureGaussian.py,14,"b'import math\nimport numpy as np\n\nclass MixtureGaussian:\n\n    def __init__(self):\n        self.weight = None\n        self.means = None\n        self.covs = None\n        pass\n\n    def distance_func(self, x1, x2):\n        return np.sum([abs(x1[i]-x2[i]) for i in range(x1.shape[-1])])\n\n    def expect(self, X, cluster_index):\n        n_sample, n_feature = X.shape\n        mean = self.means[cluster_index]\n        cov = self.covs[cluster_index]\n        ps = np.zeros(shape=(n_sample,1))\n        det = np.linalg.det(cov)\n        for i,sample in enumerate(X):\n            coefficient = 1/(math.pow(2*math.pi,0.5*n_feature) * math.sqrt(det))\n            p = coefficient * np.exp(-0.5 * (sample-mean).T @ np.linalg.inv(cov) @ (sample-mean))\n            ps[i][0] = p\n        return ps\n\n    def maximum(self, X, gammas, cluster_index):\n        gamma = (gammas[:,cluster_index]).reshape(-1,1)\n        self.means[cluster_index] = np.sum(gamma * X, axis=0) / np.sum(gamma)\n        self.covs[cluster_index] = (gamma*(X-self.means[cluster_index])).T @ \\\n                                   (X-self.means[cluster_index]) / np.sum(gamma)\n        self.weight[cluster_index] = 1/X.shape[0] * np.sum(gamma)\n\n    def fit(self, X, n_component, iteration):\n        n_sample ,n_feature = X.shape\n        self.weight = (1/n_component)*np.ones(shape=(n_component,1))\n        self.means = X[[np.random.randint(0,n_sample) for i in range(n_component)]]\n        self.covs = np.array([np.cov(X,rowvar=False) for i in range(n_component)])\n\n        for epoch in range(iteration):\n            gammas = np.zeros(shape=(n_sample, n_component))\n            for cluster_index in range(n_component):\n                gammas[:, cluster_index] = np.reshape(self.weight[cluster_index][0] * self.expect(X, cluster_index),\n                                                      (n_sample,))\n            temp = np.sum(gammas,axis=1).reshape(n_sample,1)\n            gammas /= temp\n\n            for cluster_index in range(n_component):\n                self.maximum(X, gammas, cluster_index)\n\n    def predict(self, X):\n        n_sample = X.shape[0]\n        res = np.zeros(shape=(n_sample, 1))\n        for index in range(n_sample):\n            sample = X[index]\n            distance = float(""inf"")\n            for c_index in range(len(self.means)):\n                d = self.distance_func(sample, self.means[c_index])\n                if d < distance:\n                    distance = d\n                    res[index] = c_index\n        print(res)\n        return res\n'"
Cluster/main.py,0,"b'import matplotlib.pyplot as plt\nfrom Cluster.Kmeans import Kmeans\nfrom Cluster.MixtureGaussian import MixtureGaussian\nfrom LinearDiscriminantAnalysis.LDA import LDA\nfrom sklearn import datasets\nfrom sklearn import preprocessing\n\n\ndef main():\n    iris = datasets.load_iris()\n    X = iris.data\n    Y = iris.target\n    mm = preprocessing.MinMaxScaler()\n    mm.fit_transform(X)\n    lda = LDA()\n    lda.fit(X,Y)\n    res = lda.transform(X,2)\n    kmeans = Kmeans()\n    kmeans.fit(res,cluster_number=3,iteration=2000,threshold=1e-9)\n    cluster_res = kmeans.predict(res,Y)\n    plt.subplot(2, 1, 1)\n    for i in range(len(X)):\n        if cluster_res[i] == 0:\n            plt.scatter(x=res[i][0],y=res[i][1],color=""red"")\n        if cluster_res[i] == 1:\n            plt.scatter(x=res[i][0],y=res[i][1],color=""blue"")\n        if cluster_res[i] == 2:\n            plt.scatter(x=res[i][0],y=res[i][1],color=""green"")\n    plt.title(""Kmeans"")\n    plt.xticks([])\n    plt.yticks([])\n\n    mg = MixtureGaussian()\n    mg.fit(res, n_component=3, iteration=2000)\n    cluster_res = mg.predict(res)\n    plt.subplot(2,1,2)\n    for i in range(len(X)):\n        if cluster_res[i] == 0:\n            plt.scatter(x=res[i][0],y=res[i][1],color=""red"")\n        if cluster_res[i] == 1:\n            plt.scatter(x=res[i][0],y=res[i][1],color=""blue"")\n        if cluster_res[i] == 2:\n            plt.scatter(x=res[i][0],y=res[i][1],color=""green"")\n    plt.title(""MixtureGaussian"")\n    plt.xticks([])\n    plt.yticks([])\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
DimensionReduce/KPCA.py,2,"b'import numpy as np\n\n\nclass KPCA:\n\n    def __init__(self):\n        self.alpha = None\n\n    def high_dimension_map(self, x):\n        """"""\n        feel free to try other kernel functions\n        """"""\n        return x*x\n\n    def kernal_func(self, x1, x2):\n        return self.high_dimension_map(x1).T @ self.high_dimension_map(x2)\n\n    def fit(self, X, n_component):\n        X = X.T # n*m\n        K = self.kernal_func(X, X)\n        print(K.shape)\n        eigen_value , eigen_vector = np.linalg.eig(K)\n        index = np.argsort(eigen_value)[::-1]\n        self.alpha = (eigen_vector[index[:n_component]]).T\n\n    def transform(self, X):\n        X = X.T # n*m\n        print(self.high_dimension_map(X).shape)\n        print(self.alpha.shape)\n        print(X.shape)\n        return ((self.high_dimension_map(X) @ self.alpha).T @ X).T\n\n'"
DimensionReduce/PCA.py,3,"b'import numpy as np\n\n\nclass PCA:\n\n    def __init__(self):\n        self.w = None\n        pass\n\n    def fit(self, X, n_component):\n        X = X.T\n        mean = np.mean(X,axis=0)\n        X = X-mean\n        eigen_value,eigen_vector = np.linalg.eig(X @ X.T)\n        index = np.argsort(eigen_value)[::-1]\n        self.w = eigen_vector[index[:n_component]]\n\n    def transform(self, X):\n        X = X.T\n        return (self.w @ X).T\n'"
DimensionReduce/main.py,0,"b'import matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom DimensionReduce.PCA import PCA\nfrom DimensionReduce.KPCA import KPCA\n\n\ndef main():\n    iris = datasets.load_iris()\n    X = iris.data\n    Y = iris.target\n    plt.subplot(1,2,1)\n    pca = PCA()\n    pca.fit(X, n_component=2)\n    result = pca.transform(X)\n    for i in range(len(result)):\n        if Y[i] == 0:\n            plt.scatter(x=result[i][0],y=result[i][1],color=""red"")\n        if Y[i] == 1:\n            plt.scatter(x=result[i][0],y=result[i][1],color=""blue"")\n        if Y[i] == 2:\n            plt.scatter(x=result[i][0],y=result[i][1],color=""green"")\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(""PCA result"")\n    plt.subplot(1,2,2)\n    kpca = KPCA()\n    kpca.fit(X, n_component=2)\n    result = kpca.transform(X)\n    for i in range(len(result)):\n        if Y[i] == 0:\n            plt.scatter(x=result[i][0],y=result[i][1],color=""red"")\n        if Y[i] == 1:\n            plt.scatter(x=result[i][0],y=result[i][1],color=""blue"")\n        if Y[i] == 2:\n            plt.scatter(x=result[i][0],y=result[i][1],color=""green"")\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(""KPCA result"")\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    main()\n\n'"
Ensemble/Adaboost.py,13,"b'import numpy as np\n\n\nclass Stump:\n\n    def __init__(self):\n        self.split_feature = None\n        self.split_value = None\n        pass\n\n    def predict(self, X, Y):\n        res = []\n        for sample in X:\n            if sample[self.split_feature]>=self.split_value:\n                res.append(1)\n            else:\n                res.append(-1)\n        print(np.sum([res[i]==Y[i] for i in range(len(Y))])/len(Y))\n        return np.array(res).reshape(-1,1)\n\n\nclass Adaboost:\n\n    def __init__(self):\n        self.n_sample = None\n        self.n_feature = None\n        self.clfs = []\n        self.weight = None\n        self.clf_weight = []\n        pass\n\n    def fit(self, X, Y, classifier_number,threshold):\n        self.n_sample, self.n_feature = X.shape\n        self.weight = np.array([1/self.n_sample for i in range(self.n_sample)]).reshape(self.n_sample,1)\n        self.clf_weight = [0 for i in range(classifier_number)]\n        for i in range(classifier_number):\n            clf = Stump()\n            current_error = float(""inf"")\n            for f in range(self.n_feature):\n                feature = X[:,f]\n                unique_value = np.unique(feature)\n                for v in unique_value:\n                    prediction = np.ones((self.n_sample,1))\n                    prediction[feature < v] = -1\n                    fake_pre = np.array([0 if prediction[i] == Y[i] else 1 for i in range(self.n_sample)]).reshape(self.n_sample,1)\n                    error = np.sum(fake_pre * self.weight)\n                    if error<current_error:\n                        current_error = error\n                        clf.split_feature = f\n                        clf.split_value = v\n                    if current_error<threshold:\n                        break\n                if current_error<threshold:\n                    break\n            self.clfs.append(clf)\n            res = clf.predict(X, Y)\n            self.clf_weight[i] = 0.5*np.log((1-error)/(error+1e-7))\n            self.weight = self.weight*np.exp(Y*res.reshape(self.n_sample,1)*(-self.clf_weight[i]))\n            reg = np.sum(self.weight)\n            self.weight /= reg\n\n    def predict(self, X, Y):\n        ensemble_res = np.zeros((X.shape[0],1))\n        for clf_index in range(len(self.clfs)):\n            clf = self.clfs[clf_index]\n            res = self.clf_weight[clf_index] * clf.predict(X, Y)\n        ensemble_res+=res\n        predict_y = np.sign(ensemble_res)\n        print(np.sum([predict_y[i]==Y[i] for i in range(len(predict_y))])/len(predict_y))\n'"
Ensemble/RandomForest.py,8,"b'import numpy as np\nimport random\nfrom Ensemble.Adaboost import Stump\nimport matplotlib.pyplot as plt\n\n\nclass RandomForest:\n\n    def __init__(self):\n        self.clfs = []\n        pass\n\n    def bootstrap(self, X, m):\n        n_sample = X.shape[0]\n        index = [random.randint(0,n_sample-1) for i in range(m)]\n        return index\n\n    def fit(self, X, Y, n_models,k_feature):\n        n_sample, n_feature = X.shape\n        for i in range(n_models):\n            clf = Stump()\n            bootstrap_index = self.bootstrap(X,int(0.1*n_sample))\n            input_X, input_Y = X[bootstrap_index], Y[bootstrap_index]\n            feature_index = random.sample(range(0,n_feature-1),k_feature)\n            min_error = float(""inf"")\n            for f in feature_index:\n                unique_values = np.unique(input_X[:,f])\n                for v in unique_values:\n                    prediction = np.ones(input_Y.shape)\n                    prediction[input_X[:, f] <= v] = -1\n                    error = np.sum([prediction[i]!=input_Y[i] for i in range(len(input_Y))])/len(input_Y)\n                    if error < min_error:\n                        min_error = error\n                        clf.split_feature = f\n                        clf.split_value = v\n            self.clfs.append(clf)\n\n    def predict(self, X, Y):\n        prediction = np.zeros(Y.shape)\n        basic_res = []\n        for clf in self.clfs:\n            res = clf.predict(X, Y)\n            prediction+=res\n            basic_res.append(np.sum([res[i]==Y[i] for i in range(len(Y))])/len(Y))\n        prediction = np.sign(prediction)\n        # print(np.sum([prediction[i]==Y[i] for i in range(len(Y))])/len(Y))\n        figure = plt.figure()\n        plt.plot(basic_res,color=""red"",label=""basic_classifier"")\n        plt.plot([np.sum([prediction[i]==Y[i] for i in range(len(Y))])/len(Y)]*(len(basic_res)),color=""blue"",\n                 label=""random_forest"")\n        plt.legend()\n        plt.show()\n'"
Ensemble/main.py,9,"b'import numpy as np\nimport random\nimport struct\nfrom Ensemble.Adaboost import Adaboost\nfrom Ensemble.RandomForest import RandomForest\nfrom sklearn import datasets\n\n\ndef load_data():\n    for file in [""train-images.idx3-ubyte"", ""train-labels.idx1-ubyte"", ""t10k-images.idx3-ubyte"",\n                 ""t10k-labels.idx1-ubyte""]:\n        filepath = ""../dataset/""+file\n        data = open(filepath,""rb"").read()\n        if ""image"" in filepath:\n            fmt = "">4i""\n            offset = 0\n            magic_number,image_number,row,column = struct.unpack_from(fmt,data,offset)\n            offset += struct.calcsize(fmt)\n            fmt = "">{}B"".format(row*column)\n            if ""train"" in filepath:\n                train_X = np.empty((image_number,row,column))\n                for i in range(image_number):\n                    train_X[i] = np.array(struct.unpack_from(fmt,data,offset)).reshape(row,column)\n                    offset += struct.calcsize(fmt)\n            else:\n                test_X = np.empty((image_number,row,column))\n                for i in range(image_number):\n                    test_X[i] = np.array(struct.unpack_from(fmt,data,offset)).reshape(row,column)\n                    offset += struct.calcsize(fmt)\n        else:\n            fmt = "">2i""\n            offset = 0\n            magic_number,image_number = struct.unpack_from(fmt,data,offset)\n            offset += struct.calcsize(fmt)\n            fmt = "">B""\n            if ""train"" in filepath:\n                train_Y = np.empty(image_number)\n                for i in range(image_number):\n                    train_Y[i] = struct.unpack_from(fmt,data,offset)[0]\n                    offset += struct.calcsize(fmt)\n            else:\n                test_Y = np.empty(image_number)\n                for i in range(image_number):\n                    test_Y[i] = struct.unpack_from(fmt,data,offset)[0]\n                    offset += struct.calcsize(fmt)\n    return train_X,train_Y,test_X,test_Y\n\ndef main():\n    train_X, train_Y, test_X, test_Y = load_data()\n    train_X = (train_X.reshape((train_X.shape[0], train_X.shape[1] * train_X.shape[2])))\n    train_Y = train_Y.reshape((train_Y.shape[0], 1))\n    test_X = (test_X.reshape((test_X.shape[0], test_X.shape[1] * test_X.shape[2])))\n    test_Y = test_Y.reshape((test_Y.shape[0], 1))\n    index = np.where(train_Y<=1)[0]\n    train_X = train_X[index]\n    train_Y = train_Y[index]\n    train_Y[train_Y==0] = -1\n    train_index = [random.randint(0,train_X.shape[0]-1) for i in range(1000)]\n    test_index = [random.randint(0,train_X.shape[0]-1) for i in range(1000)]\n    # clf = Adaboost()\n    # clf.fit(train_X[train_index],train_Y[train_index],10,threshold=0.1)\n    # clf.predict(train_X[test_index],train_Y[test_index])\n\n    clf = RandomForest()\n    clf.fit(train_X[train_index],train_Y[train_index],100,10)\n    clf.predict(train_X[test_index],train_Y[test_index])\n    # iris = datasets.load_iris()\n    # X = iris.data\n    # Y = iris.target\n    # index = np.where(Y<=1)[0]\n    # X = X[index]\n    # Y = Y[index]\n    # Y[Y == 0] = -1\n    # Y = np.expand_dims(Y,axis=1)\n    # clf = Adaboost()\n    # clf.fit(X,Y,10)\n    # clf.predict(X,Y)\n\n\nif __name__ == \'__main__\':\n    main()'"
KNearstNeighbors/KNN.py,2,"b'import numpy as np\nimport heapq\nfrom collections import Counter\n\n\nclass kdTreeNode:\n\n    def __init__(self, sample, target, feature_index, split_value):\n        self.sample = sample\n        self.target = target\n        self.split_feature = feature_index\n        self.split_value = split_value\n        self.left = None\n        self.right = None\n\n\nclass KNN:\n\n    def __init__(self):\n        self.n_feature = None\n        self.kdTree = None\n        self.heap = []\n        self.K = None\n\n    def distance_func(self, x1, x2):\n        return np.sum([abs(x1[i]-x2[i]) for i in range(x1.shape[-1])])\n\n    def construct_kdTree(self, X, Y, feature_index):\n        if X is None or len(X) == 0:\n            return\n        if len(X) == 1:\n            return kdTreeNode(X[0], Y[0], None, None)\n        feature = X[:,feature_index]\n        index = [(i,feature[i]) for i in range(len(X))]\n        index.sort(key=lambda x: x[1])\n        index.sort(key=lambda x: x[1])\n        median, median_value = index[len(index)//2]\n        root = kdTreeNode(X[median],Y[median], feature_index, median_value)\n        root.left = self.construct_kdTree(X[:median], Y[:median], (feature_index+1) % self.n_feature)\n        root.right = self.construct_kdTree(X[median+1:], Y[median+1:], (feature_index+1) % self.n_feature)\n        return root\n\n    def search_kdTree(self, sample, node):\n        if not node:\n            return\n        if not node.left and not node.right:\n            if len(self.heap)<self.K:\n                heapq.heappush(self.heap,(-self.distance_func(sample,node.sample), node.target))\n            else:\n                d, target = heapq.heappop(self.heap)\n                if -self.distance_func(sample,node.sample)>d:\n                    heapq.heappush(self.heap,(-self.distance_func(sample,node.sample), node.target))\n                else:\n                    heapq.heappush(self.heap,(d,target))\n            return\n        else:\n            if node.right and sample[node.split_feature]>node.split_value:\n                self.search_kdTree(sample, node.right)\n\n                if len(self.heap) < self.K:\n                    heapq.heappush(self.heap, (-self.distance_func(sample, node.sample), node.target))\n                else:\n                    d, target = heapq.heappop(self.heap)\n                    if -self.distance_func(sample, node.sample) > d:\n                        heapq.heappush(self.heap, (-self.distance_func(sample, node.sample), node.target))\n                    else:\n                        heapq.heappush(self.heap, (d, target))\n\n                tag = False\n                if len(self.heap)<self.K:\n                    tag = True\n                else:\n                    d, target = heapq.heappop(self.heap)\n                    if node.left and -self.distance_func(sample,node.left.sample)>d:\n                        tag = True\n                if tag:\n                    self.search_kdTree(sample, node.left)\n                return\n            else:\n                self.search_kdTree(sample, node.left)\n\n                if len(self.heap) < self.K:\n                    heapq.heappush(self.heap, (-self.distance_func(sample, node.sample), node.target))\n                else:\n                    d, target = heapq.heappop(self.heap)\n                    if -self.distance_func(sample, node.sample) > d:\n                        heapq.heappush(self.heap, (-self.distance_func(sample, node.sample), node.target))\n                    else:\n                        heapq.heappush(self.heap, (d, target))\n\n                tag = False\n                if len(self.heap) < self.K:\n                    tag = True\n                else:\n                    d, target = heapq.heappop(self.heap)\n                    if node.right and -self.distance_func(sample, node.right.sample) > d:\n                        tag = True\n                if tag:\n                    self.search_kdTree(sample, node.right)\n                return\n\n    def fit(self, X, Y):\n        n_sample,n_feature = X.shape\n        self.n_feature = n_feature\n        self.kdTree = self.construct_kdTree(X, Y, 0)\n\n    def predict(self, X, Y, K):\n        self.K = K\n        res = np.zeros(shape=Y.shape)\n        for i,sample in enumerate(X):\n            self.heap = []\n            heapq.heapify(self.heap)\n            self.search_kdTree(sample,self.kdTree)\n            c = Counter(ele[1] for ele in self.heap)\n            kk = None\n            vv = -float(""inf"")\n            for k,v in c.items():\n                if v>vv:\n                    kk = k\n            res[i] = kk\n        # print(res)\n        # print(Y)\n        return sum(res[i]==Y[i] for i in range(len(Y)))/len(Y)\n'"
KNearstNeighbors/main.py,2,"b'import numpy as np\nfrom sklearn import datasets\nfrom sklearn.preprocessing import MinMaxScaler\nfrom KNearstNeighbors.KNN import KNN\nimport matplotlib.pyplot as plt\n\n\ndef main():\n    iris = datasets.load_iris()\n    X = iris.data\n    Y = iris.target\n    mm = MinMaxScaler()\n    X = mm.fit_transform(X)\n    train_index = [np.random.randint(0,50) for i in range(35)] + [np.random.randint(50,100) for i in range(35)]\\\n                  + [np.random.randint(100, 150) for i in range(35)]\n    test_index = [i for i in range(150) if i not in train_index]\n    clf = KNN()\n    clf.fit(X[train_index],Y[train_index])\n    figure = plt.figure()\n    plt.plot([clf.predict(X[test_index], Y[test_index], k) for k in range(1,11)])\n    plt.xlim(1,10)\n    plt.xlabel(""neighbor number"")\n    plt.ylabel(""test accuracy"")\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    main()'"
LinearDiscriminantAnalysis/LDA.py,12,"b'import numpy as np\n\n\nclass LDA:\n\n    def __init__(self):\n        self.matrix = None\n        pass\n\n    def fit(self,x,y):\n        classes = np.unique(y)\n        features = x.shape[1]\n\n        # calculate within scatter\n        sw = np.zeros(shape=(features,features))\n        for c in classes:\n            index = np.where(y == c)[0]\n            cur_x = x[index]\n            sw += (cur_x.shape[0]-1)*np.cov(cur_x, rowvar=0)\n\n        # calculate between scatter\n        total_mean = np.mean(x,axis=0)\n        sb = np.zeros(shape=(features,features))\n        for c in classes:\n            index = np.where(y == c)[0]\n            cur_x = x[index]\n            cur_mean = np.mean(cur_x,axis=0)\n            sb += cur_x.shape[0]*((cur_mean-total_mean).reshape(cur_mean.shape[0], 1) @\n                                  np.transpose((cur_mean-total_mean).reshape(cur_mean.shape[0], 1)))\n        self.matrix = np.linalg.inv(sw)@sb\n\n    def transform(self,x,n_component):\n        eigenvalue,eigenvector = np.linalg.eigh(self.matrix)\n        index = np.argsort(eigenvalue)[::-1]\n        w = eigenvector[index[:n_component]]\n        return x @ w.T\n\n'"
LinearDiscriminantAnalysis/main.py,0,"b'import matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom LinearDiscriminantAnalysis.LDA import LDA\n\n\ndef main():\n    iris = datasets.load_iris()\n    X = iris.data\n    Y = iris.target\n    lda = LDA()\n    lda.fit(X,Y)\n    result = lda.transform(X,n_component=2)\n    figure = plt.figure()\n    for i in range(len(result)):\n        if Y[i] == 0:\n            plt.scatter(x=result[i][0],y=result[i][1],color=""red"")\n        if Y[i] == 1:\n            plt.scatter(x=result[i][0],y=result[i][1],color=""blue"")\n        if Y[i] == 2:\n            plt.scatter(x=result[i][0],y=result[i][1],color=""green"")\n    plt.show(figure)\n\n\nif __name__ == \'__main__\':\n    main()'"
LinearRegression/LassoRegression.py,10,"b'import numpy as np\n\n\nclass LassoRegression:\n\n    def __init__(self):\n        self.weight = None\n\n    def fit(self, X, Y, lambd, iteration):\n        n_sample = X.shape[0]\n        ones = np.ones(shape=(n_sample,1))\n        X = np.concatenate([X,ones],axis=1)\n        Y = Y.reshape(-1,1)\n        n_sample, n_feature = X.shape # n,m\n        self.weight = np.random.normal(0.0,1.0,size=(n_feature,1))\n        for epoch in range(iteration):\n            for k in range(n_feature):\n                temp_X = np.concatenate([X[:,:k],X[:,k+1:]],axis=1)\n                if k==0:\n                    temp_w = self.weight[k+1:]\n                elif k==n_feature-1:\n                    temp_w = self.weight[:k]\n                else:\n                    # print(k)\n                    # print(self.weight[:k].shape)\n                    # print(self.weight[k+1:].shape)\n                    # print(np.concatenate([self.weight[:k],self.weight[k+1:]],axis=0))\n                    temp_w = np.concatenate([self.weight[:k],self.weight[k+1:]],axis=0)\n                x = X[:,k].reshape(-1,1)\n                # print(Y-temp_X @ temp_w)\n                px = -2*np.sum(x*(Y-temp_X @ temp_w))\n                mx = 2 * np.sum(np.power(x,2))\n                if px>lambd:\n                    self.weight[k][0] = -(px-lambd)/mx\n                elif px<lambd:\n                    self.weight[k][0] = -(px+lambd)/mx\n                else:\n                    self.weight[k][0] = 0.0\n\n    def predict(self, X):\n        n_sample = X.shape[0]\n        ones = np.ones(shape=(n_sample,1))\n        X = np.concatenate([X,ones],axis=1)\n        prediction = X @ self.weight\n        return prediction'"
LinearRegression/LogitRegression.py,8,"b'import numpy as np\n\n\nclass LogitRegression:\n\n    def __init__(self, x, y, learning_rate, epoch):\n        self.x = x\n        self.y = y\n        self.learning_rate = learning_rate\n        self.epoch = epoch\n        temp = np.ones((self.x.shape[0], 1))\n        self.x = np.concatenate((self.x, temp),axis=1)\n        self.theta = np.random.normal(loc=0.0, scale=1.0, size=(self.x.shape[1], 1))\n        self.x_transpose = np.transpose(self.x)\n\n    def cal_gradient(self):\n        temp = (1/(1+np.exp(-np.matmul(self.x, self.theta))))-self.y\n        gradients = np.matmul(self.x_transpose, temp)\n        return gradients\n\n    def fit(self):\n        for step in range(self.epoch):\n            gradients = (1/self.x.shape[0])*self.cal_gradient()\n            self.theta -= gradients*self.learning_rate\n\n    def predict(self,test_x):\n        test_x = np.concatenate((test_x,np.ones((test_x.shape[0],1))),axis=1)\n        return 1/(1+np.exp(-(test_x @ self.theta)))\n\n'"
LinearRegression/MultiLogitRegression.py,8,"b'import numpy as np\nfrom LinearRegression.LogitRegression import LogitRegression\n\n\nclass MultiLogitRegression:\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n        self.classes = np.unique(self.y)\n        self.index = {}\n        for i in self.classes:\n            self.index[i] = np.where(y == i)[0]\n        self.classifier = {}\n\n    def fit(self):\n        for i in range(len(self.classes)):\n            for j in range(i+1,len(self.classes)):\n                print(""modeling {}th and {}th class"".format(i,j))\n                x = np.concatenate((self.x[self.index[self.classes[i]]],self.x[self.index[self.classes[j]]]),axis=0)\n                y = np.concatenate((np.ones(shape=self.index[self.classes[i]].shape),\n                                    np.zeros(shape=self.index[self.classes[j]].shape)), axis=0)\n                y = y.reshape((y.shape[0],1))\n                lcf = LogitRegression(x, y, 0.001, 5000)\n                lcf.fit()\n                self.classifier[(i,j)] = lcf\n\n    def predict(self, x):\n        temp = np.zeros(shape=(len(self.classifier),x.shape[0]))\n        for k,lcf in self.classifier.items():\n            i,j = k\n            result = lcf.predict(x)\n            result = result.reshape((result.shape[0],))\n            temp[i] += result\n            temp[j] += (1-result)\n        class_index = np.argmax(temp,axis=0)\n        return np.array([self.classes[i] for i in class_index])\n'"
LinearRegression/main.py,7,"b'import numpy as np\nimport struct\nfrom LinearRegression.MultiLogitRegression import MultiLogitRegression\nfrom LinearRegression.LassoRegression import LassoRegression\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndef load_data():\n    for file in [""train-images.idx3-ubyte"", ""train-labels.idx1-ubyte"", ""t10k-images.idx3-ubyte"",\n                 ""t10k-labels.idx1-ubyte""]:\n        filepath = ""../dataset/""+file\n        data = open(filepath,""rb"").read()\n        if ""image"" in filepath:\n            fmt = "">4i""\n            offset = 0\n            magic_number,image_number,row,column = struct.unpack_from(fmt,data,offset)\n            offset += struct.calcsize(fmt)\n            fmt = "">{}B"".format(row*column)\n            if ""train"" in filepath:\n                train_X = np.empty((image_number,row,column))\n                for i in range(image_number):\n                    train_X[i] = np.array(struct.unpack_from(fmt,data,offset)).reshape(row,column)\n                    offset += struct.calcsize(fmt)\n            else:\n                test_X = np.empty((image_number,row,column))\n                for i in range(image_number):\n                    test_X[i] = np.array(struct.unpack_from(fmt,data,offset)).reshape(row,column)\n                    offset += struct.calcsize(fmt)\n        else:\n            fmt = "">2i""\n            offset = 0\n            magic_number,image_number = struct.unpack_from(fmt,data,offset)\n            offset += struct.calcsize(fmt)\n            fmt = "">B""\n            if ""train"" in filepath:\n                train_Y = np.empty(image_number)\n                for i in range(image_number):\n                    train_Y[i] = struct.unpack_from(fmt,data,offset)[0]\n                    offset += struct.calcsize(fmt)\n            else:\n                test_Y = np.empty(image_number)\n                for i in range(image_number):\n                    test_Y[i] = struct.unpack_from(fmt,data,offset)[0]\n                    offset += struct.calcsize(fmt)\n    return train_X,train_Y,test_X,test_Y\n\n\ndef linearregression(train_X,train_Y,test_X,test_Y):\n    lcf = MultiLogitRegression(train_X, train_Y)\n    lcf.fit()\n    result = lcf.predict(test_X)\n    print(np.sum(np.array([1 if result[i] == test_Y[i][0] else 0 for i in range(len(result))]))/len(result))\n\n\ndef main():\n    # train_X, train_Y, test_X, test_Y = load_data()\n    # train_X = (1/255)*(train_X.reshape((train_X.shape[0],train_X.shape[1]*train_X.shape[2])))\n    # train_Y = train_Y.reshape((train_Y.shape[0],1))\n    # test_X = (1/255)*(test_X.reshape((test_X.shape[0],test_X.shape[1]*test_X.shape[2])))\n    # test_Y = test_Y.reshape((test_Y.shape[0],1))\n    # linearregression(train_X,train_Y,test_X,test_Y)\n    boston = datasets.load_boston()\n    X = boston.data\n    # mm = MinMaxScaler()\n    # X = mm.fit_transform(X)\n    Y = boston.target\n    lasso = LassoRegression()\n    lasso.fit(X,Y,lambd=0.2,iteration=1000)\n    pre = lasso.predict(X)\n    figure = plt.figure()\n    plt.plot(Y,label=""True Value"")\n    plt.plot(pre,label=""Predict Value"")\n    plt.legend()\n    plt.show()\n\n\n\n\nif __name__ == \'__main__\':\n    main()\n'"
NeuralNetwork/BPNetwork.py,14,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\n\nclass BPNetwork:\n\n    def __init__(self):\n        self.weight = {}\n        self.theta = {}\n        self.weight_layer = []\n        self.theta_layer = []\n        self.hidden_value = {}\n        pass\n\n    def activate_func(self, X):\n        return 1.0/(1+np.exp(-X))\n\n    def forward(self, X):\n        for i in range(len(self.weight_layer)):\n            self.hidden_value[self.weight_layer[i]] = np.mean(X,axis=0).reshape(1,-1)\n            # print(X)\n            X = X @ self.weight[self.weight_layer[i]]\n            X = X - self.theta[self.theta_layer[i]]\n            # print(X)\n            X = self.activate_func(X)\n            # print(X)\n        return X\n\n    def backward(self, Y, Y_hat, learning_rate):\n        # print(self.theta[self.theta_layer[0]])\n        # print(self.weight[self.weight_layer[0]])\n        # print(self.weight[self.weight_layer[0]].shape)\n        # print(Y_hat)\n        # print(Y)\n        g = (Y_hat @ (1-Y_hat).T) @ (Y-Y_hat)\n        # print(g)\n        self.theta[self.theta_layer[-1]] -= learning_rate*g\n        self.weight[self.weight_layer[-1]] += learning_rate*(self.hidden_value[self.weight_layer[-1]].T @ g)\n\n        for i in range(len(self.weight_layer)-2,-1,-1):\n            g = self.hidden_value[self.weight_layer[i+1]] @ (1-self.hidden_value[self.weight_layer[i+1]]).T @ \\\n                g @ self.weight[self.weight_layer[i+1]].T\n            self.theta[self.theta_layer[i]] -= learning_rate*g\n            self.weight[self.weight_layer[i]] += learning_rate*(self.hidden_value[self.weight_layer[i]].T @ g)\n\n    def fit(self, X, Y, layers, hidden_number, iteration, batch_size, learing_rate):\n        n_sample, n_feature = X.shape\n        xavier = 1/np.sqrt(n_feature)\n        unique_value = np.unique(Y)\n        out_put_size = len(unique_value)\n\n        self.weight[""input_hidden_1""] = np.random.uniform(-xavier,xavier,[n_feature,hidden_number])\n        self.weight_layer.append(""input_hidden_1"")\n        for i in range(layers-1):\n            self.weight[""hidden_""+str(i+1)+""_hidden_""+str(i+2)] = np.random.uniform(-xavier,xavier,\n                                                                                    [hidden_number,hidden_number])\n            self.weight_layer.append(""hidden_""+str(i+1)+""_hidden_""+str(i+2))\n        self.weight[""hidden_""+str(layers)+""_output""] = np.random.uniform(-xavier,xavier,[hidden_number,out_put_size])\n        self.weight_layer.append(""hidden_""+str(layers)+""_output"")\n\n        for i in range(1,layers+1):\n            self.theta[""hidden_""+str(i)] = np.zeros(shape=(1,hidden_number))\n            self.theta_layer.append(""hidden_""+str(i))\n        self.theta[""output""] = np.zeros(shape=(1,out_put_size))\n        self.theta_layer.append(""output"")\n\n        figure = plt.figure()\n        for epoch in range(iteration):\n            if epoch%500==0:\n                print(""in the epoch {}"".format(epoch))\n            for offset in range(0,n_sample,batch_size):\n                # print(offset)\n                x = X[offset:offset+batch_size]\n                y = Y[offset:offset+batch_size]\n                accumulate_y = np.zeros(shape=(1, out_put_size))\n\n                for v in np.unique(y):\n                    accumulate_y[0][int(v)] = len(y[y == v])\n                res = np.mean(self.forward(x), axis=0).reshape(1,-1)\n                accumulate_y /= len(y)\n                self.backward(accumulate_y,res,learing_rate)\n            plt.scatter(epoch,np.sum(res-accumulate_y),)\n        plt.show()\n\n    def predict(self, X, Y):\n        res = [np.argmax(self.forward(sample),axis=1) for sample in X]\n        print(sum(res[i]==Y[i] for i in range(len(res)))/len(res))\n        pass\n'"
NeuralNetwork/main.py,6,"b'import struct\nimport numpy as np\nfrom NeuralNetwork.BPNetwork import BPNetwork\n\n\ndef load_data():\n    for file in [""train-images.idx3-ubyte"", ""train-labels.idx1-ubyte"", ""t10k-images.idx3-ubyte"",\n                 ""t10k-labels.idx1-ubyte""]:\n        filepath = ""../dataset/""+file\n        data = open(filepath,""rb"").read()\n        if ""image"" in filepath:\n            fmt = "">4i""\n            offset = 0\n            magic_number,image_number,row,column = struct.unpack_from(fmt,data,offset)\n            offset += struct.calcsize(fmt)\n            fmt = "">{}B"".format(row*column)\n            if ""train"" in filepath:\n                train_X = np.empty((image_number,row,column))\n                for i in range(image_number):\n                    train_X[i] = np.array(struct.unpack_from(fmt,data,offset)).reshape(row,column)\n                    offset += struct.calcsize(fmt)\n            else:\n                test_X = np.empty((image_number,row,column))\n                for i in range(image_number):\n                    test_X[i] = np.array(struct.unpack_from(fmt,data,offset)).reshape(row,column)\n                    offset += struct.calcsize(fmt)\n        else:\n            fmt = "">2i""\n            offset = 0\n            magic_number,image_number = struct.unpack_from(fmt,data,offset)\n            offset += struct.calcsize(fmt)\n            fmt = "">B""\n            if ""train"" in filepath:\n                train_Y = np.empty(image_number)\n                for i in range(image_number):\n                    train_Y[i] = struct.unpack_from(fmt,data,offset)[0]\n                    offset += struct.calcsize(fmt)\n            else:\n                test_Y = np.empty(image_number)\n                for i in range(image_number):\n                    test_Y[i] = struct.unpack_from(fmt,data,offset)[0]\n                    offset += struct.calcsize(fmt)\n    return train_X,train_Y,test_X,test_Y\n\n\ndef main():\n    train_X, train_Y, test_X, test_Y = load_data()\n    train_X = (1 / 127.5) * (train_X.reshape((train_X.shape[0], train_X.shape[1] * train_X.shape[2]))-127.5)\n    train_Y = train_Y.reshape((train_Y.shape[0], 1))\n    test_X = (1 / 127.5) * (test_X.reshape((test_X.shape[0], test_X.shape[1] * test_X.shape[2]))-127.5)\n    test_Y = test_Y.reshape((test_Y.shape[0], 1))\n    clf = BPNetwork()\n    clf.fit(train_X,train_Y,1,25,1000,1,0.005)\n    clf.predict(test_X,test_Y)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
SVM/SupportVectorMachine.py,9,"b'import numpy as np\nimport cvxopt\n\n\nclass SupportVectorMachine:\n\n    def __init__(self):\n        self.lags = None\n        self.support_vectors = None\n        self.support_labels = None\n        self.b = None\n        pass\n\n    def kernal_func(self, X):\n        """"""\n        :param X: shape(m,n)\n        :return: shape(m,m)\n        """"""\n        return X @ X.T\n\n    def kernal_funcII(self, x1, x2):\n        # print(x1)\n        # print(x1.shape)\n        # print(x2.shape)\n        return x1 @ x2.T\n\n    def fit(self, X, Y, C, threshold):\n        """"""\n        optimization target: a(m,1)\n        """"""\n        n_sample, n_feature = X.shape\n        P = cvxopt.matrix(Y @ Y.T * self.kernal_func(X))\n        q = cvxopt.matrix(-1*np.ones(n_sample))\n        G = cvxopt.matrix(np.concatenate([-np.eye(n_sample),np.eye(n_sample)],axis=0))\n        h = cvxopt.matrix(np.concatenate([np.zeros(shape=(n_sample,1)),C*np.ones(shape=(n_sample,1))],axis=0))\n        A = cvxopt.matrix(Y,(1,n_sample),\'d\')\n        b = cvxopt.matrix(0.0)\n\n        minimization = cvxopt.solvers.qp(cvxopt.matrix(P),cvxopt.matrix(q),cvxopt.matrix(G),cvxopt.matrix(h), cvxopt.matrix(A), cvxopt.matrix(b))\n\n        lags = np.array(minimization[""x""])\n        index = np.where(lags >= threshold)[0]\n        # print(index)\n        # print(X[index])\n        self.lags = lags[index]\n        self.support_vectors = X[index]\n        self.support_labels = Y[index]\n        self.b = np.sum(self.support_labels)\n        # for i in range(len(self.lags)):\n        #     for j in range(len(self.lags)):\n        #         self.b -= self.lags[j] * self.support_labels[j] * self.kernal_funcII(self.support_vectors[j],\n        #                                                                              self.support_vectors[i])\n        self.b -= np.sum(self.lags*self.support_labels*self.kernal_funcII(self.support_vectors,self.support_vectors))\n        self.b /= len(self.support_labels)\n        # print(self.b)\n\n    def predict(self, X, Y):\n        res = []\n        for sample in X:\n            p = 0\n            for i in range(len(self.support_vectors)):\n                p += self.lags[i]*self.support_labels[i]*self.kernal_funcII(self.support_vectors[i],sample)\n            p += self.b\n            res.append(np.sign(p))\n        print(np.sum(res[i]==Y[i] for i in range(len(res)))/len(res))\n\n'"
SVM/main.py,4,"b""import numpy as np\nfrom sklearn import datasets\nfrom SVM.SupportVectorMachine import SupportVectorMachine\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef main():\n    iris = datasets.load_iris()\n    X = np.array(iris.data)\n    Y = np.array(iris.target)\n    index = np.where(Y<2)\n    Y[Y==0] = -1\n    X = X[index]\n    Y = np.expand_dims(Y[index],axis=1)\n    scaler = MinMaxScaler()\n    X = scaler.fit_transform(X)\n    svm = SupportVectorMachine()\n    svm.fit(X,Y,1,threshold=1e-7)\n    svm.predict(X,Y)\n    pass\n\n\nif __name__ == '__main__':\n    main()"""
TreeBasedModel/DecisionTree.py,11,"b'import numpy as np\n\n\nclass DecisionTreeNode:\n\n    def __init__(self, feature_index=None, split_value=None, left_tree=None, right_tree=None, value=None):\n        self.feature_index = feature_index\n        self.split_value = split_value\n        self.left_tree = left_tree\n        self.right_tree = right_tree\n        self.value = value\n\n\nclass DecisionTree:\n\n    def __init__(self, max_depth, min_sample, min_gain):\n        self.max_depth = max_depth\n        self.min_sample = min_sample\n        self.min_gain = min_gain\n        self.root = None\n\n    def cal_entropy(self, y):\n        entropy = 0\n        unique_label = np.unique(y)\n        for label in unique_label:\n            p = len(y[y == label])/len(y)\n            entropy += -p*np.log2(p)\n        return entropy\n\n    def cal_gain(self, y, y1, y2):\n        p = len(y1)/len(y)\n        gain = self.cal_entropy(y)-p*self.cal_entropy(y1)-(1-p)*self.cal_entropy(y2)\n        return gain\n\n    def split(self, XY, feature_index, value):\n        split_func = lambda x: x[feature_index] >= value\n        left = np.array([sample for sample in XY if split_func(sample)])\n        right = np.array([sample for sample in XY if not split_func(sample)])\n        return left, right\n\n    def vote4most(self,y):\n        unique_label = np.unique(y)\n        most_common_label = None\n        most_common_count = 0\n        for label in unique_label:\n            l = len(y[y == label])\n            if l>most_common_count:\n                most_common_count = l\n                most_common_label = label\n        return most_common_label\n\n    def build_tree(self, X, Y, current_depth):\n        largest_gain = 0\n        XY = np.concatenate([X,Y],axis=1)\n        n_sample,n_feature = X.shape\n        if n_sample>=self.min_sample and current_depth<=self.max_depth:\n            for feature_index in range(n_feature):\n                feature_value = X[:,feature_index]\n                unique_values = np.unique(feature_value)\n                for unique_value in unique_values:\n                    xy1, xy2 = self.split(XY,feature_index,unique_value)\n                    if len(xy1)>=1 and len(xy2)>=1:\n                        y1 = xy1[:,-1]\n                        y2 = xy2[:,-1]\n                        info_gain = self.cal_gain(Y, y1, y2)\n                        if info_gain>=largest_gain:\n                            largest_gain = info_gain\n                            best_split = {""feature_index"":feature_index,""split_value"":unique_value}\n                            best_sets = {\n                                ""left_X"": xy1[:,:n_feature],\n                                ""right_X"": xy2[:,:n_feature],\n                                ""left_Y"": xy1[:,-1],\n                                ""right_Y"": xy2[:,-1]\n                            }\n        if largest_gain>self.min_gain:\n            left_tree = self.build_tree(best_sets[""left_X""],np.expand_dims(best_sets[""left_Y""],axis=1),current_depth+1)\n            right_tree = self.build_tree(best_sets[""right_X""],np.expand_dims(best_sets[""right_Y""],axis=1),current_depth+1)\n            return DecisionTreeNode(feature_index=best_split[""feature_index""],split_value=best_split[""split_value""],\n                                    left_tree=left_tree,right_tree=right_tree)\n        return DecisionTreeNode(value=self.vote4most(Y))\n\n    def fit(self, X, Y):\n        print(""start fitting"")\n        self.root = self.build_tree(X, Y, 0)\n\n    def predict(self, x, root=None):\n        if not root:\n            root = self.root\n\n        if root.value is not None:\n            return root.value\n\n        feature_index = root.feature_index\n        x_value = x[feature_index]\n        if x_value >= root.split_value:\n            node = root.left_tree\n        else:\n            node = root.right_tree\n        return self.predict(x, node)\n\n    def predict_all(self, X, Y):\n        temp = np.array([self.predict(sample) for sample in X])\n        print(np.sum([temp[i]==Y[i] for i in range(len(temp))])/len(temp))\n        return temp\n\n\n\n\n\n\n'"
TreeBasedModel/main.py,6,"b'import struct\nimport numpy as np\nfrom TreeBasedModel.DecisionTree import DecisionTree\n\n\ndef load_data():\n    for file in [""train-images.idx3-ubyte"", ""train-labels.idx1-ubyte"", ""t10k-images.idx3-ubyte"",\n                 ""t10k-labels.idx1-ubyte""]:\n        filepath = ""../dataset/""+file\n        data = open(filepath,""rb"").read()\n        if ""image"" in filepath:\n            fmt = "">4i""\n            offset = 0\n            magic_number,image_number,row,column = struct.unpack_from(fmt,data,offset)\n            offset += struct.calcsize(fmt)\n            fmt = "">{}B"".format(row*column)\n            if ""train"" in filepath:\n                train_X = np.empty((image_number,row,column))\n                for i in range(image_number):\n                    train_X[i] = np.array(struct.unpack_from(fmt,data,offset)).reshape(row,column)\n                    offset += struct.calcsize(fmt)\n            else:\n                test_X = np.empty((image_number,row,column))\n                for i in range(image_number):\n                    test_X[i] = np.array(struct.unpack_from(fmt,data,offset)).reshape(row,column)\n                    offset += struct.calcsize(fmt)\n        else:\n            fmt = "">2i""\n            offset = 0\n            magic_number,image_number = struct.unpack_from(fmt,data,offset)\n            offset += struct.calcsize(fmt)\n            fmt = "">B""\n            if ""train"" in filepath:\n                train_Y = np.empty(image_number)\n                for i in range(image_number):\n                    train_Y[i] = struct.unpack_from(fmt,data,offset)[0]\n                    offset += struct.calcsize(fmt)\n            else:\n                test_Y = np.empty(image_number)\n                for i in range(image_number):\n                    test_Y[i] = struct.unpack_from(fmt,data,offset)[0]\n                    offset += struct.calcsize(fmt)\n    return train_X,train_Y,test_X,test_Y\n\n\n\ndef main():\n    train_X, train_Y, test_X, test_Y = load_data()\n    train_X = (train_X.reshape((train_X.shape[0],train_X.shape[1]*train_X.shape[2])))\n    train_Y = train_Y.reshape((train_Y.shape[0],1))\n    test_X = (test_X.reshape((test_X.shape[0],test_X.shape[1]*test_X.shape[2])))\n    test_Y = test_Y.reshape((test_Y.shape[0],1))\n    dct = DecisionTree(max_depth=2,min_sample=2,min_gain=1e-7)\n    dct.fit(train_X[:200],train_Y[:200])\n    dct.predict_all(test_X[:100], test_Y[:100])\n\n\nif __name__ == \'__main__\':\n    main()\n'"
