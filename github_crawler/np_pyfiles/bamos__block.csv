file_path,api_count,code
setup.py,0,"b'from setuptools import find_packages, setup\n\nsetup(\n    name=\'block\',\n    version=\'0.0.5\',\n    description=""Improved block matrix creation for numpy and PyTorch."",\n    author=\'Brandon Amos\',\n    author_email=\'bamos@cs.cmu.edu\',\n    platforms=[\'any\'],\n    license=""Apache 2.0"",\n    url=\'https://github.com/bamos/block\',\n    packages=find_packages(),\n    install_requires=[\n        \'numpy>=1<2\',\n    ]\n)\n'"
test.py,25,"b""#!/usr/bin/env python3\n\nimport numpy as np\nimport numpy.random as npr\nimport scipy.sparse.linalg as sla\n\nfrom block import block, block_diag\n\n\ndef test_np():\n    npr.seed(0)\n\n    nx, nineq, neq = 4, 6, 7\n    Q = npr.randn(nx, nx)\n    G = npr.randn(nineq, nx)\n    A = npr.randn(neq, nx)\n    D = np.diag(npr.rand(nineq))\n\n    K_ = np.bmat((\n        (Q, np.zeros((nx, nineq)), G.T, A.T),\n        (np.zeros((nineq, nx)), D, np.eye(nineq), np.zeros((nineq, neq))),\n        (G, np.eye(nineq), np.zeros((nineq, nineq + neq))),\n        (A, np.zeros((neq, nineq + nineq + neq)))\n    ))\n\n    K = block((\n        (Q,   0, G.T, A.T),\n        (0,   D, 'I',   0),\n        (G, 'I',   0,   0),\n        (A,   0,   0,   0)\n    ))\n\n    assert np.allclose(K_, K)\n\n\ndef test_diag():\n    n0, n1, n2, n3 = 4, 5, 6, 7\n    A = npr.randn(n0, n1)\n    B = npr.randn(n2, n3)\n\n    K_ = np.bmat((\n        (A, np.zeros((n0, n3))),\n        (np.zeros((n2, n1)), B)\n    ))\n\n    K = block_diag((A, B))\n\n    assert np.allclose(K_, K)\n\n\ndef test_torch():\n    import torch\n    from torch.autograd import Variable\n\n    torch.manual_seed(0)\n\n    nx, nineq, neq = 4, 6, 7\n    Q = torch.randn(nx, nx)\n    G = torch.randn(nineq, nx)\n    A = torch.randn(neq, nx)\n    D = torch.diag(torch.rand(nineq))\n\n    K_ = torch.cat((\n        torch.cat((Q, torch.zeros(nx, nineq).type_as(Q), G.t(), A.t()), 1),\n        torch.cat((torch.zeros(nineq, nx).type_as(Q), D,\n                   torch.eye(nineq).type_as(Q),\n                   torch.zeros(nineq, neq).type_as(Q)), 1),\n        torch.cat((G, torch.eye(nineq).type_as(Q), torch.zeros(\n            nineq, nineq + neq).type_as(Q)), 1),\n        torch.cat((A, torch.zeros((neq, nineq + nineq + neq))), 1)\n    ))\n\n    K = block((\n        (Q,   0, G.t(), A.t()),\n        (0,   D,   'I',     0),\n        (G, 'I',     0,     0),\n        (A,   0,     0,     0)\n    ))\n\n    assert (K - K_).norm() == 0.0\n    K = block((\n        (Variable(Q),   0, G.t(), Variable(A.t())),\n        (0,   Variable(D),   'I',     0),\n        (Variable(G), 'I',     0,     0),\n        (A,   0,     0,     0)\n    ))\n\n    assert (K.data - K_).norm() == 0.0\n\n\ndef test_linear_operator():\n    npr.seed(0)\n\n    nx, nineq, neq = 4, 6, 7\n    Q = npr.randn(nx, nx)\n    G = npr.randn(nineq, nx)\n    A = npr.randn(neq, nx)\n    D = np.diag(npr.rand(nineq))\n\n    K_ = np.bmat((\n        (Q, np.zeros((nx, nineq)), G.T, A.T),\n        (np.zeros((nineq, nx)), D, np.eye(nineq), np.zeros((nineq, neq))),\n        (G, np.eye(nineq), np.zeros((nineq, nineq + neq))),\n        (A, np.zeros((neq, nineq + nineq + neq)))\n    ))\n\n    Q_lo = sla.aslinearoperator(Q)\n    G_lo = sla.aslinearoperator(G)\n    A_lo = sla.aslinearoperator(A)\n    D_lo = sla.aslinearoperator(D)\n\n    K = block((\n        (Q_lo,    0,    G.T,    A.T),\n        (0,    D_lo,    'I',      0),\n        (G_lo,  'I',      0,      0),\n        (A_lo,    0,      0,      0)\n    ), arrtype=sla.LinearOperator)\n\n    w1 = np.random.randn(K_.shape[1])\n    assert np.allclose(K_.dot(w1), K.dot(w1))\n    w2 = np.random.randn(K_.shape[0])\n    assert np.allclose(K_.T.dot(w2), K.H.dot(w2))\n    W = np.random.randn(*K_.shape)\n    assert np.allclose(K_.dot(W), K.dot(W))\n\n\ndef test_empty():\n    A = npr.randn(3, 0)\n    B = npr.randn(3, 3)\n    out = block([[A, B]])\n    assert np.linalg.norm(out - B) == 0.0\n\n    A = npr.randn(0, 3)\n    B = npr.randn(3, 3)\n    out = block([[A], [B]])\n    assert np.linalg.norm(out - B) == 0.0\n\n\nif __name__ == '__main__':\n    test_np()\n    test_torch()\n    test_empty()\n    test_linear_operator()\n    test_diag()\n"""
block/__init__.py,0,"b""from .block import block, block_diag\n\n__all__ = ['block', 'block_diag']\n"""
block/block.py,18,"b""import numpy as np\nimport scipy.sparse.linalg as sla\nimport scipy.sparse as sp\n\ntry:\n    import torch\n    from torch.autograd import Variable\nexcept:\n    pass\n\n\nimport re\nfrom abc import ABCMeta, abstractmethod\n\n\ndef block(rows, dtype=None, arrtype=None):\n    if (not _is_list_or_tup(rows)) or len(rows) == 0 or \\\n       np.any([not _is_list_or_tup(row) for row in rows]):\n        raise RuntimeError('''\nUnexpected input: Expected a non-empty list of lists.\nIf you are interested in helping expand the functionality\nfor your use case please send in an issue or PR at\nhttp://github.com/bamos/block''')\n\n    rowLens = [len(row) for row in rows]\n    if len(np.unique(rowLens)) > 1:\n        raise RuntimeError('''\nUnexpected input: Rows are not the same length.\nRow lengths: {}'''.format(rowLens))\n\n    nRows = len(rows)\n    nCols = rowLens[0]\n    rowSizes = np.zeros(nRows, dtype=int)\n    colSizes = np.zeros(nCols, dtype=int)\n\n    backend = _get_backend(rows, dtype, arrtype)\n\n    for i, row in enumerate(rows):\n        for j, elem in enumerate(row):\n            if backend.is_complete(elem):\n                rowSz, colSz = backend.extract_shape(elem)\n                rowSizes[i] = rowSz\n                colSizes[j] = colSz\n            elif hasattr(elem, 'shape'):\n                rowSz, colSz = elem.shape\n                rowSizes[i] = rowSz\n                colSizes[j] = colSz\n            elif hasattr(elem, 'size'):\n                rowSz, colSz = elem.size()\n                rowSizes[i] = rowSz\n                colSizes[j] = colSz\n\n    cRows = []\n    for row, rowSz in zip(rows, rowSizes):\n        rowSz = int(rowSz)\n        if rowSz == 0:\n            continue\n        cCol = []\n        for elem, colSz in zip(row, colSizes):\n            colSz = int(colSz)\n            if colSz == 0:\n                continue\n            # TODO: Check types.\n            if backend.is_complete(elem):\n                cElem = elem\n            elif isinstance(elem, float) or isinstance(elem, int):\n                cElem = backend.build_full((rowSz, colSz), elem)\n            elif isinstance(elem, str):\n                if elem == 'I':\n                    assert(rowSz == colSz)\n                    cElem = backend.build_eye(rowSz)\n                elif elem == '-I':\n                    assert(rowSz == colSz)\n                    cElem = -backend.build_eye(rowSz)\n                else:\n                    assert(False)\n            else:\n                cElem = backend.convert(elem)\n            cCol.append(cElem)\n        cRows.append(cCol)\n\n    return backend.build(cRows)\n\n\ndef block_diag(elems, dtype=None, arrtype=None):\n    n = len(elems)\n    return block([[0] * i + [elem] + [0] * (n - 1 - i)\n                  for i, elem in enumerate(elems)],\n                 dtype=dtype, arrtype=arrtype)\n\n\ndef block_tridiag(main, upper, lower):\n    n = len(main)\n    assert len(main) == len(upper) + 1\n    assert len(main) == len(lower) + 1\n    mat = ()\n    for i in range(n):\n        tup = ()\n        for j in range(n):\n            if (i==j):   tup = (*tup, main[i])\n            elif (i==j-1): tup = (*tup, upper[-i])\n            elif (i==j+1): tup = (*tup, lower[i-1])\n            else: tup = (*tup,0)\n        mat = (*mat,tup)\n    return block(mat)\n\ndef _is_list_or_tup(x):\n    return isinstance(x, list) or isinstance(x, tuple)\n\n\ndef _get_backend(rows, dtype, arrtype):\n    if arrtype == np.ndarray and dtype is not None:\n        return NumpyBackend(arrtype, dtype)\n    elif arrtype == sla.LinearOperator:\n        return LinearOperatorBackend(dtype)\n    elif arrtype is not None and re.search('torch\\..*Tensor', repr(arrtype)):\n        return TorchBackend(dtype)\n    elif arrtype is not None and re.search('torch\\..*(Variable|Parameter)', repr(arrtype)):\n        return TorchVariableBackend(dtype)\n    else:\n        npb = NumpyBackend()\n        tb = TorchBackend()\n        lob = LinearOperatorBackend()\n        tvb = TorchVariableBackend()\n        for row in rows:\n            for elem in row:\n                if npb.is_complete(elem) and elem.size > 0:\n                    if dtype is None:\n                        dtype = type(elem[0, 0])\n                    if arrtype is None:\n                        arrtype = type(elem)\n                    return NumpyBackend(dtype, arrtype)\n                elif tb.is_complete(elem):\n                    return TorchBackend(type(elem))\n                elif lob.is_complete(elem):\n                    return LinearOperatorBackend(elem.dtype)\n                elif tvb.is_complete(elem):\n                    return TorchVariableBackend(type(elem.data))\n\n    assert(False)\n\n\nclass Backend():\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def extract_shape(self, x): pass\n\n    @abstractmethod\n    def build_eye(self, n): pass\n\n    @abstractmethod\n    def build_full(self, shape, fill_val): pass\n\n    @abstractmethod\n    def convert(self, x): pass\n\n    @abstractmethod\n    def build(self, rows): pass\n\n    @abstractmethod\n    def is_complete(self, rows): pass\n\n\nclass NumpyBackend(Backend):\n\n    def __init__(self, dtype=None, arrtype=None):\n        self.dtype = dtype\n        self.arrtype = arrtype\n\n    def extract_shape(self, x):\n        return x.shape\n\n    def build_eye(self, n):\n        return np.eye(n)\n\n    def build_full(self, shape, fill_val):\n        return np.full(shape, fill_val, self.dtype)\n\n    def convert(self, x):\n        assert(False)\n\n    def build(self, rows):\n        return np.bmat(rows)\n\n    def is_complete(self, x):\n        return isinstance(x, np.ndarray)\n\n\nclass TorchBackend(Backend):\n\n    def __init__(self, dtype=None):\n        self.dtype = dtype\n\n    def extract_shape(self, x):\n        return x.size()\n\n    def build_eye(self, n):\n        return torch.eye(n).type(self.dtype)\n\n    def build_full(self, shape, fill_val):\n        return fill_val * torch.ones(*shape).type(self.dtype)\n\n    def convert(self, x):\n        assert(False)\n\n    def build(self, rows):\n        compRows = []\n        for row in rows:\n            compRows.append(torch.cat(row, 1))\n        return torch.cat(compRows)\n\n    def is_complete(self, x):\n        return (re.search('torch\\..*Tensor', str(x.__class__)) is not None) \\\n            and x.ndimension() == 2\n\n\nclass TorchVariableBackend(TorchBackend):\n    def build_eye(self, n):\n        return Variable(super().build_eye(n))\n\n    def build_full(self, shape, fill_val):\n        return Variable(super().build_full(shape, fill_val))\n\n    def convert(self, x):\n        if TorchBackend.is_complete(self, x):\n            return Variable(x)\n        assert(False)\n\n    def is_complete(self, x):\n        return re.search('torch\\..*(Variable|Parameter)', str(x.__class__))\n\n\nclass LinearOperatorBackend(Backend):\n    def __init__(self, dtype=None):\n        self.dtype = dtype\n\n    def extract_shape(self, x):\n        return x.shape\n\n    def build_eye(self, n):\n        def identity(v): return v\n        return sla.LinearOperator(shape=(n, n),\n                                  matvec=identity,\n                                  rmatvec=identity,\n                                  matmat=identity,\n                                  dtype=self.dtype)\n\n    def build_full(self, shape, fill_val):\n        m, n = shape\n        if fill_val == 0:\n            return shape\n        else:\n            def matvec(v):\n                return v.sum() * fill_val * np.ones(m)\n\n            def rmatvec(v):\n                return v.sum() * fill_val * np.ones(n)\n\n            def matmat(M):\n                return M.sum(axis=0) * fill_val * np.ones((m, M.shape[1]))\n\n            return sla.LinearOperator(shape=shape,\n                                      matvec=matvec,\n                                      rmatvec=rmatvec,\n                                      matmat=matmat,\n                                      dtype=self.dtype)\n\n    def convert(self, x):\n        if (isinstance(x, (np.ndarray, sp.spmatrix))):\n            return sla.aslinearoperator(x)\n        else:\n            assert(False)\n\n    def build(self, rows):\n        col_sizes = [lo.shape[1] if self.is_complete(lo) else lo[1]\n                     for lo in rows[0]]\n        col_idxs = np.cumsum([0] + col_sizes)\n        row_sizes = [row[0].shape[0] if self.is_complete(row[0]) else row[0][0]\n                     for row in rows]\n        row_idxs = np.cumsum([0] + row_sizes)\n        m, n = sum(row_sizes), sum(col_sizes)\n\n        def matvec(v):\n            out = np.zeros(m)\n            for row, i, j in zip(rows, row_idxs[:-1], row_idxs[1:]):\n                out[i:j] = sum(lo.matvec(v[k:l]) for lo, k, l in\n                               zip(row, col_idxs[:-1], col_idxs[1:])\n                               if self.is_complete(lo))\n            return out\n\n        # The transposed list\n        cols = zip(*rows)\n\n        def rmatvec(v):\n            out = np.zeros(n)\n            for col, i, j in zip(cols, col_idxs[:-1], col_idxs[1:]):\n                out[i:j] = sum(lo.rmatvec(v[k:l]) for lo, k, l in\n                               zip(col, row_idxs[:-1], row_idxs[1:])\n                               if self.is_complete(lo))\n            return out\n\n        def matmat(M):\n            out = np.zeros((m, M.shape[1]))\n            for row, i, j in zip(rows, row_idxs[:-1], row_idxs[1:]):\n                out[i:j] = sum(lo.matmat(M[k:l]) for lo, k, l in\n                               zip(row, col_idxs[:-1], col_idxs[1:])\n                               if self.is_complete(lo))\n            return out\n\n        return sla.LinearOperator(shape=(m, n),\n                                  matvec=matvec,\n                                  rmatvec=rmatvec,\n                                  matmat=matmat,\n                                  dtype=self.dtype)\n\n    def is_complete(self, x):\n        return isinstance(x, sla.LinearOperator)\n"""
