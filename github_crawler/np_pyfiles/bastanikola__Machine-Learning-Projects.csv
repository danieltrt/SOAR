file_path,api_count,code
Artificial Neural Networks - Exploring Clients Loyalty/Artificial Neural Network - Exploring Clients Loyalty.py,0,"b""# Artificial Neural Network\n\n''' \n#Installing Theano\npip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n\n#Installing Tensorflow\nInstall Tensorflow from the website: https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html\n\n#Installing Keras\npip install --upgrade keras\n'''\n\n# Part 1 - Data Preprocessing\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Churn_Modelling.csv')\nX = dataset.iloc[:, 3:13].values\ny = dataset.iloc[:, 13].values\n\n# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_2 = LabelEncoder()\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()\nX = X[:, 1:]\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Part 2 - Creating the Artificial Neural Network!\n\n# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Initialising the Artificial Neural Network\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n\n# Adding the second hidden layer\nclassifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nclassifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n\n# Compiling the Artificial Neural Network\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the Artificial Neural Network to the Training set\nclassifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)\n\n# Part 3 - Predictions and evaluation of the model\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)"""
Breast Cancer Classification/BreastCancerClassification.py,1,"b'---------------------------------------------------------------------------------\n### Project: BREAST CANCER CLASSIFICATION - SUPPORT VECTOR MACHINE CLASSIFIER ###\n---------------------------------------------------------------------------------\n---------------------------------\n### STEP 1: PROBLEM STATEMENT ###\n---------------------------------\n\'\'\'\n- Predicting if the cancer diagnosis is benign or malignant based on several observations/features \n          30 features are used, examples:\n        - radius (mean of distances from center to points on the perimeter)\n        - texture (standard deviation of gray-scale values)\n        - perimeter\n        - area\n        - smoothness (local variation in radius lengths)\n        - compactness (perimeter^2 / area - 1.0)\n        - concavity (severity of concave portions of the contour)\n        - concave points (number of concave portions of the contour)\n        - symmetry \n        - fractal dimension (""coastline approximation"" - 1)\n\n- Datasets are linearly separable using all 30 input features\n- Number of Instances: 569\n- Class Distribution: 212 Malignant, 357 Benign\n- Target class:\n         - Malignant\n         - Benign\n\nhttps://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n\'\'\'\n-----------------------------\n### STEP 2: IMPORTING DATA ###\n-----------------------------\n#import libraries \nimport pandas as pd             # Import Pandas for data manipulation using dataframes\nimport numpy as np              # Import Numpy for data statistical analysis \nimport matplotlib.pyplot as plt # Import matplotlib for data visualisation\nimport seaborn as sns           # Statistical data visualization\n#%matplotlib inline\n\n#Import Cancer data drom the Sklearn library\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer()\ncancer\ncancer.keys()\nprint(cancer[\'DESCR\'])\nprint(cancer[\'target_names\'])\nprint(cancer[\'target\'])\nprint(cancer[\'feature_names\'])\nprint(cancer[\'data\'])\ncancer[\'data\'].shape\n\ndf_cancer = pd.DataFrame(np.c_[cancer[\'data\'], cancer[\'target\']], columns = np.append(cancer[\'feature_names\'], [\'target\']))\ndf_cancer.head()\ndf_cancer.tail()\n\n------------------------------------\n### STEP 3: VISUALIZING THE DATA ###\n------------------------------------\nsns.pairplot(df_cancer, hue = \'target\', vars = [\'mean radius\', \'mean texture\', \'mean area\', \'mean perimeter\', \'mean smoothness\'] )\nsns.countplot(df_cancer[\'target\'], label = ""Count"")\nsns.scatterplot(x = \'mean area\', y = \'mean smoothness\', hue = \'target\', data = df_cancer)\nsns.lmplot(\'mean area\', \'mean smoothness\', hue =\'target\', data = df_cancer, fit_reg=False)\n\n#Let\'s check the correlation between the variables \n#Strong correlation between the mean radius and mean perimeter, mean area and mean primeter\nplt.figure(figsize=(20,10)) \nsns.heatmap(df_cancer.corr(), annot=True)\n\n-----------------------------------------------------------\n### STEP 4: MODEL TRAINING (FINDING A PROBLEM SOLUTION) ###\n-----------------------------------------------------------\n#Let\'s drop the target label coloumns\nX = df_cancer.drop([\'target\'],axis=1)\ny = df_cancer[\'target\']\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=5)\n\nX_train.shape\nX_test.shape\ny_train.shape\ny_test.shape\n\nfrom sklearn.svm import SVC \nfrom sklearn.metrics import classification_report, confusion_matrix\n\nsvc_model = SVC()\nsvc_model.fit(X_train, y_train)\n\n------------------------------------\n### STEP 5: EVALUATING THE MODEL ###\n------------------------------------\ny_predict = svc_model.predict(X_test)\ncm = confusion_matrix(y_test, y_predict)\n\nsns.heatmap(cm, annot=True)\n\nprint(classification_report(y_test, y_predict))\n\n-----------------------------------\n### STEP 6: IMPROVING THE MODEL ###\n-----------------------------------\n\'\'\'\nIMPROVING THE MODEL - PART 1\n\nData Normalization:\nFeature scaling (Uni-based normalization) brings values into range [0,1]\nX\' = (X - Xmin)/(Xmax - Xmin)\n\'\'\'\nmin_train = X_train.min()\nrange_train = (X_train - min_train).max()\nX_train_scaled = (X_train - min_train)/range_train\n\nsns.scatterplot(x = X_train[\'mean area\'], y = X_train[\'mean smoothness\'], hue = y_train)\nsns.scatterplot(x = X_train_scaled[\'mean area\'], y = X_train_scaled[\'mean smoothness\'], hue = y_train)\n\nmin_test = X_test.min()\nrange_test = (X_test - min_test).max()\nX_test_scaled = (X_test - min_test)/range_test\n\nfrom sklearn.svm import SVC \nfrom sklearn.metrics import classification_report, confusion_matrix\n\nsvc_model = SVC()\nsvc_model.fit(X_train_scaled, y_train)\n\ny_predict = svc_model.predict(X_test_scaled)\ncm = confusion_matrix(y_test, y_predict)\n\nsns.heatmap(cm,annot=True,fmt=""d"")\n\nprint(classification_report(y_test,y_predict))\n\n\'\'\'\nIMPROVING THE MODEL - PART 2\n\nC parameter: Controls trade off between classifying training points\ncorrectly and having a smooth decision boundary:\n    -Small C (loose) makes cost (penalty) of misclassification low (soft margin)\n    -Large C (strict) makes cost of misclassification high (hard margin), forcing\n     the model to explain input data stricter and potentially over fit\n     \nGamma parameter: Controls how far the influence of a single training set reachs\n    -Large gamma: close reach (closer data points have high weight)\n    -Small gamma: far reach (more generalization solution)\n\'\'\'\nparam_grid = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001], \'kernel\': [\'rbf\']}\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = GridSearchCV(SVC(),param_grid,refit=True,verbose=4)\ngrid.fit(X_train_scaled,y_train)\n\ngrid.best_params_\ngrid.best_estimator_\n\ngrid_predictions = grid.predict(X_test_scaled)\ncm = confusion_matrix(y_test, grid_predictions)\n\nsns.heatmap(cm, annot=True)\n\nprint(classification_report(y_test,grid_predictions))\n'"
CNN-Smiling Faces Detector/SmilingFacesDetector.py,7,"b'---------------------------------------------------------------------------------\n### Project: SMILING FACES DETECTOR USING CNN - CONVOLUTIONAL NEURAL NETWORKS ###\n---------------------------------------------------------------------------------\n---------------------------------------------------\n### STEP 1: PROBLEM STATEMENT AND BUSINESS CASE ###\n---------------------------------------------------\n\'\'\'\n    - The dataset contains a series of images that can be used to solve the Happy House problem!\n    - We need to build an artificial neural network that can detect smiling faces.\n    - Only smiling people will be allowed to enter the house!\n    - The train set has 600 examples. The test set has 150 examples.\n    - Data Source: https://www.kaggle.com/iarunava/happy-house-dataset\n\'\'\'\n------------------------------\n### STEP 2: IMPORTING DATA ###\n------------------------------\n# import libraries \nimport pandas as pd             # Import Pandas for data manipulation using dataframes\nimport numpy as np              # Import Numpy for data statistical analysis \nimport matplotlib.pyplot as plt # Import matplotlib for data visualisation\nimport seaborn as sns\nimport h5py\nimport random\n\nfilename = \'train_happy.h5\'\nf = h5py.File(filename, \'r\')\n\nfor key in f.keys():\n    print(key) #Names of the groups in HDF5 file.\n\nhappy_training = h5py.File(\'train_happy.h5\', ""r"")\nhappy_testing  = h5py.File(\'test_happy.h5\', ""r"")\n\nX_train = np.array(happy_training[""train_set_x""][:]) \ny_train = np.array(happy_training[""train_set_y""][:]) \n\nX_test = np.array(happy_testing[""test_set_x""][:])\ny_test = np.array(happy_testing[""test_set_y""][:])\n\nX_train\nX_train.shape\ny_train\ny_train.shape\n\n---------------------------------------------\n### STEP #3: VISUALIZATION OF THE DATASET ###\n---------------------------------------------\ni = random.randint(1,600) # select any random index from 1 to 600\nplt.imshow( X_train[i] )\nprint(y_train[i])\n\n#Let\'s view more images in a grid format\n#Define the dimensions of the plot grid \nW_grid = 5\nL_grid = 5\n\xe2\x80\x8b\n#fig, axes = plt.subplots(L_grid, W_grid)\n#subplot return the figure object and axes object\n#we can use the axes object to plot specific figures at various locations\n\xe2\x80\x8b\nfig, axes = plt.subplots(L_grid, W_grid, figsize = (25,25))\n\xe2\x80\x8b\naxes = axes.ravel() #flaten the 15 x 15 matrix into 225 array\n\xe2\x80\x8b\nn_training = len(X_train) #get the length of the training dataset\n\xe2\x80\x8b\n#Select a random number from 0 to n_training\nfor i in np.arange(0, W_grid * L_grid): #create evenly spaces variables \n\xe2\x80\x8b\n    #Select a random number\n    index = np.random.randint(0, n_training)\n    #read and display an image with the selected index    \n    axes[i].imshow( X_train[index])\n    axes[i].set_title(y_train[index], fontsize = 25)\n    axes[i].axis(\'off\')\n\xe2\x80\x8b\nplt.subplots_adjust(hspace=0.4)\n\n-----------------------------------\n### STEP 4: TRAINING THE MODEL ###\n-----------------------------------\n# Let\'s normalize dataset\n#maximum value is 255 within our data\nX_train = X_train/255\nX_test = X_test/255\n\nX_train\nplt.imshow(X_train[9])\nX_train.shape\ny_train.shape\n\n#Import train_test_split from scikit library\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import TensorBoard\n\n\ncnn_model = Sequential()\n\n\'\'\'\n### CONVOLUTIONAL NEURAL NETWORK: FEATURE DETECTOR ###\n\n    - Convolutions use a kernel matrix to scan a given image and apply a filter to obtain a certain effect. \n    - An image Kernel is a matrix used to apply effects such as blurring and sharpening. \n    - Kernels are used in machine learning for feature extraction to select most important pixels of an image.\n    - Convolution preserves the spatial relationship between pixels. \n\n### CONVOLUTIONAL NEURAL NETWORK: RELU ###\n\n    - RELU Layers are used to add non-linearity in the feature map.\n    - It also enhances the sparsity or how scattered the feature map is.\n    - RELU Layers are used to add non-linearity in the feature map.\n    - It also enhances the sparsity or how scattered the feature map is.\n    - The gradient of the RELU does not vanish as we increase x compared to the sigmoid function\n\n### CONVOLUTIONAL NEURAL NETWORK: MAXPOOLING/FLATTENING ###\n\n    - Pooling or down sampling layers are placed after convolutional layers to reduce feature map dimensionality.\n    - This improves the computational efficiency while preserving the features.\n    - Pooling helps the model to generalize by avoiding overfitting. If one of the pixel is shifted, the pooled feature map will still be the same.\n    - Max pooling works by retaining the maximum feature response within a given sample size in a feature map.\n    - Live illustration : http://scs.ryerson.ca/~aharley/vis/conv/flat.html\n\n### CONVOLUTIONAL NEURAL NETWORK: INCREASE FILTERS/DROPOUT ###\n\n    - Improve accuracy by adding more feature detectors/filters or adding a dropout. \n    - Dropout refers to dropping out units in a neural network.\n    - Neurons develop co-dependency amongst each other during training\n    - Dropout is a regularization technique for reducing overfitting in neural networks. \n    - It enables training to occur on several architectures of the neural network\n\'\'\'\ncnn_model.add(Conv2D(64, 6, 6, input_shape = (64,64,3), activation=\'relu\'))\ncnn_model.add(MaxPooling2D(pool_size = (2, 2)))\n\ncnn_model.add(Dropout(0.2))\n\ncnn_model.add(Conv2D(64, 5, 5, activation=\'relu\'))\ncnn_model.add(MaxPooling2D(pool_size = (2, 2)))\n\ncnn_model.add(Flatten())\ncnn_model.add(Dense(output_dim = 128, activation = \'relu\'))\ncnn_model.add(Dense(output_dim = 1, activation = \'sigmoid\'))\n\n\ncnn_model.compile(loss =\'binary_crossentropy\', optimizer=Adam(lr=0.001),metrics =[\'accuracy\'])\n\nepochs = 5\nhistory = cnn_model.fit(X_train,\n                        y_train,\n                        batch_size = 30,\n                        nb_epoch = epochs,\n                        verbose = 1)\n\n------------------------------------\n### STEP 5: EVALUATING THE MODEL ###\n------------------------------------\nevaluation = cnn_model.evaluate(X_test, y_test)\nprint(\'Test Accuracy : {:.3f}\'.format(evaluation[1]))\n\n#get the predictions for the test data\npredicted_classes = cnn_model.predict_classes(X_test)\n\npredicted_classes.shape\ny_test.shape\n\nL = 5\nW = 5\nfig, axes = plt.subplots(L, W, figsize = (12,12))\naxes = axes.ravel()\n\nfor i in np.arange(0, L * W):  \n    axes[i].imshow(X_test[i])\n    axes[i].set_title(""Prediction Class = {}\\n True Class = {}"".format(predicted_classes[i], y_test[i]))\n    axes[i].axis(\'off\')\n\nplt.subplots_adjust(wspace=0.5)\n\n#axes[i].set_title(""Guess{}\\n True{}"".format(predicted_class[i], y_test[i]))\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, predicted_classes)\nplt.figure(figsize = (10,10))\nsns.heatmap(cm, annot=True)\n#Sum the diagonal element to get the total true correct values\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test.T, predicted_classes, target_names = target_names))\n\n\n\n\n'"
Linear Regression-Predicting Revenue/RevenuePrediction.py,0,"b'--------------------------------------------------------------\n### Project: REVENUE PREDICTION - SIMPLE LINEAR REGRESSION ###\n--------------------------------------------------------------\n---------------------------------\n### STEP 1: PROBLEM STATEMENT ###\n---------------------------------\n\'\'\'\nPROBLEM STATEMENT\nYou own an ice cream business and you would like to create a model that could predict the daily revenue in dollars based on the outside air temperature (degC). You decide that a Linear Regression model might be a good candidate to solve this problem.\nData set:\n\nIndependant variable X: Outside Air Temperature\nDependant variable Y: Overall daily revenue generated in dollars\n\'\'\'\n--------------------------------\n### STEP 2: LIBRARIES IMPORT ###\n--------------------------------\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n------------------------------\n### STEP 3: IMPORT DATASET ###\n------------------------------\nIceCream = pd.read_csv(""IceCreamData.csv"")\nIceCream.head()\nIceCream.tail()\nIceCream.describe()\nIceCream.info()\n\n---------------------------------\n### STEP 4: VISUALIZE DATASET ###\n---------------------------------\nsns.jointplot(x=\'Temperature\', y=\'Revenue\', data = IceCream)\nsns.pairplot(IceCream)\nsns.lmplot(x=\'Temperature\', y=\'Revenue\', data=IceCream)\n\n---------------------------------------------------\n### STEP 5: CREATE TESTING AND TRAINING DATASET ###\n---------------------------------------------------\ny = IceCream[\'Revenue\']\nX = IceCream[[\'Temperature\']]\n\nfrom sklearn.model_selection import train_test_split\n\n#splitting the data into train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\n-------------------------------\n### STEP 6: TRAIN THE MODEL ###\n-------------------------------\nX_train.shape\n\nfrom sklearn.linear_model import LinearRegression\n\n#instatiation of an object out of our class\n#when ""fit_intercept = True"" - asking the model to obtain intercept which is value of \'m\' and \'b\'\n#when ""fit_intercept = False"" - model will obtain only the \'m\' value; \'b\' will be zero by default\nregressor = LinearRegression(fit_intercept =True)\nregressor.fit(X_train,y_train)\n\nprint(\'Linear Model Coefficient (m): \', regressor.coef_)\nprint(\'Linear Model Coefficient (b): \', regressor.intercept_)\n\n------------------------------\n### STEP 7: TEST THE MODEL ###\n------------------------------\ny_predict = regressor.predict( X_test)\ny_predict\ny_test\n\n#VISUALIZE TRAIN SET RESULTS\nplt.scatter(X_train, y_train, color = \'red\')\nplt.plot(X_train, regressor.predict(X_train), color = \'blue\')\nplt.ylabel(\'Revenue [dollars]\')\nplt.xlabel(\'Temperature [degC]\')\nplt.title(\'Revenue Generated vs. Temperature @Ice Cream Stand(Training dataset)\')\n\n#VISUALIZE TEST SET RESULTS\nplt.scatter(X_test, y_test, color = \'red\')\nplt.plot(X_test, regressor.predict(X_test), color = \'blue\')\nplt.ylabel(\'Revenue [dollars]\')\nplt.xlabel(\'Hours\')\nplt.title(\'Revenue Generated vs. Hours @Ice Cream Stand(Test dataset)\')\n\ny_predict = regressor.predict(30)\ny_predict\n'"
Natural Language Processing-Restaurant Reviews/NLP & Naive Bayes.py,0,"b""# Natural Language Processing\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n\n# Cleaning the texts\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0, 1000):\n    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus.append(review)\n\n# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500)\nX = cv.fit_transform(corpus).toarray()\ny = dataset.iloc[:, 1].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\n# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)"""
