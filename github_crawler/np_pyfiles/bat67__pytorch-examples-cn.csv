file_path,api_count,code
PyTorch：优化模块optim/two_layer_net_optim.py,0,"b""import torch\n\n# N\xe6\x98\xaf\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x9bD\xe6\x98\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\xb4\xe5\xba\xa6\n# H\xe6\x98\xaf\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x9bD_out\xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\xb4\xe5\xba\xa6\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \xe4\xba\xa7\xe7\x94\x9f\xe9\x9a\x8f\xe6\x9c\xba\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe5\xbc\xa0\xe9\x87\x8f\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# \xe4\xbd\xbf\xe7\x94\xa8nn\xe5\x8c\x85\xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x92\x8c\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\nmodel = torch.nn.Sequential(\n          torch.nn.Linear(D_in, H),\n          torch.nn.ReLU(),\n          torch.nn.Linear(H, D_out),\n        )\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\n# \xe4\xbd\xbf\xe7\x94\xa8optim\xe5\x8c\x85\xe5\xae\x9a\xe4\xb9\x89\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xef\xbc\x88Optimizer\xef\xbc\x89\xe3\x80\x82Optimizer\xe5\xb0\x86\xe4\xbc\x9a\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe6\x9b\xb4\xe6\x96\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe3\x80\x82\n# \xe8\xbf\x99\xe9\x87\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8Adam\xe4\xbc\x98\xe5\x8c\x96\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x9boptim\xe5\x8c\x85\xe8\xbf\x98\xe5\x8c\x85\xe5\x90\xab\xe4\xba\x86\xe8\xae\xb8\xe5\xa4\x9a\xe5\x88\xab\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe7\xae\x97\xe6\xb3\x95\xe3\x80\x82\n# Adam\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe5\x91\x8a\xe8\xaf\x89\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe5\xba\x94\xe8\xaf\xa5\xe6\x9b\xb4\xe6\x96\xb0\xe5\x93\xaa\xe4\xba\x9b\xe5\xbc\xa0\xe9\x87\x8f\xe3\x80\x82\nlearning_rate = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor t in range(500):\n\n    # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x9a\xe9\x80\x9a\xe8\xbf\x87\xe5\x83\x8f\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbe\x93\xe5\x85\xa5x\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84y\n    y_pred = model(x)\n\n    # \xe8\xae\xa1\xe7\xae\x97\xe5\xb9\xb6\xe6\x89\x93\xe5\x8d\xb0loss\n    loss = loss_fn(y_pred, y)\n    print(t, loss.item())\n    \n    # \xe5\x9c\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xb9\x8b\xe5\x89\x8d\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8optimizer\xe5\xb0\x86\xe5\xae\x83\xe8\xa6\x81\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6(\xe8\xbf\x99\xe4\xba\x9b\xe5\xbc\xa0\xe9\x87\x8f\xe6\x98\xaf\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d)\n    optimizer.zero_grad()\n\n    # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x9a\xe6\xa0\xb9\xe6\x8d\xae\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe8\xae\xa1\xe7\xae\x97loss\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n    loss.backward()\n\n    # \xe8\xb0\x83\xe7\x94\xa8Optimizer\xe7\x9a\x84step\xe5\x87\xbd\xe6\x95\xb0\xe4\xbd\xbf\xe5\xae\x83\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\n    optimizer.step()\n    """
PyTorch：定义自己的自动求导函数/two_layer_net_custom_function.py,0,"b'import torch\n\nclass MyReLU(torch.autograd.Function):\n    """"""\n    \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87\xe5\xbb\xba\xe7\xab\x8btorch.autograd\xe7\x9a\x84\xe5\xad\x90\xe7\xb1\xbb\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe6\x88\x91\xe4\xbb\xac\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84autograd\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\n    \xe5\xb9\xb6\xe5\xae\x8c\xe6\x88\x90\xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe6\xad\xa3\xe5\x90\x91\xe5\x92\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe3\x80\x82\n    """"""\n    @staticmethod\n    def forward(ctx, x):\n        """"""\n        \xe5\x9c\xa8\xe6\xad\xa3\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe6\x8e\xa5\xe6\x94\xb6\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x8a\xe4\xb8\x8b\xe6\x96\x87\xe5\xaf\xb9\xe8\xb1\xa1\xe5\x92\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8c\x85\xe5\x90\xab\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x9b\n        \xe6\x88\x91\xe4\xbb\xac\xe5\xbf\x85\xe9\xa1\xbb\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8c\x85\xe5\x90\xab\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x8c\n        \xe5\xb9\xb6\xe4\xb8\x94\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x8a\xe4\xb8\x8b\xe6\x96\x87\xe5\xaf\xb9\xe8\xb1\xa1\xe6\x9d\xa5\xe7\xbc\x93\xe5\xad\x98\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbe\xbf\xe5\x9c\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\n        """"""\n        ctx.save_for_backward(x)\n        return x.clamp(min=0)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        """"""\n        \xe5\x9c\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe6\x8e\xa5\xe6\x94\xb6\xe5\x88\xb0\xe4\xb8\x8a\xe4\xb8\x8b\xe6\x96\x87\xe5\xaf\xb9\xe8\xb1\xa1\xe5\x92\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x8c\n        \xe5\x85\xb6\xe5\x8c\x85\xe5\x90\xab\xe4\xba\x86\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe6\xad\xa3\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe4\xba\xa7\xe7\x94\x9f\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe3\x80\x82\n        \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbb\x8e\xe4\xb8\x8a\xe4\xb8\x8b\xe6\x96\x87\xe5\xaf\xb9\xe8\xb1\xa1\xe4\xb8\xad\xe6\xa3\x80\xe7\xb4\xa2\xe7\xbc\x93\xe5\xad\x98\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\n        \xe5\xb9\xb6\xe4\xb8\x94\xe5\xbf\x85\xe9\xa1\xbb\xe8\xae\xa1\xe7\xae\x97\xe5\xb9\xb6\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x8e\xe6\xad\xa3\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe7\x9b\xb8\xe5\x85\xb3\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe3\x80\x82\n        """"""\n        x, = ctx.saved_tensors\n        grad_x = grad_output.clone()\n        grad_x[x < 0] = 0\n        return grad_x\n\n\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n# N\xe6\x98\xaf\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x9b D_in \xe6\x98\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x9b\n# H \xe6\x98\xaf\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x9b D_out \xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\xb4\xe5\xba\xa6\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \xe4\xba\xa7\xe7\x94\x9f\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\x9a\x8f\xe6\x9c\xba\xe5\xbc\xa0\xe9\x87\x8f\nx = torch.randn(N, D_in, device=device)\ny = torch.randn(N, D_out, device=device)\n\n# \xe4\xba\xa7\xe7\x94\x9f\xe9\x9a\x8f\xe6\x9c\xba\xe6\x9d\x83\xe9\x87\x8d\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\nw1 = torch.randn(D_in, H, device=device, requires_grad=True)\nw2 = torch.randn(H, D_out, device=device, requires_grad=True)\n\nlearning_rate = 1e-6\nfor t in range(500):\n    # \xe6\xad\xa3\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x9a\xe4\xbd\xbf\xe7\x94\xa8\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\x8a\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe6\x9d\xa5\xe8\xae\xa1\xe7\xae\x97\xe8\xbe\x93\xe5\x87\xba\xe5\x80\xbcy\xef\xbc\x9b\n    # \xe6\x88\x91\xe4\xbb\xac\xe9\x80\x9a\xe8\xbf\x87\xe8\xb0\x83\xe7\x94\xa8 MyReLU.apply \xe5\x87\xbd\xe6\x95\xb0\xe6\x9d\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84ReLU\n    y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n\n    # \xe8\xae\xa1\xe7\xae\x97\xe5\xb9\xb6\xe8\xbe\x93\xe5\x87\xbaloss\n    loss = (y_pred - y).pow(2).sum()\n    print(t, loss.item())\n\n    # \xe4\xbd\xbf\xe7\x94\xa8autograd\xe8\xae\xa1\xe7\xae\x97\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbf\x87\xe7\xa8\x8b\xe3\x80\x82\n    loss.backward()\n\n    with torch.no_grad():\n        # \xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe9\x87\x8d\n        w1 -= learning_rate * w1.grad\n        w2 -= learning_rate * w2.grad\n\n        # \xe5\x9c\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xb9\x8b\xe5\x90\x8e\xe6\x89\x8b\xe5\x8a\xa8\xe6\xb8\x85\xe9\x9b\xb6\xe6\xa2\xaf\xe5\xba\xa6\n        w1.grad.zero_()\n        w2.grad.zero_()'"
PyTorch：定制神经网络nn模块/two_layer_net_module.py,0,"b'import torch\n\nclass TwoLayerNet(torch.nn.Module):\n    def __init__(self, D_in, H, D_out):\n        """"""\n        \xe5\x9c\xa8\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96\xe4\xba\x86\xe4\xb8\xa4\xe4\xb8\xaann.Linear\xe6\xa8\xa1\xe5\x9d\x97\xef\xbc\x8c\xe5\xb9\xb6\xe5\xb0\x86\xe5\xae\x83\xe4\xbb\xac\xe4\xbd\x9c\xe4\xb8\xba\xe6\x88\x90\xe5\x91\x98\xe5\x8f\x98\xe9\x87\x8f\xe3\x80\x82\n        """"""\n        super(TwoLayerNet, self).__init__()\n        self.linear1 = torch.nn.Linear(D_in, H)\n        self.linear2 = torch.nn.Linear(H, D_out)\n\n    def forward(self, x):\n        """"""\n        \xe5\x9c\xa8\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe6\x8e\xa5\xe6\x94\xb6\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x8c\xe4\xb9\x9f\xe5\xbf\x85\xe9\xa1\xbb\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe5\xbc\xa0\xe9\x87\x8f\xe3\x80\x82\n        \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9d\x97\xe4\xbb\xa5\xe5\x8f\x8a\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\x8a\xe7\x9a\x84\xe4\xbb\xbb\xe6\x84\x8f\xe7\x9a\x84\xef\xbc\x88\xe5\x8f\xaf\xe5\xbe\xae\xe5\x88\x86\xe7\x9a\x84\xef\xbc\x89\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\n        """"""\n        h_relu = self.linear1(x).clamp(min=0)\n        y_pred = self.linear2(h_relu)\n        return y_pred\n\n# N\xe6\x98\xaf\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x9b D_in \xe6\x98\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x9b\n# H \xe6\x98\xaf\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x9b D_out \xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\xb4\xe5\xba\xa6\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \xe4\xba\xa7\xe7\x94\x9f\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\x9a\x8f\xe6\x9c\xba\xe5\xbc\xa0\xe9\x87\x8f\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# \xe9\x80\x9a\xe8\xbf\x87\xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96\xe4\xb8\x8a\xe9\x9d\xa2\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe7\xb1\xbb\xe6\x9d\xa5\xe6\x9e\x84\xe5\xbb\xba\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x82\nmodel = TwoLayerNet(D_in, H, D_out)\n\n# \xe6\x9e\x84\xe9\x80\xa0\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\x92\x8c\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe3\x80\x82\n# SGD\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xe5\xaf\xb9model.parameters()\xe7\x9a\x84\xe8\xb0\x83\xe7\x94\xa8\xef\xbc\x8c\n# \xe5\xb0\x86\xe5\x8c\x85\xe5\x90\xab\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe5\x8d\xb3\xe4\xb8\xa4\xe4\xb8\xaann.Linear\xe6\xa8\xa1\xe5\x9d\x97\xe7\x9a\x84\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe5\x8f\x82\xe6\x95\xb0\xe3\x80\x82\nloss_fn = torch.nn.MSELoss(reduction=\'sum\')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\nfor t in range(500):\n    # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x9a\xe9\x80\x9a\xe8\xbf\x87\xe5\x90\x91\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbc\xa0\xe9\x80\x92x\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbcy\n    y_pred = model(x)\n\n    #\xe8\xae\xa1\xe7\xae\x97\xe5\xb9\xb6\xe8\xbe\x93\xe5\x87\xbaloss\n    loss = loss_fn(y_pred, y)\n    print(t, loss.item())\n\n    # \xe6\xb8\x85\xe9\x9b\xb6\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe9\x87\x8d\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()'"
PyTorch：张量(Tensors)/two_layer_net_tensor.py,0,"b""import torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n\n# N\xe6\x98\xaf\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x9b D_in \xe6\x98\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x9b\n# H \xe6\x98\xaf\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x9b D_out \xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\xb4\xe5\xba\xa6\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \xe4\xba\xa7\xe7\x94\x9f\xe9\x9a\x8f\xe6\x9c\xba\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe6\x95\xb0\xe6\x8d\xae\nx = torch.randn(N, D_in, device=device)\ny = torch.randn(N, D_out, device=device)\n\n# \xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x9d\x83\xe9\x87\x8d\nw1 = torch.randn(D_in, H, device=device)\nw2 = torch.randn(H, D_out, device=device)\n\nlearning_rate = 1e-6\nfor t in range(500):\n    # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x9a\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbcy\n    h = x.mm(w1)\n    h_relu = h.clamp(min=0)\n    y_pred = h_relu.mm(w2)\n\n    # \xe8\xae\xa1\xe7\xae\x97\xe5\xb9\xb6\xe8\xbe\x93\xe5\x87\xbaloss\xef\xbc\x9bloss\xe6\x98\xaf\xe5\xad\x98\xe5\x82\xa8\xe5\x9c\xa8PyTorch\xe7\x9a\x84tensor\xe4\xb8\xad\xe7\x9a\x84\xe6\xa0\x87\xe9\x87\x8f\xef\xbc\x8c\xe7\xbb\xb4\xe5\xba\xa6\xe6\x98\xaf()\xef\xbc\x88\xe9\x9b\xb6\xe7\xbb\xb4\xe6\xa0\x87\xe9\x87\x8f\xef\xbc\x89\xef\xbc\x9b\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8loss.item()\xe5\xbe\x97\xe5\x88\xb0tensor\xe4\xb8\xad\xe7\x9a\x84\xe7\xba\xafpython\xe6\x95\xb0\xe5\x80\xbc\xe3\x80\x82\n    loss = (y_pred - y).pow(2).sum()\n    print(t, loss.item())\n\n    # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97w1\xe3\x80\x81w2\xe5\xaf\xb9loss\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_w2 = h_relu.t().mm(grad_y_pred)\n    grad_h_relu = grad_y_pred.mm(w2.t())\n    grad_h = grad_h_relu.clone()\n    grad_h[h < 0] = 0\n    grad_w1 = x.t().mm(grad_h)\n\n    # \xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe9\x87\x8d\n    w1 -= learning_rate * grad_w1\n    w2 -= learning_rate * grad_w2"""
PyTorch：控制流和参数共享/dynamic_net.py,0,"b'import random\nimport torch\n\nclass DynamicNet(torch.nn.Module):\n    def __init__(self, D_in, H, D_out):\n        """"""\n        \xe5\x9c\xa8\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe6\x9e\x84\xe9\x80\xa0\xe4\xba\x86\xe4\xb8\x89\xe4\xb8\xaann.Linear\xe5\xae\x9e\xe4\xbe\x8b\xef\xbc\x8c\xe5\xae\x83\xe4\xbb\xac\xe5\xb0\x86\xe5\x9c\xa8\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe6\x97\xb6\xe8\xa2\xab\xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\n        """"""\n        super(DynamicNet, self).__init__()\n        self.input_linear = torch.nn.Linear(D_in, H)\n        self.middle_linear = torch.nn.Linear(H, H)\n        self.output_linear = torch.nn.Linear(H, D_out)\n\n    def forward(self, x):\n        """"""\n        \xe5\xaf\xb9\xe4\xba\x8e\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa90\xe3\x80\x811\xe3\x80\x812\xe3\x80\x813\xef\xbc\x8c\xe5\xb9\xb6\xe9\x87\x8d\xe7\x94\xa8\xe4\xba\x86\xe5\xa4\x9a\xe6\xac\xa1\xe8\xae\xa1\xe7\xae\x97\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84middle_linear\xe6\xa8\xa1\xe5\x9d\x97\xe3\x80\x82\n        \n        \xe7\x94\xb1\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe6\x9e\x84\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8a\xa8\xe6\x80\x81\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x9c\xa8\xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe6\x97\xb6\xe4\xbd\xbf\xe7\x94\xa8\xe5\xb8\xb8\xe8\xa7\x84Python\xe6\x8e\xa7\xe5\x88\xb6\xe6\xb5\x81\xe8\xbf\x90\xe7\xae\x97\xe7\xac\xa6\xef\xbc\x8c\xe5\xa6\x82\xe5\xbe\xaa\xe7\x8e\xaf\xe6\x88\x96\xe6\x9d\xa1\xe4\xbb\xb6\xe8\xaf\xad\xe5\x8f\xa5\xe3\x80\x82\n        \n        \xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe8\xbf\x98\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe5\x9c\xa8\xe5\xae\x9a\xe4\xb9\x89\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\xbd\xa2\xe6\x97\xb6\xe5\xa4\x9a\xe6\xac\xa1\xe9\x87\x8d\xe7\x94\xa8\xe5\x90\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\xe6\x98\xaf\xe5\xae\x8c\xe5\x85\xa8\xe5\xae\x89\xe5\x85\xa8\xe7\x9a\x84\xe3\x80\x82\xe8\xbf\x99\xe6\x98\xafLua Torch\xe7\x9a\x84\xe4\xb8\x80\xe5\xa4\xa7\xe6\x94\xb9\xe8\xbf\x9b\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xbaLua Torch\xe4\xb8\xad\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\xe5\x8f\xaa\xe8\x83\xbd\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x80\xe6\xac\xa1\xe3\x80\x82\n        """"""\n        h_relu = self.input_linear(x).clamp(min=0)\n        for _ in range(random.randint(0, 3)):\n            h_relu = self.middle_linear(h_relu).clamp(min=0)\n        y_pred = self.output_linear(h_relu)\n        return y_pred\n\n\n# N\xe6\x98\xaf\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x9bD\xe6\x98\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\xb4\xe5\xba\xa6\n# H\xe6\x98\xaf\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x9bD_out\xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\xb4\xe5\xba\xa6\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \xe4\xba\xa7\xe7\x94\x9f\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe9\x9a\x8f\xe6\x9c\xba\xe5\xbc\xa0\xe9\x87\x8f\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# \xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96\xe4\xb8\x8a\xe9\x9d\xa2\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe7\xb1\xbb\xe6\x9d\xa5\xe6\x9e\x84\xe9\x80\xa0\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\nmodel = DynamicNet(D_in, H, D_out)\n\n# \xe6\x9e\x84\xe9\x80\xa0\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x88loss function\xef\xbc\x89\xe5\x92\x8c\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xef\xbc\x88Optimizer\xef\xbc\x89\xe3\x80\x82\n# \xe7\x94\xa8\xe5\xb9\xb3\xe5\x87\xa1\xe7\x9a\x84\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x99\xe4\xb8\xaa\xe5\xa5\x87\xe6\x80\xaa\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x98\xaf\xe5\x9b\xb0\xe9\x9a\xbe\xe7\x9a\x84\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86momentum\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x82\ncriterion = torch.nn.MSELoss(reduction=\'sum\')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\nfor t in range(500):\n    \n    # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x9a\xe9\x80\x9a\xe8\xbf\x87\xe5\x90\x91\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbc\xa0\xe5\x85\xa5x\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84y\xe3\x80\x82\n    y_pred = model(x)\n\n    # \xe8\xae\xa1\xe7\xae\x97\xe5\xb9\xb6\xe6\x89\x93\xe5\x8d\xb0\xe6\x8d\x9f\xe5\xa4\xb1\n    loss = criterion(y_pred, y)\n    print(t, loss.item())\n\n    # \xe6\xb8\x85\xe9\x9b\xb6\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe9\x87\x8d \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n'"
PyTorch：神经网络模块nn/two_layer_net_nn.py,0,"b""import torch\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# N\xe6\x98\xaf\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x9bD\xe6\x98\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\xb4\xe5\xba\xa6\n# H\xe6\x98\xaf\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x9bD_out\xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\xb4\xe5\xba\xa6\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \xe4\xba\xa7\xe7\x94\x9f\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe9\x9a\x8f\xe6\x9c\xba\xe5\xbc\xa0\xe9\x87\x8f\nx = torch.randn(N, D_in, device=device)\ny = torch.randn(N, D_out, device=device)\n\n\n# \xe4\xbd\xbf\xe7\x94\xa8nn\xe5\x8c\x85\xe5\xb0\x86\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xba\xe4\xb8\x80\xe7\xb3\xbb\xe5\x88\x97\xe7\x9a\x84\xe5\xb1\x82\xe3\x80\x82\n# nn.Sequential\xe6\x98\xaf\xe5\x8c\x85\xe5\x90\xab\xe5\x85\xb6\xe4\xbb\x96\xe6\xa8\xa1\xe5\x9d\x97\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9d\x97\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8c\x89\xe9\xa1\xba\xe5\xba\x8f\xe5\xba\x94\xe7\x94\xa8\xe8\xbf\x99\xe4\xba\x9b\xe6\xa8\xa1\xe5\x9d\x97\xe6\x9d\xa5\xe4\xba\xa7\xe7\x94\x9f\xe5\x85\xb6\xe8\xbe\x93\xe5\x87\xba\xe3\x80\x82\n# \xe6\xaf\x8f\xe4\xb8\xaa\xe7\xba\xbf\xe6\x80\xa7\xe6\xa8\xa1\xe5\x9d\x97\xe4\xbd\xbf\xe7\x94\xa8\xe7\xba\xbf\xe6\x80\xa7\xe5\x87\xbd\xe6\x95\xb0\xe4\xbb\x8e\xe8\xbe\x93\xe5\x85\xa5\xe8\xae\xa1\xe7\xae\x97\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe5\xb9\xb6\xe4\xbf\x9d\xe5\xad\x98\xe5\x85\xb6\xe5\x86\x85\xe9\x83\xa8\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe5\x92\x8c\xe5\x81\x8f\xe5\xb7\xae\xe5\xbc\xa0\xe9\x87\x8f\xe3\x80\x82\n# \xe5\x9c\xa8\xe6\x9e\x84\xe9\x80\xa0\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8.to()\xe6\x96\xb9\xe6\xb3\x95\xe5\xb0\x86\xe5\x85\xb6\xe7\xa7\xbb\xe5\x8a\xa8\xe5\x88\xb0\xe6\x89\x80\xe9\x9c\x80\xe7\x9a\x84\xe8\xae\xbe\xe5\xa4\x87\xe3\x80\x82\nmodel = torch.nn.Sequential(\n            torch.nn.Linear(D_in, H),\n            torch.nn.ReLU(),\n            torch.nn.Linear(H, D_out),\n        ).to(device)\n\n\n# nn\xe5\x8c\x85\xe8\xbf\x98\xe5\x8c\x85\xe5\x90\xab\xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\xae\x9a\xe4\xb9\x89\xef\xbc\x9b\n# \xe5\x9c\xa8\xe8\xbf\x99\xe7\xa7\x8d\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x8b\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe4\xbd\xbf\xe7\x94\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe5\xb9\xb3\xe6\x96\xb9\xe8\xaf\xaf\xe5\xb7\xae(MSE)\xe4\xbd\x9c\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\n# \xe8\xae\xbe\xe7\xbd\xaereduction='sum'\xef\xbc\x8c\xe8\xa1\xa8\xe7\xa4\xba\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xa1\xe7\xae\x97\xe7\x9a\x84\xe6\x98\xaf\xe5\xb9\xb3\xe6\x96\xb9\xe8\xaf\xaf\xe5\xb7\xae\xe7\x9a\x84\xe2\x80\x9c\xe5\x92\x8c\xe2\x80\x9d\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x8d\xe6\x98\xaf\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc;\n# \xe8\xbf\x99\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe4\xb8\x8e\xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x89\x8b\xe5\xb7\xa5\xe8\xae\xa1\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\xe4\xbf\x9d\xe6\x8c\x81\xe4\xb8\x80\xe8\x87\xb4\xef\xbc\x8c\n# \xe4\xbd\x86\xe6\x98\xaf\xe5\x9c\xa8\xe5\xae\x9e\xe8\xb7\xb5\xe4\xb8\xad\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87\xe8\xae\xbe\xe7\xbd\xaereduction='elementwise_mean'\xe6\x9d\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe5\x9d\x87\xe6\x96\xb9\xe8\xaf\xaf\xe5\xb7\xae\xe4\xbd\x9c\xe4\xb8\xba\xe6\x8d\x9f\xe5\xa4\xb1\xe6\x9b\xb4\xe4\xb8\xba\xe5\xb8\xb8\xe8\xa7\x81\xe3\x80\x82\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\nlearning_rate = 1e-4\nfor t in range(500):\n\n    # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x9a\xe9\x80\x9a\xe8\xbf\x87\xe5\x90\x91\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbc\xa0\xe5\x85\xa5x\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84y\xe3\x80\x82\n    # \xe6\xa8\xa1\xe5\x9d\x97\xe5\xaf\xb9\xe8\xb1\xa1\xe9\x87\x8d\xe8\xbd\xbd\xe4\xba\x86__call__\xe8\xbf\x90\xe7\xae\x97\xe7\xac\xa6\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x83\x8f\xe5\x87\xbd\xe6\x95\xb0\xe9\x82\xa3\xe6\xa0\xb7\xe8\xb0\x83\xe7\x94\xa8\xe5\xae\x83\xe4\xbb\xac\xe3\x80\x82\n    # \xe8\xbf\x99\xe4\xb9\x88\xe5\x81\x9a\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe5\x90\x91\xe6\xa8\xa1\xe5\x9d\x97\xe4\xbc\xa0\xe5\x85\xa5\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\xae\x83\xe8\xbf\x94\xe5\x9b\x9e\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe5\xbc\xa0\xe9\x87\x8f\xe3\x80\x82\n    y_pred = model(x)\n    \n    # \xe8\xae\xa1\xe7\xae\x97\xe5\xb9\xb6\xe6\x89\x93\xe5\x8d\xb0\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x82\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\xa0\xe9\x80\x92\xe5\x8c\x85\xe5\x90\xaby\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\xe5\x92\x8c\xe7\x9c\x9f\xe5\xae\x9e\xe5\x80\xbc\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x8c\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe8\xbf\x94\xe5\x9b\x9e\xe5\x8c\x85\xe5\x90\xab\xe6\x8d\x9f\xe5\xa4\xb1\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xe3\x80\x82\n    loss = loss_fn(y_pred, y)\n    print(t, loss.item())\n    \n    # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xb9\x8b\xe5\x89\x8d\xe6\xb8\x85\xe9\x9b\xb6\xe6\xa2\xaf\xe5\xba\xa6\n    model.zero_grad()\n\n    # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x9a\xe8\xae\xa1\xe7\xae\x97\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\xaf\xb9\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\xef\xbc\x88\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x89\xe3\x80\x82\n    # \xe5\x9c\xa8\xe5\x86\x85\xe9\x83\xa8\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe5\xad\x98\xe5\x82\xa8\xe5\x9c\xa8requires_grad=True\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xad\xef\xbc\x8c\n    # \xe5\x9b\xa0\xe6\xad\xa4\xe8\xbf\x99\xe4\xb8\xaa\xe8\xb0\x83\xe7\x94\xa8\xe5\xb0\x86\xe8\xae\xa1\xe7\xae\x97\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe3\x80\x82\n    loss.backward()\n\n    # \xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe9\x87\x8d\xe3\x80\x82\n    # \xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe9\x83\xbd\xe6\x98\xaf\xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x83\x8f\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\xa5\xe5\x89\x8d\xe9\x82\xa3\xe6\xa0\xb7\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xbe\x97\xe5\x88\xb0\xe5\xae\x83\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\xe5\x92\x8c\xe6\xa2\xaf\xe5\xba\xa6\n    with torch.no_grad():\n        for param in model.parameters():\n            param.data -= learning_rate * param.grad"""
PyTorch：自动求导(Autograd)/two_layer_net_autograd.py,0,"b""# \xe5\x8f\xaf\xe8\xbf\x90\xe8\xa1\x8c\xe4\xbb\xa3\xe7\xa0\x81\xe8\xa7\x81\xe6\x9c\xac\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\xad\xe7\x9a\x84 two_layer_net_autograd.py\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n\n# N\xe6\x98\xaf\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x9bD_in\xe6\x98\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x9b\n# H\xe6\x98\xaf\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x9bD_out\xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\xb4\xe5\xba\xa6  \nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \xe4\xba\xa7\xe7\x94\x9f\xe9\x9a\x8f\xe6\x9c\xba\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe6\x95\xb0\xe6\x8d\xae\nx = torch.randn(N, D_in, device=device)\ny = torch.randn(N, D_out, device=device)\n\n# \xe4\xba\xa7\xe7\x94\x9f\xe9\x9a\x8f\xe6\x9c\xba\xe6\x9d\x83\xe9\x87\x8dtensor\xef\xbc\x8c\xe5\xb0\x86requires_grad\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xbaTrue\xe6\x84\x8f\xe5\x91\xb3\xe7\x9d\x80\xe6\x88\x91\xe4\xbb\xac\xe5\xb8\x8c\xe6\x9c\x9b\xe5\x9c\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe6\x97\xb6\xe5\x80\x99\xe8\xae\xa1\xe7\xae\x97\xe8\xbf\x99\xe4\xba\x9b\xe5\x80\xbc\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\nw1 = torch.randn(D_in, H, device=device, requires_grad=True)\nw2 = torch.randn(H, D_out, device=device, requires_grad=True)\n\nlearning_rate = 1e-6\nfor t in range(500):\n\n    # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe9\x80\x92:\xe4\xbd\xbf\xe7\x94\xa8\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\x8a\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbcy\xe3\x80\x82\xe7\x94\xb1\xe4\xba\x8ew1\xe5\x92\x8cw2\xe6\x9c\x89requires_grad=True\xef\xbc\x8c\xe6\xb6\x89\xe5\x8f\x8a\xe8\xbf\x99\xe4\xba\x9b\xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe5\xb0\x86\xe8\xae\xa9PyTorch\xe6\x9e\x84\xe5\xbb\xba\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xef\xbc\x8c\xe4\xbb\x8e\xe8\x80\x8c\xe5\x85\x81\xe8\xae\xb8\xe8\x87\xaa\xe5\x8a\xa8\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\xe3\x80\x82\xe7\x94\xb1\xe4\xba\x8e\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8d\xe5\x86\x8d\xe6\x89\x8b\xe5\xb7\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe4\xbf\x9d\xe7\x95\x99\xe4\xb8\xad\xe9\x97\xb4\xe5\x80\xbc\xe7\x9a\x84\xe5\xbc\x95\xe7\x94\xa8\xe3\x80\x82\n    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n\n    # \xe8\xae\xa1\xe7\xae\x97\xe5\xb9\xb6\xe8\xbe\x93\xe5\x87\xbaloss\xef\xbc\x8closs\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba()\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x8closs.item()\xe6\x98\xaf\xe8\xbf\x99\xe4\xb8\xaa\xe5\xbc\xa0\xe9\x87\x8f\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84python\xe6\x95\xb0\xe5\x80\xbc\n    loss = (y_pred - y).pow(2).sum()\n    print(t, loss.item())\n    \n    # \xe4\xbd\xbf\xe7\x94\xa8autograd\xe8\xae\xa1\xe7\xae\x97\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe3\x80\x82\xe8\xbf\x99\xe4\xb8\xaa\xe8\xb0\x83\xe7\x94\xa8\xe5\xb0\x86\xe8\xae\xa1\xe7\xae\x97loss\xe5\xaf\xb9\xe6\x89\x80\xe6\x9c\x89requires_grad=True\xe7\x9a\x84tensor\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe3\x80\x82\xe8\xbf\x99\xe6\xac\xa1\xe8\xb0\x83\xe7\x94\xa8\xe5\x90\x8e\xef\xbc\x8cw1.grad\xe5\x92\x8cw2.grad\xe5\xb0\x86\xe5\x88\x86\xe5\x88\xab\xe6\x98\xafloss\xe5\xaf\xb9w1\xe5\x92\x8cw2\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\xbc\xa0\xe9\x87\x8f\xe3\x80\x82\n    loss.backward()\n\n\n    # \xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe9\x87\x8d\xe3\x80\x82\xe5\xaf\xb9\xe4\xba\x8e\xe8\xbf\x99\xe4\xb8\x80\xe6\xad\xa5\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaa\xe6\x83\xb3\xe5\xaf\xb9w1\xe5\x92\x8cw2\xe7\x9a\x84\xe5\x80\xbc\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8e\x9f\xe5\x9c\xb0\xe6\x94\xb9\xe5\x8f\x98\xef\xbc\x9b\xe4\xb8\x8d\xe6\x83\xb3\xe4\xb8\xba\xe6\x9b\xb4\xe6\x96\xb0\xe9\x98\xb6\xe6\xae\xb5\xe6\x9e\x84\xe5\xbb\xba\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8torch.no_grad()\xe4\xb8\x8a\xe4\xb8\x8b\xe6\x96\x87\xe7\xae\xa1\xe7\x90\x86\xe5\x99\xa8\xe9\x98\xb2\xe6\xad\xa2PyTorch\xe4\xb8\xba\xe6\x9b\xb4\xe6\x96\xb0\xe6\x9e\x84\xe5\xbb\xba\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\n    with torch.no_grad():\n        w1 -= learning_rate * w1.grad\n        w2 -= learning_rate * w2.grad\n\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xb9\x8b\xe5\x90\x8e\xe6\x89\x8b\xe5\x8a\xa8\xe7\xbd\xae\xe9\x9b\xb6\xe6\xa2\xaf\xe5\xba\xa6\n        w1.grad.zero_()\n        w2.grad.zero_()"""
TensorFlow：静态图/tf_two_layer_net.py,2,"b'import tensorflow as tf\nimport numpy as np\n\n# \xe9\xa6\x96\xe5\x85\x88\xe6\x88\x91\xe4\xbb\xac\xe5\xbb\xba\xe7\xab\x8b\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xef\xbc\x88computational graph\xef\xbc\x89\n\n# N\xe6\x98\xaf\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x9bD\xe6\x98\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x9b\n# H\xe6\x98\xaf\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x9bD_out\xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\xb4\xe5\xba\xa6\xe3\x80\x82\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \xe4\xb8\xba\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe7\x9b\xae\xe6\xa0\x87\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x9b\xe5\xbb\xbaplaceholder\xef\xbc\x9b\n# \xe5\xbd\x93\xe6\x89\xa7\xe8\xa1\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe6\x97\xb6\xef\xbc\x8c\xe4\xbb\x96\xe4\xbb\xac\xe5\xb0\x86\xe4\xbc\x9a\xe8\xa2\xab\xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\xa1\xab\xe5\x85\x85\nx = tf.placeholder(tf.float32, shape=(None, D_in))\ny = tf.placeholder(tf.float32, shape=(None, D_out))\n\n# \xe4\xb8\xba\xe6\x9d\x83\xe9\x87\x8d\xe5\x88\x9b\xe5\xbb\xbaVariable\xe5\xb9\xb6\xe7\x94\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n# TensorFlow\xe7\x9a\x84Variable\xe5\x9c\xa8\xe6\x89\xa7\xe8\xa1\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe6\x97\xb6\xe4\xb8\x8d\xe4\xbc\x9a\xe6\x94\xb9\xe5\x8f\x98\nw1 = tf.Variable(tf.random_normal((D_in, H)))\nw2 = tf.Variable(tf.random_normal((H, D_out)))\n\n# \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x9a\xe4\xbd\xbf\xe7\x94\xa8TensorFlow\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xe8\xbf\x90\xe7\xae\x97\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbcy\xe3\x80\x82\n# \xe6\xb3\xa8\xe6\x84\x8f\xe8\xbf\x99\xe6\xae\xb5\xe4\xbb\xa3\xe7\xa0\x81\xe5\xae\x9e\xe9\x99\x85\xe4\xb8\x8a\xe4\xb8\x8d\xe6\x89\xa7\xe8\xa1\x8c\xe4\xbb\xbb\xe4\xbd\x95\xe6\x95\xb0\xe5\x80\xbc\xe8\xbf\x90\xe7\xae\x97\xef\xbc\x9b\n# \xe5\xae\x83\xe5\x8f\xaa\xe6\x98\xaf\xe5\xbb\xba\xe7\xab\x8b\xe4\xba\x86\xe6\x88\x91\xe4\xbb\xac\xe7\xa8\x8d\xe5\x90\x8e\xe5\xb0\x86\xe6\x89\xa7\xe8\xa1\x8c\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe3\x80\x82\nh = tf.matmul(x, w1)\nh_relu = tf.maximum(h, tf.zeros(1))\ny_pred = tf.matmul(h_relu, w2)\n\n# \xe4\xbd\xbf\xe7\x94\xa8TensorFlow\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xe8\xbf\x90\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1\xef\xbc\x88loss\xef\xbc\x89\nloss = tf.reduce_sum((y - y_pred) ** 2.0)\n\n# \xe8\xae\xa1\xe7\xae\x97loss\xe5\xaf\xb9\xe4\xba\x8ew1\xe5\x92\x8cw2\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\ngrad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n\n# \xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe9\x87\x8d\xe3\x80\x82\xe4\xb8\xba\xe4\xba\x86\xe5\xae\x9e\xe9\x99\x85\xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe5\x9c\xa8\xe6\x89\xa7\xe8\xa1\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe6\x97\xb6\xe8\xae\xa1\xe7\xae\x97new_w1\xe5\x92\x8cnew_w2\xe3\x80\x82\n# \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x8c\xe5\x9c\xa8TensorFlow\xe4\xb8\xad\xef\xbc\x8c\xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe9\x87\x8d\xe5\x80\xbc\xe7\x9a\x84\xe8\xa1\x8c\xe4\xb8\xba\xe6\x98\xaf\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe7\x9a\x84\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86;\n# \xe4\xbd\x86\xe5\x9c\xa8PyTorch\xe4\xb8\xad\xef\xbc\x8c\xe8\xbf\x99\xe5\x8f\x91\xe7\x94\x9f\xe5\x9c\xa8\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\xbd\xa2\xe4\xb9\x8b\xe5\xa4\x96\xe3\x80\x82\nlearning_rate = 1e-6\nnew_w1 = w1.assign(w1 - learning_rate * grad_w1)\nnew_w2 = w2.assign(w2 - learning_rate * grad_w2)\n\n# \xe7\x8e\xb0\xe5\x9c\xa8\xe6\x88\x91\xe4\xbb\xac\xe6\x90\xad\xe5\xbb\xba\xe5\xa5\xbd\xe4\xba\x86\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe5\xbc\x80\xe5\xa7\x8b\xe4\xb8\x80\xe4\xb8\xaaTensorFlow\xe7\x9a\x84\xe4\xbc\x9a\xe8\xaf\x9d\xef\xbc\x88session\xef\xbc\x89\xe6\x9d\xa5\xe5\xae\x9e\xe9\x99\x85\xe6\x89\xa7\xe8\xa1\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe3\x80\x82\nwith tf.Session() as sess:\n\n    # \xe8\xbf\x90\xe8\xa1\x8c\xe4\xb8\x80\xe6\xac\xa1\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe6\x9d\xa5\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96Variable w1\xe5\x92\x8cw2\n    sess.run(tf.global_variables_initializer())\n\n    # \xe5\x88\x9b\xe5\xbb\xbanumpy\xe6\x95\xb0\xe7\xbb\x84\xe6\x9d\xa5\xe5\xad\x98\xe5\x82\xa8\xe8\xbe\x93\xe5\x85\xa5x\xe5\x92\x8c\xe7\x9b\xae\xe6\xa0\x87y\xe7\x9a\x84\xe5\xae\x9e\xe9\x99\x85\xe6\x95\xb0\xe6\x8d\xae\n    x_value = np.random.randn(N, D_in)\n    y_value = np.random.randn(N, D_out)\n    \n    for _ in range(500):\n        # \xe5\xa4\x9a\xe6\xac\xa1\xe8\xbf\x90\xe8\xa1\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe3\x80\x82\xe6\xaf\x8f\xe6\xac\xa1\xe6\x89\xa7\xe8\xa1\x8c\xe6\x97\xb6\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe9\x83\xbd\xe7\x94\xa8feed_dict\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\n        # \xe5\xb0\x86x_value\xe7\xbb\x91\xe5\xae\x9a\xe5\x88\xb0x\xef\xbc\x8c\xe5\xb0\x86y_value\xe7\xbb\x91\xe5\xae\x9a\xe5\x88\xb0y\xef\xbc\x8c\n        # \xe6\xaf\x8f\xe6\xac\xa1\xe6\x89\xa7\xe8\xa1\x8c\xe5\x9b\xbe\xe5\xbd\xa2\xe6\x97\xb6\xe6\x88\x91\xe4\xbb\xac\xe9\x83\xbd\xe8\xa6\x81\xe8\xae\xa1\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81new_w1\xe5\x92\x8cnew_w2\xef\xbc\x9b\n        # \xe8\xbf\x99\xe4\xba\x9b\xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe5\x80\xbc\xe4\xbb\xa5numpy\xe6\x95\xb0\xe7\xbb\x84\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f\xe8\xbf\x94\xe5\x9b\x9e\xe3\x80\x82\n        loss_value, _, _ = sess.run([loss, new_w1, new_w2], \n                                    feed_dict={x: x_value, y: y_value})\n        print(loss_value)'"
热身：使用NumPy/two_layer_net_numpy.py,6,"b'#%%\n# \xe5\x8f\xaf\xe8\xbf\x90\xe8\xa1\x8c\xe4\xbb\xa3\xe7\xa0\x81\xe8\xa7\x81\xe6\x9c\xac\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\xad\xe7\x9a\x84 two_layer_net_numpy.py\nimport numpy as np\n\n#%%\n# N\xe6\x98\xaf\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x9bD_in\xe6\x98\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\xb4\xe5\xba\xa6\n# H\xe6\x98\xaf\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x9bD_out\xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\xb4\xe5\xba\xa6  \nN, D_in, H, D_out = 64, 1000, 100, 10\n\n#%%\n# \xe4\xba\xa7\xe7\x94\x9f\xe9\x9a\x8f\xe6\x9c\xba\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe6\x95\xb0\xe6\x8d\xae\nx = np.random.randn(N, D_in)\ny = np.random.randn(N, D_out)\n\n#%%\n# \xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x9d\x83\xe9\x87\x8d\nw1 = np.random.randn(D_in, H)\nw2 = np.random.randn(H, D_out)\n\n#%%\nlearning_rate = 1e-6\nfor t in range(500):\n    # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x9a\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbcy\n    h = x.dot(w1)\n    h_relu = np.maximum(h, 0)\n    y_pred = h_relu.dot(w2)\n\n    # \xe8\xae\xa1\xe7\xae\x97\xe5\xb9\xb6\xe6\x98\xbe\xe7\xa4\xbaloss\xef\xbc\x88\xe6\x8d\x9f\xe5\xa4\xb1\xef\xbc\x89\n    loss = np.square(y_pred - y).sum()\n    print(t, loss)\n\n    # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97w1\xe3\x80\x81w2\xe5\xaf\xb9loss\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_w2 = h_relu.T.dot(grad_y_pred)\n    grad_h_relu = grad_y_pred.dot(w2.T)\n    grad_h = grad_h_relu.copy()\n    grad_h[h < 0] = 0\n    grad_w1 = x.T.dot(grad_h)\n\n    # \xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe9\x87\x8d\n    w1 -= learning_rate * grad_w1\n    w2 -= learning_rate * grad_w2\n'"
