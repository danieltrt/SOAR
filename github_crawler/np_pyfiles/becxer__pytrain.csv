file_path,api_count,code
__init__.py,0,b''
setup.py,0,"b""#\n#  HOW TO UPLOAD PROJECT TO PYPI REPOSITORY\n#\n#  $ python setup.py sdist upload -r pypi\n#\n\nfrom distutils.core import setup\n\nsetup(\n        name='pytrain',\n        version='0.0.12',\n        packages = [\n            'pytrain',\n            'pytrain.lib',\n            'pytrain.KNN',\n            'pytrain.LinearRegression', \n            'pytrain.LogisticRegression',\n            'pytrain.GaussianNaiveBayes', \n            'pytrain.NaiveBayes',\n            'pytrain.DecisionTreeID3', \n            'pytrain.Kmeans',\n            'pytrain.DBSCAN', \n            'pytrain.Apriori',\n            'pytrain.HierarchicalClustering',\n            'pytrain.HMM',\n            'pytrain.CRF',\n            'pytrain.NeuralNetwork',\n            'pytrain.SVM'\n            ],\n        include_package_data=True,\n        package_data={'':['*.eng']},\n        author ='becxer',\n        author_email='becxer87@gmail.com',\n        url = 'https://github.com/becxer/pytrain',\n        description ='Machinelearning library for python',\n        long_description ='Machinelearning library for python',\n        license='MIT',\n        install_requires=['numpy'],\n        classifiers=[\n                'License :: OSI Approved :: MIT License',\n                'Programming Language :: Python :: 2.7',\n        ]\n)\n\n"""
test.py,0,b'#!/usr/bin/python\nfrom test_pytrain import test_main\n\n'
pytrain/__init__.py,0,b''
test_pytrain/__init__.py,0,b'from test_Suite import *'
test_pytrain/test_Suite.py,0,"b'#\n# test_Suite class for unit-test\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nimport traceback\nimport sys\n\n\nclass test_Suite:\n\n    global_value_set = {}\n\n    def __init__(self, logging = True):\n        self.logging = logging\n        pass\n\n    def tlog(self, log_str) : \n        if self.logging : \n            print ""*Log from \'"" + self.__class__.__name__ +\\\n                    ""\' Module : \\n"",log_str\n\n    def set_global_value(self, key, value):\n        self.__class__.global_value_set[key] = value\n\n    def get_global_value(self, key):\n        return self.__class__.global_value_set[key]\n\n    def test_process(self):\n        pass\n\n    def process(self):\n        tag = ""Module \'"" + self.__class__.__name__ + ""\' ""\n        print ""-""\n        try:\n            print tag + ""is now testing ...""\n            self.test_process()\n        except:\n            traceback.print_exception(*sys.exc_info())\n            print tag + ""result ... Error""\n        else:\n            print tag + ""result ... Ok""\n'"
test_pytrain/test_Template.py,0,"b""#\n# test template\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\n\n\nclass test_Template(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        global_value = self.get_global_value('some_key')\n        self.set_global_value('another_key', 'new_value')\n        \n        self.tlog('logging somthing')\n        \n        assert 1 == 1 \n"""
test_pytrain/test_main.py,0,b'#\n# Dev test for pytrain library\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n# --------------------------------------------\nimport sys\nfrom test_lib import *\nfrom test_KNN import *\nfrom test_DecisionTreeID3 import *\nfrom test_NaiveBayes import *\nfrom test_GaussianNaiveBayes import *\nfrom test_Apriori import *\nfrom test_LinearRegression import *\nfrom test_LogisticRegression import *\nfrom test_NeuralNetwork import *\nfrom test_Kmeans import *\nfrom test_DBSCAN import *\nfrom test_HierarchicalClustering import *\nfrom test_SVM import *\nfrom test_HMM import *\nfrom test_CRF import *\n\n# Toggle dataset\nIRIS = False # Toggle for IRIS dataset testing\nMNIST = False # Toggle for MNIST dataset testing\nWORD_SPACE = False # Toggle for WORD_SPACE dataset testing\n\n# 0. Test lib modules\ntest_dataset(logging = False).process()\ntest_fs(logging = False).process()\ntest_normalize(logging = False).process()\ntest_autotest(logging = False).process()\ntest_nlp(logging = False).process()\n\n# 1. Supervised learning\n# 1-1. Categorical variables\n# - Test Decision Tree\ntest_DecisionTreeID3(logging = False).process()\ntest_DecisionTreeID3_lense(logging = False).process()\n# - Test NaiveBayes\ntest_NaiveBayes(logging = False).process()\ntest_NaiveBayes_email(logging = False).process()\n\n# 1-2. Continuous variables\n# - Test GaussianNaiveBayes\ntest_GaussianNaiveBayes(logging = False).process()\ntest_GaussianNaiveBayes_rssi(logging = False).process()\n\n# - Test KNN\nif IRIS :\n    test_KNN_iris(logging = False).process()\nif MNIST :\n    test_KNN_mnist(logging = False).process()\n# - Test LinearRegression\ntest_LinearRegression(logging = False).process()\nif IRIS :\n    test_LinearRegression_iris(logging = False).process()\nif MNIST :\n    test_LinearRegression_mnist(logging = False).process()\n# - Test LogisticRegression\ntest_LogisticRegression(logging = False).process()\nif IRIS :\n    test_LogisticRegression_iris(logging = False).process()\nif MNIST :\n    test_LogisticRegression_mnist(logging = False).process()\n# - Test FNN\ntest_FNN(logging = False).process()\nif IRIS :\n    test_FNN_iris(logging = False).process()\nif MNIST:\n    test_FNN_mnist(logging = False).process()\n# - Test SVM\ntest_SVM(logging = False).process()\ntest_SVC(logging = False).process()\nif IRIS:\n    test_SVC_iris(logging = False).process()\nif MNIST:\n    test_SVC_mnist(logging = False).process()\n\n# 1-3. Sequential variables\n# - Test HMM\ntest_HMM(logging = False).process()\ntest_HMM_BaumWelch(logging = False).process()\nif WORD_SPACE:\n    # TODO : add word spacing test\n    pass\n\n# - Test CRF\ntest_CRF(logging = True).process()\nif WORD_SPACE:\n    # TODO : add word spacing test\n    pass\n\n# 2. Unsupervised learning\n# 2-1. Association\n# - Test Apriori\ntest_Apriori(logging = False).process()\ntest_Apriori_mushroom(logging = False).process()\n# 2-2. Clustering\n# - Test Kmeans\ntest_Kmeans(logging = False).process()\n# - Test DBSCAN\ntest_DBSCAN(logging = False).process()\n# - Test HierarchicalClustering\ntest_HierarchicalClustering(logging = False).process()\n\n'
pytrain/Apriori/Apriori.py,0,"b'#\n# Apriori \n#\n# @ author becxer\n# @ reference Machine Learning in Action by Peter Harrington\n# @ e-mail becxer87@gmail.com\n#\n\nfrom numpy import *\n\nclass Apriori:\n\n    def __init__(self, mat_data):\n        self.mat_data = mat_data\n        self.len_data = float(len(mat_data))\n        self.itemsets_list = []\n        self.support_data = {}\n        self.rules = []\n\n    def fit(self, min_support, min_confidence):\n        self.min_support = min_support\n        self.min_confidence = min_confidence\n        self.itemsets_list, self.support_data = \\\n            self.generate_itemsets_support_data(min_support)\n        self.rules = \\\n            self.generate_rules(min_confidence)\n    \n    # make unique item itemsets of data set\n    def gen_first_itemsets(self):\n        first_itemsets = []\n        for row in self.mat_data:\n            for item in row:\n                if not [item] in first_itemsets:\n                    first_itemsets.append([item])\n        return map(frozenset, first_itemsets)\n    \n    # generate itemsets from previous item\n    def gen_next_itemsets(self, itemsets):\n        next_itemsets = {}\n        len_itemset = len(itemsets)\n        for i in range(len_itemset):\n            for j in range(i+1, len_itemset):\n                next_itemsets[itemsets[i] | itemsets[j]] = 1\n        return next_itemsets.keys()\n\n    # filtering and terminate item which under the min_support\n    def filter_itemsets(self, itemsets, min_support):\n        itemset_occur_count = {}\n        for row in self.mat_data:\n            for item in itemsets:\n                if item.issubset(row):\n                    itemset_occur_count[item] = \\\n                            itemset_occur_count.get(item, 0) + 1\n\n        filtered_itemsets = []\n        support_data = {}\n        for key in itemset_occur_count:\n            support = itemset_occur_count[key] / self.len_data\n            if support >= min_support:\n                filtered_itemsets.append(key)\n            support_data[key] = support\n        return filtered_itemsets, support_data\n\n    def generate_itemsets_support_data(self, min_support):\n        first_itemsets = self.gen_first_itemsets()\n        flt_first_itemsets, fspd = \\\n            self.filter_itemsets(first_itemsets, min_support)\n        res_itemsets = [flt_first_itemsets]\n        res_spd = fspd\n        now_itemsets = flt_first_itemsets\n        while len(now_itemsets) > 0 :\n            next_itemsets = self.gen_next_itemsets(now_itemsets)\n            flt_itemsets, nspd = \\\n                self.filter_itemsets(next_itemsets, min_support)\n            res_spd.update(nspd)\n            res_itemsets.append(flt_itemsets)\n            now_itemsets = flt_itemsets\n        return res_itemsets, res_spd\n\n    def generate_rules(self, min_confidence):\n        res_rules = []\n        for itemsets in self.itemsets_list:\n            for itemset in itemsets:\n                first_itemsets = [frozenset([item]) for item in itemset]\n                new_itemsets = first_itemsets\n                while len(new_itemsets) > 0 :\n                    now_itemsets = new_itemsets\n                    new_itemsets = []\n                    for toset in now_itemsets:\n                        if len(toset) < len(itemset):\n                            fromset = itemset - toset\n                            conf = self.support_data[itemset] / self.support_data[fromset]\n                            if conf > min_confidence:\n                                res_rules.append((fromset,toset,conf))\n                                new_itemsets.append(toset)\n                    new_itemsets = self.gen_next_itemsets(new_itemsets)\n        return res_rules\n\n    def get_itemsets(self):\n        return self.itemsets_list\n\n    def get_support_data(self):\n        return self.support_data\n\n    def get_rules(self):\n        return self.rules\n\n    def recommend(self, array_input):\n        user_input = frozenset(array_input)\n        res = frozenset([]) \n        for itemsets in self.itemsets_list:\n            for itemset in itemsets:\n                if user_input.issubset(itemset):\n                    res = res|(itemset-user_input)\n        return res\n\n'"
pytrain/Apriori/__init__.py,0,b'from Apriori import *\n'
pytrain/CRF/CRF.py,42,"b'#\n# CRF\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\n\nimport numpy as np\nfrom pytrain.lib import convert\n\nclass CRF:\n\n    def __init__(self, mat_data, label_data, hidden_state_labeled = True, hidden_state = -1):\n        self.eps = np.finfo(np.float).eps / 1000000000000\n        self.mat_data = mat_data\n        self.label_data = label_data\n        self.n = len(mat_data[0][0])\n        if hidden_state_labeled :\n            self.label_set = []\n            for seq_label in label_data:\n                self.label_set.extend(seq_label)\n            self.label_set = list(set(self.label_set))\n            self.label_idx = { x:i for i, x in enumerate(self.label_set)}\n            self.m = len(self.label_set)\n            self.make_freqtable()\n        elif not hidden_state_labeled:\n            self.m = hidden_state\n            self.label_set = list(range(hidden_state))\n            self.make_randomtable()\n\n    def make_randomtable(self):\n        self.pi = np.zeros(self.m)\n        self.a = np.random.random([self.m, self.m]) + self.eps\n        self.b = np.random.random([self.m, self.n]) + self.eps\n        self.a = np.log(self.a / self.a.sum(axis=1).reshape((self.m,1)))\n        self.b = np.log(self.b / self.b.sum(axis=1).reshape((self.m,1)))\n        \n    def make_freqtable(self):\n        self.pi = np.zeros(self.m)\n        self.a = np.zeros([self.m, self.m]) + self.eps\n        self.b = np.zeros([self.m, self.n]) + self.eps\n        for seq_idx, seq_label in enumerate(self.label_data):\n            for i in range(len(seq_label)):\n                now = seq_label[i]\n                now_ob = self.mat_data[seq_idx][i]\n                self.b[self.label_idx[now]] += now_ob\n                if i >= 1:\n                    prev = seq_label[i-1]\n                    self.a[self.label_idx[prev]][self.label_idx[now]] += 1\n        self.b = np.log(self.b / (self.b.sum(axis=1).reshape((self.m,1))))\n        self.a = np.log(self.a / (self.a.sum(axis=1).reshape((self.m,1))))\n        \n    def viterbi(self, array_input):\n        T = len(array_input)\n        # self.prob :: index[0] is prob, index[1] is from idx\n        self.prob = np.log(np.zeros([T, self.m, 2]) + self.eps)\n        first_ob_idx = np.nonzero(array_input[0])[0]\n        first = self.pi + self.b[:,first_ob_idx].sum(axis=1)\n        first_prob = np.transpose(np.tile(first,(self.m,1)))\n        first_prob[:,1:] = -1\n        self.prob[0] = first_prob[:,:2]\n        for t in range(1,T):\n            now_ob_idx = np.nonzero(array_input[t])[0]\n            for j in range(self.m):\n                max_prob = self.prob[t][j][0]\n                max_idx = self.prob[t][j][1]\n                for i in range(self.m):\n                    now_prob = self.prob[t-1][i][0] + \\\n                      self.a[i][j] + self.b[j,now_ob_idx].sum(axis=0)\n                    if max_prob < now_prob:\n                        max_prob = now_prob\n                        max_idx = i\n                self.prob[t][j][0] = max_prob\n                self.prob[t][j][1] = max_idx\n        last_idx = -1\n        last_max = -10000000\n        for i in range(self.m):\n            if self.prob[T-1][i][0] > last_max:\n                last_idx = int(i)\n                last_max = self.prob[T-1][i][0]\n        trace = []\n        for at in range(T-1,-1,-1):\n            trace.append(int(last_idx))\n            last_idx = self.prob[at][int(last_idx)][1]\n        return last_max, trace[::-1]\n\n    def baum_welch(self, x_input):\n        T = len(x_input)\n        # alpha : probability of state i when see time 1...t\n        alpha = np.zeros([T, self.m])\n        # beta : probability of state i when see time t...T\n        beta = np.zeros([T, self.m])\n        # gamma : probability of state i when t\n        gamma = np.zeros([T, self.m])\n        # eta : probability of moving i to j when time t\n        eta = np.zeros([T, self.m, self.m])\n\n        # Get alpha, beta\n        first_ob_idx = np.nonzero(x_input[0])[0]\n        alpha[0] = self.pi + self.b[:,first_ob_idx].sum(axis=1)\n        beta[T-1] = np.ones(self.m)\n        for t in range(1,T):\n            tr = T - t - 1\n            forw_ob_idx = np.nonzero(x_input[t])[0]\n            back_ob_idx = np.nonzero(x_input[tr])[0]\n            for j in range(self.m):\n                sum_prob_forw = 0\n                sum_prob_back = 0\n                for i in range(self.m):\n                    sum_prob_forw += np.exp(alpha[t-1][i] + self.a[i][j] + self.b[j, forw_ob_idx].sum(axis=0))\n                    sum_prob_back += np.exp(beta[tr+1][i] + self.a[j][i] + self.b[i, back_ob_idx].sum(axis=0))\n                alpha[t][j] = np.log(sum_prob_forw)\n                beta[tr][j] = np.log(sum_prob_back)\n        beta[0] += self.pi\n        \n        # Get gamma\n        for t in range(T):\n            gamma_denom = 0\n            for i in range(self.m):\n                gamma[t][i] = alpha[t][i] + beta[t][i]\n                gamma_denom += np.exp(gamma[t][i])\n            gamma_denom = np.log(gamma_denom)\n            gamma[t] -= gamma_denom\n        gamma = np.exp(gamma) # gamma is not log value !\n            \n        # Get eta\n        for t in range(1, T):\n            eta_denom = 0\n            ob_idx = np.nonzero(x_input[t])[0]\n            for i in range(self.m):\n                for j in range(self.m):\n                    eta[t-1][i][j] = alpha[t-1][i] + self.a[i][j] + \\\n                                 self.b[j, ob_idx].sum(axis=0) + beta[t][j]\n                    eta_denom += np.exp(eta[t-1][i][j])\n            eta_denom = np.log(eta_denom)\n            eta[t-1] -= eta_denom\n        eta = np.exp(eta) # eta is not log value !\n        eta[T-1] = 0\n\n        # Get expected pi, a, b\n        Epi = gamma[0]\n        Ea = eta.sum(axis=0)/np.transpose([gamma.sum(axis=0)])\n        Eb = np.transpose(np.matmul(np.array(x_input).T, gamma) \\\n          / gamma.sum(axis=0))\n        return Epi, Ea, Eb\n        \n    def fit(self, toler, epoch):\n        for i in range(epoch):\n            Epi = [];  Ea = []; Eb = []\n            for idx in range(len(self.mat_data)):\n                x_input = self.mat_data[idx]\n                epi, ea, eb = self.baum_welch(x_input)\n                Epi.append(epi)\n                Ea.append(ea)\n                Eb.append(eb)\n            npi = np.log(np.array(Epi).sum(axis=0) + self.eps) - np.log(len(Epi)) \n            na = np.log(np.array(Ea).sum(axis=0) + self.eps) - np.log(len(Ea))\n            nb = np.log(np.array(Eb).sum(axis=0) + self.eps) - np.log(len(Eb))\n\n            tolpi = np.average(np.abs(np.exp(self.pi) - np.exp(npi)))\n            tola = np.average(np.abs(np.exp(self.a) - np.exp(na)))\n            tolb = np.average(np.abs(np.exp(self.b) - np.exp(nb)))\n\n            if tolpi < toler and tola < toler and tolb < toler:\n                break\n            else:\n                self.pi = npi\n                self.a = na\n                self.b = nb\n            \n    def predict(self, array_input, with_prob = False):\n        prob, seq_of_label = self.viterbi(array_input)\n        ret = [self.label_set[x] for x in seq_of_label]\n        if with_prob:\n            return ret, prob\n        else:\n            return ret\n'"
pytrain/CRF/__init__.py,0,b'from CRF import *\n'
pytrain/DBSCAN/DBSCAN.py,0,"b""#\n# DBSCAN\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\n\nfrom numpy import *\nfrom pytrain.lib import convert\nfrom pytrain.lib import ptmath\nimport operator\n\nclass DBSCAN:\n    \n    def __init__(self, mat_data, eps, min_pts, dist_func):\n        self.mat_data = convert.list2npfloat(mat_data)\n        self.dist_func = ptmath.distfunc(dist_func)\n        self.eps = eps\n        self.min_pts = min_pts\n        self.col_len = len(self.mat_data[0])\n        self.row_len = len(self.mat_data)\n        \n    def fit(self):\n        return self.cluster()\n    \n    def cluster(self):\n        # border point : connected to corepoint but less than 'min_pts' neighbor\n        # core point : neighbor more than 'min_pts' in 'eps' area\n        map_of_data = [[] for x in range(self.row_len)]\n        core_point_flag = [False for x in range(self.row_len)]\n        \n        visited = [False for x in range(self.row_len)]\n        label_of_data = [None for x in range(self.row_len)]\n\n        # 1. struct map (with eps-neighbor of a point)\n        for i, src in enumerate(self.mat_data):\n            for j, trg in enumerate(self.mat_data):\n                if (i is not j) and self.dist_func(src, trg) <= self.eps :\n                    map_of_data[i].append(j)\n                    if len(map_of_data[i]) >= self.min_pts:\n                        core_point_flag[i] = True\n                        \n        # 2. DFS map search and tag label\n        cluster_idx = 0\n        for idx, core in enumerate(core_point_flag):\n            if core and not visited[idx]:\n                stack = [idx]\n                while len(stack) != 0 :\n                    now_p = stack.pop()\n                    if not visited[now_p] :\n                        label_of_data[now_p] = cluster_idx\n                        visited[now_p] = True\n                        if core_point_flag[now_p]:\n                            for next_p in map_of_data[now_p]:\n                                if not visited[next_p] :\n                                    stack.append(next_p)\n                cluster_idx += 1\n\n        self.label_of_data = label_of_data\n        return label_of_data\n\n    # default predictor using KNN\n    def predict(self, input_array):\n        input_array = convert.list2npfloat(input_array)\n        label_count = {}\n        for idx, trg in enumerate(self.mat_data):\n            if self.dist_func(input_array, trg) <= self.eps:\n                label = self.label_of_data[idx]\n                label_count[label] = label_count.get(label, 0) + 1\n        sorted_label_count = sorted(label_count.iteritems()\\\n                , key=operator.itemgetter(1), reverse=True)\n        return sorted_label_count[0][0]\n\n"""
pytrain/DBSCAN/__init__.py,0,b'from DBSCAN import *\n'
pytrain/DecisionTreeID3/DecisionTreeID3.py,0,"b'#\n# Basic Decision Tree\n#\n# @ author becxer\n# @ reference Machine Learning in Action by Peter Harrington\n# @ e-mail becxer87@gmail.com\n#\n\nfrom numpy import *\nfrom math import log\nimport operator\n\nclass DecisionTreeID3:\n    def __init__(self, mat_data, label_data):\n        self.mat_data = mat_data\n        self.label_data = label_data\n        self.tree = {}\n\n    # make tree with matrix_data & label_data\n    def fit(self):\n        self.tree = self.create_tree(self.mat_data,self.label_data)\n        return self.tree\n\n    def build(self):\n        return self.fit()\n\n    # search array_input in tree\n    def predict(self, array_input):\n        return self.search_tree(self.tree, array_input)\n\n    # search array_input\'s feature in tree recursively\n    # if tree node is dictionary recursive\n    # else return label data\n    def search_tree(self, tree, array_input):\n        searched_label = ""not found""\n        node_col = tree.keys()[0]\n        node_dict = tree[node_col]\n        for node_val in node_dict.keys():\n            if array_input[node_col] == node_val:\n                if type(node_dict[node_val]).__name__ == \'dict\':\n                    next_input = array_input[:node_col]\n                    next_input.extend(array_input[node_col+1:])    \n                    searched_label = self.search_tree(node_dict[node_val],next_input)\n                else : searched_label = node_dict[node_val]\n        return searched_label\n\n    # create tree to lower entropy recursively\n    # when split data, calculate each feature split matrix entropy and compare\n    # select most lower entropy and split\n    # Example)  matrix => label ::  [[A,B] , [A,C], [A,D], [A,E] ,[B,D]] => [\'YES\',\'YES\',\'YES\',\'YES\',NO\' ]\n    # output example tree )   { 0 , { \'A\' : \'YES\', \'B\' : \'No\'}}\n    #                          |      |      |     |     |\n    #                          |      |      |     |     |\n    #                       column  value    |    value  |\n    #                                      label        label\n    #\n    def create_tree(self, mat_data, label_data):\n        # if left data has same label, then return label\n        if label_data.count(label_data[0]) == len(label_data):\n            return label_data[0]\n        # if there is no feature to split, then return most major label\n        if len(mat_data[0]) == 0 or ( len(mat_data[0]) == 1 and \\\n                len(set([row[0] for row in mat_data])) == 1 ) :\n            return self.major_label_count(label_data)\n        best_col_index = self.choose_col_to_split(mat_data, label_data)\n        tree = {best_col_index:{}}\n        best_col = [row[best_col_index] for row in mat_data]\n        uniq_val = list(set(best_col)) + [None]\n        for val in uniq_val:\n            split_mat, split_label = self.split_data(\\\n                                mat_data, label_data, best_col_index, val)\n            tree[best_col_index][val] = self.create_tree(split_mat, split_label)\n        return tree\n\n    # split matrix & label data with axis and it\'s value\n    def split_data(self, mat_data, label_data, axis, split_value):\n        ret_data = []\n        ret_label = []\n        for index, row in enumerate(mat_data):\n            if row[axis] == split_value or split_value == None:\n                temp = row[:axis]\n                temp.extend(row[axis+1:])\n                ret_data.append(temp)\n                ret_label.append(label_data[index])\n        return ret_data, ret_label\n    \n    # choose column to split comparing entropy\n    def choose_col_to_split(self, mat_data, label_data):\n        num_cols = len(mat_data[0]) \n        base_ent = self.calc_shannon_ent(label_data)\n        max_info = 0.0\n        best_col = -1\n        for i in range(num_cols):\n            col = [row[i] for row in mat_data]\n            uniq_col = set(col)\n            new_ent = 0.0\n            for val in uniq_col:\n                split_mat_data, split_label_data = \\\n                         self.split_data(mat_data, label_data, i ,val)\n                prob = len(split_label_data) / float(len(label_data))\n                new_ent += prob * self.calc_shannon_ent(split_label_data)\n            info = base_ent - new_ent\n            if info >= max_info:\n                max_info = info\n                best_col = i\n        return best_col\n\n    def calc_shannon_ent(self, label_data):\n        num_entry = len(label_data)\n        label_count = {}\n        for label in label_data:\n            if label not in label_count.keys():\n                label_count[label] = 0\n            label_count[label] += 1\n        shannon_ent = 0.0\n        for key in label_count:\n            prob = float(label_count[key]) / num_entry\n            shannon_ent -= prob * log(prob,2)\n        return shannon_ent    \n\n    def major_label_count(self, label_data):\n        label_count = {}\n        for label in label_data:\n            if label not in label_count.keys():\n                label_count[label] = 0\n            label_count[label] += 1\n        sorted_label_count = sorted(label_count.iteritems(),\n                                key=operator.itemgetter(1), reverse=True)\n        return sorted_label_count[0][0]\n\n\n'"
pytrain/DecisionTreeID3/__init__.py,0,b'from DecisionTreeID3 import *\n'
pytrain/GaussianNaiveBayes/GaussianNaiveBayes.py,0,"b'#\n# Gaussian Naive Bayes\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\n\nfrom numpy import *\nfrom pytrain.lib import convert\nimport operator\n\nclass GaussianNaiveBayes:\n\n    def __init__(self, mat_data, label_data):\n        self.mat_data = convert.list2npfloat(mat_data)\n        self.label_data = label_data\n\n        self.mat_mean = {}\n        self.mat_variance = {}\n        self.label_count = {}\n        self.label_count_sum = 0\n\n    # To calculate Gaussian, we have to get mean & variance for each Label\n    def fit(self):\n        self.col_size = len(self.mat_data[0])\n        for i, label in enumerate(self.label_data):\n            self.mat_mean[label] = self.mat_mean.get(label,\\\n                    zeros(self.col_size)) + self.mat_data[i]\n            self.label_count[label] = self.label_count.get(label,0) + 1\n            self.label_count_sum += 1\n\n        self.num_label = len(self.label_count)\n        self.label_map = self.label_count.keys()\n        self.label_count_arr = array([self.label_count.values()])\n\n        self.mat_mean_arr = array(self.mat_mean.values()) /\\\n                tile(self.label_count_arr.T,(1,self.col_size))\n        self.mat_mean_arr += finfo(float).eps\n\n        for i, label in enumerate(self.label_data):\n            self.mat_variance[label] = \\\n                    self.mat_variance.get(label, zeros(self.col_size)) + \\\n                    ((self.mat_data[i] \\\n                    - self.mat_mean_arr[self.label_map.index(label)]) ** 2)\n\n        self.mat_variance_arr = (array(self.mat_variance.values())\\\n                / tile(self.label_count_arr.T,(1,self.col_size))) ** 0.5\n        self.mat_variance_arr += finfo(float).eps\n\n    # Calculate gaussian probability\n    #\n    # ARG_MAX -> Label, \n    #    P( Label_i | X )\n    #        = -1\n    #          * SIGMA(j to n) {(x_j - mean_j)^2 / 2 * vari_j^2 + log(vari_j)}\n    #          + log ( Count_Label_i ) - log ( Count_Label_all )\n    #\n    def predict(self, array_input):\n        array_input = convert.list2npfloat(array_input)\n        deviate_arr = self.mat_mean_arr - tile(array_input,(self.num_label,1))\n        gaussian_bayes = (deviate_arr ** 2 / ((self.mat_variance_arr ** 2) * 2)) * -1 \\\n                - log(self.mat_variance_arr)\n        gaussian_prob = gaussian_bayes.sum(axis=1) + log(self.label_count_arr)\\\n            -log(tile(array(self.label_count_sum),(1,self.num_label)))\n        best_label_index = gaussian_prob[0].argsort()[::-1][0]\n        return self.label_map[ best_label_index]\n\n'"
pytrain/GaussianNaiveBayes/__init__.py,0,b'from GaussianNaiveBayes import *\n'
pytrain/HMM/HMM.py,42,"b'#\n# HMM\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\n\nimport numpy as np\nfrom pytrain.lib import convert\n\nclass HMM:\n\n    def __init__(self, mat_data, label_data, hidden_state_labeled = True, hidden_state = -1):\n        self.eps = np.finfo(np.float).eps / 1000000000000\n        self.mat_data = mat_data\n        self.label_data = label_data\n        self.n = len(mat_data[0][0])\n        if hidden_state_labeled :\n            self.label_set = []\n            for seq_label in label_data:\n                self.label_set.extend(seq_label)\n            self.label_set = list(set(self.label_set))\n            self.label_idx = { x:i for i, x in enumerate(self.label_set)}\n            self.m = len(self.label_set)\n            self.make_freqtable()\n        elif not hidden_state_labeled:\n            self.m = hidden_state\n            self.label_set = list(range(hidden_state))\n            self.make_randomtable()\n\n    def make_randomtable(self):\n        self.pi = np.zeros(self.m)\n        self.a = np.random.random([self.m, self.m]) + self.eps\n        self.b = np.random.random([self.m, self.n]) + self.eps\n        self.a = np.log(self.a / self.a.sum(axis=1).reshape((self.m,1)))\n        self.b = np.log(self.b / self.b.sum(axis=1).reshape((self.m,1)))\n        \n    def make_freqtable(self):\n        self.pi = np.zeros(self.m)\n        self.a = np.zeros([self.m, self.m]) + self.eps\n        self.b = np.zeros([self.m, self.n]) + self.eps\n        for seq_idx, seq_label in enumerate(self.label_data):\n            for i in range(len(seq_label)):\n                now = seq_label[i]\n                now_ob = self.mat_data[seq_idx][i]\n                self.b[self.label_idx[now]] += now_ob\n                if i >= 1:\n                    prev = seq_label[i-1]\n                    self.a[self.label_idx[prev]][self.label_idx[now]] += 1\n        self.b = np.log(self.b / (self.b.sum(axis=1).reshape((self.m,1))))\n        self.a = np.log(self.a / (self.a.sum(axis=1).reshape((self.m,1))))\n        \n    def viterbi(self, array_input):\n        T = len(array_input)\n        # self.prob :: index[0] is prob, index[1] is from idx\n        self.prob = np.log(np.zeros([T, self.m, 2]) + self.eps)\n        first_ob_idx = np.nonzero(array_input[0])[0]\n        first = self.pi + self.b[:,first_ob_idx].sum(axis=1)\n        first_prob = np.transpose(np.tile(first,(self.m,1)))\n        first_prob[:,1:] = -1\n        self.prob[0] = first_prob[:,:2]\n        for t in range(1,T):\n            now_ob_idx = np.nonzero(array_input[t])[0]\n            for j in range(self.m):\n                max_prob = self.prob[t][j][0]\n                max_idx = self.prob[t][j][1]\n                for i in range(self.m):\n                    now_prob = self.prob[t-1][i][0] + \\\n                      self.a[i][j] + self.b[j,now_ob_idx].sum(axis=0)\n                    if max_prob < now_prob:\n                        max_prob = now_prob\n                        max_idx = i\n                self.prob[t][j][0] = max_prob\n                self.prob[t][j][1] = max_idx\n        last_idx = -1\n        last_max = -10000000\n        for i in range(self.m):\n            if self.prob[T-1][i][0] > last_max:\n                last_idx = int(i)\n                last_max = self.prob[T-1][i][0]\n        trace = []\n        for at in range(T-1,-1,-1):\n            trace.append(int(last_idx))\n            last_idx = self.prob[at][int(last_idx)][1]\n        return last_max, trace[::-1]\n\n    def baum_welch(self, x_input):\n        T = len(x_input)\n        # alpha : probability of state i when see time 1...t\n        alpha = np.zeros([T, self.m])\n        # beta : probability of state i when see time t...T\n        beta = np.zeros([T, self.m])\n        # gamma : probability of state i when t\n        gamma = np.zeros([T, self.m])\n        # eta : probability of moving i to j when time t\n        eta = np.zeros([T, self.m, self.m])\n\n        # Get alpha, beta\n        first_ob_idx = np.nonzero(x_input[0])[0]\n        alpha[0] = self.pi + self.b[:,first_ob_idx].sum(axis=1)\n        beta[T-1] = np.ones(self.m)\n        for t in range(1,T):\n            tr = T - t - 1\n            forw_ob_idx = np.nonzero(x_input[t])[0]\n            back_ob_idx = np.nonzero(x_input[tr])[0]\n            for j in range(self.m):\n                sum_prob_forw = 0\n                sum_prob_back = 0\n                for i in range(self.m):\n                    sum_prob_forw += np.exp(alpha[t-1][i] + self.a[i][j] + self.b[j, forw_ob_idx].sum(axis=0))\n                    sum_prob_back += np.exp(beta[tr+1][i] + self.a[j][i] + self.b[i, back_ob_idx].sum(axis=0))\n                alpha[t][j] = np.log(sum_prob_forw)\n                beta[tr][j] = np.log(sum_prob_back)\n        beta[0] += self.pi\n        \n        # Get gamma\n        for t in range(T):\n            gamma_denom = 0\n            for i in range(self.m):\n                gamma[t][i] = alpha[t][i] + beta[t][i]\n                gamma_denom += np.exp(gamma[t][i])\n            gamma_denom = np.log(gamma_denom)\n            gamma[t] -= gamma_denom\n        gamma = np.exp(gamma) # gamma is not log value !\n            \n        # Get eta\n        for t in range(1, T):\n            eta_denom = 0\n            ob_idx = np.nonzero(x_input[t])[0]\n            for i in range(self.m):\n                for j in range(self.m):\n                    eta[t-1][i][j] = alpha[t-1][i] + self.a[i][j] + \\\n                                 self.b[j, ob_idx].sum(axis=0) + beta[t][j]\n                    eta_denom += np.exp(eta[t-1][i][j])\n            eta_denom = np.log(eta_denom)\n            eta[t-1] -= eta_denom\n        eta = np.exp(eta) # eta is not log value !\n        eta[T-1] = 0\n\n        # Get expected pi, a, b\n        Epi = gamma[0]\n        Ea = eta.sum(axis=0)/np.transpose([gamma.sum(axis=0)])\n        Eb = np.transpose(np.matmul(np.array(x_input).T, gamma) \\\n          / gamma.sum(axis=0))\n        return Epi, Ea, Eb\n        \n    def fit(self, toler, epoch):\n        for i in range(epoch):\n            Epi = [];  Ea = []; Eb = []\n            for idx in range(len(self.mat_data)):\n                x_input = self.mat_data[idx]\n                epi, ea, eb = self.baum_welch(x_input)\n                Epi.append(epi)\n                Ea.append(ea)\n                Eb.append(eb)\n            npi = np.log(np.array(Epi).sum(axis=0) + self.eps) - np.log(len(Epi)) \n            na = np.log(np.array(Ea).sum(axis=0) + self.eps) - np.log(len(Ea))\n            nb = np.log(np.array(Eb).sum(axis=0) + self.eps) - np.log(len(Eb))\n\n            tolpi = np.average(np.abs(np.exp(self.pi) - np.exp(npi)))\n            tola = np.average(np.abs(np.exp(self.a) - np.exp(na)))\n            tolb = np.average(np.abs(np.exp(self.b) - np.exp(nb)))\n\n            if tolpi < toler and tola < toler and tolb < toler:\n                break\n            else:\n                self.pi = npi\n                self.a = na\n                self.b = nb\n            \n    def predict(self, array_input, with_prob = False):\n        prob, seq_of_label = self.viterbi(array_input)\n        ret = [self.label_set[x] for x in seq_of_label]\n        if with_prob:\n            return ret, prob\n        else:\n            return ret\n'"
pytrain/HMM/__init__.py,0,b'from HMM import *\n'
pytrain/HierarchicalClustering/HierarchicalClustering.py,0,"b'#\n# HierarchicalClustering\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\n\nfrom numpy import *\nfrom pytrain.lib import convert\nfrom pytrain.lib import ptmath\nimport operator\n\nclass HierarchicalClustering:\n\n    def __init__(self, mat_data, K, dist_func):\n        self.mat_data = convert.list2npfloat(mat_data)\n        self.dist_func = ptmath.distfunc(dist_func)\n        self.K = K\n        self.col_len = len(self.mat_data[0])\n        self.row_len = len(self.mat_data)\n        self.unique_idx = 0\n        self.group_list = []\n        self.dist_list = []\n        self.cluster_points = []\n        \n    def fit(self):\n        return self.cluster()\n\n    unique_idx = 0\n    group_map = {}\n\n    class Group:\n        def __init__(self, vt, didx):\n            self.unique_idx = HierarchicalClustering.unique_idx\n            self.vector = vt\n            self.data_idx = didx\n            HierarchicalClustering.group_map[self.unique_idx] = self\n            HierarchicalClustering.unique_idx += 1\n\n        def log(self):\n            print ""[ Group"", self.unique_idx, ""] vt : "", self.vector, "", data_idx : "", str(self.data_idx)\n\n    class Dist:\n        def __init__(self, s_grp, t_grp, dist_func):\n            self.src_idx = s_grp.unique_idx\n            self.trg_idx = t_grp.unique_idx\n            self.distance = dist_func(s_grp.vector, t_grp.vector)\n\n        def log(self):\n            print ""[ Dist ]"", self.src_idx, ""-"", self.trg_idx, "" = "", self.distance\n\n    def remove_from_dist_list(self, grp_idx):\n        self.dist_list = [dist_obj for dist_obj in self.dist_list \\\n            if (dist_obj.src_idx != grp_idx and dist_obj.trg_idx != grp_idx)]\n\n    def remove_from_group_list(self, grp_idx):\n        self.group_list = [grp_obj for grp_obj in self.group_list \\\n            if (grp_obj.unique_idx != grp_idx) ]\n        \n    def insert_new_group(self, grp):\n        for oth in self.group_list:\n            new_dis = self.Dist(grp, oth, self.dist_func)\n            for idx, old_dis in enumerate(self.dist_list):\n                if new_dis.distance >= old_dis.distance:\n                    self.dist_list.insert(idx, new_dis)\n                    break\n        self.group_list.append(grp)\n    \n    def merge_group(self, grp_1_idx, grp_2_idx):\n        grp_1 = self.group_map[grp_1_idx]\n        grp_2 = self.group_map[grp_2_idx]\n\n        mgd_vt = ( (grp_1.vector * len(grp_1.data_idx)) \\\n                       + (grp_2.vector * len(grp_2.data_idx)) ) \\\n                           / ( len(grp_1.data_idx) + len(grp_2.data_idx))\n        mgd_didx = []\n        mgd_didx.extend(grp_1.data_idx)\n        mgd_didx.extend(grp_2.data_idx)\n        mgd_grp = self.Group(mgd_vt, mgd_didx)\n        return mgd_grp\n    \n    def cluster(self):\n        # make initial groups\n        for idx, vt in enumerate(self.mat_data):\n            self.group_list.append(self.Group(vt, [idx]))\n\n        # make dist_list\n        for i, src_g in enumerate(self.group_list):\n            for j in range(i+1,len(self.group_list)):\n                trg_g = self.group_list[j]\n                self.dist_list.append(self.Dist(src_g, trg_g, self.dist_func))\n                \n        # merge group until length of group list less than K\n        self.dist_list.sort(key=lambda x : x.distance,reverse = True)\n\n        while len(self.group_list) > self.K :\n            selected_dist = self.dist_list.pop()\n            new_group = self.merge_group(selected_dist.src_idx, selected_dist.trg_idx)\n            self.remove_from_dist_list(selected_dist.src_idx)\n            self.remove_from_dist_list(selected_dist.trg_idx)\n            self.remove_from_group_list(selected_dist.src_idx)\n            self.remove_from_group_list(selected_dist.trg_idx)\n            self.insert_new_group(new_group)\n            \n        # loop group list & fill label data\n        self.label_data = [-1 for x in range(len(self.mat_data))]\n        for grp_idx, grp in enumerate(self.group_list):\n            self.cluster_points.append(grp.vector)\n            for idx in grp.data_idx:\n                self.label_data[idx] = grp_idx\n        return self.label_data\n\n    # assign input array to cluster\n    def predict(self, input_array):\n        input_array = convert.list2npfloat(input_array)\n        return self.assign_row(self.cluster_points, input_array)\n        \n    def assign_row(self, cluster_points, row):\n        min_idx = -1\n        min_dist = None\n        for i, cp in enumerate(cluster_points):\n            cp_dist = self.dist_func(row, cp)\n            if min_dist == None or min_dist > cp_dist:\n                min_dist = cp_dist\n                min_idx = i\n        return min_idx\n'"
pytrain/HierarchicalClustering/__init__.py,0,b'from HierarchicalClustering import *\n'
pytrain/KNN/KNN.py,1,"b'#\n# Basic K-Nearest Neighbors\n#\n# @ author becxer\n# @ reference Machine Learning in Action by Peter Harrington\n# @ e-mail becxer87@gmail.com\n#\n\nimport numpy as np\nfrom pytrain.lib import convert\nfrom pytrain.lib import ptmath\nimport operator\n\nclass KNN:\n    def __init__(self, mat_data, label_data, k, dist_func):\n        self.mat_data = convert.list2npfloat(mat_data)\n        self.dist_func = ptmath.distfunc(dist_func)\n        self.label_data = label_data\n        self.train_size = self.mat_data.shape[0]\n        self.k = k\n\n    def fit(self):\n        pass\n\n    # compare distance from all mat_data rows and choose most closer one\n    def predict(self, array_input):\n        array_input = convert.list2npfloat(array_input)\n\n        distances = []\n        for trg in self.mat_data:\n            distances.append(self.dist_func(array_input, trg))\n        distances = np.array(distances)\n        \n        sorted_distances = distances.argsort()\n        class_count = {}\n        for i in range(self.k):\n            kth_label = str(self.label_data[sorted_distances[i]])\n            class_count[kth_label] = class_count.get(kth_label, 0) + 1\n        sorted_class_count = sorted(class_count.iteritems()\n            , key=operator.itemgetter(1), reverse=True)\n        return sorted_class_count[0][0]\n\n'"
pytrain/KNN/__init__.py,0,b'from KNN import *\n'
pytrain/Kmeans/Kmeans.py,0,"b'#\n# Kmeans\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\n\nfrom numpy import *\nfrom pytrain.lib import convert\nfrom pytrain.lib import ptmath\nimport math\n\nclass Kmeans:\n    \n    def __init__(self, mat_data, dist_func):\n        self.mat_data = convert.list2npfloat(mat_data)\n        self.dist_func = ptmath.distfunc(dist_func)\n        self.col_len = float(len(self.mat_data[0]))\n        self.row_len = float(len(self.mat_data))\n        self.min_col = self.mat_data.min(axis=0)\n        self.max_col = self.mat_data.max(axis=0)\n\n    # assign input array to cluster\n    def predict(self, input_array):\n        input_array = convert.list2npfloat(input_array)\n        return self.assign_row(self.cluster_points, input_array)\n        \n    def assign_row(self, cluster_points, row):\n        min_idx = -1\n        min_dist = None\n        for i, cp in enumerate(cluster_points):\n            cp_dist = self.dist_func(row, cp)\n            if min_dist == None or min_dist > cp_dist:\n                min_dist = cp_dist\n                min_idx = i\n        return min_idx\n            \n    def assign_mat_data(self, cluster_points):\n        cluster_set = {}\n        # assign class for every data\n        for i, row in enumerate(self.mat_data):\n            min_idx = self.assign_row(cluster_points, row)\n            cluster_set[min_idx] = cluster_set.get(min_idx, [])\n            cluster_set[min_idx].append(row)\n        return cluster_set\n\n    \n    # silhouette cluster metric\n    #\n    # s(i) = 1 - a(i)/b(i), if a(i) < b(i)\n    # s(i) = 0, if a(i) = b(i)\n    # s(i) = b(i)/a(i) - 1, if a(i) > b(i)\n    def metric(self, cluster_points):\n        cluster_set = self.assign_mat_data(cluster_points)\n        sil_res = 0\n        for idx in cluster_set:\n            cl = cluster_set[idx]\n            for i_data in cl:\n                a_i = 0.\n                for oth_data in cl:\n                    a_i += self.dist_func(i_data, oth_data)\n                if len(cl) > 0 :\n                    a_i /= float(len(cl))\n                b_i = inf\n                for jdx in cluster_set:\n                    if jdx != idx:\n                        for oth_data in cluster_set[jdx]:\n                            dist = self.dist_func(i_data, oth_data)\n                            if b_i == None or dist < b_i:\n                                b_i = dist\n                sil_i = 0\n                if max(b_i,a_i) != 0 :\n                    sil_i = float((b_i - a_i) / max(b_i,a_i))\n                sil_res += sil_i\n        sil_res /= float(self.row_len)\n        return sil_res\n    \n    def cluster(self, K, epoch):\n        # set random point for K class\n        cluster_points = random.random_sample((K, int(self.col_len)))\n        cluster_points = (cluster_points * (self.max_col - self.min_col))\\\n          + self.min_col\n\n        # Lloyd algorithm  \n        for ep in range(epoch):\n            cluster_set = self.assign_mat_data(cluster_points)\n            # reassign K class with average of class items\n            for idx in cluster_set:\n                new_k_row = array(cluster_set[idx])\n                cluster_points[idx] = new_k_row.sum(axis=0) / len(new_k_row)\n\n        self.cluster_points = cluster_points\n        return cluster_points\n\n                \n    # Good clustering finding method\n    def fit(self, max_K ,random_try_count, epoch):\n\n        cluster_points_good = None\n        cluster_points_good_metric = None\n\n        for K in range(2,max_K+1):\n            for i in range(random_try_count):\n                cluster_points = self.cluster(K, epoch)\n                metric_val = self.metric(cluster_points)\n                if cluster_points_good_metric == None \\\n                  or metric_val > cluster_points_good_metric:\n                    cluster_points_good_metric = metric_val\n                    cluster_points_good = cluster_points\n        self.cluster_points = cluster_points_good        \n        return self.cluster_points\n\n'"
pytrain/Kmeans/__init__.py,0,b'from Kmeans import *\n'
pytrain/LinearRegression/LinearRegression.py,5,"b'#\n# Linear Regression\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\n\nimport numpy as np\nfrom pytrain.lib import convert\nimport math\nimport time\nimport random\nimport sys\n\nclass LinearRegression:\n\n    def __init__(self, mat_data, label_data):\n        self.mat_data = convert.list2npfloat(mat_data)\n        self.label_data = convert.list2npfloat(label_data)\n        \n        self.out_bit = len(label_data[0])\n        self.mat_w =  np.array([ [ (random.random() * 0.00001 + sys.float_info.epsilon)\\\n                            for i in range(len(mat_data[0]))] \\\n                                for j in range(self.out_bit) ], dtype = np.float64)\n\n        self.mat_w0 = np.array([ random.random() * 0.00001 + sys.float_info.epsilon\\\n                            for i in range(self.out_bit) ], dtype = np.float64)\n\n    #\n    # Description of differential equation\n    #\n    # k = w0 + w1 x1 + w2 x2 + w3 x3 + .. + wn xn\n    # J(k) = (y - k1)^2 + (y - k2)^2 + .. + (y - kn)^2\n    #\n    # dJ/dw1 = dJ/dk * dk/dw1 = \n    #                    - 2 * (y - k1) * x1_1\n    #                    - 2 * (y - k2) * x1_2 \n    #                    - 2 * (y - k3) * x1_3\n    #                    ...\n    # \n    # UPDATE w1 with gradient\n    # w1 = w1 - lr * dJ/dw1\n    #\n    # dJ/dw0 = dJ/dk0 * dk0/dw0 = \n    #                   - 2 * (y - k1) * 1\n    #                   - 2 * (y - k2) * 1\n    #                   ...\n    #\n    # w0 = w0 - lr * dJ/w0\n    #\n\n    def batch_update_w(self, out_bit_index, data, label):\n        w = self.mat_w[out_bit_index]\n        w0 = self.mat_w0[out_bit_index]\n        tiled_w0 = np.tile(w0,(len(data)))\n        k = (w * data).sum(axis=1) + tiled_w0\n        gd = (k - label.T[out_bit_index])\n        # dJ_dw is gradient of J(w) function\n        dJ_dw = (gd * data.T).sum(axis=1)/len(data)\n        dJ_dw0  = gd.sum(axis=0)/len(data)\n        w = w - (dJ_dw * self.lr)\n        w0 = w0 - (dJ_dw0 * self.lr)\n        self.mat_w[out_bit_index] = w\n        self.mat_w0[out_bit_index] = w0\n\n    def fit(self, lr, epoch, batch_size):\n        self.lr = lr\n        self.epoch = epoch\n        start = 0\n        end = batch_size\n        datalen = len(self.mat_data)\n        for ep in range(epoch):\n            start = 0\n            end = batch_size\n            while start < datalen :\n                for i in range(self.out_bit):\n                    self.batch_update_w(i, self.mat_data[start:end],\\\n                        self.label_data[start:end])\n                start = end\n                end += batch_size\n\n    def predict(self, array_input):\n        array_input = convert.list2npfloat(array_input)\n        return (array_input * self.mat_w).sum(axis=1) \\\n                + self.mat_w0\n\n'"
pytrain/LinearRegression/__init__.py,0,b'from LinearRegression import *\n'
pytrain/LogisticRegression/LogisticRegression.py,5,"b'#\n# Logistic Regression\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\n\nimport numpy as np\nfrom pytrain.lib import convert\nfrom pytrain.lib import ptmath\nimport math\nimport time\nimport random\nimport sys\n\nclass LogisticRegression:\n\n    def __init__(self, mat_data, label_data):\n        self.mat_data = convert.list2npfloat(mat_data)\n        self.label_data = convert.list2npfloat(label_data)\n\n        self.out_bit = len(label_data[0])\n        self.mat_w = np.array([ [random.random() * 0.001 + sys.float_info.epsilon\\\n                            for i in range(len(mat_data[0]))] \\\n                                for j in range(self.out_bit) ], dtype=np.float64)\n        self.mat_w0 = np.array([random.random() * 0.001 + sys.float_info.epsilon\\\n                            for i in range(self.out_bit) ], dtype=np.float64)\n\n    def batch_update_w(self, out_bit_index, data, label):\n        w = self.mat_w[out_bit_index]\n        w0 = self.mat_w0[out_bit_index]\n        tiled_w0 = np.tile(w0,(len(data)))\n        k = (w * data).sum(axis=1) + tiled_w0\n        sig_k = ptmath.sigmoid(k)\n        gd = (sig_k - label.T[out_bit_index])\n        dw = (gd * data.T).sum(axis=1)/len(data)\n        dw0  = gd.sum(axis=0)/len(data)\n        w = w - (dw * self.lr)\n        w0 = w0 - (dw0 * self.lr)\n        self.mat_w[out_bit_index] = w\n        self.mat_w0[out_bit_index] = w0\n\n    def fit(self, lr, epoch, batch_size):\n        self.lr = lr\n        self.epoch = epoch\n        datalen = len(self.mat_data)\n        for ep in range(epoch):\n            start = 0\n            end = batch_size\n            while start < datalen :\n                for i in range(self.out_bit):\n                    self.batch_update_w(i, self.mat_data[start:end],\\\n                        self.label_data[start:end])\n                start = end\n                end += batch_size\n\n    def predict(self, array_input):\n        array_input = convert.list2npfloat(array_input)\n        return map(round,map(ptmath.sigmoid,(array_input * self.mat_w).sum(axis=1) \\\n                + self.mat_w0))\n\n'"
pytrain/LogisticRegression/__init__.py,0,b'from LogisticRegression import *\n'
pytrain/NaiveBayes/NaiveBayes.py,0,"b'#\n# Basic Naive Bayes\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\n\nfrom numpy import *\n\nclass NaiveBayes:\n\n    # cate (category) : label\n    # word : mat (matrix)\n    def __init__(self, mat_data, label_data):\n        self.word_data = mat_data\n        self.num_word = 0\n        \n        self.cate_data = label_data\n        self.cate_count = {}\n        self.cate_count_sum = 0\n        self.cate_map = []\n        self.num_cate = 0\n        \n        self.cate_word_vector = {}\n        self.cate_word_vector_sum = {}\n    \n    # prepare matrix of category word count\n    def fit(self):\n        self.num_word = len(self.word_data[0])\n        for i, cate in enumerate(self.cate_data):\n            self.cate_word_vector[cate] = self.cate_word_vector.get(cate,\\\n                    zeros(self.num_word)) + self.word_data[i]\n            self.cate_count[cate] = self.cate_count.get(cate, 0) + 1\n            self.cate_count_sum += 1\n\n        for cate in self.cate_word_vector:\n            self.cate_word_vector_sum[cate] = self.cate_word_vector[cate].sum(axis=0)\n\n        self.num_cate = len(self.cate_count)\n        self.cate_word_vector = array(self.cate_word_vector.values())\n        self.cate_word_vector_sum = array(self.cate_word_vector_sum.values())\n        self.cate_map = array(self.cate_count.keys())\n        self.cate_count = array(self.cate_count.values())\n\n\n    # array_input is possibilities of words\n    def predict(self, array_input):\n        inprod_cate_word = self.cate_word_vector * tile(array_input,(self.num_cate,1))\n        logged_inprod_cate_word = array(map(lambda words : \\\n                map( lambda wc: log(wc+1) , words) , inprod_cate_word))\n        \n        tiled_cate_word_vector_sum = tile(self.cate_word_vector_sum.T, (self.num_word,1)).T\n        logged_tiled_cate_word_vector_sum = array(map(lambda vt_sums : \\\n                map(lambda vt_sum: log(vt_sum+ self.num_word),vt_sums),tiled_cate_word_vector_sum))\n        \n        logged_cate_word_prob = (logged_inprod_cate_word - logged_tiled_cate_word_vector_sum).sum(axis=1)\n        logged_cate_prob = array(map(lambda c : log(c)-log(self.cate_count_sum), self.cate_count))\n        \n        prob_result = logged_cate_prob + logged_cate_word_prob\n        best_cate_index = prob_result.argsort()[::-1][0]\n        return self.cate_map[best_cate_index]\n\n'"
pytrain/NaiveBayes/__init__.py,0,b'from NaiveBayes import *\n'
pytrain/NeuralNetwork/FNN.py,3,"b""#\n# Feedforward Neural Network\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\n\nimport numpy as np\nfrom pytrain.lib import convert\nfrom pytrain.lib import ptmath\nimport sys\n\nclass FNN:\n\n    def __init__(self, mat_data, label_data, hl_list):\n        self.mat_data = convert.list2npfloat(mat_data)\n        self.y = convert.list2npfloat(label_data)\n        self.il_size = self.mat_data.shape[1]\n        self.ol_size = self.y.shape[1]\n        self.hl_list = hl_list\n        self.W = {}\n        self.B = {}\n        self.hl_list.append(self.ol_size)\n        last_layer_num = self.il_size\n        for idx, hl_num in enumerate(hl_list):\n            self.W['WD_' + str(idx)] = 0.00001 * np.random.randn(hl_num, last_layer_num)\n            self.B['BD_' + str(idx)] = 0.00001 * np.random.randn(hl_num)\n            last_layer_num = hl_num\n            \n    def feedforward(self, x):\n        last_input = x\n        layer = {}\n        for idx, hl_num in enumerate(self.hl_list):\n            tmp = self.W['WD_' + str(idx)].dot(last_input) + self.B['BD_' + str(idx)]\n            last_input = ptmath.sigmoid(tmp)\n            layer['OUT_' + str(idx)] = last_input\n        return layer['OUT_' + str(len(self.hl_list)-1)], layer\n\n    def backprop_dw(self, now_x, now_W, now_B, top_delta, top_W = None):\n        err_toss = top_delta\n        now_out = now_W.dot(now_x) + now_B\n        if top_W is not None:\n            err_toss = err_toss.dot(top_W)\n        now_delta = ptmath.sigmoid_delta(now_out) * err_toss\n        now_dW = np.array([now_delta]).transpose().dot(np.array([now_x]))\n        return now_delta, now_dW\n        \n    def fit(self, lr, epoch, err_th, batch_size):\n        err = 9999.0\n        npoch = 0\n        while err > err_th and npoch < epoch :\n            err = 0\n            npoch += 1\n            for idx, x in enumerate(self.mat_data):\n                out, layer = self.feedforward(x)\n                now_err = ((self.y[idx] - out) * (self.y[idx] - out)).sum(axis=0)\n                err += now_err\n                top_delta, top_W = (out - self.y[idx], None)\n                dW = {}\n                delta = {}\n                for layer_idx in range(len(layer))[::-1]:\n                    if layer_idx > 0:\n                        now_x = layer['OUT_' + str(layer_idx-1)]\n                    else:\n                        now_x = x\n                    now_W = self.W['WD_' + str(layer_idx)]\n                    now_B = self.B['BD_' + str(layer_idx)]\n                    delta['BD_' + str(layer_idx)], dW['WD_' + str(layer_idx)] = \\\n                      self.backprop_dw(now_x, now_W, now_B, top_delta, top_W)\n                    top_delta = delta['BD_'+str(layer_idx)]\n                    top_W = now_W\n\n                for layer_idx in range(len(layer))[::-1]:\n                    self.W['WD_'+ str(layer_idx)] -= lr * dW['WD_' + str(layer_idx)]\n                    self.B['BD_' + str(layer_idx)] -= lr * delta['BD_' + str(layer_idx)]\n\n    def predict(self, array_input):\n        array_input = convert.list2npfloat(array_input)\n        out, layer = self.feedforward(array_input)\n        return out\n"""
pytrain/NeuralNetwork/__init__.py,0,b'from FNN import *\n'
pytrain/SVM/SVC.py,4,"b""#\n# SVC (SVM Multi classifier)\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\n\nimport numpy as np\nfrom pytrain.SVM import SVM\nfrom pytrain.lib import convert\nfrom pytrain.lib import ptmath\n\nclass SVC:\n\n    def __init__(self, mat_data, label_data):\n        self.x = np.mat(convert.list2npfloat(mat_data))\n        self.ys = np.mat(np.sign(convert.list2npfloat(label_data) - 0.5))\n        self.outbit = self.ys.shape[1]\n        self.svm4bit = []\n        for i in range(self.outbit):\n            self.svm4bit.append(SVM(self.x, self.ys[:,i]))\n        \n    def fit(self, C, toler, epoch, kernel = 'Linear', kernel_params = {}):\n        for i in range(self.outbit):\n            self.svm4bit[i].fit(C, toler, epoch, kernel, kernel_params)\n        \n    def predict(self, array_input):\n        array_input = np.mat(convert.list2npfloat(array_input))\n        output = []\n        for i in range(self.outbit):\n            output.append(self.svm4bit[i].predict(array_input))\n        return list(np.sign(np.array(output) + 1))\n"""
pytrain/SVM/SVM.py,12,"b""#\n# SVM (Binary classifier)\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\n\nimport numpy as np\nfrom pytrain.lib import convert\nfrom pytrain.lib import ptmath\n\nclass SVM:\n\n    def __init__(self, mat_data, label_data):\n        self.x = mat_data\n        self.y = label_data\n        self.len_row = self.x.shape[0]\n        self.alphas = np.mat(np.zeros((self.len_row, 1)))\n        self.E_cache = np.mat(np.zeros((self.len_row, 2))) # [0] is dirty bit\n        self.b = 0\n\n    def trans_data_with_kernel(self, kernel, kernel_params = {}):\n        self.K = np.mat(np.zeros((self.len_row, self.len_row)))\n        for i in range(self.len_row):\n            self.K[:,i] = self.trans_array_with_kernel(self.x[i,:], kernel, kernel_params)\n            \n    def trans_array_with_kernel(self, array_input, kernel, kernel_params = {}):\n        K = np.mat(np.zeros((self.len_row, 1)))\n        if kernel == 'Linear':\n            K = self.x * array_input.T\n        elif kernel == 'Polynomial':\n            degree = kernel_params['degree']\n            K1 = self.x * array_input.T\n            K = K1\n            for d in range(degree):\n                if d >= 1:\n                    K = np.multiply(K, K1)\n        elif kernel == 'RBF':\n            gamma = kernel_params['gamma']\n            for j in range(self.len_row):\n                delta_row = self.x[j] - array_input\n                K[j] = delta_row * delta_row.T\n            K = np.exp(K / (-1 * gamma ** 2))\n        return K\n\n    def clip_alpha(self, a, H, L):\n        if a > H:\n            a = H\n        elif a < L:\n            a = L\n        return a\n\n    def select_j_random(self, i):\n        j = i\n        while (j == i):\n            j = int(np.random.uniform(0, self.len_row))\n        return j\n    \n    def select_j(self, i):\n        max_j = i\n        max_delta_E = -1\n        Ei = self.calc_E(i)\n        valid_cache_index = np.nonzero(self.E_cache[:,0].A)[0]\n        if len(valid_cache_index) > 0:\n            for idx in valid_cache_index:\n                if idx == i: continue\n                delta_E = abs(Ei - self.E_cache[idx,1])\n                if delta_E > max_delta_E:\n                    max_j = idx\n                    max_delta_E = delta_E\n        else:\n            max_j = self.select_j_random(i)\n        return max_j\n        \n    def calc_E(self, n):\n        Fn = float(np.multiply(self.alphas, self.y).T * self.K[:,n] + self.b)\n        En = Fn - float(self.y[n])\n        return En\n\n    def update_E_cache(self, n):\n        En = self.calc_E(n)\n        self.E_cache[n] = [1, En]\n\n    def smo_loop(self, i, C, toler):\n        Ei = self.calc_E(i)\n        if (self.y[i] * Ei < -toler and self.alphas[i] < C) or (self.y[i] * Ei > toler and self.alphas[i] > 0):\n            j = self.select_j(i)\n            Ej = self.calc_E(j)\n            if self.y[i] != self.y[j] :\n                L = max(0, self.alphas[j] - self.alphas[i])\n                H = min(C, C + self.alphas[j] - self.alphas[i])\n            else:\n                L = max(0, self.alphas[j] + self.alphas[i] - C)\n                H = min(C, self.alphas[j] + self.alphas[i])\n                \n            if L == H :\n                return 0\n            eta = 2.0 * self.K[i,j] - self.K[i,i] - self.K[j,j]\n            if eta >= 0: # eta cannot be positive\n                return 0\n            \n            old_alpha_j = self.alphas[j]\n            old_alpha_i = self.alphas[i]\n\n            delta_alpha_j = self.y[j] * (Ej - Ei) / eta\n            \n            self.alphas[j] += delta_alpha_j\n            self.alphas[j] = self.clip_alpha(self.alphas[j],H,L)\n            delta_alpha_j = self.alphas[j] - old_alpha_j\n            self.update_E_cache(j)           \n            \n            delta_alpha_i =  -1 * self.y[i] * self.y[j] * delta_alpha_j\n            self.alphas[i] += delta_alpha_i\n            self.update_E_cache(i)\n            \n            delta_b_i = Ei + delta_alpha_i * self.y[i] * self.K[i,i] + delta_alpha_j * self.y[j] * self.K[i,j]\n            delta_b_j = Ej + delta_alpha_i * self.y[i] * self.K[i,j] + delta_alpha_j * self.y[j] * self.K[j,j]\n            \n            if (self.alphas[i] > 0) and (self.alphas[i] < C):\n                self.b = self.b - delta_b_i\n            elif (self.alphas[j] > 0) and (self.alphas[j] < C):\n                self.b = self.b - delta_b_j\n            else:\n                self.b = self.b - (delta_b_i + delta_b_j) / 2\n            return 1\n        else:\n            return 0\n\n    def smo_plat(self, C, toler, epoch):\n        now_epoch = 0\n        go_full_over = True\n        alpha_changed = 0\n        while (now_epoch < epoch) and ((alpha_changed > 0) or go_full_over):\n            alpha_changed = 0\n            if go_full_over:\n                for i in range(self.len_row):\n                    alpha_changed += self.smo_loop(i, C, toler)\n            else :\n                non_bounded_idx = np.nonzero((self.alphas.A > 0) * (self.alphas.A < C))[0]\n                for i in non_bounded_idx:\n                    alpha_changed += self.smo_loop(i, C, toler)\n            if go_full_over :\n                go_full_over = False\n            elif alpha_changed == 0:\n                go_full_over = True\n            now_epoch += 1\n\n    def fit(self, C, toler, epoch, kernel = 'Linear', kernel_params = {}):\n        self.kernel = kernel\n        self.kernel_params = kernel_params\n        self.trans_data_with_kernel(kernel, kernel_params)\n        self.smo_plat(C, toler, epoch)\n\n    def predict(self, array_input):\n        kernel_input = self.trans_array_with_kernel(array_input, self.kernel, self.kernel_params)\n        Fn = float(np.multiply(self.alphas, self.y).T * kernel_input + self.b)\n        return np.sign(Fn)\n"""
pytrain/SVM/__init__.py,0,b'from SVM import *\nfrom SVC import *\n'
pytrain/lib/__init__.py,0,b'\nfrom convert import *\nfrom autotest import *\nfrom nlp import *\nfrom fs import *\nfrom normalize import *\nfrom ptmath import *\nfrom dataset import *\n'
pytrain/lib/autotest.py,2,"b'#\n# library for autotest processing module\n#\n# @ author becxer\n# @ e-mail becxer@gmail.com\n#\n\nimport operator\nimport math\nimport sys\nimport numpy as np\n\n# abstracted evaluation logic\n# p_module is pytrain module that you already trained\ndef eval_predict(p_module, mat_test, label_test, log_on = True, one_hot = False):\n    test_row_size = len(mat_test)\n    error_count = 0.0\n    for i in range(test_row_size):\n        res = eval_predict_one(p_module, mat_test[i], label_test[i], log_on, one_hot)\n        if not res : error_count += 1.0\n    if log_on: print ""<"" + p_module.__class__.__name__ + "">"" +\\\n        "" error rate is "" + str(error_count / float(test_row_size))\n    return error_count/test_row_size\n\ndef eval_predict_one(p_module, input_array_test, label_one_test, log_on = True, one_hot = False):\n    res = p_module.predict(input_array_test)\n    if one_hot :\n        one_hot_res = [0 for i in range(len(res))]\n        one_hot_res[np.argmax(res)] = 1\n        res = one_hot_res\n        one_hot_label_one_test = [0 for i in range(len(label_one_test))]\n        one_hot_label_one_test[np.argmax(label_one_test)] = 1\n        label_one_test = one_hot_label_one_test\n        \n    if log_on : print ""input : \'"" + str(input_array_test[:3]) + \\\n            ""\' --> predicted : \'"" + str(res) + ""\' --? origin : \'"" \\\n                    + str(label_one_test) + ""\'""\n    if list(str(res)) != list(str(label_one_test)) :\n        return False\n    else :\n        return True\n'"
pytrain/lib/convert.py,1,"b""#\n# library for convert data format\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\n\nimport numpy as np\n\ndef list2npfloat(list_data):\n    ldtype = type(list_data).__name__\n    if ldtype == 'str' or ldtype == 'long' or ldtype == 'int' or ldtype == 'int32' or\\\n      ldtype == 'int64' or ldtype == 'float' or ldtype == 'float32' or ldtype == 'float64':\n        return float(list_data)\n    elif ldtype == 'list':\n        return np.array(map(list2npfloat, list_data), dtype=np.float64)\n    else :\n        return list_data\n"""
pytrain/lib/dataset.py,2,"b'import os, struct\nfrom pytrain.lib import fs\nfrom array import array as pyarray\nfrom numpy import append, array, int8, uint8, zeros\nimport numpy as np\nimport urllib2 as ul2\nimport os\n\ndef load_mnist(path=""."", dataset=""training"", one_hot = False):\n    data_path = download_data(path, ""mnist"")\n    train_data = os.path.join(data_path, ""MNIST_train.small.csv"")\n    dmat_train, dlabel_train = fs.csv_loader(train_data, 0)\n    test_data = os.path.join(data_path, ""MNIST_test.small.csv"")\n    dmat_test, dlabel_test = fs.csv_loader(test_data, 0)\n    dmat_train = map(lambda row : map(float, row), dmat_train)\n    dmat_test = map(lambda row : map(float, row), dmat_test)    \n    if one_hot :\n        one_hot_label = map(list,list(np.eye(10)))\n        temp_dlabel_train = []\n        temp_dlabel_test = []\n        for l in dlabel_train:\n            temp_dlabel_train.append(one_hot_label[int(l)])\n        for l in dlabel_test:\n            temp_dlabel_test.append(one_hot_label[int(l)])\n        dlabel_train = temp_dlabel_train\n        dlabel_test = temp_dlabel_test\n    if dataset == ""training"":\n        return dmat_train, dlabel_train\n    elif dataset == ""testing"":\n        return dmat_test, dlabel_test\n    else:\n        raise ValueError(""dataset must be \'testing\' or \'training\'"")\n\ndef load_iris(path=""."", dataset=""training"", one_hot = False):\n    data_path = download_data(path, ""iris"")\n    sample_data = os.path.join(data_path, ""iris.csv"")    \n    dmat_train, dlabel_train, dmat_test, dlabel_test \\\n      = fs.csv_loader(sample_data, 0.2)\n    dmat_train = map(lambda row : map(float, row), dmat_train)\n    dmat_test = map(lambda row : map(float, row), dmat_test)    \n    if one_hot:\n        one_hot_label = map(list, list(np.eye(3)))\n        temp_dlabel_train = []\n        temp_dlabel_test = []\n        for l in dlabel_train:\n            temp_dlabel_train.append(one_hot_label[int(l)])\n        for l in dlabel_test:\n            temp_dlabel_test.append(one_hot_label[int(l)])\n        dlabel_train = temp_dlabel_train\n        dlabel_test = temp_dlabel_test\n    if dataset == ""training"":\n        return dmat_train, dlabel_train\n    elif dataset == ""testing"":\n        return dmat_test, dlabel_test\n    else:\n        raise ValueError(""dataset must be \'testing\' or \'training\'"")\n\ndef download_data(path=""."", dataset_name = """"):\n    base_path = path + ""/"" + dataset_name\n    if not os.path.exists(base_path):\n        print(""Creating "" + str(base_path) + "" directory"")\n        os.makedirs(base_path)    \n        print(""Downloading "" + str(dataset_name) + "" into "" + str(base_path))\n        if dataset_name == \'iris\':\n            ds = ul2.urlopen(""https://raw.githubusercontent.com/becxer/pytrain/master/sample_data/iris/iris.csv"")\n            with open(base_path + \'/\' + \'iris.csv\', ""wb"") as ds_file:\n                ds_file.write(ds.read())\n        elif dataset_name == \'mnist\':\n            ds = ul2.urlopen(""https://raw.githubusercontent.com/becxer/pytrain/master/sample_data/mnist/MNIST_train.small.csv"")\n            with open(base_path + \'/\' + \'MNIST_train.csv\', ""wb"") as ds_file:\n                ds_file.write(ds.read())\n            ds = ul2.urlopen(""https://raw.githubusercontent.com/becxer/pytrain/master/sample_data/mnist/MNIST_test.small.csv"")\n            with open(base_path + \'/\' + \'MNIST_test.csv\', ""wb"") as ds_file:\n                ds_file.write(ds.read())        \n        print(""Download "" + str(dataset_name) + "" complete"")\n    else:\n        print(""Dataset "" + str(base_path) + "" is already exist"")\n    return base_path\n'"
pytrain/lib/fs.py,0,"b""# library for file system processing\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\n\nfrom numpy import *\nimport operator\nimport math\nimport sys\nfrom pytrain.lib import nlp\n\ndef csv_loader(filename, ho_ratio):\n    return seperate_loader(filename, ho_ratio, ',')\n\ndef tsv_loader(filename, ho_ratio):\n    return seperate_loader(filename, ho_ratio, '\\t')\n\ndef csv_loader_with_nlp(filename, ho_ratio, nlp_lib):\n    return seperate_loader_with_nlp(filename, ho_ratio, nlp_lib, ',')\n\ndef tsv_loader_with_nlp(filename, ho_ratio, nlp_lib):\n    return seperate_loader_with_nlp(filename, ho_ratio, nlp_lib, '\\t')\n\n# convert file which format is\n# [label, feature1, feature2 ... , featureN]\n# to matrix_train, label_train, matrix_test, label_test\n# according to ho_ratio\n# ho_ratio is test_set ratio how you want\ndef seperate_loader(filename, ho_ratio, delimeter):\n    fr = open(filename)\n    lines = fr.readlines()\n    lines.pop(0)\n    mat_train = []\n    mat_test = [] \n    label_train = []\n    label_test = []\n    train_index = 0\n    test_index = 0\n    split_index = 0\n    if ho_ratio != 0 :\n        split_index = math.floor(1.0 / ho_ratio)\n    for line in lines:\n        line = line.strip()\n        list_from_line = line.split(delimeter)\n        if ho_ratio == 0 or (train_index + test_index) % split_index != 0 :\n            mat_train.append(list_from_line[1:])\n            label_train.append(list_from_line[0])\n            train_index += 1\n        else :\n            mat_test.append(list_from_line[1:])\n            label_test.append(list_from_line[0])\n            test_index += 1\n    if ho_ratio == 0:\n        return mat_train,label_train\n    else :\n        return mat_train, label_train, mat_test, label_test\n\ndef seperate_loader_with_nlp(filename, ho_ratio, nlp_lib, delimeter):\n    wmat = seperate_loader(filename, ho_ratio, delimeter)\n    wmat_train, label_train  = wmat[:2]\n\n    mat_train = []\n    mat_test = []\n    label_test = []\n    \n    vocabulary = nlp_lib.extract_vocabulary(wmat_train)\n    \n    for row in wmat_train:\n        mat_train.append(nlp_lib.bag_of_word2vector(vocabulary, row))\n\n    if len(wmat) > 2 and ho_ratio != 0:\n        wmat_test, label_test = wmat[2:4]\n        for row in wmat_test:\n            mat_test.append(nlp_lib.bag_of_word2vector(vocabulary, row))\n\n    if ho_ratio == 0:\n        return mat_train,label_train, vocabulary\n    else :\n        return mat_train, label_train, vocabulary, mat_test, label_test\n\n\n# saving module to file\ndef store_module(module, filename):\n    import pickle\n    module_f = open(filename, 'w')\n    pickle.dump(module,module_f)\n    module_f.close()\n\n\n# loading module into object\ndef restore_module(filename):\n    import pickle\n    module_f = open(filename)\n    return pickle.load(module_f)\n\n"""
pytrain/lib/nlp.py,0,"b'# library for nlp\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\nimport re\n\nimport os\nfull_path = os.path.realpath(__file__)\npath, filename = os.path.split(full_path)\n\nclass nlp:\n\n    lang = ""NOLANG""\n    stopwords = []\n    lower = False\n\n    def set_with_eng(self, lower = False):\n        global path\n        self.lower = lower\n        sw_f = open(path+""/nlp_stopwords.eng"")\n        self.stopwords = map(lambda x : x.strip(), sw_f.readlines())\n\n    def set_with_kor(self):\n        pass\n\n    def __init__(self, lang_ = ""NOLANG""):\n        self.lang = lang_\n        if self.lang == ""eng"":\n            self.set_with_eng()\n        if self.lang == ""eng_lower"":\n            self.set_with_eng(lower = True)\n        elif self.lang == ""kor"":\n            self.set_with_kor()\n\n    def switch_split2words_str(self, arg):\n        splitwords = re.compile(""([\\w][\\w]*\'?\\w?)"").findall(arg)\n        splitlowerwords = [ x.lower() for x in splitwords ]\n        res = []\n        for spword in splitlowerwords:\n            if spword not in self.stopwords:\n                res.append(spword)\n        return res\n\n    def switch_split2words_list(self, arg):\n        res = []\n        for item in arg:\n            if str(type(item).__name__) == \'str\':\n                res.append(self.split2words(item))\n        return res\n\n    def split2words(self, arg):\n        switch = {\\\n            \'str\':self.switch_split2words_str,\\\n            \'list\':self.switch_split2words_list\\\n        }\n        return switch[str(type(arg).__name__)](arg)\n\n    def split2sentence(self, text):\n        # Need to improve sentence split algorithm\n        return text.split(\'\\n\')\n\n    def extract_vocabulary(self, documents):\n        vocabulary = set([])\n        for doc in documents:\n            if str(type(doc).__name__) == \'str\':\n                doc = self.split2words(doc)\n            ndoc = []\n            for w in doc:\n                if w not in self.stopwords:\n                    if self.lower : w = w.lower()\n                    ndoc.append(w)\n            vocabulary = vocabulary | set(ndoc)\n        return list(vocabulary)\n\n    def set_of_word2vector(self, vocabulary, sentence):\n        voca_vector = [0] * len(vocabulary)\n        if str(type(sentence).__name__) == \'str\':\n            sentence = self.split2words(sentence)\n        for word in sentence:\n            if self.lower : word = word.lower()\n            if word in vocabulary:\n                voca_vector[vocabulary.index(word)] = 1\n        return voca_vector\n\n    def bag_of_word2vector(self, vocabulary, sentence):\n        voca_vector = [0] * len(vocabulary)\n        if str(type(sentence).__name__) == \'str\':\n            sentence = self.split2words(sentence)\n        for word in sentence:\n            if self.lower : word = word.lower()\n            if word in vocabulary:\n                voca_vector[vocabulary.index(word)] += 1\n        return voca_vector\n\n    def set_of_wordseq2matrix(self, vocabulary, wordlist):\n        word_mat = []\n        for word in wordlist:\n            word_vector = [0] * len(vocabulary)\n            if self.lower : word = word.lower()\n            if word in vocabulary:\n                word_vector[vocabulary.index(word)] = 1\n            word_mat.append(word_vector)\n        return word_mat\n\n'"
pytrain/lib/normalize.py,0,"b'#\n# library for normalize\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\n\nfrom numpy import *\nimport operator\nimport math\nimport sys\nfrom pytrain.lib import convert\n\n# normalize matrix feature with base-min & base-max\ndef quantile(data_mat):\n    data_mat = convert.list2npfloat(data_mat)\n    min_vals = data_mat.min(0)\n    max_vals = data_mat.max(0)\n    ranges = max_vals - min_vals\n    ranges = map(lambda x : x + sys.float_info.epsilon ,ranges)\n    normalized_data_mat = zeros(shape(data_mat))\n    rowsize = data_mat.shape[0]\n    normalized_data_mat = data_mat - tile(min_vals, (rowsize,1))\n    normalized_data_mat = normalized_data_mat / tile(ranges,(rowsize,1))\n    return normalized_data_mat\n'"
pytrain/lib/ptmath.py,0,"b""#\n# library for mathmatics using by pytrain\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n#\n\nfrom numpy import *\n\ndef euclidean (vx, vy):\n    return linalg.norm(vx - vy)\n\ndef manhattan (vx, vy):\n    return array(map(abs, vx - vy)).sum(axis = 0)\n\ndef cosine_similarity (vx, vy):\n    return dot(vx, vy) / (linalg.norm(vx) * linalg.norm(vy))\n\ndef sigmoid(k):\n    return 1.0 / ( 1.0 + exp(-k))\n\ndef sigmoid_delta(k):\n    return sigmoid(k) * (1.0 - sigmoid(k))\n\ndfunc_set = {\\\n                 'euclidean' : euclidean,\\\n                 'default' : euclidean,\\\n                 'manhattan' : manhattan,\\\n                 'cosine_similarity' : cosine_similarity\\\n            }\n\ndef distfunc(dfunc_keyword):\n    dfunc = dfunc_keyword\n    if type(dfunc).__name__ == 'str':\n        if dfunc in dfunc_set:\n            dfunc = dfunc_set[dfunc]\n    if type(dfunc).__name__ == 'function':\n            return dfunc\n    else :\n        return None\n    \n"""
test_pytrain/test_Apriori/__init__.py,0,b'from test_Apriori import *\n'
test_pytrain/test_Apriori/test_Apriori.py,0,"b'#\n# test Apriori \n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.Apriori import Apriori\n\nclass test_Apriori(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        data = [[1,3,4], [2,3,5],[1,2,3,5], [2,5]]\n\n        ap = Apriori(data)\n        ap.fit(min_support = 0.5 , min_confidence = 0.7)\n        \n        itemsets_list = ap.get_itemsets()\n        support_data = ap.get_support_data()\n        rules = ap.get_rules()\n\n        self.tlog(""itemsets list : "" + str(itemsets_list))\n        self.tlog(""support_data : "" + str(support_data))\n        self.tlog(""rules : "" + str(rules))\n        \n        rec = ap.recommend([2])\n        self.tlog(""recommend with 2 : "" + str(rec))\n        assert rec == frozenset([3,5])\n\nclass test_Apriori_mushroom(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        mushroom_file = open(""sample_data/mushroom/mushroom.tsv"")\n        data = map(lambda x : x.strip().split(), mushroom_file.read().split(""\\n""))[:-1]\n        ap = Apriori(data)\n        ap.fit(min_support = 0.9, min_confidence = 0.8)\n        itemsets_list = ap.get_itemsets()\n        support_data = ap.get_support_data()\n        rules = ap.get_rules()  \n\n        self.tlog(""itemsets list : "" + str(itemsets_list))\n        self.tlog(""support_data : "" + str(support_data))\n        self.tlog(""rules : "" + str(rules))\n'"
test_pytrain/test_CRF/__init__.py,0,b'from test_CRF import *\n'
test_pytrain/test_CRF/test_CRF.py,0,"b""#\n# test CRF\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n# \n\nfrom test_pytrain import test_Suite\nfrom pytrain.CRF import CRF\nfrom pytrain.lib import autotest\nfrom pytrain.lib import nlp\nfrom numpy import *\n\nclass test_CRF(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n\n        train_mat = [\\\n                     # Sequence of characters with no space\n                     ['<s>','I','a','m','a','b','o','y'],\\\n                     ['<s>','Y','o','u','a','r','e','a','g','i','r','l'],\\\n                     ['<s>','I','a','m','a','g','o','o','d','b','o','y'],\\\n                     ['<s>','Y','o','u','a','r','e','a','g','o','o','d','g','i','r','l'],\\\n                     ]\n\n        train_label = [\\\n                     # Sequence of label tagged to space\n                     # [1 == space, 0 == no-space]\n                     [0,1,0,1,1,0,0,1],\\\n                     [0,0,0,1,0,0,1,1,0,0,0,1],\\\n                     [0,1,0,1,1,0,0,0,1,0,0,1],\\\n                     [0,0,0,1,0,0,1,1,0,0,0,1,0,0,0,1],\\\n                    ]\n\n        nlp_common = nlp()\n        voca = nlp_common.extract_vocabulary(train_mat)\n        train_wordseq_mat = []\n        for wordseq in train_mat:\n            wordseq_mat = nlp_common.set_of_wordseq2matrix(voca, wordseq)\n            train_wordseq_mat.append(wordseq_mat)\n\n        crf = CRF(train_wordseq_mat, train_label, hidden_state_labeled = True, hidden_state = 2)\n        crf.fit(toler = 0.001, epoch= 30)\n        \n        ti1 = nlp_common.set_of_wordseq2matrix(voca,['<s>','I','a','m','g','o','o','d'])\n        r1 = autotest.eval_predict_one(crf, ti1, [0,1,0,1,0,0,0,1], self.logging)\n\n        ti2 = nlp_common.set_of_wordseq2matrix(voca,['<s>','Y','o','u','a','r','e','a','b','o','y'])\n        r2 = autotest.eval_predict_one(crf, ti2, [0,0,0,1,0,0,1,1,0,0,1], self.logging)\n\n        ti3 = nlp_common.set_of_wordseq2matrix(voca,['<s>','Y','o','u','a','r','e','g','i','r','l'])\n        r3 = autotest.eval_predict_one(crf, ti3, [0,0,0,1,0,0,1,0,0,0,1], self.logging)\n\n        ti4 = nlp_common.set_of_wordseq2matrix(voca,['<s>','I','a','m','g','i','r','l'])\n        r4 = autotest.eval_predict_one(crf, ti4, [0,1,0,1,0,0,0,1], self.logging)\n"""
test_pytrain/test_DBSCAN/__init__.py,0,b'from test_DBSCAN import *\n'
test_pytrain/test_DBSCAN/test_DBSCAN.py,0,"b'#\n# test DBSCAN\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.DBSCAN import DBSCAN\nfrom pytrain.lib import autotest\nfrom numpy import *\n\nclass test_DBSCAN(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        # below data shows 6 group of each 5 point data\n        sample_mat = [\\\n                          [0.2,0.3],[1.0,0.28],[1.98,0.7],\\\n                          [0.1,1.11],[1.0,1.12],\\\n                          [5.94,0.4],[6.73,0.38],[7.42,0.97],\\\n                          [6.74,1.23],[5.91,1.20],\\\n                          [2.0,4.8],[2.74,4.78],[3.6,5.1],\\\n                          [3.1,5.3],[1.95,5.8],\\\n                          [8.94,5.2],[9.6,5.12],[10.31,5.29],\\\n                          [8.73,6.0],[9.54,5.99],\\\n                          [5.17,9.1],[5.64,8.97],[6.56,9.39],\\\n                          [4.99,9.82],[5.5,9.74],\\\n                          [11.8,1.8],[12.04,1.74],[12.9,2.0],\\\n                          [11.74,2.4],[12.11,2.32]\n                      ]\n\n        # labeling cluster for each data\n        dbscan = DBSCAN(sample_mat, eps = 1.0, min_pts = 2, dist_func = ""euclidean"")\n        label_data =  dbscan.cluster() # dbscan.fit() also same\n\n        for idx, row in enumerate(sample_mat):\n            self.tlog(str(row) + "" -> "" + str(label_data[idx]))\n        self.tlog(""labels are "" + str(list(set(label_data))))\n\n        # clustering test with unknown data\n        r1 = autotest.eval_predict_one(dbscan, [11.70, 3.0], \\\n                            dbscan.predict([11.74, 2.4]), self.logging)\n        r2 = autotest.eval_predict_one(dbscan, [8.40, 5.8], \\\n                            dbscan.predict([8.73, 6.0]), self.logging)\n        r3 = autotest.eval_predict_one(dbscan, [0.7, 0.1], \\\n                            dbscan.predict([1.08, 0.7]), self.logging)\n\n        assert (r1 and r2 and r3)\n'"
test_pytrain/test_DecisionTreeID3/__init__.py,0,b'from test_DecisionTreeID3 import *\n'
test_pytrain/test_DecisionTreeID3/test_DecisionTreeID3.py,0,"b'#\n# test Decision Tree\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.DecisionTreeID3 import DecisionTreeID3\nfrom pytrain.lib import fs\nfrom pytrain.lib import autotest\n\nclass test_DecisionTreeID3(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        \n        sample_mat = [[\'sunny\',\'cloudy\',\'rain\'],[\'cloudy\',\'sunny\',\'rain\'],[\'cloudy\',\'cloudy\',\'rain\'],\\\n                [\'cloudy\',\'cloudy\',\'rain\'],[\'cloudy\',\'sunny\',\'sunny\'],[\'sunny\',\'sunny\',\'rain\'],\\\n                [\'sunny\',\'sunny\',\'sunny\'],[\'sunny\',\'cloudy\',\'sunny\'],[\'cloudy\',\'cloudy\',\'rain\']]\n        \n        sample_label = [\'rain\',  \'rain\',  \'rain\',\\\n                \'sunny\',  \'sunny\',  \'rain\',\\\n                \'sunny\',  \'sunny\', \'sunny\']\n                \n        tree = DecisionTreeID3(sample_mat, sample_label)\n        tree_structure = tree.build()\n        self.tlog(""Tree structure : "" + str(tree_structure))\n        \n        r1 = autotest.eval_predict_one(tree, [\'cloudy\',\'cloudy\',\'rain\'] , \'sunny\', self.logging)\n        assert r1 == True \n\nclass test_DecisionTreeID3_lense(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        lense_mat_train, lense_label_train, lense_mat_test, lense_label_test=\\\n          fs.tsv_loader(""sample_data/lense/lense.tsv"", 0.3)\n        dtree_lense = DecisionTreeID3(lense_mat_train,lense_label_train)\n        tree_structure = dtree_lense.build()\n        self.tlog(""Tree structure : "" + str(tree_structure))\n        error_rate = autotest.eval_predict(dtree_lense, lense_mat_test, lense_label_test, self.logging)\n        self.tlog(""lense predict (with decision tree) error rate : "" +str(error_rate))\n        \n'"
test_pytrain/test_GaussianNaiveBayes/__init__.py,0,b'from test_GaussianNaiveBayes import *\n'
test_pytrain/test_GaussianNaiveBayes/test_GaussianNaiveBayes.py,0,"b'#\n# test Gaussian Naive Bayes\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.GaussianNaiveBayes import GaussianNaiveBayes\nfrom pytrain.lib import nlp\nfrom pytrain.lib import fs\nfrom pytrain.lib import autotest\n\nimport json\nfrom numpy import *\n\nclass test_GaussianNaiveBayes(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        train_mat = [\\\n                [-65,-55,-42],[-20,-59,-71],[-43,-49,-69],\\\n                [-61,-30,-74],[-79,-81,-40],[-71,-57,-24],\\\n                [-67,-19,-58],[-57,-73,-83],[-68,-74,-59],\\\n                [-80,-85,-79]\n            ]\n        train_label = [\'B\',\'A\',\'A\',\'A\',\'B\',\'B\',\'A\',\'C\',\'C\',\'C\']\n        \n        test_mat = [\\\n                [-45,-47,-74],[-77,-69,-25],[-64,-71,-59],\\\n                [-85,-85,-25],[-85,-85,-85]\n            ]\n        test_label = [\'A\',\'B\',\'C\',\'B\',\'C\']\n\n        gnb = GaussianNaiveBayes(train_mat,train_label)\n        gnb.fit()\n        error_rate = autotest.eval_predict(gnb, test_mat, test_label, self.logging)\n        self.tlog(""strength of signal predict error rate : "" + str(error_rate))\n\nclass test_GaussianNaiveBayes_rssi(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        MAJOR_AP_COUNT = 17\n        BAD_SIGNAL = -100\n        \n        areaf = open(""sample_data/wrm/wrm.json.dat"")\n        area_json_list = areaf.readlines()\n        areaf.close()\n        area_set = {}\n        ap_set = {}\n\n        def compare(x,y):\n            if x[\'rssi\'] < y[\'rssi\']:\n                return 1\n            elif x[\'rssi\'] == y[\'rssi\']:\n                return 0\n            else:\n                return -1\n\n        for aobj in area_json_list:\n            area = json.loads(aobj)\n            label = area[""areaID""]\n            aplist = area[""apList""]\n            aplist.sort(compare)\n            for ap in aplist[:MAJOR_AP_COUNT]:\n                ap_set[ap[\'bssid\']] = 1\n            area_set[label] = area_set.get(label,[])\n            area_set[label].append(aplist[:MAJOR_AP_COUNT])\n        \n        ap_vector_column = ap_set.keys()\n\n        train_mat = []\n        train_label = []\n\n        test_mat = []\n        test_label = []\n\n        count = 0;\n        for label in area_set:\n            for aps in area_set[label]:\n                ap_vector = tile(BAD_SIGNAL, len(ap_vector_column))\n                for ap in aps:\n                    ap_vector[ap_vector_column.index(ap[\'bssid\'])] = ap[\'rssi\']\n                \n                count += 1\n                if count % 10 == 0:\n                    test_label.append(label)\n                    test_mat.append(ap_vector)\n                else :\n                    train_label.append(label)\n                    train_mat.append(ap_vector)\n\n        gnb = GaussianNaiveBayes(train_mat,train_label)\n        gnb.fit()\n        error_rate = autotest.eval_predict(gnb, test_mat, test_label, self.logging)\n        self.tlog(""rssi predict (with GaussianNaiveBayes) error rate : "" + str(error_rate))\n\n'"
test_pytrain/test_HMM/__init__.py,0,b'from test_HMM import *\n'
test_pytrain/test_HMM/test_HMM.py,0,"b""#\n# test HMM\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n# \n\nfrom test_pytrain import test_Suite\nfrom pytrain.HMM import HMM\nfrom pytrain.lib import autotest\nfrom pytrain.lib import nlp\nfrom numpy import *\n\nclass test_HMM(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n\n        train_mat = [\\\n                     # Sequence of characters with no space\n                     ['<s>','I','a','m','a','b','o','y'],\\\n                     ['<s>','Y','o','u','a','r','e','a','g','i','r','l'],\\\n                     ['<s>','I','a','m','a','g','o','o','d','b','o','y'],\\\n                     ['<s>','Y','o','u','a','r','e','a','g','o','o','d','g','i','r','l'],\\\n                     ]\n\n        train_label = [\\\n                     # Sequence of label tagged to space\n                     # [1 == space, 0 == no-space]\n                     [0,1,0,1,1,0,0,1],\\\n                     [0,0,0,1,0,0,1,1,0,0,0,1],\\\n                     [0,1,0,1,1,0,0,0,1,0,0,1],\\\n                     [0,0,0,1,0,0,1,1,0,0,0,1,0,0,0,1],\\\n                    ]\n\n        nlp_common = nlp()\n        voca = nlp_common.extract_vocabulary(train_mat)\n        train_wordseq_mat = []\n        for wordseq in train_mat:\n            wordseq_mat = nlp_common.set_of_wordseq2matrix(voca, wordseq)\n            train_wordseq_mat.append(wordseq_mat)\n\n        hmm = HMM(train_wordseq_mat, train_label, hidden_state_labeled = True, hidden_state = 2)\n        hmm.fit(toler = 0.001, epoch= 30)\n        \n        ti1 = nlp_common.set_of_wordseq2matrix(voca,['<s>','I','a','m','g','o','o','d'])\n        r1 = autotest.eval_predict_one(hmm, ti1, [0,1,0,1,0,0,0,1], self.logging)\n\n        ti2 = nlp_common.set_of_wordseq2matrix(voca,['<s>','Y','o','u','a','r','e','a','b','o','y'])\n        r2 = autotest.eval_predict_one(hmm, ti2, [0,0,0,1,0,0,1,1,0,0,1], self.logging)\n\n        ti3 = nlp_common.set_of_wordseq2matrix(voca,['<s>','Y','o','u','a','r','e','g','i','r','l'])\n        r3 = autotest.eval_predict_one(hmm, ti3, [0,0,0,1,0,0,1,0,0,0,1], self.logging)\n\n        ti4 = nlp_common.set_of_wordseq2matrix(voca,['<s>','I','a','m','g','i','r','l'])\n        r4 = autotest.eval_predict_one(hmm, ti4, [0,1,0,1,0,0,0,1], self.logging)\n\nclass test_HMM_BaumWelch(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n\n        train_mat = [\\\n                     # Sequence of characters with no space\n                     ['<s>','I','a','m','a','b','o','y'],\\\n                     ['<s>','Y','o','u','a','r','e','a','g','i','r','l'],\\\n                     ['<s>','I','a','m','a','g','o','o','d','b','o','y'],\\\n                     ['<s>','Y','o','u','a','r','e','a','g','o','o','d','g','i','r','l'],\\\n                     ]\n                     \n        nlp_common = nlp()\n        voca = nlp_common.extract_vocabulary(train_mat)\n        train_wordseq_mat = []\n        for wordseq in train_mat:\n            wordseq_mat = nlp_common.set_of_wordseq2matrix(voca, wordseq)\n            train_wordseq_mat.append(wordseq_mat)\n\n        hmm = HMM(train_wordseq_mat, label_data =  None, hidden_state_labeled = False, hidden_state = 2)\n        hmm.fit(toler = 0.001, epoch = 30)\n        \n        ti1 = nlp_common.set_of_wordseq2matrix(voca,['<s>','I','a','m','g','o','o','d'])\n        r1 = autotest.eval_predict_one(hmm, ti1, [0,1,0,1,0,0,0,1], self.logging)\n        \n        ti2 = nlp_common.set_of_wordseq2matrix(voca,['<s>','Y','o','u','a','r','e','a','b','o','y'])\n        r2 = autotest.eval_predict_one(hmm, ti2, [0,0,0,1,0,0,1,1,0,0,1], self.logging)\n\n        ti3 = nlp_common.set_of_wordseq2matrix(voca,['<s>','Y','o','u','a','r','e','g','i','r','l'])\n        r3 = autotest.eval_predict_one(hmm, ti3, [0,0,0,1,0,0,1,0,0,0,1], self.logging)\n\n        ti4 = nlp_common.set_of_wordseq2matrix(voca,['<s>','I','a','m','g','i','r','l'])\n        r4 = autotest.eval_predict_one(hmm, ti4, [0,1,0,1,0,0,0,1], self.logging)\n"""
test_pytrain/test_HierarchicalClustering/__init__.py,0,b'from test_HierarchicalClustering import *\n'
test_pytrain/test_HierarchicalClustering/test_HierarchicalClustering.py,0,"b'#\n# test HierarchicalClustering\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.HierarchicalClustering import HierarchicalClustering\nfrom pytrain.lib import autotest\nfrom numpy import *\n\nclass test_HierarchicalClustering(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        # below data shows 6 group of each 5 point data\n        sample_mat = [\\\n                          [0.2,0.3],[1.0,0.28],[1.98,0.7],\\\n                          [0.1,1.11],[1.0,1.12],\\\n                          [5.94,0.4],[6.73,0.38],[7.42,0.97],\\\n                          [6.74,1.23],[5.91,1.20],\\\n                          [2.0,4.8],[2.74,4.78],[3.6,5.1],\\\n                          [3.1,5.3],[1.95,5.8],\\\n                          [8.94,5.2],[9.6,5.12],[10.31,5.29],\\\n                          [8.73,6.0],[9.54,5.99],\\\n                          [5.17,9.1],[5.64,8.97],[6.56,9.39],\\\n                          [4.99,9.82],[5.5,9.74],\\\n                          [11.8,1.8],[12.04,1.74],[12.9,2.0],\\\n                          [11.74,2.4],[12.11,2.32]\n                      ]\n\n        # labeling cluster for each data\n        hac = HierarchicalClustering(sample_mat, K = 6, dist_func = ""euclidean"")\n        label_data =  hac.cluster() # hac.fit() also same\n\n        for idx, row in enumerate(sample_mat):\n             self.tlog(str(row) + "" -> "" + str(label_data[idx]))\n        self.tlog(""labels are "" + str(list(set(label_data))))\n\n        # clustering test with unknown data\n        r1 = autotest.eval_predict_one(hac, [11.70, 3.0], \\\n                            hac.predict([11.74, 2.4]), self.logging)\n        r2 = autotest.eval_predict_one(hac, [8.40, 5.8], \\\n                            hac.predict([8.73, 6.0]), self.logging)\n        r3 = autotest.eval_predict_one(hac, [0.7, 0.1], \\\n                            hac.predict([1.08, 0.7]), self.logging)\n\n        assert (r1 and r2 and r3)\n'"
test_pytrain/test_KNN/__init__.py,0,b'from test_KNN import *\n'
test_pytrain/test_KNN/test_KNN.py,0,"b'#\n# test KNN\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.KNN import KNN\nfrom pytrain.lib import autotest\nfrom pytrain.lib import dataset\nimport numpy as np\n\nclass test_KNN_iris(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        iris_mat_train, iris_label_train = dataset.load_iris(""sample_data"", ""training"")\n        iris_mat_test, iris_label_test = dataset.load_iris(""sample_data"", ""testing"")\n        \n        knn = KNN(iris_mat_train, iris_label_train, 3, \'manhattan\')\n        error_rate = autotest.eval_predict(knn, iris_mat_test, iris_label_test, self.logging)\n        self.tlog(""iris predict (with basic knn) error rate :"" + str(error_rate))        \n\n\nclass test_KNN_mnist(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        dg_mat_train, dg_label_train = dataset.load_mnist(""sample_data"", ""training"") \n        dg_mat_test, dg_label_test = dataset.load_mnist(""sample_data"", ""testing"")\n\n        knn_digit = KNN(dg_mat_train, dg_label_train, 10, \'euclidean\')\n        error_rate = autotest.eval_predict(knn_digit, dg_mat_test, dg_label_test, self.logging)\n        self.tlog(""digit predict (with basic knn) error rate :"" + str(error_rate))\n\n'"
test_pytrain/test_Kmeans/__init__.py,0,b'from test_Kmeans import *\n'
test_pytrain/test_Kmeans/test_Kmeans.py,0,"b'#\n# test Kmeans\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.Kmeans import Kmeans\nfrom pytrain.lib import autotest\nfrom numpy import *\n\nclass test_Kmeans(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        # Above data shows 6 group of each 5 point data\n        sample_mat = [\\\n                          [0.2,0.3],[1.0,0.28],[1.98,0.7],\\\n                          [0.1,1.11],[1.0,1.12],\\\n                          [5.94,0.4],[6.73,0.38],[7.42,0.97],\\\n                          [6.74,1.23],[5.91,1.20],\\\n                          [2.0,4.8],[2.74,4.78],[3.6,5.1],\\\n                          [3.1,5.3],[1.95,5.8],\\\n                          [8.94,5.2],[9.6,5.12],[10.31,5.29],\\\n                          [8.73,6.0],[9.54,5.99],\\\n                          [5.17,9.1],[5.64,8.97],[6.56,9.39],\\\n                          [4.99,9.82],[5.5,9.74],\\\n                          [11.8,1.8],[12.04,1.74],[12.9,2.0],\\\n                          [11.74,2.4],[12.11,2.32]\n                      ]\n            \n        kmeans = Kmeans(sample_mat, dist_func = \'euclidean\')\n\n        # finding cluster (Fixed K)\n        cluster_point_fixed = \\\n                kmeans.cluster(K = 6, epoch = 30)\n        self.tlog(""fixed point count : "" + str(len(cluster_point_fixed)))\n        self.tlog(""cluster point : \\n"" + str(cluster_point_fixed))\n\n        # Auto finding good cluster (Flexible K)\n        cluster_point_flexible = \\\n          kmeans.fit(max_K = 7, random_try_count = 10, epoch = 30)\n        self.tlog(""flexible point count : "" + str(len(cluster_point_flexible)))\n        self.tlog(""cluster point : \\n"" + str(cluster_point_flexible))\n\n        # clustering test with unknown data\n        r1 = autotest.eval_predict_one(kmeans, [11.70, 3.0], \\\n                                        kmeans.predict([11.74, 2.4]), self.logging)\n        r2 = autotest.eval_predict_one(kmeans, [8.40, 5.8], \\\n                                        kmeans.predict([8.73, 6.0]), self.logging)\n        r3 = autotest.eval_predict_one(kmeans, [0.7, 0.1], \\\n                                        kmeans.predict([1.08, 0.7]), self.logging)\n                                        \n        assert (r1 and r2 and r3)\n'"
test_pytrain/test_LinearRegression/__init__.py,0,b'from test_LinearRegression import *\n'
test_pytrain/test_LinearRegression/test_LinearRegression.py,0,"b'#\n# test Linear Regression\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.LinearRegression import LinearRegression\nfrom pytrain.lib import dataset\nfrom pytrain.lib import autotest\n\nclass test_LinearRegression(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n\n        train_mat = [\\\n                     [0.12, 0.25],\\\n                     [3.24, 4.33],\\\n                     [0.14, 0.45],\\\n                     [7.30, 4.23],\\\n                     ]\n        train_label = [[0,1],[1,0],[0,1],[1,0]]\n        \n        linear_reg =\\\n            LinearRegression(train_mat, train_label)\n        linear_reg.fit(lr = 0.001, epoch = 1000, batch_size = 4)\n        \n        r1 = autotest.eval_predict_one(linear_reg,[0.10,0.33],[0, 1],self.logging, one_hot = True)\n        r2 = autotest.eval_predict_one(linear_reg,[4.40,4.37],[1, 0],self.logging, one_hot = True)\n        \nclass test_LinearRegression_iris(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        iris_mat_train, iris_label_train = dataset.load_iris(""sample_data"", ""training"", one_hot=True)\n        iris_mat_test, iris_label_test = dataset.load_iris(""sample_data"", ""testing"", one_hot=True)\n\n        linear_reg = LinearRegression(iris_mat_train, iris_label_train)\n        linear_reg.fit(lr = 0.0001, epoch = 1000, batch_size = 20)\n        error_rate = autotest.eval_predict(linear_reg, iris_mat_test, iris_label_test, self.logging, one_hot=True)\n        self.tlog(""iris predict (with linear regression) error rate :"" + str(error_rate))\n\nclass test_LinearRegression_mnist(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        dg_mat_train, dg_label_train = dataset.load_mnist(""sample_data"", ""training"", one_hot=True) \n        dg_mat_test, dg_label_test = dataset.load_mnist(""sample_data"", ""testing"", one_hot=True)\n\n        linear_reg = LinearRegression(dg_mat_train, dg_label_train)\n        linear_reg.fit(lr = 0.0000001, epoch = 1000, batch_size = 100)\n        error_rate = autotest.eval_predict(linear_reg, dg_mat_test, dg_label_test, self.logging, one_hot=True)\n        self.tlog(""digit predict (with linear regression) error rate :"" + str(error_rate))\n'"
test_pytrain/test_LogisticRegression/__init__.py,0,b'from test_LogisticRegression import *\n'
test_pytrain/test_LogisticRegression/test_LogisticRegression.py,0,"b'#\n# test Logistic Regression\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.LogisticRegression import LogisticRegression\nfrom pytrain.lib import fs\nfrom pytrain.lib import autotest\nfrom pytrain.lib import dataset\n\nclass test_LogisticRegression(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n\n        train_mat = [\\\n                     [0.12, 0.25],\\\n                     [3.24, 4.33],\\\n                     [0.14, 0.45],\\\n                     [7.30, 4.23],\\\n                     ]\n        train_label = [[0,1],[1,0],[0,1],[1,0]] # out bit is 1\n        \n        logistic_reg =\\\n            LogisticRegression(train_mat, train_label)\n        logistic_reg.fit(lr = 0.001, epoch = 2000, batch_size = 4)\n        \n        r1 = autotest.eval_predict_one(logistic_reg,[0.10,0.33],[0, 1],self.logging, one_hot=True)\n        r2 = autotest.eval_predict_one(logistic_reg,[4.40,4.37],[1, 0],self.logging, one_hot=True)\n        \nclass test_LogisticRegression_iris(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        iris_mat_train, iris_label_train = dataset.load_iris(""sample_data"", ""training"", one_hot=True)\n        iris_mat_test, iris_label_test = dataset.load_iris(""sample_data"", ""testing"", one_hot=True)\n\n        logistic_reg = LogisticRegression(iris_mat_train, iris_label_train)\n        logistic_reg.fit(lr = 0.001, epoch = 2000, batch_size = 30)\n        error_rate = autotest.eval_predict(logistic_reg, iris_mat_test, iris_label_test, self.logging, one_hot=True)\n        self.tlog(""iris predict (with logistic  regression) error rate :"" + str(error_rate))\n\nclass test_LogisticRegression_mnist(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        dg_mat_train, dg_label_train = dataset.load_mnist(""sample_data"", ""training"", one_hot=True) \n        dg_mat_test, dg_label_test = dataset.load_mnist(""sample_data"", ""testing"", one_hot=True)\n\n        logistic_reg = LogisticRegression(dg_mat_train, dg_label_train)\n        logistic_reg.fit(lr = 0.0001, epoch = 1000, batch_size = 100)\n        error_rate = autotest.eval_predict(logistic_reg, dg_mat_test, dg_label_test, self.logging, one_hot=True)\n        self.tlog(""digit predict (with logistic regression) error rate :"" + str(error_rate))\n'"
test_pytrain/test_NaiveBayes/__init__.py,0,b'from test_NaiveBayes import *\n'
test_pytrain/test_NaiveBayes/test_NaiveBayes.py,0,"b'#\n# test Naive Bayes\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.NaiveBayes import NaiveBayes\nfrom pytrain.lib import nlp\nfrom pytrain.lib import fs\nfrom pytrain.lib import autotest\n\nclass test_NaiveBayes(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        sample_docs = [\\\n                ""hello this is virus mail"",\\\n                ""hi this is from friend"",\\\n                ""how about buy this virus"",\\\n                ""facebook friend contact to you"",\\\n                ""I love you baby virus"",\\\n                ""what a nice day how about you""\\\n            ]\n\n        docs_label =\\\n                [\'spam\',\'real\',\'spam\',\'real\',\'spam\',\'real\']\n\n        nlp_eng = nlp(""eng"")\n\n        # extract vocabulary from docs\n        voca = nlp_eng.extract_vocabulary(sample_docs)\n        self.tlog(voca)\n        assert len(voca) == 12\n       \n        # convert docs to bag of word vector using vocabulary\n        docs_vector = []\n        for doc in sample_docs:\n            docs_vector.append(nlp_eng.bag_of_word2vector(voca, doc))\n        self.tlog(docs_vector)\n\n        # training NaiveBayes\n        nbayes = NaiveBayes(docs_vector, docs_label)\n        nbayes.fit()\n\n        # test case 1\n        tc1 = ""this is virus mail""\n        tc1_vec = nlp_eng.bag_of_word2vector(voca, tc1)\n        \n        self.tlog(tc1)\n        self.tlog(tc1_vec)\n        \n        r1 = autotest.eval_predict_one(nbayes,tc1_vec,\'spam\',self.logging)\n        assert r1 == True\n\n        # test case 2\n        tc2 = ""I love you love""\n        tc2_vec = nlp_eng.bag_of_word2vector(voca, tc2)\n        \n        self.tlog(tc2)\n        self.tlog(tc2_vec)\n\n        r2 = autotest.eval_predict_one(nbayes,tc2_vec,\'spam\',self.logging)\n        assert r2 == True\n\n\nclass test_NaiveBayes_email(test_Suite):\n\n    def __init__(self, logging =  True):\n        test_Suite.__init__(self,logging)\n\n    def test_process(self):\n\n        nlp_eng = nlp(""eng_lower"")\n\n        email_data_file = ""sample_data/email/email.tsv""\n        emailmat_train, emaillabel_train, voca, emailmat_test, emaillabel_test \\\n                = fs.tsv_loader_with_nlp(email_data_file, 0.4, nlp_eng)\n        self.tlog(voca)\n        \n        email_nbayes = NaiveBayes(emailmat_train, emaillabel_train)\n        email_nbayes.fit()\n\n        error_rate = autotest.eval_predict(email_nbayes, emailmat_test, emaillabel_test, self.logging)\n        self.tlog(""spam-mail predict (with NaiveBayes) error rate : "" +str(error_rate))\n\n        assert error_rate <= 0.1\n\n\n'"
test_pytrain/test_NeuralNetwork/__init__.py,0,b'from test_FNN import *\n'
test_pytrain/test_NeuralNetwork/test_FNN.py,0,"b'#\n# test Feedforward Neural Network\n#\n# @ author becxer\n# @ e-mail becxer87@gmail.com\n# \n\nfrom test_pytrain import test_Suite\nfrom pytrain.NeuralNetwork import FNN\nfrom pytrain.lib import autotest\nfrom pytrain.lib import dataset\nfrom numpy import *\n\nclass test_FNN(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n\n        train_mat = [\\\n                     [0.12, 0.25],\\\n                     [3.24, 4.33],\\\n                     [0.14, 0.45],\\\n                     [7.30, 4.23],\\\n                     ]\n        train_label = [[0,1],[1,0],[0,1],[1,0]] # out bit is 1\n        \n        fnn = FNN(train_mat, train_label, [3])\n        fnn.fit(lr = 0.01, epoch = 2000, err_th = 0.001, batch_size = 4)\n        \n        r1 = autotest.eval_predict_one(fnn,[0.10,0.33],[0, 1],self.logging, one_hot=True)\n        r2 = autotest.eval_predict_one(fnn,[4.40,4.37],[1, 0],self.logging, one_hot=True)\n        \nclass test_FNN_iris(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        iris_mat_train, iris_label_train = dataset.load_iris(""sample_data"", ""training"", one_hot=True)\n        iris_mat_test, iris_label_test = dataset.load_iris(""sample_data"", ""testing"", one_hot=True)\n\n        fnn = FNN(iris_mat_train, iris_label_train, [2])\n        fnn.fit(lr = 0.001, epoch = 4000, err_th = 0.00001, batch_size = 30)\n        error_rate = autotest.eval_predict(fnn, iris_mat_test, iris_label_test, self.logging, one_hot=True)\n        self.tlog(""iris predict (with fnn) error rate :"" + str(error_rate))\n\nclass test_FNN_mnist(test_Suite):\n    \n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        dg_mat_train, dg_label_train = dataset.load_mnist(""sample_data"", ""training"", one_hot=True) \n        dg_mat_test, dg_label_test = dataset.load_mnist(""sample_data"", ""testing"", one_hot=True)\n\n        fnn = FNN(dg_mat_train, dg_label_train, [400, 100])\n        fnn.fit(lr = 0.01, epoch = 1000, err_th = 0.00001, batch_size = 100)\n        error_rate = autotest.eval_predict(fnn, dg_mat_test, dg_label_test, self.logging, one_hot=True)\n        self.tlog(""digit predict (with fnn) error rate :"" + str(error_rate))\n'"
test_pytrain/test_SVM/__init__.py,0,b'from test_SVM import *\nfrom test_SVC import *\n'
test_pytrain/test_SVM/test_SVC.py,0,"b'#\n# test SVC\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.SVM import SVC\nfrom pytrain.lib import dataset\nfrom pytrain.lib import autotest\n\nclass test_SVC(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        train_mat = [\\\n                     [0.12, 0.25],\\\n                     [3.24, 4.33],\\\n                     [0.14, 0.45],\\\n                     [7.30, 4.23],\\\n                     ]\n        train_label = [[0,1], [1,0], [0,1], [1,0]] # out bit is 2\n\n        svc = SVC(train_mat, train_label)\n        svc.fit(C = 5.0, toler = 0.001, epoch = 50)\n        \n        r1 = autotest.eval_predict_one(svc,[0.10,0.33], [0., 1.], self.logging)\n        r2 = autotest.eval_predict_one(svc,[4.40,4.37], [1., 0.], self.logging)\n\n        assert r1\n        assert r2\n               \nclass test_SVC_iris(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        iris_mat_train, iris_label_train = dataset.load_iris(""sample_data"", ""training"", one_hot=True)\n        iris_mat_test, iris_label_test = dataset.load_iris(""sample_data"", ""testing"", one_hot=True)\n\n        svc = SVC(iris_mat_train, iris_label_train)\n        svc.fit(C = 1.5, toler = 0.0001, epoch = 1000, kernel = ""Polynomial"", kernel_params = {""degree"" : 3})\n        error_rate = autotest.eval_predict(svc, iris_mat_test, iris_label_test, self.logging, one_hot=True)\n        self.tlog(""iris predict (with svc) error rate :"" + str(error_rate))\n\nclass test_SVC_mnist(test_Suite):\n    \n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        dg_mat_train, dg_label_train = dataset.load_mnist(""sample_data"", ""training"", one_hot=True) \n        dg_mat_test, dg_label_test = dataset.load_mnist(""sample_data"", ""testing"", one_hot=True)\n\n        svc = SVC(dg_mat_train, dg_label_train)\n        svc.fit(C = 1.5, toler = 0.0001, epoch = 1000, kernel = ""RBF"" , kernel_params = {""gamma"" : 0.7})       \n        error_rate = autotest.eval_predict(svc, dg_mat_test, dg_label_test, self.logging, one_hot=True)\n        self.tlog(""digit predict (with svc) error rate :"" + str(error_rate))\n'"
test_pytrain/test_SVM/test_SVM.py,4,"b'#\n# test SVM\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.SVM import SVM\nfrom pytrain.lib import autotest\nimport numpy as np\n\nclass test_SVM(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        train_mat = np.mat([\\\n                     [0.12, 0.25],\\\n                     [3.24, 4.33],\\\n                     [0.14, 0.45],\\\n                     [7.30, 4.23],\\\n                     ])\n        train_label = np.mat([[-1.0], [1.0], [-1.0], [1.0]]) # out bit is 1\n\n        svm = SVM(train_mat, train_label)\n        svm.fit(C = 5.0, toler = 0.001, epoch = 50)\n        \n        r1 = autotest.eval_predict_one(svm,np.mat([0.10,0.33]), -1.0, self.logging)\n        r2 = autotest.eval_predict_one(svm,np.mat([4.40,4.37]), 1.0, self.logging)\n\n        assert r1\n        assert r2\n        \n'"
test_pytrain/test_lib/__init__.py,0,b'from test_fs import *\nfrom test_normalize import *\nfrom test_autotest import *\nfrom test_nlp import *\nfrom test_dataset import *\n'
test_pytrain/test_lib/test_autotest.py,0,"b'#\n# test lib.autotest\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.KNN import KNN\nfrom pytrain.lib import autotest\n\nclass test_autotest(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_process(self):\n        normed_dmat_train = self.get_global_value(\'normed_iris_mat_train\')\n        normed_dmat_test = self.get_global_value(\'normed_iris_mat_test\')\n        dlabel_train = self.get_global_value(\'iris_label_train\')\n        dlabel_test = self.get_global_value(\'iris_label_test\')\n\n        knn_date = KNN(normed_dmat_train, dlabel_train, 3, \'euclidean\')\n        error_rate = autotest.eval_predict(knn_date, normed_dmat_test, dlabel_test, self.logging)\n        self.tlog(""date predict (with basic knn) error rate : "" + str(error_rate))\n'"
test_pytrain/test_lib/test_dataset.py,0,"b'#\n# test lib.normalize\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.lib import dataset\n\nclass test_dataset(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_load_iris(self):\n        iris_mat_train, iris_label_train = dataset.load_iris(""sample_data"", ""training"")\n        iris_mat_test, iris_label_test = dataset.load_iris(""sample_data"", ""testing"")\n        self.tlog(""iris train data size : "" + str(len(iris_mat_train)))\n        self.tlog(""iris test data size : "" + str(len(iris_mat_test)))\n\n    def test_load_iris_one_hot(self):\n        iris_mat_train, iris_label_train = dataset.load_iris(""sample_data"", ""training"", one_hot = True)\n        iris_mat_test, iris_label_test = dataset.load_iris(""sample_data"", ""testing"", one_hot = True)\n        self.tlog(""iris train data size : "" + str(len(iris_mat_train)))\n        self.tlog(""iris test data size : "" + str(len(iris_mat_test)))\n        \n    def test_load_mnist(self):\n        mnist_mat_train, mnist_label_train \\\n          = dataset.load_mnist(""sample_data"", ""training"")\n        mnist_mat_test, mnist_label_test \\\n          = dataset.load_mnist(""sample_data"", ""testing"")\n        self.tlog(""mnist train data size : "" + str(len(mnist_mat_train)))\n        self.tlog(""mnist test data size : "" + str(len(mnist_mat_test)))\n\n    def test_load_mnist_one_hot(self):\n        mnist_mat_train, mnist_label_train \\\n          = dataset.load_mnist(""sample_data"", ""training"", one_hot = True)\n        mnist_mat_test, mnist_label_test \\\n          = dataset.load_mnist(""sample_data"", ""testing"", one_hot = True)\n        self.tlog(""mnist train data size : "" + str(len(mnist_mat_train)))\n        self.tlog(""mnist test data size : "" + str(len(mnist_mat_test)))        \n        \n    def test_process(self):\n        self.test_load_iris()\n        self.test_load_iris_one_hot()\n        self.test_load_mnist()\n        self.test_load_mnist_one_hot()\n'"
test_pytrain/test_lib/test_fs.py,0,"b'#\n# test lib.fs\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.lib import fs\nfrom pytrain.lib import nlp\n\nclass test_fs(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_fs_csv_loader(self):\n        sample_data = ""sample_data/iris/iris.csv""\n        self.tlog(""loading matrix => "" + sample_data)\n\n        dmat_train, dlabel_train, dmat_test, dlabel_test \\\n            = fs.csv_loader(sample_data, 0.2)\n\n        self.tlog(\'iris train data size : \' + str(len(dmat_train)))\n        self.tlog(\'iris test data size : \' + str(len(dmat_test)))\n\n        self.set_global_value(\'iris_mat_train\', dmat_train)\n        self.set_global_value(\'iris_label_train\', dlabel_train)\n        self.set_global_value(\'iris_mat_test\', dmat_test)\n        self.set_global_value(\'iris_label_test\', dlabel_test)\n\n    def test_fs_tsv_loader_with_nlp(self):\n        sample_words = ""sample_data/email/email.tsv""\n        self.tlog(""loading words => "" + sample_words)\n\n        nlp_eng = nlp(""eng"")\n        wordmat_train, wordlabel_train, voca, wordmat_test, wordlabel_test \\\n          = fs.tsv_loader_with_nlp(sample_words, 0.1, nlp_eng)\n\n        self.tlog(\'email data voca size : \' + str(len(voca)))\n        self.tlog(\'voca sample : \' + str(voca[:5]))\n\n    def test_process(self):\n        self.test_fs_csv_loader()\n        self.test_fs_tsv_loader_with_nlp()\n        # To see test of storing module, check test_decision_tree\n\n'"
test_pytrain/test_lib/test_nlp.py,0,"b'#\n# test lib.nlp\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.lib import nlp\n\nclass test_nlp(test_Suite):\n\n    voca = []\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_nlp_split(self):\n\n        nlp_eng = nlp(""eng"")\n\n        sentence = ""hello this is virus mail""\n        text = ""one sentence\\ntwo sentence\\nthree sentence""\n\n        words = nlp_eng.split2words(sentence)\n        words_text = nlp_eng.split2words(text)\n        split_sentence = nlp_eng.split2sentence(text)\n\n        self.tlog(words)\n        self.tlog(split_sentence)\n\n        assert words[2] == \'mail\'\n        assert len(words) == 3\n        assert split_sentence[1] == ""two sentence""\n        assert len(split_sentence) == 3\n        assert words_text[2] == ""two""\n        assert len(words_text) == 6\n\n    def test_nlp_extract_vocabulary(self):\n\n        nlp_eng = nlp(""eng"")\n        docs = [\\\n            ""Just try to enjoy it :)."",\\\n            ""It\'s very important for me!"",\\\n            ""What is your problem? you look so bad.""\\\n        ]\n        self.voca = nlp_eng.extract_vocabulary(docs)\n        self.tlog(self.voca)\n        assert len(self.voca) == 7\n\n    def test_word2vector(self):\n        nlp_eng = nlp(""eng"")\n\n        input_txt = ""try to do this one""\n        set_vector = nlp_eng.set_of_word2vector(self.voca, input_txt)\n        self.tlog(set_vector)\n        \n        input_txt2 = ""It\'s your problem. big problem. let\'s try""\n        bag_vector = nlp_eng.bag_of_word2vector(self.voca, input_txt2)\n        self.tlog(bag_vector)\n\n    def test_wordseq2matrix(self):\n        word_list_array = [\\\n            [\'I\',\'a\',\'m\',\'a\',\'b\',\'o\',\'y\'],\\\n            [\'Y\',\'o\',\'u\',\'a\',\'r\',\'e\',\'a\',\'g\',\'i\',\'r\',\'l\'],\\\n            [\'I\',\'a\',\'m\',\'a\',\'g\',\'o\',\'o\',\'d\',\'b\',\'o\',\'y\'],\\\n            [\'Y\',\'o\',\'u\',\'a\',\'r\',\'e\',\'a\',\'g\',\'o\',\'o\',\'d\',\'g\',\'i\',\'r\',\'l\'],\\\n        ]\n        nlp_common = nlp()\n        voca = nlp_common.extract_vocabulary(word_list_array)\n        word_mat_array = []\n        for word_list in word_list_array :\n            word_mat = nlp_common.set_of_wordseq2matrix(voca, word_list)\n            word_mat_array.append(word_mat)\n        self.tlog(word_mat_array)\n\n    def test_process(self):\n        self.test_nlp_split()\n        self.test_nlp_extract_vocabulary()\n        self.test_word2vector()\n        self.test_wordseq2matrix()\n'"
test_pytrain/test_lib/test_normalize.py,0,"b'#\n# test lib.normalize\n#\n# @ author becxer\n# @ email becxer87@gmail.com\n#\nfrom test_pytrain import test_Suite\nfrom pytrain.lib import normalize\n\nclass test_normalize(test_Suite):\n\n    def __init__(self, logging = True):\n        test_Suite.__init__(self, logging)\n\n    def test_normalize_quantile(self):\n        iris_mat_train = self.get_global_value(\'iris_mat_train\')\n        iris_mat_test = self.get_global_value(\'iris_mat_test\')\n        \n        normed_imat_train = normalize.quantile(iris_mat_train)\n        normed_imat_test = normalize.quantile(iris_mat_test)\n\n        self.tlog(""before normalized : \\n"" + str(iris_mat_train[:3]))\n        self.tlog(""normalized sample : \\n"" + str(normed_imat_train[:3]))\n\n        assert normed_imat_train[0][0] <= 1.0 and \\\n                normed_imat_train[0][0] >= 0.0\n\n        self.set_global_value(\'normed_iris_mat_train\',normed_imat_train)\n        self.set_global_value(\'normed_iris_mat_test\',normed_imat_test)\n\n    def test_process(self):\n        self.test_normalize_quantile()\n'"
