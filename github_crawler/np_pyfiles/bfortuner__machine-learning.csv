file_path,api_count,code
common.py,0,"b'import os\nimport sys\nimport copy\nimport math\nimport random\nfrom glob import glob\nimport operator\nimport itertools\nfrom pathlib import Path\n\nimport cv2\nimport colorsys\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import Rectangle\nimport matplotlib.lines as lines\nfrom matplotlib import animation, rc\nfrom matplotlib.patches import Polygon\nfrom IPython.display import HTML\nimport graphviz\nimport imutils\nimport skimage\nimport seaborn as sns\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.autograd as autograd\nimport torchvision.models\n\n# Config\nimport config as cfg\nimport constants as c\n\n# Modules\nfrom utils import img_loader\nfrom utils import csv_loader\nfrom utils import datasets\nfrom utils import training\nfrom utils import predictions\nfrom utils import learning_rates\n\nimport utils.metrics\nimport utils.imgs\nimport utils.files\nimport utils.metadata as meta\nimport utils.models\nimport utils.layers\n'"
config.py,0,"b""import os\n\nDATA_DIR = os.path.join('data/')\n"""
constants.py,0,"b""# Datasets\nTRAIN = 'trn'\nVAL = 'val'\nTEST = 'tst'\nFULL = 'full'\n\n# File extensions\nJPG = '.jpg'\nTIF = '.tif'\nPNG = '.png'\nGIF = '.gif'\nBCOLZ = '.bc'\nCSV = '.csv'\n\n# PyTorch\nMODEL_EXT = '.mdl'\nWEIGHTS_EXT = '.th'\nOPTIM_EXT = '.th'\n\n# Data Aug\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]"""
utils/__init__.py,0,b''
utils/csv_loader.py,0,"b'import os\nimport pandas as pd\n\n\ndef load_or_download_df(fpath, url=None):\n    if os.path.exists(fpath):\n        print(""-- found locally"")\n        df = pd.read_csv(fpath, index_col=0)\n    else:\n        print(""-- trying to download from url"")\n        try:\n            df = pd.read_csv(url)\n        except:\n            exit(""-- Unable to download from url"")\n\n        with open(fpath, \'w\') as f:\n            print(""-- writing to local file file"")\n            df.to_csv(f)\n    return df'"
utils/datasets.py,7,"b'import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets.samples_generator import make_swiss_roll\nimport torch\nimport torchvision\nfrom torchvision import transforms\nimport glob\nimport random\n\nimport config as cfg\nimport utils.metadata as meta\nfrom . import csv_loader\nfrom . import img_loader\n\n# Datasets\n# pytorch.org/docs/master/torchvision/datasets.html\n# https://github.com/bfortuner/pytorch-cheatsheet/blob/master/pytorch-cheatsheet.ipynb\n\n\ndef get_iris_data():\n    fpath = ""../data/iris.csv""\n    url = ""https://raw.githubusercontent.com/pydata/pandas/master/pandas/tests/data/iris.csv""\n    df = csv_loader.load_or_download_df(fpath, url)\n    return df\n\n\ndef get_sin_data():\n    rng = np.random.RandomState(1)\n    X = np.sort(5 * rng.rand(80, 1), axis=0)\n    y = np.sin(X).ravel()\n    y[::5] += 3 * (0.5 - rng.rand(16))\n    return X,y\n\n\ndef get_housing_data():\n    # https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html\n    fpath = ""../data/housing.csv""\n    url = ""https://raw.githubusercontent.com/ggallo/boston-housing/master/housing.csv""\n    df = csv_loader.load_or_download_df(fpath, url)\n    return df\n\n\ndef get_advertising_data():\n    fpath = ""../data/advertising.csv""\n    url = ""http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv""\n    df = csv_loader.load_or_download_df(fpath, url)\n    df = df.drop(df.columns[0], axis=1)\n    return df\n\n\ndef get_swiss_roll_data(n_samples=1000):\n    noise = 0.2\n    X, _ = make_swiss_roll(n_samples, noise)\n    X = X.astype(\'float32\')[:, [0, 2]]\n    return X, _\n\n\ndef get_swiss_roll_loader(n_samples=1000):\n    X, _ = get_swiss_roll_data(n_samples)\n    dataset = torch.utils.data.dataset.TensorDataset(\n        torch.FloatTensor(X), torch.FloatTensor(_))\n    loader = torch.utils.data.dataloader.DataLoader(\n        dataset, batch_size=100, shuffle=True)\n    return loader\n\n\ndef get_mnist_loader():\n    MNIST_MEAN = np.array([0.1307,])\n    MNIST_STD = np.array([0.3081,])\n    normTransform = transforms.Normalize(MNIST_MEAN, MNIST_STD)\n\n    trainTransform = transforms.Compose([\n        transforms.ToTensor(),\n        normTransform\n    ])\n    testTransform = transforms.Compose([\n        transforms.ToTensor(),\n        normTransform\n    ])\n\n    trainset = torchvision.datasets.MNIST(root=\'../data\', train=True,\n                                            download=True, transform=trainTransform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n                                            shuffle=True, num_workers=2)\n\n    testset = torchvision.datasets.MNIST(root=\'../data\', train=False,\n                                        download=True, transform=testTransform)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n                                            shuffle=False, num_workers=2)\n\n    return trainloader, testloader\n\n\ndef get_cifar_loader():\n    # https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py\n    CIFAR_MEAN = np.array([0.49139968, 0.48215827, 0.44653124])\n    CIFAR_STD = np.array([0.24703233, 0.24348505, 0.26158768])\n    normTransform = transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n\n    trainTransform = transforms.Compose([\n        transforms.RandomCrop(32),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        normTransform\n    ])\n    testTransform = transforms.Compose([\n        transforms.ToTensor(),\n        normTransform\n    ])\n\n    trainset = torchvision.datasets.CIFAR10(root=\'../data\', train=True,\n                                            download=True, transform=trainTransform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n                                              shuffle=True, num_workers=2)\n\n    testset = torchvision.datasets.CIFAR10(root=\'../data\', train=False,\n                                           download=True, transform=testTransform)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n                                            shuffle=False, num_workers=2)\n\n    classes = (\'plane\', \'car\', \'bird\', \'cat\',\n               \'deer\', \'dog\', \'frog\', \'horse\', \'ship\', \'truck\')\n\n    return trainloader, testloader, classes\n\n\ndef get_catsdogs_loader(imgs_dir):\n    # Need to download Kaggle cats/dogs competition\n    # And move ALL images into single directory\n    classes = [\'cat\',\'dog\']\n    class_to_idx, idx_to_class = meta.get_key_int_maps(classes)\n\n    def get_targs_from_fpaths(fpaths):\n        targs = []\n        for fpath in fpaths:\n            classname = fpath.split(\'/\')[-1].split(\'.\')[0]\n            # For one-hot sigmoid\n            #targ = meta.onehot_encode_class(\n            #    class_to_idx, classname)\n            targs.append(class_to_idx[classname])\n        return targs\n\n    normalize = transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225])\n\n    trainTransform = transforms.Compose([\n        transforms.RandomSizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        normalize\n    ])\n    testTransform = transforms.Compose([\n        transforms.Scale(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        normalize\n    ])\n\n    fpaths = glob.glob(imgs_dir + \'*.jpg\')\n    random.shuffle(fpaths)\n    trn_fpaths = fpaths[:20000]\n    val_fpaths = fpaths[20000:]\n\n    trn_targs = get_targs_from_fpaths(trn_fpaths)\n    val_targs = get_targs_from_fpaths(val_fpaths)\n\n    img_reader = \'pil\'\n    trn_dataset = FileDataset(\n        trn_fpaths, img_reader, trn_targs, trainTransform)\n    val_dataset = FileDataset(\n        val_fpaths, img_reader, val_targs, testTransform)\n\n    trn_loader = torch.utils.data.DataLoader(\n        trn_dataset, batch_size=64,\n        shuffle=True, num_workers=4)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=64,\n        shuffle=False, num_workers=2)\n\n    return trn_loader, val_loader, classes\n\n\nloaders = {\n    \'pil\': img_loader.pil_loader,\n    \'tns\': img_loader.tensor_loader,\n    \'npy\': img_loader.numpy_loader,\n    \'io\': img_loader.io_loader\n}\n\n\nclass FileDataset(torch.utils.data.Dataset):\n    def __init__(self, fpaths,\n                 img_loader=\'pil\',\n                 targets=None,\n                 transform=None,\n                 target_transform=None):\n        self.fpaths = fpaths\n        self.loader = self._get_loader(img_loader)\n        self.targets = targets\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def _get_loader(self, loader_type):\n        return loaders[loader_type]\n\n    def _get_target(self, index):\n        if self.targets is None:\n            return 1\n        target = self.targets[index]\n        if self.target_transform is not None:\n            return self.target_transform(target)\n        return int(target)\n\n    def _get_input(self, index):\n        img_path = self.fpaths[index]\n        img = self.loader(img_path)\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n\n    def __getitem__(self, index):\n        input_ = self._get_input(index)\n        target = self._get_target(index)\n        img_path = self.fpaths[index]\n        return input_, target, img_path\n\n    def __len__(self):\n        return len(self.fpaths)\n'"
utils/files.py,0,"b'import os\nimport random\nfrom glob import glob\nimport shutil\nimport gzip\nimport pickle\nimport json\nfrom contextlib import closing\nfrom zipfile import ZipFile, ZIP_DEFLATED\nimport re\nimport bcolz\n\n\n\ndef get_fnames_from_fpaths(fpaths):\n    fnames = []\n    for f in fpaths:\n        if isinstance(f, tuple):\n            f = f[0]\n        fnames.append(os.path.basename(f))\n    return fnames\n\n\ndef get_matching_files_in_dir(dirpath, regex):\n    fpaths = glob(os.path.join(dirpath,\'*.*\'))\n    match_objs, match_fpaths = [], []\n    for i in range(len(fpaths)):\n        match = re.search(regex, fpaths[i])\n        if match is not None:\n            match_objs.append(match)\n            match_fpaths.append(fpaths[i])\n    return match_objs, match_fpaths\n\n\ndef zipdir(basedir, archivename):\n    assert os.path.isdir(basedir)\n    with closing(ZipFile(archivename, ""w"", ZIP_DEFLATED)) as z:\n        for root, dirs, files in os.walk(basedir):\n            #NOTE: ignore empty directories\n            for fn in files:\n                absfn = os.path.join(root, fn)\n                zfn = absfn[len(basedir)+len(os.sep):] #XXX: relative path\n                z.write(absfn, zfn)\n\n\ndef unzipdir(archive_path, dest_path, remove=True):\n    ZipFile(archive_path).extractall(dest_path)\n    if remove:\n        os.remove(archive_path)\n\n\ndef save_json(fpath, dict_):\n    with open(fpath, \'w\') as f:\n        json.dump(dict_, f, indent=4, ensure_ascii=False)\n\n\ndef load_json(fpath):\n    with open(fpath, \'r\') as f:\n        json_ = json.load(f)\n    return json_\n\n\ndef pickle_obj(obj, fpath):\n    with open(fpath, \'wb\') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n\n\ndef unpickle_obj(fpath):\n    with open(fpath, \'rb\') as f:\n        return pickle.load(f)\n\n\ndef get_fname_from_fpath(fpath):\n    return os.path.basename(fpath)\n\n\ndef get_paths_to_files(root, file_ext=None, sort=True, strip_ext=False):\n    filepaths = []\n    fnames = []\n    for (dirpath, dirnames, filenames) in os.walk(root):\n        filepaths.extend(os.path.join(dirpath, f) \n            for f in filenames if file_ext is None or f.endswith(file_ext))\n        fnames.extend([f for f in filenames if file_ext is None or f.endswith(file_ext)])\n    if strip_ext:\n        fnames = [os.path.splitext(f)[0] for f in fnames]\n    if sort:\n        return sorted(filepaths), sorted(fnames)\n    return filepaths, fnames\n\n\ndef get_random_image_path(dir_path):\n    filepaths = get_paths_to_files(dir_path)[0]\n    return filepaths[random.randrange(len(filepaths))]\n\n\ndef save_obj(obj, out_fpath):\n    with open(out_fpath, \'wb\') as f:\n        pickle.dump(obj, f)\n\n\ndef load_obj(fpath):\n    return pickle.load(open(fpath, \'rb\'))\n\n\ndef save_bcolz_array(fpath, arr):\n    c=bcolz.carray(arr, rootdir=fpath, mode=\'w\')\n    c.flush()\n\n\ndef load_bcolz_array(fpath):\n    return bcolz.open(fpath)[:]\n\n\ndef compress_file(fpath):\n    gzip_fpath = fpath+\'.gz\'\n    with open(fpath, \'rb\') as f_in:\n        with gzip.open(gzip_fpath, \'wb\') as f_out:\n            shutil.copyfileobj(f_in, f_out)\n    return gzip_fpath\n\n\ndef write_lines(fpath, lines, compress=False):\n    lines_str = \'\\n\'.join(lines)\n    if compress:\n        fpath += \'.gz\'\n        lines_str = str.encode(lines_str)\n        f = gzip.open(fpath, \'wb\')\n    else:\n        f = open(fpath, \'w\')\n    f.write(lines_str)\n    f.close()\n    return fpath'"
utils/im2col.py,10,"b'import numpy as np\n\n# http://cs231n.github.io/assignments2017/assignment2/\n\ndef get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n    # First figure out what the size of the output should be\n    N, C, H, W = x_shape\n    assert (H + 2 * padding - field_height) % stride == 0\n    assert (W + 2 * padding - field_height) % stride == 0\n    out_height = int((H + 2 * padding - field_height) / stride + 1)\n    out_width = int((W + 2 * padding - field_width) / stride + 1)\n\n    i0 = np.repeat(np.arange(field_height), field_width)\n    i0 = np.tile(i0, C)\n    i1 = stride * np.repeat(np.arange(out_height), out_width)\n    j0 = np.tile(np.arange(field_width), field_height * C)\n    j1 = stride * np.tile(np.arange(out_width), out_height)\n    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n\n    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n\n    return (k.astype(int), i.astype(int), j.astype(int))\n\n\ndef im2col_indices(x, field_height, field_width, padding=1, stride=1):\n    """""" An implementation of im2col based on some fancy indexing """"""\n    # Zero-pad the input\n    p = padding\n    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode=\'constant\')\n\n    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n\n    cols = x_padded[:, k, i, j]\n    C = x.shape[1]\n    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n    return cols\n\n\ndef col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1,\n                   stride=1):\n    """""" An implementation of col2im based on fancy indexing and np.add.at """"""\n    N, C, H, W = x_shape\n    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n    if padding == 0:\n        return x_padded\n    return x_padded[:, :, padding:-padding, padding:-padding]'"
utils/img_loader.py,1,"b""import numpy as np \nfrom PIL import Image\nfrom skimage import io\nimport torch\n\n\ndef pil_loader(path):\n    return Image.open(path).convert('RGB')\n\n\ndef tensor_loader(path):\n    return torch.load(path)\n\n\ndef numpy_loader(path):\n    return np.load(path)\n\n\ndef io_loader(path):\n    return io.imread(path)\n   """
utils/imgs.py,7,"b'import random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom . import files\nimport torchvision\n\n\ndef plot_tensor(img, fs=(10,10), title=""""):\n    if len(img.size()) == 4:\n        img = img.squeeze(dim=0)\n    npimg = img.numpy()\n    plt.figure(figsize=fs)\n    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap=\'gray\')\n    plt.title(title)\n    plt.show()\n\ndef plot_batch(samples, title="""", fs=(10,10)):\n    plot_tensor(torchvision.utils.make_grid(samples), fs=fs, title=title)\n\ndef plot_metric(trn, tst, title):\n    plt.plot(np.stack([trn, tst], 1));\n    plt.title(title)\n    plt.show()\n\ndef load_img_as_arr(img_path):\n    return plt.imread(img_path)\n\n\ndef load_img_as_pil(img_path):\n    return Image.open(img_path).convert(\'RGB\')\n\n\ndef norm_meanstd(arr, mean, std):\n    return (arr - mean) / std\n\n\ndef denorm_meanstd(arr, mean, std):\n    return (arr * std) + mean\n\n\ndef norm255_tensor(arr):\n    """"""Given a color image/where max pixel value in each channel is 255\n    returns normalized tensor or array with all values between 0 and 1""""""\n    return arr / 255.\n\n\ndef denorm255_tensor(arr):\n    return arr * 255.\n\n\ndef plot_arr(arr, fs=(6,6), title=None):\n    if len(arr.shape) == 2:\n        plot_gray_arr(arr, fs, title)\n    else:\n        plot_img_arr(arr, fs, title)\n\n\ndef plot_img_arr(arr, fs=(6,6), title=None):\n    plt.figure(figsize=fs)\n    plt.imshow(arr.astype(\'uint8\'))\n    plt.title(title)\n    plt.show()\n\n\ndef plot_gray_arr(arr, fs=(6,6), title=None):\n    plt.figure(figsize=fs)\n    plt.imshow(arr.astype(\'float32\'), cmap=\'gray\')\n    plt.title(title)\n    plt.show()\n\n\ndef plot_imgs(imgs, titles=None, dim=(4,4), fs=(6,6)):\n    plt.figure(figsize=fs)\n    for i,img in enumerate(imgs[:dim[0]*dim[1]]):\n        # Tensor\n        if type(img) is not np.ndarray:\n            img = img.numpy().transpose((0,2,3,1))\n\n        plt.subplot(*dim, i+1)\n        if len(img.shape) == 2:\n            plt.imshow(img.astype(\'float32\'), cmap=\'gray\')\n        else:\n            plt.imshow(img.astype(\'uint8\'))\n        plt.title(titles[i])\n        plt.axis(\'off\')\n    plt.tight_layout()\n\n\ndef plot_rgb_samples(arr, dim=(4,4), figsize=(6,6)):\n    if type(arr) is not np.ndarray:\n        arr = arr.numpy().transpose((0,2,3,1))\n    plt.figure(figsize=figsize)\n    for i,img in enumerate(arr[:16]):\n        plt.subplot(*dim, i+1)\n        plt.imshow(img)\n        plt.axis(\'off\')\n    plt.tight_layout()\n\n\ndef plot_bw_samples(arr, dim=(4,4), figsize=(6,6)):\n    if type(arr) is not np.ndarray:\n        arr = arr.numpy()\n    arr = arr.reshape(arr.shape[0], 28, 28)\n    plt.figure(figsize=figsize)\n    for i,img in enumerate(arr[:16]):\n        plt.subplot(*dim, i+1)\n        plt.imshow(img.astype(\'float32\'), cmap=\'gray\')\n        plt.axis(\'off\')\n    plt.tight_layout()\n\n\ndef plot_img_from_fpath(img_path, fs=(8,8), title=None):\n    plt.figure(figsize=fs)\n    plt.imshow(plt.imread(img_path))\n    plt.title(title)\n    plt.show()\n\n\ndef plot_samples_from_dir(dir_path, shuffle=False, n=6):\n    fpaths, fnames = files.get_paths_to_files(dir_path)\n    plt.figure(figsize=(16,12))\n    start = random.randint(0,len(fpaths)-1) if shuffle else 0\n    j = 1\n    for idx in range(start, min(len(fpaths), start+n)):\n        plt.subplot(2,3,j)\n        plt.imshow(plt.imread(fpaths[idx]))\n        plt.title(fnames[idx])\n        plt.axis(\'off\')\n        j += 1\n\n\ndef cut_image(arr, mask, color=(255,255,255)):\n    arr = arr.copy()\n    mask = format_1D_binary_mask(mask.copy())\n    arr[mask > 0] = 255\n    return arr\n\n\ndef format_1D_binary_mask(mask):\n    if len(mask.shape) == 2:\n        mask = np.expand_dims(mask, 0)\n    mask = np.stack([mask,mask,mask], axis=1).squeeze().transpose(1,2,0)\n    return mask.astype(\'float32\')\n\n\ndef plot_binary_mask(arr, mask, title=None, color=(255,255,255)):\n    mask = format_1D_binary_mask(mask.copy())\n    for i in range(3):\n        arr[:,:,i][mask[:,:,i] > 0] = color[i]\n    utils.imgs.plot_img_arr(arr, title=title)\n\n\ndef plot_binary_mask_overlay(img_arr, mask, fs=(18,18), title=None):\n    mask = format_1D_binary_mask(mask.copy())\n    fig = plt.figure(figsize=fs)\n    a = fig.add_subplot(1,2,1)\n    a.set_title(title)\n    plt.imshow(img_arr.astype(\'uint8\'))\n    plt.imshow(mask, cmap=\'jet\', alpha=.5) # interpolation=\'none\'\n    plt.show()'"
utils/layers.py,0,"b'import torch.nn as nn\n\n\nclass CenterCrop(nn.Module):\n    def __init__(self, height, width):\n        super().__init__()\n        self.height = height\n        self.width = width\n\n    def forward(self, img):\n        bs, c, h, w = img.size()\n        xy1 = (w - self.width) // 2\n        xy2 = (h - self.height) // 2\n        img = img[:, :, xy2:(xy2 + self.height), xy1:(xy1 + self.width)]\n        return img\n\ndef conv_relu(in_channels, out_channels, kernel_size=3, stride=1,\n              padding=1, bias=True):\n    return [\n        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n            stride=stride, padding=padding, bias=bias),\n        nn.ReLU(inplace=True),\n    ]\n\ndef conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1,\n                 padding=1, bias=False):\n    return [\n        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n                  stride=stride, padding=padding, bias=bias),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True),\n    ]\n\ndef linear_bn_relu_drop(in_channels, out_channels, dropout=0.5, bias=False):\n    layers = [\n        nn.Linear(in_channels, out_channels, bias=bias),\n        nn.BatchNorm1d(out_channels),\n        nn.ReLU(inplace=True)\n    ]\n    if dropout > 0:\n        layers.append(nn.Dropout(dropout))\n    return layers\n\ndef get_fc(in_feat, n_classes, activation=None):\n    layers = [\n        nn.Linear(in_features=in_feat, out_features=n_classes)\n    ]\n    if activation is not None:\n        layers.append(activation)\n    return nn.Sequential(*layers)\n\ndef get_classifier(in_feat, n_classes, activation, p=0.5):\n    layers = [\n        nn.BatchNorm1d(num_features=in_feat),\n        nn.Dropout(p),\n        nn.Linear(in_features=in_feat, out_features=n_classes),\n        activation\n    ]\n    return nn.Sequential(*layers)\n\ndef get_mlp_classifier(in_feat, out_feat, n_classes, activation, p=0.01, p2=0.5):\n    layers = [\n        nn.BatchNorm1d(num_features=in_feat),\n        nn.Dropout(p),\n        nn.Linear(in_features=in_feat, out_features=out_feat),\n        nn.ReLU(),\n        nn.BatchNorm1d(num_features=out_feat),\n        nn.Dropout(p2),\n        nn.Linear(in_features=out_feat, out_features=n_classes),\n        activation\n    ]\n    return nn.Sequential(*layers)\n'"
utils/learning_rates.py,0,"b""import math\nimport operator\nimport copy\n\n\ndef set_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef get_learning_rate(optimizer):\n    return optimizer.param_groups[0]['lr']\n\n\n\nclass LearningRate():\n    def __init__(self, initial_lr, iteration_type):\n        self.initial_lr = initial_lr\n        self.iteration_type = iteration_type #epoch or mini_batch\n\n    def get_learning_rate(self, optimizer):\n        return optimizer.param_groups[0]['lr']\n\n    def set_learning_rate(self, optimizer, new_lr):\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = new_lr\n\n    def adjust(self, optimizer, lr, iteration, params=None):\n        self.set_learning_rate(optimizer, lr)\n        return lr\n\n\nclass FixedLR(LearningRate):\n    def __init__(self, initial_lr, iteration_type):\n        super().__init__(initial_lr, iteration_type)\n\n    def adjust(self, optimizer, iteration, params=None):\n        new_lr = super().get_learning_rate(optimizer)\n        return new_lr\n\n\nclass LinearLR(LearningRate):\n    def __init__(self, initial_lr, iteration_type, fixed_delta):\n        super().__init__(initial_lr, iteration_type)\n        self.fixed_delta = fixed_delta\n\n    def adjust(self, optimizer, iteration, params=None):\n        lr = super().get_learning_rate(optimizer)\n        new_lr = lr + self.fixed_delta\n        super().set_learning_rate(optimizer, new_lr)\n        return new_lr\n\n\nclass SnapshotLR(LearningRate):\n    '''https://arxiv.org/abs/1704.00109'''\n    def __init__(self, initial_lr, iteration_type,\n                 max_lr, total_iters, n_cycles):\n        '''\n        n_iters = total number of mini-batch iterations during training\n        n_cycles = total num snapshots during training\n        max_lr = starting learning rate each cycle'''\n        super().__init__(initial_lr, iteration_type)\n        self.max_lr = max_lr\n        self.total_iters = total_iters\n        self.cycles = n_cycles\n\n    def cosine_annealing(self, t):\n        '''t = current mini-batch iteration'''\n        return self.max_lr/2 * (math.cos(\n         (math.pi * (t % (self.total_iters//self.cycles))) /\n         (self.total_iters//self.cycles)) + 1)\n\n    def adjust(self, optimizer, iteration, params=None):\n        new_lr = self.cosine_annealing(iteration)\n        self.set_learning_rate(optimizer, new_lr)\n        return new_lr\n\n\nclass SnapshotParamsLR(LearningRate):\n    '''Snapshot Learning with per-parameter LRs'''\n    def __init__(self, initial_lr, iteration_type,\n                 total_iters, n_cycles):\n        '''\n        n_iters = total number of mini-batch iterations during training\n        n_cycles = total num snapshots during training\n        max_lr = starting learning rate each cycle'''\n        super().__init__(initial_lr, iteration_type)\n        self.total_iters = total_iters\n        self.cycles = n_cycles\n\n    def cosine_annealing(self, t, max_lr):\n        return max_lr/2 * (math.cos(\n         (math.pi * (t % (self.total_iters//self.cycles)))/(\n            self.total_iters//self.cycles)) + 1)\n\n    def adjust(self, optimizer, iteration, params=None):\n        lrs = []\n        for param_group in optimizer.param_groups:\n            new_lr = self.cosine_annealing(iteration, param_group['max_lr'])\n            param_group['lr'] = new_lr\n            lrs.append(new_lr)\n        return new_lr\n\n\nclass DevDecayLR(LearningRate):\n    '''https://arxiv.org/abs/1705.08292'''\n    def __init__(self, initial_lr, iteration_type,\n                 decay_factor=0.9, decay_patience=1):\n        super().__init__(initial_lr, iteration_type)\n        self.decay_factor = decay_factor\n        self.decay_patience = decay_patience\n\n    def adjust(self, optimizer, iteration, params):\n        lr = super().get_learning_rate(optimizer)\n        best_iter = params['best_iter']\n\n        if (iteration - best_iter) > self.decay_patience:\n            print('Decaying learning rate by factor: {:.5f}'.format(\n                self.decay_factor).rstrip('0'))\n            lr *= self.decay_factor\n            super().set_learning_rate(optimizer, lr)\n        return lr\n\n\nclass ScheduledLR(LearningRate):\n    def __init__(self, initial_lr, iteration_type, lr_schedule):\n        super().__init__(initial_lr, iteration_type)\n        self.lr_schedule = lr_schedule\n\n    def adjust(self, optimizer, iteration, params=None):\n        if iteration in self.lr_schedule:\n            new_lr = self.lr_schedule[iteration]\n        else:\n            new_lr = self.get_learning_rate(optimizer)\n        super().set_learning_rate(optimizer, new_lr)\n        return new_lr\n\n\nclass DecayingLR(LearningRate):\n    def __init__(self, initial_lr, iteration_type, decay, n_epochs):\n         super().__init__(initial_lr, iteration_type)\n         self.decay = decay\n         self.n_epochs = n_epochs\n\n    def exponential_decay(self, iteration, params=None):\n        '''Update learning rate to `initial_lr` decayed\n        by `decay` every `n_epochs`'''\n        return self.initial_lr * (self.decay ** (iteration // self.n_epochs))\n\n    def adjust(self, optimizer, iteration):\n        new_lr = self.exponential_decay(iteration)\n        super().set_learning_rate(optimizer, new_lr)\n        return new_lr\n\n\nclass CyclicalLR(LearningRate):\n    '''https://arxiv.org/abs/1506.01186'''\n    def __init__(self, initial_lr, iteration_type, n_iters, cycle_length,\n                 min_lr, max_lr):\n         assert initial_lr == min_lr\n         super(CyclicalLR, self).__init__(initial_lr, iteration_type)\n         self.n_iters = n_iters\n         self.cycle_length = cycle_length\n         self.min_lr = min_lr\n         self.max_lr = max_lr\n\n    def triangular(self, iteration):\n        iteration -= 1 # if iteration count starts at 1\n        cycle = math.floor(1 + iteration/self.cycle_length)\n        x = abs(iteration/(self.cycle_length/2) - 2*cycle + 1)\n        new_lr = self.min_lr + (self.max_lr - self.min_lr) * max(0, (1-x))\n        return new_lr\n\n    def adjust(self, optimizer, iteration, best_iter=1):\n        new_lr = self.triangular(iteration)\n        super().set_learning_rate(optimizer, new_lr)\n        return new_lr\n\n\n\n\n## Helpers\n\ndef cosine_annealing(lr_max, T, M, t):\n    '''\n    t = current mini-batch iteration\n    # lr(t) = f(t-1 % T//M)\n    # lr(t) = lr_max/2 * (math.cos( (math.pi * (t % T/M))/(T/M) ) + 1)\n    '''\n    return lr_max/2 * (math.cos( (math.pi * (t % (T//M)))/(T//M)) + 1)"""
utils/metadata.py,1,"b""import numpy as np\nimport pandas as pd\n\n\ndef get_key_int_maps(keys):\n    key_to_int = {name: i for i, name in enumerate(keys)}\n    int_to_key = {i: name for i, name in enumerate(keys)}\n    return key_to_int, int_to_key\n\ndef onehot_encode_class(class_to_idx, classname):\n    n_classes = len(class_to_idx.keys())\n    onehot = np.zeros(n_classes)\n    idx = class_to_idx[classname]\n    onehot[idx] = 1\n    return onehot   \n\ndef encode_column(df, column):\n    df = df.copy()\n    unique_names = df[column].unique()\n    key_to_int, int_to_key = get_key_int_maps(unique_names)\n    encoded = [key_to_int[c] for c in df[column].values]\n    df[column+'_code'] = encoded\n    return df"""
utils/metrics.py,3,"b""import operator\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import fbeta_score\nfrom sklearn import metrics as scipy_metrics\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport warnings\n\n\n\nclass Metric():\n    def __init__(self, name, minimize=True):\n        self.name = name\n        self.minimize = minimize\n\n    def get_best_epoch(self, values):\n        if self.minimize:\n            idx, value = min(enumerate(values),\n                key=operator.itemgetter(1))\n        else:\n            idx, value = max(enumerate(values),\n                key=operator.itemgetter(1))\n        epoch = idx + 1 # epochs start at 1\n        return epoch, value\n\n    def evaluate(self, loss, preds, probs, targets):\n        pass\n\n    def format(self, value):\n        pass\n\n\nclass AuxiliaryMetric():\n    def __init__(self, name, units):\n        self.name = name\n        self.units = units\n\n\nclass Accuracy(Metric):\n    def __init__(self):\n        super().__init__('Accuracy', minimize=False)\n\n    def evaluate(self, loss, preds, probs, targets):\n        return get_accuracy(preds, targets)\n\n    def format(self, value):\n        return value\n\n\nclass Loss(Metric):\n    def __init__(self):\n        super().__init__('Loss', minimize=True)\n\n    def evaluate(self, loss, preds, probs, targets):\n        return loss\n\n    def format(self, value):\n        return value\n\n\nclass F2Score(Metric):\n    def __init__(self, target_threshold=None):\n        super().__init__('F2', minimize=False)\n        self.target_threshold = target_threshold  # pseudo soft targets\n\n    def evaluate(self, loss, preds, probs, targets):\n        average = 'samples' if targets.shape[1] > 1 else 'binary'\n        if self.target_threshold is not None:\n            targets = targets > self.target_threshold\n\n        return get_f2_score(preds, targets, average)\n\n    def format(self, value):\n        return value\n\n\nclass DiceScore(Metric):\n    def __init__(self):\n        super().__init__('Dice', minimize=False)\n\n    def evaluate(self, loss, preds, probs, targets):\n        return get_dice_score(preds, targets)\n\n    def format(self, value):\n        return value\n\n\ndef get_accuracy(preds, targets):\n    preds = preds.flatten() \n    targets = targets.flatten()\n    correct = np.sum(preds==targets)\n    return correct / len(targets)\n\n\ndef get_cross_entropy_loss(probs, targets):\n    return F.binary_cross_entropy(\n              Variable(torch.from_numpy(probs)),\n              Variable(torch.from_numpy(targets).float())).data[0]\n\n\ndef get_recall(preds, targets):\n    return scipy_metrics.recall_score(targets.flatten(), preds.flatten())\n\n\ndef get_precision(preds, targets):\n    return scipy_metrics.precision_score(targets.flatten(), preds.flatten())\n\n\ndef get_roc_score(probs, targets):\n    return scipy_metrics.roc_auc_score(targets.flatten(), probs.flatten())\n\n\ndef get_dice_score(preds, targets):\n    eps = 1e-7\n    batch_size = preds.shape[0]\n    preds = preds.reshape(batch_size, -1)\n    targets = targets.reshape(batch_size, -1)\n\n    total = preds.sum(1) + targets.sum(1) + eps\n    intersection = (preds * targets).astype(float)\n    score = 2. * intersection.sum(1) / total\n    return np.mean(score)\n\n\ndef get_f2_score(y_pred, y_true, average='samples'):\n    y_pred, y_true, = np.array(y_pred), np.array(y_true)\n    return fbeta_score(y_true, y_pred, beta=2, average=average) \n"""
utils/models.py,0,"b'import torch\nimport torch.nn as nn\nimport torchvision.models\n\n\ndef load_model(fpath, cuda=True):\n    if cuda:\n        return torch.load(fpath).cuda()\n    return torch.load(fpath)\n\n\ndef save_model(model, fpath):\n    torch.save(model.cpu(), fpath)\n\n\ndef load_weights(model, fpath):\n    state = torch.load(fpath)\n    model.load_state_dict(state[\'state_dict\'])\n\n\ndef save_weights(model, fpath, epoch=None, name=None):\n    torch.save({\n        \'name\': name,\n        \'epoch\': epoch,\n        \'state_dict\': model.state_dict()\n    }, fpath)\n\n\ndef freeze_layers(model, n_layers):\n    i = 0\n    for child in model.children():\n        if i >= n_layers:\n            break\n        print(i, ""freezing"", child)\n        for param in child.parameters():\n            param.requires_grad = False\n        i += 1\n\n\ndef freeze_nested_layers(model, n_layers):\n    i = 0\n    for child in model.children():\n        for grandchild in child.children():\n            if isinstance(grandchild, torch.nn.modules.container.Sequential):\n                for greatgrand in grandchild.children():\n                    if i >= n_layers:\n                        break\n                    for param in greatgrand.parameters():\n                        param.requires_grad = False\n                    print(i, ""freezing"", greatgrand)\n                    i += 1\n            else:\n                if i >= n_layers:\n                    break\n                for param in grandchild.parameters():\n                    param.requires_grad = False\n                print(i, ""freezing"", grandchild)\n                i += 1\n\n\ndef init_nested_layers(module, conv_init, fc_init):\n    for child in module.children():\n        if len(list(child.children())) > 0:\n            init_nested_layers(child, conv_init, fc_init)\n        else:\n            init_weights(child, conv_init, fc_init)\n\n\ndef init_weights(layer, conv_init, fc_init):\n    if isinstance(layer, torch.nn.Conv2d):\n        print(""init"", layer, ""with"", conv_init)\n        conv_init(layer.weight)\n    elif isinstance(layer, torch.nn.Linear):\n        print(""init"", layer, ""with"", fc_init)\n        fc_init(layer.weight)\n\n\ndef cut_model(model, cut):\n    return nn.Sequential(*list(model.children())[:cut])\n\ndef get_resnet18(pretrained, n_freeze):\n    resnet = torchvision.models.resnet18(pretrained)\n    if n_freeze > 0:\n        freeze_layers(resnet, n_freeze)\n    return resnet\n\n\ndef get_resnet34(pretrained, n_freeze):\n    resnet = torchvision.models.resnet34(pretrained)\n    if n_freeze > 0:\n        freeze_layers(resnet, n_freeze)\n    return resnet\n\n\ndef get_resnet50(pretrained, n_freeze):\n    resnet = torchvision.models.resnet50(pretrained)\n    if n_freeze > 0:\n        freeze_layers(resnet, n_freeze)\n    return resnet\n\nclass Resnet(nn.Module):\n    def __init__(self, resnet, classifier):\n        super().__init__()\n        self.resnet = resnet\n        self.ap = nn.AdaptiveAvgPool2d((1,1))\n        self.classifier = classifier\n\n    def forward(self, x):\n        x = self.resnet(x)\n        x = self.ap(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\ndef get_classifier(in_chans, n_classes):\n    return nn.Sequential(\n        nn.Linear(in_chans, 128),\n        nn.BatchNorm1d(128),\n        nn.ReLU(),\n        nn.Dropout(0.5),\n        nn.Linear(128, n_classes),\n        nn.Softmax()\n    )\n'"
utils/predictions.py,5,"b'import os\nimport scipy\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.autograd import Variable\n\n\ndef predict_batch(net, inputs):\n    v = Variable(inputs.cuda(), volatile=True)\n    return net(v).data.cpu().numpy()\n\n\ndef get_probabilities(model, loader):\n    model.eval()\n    return np.vstack(predict_batch(model, data[0]) for data in loader)\n\n\ndef get_predictions(probs, thresholds):\n    preds = np.copy(probs)\n    preds[preds >= thresholds] = 1\n    preds[preds < thresholds] = 0\n    return preds.astype(\'uint8\')\n\n\ndef get_argmax(output):\n    val,idx = torch.max(output, dim=1)\n    return idx.data.cpu().view(-1).numpy()\n\n\ndef get_targets(loader):\n    targets = None\n    for data in loader:\n        if targets is None:\n            shape = list(data[1].size())\n            shape[0] = 0\n            targets = np.empty(shape)\n        target = data[1]\n        if len(target.size()) == 1:\n            target = target.view(-1,1)\n        target = target.numpy()\n        targets = np.vstack([targets, target])\n    return targets\n\n\ndef ensemble_with_method(arr, method):\n    if method == c.MEAN:\n        return np.mean(arr, axis=0)\n    elif method == c.GMEAN:\n        return scipy.stats.mstats.gmean(arr, axis=0)\n    elif method == c.VOTE:\n        return scipy.stats.mode(arr, axis=0)[0][0]\n    raise Exception(""Operation not found"")'"
utils/training.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport time\nimport math\nimport time\n\nfrom . import predictions\n\n\nclass QuickTrainer():\n    def __init__(self):\n        self.metrics = {\n            'loss': {\n                'trn':[],\n                'tst':[]\n            },\n            'accuracy': {\n                'trn':[],\n                'tst':[]\n            },\n        }\n\n    def run(self, net, trn_loader, tst_loader, criterion, optimizer, epochs):\n        for epoch in range(1, epochs+1):\n            trn_loss, trn_acc = train(net, trn_loader, criterion, optimizer)\n            tst_loss, tst_acc = test(net, tst_loader, criterion)\n            print('Epoch %d, TrnLoss: %.3f, TrnAcc: %.3f, TstLoss: %.3f, TstAcc: %.3f' % (\n                epoch, trn_loss, trn_acc, tst_loss, tst_acc))\n            self.metrics['loss']['trn'].append(trn_loss)\n            self.metrics['loss']['tst'].append(tst_loss)\n            self.metrics['accuracy']['trn'].append(trn_acc)\n            self.metrics['accuracy']['tst'].append(tst_acc)\n            # lr = get_learning_rate(optimizer)\n            # print(get_metric_msg('trn', self.metrics, 0))\n            # print(get_metric_msg('tst', self.metrics, 0))\n\n\ndef train(net, dataloader, criterion, optimizer):\n    net.train()\n    n_batches = len(dataloader)\n    total_loss = 0\n    total_acc = 0\n    for data in dataloader:\n        inputs = Variable(data[0].cuda())\n        targets = Variable(data[1].cuda())\n\n        output = net(inputs)\n        loss = criterion(output, targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        preds = predictions.get_argmax(output)\n        accuracy = get_accuracy(preds, targets.data.cpu().numpy())\n\n        total_loss += loss.data[0]\n        total_acc += accuracy\n\n    mean_loss = total_loss / n_batches\n    mean_acc = total_acc / n_batches\n    return mean_loss, mean_acc\n\n\ndef get_accuracy(preds, targets):\n    correct = np.sum(preds==targets)\n    return correct / len(targets)\n\n\ndef test(net, test_loader, criterion):\n    net.eval()\n    test_loss = 0\n    test_acc = 0\n    for data in test_loader:\n        inputs = Variable(data[0].cuda(), volatile=True)\n        target = Variable(data[1].cuda())\n        output = net(inputs)\n        test_loss += criterion(output, target).data[0]\n        pred = predictions.get_argmax(output)\n        test_acc += get_accuracy(pred, target.data.cpu().numpy())\n    test_loss /= len(test_loader) #n_batches\n    test_acc /= len(test_loader)\n    return test_loss, test_acc\n\n\nclass Trainer():\n    def __init__(self, metrics):\n        self.metrics = metrics\n\n    def train(self, model, optim, lr_adjuster, criterion, trn_loader,\n              val_loader, n_epochs, n_classes):\n        start_epoch = 1\n        end_epoch = start_epoch + n_epochs\n\n        for epoch in range(start_epoch, end_epoch):\n            current_lr = lr_adjuster.get_learning_rate(optim)\n\n            ### Train ###\n            trn_start_time = time.time()\n            trn_metrics = train_model(model, trn_loader, optim, criterion,\n                                      lr_adjuster, epoch, self.metrics)\n            trn_msg = log_trn_msg(trn_start_time, trn_metrics, current_lr, epoch)\n            print(trn_msg)\n\n            ### Test ###\n            val_start_time = time.time()\n            val_metrics = test_model(model, val_loader, criterion, self.metrics, n_classes)\n            val_msg = log_val_msg(val_start_time, val_metrics, current_lr)\n            print(val_msg)\n\n            ### Adjust Lr ###\n            if lr_adjuster.iteration_type == 'epoch':\n                lr_adjuster.adjust(optim, epoch+1)\n\n\ndef train_model(model, dataloader, optimizer, criterion,\n                lr_adjuster, epoch, metrics):\n    model.train()\n    n_batches = len(dataloader)\n    cur_iter = int((epoch-1) * n_batches)+1\n    metric_totals = {m.name:0 for m in metrics}\n\n    for data in dataloader:\n        inputs = Variable(data[0].cuda(async=True))\n        targets = Variable(data[1].cuda(async=True))\n\n        output = model(inputs)\n        model.zero_grad()\n\n        loss = criterion(output, targets)\n        loss_data = loss.data[0]\n        probs = output.data.cpu().numpy()\n        preds = np.argmax(probs, axis=1)\n\n        for metric in metrics:\n            score = metric.evaluate(loss_data, preds, probs, None)\n            metric_totals[metric.name] += score\n\n        loss.backward()\n        optimizer.step()\n\n        if lr_adjuster.iteration_type == 'mini_batch':\n            lr_adjuster.adjust(optimizer, cur_iter)\n        cur_iter += 1\n\n    for metric in metrics:\n        metric_totals[metric.name] /= n_batches\n\n    return metric_totals\n\n\ndef test_model(model, loader, criterion, metrics, n_classes):\n    model.eval()\n\n    loss = 0\n    probs = np.empty((0, n_classes))\n    metric_totals = {m.name:0 for m in metrics}\n\n    for data in loader:\n        inputs = Variable(data[0].cuda(async=True), volatile=True)\n        targets = Variable(data[1].cuda(async=True), volatile=True)\n\n        output = model(inputs)\n\n        loss += criterion(output, targets).data[0]\n        probs = np.vstack([probs, output.data.cpu().numpy()])\n\n    loss /= len(loader)\n    preds = np.argmax(probs, axis=1)\n    for metric in metrics:\n        score = metric.evaluate(loss, preds, probs, None)\n        metric_totals[metric.name] = score\n\n    return metric_totals\n\ndef early_stop(epoch, best_epoch, patience):\n    return (epoch - best_epoch) > patience\n\n\ndef log_trn_msg(start_time, trn_metrics, lr, epoch):\n    epoch_msg = 'Epoch {:d}'.format(epoch)\n    metric_msg = get_metric_msg('trn', trn_metrics, lr)\n    time_msg = get_time_msg(start_time)\n    combined = epoch_msg + '\\n' + metric_msg + time_msg\n    return combined\n\n\ndef log_val_msg(start_time, trn_metrics, lr):\n    metric_msg = get_metric_msg('val', trn_metrics, lr)\n    time_msg = get_time_msg(start_time)\n    combined = metric_msg + time_msg\n    return combined\n\n\ndef get_metric_msg(dset, metrics_dict, lr=0):\n    msg = dset.capitalize() + ' - '\n    for name in metrics_dict.keys():\n        print(metrics_dict[name])\n        metric_str = ('{:.4f}').format(metrics_dict[name]).lstrip('0')\n        msg += ('{:s} {:s} | ').format(name, metric_str)\n    msg += 'LR ' + '{:.6f}'.format(lr).rstrip('0').lstrip('0') + ' | '\n    return msg\n\n\ndef get_time_msg(start_time):\n    time_elapsed = time.time() - start_time\n    msg = 'Time {:.1f}m {:.2f}s'.format(\n        time_elapsed // 60, time_elapsed % 60)\n    return msg\n\ndef get_learning_rate(optimizer):\n    return optimizer.param_groups[0]['lr']\n"""
utils/visualize.py,0,b''
