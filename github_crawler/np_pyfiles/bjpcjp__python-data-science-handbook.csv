file_path,api_count,code
iPython/mprun_demo.py,0,b'def sum_of_lists(N):\n    total = 0\n    for i in range(5):\n        L = [j ^ (j >> i) for j in range(N)]\n        total += sum(L)\n        del L # remove reference to L\n    return total'
scikit/Scikit-GNB-example.py,6,"b""import numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n\nfig, ax = plt.subplots()\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\nax.set_title('Naive Bayes Model', size=14)\n\nxlim = (-8, 8)\nylim = (-15, 5)\n\nxg = np.linspace(xlim[0], xlim[1], 60)\nyg = np.linspace(ylim[0], ylim[1], 40)\nxx, yy = np.meshgrid(xg, yg)\nXgrid = np.vstack([xx.ravel(), yy.ravel()]).T\n\nfor label, color in enumerate(['red', 'blue']):\n    mask = (y == label)\n    mu, std = X[mask].mean(0), X[mask].std(0)\n    P = np.exp(-0.5 * (Xgrid - mu) ** 2 / std ** 2).prod(1)\n    Pm = np.ma.masked_array(P, P < 0.03)\n    ax.pcolorfast(xg, yg, Pm.reshape(xx.shape), alpha=0.5,\n                  cmap=color.title() + 's')\n    ax.contour(xx, yy, P.reshape(xx.shape),\n               levels=[0.01, 0.1, 0.5, 0.9],\n               colors=color, alpha=0.2)\n    \nax.set(xlim=xlim, ylim=ylim)\n\nfig.savefig('gaussian-NB.png')"""
scikit/helpers_05_08.py,7,"b""\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom ipywidgets import interact\n\n\ndef visualize_tree(estimator, X, y, boundaries=True,\n                   xlim=None, ylim=None, ax=None):\n    ax = ax or plt.gca()\n    \n    # Plot the training points\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='viridis',\n               clim=(y.min(), y.max()), zorder=3)\n    ax.axis('tight')\n    ax.axis('off')\n    if xlim is None:\n        xlim = ax.get_xlim()\n    if ylim is None:\n        ylim = ax.get_ylim()\n    \n    # fit the estimator\n    estimator.fit(X, y)\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    n_classes = len(np.unique(y))\n    Z = Z.reshape(xx.shape)\n    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n                           levels=np.arange(n_classes + 1) - 0.5,\n                           cmap='viridis', clim=(y.min(), y.max()),\n                           zorder=1)\n\n    ax.set(xlim=xlim, ylim=ylim)\n    \n    # Plot the decision boundaries\n    def plot_boundaries(i, xlim, ylim):\n        if i >= 0:\n            tree = estimator.tree_\n        \n            if tree.feature[i] == 0:\n                ax.plot([tree.threshold[i], tree.threshold[i]], ylim, '-k', zorder=2)\n                plot_boundaries(tree.children_left[i],\n                                [xlim[0], tree.threshold[i]], ylim)\n                plot_boundaries(tree.children_right[i],\n                                [tree.threshold[i], xlim[1]], ylim)\n        \n            elif tree.feature[i] == 1:\n                ax.plot(xlim, [tree.threshold[i], tree.threshold[i]], '-k', zorder=2)\n                plot_boundaries(tree.children_left[i], xlim,\n                                [ylim[0], tree.threshold[i]])\n                plot_boundaries(tree.children_right[i], xlim,\n                                [tree.threshold[i], ylim[1]])\n            \n    if boundaries:\n        plot_boundaries(0, xlim, ylim)\n\n\ndef plot_tree_interactive(X, y):\n    def interactive_tree(depth=5):\n        clf = DecisionTreeClassifier(max_depth=depth, random_state=0)\n        visualize_tree(clf, X, y)\n\n    return interact(interactive_tree, depth=[1, 5])\n\n\ndef randomized_tree_interactive(X, y):\n    N = int(0.75 * X.shape[0])\n    \n    xlim = (X[:, 0].min(), X[:, 0].max())\n    ylim = (X[:, 1].min(), X[:, 1].max())\n    \n    def fit_randomized_tree(random_state=0):\n        clf = DecisionTreeClassifier(max_depth=15)\n        i = np.arange(len(y))\n        rng = np.random.RandomState(random_state)\n        rng.shuffle(i)\n        visualize_tree(clf, X[i[:N]], y[i[:N]], boundaries=False,\n                       xlim=xlim, ylim=ylim)\n    \n    interact(fit_randomized_tree, random_state=[0, 100]);"""
