file_path,api_count,code
1. Simple Linear Regression/SLR_from_scratch.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nPython implementation of Simple Linear Regression from Scratch.\nFor explaination see the Jupyter Notebook.\n\n@author: Batakrishna\n\nRef:\n[1] https://machinelearningmastery.com/implement-simple-linear-regression-scratch-python/    \n""""""\n\nfrom math import sqrt\n\n\n# Calculate the mean of list of values \ndef mean(values):\n    return sum(values) / float(len(values))\n\n# Calculate the variance \ndef variance(values, mean):\n    return sum([(x-mean)**2 for x in values])\n\n# Calculate covariance between X and y\ndef covariance(X, mean_x, y, mean_y):\n    covar = 0.0\n    for i in range(len(X)):\n        covar += (X[i] - mean_x) * (y[i] - mean_y)\n    return covar\n\n# Estimate Coefficients\ndef coefficients(dataset):\n    X = [row[0] for row in dataset]\n    y = [row[1] for row in dataset]\n    X_mean, y_mean = mean(X), mean(y)\n    b1 = covariance(X, X_mean, y, y_mean) / variance(X, X_mean)\n    b0 = y_mean - b1 * X_mean\n    return [b0, b1]\n\n# Calculate RMSE\ndef rmse_metric(actual, predicted):\n    sum_error = 0.0 \n    for i in range(len(actual)):\n        prediction_error = predicted[i] - actual[i]\n        sum_error += (prediction_error ** 2)\n    mean_error = sum_error / float(len(actual))\n    return sqrt(mean_error)\n\n# Simple linear regression algorithm\ndef simple_linear_regression(train, test):\n    predictions = list()\n    b0, b1 = coefficients(train)\n    for row in test:\n        yhat = b0 + b1 * row[0]\n        predictions.append(yhat)\n    return predictions    \n\n# Evaluate regression algo on training dataset\ndef evaluate_algorithm(dataset, algorithm):\n    test_set = list()\n    for row in dataset:\n        row_copy = list(row)\n        row_copy[-1] = None\n        test_set.append(row_copy)   # remove the y and keep only x\n    predicted = algorithm(dataset, test_set)\n    print(predicted)\n    actual = [row[-1] for row in dataset]\n    rmse = rmse_metric(actual, predicted)\n    return rmse   \n\n# Test simple linear regression\ndataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\nrmse = evaluate_algorithm(dataset, simple_linear_regression)\nprint(\'RMSE: %.3f\' % (rmse))'"
1. Simple Linear Regression/SLR_using_statsmodels .py,0,"b'# -*- coding: utf-8 -*-\n""""""\nPython implementation of Simple Linear Regression using statsmodels.\nFor explaination see the Jupyter Notebook.\n\n@author: Batakrishna\n""""""\nimport warnings;warnings.filterwarnings(\'ignore\')\n\nimport statsmodels.api as sm\n\ndataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n\n# Input\nX = [row[0] for row in dataset]\n\n# Output\ny = [row[1] for row in dataset]\n\n# Add b_0\nX = sm.add_constant(X)\n\n# Fitting model\nmodel = sm.OLS(y, X).fit()\n\n# Prediction\npredictions = model.predict(X) \n\n# Print out the statistics\nprint(model.summary())\n\n'"
1. Simple Linear Regression/SRL_using_sklearn.py,2,"b'# -*- coding: utf-8 -*-\n""""""\nPython implementation of Simple Linear Regression using sklearn.\nFor explaination see the Jupyter Notebook.\n\n@author: Batakrishna\n""""""\nimport warnings;warnings.filterwarnings(\'ignore\')\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\ndataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n\n# Input\nX = np.asarray([row[0] for row in dataset]).reshape(-1, 1)\n\n# Output\ny = np.asarray([row[1] for row in dataset]).reshape(-1, 1)\n\n# define classifier\nregr = linear_model.LinearRegression()\n\n# Fit the model\nregr.fit(X, y)\n\n# Prediction\npredictions = regr.predict(X) \n\n# The coefficients\nprint(\'Coefficients: \\n\', regr.coef_)\n\n# The mean squared error\nprint(""Mean squared error: %.2f""\n      % mean_squared_error(y, predictions))\n\n# Explained variance score: 1 is perfect prediction\nprint(\'Variance score: %.2f\' % r2_score(y, predictions))'"
100. RL using table lookup Q-learning method/treasure_on_right.py,4,"b'""""""\nA simple example for Reinforcement Learning using table lookup Q-learning method.\nAn agent ""o"" is on the left of a 1 dimensional world, the treasure is on the rightmost location.\nRun this program and to see how the agent will improve its strategy of finding the treasure.\n""""""\n\nimport numpy as np\nimport pandas as pd\nimport time\n\nnp.random.seed(2)  # reproducible\n\n\nN_STATES = 6   # the length of the 1 dimensional world\nACTIONS = [\'left\', \'right\']     # available actions\nEPSILON = 0.9   # greedy police\nALPHA = 0.1     # learning rate\nGAMMA = 0.9    # discount factor\nMAX_EPISODES = 13   # maximum episodes\nFRESH_TIME = 0.3    # fresh time for one move\n\n\ndef build_q_table(n_states, actions):\n    table = pd.DataFrame(\n        np.zeros((n_states, len(actions))),     # q_table initial values\n        columns=actions,    # actions\'s name\n    )\n    # print(table)    # show table\n    return table\n\n\ndef choose_action(state, q_table):\n    # This is how to choose an action\n    state_actions = q_table.iloc[state, :]\n    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):  # act non-greedy or state-action have no value\n        action_name = np.random.choice(ACTIONS)\n    else:   # act greedy\n        action_name = state_actions.idxmax()    # replace argmax to idxmax as argmax means a different function in newer version of pandas\n    return action_name\n\n\ndef get_env_feedback(S, A):\n    # This is how agent will interact with the environment\n    if A == \'right\':    # move right\n        if S == N_STATES - 2:   # terminate\n            S_ = \'terminal\'\n            R = 1\n        else:\n            S_ = S + 1\n            R = 0\n    else:   # move left\n        R = 0\n        if S == 0:\n            S_ = S  # reach the wall\n        else:\n            S_ = S - 1\n    return S_, R\n\n\ndef update_env(S, episode, step_counter):\n    # This is how environment be updated\n    env_list = [\'-\']*(N_STATES-1) + [\'T\']   # \'---------T\' our environment\n    if S == \'terminal\':\n        interaction = \'Episode %s: total_steps = %s\' % (episode+1, step_counter)\n        print(\'\\r{}\'.format(interaction), end=\'\')\n        time.sleep(2)\n        print(\'\\r                                \', end=\'\')\n    else:\n        env_list[S] = \'o\'\n        interaction = \'\'.join(env_list)\n        print(\'\\r{}\'.format(interaction), end=\'\')\n        time.sleep(FRESH_TIME)\n\n\ndef rl():\n    # main part of RL loop\n    q_table = build_q_table(N_STATES, ACTIONS)\n    for episode in range(MAX_EPISODES):\n        step_counter = 0\n        S = 0\n        is_terminated = False\n        update_env(S, episode, step_counter)\n        while not is_terminated:\n\n            A = choose_action(S, q_table)\n            S_, R = get_env_feedback(S, A)  # take action & get next state and reward\n            q_predict = q_table.loc[S, A]\n            if S_ != \'terminal\':\n                q_target = R + GAMMA * q_table.iloc[S_, :].max()   # next state is not terminal\n            else:\n                q_target = R     # next state is terminal\n                is_terminated = True    # terminate this episode\n\n            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # update\n            S = S_  # move to next state\n\n            update_env(S, episode, step_counter+1)\n            step_counter += 1\n    return q_table\n\n\nif __name__ == ""__main__"":\n    q_table = rl()\n    print(\'\\r\\nQ-table:\\n\')\n    print(q_table)\n'"
2. Features Selection For Multiple Linear Regression/LR_on_Boston_Houses.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nFollowing is a example of multiple regression in Boston Houses dataset.\n\n@author: Batakrishna\n""""""\nprint(__doc__)\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\n# load the dataset\nboston = load_boston()\n\n# describe the dataset\n# print(boston.DESCR)\n\n# load the features and label\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = pd.DataFrame(boston.target, columns=[\'MEDV\'])\n\n# Split into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n\n# define the classifier\nclf = LinearRegression()\n\n# fit the model\nclf.fit(X_train, y_train)\n\n# predict for testing set\ny_pred = clf.predict(X_test)\n\n# print the confidence score\nprint(clf.score(X_test, y_test))\n\n# print the first 5 predicted values\nprint(y_pred[0:5])'"
3. Linear Regression with Gradient Descent/gradient_descent_on_insurace_data.py,0,"b""import pandas as pd\n\ndef linear_regression(X, y, m_current=0, b_current=0, epochs=1000, learning_rate=0.0001):\n     N = float(len(y))\n     costs = []\n     for i in range(epochs):\n          y_current = (m_current * X) + b_current\n          cost = sum([data**2 for data in (y-y_current)]) / N\n          costs.append(cost)\n          m_gradient = -(2/N) * sum(X * (y - y_current))\n          b_gradient = -(2/N) * sum(y - y_current)\n          m_current = m_current - (learning_rate * m_gradient)\n          b_current = b_current - (learning_rate * b_gradient)\n     return m_current, b_current, costs\n \ndf = pd.read_csv('Insurance.csv')\n\n\nm_current, b_current, costs = linear_regression(df['X'], df['Y'])\n"""
3. Linear Regression with Gradient Descent/gradient_descent_on_student.py,7,"b""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (20.0, 10.0)\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndata = pd.read_csv('student.csv')\ndata.head()\n \nmath =  data['Math'].values\nread = data['Reading'].values\nwrite = data['Writing'].values\n\n# Plotting the scores\n#fig = plt.figure(figsize=(20,10))\n#ax = Axes3D(fig)\n#ax.scatter(math, read, write, color='red')\n#plt.show()\n\n# generate X, Y and B\nm = len(math)\nx0 = np.ones(m)\nX = np.array([x0, math, read]).T\n# Initial Coefficients\nB = np.array([0, 0, 0])\nY = np.array(write)\nalpha = 0.0001\n\ndef cost_function(X, Y, B):\n    m = len(Y)\n    J = np.sum((X.dot(B) - Y) ** 2)/(2 * m)\n    return J\n\ninital_cost = cost_function(X, Y, B)\nprint(inital_cost)\n\ndef gradient_descent(X, Y, B, alpha, iterations):\n    cost_history = [0] * iterations\n    m = len(Y)\n\n    for iteration in range(iterations):\n        # Hypothesis Values\n        h = X.dot(B)\n        # Difference b/w Hypothesis and Actual Y\n        loss = h - Y\n        # Gradient Calculation\n        gradient = X.T.dot(loss) / m\n        # Changing Values of B using Gradient\n        B = B - alpha * gradient\n        # New Cost Value\n        cost = cost_function(X, Y, B)\n        cost_history[iteration] = cost\n        \n    return B, cost_history\n\n# 100000 Iterations\nnewB, cost_history = gradient_descent(X, Y, B, alpha, 100000)\n\n# New Values of B\nprint(newB)\n\n# Final Cost of new B\nprint(cost_history[-1])\n\n# Model Evaluation - RMSE\ndef rmse(Y, Y_pred):\n    rmse = np.sqrt(sum((Y - Y_pred) ** 2) / len(Y))\n    return rmse\n\n# Model Evaluation - R2 Score\ndef r2_score(Y, Y_pred):\n    mean_y = np.mean(Y)\n    ss_tot = sum((Y - mean_y) ** 2)\n    ss_res = sum((Y - Y_pred) ** 2)\n    r2 = 1 - (ss_res / ss_tot)\n    return r2\n\nY_pred = X.dot(newB)\n\nprint(rmse(Y, Y_pred))\nprint(r2_score(Y, Y_pred))    """
36. Classifying Text using Multinomial Naive Bayes /playground.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Aug 12 21:55:15 2018\n\n@author: bksahu\n""""""\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\n\ndata = fetch_20newsgroups()\n\n\ncategories = [\'alt.atheism\',\n \'comp.graphics\',\n \'comp.os.ms-windows.misc\',\n \'comp.sys.ibm.pc.hardware\',\n \'comp.sys.mac.hardware\',\n \'comp.windows.x\',\n \'misc.forsale\',\n \'rec.autos\',\n \'rec.motorcycles\',\n \'rec.sport.baseball\',\n \'rec.sport.hockey\',\n \'sci.crypt\',\n \'sci.electronics\',\n \'sci.med\',\n \'sci.space\',\n \'talk.politics.guns\',\n \'talk.politics.mideast\',\n \'talk.politics.misc\',\n \'talk.religion.misc\']\n\ntrain = fetch_20newsgroups(subset=\'train\', categories=categories)\ntest = fetch_20newsgroups(subset=\'test\', categories=categories)\n\nvec = TfidfVectorizer()\nX = vec.fit_transform(train.data, train.target)\nprint(pd.DataFrame(X.toarray(), columns=vec.get_feature_names()))\n'"
39. In-Depth - Decision Trees/helpers_05_08.py,7,"b""\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom ipywidgets import interact\n\n\ndef visualize_tree(estimator, X, y, boundaries=True,\n                   xlim=None, ylim=None, ax=None):\n    ax = ax or plt.gca()\n    \n    # Plot the training points\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='viridis',\n               clim=(y.min(), y.max()), zorder=3)\n    ax.axis('tight')\n    ax.axis('off')\n    if xlim is None:\n        xlim = ax.get_xlim()\n    if ylim is None:\n        ylim = ax.get_ylim()\n    \n    # fit the estimator\n    estimator.fit(X, y)\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    n_classes = len(np.unique(y))\n    Z = Z.reshape(xx.shape)\n    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n                           levels=np.arange(n_classes + 1) - 0.5,\n                           cmap='viridis', clim=(y.min(), y.max()),\n                           zorder=1)\n\n    ax.set(xlim=xlim, ylim=ylim)\n    \n    # Plot the decision boundaries\n    def plot_boundaries(i, xlim, ylim):\n        if i >= 0:\n            tree = estimator.tree_\n        \n            if tree.feature[i] == 0:\n                ax.plot([tree.threshold[i], tree.threshold[i]], ylim, '-k', zorder=2)\n                plot_boundaries(tree.children_left[i],\n                                [xlim[0], tree.threshold[i]], ylim)\n                plot_boundaries(tree.children_right[i],\n                                [tree.threshold[i], xlim[1]], ylim)\n        \n            elif tree.feature[i] == 1:\n                ax.plot(xlim, [tree.threshold[i], tree.threshold[i]], '-k', zorder=2)\n                plot_boundaries(tree.children_left[i], xlim,\n                                [ylim[0], tree.threshold[i]])\n                plot_boundaries(tree.children_right[i], xlim,\n                                [tree.threshold[i], ylim[1]])\n            \n    if boundaries:\n        plot_boundaries(0, xlim, ylim)\n\n\ndef plot_tree_interactive(X, y):\n    def interactive_tree(depth=5):\n        clf = DecisionTreeClassifier(max_depth=depth, random_state=0)\n        visualize_tree(clf, X, y)\n\n    return interact(interactive_tree, depth=[1, 5])\n\n\ndef randomized_tree_interactive(X, y):\n    N = int(0.75 * X.shape[0])\n    \n    xlim = (X[:, 0].min(), X[:, 0].max())\n    ylim = (X[:, 1].min(), X[:, 1].max())\n    \n    def fit_randomized_tree(random_state=0):\n        clf = DecisionTreeClassifier(max_depth=15)\n        i = np.arange(len(y))\n        rng = np.random.RandomState(random_state)\n        rng.shuffle(i)\n        visualize_tree(clf, X[i[:N]], y[i[:N]], boundaries=False,\n                       xlim=xlim, ylim=ylim)\n    \n    interact(fit_randomized_tree, random_state=[0, 100]);"""
5. k-Nearest Neighbors/kNN_from_scratch.py,2,"b'# -*- coding: utf-8 -*-\n""""""\nImplementation of k-NN in Python from scratch\n\n@author: Batakrishna\nRef: \n[1] https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/ \n""""""\n\nimport pandas as pd\nimport numpy as np\nimport operator\n\ndata = pd.read_csv(\'iris.csv\')\n\ndef euclideanDistance(data1, data2, length):\n    distance = 0\n    for x in range(length):\n        distance += np.square(data1[x] - data2[x])\n        \n    return np.sqrt(distance) \n\ndef knn(trainingSet, testInstance, k):\n    distances = {}\n    \n    length = testInstance.shape[1]\n    \n    for x in range(len(trainingSet)):\n        dist = euclideanDistance(testInstance, trainingSet.iloc[x], length)\n        distances[x] = dist[0]\n        \n    sorted_d = sorted(distances.items(), key=operator.itemgetter(1))    \n    \n    neighbors = []\n    \n    for x in range(k):\n        neighbors.append(sorted_d[x][0])\n        \n    classVotes = {}    \n\n    for x in range(len(neighbors)):\n        response = trainingSet.iloc[neighbors[x]][-1]\n        \n        if response in classVotes:\n            classVotes[response] += 1\n        else:\n            classVotes[response] = 1\n\n    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n    \n    return(sortedVotes[0][0], neighbors)\n\n\n    \ntestSet = [[7.2, 3.6, 5.1, 2.5]]\ntest = pd.DataFrame(testSet)\nk = 1\nresult, neigh = knn(data, test, k)\nprint(result)\nprint(neigh)'"
5. k-Nearest Neighbors/kNN_using_sklearn.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nImplementation of k-NN in Python using sklearn\n\n@author: Batakrishna\nRef: \n[1] https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/ \n""""""\n\nfrom sklearn.neighbors import KNeighborsClassifier\nimport pandas as pd\n\ndata = pd.read_csv(\'iris.csv\')\ntestSet = [[7.2, 3.6, 5.1, 2.5]]\ntest = pd.DataFrame(testSet)\nk = 1\n\nneigh = KNeighborsClassifier(n_neighbors=k)\nneigh.fit(data.iloc[:,0:4], data[\'Name\'])\n\nprint(neigh.predict(test))\n\nprint(neigh.kneighbors(test)[1])\n'"
59. Using keras pretrained model/script.py,3,"b""import keras\nimport numpy as np\nfrom keras.applications import vgg16, inception_v3, resnet50, mobilenet\n\n#Load the VGG model\nvgg_model = vgg16.VGG16(weights='imagenet')\n\n#Load the ResNet50 model\nresnet_model = resnet50.ResNet50(weights='imagenet')\n\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.imagenet_utils import decode_predictions\nimport matplotlib.pyplot as plt\n\nfilename = 'images/cat.jpg'\n# load an image in PIL format\noriginal = load_img(filename, target_size=(224, 224))\nprint('PIL image size',original.size)\nplt.imshow(original)\nplt.show()\n\n# convert the PIL image to a numpy array\n# IN PIL - image is in (width, height, channel)\n# In Numpy - image is in (height, width, channel)\nnumpy_image = img_to_array(original)\nplt.imshow(np.uint8(numpy_image))\nplt.show()\nprint('numpy array size',numpy_image.shape)\n\n# Convert the image / images into batch format\n# expand_dims will add an extra dimension to the data at a particular axis\n# We want the input matrix to the network to be of the form (batchsize, height, width, channels)\n# Thus we add the extra dimension to the axis 0.\nimage_batch = np.expand_dims(numpy_image, axis=0)\nprint('image batch size', image_batch.shape)\nplt.imshow(np.uint8(image_batch[0]))\n\n# prepare the image for the VGG model\nprocessed_image = vgg16.preprocess_input(image_batch.copy())\n \n# get the predicted probabilities for each class\npredictions = resnet_model.predict(processed_image)\n# print predictions\n \n# convert the probabilities to class labels\n# We will get top 5 predictions which is the default\nlabel = decode_predictions(predictions)\nprint label\n\n\n"""
6. Random Forest/randomForest.py,7,"b""import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n# Import tools needed for visualization\nfrom sklearn.tree import export_graphviz\nimport pydot\n\nfeatures = pd.read_csv('temps.csv')\n\nfeatures = pd.get_dummies(features)\n\nlabels = np.array(features['actual'])\n\nfeatures = features.drop(['actual', 'forecast_noaa', 'forecast_acc', 'forecast_under'], axis = 1)\n\nfeature_list = list(features.columns)\n\nfeatures = np.array(features) \n\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n\n\n# The baseline predictions are the historical averages\nbaseline_preds = test_features[:, feature_list.index('average')]\n\n# Baseline errors, and display average baseline error\nbaseline_errors = abs(baseline_preds - test_labels)\n\nprint('Average baseline error: ', round(np.mean(baseline_errors), 2))\n\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n\n# Train the model on training data\nrf.fit(train_features, train_labels)\n\npredictions = rf.predict(test_features)\n\n# Calculate the absolute errors\nerrors = abs(predictions - test_labels)\n\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors / test_labels)\n\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')\n\n\n# Pull out one tree from the forest\ntree = rf.estimators_[5]\n\n# Export the image to a dot file\nexport_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n\n\n## Use dot file to create a graph\n#(graph, ) = pydot.graph_from_dot_file('tree.dot')\n#\n## Write graph to a png file\n#graph.write_png('tree.png')\n\n\n# Get numerical feature importances\nimportances = list(rf.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n\n\n# New random forest with only the two most important variables\nrf_most_important = RandomForestRegressor(n_estimators= 1000, random_state=42)\n\n# Extract the two most important features\nimportant_indices = [feature_list.index('temp_1'), feature_list.index('average')]\ntrain_important = train_features[:, important_indices]\ntest_important = test_features[:, important_indices]\n\n# Train the random forest\nrf_most_important.fit(train_important, train_labels)\n\n# Make predictions and determine the error\npredictions = rf_most_important.predict(test_important)\n\nerrors = abs(predictions - test_labels)\n\n# Display the performance metrics\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n\nmape = np.mean(100 * (errors / test_labels))\naccuracy = 100 - mape\n\nprint('Accuracy:', round(accuracy, 2), '%.')\n\nimport matplotlib.pyplot as plt\n\n\n# Set the style\nplt.style.use('fivethirtyeight')\n\n# list of x locations for plotting\nx_values = list(range(len(importances)))\n\n# Make a bar chart\nplt.bar(x_values, importances, orientation = 'vertical')\n\n# Tick labels for x axis\nplt.xticks(x_values, feature_list, rotation='vertical')\n\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"""
60. Deploying Keras Deep Learning Models with Flask/run_keras_model.py,0,"b'# Load libraries\nimport flask\nimport pandas as pd\nimport tensorflow as tf\nimport keras\nfrom keras.models import load_model\n\n# instantiate flask\napp = flask.Flask(__name__)\n\n# we need to redefine our metric function in order\n# to use it when loading the model\n\n\ndef auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    keras.backend.get_session().run(tf.local_variables_initializer())\n    return auc\n\n# load the model, and pass in the custom metric function\nglobal graph\ngraph = tf.get_default_graph()\nmodel = load_model(\'games.h5\', custom_objects={\'auc\': auc})\n\n# define a predict function as an endpoint\n@app.route(""/predict"", methods=[""GET"",""POST""])\ndef predict():\n    data = {""success"": False}\n\n    params = flask.request.json\n    if (params == None):\n        params = flask.request.args\n\n    # if parameters are found, return a prediction\n    if (params != None):\n        x=pd.DataFrame.from_dict(params, orient=\'index\').transpose()\n        with graph.as_default():\n            data[""prediction""] = str(model.predict(x)[0][0])\n            data[""success""] = True\n\n    # return a response in json format\n    return flask.jsonify(data)\n\n# start the flask app, allow remote connections\napp.run()\n\n\'\'\'Tests\n1. Using browser\n  ---------------\nGoto http://127.0.0.1:5000/predict?g1=1&g2=0&g3=0&g4=0&g5=0&g6=0&g7=0&g8=0&g9=0&g10=0\n2. Using Curl\n  ------------\n$ curl -X POST -H ""Content-Type: application/json"" -d ""{ \\""g1\\"": 1, \\""g2\\"": 0, \\""g3\\"": 0, \\""g4\\"": 0, \\""g5\\"": 0, \\""g6\\"": 0, \\""g7\\"": 0, \\""g8\\"": 0, \\""g9\\"": 0, \\""g10\\"": 0 }"" http://127.0.0.1:5000/predict\n\'\'\'\n'"
64. Training convnets from scratch on a small dataset/cnn_on_dogs_vs_cats.py,0,"b'import keras\nimport os, shutil\nfrom keras import optimizers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\nimport matplotlib.pyplot as plt\nfrom keras import models\nfrom keras import layers\n\n# Change dir\nos.chdir(\'../../Downloads\')\n\n# The path to the directory where the original\n# dataset was uncompressed\noriginal_dataset_dir = \'kaggle_original_data\'\n\n# The directory where we will\n# store our smaller dataset\nbase_dir = \'cats_and_dogs_small\'\nos.mkdir(base_dir)\n\n# Directories for our training,\n# validation and test splits\ntrain_dir = os.path.join(base_dir, \'train\')\nos.mkdir(train_dir)\nvalidation_dir = os.path.join(base_dir, \'validation\')\nos.mkdir(validation_dir)\ntest_dir = os.path.join(base_dir, \'test\')\nos.mkdir(test_dir)\n\n# Directory with our training cat pictures\ntrain_cats_dir = os.path.join(train_dir, \'cats\')\nos.mkdir(train_cats_dir)\n\n# Directory with our training dog pictures\ntrain_dogs_dir = os.path.join(train_dir, \'dogs\')\nos.mkdir(train_dogs_dir)\n\n# Directory with our validation cat pictures\nvalidation_cats_dir = os.path.join(validation_dir, \'cats\')\nos.mkdir(validation_cats_dir)\n\n# Directory with our validation dog pictures\nvalidation_dogs_dir = os.path.join(validation_dir, \'dogs\')\nos.mkdir(validation_dogs_dir)\n\n# Directory with our validation cat pictures\ntest_cats_dir = os.path.join(test_dir, \'cats\')\nos.mkdir(test_cats_dir)\n\n# Directory with our validation dog pictures\ntest_dogs_dir = os.path.join(test_dir, \'dogs\')\nos.mkdir(test_dogs_dir)\n\n# Copy first 1000 cat images to train_cats_dir\nfnames = [\'cat.{}.jpg\'.format(i) for i in range(1000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_cats_dir, fname)\n    shutil.copyfile(src, dst)\n\n# Copy next 500 cat images to validation_cats_dir\nfnames = [\'cat.{}.jpg\'.format(i) for i in range(1000, 1500)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_cats_dir, fname)\n    shutil.copyfile(src, dst)\n\n# Copy next 500 cat images to test_cats_dir\nfnames = [\'cat.{}.jpg\'.format(i) for i in range(1500, 2000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(test_cats_dir, fname)\n    shutil.copyfile(src, dst)\n\n# Copy first 1000 dog images to train_dogs_dir\nfnames = [\'dog.{}.jpg\'.format(i) for i in range(1000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_dogs_dir, fname)\n    shutil.copyfile(src, dst)\n\n# Copy next 500 dog images to validation_dogs_dir\nfnames = [\'dog.{}.jpg\'.format(i) for i in range(1000, 1500)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_dogs_dir, fname)\n    shutil.copyfile(src, dst)\n\n# Copy next 500 dog images to test_dogs_dir\nfnames = [\'dog.{}.jpg\'.format(i) for i in range(1500, 2000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(test_dogs_dir, fname)\n    shutil.copyfile(src, dst)\n\nprint(\'total training cat images:\', len(os.listdir(train_cats_dir)))\nprint(\'total training dog images:\', len(os.listdir(train_dogs_dir)))\nprint(\'total validation cat images:\', len(os.listdir(validation_cats_dir)))\nprint(\'total validation dog images:\', len(os.listdir(validation_dogs_dir)))\nprint(\'total test cat images:\', len(os.listdir(test_cats_dir)))\nprint(\'total test dog images:\', len(os.listdir(test_dogs_dir)))\n\n# Data augmentation\ndatagen = ImageDataGenerator(\n      rotation_range=40,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      shear_range=0.2,\n      zoom_range=0.2,\n      horizontal_flip=True,\n      fill_mode=\'nearest\')\n\nfnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n\n# We pick one image to ""augment""\nimg_path = fnames[3]\n\n# Read the image and resize it\nimg = image.load_img(img_path, target_size=(150, 150))\n\n# Convert it to a Numpy array with shape (150, 150, 3)\nx = image.img_to_array(img)\n\n# Reshape it to (1, 150, 150, 3)\nx = x.reshape((1,) + x.shape)\n\n# The .flow() command below generates batches of randomly transformed images.\n# It will loop indefinitely, so we need to `break` the loop at some point!\ni = 0\nfor batch in datagen.flow(x, batch_size=1):\n    plt.figure(i)\n    imgplot = plt.imshow(image.array_to_img(batch[0]))\n    i += 1\n    if i % 4 == 0:\n        break\n\nplt.show()\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation=\'relu\',\n                        input_shape=(150, 150, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation=\'relu\'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation=\'relu\'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation=\'relu\'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(512, activation=\'relu\'))\nmodel.add(layers.Dense(1, activation=\'sigmoid\'))\n\nmodel.compile(loss=\'binary_crossentropy\',\n              optimizer=optimizers.RMSprop(lr=1e-4),\n              metrics=[\'acc\'])\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,)\n\n# Note that the validation data should not be augmented!\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        # This is the target directory\n        train_dir,\n        # All images will be resized to 150x150\n        target_size=(150, 150),\n        batch_size=32,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode=\'binary\')\n\nvalidation_generator = test_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(150, 150),\n        batch_size=32,\n        class_mode=\'binary\')\n\nhistory = model.fit_generator(\n      train_generator,\n      steps_per_epoch=100,\n      epochs=100,\n      validation_data=validation_generator,\n      validation_steps=50)\n\nmodel.save(\'cats_and_dogs_small.h5\')\n\nacc = history.history[\'acc\']\nval_acc = history.history[\'val_acc\']\nloss = history.history[\'loss\']\nval_loss = history.history[\'val_loss\']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, \'bo\', label=\'Training acc\')\nplt.plot(epochs, val_acc, \'b\', label=\'Validation acc\')\nplt.title(\'Training and validation accuracy\')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, \'bo\', label=\'Training loss\')\nplt.plot(epochs, val_loss, \'b\', label=\'Validation loss\')\nplt.title(\'Training and validation loss\')\nplt.legend()\n\nplt.show()\n'"
66. Feature extracting with data argumentation/script.py,5,"b""import keras \nkeras.__version__\n\nfrom keras.applications import VGG16\n\nconv_base = VGG16(weights='imagenet',\n                  include_top=False,\n                  input_shape=(150, 150, 3))\n\nimport os\nimport numpy as np\nfrom keras.preprocessing.image import ImageDataGenerator\n\nos.chdir('../../Downloads')\nbase_dir = 'cats_and_dogs_small'\n\ntrain_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\ndatagen = ImageDataGenerator(rescale=1./255)\nbatch_size = 20\n\ndef extract_features(directory, sample_count):\n    features = np.zeros(shape=(sample_count, 4, 4, 512))\n    labels = np.zeros(shape=(sample_count))\n    generator = datagen.flow_from_directory(\n        directory,\n        target_size=(150, 150),\n        batch_size=batch_size,\n        class_mode='binary')\n    i = 0\n    for inputs_batch, labels_batch in generator:\n        features_batch = conv_base.predict(inputs_batch)\n        features[i * batch_size : (i + 1) * batch_size] = features_batch\n        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n        i += 1\n        if i * batch_size >= sample_count:\n            # Note that since generators yield data indefinitely in a loop,\n            # we must `break` after every image has been seen once.\n            break\n    return features, labels\n\ntrain_features, train_labels = extract_features(train_dir, 200)\nvalidation_features, validation_labels = extract_features(validation_dir, 100)\ntest_features, test_labels = extract_features(test_dir, 100)\n\n\ntrain_features = np.reshape(train_features, (200, 4 * 4 * 512))\nvalidation_features = np.reshape(validation_features, (100, 4 * 4 * 512))\ntest_features = np.reshape(test_features, (100, 4 * 4 * 512))\n\n\nfrom keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(conv_base)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()\n\n\nprint('This is the number of trainable weights '\n      'before freezing the conv base:', len(model.trainable_weights))\n\n\nconv_base.trainable = False\n\nprint('This is the number of trainable weights '\n      'after freezing the conv base:', len(model.trainable_weights))\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(\n      rescale=1./255,\n      rotation_range=40,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      shear_range=0.2,\n      zoom_range=0.2,\n      horizontal_flip=True,\n      fill_mode='nearest')\n\n# Note that the validation data should not be augmented!\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        # This is the target directory\n        train_dir,\n        # All images will be resized to 150x150\n        target_size=(150, 150),\n        batch_size=20,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(150, 150),\n        batch_size=20,\n        class_mode='binary')\n\n\nfrom keras import optimizers\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizers.RMSprop(lr=2e-5),\n              metrics=['acc'])\n\n\nhistory = model.fit_generator(\n      train_generator,\n      steps_per_epoch=100,\n      epochs=30,\n      validation_data=validation_generator,\n      validation_steps=50,\n      verbose=2)\n\n\nmodel.save('cats_and_dogs_small.h5')\n\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n"""
67. Fine tuning using pretrained convnet/script.py,2,"b""from keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.applications import VGG16\n\nconv_base = VGG16(weights='imagenet',\n                  include_top=False,\n                  input_shape=(150, 150, 3))\nimport os\nimport numpy as np\nfrom keras.preprocessing.image import ImageDataGenerator\n\nos.chdir('../../Downloads')\nbase_dir = 'cats_and_dogs_small'\n\ntrain_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\ndatagen = ImageDataGenerator(rescale=1./255)\nbatch_size = 20\n\ndef extract_features(directory, sample_count):\n    features = np.zeros(shape=(sample_count, 4, 4, 512))\n    labels = np.zeros(shape=(sample_count))\n    generator = datagen.flow_from_directory(\n        directory,\n        target_size=(150, 150),\n        batch_size=batch_size,\n        class_mode='binary')\n    i = 0\n    for inputs_batch, labels_batch in generator:\n        features_batch = conv_base.predict(inputs_batch)\n        features[i * batch_size : (i + 1) * batch_size] = features_batch\n        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n        i += 1\n        if i * batch_size >= sample_count:\n            # Note that since generators yield data indefinitely in a loop,\n            # we must `break` after every image has been seen once.\n            break\n    return features, labels\n\ntrain_features, train_labels = extract_features(train_dir, 2000)\nvalidation_features, validation_labels = extract_features(validation_dir, 1000)\ntest_features, test_labels = extract_features(test_dir, 1000)\n\nmodel = models.Sequential()\nmodel.add(conv_base)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(\n      rescale=1./255,\n      rotation_range=40,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      shear_range=0.2,\n      zoom_range=0.2,\n      horizontal_flip=True,\n      fill_mode='nearest')\n\n# Note that the validation data should not be augmented!\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        # This is the target directory\n        train_dir,\n        # All images will be resized to 150x150\n        target_size=(150, 150),\n        batch_size=20,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(150, 150),\n        batch_size=20,\n        class_mode='binary')\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizers.RMSprop(lr=2e-5),\n              metrics=['acc'])\n\nconv_base.trainable = True\n\nset_trainable = False\nfor layer in conv_base.layers:\n    if layer.name == 'block5_conv1':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizers.RMSprop(lr=1e-5),\n              metrics=['acc'])\n\nhistory = model.fit_generator(\n      train_generator,\n      steps_per_epoch=100,\n      epochs=3,\n      validation_data=validation_generator,\n      validation_steps=50)\n\nmodel.save('cats_and_dogs_small_4.h5')\n\ntest_generator = test_datagen.flow_from_directory(\n        test_dir,\n        target_size=(150, 150),\n        batch_size=20,\n        class_mode='binary')\n\ntest_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\nprint('test acc:', test_acc)\n"""
71. Deep learning for text and sequences/char_level_one_hot_encoding.py,1,"b""import string\nimport numpy as np\n\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\ncharacters = string.printable\ntoken_index = dict(zip(range(1, len(characters) + 1), characters))\n\nmax_length = 50\nresults = np.zeros((len(samples),\n                    max_length,\n                    max(token_index.keys()) + 1))\nfor i, sample in enumerate(samples):\n    for j, character in enumerate(sample):\n        index = token_index.get(character)\n        results[i, j, index] = 1.\n\nprint(results)\n"""
71. Deep learning for text and sequences/word_level_one_hot_encoding.py,1,"b""import numpy as np\n\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\n\ntoken_index = {}\nfor sample in samples:\n\tfor word in sample.split():\n\t\tif word not in token_index:\n\t\t\ttoken_index[word] = len(token_index) + 1\n\nmax_length = 10\n\nresults = np.zeros(shape=(len(samples),\n\t\t\t\t\t\t\tmax_length,\n\t\t\t\t\t\t\tmax(token_index.values()) + 1))\nfor i, sample in enumerate(samples):\n\tfor j, word in list(enumerate(sample.split()))[:max_length]:\n\t\tindex = token_index.get(word)\n\t\tresults[i, j, index] = 1\n"""
71. Deep learning for text and sequences/word_level_one_hot_encoding_using_keras.py,0,"b""from keras.preprocessing.text import Tokenizer\n\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\n\n# Create a tokenizer to take into account the 1000 most common words\ntokenizer = Tokenizer(num_words=1000)\n# Builds the word index\ntokenizer.fit_on_texts(samples)\n# turn strings into lists of integer indices\nsequences = tokenizer.texts_to_sequences(samples)\n# One-hot encode\none_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n# Recover the word index that was computed\nword_index = tokenizer.word_index\nprint(word_index)\n"""
71. Deep learning for text and sequences/word_level_one_hot_encoding_with_hashing_trick.py,1,"b""import numpy as np\n\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\n# Stores the words as vectors of size 1,000. If you have close\n# to 1,000 words (or more), you\xe2\x80\x99ll see many hash collisions,\n# which will decrease the accuracy of this encoding method.\ndimensionality = 1000\nmax_length = 10\n\nresults = np.zeros((len(samples), max_length, dimensionality))\nfor i, sample in enumerate(samples):\n    for j, word in list(enumerate(sample.split()))[:max_length]:\n        index = abs(hash(word)) % dimensionality\n        results[i, j, index] = 1.\n"""
73. LSTM model on the imdb sentiment/lstm_model_on_the_imdb_sentiment.py,0,"b'# -*- coding: utf-8 -*-\n""""""LSTM model on the IMDB sentiment.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1qtkEu5fcLXScbdqQ38ugyBhiQOrQJacy\n""""""\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM\nfrom keras.datasets import imdb\n\nmax_features = 20000\nmaxlen = 80 # cut text after this number of words\nbatch_size = 32\n\nprint(\'Loading data...\')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), \'train sequences\')\nprint(len(x_test), \'test sequences\')\n\nprint(\'Pad sequences (samples x time)\')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint(\'x_train shape:\', x_train.shape)\nprint(\'x_test shape:\', x_test.shape)\n\nprint(\'Build model...\')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128))\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation=\'sigmoid\'))\n\nmodel.compile(loss=\'binary_crossentropy\',\n              optimizer=\'adam\',\nmetrics=[\'accuracy\'])\n\nprint(\'Train...\')\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=15,\n          validation_data=(x_test, y_test))\nscore, acc = model.evaluate(x_test, y_test,\n                            batch_size=batch_size)\nprint(\'Test score:\', score)\nprint(\'Test accuracy:\', acc)\n'"
75. NN from scratch/script.py,10,"b'import numpy as np\n\ndef sigmoid(x):\n    return 1.0/(1+ np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1.0 - x)\n\nclass NeuralNetwork:\n    def __init__(self, x, y):\n        self.input      = x\n        self.weights1   = np.random.rand(self.input.shape[1],4)\n        self.weights2   = np.random.rand(4,1)\n        self.y          = y\n        self.output     = np.zeros(self.y.shape)\n\n    def feedforward(self):\n        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n\n    def backprop(self):\n        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n\n        # update the weights with the derivative (slope) of the loss function\n        self.weights1 += d_weights1\n        self.weights2 += d_weights2\n\n\nif __name__ == ""__main__"":\n    X = np.array([[0,0,1],\n                  [0,1,1],\n                  [1,0,1],\n                  [1,1,1]])\n    y = np.array([[0],[1],[1],[0]])\n    nn = NeuralNetwork(X,y)\n\n    for i in range(1500):\n        nn.feedforward()\n        nn.backprop()\n\nprint(nn.output)\n'"
77. SimpleRNN on IMDB dataset/LSTM_on_IMDB.py,0,"b""from keras.datasets import imdb\nfrom keras.preprocessing import sequence\n\n# number of words to consider as features\nmax_features = 10000\n# cutoff text after this many words\nmaxlen = 500\nbatch_size = 32\n\nprint('Loading data...')\n(input_train, y_train), (input_test, y_test) = imdb.load_data(\n\t\t\t\t\t\tnum_words=max_features)\nprint(len(input_train), 'train sequence')\nprint(len(input_test), 'test sequence')\nprint('Pad sequences (samples x time)')\ninput_train = sequence.pad_sequences(input_train, maxlen=maxlen)\ninput_test = sequence.pad_sequences(input_test, maxlen=maxlen)\nprint('input_train shape:', input_train.shape)\nprint('input_test shape:', input_test.shape)\n\n# train a simple RNN\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(LSTM(32))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n\t      loss='binary_crossentropy',\n\t      metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n\t\t   epochs=10,\n\t\t   batch_size=128,\n\t\t   validation_split=0.2)\n\nimport matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n"""
77. SimpleRNN on IMDB dataset/naive_rnn_using_numpy.py,7,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nNumpy implementation of the forward pass of the simple RNN.\n""""""\n\nimport numpy as np\n\ntimesteps = 100\ninput_features = 32\noutput_features = 64\n\ninputs = np.random.random((timesteps, input_features))\nstate_t = np.zeros((output_features))\n\nW = np.random.random((output_features, input_features))\nU = np.random.random((output_features, output_features))\nb = np.random.random((output_features))\n\nsuccessive_outputs = []\nfor input_t in inputs:\n    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n    successive_outputs.append(output_t)\n    state_t = output_t\n    \nfinal_output_sequence = np.concatenate(successive_outputs, axis=0)'"
77. SimpleRNN on IMDB dataset/simpleRNN_on_IMDB.py,0,"b""from keras.datasets import imdb\nfrom keras.preprocessing import sequence\n\n# number of words to consider as features\nmax_features = 10000\n# cutoff text after this many words\nmaxlen = 500\nbatch_size = 32\n\nprint('Loading data...')\n(input_train, y_train), (input_test, y_test) = imdb.load_data(\n\t\t\t\t\t\tnum_words=max_features)\nprint(len(input_train), 'train sequence')\nprint(len(input_test), 'test sequence')\nprint('Pad sequences (samples x time)')\ninput_train = sequence.pad_sequences(input_train, maxlen=maxlen)\ninput_test = sequence.pad_sequences(input_test, maxlen=maxlen)\nprint('input_train shape:', input_train.shape)\nprint('input_test shape:', input_test.shape)\n\n# train a simple RNN\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, SimpleRNN\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n\t      loss='binary_crossentropy',\n\t      metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n\t\t   epochs=10,\n\t\t   batch_size=128,\n\t\t   validation_split=0.2)\n\nimport matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n"""
80. 1d convnets on sequential data/1d_convnet.py,0,"b""'''\nFollowing is a 1d convnet implmentations on IMDB dataset.\n'''\n\nfrom keras.datasets import imdb\nfrom keras.preprocessing import sequence\n\nmax_features = 10000\nmax_len = 500\n\nprint('Loading data...')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=max_len)\nx_test = sequence.pad_sequences(x_test, maxlen=max_len)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\n# create 1d convnets\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\n\nmodel = Sequential()\nmodel.add(layers.Embedding(max_features, 128, input_length=max_len))\nmodel.add(layers.Conv1D(32, 7, activation='relu'))\nmodel.add(layers.MaxPooling1D(5))\nmodel.add(layers.Conv1D(32, 7, activation='relu'))\nmodel.add(layers.GlobalMaxPooling1D())\nmodel.add(layers.Dense(1))\n\nmodel.summary()\n\nmodel.compile(optimizer=RMSprop(lr=1e-4),\n\t      loss='binary_crossentropy',\n\t      metrics=['acc'])\nhistory = model.fit(x_train, y_train,\n\t\t   epochs=10,\n\t\t   batch_size=128,\n\t\t   validation_split=0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
82. Neural Network in PyTorch/neural_network_in_pytorch.py,0,"b'# -*- coding: utf-8 -*-\n""""""Neural Network in PyTorch.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/17DDQf1m5LfKFbOgb1xZrzy31NJeVA60H\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        # kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square you can only specify a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n\nnet = Net()\nprint(net)\n\n'"
85. Linear Regression in PyTorch/lr.py,0,"b""import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1)    # reproducible\n\nx = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)\ny = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)\n\nclass Net(torch.nn.Module):\n    def __init__(self, n_feature, n_hidden, n_output):\n        super(Net, self).__init__()\n        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n\n    def forward(self, x):\n        x = F.relu(self.hidden(x))      # activation function for hidden layer\n        x = self.predict(x)             # linear output\n        return x\n\nnet = Net(n_feature=1, n_hidden=10, n_output=1)     # define the network\nprint(net)  # net architecture\n\noptimizer = torch.optim.SGD(net.parameters(), lr=0.2)\nloss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n\nplt.ion()   # something about plotting\n\nfor t in range(200):\n    prediction = net(x)     # input x and predict based on x\n\n    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n\n    optimizer.zero_grad()   # clear gradients for next train\n    loss.backward()         # backpropagation, compute gradients\n    optimizer.step()        # apply gradients\n\n    if t % 5 == 0:\n        # plot and show learning process\n        plt.cla()\n        plt.scatter(x.data.numpy(), y.data.numpy())\n        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n        plt.pause(0.1)\n\nplt.ioff()\nplt.show()\n\n"""
91. DeepDream implementation in keras/deepdream.py,6,"b'# -*- coding: utf-8 -*-\n""""""DeepDream.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1kK0JM4RCAJXZHsZoY4NOgMK0GQw_afGm\n""""""\n\nfrom keras.applications import inception_v3\nfrom keras import backend as K\n\n# We will not be training our model,\n# so we us this command to disable all training-specific operations\nK.set_learning_phase(0)\n\n# Build the InceptionV3 network.\n# The model will be loaded with pre-trained ImageNet weight.\nmodel = inception_v3.InceptionV3(weights=\'imagenet\',\n                                include_top=False)\n\n#model.summary()\n\n# Dict mapping layer names to a coefficient\n# quantifying how much the layer\'s activation\n# will contribute to the loss we will seek to maximize.\n# Note that these are layer names as they appear\n# in the build-in InceptionV3 application.\nlayer_contributions = {\n    \'mixed2\': 0.2,\n    \'mixed3\': 3.,\n    \'mixed4\': 2.,\n    \'mixed5\': 1.5,\n}\n\n""""""Now let\'s define a tensor that contains our loss, i.e. the weighted sum of the L2 norm of the activations of the layers listed above.""""""\n\n# Get the symbolic outputs of each ""key"" layer (we gave them unique names).\nlayer_dict = dict([(layer.name, layer) for layer in model.layers])\n\n# Define the loss\nloss = K.variable(0.)\nfor layer_name in layer_contributions:\n  # Add the L2 norm of the features of a layer to the loss.\n  coeff = layer_contributions[layer_name]\n  activation = layer_dict[layer_name].output\n  \n  # We avoid border artifacts by only involving non-border pixels in the loss.\n  scaling = K.prod(K.cast(K.shape(activation), \'float32\'))\n  loss += coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling\n\n# This holds our generated image\ndream = model.input\n\n# Compute the gradients of the dream with regard to the loss.\ngrads = K.gradients(loss, dream)[0]\n\n# Normalize gradients.\ngrads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n\n# Set up function to retrieve the value\n# of the loss and gradients given an input image.\noutputs = [loss, grads]\nfetch_loss_and_grads = K.function([dream], outputs)\n\ndef eval_loss_and_grads(x):\n    outs = fetch_loss_and_grads([x])\n    loss_value = outs[0]\n    grad_values = outs[1]\n    return loss_value, grad_values\n\ndef gradient_ascent(x, iterations, step, max_loss=None):\n    for i in range(iterations):\n        loss_value, grad_values = eval_loss_and_grads(x)\n        if max_loss is not None and loss_value > max_loss:\n            break\n        print(\'...Loss value at\', i, \':\', loss_value)\n        x += step * grad_values\n    return x\n\nimport scipy\nfrom keras.preprocessing import image\n\ndef resize_img(img, size):\n  img = np.copy(img)\n  factors = (1,\n            float(size[0]) / img.shape[1],\n            float(size[1]) / img.shape[2],\n            1)\n  return scipy.ndimage.zoom(img, factors, order=1)\n\ndef save_img(img, fname):\n  pil_img = deprocess_image(np.copy(img))\n  scipy.misc.imsave(fname, pil_img)\n  \ndef preprocess_image(image_path):\n    # Util function to open, resize and format pictures\n    # into appropriate tensors.\n    img = image.load_img(image_path)\n    img = image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = inception_v3.preprocess_input(img)\n    return img\n\n\ndef deprocess_image(x):\n    # Util function to convert a tensor into a valid image.\n    if K.image_data_format() == \'channels_first\':\n        x = x.reshape((3, x.shape[2], x.shape[3]))\n        x = x.transpose((1, 2, 0))\n    else:\n        x = x.reshape((x.shape[1], x.shape[2], 3))\n    x /= 2.\n    x += 0.5\n    x *= 255.\n    x = np.clip(x, 0, 255).astype(\'uint8\')\n    return x\n\n#from google.colab import files\n\n#uploaded = files.upload()\n\n#for fn in uploaded.keys():\n#  print(\'User uploaded file ""{name}"" with length {length} bytes\'.format(\n#      name=fn, length=len(uploaded[fn])))\n\nimport numpy as np\n\n# Playing with these hyperparameters will also allow you to achieve new effects\n\nstep = 0.01  # Gradient ascent step size\nnum_octave = 15  # Number of scales at which to run gradient ascent\noctave_scale = 1.4  # Size ratio between scales\niterations = 20  # Number of ascent steps per scale\n\n# If our loss gets larger than 10,\n# we will interrupt the gradient ascent process, to avoid ugly artifacts\nmax_loss = 10.\n\n# Fill this to the path to the image you want to use\nbase_image_path = \'IMG_0565-003.JPG\'\n\n# Load the image into a Numpy array\nimg = preprocess_image(base_image_path)\n\n# We prepare a list of shape tuples\n# defining the different scales at which we will run gradient ascent\noriginal_shape = img.shape[1:3]\nsuccessive_shapes = [original_shape]\nfor i in range(1, num_octave):\n    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n    successive_shapes.append(shape)\n\n# Reverse list of shapes, so that they are in increasing order\nsuccessive_shapes = successive_shapes[::-1]\n\n# Resize the Numpy array of the image to our smallest scale\noriginal_img = np.copy(img)\nshrunk_original_img = resize_img(img, successive_shapes[0])\n\nfor shape in successive_shapes:\n    print(\'Processing image shape\', shape)\n    img = resize_img(img, shape)\n    img = gradient_ascent(img,\n                          iterations=iterations,\n                          step=step,\n                          max_loss=max_loss)\n    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n    same_size_original = resize_img(original_img, shape)\n    lost_detail = same_size_original - upscaled_shrunk_original_img\n\n    img += lost_detail\n    shrunk_original_img = resize_img(original_img, shape)\n    save_img(img, fname=\'dream_at_scale_\' + str(shape) + \'.png\')\n\nsave_img(img, fname=\'final_dream.png\')\n\nfrom matplotlib import pyplot as plt\n\nplt.grid(b=None)\nplt.imshow(deprocess_image(np.copy(img)));\n'"
95. XGBoost intro/script.py,0,"b'from numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# load data\ndataset = loadtxt(\'pima-indians-diabetes.csv\', delimiter="","")\n\n# split data into X and y\nX = dataset[:,0:8]\nY = dataset[:,8]\n\n# split data into train and test sets\nseed = 7\ntest_size = 0.33\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n\n# fit model no training data\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n\n# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(""Accuracy: %.2f%%"" % (accuracy * 100.0))\n'"
97. RL with gym/cartPole.py,1,"b""import numpy as np\nimport gym\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.optimizers import Adam\n\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import EpsGreedyQPolicy\nfrom rl.memory import SequentialMemory\n\nENV_NAME = 'CartPole-v0'\n\n# get the env and extract the number of actions available in the\n# Cartpole problem\n\nenv = gym.make(ENV_NAME)\nnp.random.seed(123)\nenv.seed(123)\nnb_actions = env.action_space.n\n\n# simple NN\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))\nprint(model.summary())\n\n# policy as Epsilon Greedy and we also set our memory as Sequential\n# Memory because we want to store the result of actions we performed\n# and the rewards we get for each action\npolicy = EpsGreedyQPolicy()\nmemory = SequentialMemory(limit=50000, window_length=1)\ndqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\ntarget_model_update=1e-2, policy=policy)\ndqn.compile(Adam(lr=1e-3), metrics=['mae'])\n\ndqn.fit(env, nb_steps=5000, visualize=True, verbose=2)\n\ndqn.test(env, nb_episodes=100, visualize=True)\n"""
