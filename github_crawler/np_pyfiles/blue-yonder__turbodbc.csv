file_path,api_count,code
setup.py,0,"b'from setuptools import setup, Extension, Distribution\nimport setuptools.command.build_ext\n\nimport sys\nimport sysconfig\nimport os\nimport os.path\nimport distutils.sysconfig\n\nimport itertools\nfrom glob import iglob\n\n\ndef _get_turbodbc_libname():\n    builder = setuptools.command.build_ext.build_ext(Distribution())\n    full_name = builder.get_ext_filename(\'libturbodbc\')\n    without_lib = full_name.split(\'lib\', 1)[-1]\n    without_so = without_lib.rsplit(\'.so\', 1)[0]\n    return without_so\n\n\ndef _get_distutils_build_directory():\n    """"""\n    Returns the directory distutils uses to build its files.\n    We need this directory since we build extensions which have to link\n    other ones.\n    """"""\n    pattern = ""lib.{platform}-{major}.{minor}""\n    return os.path.join(\'build\', pattern.format(platform=sysconfig.get_platform(),\n                                                major=sys.version_info[0],\n                                                minor=sys.version_info[1]))\n\n\ndef _get_source_files(directory):\n    path = os.path.join(\'src\', directory)\n    iterable_sources = (iglob(os.path.join(root,\'*.cpp\')) for root, dirs, files in os.walk(path))\n    source_files = itertools.chain.from_iterable(iterable_sources)\n    return list(source_files)\n\n\ndef _remove_strict_prototype_option_from_distutils_config():\n    strict_prototypes = \'-Wstrict-prototypes\'\n    config = distutils.sysconfig.get_config_vars()\n    for key, value in config.items():\n        if strict_prototypes in str(value):\n            config[key] = config[key].replace(strict_prototypes, \'\')\n\n_remove_strict_prototype_option_from_distutils_config()\n\n\ndef _has_arrow_headers():\n    try:\n        import pyarrow\n        return True\n    except:\n        return False\n\n\ndef _has_numpy_headers():\n    try:\n        import numpy\n        return True\n    except:\n        return False\n\n\n\nclass _deferred_pybind11_include(object):\n    def __str__(self):\n        import pybind11\n        return pybind11.get_include()\n\n\nextra_compile_args = []\nhidden_visibility_args = []\ninclude_dirs = [\'include/\', _deferred_pybind11_include()]\n\nlibrary_dirs = [_get_distutils_build_directory()]\npython_module_link_args = []\nbase_library_link_args = []\n\nif sys.platform == \'darwin\':\n    extra_compile_args.append(\'--std=c++11\')\n    extra_compile_args.append(\'--stdlib=libc++\')\n    extra_compile_args.append(\'-mmacosx-version-min=10.9\')\n    hidden_visibility_args.append(\'-fvisibility=hidden\')\n    include_dirs.append(os.getenv(\'UNIXODBC_INCLUDE_DIR\', \'/usr/local/include/\'))\n    library_dirs.append(os.getenv(\'UNIXODBC_LIBRARY_DIR\', \'/usr/local/lib/\'))\n\n    from distutils import sysconfig\n    vars = sysconfig.get_config_vars()\n    vars[\'LDSHARED\'] = vars[\'LDSHARED\'].replace(\'-bundle\', \'\')\n    python_module_link_args.append(\'-bundle\')\n    builder = setuptools.command.build_ext.build_ext(Distribution())\n    full_name = builder.get_ext_filename(\'libturbodbc\')\n    base_library_link_args.append(\'-Wl,-dylib_install_name,@loader_path/{}\'.format(full_name))\n    base_library_link_args.append(\'-dynamiclib\')\n    odbclib = \'odbc\'\nelif sys.platform == \'win32\':\n    extra_compile_args.append(\'-DNOMINMAX\')\n    if \'BOOST_ROOT\' in os.environ:\n        include_dirs.append(os.getenv(\'BOOST_ROOT\'))\n        library_dirs.append(os.path.join(os.getenv(\'BOOST_ROOT\'), ""stage"", ""lib""))\n        library_dirs.append(os.path.join(os.getenv(\'BOOST_ROOT\'), ""lib64-msvc-14.0""))\n    else:\n        print(""warning: BOOST_ROOT enviroment variable not set"")\n    odbclib = \'odbc32\'\nelse:\n    extra_compile_args.append(\'--std=c++11\')\n    hidden_visibility_args.append(\'-fvisibility=hidden\')\n    python_module_link_args.append(""-Wl,-rpath,$ORIGIN"")\n    if \'UNIXODBC_INCLUDE_DIR\' in os.environ:\n        include_dirs.append(os.getenv(\'UNIXODBC_INCLUDE_DIR\'))\n    if \'UNIXODBC_LIBRARY_DIR\' in os.environ:\n        library_dirs.append(os.getenv(\'UNIXODBC_LIBRARY_DIR\'))\n    odbclib = \'odbc\'\n\n\ndef get_extension_modules():\n    extension_modules = []\n\n    """"""\n    Extension module which is actually a plain C++ library without Python bindings\n    """"""\n    turbodbc_sources = _get_source_files(\'cpp_odbc\') + _get_source_files(\'turbodbc\')\n    turbodbc_library = Extension(\'libturbodbc\',\n                                 sources=turbodbc_sources,\n                                 include_dirs=include_dirs,\n                                 extra_compile_args=extra_compile_args,\n                                 extra_link_args=base_library_link_args,\n                                 libraries=[odbclib],\n                                 library_dirs=library_dirs)\n    if sys.platform == ""win32"":\n        turbodbc_libs = []\n    else:\n        turbodbc_libs = [_get_turbodbc_libname()]\n        extension_modules.append(turbodbc_library)\n\n    """"""\n    An extension module which contains the main Python bindings for turbodbc\n    """"""\n    turbodbc_python_sources = _get_source_files(\'turbodbc_python\')\n    if sys.platform == ""win32"":\n        turbodbc_python_sources = turbodbc_sources + turbodbc_python_sources\n    turbodbc_python = Extension(\'turbodbc_intern\',\n                                sources=turbodbc_python_sources,\n                                include_dirs=include_dirs,\n                                extra_compile_args=extra_compile_args + hidden_visibility_args,\n                                libraries=[odbclib] + turbodbc_libs,\n                                extra_link_args=python_module_link_args,\n                                library_dirs=library_dirs)\n    extension_modules.append(turbodbc_python)\n\n    """"""\n    An extension module which contains Python bindings which require numpy support\n    to work. Not included in the standard Python bindings so this can stay optional.\n    """"""\n    if _has_numpy_headers():\n        import numpy\n        turbodbc_numpy_sources = _get_source_files(\'turbodbc_numpy\')\n        if sys.platform == ""win32"":\n            turbodbc_numpy_sources = turbodbc_sources + turbodbc_numpy_sources\n        turbodbc_numpy = Extension(\'turbodbc_numpy_support\',\n                                   sources=turbodbc_numpy_sources,\n                                   include_dirs=include_dirs + [numpy.get_include()],\n                                   extra_compile_args=extra_compile_args + hidden_visibility_args,\n                                   libraries=[odbclib] + turbodbc_libs,\n                                   extra_link_args=python_module_link_args,\n                                   library_dirs=library_dirs)\n        extension_modules.append(turbodbc_numpy)\n\n    """"""\n    An extension module which contains Python bindings which require Apache Arrow\n    support to work. Not included in the standard Python bindings so this can\n    stay optional.\n    """"""\n    if _has_arrow_headers():\n        import pyarrow\n        pyarrow_location = os.path.dirname(pyarrow.__file__)\n        # For now, assume that we build against bundled pyarrow releases.\n        pyarrow_include_dir = os.path.join(pyarrow_location, \'include\')\n        turbodbc_arrow_sources = _get_source_files(\'turbodbc_arrow\')\n        pyarrow_module_link_args = list(python_module_link_args)\n        if sys.platform == ""win32"":\n            turbodbc_arrow_sources = turbodbc_sources + turbodbc_arrow_sources\n        elif sys.platform == ""darwin"":\n            pyarrow_module_link_args.append(\'-Wl,-rpath,@loader_path/pyarrow\')\n        else:\n            pyarrow_module_link_args.append(""-Wl,-rpath,$ORIGIN/pyarrow"")\n        turbodbc_arrow = Extension(\'turbodbc_arrow_support\',\n                                   sources=turbodbc_arrow_sources,\n                                   include_dirs=include_dirs + [pyarrow_include_dir],\n                                   extra_compile_args=extra_compile_args + hidden_visibility_args,\n                                   libraries=[odbclib, \'arrow\', \'arrow_python\'] + turbodbc_libs,\n                                   extra_link_args=pyarrow_module_link_args,\n                                   library_dirs=library_dirs + [pyarrow_location])\n        extension_modules.append(turbodbc_arrow)\n\n    return extension_modules\n\n\nhere = os.path.abspath(os.path.dirname(__file__))\nwith open(os.path.join(here, \'README.md\')) as f:\n    long_description = f.read()\n\n\nsetup(name = \'turbodbc\',\n      version = \'4.0.0\',\n      description = \'turbodbc is a Python DB API 2.0 compatible ODBC driver\',\n      long_description=long_description,\n      long_description_content_type=\'text/markdown\',\n      include_package_data = True,\n      url = \'https://github.com/blue-yonder/turbodbc\',\n      author=\'Michael Koenig\',\n      author_email = \'michael.koenig@blue-yonder.com\',\n      packages = [\'turbodbc\'],\n      extras_require={\n          # We pin Apache Arrow quite restrictively until they guarantee a stable API\n          \'arrow\': [\'pyarrow>=0.15,<0.18\'],\n          \'numpy\': \'numpy>=1.16.0\'\n      },\n      classifiers = [\'Development Status :: 5 - Production/Stable\',\n                     \'Intended Audience :: Developers\',\n                     \'License :: OSI Approved :: MIT License\',\n                     \'Operating System :: POSIX :: Linux\',\n                     \'Operating System :: MacOS :: MacOS X\',\n                     \'Operating System :: Microsoft :: Windows\',\n                     \'Programming Language :: C++\',\n                     \'Programming Language :: Python :: 3.6\',\n                     \'Programming Language :: Python :: 3.7\',\n                     \'Programming Language :: Python :: 3.8\',\n                     \'Programming Language :: Python :: Implementation :: CPython\',\n                     \'Topic :: Database\'],\n      ext_modules = get_extension_modules(),\n      install_requires=[\'pybind11>=2.2.0\']\n      )\n'"
contrib/copy-to-sdist.py,0,"b'#!/usr/bin/python3\n""""""This script creates an unofficial sdist directory.\n   It is useful when cmake can not be used (eg. on Windows).""""""\n\nfrom os import makedirs, walk, chdir\nfrom os.path import dirname, join\nfrom shutil import copy2\nfrom sys import argv\n\ndef copyfile(src, dst ,reverse):\n    if reverse:\n        src, dst = dst, src\n    makedirs(dirname(dst), exist_ok=True)\n    copy2(src, dst)\n\ndef copydir(src, dst, fileend, reverse):\n    for dirpath, dirnames, filenames in walk(src):\n        for afile in filenames:\n            if afile.endswith(fileend):\n                fsrc = join(dirpath, afile)\n                fdst = join(dirpath, afile).replace(src, dst)\n                copyfile(fsrc, fdst ,reverse)\n\ndef main():\n    chdir(dirname(__file__))\n    reverse = ""-r"" in argv\n\n    copydir(""../cpp/cpp_odbc/Library/cpp_odbc"", ""sdist/include/cpp_odbc"", "".h"", reverse)\n    copydir(""../cpp/turbodbc/Library/turbodbc"", ""sdist/include/turbodbc"", "".h"", reverse)\n    copydir(""../cpp/turbodbc_numpy/Library/turbodbc_numpy"", ""sdist/include/turbodbc_numpy"", "".h"", reverse)\n    copydir(""../cpp/turbodbc_python/Library/turbodbc_python"", ""sdist/include/turbodbc_python"", "".h"", reverse)\n\n    copydir(""../cpp/cpp_odbc/Library/src"", ""sdist/src/cpp_odbc"", "".cpp"", reverse)\n    copydir(""../cpp/turbodbc/Library/src"", ""sdist/src/turbodbc"", "".cpp"", reverse)\n    copydir(""../cpp/turbodbc_numpy/Library/src"", ""sdist/src/turbodbc_numpy"", "".cpp"", reverse)\n    copydir(""../cpp/turbodbc_python/Library/src"", ""sdist/src/turbodbc_python"", "".cpp"", reverse)\n\n    copydir(""../python/turbodbc"", ""sdist/turbodbc"", "".py"", reverse)\n\n    for afile in [""setup.cfg"",\n                  ""setup.py"",\n                  ""README.md"",\n                  ""CHANGELOG.rst"",\n                  ""MANIFEST.in"",\n                  ""LICENSE"",\n                  ]:\n        fsrc = join("".."", afile)\n        fdst = join(""sdist"", afile)\n        copyfile(fsrc, fdst ,reverse)\n\nif __name__ == ""__main__"":\n    main()\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# turbodbc documentation build configuration file, created by\n# sphinx-quickstart on Sun Apr  9 09:48:15 2017.\n#\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sphinx_rtd_theme\nimport sys\nsys.path.insert(0, os.path.abspath(os.path.join(\'..\', \'python\')))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\nextensions = [\'sphinx.ext.autodoc\']\n\ntemplates_path = [\'_templates\']\n\nsource_suffix = \'.rst\'\n\nmaster_doc = \'index\'\n\nproject = u\'turbodbc\'\ncopyright = u\'2017, Michael K\xc3\xb6nig\'\nauthor = u\'Michael K\xc3\xb6nig\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = os.environ.get(\'READTHEDOCS_VERSION\', \'latest\')\n# The full version, including alpha/beta/rc tags.\nrelease = version\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n# -- Options for autodoc -------------------------------------------------\n# see https://docs.readthedocs.io/en/latest/faq.html\n\ntry:\n    from unittest.mock import MagicMock\nexcept ImportError:\n    from mock import MagicMock\n\nclass Mock(MagicMock):\n    @classmethod\n    def __getattr__(cls, name):\n        return MagicMock()\n\nMOCK_MODULES = [\'turbodbc_intern\']\nsys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'turbodbcdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'turbodbc.tex\', u\'turbodbc Documentation\',\n     u\'Michael K\xc3\xb6nig\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'turbodbc\', u\'turbodbc Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'turbodbc\', u\'turbodbc Documentation\',\n     author, \'turbodbc\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n'"
performance_scripts/measure_performance_exasol.py,0,"b'import pyodbc\nimport turbodbc\nimport numpy\nimport json\nfrom datetime import datetime, date\n\ndef connect(api, dsn):\n    if api == ""pyodbc"":\n        return pyodbc.connect(dsn=dsn)\n    else:\n        return turbodbc.connect(dsn, parameter_sets_to_buffer=100000, rows_to_buffer=100000, use_async_io=True)\n\ndef _column_data(column_type):\n    if column_type == \'INTEGER\':\n        return 42\n    if column_type == \'DOUBLE\':\n        return 3.14\n    if column_type == \'DATE\':\n        return date(2016, 01, 02)\n    if \'VARCHAR\' in column_type:\n        return \'test data\'\n    if column_type == \'TIMESTAMP\':\n        return datetime(2016, 01, 02, 03, 04, 05)\n    raise RuntimeError(""Unknown column type {}"".format(column_type))\n\n\ndef prepare_test_data(cursor, column_types, powers_of_two_lines):\n    columns = [\'col{} {}\'.format(i, type) for i, type in zip(xrange(len(column_types)), column_types)]\n    cursor.execute(\'CREATE OR REPLACE TABLE test_performance ({})\'.format(\', \'.join(columns)))\n\n    data = [_column_data(type) for type in column_types]\n    cursor.execute(\'INSERT INTO test_performance VALUES ({})\'.format(\', \'.join(\'?\' for _ in columns)), data)\n\n    for _ in xrange(powers_of_two_lines):\n        cursor.execute(\'INSERT INTO test_performance SELECT * FROM test_performance\')\n\n\ndef _fetchallnumpy(cursor):\n    cursor.fetchallnumpy()\n\ndef _stream_to_ignore(cursor):\n    for _ in cursor:\n        pass\n\n\ndef _stream_to_list(cursor):\n    [row for row in cursor]\n\n\ndef measure(cursor, extraction_method):\n    cursor.execute(\'SELECT * FROM test_performance\')\n    start = datetime.now()\n    extraction_method(cursor)\n    stop = datetime.now()\n    return (stop - start).total_seconds()\n\n\npowers_of_two = 21\nn_rows = 2**powers_of_two\nn_runs = 10\ncolumn_types = [\'INTEGER\'] #, \'INTEGER\', \'DOUBLE\', \'DOUBLE\'] #, \'VARCHAR(20)\', \'DATE\', \'TIMESTAMP\']\napi = \'pyodbc\'\n# extraction_method = _stream_to_ignore\nextraction_method = _stream_to_list\n# extraction_method = _fetchallnumpy\ndatabase = \'Exasol\'\n\nconnection = connect(api, database)\ncursor = connection.cursor()\n\nprint ""Performing benchmark with {} rows"".format(n_rows)\nprepare_test_data(cursor, column_types, powers_of_two)\n\nruns = []\nfor r in xrange(n_runs):\n    print ""Run #{}"".format(r + 1)\n    runs.append(measure(cursor, extraction_method))\n\nruns = numpy.array(runs)\nresults = {\'number_of_runs\': n_runs,\n           \'rows_per_run\': n_rows,\n           \'column_types\': column_types,\n           \'api\': api,\n           \'extraction_method\': extraction_method.__name__,\n           \'database\': database,\n           \'timings\': {\'best\': runs.min(),\n                       \'worst\': runs.max(),\n                       \'mean\': runs.mean(),\n                       \'standard_deviation\': runs.std()},\n           \'rates\': {\'best\': n_rows / runs.min(),\n                     \'worst\': n_rows / runs.max(),\n                     \'mean\': n_rows * numpy.reciprocal(runs).mean(),\n                     \'standard_deviation\': n_rows * numpy.reciprocal(runs).std()}}\n\nprint json.dumps(results, indent=4, separators=(\',\', \': \'))\nfile_name = \'results_{}_{}{}.json\'.format(database,\n                                          api,\n                                          extraction_method.__name__)\nwith open(file_name, \'w\') as file:\n    json.dump(results, file, indent=4, separators=(\',\', \': \'))\n\nprint ""Wrote results to file {}"".format(file_name)\n'"
performance_scripts/measure_performance_postgresql.py,0,"b'import pyodbc\nimport turbodbc\nimport numpy\nimport json\nimport pgdb # PyGreSQL\nimport psycopg2\nfrom datetime import datetime, date\n\ndef connect(api, dsn):\n    if api == ""pyodbc"":\n        return pyodbc.connect(dsn=dsn)\n    if api == ""PyGreSQL"":\n        return pgdb.connect(database=\'test_db\', host=\'localhost:5432\', user=\'postgres\', password=\'test\')\n    if api == ""psycopg2"":\n        return psycopg2.connect(""dbname=\'test_db\' user=\'postgres\' host=\'localhost\' password=\'test\'"")\n    else:\n        return turbodbc.connect(dsn, parameter_sets_to_buffer=100000, rows_to_buffer=100000, use_async_io=True)\n\ndef _column_data(column_type):\n    if column_type == \'INTEGER\':\n        return 42\n    if column_type == \'DOUBLE PRECISION\':\n        return 3.14\n    if column_type == \'DATE\':\n        return date(2016, 01, 02)\n    if \'VARCHAR\' in column_type:\n        return \'test data\'\n    if column_type == \'TIMESTAMP\':\n        return datetime(2016, 01, 02, 03, 04, 05)\n    raise RuntimeError(""Unknown column type {}"".format(column_type))\n\n\ndef prepare_test_data(cursor, column_types, powers_of_two_lines):\n    cursor.execute(""DROP TABLE IF EXISTS test_performance"")\n    columns = [\'col{} {}\'.format(i, type) for i, type in zip(xrange(len(column_types)), column_types)]\n    cursor.execute(\'CREATE TABLE test_performance ({})\'.format(\', \'.join(columns)))\n\n    data = [_column_data(type) for type in column_types]\n#     try:\n    cursor.execute(\'INSERT INTO test_performance VALUES ({})\'.format(\', \'.join(\'?\' for _ in columns)), data)\n#     except:\n#     cursor.execute(\'INSERT INTO test_performance VALUES ({})\'.format(\', \'.join(\'%s\' for _ in columns)), data)\n\n    for _ in xrange(powers_of_two_lines):\n        cursor.execute(\'INSERT INTO test_performance SELECT * FROM test_performance\')\n\n\ndef _fetchallnumpy(cursor):\n    cursor.fetchallnumpy()\n\ndef _stream_to_ignore(cursor):\n    for _ in cursor:\n        pass\n\n\ndef _stream_to_list(cursor):\n    [row for row in cursor]\n\n\ndef measure(cursor, extraction_method):\n    cursor.execute(\'SELECT * FROM test_performance\')\n    start = datetime.now()\n    extraction_method(cursor)\n    stop = datetime.now()\n    return (stop - start).total_seconds()\n\n\n# if __name__ == ""__main__"":\npowers_of_two = 10\nn_rows = 2**powers_of_two\nn_runs = 10\ncolumn_types = [\'INTEGER\', \'INTEGER\', \'DOUBLE PRECISION\', \'DOUBLE PRECISION\', \'VARCHAR(20)\', \'DATE\', \'TIMESTAMP\']\napi = \'turbodbc\'\n# extraction_method = _stream_to_ignore\n# extraction_method = _stream_to_list\nextraction_method = _fetchallnumpy\ndatabase = \'psql\'\n\nconnection = connect(api, database)\ncursor = connection.cursor()\n\nprint ""Performing benchmark with {} rows"".format(n_rows)\nprepare_test_data(cursor, column_types, powers_of_two)\n\nruns = []\nfor r in xrange(n_runs):\n    print ""Run #{}"".format(r + 1)\n    runs.append(measure(cursor, extraction_method))\n\nruns = numpy.array(runs)\nresults = {\'number_of_runs\': n_runs,\n           \'rows_per_run\': n_rows,\n           \'column_types\': column_types,\n           \'api\': api,\n           \'extraction_method\': extraction_method.__name__,\n           \'database\': database,\n           \'timings\': {\'best\': runs.min(),\n                       \'worst\': runs.max(),\n                       \'mean\': runs.mean(),\n                       \'standard_deviation\': runs.std()},\n           \'rates\': {\'best\': n_rows / runs.min(),\n                     \'worst\': n_rows / runs.max(),\n                     \'mean\': n_rows * numpy.reciprocal(runs).mean(),\n                     \'standard_deviation\': n_rows * numpy.reciprocal(runs).std()}}\n\nprint json.dumps(results, indent=4, separators=(\',\', \': \'))\nfile_name = \'results_{}_{}{}.json\'.format(database,\n                                          api,\n                                          extraction_method.__name__)\nwith open(file_name, \'w\') as file:\n    json.dump(results, file, indent=4, separators=(\',\', \': \'))\n\nprint ""Wrote results to file {}"".format(file_name)\n'"
python/turbodbc/__init__.py,0,"b""from .api_constants import apilevel, threadsafety, paramstyle\nfrom .connect import connect\nfrom .constructors import Date, Time, Timestamp\nfrom .exceptions import Error, InterfaceError, DatabaseError, ParameterError\nfrom .data_types import STRING, BINARY, NUMBER, DATETIME, ROWID\nfrom .options import make_options\nfrom turbodbc_intern import Rows, Megabytes\n\nimport pkg_resources\n\ntry:\n    __version__ = pkg_resources.get_distribution(__name__).version\nexcept:\n    __version__ = 'unknown'\n"""
python/turbodbc/api_constants.py,0,"b'""""""\nGlobal constants as required by PEP-249:\nhttps://www.python.org/dev/peps/pep-0249/#globals\n""""""\napilevel = ""2.0""\nthreadsafety = 1\nparamstyle = \'qmark\''"
python/turbodbc/connect.py,0,"b'from turbodbc_intern import connect as intern_connect\n\nfrom .exceptions import translate_exceptions, ParameterError\nfrom .connection import Connection\nfrom .options import make_options\n\ndef _make_connection_string(dsn, **kwargs):\n    if dsn:\n        kwargs[\'dsn\'] = dsn\n    return \';\'.join([""{}={}"".format(key, value) for key, value in kwargs.items()])\n\n\n@translate_exceptions\ndef connect(dsn=None, turbodbc_options=None, connection_string=None, **kwargs):\n    """"""\n    Create a connection with the database identified by the ``dsn`` or the ``connection_string``.\n\n    :param dsn: Data source name as given in the (unix) odbc.ini file\n           or (Windows) ODBC Data Source Administrator tool.\n    :param turbodbc_options: Options that control how turbodbc interacts with the database.\n           Create such a struct with `turbodbc.make_options()` or leave this blank to take the defaults.\n    :param connection_string: Preformatted ODBC connection string.\n           Specifying this and dsn or kwargs at the same time raises ParameterError.\n    :param \\**kwargs: You may specify additional options as you please. These options will go into\n           the connection string that identifies the database. Valid options depend on the specific database you\n           would like to connect with (e.g. `user` and `password`, or `uid` and `pwd`)\n    :return: A connection to your database\n    """"""\n    if turbodbc_options is None:\n        turbodbc_options = make_options()\n\n    if connection_string is not None and (dsn is not None or len(kwargs) > 0):\n        raise ParameterError(""Both connection_string and dsn or kwargs specified"")\n\n    if connection_string is None:\n        connection_string = _make_connection_string(dsn, **kwargs)\n\n    connection = Connection(intern_connect(connection_string,\n                                           turbodbc_options))\n\n    return connection'"
python/turbodbc/connection.py,0,"b'from weakref import WeakSet\n\nfrom .exceptions import translate_exceptions, InterfaceError\nfrom .cursor import Cursor\n\n\nclass Connection(object):\n    def _assert_valid(self):\n        if self.impl is None:\n            raise InterfaceError(""Connection already closed"")\n\n    def __init__(self, impl):\n        self.impl = impl\n        self.cursors = WeakSet([])\n\n    @translate_exceptions\n    def cursor(self):\n        """"""\n        Create a new ``Cursor`` instance associated with this ``Connection``\n\n        :return: A new ``Cursor`` instance\n        """"""\n        self._assert_valid()\n        c = Cursor(self.impl.cursor())\n        self.cursors.add(c)\n        return c\n\n    @translate_exceptions\n    def commit(self):\n        """"""\n        Commits the current transaction\n        """"""\n        self._assert_valid()\n        self.impl.commit()\n\n    @translate_exceptions\n    def rollback(self):\n        """"""\n        Roll back all changes in the current transaction\n        """"""\n        self._assert_valid()\n        self.impl.rollback()\n\n    def close(self):\n        """"""\n        Close the connection and all associated cursors. This will implicitly\n        roll back any uncommitted operations.\n        """"""\n        for c in self.cursors:\n            c.close()\n        self.cursors = []\n        self.impl = None\n\n    @property\n    def autocommit(self):\n        """"""\n        This attribute controls whether changes are automatically committed after each\n        execution or not.\n        """"""\n        return self.impl.autocommit_enabled()\n\n    @autocommit.setter\n    def autocommit(self, value):\n        self.impl.set_autocommit(value)\n\n\n    def __enter__(self):\n        """"""\n        Conformance to PEP-343\n        """"""\n        return self\n\n    def __exit__(self, type, value, traceback):\n        """"""\n        Conformance to PEP-343\n        """"""\n        return self.close()\n\n'"
python/turbodbc/constructors.py,0,"b'""""""\nThis file \'implements\' the constructors required by \nhttps://www.python.org/dev/peps/pep-0249/#type-objects-and-constructors\n""""""\n\nfrom datetime import date as Date\nfrom datetime import time as Time\nfrom datetime import datetime as Timestamp'"
python/turbodbc/cursor.py,0,"b'from itertools import islice\nfrom collections import OrderedDict\n\nfrom turbodbc_intern import make_row_based_result_set, make_parameter_set\n\nfrom .exceptions import translate_exceptions, InterfaceError, Error\n\n\n_NO_NUMPY_SUPPORT_MSG = ""This installation of turbodbc does not support NumPy extensions. "" \\\n                        ""Please install the `numpy` package. If you have built turbodbc from source, "" \\\n                        ""you may also need to reinstall turbodbc to compile the extensions.""\n_NO_ARROW_SUPPORT_MSG = ""This installation of turbodbc does not support Apache Arrow extensions. "" \\\n                        ""Please install the `pyarrow` package. If you have built turbodbc from source, "" \\\n                        ""you may also need to reinstall turbodbc to compile the extensions.""\n\ndef _has_numpy_support():\n    try:\n        import turbodbc_numpy_support\n        return True\n    except ImportError:\n        return False\n\n\ndef _has_arrow_support():\n    try:\n        import turbodbc_arrow_support\n        return True\n    except ImportError:\n        return False\n\n\ndef _make_masked_arrays(result_batch):\n    from numpy.ma import MaskedArray\n    from numpy import object_\n    masked_arrays = []\n    for data, mask in result_batch:\n        if isinstance(data, list):\n            masked_arrays.append(MaskedArray(data=data, mask=mask, dtype=object_))\n        else:\n            masked_arrays.append(MaskedArray(data=data, mask=mask))\n    return masked_arrays\n\n\ndef _assert_numpy_column_preconditions(columns):\n    from numpy.ma import MaskedArray\n    from numpy import ndarray\n    n_columns = len(columns)\n    for index, column in enumerate(columns, start=1):\n        if type(column) not in [MaskedArray, ndarray]:\n            raise InterfaceError(""Bad type for column {} of {}. Only numpy.ndarray and numpy.ma.MaskedArrays are supported"".format(index, n_columns))\n        if column.ndim != 1:\n            raise InterfaceError(""Column {} of {} is not one-dimensional"".format(index, n_columns))\n        if not column.flags.c_contiguous:\n            raise InterfaceError(""Column {} of {} is not contiguous"".format(index, n_columns))\n\n    lengths = [len(column) for column in columns]\n    all_same_length = all(l == lengths[0] for l in lengths)\n    if not all_same_length:\n        raise InterfaceError(""All columns must have the same length, got lengths {}"".format(lengths))\n\n\nclass Cursor(object):\n    """"""\n    This class allows you to send SQL commands and queries to a database and retrieve\n    associated result sets.\n    """"""\n    def __init__(self, impl):\n        self.impl = impl\n        self.result_set = None\n        self.rowcount = -1\n        self.arraysize = 1\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        element = self.fetchone()\n        if element is None:\n            raise StopIteration\n        else:\n            return element\n\n    def _assert_valid(self):\n        if self.impl is None:\n            raise InterfaceError(""Cursor already closed"")\n\n    def _assert_valid_result_set(self):\n        if self.result_set is None:\n            raise InterfaceError(""No active result set"")\n\n    @property\n    def description(self):\n        """"""\n        Retrieve a description of the columns in the current result set\n\n        :return: A tuple of seven elements. Only some elements are meaningful:\\n\n                 *   Element #0 is the name of the column\n                 *   Element #1 is the type code of the column\n                 *   Element #6 is true if the column may contain ``NULL`` values\n        """"""\n        if self.result_set:\n            info = self.result_set.get_column_info()\n            return [(c.name, c.type_code(), None, None, None, None, c.supports_null_values) for c in info]\n        else:\n            return None\n\n    def _execute(self):\n        self.impl.execute()\n        self.rowcount = self.impl.get_row_count()\n        cpp_result_set = self.impl.get_result_set()\n        if cpp_result_set:\n            self.result_set = make_row_based_result_set(cpp_result_set)\n        else:\n            self.result_set = None\n        return self\n\n    @translate_exceptions\n    def execute(self, sql, parameters=None):\n        """"""\n        Execute an SQL command or query\n\n        :param sql: A (unicode) string that contains the SQL command or query. If you would like to\n               use parameters, please use a question mark ``?`` at the location where the\n               parameter shall be inserted.\n        :param parameters: An iterable of parameter values. The number of values must match\n               the number of parameters in the SQL string.\n        :return: The ``Cursor`` object to allow chaining of operations.\n        """"""\n        self.rowcount = -1\n        self._assert_valid()\n        self.impl.prepare(sql)\n        if parameters:\n            buffer = make_parameter_set(self.impl)\n            buffer.add_set(parameters)\n            buffer.flush()\n        return self._execute()\n\n    @translate_exceptions\n    def executemany(self, sql, parameters=None):\n        """"""\n        Execute an SQL command or query with multiple parameter sets passed in a row-wise fashion.\n        This function is part of PEP-249.\n\n        :param sql: A (unicode) string that contains the SQL command or query. If you would like to\n               use parameters, please use a question mark ``?`` at the location where the\n               parameter shall be inserted.\n        :param parameters: An iterable of iterable of parameter values. The outer iterable represents\n               separate parameter sets. The inner iterable contains parameter values for a given\n               parameter set. The number of values of each set must match the number of parameters\n               in the SQL string.\n        :return: The ``Cursor`` object to allow chaining of operations.\n        """"""\n        self.rowcount = -1\n        self._assert_valid()\n        self.impl.prepare(sql)\n\n        if parameters:\n            buffer = make_parameter_set(self.impl)\n            for parameter_set in parameters:\n                buffer.add_set(parameter_set)\n            buffer.flush()\n\n        return self._execute()\n\n    @translate_exceptions\n    def executemanycolumns(self, sql, columns):\n        """"""\n        Execute an SQL command or query with multiple parameter sets that are passed in\n        a column-wise fashion as opposed to the row-wise parameters in ``executemany()``.\n        This function is a turbodbc-specific extension to PEP-249.\n\n        :param sql: A (unicode) string that contains the SQL command or query. If you would like to\n               use parameters, please use a question mark ``?`` at the location where the\n               parameter shall be inserted.\n        :param columns: An iterable of NumPy MaskedArrays. The Arrays represent the columnar\n               parameter data,\n        :return: The ``Cursor`` object to allow chaining of operations.\n        """"""\n        self.rowcount = -1\n        self._assert_valid()\n\n        self.impl.prepare(sql)\n\n        if _has_arrow_support():\n            import pyarrow as pa\n\n            def _num_chunks(c):\n                if not isinstance(c, pa.ChunkedArray):\n                    # pyarrow < 0.15\n                    c = c.data\n                return c.num_chunks\n\n            if isinstance(columns, pa.Table):\n                from turbodbc_arrow_support import set_arrow_parameters\n\n                for column in columns.itercolumns():\n                    if _num_chunks(column) != 1:\n                        raise NotImplementedError(""Chunked Arrays are ""\n                                                  ""not yet supported"")\n\n                set_arrow_parameters(self.impl, columns)\n                return self._execute()\n\n        # Workaround to give users a better error message without a need\n        # to import pyarrow\n        if columns.__class__.__module__.startswith(\'pyarrow\'):\n            raise Error(_NO_ARROW_SUPPORT_MSG)\n\n        if not _has_numpy_support():\n            raise Error(_NO_NUMPY_SUPPORT_MSG)\n\n        _assert_numpy_column_preconditions(columns)\n\n\n        from numpy.ma import MaskedArray\n        from turbodbc_numpy_support import set_numpy_parameters\n        split_arrays = []\n        for column in columns:\n            if isinstance(column, MaskedArray):\n                split_arrays.append((column.data, column.mask, str(column.dtype)))\n            else:\n                split_arrays.append((column, False, str(column.dtype)))\n        set_numpy_parameters(self.impl, split_arrays)\n\n        return self._execute()\n\n    @translate_exceptions\n    def fetchone(self):\n        """"""\n        Returns a single row of a result set. Requires an active result set on the database\n        generated with ``execute()`` or ``executemany()``.\n\n        :return: Returns ``None`` when no more rows are available in the result set\n        """"""\n        self._assert_valid_result_set()\n        result = self.result_set.fetch_row()\n        if len(result) == 0:\n            return None\n        else:\n            return result\n\n    @translate_exceptions\n    def fetchall(self):\n        """"""\n        Fetches a list of all rows in the active result set generated with ``execute()`` or\n        ``executemany()``.\n\n        :return: A list of rows\n        """"""\n        return [row for row in self]\n\n    @translate_exceptions\n    def fetchmany(self, size=None):\n        """"""\n        Fetches a batch of rows in the active result set generated with ``execute()`` or\n        ``executemany()``.\n\n        :param size: Controls how many rows are returned. The default ``None`` means that\n               the value of Cursor.arraysize is used.\n        :return: A list of rows\n        """"""\n        if size is None:\n            size = self.arraysize\n        if (size <= 0):\n            raise InterfaceError(""Invalid arraysize {} for fetchmany()"".format(size))\n\n        return [row for row in islice(self, size)]\n\n    def fetchallnumpy(self):\n        """"""\n        Fetches all rows in the active result set generated with ``execute()`` or\n        ``executemany()``.\n\n        :return: An ``OrderedDict`` of *columns*, where the keys of the dictionary\n                 are the column names. The columns are of NumPy\'s ``MaskedArray``\n                 type, where the optimal data type for each result set column is\n                 chosen automatically.\n        """"""\n        from numpy.ma import concatenate\n        batches = list(self._numpy_batch_generator())\n        column_names = [description[0] for description in self.description]\n        return OrderedDict(zip(column_names, [concatenate(column) for column in zip(*batches)]))\n\n    def fetchnumpybatches(self):\n        """"""\n        Returns an iterator over all rows in the active result set generated with ``execute()`` or\n        ``executemany()``.\n\n        :return: An iterator you can use to iterate over batches of rows of the result set. Each\n                 batch consists of an ``OrderedDict`` of NumPy ``MaskedArray`` instances. See\n                 ``fetchallnumpy()`` for details.\n        """"""\n        batchgen = self._numpy_batch_generator()\n        column_names = [description[0] for description in self.description]\n        for next_batch in batchgen:\n            yield OrderedDict(zip(column_names, next_batch))\n\n    def _numpy_batch_generator(self):\n        self._assert_valid_result_set()\n        if not _has_numpy_support():\n            raise Error(_NO_NUMPY_SUPPORT_MSG)\n\n        from turbodbc_numpy_support import make_numpy_result_set\n        numpy_result_set = make_numpy_result_set(self.impl.get_result_set())\n        first_run = True\n        while True:\n            result_batch = _make_masked_arrays(numpy_result_set.fetch_next_batch())\n            is_empty_batch = (len(result_batch[0]) == 0)\n            if is_empty_batch and not first_run:\n                return # Let us return a typed result set at least once\n            first_run = False\n            yield result_batch\n\n    def fetcharrowbatches(self, strings_as_dictionary=False, adaptive_integers=False):\n        """"""\n        Fetches rows in the active result set generated with ``execute()`` or\n        ``executemany()`` as an iterable of arrow tables.\n\n        :param strings_as_dictionary: If true, fetch string columns as\n                 dictionary[string] instead of a plain string column.\n\n        :param adaptive_integers: If true, instead of the integer type returned\n                by the database (driver), this produce integer columns with the\n                smallest possible integer type in which all values can be\n                stored. Be aware that here the type depends on the resulting\n                data.\n\n        :return: generator of ``pyarrow.Table``\n        """"""\n        self._assert_valid_result_set()\n        if _has_arrow_support():\n            from turbodbc_arrow_support import make_arrow_result_set\n            rs = make_arrow_result_set(\n                self.impl.get_result_set(),\n                strings_as_dictionary,\n                adaptive_integers)\n            first_run = True\n            while True:\n                table = rs.fetch_next_batch()\n                is_empty_batch = (len(table) == 0)\n                if is_empty_batch and not first_run:\n                    return # Let us return a typed result set at least once\n                first_run = False\n                yield table\n        else:\n            raise Error(_NO_ARROW_SUPPORT_MSG)\n\n    def fetchallarrow(self, strings_as_dictionary=False, adaptive_integers=False):\n        """"""\n        Fetches all rows in the active result set generated with ``execute()`` or\n        ``executemany()``.\n\n        :param strings_as_dictionary: If true, fetch string columns as\n                 dictionary[string] instead of a plain string column.\n\n        :param adaptive_integers: If true, instead of the integer type returned\n                by the database (driver), this produce integer columns with the\n                smallest possible integer type in which all values can be\n                stored. Be aware that here the type depends on the resulting\n                data.\n\n        :return: ``pyarrow.Table``\n        """"""\n        self._assert_valid_result_set()\n        if _has_arrow_support():\n            from turbodbc_arrow_support import make_arrow_result_set\n            return make_arrow_result_set(\n                    self.impl.get_result_set(),\n                    strings_as_dictionary,\n                    adaptive_integers).fetch_all()\n        else:\n            raise Error(_NO_ARROW_SUPPORT_MSG)\n\n    def close(self):\n        """"""\n        Close the cursor.\n        """"""\n        self.result_set = None\n        if self.impl is not None:\n            self.impl._reset()\n        self.impl = None\n\n    def setinputsizes(self, sizes):\n        """"""\n        Has no effect since turbodbc automatically picks appropriate\n        return types and sizes. Method exists since PEP-249 requires it.\n        """"""\n        pass\n\n    def setoutputsize(self, size, column=None):\n        """"""\n        Has no effect since turbodbc automatically picks appropriate\n        input types and sizes. Method exists since PEP-249 requires it.\n        """"""\n        pass\n\n    def __enter__(self):\n        """"""\n        Conformance to PEP-343\n        """"""\n        return self\n\n    def __exit__(self, type, value, traceback):\n        """"""\n        Conformance to PEP-343\n        """"""\n        return self.close()\n'"
python/turbodbc/data_types.py,0,"b'class DataType(object):\n    def __init__(self, matched_type_codes):\n        self._matched_type_codes = matched_type_codes\n\n    def __eq__(self, other):\n        return other in self._matched_type_codes\n\n    def __ne__(self, other):\n        return not (self == other)\n\n# Type codes according to underlying C++ library:\n_BOOLEAN_CODE = 0\n_INTEGER_CODE = 10\n_FLOATING_POINT_CODE = 20\n_STRING_CODE = 30\n_UNICODE_CODE = 31\n_TIMESTAMP_CODE = 40\n_DATE_CODE = 41\n\n# data types according to https://www.python.org/dev/peps/pep-0249/#type-objects-and-constructors\nSTRING = DataType([_STRING_CODE, _UNICODE_CODE])\nBINARY = DataType([])\nNUMBER = DataType([_BOOLEAN_CODE, _INTEGER_CODE, _FLOATING_POINT_CODE])\nDATETIME = DataType([_DATE_CODE, _TIMESTAMP_CODE])\nROWID = DataType([])'"
python/turbodbc/exceptions.py,0,"b'from functools import wraps\n\nfrom turbodbc_intern import Error as InternError\nfrom turbodbc_intern import InterfaceError as InternInterfaceError\n\n\nclass Error(Exception):\n    """"""\n    turbodbc\'s basic error class\n    """"""\n    pass\n\n\nclass InterfaceError(Error):\n    """"""\n    An error that is raised whenever you use turbodbc incorrectly\n    """"""\n    pass\n\n\nclass DatabaseError(Error):\n    """"""\n    An error that is raised when the database encounters an error while processing\n    your commands and queries\n    """"""\n    pass\n\n\nclass ParameterError(Error):\n    """"""\n    An error that is raised when you use connection arguments that are supposed\n    to be mutually exclusive\n    """"""\n    pass\n\n\ndef translate_exceptions(f):\n    @wraps(f)\n    def wrapper(*args, **kwds):\n        try:\n            return f(*args, **kwds)\n        except InternError as e:\n            raise DatabaseError(str(e))\n        except InternInterfaceError as e:\n            raise InterfaceError(str(e))\n    return wrapper'"
python/turbodbc/options.py,0,"b'from turbodbc_intern import Options\n\ndef make_options(read_buffer_size=None,\n                 parameter_sets_to_buffer=None,\n                 varchar_max_character_limit=None,\n                 prefer_unicode=None,\n                 use_async_io=None,\n                 autocommit=None,\n                 large_decimals_as_64_bit_types=None,\n                 limit_varchar_results_to_max=None,\n                 force_extra_capacity_for_unicode=None,\n                 fetch_wchar_as_char=None):\n    """"""\n    Create options that control how turbodbc interacts with a database. These\n    options affect performance for the most part, but some options may require adjustment\n    so that all features work correctly with certain databases.\n\n    If a parameter is set to `None`, this means the default value is used.\n\n    :param read_buffer_size: Affects performance. Controls the size of batches fetched from the\n     database when reading result sets. Can be either an instance of ``turbodbc.Megabytes`` (recommended)\n     or ``turbodbc.Rows``.\n    :param parameter_sets_to_buffer: Affects performance. Number of parameter sets (rows) which shall be\n     transferred to the server in a single batch when ``executemany()`` is called. Must be an integer.\n    :param varchar_max_character_limit: Affects behavior/performance. If a result set contains fields\n     of type ``VARCHAR(max)`` or ``NVARCHAR(max)`` or the equivalent type of your database, buffers\n     will be allocated to hold the specified number of characters. This may lead to truncation. The\n     default value is ``65535`` characters. Please note that large values reduce the risk of\n     truncation, but may affect the number of rows in a batch of result sets (see ``read_buffer_size``).\n     Please note that this option only relates to retrieving results, not sending parameters to the\n     database.\n    :param use_async_io: Affects performance. Set this option to ``True`` if you want to use asynchronous\n     I/O, i.e., while Python is busy converting database results to Python objects, new result sets are\n     fetched from the database in the background.\n    :param prefer_unicode: May affect functionality and performance. Some databases do not support\n     strings encoded with UTF-8, leading to UTF-8 characters being misinterpreted, misrepresented, or\n     downright rejected. Set this option to ``True`` if you want to transfer character data using the\n     UCS-2/UCS-16 encoding that use (multiple) two-byte instead of (multiple) one-byte characters.\n    :param autocommit: Affects behavior. If set to ``True``, all queries and commands executed\n     with ``cursor.execute()`` or ``cursor.executemany()`` will be succeeded by an implicit ``COMMIT``\n     operation, persisting any changes made to the database. If not set or set to ``False``,\n     users has to take care of calling ``cursor.commit()`` themselves.\n    :param large_decimals_as_64_bit_types: Affects behavior. If set to ``True``, ``DECIMAL(x, y)``\n     results with ``x > 18`` will be rendered as 64 bit integers (``y == 0``) or 64 bit floating\n     point numbers (``y > 0``), respectively. Use this option if your decimal data types are larger\n     than the data they actually hold. Using this data type can lead to overflow errors and loss\n     of precision. If not set or set to ``False``, large decimals are rendered as strings.\n    :param limit_varchar_results_to_max: Affects behavior/performance. If set to ``True``,\n     any text-like fields such as ``VARCHAR(n)`` and ``NVARCHAR(n)`` will be limited to a maximum\n     size of ``varchar_max_character_limit`` characters. This may lead to values being truncated,\n     but reduces the amount of memory required to allocate string buffers, leading to larger, more\n     efficient batches. If not set or set to ``False``, strings can exceed ``varchar_max_character_limit``\n     in size if the database reports them this way. For fields such as ``TEXT``, some databases\n     report a size of 2 billion characters.\n     Please note that this option only relates to retrieving results, not sending parameters to the\n     database.\n    :param force_extra_capacity_for_unicode Affects behavior/performance. Some ODBC drivers report the\n     length of the ``VARCHAR``/``NVARCHAR`` field rather than the number of code points for which space is required\n     to be allocated, resulting in string truncations. Set this option to ``True`` to increase the memory\n     allocated for ``VARCHAR`` and ``NVARCHAR`` fields and prevent string truncations.\n     Please note that this option only relates to retrieving results, not sending parameters to the\n     database.\n    :param fetch_wchar_as_char Affects behavior. Some ODBC drivers retrieve single byte encoded strings\n     into ``NVARCHAR`` fields of result sets, which are decoded incorrectly by turbodbc default settings,\n     resulting in corrupt strings. Set this option to ``True`` to have turbodbc treat ``NVARCHAR`` types\n     as narrow character types when decoding the fields in result sets.\n     Please note that this option only relates to retrieving results, not sending parameters to the\n     database.\n    :return: An option struct that is suitable to pass to the ``turbodbc_options`` parameter of\n     ``turbodbc.connect()``\n    """"""\n    options = Options()\n\n    if not read_buffer_size is None:\n        options.read_buffer_size = read_buffer_size\n\n    if not parameter_sets_to_buffer is None:\n        options.parameter_sets_to_buffer = parameter_sets_to_buffer\n\n    if not varchar_max_character_limit is None:\n        options.varchar_max_character_limit = varchar_max_character_limit\n\n    if not prefer_unicode is None:\n        options.prefer_unicode = prefer_unicode\n\n    if not use_async_io is None:\n        options.use_async_io = use_async_io\n\n    if not autocommit is None:\n        options.autocommit = autocommit\n\n    if not large_decimals_as_64_bit_types is None:\n        options.large_decimals_as_64_bit_types = large_decimals_as_64_bit_types\n\n    if not limit_varchar_results_to_max is None:\n        options.limit_varchar_results_to_max = limit_varchar_results_to_max\n\n    if not force_extra_capacity_for_unicode is None:\n        options.force_extra_capacity_for_unicode = force_extra_capacity_for_unicode\n\n    if not fetch_wchar_as_char is None:\n        options.fetch_wchar_as_char = fetch_wchar_as_char\n\n    return options\n'"
python/turbodbc_test/helpers.py,0,"b'from contextlib import contextmanager\n\nimport json\nimport os\nimport pytest\n\nimport turbodbc\n\n\ndef generate_microseconds_with_precision(digits):\n    microseconds = 0;\n    for i in range(digits):\n        microseconds = 10 * microseconds + i + 1\n    for i in range(6 - digits):\n        microseconds *= 10\n\n    return microseconds\n\n\ndef _get_config_files():\n    variable = \'TURBODBC_TEST_CONFIGURATION_FILES\'\n    try:\n        raw = os.environ[variable]\n        file_names = raw.split(\',\')\n        return [file_name.strip() for file_name in file_names]\n    except KeyError:\n        raise KeyError(\'Please set the environment variable {} to specify the configuration files as a comma-separated list\'.format(variable))\n\n\ndef get_credentials(configuration):\n    if \'user\' in configuration:\n        return {configuration[\'capabilities\'][\'connection_user_option\']: configuration[\'user\'],\n                configuration[\'capabilities\'][\'connection_password_option\']: configuration[\'password\']}\n    else:\n        return {}\n\ndef _load_configuration(file_name):\n    with open(file_name, \'r\') as f:\n        return json.load(f)\n\n\ndef _get_configuration(file_name):\n    conf = _load_configuration(file_name)\n    return (conf[\'data_source_name\'], conf)\n\n\ndef _get_configurations():\n    return [_get_configuration(file_name) for file_name in _get_config_files()]\n\n\n""""""\nUse this decorator to execute a test function once for each database configuration.\n\nPlease note the test function *must* take the parameters `dsn` and `configuration`,\nand in that order.\n\nExample:\n\n@for_each_database\ndef test_important_stuff(dsn, configuration):\n    assert 1 == 2\n""""""\nfor_each_database = pytest.mark.parametrize(""dsn,configuration"",\n                                            _get_configurations())\n\n""""""\nUse this decorator to execute a test function once for each database configuration\nexcept for those databases configurations listed in the parameter.\n\nPlease note the test function *must* take the parameters `dsn` and `configuration`,\nand in that order.\n\nExample:\n\n@for_each_database_except([""MySQL""])\ndef test_important_stuff(dsn, configuration):\n    assert 1 == 2\n""""""\ndef for_each_database_except(exceptions):\n    configurations = _get_configurations()\n    return pytest.mark.parametrize(""dsn,configuration"",\n                                   [c for c in configurations if c[0] not in exceptions])\n\n\n\n""""""\nUse this decorator to execute a test function once for a single database configuration.\n\nPlease note the test function *must* take the parameters `dsn` and `configuration`,\nand in that order.\n\nExample:\n\n@for_one_database\ndef test_important_stuff(dsn, configuration):\n    assert 1 == 2\n""""""\nfor_one_database = pytest.mark.parametrize(""dsn,configuration"",\n                                           [_get_configuration(_get_config_files()[0])])\n\n\n\n@contextmanager\ndef open_connection(configuration,\n                    rows_to_buffer=None,\n                    parameter_sets_to_buffer=100,\n                    **turbodbc_options):\n    dsn = configuration[\'data_source_name\']\n    prefer_unicode = configuration.get(\'prefer_unicode\', False)\n    read_buffer_size = turbodbc.Rows(rows_to_buffer) if rows_to_buffer else turbodbc.Megabytes(1)\n\n    options = turbodbc.make_options(read_buffer_size=read_buffer_size,\n                                    parameter_sets_to_buffer=parameter_sets_to_buffer,\n                                    prefer_unicode=prefer_unicode,\n                                    **turbodbc_options)\n    connection = turbodbc.connect(dsn, turbodbc_options=options, **get_credentials(configuration))\n\n    yield connection\n    connection.close()\n\n\n@contextmanager\ndef open_cursor(configuration,\n                rows_to_buffer=None,\n                parameter_sets_to_buffer=100,\n                **turbodbc_options):\n    with open_connection(configuration,\n                         rows_to_buffer,\n                         parameter_sets_to_buffer,\n                         **turbodbc_options) as connection:\n        cursor = connection.cursor()\n        yield cursor\n        cursor.close()\n'"
python/turbodbc_test/query_fixture.py,0,"b'from contextlib import contextmanager\n\nimport random\n\n\ndef unique_table_name():\n    return \'test_{}\'.format(random.randint(0, 1000000000))\n\n@contextmanager\ndef query_fixture(cursor, configuration, fixture_key):\n    """"""\n    Context manager used to set up fixtures for setting up queries.\n    :param cursor: This cursor is used to execute queries\n    :param configuration: A dictionary containing configuration and fixtures\n    :param fixture_key: Identifies the fixture\n    \n    The context manager performs the following tasks:\n    * If present, create a table or view based on what is written in the\n      fixture\'s ""table"" or ""view"" key\n    * If present, execute all queries listed in the fixture\'s ""setup"" section\n    * If present, return the query listed in the fixture\'s ""payload"" section;\n      else return the name of a temporary table\n    * Clean up any tables or views created\n    \n    The fixtures dictionary should have the following format:\n    \n    {\n        ""setup"": {\n            ""view"": {\n                ""create"": [""CREATE OR REPLACE VIEW {table_name} AS {content}""],\n                ""drop"": [""DROP VIEW {table_name}""]\n            },\n            ""table"": {\n                ""create"": [""CREATE OR REPLACE TABLE {table_name} ({content})""],\n                ""drop"": [""DROP TABLE {table_name}""]\n            }\n        },\n        ""fixtures"": {\n            ""my_fixture_key_with_a_table"": {\n                ""table"": ""field1 INTEGER, field2 VARCHAR(10)"",\n                ""setup"": [""A POTENTIALLY EMPTY LIST OF ADDITIONAL SQL QUERIES""],\n                ""payload"": ""SELECT 42""\n            }\n            ""my_fixture_key_with_a_view"": {\n                ""view"": ""SELECT 42"",\n                ""setup"": [""A POTENTIALLY EMPTY LIST OF ADDITIONAL SQL QUERIES""],\n            }\n\n        }\n    }\n    \n    All sections are optional. Queries may contain\n    ""{table_name}"" to be replaced with a random table (or view) name.\n    """"""\n    fixture = configuration[\'queries\'][fixture_key]\n    table_name = unique_table_name()\n    \n    def _execute_queries(queries, replacements):\n        if not isinstance(queries, list):\n            queries = [queries]\n\n        for query in queries:\n            try:\n                cursor.execute(query.format(**replacements))\n            except Exception as error:\n                raise type(error)(\'Error executing query ""{}"": {}\'.format(query, error))\n\n    def create_objects():\n        if \'view\' in fixture:\n            queries = configuration[\'setup\'][\'view\'][\'create\']\n            replacements = {\'table_name\': table_name,\n                            \'content\': fixture[\'view\']}\n            _execute_queries(queries, replacements)\n        if \'table\' in fixture:\n            queries = configuration[\'setup\'][\'table\'][\'create\']\n            replacements = {\'table_name\': table_name,\n                            \'content\': fixture[\'table\']}\n            _execute_queries(queries, replacements)\n\n    def drop_objects():\n        if \'view\' in fixture:\n            queries = configuration[\'setup\'][\'view\'][\'drop\']\n            replacements = {\'table_name\': table_name}\n            _execute_queries(queries, replacements)\n        if \'table\' in fixture:\n            queries = configuration[\'setup\'][\'table\'][\'drop\']\n            replacements = {\'table_name\': table_name}\n            _execute_queries(queries, replacements)\n\n    create_objects()\n    try:\n        if \'setup\' in fixture:\n            replacements = {\'table_name\': table_name}\n            _execute_queries(fixture[\'setup\'], replacements)\n        if \'payload\' in fixture:\n            yield fixture[\'payload\'].format(table_name=table_name)\n        else:\n            yield table_name\n    finally:\n        drop_objects()\n'"
python/turbodbc_test/test_connect.py,0,"b'import pytest\n\nfrom turbodbc import connect, DatabaseError, ParameterError\nfrom turbodbc.connect import _make_connection_string\nfrom turbodbc.connection import Connection\n\nfrom helpers import for_one_database, get_credentials\n\ndef _test_connection_string(expected, actual):\n    assert len(expected) == len(actual)\n    expected_tokens = expected.split(\';\')\n    actual_tokens = actual.split(\';\')\n    assert len(expected_tokens) == len(actual_tokens)\n\n    for token in expected_tokens:\n        assert token in actual_tokens\n\n\ndef test_make_connection_string_with_dsn():\n    connection_string = _make_connection_string(\'my_dsn\', user=\'my_user\')\n    _test_connection_string(connection_string, \'dsn=my_dsn;user=my_user\')\n\n\ndef test_make_connection_string_without_dsn():\n    connection_string = _make_connection_string(None, user=\'my_user\')\n    _test_connection_string(connection_string, \'user=my_user\')\n\n\n@for_one_database\ndef test_connect_returns_connection_when_successful(dsn, configuration):\n    connection = connect(dsn, **get_credentials(configuration))\n    assert isinstance(connection, Connection)\n\n\n@for_one_database\ndef test_connect_returns_connection_with_explicit_dsn(dsn, configuration):\n    connection = connect(dsn=dsn, **get_credentials(configuration))\n    assert isinstance(connection, Connection)\n\n\ndef test_connect_raises_on_invalid_dsn():\n    invalid_dsn = \'This data source does not exist\'\n    with pytest.raises(DatabaseError):\n        connect(invalid_dsn)\n\n\n@for_one_database\ndef test_connect_raises_on_invalid_additional_option(dsn, configuration):\n    additional_option = {configuration[\'capabilities\'][\'connection_user_option\']: \'invalid user\'}\n    with pytest.raises(DatabaseError):\n        connect(dsn=dsn, **additional_option)\n\n\ndef test_connect_raises_on_ambiguous_parameters():\n    with pytest.raises(ParameterError):\n        connect(""foo"", connection_string=""DRIVER=bar;SERVER=baz;"")\n    with pytest.raises(ParameterError):\n        connect(connection_string=""DRIVER=foo;SERVER=bar;"", baz=""qux"")\n\n\n@for_one_database\ndef test_connect_with_connection_string(dsn, configuration):\n    connection_string = ""DSN=%s;"" % dsn\n    for para, val in get_credentials(configuration).items():\n        connection_string = connection_string + ""%s=%s;"" % (para, val)\n    connection = connect(connection_string=connection_string)\n    connection.cursor().execute(""SELECT \'foo\'"")\n    connection.close()\n'"
python/turbodbc_test/test_connection.py,0,"b'import pytest\n\nfrom turbodbc import connect, InterfaceError, DatabaseError, make_options\n\nfrom helpers import for_one_database, get_credentials\nfrom query_fixture import unique_table_name\n\n\n@for_one_database\ndef test_cursor_on_closed_connection_raises(dsn, configuration):\n    connection = connect(dsn, **get_credentials(configuration))\n    connection.close()\n\n    with pytest.raises(InterfaceError):\n        connection.cursor()\n\n\n@for_one_database\ndef test_closing_twice_is_ok(dsn, configuration):\n    connection = connect(dsn, **get_credentials(configuration))\n\n    connection.close()\n    connection.close()\n\n\n@for_one_database\ndef test_closing_connection_closes_all_cursors(dsn, configuration):\n    connection = connect(dsn, **get_credentials(configuration))\n    cursor_1 = connection.cursor()\n    cursor_2 = connection.cursor()\n    connection.close()\n\n    with pytest.raises(InterfaceError):\n        cursor_1.execute(""SELECT 42"")\n\n    with pytest.raises(InterfaceError):\n        cursor_2.execute(""SELECT 42"")\n\n\n@for_one_database\ndef test_no_autocommit(dsn, configuration):\n    connection = connect(dsn, **get_credentials(configuration))\n\n    connection.cursor().execute(\'CREATE TABLE test_no_autocommit (a INTEGER)\')\n    connection.close()\n\n    connection = connect(dsn, **get_credentials(configuration))\n    with pytest.raises(DatabaseError):\n        connection.cursor().execute(\'SELECT * FROM test_no_autocommit\')\n\n\n@for_one_database\ndef test_commit_on_closed_connection_raises(dsn, configuration):\n    connection = connect(dsn, **get_credentials(configuration))\n    connection.close()\n\n    with pytest.raises(InterfaceError):\n        connection.commit()\n\n\n@for_one_database\ndef test_commit(dsn, configuration):\n    table_name = unique_table_name()\n    connection = connect(dsn, **get_credentials(configuration))\n\n    connection.cursor().execute(\'CREATE TABLE {} (a INTEGER)\'.format(table_name))\n    connection.commit()\n\n    connection.close()\n\n    connection = connect(dsn, **get_credentials(configuration))\n    cursor = connection.cursor()\n    cursor.execute(\'SELECT * FROM {}\'.format(table_name))\n    results = cursor.fetchall()\n    assert results == []\n    \n    cursor.execute(\'DROP TABLE {}\'.format(table_name))\n    connection.commit()\n\n\n@for_one_database\ndef test_rollback_on_closed_connection_raises(dsn, configuration):\n    connection = connect(dsn, **get_credentials(configuration))\n    connection.close()\n\n    with pytest.raises(InterfaceError):\n        connection.rollback()\n\n\n@for_one_database\ndef test_rollback(dsn, configuration):\n    table_name = unique_table_name()\n    connection = connect(dsn, **get_credentials(configuration))\n\n    connection.cursor().execute(\'CREATE TABLE {} (a INTEGER)\'.format(table_name))\n    connection.rollback()\n\n    with pytest.raises(DatabaseError):\n        connection.cursor().execute(\'SELECT * FROM {}\'.format(table_name))\n\n\n@for_one_database\ndef test_autocommit_enabled_at_start(dsn, configuration):\n    table_name = unique_table_name()\n    options = make_options(autocommit=True)\n    connection = connect(dsn, turbodbc_options=options, **get_credentials(configuration))\n\n    connection.cursor().execute(\'CREATE TABLE {} (a INTEGER)\'.format(table_name))\n    connection.close()\n\n    connection = connect(dsn, **get_credentials(configuration))\n    cursor = connection.cursor()\n    cursor.execute(\'SELECT * FROM {}\'.format(table_name))\n    results = cursor.fetchall()\n    assert results == []\n\n    cursor.execute(\'DROP TABLE {}\'.format(table_name))\n    connection.commit()\n\n\n@for_one_database\ndef test_autocommit_switching(dsn, configuration):\n    table_name = unique_table_name()\n\n    connection = connect(dsn, **get_credentials(configuration))\n    connection.autocommit = True   # <---\n    connection.cursor().execute(\'CREATE TABLE {} (a INTEGER)\'.format(table_name))\n    connection.close()\n\n    options = make_options(autocommit=True)\n    connection = connect(dsn, turbodbc_options=options, **get_credentials(configuration))\n    connection.autocommit = False  # <---\n    connection.cursor().execute(\'INSERT INTO {} VALUES (?)\'.format(table_name), [42])\n    connection.close()\n\n    # table is there, but data was not persisted\n    connection = connect(dsn, **get_credentials(configuration))\n    cursor = connection.cursor()\n    cursor.execute(\'SELECT * FROM {}\'.format(table_name))\n    results = cursor.fetchall()\n    assert results == []\n\n    cursor.execute(\'DROP TABLE {}\'.format(table_name))\n    connection.commit()\n\n\n@for_one_database\ndef test_autocommit_querying(dsn, configuration):\n    connection = connect(dsn, **get_credentials(configuration))\n    assert connection.autocommit == False\n    connection.autocommit = True\n    assert connection.autocommit == True\n    connection.close()\n\n@for_one_database\ndef test_pep343_with_statement(dsn, configuration):\n\n    with connect(dsn, **get_credentials(configuration)) as connection:\n        cursor = connection.cursor()\n\n    # connection should be closed, test it with the cursor\n    with pytest.raises(InterfaceError):\n        cursor.execute(""SELECT 42"")\n'"
python/turbodbc_test/test_constructors.py,0,"b'import datetime\n\nfrom turbodbc import Date, Time, Timestamp\n\ndef test_constructors_date():\n    d = Date(2016, 1, 4)\n    assert d == datetime.date(2016, 1, 4)\n\ndef test_constructors_time():\n    t = Time(1, 2, 3)\n    assert t == datetime.time(1, 2, 3)\n\ndef test_constructors_timestamp():\n    ts = Timestamp(2016, 1, 2, 3, 4, 5)\n    assert ts == datetime.datetime(2016, 1, 2, 3, 4, 5)'"
python/turbodbc_test/test_cursor_async_io.py,0,"b'import pytest\n\nfrom query_fixture import query_fixture\nfrom helpers import for_one_database, open_cursor\n\n\n@for_one_database\ndef test_many_batches_with_async_io(dsn, configuration):\n    with open_cursor(configuration, use_async_io=True) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            # insert 2^16 rows\n            cursor.execute(""INSERT INTO {} VALUES (1)"".format(table_name))\n            for _ in range(16):\n                cursor.execute(""INSERT INTO {} SELECT * FROM {}"".format(table_name,\n                                                                        table_name))\n\n            cursor.execute(""SELECT * FROM {}"".format(table_name))\n            assert sum(1 for _ in cursor) == 2**16\n'"
python/turbodbc_test/test_cursor_basics.py,0,"b'import pytest\n\nfrom turbodbc import connect, InterfaceError, Error, DatabaseError\n\nfrom helpers import for_one_database, get_credentials, open_cursor, for_each_database\nfrom query_fixture import query_fixture\n\n\n@for_one_database\ndef test_new_cursor_properties(dsn, configuration):\n    connection = connect(dsn, **get_credentials(configuration))\n    cursor = connection.cursor()\n\n    # https://www.python.org/dev/peps/pep-0249/#rowcount\n    assert cursor.rowcount == -1\n    assert None == cursor.description\n    assert cursor.arraysize == 1\n\n\n@for_one_database\ndef test_closed_cursor_raises_when_used(dsn, configuration):\n    connection = connect(dsn, **get_credentials(configuration))\n    cursor = connection.cursor()\n\n    cursor.close()\n\n    with pytest.raises(InterfaceError):\n        cursor.execute(""SELECT 42"")\n\n    with pytest.raises(InterfaceError):\n        cursor.executemany(""SELECT 42"")\n\n    with pytest.raises(InterfaceError):\n        cursor.executemanycolumns(""SELECT 42"", [])\n\n    with pytest.raises(InterfaceError):\n        cursor.fetchone()\n\n    with pytest.raises(InterfaceError):\n        cursor.fetchmany()\n\n    with pytest.raises(InterfaceError):\n        cursor.fetchall()\n\n    with pytest.raises(InterfaceError):\n        next(cursor)\n\n\n@for_one_database\ndef test_closing_twice_does_not_raise(dsn, configuration):\n    connection = connect(dsn, **get_credentials(configuration))\n    cursor = connection.cursor()\n\n    cursor.close()\n    cursor.close()\n\n\n@for_one_database\ndef test_open_cursor_without_result_set_raises(dsn, configuration):\n    connection = connect(dsn, **get_credentials(configuration))\n    cursor = connection.cursor()\n\n    with pytest.raises(InterfaceError):\n        cursor.fetchone()\n\n\n@for_one_database\ndef test_setinputsizes_does_not_raise(dsn, configuration):\n    """"""\n    It is legal for setinputsizes() to do nothing, so anything except\n    raising an exception is ok\n    """"""\n    cursor = connect(dsn, **get_credentials(configuration)).cursor()\n    cursor.setinputsizes([10, 20])\n\n\n@for_one_database\ndef test_setoutputsize_does_not_raise(dsn, configuration):\n    """"""\n    It is legal for setinputsizes() to do nothing, so anything except\n    raising an exception is ok\n    """"""\n    cursor = connect(dsn, **get_credentials(configuration)).cursor()\n    cursor.setoutputsize(1000, 42) # with column\n    cursor.setoutputsize(1000, column=42) # with column\n    cursor.setoutputsize(1000) # without column\n\n\n@for_one_database\ndef test_rowcount_is_reset_after_execute_raises(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.execute(""INSERT INTO {} VALUES (?)"".format(table_name), [42])\n            assert cursor.rowcount == 1\n            with pytest.raises(Error):\n                cursor.execute(""this is not even a valid SQL statement"")\n            assert cursor.rowcount == -1\n\n\n@for_one_database\ndef test_rowcount_is_reset_after_executemany_raises(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.execute(""INSERT INTO {} VALUES (?)"".format(table_name), [42])\n            assert cursor.rowcount == 1\n            with pytest.raises(Error):\n                cursor.executemany(""this is not even a valid SQL statement"")\n            assert cursor.rowcount == -1\n\n\n@for_one_database\ndef test_rowcount_is_reset_after_executemanycolumns_raises(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.execute(""INSERT INTO {} VALUES (?)"".format(table_name), [42])\n            assert cursor.rowcount == 1\n            with pytest.raises(Error):\n                cursor.executemanycolumns(""this is not even a valid SQL statement"", [])\n            assert cursor.rowcount == -1\n\n\n@for_one_database\ndef test_connection_does_not_strongly_reference_cursors(dsn, configuration):\n    connection = connect(dsn, **get_credentials(configuration))\n    cursor = connection.cursor()\n    import sys\n    assert sys.getrefcount(cursor) == 2\n\n\n@for_one_database\ndef test_pep343_with_statement(dsn, configuration):\n    with connect(dsn, **get_credentials(configuration)) as connection:\n        with connection.cursor() as cursor:\n            cursor.execute(""SELECT 42"")\n\n        # cursor should be closed\n        with pytest.raises(InterfaceError):\n            cursor.execute(""SELECT 42"")\n\n\n@for_each_database\ndef test_insert_duplicate_uniquecol_raises(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT DUPLICATE UNIQUECOL\') as table_name:\n            with pytest.raises(DatabaseError) as ex:\n                cursor.execute(""INSERT INTO {table_name} VALUES (1)"".format(table_name=table_name))\n                # some databases (e.g. exasol) report failure not in the execute statement above, but only\n                # when closing the odbc handle, i.e. at cursor.close:\n                cursor.close()\n'"
python/turbodbc_test/test_cursor_insert.py,0,"b'import datetime\n\nfrom query_fixture import query_fixture\nfrom helpers import for_each_database, for_one_database, open_cursor, generate_microseconds_with_precision\n\n\ndef _test_insert_many(configuration, fixture_name, data):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, fixture_name) as table_name:\n            cursor.executemany(""INSERT INTO {} VALUES (?)"".format(table_name), data)\n            assert len(data) == cursor.rowcount\n            cursor.execute(""SELECT a FROM {} ORDER BY a"".format(table_name))\n            inserted = cursor.fetchall()\n            data = [list(row) for row in data]\n            assert data == inserted\n\n\ndef _test_insert_one(configuration, fixture_name, data):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, fixture_name) as table_name:\n            cursor.execute(""INSERT INTO {} VALUES (?)"".format(table_name), data)\n            assert 1 == cursor.rowcount\n            cursor.execute(""SELECT a FROM {}"".format(table_name))\n            inserted = cursor.fetchall()\n            assert [list(data)] == inserted\n\n\n@for_one_database\ndef test_execute_with_tuple(dsn, configuration):\n    as_tuple = (1, )\n    _test_insert_one(configuration, \'INSERT INTEGER\', as_tuple)\n\n\n@for_each_database\ndef test_insert_with_execute(dsn, configuration):\n    as_list = [1]\n    _test_insert_one(configuration, \'INSERT INTEGER\', as_list)\n\n\n@for_each_database\ndef test_insert_string_column(dsn, configuration):\n    _test_insert_many(configuration,\n                      \'INSERT STRING\',\n                      [[\'hello\'], [\'my\'], [\'test case\']])\n\n\n@for_each_database\ndef test_insert_string_max_column(dsn, configuration):\n    _test_insert_many(configuration,\n                      \'INSERT STRING MAX\',\n                      [[\'hello\'], [\'my\'], [\'test case\']])\n\n\n@for_each_database\ndef test_insert_string_column_with_truncation(dsn, configuration):\n    with open_cursor(configuration,\n                     varchar_max_character_limit=9,\n                     limit_varchar_results_to_max=True) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT LONG STRING\') as table_name:\n            cursor.execute(""INSERT INTO {} VALUES (?)"".format(table_name), [\'Truncated strings suck\'])\n            cursor.execute(""SELECT a FROM {}"".format(table_name))\n\n            assert cursor.fetchall() == [[\'Truncated\']]\n\n\n@for_each_database\ndef test_insert_unicode_column(dsn, configuration):\n    _test_insert_many(configuration,\n                      \'INSERT UNICODE\',\n                      [[u\'a I \\u2665 unicode\'], [u\'b I really d\\u00f8\']])\n\n\n@for_each_database\ndef test_insert_unicode_max_column(dsn, configuration):\n    _test_insert_many(configuration,\n                      \'INSERT UNICODE MAX\',\n                      [[u\'a I \\u2665 unicode\'], [u\'b I really d\\u00f8\']])\n\n\n@for_each_database\ndef test_insert_unicode_column_with_truncation(dsn, configuration):\n    with open_cursor(configuration,\n                     varchar_max_character_limit=4,\n                     limit_varchar_results_to_max=True) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT UNICODE MAX\') as table_name:\n            cursor.execute(""INSERT INTO {} VALUES (?)"".format(table_name), [u\'I \\u2665 truncated\'])\n            cursor.execute(""SELECT a FROM {}"".format(table_name))\n\n            # depending on the database and the settings, this test may cut through the\n            # multi-byte UTF-8 representation of \\u2665, or it may not if UTF-16 characters\n            # are used. Hence, the assertions are more fuzzy than expected.\n            truncated = cursor.fetchall()[0]\n            assert len(truncated) > 0\n            assert len(truncated) <= 4\n\n\n@for_each_database\ndef test_insert_bool_column(dsn, configuration):\n    _test_insert_many(configuration,\n                      \'INSERT BOOL\',\n                      [[False], [True], [True]])\n\n\n@for_one_database\ndef test_execute_many_with_tuple(dsn, configuration):\n    _test_insert_many(configuration,\n                      \'INSERT INTEGER\',\n                      [(1, ), (2, ), (3, )])\n\n@for_each_database\ndef test_insert_integer_column(dsn, configuration):\n    _test_insert_many(configuration,\n                      \'INSERT INTEGER\',\n                      [[1], [2], [3]])\n\n\n@for_each_database\ndef test_insert_double_column(dsn, configuration):\n    _test_insert_many(configuration,\n                      \'INSERT DOUBLE\',\n                      [[1.23], [2.71], [3.14]])\n\n\n@for_each_database\ndef test_insert_date_column(dsn, configuration):\n    _test_insert_many(configuration,\n                      \'INSERT DATE\',\n                      [[datetime.date(2015, 12, 31)],\n                       [datetime.date(2016, 1, 15)],\n                       [datetime.date(2016, 2, 3)]])\n\n@for_each_database\ndef test_insert_timestamp_column(dsn, configuration):\n    supported_digits = configuration[\'capabilities\'][\'fractional_second_digits\']\n    fractional = generate_microseconds_with_precision(supported_digits)\n\n    _test_insert_many(configuration,\n                      \'INSERT TIMESTAMP\',\n                      [[datetime.datetime(2015, 12, 31, 1, 2, 3, fractional)],\n                       [datetime.datetime(2016, 1, 15, 4, 5, 6, fractional * 2)],\n                       [datetime.datetime(2016, 2, 3, 7, 8, 9, fractional * 3)]])\n\n\n@for_each_database\ndef test_insert_null(dsn, configuration):\n    _test_insert_many(configuration,\n                      \'INSERT INTEGER\',\n                      [[None]])\n\n@for_each_database\ndef test_insert_mixed_data_columns(dsn, configuration):\n    # second column has mixed data types in the same column\n    # first column makes sure values of ""good"" columns are not affected\n    to_insert = [[23, 1.23],\n                 [42, 2]]\n\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT MIXED\') as table_name:\n            cursor.executemany(""INSERT INTO {} VALUES (?, ?)"".format(table_name), to_insert)\n            assert len(to_insert) == cursor.rowcount\n            cursor.execute(""SELECT a, b FROM {} ORDER BY a"".format(table_name))\n            inserted = [list(row) for row in cursor.fetchall()]\n            assert to_insert == inserted\n\n\n@for_each_database\ndef test_insert_no_parameter_list(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.executemany(""INSERT INTO {} VALUES (?)"".format(table_name))\n            assert 0 == cursor.rowcount\n            cursor.execute(""SELECT a FROM {}"".format(table_name))\n            inserted = [list(row) for row in cursor.fetchall()]\n            assert 0 == len(inserted)\n\n\n@for_each_database\ndef test_insert_empty_parameter_list(dsn, configuration):\n    to_insert = []\n\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.executemany(""INSERT INTO {} VALUES (?)"".format(table_name), to_insert)\n            assert 0 == cursor.rowcount\n            cursor.execute(""SELECT a FROM {}"".format(table_name))\n            inserted = [list(row) for row in cursor.fetchall()]\n            assert to_insert == inserted\n\n\n@for_each_database\ndef test_insert_number_of_rows_exceeds_buffer_size(dsn, configuration):\n    buffer_size = 3\n    numbers = buffer_size * 2 + 1\n    data = [[i] for i in range(numbers)]\n\n    with open_cursor(configuration, parameter_sets_to_buffer=buffer_size) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.executemany(""INSERT INTO {} VALUES (?)"".format(table_name), data)\n            assert len(data) == cursor.rowcount\n            cursor.execute(""SELECT a FROM {} ORDER BY a"".format(table_name))\n            inserted = [list(row) for row in cursor.fetchall()]\n            assert data == inserted\n\n\n@for_each_database\ndef test_description_after_insert(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.execute(""INSERT INTO {} VALUES (42)"".format(table_name))\n            assert None == cursor.description\n\n\n@for_each_database\ndef test_string_with_differing_lengths(dsn, configuration):\n    long_strings = [[\'x\' * 5], [\'x\' * 50], [\'x\' * 500]]\n    to_insert = [[1]] # use integer to force rebind to string buffer afterwards\n    to_insert.extend(long_strings)\n    expected = [[\'1\']]\n    expected.extend(long_strings)\n\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT LONG STRING\') as table_name:\n            cursor.executemany(""INSERT INTO {} VALUES (?)"".format(table_name), to_insert)\n            assert len(to_insert) == cursor.rowcount\n            cursor.execute(""SELECT a FROM {}"".format(table_name))\n            inserted = [list(row) for row in cursor.fetchall()]\n            assert expected == inserted\n\n\n@for_each_database\ndef test_rowcount_works_without_parameter_sets(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.execute(""INSERT INTO {} VALUES (42), (17)"".format(table_name))\n            assert cursor.rowcount == 2\n'"
python/turbodbc_test/test_cursor_select.py,0,"b'import datetime\nimport pytest\n\nimport turbodbc\n\nfrom query_fixture import query_fixture\nfrom helpers import open_cursor, for_each_database, for_one_database, for_each_database_except\n\n\ndef _test_single_row_result_set(configuration, query, expected_row):\n    with open_cursor(configuration) as cursor:\n        cursor.execute(query)\n\n        if configuration[""capabilities""][\'supports_row_count\']:\n            assert cursor.rowcount == 1\n        else:\n            assert cursor.rowcount == -1\n\n        row = cursor.fetchone()\n        assert row == expected_row\n\n        row = cursor.fetchone()\n        assert None == row\n\n\n@for_each_database\ndef test_select_with_too_many_parameters_raises(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with pytest.raises(turbodbc.Error):\n            cursor.execute(""SELECT 42"", [42])\n\n        with pytest.raises(turbodbc.Error):\n            cursor.executemany(""SELECT 42"", [[42]])\n\n\n@for_each_database\ndef test_select_single_row_NULL_result(dsn, configuration):\n    _test_single_row_result_set(configuration, ""SELECT NULL"", [None])\n\n\n@for_each_database\ndef test_select_single_row_integer_result(dsn, configuration):\n    _test_single_row_result_set(configuration, ""SELECT 42"", [42])\n\n\n@for_each_database\ndef test_select_single_row_bool_result(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT TRUE\') as query:\n            cursor.execute(query)\n            row = cursor.fetchone()\n            assert row == [True]\n        with query_fixture(cursor, configuration, \'SELECT FALSE\') as query:\n            cursor.execute(query)\n            row = cursor.fetchone()\n            assert row == [False]\n\n\n@for_each_database\ndef test_select_single_row_string_result(dsn, configuration):\n    _test_single_row_result_set(configuration, ""SELECT \'value\'"", [""value""])\n\n\n@for_each_database\ndef test_select_single_row_unicode_result(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT UNICODE\') as query:\n            cursor.execute(query)\n            row = cursor.fetchone()\n            assert row == [u\'I \\u2665 unicode\']\n\n\n@for_each_database\ndef test_select_single_row_double_result(configuration, dsn):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT DOUBLE\') as query:\n            cursor.execute(query)\n            row = cursor.fetchone()\n            assert row == [3.14]\n\n\n@for_each_database\ndef test_select_single_row_date_result(configuration, dsn):\n    _test_single_row_result_set(configuration,\n                                ""SELECT CAST(\'2015-12-31\' AS DATE) AS a"",\n                                [datetime.date(2015, 12, 31)])\n\n@for_each_database\ndef test_select_single_row_timestamp_result(configuration, dsn):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT TIMESTAMP\') as query:\n            cursor.execute(query)\n            row = cursor.fetchone()\n            assert row == [datetime.datetime(2015, 12, 31, 1, 2, 3)]\n\n\n@for_each_database\ndef test_select_single_row_large_numeric_result_as_string(configuration, dsn):\n    _test_single_row_result_set(configuration,\n                                ""SELECT -1234567890123.123456789"",\n                                [\'-1234567890123.123456789\'])\n\n\n@for_each_database\ndef test_select_single_row_large_integer_decimal_as_64_bit_type(configuration, dsn):\n    with open_cursor(configuration, large_decimals_as_64_bit_types=True) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT LARGE INTEGER DECIMAL\') as query:\n            cursor.execute(query)\n            row = cursor.fetchone()\n            assert row == [42]\n\n\n@for_each_database\ndef test_select_single_row_large_fractional_decimal_as_64_bit_type(configuration, dsn):\n    with open_cursor(configuration, large_decimals_as_64_bit_types=True) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT LARGE FRACTIONAL DECIMAL\') as query:\n            cursor.execute(query)\n            row = cursor.fetchone()\n            assert row == pytest.approx([3.14])\n\n\n@for_each_database\ndef test_select_single_row_multiple_columns(configuration, dsn):\n    _test_single_row_result_set(configuration,\n                                ""SELECT 40, 41, 42, 43"",\n                                [40, 41, 42, 43])\n\n\n@for_each_database\ndef test_fetchone(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT MULTIPLE INTEGERS\') as query:\n            cursor.execute(query)\n            row = cursor.fetchone()\n            assert row == [42]\n            row = cursor.fetchone()\n            assert row == [43]\n            row = cursor.fetchone()\n            assert row == [44]\n\n            row = cursor.fetchone()\n            assert None == row\n\n\n@for_each_database\ndef test_fetchall(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT MULTIPLE INTEGERS\') as query:\n            cursor.execute(query)\n            rows = cursor.fetchall()\n            assert len(rows) == 3\n            assert rows[0] == [42]\n            assert rows[1] == [43]\n            assert rows[2] == [44]\n\n\n@for_each_database\ndef test_fetchmany_with_default_arraysize(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT MULTIPLE INTEGERS\') as query:\n            cursor.execute(query)\n            rows = cursor.fetchmany()\n            assert len(rows) == 1\n            assert rows[0] == [42]\n\n\n@for_each_database\ndef test_fetchmany_with_arraysize_parameter(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT MULTIPLE INTEGERS\') as query:\n            cursor.execute(query)\n            arraysize_parameter = 2\n\n            rows = cursor.fetchmany(arraysize_parameter)\n            assert len(rows) == arraysize_parameter\n            assert rows[0] == [42]\n            assert rows[1] == [43]\n\n            # arraysize exceeds number of remaining rows\n            rows = cursor.fetchmany(arraysize_parameter)\n            assert len(rows) == 1\n            assert rows[0] == [44]\n\n\n@for_each_database\ndef test_fetchmany_with_global_arraysize(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT MULTIPLE INTEGERS\') as query:\n            cursor.execute(query)\n\n            arraysize_parameter = 2\n            cursor.arraysize = arraysize_parameter\n\n            rows = cursor.fetchmany()\n            assert len(rows) == arraysize_parameter\n            assert rows[0] == [42]\n            assert rows[1] == [43]\n\n            # arraysize exceeds number of remaining rows\n            rows = cursor.fetchmany()\n            assert len(rows) == 1\n            assert rows[0] == [44]\n\n\n@for_each_database\ndef test_fetchmany_with_bad_arraysize_parameter_raises(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT MULTIPLE INTEGERS\') as query:\n            cursor.execute(query)\n\n            with pytest.raises(turbodbc.InterfaceError):\n                cursor.fetchmany(-1)\n            with pytest.raises(turbodbc.InterfaceError):\n                cursor.fetchmany(0)\n\n\n@for_each_database\ndef test_fetchmany_with_bad_global_arraysize_raises(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT MULTIPLE INTEGERS\') as query:\n            cursor.execute(query)\n\n            cursor.arraysize = -1\n            with pytest.raises(turbodbc.InterfaceError):\n                cursor.fetchmany()\n\n            cursor.arraysize = 0\n            with pytest.raises(turbodbc.InterfaceError):\n                cursor.fetchmany()\n\n\n@for_each_database\ndef test_number_of_rows_exceeds_buffer_size(dsn, configuration):\n    buffer_size = 3\n    with open_cursor(configuration, rows_to_buffer=buffer_size) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            numbers = buffer_size * 2 + 1\n            for i in range(numbers):\n                cursor.execute(""INSERT INTO {} VALUES ({})"".format(table_name, i))\n\n            cursor.execute(""SELECT a FROM {}"".format(table_name))\n            retrieved = cursor.fetchall()\n            actual_sum = sum([row[0] for row in retrieved])\n            expected_sum = sum(range(numbers))\n            assert expected_sum == actual_sum\n\n\n@for_each_database\ndef test_description(dsn, configuration):\n    capabilities = configuration[\'capabilities\']\n\n    with open_cursor(configuration) as cursor:\n        assert None == cursor.description\n\n        def fix_case(string):\n            if capabilities[\'reports_column_names_as_upper_case\']:\n                return string.upper()\n            else:\n                return string\n\n        with query_fixture(cursor, configuration, \'DESCRIPTION\') as table_name:\n            cursor.execute(""SELECT * FROM {}"".format(table_name))\n\n            nullness_for_null_column = not capabilities[\'indicates_null_columns\']\n\n            expected = [(fix_case(\'as_int\'), turbodbc.NUMBER, None, None, None, None, True),\n                        (fix_case(\'as_double\'), turbodbc.NUMBER, None, None, None, None, True),\n                        (fix_case(\'as_varchar\'), turbodbc.STRING, None, None, None, None, True),\n                        (fix_case(\'as_date\'), turbodbc.DATETIME, None, None, None, None, True),\n                        (fix_case(\'as_timestamp\'), turbodbc.DATETIME, None, None, None, None, True),\n                        (fix_case(\'as_int_not_null\'), turbodbc.NUMBER, None, None, None, None, nullness_for_null_column)]\n            assert expected == cursor.description\n\n\n@for_each_database_except([""MySQL""])\ndef test_unicode_column_names(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'UNICODE COLUMN NAME\') as table_name:\n            cursor.execute(""SELECT * FROM {}"".format(table_name))\n            assert cursor.description[0][0] == u""I \\u2665 Unicode""\n\n\n@for_one_database\ndef test_execute_supports_chaining(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        rows = cursor.execute(""SELECT 42"").fetchall()\n        assert rows == [[42]]\n\n\n@for_one_database\ndef test_executemany_supports_chaining(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        rows = cursor.executemany(""SELECT 42"").fetchall()\n        assert rows == [[42]]\n'"
python/turbodbc_test/test_cursor_unsupported_features.py,0,"b'import pytest\n\nfrom turbodbc import connect\n\nfrom helpers import for_one_database, get_credentials\n\n""""""\nTest optional features mentioned in PEP-249 ""behave"" as specified \n""""""\n\n@for_one_database\ndef test_callproc_unsupported(dsn, configuration):\n    cursor = connect(dsn, **get_credentials(configuration)).cursor()\n\n    with pytest.raises(AttributeError):\n        cursor.callproc()\n\n\n@for_one_database\ndef test_nextset_unsupported(dsn, configuration):\n    cursor = connect(dsn, **get_credentials(configuration)).cursor()\n\n    with pytest.raises(AttributeError):\n        cursor.nextset()\n'"
python/turbodbc_test/test_data_types.py,0,"b'import turbodbc.data_types\nfrom turbodbc import STRING, BINARY, NUMBER, DATETIME, ROWID\n\nALL_TYPE_CODES = [turbodbc.data_types._BOOLEAN_CODE,\n                  turbodbc.data_types._INTEGER_CODE,\n                  turbodbc.data_types._FLOATING_POINT_CODE,\n                  turbodbc.data_types._STRING_CODE,\n                  turbodbc.data_types._UNICODE_CODE,\n                  turbodbc.data_types._TIMESTAMP_CODE,\n                  turbodbc.data_types._DATE_CODE]\n\nALL_DATA_TYPES = [STRING, BINARY, NUMBER, DATETIME, ROWID]\n\n\ndef test_each_type_code_matches_one_data_type():\n    for type_code in ALL_TYPE_CODES:\n        matches = [type for type in ALL_DATA_TYPES if type_code == type]\n        assert 1 == len(matches)\n\n\ndef test_each_type_code_mismatches_all_but_one_data_type():\n    for type_code in ALL_TYPE_CODES:\n        mismatches = [type for type in ALL_DATA_TYPES if type_code != type]\n        expected = len(ALL_DATA_TYPES) - 1\n        assert expected == len(mismatches)\n'"
python/turbodbc_test/test_executemanycolumns.py,0,"b'import datetime\n\nimport pytest\n\nfrom numpy.ma import MaskedArray\nfrom numpy import array\n\nfrom query_fixture import query_fixture\nfrom helpers import open_cursor, for_each_database, for_one_database, generate_microseconds_with_precision\n\ncolumn_backends = [""numpy""]\n\ntry:\n    import pyarrow as pa\n    import turbodbc_arrow_support\n    column_backends.append(\'arrow\')\n    # column_backends.append(\'pandas\')\nexcept:\n    pass\n\nfor_each_column_backend = pytest.mark.parametrize(""column_backend"",\n                                                  column_backends)\n\n\ndef _to_columns(values, dtype, column_backend):\n    if column_backend == \'numpy\':\n        return [array(values, dtype=dtype)]\n    elif column_backend == \'arrow\':\n        columns = pa.Array.from_pandas(array(values, dtype=dtype))\n        return pa.Table.from_arrays([columns], [\'column\'])\n\n\ndef _to_masked_columns(values, dtype, mask, column_backend):\n    if column_backend == \'numpy\':\n        return [MaskedArray(values, mask=mask, dtype=dtype)]\n    elif column_backend == \'arrow\':\n        columns = pa.array(array(values, dtype=dtype), mask=array(mask))\n        return pa.Table.from_arrays([columns], [\'column\'])\n\n\ndef _test_basic_column(configuration, fixture, values, dtype, column_backend):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, fixture) as table_name:\n            columns = _to_columns(values, dtype, column_backend)\n            cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n            assert cursor.rowcount == len(values)\n\n            results = cursor.execute(""SELECT A FROM {} ORDER BY A"".format(table_name)).fetchall()\n            assert results == [[value] for value in sorted(values)]\n\n\ndef _test_column_matches_buffer_size(configuration, fixture, values, dtype, column_backend):\n    with open_cursor(configuration, parameter_sets_to_buffer=len(values)) as cursor:\n        with query_fixture(cursor, configuration, fixture) as table_name:\n            columns = _to_columns(values, dtype, column_backend)\n            cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n            assert cursor.rowcount == len(values)\n\n            results = cursor.execute(""SELECT A FROM {} ORDER BY A"".format(table_name)).fetchall()\n            assert results == [[value] for value in sorted(values)]\n\n\ndef _test_column_exceeds_buffer_size(configuration, fixture, values, dtype, column_backend):\n    with open_cursor(configuration, parameter_sets_to_buffer=2) as cursor:\n        with query_fixture(cursor, configuration, fixture) as table_name:\n            columns = _to_columns(values, dtype, column_backend)\n            cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n            assert cursor.rowcount == len(values)\n\n            results = cursor.execute(""SELECT A FROM {} ORDER BY A"".format(table_name)).fetchall()\n            assert results == [[value] for value in sorted(values)]\n\n\ndef _test_masked_column(configuration, fixture, values, dtype, column_backend):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, fixture) as table_name:\n            columns = _to_masked_columns(values, dtype, [False, True, False], column_backend)\n            cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n            assert cursor.rowcount == len(values)\n\n            results = cursor.execute(""SELECT A FROM {} ORDER BY A"".format(table_name)).fetchall()\n            assert results == [[value] for value in [values[0], values[2]]] + [[None]] or \\\n                results == [[None]] + [[value] for value in [values[0], values[2]]]\n\n\ndef _test_masked_column_with_shrunk_mask(configuration, fixture, values, dtype):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, fixture) as table_name:\n            columns = [MaskedArray(values, mask=False, dtype=dtype)]\n            columns[0].shrink_mask()\n            cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n            assert cursor.rowcount == len(values)\n\n            results = cursor.execute(""SELECT A FROM {} ORDER BY A"".format(table_name)).fetchall()\n            assert results == [[value] for value in sorted(values)]\n\n\ndef _test_masked_column_exceeds_buffer_size(configuration, fixture, values, dtype, column_backend):\n    with open_cursor(configuration, parameter_sets_to_buffer=2) as cursor:\n        with query_fixture(cursor, configuration, fixture) as table_name:\n            columns = _to_masked_columns(values, dtype, [True, False, True], column_backend)\n            cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n            assert cursor.rowcount == len(values)\n\n            results = cursor.execute(""SELECT A FROM {} ORDER BY A"".format(table_name)).fetchall()\n            assert results == [[values[1]], [None], [None]] or \\\n                results == [[None], [None], [values[1]]]\n\n\ndef _test_single_masked_value(configuration, fixture, values, dtype):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, fixture) as table_name:\n            columns = [MaskedArray([values[0]], mask=[True], dtype=dtype)]\n            cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n            assert cursor.rowcount == 1\n\n            results = cursor.execute(""SELECT A FROM {} ORDER BY A"".format(table_name)).fetchall()\n            assert results == [[None]]\n\n\ndef _full_column_tests(configuration, fixture, values, dtype, column_backend):\n    _test_basic_column(configuration, fixture, values, dtype, column_backend)\n    _test_column_matches_buffer_size(configuration, fixture, values, dtype, column_backend)\n    _test_column_exceeds_buffer_size(configuration, fixture, values, dtype, column_backend)\n    _test_masked_column(configuration, fixture, values, dtype, column_backend)\n    _test_masked_column_exceeds_buffer_size(configuration, fixture, values, dtype, column_backend)\n    if column_backend == \'numpy\':\n        # For these tests there is no equivalent input structure in Python\n        _test_masked_column_with_shrunk_mask(configuration, fixture, values, dtype)\n        _test_single_masked_value(configuration, fixture, values, dtype)\n\n\n\n@for_each_column_backend\n@for_each_database\n@pytest.mark.parametrize(\'dtype\', [\n    \'int8\', \'int16\', \'int32\', \'int64\',\n    \'uint8\', \'uint16\', \'uint32\', \'uint64\',\n])\ndef test_integer_column(dsn, configuration, column_backend, dtype):\n    if column_backend == \'numpy\' and dtype != \'int64\':\n        pytest.skip(""numpy INSERTs only support int64 as integeral dtype"")\n    elif dtype == \'uint64\':\n        pytest.skip(""uint64 values may be too large to fit into int64"")\n    else:\n        _full_column_tests(configuration, ""INSERT INTEGER"", [17, 23, 42], dtype, column_backend)\n\n\n@for_each_column_backend\n@for_each_database\ndef test_float64_column(dsn, configuration, column_backend):\n    _full_column_tests(configuration, ""INSERT DOUBLE"", [2.71, 3.14, 6.25], \'float64\', column_backend)\n\n\n@for_each_column_backend\n@for_each_database\ndef test_datetime64_microseconds_column(dsn, configuration, column_backend):\n    supported_digits = configuration[\'capabilities\'][\'fractional_second_digits\']\n    fractional = generate_microseconds_with_precision(supported_digits)\n\n    _full_column_tests(configuration,\n                       ""INSERT TIMESTAMP"",\n                       [datetime.datetime(2015, 12, 31, 1, 2, 3, fractional),\n                        datetime.datetime(2016, 1, 1, 4, 5, 6, fractional),\n                        datetime.datetime(2017, 5, 6, 7, 8, 9, fractional)],\n                       \'datetime64[us]\',\n                       column_backend)\n\n\n@for_each_column_backend\n@for_each_database\ndef test_datetime64_nanoseconds_column(dsn, configuration, column_backend):\n    supported_digits = configuration[\'capabilities\'][\'fractional_second_digits\']\n    # C++ unit test checks that conversion method is capable of nanosecond precision\n    fractional = generate_microseconds_with_precision(supported_digits)\n\n    _full_column_tests(configuration,\n                       ""INSERT TIMESTAMP"",\n                       [datetime.datetime(2015, 12, 31, 1, 2, 3, fractional),\n                        datetime.datetime(2016, 1, 1, 4, 5, 6, fractional),\n                        datetime.datetime(2017, 5, 6, 7, 8, 9, fractional)],\n                       \'datetime64[ns]\',\n                       column_backend)\n\n\n@for_each_column_backend\n@for_each_database\ndef test_datetime64_days_column(dsn, configuration, column_backend):\n    _full_column_tests(configuration,\n                       ""INSERT DATE"",\n                       [datetime.date(2015, 12, 31),\n                        datetime.date(2016, 1, 1),\n                        datetime.date(2017, 5, 6)],\n                       \'datetime64[D]\',\n                       column_backend)\n\n\n@for_each_column_backend\n@for_each_database\ndef test_boolean_column(dsn, configuration, column_backend):\n    _full_column_tests(configuration, ""INSERT BOOL"", [True, False, True], \'bool\', column_backend)\n\n\n@for_each_column_backend\n@for_each_database\ndef test_string_column(dsn, configuration, column_backend):\n    _full_column_tests(configuration,\n                       ""INSERT STRING"",\n                       [""Simple"", ""Non-unicode"", ""Strings""],\n                       \'object\',\n                       column_backend)\n\n\n@for_each_column_backend\n@for_each_database\ndef test_unicode_column(dsn, configuration, column_backend):\n    _full_column_tests(configuration,\n                       ""INSERT UNICODE"",\n                       [u""a\\u2665\\u2665\\u2665\\u2665\\u2665"", u""b\\u2665"", u""c\\u2665\\u2665\\u2665""],\n                       \'object\',\n                       column_backend)\n\n\ndef _test_none_in_string_column(configuration, fixture, column_backend):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, fixture) as table_name:\n            columns = _to_columns([None], \'object\', column_backend)\n            cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n\n            results = cursor.execute(""SELECT A FROM {} ORDER BY A"".format(table_name)).fetchall()\n            assert results == [[None]]\n\n\n@for_each_column_backend\n@for_each_database\ndef test_string_column_with_None(dsn, configuration, column_backend):\n    _test_none_in_string_column(configuration, ""INSERT STRING"", column_backend)\n\n\n@for_each_column_backend\n@for_each_database\ndef test_unicode_column_with_None(dsn, configuration, column_backend):\n    _test_none_in_string_column(configuration, ""INSERT UNICODE"", column_backend)\n\n\n@for_each_column_backend\n@for_each_database\ndef test_multiple_columns(dsn, configuration, column_backend):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT TWO INTEGER COLUMNS\') as table_name:\n            columns = [array([17, 23, 42], dtype=\'int64\'), array([3, 2, 1], dtype=\'int64\')]\n            if column_backend == \'arrow\':\n                columns = [pa.Array.from_pandas(x) for x in columns]\n                columns = pa.Table.from_arrays(columns, [\'column1\', \'column2\'])\n            cursor.executemanycolumns(""INSERT INTO {} VALUES (?, ?)"".format(table_name), columns)\n\n            results = cursor.execute(""SELECT A, B FROM {} ORDER BY A"".format(table_name)).fetchall()\n            assert results == [[17, 3], [23, 2], [42, 1]]\n\n\n@for_one_database\ndef test_execute_many_columns_creates_result_set(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        cursor.executemanycolumns(""SELECT 42"", [])\n        assert cursor.fetchall() == [[42]]\n\n\n@for_one_database\ndef test_execute_many_columns_supports_chaining(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        rows = cursor.executemanycolumns(""SELECT 42"", []).fetchall()\n        assert rows == [[42]]\n'"
python/turbodbc_test/test_executemanycolumns_corner_cases.py,0,"b'from numpy.ma import MaskedArray\nfrom numpy import array\nfrom mock import patch\n\nimport pytest\n\nimport turbodbc\n\nfrom turbodbc import InterfaceError\n\nfrom query_fixture import query_fixture\nfrom helpers import open_cursor, for_one_database\n\n\ntry:\n    import pyarrow as pa\n    import turbodbc_arrow_support\n\n    HAVE_ARROW = True\nexcept:\n    HAVE_ARROW = False\n\n\narrow_support = pytest.mark.skipif(not HAVE_ARROW, reason=""not build with Arrow support"")\n\n\n@for_one_database\ndef test_column_of_unsupported_type_raises(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            columns = [""this is not a NumPy MaskedArray""]\n            with pytest.raises(turbodbc.InterfaceError):\n                cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n\n\n@for_one_database\ndef test_columns_of_unequal_sizes_raise(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            columns = [MaskedArray([1, 2, 3], mask=False, dtype=\'int64\'),\n                       MaskedArray([1, 2], mask=False, dtype=\'int64\')]\n            with pytest.raises(turbodbc.InterfaceError):\n                cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n\n\n@for_one_database\ndef test_column_with_incompatible_dtype_raises(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            columns = [MaskedArray([1, 2, 3], mask=False, dtype=\'int16\')]\n            with pytest.raises(turbodbc.InterfaceError):\n                cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n\n\n@for_one_database\ndef test_column_with_multiple_dimensions_raises(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            two_dimensional = array([[1, 2, 3], [4, 5, 6]], dtype=\'int64\')\n            columns = [two_dimensional]\n            with pytest.raises(turbodbc.InterfaceError):\n                cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n\n\n@for_one_database\ndef test_column_with_non_contiguous_data_raises(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            two_dimensional = array([[1, 2, 3], [4, 5, 6]], dtype=\'int64\')\n            one_dimensional = two_dimensional[:, 1]\n            assert one_dimensional.flags.c_contiguous == False\n            columns = [one_dimensional]\n            with pytest.raises(turbodbc.InterfaceError):\n                cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n\n\n@for_one_database\ndef test_number_of_columns_does_not_match_parameter_count(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            columns = [array([42], dtype=\'int64\'), array([17], dtype=\'int64\')]\n            with pytest.raises(turbodbc.InterfaceError):\n                cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n\n\n@for_one_database\ndef test_passing_empty_list_of_columns_is_ok(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.executemanycolumns(""INSERT INTO {} VALUES (42)"".format(table_name), [])\n\n            results = cursor.execute(""SELECT A FROM {} ORDER BY A"".format(table_name)).fetchall()\n            assert results == [[42]]\n\n\n@for_one_database\ndef test_passing_empty_column_is_ok(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            columns = [array([], dtype=\'int64\')]\n            cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), columns)\n\n            results = cursor.execute(""SELECT A FROM {} ORDER BY A"".format(table_name)).fetchall()\n            assert results == []\n\n\n@for_one_database\ndef test_executemanycolumns_without_numpy_support(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with patch(\'turbodbc.cursor._has_numpy_support\', return_value=False):\n            with pytest.raises(turbodbc.Error):\n                cursor.executemanycolumns(""SELECT 42"", [])\n\n\n@arrow_support\n@for_one_database\ndef test_arrow_table_exceeds_expected_columns(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT TWO INTEGER COLUMNS\') as table_name:\n            columns = [array([17, 23, 42], dtype=\'int64\'), array([3, 2, 1], dtype=\'int64\'), array([17, 23, 42], dtype=\'int64\')]\n            columns = [pa.Array.from_pandas(x) for x in columns]\n            columns = pa.Table.from_arrays(columns, [\'column1\', \'column2\', \'column3\'])\n            # InterfaceError: Number of passed columns (3) is not equal to the number of parameters (2)\n            with pytest.raises(InterfaceError):\n                cursor.executemanycolumns(""INSERT INTO {} VALUES (?, ?)"".format(table_name), columns)\n\n@arrow_support\n@for_one_database\ndef test_arrow_table_chunked_arrays_not_supported(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            arr = pa.array([1, 2])\n            rb = pa.RecordBatch.from_arrays([arr], [\'a\'])\n            table = pa.Table.from_batches([rb, rb])\n            with pytest.raises(NotImplementedError):\n                cursor.executemanycolumns(""INSERT INTO {} VALUES (?)"".format(table_name), table)\n'"
python/turbodbc_test/test_has_arrow_support.py,0,"b'from mock import patch\n\nfrom turbodbc.cursor import _has_arrow_support\n\nimport pytest\n\n# Skip all parquet tests if we can\'t import pyarrow.parquet\npytest.importorskip(\'pyarrow\')\n\n# Ignore these with pytest ... -m \'not parquet\'\npyarrow = pytest.mark.pyarrow\n\n\n@pyarrow\ndef test_has_arrow_support_fails():\n    with patch(""builtins.__import__"", side_effect=ImportError):\n        assert _has_arrow_support() == False\n\n\n@pyarrow\ndef test_has_arrow_support_succeeds():\n    assert _has_arrow_support() == True\n'"
python/turbodbc_test/test_has_numpy_support.py,0,"b'from mock import patch\n\nfrom turbodbc.cursor import _has_numpy_support\n\nimport pytest\n\n# Skip all parquet tests if we can\'t import pyarrow.parquet\npytest.importorskip(\'numpy\')\n\n# Ignore these with pytest ... -m \'not parquet\'\nnumpy = pytest.mark.numpy\n\n\n# Skip all parquet tests if we can\'t import pyarrow.parquet\n@pytest.mark.numpy\ndef test_has_numpy_support_fails():\n    with patch(""builtins.__import__"", side_effect=ImportError):\n        assert _has_numpy_support() == False\n\n\n@pytest.mark.numpy\ndef test_has_numpy_support_succeeds():\n    assert _has_numpy_support() == True\n'"
python/turbodbc_test/test_multiple_connections.py,0,"b'from helpers import open_cursor, for_one_database\n\n@for_one_database\ndef test_multiple_open_connections(dsn, configuration):\n    with open_cursor(configuration) as cursor_1:\n        assert cursor_1.executemany(""SELECT 42"").fetchall() == [[42]]\n\n        with open_cursor(configuration) as cursor_2:\n            cursor_2.executemany(""SELECT 2"")\n            cursor_1.executemany(""SELECT 1"")\n            assert cursor_2.fetchall() == [[2]]\n            assert cursor_1.fetchall() == [[1]]\n\n        assert cursor_1.executemany(""SELECT 123"").fetchall() == [[123]]\n'"
python/turbodbc_test/test_options.py,0,"b'from turbodbc import make_options, Rows\n\ndef test_options_without_parameters():\n    options = make_options()\n    # one of the default parameters tested in the C++ part\n    assert options.parameter_sets_to_buffer == 1000\n\n\ndef test_options_with_overrides():\n    options = make_options(read_buffer_size=Rows(123),\n                           parameter_sets_to_buffer=2500,\n                           varchar_max_character_limit=42,\n                           prefer_unicode=True,\n                           use_async_io=True,\n                           autocommit=True,\n                           large_decimals_as_64_bit_types=True,\n                           limit_varchar_results_to_max=True,\n                           force_extra_capacity_for_unicode=True,\n                           fetch_wchar_as_char=True)\n\n    assert options.read_buffer_size.rows == 123\n    assert options.parameter_sets_to_buffer == 2500\n    assert options.varchar_max_character_limit == 42\n    assert options.prefer_unicode == True\n    assert options.use_async_io == True\n    assert options.autocommit == True\n    assert options.large_decimals_as_64_bit_types == True\n    assert options.limit_varchar_results_to_max == True\n    assert options.force_extra_capacity_for_unicode == True\n    assert options.fetch_wchar_as_char == True\n'"
python/turbodbc_test/test_select_arrow.py,0,"b'from collections import OrderedDict\nfrom mock import patch\n\nimport datetime\nimport gc\nimport pytest\nimport sys\nimport turbodbc\n\nfrom query_fixture import query_fixture\nfrom helpers import open_cursor, for_each_database, for_one_database, generate_microseconds_with_precision\n\n# Skip all parquet tests if we can\'t import pyarrow.parquet\npa = pytest.importorskip(\'pyarrow\')\n\n# Ignore these with pytest ... -m \'not parquet\'\npyarrow = pytest.mark.pyarrow\n\n\ndef _fix_case(configuration, string):\n    """"""\n    some databases return column names in upper case\n    """"""\n    capabilities = configuration[\'capabilities\']\n    if capabilities[\'reports_column_names_as_upper_case\']:\n        return string.upper()\n    else:\n        return string\n\n\n@for_one_database\n@pyarrow\ndef test_no_arrow_support(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        cursor.execute(""SELECT 42"")\n        with patch(\'turbodbc.cursor._has_arrow_support\', return_value=False):\n            with pytest.raises(turbodbc.Error):\n                cursor.fetchallarrow()\n\n\n@for_one_database\n@pyarrow\ndef test_arrow_without_result_set_raises(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with pytest.raises(turbodbc.InterfaceError):\n            cursor.fetchallarrow()\n\n\n@for_each_database\n@pyarrow\ndef test_arrow_empty_column(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.execute(""SELECT a FROM {}"".format(table_name))\n            result = cursor.fetchallarrow()\n            assert isinstance(result, pa.Table)\n            assert result.num_columns == 1\n            assert result.num_rows == 0\n\n\n@for_each_database\n@pyarrow\ndef test_arrow_reference_count(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.execute(""SELECT a FROM {}"".format(table_name))\n            result = cursor.fetchallarrow()\n            gc.collect()\n            assert sys.getrefcount(result) == 2\n\n\n@for_each_database\n@pyarrow\ndef test_arrow_int_column(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        cursor.execute(""SELECT 42 AS a"")\n        result = cursor.fetchallarrow()\n        assert isinstance(result, pa.Table)\n        assert result.num_columns == 1\n        assert result.num_rows == 1\n        assert result.schema[0].name == _fix_case(configuration, ""a"")\n        assert str(result.column(0).type) == ""int64""\n        assert result.column(0).to_pylist() == [42]\n\n\n@for_each_database\n@pyarrow\ndef test_arrow_int_column_adaptive(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        cursor.execute(""SELECT 42 AS a"")\n        result = cursor.fetchallarrow(adaptive_integers=True)\n        assert str(result.schema[0].type) == ""int8""\n        assert result.column(0).to_pylist() == [42]\n\n\n@for_each_database\n@pyarrow\ndef test_arrow_double_column(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT DOUBLE\') as query:\n            cursor.execute(query)\n            result = cursor.fetchallarrow()\n            assert isinstance(result, pa.Table)\n            assert result.num_columns == 1\n            assert result.num_rows == 1\n            assert result.schema[0].name == _fix_case(configuration, ""a"")\n            assert str(result.schema[0].type) == ""double""\n            assert result.column(0).to_pylist() == [3.14]\n\n\n@for_each_database\n@pyarrow\ndef test_arrow_boolean_column(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INDEXED BOOL\') as table_name:\n            cursor.executemany(\'INSERT INTO {} VALUES (?, ?)\'.format(table_name),\n                                [[True, 1], [False, 2], [True, 3]])\n            cursor.execute(\'SELECT a FROM {} ORDER BY b\'.format(table_name))\n            result = cursor.fetchallarrow()\n            assert isinstance(result, pa.Table)\n            assert result.num_columns == 1\n            assert result.num_rows == 3\n            assert result.schema[0].name == _fix_case(configuration, ""a"")\n            assert str(result.schema[0].type) == ""bool""\n            assert result.column(0).to_pylist() == [True, False, True]\n\n\n@for_each_database\n@pyarrow\ndef test_arrow_binary_column_with_null(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT TWO INTEGER COLUMNS\') as table_name:\n            cursor.executemany(""INSERT INTO {} VALUES (?, ?)"".format(table_name),\n                               [[42, 1], [None, 2]]) # second column to enforce ordering\n            cursor.execute(""SELECT a FROM {} ORDER BY b"".format(table_name))\n            result = cursor.fetchallarrow()\n            assert isinstance(result, pa.Table)\n            assert result.num_columns == 1\n            assert result.num_rows == 2\n            assert result.schema[0].name == _fix_case(configuration, ""a"")\n            assert str(result.schema[0].type) == ""int64""\n            assert result.column(0).to_pylist() == [42, None]\n            assert result.column(0).null_count == 1\n\n\n@for_each_database\n@pyarrow\ndef test_arrow_binary_column_larger_than_batch_size(dsn, configuration):\n    with open_cursor(configuration, rows_to_buffer=2) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.executemany(""INSERT INTO {} VALUES (?)"".format(table_name),\n                               [[1], [2], [3], [4], [5]])\n            cursor.execute(""SELECT a FROM {} ORDER BY a"".format(table_name))\n            result = cursor.fetchallarrow()\n            assert isinstance(result, pa.Table)\n            assert result.column(0).to_pylist() == [1, 2, 3, 4, 5]\n\n\n@for_each_database\n@pyarrow\ndef test_arrow_timestamp_column(dsn, configuration):\n    supported_digits = configuration[\'capabilities\'][\'fractional_second_digits\']\n    fractional = generate_microseconds_with_precision(supported_digits)\n    timestamp = datetime.datetime(2015, 12, 31, 1, 2, 3, fractional)\n\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT TIMESTAMP\') as table_name:\n            cursor.execute(\'INSERT INTO {} VALUES (?)\'.format(table_name), [timestamp])\n            cursor.execute(\'SELECT a FROM {}\'.format(table_name))\n            result = cursor.fetchallarrow()\n            assert result.column(0).to_pylist() == [timestamp]\n\n\n@for_each_database\n@pyarrow\ndef test_arrow_date_column(dsn, configuration):\n    date = datetime.date(2015, 12, 31)\n\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT DATE\') as table_name:\n            cursor.execute(\'INSERT INTO {} VALUES (?)\'.format(table_name), [date])\n            cursor.execute(\'SELECT a FROM {}\'.format(table_name))\n            result = cursor.fetchallarrow()\n            result.column(0).to_pylist() == [datetime.date(2015, 12, 31)]\n\n\n@for_each_database\n@pyarrow\ndef test_arrow_timelike_column_with_null(dsn, configuration):\n    fill_value = 0;\n\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT TIMESTAMP\') as table_name:\n            cursor.execute(\'INSERT INTO {} VALUES (?)\'.format(table_name), [None])\n            cursor.execute(\'SELECT a FROM {}\'.format(table_name))\n            result = cursor.fetchallarrow()\n            assert result.column(0).to_pylist() == [None]\n\n\n@for_each_database\n@pyarrow\ndef test_arrow_timelike_column_larger_than_batch_size(dsn, configuration):\n    timestamps = [datetime.datetime(2015, 12, 31, 1, 2, 3),\n                  datetime.datetime(2016, 1, 5, 4, 5, 6),\n                  datetime.datetime(2017, 2, 6, 7, 8, 9),\n                  datetime.datetime(2018, 3, 7, 10, 11, 12),\n                  datetime.datetime(2019, 4, 8, 13, 14, 15)]\n\n    with open_cursor(configuration, rows_to_buffer=2) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT TIMESTAMP\') as table_name:\n            cursor.executemany(\'INSERT INTO {} VALUES (?)\'.format(table_name),\n                               [[timestamp] for timestamp in timestamps])\n            cursor.execute(\'SELECT a FROM {} ORDER BY a\'.format(table_name))\n            result = cursor.fetchallarrow()\n            assert result.column(0).to_pylist() == timestamps\n\n\n@for_each_database\n@pyarrow\n@pytest.mark.parametrize(""strings_as_dictionary"", [True, False])\ndef test_arrow_string_column(dsn, configuration, strings_as_dictionary):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT UNICODE\') as table_name:\n            cursor.execute(\'INSERT INTO {} VALUES (?)\'.format(table_name), [u\'unicode \\u2665\'])\n            cursor.execute(\'SELECT a FROM {}\'.format(table_name))\n            result = cursor.fetchallarrow(strings_as_dictionary=strings_as_dictionary)\n            assert result.column(0).to_pylist() == [u\'unicode \\u2665\']\n\n\n@for_each_database\n@pyarrow\n@pytest.mark.parametrize(""strings_as_dictionary"", [True, False])\ndef test_arrow_string_column_with_null(dsn, configuration, strings_as_dictionary):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT STRING\') as table_name:\n            cursor.execute(\'INSERT INTO {} VALUES (?)\'.format(table_name), [None])\n            cursor.execute(\'SELECT a FROM {}\'.format(table_name))\n            result = cursor.fetchallarrow(strings_as_dictionary=strings_as_dictionary)\n            result.column(0).null_count == 1\n            result.column(0).to_pylist() == [None]\n\n\n@for_each_database\n@pyarrow\n@pytest.mark.parametrize(""strings_as_dictionary"", [True, False])\ndef test_arrow_string_column_larger_than_batch_size(dsn, configuration, strings_as_dictionary):\n    strings = [u\'abc\',\n               u\'def\',\n               u\'ghi\',\n               u\'jkl\',\n               u\'mno\']\n    with open_cursor(configuration, rows_to_buffer=2) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT STRING\') as table_name:\n            cursor.executemany(\'INSERT INTO {} VALUES (?)\'.format(table_name),\n                               [[string] for string in strings])\n            cursor.execute(\'SELECT a FROM {} ORDER BY a\'.format(table_name))\n            result = cursor.fetchallarrow(strings_as_dictionary=strings_as_dictionary)\n            result.column(0).to_pylist() == strings\n\n\n@for_each_database\n@pyarrow\ndef test_arrow_two_columns(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT TWO INTEGER COLUMNS\') as table_name:\n            cursor.executemany(""INSERT INTO {} VALUES (?, ?)"".format(table_name),\n                               [[1, 42], [2, 41]])\n            cursor.execute(""SELECT a, b FROM {} ORDER BY a"".format(table_name))\n            result = cursor.fetchallarrow()\n            assert result.to_pydict() == OrderedDict([\n                (_fix_case(configuration, \'a\'), [1, 2]),\n                (_fix_case(configuration, \'b\'), [42, 41])]\n            )\n\n\n@for_each_database\n@pyarrow\ndef test_arrow_two_columns(dsn, configuration):\n    with open_cursor(configuration, rows_to_buffer=1) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT TWO INTEGER COLUMNS\') as table_name:\n            cursor.executemany(""INSERT INTO {} VALUES (?, ?)"".format(table_name),\n                               [[1, 42], [2, 41]])\n            cursor.execute(""SELECT a, b FROM {} ORDER BY a"".format(table_name))\n            result = list(cursor.fetcharrowbatches())\n            assert len(result) == 2\n'"
python/turbodbc_test/test_select_numpy.py,0,"b'import datetime\nfrom collections import OrderedDict\n\nfrom numpy.ma import MaskedArray\nfrom numpy.testing import assert_equal\nimport numpy\nimport pytest\nfrom mock import patch\n\nimport turbodbc\n\nfrom query_fixture import query_fixture\nfrom helpers import open_cursor, for_each_database, for_one_database, generate_microseconds_with_precision\n\n\ndef _fix_case(configuration, string):\n    """"""\n    some databases return column names in upper case\n    """"""\n    capabilities = configuration[\'capabilities\']\n    if capabilities[\'reports_column_names_as_upper_case\']:\n        return string.upper()\n    else:\n        return string\n\n\n@for_one_database\ndef test_no_numpy_support(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        cursor.execute(""SELECT 42"")\n        with patch(\'turbodbc.cursor._has_numpy_support\', return_value=False):\n            with pytest.raises(turbodbc.Error):\n                cursor.fetchallnumpy()\n\n\n@for_one_database\ndef test_numpy_without_result_set_raises(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with pytest.raises(turbodbc.InterfaceError):\n            cursor.fetchallnumpy()\n\n\n@for_each_database\ndef test_numpy_empty_column(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.execute(""SELECT a FROM {}"".format(table_name))\n            results = cursor.fetchallnumpy()\n            assert isinstance(results, OrderedDict)\n            assert len(results) == 1 # ncols\n            assert isinstance(results[_fix_case(configuration, \'a\')], MaskedArray)\n\n@for_each_database\ndef test_numpy_empty_column_batch_fetch(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.execute(""SELECT a FROM {}"".format(table_name))\n            batches = cursor.fetchnumpybatches()\n            for idx, batch in enumerate(batches):\n                assert isinstance(batch, OrderedDict)\n                assert len(batch) == 1 # ncols\n                assert isinstance(batch[_fix_case(configuration, \'a\')], MaskedArray)\n                assert_equal(len(batch[_fix_case(configuration, \'a\')]), 0)\n            assert_equal(idx, 0)\n\n@for_each_database\ndef test_numpy_int_column(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        cursor.execute(""SELECT 42 AS a"")\n        results = cursor.fetchallnumpy()\n        expected = MaskedArray([42], mask=[0])\n        assert results[_fix_case(configuration, \'a\')].dtype == numpy.int64\n        assert_equal(results[_fix_case(configuration, \'a\')], expected)\n\n\n@for_each_database\ndef test_numpy_double_column(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'SELECT DOUBLE\') as query:\n            cursor.execute(query)\n            results = cursor.fetchallnumpy()\n            expected = MaskedArray([3.14], mask=[0])\n            assert results[_fix_case(configuration, \'a\')].dtype == numpy.float64\n            assert_equal(results[_fix_case(configuration, \'a\')], expected)\n\n\n@for_each_database\ndef test_numpy_boolean_column(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INDEXED BOOL\') as table_name:\n            cursor.executemany(\'INSERT INTO {} VALUES (?, ?)\'.format(table_name),\n                                [[True, 1], [False, 2], [True, 3]])\n            cursor.execute(\'SELECT a FROM {} ORDER BY b\'.format(table_name))\n            results = cursor.fetchallnumpy()\n            expected = MaskedArray([True, False, True], mask=[0])\n            assert results[_fix_case(configuration, \'a\')].dtype == numpy.bool_\n            assert_equal(results[_fix_case(configuration, \'a\')], expected)\n\n\n@for_each_database\ndef test_numpy_binary_column_with_null(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT TWO INTEGER COLUMNS\') as table_name:\n            cursor.executemany(""INSERT INTO {} VALUES (?, ?)"".format(table_name),\n                               [[42, 1], [None, 2]]) # second column to enforce ordering\n            cursor.execute(""SELECT a FROM {} ORDER BY b"".format(table_name))\n            results = cursor.fetchallnumpy()\n            expected = MaskedArray([42, 0], mask=[0, 1])\n            assert_equal(results[_fix_case(configuration, \'a\')], expected)\n\n\n@for_each_database\ndef test_numpy_binary_column_larger_than_batch_size(dsn, configuration):\n    with open_cursor(configuration, rows_to_buffer=2) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.executemany(""INSERT INTO {} VALUES (?)"".format(table_name),\n                               [[1], [2], [3], [4], [5]])\n            cursor.execute(""SELECT a FROM {} ORDER BY a"".format(table_name))\n            results = cursor.fetchallnumpy()\n            expected = MaskedArray([1, 2, 3, 4, 5], mask=False)\n            assert_equal(results[_fix_case(configuration, \'a\')], expected)\n\n@for_each_database\ndef test_numpy_batch_fetch(dsn, configuration):\n    with open_cursor(configuration, rows_to_buffer=2) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT INTEGER\') as table_name:\n            cursor.executemany(""INSERT INTO {} VALUES (?)"".format(table_name),\n                               [[1], [2], [3], [4], [5]])\n            cursor.execute(""SELECT a FROM {} ORDER BY a"".format(table_name))\n            batches = cursor.fetchnumpybatches()\n            expected_batches = [MaskedArray([1, 2], mask=False),\n                                MaskedArray([3, 4], mask=False),\n                                MaskedArray([5],    mask=False)]\n            for idx, batch in enumerate(batches):\n                expected = expected_batches[idx]\n                assert_equal(batch[_fix_case(configuration, \'a\')], expected)\n            assert_equal(idx, 2)\n\n@for_each_database\ndef test_numpy_timestamp_column(dsn, configuration):\n    supported_digits = configuration[\'capabilities\'][\'fractional_second_digits\']\n    fractional = generate_microseconds_with_precision(supported_digits)\n    timestamp = datetime.datetime(2015, 12, 31, 1, 2, 3, fractional)\n\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT TIMESTAMP\') as table_name:\n            cursor.execute(\'INSERT INTO {} VALUES (?)\'.format(table_name), [timestamp])\n            cursor.execute(\'SELECT a FROM {}\'.format(table_name))\n            results = cursor.fetchallnumpy()\n            expected = MaskedArray([timestamp], mask=[0], dtype=\'datetime64[us]\')\n            assert results[_fix_case(configuration, \'a\')].dtype == numpy.dtype(\'datetime64[us]\')\n            assert_equal(results[_fix_case(configuration, \'a\')], expected)\n\n\n@for_each_database\ndef test_numpy_date_column(dsn, configuration):\n    date = datetime.date(3999, 12, 31)\n\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT DATE\') as table_name:\n            cursor.execute(\'INSERT INTO {} VALUES (?)\'.format(table_name), [date])\n            cursor.execute(\'SELECT a FROM {}\'.format(table_name))\n            results = cursor.fetchallnumpy()\n            expected = MaskedArray([date], mask=[0], dtype=\'datetime64[D]\')\n            assert results[_fix_case(configuration, \'a\')].dtype == numpy.dtype(\'datetime64[D]\')\n            assert_equal(results[_fix_case(configuration, \'a\')], expected)\n\n\n@for_each_database\ndef test_numpy_timelike_column_with_null(dsn, configuration):\n    fill_value = 0;\n\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT TIMESTAMP\') as table_name:\n            cursor.execute(\'INSERT INTO {} VALUES (?)\'.format(table_name), [None])\n            cursor.execute(\'SELECT a FROM {}\'.format(table_name))\n            results = cursor.fetchallnumpy()\n            expected = MaskedArray([42], mask=[1], dtype=\'datetime64[us]\')\n            assert_equal(results[_fix_case(configuration, \'a\')].filled(fill_value),\n                         expected.filled(fill_value))\n\n\n@for_each_database\ndef test_numpy_timelike_column_larger_than_batch_size(dsn, configuration):\n    timestamps = [datetime.datetime(2015, 12, 31, 1, 2, 3),\n                  datetime.datetime(2016, 1, 5, 4, 5, 6),\n                  datetime.datetime(2017, 2, 6, 7, 8, 9),\n                  datetime.datetime(2018, 3, 7, 10, 11, 12),\n                  datetime.datetime(2019, 4, 8, 13, 14, 15)]\n\n    with open_cursor(configuration, rows_to_buffer=2) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT TIMESTAMP\') as table_name:\n            cursor.executemany(\'INSERT INTO {} VALUES (?)\'.format(table_name),\n                               [[timestamp] for timestamp in timestamps])\n            cursor.execute(\'SELECT a FROM {} ORDER BY a\'.format(table_name))\n            results = cursor.fetchallnumpy()\n            expected = MaskedArray(timestamps, mask=[0], dtype=\'datetime64[us]\')\n            assert_equal(results[_fix_case(configuration, \'a\')], expected)\n\n\n@for_each_database\ndef test_numpy_string_column(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT STRING\') as table_name:\n            cursor.execute(\'INSERT INTO {} VALUES (?)\'.format(table_name), [u\'this is a test\'])\n            cursor.execute(\'SELECT a FROM {}\'.format(table_name))\n            results = cursor.fetchallnumpy()\n            expected = MaskedArray([u\'this is a test\'], mask=[0], dtype=numpy.object_)\n            assert results[_fix_case(configuration, \'a\')].dtype == numpy.object_\n            assert_equal(results[_fix_case(configuration, \'a\')], expected)\n\n\n@for_each_database\ndef test_numpy_string_column_with_truncation(dsn, configuration):\n    with open_cursor(configuration, varchar_max_character_limit=9, limit_varchar_results_to_max=True) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT STRING MAX\') as table_name:\n            cursor.execute(\'INSERT INTO {} VALUES (?)\'.format(table_name), [u\'truncated string\'])\n            cursor.execute(\'SELECT a FROM {}\'.format(table_name))\n            results = cursor.fetchallnumpy()\n            expected = MaskedArray([u\'truncated\'], mask=[0], dtype=numpy.object_)\n            assert results[_fix_case(configuration, \'a\')].dtype == numpy.object_\n            assert_equal(results[_fix_case(configuration, \'a\')], expected)\n\n\n@for_each_database\ndef test_numpy_unicode_column(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT UNICODE\') as table_name:\n            cursor.execute(\'INSERT INTO {} VALUES (?)\'.format(table_name), [u\'unicode \\u2665\'])\n            cursor.execute(\'SELECT a FROM {}\'.format(table_name))\n            results = cursor.fetchallnumpy()\n            expected = MaskedArray([u\'unicode \\u2665\'], mask=[0], dtype=numpy.object_)\n            assert results[_fix_case(configuration, \'a\')].dtype == numpy.object_\n            assert_equal(results[_fix_case(configuration, \'a\')], expected)\n\n\n@for_each_database\ndef test_numpy_unicode_column_with_truncation(dsn, configuration):\n    with open_cursor(configuration, rows_to_buffer=1, varchar_max_character_limit=9, limit_varchar_results_to_max=True) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT UNICODE MAX\') as table_name:\n            cursor.execute(\'INSERT INTO {} VALUES (?)\'.format(table_name), [u\'truncated string\'])\n            cursor.execute(\'SELECT a FROM {}\'.format(table_name))\n            results = cursor.fetchallnumpy()\n            expected = MaskedArray([u\'truncated\'], mask=[0], dtype=numpy.object_)\n            assert results[_fix_case(configuration, \'a\')].dtype == numpy.object_\n            assert_equal(results[_fix_case(configuration, \'a\')], expected)\n\n\n@for_each_database\ndef test_numpy_string_column_with_null(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT STRING\') as table_name:\n            cursor.execute(\'INSERT INTO {} VALUES (?)\'.format(table_name), [None])\n            cursor.execute(\'SELECT a FROM {}\'.format(table_name))\n            results = cursor.fetchallnumpy()\n            expected = MaskedArray([None], mask=[0], dtype=numpy.object_)\n            assert_equal(results[_fix_case(configuration, \'a\')], expected)\n\n\n@for_each_database\ndef test_numpy_string_column_larger_than_batch_size(dsn, configuration):\n    strings = [u\'abc\',\n               u\'def\',\n               u\'ghi\',\n               u\'jkl\',\n               u\'mno\']\n    with open_cursor(configuration, rows_to_buffer=2) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT STRING\') as table_name:\n            cursor.executemany(\'INSERT INTO {} VALUES (?)\'.format(table_name),\n                               [[string] for string in strings])\n            cursor.execute(\'SELECT a FROM {} ORDER BY a\'.format(table_name))\n            results = cursor.fetchallnumpy()\n            expected = MaskedArray(strings, mask=[0], dtype=numpy.object_)\n            assert_equal(results[_fix_case(configuration, \'a\')], expected)\n\n\n@for_each_database\ndef test_numpy_two_columns(dsn, configuration):\n    with open_cursor(configuration) as cursor:\n        with query_fixture(cursor, configuration, \'INSERT TWO INTEGER COLUMNS\') as table_name:\n            cursor.executemany(""INSERT INTO {} VALUES (?, ?)"".format(table_name),\n                               [[1, 42], [2, 41]])\n            cursor.execute(""SELECT a, b FROM {} ORDER BY a"".format(table_name))\n            results = cursor.fetchallnumpy()\n            assert_equal(results[_fix_case(configuration, \'a\')], MaskedArray([1, 2], mask=False))\n            assert_equal(results[_fix_case(configuration, \'b\')], MaskedArray([42, 41], mask=False))\n'"
