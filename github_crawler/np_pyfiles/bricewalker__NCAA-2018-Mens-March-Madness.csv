file_path,api_count,code
clusterone.py,11,"b'import prepare_dataset\n\n# Common imports\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport collections\nimport os\nimport sys\nimport math\nfrom math import pi\nfrom math import sqrt\nimport csv\nimport urllib\nimport pickle\nimport random\nimport statsmodels.api as sm\nfrom patsy import dmatrices\n\n# Math and descriptive stats\nfrom math import sqrt\nfrom scipy import stats\n\n# Sci-kit Learn modules for machine learning\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.externals import joblib\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier, AdaBoostClassifier, AdaBoostRegressor\nfrom sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor, VotingClassifier\nfrom sklearn.ensemble import IsolationForest, RandomForestClassifier, RandomForestRegressor, RandomTreesEmbedding\nfrom sklearn.svm import SVR, LinearSVC, SVC, LinearSVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.decomposition import PCA, KernelPCA\n\n# Boosting libraries\nimport lightgbm as lgb\nimport xgboost\n\n# Deep Learning modules\nfrom keras.layers import Input, Dense, Dropout, Flatten, Embedding, merge, Activation\nfrom keras.layers import Convolution2D, MaxPooling2D, Convolution1D\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils, to_categorical\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport tensorflow as tf\n\nfrom trueskill import TrueSkill, Rating, rate_1vs1\n\nXtrain = np.load(""Data/PrecomputedMatrices/X_train.npy"")\nytrain = np.load(""Data/PrecomputedMatrices/y_train.npy"")\nXtrain = np.nan_to_num(Xtrain)\nytrain = np.nan_to_num(ytrain)\n\nX_train, X_test, Y_train, Y_test = train_test_split(Xtrain, ytrain)\n\nss = StandardScaler()\nX_train_scaled = ss.fit_transform(X_train)\nX_test_scaled = ss.transform(X_test)\n\nlog = LogisticRegression(random_state=95)\n\nknn = KNeighborsClassifier(algorithm=\'kd_tree\', leaf_size=18, metric=\'minkowski\',\n                           metric_params=None, n_jobs=1, n_neighbors=60, p=1,\n                           weights=\'uniform\')\n\nrfc = RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\'gini\',\n                             max_depth=40, max_features=\'log2\', max_leaf_nodes=200,\n                             min_impurity_decrease=0, min_impurity_split=None,\n                             min_samples_leaf=4, min_samples_split=500,\n                             min_weight_fraction_leaf=0, n_estimators=1000, n_jobs=1,\n                             oob_score=False, random_state=None, verbose=0,\n                             warm_start=False)\n\netrees = ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion=\'gini\',\n                              max_depth=40, max_features=\'log2\', max_leaf_nodes=200,\n                              min_impurity_decrease=0, min_impurity_split=None,\n                              min_samples_leaf=4, min_samples_split=500,\n                              min_weight_fraction_leaf=0, n_estimators=1000, n_jobs=1,\n                              oob_score=False, random_state=None, verbose=0, warm_start=False)\n\ngbc = GradientBoostingClassifier(criterion=\'friedman_mse\', init=None,\n                                 learning_rate=0.1, loss=\'deviance\', max_depth=3,\n                                 max_features=None, max_leaf_nodes=16,\n                                 min_impurity_decrease=0.2, min_impurity_split=None,\n                                 min_samples_leaf=220, min_samples_split=2,\n                                 min_weight_fraction_leaf=0, n_estimators=150, presort=\'auto\',\n                                 random_state=None, subsample=1, verbose=0, warm_start=False)\n\nlgbm = lgb.LGBMClassifier(bagging_freq=0, bagging_seed=95, boosting_type=\'gbdt\',\n                          class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,\n                          feature_fraction_seed=95, lambda_l1=0.0, lambda_l2=0.0,\n                          learning_rate=0.1, max_depth=12, min_child_samples=20,\n                          min_child_weight=0.001, min_data_in_leaf=10, min_split_gain=0,\n                          n_estimators=100, n_jobs=-1, num_boost_round=40, num_leaves=25,\n                          num_threads=4, objective=None, random_state=None, reg_alpha=0.0,\n                          reg_lambda=0.0, silent=True, subsample=1.0,\n                          subsample_for_bin=200000, subsample_freq=1)\n\ndef model_function(layer_one_neurons=184, layer_two_neurons=184, layer_three_neurons=184): \n    model = Sequential()\n    model.add(Dense(layer_one_neurons, input_dim=X_train.shape[1], activation=\'relu\'))\n    model.add(Dense(layer_two_neurons, activation=\'relu\'))\n    model.add(Dense(layer_three_neurons, activation=\'relu\'))\n    model.add(Dense(1, activation=\'sigmoid\'))\n    model.compile(loss=\'binary_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n    return model\n\nnn_model = KerasClassifier(build_fn=model_function, epochs=200)\n\n# Soft Voting to get the probabilities\nmodel_soft = VotingClassifier(estimators=[(\'gbc\', gbc), (\'rfr\', rfr), (\'etrees\', etrees),\n                                          (\'knn\', knn), (\'lgbm\', lgbm), (\'log\', log)], voting=\'soft\', n_jobs=-1)\n\nmodel_soft.fit(X_train, Y_train)\nnn_model.fit(X_train_scaled, Y_train)\n\ndef classPredictGame(team_1_vector, team_2_vector, home):\n    diff = [a - b for a, b in zip(team_1_vector, team_2_vector)]\n    diff.append(home)\n    diff = np.nan_to_num(diff)\n    \n    return model_soft.predict_proba(np.array([diff]))[0][1]\n    #return calibrated_model.predict([diff])[0][1] # Depends on model(s) chosen\n\ndef loadTeamVectors(years):\n    listDictionaries = []\n    for year in years:\n        curVectors = np.load(""Data/PrecomputedMatrices/Stage2/"" + str(year) + ""TeamVectors.npy"").item()\n        listDictionaries.append(curVectors)\n    return listDictionaries\n\ndef classCreatePrediction():\n#    if os.path.exists(""Data/Predictions/class_results.csv""):\n#        print (\'There are already precomputed predictions.\')\n#    else:    \n        years = range(2018,2019)\n        listDictionaries = loadTeamVectors(years)\n        print (""Loaded the team vectors."")\n        results = [[0 for x in range(2)] for x in range(len(sample_sub_pd.index))]\n        for index, row in sample_sub_pd.iterrows():\n            matchupId = row[\'ID\']\n            year = int(matchupId[0:4]) \n            teamVectors = listDictionaries[year - years[0]]\n            team1Id = int(matchupId[5:9])\n            team2Id = int(matchupId[10:14])\n            team1Vector = teamVectors[team1Id] \n            team2Vector = teamVectors[team2Id]\n            pred = classPredictGame(team1Vector, team2Vector, 0)\n            results[index][0] = matchupId\n            results[index][1] = pred\n        results = pd.np.array(results)\n        firstRow = [[0 for x in range(2)] for x in range(1)]\n        firstRow[0][0] = \'ID\'\n        firstRow[0][1] = \'Pred\'\n        with open(""Data/Predictions/class_results.csv"", ""w"") as f:\n            writer = csv.writer(f)\n            writer.writerows(firstRow)\n            writer.writerows(results)\n            print(""Saved Results."")\n\nclassCreatePrediction()\n\ndef nnPredictGame(team_1_vector, team_2_vector, home):\n    diff = [a - b for a, b in zip(team_1_vector, team_2_vector)]\n    diff.append(home)\n    diff = np.nan_to_num(diff)\n    \n    return nn_model.predict_proba(np.array([diff]))[0][1]\n    #return calibrated_model.predict([diff])[0][1] # Depends on model(s) chosen\n\ndef nnCreatePrediction():\n#    if os.path.exists(""Data/Predictions/nn_results.csv""):\n#        print (\'There are already precomputed predictions.\')\n#    else:    \n        years = range(2018,2019)\n        listDictionaries = loadTeamVectors(years)\n        print (""Loaded the team vectors."")\n        results = [[0 for x in range(2)] for x in range(len(sample_sub_pd.index))]\n        for index, row in sample_sub_pd.iterrows():\n            matchupId = row[\'ID\']\n            year = int(matchupId[0:4]) \n            teamVectors = listDictionaries[year - years[0]]\n            team1Id = int(matchupId[5:9])\n            team2Id = int(matchupId[10:14])\n            team1Vector = teamVectors[team1Id] \n            team2Vector = teamVectors[team2Id]\n            pred = nnPredictGame(team1Vector, team2Vector, 0)\n            results[index][0] = matchupId\n            results[index][1] = pred\n        results = pd.np.array(results)\n        firstRow = [[0 for x in range(2)] for x in range(1)]\n        firstRow[0][0] = \'ID\'\n        firstRow[0][1] = \'Pred\'\n        with open(""Data/Predictions/nn_results.csv"", ""w"") as f:\n            writer = csv.writer(f)\n            writer.writerows(firstRow)\n            writer.writerows(results)\n            print(""Saved Results."")\n        \nnnCreatePrediction()\n\nclass_results = pd.read_csv(\'Data/Predictions/class_results.csv\')\nnn_results = pd.read_csv(\'Data/Predictions/nn_results.csv\')\ntrueskill_results = pd.read_csv(\'Data/Predictions/trueskill_results.csv\')\n\nfinal_predictions = sample_sub_pd\nfinal_predictions.Pred = (class_results.Pred * 0.40 + nn_results.Pred * 0.40 + trueskill_results.Pred * 0.20)\n\nfinal_predictions.to_csv(\'NCAA_Predictions.csv\', index=None)'"
prepare_dataset.py,12,"b'# Common imports\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport collections\nimport os\nimport sys\nimport math\nfrom math import pi\nfrom math import sqrt\nimport csv\nimport urllib\nimport pickle\nimport random\nimport statsmodels.api as sm\nfrom patsy import dmatrices\n\n# Math and descriptive stats\nfrom math import sqrt\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.stats.stats import pearsonr\nfrom scipy.special import boxcox1p, inv_boxcox1p\n\n# Sci-kit Learn modules for machine learning\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, log_loss\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.externals import joblib\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier, AdaBoostClassifier, AdaBoostRegressor\nfrom sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor, VotingClassifier\nfrom sklearn.ensemble import IsolationForest, RandomForestClassifier, RandomForestRegressor, RandomTreesEmbedding\nfrom sklearn.svm import SVR, LinearSVC, SVC, LinearSVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.decomposition import PCA, KernelPCA\n\n# Boosting libraries\nimport lightgbm as lgb\nimport xgboost\n\n# Deep Learning modules\nfrom keras.layers import Input, Dense, Dropout, Flatten, Embedding, merge, Activation\nfrom keras.layers import Convolution2D, MaxPooling2D, Convolution1D\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils, to_categorical\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport tensorflow as tf\n\nfrom trueskill import TrueSkill, Rating, rate_1vs1\n\nK = 20.\nHOME_ADVANTAGE = 100.\n\n# Kaggle provided data\nreg_season_compact_pd = pd.read_csv(\'Data/KaggleData/RegularSeasonCompactResults.csv\')\nseasons_pd = pd.read_csv(\'Data/KaggleData/Seasons.csv\')\nteams_pd = pd.read_csv(\'Data/KaggleData/Teams.csv\')\ntourney_compact_pd = pd.read_csv(\'Data/KaggleData/NCAATourneyCompactResults.csv\')\ntourney_detailed_pd = pd.read_csv(\'Data/KaggleData/NCAATourneyDetailedResults.csv\')\nconference_pd = pd.read_csv(\'Data/KaggleData/Conference.csv\')\ntourney_results_pd = pd.read_csv(\'Data/KaggleData/TourneyResults.csv\')\nsample_sub_pd = pd.read_csv(\'Data/KaggleData/sample_submission.csv\')\ntourney_seeds_pd = pd.read_csv(\'Data/KaggleData/NCAATourneySeeds.csv\')\nteam_conferences_pd = pd.read_csv(\'Data/KaggleData/TeamConferences.csv\')\nsample_sub_pd = pd.read_csv(\'Data/KaggleData/SampleSubmissionStage1.csv\')\nseeds_pd = pd.read_csv(\'Data/KaggleData/NCAATourneySeeds.csv\')\n# Data I created\nelos_ratings_pd = pd.read_csv(\'Data/Ratings/season_elos.csv\')\nenriched_pd = pd.read_csv(\'Data/KaggleData/NCAATourneyDetailedResultsEnriched.csv\')\n\n# Prelim stage 2 data\n#tourney_seeds_pd = pd.read_csv(\'Data/KaggleData/NCAATourneySeeds_SampleTourney2018.csv\')\n#seasons_pd = pd.read_csv(\'Data/KaggleData/Seasons_SampleTourney2018.csv\')\n#slots_pd = pd.read_csv(\'Data/KaggleData/NCAATourneySlots_SampleTourney2018.csv\')\n#sample_sub_pd = pd.read_csv(\'Data/KaggleData/SampleSubmissionStage2_SampleTourney2018.csv\')\n#reg_season_compact_pd = pd.read_csv(\'Data/KaggleData/RegularSeasonCompactResults_Prelim2018.csv\')\n#reg_season_detailed_pd = pd.read_csv(\'Data/KaggleData/RegularSeasonDetailedResults_Prelim2018.csv\')\n\n# Final stage 2 data\nreg_season_compact_pd = pd.read_csv(\'Data/Stage2UpdatedDataFiles/RegularSeasonCompactResults.csv\')\nseasons_pd = pd.read_csv(\'Data/Stage2UpdatedDataFiles/Seasons.csv\')\nteams_pd = pd.read_csv(\'Data/Stage2UpdatedDataFiles/Teams.csv\')\nslots_pd = pd.read_csv(\'Data/Stage2UpdatedDataFiles/NCAATourneySlots.csv\')\nseeds_pd = pd.read_csv(\'Data/Stage2UpdatedDataFiles/NCAATourneySeeds.csv\')\nreg_season_detailed_pd = pd.read_csv(\'Data/KaggleData/RegularSeasonDetailedResults.csv\')\ntourney_seeds_pd = pd.read_csv(\'Data/Stage2UpdatedDataFiles/NCAATourneySeeds.csv\')\nsample_sub_pd = pd.read_csv(\'Data/Stage2UpdatedDataFiles/SampleSubmissionStage2.csv\')\n# Data I created\nenriched_pd = pd.read_csv(\'Data/Stage2UpdatedDataFiles/NCAATourneyDetailedResultsEnriched2018.csv\')\n\ndef createTourneyFeats(): \n    # Advanced tournament data\n    df = pd.read_csv(\'Data/KaggleData/NCAATourneyDetailedResults.csv\')\n    # Points Winning/Losing Team\n    df[\'WPts\'] = df.apply(lambda row: 2*row.WFGM + row.WFGM3 + row.WFTM, axis=1)\n    df[\'LPts\'] = df.apply(lambda row: 2*row.LFGM + row.LFGM3 + row.LFTM, axis=1)\n    # Calculate Winning/losing Team Possesion Feature\n    wPos = df.apply(lambda row: 0.96*(row.WFGA + row.WTO + 0.44*row.WFTA - row.WOR), axis=1)\n    df[\'WPos\'] = df.apply(lambda row: 0.96*(row.WFGA + row.WTO + 0.44*row.WFTA - row.WOR), axis=1)\n    lPos = df.apply(lambda row: 0.96*(row.LFGA + row.LTO + 0.44*row.LFTA - row.LOR), axis=1)\n    df[\'LPos\'] = lPos = df.apply(lambda row: 0.96*(row.LFGA + row.LTO + 0.44*row.LFTA - row.LOR), axis=1)\n    df[\'Pos\'] = (wPos+lPos)/2\n    # Offensive efficiency (OffRtg) = 100 x (Points / Possessions)\n    df[\'WOffRtg\'] = df.apply(lambda row: 100 * (row.WPts / row.Pos), axis=1)\n    df[\'LOffRtg\'] = df.apply(lambda row: 100 * (row.LPts / row.Pos), axis=1)\n    # Defensive efficiency (DefRtg) = 100 x (Opponent points / Opponent possessions)\n    df[\'WDefRtg\'] = df.LOffRtg\n    df[\'LDefRtg\'] = df.WOffRtg\n    # Net Rating = Off.Rtg - Def.Rtg\n    df[\'WNetRtg\'] = df.apply(lambda row:(row.WOffRtg - row.WDefRtg), axis=1)\n    df[\'LNetRtg\'] = df.apply(lambda row:(row.LOffRtg - row.LDefRtg), axis=1)                       \n    # Assist Ratio : Percentage of team possessions that end in assists\n    df[\'WAstR\'] = df.apply(lambda row: 100 * row.WAst / (row.WFGA + 0.44*row.WFTA + row.WAst + row.WTO), axis=1)\n    df[\'LAstR\'] = df.apply(lambda row: 100 * row.LAst / (row.LFGA + 0.44*row.LFTA + row.LAst + row.LTO), axis=1)\n    # Turnover Ratio: Number of turnovers of a team per 100 possessions used.\n    # (TO * 100) / (FGA + (FTA * 0.44) + AST + TO\n    df[\'WTOR\'] = df.apply(lambda row: 100 * row.LAst / (row.LFGA + 0.44*row.LFTA + row.LAst + row.LTO), axis=1)\n    df[\'LTOR\'] = df.apply(lambda row: 100 * row.LAst / (row.LFGA + 0.44*row.LFTA + row.LAst + row.LTO), axis=1)                  \n    # The Shooting Percentage : Measure of Shooting Efficiency (FGA/FGA3, FTA)\n    df[\'WTSP\'] = df.apply(lambda row: 100 * row.WPts / (2 * (row.WFGA + 0.44 * row.WFTA)), axis=1)\n    df[\'LTSP\'] = df.apply(lambda row: 100 * row.LPts / (2 * (row.LFGA + 0.44 * row.LFTA)), axis=1)\n    # eFG% : Effective Field Goal Percentage adjusting for the fact that 3pt shots are more valuable \n    df[\'WeFGP\'] = df.apply(lambda row:(row.WFGM + 0.5 * row.WFGM3) / row.WFGA, axis=1)      \n    df[\'LeFGP\'] = df.apply(lambda row:(row.LFGM + 0.5 * row.LFGM3) / row.LFGA, axis=1)   \n    # FTA Rate : How good a team is at drawing fouls.\n    df[\'WFTAR\'] = df.apply(lambda row: row.WFTA / row.WFGA, axis=1)\n    df[\'LFTAR\'] = df.apply(lambda row: row.LFTA / row.LFGA, axis=1)                       \n    # OREB% : Percentage of team offensive rebounds\n    df[\'WORP\'] = df.apply(lambda row: row.WOR / (row.WOR + row.LDR), axis=1)\n    df[\'LORP\'] = df.apply(lambda row: row.LOR / (row.LOR + row.WDR), axis=1)\n    # DREB% : Percentage of team defensive rebounds\n    df[\'WDRP\'] = df.apply(lambda row: row.WDR / (row.WDR + row.LOR), axis=1)\n    df[\'LDRP\'] = df.apply(lambda row: row.LDR / (row.LDR + row.WOR), axis=1)                                      \n    # REB% : Percentage of team total rebounds\n    df[\'WRP\'] = df.apply(lambda row: (row.WDR + row.WOR) / (row.WDR + row.WOR + row.LDR + row.LOR), axis=1)\n    df[\'LRP\'] = df.apply(lambda row: (row.LDR + row.WOR) / (row.WDR + row.WOR + row.LDR + row.LOR), axis=1) \n    df[\'WPIE\'] = df.apply(lambda row: (row.WDR + row.WOR) / (row.WDR + row.WOR + row.LDR + row.LOR), axis=1)\n    wtmp = df.apply(lambda row: row.WPts + row.WFGM + row.WFTM - row.WFGA - row.WFTA + row.WDR + 0.5*row.WOR + row.WAst +row.WStl + 0.5*row.WBlk - row.WPF - row.WTO, axis=1)\n    ltmp = df.apply(lambda row: row.LPts + row.LFGM + row.LFTM - row.LFGA - row.LFTA + row.LDR + 0.5*row.LOR + row.LAst +row.LStl + 0.5*row.LBlk - row.LPF - row.LTO, axis=1) \n    df[\'WPIE\'] = wtmp/(wtmp + ltmp)\n    df[\'LPIE\'] = ltmp/(wtmp + ltmp)\n\n    df.to_csv(\'Data/Stage2UpdatedDataFiles/NCAATourneyDetailedResultsEnriched2018.csv\', index=False)\n\ndef createEloRating():\n    # Creating custom Elo ratings. This takes a long time to run so beware!\n    team_ids = set(reg_season_compact_pd.WTeamID).union(set(reg_season_compact_pd.LTeamID))\n\n    elo_dict = dict(zip(list(team_ids), [1500] * len(team_ids)))\n\n    reg_season_compact_pd[\'margin\'] = reg_season_compact_pd.WScore - reg_season_compact_pd.LScore\n    reg_season_compact_pd[\'w_elo\'] = None\n    reg_season_compact_pd[\'l_elo\'] = None\n\n    def elo_pred(elo1, elo2):\n        return(1. / (10. ** (-(elo1 - elo2) / 400.) + 1.))\n\n    def expected_margin(elo_diff):\n        return((7.5 + 0.006 * elo_diff))\n\n    def elo_update(w_elo, l_elo, margin):\n        elo_diff = w_elo - l_elo\n        pred = elo_pred(w_elo, l_elo)\n        mult = ((margin + 3.) ** 0.8) / expected_margin(elo_diff)\n        update = K * mult * (1 - pred)\n        return(pred, update)\n\n    assert np.all(reg_season_compact_pd.index.values == np.array(range(reg_season_compact_pd.shape[0]))), ""Index is out of order.""\n\n    preds = []\n\n    # Loop over all rows\n    for i in range(reg_season_compact_pd.shape[0]):\n\n        # Get key data from each row\n        w = reg_season_compact_pd.at[i, \'WTeamID\']\n        l = reg_season_compact_pd.at[i, \'LTeamID\']\n        margin = reg_season_compact_pd.at[i, \'margin\']\n        wloc = reg_season_compact_pd.at[i, \'WLoc\']\n\n        # Home court advantage?\n        w_ad, l_ad, = 0., 0.\n        if wloc == ""H"":\n            w_ad += HOME_ADVANTAGE\n        elif wloc == ""A"":\n            l_ad += HOME_ADVANTAGE\n\n        # Get elo updates as a result of each game\n        pred, update = elo_update(elo_dict[w] + w_ad,\n                                  elo_dict[l] + l_ad, \n                                  margin)\n        elo_dict[w] += update\n        elo_dict[l] -= update\n        preds.append(pred)\n\n        # Store elos in new dataframe\n        reg_season_compact_pd.loc[i, \'w_elo\'] = elo_dict[w]\n        reg_season_compact_pd.loc[i, \'l_elo\'] = elo_dict[l]\n\ndef seed_to_int(seed):\n# Convert seeds to integers\n    s_int = int(seed[1:3])\n    return s_int\nseeds_pd[\'seed_int\'] = seeds_pd.Seed.apply(seed_to_int)\nseeds_pd.drop(labels=[\'Seed\'], inplace=True, axis=1) # This is the string label\n\nteamList = teams_pd[\'TeamName\'].tolist()\nNCAAChampionsList = tourney_results_pd[\'NCAA Champion\'].tolist()\n\ndef Power6Conf(team_id):\n    team_pd = team_conferences_pd[(team_conferences_pd[\'Season\'] == 2018) & (team_conferences_pd[\'TeamID\'] == team_id)]\n    if (len(team_pd) == 0):\n        return 0\n    confName = team_pd.iloc[0][\'ConfAbbrev\']\n    return int(confName == \'sec\' or confName == \'acc\'or confName == \'big_ten\' or confName == \'big_twelve\' or confName == \'big_east\' or confName == \'pac_twelve\')\n\ndef createTeamName(team_id):\n    return teams_pd[teams_pd[\'TeamID\'] == team_id].values[0][1]\n\ndef findNumChampionships(team_id):\n    name = createTeamName(team_id)\n    return NCAAChampionsList.count(name)\n\ndef handleCases(arr):\n    indices = []\n    listLen = len(arr)\n    for i in range(listLen):\n        if (arr[i] == \'St\' or arr[i] == \'FL\'):\n            indices.append(i)\n    for p in indices:\n        arr[p-1] = arr[p-1] + \' \' + arr[p]\n    for i in range(len(indices)): \n        arr.remove(arr[indices[i] - i])\n    return arr\n\ndef checkConferenceChamp(team_id, year):\n    year_conf_pd = conference_pd[conference_pd[\'Year\'] == year]\n    champs = year_conf_pd[\'Regular Season Champ\'].tolist()\n# In case of a tie\n    champs_separated = [words for segments in champs for words in segments.split()]\n    name = createTeamName(team_id)\n    champs_separated = handleCases(champs_separated)\n    if (name in champs_separated):\n        return 1\n    else:\n        return 0\n\ndef checkConferenceTourneyChamp(team_id, year):\n    year_conf_pd = conference_pd[conference_pd[\'Year\'] == year]\n    champs = year_conf_pd[\'Tournament Champ\'].tolist()\n    name = createTeamName(team_id)\n    if (name in champs):\n        return 1\n    else:\n        return 0\n\ndef getTourneyAppearances(team_id):\n    return len(tourney_seeds_pd[tourney_seeds_pd[\'TeamID\'] == team_id].index)\n\n# Fixing names in csv\'s with differing formats\ndef handleDifferentCSV(df):\n    df[\'School\'] = df[\'School\'].replace(\'(State)\', \'St\', regex=True) \n    df[\'School\'] = df[\'School\'].replace(\'Albany (NY)\', \'Albany NY\') \n    df[\'School\'] = df[\'School\'].replace(\'Boston University\', \'Boston Univ\')\n    df[\'School\'] = df[\'School\'].replace(\'Central Michigan\', \'C Michigan\')\n    df[\'School\'] = df[\'School\'].replace(\'(Eastern)\', \'E\', regex=True)\n    df[\'School\'] = df[\'School\'].replace(\'Louisiana St\', \'LSU\')\n    df[\'School\'] = df[\'School\'].replace(\'North Carolina St\', \'NC State\')\n    df[\'School\'] = df[\'School\'].replace(\'Southern California\', \'USC\')\n    df[\'School\'] = df[\'School\'].replace(\'University of California\', \'California\', regex=True) \n    df[\'School\'] = df[\'School\'].replace(\'American\', \'American Univ\')\n    df[\'School\'] = df[\'School\'].replace(\'Arkansas-Little Rock\', \'Ark Little Rock\')\n    df[\'School\'] = df[\'School\'].replace(\'Arkansas-Pine Bluff\', \'Ark Pine Bluff\')\n    df[\'School\'] = df[\'School\'].replace(\'Bowling Green St\', \'Bowling Green\')\n    df[\'School\'] = df[\'School\'].replace(\'Brigham Young\', \'BYU\')\n    df[\'School\'] = df[\'School\'].replace(\'Cal Poly\', \'Cal Poly SLO\')\n    df[\'School\'] = df[\'School\'].replace(\'Centenary (LA)\', \'Centenary\')\n    df[\'School\'] = df[\'School\'].replace(\'Central Connecticut St\', \'Central Conn\')\n    df[\'School\'] = df[\'School\'].replace(\'Charleston Southern\', \'Charleston So\')\n    df[\'School\'] = df[\'School\'].replace(\'Coastal Carolina\', \'Coastal Car\')\n    df[\'School\'] = df[\'School\'].replace(\'College of Charleston\', \'Col Charleston\')\n    df[\'School\'] = df[\'School\'].replace(\'Cal St Fullerton\', \'CS Fullerton\')\n    df[\'School\'] = df[\'School\'].replace(\'Cal St Sacramento\', \'CS Sacramento\')\n    df[\'School\'] = df[\'School\'].replace(\'Cal St Bakersfield\', \'CS Bakersfield\')\n    df[\'School\'] = df[\'School\'].replace(\'Cal St Northridge\', \'CS Northridge\')\n    df[\'School\'] = df[\'School\'].replace(\'East Tennessee St\', \'ETSU\')\n    df[\'School\'] = df[\'School\'].replace(\'Detroit Mercy\', \'Detroit\')\n    df[\'School\'] = df[\'School\'].replace(\'Fairleigh Dickinson\', \'F Dickinson\')\n    df[\'School\'] = df[\'School\'].replace(\'Florida Atlantic\', \'FL Atlantic\')\n    df[\'School\'] = df[\'School\'].replace(\'Florida Gulf Coast\', \'FL Gulf Coast\')\n    df[\'School\'] = df[\'School\'].replace(\'Florida International\', \'Florida Intl\')\n    df[\'School\'] = df[\'School\'].replace(\'George Washington\', \'G Washington\')\n    df[\'School\'] = df[\'School\'].replace(\'Georgia Southern\', \'Ga Southern\')\n    df[\'School\'] = df[\'School\'].replace(\'Gardner-Webb\', \'Gardner Webb\')\n    df[\'School\'] = df[\'School\'].replace(\'Illinois-Chicago\', \'IL Chicago\')\n    df[\'School\'] = df[\'School\'].replace(\'Kent St\', \'Kent\')\n    df[\'School\'] = df[\'School\'].replace(\'Long Island University\', \'Long Island\')\n    df[\'School\'] = df[\'School\'].replace(\'Loyola Marymount\', \'Loy Marymount\')\n    df[\'School\'] = df[\'School\'].replace(\'Loyola (MD)\', \'Loyola MD\')\n    df[\'School\'] = df[\'School\'].replace(\'Loyola (IL)\', \'Loyola-Chicago\')\n    df[\'School\'] = df[\'School\'].replace(\'Massachusetts\', \'MA Lowell\')\n    df[\'School\'] = df[\'School\'].replace(\'Maryland-Eastern Shore\', \'MD E Shore\')\n    df[\'School\'] = df[\'School\'].replace(\'Miami (FL)\', \'Miami FL\')\n    df[\'School\'] = df[\'School\'].replace(\'Miami (OH)\', \'Miami OH\')\n    df[\'School\'] = df[\'School\'].replace(\'Missouri-Kansas City\', \'Missouri KC\')\n    df[\'School\'] = df[\'School\'].replace(\'Monmouth\', \'Monmouth NJ\')\n    df[\'School\'] = df[\'School\'].replace(\'Mississippi Valley St\', \'MS Valley St\')\n    df[\'School\'] = df[\'School\'].replace(\'Montana St\', \'MTSU\')\n    df[\'School\'] = df[\'School\'].replace(\'Northern Colorado\', \'N Colorado\')\n    df[\'School\'] = df[\'School\'].replace(\'North Dakota St\', \'N Dakota St\')\n    df[\'School\'] = df[\'School\'].replace(\'Northern Illinois\', \'N Illinois\')\n    df[\'School\'] = df[\'School\'].replace(\'Northern Kentucky\', \'N Kentucky\')\n    df[\'School\'] = df[\'School\'].replace(\'North Carolina A&T\', \'NC A&T\')\n    df[\'School\'] = df[\'School\'].replace(\'North Carolina Central\', \'NC Central\')\n    df[\'School\'] = df[\'School\'].replace(\'Pennsylvania\', \'Penn\')\n    df[\'School\'] = df[\'School\'].replace(\'South Carolina St\', \'S Carolina St\')\n    df[\'School\'] = df[\'School\'].replace(\'Southern Illinois\', \'S Illinois\')\n    df[\'School\'] = df[\'School\'].replace(\'UC-Santa Barbara\', \'Santa Barbara\')\n    df[\'School\'] = df[\'School\'].replace(\'Southeastern Louisiana\', \'SE Louisiana\')\n    df[\'School\'] = df[\'School\'].replace(\'Southeast Missouri St\', \'SE Missouri St\')\n    df[\'School\'] = df[\'School\'].replace(\'Stephen F. Austin\', \'SF Austin\')\n    df[\'School\'] = df[\'School\'].replace(\'Southern Methodist\', \'SMU\')\n    df[\'School\'] = df[\'School\'].replace(\'Southern Mississippi\', \'Southern Miss\')\n    df[\'School\'] = df[\'School\'].replace(\'Southern\', \'Southern Univ\')\n    df[\'School\'] = df[\'School\'].replace(\'St. Bonaventure\', \'St Bonaventure\')\n    df[\'School\'] = df[\'School\'].replace(\'St. Francis (NY)\', \'St Francis NY\')\n    df[\'School\'] = df[\'School\'].replace(\'Saint Francis (PA)\', \'St Francis PA\')\n    df[\'School\'] = df[\'School\'].replace(\'St. John\\\'s (NY)\', \'St John\\\'s\')\n    df[\'School\'] = df[\'School\'].replace(\'Saint Joseph\\\'s\', \'St Joseph\\\'s PA\')\n    df[\'School\'] = df[\'School\'].replace(\'Saint Louis\', \'St Louis\')\n    df[\'School\'] = df[\'School\'].replace(\'Saint Mary\\\'s (CA)\', \'St Mary\\\'s CA\')\n    df[\'School\'] = df[\'School\'].replace(\'Mount Saint Mary\\\'s\', \'Mt St Mary\\\'s\')\n    df[\'School\'] = df[\'School\'].replace(\'Saint Peter\\\'s\', \'St Peter\\\'s\')\n    df[\'School\'] = df[\'School\'].replace(\'Texas A&M-Corpus Christian\', \'TAM C. Christian\')\n    df[\'School\'] = df[\'School\'].replace(\'Texas Christian\', \'TCU\')\n    df[\'School\'] = df[\'School\'].replace(\'Tennessee-Martin\', \'TN Martin\')\n    df[\'School\'] = df[\'School\'].replace(\'Texas-Rio Grande Valley\', \'UTRGV\')\n    df[\'School\'] = df[\'School\'].replace(\'Texas Southern\', \'TX Southern\')\n    df[\'School\'] = df[\'School\'].replace(\'Alabama-Birmingham\', \'UAB\')\n    df[\'School\'] = df[\'School\'].replace(\'UC-Davis\', \'UC Davis\')\n    df[\'School\'] = df[\'School\'].replace(\'UC-Irvine\', \'UC Irvine\')\n    df[\'School\'] = df[\'School\'].replace(\'UC-Riverside\', \'UC Riverside\')\n    df[\'School\'] = df[\'School\'].replace(\'Central Florida\', \'UCF\')\n    df[\'School\'] = df[\'School\'].replace(\'Louisiana-Lafayette\', \'ULL\')\n    df[\'School\'] = df[\'School\'].replace(\'Louisiana-Monroe\', \'ULM\')\n    df[\'School\'] = df[\'School\'].replace(\'Maryland-Baltimore County\', \'UMBC\')\n    df[\'School\'] = df[\'School\'].replace(\'North Carolina-Asheville\', \'UNC Asheville\')\n    df[\'School\'] = df[\'School\'].replace(\'North Carolina-Greensboro\', \'UNC Greensboro\')\n    df[\'School\'] = df[\'School\'].replace(\'North Carolina-Wilmington\', \'UNC Wilmington\')\n    df[\'School\'] = df[\'School\'].replace(\'Nevada-Las Vegas\', \'UNLV\')\n    df[\'School\'] = df[\'School\'].replace(\'Texas-Arlington\', \'UT Arlington\')\n    df[\'School\'] = df[\'School\'].replace(\'Texas-San Antonio\', \'UT San Antonio\')\n    df[\'School\'] = df[\'School\'].replace(\'Texas-El Paso\', \'UTEP\')\n    df[\'School\'] = df[\'School\'].replace(\'Virginia Commonwealth\', \'VA Commonwealth\')\n    df[\'School\'] = df[\'School\'].replace(\'Western Carolina\', \'W Carolina\')\n    df[\'School\'] = df[\'School\'].replace(\'Western Illinois\', \'W Illinois\')\n    df[\'School\'] = df[\'School\'].replace(\'Western Kentucky\', \'WKU\')\n    df[\'School\'] = df[\'School\'].replace(\'Western Michigan\', \'W Michigan\')\n    df[\'School\'] = df[\'School\'].replace(\'Abilene Christian\', \'Abilene Chr\')\n    df[\'School\'] = df[\'School\'].replace(\'Montana State\', \'Montana St\')\n    df[\'School\'] = df[\'School\'].replace(\'Central Arkansas\', \'Cent Arkansas\')\n    df[\'School\'] = df[\'School\'].replace(\'Houston Baptist\', \'Houston Bap\')\n    df[\'School\'] = df[\'School\'].replace(\'South Dakota St\', \'S Dakota St\')\n    df[\'School\'] = df[\'School\'].replace(\'Maryland-Eastern Shore\', \'MD E Shore\')\n    return df\n\ndef createHomeStat(row):\n    if (row == \'H\'):\n        home = 1\n    if (row == \'A\'):\n        home = -1\n    if (row == \'N\'):\n        home = 0\n    return home\n\ndef createTrueskillRating():\n    ts = TrueSkill(draw_probability=0.01) # 0.01 is arbitary small number\n    beta = 25 / 6  # default value\n\n    def win_probability(p1, p2):\n        delta_mu = p1.mu - p2.mu\n        sum_sigma = p1.sigma * p1.sigma + p2.sigma * p2.sigma\n        denom = np.sqrt(2 * (beta * beta) + sum_sigma)\n        return ts.cdf(delta_mu / denom)\n\n    submit = sample_sub_pd\n    submit[[\'Season\', \'Team1\', \'Team2\']] = submit.apply(lambda r:pd.Series([int(t) for t in r.ID.split(\'_\')]), axis=1)\n\n    df_tour = reg_season_compact_pd\n    teamIds = np.unique(np.concatenate([df_tour.WTeamID.values, df_tour.LTeamID.values]))\n    ratings = { tid:ts.Rating() for tid in teamIds }\n\n    def feed_season_results(season):\n        print(""season = {}"".format(season))\n        df1 = df_tour[df_tour.Season == season]\n        for r in df1.itertuples():\n            ratings[r.WTeamID], ratings[r.LTeamID] = rate_1vs1(ratings[r.WTeamID], ratings[r.LTeamID])\n\n    def update_pred(season):\n        beta = np.std([r.mu for r in ratings.values()]) \n        print(""beta = {}"".format(beta))\n        submit.loc[submit.Season==season, \'Pred\'] = submit[submit.Season==season].apply(lambda r:win_probability(ratings[r.Team1], ratings[r.Team2]), axis=1)\n\n    for season in sorted(df_tour.Season.unique()[:-1]): # exclude last 4 years [:-4]/ last 1 year [:-1]\n        feed_season_results(season)\n\n#    update_pred(2014)\n#    feed_season_results(2014)\n#    update_pred(2015)\n#    feed_season_results(2015)\n#    update_pred(2016)\n#    feed_season_results(2016)\n#    update_pred(2017)\n    feed_season_results(2017)\n    update_pred(2018)\n\n    submit.drop([\'Season\', \'Team1\', \'Team2\'], axis=1, inplace=True)\n    submit.to_csv(\'Data/Predictions/trueskill_results2018.csv\', index=None)\n\ndef getSeasonTourneyData(team_id, year):\n    year_data_pd = reg_season_compact_pd[reg_season_compact_pd[\'Season\'] == year]\n# Elo   \n    year_pd = year_data_pd.copy()\n    year_pd = year_pd.loc[(year_pd.WTeamID == team_id) | (year_pd.LTeamID == team_id), :]\n    year_pd.sort_values([\'Season\', \'DayNum\'], inplace=True)\n    year_pd.drop_duplicates([\'Season\'], keep=\'last\', inplace=True)\n    w_mask = year_pd.WTeamID == team_id\n    l_mask = year_pd.LTeamID == team_id\n    year_pd[\'season_elo\'] = None\n    year_pd.loc[w_mask, \'season_elo\'] = year_pd.loc[w_mask, \'w_elo\']\n    year_pd.loc[l_mask, \'season_elo\'] = year_pd.loc[l_mask, \'l_elo\']\n    elo = year_pd.season_elo\n    elo = elo.values.mean()\n# Points per game\n    gamesWon = year_data_pd[year_data_pd.WTeamID == team_id] \n    totalPointsScored = gamesWon[\'WScore\'].sum()\n    gamesLost = year_data_pd[year_data_pd.LTeamID == team_id] \n    totalGames = gamesWon.append(gamesLost)\n    numGames = len(totalGames.index)\n    totalPointsScored += gamesLost[\'LScore\'].sum()\n# Number of points allowed\n    totalPointsAllowed = gamesWon[\'LScore\'].sum()\n    totalPointsAllowed += gamesLost[\'WScore\'].sum()\n# Scraped data    \n    stats_SOS_pd = pd.read_csv(\'Data/RegSeasonStats/MMStats_\'+str(year)+\'.csv\')\n    stats_SOS_pd = handleDifferentCSV(stats_SOS_pd)\n    ratings_pd = pd.read_csv(\'Data/RatingStats/RatingStats_\'+str(year)+\'.csv\')\n    ratings_pd = handleDifferentCSV(ratings_pd)\n    \n    name = createTeamName(team_id)\n    team = stats_SOS_pd[stats_SOS_pd[\'School\'] == name]\n    team_rating = ratings_pd[ratings_pd[\'School\'] == name]\n    if (len(team.index) == 0 or len(team_rating.index) == 0):\n        total3sMade = 0\n        totalTurnovers = 0\n        totalAssists = 0\n        sos = 0\n        totalRebounds = 0\n        srs = 0\n        totalSteals = 0\n    else:\n        total3sMade = team[\'X3P\'].values[0]\n        totalTurnovers = team[\'TOV\'].values[0]\n        if (math.isnan(totalTurnovers)):\n            totalTurnovers = 0\n        totalAssists = team[\'AST\'].values[0]\n        if (math.isnan(totalAssists)):\n            totalAssists = 0\n        sos = team[\'SOS\'].values[0]\n        srs = team[\'SRS\'].values[0]\n        totalRebounds = team[\'TRB\'].values[0]\n        if (math.isnan(totalRebounds)):\n            totalRebounds = 0\n        totalSteals = team[\'STL\'].values[0]\n        if (math.isnan(totalSteals)):\n            totalSteals = 0\n    \n# Finding tourney seed\n    tourneyYear = tourney_seeds_pd[tourney_seeds_pd[\'Season\'] == year]\n    seed = tourneyYear[tourneyYear[\'TeamID\'] == team_id]\n    if (len(seed.index) != 0):\n        seed = seed.values[0][1]\n        tournamentSeed = int(seed[1:3])\n    else:\n        tournamentSeed = 25\n\n# Number of wins and losses\n    numWins = len(gamesWon.index)\n# Preventing division by 0\n    if numGames == 0:\n        avgPointsScored = 0\n        avgPointsAllowed = 0\n        avg3sMade = 0\n        avgTurnovers = 0\n        avgAssists = 0\n        avgRebounds = 0\n        avgSteals = 0\n    else:\n        avgPointsScored = totalPointsScored/numGames\n        avgPointsAllowed = totalPointsAllowed/numGames\n        avg3sMade = total3sMade/numGames\n        avgTurnovers = totalTurnovers/numGames\n        avgAssists = totalAssists/numGames\n        avgRebounds = totalRebounds/numGames\n        avgSteals = totalSteals/numGames\n        \n# Tourney data   \n    enriched_df = enriched_pd[enriched_pd[\'Season\'] == year]\n    enriched_df = enriched_df.loc[(enriched_df.WTeamID == team_id) | (enriched_df.LTeamID == team_id), :]\n    w_mask = enriched_df.WTeamID == team_id\n    l_mask = enriched_df.LTeamID == team_id\n    enriched_df[\'Score\'] = 0\n    enriched_df[\'FGM\'] = 0\n    enriched_df[\'FGA\'] = 0\n    enriched_df[\'FGM3\'] = 0\n    enriched_df[\'FGA3\'] = 0\n    enriched_df[\'FTM\'] = 0\n    enriched_df[\'FTA\'] = 0\n    enriched_df[\'OR\'] = 0\n    enriched_df[\'DR\'] = 0\n    enriched_df[\'Ast\'] = 0\n    enriched_df[\'TO\'] = 0\n    enriched_df[\'Stl\'] = 0\n    enriched_df[\'Blk\'] = 0\n    enriched_df[\'PF\'] = 0\n    enriched_df[\'Pts\'] = 0\n    enriched_df[\'Pos\'] = 0\n    enriched_df[\'OffRtg\'] = 0\n    enriched_df[\'DefRtg\'] = 0\n    enriched_df[\'NetRtg\'] = 0\n    enriched_df[\'AstR\'] = 0\n    enriched_df[\'TOR\'] = 0\n    enriched_df[\'TSP\'] = 0\n    enriched_df[\'eFGP\'] = 0\n    enriched_df[\'FTAR\'] = 0\n    enriched_df[\'ORP\'] = 0\n    enriched_df[\'DRP\'] = 0\n    enriched_df[\'RP\'] = 0\n    enriched_df[\'PIE\'] = 0\n    enriched_df.loc[w_mask, \'Score\'] = enriched_df.loc[w_mask, \'WScore\']\n    enriched_df.loc[l_mask, \'Score\'] = enriched_df.loc[l_mask, \'LScore\']\n    Score = enriched_df.Score.values.mean()\n    enriched_df.loc[w_mask, \'FGM\'] = enriched_df.loc[w_mask, \'WFGM\']\n    enriched_df.loc[l_mask, \'FGM\'] = enriched_df.loc[l_mask, \'LFGM\']\n    FGM = enriched_df.FGM.values.mean()\n    enriched_df.loc[w_mask, \'FGA\'] = enriched_df.loc[w_mask, \'WFGA\']\n    enriched_df.loc[l_mask, \'FGA\'] = enriched_df.loc[l_mask, \'LFGA\']\n    FGA = enriched_df.FGA.values.mean()\n    enriched_df.loc[w_mask, \'FGM3\'] = enriched_df.loc[w_mask, \'WFGM3\']\n    enriched_df.loc[l_mask, \'FGM3\'] = enriched_df.loc[l_mask, \'LFGM3\']\n    FGM3 = enriched_df.FGM3.values.mean()\n    enriched_df.loc[w_mask, \'FGA3\'] = enriched_df.loc[w_mask, \'WFGA3\']\n    enriched_df.loc[l_mask, \'FGA3\'] = enriched_df.loc[l_mask, \'LFGA3\']\n    FGA3 = enriched_df.FGA3.values.mean()\n    enriched_df.loc[w_mask, \'FTM\'] = enriched_df.loc[w_mask, \'WFTM\']\n    enriched_df.loc[l_mask, \'FTM\'] = enriched_df.loc[l_mask, \'LFTM\']\n    FTM = enriched_df.FTM.values.mean()\n    enriched_df.loc[w_mask, \'FTA\'] = enriched_df.loc[w_mask, \'WFTA\']\n    enriched_df.loc[l_mask, \'FTA\'] = enriched_df.loc[l_mask, \'LFTA\']\n    FTA = enriched_df.FTA.values.mean()\n    enriched_df.loc[w_mask, \'OR\'] = enriched_df.loc[w_mask, \'WOR\']\n    enriched_df.loc[l_mask, \'OR\'] = enriched_df.loc[l_mask, \'LOR\']\n    OR = enriched_df.OR.values.mean()\n    enriched_df.loc[w_mask, \'DR\'] = enriched_df.loc[w_mask, \'WDR\']\n    enriched_df.loc[l_mask, \'DR\'] = enriched_df.loc[l_mask, \'LDR\']\n    DR = enriched_df.DR.values.mean()\n    enriched_df.loc[w_mask, \'Ast\'] = enriched_df.loc[w_mask, \'WAst\']\n    enriched_df.loc[l_mask, \'Ast\'] = enriched_df.loc[l_mask, \'LAst\']\n    Ast = enriched_df.Ast.values.mean()\n    enriched_df.loc[w_mask, \'TO\'] = enriched_df.loc[w_mask, \'WTO\']\n    enriched_df.loc[l_mask, \'TO\'] = enriched_df.loc[l_mask, \'LTO\']\n    TO = enriched_df.TO.values.mean()\n    enriched_df.loc[w_mask, \'Stl\'] = enriched_df.loc[w_mask, \'WStl\']\n    enriched_df.loc[l_mask, \'Stl\'] = enriched_df.loc[l_mask, \'LStl\']\n    Stl = enriched_df.Stl.values.mean()\n    enriched_df.loc[w_mask, \'Blk\'] = enriched_df.loc[w_mask, \'WBlk\']\n    enriched_df.loc[l_mask, \'Blk\'] = enriched_df.loc[l_mask, \'LBlk\']\n    Blk = enriched_df.Blk.values.mean()\n    enriched_df.loc[w_mask, \'PF\'] = enriched_df.loc[w_mask, \'WPF\']\n    enriched_df.loc[l_mask, \'PF\'] = enriched_df.loc[l_mask, \'LPF\']\n    PF = enriched_df.PF.values.mean()\n    enriched_df.loc[w_mask, \'Pts\'] = enriched_df.loc[w_mask, \'WPts\']\n    enriched_df.loc[l_mask, \'Pts\'] = enriched_df.loc[l_mask, \'LPts\']\n    Pts = enriched_df.Pts.values.mean()\n    enriched_df.loc[w_mask, \'Pos\'] = enriched_df.loc[w_mask, \'WPos\']\n    enriched_df.loc[l_mask, \'Pos\'] = enriched_df.loc[l_mask, \'LPos\']\n    Pos = enriched_df.Pos.values.mean()\n    enriched_df.loc[w_mask, \'OffRtg\'] = enriched_df.loc[w_mask, \'WOffRtg\']\n    enriched_df.loc[l_mask, \'OffRtg\'] = enriched_df.loc[l_mask, \'LOffRtg\']\n    OffRtg = enriched_df.OffRtg.values.mean()\n    enriched_df.loc[w_mask, \'DefRtg\'] = enriched_df.loc[w_mask, \'WDefRtg\']\n    enriched_df.loc[l_mask, \'DefRtg\'] = enriched_df.loc[l_mask, \'LDefRtg\']\n    DefRtg = enriched_df.DefRtg.values.mean()\n    enriched_df.loc[w_mask, \'NetRtg\'] = enriched_df.loc[w_mask, \'WNetRtg\']\n    enriched_df.loc[l_mask, \'NetRtg\'] = enriched_df.loc[l_mask, \'LNetRtg\']\n    NetRtg = enriched_df.NetRtg.values.mean()\n    enriched_df.loc[w_mask, \'AstR\'] = enriched_df.loc[w_mask, \'WAstR\']\n    enriched_df.loc[l_mask, \'AstR\'] = enriched_df.loc[l_mask, \'LAstR\']\n    AstR = enriched_df.AstR.values.mean()\n    enriched_df.loc[w_mask, \'TOR\'] = enriched_df.loc[w_mask, \'WTOR\']\n    enriched_df.loc[l_mask, \'TOR\'] = enriched_df.loc[l_mask, \'LTOR\']\n    TOR = enriched_df.TOR.values.mean()\n    enriched_df.loc[w_mask, \'TSP\'] = enriched_df.loc[w_mask, \'WTSP\']\n    enriched_df.loc[l_mask, \'TSP\'] = enriched_df.loc[l_mask, \'LTSP\']\n    TSP = enriched_df.TSP.values.mean()\n    enriched_df.loc[w_mask, \'eFGP\'] = enriched_df.loc[w_mask, \'WeFGP\']\n    enriched_df.loc[l_mask, \'eFGP\'] = enriched_df.loc[l_mask, \'LeFGP\']\n    eFGP = enriched_df.eFGP.values.mean()\n    enriched_df.loc[w_mask, \'FTAR\'] = enriched_df.loc[w_mask, \'WFTAR\']\n    enriched_df.loc[l_mask, \'FTAR\'] = enriched_df.loc[l_mask, \'LFTAR\']\n    FTAR = enriched_df.FTAR.values.mean()\n    enriched_df.loc[w_mask, \'ORP\'] = enriched_df.loc[w_mask, \'WORP\']\n    enriched_df.loc[l_mask, \'ORP\'] = enriched_df.loc[l_mask, \'LORP\']\n    ORP = enriched_df.ORP.values.mean()\n    enriched_df.loc[w_mask, \'DRP\'] = enriched_df.loc[w_mask, \'WDRP\']\n    enriched_df.loc[l_mask, \'DRP\'] = enriched_df.loc[l_mask, \'LDRP\']\n    DRP = enriched_df.DRP.values.mean()\n    enriched_df.loc[w_mask, \'RP\'] = enriched_df.loc[w_mask, \'WRP\']\n    enriched_df.loc[l_mask, \'RP\'] = enriched_df.loc[l_mask, \'LRP\']\n    RP = enriched_df.RP.values.mean()\n    enriched_df.loc[w_mask, \'PIE\'] = enriched_df.loc[w_mask, \'WPIE\']\n    enriched_df.loc[l_mask, \'PIE\'] = enriched_df.loc[l_mask, \'LPIE\']\n    PIE = enriched_df.PIE.values.mean()\n     \n    return [numWins, avgPointsScored, avgPointsAllowed, Power6Conf(team_id), avg3sMade, avgAssists, avgTurnovers,\n            checkConferenceChamp(team_id, year), checkConferenceTourneyChamp(team_id, year), tournamentSeed,\n            sos, srs, avgRebounds, avgSteals, getTourneyAppearances(team_id), findNumChampionships(team_id), elo,\n            FGM, FGA, FGM3, FGA3, FTM, FTA, OR, DR, Ast, TO, Stl, Blk, PF, Pts, Pos, OffRtg, DefRtg, NetRtg, Score,\n            AstR, TOR, TSP, eFGP, FTAR, ORP, DRP, RP, PIE, numWins * 2, avgPointsScored * 2, avgPointsAllowed * 2, \n            Power6Conf(team_id) * 2, avg3sMade * 2, avgAssists * 2, avgTurnovers * 2,\n            checkConferenceChamp(team_id, year) * 2, checkConferenceTourneyChamp(team_id, year) * 2, \n            tournamentSeed * 2, sos * 2, srs * 2, avgRebounds * 2, avgSteals * 2, getTourneyAppearances(team_id) * 2, \n            findNumChampionships(team_id) * 2, elo * 2, FGM * 2, FGA * 2, FGM3 * 2, FGA3 * 2, FTM * 2, FTA * 2,\n            OR * 2, DR * 2, Ast * 2, TO * 2, Stl * 2, Blk * 2, PF * 2, Pts * 2, Pos * 2, OffRtg * 2, DefRtg * 2, \n            NetRtg * 2, Score * 2, AstR * 2, TOR * 2, TSP * 2, eFGP * 2, FTAR * 2, ORP * 2, DRP * 2, RP * 2, \n            PIE * 2, numWins * 3, avgPointsScored * 3, avgPointsAllowed * 3, Power6Conf(team_id) * 3, \n            avg3sMade * 3, avgAssists * 3, avgTurnovers * 3, checkConferenceChamp(team_id, year) * 3, \n            checkConferenceTourneyChamp(team_id, year) * 3, tournamentSeed * 3, sos * 3, srs * 3, avgRebounds * 3,\n            avgSteals * 3, getTourneyAppearances(team_id) * 3, findNumChampionships(team_id) * 3, elo * 3, FGM * 3,\n            FGA * 3, FGM3 * 3, FGA3 * 3, FTM * 3, FTA * 3, OR * 3, DR * 3, Ast * 3, TO * 3, Stl * 3, Blk * 3,\n            PF * 3, Pts * 3, Pos * 3, OffRtg * 3, DefRtg * 3, NetRtg * 3, Score * 3, AstR * 3, TOR * 3, TSP * 3,\n            eFGP * 3, FTAR * 3, ORP * 3, DRP * 3, RP * 3, PIE * 3]\n\ndef compareTwoTeams(id_1, id_2, year):\n    team_1 = getSeasonTourneyData(id_1, year)\n    team_2 = getSeasonTourneyData(id_2, year)\n    diff = [a - b for a, b in zip(team_1, team_2)]\n    return diff\n\ndef createStatDict(year):\n    statDictionary = collections.defaultdict(list)\n    for team in teamList:\n        team_id = teams_pd[teams_pd[\'TeamName\'] == team].values[0][0]\n        team_vector = getSeasonTourneyData(team_id, year)\n        statDictionary[team_id] = team_vector\n    return statDictionary\n\ndef createTrainingSet(years, stage1Years, Stage2Year):\n    createTourneyFeats()\n    createEloRating()\n    createTrueskillRating()\n    totalNumGames = 0\n    for year in years:\n        season = reg_season_compact_pd[reg_season_compact_pd[\'Season\'] == year]\n        totalNumGames += len(season.index)\n        tourney = tourney_compact_pd[tourney_compact_pd[\'Season\'] == year]\n        totalNumGames += len(tourney.index)\n    numFeatures = len(getSeasonTourneyData(1181,2012))\n    X_train = np.zeros(( totalNumGames, numFeatures + 1))\n    y_train = np.zeros(( totalNumGames ))\n    indexCounter = 0\n    for year in years:\n        team_vectors = createStatDict(year)\n        season = reg_season_compact_pd[reg_season_compact_pd[\'Season\'] == year]\n        numGamesInSeason = len(season.index)\n        tourney = tourney_compact_pd[tourney_compact_pd[\'Season\'] == year]\n        numGamesInSeason += len(tourney.index)\n        xTrainSeason = np.zeros(( numGamesInSeason, numFeatures + 1))\n        yTrainSeason = np.zeros(( numGamesInSeason ))\n        counter = 0\n        for index, row in season.iterrows():\n            w_team = row[\'WTeamID\']\n            w_vector = team_vectors[w_team]\n            l_team = row[\'LTeamID\']\n            l_vector = team_vectors[l_team]\n            diff = [a - b for a, b in zip(w_vector, l_vector)]\n            home = createHomeStat(row[\'WLoc\'])\n            if (counter % 2 == 0):\n                diff.append(home) \n                xTrainSeason[counter] = diff\n                yTrainSeason[counter] = 1\n            else:\n                diff.append(-home)\n                xTrainSeason[counter] = [ -p for p in diff]\n                yTrainSeason[counter] = 0\n            counter += 1\n        for index, row in tourney.iterrows():\n            w_team = row[\'WTeamID\']\n            w_vector = team_vectors[w_team]\n            l_team = row[\'LTeamID\']\n            l_vector = team_vectors[l_team]\n            diff = [a - b for a, b in zip(w_vector, l_vector)]\n            home = 0\n            if (counter % 2 == 0):\n                diff.append(home) \n                xTrainSeason[counter] = diff\n                yTrainSeason[counter] = 1\n            else:\n                diff.append(-home)\n                xTrainSeason[counter] = [ -p for p in diff]\n                yTrainSeason[counter] = 0\n            counter += 1\n        X_train[indexCounter:numGamesInSeason+indexCounter] = xTrainSeason\n        y_train[indexCounter:numGamesInSeason+indexCounter] = yTrainSeason\n        indexCounter += numGamesInSeason\n        print (\'Finished year:\', year)\n        if (year in stage1Years):\n            np.save(\'Data/PrecomputedMatrices/TeamVectors/\' + str(year) + \'TeamVectors\', team_vectors)\n        if (year == stage2Year):\n            np.save(\'Data/PrecomputedMatrices/Stage2/\' + str(year) + \'TeamVectors\', team_vectors)\n    return X_train, y_train\n\ndef createAndSave(years, stage1Years, stage2Year):\n    X_train, y_train = createTrainingSet(years, stage1Years, stage2Year)\n    np.save(\'Data/PrecomputedMatrices/X_train\', X_train)\n    np.save(\'Data/PrecomputedMatrices/y_train\', y_train)\n\nyears = range(1994,2019)\n# Saves the team vectors for the following years\nstage1Years = range(2014,2018)\nstage2Year = 2018\nif os.path.exists(""Data/PrecomputedMatrices/X_train.npy"") and os.path.exists(""Data/PrecomputedMatrices/y_train.npy""):\n    print (\'There are already precomputed X_train, and y_train matricies.\')\n    os.remove(""Data/PrecomputedMatrices/X_train.npy"")\n    os.remove(""Data/PrecomputedMatrices/y_train.npy"")\n    createAndSave(years, stage1Years, stage2Year)\nelse:\n    createAndSave(years, stage1Years, stage2Year)'"
train_model.py,11,"b'import prepare_dataset\n\n# Common imports\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport collections\nimport os\nimport sys\nimport math\nfrom math import pi\nfrom math import sqrt\nimport csv\nimport urllib\nimport pickle\nimport random\nimport statsmodels.api as sm\nfrom patsy import dmatrices\n\n# Math and descriptive stats\nfrom math import sqrt\nfrom scipy import stats\n\n# Sci-kit Learn modules for machine learning\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.externals import joblib\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier, AdaBoostClassifier, AdaBoostRegressor\nfrom sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor, VotingClassifier\nfrom sklearn.ensemble import IsolationForest, RandomForestClassifier, RandomForestRegressor, RandomTreesEmbedding\nfrom sklearn.svm import SVR, LinearSVC, SVC, LinearSVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.decomposition import PCA, KernelPCA\n\n# Boosting libraries\nimport lightgbm as lgb\nimport xgboost\n\n# Deep Learning modules\nfrom keras.layers import Input, Dense, Dropout, Flatten, Embedding, merge, Activation\nfrom keras.layers import Convolution2D, MaxPooling2D, Convolution1D\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils, to_categorical\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport tensorflow as tf\n\nfrom trueskill import TrueSkill, Rating, rate_1vs1\n\nXtrain = np.load(""Data/PrecomputedMatrices/X_train.npy"")\nytrain = np.load(""Data/PrecomputedMatrices/y_train.npy"")\nXtrain = np.nan_to_num(Xtrain)\nytrain = np.nan_to_num(ytrain)\n\nX_train, X_test, Y_train, Y_test = train_test_split(Xtrain, ytrain)\n\nss = StandardScaler()\nX_train_scaled = ss.fit_transform(X_train)\nX_test_scaled = ss.transform(X_test)\n\nlog = LogisticRegression(random_state=95)\n\nknn = KNeighborsClassifier(algorithm=\'kd_tree\', leaf_size=18, metric=\'minkowski\',\n                           metric_params=None, n_jobs=1, n_neighbors=60, p=1,\n                           weights=\'uniform\')\n\nrfc = RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\'gini\',\n                             max_depth=40, max_features=\'log2\', max_leaf_nodes=200,\n                             min_impurity_decrease=0, min_impurity_split=None,\n                             min_samples_leaf=4, min_samples_split=500,\n                             min_weight_fraction_leaf=0, n_estimators=1000, n_jobs=1,\n                             oob_score=False, random_state=None, verbose=0,\n                             warm_start=False)\n\netrees = ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion=\'gini\',\n                              max_depth=40, max_features=\'log2\', max_leaf_nodes=200,\n                              min_impurity_decrease=0, min_impurity_split=None,\n                              min_samples_leaf=4, min_samples_split=500,\n                              min_weight_fraction_leaf=0, n_estimators=1000, n_jobs=1,\n                              oob_score=False, random_state=None, verbose=0, warm_start=False)\n\ngbc = GradientBoostingClassifier(criterion=\'friedman_mse\', init=None,\n                                 learning_rate=0.1, loss=\'deviance\', max_depth=3,\n                                 max_features=None, max_leaf_nodes=16,\n                                 min_impurity_decrease=0.2, min_impurity_split=None,\n                                 min_samples_leaf=220, min_samples_split=2,\n                                 min_weight_fraction_leaf=0, n_estimators=150, presort=\'auto\',\n                                 random_state=None, subsample=1, verbose=0, warm_start=False)\n\nlgbm = lgb.LGBMClassifier(bagging_freq=0, bagging_seed=95, boosting_type=\'gbdt\',\n                          class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,\n                          feature_fraction_seed=95, lambda_l1=0.0, lambda_l2=0.0,\n                          learning_rate=0.1, max_depth=12, min_child_samples=20,\n                          min_child_weight=0.001, min_data_in_leaf=10, min_split_gain=0,\n                          n_estimators=100, n_jobs=-1, num_boost_round=40, num_leaves=25,\n                          num_threads=4, objective=None, random_state=None, reg_alpha=0.0,\n                          reg_lambda=0.0, silent=True, subsample=1.0,\n                          subsample_for_bin=200000, subsample_freq=1)\n\ndef model_function(layer_one_neurons=184, layer_two_neurons=184, layer_three_neurons=184): \n    model = Sequential()\n    model.add(Dense(layer_one_neurons, input_dim=X_train.shape[1], activation=\'relu\'))\n    model.add(Dense(layer_two_neurons, activation=\'relu\'))\n    model.add(Dense(layer_three_neurons, activation=\'relu\'))\n    model.add(Dense(1, activation=\'sigmoid\'))\n    model.compile(loss=\'binary_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n    return model\n\nnn_model = KerasClassifier(build_fn=model_function, epochs=200)\n\n# Soft Voting to get the probabilities\nmodel_soft = VotingClassifier(estimators=[(\'gbc\', gbc), (\'rfr\', rfr), (\'etrees\', etrees),\n                                          (\'knn\', knn), (\'lgbm\', lgbm), (\'log\', log)], voting=\'soft\', n_jobs=-1)\n\nmodel_soft.fit(X_train, Y_train)\nnn_model.fit(X_train_scaled, Y_train)\n\ndef classPredictGame(team_1_vector, team_2_vector, home):\n    diff = [a - b for a, b in zip(team_1_vector, team_2_vector)]\n    diff.append(home)\n    diff = np.nan_to_num(diff)\n    \n    return model_soft.predict_proba(np.array([diff]))[0][1]\n    #return calibrated_model.predict([diff])[0][1] # Depends on model(s) chosen\n\ndef loadTeamVectors(years):\n    listDictionaries = []\n    for year in years:\n        curVectors = np.load(""Data/PrecomputedMatrices/Stage2/"" + str(year) + ""TeamVectors.npy"").item()\n        listDictionaries.append(curVectors)\n    return listDictionaries\n\ndef classCreatePrediction():\n#    if os.path.exists(""Data/Predictions/class_results.csv""):\n#        print (\'There are already precomputed predictions.\')\n#    else:    \n        years = range(2018,2019)\n        listDictionaries = loadTeamVectors(years)\n        print (""Loaded the team vectors."")\n        results = [[0 for x in range(2)] for x in range(len(sample_sub_pd.index))]\n        for index, row in sample_sub_pd.iterrows():\n            matchupId = row[\'ID\']\n            year = int(matchupId[0:4]) \n            teamVectors = listDictionaries[year - years[0]]\n            team1Id = int(matchupId[5:9])\n            team2Id = int(matchupId[10:14])\n            team1Vector = teamVectors[team1Id] \n            team2Vector = teamVectors[team2Id]\n            pred = classPredictGame(team1Vector, team2Vector, 0)\n            results[index][0] = matchupId\n            results[index][1] = pred\n        results = pd.np.array(results)\n        firstRow = [[0 for x in range(2)] for x in range(1)]\n        firstRow[0][0] = \'ID\'\n        firstRow[0][1] = \'Pred\'\n        with open(""Data/Predictions/class_results.csv"", ""w"") as f:\n            writer = csv.writer(f)\n            writer.writerows(firstRow)\n            writer.writerows(results)\n            print(""Saved Results."")\n\nclassCreatePrediction()\n\ndef nnPredictGame(team_1_vector, team_2_vector, home):\n    diff = [a - b for a, b in zip(team_1_vector, team_2_vector)]\n    diff.append(home)\n    diff = np.nan_to_num(diff)\n    \n    return nn_model.predict_proba(np.array([diff]))[0][1]\n    #return calibrated_model.predict([diff])[0][1] # Depends on model(s) chosen\n\ndef nnCreatePrediction():\n#    if os.path.exists(""Data/Predictions/nn_results.csv""):\n#        print (\'There are already precomputed predictions.\')\n#    else:    \n        years = range(2018,2019)\n        listDictionaries = loadTeamVectors(years)\n        print (""Loaded the team vectors."")\n        results = [[0 for x in range(2)] for x in range(len(sample_sub_pd.index))]\n        for index, row in sample_sub_pd.iterrows():\n            matchupId = row[\'ID\']\n            year = int(matchupId[0:4]) \n            teamVectors = listDictionaries[year - years[0]]\n            team1Id = int(matchupId[5:9])\n            team2Id = int(matchupId[10:14])\n            team1Vector = teamVectors[team1Id] \n            team2Vector = teamVectors[team2Id]\n            pred = nnPredictGame(team1Vector, team2Vector, 0)\n            results[index][0] = matchupId\n            results[index][1] = pred\n        results = pd.np.array(results)\n        firstRow = [[0 for x in range(2)] for x in range(1)]\n        firstRow[0][0] = \'ID\'\n        firstRow[0][1] = \'Pred\'\n        with open(""Data/Predictions/nn_results.csv"", ""w"") as f:\n            writer = csv.writer(f)\n            writer.writerows(firstRow)\n            writer.writerows(results)\n            print(""Saved Results."")\n        \nnnCreatePrediction()\n\nclass_results = pd.read_csv(\'Data/Predictions/class_results.csv\')\nnn_results = pd.read_csv(\'Data/Predictions/nn_results.csv\')\ntrueskill_results = pd.read_csv(\'Data/Predictions/trueskill_results.csv\')\n\nfinal_predictions = sample_sub_pd\nfinal_predictions.Pred = (class_results.Pred * 0.40 + nn_results.Pred * 0.40 + trueskill_results.Pred * 0.20)\n\nfinal_predictions.to_csv(\'NCAA_Predictions.csv\', index=None)'"
