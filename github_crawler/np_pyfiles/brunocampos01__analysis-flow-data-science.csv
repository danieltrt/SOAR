file_path,api_count,code
flow_analysis/src/__init__.py,0,b''
flow_analysis/src/setup.py,0,"b'# Create packages to prepare deploy.\nfrom io import open\nfrom os import path\n\nfrom setuptools import find_packages, setup\n\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, \'../README.md\')) as f:\n    long_description = f.read()\n\nwith open(path.join(here, \'../requirements.txt\')) as f:\n    requirements = f.read()\n\nsetup(\n    name=here,\n    version=\'0.1\',\n    author=\'brunocampos01\',\n    author_email=\'brunocampos01@gmail.com\',\n    url=""https://github.com/brunocampos01"",\n    description=\'PREPARE DEPLOY\',\n    long_description=long_description,\n    license=\'MIT\',\n    packages=find_packages(),\n    scripts=[\'environment/make\',\n             \'environment/prepare_environment.py\',\n             \'environment/show_config_environment.sh\'\n             \'environment/show_struture_project.sh\'],\n    install_requires=requirements,\n    classifiers=[\n        \'Development Status :: Production/Stable\',\n        \'Environment :: Jupyter-notebook\',\n        \'Environment :: Web Environment\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Data Scientist\',\n        \'License :: OSI Approved :: MIT\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Topic :: System :: Data Science\',\n    ],\n)\n'"
flow_analysis/src/dump_data/dump_data.py,0,"b'import datetime\nimport json\nimport os\nimport re\nimport time\nfrom urllib.error import HTTPError\nfrom urllib.error import URLError\nfrom urllib.request import urlopen\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\ndef get_date_today():\n    now = datetime.datetime.now()\n    now = str(now.strftime(""%Y-%m-%d""))\n    return now\n\n\ndef get_links_csv(url):\n    """"""\n    Function to download links csv in page\n    """"""\n    print(\'Try analysing page ...\')\n    links_pg = []\n\n    try:\n        # Testing connection\n        requests.get(url, timeout=5)\n\n        html = urlopen(url)\n\n        # Create a parser instance to navigate\n        page = BeautifulSoup(html, ""html.parser"")\n\n        # find in tree HTML\n        for link in page.find_all(\'a\',\n                                  attrs={\'href\': re.compile(""csv"")}):\n            links_pg.append(link.attrs[\'href\'])\n            print(link.attrs[\'href\'])\n\n        return links_pg\n\n    except HTTPError as e:\n        print(\'HTTP error\', e)\n    except URLError as e:\n        print(\'Server not found, \', e)\n\n\ndef dump_file_csv(url):\n    """"""\n    Create file data.csv\n    """"""\n    list_links_csv = get_links_csv(url)\n\n    # Serialize csv in data/\n    data = \'data/dumps/\'\n\n    for link in list_links_csv:\n        # last string\n        file_name = link.split(\'/\')[-1]\n\n        # create response object\n        r = requests.get(link, stream=True)\n\n        # download started\n        with open(data + file_name, \'wb\') as f:\n            for chunk in r.iter_content():\n                f.write(chunk)\n        print(f""{data + file_name} downloaded!"")\n'"
flow_analysis/src/dump_data/etl_PETR4.py,2,"b'# coding: utf-8\nimport datetime\n\nimport numpy as np\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom sqlalchemy import create_engine\n\n# Global\nyear = datetime.date.today().year\nmonth = datetime.date.today().month\nday = datetime.date.today().day\ndata_inicial = (f\'{day}/{month}/{year - 10}\')\ndata_final = (f\'{day}/{month}/{year}\')\nativo = \'PETR4\'\nurl = (f""https://www.infomoney.com.br/Pages/Download/Download.aspx?""\n       f""dtIni={data_inicial}&dtFinish={data_final}""\n       f""&Ativo={ativo}""\n       f""&Semana=null""\n       f""&Per=null""\n       f""&type=2""\n       f""&Stock={ativo}""\n       f""&StockType=1"")\n\n\ndef prepare_url(url: str):\n    """"""\n    Constru\xc3\xa7\xc3\xa3o da URL\n    :return: http response\n    """"""\n    response = requests.get(url, timeout=60)\n\n    print(""get page: "", response)\n    return response\n\n\ndef create_parser(response: str):\n    """"""\n    Cria\xc3\xa7\xc3\xa3o do parser para navega\xc3\xa7\xc3\xa3o na \xc3\xa1rvore DOM\n    :return: bs4.element\n    """"""\n    content = BeautifulSoup(response.content, ""html.parser"")\n    return content.find_all(""tr"")\n\n\ndef remove_multiple_spaces(string):\n    """"""\n    Se \xc3\xa9 String, ent\xc3\xa3o faz remo\xc3\xa7\xc3\xa3o de espa\xc3\xa7os em branco\n    replace: / por espa\xc3\xa7o em branco\n    replace: , por . para facilitar a convers\xc3\xa3o de tipos\n    replace: todas as palavras que entraram na tabela\n    """"""\n    if type(string) == str:\n        string_n = \' \'.join(string.split()) \\\n            .replace(\'/\', \' \') \\\n            .replace(\',\', \'.\')\n\n        string = string_n \\\n            .replace(\'Fech.\', \'-1\') \\\n            .replace(\'Var.Dia\',\n                     \'-1\')\n        return string\n\n    return string\n\n\ndef generate_table(table: \'bs4.element.ResultSet\'):\n    # neste dataframe vou armazenar a data e o fechamento di\xc3\xa1rio\n    df_petr4 = pd.DataFrame(columns=[\'Ano\', \'Fechamento\'])\n\n    # percorre tabela\n    for row in table:\n        # .text vem do requests e garante o scrapping\n        text = row.text\n        text = remove_multiple_spaces(text)\n\n        # insere dados diferentes a cada linha\n        text_dados = text.split(sep="" "")\n\n        # quando \xc3\xa9 feito o slice do text_dados \xc3\xa9 retornado um list\n        ano = np.asarray(text_dados[2:3])\n        fechamento = np.asarray(text_dados[3:4])\n\n        # vari\xc3\xa1vel p organizar a inser\xc3\xa7\xc3\xa3o no dataframe\n        dados = pd.DataFrame([[ano, fechamento]],\n                             columns=[\'Ano\', \'Fechamento\'])\n        df_petr4 = df_petr4.append(dados)\n\n    # remove linhas com Strings\n    df_petr4 = df_petr4[1:]\n\n    # Convers\xc3\xa3o de tipos object\n    df_petr4.Ano = df_petr4.Ano.astype(\n        \'int16\')  # int8 n\xc3\xa3o aceitou, pequeno demais\n    df_petr4.Fechamento = df_petr4.Fechamento.astype(\'float16\')\n\n    """""" Granularidade anual: \n    - tanto a taxa selic quanto a PETR4 devem ser de mesma granularidade\n    - n\xc3\xadvel do gr\xc3\xa3o = ano\n    """"""\n    # dataframe somente com os fechamentos do ano\n    df_petr4_clean = pd.DataFrame(columns=[\'Ano\', \'Fechamento\'])\n\n    # for serve para get fechamento do ano\n    for ano in range(year - 9, year + 1):\n        df_petr4_year = df_petr4[(df_petr4[\'Ano\'] == ano)]\n        fechamento_ano = df_petr4_year.Fechamento.iloc[0]\n\n        # vari\xc3\xa1vel p organizar a inser\xc3\xa7\xc3\xa3o no dataframe\n        dados = pd.DataFrame([[ano, fechamento_ano]],\n                             columns=[\'Ano\', \'Fechamento\'])\n        df_petr4_clean = df_petr4_clean.append(dados)\n\n    print(df_petr4_clean.info())\n    return df_petr4_clean\n\n\ndef dataframe_to_sqlite(ativo: str, dataframe: \'dataframe\'):\n    engine = create_engine(\'sqlite:///data/desafio_AAWZ.db\')\n    dataframe.to_sql(ativo, con=engine,\n              if_exists=\'replace\',\n              index=False)\n    print(f\'\\n{ativo} salvo!\')\n\n\ndef main():\n    response = prepare_url(url=url)\n    parser_bs = create_parser(response)\n    df = generate_table(parser_bs)\n    dataframe_to_sqlite(ativo=ativo, dataframe=df)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
flow_analysis/src/environment/__init__.py,0,b''
flow_analysis/src/environment/jupyter_notebook_config.py,0,"b""c.NotebookApp.ip = '0.0.0.0'\n\n# The port the notebook server will listen on\nc.NotebookApp.port = 8888\n\n# Whether to open in a browser after starting\nc.NotebookApp.open_browser = False\n\n# Set the Access-Control-Allow-Credentials: true header\nc.NotebookApp.allow_password_change = False\n\n# https://github.com/jupyter/notebook/issues/3130\nc.FileContentsManager.delete_to_trash = False\n"""
flow_analysis/src/environment/prepare_env.py,0,"b'import os\n\n\ndef save_requirements():\n    command = os.popen(\'bash src/environment/create_requirements.sh\')\n    print(command.read())\n    print(50 * \'-\')\n\n\ndef save_tree_directory():\n    command = os.popen(""bash src/environment/show_struture_project.sh"")\n    print(command.read())\n    print(50 * \'-\')\n\n\ndef save_config():\n    command = os.popen(\'bash src/environment/show_config_environment.sh\')\n    print(command.read())\n    print(50 * \'-\')\n\n\ndef main():\n    save_requirements()\n    save_config()\n    save_tree_directory()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
flow_analysis/src/environment/test_environment.py,0,"b'import sys\n\n# This project need python 3\nREQUIRED_PYTHON = ""python3""\n\n\ndef check_python_version(python_req: str):\n    # major = 3\n    system_major = sys.version_info.major\n\n    if python_req == ""python"":\n        required_major = 2\n    elif python_req == ""python3"":\n        required_major = 3\n    else:\n        raise ValueError(""Unrecognized python interpreter: {}""\n                         .format(python_req))\n\n    if system_major != required_major:\n        raise TypeError(""This project requires Python {}. Found: Python {}""\n                        .format(required_major, sys.version))\n    else:\n        print(""Python Version: {}"".format(sys.version))  # e.g 3.7.3\n        print("">>> Development environment passes all tests!"")\n\n\ndef main():\n    check_python_version(python_req = REQUIRED_PYTHON)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
flow_analysis/src/pyanalytics/__init__.py,0,b''
flow_analysis/src/visualization/visuals.py,5,"b'###########################################\n# Suppress matplotlib user warnings\n# Necessary for newer version of matplotlib\nimport warnings\n\nwarnings.filterwarnings(""ignore"", category=UserWarning,\n                        module=""matplotlib"")\n\n# Display inline matplotlib plots with IPython\nfrom IPython import get_ipython\n\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\n###########################################\n\nimport matplotlib.pyplot as pl\nimport matplotlib.patches as mpatches\nimport numpy as np\n\n\ndef distribution(data, transformed=False):\n    """"""\n    Visualization code for displaying skewed distributions of features\n    """"""\n\n    # Create figure\n    fig = pl.figure(figsize=(11, 5))\n\n    # Skewed feature plotting\n    for i, feature in enumerate([\'capital-gain\', \'capital-loss\']):\n        ax = fig.add_subplot(1, 2, i + 1)\n        ax.hist(data[feature], bins=25, color=\'#00A0A0\')\n        ax.set_title(""\'%s\' Feature Distribution"" % feature,\n                     fontsize=14)\n        ax.set_xlabel(""Value"")\n        ax.set_ylabel(""Number of Records"")\n        ax.set_ylim((0, 2000))\n        ax.set_yticks([0, 500, 1000, 1500, 2000])\n        ax.set_yticklabels([0, 500, 1000, 1500, "">2000""])\n\n    # Plot aesthetics\n    if transformed:\n        fig.suptitle(\n            ""Log-transformed Distributions of Continuous Census Data Features"",\n            fontsize=16, y=1.03)\n    else:\n        fig.suptitle(\n            ""Skewed Distributions of Continuous Census Data Features"",\n            fontsize=16, y=1.03)\n\n    fig.tight_layout()\n    fig.show()\n\n\ndef evaluate(results, accuracy, f1):\n    """"""\n    Visualization code to display results of various learners.\n\n    inputs:\n      - learners: a list of supervised learners\n      - stats: a list of dictionaries of the statistic results from \'train_predict()\'\n      - accuracy: The score for the naive predictor\n      - f1: The score for the naive predictor\n    """"""\n\n    # Create figure\n    fig, ax = pl.subplots(2, 3, figsize=(11, 7))\n\n    # Constants\n    bar_width = 0.3\n    colors = [\'#A00000\', \'#00A0A0\', \'#00A000\']\n\n    # Super loop to plot four panels of data\n    for k, learner in enumerate(results.keys()):\n        for j, metric in enumerate(\n            [\'train_time\', \'acc_train\', \'f_train\', \'pred_time\',\n             \'acc_test\', \'f_test\']):\n            for i in np.arange(3):\n                # Creative plot code\n                ax[int(j / 3), int(j % 3)].bar(i + k * bar_width,\n                                               results[learner][i][\n                                                   metric],\n                                               width=bar_width,\n                                               color=colors[k])\n                ax[int(j / 3), int(j % 3)].set_xticks(\n                    [0.45, 1.45, 2.45])\n                ax[int(j / 3), int(j % 3)].set_xticklabels(\n                    [""1%"", ""10%"", ""100%""])\n                ax[int(j / 3), int(j % 3)].set_xlabel(\n                    ""Training Set Size"")\n                ax[int(j / 3), int(j % 3)].set_xlim((-0.1, 3.0))\n\n    # Add unique y-labels\n    ax[0, 0].set_ylabel(""Time (in seconds)"")\n    ax[0, 1].set_ylabel(""Accuracy Score"")\n    ax[0, 2].set_ylabel(""F-score"")\n    ax[1, 0].set_ylabel(""Time (in seconds)"")\n    ax[1, 1].set_ylabel(""Accuracy Score"")\n    ax[1, 2].set_ylabel(""F-score"")\n\n    # Add titles\n    ax[0, 0].set_title(""Model Training"")\n    ax[0, 1].set_title(""Accuracy Score on Training Subset"")\n    ax[0, 2].set_title(""F-score on Training Subset"")\n    ax[1, 0].set_title(""Model Predicting"")\n    ax[1, 1].set_title(""Accuracy Score on Testing Set"")\n    ax[1, 2].set_title(""F-score on Testing Set"")\n\n    # Add horizontal lines for naive predictors\n    ax[0, 1].axhline(y=accuracy, xmin=-0.1, xmax=3.0, linewidth=1,\n                     color=\'k\', linestyle=\'dashed\')\n    ax[1, 1].axhline(y=accuracy, xmin=-0.1, xmax=3.0, linewidth=1,\n                     color=\'k\', linestyle=\'dashed\')\n    ax[0, 2].axhline(y=f1, xmin=-0.1, xmax=3.0, linewidth=1, color=\'k\',\n                     linestyle=\'dashed\')\n    ax[1, 2].axhline(y=f1, xmin=-0.1, xmax=3.0, linewidth=1, color=\'k\',\n                     linestyle=\'dashed\')\n\n    # Set y-limits for score panels\n    ax[0, 1].set_ylim((0, 1))\n    ax[0, 2].set_ylim((0, 1))\n    ax[1, 1].set_ylim((0, 1))\n    ax[1, 2].set_ylim((0, 1))\n\n    # Create patches for the legend\n    patches = []\n    for i, learner in enumerate(results.keys()):\n        patches.append(mpatches.Patch(color=colors[i], label=learner))\n    pl.legend(handles=patches, bbox_to_anchor=(-.80, 2.53),\n              loc=\'upper center\', borderaxespad=0., ncol=3,\n              fontsize=\'x-large\')\n\n    # Aesthetics\n    pl.suptitle(\n        ""Performance Metrics for Three Supervised Learning Models"",\n        fontsize=16, y=1.10)\n    pl.tight_layout()\n    pl.show()\n\n\ndef feature_plot(importances, X_train, y_train):\n    # Display the five most important features\n    indices = np.argsort(importances)[::-1]\n    columns = X_train.columns.values[indices[:5]]\n    values = importances[indices][:5]\n\n    # Creat the plot\n    fig = pl.figure(figsize=(9, 5))\n    pl.title(\n        ""Normalized Weights for First Five Most Predictive Features"",\n        fontsize=16)\n    pl.bar(np.arange(5), values, width=0.6, align=""center"",\n           color=\'#00A000\',\n           label=""Feature Weight"")\n    pl.bar(np.arange(5) - 0.3, np.cumsum(values), width=0.2,\n           align=""center"", color=\'#00A0A0\',\n           label=""Cumulative Feature Weight"")\n    pl.xticks(np.arange(5), columns)\n    pl.xlim((-0.5, 4.5))\n    pl.ylabel(""Weight"", fontsize=12)\n    pl.xlabel(""Feature"", fontsize=12)\n\n    pl.legend(loc=\'upper center\')\n    pl.tight_layout()\n    pl.show()\n'"
