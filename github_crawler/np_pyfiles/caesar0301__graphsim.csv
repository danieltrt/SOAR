file_path,api_count,code
setup.py,0,"b'import os\nfrom setuptools import setup, find_packages\n\n__version__ = \'0.2.12\'\n__author__ = \'Xiaming Chen\'\n__email__ = \'chenxm35@gmail.com\'\n\n# build libtacsim automatically\nrootdir = os.path.dirname(os.path.realpath(\'__file__\'))\nmoddir = os.path.join(rootdir, \'libtacsim\')\nres = os.system(\'cd %s && scons install && cd -\' % moddir)\nif res > 0:\n    raise RuntimeError(\'Failed to build libtacsim.\')\n\nsetup(\n    name=""graphsim"",\n    version=__version__,\n    url=\'https://github.com/caesar0301/graphsim\',\n    author=__author__,\n    author_email=__email__,\n    description=\'Graph similarity algorithms based on NetworkX.\',\n    long_description=open(os.path.join(os.path.dirname(__file__), \'README.md\')).read(),\n    license=""BSC License"",\n    packages=find_packages(),\n    keywords=[\'graph\', \'graph similarity\', \'graph matching\'],\n    install_requires=[\n        \'networkx==1.11\',\n        \'numpy>=1.13\',\n        \'typedecorator>=0.0.4\'\n    ],\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Science/Research\',\n\t\'License :: OSI Approved :: BSD License\',\n        \'Operating System :: MacOS\',\n        \'Operating System :: Unix\',\n        \'Programming Language :: C\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ],\n)\n'"
graphsim/__init__.py,0,b'from .iter import *\n'
tests/test_gs.py,0,"b""import networkx as nx\n\nimport graphsim as gs\n\nif __name__ == '__main__':\n    G1 = nx.DiGraph()\n    G1.add_weighted_edges_from([(1,0,8), (0,2,12), (1,2,10), (2,3,15)])\n    G1.node[0]['weight'] = 1\n    G1.node[1]['weight'] = 1\n    G1.node[2]['weight'] = 5\n    G1.node[3]['weight'] = 1\n\n    G2 = nx.DiGraph()\n    G2.add_weighted_edges_from([(0,1,15), (1,2,10)])\n    G2.node[0]['weight'] = 1\n    G2.node[1]['weight'] = 3\n    G2.node[2]['weight'] = 1\n\n    print(gs.tacsim_in_C(G1, G2))\n\n    print(gs.tacsim_in_C(G1))\n\n    print(gs.tacsim_combined_in_C(G1, G2))\n"""
graphsim/iter/ASCOS.py,0,"b'""""""\nASCOS similarity measure\n""""""\n#!/usr/bin/env python\n# Copyright (C) 2004-2010 by\n# Hung-Hsuan Chen <hhchen@psu.edu>\n# All rights reserved.\n# BSD license.\n# NetworkX:http://networkx.lanl.gov/.\n__author__ = """"""Hung-Hsuan Chen (hhchen@psu.edu)""""""\n\nimport copy\nimport math\nimport networkx as nx\nimport numpy\n\n__all__ = [\'ascos\']\n\n\ndef ascos(G, c=0.9, max_iter=100, is_weighted=False, remove_neighbors=False, remove_self=False, dump_process=False):\n    """"""Return the ASCOS similarity between nodes\n\n    Parameters\n    -----------\n    G: graph\n      A NetworkX graph\n    c: float, 0 < c <= 1\n      The number represents the relative importance between in-direct neighbors\n      and direct neighbors\n    max_iter: integer\n      The number specifies the maximum number of iterations for ASCOS\n      calculation\n    is_weighted: boolean\n      Whether use weighted ASCOS or not\n    remove_neighbors: boolean\n      if true, the similarity value between neighbor nodes is set to zero\n    remove_self: boolean\n      if true, the similarity value between a node and itself is set to zero\n    dump_process: boolean\n      if true, the calculation process is dumped\n\n    Returns\n    -------\n    node_ids : list of node ids\n    sim : numpy matrix\n      sim[i,j] is the similarity value between node_ids[i] and node_ids[j]\n\n    Examples\n    --------\n    >>> G = nx.Graph()\n    >>> G.add_edges_from([(0,7), (0,1), (0,2), (0,3), (1,4), (2,4), (3,4), (4,5), (4,6)])\n    >>> networkx_addon.similarity.ascos(G)\n\n    Notes\n    -----\n\n    References\n    ----------\n    [1] ASCOS: an Asymmetric network Structure COntext Similarity measure.\n    Hung-Hsuan Chen and C. Lee Giles.    ASONAM 2013\n    """"""\n\n    if type(G) == nx.MultiGraph or type(G) == nx.MultiDiGraph:\n        raise Exception(""ascos() not defined for graphs with multiedges."")\n\n    if G.is_directed():\n        raise Exception(""ascos() not defined for directed graphs."")\n\n    node_ids = G.nodes()\n    node_id_lookup_tbl = { }\n    for i, n in enumerate(node_ids):\n        node_id_lookup_tbl[n] = i\n\n    nb_ids = [G.neighbors(n) for n in node_ids]\n    nbs = [ ]\n    for nb_id in nb_ids:\n        nbs.append([node_id_lookup_tbl[n] for n in nb_id])\n    del(node_id_lookup_tbl)\n\n    n = G.number_of_nodes()\n    sim = numpy.eye(n)\n    sim_old = numpy.zeros(shape = (n, n))\n\n    for iter_ctr in range(max_iter):\n        if _is_converge(sim, sim_old, n, n):\n            break\n        sim_old = copy.deepcopy(sim)\n        for i in range(n):\n            if dump_process:\n                print(iter_ctr, \':\', i, \'/\', n)\n            for j in range(n):\n                if not is_weighted:\n                    if i == j:\n                        continue\n                    s_ij = 0.0\n                    for n_i in nbs[i]:\n                        s_ij += sim_old[n_i, j]\n                    sim[i, j] = c * s_ij / len(nbs[i]) if len(nbs[i]) > 0 else 0\n                else:\n                    if i == j:\n                        continue\n                    s_ij = 0.0\n                    for n_i in nbs[i]:\n                        w_ik = G[node_ids[i]][node_ids[n_i]][\'weight\'] if \'weight\' in G[node_ids[i]][node_ids[n_i]] else 1\n                        s_ij += float(w_ik) * (1 - math.exp(-w_ik)) * sim_old[n_i, j]\n\n                    w_i = G.degree(weight=\'weight\')[node_ids[i]]\n                    sim[i, j] = c * s_ij / w_i if w_i > 0 else 0\n\n    if remove_self:\n        for i in range(n):\n            sim[i,i] = 0\n\n    if remove_neighbors:\n        for i in range(n):\n            for j in nbs[i]:\n                sim[i,j] = 0\n\n    return node_ids, sim\n\ndef _is_converge(sim, sim_old, nrow, ncol, eps=1e-4):\n    for i in range(nrow):\n        for j in range(ncol):\n            if abs(sim[i,j] - sim_old[i,j]) >= eps:\n                return False\n    return True\n'"
graphsim/iter/BVD04.py,7,"b'#!/usr/bin/env python\n# Copyright (C) 2015 by\n# Xiaming Chen <chen_xm@sjtu.edu.cn>\n# All rights reserved.\n# BSD license.\nimport itertools\n\nimport numpy as np\nimport networkx as nx\nfrom typedecorator import params, returns\n\n__author__ = ""Xiaming Chen""\n__email__ = ""chen_xm@sjtu.edu.cn""\n\n__all__ = [ \'nsim_bvd04\' ]\n\n\n@params(G1=nx.DiGraph, G2=nx.DiGraph, max_iter=int, eps=float)\ndef nsim_bvd04(G1, G2, max_iter=100, eps=1e-4):\n    """"""\n    Algorithm to calculate node-node similarity matrix of\n    two directed graphs.\n\n    Return\n    ------\n    A 2d similarity matrix of |V1| x |V2|.\n\n    Reference\n    ---------\n    Blondel, Vincent D. et al. ""A Measure of Similarity between Graph Vertices:\n    Applications to Synonym Extraction and Web Searching."" SIAM Review (2004)\n    """"""\n    N = len(G1.nodes())\n    M = len(G2.nodes())\n    A = nx.adjacency_matrix(G1).todense()\n    B = nx.adjacency_matrix(G2).todense()\n    nsim_prev = np.zeros((M, N))\n    nsim = np.ones((M, N))\n\n    for i in range(max_iter):\n        if np.allclose(nsim, nsim_prev, atol=eps):\n            break\n\n        nsim_prev = np.copy(nsim)\n        nsim = np.dot(np.dot(B, nsim_prev), A.T) + \\\n            np.dot(np.dot(B.T, nsim_prev), A)\n\n        fnorm = np.linalg.norm(nsim, ord=\'fro\')\n        nsim = nsim / fnorm\n\n    print(""Converge after %d iterations (eps=%f)."" % (i, eps))\n\n    return nsim.T\n\n\nif __name__ == \'__main__\':\n    # Example of Fig. 1.2 in paper BVD04.\n    G1 = nx.DiGraph()\n    G1.add_edges_from([(1,2), (2,1), (1,3), (4,1), (2,3), (3,2), (4,3)])\n\n    G2 = nx.DiGraph()\n    G2.add_edges_from([(1,4), (1,3), (3,1), (6,1), (6,4), (6,3), (3,6), (2,4), (2,6), (3,5)])\n    nsim = nsim_bvd04(G1, G2)\n    print(nsim)\n'"
graphsim/iter/HITS.py,11,"b'#!/usr/bin/env python\n# Copyright (C) 2015 by\n# Xiaming Chen <chen_xm@sjtu.edu.cn>\n# All rights reserved.\n# BSD license.\nimport itertools\n\nimport numpy as np\nimport networkx as nx\nfrom typedecorator import params, returns\n\n__author__ = ""Xiaming Chen""\n__email__ = ""chen_xm@sjtu.edu.cn""\n\n__all__ = [ \'hits\' ]\n\n\ndef normalized(a, axis=0, order=2):\n    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n    l2[l2==0] = 1\n    return a / np.expand_dims(l2, axis)\n\n\n@params(G=nx.DiGraph, max_iter=int, eps=float)\ndef hits(G, max_iter=100, eps=1e-4):\n    """"""HITS algorithm:\n    calculate the hub and authority scores for nodes in a graph.\n\n    Return\n    ------\n    A 2d matrix of [hub, auth] scores.\n\n    Reference\n    ---------\n    [1] Kleinberg, Jon M. ""Authoritative Sources in a Hyperlinked\n        Environment."" JACM, 1999.\n    """"""\n    N = len(G.nodes())\n    A = nx.adjacency_matrix(G).todense()\n    Mu = np.concatenate((np.zeros((N, N)), A), 1)\n    Md = np.concatenate((A.T, np.zeros((N, N))), 1)\n    M = np.concatenate((Mu, Md), 0)\n\n    ha_prev = np.zeros((N*2, 1))\n    ha = np.ones((N*2, 1))\n\n    for i in range(max_iter):\n        if np.allclose(ha, ha_prev, atol=eps):\n            break\n        ha_prev = np.copy(ha)\n        ha = normalized(np.dot(M, ha_prev))\n\n    print(""Converge after %d iterations (eps=%f)."" % (i, eps))\n\n    return np.reshape(ha, newshape=(N, 2), order=1)\n\n\nif __name__ == \'__main__\':\n    G = nx.DiGraph()\n    G.add_edges_from([(1,4), (1,5), (1,6), (2,5), (2,7), (3,4), (3,5), (3,6), (3,7)])\n    print(hits(G))\n'"
graphsim/iter/HS03.py,15,"b'#!/usr/bin/env python\n# Copyright (C) 2015 by\n# Xiaming Chen <chen_xm@sjtu.edu.cn>\n# All rights reserved.\n# BSD license.\nimport itertools\n\nimport numpy as np\nimport networkx as nx\nfrom typedecorator import params, returns\n\n__author__ = ""Xiaming Chen""\n__email__ = ""chen_xm@sjtu.edu.cn""\n\n__all__ = [\'nsim_hs03\']\n\n\n@params(G1=nx.DiGraph, G2=nx.DiGraph, max_iter=int, eps=float)\ndef nsim_hs03(G1, G2, max_iter=100, eps=1e-4):\n    """"""\n    References\n    ----------\n    [1] Heymans, M., Singh, A. Deriving phylogenetic trees from the similarity\n        analysis of metabolic pathways. Bioinformatics, 2003\n    """"""\n    N = len(G1.nodes())\n    M = len(G2.nodes())\n    A = nx.adjacency_matrix(G1).todense()\n    B = nx.adjacency_matrix(G2).todense()\n    Ia = np.ones((N, N))\n    Ib = np.ones((M, M))\n\n    nsim_prev = np.zeros((M, N))\n    nsim = np.ones((M, N))\n\n    for i in range(max_iter):\n        if np.allclose(nsim, nsim_prev, atol=eps):\n            break\n\n        nsim_prev = np.copy(nsim)\n        nsim = \\\n            np.dot(np.dot(B, nsim_prev), A.T) + \\\n            np.dot(np.dot(B.T, nsim_prev), A) + \\\n            np.dot(np.dot((Ib-B), nsim_prev), (Ia-A).T) + \\\n            np.dot(np.dot((Ib-B).T, nsim_prev), (Ia-A)) - \\\n            np.dot(np.dot(B, nsim_prev), (Ia-A).T) - \\\n            np.dot(np.dot(B.T, nsim_prev), (Ia-A)) - \\\n            np.dot(np.dot((Ib-B), nsim_prev), A.T) - \\\n            np.dot(np.dot((Ib-B).T, nsim_prev), A)\n\n        fnorm = np.linalg.norm(nsim, ord=\'fro\')\n        nsim = nsim / fnorm\n\n    print(""Converge after %d iterations (eps=%f)."" % (i, eps))\n\n    return nsim.T\n\nif __name__ == \'__main__\':\n    # Example of Fig. 1.2 in paper BVD04.\n    G1 = nx.DiGraph()\n    G1.add_edges_from([(1,2), (2,1), (1,3), (4,1), (2,3), (3,2), (4,3)])\n\n    G2 = nx.DiGraph()\n    G2.add_edges_from([(1,4), (1,3), (3,1), (6,1), (6,4), (6,3), (3,6), (2,4), (2,6), (3,5)])\n    nsim = nsim_hs03(G1, G2)\n    print(nsim)\n'"
graphsim/iter/SimCpl.py,0,"b""import itertools\n\nimport numpy as np\nimport networkx as nx\nfrom typedecorator import params, returns\n\n\n__all__ = [ 'similarity_coupling' ]\n\n\n@params(G1=nx.DiGraph, G2=nx.DiGraph, max_iter=int, eps=float)\ndef similarity_coupling(G1, G2, max_iter=100, eps=1e-4):\n    pass"""
graphsim/iter/SimFld.py,0,"b'import itertools\n\nimport numpy as np\nimport networkx as nx\nfrom typedecorator import params, returns\n\n\n__all__ = [ \'similarity_flooding\' ]\n\n\ndef similarity_flooding(G):\n    """""" A generalization of algorithm BVD04.\n\n    Reference\n    ---------\n    [1] Melnik, Sergey, Hector Garcia-Molina, and Erhard Rahm.\n        ""Similarity Flooding: A Versatile Graph Matching Algorithm and\n        Its Application to Schema Matching."" ICDE, 2002.\n    """"""\n    pass\n\n\nif __name__ == \'__main__\':\n    pass'"
graphsim/iter/SimRank.py,8,"b'""""""\nSimRank similarity measure.\n""""""\n#!/usr/bin/env python\n# Copyright (C) 2015 by\n# Xiaming Chen <chen_xm@sjtu.edu.cn>\n# All rights reserved.\n# BSD license.\nimport itertools\n\nimport numpy as np\nimport networkx as nx\nfrom typedecorator import params, returns\n\n__author__ = ""Xiaming Chen""\n__email__ = ""chen_xm@sjtu.edu.cn""\n\n__all__ = [ \'simrank\', \'simrank_bipartite\' ]\n\n\n@params(G=nx.Graph, r=float, max_iter=int, eps=float)\ndef simrank(G, r=0.8, max_iter=100, eps=1e-4):\n    """""" Algorithm of G. Jeh and J. Widom. SimRank: A Measure\n    of Structural-Context Similarity. In KDD\'02.\n\n    Thanks to Jon Tedesco\'s answer in SO question #9767773.\n    """"""\n    if isinstance(G, nx.MultiGraph):\n        assert(""The SimRank of MultiGraph is not supported."")\n\n    if isinstance(G, nx.MultiDiGraph):\n        assert(""The SimRank of MultiDiGraph is not supported."")\n\n    directed = False\n    if isinstance(G, nx.DiGraph):\n        directed = True\n\n    nodes = G.nodes()\n    nodes_i = {}\n    for (k, v) in [(nodes[i], i) for i in range(0, len(nodes))]:\n        nodes_i[k] = v\n\n    sim_prev = np.zeros(len(nodes))\n    sim = np.identity(len(nodes))\n\n    for i in range(max_iter):\n        if np.allclose(sim, sim_prev, atol=eps):\n            break\n\n        sim_prev = np.copy(sim)\n        for u, v in itertools.product(nodes, nodes):\n            if u is v: continue\n\n            if directed:\n                u_ns, v_ns = G.predecessors(u), G.predecessors(v)\n            else:\n                u_ns, v_ns = G.neighbors(u), G.neighbors(v)\n\n            # Evaluating the similarity of current nodes pair\n            if len(u_ns) == 0 or len(v_ns) == 0:\n                sim[nodes_i[u]][nodes_i[v]] = 0\n            else:\n                s_uv = sum([sim_prev[nodes_i[u_n]][nodes_i[v_n]] for u_n, v_n in itertools.product(u_ns, v_ns)])\n                sim[nodes_i[u]][nodes_i[v]] = (r * s_uv) / (len(u_ns) * len(v_ns))\n\n    print(""Converge after %d iterations (eps=%f)."" % (i, eps))\n\n    return sim\n\n\n@params(G=nx.DiGraph, r=float, max_iter=int, eps=float)\ndef simrank_bipartite(G, r=0.8, max_iter=100, eps=1e-4):\n    """""" A bipartite version in the paper.\n    """"""\n    if not nx.is_bipartite(G):\n        assert(""A bipartie graph is required."")\n\n    nodes = G.nodes()\n    nodes_i = {}\n    for (k, v) in [(nodes[i], i) for i in range(0, len(nodes))]:\n        nodes_i[k] = v\n\n    sim_prev = np.zeros(len(nodes))\n    sim = np.identity(len(nodes))\n\n    lns = {}\n    rns = {}\n    for n in nodes:\n        preds = G.predecessors(n)\n        succs = G.successors(n)\n        if len(preds) == 0:\n            lns[n] = succs\n        else:\n            rns[n] = preds\n\n    def _update_partite(ns):\n        for u, v in itertools.product(ns.keys(), ns.keys()):\n            if u is v: continue\n            u_ns, v_ns = ns[u], ns[v]\n            if len(u_ns) == 0 or len(v_ns) == 0:\n                sim[nodes_i[u]][nodes_i[v]] = 0\n            else:\n                s_uv = sum([sim_prev[nodes_i[u_n]][nodes_i[v_n]] for u_n, v_n in itertools.product(u_ns, v_ns)])\n                sim[nodes_i[u]][nodes_i[v]] = (r * s_uv) / (len(u_ns) * len(v_ns))\n\n    for i in range(max_iter):\n        if np.allclose(sim, sim_prev, atol=eps):\n            break\n        sim_prev = np.copy(sim)\n        _update_partite(lns)\n        _update_partite(rns)\n\n    print(""Converge after %d iterations (eps=%f)."" % (i, eps))\n\n    return sim\n\n\nif __name__ == \'__main__\':\n    # Example university web graph in the paper\n    G = nx.DiGraph()\n    G.add_edges_from([(1,2), (1,3), (2,4), (4,1), (3,5), (5,3)])\n    print(simrank(G))\n\n    # Example bipartie graph of cake-bakers in the paper\n    G = nx.DiGraph()\n    G.add_edges_from([(1,3), (1,4), (1,5), (2,4), (2,5), (2,6)])\n    print(simrank_bipartite(G))\n'"
graphsim/iter/TACSim.py,17,"b'""""""\nTopology-Attributes Coupling Simmilarity (TACSim) measure.\n""""""\n#!/usr/bin/env python\n# Copyright (C) 2015 by\n# Xiaming Chen <chen_xm@sjtu.edu.cn>\n# All rights reserved.\n# BSD license.\nimport itertools, copy\n\nimport numpy as np\nimport networkx as nx\nfrom typedecorator import params\n\n__author__ = ""Xiaming Chen""\n__email__ = ""chen_xm@sjtu.edu.cn""\n\n__all__ = [ \'tacsim\', \'tacsim_combined\', \'normalized\', \'node_edge_adjacency\' ]\n\n\ndef _strength_nodes(nw1, nw2, ew):\n    return 1.0 * nw1 * nw2 / np.power(ew, 2)\n\n\ndef _strength_edges(ew1, ew2, nw):\n    return 1.0 * np.power(nw, 2) / (ew1 * ew2)\n\n\ndef _coherence(s1, s2):\n    return 2.0 * np.sqrt(s1 * s2) / (s1 + s2)\n\n\ndef _converged(nsim, nsim_prev, esim, esim_prev, eps=1e-4):\n    if np.allclose(nsim, nsim_prev, atol=eps) and \\\n            np.allclose(esim, esim_prev, atol=eps):\n        return True\n    return False\n\n\ndef normalized(a, axis=None, order=None):\n    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n    l2[l2==0] = 1\n    return a / l2\n\n\ndef _mask_lower_values(m, tol=1e-6):\n    m[abs(m) < tol] = 0.0\n    return m\n\n\ndef _graph_elements(G, node_attribute=\'weight\', edge_attribute=\'weight\', dummy_eps=1e-3):\n    """""" Generate strength matrices and node-edge indexes mapping of nodes and edges.\n    """"""\n    nodes = G.nodes()\n    edges = G.edges()\n    V = len(nodes)\n    E = len(edges)\n    node_id_lookup_tbl = {}\n    edge_id_lookup_tbl = {}\n    node_weight_vec = np.ones(V)\n    edge_weight_vec = np.ones(E)\n    node_strength_mat = np.empty((V, V)); node_strength_mat.fill(-1)\n    edge_strength_mat = np.empty((E, E)); edge_strength_mat.fill(-1)\n    node_edge_map = {}\n    edge_node_map = {}\n\n    for i, n in enumerate(nodes):\n        node_id_lookup_tbl[n] = i\n        try:\n            node_weight_vec[i] = G.node[n][node_attribute]\n            if node_weight_vec[i] < 0: # to fix dummy nodes\n                node_weight_vec[i] = dummy_eps\n        except KeyError:\n            node_weight_vec[i] = 1\n\n    node_weight_vec = normalized(node_weight_vec)\n\n    for i, e in enumerate(edges):\n        edge_id_lookup_tbl[e] = i\n        try:\n            edge_weight_vec[i] = G.edge[e[0]][e[1]][edge_attribute]\n            if edge_weight_vec[i] <= 0: # to fix dummy edges\n                edge_weight_vec[i] = dummy_eps\n        except KeyError:\n            edge_weight_vec[i] = 1\n\n    edge_weight_vec = normalized(edge_weight_vec)\n\n    for e in edges:\n        n0, n1 = node_id_lookup_tbl[e[0]], node_id_lookup_tbl[e[1]]\n        e01 = edge_id_lookup_tbl[e]\n        node_strength_mat[n0][n1] = \\\n            _strength_nodes(node_weight_vec[n0], node_weight_vec[n1], edge_weight_vec[e01])\n        # record node-node intersection\n        if n0 not in node_edge_map:\n            node_edge_map[n0] = {}\n        node_edge_map[n0][n1] = e01\n\n    for n in nodes:\n        n01 = node_id_lookup_tbl[n]\n        preds = G.predecessors(n)\n        succs = G.successors(n)\n        for p in preds:\n            for s in succs:\n                e0, e1 = edge_id_lookup_tbl[(p, n)], edge_id_lookup_tbl[(n, s)]\n                edge_strength_mat[e0][e1] = \\\n                    _strength_edges(edge_weight_vec[e0], edge_weight_vec[e1], node_weight_vec[n01])\n                # record edge-edge intersection\n                if e0 not in edge_node_map:\n                    edge_node_map[e0] = {}\n                edge_node_map[e0][e1] = n01\n\n    return node_strength_mat, node_edge_map, edge_strength_mat, edge_node_map\n\n\ndef tacsim(G1, G2=None, node_attribute=\'weight\', edge_attribute=\'weight\', max_iter=100, eps=1e-4, tol=1e-6):\n    """""" Calculate the TACSim measure of two attributed, directed graph.\n    """"""\n    if isinstance(G1, nx.MultiDiGraph):\n        assert(""MultiDiGraph is not supported by TACSim."")\n\n    nsm1, nem1, esm1, enm1 = _graph_elements(G1, node_attribute, edge_attribute)\n\n    if G2 is None:\n        nsm2, nem2, esm2, enm2 = nsm1, nem1, esm1, enm1\n        G2 = G1\n    else:\n        nsm2, nem2, esm2, enm2 = _graph_elements(G2, node_attribute, edge_attribute)\n\n    N = len(G1.nodes())\n    M = len(G2.nodes())\n    nsim_prev = np.zeros((N, M))\n    nsim = np.ones((N, M))\n\n    P = len(G1.edges())\n    Q = len(G2.edges())\n    esim_prev = np.zeros((P, Q))\n    esim = np.ones((P, Q))\n\n    for itrc in range(max_iter):\n        if _converged(nsim, nsim_prev, esim, esim_prev):\n            break\n\n        nsim_prev = copy.deepcopy(nsim)\n        esim_prev = copy.deepcopy(esim)\n\n        # Update node similarity, in and out node neighbors\n        for i, j in itertools.product(range(N), range(M)):\n            u_in = [u for u in range(N) if nsm1[u,i] >= 0]\n            v_in = [v for v in range(M) if nsm2[v,j] >= 0]\n            for u, v in itertools.product(u_in, v_in):\n                u_edge = nem1[u][i]\n                v_edge = nem2[v][j]\n                nsim[i][j] += 0.5 * _coherence(nsm1[u,i], nsm2[v,j]) * (nsim_prev[u,v] + esim_prev[u_edge][v_edge])\n\n            u_out = [u for u in range(N) if nsm1[i,u] >= 0]\n            v_out = [v for v in range(M) if nsm2[j,v] >= 0]\n            for u, v in itertools.product(u_out, v_out):\n                u_edge = nem1[i][u]\n                v_edge = nem2[j][v]\n                nsim[i][j] += 0.5 * _coherence(nsm1[i,u], nsm2[j,v]) * (nsim_prev[u,v] + esim_prev[u_edge][v_edge])\n\n        # Update edge similarity, in and out edge neighbors\n        for i, j in itertools.product(range(P), range(Q)):\n            u_in = [u for u in range(P) if esm1[u,i] >= 0]\n            v_in = [v for v in range(Q) if esm2[v,j] >= 0]\n            for u, v in itertools.product(u_in, v_in):\n                u_node = enm1[u][i]\n                v_node = enm2[v][j]\n                esim[i][j] += 0.5 * _coherence(esm1[u,i], esm2[v,j]) * (esim_prev[u,v] + nsim_prev[u_node][v_node])\n\n            u_out = [u for u in range(P) if esm1[i,u] >= 0]\n            v_out = [v for v in range(Q) if esm2[j,v] >= 0]\n            for u, v in itertools.product(u_out, v_out):\n                u_node = enm1[i][u]\n                v_node = enm2[j][v]\n                esim[i][j] += 0.5 * _coherence(esm1[i,u], esm2[j,v]) * (esim_prev[u,v] + nsim_prev[u_node][v_node])\n\n        nsim = normalized(nsim)\n        esim = normalized(esim)\n\n    print(""Converge after %d iterations (eps=%f)."" % (itrc, eps))\n\n    return _mask_lower_values(nsim, tol), _mask_lower_values(esim, tol)\n\n\n@params(G=nx.DiGraph)\ndef node_edge_adjacency(G):\n    """""" Node-edge adjacency matrix: source nodes\n    """"""\n    edges = G.edges()\n    nodes = G.nodes()\n    node_index = {}\n    for i in range(0, len(nodes)):\n        node_index[nodes[i]] = i\n\n    ne_src_mat = np.zeros([len(nodes), len(edges)])\n    ne_dst_mat = np.zeros([len(nodes), len(edges)])\n\n    for i in range(0, len(edges)):\n        s, t = edges[i]\n        ne_src_mat[node_index[s]][i] = 1\n        ne_dst_mat[node_index[t]][i] = 1\n\n    return ne_src_mat, ne_dst_mat\n\n\ndef tacsim_combined(G1, G2=None, node_attribute=\'weight\', edge_attribute=\'weight\', lamb = 0.5, norm=True):\n    """""" Combined similarity based on original tacsim scores. Refer to paper Mesos.\n    """"""\n    # X: node similarity; Y: edge similarity\n    X, Y = tacsim(G1, G2, node_attribute, edge_attribute)\n\n    As, At = node_edge_adjacency(G1)\n    if G2 is None:\n        Bs, Bt = As, At\n    else:\n        Bs, Bt = node_edge_adjacency(G2)\n\n    Z = Y + lamb * np.dot(np.dot(As.T, X), Bs) + (1-lamb) * np.dot(np.dot(At.T, X), Bt)\n\n    if norm:\n        return normalized(Z)\n    else:\n        return Z\n\n\nif __name__ == \'__main__\':\n    G1 = nx.DiGraph()\n    G1.add_weighted_edges_from([(1,0,8), (0,2,12), (1,2,10), (2,3,15)])\n    G1.node[0][\'weight\'] = 1\n    G1.node[1][\'weight\'] = 1\n    G1.node[2][\'weight\'] = 5\n    G1.node[3][\'weight\'] = 1\n\n    G2 = nx.DiGraph()\n    G2.add_weighted_edges_from([(0,1,15), (1,2,10)])\n    G2.node[0][\'weight\'] = 1\n    G2.node[1][\'weight\'] = 3\n    G2.node[2][\'weight\'] = 1\n\n    print(tacsim(G1, G2))\n    print(tacsim(G1))\n\n    print(tacsim_combined(G1, G2))\n'"
graphsim/iter/TACSim_in_C.py,8,"b'import ctypes.util\nfrom ctypes import *\n\nimport networkx as nx\nimport numpy as np\nimport os\n\nfrom .TACSim import node_edge_adjacency, normalized\n\n__all__ = [\'tacsim_in_C\', \'tacsim_combined_in_C\']\n\n\ndef find_clib():\n    # Find and load tacsim library\n    tacsimlib = ctypes.util.find_library(\'tacsim\')\n    if not tacsimlib:\n        try:\n            install_lib_dir = os.getenv(\'LIBTACSIM_LIB_DIR\', \'/usr/local/lib/\')\n            libc = ctypes.cdll.LoadLibrary(os.path.join(install_lib_dir, \'libtacsim.so\'))\n        except:\n            raise RuntimeError(""Can\'t find libtacsim. Please install it first."")\n    else:\n        libc = CDLL(tacsimlib, mode=ctypes.RTLD_GLOBAL)\n    return libc\n\n\ndef graph_properties(G, node_attribute=\'weight\', edge_attribute=\'weight\',\n                     min_node_weight=1e-4, min_edge_weight=1e-4):\n    nodes = G.nodes()\n    edges = G.edges()\n    V = len(nodes)\n    E = len(edges)\n    nnadj = np.zeros((V, V), dtype=np.int)\n    nnadj.fill(-1)\n    node_weight_vec = np.ones(V, dtype=np.double)\n    edge_weight_vec = np.ones(E, dtype=np.double)\n\n    node_id_lookup_tbl = {}\n    for i, n in enumerate(nodes):\n        node_id_lookup_tbl[n] = i\n        nw = max(min_node_weight, G.node[n][node_attribute])\n        node_weight_vec[i] = nw\n\n    edges = [(node_id_lookup_tbl[e[0]], node_id_lookup_tbl[e[1]], e[2]) for e in G.edges(data=True)]\n    sorted(edges, key=lambda x: (x[0], x[1]))\n\n    for i in range(len(edges)):\n        src, dst, weight = edges[i]\n        nnadj[src][dst] = i\n        edge_weight_vec[i] = max(min_edge_weight, weight[edge_attribute])\n\n    return nnadj, node_weight_vec, edge_weight_vec, V, E\n\n\ndef matrix_to_cpointer(arr, shape, dtype=c_double):\n    row, col = shape\n    DTARR = dtype * row\n    PTR_DT = POINTER(dtype)\n    PTR_DTARR = PTR_DT * col\n\n    ptr = PTR_DTARR()\n    for i in range(row):\n        ptr[i] = DTARR()\n        for j in range(col):\n            ptr[i][j] = arr[i][j]\n\n    return ptr\n\n\ndef vector_to_cpointer(vec, vlen, dtype=c_double):\n    DTARR = dtype * vlen\n    ptr = DTARR()\n    for i in range(vlen):\n        ptr[i] = vec[i]\n    return ptr\n\n\ndef cpointer_to_matrix(ptr, shape):\n    mat = np.empty(shape)\n    for i in range(shape[0]):\n        for j in range(shape[1]):\n            mat[i][j] = ptr[i][j]\n    return mat\n\n\ndef cpointer_to_ndarray(ptr, size, dtype, shape):\n    """""" Reverse of ndarray.ctypes.data_as. There are still\n        some problems to use this method.\n    """"""\n    buf = np.core.multiarray.int_asbuffer(\n        ctypes.addressof(ptr.contents), np.dtype(dtype).itemsize * size)\n    arr = np.ndarray(shape, dtype=dtype, buffer=buf)\n    return arr\n\n\ndef tacsim_in_C(G1, G2=None, node_attribute=\'weight\', edge_attribute=\'weight\',\n                min_node_weight=1e-4, min_edge_weight=1e-4,\n                max_iter=100, eps=1e-4, tol=1e-6):\n    libc = find_clib()\n\n    nsim = POINTER(POINTER(c_double))()\n    esim = POINTER(POINTER(c_double))()\n\n    if G2 is None:\n        # Calculate self-similarity of an attributed graph\n        calculate_tacsim_self = libc.calculate_tacsim_self\n        calculate_tacsim_self.argtypes = [\n            POINTER(POINTER(c_int)),\n            POINTER(c_double),\n            POINTER(c_double), c_int, c_int,\n            POINTER(POINTER(POINTER(c_double))),\n            POINTER(POINTER(POINTER(c_double))),\n            c_int, c_double, c_double\n        ]\n        calculate_tacsim_self.restype = c_int\n\n        # Convert graph attributes to ctypes\n        nnadj, nwgt, ewgt, nlen, elen = graph_properties(G1,\n                                                         node_attribute, edge_attribute, min_node_weight,\n                                                         min_edge_weight)\n\n        calculate_tacsim_self(\n            matrix_to_cpointer(nnadj, (nlen, nlen), dtype=c_int),\n            vector_to_cpointer(nwgt, nlen, dtype=c_double),\n            vector_to_cpointer(ewgt, elen, dtype=c_double),\n            c_int(nlen), c_int(elen),\n            byref(nsim), byref(esim),\n            c_int(max_iter), c_double(eps), c_double(tol)\n        )\n\n        nsim2 = cpointer_to_matrix(nsim, (nlen, nlen))\n        esim2 = cpointer_to_matrix(esim, (elen, elen))\n\n    else:\n        # Calculate similarity of two attributed graphs\n        calculate_tacsim = libc.calculate_tacsim\n        calculate_tacsim.argtypes = [\n            POINTER(POINTER(c_int)),\n            POINTER(c_double),\n            POINTER(c_double), c_int, c_int,\n            POINTER(POINTER(c_int)),\n            POINTER(c_double),\n            POINTER(c_double), c_int, c_int,\n            POINTER(POINTER(POINTER(c_double))),\n            POINTER(POINTER(POINTER(c_double))),\n            c_int, c_double, c_double\n        ]\n        calculate_tacsim.restype = c_int\n\n        nnadj, nwgt, ewgt, nlen, elen = graph_properties(G1,\n                                                         node_attribute, edge_attribute, min_node_weight,\n                                                         min_edge_weight)\n        nnadj2, nwgt2, ewgt2, nlen2, elen2 = graph_properties(G2,\n                                                              node_attribute, edge_attribute, min_node_weight,\n                                                              min_edge_weight)\n\n        calculate_tacsim(\n            matrix_to_cpointer(nnadj, (nlen, nlen), dtype=c_int),\n            vector_to_cpointer(nwgt, nlen, dtype=c_double),\n            vector_to_cpointer(ewgt, elen, dtype=c_double),\n            c_int(nlen), c_int(elen),\n            matrix_to_cpointer(nnadj2, (nlen2, nlen2), dtype=c_int),\n            vector_to_cpointer(nwgt2, nlen2, dtype=c_double),\n            vector_to_cpointer(ewgt2, elen2, dtype=c_double),\n            c_int(nlen2), c_int(elen2),\n            byref(nsim), byref(esim),\n            c_int(max_iter), c_double(eps), c_double(tol)\n        )\n\n        nsim2 = cpointer_to_matrix(nsim, (nlen, nlen2))\n        esim2 = cpointer_to_matrix(esim, (elen, elen2))\n\n    return nsim2, esim2\n\n\ndef tacsim_combined_in_C(G1, G2=None, node_attribute=\'weight\', edge_attribute=\'weight\', lamb=0.5, norm=True):\n    """""" Combined similarity based on original tacsim scores. Refer to paper Mesos.\n    """"""\n    # X: node similarity; Y: edge similarity\n    X, Y = tacsim_in_C(G1, G2, node_attribute, edge_attribute)\n\n    As, At = node_edge_adjacency(G1)\n    if G2 is None:\n        Bs, Bt = As, At\n    else:\n        Bs, Bt = node_edge_adjacency(G2)\n\n    Z = Y + lamb * np.dot(np.dot(As.T, X), Bs) + (1 - lamb) * np.dot(np.dot(At.T, X), Bt)\n\n    if norm:\n        return normalized(Z)\n    else:\n        return Z\n\n\nif __name__ == \'__main__\':\n    G1 = nx.DiGraph()\n    G1.add_weighted_edges_from([(1, 0, 8), (0, 2, 12), (1, 2, 10), (2, 3, 15)])\n    G1.node[0][\'weight\'] = 1\n    G1.node[1][\'weight\'] = 1\n    G1.node[2][\'weight\'] = 5\n    G1.node[3][\'weight\'] = 1\n\n    G2 = nx.DiGraph()\n    G2.add_weighted_edges_from([(0, 1, 15), (1, 2, 10)])\n    G2.node[0][\'weight\'] = 1\n    G2.node[1][\'weight\'] = 3\n    G2.node[2][\'weight\'] = 1\n\n    print(tacsim_in_C(G1, G2))\n\n    print(tacsim_in_C(G1))\n\n    print(tacsim_combined_in_C(G1, G2))\n'"
graphsim/iter/__init__.py,0,b'from .ASCOS import *\nfrom .BVD04 import *\nfrom .HITS import *\nfrom .HS03 import *\nfrom .SimRank import *\nfrom .TACSim import *\nfrom .TACSim_in_C import *'
