file_path,api_count,code
src/hh_research.py,6,"b'""""""\r\n------------------------------------------------------------------------\r\n\r\nTitle         : hh_research.py\r\nAuthor        : Alexander Kapitanov\r\nE-mail        :\r\nLang.         : python\r\nCompany       :\r\nRelease Date  : 2019/08/14\r\n\r\n------------------------------------------------------------------------\r\n\r\nDescription   :\r\n    HeadHunter (hh.ru) research script.\r\n\r\n    1. Get data from hh.ru by user request (i.e. \'Machine learning\')\r\n    2. Collect all vacancies.\r\n    3. Parse JSON and get useful values: salary, experience, name,\r\n    skills, employer name etc.\r\n    4. Calculate some statistics: average salary, median, std, variance\r\n\r\n------------------------------------------------------------------------\r\n\r\nGNU GENERAL PUBLIC LICENSE\r\nVersion 3, 29 June 2007\r\n\r\nCopyright (c) 2019 Kapitanov Alexander\r\n\r\nThis program is free software: you can redistribute it and/or modify\r\nit under the terms of the GNU General Public License as published by\r\nthe Free Software Foundation, either version 3 of the License, or\r\n(at your option) any later version.\r\n\r\nYou should have received a copy of the GNU General Public License\r\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\r\n\r\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\r\nAPPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\r\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM ""AS IS"" WITHOUT\r\nWARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT\r\nNOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\r\nFOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND\r\nPERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE\r\nDEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR\r\nOR CORRECTION.\r\n\r\n------------------------------------------------------------------------\r\n""""""\r\nimport argparse\r\nimport hashlib\r\nimport os\r\nimport pickle\r\nimport re\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nfrom urllib.parse import urlencode\r\n\r\nimport matplotlib.pyplot as plt\r\nimport nltk\r\nimport numpy as np\r\nimport pandas as pd\r\nimport requests\r\nimport seaborn as sns\r\nfrom tqdm import tqdm\r\n\r\ntry:\r\n    nltk.download(""stopwords"")\r\nexcept:\r\n    print(r""[INFO] You have downloaded stopwords!"")\r\n\r\nCACHE_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), ""cache"")\r\nAPI_BASE_URL = ""https://api.hh.ru/vacancies""\r\n\r\nDEFAULT_PARAMETERS = {\r\n    ""area"": 1,\r\n    ""per_page"": 50,\r\n}\r\n\r\nHH_URL = API_BASE_URL + ""?"" + urlencode(DEFAULT_PARAMETERS)\r\n\r\nEX_URL = ""https://api.exchangerate-api.com/v4/latest/RUB""\r\nMAX_WORKERS = int(os.getenv(""MAX_WORKERS"", 5))\r\n\r\nexchange_rates = {}\r\n\r\n\r\ndef update_exchange_rates():\r\n    """"""\r\n    Parse exchange rate for RUB, USD, EUR and save them to `exchange_rates`\r\n    """"""\r\n    try:\r\n        print(""[INFO] Try to get rates from URL..."")\r\n        resp = requests.get(EX_URL)\r\n        rates = resp.json()[""rates""]\r\n\r\n    except requests.exceptions.SSLError:\r\n        print(""[FAIL] Cannot get exchange rate! Try later or change host API"")\r\n        exit(""[INFO] Exit from script. Cannot get data from URL!"")\r\n\r\n    for curr in [""RUB"", ""USD"", ""EUR""]:\r\n        exchange_rates[curr] = rates[curr]\r\n\r\n    # Change \'RUB\' to \'RUR\'\r\n    exchange_rates[""RUR""] = exchange_rates.pop(""RUB"")\r\n    print(f""[INFO] Get exchange rates: {exchange_rates}"")\r\n\r\n\r\ndef clean_tags(str_html):\r\n    """"""\r\n    Remove HTML tags from string (text)\r\n\r\n    Parameters\r\n    ----------\r\n    str_html: str\r\n        Input string with tags\r\n\r\n    Returns\r\n    -------\r\n    string\r\n        Clean text without tags\r\n\r\n    """"""\r\n    pat = re.compile(""<.*?>"")\r\n    res = re.sub(pat, """", str_html)\r\n    return res\r\n\r\n\r\ndef get_vacancy(vacancy_id):\r\n    # Vacancy URL\r\n    url = f""https://api.hh.ru/vacancies/{vacancy_id}""\r\n    vacancy = requests.api.get(url).json()\r\n\r\n    # Extract salary\r\n    salary = vacancy[""salary""]\r\n\r\n    # Calculate salary:\r\n    # Get salary into {RUB, USD, EUR} with {Gross} parameter and\r\n    # return a new salary in RUB.\r\n    cl_ex = {""from"": None, ""to"": None}\r\n    if salary:\r\n        # fn_gr = lambda: 0.87 if vsal[\'gross\'] else 1\r\n        def fn_gr():\r\n            return 0.87 if vacancy[""salary""][""gross""] else 1\r\n\r\n        for i in cl_ex:\r\n            if vacancy[""salary""][i] is not None:\r\n                cl_ex[i] = int(fn_gr() * salary[i] / exchange_rates[salary[""currency""]])\r\n\r\n    # Create pages tuple\r\n    return (\r\n        vacancy_id,\r\n        vacancy[""employer""][""name""],\r\n        vacancy[""name""],\r\n        salary is not None,\r\n        cl_ex[""from""],\r\n        cl_ex[""to""],\r\n        vacancy[""experience""][""name""],\r\n        vacancy[""schedule""][""name""],\r\n        [el[""name""] for el in vacancy[""key_skills""]],\r\n        clean_tags(vacancy[""description""]),\r\n    )\r\n\r\n\r\ndef get_vacancies(query, refresh=False):\r\n    """"""\r\n    Parse vacancy JSON: get vacancy name, salary, experience etc.\r\n\r\n    Parameters\r\n    ----------\r\n    query: str\r\n        Search query\r\n    refresh: bool\r\n        Refresh cached data\r\n\r\n    Returns\r\n    -------\r\n    list\r\n        List of useful arguments from vacancies\r\n\r\n    """"""\r\n    cache_hash = hashlib.md5(query.encode()).hexdigest()\r\n    cache_file_name = os.path.join(CACHE_DIR, cache_hash)\r\n    try:\r\n        if not refresh:\r\n            return pickle.load(open(cache_file_name, ""rb""))\r\n\r\n    except (FileNotFoundError, pickle.UnpicklingError):\r\n        pass\r\n\r\n    ids = []\r\n    parameters = {""text"": query, **DEFAULT_PARAMETERS}\r\n    url = API_BASE_URL + ""?"" + urlencode(parameters)\r\n    nm_pages = requests.get(url).json()[""pages""]\r\n    for i in range(nm_pages + 1):\r\n        resp = requests.get(url, {""page"": i})\r\n        data = resp.json()\r\n        if ""items"" not in data:\r\n            break\r\n        ids.extend(x[""id""] for x in data[""items""])\r\n\r\n    vacancies = []\r\n    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\r\n        for vacancy in tqdm(executor.map(get_vacancy, ids), total=len(ids)):\r\n            vacancies.append(vacancy)\r\n\r\n    pickle.dump(vacancies, open(cache_file_name, ""wb""))\r\n    return vacancies\r\n\r\n\r\ndef prepare_df(dct_df):\r\n    """"""\r\n    Prepare data frame and save results\r\n\r\n    Parameters\r\n    ----------\r\n    dct_df: list\r\n        List of parsed json dicts\r\n\r\n    """"""\r\n    # List of columns\r\n    df_cols = [\r\n        ""Id"",\r\n        ""Employer"",\r\n        ""Name"",\r\n        ""Salary"",\r\n        ""From"",\r\n        ""To"",\r\n        ""Experience"",\r\n        ""Schedule"",\r\n        ""Keys"",\r\n        ""Description"",\r\n    ]\r\n    # Create pandas dataframe\r\n    df = pd.DataFrame(data=dct_df, columns=df_cols)\r\n    # Print some info from data frame\r\n    print(df[df[""Salary""]][[""Employer"", ""From"", ""To"", ""Experience"", ""Schedule""]][0:10])\r\n    # Save to file\r\n    df.to_csv(r""hh_data.csv"", index=False)\r\n\r\n\r\ndef analyze_df():\r\n    """"""\r\n    Load data frame and analyze results\r\n\r\n    """"""\r\n\r\n    sns.set()\r\n    print(""\\n\\n[INFO] Load table and analyze results"")\r\n    df = pd.read_csv(""hh_data.csv"")\r\n    print(df[df[""Salary""]][0:7])\r\n\r\n    print(""\\nNumber of vacancies: {}"".format(df[""Id""].count()))\r\n    print(""\\nVacancy with max salary: "")\r\n    print(df.iloc[df[[""From"", ""To""]].idxmax()])\r\n    print(""\\nVacancy with min salary: "")\r\n    print(df.iloc[df[[""From"", ""To""]].idxmin()])\r\n\r\n    print(""\\n[INFO] Describe salary data frame"")\r\n    df_stat = df[[""From"", ""To""]].describe().applymap(np.int32)\r\n    print(df_stat.iloc[list(range(4)) + [-1]])\r\n\r\n    print(\'\\n[INFO] Average statistics (filter for ""From""-""To"" parameters):\')\r\n    comb_ft = np.nanmean(df[df[""Salary""]][[""From"", ""To""]].to_numpy(), axis=1)\r\n    print(""Describe salary series:"")\r\n    print(""Min    : %d"" % np.min(comb_ft))\r\n    print(""Max    : %d"" % np.max(comb_ft))\r\n    print(""Mean   : %d"" % np.mean(comb_ft))\r\n    print(""Median : %d"" % np.median(comb_ft))\r\n\r\n    print(""\\nMost frequently used words [Keywords]:"")\r\n    # Collect keys from df\r\n    keys_df = df[""Keys""].to_list()\r\n    # Create a list of keys for all vacancies\r\n    lst_keys = []\r\n    for keys_elem in keys_df:\r\n        for el in keys_elem[1:-1].split("", ""):\r\n            if el != """":\r\n                lst_keys.append(re.sub(""\'"", """", el.lower()))\r\n    # Unique keys and their counter\r\n    set_keys = set(lst_keys)\r\n    # Dict: {Key: Count}\r\n    dct_keys = {el: lst_keys.count(el) for el in set_keys}\r\n    # Sorted dict\r\n    srt_keys = dict(sorted(dct_keys.items(), key=lambda x: x[1], reverse=True))\r\n    # Return pandas series\r\n    most_keys = pd.Series(srt_keys, name=""Keys"")\r\n    print(most_keys[:12])\r\n\r\n    print(""\\nMost frequently used words [Description]:"")\r\n    # Collect keys from df\r\n    words_df = df[""Description""].to_list()\r\n    # Long string - combine descriptions\r\n    words_ls = "" "".join(\r\n        [re.sub("" +"", "" "", re.sub(r""\\d+"", """", el.strip().lower())) for el in words_df]\r\n    )\r\n    # Find all words\r\n    words_re = re.findall(""[a-zA-Z]+"", words_ls)\r\n    # Filter words with length < 3\r\n    words_l2 = [el for el in words_re if len(el) > 2]\r\n    # Unique words\r\n    words_st = set(words_l2)\r\n    # Remove \'stop words\'\r\n    stop_words = set(nltk.corpus.stopwords.words(""english""))\r\n    # XOR for dictionary\r\n    words_st ^= stop_words\r\n    words_st ^= {""amp"", ""quot""}\r\n    # Dictionary - {Word: Counter}\r\n    words_cnt = {el: words_l2.count(el) for el in words_st}\r\n    # Pandas series\r\n    most_words = pd.Series(\r\n        dict(sorted(words_cnt.items(), key=lambda x: x[1], reverse=True))\r\n    )\r\n    print(most_words[:12])\r\n\r\n    print(""\\n[INFO] Plot results. Close figure box to continue..."")\r\n    fz = plt.figure(""Salary plots"", figsize=(12, 8))\r\n    fz.add_subplot(2, 2, 1)\r\n    plt.title(""From / To: Boxplot"")\r\n    sns.boxplot(data=df[[""From"", ""To""]].dropna() / 1000, width=0.4)\r\n    plt.ylabel(""Salary x 1000 [RUB]"")\r\n    fz.add_subplot(2, 2, 2)\r\n    plt.title(""From / To: Swarmplot"")\r\n    sns.swarmplot(data=df[[""From"", ""To""]].dropna() / 1000, size=6)\r\n\r\n    fz.add_subplot(2, 2, 3)\r\n    plt.title(""From: Distribution "")\r\n    sns.distplot(df[""From""].dropna() / 1000, bins=14, color=""C0"")\r\n    plt.grid(True)\r\n    plt.xlabel(""Salary x 1000 [RUB]"")\r\n    plt.xlim([-50, df[""From""].max() / 1000])\r\n    plt.yticks([], [])\r\n\r\n    fz.add_subplot(2, 2, 4)\r\n    plt.title(""To: Distribution"")\r\n    sns.distplot(df[""To""].dropna() / 1000, bins=14, color=""C1"")\r\n    plt.grid(True)\r\n    plt.xlim([-50, df[""To""].max() / 1000])\r\n    plt.xlabel(""Salary x 1000 [RUB]"")\r\n    plt.yticks([], [])\r\n    plt.tight_layout()\r\n    plt.show()\r\n\r\n\r\ndef run():\r\n    """"""\r\n    Main function - combine all methods together\r\n\r\n    """"""\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(""query"", help=\'Search query (e.g. ""Machine learning"")\')\r\n    parser.add_argument(\r\n        ""--refresh"",\r\n        help=""Refresh cached data from HH API"",\r\n        action=""store_true"",\r\n        default=False,\r\n    )\r\n    args = parser.parse_args()\r\n\r\n    update_exchange_rates()\r\n    print(""[INFO] Collect data from JSON. Create list of vacancies..."")\r\n    vac_list = get_vacancies(args.query, args.refresh)\r\n    print(""[INFO] Prepare data frame..."")\r\n    prepare_df(vac_list)\r\n    analyze_df()\r\n    print(""[INFO] Done! Exit()"")\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    run()\r\n'"
