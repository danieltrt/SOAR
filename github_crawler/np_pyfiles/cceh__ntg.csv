file_path,api_count,code
doc_src/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# NTG documentation build configuration file, created by\n# sphinx-quickstart on Tue May 17 17:49:37 2016.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nroot = os.path.abspath (\'..\')\nsys.path.insert (0, \'/home/highlander/uni/prj/suffix_tree/suffix_tree\')\nsys.path.insert (0, root)\nsys.path.insert (0, os.path.join (root, \'server\'))\nsys.path.insert (0, os.path.join (root, \'client\'))\nsys.path.insert (0, os.path.join (root, \'scripts/cceh\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.graphviz\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.imgconverter\',\n    \'sphinxcontrib.autoprogram\',\n    \'sphinxcontrib.httpdomain\',\n    \'sphinxcontrib.plantuml\',\n\n    \'autojsdoc.autojsdoc\',\n\n    \'sqlalchemy-uml.sqlalchemy-uml\',\n]\n\nautojsdoc_structure_json = \'jsdoc/structure.json\'\nautojsdoc_members = True\nautojsdoc_title = True\n\nsauml_arguments = [\'postgresql+psycopg2://ntg@localhost:5432/acts_ph4\']\nsauml_dot_table = \'bgcolor=#e7f2fa&color=#2980B9\'\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'NTG\'\ncopyright = \'2016-19 CCeH - Licensed under the GNU GPL v3 or later\'\nauthor = \'Marcello Perathoner\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'0.1\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'0.1.1\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = \'en\'\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\ndefault_role = \'any\'\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\nadd_module_names = False\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n\nimport sphinx_rtd_theme\nhtml_theme = ""sphinx_rtd_theme"" # nature\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': False,\n    \'navigation_depth\': 3,\n}\n\n# Add any paths that contain custom themes here, relative to this directory.\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\ndef setup (app):\n    app.add_stylesheet (\'my_theme.css\')\n\n# The name for this set of Sphinx documents.\n# ""<project> v<release> documentation"" by default.\n#html_title = \'NTG v0.1.1\'\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'doc_src/_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not None, a \'Last updated on:\' timestamp is inserted at every page\n# bottom, using the given strftime format.\n# The empty string is equivalent to \'%b %d, %Y\'.\n#html_last_updated_fmt = None\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'h\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'r\', \'sv\', \'tr\', \'zh\'\n#html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# \'ja\' uses this config value.\n# \'zh\' user can custom change `jieba` dictionary path.\n#html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'NTGdoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n\n# Latex figure (float) alignment\n#\'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'NTG.tex\', \'NTG Documentation\',\n     \'Marcello Perathoner\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'ntg\', \'NTG Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'NTG\', \'NTG Documentation\',\n     author, \'NTG\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n'"
ntg_common/__init__.py,0,"b'""""""This package contains classes and functions common to the application server\nand the database conversion routines.\n\n""""""\n'"
ntg_common/cbgm_common.py,36,"b'# -*- encoding: utf-8 -*-\n\n""""""Common routines for the CBGM.\n\n""""""\n\nimport collections\nimport logging\n\nimport networkx as nx\nimport numpy as np\n\nfrom ntg_common import db_tools\nfrom ntg_common.db_tools import execute, executemany, executemany_raw\nfrom ntg_common.tools import log\n\n\nclass CBGM_Params ():\n    """""" Structure that holds intermediate results of the CBGM. """"""\n\n    n_mss = 0\n    ""No. of manuscripts""\n\n    n_passages = 0\n    ""No. of passages""\n\n    n_ranges = 0\n    ""No. of ranges""\n\n    ranges = None\n    ""list of (named tuple Range)""\n\n    variant_matrix = None\n    """"""Boolean (1 x passages) matrix of invariant passages.  We will need this the\n    day we decide *not* to eliminate all invariant readings from the\n    database.\n\n    """"""\n\n    labez_matrix = None\n    """"""Integer matrix (mss x passages) of labez.  Each entry represents one reading:\n    0 = lacuna, 1 = \'a\', 2 = \'b\', ...  Used by the pre-coherence computations.\n\n    """"""\n\n    def_matrix = None\n    """"""Boolean matrix (mss x passages) set if ms. is defined at passage.""""""\n\n    and_matrix = None\n    """"""Integer matrix (ranges x mss x mss) with counts of the passages that are\n    defined in both mss.\n\n    """"""\n\n    eq_matrix = None\n    """"""Integer matrix (ranges x mss x mss) with counts of the passages that are\n    equal in both mss.\n\n    """"""\n\n    parent_matrix = None\n    """"""Integer matrix (ranges x mss x mss) with counts of the passages that are\n    older in ms1 than in ms2, using only immediate descendence.  This matrix is\n    asymmetrical.\n\n    """"""\n\n    ancestor_matrix = None\n    """"""Integer matrix (ranges x mss x mss) with counts of the passages that are\n    older in ms1 than in ms2.  This matrix is asymmetrical.\n\n    """"""\n\n    unclear_parent_matrix = None\n    """"""Integer matrix (ranges x mss x mss) with counts of the passages whose\n    relationship is unclear in ms1 and ms2, using only immediate descendence.\n\n    """"""\n\n    unclear_ancestor_matrix = None\n    """"""Integer matrix (ranges x mss x mss) with counts of the passages whose\n    relationship is unclear in ms1 and ms2.\n\n    """"""\n\n\ndef create_labez_matrix (dba, parameters, val):\n    """"""Create the :attr:`labez matrix <scripts.cceh.cbgm.CBGM_Params.labez_matrix>`.""""""\n\n    with dba.engine.begin () as conn:\n\n        np.set_printoptions (threshold = 30)\n\n        # get passages\n        res = execute (conn, """"""\n        SELECT count (*)\n        FROM passages\n        """""", parameters)\n        val.n_passages = res.fetchone ()[0]\n\n        # get matrix of invariant passages\n        # Initialize all passages to \'variant\'\n        variant_matrix = np.ones ((1, val.n_passages), np.bool_)\n\n        res = execute (conn, """"""\n        SELECT pass_id - 1\n        FROM passages\n        WHERE NOT (variant)\n        """""", parameters)\n\n        for row in res:\n            variant_matrix [0, row[0]] = False\n        val.variant_matrix = variant_matrix\n\n        # get no. of manuscripts\n        res = execute (conn, """"""\n        SELECT count (*)\n        FROM manuscripts\n        """""", parameters)\n        val.n_mss = res.fetchone ()[0]\n\n        # get no. of ranges\n        Range = collections.namedtuple (\'Range\', \'rg_id range start end\')\n        res = execute (conn, """"""\n        SELECT rg_id, range, MIN (pass_id) - 1 AS first_id, MAX (pass_id) AS last_id\n        FROM ranges ch\n        JOIN passages p ON ch.passage @> p.passage\n        GROUP BY rg_id, range\n        ORDER BY lower (ch.passage), upper (ch.passage) DESC\n        """""", parameters)\n        val.n_ranges = res.rowcount\n        val.ranges = list (map (Range._make, res))\n        log (logging.INFO, \'  No. of ranges: \' + str (val.n_ranges))\n\n        # Matrix ms x pass\n\n        # Initialize all manuscripts to the labez \'a\'\n        labez_matrix  = np.broadcast_to (np.array ([1], np.uint32), (val.n_mss, val.n_passages)).copy ()\n\n        # overwrite matrix where actual labez is not \'a\'\n        res = execute (conn, """"""\n        SELECT ms_id - 1, pass_id - 1, ord_labez (labez) as labez\n        FROM apparatus a\n        WHERE labez != \'a\' AND cbgm\n        """""", parameters)\n\n        for row in res:\n            labez_matrix [row[0], row[1]] = row[2]\n\n        # clear matrix where reading is uncertain\n        res = execute (conn, """"""\n        SELECT DISTINCT ms_id - 1, pass_id - 1\n        FROM apparatus\n        WHERE certainty != 1.0\n        """""", parameters)\n\n        for row in res:\n            labez_matrix [row[0], row[1]] = 0\n\n        val.labez_matrix = labez_matrix\n\n        # Boolean matrix ms x pass set where passage is defined\n        val.def_matrix = np.greater (val.labez_matrix, 0)\n        val.def_matrix = np.logical_and (val.def_matrix, val.variant_matrix) # mask invariant passages\n\n        log (logging.INFO, \'  Size of the labez matrix: \' + str (val.labez_matrix.shape))\n\n\ndef count_by_range (a, range_starts, range_ends):\n    """"""Count true bits in array ranges\n\n    Count the bits that are true in multiple ranges of the same array of booleans.\n\n    :param numpy.Array a:      Input array\n    :type a: np.Array of np.bool:\n    :param int[] range_starts: Starting offsets of the ranges to count.\n    :param int[] range_ends:   Ending offsets of the ranges to count.\n\n    """"""\n    cs = np.cumsum (a)    # cs[0] = a[0], cs[1] = cs[0] + a[1], ..., cs[n] = total\n    cs = np.insert (cs, 0, 0)\n    cs_start = cs[range_starts] # get the sums at the range beginnings\n    cs_end   = cs[range_ends]   # get the sums at the range ends\n    return cs_end - cs_start\n\n\ndef calculate_mss_similarity_preco (_dba, _parameters, val):\n    r""""""Calculate pre-coherence mss similarity\n\n    The pre-coherence similarity is defined as:\n\n    .. math::\n\n        \\mbox{similarity}=\\frac{\\mbox{equal passages}}{\\mbox{passages in common}}\n\n    ..\n\n        Kapitelweise f\xc3\xbcllen auf Basis von Vergleichen einzelner\n        Variantenspektren in ECM_Acts_Sp.  Vergleich von je zwei Handschriften:\n        An wieviel Stellen haben sie gemeinsam Text, an wieviel Stellen stimmen\n        sie \xc3\xbcberein bzw. unterscheiden sie sich (inklusive Quotient)?  Die\n        Informationen werden sowohl auf Kapitel- wie auch Buchebene\n        festgehalten.\n\n        --VGA/VG05_all3.pl\n\n    """"""\n\n    # Matrix range x ms x ms with count of the passages that are defined in both mss\n    val.and_matrix = np.zeros ((val.n_ranges, val.n_mss, val.n_mss), dtype = np.uint16)\n\n    # Matrix range x ms x ms with count of the passages that are equal in both mss\n    val.eq_matrix  = np.zeros ((val.n_ranges, val.n_mss, val.n_mss), dtype = np.uint16)\n\n    val.range_starts = [ch.start for ch in val.ranges]\n    val.range_ends   = [ch.end   for ch in val.ranges]\n\n    # pre-genealogical coherence outputs symmetrical matrices\n    # loop over all mss O(n_mss\xc2\xb2 * n_ranges * n_passages)\n\n    for j in range (0, val.n_mss):\n        labezj = val.labez_matrix[j]\n        defj   = val.def_matrix[j]\n\n        for k in range (j + 1, val.n_mss):\n            labezk = val.labez_matrix[k]\n            defk   = val.def_matrix[k]\n\n            def_and  = np.logical_and (defj, defk)\n            labez_eq = np.logical_and (def_and, np.equal (labezj, labezk))\n\n            val.and_matrix[:,j,k] = val.and_matrix[:,k,j] = count_by_range (def_and, val.range_starts, val.range_ends)\n            val.eq_matrix[:,j,k]  = val.eq_matrix[:,k,j]  = count_by_range (labez_eq, val.range_starts, val.range_ends)\n\n\ndef calculate_mss_similarity_postco (dba, parameters, val, do_checks = True):\n    """"""Calculate post-coherence mss similarity\n\n    Genealogical coherence outputs asymmetrical matrices.\n    Loop over all mss O(n_mss\xc2\xb2 * n_ranges * n_passages).\n\n    """"""\n\n    with dba.engine.begin () as conn:\n\n        # Load all passages into memory\n\n        res = execute (conn, """"""\n        SELECT pass_id, begadr, endadr FROM passages\n        ORDER BY pass_id\n        """""", parameters)\n\n        stemmas = dict ()\n        for pass_id, begadr, endadr in res.fetchall ():\n            G = db_tools.local_stemma_to_nx (conn, pass_id, True) # True == add isolated roots\n\n            if do_checks:\n                # sanity tests\n                # connect the graph through a root node for the following tests:\n                G.add_node (\'root\', label = \'root\')\n                G.add_edge (\'root\', \'*\')\n                G.add_edge (\'root\', \'?\')\n                if not nx.is_weakly_connected (G):\n                    # use it anyway\n                    log (logging.WARNING, ""Local Stemma @ %s-%s is not connected (pass_id=%s)."" %\n                         (begadr, endadr, pass_id))\n                if not nx.is_directed_acyclic_graph (G):\n                    # don\'t use these\n                    log (logging.ERROR, ""Local Stemma @ %s-%s is not a directed acyclic graph (pass_id=%s)."" %\n                         (begadr, endadr, pass_id))\n                    continue\n                # ... and remove it again\n                G.remove_node (\'root\')\n\n            G.nodes[\'*\'][\'mask\'] = 0\n            G.nodes[\'?\'][\'mask\'] = 1 # bitmask == 1 signifies source is unclear\n\n            # build node bitmasks.  Every node gets a different bit set.\n            i = 1\n            for n in sorted (G.nodes ()):\n                attrs = G.nodes[n]\n                attrs[\'parents\'] = 0\n                attrs[\'ancestors\'] = 0\n                if \'mask\' not in attrs:\n                    i += 1\n                    if i < 64:\n                        attrs[\'mask\'] = (1 << i)\n                    else:\n                        attrs[\'mask\'] = 0\n                        # mask is 64 bit only\n                        log (logging.ERROR, ""Too many cliques in local stemma @ %s-%s (pass_id=%s)."" %\n                             (begadr, endadr, pass_id))\n\n            # build the parents bit mask. We set the bits of the parent nodes.\n            for n in G:\n                mask = G.nodes[n][\'mask\']\n                for succ in G.successors (n):\n                    G.nodes[succ][\'parents\'] |= mask\n\n            # build the ancestors mask.  We set the bits of all node ancestors.\n            TC = nx.transitive_closure (G)\n            for n in TC:\n                # transitive_closure does not copy attributes !\n                mask = G.nodes[n][\'mask\']\n                for succ in TC.successors (n):\n                    G.nodes[succ][\'ancestors\'] |= mask\n\n            # save the graph for later\n            stemmas[pass_id - 1] = G\n\n        # Matrix mss x passages containing the bitmask of the current reading\n        mask_matrix     = np.zeros ((val.n_mss, val.n_passages), np.uint64)\n        # Matrix mss x passages containing the bitmask of the parent readings\n        parent_matrix   = np.zeros ((val.n_mss, val.n_passages), np.uint64)\n        # Matrix mss x passages containing the bitmask of the ancestral readings\n        ancestor_matrix = np.zeros ((val.n_mss, val.n_passages), np.uint64)\n\n        # load ms x pass\n        res = execute (conn, """"""\n        SELECT pass_id - 1 AS pass_id,\n               ms_id   - 1 AS ms_id,\n               labez_clique (labez, clique) AS labez_clique\n        FROM apparatus_cliques_view a\n        WHERE labez !~ \'^z[u-z]\' AND cbgm\n        ORDER BY pass_id\n        """""", parameters)\n\n        LocStemEd = collections.namedtuple (\'LocStemEd\', \'pass_id ms_id labez_clique\')\n        rows = list (map (LocStemEd._make, res))\n\n        # If ((current bitmask of ms j) and (ancestor bitmask of ms k) > 0) then\n        # ms j is an ancestor of ms k.\n\n        error_count = 0\n        for row in rows:\n            try:\n                attrs = stemmas[row.pass_id].nodes[row.labez_clique]\n                mask_matrix     [row.ms_id, row.pass_id] = attrs[\'mask\']\n                parent_matrix   [row.ms_id, row.pass_id] = attrs[\'parents\']\n                ancestor_matrix [row.ms_id, row.pass_id] = attrs[\'ancestors\']\n            except KeyError:\n                error_count += 1\n                # print (row.pass_id + 1)\n                # print (str (e))\n\n        # Matrix mss x passages containing True if source is unclear (s1 = \'?\')\n        quest_matrix = np.bitwise_and (parent_matrix, 1)  # 1 means source unclear\n\n        if error_count:\n            log (logging.WARNING, ""Could not find labez and clique in LocStem in %d cases."" % error_count)\n        log (logging.DEBUG, ""mask:\\n""      + str (mask_matrix))\n        log (logging.DEBUG, ""parents:\\n""   + str (parent_matrix))\n        log (logging.DEBUG, ""ancestors:\\n"" + str (ancestor_matrix))\n        log (logging.DEBUG, ""quest:\\n""     + str (quest_matrix))\n\n        def postco (mask_matrix, anc_matrix):\n\n            local_stemmas_with_loops = set ()\n\n            # Matrix range x ms x ms with count of the passages that are older in ms1 than in ms2\n            ancestor_matrix = np.zeros ((val.n_ranges, val.n_mss, val.n_mss), dtype = np.uint16)\n\n            # Matrix range x ms x ms with count of the passages whose relationship is unclear in ms1 and ms2\n            unclear_matrix  = np.zeros ((val.n_ranges, val.n_mss, val.n_mss), dtype = np.uint16)\n\n            for j in range (0, val.n_mss):\n                for k in range (0, val.n_mss):\n                    # See: VGA/VGActs_allGenTab3Ph3.pl\n\n                    # set bit if the reading of j is ancestral to the reading of k\n                    varidj_is_older = np.bitwise_and (mask_matrix[j], anc_matrix[k]) > 0\n                    varidk_is_older = np.bitwise_and (mask_matrix[k], anc_matrix[j]) > 0\n\n                    if j == 0 and k > 0 and varidk_is_older.any ():\n                        log (logging.ERROR, ""Found varid older than A in msid: %d = %s""\n                             % (k, np.nonzero (varidk_is_older)))\n\n                    # error check for loops\n                    if do_checks:\n                        check = np.logical_and (varidj_is_older, varidk_is_older)\n                        if np.any (check):\n                            not_check       = np.logical_not (check)\n                            varidj_is_older = np.logical_and (varidj_is_older, not_check)\n                            varidk_is_older = np.logical_and (varidk_is_older, not_check)\n\n                            local_stemmas_with_loops |= set (np.nonzero (check)[0])\n\n                    # wenn die vergl. Hss. von einander abweichen u. eine von ihnen\n                    # Q1 = \'?\' hat, UND KEINE VON IHNEN QUELLE DER ANDEREN IST, ist\n                    # die Beziehung \'UNCLEAR\'\n\n                    unclear = np.logical_and (val.def_matrix[j], val.def_matrix[k])\n                    unclear = np.logical_and (unclear, np.not_equal (val.labez_matrix[j], val.labez_matrix[k]))\n                    unclear = np.logical_and (unclear, np.logical_or (quest_matrix[j], quest_matrix[k]))\n                    unclear = np.logical_and (unclear, np.logical_not (np.logical_or (varidj_is_older, varidk_is_older)))\n\n                    ancestor_matrix[:,j,k] = count_by_range (varidj_is_older, val.range_starts, val.range_ends)\n                    unclear_matrix[:,j,k]  = count_by_range (unclear, val.range_starts, val.range_ends)\n\n            if local_stemmas_with_loops:\n                log (logging.ERROR, ""Found loops in local stemmata: %s"" % sorted (local_stemmas_with_loops))\n\n            return ancestor_matrix, unclear_matrix\n\n        val.parent_matrix,   val.unclear_parent_matrix   = postco (mask_matrix, parent_matrix)\n        val.ancestor_matrix, val.unclear_ancestor_matrix = postco (mask_matrix, ancestor_matrix)\n\n\ndef write_affinity_table (dba, parameters, val):\n    """"""Write back the new affinity (and ms_ranges) tables.\n\n    """"""\n\n    with dba.engine.begin () as conn:\n        # perform sanity tests\n\n        # varid older than ms A\n        if val.ancestor_matrix[0,:,0].any ():\n            log (logging.ERROR, ""Found varid older than A in msids: %s""\n                 % (np.nonzero (val.ancestor_matrix[0,:,0])))\n\n        # norel < 0\n        norel_matrix = (val.and_matrix - val.eq_matrix - val.ancestor_matrix -\n                        np.transpose (val.ancestor_matrix, (0, 2, 1)) - val.unclear_ancestor_matrix)\n        if np.less (norel_matrix, 0).any ():\n            log (logging.ERROR, ""norel < 0 in mss. %s""\n                 % (np.nonzero (np.less (norel_matrix, 0))))\n\n        # calculate ranges lengths using numpy\n        params = []\n        for i in range (0, val.n_mss):\n            for range_ in val.ranges:\n                length = int (np.sum (val.def_matrix[i, range_.start:range_.end]))\n                params.append ( { \'ms_id\': i + 1, \'range\': range_.rg_id, \'length\': length } )\n\n        executemany (conn, """"""\n        UPDATE ms_ranges\n        SET length = :length\n        WHERE ms_id = :ms_id AND rg_id = :range\n        """""", parameters, params)\n\n        log (logging.INFO, ""  Filling Affinity table ..."")\n\n        # execute (conn, ""TRUNCATE affinity"", parameters) # fast but needs access exclusive lock\n        execute (conn, ""DELETE FROM affinity"", parameters)\n\n        for i, range_ in enumerate (val.ranges):\n            values = []\n            for j in range (0, val.n_mss):\n                for k in range (0, val.n_mss):\n                    if j != k:\n                        common = int (val.and_matrix[i,j,k])\n                        equal  = int (val.eq_matrix[i,j,k])\n                        if common > 0:\n                            values.append ( (\n                                range_.rg_id,\n                                j + 1,\n                                k + 1,\n                                float (equal) / common,\n                                common,\n                                equal,\n                                int (val.ancestor_matrix[i,j,k]),\n                                int (val.ancestor_matrix[i,k,j]),\n                                int (val.unclear_ancestor_matrix[i,j,k]),\n                                int (val.parent_matrix[i,j,k]),\n                                int (val.parent_matrix[i,k,j]),\n                                int (val.unclear_parent_matrix[i,j,k]),\n                            ) )\n\n            # speed gain for using executemany_raw: 65s to 55s :-(\n            # probably the bottleneck here is string formatting with %s\n            executemany_raw (conn, """"""\n            INSERT INTO affinity (rg_id, ms_id1, ms_id2,\n                                  affinity, common, equal,\n                                  older, newer, unclear,\n                                  p_older, p_newer, p_unclear)\n            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n            """""", parameters, values)\n\n        log (logging.DEBUG, ""eq:""        + str (val.eq_matrix))\n        log (logging.DEBUG, ""ancestor:""  + str (val.ancestor_matrix))\n        log (logging.DEBUG, ""unclear:""   + str (val.unclear_ancestor_matrix))\n        log (logging.DEBUG, ""and:""       + str (val.and_matrix))\n'"
ntg_common/config.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""The commandline and configuration stuff.""""""\n\nimport logging\nimport types\n\nclass Args:\n    pass\n\nargs = Args ()\n"""""" Globally accessible arguments from command line. """"""\n\n\nclass Formatter (logging.Formatter):\n    """""" Allows colorful formatting of log lines. """"""\n    COLORS = {\n        logging.CRITICAL : (\'\\x1B[38;2;255;0;0m\',  \'\\x1B[0m\'),\n        logging.ERROR    : (\'\\x1B[38;2;255;0;0m\',  \'\\x1B[0m\'),\n        logging.WARN     : (\'\\x1B[38;2;255;64;0m\', \'\\x1B[0m\'),\n        logging.INFO     : (\'\', \'\'),\n        logging.DEBUG    : (\'\', \'\'),\n    }\n\n    def format (self, record):\n        record.esc0, record.esc1 = self.COLORS[record.levelno]\n        return super ().format (record)\n\n\ndef config_from_pyfile (filename):\n    """"""Mimic Flask config files.\n\n    Emulate the Flask config file parser so we can use the same config files for both,\n    the server and this script.\n\n    """"""\n\n    d = types.ModuleType (\'config\')\n    d.__file__ = filename\n    try:\n        with open (filename) as config_file:\n            exec (compile (config_file.read (), filename, \'exec\'), d.__dict__)\n    except IOError as e:\n        e.strerror = \'Unable to load configuration file (%s)\' % e.strerror\n        raise\n\n    conf = {}\n    for key in dir (d):\n        if key.isupper ():\n            conf[key] = getattr (d, key)\n    return conf\n\n\ndef init_logging (args, *handlers):\n    """""" Init the logging stuff. """"""\n\n    LOG_LEVELS = {\n        0: logging.CRITICAL,  #\n        1: logging.ERROR,     # -v\n        2: logging.WARN,      # -vv\n        3: logging.INFO,      # -vvv\n        4: logging.DEBUG      # -vvvv\n    }\n    args.log_level = LOG_LEVELS.get (args.verbose, logging.DEBUG)\n\n    root = logging.getLogger ()\n    root.setLevel (args.log_level)\n\n    formatter = Formatter (\n        fmt = \'{esc0}{relativeCreated:6.0f} - {levelname:7} - {message}{esc1}\',\n        style=\'{\'\n    )\n\n    if not handlers:\n        handlers = [logging.StreamHandler ()] # stderr\n\n    for handler in handlers:\n        handler.setFormatter (formatter)\n        root.addHandler (handler)\n\n    if args.log_level == logging.INFO:\n        # sqlalchemy is way too verbose on level INFO\n        sqlalchemy_logger = logging.getLogger (\'sqlalchemy.engine\')\n        sqlalchemy_logger.setLevel (logging.WARN)\n'"
ntg_common/db.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""This module contains the sqlalchemy classes that create the database structure\nwe need for doing the CBGM and running the application server.\n\n""""""\n\nfrom sqlalchemy import String, Integer, Float, Boolean, DateTime, Column, Index, ForeignKey\nfrom sqlalchemy import UniqueConstraint, CheckConstraint, ForeignKeyConstraint, PrimaryKeyConstraint\nfrom sqlalchemy.dialects.postgresql import TSTZRANGE\nfrom sqlalchemy.ext import compiler\nfrom sqlalchemy.ext.declarative import declarative_base, declared_attr\nfrom sqlalchemy.schema import DDLElement\nfrom sqlalchemy.sql import text\nfrom sqlalchemy_utils import IntRangeType\n\n# let sqlalchemy manage our views\n\nclass CreateView (DDLElement):\n    def __init__ (self, name, sql):\n        self.name = name\n        self.sql = sql.strip ()\n\nclass DropView (DDLElement):\n    def __init__ (self, name):\n        self.name = name\n\n@compiler.compiles(CreateView)\ndef compile (element, compiler, **kw):\n    return \'CREATE OR REPLACE VIEW %s AS %s\' % (element.name, element.sql)\n\n@compiler.compiles(DropView)\ndef compile (element, compiler, **kw):\n    # Use CASCADE to drop dependent views because we drop the views in the same\n    # order as we created them instead of correctly using the reverse order.\n    return \'DROP VIEW IF EXISTS %s CASCADE\' % (element.name)\n\ndef view (name, metadata, sql):\n    CreateView (name, sql).execute_at (\'after-create\', metadata)\n    DropView (name).execute_at (\'before-drop\', metadata)\n\n\n# let sqlalchemy manage our functions\n\nclass CreateFunction (DDLElement):\n    def __init__ (self, name, params, returns, sql, **kw):\n        self.name       = name\n        self.params     = params\n        self.returns    = returns\n        self.sql        = sql.strip ()\n        self.language   = kw.get (\'language\', \'SQL\')\n        self.volatility = kw.get (\'volatility\', \'VOLATILE\')\n\nclass DropFunction (DDLElement):\n    def __init__ (self, name, params):\n        self.name   = name\n        self.params = params\n\n@compiler.compiles(CreateFunction)\ndef compile (element, compiler, **kw):\n    return \'CREATE OR REPLACE FUNCTION {name} ({params}) RETURNS {returns} LANGUAGE {language} {volatility} AS $$ {sql} $$\'.format (**element.__dict__)\n\n@compiler.compiles(DropFunction)\ndef compile (element, compiler, **kw):\n    return \'DROP FUNCTION IF EXISTS {name} ({params}) CASCADE\'.format (**element.__dict__)\n\ndef function (name, metadata, params, returns, sql, **kw):\n    CreateFunction (name, params, returns, sql, **kw).execute_at (\'after-create\', metadata)\n    DropFunction (name, params).execute_at (\'before-drop\', metadata)\n\n\n# let sqlalchemy manage our foreign data wrappers\n\nclass CreateFDW (DDLElement):\n    def __init__ (self, name, pg_db, mysql_db):\n        self.name     = name\n        self.pg_db    = pg_db\n        self.mysql_db = mysql_db\n\nclass DropFDW (DDLElement):\n    def __init__ (self, name, pg_db, mysql_db):\n        self.name     = name\n        self.pg_db    = pg_db\n        self.mysql_db = mysql_db\n\n@compiler.compiles(CreateFDW)\ndef compile (element, compiler, **kw):\n    pp = element.pg_db.params\n    mp = element.mysql_db.params\n    return \'\'\'\n    CREATE SCHEMA {name};\n    -- GRANT CONNECT ON DATABASE XXX  TO ntg_readonly;\n    GRANT USAGE ON SCHEMA ntg TO ntg_readonly;\n    GRANT SELECT ON ALL TABLES IN SCHEMA ntg TO ntg_readonly;\n    GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA ntg TO ntg_readonly;\n    -- Following commands don\'t work because you have to be superuser:\n    -- CREATE EXTENSION mysql_fdw;\n    -- GRANT USAGE ON FOREIGN DATA WRAPPER mysql_fdw TO {username};\n    CREATE SERVER {name}_server FOREIGN DATA WRAPPER mysql_fdw OPTIONS (host \'{host}\', port \'{port}\');\n    CREATE USER MAPPING FOR {pg_user} SERVER {name}_server OPTIONS (username \'{username}\', password \'{password}\');\n    IMPORT FOREIGN SCHEMA ""{database}"" FROM SERVER {name}_server INTO {name};\n    \'\'\'.format (name = element.name, pg_database = pp[\'database\'], pg_user = pp[\'user\'], **mp)\n\n@compiler.compiles(DropFDW)\ndef compile (element, compiler, **kw):\n    pp = element.pg_db.params\n    mp = element.mysql_db.params\n    return \'\'\'\n    DROP SCHEMA IF EXISTS {name} CASCADE;\n    DROP USER MAPPING IF EXISTS FOR {pg_user} SERVER {name}_server;\n    DROP SERVER IF EXISTS {name}_server;\n    \'\'\'.format (name = element.name, pg_database = pp[\'database\'], pg_user = pp[\'user\'], **mp)\n\ndef fdw (name, metadata, pg_database, mysql_db):\n    CreateFDW (name, pg_database, mysql_db).execute_at (\'after-create\', metadata)\n    DropFDW (name, pg_database, mysql_db).execute_at (\'before-drop\', metadata)\n\n\n# let sqlalchemy manage generic stuff like triggers, aggregates, unique partial indices\n\nclass CreateGeneric (DDLElement):\n    def __init__ (self, create_cmd):\n        self.create = create_cmd\n\nclass DropGeneric (DDLElement):\n    def __init__ (self, drop_cmd):\n        self.drop = drop_cmd\n\n@compiler.compiles(CreateGeneric)\ndef compile (element, compiler, **kw):\n    return element.create\n\n@compiler.compiles(DropGeneric)\ndef compile (element, compiler, **kw):\n    return element.drop\n\ndef generic (metadata, create_cmd, drop_cmd):\n    CreateGeneric (create_cmd).execute_at (\'after-create\', metadata)\n    DropGeneric (drop_cmd).execute_at (\'before-drop\', metadata)\n\n\n# Input tables from the Nestle-Aland database\n\nBase = declarative_base ()\n\nCreateGeneric (\'CREATE SCHEMA ntg\').execute_at (\'before-create\', Base.metadata)\nDropGeneric (\'DROP SCHEMA IF EXISTS ntg CASCADE\').execute_at (\'after-drop\', Base.metadata)\n\nBase.metadata.schema = \'ntg\'\n\nclass Att (Base):\n    r""""""Input buffer table for the Nestle-Aland ECM_Acts*GVZ tables.\n\n    The Nestle-Aland database contains one table for each chapter (for\n    historical reasons). As first step we copy those many tables into one big\n    table, this one.\n\n    This table contains a `negative apparatus <transform-positive>` of all\n    manuscripts.  For the CBGM the data in this table has to be normalised into\n    our database structure and converted into a positive apparatus.\n\n    .. sauml::\n       :include: att\n\n    .. _att:\n\n    .. attribute:: hsnr\n\n       Interne Handschriftnummer.\n\n    .. attribute:: hs\n\n       Siglum der Handschrift.  An das Siglum werden :ref:`Suffixe <suffix>`\n       angeh\xc3\xa4ngt, die die Hand und die Lesung bezeichnen.  Im Laufe der\n       Verarbeitung werden die Lesarten reduziert, bis nur eine Lesart pro\n       Handschrift \xc3\xbcbrigbleibt.  Parallel dazu werden die Suffixe von den Siglen\n       entfernt.\n\n    .. attribute:: begadr, endadr\n\n       Zusammengesetzt aus Buch, Kapitel, Vers, Wort.  Es werden W\xc3\xb6rter und\n       Zwischenr\xc3\xa4ume gez\xc3\xa4hlt.  Gerade Zahlen bezeichnen ein Wort, ungerade einen\n       Zwischenraum.\n\n    .. attribute:: labez\n\n       See the :ref:`description of this field in table readings <labez>`.\n\n    .. attribute:: labezsuf\n\n       Contains auxiliary information about the reading:\n\n       .. data:: f\n\n          Fehler.  The reading is considered a scribal error.\n\n       .. data:: o\n\n          Orthographicum.  The reading is considered an orthographical variant,\n          eg. a variant place name.\n\n       If the labez is \'zw\' labezsuf contains a ""/""-separated list of possible\n       readings, eg. ""a/b_o/c_f"" means this reading may be \'a\' or an\n       orthographicum of \'b\' or a scribal error of \'c\'.\n\n    .. _suffix:\n\n    .. attribute:: suffix\n\n       .. data:: \\*\n\n          Erste, urspr\xc3\xbcngliche Hand\n\n       .. data:: C*\n\n          Von erster Hand korrigiert\n\n       .. data:: C1\n\n          Erster Korrektor (Korrektoren der ersten Stunde)\n\n       .. data:: C2\n\n          Zweiter Korrektor (Korrektoren aus sp\xc3\xa4teren Jahrhunderten)\n\n       .. data:: C\n\n          Korrektor (Korrektor aus ungewisser Epoche)\n\n       .. data:: L1, L2\n\n          Unterschiedliche Lesungen in einem Lektionar.  L2 ist f\xc3\xbcr die CBGM\n          nicht relevant.\n\n       .. data:: T1, T2\n\n          Unterschiedliche Lesungen des Textes der ersten Hand.  Die erste Hand\n          hat diese Passagen mehrmals abgeschrieben, vielleicht aus\n          unterschiedlicher Quelle.  Bei fehlender \xc3\x9cbereinstimmung mu\xc3\x9f \'zw\'\n          gesetzt werden.\n\n       .. data:: A\n\n          Vom Schreiber selbst gekennzeichnete alternative Lesart.  F\xc3\xbcr die CBGM\n          nicht relevant.\n\n       .. data:: K\n\n          Varianten im Kommentar einer Handschrift.  F\xc3\xbcr die CBGM nicht relevant.\n\n       .. data:: s, s1, s2\n\n          (supplement) Nachtr\xc3\xa4gliche Erg\xc3\xa4nzung verlorener Stellen.  Bei nur\n          einer Erg\xc3\xa4nzung wird \'s\' verwendet.  Bei mehreren Erg\xc3\xa4nzungen werden\n          \'s1\', \'s2\', etc. f\xc3\xbcr jeweils einen Abschnitt verwendet.  Erg\xc3\xa4nzungen\n          k\xc3\xb6nnen nicht die Authorit\xc3\xa4t der jeweiligen Hs beanspruchen.\n\n       .. data:: V\n\n          (vid, ut videtur) augenscheinlich.  Unsichere aber h\xc3\xb6chst\n          wahrscheinliche Lesung.  Ist f\xc3\xbcr die CBGM als sichere Lesart zu\n          akzeptieren.\n\n    .. attribute:: fehler\n\n       Denotes an orthografical error in Catholic Letters.  This became part of\n       labezsuf in the other books.\n\n    .. attribute:: base\n\n       Basistext. Nur relevant bei :ref:`Fehlversen <fehlvers>`.\n\n       ""\'base = b\' steht f\xc3\xbcr eine alternative Subvariante (dem Textus receptus).""\n       -- prepare4cbgm_10.py\n\n       .. data:: a\n\n          Urtext\n\n       .. data:: b\n\n          :ref:`Textus Receptus <rt>`.\n\n    .. attribute:: comp\n\n       ""Eine variierte Stelle ist eine umfasste Stelle, wenn comp = \'x\' ist.""\n       -- prepare4cbgm_10.py\n\n       .. data:: x\n\n          :ref:`Umfa\xc3\x9fte Variante <umfasst>`.\n\n    .. attribute:: lekt\n\n       Lektionen in einem Lektionar.\n\n    """"""\n\n    __tablename__ = \'att\'\n\n    id           = Column (Integer,       primary_key = True, autoincrement = True)\n    hsnr         = Column (Integer,       nullable = False, index = True)\n    hs           = Column (String(32),    nullable = False, index = True)\n    begadr       = Column (Integer,       nullable = False, index = True)\n    endadr       = Column (Integer,       nullable = False, index = True)\n    labez        = Column (String(64),    nullable = False, server_default = \'\')\n    labezsuf     = Column (String(64),    server_default = \'\')\n    certainty    = Column (Float(16),     nullable = False, server_default = \'1.0\')\n    lemma        = Column (String(1024),  server_default = \'\')\n    lesart       = Column (String(1024),  server_default = \'\')\n    labezorig    = Column (String(32),    nullable = False, server_default = \'\')\n    labezsuforig = Column (String(64),    server_default = \'\')\n    suffix2      = Column (String(32),    server_default = \'\')\n    kontrolle    = Column (String(1),     server_default = \'\')\n    fehler       = Column (String(2),     server_default = \'\')\n    suff         = Column (String(32),    server_default = \'\')\n    vid          = Column (String(32),    server_default = \'\')\n    vl           = Column (String(32),    server_default = \'\')\n    korr         = Column (String(32),    server_default = \'\')\n    lekt         = Column (String(32),    server_default = \'\')\n    komm         = Column (String(32),    server_default = \'\')\n    anfalt       = Column (Integer)\n    endalt       = Column (Integer)\n    labezalt     = Column (String(32),    server_default = \'\')\n    lasufalt     = Column (String(32),    server_default = \'\')\n    base         = Column (String(8),     server_default = \'\')\n    over         = Column (String(1),     server_default = \'\')\n    comp         = Column (String(1),     server_default = \'\')\n    over1        = Column (String(1),     server_default = \'\')\n    comp1        = Column (String(1),     server_default = \'\')\n    printout     = Column (String(32),    server_default = \'\')\n    category     = Column (String(1),     server_default = \'\')\n    passage      = Column (IntRangeType,  nullable = False)\n\n    __table_args__ = (\n        Index (\'ix_att_begadr_endadr_hs\', begadr, endadr, hs),\n        Index (\'ix_att_hs_passage\', hs, passage, unique = True, postgresql_where = certainty == 1.0),\n    )\n\n\nclass Lac (Base):\n    """"""Input buffer table for the Nestle-Aland ECM_Acts*GVZLac tables.\n\n    The Nestle-Aland database contains one table for each chapter (for\n    historical reasons). As first step we copy those many tables into one big\n    table, this one.\n\n    This table contains a list of all lacunae in all manuscripts.  It records\n    the start and end of each lacuna.  A lacuna entry generally spans many\n    passages in the Att table.\n\n    This table has the same structure as table Att.  For the description of the\n    columns see :ref:`table Att <att>`.\n\n    .. sauml::\n       :include: lac\n\n    """"""\n\n    __tablename__ = \'lac\'\n\n    id        = Column (Integer,       primary_key = True, autoincrement = True)\n    hsnr      = Column (Integer,       nullable = False)\n    hs        = Column (String(32),    nullable = False)\n    begadr    = Column (Integer,       nullable = False)\n    endadr    = Column (Integer,       nullable = False)\n    labez     = Column (String(64),    server_default = \'\')\n    labezsuf  = Column (String(64),    server_default = \'\')\n    lemma     = Column (String(1024),  server_default = \'\')\n    lesart    = Column (String(1024),  server_default = \'\')\n    suffix2   = Column (String(32),    server_default = \'\')\n    kontrolle = Column (String(1),     server_default = \'\')\n    fehler    = Column (String(2),     server_default = \'\')\n    suff      = Column (String(32),    server_default = \'\')\n    vid       = Column (String(32),    server_default = \'\')\n    vl        = Column (String(32),    server_default = \'\')\n    korr      = Column (String(32),    server_default = \'\')\n    lekt      = Column (String(32),    server_default = \'\')\n    komm      = Column (String(32),    server_default = \'\')\n    anfalt    = Column (Integer)\n    endalt    = Column (Integer)\n    labezalt  = Column (String(32),    server_default = \'\')\n    lasufalt  = Column (String(32),    server_default = \'\')\n    base      = Column (String(8),     server_default = \'\')\n    over      = Column (String(1),     server_default = \'\')\n    comp      = Column (String(1),     server_default = \'\')\n    over1     = Column (String(1),     server_default = \'\')\n    comp1     = Column (String(1),     server_default = \'\')\n    printout  = Column (String(32),    server_default = \'\')\n    category  = Column (String(1),     server_default = \'\')\n    passage   = Column (IntRangeType,  nullable = False)\n\n    __table_args__ = (\n        Index (\'ix_lac_passage_gist\', passage, postgresql_using = \'gist\'),\n        UniqueConstraint (hs, passage)\n    )\n\n\nfunction (\'char_labez\', Base.metadata, \'l INTEGER\', \'CHAR (3)\', \'\'\'\n    SELECT CASE WHEN l > 0 THEN chr (l + 96) ELSE \'z\' END\n    \'\'\', volatility = \'IMMUTABLE\')\n\nfunction (\'ord_labez\', Base.metadata, \'l CHAR (2)\', \'INTEGER\', \'\'\'\n    SELECT CASE WHEN ascii (l) >= 122 THEN 0 ELSE ascii (l) - 96 END\n    \'\'\', volatility = \'IMMUTABLE\')\n\nfunction (\'adr2bk_id\', Base.metadata, \'adr INTEGER\', \'INTEGER\', \'\'\'\n    SELECT (adr / 10000000)\n    \'\'\', volatility = \'IMMUTABLE\')\n\nfunction (\'adr2chapter\', Base.metadata, \'adr INTEGER\', \'INTEGER\', \'\'\'\n    SELECT ((adr / 100000) %% 100)\n    \'\'\', volatility = \'IMMUTABLE\')\n\nfunction (\'adr2verse\', Base.metadata, \'adr INTEGER\', \'INTEGER\', \'\'\'\n    SELECT ((adr / 1000) %% 100)\n    \'\'\', volatility = \'IMMUTABLE\')\n\nfunction (\'adr2word\', Base.metadata, \'adr INTEGER\', \'INTEGER\', \'\'\'\n    SELECT (adr %% 1000)\n    \'\'\', volatility = \'IMMUTABLE\')\n\n\n# Tables for the CBGM / App Server\n\nBase2 = declarative_base ()\nBase2.metadata.schema = \'ntg\'\n\nclass Manuscripts (Base2):\n    """"""A table that lists all the manuscripts.\n\n    This table lists all the manuscripts of New Testament that we have collated\n    for the edition.\n\n    .. sauml::\n       :include: manuscripts\n\n    .. attribute:: ms_id\n\n       The primary key.  We use a surrogate integer key because we need to\n       interface with numpy, which only allows for integer row and column\n       indices.  If we every lose this requirement, hsnr will become our primary\n       key.\n\n    .. attribute:: hsnr\n\n        The project-internal number of the manuscript.\n\n        This is a six-digit number. The first digit encodes the type of the\n        manuscript: 1 = papyrus, 2 = uncial, 3 = minuscule, 4 = lectionary, 5 =\n        patristic citations and versions.  The next 4 digits are taken from the\n        digits of the Gregory-Aland number, eg. P45 would yield 0045.  The last\n        digit encodes supplements: 0 = original ms., 1 = first supplement, 2 =\n        second supplement.\n\n        N.B. Patristic citations and versions are not used in the CBGM, and thus\n        purged from the database.\n\n    .. attribute:: hs\n\n        The Gregory-Aland number of the manuscript. eg. \'P45\', \'03\', or \'1739\'.\n\n    """"""\n\n    __tablename__ = \'manuscripts\'\n\n    ms_id     = Column (Integer,       primary_key = True, autoincrement = True)\n    hsnr      = Column (Integer,       nullable = False, unique = True)\n    hs        = Column (String (32),   nullable = False, unique = True)\n\n\nclass Books (Base2):\n    """"""A table that lists all the books of the New Testament.\n\n    This table lists all the books of the NT and the book id given to them.\n\n    .. sauml::\n       :include: books\n\n    .. attribute:: bk_id\n\n        The book id: 1 - 27 (Matthew - Revelation).\n\n    .. attribute:: siglum\n\n        The book siglum, eg. \'Mt\'\n\n    .. attribute:: book\n\n        The book name, eg. \'Matthew\'\n\n    .. attribute:: passage\n\n        The book extent in passages.\n\n    """"""\n\n    __tablename__ = \'books\'\n\n    bk_id     = Column (Integer,       primary_key = True, autoincrement = True)\n\n    siglum    = Column (String,        nullable = False)\n    book      = Column (String,        nullable = False)\n    passage   = Column (IntRangeType,  nullable = False)\n\n    __table_args__ = (\n        UniqueConstraint (siglum),\n        UniqueConstraint (book),\n        Index (\'ix_books_passage_gist\', passage, postgresql_using = \'gist\'),\n    )\n\n\nclass Passages (Base2):\n    """"""A table that lists the variant passages.\n\n    This table lists all the passages we established during the collation of the\n    manuscripts.  Passages that are the same in all manuscripts (invariant) are\n    purged because they are irrelevant to the CBGM.\n\n    .. sauml::\n       :include: passages\n\n    .. attribute:: pass_id\n\n       The primary key.  We use a surrogate integer key because we need to\n       interface with numpy, which only allows for integer row and column\n       indices.  If we every lose this requirement, passage will become our\n       primary key (but join performance should also be considered).\n\n    .. attribute:: passage\n\n        The extent of the passage.\n\n        The beginning and end of every passage is encoded in this way:\n\n          book id * 10,000,000 +\n          chapter *    100,000 +\n          verse   *      1,000 +\n          word    *          2\n\n        Words are always even and the space between to words is always odd.\n\n    .. attribute:: variant\n\n        True if this passage is a variant passage.  (It has at least two\n        different certain readings.)\n\n    .. attribute:: spanning\n\n        True if this passage is spanning other passages.\n\n    .. attribute:: spanned\n\n        True if this passage is spanned by other passages.\n\n    .. attribute:: fehlvers\n\n        True if this passage is a later addition, eg. the pericope adulterae.\n\n        Only set if the passages is spanned.\n\n    """"""\n\n    __tablename__ = \'passages\'\n\n    pass_id   = Column (Integer,       primary_key = True, autoincrement = True)\n\n    bk_id     = Column (Integer,       nullable = False)\n\n    begadr    = Column (Integer,       nullable = False)\n    endadr    = Column (Integer,       nullable = False)\n    passage   = Column (IntRangeType,  nullable = False)\n\n    variant   = Column (Boolean,       nullable = False, server_default = \'False\')\n    spanning  = Column (Boolean,       nullable = False, server_default = \'False\')\n    spanned   = Column (Boolean,       nullable = False, server_default = \'False\')\n    fehlvers  = Column (Boolean,       nullable = False, server_default = \'False\')\n\n    __table_args__ = (\n        ForeignKeyConstraint ([bk_id], [\'books.bk_id\'], ondelete = \'CASCADE\'),\n        UniqueConstraint (passage, name = \'unique_passages_passage\'), # needs name\n        Index (\'ix_passages_passage_gist\', passage, postgresql_using = \'gist\'),\n    )\n\n\nclass Readings (Base2):\n    """"""A table that contains the different readings found at each passage.\n\n    First scribal errors are corrected and orthographical differences are\n    normalized, then equal readings are grouped.  Each group of readings is\n    assigned an id, the \'labez\'.\n\n    .. sauml::\n       :include: readings\n\n    .. _labez:\n\n    .. attribute:: labez\n\n        (LesArtBEZeichnung).  Usually \'a\' indicates the original reading, while\n        \'b\'...\'y\' indicate readings relegated to the apparatus, although this is\n        not necessarily so.  Exceptions are the :ref:`Fehlverse <fehlvers>` and\n        the cases where no original reading could be assessed.\n\n        Readings starting with \'z\' have special meaning:\n\n        .. data:: zu\n\n            Hier nicht zitierbar aufgrund einer \xc3\xbcbergreifenden Variante.  Diese\n            umfa\xc3\x9fte Variante wurde schon in der umfassenden Variante\n            verzeichnet.  Entspricht in der ECM einem Pfeil nach oben.  In der\n            CBGM ist \'zu\' wie \'zz\' zu behandeln.\n\n        .. data:: zv\n\n            There is an illegible addition in the manuscript(s) cited which\n            makes it impossible to ascribe it to a known variant.\n\n        .. data:: zw\n\n            What remains of the text of the manuscript(s) cited would allow\n            reconstruction in agreement with two or more different variants.\n            Entspricht in der ECM einem Doppelpfeil nach links-rechts.\n\n            The reading \'zw\' is used only in the Att table.  In this table each\n            questionable reading will get its own entry with a certainty < 1.0.\n\n        .. data:: zz\n\n            The reading is too lacunose to be identified.\n\n            Alle Verzeichnungen, die aus der Tabelle der Lacunae erzeugt wurden,\n            erhalten labez = \'zz\'.\n\n            Ein Wort steht nicht in der systematischen L\xc3\xbcckenliste wenn\n            mindestens ein Buchstabe vorhanden ist.  In diesem Fall steht es in\n            der stellenbezogenen L\xc3\xbcckenliste.\n\n        Caveat: die Lesart der Handschrift \'A\' kann trotz negativem Apparat in\n        der Tabelle Att in derselben Passage mehrmals vorkommen, weil an einigen\n        Stellen im Nestle-Aland ein positiver Apparat benutzt wurde.\n\n    .. attribute:: lesart\n\n        The normalized reading.  Scribal errors are silently corrected and\n        orthographic variants are normalized.  Following abbreviations are used:\n\n        .. data:: om\n\n          Missing text (omissio).  The scribe did not write any text.\n\n        .. data:: NULL\n\n          Missing substrate (lacuna).  The manuscript is damaged or missing.\n    """"""\n\n    __tablename__ = \'readings\'\n\n    pass_id   = Column (Integer,       nullable = False)\n    labez     = Column (String (3),    nullable = False)\n\n    lesart    = Column (String (1024))\n\n    __table_args__ = (\n        PrimaryKeyConstraint (pass_id, labez),\n        ForeignKeyConstraint ([pass_id], [\'passages.pass_id\'], ondelete = \'CASCADE\'),\n    )\n\n\nclass Apparatus (Base2):\n    """"""A table that contains the `positive apparatus <transform-positive>`.\n\n    .. sauml::\n       :include: apparatus\n\n    .. attribute:: cbgm\n\n        True if this entry is eligible for CBGM, eg. is by the orginal scribe\n        and is 100% certain.  There can be only one entry eligible for CBGM for\n        every manuscript and passage.\n\n    .. attribute:: labezsuf\n\n        Contains auxiliary information about the reading:\n\n        .. data:: f\n\n            Fehler.  The reading is considered a scribal error.\n\n        .. data:: o\n\n           Orthographicum.  The reading is considered an orthographical variant,\n           eg. a variant place name.\n\n    .. attribute:: certainty\n\n        Certainty of the reading for the purposes of the CBGM.  Only readings\n        with a certainty of 1.0 are used by the CBGM. A certainty of 1.0 is\n        given if the reading unequivocally witnesses for one labez.\n\n        There can be only one reading with a certainty of 1.0, but multiple\n        readings with certainty < 1.0, all of them summing to no more than 1.0.\n\n    .. attribute:: lesart\n\n        The actual reading offered by the manuscript.  A lacuna is stored as\n        NULL.  Omitted text is stored as the empty string.\n\n        This field is also set to NULL if the manuscript offers the same reading\n        as recorded in the :class:`~ntg_common.db.Readings` table for the\n        manuscript\'s labez.  (Saves space and can easily be reconstructed.)\n\n        As a rule of thumb: For \'f\' and \'o\' readings (errors and orthographic\n        variants) the actual reading will be inserted.\n\n    .. attribute:: origin\n\n        Used only for debugging.  Shows where this entry in the positive\n        apparatus came from.\n\n        .. data:: ATT\n\n           Copied from negative apparatus in att table in\n           :meth:`~scripts.cceh.prepare.fill_apparatus_table`.\n\n        .. data:: BYZ\n\n           Deduced `mt` in :meth:`~scripts.cceh.prepare.build_MT_text`.\n\n        .. data:: DEF\n\n           Default value from conversion to positive apparatus in\n           :meth:`~scripts.cceh.prepare.fill_apparatus_table`.\n\n        .. data:: LAC\n\n           Unrolled lacuna from lac table in\n           :meth:`~scripts.cceh.prepare.fill_apparatus_table`.\n\n        .. data:: LOC\n\n           Deduced from locstem table in\n           :meth:`~scripts.cceh.cbgm.build_A_text`.\n\n        .. data:: ZW\n\n           Uncertain reading from unrolling of \'zw\' in\n           :meth:`~scripts.cceh.prepare.unroll_zw`.\n\n    """"""\n\n    __tablename__ = \'apparatus\'\n\n    ms_id     = Column (Integer,       nullable = False, index = True)\n    pass_id   = Column (Integer,       nullable = False)\n    labez     = Column (String (3),    nullable = False)\n\n    cbgm      = Column (Boolean,       nullable = False)\n    labezsuf  = Column (String (64),   nullable = False, server_default = \'\')\n    certainty = Column (Float (16),    nullable = False, server_default = \'1.0\')\n    lesart    = Column (String (1024), nullable = True,  server_default = None)\n    origin    = Column (String (3),    nullable = False)\n\n    __table_args__ = (\n        PrimaryKeyConstraint (pass_id, ms_id, labez),\n        ForeignKeyConstraint ([ms_id], [\'manuscripts.ms_id\'], ondelete = \'CASCADE\'),\n        ForeignKeyConstraint ([pass_id, labez], [\'readings.pass_id\', \'readings.labez\'], ondelete = \'CASCADE\'),\n        Index (\'ix_apparatus_pass_id_ms_id\', pass_id, ms_id, unique = True, postgresql_where = cbgm == True),\n        CheckConstraint (\'certainty > 0.0 AND certainty <= 1.0\'),\n        CheckConstraint (\'(certainty = 1.0) >= cbgm\'),  # cbgm implies 100% certainty\n    )\n\n\nclass TTS_Mixin (object):\n    # use of declared_attr to create these columns last in table\n    @declared_attr\n    def sys_period (cls):\n        return Column (TSTZRANGE,  nullable = False, server_default = text (\'tstzrange (now (), NULL)\'))\n\n    @declared_attr\n    def user_id_start (cls):\n        return Column (Integer,    nullable = False)\n\n    @declared_attr\n    def user_id_stop (cls):\n        return Column (Integer,    nullable = True)\n\n\nclass Cliques_Mixin (TTS_Mixin):\n    pass_id   = Column (Integer,    nullable = False)\n    labez     = Column (String (3), nullable = False)\n    clique    = Column (String (2), nullable = False, server_default = \'1\')\n\n\nclass Cliques (Cliques_Mixin, Base2):\n    """"""A table that contains the cliques at every passage\n\n    A clique is a set of strongly related manuscripts that offer the same\n    reading.  A reading may have been originated independently more than once,\n    but in a clique a reading has been originated only once.\n\n    This is the current table of a transaction state table pair.\n    :class:`Cliques_TTS` is the table that contains the past rows.  See\n    :ref:`transaction-time state tables<tts>`.\n\n    .. sauml::\n       :include: cliques\n\n    .. attribute:: clique\n\n        Name of the Clique.  \'1\', \'2\' ...\n\n    .. attribute:: sys_period\n\n       The time period in which this row is valid.  In this table all rows are\n       still valid, so the end of the period is not set.\n\n    .. attribute:: user_id_start\n\n       The id of the user making the change at the start of the validity period.\n       See: :class:`.User`.\n\n    .. attribute:: user_id_stop\n\n       The id of the user making the change at the end of the validity period.\n       See: :class:`.User`.\n\n    """"""\n\n    __tablename__ = \'cliques\'\n\n    __table_args__ = (\n        PrimaryKeyConstraint (\'pass_id\', \'labez\', \'clique\'),\n        ForeignKeyConstraint ([\'pass_id\', \'labez\'], [\'readings.pass_id\', \'readings.labez\'],\n                              ondelete = \'CASCADE\'),\n    )\n\n\nclass Cliques_TTS (Cliques_Mixin, Base2):\n    """"""A table that contains the cliques at every passage\n\n    This is the past table of a transaction state table pair.  :class:`Cliques`\n    is the table that contains the current rows.  The structure of the two\n    tables is the same.  See :ref:`transaction-time state tables<tts>`.\n\n    """"""\n\n    __tablename__ = \'cliques_tts\'\n\n    __table_args__ = (\n        PrimaryKeyConstraint (\'pass_id\', \'labez\', \'clique\', \'sys_period\'),\n    )\n\n\nclass MsCliques_Mixin (TTS_Mixin):\n    ms_id         = Column (Integer,    nullable = False, index = True)\n    pass_id       = Column (Integer,    nullable = False)\n    labez         = Column (String (3), nullable = False)\n    clique        = Column (String (2), nullable = False, server_default = \'1\')\n\n\nclass MsCliques (MsCliques_Mixin, Base2):\n    """"""A table that relates manuscripts and cliques.\n\n    This table records the editorial decisions regarding which manuscripts\n    represent each clique.  The editors decide which reading is derived from\n    which other reading(s) at each passage.\n\n    This is the current table of a transaction state table pair.\n    :class:`MsCliques_TTS` is the table that contains the past rows.  See\n    :ref:`transaction-time state tables<tts>`.\n\n    .. sauml::\n       :include: ms_cliques\n\n    .. attribute:: labez, clique\n\n       The clique this manuscript represents.\n\n    .. attribute:: sys_period\n\n       The time period in which this row is valid.  In this table all rows are\n       still valid, so the end of the period is not set.\n\n    .. attribute:: user_id_start\n\n       The id of the user making the change at the start of the validity period.\n       See: :class:`.User`.\n\n    .. attribute:: user_id_stop\n\n       The id of the user making the change at the end of the validity period.\n       See: :class:`.User`.\n\n    """"""\n\n    __tablename__ = \'ms_cliques\'\n\n    __table_args__ = (\n        # Apparatus can have more than one labez per passage per manuscript\n        PrimaryKeyConstraint (\'ms_id\', \'pass_id\', \'labez\'),\n        ForeignKeyConstraint ([\'ms_id\', \'pass_id\', \'labez\'],\n                              [\'apparatus.ms_id\', \'apparatus.pass_id\', \'apparatus.labez\'],\n                              deferrable = True, ondelete = \'CASCADE\'),\n        ForeignKeyConstraint ([\'pass_id\', \'labez\', \'clique\'],\n                              [\'cliques.pass_id\', \'cliques.labez\', \'cliques.clique\'],\n                              ondelete = \'CASCADE\'),\n    )\n\n\nclass MsCliques_TTS (MsCliques_Mixin, Base2):\n    """"""A table that relates manuscripts and cliques.\n\n    This is the past table of a transaction state table pair.\n    :class:`MsCliques` is the table that contains the current rows.  The\n    structure of the two tables is the same.  See :ref:`transaction-time state\n    tables<tts>`.\n\n    """"""\n\n    __tablename__ = \'ms_cliques_tts\'\n\n    __table_args__ = (\n        PrimaryKeyConstraint (\'ms_id\', \'pass_id\', \'labez\', \'sys_period\'),\n    )\n\n\nclass LocStem_Mixin (TTS_Mixin):\n    pass_id       = Column (Integer,    nullable = False)\n    labez         = Column (String (3), nullable = False)\n    clique        = Column (String (2), nullable = False, server_default = \'1\')\n    source_labez  = Column (String (3), nullable = False)\n    source_clique = Column (String (2), nullable = False, server_default = \'1\')\n\n\nclass LocStem (LocStem_Mixin, Base2):\n    """"""A table that contains the priority of the cliques at each passage\n\n    This table contains one DAG (directed acyclic graph) of cliques for each\n    passage.  The editors decide from which other clique(s) each clique is\n    derived, or if it is original.\n\n    This is the current table of a transaction state table pair.\n    :class:`LocStem_TTS` is the table that contains the past rows.  See\n    :ref:`transaction-time state tables<tts>`.\n\n\n    .. sauml::\n       :include: locstem\n\n    .. attribute:: labez, clique\n\n       The younger clique which was derived from the older clique.\n\n    .. attribute:: source_labez, source_clique\n\n       The older clique that was the source of the younger clique, or \'*\' if\n       the reading is original, or \'?\' if the source is unknown.\n\n       Note: These columns should have a foreign key constraint into the cliques\n       table but do not, because postgres doesn\'t support partial foreign keys\n       and the cliques table does not contain the \'*\' and \'?\'  pseudo-cliques.\n\n    .. attribute:: sys_period\n\n       The time period in which this row is valid.  In this table all rows are\n       still valid, so the end of the period is not set.\n\n    .. attribute:: user_id_start\n\n       The id of the user making the change at the start of the validity period.\n       See: :class:`.User`.\n\n    .. attribute:: user_id_stop\n\n       The id of the user making the change at the end of the validity period.\n       See: :class:`.User`.\n\n    """"""\n\n    __tablename__ = \'locstem\'\n\n    __table_args__ = (\n        PrimaryKeyConstraint (\'pass_id\', \'labez\', \'clique\', \'source_labez\', \'source_clique\'),\n        Index (\'ix_locstem_unique_original\', \'pass_id\', unique = True,\n               postgresql_where = text (""source_labez = \'*\'"")),\n        ForeignKeyConstraint ([\'pass_id\', \'labez\', \'clique\'],\n                              [\'cliques.pass_id\', \'cliques.labez\', \'cliques.clique\'],\n                              ondelete = \'CASCADE\'),\n        CheckConstraint (\'labez != source_labez\', name=\'check_same_source\'),\n    )\n\n\nclass LocStem_TTS (LocStem_Mixin, Base2):\n    """"""A table that contains the priority of the cliques at each passage\n\n    This is the past table of a transaction state table pair.  :class:`LocStem`\n    is the table that contains the current rows.  The structure of the two\n    tables is the same.  See :ref:`transaction-time state tables<tts>`.\n\n    """"""\n\n    __tablename__ = \'locstem_tts\'\n\n    __table_args__ = (\n        PrimaryKeyConstraint (\'pass_id\', \'labez\', \'clique\', \'source_labez\', \'source_clique\', \'sys_period\'),\n    )\n\n\nclass Notes_Mixin (TTS_Mixin):\n    pass_id   = Column (Integer, nullable = False)\n    note      = Column (String,  nullable = False)\n\n\nclass Notes (Notes_Mixin, Base2):\n    """"""A table that contains editorial notes attached to passages.\n\n    This is the current table of a transaction state table pair.\n    :class:`Notes_TTS` is the table that contains the past rows.  See\n    :ref:`transaction-time state tables<tts>`.\n\n    """"""\n\n    __tablename__ = \'notes\'\n\n    __table_args__ = (\n        PrimaryKeyConstraint (\'pass_id\'),\n        ForeignKeyConstraint ([\'pass_id\'], [\'passages.pass_id\'], ondelete = \'CASCADE\'),\n    )\n\n\nclass Notes_TTS (Notes_Mixin, Base2):\n    """"""A table that contains editorial notes attached to passages.\n\n    This is the past table of a transaction state table pair.  :class:`Notes` is\n    the table that contains the current rows.  The structure of the two tables\n    is the same.  See :ref:`transaction-time state tables<tts>`.\n\n    """"""\n\n    __tablename__ = \'notes_tts\'\n\n    __table_args__ = (\n        PrimaryKeyConstraint (\'pass_id\', \'sys_period\'),\n    )\n\n\nclass Import_Cliques (Cliques_Mixin, Base2):\n    """"""A table to help importing of saved state.\n\n    This table is used only by the load_edits.py script.\n    """"""\n\n    __tablename__ = \'import_cliques\'\n\n    pass_id = Column (Integer)\n    passage = Column (IntRangeType, nullable = False)\n\n    __table_args__ = (\n        PrimaryKeyConstraint (\'passage\', \'labez\', \'clique\', \'sys_period\'),\n    )\n\n\nclass Import_MsCliques (MsCliques_Mixin, Base2):\n    """"""A table to help importing of saved state.\n\n    This table is used only by the load_edits.py script.\n    """"""\n\n    __tablename__ = \'import_ms_cliques\'\n\n    pass_id = Column (Integer)\n    ms_id   = Column (Integer)\n    passage = Column (IntRangeType, nullable = False)\n    hsnr    = Column (Integer,      nullable = False)\n\n    __table_args__ = (\n        PrimaryKeyConstraint (\'hsnr\', \'passage\', \'labez\', \'sys_period\'),\n    )\n\n\nclass Import_LocStem (LocStem_Mixin, Base2):\n    """"""A table to help importing of saved state.\n\n    This table is only used by the load_edits.py script.\n\n    """"""\n\n    __tablename__ = \'import_locstem\'\n\n    pass_id = Column (Integer)\n    passage = Column (IntRangeType, nullable = False)\n\n    __table_args__ = (\n        PrimaryKeyConstraint (\'passage\', \'labez\', \'clique\', \'source_labez\', \'source_clique\', \'sys_period\'),\n    )\n\n\nclass Import_Notes (Notes_Mixin, Base2):\n    """"""A table to help importing of saved state.\n\n    This table is only used by the load_edits.py script.\n\n    """"""\n\n    __tablename__ = \'import_notes\'\n\n    pass_id = Column (Integer)\n    passage = Column (IntRangeType, nullable = False)\n\n    __table_args__ = (\n        PrimaryKeyConstraint (\'passage\', \'sys_period\'),\n    )\n\n\nclass Ranges (Base2):\n    """"""A table that contains the ranges of passages for the CBGM.\n\n    The CBGM is agnostic about the division in books and chapters of the NT.  It\n    can be run on any range of passages (in theory even on sets of non-contiguous\n    passages, although not yet in this implementation).\n\n    This table contains all ranges we are interested in, that is, one range for\n    each chapter of each book and also one range for each whole book.  The\n    ranges corresponding to chapters are named by the chapter number, \'1\', \'2\',\n    ...  The whole book range is called \'All\'.\n\n    .. sauml::\n       :include: ranges\n\n    .. attribute:: range\n\n        The name of the range, eg. \'1\' for Chapter 1.\n\n    .. attribute:: passage\n\n        The extent of the range.\n\n    """"""\n\n    __tablename__ = \'ranges\'\n\n    rg_id     = Column (Integer,          primary_key = True, autoincrement = True)\n\n    bk_id     = Column (Integer,          nullable = False)\n\n    range_    = Column (\'range\', String,  nullable = False)\n\n    passage   = Column (IntRangeType,     nullable = False)\n\n    __table_args__ = (\n        ForeignKeyConstraint ([bk_id], [\'books.bk_id\'], ondelete = \'CASCADE\'),\n        Index (\'ix_ranges_passage_gist\', passage, postgresql_using = \'gist\'),\n    )\n\n\nclass Ms_Ranges (Base2):\n    """"""A table that contains CBGM output related to each manuscript.\n\n    Here we hold values that are calculated by CBGM related to one manuscript.\n\n    .. sauml::\n       :include: ms_ranges\n\n    .. attribute:: length\n\n        Calculated: no. of defined passages in the manuscript inside this range.\n\n    """"""\n\n    __tablename__ = \'ms_ranges\'\n\n    rg_id      = Column (Integer, nullable = False)\n    ms_id      = Column (Integer, nullable = False)\n\n    length     = Column (Integer)\n\n    __table_args__ = (\n        PrimaryKeyConstraint (rg_id, ms_id),\n        ForeignKeyConstraint ([ms_id], [\'manuscripts.ms_id\'], ondelete = \'CASCADE\'),\n        ForeignKeyConstraint ([rg_id], [\'ranges.rg_id\'], ondelete = \'CASCADE\'),\n    )\n\n\nclass Affinity (Base2):\n    r""""""A table that contains CBGM output related to each pair of manuscripts.\n\n    This table contains the actual results of applying the CBGM: the priority of\n    the manuscripts.  It has one row for each pair of manuscripts that have at\n    least one passage in common and each range we are interested in.\n\n    Two sets of data are included, one for the recursive interpretation of the\n    locstem data, and one for the backward-compatible non-recurisve\n    interpretation (with \'p\\_\' prefix).\n\n    .. sauml::\n       :include: affinity\n\n    .. attribute:: common\n\n        No. of passages defined in both manuscripts.\n\n    .. attribute:: equal\n\n        No. of passages that have the same reading in both manuscripts.\n\n    .. attribute:: affinity\n\n        equal / common\n\n    .. attribute:: older\n\n        No. of passages that have an older reading in ms1.\n\n    .. attribute:: newer\n\n        No. of passages that have an newer reading in ms1.\n\n    .. attribute:: unclear\n\n        No. of passages where it is unclear which reading is older.\n\n    """"""\n\n    __tablename__ = \'affinity\'\n\n    rg_id      = Column (Integer,       nullable = False)\n    ms_id1     = Column (Integer,       nullable = False)\n    ms_id2     = Column (Integer,       nullable = False)\n\n    affinity   = Column (Float,         nullable = False, server_default = \'0\')\n\n    common     = Column (Integer,       nullable = False)\n    equal      = Column (Integer,       nullable = False)\n\n    older      = Column (Integer,       nullable = False)\n    newer      = Column (Integer,       nullable = False)\n    unclear    = Column (Integer,       nullable = False)\n\n    p_older    = Column (Integer,       nullable = False)\n    p_newer    = Column (Integer,       nullable = False)\n    p_unclear  = Column (Integer,       nullable = False)\n\n    __table_args__ = (\n        PrimaryKeyConstraint (rg_id, ms_id1, ms_id2),\n        Index (\'ix_affinity_rg_id_ms_id2\', rg_id, ms_id2),\n        ForeignKeyConstraint ([rg_id, ms_id1],\n                              [\'ms_ranges.rg_id\', \'ms_ranges.ms_id\'],\n                              ondelete = \'CASCADE\'),\n        ForeignKeyConstraint ([rg_id, ms_id2],\n                              [\'ms_ranges.rg_id\', \'ms_ranges.ms_id\'],\n                              ondelete = \'CASCADE\'),\n    )\n\n\nfunction (\'labez_array_to_string\', Base2.metadata, \'a CHAR[]\', \'CHAR\', \'\'\'\nSELECT array_to_string (a, \'/\', \'\')\n\'\'\', volatility = \'IMMUTABLE\')\n\nfunction (\'varnew2labez\', Base2.metadata, \'varnew CHAR (2)\', \'CHAR\', \'\'\'\nSELECT REGEXP_REPLACE (varnew, \'[0-9]+$\', \'\')\n\'\'\', volatility = \'IMMUTABLE\')\n\nfunction (\'varnew2clique\', Base2.metadata, \'varnew CHAR (2)\', \'CHAR\', \'\'\'\nSELECT COALESCE (NULLIF (REGEXP_REPLACE (varnew, \'^[^0-9]+\', \'\'), \'\'), \'1\')\n\'\'\', volatility = \'IMMUTABLE\')\n\nfunction (\'labez_clique\', Base2.metadata, \'labez CHAR, clique CHAR\', \'CHAR\', \'\'\'\nSELECT labez || COALESCE (NULLIF (clique, \'1\'), \'\')\n\'\'\', volatility = \'IMMUTABLE\')\n\nfunction (\'reading\', Base2.metadata, \'labez CHAR, lesart VARCHAR\', \'VARCHAR\', \'\'\'\nSELECT CASE WHEN labez = \'zz\' THEN \'\' WHEN labez = \'zu\' THEN \'overlap\' ELSE COALESCE (NULLIF (lesart, \'\'), \'om\') END\n\'\'\', volatility = \'IMMUTABLE\')\n\nfunction (\'close_period\', Base2.metadata, \'period TSTZRANGE\', \'TSTZRANGE\', \'\'\'\nSELECT TSTZRANGE (LOWER (period), NOW ())\n\'\'\', volatility = \'IMMUTABLE\')\n\ngeneric (Base2.metadata, \'\'\'\nCREATE AGGREGATE labez_agg (CHAR) (\n  sfunc = array_append,\n  stype = char[],\n  initcond = \'{}\',\n  finalfunc = labez_array_to_string\n)\n\'\'\', \'\'\'\nDROP AGGREGATE IF EXISTS labez_agg (CHAR) CASCADE\n\'\'\'\n)\n\nview (\'ranges_view\', Base2.metadata, \'\'\'\n    SELECT bk.bk_id, bk.siglum, bk.book, ch.rg_id, ch.range, ch.passage\n    FROM books bk\n    JOIN ranges ch USING (bk_id)\n    \'\'\')\n\nview (\'ms_ranges_view\', Base2.metadata, \'\'\'\n    SELECT ch.*, mc.ms_id, mc.length\n    FROM ms_ranges mc\n    JOIN ranges_view ch USING (rg_id)\n    \'\'\')\n\nview (\'passages_view\', Base2.metadata, \'\'\'\n    SELECT b.bk_id, b.siglum, b.book,\n           adr2chapter (p.begadr) AS chapter,\n           adr2verse   (p.begadr) AS verse,\n           adr2word    (p.begadr) AS word,\n           p.pass_id, p.begadr, p.endadr, p.passage, p.variant, p.spanning, p.spanned, p.fehlvers\n    FROM passages p\n    JOIN books b USING (bk_id)\n    \'\'\')\n\nview (\'passages_view_lemma\', Base2.metadata, \'\'\'\n    SELECT p.*, COALESCE (rl.lesart, \'undef\') AS lemma\n    FROM passages_view p\n    LEFT JOIN (\n      SELECT r.pass_id, r.lesart\n      FROM readings r\n      JOIN locstem l ON (l.pass_id, l.labez, l.source_labez) = (r.pass_id, r.labez, \'*\')\n    ) rl ON (p.pass_id = rl.pass_id)\n    ORDER by p.pass_id;\n    \'\'\')\n\nview (\'locstem_view\', Base2.metadata, \'\'\'\n    SELECT p.begadr, p.endadr, p.passage, p.fehlvers, locstem.*\n    FROM locstem\n    JOIN passages p USING (pass_id)\n    \'\'\')\n\nview (\'readings_view\', Base2.metadata, \'\'\'\n    SELECT p.begadr, p.endadr, p.passage, r.*\n    FROM readings r\n    JOIN passages p USING (pass_id)\n    \'\'\')\n\nview (\'cliques_view\', Base2.metadata, \'\'\'\n    SELECT r.begadr, r.endadr, r.passage, r.lesart, q.*\n    FROM cliques q\n    JOIN readings_view r USING (pass_id, labez)\n    \'\'\')\n\nview (\'ms_cliques_view\', Base2.metadata, \'\'\'\n    SELECT q.begadr, q.endadr, q.passage, q.lesart, ms.hs, ms.hsnr, mq.*\n    FROM ms_cliques mq\n    JOIN cliques_view q USING (pass_id, labez, clique)\n    JOIN manuscripts ms USING (ms_id)\n    \'\'\')\n\nview (\'notes_view\', Base2.metadata, \'\'\'\n    SELECT p.begadr, p.endadr, p.passage, p.fehlvers, n.*\n    FROM notes n\n    JOIN passages p USING (pass_id)\n    \'\'\')\n\nview (\'apparatus_view\', Base2.metadata, \'\'\'\n    SELECT p.pass_id, p.begadr, p.endadr, p.passage, p.spanning, p.spanned, p.fehlvers,\n           ms.ms_id, ms.hs, ms.hsnr,\n           a.labez, a.cbgm, a.labezsuf, a.certainty, a.origin,\n           COALESCE (a.lesart, r.lesart) AS lesart\n    FROM apparatus a\n    JOIN readings r     USING (pass_id, labez)\n    JOIN passages p     USING (pass_id)\n    JOIN manuscripts ms USING (ms_id)\n    \'\'\')\n\nview (\'apparatus_cliques_view\', Base2.metadata, \'\'\'\n    SELECT a.*, q.clique, labez_clique (q.labez, q.clique) as labez_clique FROM apparatus_view a\n    LEFT JOIN ms_cliques q USING (ms_id, pass_id, labez)\n    \'\'\')\n\nview (\'apparatus_view_agg\', Base2.metadata, \'\'\'\n   SELECT pass_id, ms_id, hs, hsnr,\n          MODE () WITHIN GROUP (ORDER BY lesart) AS lesart,\n          labez_agg (labez    ORDER BY labez)    AS labez,\n          labez_agg (clique   ORDER BY clique)   AS clique,\n          labez_agg (labezsuf ORDER BY labezsuf) AS labezsuf,\n          labez_agg (labez_clique                ORDER BY labez_clique)            AS labez_clique,\n          labez_agg (labez || labezsuf           ORDER BY labez, labezsuf)         AS labez_labezsuf,\n          labez_agg (labez || labezsuf || clique ORDER BY labez, labezsuf, clique) AS labez_labezsuf_clique,\n          MAX (certainty) AS certainty\n   FROM apparatus_cliques_view\n   GROUP BY pass_id, ms_id, hs, hsnr\n   \'\'\')\n\nview (\'affinity_view\', Base2.metadata, \'\'\'\n    SELECT ch.bk_id, ch.rg_id, ch.range, ms_id1, ms_id2, common, equal,\n           older, newer, unclear,\n           affinity,\n           ch1.length AS ms1_length,\n           ch2.length AS ms2_length\n    FROM affinity aff\n    JOIN ranges_view ch USING (rg_id)\n    JOIN ms_ranges ch1 ON (aff.ms_id1, aff.rg_id) = (ch1.ms_id, ch1.rg_id)\n    JOIN ms_ranges ch2 ON (aff.ms_id2, aff.rg_id) = (ch2.ms_id, ch2.rg_id)\n    \'\'\')\n\nview (\'affinity_p_view\', Base2.metadata, \'\'\'\n    SELECT ch.bk_id, ch.rg_id, ch.range, ms_id1, ms_id2, common, equal,\n           p_older as older, p_newer as newer, p_unclear as unclear,\n           affinity,\n           ch1.length AS ms1_length,\n           ch2.length AS ms2_length\n    FROM affinity aff\n    JOIN ranges_view ch USING (rg_id)\n    JOIN ms_ranges ch1 ON (aff.ms_id1, aff.rg_id) = (ch1.ms_id, ch1.rg_id)\n    JOIN ms_ranges ch2 ON (aff.ms_id2, aff.rg_id) = (ch2.ms_id, ch2.rg_id)\n    \'\'\')\n\nview (\'export_cliques\', Base2.metadata, \'\'\'\n    SELECT passage, labez, clique, sys_period, user_id_start, user_id_stop\n    FROM cliques_view\n    WHERE user_id_start != 0\n    UNION\n    SELECT p.passage, labez, clique, sys_period, user_id_start, user_id_stop\n    FROM cliques_tts cq\n    JOIN passages p USING (pass_id)\n    ORDER BY passage, sys_period, labez, clique\n    \'\'\')\n\nview (\'export_ms_cliques\', Base2.metadata, \'\'\'\n    SELECT passage, hsnr, labez, clique, sys_period, user_id_start, user_id_stop\n    FROM ms_cliques_view\n    WHERE user_id_start != 0\n    UNION\n    SELECT p.passage, m.hsnr, labez, clique, sys_period, user_id_start, user_id_stop\n    FROM ms_cliques_tts mcq\n    JOIN passages p USING (pass_id)\n    JOIN manuscripts m USING (ms_id)\n    ORDER BY passage, sys_period, hsnr, labez, clique\n    \'\'\')\n\nview (\'export_locstem\', Base2.metadata, \'\'\'\n    SELECT passage, labez, clique, source_labez, source_clique,\n           sys_period, user_id_start, user_id_stop\n    FROM locstem_view\n    UNION\n    SELECT p.passage, labez, clique, source_labez, source_clique,\n           sys_period, user_id_start, user_id_stop\n    FROM locstem_tts lt\n    JOIN passages p USING (pass_id)\n    ORDER BY passage, sys_period, labez, clique\n    \'\'\')\n\nview (\'export_notes\', Base2.metadata, \'\'\'\n    SELECT passage, note, sys_period, user_id_start, user_id_stop\n    FROM notes_view\n    UNION\n    SELECT p.passage, note, sys_period, user_id_start, user_id_stop\n    FROM notes_tts\n    JOIN passages p USING (pass_id)\n    ORDER BY passage, sys_period\n    \'\'\')\n\ngeneric (Base2.metadata, \'\'\'\nCREATE UNIQUE INDEX IF NOT EXISTS readings_unique_pass_id_lesart ON readings (pass_id, lesart) WHERE labez !~ \'^z\'\n\'\'\', \'\'\'\nDROP INDEX IF EXISTS readings_unique_pass_id_lesart\n\'\'\'\n)\n\n\nLOCSTEM_REC = \'\'\'\nWITH RECURSIVE locstem_rec (pass_id, labez, clique, source_labez, source_clique) AS (\n  SELECT pass_id, labez, clique, source_labez, source_clique\n  FROM locstem i\n  WHERE i.pass_id = passage_id AND i.labez = labez1 AND i.clique = clique1\n  UNION\n  SELECT l.pass_id, l.labez, l.clique, l.source_labez, l.source_clique\n  FROM locstem l, locstem_rec r\n  WHERE l.pass_id = r.pass_id AND l.labez = r.source_labez AND l.clique = r.source_clique\n  )\n\'\'\'\n\nfunction (\'is_older\', Base2.metadata, \'passage_id INTEGER, labez2 CHAR, clique2 CHAR, labez1 CHAR, clique1 CHAR\', \'BOOLEAN\', LOCSTEM_REC + \'\'\'\nSELECT EXISTS (SELECT * FROM locstem_rec WHERE source_labez = labez2 AND source_clique = clique2);\n\'\'\', volatility = \'STABLE\')\n\nfunction (\'is_unclear\', Base2.metadata, \'passage_id INTEGER, labez1 CHAR, clique1 CHAR\', \'BOOLEAN\', LOCSTEM_REC + \'\'\'\nSELECT EXISTS (SELECT * FROM locstem_rec WHERE source_labez = \'?\');\n\'\'\', volatility = \'STABLE\')\n\nfunction (\'is_p_older\', Base2.metadata, \'passage_id INTEGER, labez2 CHAR, clique2 CHAR, labez1 CHAR, clique1 CHAR\', \'BOOLEAN\', \'\'\'\nSELECT EXISTS (SELECT * FROM locstem\n               WHERE pass_id = passage_id AND\n                     labez = labez1 AND clique = clique1 AND\n                     source_labez = labez2 AND source_clique = clique2);\n\'\'\', volatility = \'STABLE\')\n\nfunction (\'is_p_unclear\', Base2.metadata, \'passage_id INTEGER, labez1 CHAR, clique1 CHAR\', \'BOOLEAN\', \'\'\'\nSELECT EXISTS (SELECT * FROM locstem\n               WHERE pass_id = passage_id AND\n                     labez = labez1 AND clique = clique1 AND\n                     source_labez = \'?\');\n\'\'\', volatility = \'STABLE\')\n\nfunction (\'user_id\', Base2.metadata, \'\', \'INTEGER\', \'\'\'\nSELECT current_setting (\'ntg.user_id\')::int;\n\'\'\', volatility = \'STABLE\')\n\n# implement automagic TTS on cliques, locstem, ms_cliques, and notes\n\nfunction (\'cliques_trigger_f\', Base2.metadata, \'\', \'TRIGGER\', \'\'\'\n   BEGIN\n      IF TG_OP IN (\'UPDATE\', \'DELETE\') THEN\n        -- transfer data to tts table\n        INSERT INTO cliques_tts (pass_id, labez, clique,\n                                 sys_period, user_id_start, user_id_stop)\n        VALUES (OLD.pass_id, OLD.labez, OLD.clique,\n                close_period (OLD.sys_period), OLD.user_id_start, user_id ());\n      END IF;\n      IF TG_OP IN (\'UPDATE\', \'INSERT\') THEN\n        NEW.sys_period = tstzrange (now (), NULL);\n        NEW.user_id_start = user_id ();\n        RETURN NEW;\n      END IF;\n      RETURN OLD;\n   END;\n\'\'\', language = \'plpgsql\', volatility = \'VOLATILE\')\n\nfunction (\'locstem_trigger_f\', Base2.metadata, \'\', \'TRIGGER\', \'\'\'\n   BEGIN\n      IF TG_OP IN (\'UPDATE\', \'DELETE\') THEN\n        -- transfer data to tts table\n        INSERT INTO locstem_tts (pass_id, labez, clique, source_labez, source_clique,\n                                 sys_period, user_id_start, user_id_stop)\n        VALUES (OLD.pass_id, OLD.labez, OLD.clique, OLD.source_labez, OLD.source_clique,\n                close_period (OLD.sys_period), OLD.user_id_start, user_id ());\n      END IF;\n      IF TG_OP IN (\'UPDATE\', \'INSERT\') THEN\n        NEW.sys_period = tstzrange (now (), NULL);\n        NEW.user_id_start = user_id ();\n        RETURN NEW;\n      END IF;\n      RETURN OLD;\n   END;\n\'\'\', language = \'plpgsql\', volatility = \'VOLATILE\')\n\nfunction (\'ms_cliques_trigger_f\', Base2.metadata, \'\', \'TRIGGER\', \'\'\'\n   BEGIN\n      IF TG_OP IN (\'UPDATE\', \'DELETE\') THEN\n        -- transfer data to tts table\n        INSERT INTO ms_cliques_tts (pass_id, ms_id, labez, clique,\n                                    sys_period, user_id_start, user_id_stop)\n        VALUES (OLD.pass_id, OLD.ms_id, OLD.labez, OLD.clique,\n                close_period (OLD.sys_period), OLD.user_id_start, user_id ());\n      END IF;\n      IF TG_OP IN (\'UPDATE\', \'INSERT\') THEN\n        NEW.sys_period = tstzrange (now (), NULL);\n        NEW.user_id_start = user_id ();\n        RETURN NEW;\n      END IF;\n      RETURN OLD;\n   END;\n\'\'\', language = \'plpgsql\', volatility = \'VOLATILE\')\n\nfunction (\'notes_trigger_f\', Base2.metadata, \'\', \'TRIGGER\', \'\'\'\n   BEGIN\n      IF TG_OP IN (\'UPDATE\', \'DELETE\') THEN\n        -- transfer data to tts table\n        INSERT INTO notes_tts (pass_id, note,\n                               sys_period, user_id_start, user_id_stop)\n        VALUES (OLD.pass_id, OLD.note,\n                close_period (OLD.sys_period), OLD.user_id_start, user_id ());\n      END IF;\n      IF TG_OP IN (\'UPDATE\', \'INSERT\') THEN\n        NEW.sys_period = tstzrange (now (), NULL);\n        NEW.user_id_start = user_id ();\n        RETURN NEW;\n      END IF;\n      RETURN OLD;\n   END;\n\'\'\', language = \'plpgsql\', volatility = \'VOLATILE\')\n\ngeneric (Base2.metadata, \'\'\'\n    CREATE TRIGGER cliques_trigger\n    BEFORE INSERT OR UPDATE OR DELETE ON cliques\n    FOR EACH ROW EXECUTE PROCEDURE cliques_trigger_f ()\n\'\'\', \'\'\'\n    DROP TRIGGER IF EXISTS cliques_trigger ON locstem\n\'\'\'\n)\n\ngeneric (Base2.metadata, \'\'\'\n    CREATE TRIGGER locstem_trigger\n    BEFORE INSERT OR UPDATE OR DELETE ON locstem\n    FOR EACH ROW EXECUTE PROCEDURE locstem_trigger_f ()\n\'\'\', \'\'\'\n    DROP TRIGGER IF EXISTS locstem_trigger ON locstem\n\'\'\'\n)\n\ngeneric (Base2.metadata, \'\'\'\n    CREATE TRIGGER ms_cliques_trigger\n    BEFORE INSERT OR UPDATE OR DELETE ON ms_cliques\n    FOR EACH ROW EXECUTE PROCEDURE ms_cliques_trigger_f ()\n\'\'\', \'\'\'\n    DROP TRIGGER IF EXISTS ms_cliques_trigger ON ms_cliques\n\'\'\'\n)\n\ngeneric (Base2.metadata, \'\'\'\n    CREATE TRIGGER notes_trigger\n    BEFORE INSERT OR UPDATE OR DELETE ON notes\n    FOR EACH ROW EXECUTE PROCEDURE notes_trigger_f ()\n\'\'\', \'\'\'\n    DROP TRIGGER IF EXISTS notes_trigger ON notes\n\'\'\'\n)\n\n# implement inserts on view apparatus_cliques_view\n\nfunction (\'apparatus_cliques_view_trigger_f\', Base2.metadata, \'\', \'TRIGGER\', \'\'\'\n   BEGIN\n      IF TG_OP = \'INSERT\' THEN\n        INSERT INTO apparatus  (pass_id, ms_id, labez, lesart, cbgm, origin)\n               VALUES (NEW.pass_id, NEW.ms_id, NEW.labez, NEW.lesart, NEW.cbgm, NEW.origin);\n        INSERT INTO ms_cliques (pass_id, ms_id, labez, clique)\n               VALUES (NEW.pass_id, NEW.ms_id, NEW.labez, NEW.clique);\n      ELSIF TG_OP = \'UPDATE\' THEN\n        SET CONSTRAINTS ALL DEFERRED;\n        UPDATE apparatus\n        SET pass_id = NEW.pass_id, ms_id = NEW.ms_id, labez = NEW.labez, lesart = NEW.lesart, cbgm = NEW.cbgm, origin = NEW.origin\n        WHERE (pass_id, ms_id) = (OLD.pass_id, OLD.ms_id);\n        UPDATE ms_cliques\n        SET pass_id = NEW.pass_id, ms_id = NEW.ms_id, labez = NEW.labez, clique = NEW.clique\n        WHERE (pass_id, ms_id) = (OLD.pass_id, OLD.ms_id);\n      ELSIF TG_OP = \'DELETE\' THEN\n        DELETE FROM ms_cliques\n        WHERE (pass_id, ms_id) = (OLD.pass_id, OLD.ms_id);\n        DELETE FROM apparatus\n        WHERE (pass_id, ms_id) = (OLD.pass_id, OLD.ms_id);\n      END IF;\n      RETURN NEW;\n    END;\n\'\'\', language = \'plpgsql\', volatility = \'VOLATILE\')\n\ngeneric (Base2.metadata, \'\'\'\n    CREATE TRIGGER apparatus_cliques_view_trigger\n    INSTEAD OF INSERT OR UPDATE OR DELETE ON apparatus_cliques_view\n    FOR EACH ROW EXECUTE PROCEDURE apparatus_cliques_view_trigger_f ();\n\'\'\', \'\'\'\n    DROP TRIGGER IF EXISTS apparatus_cliques_view_trigger ON apparatus_cliques_view\n\'\'\'\n)\n\n\nBase4 = declarative_base ()\nBase4.metadata.schema = \'ntg\'\n\nclass Nestle (Base4):\n    """"""The Leitzeile from the Nestle-Aland\n\n    Dient der Darstellung der Leitzeile im Navigator.\n\n    .. sauml::\n       :include: nestle\n\n    .. _nestle_table:\n\n    .. attribute:: begadr, endadr\n\n       Zusammengesetzt aus Buch, Kapitel, Vers, Wort.  Es werden W\xc3\xb6rter und\n       Zwischenr\xc3\xa4ume gez\xc3\xa4hlt.  Gerade Zahlen bezeichnen ein Wort, ungerade einen\n       Zwischenraum.  In dieser Tabelle sind nur W\xc3\xb6rter enthalten, keine\n       Zwischenr\xc3\xa4ume.  Jedes Wort hat einen eigenen Eintrag, d.h. f\xc3\xbcr alle\n       Eintr\xc3\xa4ge gilt: begadr = endadr.\n\n    .. attribute:: lemma\n\n       Das Lemma-Wort in der Leitzeile.\n\n    """"""\n\n    __tablename__ = \'nestle\'\n\n    id        = Column (Integer,       primary_key = True, autoincrement = True)\n\n    begadr    = Column (Integer,       nullable = False)\n    endadr    = Column (Integer,       nullable = False)\n    passage   = Column (IntRangeType,  nullable = False)\n\n    lemma     = Column (String(1024),  server_default = \'\')\n\n    __table_args__ = (\n        UniqueConstraint (passage, name = \'unique_nestle_passage\'), # needs name\n        Index (\'ix_nestle_passage_gist\', passage, postgresql_using = \'gist\'),\n    )\n\n\n# Tables for flask_login / flask_user / flask_security / whatever\n\nBase3 = declarative_base ()\nBase3.metadata.schema = \'ntg\'\n\nclass _User ():\n    __tablename__ = \'user\'\n\n    id           = Column (Integer,      primary_key = True)\n    username     = Column (String (50),  nullable = False, unique = True)\n    password     = Column (String (255), nullable = False, server_default = \'\')\n    email        = Column (String (255), nullable = False, unique = True)\n    active       = Column (Boolean,      nullable = False, server_default = \'0\')\n    confirmed_at = Column (DateTime)\n    first_name   = Column (String (100), nullable = False, server_default = \'\')\n    last_name    = Column (String (100), nullable = False, server_default = \'\')\n\n\nclass _Role ():\n    __tablename__ = \'role\'\n\n    id          = Column (Integer,      primary_key = True)\n    name        = Column (String  (80), unique = True)\n    description = Column (String (255), nullable = False, server_default = \'\')\n\n\nclass _Roles_Users ():\n    __tablename__ = \'roles_users\'\n\n    id      = Column (Integer, primary_key = True)\n\n    @declared_attr\n    def user_id (cls):\n        return Column (Integer, ForeignKey (\'user.id\', ondelete=\'CASCADE\'))\n\n    @declared_attr\n    def role_id (cls):\n        return Column (Integer, ForeignKey (\'role.id\', ondelete=\'CASCADE\'))\n\n\nclass User (_User, Base3):\n    pass\n\nclass Role (_Role, Base3):\n    pass\n\nclass Roles_Users (_Roles_Users, Base3):\n    pass\n'"
ntg_common/db_tools.py,0,"b'# -*- encoding: utf-8 -*-\n\n"""""" This module contains functions for database access. """"""\n\nimport collections\nimport csv\nimport configparser\nimport datetime\nimport io\nimport itertools\nimport logging\nimport os\nimport os.path\nimport textwrap\nimport time\n\nimport networkx as nx\nimport sqlalchemy\nfrom sqlalchemy.sql import text\n\nfrom .tools import log\nfrom .config import args\n\n\n# mimics mysql Ver 15.1 Distrib 10.1.29-MariaDB\nMYSQL_DEFAULT_FILES  = ( \'/etc/my.cnf\', \'/etc/mysql/my.cnf\', \'~/.my.cnf\' )\nMYSQL_DEFAULT_GROUPS = ( \'mysql\', \'client\', \'client-server\', \'client-mariadb\' )\n\n\ndef execute (conn, sql, parameters, debug_level = logging.DEBUG):\n    sql = sql.strip ().format (**parameters)\n    start_time = datetime.datetime.now ()\n    result = conn.execute (text (sql), parameters)\n    log (debug_level, \'%d rows in %.3fs\', result.rowcount, (datetime.datetime.now () - start_time).total_seconds ())\n    return result\n\n\ndef executemany (conn, sql, parameters, param_array, debug_level = logging.DEBUG):\n    sql = sql.strip ().format (**parameters)\n    start_time = datetime.datetime.now ()\n    result = conn.execute (text (sql), param_array)\n    log (debug_level, \'%d rows in %.3fs\', result.rowcount, (datetime.datetime.now () - start_time).total_seconds ())\n    return result\n\n\ndef executemany_raw (conn, sql, parameters, param_array, debug_level = logging.DEBUG):\n    sql = sql.strip ().format (**parameters)\n    start_time = datetime.datetime.now ()\n    result = conn.execute (sql, param_array)\n    log (debug_level, \'%d rows in %.3fs\', result.rowcount, (datetime.datetime.now () - start_time).total_seconds ())\n    return result\n\n\ndef rollback (conn, debug_level = logging.DEBUG):\n    start_time = datetime.datetime.now ()\n    result = conn.execute (\'ROLLBACK\')\n    log (debug_level, \'rollback in %.3fs\', (datetime.datetime.now () - start_time).total_seconds ())\n    return result\n\n\n# def execute_pandas (conn, sql, parameters, debug_level = logging.DEBUG):\n#     sql = sql.format (**parameters)\n#     log (debug_level, sql.rstrip () + \';\')\n#     return pd.read_sql_query (text (sql), conn, parameters)\n\ndef _debug (conn, msg, sql, parameters, level):\n    # print values\n    if args.log_level <= level:\n        result = execute (conn, sql, parameters)\n        if result.rowcount > 0:\n            log (level, msg + \'\\n\' + tabulate (result))\n\ndef debug (conn, msg, sql, parameters):\n    _debug (conn, msg, sql, parameters, logging.DEBUG)\n\ndef info (conn, msg, sql, parameters):\n    _debug (conn, msg, sql, parameters, logging.INFO)\n\ndef warn (conn, msg, sql, parameters):\n    _debug (conn, msg, sql, parameters, logging.WARNING)\n\n\ndef fix (conn, msg, check_sql, fix_sql, parameters):\n    """"""Check and eventually fix errors.\n\n    Executes the check_sql statement to check for a possible error conditions\n    and, if rows emerge, prints a warning and executes the fix_sql statement.\n    The fix_sql statement should be written as to fix the errors reported by the\n    check_sql statement.  Finally it executes the check_sql statement again and\n    reports an error if the error condition still exists.\n\n    :param str msg:       The warning / error message\n    :param str check_sql: The sql statement that checks for the error condition.\n    :param str fix_sql:   The sql statement that fixes the error condition.\n\n    """"""\n\n    # print unfixed values\n    result = execute (conn, check_sql, parameters)\n    if result.rowcount > 0:\n        # apply fix\n        if args.log_level <= logging.WARNING:\n            log (logging.WARNING, msg + \'\\n\' + tabulate (result))\n        if fix_sql:\n            execute (conn, fix_sql, parameters)\n            # print fixed values\n            result = execute (conn, check_sql, parameters)\n            if result.rowcount > 0:\n                log (logging.ERROR, msg + \'\\n\' + tabulate (result))\n\n\ndef to_csv (fields, rows):\n    fp = io.StringIO ()\n    writer = csv.DictWriter (fp, fields, restval=\'\', extrasaction=\'raise\', dialect=\'excel\')\n    writer.writeheader ()\n    for r in rows:\n        writer.writerow (r._asdict ())\n    return fp.getvalue ()\n\n\ndef tabulate (res):\n    """""" Format and output a rowset\n\n    Uses an output format similar to the one produced by the mysql commandline\n    utility.\n\n    """"""\n    cols = range (0, len (res.keys ()))\n    rowlen = dict()\n    a = []\n\n    def line ():\n        for i in cols:\n            a.append (\'+\')\n            a.append (\'-\' * (rowlen[i] + 2))\n        a.append (\'+\\n\')\n\n    # convert database types to strings\n    rows = []\n    for row in res.fetchall():\n        newrow = []\n        for i in cols:\n            if row[i] is None:\n                newrow.append (\'NULL\')\n            else:\n                newrow.append (str (row[i]))\n        rows.append (newrow)\n\n    # calculate column widths\n    for i in cols:\n        rowlen[i] = len (res.keys ()[i])\n\n    for row in rows:\n        for i in cols:\n            rowlen[i] = max (rowlen[i], len (row[i]))\n\n    for i in cols:\n        rowlen[i] = min (80, rowlen[i])\n\n    # output header\n    line ()\n    for i in cols:\n        a.append (\'| {:<{align}} \'.format (res.keys ()[i], align = rowlen[i]))\n    a.append (\'|\\n\')\n    line ()\n\n    # output rows\n    for row in rows:\n        offset = 0\n        for i in cols:\n            if row[i] != \'\':\n                lines = itertools.chain (* [textwrap.wrap (line, width = 78) for line in row[i].splitlines ()])\n                prefix = \'|\\n\' + \' \' * offset\n                for n, l in enumerate (lines):\n                    a.append (\'{:}| {:<{align}} \'.format (prefix if n > 0 else \'\', l, align = rowlen[i]))\n            else:\n                a.append (\'| {:<{align}} \'.format (\'\', align = rowlen[i]))\n            offset += rowlen[i] + 3\n        a.append (\'|\\n\')\n    line ()\n    a.append (\'%d rows\\n\' % len (rows))\n\n    return \'\'.join (a)\n\n\ndef truncate_editor_tables (conn):\n    execute (conn, """"""\n    TRUNCATE cliques_tts, ms_cliques_tts, locstem_tts, notes_tts RESTART IDENTITY;\n    TRUNCATE cliques, ms_cliques, locstem, notes RESTART IDENTITY;\n    """""", {})\n\n\ndef init_default_cliques (conn):\n    """"""Generate a default cliques table.\n\n    In a default cliques table there is a default clique \'1\' for every reading\n    in the readings table.\n\n    """"""\n\n    execute (conn, """"""\n    ALTER TABLE cliques DISABLE TRIGGER cliques_trigger;\n    INSERT INTO cliques (pass_id, labez, clique, user_id_start)\n    SELECT pass_id, labez, \'1\', 0\n    FROM readings r;\n    ALTER TABLE cliques ENABLE TRIGGER cliques_trigger;\n    """""", {})\n\n\ndef init_default_ms_cliques (conn):\n    """"""Generate a default ms_cliques table.\n\n    In a default ms_cliques table there is a default clique \'1\' for every reading\n    in the apparatus table.\n\n    """"""\n\n    execute (conn, """"""\n    ALTER TABLE ms_cliques DISABLE TRIGGER ms_cliques_trigger;\n    INSERT INTO ms_cliques (ms_id, pass_id, labez, clique, user_id_start)\n    SELECT a.ms_id, a.pass_id, a.labez, \'1\', 0\n    FROM apparatus a;\n    ALTER TABLE ms_cliques ENABLE TRIGGER ms_cliques_trigger;\n    """""", {})\n\n\ndef init_default_locstem (conn):\n    """"""Generate a default locstem table.\n\n    In a default LocStemEd, labez \'a\' is the original reading and every other\n    reading depends on labez \'a\', except in a Fehlvers, where \'b\' is of unknown\n    origin and every other reading depends on \'b\'.\n\n    """"""\n\n    execute (conn, """"""\n    ALTER TABLE locstem DISABLE TRIGGER locstem_trigger;\n    """""", {})\n\n    # insert \'a\' as original reading (or \'b\' as unknown if Fehlvers)\n    execute (conn, """"""\n    INSERT INTO locstem (pass_id, labez, clique, source_labez, source_clique, user_id_start)\n    SELECT pass_id, \'a\', \'1\', \'*\', \'1\', 0\n    FROM passages p\n    WHERE NOT fehlvers;\n\n    INSERT INTO locstem (pass_id, labez, clique, source_labez, source_clique, user_id_start)\n    SELECT pass_id, \'b\', \'1\', \'?\', \'1\', 0\n    FROM passages p\n    WHERE fehlvers;\n    """""", {})\n\n    # make other readings dependent on \'a\' (or \'b\' in Fehlvers)\n    execute (conn, """"""\n    INSERT INTO locstem (pass_id, labez, clique, source_labez, source_clique, user_id_start)\n    SELECT c.pass_id, c.labez, c.clique, \'a\', \'1\', 0\n    FROM cliques_view c\n      JOIN passages p USING (pass_id)\n    WHERE NOT p.fehlvers AND c.labez != \'a\' AND c.labez !~ \'^z[u-z]\';\n\n    INSERT INTO locstem (pass_id, labez, clique, source_labez, source_clique, user_id_start)\n    SELECT c.pass_id, c.labez, c.clique, \'b\', \'1\', 0\n    FROM cliques_view c\n      JOIN passages p USING (pass_id)\n    WHERE p.fehlvers AND c.labez != \'b\' AND c.labez !~ \'^z[u-z]\'\n    """""", {})\n\n    execute (conn, """"""\n    ALTER TABLE locstem ENABLE TRIGGER locstem_trigger;\n    """""", {})\n\n\nclass MySQLEngine ():\n    """""" Database Interface """"""\n\n    def __init__ (self, fn = MYSQL_DEFAULT_FILES, group = MYSQL_DEFAULT_GROUPS, db = \'\'):\n        # self.params is only needed to configure the MySQL FDW in Postgres\n        self.params = self.get_connection_params (fn, group)\n        self.params[\'database\'] = db\n\n        url = sqlalchemy.engine.url.URL (**(dict (self.params, password = \'password\')))\n        log (logging.DEBUG, \'MySQLEngine: Connecting to URL: {url}\'.format (url = url))\n        url = sqlalchemy.engine.url.URL (**self.params)\n        self.engine = sqlalchemy.create_engine (url)\n\n        self.connection = self.connect ()\n\n\n    @staticmethod\n    def get_connection_params (filenames, groups):\n        if isinstance (filenames, str):\n            filenames = [ filenames ]\n        if isinstance (groups, str):\n            groups = [ groups ]\n\n        filenames = map (os.path.expanduser, filenames)\n        config = configparser.ConfigParser ()\n        config.read (filenames)\n        parameters = {\n            \'drivername\' : \'drivername\',\n            \'host\' : \'host\',\n            \'port\' : \'port\',\n            \'database\' : \'database\',\n            \'user\' : \'username\',\n            \'password\' : \'password\',\n        }\n        options = {\n            \'default-character-set\' : \'charset\',\n        }\n        params = {\n            \'drivername\' : \'mysql\',\n            \'host\' : \'localhost\',\n            \'port\' : \'3306\',\n            \'query\' : {\n                \'charset\' : \'utf8mb4\',\n            }\n        }\n\n        for group in groups:\n            section = config[group]\n            for key, alias in parameters.items ():\n                if key in section:\n                    params[alias] = section[key].strip (\'""\')\n            for key, alias in options.items ():\n                if key in section:\n                    params[\'query\'][alias] = section[key].strip (\'""\')\n        return params\n\n\n    def connect (self):\n        connection = self.engine.connect ()\n        # Make MySQL more compatible with other SQL databases\n        connection.execute (""SET sql_mode=\'ANSI\'"")\n        return connection\n\n\nclass PostgreSQLEngine ():\n    """""" PostgreSQL Database Interface """"""\n\n    @staticmethod\n    def receive_checkout (dbapi_connection, _connection_record, _connection_proxy):\n        """"""Set a default for the postgres variable ntg.user_id that is used by\n        :ref:`tts` tables.\n\n        """"""\n\n        # just give a default value so postgres won\'t choke.  the actual user is\n        # set in editor.py\n        dbapi_connection.cursor ().execute (""SET ntg.user_id = 0"")\n\n\n    def __init__ (self, **kwargs):\n\n        args = self.get_connection_params (kwargs)\n\n        # if host is None use the socket in /var/lib/postgresql\n        args[\'host_port\'] = \'\'\n        if args[\'host\'] is not None:\n            args[\'host_port\'] = \'{host}:{port}\'.format (**args)\n\n        self.url = \'postgresql+psycopg2://{user}@{host_port}/{database}?sslmode=disable&server_side_cursors\'.format (**args)\n\n        log (logging.DEBUG, ""PostgreSQLEngine: Connecting to URL: {url}"".format (url = self.url))\n\n        self.engine = sqlalchemy.create_engine (\n            self.url,\n            use_batch_mode = True\n        )\n\n        self.params = args\n\n        sqlalchemy.event.listen (self.engine, \'checkout\', self.receive_checkout)\n\n        self.wait_for_server ()\n\n\n    def connect (self):\n        return self.engine.connect ()\n\n\n    def get_connection_params (self, args = {}):\n        """"""Get sqlalchemy connection parameters.\n\n        Try to get the connection parameters in turn from these sources:\n\n        1. get host, port, database, user from args\n        2. get PGHOST, PGPORT, PGDATABASE, PGUSER from args\n        3. get PGHOST, PGPORT, PGDATABASE, PGUSER from environment\n        4. use defaults\n\n        N.B. The postgres client library automatically reads the password from\n        the file :file:`~/.pgpass`.  It should be configured there.\n\n        """"""\n\n        defaults = {\n            \'host\'     : \'localhost\',\n            \'port\'     : \'5432\',\n            \'database\' : \'ntg\',\n            \'user\'     : \'ntg\',\n        }\n        res = {}\n\n        for p in defaults:\n            pu = \'PG\' + p.upper ()\n            res[p] = args.get (p) or args.get (pu) or os.environ.get (pu) or defaults[p]\n\n        return res\n\n\n    def vacuum (self):\n        """"""Vacuum the database.""""""\n\n        # turn off auto-transaction because vacuum won\'t work in a transaction\n        connection = self.engine.raw_connection ()\n        connection.set_isolation_level (0)\n        connection.cursor ().execute (""VACUUM FULL ANALYZE"")\n        log (logging.INFO, \'\'.join (connection.notices))\n\n\n    def wait_for_server (self, retries = 60):\n        """""" Wait for the Postgres server to come up. """"""\n\n        while retries > 0:\n            try:\n                self.connect ()\n                return\n            except sqlalchemy.exc.OperationalError as e:\n                if retries <= 0:\n                    raise # give up\n                log (logging.INFO, ""Waiting for Postgres server to come up.  Retrying (%d) ..."" % retries)\n                retries -= 1\n                time.sleep (1.0)\n\n\ndef local_stemma_to_nx (conn, pass_id, add_isolated_roots = False):\n    """"""Load a passage from the database into an nx Graph.\n\n    :param bool add_isolated_roots: Add an \'*\' or \'?\' node even if they are\n                                    isolated.  Needed in edit mode.\n\n    """"""\n\n    res = execute (conn, """"""\n    SELECT labez,\n           clique,\n           labez_clique (labez, clique) AS labez_clique,\n           source_labez,\n           source_clique,\n           labez_clique (source_labez, source_clique) AS source_labez_clique\n    FROM locstem l\n    WHERE labez !~ \'^z[u-z]\' AND pass_id = :pass_id\n    ORDER BY labez, clique\n    """""", dict (pass_id = pass_id))\n\n    Variant = collections.namedtuple (\'stemma_json_variant\',\n                                      \'labez clique labez_clique source_labez source_clique source_labez_clique\')\n\n    rows = list (map (Variant._make, res))\n\n    graph = nx.DiGraph ()\n\n    more_params = dict ()\n    if add_isolated_roots:\n        more_params[\'draggable\'] = \'1\'\n        more_params[\'droptarget\'] = \'1\'\n\n    for row in rows:\n        graph.add_node (row.labez_clique, label = row.labez_clique,\n                    labez = row.labez, clique = row.clique, labez_clique = row.labez_clique,\n                    **more_params)\n        graph.add_edge (row.source_labez_clique, row.labez_clique)\n\n    more_params = dict ()\n    if add_isolated_roots:\n        # Add \'*\' and \'?\' nodes\n        graph.add_node (\'*\')\n        graph.add_node (\'?\')\n        more_params[\'droptarget\'] = \'1\'\n\n    if \'*\' in graph:\n        graph.nodes[\'*\'].update (label = \'*\', labez=\'*\', clique=\'1\', labez_clique=\'*\', **more_params)\n    if \'?\' in graph:\n        graph.nodes[\'?\'].update (label = \'?\', labez=\'?\', clique=\'1\', labez_clique=\'?\', **more_params)\n\n    return graph\n'"
ntg_common/exceptions.py,0,"b'class EditException (Exception):\n    """""" See: http://flask.pocoo.org/docs/0.12/patterns/apierrors/ """"""\n\n    default_status_code = 400\n\n    def __init__ (self, message, status_code=None, payload=None):\n        Exception.__init__ (self)\n        self.message     = \'Error: \' + message\n        self.status_code = status_code or self.default_status_code\n        self.payload     = payload\n\n    def to_dict (self):\n        rv = dict ()\n        rv[\'status\']  = self.status_code\n        rv[\'message\'] = self.message\n        if self.payload:\n            rv[\'payload\'] = self.payload\n        return rv\n\n\nclass EditError (EditException):\n    pass\n\n\nclass PrivilegeError (EditException):\n    pass\n'"
ntg_common/plot.py,0,"b'# -*- encoding: utf-8 -*-\n\n"""""" This module contains plot functions. """"""\n\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\n\n\ndef mss_labels (mss):\n    """"""Build ticks and labels for manuscript axis.\n\n    Use one tick at the start of each range (papyri, maiuscles, ...) and then\n    one every 5 mss.\n\n    Parameter must be a list of named tuples of class Mss.\n\n    """"""\n\n    range_ = -1\n    c = 0\n    ticks = []\n    for n, ms in enumerate (mss):\n        i = ms.hsnr // 100000\n        if i > range_:\n            ticks.append (n)\n            range_ = i\n            c = 0\n            continue\n        c += 1\n        if (c % 5) == 0:\n            ticks.append (n)\n\n    labels = [mss[i].hs for i in ticks]\n    return ticks, labels\n\n\ndef passages_labels (passages):\n    """"""Build ticks and labels for passages axis.\n\n    Use one tick every verse.\n\n    """"""\n\n    def group_by (s):\n        return s[:3]\n\n    def title (s):\n        return s[1:3]\n\n    group = None\n    ticks = []\n    labels = []\n\n    for i, passage in enumerate (passages):\n        g = group_by (passage)\n        if g != group:\n            ticks.append (i)\n            labels.append (title (passage))\n            group = g\n\n    return ticks, labels\n\n\ndef colormap_bw ():\n    # return colors.from_levels_and_colors ([0, 0.5, 1], [\'white\', \'black\'])\n    return (colors.LinearSegmentedColormap.from_list (\'BW\', [\'white\', \'black\']),\n            colors.Normalize (vmin = 0.0, vmax = 1.0))\n\n\ndef colormap_affinity ():\n    return (\'jet\', # colors.LinearSegmentedColormap.from_list (\'Affinity\', [\'white\', \'blue\']),\n            colors.Normalize (vmin = 0.0, vmax = 1.0))\n\n\ndef heat_matrix (m, caption, ticks_labels_x, ticks_labels_y, colormap):\n    """""" Plot a heat map of the matrix. """"""\n\n    plt.matshow (m, fignum = 0, aspect = \'auto\', cmap = colormap[0], norm = colormap[1])\n    plt.colorbar ()\n\n    plt.xticks (ticks_labels_x[0], ticks_labels_x[1], rotation=\'vertical\')\n    plt.yticks (ticks_labels_y[0], ticks_labels_y[1])\n    axes = plt.gca ()\n    axes.tick_params (direction = \'out\', pad = 5)\n\n    plt.title (caption, y = 20.0)\n'"
ntg_common/src_db.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""This module is for documentation purposes only.  It contains sqlalchemy\nclasses that show the source database structure.  We never actually use these\nclasses in the code.\n\n""""""\n\nfrom sqlalchemy import String, Integer, Column, text, PrimaryKeyConstraint\n\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base ()\n"""""" Input tables we only want to document, not use. """"""\n\n\nclass Acts01GVZ (Base):\n    """"""This table contains a negative apparatus.\n\n    .. sauml:: mysql:///ECM_ActsPh4?read_default_group=ntg\n       :include: Acts01GVZ\n\n    """"""\n\n    __tablename__ = \'Acts01GVZ\'\n\n    id          = Column (Integer, primary_key=True)\n    BUCH        = Column (Integer)\n    KAPANF      = Column (Integer)\n    VERSANF     = Column (Integer)\n    LEMMA       = Column (String (1024))\n    WORTANF     = Column (Integer)\n    WORTEND     = Column (Integer)\n    LABEZ       = Column (String (32))\n    LESART      = Column (String (1024))\n    HS          = Column (String (32))\n    SUFFIX1     = Column (String (32))\n    SUFFIX2     = Column (String (32))\n    KONTROLLE   = Column (String (1))\n    KAPEND      = Column (Integer, server_default=text(""\'0\'""))\n    VERSEND     = Column (Integer, server_default=text(""\'0\'""))\n    LABEZSUF    = Column (String (32))\n    fehler      = Column (Integer, server_default=text(""\'0\'""))\n    HSNR        = Column (Integer)\n    SUFF        = Column (String (32))\n    VID         = Column (String (32))\n    VL          = Column (String (32))\n    KORR        = Column (String (32))\n    LEKT        = Column (String (32))\n    KOMM        = Column (String (32))\n    BEGADR      = Column (Integer)\n    ENDADR      = Column (Integer)\n    ADR         = Column (Integer)\n    ANFALT      = Column (Integer)\n    ENDALT      = Column (Integer)\n    LABEZALT    = Column (String (32))\n    LASUFALT    = Column (String (32))\n    ZUSATZ      = Column (String (255))\n    LesartenKey = Column (Integer, nullable=False, server_default=text(""\'0\'""))\n    base        = Column (String (1), server_default=text(""\'\'""))\n    over        = Column (String (1), server_default=text(""\'\'""))\n    comp        = Column (String (1), server_default=text(""\'\'""))\n    over1       = Column (String (1), server_default=text(""\'\'""))\n    comp1       = Column (String (1), server_default=text(""\'\'""))\n\n\nclass Acts01GVZlac (Base):\n    """"""This table contains a list of lacunae.\n\n    .. sauml:: mysql:///ECM_ActsPh4?read_default_group=ntg\n       :include: Acts01GVZlac\n\n    """"""\n\n    __tablename__ = \'Acts01GVZlac\'\n\n    id          = Column (Integer,       primary_key=True)\n    BUCH        = Column (Integer,       nullable=False)\n    KAPANF      = Column (Integer,       nullable=False)\n    VERSANF     = Column (Integer,       nullable=False)\n    LEMMA       = Column (String (1024), nullable=False)\n    WORTANF     = Column (Integer,       nullable=False)\n    WORTEND     = Column (Integer)\n    LABEZ       = Column (String (255),  nullable=False)\n    LESART      = Column (String (1024), nullable=False)\n    HS          = Column (String (255))\n    SUFFIX1     = Column (String (255))\n    SUFFIX2     = Column (String (255))\n    KONTROLLE   = Column (String (1))\n    KAPEND      = Column (Integer, server_default=text(""\'0\'""))\n    VERSEND     = Column (Integer, server_default=text(""\'0\'""))\n    LABEZSUF    = Column (String (255))\n    fehler      = Column (Integer, server_default=text(""\'0\'""))\n    HSNR        = Column (Integer)\n    SUFF        = Column (String (255))\n    VID         = Column (String (255))\n    VL          = Column (String (255))\n    KORR        = Column (String (255))\n    LEKT        = Column (String (255))\n    KOMM        = Column (String (255))\n    BEGADR      = Column (Integer)\n    ENDADR      = Column (Integer)\n    ADR         = Column (Integer)\n    ANFALT      = Column (Integer)\n    ENDALT      = Column (Integer)\n    LABEZALT    = Column (String (255))\n    LASUFALT    = Column (String (255))\n    ZUSATZ      = Column (String (255))\n    printout    = Column (String (255))\n    category    = Column (String (1))\n    base        = Column (String (1), server_default=text(""\'\'""))\n    over        = Column (String (1), server_default=text(""\'\'""))\n    comp        = Column (String (1), server_default=text(""\'\'""))\n    over1       = Column (String (1), server_default=text(""\'\'""))\n    comp1       = Column (String (1), server_default=text(""\'\'""))\n\n\nclass LocStemEdAct01 (Base):\n    """"""A table that contains the priority of the cliques at each passage\n\n    This table contains the main output of the editors.  The editors decide\n    which reading is derived from which other reading(s) at each passage.\n\n    .. sauml:: mysql:///VarGenAtt_ActPh4?read_default_group=ntg\n       :include: LocStemEdAct01\n\n    .. attribute:: id\n\n       Primary key\n\n    .. attribute:: varid\n\n       Same as :ref:`labez <labez>`.\n\n    .. attribute:: varnew\n\n       This is the :ref:`labez <labez>` concatenated with the number of the :ref:`split <split>`.\n\n    .. attribute:: s1\n\n       Source of this reading.\n\n    .. attribute:: s2\n\n       Optionally second source of reading.\n\n    .. attribute:: begadr, endadr\n\n       The passage.\n\n    .. attribute:: w\n\n       Flag :ref:`""Western Text"" <wt>`.  Not needed for the CBGM.\n\n    """"""\n\n    __tablename__ = \'LocStemdEdAct\'\n\n    id     = Column (Integer,    primary_key=True)\n    VARID  = Column (String (2), nullable=False, server_default=text (""\'\'""))\n    VARNEW = Column (String (2), nullable=False)\n    S1     = Column (String (2), nullable=False, server_default=text (""\'\'""))\n    S2     = Column (String (2), nullable=False, server_default=text (""\'\'""))\n    PRS1   = Column (String (2), nullable=False, server_default=text (""\'\'""))\n    PRS2   = Column (String (2), nullable=False, server_default=text (""\'\'""))\n    BEGADR = Column (Integer,    nullable=False, server_default=text (""\'0\'""))\n    ENDADR = Column (Integer,    nullable=False, server_default=text (""\'0\'""))\n    CHECK  = Column (String (2), nullable=False, server_default=text (""\'\'""))\n    CHECK2 = Column (String (2))\n    w      = Column (String (1), server_default=text(""\'\'""))\n\n\nclass RdgAct01 (Base):\n    """"""This table contains readings.\n\n    .. sauml:: mysql:///VarGenAtt_ActPh4?read_default_group=ntg\n       :include: RdgAct01\n\n    """"""\n\n    __tablename__ = \'RdgAct\'\n\n    id        = Column (Integer,       primary_key=True)\n    BUCH      = Column (Integer,       nullable=False, server_default=text (""\'0\'""))\n    KAPANF    = Column (Integer,       nullable=False, server_default=text (""\'0\'""))\n    VERSANF   = Column (Integer,       nullable=False, server_default=text (""\'0\'""))\n    WORTANF   = Column (Integer,       nullable=False, server_default=text (""\'0\'""))\n    KAPEND    = Column (Integer,       nullable=False, server_default=text (""\'0\'""))\n    VERSEND   = Column (Integer,       nullable=False, server_default=text (""\'0\'""))\n    WORTEND   = Column (Integer,       nullable=False, server_default=text (""\'0\'""))\n    LABEZ     = Column (String (2),    nullable=False)\n    VARID     = Column (String (2),    server_default=text (""\'\'""))\n    LABEZSUF  = Column (String (30),   nullable=False)\n    FEHLER    = Column (String (10),   nullable=False)\n    LESART    = Column (String (1024), nullable=False)\n    KOMMENTAR = Column (String (50),   nullable=False)\n    SPRACHE   = Column (String (1),    nullable=False)\n    BEGADR    = Column (Integer,       nullable=False, server_default=text (""\'0\'""))\n    ENDADR    = Column (Integer,       nullable=False, server_default=text (""\'0\'""))\n    ADR       = Column (Integer,       nullable=False, server_default=text (""\'0\'""))\n    UEBER     = Column (String (1),    nullable=False)\n    UNTER     = Column (String (1),    nullable=False)\n    Z_ZAHL    = Column (Integer,       nullable=False, server_default=text (""\'0\'""))\n    BYZ       = Column (String (1),    nullable=False)\n    KONTROLLE = Column (String (1),    nullable=False)\n    zv        = Column (String (1),    server_default=text (""\'\'""))\n    al        = Column (String (1),    server_default=text (""\'\'""))\n\n\nclass VarGenAttAct01 (Base):\n    """"""This table relates splits to manuscripts.\n\n    .. sauml:: mysql:///VarGenAtt_ActPh4?read_default_group=ntg\n       :include: VarGenAttAct01\n\n    """"""\n\n    __tablename__ = \'VarGenAttAct\'\n\n    BOOK   = Column (Integer,     nullable=False, server_default=text (""\'0\'""))\n    CHBEG  = Column (Integer,     nullable=False, server_default=text (""\'0\'""))\n    VBEG   = Column (Integer,     nullable=False, server_default=text (""\'0\'""))\n    WBEG   = Column (Integer,     nullable=False, server_default=text (""\'0\'""))\n    WEND   = Column (Integer,     nullable=False, server_default=text (""\'0\'""))\n    VARID  = Column (String (2),  nullable=False, server_default=text (""\'\'""))\n    VARNEW = Column (String (2),  nullable=False)\n    S1     = Column (String (2),  nullable=False, server_default=text (""\'\'""))\n    S2     = Column (String (2),  nullable=False, server_default=text (""\'\'""))\n    PRS1   = Column (String (2),  nullable=False, server_default=text (""\'\'""))\n    PRS1A  = Column (String (2),  nullable=False, server_default=text (""\'\'""))\n    PRS1B  = Column (String (2),  nullable=False, server_default=text (""\'\'""))\n    PRS2   = Column (String (2),  nullable=False, server_default=text (""\'\'""))\n    PRS2A  = Column (String (2),  nullable=False, server_default=text (""\'\'""))\n    PRS2B  = Column (String (2),  nullable=False, server_default=text (""\'\'""))\n    WITN   = Column (String (10), nullable=False, server_default=text (""\'\'""))\n    MS     = Column (Integer,     nullable=False, server_default=text (""\'0\'""))\n    CHEND  = Column (Integer,     nullable=False, server_default=text (""\'0\'""))\n    VEND   = Column (Integer,     nullable=False, server_default=text (""\'0\'""))\n    BEGADR = Column (Integer,     nullable=False, server_default=text (""\'0\'""))\n    ENDADR = Column (Integer,     nullable=False, server_default=text (""\'0\'""))\n    CHECK  = Column (String (2),  nullable=False, server_default=text (""\'\'""))\n\n    __table_args__ = (\n        PrimaryKeyConstraint (BEGADR, ENDADR, MS),\n    )\n'"
ntg_common/tools.py,0,"b'# -*- encoding: utf-8 -*-\n\n"""""" This module contains some useful functions. """"""\n\nimport logging\nimport subprocess\n\nBOOKS = [\n    # id, siglum, name,            no. of chapters\n    ( 1, ""Mt"",   ""Matthew"",        28),\n\n    ( 2, ""Mc"",   ""Mark"",           16),\n\n    ( 3, ""L"",    ""Luke"",           24),\n\n    ( 4, ""J"",    ""John"",           21),\n\n    ( 5, ""Acts"", ""Acts"",           28),\n\n    ( 6, ""R"",    ""Romans"",         16),\n    ( 7, ""1K"",   ""1 Corinthians"",  16),\n    ( 8, ""2K"",   ""2 Corinthians"",  13),\n    ( 9, ""G"",    ""Galatians"",       6),\n    (10, ""E"",    ""Ephesians"",       6),\n    (11, ""Ph"",   ""Philippians"",     4),\n    (12, ""Kol"",  ""Colossians"",      4),\n    (13, ""1Th"",  ""1 Thessalonians"", 5),\n    (14, ""2Th"",  ""2 Thessalonians"", 3),\n\n    (15, ""1T"",   ""1 Timothy"",       6),\n    (16, ""2T"",   ""2 Timothy"",       4),\n    (17, ""Tt"",   ""Titus"",           3),\n    (18, ""Phm"",  ""Philemon"",        1),\n\n    (19, ""H"",    ""Hebrews"",        13),\n\n    (20, ""Jc"",   ""James"",           5),\n    (21, ""1P"",   ""1 Peter"",         5),\n    (22, ""2P"",   ""2 Peter"",         3),\n    (23, ""1J"",   ""1 John"",          5),\n    (24, ""2J"",   ""2 John"",          1),\n    (25, ""3J"",   ""3 John"",          1),\n    (26, ""Jd"",   ""Jude"",            1),\n\n    (27, ""Ap"",   ""Revelation"",     22),\n\n    (210, ""2Sam"", ""2 Samuel"",       2), # Hack for Septuaginta Test\n]\n"""""" Titles of the NT books """"""\n\nBYZ_HSNR = {\n    ""Acts""     : ""(300010, 300180, 300350, 303300, 303980, 304240, 312410)"",\n    ""CL""       : ""(300010, 300180, 300350, 303300, 303980, 304240, 312410)"",\n    ""John""     : ""(200070, 200280, 200450, 300180, 300350, 302260, 313200)"",\n    ""Mark""     : ""(300030, 300180, 300350, 301050, 302610, 303510, 326070)"",\n    ""2 Samuel"" : None,\n}\n""""""Manuscripts attesting the Byzantine Text.\n\nWe use these manuscripts as templates to establish the Byzantine Text according\nto our rules.\n\n""""""\n\n# Mk  7:16, 9:44, 9:46, 11:26, 15:28, 16:9-20\n\nFEHLVERSE = """"""\n    (\n      begadr >= 20716000 and endadr <= 20716999 or\n      begadr >= 20944000 and endadr <= 20944999 or\n      begadr >= 20946000 and endadr <= 20946999 or\n      begadr >= 21126000 and endadr <= 21126999 or\n      begadr >= 21528000 and endadr <= 21528999 or\n      begadr >= 21609000 and endadr <= 21620999 or\n      begadr >= 21608068 and endadr <= 21620999 or\n\n      begadr >= 50837002 and endadr <= 50837047 or\n      begadr >= 51534002 and endadr <= 51534013 or\n      begadr >= 52406020 and endadr <= 52408015 or\n      begadr >= 52829002 and endadr <= 52829025\n    )\n    """"""\n""""""Verses added in later times.\n\nThese verses were added to the NT in later times. Because they are not original\nthey are not included in the text of manuscript \'A\'.\n\n""""""\n\n\nlogger = logging.getLogger ()\n\ndef quote (s):\n    if \' \' in s:\n        return \'""\' + s + \'""\'\n    return s\n\n\ndef log (level, msg, *aargs, **_kwargs):\n    """"""\n    Low level log function\n    """"""\n\n    logger.log (level, msg, *aargs)\n\n\ndef get_book_by_id (id_):\n    for b in BOOKS:\n        if b[0] == id_:\n            return b\n    return None\n\n\ndef graphviz_layout (dot, format = \'dot\'):\n    """"""Call the GraphViz dot program to generate an image but mostly to precompute\n    the graph layout.\n\n    """"""\n\n    cmdline = [\'dot\', \'-T%s\' % format]\n\n    p = subprocess.Popen (\n        cmdline,\n        stdin  = subprocess.PIPE,\n        stdout = subprocess.PIPE,\n        stderr = subprocess.PIPE)\n\n    try:\n        outs, errs = p.communicate (dot.encode (\'utf-8\'), timeout = 15)\n    except subprocess.TimeoutExpired:\n        p.kill ()\n        outs, errs = p.communicate ()\n\n    #if p.returncode != 0:\n    #    raise subprocess.CalledProcessError (\n    #        \'Program terminated with status: %d. stderr is: %s\' % (\n    #            p.returncode, errs))\n    if errs:\n        log (logging.ERROR, errs)\n\n    return outs\n'"
server/__init__.py,0,"b'""""""This package implements the application server for CBGM.\n\n""""""\n'"
server/__main__.py,0,"b'#!/usr/bin/python3\n# -*- encoding: utf-8 -*-\n\n""""""This package implememts the API server for CBGM.\n\nThe main Flask driver.\n\nThis module sets up the main Flask application for user authentication and the\nsub-apps for each book.\n\n""""""\n\nimport argparse\nimport collections\nimport glob\nimport logging\nimport os\nimport os.path\nimport time\n\nimport flask\nfrom flask import current_app\nimport flask_sqlalchemy\nimport flask_user\nimport flask_login\nimport flask_mail\nfrom werkzeug.middleware.dispatcher import DispatcherMiddleware\n\nfrom ntg_common.config import args, init_logging\nfrom ntg_common import db_tools\nfrom ntg_common.exceptions import EditException\n\nimport login\nimport main\nimport info\nimport static\nimport textflow\nimport comparison\nimport editor\nimport set_cover\nimport checks\n\ndba = flask_sqlalchemy.SQLAlchemy ()\nuser, _role, _roles_users = login.declare_user_model_on (dba)\ndb_adapter = flask_user.SQLAlchemyAdapter (dba, user)\nlogin_manager = flask_login.LoginManager ()\nlogin_manager.anonymous_user = login.AnonymousUserMixin\nuser_manager = flask_user.UserManager (db_adapter)\nmail = flask_mail.Mail ()\n\n\nclass Config ():\n    """""" Default configuration object. """"""\n\n    APPLICATION_HOST    = \'localhost\'\n    APPLICATION_PORT    = 5000\n    APPLICATION_DESCRIPTION = \'\'\n    CONFIG_FILE         = \'_global.conf\' # default relative to /instance\n    STATIC_FOLDER       = \'static\'\n    STATIC_URL_PATH     = \'static\'\n    AFTER_LOGIN_URL     = None\n    USE_RELOADER        = False\n    USE_DEBUGGER        = False\n    SERVER_START_TIME   = str (int (time.time ())) # for cache busting\n    READ_ACCESS         = \'none\'\n    READ_ACCESS_PRIVATE = \'none\'\n    WRITE_ACCESS        = \'none\'\n    CORS_ALLOW_ORIGIN   = \'*\'\n    SQLALCHEMY_TRACK_MODIFICATIONS = False\n\n\ndef build_parser (default_config_file = Config.CONFIG_FILE):\n    """""" Build the commandline parser. """"""\n\n    parser = argparse.ArgumentParser (description = __doc__)\n\n    parser.add_argument (\n        \'-v\', \'--verbose\', dest=\'verbose\', action=\'count\',\n        help=\'increase output verbosity\', default=0\n    )\n    parser.add_argument (\n        \'-c\', \'--config-file\', dest=\'config_file\',\n        default=default_config_file, metavar=\'CONFIG_FILE\',\n        help=""the config file (default=\'./instance/%s\')"" % default_config_file\n    )\n    return parser\n\n\ndef do_init_app (app):\n    """""" Initializations to do on main and sub apps. """"""\n\n    app.config[\'APPLICATION_ROOT\'] = app.config[\'APPLICATION_ROOT\'].rstrip (\'/\')\n\n    dba.init_app (app)\n    mail.init_app (app)\n    login.init_app (app)\n    user_manager.init_app (app, login_manager = login_manager,\n                           make_safe_url_function = login.make_safe_url)\n\n    @app.errorhandler (EditException)\n    def handle_invalid_edit (ex):\n        response = flask.jsonify (ex.to_dict ())\n        response.status_code = ex.status_code\n        return response\n\n    @app.after_request\n    def add_headers (response):\n        response.headers[\'Access-Control-Allow-Origin\'] = current_app.config[\'CORS_ALLOW_ORIGIN\']\n        response.headers[\'Access-Control-Allow-Credentials\'] = \'true\'\n        # response.headers[\'Cache-Control\'] = \'private, max-age=3600\'\n        response.headers[\'Content-Security-Policy\'] = ""worker-src blob:""\n        response.headers[\'Server\'] = \'Jetty 0.8.15\'\n        return response\n\n    app.logger.info (""Mounted {name} at {host}:{port}{mount} from conf {conf}"".format (\n        name  = app.config[\'APPLICATION_NAME\'],\n        host  = app.config[\'APPLICATION_HOST\'],\n        port  = app.config[\'APPLICATION_PORT\'],\n        mount = app.config[\'APPLICATION_ROOT\'],\n        conf  = app.config[\'CONFIG_FILE\']\n    ))\n\n\ndef create_app (Config):\n    """""" App creation function """"""\n\n    instance_path = os.path.abspath (\'instance\')\n\n    app = flask.Flask (__name__)\n\n    global_config = os.path.join (instance_path, Config.CONFIG_FILE)\n    app.config.from_object (Config)\n    app.config.from_pyfile (global_config)\n\n    # pylint: disable=no-member\n    app.logger.setLevel (Config.LOG_LEVEL)\n    app.logger.info (""Instance path: {ip}"".format (ip = instance_path))\n\n    app.register_blueprint (static.bp)\n    app.register_blueprint (login.bp)\n\n    app.config.dba = db_tools.PostgreSQLEngine (**app.config)\n    user_db_url = app.config.dba.url\n    # tell flask_sqlalchemy where the user authentication database is\n    app.config[\'SQLALCHEMY_DATABASE_URI\'] = user_db_url\n\n    static.init_app (app)\n    do_init_app (app)\n\n    instances = collections.OrderedDict ()\n    extra_files = [instance_path + \'/\' + Config.CONFIG_FILE]\n\n    for fn in glob.glob (instance_path + \'/*.conf\'):\n        extra_files.append (fn)\n        fn = os.path.basename (fn)\n        if fn == Config.CONFIG_FILE:\n            continue\n\n        sub_app = flask.Flask (__name__)\n        sub_app.config.from_object (Config)\n        sub_app.config.from_pyfile (global_config)\n        sub_app.config.from_pyfile (os.path.join (instance_path, fn))\n        sub_app.config[\'CONFIG_FILE\'] = fn\n        sub_app.config[\'APPLICATION_DIR\'] = sub_app.config[\'APPLICATION_ROOT\']\n        sub_app.config[\'APPLICATION_ROOT\'] = os.path.join (\n            app.config[\'APPLICATION_ROOT\'], sub_app.config[\'APPLICATION_ROOT\']\n        )\n        sub_app.register_blueprint (main.bp)\n        sub_app.register_blueprint (textflow.bp)\n        sub_app.register_blueprint (comparison.bp)\n        sub_app.register_blueprint (editor.bp)\n        sub_app.register_blueprint (set_cover.bp)\n        sub_app.register_blueprint (checks.bp)\n\n        sub_app.config.dba = db_tools.PostgreSQLEngine (**sub_app.config)\n        sub_app.config[\'SQLALCHEMY_DATABASE_URI\'] = user_db_url\n\n        do_init_app (sub_app)\n        main.init_app (sub_app)\n        textflow.init_app (sub_app)\n        comparison.init_app (sub_app)\n        editor.init_app (sub_app)\n        set_cover.init_app (sub_app)\n        checks.init_app (sub_app)\n\n        instances[sub_app.config[\'APPLICATION_ROOT\']] = sub_app\n\n    info_app = flask.Flask (__name__)\n    info_app.config.update (app.config)\n    info_app.register_blueprint (info.bp)\n    do_init_app (info_app)\n    info.init_app (app, instances)\n\n    instances[app.config[\'APPLICATION_ROOT\']] = info_app\n\n    d = DispatcherMiddleware (app, instances)\n    d.config = app.config\n    d.config[\'EXTRA_FILES\'] = extra_files\n    return d\n\n\nif __name__ == ""__main__"":\n    from werkzeug.serving import run_simple\n\n    build_parser ().parse_args (namespace = args)\n    init_logging (\n        args,\n        flask.logging.default_handler,\n        logging.FileHandler (\'server.log\')\n    )\n\n    Config.LOG_LEVEL   = args.log_level\n    Config.CONFIG_FILE = args.config_file\n    app = create_app (Config)\n\n    run_simple (\n        app.config[\'APPLICATION_HOST\'],\n        app.config[\'APPLICATION_PORT\'],\n        app,\n        use_reloader = app.config[\'USE_RELOADER\'],\n        use_debugger = app.config[\'USE_DEBUGGER\'],\n        extra_files  = app.config[\'EXTRA_FILES\']\n    )\n'"
server/checks.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""The API server for CBGM.  The checks module.\n\n""""""\n\nimport collections\nimport itertools\nimport logging\n\nimport flask\nfrom flask import request, current_app\n\nimport numpy as np\n\nfrom ntg_common.db_tools import execute\nfrom ntg_common.cbgm_common import CBGM_Params, create_labez_matrix\nfrom ntg_common import tools\n\nfrom helpers import Passage, Manuscript, make_json_response, csvify\n\n\nbp = flask.Blueprint (\'checks\', __name__)\n\n\ndef init_app (app):\n    """""" Init the Flask app. """"""\n\n    pass\n\n\ndef congruence (conn, passage):\n    """"""Check the congruence.\n\n    ""Das Pr\xc3\xbcfprogramm soll eine Inkongruenz anzeigen, wenn der Zeuge einer Lesart\n    x, die im lokalen Stemma von y abh\xc3\xa4ngt UND (bei x keinen pV mit Conn <= 5\n    hat ODER bei y keinen pV mit h\xc3\xb6herem Rang hat als ein weiterer pV bei einer\n    anderen Variante), nicht mit x ODER x(n) der Quelle ""?"" zugeordnet wird.""\n    -- email K. Wachtel 16.01.2020\n\n    Wenn Lesart x im lokalen Stemma von y != ? abh\xc3\xa4ngt, mu\xc3\x9f jeder Zeuge der\n    Lesart x:\n\n    1. einen pV(conn=5) der Lesart x haben, oder\n\n    2. der h\xc3\xb6chste pV(!= zz) die Lesart y haben.\n\n    Wenn Lesart x im lokalen Stemma von ? abh\xc3\xa4ngt, ist keine Aussage m\xc3\xb6glich.\n\n    """"""\n\n    res = execute (conn, """"""\n    -- get the closest ancestors ms1 for every manuscript ms2\n    WITH ranks AS (\n      SELECT\n        aff.ms_id1,\n        aff.ms_id2,\n        ms1.hs as hs1,\n        ms2.hs as hs2,\n        q1.labez AS labez1,\n        q2.labez AS labez2,\n        q1.clique AS clique1,\n        q2.clique AS clique2,\n        labez_clique (q1.labez, q1.clique) as lq1,\n        labez_clique (q2.labez, q2.clique) as lq2,\n        l.source_labez,\n        l.source_clique,\n        labez_clique (l.source_labez, l.source_clique) as source_lq,\n        rank () OVER (PARTITION BY ms_id2 ORDER BY affinity DESC, common, older, newer DESC, ms_id1) AS rank,\n        affinity\n      FROM affinity_p_view aff\n        JOIN manuscripts ms1 ON ms1.ms_id = aff.ms_id1\n        JOIN manuscripts ms2 ON ms2.ms_id = aff.ms_id2\n        JOIN apparatus_cliques_view q1 ON q1.ms_id = aff.ms_id1 AND q1.pass_id = :pass_id\n        JOIN apparatus_cliques_view q2 ON q2.ms_id = aff.ms_id2 AND q2.pass_id = :pass_id\n        JOIN locstem l ON (l.pass_id, l.labez, l.clique) = (q2.pass_id, q2.labez, q2.clique)\n      WHERE ms_id1 NOT IN :exclude\n        AND ms_id2 NOT IN :exclude\n        AND q1.labez != \'zz\'\n        AND q2.labez != \'zz\'\n        AND q1.certainty = 1.0\n        AND q2.certainty = 1.0\n        AND aff.rg_id = :rg_id\n        AND aff.newer < aff.older\n        AND aff.common > aff.ms2_length / 2\n      ORDER BY affinity DESC\n    )\n\n    -- output mss that fail both rules\n    SELECT hs1, hs2, ms_id1, ms_id2, lq1, lq2, rank\n    FROM ranks r\n    WHERE lq1 != lq2\n      AND r.source_labez != \'?\'\n      AND r.rank <= :connectivity\n      AND -- ms2 fails rule 1\n        NOT EXISTS (\n          SELECT 1 FROM ranks rr\n          WHERE rr.ms_id2 = r.ms_id2\n            AND rr.lq1    = r.lq2\n            AND rr.rank  <= :connectivity\n        )\n      AND -- ms2 fails rule 2\n        NOT EXISTS (\n          SELECT * FROM ranks rr\n          WHERE rr.ms_id2     = r.ms_id2\n            AND (rr.source_lq = r.lq1 OR rr.source_labez = \'?\')\n            AND rr.rank <= 1\n        )\n    ORDER BY hs2, rank\n    """""", dict (\n        rg_id   = passage.range_id (\'All\'),\n        pass_id = passage.pass_id,\n        connectivity = 5,\n        exclude = (2,),\n    ))\n\n    Ranks = collections.namedtuple (\'Ranks\', \'ms1 ms2 ms_id1 ms_id2 labez1 labez2 rank\')\n    ranks = list (map (Ranks._make, res))\n\n    tools.log (logging.INFO, \'rg_id: \' + str (passage.range_id (\'All\')))\n\n    return ranks\n\n\ndef congruence_list (conn, passage, range_id):\n    """"""Check the congruence.\n\n    ""Das Pr\xc3\xbcfprogramm soll eine Inkongruenz anzeigen, wenn der Zeuge einer Lesart\n    x, die im lokalen Stemma von y abh\xc3\xa4ngt UND (bei x keinen pV mit Conn <= 5\n    hat ODER bei y keinen pV mit h\xc3\xb6herem Rang hat als ein weiterer pV bei einer\n    anderen Variante), nicht mit x ODER x(n) der Quelle ""?"" zugeordnet wird.""\n    -- email K. Wachtel 16.01.2020\n\n    Wenn Lesart x im lokalen Stemma von y != ? abh\xc3\xa4ngt, mu\xc3\x9f jeder Zeuge der\n    Lesart x:\n\n    1. einen pV(conn=5) der Lesart x haben, oder\n\n    2. der h\xc3\xb6chste pV(!= zz) die Lesart y haben.\n\n    Wenn Lesart x im lokalen Stemma von ? abh\xc3\xa4ngt, ist keine Aussage m\xc3\xb6glich.\n\n    """"""\n\n    res = execute (conn, """"""\n    -- get the closest ancestors ms1 for every manuscript ms2\n    WITH ranked AS (\n      SELECT\n        ms_id1,\n        ms_id2,\n        rank () OVER (PARTITION BY ms_id2 ORDER BY affinity DESC, common, older, newer DESC, ms_id1) AS rank,\n        affinity\n      FROM affinity_p_view aff\n      WHERE ms_id1 NOT IN :exclude\n        AND ms_id2 NOT IN :exclude\n        AND aff.rg_id = :rg_id\n        AND aff.newer < aff.older\n        AND aff.common > aff.ms2_length / 2\n      ORDER BY ms_id2, affinity DESC\n    ),\n\n    -- get readings\n    readings AS (\n      SELECT\n        p.pass_id,\n        p.begadr,\n        p.endadr,\n        r.ms_id1,\n        r.ms_id2,\n        r.rank,\n        q1.labez AS labez1,\n        q2.labez AS labez2,\n        q1.clique AS clique1,\n        q2.clique AS clique2,\n        labez_clique (q1.labez, q1.clique) as lq1,\n        labez_clique (q2.labez, q2.clique) as lq2,\n        l.source_labez  as source_l2,\n        l.source_clique as source_q2,\n        labez_clique (l.source_labez, l.source_clique) as source_lq2,\n        row_number () OVER (PARTITION BY p.pass_id, r.ms_id2 ORDER BY r.rank) as row_no,\n        count (*) FILTER (WHERE q1.labez !~ \'^z\') OVER (\n           PARTITION BY p.pass_id, r.ms_id2\n           ORDER BY r.rank\n           ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as row_no_no_zz\n      FROM passages p\n        JOIN ranges rg ON (rg.rg_id = :range_id AND rg.passage @> p.passage)\n      CROSS JOIN ranked r\n        JOIN apparatus_cliques_view q1 ON q1.ms_id = r.ms_id1 AND q1.pass_id = p.pass_id\n        JOIN apparatus_cliques_view q2 ON q2.ms_id = r.ms_id2 AND q2.pass_id = p.pass_id\n        JOIN locstem l ON (l.pass_id, l.labez, l.clique) = (q2.pass_id, q2.labez, q2.clique)\n      WHERE q1.certainty = 1.0\n        AND q2.certainty = 1.0\n        AND r.rank <= 2 * :connectivity -- speed things up\n      ORDER BY pass_id, ms_id2, rank\n    )\n\n    -- output mss that fail both rules\n    SELECT\n      r.pass_id, r.begadr, r.endadr, ms1.hs AS hs1, ms2.hs AS hs2, r.ms_id1, r.ms_id2, r.lq1, r.lq2, r.rank\n    FROM readings r\n      JOIN manuscripts ms1 ON ms1.ms_id = r.ms_id1\n      JOIN manuscripts ms2 ON ms2.ms_id = r.ms_id2\n    WHERE r.lq2       != lq1\n      AND r.labez1    !~ \'^z\'\n      AND r.labez2    !~ \'^z\'\n      AND r.source_l2 != \'?\'\n      AND r.row_no    = 1\n      -- ancestor ms1 reads different from descendant ms2\n      AND -- ms2 fails rule 1 (mu\xc3\x9f einen pV(conn=5) der Lesart x haben)\n        NOT EXISTS (\n          SELECT 1 FROM readings c\n          WHERE c.ms_id2  = r.ms_id2\n            AND c.pass_id = r.pass_id\n            AND c.lq1     = r.lq2\n            AND c.row_no <= :connectivity\n        )\n      AND -- ms2 fails rule 2 (der h\xc3\xb6chste pV(!= zz) mu\xc3\x9f die Lesart y haben)\n        NOT EXISTS (\n          SELECT 1 FROM readings c\n          WHERE c.ms_id2  = r.ms_id2\n            AND c.pass_id = r.pass_id\n            AND c.lq1     = r.source_lq2\n            AND c.row_no_no_zz = 1\n        )\n\n    ORDER BY pass_id, ms1.hsnr\n    """""", dict (\n        rg_id = passage.range_id (\'All\'),\n        range_id = range_id,\n        connectivity = 5,\n        exclude = (1,2),\n    ))\n\n    Ranks = collections.namedtuple (\'Ranks\', \'pass_id begadr endadr ms1 ms2 ms_id1 ms_id2 labez1 labez2 rank\')\n    ranks = []\n    for r in res:\n        rank = Ranks._make (r)._asdict ()\n        rank[\'hr\'] = Passage.static_to_hr (rank[\'begadr\'], rank[\'endadr\'])\n        ranks.append (rank)\n\n    return ranks\n\n\n@bp.route (\'/checks/congruence.json/<passage_or_id>\')\ndef congruence_json (passage_or_id):\n    """""" Endpoint: check the congruence """"""\n\n    with current_app.config.dba.engine.begin () as conn:\n        passage = Passage (conn, passage_or_id)\n        return make_json_response (congruence (conn, passage))\n\n\n@bp.route (\'/checks/congruence_list.json/<range_id>\')\ndef congruence_list_json (range_id):\n    """""" Endpoint: check the congruence """"""\n\n    with current_app.config.dba.engine.begin () as conn:\n        passage = Passage (conn, 1)\n        return make_json_response (congruence_list (conn, passage, range_id))\n'"
server/comparison.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""The comparison function of the API server for CBGM.""""""\n\nimport collections\n\nimport flask\nfrom flask import request, current_app\n\nfrom ntg_common.db_tools import execute\n\nfrom login import auth\nfrom helpers import csvify, parameters, Passage, Manuscript\n\n\nbp = flask.Blueprint (\'comparison\', __name__)\n\n\ndef init_app (_app):\n    """""" Initialize the flask app. """"""\n\n\n_ComparisonRow = collections.namedtuple (\n    \'ComparisonRow\',\n    \'rg_id, range, common, equal, older, newer, unclear, affinity, rank, length1, length2\'\n)\n\n\nclass _ComparisonRowCalcFields (_ComparisonRow):\n    __slots__ = ()\n\n    _fields = _ComparisonRow._fields + (\'norel\', )\n\n    @property\n    def norel (self):\n        return self.common - self.equal - self.older - self.newer - self.unclear\n\n    def _asdict (self):\n        return collections.OrderedDict (zip (self._fields, self + (self.norel, )))\n\n\ndef comparison_summary ():\n    """"""Output comparison of 2 witnesses, chapter summary.\n\n    Outputs a summary of the differences between 2 manuscripts, one summary row\n    per chapters.\n\n    """"""\n\n    with current_app.config.dba.engine.begin () as conn:\n        ms1 = Manuscript (conn, request.args.get (\'ms1\') or \'A\')\n        ms2 = Manuscript (conn, request.args.get (\'ms2\') or \'A\')\n\n        res = execute (conn, """"""\n        (WITH ranks AS (\n          SELECT ms_id1, ms_id2, rg_id, rank () OVER (PARTITION BY rg_id ORDER BY affinity DESC) AS rank, affinity\n          FROM affinity aff\n          WHERE ms_id1 = :ms_id1\n            AND {prefix}newer > {prefix}older\n          ORDER BY affinity DESC\n        )\n\n        SELECT a.rg_id, a.range, a.common, a.equal,\n               a.older, a.newer, a.unclear, a.affinity, r.rank, ms1_length, ms2_length\n        FROM {view} a\n        JOIN ranks r     USING (rg_id, ms_id1, ms_id2)\n        WHERE a.ms_id1 = :ms_id1 AND a.ms_id2 = :ms_id2\n        )\n\n        UNION\n\n        (WITH ranks2 AS (\n          SELECT ms_id1, ms_id2, rg_id, rank () OVER (PARTITION BY rg_id ORDER BY affinity DESC) AS rank, affinity\n          FROM affinity aff\n          WHERE ms_id2 = :ms_id2\n            AND {prefix}newer < {prefix}older\n          ORDER BY affinity DESC\n        )\n\n        SELECT a.rg_id, a.range, a.common, a.equal,\n               a.older, a.newer, a.unclear, a.affinity, r.rank, ms1_length, ms2_length\n        FROM {view} a\n        JOIN ranks2 r USING (rg_id, ms_id1, ms_id2)\n        WHERE a.ms_id1 = :ms_id1 AND a.ms_id2 = :ms_id2\n        )\n\n        UNION\n\n        SELECT a.rg_id, a.range, a.common, a.equal,\n               a.older, a.newer, a.unclear, a.affinity, NULL, ms1_length, ms2_length\n        FROM {view} a\n        WHERE a.ms_id1 = :ms_id1 AND a.ms_id2 = :ms_id2 AND a.newer = a.older\n\n        ORDER BY rg_id\n        """""", dict (parameters, ms_id1 = ms1.ms_id, ms_id2 = ms2.ms_id,\n                   view = \'affinity_p_view\', prefix = \'p_\'))\n\n        return list (map (_ComparisonRowCalcFields._make, res))\n\n\n_ComparisonDetailRow = collections.namedtuple (\n    \'ComparisonDetailRow\',\n    \'pass_id begadr endadr labez_clique1 lesart1 labez_clique2 lesart2 older newer unclear\'\n)\n\nclass _ComparisonDetailRowCalcFields (_ComparisonDetailRow):\n    __slots__ = ()\n\n    _fields = _ComparisonDetailRow._fields + (\'pass_hr\', \'norel\')\n\n    @property\n    def pass_hr (self):\n        return Passage.static_to_hr (self.begadr, self.endadr)\n\n    @property\n    def norel (self):\n        return not (self.older or self.newer or self.unclear)\n\n    def _asdict (self):\n        return collections.OrderedDict (zip (self._fields, self + (self.pass_hr, self.norel)))\n\n\ndef comparison_detail ():\n    """"""Output comparison of 2 witnesses, chapter detail.\n\n    Outputs a detail of the differences between 2 manuscripts in one chapter.\n    """"""\n\n    with current_app.config.dba.engine.begin () as conn:\n        ms1 = Manuscript (conn, request.args.get (\'ms1\') or \'A\')\n        ms2 = Manuscript (conn, request.args.get (\'ms2\') or \'A\')\n        range_ = request.args.get (\'range\') or \'All\'\n\n        res = execute (conn, """"""\n        SELECT p.pass_id, p.begadr, p.endadr, v1.labez_clique, v1.lesart,\n                                              v2.labez_clique, v2.lesart,\n          is_p_older (p.pass_id, v1.labez, v1.clique, v2.labez, v2.clique) AS older,\n          is_p_older (p.pass_id, v2.labez, v2.clique, v1.labez, v1.clique) AS newer,\n          is_p_unclear (p.pass_id, v1.labez, v1.clique) OR\n          is_p_unclear (p.pass_id, v2.labez, v2.clique) AS unclear\n        FROM (SELECT * FROM ranges WHERE range = :range_) r\n          JOIN passages p ON (r.passage @> p.passage )\n          JOIN apparatus_cliques_view v1 USING (pass_id)\n          JOIN apparatus_cliques_view v2 USING (pass_id)\n        WHERE v1.ms_id = :ms1 AND v2.ms_id = :ms2\n          AND v1.labez != v2.labez AND v1.labez !~ \'^z\' AND v2.labez !~ \'^z\'\n          AND v1.cbgm AND v2.cbgm\n        ORDER BY p.pass_id\n        """""", dict (parameters, ms1 = ms1.ms_id, ms2 = ms2.ms_id, range_ = range_))\n\n        return list (map (_ComparisonDetailRowCalcFields._make, res))\n\n\n@bp.route (\'/comparison-summary.csv\')\ndef comparison_summary_csv ():\n    """"""Endpoint. Serve a CSV table. (see also :func:`comparison_summary`)""""""\n\n    auth ()\n\n    return csvify (_ComparisonRowCalcFields._fields, comparison_summary ())\n\n\n@bp.route (\'/comparison-detail.csv\')\ndef comparison_detail_csv ():\n    """"""Endpoint. Serve a CSV table. (see also :func:`comparison_detail`)""""""\n\n    auth ()\n\n    return csvify (_ComparisonDetailRowCalcFields._fields, comparison_detail ())\n'"
server/editor.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""The API server for CBGM.  Stemma editing module. """"""\n\nimport collections\nimport logging\nimport re\n\nimport flask\nfrom flask import request, current_app\nimport flask_login\nimport networkx as nx\nimport sqlalchemy\n\nfrom ntg_common import tools\nfrom ntg_common import db_tools\nfrom ntg_common.exceptions import EditError, PrivilegeError\nfrom ntg_common.db_tools import execute\n\nfrom login import auth, private_auth, edit_auth\nfrom helpers import parameters, Passage, make_json_response, make_text_response\n\n# FIXME: this is too lax but we need to accomodate one spurious \'z\' reading\nRE_VALID_LABEZ  = re.compile (\'^([*]|[?]|[a-z]{1,2}|z[u-z])$\')\nRE_VALID_CLIQUE = re.compile (\'^[0-9]$\')\n\n# FIXME: this is too lax but we need to accomodate one spurious \'z\' reading\nRE_EXTRACT_LABEZ = re.compile (\'^([*]|[?]|[a-z]{1,2})\')\n\nbp = flask.Blueprint (\'editor\', __name__)\n\ndef init_app (_app):\n    """""" Initialize the flask app. """"""\n\n\n@bp.route (\'/stemma-edit/<passage_or_id>\', methods = [\'POST\'])\ndef stemma_edit (passage_or_id):\n    """"""Edit a local stemma.\n\n    Called from local-stemma.js (split, merge, move) and textflow.js (move-manuscripts).\n\n    """"""\n\n    edit_auth ()\n\n    args = request.get_json ()\n\n    action = args.get (\'action\')\n\n    if action not in (\'add\', \'del\', \'split\', \'merge\', \'move\', \'move-manuscripts\'):\n        raise EditError (\'Bad request\')\n\n    params = { }\n    for n in \'labez_old labez_new source_labez\'.split ():\n        if n in args:\n            params[n] = args.get (n)\n            if not RE_VALID_LABEZ.match (params[n]):\n                raise EditError (\'Bad request\')\n    for n in \'clique_old clique_new source_clique\'.split ():\n        if n in args:\n            params[n] = args.get (n)\n            if not RE_VALID_CLIQUE.match (params[n]):\n                raise EditError (\'Bad request\')\n\n    def integrity_error (e):\n        if \'ix_locstem_unique_original\' in str (e):\n            raise EditError (\n                \'\'\'Only one original reading allowed. If you want to change the original\n                reading, first remove the old original reading.<br/><br/>\'\'\' + str (e)\n            )\n        if \'locstem_pkey\' in str (e):\n            raise EditError (\n                \'\'\'This readings already dependes on that reading.<br/><br/>\'\'\' + str (e)\n            )\n        if \'same_source\' in str (e):\n            raise EditError (\n                \'\'\'A reading cannot be derived from the same reading.\n                If you want to <b>merge two readings</b>, use shift + drag.\'\'\'\n            )\n        raise EditError (str (e))\n\n    with current_app.config.dba.engine.begin () as conn:\n        passage = Passage (conn, passage_or_id)\n        params[\'pass_id\'] = passage.pass_id\n        params[\'user_id\'] = flask_login.current_user.id\n\n        res = execute (conn, """"""\n        SET LOCAL ntg.user_id = :user_id;\n        """""", dict (parameters, **params))\n\n        if action == \'move\':\n            # reassign a source reading\n            # there may be multiple existent assignments, there\'ll be only one left\n            try:\n                res = execute (conn, """"""\n                DELETE FROM locstem\n                WHERE (pass_id, labez, clique) = (:pass_id, :labez_old, :clique_old);\n                INSERT INTO locstem (pass_id, labez, clique, source_labez, source_clique)\n                VALUES (:pass_id, :labez_old, :clique_old, :labez_new, :clique_new)\n                """""", dict (parameters, **params))\n            except sqlalchemy.exc.IntegrityError as e:\n                integrity_error (e)\n            except sqlalchemy.exc.DatabaseError as e:\n                raise EditError (str (e))\n\n        if action == \'del\':\n            # remove a source reading\n            try:\n                # check if we are asked to remove the only link,\n                # in that case reassign to \'unknown\'\n                res = execute (conn, """"""\n                SELECT pass_id\n                FROM locstem\n                WHERE (pass_id, labez, clique) = (:pass_id, :labez_old, :clique_old);\n                """""", dict (parameters, **params))\n\n                tools.log (logging.INFO, \'Deleting: \' + str (params))\n\n                if res.rowcount > 1:\n                    res = execute (conn, """"""\n                    DELETE FROM locstem\n                    WHERE (pass_id, labez, clique) = (:pass_id, :labez_old, :clique_old)\n                      AND (source_labez, source_clique) = (:source_labez, :source_clique)\n                    """""", dict (parameters, **params))\n                else:\n                    res = execute (conn, """"""\n                    UPDATE locstem\n                    SET (source_labez, source_clique) = (\'?\', \'1\')\n                    WHERE (pass_id, labez, clique) = (:pass_id, :labez_old, :clique_old);\n                    """""", dict (parameters, **params))\n            except sqlalchemy.exc.IntegrityError as e:\n                integrity_error (e)\n            except sqlalchemy.exc.DatabaseError as e:\n                raise EditError (str (e))\n\n        if action == \'add\':\n            # add a source reading\n            try:\n                res = execute (conn, """"""\n                INSERT INTO locstem (pass_id, labez, clique, source_labez, source_clique)\n                VALUES (:pass_id, :labez_old, :clique_old, :labez_new, :clique_new)\n                """""", dict (parameters, **params))\n            except sqlalchemy.exc.IntegrityError as e:\n                integrity_error (e)\n            except sqlalchemy.exc.DatabaseError as e:\n                raise EditError (str (e))\n\n        if action in (\'add\', \'del\', \'move\'):\n            # test the still uncommitted changes\n\n            graph = db_tools.local_stemma_to_nx (conn, passage.pass_id)\n\n            # test: not a DAG\n            if not nx.is_directed_acyclic_graph (graph):\n                raise EditError (\'The new graph contains cycles.\')\n            # test: not connected\n            graph.add_edge (\'*\', \'?\')\n            if not nx.is_weakly_connected (graph):\n                raise EditError (\'The new graph is not connected.\')\n\n        elif action == \'split\':\n            # Get the lowest free integer for the new clique. See: #122\n            res = execute (conn, """"""\n            SELECT clique\n            FROM  cliques\n            WHERE pass_id = :pass_id AND labez = :labez_old\n            """""", dict (parameters, **params))\n\n            taken = set ([int (r[0]) for r in res])\n            n = 1\n            while n in taken:\n                n += 1\n            params[\'clique_next\'] = str (n)\n\n            # insert into cliques table\n            res = execute (conn, """"""\n            INSERT INTO cliques (pass_id, labez, clique)\n            VALUES (:pass_id, :labez_old, :clique_next)\n            """""", dict (parameters, **params))\n\n            # insert into locstem table with source = \'?\'\n            res = execute (conn, """"""\n            INSERT INTO locstem (pass_id, labez, clique, source_labez, source_clique)\n            VALUES (:pass_id, :labez_old, :clique_next, \'?\', \'1\')\n            """""", dict (parameters, **params))\n\n        elif action == \'merge\':\n            # merge two cliques (eg. b1, b2) into one clique (eg. b1)\n            #\n            # reassign manuscripts to merged clique\n            res = execute (conn, """"""\n            UPDATE ms_cliques\n            SET clique = :clique_new\n            WHERE (pass_id, labez, clique) = (:pass_id, :labez_old, :clique_old)\n            """""", dict (parameters, **params))\n\n            # reassign sources to merged clique\n            res = execute (conn, """"""\n            UPDATE locstem\n            SET source_clique = :clique_new\n            WHERE (pass_id, source_labez, source_clique) = (:pass_id, :labez_old, :clique_old)\n            """""", dict (parameters, **params))\n\n            # remove clique from locstem\n            res = execute (conn, """"""\n            DELETE FROM locstem\n            WHERE (pass_id, labez, clique) = (:pass_id, :labez_old, :clique_old)\n            """""", dict (parameters, **params))\n\n            # remove clique from cliques\n            res = execute (conn, """"""\n            DELETE FROM cliques\n            WHERE (pass_id, labez, clique) = (:pass_id, :labez_old, :clique_old)\n            """""", dict (parameters, **params))\n\n        elif action == \'move-manuscripts\':\n            # reassign a set of manuscripts to a new clique\n            ms_ids = set (args.get (\'ms_ids\') or [])\n\n            res = execute (conn, """"""\n            UPDATE apparatus_cliques_view\n            SET clique = :clique_new\n            WHERE (pass_id, labez, clique) = (:pass_id, :labez_old, :clique_old)\n              AND ms_id IN :ms_ids\n            """""", dict (parameters, ms_ids = tuple (ms_ids), **params))\n\n            tools.log (logging.INFO, \'Moved ms_ids: \' + str (ms_ids))\n\n        # return the changed passage\n        passage = Passage (conn, passage_or_id)\n        return make_json_response (passage.to_json ())\n\n    raise EditError (\'Could not edit local stemma.\')\n\n\n@bp.route (\'/notes.txt/<passage_or_id>\', methods = [\'GET\', \'PUT\'])\ndef notes_txt (passage_or_id):\n    """"""Read or write the editor notes for a passage\n\n    """"""\n\n    private_auth ()\n\n    with current_app.config.dba.engine.begin () as conn:\n        passage = Passage (conn, passage_or_id)\n\n        if request.method == \'PUT\':\n\n            edit_auth ()\n            json = request.get_json ()\n\n            res = execute (conn, """"""\n            SET LOCAL ntg.user_id = :user_id;\n            """""", dict (parameters, user_id = flask_login.current_user.id))\n\n            # check for edit conflicts\n            res = execute (conn, """"""\n            SELECT * FROM notes\n            WHERE pass_id = :pass_id AND note != :old_note\n            """""", dict (parameters,\n                       pass_id  = passage.pass_id,\n                       old_note = json[\'original\']))\n            for row in res:\n                return make_json_response (\n                    status = 409,\n                    message = \'Cannot save. The note was edited by another user.\'\n                )\n\n            # save\n            res = execute (conn, """"""\n            INSERT INTO notes AS n (pass_id, note)\n            VALUES (:pass_id, :note)\n            ON CONFLICT (pass_id) DO\n            UPDATE\n            SET note = :note\n            WHERE n.pass_id = EXCLUDED.pass_id\n            """""", dict (parameters,\n                       pass_id = passage.pass_id,\n                       note    = json[\'note\']))\n\n            return make_json_response (message = \'Note saved.\')\n\n        res = execute (conn, """"""\n        SELECT note\n        FROM notes\n        WHERE pass_id = :pass_id\n        """""", dict (parameters, pass_id = passage.pass_id))\n\n        if res.rowcount > 0:\n            return make_text_response (res.fetchone ()[0])\n        return make_text_response (\'\')\n\n\n@bp.route (\'/notes.json/<range_id>\')\ndef notes_json (range_id):\n    """"""Endpoint.  Get a list of all editor notes.""""""\n\n    private_auth ()\n\n    with current_app.config.dba.engine.begin () as conn:\n        res = execute (conn, """"""\n        SELECT pass_id, begadr, endadr, note\n        FROM passages_view p\n        JOIN ranges rg\n          ON (rg.passage @> p.passage)\n        JOIN notes\n          USING (pass_id)\n        WHERE rg.rg_id = :range_id\n        ORDER BY pass_id\n        """""", dict (parameters, range_id = range_id))\n\n        Notes = collections.namedtuple (\'Notes\', \'pass_id, begadr, endadr, note\')\n        notes = []\n        for r in res:\n            note = Notes._make (r)._asdict ()\n            note[\'hr\'] = Passage.static_to_hr (note[\'begadr\'], note[\'endadr\'])\n            notes.append (note)\n\n        return make_json_response (notes)\n'"
server/helpers.py,0,"b'#!/usr/bin/python3\n# -*- encoding: utf-8 -*-\n\n"""""" An application server for CBGM.  Helper classes. """"""\n\nimport collections\nimport itertools\nimport re\nimport os\nimport os.path\n\nimport flask\nimport flask_login\n\nfrom ntg_common import tools\nfrom ntg_common.db_tools import execute, to_csv\n\n\nparameters = dict ()\n\n\nLANGUAGES = {\n    \'en\': \'English\',\n    \'de\': \'Deutsch\'\n}\n\n\nLABEZ_I18N = {\n    \'zu\':      \'Overlap\',\n    \'zv\':      \'Lac\',\n    \'zw\':      \'Dubious\',\n    \'zz\':      \'Lac\',\n    \'lac\':     \'Lac\',\n    \'all\':     \'All\',\n    \'all+lac\': \'All+Lac\',\n}\n\n\nEXCLUDE_REGEX_MAP = {\n    \'A\'  : \'A\',\n    \'MT\' : \'MT\',\n    \'F\'  : \'F[1-9\xce\xa0][0-9]*\'\n}\n"""""" Regexes for the include/exclude toolbar buttons. """"""\n\n\ndef get_excluded_ms_ids (conn, include):\n    """"""Get the ms_ids of manuscripts to exclude.\n\n    Helps to implement the buttons ""A"", ""MT"", and ""F"" in the toolbar.\n\n    """"""\n\n    exclude = set (EXCLUDE_REGEX_MAP.keys ()) - set (include)\n    if not exclude:\n        return tuple ([-1]) # a non-existing id to avoid an SQL syntax error\n    exclude = [ EXCLUDE_REGEX_MAP[x] for x in exclude]\n\n    # get ids of nodes to exclude\n    res = execute (conn, """"""\n    SELECT ms_id\n    FROM manuscripts\n    WHERE hs ~ \'^({exclude})$\'\n    ORDER BY ms_id\n    """""", dict (parameters, exclude = \'|\'.join (exclude)))\n\n    return tuple ([ row[0] for row in res ] or [ -1 ])\n\n\nclass Bag ():\n    """""" Class to stick values in. """"""\n\n\nclass Manuscript ():\n    """""" Represent one manuscript. """"""\n\n    RE_HSNR = re.compile (r\'^\\d{6}$\')                      # 300180\n    RE_MSID = re.compile (r\'^id\\d+$\')                      # id123\n    RE_HS   = re.compile (r\'^([PL]?[s\\d]+|A|MT)$\', re.I)   # 01.\n\n    def __init__ (self, conn, manuscript_id_or_hs_or_hsnr):\n        """""" Initialize from manuscript id or hs or hsnr. """"""\n\n        self.conn = conn\n        self.ms_id = self.hs = self.hsnr = None\n\n        if Manuscript.RE_HSNR.search (manuscript_id_or_hs_or_hsnr):\n            where = \'hsnr\'\n            param = int (manuscript_id_or_hs_or_hsnr)\n        elif Manuscript.RE_MSID.search (manuscript_id_or_hs_or_hsnr):\n            where = \'ms_id\'\n            param = int (manuscript_id_or_hs_or_hsnr[2:])\n        elif Manuscript.RE_HS.search (manuscript_id_or_hs_or_hsnr):\n            where = \'hs\'\n            param = manuscript_id_or_hs_or_hsnr\n        else:\n            return\n\n        res = execute (conn, """"""\n        SELECT ms_id, hs, hsnr\n        FROM manuscripts\n        WHERE {where} = :param\n        """""", dict (parameters, where = where, param = param))\n\n        row = res.fetchone ()\n        if row is not None:\n            self.ms_id, self.hs, self.hsnr = row\n\n\n    def get_length (self, rg_id):\n        # Get the length of the manuscript, ie. the no. of existing passages\n\n        res = execute (self.conn, """"""\n        SELECT length\n        FROM ms_ranges\n        WHERE ms_id = :ms_id AND rg_id = :rg_id\n        """""", dict (parameters, ms_id = self.ms_id, rg_id = rg_id))\n\n        return res.fetchone ()[0]\n\n\n    def to_json (self):\n        return {\n            \'ms_id\'  : self.ms_id,\n            \'hs\'     : self.hs,\n            \'hsnr\'   : self.hsnr,\n        }\n\n\nclass Word ():\n    """""" Represents one word address. """"""\n\n    RE_HR_WORD = re.compile (r\'^(?:(\\d?\\w+)\\s+)?(?:(\\d+):)?(?:(\\d+)/)?(\\d+)$\')\n\n    def __init__ (self, w = 0):\n        w = int (w)\n        self.word    = w % 1000\n        w //= 1000\n        self.verse   = w % 100\n        w //= 100\n        self.chapter = w % 100\n        w //= 100\n        self.book    = w\n\n\n    def __str__ (self):\n        return str (10000000 * self.book + 100000 * self.chapter + 1000 * self.verse + self.word)\n\n\n    def parse (self, s):\n        # Parse an address from the format: ""Act 1:2/3-4""\n        m = Word.RE_HR_WORD.match (s)\n        if m:\n            self.book = 0\n            if m.group (1):\n                bk = m.group (1).lower ()\n                for b in tools.BOOKS:\n                    if b[1].lower () == bk or b[2].lower () == bk:\n                        self.book = b[0]\n                        break\n            self.chapter = int (m.group (2) or \'0\')\n            self.verse   = int (m.group (3) or \'0\')\n            self.word    = int (m.group (4) or \'0\')\n        return self\n\n\n    def format (self, start = None):\n        # Format a word to the format: ""Acts 1:2/3-4""\n        if start is None:\n            return ""%s %d:%d/%d"" % (\n                tools.get_book_by_id (self.book)[1], self.chapter, self.verse, self.word)\n\n        if start.book != self.book:\n            return "" - %s %d:%d/%d"" % (\n                tools.get_book_by_id (self.book)[1], self.chapter, self.verse, self.word)\n        if start.chapter != self.chapter:\n            return "" - %d:%d/%d"" % (self.chapter, self.verse, self.word)\n        if start.verse != self.verse:\n            return "" - %d/%d"" % (self.verse, self.word)\n        if start.word != self.word:\n            return ""-%d"" % self.word\n        return """"\n\n\nclass Passage ():\n    """""" Represents one passage. """"""\n\n    def __init__ (self, conn, passage_or_id):\n        """""" Initialize from passage or passage id. """"""\n\n        self.conn = conn\n        self.pass_id, self.start, self.end, self.bk_id, self.chapter = 0, 0, 0, 0, 0\n        start, end =  self.fix (str (passage_or_id))\n\n        if int (start) > 10000000:\n            res = execute (conn, """"""\n            SELECT pass_id, begadr, endadr, adr2bk_id (begadr), adr2chapter (begadr)\n            FROM passages\n            WHERE begadr = :begadr AND endadr = :endadr\n            """""", dict (parameters, begadr = start, endadr = end))\n        else:\n            res = execute (conn, """"""\n            SELECT pass_id, begadr, endadr, adr2bk_id (begadr), adr2chapter (begadr)\n            FROM passages\n            WHERE pass_id = :pass_id\n            """""", dict (parameters, pass_id = start))\n\n        row = res.fetchone ()\n        if row is not None:\n            self.pass_id, self.start, self.end, self.bk_id, self.chapter = row\n\n\n    @staticmethod\n    def static_to_hr (start, end):\n        # return passage in human-readable format\n        s = Word (start)\n        if start == end:\n            return s.format ()\n        e = Word (end)\n        return s.format () + e.format (s)\n\n\n    def to_hr (self):\n        # return passage in human-readable format\n        return Passage.static_to_hr (self.start, self.end)\n\n\n    def to_json (self):\n        s = Word (self.start)\n        hr = self.to_hr ()\n        bk = tools.get_book_by_id (self.bk_id)\n        return {\n            \'pass_id\'  : self.pass_id,\n            \'hr\'       : hr,\n            \'start\'    : str (self.start),\n            \'end\'      : str (self.end),\n            \'passage\'  : self.to_passage (),\n            \'bk_id\'    : self.bk_id,\n            \'rg_id\'    : self.range_id (),\n            \'siglum\'   : bk[1],\n            \'book\'     : bk[2],\n            \'chapter\'  : str (self.chapter),\n            \'verse\'    : s.verse,\n            \'word\'     : hr.split (\'/\', 1)[1],\n        }\n\n\n    @staticmethod\n    def parse (hr):\n        if \'-\' in hr:\n            s, e = hr.split (\'-\')\n            s = Word ().parse (s.strip ())\n            e = Word ().parse (e.strip ())\n            e.book    = e.book    or s.book\n            e.chapter = e.chapter or s.chapter\n            e.verse   = e.verse   or s.verse\n            e.word    = e.word    or s.word\n            return ""%s-%s"" % (str (s), str (e))\n        return str (Word ().parse (hr))\n\n\n    @staticmethod\n    def fix (passage):\n        if \'-\' in passage:\n            start, end = passage.split (\'-\')\n        else:\n            start, end = passage, passage\n\n        start = str (int (start))\n        end   = str (int (end))\n        cut   = len (start) - len (end)\n        return start, start[:cut] + end\n\n\n    def range_id (self, range_ = None):\n        """""" Return the id of the range containing this passage. """"""\n\n        range_ = range_ or str (self.chapter)\n\n        res = execute (self.conn, """"""\n        SELECT rg_id\n        FROM ranges_view\n        WHERE bk_id = :bk_id AND range = :range_\n        """""", dict (parameters, bk_id = self.bk_id, range_ = range_))\n\n        row = res.fetchone ()\n        return row[0] if row is not None else None\n\n\n    def request_rg_id (self, request):\n        return int (request.args.get (\'rg_id\', 0)) or self.range_id (\'All\')\n\n\n    def to_passage (self):\n        # get a passage id in the form ""start-end""\n        if self.start == self.end:\n            return str (self.start)\n        common = len (os.path.commonprefix ((str (self.start), str (self.end))))\n        return str (self.start) + \'-\' + str (self.end)[common:]\n\n\n    def readings (self, prefix = None, suffix = None, delete = None):\n        # Get a list of all readings for this passage\n\n        prefix = prefix or []\n        suffix = suffix or []\n        delete = delete or []\n\n        res = execute (self.conn, """"""\n        SELECT labez\n        FROM readings\n        WHERE pass_id = :pass_id AND labez != \'zz\'\n        ORDER BY labez\n        """""", dict (parameters, pass_id = self.pass_id))\n\n        d = collections.OrderedDict ()\n        for p in prefix:\n            d[p] = p\n        for row in res:\n            d[row[0]] = row[0]\n        for s in suffix:\n            d[s] = s\n        for dd in delete:\n            if dd in d:\n                del d[dd]\n        for k in d.keys ():\n            d[k] = LABEZ_I18N.get (d[k], d[k])\n\n        Readings = collections.namedtuple (\'Readings\', \'labez labez_i18n\')\n        return [ Readings._make (r)._asdict () for r in d.items () ]\n\n\n    def cliques (self, prefix = None, suffix = None, delete = None):\n        # Get a list of all cliques for this passage\n\n        prefix = prefix or []\n        suffix = suffix or []\n        delete = delete or []\n\n        res = execute (self.conn, """"""\n        SELECT labez, clique, labez_clique (labez, clique) AS labez_clique\n        FROM cliques\n        WHERE pass_id = :pass_id\n        ORDER BY labez, clique\n        """""", dict (parameters, pass_id = self.pass_id))\n\n        Cliques = collections.namedtuple (\'Cliques\', \'labez clique labez_clique\')\n        return [ Cliques._make (r)._asdict ()\n                 for r in prefix + list (res.fetchall ()) + suffix if r not in delete ]\n\n\ndef get_locale ():\n    return flask.request.accept_languages.best_match (\'en\')\n    # return flask.request.accept_languages.best_match (LANGUAGES.keys ())\n\n\ndef cache (response):\n    response.headers[\'Cache-Control\'] = \'private, max-age=3600\'\n    return response\n\n\ndef make_json_response (json = None, status = 200, message = None):\n    d = dict (status = status)\n    if json is not None:\n        d[\'data\'] = json\n    if message is not None:\n        d[\'message\'] = message\n    return flask.make_response (flask.json.jsonify (d), status, {\n        \'content-type\' : \'application/json;charset=utf-8\',\n        \'Access-Control-Allow-Origin\' : \'*\',\n    })\n\n\ndef make_dot_response (dot, status = 200):\n    return flask.make_response (dot, status, {\n        \'content-type\' : \'text/vnd.graphviz;charset=utf-8\',\n        \'Access-Control-Allow-Origin\' : \'*\',\n    })\n\n\ndef make_csv_response (csv, status = 200):\n    return flask.make_response (csv, status, {\n        \'content-type\' : \'text/csv;charset=utf-8\',\n        \'Access-Control-Allow-Origin\' : \'*\',\n    })\n\n\ndef make_png_response (png, status = 200):\n    return flask.make_response (png, status, {\n        \'content-type\' : \'image/png\',\n        \'Access-Control-Allow-Origin\' : \'*\',\n    })\n\n\ndef make_text_response (text, status = 200):\n    return flask.make_response (text, status, {\n        \'content-type\' : \'text/plain;charset=utf-8\',\n        \'Access-Control-Allow-Origin\' : \'*\',\n    })\n\n\ndef csvify (fields, rows):\n    """""" Send a HTTP response in CSV format. """"""\n\n    return make_csv_response (to_csv (fields, rows))\n\n\nDOT_SKELETON = """"""\nstrict digraph G {{\n        graph [nodesep={nodesep},\n               ordering=out,\n               rankdir=TB,\n               ranksep={ranksep},\n               size={size:.2f},\n               fontname=""LiberationSans-Regular"", // like Arial\n               fontsize={fontsize},\n               remincross=true\n        ];\n        node [shape=ellipse,\n              height=0.3,\n              width=0.3,\n              margin=0.005\n        ];\n        edge [arrowhead=normal,\n              arrowtail=none,\n              labelangle=-15.0,\n              labeldistance=2.0,\n        ];\n""""""\n\ndef clip (lo, x, hi):\n    return max (lo, min (hi, float (x)))\n\n\ndef dot_skeleton (width = 960.0, fontsize = 10.0, ranksep = 0.4, nodesep = 0.1):\n    # We have to convert the values the browser sends (96 dpi) into\n    # values that GraphViz accepts (72 dpi).\n\n    # Why dpi = 96 ? See: https://www.w3.org/TR/css3-values/#reference-pixel\n\n    # All input to GraphViz assumes 72pt = 1 inch and 72 dpi regardless of the\n    # value of dpi. dpi is used only for bitmap and svg output.\n\n    # sanitize input\n    width    = clip (10.0, width,   1600.0)\n    fontsize = clip ( 6.0, fontsize,  72.0)\n    ranksep  = clip ( 0.0, ranksep,   10.0)\n    nodesep  = clip ( 0.0, nodesep,   10.0)\n\n    return [DOT_SKELETON.format (\n        ranksep = ranksep,\n        nodesep = nodesep,\n        size = width / 96,               # convert px => inch\n        fontsize = (fontsize / 96) * 72  # convert px => pt (72 pt = 1 inch)\n    )]\n\n\ndef nx_to_dot (graph, width = 960.0, fontsize = 10.0, nodesep = 0.1):\n    """"""Convert an nx graph into a dot file.\n\n    We\'d like to sort the nodes in the graph, but nx internally uses\n    dictionaries ""all the way down"".  Thus the only chance to sort nodes and\n    edges is while writing the file.  This function is a lightweight\n    re-implementation of nx.nx_pydot.to_pydot ().\n\n    """"""\n\n    dot = dot_skeleton (width = width, fontsize = fontsize, nodesep = nodesep)\n\n    # Copy nodes and sort them.  (Sorting nodes is important too.)\n    for n, nodedata in sorted (graph.nodes (data = True)):\n        dot.append (""\\""%s\\"" [%s];"" %\n                    (n, \',\'.join ([""\\""%s\\""=\\""%s\\"""" % (k, v) for k, v in nodedata.items ()])))\n\n    # Copy edges and sort them.\n    for u, v, edgedata in sorted (graph.edges (data = True)):\n        dot.append (""\\""%s\\"" -> \\""%s\\"" [%s];"" %\n                    (u, v, \',\'.join ([""\\""%s\\""=\\""%s\\"""" % (k, v) for k, v in edgedata.items ()])))\n\n    dot.append (\'}\\n\')\n\n    return \'\\n\'.join (dot)\n\n\ndef nx_to_dot_subgraphs (graph, field, width = 960.0, fontsize = 10.0):\n    """"""Convert an nx graph into a dot file.\n\n    We\'d like to sort the nodes in the graph, but nx internally uses\n    dictionaries ""all the way down"".  Thus the only chance to sort nodes and\n    edges is while writing the file.  This function is a lightweight\n    re-implementation of nx.nx_pydot.to_pydot ().\n\n    """"""\n\n    dot = dot_skeleton (width = width, fontsize = fontsize, ranksep = 1.2)\n\n    # Copy nodes and sort them.  (Sorting nodes is important too.)\n    sorted_nodes = sorted (graph, key = lambda n: (graph.nodes[n][field], graph.nodes[n][\'hsnr\']))\n    for key, nodes_for_key in itertools.groupby (sorted_nodes,\n                                                 key = lambda n: graph.nodes[n][field]):\n        dot.append (""subgraph \\""cluster_%s\\"" {"" % key)\n        dot.append (""style=rounded"")\n        dot.append (""labeljust=l"")\n        dot.append (""labelloc=c"")\n        dot.append (""rank=%s"" % (\'source\' if key in (\'a\', \'a1\') else \'same\'))\n        dot.append (""label=\\""%s\\"""" % key)\n        for n in nodes_for_key:\n            attr = graph.nodes[n]\n            dot.append (""\\""%s\\"" [%s];"" %\n                        (n, \',\'.join ([""\\""%s\\""=\\""%s\\"""" % (k, v) for k, v in attr.items ()])))\n        dot.append (""}"")\n\n    # Copy edges and sort them.\n    for u, v, edgedata in sorted (graph.edges (data = True)):\n        dot.append (""\\""%s\\"" -> \\""%s\\"" [%s];"" %\n                    (u, v, \',\'.join ([""\\""%s\\""=\\""%s\\"""" % (k, v) for k, v in edgedata.items ()])))\n\n    dot.append (\'}\\n\')\n\n    return \'\\n\'.join (dot)\n'"
server/info.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""An application server for CBGM.  Root info endpoint. """"""\n\nimport collections\n\nimport flask\nfrom flask import current_app\nimport flask_login\n\nfrom helpers import make_json_response\nfrom login import user_can_read, user_can_write\n\nbp = flask.Blueprint (\'info\', __name__)\n\ninstances = collections.OrderedDict ()\n\ndef init_app (app, instances_):\n    """""" Initialize the flask app. """"""\n\n    global instances\n    instances.update (instances_)\n\n\n@bp.route (\'/user.json\')\ndef user_json ():\n    """"""Endpoint.  Serve information about the current user.""""""\n\n    user = flask_login.current_user\n    logged_in = user.is_authenticated\n    roles = [\'public\']\n    if logged_in:\n        roles += [r.name for r in user.roles]\n\n    return make_json_response ({\n        \'username\'  : user.username if logged_in else \'anonymous\',\n        \'roles\'     : roles,\n        \'can_login\' : current_app.config[\'AFTER_LOGIN_URL\'] is not None\n    })\n\n\n@bp.route (\'/info.json\')\ndef index ():\n    """"""Endpoint.  Serve general info about all registered apps.""""""\n\n    def copy (a):\n        return {\n            \'application_name\'        : a.config[\'APPLICATION_NAME\'],\n            \'application_root\'        : a.config[\'APPLICATION_DIR\'].rstrip (\'/\') + \'/\',\n            \'application_description\' : a.config[\'APPLICATION_DESCRIPTION\'],\n            \'user_can_write\'          : user_can_write (a),\n        }\n\n    apps = sorted (instances.values (), key = lambda a: a.config[\'APPLICATION_NAME\'])\n\n    return make_json_response ({\n        \'instances\' : [copy (app) for app in apps if user_can_read (app)],\n    })\n\n\n@bp.route (\'/messages.json\')\ndef messages_json ():\n    """"""Endpoint.  Serve the flashed messages.""""""\n\n    return make_json_response ({\n        \'messages\' : [\n            {\n                \'message\'  : m[1],\n                \'category\' : m[0],\n            }\n            for m in (flask.get_flashed_messages (with_categories = True) or [])\n        ]\n    })\n'"
server/login.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""An application server for CBGM.  User management module.  """"""\n\nimport urllib\n\nimport flask\nfrom flask import make_response, current_app\nfrom flask_user import UserMixin\nimport flask_login\n\nfrom ntg_common import db as dbx\nfrom ntg_common.exceptions import PrivilegeError\nfrom helpers import make_json_response\n\n\nbp = flask.Blueprint (\'login\', __name__)\n\n\ndef init_app (app):\n    """""" Initialize the flask app. """"""\n\n    app.config[\'USER_AFTER_LOGIN_ENDPOINT\']  = \'login.after_login\'\n    app.config[\'USER_AFTER_LOGOUT_ENDPOINT\'] = \'login.after_login\'\n\n\ndef user_can_read (app):\n    """""" Return True if user has read access. """"""\n\n    read_access = app.config[\'READ_ACCESS\']\n\n    if read_access == \'public\':\n        return True\n\n    return flask_login.current_user.has_role (read_access)\n\n\ndef user_can_read_private (app):\n    """""" Return True if user has read access. """"""\n\n    read_access_private = app.config[\'READ_ACCESS_PRIVATE\']\n\n    if read_access_private == \'public\':\n        return True\n\n    return flask_login.current_user.has_role (read_access_private)\n\n\ndef user_can_write (app):\n    """""" Return True if user has write access. """"""\n\n    write_access = app.config[\'WRITE_ACCESS\']\n\n    if write_access == \'public\':\n        return True\n\n    return flask_login.current_user.has_role (write_access)\n\n\ndef auth ():\n    """""" Check if user is authorized to see what follows. """"""\n\n    if not user_can_read (current_app):\n        read_access = current_app.config[\'READ_ACCESS\']\n        raise PrivilegeError (\'You don\\\'t have %s privilege.\' % read_access)\n\n\ndef private_auth ():\n    """""" Check if user is authorized to see what follows. """"""\n\n    if not user_can_read_private (current_app):\n        read_access_private = current_app.config[\'READ_ACCESS_PRIVATE\']\n        raise PrivilegeError (\'You don\\\'t have %s privilege.\' % read_access_private)\n\n\ndef edit_auth ():\n    """""" Check if user is authorized to edit. """"""\n\n    if not user_can_write (current_app):\n        write_access = current_app.config[\'WRITE_ACCESS\']\n        raise PrivilegeError (\'You don\\\'t have %s privilege.\' % write_access)\n\n\ndef make_safe_url (url):\n    """"""Turns an unsafe absolute URL into a safe relative URL\n    by removing the scheme and the hostname\n\n    Example: make_safe_url(\'http://hostname/path1/path2?q1=v1&q2=v2#fragment\')\n             returns: \'/path1/path2?q1=v1&q2=v2#fragment\n\n    Copied from flask_user/views.py because it was defective.\n    """"""\n\n    parts = urllib.parse.urlsplit (url)\n    return urllib.parse.urlunsplit ( (\'\', \'\', parts[2], parts[3], parts[4]) )\n\n\ndef declare_user_model_on (db): # db = flask_sqlalchemy.SQLAlchemy ()\n    """""" Declare the user model on flask_sqlalchemy. """"""\n\n    # global User, Role, Roles_Users\n    # pylint: disable=protected-access\n\n    class User (db.Model, dbx._User, UserMixin):\n        __tablename__ = \'user\'\n\n        roles = db.relationship (\n            \'Role\',\n            secondary = \'roles_users\',\n            backref = db.backref (\'users\', lazy=\'dynamic\')\n        )\n\n    class Role (db.Model, dbx._Role):\n        __tablename__ = \'role\'\n\n    class Roles_Users (db.Model, dbx._Roles_Users):\n        __tablename__ = \'roles_users\'\n\n    return User, Role, Roles_Users\n\n\nclass AnonymousUserMixin (flask_login.AnonymousUserMixin):\n    \'\'\'\n    This is the default object for representing an anonymous user.\n    \'\'\'\n\n    def __init__ (self):\n        self.id = 666\n\n    def has_role (self, *_specified_role_names):\n        return False\n\n\n@bp.route (\'/user/after_login\')\ndef after_login ():\n    return flask.redirect (current_app.config[\'AFTER_LOGIN_URL\'])\n'"
server/main.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""The API server for CBGM.  The main functions.""""""\n\nimport collections\nimport logging\nimport re\n\nimport flask\nfrom flask import request, current_app\nimport flask_login\n\nfrom ntg_common.db_tools import execute\nfrom ntg_common import tools\n\nfrom login import auth\nfrom helpers import parameters, Passage, Manuscript, cache, csvify, get_excluded_ms_ids, \\\n     make_json_response\n\nbp = flask.Blueprint (\'main\', __name__)\n\n\ndef init_app (app):\n    """""" Initialize the flask app. """"""\n\n    app.config.bk_id = None\n    app.config.rg_id_all = None\n\n    with app.config.dba.engine.begin () as conn:\n        try:\n            res = execute (conn, """"""\n            SELECT bk_id\n            FROM books\n            WHERE book = :book\n            """""", { \'book\' : app.config[\'BOOK\'] })\n            app.config.bk_id = res.fetchone ()[0]\n\n            res = execute (conn, """"""\n            SELECT rg_id\n            FROM ranges\n            WHERE bk_id = :bk_id AND range = \'All\'\n            """""", { \'bk_id\' : app.config.bk_id })\n            rg_id = res.fetchone ()[0]\n\n            app.config.rg_id_all = rg_id\n        except:\n            pass # FIXME\n\n\ndef _f_map_word (t):\n    """"""Helper function for the :func:`suggest_json` function.\n\n    Formats the entry of the last field of the navigation gadget.\n    """"""\n\n    if t.kapanf != t.kapend:\n        t2 = ""%s-%s:%s/%s"" % (t.wortanf, t.kapend, t.versend, t.wortend)\n    elif t.versanf != t.versend:\n        t2 = ""%s-%s/%s"" % (t.wortanf, t.versend, t.wortend)\n    elif t.wortanf != t.wortend:\n        t2 = ""%s-%s"" % (t.wortanf, t.wortend)\n    else:\n        t2 = ""%s"" % t.wortanf\n    return [t2, t2, t.lemma]\n\n\n@bp.route (\'/application.json\')\ndef application_json ():\n    """"""Endpoint.  Serve information about the application.""""""\n\n    conf = current_app.config\n\n    return make_json_response ({\n        \'name\'                : conf[\'APPLICATION_NAME\'],\n        \'root\'                : conf[\'APPLICATION_ROOT\'],\n        \'read_access\'         : conf[\'READ_ACCESS\'],\n        \'read_access_private\' : conf[\'READ_ACCESS_PRIVATE\'],\n        \'write_access\'        : conf[\'WRITE_ACCESS\'],\n        \'start\'               : conf[\'SERVER_START_TIME\'],\n        \'rg_id_all\'           : conf.rg_id_all,\n    })\n\n\n@bp.route (\'/ranges.json/\')\ndef ranges_json ():\n    """"""Endpoint.  Serve a list of ranges.\n\n    Serves a list of the configured ranges that are contained inside a book in\n    the NT.\n\n    """"""\n\n    conf = current_app.config\n    with conf.dba.engine.begin () as conn:\n        res = execute (conn, """"""\n        SELECT DISTINCT bk_id, book, rg_id, range, lower (rg.passage) as begadr, upper (rg.passage) as endadr\n        FROM ranges_view rg\n        WHERE bk_id = :bk_id\n        ORDER BY begadr, endadr DESC\n        """""", dict (parameters, bk_id = conf.bk_id))\n\n        Ranges = collections.namedtuple (\'Ranges\', \'bk_id, book, rg_id, range, begadr, endadr\')\n        ranges = [ Ranges._make (r)._asdict () for r in res ]\n\n        return cache (make_json_response (ranges))\n\n\n@bp.route (\'/passage.json/\')\n@bp.route (\'/passage.json/<passage_or_id>\')\ndef passage_json (passage_or_id = None):\n    """"""Endpoint.  Serve information about a passage.\n\n    Return information about a passage or navigate to it.\n\n    :param string passage_or_id: The passage id.\n    :param string siglum:        The siglum of the book to navigate to.\n    :param string chapter:       The chapter to navigate to.\n    :param string verse:         The verse to navigate to.\n    :param string word:          The word (range) to navigate to.\n    :param string button:        The button pressed.\n\n    """"""\n\n    auth ()\n\n    passage_or_id = request.args.get (\'pass_id\') or passage_or_id or \'0\'\n\n    siglum  = request.args.get (\'siglum\')\n    chapter = request.args.get (\'chapter\')\n    verse   = request.args.get (\'verse\')\n    word    = request.args.get (\'word\')\n    button  = request.args.get (\'button\')\n\n    with current_app.config.dba.engine.begin () as conn:\n        if siglum and chapter and verse and word and button == \'Go\':\n            parsed_passage = Passage.parse (""%s %s:%s/%s"" % (siglum, chapter, verse, word))\n            passage = Passage (conn, parsed_passage)\n            return make_json_response (passage.to_json ())\n\n        if button in (\'-1\', \'1\'):\n            passage = Passage (conn, passage_or_id)\n            passage = Passage (conn, int (passage.pass_id) + int (button))\n            return make_json_response (passage.to_json ())\n\n        passage = Passage (conn, passage_or_id)\n        return cache (make_json_response (passage.to_json ()))\n\n\n@bp.route (\'/readings.json/<passage_or_id>\')\ndef readings_json (passage_or_id):\n    """""" Endpoint.  Serve all readings found in a passage.\n\n    :param string passage_or_id: The passage id.\n\n    """"""\n\n    auth ()\n\n    with current_app.config.dba.engine.begin () as conn:\n        passage = Passage (conn, passage_or_id)\n        return cache (make_json_response (passage.readings ()))\n\n\n@bp.route (\'/cliques.json/<passage_or_id>\')\ndef cliques_json (passage_or_id):\n    """""" Endpoint.  Serve all cliques found in a passage.\n\n    :param string passage_or_id: The passage id.\n\n    """"""\n\n    auth ()\n\n    with current_app.config.dba.engine.begin () as conn:\n        passage = Passage (conn, passage_or_id)\n        return make_json_response (passage.cliques ())\n\n\n@bp.route (\'/leitzeile.json/<passage_or_id>\')\ndef leitzeile_json (passage_or_id):\n    """"""Endpoint.  Serve the leitzeile for the verse containing passage_or_id. """"""\n\n    auth ()\n\n    with current_app.config.dba.engine.begin () as conn:\n        passage = Passage (conn, passage_or_id)\n        verse_start = (passage.start // 1000) * 1000\n        verse_end = verse_start + 999\n\n        res = execute (conn, """"""\n        SELECT l.begadr, l.endadr, l.lemma, ARRAY_AGG (p.pass_id)\n        FROM nestle l\n          LEFT JOIN passages p ON (p.passage @> l.passage)\n        WHERE int4range (:start, :end + 1) @> l.passage\n        GROUP BY l.begadr, l.endadr, l.lemma\n\n        UNION -- get the insertions\n\n        SELECT p.begadr, p.endadr, \'\', ARRAY_AGG (p.pass_id)\n        FROM passages_view p\n        WHERE int4range (:start, :end + 1) @> p.passage AND (begadr % 2) = 1\n        GROUP BY p.begadr, p.endadr\n\n        ORDER BY begadr, endadr DESC\n        """""", dict (parameters, start = verse_start, end = verse_end))\n\n        Leitzeile = collections.namedtuple (\n            \'Leitzeile\', \'begadr, endadr, lemma, pass_ids\')\n        leitzeile = [ Leitzeile._make (r)._asdict () for r in res ]\n\n        return make_json_response (leitzeile)\n\n\n@bp.route (\'/suggest.json\')\ndef suggest_json ():\n    """"""Endpoint.  The suggestion drop-downs in the navigator.\n\n    Serves a list of books, chapters, verses, or words that the user can enter\n    in the navigation gadget.  It suggests only entities that are actually in\n    the database.\n\n    """"""\n\n    auth ()\n\n    # the name of the current field\n    field   = request.args.get (\'currentfield\')\n\n    # the term the user entered in the current field\n    term    = request.args.get (\'term\') or \'\'\n    term    = \'^\' + re.escape (term.split (\'-\')[0])\n\n    # terms entered in previous fields\n    siglum  = request.args.get (\'siglum\')  or \'\'\n    chapter = request.args.get (\'chapter\') or 0\n    verse   = request.args.get (\'verse\')   or 0\n\n    Words = collections.namedtuple (\n        \'Words\', \'kapanf, versanf, wortanf, kapend, versend, wortend, lemma\')\n\n    res = []\n    with current_app.config.dba.engine.begin () as conn:\n\n        if field == \'siglum\':\n            # only show those books that actually are in the database\n            res = execute (conn, """"""\n            SELECT DISTINCT siglum, siglum, book, bk_id\n            FROM passages_view b\n            WHERE siglum ~ :term OR book ~ :term\n            ORDER BY bk_id\n            """""", dict (parameters, term = term))\n            return flask.json.jsonify (\n                [ { \'value\' : r[0], \'label\' : r[1], \'description\' : r[2] } for r in res ])\n\n        if field == \'chapter\':\n            res = execute (conn, """"""\n            SELECT range, range\n            FROM ranges_view\n            WHERE siglum = :siglum AND range ~ \'[1-9][0-9]*\' AND range ~ :term\n            ORDER BY range::integer\n            """""", dict (parameters, siglum = siglum, term = term))\n            return flask.json.jsonify ([ { \'value\' : r[0], \'label\' : r[1] } for r in res ])\n\n        if field == \'verse\':\n            res = execute (conn, """"""\n            SELECT DISTINCT verse, verse\n            FROM passages_view\n            WHERE variant AND siglum = :siglum AND chapter = :chapter AND verse::varchar ~ :term\n            ORDER BY verse\n            """""", dict (parameters, siglum = siglum, chapter = chapter, term = term))\n            return flask.json.jsonify ([ { \'value\' : r[0], \'label\' : r[1] } for r in res ])\n\n        if field == \'word\':\n            res = execute (conn, """"""\n            SELECT chapter, verse, word,\n                            adr2chapter (p.endadr), adr2verse (p.endadr), adr2word (p.endadr),\n                            COALESCE (string_agg (n.lemma, \' \' ORDER BY n.begadr), \'\') as lemma\n            FROM passages_view p\n            LEFT JOIN nestle n\n              ON (p.passage @> n.passage)\n            WHERE variant AND siglum = :siglum AND chapter = :chapter AND verse = :verse AND word::varchar ~ :term\n            GROUP BY chapter, verse, word, p.endadr\n            ORDER BY word, adr2verse (p.endadr), adr2word (p.endadr)\n            """""", dict (parameters, siglum = siglum, chapter = chapter, verse = verse, term = term))\n            res = map (Words._make, res)\n            res = map (_f_map_word, res)\n            return flask.json.jsonify (\n                [ { \'value\' : r[0], \'label\' : r[1], \'description\' : r[2] } for r in res ])\n\n    return flask.json.jsonify ([])\n\n\n@bp.route (\'/manuscript.json/<hs_hsnr_id>\')\ndef manuscript_json (hs_hsnr_id):\n    """"""Endpoint.  Serve information about a manuscript.\n\n    :param string hs_hsnr_id: The hs, hsnr or id of the manuscript.\n\n    """"""\n\n    auth ()\n\n    hs_hsnr_id = request.args.get (\'ms_id\') or hs_hsnr_id\n\n    with current_app.config.dba.engine.begin () as conn:\n        ms = Manuscript (conn, hs_hsnr_id)\n        return make_json_response (ms.to_json ())\n\n\n@bp.route (\'/manuscript-full.json/<passage_or_id>/<hs_hsnr_id>\')\ndef manuscript_full_json (passage_or_id, hs_hsnr_id):\n    """"""Endpoint.  Serve information about a manuscript.\n\n    :param string hs_hsnr_id: The hs, hsnr or id of the manuscript.\n\n    """"""\n\n    auth ()\n\n    hs_hsnr_id = request.args.get (\'ms_id\') or hs_hsnr_id\n\n    with current_app.config.dba.engine.begin () as conn:\n        passage   = Passage (conn, passage_or_id)\n        ms        = Manuscript (conn, hs_hsnr_id)\n        rg_id     = passage.request_rg_id (request)\n\n        if ms.ms_id is None:\n            return cache (make_json_response (None, 400, \'Bad request: No such manuscript.\'))\n\n        json = ms.to_json ()\n        json[\'length\'] = ms.get_length (rg_id)\n\n        # Get the attestation(s) of the manuscript (may be uncertain eg. a/b/c)\n        res = execute (conn, """"""\n        SELECT labez, clique, labez_clique, certainty\n        FROM apparatus_view_agg\n        WHERE ms_id = :ms_id AND pass_id = :pass_id\n        """""", dict (parameters, ms_id = ms.ms_id, pass_id = passage.pass_id))\n\n        row = res.fetchone ()\n        if row is not None:\n            json[\'labez\'], json[\'clique\'], json[\'labez_clique\'], json[\'certainty\'] = row\n\n        # Get the affinity of the manuscript to all manuscripts\n        res = execute (conn, """"""\n        SELECT avg (a.affinity) as aa,\n        percentile_cont(0.5) WITHIN GROUP (ORDER BY a.affinity) as ma\n        FROM affinity a\n        WHERE a.ms_id1 = :ms_id1 AND a.rg_id = :rg_id\n        """""", dict (parameters, ms_id1 = ms.ms_id, rg_id = rg_id))\n\n        json[\'aa\'], json[\'ma\'] = 0.0, 0.0\n        row = res.fetchone ()\n        if row is not None:\n            json[\'aa\'], json[\'ma\'] = row\n\n        # Get the affinity of the manuscript to MT\n        #\n        # For a description of mt and mtp see the comment in\n        # ActsMsListValPh3.pl and\n        # http://intf.uni-muenster.de/cbgm/actsPh3/guide_en.html#Ancestors\n\n        res = execute (conn, """"""\n        SELECT a.affinity as mt, a.equal::float / c.length as mtp\n        FROM affinity a\n        JOIN ms_ranges c\n          ON (a.ms_id1, a.rg_id) = (c.ms_id, c.rg_id)\n        WHERE a.ms_id1 = :ms_id1 AND a.ms_id2 = 2 AND a.rg_id = :rg_id\n        """""", dict (parameters, ms_id1 = ms.ms_id, rg_id = rg_id))\n\n        json[\'mt\'], json[\'mtp\'] = 0.0, 0.0\n        row = res.fetchone ()\n        if row is not None:\n            json[\'mt\'], json[\'mtp\'] = row\n\n        return cache (make_json_response (json))\n\n\n@bp.route (\'/relatives.csv/<passage_or_id>/<hs_hsnr_id>\')\ndef relatives_csv (passage_or_id, hs_hsnr_id):\n    """"""Output a table of the nearest relatives of a manuscript.\n\n    Output a table of the nearest relatives/ancestors/descendants of a\n    manuscript and what they attest.\n\n    """"""\n\n    auth ()\n\n    type_     = request.args.get (\'type\') or \'rel\'\n    limit     = int (request.args.get (\'limit\') or 0)\n    labez     = request.args.get (\'labez\') or \'all\'\n    mode      = request.args.get (\'mode\') or \'sim\'\n    include   = request.args.getlist (\'include[]\') or []\n    fragments = request.args.getlist (\'fragments[]\') or []\n\n    view = \'affinity_view\' if mode == \'rec\' else \'affinity_p_view\'\n\n    where = \'\'\n    if type_ == \'anc\':\n        where =  \' AND older < newer\'\n    if type_ == \'des\':\n        where =  \' AND older >= newer\'\n\n    if labez == \'all\':\n        where += "" AND labez !~ \'^z\'""\n    elif labez == \'all+lac\':\n        pass\n    else:\n        where += "" AND labez = \'%s\'"" % labez\n\n    if \'fragments\' in fragments:\n        frag_where = \'\'\n    else:\n        frag_where = \'AND aff.common > aff.ms1_length / 2\'\n\n    limit = \'\' if limit == 0 else \' LIMIT %d\' % limit\n\n    with current_app.config.dba.engine.begin () as conn:\n\n        passage   = Passage (conn, passage_or_id)\n        ms        = Manuscript (conn, hs_hsnr_id)\n        rg_id     = passage.request_rg_id (request)\n\n        exclude = get_excluded_ms_ids (conn, include)\n\n        # Get the X most similar manuscripts and their attestations\n        res = execute (conn, """"""\n        /* get the LIMIT closest ancestors for this node */\n        WITH ranks AS (\n          SELECT ms_id1, ms_id2,\n            rank () OVER (ORDER BY affinity DESC, common, older, newer DESC, ms_id2) AS rank,\n            affinity\n          FROM {view} aff\n          WHERE ms_id1 = :ms_id1 AND aff.rg_id = :rg_id AND ms_id2 NOT IN :exclude\n            AND newer > older {frag_where}\n          ORDER BY affinity DESC\n        )\n\n        SELECT r.rank,\n               aff.ms_id2 as ms_id,\n               ms.hs,\n               ms.hsnr,\n               aff.ms2_length,\n               aff.common,\n               aff.equal,\n               aff.older,\n               aff.newer,\n               aff.unclear,\n               aff.common - aff.equal - aff.older - aff.newer - aff.unclear as norel,\n               CASE WHEN aff.newer < aff.older THEN \'\'\n                    WHEN aff.newer = aff.older THEN \'-\'\n                    ELSE \'>\'\n               END as direction,\n               aff.affinity,\n               a.labez,\n               a.certainty\n        FROM\n          {view} aff\n        JOIN apparatus_view_agg a\n          ON aff.ms_id2 = a.ms_id\n        JOIN manuscripts ms\n          ON aff.ms_id2 = ms.ms_id\n        LEFT JOIN ranks r\n          ON r.ms_id2 = aff.ms_id2\n        WHERE aff.ms_id2 NOT IN :exclude AND aff.ms_id1 = :ms_id1\n              AND aff.rg_id = :rg_id AND aff.common > 0\n              AND a.pass_id = :pass_id {where} {frag_where}\n        ORDER BY affinity DESC, r.rank, newer DESC, older DESC, hsnr\n        {limit}\n        """""", dict (parameters, where = where, frag_where = frag_where,\n                   ms_id1 = ms.ms_id, hsnr = ms.hsnr,\n                   pass_id = passage.pass_id, rg_id = rg_id, limit = limit,\n                   view = view, exclude = exclude))\n\n        Relatives = collections.namedtuple (\n            \'Relatives\',\n            \'rank ms_id hs hsnr length common equal older newer unclear norel direction affinity labez certainty\'\n        )\n        return csvify (Relatives._fields, list (map (Relatives._make, res)))\n\n\n@bp.route (\'/apparatus.json/<passage_or_id>\')\ndef apparatus_json (passage_or_id):\n    """""" The contents of the apparatus table. """"""\n\n    auth ()\n\n    with current_app.config.dba.engine.begin () as conn:\n        passage = Passage (conn, passage_or_id)\n\n        # list of labez => lesart\n        res = execute (conn, """"""\n        SELECT labez, reading (labez, lesart)\n        FROM readings\n        WHERE pass_id = :pass_id\n        ORDER BY labez\n        """""", dict (parameters, pass_id = passage.pass_id))\n\n        Readings = collections.namedtuple (\'Readings\', \'labez lesart\')\n        readings = [ Readings._make (r)._asdict () for r in res ]\n\n        # list of labez_clique => manuscripts\n        res = execute (conn, """"""\n        SELECT labez, clique, labez_clique, labezsuf, reading (labez, lesart), ms_id, hs, hsnr, certainty\n        FROM apparatus_view_agg\n        WHERE pass_id = :pass_id\n        ORDER BY hsnr, labez, clique\n        """""", dict (parameters, pass_id = passage.pass_id))\n\n        Manuscripts = collections.namedtuple (\n            \'Manuscripts\',\n            \'labez clique labez_clique labezsuf lesart ms_id hs hsnr certainty\'\n        )\n        manuscripts = [ Manuscripts._make (r)._asdict () for r in res ]\n\n        return make_json_response ({\n            \'readings\'    : readings,\n            \'manuscripts\' : manuscripts,\n        })\n\n    return \'Error\'\n\n\n@bp.route (\'/attestation.json/<passage_or_id>\')\ndef attestation_json (passage_or_id):\n    """"""Answer with a list of the attestations of all manuscripts at one specified\n    passage.""""""\n\n    auth ()\n\n    with current_app.config.dba.engine.begin () as conn:\n        passage = Passage (conn, passage_or_id)\n\n        res = execute (conn, """"""\n        SELECT ms_id, labez\n        FROM apparatus\n        WHERE pass_id = :pass_id\n        ORDER BY ms_id\n        """""", dict (parameters, pass_id = passage.pass_id))\n\n        attestations = {}\n        for row in res:\n            ms_id, labez = row\n            attestations[str (ms_id)] = labez\n\n        return make_json_response ({\n            \'attestations\': attestations\n        })\n\n\n@bp.route (\'/attesting.csv/<passage_or_id>/<labez>\')\ndef attesting_csv (passage_or_id, labez):\n    """""" Serve all relatives of all mss. attesting labez at passage. """"""\n\n    auth ()\n\n    with current_app.config.dba.engine.begin () as conn:\n        passage = Passage (conn, passage_or_id)\n\n        res = execute (conn, """"""\n        SELECT ms_id, hs, hsnr\n        FROM apparatus_view\n        WHERE pass_id = :pass_id AND labez = :labez\n        ORDER BY hsnr\n        """""", dict (parameters, pass_id = passage.pass_id, labez = labez))\n\n        Attesting = collections.namedtuple (\'Attesting\', \'ms_id hs hsnr\')\n\n        return csvify (Attesting._fields, list (map (Attesting._make, res)))\n'"
server/set_cover.py,40,"b'# -*- encoding: utf-8 -*-\n\n""""""The API server for CBGM.  The optimal substemma and set cover module.\n\nSee: CBGM_Pres.pdf p. 490ff.\n\n""""""\n\nimport collections\nimport itertools\n\nimport flask\nfrom flask import request, current_app\n\nimport numpy as np\n\nfrom ntg_common.db_tools import execute\nfrom ntg_common.cbgm_common import CBGM_Params, create_labez_matrix\n\nfrom helpers import Passage, Manuscript, make_json_response, csvify\n\n\nMAX_COVER_SIZE = 12\n\n\nbp = flask.Blueprint (\'set_cover\', __name__)\n\n\ndef get_ancestors (conn, rg_id, ms_id):\n    """""" Get all ancestors of ms. """"""\n\n    mode = \'sim\'\n    view = \'affinity_view\' if mode == \'rec\' else \'affinity_p_view\'\n\n    res = execute (conn, """"""\n    SELECT aff.ms_id2 as ms_id\n    FROM\n      {view} aff\n    WHERE aff.ms_id1 = :ms_id1 AND aff.rg_id = :rg_id\n          AND aff.common > 0 AND aff.older < aff.newer\n    ORDER BY affinity DESC, newer DESC, older DESC\n    """""", dict (ms_id1  = ms_id,\n               rg_id   = rg_id,\n               view    = view))\n\n    return frozenset ([r[0] for r in res])\n\n\ndef powerset (iterable):\n    """"""powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)""""""\n    s = list (iterable)\n    return itertools.chain.from_iterable (\n        itertools.combinations (s, r) for r in range (len (s) + 1))\n\n\nWITH_SELECT = """"""\n  SELECT pass_id, labez, clique,\n         (1 << (ROW_NUMBER () OVER (PARTITION BY pass_id ORDER BY labez, clique)::integer)) AS rn\n  FROM cliques\n  WHERE labez !~ \'^z\'\n""""""\n\ndef init (db):\n    """""" Do some preparative calculations and cache the results. """"""\n\n    val = CBGM_Params ()\n\n    with db.engine.begin () as conn:\n        # get max number of different cliques in any one passage\n        res = execute (conn, """"""\n        SELECT MAX (c)\n        FROM (\n          SELECT COUNT ((labez, clique)) AS c\n          FROM locstem\n          WHERE labez !~ \'^z\'\n          GROUP BY pass_id\n        ) AS foo\n        """""", {})\n        n_cliques = res.fetchone ()[0]\n        # see that the bitmask fits into uint64\n        # one bit is reserved for \'unknown\' derivation\n        assert n_cliques < 64\n\n        # load all attestations into one big numpy array\n        create_labez_matrix (db, {}, val)\n\n        # build a mask of all readings of all mss.\n        # every labez_clique gets an id (in the range 1..63)\n\n        # Matrix mss x passages containing the bitmask of all manuscripts readings\n        val.mask_matrix = np.zeros ((val.n_mss, val.n_passages), dtype = np.uint64)\n\n        res = execute (conn, """"""\n        WITH rn AS (\n          {with}\n        )\n        SELECT msq.ms_id, msq.pass_id, rn1.rn\n        FROM ms_cliques AS msq\n        JOIN (select * from rn) as rn1\n          USING (pass_id, labez, clique)\n        """""", { \'with\' : WITH_SELECT })\n\n        mask_row = collections.namedtuple (\'Mask_Row\', \'ms_id, pass_id, shift\')\n        for r in res:\n            mask = mask_row._make (r)\n            val.mask_matrix[mask.ms_id - 1, mask.pass_id - 1] = np.uint64 (mask.shift)\n\n    return val\n\n\ndef build_explain_matrix (conn, val, ms_id):\n    """"""Build the explain matrix.\n\n    A matrix of 1 x n_passages containing the bitmask of all those readings that\n    would explain the reading in the manuscript under scrutiny.\n\n    Bit 1 means: the reading stems from an unknown source.\n    Bit 2..64 are the bitmask of all cliques.\n\n    """"""\n\n    explain_matrix = np.zeros (val.n_passages, dtype = np.uint64)\n\n    res = execute (conn, """"""\n    WITH RECURSIVE\n    rn AS (\n      {with}\n    ),\n    lsrn AS (\n      SELECT ls.pass_id, ls.labez, ls.clique,\n        -- set 1 as flag for unknown derivation\n        CASE WHEN ls.source_labez = \'?\' THEN rn1.rn | 1 ELSE rn1.rn END AS rn1,\n        rn2.rn AS rn2\n      FROM locstem ls\n      JOIN rn as rn1\n        USING (pass_id, labez, clique)\n      LEFT JOIN rn as rn2\n        ON (ls.pass_id, ls.source_labez, ls.source_clique) = (rn2.pass_id, rn2.labez, rn2.clique)\n    ),\n    lsrec (pass_id, rn1, rn2) AS (\n      SELECT lsrn.pass_id, lsrn.rn1, lsrn.rn2\n      FROM ms_cliques AS msq\n        JOIN lsrn USING (pass_id, labez, clique)\n      WHERE ms_id = :ms_id\n    UNION\n      SELECT lsrn.pass_id, lsrn.rn1, lsrn.rn2\n      FROM lsrec\n      JOIN lsrn\n        ON (lsrn.pass_id = lsrec.pass_id AND (lsrn.rn1 & ~B\'1\'::integer) = lsrec.rn2)\n    )\n    SELECT pass_id, BIT_OR (rn1) AS rn\n    FROM lsrec\n    GROUP BY pass_id\n    ORDER BY pass_id;\n    """""", { \'with\' : WITH_SELECT, \'ms_id\' : ms_id })\n\n    explain_row = collections.namedtuple (\'Explain_Row\', \'pass_id, mask\')\n    for r in res:\n        mask = explain_row._make (r)\n        explain_matrix[mask.pass_id - 1] = np.uint64 (mask.mask)\n\n    return explain_matrix\n\n\ndef init_app (app):\n    """""" Init the Flask app. """"""\n\n    app.config.val = None\n\n\n@bp.route (\'/set-cover.json/<hs_hsnr_id>\')\ndef set_cover_json (hs_hsnr_id):\n    """""" Approximate the minimum set cover for a manuscript.\n\n    See: https://en.wikipedia.org/wiki/Set_cover_problem\n    """"""\n\n    include    = request.args.getlist (\'include[]\') or []\n    pre_select = (request.args.get (\'pre_select\') or \'\').split ()\n    response   = {}\n\n    with current_app.config.dba.engine.begin () as conn:\n        if current_app.config.val is None:\n            current_app.config.val = init (current_app.config.dba)\n        val = current_app.config.val\n\n        cover = []\n\n        ms = Manuscript (conn, hs_hsnr_id)\n        response[\'ms\'] = ms.to_json ()\n        ms_id = ms.ms_id - 1  # numpy indices start at 0\n\n        # allow user to pre-select a set of manuscripts\n        pre_selected = [ Manuscript (conn, anc_id) for anc_id in pre_select ]\n        response[\'mss\'] = [s.to_json () for s in pre_selected]\n\n        np.set_printoptions (edgeitems = 8, linewidth = 100)\n\n        # The mss x passages boolean matrix that is TRUE whenever the inspected\n        # ms. and the source ms. are both defined.\n        b_common = np.logical_and (val.def_matrix, val.def_matrix[ms_id])\n\n        # Remove mss. we don\'t want to compare\n        b_common[ms_id] = False  # don\'t find original ms.\n        if \'A\' not in include and \'A\' not in pre_select:\n            b_common[0] = False\n        if \'MT\' not in include and \'MT\' not in pre_select:\n            b_common[1] = False\n        # also eliminate all descendants\n        ancestors = get_ancestors (conn, current_app.config.rg_id_all, ms.ms_id)\n        for i in range (0, val.n_mss):\n            if (i + 1) not in ancestors:\n                b_common[i] = False\n\n        n_defined = np.count_nonzero (val.def_matrix[ms_id])\n        response[\'ms\'][\'open\'] = n_defined\n\n        # mask_matrix ist the mss x passages matrix containing the bitmask of\n        # all readings\n        explain_matrix       = build_explain_matrix (conn, val, ms.ms_id)\n        explain_equal_matrix = val.mask_matrix[ms_id]\n\n        # The mss x passages boolean matrix that is TRUE whenever the inspected ms.\n        # agrees with the potential source ms.\n        b_equal = np.bitwise_and (val.mask_matrix, explain_equal_matrix) > 0\n        b_equal = np.logical_and (b_equal, b_common)\n\n        # The mss x passages boolean matrix that is TRUE whenever the inspected ms.\n        # agrees with the potential source ms. or is posterior to it.\n        b_post = np.bitwise_and (val.mask_matrix, explain_matrix) > 0\n        b_post = np.logical_and (b_post, b_common)\n\n        # The 1 x passages boolean matrix that is TRUE whenever the passage is\n        # still unexplained.\n        b_open = np.copy (val.def_matrix[ms_id])\n\n        # The 1 x passages boolean matrix that is TRUE whenever the source of\n        # the reading in the inspected ms. is unknown\n        b_unknown = np.bitwise_and (explain_matrix, 0x1)\n        b_unknown = np.logical_and (b_unknown, b_open)\n\n        n_explained = 0\n\n        for n in range (0, MAX_COVER_SIZE):\n            if n < len (pre_selected):\n                # use manuscript pre-selected by user\n                ms_id_most_similar = pre_selected[n].ms_id - 1\n            else:\n                # find manuscript that explains the most passages by agreement\n                ms_id_most_similar = int (np.argmax (np.sum (b_equal, axis = 1)))\n\n            ms_most_similar = Manuscript (conn, \'id\' + str (ms_id_most_similar + 1))\n            d = ms_most_similar.to_json ()\n\n            b_explained = np.copy (b_post[ms_id_most_similar])\n            n_explains = int (np.count_nonzero (b_explained))\n            # exit if no passages could be explained\n            if n_explains == 0:\n                break\n\n            n_equal = int (np.count_nonzero (b_equal[ms_id_most_similar]))\n\n            # remove ""explained"" readings, so they will not be matched again\n            b_post[:,b_explained] = False\n            b_equal[:,b_explained] = False\n            b_unknown[b_explained] = False\n            b_open[b_explained]    = False\n\n            n_explained += n_explains\n            n_unknown = int (np.count_nonzero (b_unknown))\n            n_open    = int (np.count_nonzero (b_open))\n\n            d[\'explains\']  = n_explains\n            d[\'explained\'] = n_explained\n            d[\'equal\']     = n_equal\n            d[\'post\']      = n_explains - n_equal\n            d[\'unknown\']   = n_unknown\n            d[\'open\']      = n_open - n_unknown\n            d[\'n\']         = n + 1\n            cover.append (d)\n\n        # output list\n        response[\'cover\'] = cover\n        return make_json_response (response)\n\n\nclass Combination ():\n    """""" Represents a combination of manuscripts. """"""\n\n    def __init__ (self, iterator, index):\n        """""" Init from an iterable of Manuscripts. """"""\n\n        self.mss   = list (iterator)\n        self.index = index\n        self.len   = len (self.mss)\n        self.vec   = np.array ([ ms.ms_id - 1 for ms in self.mss ])\n        self.n_explained_equal = 0\n        self.n_explained_post  = 0\n        self.n_unknown         = 0\n        self.n_open            = 0\n        self.hint              = False\n        self.unknown_indices   = tuple ([-1])\n        self.open_indices      = tuple ([-1])\n\n\n    def score (self):\n        """""" Calculate the score for the given combination. """"""\n\n        return 10 * self.n_explained_equal + 5 * self.n_explained_post\n\n    def explained (self):\n        """""" Calculate how many variants are explained by this combination. """"""\n\n        return self.n_explained_equal + self.n_explained_post\n\n    def to_json (self):\n        """""" Output the combination in JSON format. """"""\n\n        return {\n            \'index\'  : self.index,\n            \'mss\'    : [ms.to_json () for ms in self.mss],\n            \'count\'  : self.len,\n            \'equal\'  : self.n_explained_equal,\n            \'post\'   : self.n_explained_post,\n            \'unknown\': self.n_unknown,\n            \'open\'   : self.n_open,\n            \'hint\'   : self.hint\n        }\n\n    def to_csv (self):\n        """""" Output the combination in CSV format. """"""\n\n        return [\n            \' \'.join ([ms.hs for ms in self.mss]),\n            self.len,\n            self.n_explained_equal,\n            self.n_explained_post,\n            self.n_unknown,\n            self.n_open,\n            self.hint\n        ]\n\n\ndef _optimal_substemma (ms_id, explain_matrix, combinations, mode):\n    """"""Do an exhaustive search for the combination among a given set of ancestors\n    that best explains a given manuscript.\n\n    """"""\n\n    ms_id = ms_id - 1  # numpy indices start at 0\n    val = current_app.config.val\n\n    b_defined = val.def_matrix[ms_id]\n    # remove variants where the inspected ms is undefined\n    b_common = np.logical_and (val.def_matrix, b_defined)\n\n    explain_equal_matrix = val.mask_matrix[ms_id]\n\n    # The mss x passages boolean matrix that is TRUE whenever the inspected ms.\n    # agrees with the potential source ms.\n    b_equal = np.bitwise_and (val.mask_matrix, explain_equal_matrix) > 0\n    b_equal = np.logical_and (b_equal, b_common)\n\n    # The mss x passages boolean matrix that is TRUE whenever the inspected ms.\n    # agrees with the potential source ms. or is posterior to it.\n    b_post = np.bitwise_and (val.mask_matrix, explain_matrix) > 0\n    b_post = np.logical_and (b_post, b_common)\n\n    for comb in combinations:\n        # how many passages does this combination explain?\n        # pylint: disable=no-member\n        b_explained_equal = np.logical_or.reduce (b_equal[comb.vec])\n        b_explained_post  = np.logical_or.reduce (b_post[comb.vec])\n        b_explained_post  = np.logical_and (b_explained_post, np.logical_not (b_explained_equal))\n        b_explained       = np.logical_or (b_explained_equal, b_explained_post)\n\n        comb.n_explained_equal = np.count_nonzero (b_explained_equal)\n        comb.n_explained_post  = np.count_nonzero (b_explained_post)\n\n        unexplained_matrix = np.copy (explain_matrix)\n        unexplained_matrix[np.logical_not (b_defined)] = 0\n        unexplained_matrix[b_explained] = 0\n        b_unknown = np.bitwise_and (unexplained_matrix, 0x1) > 0\n        unexplained_matrix[b_unknown] = 0\n        b_open = unexplained_matrix > 0\n\n        comb.n_unknown = np.count_nonzero (b_unknown)\n        comb.n_open = np.count_nonzero (b_open)\n\n        if mode == \'detail\':\n            comb.open_indices    = tuple (int (n + 1) for n in np.nonzero (b_open)[0])\n            comb.unknown_indices = tuple (int (n + 1) for n in np.nonzero (b_unknown)[0])\n\n    if mode == \'search\':\n        # add the \'hint\' column\n        def key_len (c):\n            return c.len\n\n        def key_explained (c):\n            return -c.explained ()\n\n        for _k, g in itertools.groupby (sorted (combinations, key = key_len), key = key_len):\n            sorted (g, key = key_explained)[0].hint = True\n\n\n@bp.route (\'/optimal-substemma.json\')\ndef optimal_substemma_json ():\n    """"""Normalize parameters only and add some general info.\n    """"""\n\n    if current_app.config.val is None:\n        current_app.config.val = init (current_app.config.dba)\n    val = current_app.config.val\n\n    with current_app.config.dba.engine.begin () as conn:\n        # the manuscript to explain\n        ms = Manuscript (conn, request.args.get (\'ms\'))\n\n        # get the selected set of ancestors and build all combinations of that set\n        selected = [ Manuscript (conn, anc_id)\n                     for anc_id in (request.args.get (\'selection\') or \'\').split () ]\n        response = {\n            \'ms\'  : ms.to_json (),\n            \'mss\' : [s.to_json () for s in selected],\n        }\n        n_defined = np.count_nonzero (val.def_matrix[ms.ms_id - 1])\n        response[\'ms\'][\'open\'] = n_defined\n\n        return make_json_response (response)\n\n\n_OptimalSubstemmaRow = collections.namedtuple (\n    \'OptimalSubstemmaRow\',\n    \'mss count equal post unknown open hint\'\n)\n\n@bp.route (\'/optimal-substemma.csv\')\ndef optimal_substemma_csv ():\n    """"""Do an exhaustive search for the combination among a given set of ancestors\n    that best explains a given manuscript.\n\n    """"""\n\n    if current_app.config.val is None:\n        current_app.config.val = init (current_app.config.dba)\n    val = current_app.config.val\n\n    with current_app.config.dba.engine.begin () as conn:\n        # the manuscript to explain\n        ms = Manuscript (conn, request.args.get (\'ms\'))\n\n        # get the selected set of ancestors and build all combinations of that set\n        selected = [ Manuscript (conn, anc_id)\n                     for anc_id in (request.args.get (\'selection\') or \'\').split () ]\n        combinations = []\n        i = 0\n        for l in range (len (selected)):\n            for c in itertools.combinations (selected, l + 1):\n                combinations.append (Combination (c, i))\n                i += 1\n\n        explain_matrix = build_explain_matrix (conn, val, ms.ms_id)\n        _optimal_substemma (ms.ms_id, explain_matrix, combinations, mode = \'search\')\n\n        res = [c.to_csv () for c in combinations]\n\n        return csvify (_OptimalSubstemmaRow._fields,\n                       list (map (_OptimalSubstemmaRow._make, res)))\n\n\n_OptimalSubstemmaDetailRow = collections.namedtuple (\n    \'OptimalSubstemmaDetailRow\',\n    \'type pass_id begadr endadr labez_clique lesart\'\n)\n\nclass _OptimalSubstemmaDetailRowCalcFields (_OptimalSubstemmaDetailRow):\n    __slots__ = ()\n\n    _fields = _OptimalSubstemmaDetailRow._fields + (\'pass_hr\', )\n\n    @property\n    def pass_hr (self):\n        """""" Add a field with a human-readable passage id. """"""\n        return Passage.static_to_hr (self.begadr, self.endadr)\n\n    def _asdict (self):\n        return collections.OrderedDict (zip (self._fields, self + (self.pass_hr, )))\n\n\n@bp.route (\'/optimal-substemma-detail.csv\')\ndef optimal_substemma_detail_csv ():\n    """"""Report details about one combination of ancestors.\n    """"""\n\n    if current_app.config.val is None:\n        current_app.config.val = init (current_app.config.dba)\n    val = current_app.config.val\n\n    with current_app.config.dba.engine.begin () as conn:\n        # the manuscript to explain\n        ms = Manuscript (conn, request.args.get (\'ms\'))\n\n        # get the selected set of ancestors\n        selected = [ Manuscript (conn, anc_id)\n                     for anc_id in (request.args.get (\'selection\') or \'\').split () ]\n\n        combinations   = [Combination (selected, 0)]\n        explain_matrix = build_explain_matrix (conn, val, ms.ms_id)\n        _optimal_substemma (ms.ms_id, explain_matrix, combinations, mode = \'detail\')\n\n        res = execute (conn, """"""\n        SELECT \'unknown\' as type, p.pass_id, p.begadr, p.endadr, v.labez_clique, v.lesart\n        FROM passages p\n          JOIN apparatus_cliques_view v USING (pass_id)\n        WHERE v.ms_id = :ms_id AND pass_id IN :unknown_pass_ids\n        UNION\n        SELECT \'open\' as type, p.pass_id, p.begadr, p.endadr, v.labez_clique, v.lesart\n        FROM passages p\n          JOIN apparatus_cliques_view v USING (pass_id)\n        WHERE v.ms_id = :ms_id AND pass_id IN :open_pass_ids\n        """""", dict (\n            ms_id = ms.ms_id,\n            unknown_pass_ids = combinations[0].unknown_indices or (-1, ),\n            open_pass_ids    = combinations[0].open_indices    or (-1, )\n        ))\n\n        return csvify (_OptimalSubstemmaDetailRowCalcFields._fields,\n                       list (map (_OptimalSubstemmaDetailRowCalcFields._make, res)))\n'"
server/static.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""An application server for CBGM.  Serve static files.  """"""\n\nimport flask\nfrom flask import request, current_app, send_from_directory\n\n\nbp = flask.Blueprint (\'static\', __name__)\n\n\ndef init_app (app):\n    """""" Initialize the flask app. """"""\n    app.static_folder   = app.config[\'STATIC_FOLDER\']\n    app.static_url_path = app.config[\'STATIC_URL_PATH\']\n\n\n@bp.route (\'/api.conf.js\')\n@bp.route (\'/js/<path:path>\')\n@bp.route (\'/images/<path:path>\')\n@bp.route (\'/pdfs/<path:path>\')\n@bp.route (\'/webfonts/<path:path>\')\ndef static_folder (path = None):\n    """"""Endpoint.  Serve static files.""""""\n\n    return send_from_directory (current_app.static_folder, request.path[1:])\n\n\n@bp.route (\'/\')\n@bp.route (\'/<path:path>\')\ndef index_html (path = None):\n    """"""Endpoint.  Serve the index.""""""\n\n    return send_from_directory (current_app.static_folder, \'index.html\')\n'"
server/textflow.py,0,"b'#!/usr/bin/python3\n# -*- encoding: utf-8 -*-\n\n""""""The API server for CBGM.  The textflow and stemmata diagrams.""""""\n\nimport collections\n\nimport flask\nfrom flask import request, current_app\nimport flask_login\nimport networkx as nx\n\nfrom ntg_common.db_tools import execute\nfrom ntg_common import tools\nfrom ntg_common import db_tools\n\nfrom login import auth, user_can_write\nimport helpers\nfrom helpers import parameters, Passage, get_excluded_ms_ids, \\\n     make_dot_response, make_png_response\nfrom checks import congruence\n\n\nbp = flask.Blueprint (\'textflow\', __name__)\n\n\ndef init_app (_app):\n    """""" Initialize the flask app. """"""\n\n\nSHAPES = {\n    \'a\' : \'ellipse\',\n    \'b\' : \'box\',\n    \'c\' : \'pentagon\',\n    \'d\' : \'hexagon\',\n    \'e\' : \'septagon\',\n    \'f\' : \'octagon\',\n    \'g\' : \'diamond\',\n    \'h\' : \'trapezium\',\n    \'i\' : \'parallelogram\',\n    \'j\' : \'house\',\n    \'k\' : \'invtrapezium\',\n    \'l\' : \'invparallelogram\',\n    \'m\' : \'invhouse\',\n}\n\n\ndef remove_z_leaves (graph):\n    """""" Removes leaves (recursively) if they read z. """"""\n\n    # We cannot use DFS because we don\'t know the root.\n    try:\n        nodes = list (nx.topological_sort (graph))\n    except nx.NetworkXUnfeasible:\n        return\n\n    for n in reversed (nodes):\n        atts = graph.nodes[n]\n        if graph.out_degree (n) == 0 and \'labez\' in atts and atts[\'labez\'][0] == \'z\':\n            graph.remove_node (n)\n\n\ndef textflow (passage_or_id):\n    """""" Output a stemma of manuscripts. """"""\n\n    labez        = request.args.get (\'labez\') or \'\'\n    hyp_a        = request.args.get (\'hyp_a\') or \'A\'\n    connectivity = int (request.args.get (\'connectivity\') or 10)\n    width        = float (request.args.get (\'width\') or 0.0)\n    fontsize     = float (request.args.get (\'fontsize\') or 10.0)\n    mode         = request.args.get (\'mode\') or \'sim\'\n\n    include      = request.args.getlist (\'include[]\')   or []\n    fragments    = request.args.getlist (\'fragments[]\') or []\n    checks       = request.args.getlist (\'checks[]\') or []\n    var_only     = request.args.getlist (\'var_only[]\')  or []\n    cliques      = request.args.getlist (\'cliques[]\')   or []\n\n    fragments = \'fragments\' in fragments\n    checks    = \'checks\'    in checks\n    var_only  = \'var_only\'  in var_only   # Panel: Coherence at Variant Passages (GraphViz)\n    cliques   = \'cliques\'   in cliques    # consider or ignore cliques\n    leaf_z    = \'Z\'         in include    # show leaf z nodes in global textflow?\n\n    view = \'affinity_view\' if mode == \'rec\' else \'affinity_p_view\'\n\n    global_textflow = not ((labez != \'\') or var_only)\n    rank_z = False  # include z nodes in ranking?\n\n    if global_textflow:\n        connectivity = 1\n        rank_z = True\n    if connectivity == 21:\n        connectivity = 9999\n\n    labez_where = \'\'\n    frag_where = \'\'\n    z_where = \'\'\n\n    if labez != \'\':\n        labez_where = \'AND app.cbgm AND app.labez = :labez\'\n        if hyp_a != \'A\':\n            labez_where = \'AND app.cbgm AND (app.labez = :labez OR (app.ms_id = 1 AND :hyp_a = :labez))\'\n\n    if not fragments:\n        frag_where = \'AND a.common > a.ms1_length / 2\'\n\n    if not rank_z:\n        z_where = ""AND app.labez !~ \'^z\' AND app.certainty = 1.0""\n\n    group_field = \'labez_clique\' if cliques else \'labez\'\n\n    with current_app.config.dba.engine.begin () as conn:\n        passage = Passage (conn, passage_or_id)\n        rg_id   = passage.request_rg_id (request)\n\n        exclude = get_excluded_ms_ids (conn, include)\n\n        # nodes query\n        #\n        # get all nodes or all nodes (hypothetically) attesting labez\n\n        res = execute (conn, """"""\n        SELECT ms_id\n        FROM apparatus app\n        WHERE pass_id = :pass_id AND ms_id NOT IN :exclude {labez_where} {z_where}\n        """""", dict (parameters, exclude = exclude,\n                   pass_id = passage.pass_id, labez = labez,\n                   hyp_a = hyp_a, labez_where = labez_where, z_where = z_where))\n\n        nodes = { row[0] for row in res }\n        if not nodes:\n            nodes = { -1 } # avoid SQL syntax error\n\n        # rank query\n        #\n        # query to get the closest ancestors for every node with rank <= connectivity\n\n        query = """"""\n        SELECT ms_id1, ms_id2, rank\n        FROM (\n          SELECT ms_id1, ms_id2, rank () OVER (PARTITION BY ms_id1\n             ORDER BY affinity DESC, common, older, newer DESC, ms_id2) AS rank\n          FROM {view} a\n          WHERE ms_id1 IN :nodes AND a.rg_id = :rg_id AND ms_id2 NOT IN :exclude\n            AND newer > older {frag_where}\n        ) AS r\n        WHERE rank <= :connectivity\n        ORDER BY rank\n        """"""\n\n        res = execute (conn, query,\n                       dict (parameters, nodes = tuple (nodes), exclude = exclude,\n                             rg_id = rg_id, pass_id = passage.pass_id, view = view,\n                             labez = labez, connectivity = connectivity,\n                             frag_where = frag_where, hyp_a = hyp_a))\n\n        Ranks = collections.namedtuple (\'Ranks\', \'ms_id1 ms_id2 rank\')\n        ranks = list (map (Ranks._make, res))\n\n        # Initially build an unconnected graph with one node for each\n        # manuscript.  We will connect the nodes later.  Finally we will remove\n        # unconnected nodes.\n\n        graph = nx.DiGraph ()\n\n        dest_nodes = { r.ms_id1 for r in ranks }\n        src_nodes  = { r.ms_id2 for r in ranks }\n\n        res = execute (conn, """"""\n        SELECT ms.ms_id, ms.hs, ms.hsnr, a.labez, a.clique, a.labez_clique, a.certainty\n        FROM apparatus_view_agg a\n        JOIN manuscripts ms USING (ms_id)\n        WHERE pass_id = :pass_id AND ms_id IN :ms_ids\n        """""", dict (parameters,\n                   ms_ids = tuple (src_nodes | dest_nodes | nodes),\n                   pass_id = passage.pass_id))\n\n        Mss = collections.namedtuple (\'Mss\', \'ms_id hs hsnr labez clique labez_clique certainty\')\n        mss = list (map (Mss._make, res))\n\n        for ms in mss:\n            attrs = {}\n            attrs[\'hs\']           = ms.hs\n            attrs[\'hsnr\']         = ms.hsnr\n            attrs[\'labez\']        = ms.labez if ms.certainty == 1.0 else \'zw \' + ms.labez\n            attrs[\'clique\']       = ms.clique\n            attrs[\'labez_clique\'] = ms.labez_clique if ms.certainty == 1.0 else \'zw \' + ms.labez_clique\n            attrs[\'ms_id\']        = ms.ms_id\n            attrs[\'label\']        = ms.hs\n            attrs[\'certainty\']    = ms.certainty\n            attrs[\'clickable\']    = \'1\'\n            if ms.ms_id == 1 and hyp_a != \'A\':\n                attrs[\'labez\']        = hyp_a[0]\n                attrs[\'clique\']       = \'\'\n                attrs[\'labez_clique\'] = hyp_a[0]\n            # FIXME: attrs[\'shape\'] = SHAPES.get (attrs[\'labez\'], SHAPES[\'a\'])\n            graph.add_node (ms.ms_id, **attrs)\n\n        # Connect the nodes\n        #\n        # Step 1: If the node has internal parents, keep only the top-ranked\n        # internal parent.\n        #\n        # Step 2: If the node has no internal parents, keep the top-ranked\n        # parents for each external attestation.\n        #\n        # Assumption: ranks are sorted top-ranked first\n\n        def is_z_node (n):\n            labez = n[\'labez\']\n            cert  = n[\'certainty\']\n            return (labez[0] == \'z\') or (cert < 1.0)\n\n        tags = set ()\n        for step in (1, 2):\n            for r in ranks:\n                a1 = graph.nodes[r.ms_id1]\n                if not r.ms_id2 in graph.nodes:\n                    continue\n                a2 = graph.nodes[r.ms_id2]\n                if not (global_textflow) and is_z_node (a2):\n                    # disregard zz / zw\n                    continue\n                if step == 1 and a1[group_field] != a2[group_field]:\n                    # differing attestations are handled in step 2\n                    continue\n                if r.ms_id1 in tags:\n                    # an ancestor of this node that lays within the node\'s\n                    # attestation was already seen.  we need not look into other\n                    # attestations\n                    continue\n                if str (r.ms_id1) + a2[group_field] in tags:\n                    # an ancestor of this node that lays within this attestation\n                    # was already seen.  we need not look into further nodes\n                    continue\n                # add a new parent\n                if r.rank > 1:\n                    graph.add_edge (r.ms_id2, r.ms_id1, rank = r.rank, headlabel = r.rank)\n                else:\n                    graph.add_edge (r.ms_id2, r.ms_id1)\n\n                if a1[group_field] == a2[group_field]:\n                    # tag: has ancestor node within the same attestation\n                    tags.add (r.ms_id1)\n                else:\n                    # tag: has ancestor node with this other attestation\n                    tags.add (str (r.ms_id1) + a2[group_field])\n\n        if not leaf_z:\n            remove_z_leaves (graph)\n\n        # the if clause fixes #83\n        graph.remove_nodes_from ([n for n in nx.isolates (graph)\n                                  if graph.nodes[n][\'labez\'] != labez])\n\n        if var_only:\n            # Panel: Coherence at Variant Passages (GraphViz)\n            #\n            # if one predecessor is within the same attestation then remove all\n            # other predecessors that are not within the same attestation\n            for n in graph:\n                within = False\n                attestation_n = graph.nodes[n][group_field]\n                for p in graph.predecessors (n):\n                    if graph.nodes[p][group_field] == attestation_n:\n                        within = True\n                        break\n                if within:\n                    for p in graph.predecessors (n):\n                        if graph.nodes[p][group_field] != attestation_n:\n                            graph.remove_edge (p, n)\n\n            # remove edges between nodes within the same attestation\n            for u, v in list (graph.edges ()):\n                if graph.nodes[u][group_field] == graph.nodes[v][group_field]:\n                    graph.remove_edge (u, v)\n\n            # remove now isolated nodes\n            graph.remove_nodes_from (list (nx.isolates (graph)))\n\n            # unconstrain backward edges (yields a better GraphViz layout)\n            for u, v in graph.edges ():\n                if graph.nodes[u][group_field] > graph.nodes[v][group_field]:\n                    graph.adj[u][v][\'constraint\'] = \'false\'\n\n        else:\n            for n in graph:\n                # Use a different label if the parent\'s labez_clique differs from this\n                # node\'s labez_clique.\n                pred = list (graph.predecessors (n))\n                attrs = graph.nodes[n]\n                if not pred:\n                    attrs[\'label\'] = ""%s: %s"" % (attrs[\'labez_clique\'], attrs[\'hs\'])\n                for p in pred:\n                    if attrs[\'labez_clique\'] != graph.nodes[p][\'labez_clique\']:\n                        attrs[\'label\'] = ""%s: %s"" % (attrs[\'labez_clique\'], attrs[\'hs\'])\n                        graph.adj[p][n][\'style\'] = \'dashed\'\n\n        if checks:\n            for rank in congruence (conn, passage):\n                try:\n                    graph.adj[rank.ms_id1][rank.ms_id2][\'style\'] = \'bold\'\n                except KeyError:\n                    pass\n\n    if var_only:\n        dot = helpers.nx_to_dot_subgraphs (graph, group_field, width, fontsize)\n    else:\n        dot = helpers.nx_to_dot (graph, width, fontsize)\n    return dot\n\n\n@bp.route (\'/textflow.dot/<passage_or_id>\')\ndef textflow_dot (passage_or_id):\n    """""" Return a textflow diagram in .dot format. """"""\n\n    auth ()\n\n    dot = textflow (passage_or_id)\n    dot = tools.graphviz_layout (dot)\n    return make_dot_response (dot)\n\n\n@bp.route (\'/textflow.png/<passage_or_id>\')\ndef textflow_png (passage_or_id):\n    """""" Return a textflow diagram in .png format. """"""\n\n    auth ()\n\n    dot = textflow (passage_or_id)\n    png = tools.graphviz_layout (dot, format = \'png\')\n    return make_png_response (png)\n\n\ndef stemma (passage_or_id):\n    """"""Serve a local stemma in dot format.\n\n    A local stemma is a DAG (directed acyclic graph).  The layout of the DAG is\n    precomputed on the server using GraphViz.  GraphViz adds a precomputed\n    position to each node and a precomputed bezier path to each edge.\n\n    N.B. I also considered client-side layout of DAGs, but found only 2 viable\n    libraries:\n\n    - dagre.  Javascript clone of GraphViz.  Unmaintained.  Buggy.  Does not\n      work well with require.js.\n\n    - viz.js.  GraphViz cross-compiled to Javascript with Emscripten.  Huge.\n      Promising but still early days.\n\n    Both libraries have their drawbacks so the easiest way out was to precompute\n    the layout on the server.\n\n    """"""\n\n    width    = float (request.args.get (\'width\') or 0.0)\n    fontsize = float (request.args.get (\'fontsize\') or 10.0)\n\n    with current_app.config.dba.engine.begin () as conn:\n        passage = Passage (conn, passage_or_id)\n        graph = db_tools.local_stemma_to_nx (\n            conn, passage.pass_id, user_can_write (current_app)\n        )\n        dot = helpers.nx_to_dot (graph, width, fontsize, nodesep = 0.2)\n        return dot\n\n\n@bp.route (\'/stemma.dot/<passage_or_id>\')\ndef stemma_dot (passage_or_id):\n    """""" Return a local stemma diagram in .dot format. """"""\n\n    auth ()\n\n    dot = stemma (passage_or_id)\n    dot = tools.graphviz_layout (dot)\n    return make_dot_response (dot)\n\n\n@bp.route (\'/stemma.png/<passage_or_id>\')\ndef stemma_png (passage_or_id):\n    """""" Return a local stemma diagram in .png format. """"""\n\n    auth ()\n\n    dot = stemma (passage_or_id)\n    png = tools.graphviz_layout (dot, format = \'png\')\n    return make_png_response (png)\n'"
scripts/cceh/__init__.py,0,"b'"""""" Package """"""\n'"
scripts/cceh/cbgm.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""Perform the CBGM.\n\nThis script\n\n- rebuilds the \'A\' text from the local stemmas,\n- calculates the pre-coherence similarity of manuscripts, and\n- calculates the post-coherence ancestrality of manuscripts.\n\nThis script updates the tables shown in red in the `overview <db-overwiew>`.\nIt also updates the Apparatus table where manuscript \'A is concerned.\n\n""""""\n\nimport argparse\nimport collections\nimport logging\n\nimport networkx as nx\nimport numpy as np\n\nfrom ntg_common import db\nfrom ntg_common import db_tools\nfrom ntg_common.db_tools import execute, executemany, executemany_raw, warn, debug\nfrom ntg_common.tools import log\nfrom ntg_common.config import args, init_logging, config_from_pyfile\n\nfrom ntg_common.cbgm_common import CBGM_Params, create_labez_matrix, \\\n    calculate_mss_similarity_preco, calculate_mss_similarity_postco, write_affinity_table\n\nMS_ID_A  = 1\n\ndef build_A_text (dba, parameters):\n    """"""Build the \'A\' text\n\n    The editors\' reconstruction of the archetype is recorded in the locstem\n    table. This functions generates a virtual manuscript \'A\' from those choices.\n\n    The designation of a passage as \'Fehlvers\' is an editorial decision that the\n    verse is not original, so we set \'zu\'.\n\n    If the editors came to no final decision, no \'original\' reading will be\n    found in locstem.  In this case we set \'A\' to \'zz\' and there will be a gap\n    in the reconstructed text.\n\n    The Lesart of \'A\' is always NULL, because it is a virtual manuscript.\n\n    """"""\n\n    with dba.engine.begin () as conn:\n\n        execute (conn, """"""\n        DELETE FROM ms_cliques     WHERE ms_id = :ms_id;\n        DELETE FROM ms_cliques_tts WHERE ms_id = :ms_id;\n        DELETE FROM apparatus      WHERE ms_id = :ms_id;\n        """""", dict (parameters, ms_id = MS_ID_A))\n\n        # Fill with the original reading in locstem or \'zz\' if none found\n        execute (conn, """"""\n        INSERT INTO apparatus_cliques_view (ms_id, pass_id, labez, clique, cbgm, origin, lesart)\n          SELECT :ms_id, p.pass_id, COALESCE (l.labez, \'zz\'), COALESCE (l.clique, \'1\'), true, \'LOC\', NULL\n          FROM passages p\n          LEFT JOIN locstem l ON (l.pass_id, l.source_labez) = (p.pass_id, \'*\')\n          WHERE NOT p.fehlvers\n        """""", dict (parameters, ms_id = MS_ID_A))\n\n        # Fill Fehlverse with labez \'zu\'\n        execute (conn, """"""\n        INSERT INTO apparatus_cliques_view (ms_id, pass_id, labez, clique, cbgm, origin, lesart)\n          SELECT :ms_id, p.pass_id, \'zu\', \'1\', true, \'LOC\', NULL\n          FROM passages p\n          WHERE p.fehlvers\n        """""", dict (parameters, ms_id = MS_ID_A))\n\n\ndef build_parser ():\n    parser = argparse.ArgumentParser (description = __doc__)\n\n    parser.add_argument (\'profile\', metavar=\'path/to/file.conf\',\n                         help=""a .conf file (required)"")\n    parser.add_argument (\'-v\', \'--verbose\', dest=\'verbose\', action=\'count\',\n                         help=\'increase output verbosity\', default=0)\n    return parser\n\n\nif __name__ == \'__main__\':\n\n    build_parser ().parse_args (namespace = args)\n    config = config_from_pyfile (args.profile)\n\n    init_logging (\n        args,\n        logging.StreamHandler (), # stderr\n        logging.FileHandler (\'cbgm.log\')\n    )\n\n    db = db_tools.PostgreSQLEngine (**config)\n    parameters = dict ()\n    v = CBGM_Params ()\n\n    log (logging.INFO, ""Rebuilding the \'A\' text ..."")\n    build_A_text (db, parameters)\n\n    log (logging.INFO, ""Creating the labez matrix ..."")\n    create_labez_matrix (db, parameters, v)\n\n    log (logging.INFO, ""Calculating mss similarity pre-co ..."")\n    calculate_mss_similarity_preco (db, parameters, v)\n\n    log (logging.INFO, ""Calculating mss similarity post-co ..."")\n    calculate_mss_similarity_postco (db, parameters, v)\n\n    log (logging.INFO, ""Writing affinity table ..."")\n    write_affinity_table (db, parameters, v)\n\n    log (logging.INFO, ""Vacuum ..."")\n    db.vacuum ()\n\n    log (logging.INFO, ""Done"")\n'"
scripts/cceh/import.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""Import databases from mysql.\n\nThis script initializes the postgres database and then imports data from one or\nmore mysql databases.\n\n.. note::\n\n   Make sure to follow the steps in `database-access` first.\n\nThe source databases are:\n\n1. a database containing the apparatus of the *Editio Critica Maior*\n   publication (ECM).\n\n2. a database containing the editorial decisions regarding the priority of the\n   readings (VarGen).\n\n3. a database containing the \xe2\x80\x9cLeitzeile\xe2\x80\x9d (Nestle).\n\nThe source tables for Acts are partitioned into 28 chapters.  This is a\nhistorical incident: The software used when the CBGM was first implemented could\nnot handle big tables.  The import script is able to join partitioned tables.\n\nAfter running this script should run the `prepare.py` script.\n\n""""""\n\nimport argparse\nimport collections\nimport logging\nimport re\n\nimport sqlalchemy\n\nfrom ntg_common import db\nfrom ntg_common import db_tools\nfrom ntg_common.config import args, init_logging, config_from_pyfile\nfrom ntg_common.db_tools import execute, warn, debug\nfrom ntg_common.tools import log\n\n\ndef copy_table_fdw (conn, dest_table, fdw, source_table):\n    """"""Copy a table. """"""\n\n    execute (conn, """"""\n    DROP TABLE IF EXISTS {dest_table};\n    """""", dict (parameters, dest_table = dest_table))\n\n    execute (conn, """"""\n    SELECT * INTO {dest_table} FROM  {fdw}.""{source_table}""\n    """""", dict (parameters, fdw = fdw, dest_table = dest_table, source_table = source_table))\n\n\ndef concat_tables_fdw (conn, meta, dest_table, fdw, table_mask):\n    """"""Concatenate multiple tables into one.""""""\n\n    table_mask = re.compile (\'^%s$\' % table_mask)\n\n    # find the set of fields common to all input tables.  check types also.  it\n    # is ridiculous that we have to do this but the table structures are highly\n    # inconsistent even between chapters of the same book.\n    source_table = None\n    column_set = collections.OrderedDict ()\n    for t in sorted (meta.tables.keys ()):\n        if table_mask.match (t):\n            source_model = sqlalchemy.Table (t, meta, autoload = True)\n            if source_table is None:\n                source_table = t\n                for c in source_model.columns:\n                    column_set[c.name] = c.type.python_type\n            else:\n                col_set = { c.name : c.type.python_type for c in source_model.columns }\n                for name, type_ in list (column_set.items ()):\n                    if col_set.get (name, \'\') != type_:\n                        del column_set[name]\n\n    # create a table with those fields common to all input tables, lowercase the\n    # field names\n    execute (conn, """"""\n    DROP TABLE IF EXISTS {dest_table}\n    """""", dict (parameters, dest_table = dest_table))\n\n    execute (conn, """"""\n    CREATE TABLE {dest_table} ( LIKE {fdw}.""{source_table}"" )\n    """""", dict (parameters, dest_table = dest_table, source_table = source_table, fdw = fdw))\n\n    source_model = sqlalchemy.Table (source_table, meta, autoload = True)\n    cols = [column.name for column in source_model.columns]\n\n    for column in cols:\n        if column in column_set:\n            if column != column.lower ():\n                execute (conn, \'ALTER TABLE {dest_table} RENAME COLUMN ""{source_column}"" TO ""{dest_column}""\',\n                         dict (parameters, dest_table = dest_table, source_column = column, dest_column = column.lower ()))\n        else:\n            execute (conn, \'ALTER TABLE {dest_table} DROP COLUMN ""{source_column}""\',\n                     dict (parameters, dest_table = dest_table, source_column = column, dest_column = column.lower ()))\n\n    execute (conn, """"""COMMIT"""""", parameters);\n\n    # concat the input tables\n    for source_table in sorted (meta.tables.keys ()):\n        if not table_mask.match (source_table):\n            continue\n        log (logging.DEBUG, ""    Copying table %s"" % source_table)\n\n        source_columns = [\'""\' + column + \'""\'          for column in column_set.keys ()]\n        dest_columns   = [\'""\' + column.lower () + \'""\' for column in column_set.keys ()]\n\n        execute (conn, """"""\n        INSERT INTO {dest_table} ({dest_columns})\n        SELECT {source_columns}\n        FROM {fdw}.""{source_table}""\n        """""", dict (parameters, source_table = source_table, dest_table = dest_table, fdw = fdw,\n                   source_columns = \', \'.join (source_columns),\n                   dest_columns = \', \'.join (dest_columns)))\n\n\ndef import_att_fdw (dbsrc, dbdest, parameters):\n    """"""Import att and lac tables from mysql.\n\n    Import the (28 * 2) mysql tables to 2 tables in the postgres database.\n\n    """"""\n\n    log (logging.INFO, ""  Importing mysql att tables ..."")\n\n    dbsrc_meta = sqlalchemy.schema.MetaData (bind = dbsrc.engine)\n    dbsrc_meta.reflect ()\n\n    with dbdest.engine.begin () as dest:\n        concat_tables_fdw (dest, dbsrc_meta, \'original_att\', \'app_fdw\', config[\'MYSQL_ATT_TABLES\'])\n\n    with dbdest.engine.begin () as dest:\n        if config.get (\'MYSQL_LAC_TABLES\'):\n            log (logging.INFO, ""  Importing mysql lac tables ..."")\n            concat_tables_fdw (dest, dbsrc_meta, \'original_lac\', \'app_fdw\', config[\'MYSQL_LAC_TABLES\'])\n        else:\n            # no lacuna tables provided (eg. John)\n            execute (dest, """"""\n\t        DROP TABLE IF EXISTS original_lac;\n\t        CREATE TABLE original_lac (LIKE original_att);\n            """""", parameters)\n\n    with dbdest.engine.begin () as dest:\n        execute (dest, """"""\n        ALTER TABLE original_att RENAME COLUMN anfadr TO begadr;\n        ALTER TABLE original_lac RENAME COLUMN anfadr TO begadr;\n        """""", parameters)\n\n\ndef import_genealogical_fdw (dbsrc, dbdest, parameters):\n    """"""Import genealogical tables from mysql.\n\n    Import the (28 * 3) mysql tables to 3 tables in the postgres database.\n\n    """"""\n\n    if not config.get (\'MYSQL_VG_DB\'):\n        return\n\n    dbsrc_meta = sqlalchemy.schema.MetaData (bind = dbsrc.engine)\n    dbsrc_meta.reflect ()\n\n    with dbdest.engine.begin () as dest:\n        if config.get (\'MYSQL_LOCSTEM_TABLES\'):\n            log (logging.INFO, ""  Importing mysql locstem tables ..."")\n            concat_tables_fdw (dest, dbsrc_meta, \'original_locstemed\', \'var_fdw\', config[\'MYSQL_LOCSTEM_TABLES\'])\n\n    with dbdest.engine.begin () as dest:\n        if config.get (\'MYSQL_RDG_TABLES\'):\n            log (logging.INFO, ""  Importing mysql rdg tables ..."")\n            concat_tables_fdw (dest, dbsrc_meta, \'original_rdg\',       \'var_fdw\', config[\'MYSQL_RDG_TABLES\'])\n\n    with dbdest.engine.begin () as dest:\n        if config.get (\'MYSQL_VAR_TABLES\'):\n            log (logging.INFO, ""  Importing mysql var tables ..."")\n            concat_tables_fdw (dest, dbsrc_meta, \'original_var\',       \'var_fdw\', config[\'MYSQL_VAR_TABLES\'])\n\n    with dbdest.engine.begin () as dest:\n        if config.get (\'MYSQL_MEMO_TABLE\'):\n            log (logging.INFO, ""  Importing mysql memo table ..."")\n            copy_table_fdw    (dest,             \'original_memo\',      \'var_fdw\', config[\'MYSQL_MEMO_TABLE\'])\n            execute (dest, """"""\n            ALTER TABLE original_memo RENAME COLUMN anfadr TO begadr;\n            """""", parameters)\n\n\ndef import_nestle_fdw (dbsrc, dbdest, parameters):\n    """"""Import Nestle table from mysql.""""""\n\n    if config.get (\'MYSQL_NESTLE_TABLE\'):\n        with dbdest.engine.begin () as dest:\n            log (logging.INFO, ""  Importing mysql nestle table ..."")\n            copy_table_fdw (dest, \'original_nestle\', \'nestle_fdw\', config[\'MYSQL_NESTLE_TABLE\'])\n\n\ndef build_parser ():\n    parser = argparse.ArgumentParser (description = __doc__)\n\n    parser.add_argument (\'profile\', metavar=\'path/to/file.conf\',\n                         help=""a .conf file (required)"")\n    parser.add_argument (\'-v\', \'--verbose\', dest=\'verbose\', action=\'count\',\n                         help=\'increase output verbosity\', default=0)\n    return parser\n\n\nif __name__ == \'__main__\':\n\n    build_parser ().parse_args (namespace = args)\n    config = config_from_pyfile (args.profile)\n\n    init_logging (\n        args,\n        logging.StreamHandler (),\n        logging.FileHandler (\'import.log\')\n    )\n\n    parameters = dict ()\n\n    dbsrc1 = db_tools.MySQLEngine      (config[\'MYSQL_CONF\'], config[\'MYSQL_GROUP\'], config[\'MYSQL_ECM_DB\'])\n    dbsrc2 = db_tools.MySQLEngine      (config[\'MYSQL_CONF\'], config[\'MYSQL_GROUP\'], config[\'MYSQL_VG_DB\'])\n    dbsrc3 = db_tools.MySQLEngine      (config[\'MYSQL_CONF\'], config[\'MYSQL_GROUP\'], config[\'MYSQL_NESTLE_DB\'])\n    dbdest = db_tools.PostgreSQLEngine (**config)\n\n    db.fdw (\'app_fdw\',    db.Base.metadata,  dbdest, dbsrc1)\n    db.fdw (\'var_fdw\',    db.Base2.metadata, dbdest, dbsrc2)\n    db.fdw (\'nestle_fdw\', db.Base4.metadata, dbdest, dbsrc3)\n\n    log (logging.INFO, ""Creating Database Schema ..."")\n\n    db.Base.metadata.drop_all  (dbdest.engine)\n    db.Base2.metadata.drop_all (dbdest.engine)\n    db.Base4.metadata.drop_all (dbdest.engine)\n\n    db.Base.metadata.create_all  (dbdest.engine)\n    db.Base2.metadata.create_all (dbdest.engine)\n    db.Base4.metadata.create_all (dbdest.engine)\n\n    log (logging.INFO, ""Importing mysql tables ..."")\n\n    import_att_fdw (dbsrc1, dbdest, parameters)\n\n    import_genealogical_fdw (dbsrc2, dbdest, parameters)\n\n    import_nestle_fdw (dbsrc3, dbdest, parameters)\n\n    log (logging.INFO, ""Done"")\n'"
scripts/cceh/load_edits.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""Load a saved state of the editor tables.\n\nThis script loads the state of the tables with the editorial decisions as saved\nby the save_edits.py script.  It does not touch the apparatus tables.\n\nWhile loading, it checks for errors and discrepancies, eg. different passage\naddresses.  Passages in the apparatus that are not in the save state are reset\nto the default of: reading \'a\' is original and every other reading is derived\nfrom \'a\'.\n\n""""""\n\nimport argparse\nimport logging\nimport sys\n\nimport lxml\n\nfrom ntg_common import db\nfrom ntg_common import db_tools\nfrom ntg_common.db_tools import execute, executemany, warn, info, fix\nfrom ntg_common.tools import log\nfrom ntg_common.config import args, init_logging, config_from_pyfile\n\n\ndef build_parser ():\n    parser = argparse.ArgumentParser (description = __doc__)\n\n    parser.add_argument (\'-v\', \'--verbose\', dest=\'verbose\', action=\'count\',\n                         help=\'increase output verbosity\', default=0)\n    parser.add_argument (\'-i\', \'--input\', metavar=\'path/to/input.xml\',\n                         help=""the input file (required)"", required=True)\n    parser.add_argument (\'profile\', metavar=\'path/to/file.conf\',\n                         help=""a .conf file (required)"")\n    return parser\n\n\nif __name__ == \'__main__\':\n    build_parser ().parse_args (namespace = args)\n    config = config_from_pyfile (args.profile)\n\n    init_logging (\n        args,\n        logging.StreamHandler (), # stderr\n        logging.FileHandler (\'load_edits.log\')\n    )\n\n    parameters = dict ()\n    db = db_tools.PostgreSQLEngine (**config)\n\n    tree = lxml.etree.parse (args.input if args.input != \'-\' else sys.stdin)\n\n    with db.engine.begin () as conn:\n        db_tools.truncate_editor_tables (conn)\n\n    log (logging.INFO, ""Loading cliques ..."")\n\n    with db.engine.begin () as conn:\n        values = []\n        for row in tree.xpath (\'/sql/export_cliques/row\'):\n            values.append ({ e.tag : e.text for e in row })\n\n        execute (conn, """"""\n        TRUNCATE import_cliques;\n        """""", parameters)\n\n        executemany (conn, """"""\n        INSERT INTO import_cliques (passage, labez, clique,\n                                    sys_period, user_id_start, user_id_stop)\n        VALUES (:passage, :labez, :clique,\n                :sys_period, :user_id_start, :user_id_stop)\n        """""", parameters, values)\n\n        execute (conn, """"""\n        UPDATE import_cliques u\n        SET pass_id = r.pass_id\n        FROM readings_view r\n        WHERE (u.passage, u.labez) = (r.passage, r.labez)\n        """""", parameters)\n\n        info (conn, ""Discarded cliques"", """"""\n        SELECT passage, ARRAY_AGG (labez_clique (labez, clique) ORDER BY labez, clique) AS old_clique\n        FROM import_cliques\n        WHERE pass_id IS NULL AND UPPER_INF (sys_period)\n        GROUP BY passage\n        ORDER BY passage\n        """""", parameters)\n\n        execute (conn, """"""\n        DELETE FROM import_cliques\n        WHERE pass_id IS NULL\n        """""", parameters)\n\n    with db.engine.begin () as conn:\n        execute (conn, """"""\n        ALTER TABLE cliques DISABLE TRIGGER cliques_trigger;\n\n        INSERT INTO cliques (pass_id, labez, clique,\n                             sys_period, user_id_start, user_id_stop)\n        SELECT pass_id, labez, clique,\n               sys_period, user_id_start, user_id_stop\n        FROM import_cliques\n        WHERE UPPER_INF (sys_period);\n\n        INSERT INTO cliques_tts (pass_id, labez, clique,\n                                 sys_period, user_id_start, user_id_stop)\n        SELECT pass_id, labez, clique,\n               sys_period, user_id_start, user_id_stop\n        FROM import_cliques\n        WHERE NOT UPPER_INF (sys_period);\n\n        -- default entries\n        INSERT INTO cliques (pass_id, labez, clique, user_id_start)\n        SELECT pass_id, labez, \'1\', 0\n        FROM readings r\n        ON CONFLICT (pass_id, labez, clique) DO NOTHING;\n\n        ALTER TABLE cliques ENABLE TRIGGER cliques_trigger;\n        """""", parameters)\n\n\n    log (logging.INFO, ""Loading ms_cliques ..."")\n\n    with db.engine.begin () as conn:\n        values = []\n        for row in tree.xpath (\'/sql/export_ms_cliques/row\'):\n            values.append ({ e.tag : e.text for e in row })\n\n        execute (conn, """"""\n        TRUNCATE import_ms_cliques;\n        """""", parameters)\n\n        executemany (conn, """"""\n        INSERT INTO import_ms_cliques (hsnr, passage, labez, clique,\n                                       sys_period, user_id_start, user_id_stop)\n        VALUES (:hsnr, :passage, :labez, :clique,\n                :sys_period, :user_id_start, :user_id_stop)\n        """""", parameters, values)\n\n        # do not refer to cliques_view as it may not contain cliques in the history table\n        execute (conn, """"""\n        UPDATE import_ms_cliques u\n        SET ms_id = a.ms_id, pass_id = a.pass_id\n        FROM apparatus_view a\n        WHERE (u.passage, u.labez, u.hsnr) = (a.passage, a.labez, a.hsnr)\n        """""", parameters)\n\n        info (conn, ""Discarded ms_cliques"", """"""\n        SELECT passage, labez_clique (labez, clique) AS old_clique, ARRAY_AGG (hsnr ORDER BY hsnr) AS hsnr\n        FROM import_ms_cliques\n        WHERE (pass_id IS NULL OR ms_id IS NULL) AND UPPER_INF (sys_period)\n        GROUP BY passage, labez, clique\n        ORDER BY passage, labez, clique\n        """""", parameters)\n\n        execute (conn, """"""\n        DELETE FROM import_ms_cliques\n        WHERE pass_id IS NULL OR ms_id IS NULL\n        """""", parameters)\n\n    with db.engine.begin () as conn:\n        execute (conn, """"""\n        ALTER TABLE ms_cliques DISABLE TRIGGER ms_cliques_trigger;\n\n        INSERT INTO ms_cliques AS u (ms_id, pass_id, labez, clique,\n                                     sys_period, user_id_start, user_id_stop)\n        SELECT ms_id, pass_id, labez, clique,\n               sys_period, user_id_start, user_id_stop\n        FROM import_ms_cliques\n        WHERE UPPER_INF (sys_period);\n\n        INSERT INTO ms_cliques_tts AS u (ms_id, pass_id, labez, clique,\n                                         sys_period, user_id_start, user_id_stop)\n        SELECT ms_id, pass_id, labez, clique,\n               sys_period, user_id_start, user_id_stop\n        FROM import_ms_cliques\n        WHERE NOT UPPER_INF (sys_period);\n\n        -- default entries\n        INSERT INTO ms_cliques AS u (ms_id, pass_id, labez, clique, user_id_start)\n        SELECT a.ms_id, a.pass_id, a.labez, \'1\', 0\n        FROM apparatus a\n        ON CONFLICT (ms_id, pass_id, labez) DO NOTHING;\n\n        ALTER TABLE ms_cliques ENABLE TRIGGER ms_cliques_trigger;\n        """""", parameters)\n\n\n    log (logging.INFO, ""Loading locstem ..."")\n\n    values = []\n    for row in tree.xpath (\'/sql/export_locstem/row\'):\n        values.append ({ e.tag : e.text for e in row })\n\n    with db.engine.begin () as conn:\n        execute (conn, """"""\n        TRUNCATE import_locstem;\n        """""", parameters)\n\n        executemany (conn, """"""\n        INSERT INTO import_locstem (passage, labez, clique, source_labez, source_clique,\n                                   sys_period, user_id_start, user_id_stop)\n        VALUES (:passage, :labez, :clique, :source_labez, :source_clique,\n                :sys_period, :user_id_start, :user_id_stop)\n        """""", parameters, values)\n\n        # set the pass_id\n        execute (conn, """"""\n        UPDATE import_locstem u\n        SET pass_id = p.pass_id\n        FROM passages p\n        WHERE u.passage = p.passage;\n        """""", parameters)\n\n        warn (conn, ""Discarded Passages"", """"""\n        SELECT passage AS old_passage,\n               ARRAY_AGG (labez_clique (labez, clique) ORDER BY labez, clique) AS old_clique\n        FROM import_locstem\n        WHERE pass_id IS NULL AND UPPER_INF (sys_period)\n        GROUP BY passage\n        ORDER BY passage\n        """""", parameters)\n\n        execute (conn, """"""\n        DELETE FROM import_locstem\n        WHERE pass_id IS NULL;\n        """""", parameters)\n\n        # list all new passages\n        warn (conn, ""New Passages"", """"""\n        SELECT passage AS new_passage,\n               ARRAY_AGG (DISTINCT labez ORDER BY labez) AS new_labez\n        FROM readings_view\n        WHERE pass_id NOT IN (\n          SELECT pass_id\n          FROM import_locstem\n        )\n        AND labez !~ \'^zz\'\n        GROUP BY passage\n        ORDER BY passage\n        """""", parameters)\n\n        # list all new readings (except where already listed as passage before)\n        warn (conn, ""New Readings"", """"""\n        SELECT passage AS old_passage,\n               ARRAY_AGG (DISTINCT labez ORDER BY labez) AS new_labez\n        FROM readings_view\n        WHERE pass_id IN (\n          SELECT pass_id\n          FROM import_locstem\n        )\n        AND (pass_id, labez) NOT IN (\n          SELECT pass_id, labez\n          FROM import_locstem\n        )\n        AND labez !~ \'^z[u-z]\'\n        GROUP BY passage\n        ORDER BY passage\n        """""", parameters)\n\n        # delete obsolete stuff\n        execute (conn, """"""\n        DELETE FROM import_locstem\n        WHERE (pass_id, labez, clique) NOT IN (\n          SELECT pass_id, labez, clique\n          FROM cliques\n        );\n\n        DELETE FROM import_locstem\n        WHERE source_labez NOT IN (\'*\', \'?\')\n        AND (pass_id, source_labez, source_clique) NOT IN (\n          SELECT pass_id, labez, clique\n          FROM cliques\n        );\n        """""", parameters)\n\n\n    with db.engine.begin () as conn:\n        execute (conn, """"""\n        ALTER TABLE locstem DISABLE TRIGGER locstem_trigger;\n\n        INSERT INTO locstem AS u (pass_id, labez, clique, source_labez, source_clique,\n                                  sys_period, user_id_start, user_id_stop)\n        SELECT i.pass_id, i.labez, i.clique, i.source_labez, i.source_clique,\n               i.sys_period, i.user_id_start, i.user_id_stop\n        FROM import_locstem i\n        WHERE UPPER_INF (i.sys_period);\n\n        INSERT INTO locstem_tts AS u (pass_id, labez, clique, source_labez, source_clique,\n                                     sys_period, user_id_start, user_id_stop)\n        SELECT i.pass_id, i.labez, i.clique, i.source_labez, i.source_clique,\n               i.sys_period, i.user_id_start, i.user_id_stop\n        FROM import_locstem i\n        WHERE NOT UPPER_INF (i.sys_period);\n\n        -- insert default dependency: \'a1\' is original\n        INSERT INTO locstem (pass_id, labez, clique, source_labez, source_clique, user_id_start)\n        SELECT pass_id, \'a\', \'1\', \'*\', \'1\', 0\n        FROM cliques q\n        WHERE labez = \'a\' AND clique = \'1\'\n        AND (pass_id, labez, clique) NOT IN (\n          SELECT pass_id, labez, clique\n          FROM locstem\n        );\n\n        -- insert default dependency: not \'a1\' depends on \'a1\'\n        INSERT INTO locstem (pass_id, labez, clique, source_labez, source_clique, user_id_start)\n        SELECT pass_id, labez, clique, \'?\', \'1\', 0\n        FROM cliques q\n        WHERE NOT (labez = \'a\' AND clique = \'1\') AND labez !~ \'^z[u-z]\'\n        -- on conflict do nothing would not work here because\n        -- one reading may depend on arbitrary many readings\n        -- and we\'d just make everybody also depend on \'a\'\n        AND (pass_id, labez, clique) NOT IN (\n          SELECT pass_id, labez, clique\n          FROM locstem\n        );\n\n        ALTER TABLE locstem ENABLE TRIGGER locstem_trigger;\n        """""", parameters)\n\n\n    log (logging.INFO, ""Loading notes ..."")\n\n    with db.engine.begin () as conn:\n        values = []\n        for row in tree.xpath (\'/sql/export_notes/row\'):\n            values.append ({ e.tag : e.text for e in row })\n\n        execute (conn, """"""\n        TRUNCATE import_notes;\n        """""", parameters)\n\n        executemany (conn, """"""\n        INSERT INTO import_notes (passage, note,\n                                  sys_period, user_id_start, user_id_stop)\n        VALUES (:passage, COALESCE (:note, \'\'),\n                :sys_period, :user_id_start, :user_id_stop)\n        """""", parameters, values)\n\n        execute (conn, """"""\n        UPDATE import_notes u\n        SET pass_id = p.pass_id\n        FROM passages p\n        WHERE u.passage = p.passage\n        """""", parameters)\n\n        warn (conn, ""Discarded Notes"", """"""\n        SELECT passage, note\n        FROM import_notes\n        WHERE pass_id IS NULL AND UPPER_INF (sys_period)\n        ORDER BY passage\n        """""", parameters)\n\n        execute (conn, """"""\n        DELETE FROM import_notes\n        WHERE pass_id IS NULL\n        """""", parameters)\n\n    with db.engine.begin () as conn:\n        execute (conn, """"""\n        ALTER TABLE notes DISABLE TRIGGER notes_trigger;\n\n        INSERT INTO notes AS u (pass_id, note,\n                                sys_period, user_id_start, user_id_stop)\n        SELECT pass_id, note,\n               sys_period, user_id_start, user_id_stop\n        FROM import_notes\n        WHERE UPPER_INF (sys_period);\n\n        INSERT INTO notes_tts AS u (pass_id, note,\n                                    sys_period, user_id_start, user_id_stop)\n        SELECT pass_id, note,\n               sys_period, user_id_start, user_id_stop\n        FROM import_notes\n        WHERE NOT UPPER_INF (sys_period);\n\n        ALTER TABLE notes ENABLE TRIGGER notes_trigger;\n        """""", parameters)\n\n\n    log (logging.INFO, ""Done"")\n'"
scripts/cceh/load_edits_old_format.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""Load a saved state of the editor tables.\n\nThis script loads the state of the tables with the editorial decisions as saved\nby the save_edits.py script.  It does not touch the apparatus tables.\n\nWhile loading, it checks for errors and discrepancies, eg. different passage\naddresses.  Passages in the apparatus that are not in the save state are reset\nto the default of: reading \'a\' is original and every other reading is derived\nfrom \'a\'.\n\n""""""\n\nimport argparse\nimport logging\nimport sys\n\nimport lxml\n\nfrom ntg_common import db\nfrom ntg_common import db_tools\nfrom ntg_common.db_tools import execute, executemany, warn\nfrom ntg_common.tools import log\nfrom ntg_common.config import args, init_logging, config_from_pyfile\n\n\ndef build_parser ():\n    parser = argparse.ArgumentParser (description = __doc__)\n\n    parser.add_argument (\'-v\', \'--verbose\', dest=\'verbose\', action=\'count\',\n                         help=\'increase output verbosity\', default=0)\n    parser.add_argument (\'-i\', \'--input\', metavar=\'path/to/input.xml\',\n                         help=""the input file (required)"", required=True)\n    parser.add_argument (\'profile\', metavar=\'path/to/file.conf\',\n                         help=""a .conf file (required)"")\n    return parser\n\n\nif __name__ == \'__main__\':\n\n    build_parser ().parse_args (namespace = args)\n    config = config_from_pyfile (args.profile)\n\n    init_logging (\n        args,\n        logging.StreamHandler (), # stderr\n        logging.FileHandler (\'load_edits.log\')\n    )\n\n    parameters = dict ()\n    db = db_tools.PostgreSQLEngine (**config)\n\n    tree = lxml.etree.parse (args.input if args.input != \'-\' else sys.stdin)\n\n    with db.engine.begin () as conn:\n        db_tools.truncate_editor_tables (conn)\n\n        log (logging.INFO, ""Build default cliques ..."")\n        db_tools.init_default_cliques (conn)\n        log (logging.INFO, ""Build default ms_cliques ..."")\n        db_tools.init_default_ms_cliques (conn)\n        log (logging.INFO, ""Build default locstem ..."")\n        db_tools.init_default_locstem (conn)\n        # default notes is an empty table\n\n    log (logging.INFO, ""Loading cliques ..."")\n\n    with db.engine.begin () as conn:\n        values = []\n        for row in tree.xpath (\'/sql/export_cliques/row\'):\n            values.append ({ e.tag : e.text for e in row })\n\n        execute (conn, """"""\n        TRUNCATE import_cliques;\n        """""", parameters)\n\n        executemany (conn, """"""\n        INSERT INTO import_cliques (passage, labez, clique,\n                                    sys_period, user_id_start, user_id_stop)\n        VALUES (:passage, :labez, :clique,\n                :sys_period, :user_id_start, :user_id_stop)\n        """""", parameters, values)\n\n        execute (conn, """"""\n        UPDATE import_cliques u\n        SET pass_id = r.pass_id\n        FROM readings_view r\n        WHERE (u.passage, u.labez) = (r.passage, r.labez)\n        """""", parameters)\n\n        warn (conn, ""Discarded cliques"", """"""\n        SELECT passage, labez, clique\n        FROM import_cliques\n        WHERE pass_id IS NULL\n        ORDER BY passage, labez, clique\n        """""", parameters)\n\n        execute (conn, """"""\n        DELETE FROM import_cliques\n        WHERE pass_id IS NULL\n        """""", parameters)\n\n    with db.engine.begin () as conn:\n        execute (conn, """"""\n        ALTER TABLE cliques DISABLE TRIGGER cliques_trigger;\n\n        INSERT INTO cliques (pass_id, labez, clique,\n                             sys_period, user_id_start, user_id_stop)\n        SELECT pass_id, labez, clique,\n               sys_period, user_id_start, user_id_stop\n        FROM import_cliques\n        WHERE UPPER_INF (sys_period);\n\n        INSERT INTO cliques_tts (pass_id, labez, clique,\n                                 sys_period, user_id_start, user_id_stop)\n        SELECT pass_id, labez, clique,\n               sys_period, user_id_start, user_id_stop\n        FROM import_cliques\n        WHERE NOT UPPER_INF (sys_period);\n\n        ALTER TABLE cliques ENABLE TRIGGER cliques_trigger;\n        """""", parameters)\n\n\n    log (logging.INFO, ""Loading ms_cliques ..."")\n\n    with db.engine.begin () as conn:\n        values = []\n        for row in tree.xpath (\'/sql/export_ms_cliques/row\'):\n            values.append ({ e.tag : e.text for e in row })\n\n        execute (conn, """"""\n        TRUNCATE import_ms_cliques;\n        """""", parameters)\n\n        executemany (conn, """"""\n        INSERT INTO import_ms_cliques (hsnr, passage, labez, clique,\n                                       sys_period, user_id_start, user_id_stop)\n        VALUES (:hsnr, :passage, :labez, :clique,\n                :sys_period, :user_id_start, :user_id_stop)\n        """""", parameters, values)\n\n        # do not refer to cliques_view as that could kill entries in the history\n        execute (conn, """"""\n        UPDATE import_ms_cliques u\n        SET ms_id = a.ms_id, pass_id = a.pass_id\n        FROM apparatus_view a\n        WHERE (u.passage, u.labez, u.hsnr) = (a.passage, a.labez, a.hsnr)\n        """""", parameters)\n\n        warn (conn, ""Discarded ms_cliques"", """"""\n        SELECT hsnr, passage, labez, clique\n        FROM import_ms_cliques\n        WHERE pass_id IS NULL OR ms_id IS NULL\n        ORDER BY hsnr, passage, labez, clique\n        """""", parameters)\n\n        execute (conn, """"""\n        DELETE FROM import_ms_cliques\n        WHERE pass_id IS NULL OR ms_id IS NULL\n        """""", parameters)\n\n    with db.engine.begin () as conn:\n        execute (conn, """"""\n        ALTER TABLE ms_cliques DISABLE TRIGGER ms_cliques_trigger;\n\n        INSERT INTO ms_cliques AS u (ms_id, pass_id, labez, clique,\n                                     sys_period, user_id_start, user_id_stop)\n        SELECT ms_id, pass_id, labez, clique,\n               sys_period, user_id_start, user_id_stop\n        FROM import_ms_cliques\n        WHERE UPPER_INF (sys_period)\n        ON CONFLICT (ms_id, pass_id, labez) DO\n        UPDATE\n        SET (clique, sys_period, user_id_start, user_id_stop) =\n            (EXCLUDED.clique, EXCLUDED.sys_period, EXCLUDED.user_id_start, EXCLUDED.user_id_stop)\n        WHERE (u.ms_id, u.pass_id, u.labez) = (EXCLUDED.ms_id, EXCLUDED.pass_id, EXCLUDED.labez);\n\n        INSERT INTO ms_cliques_tts AS u (ms_id, pass_id, labez, clique,\n                                         sys_period, user_id_start, user_id_stop)\n        SELECT ms_id, pass_id, labez, clique,\n               sys_period, user_id_start, user_id_stop\n        FROM import_ms_cliques\n        WHERE NOT UPPER_INF (sys_period);\n\n        ALTER TABLE ms_cliques ENABLE TRIGGER ms_cliques_trigger;\n        """""", parameters)\n\n\n    log (logging.INFO, ""Loading locstem ..."")\n\n    with db.engine.begin () as conn:\n        values = []\n        for row in tree.xpath (\'/sql/export_locstem/row\'):\n            values.append ({ e.tag : e.text for e in row })\n\n        # fix schema changed in #79\n        for v in values:\n            if v[\'source_labez\'] is None:\n                v[\'source_labez\']  = \'*\' if v[\'original\'] == \'true\' else \'?\'\n                v[\'source_clique\'] = \'1\'\n\n        execute (conn, """"""\n        TRUNCATE import_locstem;\n        """""", parameters)\n\n        executemany (conn, """"""\n        INSERT INTO import_locstem (passage, labez, clique, source_labez, source_clique,\n                                   sys_period, user_id_start, user_id_stop)\n        VALUES (:passage, :labez, :clique, :source_labez, :source_clique,\n                :sys_period, :user_id_start, :user_id_stop)\n        """""", parameters, values)\n\n        # do not refer to cliques_view as that could kill entries in the history\n        execute (conn, """"""\n        UPDATE import_locstem u\n        SET pass_id = r.pass_id\n        FROM readings_view r\n        WHERE (u.passage, u.labez) = (r.passage, r.labez)\n        """""", parameters)\n\n        warn (conn, ""Discarded LocStem"", """"""\n        SELECT passage, labez, clique\n        FROM import_locstem\n        WHERE pass_id IS NULL\n        ORDER BY passage, labez, clique\n        """""", parameters)\n\n        execute (conn, """"""\n        DELETE FROM import_locstem\n        WHERE pass_id IS NULL\n        """""", parameters)\n\n        execute (conn, """"""\n        UPDATE locstem\n        SET source_labez = \'?\'\n        WHERE pass_id = 1477\n        """""", parameters)\n\n    with db.engine.begin () as conn:\n        execute (conn, """"""\n        ALTER TABLE locstem DISABLE TRIGGER locstem_trigger;\n        CREATE UNIQUE INDEX ix_locstem_import ON locstem (pass_id, labez, clique);\n\n        INSERT INTO locstem AS u (pass_id, labez, clique, source_labez, source_clique,\n                                  sys_period, user_id_start, user_id_stop)\n        SELECT pass_id, labez, clique, source_labez, source_clique,\n               sys_period, user_id_start, user_id_stop\n        FROM import_locstem\n        WHERE UPPER_INF (sys_period)\n        ORDER BY source_labez DESC\n        ON CONFLICT (pass_id, labez, clique) DO\n        UPDATE\n        SET (source_labez, source_clique, sys_period, user_id_start, user_id_stop) =\n            (EXCLUDED.source_labez, EXCLUDED.source_clique,\n             EXCLUDED.sys_period, EXCLUDED.user_id_start, EXCLUDED.user_id_stop)\n        WHERE (u.pass_id, u.labez, u.clique) = (EXCLUDED.pass_id, EXCLUDED.labez, EXCLUDED.clique);\n\n        INSERT INTO locstem_tts AS u (pass_id, labez, clique, source_labez, source_clique,\n                                     sys_period, user_id_start, user_id_stop)\n        SELECT pass_id, labez, clique, source_labez, source_clique,\n               sys_period, user_id_start, user_id_stop\n        FROM import_locstem\n        WHERE NOT UPPER_INF (sys_period);\n\n        DROP INDEX ix_locstem_import;\n        ALTER TABLE locstem ENABLE TRIGGER locstem_trigger;\n        """""", parameters)\n\n\n    log (logging.INFO, ""Loading notes ..."")\n\n    with db.engine.begin () as conn:\n        values = []\n        for row in tree.xpath (\'/sql/export_notes/row\'):\n            values.append ({ e.tag : e.text for e in row })\n\n        execute (conn, """"""\n        TRUNCATE import_notes;\n        """""", parameters)\n\n        executemany (conn, """"""\n        INSERT INTO import_notes (passage, note,\n                                  sys_period, user_id_start, user_id_stop)\n        VALUES (:passage, COALESCE (:note, \'\'),\n                :sys_period, :user_id_start, :user_id_stop)\n        """""", parameters, values)\n\n        execute (conn, """"""\n        UPDATE import_notes u\n        SET pass_id = p.pass_id\n        FROM passages p\n        WHERE u.passage = p.passage\n        """""", parameters)\n\n        warn (conn, ""Discarded Notes"", """"""\n        SELECT passage\n        FROM import_notes\n        WHERE pass_id IS NULL\n        ORDER BY passage\n        """""", parameters)\n\n        execute (conn, """"""\n        DELETE FROM import_notes\n        WHERE pass_id IS NULL\n        """""", parameters)\n\n    with db.engine.begin () as conn:\n        execute (conn, """"""\n        ALTER TABLE notes DISABLE TRIGGER notes_trigger;\n\n        INSERT INTO notes AS u (pass_id, note,\n                                sys_period, user_id_start, user_id_stop)\n        SELECT pass_id, note,\n               sys_period, user_id_start, user_id_stop\n        FROM import_notes\n        WHERE UPPER_INF (sys_period);\n\n        INSERT INTO notes_tts AS u (pass_id, note,\n                                    sys_period, user_id_start, user_id_stop)\n        SELECT pass_id, note,\n               sys_period, user_id_start, user_id_stop\n        FROM import_notes\n        WHERE NOT UPPER_INF (sys_period);\n\n        ALTER TABLE notes ENABLE TRIGGER notes_trigger;\n        """""", parameters)\n\n\n    log (logging.INFO, ""Done"")\n'"
scripts/cceh/mk_users.py,0,"b'#!/usr/bin/python3\n# -*- encoding: utf-8 -*-\n\n""""""Initialize the database for user authentication and authorization.""""""\n\nimport argparse\nimport datetime\nimport logging\n\nfrom passlib.context import CryptContext\n\nfrom ntg_common import db\nfrom ntg_common import db_tools\nfrom ntg_common.db_tools import execute\nfrom ntg_common.tools import log\nfrom ntg_common.config import args, init_logging, config_from_pyfile\n\n\ndef build_parser ():\n    parser = argparse.ArgumentParser (description = __doc__)\n\n    parser.add_argument (\'profile\', metavar=\'path/to/_global.conf\',\n                         help=""path to the _global.conf file (required)"")\n\n    parser.add_argument (\'-e\', \'--email\',    required = True, help=\'email\')\n    parser.add_argument (\'-u\', \'--username\', default = \'\',    help=\'username\')\n    parser.add_argument (\'-p\', \'--password\', default = \'\',    help=\'password\')\n\n    parser.add_argument (\'-v\', \'--verbose\', dest=\'verbose\', action=\'count\',\n                         help=\'increase output verbosity\', default=0)\n    return parser\n\n\nif __name__ == \'__main__\':\n\n    build_parser ().parse_args (namespace = args)\n    config = config_from_pyfile (args.profile)\n\n    init_logging (\n        args,\n        logging.StreamHandler (), # stderr\n        logging.FileHandler (\'mk_users.log\')\n    )\n\n    dba = db_tools.PostgreSQLEngine (**config)\n\n    db.Base3.metadata.drop_all   (dba.engine)\n    db.Base3.metadata.create_all (dba.engine)\n\n    pwd_context = CryptContext (schemes = [ config[\'USER_PASSWORD_HASH\'] ])\n\n    with dba.engine.begin () as src:\n        execute (src, ""INSERT INTO role (id, name, description) VALUES (1, \'admin\',  \'Administrator\')"", {})\n        execute (src, ""INSERT INTO role (id, name, description) VALUES (2, \'editor\', \'Editor\')"", {})\n\n        if args.email:\n            params = {\n                ""username""     : args.username,\n                ""email""        : args.email,\n                ""password""     : pwd_context.hash (args.password) if args.password else \'\',\n                ""active""       : True,\n                ""confirmed_at"" : datetime.datetime.now ()\n            }\n\n            execute (src,\n                     ""INSERT INTO \\""user\\"" (id, username, email, password, active, confirmed_at) "" +\n                     ""VALUES (1, :username, :email, :password, :active, :confirmed_at)"",\n                     params)\n            execute (src, ""INSERT INTO roles_users (id, user_id, role_id) VALUES (1, 1, 1)"", {})\n            execute (src, ""INSERT INTO roles_users (id, user_id, role_id) VALUES (2, 1, 2)"", {})\n'"
scripts/cceh/prepare.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""Initialize a CBGM database.\n\nThis script converts the databases structure used in the production of the *ECM*\ninto a database structure suitable for doing CBGM. It\n\n- normalizes the databases,\n- removes manuscripts, passages and readings irrelevant to the CBGM,\n- builds a positive apparatus from the negative apparatus,\n- reconstructs the `mt`.\n\nDatabase normalization is the usual process of restructuring your tables so they\ndon\'t contain redundant data.\n\nThe database must then be purged from all readings that are relevant for the\n*ECM* and the *Nestle-Aland* only, but not for the CBGM, eg. all passages\nwithout variants (about 2/3 of the New Testament), all corrections except those\nby the first hand, and readings that are clearly orthographic errors or\ndiffering only by orthgographic convention.\n\nThe script then `transforms <transform-positive>` the negative apparatus into a\npositive apparatus, that is, an apparatus that is defined for all manuscripts at\nall passages.\n\nFinally the script reconstructs the `mt`.\n\nAfter running this script you should run the `cbgm.py` script.\n\n   Ausgangspunkt ist der Apparat mit allen f\xc3\xbcr die Druckfassung notwendigen\n   Informationen.  Diese Datenbasis muss f\xc3\xbcr die CBGM bearbeitet werden.  Die\n   Ausgangsdaten stellen einen negativen Apparat dar, d.h. die griechischen\n   handschriftlichen Zeugen, die mit dem rekonstruierten Ausgangstext\n   \xc3\xbcbereinstimmen, werden nicht ausdr\xc3\xbccklich aufgelistet.  Aufgelistet werden\n   alle Zeugen, die von diesem Text abweichen bzw. Korrekturen oder\n   Alternativlesarten haben.  Ziel ist es, einen positiven Apparat zu erhalten.\n   Wir ben\xc3\xb6tigen einen Datensatz pro griechischem handschriftlichen Zeugen\n   erster Hand und variierten Stelle (einschlie\xc3\x9flich der L\xc3\xbccken).  D.h. f\xc3\xbcr jede\n   variierte Stelle liegt die explizite Information vor, ob die Handschrift dem\n   Ausgangstext folgt, einen anderen Text oder gar keinen Text hat, weil\n   z.B. die Seite besch\xc3\xa4digt ist.  Korrekturen oder Alternativlesarten werden\n   f\xc3\xbcr die CBGM ignoriert.\n\n   -- ArbeitsablaufCBGMApg_Db.docx\n\n""""""\n\nimport argparse\nimport collections\nimport html\nimport itertools\nimport logging\nimport operator\nimport re\nimport sys\n\nimport sqlalchemy\n\nfrom ntg_common import db\nfrom ntg_common import tools\nfrom ntg_common import db_tools\nfrom ntg_common.db_tools import execute, executemany, executemany_raw, warn, debug, fix\nfrom ntg_common.tools import log\nfrom ntg_common.config import args, init_logging, config_from_pyfile\n\nMS_ID_MT = 2\n\nbook = None\n\n\nMISFORMED_LABEZ_TEST = """"""\nSELECT labez, labezsuf, adr2chapter (begadr) AS chapter, count (*) AS count\nFROM att\nWHERE labez !~ :re_labez\nGROUP BY labez, labezsuf, adr2chapter (begadr)\nORDER BY labez, labezsuf, adr2chapter (begadr)\n""""""\n\nMISFORMED_HS_TEST = """"""\nSELECT hs, hsnr, adr2chapter (begadr) AS chapter, count (*) AS count\nFROM {t}\nWHERE hs !~ :re_hs_t\nGROUP BY hsnr, hs, adr2chapter (begadr)\nORDER BY hsnr, hs, adr2chapter (begadr)\n""""""\n\nHS_TO_HSNR_TEST = """"""\nSELECT hs, array_agg (DISTINCT hsnr) AS hsnr\nFROM att\nGROUP BY hs\nHAVING count (DISTINCT hsnr) > 1\n""""""\n\nHSNR_TO_HS_TEST = """"""\nSELECT hsnr, array_agg (DISTINCT hs) AS hs\nFROM att\nGROUP BY hsnr\nHAVING count (DISTINCT hs) > 1\n""""""\n\nLABEZ_TO_LESART_TEST = """"""\nSELECT begadr, endadr, lesart, array_agg (DISTINCT labez) AS labez\nFROM att\nWHERE labez !~ \'^z[u-z]\' AND labezsuf = \'\'\nGROUP BY begadr, endadr, lesart\nHAVING COUNT (DISTINCT labez) > 1\n""""""\n\nLESART_TO_LABEZ_TEST = """"""\nSELECT begadr, endadr, labez, array_agg (DISTINCT lesart) AS lesart\nFROM att\nWHERE labez !~ \'^z[u-z]\' AND labezsuf = \'\'\nGROUP BY begadr, endadr, labez\nHAVING count (DISTINCT lesart) > 1\n""""""\n\nMULTIPLE_HS_CANDIDATES_TEST = """"""\nSELECT begadr, endadr, array_agg (hs order by hs) as hs, array_agg (labez order by hs) as labez\nFROM att\nGROUP BY hsnr, begadr, endadr\nHAVING count(*) > 1\nORDER BY begadr;\n""""""\n\ndef copy_table (conn, source_table, dest_table, where = \'\'):\n    """""" Make a copy of a table. """"""\n\n    if where:\n        where = \'WHERE \' + where\n\n    execute (conn, """"""\n    DROP TABLE IF EXISTS {dest_table};\n    SELECT * INTO {dest_table}\n    FROM {source_table}\n    {where};\n    """""", dict (parameters, dest_table = dest_table, source_table = source_table, where = where))\n\n\ndef copy_att (dba, parameters):\n\n    att_model = sqlalchemy.Table (\'att\', db.Base.metadata)\n    lac_model = sqlalchemy.Table (\'lac\', db.Base.metadata)\n\n    dest_columns_att = set ([c.name.lower () for c in att_model.columns])\n    dest_columns_lac = set ([c.name.lower () for c in lac_model.columns])\n\n    # these columns get special treatment\n    # id is irrelevant, row gets a new id anyway\n    dest_columns_att -= set ((\'id\', ))\n    dest_columns_lac -= set ((\'id\', ))\n\n    dba_meta = sqlalchemy.schema.MetaData (bind = dba.engine)\n    dba_meta.reflect ()\n\n    with dba.engine.begin () as dest:\n\n        if book == \'CL\':\n            execute (dest, """"""\n            UPDATE original_att SET labezsuf = labezsuf || fehler WHERE fehler != \'\'\n            """""", parameters)\n\n        if book == \'2 Samuel\':\n            for source_table in (\'original_att\', \'original_lac\'):\n                # set a dummy hsnr so it will copy into att\n                execute (dest, """"""\n                UPDATE {source_table} SET hsnr = 0\n                """""", dict (parameters, source_table = source_table))\n\n        execute (dest, """"""\n        TRUNCATE att, lac RESTART IDENTITY\n        """""", parameters)\n\n    with dba.engine.begin () as dest:\n\n        for source_table in (\'original_att\', \'original_lac\'):\n            is_lac_table = source_table.endswith (\'lac\')\n\n            dest_table   = \'lac\' if is_lac_table else \'att\'\n            dest_columns = dest_columns_lac  if is_lac_table else dest_columns_att\n\n            source_model = sqlalchemy.Table (source_table, dba_meta, autoload = True)\n            columns = [column.name for column in source_model.columns if column.name.lower () in dest_columns]\n            source_columns = [\'""\' + column + \'""\' for column in columns]\n            dest_columns   = [column.lower ()    for column in columns]\n\n            log (logging.INFO, \'          Copying table %s\' % source_table)\n\n            rows = execute (dest, """"""\n            INSERT INTO {dest_table} ({dest_columns}, passage)\n            SELECT {source_columns}, int4range (begadr, endadr + 1)\n            FROM {source_table} s\n            WHERE endadr >= begadr\n            ON CONFLICT DO NOTHING\n            """""", dict (parameters, source_table = source_table, dest_table = dest_table,\n                       source_columns = \', \'.join (source_columns),\n                       dest_columns = \', \'.join (dest_columns)))\n\n    with dba.engine.begin () as conn:\n        log (logging.INFO, \'          Tweaking tables\')\n        if book == \'John\':\n            # we cannot delete \'A\' even if he have a positive apparatus because\n            # \'A\' holds one reading not found in any collated ms.\n\n            # fix MT\n            execute (conn, """"""\n            UPDATE att SET hsnr = 1 WHERE hs = \'MT\';\n            """""", parameters)\n\n        if book in (\'Acts\', \'Mark\'):\n            # we cannot delete \'A\' because in a negative apparatus it holds unique readings.\n            # delete Patristic texts\n            execute (conn, """"""\n            DELETE FROM att WHERE hsnr >= 500000;\n            DELETE FROM lac WHERE hsnr >= 500000;\n            """""", parameters)\n\n        if book == \'2 Samuel\':\n            for t in (\'att\', \'lac\'):\n                execute (conn, """"""\n                UPDATE {t} SET hs = CASE\n                WHEN hs = \'Base-Text_2Sam\'                   THEN \'A\'\n                WHEN hs = \'M_Paris_BN_Coisl.1\'               THEN \'P1\'\n                WHEN hs = \'M_Paris_BN_Coisl.1-C\'             THEN \'P1-C\'\n                WHEN hs = \'V_Rom_Bibl.Vat.,_Vat._gr._2106\'   THEN \'R2106\'\n                WHEN hs = \'V_Rom_Bibl.Vat.,_Vat._gr._2106-C\' THEN \'R2106-C\'\n                WHEN hs ~ \'^02.*-C\'                          THEN \'02-C\'\n                WHEN hs ~ \'^02\'                              THEN \'02\'\n                WHEN hs ~ \'^03.*-C\'                          THEN \'03-C\'\n                WHEN hs ~ \'^03\'                              THEN \'03\'\n                ELSE hs\n                END\n                """""", dict (parameters, t = t))\n\n                execute (conn, """"""\n                UPDATE {t} SET hsnr = CASE\n                WHEN hs = \'A\'               THEN 1\n                WHEN hs ~ \'^P1\'             THEN 2110001\n                WHEN hs ~ \'^R2106\'          THEN 2110002\n                ELSE 2100000 + CAST ((regexp_match (hs, \'^[0-9]+\'))[1] AS INTEGER)\n                END\n                """""", dict (parameters, t = t))\n\n        # make a backup of the original labez\n        execute (conn, """"""\n        UPDATE att\n        SET labezorig = labez, labezsuforig = labezsuf\n        """""", parameters)\n\n        # unify\n        for t in (\'att\', \'lac\'):\n            execute (conn, """"""\n            UPDATE {t}\n            SET lemma    = TRIM (COALESCE (lemma,    \'\')),\n                lesart   = TRIM (COALESCE (lesart,   \'\')),\n                labez    = TRIM (COALESCE (labez,    \'\')),\n                labezsuf = TRIM (COALESCE (labezsuf, \'\'))\n            """""", dict (parameters, t = t))\n\n    # Fix data entry errors.\n    # These errors should be fixed in the original database.\n\n    with dba.engine.begin () as conn:\n        log (logging.INFO, \'          Fixing data entry errors\')\n\n        if book == \'Acts\':\n            for t in (\'att\', \'lac\'):\n                fix (conn, ""Misformed hs Acts"", MISFORMED_HS_TEST, """"""\n                """""", dict (parameters, t = t))\n\n            fix (conn, ""Misformed labez Acts"", MISFORMED_LABEZ_TEST, """"""\n            UPDATE att\n            SET labez = \'a\', labezsuf = \'f\'\n            WHERE labez = \'af\';\n            UPDATE att\n            SET labez = \'c\', labezsuf = \'o\'\n            WHERE labez = \'co\';\n            UPDATE att\n            SET labez = \'d\', labezsuf = \'f\'\n            WHERE labez = \'df\';\n            UPDATE att\n            SET labez = \'a/ao1-4\'\n            WHERE labez = \'a/ao1-ao4\';\n            """""", parameters)\n\n        if book == \'CL\':\n            for t in (\'att\', \'lac\'):\n                fix (conn, ""Misformed hs CL (%s)"" % t, MISFORMED_HS_TEST, """"""\n                UPDATE {t} SET hs = \'2718\'  WHERE hs = \'\'  AND hsnr = 327180;\n                UPDATE {t} SET hs = \'2718s\' WHERE              hsnr = 327181;\n                """""", dict (parameters, t = t))\n\n            execute (conn, """"""\n            UPDATE att SET labezsuf = \'\' WHERE labezsuf = \'(Teil-) L\xc5\xb8cke\';\n            """""", parameters)\n\n            fix (conn, ""More than one labez for lesart CL"", LABEZ_TO_LESART_TEST, """"""\n            UPDATE att\n            SET labez = \'a\'\n            WHERE (begadr, endadr, hs) =  (260105012, 260105020, \'03\');\n            """""", parameters)\n\n            fix (conn, ""More than one lesart for labez CL"", LESART_TO_LABEZ_TEST, """"""\n            """""", parameters)\n\n\n        if book == \'Mark\':\n            # Delete Inscriptio. -- Meeting 28.06.2018\n            execute (conn, """"""\n            DELETE FROM att\n            WHERE (begadr, endadr) = (20000002, 20000004)\n            """""", parameters)\n\n            # Rule: zv should be treated like zz. -- Meeting 28.06.2018\n            execute (conn, """"""\n            UPDATE att\n            SET labez = \'zz\', labezsuf = \'\'\n            WHERE labez ~ \'^zv\'\n            """""", parameters)\n\n            for t in (\'att\', \'lac\'):\n                fix (conn, ""Misformed hs Mark (%s)"" % t, MISFORMED_HS_TEST, r""""""\n                UPDATE {t}\n                SET hs = REPLACE (hs, \'S\', \'s\')\n                WHERE hs ~ \'S\';\n                """""", dict (parameters, t = t))\n\n            fix (conn, ""Misformed labez Mark"", MISFORMED_LABEZ_TEST, r""""""\n            UPDATE att\n            SET labezsuf = REGEXP_REPLACE (labezsuf, \'^(\\d)_f$\', \'_f\\1\')\n            WHERE labezsuf ~ \'^\\d_f$\';\n            UPDATE att\n            SET labez = SUBSTRING (labez, 1, 1), labezsuf = SUBSTRING (labez, 2)\n            WHERE labez ~ \'^[a-y][of][1-9]?$\';\n            UPDATE att\n            SET labez = \'zw\', labezsuf = SUBSTRING (labez, 3) || labezsuf\n            WHERE labez ~ \'^zw[a-y]\';\n            """""", parameters)\n\n\n        if book == \'John\':\n            for t in (\'att\', \'lac\'):\n                fix (conn, ""Misformed hs John"", MISFORMED_HS_TEST, """"""\n                """""", dict (parameters, t = t))\n\n            fix (conn, ""Misformed labez John"", MISFORMED_LABEZ_TEST, r""""""\n            UPDATE att\n            SET labez = REGEXP_REPLACE (labez, \'\\(f\\??\\)\', \'\'), labezsuf = \'f\'\n            WHERE labez ~ \'\\(f\\??\\)\';\n            UPDATE att\n            SET labez = \'zz\'\n            WHERE labez ~ \'\\?\';\n            """""", parameters)\n\n        if book == \'2 Samuel\':\n            for t in (\'att\', \'lac\'):\n                fix (conn, ""Misformed hs 2 Samuel"", MISFORMED_HS_TEST, """"""\n                """""", dict (parameters, t = t))\n\n            fix (conn, ""Misformed labez 2 Samuel"", MISFORMED_LABEZ_TEST, r""""""\n            UPDATE att\n            SET labez = \'c\', labezsuf = \'f\'\n            WHERE labez = \'cf\';\n            UPDATE att\n            SET labez = SUBSTRING (labez, 1, 1) , labezsuf = SUBSTRING (labez, 3)\n            WHERE labez ~ \'^._f$\';\n            UPDATE att\n            SET labez = \'zw\', labezsuf = SUBSTRING (labez, 3)\n            WHERE labez ~ \'^zw.\';\n            """""", parameters)\n\n        execute (conn, """"""\n        UPDATE att\n        SET labez = labezsuf, labezsuf = \'\'\n        WHERE labezsuf ~ \'[-/]\'\n        """""", parameters)\n\n        execute (conn, """"""\n        UPDATE att\n        SET lesart = \'\'\n        WHERE lesart ~ \'^om[.]?$\';\n        """""", parameters)\n\n        if book == \'Acts\':\n            fix (conn, ""Wrong hs Acts"", """"""\n            SELECT hs, hsnr, begadr, endadr\n            FROM att\n            WHERE hs = \'L156s1\'\n            """""", """"""\n            UPDATE att\n            SET hs = \'L156s\'\n            WHERE hs = \'L156s1\' AND begadr = 50311014 AND endadr = 50311014\n            """""", parameters)\n\n            fix (conn, ""Wrong hsnr Acts"", """"""\n            SELECT DISTINCT hs, hsnr, adr2chapter (begadr) AS chapter\n            FROM att\n            WHERE hsnr = 411881 AND hs !~ \'s[1-9]?\'\n               OR hsnr = 411880 AND hs  ~ \'s[1-9]?\'\n            """""", """"""\n            UPDATE att SET hsnr = 411881 WHERE hsnr = 411880 AND hs ~ \'[Ss]\';\n            DELETE FROM lac WHERE hsnr = 411882;\n            UPDATE lac SET hs = REGEXP_REPLACE (hs, \'[Ss][1-2]*\', \'s\') WHERE hsnr = 411881;\n            """""", parameters)\n\n            fix (conn, ""A reads \'f\'"", """"""\n            SELECT begadr, endadr, hs, hsnr, labez, labezsuf\n            FROM att\n            WHERE hs = \'A\' and labez = \'f\'\n            """""", """"""\n            DELETE FROM att\n            WHERE hs = \'A\' and labez = \'f\'\n            """""", parameters)\n\n            fix (conn, ""More than one labez for lesart Acts"", LABEZ_TO_LESART_TEST, """"""\n            """""", parameters)\n\n            fix (conn, ""More than one lesart for labez Acts"", LESART_TO_LABEZ_TEST, """"""\n            UPDATE att\n            SET labez = \'p\'\n            WHERE (begadr, endadr, hs) =  (52621006, 52621010, \'431\');\n            """""", parameters)\n\n\n        if book == \'John\':\n            fix (conn, ""More than one labez for lesart John"", LABEZ_TO_LESART_TEST, """"""\n            UPDATE att\n            SET labez = \'i\'\n            WHERE (begadr, endadr) = (40206008, 40206024) AND labez ~ \'^i[12]$\';\n            """""", parameters)\n\n            fix (conn, ""More than one lesart for labez John"", LESART_TO_LABEZ_TEST, """"""\n            """""", parameters)\n\n\n    with dba.engine.begin () as conn:\n        if book == \'Acts\':\n            # Clean up the lacunae table.\n            # Any errors in the lacunae table will wreak havoc with lacunae unrolling.\n\n            debug (conn, ""nested lacunae"", """"""\n            SELECT l.id, l.hs, l.begadr, l.endadr\n            FROM lac l\n            JOIN lac l2\n              ON l.hs = l2.hs AND l.passage != l2.passage AND l.passage <@ l2.passage\n            """""", parameters)\n\n            # Lac with begadr > endadr\n            fix (conn, ""Lac with begadr > endadr"", """"""\n            SELECT *\n            FROM lac\n            WHERE begadr > endadr\n            """""", """"""\n            UPDATE lac\n            SET begadr = endadr, endadr = begadr\n            WHERE begadr > endadr\n            """""", parameters)\n\n            # Check consistency between Att and Lac tables\n            fix (conn, ""Manuscripts found in lac table but not in att table"", """"""\n            SELECT hsnr, array_agg (adr2chapter (begadr) ORDER BY begadr) as chapters\n            FROM lac\n            WHERE hsnr NOT IN (\n              SELECT DISTINCT hsnr FROM att\n            )\n            GROUP BY hsnr\n            """""", """"""\n            DELETE\n            FROM lac\n            WHERE hsnr NOT IN (\n              SELECT DISTINCT hsnr FROM att\n            )\n            """""", parameters)\n\n            # Fix inconsistencies in endadr between Att and Lac\n            fix (conn, ""Chapters shorter in Lac than in Att"", """"""\n            SELECT a.chapter, attend, lacend\n            FROM (SELECT adr2chapter (begadr) AS chapter, MAX (endadr) AS attend FROM att GROUP BY 1) AS a\n            JOIN (SELECT adr2chapter (begadr) AS chapter, MAX (endadr) AS lacend FROM lac GROUP BY 1) AS l\n              USING (chapter)\n            WHERE attend > lacend\n            ORDER BY a.chapter\n            """""", """"""\n            UPDATE lac\n            SET endadr = 50301004\n            WHERE endadr IN (50247036, 50247042);\n            UPDATE lac\n            SET endadr = 50760037\n            WHERE endadr = 50760036;\n            UPDATE lac\n            SET begadr = 51201001\n            WHERE begadr = 51201002;\n            UPDATE lac\n            SET endadr = 52831035\n            WHERE endadr = 52831034;\n            UPDATE lac\n            SET passage = int4range (begadr, endadr + 1);\n            """""", parameters)\n\n\ndef process_commentaries (dba, parameters):\n    """"""Process commentaries\n\n    Commentaries often contain more than one reading of the same passage.  If\n    those readings are different we must degrade them to uncertain status.\n\n    Also promote the manuscript to \'normal\' status by stripping the commentary\n    suffix (in re_comm) from the hs.\n\n        20. Mai 2015.  Commentary manuscripts like 307 cannot be treated like\n        lectionaries where we choose the first text.  If a T1 or T2 reading is\n        found they have to be deleted.  A new zw reading is created containing\n        the old readings as suffix.\n\n        This has to be done as long as both witnesses are present.\n\n        If the counterpart of one entry belongs to the list of lacunae the\n        witness will be treated as normal witness. The T notation can be\n        deleted.\n\n        --prepare4cbgm_5b.py\n\n    """"""\n\n    if \'re_comm\' not in parameters:\n        return\n\n    with dba.engine.begin () as conn:\n\n        # Fix duplicate labez by keeping only non-f readings.  We need this\n        # because we are mixing two different concepts again: An uncertain\n        # reading is *one* reading that may be read in different ways, but a\n        # commentary may well contain *two* different readings that are not\n        # uncertain at all.  The bottom line is: a commentary may offer two\n        # readings with the same labez (one with an \'f\' labezsuf), which will\n        # break the primary key of the Apparatus even if marked as uncertain.\n        execute (conn, """"""\n        DELETE FROM att u\n        WHERE id IN (\n          SELECT id\n          FROM (\n            SELECT id, ROW_NUMBER () OVER (partition BY hsnr, begadr, endadr, labez ORDER BY labezsuf) AS rownum\n            FROM att\n            WHERE hs ~ :re_comm\n          ) t\n          WHERE t.rownum > 1\n        )\n        """""", parameters)\n\n        # Promote manuscript to normal status by stripping T[1-9] from hs.  If\n        # more than one T-reading is found, degrade both readings to uncertain\n        # status.\n        res = execute (conn, """"""\n        UPDATE att u\n        SET hs = REGEXP_REPLACE (hs, :re_comm, \'\'),\n            certainty = 1.0 / cnt\n        FROM (\n          SELECT hsnr, begadr, endadr, count (*) as cnt\n          FROM att a\n          WHERE hs ~ :re_comm\n          GROUP BY hsnr, begadr, endadr\n          HAVING count (*) > 1\n        ) AS t\n        WHERE (u.hsnr, u.begadr, u.endadr) = (t.hsnr, t.begadr, t.endadr)\n        """""", parameters)\n\n        # Promote lacuna to normal status\n        res = execute (conn, """"""\n        UPDATE lac u\n        SET hs = REGEXP_REPLACE (hs, :re_comm, \'\')\n        WHERE hs ~ :re_comm\n        """""", parameters)\n\n\ndef delete_corrector_hands (dba, parameters):\n    """"""Delete later hands\n\n    Delete all corrections except those by the first hand.\n\n        Lesarten l\xc3\xb6schen, die nicht von der ersten Hand stammen.  [...]\n        Ausnahme: Bei Selbstkorrekturen wird die *-Lesart gel\xc3\xb6scht und die\n        C*-Lesart beibehalten.\n\n        --prepare4cbgm_6.py\n\n    """"""\n\n    if \'re_corr\' not in parameters:\n        return\n\n    if \'re_corr_keep\' not in parameters:\n        return\n\n    with dba.engine.begin () as conn:\n        for t in (\'att\', \'lac\'):\n            # Delete all corrections except those by the original hand\n            execute (conn, """"""\n            DELETE FROM {t}\n            WHERE hs ~ :re_corr AND hs !~ :re_corr_keep\n            """""", dict (parameters, t = t))\n\n            # Delete all other readings if there is a C* reading.\n            execute (conn, """"""\n            DELETE FROM {t}\n            WHERE (hsnr, begadr, endadr) IN (\n              SELECT DISTINCT hsnr, begadr, endadr\n              FROM {t}\n              WHERE hs ~  :re_corr_keep AND labez != \'zz\'\n            ) AND   hs !~ :re_corr_keep\n            """""", dict (parameters, t = t))\n\n\ndef delete_lectionaries (dba, parameters):\n    """"""Delete secondary lectionary readings\n\n    Also delete lectionary readings except L1.\n\n        Bei mehreren Lektionslesarten gilt die L1-Lesart.\n\n        --prepare4cbgm_6.py\n\n    """"""\n\n    if \'re_suppress\' not in parameters:\n        return\n\n    with dba.engine.begin () as conn:\n        for t in (\'att\', \'lac\'):\n            execute (conn, """"""\n            DELETE FROM {t}\n            WHERE hs ~ :re_suppress\n            """""", dict (parameters, t = t))\n\n\ndef process_sigla (dba, parameters):\n    """"""Process Sigla\n\n    Rewrite the manuscript sigla (hs) and delete all suffixes.\n\n        Handschriften, die mit einem ""V"" f\xc3\xbcr videtur gekennzeichnet sind, werden\n        ebenso wie alle anderen behandelt.  Das ""V"" kann also getilgt werden.\n        Die Eintragungen f\xc3\xbcr ""urspr\xc3\xbcnglich (*)"" und ""C*"" werden ebenfalls\n        gel\xc3\xb6scht.  Schlie\xc3\x9flich auch die Zus\xc3\xa4tze zur Handschriftennummer wie\n        ""T1"".  Diese Eintragungen werden (bisher) einfach an die\n        Handschriftenbezeichnung angeh\xc3\xa4ngt.\n\n        Der Eintrag \'videtur\', gekennzeichnet durch ein \'V\' hinter der\n        Handschriftennummer, spielt f\xc3\xbcr die CBGM keine Rolle.  Ein eventuell\n        vorhandenes \'V\' muss getilgt werden.  Gleiches gilt f\xc3\xbcr die Eintr\xc3\xa4ge \'*\'\n        und \'C*\'.\n\n        --prepare4cbgm_6b.py\n\n    """"""\n\n    with dba.engine.begin () as conn:\n\n        if book in (\'Acts\', \'CL\'):\n            fix (conn, ""Duplicate readings"", """"""\n            SELECT hs, hsnr, begadr, endadr, labez, labezsuf, lesart FROM att\n            WHERE (hsnr, begadr, endadr) IN (\n               SELECT hsnr, begadr, endadr\n               FROM att\n               WHERE certainty = 1.0\n               GROUP BY hsnr, begadr, endadr\n               HAVING count (*) > 1\n            )\n            ORDER BY begadr, endadr, hsnr, hs\n            """""", """"""\n            DELETE FROM att\n            WHERE (hs, begadr, endadr) = (\'P74\', 50124030, 50125002)\n            """""", parameters)\n\n        if book in (\'Acts\', \'Mark\', \'John\'):\n            warn (conn, ""Hs with more than one hsnr"", HS_TO_HSNR_TEST, parameters)\n\n            # fix duplicate readings by keeping only the alphabetically lowest labez\n            fix (conn, ""More that one candidate Hs"", MULTIPLE_HS_CANDIDATES_TEST, """"""\n            DELETE FROM att\n            WHERE id IN (\n              SELECT id\n              FROM (SELECT id, ROW_NUMBER () OVER (partition BY begadr, endadr, hsnr ORDER BY labez) AS rownum\n                    FROM att) t\n              WHERE t.rownum > 1\n            )\n            """""", parameters)\n\n        if book == \'CL\':\n            fix (conn, ""Hs with more than one hsnr CL"", HS_TO_HSNR_TEST, """"""\n            UPDATE att SET hs = \'1831s\' WHERE hsnr = 318311;\n            UPDATE att SET hs = \'206s\'  WHERE hsnr = 302061;\n            """""", parameters)\n\n    for t in (\'att\', \'lac\'):\n        with dba.engine.begin () as conn:\n            execute (conn, """"""\n            UPDATE {t}\n            SET hs = SUBSTRING (hs, :re_hs)\n            """""", dict (parameters, t = t))\n\n\n    with dba.engine.begin () as conn:\n\n        if book in (\'Acts\', \'CL\'):\n            fix (conn, ""Hsnr with more than one hs Acts"", HSNR_TO_HS_TEST, """"""\n            UPDATE att AS t\n            SET hs = g.minhs\n            FROM (SELECT min (hs) AS minhs, hsnr FROM att GROUP BY hsnr) AS g\n            WHERE t.hsnr = g.hsnr\n            """""", parameters)\n\n        if book == \'John\':\n            fix (conn, ""Hsnr with more than one hs John"", HSNR_TO_HS_TEST, """"""\n            UPDATE att\n            SET hsnr = hsnr + 1\n            WHERE hs ~ \'S\' AND hsnr IN (401410, 406400, 407040, 410000, 410760, 410910, 416920)\n            """""", parameters)\n\n\ndef unroll_zw (dba, parameters):\n    """"""Unroll \'zw\' entries\n\n    When a reading cannot be classified under a labez with absolute certainty,\n    the apparatus sets the labez to \'zw\' (zweifelhaft, dubious) and offers a\n    list of candidate labez in labezsuf.  This list of candidates in labezsuf\n    has to be normalized into multiple table records.\n\n    If :math:`N > 1` labez candidates exists, the certainty of the unrolled\n    records will be set to :math:`1 / N`.\n\n    But if all candidate labez differ only in their errata or ortographica\n    suffix, as in \'a/ao1/ao2\' or \'b/b_f\' then we will output only one record\n    with a certainty of 1.0.\n\n    Caveat: in Mark, labezsuf may contain a list of candidate labez even if\n    labez is not \'zw\'.  This is why we no longer look for \'zw\' in labez but for\n    \'/\' in labezsuf.\n\n        zw-Lesarten der \xc3\xbcbergeordneten Variante zuordnen, wenn ausschliesslich\n        verschiedene Lesarten derselben Variante infrage kommen (z.B. zw a/ao\n        oder b/bo_f).  In diesen F\xc3\xa4llen tritt die Buchstabenkennung der\n        \xc3\xbcbergeordneten Variante in labez an die Stelle von \'zw\'.\n\n        --prepare4cbgm_7.py\n\n    """"""\n\n    with dba.engine.begin () as conn:\n\n        fields = """"""\n        hsnr, hs, begadr, endadr, labez, labezsuf, labezorig, labezsuforig,\n        certainty, lemma, lesart, passage\n        """"""\n\n        Zw = collections.namedtuple (\'Zw\', fields)\n\n        # labez, because labezsuf has been moved into labez in a previous step\n        res = execute (conn, """"""\n        SELECT {fields}\n        FROM att\n        WHERE labez ~ \'[-/]\'\n        """""", dict (parameters, fields = fields))\n\n        zws = list (map (Zw._make, res))\n        updated = 0\n\n        if zws:\n            params_list = [] # accumulator\n            for zw in zws:\n                segments = []\n                for segment in zw.labez.split (\'/\'):\n                    segment = segment.strip (\'?\').strip ()\n                    if len (segment) == 0:\n                        continue\n                    m = re.match (r\'(\\w)-(\\w)\', segment)\n                    if m:\n                        segments += [chr (n) for n in range (ord (m.group (1)), ord (m.group (2)) + 1)]\n                        continue\n                    m = re.match (r\'(\\w+)(\\d)-(\\d)\', segment)\n                    if m:\n                        segments += [m.group (1) + chr (n) for n in range (ord (m.group (2)), ord (m.group (3)) + 1)]\n                        continue\n                    segments.append (segment)\n\n                unique_labez = collections.defaultdict (list)\n                for seg in segments:\n                    unique_labez[seg[0]].append (seg[1:].replace (\'_\', \'\'))\n\n                options = len (unique_labez)\n                if options > 0:\n                    updated += options\n                    certainty = 1.0 / options #  if options > 1 else 0.9\n                    more_params = zw._asdict ()\n                    for k, v in unique_labez.items ():\n                        params_list.append (dict (more_params, labez = k, labezsuf = \'/\'.join (v), certainty = certainty))\n\n            execute (conn, """"""\n            DELETE FROM att\n            WHERE labez ~ \'[-/]\'\n            """""", dict (parameters))\n\n            executemany (conn, """"""\n            INSERT INTO att ({fields})\n            VALUES ({values})\n            """""", dict (parameters, fields = fields, values = \':\' + \', :\'.join (Zw._fields)), params_list)\n\n            # eg. if labezsuf = \'a/b/c/d/e2\', zap the \'2\' until we don\'t know what it means\n            # execute (conn, """"""\n            # UPDATE att\n            # SET labezsuf = \'\'\n            # WHERE labezsuf ~ \'^[0-9]$\';\n            # """""", parameters)\n\n    log (logging.DEBUG, ""          %d \'zw\' rows unrolled"" % updated)\n\n\n\ndef delete_invariant_passages (dba, parameters):\n    """"""Delete invariant passages\n\n    A passage is invariant if all defined manuscripts offer the same\n    (regularized) text.  Invariant passages are irrelevant to the CBGM.\n\n        Stellen l\xc3\xb6schen, an denen nur eine oder mehrere f- oder o-Lesarten vom\n        A-Text abweichen. Hier gibt es also keine Variante.\n\n        Nicht l\xc3\xb6schen, wenn an dieser variierten Stelle eine Variante \'b\' - \'y\'\n        erscheint.\n\n        \xc3\x84nderung 2014-12-16: Act 28,29/22 geh\xc3\xb6rt zu einem Fehlvers.  Dort gibt\n        es u.U. keine Variante neben b, sondern nur ein Orthographicum.  Wir\n        suchen also nicht mehr nach einer Variante \'b\' bis \'y\', sondern z\xc3\xa4hlen\n        die Varianten.  Liefert getReadings nur 1 zur\xc3\xbcck, gibt es keine\n        Varianten.\n\n        --prepare4cbgm_5.py\n\n    """"""\n\n    with dba.engine.begin () as conn:\n\n        execute (conn, """"""\n        DELETE FROM att\n        WHERE (begadr, endadr) IN (\n          SELECT begadr, endadr\n          FROM (\n            SELECT DISTINCT begadr, endadr, labez\n            FROM att\n            WHERE labez !~ \'^z[u-z]\' AND certainty = 1.0\n          ) AS i\n          GROUP BY begadr, endadr\n          HAVING count (*) <= 1\n        )\n        """""", parameters)\n\n\ndef mark_invariant_passages (dba, parameters):\n    """"""Mark invariant passages\n\n    A passage is invariant if all defined manuscripts offer the same\n    (regularized) text.  Invariant passages are irrelevant to the CBGM.\n\n    We need to display the ""Leitzeile"", so we cannot simply delete these\n    passages.  We mark them as invariant instead.\n\n    FIXME: this is moot since we get the Leitzeile from a different database\n    altogether.  OTOH :class:`~ntg_common.db.Att` never contained all passages\n    anyway, so it was impossible to extract the Leitzeile out of it.\n\n        Stellen l\xc3\xb6schen, an denen nur eine oder mehrere f- oder o-Lesarten\n        vom A-Text abweichen. Hier gibt es also keine Variante.\n\n        Nicht l\xc3\xb6schen, wenn an dieser variierten Stelle eine Variante \'b\'\n        - \'y\' erscheint.\n\n        \xc3\x84nderung 2014-12-16: Act 28,29/22 geh\xc3\xb6rt zu einem Fehlvers.  Dort\n        gibt es u.U. keine Variante neben b, sondern nur ein\n        Orthographicum.  Wir suchen also nicht mehr nach einer Variante\n        \'b\' bis \'y\', sondern z\xc3\xa4hlen die Varianten.  Liefert getReadings\n        nur 1 zur\xc3\xbcck, gibt es keine Varianten.\n\n        --prepare4cbgm_5.py\n\n    """"""\n\n    with dba.engine.begin () as conn:\n\n        execute (conn, """"""\n        UPDATE passages p\n        SET variant = False\n        WHERE pass_id IN (\n          SELECT pass_id\n          FROM (\n            SELECT DISTINCT pass_id, labez\n            FROM apparatus\n            WHERE labez !~ \'^z[u-z]\' AND certainty = 1.0\n          ) AS i\n          GROUP BY pass_id\n          HAVING count (*) <= 1\n        )\n        """""", parameters)\n\n\ndef copy_nestle (dbdest, parameters):\n    """"""Make a working copy of the Nestle table.""""""\n\n    with dbdest.engine.begin () as dest:\n        execute (dest, """"""\n        TRUNCATE nestle RESTART IDENTITY\n        """""", parameters)\n\n        execute (dest, """"""\n        INSERT INTO nestle (begadr, endadr, passage, lemma)\n        SELECT adr, adr, int4range (adr, adr + 1), content\n        FROM original_nestle\n        """""", parameters)\n\n\ndef copy_genealogical (dbdest, parameters):\n    """"""Copy / fix genealogical data\n\n        kopiert die Daten aus ECM_Acts_CBGM nach VarGenAtt_Act, zur weiteren\n        Bearbeitung im Stemma-Editor\n\n        --VGA/Att2CBGMPh3.pl\n\n        Kopiert die genealogischen Informationen von einer Phase zur n\xc3\xa4chsten.\n\n        Splitts m\xc3\xbcssen nur dort \xc3\xbcbertragen werden, wo sich der Apparat nicht\n        ge\xc3\xa4ndert hat.  Hat sich der Apparat ge\xc3\xa4ndert, d.h. gibt es in der neuen\n        Phase f\xc3\xbcr eine Adresse keine Entsprechung in der vorhergehenden Phase,\n        so werden die Defaultwerte eingetragen.  Zuerst muss festgestellt\n        werden, wo Splitts oder Zusammenlegungen stattgefunden haben.  Dann\n        werden diese Lesarten gel\xc3\xb6scht und aus der vorhergehenden Phase kopiert.\n\n        Stellen mit ge\xc3\xa4nderter Leitzeile werden zun\xc3\xa4chst einfach \xc3\xbcbergangen.\n\n        Defaultwerte eintragen, wenn eine variierte Stelle mit gleicher\n        numerischer Adresse mehr Lesarten als in der vorhergehenden Phase hat,\n        die nicht nur versionell bezeugt sind.\n\n        -- VGA/PortCBGMInfoPh3.pl\n\n    VarGenAtt_ActPh2 -> VarGenAtt_ActPh3\n\n    1. copy entries from att with default values (every other labez depends on\n       a) (done in the last step)\n\n    2. overwrite with values from locstemed (done in this step)\n\n    FIXME: do we really need step 1? To provide the new passages if the\n    apparatus changed? At least it doesn\'t do any damage because there should be\n    no passages in att that are not in locstemed also.\n\n    """"""\n\n    with dbdest.engine.begin () as dest:\n\n        if \'MYSQL_LOCSTEM_TABLES\' in config:\n            copy_table (dest, \'original_locstemed\', \'tmp_locstemed\', ""varid !~ \'^z[u-z]\'"")\n\n        if \'MYSQL_RDG_TABLES\' in config:\n            copy_table (dest, \'original_rdg\',       \'tmp_rdg\')\n\n        if \'MYSQL_VAR_TABLES\' in config:\n            copy_table (dest, \'original_var\',       \'tmp_var\', ""varid !~ \'^z[u-z]\'"")\n\n        if (book == \'Acts\') and (\'MYSQL_LOCSTEM_TABLES\' in config) and (\'MYSQL_VAR_TABLES\' in config):\n            for table in (\'tmp_locstemed\', \'tmp_var\'):\n                # fix \'cf\' and \'df\' in locstem and var\n                execute (dest, """"""\n                UPDATE {table}\n                SET varid = \'c\', varnew = \'c\'\n                WHERE varnew = \'cf\';\n                UPDATE {table}\n                SET varid = \'d\', varnew = \'d\'\n                WHERE varnew = \'df\';\n                """""", dict (parameters, table = table))\n\n                # fix \'m\' \'n\' \'o\' in locstem and var\n                execute (dest, """"""\n                DELETE FROM {table}\n                WHERE (begadr, endadr, varid) = (52621006, 52621010, \'n\');\n                """""", dict (parameters, table = table))\n\n                for old, new in zip (\'o p\'.split (), \'n o\'.split ()):\n                    execute (dest, """"""\n                    UPDATE {table}\n                    SET varid = :new, varnew = :new\n                    WHERE (begadr, endadr, varid) = (52621006, 52621010, :old);\n                    """""", dict (parameters, old = old, new = new, table = table))\n\n            execute (dest, """"""\n            UPDATE tmp_locstemed\n            SET varid = \'a\', varnew = \'a\', s1 = \'*\'\n            WHERE (begadr, endadr, varnew) = (51413002, 51413044, \'e\')\n            """""", parameters)\n\n            # add missing \'d\' in locstem\n            execute (dest, """"""\n            INSERT INTO tmp_locstemed (id, begadr, endadr, varid, varnew, s1, s2, prs1, prs2, ""check"")\n            VALUES (0, 51313038, 51313038, \'d\', \'d\', \'?\', \'\', \'\', \'\', \'\');\n            UPDATE tmp_var\n            SET s1 = \'?\'\n            WHERE (begadr, endadr, varnew) = (51313038, 51313038, \'d\');\n            """""", parameters)\n\n            # duplicate rows\n            fix (dest, ""Duplicates in LocStem"", """"""\n            SELECT begadr, endadr, varnew\n            FROM tmp_locstemed\n            GROUP BY begadr, endadr, varnew\n            HAVING count (*) > 1\n            """""", """"""\n            DELETE FROM tmp_locstemed\n            WHERE (begadr, endadr, varnew, s1) = (51702028, 51702030, \'c2\', \'?\');\n            DELETE FROM tmp_locstemed\n            WHERE (begadr, endadr, varnew, s1) = (52830006, 52830014, \'d\', \'?\')\n            """""", parameters)\n\n            # duplicate rows\n            fix (dest, ""Duplicates in VarGenAtt"", """"""\n            SELECT begadr, endadr, witn\n            FROM tmp_var\n            GROUP BY witn, begadr, endadr\n            HAVING count (*) > 1\n            """""", """"""\n            DELETE FROM tmp_var\n            WHERE (begadr, endadr) = (52212026, 52212028) AND witn = \'1838\'\n            """""", parameters)\n\n            execute (dest, """"""\n            DELETE FROM tmp_locstemed\n            WHERE (begadr, endadr) = (52209026, 52209034);\n            """""", parameters)\n\n            execute (dest, """"""\n            UPDATE tmp_locstemed\n            SET s1 = \'h1\'\n            WHERE begadr = 50247038 AND endadr = 50301004 AND s1 = \'h\';\n            UPDATE tmp_locstemed\n            SET s1 = \'a1\'\n            WHERE begadr = 50313008 AND endadr = 50313008 AND s1 = \'a\';\n            UPDATE tmp_locstemed\n            SET s1 = \'c1\'\n            WHERE begadr = 50412022 AND endadr = 50412040 AND s1 = \'c\';\n            UPDATE tmp_locstemed\n            SET s1 = \'a1\'\n            WHERE begadr = 50516018 AND endadr = 50516018 AND s1 = \'a\';\n            UPDATE tmp_locstemed\n            SET s1 = \'b1\'\n            WHERE begadr = 51915006 AND endadr = 51915008 AND s1 = \'b\';\n            UPDATE tmp_locstemed\n            SET s1 = \'b1\'\n            WHERE begadr = 52525008 AND endadr = 52525016 AND s1 = \'b\';\n            UPDATE tmp_locstemed\n            SET s1 = \'a\'\n            WHERE begadr = 52507028 AND endadr = 52507028 AND s1 = \'b\';\n            """""", parameters)\n\n            execute (dest, """"""\n            UPDATE tmp_var\n            SET s1 = \'a1\'\n            WHERE begadr = 51314016 AND endadr = 51314022 AND witn = \'2147\';\n\n            UPDATE tmp_var\n            SET varid = \'a\', varnew = \'a1\', s1 = \'*\'\n            WHERE begadr = 51342020 AND endadr = 51342026 AND witn = \'383\';\n            UPDATE tmp_var\n            SET varid = \'b\', varnew = \'b\', s1 = \'a1\'\n            WHERE begadr = 51314016 AND endadr = 51314022 AND witn = \'2718\';\n            UPDATE tmp_var\n            SET varid = \'a\', varnew = \'a2\', s1 = \'?\'\n            WHERE begadr = 50405022 AND endadr = 50405034 AND witn = \'L156s\';\n            UPDATE tmp_var\n            SET varid = \'f\', varnew = \'f\', s1 = \'a\'\n            WHERE begadr = 51201014 and endadr = 51201022 AND witn = \'L1188\';\n            UPDATE tmp_var\n            SET varid = \'b\', varnew = \'b\', s1 = \'a\'\n            WHERE begadr = 51309022 and endadr = 51309022 AND witn = \'L1188\';\n            UPDATE tmp_var\n            SET varid = \'c\', varnew = \'c2\', s1 = \'a1\'\n            WHERE begadr = 51324020 and endadr = 51324026 AND witn = \'L1188\';\n            UPDATE tmp_var\n            SET varid = \'d\', varnew = \'d\', s1 = \'a\'\n            WHERE begadr = 52207002 and endadr = 52207004 AND witn = \'L1188\';\n            """""", parameters)\n\n        # memo\n\n        if \'MYSQL_MEMO_TABLE\' in config:\n            copy_table (dest, \'original_memo\', \'tmp_memo\')\n\n            execute (dest, r""""""\n            UPDATE tmp_memo\n            SET remarks = TRIM (BOTH FROM REGEXP_REPLACE (remarks, \'\\s*\\r?\\n\', E\'\\n\', \'g\'));\n            UPDATE tmp_memo\n            SET remarks = NULL\n            WHERE remarks = \'\'\n            """""", parameters)\n\n            # convert html-escaped characters into utf-8\n            res = execute (dest, """"""\n            SELECT id, remarks\n            FROM tmp_memo\n            WHERE remarks ~ \'&\'\n            """""", parameters)\n\n            params = []\n            for row in res.fetchall ():\n                params.append ([\n                    html.unescape (row[\'remarks\']),\n                    row[\'id\'],\n                ])\n\n            if params:\n                executemany_raw (dest, """"""\n                UPDATE tmp_memo\n                SET remarks = %s\n                WHERE id = %s\n                """""", parameters, params)\n\n            # fix it by keeping only the highest id\n            fix (dest, ""Multiple remarks for passage"", """"""\n            SELECT begadr, endadr, count (*) AS count\n            FROM tmp_memo\n            GROUP BY begadr, endadr\n            HAVING count (*) > 1;\n            """""", """"""\n            DELETE FROM tmp_memo\n            WHERE id IN (\n              SELECT id\n              FROM (SELECT id, ROW_NUMBER () OVER (partition BY begadr, endadr ORDER BY id DESC) AS rownum\n                    FROM tmp_memo) t\n              WHERE t.rownum > 1\n            )\n            """""", parameters)\n\n\ndef fill_passages_table (dba, parameters):\n    """""" Create the Passages table. """"""\n\n    with dba.engine.begin () as conn:\n\n        execute (conn, """"""\n        TRUNCATE books RESTART IDENTITY CASCADE\n        """""", parameters)\n\n        # The Books Table\n\n        Book = collections.namedtuple (\'Book\', \'bk_id siglum book ranges\')\n\n        executemany (conn, """"""\n        INSERT INTO books (bk_id, siglum, book, passage)\n        VALUES (:bk_id, :siglum, :book, int4range (:bk_id * 10000000, (:bk_id + 1) * 10000000))\n        """""", parameters, [ Book._make (b)._asdict () for b in tools.BOOKS if b[3] > 0])\n\n        # The Ranges Table\n\n        params = []\n        for b in map (Book._make, tools.BOOKS):\n            if b.ranges > 0:\n                offset = 10000000 * b.bk_id\n                params.append ([b.bk_id, \'All\', offset, offset + 10000000])\n                for ch in range (1, b.ranges + 1):\n                    params.append ([b.bk_id, str (ch), offset + ch * 100000, offset + ((ch + 1) * 100000)])\n\n        executemany_raw (conn, """"""\n        INSERT INTO ranges (bk_id, range, passage)\n        VALUES (%s, %s, int4range (%s, %s))\n        """""", parameters, params)\n\n        # The Passages Table\n\n        execute (conn, """"""\n        INSERT INTO passages (begadr, endadr, passage, variant, bk_id)\n        SELECT begadr, endadr, int4range (begadr, endadr + 1),\n               True, bk_id\n        FROM att a\n        JOIN books b\n          ON b.passage @> begadr\n        GROUP BY begadr, endadr, int4range (begadr, endadr + 1), bk_id\n        ORDER BY begadr, endadr DESC\n        """""", parameters)\n\n        # Mark Nested Passages\n\n        execute (conn, """"""\n        UPDATE passages p\n        SET spanned = EXISTS (\n          SELECT passage FROM passages o\n          WHERE o.passage @> p.passage AND p.pass_id != o.pass_id\n        ),\n        spanning = EXISTS (\n          SELECT passage FROM passages i\n          WHERE i.passage <@ p.passage AND p.pass_id != i.pass_id\n        )\n        """""", parameters)\n\n        # Mark Fehlverse\n\n        execute (conn, """"""\n        UPDATE passages p\n        SET fehlvers = True\n        WHERE p.spanned AND {fehlverse}\n        """""", dict (parameters, fehlverse = tools.FEHLVERSE))\n\n        execute (conn, """"""\n        UPDATE passages p\n        SET fehlvers = True\n        WHERE passage = \'[51534013,51534014)\';\n        """""", parameters) # not spanned because inserted after the end\n\n        # Notes\n\n        if \'MYSQL_MEMO_TABLE\' in config:\n            # keep only last memo if multiple memos found\n            execute (conn, """"""\n            ALTER TABLE notes DISABLE TRIGGER notes_trigger;\n            INSERT INTO notes (pass_id, note, user_id_start)\n            SELECT p.pass_id, m.remarks, 0\n            FROM tmp_memo m\n            JOIN passages p\n              USING (begadr, endadr)\n            WHERE m.remarks IS NOT NULL\n            ORDER BY m.id DESC\n            ON CONFLICT DO NOTHING;\n            ALTER TABLE notes ENABLE TRIGGER notes_trigger;\n            """""", parameters)\n\n\ndef fill_manuscripts_table (dba, parameters):\n    """""" Create the Manuscripts and Ms_Ranges tables. """"""\n\n    with dba.engine.begin () as conn:\n\n        execute (conn, """"""\n        TRUNCATE manuscripts RESTART IDENTITY CASCADE\n        """""", parameters)\n\n        # The Manuscripts Table\n\n        # ms_id = 1\n        execute (conn, """"""\n        INSERT INTO manuscripts (hs, hsnr) VALUES (\'A\', 0)\n        """""", parameters)\n\n        # ms_id = 2\n        execute (conn, """"""\n        SELECT setval (\'manuscripts_ms_id_seq\', :ms_id_mt, FALSE);\n        INSERT INTO manuscripts (hs, hsnr) VALUES (\'MT\', 1)\n        """""", dict (parameters, ms_id_mt = MS_ID_MT))\n\n        # ms_id = 3, 4, 5, ...\n        execute (conn, """"""\n        INSERT INTO manuscripts (hs, hsnr)\n        SELECT DISTINCT hs, hsnr\n        FROM att\n        WHERE hsnr >= 100000\n        ORDER BY hsnr\n        """""", parameters)\n\n        # Init the Ms_Ranges Table\n\n        execute (conn, """"""\n        INSERT INTO ms_ranges (ms_id, rg_id, length)\n        SELECT ms.ms_id, ch.rg_id, 0\n        FROM manuscripts ms\n        CROSS JOIN ranges ch\n        """""", parameters)\n\n\ndef fill_readings_table (dba, parameters):\n    """""" Create the readings table. """"""\n\n    with dba.engine.begin () as conn:\n\n        execute (conn, """"""\n        TRUNCATE readings RESTART IDENTITY CASCADE\n        """""", parameters)\n\n        execute (conn, """"""\n        INSERT INTO readings (pass_id, labez, lesart)\n        SELECT p.pass_id, a.labez, a.lesart\n        FROM att a\n        JOIN passages p\n        USING (begadr, endadr)\n        WHERE labez != \'zz\' AND certainty = 1.0 AND labezsuf = \'\' AND labezorig != \'zw\'\n        GROUP BY p.pass_id, a.labez, a.lesart\n        """""", parameters)\n\n        # Add an \'f\' or \'o\' reading where there is no standard reading.\n        # The MODE() aggregate function arbitrarily selects the first value if\n        # there are multiple equally frequent values.\n        # This is just a hack to get *any* reading in place.\n        #\n        # FIXME: this excludes all readings attested only with a certainty <\n        # 1.0. Do we want this? or do we want (ORDER BY a.lesart, a.certainty\n        # DESC)? See also issue #97.\n        execute (conn, """"""\n        INSERT INTO readings (pass_id, labez, lesart)\n        SELECT p.pass_id, a.labez, MODE () WITHIN GROUP (ORDER BY a.lesart) AS lesart\n        FROM att a\n        JOIN passages p\n        USING (begadr, endadr)\n        WHERE labez != \'zz\' AND certainty = 1.0 AND labezsuf != \'\'\n        GROUP BY p.pass_id, a.labez\n        ON CONFLICT DO NOTHING\n        """""", parameters)\n\n        # We need to have \'zz\' readings for referential integrity, but we must\n        # set them to NULL.  \'zz\' readings in `att` may still contain a few\n        # readable characters, but those characters cannot be put in the\n        # readings table because what is recorded there is shared by all mss.\n\n        # Insert a \'zz\' reading for every passage, and a \'zu\' for Fehlverse.\n        execute (conn, """"""\n        INSERT INTO readings (pass_id, labez, lesart)\n        SELECT p.pass_id, \'zz\', NULL\n        FROM passages p;\n        INSERT INTO readings (pass_id, labez, lesart)\n        SELECT p.pass_id, \'zu\', NULL\n        FROM passages p\n        WHERE p.fehlvers\n        ON CONFLICT DO NOTHING;\n        """""", parameters)\n\n\ndef fill_cliques_table (db, parameters):\n\n    with db.engine.begin () as conn:\n\n        execute (conn, """"""\n        TRUNCATE cliques RESTART IDENTITY CASCADE;\n        ALTER TABLE cliques DISABLE TRIGGER cliques_trigger;\n        """""", parameters)\n\n        if \'MYSQL_LOCSTEM_TABLES\' in config:\n            if book == \'John\':\n                execute (conn, """"""\n                DELETE FROM tmp_locstemed l\n                WHERE NOT EXISTS (\n                  SELECT 1 FROM readings_view r\n                  WHERE (l.begadr, l.endadr, l.varid) = (r.begadr, r.endadr, r.labez)\n                )\n                """""", parameters)\n\n            fix (conn, ""Readings in locstemed but not in readings"", """"""\n            SELECT l.begadr, l.endadr, l.varid\n            FROM tmp_locstemed l\n            LEFT JOIN readings_view r\n              ON (l.begadr, l.endadr, l.varid) = (r.begadr, r.endadr, r.labez)\n            WHERE r.labez IS NULL\n            ORDER BY l.begadr, l.endadr, l.varid\n            """""", """"""\n            DELETE FROM tmp_locstemed\n            WHERE (begadr, endadr, varid) = (50323002, 50323006, \'e\')\n               OR (begadr, endadr, varid) = (50424028, 50424030, \'e\');\n            """""", parameters)\n\n            fix (conn, ""Readings in readings but not in locstemed"", """"""\n            SELECT r.begadr, r.endadr, r.labez\n            FROM readings_view r\n            LEFT JOIN tmp_locstemed l\n              ON (r.begadr, r.endadr, r.labez) = (l.begadr, l.endadr, l.varid)\n            WHERE r.labez !~ \'^z[u-z]\' AND l.varid IS NULL\n            ORDER BY r.pass_id, r.labez\n            """""", """"""\n            INSERT INTO tmp_locstemed (begadr, endadr, varid, varnew, s1)\n            VALUES (52621006, 52621010, \'p\', \'p\', \'b1\')\n            """""", parameters)\n\n        # copy all known readings into cliques\n\n        execute (conn, """"""\n        INSERT INTO cliques (pass_id, labez, user_id_start)\n        SELECT r.pass_id, r.labez, 0\n        FROM readings r\n        """""", parameters)\n\n        if \'MYSQL_LOCSTEM_TABLES\' in config:\n\n            # add \'editor\' cliques from locstem\n\n            execute (conn, """"""\n            INSERT INTO cliques (pass_id, labez, clique, user_id_start)\n            SELECT p.pass_id, varnew2labez (varnew), varnew2clique (varnew), 0\n            FROM tmp_locstemed l\n              JOIN passages p\n              USING (begadr, endadr)\n            WHERE varnew !~ \'^z[u-z]\'\n            GROUP BY p.pass_id, varnew2labez (varnew), varnew2clique (varnew)\n            ON CONFLICT DO NOTHING\n            """""", parameters)\n\n        execute (conn, """"""\n        ALTER TABLE cliques ENABLE TRIGGER cliques_trigger;\n        """""", parameters)\n\n\ndef fill_locstem_table (db, parameters):\n\n    with db.engine.begin () as conn:\n\n        execute (conn, """"""\n        TRUNCATE locstem_tts RESTART IDENTITY CASCADE;\n        TRUNCATE locstem     RESTART IDENTITY CASCADE;\n        ALTER TABLE locstem DISABLE TRIGGER locstem_trigger;\n        """""", parameters)\n\n        if \'MYSQL_LOCSTEM_TABLES\' in config:\n\n            fix (conn, ""Sources in LocStemEd but not in Cliques"", """"""\n            SELECT begadr, endadr, varnew, s1\n            FROM tmp_locstemed l\n            WHERE s1 NOT IN (\'*\', \'?\')\n              AND l.varnew !~ \'^z[u-z]\'\n              AND NOT EXISTS (\n                SELECT 1 FROM cliques_view q\n                WHERE (q.begadr, q.endadr, q.labez, q.clique) =\n                      (l.begadr, l.endadr, varnew2labez (l.s1), varnew2clique (l.s1))\n              )\n            """""", """"""\n            DELETE FROM tmp_locstemed l\n            WHERE l.s1 NOT IN (\'*\', \'?\')\n              AND NOT EXISTS (\n                SELECT 1 FROM cliques_view q\n                WHERE (q.begadr, q.endadr, q.labez, q.clique) =\n                      (l.begadr, l.endadr, varnew2labez (l.s1), varnew2clique (l.s1))\n            )\n            """""", parameters)\n\n            # check generated locstem\n            fix (conn, ""Source loops in locstem"", """"""\n            SELECT begadr, endadr, varid, varnew, s1\n            FROM tmp_locstemed\n            WHERE varnew = s1\n            """""", """"""\n            DELETE FROM tmp_locstemed\n            WHERE varnew = s1\n            """""", parameters)\n\n            # copy cliques into locstem and get source readings from tmp_locstemed\n\n            execute (conn, """"""\n            INSERT INTO locstem (pass_id, labez, clique, source_labez, source_clique, user_id_start)\n            SELECT c.pass_id, c.labez, c.clique,\n                   COALESCE (varnew2labez  (l.s1), \'?\'),\n                   varnew2clique (l.s1),\n                   0\n            FROM cliques_view c\n            LEFT JOIN tmp_locstemed l\n              ON (c.begadr, c.endadr, c.labez, c.clique) =\n                 (l.begadr, l.endadr, varnew2labez (l.varnew), varnew2clique (l.varnew))\n            WHERE c.labez !~ \'^z[u-z]\'\n            ON CONFLICT DO NOTHING\n            """""", parameters)\n\n        else:\n            db_tools.init_default_locstem (conn)\n\n\n        execute (conn, """"""\n        ALTER TABLE locstem ENABLE TRIGGER locstem_trigger;\n        """""", parameters)\n\n\ndef fill_apparatus_table (dba, parameters):\n    """"""Fill the apparatus table with a positive apparatus.\n\n    .. _transform-positive:\n\n    The :class:`~ntg_common.db.Att` table contains a negative apparatus.  A\n    negative apparatus contains the text of the archetypus (manuscript \'A\'), but\n    other manuscripts only when they offer a different reading.\n\n    The :class:`~ntg_common.db.Apparatus` table contains a positive apparatus.\n    A positive apparatus contains the text of all manuscripts at all passages.\n\n    Steps to transform the negative apparatus into a positive apparatus\n\n    1. Set all passages in all manuscripts to the reading \'a\'.\n\n    2. Overwrite all Fehlverse in all manuscripts with the reading \'zu\'.\n\n    3. Unroll the :class:`~ntg_common.db.Lac` table.  Entries in Lac table may\n       span multiple passages.  Overwrite every passage that is inside a lacuna\n       with \'zz\'.\n\n    4. Overwrite with the readings from the negative apparatus in the\n       :class:`~ntg_common.db.Att` table, if there is such a reading.\n\n    N.B. Readings in the negative apparatus do sometimes override lacunae.\n\n    In the result we have one entry in the apparatus for every manuscript and\n    every passage.\n\n    See paper: *Arbeitsablauf CBGM auf Datenbankebene, I. Vorbereitung der\n    Datenbasis f\xc3\xbcr CBGM*\n\n    """"""\n\n    with dba.engine.begin () as conn:\n\n        execute (conn, """"""\n        TRUNCATE apparatus RESTART IDENTITY CASCADE\n        """""", parameters)\n\n        # 1. Set all passages in all manuscripts to the reading \'a\'.\n        # 2. Overwrite all Fehlverse in all manuscripts with the reading \'zu\'.\n\n        log (logging.INFO, ""          Filling default reading of \'a\' ..."")\n\n        execute (conn, """"""\n        INSERT INTO apparatus (ms_id, pass_id, labez, labezsuf, cbgm, certainty, lesart, origin)\n        SELECT ms.ms_id, p.pass_id, CASE WHEN p.fehlvers THEN \'zu\' ELSE \'a\' END, \'\', true, 1.0, NULL, \'DEF\'\n        FROM passages p\n          CROSS JOIN manuscripts ms\n        """""", parameters)\n\n    with dba.engine.begin () as conn:\n\n        # 3. Unroll the :class:`~ntg_common.db.Lac` table.\n\n        log (logging.INFO, ""          Unrolling lacunae ..."")\n\n        execute (conn, """"""\n        UPDATE apparatus app\n        SET labez = \'zz\', cbgm = true, labezsuf = \'\', certainty = 1.0, lesart = NULL, origin = \'LAC\'\n        FROM (\n          SELECT DISTINCT ms.ms_id, p.pass_id\n          FROM lac l\n          JOIN manuscripts ms USING (hsnr)\n          JOIN passages p     ON p.passage <@ l.passage\n        ) AS lacs\n        WHERE (lacs.pass_id, lacs.ms_id) = (app.pass_id, app.ms_id)\n        """""", parameters)\n\n\n    with dba.engine.begin () as conn:\n\n        # 4. Overwrite with readings from :class:`~ntg_common.db.Att`\n        #\n        # N.B. must be done after lacunae unrolling because readings\n        # in att do ""override"" lacunae\n\n        log (logging.INFO, ""          Filling in readings from negative apparatus ..."")\n\n        execute (conn, """"""\n        UPDATE apparatus app\n        SET labez = a.labez, cbgm = a.certainty = 1.0, labezsuf = COALESCE (a.labezsuf, \'\'),\n            certainty = a.certainty, lesart = NULLIF (a.lesart, r.lesart), origin = \'ATT\'\n        FROM passages p, manuscripts ms, readings r, att a\n        WHERE app.pass_id = p.pass_id AND p.passage = a.passage AND r.pass_id = p.pass_id\n          AND app.ms_id = ms.ms_id  AND ms.hsnr = a.hsnr AND r.labez = a.labez\n          AND a.certainty = 1.0;\n        UPDATE apparatus app\n        SET lesart = NULL\n        WHERE labez = \'zz\' AND lesart = \'\'\n        """""", parameters)\n\n        # Insert uncertain readings\n\n        execute (conn, """"""\n        DELETE FROM apparatus app\n          USING passages p, manuscripts ms, att a\n          WHERE p.pass_id  = app.pass_id AND\n                ms.ms_id = app.ms_id AND\n                a.passage = p.passage AND a.hsnr = ms.hsnr AND\n                a.certainty < 1.0\n        """""", parameters)\n\n        execute (conn, """"""\n        INSERT INTO apparatus (pass_id, ms_id, labez, cbgm, labezsuf, certainty, lesart, origin)\n        SELECT p.pass_id, ms.ms_id, a.labez, a.certainty = 1.0, COALESCE (a.labezsuf, \'\'),\n               a.certainty, NULLIF (a.lesart, r.lesart), \'ZW\'\n        FROM passages p, manuscripts ms, readings r, att a\n        WHERE p.pass_id = r.pass_id AND p.passage = a.passage\n          AND ms.hsnr = a.hsnr AND r.labez = a.labez\n          AND a.certainty < 1.0\n        """""", parameters)\n\n\ndef fill_ms_cliques_table (dba, parameters):\n    """""" Create the ms_cliques table.\n\n    """"""\n\n    with dba.engine.begin () as conn:\n\n        execute (conn, """"""\n        TRUNCATE ms_cliques_tts RESTART IDENTITY CASCADE;\n        TRUNCATE ms_cliques     RESTART IDENTITY CASCADE;\n        ALTER TABLE ms_cliques DISABLE TRIGGER ms_cliques_trigger;\n        """""", parameters)\n\n        # insert cliques from apparatus\n        execute (conn, """"""\n        INSERT INTO ms_cliques (pass_id, ms_id, labez, clique, user_id_start)\n        SELECT DISTINCT pass_id, ms_id, labez, \'1\', 0\n        FROM apparatus a\n        """""", parameters)\n        # FIXME WHERE cbgm\n\n        if \'MYSQL_VAR_TABLES\' in config:\n\n            # Data entry fixes\n\n            log (logging.INFO, ""          Doing sanity checks ..."")\n\n            fix (conn, ""Readings in tmp_var != Apparatus"", """"""\n            SELECT a.pass_id, a.begadr, a.endadr, a.ms_id, a.hs, a.hsnr, a.labez, v.varid, v.varnew\n            FROM tmp_var v\n            JOIN apparatus_view a\n              ON (v.begadr, v.endadr, v.ms) = (a.begadr, a.endadr, a.hsnr)\n            WHERE v.varid != a.labez AND v.varid !~ \'^z[u-z]\' AND a.cbgm\n            ORDER BY a.begadr, a.endadr, a.hsnr, a.labez;\n            """""", """"""\n            UPDATE tmp_var v\n            SET varid = \'zu\', varnew = \'zu\'\n            FROM apparatus_view a\n            WHERE (v.begadr, v.endadr, v.ms, \'zu\') = (a.begadr, a.endadr, a.hsnr, a.labez);\n            UPDATE tmp_var v\n            SET varid = \'a\', varnew = \'a1\'\n            FROM apparatus_view a\n            WHERE (v.begadr, v.endadr, v.ms) = (a.begadr, a.endadr, a.hsnr)\n              AND (a.begadr, a.endadr, a.labez, v.varid) = (51122038, 51122040, \'a\', \'b\');\n            UPDATE tmp_var v\n            SET varid = \'d\', varnew = \'d1\'\n            FROM apparatus_view a\n            WHERE (v.begadr, v.endadr, v.ms) = (a.begadr, a.endadr, a.hsnr)\n              AND (a.begadr, a.endadr, a.labez, v.varid) = (50405022, 50405034, \'d\', \'a\');\n            UPDATE tmp_var v\n            SET varid = \'p\', varnew = \'p\'\n            FROM apparatus_view a\n            WHERE (v.begadr, v.endadr, v.ms) = (a.begadr, a.endadr, a.hsnr)\n              AND (a.begadr, a.endadr, a.hs) = (52621006, 52621010, \'431\');\n            """""", parameters)\n\n            # update cliques > 1 from varnew\n            execute (conn, """"""\n            UPDATE ms_cliques u\n            SET clique = varnew2clique (v.varnew)\n            FROM tmp_var v, manuscripts ms, cliques_view cq\n            WHERE (cq.pass_id, cq.labez) = (u.pass_id, u.labez)\n              AND u.ms_id = ms.ms_id AND ms.hsnr = v.ms\n              AND cq.labez = varnew2labez (v.varnew)\n              AND cq.clique = varnew2clique (v.varnew)\n              AND cq.passage = int4range (v.begadr, v.endadr + 1)\n              AND v.varnew !~ \'^z[uvw]\'\n              AND varnew2clique (v.varnew) != \'1\'\n            """""", parameters)\n\n        execute (conn, """"""\n        ALTER TABLE ms_cliques ENABLE TRIGGER ms_cliques_trigger;\n        """""", parameters)\n\n\ndef build_MT_text (dba, parameters):\n    """"""Reconstruct the Majority Text\n\n    Build a virtual manuscript that reconstructs the `mt`.\n\n        .. _mt_rules:\n\n        Im Laufe der Textgeschichte hat sich eine Textform durchgesetzt, der\n        sogenannte Mehrheitstext, der auch Byzantinischer Text genannt wird.\n        Diese Textform wird exemplarisch durch die sieben Handschriften 1, 18,\n        35, 330, 398, 424 und 1241 repr\xc3\xa4sentiert.  F\xc3\xbcr jede variierte Stelle\n        wird nun gez\xc3\xa4hlt und festgehalten, wieviele dieser sieben Handschriften\n        bei einer Lesart vertreten sind.  Eine Lesart gilt als Mehrheitslesart,\n        wenn sie\n\n        a) von mindestens sechs der oben genannten repr\xc3\xa4sentativen Handschriften\n           bezeugt wird und h\xc3\xb6chstens eine Handschrift abweicht, oder\n        b) von f\xc3\xbcnf Repr\xc3\xa4sentanten bezeugt wird und zwei mit unterschiedlichen\n           Lesarten abweichen.\n\n        --PreCo/PreCoActs/ActsMT2.pl\n\n    """"""\n\n    with dba.engine.begin () as conn:\n\n        execute (conn, """"""\n        DELETE FROM ms_cliques     WHERE ms_id = :ms_id;\n        DELETE FROM ms_cliques_tts WHERE ms_id = :ms_id;\n        DELETE FROM apparatus      WHERE ms_id = :ms_id;\n        """""", dict (parameters, ms_id = MS_ID_MT))\n\n        byzlist = tools.BYZ_HSNR[book]\n        if byzlist is not None:\n            # Insert MT where defined according to our rules\n            execute (conn, """"""\n            INSERT INTO apparatus_cliques_view (ms_id, pass_id, labez, clique, lesart, cbgm, origin)\n            SELECT :ms_id, pass_id, labez, clique, NULL, true, \'BYZ\'\n            FROM (\n                SELECT pass_id,\n                       (ARRAY_AGG (labez  ORDER BY cnt DESC))[1] AS labez,\n                       (ARRAY_AGG (clique ORDER BY cnt DESC))[1] AS clique,\n                       ARRAY_AGG (cnt    ORDER BY cnt DESC) AS mask\n                FROM (\n                  SELECT pass_id, labez, clique, count (*) AS cnt\n                  FROM apparatus_cliques_view a\n                  WHERE hsnr IN {byzlist}\n                  GROUP BY pass_id, labez, clique\n                ) AS q1\n                GROUP BY pass_id\n            ) AS q2\n            WHERE mask IN (\'{{7}}\', \'{{6,1}}\', \'{{5,1,1}}\')\n            """""", dict (parameters, ms_id = MS_ID_MT, byzlist = byzlist))\n\n        # Insert MT as \'zz\' where undefined\n        execute (conn, """"""\n        INSERT INTO apparatus_cliques_view (ms_id, pass_id, labez, clique, lesart, cbgm, origin)\n        SELECT :ms_id, p.pass_id, \'zz\', \'1\', NULL, true, \'BYZ\'\n        FROM passages p\n        WHERE NOT EXISTS (\n            SELECT 1\n            FROM ms_cliques q\n            WHERE p.pass_id = q.pass_id AND q.ms_id = :ms_id\n        )\n        """""", dict (parameters, ms_id = MS_ID_MT))\n\n\ndef print_stats (dba, parameters):\n\n    with dba.engine.begin () as conn:\n\n        res = execute (conn, ""SELECT count (distinct hs) FROM att"", parameters)\n        hs = res.scalar ()\n        log (logging.INFO, ""hs       = {cnt}"".format (cnt = hs))\n\n        res = execute (conn, ""SELECT count (*) FROM (SELECT DISTINCT begadr, endadr FROM att) AS sq"", parameters)\n        passages = res.scalar ()\n        log (logging.INFO, ""passages = {cnt}"".format (cnt = passages))\n\n        log (logging.INFO, ""hs * passages      = {cnt}"".format (cnt = hs * passages))\n\n        res = execute (conn, ""SELECT count(*) FROM att"", parameters)\n        att = res.scalar ()\n        res = execute (conn, ""SELECT count(*) FROM lac"", parameters)\n        lac = res.scalar ()\n\n        log (logging.INFO, ""rows in att        = {cnt}"".format (cnt = att))\n        log (logging.INFO, ""rows in lac        = {cnt}"".format (cnt = lac))\n\n        debug (conn, \'Table Sizes\', """"""\n        SELECT *, pg_size_pretty(total_bytes) AS total\n            , pg_size_pretty(index_bytes) AS INDEX\n            , pg_size_pretty(toast_bytes) AS toast\n            , pg_size_pretty(table_bytes) AS TABLE\n          FROM (\n          SELECT *, total_bytes-index_bytes-COALESCE(toast_bytes,0) AS table_bytes FROM (\n              SELECT c.oid,nspname AS table_schema, relname AS TABLE_NAME\n                      , c.reltuples AS row_estimate\n                      , pg_total_relation_size(c.oid) AS total_bytes\n                      , pg_indexes_size(c.oid) AS index_bytes\n                      , pg_total_relation_size(reltoastrelid) AS toast_bytes\n                  FROM pg_class c\n                  LEFT JOIN pg_namespace n ON n.oid = c.relnamespace\n                  WHERE relkind = \'r\'\n          ) a\n        ) a where table_schema = \'public\' order by table_bytes desc;\n        """""", parameters)\n\n\ndef build_parser ():\n    parser = argparse.ArgumentParser (description = __doc__)\n\n    parser.add_argument (\'profile\', metavar=\'path/to/file.conf\',\n                         help=""a .conf file (required)"")\n    parser.add_argument (\'-v\', \'--verbose\', dest=\'verbose\', action=\'count\',\n                         help=\'increase output verbosity\', default=0)\n    parser.add_argument (\'-r\', \'--range\', default=\'\',\n                         help=\'range of steps (default: all)\')\n    return parser\n\n\nif __name__ == \'__main__\':\n\n    build_parser ().parse_args (namespace = args)\n    config = config_from_pyfile (args.profile)\n\n    init_logging (\n        args,\n        logging.StreamHandler (), # stderr\n        logging.FileHandler (\'prepare.log\')\n    )\n\n    if not re.match (\'^[-0-9]*$\', args.range):\n        print (""Error in range option"")\n        parser.print_help ()\n        sys.exit ()\n\n    if \'-\' in args.range:\n        args.range = args.range.split (\'-\')\n    else:\n        args.range = [args.range, args.range]\n\n    args.range[0] = int (args.range[0] or  1)\n    args.range[1] = int (args.range[1] or 99)\n\n    parameters = dict ()\n    book = config[\'BOOK\']\n    if book in (\'Acts\', \'CL\'):\n        parameters[\'re_hs_t\']  = \'^(A|MT|([P0L]?[1-9][0-9]*)(s[1-9]?)?)\'  # hs test\n        parameters[\'re_hs\']    = \'^(A|MT|([P0L]?[1-9][0-9]*)(s[1-9]?)?)\'\n        parameters[\'re_corr\']  = \'C[*1-9]?\'  # correctors\n        parameters[\'re_corr_keep\'] = \'C[*]\'\n        parameters[\'re_suppress\']  = \'.[AK]|L2$\' # suppress these mss. (eg. secondary readings of lectionaries)\n                                             # do not match \'A\' and \'L2010\' !!!\n        parameters[\'re_comm\']  = \'T[1-9]\'    # commentaries\n        parameters[\'re_labez\'] = \'^([a-y]|z[u-z])$\'\n    if book == \'Mark\':\n        parameters[\'re_hs_t\']      = \'^(A|([P0L]?[1-9][0-9]*s?(-[1-9])?(C([1-9][a-z]?)?)?[*]?([AKL][1-9]?)?[Vr]*))$\'\n        parameters[\'re_hs\']        = \'^(A|MT|([P0L]?[1-9][0-9]*s?))\'\n        parameters[\'re_corr\']      = \'C([*]|([1-9][a-z]?))?\'  # correctors\n        parameters[\'re_corr_keep\'] = \'C[*]\'                   # correctors to keep\n        parameters[\'re_suppress\']  = \'-[2-9]|.[ABDEFHJK]|.L2\' # suppress these mss. (eg. secondary readings of lectionaries)\n        parameters[\'re_comm\']      = \'T[1-9]\'    # commentaries (there are none)\n        parameters[\'re_labez\']     = \'^([a-z]|y[a-t]|z[u-z])$\'\n    if book == \'John\':\n        parameters[\'re_hs_t\']      = \'^(A|MT|F\xce\xa0|([P0LF]?[1-9][0-9]*)S?(C[*]?)?)\'\n        parameters[\'re_hs\']        = \'^(A|MT|F\xce\xa0|([P0LF]?[1-9][0-9]*)S?(C[*]?)?)\'\n        parameters[\'re_suppress\']  = \'-[2-9]\'    # lectionaries\n        parameters[\'re_corr\']      = \'(C[*1-9]?A?([a-z]+2?)?)\'\n        parameters[\'re_corr_keep\'] = \'C[*]\'\n        parameters[\'re_labez\']     = \'^([a-z]+(/[a-z]+)*|z[u-z])$\'\n    if book == \'2 Samuel\':\n        parameters[\'re_hs_t\']      = \'^.*$\'\n        parameters[\'re_hs\']        = \'^(A|([PR]?[0-9]+(-C)?))\'\n        parameters[\'re_corr\']      = \'-?[C*]\'                  # correctors\n        parameters[\'re_corr_keep\'] = \'[*]\'                     # correctors to keep\n        parameters[\'re_suppress\']  = \'-firsthandV\'             # suppress these mss.\n        parameters[\'re_comm\']      = \'T[1-9]\'                  # commentaries (there are none)\n        parameters[\'re_labez\']     = \'^([a-y]|z[u-z])$\'\n\n    dbdest = db_tools.PostgreSQLEngine (**config)\n\n    try:\n        for step in range (args.range[0], args.range[1] + 1):\n            if step == 2:\n                log (logging.INFO, ""Step  2 : Making a working copy of the att and lac tables ..."")\n                copy_att (dbdest, parameters)\n                continue\n            if step == 3:\n                log (logging.INFO, ""Step  3 : Processing Commentaries ..."")\n                process_commentaries (dbdest, parameters)\n                continue\n            if step == 4:\n                log (logging.INFO, ""Step  4 : Delete corrector hands ..."")\n                delete_corrector_hands (dbdest, parameters)\n                continue\n            if step == 5:\n                log (logging.INFO, ""Step  5 : Delete lectionaries ..."")\n                delete_lectionaries (dbdest, parameters)\n                continue\n            if step == 6:\n                log (logging.INFO, ""Step  6 : Remove suffixes from sigla ..."")\n                process_sigla (dbdest, parameters)\n                continue\n            if step == 7:\n                log (logging.INFO, ""Step  7 : Unroll \'zw\' ..."")\n                unroll_zw (dbdest, parameters)\n                continue\n            if step == 8:\n                log (logging.INFO, ""Step  8 : Deleting invariant passages ..."")\n                delete_invariant_passages (dbdest, parameters)\n                continue\n            if step == 9:\n                if args.verbose >= 1:\n                    log (logging.INFO, ""Step  9 : Printing stats ..."")\n                    print_stats (dbdest, parameters)\n                continue\n\n            if step == 31:\n                log (logging.INFO, ""Step 31 : Making a working copy of the CBGM tables ..."")\n                copy_genealogical (dbdest, parameters)\n                copy_nestle       (dbdest, parameters)\n                continue\n\n            if step == 32:\n                log (logging.INFO, ""Step 32 : Filling the Passages table ..."")\n                fill_passages_table (dbdest, parameters)\n\n                log (logging.INFO, ""          Filling the Manuscripts table ..."")\n                fill_manuscripts_table (dbdest, parameters)\n\n                log (logging.INFO, ""          Filling the Readings table ..."")\n                fill_readings_table (dbdest, parameters)\n\n                log (logging.INFO, ""          Filling the Cliques table ..."")\n                fill_cliques_table (dbdest, parameters)\n                continue\n\n            if step == 33:\n                log (logging.INFO, ""Step 33 : Filling the LocStem table ..."")\n                fill_locstem_table (dbdest, parameters)\n                continue\n\n            if step == 34:\n                log (logging.INFO, ""Step 34 : Filling the Apparatus table with a positive apparatus ..."")\n                fill_apparatus_table (dbdest, parameters)\n                mark_invariant_passages (dbdest, parameters)\n                continue\n\n            if step == 35:\n                log (logging.INFO, ""Step 35 : Filling the MsCliques table ..."")\n                fill_ms_cliques_table (dbdest, parameters)\n                continue\n\n            if step == 41:\n                log (logging.INFO, ""Step 41 : Building the \'MT\' text ..."")\n                build_MT_text (dbdest, parameters)\n                continue\n\n            if step == 99:\n                log (logging.INFO, ""Step 99 : Vacuum ..."")\n                dbdest.vacuum ()\n\n    except KeyboardInterrupt:\n        pass\n\n    log (logging.INFO, ""          Done"")\n'"
scripts/cceh/save_edits.py,0,"b'# -*- encoding: utf-8 -*-\n\n""""""Save the state of the editor tables.\n\nThis script saves the tables containing the editorial decisions.  It does not\nsave the apparatus tables.\n\n""""""\n\nimport argparse\nimport logging\nimport sys\n\nfrom ntg_common import db\nfrom ntg_common import db_tools\nfrom ntg_common.db_tools import execute\nfrom ntg_common.tools import log\nfrom ntg_common.config import args, init_logging, config_from_pyfile\n\n\ndef build_parser ():\n    parser = argparse.ArgumentParser (description = __doc__)\n\n    parser.add_argument (\'-v\', \'--verbose\', dest=\'verbose\', action=\'count\',\n                         help=\'increase output verbosity\', default=0)\n    parser.add_argument (\'-o\', \'--output\', metavar=\'path/to/output.xml\',\n                         help=""the output file (required)"", required=True)\n    parser.add_argument (\'profile\', metavar=\'path/to/file.conf\',\n                         help=""a .conf file (required)"")\n    return parser\n\n\nif __name__ == \'__main__\':\n\n    build_parser ().parse_args (namespace = args)\n    config = config_from_pyfile (args.profile)\n\n    init_logging (\n        args,\n        logging.StreamHandler (), # stderr\n        logging.FileHandler (\'save_edits.log\')\n    )\n\n    book = config[\'BOOK\']\n    parameters = dict ()\n    db = db_tools.PostgreSQLEngine (**config)\n\n    log (logging.INFO, ""Saving changes ..."")\n\n    if args.output == \'-\':\n        fp = sys.stdout\n    else:\n        fp = open (args.output, \'w\', encoding=\'utf-8\')\n\n    with db.engine.begin () as conn:\n        fp.write (\'<?xml version=""1.0"" encoding=""utf-8"" ?>\\n\\n\')\n\n        fp.write (\'<sql profile=""%s"">\\n\' % args.profile)\n\n        res = execute (conn, """"""\n        SELECT (table_to_xml (\'export_cliques\', true, false, \'\'))\n        """""", parameters)\n\n        fp.write (res.fetchone ()[0])\n        fp.write (\'\\n\')\n\n        res = execute (conn, """"""\n        SELECT (table_to_xml (\'export_ms_cliques\', true, false, \'\'))\n        """""", parameters)\n\n        fp.write (res.fetchone ()[0])\n        fp.write (\'\\n\')\n\n        res = execute (conn, """"""\n        SELECT (table_to_xml (\'export_locstem\', true, false, \'\'))\n        """""", parameters)\n\n        fp.write (res.fetchone ()[0])\n        fp.write (\'\\n\')\n\n        res = execute (conn, """"""\n        SELECT (table_to_xml (\'export_notes\', true, false, \'\'))\n        """""", parameters)\n\n        fp.write (res.fetchone ()[0])\n        fp.write (\'</sql>\\n\')\n\n    if fp != sys.stdout:\n        fp.close ()\n\n    log (logging.INFO, ""Done"")\n'"
scripts/python/Address.py,0,"b'#! /usr/bin/python\n#-*- encoding: utf-8 -*-\n""""""\nHilfsmodul zur Verwaltung von Bibelstellen.\n""""""\n__author__=""volker.krueger@uni-muenster.de""\n\ndef decodeAdr(anfadr, endadr):\n    """"""\n    Generiere siebenteilige Adressvariablen aus den zusammengesetzten\n    Anfang- und Endadresse.\n    """"""\n    sAnf    = str(anfadr)\n    sEnd    = str(endadr)\n    if len(sAnf) == 8:\n        buch    = int(sAnf[0])\n        kapanf  = int(sAnf[1:3])\n        versanf = int(sAnf[3:5])\n        wortanf = int(sAnf[5:])\n    else:\n        buch    = int(sAnf[0:2])\n        kapanf  = int(sAnf[2:4])\n        versanf = int(sAnf[4:6])\n        wortanf = int(sAnf[6:])\n    if len(sEnd) == 8:\n        kapend  = int(sEnd[1:3])\n        versend = int(sEnd[3:5])\n        wortend = int(sEnd[5:])\n    else:\n        kapend  = int(sEnd[2:4])\n        versend = int(sEnd[4:6])\n        wortend = int(sEnd[6:])\n    return buch, kapanf, versanf, wortanf, kapend, versend, wortend\ndef encodeAdr(buch, kapanf, versanf, wortanf, kapend, versend, wortend):\n    """"""\n    Generiere zusammengesetzte Adressen (als Zahlen) aus sieben Einzelvariablen.\n    """"""\n    sAnf = sEnd = """"\n    sAnf += str(buch)\n    sAnf += formatNumber(kapanf, 2)\n    sAnf += formatNumber(versanf, 2)\n    sAnf += formatNumber(wortanf, 3)\n    sEnd += str(buch)\n    sEnd += formatNumber(kapend, 2)\n    sEnd += formatNumber(versend, 2)\n    sEnd += formatNumber(wortend, 3)\n    return int(sAnf), int(sEnd)\n\ndef encodeSingleAdr(buch, kapitel, vers, wort):\n    s  = """"\n    s += str(buch)\n    s += formatNumber(kapitel, 2)\n    s += formatNumber(vers, 2)\n    s += formatNumber(wort, 3)\n    return int(s)\n\ndef formatNumber(number, count):\n    """"""\n    Gibt eine Zahl als String zurueck. Fehlende Stellen koennen mit Nullen\n    aufgefuellt werden.\n    """"""\n    s = str(number)\n    while len(s) < count:\n        s = ""0"" + s\n    return s\n\ndef hs2hsnr(hs):\n    if hs == ""A"":\n        return 0\n    result = """"\n    shs = str(hs)\n    # cut off \'*\' or \'C\', \'C1\' etc.\n    delimiters = (""*"", ""C"", ""A"", ""K"", ""L"", ""T"", ""V"")\n    for delim in delimiters:\n        sPos = shs.find(delim, 1) # not the first position of the string\n        if sPos >= 1:\n            shs = shs[:sPos]\n    # check first character\n    if shs[0] == ""P"":\n        result = ""1""\n        shs = shs[1:]\n    elif shs[0] == ""0"":\n        result = ""2""\n        shs = shs[1:]\n    elif shs[0] == ""L"":\n        result = ""4""\n        shs = shs[1:]\n    else:\n        result = ""3""\n    # looking for a supplement\n    supplement = ""0""\n    sPos = -1\n    sPos = shs.find(""S"")\n    if sPos < 0:\n        sPos = shs.find(""s"")\n        if sPos >= 0:\n            supplement = ""1""\n    else:\n        supplement = ""1""\n    if sPos >= 0:\n        if not (shs.endswith(""s"") or shs.endswith(""S"")):\n            supplement = shs[sPos+1:]\n        result += formatNumber(shs[:sPos], 4) + supplement\n    else:\n        result += formatNumber(shs, 4) + supplement\n    return int(result)\n\ndef hsnr2hs(hsnr):\n    """"""\n    Handschriftennummer -> GA-Nummer\n    """"""\n    if hsnr == 0:\n        return ""A""\n    result = """"\n    s = str(hsnr)\n    if s.startswith(""1""):\n        result += ""P""\n    elif s.startswith(""2""):\n        result += ""0""\n    elif s.startswith(""4""):\n        result += ""L""\n    nr = s[1:5]\n    result += str(int(nr))\n    if s[5] != ""0"":\n        result += ""s"" + s[5]\n    return result\n'"
scripts/python/ComparePhases.py,0,"b'#!/usr/bin/python\n""""""\nFile name: ComparePhases.py\nFeststellung der Unterschiede in den Apparaten von Phase 1 und 2.\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\ndef main(mode=""remote""):\n\t# static variables\n\tPHASE1 = ""`ECM_ActsPh2`.`Acts15GVZ`""\n\tPHASE2 = ""`ECM_ActsPh4`.`Acts15GVZ`""\n\t# configure database connection\n\timport db_access3, Address\n\tfrom NestleAland import getBookName\n\tconn = db_access3.DBA(mode)\n\tcursor = conn.cursor()\n\t# print temporary table\n\tdef printTable():\n\t\tcmd  = ""select anfadr, endadr, labez, labezsuf, reading1 ""\n\t\tcmd += ""from `ECM_Acts`.`TempTable` order by anfadr asc, endadr desc, labez asc; ""\n\t\tcursor.execute(cmd)\n\t\trows = cursor.fetchall()\n\t\tfor r in rows:\n\t\t\tanf = r[0]\n\t\t\tend = r[1]\n\t\t\tlab = r[2]\n\t\t\tlas = r[3]\n\t\t\trd1 = r[4]\n\t\t\tb, bc, bv, bw, ec, ev, ew = Address.decodeAdr(anf, end)\n\t\t\tsb = getBookName(b)\n\t\t\ts = db_access3.formatAdr(sb, bc, bv, bw, ec, ev, ew)\n\t\t\ts1 = s2 = """"\n\t\t\tif rd1 != None:\n\t\t\t\ts1 = ""\\n\\t>"" + rd1.decode(\'utf8\') + ""< ""\n\t\t\tprint ""%s%s%s%s "" % (s, lab, las, s1)\n\t# file to generate sql statements for patristic citation database\n\tfd = open(""deleatur.sql"", ""w"")\n\t# create temporary table\n\tcmd  = ""drop table if exists `ECM_Acts`.`TempTable`; ""\n\tcursor.execute(cmd)\n\tcmd  = ""CREATE TABLE `ECM_Acts`.`TempTable` (""\n\tcmd += ""`anfadr` int(11) NOT NULL, ""\n\tcmd += ""`endadr` int(11) NOT NULL, ""\n\tcmd += ""`labez` varchar(8) NOT NULL, ""\n\tcmd += ""`labezsuf` varchar(16) default \'\', ""\n\tcmd += ""`reading1` varchar(1024) character set utf8 collate utf8_bin default NULL""\n\tcmd += "") ENGINE=MyISAM DEFAULT CHARSET=latin1"" \n\tcursor.execute(cmd)\n\n\tprint ""old:""\n\tcmd  = ""select distinct anfadr, endadr, labez, labezsuf from %s order by anfadr, endadr desc; "" % (PHASE1)\n\tcursor.execute(cmd)\n\trows = cursor.fetchall()\n\tfor row in rows:\n\t\tanf = row[0]\n\t\tend = row[1]\n\t\tlab = row[2]\n\t\tlas = row[3]\n\t\tcmd  = ""select count(*) from %s "" % (PHASE2)\n\t\tcmd += ""where anfadr = %s and endadr = %s "" % (anf, end)\n\t\tcmd += ""and labez = \'%s\' and labezsuf = \'%s\' "" % (lab, las)\n\t\tcursor.execute(cmd)\n\t\tres = cursor.fetchone()\n\t\tif res[0] == 0:\n\t\t\tcmd  = ""insert into `ECM_Acts`.`TempTable` ""\n\t\t\tcmd += ""(anfadr, endadr, labez, labezsuf) values ""\n\t\t\tcmd += ""(%d, %d, \'%s\', \'%s\'); "" % (anf, end, lab, las)\n\t\t\tcursor.execute(cmd)\n\t\t\t# generate sql file\n\t\t\ts  = ""delete from `Apparat_Zitate`.`Cit2Reading` ""\n\t\t\ts += ""where anfadr = %d and endadr = %d "" % (anf, end)\n\t\t\ts += ""and labez = \'%s\' and labezsuf = \'%s\'; \\n"" % (lab, las)\n\t\t\tfd.write(s)\n\tcmd  = ""select distinct a.anfadr, a.endadr, a.labez, a.labezsuf, a.lesart from %s a, %s b "" % (PHASE1, PHASE2)\n\tcmd += ""where a.anfadr = b.anfadr and a.endadr = b.endadr ""\n\tcmd += ""and a.labez = b.labez and a.labezsuf = b.labezsuf ""\n\tcmd += ""and a.lesart <> b.lesart ""\n\tcmd += ""order by b.anfadr, b.endadr desc, b.labez; ""\n\tcursor.execute(cmd)\n\trows = cursor.fetchall()\n\tfor row in rows:\n\t\tanf = row[0]\n\t\tend = row[1]\n\t\tlab = row[2]\n\t\tlas = row[3]\n\t\ttry:\n\t\t\trdg = row[4].decode(\'utf8\')\n\t\texcept:\n\t\t\trdg = """"\n\t\t# check if already entered in the table\n#\t\tcmd  = ""select count(*) from `ECM_Acts`.`TempTable` ""\n#\t\tcmd += ""where anfadr = %s and endadr = %s "" % (anf, end)\n#\t\tcmd += ""and labez = \'%s\' and labezsuf = \'%s\'; "" % (lab, las)\n#\t\tcount = cursor.execute(cmd)\n#\t\tif count == 0:\n\t\tcmd  = ""insert into `ECM_Acts`.`TempTable` ""\n\t\tcmd += ""(anfadr, endadr, labez, labezsuf, reading1) values ""\n\t\tcmd += ""(%d, %d, \'%s\', \'%s\', \'%s\'); "" % (anf, end, lab, las, rdg)\n\t\tcursor.execute(cmd)\n\tprintTable()\n\t# truncate temp table\n\tcmd  = ""truncate `ECM_Acts`.`TempTable`; ""\n\tcursor.execute(cmd)\n\t\n\tprint """"\n\tprint ""new:""\n\tcmd  = ""select distinct anfadr, endadr, labez, labezsuf from %s order by anfadr, endadr desc; "" % (PHASE2)\n\tcursor.execute(cmd)\n\trows = cursor.fetchall()\n\tfor row in rows:\n\t\tanf = row[0]\n\t\tend = row[1]\n\t\tlab = row[2]\n\t\tlas = row[3]\n\t\tcmd  = ""select count(*) from %s "" % (PHASE1)\n\t\tcmd += ""where anfadr = %s and endadr = %s "" % (anf, end)\n\t\tcmd += ""and labez = \'%s\' and labezsuf = \'%s\' "" % (lab, las)\n\t\tcursor.execute(cmd)\n\t\tres = cursor.fetchone()\n\t\tif res[0] == 0:\n\t\t\tcmd  = ""insert into `ECM_Acts`.`TempTable` ""\n\t\t\tcmd += ""(anfadr, endadr, labez, labezsuf) values ""\n\t\t\tcmd += ""(%d, %d, \'%s\', \'%s\'); "" % (anf, end, lab, las)\n\t\t\tcursor.execute(cmd)\n\n\tcmd  = ""select distinct b.anfadr, b.endadr, b.labez, b.labezsuf, b.lesart from %s a, %s b "" % (PHASE1, PHASE2)\n\tcmd += ""where a.anfadr = b.anfadr and a.endadr = b.endadr ""\n\tcmd += ""and a.labez = b.labez and a.labezsuf = b.labezsuf ""\n\tcmd += ""and a.lesart <> b.lesart ""\n\tcmd += ""order by b.anfadr, b.endadr desc, b.labez; ""\n\tcursor.execute(cmd)\n\trows = cursor.fetchall()\n\tfor row in rows:\n\t\tanf = row[0]\n\t\tend = row[1]\n\t\tlab = row[2]\n\t\tlas = row[3]\n\t\ttry:\n\t\t\trdg = row[4].decode(\'utf8\')\n\t\texcept:\n\t\t\trdg = """"\n\t\t# check if already entered in the table\n#\t\tcmd  = ""select count(*) from `ECM_Acts`.`TempTable` ""\n#\t\tcmd += ""where anfadr = %s and endadr = %s "" % (anf, end)\n#\t\tcmd += ""and labez = \'%s\' and labezsuf = \'%s\'; "" % (lab, las)\n#\t\tcount = cursor.execute(cmd)\n#\t\tif count == 0:\n\t\tcmd  = ""insert into `ECM_Acts`.`TempTable` ""\n\t\tcmd += ""(anfadr, endadr, labez, labezsuf, reading1) values ""\n\t\tcmd += ""(%d, %d, \'%s\', \'%s\', \'%s\'); "" % (anf, end, lab, las, rdg)\n\t\tcursor.execute(cmd)\n\t# read temporary table\n\tprintTable()\n\t# drop temporary table\n\tcmd  = ""drop table `ECM_Acts`.`TempTable`; ""\n\tcursor.execute(cmd)\n\t# closing database\n\tcursor.close()\n\tconn.close()\n\t# close file handle\n\tfd.close()\n\nif __name__ == \'__main__\':\n\tmain()\n'"
scripts/python/ComparePhasesExtended.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n""""""\nFile name: ComparePhasesExtended.py\nFeststellung der Unterschiede in den Apparaten von Phase 2 und Annettes Arbeitsbereich.\nEinbezogen werden auf jeden Fall die rein versionell bezeugten Stellen.\nDie Darstellung beschr\xc3\xa4nkt sich auf die Stellen, die im Stemmaeditor mit ""versions""\ngekennzeichnet sind.\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef versionRequested(cursor, anfadr, endadr):\n\tx = str(anfadr)\n\tchap = x[-7:-5]\n\tREF_DB = ""VarGenAtt_ActPh3""\n\tREF_TAB = ""NotesAct"" + chap\n\tcmd = ""SELECT `ZV` FROM `%s`.`%s` "" % (REF_DB, REF_TAB)\n\tcmd += ""WHERE `ANFADR` = %s AND `ENDADR` = %s;"" % (anfadr, endadr)\n\tcursor.execute(cmd)\n\tresult = cursor.fetchone()\n\tif result is None:\n\t\treturn False\n\tif result[0] == \'x\':\n\t\treturn True\n\telse:\n\t\treturn False\n\n\ndef versionsTestifiedOnly(cursor, anfadr, endadr):\n\tREF_DB = ""Apparat_Annette""\n\tx = str(anfadr)\n\tchap = x[-7:-5]\n\tREF_TAB = ""Acts"" + chap + ""GVZ""\n\t# get passages with manuscript numbers greater or equal 500000\n\tcmd = ""SELECT COUNT(*) FROM `%s`.`%s` "" % (REF_DB, REF_TAB)\n\tcmd += ""WHERE `HSNR` >= 500000 ""\n\tcmd += ""AND `ANFADR` = %s AND `ENDADR` = %s;"" % (anfadr, endadr)\n\tcursor.execute(cmd)\n\tresult = cursor.fetchone()\n\tcount_versions = int(result[0])\n\tif count_versions < 1:\n\t\treturn False\n\t# are there other manuscripts at this passages\n\tcmd = ""SELECT COUNT(*) FROM `%s`.`%s` "" % (REF_DB, REF_TAB)\n\tcmd += ""WHERE `HSNR` < 500000 AND `HSNR` > 0 ""\n\tcmd += ""AND `ANFADR` = %s AND `ENDADR` = %s;"" % (anfadr, endadr)\n\tcursor.execute(cmd)\n\tresult = cursor.fetchone()\n\tcount_other = int(result[0])\n\tif count_other > 0:\n\t\treturn False\n\treturn True\n\n\ndef main(mode=""remote""):\n\t# static variables\n\tPHASE1 = ""`ECM_ActsPh3`.`Acts01GVZ`""\n\tPHASE2 = ""`Apparat_Annette`.`Acts01GVZ`""\n\t# configure database connection\n\timport db_access3, Address\n\tfrom NestleAland import getBookName\n\tconn = db_access3.DBA(mode)\n\tcursor = conn.cursor()\n\t# print temporary table\n\tdef printTable():\n\t\tcmd  = ""select anfadr, endadr, labez, labezsuf, reading1 ""\n\t\tcmd += ""from `ECM_Acts`.`TempTable` order by anfadr asc, endadr desc, labez asc; ""\n\t\tcursor.execute(cmd)\n\t\trows = cursor.fetchall()\n\t\tfor r in rows:\n\t\t\tanf = r[0]\n\t\t\tend = r[1]\n\t\t\tlab = r[2]\n\t\t\tlas = r[3]\n\t\t\trd1 = r[4]\n\t\t\tb, bc, bv, bw, ec, ev, ew = Address.decodeAdr(anf, end)\n\t\t\tsb = getBookName(b)\n\t\t\ts = db_access3.formatAdr(sb, bc, bv, bw, ec, ev, ew)\n\t\t\ts1 = s2 = """"\n\t\t\tif rd1 != None:\n\t\t\t\ts1 = ""\\n\\t>"" + rd1.decode(\'utf8\') + ""< ""\n\t\t\tprint ""%s%s%s%s "" % (s, lab, las, s1)\n\t# file to generate sql statements for patristic citation database\n\tfd = open(""deleatur.sql"", ""w"")\n\t# create temporary table\n\tcmd  = ""drop table if exists `ECM_Acts`.`TempTable`; ""\n\tcursor.execute(cmd)\n\tcmd  = ""CREATE TABLE `ECM_Acts`.`TempTable` (""\n\tcmd += ""`anfadr` int(11) NOT NULL, ""\n\tcmd += ""`endadr` int(11) NOT NULL, ""\n\tcmd += ""`labez` varchar(8) NOT NULL, ""\n\tcmd += ""`labezsuf` varchar(16) default \'\', ""\n\tcmd += ""`reading1` varchar(1024) character set utf8 collate utf8_bin default NULL""\n\tcmd += "") ENGINE=MyISAM DEFAULT CHARSET=latin1"" \n\tcursor.execute(cmd)\n\n\tprint ""old:""\n\tcmd  = ""select distinct anfadr, endadr, labez, labezsuf from %s order by anfadr, endadr desc; "" % (PHASE1)\n\tcursor.execute(cmd)\n\trows = cursor.fetchall()\n\tfor row in rows:\n\t\tanf = row[0]\n\t\tend = row[1]\n\t\t# skip if no versions are required and it is are greek testified passage\n\t\tif not versionRequested(cursor, anf, end) and not versionsTestifiedOnly(cursor, anf, end):\n\t\t\tcontinue\n\t\tlab = row[2]\n\t\tlas = row[3]\n\t\tcmd  = ""select count(*) from %s "" % (PHASE2)\n\t\tcmd += ""where anfadr = %s and endadr = %s "" % (anf, end)\n\t\tcmd += ""and labez = \'%s\' and labezsuf = \'%s\' "" % (lab, las)\n\t\tcursor.execute(cmd)\n\t\tres = cursor.fetchone()\n\t\tif res[0] == 0:\n\t\t\tcmd  = ""insert into `ECM_Acts`.`TempTable` ""\n\t\t\tcmd += ""(anfadr, endadr, labez, labezsuf) values ""\n\t\t\tcmd += ""(%d, %d, \'%s\', \'%s\'); "" % (anf, end, lab, las)\n\t\t\tcursor.execute(cmd)\n\t\t\t# generate sql file\n\t\t\ts  = ""delete from `Apparat_Zitate`.`Cit2Reading` ""\n\t\t\ts += ""where anfadr = %d and endadr = %d "" % (anf, end)\n\t\t\ts += ""and labez = \'%s\' and labezsuf = \'%s\'; \\n"" % (lab, las)\n\t\t\tfd.write(s)\n\tcmd  = ""select distinct a.anfadr, a.endadr, a.labez, a.labezsuf, a.lesart from %s a, %s b "" % (PHASE1, PHASE2)\n\tcmd += ""where a.anfadr = b.anfadr and a.endadr = b.endadr ""\n\tcmd += ""and a.labez = b.labez and a.labezsuf = b.labezsuf ""\n\tcmd += ""and a.lesart <> b.lesart ""\n\tcmd += ""order by b.anfadr, b.endadr desc, b.labez; ""\n\tcursor.execute(cmd)\n\trows = cursor.fetchall()\n\tfor row in rows:\n\t\tanf = row[0]\n\t\tend = row[1]\n\t\tlab = row[2]\n\t\tlas = row[3]\n\t\ttry:\n\t\t\trdg = row[4].decode(\'utf8\')\n\t\texcept:\n\t\t\trdg = """"\n\t\t# check if already entered in the table\n#\t\tcmd  = ""select count(*) from `ECM_Acts`.`TempTable` ""\n#\t\tcmd += ""where anfadr = %s and endadr = %s "" % (anf, end)\n#\t\tcmd += ""and labez = \'%s\' and labezsuf = \'%s\'; "" % (lab, las)\n#\t\tcount = cursor.execute(cmd)\n#\t\tif count == 0:\n\t\tcmd  = ""insert into `ECM_Acts`.`TempTable` ""\n\t\tcmd += ""(anfadr, endadr, labez, labezsuf, reading1) values ""\n\t\tcmd += ""(%d, %d, \'%s\', \'%s\', \'%s\'); "" % (anf, end, lab, las, rdg)\n\t\tcursor.execute(cmd)\n\tprintTable()\n\t# truncate temp table\n\tcmd  = ""truncate `ECM_Acts`.`TempTable`; ""\n\tcursor.execute(cmd)\n\t\n\tprint """"\n\tprint ""new:""\n\tcmd  = ""select distinct anfadr, endadr, labez, labezsuf from %s order by anfadr, endadr desc; "" % (PHASE2)\n\tcursor.execute(cmd)\n\trows = cursor.fetchall()\n\tfor row in rows:\n\t\tanf = row[0]\n\t\tend = row[1]\n\t\tif not versionRequested(cursor, anf, end) and not versionsTestifiedOnly(cursor, anf, end):\n\t\t\tcontinue\n\t\tlab = row[2]\n\t\tlas = row[3]\n\t\tcmd  = ""select count(*) from %s "" % (PHASE1)\n\t\tcmd += ""where anfadr = %s and endadr = %s "" % (anf, end)\n\t\tcmd += ""and labez = \'%s\' and labezsuf = \'%s\' "" % (lab, las)\n\t\tcursor.execute(cmd)\n\t\tres = cursor.fetchone()\n\t\tif res[0] == 0:\n\t\t\tcmd  = ""insert into `ECM_Acts`.`TempTable` ""\n\t\t\tcmd += ""(anfadr, endadr, labez, labezsuf) values ""\n\t\t\tcmd += ""(%d, %d, \'%s\', \'%s\'); "" % (anf, end, lab, las)\n\t\t\tcursor.execute(cmd)\n\n\tcmd  = ""select distinct b.anfadr, b.endadr, b.labez, b.labezsuf, b.lesart from %s a, %s b "" % (PHASE1, PHASE2)\n\tcmd += ""where a.anfadr = b.anfadr and a.endadr = b.endadr ""\n\tcmd += ""and a.labez = b.labez and a.labezsuf = b.labezsuf ""\n\tcmd += ""and a.lesart <> b.lesart ""\n\tcmd += ""order by b.anfadr, b.endadr desc, b.labez; ""\n\tcursor.execute(cmd)\n\trows = cursor.fetchall()\n\tfor row in rows:\n\t\tanf = row[0]\n\t\tend = row[1]\n\t\tlab = row[2]\n\t\tlas = row[3]\n\t\ttry:\n\t\t\trdg = row[4].decode(\'utf8\')\n\t\texcept:\n\t\t\trdg = """"\n\t\t# check if already entered in the table\n#\t\tcmd  = ""select count(*) from `ECM_Acts`.`TempTable` ""\n#\t\tcmd += ""where anfadr = %s and endadr = %s "" % (anf, end)\n#\t\tcmd += ""and labez = \'%s\' and labezsuf = \'%s\'; "" % (lab, las)\n#\t\tcount = cursor.execute(cmd)\n#\t\tif count == 0:\n\t\tcmd  = ""insert into `ECM_Acts`.`TempTable` ""\n\t\tcmd += ""(anfadr, endadr, labez, labezsuf, reading1) values ""\n\t\tcmd += ""(%d, %d, \'%s\', \'%s\', \'%s\'); "" % (anf, end, lab, las, rdg)\n\t\tcursor.execute(cmd)\n\t# read temporary table\n\tprintTable()\n\t# drop temporary table\n\tcmd  = ""drop table `ECM_Acts`.`TempTable`; ""\n\tcursor.execute(cmd)\n\t# closing database\n\tcursor.close()\n\tconn.close()\n\t# close file handle\n\tfd.close()\n\nif __name__ == \'__main__\':\n\tmain()\n'"
scripts/python/Fehlverse.py,0,"b'#!/usr/bin/python\n\n__author__ = ""volker.krueger@uni-muenster.de""\n\nclass Fehlvers(object):\n\t""""""\n\tDie Klasse Fehlvers enthaelt in der Liste addresses\n\tdie Anfang- und Endadressen (je als Tupel) der Fehl-\n\tverse. \n\tDa es sich um eine uebersichtliche Liste handelt,\n\tlege ich sie nicht in der Datenbank ab.\n\tDie Liste wird im Laufe der Bearbeitung wachsen.\n\t""""""\n\tdef __init__(self):\n\t\tself.addresses = []\n\t\tself.addresses.append((50837002, 50837046))\n\t\tself.addresses.append((51534002, 51534012))\n\t\tself.addresses.append((52406020, 52408014))\n\t\tself.addresses.append((52829002, 52829024))\n\n\tdef isFehlvers(self, address):\n\t\t""""""\n\t\tDie Methode gibt True zurueck, wenn die genannte\n\t\tAdresse in einem Fehlvers liegt.\n\t\tLiegt sie nur teilweise im Fehlvers, so wird False\n\t\tzurueckgegeben.\n\t\t""""""\n\t\tfor addr in self.addresses:\n\t\t\tif address >= addr[0] and address <= addr[1]:\n\t\t\t\treturn True\n\t\treturn False\n'"
scripts/python/NestleAland.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\'\'\'\nTools\n\'\'\'\nimport Fehlverse\nimport db_access3\nimport Address\n\nNT = { 1: (""Mt"", ""Matthew""),\n\t   2: (""Mc"", ""Mark""),\n\t   3: (""L"", ""Luke""),\n\t   4: (""J"", ""John""),\n\t   5: (""Act"", ""Acts""),\n\t   6: (""R"", ""Romans""),\n\t   7: (""1K"", ""1Corinthians""),\n\t   8: (""2K"", ""2Corinthians""),\n\t   9: (""G"", ""Galatians""),\n\t  10: (""E"", ""Ephesians""),\n\t  11: (""Ph"", ""Philippians""),\n\t  12: (""Kol"", ""Colossians""),\n\t  13: (""1Th"", ""1Thessalonians""),\n\t  14: (""2Th"", ""2Thessalonians""),\n\t  15: (""1T"", ""1Timothy""),\n\t  16: (""2T"", ""2Timothy""),\n\t  17: (""Tt"", ""Titus""),\n\t  18: (""Phm"", ""Philemon""),\n\t  19: (""H"", ""Hebrews""),\n\t  20: (""Jc"", ""James""),\n\t  21: (""1P"", ""1Peter""),\n\t  22: (""2P"", ""2Peter""),\n\t  23: (""1J"", ""1John""),\n\t  24: (""2J"", ""2John""),\n\t  25: (""3J"", ""3John""),\n\t  26: (""Jd"", ""Jude""),\n\t  27: (""Ap"", ""Revelation"")\n\t }\n\ndef getBookName(number):\n    try:\n        return NT[number][0]\n    except KeyError:\n        return """"\ndef getLongName(number):\n    try:\n        return NT[number][1]\n    except KeyError:\n        return """"\n\ndef getBookNumber(name):\n    \'\'\'\n    Returning the ordering number of the given book\n    according to NA27.\n    \'\'\'\n    for n in range(1, 28):\n        if name == NT[n][0] or name == NT[n][1]:\n            return n\n    return 0\n\ndef getMaxWord(dbcursor, book, chapter, verse):\n    """"""\n    Return the maximal word number of a verse.\n    The method is used to correct errors of Collate.\n    """"""\n    try:\n        cmd  = ""select max(word) from Apparat.Nestle ""\n        cmd += ""where book = %d "" % book\n        cmd += ""and chapter = %d "" % chapter\n        cmd += ""and verse = %d "" % verse\n        dbcursor.execute(cmd)\n        row = dbcursor.fetchone()\n        return row[0]\n    except:\n        print ""Error in NestleAland.getMaxWord()""\n\nclass NA:\n    def __init__(self, _edition=28):\n        \'\'\'\n        Konstruktor\n        Die Datenbank enth\xc3\xa4lt den Nestletext der Auflagen 28 und\n        der gerade entstehenden 29. Auflage (= ECM Acta). Andere\n        Auflagen k\xc3\xb6nnen nicht abgefragt werden.\n        \'\'\'\n        #import MySQLdb\n        self.fehlverse = Fehlverse.Fehlvers()\n        dba = db_access3.DBA(""remote"")\n        self.cursor = dba.cursor()\n        self.edition = _edition\n        if self.edition < 28 or self.edition > 29:\n            print ""Selected edition is not available!""\n            print ""Using edition 28 instead.""\n            self.edition = 28\n\n    def __del__(self):\n        \'\'\'\n        Destruktor\n        \'\'\'\n        self.cursor.close()\n\n    def getMaxChapter(self, book):\n        """"""\n        FIXME: Datenbasis R + 1K checken!\n        """"""\n        cmd  = ""select max(chapter) from Apparat.Nestle ""\n        cmd+= ""where book = %d "" % (book)\n        self.cursor.execute(cmd)\n        row = self.cursor.fetchone()\n        if row == None:\n            return 0\n        return row[0]\n\n    def getMaxVerse(self, book, chapter):\n        cmd  = ""select max(verse) from Apparat.Nestle ""\n        cmd += ""where book = %d and chapter = %d "" % (book, chapter)\n        self.cursor.execute(cmd)\n        row = self.cursor.fetchone()\n        if row == None:\n            return 0 # Fehlverse\n        return row[0]\n\n    def getMaxWord(self, book, chapter, verse):\n        cmd  = ""select max(word) from Apparat.Nestle ""\n        cmd += ""where book = %d and chapter = %d and verse = %d "" % (book, chapter, verse)\n        self.cursor.execute(cmd)\n        row = self.cursor.fetchone()\n        if row == None:\n            return 0\n        return row[0]\n\n    def getNestleText(self, book, bchap, bvers, bword, echap, evers, eword, additamenta=False):\n        """"""\n        If the address represents an additamentum and the parameter additamenta is false,\n        the text will be skipped. It is printed only if this parameter is set to true.\n        """"""\n        result = """"\n        fehlvers_active = False\n        fehlvers_started = False\n        if not additamenta and bword % 2 == 1 and eword % 2 == 1:\n            return result\n        chapter  = bchap\n        verse    = bvers\n        word     = bword\n        maxverse = self.getMaxVerse(book, chapter)\n        maxword  = self.getMaxWord(book, chapter, verse)\n        if maxword == 0:\n            return result\n        if eword == maxword + 1: # Endlosschleife verhindern\n            maxword += 1\n        while True:\n            address = Address.encodeSingleAdr(book, chapter, verse, word) # ->Address.py\n            fehlvers_active = self.fehlverse.isFehlvers(address)\n            cmd  = ""select content from Apparat.Nestle where book = %d "" % (book)\n            cmd += ""and chapter = %d and verse = %d and word = %d "" % (chapter, verse, word)\n            self.cursor.execute(cmd)\n            row = self.cursor.fetchone()\n            if row == None:\n                pass \n            else:\n                # Anfangsklammern setzen\n                if fehlvers_active and not fehlvers_started:\n                    result += ""[[""\n                    fehlvers_started = True\n                # Schlussklammern setzen\n                if not fehlvers_active and fehlvers_started:\n                    result += ""]]""\n                    fehlvers_started = False\n                # Rueckgabestring schreiben\n                result += row[0] + "" ""\n            # Abbruchbedingung definieren\n            if chapter == echap and verse == evers and word == eword:\n                break\n            # second condition in case of an error in Apparat.LookUpNT\n            if chapter > echap:\n                break\n            word += 1\n            if word > maxword:\n                verse += 1\n                if verse > maxverse:\n                    chapter += 1\n                    verse = 1\n                word   = 2\n                maxword = self.getMaxWord(book, chapter, verse)\n                maxverse = self.getMaxVerse(book, chapter)\n        result[:-1] # cut off last white space\n        if fehlvers_started: # schliessende Doppelklammer falls geoeffnet\n            result += ""]]""\n        return result\n'"
scripts/python/access.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n""""""\nOne fact - one place.\nEdit here which database server to access.\n\'local\' and \'remote\' are configured server in db_access3.py\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef get():\n    return ""remote""\n#    return ""local""\n'"
scripts/python/db_access3.py,0,"b'#! /usr/bin/python\n#-*- encoding: utf-8 -*-\n\'\'\'\nModule imported by printer3.py and printer4.py etc.\n\'\'\'\n__author__ = ""volker.krueger@uni-muenster.de""\n\nimport MySQLdb\n\n#Configure database access here\nlocal_dict = {\n    ""host""   : ""localhost"",\n    ""user""   : ""root"",\n    ""passwd"" : ""xxx"",\n    ""db""     : ""apparat"",\n    ""charset"": ""utf8"" }\n\nremote_dict = {\n    ""host""   : ""your_hostname"",\n    ""user""   : ""your_username"",\n    ""passwd"" : ""your_password"",\n    ""db""     : ""Apparat"",\n    ""charset"": ""utf8"" }\n\n\nclass DBA(object):\n    def __init__(self, s):\n        self.__d = {}\n        if s == ""local"":\n            self.__d = local_dict\n        elif s == ""remote"":\n            self.__d = remote_dict\n        else:\n            self.__d = None\n        if self.__d != None:\n            self.connection = MySQLdb.connect(host=self.__d.get(""host""), \\\n                                              user=self.__d.get(""user""), \\\n                                              passwd=self.__d.get(""passwd""), \\\n                                              db=self.__d.get(""db""), \\\n                                              charset=self.__d.get(""charset""))\n            self.owncursor = self.connection.cursor()\n    def __del__(self):\n        #self.close()\n        pass\n    def getHost(self):\n        return self.__d.get(""host"")\n    def getUser(self):\n        return self.__d.get(""user"")\n    def getPasswd(self):\n        return self.__d.get(""passwd"")\n    def getDb(self):\n        return self.__d.get(""db"")\n    def getCharset(self):\n        return self.__d.get(""charset"")\n    def autocommit(self, value):\n        self.connection.autocommit(value)\n    def commit(self):\n        self.connection.commit()\n    def rollback(self):\n        self.connection.rollback()\n    def cursor(self):\n        return self.connection.cursor()\n    def close(self):\n        self.connection.close()\n    def getMssAvailable(self, book):\n        \'\'\'\n        Giving all manuscripts collated for the book identified by the parameter.\n        \'\'\'\n        cmd  = ""select hsnr from `Apparat`.`mss_available` ""\n        cmd += ""where book = %d "" % (book)\n        cmd += ""and hsnr < 500000 "" # Fehlverse ausschliessen\n        cmd += ""order by hsnr ""\n        self.owncursor.execute(cmd)\n        return self.owncursor.fetchall()\n    def getPassages(self, database, table, startverse = 0, endverse = 0, firstword = 0, lastword = 0):\n        """"""\n        Getting all passages of a witness table. The program should work\n        independently from a LesartenOnly table.\n        """"""\n        cmd  = ""select distinct anfadr, endadr, buch, kapanf, versanf, wortanf, ""\n        cmd += ""kapend, versend, wortend from `%s`.`%s` where 1 "" % (database, table)\n        cond = """"\n        if int(startverse) > 0 and int(endverse) > 0:\n            cond  = ""and versanf >= %s and versend <= %s "" % (startverse, endverse)\n            if int(firstword) > 0 and int(lastword) > 0:\n                cond  = ""and ((versanf = %s and wortanf >= %s) or versanf > %s) "" % (startverse, firstword, startverse)\n                cond += ""and ((versend = %s and wortend <= %s) or versend < %s) "" % (endverse, lastword, endverse)\n        cmd += cond + ""order by anfadr asc, endadr desc ""\n        count = self.owncursor.execute(cmd)\n        return self.owncursor.fetchall(), count\n    def getReadings(self, database, table, anfadr, endadr):\n        """"""\n        Getting all the readings which belong to an address described by the\n        result set of \'row_passages\'.\n        """"""\n        cmd  = ""select distinct anfadr, endadr, labez, labezsuf, ""\n        cmd += ""buch, kapanf, versanf, wortanf, kapend, versend, wortend, lesart ""\n        cmd += ""from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %d and endadr = %d "" % (anfadr, endadr)\n        cmd += ""order by labez, labezsuf ""\n        #print cmd\n        count = self.owncursor.execute(cmd)\n        return self.owncursor.fetchall(), count\n    def getReadings2(self, database, table, anfadr, endadr):\n        """"""\n        Getting all readings which belong to one passage. Fehlerlesarten\n        are included - orthographica too.\n        """"""\n        cmd  = ""select distinct anfadr, endadr, labez, \'\', ""\n        cmd += ""buch, kapanf, versanf, wortanf, kapend, versend, wortend, lesart ""\n        cmd += ""from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %d and endadr = %d "" % (anfadr, endadr)\n        cmd += ""order by labez, labezsuf ""\n        count = self.owncursor.execute(cmd)\n        return self.owncursor.fetchall(), count\n    def getWitnesses(self, database, table, anfadr, endadr, labez, labezsuf):\n        """"""\n        Getting all witnesses of a reading described by the result set of\n        \'row_readings\'.\n        """"""\n        cmd  = ""select hs, suffix2, hsnr, lemma, lesart, fehler from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %s and endadr = %s "" % (anfadr, endadr)\n        cmd += ""and labez = \'%s\' and labezsuf = \'%s\' "" % (labez, labezsuf)\n        cmd += ""order by hsnr, labezsuf ""\n        #  print cmd\n        count = self.owncursor.execute(cmd)\n        return self.owncursor.fetchall(), count\n    def getVersions(self, database, table, anfadr, endadr, labez, labezsuf):\n        cmd  = ""select hss, suffix2, hsnr, vers_lesart, fehler from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %s and endadr = %s "" % (anfadr, endadr)\n        cmd += ""and labez = \'%s\' and labezsuf = \'%s\' "" % (labez.encode(\'utf-8\'), labezsuf.encode(\'utf-8\'))\n        cmd += ""order by hsnr, labezsuf ""\n        #  print cmd\n        count = self.owncursor.execute(cmd)\n        return self.owncursor.fetchall(), count\n    def getAllWitnesses(self, database, table, anfadr, endadr):\n        """"""\n        Returns all witnesses of a passage.\n        """"""\n        cmd  = ""select hs, hsnr from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %d and endadr = %d "" % (anfadr, endadr)\n        cmd += ""order by hsnr ""\n        count = self.owncursor.execute(cmd)\n        return self.owncursor.fetchall(), count\n    def countWitnessesDifferentFromA(self, database, table, anfadr, endadr):\n        \'\'\'\n        How many greek witnesses differ from the Ausgangstext?\n        Orthographica are not included.\n        \'\'\'\n        cmd  = ""select count(hsnr) from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %d and endadr = %d "" % (anfadr, endadr)\n        cmd += ""and (labez <> \'a\' ""\n        cmd += ""or labez = \'a\' and (labezsuf <> \'\' or labezsuf is not null)) ""\n        cmd += ""and hsnr < 500000 ""\n        void = self.owncursor.execute(cmd)\n        res  = self.owncursor.fetchone()\n        return res[0]\n    def getLacunaeAndDubia(self, database, table, anfadr, endadr, lacunae = None):\n        \'\'\'\n        Returns witnesses having a lacuna at the passage or having labez \'zz\'.\n        \'\'\'\n        if lacunae == None:\n            lacunae = table + ""lac""\n        cmd  = ""(select hs, hsnr from %s.%s "" % (database, lacunae) # here: table of lacunae\n        cmd += ""where anfadr <= %d and endadr >= %d "" % (int(endadr), int(anfadr))\n        cmd += ""order by hsnr, suffix2) union ""\n        cmd += ""(select hs, hsnr from %s.%s "" % (database, table) # here: table of witnesses\n        cmd += ""where anfadr = %d and endadr = %d "" % (anfadr, endadr)\n        cmd += ""and labez = \'zz\' ""\n        cmd += ""order by hsnr) ""\n        count = self.owncursor.execute(cmd)\n        # sort result sets into an ascending order\n        res  = [] # tupel to return\n        rel  = {} # dictionary hsnr: hs\n        hsnr = [] # list containing hsnr\n        rows = self.owncursor.fetchall()\n        for row in rows:\n            hs = row[0]\n            nr = row[1]\n            if nr not in hsnr:\n                hsnr.append(nr)\n                rel[nr] = (hs, nr, )\n        hsnr.sort()\n        for nr in hsnr:\n            res.append(rel[nr])\n        return res, count\n\n\ndef formatAdr(b, bc, bv, bw, ec, ev, ew):\n    """"""\n    Write the seven parameters in a nicely formatted way.\n    """"""\n    s = """"\n    if bc == ec and bv == ev and bw == ew:\n        s = ""%s%s:%s/%s"" % (b, bc, bv, bw)\n    else:\n        if bc == ec and bv == ev:\n            s = ""%s%s:%s/%s-%s"" % (b, bc, bv, bw, ew)\n        else:\n            if bc == ec:\n                s = ""%s%s:%s/%s-%s/%s"" % (b, bc, bv, bw, ev, ew)\n            else:\n                s = ""%s%s:%s/%s-%s:%s/%s"" % (b, bc, bv, bw, ec, ev, ew)\n    return s\n'"
scripts/python/prepare4cbgm.py,0,"b'#!/usr/bin/python\n#-*- encoding: utf-8 -*-\n""""""\n9. Stellenbezogene Lueckenliste fuellen\n    9.0. Vorhandene Tabelle leeren\n    9.1. Alle Passages auflisten\n    9.2. Alle Handschriften der Systematischen Lueckenliste auflisten\n    9.3. Schleife ueber alle Handschriften\n    9.4. Schleife ueber alle Passages\n    9.5. Eintrag in ActsNNattLac, wenn die Handschrift an genannter Stelle\n         in der Systematischen Lueckenliste verzeichnet ist\n""""""\n__author__=""volker.krueger@uni-muenster.de""\n\ndef enter2LocalLacList(cursor, hs, db, lactable, anfadr, endadr):\n    import Address\n    b, bc, bv, bw, ec, ev, ew = Address.decodeAdr(anfadr, endadr)\n    hsnr = Address.hs2hsnr(hs)\n    cmd  = ""insert into %s.%s "" % (db, lactable)\n    cmd += ""(buch, kapanf, versanf, wortanf, kapend, versend, wortend, ""\n    cmd += ""anfadr, endadr, hs, hsnr, anfalt, endalt) ""\n    cmd += ""values (%d, %d, %d, %d, %d, %d, %d, "" % (b, bc, bv, bw, ec, ev, ew)\n    cmd += ""%d, %d, \'%s\', %d, %d, %d) "" % (anfadr, endadr, hs, hsnr, anfadr, endadr)\n    cursor.execute(cmd)\n\ndef main_9(db1, db2, tab1, tab2, mode=""remote""):\n    import db_access3\n    dba = db_access3.DBA(mode)\n    cursor = dba.cursor()\n    sourcetable = tab1\n    lactable = sourcetable + ""Lac""\n    # 0. Truncate table\n    cmd  = ""truncate %s.%s "" % (db1, lactable)\n    cursor.execute(cmd)\n    dba.commit()\n    # 1.1.\n    passages, passcount = dba.getPassages(db1, sourcetable)\n    # 1.2.\n    cmd  = ""select distinct hs from %s.%s "" % (db2, tab2)\n    cmd += ""order by hsnr ""\n    cursor.execute(cmd)\n    mss = cursor.fetchall()\n    # 1.3.\n    for ms in mss:\n        hs = ms[0]\n        for passage in passages:\n            anfadr = passage[0]\n            endadr = passage[1]\n            cmd  = ""select count(id) from %s.%s "" % (db2, tab2)\n            cmd += ""where anfadr <= %d and endadr >= %d "" % (anfadr, endadr)\n            cmd += ""and hs = \'%s\' "" % (hs)\n            cursor.execute(cmd)\n            result = cursor.fetchone()\n            rescount = result[0]\n            if rescount > 0:\n                enter2LocalLacList(cursor, hs, db1, lactable, anfadr, endadr)\n    dba.commit() # it\'s an InnoDB table\n    cursor.close()\n    dba.close()\n\nif __name__ == ""__main__"":\n    from optparse import OptionParser\n    import sys, time\n    print time.ctime()\n    parser = OptionParser()\n    parser.add_option(""-d"", ""--database"", dest=""database"", help=""Giving database"")\n    parser.add_option(""-m"", ""--mode"", dest=""mode"", help=""choose \'local\' or \'remote\' database access, \'remote\' is default"")\n    parser.add_option(""-t"", ""--table"", dest=""table"", help=""Giving table name attestations"")\n    parser.add_option(""-e"", ""--ref_db"", dest=""ref_db"", help=""Database of lacuna table"")\n    parser.add_option(""-l"", ""--lactable"", dest=""lactable"", help=""Systematic lacunae table"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    if opts.database == None or opts.table == None or opts.ref_db == None:\n        print ""Error: At least one parameter necessary is missing!""\n        print ""See python %s -h"" % sys.argv[0]\n        sys.exit(1)\n    main_9(opts.database, opts.ref_db, opts.table, opts.lactable)\n    print ""%s finished at %s"" % (sys.argv[0], time.ctime())'"
scripts/python/prepare4cbgm_1.py,0,"b'#!/usr/bin/python\n#-*- encoding: utf-8 -*-\n""""""\n1. Tabelle fuer die Korrektur mit neuem Datum von\n    `ECM_Acts` nach `ECM_Acts_Update` kopieren.\n    (Z.B. Acts01C20111223 >> Acts01C20120125)\n    1.0. Loeschen der bereits vorhandenen Tabellen in der Zieldatenbank\n    1.1. Anzeige aller Tabellen in der Quelldatenbank\n    1.2. Generieren eines Datumsstempels\n    1.3. Ausschneiden der Buch-, Kapitel- und Versionsbezeichnung\n    1.4. Tabellen anlegen mit \'create table like ...\'\n    1.5. Befuellen mit \'insert into ... select * from ...\'\nVgl. die Datei ArbeitsablaufCBGMApg1.pdf in der Mail\nvon Klaus am 25.01.2012\n""""""\n__author__=""volker.krueger@uni-muenster.de""\n\ndef main_1(sourceDB, targetDB, chapter=0, verbose=False):\n    def printer(s):\n        """"""\n        Print information if needed.\n        """"""\n        if verbose:\n            print s\n    import access\n    import db_access3, datetime\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n#    sourceDB = ""Apparat_Annette""\n#    targetDB = ""ECM_Acts_UpdatePh2""\n    # 1.0. Loeschen der bereits vorhandenen Tabellen in der Zieldatenbank\n    cmd  = ""drop database `%s` "" % (targetDB)\n    cursor.execute(cmd)\n    cmd  = ""create database `%s` "" % (targetDB)\n    cursor.execute(cmd)\n    if chapter == 0:\n        # 1.1. Anzeige aller Tabellen in der Quelldatenbank\n        cmd = ""show tables from `%s` like \'Acts%%\'; "" % (sourceDB)\n    else:\n        # 1.1. Anzeige der (beiden) Tabellen eines Kapitels\n        if chapter < 10:\n            sChapter = ""0"" + str(chapter)\n        else:\n            sChapter = str(chapter)\n        cmd = ""show tables from `%s` like \'Acts%s%%\'; "" % (sourceDB, sChapter)\n    printer(cmd)\n    cursor.execute(cmd)\n    tables = cursor.fetchall()\n    for table in tables:\n        # 1.2. Generieren eines Datumsstempels\n        d = ""%s"" % datetime.date.today()\n        datum = d[:4] + d[5:7] + d[8:]\n        # 1.3. Ausschneiden der Buch-, Kapitel- und Versionsbezeichnung\n        name = table[0][:9]\n        # Neuen Namen zusammensetzen\n        name += datum\n        # Bei Bedarf das Suffix \'lac\' anfuegen\n        if table[0].endswith(""lac""):\n            name += ""lac""\n        # 1.4. Tabellen anlegen mit \'create table like ...\'\n        cmd = ""create table `%s`.`%s` "" % (targetDB, name)\n        cmd += ""like `%s`.`%s` "" % (sourceDB, table[0])\n        printer(cmd)\n        cursor.execute(cmd)\n        # 1.5. Befuellen mit \'insert into ... select * from ...\'\n        cmd  = ""insert into `%s`.`%s` "" % (targetDB, name)\n        cmd += ""select * from `%s`.`%s` "" % (sourceDB, table[0])\n        printer(cmd)\n        cursor.execute(cmd)\n        dba.commit()\n    cursor.close()\n    dba.close()\n\nif __name__ == ""__main__"":\n    import sys\n    from optparse import OptionParser\n    usage = ""Usage: %s [options]"" % sys.argv[0]\n    usage += ""\\nCopy apparatus data into a new database.""\n    usage += ""\\nTAKE CARE:""\n    usage += ""\\nThe script copies all chapters of a database!""\n    parser = OptionParser(usage=usage)\n    parser.add_option(""-s"", ""--source"", dest=""source"", help=""Giving source database"")\n    parser.add_option(""-t"", ""--target"", dest=""target"", help=""Giving target database"")\n    parser.add_option(""-c"", ""--chapter"", dest=""chapter"", help=""Optionally giving one chapter"")\n    parser.add_option(""-v"", ""--verbose"", dest=""verbose"", action=""store_true"", help=""Verbose mode"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    source = target = """"\n    chapter = 0\n    try:\n        if len(opts.source) > 0:\n            source = opts.source\n        if len(opts.target) > 0:\n            target = opts.target\n        if len(opts.chapter) > 0:\n            chapter = int(opts.chapter)\n    except:\n        pass\n    if source == """" or target == """":\n        print ""At least one parameter is missing.""\n        print ""python %s -h"" % (sys.argv[0])\n        sys.exit(1)\n    main_1(source, target, chapter, opts.verbose)'"
scripts/python/prepare4cbgm_10.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n""""""\n10. Bezeugung der a-Lesarten auffuellen. Sie setzt sich zusammen aus allen\n    in der \'ActsMsList\' fuer das jeweilige Kapitel gefuehrten Handschriften,\n    die an der jeweils bearbeiteten Stelle noch nicht bei einer Variante\n    oder in der Lueckenliste stehen.\n\n    Besondere Aufmerksamkeit ist bei den Fehlversen notwendig:\n    Im Bereich der Fehlverse darf nicht einfach die a-Bezeugung\n    aufgefuellt werden. Stattdessen muss, wenn die variierte Stelle\n    zu einer umfassten Einheit gehoert und das Feld \'base\' den\n    Inhalt \'a\' hat, die neue Lesartenbezeichnung \'zu\' eingetragen werden.\n    \'base = b\' steht f\xc3\xbcr eine alternative Subvariante (dem Textus receptus).\n\n    Eine variierte Stelle ist eine umfasste Stelle, wenn comp = \'x\' ist.\n\nVgl. die Datei ArbeitsablaufCBGMApg1.pdf in der Mail\nvon Klaus am 25.01.2012\n\nErg\xc3\xa4nzung vom 13.03.2015:\nDas Skript arbeitet mit zwei Unterabfragen auf die Bezeugungstabelle und \nauf die stellenbezogene L\xc3\xbcckenliste. Die Performance kann man dabei sp\xc3\xbcrbar \nerh\xc3\xb6hen, wenn man je einen Index auf das gesuchte Feld HSNR erstellt:\nCREATE INDEX hsnr_index ON ActsNNatt (hsnr);\nCREATE INDEX hsnr_index ON ActsNNattLac (hsnr);\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef main_10(database, chapter, verbose=False):\n    def printer(s):\n        """"""\n        Print information if needed.\n        """"""\n        if verbose:\n            print s\n    import access\n    import db_access3\n    from Address import formatNumber, decodeAdr\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n    databaseAtt = database\n    # Generate name of newtable and lactable\n    sChapter = str(chapter)\n    if int(chapter) < 10:\n        sChapter = ""0"" + sChapter\n    tableAtt = ""Acts"" + sChapter + ""att""\n    tableAttLac = tableAtt + ""Lac""\n    if database.endswith(""2""):\n        tableAtt += ""_2""\n        tableAttLac += ""_2""\n    if database.endswith(""3""):\n        tableAtt += ""_3""\n        tableAttLac += ""_3""\n\n    databaseList = ""ECM_Acts_Mss""\n    tableList = ""ActsMsList_2""  # ""ActsMsList""\n    pattern = ""Apg"" + formatNumber(chapter, 2)\n    # Alle variierten Stellen mit ihren a-Lesarten\n    cmd = ""select distinct anfadr, endadr, lesart, labez, base ""\n    cmd += ""from `%s`.`%s` "" % (databaseAtt, tableAtt)\n    cmd += ""where `hs` = \'A\' ""\n#    cmd += ""where (labez like \'a\' and labezsuf like \'\' and base like \'a\') ""\n#    cmd += ""or (labez like \'b\' and labezsuf like \'\' and base like \'b\') ""\n#    cmd += ""order by anfadr asc, endadr desc ""\n#    print cmd\n    cursor.execute(cmd)\n    passages = cursor.fetchall()\n    for passage in passages:\n        anfadr = passage[0]\n        endadr = passage[1]\n        s = ""Working on %d/%d."" % (anfadr, endadr)\n        printer(s)\n        lesart = passage[2]\n        labez = passage[3]\n        base = passage[4]\n        if lesart is None:\n            lesart = """"\n        b, bc, bv, bw, ec, ev, ew = decodeAdr(anfadr, endadr)\n        # Alle Handschriften, die in diesem Kapitel Text,\n        # d.h. keine L\xc3\xbccke haben, die das ganze Kapitel umfasst.\n        # 1. \'a\' auffuellen\n        cmd = ""insert into `%s`.`%s` "" % (databaseAtt, tableAtt)\n        cmd += ""(hsnr, hs, anfadr, endadr, buch, kapanf, versanf, wortanf, ""\n        cmd += ""kapend, versend, wortend, labez, labezsuf, anfalt, endalt, ""\n        cmd += ""lesart, base) ""\n        cmd += ""select ms, witn, %d, %d, "" % (anfadr, endadr)\n        cmd += ""%d, %d, %d, %d, "" % (b, bc, bv, bw)\n        cmd += ""%d, %d, %d, "" % (ec, ev, ew)\n        cmd += ""\'%s\', \'\', "" % (labez)\n        cmd += ""%d, "" % (anfadr)\n        cmd += ""%d, "" % (endadr)\n        cmd += ""\'%s\', "" % (lesart.decode(""utf-8""))\n        cmd += ""\'%s\' "" % (base)\n        cmd += ""from `%s`.`%s` "" % (databaseList, tableList)  # ActsMsList\n        cmd += ""where %s = 1 "" % (pattern)\n        cmd += ""and ms not in (select hsnr from `%s`.`%s` "" % (databaseAtt,\n                                                               tableAttLac)\n        cmd += ""where anfadr >= %d and endadr <= %d) "" % (anfadr, endadr)\n        cmd += ""and ms not in (select hsnr from `%s`.`%s` "" % (databaseAtt,\n                                                               tableAtt)\n        cmd += ""where anfadr = %d and endadr = %d ) "" % (anfadr, endadr)\n        cmd += ""order by ms ""\n        cursor.execute(cmd)\n        dba.commit()\n        # 2. \'zz\' auffuellen\n        cmd = ""insert into `%s`.`%s` "" % (databaseAtt, tableAtt)\n        cmd += ""(hsnr, hs, anfadr, endadr, buch, kapanf, versanf, wortanf, ""\n        cmd += ""kapend, versend, wortend, labez, labezsuf, anfalt, endalt, ""\n        cmd += ""lesart) ""\n        cmd += ""select ms, witn, %d, %d, "" % (anfadr, endadr)\n        cmd += ""%d, %d, %d, %d, "" % (b, bc, bv, bw)\n        cmd += ""%d, %d, %d, \'zz\', \'\', %d, %d, \'%s\' "" % (ec, ev, ew, anfadr,\n                                                        endadr, lesart)\n        cmd += ""from `%s`.`%s` "" % (databaseList, tableList)\n        cmd += ""where %s = 1 "" % (pattern)\n        cmd += ""and ms not in (select hsnr from `%s`.`%s` "" % (databaseAtt,\n                                                               tableAtt)\n        cmd += ""where anfadr = %d and endadr = %d ) "" % (anfadr, endadr)\n        cmd += ""order by ms ""\n        cursor.execute(cmd)\n        dba.commit()\n        # Ergaenzendes Update zur korrekten Verzeichnung der Fehlverse\n        # 3. Update der umfassten variierten Stellen\n        # Das Feld \'comp\' wird durch das Skript umfasstevarianten.py\n        # beschrieben und kennzeichnet umfasste Varianten.\n        cmd = ""update `%s`.`%s` "" % (databaseAtt, tableAtt)\n        cmd += ""set labez = \'zu\', ""\n        cmd += ""lesart = \'\' ""\n        cmd += ""where comp like \'x\' and base like \'a\' ""\n        # Adressen der Fehlverse hier hart kodieren:\n        # vgl. die Datenbanktabelle Apparat.Fehlverse.\n        # Diese Adressen schlie\xc3\x9fen Additamenta ein.\n        cmd += ""and (""\n        cmd += ""anfadr >= 50837002 and endadr <= 50837047 ""\n        cmd += ""or anfadr >= 51534002 and endadr <= 51534013 ""\n        cmd += ""or anfadr >= 52506020 and endadr <= 52408015 ""\n        cmd += ""or anfadr >= 52829002 and endadr <= 52829025 ""\n        cmd += ""); ""\n        cursor.execute(cmd)\n        dba.commit()\n        ## Weitere Ergaenzung:\n        ## 4. Quelle der b-Lesarten gilt als urspruenglich, alle\n        ## anderen Lesarten stammen per default von b ab.\n        #cmd = ""update `%s`.`%s` "" % (databaseAtt, tableAtt)\n        #cmd += ""set s1 = \'*\' ""\n        #cmd += ""where labez = \'b\' and (""\n        #cmd += ""anfadr >= 50837002 and endadr <= 50837047 ""\n        #cmd += ""or anfadr >= 51534002 and endadr <= 51534013 ""\n        #cmd += ""or anfadr >= 52506020 and endadr <= 52408015 ""\n        #cmd += ""or anfadr >= 52829002 and endadr <= 52829025) ""\n        #cursor.execute(cmd)  # FIXME: TESTEN\n        #cmd = ""update `%s`.`%s` "" % (databaseAtt, tableAtt)\n        #cmd += ""set s1 = \'b\' ""\n        #cmd += ""where labez <> \'b\' and (""\n        #cmd += ""anfadr >= 50837002 and endadr <= 50837047 ""\n        #cmd += ""or anfadr >= 51534002 and endadr <= 51534013 ""\n        #cmd += ""or anfadr >= 52506020 and endadr <= 52408015 ""\n        #cmd += ""or anfadr >= 52829002 and endadr <= 52829025) ""\n        #cursor.execute(cmd)  # FIXME: TESTEN\n    # Nachbesserung: zz-Zeugen haben keine Lesart\n    cmd = ""update `%s`.`%s` "" % (databaseAtt, tableAtt)\n    cmd += ""set lesart = \'\' where labez = \'zz\' ""\n    cursor.execute(cmd)\n    dba.commit()\n    cursor.close()\n    dba.close()\n\nif __name__ == ""__main__"":\n    from optparse import OptionParser\n    parser = OptionParser()\n    parser.add_option(""-d"", ""--database"", dest=""database"",\n                      help=""Giving database name"")\n    parser.add_option(""-c"", ""--chapter"", dest=""chapter"",\n                      help=""Chapter to add a-testation"")\n    parser.add_option(""-v"", ""--verbose"", dest=""verbose"",\n                      action=""store_true"", help=""Verbose mode"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    if opts.database is None or opts.chapter is None:\n        import sys\n        print ""At least one parameter is missing!""\n        print ""See python %s -h"" % sys.argv[0]\n        sys.exit(1)\n    main_10(opts.database, opts.chapter, opts.verbose)\n'"
scripts/python/prepare4cbgm_11.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n""""""\n8. Handschriftenliste \'ActsMsList\' anlegen. Die Handschrift bekommt in dem\n    entsprechenden Kapitel eine 1, wenn sie dort Text enthaelt. Mit anderen\n    Worten: Sie bekommt eine 0, wenn das ganze Kapitel eine Luecke ist. \n    Es wird hier auf die systematische Lueckenliste zurueckgegriffen.\n    Ergaenzung vom 08.11.2012:\n    Es kann sein, dass der erhaltene Text nur als zz bzw. zw gelistet ist. \n    Er kann dann fuer die CBGM nicht verwertet werden. Kapitel, die keine\n    echte Variante enthalten, muessen ebenfalls eine 0 erhalten. \n\nVgl. die Datei ArbeitsablaufCBGMApg1.pdf in der Mail\nvon Klaus am 25.01.2012\n""""""\nimport parser\n\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef checkChapter(cursor, attTable, hsnr):\n\t""""""\n\tEnthaelt das Kapitel mindestens eine echte Variante?\n\t""""""\n\tcmd  = ""select count(distinct labez) from %s "" % (attTable)\n\tcmd += ""where labez not like \'z_\' ""\n\tcmd += ""and hsnr = %d "" % (hsnr)\n\t#print cmd\n\tcursor.execute(cmd)\n\tres = cursor.fetchone()\n\tcount = int(res[0])\n\tif count > 0:\n\t\treturn 1\n\treturn 0\n\n\ndef createMsList(cursor, database, table):\n    """"""\n    (Neu) Anlegen der Handschriftentabelle.\n    """"""\n    # Alte Tabelle l\xc3\xb6schen, wenn vorhanden\n    cmd  = ""drop table if exists `%s`.`%s` "" % (database, table)\n    cursor.execute(cmd)\n    # Tabelle neu anlegen\n    cmd  = ""create table `%s`.`%s` "" % (database, table)\n    cmd += ""(id int auto_increment primary key, ""\n    cmd += ""`CHECK` varchar(1) default \'\', ""\n    cmd += ""WITN varchar(8) not null default \'\', ""\n    cmd += ""MS int not null default -1, ""\n    cmd += ""Apg01 int not null default -1, ""\n    cmd += ""Apg02 int not null default -1, ""\n    cmd += ""Apg03 int not null default -1, ""\n    cmd += ""Apg04 int not null default -1, ""\n    cmd += ""Apg05 int not null default -1, ""\n    cmd += ""Apg06 int not null default -1, ""\n    cmd += ""Apg07 int not null default -1, ""\n    cmd += ""Apg08 int not null default -1, ""\n    cmd += ""Apg09 int not null default -1, ""\n    cmd += ""Apg10 int not null default -1, ""\n    cmd += ""Apg11 int not null default -1, ""\n    cmd += ""Apg12 int not null default -1, ""\n    cmd += ""Apg13 int not null default -1, ""\n    cmd += ""Apg14 int not null default -1, ""\n    cmd += ""Apg15 int not null default -1, ""\n    cmd += ""Apg16 int not null default -1, ""\n    cmd += ""Apg17 int not null default -1, ""\n    cmd += ""Apg18 int not null default -1, ""\n    cmd += ""Apg19 int not null default -1, ""\n    cmd += ""Apg20 int not null default -1, ""\n    cmd += ""Apg21 int not null default -1, ""\n    cmd += ""Apg22 int not null default -1, ""\n    cmd += ""Apg23 int not null default -1, ""\n    cmd += ""Apg24 int not null default -1, ""\n    cmd += ""Apg25 int not null default -1, ""\n    cmd += ""Apg26 int not null default -1, ""\n    cmd += ""Apg27 int not null default -1, ""\n    cmd += ""Apg28 int not null default -1, ""\n    # und eine Spalte als summarische Angabe: \n    cmd += ""Apg int not null default -1) ""\n    cursor.execute(cmd)\n\n\ndef insertMss(cursor, database, table):\n    # Handschriftennummern einlesen\n    cmd  = ""select hsnr, hs from `%s`.`MssActsECM` order by hsnr "" % (database)\n    cursor.execute(cmd)\n    rows = cursor.fetchall()\n    for row in rows:\n        cmd  = ""insert into `%s`.`%s` (ms, witn) value (%s, \'%s\') "" % (database,\n        \t\t\t\t\t\t\t       table,\n        \t\t\t\t\t\t\t       row[0],\n        \t\t\t\t\t\t\t       row[1])\n        cursor.execute(cmd)\n\n\ndef main_8(databaseAtt, tab, recreateTable=False, verbose=False):\n    def printer(s):\n        """"""\n        Print information if needed.\n        """"""\n        if verbose:\n            print s\n    from NestleAland import NA\n    import access\n    import db_access3\n    na = NA()\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n    databaseList = ""ECM_Acts_Mss""\n    table = ""ActsMsList_2"" \n    chapter = tab[4:6]\n    attTable = ""`ECM_Acts_CBGMPh3`.`Acts""+chapter+""att_3`""\n#    # Anlegen der Tabelle\n#    if recreateTable:\n#        createMsList(cursor, databaseList, table)\n#        # Einlesen der Handschriftennummern - mehr noch nicht\n#        insertMss(cursor, databaseList, table)\n    # Welche Handschriftennummern gibt es in der Tabelle?\n    cmd  = ""select ms from `%s`.`%s` order by `ms` "" % (databaseList, table)\n    cursor.execute(cmd)\n    mss = cursor.fetchall()\n    for ms in mss:\n        # Update der einzelnen Zellen pro Handschrift und Kapitel\n        cmd = ""select versanf, versend, wortanf, wortend ""\n        cmd += ""from `%s`.`%s` "" % (databaseAtt, tab)\n        cmd += ""where hsnr = %d and kapanf = %d "" % (ms[0], int(chapter))\n        #print cmd\n        # tab ist hier die Lueckenliste!\n        counter = cursor.execute(cmd)\n        #print ""counter ist "",counter\n        result = cursor.fetchone()\n        if counter == 1:\n            try:\n                #print ""ms ist "", ms\n                #print result\n                versanf = result[0]\n                versend = result[1]\n                wortanf = result[2]\n                wortend = result[3]\n                maxvers = na.getMaxVerse(5, int(chapter))\n                maxword = na.getMaxWord(5, int(chapter), int(maxvers))\n                if versanf == 1 and wortanf == 2 and versend == maxvers and wortend >= maxword: # Lacune umgreift ganzes Kapitel\n                    value = 0\n                else: # es ist (mindestens teilweise) Text erhalten\n                    value = checkChapter(cursor, attTable, ms)\n            except TypeError:\n                value = 1\n        else:\n            #value = 1\n            value = checkChapter(cursor, attTable, ms)\n        cmd  = ""update `%s`.`%s` "" % (databaseList, table)\n        cmd += ""set Apg%s = %d where ms = %s "" % (chapter, value, ms[0])\n        printer(cmd)\n        cursor.execute(cmd)\n        #print cmd\n    # Update der summarischen Angabe \'Apg\'\n    for ms in mss:\n        cmd  = ""select Apg01, Apg02, Apg03, Apg04, Apg05, Apg06, Apg07, Apg08, ""\n        cmd += ""Apg09, Apg10, Apg11, Apg12, Apg13, Apg14, Apg15, Apg16, Apg17, Apg18, ""\n        cmd += ""Apg19, Apg20, Apg21, Apg22, Apg23, Apg24, Apg25, Apg26, Apg27, Apg28 ""\n        cmd += ""from `%s`.`%s` "" % (databaseList, table)\n        cmd += ""where ms = %s "" % (ms[0])\n        cursor.execute(cmd)\n        result = cursor.fetchone()\n        sum = 0\n        for n in range(28):\n            sum += int(result[n])\n        # Summe == 28 d.h. alle Kapitel haben Text\n        if sum == 28:\n            cmd  = ""update `%s`.`%s` "" % (databaseList, table)\n            cmd += ""set Apg = 1 where ms = %s "" % (ms[0])\n        else:\n            cmd  = ""update `%s`.`%s` "" % (databaseList, table)\n            cmd += ""set Apg = 0 where ms = %s "" % (ms[0])\n        cursor.execute(cmd)\n    cursor.close()\n    dba.close()\n\nif __name__ == ""__main__"":\n    import sys\n    from optparse import OptionParser\n    parser = OptionParser()\n    parser.add_option(""-r"", ""--recreate"", dest=""recreate"", action=""store_true"", help=""Recreate ECM_Acts_Mss.ActsMsList_2"")\n    parser.add_option(""-d"", ""--database"", dest=""database"", help=""Database containing systematic lacunae list"")\n    parser.add_option(""-t"", ""--table"", dest=""table"", help=""Giving name systematic lacunae list"")\n    parser.add_option(""-v"", ""--verbose"", dest=""verbose"", action=""store_true"", help=""Verbose mode"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    if opts.database is None:\n        print ""Error: Database name is missing!""\n        print ""See python %s -h"" % sys.argv[0]\n        sys.exit(1)\n    if opts.table is None:\n        print ""Error: Table name is missing!""\n        print ""See python %s -h"" % sys.argv[0]\n        sys.exit(1)\n    if opts.recreate is None:\n        main_8(opts.database, opts.table, opts.verbose)\n    else:\n        main_8(opts.database, opts.table, opts.recreate, opts.verbose)\n'"
scripts/python/prepare4cbgm_1b.py,0,"b'#!/usr/bin/python\n#-*- encoding: utf-8 -*-\n""""""\nVersionelle Eintr\xc3\xa4ge (Handschriftennummer > 500000) l\xc3\xb6schen.\n""""""\n__author__=""volker.krueger@uni-muenster.de""\n\ndef main_1b(database, chapter=0, verbose=False):\n    def printer(s):\n        """"""\n        Print information if needed.\n        """"""\n        if verbose:\n            print s\n    import access\n    import db_access3\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n    if chapter == 0:\n        # 1.1. Anzeige aller Tabellen in der Quelldatenbank\n        cmd = ""show tables from `%s` like \'Acts%%\'; "" % (database)\n    else:\n        # 1.1. Anzeige der (beiden) Tabellen eines Kapitels\n        if int(chapter) < 10:\n            sChapter = ""0"" + str(chapter)\n        else:\n            sChapter = str(chapter)\n        cmd = ""show tables from `%s` like \'Acts%s%%\'; "" % (database, sChapter)\n    printer(cmd)\n    cursor.execute(cmd)\n    tables = cursor.fetchall()\n    for table in tables:\n        # L\xc3\xbcckenliste \xc3\xbcberspringen\n        if table[0].endswith(""lac""):\n            continue\n        # 1.2. Datens\xc3\xa4tze mit Handschriftennummer > 500000 l\xc3\xb6schen\n        cmd = ""delete from `%s`.`%s` "" % (database, table[0])\n        cmd += ""where `hsnr` >= 500000 ""\n        printer(cmd)\n        cursor.execute(cmd)\n        dba.commit()\n    cursor.close()\n    dba.close()\n\nif __name__ == ""__main__"":\n    from optparse import OptionParser\n    parser = OptionParser()\n    parser.add_option(""-d"", ""--database"", dest=""database"", help=""Giving database"")\n    parser.add_option(""-c"", ""--chapter"", dest=""chapter"", help=""Optionally giving one chapter"")\n    parser.add_option(""-v"", ""--verbose"", dest=""verbose"", action=""store_true"", help=""Verbose mode"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    if opts.database is None or opts.chapter is None:\n        import sys\n        print ""At least one parameter is missing.""\n        print ""python %s -h"" % (sys.argv[0])\n        sys.exit(1)\n    main_1b(opts.database, opts.chapter, opts.verbose)\n'"
scripts/python/prepare4cbgm_2.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n""""""\n2. Korrekturen in den Acts-Tabellen: L-Notierungen nur im Feld LEKT,\n*- u. C-Notierungen nur im Feld KORR.\nGelegentlich steht an Stellen, an denen mehrere Lektionen desselben\nLektionars zu verzeichnen sind, in KORR ein ueberfluessiges \'L\' ohne Nummer.\nEs kommt auch vor, dass L1 und L2 in KORR stehen oder C-Notierungen in LEKT.\n\nVgl. die Datei ArbeitsablaufCBGMApg1.pdf in der Mail\nvon Klaus am 25.01.2012\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef main_2(db, chapter, verbose=False):\n    def printer(s):\n        """"""\n        Print information if needed.\n        """"""\n        if verbose:\n            print s\n    import access\n    import db_access3\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n    # Tabellennamen erschlie\xc3\x9fen\n    sChapter = str(chapter)\n    if int(chapter) < 10:\n        sChapter = ""0"" + sChapter\n    cmd = ""show tables from `%s` like \'Acts%s%%\'; "" % (db, sChapter)\n    cursor.execute(cmd)\n    tables = cursor.fetchall()\n    for table in tables:\n        cmd = ""select korr, lekt, id from %s.%s "" % (db, table[0])\n        cursor.execute(cmd)\n        rows = cursor.fetchall()\n        for row in rows:\n            korr = row[0]\n            lekt = row[1]\n            id = row[2]\n            if korr is not None:\n                if korr.find(""L"") >= 0:\n                    cmd = ""update %s.%s "" % (db, table[0])\n                    cmd += ""set lekt = \'%s\' where id = %d "" % (korr, id)\n                    printer(cmd)\n                    cursor.execute(cmd)\n                    cmd = ""update %s.%s "" % (db, table[0])\n                    cmd += ""set korr = \'\' where id = %d "" % (id)\n                    printer(cmd)\n                    cursor.execute(cmd)\n            if lekt is not None:\n                pos = lekt.find(""C"")\n                if pos >= 0:\n                    pass\n                else:\n                    pos = lekt.find(""*"")\n                if pos >= 0:\n                    cmd = ""update %s.%s "" % (db, table[0])\n                    cmd += ""set korr = \'%s\' where id = %d "" % (lekt, id)\n                    printer(cmd)\n                    cursor.execute(cmd)\n                    cmd = ""update %s.%s "" % (db, table[0])\n                    cmd += ""set lekt = \'\' where id = %d "" % (id)\n                    printer(cmd)\n                    cursor.execute(cmd)\n            dba.commit()\n    cursor.close()\n    dba.close()\n\nif __name__ == \'__main__\':\n    import sys\n    from optparse import OptionParser\n    parser = OptionParser()\n    parser.add_option(""-d"", ""--database"", dest=""database"",\n                      help=""Giving database name"")\n    parser.add_option(""-c"", ""--chapter"", dest=""chapter"",\n                      help=""Giving chapter number"")\n    parser.add_option(""-v"", ""--verbose"", dest=""verbose"",\n                      action=""store_true"", help=""Verbose mode"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    if opts.database is None or opts.chapter is None:\n        print ""Error: At least one parameter necessary is missing!""\n        print ""See python %s -h"" % sys.argv[0]\n        sys.exit(1)\n    main_2(opts.database, opts.chapter, opts.verbose)'"
scripts/python/prepare4cbgm_3.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n""""""\n3. Tabellenstruktur bereinigen: Felder SUFFIX1, ADR,\n   ZUSATZ, LesartenKey loeschen\n\n   3.1. Anzeige aller Tabellen in der Datenbank\n   3.2. Tabellenfelder loeschen mit \'alter table ..., drop ...\'\nVgl. die Datei ArbeitsablaufCBGMApg1.pdf in der Mail\nvon Klaus am 25.01.2012\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef main_3(db, chapter, verbose=False):\n    def printer(s):\n        """"""\n        Print information if needed.\n        """"""\n        if verbose:\n            print s\n    import access\n    import db_access3\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n    # Tabellennamen erschlie\xc3\x9fen\n    sChapter = str(chapter)\n    if int(chapter) < 10:\n        sChapter = ""0"" + sChapter\n    cmd = ""show tables from `%s` like \'Acts%s%%\'; "" % (db, sChapter)\n    cursor.execute(cmd)\n    tables = cursor.fetchall()\n    for table in tables:\n        # Tabellenfelder loeschen mit \'alter table ..., drop ...\'\n        cmd = ""alter table `%s`.`%s` "" % (db, table[0])\n        cmd += ""drop SUFFIX1, drop ADR, drop ZUSATZ, drop LesartenKey ""\n        printer(cmd)\n        try:\n            cursor.execute(cmd)\n        except:\n            pass\n    dba.commit()\n    cursor.close()\n    dba.close()\n\nif __name__ == ""__main__"":\n    from optparse import OptionParser\n    parser = OptionParser()\n    parser.add_option(""-d"", ""--database"", dest=""database"")\n    parser.add_option(""-c"", ""--chapter"", dest=""chapter"")\n    parser.add_option(""-v"", ""--verbose"", dest=""verbose"", action=""store_true"", help=""Verbose mode"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    if opts.database is None or opts.chapter is None:\n        import sys\n        print ""Error: At least one parameter necessary is missing!""\n        print ""See python %s -h"" % sys.argv[0]\n        sys.exit(1)\n    main_3(opts.database, opts.chapter, opts.verbose)'"
scripts/python/prepare4cbgm_4.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n""""""\n4. Tabellen in ECM_Acts_Update (bzw. -Ph2) nach ECM_Acts_CBGM (bzw. -Ph2)\n    kopieren. Die folgenden Arbeitsschritte (5 ff) beziehen sich auf\n    Acts<n>att.\n\nVgl. die Datei ArbeitsablaufCBGMApg1.pdf in der Mail\nvon Klaus am 25.01.2012\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef main_4(source, target, chapter, verbose=False):\n    def printer(s):\n        """"""\n        Print information if needed.\n        """"""\n        if verbose:\n            print s\n    import access\n    import db_access3\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n    # Datenbank ausw\xc3\xa4hlen\n    cmd = ""use %s"" % (source)\n    cursor.execute(cmd)\n    dba.commit()\n    # Anfang des Tabellennamens generieren\n    sChapter = str(chapter)\n    if int(chapter) < 10:\n        sChapter = ""0"" + sChapter\n    tablename = ""Acts"" + sChapter\n    # Tabellen erfragen\n    cmd = ""show tables like \'%s%%\'"" % (tablename)\n    cursor.execute(cmd)\n    tables = cursor.fetchall()\n    for t in tables:\n        table = t[0]\n        # Ausschneiden der Buch- und Kapitelbezeichnung\n        name = tablename[0:6]\n        # Neuen Namen zusammensetzen vgl. unten\n        name += ""att""\n        if table.endswith(""lac""):\n            name += ""Lac""\n        if target.endswith(""2""):\n            name += ""_2""\n        if target.endswith(""3""):\n            name += ""_3""\n        # Evtl. vorhandene Tabelle loeschen\n        cmd = ""drop table if exists `%s`.`%s` "" % (target, name)\n        printer(cmd)\n        cursor.execute(cmd)\n        # Tabellen anlegen mit \'create table like ...\'\n        cmd = ""create table `%s`.`%s` "" % (target, name)\n        cmd += ""like `%s`.`%s` "" % (source, table)\n        printer(cmd)\n        cursor.execute(cmd)\n        # Befuellen mit \'insert into ... select * from ...\'\n        cmd = ""insert into `%s`.`%s` "" % (target, name)\n        cmd += ""select * from `%s`.`%s` "" % (source, table)\n        printer(cmd)\n        cursor.execute(cmd)\n    dba.commit()\n    cursor.close()\n    dba.close()\n\n\nif __name__ == ""__main__"":\n    from optparse import OptionParser\n    parser = OptionParser()\n    parser.add_option(""-s"", ""--source"", dest=""source"",\n                      help=""Giving source database"")\n    parser.add_option(""-t"", ""--target"", dest=""target"",\n                      help=""Giving target database"")\n    parser.add_option(""-c"", ""--chapter"", dest=""chapter"",\n                      help=""Giving one chapter"")\n    parser.add_option(""-v"", ""--verbose"", dest=""verbose"",\n                      action=""store_true"", help=""Verbose mode"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    if opts.source is None or opts.target is None or opts.chapter is None:\n        import sys\n        print ""At least one parameter is missing!""\n        print ""Call %s -h"" % sys.argv[0]\n        sys.exit(1)\n    main_4(opts.source, opts.target, opts.chapter, opts.verbose)'"
scripts/python/prepare4cbgm_5.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n""""""\n5. Stellen loeschen, an denen nur eine oder mehrere f- oder\no-Lesarten vom A-Text abweichen.\n\nNicht l\xc3\xb6schen, wenn an dieser variierten Stelle eine\nVariante \'b\' - \'y\' erscheint.\n\n\xc3\x84nderung 2014-12-16:\nAct 28,29/22 geh\xc3\xb6rt zu einem Fehlvers. Dort gibt es u.U. keine Variante neben\nb, sondern nur ein Orthographicum. Wir suchen also nicht mehr noch einer\nVariante \'b\' bis \'y\', sondern z\xc3\xa4hlen die Varianten. Liefert getReadings nur 1\nzur\xc3\xbcck, gibt es keine Varianten.\n\nVgl. die Datei ArbeitsablaufCBGMApg1.pdf in der Mail\nvon Klaus am 25.01.2012\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef main_5(database, chapter, verbose=False):\n    def printer(s):\n        """"""\n        Print information if needed.\n        """"""\n        if verbose:\n            print s\n    import access\n    import db_access3\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n    # Tabellennamen aus chapter generieren\n    sChapter = str(chapter)\n    if int(chapter) < 10:\n        sChapter = ""0"" + sChapter\n    table = ""Acts"" + sChapter + ""att""\n    if database.endswith(""2""):\n        table += ""_2""\n    if database.endswith(""3""):\n        table += ""_3""\n    # Alle variierten Stellen auflisten\n    passages, void = dba.getPassages(database, table)\n    for passage in passages:\n        anfadr = passage[0]\n        endadr = passage[1]\n        # new code\n        # get readings without labezsuf and overlapping variants\n        cmd = ""select distinct anfadr, endadr, labez ""\n        cmd += ""from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %s and endadr = %s "" % (anfadr, endadr)\n        cmd += ""and labez not like \'z%%\'; ""\n        count = cursor.execute(cmd)\n        if count == 1:\n            cmd = ""delete from `%s`.`%s` "" % (database, table)\n            cmd += ""where anfadr = %s and endadr = %s "" % (anfadr, endadr)\n            cursor.execute(cmd)\n        # old code\n        ## Pro variierter Stelle alle Lesarten auflisten\n        #readings, void = dba.getReadings(database, table, anfadr, endadr)\n        #HasVariant = False\n        #for reading in readings:\n        #    labez = reading[2]\n        #    # Gibt es eine Lesart anders als \'a\' oder \'z...\'?\n        #    if labez in (""b"", ""c"", ""d"", ""e"", ""f"", ""g"", ""h"", ""i"", ""j"",\n        #                 ""k"", ""l"", ""m"", ""n"", ""o"", ""p"", ""q"", ""r"", ""s"",\n        #                 ""t"", ""u"", ""v"", ""w"", ""x"", ""y""):\n        #        HasVariant = True\n        #        break\n        #if not HasVariant:\n        #    # Wenn nicht, dann diesen Eintrag loeschen, da es sich nicht\n        #    # um eine variierte Stelle handelt: Es gibt keine Varianten\n        #    cmd = ""delete from `%s`.`%s` "" % (database, table)\n        #    cmd += ""where anfadr = %s and endadr = %s "" % (anfadr, endadr)\n        #    printer(cmd)\n        #    cursor.execute(cmd)\n        dba.commit()\n    cursor.close()\n    dba.close()\n\nif __name__ == ""__main__"":\n    from optparse import OptionParser\n    parser = OptionParser()\n    parser.add_option(""-d"", ""--database"", dest=""database"",\n                      help=""Giving database name"")\n    parser.add_option(""-c"", ""--chapter"", dest=""chapter"",\n                      help=""Giving chapter number"")\n    parser.add_option(""-v"", ""--verbose"", dest=""verbose"",\n                      action=""store_true"", help=""Verbose mode"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    if opts.database is None or opts.chapter is None:\n        import sys\n        print ""At least one parameter is missing!""\n        print ""See python %s -h"" % sys.argv[0]\n        sys.exit(1)\n    main_5(opts.database, opts.chapter, opts.verbose)'"
scripts/python/prepare4cbgm_5b.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n""""""\n5b. 20. Mai 2015\nCommentary manuscripts like 307 cannot be treated like lectionaries where we\nchoose the first text. If a T1 or T2 reading is found they have to be\ndeleted. A new zw reading is created containing the old readings as suffix.\n\nThis has to be done as long as both witnesses are present.\n\nIf the counterpart of one entry belongs to the list of lacunae\nthe witness will be treated as normal witness. The T notation can be deleted.\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef getTableName(cursor, database, chapter):\n    """"""\n    Tabellennamen aus chapter generieren.\n    Es gibt eine Apparattabelle und eine L\xc3\xbcckenliste pro Kapitel.\n    """"""\n    table = """"\n    sChapter = str(chapter)\n    if int(chapter) < 10:\n        sChapter = ""0"" + sChapter\n    cmd = ""SHOW TABLES FROM %s LIKE \'Acts%s%%\';"" % (database, sChapter)\n    cursor.execute(cmd)\n    tables = cursor.fetchall()\n    for t in tables:\n        s = str(t[0])\n        if s.endswith(""lac""):\n            continue\n        table = s\n    return table\n\ndef main_5b(database, chapter, verbose=False):\n    def printer(s):\n        """"""\n        Print information if needed.\n        """"""\n        if verbose:\n            print s\n    import access\n    import db_access3\n    # Open handles\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n    # Choose database\n    cmd = ""USE %s;"" % (database)\n    cursor.execute(cmd)\n    dba.commit()\n    table = getTableName(cursor, database, chapter)\n    # Select T+number witnesses ordered by addresses\n    cmd = ""SELECT DISTINCT `ANFADR`, `ENDADR` FROM `%s`.`%s` "" % (database, table)\n    cmd += ""WHERE `HS` LIKE \'%%T%%\' ""\n    cmd += ""ORDER BY `ANFADR`, `ENDADR` DESC;""\n    cursor.execute(cmd)\n    addresses = cursor.fetchall()\n    for adr in addresses:\n        anf = adr[0]\n        end = adr[1]\n        cmd = ""SELECT DISTINCT `HSNR` FROM `%s`.`%s` "" % (database, table)\n        cmd += ""WHERE `ANFADR` = %s AND `ENDADR` = %s "" % (anf, end)\n        cmd += ""AND `HS` LIKE \'%%T%%\';""\n        cursor.execute(cmd)\n        hss = cursor.fetchall()\n        for hsnr in hss:\n            # three lists to store information:\n            del_ids = []\n            labez_s = []\n            labezsuf_s = []\n            cmd = ""SELECT `ID`, `HS`, `LABEZ`, `ANFADR`, `ENDADR`, `BUCH`, ""\n            cmd += ""`KAPANF`, `VERSANF`, `WORTANF`, `KAPEND`, `VERSEND`, `WORTEND`, ""\n            cmd += ""`HSNR`, `LABEZSUF` ""\n            cmd += ""FROM `%s`.`%s` "" % (database, table)\n            cmd += ""WHERE `HS` LIKE \'%%T%%\' ""\n            cmd += ""AND `ANFADR` = %s AND `ENDADR` = %s "" % (anf, end)\n            cmd += ""AND `HSNR` = %s "" % (hsnr)\n            cmd += ""ORDER BY `LABEZ`, `LABEZSUF`;""\n            count = cursor.execute(cmd)\n            if count == 1:\n                # the counterpart seems to be a lacuna: treat this entry as\n                # normal witness and delete the T notation\n                row = cursor.fetchone()\n                ident = row[0]\n                hs = row[1]\n                pos = hs.find(""T"")\n                hs = hs[:len(hs)-2] # chop off T1 etc.\n                cmd = ""UPDATE `%s`.`%s` "" % (database, table)\n                cmd += ""SET HS = \'%s\' "" % (hs)\n                cmd += ""WHERE id = %s;"" % (ident)\n                print cmd\n#               cursor.execute(cmd)\n            else: # count > 1\n                rows = cursor.fetchall()\n                for row in rows:\n                    del_ids.append(row[0])\n                    labez_s.append(row[2])\n                    labezsuf_s.append(row[13])\n                    book = row[5]\n                    bc = row[6]\n                    bv = row[7]\n                    bw = row[8]\n                    ec = row[9]\n                    ev = row[10]\n                    ew = row[11]\n                    anf = row[3]\n                    end = row[4]\n                    labez = row[2]\n                    hs = row[1]\n                    hsnr = row[12]\n                    pos = hs.find(""T"")\n                    hs = hs[:pos]\n                # Delete these witnesses\n                for i in del_ids:\n                    cmd = ""DELETE FROM `%s`.`%s` "" % (database, table)\n                    cmd += ""WHERE `ID` = %d;"" % (i)\n                    print cmd\n#                    cursor.execute(cmd)\n                # Insert a new zw reading\n                suffix2 = """"\n                for n in range(len(del_ids)):\n                    l1 = labez_s[n]\n                    l2 = labezsuf_s[n]\n                    suffix2 = suffix2 + l1\n                    if len(l2) > 0:\n                        suffix2 = suffix2 + ""_"" + l2\n                    suffix2 = suffix2 + ""/""\n                suffix2 = suffix2[:len(suffix2)-1] # chop off last slash\n                cmd = ""INSERT INTO `%s`.`%s` "" % (database, table)\n                cmd += ""(BUCH, KAPANF, VERSANF, WORTANF, KAPEND, VERSEND, WORTEND, ""\n                cmd += ""ANFADR, ENDADR, LABEZ, LABEZSUF, HS, HSNR) VALUES (""\n                cmd += ""%d, %d, %d, %d, %d, %d, %d, "" % (book, bc, bv, bw, ec, ev, ew)\n                cmd += ""%d, %d, \'zw\', \'%s\', \'%s\', %d"" % (anf, end, suffix2, hs, hsnr)\n                cmd += "");""\n                print cmd\n#               cursor.execute(cmd)\n    # Inno-DB tables need an explicit commit statement\n    dba.commit()\n    # Close handles\n    cursor.close()\n    dba.close()\n\nif __name__ == ""__main__"":\n    from optparse import OptionParser\n    parser = OptionParser()\n    parser.add_option(""-d"", ""--database"", dest=""database"",\n                      help=""Giving database name"")\n    parser.add_option(""-c"", ""--chapter"", dest=""chapter"",\n                      help=""Giving chapter number"")\n    parser.add_option(""-v"", ""--verbose"", dest=""verbose"",\n                      action=""store_true"", help=""Verbose mode"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    if opts.database is None or opts.chapter is None:\n        import sys\n        print ""At least one parameter necessary is missing!""\n        print ""See python %s -h"" % sys.argv[0]\n        sys.exit(1)\n    main_5b(opts.database, opts.chapter, opts.verbose)'"
scripts/python/prepare4cbgm_6.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n""""""\n6. Lesarten, die nicht von der ersten Hand stammen, loeschen.\nBei mehreren Lektionslesarten gilt die L1-Lesart.\nAusnahme: Bei Selbstkorrekturen wird die *-Lesart geloescht und die\nC*-Lesart beibehalten.\n\nErweiterung vom 15.02.2013: Wenn die einzige Variante an einer Stelle\nnur von einem oder mehreren Korrektoren bezeugt ist (z.B. 26:8/17),\ngehoert die Stelle nicht in die Tabelle.\nEs muss also noch eine Pruefung stattfinden, ob nach diesem Vorgang\neine Stelle noch immer eine variierte Stelle ist. Wenn nicht, kann\nder Datensatz geloescht werden.\n\nVgl. die Datei ArbeitsablaufCBGMApg1.pdf in der Mail\nvon Klaus am 25.01.2012\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef main_6(database, chapter, verbose=False):\n    def printer(s):\n        """"""\n        Print information if needed.\n        """"""\n        if verbose:\n            print s\n    import access\n    import db_access3\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n    # Tabellennamen aus chapter generieren\n    sChapter = str(chapter)\n    if int(chapter) < 10:\n        sChapter = ""0"" + sChapter\n    table = ""Acts"" + sChapter + ""att""\n    if database.endswith(""2""):\n        table += ""_2""\n    if database.endswith(""3""):\n        table += ""_3""\n    # Zuerst die einfachen Faelle:\n    cmd = ""delete from `%s`.`%s` "" % (database, table)\n    cmd += ""where (lekt = \'L2\' or korr in (\'C\', \'C1\', \'C2\', \'C3\', \'A\', \'K\')) ""\n    cmd += ""and suffix2 <> \'%C*%\' ""\n    printer(cmd)\n    cursor.execute(cmd)\n    # Nachbesserung notwendig:\n    cmd = ""delete from `%s`.`%s` "" % (database, table)\n    cmd += ""where suffix2 like \'%L2%\' or suffix2 like \'%A%\' ""\n    cmd += ""or suffix2 like \'%K%\' ""\n    printer(cmd)\n    cursor.execute(cmd)\n    # Sonderfall Selbstkorrektur: C*\n    cmd = ""select anfadr, endadr, hsnr from `%s`.`%s` "" % (database, table)\n    cmd += ""where suffix2 like \'%%C*\' ""\n    cursor.execute(cmd)\n    rows = cursor.fetchall()\n    for row in rows:\n        anfadr = row[0]\n        endadr = row[1]\n        hsnr = row[2]\n        cmd = ""delete from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %s and endadr = %s "" % (anfadr, endadr)\n        cmd += ""and hsnr = %d "" % (hsnr)\n        cmd += ""and suffix2 like \'%%*%%\' and suffix2 not like \'%%C*\' ""\n        printer(cmd)\n        cursor.execute(cmd)\n    dba.commit()\n    # Eintraege loeschen, die nun keine Varianten mehr haben\n    passages, void = dba.getPassages(database, table)\n    for p in passages:\n        anfadr = p[0]\n        endadr = p[1]\n        cmd = ""select count(distinct labez) ""\n        cmd += ""from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %d and endadr = %d "" % (anfadr, endadr)\n        cmd += ""and labez not like \'a\' and labez not like \'z%%\'""\n        cursor.execute(cmd)\n        res = cursor.fetchone()\n        if res[0] == 0:\n            cmd = ""delete from `%s`.`%s` "" % (database, table)\n            cmd += ""where anfadr = %d and endadr = %d "" % (anfadr, endadr)\n            printer(cmd)\n            cursor.execute(cmd)\n    dba.commit()\n    # Eintraege - Ende\n    cursor.close()\n    dba.close()\n\nif __name__ == ""__main__"":\n    from optparse import OptionParser\n    parser = OptionParser()\n    parser.add_option(""-d"", ""--database"", dest=""database"",\n                      help=""Giving database name"")\n    parser.add_option(""-c"", ""--chapter"", dest=""chapter"",\n                      help=""Giving chapter number"")\n    parser.add_option(""-v"", ""--verbose"", dest=""verbose"",\n                      action=""store_true"", help=""Verbose mode"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    if opts.database is None or opts.chapter is None:\n        import sys\n        print ""At least one parameter necessary is missing!""\n        print ""See python %s -h"" % sys.argv[0]\n        sys.exit(1)\n    main_6(opts.database, opts.chapter, opts.verbose)'"
scripts/python/prepare4cbgm_6b.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n""""""\n4b. Die Eintrag \'videtur\', gekennzeichnet durch ein \'V\'\nhinter der Handschriftennummer, spielt fuer die CBGM keine Rolle.\nEin eventuell vorhandenes \'V\' muss getilgt werden.\nGleiches gilt fuer die Eintraege \'*\' und \'C*\'.\n\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef killVStarC(s):\n    """"""\n    killVStarC() entfernt ein \'V\', \'*\', \'T1\' und \'C*\' aus einer Zeichenkette.\n\n    The method deletes a \'V\', \'C\', \'T\' or a star from a string.\n    """"""\n    pattern = (""V"", ""C"", ""*"")\n    for char in pattern:\n        pos = s.find(char)\n        if pos > -1:\n            s = s[:pos] + s[pos+1:]\n    pos = s.find(""T1"")\n    if pos > -1:\n        s = s[:pos]\n    return s\n\n\ndef main_6b(database, chapter, verbose=False):\n    def printer(s):\n        """"""\n        Print information if needed.\n        """"""\n        if verbose:\n            print s\n    import access\n    import db_access3\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n    # Tabellennamen aus chapter generieren\n    sChapter = str(chapter)\n    if int(chapter) < 10:\n        sChapter = ""0"" + sChapter\n    table = ""Acts"" + sChapter + ""att""\n    if database.endswith(""2""):\n        table += ""_2""\n    if database.endswith(""3""):\n        table += ""_3""\n    # Alle in Frage kommenden Stellen auflisten\n    cmd = ""select anfadr, endadr, HS from `%s`.`%s` "" % (database, table)\n    cmd += ""where HS like \'%%V%%\' or HS like \'%%*%%\' or HS like \'%%C%%\' ""\n    cursor.execute(cmd)\n    rows = cursor.fetchall()\n    for r in rows:\n        anf = r[0]\n        end = r[1]\n        wit = r[2]\n        # \'V\', \'C\' oder \'*\' aus der Handschriftenbezeichnung entfernen\n        wit_new = killVStarC(wit)\n        # Update der Datenbanktabelle\n        cmd = ""update `%s`.`%s` "" % (database, table)\n        cmd += ""set HS = \'%s\' "" % (wit_new)\n        cmd += ""where anfadr = %d and endadr = %d "" % (anf, end)\n        cmd += ""and HS = \'%s\' "" % (wit)\n        printer(cmd)  # Kontrollausgabe\n        cursor.execute(cmd)\n        dba.commit()\n    cursor.close()\n    dba.close()\n\nif __name__ == ""__main__"":\n    from optparse import OptionParser\n    parser = OptionParser()\n    parser.add_option(""-d"", ""--database"", dest=""database"",\n                      help=""Giving database name"")\n    parser.add_option(""-c"", ""--chapter"", dest=""chapter"",\n                      help=""Giving chapter number"")\n    parser.add_option(""-v"", ""--verbose"", dest=""verbose"",\n                      action=""store_true"", help=""Verbose mode"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    if opts.database is None or opts.chapter is None:\n        import sys\n        print ""At least one parameter necessary is missing!""\n        print ""See python %s -h"" % sys.argv[0]\n        sys.exit(1)\n    main_6b(opts.database, opts.chapter, opts.verbose)\n'"
scripts/python/prepare4cbgm_7.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n""""""\n7. zw-Lesarten der uebergeordneten Variante zuordnen, wenn ausschliesslich\nverschiedene Lesarten derselben Variante infrage kommen (z.B. zw a/ao oder\nb/bo_f). In diesen Faellen tritt die Buchstabenkennung der uebergeordneten\nVariante in LABEZ an die Stelle von \'zw\'.\n\nDa die Stringmethode strip() nicht wie gewuenscht funktioniert, schreibe ich\nmir eine eigene: mystrip.\n\nVgl. die Datei ArbeitsablaufCBGMApg1.pdf in der Mail\nvon Klaus am 25.01.2012\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef mystrip(source, pattern):\n    """"""\n    Entfernen der in pattern genannten Zeichen aus der Zeichenkette \'source\'.\n    """"""\n    result = source[0]\n    for c in source[1:]:\n        if c not in pattern:\n            result += c\n    return result\n\n\ndef matches(pattern, input):\n    """"""\n    Stimmt jedes einzelne Zeichen in \'input\' mit dem\n    Zeichen \'pattern\' ueberein?\n    """"""\n    result = True\n    for c in input:\n        if c != pattern:\n            return False\n    return result\n\n\ndef main_7(database, chapter, verbose=False):\n    def printer(s):\n        """"""\n        Print information if needed.\n        """"""\n        if verbose:\n            print s\n    import access\n    import db_access3\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n    # Tabellennamen aus chapter generieren\n    sChapter = str(chapter)\n    if int(chapter) < 10:\n        sChapter = ""0"" + sChapter\n    table = ""Acts"" + sChapter + ""att""\n    if database.endswith(""2""):\n        table += ""_2""\n    if database.endswith(""3""):\n        table += ""_3""\n    # Zu loeschende Zeichen definieren\n    pattern = ""fo_1234567890/""\n    # Alle \'zw\'-Lesarten auflisten\n    cmd = ""select labezsuf, anfadr, endadr from `%s`.`%s` "" % (database, table)\n    cmd += ""where labez like \'zw\' ""\n    # F\xc3\xa4lle wie ""zw e/f"" m\xc3\xbcssen aber stehen bleiben, z.B. Acta 4,2/20-34\n    # alternativ k\xc3\xb6nnte man die beiden folgenden Zeilen streichen,\n    # und in der Schleife ein continue setzen, wenn ein RegEx zutrifft.\n    cmd += ""and labezsuf not like \'f%%\' ""\n    cmd += ""and labezsuf not like \'%%/f%%\' ""\n    cursor.execute(cmd)\n    rows = cursor.fetchall()\n    for row in rows:\n        labezsuf = row[0]\n        anfadr = row[1]\n        endadr = row[2]\n        s = mystrip(labezsuf, pattern)\n        # Labez einheitlich immer gleich?\n        b = matches(s[0], s)\n        # Wenn wahr, dann diesen Datensatz updaten\n        if b:\n            cmd = ""update `%s`.`%s` "" % (database, table)\n            cmd += ""set labez = \'%s\' "" % (s[0])\n            cmd += ""where labez like \'zw\' ""\n            cmd += ""and labezsuf like \'%s\' "" % (labezsuf)\n            cmd += ""and anfadr = %s and endadr = %s "" % (anfadr, endadr)\n            printer(cmd)\n            cursor.execute(cmd)\n        dba.commit()\n    cursor.close()\n    dba.close()\n\nif __name__ == \'__main__\':\n    from optparse import OptionParser\n    parser = OptionParser()\n    parser.add_option(""-d"", ""--database"", dest=""database"",\n                      help=""Giving database name"")\n    parser.add_option(""-c"", ""--chapter"", dest=""chapter"",\n                      help=""Giving chapter number"")\n    parser.add_option(""-v"", ""--verbose"", dest=""verbose"",\n                      action=""store_true"", help=""Verbose mode"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    if opts.database is None or opts.chapter is None:\n        import sys\n        print ""At least one parameter is missing!""\n        print ""See python %s -h"" % sys.argv[0]\n        sys.exit(1)\n    main_7(opts.database, opts.chapter, opts.verbose)'"
scripts/python/prepare4cbgm_9.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n""""""\n9. Stellenbezogene Lueckenliste fuellen\n    9.0. Vorhandene Tabelle leeren\n    9.1. Alle Passages auflisten\n    9.2. Alle Handschriften der Systematischen Lueckenliste auflisten\n    9.3. Schleife ueber alle Handschriften\n    9.4. Schleife ueber alle Passages\n    9.5. Eintrag in ActsNNattLac, wenn die Handschrift an\n         genannter Stelle in der Systematischen Lueckenliste verzeichnet ist\n\nVgl. die Datei ArbeitsablaufCBGMApg1.pdf in der Mail\nvon Klaus am 25.01.2012\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef enter2LocalLacList(cursor, hs, db, lactable, anfadr, endadr):\n    """"""\n    Insert new dataset into resulting table.\n    """"""\n    import Address\n    b, bc, bv, bw, ec, ev, ew = Address.decodeAdr(anfadr, endadr)\n    hsnr = Address.hs2hsnr(hs)\n    cmd = ""insert into %s.%s "" % (db, lactable)\n    cmd += ""(buch, kapanf, versanf, wortanf, kapend, versend, wortend, ""\n    cmd += ""anfadr, endadr, hs, hsnr, anfalt, endalt) ""\n    cmd += ""values (%d, %d, %d, %d, %d, %d, %d, "" % (b, bc, bv, bw,\n                                                     ec, ev, ew)\n    cmd += ""%d, %d, \'%s\', %d, %d, %d)"" % (anfadr, endadr, hs, hsnr,\n                                          anfadr, endadr)\n    #print cmd\n    cursor.execute(cmd)\n\n\ndef getSysLacTable(cursor, db, sChapter):\n    """"""\n    Looking for the name of the systematic lacuna table.\n    """"""\n    pattern = ""Acts"" + sChapter + ""%lac""\n    cmd = ""use `%s`"" % (db)\n    cursor.execute(cmd)\n    cmd = ""show tables like \'%s\'"" % (pattern)\n    cursor.execute(cmd)\n    tables = cursor.fetchall()\n    #print tables\n    return tables[0][0]\n\n\ndef main_9(db1, db2, chapter, verbose=False):\n    def printer(s):\n        """"""\n        Print information if needed.\n        """"""\n        if verbose:\n            print s\n    import access\n    import db_access3\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n    # Generate name of newtable and lactable\n    sChapter = str(chapter)\n    if int(chapter) < 10:\n        sChapter = ""0"" + sChapter\n    newtable = ""Acts"" + sChapter + ""att""\n    lactable = newtable + ""Lac""\n    if db1.endswith(""2""):\n        newtable += ""_2""\n        lactable += ""_2""\n    if db1.endswith(""3""):\n        newtable += ""_3""\n        lactable += ""_3""\n    # Query systematic lacuna table name\n    sys_lac_table = getSysLacTable(cursor, db2, sChapter)\n    # 9.0. Truncate lacuna table\n    cmd = ""truncate %s.%s "" % (db1, lactable)\n    printer(cmd)\n    cursor.execute(cmd)\n    dba.commit()\n    # 9.1.\n    passages, passcount = dba.getPassages(db1, newtable)\n    # 9.2.\n    cmd = ""select distinct hs from %s.%s "" % (db2, sys_lac_table)\n    cmd += ""order by hsnr ""\n    printer(cmd)\n    cursor.execute(cmd)\n    mss = cursor.fetchall()\n    # get max endadr\n    cmd = ""select max(endadr) from `%s`.`%s` "" % (db2, sys_lac_table)\n    cursor.execute(cmd)\n    result = cursor.fetchone()\n    max_endadr = result[0]\n    # 9.3.\n    for ms in mss:\n        hs = ms[0]\n        # 9.4.\n        for passage in passages:\n            anfadr = passage[0]\n            endadr = passage[1]\n            cmd = ""select count(id) from %s.%s "" % (db2, sys_lac_table)\n            if endadr < max_endadr:\n                cmd += ""where anfadr <= %d and endadr >= %d "" % (anfadr,\n                                                                 endadr)\n            else:\n                cmd += ""where anfadr <= %d and endadr >= %d "" % (anfadr,\n                                                                 endadr-1)\n            cmd += ""and hs = \'%s\' "" % (hs)\n            printer(cmd)\n            cursor.execute(cmd)\n            result = cursor.fetchone()\n            rescount = result[0]\n            # 9.5.\n            if rescount > 0:\n                enter2LocalLacList(cursor, hs, db1, lactable, anfadr, endadr)\n    dba.commit()  # it\'s an InnoDB table\n    cursor.close()\n    dba.close()\n\nif __name__ == ""__main__"":\n    from optparse import OptionParser\n    parser = OptionParser()\n    parser.add_option(""-d"", ""--database"", dest=""database"",\n                      help=""Giving database"")\n    parser.add_option(""-e"", ""--ref_db"", dest=""ref_db"",\n                      help=""Database of lacuna table"")\n    parser.add_option(""-c"", ""--chapter"", dest=""chapter"",\n                      help=""Chapter number"")\n    parser.add_option(""-v"", ""--verbose"", dest=""verbose"",\n                      action=""store_true"", help=""Verbose mode"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    if opts.database is None or opts.ref_db is None or opts.chapter is None:\n        import sys\n        print ""Error: At least one parameter necessary is missing!""\n        print ""See python %s -h"" % sys.argv[0]\n        sys.exit(1)\n    main_9(opts.database, opts.ref_db, opts.chapter, opts.verbose)'"
scripts/python/sucher.py,0,"b'#!/usr/bin/python\n#-*- encoding: utf-8 -*-\n\n__author__ = ""volker.krueger@uni-muenster.de""\n\nimport sys\n\ndef main(table):\n    import db_access3\n    DB = ""ECM_Acts_CBGMPh2""\n    TABLE = table\n    dba = db_access3.DBA(""remote"")\n    cursor = dba.cursor()\n    cmd = ""select count(distinct anfadr, endadr) from %s.%s "" % (DB, TABLE)\n    cursor.execute(cmd)\n    res = cursor.fetchone()\n    PASSCOUNT = res[0]\n    print ""%s hat %d variierte Stellen."" % (TABLE, PASSCOUNT)\n    cmd = ""select count(distinct hsnr) from %s.%s "" % (DB, TABLE)\n    cursor.execute(cmd)\n    res = cursor.fetchone()\n    HSNRCOUNT = res[0]\n    print ""%s hat %d Handschriften."" % (TABLE, HSNRCOUNT)\n    cmd = ""select count(*) from %s.%s"" % (DB, TABLE)\n    cursor.execute(cmd)\n    res = cursor.fetchone()\n    SUM = res[0]\n    if SUM == PASSCOUNT * HSNRCOUNT:\n        print ""magisches Produkt stimmt""\n    else:\n        cmd = ""select distinct hsnr from %s.%s "" % (DB, TABLE)\n        cursor.execute(cmd)\n        hsnrs = cursor.fetchall()\n        for hsnr in hsnrs:\n            cmd = ""select count(hsnr) from %s.%s where hsnr = %d "" % (DB, TABLE, hsnr[0])\n            cursor.execute(cmd)\n            c = cursor.fetchone()\n            if int(c[0]) != PASSCOUNT:\n                print hsnr\n                if int(c[0]) > PASSCOUNT:\n                    # Jede Handschrift darf es pro variierter Stelle nur einmal geben\n                    passages, void = dba.getPassages(DB, TABLE)\n                    for passage in passages:\n                        cmd  = ""select id from %s.%s "" % (DB, TABLE)\n                        cmd += ""where anfadr = %d and endadr = %d and hsnr = %d "" % (passage[0], passage[1], hsnr[0])\n                        cursor.execute(cmd)\n                        res = cursor.fetchall()\n                        if len(res) > 1:\n                            print ""\\tBei %d/%d ist der Zeuge mehrfach verzeichnet. Vgl. ID:"" % (passage[0], passage[1])\n                            print ""\\t"", res\n                else:\n                    print ""\\t%s"" % (c[0])\n    cursor.close()\n    dba.close()\n\nif __name__ == \'__main__\':\n    if len(sys.argv) < 2:\n        print ""Please enter table name as single parameter""\n        sys.exit(1)\n    main(sys.argv[1])'"
scripts/python/sucher2.py,0,"b'#!/usr/bin/python\n#-*- encoding: utf-8 -*-\n\n__author__ = ""volker.krueger@uni-muenster.de""\n\nimport sys\n\ndef main(table):\n    import db_access3\n    DB = ""ECM_Acts_CBGM""\n    TABLE = table\n    dba = db_access3.DBA(""remote"")\n    cursor = dba.cursor()\n    cmd = ""select count(distinct anfadr, endadr) from %s.%s "" % (DB, TABLE)\n    cursor.execute(cmd)\n    res = cursor.fetchone()\n    PASSCOUNT = res[0]\n    print ""%s hat %d variierte Stellen."" % (TABLE, PASSCOUNT)\n    cmd = ""select count(distinct hsnr) from %s.%s "" % (DB, TABLE)\n    cursor.execute(cmd)\n    res = cursor.fetchone()\n    HSNRCOUNT = res[0]\n    print ""%s hat %d Handschriften."" % (TABLE, HSNRCOUNT)\n    cmd = ""select distinct hsnr from %s.%s "" % (DB, TABLE)\n    cursor.execute(cmd)\n    hsnrs = cursor.fetchall()\n    cmd = ""select distinct anfadr, endadr from %s.%s "" % (DB, TABLE)\n    cursor.execute(cmd)\n    passages = cursor.fetchall()\n    for hsnr in hsnrs:\n        for passage in passages:\n            cmd  = ""select count(id) from %s.%s "" % (DB, TABLE)\n            cmd += ""where hsnr = %d "" % (hsnr[0])\n            cmd += ""and anfadr = %d and endadr = %d "" % (passage[0], passage[1])\n            cursor.execute(cmd)\n            res = cursor.fetchone()\n            if res[0] != 1:\n                print ""Fehler bei Handschrift %d an Stelle %d/%d. "" % (hsnr[0], passage[0], passage[1])\n    cursor.close()\n    dba.close()\n\nif __name__ == \'__main__\':\n    if len(sys.argv) < 2:\n        print ""Please enter table name as single parameter""\n        sys.exit(1)\n    main(sys.argv[1])'"
scripts/python/sucherPh3.py,0,"b'#!/usr/bin/python\n#-*- encoding: utf-8 -*-\n\n__author__ = ""volker.krueger@uni-muenster.de""\n\nimport sys\n\ndef main(table):\n    import access\n    import db_access3\n    DB = ""ECM_Acts_CBGMPh3""\n    TABLE = table\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n    cmd = ""select count(distinct anfadr, endadr) from %s.%s "" % (DB, TABLE)\n    cursor.execute(cmd)\n    res = cursor.fetchone()\n    PASSCOUNT = res[0]\n    print ""%s hat %d variierte Stellen."" % (TABLE, PASSCOUNT)\n    cmd = ""select count(distinct hsnr) from %s.%s "" % (DB, TABLE)\n    cursor.execute(cmd)\n    res = cursor.fetchone()\n    HSNRCOUNT = res[0]\n    print ""%s hat %d Handschriften."" % (TABLE, HSNRCOUNT)\n    cmd = ""select count(*) from %s.%s"" % (DB, TABLE)\n    cursor.execute(cmd)\n    res = cursor.fetchone()\n    SUM = res[0]\n    if SUM == PASSCOUNT * HSNRCOUNT:\n        print ""magisches Produkt stimmt""\n    else:\n        cmd = ""select distinct hsnr from %s.%s "" % (DB, TABLE)\n        cursor.execute(cmd)\n        hsnrs = cursor.fetchall()\n        for hsnr in hsnrs:\n            cmd = ""select count(hsnr) from %s.%s where hsnr = %d "" % (DB, TABLE, hsnr[0])\n            cursor.execute(cmd)\n            c = cursor.fetchone()\n            if int(c[0]) != PASSCOUNT:\n                print hsnr\n                if int(c[0]) > PASSCOUNT:\n                    # Jede Handschrift darf es pro variierter Stelle nur einmal geben\n                    passages, void = dba.getPassages(DB, TABLE)\n                    for passage in passages:\n                        cmd  = ""select id from %s.%s "" % (DB, TABLE)\n                        cmd += ""where anfadr = %d and endadr = %d and hsnr = %d "" % (passage[0], passage[1], hsnr[0])\n                        cursor.execute(cmd)\n                        res = cursor.fetchall()\n                        if len(res) > 1:\n                            print ""\\tBei %d/%d ist der Zeuge mehrfach verzeichnet. Vgl. ID:"" % (passage[0], passage[1])\n                            print ""\\t"", res\n                else:\n                    print ""\\t%s"" % (c[0])\n    cursor.close()\n    dba.close()\n\nif __name__ == \'__main__\':\n    if len(sys.argv) < 2:\n        print ""Please enter table name as single parameter""\n        sys.exit(1)\n    main(sys.argv[1])\n'"
scripts/perl/VGA/Check4SplittedSources.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\n""""""\nAre there passages in the stemma editor where splitted readings\nare the source of other readings, being noted incorrectly?\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef main(database, table, mode=""remote""):\n    import db_access3\n    dba = db_access3.DBA(mode)\n    cursor = dba.cursor()\n    # get the passages\n    cmd = ""select distinct begadr, endadr from `%s`.`%s` "" % (database, table)\n    cursor.execute(cmd)\n    rows = cursor.fetchall()\n    for r in rows:\n        anfadr = r[0]\n        endadr = r[1]\n        # get the readings and their sources\n        cmd = ""select varid, varnew from `%s`.`%s` "" % (database, table)\n        cmd += ""where begadr = %d and endadr = %d "" % (anfadr, endadr)\n        cmd += ""and (varnew like \'%%1\' or ""\n        cmd += ""varnew like \'%%2\' or ""\n        cmd += ""varnew like \'%%3\' or ""\n        cmd += ""varnew like \'%%4\' or ""\n        cmd += ""varnew like \'%%5\' or ""\n        cmd += ""varnew like \'%%6\' or ""\n        cmd += ""varnew like \'%%7\' or ""\n        cmd += ""varnew like \'%%8\' or ""\n        cmd += ""varnew like \'%%9\') ""\n        cursor.execute(cmd)\n        result = cursor.fetchall()\n        for n in result:\n            varid = n[0]\n            varnew = n[1]\n            # are there any readings having varid instead of varnew as S1 or S2?\n            cmd = ""select distinct varid, varnew, s1, s2 from `%s`.`%s` "" % (database, table)\n            cmd += ""where begadr = %d and endadr = %d "" % (anfadr, endadr)\n            cmd += ""and (s1 = \'%s\' or s2 = \'%s\') "" % (varid, varid)\n            cmd += ""and varid not like \'z%%\' ""\n            cursor.execute(cmd)\n            mistakes = cursor.fetchall()\n            for m in mistakes:\n                print ""%d/%d:%s has the wrong source(s): %s"" % (anfadr, endadr, varnew, m[2])\n    cursor.close()\n    dba.close()\n\n\nif __name__ == \'__main__\':\n    from optparse import OptionParser\n    parser = OptionParser()\n    parser.add_option(""-d"", ""--database"", dest=""database"")\n    parser.add_option(""-t"", ""--table"", dest=""table"")\n    (opts, args) = parser.parse_args()\n    parser.destroy()\n    if opts.database is None or opts.table is None:\n        import sys\n        print ""At least one parameter is missing.""\n        sys.exit(1)\n    main(opts.database, opts.table)'"
scripts/perl/VGA/access.py,0,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n""""""\nOne fact - one place.\nEdit here which database server to access.\n\'local\' and \'remote\' are configured server in db_access3.py\n""""""\n__author__ = ""volker.krueger@uni-muenster.de""\n\n\ndef get():\n    return ""remote""\n#    return ""local""\n'"
scripts/perl/VGA/db_access3.py,0,"b'#! /usr/bin/python\n#-*- encoding: utf-8 -*-\n\'\'\'\nModule imported by printer3.py and printer4.py etc.\n\'\'\'\n__author__ = ""volker.krueger@uni-muenster.de""\n\nimport MySQLdb\n\n#Configure database access here\nlocal_dict = {\n    ""host""   : ""localhost"",\n    ""user""   : ""root"",\n    ""passwd"" : ""xxx"",\n    ""db""     : ""apparat"",\n    ""charset"": ""utf8"" }\n\nremote_dict = {\n    ""host""   : ""your_hostname"",\n    ""user""   : ""your_username"",\n    ""passwd"" : ""your_password"",\n    ""db""     : ""Apparat"",\n    ""charset"": ""utf8"" }\n\n\nclass DBA(object):\n    def __init__(self, s):\n        self.__d = {}\n        if s == ""local"":\n            self.__d = local_dict\n        elif s == ""remote"":\n            self.__d = remote_dict\n        else:\n            self.__d = None\n        if self.__d != None:\n            self.connection = MySQLdb.connect(host=self.__d.get(""host""), \\\n                                              user=self.__d.get(""user""), \\\n                                              passwd=self.__d.get(""passwd""), \\\n                                              db=self.__d.get(""db""), \\\n                                              charset=self.__d.get(""charset""))\n            self.owncursor = self.connection.cursor()\n    def __del__(self):\n        #self.close()\n        pass\n    def getHost(self):\n        return self.__d.get(""host"")\n    def getUser(self):\n        return self.__d.get(""user"")\n    def getPasswd(self):\n        return self.__d.get(""passwd"")\n    def getDb(self):\n        return self.__d.get(""db"")\n    def getCharset(self):\n        return self.__d.get(""charset"")\n    def autocommit(self, value):\n        self.connection.autocommit(value)\n    def commit(self):\n        self.connection.commit()\n    def rollback(self):\n        self.connection.rollback()\n    def cursor(self):\n        return self.connection.cursor()\n    def close(self):\n        self.connection.close()\n    def getMssAvailable(self, book):\n        \'\'\'\n        Giving all manuscripts collated for the book identified by the parameter.\n        \'\'\'\n        cmd  = ""select hsnr from `Apparat`.`mss_available` ""\n        cmd += ""where book = %d "" % (book)\n        cmd += ""and hsnr < 500000 "" # Fehlverse ausschliessen\n        cmd += ""order by hsnr ""\n        self.owncursor.execute(cmd)\n        return self.owncursor.fetchall()\n    def getPassages(self, database, table, startverse = 0, endverse = 0, firstword = 0, lastword = 0):\n        """"""\n        Getting all passages of a witness table. The program should work\n        independently from a LesartenOnly table.\n        """"""\n        cmd  = ""select distinct anfadr, endadr, buch, kapanf, versanf, wortanf, ""\n        cmd += ""kapend, versend, wortend from `%s`.`%s` where 1 "" % (database, table)\n        cond = """"\n        if int(startverse) > 0 and int(endverse) > 0:\n            cond  = ""and versanf >= %s and versend <= %s "" % (startverse, endverse)\n            if int(firstword) > 0 and int(lastword) > 0:\n                cond  = ""and ((versanf = %s and wortanf >= %s) or versanf > %s) "" % (startverse, firstword, startverse)\n                cond += ""and ((versend = %s and wortend <= %s) or versend < %s) "" % (endverse, lastword, endverse)\n        cmd += cond + ""order by anfadr asc, endadr desc ""\n        count = self.owncursor.execute(cmd)\n        return self.owncursor.fetchall(), count\n    def getReadings(self, database, table, anfadr, endadr):\n        """"""\n        Getting all the readings which belong to an address described by the\n        result set of \'row_passages\'.\n        """"""\n        cmd  = ""select distinct anfadr, endadr, labez, labezsuf, ""\n        cmd += ""buch, kapanf, versanf, wortanf, kapend, versend, wortend, lesart ""\n        cmd += ""from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %d and endadr = %d "" % (anfadr, endadr)\n        cmd += ""order by labez, labezsuf ""\n        #print cmd\n        count = self.owncursor.execute(cmd)\n        return self.owncursor.fetchall(), count\n    def getReadings2(self, database, table, anfadr, endadr):\n        """"""\n        Getting all readings which belong to one passage. Fehlerlesarten\n        are included - orthographica too.\n        """"""\n        cmd  = ""select distinct anfadr, endadr, labez, \'\', ""\n        cmd += ""buch, kapanf, versanf, wortanf, kapend, versend, wortend, lesart ""\n        cmd += ""from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %d and endadr = %d "" % (anfadr, endadr)\n        cmd += ""order by labez, labezsuf ""\n        count = self.owncursor.execute(cmd)\n        return self.owncursor.fetchall(), count\n    def getWitnesses(self, database, table, anfadr, endadr, labez, labezsuf):\n        """"""\n        Getting all witnesses of a reading described by the result set of\n        \'row_readings\'.\n        """"""\n        cmd  = ""select hs, suffix2, hsnr, lemma, lesart, fehler from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %s and endadr = %s "" % (anfadr, endadr)\n        cmd += ""and labez = \'%s\' and labezsuf = \'%s\' "" % (labez, labezsuf)\n        cmd += ""order by hsnr, labezsuf ""\n        #  print cmd\n        count = self.owncursor.execute(cmd)\n        return self.owncursor.fetchall(), count\n    def getVersions(self, database, table, anfadr, endadr, labez, labezsuf):\n        cmd  = ""select hss, suffix2, hsnr, vers_lesart, fehler from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %s and endadr = %s "" % (anfadr, endadr)\n        cmd += ""and labez = \'%s\' and labezsuf = \'%s\' "" % (labez.encode(\'utf-8\'), labezsuf.encode(\'utf-8\'))\n        cmd += ""order by hsnr, labezsuf ""\n        #  print cmd\n        count = self.owncursor.execute(cmd)\n        return self.owncursor.fetchall(), count\n    def getAllWitnesses(self, database, table, anfadr, endadr):\n        """"""\n        Returns all witnesses of a passage.\n        """"""\n        cmd  = ""select hs, hsnr from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %d and endadr = %d "" % (anfadr, endadr)\n        cmd += ""order by hsnr ""\n        count = self.owncursor.execute(cmd)\n        return self.owncursor.fetchall(), count\n    def countWitnessesDifferentFromA(self, database, table, anfadr, endadr):\n        \'\'\'\n        How many greek witnesses differ from the Ausgangstext?\n        Orthographica are not included.\n        \'\'\'\n        cmd  = ""select count(hsnr) from `%s`.`%s` "" % (database, table)\n        cmd += ""where anfadr = %d and endadr = %d "" % (anfadr, endadr)\n        cmd += ""and (labez <> \'a\' ""\n        cmd += ""or labez = \'a\' and (labezsuf <> \'\' or labezsuf is not null)) ""\n        cmd += ""and hsnr < 500000 ""\n        void = self.owncursor.execute(cmd)\n        res  = self.owncursor.fetchone()\n        return res[0]\n    def getLacunaeAndDubia(self, database, table, anfadr, endadr, lacunae = None):\n        \'\'\'\n        Returns witnesses having a lacuna at the passage or having labez \'zz\'.\n        \'\'\'\n        if lacunae == None:\n            lacunae = table + ""lac""\n        cmd  = ""(select hs, hsnr from %s.%s "" % (database, lacunae) # here: table of lacunae\n        cmd += ""where anfadr <= %d and endadr >= %d "" % (int(endadr), int(anfadr))\n        cmd += ""order by hsnr, suffix2) union ""\n        cmd += ""(select hs, hsnr from %s.%s "" % (database, table) # here: table of witnesses\n        cmd += ""where anfadr = %d and endadr = %d "" % (anfadr, endadr)\n        cmd += ""and labez = \'zz\' ""\n        cmd += ""order by hsnr) ""\n        count = self.owncursor.execute(cmd)\n        # sort result sets into an ascending order\n        res  = [] # tupel to return\n        rel  = {} # dictionary hsnr: hs\n        hsnr = [] # list containing hsnr\n        rows = self.owncursor.fetchall()\n        for row in rows:\n            hs = row[0]\n            nr = row[1]\n            if nr not in hsnr:\n                hsnr.append(nr)\n                rel[nr] = (hs, nr, )\n        hsnr.sort()\n        for nr in hsnr:\n            res.append(rel[nr])\n        return res, count\n\n\ndef formatAdr(b, bc, bv, bw, ec, ev, ew):\n    """"""\n    Write the seven parameters in a nicely formatted way.\n    """"""\n    s = """"\n    if bc == ec and bv == ev and bw == ew:\n        s = ""%s%s:%s/%s"" % (b, bc, bv, bw)\n    else:\n        if bc == ec and bv == ev:\n            s = ""%s%s:%s/%s-%s"" % (b, bc, bv, bw, ew)\n        else:\n            if bc == ec:\n                s = ""%s%s:%s/%s-%s/%s"" % (b, bc, bv, bw, ev, ew)\n            else:\n                s = ""%s%s:%s/%s-%s:%s/%s"" % (b, bc, bv, bw, ec, ev, ew)\n    return s\n'"
scripts/perl/VGA/sucherPh3.py,0,"b'#!/usr/bin/python\n#-*- encoding: utf-8 -*-\n\n__author__ = ""volker.krueger@uni-muenster.de""\n\nimport sys\n\ndef main(table):\n    import access\n    import db_access3\n    DB = ""VarGenAtt_ActPh3""\n    TABLE = table\n    dba = db_access3.DBA(access.get())\n    cursor = dba.cursor()\n    cmd = ""select count(distinct begadr, endadr) from %s.%s "" % (DB, TABLE)\n    cursor.execute(cmd)\n    res = cursor.fetchone()\n    PASSCOUNT = res[0]\n    print ""%s hat %d variierte Stellen."" % (TABLE, PASSCOUNT)\n    cmd = ""select count(distinct ms) from %s.%s "" % (DB, TABLE)\n    cursor.execute(cmd)\n    res = cursor.fetchone()\n    HSNRCOUNT = res[0]\n    print ""%s hat %d Handschriften."" % (TABLE, HSNRCOUNT)\n    cmd = ""select count(*) from %s.%s"" % (DB, TABLE)\n    cursor.execute(cmd)\n    res = cursor.fetchone()\n    SUM = res[0]\n    if SUM == PASSCOUNT * HSNRCOUNT:\n        print ""magisches Produkt stimmt""\n    else:\n        cmd = ""select distinct ms from %s.%s "" % (DB, TABLE)\n        cursor.execute(cmd)\n        hsnrs = cursor.fetchall()\n        for hsnr in hsnrs:\n            cmd = ""select count(ms) from %s.%s where ms = %d "" % (DB, TABLE, hsnr[0])\n            cursor.execute(cmd)\n            c = cursor.fetchone()\n            count = int(c[0])\n            if count != PASSCOUNT:\n                print hsnr\n                if count != PASSCOUNT:\n                    # Jede Handschrift darf es pro variierter Stelle nur einmal geben\n                    cmd = ""select distinct begadr, endadr from %s.%s "" % (DB, TABLE)\n                    cursor.execute(cmd)\n                    passages = cursor.fetchall()\n                    for passage in passages:\n                        cmd  = ""select varid, witn from %s.%s "" % (DB, TABLE)\n                        cmd += ""where begadr = %d and endadr = %d and ms = %d "" % (passage[0], passage[1], hsnr[0])\n                        cursor.execute(cmd)\n                        res = cursor.fetchall()\n                        if len(res) > 1:\n                            print ""\\tBei %d/%d ist der Zeuge mehrfach verzeichnet. Vgl. ID:"" % (passage[0], passage[1])\n                            print ""\\t"", res\n                        else:\n                            print ""\\t%s"" % (count)\n                            break\n    cursor.close()\n    dba.close()\n\nif __name__ == \'__main__\':\n    if len(sys.argv) < 2:\n        print ""Please enter table name as single parameter""\n        sys.exit(1)\n    main(sys.argv[1])\n'"
