file_path,api_count,code
assign_1/Assign_1_a.py,10,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\n# Number of training samples\nN = 10\n# Generate equispaced floats in the interval [0, 2pi]\nx = np.linspace(0, 2*np.pi, N)\n# Generate noise\nmean = 0\nstd = 0.05\n# Generate some numbers from the sine function\ny = np.sin(x)\n# Add noise\ny += np.random.normal(mean, std, N)\nx = np.transpose(x)\ntemp = np.full(np.shape(x),1)\n# W = np.zeros((N,N))\nX = np.column_stack((temp,x))\nW = np.matmul(np.linalg.pinv(X),y)\nprint(W)\ny1 = np.matmul(X,W)\nplt.plot(x,y,\'*\',label=\'True values\')\nplt.plot(x,y1,\'r\',label=\'estimated\')\nplt.legend()\nplt.show()\n\n# Error as variance of error in prediction\nerror = (y1-y)\nprint(""Variance of error ="",np.var(error))\n'"
assign_1/Assign_1_b.py,11,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\ndef polybasisfunc(x,degree,number_of_samples):\n    phi = np.zeros((number_of_samples,degree+1))\n    deg = list(range(0,degree+1))\n    k = list(range(0,number_of_samples))\n    for i in k:\n        for j in deg:\n            # print(x[i])\n            phi[i][j] = x[i]**j\n    return phi\n\n# Number of training samples\nN = 10\n# Generate equispaced floats in the interval [0, 2pi]\nx = np.linspace(0, 2*np.pi, N)\n# Generate noise\nmean = 0\nstd = 0.05\n# Generate some numbers from the sine function\ny = np.sin(x)\n# Add noise\ny += np.random.normal(mean, std, N)\nprint(""Enter degree of polynomial:"")\ndegree = input()\ndegree = int(degree)\nprint(""Enter testing samples:"")\ntest_samples = input()\ntest_samples = int(test_samples)\nphi = polybasisfunc(x,degree,N)\n# print(phi)\nW = np.zeros((N,N))\nW = np.matmul(np.linalg.pinv(phi),y)\n# print(W)\nx1 = np.linspace(0, 2*np.pi, test_samples)\nphi1 = polybasisfunc(x1,degree,test_samples)\ny1 = np.matmul(phi1,W)\n# print(y1)\n# X = np.reshape(x,(10,1))\n# y1 = np.matmul(X,W)\n# y1 = np.polynomial.polynomial(W)\n# print(y1)\n# error = (y1-y)\n# print(error)\nplt.plot(x,y,\'*\',label=\'True Values\')\nplt.plot(x1,y1,\'.\',label=\'Estimated\')\nplt.legend()\nplt.show()\n'"
assign_1/Assign_1_c.py,16,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\n# Polynomial basis function and its matrix generator\n# This function has a hardcoded bias included. So for removal of it in further steps,\n# np.delete() is used\n\ndef polybasisfunc(x,degree,number_of_samples):\n    phi = np.zeros((number_of_samples,degree+1))\n    deg = list(range(0,degree+1))\n    k = list(range(0,number_of_samples))\n    for i in k:\n        for j in deg:\n            # print(x[i])\n            phi[i][j] = x[i]**j\n    return phi\n\n\n# Number of training samples\nN = 10\n# Generate equispaced floats in the interval [0, 2pi]\nx = np.linspace(0, 2*np.pi, N)\n# Generate noise\nmean = 0\nstd = 0.05\n# Generate some numbers from the sine function\ny = np.sin(x)\n# Add noise\ny += np.random.normal(mean, std, N)\n\n\n# Ask user for parameters\nprint(""Enter degree of polynomial:"")\ndegree = input()\ndegree = int(degree)\nprint(""Enter testing samples:"")\ntest_samples = input()\ntest_samples = int(test_samples)\nprint(""Enter Lagrangian Multiplier:"")\nL = input()\nL = float(L)\n\n\n\n# Ridge regression\n## Not zero centred data\nphi = polybasisfunc(x,degree,N)\nW = np.matmul(np.matmul(np.linalg.inv(np.matmul(phi.T,phi)+(L*np.identity(degree+1))),phi.T),y)\nx1 = np.linspace(0, 2*np.pi, test_samples)\nphi1 = polybasisfunc(x1,degree,test_samples)\ny1 = np.matmul(phi1,W)\n# Plotting\nplt.title(""Regression on raw data"")\nplt.plot(x,y,\'*\',label=\'True Values\')\nplt.plot(x1,y1,\'.\',label=\'Estimated\')\nplt.legend()\nplt.show()\n\n\n## Not zero centred data and bias ommited\nzeroy = y - np.mean(y)\n# print(np.mean(y),np.mean(zeroy))\nphi = np.delete(phi,0,1)\nW = np.matmul(np.matmul(np.linalg.inv(np.matmul(phi.T,phi)+(L*np.identity(degree))),phi.T),zeroy)\n# print(np.shape(W))\nx1 = np.linspace(0, 2*np.pi, test_samples)\nphi1 = polybasisfunc(x1,degree,test_samples)\nphi1 = np.delete(phi1,0,1)\ny1 = np.matmul(phi1,W) + np.mean(y)\nplt.title(""Regression on zero-centred data"")\nplt.plot(x,y,\'*\',label=\'True Values\')\nplt.plot(x1,y1,\'.\',label=\'Estimated\')\nplt.legend()\nplt.show()\n'"
assign_1/Assign_1_d.py,16,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\n# Number of training samples\nN = 50\n# Generate equispaced floats in the interval [0, 2pi]\nx = np.linspace(0, 2*np.pi, N)\n# Generate noise\nmean = 0\nstd = 0.01\n# Generate some numbers from the sine function\ny = np.sin(x)\n# Add noise\ny += np.random.normal(mean, std, N)\nx = np.transpose(x)\ntemp = np.full(np.shape(x),1)\n# W = np.zeros((N,N))\nX = np.column_stack((temp,x))\n# Most likelihood weights\nW = np.matmul(np.linalg.pinv(X),y)\nprint(W)\ny1 = np.matmul(X,W)\n# error = (y1-y)\nplt.plot(x,y,\'*\',label=\'True Values\')\nplt.plot(x,y1,\'r\',label=\'Estimated\')\nplt.legend()\nplt.show()\n\n# Most likelihood case variance 1/{beta}\n# beta = np.var(y-y1)\nstd = np.var(y-y1)**0.5\ny2 = np.random.normal(y1,std,N)\n# labels = np.random.normal(y1,std1,N)\n# error = y1 - labels\n# # plt.plot(x,error,\'b\')\n# # plt.show()\n# variance = np.var(error)\nprint(""Most likelihood variance ="",np.var(y-y1))\nprint(""Most likelihood labels by adding gaussian noise to estimated labels="")\nprint(y2)\nprint(""Actual labels ="")\nprint(y1)\nerror = y1-y2\nprint(""error for each label :"")\nprint(error)\nprint(""Mean and variance of error ="",np.mean(error),np.var(error))\n\nplt.title(""Plot with all possible labels"")\nplt.plot(x,y,\'*\',label=\'True Values\')\nplt.plot(x,y1,\'r\',label=\'Estimated\')\nplt.plot(x,y2,\'.\',label=\'Generated labels from Y_estim\')\nplt.legend()\nplt.show()\n'"
assign_1/Assign_1_d_2.py,19,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\ndef polybasisfunc(x,degree,number_of_samples):\n    phi = np.zeros((number_of_samples,degree+1))\n    deg = list(range(0,degree+1))\n    k = list(range(0,number_of_samples))\n    for i in k:\n        for j in deg:\n            # print(x[i])\n            phi[i][j] = x[i]**j\n    return phi\n\n# Number of training samples\nN = 30\n# Generate equispaced floats in the interval [0, 2pi]\nx = np.linspace(0, 2*np.pi, N)\n# Generate noise\nmean = 0\nstd = 0.05\n# Generate some numbers from the sine function\ny = np.sin(x)\n# Add noise\ny += np.random.normal(mean, std, N)\nprint(""Enter degree of polynomial:"")\ndegree = input()\ndegree = int(degree)\nprint(""Enter testing samples:"")\ntest_samples = input()\ntest_samples = int(test_samples)\nphi = polybasisfunc(x,degree,N)\n# print(phi)\nW = np.zeros((N,N))\nW = np.matmul(np.linalg.pinv(phi),y)\n# print(W)\nx1 = np.linspace(0, 2*np.pi, test_samples)\nphi1 = polybasisfunc(x1,degree,test_samples)\ny1 = np.matmul(phi1,W)\n# print(y1)\n# X = np.reshape(x,(10,1))\n# y1 = np.matmul(X,W)\n# y1 = np.polynomial.polynomial(W)\n# print(y1)\n# error = (y1-y)\n# print(error)\nplt.plot(x,y,\'*\',label=\'True Values\')\nplt.plot(x1,y1,\'.\',label=\'Estimated\')\nplt.legend()\nplt.show()\n\nx1 = np.linspace(0, 2*np.pi, N)\nphi1 = polybasisfunc(x1,degree,N)\ny1 = np.matmul(phi1,W)\n\nstd = np.var(y-y1)**0.5\ny2 = np.random.normal(y1,std,N)\n# labels = np.random.normal(y1,std1,N)\n# error = y1 - labels\n# # plt.plot(x,error,\'b\')\n# # plt.show()\n# variance = np.var(error)\nprint(""Most likelihood variance ="",np.var(y-y1))\nprint(""Most likelihood labels by adding gaussian noise to estimated labels="")\nprint(y2)\nprint(""Actual labels ="")\nprint(y1)\nerror = y1-y2\nprint(""error for each label :"")\nprint(error)\nprint(""Mean and variance of error ="",np.mean(error),np.var(error))\n\nplt.title(""Plot with all possible labels"")\nplt.plot(x,y,\'*\',label=\'True Values\')\nplt.plot(x,y1,\'r\',label=\'Estimated\')\nplt.plot(x,y2,\'.\',label=\'Generated labels from Y_estim\')\nplt.legend()\nplt.show()\n'"
assign_1/Assign_1_e.py,16,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\n# Polynomial basis function and its matrix generator\n# This function has a hardcoded bias included. So for removal of it in further steps,\n# np.delete() is used\n\ndef polybasisfunc(x,degree,number_of_samples):\n    phi = np.zeros((number_of_samples,degree+1))\n    deg = list(range(0,degree+1))\n    k = list(range(0,number_of_samples))\n    for i in k:\n        for j in deg:\n            # print(x[i])\n            phi[i][j] = x[i]**j\n    return phi\n\n\n# Number of training samples\nN = 10\n# Generate equispaced floats in the interval [0, 2pi]\nx = np.linspace(0, 2*np.pi, N)\n# Generate noise\nmean = 0\nstd = 0.05\n# Generate some numbers from the sine function\ny = np.sin(x)\n# Add noise\ny += np.random.normal(mean, std, N)\n\n\n# Ask user for parameters\nprint(""Enter degree of polynomial:"")\ndegree = input()\ndegree = int(degree)\nprint(""Enter testing samples:"")\ntest_samples = input()\ntest_samples = int(test_samples)\nprint(""Enter alpha (standard deviation of weights)"")\nalpha = input()\nalpha = float(alpha)\nprint(""Enter sigma (standard deviation of labels)"")\nsigma = input()\nsigma = float(sigma)\nL = ((sigma**2)/(alpha**2))\n\n# Ridge regression\n## Not zero centred data\nphi = polybasisfunc(x,degree,N)\nW = np.matmul(np.matmul(np.linalg.inv(np.matmul(phi.T,phi)+(L*np.identity(degree+1))),phi.T),y)\nx1 = np.linspace(0, 2*np.pi, test_samples)\nphi1 = polybasisfunc(x1,degree,test_samples)\ny1 = np.matmul(phi1,W)\n# Plotting\nplt.title(""Regression on raw data"")\nplt.plot(x,y,\'*\',label=\'True Values\')\nplt.plot(x1,y1,\'.\',label=\'Estimated\')\nplt.legend()\nplt.show()\n\n\n## Not zero centred data and bias ommited\nzeroy = y - np.mean(y)\n# print(np.mean(y),np.mean(zeroy))\nphi = np.delete(phi,0,1)\nW = np.matmul(np.matmul(np.linalg.inv(np.matmul(phi.T,phi)+(L*np.identity(degree))),phi.T),zeroy)\n# print(np.shape(W))\nx1 = np.linspace(0, 2*np.pi, test_samples)\nphi1 = polybasisfunc(x1,degree,test_samples)\nphi1 = np.delete(phi1,0,1)\ny1 = np.matmul(phi1,W) + np.mean(y)\nplt.title(""Regression on zero-centred data"")\nplt.plot(x,y,\'*\',label=\'True Values\')\nplt.plot(x1,y1,\'.\',label=\'Estimated\')\nplt.legend()\nplt.show()\n'"
assign_2/assign_2_a.py,20,"b""import numpy as np\nimport pandas as pd\n\n# Import Data\ndf_X = pd.read_csv('X.csv',header=None)\ndf_Y = pd.read_csv('Y.csv',header=None)\nX = np.array(df_X,dtype=np.float64)\n# X = X.T\nY = np.array(df_Y,dtype=np.float64)\nX_test = np.array([[1,1],[1,-1],[-1,1],[-1,-1],[2,0]])\n\n# Get number of labels in both classes for class occurance\n# probability\nunique, counts = np.unique(Y, return_counts=True)\nprint(unique[0],counts[0],len(Y))\npr0 = float(counts[0])/len(Y)\npr1 = float(counts[1])/len(Y)\n\n############################################\n# Various means and variances for given data\n############################################\n\n# Consider labels with 1 and -1 and store their indices\ny = Y\ny_0 = np.where(y==-1.)[0]\ny_1 = np.where(y==1.)[0]\n# Calculate label specific mean and variances with all combinations\nm00 = np.mean(X[0][y_0])\nm01 = np.mean(X[0][y_1])\nm10 = np.mean(X[1][y_0])\nm11 = np.mean(X[1][y_1])\nv00 = np.var(X[0][y_0])\nv01 = np.var(X[0][y_1])\nv10 = np.var(X[1][y_0])\nv11 = np.var(X[1][y_1])\ns00 = np.std(X[0][y_0])\ns01 = np.std(X[0][y_1])\ns10 = np.std(X[1][y_0])\ns11 = np.std(X[1][y_1])\n######################\n#  Calculate Pr(y=k|x)\n######################\n\nfor i in range(0,len(X_test)):\n    pr_1 = (1.0/(2*np.pi*s01*s11))*(np.exp(-((X_test[i][0]-m01)**2)/(2.0*v01)))*(np.exp(-((X_test[i][1]-m11)**2)/(2.0*v11)))*pr1\n    pr_0 = (1.0/(2*np.pi*s00*s10))*(np.exp(-((X_test[i][0]-m00)**2)/(2.0*v00)))*(np.exp(-((X_test[i][1]-m10)**2)/(2.0*v10)))*pr0\n    print(pr_1,pr_0,X_test[i])\n    if pr_1 > pr_0:\n        print(X_test[i],'1')\n    else:\n        print(X_test[i],'-1')\n"""
assign_2/assign_2_b.py,10,"b'import numpy as np\nimport pandas as pd\n\ndef euclidean_distances(X,X_train):\n\tdist = np.zeros((1000,1))\n\tfor j in np.arange(0,1000):\n\t\tdist[j] = np.linalg.norm(X-X_train[j])\n\tdist = np.reshape(dist,1000)\n\treturn dist\n\ndf_X = pd.read_csv(\'X.csv\',header=None)\ndf_Y = pd.read_csv(\'Y.csv\',header=None)\nX = np.array(df_X,dtype=np.float64)\nX = X.T\nY = np.array(df_Y,dtype=np.float64)\nX_test = np.array([[1,1],[1,-1],[-1,1],[-1,-1]])\n\nprint(""Give K for the K nearest neighbors \\n"")\nK = input()\nK = int(K)\n\nfor i in range(0,len(X_test)):\n\tdist = euclidean_distances(X_test[i],X)\n\tidx = np.argpartition(dist, K)\n\tb = np.take(Y, idx[:K])\n\testimate = float(np.sum(b)/K)\n\tif estimate>0 :\n\t\tprint(X_test[i],"":1"")\n\telse:\n\t\tprint(X_test[i],"":-1"")\n'"
assign_2/assign_2_b_with_bubblesort.py,9,"b'import numpy as np\nimport pandas as pd\n\ndef euclidean_distances(X,X_train):\n\tdist = np.zeros((1000,1))\n\tfor j in np.arange(0,1000):\n\t\tdist[j] = ((X[0]-X_train[j][0])**2)+((X[1]-X_train[j][1])**2)\n\tdist = np.sqrt(dist)\n\tdist = np.reshape(dist,1000)\n\treturn dist\n\ndef bubbleSort(arr):\n\tn = len(arr)\n\tfor i in range(0,n):\n\t\tfor j in range(0, i):\n\t\t\tif arr[j][0] > arr[j+1][0]:\n\t\t\t\ttemp1 = arr[j][0]\n\t\t\t\ttemp2 = arr[j][1]\n\t\t\t\tarr[j][0] = arr[j+1][0]\n\t\t\t\tarr[j][1] = arr[j+1][1]\n\t\t\t\tarr[j+1][0] = temp1\n\t\t\t\tarr[j+1][1] = temp2\n\treturn arr\n\ndf_X = pd.read_csv(\'X.csv\',header=None)\ndf_Y = pd.read_csv(\'Y.csv\',header=None)\nX = np.array(df_X,dtype=np.float64)\nX = X.T\nY = np.array(df_Y,dtype=np.float64)\nX_test = np.array([[1,1],[1,-1],[-1,1],[-1,-1],[1e-5,0.1]])\n\nprint(""Give K for the K nearest neighbors \\n"")\nK = input()\nK = int(K)\n\nfor i in range(0,len(X_test)):\n\tdist = euclidean_distances(X_test[i],X)\n\ttemp = np.array((dist,Y.reshape(1000,)))\n\ttemp = temp.T\n\ttemp = bubbleSort(temp)\n\ttemp = temp[:K]\n\ttemp1 = [x[1] for x in temp]\n\testimate = float(np.sum(temp1)/K)\n\tif estimate>0 :\n\t\tprint(X_test[i],"":1"")\n\telse:\n\t\tprint(X_test[i],"":-1"")\n'"
assign_3/MLP_NN.py,19,"b'\nimport numpy as np\n\n# Parameters\nprint(""Give Number of Hidden Layer nodes :"")\nM = int(input())\nprint(""Give number of training samples per bit pair:"")\nn = int(input())\nprint(""Give std.dev for noise :"")\nnoise = float(input())\nprint(""Give number of Epochs :"")\nepochs = int(input())\nprint(""Give learning rate :"")\nlr = float(input())\nprint(""Give the operation you need to train (Enter text in CAPS):"")\nprint(""1. XOR"")\nprint(""2. OR"")\nprint(""3. AND"")\noption = input()\n# Generate Datasets\n\nInput = np.array([[0,0],[0,1],[1,0],[1,1]])\nif option==\'XOR\':\n    Output = np.array([[0],[1],[1],[0]])\nif option==\'OR\':\n    Output = np.array([[0],[1],[1],[1]])\nif option==\'AND\':\n    Output = np.array([[0],[0],[0],[1]])\n\n\nX = []\nY = []\n\nfor i in range(len(Input)):\n    for j in range(n):\n        X.append(Input[i]+np.random.normal(0,noise,(1,2)))\n        Y.append(Output[i]+np.random.normal(0,noise))\n\nX = np.array(X)\nX = X.reshape((len(Y),2))\nY = np.array(Y)\n\n# defining functions\n\ndef sigm(x):\n    return 1/(1+np.exp(-x))\n\ndef diff_sigm(x):\n    return (sigm(x)-sigm(x)**2)\n\ndef layer(x,W,b):\n    return (np.matmul(W.T,x.reshape(len(x),1)) + b)\n\ndef sq_err(y,Y):\n    return (y-Y)**2\n# Initializing weights\n\nW1 = np.random.normal(0,1,(2,M))\nBi1 = np.random.normal(0,1,(M,1))\nW2 = np.random.normal(0,1,(M,1))\nBi2 = np.random.normal(0,1,(1,1))\n\n\n# In[2]:\n\n\n# training\n\nfor i in range(epochs):\n    w1 = np.zeros(W1.shape)\n    b1 = np.zeros(Bi1.shape)\n    w2 = np.zeros(W2.shape)\n    b2 = np.zeros(Bi2.shape)\n    for j in range(len(Y)):\n        #forward path\n        out1 = layer(X[j],W1,Bi1)\n#         print(out1)\n        z = sigm(out1)\n#         print(z)\n        out2 = layer(z,W2,Bi2)\n        y = sigm(out2)\n#         print(y)\n        #backpropagation\n        b2 += 2*(y-Y[j])*diff_sigm(out2)\n        w2 += 2*(y-Y[j])*diff_sigm(out2)*z\n        loss = sq_err(y,Y[j])\n        for k in range(M):\n            b1[k] += (2*(y-Y[j])*diff_sigm(out2)*diff_sigm(out1[k])*W2[k]).reshape(1,)\n            w1[:,k] += (2*(y-Y[j])*diff_sigm(out2)*diff_sigm(out1[k])*W2[k]*X[j]).reshape(2,)\n    print(loss)\n    W1 -= lr*w1\n    W2 -= lr*w2\n    Bi1 -= lr*b1\n    Bi2 -= lr*b2\n\n\nwhile(1):\n    a=input(""Enter test sample (comma seperated pair): "").split(\',\')\n    for i in range(0,2):\n        a[i]=float(a[i])\n    a=np.array(a)\n    out_1 = layer(a,W1,Bi1)\n    z = sigm(out_1)\n    out_2 = layer(z,W2,Bi2)\n    y_pred= sigm(out_2)\n    if(y_pred >= 0.5):\n        y_pred = 1\n    else :\n        y_pred = 0\n    print(option,\':\',y_pred)\n'"
assign_3/SVM.py,7,"b""import numpy as np\nimport cvxpy as cp\nimport pandas as pd\n\ndf_X = pd.read_csv('Xsvm.csv',header=None)\ndf_Y = pd.read_csv('ysvm.csv',header=None)\nX = np.array(df_X,dtype=np.float64)\nY = np.array(df_Y,dtype=np.float64)\nprint(X.shape,Y.shape)\n\n# Convex Optimization\na = cp.Variable(len(Y))\nR1 = cp.matmul(cp.diag(a),Y)\nR2 = cp.matmul(X.T,R1)\nR4 = cp.norm(R2)**2\nR4.shape\n\nP1 = cp.sum(a)\nConst1 = P1 - 0.5*R4\nConst2 = cp.matmul(a.T,Y)\nConst3 = [0<=a,Const2 == 0]\nobj = cp.Maximize(Const1)\nprob = cp.Problem(obj, Const3)\nprob.solve(verbose=True)\n# print(a.value)\n\nA = (np.array(a.value)).reshape(500,1)\nW = np.zeros((2,))\nfor i in range(len(Y)):\n    W += A[i]*Y[i]*(X[i].T)\n    if(A[i]>1e-4):\n        print(i)\n\nprint(W)\nW0 = (1/Y[281]) - np.dot(W,X[281])\nprint(W0)\n\nTest = np.array([[2,0.5],[0.8,0.7],[1.58,1.33],[0.008, 0.001]])\nfor i in range(len(Test)):\n    est = np.sign(np.dot(W,Test[i])+W0)\n    print(Test[i],est)\n\n# Verification\nfrom sklearn import svm\n\nclf = svm.SVC()\nclf.fit(X,Y)\nclf.predict(Test)\n"""
kernel_methods/SVM.py,7,"b""import numpy as np\nimport cvxpy as cp\nimport pandas as pd\n\ndf_X = pd.read_csv('Xsvm.csv',header=None)\ndf_Y = pd.read_csv('ysvm.csv',header=None)\nX = np.array(df_X,dtype=np.float64)\nY = np.array(df_Y,dtype=np.float64)\nprint(X.shape,Y.shape)\n\n# Convex Optimization\na = cp.Variable(len(Y))\nR1 = cp.matmul(cp.diag(a),Y)\nR2 = cp.matmul(X.T,R1)\nR4 = cp.norm(R2)**2\nR4.shape\n\nP1 = cp.sum(a)\nConst1 = P1 - 0.5*R4\nConst2 = cp.matmul(a.T,Y)\nConst3 = [0<=a,Const2 == 0]\nobj = cp.Maximize(Const1)\nprob = cp.Problem(obj, Const3)\nprob.solve(verbose=True)\n# print(a.value)\n\nA = (np.array(a.value)).reshape(500,1)\nW = np.zeros((2,))\nfor i in range(len(Y)):\n    W += A[i]*Y[i]*(X[i].T)\n    if(A[i]>1e-4):\n        print(i)\n\nprint(W)\nW0 = (1/Y[281]) - np.dot(W,X[281])\nprint(W0)\n\nTest = np.array([[1.9, 0.4],[0.9, 0.9],[1.4, 1.5],[0.01, 0.005]])\nfor i in range(len(Test)):\n    est = np.sign(np.dot(W,Test[i])+W0)\n    print(Test[i],est)\n\n# Verification\nfrom sklearn import svm\n\nclf = svm.SVC()\nclf.fit(X,Y)\nclf.predict(Test)\n"""
Concentration_Inequalities/HW0/CLT.py,10,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# Determine the number of IIDs taken for CLT approximation\nn = 10000\n# Number of samples being called for each IIDs\nk = 100000\n# Uniform Random Variable\na = np.zeros((k,))\nfor i in range(n):\n    a += np.random.uniform(-100,100,(k,))\na = a/n\n\n# Rayleigh RV with scale 1\nb = np.zeros((k,))\nfor i in range(n):\n    b += np.random.rayleigh(1, (k,))\nb = b/n\n\n# Poisson RV with Lambda = 1\nc = np.zeros((k,))\nfor i in range(n):\n    c += np.random.poisson(1,(k,))\nc = c/n\n\n# Log-Normal RV\nd = np.zeros((k,))\nfor i in range(n):\n    d += np.random.lognormal(0,0.1,(k,))\nd = d/n\n\nclts = [a,b,c,d]\nnames = [""Uniform"",""Rayleigh"",""Poisson"",""Log-Normal""]\n\nfig, ax = plt.subplots(4, 1)\n\nfor i in range(len(clts)):\n    hist, bins = np.histogram(clts[i], bins=50, normed=True)\n    bin_centers = (bins[1:]+bins[:-1])*0.5\n    ax[i].set_title(names[i])\n    ax[i].plot(bin_centers, hist)\nplt.show()\n'"
Concentration_Inequalities/HW0/LLN.py,7,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\n## LLN basically states as we take more and more samples from a single distribution and take the mean of them\n## The mean tends to the mean of the original distribution\n\n## Here I am plotting a graph of the mean of the samples drawn from a given distribution vs\n## the number of samples chosen\n\n# Uniform Random Variable\nn1 = 10000\na = []\nfor i in range(n1):\n    a.append(np.mean(np.random.uniform(-10,10,(i,))))\n\n# Log-Normal Random Variable\nn2 = 10000\nb = []\nmean = 0.0\nsigma = 1.0\nfor i in range(n2):\n    b.append(np.mean(np.random.lognormal(mean,sigma,(i,))))\n\nact_mean = np.exp(mean + (sigma**2)/2)\n# Gaussian Random Variable\nn3 = 10000\nc = []\nfor i in range(n3):\n    c.append(np.mean(np.random.normal(0,1,(i,))))\n\n# Laplace Random Variable\nn4 = 10000\nd = []\nfor i in range(n4):\n    d.append(np.mean(np.random.laplace(0,0.1,(i,))))\n\nllns = [a,b,c,d]\nnames = [""Uniform"",""Log-Normal"",""Gaussian"", ""Laplace""]\nfig, ax = plt.subplots(4, 1)\n\nfor i in range(len(llns)):\n    ax[i].set_title(names[i])\n    ax[i].plot(llns[i])\n    if (i != 1):\n        ax[i].plot(np.zeros(np.shape(llns[i])), \'r\')\n    else:\n        ax[i].plot(np.zeros(np.shape(llns[i])) + act_mean, \'r\')\nplt.show()\n'"
Concentration_Inequalities/HW1/1.py,8,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\ndef tail_prob(data,t):\n    probs = []\n    for i in range(0,len(t)):\n        norm = data - np.mean(data)\n        temp = np.count_nonzero(norm > t[i])\n        temp = temp*1.0/len(data)\n        probs.append(temp)\n    return probs\n\n# Degrees of freedom\nn = 100\n# Samples per distribution selected\nk = 100000\n# Reference for plots\nt = np.linspace(0,100,k)\ndata = np.zeros((k,))\n\n# Generating Chi-Squared samples\nfor i in range(n):\n    x = np.random.normal(0,1,(k,))\n    data = data + x**2\n# Reference Gaussian generation. Variance is chosen as the variance of the chi-squared samples generated before\nref_gaussian = np.random.normal(0,np.std(data),(k,))\n\ntail = np.array(tail_prob(data,t))\nplt.plot(t,tail,label=\'Chi-square\')\n\ntail1 = np.array(tail_prob(ref_gaussian,t))\nplt.plot(t,tail1,label=\'Gaussian\')\nplt.ylabel(""Tail probabilities"")\nplt.legend()\nplt.show()\n'"
Concentration_Inequalities/HW1/2a.py,5,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\ndef tail_prob(data,t):\n    probs = []\n    for i in range(0,len(t)):\n        norm = data - np.mean(data)\n        temp = np.count_nonzero(norm > t[i])\n        temp = temp*1.0/len(data)\n        probs.append(temp)\n    return probs\n\n# Samples per distribution\nk = 100000\n# Reference for plots\nt = np.linspace(0,100,k)\n\n# Data generation\nstd = 10\nstd_ref = 2*std\ndata = np.random.normal(0,std,k)\nref_gaussian = np.random.normal(0,std_ref,k)\n\ntail = tail_prob(data,t)\ntail1 = tail_prob(ref_gaussian,t)\nplt.plot(t,tail,label=""Gaussian"")\nplt.plot(t,tail1,label=""Reference Gaussian"")\nplt.legend()\nplt.show()\n'"
Concentration_Inequalities/HW1/2b.py,6,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\ndef tail_prob(data,t):\n    probs = []\n    for i in range(0,len(t)):\n        norm = data - np.mean(data)\n        temp = np.count_nonzero(norm > t[i])\n        temp = temp*1.0/len(data)\n        probs.append(temp)\n    return probs\n\n# Samples per distribution\nk = 100000\n# Reference for plots\nt = np.linspace(0,100,k)\n\n# Data generation\ndata = np.random.uniform(-10,10,k)\nstd_ref = 2*np.std(data)\nref_gaussian = np.random.normal(0,std_ref,k)\n\ntail = tail_prob(data,t)\ntail1 = tail_prob(ref_gaussian,t)\nplt.plot(t,tail,label=""Uniform distribution"")\nplt.plot(t,tail1,label=""Reference Gaussian"")\nplt.legend()\nplt.show()\n'"
Concentration_Inequalities/HW1/2c.py,5,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\ndef tail_prob(data,t):\n    probs = []\n    for i in range(0,len(t)):\n        norm = data - np.mean(data)\n        temp = np.count_nonzero(norm > t[i])\n        temp = temp*1.0/len(data)\n        probs.append(temp)\n    return probs\n\n# Samples per distribution\nk = 100000\n# Reference for plots\nt = np.linspace(0,100,k)\n\n# Data generation for laplacian distribution\nlambd = 3\ndata = np.random.laplace(0,lambd,k)\nref_gaussian = np.random.normal(0,np.std(data),k)\n\ntail = tail_prob(data,t)\ntail1 = tail_prob(ref_gaussian,t)\nplt.plot(t,tail,label=""Laplacian"")\nplt.plot(t,tail1,label=""Reference Gaussian"")\nplt.legend()\nplt.show()\n'"
Concentration_Inequalities/HW1/2d.py,6,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\ndef tail_prob(data,t):\n    probs = []\n    for i in range(0,len(t)):\n        norm = data - np.mean(data)\n        temp = np.count_nonzero(norm > t[i])\n        temp = temp*1.0/len(data)\n        probs.append(temp)\n    return probs\n\n# Samples per distribution\nk = 100000\n# Reference for plots\nt = np.linspace(0,100,k)\n\n# Data generation\ndata = np.random.lognormal(0,1,k)\nstd_ref = np.std(data)\nref_gaussian = np.random.normal(0,std_ref,k)\n\ntail = tail_prob(data,t)\ntail1 = tail_prob(ref_gaussian,t)\nplt.plot(t,tail,label=""Heavy Tailed distribution (lognormal)"")\nplt.plot(t,tail1,label=""Reference Gaussian"")\nplt.legend()\nplt.show()\n'"
Concentration_Inequalities/HW1/2e.py,7,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\ndef tail_prob(data,t):\n    probs = []\n    for i in range(0,len(t)):\n        norm = data - np.mean(data)\n        temp = np.count_nonzero(norm > t[i])\n        temp = temp*1.0/len(data)\n        probs.append(temp)\n    return probs\n\n# Number of bounded distros to be summed up\nn = 20\n# Samples per distribution\nk = 10000\n# Reference for plots\nt = np.linspace(0,100,k)\n\ndata = np.zeros((k,))\n\n# Generating Chi-Squared samples\nfor i in range(n):\n    x = np.random.uniform(-1,1,(k,))\n    data = data + x\n\nstd_ref = 2*np.std(data)\nref_gaussian = np.random.normal(0,std_ref,k)\n\ntail = tail_prob(data,t)\ntail1 = tail_prob(ref_gaussian,t)\nplt.plot(t,tail,label=""Bounded RVs summed (Uniform)"")\nplt.plot(t,tail1,label=""Reference Gaussian"")\nplt.legend()\nplt.show()\n'"
Concentration_Inequalities/HW1/5.py,10,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\n## Comparing tightness among Chernoff, Hoeffding and Bennets inequalities is basically checking out\n## Which of the inequalities model the tail probabilities the best\n\n## For convinience, Uniform random variable has been chosen with 0 mean and bounded between -10 and 10\n## n such uniforms are added up to get make Hoeffding and Bennets relevant\n\nsamples = 10000\n## Lambdas for Bennets, Chernoff and Hoeffding\nt = np.linspace(0,100,1e4)\n# Fix the number of distributions to add up\nk = 10\n# Fix bounds on uniform random variable\na = -5.0\nb = 5.0\n\ndef tail_prob(data,t):\n    probs = []\n    for i in range(0,len(t)):\n        norm = data - np.mean(data)\n        temp = np.count_nonzero(norm > t[i])\n        temp = temp*1.0/len(data)\n        probs.append(temp)\n    return probs\n\ndata = np.zeros((samples,))\nfor i in range(k):\n    data += np.random.uniform(a,b,samples)\n\ndef h(x):\n    return (1+x)*np.log(1+x)-x\n\n# Variance formula for Uniform RV\nv = (k*a**2)/3.0\n\n## the bounds\nbennet = np.exp(-v*h(b*t*1.0/v)/b**2) ## The expression from problem 4\nhoeff = np.exp(-2*np.square(t)/(k*np.square(a*2)))\ntail = np.array(tail_prob(data,t))\n\n## Chernoff bound must hold for any s, so fixing s to prevent multivariable plotting dilemma\ns = 1e-1\nchernoff = np.power((np.exp(s*a)-np.exp(-s*a))/(2*s*a),k) * np.exp(-s*t)\n\nplt.plot(tail,label='Tail probabilities')\nplt.plot(bennet,label='Bennet Bound')\nplt.plot(hoeff,label='Hoeffding Bound')\nplt.plot(chernoff,label='Chernoff Bound')\nplt.legend()\nplt.show()\n"""
DeepLearning/HW1/coverts.py,9,"b'class conv2D():\n    def __init__(self):\n        inp = np.random.uniform(0,1,(28,28,1))\n        ker_dim = (5,5)\n        ker_val = np.random.normal(0,1,self.ker_dim)\n        stride = 1\n        padding = \'True\'\n        activation = \'relu\'\n\n    def operate(self):\n\n        h,w,d = np.shape(inp)\n        ker_list = list(ker_dim)\n        ker_list.append(d)\n        ker_dim = tuple(ker_list)\n\n        print(ker_val)\n\n        if(ker_dim != ker_val.shape):\n            print(""Enter proper shaped kernel values"")\n            return\n\n        ini_w = 0\n        ini_h = 0\n\n        pad_w = (((stride-1)*w) + ker_list[1] - stride)/2\n        pad_w = int(pad_w)\n\n        pad_h = (((stride-1)*h) + ker_list[0] - stride)/2\n        pad_h = int(pad_h)\n\n        if padding == ""True"":\n            pad_inp = np.zeros(((h+2*pad_h),(w+2*pad_w),d))\n            for i in range(d):\n                pad_inp[:,:,i] = np.pad(inp[:,:,i], ((pad_h,pad_h),(pad_w,pad_w)), \'constant\')\n            pad_inp = pad_inp.astype(int)\n            out_w = int(((w + 2*pad_w - ker_list[1])/stride) + 1)\n            out_h = int(((h + 2*pad_h - ker_list[0])/stride) + 1)\n\n        else :\n            pad_inp = inp\n            out_w = int(((w - ker_list[1])/stride) + 1)\n            out_h = int(((h - ker_list[0])/stride) + 1)\n\n        pad_inp = pad_inp.astype(int)\n\n        conv_out = np.zeros((out_h,out_w))\n        for i in range(1,out_h+1):\n            for j in range(1,out_w+1):\n                temp = pad_inp[ini_h:ini_h + ker_list[0] , ini_w:ini_w + ker_list[1]]\n                temp = temp.astype(float)\n                ker_val = ker_val.astype(float)\n                if temp.shape == ker_val.shape :\n                    conv_out[i-1][j-1] = np.sum(temp*ker_val)\n                ini_w += stride\n            ini_h += stride\n            ini_w = 0\n\n        if activation == \'sigm\' :\n            conv_out = sigmoid(conv_out)\n\n        elif activation == \'relu\' :\n            conv_out = relu(conv_out)\n\n        elif activation == \'tanh\' :\n            conv_out = tanh(conv_out)\n\n        return conv_out, ker_val\n'"
GANs/Tensorflow_practice/TFtuts_2.py,0,"b""import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('/tmp/',one_hot=True)\nhl_1 = 500\nhl_2 = 500\nhl_3 = 500\nclasses = 10\nbatch_size = 1000\nx = tf.placeholder('float',[None,784])\ny = tf.placeholder('float')\n\ndef neural_net(data):\n    hidden_layer_1 = {'weights':tf.Variable(tf.random_normal([784,hl_1])),\n                     'biases':tf.Variable(tf.random_normal([hl_1]))}\n    hidden_layer_2 = {'weights':tf.Variable(tf.random_normal([hl_1,hl_2])),\n                     'biases':tf.Variable(tf.random_normal([hl_2]))}\n    hidden_layer_3 = {'weights':tf.Variable(tf.random_normal([hl_2,hl_3])),\n                     'biases':tf.Variable(tf.random_normal([hl_3]))}\n    output_layer = {'weights':tf.Variable(tf.random_normal([hl_3,classes])),\n                     'biases':tf.Variable(tf.random_normal([classes]))}\n\n    l1 = tf.add(tf.matmul(data,hidden_layer_1['weights']),hidden_layer_1['biases'])\n    l1 = tf.nn.relu(l1)\n    l2 = tf.add(tf.matmul(l1,hidden_layer_2['weights']),hidden_layer_2['biases'])\n    l2 = tf.nn.relu(l2)\n    l3 = tf.add(tf.matmul(l2,hidden_layer_3['weights']),hidden_layer_3['biases'])\n    l3 = tf.nn.relu(l3)\n    out = tf.matmul(l3,output_layer['weights'])+output_layer['biases']\n    return out\n\ndef train(x):\n    prediction = neural_net(x)\n    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n    optimizer = tf.train.AdamOptimizer().minimize(cost)\n    hm_epochs = 50\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        for epoch in range(hm_epochs):\n            epoch_loss = 0\n            for _ in range(int(mnist.train.num_examples/batch_size)):\n                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n                epoch_loss += c\n            print(epoch,'/',hm_epochs,'loss:',epoch_loss)\n\n        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n        print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n\ntrain(x)\n"""
GANs/Tensorflow_practice/TFtuts_2_i.py,0,"b'import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist= input_data.read_data_sets(""/tmp/data/"", one_hot=True)\nn_nodes_hl1=500\nn_nodes_hl2=500\nn_nodes_hl3=500\nn_classes=10\nbatch_size=100\nx=tf.placeholder(\'float\',[None,784])\ny=tf.placeholder(\'float\')\n\ndef neural(data):\n    hidden_1_layer={\'weights\':tf.Variable(tf.random_normal([784, n_nodes_hl1])),\n    \'biases\':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n    hidden_2_layer={\'weights\':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n    \'biases\':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n    hidden_3_layer={\'weights\':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n    \'biases\':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n    output_layer={\'weights\':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n    \'biases\':tf.Variable(tf.random_normal([n_classes]))}\n\n    l1=tf.add(tf.matmul(data, hidden_1_layer[\'weights\']), hidden_1_layer[\'biases\'])\n    li= tf.nn.relu(l1)\n    l2=tf.add(tf.matmul(l1, hidden_2_layer[\'weights\']), hidden_2_layer[\'biases\'])\n    l2= tf.nn.relu(l2)\n    l3=tf.add(tf.matmul(l2, hidden_3_layer[\'weights\']), hidden_3_layer[\'biases\'])\n    l3= tf.nn.relu(l3)\n    output= tf.matmul(l3, output_layer[\'weights\'])+ output_layer[\'biases\']\n    return output\n\ndef train(x):\n    prediction=neural(x)\n    cost= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n    optimizer=tf.train.AdamOptimizer().minimize(cost)\n    hm_epochs=50\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        for epoch in range(hm_epochs):\n            epoch_loss=0\n            for _ in range(int(mnist.train.num_examples/batch_size)):\n                epoch_x,epoch_y = mnist.train.next_batch(batch_size)\n                _,c=sess.run([optimizer,cost],feed_dict={x: epoch_x, y: epoch_y})\n                epoch_loss += c\n            print(\'Epoch\', epoch, \'completed out of\', hm_epochs, \'loss:\',epoch_loss)\n\n        correct= tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n        accuracy= tf.reduce_mean(tf.cast(correct,\'float\'))\n        print(\'Accuracy:\',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n\ntrain(x)\n'"
Representation_learning/HW0/K-Means.py,13,"b'#!/usr/bin/env python\n# coding: utf-8\n\n# In[2]:\n\n\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n\n# In[3]:\n\n\n### Clustering \ndef Clustering(centroids,img):\n    b = []\n    c = []\n    for i in range(len(img)):\n        for j in range(len(img[0])):\n            a = []\n            for k in range(len(centroids)):\n                a.append(np.linalg.norm(centroids[k]-img[i][j]))\n            b.append(a.index(min(a)))\n            c.append(img[i][j])\n    return np.array(b),(np.array(c,dtype=float))\n\n\n# In[4]:\n\n\n### Centroid for new cluster\ndef centroid(b,c,K,points_p_clus):\n    cent = 0\n    cent1 = []\n    for j in range(K):\n        cent = np.mean(c[np.argwhere(b==j)],axis=0)\n        cent1.append(cent)\n    return np.array(cent1)\n\n\n# In[5]:\n\n\ndef Cluster_plotting(centroids,img,cluster_index):\n    b = []\n    c = []\n    d = np.full(img.shape,0)\n    for i in range(len(img)):\n        for j in range(len(img[0])):\n            a = []\n            for k in range(len(centroids)):\n                a.append(np.linalg.norm(centroids[k]-img[i][j]))\n            if a.index(min(a)) == cluster_index:\n                d[i][j] = img[i][j]\n#     print(d[20][55])\n    plt.imshow(d)\n\n\n# In[6]:\n\n\n### Access the image\npic = Image.open(\'wildlife-bears2.jpg\')\nimg = np.array(pic)\n\n\n# In[7]:\n\n\n### Asking inputs\nprint(""Give number of clusters :"")\nK = int(input())\nprint(""Give threshold E :"")\nE = float(input())\n\n### Initialize random centroids\n### The way im initializing the centroids is a bit bizzare \n### I\'m choosing K [i,j] pixels which are a part of gaussian applied with standard deviation (min(width,height)/10)\n### Also, randomly picking pixels is causing issues sometimes.\n\nCent = []\nfor i in range(K):\n    (w,h)=int(np.random.normal(len(img)/2,len(img)/10)),int(np.random.normal(len(img[0])/2,len(img)/10))\n    Cent.append(img[w][h])\n# Cent = np.array([[37,35,255],[215,0,24],[0,221,255]])\n\n\n# In[8]:\n\n\nb,c = Clustering(Cent,img)\n\n\n# In[9]:\n\n\nprint(b.shape,c.shape)\npoints_per_cluster = []\nfor i in range(K):\n    points_per_cluster.append(np.count_nonzero(b==i))\nprint(points_per_cluster)\n\n\n# In[10]:\n\n\n# New_Cent = centroid(b,c,K,points_per_cluster)\n# print(New_Cent)\n\n\n# In[11]:\n\n\nNew_Cent = []\ndiff = E+1\nwhile(diff>E):\n    b,c = Clustering(Cent,img)\n    points_per_cluster = []\n    for i in range(K):\n        points_per_cluster.append(np.count_nonzero(b==i))\n    New_Cent = centroid(b,c,K,points_per_cluster)\n    diff = np.linalg.norm(New_Cent-Cent)\n    print(diff)\n    Cent = New_Cent\n\n\n# In[12]:\n\n\npoints_per_cluster = []\nfor i in range(K):\n    points_per_cluster.append(np.count_nonzero(b==i))\nprint(points_per_cluster)\n\n\n# In[13]:\n\n\nplt.imshow(img)\n\n\n# In[15]:\n\n\nCluster_plotting(Cent,img,0)\n\n\n# In[16]:\n\n\nCluster_plotting(Cent,img,1)\n\n'"
Representation_learning/HW0/MLE_GrndTruth_against_various_distributions.py,45,"b'#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# In[2]:\n\n\n#Number of initialized data points\nN_points = 100000\n#Number of columns in histogram\nn_bins = 100\nGround_truth = np.random.normal(1000,10,N_points)\n\n\n# All plots in on place\n\n# In[3]:\n\n\nTH = np.mean(Ground_truth)\nGenerated_binomial = np.random.binomial(TH,1,N_points)\n\nLambda = np.mean(Ground_truth)\nGenerated_poisson = np.random.poisson(Lambda,N_points)\n\nBeta = np.mean(Ground_truth)\nGenerated_exponential = np.random.exponential(Beta,N_points)\n\nMean = np.mean(Ground_truth)\nStd_dev = np.std(Ground_truth) \nGenerated_Gaussian = np.random.normal(Mean,Std_dev,N_points)\n\nMu = np.median(Ground_truth)\nBeta1 = np.mean(np.abs(Ground_truth-Mu))\nGenerated_laplace = np.random.laplace(Mu,Beta1,N_points)\n\n\n# In[4]:\n\n\nfig, axs = plt.subplots(1, 5, sharey=True, tight_layout=True)\naxs[0].hist(Ground_truth, bins=n_bins,label = ""Ground truth"")\naxs[0].legend()\n# axs[1].hist(Generated_binomial, bins=n_bins)\naxs[1].hist(Generated_poisson, bins=n_bins, label = ""Poisson"",color = \'c\')\naxs[1].hist(Ground_truth, bins=n_bins,label = ""Ground truth"",color=\'r\',alpha=0.5)\naxs[1].legend()\n\naxs[2].hist(Generated_exponential, bins=n_bins,label = ""Exponential"",color=\'c\')\naxs[2].hist(Ground_truth, bins=n_bins,label = ""Ground truth"",color=\'r\',alpha=0.5)\naxs[2].legend()\n\naxs[3].hist(Generated_Gaussian, bins=n_bins,label = ""Gaussian"",color=\'c\')\naxs[3].hist(Ground_truth, bins=n_bins,label = ""Ground truth"",color=\'r\',alpha=0.5)\naxs[3].legend()\n\naxs[4].hist(Generated_laplace, bins=n_bins,label = ""Laplace"",color=\'c\')\naxs[4].hist(Ground_truth, bins=n_bins,label = ""Ground truth"",color=\'r\',alpha=0.5)\naxs[4].legend()\n\nfig.set_size_inches(18.5, 10.5, forward=True)\n# plt.title(""Histograms of Ground truth, Poisson, Exponential, Gaussian and Laplace distributions"")\n# plt.savefig(""Distributions_Grndtruth_Gaussian.png"")\n\n\n# Binomial distribution has a condition that the probability must be between 0 and 1\n# Hence choosing a different dataset which can cater these criterion\n# Also, the binomial is a single trial binomial. For multi trial binomial, the math doesn\'t work out\n\n# In[5]:\n\n\nGround_truth_1 = np.random.uniform(0,1,N_points)\n\nTH = np.mean(Ground_truth_1)\nGenerated_binomial = np.random.binomial(TH,1,N_points)\n\nLambda = np.mean(Ground_truth_1)\nGenerated_poisson = np.random.poisson(Lambda,N_points)\n\nBeta = np.mean(Ground_truth_1)\nGenerated_exponential = np.random.exponential(Beta,N_points)\n\nMean = np.mean(Ground_truth_1)\nStd_dev = np.std(Ground_truth_1) \nGenerated_Gaussian = np.random.normal(Mean,Std_dev,N_points)\n\nMu = np.median(Ground_truth_1)\nBeta1 = np.abs(np.mean(np.abs(Ground_truth_1-Mu)))\nGenerated_laplace = np.random.laplace(Mu,Beta1,N_points)\n\n\n# In[6]:\n\n\nfig, axs = plt.subplots(1, 6, sharey=True, tight_layout=False)\naxs[0].hist(Ground_truth_1, bins=n_bins,label=""Ground truth"")\n\naxs[1].hist(Generated_binomial, bins=n_bins,label=""Binomial"")\naxs[1].hist(Ground_truth_1, bins=n_bins,label = ""Ground truth"",color=\'r\',alpha=0.5)\naxs[1].legend()\n\naxs[2].hist(Generated_poisson, bins=n_bins,label=""Poisson"")\naxs[2].hist(Ground_truth_1, bins=n_bins,label = ""Ground truth"",color=\'r\',alpha=0.5)\naxs[2].legend()\n\naxs[3].hist(Generated_exponential, bins=n_bins,label=""Exponential"")\naxs[3].hist(Ground_truth_1, bins=n_bins,label = ""Ground truth"",color=\'r\',alpha=0.5)\naxs[3].legend()\n\naxs[4].hist(Generated_Gaussian, bins=n_bins,label=""Gaussian"")\naxs[4].hist(Ground_truth_1, bins=n_bins,label = ""Ground truth"",color=\'r\',alpha=0.5)\naxs[4].legend()\n\naxs[5].hist(Generated_laplace, bins=n_bins,label=""Laplacian"")\naxs[5].hist(Ground_truth_1, bins=n_bins,label = ""Ground truth"",color=\'r\',alpha=0.5)\naxs[5].legend()\n\nfig.set_size_inches(18.5, 10.5, forward=True)\n# plt.title(""Histograms of Ground truth, Binomial, Poisson, Exponential, Gaussian and Laplace distributions"")\n# plt.savefig(""Distributions_GrndTrth_Uniform.png"")\n\n\n# One on one comparision\n\n# In[7]:\n\n\nprint(""Select distribution by number"")\nprint(""1. Binomial"")\nprint(""2. Poisson"")\nprint(""3. Exponential"")\nprint(""4. Gaussian"")\nprint(""5. Laplacian"")\ndistri = int(input())\n\n\n# Here I\'ve kept a plot with binomial distribution samples generated with different tries parameter N\n\n# In[8]:\n\n\nif(distri == 1):\n    Ground_truth_u = np.random.uniform(0,1,N_points)    \n    TH = np.mean(Ground_truth_u)\n    Generated_bin_1 = np.random.binomial(1,TH,N_points)\n    Generated_bin_5 = np.random.binomial(5,TH,N_points)\n    Generated_bin_10 = np.random.binomial(10,TH,N_points)\n    Generated_bin_100 = np.random.binomial(100,TH,N_points)\n    Generated_bin_1000 = np.random.binomial(1000,TH,N_points)  \n    Generated_bin_10000 = np.random.binomial(10000,TH,N_points)  \n    Generated_bin_100000 = np.random.binomial(100000,TH,N_points)  \n    \n    fig, axs = plt.subplots(1, 8, sharey=True, tight_layout=True)\n    axs[0].hist(Ground_truth_u, bins=n_bins)\n    axs[1].hist(Generated_bin_1, bins=n_bins)\n    axs[2].hist(Generated_bin_5, bins=n_bins)\n    axs[3].hist(Generated_bin_10, bins=n_bins)\n    axs[4].hist(Generated_bin_100, bins=n_bins)\n    axs[5].hist(Generated_bin_1000, bins=n_bins)    \n    axs[6].hist(Generated_bin_10000, bins=n_bins)    \n    axs[7].hist(Generated_bin_100000, bins=n_bins)\n    fig.set_size_inches(18.5, 10.5, forward=True)\n#     plt.title(""Binomial generated with varying tries (N)"")\n#     plt.savefig(""Distributions_Binomial_different_tries.png"") \n\n\n# In[9]:\n\n\nif(distri == 2):\n    Lambda = np.mean(Ground_truth)\n    Generated = np.random.poisson(Lambda,N_points)\n    fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n    axs[0].hist(Ground_truth, bins=n_bins)\n    axs[1].hist(Generated, bins=n_bins)\n\n\n# In[10]:\n\n\nif(distri == 3):\n    Beta = np.mean(Ground_truth)\n    Generated = np.random.exponential(Beta,N_points)\n    fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n    axs[0].hist(Ground_truth, bins=n_bins)\n    axs[1].hist(Generated, bins=n_bins)\n\n\n# In[11]:\n\n\nif(distri == 4):\n    Mean = np.mean(Ground_truth)\n    Std_dev = np.std(Ground_truth) \n    Generated = np.random.normal(Mean,Std_dev,N_points)\n    fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n    axs[0].hist(Ground_truth, bins=n_bins)\n    axs[1].hist(Generated, bins=n_bins)\n\n\n# In[12]:\n\n\nif(distri == 5):\n    Mu = np.median(Ground_truth)\n    Beta = np.mean(np.abs(Ground_truth-Mu))\n    Generated = np.random.laplace(Mu,Beta,N_points)\n    fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n    axs[0].hist(Ground_truth, bins=n_bins)\n    axs[1].hist(Generated, bins=n_bins)\n\n\n# In[ ]:\n\n\n\n\n'"
Representation_learning/HW0/MLE_with_same_distributions.py,17,"b'#!/usr/bin/env python\n# coding: utf-8\n\n# In[12]:\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# In[13]:\n\n\n#Number of initialized data points\nN_points = 100000\n#Number of columns in histogram\nn_bins = 100\n\n\n# In[32]:\n\n\nprint(""Select distribution by number"")\nprint(""1. Binomial"")\nprint(""2. Poisson"")\nprint(""3. Exponential"")\nprint(""4. Gaussian"")\nprint(""5. Laplacian"")\ndistri = int(input())\n\n\n# In[22]:\n\n\nif(distri == 1):\n    print(""Give probability factor for Binomial distribution generation"")\n    p = float(input())\n    while p>1 or p<0 :\n        print(""Re enter p with proper bounds"")\n        p = float(input())\n    Ground_truth = np.random.binomial(1,p,N_points)    \n    TH = np.mean(Ground_truth)\n    Generated_bin_1 = np.random.binomial(1,TH,N_points) \n    \n    fig, axs = plt.subplots(1, 3, sharey=True, tight_layout=True)\n    axs[0].hist(Ground_truth, bins=n_bins,alpha=0.5,color=\'r\',label=\'Ground truth\')\n    axs[0].legend()\n    \n    axs[1].hist(Generated_bin_1, bins=n_bins,label=""Generated"")\n    axs[1].legend()\n    \n    axs[2].hist(Generated_bin_1, bins=n_bins,label=\'Generated\')\n    axs[2].hist(Ground_truth, bins=n_bins,alpha=0.5,color=\'r\',label=\'Ground truth\')\n    axs[2].legend()\n    fig.set_size_inches(18.5, 10.5, forward=True)\n#     plt.savefig(""Bin_Vs_Bin.png"")\n\n\n# In[24]:\n\n\nif(distri == 2):\n    print(""Enter lambda for poisson"")\n    Lambda_1 = float(input())\n    while Lambda_1<0 :\n        print(""Enter proper lambda (Lambda >= 0)"")\n        Lambda_1 = float(input())\n    \n    Ground_truth = np.random.poisson(Lambda_1,N_points) \n    Lambda = np.mean(Ground_truth)\n    Generated = np.random.poisson(Lambda,N_points)\n    fig, axs = plt.subplots(1, 3, sharey=True, tight_layout=True)\n    axs[1].hist(Generated, bins=n_bins,label=\'Generated\')\n    axs[1].legend()\n    \n    axs[0].hist(Ground_truth, bins=n_bins,alpha=0.5,color=\'r\',label=\'Ground truth\')\n    axs[0].legend()\n    \n    axs[2].hist(Generated, bins=n_bins,label=\'Generated\')\n    axs[2].hist(Ground_truth, bins=n_bins,alpha=0.5,color=\'r\',label=\'Ground truth\')\n    axs[2].legend()\n    fig.set_size_inches(18.5, 10.5, forward=True)\n#     plt.savefig(""Lambda_Vs_Lambda.png"")    \n\n\n# In[29]:\n\n\nif(distri == 3):\n    print(""Enter beta (1/lambda) for Exponential distribution"")\n    beta = float(input())\n    while beta<0 :\n        print(""Enter proper beta (beta >= 0)"")\n        beta = float(input())\n    Ground_truth = np.random.exponential(beta,N_points) \n    beta1 = np.mean(Ground_truth)\n    Generated = np.random.exponential(beta1,N_points)\n    fig, axs = plt.subplots(1, 3, sharey=True, tight_layout=True)\n    axs[0].hist(Ground_truth, bins=n_bins,label=\'Ground truth\',color=\'r\',alpha=0.5)\n    axs[0].legend()\n    \n    axs[1].hist(Generated, bins=n_bins,label=\'Generated\')\n    axs[1].legend()\n    \n    axs[2].hist(Generated, bins=n_bins,label=\'Generated\')\n    axs[2].hist(Ground_truth, bins=n_bins,label=\'Ground truth\',color=\'r\',alpha=0.5)\n    axs[2].legend()\n    \n    fig.set_size_inches(18.5, 10.5, forward=True)\n    \n#     plt.savefig(""Exp_Vs_Exp.png"")        \n\n\n# In[31]:\n\n\nif(distri == 4):\n    print(""Enter the mean for ground truth gaussian"")\n    mean = float(input())\n    print(""Give std dev for the ground truth gaussian"")\n    std = float(input())\n    while std<0:\n        print(""Give std dev for the ground truth properly (>0)"")\n        std = float(input())\n    Ground_truth = np.random.normal(mean,std,N_points)\n    Mean = np.mean(Ground_truth)\n    Std_dev = np.std(Ground_truth) \n    Generated = np.random.normal(Mean,Std_dev,N_points)\n    fig, axs = plt.subplots(1, 3, sharey=True, tight_layout=True)\n    axs[0].hist(Ground_truth, bins=n_bins,color=\'r\',alpha=0.5,label=""Ground truth"")\n    axs[0].legend()\n    \n    axs[1].hist(Generated, bins=n_bins,label=""Generated"")\n    axs[1].legend()\n    \n    axs[2].hist(Generated, bins=n_bins,label=""Generated"")\n    axs[2].hist(Ground_truth, bins=n_bins,color=\'r\',alpha=0.5,label=""Ground truth"")\n    axs[2].legend()\n    \n    fig.set_size_inches(18.5, 10.5, forward=True)\n    \n#     plt.savefig(""Gauss_Vs_Gauss.png"")\n\n\n# In[41]:\n\n\nif(distri == 5):\n    print(""Give Mu for Laplace"")\n    Mu1 = float(input())\n    print(""Give Beta for Laplace"")\n    Beta1 = float(input())\n    Ground_truth = np.random.laplace(Mu1,Beta1,N_points)\n    Mu = np.median(Ground_truth)\n    Beta = np.mean(np.abs(Ground_truth-Mu))\n    Generated = np.random.laplace(Mu,Beta,N_points)\n    fig, axs = plt.subplots(1, 3, sharey=True, tight_layout=True)\n    axs[0].hist(Ground_truth, bins=n_bins,label=""Ground truth"",alpha=0.5)\n    axs[0].legend()\n    \n    axs[1].hist(Generated, bins=n_bins, label=""Generated"")\n    axs[1].legend()\n    \n    axs[2].hist(Generated, bins=n_bins, label=""Generated"")\n    axs[2].hist(Ground_truth, bins=n_bins,label=""Ground truth"",alpha=0.5)\n    axs[2].legend()\n    \n    fig.set_size_inches(18.5, 10.5, forward=True)\n    \n#     plt.savefig(""Laplace_Vs_Laplace.png"")\n\n\n# In[ ]:\n\n\n\n\n'"
Representation_learning/HW0/PCA.py,6,"b""#!/usr/bin/env python\n# coding: utf-8\n\n# In[2]:\n\n\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n\n# In[3]:\n\n\ndef Mean_centering(X):\n    X = X.astype('float')\n    for i in range(len(X)):\n        X[i] = X[i] - np.mean(X[i])\n    print(np.mean(X[0]))\n    return X\n\n\n# In[4]:\n\n\npic = Image.open('wildlife-bears2.jpg')\nimg = (np.array(pic))\n\n\n# In[5]:\n\n\na,b,c = img.shape\nprint(a,b,c)\n\n\n# In[6]:\n\n\nFlat = img.reshape((a*b,c))\nFlat = Flat.astype('float32')\n\n\n# In[7]:\n\n\nFlat = Flat.T\nFlat.shape\n\n\n# In[8]:\n\n\nFlat = Mean_centering(Flat)\n\n\n# In[9]:\n\n\nCx = (np.matmul(Flat,Flat.T)/(len(Flat[0])))\n\n\n# In[10]:\n\n\nLx,Ex = np.linalg.eig(Cx)\n\n\n# In[11]:\n\n\nY = np.matmul(Ex.T,Flat)\nY = Y.T\nY = Y.reshape(img.shape)\n\n\n# In[12]:\n\n\nY = Y.astype(int)\n\n\n# In[13]:\n\n\nplt.imshow(Y)\n\n\n# In[14]:\n\n\nplt.imshow(pic)\n\n"""
Representation_learning/HW1/GMM.py,24,"b'import numpy as np\nfrom math import log\n### Dataset generation\n#\n\n# np.random.seed(42)\n\nprint(""A note to the user\\n This code has a function of generating data on it\'s own \\n from a given number of multivariate_normal functions. \\n This makes the covariance matrices go to Singular sometimes even though they are ensured to be positive semi-definite."")\n\nprint(""Also in case of high number of Modes, the calculations of gaussian PDF fails and returns Nans"")\nprint(""In SUCH a case, please execute the code again \\n"")\nprint(""\\n"")\n\nprint(""Give the degree for the data to be generated"")\nD = int(input())\nprint(""Give the number of modes the data must have"")\nNm = int(input())\nprint(""Give the number of samples per mode"")\nNspm = int(input())\nprint(""Give log likelihood error tolerance"")\nerr = float(input())\n\n# D = 5\n# Nm = 2\n# Nspm = 100\nNs = Nspm*Nm\n\nMean = []\nCov = []\ndata = []\nfor i in range(Nm):\n    Mean.append(np.random.uniform(1,2,D))\n    temp1 = np.random.uniform(1,2,(D,D))\n    Cov.append(np.matmul(temp1,temp1.T) + 1)\n    for j in range(Nspm):\n        data.append(np.random.multivariate_normal(Mean[i],Cov[i]))\n\ndef MulGaus_pdf(x,mean,cov):\n    # Reshaping and converting for simpler steps\n    x = np.array(x)\n    mean = np.array(mean)\n    cov = np.array(cov)\n    d = len(mean)\n    x = x.reshape((d,1))\n    mean = mean.reshape((d,1))\n    # Actual Functions\n    temp = np.matmul((x-mean).T,np.linalg.inv(cov))\n    temp = np.matmul(temp,(x-mean))\n    num = np.exp(-0.5*temp)\n    den = np.sqrt(np.linalg.det(2*np.pi*cov))\n    pdf = num/den\n    return pdf\n\n\'\'\'\n# Debug for PDF Function\n# print(MulGaus_pdf(Mean[0],Mean[0],Cov[0]))\n# den = np.sqrt(np.linalg.det(2*np.pi*Cov[0]))\n# print(1/den)\n\'\'\'\n\ndef posterior(data,mean_list,cov_list,mix_list,i,k):\n    post = mix_list[k]*MulGaus_pdf(data[i],mean_list[k],cov_list[k])\n    post_sum = 0\n    for w in range(len(mix_list)):\n        post_sum += mix_list[w]*MulGaus_pdf(data[i],mean_list[w],cov_list[w])\n    gamma = post/post_sum\n    return (np.asscalar(post), np.asscalar(post_sum), np.asscalar(gamma))\n\ndef posterior_sum(data,mean_list,cov_list,mix_list,i):\n    _,temp,_ = posterior(data,mean_list,cov_list,mix_list,i,0)\n    return temp\n\n\'\'\'\n# Debug for posterior function\nmix = np.random.uniform(1,10,len(Mean))\nmix = mix/np.sum(mix)\nprint(posterior(data,Mean,Cov,mix,1,0))\nprint(posterior_sum(data,Mean,Cov,mix,1))\n\'\'\'\n\ndef log_likelihood(data,mean_list,cov_list,mix_list):\n    log_like = 0\n    for i in range(len(data)):\n        post_sum = posterior_sum(data,mean_list,cov_list,mix_list,i)\n        log_post_sum = np.log(post_sum)\n        log_like += log_post_sum\n    # print(log_like)\n    return np.asscalar(log_like)\n\n\'\'\'\n# print(log_likelihood(data,Mean,Cov,mix))\n\'\'\'\n\n\n#####################################################################\n### Random initialisation for the mean, covariance and mixing factors\n#####################################################################\n\nmix = np.random.uniform(2,3,len(Mean))\nmix_list = mix/np.sum(mix)\nmean_list = []\ncov_list = []\nfor i in range(Nm):\n    mean_list.append(np.random.uniform(1,5,D))\n    temp1 = np.random.uniform(1,5,(D,D))\n    cov_list.append(np.matmul(temp1,temp1.T))\nprint(\'old log likelihood =\', log_likelihood(data,mean_list,cov_list,mix_list))\n#############################\n### Expeectation Maximization\n#############################\nitera = 0\nwhile(itera<100):\n    mean_list_temp = []\n    cov_list_temp = []\n    mix_list_temp = []\n    log_o = log_likelihood(data,mean_list,cov_list,mix_list)\n    for w in range(Nm):\n        Nk = 0\n        mean_num = 0\n        cov_num = 0\n        for i in range(Ns):\n            _,_,gam = posterior(data,mean_list,cov_list,mix_list,i,w)\n            Nk += gam\n        for x in range(Ns):\n            _,_,gam = posterior(data,mean_list,cov_list,mix_list,x,w)\n            mean_num += gam*data[x]\n            cov_num += gam*np.outer((data[x]-mean_list[w]),(data[x]-mean_list[w]))\n        mean_list_temp.append(mean_num/Nk)\n        cov_list_temp.append(cov_num/Nk)\n        mix_list_temp.append(Nk/Ns)\n    itera += 1\n    mean_list = mean_list_temp\n    cov_list = cov_list_temp\n    mix_list = mix_list_temp\n    log_n = log_likelihood(data,mean_list_temp,cov_list_temp,mix_list_temp)\n    error = log_n - log_o\n    print(\'new log likelihood = \',log_n,\'Log likelihood error = \',error)\n    if(error<err):\n        break\n###\nprint(""###############"")\nprint(""# Mixture Vector"")\nprint(""###############"")\nprint(mix_list)\nprint(""###############"")\nprint(""mean of data and estimated mean"")\nprint(""###############"")\nprint(mean_list)\nprint(Mean)\nprint(""###############"")\nprint(""Covariance of data and estimated Covariances"")\nprint(""###############"")\nprint(cov_list)\nprint(Cov)\n###\n'"
Representation_learning/HW1/GMM_same_gnd_truth.py,23,"b'import numpy as np\nfrom math import log\n### Dataset generation\n#\n\n# np.random.seed(42)\n\nprint(""A note to the user\\n This code has a function of generating data on it\'s own \\n from a given number of multivariate_normal functions. \\n This makes the covariance matrices go to Singular sometimes even though they are ensured to be positive semi-definite."")\nprint(""Or sometimes even in case of small number of samples per mode \\n results in covariance matrix collapse"")\nprint(""In SUCH a case, please execute the code again \\n"")\nprint(""\\n"")\n\nprint(""Give the degree for the data to be generated"")\nD = int(input())\nprint(""Give the number of modes the GMM must fit on"")\nNm = int(input())\nprint(""Give the number of samples per mode"")\nNspm = int(input())\nprint(""Give log likelihood error tolerance"")\nerr = float(input())\n\n# D = 5\n# Nm = 2\n# Nspm = 100\nNs = Nspm*Nm\n\nMean = []\nCov = []\ndata = []\nfor i in range(Nm):\n    Mean.append(np.linspace(1,2,D))\n    Cov.append(np.identity(D))\n    for j in range(Nspm):\n        data.append(np.random.multivariate_normal(Mean[i],Cov[i]))\n\ndef MulGaus_pdf(x,mean,cov):\n    # Reshaping and converting for simpler steps\n    x = np.array(x)\n    mean = np.array(mean)\n    cov = np.array(cov)\n    d = len(mean)\n    x = x.reshape((d,1))\n    mean = mean.reshape((d,1))\n    # Actual Functions\n    temp = np.matmul((x-mean).T,np.linalg.inv(cov))\n    temp = np.matmul(temp,(x-mean))\n    num = np.exp(-0.5*temp)\n    den = np.sqrt(np.linalg.det(2*np.pi*cov))\n    pdf = num/den\n    return pdf\n\n\'\'\'\n# Debug for PDF Function\n# print(MulGaus_pdf(Mean[0],Mean[0],Cov[0]))\n# den = np.sqrt(np.linalg.det(2*np.pi*Cov[0]))\n# print(1/den)\n\'\'\'\n\ndef posterior(data,mean_list,cov_list,mix_list,i,k):\n    post = mix_list[k]*MulGaus_pdf(data[i],mean_list[k],cov_list[k])\n    post_sum = 0\n    for w in range(len(mix_list)):\n        post_sum += mix_list[w]*MulGaus_pdf(data[i],mean_list[w],cov_list[w])\n    gamma = post/post_sum\n    return (np.asscalar(post), np.asscalar(post_sum), np.asscalar(gamma))\n\ndef posterior_sum(data,mean_list,cov_list,mix_list,i):\n    _,temp,_ = posterior(data,mean_list,cov_list,mix_list,i,0)\n    return temp\n\n\'\'\'\n# Debug for posterior function\nmix = np.random.uniform(1,10,len(Mean))\nmix = mix/np.sum(mix)\nprint(posterior(data,Mean,Cov,mix,1,0))\nprint(posterior_sum(data,Mean,Cov,mix,1))\n\'\'\'\n\ndef log_likelihood(data,mean_list,cov_list,mix_list):\n    log_like = 0\n    for i in range(len(data)):\n        post_sum = posterior_sum(data,mean_list,cov_list,mix_list,i)\n        log_post_sum = np.log(post_sum)\n        log_like += log_post_sum\n    # print(log_like)\n    return np.asscalar(log_like)\n\n\'\'\'\n# print(log_likelihood(data,Mean,Cov,mix))\n\'\'\'\n\n\n#####################################################################\n### Random initialisation for the mean, covariance and mixing factors\n#####################################################################\n\nmix = np.random.uniform(2,3,len(Mean))\nmix_list = mix/np.sum(mix)\nmean_list = []\ncov_list = []\nfor i in range(Nm):\n    mean_list.append(np.random.uniform(1,5,D))\n    temp1 = np.random.uniform(1,5,(D,D))\n    cov_list.append(np.matmul(temp1,temp1.T))\nprint(\'old log likelihood =\', log_likelihood(data,mean_list,cov_list,mix_list))\n#############################\n### Expeectation Maximization\n#############################\nitera = 0\nwhile(itera<100):\n    mean_list_temp = []\n    cov_list_temp = []\n    mix_list_temp = []\n    log_o = log_likelihood(data,mean_list,cov_list,mix_list)\n    for w in range(Nm):\n        Nk = 0\n        mean_num = 0\n        cov_num = 0\n        for i in range(Ns):\n            _,_,gam = posterior(data,mean_list,cov_list,mix_list,i,w)\n            Nk += gam\n        for x in range(Ns):\n            _,_,gam = posterior(data,mean_list,cov_list,mix_list,x,w)\n            mean_num += gam*data[x]\n            cov_num += gam*np.outer((data[x]-mean_list[w]),(data[x]-mean_list[w]))\n        mean_list_temp.append(mean_num/Nk)\n        cov_list_temp.append(cov_num/Nk)\n        mix_list_temp.append(Nk/Ns)\n    itera += 1\n    mean_list = mean_list_temp\n    cov_list = cov_list_temp\n    mix_list = mix_list_temp\n    log_n = log_likelihood(data,mean_list_temp,cov_list_temp,mix_list_temp)\n    error = log_n - log_o\n    print(\'new log likelihood = \',log_n,\'Log likelihood error = \',error)\n    if(error<err):\n        break\n###\nprint(mix_list)\nprint(""###############"")\nprint(mean_list)\nprint(Mean)\nprint(""###############"")\nprint(cov_list)\nprint(Cov)\n###\n'"
trash/assign_1/rtash.py,11,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\ndef polybasisfunc(x,degree,number_of_samples):\n    phi = np.zeros((number_of_samples,degree+1))\n    deg = list(range(0,degree+1))\n    k = list(range(0,number_of_samples))\n    for i in k:\n        for j in deg:\n            # print(x[i])\n            phi[i][j] = x[i]**j\n    return phi\n\n# Number of training samples\nN = 100\n# Generate equispaced floats in the interval [0, 2pi]\nx = np.linspace(0, 2*np.pi, N)\n# Generate noise\nmean = 0\nstd = 0.05\n# Generate some numbers from the sine function\ny = np.sin(x)\n# Add noise\ny += np.random.normal(mean, std, N)\nprint(""Enter degree of polynomial:"")\ndegree = input()\ndegree = int(degree)\nprint(""Enter testing samples:"")\ntest_samples = input()\ntest_samples = int(test_samples)\nphi = polybasisfunc(x,degree,N)\n# print(phi)\nW = np.zeros((N,N))\nW = np.matmul(np.linalg.pinv(phi),y)\n# print(W)\nx1 = np.linspace(0, 2*np.pi, test_samples)\nphi1 = polybasisfunc(x1,degree,test_samples)\ny1 = np.matmul(phi1,W)\n# print(y1)\n# X = np.reshape(x,(10,1))\n# y1 = np.matmul(X,W)\n# y1 = np.polynomial.polynomial(W)\n# print(y1)\n# error = (y1-y)\n# print(error)\nplt.plot(x,y,\'*\')\nplt.plot(x1,y1,\'.\')\nplt.show()\n'"
Representation_learning/HW1/Trash/GMM.py,23,"b'import numpy as np\nfrom math import log\n### Dataset generation\n#\n\n# np.random.seed(42)\n\nprint(""A note to the user\\n This code has a function of generating data on it\'s own \\n from a given number of multivariate_normal functions. \\n This makes the covariance matrices go to Singular sometimes even though they are ensured to be positive semi-definite."")\n\nprint(""In SUCH a case, please execute the code again \\n"")\nprint(""\\n"")\n\nprint(""Give the degree for the data to be generated"")\nD = int(input())\nprint(""Give the number of modes the data must have"")\nNm = int(input())\nprint(""Give the number of samples per mode"")\nNspm = int(input())\nprint(""Give log likelihood error tolerance"")\nerr = float(input())\n\n# D = 5\n# Nm = 2\n# Nspm = 100\nNs = Nspm*Nm\n\nMean = []\nCov = []\ndata = []\nfor i in range(Nm):\n    Mean.append(np.linspace(1,2,D))\n    Cov.append(np.identity(D)*(i+1))\n    for j in range(Nspm):\n        data.append(np.random.multivariate_normal(Mean[i],Cov[i]))\n\ndef MulGaus_pdf(x,mean,cov):\n    # Reshaping and converting for simpler steps\n    x = np.array(x)\n    mean = np.array(mean)\n    cov = np.array(cov)\n    d = len(mean)\n    x = x.reshape((d,1))\n    mean = mean.reshape((d,1))\n    # Actual Functions\n    temp = np.matmul((x-mean).T,np.linalg.inv(cov))\n    temp = np.matmul(temp,(x-mean))\n    num = np.exp(-0.5*temp)\n    den = np.sqrt(np.linalg.det(2*np.pi*cov))\n    pdf = num/den\n    return pdf\n\n\'\'\'\n# Debug for PDF Function\n# print(MulGaus_pdf(Mean[0],Mean[0],Cov[0]))\n# den = np.sqrt(np.linalg.det(2*np.pi*Cov[0]))\n# print(1/den)\n\'\'\'\n\ndef posterior(data,mean_list,cov_list,mix_list,i,k):\n    post = mix_list[k]*MulGaus_pdf(data[i],mean_list[k],cov_list[k])\n    post_sum = 0\n    for w in range(len(mix_list)):\n        post_sum += mix_list[w]*MulGaus_pdf(data[i],mean_list[w],cov_list[w])\n    gamma = post/post_sum\n    return (np.asscalar(post), np.asscalar(post_sum), np.asscalar(gamma))\n\ndef posterior_sum(data,mean_list,cov_list,mix_list,i):\n    _,temp,_ = posterior(data,mean_list,cov_list,mix_list,i,0)\n    return temp\n\n\'\'\'\n# Debug for posterior function\nmix = np.random.uniform(1,10,len(Mean))\nmix = mix/np.sum(mix)\nprint(posterior(data,Mean,Cov,mix,1,0))\nprint(posterior_sum(data,Mean,Cov,mix,1))\n\'\'\'\n\ndef log_likelihood(data,mean_list,cov_list,mix_list):\n    log_like = 0\n    for i in range(len(data)):\n        post_sum = posterior_sum(data,mean_list,cov_list,mix_list,i)\n        log_post_sum = np.log(post_sum)\n        log_like += log_post_sum\n    # print(log_like)\n    return np.asscalar(log_like)\n\n\'\'\'\n# print(log_likelihood(data,Mean,Cov,mix))\n\'\'\'\n\n\n#####################################################################\n### Random initialisation for the mean, covariance and mixing factors\n#####################################################################\n\nmix = np.random.uniform(2,3,len(Mean))\nmix_list = mix/np.sum(mix)\nmean_list = []\ncov_list = []\nfor i in range(Nm):\n    mean_list.append(np.random.uniform(1,5,D))\n    temp1 = np.random.uniform(1,5,(D,D))\n    cov_list.append(np.matmul(temp1,temp1.T))\nprint(\'old log likelihood =\', log_likelihood(data,mean_list,cov_list,mix_list))\n#############################\n### Expeectation Maximization\n#############################\nitera = 0\nwhile(itera<100):\n    mean_list_temp = []\n    cov_list_temp = []\n    mix_list_temp = []\n    log_o = log_likelihood(data,mean_list,cov_list,mix_list)\n    for w in range(Nm):\n        Nk = 0\n        mean_num = 0\n        cov_num = 0\n        for i in range(Ns):\n            _,_,gam = posterior(data,mean_list,cov_list,mix_list,i,w)\n            Nk += gam\n        for x in range(Ns):\n            _,_,gam = posterior(data,mean_list,cov_list,mix_list,x,w)\n            mean_num += gam*data[x]\n            cov_num += gam*np.outer((data[x]-mean_list[w]),(data[x]-mean_list[w]))\n        mean_list_temp.append(mean_num/Nk)\n        cov_list_temp.append(cov_num/Nk)\n        mix_list_temp.append(Nk/Ns)\n    itera += 1\n    mean_list = mean_list_temp\n    cov_list = cov_list_temp\n    mix_list = mix_list_temp\n    log_n = log_likelihood(data,mean_list_temp,cov_list_temp,mix_list_temp)\n    error = log_n - log_o\n    print(\'new log likelihood = \',log_n,\'Log likelihood error = \',error)\n    if(error<err):\n        break\n###\nprint(mix_list)\nprint(""###############"")\nprint(mean_list)\nprint(Mean)\nprint(""###############"")\nprint(cov_list)\nprint(Cov)\n###\n'"
Representation_learning/HW1/Trash/GMM_Final.py,15,"b'# In[238]:\nimport numpy as np\n\n###\nprint(""For the sake of verification, data was generated from a Gaussian Mixture"")\nprint(""Give degree of data for generation (in positive integers)"")\ndeg = int(input())\nprint(""Number of modes for generation"")\nNm = int(input())\nprint(""Number of data points"")\nNs = int(input())\n\n### Data Generation\nMean = []\nCov = []\ndata = []\nMix = np.random.uniform(10,100,Nm)\nMix = Mix/np.sum(Mix)\nMix = Mix.tolist()\nfor i in range(Nm):\n    Mean.append(np.random.randint(1,10,deg))\n    temp = np.diag(np.random.randint(2,9,deg))\n    Cov.append(temp)\nfor i in range(Ns):\n    temp1 = 0\n    for j in range(Nm):\n        temp1 += Mix[j] * np.random.multivariate_normal(Mean[j],Cov[j],1)\n    data.append(temp1)\ndata = np.array(data)\ndata = data.reshape(Ns,deg)\n\n# In[241]:\ndef gaus_mul_pdf(x,mean,cov,degree):\n    x = x.reshape((degree,1))\n    mean = mean.reshape((degree,1))\n    ph = np.matmul(np.matmul(((x-mean).T),np.linalg.inv(cov)),(x-mean))\n    num = np.exp((-0.5)*ph)\n    den = np.sqrt(np.linalg.det(2*np.pi*cov))\n    temp = num/den\n    temp = np.asscalar(temp)\n    return temp\n\ndef gam(mix,data_matrix,mean_matrix,cov_matrix,Nm,i,k,degree):\n    temp = 0\n    for w in range(Nm):\n        temp +=  mix[w]*gaus_mul_pdf(data_matrix[i],mean_matrix[w],cov_matrix[w],degree)\n    temp1 = mix[k]*gaus_mul_pdf(data_matrix[i],mean_matrix[k],cov_matrix[k],degree)/temp\n    return temp1\n\ndef Nk(mix,data_matrix,mean_matrix,cov_matrix,Nm,k,Ns,degree):\n    temp = 0\n    for w in range(Ns):\n        temp = temp + gam(mix,data_matrix,mean_matrix,cov_matrix,Nm,w,k,degree)\n    return temp\n\ndef log_likelihood(data,mean_matrix,cov_matrix,mix,Nm,Ns,degree):\n    temp = 0\n    temp1 = 0\n    for i in range(Ns):\n        for j in range(Nm):\n            temp += mix[j]*gaus_mul_pdf(data[i],mean_matrix[j],cov_matrix[j],degree)\n        temp1 += np.log(temp)\n    return temp1\n\nmix = []\ncov_matrix = []\nmean_matrix = []\nfor i in range(Nm):\n    mean_matrix.append(np.random.randint(1,10,deg))\n    temp = np.diag(np.random.randint(1,10,deg))\n    cov_matrix.append(temp)\n    mix.append(float(1.0/Nm))\n\nlog_likelihood(data,mean_matrix,cov_matrix,mix,Nm,Ns,deg)\n\nprint(""Give log error delta"")\nerror = float(input())\nitera = 0\nerr_delta = 100.0\nlog_o = log_likelihood(data,mean_matrix,cov_matrix,mix,Nm,Ns,deg)\n\nwhile(itera<100):\n    new_mean_matrix = []\n    new_cov_matrix = []\n    new_mix = []\n#     print(mix)\n    for k in range(Nm):\n        meh = Nk(mix,data,mean_matrix,cov_matrix,Nm,k,Ns,deg)\n        temp1 = 0\n        temp2 = 0\n        temp3 = 0\n        for i in range(Ns):\n            temp1 = np.outer((data[i]-mean_matrix[k]),(data[i]-mean_matrix[k]))\n            temp2 += gam(mix,data,mean_matrix,cov_matrix,Nm,i,k,deg)*temp1\n            temp3 += gam(mix,data,mean_matrix,cov_matrix,Nm,i,k,deg)*data[i]\n        new_cov_matrix.append(temp2/meh)\n        new_mean_matrix.append(temp3/meh)\n        new_mix.append(meh/Ns)\n    mean_matrix = new_mean_matrix\n    cov_matrix = new_cov_matrix\n    mix = new_mix\n    log_n = log_likelihood(data,mean_matrix,cov_matrix,mix,Nm,Ns,deg)\n    err_delta = np.abs(log_o - log_n)\n#     print(""### "",err_delta)\n    print(log_o)\n    itera += 1\n    log_o = log_n\n    if err_delta<error:\n        break\n# In[248]:\nprint(mix)\nprint(Mix)\nprint(mean_matrix)\nprint(Mean)\nprint(cov_matrix)\nprint(Cov)\n'"
Representation_learning/HW1/Trash/GMM_Image.py,23,"b'import numpy as np\nfrom math import log\n### Dataset generation\n#\nprint(""Give the degree for the data to be generated"")\nD = int(input())\nprint(""Give the number of modes the data must have"")\nNm = int(input())\nprint(""Give the number of samples per mode"")\nNspm = int(input())\nprint(""Give log likelihood error tolerance"")\nerr = float(input())\n\n# D = 5\n# Nm = 2\n# Nspm = 100\nNs = Nspm*Nm\n\nMean = []\nCov = []\ndata = []\nfor i in range(Nm):\n    Mean.append(np.random.uniform(1,10,D))\n    temp1 = np.random.uniform(1,5,(D,D))\n    Cov.append(np.matmul(temp1,temp1.T))\n    for j in range(Nspm):\n        data.append(np.random.multivariate_normal(Mean[i],Cov[i]))\n\ndef MulGaus_pdf(x,mean,cov):\n    # Reshaping and converting for simpler steps\n    x = np.array(x)\n    mean = np.array(mean)\n    cov = np.array(cov)\n    d = len(mean)\n    x = x.reshape((d,1))\n    mean = mean.reshape((d,1))\n    # Actual Functions\n    temp = np.matmul((x-mean).T,np.linalg.inv(cov))\n    temp = np.matmul(temp,(x-mean))\n    num = np.exp(-0.5*temp)\n    den = np.sqrt(np.linalg.det(2*np.pi*cov))\n    pdf = num/den\n    return pdf\n\n\'\'\'\n# Debug for PDF Function\n# print(MulGaus_pdf(Mean[0],Mean[0],Cov[0]))\n# den = np.sqrt(np.linalg.det(2*np.pi*Cov[0]))\n# print(1/den)\n\'\'\'\n\ndef posterior(data,mean_list,cov_list,mix_list,i,k):\n    post = mix[k]*MulGaus_pdf(data[i],mean_list[k],cov_list[k])\n    post_sum = 0\n    for w in range(len(mix_list)):\n        post_sum += mix[w]*MulGaus_pdf(data[i],mean_list[w],cov_list[w])\n    gamma = post/post_sum\n    return (np.asscalar(post), np.asscalar(post_sum), np.asscalar(gamma))\n\ndef posterior_sum(data,mean_list,cov_list,mix_list,i):\n    _,temp,_ = posterior(data,mean_list,cov_list,mix_list,i,0)\n    return temp\n\n\'\'\'\n# Debug for posterior function\nmix = np.random.uniform(1,10,len(Mean))\nmix = mix/np.sum(mix)\nprint(posterior(data,Mean,Cov,mix,1,0))\nprint(posterior_sum(data,Mean,Cov,mix,1))\n\'\'\'\n\ndef log_likelihood(data,mean_list,cov_list,mix_list):\n    log_like = 0\n    for i in range(len(data)):\n        post_sum = posterior_sum(data,mean_list,cov_list,mix_list,i)\n        log_post_sum = np.log(post_sum)\n        log_like += log_post_sum\n    # print(log_like)\n    return np.asscalar(log_like)\n\n\'\'\'\n# print(log_likelihood(data,Mean,Cov,mix))\n\'\'\'\n\n\n#####################################################################\n### Random initialisation for the mean, covariance and mixing factors\n#####################################################################\n\nmix = np.random.uniform(1,10,len(Mean))\nmix_list = mix/np.sum(mix)\nmean_list = []\ncov_list = []\nfor i in range(Nm):\n    mean_list.append(np.random.uniform(0,50,D))\n    temp1 = np.random.uniform(2,10,(D,D))\n    cov_list.append(np.matmul(temp1,temp1.T))\nprint(\'old log likelihood =\', log_likelihood(data,mean_list,cov_list,mix_list))\n#############################\n### Expeectation Maximization\n#############################\nitera = 0\nwhile(itera<100):\n    mean_list_temp = []\n    cov_list_temp = []\n    mix_list_temp = []\n    log_o = log_likelihood(data,mean_list,cov_list,mix_list)\n    for w in range(Nm):\n        Nk = 0\n        mean_num = 0\n        cov_num = 0\n        for i in range(Ns):\n            _,_,gam = posterior(data,mean_list,cov_list,mix_list,i,w)\n            Nk += gam\n        for x in range(Ns):\n            _,_,gam = posterior(data,mean_list,cov_list,mix_list,x,w)\n            mean_num += gam*data[x]\n            cov_num += gam*np.outer((data[x]-mean_list[w]),(data[x]-mean_list[w]))\n        mean_list_temp.append(mean_num/Nk)\n        cov_list_temp.append(cov_num/Nk)\n        mix_list_temp.append(Nk/Ns)\n    itera += 1\n    mean_list = mean_list_temp\n    cov_list = cov_list_temp\n    mix_list = mix_list_temp\n    log_n = log_likelihood(data,mean_list_temp,cov_list_temp,mix_list_temp)\n    error = log_n - log_o\n    print(\'new log likelihood = \',log_n,\'Log likelihood error = \',error)\n    if(error<err):\n        break\n###\nprint(mean_list)\nprint(Mean)\nprint(""###############"")\nprint(cov_list)\nprint(Cov)\n###\n'"
trash/assign_3/trash/assign_3_a.py,3,"b""import numpy as np\nimport cvxpy as cvx\nimport pandas as pd\n\ndf_X = pd.read_csv('Xsvm.csv',header=None)\ndf_Y = pd.read_csv('ysvm.csv',header=None)\n\nX = np.array(df_X,dtype=np.float64)\nY = np.array(df_Y,dtype=np.float64)\ntemp = np.hstack((X,Y))\n"""
trash/assign_3/trash/assign_3_b.py,8,"b""\n# coding: utf-8\n\n# In[3]:\n\n\nimport numpy as np\nimport cvxpy as cp\nimport pandas as pd\n\ndf_X = pd.read_csv('Xsvm.csv',header=None)\ndf_Y = pd.read_csv('ysvm.csv',header=None)\nX = np.array(df_X,dtype=np.float64)\nY = np.array(df_Y,dtype=np.float64)\nprint(X.shape,Y.shape)\n\n\n# In[4]:\n\n\n# Convex Optimization\na = cp.Variable(len(Y))\nR1 = cp.matmul(cp.diag(a),Y)\nR2 = cp.matmul(X.T,R1)\nR4 = cp.norm(R2)**2\nR4.shape\n\n\n# In[5]:\n\n\nP1 = cp.sum(a)\nConst1 = P1 - 0.5*R4\n# Const1 = np.reshape(Const1,(1,))\n\n\n# In[6]:\n\n\nConst2 = cp.matmul(a.T,Y)\nConst3 = [0<=a,Const2 == 0]\nobj = cp.Maximize(Const1)\nprob = cp.Problem(obj, Const3)\nprob.solve(verbose=True)\n\n\n# In[7]:\n\n\nprint(a.value)\n\n\n# In[8]:\n\n\nA = (np.array(a.value)).reshape(500,1)\n\n\n# In[9]:\n\n\nW = np.zeros((2,))\nfor i in range(len(Y)):\n    W += A[i]*Y[i]*(X[i].T)\n    if(A[i]>1e-4):\n        print(i)\n\n\n# In[10]:\n\n\nprint(W)\n\n\n# In[11]:\n\n\nW0 = (1/Y[281]) - np.dot(W,X[281])\nprint(W0)\n\n\n# In[12]:\n\n\nTest = np.array([[2,0.5],[0.8,0.7],[1.58,1.33],[0.008, 0.001]])\n\nfor i in range(len(Test)):\n    est = np.sign(np.dot(W,Test[i])+W0)\n    print(Test[i],est)\n\n\n# In[14]:\n\n\n# Verification\nfrom sklearn import svm\n\nclf = svm.SVC()\nclf.fit(X,Y)\n\n\n# In[15]:\n\n\nclf.predict(Test)\n\n"""
trash/assign_3/trash/plot.py,19,"b'import matplotlib.pyplot as plt\nimport numpy as np\n\n# Parameters\nprint(""Give Number of Hidden Layer nodes :"")\nM = int(input())\nprint(""Give number of training samples per bit pair:"")\nn = int(input())\nprint(""Give std.dev for noise :"")\nnoise = float(input())\nprint(""Give number of Epochs :"")\nepochs = int(input())\nprint(""Give learning rate :"")\nlr = float(input())\nprint(""Give the operation you need to train (Enter text in CAPS):"")\nprint(""1. XOR"")\nprint(""2. OR"")\nprint(""3. AND"")\noption = input()\n# Generate Datasets\n\nInput = np.array([[0,0],[0,1],[1,0],[1,1]])\nif option==\'XOR\':\n    Output = np.array([[0],[1],[1],[0]])\nif option==\'OR\':\n    Output = np.array([[0],[1],[1],[1]])\nif option==\'AND\':\n    Output = np.array([[0],[0],[0],[1]])\n\n\nX = []\nY = []\n\nfor i in range(len(Input)):\n    for j in range(n):\n        X.append(Input[i]+np.random.normal(0,noise,(1,2)))\n        Y.append(Output[i]+np.random.normal(0,noise))\n\nX = np.array(X)\nX = X.reshape((len(Y),2))\nY = np.array(Y)\n\n# defining functions\n\ndef sigm(x):\n    return 1/(1+np.exp(-x))\n\ndef diff_sigm(x):\n    return (sigm(x)-sigm(x)**2)\n\ndef layer(x,W,b):\n    return (np.matmul(W.T,x.reshape(len(x),1)) + b)\n\ndef sq_err(y,Y):\n    return (y-Y)**2\n# Initializing weights\n\nW1 = np.random.normal(0,1,(2,M))\nBi1 = np.random.normal(0,1,(M,1))\nW2 = np.random.normal(0,1,(M,1))\nBi2 = np.random.normal(0,1,(1,1))\n\n\n# In[2]:\n\n\n# training\nloss_plot = []\nfor i in range(epochs):\n    w1 = np.zeros(W1.shape)\n    b1 = np.zeros(Bi1.shape)\n    w2 = np.zeros(W2.shape)\n    b2 = np.zeros(Bi2.shape)\n    for j in range(len(Y)):\n        #forward path\n        out1 = layer(X[j],W1,Bi1)\n#         print(out1)\n        z = sigm(out1)\n#         print(z)\n        out2 = layer(z,W2,Bi2)\n        y = sigm(out2)\n#         print(y)\n        #backpropagation\n        b2 += 2*(y-Y[j])*diff_sigm(out2)\n        w2 += 2*(y-Y[j])*diff_sigm(out2)*z\n        loss = sq_err(y,Y[j])\n        for k in range(M):\n            b1[k] += (2*(y-Y[j])*diff_sigm(out2)*diff_sigm(out1[k])*W2[k]).reshape(1,)\n            w1[:,k] += (2*(y-Y[j])*diff_sigm(out2)*diff_sigm(out1[k])*W2[k]*X[j]).reshape(2,)\n    print(loss)\n    loss_plot.append(loss)\n    W1 -= lr*w1\n    W2 -= lr*w2\n    Bi1 -= lr*b1\n    Bi2 -= lr*b2\n\nloss_plot = (np.array(loss_plot)).reshape((1,len(loss_plot)))\nplt.plot(loss_plot)\nplt.show()\n'"
