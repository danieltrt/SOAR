file_path,api_count,code
Submission.py,2,"b'from urllib import urlencode\nfrom urllib2 import urlopen\nfrom json import loads, dumps\nfrom collections import OrderedDict\nimport numpy as np\nimport os\n\n\n\nclass Submission():\n\n    def __init__(self, homework, part_names, srcs, output):\n        self.__homework = homework\n        self.__part_names = part_names\n        self.__srcs = srcs\n        self.__output = output\n        self.__submit_url = \'https://www-origin.coursera.org/api/onDemandProgrammingImmediateFormSubmissions.v1\'\n        self.__login = None\n        self.__password = None\n\n    def submit(self):\n        print \'==\\n== Submitting Solutions | Programming Exercise %s\\n==\' % self.__homework\n        self.login_prompt()\n\n        parts = OrderedDict()\n        for part_id, _ in enumerate(self.__srcs,1):\n            parts[str(part_id)] = {\'output\': self.__output(part_id)}\n\n        result, response = self.request(parts)\n        response = loads(response)\n        try:\n            print response[\'errorMessage\']\n            return\n        except:\n            pass\n        print \'==\'\n        print \'== %43s | %9s | %-s\' % (\'Part Name\', \'Score\', \'Feedback\')\n        print \'== %43s | %9s | %-s\' % (\'---------\', \'-----\', \'--------\')\n        \n\n        for part in parts:\n            partFeedback = response[\'partFeedbacks\'][part]\n            partEvaluation = response[\'partEvaluations\'][part]\n            score = \'%d / %3d\' % (partEvaluation[\'score\'], partEvaluation[\'maxScore\'])\n            print \'== %43s | %9s | %-s\' % (self.__part_names[int(part)-1], score, partFeedback)\n\n        evaluation = response[\'evaluation\']\n    \n\n        totalScore = \'%d / %d\' % (evaluation[\'score\'], evaluation[\'maxScore\'])\n        print \'==                                   --------------------------------\'\n        print \'== %43s | %9s | %-s\\n\' % (\' \', totalScore, \' \')\n        print \'==\'\n\n        if not os.path.isfile(\'token.txt\'):\n            with open(\'token.txt\', \'w\') as f:\n                f.write(self.__login + \'\\n\')\n                f.writelines(self.__password)\n\n\n    def login_prompt(self):\n        try:\n            with open(\'token.txt\', \'r\') as f:\n                self.__login = f.readline().strip()\n                self.__password = f.readline().strip()\n        except IOError:\n            pass\n\n        if self.__login is not None and self.__password is not None:\n            reenter = raw_input(\'Use token from last successful submission (%s)? (Y/n): \' % self.__login)\n\n            if reenter == \'\' or reenter[0] == \'Y\' or reenter[0] == \'y\':\n                return\n\n        if os.path.isfile(\'token.txt\'):\n            os.remove(\'token.txt\')\n        self.__login = raw_input(\'Login (email address): \')\n        self.__password = raw_input(\'Token: \')\n\n    def request(self, parts):\n\n        params = {\n            \'assignmentSlug\': self.__homework,\n            \'secret\': self.__password,\n            \'parts\': parts,\n            \'submitterEmail\': self.__login}\n\n        params = urlencode({\'jsonBody\': dumps(params)})\n        f = urlopen(self.__submit_url, params)\n        try:\n            return 0, f.read()\n        finally:\n            f.close()\n\ndef sprintf(fmt, arg):\n    ""emulates (part of) Octave sprintf function""\n    if isinstance(arg, tuple):\n        # for multiple return values, only use the first one\n        arg = arg[0]\n\n    if isinstance(arg, (np.ndarray, list)):\n        # concatenates all elements, column by column\n        return \' \'.join(fmt % e for e in np.asarray(arg).ravel(\'F\'))\n    else:\n        return fmt % arg'"
__init__.py,0,b''
show.py,0,"b'from matplotlib import use\nuse(\'TkAgg\')\nimport matplotlib.pyplot as plt\n\ndef show():\n    wm = plt.get_current_fig_manager()\n    wm.window.wm_geometry(""+0+0"")\n    plt.show(block=False)\n    wm.window.attributes(\'-topmost\', 1)\n'"
ex1/__init__.py,0,b''
ex1/computeCost.py,4,"b'import numpy as np\nfrom numpy import ones\n\n# import pandas as pd \n# df = pd.read_csv(\'ex1data1.txt\',names=[\'X\',\'y\'])\n\n# y = df[\'y\']\n# m = y.size\n\n\n# X = df.as_matrix(columns=[\'X\'])\n# X = np.append(ones((m,1)),X,axis=1)\n# theta = np.zeros(2)\n\ndef computeCost(X, y, theta):\n\t""""""\n\t   computes the cost of using theta as the parameter for linear \n\t   regression to fit the data points in X and y\n\t""""""\n\tm = y.size\n\tJ = 0\n\n\t# ====================== YOUR CODE HERE ======================\n\t# Instructions: Compute the cost of a particular choice of theta\n\t#               You should set J to the cost.\n\th = np.dot(X,theta)\n\tsq_error = np.sum(np.square(h - y))\t\n\tJ =  (sq_error) / (2 * m) \n# =========================================================================\n\n\treturn J\n\n# computeCost(X, y, theta)'"
ex1/computeCostMulti.py,2,"b'import numpy as np\n\n\ndef computeCostMulti(X, y, theta):\n    """"""\n     Compute cost for linear regression with multiple variables\n       J = computeCost(X, y, theta) computes the cost of using theta as the\n       parameter for linear regression to fit the data points in X and y\n    """"""\n    m = y.size\n    J = 0\n# ====================== YOUR CODE HERE ======================\n# Instructions: Compute the cost of a particular choice of theta\n#               You should set J to the cost.\n\n    h = np.dot(X, theta)\n    sq_error = np.sum(np.square(h - y))\n    J = (sq_error) / (2 * m)\n# =========================================================================\n\n    return J\n'"
ex1/ex1.py,14,"b'from matplotlib import use, cm\nuse(\'TkAgg\')\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import axes3d\nfrom sklearn import linear_model\n\nfrom gradientDescent import gradientDescent\nfrom computeCost import computeCost\nfrom warmUpExercise import warmUpExercise\nfrom plotData import plotData\nfrom show import show\n\n## Machine Learning Online Class - Exercise 1: Linear Regression\n\n#  Instructions\n#  ------------\n#\n#  This file contains code that helps you get started on the\n#  linear exercise. You will need to complete the following modules\n#  in this exericse:\n#\n#     warmUpExercise.py\n#     plotData.py\n#     gradientDescent.py\n#     computeCost.py\n#     gradientDescentMulti.py\n#     computeCostMulti.py\n#     featureNormalize.py\n#     normalEqn.py\n#\n#  For this exercise, you will not need to change any code in this file,\n#  or any other files other than those mentioned above.\n#\n# x refers to the population size in 10,000s\n# y refers to the profit in $10,000s\n\n# ==================== Part 1: Basic Function ====================\n# Complete warmUpExercise.py\nprint \'Running warmUpExercise ...\'\nprint \'5x5 Identity Matrix:\'\nwarmup = warmUpExercise()\nprint warmup\nraw_input(""Program paused. Press Enter to continue..."")\n\n# ======================= Part 2: Plotting =======================\ndata = np.loadtxt(\'ex1data1.txt\', delimiter=\',\')\nm = data.shape[0]\nX = np.vstack(zip(np.ones(m),data[:,0]))\ny = data[:, 1]\n\n# Plot Data\n# Note: You have to complete the code in plotData.py\nprint \'Plotting Data ...\'\nplotData(data)\nshow()\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n# =================== Part 3: Gradient descent ===================\nprint \'Running Gradient Descent ...\'\ntheta = np.zeros(2)\n\n# compute and display initial cost\nJ = computeCost(X, y, theta)\nprint \'cost: %0.4f \' % J\n\n# Some gradient descent settings\niterations = 1500\nalpha = 0.01\n\n# run gradient descent\ntheta, J_history = gradientDescent(X, y, theta, alpha, iterations)\n\n# print theta to screen\nprint \'Theta found by gradient descent: \'\nprint \'%s %s \\n\' % (theta[0], theta[1])\n\n# Plot the linear fit\nplt.figure()\nplotData(data)\nplt.plot(X[:, 1], X.dot(theta), \'-\', label=\'Linear regression\')\nplt.legend(loc=\'upper right\', shadow=True, fontsize=\'x-large\', numpoints=1)\nshow()\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n# Predict values for population sizes of 35,000 and 70,000\npredict1 = np.array([1, 3.5]).dot(theta)\npredict2 = np.array([1, 7]).dot(theta)\nprint \'For population = 35,000, we predict a profit of {:.4f}\'.format(predict1*10000)\nprint \'For population = 70,000, we predict a profit of {:.4f}\'.format(predict2*10000)\n\n# ============= Part 4: Visualizing J(theta_0, theta_1) =============\nprint \'Visualizing J(theta_0, theta_1) ...\'\n\n# Grid over which we will calculate J\ntheta0_vals = np.linspace(-10, 10, X.shape[0])\ntheta1_vals = np.linspace(-1, 4, X.shape[0])\n\n# initialize J_vals to a matrix of 0\'s\nJ_vals=np.array(np.zeros(X.shape[0]).T)\n\nfor i in range(theta0_vals.size):\n    col = []\n    for j in range(theta1_vals.size):\n        t = np.array([theta0_vals[i],theta1_vals[j]])\n        col.append(computeCost(X, y, t.T))\n    J_vals=np.column_stack((J_vals,col))\n\n# Because of the way meshgrids work in the surf command, we need to\n# transpose J_vals before calling surf, or else the axes will be flipped\nJ_vals = J_vals[:,1:].T\ntheta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)\n\n# Surface plot\nfig = plt.figure()\nax = fig.gca(projection=\'3d\')\nax.plot_surface(theta0_vals, theta1_vals, J_vals, rstride=8, cstride=8, alpha=0.3,\n                cmap=cm.coolwarm, linewidth=0, antialiased=False)\nax.set_xlabel(r\'$\\theta_0$\')\nax.set_ylabel(r\'$\\theta_1$\')\nax.set_zlabel(r\'J($\\theta$)\')\nshow()\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n# Contour plot\nplt.figure()\n\n# Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\nax = plt.contour(theta0_vals, theta1_vals, J_vals, np.logspace(-2, 3, 20))\nplt.clabel(ax, inline=1, fontsize=10)\nplt.xlabel(r\'$\\theta_0$\')\nplt.ylabel(r\'$\\theta_1$\')\nplt.plot(0.0, 0.0, \'rx\', linewidth=2, markersize=10)\nshow()\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n# =============Use Scikit-learn =============\nregr = linear_model.LinearRegression(fit_intercept=False, normalize=True)\nregr.fit(X, y)\n\nprint \'Theta found by scikit: \'\nprint \'%s %s \\n\' % (regr.coef_[0], regr.coef_[1])\n\npredict1 = np.array([1, 3.5]).dot(regr.coef_)\npredict2 = np.array([1, 7]).dot(regr.coef_)\nprint \'For population = 35,000, we predict a profit of {:.4f}\'.format(predict1*10000)\nprint \'For population = 70,000, we predict a profit of {:.4f}\'.format(predict2*10000)\n\nplt.figure()\nplotData(data)\nplt.plot(X[:, 1],  X.dot(regr.coef_), \'-\', color=\'black\', label=\'Linear regression wit scikit\')\nplt.legend(loc=\'upper right\', shadow=True, fontsize=\'x-large\', numpoints=1)\nshow()\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n\n'"
ex1/ex1_multi.py,8,"b'from matplotlib import use\nuse(\'TkAgg\')\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom gradientDescentMulti import gradientDescentMulti\nfrom normalEqn import normalEqn\nfrom featureNormalize import featureNormalize\nfrom show import show\n# ================ Part 1: Feature Normalization ================\n\nprint \'Loading data ...\'\n\n# Load Data\ndata = np.loadtxt(\'ex1data2.txt\', delimiter=\',\')\nX = data[:, :2]\ny = data[:, 2]\nm = y.T.size\n\n\n# Print out some data points\nprint \'First 10 examples from the dataset:\'\nprint np.column_stack( (X[:10], y[:10]) )\nraw_input(""Program paused. Press Enter to continue..."")\n\n# Scale features and set them to zero mean\nprint \'Normalizing Features ...\'\n\nX, mu, sigma = featureNormalize(X)\nprint \'[mu] [sigma]\'\nprint mu, sigma\n\n# Add intercept term to X\nX = np.concatenate((np.ones((m, 1)), X), axis=1)\n\n\n# ================ Part 2: Gradient Descent ================\n#\n# ====================== YOUR CODE HERE ======================\n# Instructions: We have provided you with the following starter\n#               code that runs gradient descent with a particular\n#               learning rate (alpha).\n#\n#               Your task is to first make sure that your functions -\n#               computeCost and gradientDescent already work with\n#               this starter code and support multiple variables.\n#\n#               After that, try running gradient descent with\n#               different values of alpha and see which one gives\n#               you the best result.\n#\n#               Finally, you should complete the code at the end\n#               to predict the price of a 1650 sq-ft, 3 br house.\n#\n# Hint: By using the \'hold on\' command, you can plot multiple\n#       graphs on the same figure.\n#\n# Hint: At prediction, make sure you do the same feature normalization.\n#\n\nprint \'Running gradient descent ...\'\n\n# Choose some alpha value\nalpha = 0.01\nnum_iters = 400\n\n# Init Theta and Run Gradient Descent \ntheta = np.zeros(3)\ntheta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)\n\n# Plot the convergence graph\nplt.plot(J_history, \'-b\')\nplt.xlabel(\'Number of iterations\')\nplt.ylabel(\'Cost J\')\nshow()\nraw_input(""Program paused. Press Enter to continue..."")\n\n# Display gradient descent\'s result\nprint \'Theta computed from gradient descent: \'\nprint theta\n\n# Estimate the price of a 1650 sq-ft, 3 br house\nprice = np.array([1,3,1650]).dot(theta)\n\nprint \'Predicted price of a 1650 sq-ft, 3 br house\'\nprint \'(using gradient descent): \'\nprint price\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n# ================ Part 3: Normal Equations ================\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: The following code computes the closed form\n#               solution for linear regression using the normal\n#               equations. You should complete the code in\n#               normalEqn.m\n#\n#               After doing so, you should complete this code\n#               to predict the price of a 1650 sq-ft, 3 br house.\n#\n\nprint \'Solving with normal equations...\'\n\n# Load Data\ndata = np.loadtxt(\'ex1data2.txt\', delimiter=\',\')\nX = data[:, :2]\ny = data[:, 2]\nm = y.T.size\n\n# Add intercept term to X\nX = np.concatenate((np.ones((m,1)), X), axis=1)\n\n# Calculate the parameters from the normal equation\ntheta = normalEqn(X, y)\n\n# Display normal equation\'s result\nprint \'Theta computed from the normal equations:\'\nprint \' %s \\n\' % theta\n\n# Estimate the price of a 1650 sq-ft, 3 br house\nprice = np.array([1, 3, 1650]).dot(theta)\n\n# ============================================================\n\nprint ""Predicted price of a 1650 sq-ft, 3 br house ""\nprint \'(using normal equations):\\n $%f\\n\' % price\n\nraw_input(""Program paused. Press Enter to continue..."")\n'"
ex1/featureNormalize.py,5,"b'import numpy as np\n\n\ndef featureNormalize(X):\n    """"""\n       returns a normalized version of X where\n       the mean value of each feature is 0 and the standard deviation\n       is 1. This is often a good preprocessing step to do when\n       working with learning algorithms.\n    """"""\n    # ====================== YOUR CODE HERE ======================\n    # Instructions: First, for each feature dimension, compute the mean\n    #               of the feature and subtract it from the dataset,\n    #               storing the mean value in mu. Next, compute the\n    #               standard deviation of each feature and divide\n    #               each feature by it\'s standard deviation, storing\n    #               the standard deviation in sigma.\n    #\n    #               Note that X is a matrix where each column is a\n    #               feature and each row is an example. You need\n    #               to perform the normalization separately for\n    #               each feature.\n    #\n    # Hint: You might find the \'mean\' and \'std\' functions useful.\n    \n    mu = np.mean(X[:, :1])\n    sigma = np.std(X[:, :1])\n\n    mu1 = np.mean(X[:, 1:])\n    sigma1 = np.std(X[:, 1:])\n\n    x_ = (X[:, :1] - mu) / sigma\n    x1_ = (X[:, 1:] - mu1) / sigma1\n\n    X_norm = np.append(x_, x1_, axis=1)\n    return X_norm\n\n# ============================================================\n\n    return X_norm, mu, sigma\n'"
ex1/gradientDescent.py,4,"b'from computeCost import computeCost\nimport numpy as np\nimport pandas as pd\ndf = pd.read_csv(\'ex1data1.txt\', names=[\'X\', \'y\'])\n\ny = df[\'y\']\nm = y.size\n\n\nX = df.as_matrix(columns=[\'X\'])\nX = np.append(np.ones((m, 1)), X, axis=1)\ntheta = np.zeros(2)\nalpha = 0.01\nnum_iters = 1500\n\n\ndef gradientDescent(X, y, theta, alpha, num_iters):\n    """"""\n     Performs gradient descent to learn theta\n       theta = gradientDescent(x, y, theta, alpha, num_iters) updates theta by\n       taking num_iters gradient steps with learning rate alpha\n    """"""\n\n    # Initialize some useful values\n    J_history = []\n    m = y.size  # number of training examples\n\n    for i in range(num_iters):\n        #   ====================== YOUR CODE HERE ======================\n        # Instructions: Perform a single gradient step on the parameter vector\n        #               theta.\n        #\n        # Hint: While debugging, it can be useful to print out the values\n        #       of the cost function (computeCost) and gradient here.\n        #\n        h = np.dot(X, theta)\n        theta = theta - (alpha / m) * np.dot(X.T, (h - y))\n\n        # ============================================================\n\n        # Save the cost J in every iteration\n        J_history.append(computeCost(X, y, theta))\n\n    return theta, J_history\n\ngradientDescent(X, y, theta, alpha, num_iters)\n'"
ex1/gradientDescentMulti.py,2,"b'from computeCostMulti import computeCostMulti\nimport numpy as np\n\n\ndef gradientDescentMulti(X, y, theta, alpha, num_iters):\n    """"""\n     Performs gradient descent to learn theta\n       theta = gradientDescent(x, y, theta, alpha, num_iters) updates theta by\n       taking num_iters gradient steps with learning rate alpha\n    """"""\n\n    # Initialize some useful values\n    J_history = []\n    m = y.size  # number of training examples\n\n    for i in range(num_iters):\n        #   ====================== YOUR CODE HERE ======================\n        # Instructions: Perform a single gradient step on the parameter vector\n        #               theta.\n        #\n        # Hint: While debugging, it can be useful to print out the values\n        #       of the cost function (computeCost) and gradient here.\n        #\n        h = np.dot(X, theta)\n        theta = theta - ((alpha / m) * (np.dot(X.T, (h - y))))\n        # Save the cost J in every iteration\n        J_history.append(computeCostMulti(X, y, theta))\n\n        # ============================================================\n\n        # Save the cost J in every iteration\n        J_history.append(computeCostMulti(X, y, theta))\n\n        # ============================================================\n\n    return theta, J_history\n'"
ex1/normalEqn.py,2,"b'import numpy as np\nimport pandas as pd\n\n\ndf = pd.read_csv(\'ex1data1.txt\', names=[\'X\', \'y\'])\ny = df[\'y\']\nm = y.size\n\n\nX = df.as_matrix(columns=[\'X\'])\nX = np.append(np.ones((m, 1)), X, axis=1)\n\n\ndef normalEqn(X, y):\n\t"""""" Computes the closed-form solution to linear regression\n\t   normalEqn(X,y) computes the closed-form solution to linear\n\t   regression using the normal equations.\n\t""""""\n\n\ttheta = 0\n# ====================== YOUR CODE HERE ======================\n# Instructions: Complete the code to compute the closed form solution\n#               to linear regression and put the result in theta.\n#\n\n# ---------------------- Sample Solution ----------------------\n\t\n\ttheta = np.matmul((np.matmul(np.linalg.inv(np.matmul((X.T), X)), X.T)), y)\n\t\n\n\n\n# -------------------------------------------------------------\n\n\treturn theta\n\n# ============================================================\nprint (normalEqn(X, y))\n'"
ex1/plotData.py,1,"b'import matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.genfromtxt(\'ex1data1.txt\', delimiter=\',\')\n\ndef plotData(data):\n    """"""\n    plots the data points and gives the figure axes labels of\n    population and profit.\n    """"""\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: Plot the training data into a figure using the\n#               ""figure"" and ""plot"" commands. Set the axes labels using\n#               the ""xlabel"" and ""ylabel"" commands. Assume the\n#               population and revenue data have been passed in\n#               as the x and y arguments of this function.\n#\n# Hint: You can use the \'rx\' option with plot to have the markers\n#       appear as red crosses. Furthermore, you can make the\n#       markers larger by using plot(..., \'rx\', \'MarkerSize\', 10);\n\n    # print data\n    row1 = data[:,:1]\n    row2 = data[:,1:]\n    plt.scatter(row1,row2,c=\'b\')\n    # plt.show()\t# open a new figure window\n\n# ============================================================\n# plotData(data)'"
ex1/submit.py,8,"b'import numpy as np\n\nfrom Submission import Submission\nfrom Submission import sprintf\n\n__all__ = [\'submit\']\n\nhomework = \'linear-regression\'\n\npart_names = [\n    \'Warm up exercise\',\n    \'Computing Cost (for one variable)\',\n    \'Gradient Descent (for one variable)\',\n    \'Feature Normalization\',\n    \'Computing Cost (for multiple variables)\',\n    \'Gradient Descent (for multiple variables)\',\n    \'Normal Equations\',\n    ]\n\nsrcs = [\n    \'warmUpExercise.py\',\n    \'computeCost.py\',\n    \'gradientDescent.py\',\n    \'featureNormalize.py\',\n    \'computeCostMulti.py\',\n    \'gradientDescentMulti.py\',\n    \'normalEqn.py\',\n    ]\n\n\ndef output(part_id):\n    X1 = np.column_stack((np.ones(20), np.exp(1) + np.exp(2) * np.linspace(0.1, 2, 20)))\n    Y1 = X1[:,1] + np.sin(X1[:,0]) + np.cos(X1[:,1])\n    X2 = np.column_stack((X1, X1[:,1]**0.5, X1[:,1]**0.25))\n    Y2 = np.power(Y1, 0.5) + Y1\n\n    fname = srcs[part_id-1].rsplit(\'.\',1)[0]\n    mod = __import__(fname, fromlist=[fname], level=1)\n    func = getattr(mod, fname)\n\n    if part_id == 1:\n        return sprintf(\'%0.5f \', func())\n    elif part_id == 2:\n        return sprintf(\'%0.5f \', func(X1, Y1, np.array([0.5, -0.5])))\n    elif part_id == 3:\n        return sprintf(\'%0.5f \', func(X1, Y1, np.array([0.5, -0.5]), 0.01, 10))\n    elif part_id == 4:\n        return sprintf(\'%0.5f \', func(X2[:,1:4]))\n    elif part_id == 5:\n        return sprintf(\'%0.5f \', func(X2, Y2, np.array([0.1, 0.2, 0.3, 0.4])))\n    elif part_id == 6:\n        return sprintf(\'%0.5f \', func(X2, Y2, np.array([-0.1, -0.2, -0.3, -0.4]), 0.01, 10))\n    elif part_id == 7:\n        return sprintf(\'%0.5f \', func(X2, Y2))\n\ns = Submission(homework, part_names, srcs, output)\ntry:\n    s.submit()\nexcept Exception as ex:\n    template = ""An exception of type {0} occured. Messsage:\\n{1!r}""\n    message = template.format(type(ex).__name__, ex.args)\n    print message\n'"
ex1/warmUpExercise.py,0,"b'from numpy import eye\n\n\ndef warmUpExercise():\n    """""" an example function that returns the 5x5 identity matrix\n   """"""\n\n# ============= YOUR CODE HERE ==============\n# Instructions: Return the 5x5 identity matrix\n#               In octave, we return values by defining which variables\n#               represent the return values (at the top of the file)\n#               and then set them accordingly.\n\n    return eye(5)\n\n# ===========================================\n'"
ex2/__init__.py,0,b''
ex2/costFunction.py,7,"b'from numpy import log, matmul, ones, asmatrix\nfrom sigmoid import sigmoid\nimport numpy as np\n\n# data = np.loadtxt(\'ex2data1.txt\', delimiter=\',\')\n# X = asmatrix(data[:, 0:2])\n# y = (asmatrix(data[:, 2])).T\n# (m, n) = X.shape\n# X = np.append(ones((m, 1)), X, axis=1)\n# theta = asmatrix(np.zeros(n + 1))\n\ndef costFunction(theta, X, y):\n\t"""""" computes the cost of using theta as the\n\tparameter for logistic regression and the\n\tgradient of the cost w.r.t. to the parameters.""""""\n\n\t# Initialize some useful values\n\tm = y.size  # number of training examples\n\n\t\n\t# ====================== YOUR CODE HERE ======================\n\t# Instructions: Compute the cost of a particular choice of theta.\n\t#               You should set J to the cost.\n\t#               Compute the partial derivatives and set grad to the partial\n\t#               derivatives of the cost w.r.t. each parameter in theta\n\t#\n\th = sigmoid(np.dot( X,theta.T))\n    \n\ta = np.multiply(y , np.log(h))\n\tb = np.multiply((1 - y) , log(1 - h))\n\n\tJ = -1 *(1./m ) * ((a+b).sum())\n\t# J = (1 / m) * (np.sum(np.transpose(a + b)))\n\t# Note: grad should have the same dimensions as theta\n    \n\treturn J\n\t\n\n# print costFunction(theta, X, y)\n'"
ex2/costFunctionReg.py,1,"b'from costFunction import costFunction\nimport numpy as np\n\n\n\ndef costFunctionReg(theta, X, y, Lambda):\n\t""""""\n\tCompute cost and gradient for logistic regression with regularization\n\n\tcomputes the cost of using theta as the parameter for regularized logistic regression and the\n\tgradient of the cost w.r.t. to the parameters.\n\t""""""\n\t# Initialize some useful values\n\tm = len(y)   # number of training examples\n\tJ = 0\n\t# ====================== YOUR CODE HERE ======================\n\t# Instructions: Compute the cost of a particular choice of theta.\n\t#               You should set J to the cost.\n\t#               Compute the partial derivatives and set grad to the partial\n\t#               derivatives of the cost w.r.t. each parameter in theta\n\tth = theta[1:]\n\tJ = costFunction(theta,X,y) + ( Lambda / (2 * m) )* (np.sum((np.power(th,2))))\n\t# =============================================================\n\n\treturn J\n\n# costFunctionReg(theta, X, y, Lambda)'"
ex2/ex2.py,5,"b'# Logistic Regression\nfrom matplotlib import use\n\nuse(\'TkAgg\')\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\n\nfrom costFunction import costFunction\nfrom gradientFunction import gradientFunction\nfrom sigmoid import sigmoid\nfrom predict import predict\nfrom show import show\n\n## Machine Learning Online Class - Exercise 2: Logistic Regression\n#\n#  Instructions\n#  ------------\n#\n#  This file contains code that helps you get started on the second part\n#  of the exercise which covers regularization with logistic regression.\n#\n#  You will need to complete the following functions in this exericse:\n#\n#     sigmoid.py\n#     costFunction.py\n#     gradientFunction.py\n#     predict.py\n#     costFunctionReg.py\n#     gradientFunctionReg.py\n#     n.b. This files differ in number from the Octave version of ex2.\n#          This is due to the scipy optimization taking only scalar\n#          functions where fmiunc in Octave takes functions returning\n#          multiple values.\n#\n#  For this exercise, you will not need to change any code in this file,\n#  or any other files other than those mentioned above.\n#\n\nfrom ml import plotData, plotDecisionBoundary\n# Load Data\n#  The first two columns contains the exam scores and the third column\n#  contains the label.\n\ndata = np.loadtxt(\'ex2data1.txt\', delimiter=\',\')\nX = data[:, 0:2]\ny = data[:, 2]\n\n# ==================== Part 1: Plotting ====================\n\nprint \'Plotting data with + indicating (y = 1) examples and o indicating (y = 0) examples.\'\n\nplotData(X, y)\nplt.legend([\'Admitted\', \'Not admitted\'], loc=\'upper right\', shadow=True, fontsize=\'x-large\', numpoints=1)\n\nplt.xlabel(\'Exam 1 score\')\nplt.ylabel(\'Exam 2 score\')\nshow()\nraw_input(""Program paused. Press Enter to continue..."")\n\n\n# # ============ Part 2: Compute Cost and Gradient ============\n# #  Setup the data matrix appropriately, and add ones for the intercept term\nm, n = X.shape\n\n# Add intercept term to x and X_test\nX = np.concatenate((np.ones((m, 1)), X), axis=1)\n\n# Initialize fitting parameters\ninitial_theta = np.zeros(n + 1)\n\n# Compute and display initial cost and gradient\ncost = costFunction(initial_theta, X, y)\n\nprint \'Cost at initial theta (zeros): %f\' % cost\n\ngrad = gradientFunction(initial_theta, X, y)\n\nprint \'Gradient at initial theta (zeros): \' + str(grad)\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n# ============= Part 3: Optimizing using scipy  =============\nres = minimize(costFunction, initial_theta, method=\'TNC\',\n               jac=False, args=(X, y), options={\'gtol\': 1e-3, \'disp\': True, \'maxiter\': 1000})\n\ntheta = res.x\ncost = res.fun\n\n# Print theta to screen\nprint \'Cost at theta found by scipy: %f\' % cost\nprint \'theta:\', [""%0.4f"" % i for i in theta]\n\n# Plot Boundary\nplotDecisionBoundary(theta, X, y)\n\n# Labels and Legend\nplt.legend([\'Admitted\', \'Not admitted\'], loc=\'upper right\', shadow=True, fontsize=\'x-large\', numpoints=1)\nplt.xlabel(\'Exam 1 score\')\nplt.ylabel(\'Exam 2 score\')\nshow()\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n#  ============== Part 4: Predict and Accuracies ==============\n\n#  Predict probability for a student with score 45 on exam 1\n#  and score 85 on exam 2\n\nprob = sigmoid(np.array([1, 45, 85]).dot(theta))\nprint \'For a student with scores 45 and 85, we predict an admission probability of %f\' % prob\n\n# Compute accuracy on our training set\np = predict(theta, X)\nprint y.shape\nacc = 1.0*np.where(p == y)[0].size/len(p) * 100\nprint \'Train Accuracy: %f\' % acc\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n'"
ex2/ex2_reg.py,4,"b'# Logistic Regression\nfrom matplotlib import use\n\nuse(\'TkAgg\')\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\n\nimport pandas as pd\n\nfrom ml import mapFeature, plotData, plotDecisionBoundary\nfrom show import show\nfrom costFunctionReg import costFunctionReg\nfrom gradientFunctionReg import gradientFunctionReg\nfrom sigmoid import sigmoid\n\n\ndef optimize(Lambda):\n\n    result = minimize(costFunctionReg, initial_theta, method=\'L-BFGS-B\',\n               jac=gradientFunctionReg, args=(X.as_matrix(), y, Lambda),\n               options={\'gtol\': 1e-4, \'disp\': False, \'maxiter\': 1000})\n\n    return result\n\n\n# Plot Boundary\ndef plotBoundary(theta, X, y):\n    plotDecisionBoundary(theta, X.values, y.values)\n    plt.title(r\'$\\lambda$ = \' + str(Lambda))\n\n    # Labels and Legend\n    plt.xlabel(\'Microchip Test 1\')\n    plt.ylabel(\'Microchip Test 2\')\n    show()\n\n\n\n# Initialization\n\n# Load Data\n#  The first two columns contains the X values and the third column\n#  contains the label (y).\n\ndata = pd.read_csv(\'ex2data2.txt\', header=None, names=[1,2,3])\nX = data[[1, 2]]\ny = data[[3]]\n\nplotData(X.values, y.values)\n\n# Labels and Legend\nplt.xlabel(\'Microchip Test 1\')\nplt.ylabel(\'Microchip Test 2\')\nshow()\nraw_input(""Program paused. Press Enter to continue..."")\n\n\n# =========== Part 1: Regularized Logistic Regression ============\n\n# Add Polynomial Features\n\n# Note that mapFeature also adds a column of ones for us, so the intercept\n# term is handled\nX = X.apply(mapFeature, axis=1)\n\n# Initialize fitting parameters\ninitial_theta = np.zeros(X.shape[1])\n\n# Set regularization parameter lambda to 1\nLambda = 0.0\n\n# Compute and display initial cost and gradient for regularized logistic\n# regression\ncost = costFunctionReg(initial_theta, X, y, Lambda)\n\nprint \'Cost at initial theta (zeros): %f\' % cost\n\n# ============= Part 2: Regularization and Accuracies =============\n\n# Optimize and plot boundary\n\nLambda = 1.0\nresult = optimize(Lambda)\ntheta = result.x\ncost = result.fun\n\n# Print to screen\nprint \'lambda = \' + str(Lambda)\nprint \'Cost at theta found by scipy: %f\' % cost\nprint \'theta:\', [""%0.4f"" % i for i in theta]\n\nraw_input(""Program paused. Press Enter to continue..."")\n\nplotBoundary(theta, X, y)\n\n# Compute accuracy on our training set\np = np.round(sigmoid(X.dot(theta)))\nacc = np.mean(np.where(p == y.T,1,0)) * 100\nprint \'Train Accuracy: %f\' % acc\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n# ============= Part 3: Optional Exercises =============\n\n\nfor Lambda in np.arange(0.0,10.1,1.0):\n    result = optimize(Lambda)\n    theta = result.x\n    print \'lambda = \' + str(Lambda)\n    print \'theta:\', [""%0.4f"" % i for i in theta]\n    plotBoundary(theta, X, y)\nraw_input(""Program paused. Press Enter to continue..."")\n'"
ex2/gradientFunction.py,5,"b'from sigmoid import sigmoid\nfrom numpy import squeeze, asarray\n\nfrom numpy import log, matmul, ones, asmatrix\nfrom sigmoid import sigmoid\nimport numpy as np\n\ndata = np.loadtxt(\'ex2data1.txt\', delimiter=\',\')\nX = asmatrix(data[:, 0:2])\ny = (asmatrix(data[:, 2])).T\n(m, n) = X.shape\nX = np.append(ones((m, 1)), X, axis=1)\ntheta = asmatrix(np.zeros(n + 1))\n\n\ndef gradientFunction(theta, X, y):\n\t""""""\n\tCompute cost and gradient for logistic regression with regularization\n\n\tcomputes the cost of using theta as the parameter for regularized logistic regression and the\n\tgradient of the cost w.r.t. to the parameters.\n\t""""""\n\n\tm = len(y)   # number of training examples\n\tgrad = 0\n\t# ====================== YOUR CODE HERE ======================\n\t# Instructions: Compute the gradient of a particular choice of theta.\n\t#               Compute the partial derivatives and set grad to the partial\n\t#               derivatives of the cost w.r.t. each parameter in theta\n\th = sigmoid(np.dot( X,theta.T))\n\tgrad = (1. / m) * np.dot((X.T) , (h - y))\n\t# =============================================================\n\n\treturn grad.T\n\n\n# print (gradientFunction(theta, X, y))'"
ex2/gradientFunctionReg.py,0,"b'from numpy import asfortranarray, squeeze, asarray\n\nfrom gradientFunction import gradientFunction\n\n\ndef gradientFunctionReg(theta, X, y, Lambda):\n\t""""""\n\tCompute cost and gradient for logistic regression with regularization\n\n\tcomputes the cost of using theta as the parameter for regularized logistic regression and the\n\tgradient of the cost w.r.t. to the parameters.\n\t""""""\n\tm = len(y)   # number of training examples\n\tgrad = 0\n\t# ====================== YOUR CODE HERE ======================\n\t# Instructions: Compute the gradient of a particular choice of theta.\n\t#               Compute the partial derivatives and set grad to the partial\n\t#               derivatives of the cost w.r.t. each parameter in theta\n\n\n\tgrad_no_regulation = gradientFunction(theta, X, y)\n\tgrad = gradientFunction(theta, X, y) + (Lambda / m) * theta\n\tgrad[0] = grad_no_regulation[0]\n\t# =============================================================\n\n\treturn grad\n'"
ex2/ml.py,6,"b'import numpy as np\nfrom matplotlib import pyplot as plt\nfrom pandas import Series\nfrom mpl_toolkits.mplot3d import axes3d\n\n\ndef plotData(X,y):\n    pos = X[np.where(y==1,True,False).flatten()]\n    neg = X[np.where(y==0,True,False).flatten()]\n    plt.plot(pos[:,0], pos[:,1], \'+\', markersize=7, markeredgecolor=\'black\', markeredgewidth=2)\n    plt.plot(neg[:,0], neg[:,1], \'o\', markersize=7, markeredgecolor=\'black\', markerfacecolor=\'yellow\')\n\ndef plotDecisionBoundary(theta, X, y):\n    """"""\n    Plots the data points X and y into a new figure with the decision boundary defined by theta\n      PLOTDECISIONBOUNDARY(theta, X,y) plots the data points with + for the\n      positive examples and o for the negative examples. X is assumed to be\n      a either\n      1) Mx3 matrix, where the first column is an all-ones column for the\n         intercept.\n      2) MxN, N>3 matrix, where the first column is all-ones\n    """"""\n\n    # Plot Data\n    plt.figure()\n    plotData(X[:,1:], y)\n\n    if X.shape[1] <= 3:\n        # Only need 2 points to define a line, so choose two endpoints\n        plot_x = np.array([min(X[:, 2]),  max(X[:, 2])])\n\n        # Calculate the decision boundary line\n        plot_y = (-1./theta[2])*(theta[1]*plot_x + theta[0])\n\n        # Plot, and adjust axes for better viewing\n        plt.plot(plot_x, plot_y)\n\n    else:\n        # Here is the grid range\n        u = np.linspace(-1, 1.5, 50)\n        v = np.linspace(-1, 1.5, 50)\n        z = [\n                np.array([mapFeature2(u[i], v[j]).dot(theta) for i in range(len(u))])\n                for j in range(len(v))\n            ]\n        plt.contour(u,v,z, levels=[0.0])\n\n    # Legend, specific for the exercise\n    # axis([30, 100, 30, 100])\n\ndef mapFeature(X, degree=6):\n    """"""\n    Feature mapping function to polynomial features\n\n    MAPFEATURE(X, degree) maps the two input features\n    to quadratic features used in the regularization exercise.\n\n    Returns a new feature array with more features, comprising of\n    X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..\n    """"""\n    quads = Series([X.iloc[0]**(i-j) * X.iloc[1]**j for i in range(1,degree+1) for j in range(i+1)])\n    return Series([1]).append([X,quads])\n\ndef mapFeature2(X1, X2, degree=6):\n    """"""\n    Feature mapping function to polynomial features\n\n    MAPFEATURE(X, degree) maps the two input features\n    to quadratic features used in the regularization exercise.\n\n    Returns a new feature array with more features, comprising of\n    X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..\n    """"""\n    quads = Series([X1**(i-j) * X2**j for i in range(1,degree+1) for j in range(i+1)])\n    return Series([1]).append([Series(X1), Series(X2), quads])\n'"
ex2/predict.py,4,"b'from numpy import round\nimport numpy as np\nfrom sigmoid import sigmoid\n\n\n# theta = [-25.8142, 0.2111, 0.2070]\n# theta = np.asmatrix(theta)\n# X = [[1, 45, 85],[1, 40 ,50]]\n# X = np.asmatrix(X)\n# print X.shape\n# print theta.shape\n\ndef predict(theta, X):\n\n\t"""""" computes the predictions for X using a threshold at 0.5\n\t(i.e., if sigmoid(theta\'*x) >= 0.5, predict 1)\n\t""""""\n\t\n\t# ====================== YOUR CODE HERE ======================\n\t# Instructions: Complete the following code to make predictions using\n\t#               your learned logistic regression parameters.\n\t#               You should set p to a vector of 0\'s and 1\'s\n\t#\n\t(m,n) = X.shape\n\tp = np.zeros((m,1)) \n\tp = sigmoid(np.dot(X , theta.T)) \n\t# =========================================================================\n\t# print ""hello""\n\treturn [1 if x >= 0.5 else 0 for x in p]\n\t\n\n# print (predict(theta,X))\t\n\n\n\n'"
ex2/sigmoid.py,0,"b'from numpy import exp\n\n\ndef sigmoid(z):\n    """"""computes the sigmoid of z.""""""\n    g = 0\n    # ====================== YOUR CODE HERE ======================\n    # Instructions: Compute the sigmoid of each value of z (z can be a matrix,\n    #               vector or scalar).\n    g = 1 / (1 + exp(-z))\n# =============================================================\n    return g\n'"
ex2/submit.py,9,"b'import numpy as np\n\nfrom Submission import Submission\nfrom Submission import sprintf\n\n__all__ = [\'submit\']\n\nhomework = \'logistic-regression\'\n\npart_names = [\n    \'Sigmoid Function\',\n    \'Logistic Regression Cost\',\n    \'Logistic Regression Gradient\',\n    \'Predict\',\n    \'Regularized Logistic Regression Cost\',\n    \'Regularized Logistic Regression Gradient\',\n    ]\n\nsrcs = [\n    \'sigmoid.py\',\n    \'costFunction.py\',\n    \'gradientFunction.py\',\n    \'predict.py\',\n    \'costFunctionReg.py\',\n    \'gradientFunctionReg.py\',\n    ]\n\n\ndef output(part_id):\n    X = np.column_stack((np.ones(20),\n                          (np.exp(1) * np.sin(np.linspace(1, 20, 20))),\n                          (np.exp(0.5) * np.cos(np.linspace(1, 20, 20)))))\n    Y = np.sin(X[:,0] + X[:,1]) > 0\n\n    fname = srcs[part_id-1].rsplit(\'.\',1)[0]\n    mod = __import__(fname, fromlist=[fname], level=1)\n    func = getattr(mod, fname)\n\n    if part_id == 1:\n        return sprintf(\'%0.5f \', func(X))\n    elif part_id == 2:\n        return sprintf(\'%0.5f \', func(np.array([0.25, 0.5, -0.5]), X, Y))\n    elif part_id == 3:\n        return sprintf(\'%0.5f \', func(np.array([0.25, 0.5, -0.5]), X, Y))\n    elif part_id == 4:\n        return sprintf(\'%0.5f \', func(np.array([0.25, 0.5, -0.5]), X))\n    elif part_id == 5:\n        return sprintf(\'%0.5f \', func(np.array([0.25, 0.5, -0.5]), X, Y, 0.1))\n    elif part_id == 6:\n        return sprintf(\'%0.5f \', func(np.array([0.25, 0.5, -0.5]), X, Y, 0.1))\n\ns = Submission(homework, part_names, srcs, output)\ntry:\n    s.submit()\nexcept Exception as ex:\n    template = ""An exception of type {0} occured. Messsage:\\n{1!r}""\n    message = template.format(type(ex).__name__, ex.args)\n    print message\n'"
ex3/__init__.py,0,b''
ex3/displayData.py,9,"b'import numpy as np\nfrom matplotlib import use\nuse(\'TkAgg\')\nimport matplotlib.pyplot as plt\n\nfrom show import show\n\ndef displayData(X):\n    """"""displays 2D data\n      stored in X in a nice grid. It returns the figure handle h and the\n      displayed array if requested.""""""\n\n# Compute rows, cols\n    m, n = X.shape\n    example_width = round(np.sqrt(n))\n    example_height = (n / example_width)\n\n# Compute number of items to display\n    display_rows = np.floor(np.sqrt(m))\n    display_cols = np.ceil(m / display_rows)\n\n# Between images padding\n    pad = 1\n\n# Setup blank display\n    display_array = - np.ones((pad + display_rows * (example_height + pad),\n                           pad + display_cols * (example_width + pad)))\n\n# Copy each example into a patch on the display array\n    curr_ex = 0\n    for j in np.arange(display_rows):\n        for i in np.arange(display_cols):\n            if curr_ex > m:\n                break\n            # Get the max value of the patch\n            max_val = np.max(np.abs(X[curr_ex, : ]))\n            rows = [pad + j * (example_height + pad) + x for x in np.arange(example_height+1)]\n            cols = [pad + i * (example_width + pad)  + x for x in np.arange(example_width+1)]\n            display_array[min(rows):max(rows), min(cols):max(cols)] = X[curr_ex, :].reshape(example_height, example_width) / max_val\n            curr_ex = curr_ex + 1\n        if curr_ex > m:\n            break\n\n# Display Image\n    display_array = display_array.astype(\'float32\')\n    plt.imshow(display_array.T)\n    plt.set_cmap(\'gray\')\n# Do not show axis\n    plt.axis(\'off\')\n    show()\n\n'"
ex3/ex3.py,2,"b'## Machine Learning Online Class - Exercise 3 | Part 1: One-vs-all\nimport scipy.io\nimport numpy as np\nfrom matplotlib import use\nuse(\'TkAgg\')\n\nfrom oneVsAll import oneVsAll\nfrom predictOneVsAll import predictOneVsAll\nfrom displayData import displayData\n\n#  Instructions\n#  ------------\n# \n#  This file contains code that helps you get started on the\n#  linear exercise. You will need to complete the following functions \n#  in this exericse:\n#\n#     lrCostFunction.m (logistic regression cost function)\n#     oneVsAll.m\n#     predictOneVsAll.m\n#     predict.m\n#\n#  For this exercise, you will not need to change any code in this file,\n#  or any other files other than those mentioned above.\n#\n\n## Setup the parameters you will use for this part of the exercise\ninput_layer_size  = 400  # 20x20 Input Images of Digits\nnum_labels = 10          # 10 labels, from 1 to 10\n                         # (note that we have mapped ""0"" to label 10)\n\n## =========== Part 1: Loading and Visualizing Data =============\n#  We start the exercise by first loading and visualizing the dataset. \n#  You will be working with a dataset that contains handwritten digits.\n#\n\n# Load Training Data\nprint \'Loading and Visualizing Data ...\'\n\ndata = scipy.io.loadmat(\'ex3data1.mat\') # training data stored in arrays X, y\nX = data[\'X\']\ny = data[\'y\']\nm, _ = X.shape\n\n# Randomly select 100 data points to display\nrand_indices = np.random.permutation(range(m))\nsel = X[rand_indices[0:100], :]\n\ndisplayData(sel)\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## ============ Part 2: Vectorize Logistic Regression ============\n#  In this part of the exercise, you will reuse your logistic regression\n#  code from the last exercise. You task here is to make sure that your\n#  regularized logistic regression implementation is vectorized. After\n#  that, you will implement one-vs-all classification for the handwritten\n#  digit dataset.\n#\n\nprint \'Training One-vs-All Logistic Regression...\'\n\nLambda = 0.1\nall_theta = oneVsAll(X, y, num_labels, Lambda)\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n\n## ================ Part 3: Predict for One-Vs-All ================\n#  After ...\npred = predictOneVsAll(all_theta, X)\n\naccuracy = np.mean(np.double(pred == np.squeeze(y))) * 100\nprint \'\\nTraining Set Accuracy: %f\\n\' % accuracy\n\n'"
ex3/ex3_nn.py,6,"b'## Machine Learning Online Class - Exercise 3 | Part 2: Neural Networks\n\n#  Instructions\n#  ------------\n# \n#  This file contains code that helps you get started on the\n#  linear exercise. You will need to complete the following functions \n#  in this exericse:\n#\n#     lrCostFunction.m (logistic regression cost function)\n#     oneVsAll.m\n#     predictOneVsAll.m\n#     predict.m\n#\n#  For this exercise, you will not need to change any code in this file,\n#  or any other files other than those mentioned above.\n#\nfrom matplotlib import use\nuse(\'TkAgg\')\nimport scipy.io\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom displayData import displayData\nfrom predict import predict\n\n## Setup the parameters you will use for this exercise\ninput_layer_size  = 400  # 20x20 Input Images of Digits\nhidden_layer_size = 25   # 25 hidden units\nnum_labels = 10          # 10 labels, from 1 to 10   \n                          # (note that we have mapped ""0"" to label 10)\n\n## =========== Part 1: Loading and Visualizing Data =============\n#  We start the exercise by first loading and visualizing the dataset. \n#  You will be working with a dataset that contains handwritten digits.\n#\n\n# Load Training Data\nprint \'Loading and Visualizing Data ...\'\n\ndata = scipy.io.loadmat(\'ex3data1.mat\')\nX = data[\'X\']\ny = data[\'y\']\nm, _ = X.shape\n\n# Randomly select 100 data points to display\nsel = np.random.permutation(range(m))\nsel = sel[0:100]\n\ndisplayData(X[sel,:])\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## ================ Part 2: Loading Pameters ================\n# In this part of the exercise, we load some pre-initialized \n# neural network parameters.\n\nprint \'Loading Saved Neural Network Parameters ...\'\n\n# Load the weights into variables Theta1 and Theta2\ndata = scipy.io.loadmat(\'ex3weights.mat\')\nTheta1 = data[\'Theta1\']\nTheta2 = data[\'Theta2\']\n\n## ================= Part 3: Implement Predict =================\n#  After training the neural network, we would like to use it to predict\n#  the labels. You will now implement the ""predict"" function to use the\n#  neural network to predict the labels of the training set. This lets\n#  you compute the training set accuracy.\n\npred = predict(Theta1, Theta2, X)\n\nprint \'Training Set Accuracy: %f\\n\', np.mean(np.double(pred == np.squeeze(y))) * 100\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n#  To give you an idea of the network\'s output, you can also run\n#  through the examples one at the a time to see what it is predicting.\n\n#  Randomly permute examples\nrp = np.random.permutation(range(m))\n\nplt.figure()\nfor i in range(m):\n    # Display\n    X2 = X[rp[i],:]\n    print \'Displaying Example Image\'\n    X2 = np.matrix(X[rp[i]])\n    displayData(X2)\n\n    pred = predict(Theta1, Theta2, X2.getA())\n    pred = np.squeeze(pred)\n    print \'Neural Network Prediction: %d (digit %d)\\n\' % (pred, np.mod(pred, 10))\n    \n    raw_input(""Program paused. Press Enter to continue..."")\n    plt.close()\n\n\n'"
ex3/lrCostFunction.py,0,"b'from ex2.costFunctionReg import costFunctionReg\n\ndef lrCostFunction(theta, X, y, Lambda):\n\t""""""computes the cost of using\n\ttheta as the parameter for regularized logistic regression and the\n\tgradient of the cost w.r.t. to the parameters.\n\t""""""\n\n\t# ====================== YOUR CODE HERE ======================\n\t# Instructions: Compute the cost of a particular choice of theta.\n\t#               You should set J to the cost.\n\t#\n\t# Hint: The computation of the cost function and gradients can be\n\t#       efficiently vectorized. For example, consider the computation\n\t#\n\t#           sigmoid(X * theta)\n\t#\n\t#       Each row of the resulting matrix will contain the value of the\n\t#       prediction for that example. You can make use of this to vectorize\n\t#       the cost function and gradient computations. \n\t#\n\tJ = 0\n\n\n\t# =============================================================\n\n\treturn J\n'"
ex3/oneVsAll.py,3,"b'import numpy as np\nfrom scipy.optimize import minimize\n\nfrom lrCostFunction import lrCostFunction\nfrom ex2.gradientFunctionReg import gradientFunctionReg\n\n\ndef oneVsAll(X, y, num_labels, Lambda):\n    """"""trains multiple logistic regression classifiers and returns all\n        the classifiers in a matrix all_theta, where the i-th row of all_theta\n        corresponds to the classifier for label i\n    """"""\n\n# Some useful variables\n    m, n = X.shape\n\n# You need to return the following variables correctly \n    all_theta = np.zeros((num_labels, n + 1))\n\n# Add ones to the X data matrix\n    X = np.column_stack((np.ones((m, 1)), X))\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: You should complete the following code to train num_labels\n#               logistic regression classifiers with regularization\n#               parameter lambda. \n#\n# Hint: theta(:) will return a column vector.\n#\n# Hint: You can use y == c to obtain a vector of 1\'s and 0\'s that tell use \n#       whether the ground truth is true/false for this class.\n#\n# Note: For this assignment, we recommend using fmincg to optimize the cost\n#       function. It is okay to use a for-loop (for c = 1:num_labels) to\n#       loop over the different classes.\n\n    # Set Initial theta\n    initial_theta = np.zeros((n + 1, 1))\n\n    # This function will return theta and the cost\n\n\n\n# =========================================================================\n\n    return all_theta\n\n'"
ex3/predict.py,0,"b'import numpy as np\n\nfrom ex2.sigmoid import sigmoid\n\ndef predict(Theta1, Theta2, X):\n    """""" outputs the predicted label of X given the\n    trained weights of a neural network (Theta1, Theta2)\n    """"""\n\n# Useful values\n    m, _ = X.shape\n    num_labels, _ = Theta2.shape\n    p  = 0\n# ====================== YOUR CODE HERE ======================\n# Instructions: Complete the following code to make predictions using\n#               your learned neural network. You should set p to a \n#               vector containing labels between 1 to num_labels.\n#\n# Hint: The max function might come in useful. In particular, the max\n#       function can also return the index of the max element, for more\n#       information see \'help max\'. If your examples are in rows, then, you\n#       can use max(A, [], 2) to obtain the max for each row.\n#\n\n# =========================================================================\n\n    return p + 1        # add 1 to offset index of maximum in A row\n\n'"
ex3/predictOneVsAll.py,2,"b'import numpy as np\n\nfrom ex2.sigmoid import sigmoid\n\ndef predictOneVsAll(all_theta, X):\n    """"""will return a vector of predictions\n  for each example in the matrix X. Note that X contains the examples in\n  rows. all_theta is a matrix where the i-th row is a trained logistic\n  regression theta vector for the i-th class. You should set p to a vector\n  of values from 1..K (e.g., p = [1 3 1 2] predicts classes 1, 3, 1, 2\n  for 4 examples) """"""\n\n    m = X.shape[0]\n\n    # You need to return the following variables correctly\n    p = np.zeros((m, 1))\n\n    # Add ones to the X data matrix\n    X = np.column_stack((np.ones((m, 1)), X))\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: Complete the following code to make predictions using\n#               your learned logistic regression parameters (one-vs-all).\n#               You should set p to a vector of predictions (from 1 to\n#               num_labels).\n#\n# Hint: This code can be done all vectorized using the max function.\n#       In particular, the max function can also return the index of the \n#       max element, for more information see \'help max\'. If your examples \n#       are in rows, then, you can use max(A, [], 2) to obtain the max \n#       for each row.\n#       \n\n\n# =========================================================================\n\n    return p + 1    # add 1 to offset index of maximum in A row\n'"
ex3/submit.py,11,"b'import numpy as np\n\nfrom Submission import Submission\nfrom Submission import sprintf\nfrom lrCostFunction import lrCostFunction\nfrom oneVsAll import oneVsAll\nfrom predictOneVsAll import predictOneVsAll\nfrom predict import predict\nfrom ex2.gradientFunctionReg import gradientFunctionReg\n\nhomework = \'multi-class-classification-and-neural-networks\'\n\npart_names = [\n  \'Regularized Logistic Regression\',\n  \'One-vs-All Classifier Training\',\n  \'One-vs-All Classifier Prediction\',\n  \'Neural Network Prediction Function\',\n  ]\n\nsrcs = [\n  \'lrCostFunction.py\',\n  \'oneVsAll.py\',\n  \'predictOneVsAll.py\',\n  \'predict.py\',\n  ]\n\n\ndef output(part_id):\n    # Random Test Cases\n    X = np.column_stack((np.ones(20),\n                          (np.exp(1) * np.sin(np.linspace(1, 20, 20))),\n                          (np.exp(0.5) * np.cos(np.linspace(1, 20, 20)))))\n    y = np.sin(X[:,0] + X[:,1]) > 0\n\n    Xm = np.array([[-1,-1],[-1,-2],[-2,-1],[-2,-2],[1,1],[1,2],[2,1],[2,2],[-1,1],\n          [-1,2],[-2,1],[-2,2],[1,-1],[1,-2],[-2,-1],[-2,-2]]).reshape((16,2))\n    ym = np.array([1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4]).reshape(16,1)\n    t1 = np.sin(np.array(range(1,24,2)).reshape(3,4).T)\n    t2 = np.cos(np.array(range(1,40,2)).reshape(5,4).T)\n\n    fname = srcs[part_id-1].rsplit(\'.\',1)[0]\n    mod = __import__(fname, fromlist=[fname], level=1)\n    func = getattr(mod, fname)\n\n    if part_id == 1:\n        J = lrCostFunction(np.array([0.25, 0.5, -0.5]), X, y, 0.1)\n        grad = gradientFunctionReg(np.array([0.25, 0.5, -0.5]), X, y, 0.1)\n        return sprintf(\'%0.5f \', np.hstack((J, grad)).tolist())\n    elif part_id == 2:\n        return sprintf(\'%0.5f \', oneVsAll(Xm, ym, 4, 0.1))\n    elif part_id == 3:\n        return sprintf(\'%0.5f \', predictOneVsAll(t1, Xm))\n    elif part_id == 4:\n        return sprintf(\'%0.5f \', predict(t1, t2, Xm))\n\ns = Submission(homework, part_names, srcs, output)\n\ntry:\n\ts.submit()\nexcept Exception as ex:\n   template = ""An exception of type {0} occured. Messsage:\\n{1!r}""\n   message = template.format(type(ex).__name__, ex.args)\n   print message\n\n'"
ex4/__init__.py,0,b''
ex4/checkNNGradients.py,4,"b'import numpy as np\n\nfrom debugInitializeWeights import debugInitializeWeights\nfrom computeNumericalGradient import computeNumericalGradient\nfrom nnCostFunction import nnCostFunction\n\ndef checkNNGradients(Lambda = 0):\n\n    """"""Creates a small neural network to check the\n    backpropagation gradients, it will output the analytical gradients\n    produced by your backprop code and the numerical gradients (computed\n    using computeNumericalGradient). These two gradient computations should\n    result in very similar values.\n    """"""\n\n    input_layer_size = 3\n    hidden_layer_size = 5\n    num_labels = 3\n    m = 5\n\n    # We generate some \'random\' test data\n    Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)\n    Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)\n\n    # Reusing debugInitializeWeights to generate X\n    X  = debugInitializeWeights(m, input_layer_size - 1)\n    y  = np.mod(range(1, m+1), num_labels)\n\n    # Unroll parameters\n    nn_params = np.hstack((Theta1.T.ravel(), Theta2.T.ravel()))\n\n    # Short hand for cost function\n\n    costFunc = lambda p: nnCostFunction(p, input_layer_size, hidden_layer_size, num_labels, X, y, Lambda)\n\n    numgrad = computeNumericalGradient(costFunc, nn_params)\n    grad = costFunc(nn_params)[1]\n\n    # Visually examine the two gradient computations.  The two columns\n    # you get should be very similar.\n    print np.column_stack((numgrad, grad))\n\n    print \'The above two columns you get should be very similar.\\n\' \\\n             \'(Left-Your Numerical Gradient, Right-Analytical Gradient)\\n\\n\'\n\n    # Evaluate the norm of the difference between two solutions.\n    # If you have a correct implementation, and assuming you used EPSILON = 0.0001\n    # in computeNumericalGradient.m, then diff below should be less than 1e-9\n    diff = np.linalg.norm(numgrad-grad)/np.linalg.norm(numgrad+grad)\n\n    print \'If your backpropagation implementation is correct, then\\n \' \\\n          \'the relative difference will be small (less than 1e-9). \\n\' \\\n          \'\\nRelative Difference: %g\\n\' % diff\n\n'"
ex4/computeNumericalGradient.py,2,"b'import numpy as np\n\ndef computeNumericalGradient(J, theta):\n    """"""computes the numerical gradient of the function J around theta.\n    Calling y = J(theta) should return the function value at theta.\n    """"""\n# Notes: The following code implements numerical gradient checking, and \n#        returns the numerical gradient.It sets numgrad(i) to (a numerical \n#        approximation of) the partial derivative of J with respect to the \n#        i-th input argument, evaluated at theta. (i.e., numgrad(i) should \n#        be the (approximately) the partial derivative of J with respect \n#        to theta(i).)\n\n    numgrad = np.zeros(theta.shape[0])\n    perturb = np.zeros(theta.shape[0])\n    e = 1e-4\n    for p in range(theta.size):\n\n        # Set perturbation vector\n        perturb[p] = e\n        loss1 = J(theta - perturb)\n        loss2 = J(theta + perturb)\n\n        # Compute Numerical Gradient\n        numgrad[p] = (loss2[0] - loss1[0]) / (2*e)\n        perturb[p] = 0\n\n    return numgrad\n\n'"
ex4/debugInitializeWeights.py,2,"b'import numpy as np\n\ndef debugInitializeWeights(fan_out, fan_in):\n    """"""initializes the weights of a layer with fan_in incoming connections\n    and fan_out outgoing connections using a fix set of values\n\n    Note that W should be set to a matrix of size(1 + fan_in, fan_out) as\n    the first row of W handles the ""bias"" terms\n    """"""\n\n# Set W to zeros\n    W = np.zeros((fan_out, 1 + fan_in))\n\n# Initialize W using ""sin"", this ensures that W is always of the same\n# values and will be useful for debugging\n    W = np.reshape(np.sin(range(1, W.size+1)), W.T.shape).T / 10.0\n    return W\n\n# =========================================================================\n\n'"
ex4/ex4.py,8,"b'## Machine Learning Online Class - Exercise 4 Neural Network Learning\n\n#  Instructions\n#  ------------\n# \n#  This file contains code that helps you get started on the\n#  linear exercise. You will need to complete the following functions \n#  in this exericse:\n#\n#     sigmoidGradient.m\n#     randInitializeWeights.m\n#     nnCostFunction.m\n#\n#  For this exercise, you will not need to change any code in this file,\n#  or any other files other than those mentioned above.\n#\n\nimport numpy as np\nimport scipy.io\nfrom scipy.optimize import minimize\n\nfrom ex3.displayData import displayData\nfrom ex3.predict import predict\nfrom nnCostFunction import nnCostFunction\nfrom sigmoidGradient import sigmoidGradient\nfrom randInitializeWeights import randInitializeWeights\nfrom checkNNGradients import checkNNGradients\n\n## Setup the parameters you will use for this exercise\ninput_layer_size  = 400  # 20x20 Input Images of Digits\nhidden_layer_size = 25   # 25 hidden units\nnum_labels = 10          # 10 labels, from 1 to 10   \n                          # (note that we have mapped ""0"" to label 10)\n\n## =========== Part 1: Loading and Visualizing Data =============\n#  We start the exercise by first loading and visualizing the dataset. \n#  You will be working with a dataset that contains handwritten digits.\n#\n\n# Load Training Data\nprint \'Loading and Visualizing Data ...\'\n\ndata = scipy.io.loadmat(\'ex4data1.mat\')\nX = data[\'X\']\ny = data[\'y\']\nm, _ = X.shape\n\n# Randomly select 100 data points to display\nrand_indices = np.random.permutation(range(m))\nsel = X[rand_indices[0:100], :]\n\ndisplayData(sel)\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n\n## ================ Part 2: Loading Parameters ================\n# In this part of the exercise, we load some pre-initialized \n# neural network parameters.\n\nprint \'Loading Saved Neural Network Parameters ...\'\n\n# Load the weights into variables Theta1 and Theta2\ndata = scipy.io.loadmat(\'ex4weights.mat\')\nTheta1 = data[\'Theta1\']\nTheta2 = data[\'Theta2\']\ny = np.squeeze(y)\n\n# Unroll parameters \nnn_params = np.hstack((Theta1.T.ravel(), Theta2.T.ravel()))\n\n## ================ Part 3: Compute Cost (Feedforward) ================\n#  To the neural network, you should first start by implementing the\n#  feedforward part of the neural network that returns the cost only. You\n#  should complete the code in nnCostFunction.m to return cost. After\n#  implementing the feedforward to compute the cost, you can verify that\n#  your implementation is correct by verifying that you get the same cost\n#  as us for the fixed debugging parameters.\n#\n#  We suggest implementing the feedforward cost *without* regularization\n#  first so that it will be easier for you to debug. Later, in part 4, you\n#  will get to implement the regularized cost.\n#\nprint \'Feedforward Using Neural Network ...\'\n\n# Weight regularization parameter (we set this to 0 here).\nLambda = 0\n\nJ, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n    num_labels, X, y, Lambda)\n\nprint \'Cost at parameters (loaded from ex4weights): %f \\n(this value should be about 0.287629)\\n\' % J\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## =============== Part 4: Implement Regularization ===============\n#  Once your cost function implementation is correct, you should now\n#  continue to implement the regularization with the cost.\n#\n\nprint \'Checking Cost Function (w/ Regularization) ...\'\n\n# Weight regularization parameter (we set this to 1 here).\nLambda = 1\n\nJ, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, Lambda)\n\nprint \'Cost at parameters (loaded from ex4weights): %f \\n(this value should be about 0.383770)\' % J\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n\n## ================ Part 5: Sigmoid Gradient  ================\n#  Before you start implementing the neural network, you will first\n#  implement the gradient for the sigmoid function. You should complete the\n#  code in the sigmoidGradient.m file.\n#\n\nprint \'Evaluating sigmoid gradient...\'\n\ng = sigmoidGradient(np.array([1, -0.5, 0, 0.5, 1]))\nprint \'Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]: \'\nprint g\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n\n## ================ Part 6: Initializing Pameters ================\n#  In this part of the exercise, you will be starting to implment a two\n#  layer neural network that classifies digits. You will start by\n#  implementing a function to initialize the weights of the neural network\n#  (randInitializeWeights.m)\n\nprint \'Initializing Neural Network Parameters ...\'\n\ninitial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\ninitial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n\n# Unroll parameters\ninitial_nn_params = np.hstack((initial_Theta1.T.ravel(), initial_Theta2.T.ravel()))\n\n\n## =============== Part 7: Implement Backpropagation ===============\n#  Once your cost matches up with ours, you should proceed to implement the\n#  backpropagation algorithm for the neural network. You should add to the\n#  code you\'ve written in nnCostFunction.m to return the partial\n#  derivatives of the parameters.\n#\nprint \'Checking Backpropagation... \'\n\n#  Check gradients by running checkNNGradients\ncheckNNGradients()\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n\n## =============== Part 8: Implement Regularization ===============\n#  Once your backpropagation implementation is correct, you should now\n#  continue to implement the regularization with the cost and gradient.\n#\n\nprint \'Checking Backpropagation (w/ Regularization) ... \'\n\n#  Check gradients by running checkNNGradients\nLambda = 3.0\ncheckNNGradients(Lambda)\n\n# Also output the costFunction debugging values\ndebug_J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, Lambda)\n\nprint \'Cost at (fixed) debugging parameters (w/ lambda = 10): %f (this value should be about 0.576051)\\n\\n\' % debug_J\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n\n## =================== Part 8: Training NN ===================\n#  You have now implemented all the code necessary to train a neural \n#  network. To train your neural network, we will now use ""fmincg"", which\n#  is a function which works similarly to ""fminunc"". Recall that these\n#  advanced optimizers are able to train our cost functions efficiently as\n#  long as we provide them with the gradient computations.\n#\nprint \'Training Neural Network... \'\n\n#  After you have completed the assignment, change the MaxIter to a larger\n#  value to see how more training helps.\n# options = optimset(\'MaxIter\', 50)\n\n#  You should also try different values of lambda\nLambda = 1\n\ncostFunc = lambda p: nnCostFunction(p, input_layer_size, hidden_layer_size, num_labels, X, y, Lambda)[0]\ngradFunc = lambda p: nnCostFunction(p, input_layer_size, hidden_layer_size, num_labels, X, y, Lambda)[1]\n\nresult = minimize(costFunc, initial_nn_params, method=\'CG\', jac=gradFunc, options={\'disp\': True, \'maxiter\': 50.0})\nnn_params = result.x\ncost = result.fun\n\n# Obtain Theta1 and Theta2 back from nn_params\nTheta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n                   (hidden_layer_size, input_layer_size + 1), order=\'F\').copy()\nTheta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):],\n                   (num_labels, (hidden_layer_size + 1)), order=\'F\').copy()\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n\n## ================= Part 9: Visualize Weights =================\n#  You can now ""visualize"" what the neural network is learning by \n#  displaying the hidden units to see what features they are capturing in \n#  the data.\n\nprint \'Visualizing Neural Network... \'\n\ndisplayData(Theta1[:, 1:])\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## ================= Part 10: Implement Predict =================\n#  After training the neural network, we would like to use it to predict\n#  the labels. You will now implement the ""predict"" function to use the\n#  neural network to predict the labels of the training set. This lets\n#  you compute the training set accuracy.\n\npred = predict(Theta1, Theta2, X)\n\naccuracy = np.mean(np.double(pred == y)) * 100\nprint \'Training Set Accuracy: %f\\n\'% accuracy\n\n\nraw_input(""Program paused. Press Enter to exit..."")\n'"
ex4/nnCostFunction.py,3,"b'import numpy as np\n\nfrom ex2.sigmoid import sigmoid\nfrom sigmoidGradient import sigmoidGradient\n\n\ndef nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, Lambda):\n\n    """"""computes the cost and gradient of the neural network. The\n  parameters for the neural network are ""unrolled"" into the vector\n  nn_params and need to be converted back into the weight matrices.\n\n  The returned parameter grad should be a ""unrolled"" vector of the\n  partial derivatives of the neural network.\n    """"""\n\n# Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n# for our 2 layer neural network\n# Obtain Theta1 and Theta2 back from nn_params\n    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n                       (hidden_layer_size, input_layer_size + 1), order=\'F\').copy()\n\n    Theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):],\n                       (num_labels, (hidden_layer_size + 1)), order=\'F\').copy()\n\n\n\n# Setup some useful variables\n    m, _ = X.shape\n\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: You should complete the code by working through the\n#               following parts.\n#\n# Part 1: Feedforward the neural network and return the cost in the\n#         variable J. After implementing Part 1, you can verify that your\n#         cost function computation is correct by verifying the cost\n#         computed in ex4.m\n#\n# Part 2: Implement the backpropagation algorithm to compute the gradients\n#         Theta1_grad and Theta2_grad. You should return the partial derivatives of\n#         the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n#         Theta2_grad, respectively. After implementing Part 2, you can check\n#         that your implementation is correct by running checkNNGradients\n#\n#         Note: The vector y passed into the function is a vector of labels\n#               containing values from 1..K. You need to map this vector into a \n#               binary vector of 1\'s and 0\'s to be used with the neural network\n#               cost function.\n#\n#         Hint: We recommend implementing backpropagation using a for-loop\n#               over the training examples if you are implementing it for the \n#               first time.\n#\n# Part 3: Implement regularization with the cost function and gradients.\n#\n#         Hint: You can implement this around the code for\n#               backpropagation. That is, you can compute the gradients for\n#               the regularization separately and then add them to Theta1_grad\n#               and Theta2_grad from Part 2.\n#\n\n\n\n    # -------------------------------------------------------------\n\n    # =========================================================================\n\n    # Unroll gradient\n    grad = np.hstack((Theta1_grad.T.ravel(), Theta2_grad.T.ravel()))\n\n\n    return J, grad'"
ex4/randInitializeWeights.py,0,"b'import numpy as np\n\ndef randInitializeWeights(L_in, L_out):\n    """"""randomly initializes the weights of a layer with L_in incoming connections and L_out outgoing\n      connections.\n\n      Note that W should be set to a matrix of size(L_out, 1 + L_in) as the column row of W handles the ""bias"" terms\n    """"""\n\n    # ====================== YOUR CODE HERE ======================\n    # Instructions: Initialize W randomly so that we break the symmetry while\n    #               training the neural network.\n    #\n    # Note: The first row of W corresponds to the parameters for the bias units\n    #\n\n\n\n# =========================================================================\n\n    return W\n'"
ex4/sigmoidGradient.py,0,"b'from ex2.sigmoid import sigmoid\n\ndef sigmoidGradient(z):\n    """"""computes the gradient of the sigmoid function\n    evaluated at z. This should work regardless if z is a matrix or a\n    vector. In particular, if z is a vector or matrix, you should return\n    the gradient for each element.""""""\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: Compute the gradient of the sigmoid function evaluated at\n#               each value of z (z can be a matrix, vector or scalar).\n\n\n# =============================================================\n\n    return g\n'"
ex4/submit.py,8,"b'import numpy as np\n\nfrom Submission import Submission\nfrom Submission import sprintf\n\nhomework = \'neural-network-learning\'\n\npart_names = [\n  \'Feedforward and Cost Function\',\n  \'Regularized Cost Function\',\n  \'Sigmoid Gradient\',\n  \'Neural Network Gradient (Backpropagation)\',\n  \'Regularized Gradient\',\n  ]\n\nsrcs = [\n  \'nnCostFunction.py\',\n  \'nnCostFunction.py\',\n  \'sigmoidGradient.py\',\n  \'nnCostFunction.py\',\n  \'nnCostFunction.py\',\n  ]\n\n\ndef output(part_id):\n    # Random Test Cases\n    X = np.reshape(3.0*np.sin(np.linspace(1, 30, 30)), (3, 10), order=\'F\')\n    Xm = np.reshape(np.sin(np.linspace(1, 32, 32)), (16, 2), order=\'F\')/5.0\n    ym = np.array(1 + np.mod(range(1,17),4))\n    t1 = np.sin(np.reshape(range(1,24,2), (4,3), order=\'F\'))\n    t2 = np.cos(np.reshape(range(1,40,2), (4,5), order=\'F\'))\n    t = np.hstack((t1.T.ravel(), t2.T.ravel()))\n\n    fname = srcs[part_id-1].rsplit(\'.\',1)[0]\n    mod = __import__(fname, fromlist=[fname], level=1)\n    func = getattr(mod, fname)\n\n    if part_id == 1:\n        J, grad = func(t, 2.0, 4.0, 4.0, Xm, ym, 0.0)\n        return sprintf(\'%0.5f \', J)\n    elif part_id == 2:\n        J, grad = func(t, 2.0, 4.0, 4.0, Xm, ym, 1.5)\n        return sprintf(\'%0.5f \', J)\n    elif part_id == 3:\n        return sprintf(\'%0.5f \', func(X))\n    elif part_id == 4:\n        J, grad = func(t, 2, 4, 4, Xm, ym, 0)\n        return sprintf(\'%0.5f \', np.hstack((J, grad)).tolist())\n    elif part_id == 5:\n        J, grad = func(t, 2, 4, 4, Xm, ym, 1.5)\n        return sprintf(\'%0.5f \', np.hstack((J, grad)).tolist())\n\ns = Submission(homework, part_names, srcs, output)\ntry:\n    s.submit()\nexcept Exception as ex:\n    template = ""An exception of type {0} occured. Messsage:\\n{1!r}""\n    message = template.format(type(ex).__name__, ex.args)\n    print message\n'"
ex5/ex5.py,12,"b'\nimport scipy.io\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom linearRegCostFunction import linearRegCostFunction\nfrom trainLinearReg import trainLinearReg\nfrom learningCurve import learningCurve\nfrom polyFeatures import polyFeatures\nfrom featureNormalize import featureNormalize\nfrom plotFit import plotFit\nfrom validationCurve import validationCurve\n\n## Machine Learning Online Class\n#  Exercise 5 | Regularized Linear Regression and Bias-Variance\n#\n#  Instructions\n#  ------------\n# \n#  This file contains code that helps you get started on the\n#  exercise. You will need to complete the following functions:\n#\n#     linearRegCostFunction.m\n#     learningCurve.m\n#     validationCurve.m\n#\n#  For this exercise, you will not need to change any code in this file,\n#  or any other files other than those mentioned above.\n#\n\n## =========== Part 1: Loading and Visualizing Data =============\n#  We start the exercise by first loading and visualizing the dataset. \n#  The following code will load the dataset into your environment and plot\n#  the data.\n#\n\n# Load Training Data\nprint \'Loading and Visualizing Data ...\'\n\n# Load from ex5data1: \n# You will have X, y, Xval, yval, Xtest, ytest in your environment\ndata = scipy.io.loadmat(\'ex5data1.mat\')\n\n# m = Number of examples\nX = data[\'X\'][:, 0]\ny = data[\'y\'][:, 0]\nXval = data[\'Xval\'][:, 0]\nyval = data[\'yval\'][:, 0]\nXtest = data[\'Xtest\'][:, 0]\n\nm = X.size\n\n# Plot training data\nplt.scatter(X, y, marker=\'x\', s=60, edgecolor=\'r\', lw=1.5)\nplt.ylabel(\'Water flowing out of the dam (y)\')            # Set the y-axis label\nplt.xlabel(\'Change in water level (x)\')     # Set the x-axis label\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## =========== Part 2: Regularized Linear Regression Cost =============\n#  You should now implement the cost function for regularized linear \n#  regression. \n#\n\ntheta = np.array([1, 1])\nJ = linearRegCostFunction(np.column_stack((np.ones(m), X)), y, theta, 1)[0]\n\nprint \'Cost at theta = [1  1]: %f \\n(this value should be about 303.993192)\\n\' % J\n\nraw_input(""Program paused. Press Enter to continue..."") \n\n## =========== Part 3: Regularized Linear Regression Gradient =============\n#  You should now implement the gradient for regularized linear \n#  regression.\n#\n\ntheta = np.array([1, 1])\nJ, grad = linearRegCostFunction(np.column_stack((np.ones(m), X)), y, theta, 1)\n\nprint \'Gradient at theta = [1  1]:  [%f %f] \\n(this value should be about [-15.303016 598.250744])\\n\' %(grad[0], grad[1])\n\nraw_input(""Program paused. Press Enter to continue..."") \n\n\n## =========== Part 4: Train Linear Regression =============\n#  Once you have implemented the cost and gradient correctly, the\n#  trainLinearReg function will use your cost function to train \n#  regularized linear regression.\n# \n#  Write Up Note: The data is non-linear, so this will not give a great \n#                 fit.\n#\n\n#  Train linear regression with Lambda = 0\nLambda = 0\ntheta = trainLinearReg(np.column_stack((np.ones(m), X)), y, 1)\n\n#  Plot fit over the data\nplt.scatter(X, y, marker=\'x\', s=20, edgecolor=\'r\', lw=1.5)\nplt.ylabel(\'Water flowing out of the dam (y)\')            # Set the y-axis label\nplt.xlabel(\'Change in water level (x)\')     # Set the x-axis label\nplt.plot(X, np.column_stack((np.ones(m), X)).dot(theta), \'--\', lw=2.0)\n\nraw_input(""Program paused. Press Enter to continue..."") \n\n\n## =========== Part 5: Learning Curve for Linear Regression =============\n#  Next, you should implement the learningCurve function. \n#\n#  Write Up Note: Since the model is underfitting the data, we expect to\n#                 see a graph with ""high bias"" -- slide 8 in ML-advice.pdf \n#\n\nLambda = 0\nerror_train, error_val = learningCurve(np.column_stack((np.ones(m), X)), y,\n                                       np.column_stack((np.ones(Xval.shape[0]), Xval)), yval, Lambda)\nplt.figure()\nplt.plot(range(m), error_train, color=\'b\', lw=0.5, label=\'Train\')\nplt.plot(range(m), error_val, color=\'r\', lw=0.5, label=\'Cross Validation\')\nplt.title(\'Learning curve for linear regression\')\nplt.legend()\nplt.xlabel(\'Number of training examples\')\nplt.ylabel(\'Error\')\n\nplt.xlim(0, 13)\nplt.ylim(0, 150)\nplt.legend(loc=\'upper right\', shadow=True, fontsize=\'x-large\', numpoints=1)\n\nprint \'Training Examples\\tTrain Error\\tCross Validation Error\'\nfor i in range(m):\n    print \'  \\t%d\\t\\t%f\\t%f\' % (i, error_train[i], error_val[i])\n\nraw_input(""Program paused. Press Enter to continue..."") \n\n## =========== Part 6: Feature Mapping for Polynomial Regression =============\n#  One solution to this is to use polynomial regression. You should now\n#  complete polyFeatures to map each example into its powers\n#\n\np = 8\n\n# Map X onto Polynomial Features and Normalize\nX_poly = polyFeatures(X, p)\nX_poly, mu, sigma = featureNormalize(X_poly)  # Normalize\nX_poly = np.column_stack((np.ones(m), X_poly))                   # Add Ones\n\n# Map X_poly_test and normalize (using mu and sigma)\nX_poly_test = polyFeatures(Xtest, p)\nX_poly_test = X_poly_test - mu\nX_poly_test = X_poly_test / sigma\nX_poly_test = np.column_stack((np.ones(X_poly_test.shape[0]), X_poly_test))        # Add Ones\n\n# Map X_poly_val and normalize (using mu and sigma)\nX_poly_val = polyFeatures(Xval, p)\nX_poly_val = X_poly_val - mu\nX_poly_val = X_poly_val / sigma\nX_poly_val = np.column_stack((np.ones(X_poly_test.shape[0]), X_poly_val))           # Add Ones\n\nprint \'Normalized Training Example 1:\'\nprint X_poly[0, :]\n\nprint \'\\nProgram paused. Press enter to continue.\'\n\n\n\n## =========== Part 7: Learning Curve for Polynomial Regression =============\n#  Now, you will get to experiment with polynomial regression with multiple\n#  values of Lambda. The code below runs polynomial regression with \n#  Lambda = 0. You should try running the code with different values of\n#  Lambda to see how the fit and learning curve change.\n#\n\nLambda = 0\ntheta = trainLinearReg(X_poly, y, Lambda, method=\'BFGS\', maxiter=10)\n\n# Plot training data and fit\nplt.figure()\nplt.scatter(X, y, marker=\'x\', s=10, edgecolor=\'r\', lw=1.5)\n\nplotFit(min(X), max(X), mu, sigma, theta, p)\n\nplt.xlabel(\'Change in water level (x)\')            # Set the y-axis label\nplt.ylabel(\'Water flowing out of the dam (y)\')     # Set the x-axis label\n# plt.plot(X, np.column_stack((np.ones(m), X)).dot(theta), marker=\'_\',  lw=2.0)\nplt.title(\'Polynomial Regression Fit (Lambda = %f)\' % Lambda)\n\nerror_train, error_val = learningCurve(X_poly, y, X_poly_val, yval, Lambda)\nplt.plot(range(m), error_train, label=\'Train\')\nplt.plot(range(m), error_val, label=\'Cross Validation\')\nplt.title(\'Polynomial Regression Learning Curve (Lambda = %f)\' % Lambda)\nplt.xlabel(\'Number of training examples\')\nplt.ylabel(\'Error\')\nplt.xlim(0, 13)\nplt.ylim(0, 150)\nplt.legend()\n\nprint \'Polynomial Regression (Lambda = %f)\\n\\n\' % Lambda\nprint \'# Training Examples\\tTrain Error\\tCross Validation Error\'\nfor i in range(m):\n    print \'  \\t%d\\t\\t%f\\t%f\' % (i, error_train[i], error_val[i])\n\nraw_input(""Program paused. Press Enter to continue..."") \n\n## =========== Part 8: Validation for Selecting Lambda =============\n#  You will now implement validationCurve to test various values of \n#  Lambda on a validation set. You will then use this to select the\n#  ""best"" Lambda value.\n#\n\nLambda_vec, error_train, error_val = validationCurve(X_poly, y, X_poly_val, yval)\n\nplt.plot(Lambda_vec, error_train, Lambda_vec, error_val)\nplt.legend(\'Train\', \'Cross Validation\')\nplt.xlabel(\'Lambda\')\nplt.ylabel(\'Error\')\n\nprint \'Lambda\\t\\tTrain Error\\tValidation Error\'\nfor i in range(Lambda_vec.size):\n    print \' %f\\t%f\\t%f\' % (Lambda_vec[i], error_train[i], error_val[i])\n\nraw_input(""Program paused. Press Enter to continue..."") \n'"
ex5/featureNormalize.py,2,"b'import numpy as np\n\n\ndef featureNormalize(X):\n    """""" returns a normalized version of X where\n    the mean value of each feature is 0 and the standard deviation\n    is 1. This is often a good preprocessing step to do when\n    working with learning algorithms.\n    """"""\n\n    mu = np.mean(X, axis=0)\n    X_norm = X - mu\n\n    sigma = np.std(X_norm, axis=0, ddof=1)\n    X_norm = X_norm / sigma\n\n# ============================================================\n    return X_norm, mu, sigma'"
ex5/learningCurve.py,2,"b'import numpy as np\n\nfrom trainLinearReg import trainLinearReg\nfrom linearRegCostFunction import linearRegCostFunction\n\ndef learningCurve(X, y, Xval, yval, Lambda):\n    """"""returns the train and\n    cross validation set errors for a learning curve. In particular,\n    it returns two vectors of the same length - error_train and\n    error_val. Then, error_train(i) contains the training error for\n    i examples (and similarly for error_val(i)).\n\n    In this function, you will compute the train and test errors for\n    dataset sizes from 1 up to m. In practice, when working with larger\n    datasets, you might want to do this in larger intervals.\n    """"""\n\n# Number of training examples\n    m, _ = X.shape\n\n# You need to return these values correctly\n    error_train = np.zeros(m)\n    error_val   = np.zeros(m)\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: Fill in this function to return training errors in \n#               error_train and the cross validation errors in error_val. \n#               i.e., error_train(i) and \n#               error_val(i) should give you the errors\n#               obtained after training on i examples.\n#\n# Note: You should evaluate the training error on the first i training\n#       examples (i.e., X(1:i, :) and y(1:i)).\n#\n#       For the cross-validation error, you should instead evaluate on\n#       the _entire_ cross validation set (Xval and yval).\n#\n# Note: If you are using your cost function (linearRegCostFunction)\n#       to compute the training and cross validation error, you should \n#       call the function with the lambda argument set to 0. \n#       Do note that you will still need to use lambda when running\n#       the training to obtain the theta parameters.\n#\n# Hint: You can loop over the examples with the following:\n#\n#       for i = 1:m\n#           # Compute train/cross validation errors using training examples \n#           # X(1:i, :) and y(1:i), storing the result in \n#           # error_train(i) and error_val(i)\n#           ....\n#           \n#       end\n#\n\n# ---------------------- Sample Solution ----------------------\n\n\n\n# -------------------------------------------------------------------------\n\n# =========================================================================\n\n    return error_train, error_val'"
ex5/linearRegCostFunction.py,0,"b'import numpy as np\ndef linearRegCostFunction(X, y, theta, Lambda):\n    """"""computes the\n    cost of using theta as the parameter for linear regression to fit the\n    data points in X and y. Returns the cost in J and the gradient in grad\n    """"""\n# Initialize some useful values\n\n    m = y.size # number of training examples\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: Compute the cost and gradient of regularized linear \n#               regression for a particular choice of theta.\n#\n#               You should set J to the cost and grad to the gradient.\n#\n\n\n# =========================================================================\n\n    return J, grad'"
ex5/plotFit.py,2,"b'import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom polyFeatures import polyFeatures\n\ndef plotFit(min_x, max_x, mu, sigma, theta, p):\n    """"""plots the learned polynomial fit with power p\n    and feature normalization (mu, sigma).\n    """"""\n\n# We plot a range slightly bigger than the min and max values to get\n# an idea of how the fit will vary outside the range of the data points\n    x = np.arange(min_x - 15, max_x + 25, 0.05).T\n\n# Map the X values \n    X_poly = polyFeatures(x, p)\n    X_poly = X_poly - mu\n    X_poly = X_poly / sigma\n\n# Add ones\n    X_poly = np.column_stack((np.ones(x.shape[0]), X_poly))\n\n# Plot\n    plt.plot(x, X_poly.dot(theta), \'--\', lw=2)\n\n'"
ex5/polyFeatures.py,1,"b'import numpy as np\n\ndef polyFeatures(X, p):\n    """"""takes a data matrix X (size m x 1) and\n    maps each example into its polynomial features where\n    X_poly(i, :) = [X(i) X(i).^2 X(i).^3 ...  X(i).^p]\n    """"""\n# You need to return the following variables correctly.\n    X_poly = np.zeros((X.size, p))\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: Given a vector X, return a matrix X_poly where the p-th \n#               column of X contains the values of X to the p-th power.\n#\n# \n\n# =========================================================================\n\n    return X_poly\n'"
ex5/submit.py,12,"b'import numpy as np\n\nfrom Submission import Submission\nfrom Submission import sprintf\n\nhomework = \'regularized-linear-regression-and-bias-variance\'\n\npart_names = [\n  \'Regularized Linear Regression Cost Function\',\n  \'Regularized Linear Regression Gradient\',\n  \'Learning Curve\',\n  \'Polynomial Feature Mapping\',\n  \'Validation Curve\'\n  ]\n\nsrcs = [\n  \'linearRegCostFunction.py\',\n  \'linearRegCostFunction.py\',\n  \'learningCurve.py\',\n  \'polyFeatures.py\',\n  \'validationCurve.py\'\n  ]\n\n\ndef output(part_id):\n    # Random Test Cases\n    X = np.column_stack((np.ones(10),\n                          (np.sin(np.arange(1, 16, 1.5))),\n                          (np.cos(np.arange(1, 16, 1.5)))))\n    y = np.sin(np.arange(1, 30, 3))\n\n    Xval = np.column_stack((np.ones(10),\n                          (np.sin(np.arange(0, 14, 1.5))),\n                          (np.cos(np.arange(0, 14, 1.5)))))\n    yval = np.sin(np.arange(1,11))\n\n    fname = srcs[part_id-1].rsplit(\'.\',1)[0]\n    mod = __import__(fname, fromlist=[fname], level=1)\n    func = getattr(mod, fname)\n\n    if part_id == 1:\n        J, _ = func(X, y, np.array([0.1, 0.2, 0.3]), 0.5)\n        return sprintf(\'%0.5f \', J)\n    elif part_id == 2:\n        _, grad = func(X, y, np.array([0.1, 0.2, 0.3]), 0.5)\n        return sprintf(\'%0.5f \', grad)\n    elif part_id == 3:\n        error_train, error_val = func(X, y, Xval, yval, 1)\n        return sprintf(\'%0.5f \', np.hstack((error_train, error_val)))\n    elif part_id == 4:\n        X_poly = func(X[1, :].T, 8)\n        return sprintf(\'%0.5f \', X_poly)\n    elif part_id == 5:\n        lambda_vec, error_train, error_val = func(X, y, Xval, yval)\n        return sprintf(\'%0.5f\', np.hstack((lambda_vec, error_train, error_val)))\n\ns = Submission(homework, part_names, srcs, output)\ntry:\n    s.submit()\nexcept Exception as ex:\n    template = ""An exception of type {0} occured. Messsage:\\n{1!r}""\n    message = template.format(type(ex).__name__, ex.args)\n    print message\n'"
ex5/trainLinearReg.py,1,"b'from scipy.optimize import minimize\n\nimport numpy as np\n\nfrom linearRegCostFunction import linearRegCostFunction\n\n\ndef trainLinearReg(X, y, Lambda, method=\'CG\', maxiter=200):\n\n    """"""trains linear regression using\n    the dataset (X, y) and regularization parameter lambda. Returns the\n    trained parameters theta.\n    """"""\n\n# Initialize Theta\n    initial_theta = np.zeros(X.shape[1])\n\n# Create ""short hand"" for the cost function to be minimized\n    costFunction = lambda t: linearRegCostFunction(X, y, t, Lambda)[0]\n    gradFunction = lambda t: linearRegCostFunction(X, y, t, Lambda)[1]\n\n    result = minimize(costFunction, initial_theta, method=method, jac=None, options={\'disp\': True, \'maxiter\': maxiter})\n\n    return result.x\n'"
ex5/validationCurve.py,3,"b'import numpy as np\n\nfrom trainLinearReg import trainLinearReg\nfrom linearRegCostFunction import linearRegCostFunction\n\ndef validationCurve(X, y, Xval, yval):\n    """"""returns the train\n    and validation errors (in error_train, error_val)\n    for different values of lambda. You are given the training set (X,\n    y) and validation set (Xval, yval).\n    """"""\n\n# Selected values of lambda (you should not change this)\n    lambda_vec = np.array([0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10])\n\n# You need to return these variables correctly.\n    error_train = np.zeros(lambda_vec.size)\n    error_val = np.zeros(lambda_vec.size)\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: Fill in this function to return training errors in \n#               error_train and the validation errors in error_val. The \n#               vector lambda_vec contains the different lambda parameters \n#               to use for each calculation of the errors, i.e, \n#               error_train(i), and error_val(i) should give \n#               you the errors obtained after training with \n#               lambda = lambda_vec(i)\n#\n# Note: You can loop over lambda_vec with the following:\n#\n#       for i = 1:length(lambda_vec)\n#           lambda = lambda_vec(i)\n#           # Compute train / val errors when training linear \n#           # regression with regularization parameter lambda\n#           # You should store the result in error_train(i)\n#           # and error_val(i)\n#           ....\n#           \n#       end\n#\n#\n\n\n\n# =========================================================================\n\n    return lambda_vec, error_train, error_val'"
ex6/dataset3Params.py,0,"b'import numpy as np\nimport sklearn.svm\n\n\ndef dataset3Params(X, y, Xval, yval):\n    """"""returns your choice of C and sigma. You should complete\n    this function to return the optimal C and sigma based on a\n    cross-validation set.\n    """"""\n\n# You need to return the following variables correctly.\n    C = 1\n    sigma = 0.3\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: Fill in this function to return the optimal C and sigma\n#               learning parameters found using the cross validation set.\n#               You can use svmPredict to predict the labels on the cross\n#               validation set. For example, \n#                   predictions = svmPredict(model, Xval)\n#               will return the predictions on the cross validation set.\n#\n#  Note: You can compute the prediction error using \n#        mean(double(predictions ~= yval))\n#\n\n\n# =========================================================================\n    return C, sigma\n'"
ex6/emailFeatures.py,1,"b'import numpy as np\n\n\ndef emailFeatures(word_indices):\n    """"""takes in a word_indices vector and\n    produces a feature vector from the word indices.\n    """"""\n\n# Total number of words in the dictionary\n    n = 1899\n\n# You need to return the following variables correctly.\n    x = np.zeros(n)\n# ====================== YOUR CODE HERE ======================\n# Instructions: Fill in this function to return a feature vector for the\n#               given email (word_indices). To help make it easier to \n#               process the emails, we have have already pre-processed each\n#               email and converted each word in the email into an index in\n#               a fixed dictionary (of 1899 words). The variable\n#               word_indices contains the list of indices of the words\n#               which occur in one email.\n# \n#               Concretely, if an email has the text:\n#\n#                  The quick brown fox jumped over the lazy dog.\n#\n#               Then, the word_indices vector for this text might look \n#               like:\n#               \n#                   60  100   33   44   10     53  60  58   5\n#\n#               where, we have mapped each word onto a number, for example:\n#\n#                   the   -- 60\n#                   quick -- 100\n#                   ...\n#\n#              (note: the above numbers are just an example and are not the\n#               actual mappings).\n#\n#              Your task is take one such word_indices vector and construct\n#              a binary feature vector that indicates whether a particular\n#              word occurs in the email. That is, x(i) = 1 when word i\n#              is present in the email. Concretely, if the word \'the\' (say,\n#              index 60) appears in the email, then x(60) = 1. The feature\n#              vector should look like:\n#\n#              x = [ 0 0 0 0 1 0 0 0 ... 0 0 0 0 1 ... 0 0 0 1 0 ..]\n#\n#\n\n\n# =========================================================================\n\n    return x'"
ex6/ex6.py,2,"b'## Machine Learning Online Class\n#  Exercise 6 | Support Vector Machines\n#\n#  Instructions\n#  ------------\n# \n#  This file contains code that helps you get started on the\n#  exercise. You will need to complete the following functions:\n#\n#     gaussianKernel.m\n#     dataset3Params.m\n#     processEmail.m\n#     emailFeatures.m\n#\n#  For this exercise, you will not need to change any code in this file,\n#  or any other files other than those mentioned above.\n#\nfrom matplotlib import use, cm\nuse(\'TkAgg\')\nimport numpy as np\nimport scipy.io\nfrom sklearn import svm\nfrom dataset3Params import dataset3Params\nfrom plotData import plotData\nfrom visualizeBoundary import visualizeBoundary\nfrom visualizeBoundaryLinear import visualizeBoundaryLinear\n\n## =============== Part 1: Loading and Visualizing Data ================\n#  We start the exercise by first loading and visualizing the dataset. \n#  The following code will load the dataset into your environment and plot\n#  the data.\n#\n\nprint \'Loading and Visualizing Data ...\'\n\n# Load from ex6data1: \n# You will have X, y in your environment\ndata = scipy.io.loadmat(\'ex6data1.mat\')\nX = data[\'X\']\ny = data[\'y\'].flatten()\n\n# Plot training data\nplotData(X, y)\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## ==================== Part 2: Training Linear SVM ====================\n#  The following code will train a linear SVM on the dataset and plot the\n#  decision boundary learned.\n#\n\n# Load from ex6data1:\n# You will have X, y in your environment\ndata = scipy.io.loadmat(\'ex6data1.mat\')\nX = data[\'X\']\ny = data[\'y\'].flatten()\n\nprint \'Training Linear SVM ...\'\n\n# You should try to change the C value below and see how the decision\n# boundary varies (e.g., try C = 1000)\n\nC = 1\nclf = svm.SVC(C=C, kernel=\'linear\', tol=1e-3, max_iter=20)\nmodel = clf.fit(X, y)\nvisualizeBoundaryLinear(X, y, model)\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## =============== Part 3: Implementing Gaussian Kernel ===============\n#  You will now implement the Gaussian kernel to use\n#  with the SVM. You should complete the code in gaussianKernel.m\n#\nprint \'Evaluating the Gaussian Kernel ...\'\n\nx1 = np.array([1, 2, 1])\nx2 = np.array([0, 4, -1])\nsigma = 2\n# sim = gaussianKernel(x1, x2, sigma)\n#\n# print \'Gaussian Kernel between x1 = [1 2 1], x2 = [0 4 -1], sigma = %0.5f : \' \\\n#        \'\\t%f\\n(this value should be about 0.324652)\\n\' % (sigma, sim)\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## =============== Part 4: Visualizing Dataset 2 ================\n#  The following code will load the next dataset into your environment and\n#  plot the data.\n#\n\nprint \'Loading and Visualizing Data ...\'\n\n# Load from ex6data2:\n# You will have X, y in your environment\ndata = scipy.io.loadmat(\'ex6data2.mat\')\nX = data[\'X\']\ny = data[\'y\'].flatten()\n\n# Plot training data\nplotData(X, y)\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## ========== Part 5: Training SVM with RBF Kernel (Dataset 2) ==========\n#  After you have implemented the kernel, we can now use it to train the\n#  SVM classifier.\n#\nprint \'Training SVM with RBF Kernel (this may take 1 to 2 minutes) ...\'\n\n# Load from ex6data2:\n# You will have X, y in your environment\ndata = scipy.io.loadmat(\'ex6data2.mat\')\nX = data[\'X\']\ny = data[\'y\'].flatten()\n\n# SVM Parameters\nC = 1\nsigma = 0.1\ngamma = 1.0 / (2.0 * sigma ** 2)\n\n# We set the tolerance and max_passes lower here so that the code will run\n# faster. However, in practice, you will want to run the training to\n# convergence.\n\nclf = svm.SVC(C=C, kernel=\'rbf\', tol=1e-3, max_iter=200, gamma=gamma)\nmodel = clf.fit(X, y)\nvisualizeBoundary(X, y, model)\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## =============== Part 6: Visualizing Dataset 3 ================\n#  The following code will load the next dataset into your environment and\n#  plot the data.\n#\n\nprint \'Loading and Visualizing Data ...\'\n\n# Load from ex6data3:\n# You will have X, y in your environment\ndata = scipy.io.loadmat(\'ex6data3.mat\')\nX = data[\'X\']\ny = data[\'y\'].flatten()\n\n# Plot training data\nplotData(X, y)\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## ========== Part 7: Training SVM with RBF Kernel (Dataset 3) ==========\n\n#  This is a different dataset that you can use to experiment with. Try\n#  different values of C and sigma here.\n#\n\n# Load from ex6data3:\n# You will have X, y in your environment\ndata = scipy.io.loadmat(\'ex6data3.mat\')\nXval = data[\'Xval\']\nyval = data[\'yval\'].flatten()\n\n# Try different SVM Parameters here\nC, sigma = dataset3Params(X, y, Xval, yval)\ngamma = 1.0 / (2.0 * sigma ** 2)\n# Train the SVM\n\nclf = svm.SVC(C=C, kernel=\'rbf\', tol=1e-3, max_iter=200, gamma=gamma)\nmodel = clf.fit(X, y)\nvisualizeBoundary(X, y, model)\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n'"
ex6/ex6_spam.py,2,"b'## Machine Learning Online Class\n#  Exercise 6 | Spam Classification with SVMs\n#\n#  Instructions\n#  ------------\n# \n#  This file contains code that helps you get started on the\n#  exercise. You will need to complete the following functions:\n#\n#     gaussianKernel.m\n#     dataset3Params.m\n#     processEmail.m\n#     emailFeatures.m\n#\n#  For this exercise, you will not need to change any code in this file,\n#  or any other files other than those mentioned above.\n#\nimport numpy as np\nimport scipy.io\nfrom sklearn import svm\nfrom collections import OrderedDict\n\nfrom processEmail import processEmail\nfrom emailFeatures import emailFeatures\nfrom getVocabList import getVocabList\n\n## ==================== Part 1: Email Preprocessing ====================\n#  To use an SVM to classify emails into Spam v.s. Non-Spam, you first need\n#  to convert each email into a vector of features. In this part, you will\n#  implement the preprocessing steps for each email. You should\n#  complete the code in processEmail.m to produce a word indices vector\n#  for a given email.\n\nprint \'Preprocessing sample email (emailSample1.txt)\'\n\n# Extract Features\nfile = open(\'emailSample1.txt\', \'r\')\nfile_contents = file.readlines()\nword_indices  = processEmail(\'\'.join(file_contents))\n\n# Print Stats\nprint \'Word Indices: \'\nprint word_indices\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## ==================== Part 2: Feature Extraction ====================\n#  Now, you will convert each email into a vector of features in R^n.\n#  You should complete the code in emailFeatures.m to produce a feature\n#  vector for a given email.\n\nprint \'Extracting features from sample email (emailSample1.txt)\'\n\n# Extract Features\nfile = open(\'emailSample1.txt\')\nfile_contents = file.readlines()\nword_indices = processEmail(\'\'.join(file_contents))\nfeatures = emailFeatures(word_indices)\n\n# Print Stats\nprint \'Length of feature vector: %d\'% features.size\nprint \'Number of non-zero entries: %d\'% sum(features > 0)\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## =========== Part 3: Train Linear SVM for Spam Classification ========\n#  In this section, you will train a linear classifier to determine if an\n#  email is Spam or Not-Spam.\n\n# Load the Spam Email dataset\n# You will have X, y in your environment\ndata = scipy.io.loadmat(\'spamTrain.mat\')\nX = data[\'X\']\ny = data[\'y\'].flatten()\n\nprint \'Training Linear SVM (Spam Classification)\'\nprint \'(this may take 1 to 2 minutes) ...\'\n\nC = 0.1\nclf = svm.SVC(C=C, kernel=\'linear\', tol=1e-3, max_iter=200)\nmodel = clf.fit(X, y)\n\np = model.predict(X)\n\nprint \'Training Accuracy: %f\', np.mean(np.double(p == y)) * 100\n\n## =================== Part 4: Test Spam Classification ================\n#  After training the classifier, we can evaluate it on a test set. We have\n#  included a test set in spamTest.mat\n\n# Load the test dataset\n# You will have Xtest, ytest in your environment\ndata = scipy.io.loadmat(\'spamTest.mat\')\nXtest = data[\'Xtest\']\nytest = data[\'ytest\']\n\nprint \'Evaluating the trained Linear SVM on a test set ...\'\n\np = model.predict(Xtest)\n\nprint \'Test Accuracy: %f\', np. mean(np.double(p == ytest)) * 100\n\n\n## ================= Part 5: Top Predictors of Spam ====================\n#  Since the model we are training is a linear SVM, we can inspect the\n#  weights learned by the model to understand better how it is determining\n#  whether an email is spam or not. The following code finds the words with\n#  the highest weights in the classifier. Informally, the classifier\n#  \'thinks\' that these words are the most likely indicators of spam.\n#\n\n# Sort the weights and obtain the vocabulary list\n\nt = sorted(list(enumerate(model.coef_[0])),key=lambda e: e[1], reverse=True)\nd = OrderedDict(t)\nidx = d.keys()\nweight = d.values()\nvocabList = getVocabList()\n\nprint \'Top predictors of spam: \'\nfor i in range(15):\n    print \' %-15s (%f)\' %(vocabList[idx[i]], weight[i])\n\nprint \'Program paused. Press enter to continue.\'\n\n## =================== Part 6: Try Your Own Emails =====================\n#  Now that you\'ve trained the spam classifier, you can use it on your own\n#  emails! In the starter code, we have included spamSample1.txt,\n#  spamSample2.txt, emailSample1.txt and emailSample2.txt as examples.\n#  The following code reads in one of these emails and then uses your\n#  learned SVM classifier to determine whether the email is Spam or\n#  Not Spam\n\n# Set the file to be read in (change this to spamSample2.txt,\n# emailSample1.txt or emailSample2.txt to see different predictions on\n# different emails types). Try your own emails as well!\nfilename = \'spamSample1.txt\'\n\n# Read and predict\n\nfile = open(filename)\nfile_contents = file.readlines()\nword_indices = processEmail(\'\'.join(file_contents))\nx = emailFeatures(word_indices)\np = model.predict(x)\n\nprint \'Processed %s\\n\\nSpam Classification: %d\' % (filename, p)\nprint \'(1 indicates spam, 0 indicates not spam)\'\n\n'"
ex6/gaussianKernel.py,0,"b'import numpy as np\n\n\ndef gaussianKernel(x1, x2, sigma):\n    """"""returns a gaussian kernel between x1 and x2\n    and returns the value in sim\n    """"""\n\n# Ensure that x1 and x2 are column vectors\n#     x1 = x1.ravel()\n#     x2 = x2.ravel()\n\n# You need to return the following variables correctly.\n    sim = 0\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: Fill in this function to return the similarity between x1\n#               and x2 computed using a Gaussian kernel with bandwidth\n#               sigma\n#\n#\n\n\n# =============================================================\n    return sim'"
ex6/getVocabList.py,0,"b'import numpy as np\n\n\ndef getVocabList():\n\n    """"""reads the fixed vocabulary list in vocab.txt\n    and returns a cell array of the words in vocabList.\n    """"""\n\n## Read the fixed vocabulary list\n    with open(\'vocab.txt\') as f:\n\n# Store all dictionary words in cell array vocab{}\n\n# For ease of implementation, we use a struct to map the strings => integers\n# In practice, you\'ll want to use some form of hashmap\n        vocabList = []\n        for line in f:\n            idx, w = line.split()\n            vocabList.append(w)\n\n    return vocabList\n'"
ex6/linearKernel.py,0,"b'def linearKernel(x1, x2):\n    """"""returns a linear kernel between x1 and x2\n    and returns the value in sim\n    """"""\n\n# Ensure that x1 and x2 are column vectors\n    x1 = x1.ravel()\n    x2 = x2.ravel()\n\n# Compute the kernel\n    sim = x1.T.dot(x2)  # dot product\n\n    return sim\n'"
ex6/plotData.py,2,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom show import show\ndef plotData(X, y):\n    """"""plots the data points with + for the positive examples\n    and o for the negative examples. X is assumed to be a Mx2 matrix.\n\n    Note: This was slightly modified such that it expects y = 1 or y = 0\n    """"""\n    plt.figure()\n\n# Find Indices of Positive and Negative Examples\n    pos = np.where(y==1, True, False).flatten()\n    neg = np.where(y==0, True, False).flatten()\n\n# Plot Examples\n    plt.plot(X[pos,0], X[pos, 1], \'k+\', linewidth=1, markersize=7)\n    plt.plot(X[neg,0], X[neg, 1], \'ko\', color=\'y\', markersize=7)\n    show()\n\n'"
ex6/porterStemmer.py,0,"b'""""""Porter Stemming Algorithm\nThis is the Porter stemming algorithm, ported to Python from the\nversion coded up in ANSI C by the author. It may be be regarded\nas canonical, in that it follows the algorithm presented in\n\nPorter, 1980, An algorithm for suffix stripping, Program, Vol. 14,\nno. 3, pp 130-137,\n\nonly differing from it at the points maked --DEPARTURE-- below.\n\nSee also http://www.tartarus.org/~martin/PorterStemmer\n\nThe algorithm as described in the paper could be exactly replicated\nby adjusting the points of DEPARTURE, but this is barely necessary,\nbecause (a) the points of DEPARTURE are definitely improvements, and\n(b) no encoding of the Porter stemmer I have seen is anything like\nas exact as this version, even with the points of DEPARTURE!\n\nVivake Gupta (v@nano.com)\n\nRelease 1: January 2001\n\nFurther adjustments by Santiago Bruno (bananabruno@gmail.com)\nto allow word input not restricted to one word per line, leading\nto:\n\nrelease 2: July 2008\n""""""\n\nimport sys\n\nclass PorterStemmer:\n\n    def __init__(self):\n        """"""The main part of the stemming algorithm starts here.\n        b is a buffer holding a word to be stemmed. The letters are in b[k0],\n        b[k0+1] ... ending at b[k]. In fact k0 = 0 in this demo program. k is\n        readjusted downwards as the stemming progresses. Zero termination is\n        not in fact used in the algorithm.\n\n        Note that only lower case sequences are stemmed. Forcing to lower case\n        should be done before stem(...) is called.\n        """"""\n\n        self.b = """"  # buffer for word to be stemmed\n        self.k = 0\n        self.k0 = 0\n        self.j = 0   # j is a general offset into the string\n\n    def cons(self, i):\n        """"""cons(i) is TRUE <=> b[i] is a consonant.""""""\n        if self.b[i] == \'a\' or self.b[i] == \'e\' or self.b[i] == \'i\' or self.b[i] == \'o\' or self.b[i] == \'u\':\n            return 0\n        if self.b[i] == \'y\':\n            if i == self.k0:\n                return 1\n            else:\n                return (not self.cons(i - 1))\n        return 1\n\n    def m(self):\n        """"""m() measures the number of consonant sequences between k0 and j.\n        if c is a consonant sequence and v a vowel sequence, and <..>\n        indicates arbitrary presence,\n\n           <c><v>       gives 0\n           <c>vc<v>     gives 1\n           <c>vcvc<v>   gives 2\n           <c>vcvcvc<v> gives 3\n           ....\n        """"""\n        n = 0\n        i = self.k0\n        while 1:\n            if i > self.j:\n                return n\n            if not self.cons(i):\n                break\n            i = i + 1\n        i = i + 1\n        while 1:\n            while 1:\n                if i > self.j:\n                    return n\n                if self.cons(i):\n                    break\n                i = i + 1\n            i = i + 1\n            n = n + 1\n            while 1:\n                if i > self.j:\n                    return n\n                if not self.cons(i):\n                    break\n                i = i + 1\n            i = i + 1\n\n    def vowelinstem(self):\n        """"""vowelinstem() is TRUE <=> k0,...j contains a vowel""""""\n        for i in range(self.k0, self.j + 1):\n            if not self.cons(i):\n                return 1\n        return 0\n\n    def doublec(self, j):\n        """"""doublec(j) is TRUE <=> j,(j-1) contain a double consonant.""""""\n        if j < (self.k0 + 1):\n            return 0\n        if (self.b[j] != self.b[j-1]):\n            return 0\n        return self.cons(j)\n\n    def cvc(self, i):\n        """"""cvc(i) is TRUE <=> i-2,i-1,i has the form consonant - vowel - consonant\n        and also if the second c is not w,x or y. this is used when trying to\n        restore an e at the end of a short  e.g.\n\n           cav(e), lov(e), hop(e), crim(e), but\n           snow, box, tray.\n        """"""\n        if i < (self.k0 + 2) or not self.cons(i) or self.cons(i-1) or not self.cons(i-2):\n            return 0\n        ch = self.b[i]\n        if ch == \'w\' or ch == \'x\' or ch == \'y\':\n            return 0\n        return 1\n\n    def ends(self, s):\n        """"""ends(s) is TRUE <=> k0,...k ends with the string s.""""""\n        length = len(s)\n        if s[length - 1] != self.b[self.k]: # tiny speed-up\n            return 0\n        if length > (self.k - self.k0 + 1):\n            return 0\n        if self.b[self.k-length+1:self.k+1] != s:\n            return 0\n        self.j = self.k - length\n        return 1\n\n    def setto(self, s):\n        """"""setto(s) sets (j+1),...k to the characters in the string s, readjusting k.""""""\n        length = len(s)\n        self.b = self.b[:self.j+1] + s + self.b[self.j+length+1:]\n        self.k = self.j + length\n\n    def r(self, s):\n        """"""r(s) is used further down.""""""\n        if self.m() > 0:\n            self.setto(s)\n\n    def step1ab(self):\n        """"""step1ab() gets rid of plurals and -ed or -ing. e.g.\n\n           caresses  ->  caress\n           ponies    ->  poni\n           ties      ->  ti\n           caress    ->  caress\n           cats      ->  cat\n\n           feed      ->  feed\n           agreed    ->  agree\n           disabled  ->  disable\n\n           matting   ->  mat\n           mating    ->  mate\n           meeting   ->  meet\n           milling   ->  mill\n           messing   ->  mess\n\n           meetings  ->  meet\n        """"""\n        if self.b[self.k] == \'s\':\n            if self.ends(""sses""):\n                self.k = self.k - 2\n            elif self.ends(""ies""):\n                self.setto(""i"")\n            elif self.b[self.k - 1] != \'s\':\n                self.k = self.k - 1\n        if self.ends(""eed""):\n            if self.m() > 0:\n                self.k = self.k - 1\n        elif (self.ends(""ed"") or self.ends(""ing"")) and self.vowelinstem():\n            self.k = self.j\n            if self.ends(""at""):   self.setto(""ate"")\n            elif self.ends(""bl""): self.setto(""ble"")\n            elif self.ends(""iz""): self.setto(""ize"")\n            elif self.doublec(self.k):\n                self.k = self.k - 1\n                ch = self.b[self.k]\n                if ch == \'l\' or ch == \'s\' or ch == \'z\':\n                    self.k = self.k + 1\n            elif (self.m() == 1 and self.cvc(self.k)):\n                self.setto(""e"")\n\n    def step1c(self):\n        """"""step1c() turns terminal y to i when there is another vowel in the stem.""""""\n        if (self.ends(""y"") and self.vowelinstem()):\n            self.b = self.b[:self.k] + \'i\' + self.b[self.k+1:]\n\n    def step2(self):\n        """"""step2() maps double suffices to single ones.\n        so -ization ( = -ize plus -ation) maps to -ize etc. note that the\n        string before the suffix must give m() > 0.\n        """"""\n        if self.b[self.k - 1] == \'a\':\n            if self.ends(""ational""):   self.r(""ate"")\n            elif self.ends(""tional""):  self.r(""tion"")\n        elif self.b[self.k - 1] == \'c\':\n            if self.ends(""enci""):      self.r(""ence"")\n            elif self.ends(""anci""):    self.r(""ance"")\n        elif self.b[self.k - 1] == \'e\':\n            if self.ends(""izer""):      self.r(""ize"")\n        elif self.b[self.k - 1] == \'l\':\n            if self.ends(""bli""):       self.r(""ble"") # --DEPARTURE--\n            # To match the published algorithm, replace this phrase with\n            #   if self.ends(""abli""):      self.r(""able"")\n            elif self.ends(""alli""):    self.r(""al"")\n            elif self.ends(""entli""):   self.r(""ent"")\n            elif self.ends(""eli""):     self.r(""e"")\n            elif self.ends(""ousli""):   self.r(""ous"")\n        elif self.b[self.k - 1] == \'o\':\n            if self.ends(""ization""):   self.r(""ize"")\n            elif self.ends(""ation""):   self.r(""ate"")\n            elif self.ends(""ator""):    self.r(""ate"")\n        elif self.b[self.k - 1] == \'s\':\n            if self.ends(""alism""):     self.r(""al"")\n            elif self.ends(""iveness""): self.r(""ive"")\n            elif self.ends(""fulness""): self.r(""ful"")\n            elif self.ends(""ousness""): self.r(""ous"")\n        elif self.b[self.k - 1] == \'t\':\n            if self.ends(""aliti""):     self.r(""al"")\n            elif self.ends(""iviti""):   self.r(""ive"")\n            elif self.ends(""biliti""):  self.r(""ble"")\n        elif self.b[self.k - 1] == \'g\': # --DEPARTURE--\n            if self.ends(""logi""):      self.r(""log"")\n        # To match the published algorithm, delete this phrase\n\n    def step3(self):\n        """"""step3() dels with -ic-, -full, -ness etc. similar strategy to step2.""""""\n        if self.b[self.k] == \'e\':\n            if self.ends(""icate""):     self.r(""ic"")\n            elif self.ends(""ative""):   self.r("""")\n            elif self.ends(""alize""):   self.r(""al"")\n        elif self.b[self.k] == \'i\':\n            if self.ends(""iciti""):     self.r(""ic"")\n        elif self.b[self.k] == \'l\':\n            if self.ends(""ical""):      self.r(""ic"")\n            elif self.ends(""ful""):     self.r("""")\n        elif self.b[self.k] == \'s\':\n            if self.ends(""ness""):      self.r("""")\n\n    def step4(self):\n        """"""step4() takes off -ant, -ence etc., in context <c>vcvc<v>.""""""\n        if self.b[self.k - 1] == \'a\':\n            if self.ends(""al""): pass\n            else: return\n        elif self.b[self.k - 1] == \'c\':\n            if self.ends(""ance""): pass\n            elif self.ends(""ence""): pass\n            else: return\n        elif self.b[self.k - 1] == \'e\':\n            if self.ends(""er""): pass\n            else: return\n        elif self.b[self.k - 1] == \'i\':\n            if self.ends(""ic""): pass\n            else: return\n        elif self.b[self.k - 1] == \'l\':\n            if self.ends(""able""): pass\n            elif self.ends(""ible""): pass\n            else: return\n        elif self.b[self.k - 1] == \'n\':\n            if self.ends(""ant""): pass\n            elif self.ends(""ement""): pass\n            elif self.ends(""ment""): pass\n            elif self.ends(""ent""): pass\n            else: return\n        elif self.b[self.k - 1] == \'o\':\n            if self.ends(""ion"") and (self.b[self.j] == \'s\' or self.b[self.j] == \'t\'): pass\n            elif self.ends(""ou""): pass\n            # takes care of -ous\n            else: return\n        elif self.b[self.k - 1] == \'s\':\n            if self.ends(""ism""): pass\n            else: return\n        elif self.b[self.k - 1] == \'t\':\n            if self.ends(""ate""): pass\n            elif self.ends(""iti""): pass\n            else: return\n        elif self.b[self.k - 1] == \'u\':\n            if self.ends(""ous""): pass\n            else: return\n        elif self.b[self.k - 1] == \'v\':\n            if self.ends(""ive""): pass\n            else: return\n        elif self.b[self.k - 1] == \'z\':\n            if self.ends(""ize""): pass\n            else: return\n        else:\n            return\n        if self.m() > 1:\n            self.k = self.j\n\n    def step5(self):\n        """"""step5() removes a final -e if m() > 1, and changes -ll to -l if\n        m() > 1.\n        """"""\n        self.j = self.k\n        if self.b[self.k] == \'e\':\n            a = self.m()\n            if a > 1 or (a == 1 and not self.cvc(self.k-1)):\n                self.k = self.k - 1\n        if self.b[self.k] == \'l\' and self.doublec(self.k) and self.m() > 1:\n            self.k = self.k -1\n\n    def stem(self, p, i, j):\n        """"""In stem(p,i,j), p is a char pointer, and the string to be stemmed\n        is from p[i] to p[j] inclusive. Typically i is zero and j is the\n        offset to the last character of a string, (p[j+1] == \'\\0\'). The\n        stemmer adjusts the characters p[i] ... p[j] and returns the new\n        end-point of the string, k. Stemming never increases word length, so\n        i <= k <= j. To turn the stemmer into a module, declare \'stem\' as\n        extern, and delete the remainder of this file.\n        """"""\n        # copy the parameters into statics\n        self.b = p\n        self.k = j\n        self.k0 = i\n        if self.k <= self.k0 + 1:\n            return self.b # --DEPARTURE--\n\n        # With this line, strings of length 1 or 2 don\'t go through the\n        # stemming process, although no mention is made of this in the\n        # published algorithm. Remove the line to match the published\n        # algorithm.\n\n        self.step1ab()\n        self.step1c()\n        self.step2()\n        self.step3()\n        self.step4()\n        self.step5()\n        return self.b[self.k0:self.k+1]\n\n\ndef porterStemmer(word):\n    p = PorterStemmer()\n    return p.stem(word, 0, len(word)-1)\n'"
ex6/processEmail.py,0,"b'from string import lower\nfrom porterStemmer import porterStemmer\nfrom getVocabList import getVocabList\nimport re\n\ndef processEmail(email_contents):\n    """"""preprocesses a the body of an email and\n    returns a list of word_indices\n    word_indices = PROCESSEMAIL(email_contents) preprocesses\n    the body of an email and returns a list of indices of the\n    words contained in the email.\n    """"""\n\n# Load Vocabulary\n    vocabList = getVocabList()\n\n# Init return value\n    word_indices = []\n\n# ========================== Preprocess Email ===========================\n\n# Find the Headers ( \\n\\n and remove )\n# Uncomment the following lines if you are working with raw emails with the\n# full headers\n\n# hdrstart = strfind(email_contents, ([chr(10) chr(10)]))\n# email_contents = email_contents(hdrstart(1):end)\n\n# Lower case\n    email_contents = lower(email_contents)\n\n# Strip all HTML\n# Looks for any expression that starts with < and ends with > and replace\n# and does not have any < or > in the tag it with a space\n    rx = re.compile(\'<[^<>]+>|\\n\')\n    email_contents = rx.sub(\' \', email_contents)\n# Handle Numbers\n# Look for one or more characters between 0-9\n    rx = re.compile(\'[0-9]+\')\n    email_contents = rx.sub(\'number \', email_contents)\n\n# Handle URLS\n# Look for strings starting with http:// or https://\n    rx = re.compile(\'(http|https)://[^\\s]*\')\n    email_contents = rx.sub(\'httpaddr \', email_contents)\n\n# Handle Email Addresses\n# Look for strings with @ in the middle\n    rx = re.compile(\'[^\\s]+@[^\\s]+\')\n    email_contents = rx.sub(\'emailaddr \', email_contents)\n\n# Handle $ sign\n    rx = re.compile(\'[$]+\')\n    email_contents = rx.sub(\'dollar \', email_contents)\n\n# ========================== Tokenize Email ===========================\n\n# Output the email to screen as well\n    print \'==== Processed Email ====\\n\'\n\n# Process file\n    l = 0\n\n# Remove any non alphanumeric characters\n    rx = re.compile(\'[^a-zA-Z0-9 ]\')\n    email_contents = rx.sub(\'\', email_contents).split()\n\n    for str in email_contents:\n\n        # Tokenize and also get rid of any punctuation\n        # str = re.split(\'[\' + re.escape(\' @$/#.-:&*+=[]?!(){},\'\'"">_<#\')\n        #                                + chr(10) + chr(13) + \']\', str)\n\n        # Stem the word\n        # (the porterStemmer sometimes has issues, so we use a try catch block)\n        try:\n            str = porterStemmer(str.strip())\n        except:\n            str = \'\'\n            continue\n\n        # Skip the word if it is too short\n        if len(str) < 1:\n           continue\n\n        # Look up the word in the dictionary and add to word_indices if\n        # found\n        # ====================== YOUR CODE HERE ======================\n        # Instructions: Fill in this function to add the index of str to\n        #               word_indices if it is in the vocabulary. At this point\n        #               of the code, you have a stemmed word from the email in\n        #               the variable str. You should look up str in the\n        #               vocabulary list (vocabList). If a match exists, you\n        #               should add the index of the word to the word_indices\n        #               vector. Concretely, if str = \'action\', then you should\n        #               look up the vocabulary list to find where in vocabList\n        #               \'action\' appears. For example, if vocabList{18} =\n        #               \'action\', then, you should add 18 to the word_indices\n        #               vector (e.g., word_indices = [word_indices  18] ).\n        #\n        # Note: vocabList{idx} returns a the word with index idx in the\n        #       vocabulary list.\n        #\n        # Note: You can use strcmp(str1, str2) to compare two strings (str1 and\n        #       str2). It will return 1 only if the two strings are equivalent.\n        #\n\n\n\n\n        # =============================================================\n\n        # Print to screen, ensuring that the output lines are not too long\n        if (l + len(str) + 1) > 78:\n            print str\n            l = 0\n        else:\n            print str,\n            l = l + len(str) + 1\n\n# Print footer\n    print \'\\n=========================\'\n    return word_indices\n\n'"
ex6/submit.py,6,"b'import numpy as np\nimport scipy.io\n\nfrom Submission import Submission\nfrom Submission import sprintf\n\nhomework = \'support-vector-machines\'\n\npart_names = [\n  \'Gaussian Kernel\',\n  \'Parameters (C, sigma) for Dataset 3\',\n  \'Email Preprocessing\',\n  \'Email Feature Extraction\',\n  ]\n\nsrcs = [\n  \'gaussianKernel.py\',\n  \'dataset3Params.py\',\n  \'processEmail.py\',\n  \'emailFeatures.py\',\n  ]\n\n\ndef output(part_id):\n    # Random Test Cases\n    x1 = np.sin(np.arange(1,11))\n    x2 = np.cos(np.arange(1,11))\n    ec = \'the quick brown fox jumped over the lazy dog\'\n    wi = np.abs(np.round(x1 * 1863))\n    wi = np.hstack((wi, wi))\n\n    fname = srcs[part_id-1].rsplit(\'.\',1)[0]\n    mod = __import__(fname, fromlist=[fname], level=1)\n    func = getattr(mod, fname)\n\n    if part_id == 1:\n        sim = func(x1, x2, 2)\n        return sprintf(\'%0.5f \', sim)\n    elif part_id == 2:\n        data = scipy.io.loadmat(\'ex6data3.mat\')\n        X = data[\'X\']\n        y = data[\'y\'].flatten()\n        Xval = data[\'Xval\']\n        yval = data[\'yval\'].flatten()\n        C, sigma = func(X, y, Xval, yval)\n        return sprintf(\'%0.5f \', np.hstack((C, sigma)))\n    elif part_id == 3:\n        word_indices = np.array(func(ec))\n        return sprintf(\'%d \', (word_indices + 1).tolist())\n    elif part_id == 4:\n        x = func(wi)\n        return sprintf(\'%d\', x)\n\ns = Submission(homework, part_names, srcs, output)\ntry:\n    s.submit()\nexcept Exception as ex:\n    template = ""An exception of type {0} occured. Messsage:\\n{1!r}""\n    message = template.format(type(ex).__name__, ex.args)\n    print message\n'"
ex6/visualizeBoundary.py,5,"b'import numpy as np\nfrom plotData import plotData\nfrom matplotlib import pyplot as plt\n\ndef visualizeBoundary(X, y, model):\n    """"""plots a non-linear decision boundary learned by the\n    SVM and overlays the data on it""""""\n\n# Plot the training data on top of the boundary\n    plotData(X, y)\n\n    # Make classification predictions over a grid of values\n    x1plot = np.linspace(min(X[:,0]), max(X[:,0]), X.shape[0]).T\n    x2plot = np.linspace(min(X[:,1]), max(X[:,1]), X.shape[0]).T\n    X1, X2 = np.meshgrid(x1plot, x2plot)\n    vals = np.zeros(X1.shape)\n\n    for i in range(X1.shape[1]):\n        this_X = np.column_stack((X1[:, i], X2[:, i]))\n        vals[:, i] = model.predict(this_X)\n\n    # Plot the SVM boundary\n    #contour(X1, X2, vals, [0 0], \'Color\', \'b\')\n    plt.contour(X1, X2, vals, levels=[0.0, 0.0])\n'"
ex6/visualizeBoundaryLinear.py,1,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom plotData import plotData\n\n\ndef visualizeBoundaryLinear(X, y, model):\n    """"""plots a linear decision boundary\n    learned by the SVM and overlays the data on it\n    """"""\n\n    w = model.coef_.flatten()\n    b = model.intercept_.flatten()\n    xp = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)\n    yp = -(w[0]*xp + b)/w[1]\n    plotData(X, y)\n    plt.plot(xp, yp, \'-b\')\n\n'"
ex7/computeCentroids.py,0,"b'import numpy as np\n\n\ndef computeCentroids(X, idx, K):\n    """"""returns the new centroids by\n    computing the means of the data points assigned to each centroid. It is\n    given a dataset X where each row is a single data point, a vector\n    idx of centroid assignments (i.e. each entry in range [1..K]) for each\n    example, and K, the number of centroids. You should return a matrix\n    centroids, where each row of centroids is the mean of the data points\n    assigned to it.\n    """"""\n\n# Useful variables\n    m, n = X.shape\n\n# You need to return the following variables correctly.\n    centroids = []\n\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: Go over every centroid and compute mean of all points that\n#               belong to it. Concretely, the row vector centroids(i, :)\n#               should contain the mean of the data points assigned to\n#               centroid i.\n#\n# Note: You can use a for-loop over the centroids to compute this.\n# \n\n\n# =============================================================\n\n    return centroids\n'"
ex7/drawLine.py,1,"b'import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom show import show\n\ndef drawLine(p1, p2, varargin):\n    """"""Draws a line from point p1 to point p2 and holds the\n    current figure\n    """"""\n\n    plt.plot(np.column_stack(p1(1), p2(1)), np.column_stack(p1(2), p2(2)), varargin)\n    show()'"
ex7/ex7.py,2,"b'## Machine Learning Online Class\n#  Exercise 7 | Principle Component Analysis and K-Means Clustering\n#\n#  Instructions\n#  ------------\n#\n#  This file contains code that helps you get started on the\n#  exercise. You will need to complete the following functions:\n#\n#     pca.m\n#     projectData.m\n#     recoverData.m\n#     computeCentroids.m\n#     findClosestCentroids.m\n#     kMeansInitCentroids.m\n#\n#  For this exercise, you will not need to change any code in this file,\n#  or any other files other than those mentioned above.\n#\n\n## ================= Part 1: Find Closest Centroids ====================\n#  To help you implement K-Means, we have divided the learning algorithm \n#  into two functions -- findClosestCentroids and computeCentroids. In this\n#  part, you shoudl complete the code in the findClosestCentroids function. \n#\nfrom matplotlib import use, cm\nuse(\'TkAgg\')\nimport numpy as np\nimport scipy.io\nimport scipy.misc\nimport matplotlib.pyplot as plt\n\nfrom findClosestCentroids import findClosestCentroids\nfrom computeCentroids import computeCentroids\nfrom runkMeans import runkMeans\nfrom kMeansInitCentroids import kMeansInitCentroids\nfrom show import show\n\nprint \'Finding closest centroids.\'\n\n# Load an example dataset that we will be using\ndata = scipy.io.loadmat(\'ex7data2.mat\')\nX = data[\'X\']\n\n# Select an initial set of centroids\nK = 3 # 3 Centroids\ninitial_centroids = np.array([[3, 3], [6, 2], [8, 5]])\n\n# Find the closest centroids for the examples using the\n# initial_centroids\nval, idx = findClosestCentroids(X, initial_centroids)\n\nprint \'Closest centroids for the first 3 examples:\'\nprint idx[0:3].tolist()\nprint \'(the closest centroids should be 0, 2, 1 respectively)\'\n\nraw_input(""Program paused. Press Enter to continue..."") \n\n## ===================== Part 2: Compute Means =========================\n#  After implementing the closest centroids function, you should now\n#  complete the computeCentroids function.\n#\nprint \'Computing centroids means.\'\n\n#  Compute means based on the closest centroids found in the previous part.\ncentroids = computeCentroids(X, idx, K)\n\nprint \'Centroids computed after initial finding of closest centroids:\'\nfor c in centroids:\n    print c\n\nprint \'(the centroids should be\'\nprint \'   [ 2.428301 3.157924 ]\'\nprint \'   [ 5.813503 2.633656 ]\'\nprint \'   [ 7.119387 3.616684 ]\'\n\nraw_input(""Program paused. Press Enter to continue..."") \n\n\n## =================== Part 3: K-Means Clustering ======================\n#  After you have completed the two functions computeCentroids and\n#  findClosestCentroids, you have all the necessary pieces to run the\n#  kMeans algorithm. In this part, you will run the K-Means algorithm on\n#  the example dataset we have provided. \n#\nprint \'Running K-Means clustering on example dataset.\'\n\n# Load an example dataset\ndata = scipy.io.loadmat(\'ex7data2.mat\')\nX = data[\'X\']\n\n# Settings for running K-Means\nK = 3\nmax_iters = 10\n\n# For consistency, here we set centroids to specific values\n# but in practice you want to generate them automatically, such as by\n# settings them to be random examples (as can be seen in\n# kMeansInitCentroids).\ninitial_centroids = [[3, 3], [6, 2], [8, 5]]\n\n# Run K-Means algorithm. The \'true\' at the end tells our function to plot\n# the progress of K-Means\ncentroids, idx = runkMeans(X, initial_centroids, max_iters, True)\nprint \'K-Means Done.\'\n\nraw_input(""Program paused. Press Enter to continue..."") \n\n## ============= Part 4: K-Means Clustering on Pixels ===============\n#  In this exercise, you will use K-Means to compress an image. To do this,\n#  you will first run K-Means on the colors of the pixels in the image and\n#  then you will map each pixel on to it\'s closest centroid.\n#  \n#  You should now complete the code in kMeansInitCentroids.m\n#\n\nprint \'Running K-Means clustering on pixels from an image.\'\n\n#  Load an image of a bird\nA = scipy.misc.imread(\'bird_small.png\')\n\n# If imread does not work for you, you can try instead\n#   load (\'bird_small.mat\')\n\nA = A / 255.0 # Divide by 255 so that all values are in the range 0 - 1\n\n# Size of the image\nimg_size = A.shape\n\n# Reshape the image into an Nx3 matrix where N = number of pixels.\n# Each row will contain the Red, Green and Blue pixel values\n# This gives us our dataset matrix X that we will use K-Means on.\nX = A.reshape(img_size[0] * img_size[1], 3)\n\n# Run your K-Means algorithm on this data\n# You should try different values of K and max_iters here\nK = 16 \nmax_iters = 10\n\n# When using K-Means, it is important the initialize the centroids\n# randomly. \n# You should complete the code in kMeansInitCentroids.m before proceeding\ninitial_centroids = kMeansInitCentroids(X, K)\n\n# Run K-Means\ncentroids, idx = runkMeans(X, initial_centroids, max_iters)\n\nraw_input(""Program paused. Press Enter to continue..."") \n\n\n## ================= Part 5: Image Compression ======================\n#  In this part of the exercise, you will use the clusters of K-Means to\n#  compress an image. To do this, we first find the closest clusters for\n#  each example. After that, we \n\nprint \'Applying K-Means to compress an image.\'\n\n# Find closest cluster members\n_, idx = findClosestCentroids(X, centroids)\n\n# Essentially, now we have represented the image X as in terms of the\n# indices in idx. \n\n# We can now recover the image from the indices (idx) by mapping each pixel\n# (specified by it\'s index in idx) to the centroid value\nX_recovered = np.array([centroids[e] for e in idx])\n\n# Reshape the recovered image into proper dimensions\nX_recovered = X_recovered.reshape(img_size[0], img_size[1], 3)\n\n# Display the original image \nplt.subplot(1, 2, 1)\nplt.imshow(A)\nplt.title(\'Original\')\nshow()\n\n# Display compressed image side by side\nplt.subplot(1, 2, 2)\nplt.imshow(X_recovered)\nplt.title(\'Compressed, with %d colors.\' % K)\nshow()\n\nraw_input(""Program paused. Press Enter to continue..."")'"
ex7/ex7_pca.py,4,"b'## Machine Learning Online Class\n#  Exercise 7 | Principle Component Analysis and K-Means Clustering\n#\n#  Instructions\n#  ------------\n#\n#  This file contains code that helps you get started on the\n#  exercise. You will need to complete the following functions:\n#\n#     pca.m\n#     projectData.m`\n#     recoverData.m\n#     computeCentroids.m\n#     findClosestCentroids.m\n#     kMeansInitCentroids.m\n#\n#  For this exercise, you will not need to change any code in this file,\n#  or any other files other than those mentioned above.\n\nfrom matplotlib import use\nuse(\'TkAgg\')\nimport numpy as np\nimport scipy.io\nimport scipy.misc\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom featureNormalize import featureNormalize\nfrom pca import pca\nfrom projectData import projectData\nfrom recoverData import recoverData\nfrom kMeansInitCentroids import kMeansInitCentroids\nfrom runkMeans import runkMeans\nfrom plotDataPoints import plotDataPoints\nfrom ex3.displayData import displayData\nfrom show import show\n\n## ================== Part 1: Load Example Dataset  ===================\n#  We start this exercise by using a small dataset that is easily to\n#  visualize\n\nprint \'Visualizing example dataset for PCA.\'\n#  The following command loads the dataset. You should now have the \n#  variable X in your environment\ndata = scipy.io.loadmat(\'ex7data1.mat\')\nX = data[\'X\']\n\n#  Visualize the example dataset\nplt.scatter(X[:, 0], X[:, 1], marker=\'o\', color=\'b\', facecolors=\'none\', lw=1.0)\nplt.axis([0.5, 6.5, 2, 8])\nplt.axis(\'equal\')\nshow()\n\nraw_input(\'Program paused. Press Enter to continue...\')  \n\n## =============== Part 2: Principal Component Analysis ===============\n#  You should now implement PCA, a dimension reduction technique. You\n#  should complete the code in pca.m\n#\nprint \'Running PCA on example dataset.\'\n\n#  Before running PCA, it is important to first normalize X\nX_norm, mu, sigma = featureNormalize(X)\n\n#  Run PCA\nU, S, V = pca(X_norm)\n\n#  Compute mu, the mean of the each feature\n\n#  Draw the eigenvectors centered at mean of data. These lines show the\n#  directions of maximum variations in the dataset.\nmu2 = mu + 1.5 * S.dot(U.T)\nplt.plot([mu[0], mu2[0, 0]], [mu[1], mu2[0, 1]], \'-k\', lw=2)\nplt.plot([mu[0], mu2[1, 0]], [mu[1], mu2[1, 1]], \'-k\', lw=2)\nshow()\n\nprint \'Top eigenvector: \'\nprint \' U(:,1) = %f %f \', U[0,0], U[1,0]\nprint \'(you should expect to see -0.707107 -0.707107)\'\n\nraw_input(\'Program paused. Press Enter to continue...\')  \n\n\n## =================== Part 3: Dimension Reduction ===================\n#  You should now implement the projection step to map the data onto the \n#  first k eigenvectors. The code will then plot the data in this reduced \n#  dimensional space.  This will show you what the data looks like when \n#  using only the corresponding eigenvectors to reconstruct it.\n#\n#  You should complete the code in projectData.m\n#\nprint \'Dimension reduction on example dataset.\'\n\n#  Plot the normalized dataset (returned from pca)\nplt.figure()\nplt.scatter(X_norm[:, 0], X_norm[:, 1], marker=\'o\', color=\'b\', facecolors=\'none\', lw=1.0)\nplt.axis([-4, 3, -4, 3]) #axis square\nplt.axis(\'equal\')\nshow()\n\n#  Project the data onto K = 1 dimension\nK = 1\nZ = projectData(X_norm, U, K)\nprint \'Projection of the first example: %f\', Z[0]\nprint \'(this value should be about 1.481274)\'\n\nX_rec  = recoverData(Z, U, K)\nprint \'Approximation of the first example: %f %f\'% (X_rec[0, 0], X_rec[0, 1])\nprint \'(this value should be about  -1.047419 -1.047419)\'\n\n#  Draw lines connecting the projected points to the original points\nplt.scatter(X_rec[:, 0], X_rec[:, 1], marker=\'o\', color=\'r\', facecolor=\'none\', lw=1.0)\nfor i in range(len(X_norm)):\n    plt.plot([X_norm[i, 0], X_rec[i, 0]], [X_norm[i, 1], X_rec[i, 1]], \'--k\')\n\nshow()\nraw_input(\'Program paused. Press Enter to continue...\')  \n\n## =============== Part 4: Loading and Visualizing Face Data =============\n#  We start the exercise by first loading and visualizing the dataset.\n#  The following code will load the dataset into your environment\n#\nprint \'Loading face dataset.\'\n\n#  Load Face dataset\ndata = scipy.io.loadmat(\'ex7faces.mat\')\nX = data[\'X\']\n\n#  Display the first 100 faces in the dataset\ndisplayData(X[0:100, :])\n\nraw_input(\'Program paused. Press Enter to continue...\')  \n\n## =========== Part 5: PCA on Face Data: Eigenfaces  ===================\n#  Run PCA and visualize the eigenvectors which are in this case eigenfaces\n#  We display the first 36 eigenfaces.\n#\nprint \'Running PCA on face dataset.\\n(this might take a minute or two ...)\\n\\n\'\n\n#  Before running PCA, it is important to first normalize X by subtracting \n#  the mean value from each feature\nX_norm, mu, sigma = featureNormalize(X)\n\n#  Run PCA\nU, S, V = pca(X_norm)\n\n#  Visualize the top 36 eigenvectors found\ndisplayData(U[:, 1:36].T)\n\nraw_input(\'Program paused. Press Enter to continue...\')  \n\n## ============= Part 6: Dimension Reduction for Faces =================\n#  Project images to the eigen space using the top k eigenvectors \n#  If you are applying a machine learning algorithm \nprint \'Dimension reduction for face dataset.\'\n\nK = 100\nZ = projectData(X_norm, U, K)\n\nprint \'The projected data Z has a size of: \'\nprint \'%d %d\'% Z.shape\n\nraw_input(\'Program paused. Press Enter to continue...\')  \n\n## ==== Part 7: Visualization of Faces after PCA Dimension Reduction ====\n#  Project images to the eigen space using the top K eigen vectors and \n#  visualize only using those K dimensions\n#  Compare to the original input, which is also displayed\n\nprint \'Visualizing the projected (reduced dimension) faces.\'\n\nK = 100\nX_rec  = recoverData(Z, U, K)\n\n# Display normalized data\nplt.subplot(1, 2, 1)\ndisplayData(X_norm[:100,:])\nplt.title(\'Original faces\')\nplt.axis(\'equal\')\n\n# Display reconstructed data from only k eigenfaces\nplt.subplot(1, 2, 2)\ndisplayData(X_rec[:100,:])\nplt.title(\'Recovered faces\')\nplt.axis(\'equal\')\nshow()\nraw_input(\'Program paused. Press Enter to continue...\')  \n\n\n## === Part 8(a): Optional (ungraded) Exercise: PCA for Visualization ===\n#  One useful application of PCA is to use it to visualize high-dimensional\n#  data. In the last K-Means exercise you ran K-Means on 3-dimensional \n#  pixel colors of an image. We first visualize this output in 3D, and then\n#  apply PCA to obtain a visualization in 2D.\n\n# Re-load the image from the previous exercise and run K-Means on it\n# For this to work, you need to complete the K-Means assignment first\nA = scipy.misc.imread(\'bird_small.png\')\n\n# If imread does not work for you, you can try instead\n#   load (\'bird_small.mat\')\n\nA = A / 255.0\nimg_size = A.shape\nX = A.reshape(img_size[0] * img_size[1], 3)\nK = 16 \nmax_iters = 10\ninitial_centroids = kMeansInitCentroids(X, K)\ncentroids, idx = runkMeans(X, initial_centroids, max_iters)\n\n#  Sample 1000 random indexes (since working with all the data is\n#  too expensive. If you have a fast computer, you may increase this.\nsel = np.floor(np.random.random(1000) * len(X)) + 1\n\n#  Setup Color Palette\n\n#  Visualize the data and centroid memberships in 3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\'3d\')\nXs = np.array([X[s] for s in sel])\nxs = Xs[:, 0]\nys = Xs[:, 1]\nzs = Xs[:, 2]\ncmap = plt.get_cmap(""jet"")\nidxn = sel.astype(\'float\')/max(sel.astype(\'float\'))\ncolors = cmap(idxn)\n# ax = Axes3D(fig)\nax.scatter3D(xs, ys, zs=zs, edgecolors=colors, marker=\'o\', facecolors=\'none\', lw=0.4, s=10)\n\nplt.title(\'Pixel dataset plotted in 3D. Color shows centroid memberships\')\nshow()\nraw_input(\'Program paused. Press Enter to continue...\')\n\n## === Part 8(b): Optional (ungraded) Exercise: PCA for Visualization ===\n# Use PCA to project this cloud to 2D for visualization\n\n# Subtract the mean to use PCA\nX_norm, mu, sigma = featureNormalize(X)\n\n# PCA and project the data to 2D\nU, S, V = pca(X_norm)\nZ = projectData(X_norm, U, 2)\n\n# Plot in 2D\nplt.figure()\nzs = np.array([Z[s] for s in sel])\nidxs = np.array([idx[s] for s in sel])\n\n# plt.scatter(zs[:,0], zs[:,1])\nplotDataPoints(zs, idxs)\nplt.title(\'Pixel dataset plotted in 2D, using PCA for dimensionality reduction\')\nshow()\nraw_input(\'Program paused. Press Enter to continue...\')\n'"
ex7/featureNormalize.py,2,"b'import numpy as np\n\n\ndef featureNormalize(X):\n    """"""\n    returns a normalized version of X where\n    the mean value of each feature is 0 and the standard deviation\n    is 1. This is often a good preprocessing step to do when\n    working with learning algorithms.\n    """"""\n\n    mu = np.mean(X, axis=0)\n    X_norm = X - mu\n\n    sigma = np.std(X_norm, axis=0, ddof=1)\n    X_norm = X_norm / sigma\n\n    return X_norm, mu, sigma'"
ex7/findClosestCentroids.py,1,"b'import numpy as np\n\n\ndef findClosestCentroids(X, centroids):\n    """"""returns the closest centroids\n    in idx for a dataset X where each row is a single example. idx = m x 1\n    vector of centroid assignments (i.e. each entry in range [1..K])\n    """"""\n\n# Set K\n    K = len(centroids)\n\n# You need to return the following variables correctly.\n    idx = np.zeros(X.shape[0])\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: Go over every example, find its closest centroid, and store\n#               the index inside idx at the appropriate location.\n#               Concretely, idx(i) should contain the index of the centroid\n#               closest to example i. Hence, it should be a value in the \n#               range 1..K\n#\n# Note: You can use a for-loop over the examples to compute this.\n\n\n# =============================================================\n\n    return val, idx\n\n'"
ex7/kMeansInitCentroids.py,1,"b'import numpy as np\n\n\ndef kMeansInitCentroids(X, K):\n    """"""returns K initial centroids to be\n    used with the K-Means on the dataset X\n    """"""\n\n# You should return this values correctly\n    centroids = np.zeros((K, X.shape[1]))\n\n# ====================== YOUR CODE HERE ======================\n# Instructions: You should set centroids to randomly chosen examples from\n#               the dataset X\n#\n\n\n# =============================================================\n    return centroids\n'"
ex7/pca.py,0,"b'import numpy as np\n\n\ndef pca(X):\n    """"""computes eigenvectors of the covariance matrix of X\n      Returns the eigenvectors U, the eigenvalues (on diagonal) in S\n    """"""\n\n    # Useful values\n    m, n = X.shape\n\n    # You need to return the following variables correctly.\n\n    # ====================== YOUR CODE HERE ======================\n    # Instructions: You should first compute the covariance matrix. Then, you\n    #               should use the ""svd"" function to compute the eigenvectors\n    #               and eigenvalues of the covariance matrix.\n    #\n    # Note: When computing the covariance matrix, remember to divide by m (the\n    #       number of examples).\n    #\n\n\n# =========================================================================\n    return U, S, V'"
ex7/plotDataPoints.py,1,"b'import matplotlib.pyplot as plt\n\nfrom show import show\n\ndef plotDataPoints(X, idx):\n\n    """"""plots data points in X, coloring them so that those\n    with the same index assignments in idx have the same color\n    """"""\n    pass\n    # Create palette\n    # palette = hsv(K + 1)\n    # colors = palette(idx, :)\n    #\n    # # Plot the data\n\n    # c = dict(enumerate(np.eye(3)))\n    # colors=idx\n    map = plt.get_cmap(""jet"")\n    idxn = idx.astype(\'float\')/max(idx.astype(\'float\'))\n    colors = map(idxn)\n    plt.scatter(X[:, 0], X[:, 1], 15, edgecolors=colors, marker=\'o\', facecolors=\'none\', lw=0.5)\n    show()'"
ex7/plotProgresskMeans.py,0,"b'import matplotlib.pyplot as plt\n\nfrom plotDataPoints import plotDataPoints\nfrom show import show\n\ndef plotProgresskMeans(X, centroids, previous, idx, K, i, color):\n    """"""plots the data\n    points with colors assigned to each centroid. With the previous\n    centroids, it also plots a line between the previous locations and\n    current locations of the centroids.\n    """"""\n\n# Plot the examples\n    plotDataPoints(X, idx)\n\n# Plot the centroids as black x\'s\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker=\'x\', s=60, lw=3, edgecolor=\'k\')\n\n# Plot the history of the centroids with lines\n    for j in range(len(centroids)):\n        plt.plot([centroids[j,0], previous[j,0]],\n                 [centroids[j,1], previous[j,1]], c=color)\n\n# Title\n    plt.title(\'Iteration number %d\' % i)\n    show()\n    raw_input(""Program paused. Press Enter to continue..."")\n\n'"
ex7/projectData.py,0,"b'def projectData(X, U, K):\n    """"""computes the projection of\n    the normalized inputs X into the reduced dimensional space spanned by\n    the first K columns of U. It returns the projected examples in Z.\n    """"""\n\n    # ====================== YOUR CODE HERE ======================\n    # Instructions: Compute the projection of the data using only the top K\n    #               eigenvectors in U (first K columns).\n    #               For the i-th example X(i,:), the projection on to the k-th\n    #               eigenvector is given as follows:\n    #                    x = X(i, :)\'\n    #                    projection_k = x\' * U(:, k)\n    #\n\n\n\n    # =============================================================\n\n\n    return Z\n'"
ex7/recoverData.py,0,"b'def recoverData(Z, U, K):\n    """"""\n    recovers an approximation the\n    original data that has been reduced to K dimensions. It returns the\n    approximate reconstruction in X_rec.\n    """"""\n\n\n    # ====================== YOUR CODE HERE ======================\n    # Instructions: Compute the approximation of the data by projecting back\n    #               onto the original space using the top K eigenvectors in U.\n    #\n    #               For the i-th example Z(i,:), the (approximate)\n    #               recovered data for dimension j is given as follows:\n    #                    v = Z(i, :)\'\n    #                    recovered_j = v\' * U(j, 1:K)\'\n    #\n    #               Notice that U(j, 1:K) is a row vector.\n    #\n\n\n    # =============================================================\n\n    return X_rec\n'"
ex7/runkMeans.py,4,"b'from computeCentroids import computeCentroids\nfrom plotProgresskMeans import plotProgresskMeans\nfrom findClosestCentroids import findClosestCentroids\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport itertools\n\n\ndef runkMeans(X, initial_centroids, max_iters, plot_progress=False):\n    """"""runs the K-Means algorithm on data matrix X, where each\n    row of X is a single example. It uses initial_centroids used as the\n    initial centroids. max_iters specifies the total number of interactions\n    of K-Means to execute. plot_progress is a true/false flag that\n    indicates if the function should also plot its progress as the\n    learning happens. This is set to false by default. runkMeans returns\n    centroids, a Kxn matrix of the computed centroids and idx, a m x 1\n    vector of centroid assignments (i.e. each entry in range [1..K])\n    """"""\n\n# Plot the data if we are plotting progress\n    if plot_progress:\n        plt.figure()\n\n# Initialize values\n    m, n = X.shape\n    K = len(initial_centroids)\n    centroids = initial_centroids\n    previous_centroids = centroids\n    idx = np.zeros(m)\n    c = itertools.cycle(\'012\')\n    rgb = np.eye(3)\n# Run K-Means\n    for i in range(max_iters):\n    \n        # Output progress\n        print \'K-Means iteration %d/%d...\' % (i, max_iters)\n\n        # For each example in X, assign it to the closest centroid\n        _, idx = findClosestCentroids(X, centroids)\n    \n        # Optionally, plot progress here\n        if plot_progress:\n            color = rgb[int(next(c))]\n            plotProgresskMeans(X, np.array(centroids),\n                               np.array(previous_centroids), idx, K, i, color)\n            previous_centroids = centroids\n            # raw_input(""Press Enter to continue..."")\n\n    # Given the memberships, compute new centroids\n        centroids = computeCentroids(X, idx, K)\n\n# Hold off if we are plotting progress\n    if plot_progress:\n        pass\n    # hold off\n    return centroids, idx\n'"
ex7/submit.py,4,"b'import numpy as np\n\nfrom Submission import Submission\nfrom Submission import sprintf\n\nhomework = \'k-means-clustering-and-pca\'\n\npart_names = [\n  \'Find Closest Centroids (k-Means)\',\n  \'Compute Centroid Means (k-Means)\',\n  \'PCA\',\n  \'Project Data (PCA)\',\n  \'Recover Data (PCA)\',\n  ]\n\nsrcs = [\n  \'findClosestCentroids.py\',\n  \'computeCentroids.py\',\n  \'pca.py\',\n  \'projectData.py\',\n  \'recoverData\'\n  ]\n\n\ndef output(part_id):\n    # Random Test Cases\n    X = np.sin(np.arange(1,166)).reshape((11, 15)).T\n    Z = np.cos(np.arange(1,122)).reshape((11, 11)).T\n    C = Z[:5, :]\n    idx = (np.mod(np.arange(1,16), 3)).T\n\n    fname = srcs[part_id-1].rsplit(\'.\',1)[0]\n    mod = __import__(fname, fromlist=[fname], level=1)\n    func = getattr(mod, fname)\n\n    if part_id == 1:\n        idx = func(X, C)\n        return sprintf(\'%0.5f \', idx[1]+1)\n    elif part_id == 2:\n        centroids = func(X, idx, 3)\n        return sprintf(\'%0.5f \', centroids)\n    elif part_id == 3:\n        U, S, V = func(X)\n        return sprintf(\'%0.5f \', abs(np.hstack((U.T.flatten(), S.T.flatten()))))\n    elif part_id == 4:\n        X_proj = func(X, Z, 5)\n        return sprintf(\'%0.5f \', X_proj.T.flatten())\n    elif part_id == 5:\n        X_rec = func(X[:, :5], Z, 5)\n        return sprintf(\'%0.5f \', X_rec.T.flatten())\n\ns = Submission(homework, part_names, srcs, output)\ntry:\n    s.submit()\nexcept Exception as ex:\n    template = ""An exception of type {0} occured. Messsage:\\n{1!r}""\n    message = template.format(type(ex).__name__, ex.args)\n    print message\n'"
ex8/checkCostFunction.py,10,"b'import numpy as np\nfrom ex4.computeNumericalGradient import computeNumericalGradient\nfrom cofiCostFunc import cofiCostFunc\n\ndef checkCostFunction(Lambda=0):\n    """"""Creates a collaborative filering problem\n    to check your cost function and gradients, it will output the\n    analytical gradients produced by your code and the numerical gradients\n    (computed using computeNumericalGradient). These two gradient\n    computations should result in very similar values.\n    """"""\n\n    ## Create small problem\n    X_t = np.random.rand(4, 3)\n    Theta_t = np.random.rand(5, 3)\n\n    # Zap out most entries\n    Y = X_t.dot(Theta_t.T)\n    Y[np.where(np.random.random_sample(Y.shape) > 0.5, True, False)] = 0\n    R = np.zeros(Y.shape)\n    R[np.where(Y != 0, True, False)] = 1\n\n    ## Run Gradient Checking\n    X = np.random.random_sample(X_t.shape)\n    Theta = np.random.random_sample(Theta_t.shape)\n    num_users = Y.shape[1]\n    num_movies = Y.shape[0]\n    num_features = Theta_t.shape[1]\n\n   # Unroll parameters\n    params = np.hstack((X.T.flatten(), Theta.T.flatten()))\n\n    costFunc = lambda t: cofiCostFunc(t, Y, R, num_users, num_movies, num_features, Lambda)\n\n    def costFunc_w(t):\n        Jgrad = costFunc(t)\n        return Jgrad\n\n    numgrad = computeNumericalGradient(costFunc_w, params)\n\n    cost, grad = cofiCostFunc(params, Y, R, num_users, num_movies, num_features, Lambda)\n\n\n    print np.column_stack((numgrad, grad))\n\n    print \'The above two columns you get should be very similar.\\n\' \\\n             \'(Left-Your Numerical Gradient, Right-Analytical Gradient)\\n\\n\'\n\n    diff = np.linalg.norm(numgrad-grad)/np.linalg.norm(numgrad+grad)\n\n    print \'If your backpropagation implementation is correct, then\\n \' \\\n          \'the relative difference will be small (less than 1e-9). \\n\' \\\n          \'\\nRelative Difference: %g\\n\' % diff\n\n'"
ex8/cofiCostFunc.py,5,"b'import numpy as np\n\n\ndef cofiCostFunc(params, Y, R, num_users, num_movies, num_features, Lambda):\n    """"""returns the cost and gradient for the\n    """"""\n\n    # Unfold the U and W matrices from params\n    X = np.array(params[:num_movies*num_features]).reshape(num_features, num_movies).T.copy()\n    Theta = np.array(params[num_movies*num_features:]).reshape(num_features, num_users).T.copy()\n\n\n    # You need to return the following values correctly\n    J = 0\n    X_grad = np.zeros(X.shape)\n    Theta_grad = np.zeros(Theta.shape)\n\n    # ====================== YOUR CODE HERE ======================\n    # Instructions: Compute the cost function and gradient for collaborative\n    #               filtering. Concretely, you should first implement the cost\n    #               function (without regularization) and make sure it is\n    #               matches our costs. After that, you should implement the\n    #               gradient and use the checkCostFunction routine to check\n    #               that the gradient is correct. Finally, you should implement\n    #               regularization.\n    #\n    # Notes: X - num_movies  x num_features matrix of movie features\n    #        Theta - num_users  x num_features matrix of user features\n    #        Y - num_movies x num_users matrix of user ratings of movies\n    #        R - num_movies x num_users matrix, where R(i, j) = 1 if the\n    #            i-th movie was rated by the j-th user\n    #\n    # You should set the following variables correctly:\n    #\n    #        X_grad - num_movies x num_features matrix, containing the\n    #                 partial derivatives w.r.t. to each element of X\n    #        Theta_grad - num_users x num_features matrix, containing the\n    #                     partial derivatives w.r.t. to each element of Theta\n\n\n    # =============================================================\n\n    grad = np.hstack((X_grad.T.flatten(),Theta_grad.T.flatten()))\n\n    return J, grad\n'"
ex8/estimateGaussian.py,0,"b'import numpy as np\n\n\ndef estimateGaussian(X):\n    """"""\n    This function estimates the parameters of a\n    Gaussian distribution using the data in X\n      The input X is the dataset with each n-dimensional data point in one row\n      The output is an n-dimensional vector mu, the mean of the data set\n      and the variances sigma^2, an n x 1 vector\n    """"""\n    m = len(X)\n\n    # ====================== YOUR CODE HERE ======================\n    # Instructions: Compute the mean of the data and the variances\n    #               In particular, mu(i) should contain the mean of\n    #               the data for the i-th feature and sigma2(i)\n    #               should contain variance of the i-th feature.\n    #\n\n\n\n# =============================================================\n\n    return mu, sigma2\n\n\n'"
ex8/ex8.py,1,"b'from matplotlib import use, cm\nuse(\'TkAgg\')\nimport numpy as np\nimport scipy.io\nimport matplotlib.pyplot as plt\n\nfrom estimateGaussian import estimateGaussian\nfrom selectThreshold import selectThreshold\nfrom multivariateGaussian import multivariateGaussian\nfrom visualizeFit import visualizeFit\nfrom show import show\n\n## Machine Learning Online Class\n#  Exercise 8 | Anomaly Detection and Collaborative Filtering\n#\n#  Instructions\n#  ------------\n#\n#  This file contains code that helps you get started on the\n#  exercise. You will need to complete the following functions:\n#\n#     estimateGaussian.m\n#     selectThreshold.m\n#     cofiCostFunc.m\n#\n#  For this exercise, you will not need to change any code in this file,\n#  or any other files other than those mentioned above.\n#\n\n## ================== Part 1: Load Example Dataset  ===================\n#  We start this exercise by using a small dataset that is easy to\n#  visualize.\n#\n#  Our example case consists of 2 network server statistics across\n#  several machines: the latency and throughput of each machine.\n#  This exercise will help us find possibly faulty (or very fast) machines.\n#\n\nprint \'Visualizing example dataset for outlier detection.\'\n\n#  The following command loads the dataset. You should now have the\n#  variables X, Xval, yval in your environment\ndata = scipy.io.loadmat(\'ex8data1.mat\')\nX = data[\'X\']\nXval = data[\'Xval\']\nyval = data[\'yval\'].flatten()\n\n#  Visualize the example dataset\nplt.plot(X[:, 0], X[:, 1], \'bx\')\nplt.axis([0, 30, 0, 30])\nplt.xlabel(\'Latency (ms)\')\nplt.ylabel(\'Throughput (mb/s)\')\nshow()\nraw_input(""Program paused. Press Enter to continue..."")  \n\n\n## ================== Part 2: Estimate the dataset statistics ===================\n#  For this exercise, we assume a Gaussian distribution for the dataset.\n#\n#  We first estimate the parameters of our assumed Gaussian distribution, \n#  then compute the probabilities for each of the points and then visualize \n#  both the overall distribution and where each of the points falls in \n#  terms of that distribution.\n#\nprint \'Visualizing Gaussian fit.\'\n\n#  Estimate my and sigma2\nmu, sigma2 = estimateGaussian(X)\n\n#  Returns the density of the multivariate normal at each data point (row) \n#  of X\np = multivariateGaussian(X, mu, sigma2)\n\n#  Visualize the fit\nvisualizeFit(X,  mu, sigma2)\nplt.xlabel(\'Latency (ms)\')\nplt.ylabel(\'Throughput (mb/s)\')\nshow()\n\nraw_input(""Program paused. Press Enter to continue..."")  \n\n## ================== Part 3: Find Outliers ===================\n#  Now you will find a good epsilon threshold using a cross-validation set\n#  probabilities given the estimated Gaussian distribution\n# \n\npval = multivariateGaussian(Xval, mu, sigma2)\n\nepsilon, F1 = selectThreshold(yval, pval)\nprint \'Best epsilon found using cross-validation: %e\' % epsilon\nprint \'Best F1 on Cross Validation Set:  %f\' % F1\nprint \'   (you should see a value epsilon of about 8.99e-05)\'\n\n#  Find the outliers in the training set and plot the\noutliers = np.where(p < epsilon, True, False)\n\n#  Draw a red circle around those outliers\nplt.plot(X[outliers, 0], X[outliers, 1], \'ro\', lw=2, markersize=10, fillstyle=\'none\', markeredgewidth=1)\nshow()\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## ================== Part 4: Multidimensional Outliers ===================\n#  We will now use the code from the previous part and apply it to a \n#  harder problem in which more features describe each datapoint and only \n#  some features indicate whether a point is an outlier.\n#\n\n#  Loads the second dataset. You should now have the\n#  variables X, Xval, yval in your environment\ndata = scipy.io.loadmat(\'ex8data2.mat\')\nX = data[\'X\']\nXval = data[\'Xval\']\nyval = data[\'yval\'].flatten()\n\n#  Apply the same steps to the larger dataset\nmu, sigma2 = estimateGaussian(X)\n\n#  Training set \np = multivariateGaussian(X, mu, sigma2)\n\n#  Cross-validation set\npval = multivariateGaussian(Xval, mu, sigma2)\n\n#  Find the best threshold\nepsilon, F1 = selectThreshold(yval, pval)\n\nprint \'Best epsilon found using cross-validation: %e\' % epsilon\nprint \'Best F1 on Cross Validation Set:  %f\' % F1\nprint \'# Outliers found: %d\' % sum(p < epsilon)\nprint \'   (you should see a value epsilon of about 1.38e-18)\'\n\n\n\n'"
ex8/ex8_cofi.py,10,"b'## Machine Learning Online Class\n#  Exercise 8 | Anomaly Detection and Collaborative Filtering\n#\n#  Instructions\n#  ------------\n#\n#  This file contains code that helps you get started on the\n#  exercise. You will need to complete the following functions:\n#\n#     estimateGaussian.m\n#     selectThreshold.m\n#     cofiCostFunc.m\n#\n#  For this exercise, you will not need to change any code in this file,\n#  or any other files other than those mentioned above.\n#\nfrom matplotlib import use, cm\nuse(\'TkAgg\')\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.io\nfrom scipy.optimize import minimize\nfrom show import show\n\n## =============== Part 1: Loading movie ratings dataset ================\n#  You will start by loading the movie ratings dataset to understand the\n#  structure of the data.\n#\nfrom cofiCostFunc import cofiCostFunc\nfrom checkCostFunction import checkCostFunction\nfrom loadMovieList import loadMovieList\nfrom normalizeRatings import normalizeRatings\n\nprint \'Loading movie ratings dataset.\'\n\n#  Load data\ndata = scipy.io.loadmat(\'ex8_movies.mat\')\nY = data[\'Y\']\nR = data[\'R\'].astype(bool)\n#  Y is a 1682x943 matrix, containing ratings (1-5) of 1682 movies on \n#  943 users\n#\n#  R is a 1682x943 matrix, where R(i,j) = 1 if and only if user j gave a\n#  rating to movie i\n\n#  From the matrix, we can compute statistics like average rating.\nprint \'Average rating for movie 1 (Toy Story): %f / 5\' % np.mean(Y[0, R[0, :]])\n\n#  We can ""visualize"" the ratings matrix by plotting it with imagesc\n\nplt.figure()\nplt.imshow(Y, aspect=\'equal\', origin=\'upper\', extent=(0, Y.shape[1], 0, Y.shape[0]/2.0))\nplt.ylabel(\'Movies\')\nplt.xlabel(\'Users\')\nshow()\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n## ============ Part 2: Collaborative Filtering Cost Function ===========\n#  You will now implement the cost function for collaborative filtering.\n#  To help you debug your cost function, we have included set of weights\n#  that we trained on that. Specifically, you should complete the code in \n#  cofiCostFunc.m to return J.\n\n#  Load pre-trained weights (X, Theta, num_users, num_movies, num_features)\ndata = scipy.io.loadmat(\'ex8_movieParams.mat\')\nX = data[\'X\']\nTheta = data[\'Theta\']\nnum_users = data[\'num_users\']\nnum_movies = data[\'num_movies\']\nnum_features = data[\'num_features\']\n\n#  Reduce the data set size so that this runs faster\nnum_users = 4\nnum_movies = 5\nnum_features = 3\nX = X[:num_movies, :num_features]\nTheta = Theta[:num_users, :num_features]\nY = Y[:num_movies, :num_users]\nR = R[:num_movies, :num_users]\n\n#  Evaluate cost function\nJ, grad = cofiCostFunc(np.hstack((X.T.flatten(), Theta.T.flatten())), Y, R, num_users, num_movies,\n               num_features, 0)\n           \nprint \'Cost at loaded parameters: %f \\n(this value should be about 22.22)\' % J\n\nraw_input(""Program paused. Press Enter to continue..."")  \n\n\n## ============== Part 3: Collaborative Filtering Gradient ==============\n#  Once your cost function matches up with ours, you should now implement \n#  the collaborative filtering gradient function. Specifically, you should \n#  complete the code in cofiCostFunc.m to return the grad argument.\n#  \nprint \'Checking Gradients (without regularization) ...\'\n\n#  Check gradients by running checkNNGradients\ncheckCostFunction()\n\nraw_input(""Program paused. Press Enter to continue..."")\n\n\n## ========= Part 4: Collaborative Filtering Cost Regularization ========\n#  Now, you should implement regularization for the cost function for \n#  collaborative filtering. You can implement it by adding the cost of\n#  regularization to the original cost computation.\n#  \n\n#  Evaluate cost function\nJ, grad = cofiCostFunc(np.hstack((X.T.flatten(), Theta.T.flatten())), Y, R, num_users, num_movies,\n               num_features, 1.5)\n           \nprint \'Cost at loaded parameters (lambda = 1.5): %f \\n(this value should be about 31.34)\\n\' % J\n\nraw_input(""Program paused. Press Enter to continue..."")  \n\n\n## ======= Part 5: Collaborative Filtering Gradient Regularization ======\n#  Once your cost matches up with ours, you should proceed to implement \n#  regularization for the gradient. \n#\n\n#  \nprint \'Checking Gradients (with regularization) ...\'\n\n#  Check gradients by running checkNNGradients\ncheckCostFunction(1.5)\n\nraw_input(""Program paused. Press Enter to continue..."")  \n\n\n## ============== Part 6: Entering ratings for a new user ===============\n#  Before we will train the collaborative filtering model, we will first\n#  add ratings that correspond to a new user that we just observed. This\n#  part of the code will also allow you to put in your own ratings for the\n#  movies in our dataset!\n#\nmovieList = loadMovieList()\n\n#  Initialize my ratings\nmy_ratings = np.zeros(1682)\n\n# Check the file movie_idx.txt for id of each movie in our dataset\n# For example, Toy Story (1995) has ID 1, so to rate it ""4"", you can set\nmy_ratings[0] = 4\n\n# Or suppose did not enjoy Silence of the Lambs (1991), you can set\nmy_ratings[97] = 2\n\n# We have selected a few movies we liked / did not like and the ratings we\n# gave are as follows:\nmy_ratings[6] = 3\nmy_ratings[11] = 5\nmy_ratings[53] = 4\nmy_ratings[63] = 5\nmy_ratings[65] = 3\nmy_ratings[68] = 5\nmy_ratings[182] = 4\nmy_ratings[225] = 5\nmy_ratings[354] = 5\n\nprint \'New user ratings:\'\nfor i in range(len(my_ratings)):\n    if my_ratings[i] > 0:\n        print \'Rated %d for %s\\n\' % (my_ratings[i], movieList[i])\n\nraw_input(""Program paused. Press Enter to continue..."")  \n\n\n## ================== Part 7: Learning Movie Ratings ====================\n#  Now, you will train the collaborative filtering model on a movie rating \n#  dataset of 1682 movies and 943 users\n#\n\nprint \'\\nTraining collaborative filtering...\'\n\n#  Load data\ndata = scipy.io.loadmat(\'ex8_movies.mat\')\nY = data[\'Y\']\nR = data[\'R\'].astype(bool)\n\n#  Y is a 1682x943 matrix, containing ratings (1-5) of 1682 movies by \n#  943 users\n#\n#  R is a 1682x943 matrix, where R(i,j) = 1 if and only if user j gave a\n#  rating to movie i\n\n#  Add our own ratings to the data matrix\nY = np.column_stack((my_ratings, Y))\nR = np.column_stack((my_ratings, R)).astype(bool)\n\n#  Normalize Ratings\nYnorm, Ymean = normalizeRatings(Y, R)\n\n#  Useful Values\nnum_users = Y.shape[1]\nnum_movies = Y.shape[0]\nnum_features = 10\n\n# Set Initial Parameters (Theta, X)\nX = np.random.rand(num_movies, num_features)\nTheta = np.random.rand(num_users, num_features)\n\ninitial_parameters = np.hstack((X.T.flatten(), Theta.T.flatten()))\n# Set Regularization\nLambda = 10\n\ncostFunc = lambda p: cofiCostFunc(p, Ynorm, R, num_users, num_movies, num_features, Lambda)[0]\ngradFunc = lambda p: cofiCostFunc(p, Ynorm, R, num_users, num_movies, num_features, Lambda)[1]\n\nresult = minimize(costFunc, initial_parameters, method=\'CG\', jac=gradFunc, options={\'disp\': True, \'maxiter\': 1000.0})\ntheta = result.x\ncost = result.fun\n\n\n# Unfold the returned theta back into U and W\nX = theta[:num_movies*num_features].reshape(num_movies, num_features)\nTheta = theta[num_movies*num_features:].reshape(num_users, num_features)\n\nprint \'Recommender system learning completed.\'\n\nraw_input(""Program paused. Press Enter to continue..."")  \n\n## ================== Part 8: Recommendation for you ====================\n#  After training the model, you can now make recommendations by computing\n#  the predictions matrix.\n#\n\np = X.dot(Theta.T)\nmy_predictions = p[:, 0] + Ymean\n\nmovieList = loadMovieList()\n\n# sort predictions descending\npre=np.array([[idx, p] for idx, p in enumerate(my_predictions)])\npost = pre[pre[:,1].argsort()[::-1]]\nr = post[:,1]\nix = post[:,0]\n\nprint \'\\nTop recommendations for you:\'\nfor i in range(10):\n    j = int(ix[i])\n    print \'Predicting rating %.1f for movie %s\\n\' % (my_predictions[j], movieList[j])\n\nprint \'\\nOriginal ratings provided:\'\nfor i in range(len(my_ratings)):\n    if my_ratings[i] > 0:\n        print \'Rated %d for %s\\n\' % (my_ratings[i], movieList[i])\n'"
ex8/loadMovieList.py,0,"b'import io\n\n\ndef loadMovieList():\n    """"""\n    reads the fixed movie list in movie.txt\n    and returns a cell array of the words in movieList.\n    """"""\n\n    ## Read the fixed movieulary list\n    with io.open(\'movie_ids.txt\', encoding=\'ISO-8859-1\') as f:\n\n        # Store all movies in cell array movie{}\n        n = 1682  # Total number of movies \n\n        movieList = []\n        for i in range(n):\n            # Read line\n            line = f.readline()\n            # Word Index (can ignore since it will be = i)\n            str = line.split()\n            # Actual Word\n            movieList.append(\' \'.join(str[1:]).strip())\n        return movieList\n'"
ex8/multivariateGaussian.py,3,"b'import numpy as np\n\n\ndef multivariateGaussian(X, mu, Sigma2):\n    """"""Computes the probability\n    density function of the examples X under the multivariate gaussian\n    distribution with parameters mu and Sigma2. If Sigma2 is a matrix, it is\n    treated as the covariance matrix. If Sigma2 is a vector, it is treated\n    as the \\sigma^2 values of the variances in each dimension (a diagonal\n    covariance matrix)\n    """"""\n    k = len(mu)\n\n    if Sigma2.ndim == 1:\n        Sigma2 = np.diag(Sigma2)\n\n    X = X - mu\n    p = (2 * np.pi) ** (- k / 2) * np.linalg.det(Sigma2) ** (-0.5) * \\\n        np.exp(-0.5 * np.sum(X.dot(np.linalg.pinv(Sigma2))*X, axis=1))\n\n    return p'"
ex8/normalizeRatings.py,3,"b'import numpy as np\n\n\ndef normalizeRatings(Y, R):\n    """"""normalized Y so that each movie has a rating of 0 on average,\n    and returns the mean rating in Ymean.\n    """"""\n\n    m, n = Y.shape\n    Ymean = np.zeros(m)\n    Ynorm = np.zeros(Y.shape)\n\n    for i in range(n):\n        idx = (R[i,:]==1).nonzero()[0]\n        if len(idx):\n            Ymean[i] = np.mean(Y[i, idx])\n            Ynorm[i, idx] = Y[i, idx] - Ymean[i]\n        else:\n            Ymean[i] = 0.0\n            Ynorm[i,idx] = 0.0\n\n    return Ynorm, Ymean\n'"
ex8/selectThreshold.py,2,"b'import numpy as np\nimport math\n\ndef selectThreshold(yval, pval):\n    """"""\n    finds the best\n    threshold to use for selecting outliers based on the results from a\n    validation set (pval) and the ground truth (yval).\n    """"""\n\n    bestEpsilon = 0\n    bestF1 = 0\n\n    stepsize = (np.max(pval) - np.min(pval)) / 1000.0\n    for epsilon in np.arange(np.min(pval),np.max(pval), stepsize):\n\n        # ====================== YOUR CODE HERE ======================\n        # Instructions: Compute the F1 score of choosing epsilon as the\n        #               threshold and place the value in F1. The code at the\n        #               end of the loop will compare the F1 score for this\n        #               choice of epsilon and set it to be the best epsilon if\n        #               it is better than the current choice of epsilon.\n        #\n        # Note: You can use predictions = (pval < epsilon) to get a binary vector\n        #       of 0\'s and 1\'s of the outlier predictions\n\n\n        # =============================================================\n\n        if F1 > bestF1:\n           bestF1 = F1\n           bestEpsilon = epsilon\n\n    return bestEpsilon, bestF1\n\n\n\n\n\n\n'"
ex8/submit.py,8,"b'import numpy as np\n\nfrom Submission import Submission\nfrom Submission import sprintf\n\nhomework = \'anomaly-detection-and-recommender-systems\'\n\npart_names = [\n  \'Estimate Gaussian Parameters\',\n  \'Select Threshold\',\n  \'Collaborative Filtering Cost\',\n  \'Collaborative Filtering Gradient\',\n  \'Regularized Cost\',\n  \'Regularized Gradient\',\n  ]\n\nsrcs = [\n  \'estimateGaussian.py\',\n  \'selectThreshold.py\',\n  \'cofiCostFunc.py\',\n  \'cofiCostFunc.py\',\n  \'cofiCostFunc.py\',\n  \'cofiCostFunc.py\',\n  ]\n\n\ndef output(part_id):\n    # Random Test Cases\n    n_u = 3\n    n_m = 4\n    n = 5\n    X = np.sin(np.arange(1,n_m*n+1)).reshape((n, n_m)).T\n    Theta = np.cos(np.arange(1,n_u*n+1)).reshape((n, n_u)).T\n    Y = np.sin(np.arange(1,2.0*n_m*n_u,2.0)).reshape((n_u, n_m)).T\n    R = Y > 0.5\n    pval = np.hstack((np.abs(Y.T.flatten()), 0.001, 1.0))\n    yval = np.hstack((R.T.flatten(), 1.0, 0.0)).astype(\'bool\')\n    params = np.hstack((X.T.flatten(), Theta.T.flatten()))\n\n    fname = srcs[part_id-1].rsplit(\'.\',1)[0]\n    mod = __import__(fname, fromlist=[fname], level=1)\n    func = getattr(mod, fname)\n\n    if part_id == 1:\n        mu, sigma2 = func(X)\n        return sprintf(\'%0.5f \', np.hstack((mu.T.flatten(), sigma2.T.flatten())))\n    elif part_id == 2:\n        bestEpsilon, bestF1 = func(yval, pval)\n        return sprintf(\'%0.5f \', np.hstack((bestEpsilon, bestF1)))\n    elif part_id == 3:\n        J, grad = func(params, Y, R, n_u, n_m, n, 0.0)\n        return sprintf(\'%0.5f \', J)\n    elif part_id == 4:\n        J, grad = func(params, Y, R, n_u, n_m, n, 0.0)\n        return sprintf(\'%0.5f \', grad.T.flatten())\n    elif part_id == 5:\n        J, grad = func(params, Y, R, n_u, n_m, n, 1.5)\n        return sprintf(\'%0.5f \', J)\n    elif part_id == 6:\n        J, grad = func(params, Y, R, n_u, n_m, n, 1.5)\n        return sprintf(\'%0.5f \', grad.T.flatten())\n\ns = Submission(homework, part_names, srcs, output)\ntry:\n    s.submit()\nexcept Exception as ex:\n    template = ""An exception of type {0} occured. Messsage:\\n{1!r}""\n    message = template.format(type(ex).__name__, ex.args)\n    print message\n'"
ex8/visualizeFit.py,5,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom math import isinf\nfrom multivariateGaussian import multivariateGaussian\n\nfrom show import show\n\n\ndef visualizeFit(X, mu, sigma2):\n    """"""\n    This visualization shows you the\n    probability density function of the Gaussian distribution. Each example\n    has a location (x1, x2) that depends on its feature values.\n    """"""\n    n = np.linspace(0,35,71)\n    X1 = np.meshgrid(n,n)\n    Z = multivariateGaussian(np.column_stack((X1[0].T.flatten(), X1[1].T.flatten())),mu,sigma2)\n    Z = Z.reshape(X1[0].shape)\n\n    plt.plot(X[:, 0], X[:, 1],\'bx\')\n    # Do not plot if there are infinities\n    if not isinf(np.sum(Z)):\n        plt.contour(X1[0], X1[1], Z, 10.0**np.arange(-20, 0, 3).T)\n        show()\n'"
gui/Submission.py,2,"b'from urllib import urlencode\nfrom urllib2 import urlopen\nfrom json import loads, dumps\nfrom collections import OrderedDict\nimport numpy as np\nimport os\n\n\n\nclass Submission():\n\n    def __init__(self, homework, part_names, srcs, output):\n        self.__homework = homework\n        self.__part_names = part_names\n        self.__srcs = srcs\n        self.__output = output\n        self.__submit_url = \'https://www-origin.coursera.org/api/onDemandProgrammingImmediateFormSubmissions.v1\'\n        self.__login = None\n        self.__password = None\n\n    def __init__(self):\n        self.__submit_url = \'https://www-origin.coursera.org/api/onDemandProgrammingImmediateFormSubmissions.v1\'\n        \n        \n    def submit(self):\n        print \'==\\n== Submitting Solutions | Programming Exercise %s\\n==\' % self.__homework\n        self.login_prompt()\n\n        parts = OrderedDict()\n        for part_id, _ in enumerate(self.__srcs,1):\n            parts[str(part_id)] = {\'output\': self.__output(part_id)}\n\n        result, response = self.request(parts)\n        response = loads(response)\n        try:\n            print response[\'errorMessage\']\n            return\n        except:\n            pass\n        print \'==\'\n        print \'== %43s | %9s | %-s\' % (\'Part Name\', \'Score\', \'Feedback\')\n        print \'== %43s | %9s | %-s\' % (\'---------\', \'-----\', \'--------\')\n        \n\n        for part in parts:\n            partFeedback = response[\'partFeedbacks\'][part]\n            partEvaluation = response[\'partEvaluations\'][part]\n            score = \'%d / %3d\' % (partEvaluation[\'score\'], partEvaluation[\'maxScore\'])\n            print \'== %43s | %9s | %-s\' % (self.__part_names[int(part)-1], score, partFeedback)\n\n        evaluation = response[\'evaluation\']\n    \n\n        totalScore = \'%d / %d\' % (evaluation[\'score\'], evaluation[\'maxScore\'])\n        print \'==                                   --------------------------------\'\n        print \'== %43s | %9s | %-s\\n\' % (\' \', totalScore, \' \')\n        print \'==\'\n\n        if not os.path.isfile(\'token.txt\'):\n            with open(\'token.txt\', \'w\') as f:\n                f.write(self.__login + \'\\n\')\n                f.writelines(self.__password)\n        return response\n\n    def login_prompt(self):\n        try:\n            with open(\'token.txt\', \'r\') as f:\n                self.__login = f.readline().strip()\n                self.__password = f.readline().strip()\n        except IOError:\n            pass\n\n        if self.__login is not None and self.__password is not None:\n            reenter = raw_input(\'Use token from last successful submission (%s)? (Y/n): \' % self.__login)\n\n            if reenter == \'\' or reenter[0] == \'Y\' or reenter[0] == \'y\':\n                return\n\n        if os.path.isfile(\'token.txt\'):\n            os.remove(\'token.txt\')\n        self.__login = raw_input(\'Login (email address): \')\n        self.__password = raw_input(\'Token: \')\n\n    def request(self, parts):\n\n        params = {\n            \'assignmentSlug\': self.__homework,\n            \'secret\': self.__password,\n            \'parts\': parts,\n            \'submitterEmail\': self.__login}\n\n        params = urlencode({\'jsonBody\': dumps(params)})\n        f = urlopen(self.__submit_url, params)\n        try:\n            return 0, f.read()\n        finally:\n            f.close()\n\ndef sprintf(fmt, arg):\n    ""emulates (part of) Octave sprintf function""\n    if isinstance(arg, tuple):\n        # for multiple return values, only use the first one\n        arg = arg[0]\n\n    if isinstance(arg, (np.ndarray, list)):\n        # concatenates all elements, column by column\n        return \' \'.join(fmt % e for e in np.asarray(arg).ravel(\'F\'))\n    else:\n        return fmt % arg'"
gui/interface.py,0,"b""import os\nimport pypandoc\nfrom kivy.app import App\nfrom kivy.uix.boxlayout import BoxLayout\nfrom kivy.uix.gridlayout import GridLayout\nfrom kivy.uix.floatlayout import FloatLayout\nfrom kivy.uix.button import Button\nfrom kivy.uix.codeinput import CodeInput\nfrom kivy.uix.label import Label\nfrom kivy.uix.rst import RstDocument\nfrom kivy.uix.textinput import TextInput\nfrom kivy.uix.spinner import Spinner\nfrom kivy.uix.togglebutton import ToggleButton\nfrom kivy.uix.splitter import Splitter\nfrom kivy.uix.scrollview import ScrollView\nfrom kivy.uix.popup import Popup\nfrom kivy.clock import Clock\nfrom functools import partial\nfrom kivy.animation import Animation\nfrom Submission import Submission\n\n\nclass resourceHandler():\n    def __init__(self, **args):\n        pass\n\n\n    def read_token(self,instance):\n        path = '../'+instance.current_ex+'/token.txt'\n        try:\n            credentials = open(path)\n            instance.email = credentials.readline().strip()\n            instance.token = credentials.readline().strip()\n            return True\n        except Exception, e:\n            return False\n\n\n\n    def files(self,excercise):\n        path = 'res/'+excercise+'/sources.txt'\n        filehandler = open(path)\n        filelist=[]\n        while True:\n            try:\n                filelist.append(filehandler.next())\n            except Exception, e:\n                return filelist\n    def manual(self, excercise):\n        path = 'res/'+excercise+'/manual.md'\n        return pypandoc.convert(path,'rst')\n\n    def writeFile(self,excercise,filename):\n        path = '../'+excercise+'/'+filename\n        f=open(path,'w')\n        return f\n\n    def readFile(self,excercise,filename):\n        path = '../'+excercise+'/'+filename\n        #print 'Opening ',path\n        f=open(path,'r')\n        return f.read()\n\nclass MainScreen(BoxLayout):\n\n    def __init__(self, welcome=False, **kwargs):\n        super(MainScreen, self).__init__(**kwargs)\n        self.orientation='vertical'\n        self.current_ex = 'ex1'\n        self.current_file = 'warmUpExercise.py'\n        self.submit_ob = Submission()\n        self.element=resourceHandler()\n\n        if welcome:\n            welcome_popup = Popup(title='Coursera ML in Python', content=Label(text='Hello World'),size_hint=(1, 1))\n            self.add_widget(welcome_popup)\n            welcome_popup.open()\n            Clock.schedule_once(self.start_app,3)\n        else:\n            self.bind(size=self.draw_screen)\n  \n    def start_app(self,*args):\n        self.draw_screen()\n        self.bind(size=self.draw_screen)\n\n    def draw_screen(self,*args):\n        self.clear_widgets()\n        self.add_widget(self.titlebar())\n        self.add_widget(self.maineditor())\n        scrollbar = ScrollView(size_hint=(1,None))\n        #TODO: Make filebar scrollable for smaller screens and many files\n        #scrollbar.add_widget(self.filebar())\n        #self.add_widget(scrollbar)\n        self.add_widget(self.filebar())\n        self.add_widget(self.console())\n\n    def titlebar(self):\n        layout=BoxLayout(padding='2sp',size_hint=(1,None),height='65sp')\n        layout.orientation='horizontal'\n\n        #credentials = self.accept_credentials()\n        self.submit_popup = Popup(title='Enter credentials',content=self.accept_credentials(),size_hint=(0.6, 0.35))\n        #credentials.children[1].bind(on_press=self.submit_popup.dismiss)\n\n        submit = Button(text='Submit',size_hint=(0.4,1))\n        if self.element.read_token(self):\n            submit.bind(on_press=partial(self.submit_assignment))\n        else:\n            submit.bind(on_press=self.submit_popup.open)\n\n        run = Button(text='Run',size_hint=(0.4,1))\n        run.bind(on_press=self.run)\n\n        ex_dropdown = Spinner(text=self.current_ex,size_hint=(1,1))\n        ex_dropdown.values = os.listdir('./res/')\n        ex_dropdown.bind(text=self.updateExercise)\n\n        layout.add_widget(run)\n        layout.add_widget(ex_dropdown)\n        layout.add_widget(submit)\n\n        return layout\n\n\n    def console(self):\n        layout = FloatLayout(size_hint=(1,None),height=100)\n        self.info_label = TextInput(size_hint=(1,None),readonly=True,background_color=(0,0,0,1),foreground_color=(1,1,1,1),opacity=0)\n        self.info_label.text_size = self.size\n        self.info_label.text = 'console'\n        self.info_label.height = '150pt'\n        self.info_label.top = 0\n        layout.add_widget(self.info_label)\n        return layout\n\n\n    def accept_credentials(self):\n        main_layout= BoxLayout(padding='2sp')\n        main_layout.orientation='vertical'\n        layout=GridLayout(padding='2sp')\n        layout.cols=2\n        layout.add_widget(Label(text='Email id:'))\n        email = TextInput(multiline=False)\n        layout.add_widget(email)\n        token = TextInput(multiline=False)\n        layout.add_widget(Label(text='Submission Token:'))\n        layout.add_widget(token)\n        main_layout.add_widget(layout)\n        submit = Button(text='Submit',size_hint=(1,0.4))\n        submit.bind(on_press=partial(self.submit_assignment,email,token))\n        main_layout.add_widget(submit)\n        return main_layout\n\n    def submit_assignment(self,*largs):\n        #Make submit_ob local if not used anywhere else \n        if len(largs)>1:\n            self.submit_ob.__login = largs[0].text\n            self.submit_ob.__password = largs[1].text\n        else:\n            self.submit_ob.__login=self.email\n            self.submit_ob.__password=self.token\n\n        print 'Email',self.submit_ob.__login\n        print 'Token', self.submit_ob.__password\n        self.submit_popup.dismiss()\n        #TODO:submission call\n        #self.show_error(self.submit_ob.submit())\n\n\n    def updateExercise(self,spinner,text):\n        self.current_ex=text\n        current_file = self.element.files(self.current_ex)[0]\n        if current_file.endswith('\\n'):\n            current_file=current_file[:-1]\n        self.current_file= current_file\n        self.draw_screen()\n        print 'Current Exercise changed to: ', self.current_ex\n\n\n\n    def run(self,instance):\n        #TODO: Display output in popup\n        self.show_error('Cannot run')\n        print('The button <%s> is being pressed' % instance.text)\n\n    \n   \n        \n    def maineditor(self):\n        layout=BoxLayout()\n        if self.width < self.height:\n            layout.orientation='vertical'\n        else:\n            layout.orientation='horizontal'\n        #self.bind(self.current_ex=self.update_currentFile)\n        man = self.element.manual(self.current_ex)\n        codeFile = self.element.readFile(self.current_ex,self.current_file)\n        code = CodeInput(text=codeFile)\n        code.bind(focus =self.schedule_reload)\n        splitter = Splitter()\n        if layout.orientation == 'vertical':\n            splitter.sizable_from='bottom'\n        else:\n            splitter.sizable_from='right'\n        splitter.add_widget(code)\n        layout.add_widget(splitter)\n\n        layout.add_widget(RstDocument(text=man))\n        return layout\n\n    def saveAssignment(self,assignment,*largs):\n        print 'callback called'\n        try:\n            if not self.element.readFile(self.current_ex,self.current_file)==assignment.text:\n                filehandler = self.element.writeFile(self.current_ex,self.current_file)\n                filehandler.write(assignment.text)\n                print 'INFO: Autosaved file'\n        except Exception, e:\n            raise e\n            self.show_error(e)\n\n\n    def schedule_reload(self,instance,value):\n        if value:\n            #Schedule Update\n            self.callback = partial(self.saveAssignment,instance)\n            Clock.schedule_interval(self.callback,5)\n        else:\n            #TODO:When clicking on another file, both focus=False and filebar button callbacks are executed simultaneously leading to deadlock\n            Clock.unschedule(self.callback)\n            #self.saveAssignment(instance)\n            #Update now\n            \n    \n    def filebar(self):\n        layout=BoxLayout(padding='2sp',size_hint=(1,None),height='100sp')\n        layout.orientation='horizontal'\n        files = self.element.files(self.current_ex)\n        for f in files:\n            if f.strip() == self.current_file:\n                button = ToggleButton(text=f,group = self.current_ex,state='down')\n            else:\n                button = ToggleButton(text=f,group = self.current_ex,state='normal')\n            button.bind(on_press=self.update_currentFile)\n            layout.add_widget(button)\n        return layout\n\n    def update_currentFile(self,instance):\n        if instance.text.endswith('\\n'):\n            instance.text=instance.text[:-1]\n        self.current_file = instance.text\n        self.draw_screen()\n        print 'Current file changed to: ', self.current_file\n\n    def show_error(self, e):\n        self.info_label.text = str(e)\n        duration = len(self.info_label.text)/10\n        anim = Animation(top=190.0, opacity=1, d=0.5) +\\\n            Animation(top=190.0, d=duration) +\\\n            Animation(top=0, opacity=0, d=2)        \n        anim.start(self.info_label)\n\n\n\nclass CourseraApp(App):\n\n    def build(self):\n        return MainScreen(welcome=True)\n    def on_pause(self):\n        return True\n\n\nif __name__ == '__main__':\t\n    CourseraApp().run()\n"""
