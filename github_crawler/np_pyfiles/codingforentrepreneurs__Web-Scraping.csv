file_path,api_count,code
src/scrape-depth.py,0,"b'to_scrape = set()\nto_scrape.add(""/path-1"")\nto_scrape.add(\'/path-2\')\n\n# for i in list(scrape_items):\n\nscraped_items = set() # uniqueness\nscraped_items.add(""/path-2"")\n\ndef fetch_links_words(url):\n    print(url, ""scraping.."")\n    return set([""/path-3"", ""/path-4""]), [""words1"", ""words2""]\n\n# , \ndef scrape_links(to_scrape, scraped, current_depth=0, max_depth=3, words=[]):\n    if current_depth <= max_depth:\n        new_set_to_scrape = set() \n        while to_scrape:\n            item = to_scrape.pop() \n            if item not in scraped:\n                new_paths, new_words = fetch_links_words(item)\n                words += new_words\n                new_set_to_scrape = (new_set_to_scrape | new_paths) # removes extra\n            scraped.add(item)\n        current_depth += 1\n        return scrape_links(new_set_to_scrape, scraped, current_depth=current_depth, max_depth=max_depth, words=words)\n    return scraped, words\n\n\nscraped, words = scrape_links(to_scrape, set(), current_depth=0, max_depth=3)\nprint(scraped, words)\n\n\n\n\n'"
src/scrape1.py,0,"b'import csv\nimport datetime\nimport os\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urlparse\nfrom collections import Counter\nfrom stop_words import get_stop_words\n# print(""Hello world\\"""")\n# my_url = ""http://joincfe.com/blog/""\n\ndef clean_word(word):\n    word = word.replace(""!"", """") #.split()\n    word = word.replace(""?"", """")\n    word = word.replace(""."", """")\n    word = word.replace("":"", """")\n    word = word.replace("","", """")\n    word = word.replace("";"", """")\n    word = word.replace("")"", """")\n    word = word.replace(""("", """")\n    word = word.replace(""-"", """")\n    word = word.replace(""--"", """")\n    word = word.replace(\'\xe2\x80\x94\', """")\n    return word\n\ndef clean_up_words(words):\n    new_words = [] # empty list\n    pkg_stop_words = get_stop_words(\'en\')\n    my_stop_words = [\'the\', \'is\', \'and\', \'thisfacebooktwitteremailredditprint\']\n    for word in words:\n        word = word.lower()\n        cleaned_word = clean_word(word)\n        if cleaned_word in my_stop_words or cleaned_word in pkg_stop_words:\n            pass\n        else:\n            new_words.append(cleaned_word)\n    return new_words\n\ndef create_csv_path(csv_path):\n    if not os.path.exists(csv_path):\n        with open(csv_path, \'w\') as csvfile: # open that path w = write/create\n            header_columns = [\'word\', \'count\', \'timestamp\']\n            writer = csv.DictWriter(csvfile, fieldnames=header_columns)\n            writer.writeheader()\n\nsaved_domains = {\n    ""joincfe.com"": ""main-container"",\n    ""tim.blog"": ""content-area""\n}\n\nmy_url = input(""Enter the url to scrape: "") \n\nprint(""Grabbing..."", my_url)\ndomain = urlparse(my_url).netloc # domain name\nprint(""via domain"", domain)\n\nresponse = requests.get(my_url) # go to the url and get it\nprint(""Status is"", response.status_code) # 200, 403, 404, 500, 503\n\nif response.status_code != 200: # not equal, == equal\n    print(""You can\'t scrape this"", response.status_code)\nelse:\n    print(""Scraping.."")\n    # print(response.text)\n    html = response.text\n    soup = BeautifulSoup(html, ""html.parser"")\n    if domain in saved_domains:\n        div_class = saved_domains[domain]\n        body_ = soup.find(""div"", {""class"": div_class})\n    else:\n        body_ = soup.find(""body"")\n    #print(body_.text)\n    words = body_.text.split() # removing stop words\n    clean_words = clean_up_words(words)\n    word_counts = Counter(clean_words)\n    print(word_counts.most_common(30))\n    filename = domain.replace(""."", ""-"") + \'.csv\' # tim.blog, joincfe.com    \n    path  = \'csv/\' + filename # os.path\n    timestamp = datetime.datetime.now() # timestamp\n    create_csv_path(path)\n    with open(path, \'a\') as csvfile: # open that path w = write/create\n        header_columns = [\'word\', \'count\', \'timestamp\']\n        writer = csv.DictWriter(csvfile, fieldnames=header_columns)\n        for word, count in word_counts.most_common(30):\n            writer.writerow({\n                    ""count"": count,\n                    ""word"": word,\n                    ""timestamp"": timestamp\n                })\n\n\n\n\n'"
src/scrape2.py,0,"b'import csv\nimport datetime\nimport os\nimport requests\nimport re\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urlparse\nfrom collections import Counter\nfrom stop_words import get_stop_words\n\nsaved_domains = {\n    ""codingforentrepreneurs.com"": {\n        ""tag"": ""div"",\n        ""class"": ""main-container"",\n        ""regex"": r""^/blog/(?P<slug>[\\w-]+)/$"",\n    },\n    ""www.codingforentrepreneurs.com"": {\n        ""tag"": ""div"",\n        ""class"": ""main-container"",\n        ""regex"": r""^/blog/(?P<slug>[\\w-]+)/$"",\n    },\n    ""tim.blog"": {\n        ""tag"": ""div"",\n        ""class"": ""content-area"",\n        ""regex"": r""^/(?P<year>\\d+){4}/(?P<month>\\d+){2}/(?P<day>\\d+){2}/(?P<slug>[\\w-]+)/$""\n    },\n}\n\n\n\n\ndef clean_word(word):\n    word = word.replace(""!"", """") #.split()\n    word = word.replace(""?"", """")\n    word = word.replace(""."", """")\n    word = word.replace("":"", """")\n    word = word.replace("","", """")\n    word = word.replace("";"", """")\n    word = word.replace("")"", """")\n    word = word.replace(""("", """")\n    word = word.replace(""}"", """")\n    word = word.replace(""{"", """")\n    word = word.replace(""-"", """")\n    word = word.replace(""--"", """")\n    word = word.replace(\'\xe2\x80\x94\', """")\n    word = word.replace(""i\'m"", ""i am"")\n    word = word.replace(""it\'s"", ""it is"")\n    word = word.strip()\n    return word\n\ndef clean_up_words(words):\n    new_words = [] # empty list\n    pkg_stop_words = get_stop_words(\'en\')\n    my_stop_words = [\n                \'the\', \n                \'is\', \n                \'and\', \n                \'thisfacebooktwitteremailredditprint\', \n                \'\',\n                \'reply\',\n                \'likelike\',\n                \'likeliked\',\n                \'comments\',\n                \'commenting\',\n                \'/\',\n                \'=\'\n                ]\n    for word in words:\n        word = word.lower()\n        cleaned_word = clean_word(word)\n        if cleaned_word in my_stop_words or cleaned_word in pkg_stop_words:\n            pass\n        else:\n            new_words.append(cleaned_word)\n    return new_words\n\ndef fetch_url(url):\n    \'\'\'\n    Request package 1 time\n    \'\'\'\n    response = requests.Response()\n    try:\n        response = requests.get(url)\n        #print(dir(response)) # python\n        #print(response.__class__) # python\n    except requests.exceptions.ConnectionError:\n        print(""Could not connect to the url. Please try again."")\n    return response\n\n\n\ndef validate_url(url):\n    http_regex = r\'^https?://\' # https or http # validate .co, .com, \n    pattern = re.compile(http_regex)\n    is_a_match = pattern.match(url) # regex match or None\n    if is_a_match is None:\n        raise ValueError(""This url does not start with http:// or https://"")\n    return url\n\ndef append_http(url):\n    if not url.startswith(""http""):\n        return f\'http://{url}\'\n    return url\n\ndef end_program():\n    raise KeyboardInterrupt(""Program forced to quit."")\n\ndef get_input():\n    url = input(""What is your url? "")\n    if url == \'q\':\n        return end_program()\n    url = append_http(url)\n    try:\n        validate_url(url)\n    except ValueError as err:\n        print(err)\n        print(""Type \'q\' to quit lookup"")\n        return get_input()\n    return url\n\n\ndef soupify(html):\n    soup = BeautifulSoup(html, \'html.parser\')\n    return soup\n\n\ndef get_domain_name(url):\n    return urlparse(url).netloc\n\ndef get_path_name(url):\n    return urlparse(url).path\n\ndef get_url_lookup_class(url):\n    domain_name = get_domain_name(url)\n    lookup_class = {}\n    if domain_name in saved_domains:\n        \'\'\'\n        change this to use a CSV file instead.\n        \'\'\'\n        lookup_class = saved_domains[domain_name]\n    return lookup_class\n\n\ndef get_content_data(soup, url):\n    lookup_dict = get_url_lookup_class(url)\n    if lookup_dict is None or ""tag"" not in lookup_dict:\n        return soup.find(""body"")\n    return soup.find(lookup_dict[\'tag\'], {""class"": lookup_dict[\'class\']})\n\n\ndef parse_links(soup):\n    # <a href=\'/abc/\'>Link</a>\n    a_tags = soup.find_all(""a"", href=True)\n    #print(a_tags)\n    links = []\n    for a in a_tags:\n        link = a[\'href\'] # \'/abc/\', \'/another/\'\n        links.append(link)\n    return links\n\ndef get_local_paths(soup, url):\n    links = parse_links(soup) # list of links\n    local_paths = []\n    domain_name = get_domain_name(url)\n    for link in links:\n        link_domain = get_domain_name(link) # <a href=\'http://yourodmain.com/some-local/-link/\'>\n        if link_domain == domain_name:\n            path = get_path_name(link)\n            local_paths.append(path)\n        elif link.startswith(""/""): # <a href=\'/some-local/-link/\'>\n            local_paths.append(link)\n    return list(set(local_paths)) # removes duplicates and returns list\n\n\'\'\'\n[\'/category/the-tim-ferriss-show/\', \n\'/2018/03/29/discipline-sex-psychedelics-and-more-the-return-of-drunk-dialing/\', \n\'/author/tferriss/\', \n\'/2018/03/12/aubrey-marcus/\', \n\'/2018/03/21/how-to-prioritize-your-life-and-make-time-for-what-matters/\', \n\'/2018/03/15/frank-blake/\', \'/2018/03/05/jack-kornfield/\', \'/2017/11/03/sharon-salzberg/\', \'/2016/12/20/becoming-the-best-version-of-you/\', \'/2018/03/08/joe-gebbia-co-founder-of-airbnb/\', \'/page/2/\', \'/2018/03/25/daniel-pink/\', \'/2017/09/13/ray-dalio/\', \'/2017/01/12/how-to-design-a-life-debbie-millman/\']\nr""^/(?P<year>\\d+){4}/(?P<month>\\d+){2}/(?P<day>\\d+){2}/(?P<slug>[\\w-]+)/$"" # common url expressions joincfe.com/blog\nr""^/blog/(?P<slug>[\\w-]+)/$""\n\n\'\'\'\n\ndef get_regex_pattern(root_domain):\n    pattern = r""^/(?P<slug>[\\w-]+)$""\n    if root_domain in saved_domains:\n        regex = saved_domains[root_domain].get(""regex"")\n        if regex is not None:\n            pattern = regex\n    return pattern\n\ndef match_regex(string, regex):\n    pattern = re.compile(regex)\n    is_a_match = pattern.match(string) # regex match or None\n    if is_a_match is None:\n        return False\n    return True\n\ndef get_regex_local_paths(soup, url):\n    \'\'\'\n    This will scrape local paths based on a regular expression\n    from the get_regex_pattern_method\n\n    :soup -- soupified html \n    :url -- local domain url\n    returns list of local paths from the url\n\n    \'\'\'\n    links = parse_links(soup) \n    local_paths = []\n    domain_name = get_domain_name(url)\n    regex = get_regex_pattern(domain_name)\n    for link in links:\n        link_domain = get_domain_name(link) \n        if link_domain == domain_name:\n            path = get_path_name(link)\n            is_match = match_regex(path, regex)\n            if is_match:\n                local_paths.append(path)\n        elif link.startswith(""/""): \n            is_match = match_regex(link, regex)\n            if is_match:\n                local_paths.append(path)\n    return list(set(local_paths)) \n\n\ndef parse_blog_post(path, url):\n    domain_name = get_domain_name(url)\n    lookup_url = f""http://{domain_name}{path}"" \n    lookup_response = fetch_url(lookup_url)\n    if lookup_response.status_code in range(200, 299):\n        lookup_soup = soupify(lookup_response.text)\n        lookup_html_soup = get_content_data(lookup_soup, lookup_url)\n        words = lookup_html_soup.text.split()\n        clean_words = clean_up_words(words)\n        #print(clean_words)\n    return clean_words\n\ndef main():\n    url = get_input()\n    response = fetch_url(url)\n    if response.status_code not in range(200, 299): # Http Status Codes\n        print(f""Invalid request, you cannot view this. {response.status_code}"")\n        return None\n    response_html = response.text # html\n    soup = soupify(response_html)\n    html_soup = get_content_data(soup, url)\n    #print(html_text)\n    paths = get_regex_local_paths(html_soup, url)\n    words = []\n    for path in paths:\n        clean_words = parse_blog_post(path, url)\n        #print(clean_words)\n        words = words + clean_words\n    print(Counter(words).most_common(100))\n\n    # call my url\n    # parse\n    # save\n\n#main()\n\n\n\ndef fetch_links_words(url):\n    print(url, ""scraping.."")\n    response            = fetch_url(url)\n    soup                = soupify(response.text)\n    html_soup           = get_content_data(soup, url)\n    local_paths         = get_regex_local_paths(html_soup, url) # /this/is/some/path\n    domain_name         = get_domain_name(url)\n    to_scrape           = [f""http://{domain_name}{path}"" for path in local_paths]\n    words               = []\n    if html_soup:\n        words           = html_soup.text.split()\n    clean_words         = clean_up_words(words)\n    return set(to_scrape), clean_words\n\n# , \ndef scrape_links(to_scrape, scraped, current_depth=0, max_depth=3, words=[]):\n    if current_depth <= max_depth:\n        new_set_to_scrape = set() \n        while to_scrape:\n            item = to_scrape.pop() \n            if item not in scraped:\n                new_paths, new_words = fetch_links_words(item)\n                words += new_words\n                new_set_to_scrape = (new_set_to_scrape | new_paths) # removes extra\n            scraped.add(item)\n        current_depth += 1\n        return scrape_links(new_set_to_scrape, scraped, current_depth=current_depth, max_depth=max_depth, words=words)\n    return scraped, words\n\n\n\ndef main_with_depth():\n    url                         = \'http://tim.blog/\' #get_input()\n    to_scrape_a, new_words      = fetch_links_words(url)\n    scraped_a                   = set([url])\n    final_scraped_items, final_words = scrape_links(to_scrape_a, scraped_a, current_depth=0, max_depth=3, words=new_words)\n    print(final_scraped_items)\n\n\n\nmain_with_depth()\n\n\n\n'"
