file_path,api_count,code
setup.py,0,"b""from setuptools import setup\r\n\r\nsetup(\r\n    name='trefoil',\r\n    version='0.3.1.1',\r\n    packages=['trefoil',\r\n              'trefoil.analysis', 'trefoil.cli',\r\n              'trefoil.geometry', 'trefoil.geometry.tests',\r\n              'trefoil.netcdf', 'trefoil.render',\r\n              'trefoil.render.renderers', 'trefoil.render.renderers.tests',\r\n              'trefoil.utilities', 'trefoil.utilities.tests',\r\n              # for temporary backward compatibility only!  Will be removed in near future\r\n              'clover'],\r\n    url='https://github.com/consbio/trefoil',\r\n    license='see LICENSE',\r\n    author='databasin',\r\n    author_email='databasinadmin@consbio.org',\r\n    description='Useful tools for spatial analysis using numpy and NetCDF',\r\n    long_description_content_type='text/markdown',\r\n    long_description=open('README.md').read(),\r\n    install_requires=[\r\n        'affine>=1.0',\r\n        'click',\r\n        'jinja2',\r\n        'palettable',\r\n        'pytz',\r\n        'six',\r\n        'fiona>=1.6.0',\r\n        'netCDF4>=1.1.1',\r\n        'Numpy',\r\n        'Pillow>=2.9.0',\r\n        'pyproj',\r\n        'rasterio>=1.0a12',\r\n    ],\r\n    entry_points='''\r\n        [console_scripts]\r\n        trefoil=trefoil.cli.main:cli\r\n    '''\r\n)\r\n"""
clover/__init__.py,0,"b'# This is here simply to aid migration to trefoil.  It will be removed in a future version!\n\nimport sys\nimport warnings\n\nimport trefoil\n\nwarnings.simplefilter(\'always\', DeprecationWarning)\nwarnings.warn(\n    ""the package name \'clover\' has been deprecated; use \'trefoil\' instead"",\n    DeprecationWarning)\n\nsys.modules[\'clover\'] = trefoil'"
trefoil/__init__.py,0,b''
trefoil/analysis/__init__.py,0,b''
trefoil/analysis/summary.py,0,"b'import numpy\r\nimport time\r\n\r\n\r\nVALID_ZONAL_STATISTICS = {\'mean\', \'min\', \'max\', \'std\', \'sum\', \'count\'}\r\n\r\n\r\ndef summarize_count_by_category(values):\r\n    """"""\r\n    Tallys the pixel counts for each unique value found in values.\r\n\r\n    All masking must be done prior to calling this function.\r\n\r\n    :param values:  pixel values.\r\n    :return: dictionary mapping value to count\r\n    """"""\r\n\r\n    #TODO: make sure this has areas of unique_value = 0\r\n    flat_values = values.ravel()\r\n    weights = None\r\n    if isinstance(values, numpy.ma.masked_array):\r\n        weights = flat_values.mask == False  # make sure all areas that are in mask are given a weight of 0, and not counted\r\n    bincounts = numpy.bincount(flat_values, weights=weights)\r\n    nonzero_indices = numpy.flatnonzero(bincounts)\r\n    results = dict(numpy.vstack((nonzero_indices, bincounts[nonzero_indices])).T.astype(numpy.uint64))\r\n    return results\r\n\r\n    #alternative method, will work for non-integer values\r\n    # flat_values = values.ravel()\r\n    # results = dict()\r\n    # for value in numpy.ma.unique(flat_values):\r\n    #     if not isinstance(value, numpy.ma.core.MaskedConstant):\r\n    #         results[value] = numpy.ma.count(numpy.ma.masked_array(flat_values, mask=flat_values != value))\r\n    # return results\r\n\r\n\r\ndef summarize_areas_by_category(values, areas):\r\n    """"""\r\n    Tallys the areas for each unique value found in values based on the amount within each pixel.\r\n\r\n    All masking must be done prior to calling this function.\r\n\r\n    :param values:  pixel values.  Assumes that this has already been masked where areas==0\r\n    :param areas: areas measured for each pixel (typically area of intersection of a feature w/in pixel)\r\n    :return: dictionary mapping value to total area\r\n    """"""\r\n\r\n    flat_values = values.ravel()\r\n    flat_areas = areas.ravel()\r\n    results = dict()\r\n    for value in numpy.ma.unique(flat_values):\r\n        if not isinstance(value, numpy.ma.core.MaskedConstant):\r\n            results[value] =  numpy.ma.masked_array(flat_areas, mask=flat_values != value).sum()\r\n    return results\r\n\r\n\r\n# TODO: see numpy.ma.average, which does weighted statistics: http://docs.scipy.org/doc/numpy/reference/generated/numpy.ma.average.html#numpy.ma.average\r\n# In simple tests, it produces same results, but faster!\r\ndef calculate_weighted_statistics(values, weights, statistics):\r\n    """"""\r\n    Calculates weighted statistics\r\n\r\n    :param values: pixel values\r\n    :params weights: weight of each pixel, where 0 > weight >= 1  (areas of 0 weight should be masked out first).  Weights\r\n    can be thought of as the proportion of each pixel occupied by some feature of interest.\r\n    :param statistics: list of statistics to be calculated.  Currently supports: MEAN, STD\r\n    :return: a list with each of the results, in the order the original statistic was requested\r\n    """"""\r\n\r\n    supported_statistics = {""MEAN"", ""STD""}\r\n    unsupported_statistics = set(statistics).difference(supported_statistics)\r\n    if unsupported_statistics:\r\n        raise ValueError(""Unsupported statistics: %s"" % unsupported_statistics)\r\n\r\n    results = []\r\n    weighted_values = values * weights\r\n    for statistic in statistics:\r\n        if statistic == ""MEAN"":\r\n            #must account for the mask of both values and weights in calculating sum\r\n            results.append(weighted_values.sum() / numpy.ma.masked_array(weights, mask=weights.mask + values.mask).sum())\r\n        elif statistic == ""STD"":\r\n            results.append(weighted_values.std())\r\n\r\n    return results\r\n\r\n\r\ndef statistic_by_interval(values, interval, statistic=\'mean\'):\r\n    """"""\r\n    Calculates statistics from values across an interval.\r\n\r\n    For example, to sum monthly data up to annual data: statistic_by_interval(monthy_values, 12, \'sum\') => one entry per year\r\n\r\n    :param values: values (must have 3 dimensions)\r\n    :param interval: interval over which to sum  (e.g., 12 for summing months to year)\r\n    :param statistic: one of \'mean\', \'sum\'\r\n    :return: a numpy array with shape (values.shape[0] / interval, values.shape[1], values.shape[2])\r\n    """"""\r\n\r\n    if not statistic in (\'mean\', \'sum\'):\r\n        raise ValueError(\'Unsupported statistic {0}\'.format(statistic))\r\n\r\n    assert len(values.shape) == 3  # Anything else is not handled correctly right now\r\n    assert values.shape[0] % interval == 0\r\n\r\n    num_intervals = int(values.shape[0] / interval)\r\n    # Reshape to groups of intervals, intervals, then remaining shape of values\r\n    temp = values.reshape(num_intervals, interval, values.shape[1], values.shape[2])\r\n\r\n    if statistic == \'mean\':\r\n        return temp[:, :interval, :, :].mean(axis=1)\r\n    elif statistic == \'sum\':\r\n        return temp[:, :interval, :, :].sum(axis=1)\r\n\r\n\r\n# TODO: this might make more sense as a loop in Cython\r\ndef calculate_zonal_statistics(zones, zone_values, values, statistics):\r\n    """"""\r\n    Calculate zonal statistics for each zone in zones.\r\n\r\n    Parameters\r\n    zones: numpy ndarray\r\n        expected to be 2D\r\n    zone_values: list-like\r\n        1D array of zone values.  Index in this list used to match the zone\r\n        index in the zones array.\r\n    values: numpy ndarray\r\n        expected to be 2 or 3D.\r\n        instead of a single value\r\n    statistics: list-like\r\n        must be one of mean, min, max, std, sum, count\r\n\r\n    Returns\r\n    -------\r\n    dict:  {zone: {statistic: value or [values]} }\r\n    """"""\r\n\r\n    if set(statistics).difference(VALID_ZONAL_STATISTICS):\r\n        raise ValueError(\'One or more statistics is not supported {0}\'.format(statistics))\r\n\r\n    if not len(values.shape) in (2, 3):\r\n        raise ValueError(\'Input values expected to be 2 or 3D\')\r\n\r\n    if not hasattr(values, \'mask\'):\r\n        values = numpy.ma.masked_array(values)\r\n\r\n    axis = None\r\n    if len(values.shape) == 3:\r\n        values = values.reshape(values.shape[0], values.shape[1] * values.shape[2])\r\n        zones = zones.flat\r\n        axis = 1\r\n\r\n    results = {}\r\n    for zone_idx, zone in enumerate(zone_values):\r\n        if hasattr(zone, \'item\'):\r\n            zone = zone.item()\r\n\r\n        zone_mask = zones != zone_idx  # mask out everything that is NOT the zone\r\n        masked = numpy.ma.masked_array(values, mask=values.mask | zone_mask)\r\n\r\n        # skip if all pixels are masked\r\n        if masked.mask.min() == True:\r\n            continue\r\n\r\n        zone_results = dict()\r\n        for statistic in statistics:\r\n            # Call the function dynamically\r\n            if statistic == \'count\':\r\n                zone_results[statistic] = (masked.mask == False).sum(axis=axis)\r\n            else:\r\n                zone_results[statistic] = getattr(masked, statistic)(axis=axis)\r\n\r\n        results[zone] = zone_results\r\n\r\n    return results'"
trefoil/analysis/timeseries.py,0,"b'import numpy\r\n\r\nfrom trefoil.analysis.summary import summarize_areas_by_category, calculate_weighted_statistics\r\nfrom trefoil.utilities.window import Window\r\n\r\n# Days per month from Tim, starting with January.  Useful for weighting statistics when rolling months up to year.\r\n# Assumes 365 day calendar with no leap years\r\nDAYS_PER_MONTH = (31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)\r\nMONTH_LABELS = (\'Jan\', \'Feb\', \'Mar\', \'Apr\', \'May\', \'Jun\', \'Jul\', \'Aug\', \'Sep\', \'Oct\', \'Nov\', \'Dec\')\r\n\r\n\r\ndef extract_categorical_timeseries_by_area(values, areas, timestep_indicies, window=None):\r\n    """"""\r\n    Extract a timeseries array for each category found within the values of variable, using the areas occupied by\r\n    each category within each pixel.\r\n\r\n    :param values: the values for a given variable from the netcdf file.  Must be organized on only 3 dimensions: time, row, col\r\n    :param areas: the areas of each pixel to count toward total area of given category in each timestep\r\n    :param timestep_indicies: the indices over which to extract the time series from the values of variable\r\n    #:param row_offset: the offset from the starting row coordinate of values to the starting row coordinate of areas\r\n    #:param col_offset: the offset from the starting column coordinate of values to the starting column coordinate of areas\r\n    :param window: the subdomain coordinates of areas within values.  Must\r\n    :return: a dictionary of the categorical values found, each with a full timeseries\r\n    """"""\r\n\r\n    assert len(values.shape) == 3\r\n\r\n    if window is not None:\r\n        assert isinstance(window, Window)\r\n\r\n    results = dict()\r\n    num_timesteps = len(timestep_indicies)\r\n    for index in timestep_indicies:\r\n        if window is not None:\r\n            data = window.clip(values, index).ravel()\r\n        else:\r\n            data = values[index].ravel()\r\n        data = numpy.ma.masked_array(data, mask=areas.mask)\r\n        category_summary = summarize_areas_by_category(data.astype(""i""), areas)\r\n        for category in category_summary:\r\n            if not category in results:\r\n                results[category] = numpy.zeros(num_timesteps)\r\n            results[category][index] = category_summary[category]\r\n    return results\r\n\r\n\r\ndef extract_statistics_timeseries_by_weight(values, weights, statistics, window=None):\r\n    """"""\r\n    Extract weighted time series statistics,\r\n    :param values: the values for a given variable from the netcdf file.  Must be organized on only 3 dimensions: time, row, col\r\n    :param weights: the weight of each pixel for the statistic\r\n    :param statistics: a tuple indicating the statistics to be calculated, e.g., (""MEAN"", ""STD"").  Note: statistics\r\n    do not account for different weights between time periods (e.g., months of different durations).\r\n    :param window: the subdomain coordinates of areas within values.\r\n    :return: a dictionary of statistic name to time series array\r\n    """"""\r\n\r\n    assert len(values.shape) == 3\r\n\r\n    if window is not None:\r\n        assert isinstance(window, Window)\r\n\r\n    results = dict()\r\n    for statistic in statistics:\r\n        results[statistic] = numpy.zeros(values.shape[0])\r\n\r\n    for index in xrange(values.shape[0]):\r\n        if window is not None:\r\n            data = window.clip(values, index).ravel()\r\n        else:\r\n            data = values[index].ravel()\r\n        data = numpy.ma.masked_array(data, mask=weights.mask)\r\n        statistics_results = calculate_weighted_statistics(data, weights, statistics)\r\n        for stat_index, statistic in enumerate(statistics):\r\n            results[statistic][index] = statistics_results[stat_index]\r\n    return results\r\n\r\n\r\ndef linear_regression(timesteps, values, full=False):\r\n    """"""Perform linear regression using linear algebra operators\r\n\r\n    Note: does not account for missing data within time series.\r\n\r\n    :param timesteps: 1D array of timesteps to use for x value of linear regression\r\n    :param values: 3D array of data to use for y value of linear regression, assumes timestep is first axis\r\n    :param full: return full statistics or just slopes & intercepts.  Default is False.  If True, requires scipy.\r\n    :returns: (slopes, intercepts) or (slopes, intercepts, r-squared, p-value) if full is True\r\n    """"""\r\n\r\n    # ideas from:\r\n    # http://stackoverflow.com/questions/20343500/efficient-1d-linear-regression-for-each-element-of-3d-numpy-array\r\n    # http://stackoverflow.com/questions/3054191/converting-numpy-lstsq-residual-value-to-r2\r\n    # p-value calculation derived from scipy: https://github.com/scipy/scipy/blob/master/scipy/stats/stats.py\r\n\r\n    assert len(values.shape) == 3\r\n    assert values.shape[0] == timesteps.shape[0]\r\n\r\n    shape = values.shape\r\n\r\n    y = values.reshape((shape[0], shape[1] * shape[2]))\r\n    fit, residuals = numpy.linalg.lstsq(numpy.c_[timesteps, numpy.ones_like(timesteps)], y)[:2]\r\n    slopes = fit[0].reshape((shape[1], shape[2]))\r\n    intercepts = fit[1].reshape((shape[1], shape[2]))\r\n\r\n    mask = None\r\n    if hasattr(values, \'mask\'):\r\n        mask = values.mask[0]\r\n        slopes = numpy.ma.masked_array(slopes, mask=mask)\r\n        intercepts = numpy.ma.masked_array(intercepts, mask=mask)\r\n\r\n    if not full:\r\n        return slopes, intercepts\r\n\r\n\r\n    # T-distribution used for p-value requires scipy\r\n    from scipy.stats.distributions import t as t_dist\r\n\r\n    # Calculate R2 value\r\n    r2 = (1 - residuals / (y.shape[0] * y.var(axis=0)))\r\n    r = numpy.sqrt(r2)\r\n    r2 = r2.reshape((shape[1], shape[2]))\r\n\r\n    # Calculate p-value\r\n    tiny = 1.0e-20\r\n    df = timesteps.shape[0] - 2\r\n    t = r * numpy.sqrt(df / ((1.0 - r + tiny)*(1.0 + r + tiny)))\r\n    p = (2 * t_dist.sf(numpy.abs(t), df)).reshape(shape[1], shape[2])\r\n\r\n    if mask is not None:\r\n        r2 = numpy.ma.masked_array(r2, mask=mask)\r\n        p = numpy.ma.masked_array(p, mask=mask)\r\n\r\n    return slopes, intercepts, r2, p\r\n'"
trefoil/cli/__init__.py,0,"b'import click\n\n\n@click.group(help=""Command line interface for trefoil"")\ndef cli():\n    pass'"
trefoil/cli/calc.py,0,"b'import os\r\nimport glob\r\nimport numpy\r\nimport click\r\nfrom netCDF4 import Dataset\r\nfrom trefoil.netcdf.utilities import copy_variable_dimensions, copy_variable, get_fill_value, copy_attributes\r\nfrom trefoil.cli import cli\r\n\r\n\r\ndef calculate_delta(baseline_data, comp_data, do_proportion=False):\r\n    dif = comp_data - baseline_data\r\n    if not do_proportion:\r\n        return dif\r\n    else:\r\n        return dif / baseline_data  # TODO: fix cases of 0\'s in baseline data with very small value\r\n\r\n\r\n@cli.command(short_help=\'Calculate delta values into new datasets based on a baseline\')\r\n@click.argument(\'baseline\', type=click.Path(exists=True))\r\n@click.argument(\'files\')\r\n@click.argument(\'variable\')\r\n@click.option(\'--bidx\', type=click.INT, default=0, help=\'Index in baseline if 3D (default 0)\')\r\n@click.option(\'--proportion\', is_flag=True, default=False, help=\'Use proportion instead of difference\')\r\n@click.option(\'--outdir\', default=\'./\', help=\'Output directory\')\r\ndef delta(baseline, files, variable, bidx, proportion, outdir):\r\n    if not os.path.exists(outdir):\r\n        os.makedirs(outdir)\r\n\r\n    with Dataset(baseline) as baseline_ds:\r\n        baseline_data = baseline_ds.variables[variable]\r\n        if len(baseline_data.shape) == 3:\r\n            baseline_data = baseline_data[bidx]\r\n\r\n        filenames = glob.glob(files)\r\n        if not filenames:\r\n            raise click.BadParameter(\'No files found matching pattern: {0}\'.format(files), param=\'files\', param_hint=\'files\')\r\n\r\n        for filename in filenames:\r\n            print(\'Calculating delta against\', filename)\r\n            with Dataset(filename) as comp_ds:\r\n                with Dataset(os.path.join(outdir, filename.replace(\'.nc\', \'_delta.nc\')), \'w\') as out_ds:\r\n                    comp_var = comp_ds.variables[variable]\r\n                    copy_variable_dimensions(comp_ds, out_ds, variable)\r\n                    comp_data = comp_var\r\n\r\n                    out_var = out_ds.createVariable(variable + \'_delta\', numpy.float32, dimensions=comp_var.dimensions)\r\n\r\n                    if len(comp_data.shape) == 3:\r\n                        # Assumes 3rd dimension is first\r\n                        for i in range(0, comp_data.shape[0]):\r\n                            out_var[i] = calculate_delta(baseline_data, comp_data[i], proportion)\r\n\r\n                    else:\r\n                        out_var[:] = calculate_delta(baseline_data, comp_data, proportion)\r\n\r\n\r\n#All files must have the same dimensions, and variable must have 3 dimensions with the first being time\r\n@cli.command(short_help=\'Bin time series data by interval\')\r\n@click.argument(\'files\')\r\n@click.argument(\'variable\')\r\n@click.option(\'--outdir\', default=\'./\', help=\'Output directory\')\r\n@click.option(\'--statistic\', type=click.Choice([\'mean\', \'sum\']), default=\'mean\', help=\'Statistic for aggregating data\', show_default=True)\r\n@click.option(\'--interval\', type=click.INT, default=1, help=\'Interval in number of time steps for aggregating data\', show_default=True)\r\n@click.option(\'--zip\', \'zlib\', is_flag=True, default=False, help=\'Use zlib compression of data and coordinate variables\')\r\ndef bin_ts(files, variable, outdir, statistic, interval, zlib):\r\n    """"""\r\n    Bin time series data by an interval, according to a statistic.\r\n\r\n\r\n    """"""\r\n\r\n    if not interval > 0:\r\n        raise click.BadParameter(\'must be > 0\', param=\'--interval\', param_hint=\'--interval\')\r\n\r\n    if not os.path.exists(outdir):\r\n        os.makedirs(outdir)\r\n\r\n    filenames = glob.glob(files)\r\n    if not filenames:\r\n        raise click.BadParameter(\'No files found matching pattern: {0}\'.format(files), param=\'files\', param_hint=\'files\')\r\n\r\n    for filename in filenames:\r\n        click.echo(\'Aggregating data for {0}\'.format(filename))\r\n\r\n        with Dataset(filename) as ds:\r\n            if not variable in ds.variables:\r\n                raise click.BadParameter(\'variable {0} was not found in file: {1}\'.format(variable, filename),\r\n                                         param=\'variable\', param_hint=\'VARIABLE\')\r\n            var_obj = ds.variables[variable]\r\n\r\n            if not len(var_obj.dimensions) == 3:\r\n                raise click.BadParameter(\'variable {0} must have 3 dimensions: {1}\'.format(variable, filename),\r\n                                         param=\'variable\', param_hint=\'VARIABLE\')\r\n\r\n            spatial_dims = var_obj.dimensions[-2:]\r\n            z_dim = var_obj.dimensions[0]\r\n            num_intervals = var_obj.shape[0] // interval\r\n\r\n            if var_obj.shape[0] % interval != 0:\r\n                click.echo(\'WARNING: Anything beyond the last full interval will be dropped\')\r\n\r\n            with Dataset(os.path.join(outdir, filename.replace(\'.nc\', \'_bin.nc\')), \'w\') as out_ds:\r\n                for dim in spatial_dims:\r\n                    copy_variable(ds, out_ds, dim, zlib=zlib)\r\n\r\n                out_ds.createDimension(z_dim, num_intervals)\r\n                if z_dim in ds.variables:\r\n                    z_var = ds.variables[z_dim]\r\n                    out_z_var = out_ds.createVariable(\r\n                        z_dim, z_var.dtype, dimensions=(z_dim,), fill_value=get_fill_value(z_var.dtype), zlib=zlib\r\n                    )\r\n                    copy_attributes(z_var, out_z_var, z_var.ncattrs())\r\n                    out_z_var[:] = z_var[:num_intervals*interval:interval]\r\n\r\n                out_var = out_ds.createVariable(\r\n                    variable, var_obj.dtype, dimensions=var_obj.dimensions,\r\n                    fill_value=get_fill_value(var_obj.dtype), zlib=zlib\r\n                )\r\n                copy_attributes(var_obj, out_var, var_obj.ncattrs())\r\n\r\n                # Due to memory issues, we have to do this more carefully than existing method in analysis.summary.statistic_by_interval\r\n                # TODO: pick appropriate approach based on total size of array\r\n                for i in range(num_intervals):\r\n                    subset = var_obj[i*interval:(i+1)*interval]\r\n\r\n                    if statistic == \'mean\':\r\n                        out_var[i] = subset.mean(axis=0)\r\n                    elif statistic == \'sum\':\r\n                        out_var[i] = subset.sum(axis=0)\r\n'"
trefoil/cli/convert.py,0,"b'import glob\nfrom datetime import datetime\nimport re\nfrom operator import itemgetter\n\nfrom netCDF4 import Dataset\n\nimport numpy\nimport click\nfrom pyproj import Proj\nimport rasterio\nfrom rasterio.crs import CRS\nfrom rasterio.windows import get_data_window, union\n\nfrom trefoil.cli import cli\nfrom trefoil.netcdf.variable import SpatialCoordinateVariables, DateVariable\nfrom trefoil.netcdf.crs import set_crs\nfrom trefoil.netcdf.utilities import get_pack_atts, get_fill_value\nfrom trefoil.geometry.bbox import BBox\n\n\nDATE_REGEX = re.compile(\'%[yYmd]\')  # TODO: add all appropriate strftime directives\n\n\n@cli.command(short_help=\'Convert rasters to NetCDF\')\n@click.argument(\'files\')\n@click.argument(\'output\', type=click.Path())\n@click.argument(\'variable\', type=click.STRING)\n@click.option(\'--dtype\', type=click.Choice([\'float32\', \'float64\', \'int8\', \'int16\', \'int32\', \'uint8\', \'uint16\', \'uint32\']), default=None, help=\'Data type of output variable.  Will be inferred from input raster if not provided.\')\n@click.option(\'--src-crs\', default=None, type=click.STRING, help=\'Source coordinate reference system (limited to EPSG codes, e.g., EPSG:4326).  Will be read from file if not provided.\')\n@click.option(\'--x\', \'x_name\', type=click.STRING, help=\'Name of x dimension and variable (default: lon or x)\')\n@click.option(\'--y\', \'y_name\', type=click.STRING, help=\'Name of y dimension and variable (default: lat or y)\')\n@click.option(\'--z\', \'z_name\', type=click.STRING, default=\'time\', help=\'Name of z dimension and variable\', show_default=True)\n@click.option(\'--datetime-pattern\', type=click.STRING, help=\'strftime-style pattern to parse date and time from the filename\')\n@click.option(\'--netcdf3\', is_flag=True, default=False, help=\'Output in NetCDF3 version instead of NetCDF4\')\n@click.option(\'--zip\', \'compress\', is_flag=True, default=False, help=\'Use zlib compression of data and coordinate variables\')\n@click.option(\'--packed\', is_flag=True, default=False, help=\'Pack floating point values into an integer (will lose precision)\')\n@click.option(\'--xy-dtype\', type=click.Choice([\'float32\', \'float64\']), default=\'float32\', help=\'Data type of spatial coordinate variables.\', show_default=True)\n# @click.option(\'--z-dtype\', type=click.Choice([\'float32\', \'float64\', \'int8\', \'int16\', \'int32\', \'uint8\', \'uint16\', \'uint32\']), default=None, help=\'Data type of z variable.  Will be inferred from values if not provided.\')\n@click.option(\'--calendar\', type=click.STRING, default=\'standard\', help=\'Calendar to use if z dimension is a date type\', show_default=True)\n@click.option(\'--autocrop\', is_flag=True, default=False, help=\'Automatically crop to data bounds (trim NODATA)\')\ndef to_netcdf(\n    files,\n    output,\n    variable,\n    dtype,\n    src_crs,\n    x_name,\n    y_name,\n    z_name,\n    datetime_pattern,\n    netcdf3,\n    compress,\n    packed,\n    xy_dtype,\n    # z_dtype,\n    calendar,\n    autocrop):\n    """"""\n    Convert rasters to NetCDF and stack them according to a dimension.\n\n    X and Y dimension names will be named according to the source projection (lon, lat if geographic projection, x, y\n    otherwise) unless specified.\n\n    Will overwrite an existing NetCDF file.\n\n    Only the first band of the input will be turned into a NetCDF file.\n    """"""\n\n    # TODO: add format string template to this to parse out components\n\n    filenames = list(glob.glob(files))\n    if not filenames:\n        raise click.BadParameter(\'No files found matching that pattern\', param=\'files\', param_hint=\'FILES\')\n\n    z_values = []\n\n    if datetime_pattern is not None:\n        datetimes = (datetime.strptime(x, datetime_pattern) for x in filenames)\n\n        # Sort both datimes and filenames by datetimes\n        z_values, filenames = [list(x) for x in zip(*sorted(zip(datetimes, filenames), key=itemgetter(0)))]\n\n    items = tuple(enumerate(filenames))\n\n    has_z = len(filenames) > 1\n\n    if has_z and not z_name:\n        raise click.BadParameter(\'Required when > 1 input file\', param=\'--z\', param_hint=\'--z\')\n\n    if src_crs:\n        src_crs = CRS.from_string(src_crs)\n\n    template_ds = rasterio.open(filenames[0])\n    src_crs = template_ds.crs or src_crs\n\n    if not src_crs:\n        raise click.BadParameter(\'Required when no CRS information available in source files\', param=\'--src-crs\',\n                                 param_hint=\'--src-crs\')\n\n    prj = Proj(**src_crs.to_dict())\n    bounds = template_ds.bounds\n    width = template_ds.width\n    height = template_ds.height\n    window = None\n\n    src_dtype = numpy.dtype(template_ds.dtypes[0])\n    dtype = numpy.dtype(dtype) if dtype else src_dtype\n\n    if dtype == src_dtype:\n        fill_value = template_ds.nodata\n        if src_dtype.kind in (\'u\', \'i\'):\n            # nodata always comes from rasterio as floating point\n            fill_value = int(fill_value)\n    else:\n        fill_value = get_fill_value(dtype)\n\n    x_name = x_name or (\'lon\' if src_crs.is_geographic else \'x\')\n    y_name = y_name or (\'lat\' if src_crs.is_geographic else \'y\')\n\n    var_kwargs = {\n        \'fill_value\': fill_value\n    }\n\n    format = \'NETCDF3_CLASSIC\' if netcdf3 else \'NETCDF4\'\n\n    with Dataset(output, \'w\', format=format) as out:\n        if packed or autocrop:\n            mins = []\n            maxs = []\n            windows = []\n\n            click.echo(\'Inspecting input datasets...\')\n            with click.progressbar(items) as iter:\n                for index, filename in iter:\n                    with rasterio.open(filename) as src:\n                        data = src.read(1, masked=True)\n                        if packed:\n                            mins.append(data.min())\n                            maxs.append(data.max())\n                        if autocrop:\n                            data_window = get_data_window(data)\n                            if data_window != ((0, height), (0, width)):\n                                windows.append(data_window)\n\n            if packed:\n                min_value = min(mins)\n                max_value = max(maxs)\n                scale, offset = get_pack_atts(dtype, min_value, max_value)\n            if autocrop and windows:\n                window = union(windows)\n                bounds = template_ds.window_bounds(window)\n                height = window[0][1] - window[0][0]\n                width = window[1][1] - window[1][0]\n\n        coords = SpatialCoordinateVariables.from_bbox(BBox(bounds, prj), width, height, xy_dtype)\n        coords.add_to_dataset(out, x_name, y_name, zlib=compress)\n\n        var_dimensions = [y_name, x_name]\n        shape = list(coords.shape)\n        if has_z:\n            shape.insert(0, len(filenames))\n            out.createDimension(z_name, shape[0])\n            var_dimensions.insert(0, z_name)\n            if z_values:\n                dates = DateVariable(numpy.array(z_values),\n                                     units_start_date=z_values[0], calendar=calendar)\n                dates.add_to_dataset(out, z_name)\n\n\n        click.echo(\'Creating {0}:{1} with shape {2}\'.format(output, variable, shape))\n\n        out_var = out.createVariable(variable, dtype, dimensions=var_dimensions,\n                                     zlib=compress, **var_kwargs)\n        set_crs(out, variable, prj, set_proj4_att=True)\n\n        if packed:\n            out_var.setncattr(\'scale_factor\', scale)\n            out_var.setncattr(\'add_offset\', offset)\n\n\n\n        click.echo(\'Copying data from input files...\')\n        with click.progressbar(items) as iter:\n            for index, filename in iter:\n                with rasterio.open(filename) as src:\n                    data = src.read(1, masked=True, window=window)\n\n                    if has_z:\n                        out_var[index, :] = data\n                    else:\n                        out_var[:] = data\n\n                out.sync()\n'"
trefoil/cli/crs.py,0,"b""import click\nfrom netCDF4 import Dataset\nfrom pyproj import Proj\n\nfrom trefoil.netcdf import crs\nfrom trefoil.cli import cli\n\n\n@cli.command(short_help='Set spatial reference information for variables in an existing dataset')\n@click.argument('filename', type=click.Path(exists=True))\n@click.argument('proj4')\n@click.option('--only', 'variables', default=None, help='Set CRS only for the specified variables')\ndef set_crs(filename, proj4, variables):\n    try:\n        proj = Proj(proj4)\n    except RuntimeError:\n        raise click.BadArgumentUsage('Invalid projection: ' + proj4)\n\n    with Dataset(filename, 'a') as ds:\n        if not variables:\n            variables_li = [v for v in ds.variables if v not in ds.dimensions]\n        else:\n            variables_li = [x.strip() for x in variables.split(',')]\n            bad_variables = set(variables_li).difference(ds.variables.keys())\n            if bad_variables:\n                raise click.BadArgumentUsage(\n                    'The following variables do not exist in this dataset: ' + ', '.join(bad_variables)\n                )\n\n        for variable in variables_li:\n            crs.set_crs(ds, variable, proj)\n\n"""
trefoil/cli/extract.py,0,"b'import os\r\nimport glob\r\nimport click\r\nfrom netCDF4 import Dataset\r\nfrom trefoil.netcdf.utilities import copy_variable\r\nfrom trefoil.cli import cli\r\n\r\n\r\n@cli.command(short_help=\'Extract variables from files into new datasets in a new directory\')\r\n@click.argument(\'files\')\r\n@click.argument(\'variables\')\r\n@click.argument(\'outdir\')\r\n@click.option(\'--compress\', is_flag=True, default=False, help=\'compress the variable (least_significant_digit=3)\')\r\n#TODO: add ability to subset by slices\r\ndef extract(files, variables, outdir, compress):\r\n    """"""Extracts variables from files into new datasets.  Files will be named the same, and placed in outdir""""""\r\n\r\n    filenames = glob.glob(files)\r\n    if not filenames:\r\n        raise click.BadParameter(\'No files found matching that pattern\', param=\'files\', param_hint=\'FILES\')\r\n\r\n    variables = variables.split(\',\')\r\n\r\n    if not os.path.exists(outdir):\r\n        os.makedirs(outdir)\r\n\r\n    kwargs = {}\r\n    if compress:\r\n        kwargs.update({\r\n            \'zlib\': True,\r\n            \'least_significant_digit\': 3\r\n        })\r\n\r\n    for filename in filenames:\r\n        print(\'Extracting from {0}\'.format(filename))\r\n        with Dataset(filename) as infile:\r\n            with Dataset(os.path.join(outdir, os.path.split(filename)[1]), \'w\') as outfile:\r\n                for variable in variables:\r\n                    copy_variable(infile, outfile, variable, **kwargs)\r\n'"
trefoil/cli/info.py,0,"b'import os\r\nimport glob\r\nimport click\r\nfrom netCDF4 import Dataset\r\nfrom trefoil.netcdf.describe import describe as describe_netcdf\r\nfrom trefoil.netcdf.utilities import collect_statistics, get_dtype_string\r\nfrom trefoil.cli import cli\r\nfrom trefoil.cli.utilities import get_mask\r\n\r\n\r\ndef print_dict(d, depth=0):\r\n    space = \'  \'\r\n    keys = sorted(d.keys())\r\n    for key in keys:\r\n        value = d[key]\r\n        if isinstance(value, dict) and len(value) > 1:\r\n            click.echo(\'{0}{1}:\'.format(space * depth, key))\r\n            print_dict(value, depth=depth+1)\r\n            click.echo(\'\')\r\n        else:\r\n            if isinstance(value, float):\r\n                click.echo(\'{0}{1}: {2:g}\'.format(space * depth, key, value))\r\n            else:\r\n                click.echo(\'{0}{1}: {2}\'.format(space * depth, key, value))\r\n\r\n\r\n@cli.command(short_help=\'Describe netCDF files\')\r\n@click.argument(\'files\')\r\ndef describe(files):\r\n    """"""Describe netCDF datasets""""""\r\n\r\n    filenames = glob.glob(files)\r\n    if not filenames:\r\n        raise click.BadParameter(\'No files found matching that pattern\', param=\'files\', param_hint=\'FILES\')\r\n\r\n    for filename in filenames:\r\n        click.echo(\'# {0} #\'.format(filename))\r\n        results = describe_netcdf(filename)\r\n        click.echo(\'## Attributes ##\')\r\n        print_dict(results[\'attributes\'])\r\n        click.echo(\'\\n## Dimensions ##\')\r\n        print_dict(results[\'dimensions\'])\r\n        click.echo(\'\\n## Variables ##\')\r\n        print_dict(results[\'variables\'])\r\n        click.echo(\'\')\r\n\r\n\r\n@cli.command(short_help=\'List variables in netCDF file\')\r\n@click.argument(\'filename\', type=click.Path(exists=True))\r\ndef variables(filename):\r\n    ds = Dataset(filename)\r\n    \r\n    click.echo(\'## Data Variables ##\')\r\n    variables = [v for v in ds.variables if v not in ds.dimensions]\r\n    variables.sort()\r\n    for varname in variables:\r\n        variable = ds.variables[varname]\r\n        click.echo(\'{0}: dimensions{1}  dtype:{2}\'.format(varname, tuple([str(d) for d in variable.dimensions]), get_dtype_string(variable)))\r\n\r\n    click.echo(\'\\n## Dimension Variables ##\')\r\n    variables = [v for v in ds.variables if v in ds.dimensions]\r\n    variables.sort()\r\n    for varname in variables:\r\n        variable = ds.variables[varname]\r\n        click.echo(\'{0}({1})  dtype:{2}\'.format(varname, len(ds.dimensions[varname]), get_dtype_string(variable)))\r\n\r\n\r\n@cli.command(short_help=\'Display statistics for variables within netCDF files\')\r\n@click.argument(\'files\')\r\n@click.argument(\'variables\')\r\n@click.option(\'--mask\', \'mask_path\', default=None, help=\'Mask dataset:variable (e.g., mask.nc:mask).  Mask variable assumed to be named ""mask"" unless otherwise provided\')\r\ndef stats(files, variables, mask_path):\r\n    """"""Calculate statistics for each variable across all files""""""\r\n\r\n    filenames = glob.glob(files)\r\n    if not filenames:\r\n        raise click.BadParameter(\'No files found matching that pattern\', param=\'files\', param_hint=\'FILES\')\r\n\r\n    mask = get_mask(mask_path) if mask_path is not None else None\r\n\r\n    click.echo(\'Collecting statistics from {0} files\'.format(len(filenames)))\r\n\r\n    variables = variables.split(\',\')\r\n\r\n    statistics = collect_statistics(filenames, variables, mask=mask)\r\n    for variable in variables:\r\n        click.echo(\'## {0} ##\'.format(variable))\r\n        print_dict(statistics[variable])\r\n        click.echo(\'\')\r\n'"
trefoil/cli/main.py,0,"b'from trefoil.cli import cli\n\nfrom trefoil.cli.calc import delta, bin_ts\nfrom trefoil.cli.convert import to_netcdf\nfrom trefoil.cli.crs import set_crs\nfrom trefoil.cli.extract import extract\nfrom trefoil.cli.info import describe, variables, stats\nfrom trefoil.cli.mask import mask\nfrom trefoil.cli.map_eems import map_eems\nfrom trefoil.cli.render_netcdf import render_netcdf\nfrom trefoil.cli.render_tif import render_tif\nfrom trefoil.cli.warp import warp\nfrom trefoil.cli.zones import zones\n'"
trefoil/cli/map_eems.py,0,"b'import os\r\nimport tempfile\r\nimport json\r\nimport webbrowser\r\nfrom netCDF4 import Dataset\r\nimport click\r\nfrom pyproj import Proj\r\nfrom rasterio.crs import CRS\r\nfrom rasterio.warp import calculate_default_transform\r\nfrom rasterio.enums import Resampling\r\nfrom jinja2 import Environment, PackageLoader\r\n\r\nfrom trefoil.netcdf.variable import SpatialCoordinateVariables\r\nfrom trefoil.geometry.bbox import BBox\r\nfrom trefoil.netcdf.warp import warp_array\r\nfrom trefoil.netcdf.crs import get_crs, is_geographic\r\nfrom trefoil.cli import cli\r\nfrom trefoil.cli.utilities import render_image, palette_to_stretched_renderer, get_leaflet_anchors\r\n\r\n\r\n# Requires EEMS installed from: https://github.com/MikeTheReader/EEMS\r\n# TWSAndExplorer branch, v2.0.1\r\n# imported inline below\r\n\r\n\r\n\r\n# Common defaults for usability wins\r\nDEFAULT_PALETTES = {\r\n    \'fuzzy\': \'colorbrewer.diverging.Spectral_5\',\r\n    \'raw\': \'colorbrewer.sequential.Greys_5\'\r\n}\r\n\r\n\r\n@cli.command(short_help=""Render a NetCDF EEMS model to a web map"")\r\n@click.argument(\'EEMS_FILE\', type=click.Path(exists=True))\r\n# @click.argument(\'output_directory\', type=click.Path())  # TODO: temp file instead?\r\n@click.option(\'--scale\', default=1.0, help=\'Scale factor for data pixel to screen pixel size\')\r\n@click.option(\'--format\', default=\'png\', type=click.Choice([\'png\', \'jpg\', \'webp\']), show_default=True)\r\n# Projection related options\r\n@click.option(\'--src-crs\', \'--src_crs\', default=None, type=click.STRING, help=\'Source coordinate reference system (limited to EPSG codes, e.g., EPSG:4326).  Will be read from file if not provided.\')\r\n@click.option(\'--resampling\', default=\'nearest\', type=click.Choice((\'nearest\', \'cubic\', \'lanczos\', \'mode\')), help=\'Resampling method for reprojection (default: nearest\')\r\ndef map_eems(\r\n        eems_file,\r\n        # output_directory,\r\n        scale,\r\n        format,\r\n        src_crs,\r\n        resampling):\r\n    """"""\r\n    Render a NetCDF EEMS model to a web map.\r\n    """"""\r\n\r\n    from EEMSBasePackage import EEMSCmd, EEMSProgram\r\n\r\n\r\n    model = EEMSProgram(eems_file)\r\n\r\n    # For each data producing command, store the netcdf file that contains it\r\n    file_vars = dict()\r\n    raw_variables = set()\r\n    for cmd in model.orderedCmds:  # This is bottom up, may want to invert\r\n        filename = None\r\n        variable = None\r\n        if cmd.HasResultName():\r\n            filename = cmd.GetParam(\'OutFileName\')\r\n            variable = cmd.GetResultName()\r\n        elif cmd.IsReadCmd():\r\n            filename = cmd.GetParam(\'OutFileName\')\r\n            variable = cmd.GetParam(\'NewFieldName\')\r\n            raw_variables.add(variable)\r\n\r\n        if filename and variable:\r\n            if not filename in file_vars:\r\n                file_vars[filename] = []\r\n            file_vars[filename].append(variable)\r\n\r\n\r\n    filenames =file_vars.keys()\r\n    for filename in filenames:\r\n        if not os.path.exists(filename):\r\n            raise click.ClickException(\'Could not find data file from EEMS model: {0}\'.format(filename))\r\n\r\n\r\n    dst_crs = \'EPSG:3857\'\r\n\r\n    output_directory = tempfile.mkdtemp()\r\n    click.echo(\'Using temp directory: {0}\'.format(output_directory))\r\n    # if not os.path.exists(output_directory):\r\n    #     os.makedirs(output_directory)\r\n\r\n    # Since fuzzy renderer is hardcoded, we can output it now\r\n    fuzzy_renderer = palette_to_stretched_renderer(DEFAULT_PALETTES[\'fuzzy\'], \'1,-1\')\r\n    fuzzy_renderer.get_legend(image_height=150)[0].to_image().save(os.path.join(output_directory, \'fuzzy_legend.png\'))\r\n\r\n    template_filename = filenames[0]\r\n    template_var = file_vars[template_filename][0]\r\n    with Dataset(template_filename) as ds:\r\n        var_obj = ds.variables[template_var]\r\n        dimensions = var_obj.dimensions\r\n        shape = var_obj.shape\r\n        num_dimensions = len(shape)\r\n        if num_dimensions != 2:\r\n            raise click.ClickException(\'Only 2 dimensions are allowed on data variables for now\')\r\n\r\n        ds_crs = get_crs(ds, template_var)\r\n        if not ds_crs and is_geographic(ds, template_var):\r\n            ds_crs = \'EPSG:4326\'  # Assume all geographic data is WGS84\r\n\r\n        src_crs = CRS.from_string(ds_crs) if ds_crs else CRS({\'init\': src_crs}) if src_crs else None\r\n\r\n        # get transforms, assume last 2 dimensions on variable are spatial in row, col order\r\n        y_dim, x_dim = dimensions[-2:]\r\n        coords = SpatialCoordinateVariables.from_dataset(\r\n            ds, x_dim, y_dim, projection=Proj(src_crs) if src_crs else None\r\n        )\r\n    #\r\n    #     if mask is not None and not mask.shape == shape[-2:]:\r\n    #         # Will likely break before this if collecting statistics\r\n    #         raise click.BadParameter(\r\n    #             \'mask variable shape does not match shape of input spatial dimensions\',\r\n    #             param=\'--mask\', param_hint=\'--mask\'\r\n    #         )\r\n    #\r\n        if not src_crs:\r\n            raise click.BadParameter(\'must provide src_crs to reproject\',\r\n                                     param=\'--src-crs\',\r\n                                     param_hint=\'--src-crs\')\r\n\r\n        dst_crs = CRS.from_string(dst_crs)\r\n\r\n        src_height, src_width = coords.shape\r\n        dst_transform, dst_width, dst_height = calculate_default_transform(\r\n            src_crs, dst_crs, src_width, src_height,\r\n            *coords.bbox.as_list()\r\n        )\r\n\r\n        reproject_kwargs = {\r\n            \'src_crs\': src_crs,\r\n            \'src_transform\': coords.affine,\r\n            \'dst_crs\': dst_crs,\r\n            \'dst_transform\': dst_transform,\r\n            \'resampling\': getattr(Resampling, resampling),\r\n            \'dst_shape\': (dst_height, dst_width)\r\n        }\r\n\r\n        if not (dst_crs or src_crs):\r\n            raise click.BadParameter(\'must provide valid src_crs to get interactive map\',\r\n                                     param=\'--src-crs\', param_hint=\'--src-crs\')\r\n\r\n        leaflet_anchors = get_leaflet_anchors(BBox.from_affine(dst_transform, dst_width, dst_height,\r\n                                                       projection=Proj(dst_crs) if dst_crs else None))\r\n\r\n\r\n    layers = {}\r\n    for filename in filenames:\r\n        with Dataset(filename) as ds:\r\n            click.echo(\'Processing dataset {0}\'.format(filename))\r\n\r\n            for variable in file_vars[filename]:\r\n                click.echo(\'Processing variable {0}\'.format(variable))\r\n\r\n                if not variable in ds.variables:\r\n                    raise click.ClickException(\'variable {0} was not found in file: {1}\'.format(variable, filename))\r\n\r\n                var_obj = ds.variables[variable]\r\n                if not var_obj.dimensions == dimensions:\r\n                    raise click.ClickException(\'All datasets must have the same dimensions for {0}\'.format(variable))\r\n\r\n                data = var_obj[:]\r\n                # if mask is not None:\r\n                #     data = numpy.ma.masked_array(data, mask=mask)\r\n\r\n\r\n                if variable in raw_variables:\r\n                    palette = DEFAULT_PALETTES[\'raw\']\r\n                    palette_stretch = \'{0},{1}\'.format(data.max(), data.min())\r\n\r\n                    renderer = palette_to_stretched_renderer(palette, palette_stretch)\r\n                    renderer.get_legend(image_height=150, max_precision=2)[0].to_image().save(os.path.join(output_directory, \'{0}_legend.png\'.format(variable)))\r\n                else:\r\n                    renderer = fuzzy_renderer\r\n\r\n                image_filename = os.path.join(output_directory, \'{0}.{1}\'.format(variable, format))\r\n                data = warp_array(data, **reproject_kwargs)\r\n                render_image(renderer, data, image_filename, scale=scale, format=format)\r\n\r\n                local_filename = os.path.split(image_filename)[1]\r\n                layers[variable] = local_filename\r\n\r\n\r\n    index_html = os.path.join(output_directory, \'index.html\')\r\n    with open(index_html, \'w\') as out:\r\n        template = Environment(loader=PackageLoader(\'trefoil.cli\')).get_template(\'eems_map.html\')\r\n        out.write(\r\n            template.render(\r\n                layers=json.dumps(layers),\r\n                bounds=str(leaflet_anchors),\r\n                tree=[[cmd, depth] for (cmd, depth) in model.GetCmdTree()],\r\n                raw_variables=list(raw_variables)\r\n            )\r\n        )\r\n\r\n    webbrowser.open(index_html)\r\n'"
trefoil/cli/mask.py,0,"b'import click\r\nfrom pyproj import Proj\r\nfrom netCDF4 import Dataset\r\nimport numpy\r\nimport fiona\r\nimport rasterio\r\nfrom rasterio.crs import CRS\r\nfrom rasterio.features import rasterize\r\nfrom rasterio.warp import transform_geom\r\nfrom rasterio.rio.options import file_in_arg, file_out_arg\r\n\r\nfrom trefoil.cli import cli\r\nfrom trefoil.netcdf.variable import SpatialCoordinateVariables\r\nfrom trefoil.netcdf.crs import get_crs, is_geographic\r\nfrom trefoil.netcdf.utilities import data_variables, get_fill_value\r\n\r\n\r\n@cli.command(short_help=\'Create a NetCDF mask from a shapefile\')\r\n@file_in_arg\r\n@file_out_arg\r\n@click.option(\'--variable\', type=click.STRING, default=\'mask\', help=\'Name of output mask variable\', show_default=True)\r\n@click.option(\'--like\', help=\'Template NetCDF dataset\', type=click.Path(exists=True), required=True)\r\n@click.option(\'--netcdf3\', is_flag=True, default=False, help=\'Output in NetCDF3 version instead of NetCDF4\')\r\n@click.option(\'--all-touched\', is_flag=True, default=False, help=\'Turn all touched pixels into mask (otherwise only pixels with centroid in features)\')\r\n@click.option(\'--invert\', is_flag=True, default=False, help=\'Create inverted mask (opposite of numpy mask, True where there are features)\')\r\n@click.option(\'--zip\', is_flag=True, default=False, help=\'Use zlib compression of data and coordinate variables\')\r\n# TODO: add option to create a mask for each feature\r\ndef mask(\r\n    input,\r\n    output,\r\n    variable,\r\n    like,\r\n    netcdf3,\r\n    all_touched,\r\n    invert,\r\n    zip):\r\n\r\n    """"""\r\n    Create a NetCDF mask from a shapefile.\r\n\r\n    Values are equivalent to a numpy mask: 0 for unmasked areas, and 1 for masked areas.\r\n\r\n    Template NetCDF dataset must have a valid projection defined or be inferred from dimensions (e.g., lat / long)\r\n    """"""\r\n\r\n    with Dataset(like) as template_ds:\r\n        template_varname = data_variables(template_ds).keys()[0]\r\n        template_variable = template_ds.variables[template_varname]\r\n        template_crs = get_crs(template_ds, template_varname)\r\n\r\n        if template_crs:\r\n            template_crs = CRS.from_string(template_crs)\r\n        elif is_geographic(template_ds, template_varname):\r\n            template_crs = CRS({\'init\': \'EPSG:4326\'})\r\n        else:\r\n            raise click.UsageError(\'template dataset must have a valid projection defined\')\r\n\r\n        spatial_dimensions = template_variable.dimensions[-2:]\r\n        mask_shape = template_variable.shape[-2:]\r\n\r\n        template_y_name, template_x_name = spatial_dimensions\r\n        coords = SpatialCoordinateVariables.from_dataset(\r\n            template_ds,\r\n            x_name=template_x_name,\r\n            y_name=template_y_name,\r\n            projection=Proj(**template_crs.to_dict())\r\n        )\r\n\r\n\r\n    with fiona.open(input, \'r\') as shp:\r\n        transform_required = CRS(shp.crs) != template_crs\r\n\r\n        # Project bbox for filtering\r\n        bbox = coords.bbox\r\n        if transform_required:\r\n            bbox = bbox.project(Proj(**shp.crs), edge_points=21)\r\n\r\n        geometries = []\r\n        for f in shp.filter(bbox=bbox.as_list()):\r\n            geom = f[\'geometry\']\r\n            if transform_required:\r\n                geom = transform_geom(shp.crs, template_crs, geom)\r\n\r\n            geometries.append(geom)\r\n\r\n    click.echo(\'Converting {0} features to mask\'.format(len(geometries)))\r\n\r\n    if invert:\r\n        fill_value = 0\r\n        default_value = 1\r\n    else:\r\n        fill_value = 1\r\n        default_value = 0\r\n\r\n    with rasterio.Env():\r\n        # Rasterize features to 0, leaving background as 1\r\n        mask = rasterize(\r\n            geometries,\r\n            out_shape=mask_shape,\r\n            transform=coords.affine,\r\n            all_touched=all_touched,\r\n            fill=fill_value,\r\n            default_value=default_value,\r\n            dtype=numpy.uint8\r\n        )\r\n\r\n    format = \'NETCDF3_CLASSIC\' if netcdf3 else \'NETCDF4\'\r\n    dtype = \'int8\' if netcdf3 else \'uint8\'\r\n\r\n    with Dataset(output, \'w\', format=format) as out:\r\n        coords.add_to_dataset(out, template_x_name, template_y_name)\r\n        out_var = out.createVariable(variable, dtype, dimensions=spatial_dimensions, zlib=zip,\r\n                                     fill_value=get_fill_value(dtype))\r\n        out_var[:] = mask\r\n'"
trefoil/cli/render_netcdf.py,0,"b'""""""\r\nRender a set of NetCDF files to images.\r\n\r\nStretched renderers may have one of the following colormap values:\r\n1.0 (absolute)\r\nmax (calculate max across datasets)\r\n0.5*max (calculate max across datasets, and multiply by value)\r\n\r\nTODO:\r\n* connect palettes to create matching class breaks\r\n* combine palette and scale over which to stretch\r\n\r\n\r\n""""""\r\n\r\nimport os\r\nimport glob\r\nimport json\r\nimport webbrowser\r\n\r\nimport numpy\r\nfrom netCDF4 import Dataset\r\nimport click\r\nfrom pyproj import Proj\r\nfrom rasterio.crs import CRS\r\nfrom rasterio.warp import calculate_default_transform\r\nfrom rasterio.enums import Resampling\r\nfrom jinja2 import Environment, PackageLoader\r\n\r\nfrom trefoil.render.renderers.utilities import renderer_from_dict\r\nfrom trefoil.render.renderers.legend import composite_elements\r\nfrom trefoil.netcdf.variable import SpatialCoordinateVariables\r\nfrom trefoil.geometry.bbox import BBox\r\nfrom trefoil.netcdf.warp import warp_array\r\nfrom trefoil.netcdf.crs import get_crs, is_geographic\r\nfrom trefoil.cli import cli\r\nfrom trefoil.cli.utilities import (\r\n    render_image, collect_statistics, colormap_to_stretched_renderer,\r\n    palette_to_stretched_renderer, palette_to_classified_renderer,\r\n    get_leaflet_anchors, get_mask)\r\n\r\n\r\n\r\n# Common defaults for usability wins\r\nDEFAULT_PALETTES = {\r\n    \'tmin\': (\'colorbrewer.sequential.YlOrRd_5\', \'min,max\'),\r\n    \'tmax\': (\'colorbrewer.sequential.YlOrRd_5\', \'min,max\'),\r\n    \'ppt\': (\'colorbrewer.diverging.RdYlGn_5\', \'min,max\'),\r\n    \'pet\': (\'colorbrewer.diverging.RdYlGn_5\', \'max,min\')\r\n}\r\n\r\n\r\n\r\n@cli.command(short_help=""Render netcdf files to images"")\r\n@click.argument(\'filename_pattern\')\r\n@click.argument(\'variable\')\r\n@click.argument(\'output_directory\', type=click.Path())\r\n@click.option(\'--renderer_file\', help=\'File containing renderer JSON\', type=click.Path(exists=True))\r\n@click.option(\'--save\', \'save_file\', type=click.Path(), default=None, help=\'Save renderer to renderer_file\')\r\n@click.option(\'--renderer_type\', type=click.Choice([\'stretched\', \'classified\']), default=\'stretched\', help=\'Name of renderer.\', show_default=True)\r\n@click.option(\'--colormap\', default=None, help=\'Provide colormap as comma-separated lookup of value to hex color code.  (Example: -1:#FF0000,1:#0000FF)\')\r\n@click.option(\'--fill\', type=click.FLOAT, default=None, help=\'Fill value (will be rendered as transparent)\')\r\n@click.option(\'--colorspace\', default=\'hsv\', type=click.Choice([\'hsv\', \'rgb\']), help=\'Color interpolation colorspace\')\r\n@click.option(\'--palette\', default=None, help=\'Palettable color palette (Example: colorbrewer.sequential.Blues_3)\')\r\n@click.option(\'--palette_stretch\', default=\'min,max\', help=\'Value range over which to apply the palette when using stretched renderer (comma-separated)\', show_default=True)\r\n@click.option(\'--scale\', default=1.0, help=\'Scale factor for data pixel to screen pixel size\')\r\n@click.option(\'--id_variable\', help=\'ID variable used to provide IDs during image generation.  Must be of same dimensionality as first dimension of variable (example: time).  Guessed from the 3rd dimension\')\r\n@click.option(\'--lh\', default=150, help=\'Height of the legend in pixels [default: 150]\')\r\n@click.option(\'--legend_breaks\', default=None, type=click.INT, help=\'Number of breaks to show on legend for stretched renderer\')\r\n@click.option(\'--legend_ticks\', default=None, type=click.STRING, help=\'Legend tick values for stretched renderer\')\r\n@click.option(\'--legend_precision\', default=2, type=click.INT, help=\'Number of decimal places of precision for legend labels\', show_default=True)\r\n@click.option(\'--format\', default=\'png\', type=click.Choice([\'png\', \'jpg\', \'webp\']), show_default=True)\r\n# Projection related options\r\n@click.option(\'--src-crs\', \'--src_crs\', default=None, type=click.STRING, help=\'Source coordinate reference system (limited to EPSG codes, e.g., EPSG:4326).  Will be read from file if not provided.\')\r\n@click.option(\'--dst-crs\', \'--dst_crs\', default=None, type=click.STRING, help=\'Destination coordinate reference system\')\r\n@click.option(\'--res\', default=None, type=click.FLOAT, help=\'Destination pixel resolution in destination coordinate system units\' )\r\n@click.option(\'--resampling\', default=\'nearest\', type=click.Choice((\'nearest\', \'cubic\', \'lanczos\', \'mode\')), help=\'Resampling method for reprojection (default: nearest\')\r\n@click.option(\'--anchors\', default=False, is_flag=True, help=\'Print anchor coordinates for use in Leaflet ImageOverlay\')\r\n@click.option(\'--map\', \'interactive_map\', default=False, is_flag=True, help=\'Open in interactive map\')\r\n# Other options\r\n@click.option(\'--mask\', \'mask_path\', default=None, help=\'Mask dataset:variable (e.g., mask.nc:mask).  Mask variable assumed to be named ""mask"" unless otherwise provided\')\r\ndef render_netcdf(\r\n        filename_pattern,\r\n        variable,\r\n        output_directory,\r\n        renderer_file,\r\n        save_file,\r\n        renderer_type,\r\n        colormap,\r\n        fill,\r\n        colorspace,\r\n        palette,\r\n        palette_stretch,\r\n        scale,\r\n        id_variable,\r\n        lh,\r\n        legend_breaks,\r\n        legend_ticks,\r\n        legend_precision,\r\n        format,\r\n        src_crs,\r\n        dst_crs,\r\n        res,\r\n        resampling,\r\n        anchors,\r\n        interactive_map,\r\n        mask_path):\r\n    """"""\r\n    Render netcdf files to images.\r\n\r\n    colormap is ignored if renderer_file is provided\r\n\r\n    --dst-crs is ignored if using --map option (always uses EPSG:3857\r\n\r\n    If no colormap or palette is provided, a default palette may be chosen based on the name of the variable.\r\n\r\n    If provided, mask must be 1 for areas to be masked out, and 0 otherwise.  It\r\n    must be in the same CRS as the input datasets, and have the same spatial\r\n    dimensions.\r\n\r\n    """"""\r\n\r\n    # Parameter overrides\r\n    if interactive_map:\r\n        dst_crs = \'EPSG:3857\'\r\n\r\n    filenames = glob.glob(filename_pattern)\r\n    if not filenames:\r\n        raise click.BadParameter(\'No files found matching that pattern\', param=\'filename_pattern\', param_hint=\'FILENAME_PATTERN\')\r\n\r\n    if not os.path.exists(output_directory):\r\n        os.makedirs(output_directory)\r\n\r\n    mask = get_mask(mask_path) if mask_path is not None else None\r\n\r\n    if renderer_file is not None and not save_file:\r\n        if not os.path.exists(renderer_file):\r\n            raise click.BadParameter(\'does not exist\', param=\'renderer_file\', param_hint=\'renderer_file\')\r\n\r\n        # see https://bitbucket.org/databasin/ncdjango/wiki/Home for format\r\n        renderer_dict = json.loads(open(renderer_file).read())\r\n\r\n        if variable in renderer_dict and not \'colors\' in renderer_dict:\r\n            renderer_dict = renderer_dict[variable]\r\n\r\n        renderer_type = renderer_dict[\'type\']\r\n        if renderer_type == \'stretched\':\r\n            colors = \',\'.join([str(c[0]) for c in renderer_dict[\'colors\']])\r\n            if \'min\' in colors or \'max\' in colors or \'mean\' in colors:\r\n                statistics = collect_statistics(filenames, (variable,), mask=mask)[variable]\r\n                for entry in renderer_dict[\'colors\']:\r\n                    if isinstance(entry[0], basestring):\r\n                        if entry[0] in (\'min\', \'max\', \'mean\'):\r\n                            entry[0] = statistics[entry[0]]\r\n                        elif \'*\' in entry[0]:\r\n                            rel_value, statistic = entry[0].split(\'*\')\r\n                            entry[0] = float(rel_value) * statistics[statistic]\r\n\r\n        renderer = renderer_from_dict(renderer_dict)\r\n\r\n    else:\r\n\r\n        if renderer_type == \'stretched\':\r\n            if palette is not None:\r\n                renderer = palette_to_stretched_renderer(palette, palette_stretch, filenames, variable, fill_value=fill, mask=mask)\r\n\r\n            elif colormap is None and variable in DEFAULT_PALETTES:\r\n                palette, palette_stretch = DEFAULT_PALETTES[variable]\r\n                renderer = palette_to_stretched_renderer(palette, palette_stretch, filenames, variable, fill_value=fill, mask=mask)\r\n\r\n            else:\r\n                if colormap is None:\r\n                    colormap = \'min:#000000,max:#FFFFFF\'\r\n                renderer = colormap_to_stretched_renderer(colormap, colorspace, filenames, variable, fill_value=fill, mask=mask)\r\n\r\n        elif renderer_type == \'classified\':\r\n            if not palette:\r\n                raise click.BadParameter(\'palette required for classified (for now)\',\r\n                                         param=\'--palette\', param_hint=\'--palette\')\r\n\r\n            renderer = palette_to_classified_renderer(palette, filenames, variable, method=\'equal\', fill_value=fill, mask=mask)  # TODO: other methods\r\n\r\n    if save_file:\r\n\r\n        if os.path.exists(save_file):\r\n            with open(save_file, \'r+\') as output_file:\r\n                data = json.loads(output_file.read())\r\n                output_file.seek(0)\r\n                output_file.truncate()\r\n                data[variable] = renderer.serialize()\r\n                output_file.write(json.dumps(data, indent=4))\r\n        else:\r\n            with open(save_file, \'w\') as output_file:\r\n                output_file.write(json.dumps({variable: renderer.serialize()}))\r\n\r\n    if renderer_type == \'stretched\':\r\n        if legend_ticks is not None and not legend_breaks:\r\n            legend_ticks = [float(v) for v in legend_ticks.split(\',\')]\r\n\r\n        legend = renderer.get_legend(image_height=lh, breaks=legend_breaks, ticks=legend_ticks, max_precision=legend_precision)[0].to_image()\r\n\r\n    elif renderer_type == \'classified\':\r\n        legend = composite_elements(renderer.get_legend())\r\n\r\n    legend.save(os.path.join(output_directory, \'{0}_legend.png\'.format(variable)))\r\n\r\n    with Dataset(filenames[0]) as ds:\r\n        var_obj = ds.variables[variable]\r\n        dimensions = var_obj.dimensions\r\n        shape = var_obj.shape\r\n        num_dimensions = len(shape)\r\n\r\n        if num_dimensions == 3:\r\n            if id_variable:\r\n                if shape[0] != ds.variables[id_variable][:].shape[0]:\r\n                    raise click.BadParameter(\'must be same dimensionality as 3rd dimension of {0}\'.format(variable),\r\n                                             param=\'--id_variable\', param_hint=\'--id_variable\')\r\n            else:\r\n                # Guess from the 3rd dimension\r\n                guess = dimensions[0]\r\n                if guess in ds.variables and ds.variables[guess][:].shape[0] == shape[0]:\r\n                    id_variable = guess\r\n\r\n        ds_crs = get_crs(ds, variable)\r\n        if not ds_crs and is_geographic(ds, variable):\r\n            ds_crs = \'EPSG:4326\'  # Assume all geographic data is WGS84\r\n\r\n        src_crs = CRS.from_string(ds_crs) if ds_crs else CRS({\'init\': src_crs}) if src_crs else None\r\n\r\n        # get transforms, assume last 2 dimensions on variable are spatial in row, col order\r\n        y_dim, x_dim = dimensions[-2:]\r\n        coords = SpatialCoordinateVariables.from_dataset(\r\n            ds, x_dim, y_dim, projection=Proj(src_crs.to_dict()) if src_crs else None\r\n        )\r\n\r\n        if mask is not None and not mask.shape == shape[-2:]:\r\n            # Will likely break before this if collecting statistics\r\n            raise click.BadParameter(\r\n                \'mask variable shape does not match shape of input spatial dimensions\',\r\n                param=\'--mask\', param_hint=\'--mask\'\r\n            )\r\n\r\n        flip_y = False\r\n        reproject_kwargs = None\r\n        if dst_crs is not None:\r\n            if not src_crs:\r\n                raise click.BadParameter(\'must provide src_crs to reproject\',\r\n                                         param=\'--src-crs\',\r\n                                         param_hint=\'--src-crs\')\r\n\r\n            dst_crs = CRS.from_string(dst_crs)\r\n\r\n            src_height, src_width = coords.shape\r\n            dst_transform, dst_width, dst_height = calculate_default_transform(\r\n                src_crs, dst_crs, src_width, src_height,\r\n                *coords.bbox.as_list(), resolution=res\r\n            )\r\n\r\n            reproject_kwargs = {\r\n                \'src_crs\': src_crs,\r\n                \'src_transform\': coords.affine,\r\n                \'dst_crs\': dst_crs,\r\n                \'dst_transform\': dst_transform,\r\n                \'resampling\': getattr(Resampling, resampling),\r\n                \'dst_shape\': (dst_height, dst_width)\r\n            }\r\n\r\n        else:\r\n            dst_transform = coords.affine\r\n            dst_height, dst_width = coords.shape\r\n            dst_crs = src_crs\r\n\r\n            if coords.y.is_ascending_order():\r\n                # Only needed if we are not already reprojecting the data, since that will flip it automatically\r\n                flip_y = True\r\n\r\n        if anchors or interactive_map:\r\n            if not (dst_crs or src_crs):\r\n                raise click.BadParameter(\'must provide at least src_crs to get Leaflet anchors or interactive map\',\r\n                                         param=\'--src-crs\', param_hint=\'--src-crs\')\r\n\r\n            leaflet_anchors = get_leaflet_anchors(BBox.from_affine(dst_transform, dst_width, dst_height,\r\n                                                           projection=Proj(dst_crs) if dst_crs else None))\r\n\r\n            if anchors:\r\n                click.echo(\'Anchors: {0}\'.format(leaflet_anchors))\r\n\r\n\r\n    layers = {}\r\n    for filename in filenames:\r\n        with Dataset(filename) as ds:\r\n            click.echo(\'Processing {0}\'.format(filename))\r\n\r\n            filename_root = os.path.split(filename)[1].replace(\'.nc\', \'\')\r\n\r\n            if not variable in ds.variables:\r\n                raise click.BadParameter(\'variable {0} was not found in file: {1}\'.format(variable, filename),\r\n                                         param=\'variable\', param_hint=\'VARIABLE\')\r\n\r\n            var_obj = ds.variables[variable]\r\n            if not var_obj.dimensions == dimensions:\r\n                raise click.ClickException(\'All datasets must have the same dimensions for {0}\'.format(variable))\r\n\r\n            if num_dimensions == 2:\r\n                data = var_obj[:]\r\n                if mask is not None:\r\n                    data = numpy.ma.masked_array(data, mask=mask)\r\n                image_filename = os.path.join(output_directory, \'{0}_{1}.{2}\'.format(filename_root, variable, format))\r\n                if reproject_kwargs:\r\n                    data = warp_array(data, **reproject_kwargs)\r\n                render_image(renderer, data, image_filename, scale, flip_y=flip_y, format=format)\r\n\r\n                local_filename = os.path.split(image_filename)[1]\r\n                layers[os.path.splitext(local_filename)[0]] = local_filename\r\n\r\n            elif num_dimensions == 3:\r\n                for index in range(shape[0]):\r\n                    id = ds.variables[id_variable][index] if id_variable is not None else index\r\n                    image_filename = os.path.join(output_directory, \'{0}_{1}__{2}.{3}\'.format(filename_root, variable, id, format))\r\n                    data = var_obj[index]\r\n                    if mask is not None:\r\n                        data = numpy.ma.masked_array(data, mask=mask)\r\n                    if reproject_kwargs:\r\n                        data = warp_array(data, **reproject_kwargs)\r\n                    render_image(renderer, data, image_filename, scale, flip_y=flip_y, format=format)\r\n\r\n                    local_filename = os.path.split(image_filename)[1]\r\n                    layers[os.path.splitext(local_filename)[0]] = local_filename\r\n\r\n\r\n\r\n            # TODO: not tested recently.  Make sure still correct\r\n            # else:\r\n            #     # Assume last 2 components of shape are lat & lon, rest are iterated over\r\n            #     id_variables = None\r\n            #     if id_variable is not None:\r\n            #         id_variables = id_variable.split(\',\')\r\n            #         for index, name in enumerate(id_variables):\r\n            #             if name:\r\n            #                 assert data.shape[index] == ds.variables[name][:].shape[0]\r\n            #\r\n            #     ranges = []\r\n            #     for dim in data.shape[:-2]:\r\n            #         ranges.append(range(0, dim))\r\n            #     for combined_index in product(*ranges):\r\n            #         id_parts = []\r\n            #         for index, dim_index in enumerate(combined_index):\r\n            #             if id_variables is not None and index < len(id_variables) and id_variables[index]:\r\n            #                 id = ds.variables[id_variables[index]][dim_index]\r\n            #\r\n            #                 if not isinstance(id, basestring):\r\n            #                     if isinstance(id, Iterable):\r\n            #                         id = \'_\'.join((str(i) for i in id))\r\n            #                     else:\r\n            #                         id = str(id)\r\n            #\r\n            #                 id_parts.append(id)\r\n            #\r\n            #             else:\r\n            #                 id_parts.append(str(dim_index))\r\n            #\r\n            #         combined_id = \'_\'.join(id_parts)\r\n            #         image_filename = os.path.join(output_directory, \'{0}__{1}.{2}\'.format(filename_root, combined_id, format))\r\n            #         if reproject_kwargs:\r\n            #             data = warp_array(data, **reproject_kwargs)  # NOTE: lack of index will break this\r\n            #         render_image(renderer, data[combined_index], image_filename, scale, flip_y=flip_y, format=format)\r\n            #\r\n            #         local_filename = os.path.split(image_filename)[1]\r\n            #         layers[os.path.splitext(local_filename)[0]] = local_filename\r\n\r\n\r\n    if interactive_map:\r\n        index_html = os.path.join(output_directory, \'index.html\')\r\n        with open(index_html, \'w\') as out:\r\n            template = Environment(loader=PackageLoader(\'trefoil.cli\')).get_template(\'map.html\')\r\n            out.write(\r\n                template.render(\r\n                    layers=json.dumps(layers),\r\n                    bounds=str(leaflet_anchors),\r\n                    variable=variable\r\n                )\r\n            )\r\n\r\n        webbrowser.open(index_html)\r\n'"
trefoil/cli/render_tif.py,0,"b'""""""\nRender a set of GeoTIFF files to images.\n\nStretched renderers may have one of the following colormap values:\n1.0 (absolute)\nmax (calculate max across datasets)\n0.5*max (calculate max across datasets, and multiply by value)\n""""""\n\n\nimport importlib\nimport os\nimport glob\nimport click\nimport json\nimport numpy\nfrom PIL.Image import ANTIALIAS, NEAREST\nfrom pyproj import Proj\n\nimport rasterio\nfrom rasterio.warp import reproject, calculate_default_transform\nfrom rasterio.enums import Resampling\n\nfrom trefoil.utilities.color import Color\nfrom trefoil.render.renderers.stretched import StretchedRenderer\nfrom trefoil.render.renderers.unique import UniqueValuesRenderer\nfrom trefoil.render.renderers.utilities import renderer_from_dict\nfrom trefoil.netcdf.utilities import collect_statistics\nfrom trefoil.geometry.bbox import BBox\nfrom trefoil.cli import cli\n\n\ndef _colormap_to_stretched_renderer(colormap, colorspace=\'hsv\', filenames=None, variable=None):\n    statistics = None\n    if \'min:\' in colormap or \'max:\' in colormap or \'mean\' in colormap:\n        if not filenames and variable:\n            raise ValueError(\'filenames and variable are required inputs to use colormap with statistics\')\n        statistics = collect_statistics(filenames, (variable,))[variable]\n\n        for value in (\'min\', \'max\', \'mean\'):\n            colormap = colormap.replace(value, statistics[value])\n\n    return StretchedRenderer(_parse_colormap(colormap), colorspace=colorspace)\n\n\ndef _parse_colormap(colormap_str):\n    colormap = []\n    for entry in colormap_str.split(\',\'):\n        value, color = entry.split(\':\')\n        colormap.append((float(value), Color.from_hex(color)))\n    return colormap\n\n\ndef _palette_to_stretched_renderer(palette_path, values, filenames=None, variable=None):\n    index = palette_path.rindex(\'.\')\n    palette = getattr(importlib.import_module(\'palettable.\' + palette_path[:index]), palette_path[index+1:])\n\n    values = values.split(\',\')\n    if not len(values) > 1:\n        raise ValueError(\'Must provide at least 2 values for palette-based stretched renderer\')\n\n    statistics = None\n    if \'min\' in values or \'max\' in values:\n        if not filenames and variable:\n            raise ValueError(\'filenames and variable are required inputs to use palette with statistics\')\n        statistics = collect_statistics(filenames, (variable,))[variable]\n\n        for statistic in (\'min\', \'max\'):\n            if statistic in values:\n                values[values.index(statistic)] = statistics[statistic]\n\n    hex_colors = palette.hex_colors\n\n    # TODO: this only works cleanly for min:max or 2 endpoint values.  Otherwise require that the number of palette colors match the number of values\n\n    colors = [(values[0], Color.from_hex(hex_colors[0]))]\n\n    intermediate_colors = hex_colors[1:-1]\n    if intermediate_colors:\n        interval = (values[-1] - values[0]) / (len(intermediate_colors) + 1)\n        for i, color in enumerate(intermediate_colors):\n            colors.append((values[0] + (i + 1) * interval, Color.from_hex(color)))\n\n    colors.append((values[-1], Color.from_hex(hex_colors[-1])))\n\n    return StretchedRenderer(colors, colorspace=\'rgb\')  # I think all palettable palettes are in RGB ramps\n\n\ndef render_image(renderer, data, filename, scale=1, reproject_kwargs=None):\n    if reproject_kwargs is not None:\n\n        with rasterio.Env():\n            out = numpy.empty(shape=reproject_kwargs[\'dst_shape\'], dtype=data.dtype)\n            out.fill(data.fill_value)\n            reproject(data, out, **reproject_kwargs)\n            # Reapply mask\n            data = numpy.ma.masked_array(out, mask=out == data.fill_value)\n\n\n    resampling = ANTIALIAS\n    if renderer.name == \'unique\':\n        resampling = NEAREST\n\n    img = renderer.render_image(data)\n    if scale != 1:\n        img = img.resize((numpy.array(data.shape[::-1]) * scale).astype(numpy.uint), resampling)\n    img.save(filename)\n\n\n@cli.command(short_help=""Render Single-Band GeoTIFF files to images"")\n@click.argument(\'filename_pattern\')\n@click.argument(\'output_directory\', type=click.Path())\n@click.option(\'--renderer_file\', help=\'File containing renderer JSON\', type=click.Path())\n@click.option(\'--save\', default=False, is_flag=True, help=\'Save renderer to renderer_file\')\n@click.option(\'--renderer_type\', default=\'stretched\', type=click.Choice([\'stretched\', \'unique\']), help=\'Name of renderer [default: stretched].  (other types not yet implemented)\')\n@click.option(\'--colormap\', default=\'min:#000000,max:#FFFFFF\', help=\'Provide colormap as comma-separated lookup of value to hex color code.  (Example: -1:#FF0000,1:#0000FF) [default: min:#000000,max:#FFFFFF]\')\n@click.option(\'--colorspace\', default=\'hsv\', type=click.Choice([\'hsv\', \'rgb\']), help=\'Color interpolation colorspace\')\n@click.option(\'--palette\', default=None, help=\'Palettable color palette (Example: colorbrewer.sequential.Blues_3)\')\n@click.option(\'--scale\', default=1.0, help=\'Scale factor for data pixel to screen pixel size\')\n@click.option(\'--id_variable\', help=\'ID variable used to provide IDs during image generation.  Must be of same dimensionality as first dimension of variable (example: time)\')\n@click.option(\'--lh\', default=150, help=\'Height of the legend in pixels [default: 150]\')\n@click.option(\'--legend_breaks\', default=None, type=click.INT, help=\'Number of breaks to show on legend for stretched renderer\')\n@click.option(\'--legend_ticks\', default=None, type=click.STRING, help=\'Legend tick values for stretched renderer\')\n# Projection related options\n@click.option(\'--src_crs\', default=None, type=click.STRING, help=\'Source coordinate reference system (limited to EPSG codes, e.g., EPSG:4326).  Will be read from file if not provided.\')\n@click.option(\'--dst_crs\', default=None, type=click.STRING, help=\'Destination coordinate reference system\')\n@click.option(\'--res\', default=None, type=click.FLOAT, help=\'Destination pixel resolution in destination coordinate system units\' )\n@click.option(\'--resampling\', default=\'nearest\', type=click.Choice((\'nearest\', \'cubic\', \'lanczos\', \'mode\')), help=\'Resampling method for reprojection (default: nearest\')\n@click.option(\'--anchors\', default=False, is_flag=True, help=\'Print anchor coordinates for use in Leaflet ImageOverlay\')\n# TODO: option with transform info if not a geo format\ndef render_tif(\n        filename_pattern,\n        output_directory,\n        renderer_file,\n        save,\n        renderer_type,\n        colormap,\n        colorspace,\n        palette,\n        scale,\n        id_variable,\n        lh,\n        legend_breaks,\n        legend_ticks,\n        src_crs,\n        dst_crs,\n        res,\n        resampling,\n        anchors):\n    """"""\n    Render single-band GeoTIFF files to images.\n\n    colormap is ignored if renderer_file is provided\n    """"""\n\n    filenames = glob.glob(filename_pattern)\n    if not filenames:\n        raise click.BadParameter(\'No files found matching that pattern\', param=\'filename_pattern\', param_hint=\'FILENAME_PATTERN\')\n\n    if not os.path.exists(output_directory):\n        os.makedirs(output_directory)\n\n    if renderer_file is not None and not save:\n        if not os.path.exists(renderer_file):\n            raise click.BadParameter(\'does not exist\', param=\'renderer_file\', param_hint=\'renderer_file\')\n\n        # see https://bitbucket.org/databasin/ncdjango/wiki/Home for format\n        renderer_dict = json.loads(open(renderer_file).read())\n\n        # if renderer_dict[\'type\'] == \'stretched\':\n        #     colors = \',\'.join([str(c[0]) for c in renderer_dict[\'colors\']])\n        #     if \'min\' in colors or \'max\' in colors or \'mean\' in colors:\n        #         statistics = collect_statistics(filenames, (variable,))[variable]\n        #         for entry in renderer_dict[\'colors\']:\n        #             if isinstance(entry[0], basestring):\n        #                 if entry[0] in (\'min\', \'max\', \'mean\'):\n        #                     entry[0] = statistics[entry[0]]\n        #                 elif \'*\' in entry[0]:\n        #                     rel_value, statistic = entry[0].split(\'*\')\n        #                     entry[0] = float(rel_value) * statistics[statistic]\n\n        renderer = renderer_from_dict(renderer_dict)\n\n    else:\n\n        if renderer_type == \'stretched\':\n            # if palette is not None:\n            #     renderer = _palette_to_stretched_renderer(palette, \'min,max\', filenames, variable)\n            #\n            # else:\n            renderer = _colormap_to_stretched_renderer(colormap, colorspace, filenames)\n        elif renderer_type == \'unique\':\n            renderer = UniqueValuesRenderer(_parse_colormap(colormap), colorspace)\n\n        else:\n            raise NotImplementedError(\'other renderers not yet built\')\n\n    # if save:\n    #     if not renderer_file:\n    #         raise click.BadParameter(\'must be provided to save\', param=\'renderer_file\', param_hint=\'renderer_file\')\n    #\n    #     if os.path.exists(renderer_file):\n    #         with open(renderer_file, \'r+\') as output_file:\n    #             data = json.loads(output_file.read())\n    #             output_file.seek(0)\n    #             output_file.truncate()\n    #             data[variable] = renderer.serialize()\n    #             output_file.write(json.dumps(data, indent=4))\n    #     else:\n    #         with open(renderer_file, \'w\') as output_file:\n    #             output_file.write(json.dumps({variable: renderer.serialize()}))\n\n\n    if renderer_type == \'streteched\':\n        if legend_ticks is not None and not legend_breaks:\n            legend_ticks = [float(v) for v in legend_ticks.split(\',\')]\n\n        legend = renderer.get_legend(image_height=lh, breaks=legend_breaks, ticks=legend_ticks, max_precision=2)[0].to_image()\n\n    elif renderer_type == \'unique\':\n        legend = renderer.get_legend(image_height=lh)[0].to_image()\n\n    legend.save(os.path.join(output_directory, \'legend.png\'))\n\n\n    for filename in filenames:\n        with rasterio.open(filename) as ds:\n            print(\'Processing\',filename)\n            filename_root = os.path.split(filename)[1].replace(\'.nc\', \'\')\n\n            data = ds.read(1, masked=True)\n\n            # # get transforms, assume last 2 dimensions on variable are spatial in row, col order\n            # y_dim, x_dim = ds.variables[variable].dimensions[-2:]\n            # y_len, x_len = data.shape[-2:]\n            # coords = SpatialCoordinateVariables.from_dataset(ds, x_dim, y_dim)#, projection=Proj(src_crs))\n            #\n            # if coords.y.is_ascending_order():\n            #     data = data[::-1]\n            #\n            reproject_kwargs = None\n            if dst_crs is not None:\n                # TODO: extract this out into a general trefoil reprojection function\n                ds_crs = ds.crs\n                if not (src_crs or ds_crs):\n                    raise click.BadParameter(\'must provide src_crs to reproject\', param=\'src_crs\', param_hint=\'src_crs\')\n\n                dst_crs = {\'init\': dst_crs}\n                src_crs = ds_crs if ds_crs else {\'init\': src_crs}\n\n                left, bottom, top, right = ds.bounds\n                dst_affine, dst_width, dst_height = calculate_default_transform(left, bottom, right, top, ds.width, ds.height, src_crs, dst_crs)\n                dst_shape = (dst_height, dst_width)\n\n\n                # proj_bbox = coords.bbox.project(Proj(dst_crs))\n                #\n                # x_dif = proj_bbox.xmax - proj_bbox.xmin\n                # y_dif = proj_bbox.ymax - proj_bbox.ymin\n                #\n                # total_len = float(x_len + y_len)\n                # # Cellsize is dimension weighted average of x and y dimensions per projected pixel, unless otherwise provided\n                # avg_cellsize = ((x_dif / float(x_len)) * (float(x_len) / total_len)) + ((y_dif / float(y_len)) * (float(y_len) / total_len))\n                #\n                # cellsize = res or avg_cellsize\n                # dst_affine = Affine(cellsize, 0, proj_bbox.xmin, 0, -cellsize, proj_bbox.ymax)\n                # dst_shape = (\n                #     max(int(ceil((y_dif) / cellsize)), 1),  # height\n                #     max(int(ceil(x_dif / cellsize)), 1)  # width\n                # )\n\n                # TODO: replace with method in rasterio\n                reproject_kwargs = {\n                    \'src_crs\': src_crs,\n                    \'src_transform\': ds.affine,\n                    \'dst_crs\': dst_crs,\n                    \'dst_transform\': dst_affine,\n                    \'resampling\': getattr(Resampling, resampling),\n                    \'dst_shape\': dst_shape\n                }\n\n                if anchors:\n                    # Reproject the bbox of the output to WGS84\n                    full_bbox = BBox((dst_affine.c, dst_affine.f + dst_affine.e * dst_shape[0],\n                                     dst_affine.c + dst_affine.a * dst_shape[1], dst_affine.f),\n                                     projection=Proj(dst_crs))\n                    wgs84_bbox = full_bbox.project(Proj(init=\'EPSG:4326\'))\n                    print(\'WGS84 Anchors: {0}\'.format([[wgs84_bbox.ymin, wgs84_bbox.xmin], [wgs84_bbox.ymax, wgs84_bbox.xmax]]))\n\n            elif anchors:\n                # Reproject the bbox of the output to WGS84\n                    full_bbox = BBox(ds.bounds, projection=Proj(ds.crs))\n                    wgs84_bbox = full_bbox.project(Proj(init=\'EPSG:4326\'))\n                    print(\'WGS84 Anchors: {0}\'.format([[wgs84_bbox.ymin, wgs84_bbox.xmin], [wgs84_bbox.ymax, wgs84_bbox.xmax]]))\n\n            image_filename = os.path.join(output_directory,\n                                          \'{0}.png\'.format(filename_root))\n            render_image(renderer, data, image_filename, scale, reproject_kwargs=reproject_kwargs)\n'"
trefoil/cli/utilities.py,0,"b'import importlib\n\nimport click\nimport os\nimport numpy\nfrom PIL.Image import ANTIALIAS\nfrom pyproj import Proj\nfrom netCDF4 import Dataset\n\nfrom trefoil.utilities.color import Color\nfrom trefoil.netcdf.utilities import collect_statistics, resolve_dataset_variable\nfrom trefoil.render.renderers.stretched import StretchedRenderer\nfrom trefoil.render.renderers.classified import ClassifiedRenderer\n\n\ndef render_image(renderer, data, filename, scale=1, flip_y=False, format=\'png\'):\n    if flip_y:\n        data = data[::-1]\n\n    img = renderer.render_image(data)\n    if scale != 1:\n        img = img.resize((numpy.array(data.shape[::-1]) * scale).astype(numpy.uint), ANTIALIAS)\n\n    kwargs = {}\n    if format == \'png\':\n        kwargs[\'optimize\'] = True\n    elif format == \'jpg\':\n        img = img.convert(\'RGB\')\n        kwargs[\'progressive\'] = True\n    elif format == \'webp\':\n        img = img.convert(\'RGBA\')\n        kwargs[\'lossless\'] = True\n\n    img.save(filename, **kwargs)\n\n\ndef colormap_to_stretched_renderer(colormap, colorspace=\'hsv\', filenames=None, variable=None, fill_value=None, mask=None):\n    statistics = None\n    if \'min:\' in colormap or \'max:\' in colormap or \'mean\' in colormap:\n        if not filenames and variable:\n            raise ValueError(\'filenames and variable are required inputs to use colormap with statistics\')\n        statistics = collect_statistics(filenames, (variable,), mask=mask)[variable]\n\n    colors = []\n    for entry in colormap.split(\',\'):\n        value, color = entry.split(\':\')\n        # TODO: add proportions of statistics\n        if value in (\'min\', \'max\', \'mean\'):\n            value = statistics[value]\n        else:\n            value = float(value)\n        colors.append((value, Color.from_hex(color)))\n\n    return StretchedRenderer(colors, colorspace=colorspace, fill_value=fill_value)\n\n\ndef get_palette(palette_path):\n    index = palette_path.rindex(\'.\')\n    return getattr(importlib.import_module(\'palettable.\' + palette_path[:index]), palette_path[index+1:])\n\n\ndef palette_to_stretched_renderer(palette_path, values, filenames=None, variable=None, fill_value=None, mask=None):\n    palette = get_palette(palette_path)\n\n    values = values.split(\',\')\n    if not len(values) > 1:\n        raise ValueError(\'Must provide at least 2 values for palette-based stretched renderer\')\n\n    if \'min\' in values or \'max\' in values:\n        if not filenames and variable:\n            raise ValueError(\'filenames and variable are required inputs to use palette with statistics\')\n        statistics = collect_statistics(filenames, (variable,), mask=mask)[variable]\n\n        for statistic in (\'min\', \'max\'):\n            if statistic in values:\n                values[values.index(statistic)] = statistics[statistic]\n\n    values = [float(v) for v in values]  # in case any are still strings\n\n    hex_colors = palette.hex_colors\n\n    # TODO: this only works cleanly for min:max or 2 endpoint values.  Otherwise require that the number of palette colors match the number of values\n\n    colors = [(values[0], Color.from_hex(hex_colors[0]))]\n\n    intermediate_colors = hex_colors[1:-1]\n    if intermediate_colors:\n        interval = (values[-1] - values[0]) / (len(intermediate_colors) + 1)\n        for i, color in enumerate(intermediate_colors):\n            colors.append((values[0] + (i + 1) * interval, Color.from_hex(color)))\n\n    colors.append((values[-1], Color.from_hex(hex_colors[-1])))\n\n    return StretchedRenderer(colors, colorspace=\'rgb\', fill_value=fill_value)  # I think all palettable palettes are in RGB ramps\n\n\ndef palette_to_classified_renderer(palette_path, filenames, variable, method=\'equal\', fill_value=None, mask=None):\n    palette = get_palette(palette_path)\n    num_breaks = palette.number\n    colors = [Color(r, g, b) for (r, g, b) in palette.colors]\n\n    if method == \'equal\':\n        statistics = collect_statistics(filenames, (variable,), mask=mask)[variable]\n        step = (statistics[\'max\'] - statistics[\'min\']) / num_breaks\n        breaks = numpy.linspace(statistics[\'min\'] + step, statistics[\'max\'], num_breaks)\n\n    return ClassifiedRenderer(zip(breaks, colors), fill_value=fill_value)\n\n\ndef get_leaflet_anchors(bbox):\n    """"""\n    Returns Leaflet anchor coordinates for creating an ImageOverlay layer.\n    """"""\n\n    wgs84_bbox = bbox.project(Proj(init=\'EPSG:4326\'))\n    return [[wgs84_bbox.ymin, wgs84_bbox.xmin], [wgs84_bbox.ymax, wgs84_bbox.xmax]]\n\ndef get_mask(mask_path):\n    """"""\n    Returns a numpy style mask from a netCDF file.\n\n    Parameters\n    ----------\n    mask_path: string, a compound path of dataset:variable\n\n    Returns\n    -------\n    boolean mask  (True where mask will be applied)\n    """"""\n\n    mask_path, mask_variable = resolve_dataset_variable(mask_path)\n    if not mask_variable:\n        mask_variable = \'mask\'\n\n    with Dataset(mask_path) as mask_ds:\n        if not mask_variable in mask_ds.variables:\n            raise click.BadParameter(\n                \'mask variable not found: {0}\'.format(mask_variable),\n                 param=\'--mask\', param_hint=\'--mask\'\n            )\n\n        return mask_ds.variables[mask_variable][:].astype(\'bool\')'"
trefoil/cli/warp.py,0,"b'import os\nimport glob\nfrom netCDF4 import Dataset\n\nimport click\nfrom rasterio.enums import Resampling\n\nfrom trefoil.cli import cli\nfrom trefoil.netcdf.warp import warp_like\nfrom trefoil.netcdf.crs import get_crs\nfrom trefoil.netcdf.utilities import data_variables\n\n\n@cli.command(short_help=""Warp NetCDF files to match a template"")\n@click.argument(\'filename_pattern\')\n@click.argument(\'output_directory\', type=click.Path())\n@click.option(\'--variables\', help=\'comma-delimited list of variables to warp.  Default: all data variables\', default=None)\n@click.option(\'--src-crs\', help=\'Source Coordinate Reference System (only used if none found in source dataset)\',\n              default=\'EPSG:4326\', show_default=True)\n@click.option(\'--like\', help=\'Template dataset\', type=click.Path(exists=True), required=True)  # Required for now\n@click.option(\'--resampling\', default=\'nearest\',\n              type=click.Choice((\'nearest\', \'cubic\', \'lanczos\', \'mode\')),\n              help=\'Resampling method for reprojection\', show_default=True)\ndef warp(\n    filename_pattern,\n    output_directory,\n    variables,\n    src_crs,\n    like,\n    resampling):\n\n    if variables:\n        variables = variables.strip().split(\',\')\n\n    filenames = glob.glob(filename_pattern)\n    if not filenames:\n        raise click.BadParameter(\'No files found matching that pattern\', param=\'filename_pattern\', param_hint=\'FILENAME_PATTERN\')\n\n    if not os.path.exists(output_directory):\n        os.makedirs(output_directory)\n\n    # For now, template dataset is required\n    template_ds = Dataset(like)\n    template_varname = data_variables(template_ds).keys()[0]\n\n    for filename in filenames:\n        with Dataset(filename) as ds:\n            if not variables:\n                ds_variables = data_variables(ds).keys()\n            else:\n                # filter to only variables present in this dataset\n                ds_variables = [v for v in variables if v in ds.variables]\n\n            ds_crs = get_crs(ds, ds_variables[0]) or src_crs\n\n            with Dataset(os.path.join(output_directory, os.path.split(filename)[1]), \'w\') as out_ds:\n                click.echo(\'Processing: {0}\'.format(filename))\n\n                warp_like(\n                    ds,\n                    ds_projection=ds_crs,\n                    variables=ds_variables,\n                    out_ds=out_ds,\n                    template_ds=template_ds,\n                    template_varname=template_varname,\n                    resampling=getattr(Resampling, resampling)\n                )\n'"
trefoil/cli/zones.py,0,"b'import csv\nimport glob\nimport os\nimport time\nimport json\n\nimport click\nfrom pyproj import Proj\nfrom netCDF4 import Dataset\nimport numpy\nimport fiona\nimport rasterio\nfrom rasterio.crs import CRS\nfrom rasterio.features import rasterize\nfrom rasterio.warp import transform_geom\nfrom rasterio.rio.options import file_in_arg, file_out_arg\n\nfrom trefoil.cli import cli\nfrom trefoil.analysis.summary import VALID_ZONAL_STATISTICS, calculate_zonal_statistics\nfrom trefoil.netcdf.variable import SpatialCoordinateVariables\nfrom trefoil.netcdf.crs import get_crs, is_geographic\nfrom trefoil.netcdf.utilities import data_variables, get_fill_value\n\n\n@cli.command(short_help=\'Create zones in a NetCDF from features in a shapefile\')\n@click.argument(\'input\', type=click.Path(exists=True))\n@file_out_arg\n@click.option(\'--variable\', type=click.STRING, default=\'zone\', help=\'Name of output zones variable\', show_default=True)\n@click.option(\'--attribute\', type=click.STRING, default=None, help=\'Name of attribute in shapefile to use for zones (default: feature ID)\')\n@click.option(\'--like\', help=\'Template NetCDF dataset\', type=click.Path(exists=True), required=True)\n@click.option(\'--netcdf3\', is_flag=True, default=False, help=\'Output in NetCDF3 version instead of NetCDF4\')\n@click.option(\'--zip\', is_flag=True, default=False, help=\'Use zlib compression of data and coordinate variables\')\ndef zones(\n    input,\n    output,\n    variable,\n    attribute,\n    like,\n    netcdf3,\n    zip):\n\n    """"""\n    Create zones in a NetCDF from features in a shapefile.  This is intended\n    to be used as input to zonal statistics functions; it is not intended\n    as a direct replacement for rasterizing geometries into NetCDF.\n\n    Only handles < 65,535 features for now.\n\n    If --attribute is provided, any features that do not have this will not be\n    assigned to zones.\n\n    A values lookup will be used to store values.  The zones are indices of\n    the unique values encountered when extracting features.\n    The original values are stored in an additional variable with the name of\n    the zones variable plus \'_values\'.\n\n    Template NetCDF dataset must have a valid projection defined or be inferred\n    from dimensions (e.g., lat / long).\n    """"""\n\n    with Dataset(like) as template_ds:\n        template_varname = list(data_variables(template_ds).keys())[0]\n        template_variable = template_ds.variables[template_varname]\n        template_crs = get_crs(template_ds, template_varname)\n\n        if template_crs:\n            template_crs = CRS.from_string(template_crs)\n        elif is_geographic(template_ds, template_varname):\n            template_crs = CRS({\'init\': \'EPSG:4326\'})\n        else:\n            raise click.UsageError(\'template dataset must have a valid projection defined\')\n\n        spatial_dimensions = template_variable.dimensions[-2:]\n        out_shape = template_variable.shape[-2:]\n\n        template_y_name, template_x_name = spatial_dimensions\n        coords = SpatialCoordinateVariables.from_dataset(\n            template_ds,\n            x_name=template_x_name,\n            y_name=template_y_name,\n            projection=Proj(**template_crs.to_dict())\n        )\n\n\n    with fiona.open(input, \'r\') as shp:\n        if attribute:\n            if not attribute in shp.meta[\'schema\'][\'properties\']:\n                raise click.BadParameter(\'{0} not found in dataset\'.format(attribute),\n                                         param=\'--attribute\', param_hint=\'--attribute\')\n\n            att_dtype = shp.meta[\'schema\'][\'properties\'][attribute].split(\':\')[0]\n            if not att_dtype in (\'int\', \'str\'):\n                raise click.BadParameter(\'integer or string attribute required\'.format(attribute),\n                                         param=\'--attribute\', param_hint=\'--attribute\')\n\n        transform_required = CRS(shp.crs) != template_crs\n        geometries = []\n        values = set()\n        values_lookup = {}\n\n        # Project bbox for filtering\n        bbox = coords.bbox\n        if transform_required:\n            bbox = bbox.project(Proj(**shp.crs), edge_points=21)\n\n        index = 0\n        for f in shp.filter(bbox=bbox.as_list()):\n            value = f[\'properties\'].get(attribute) if attribute else int(f[\'id\'])\n            if value is not None:\n                geom = f[\'geometry\']\n                if transform_required:\n                    geom = transform_geom(shp.crs, template_crs, geom)\n\n                geometries.append((geom, index))\n\n                if not value in values:\n                    values.add(value)\n                    values_lookup[index] = value\n                    index += 1\n\n            # Otherwise, these will not be rasterized\n\n        num_geometries = len(geometries)\n        # Save a slot at the end for nodata\n        if num_geometries < 255:\n            dtype = numpy.dtype(\'uint8\')\n        elif num_geometries < 65535:\n            dtype = numpy.dtype(\'uint16\')\n        else:\n            raise click.UsageError(\'Too many features to rasterize: {0}, Exceptioning...\'.format(num_geometries))\n\n        fill_value = get_fill_value(dtype)\n\n        click.echo(\'Rasterizing {0} features into zones\'.format(num_geometries))\n\n    with rasterio.Env():\n        zones = rasterize(\n            geometries,\n            out_shape=out_shape,\n            transform=coords.affine,\n            all_touched=False, # True produces undesirable results for adjacent polygons\n            fill=fill_value,\n            dtype=dtype\n        )\n\n    format = \'NETCDF4\'\n    out_dtype = dtype\n    if netcdf3:\n        format = \'NETCDF3_CLASSIC\'\n        if dtype == numpy.uint8:\n            out_dtype = numpy.dtype(\'int16\')\n        elif dtype == numpy.uint16:\n            out_dtype = numpy.dtype(\'int32\')\n\n        # Have to convert fill_value to mask since we changed data type\n        zones = numpy.ma.masked_array(zones, mask=(zones == fill_value))\n\n\n    with Dataset(output, \'w\', format=format) as out:\n        values_varname = \'{0}_values\'.format(variable)\n        coords.add_to_dataset(out, template_x_name, template_y_name)\n        out_var = out.createVariable(variable, out_dtype,\n                                     dimensions=spatial_dimensions,\n                                     zlib=zip,\n                                     fill_value=get_fill_value(out_dtype))\n        out_var.setncattr(\'values\', values_varname)\n        out_var[:] = zones\n\n        out_values = numpy.array([values_lookup[k] for k in range(0, len(values_lookup))])\n        if netcdf3 and out_values.dtype == numpy.int64:\n            out_values = out_values.astype(\'int32\')\n\n        out.createDimension(values_varname, len(out_values))\n        values_var = out.createVariable(values_varname, out_values.dtype,\n                                        dimensions=(values_varname, ),\n                                        zlib=zip)\n        values_var[:] = out_values\n\n\n@cli.command(short_help=\'Calculate zonal statistics for a series of NetCDF files\')\n@click.argument(\'zones\', type=click.Path(exists=True))\n@click.argument(\'filename_pattern\')\n@click.argument(\'output\', type=click.Path())\n@click.option(\'--variables\',  type=click.STRING, default=None, help=\'Comma-separated list of variables (if not provided, will use all data variables)\')\n@click.option(\'--statistics\', type=click.STRING, default=\'mean\', help=\'Comma-separated list of statistics (available: mean,min,max,std,sum,count)\', show_default=True)\n# TODO: consider using shorthand notation for zones:zone_variable instead\n@click.option(\'--zone_variable\', type=click.STRING, default=\'zone\', help=\'Name of output zones variable\', show_default=True)\n# TODO: precision\ndef zonal_stats(\n    zones,\n    filename_pattern,\n    output,\n    variables,\n    statistics,\n    zone_variable):\n\n    """"""\n    Calculate zonal statistics for a series of NetCDF files.\n\n    Zones must be created using the \'zones\' command.\n\n    The output file can either be a CSV (recommended) or JSON format file, which\n    is automatically determined from the file extension of the output filename.\n\n    See docs/cli.md for more information about output format.\n    """"""\n\n    start = time.time()\n\n    if variables:\n        variables = variables.split(\',\')\n\n    statistics = statistics.split(\',\')\n    if set(statistics).difference(VALID_ZONAL_STATISTICS):\n        raise click.BadParameter(\n            \'One or more statistics is not supported {0}\'.format(statistics),\n             param=\'--statistics\', param_hint=\'--statistics\'\n        )\n\n    filenames = glob.glob(filename_pattern)\n    if not filenames:\n        raise click.BadParameter(\n            \'No files found matching that pattern\',\n             param=\'filename_pattern\', param_hint=\'FILENAME_PATTERN\'\n        )\n\n    with Dataset(zones) as zones_ds:\n        if not zone_variable in zones_ds.variables:\n            raise click.BadParameter(\n                \'zone variable not found: {0}\'.format(zone_variable),\n                 param=\'--zone_variable\', param_hint=\'--zone_variable\'\n            )\n\n        values_variable = \'{0}_values\'.format(zone_variable)\n        if not values_variable in zones_ds.variables:\n            raise click.BadParameter(\n                \'zone values variable not found: {0}_values\'.format(zone_variable),\n                 param=\'--zone_variable\', param_hint=\'--zone_variable\'\n            )\n\n        zones = zones_ds.variables[zone_variable][:]\n        zone_values = zones_ds.variables[values_variable][:]\n\n    with Dataset(filenames[0]) as ds:\n        if variables is not None:\n            if set(variables).difference(ds.variables.keys()):\n                raise click.BadParameter(\n                    \'One or more variables were not found in {0}\'.format(filenames[0]),\n                     param=\'--variables\', param_hint=\'--variables\'\n                )\n            first_variable = variables[0]\n\n        else:\n            first_variable = list(data_variables(ds).keys())[0]\n\n        var_obj = ds.variables[first_variable]\n        dimensions = var_obj.dimensions\n        shape = var_obj.shape\n        num_dimensions = len(shape)\n        if not var_obj.shape[-2:] == zones.shape:\n            raise click.UsageError(\n                \'All datasets must have same shape for last 2 dimensions as zones\')\n        if num_dimensions > 3:\n            raise click.UsageError(\'This does not handle > 3 dimensions\')\n        elif num_dimensions == 3:\n            z_values = ds.variables[dimensions[0]][:]\n\n    results = {}\n    for filename in filenames:\n        with Dataset(filename) as ds:\n            filename_root = os.path.split(filename)[1].replace(\'.nc\', \'\')\n\n            click.echo(\'Processing {0}\'.format(filename))\n\n            if variables is not None and set(variables).difference(ds.variables.keys()):\n                raise click.BadParameter(\n                    \'One or more variables were not found in {0}\'.format(filenames[0]),\n                     param=\'--variables\', param_hint=\'--variables\'\n                )\n\n            results[filename_root] = dict()\n            for variable in (variables or list(data_variables(ds).keys())):\n                var_obj = ds.variables[variable]\n\n                if not var_obj.dimensions[:] == dimensions:\n                    raise click.UsageError(\n                        \'All datasets must have the same dimensions for {0}\'.format(variable))\n\n                if num_dimensions == 3:\n                    results[filename_root][variable] = dict()\n\n                    for z_idx in range(shape[0]):\n                        z_value = z_values[z_idx].item()  # TODO: actually need to resolve z_idx to a time value in time variable!\n                        data = numpy.ma.masked_array(var_obj[z_idx])\n                        results[filename_root][variable][z_value] = calculate_zonal_statistics(zones, zone_values, data, statistics)\n\n                    # this way works too, but may run out of memory\n                    # output below would need to be updated to use this though\n                    # data = numpy.ma.masked_array(var_obj[:])\n                    # results[filename_root][variable] = calculate_zonal_statistics(zones, zone_values, data, statistics)\n\n                else:\n                    data = numpy.ma.masked_array(var_obj[:])\n                    results[filename_root][variable] = calculate_zonal_statistics(zones, zone_values, data, statistics)\n\n    with open(output, \'wb\') as outfile:\n        if os.path.splitext(output)[1] == \'.json\':\n            outfile.write(json.dumps(results, indent=2))\n        else:\n            writer = csv.writer(outfile)\n            header = [\'filename\', \'variable\']\n            if num_dimensions == 3:\n                header += [dimensions[0]]\n            header += [\'zone\'] + statistics\n            writer.writerow(header)\n\n            rows = []\n\n            for filename in results:\n                for variable in results[filename]:\n                    if num_dimensions == 3:\n                        for z_value in results[filename][variable]:\n                            for zone in results[filename][variable][z_value]:\n                                result = results[filename][variable][z_value][zone]\n                                rows.append([filename, variable, z_value, zone] + [result[stat] for stat in statistics])\n                    else:\n                        for zone in results[filename][variable]:\n                            result = results[filename][variable][zone]\n                            rows.append([filename, variable, zone] + [result[stat] for stat in statistics])\n\n            for row in rows:\n                writer.writerow(row)\n\n    click.echo(\'Elapsed: {0:.2f}\'.format(time.time() - start))\n'"
trefoil/examples/csv_pivot.py,0,"b'""""""\r\nExample to demonstrate creating a pivot table from the output of zonal stats CLI\r\n""""""\r\n\r\nimport time\r\nimport pandas\r\n\r\n\r\n# Return a pipe-delimited combination of value from every column up through zone\r\ndef get_key(row):\r\n    key_parts = []\r\n    for col in row.keys():\r\n        if col == \'zone\':\r\n            return \'|\'.join(key_parts)\r\n\r\n        key_parts.append(str(row[col]))\r\n\r\n\r\nstart = time.time()\r\n\r\ninfilename = \'/tmp/test.csv\'\r\ndf = pandas.read_csv(infilename)\r\ndf[\'key\'] = df.apply(lambda x: get_key(x), axis=1)\r\nsub_df = df[[\'key\', \'zone\', \'mean\']]\r\npivot = sub_df.pivot(\'zone\', columns=\'key\')\r\n\r\n# Need to manually create the CSV instead of letting pandas do it, due to composite header\r\n# we don\'t want\r\nwith open(\'/tmp/pivot.csv\', \'w\') as outfile:\r\n    header = \',\'.join( [\'zone\'] + pivot.columns.levels[1].tolist())\r\n    csv_data = pivot.to_csv(None, index=True, header=False)\r\n    outfile.write(header + \'\\n\' + csv_data)\r\n\r\nprint(\'Elapsed: {0:.2f}\'.format(time.time() - start))'"
trefoil/examples/warp_example.py,0,"b""from netCDF4 import Dataset\n\nfrom pyproj import Proj\nfrom rasterio.enums import Resampling\n\nfrom trefoil.netcdf.utilities import data_variables\nfrom trefoil.netcdf.warp import warp_like\n\ntemplate_ds = Dataset('../test_data/ca_ru_1km.nc')\ntemplate_variable_name = 'data'\n\n# ds = Dataset('../test_data/tmin.nc')\nds = Dataset('c:/temp/lc_800m_lu_nbp.nc')\nvariables = data_variables(ds).keys()  # ['tmin']\n\n\nwith Dataset('c:/temp/out.nc', 'w') as out_ds:\n    warp_like(\n        ds,\n        ds_projection='EPSG:4326',  # source data are in geographic w/ WGS84 datum\n        variables=variables,\n        out_ds=out_ds,\n        template_ds=template_ds,\n        template_varname=template_variable_name,\n        resampling=Resampling.cubic  # could also be Resampling.nearest\n    )\n\n\n\n\n\n\n\n\n\n\n"""
trefoil/geometry/__init__.py,0,b''
trefoil/geometry/bbox.py,0,"b'import copy, math\r\nfrom itertools import product\r\nfrom pyproj import Proj, transform\r\nfrom six import text_type\r\n\r\n\r\nclass BBox(object):\r\n    """"""\r\n    Encapsulates bounding box related logic with associated projection information (must be a pyproj projection object).\r\n    """"""\r\n\r\n    def __init__(self, bbox, projection=None):\r\n        self.xmin = None\r\n        self.ymin = None\r\n        self.xmax = None\r\n        self.ymin = None\r\n        self.projection = None\r\n\r\n        if isinstance(bbox, BBox):\r\n            for att in (""xmin"", ""ymin"", ""xmax"", ""ymax"", ""projection""):\r\n                setattr(self, att, getattr(bbox, att))\r\n        elif isinstance(bbox, (list, tuple)) and len(bbox) == 4:\r\n            self.xmin = bbox[0]\r\n            self.ymin = bbox[1]\r\n            self.xmax = bbox[2]\r\n            self.ymax = bbox[3]\r\n        if projection:\r\n            assert isinstance(projection, Proj)\r\n            self.projection = projection\r\n\r\n        self.width = abs(self.xmax - self.xmin)\r\n        self.height = abs(self.ymax - self.ymin)\r\n\r\n    def __unicode__(self):\r\n        return text_type(self.as_list())\r\n\r\n    def __repr__(self):\r\n        return text_type(self.as_list())\r\n\r\n    @classmethod\r\n    def from_affine(cls, affine, width, height, projection=None):\r\n        """"""\r\n        Return new BBox object based on an Affine object\r\n        """"""\r\n        return cls(\r\n            (affine.c, affine.f + affine.e * height, affine.c + affine.a * width, affine.f),\r\n            projection=projection\r\n        )\r\n\r\n    def clone(self):\r\n        return copy.copy(self)\r\n\r\n    def as_list(self):\r\n        return [getattr(self, key) for key in (""xmin"", ""ymin"", ""xmax"", ""ymax"")]\r\n\r\n    def as_dict(self):\r\n        info = {key: getattr(self, key) for key in (""xmin"", ""ymin"", ""xmax"", ""ymax"")}\r\n        if self.projection:\r\n            info[\'proj4\'] = self.projection.srs\r\n        return info\r\n\r\n    def is_geographic(self):\r\n        return self.projection.is_latlong()\r\n\r\n    def project(self, target_projection, edge_points=9):\r\n        """"""\r\n        target_projection must be a pyproj projection.\r\n        Densifies the edges with edge_points points between corners, and projects all of them.\r\n        Returns the outer bounds of the projected coords.\r\n\r\n        Note: beware projection issues when going to projections that don\'t fully encapsulate the world domain\r\n        (e.g., Web Mercator has singularities above and below latitudes of ~ 85).\r\n        """"""\r\n\r\n        assert self.projection and isinstance(target_projection, Proj)\r\n\r\n        if target_projection.srs == self.projection.srs:\r\n            return self.clone()\r\n\r\n        if edge_points < 2:\r\n            # use corners only\r\n            x_values, y_values = transform(self.projection, target_projection, [self.xmin, self.xmax], [self.ymin, self.ymax])\r\n            return BBox((x_values[0], y_values[0], x_values[1], y_values[1]), projection=target_projection)\r\n\r\n        samples = range(0, edge_points)\r\n        xstep = float(self.xmax-self.xmin)/(edge_points-1)\r\n        ystep = float(self.ymax-self.ymin)/(edge_points-1)\r\n        x_values = []\r\n        y_values = []\r\n        for i, j in product(samples, samples):\r\n            x_values.append(self.xmin + xstep * i)\r\n            y_values.append(self.ymin + ystep * j)\r\n        # TODO: check for bidrectional consistency, as is done in ncserve BoundingBox.project() method\r\n        x_values, y_values = transform(self.projection, target_projection, x_values, y_values)\r\n        return BBox((min(x_values), min(y_values), max(x_values), max(y_values)), target_projection)\r\n\r\n    def get_local_albers_projection(self):\r\n        """"""\r\n        Project bbox to geographic coordinates, create a custom Albers projection centered over the bbox that minimizes\r\n        area distortions.  Uses 1/6 inset from ymin and ymax to define latitude bounds, and centerline between xmin and xmax\r\n        to define central meridian.\r\n        Coordinates must be within the world domain.\r\n        """"""\r\n\r\n        inset_factor = 1.0 / 6.0\r\n        geo_bbox = self.project(Proj(init=""EPSG:4326""))\r\n        # Make sure we are within world domain\r\n        assert geo_bbox.xmin >= -180 and geo_bbox.xmax <= 180 and geo_bbox.ymin >= -90 and geo_bbox.ymax <= 90\r\n        inset = math.fabs((geo_bbox.ymax - geo_bbox.ymin) * inset_factor)\r\n        return Proj(proj=\'aea\', lat_1=geo_bbox.ymin + inset, lat_2=geo_bbox.ymax - inset, lat_0=0,\r\n                    lon_0=((geo_bbox.xmax-geo_bbox.xmin)/2.0) + geo_bbox.xmin,\r\n                    x_0=0, y_0=0, ellps=\'WGS84\', datum=\'WGS84\', units=\'m\', no_defs=True)\r\n\r\n\r\ndef union_bbox(bboxes):\r\n    """"""\r\n    Return the bounding box that includes all bounding boxes.\r\n    """"""\r\n\r\n    bboxes = [x for x in bboxes if x is not None]\r\n    if not bboxes:\r\n        return None\r\n\r\n    x_set = {b.xmin for b in bboxes} | {b.xmax for b in bboxes}\r\n    y_set = {b.ymin for b in bboxes} | {b.ymax for b in bboxes}\r\n\r\n    return BBox((min(x_set), min(y_set), max(x_set), max(y_set)), projection=bboxes[0].projection)'"
trefoil/netcdf/__init__.py,0,b''
trefoil/netcdf/conversion.py,0,"b'from netCDF4 import Dataset\r\nimport os\r\nimport time\r\n\r\nimport rasterio\r\nimport pyproj\r\nfrom six import string_types\r\n\r\nfrom trefoil.netcdf.crs import set_crs\r\nfrom trefoil.netcdf.variable import SpatialCoordinateVariables\r\nfrom trefoil.geometry.bbox import BBox\r\nfrom trefoil.netcdf.crs import get_crs\r\nfrom trefoil.utilities.conversion import array_to_raster\r\n\r\n\r\ndef raster_to_netcdf(filename_or_raster, outfilename=None, variable_name=\'data\', format=\'NETCDF4\', **kwargs):\r\n    """"""\r\n    Parameters\r\n    ----------\r\n    filename_or_raster: name of file to open with rasterio, or opened rasterio raster dataset\r\n    outfilename: name of output file.  If blank, will be same name as input with *.nc extension added\r\n    variable_name: output format for netCDF file: NETCDF3_CLASSIC, NETCDF3_64BIT, NETCDF4_CLASSIC, NETCDF4\r\n    format\r\n    kwargs: arguments passed to variable creation: zlib\r\n\r\n    Note: only rasters with descending y coordinates are currently supported\r\n    """"""\r\n\r\n    start = time.time()\r\n\r\n    if isinstance(filename_or_raster, string_types):\r\n        if not os.path.exists(filename_or_raster):\r\n            raise ValueError(\'File does not exist: {0}\'.format(filename_or_raster))\r\n\r\n        src = rasterio.open(filename_or_raster)\r\n        managed_raster = True\r\n    else:\r\n        src = filename_or_raster\r\n        managed_raster = False\r\n\r\n    if not src.count == 1:\r\n        raise NotImplementedError(\'ERROR: multi-band rasters not yet supported for this operation\')\r\n\r\n    prj = pyproj.Proj(**src.crs)\r\n\r\n    outfilename = outfilename or src.name + \'.nc\'\r\n    with Dataset(outfilename, \'w\', format=format) as target:\r\n        if prj.is_latlong():\r\n            x_varname = \'longitude\'\r\n            y_varname = \'latitude\'\r\n        else:\r\n            x_varname = \'x\'\r\n            y_varname = \'y\'\r\n\r\n        # TODO: may need to do this in blocks if source is big\r\n        data = src.read(1, masked=True)\r\n\r\n        coords = SpatialCoordinateVariables.from_bbox(BBox(src.bounds, prj), src.width, src.height)\r\n        coords.add_to_dataset(target, x_varname, y_varname, **kwargs)\r\n\r\n        out_var = target.createVariable(variable_name, data.dtype, dimensions=(y_varname, x_varname), **kwargs)\r\n        out_var[:] = data\r\n        set_crs(target, variable_name, prj, set_proj4_att=False)\r\n\r\n    if managed_raster:\r\n        src.close()\r\n\r\n    print(\'Elapsed {0:.3f} seconds\'.format(time.time() - start))\r\n\r\n\r\ndef netcdf_to_raster(\r\n        path_or_dataset,\r\n        variable_name,\r\n        outfilename,\r\n        index=0,\r\n        projection=None):\r\n    """"""\r\n    Exports a 2D slice from a netcdf file to a raster file.\r\n    Only GeoTiffs are supported at this time.\r\n\r\n\r\n    Parameters\r\n    ----------\r\n    path_or_dataset: path to NetCDF file or open Dataset\r\n    variable_name: name of data variable to export from dataset\r\n    outfilename: output filename\r\n    index: index within 3rd dimension (in first position) or 0\r\n    projection: pyproj.Proj object.  Automatically determined from file if possible\r\n    """"""\r\n\r\n    if isinstance(path_or_dataset, string_types):\r\n        dataset = Dataset(path_or_dataset)\r\n    else:\r\n        dataset = path_or_dataset\r\n\r\n    projection = projection or get_crs(dataset, variable_name)\r\n    if not projection:\r\n        raise ValueError(\'Projection must be provided; \'\r\n                         \'no projection information can be determined from file\')\r\n\r\n    # TODO figure out cleaner way to get affine or coords\r\n    y_name, x_name = dataset.variables[variable_name].dimensions[:2]\r\n    coords = SpatialCoordinateVariables.from_dataset(\r\n        dataset, x_name, y_name, projection=projection)\r\n    affine = coords.affine\r\n\r\n    if outfilename.lower().endswith(\'.tif\'):\r\n        format = \'GTiff\'\r\n    else:\r\n        raise ValueError(\'Only GeoTiff outputs supported, filename must have .tif extension\')\r\n\r\n    variable = dataset.variables[variable_name]\r\n    ndims = len(variable.shape)\r\n    if ndims == 2:\r\n        if index != 0:\r\n            raise ValueError(\'Index out of range, must be 0\')\r\n        data = variable[:]\r\n    elif ndims == 3:\r\n        # Assumes that time dimension is first\r\n        if index < 0 or index >= variable.shape[0]:\r\n            raise ValueError(\'Index out of range, \'\r\n                             \'must be between 0 and {0}\'.variable.shape[0])\r\n        data = variable[index]\r\n\r\n    else:\r\n        raise ValueError(\r\n            \'Unsupported number of dimensions {0} for variable {1}, \'\r\n            \'must be 2 or 3\'.format(ndims, variable_name))\r\n\r\n    array_to_raster(\r\n        data,\r\n        outfilename,\r\n        format=format,\r\n        projection=projection,\r\n        affine=affine)'"
trefoil/netcdf/crs.py,0,"b'""""""\r\nProvides conversion functions between PROJ4 strings and CF Convention projection\r\nparameters stored as attributes on a grid_mapping variable\r\n\r\nConversions are loosely based on OCGIS approach to CRS:\r\nhttps://github.com/NCPP/ocgis/blob/master/src/ocgis/interface/base/crs.py\r\n""""""\r\n\r\n\r\nimport logging\r\nimport os\r\nimport re\r\nfrom pyproj import Proj, pj_list, pj_ellps, pyproj_datadir\r\n\r\nfrom trefoil.netcdf.utilities import get_ncattrs, set_ncattrs\r\nfrom rasterio.crs import CRS\r\n\r\n\r\nPROJ4_GEOGRAPHIC = \'+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\'\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\ndef invert_dict(dictionary):\r\n    return {v: k for k, v in dictionary.items()}\r\n\r\n\r\ndef epsg_to_proj4(epsg_code):\r\n    data = open(os.path.join(pyproj_datadir, \'epsg\')).read()\r\n    match = re.search(\'(?<=<{0}>).*(?=<>)\'.format(epsg_code), data)\r\n    if not match:\r\n        raise ValueError(\'ERROR: EPSG {0} not found in proj4 data file\'.format(epsg_code))\r\n\r\n    return match.group().strip().replace(\'longlat\', \'latlong\')  # pyproj stores the longlat instead of latlong as used here\r\n\r\n\r\nPROJ4_KEY = \'proj4\'\r\n\r\nPROJ4_CF_ELLIPSOID_MAP = {\r\n    \'a\': \'semi_major_axis\',\r\n    \'b\': \'semi_minor_axis\',\r\n    \'rf\': \'inverse_flattening\'\r\n}\r\n\r\nPROJ4_CF_NAMES = {\r\n    \'aea\': \'albers_conical_equal_area\',\r\n    \'latlong\': \'latitude_longitude\',\r\n    \'laea\': \'lambert_azimuthal_equal_area\',\r\n    \'lcc\': \'lambert_conformal_conic\',\r\n    \'stere\': \'polar_stereographic\',\r\n    \'tmerc\': \'transverse_mercator\',\r\n    \'utm\': \'universal_transverse_mercator\'\r\n}\r\n\r\nPROJ4_CF_PARAM_MAP = {\r\n    \'aea\': {\r\n        \'lat_0\': \'latitude_of_projection_origin\',\r\n        \'lat_{0}\': \'standard_parallel\',\r\n        \'lon_0\': \'longitude_of_central_meridian\',\r\n        \'x_0\': \'false_easting\',\r\n        \'y_0\': \'false_northing\'\r\n    },\r\n    \'latlong\': {},  # No extra parameters for lat / long\r\n    \'laea\': {\r\n        \'lat_0\': \'latitude_of_projection_origin\',\r\n        \'lon_0\': \'longitude_of_projection_origin\',\r\n        \'x_0\': \'false_easting\',\r\n        \'y_0\': \'false_northing\'\r\n    },\r\n    \'lcc\': {\r\n        \'lat_0\': \'latitude_of_projection_origin\',\r\n        \'lat_{0}\': \'standard_parallel\',\r\n        \'lon_0\': \'longitude_of_central_meridian\',\r\n        \'x_0\': \'false_easting\',\r\n        \'y_0\': \'false_northing\'\r\n    },\r\n\r\n    \'stere\': {\r\n        \'k_0\': \'scale_factor\',\r\n        \'lat_0\': \'latitude_of_projection_origin\',\r\n        \'lat_ts\': \'standard_parallel\',\r\n        \'lon_0\': \'straight_vertical_longitude_from_pole\',\r\n        \'x_0\': \'false_easting\',\r\n        \'y_0\': \'false_northing\'\r\n    },\r\n    \'tmerc\': {\r\n        \'k_0\': \'scale_factor\',\r\n        \'lat_0\': \'latitude_of_projection_origin\',\r\n        \'lon_0\': \'longitude_of_central_meridian\',\r\n        \'x_0\': \'false_easting\',\r\n        \'y_0\': \'false_northing\'\r\n    },\r\n    \'utm\': {\r\n        \'zone\': \'utm_zone_number\'\r\n    }\r\n}\r\n\r\n# Build inverse maps\r\nCF_PROJ4_ELLPSOID_MAP = invert_dict(PROJ4_CF_ELLIPSOID_MAP)\r\nCF_PROJ4_NAMES = invert_dict(PROJ4_CF_NAMES)\r\nCF_PROJ4_PARAM_MAP = {PROJ4_CF_NAMES[k]: invert_dict(PROJ4_CF_PARAM_MAP[k]) for k in PROJ4_CF_PARAM_MAP}\r\n\r\n\r\ndef get_crs(dataset, variable_name):\r\n    """"""\r\n    Return a PROJ4 projection string for a variable in a dataset.\r\n    If non-standard \'proj4\' attribute is found in attributes of dataset or\r\n    variable, that is used instead.  Otherwise, the projection parameters are\r\n    extracted from attributes in the grid_mapping variable in the dataset\r\n    referenced from the data variable\'s attributes.\r\n\r\n    :param dataset: open netCDF dataset\r\n    :param variable_name: name of data variable to extract projection\r\n    :return: PROJ4 projection string or None\r\n    """"""\r\n\r\n    ncatts = get_ncattrs(dataset.variables[variable_name])\r\n    dsatts = get_ncattrs(dataset)\r\n\r\n    # If dataset already includes proj4 string, just use it\r\n    existing_proj4 = dsatts.get(PROJ4_KEY) or ncatts.get(PROJ4_KEY)\r\n    if existing_proj4:\r\n        return existing_proj4\r\n\r\n    # Attempt to construct proj4 string based on CF convention parameters\r\n    if \'grid_mapping\' not in ncatts:\r\n        logger.debug(\'grid_mapping attribute not found for variable {0}\'.format(variable_name))\r\n        return None\r\n\r\n    if ncatts[\'grid_mapping\'] not in dataset.variables:\r\n        logger.debug(\'grid_mapping variable {0} not found in dataset\'.format(ncatts[\'grid_mapping\']))\r\n        return None\r\n\r\n    crs_variable = dataset.variables[ncatts[\'grid_mapping\']]\r\n    crs_atts = get_ncattrs(crs_variable)\r\n\r\n    cf_crs_name = crs_atts.get(\'grid_mapping_name\')\r\n    if not (cf_crs_name and cf_crs_name in CF_PROJ4_PARAM_MAP):\r\n        # Could not determine projection name\r\n        logger.debug(\'No supported projection found for {0}\'.format(cf_crs_name))\r\n        return None\r\n\r\n    param_map = CF_PROJ4_PARAM_MAP[cf_crs_name]\r\n\r\n    proj4_params = {\'proj\': CF_PROJ4_NAMES[cf_crs_name]}\r\n\r\n    expected_params = set(CF_PROJ4_PARAM_MAP[cf_crs_name].keys())\r\n    if expected_params.difference(crs_atts):\r\n        logger.debug(\'Missing expected parameters {0}\'.format(expected_params.difference(crs_atts)))\r\n\r\n    for param in expected_params.intersection(crs_atts):\r\n        value = crs_atts[param]\r\n\r\n        if param == \'standard_parallel\' and \'{\' in param_map[param]:\r\n            # Special case: variable number of standard parallels\r\n            value = list(value)\r\n            for index, val in enumerate(value, start=1):\r\n                proj4_params[param_map[param].format(index)] = val\r\n        else:\r\n            proj4_params[param_map[param]] = value\r\n\r\n    for param in set(CF_PROJ4_ELLPSOID_MAP.keys()).intersection(crs_atts):\r\n        proj4_params[CF_PROJ4_ELLPSOID_MAP[param]] = crs_atts[param]\r\n\r\n    try:\r\n        return Proj(**CRS(proj4_params).to_dict()).srs\r\n\r\n    except:\r\n        # Could not create valid projection\r\n        logger.debug(\'Could not create valid Proj4 projection from parameters\')\r\n\r\n    return None\r\n\r\n\r\ndef set_crs(dataset, variable_name, projection, set_proj4_att=False):\r\n    """"""\r\n    Set the projection information into a grid_mapping variable and reference it\r\n    from the data variable.\r\n\r\n    :param dataset: dataset open in write or append mode\r\n    :param variable_name: name of data variable to attach projection to\r\n    :param projection: pyproj.Proj projection object\r\n    :param set_proj4_att: if True, set the \'proj4\' attribute on the variable\r\n    """"""\r\n\r\n    if not isinstance(projection, Proj):\r\n        raise ValueError(\'Projection must be instance of pyproj.Proj\')\r\n\r\n    variable = dataset.variables[variable_name]\r\n\r\n    if \'epsg:\' in projection.srs:\r\n        proj_string = epsg_to_proj4(re.search(\'(?<=epsg:)\\d+\', projection.srs).group())\r\n    else:\r\n        proj_string = projection.srs\r\n\r\n    if set_proj4_att:\r\n        variable.setncattr(PROJ4_KEY, proj_string)\r\n\r\n    proj_data = CRS.from_string(proj_string).to_dict()\r\n    proj_key = proj_data[\'proj\']\r\n    if not proj_key in PROJ4_CF_PARAM_MAP.keys():\r\n        raise ValueError(\'CF Convention mapping is not yet available for projection {0}\'.format(proj_key))\r\n\r\n    crs_variable_name = \'crs_{0}\'.format(pj_list[proj_key].replace(\' \', \'_\').replace(\'/\', \'\'))\r\n    if not crs_variable_name in dataset.variables:\r\n        crs_variable = dataset.createVariable(crs_variable_name, \'S1\')\r\n\r\n        ncatts = {\'grid_mapping_name\': PROJ4_CF_NAMES[proj_key]}\r\n\r\n        out_proj_params = PROJ4_CF_PARAM_MAP[proj_key]\r\n        for param in out_proj_params:\r\n            if param.count(\'{\'):\r\n                # Special case - standard parallel\r\n                keys = [param.format(i) for i in (1, 2)]\r\n                values = [proj_data[key] for key in keys if key in proj_data]\r\n                if values:\r\n                    if len(values) == 1:\r\n                        values = values[0]\r\n                    ncatts[out_proj_params[param]] = values\r\n\r\n            elif param in proj_data:\r\n                ncatts[out_proj_params[param]] = proj_data[param]\r\n\r\n        if \'datum\' in proj_data and not \'ellps\' in proj_data:\r\n            # Not all datums link to available pj_ellps keys, some had to be added manually here\r\n            if proj_data[\'datum\'] in pj_ellps:\r\n                proj_data[\'ellps\'] = proj_data[\'datum\']\r\n            elif proj_data[\'datum\'] == \'NAD83\':\r\n                proj_data[\'ellps\'] = \'GRS80\'\r\n            elif proj_data[\'datum\'] == \'NAD27\':\r\n                proj_data[\'ellps\'] = \'clrk66\'\r\n            else:\r\n                raise ValueError(\'projection ellipsoid must be specified, datum {0}\'\r\n                                 \'does not match a known ellipsoid\'.format(proj_data[\'datum\']))\r\n\r\n        # Extract out parameters of known ellipsoids\r\n        if \'ellps\' in proj_data:\r\n            if not proj_data[\'ellps\'] in pj_ellps:\r\n                raise ValueError(\'projection ellipsoid does not match a known ellipsoid\')\r\n\r\n            ellipsoid_params = pj_ellps[proj_data[\'ellps\']]\r\n            for param in set(PROJ4_CF_ELLIPSOID_MAP.keys()).intersection(ellipsoid_params):\r\n                proj_data[param] = ellipsoid_params[param]\r\n\r\n        for param in set(PROJ4_CF_ELLIPSOID_MAP.keys()).intersection(proj_data):\r\n            ncatts[PROJ4_CF_ELLIPSOID_MAP[param]] = proj_data[param]\r\n\r\n        set_ncattrs(crs_variable, ncatts)\r\n\r\n    variable.setncattr(\'grid_mapping\', crs_variable_name)\r\n\r\n\r\n\r\ndef is_geographic(dataset, variable_name):\r\n    """"""\r\n    Try to determine if dataset appears to be geographic.  This is a fallback if a true CRS cannot be obtained using other\r\n    functions.  Currently limited to checking names of spatial dimensions.\r\n\r\n    :param dataset: open netCDF dataset\r\n    :param variable_name: name of data variable\r\n    :returns: True if variable appears to be in geographic coordinates\r\n    """"""\r\n\r\n    options = (\r\n        {\'lat\', \'lon\'},\r\n        {\'lat\', \'long\'},\r\n        {\'latitude\', \'longitude\'}\r\n    )\r\n\r\n    variable = dataset.variables[variable_name]\r\n    dim_names = set([d.lower() for d in variable.dimensions[-2:]])\r\n\r\n    for option in options:\r\n        if not option.difference(dim_names):\r\n            return True\r\n\r\n    return False\r\n\r\n\r\n'"
trefoil/netcdf/describe.py,0,"b'import numpy\r\nfrom six import string_types\r\nfrom netCDF4 import Dataset\r\nfrom pyproj import Proj\r\n\r\nfrom trefoil.netcdf.crs import get_crs, is_geographic, PROJ4_GEOGRAPHIC\r\nfrom trefoil.netcdf.utilities import get_ncattrs\r\nfrom trefoil.netcdf.variable import SpatialCoordinateVariable, SpatialCoordinateVariables, DateVariable\r\n\r\n\r\nX_DIMENSION_STANDARD_NAMES = (\'longitude\', \'grid_longitude\', \'projection_x_coordinate\')\r\nX_DIMENSION_COMMON_NAMES = (\'longitude\', \'lon\', \'long\', \'x\')\r\nY_DIMENSION_STANDARD_NAMES = (\'latitude\', \'grid_latitude\', \'projection_y_coordinate\')\r\nY_DIMENSION_COMMON_NAMES = (\'latitude\', \'lat\', \'y\')\r\nTIME_DIMENSION_STANDARD_NAMES = (\'time\',)\r\nTIME_DIMENSION_COMMON_NAMES = (\'time\', \'year\', \'years\')  # TODO: months?\r\n\r\n\r\ndef get_interval(data):\r\n    if data.shape[0] > 1:\r\n        unique_intervals = numpy.unique(data[1:] - data[:-1])\r\n        if unique_intervals.shape[0] == 1:\r\n            return numpy.abs(unique_intervals[0]).item()\r\n\r\n    # Not equal interval, interval doesn\'t apply\r\n    return None\r\n\r\n\r\ndef describe(path_or_dataset):\r\n    if isinstance(path_or_dataset, string_types):\r\n        dataset = Dataset(path_or_dataset)\r\n    else:\r\n        dataset = path_or_dataset\r\n\r\n    description = {\r\n        \'dimensions\': {},\r\n        \'variables\': {},\r\n        \'attributes\': get_ncattrs(dataset)\r\n    }\r\n\r\n    for dimension_name in dataset.dimensions:\r\n        dimension = dataset.dimensions[dimension_name]\r\n        description[\'dimensions\'][dimension_name] = {\r\n            \'length\': len(dimension)\r\n        }\r\n\r\n    for variable_name in dataset.variables:\r\n        variable = dataset.variables[variable_name]\r\n\r\n        if not variable.dimensions:\r\n            # Do not collect info about dimensionless variables (e.g., CRS variable)\r\n            continue\r\n\r\n        dtype = str(variable.dtype)\r\n        if ""\'"" in dtype:\r\n            dtype = dtype.split(""\'"")[1]\r\n\r\n        attributes = get_ncattrs(variable)\r\n        variable_info = {\r\n            \'attributes\': attributes,\r\n            \'dimensions\': variable.dimensions,\r\n            \'data_type\': dtype,\r\n            \'name\': attributes.get(\'long_name\') or attributes.get(\'standard_name\') or variable_name\r\n        }\r\n\r\n        if dtype not in (\'str\', ):\r\n            if len(variable.shape) > 2:\r\n                # Avoid loading the entire array into memory by iterating along the first index (usually time)\r\n                variable_info.update({\r\n                    \'min\': min(variable[i, :].min().item() for i in range(variable.shape[0])),\r\n                    \'max\': max(variable[i, :].max().item() for i in range(variable.shape[0]))\r\n                })\r\n            else:\r\n                data = variable[:]\r\n                variable_info.update({\r\n                    \'min\': data.min().item(),\r\n                    \'max\': data.max().item()\r\n                })\r\n\r\n        if variable_name in dataset.dimensions and dtype not in (\'str\', ):\r\n            dimension_variable = dataset.variables[variable_name]\r\n            if len(dimension_variable.dimensions) == 1:  # range dimensions don\'t make sense for interval\r\n                interval = get_interval(dimension_variable)\r\n                if interval:\r\n                    variable_info[\'interval\'] = interval\r\n\r\n        else:\r\n            # Data variable\r\n            proj4 = get_crs(dataset, variable_name)\r\n\r\n            #extent\r\n            if len(variable.dimensions) >= 2:\r\n                x_variable_name = None\r\n                y_variable_name = None\r\n                time_variable_name = None\r\n                for dimension_name in (x for x in variable.dimensions if x in dataset.variables):\r\n                    attributes = get_ncattrs(dataset.variables[dimension_name])\r\n                    standard_name = attributes.get(\'standard_name\', None)\r\n                    if standard_name in X_DIMENSION_STANDARD_NAMES or dimension_name in X_DIMENSION_COMMON_NAMES:\r\n                        x_variable_name = dimension_name\r\n                    elif standard_name in Y_DIMENSION_STANDARD_NAMES or dimension_name in Y_DIMENSION_COMMON_NAMES:\r\n                        y_variable_name = dimension_name\r\n                    elif standard_name in TIME_DIMENSION_STANDARD_NAMES or dimension_name in TIME_DIMENSION_COMMON_NAMES:\r\n                        if len(dataset.dimensions[dimension_name]) > 1:\r\n                            time_variable_name = dimension_name\r\n                if x_variable_name and y_variable_name:\r\n                    if proj4 is None and is_geographic(dataset, variable_name):\r\n                        # Assume WGS84\r\n                        proj4 = PROJ4_GEOGRAPHIC\r\n\r\n                    coordinates = SpatialCoordinateVariables(\r\n                        SpatialCoordinateVariable(dataset.variables[x_variable_name]),\r\n                        SpatialCoordinateVariable(dataset.variables[y_variable_name]),\r\n                        Proj(str(proj4)) if proj4 else None\r\n                    )\r\n\r\n                    variable_info[\'spatial_grid\'] = {\r\n                        \'extent\': coordinates.bbox.as_dict(),\r\n                        \'x_dimension\': x_variable_name,\r\n                        \'x_resolution\': coordinates.x.pixel_size,\r\n                        \'y_dimension\': y_variable_name,\r\n                        \'y_resolution\': coordinates.y.pixel_size\r\n                    }\r\n                if time_variable_name:\r\n                    time_variable = dataset.variables[time_variable_name]\r\n\r\n                    time_info = {\r\n                        \'dimension\': time_variable_name,\r\n                    }\r\n\r\n                    try:\r\n                        date_variable = DateVariable(time_variable)\r\n                        values = date_variable.datetimes\r\n                        time_info[\'extent\'] = [values.min().isoformat(), values.max().isoformat()]\r\n                        time_info[\'interval_unit\'] = date_variable.unit\r\n                        interval = get_interval(time_variable)\r\n                        if interval is not None:\r\n                            time_info[\'interval\'] = interval\r\n\r\n                    except ValueError:\r\n                        pass\r\n\r\n                    variable_info[\'time\'] = time_info\r\n\r\n            if proj4:\r\n                variable_info[\'proj4\'] = proj4\r\n\r\n        description[\'variables\'][variable_name] = variable_info\r\n\r\n    return description\r\n'"
trefoil/netcdf/utilities.py,0,"b'import os\r\nimport re\r\nimport six\r\nfrom collections import OrderedDict\r\nimport numpy\r\nimport math\r\nfrom netCDF4 import Dataset, default_fillvals\r\n\r\n\r\nNUMBER_REGEXP = re.compile(\'\\d+\')\r\n\r\n\r\ndef get_fill_value(dtype):\r\n    if isinstance(dtype, six.string_types):\r\n        dtype = numpy.dtype(dtype)\r\n    return default_fillvals[dtype.str[1:]]\r\n\r\n\r\ndef get_fill_value_for_variable(variable):\r\n    if hasattr(variable, \'fill_value\'):\r\n        return variable.fill_value\r\n    elif hasattr(variable, \'_FillValue\'):\r\n        return variable._FillValue\r\n    elif hasattr(variable, \'missing_value\'):\r\n        return variable.missing_value\r\n    return get_fill_value(variable.dtype)\r\n\r\n\r\ndef get_dtype_string(variable):\r\n    dtype = str(variable.dtype)\r\n    if ""\'"" in dtype:\r\n        dtype = dtype.split(""\'"")[1]\r\n\r\n    return dtype\r\n\r\n\r\ndef copy_dimension(source_dataset, target_dataset, name, overwrite=True, allow_unlimited=False):\r\n    """"""\r\n    Copies a dimension to a netCDF file, deleting it if it already exists (unless overwrite is false, in which case\r\n    this raises an exception).\r\n    Copies the size if the dimension is not unlimited.\r\n\r\n    :param source_dataset: the source netCDF dataset\r\n    :param target_dataset: the target netCDF to copy into.  Must be in edit / write mode.\r\n    :param name: the name of the dimension\r\n    :param overwrite: if true, overwrite the dimension if found in target.  May cause bad side effects if other variables\r\n    :param allow_unlimited: if true, allow unlimited dimensions to remain unlimited\r\n    depend on that dimension in the target.\r\n\r\n    :return new dimension\r\n    """"""\r\n\r\n    if name in target_dataset.dimensions:\r\n        if overwrite:\r\n            del target_dataset.dimensions[name]\r\n        else:\r\n            raise Exception(""Target dimension already exists, and overwrite is false"")\r\n    source_dimension = source_dataset.dimensions[name]\r\n    if allow_unlimited and source_dimension.isunlimited():\r\n        return target_dataset.createDimension(name, None)\r\n    else:\r\n        return target_dataset.createDimension(name, len(source_dimension))\r\n\r\n\r\ndef copy_variable(source_dataset, target_dataset, name, out_name=None, overwrite=True, **kwargs):\r\n    """"""\r\n    Copies a variable to a netCDF file, deleting it if it already exists (unless overwrite is false).\r\n    Copies the required dimensions first, if they don\'t already exist.\r\n    Copies the variable\'s attributes across.\r\n    Raises exception if dimensions already exist and do not match the size required by this variable.\r\n\r\n    :param source_dataset: the source netCDF dataset\r\n    :param target_dataset: the target netCDF to copy into.  Must be in edit / write mode.\r\n    :param name: the name of the variable\r\n    :param out_name: the output name of the variable\r\n    :param overwrite: if true, overwrite the variable if it exists in the target\r\n    """"""\r\n\r\n    if not out_name:\r\n        out_name = name\r\n\r\n    if out_name in target_dataset.variables:\r\n        if overwrite:\r\n            del target_dataset.variables[out_name]\r\n        else:\r\n            raise Exception(""Target variable already exists, and overwrite is false"")\r\n\r\n    source_variable = source_dataset.variables[name]\r\n    for dimension_name in source_variable.dimensions:\r\n        if dimension_name in target_dataset.dimensions:\r\n            source_dimension = source_dataset.dimensions[dimension_name]\r\n            target_dimension = target_dataset.dimensions[dimension_name]\r\n            if not (len(target_dimension) == len(source_dimension) or\r\n                        target_dimension.isunlimited() == source_dimension.isunlimited()):\r\n                raise Exception(""Dimension already exists in target, but has different size"")\r\n        else:\r\n            copy_dimension(source_dataset, target_dataset, dimension_name)\r\n        if (dimension_name in source_dataset.variables and not dimension_name in target_dataset.variables\r\n            and dimension_name != name):\r\n            copy_variable(source_dataset, target_dataset, dimension_name)\r\n\r\n    if \'fill_value\' not in kwargs and source_variable.dtype != numpy.dtype(\'str\'):\r\n        kwargs[\'fill_value\'] = get_fill_value_for_variable(source_variable)\r\n\r\n    target_variable = target_dataset.createVariable(out_name, source_variable.dtype, source_variable.dimensions, **kwargs)\r\n    target_variable[:] = source_variable[:]\r\n    for attribute_name in source_variable.ncattrs():\r\n        if not attribute_name in target_variable.ncattrs():\r\n            target_variable.setncattr(attribute_name, source_variable.getncattr(attribute_name))\r\n\r\n\r\ndef copy_attributes(source, target, attribute_names, overwrite=True):\r\n    """"""\r\n    Copies attributes from source object to target object, overwriting if already exists.\r\n\r\n    :param source: the source object (dataset, variable, etc)\r\n    :param target: the target object\r\n    :param attribute_names: tuple / list of attribute names to copy across\r\n    """"""\r\n\r\n    for attribute_name in attribute_names:\r\n        # Skip protected attributes that must be set in other ways\r\n        if attribute_name in (\'_FillValue\'):\r\n            continue\r\n\r\n        if hasattr(target, attribute_name) and not overwrite:\r\n            raise Exception(""Attribute already exists in target, but overwrite is false"")\r\n        setattr(target, attribute_name, getattr(source, attribute_name))\r\n\r\n\r\ndef copy_variable_dimensions(source_dataset, target_dataset, name, overwrite=True, **kwargs):\r\n    """"""\r\n    Copies the dimensions for a variable to target_dataset.\r\n\r\n    :param source_dataset: the source netCDF dataset\r\n    :param target_dataset: the target netCDF to copy into.  Must be in edit / write mode.\r\n    :param name: the name of the variable whose dimensions we want to copy\r\n    :param overwrite: if true, overwrite the dimensions if they exists in the target\r\n    :param kwargs: kwargs passed to copy_dimensions for each dimension\r\n    """"""\r\n\r\n    for dimension_name in source_dataset.variables[name].dimensions:\r\n        if dimension_name in source_dataset.variables:\r\n            copy_variable(source_dataset, target_dataset, dimension_name, overwrite=overwrite, **kwargs)\r\n        else:\r\n            copy_dimension(source_dataset, target_dataset, dimension_name, overwrite=overwrite, **kwargs)\r\n\r\n\r\ndef create_variable_like(target_dataset, target_name, like_dataset, like_name, overwrite=True, **kwargs):\r\n    """"""\r\n    Creates a new variable like an existing variable\r\n\r\n    :param target_dataset: the dataset in which to create the variable and associated dimensions.  Must be in write / edit mode.\r\n    :param target_name: name of variable to create\r\n     :param like_dataset: dataset that contains the variable used as a template\r\n    :param like_name: template variable\r\n    :param overwrite: if true, overwrite the variable if it exists in the target\r\n    :param kwargs: passed to variable create and to copy dimensions\r\n    :return: netCDF Variable object\r\n    """"""\r\n\r\n    if target_name in target_dataset.variables:\r\n        if overwrite:\r\n            del target_dataset.variables[target_name]\r\n        else:\r\n            raise Exception(""Target variable already exists, and overwrite is false"")\r\n\r\n    copy_variable_dimensions(like_dataset, target_dataset, like_name, overwrite=overwrite, **kwargs)\r\n    like_variable = like_dataset.variables[like_name]\r\n    return target_dataset.createVariable(target_name, like_variable.dtype, like_variable.dimensions, **kwargs)\r\n\r\n\r\ndef concat_variable_along_dimension(source_datasets, target_dataset, variable_name, dimension_name, **kwargs):\r\n    """"""\r\n    Creates a new variable in target and concatenates values for that variable along a new dimension of size equal\r\n    to the number of sources.\r\n\r\n    :param sources: open source datasets to copy variable from.  Must all be of the same dimensionality.  The first\r\n    source will be used as the template for the new variable created, and it is the only one used for any attributes\r\n    copied from the source to the target.\r\n    :param target: open target dataset (in write mode) to write concatenated values to\r\n    :param variable_name: name of variable to concatenate from source into target\r\n    :param dimension_name: name of new dimension; will be created with length equal to number of sources\r\n    :param kwargs: additional kwargs for creation of variable in target dataset\r\n    """"""\r\n\r\n\r\n    assert len(source_datasets) > 1\r\n\r\n    labels = None\r\n    if isinstance(source_datasets, OrderedDict):\r\n        labels = source_datasets.keys()\r\n        source_datasets = source_datasets.values()\r\n\r\n    # Initialize the variable using the first source as the template\r\n    source_dataset = source_datasets[0]\r\n    source_variable = source_dataset.variables[variable_name]\r\n    for dim in source_variable.dimensions:\r\n        copy_dimension(source_dataset, target_dataset, dim)\r\n        if dim in source_dataset.variables:\r\n            copy_variable(source_dataset, target_dataset, dim)\r\n\r\n    target_dataset.createDimension(dimension_name, len(source_datasets))\r\n    if labels is not None:\r\n        label_variable = target_dataset.createVariable(dimension_name, ""string"", (dimension_name,))\r\n        for index, label in enumerate(labels):\r\n            label_variable[index] = label\r\n\r\n    if not \'fill_value\' in kwargs:\r\n        kwargs[\'fill_value\'] = get_fill_value_for_variable(source_variable)\r\n\r\n    dimensions = list(source_variable.dimensions)\r\n    dimensions.insert(0, dimension_name)\r\n    target_variable = target_dataset.createVariable(variable_name, source_variable.dtype, dimensions, **kwargs)\r\n    for attribute_name in source_variable.ncattrs():\r\n        if not attribute_name in target_variable.ncattrs():\r\n            target_variable.setncattr(attribute_name, source_variable.getncattr(attribute_name))\r\n\r\n    for index, source_dataset in enumerate(source_datasets):\r\n        target_variable[index,] = source_dataset.variables[variable_name][:]\r\n\r\n\r\ndef extract_subset(source_dataset, target_dataset, name, slices, target_name=None, blocksize=100000000, **kwargs):\r\n    """"""\r\n    Extracts a subset of this variable along time and / or spatial coordinates into a new dataset, copying along\r\n    all dimensions and attributes associated with it from the original.\r\n\r\n    :param source_dataset: open source datasets to copy variable from\r\n    :param target_dataset: open target dataset (in write mode)\r\n    :param name: name of the variable to subset\r\n    :param slices: tuple of slice objects or None; one for each dimension.  Example: (slice(0, 2), None, slice(100, 200))\r\n    :param target_name: if None, will default to name\r\n    :param kwargs: additional parameters to pass to createVariable function\r\n\r\n    :return: the new variable in the target dataset\r\n    """"""\r\n\r\n    if six.PY2:\r\n        blocksize = long(blocksize)\r\n\r\n    source_variable = source_dataset.variables[name]\r\n    assert len(slices) == len(source_variable.dimensions)\r\n\r\n    slices = list(slices)\r\n\r\n    if name in target_dataset.variables:\r\n        raise ValueError(\'Variable with name {0} is already present in target dataset\'.format(name))\r\n\r\n    target_shape = []\r\n    for index, dimension in enumerate(source_variable.dimensions):\r\n        if dimension == name:  # current variable; break or we get infinite loop\r\n            break\r\n\r\n        # Calculate slices based on dimensions, if not provided\r\n        if slices[index] is None or slices[index].start is None:\r\n            slices[index] = slice(0, len(source_dataset.dimensions[dimension]))\r\n\r\n        cur_slice = slices[index]\r\n        dimension_length = cur_slice.stop - cur_slice.start\r\n        target_shape.append(dimension_length)\r\n\r\n        if dimension in target_dataset.dimensions:\r\n            if not len(target_dataset.dimensions[dimension]) == dimension_length:\r\n                raise ValueError(\'Target dimension already in target dataset, but with different length\')\r\n        else:\r\n            target_dataset.createDimension(dimension, dimension_length)\r\n\r\n        if dimension in source_dataset.variables and not dimension in target_dataset.variables:\r\n            extract_subset(source_dataset, target_dataset, dimension, (cur_slice,))\r\n\r\n    if not \'fill_value\' in kwargs:\r\n        kwargs[\'fill_value\'] = get_fill_value_for_variable(source_variable)\r\n\r\n    target_variable = target_dataset.createVariable(\r\n        target_name or name,\r\n        source_variable.dtype,\r\n        source_variable.dimensions,\r\n        **kwargs\r\n    )\r\n\r\n    target_size = numpy.product(target_shape)\r\n    if target_size < blocksize:\r\n        target_variable[:] = source_variable[slices]\r\n    else:\r\n        # Have to copy in blocks to avoid memory errors\r\n        # Assume that we can do this in increments of the first dimension\r\n        if numpy.product(target_shape[1:]) >= blocksize:\r\n            raise NotImplementedError(\'blocksize must be greater than the product of the shape for all secondary dimensions\')\r\n\r\n        increment = int(math.floor(float(target_shape[0] * blocksize) / float(target_size)))\r\n        num_increments = int(math.ceil(float(target_shape[0]) / increment))\r\n\r\n        for i in range(0, num_increments):\r\n            first_slice = slice(slices[0].start + (i * increment), min(slices[0].start + ((i + 1) * increment), slices[0].stop))\r\n            updated_slices = [first_slice] + slices[1:]\r\n            target_variable[(i*increment):min((i*increment) + increment, target_shape[0])] = source_variable[updated_slices]\r\n\r\n    # Copy source variable attributes\r\n    for attribute_name in source_variable.ncattrs():\r\n        if not attribute_name in target_variable.ncattrs():\r\n            target_variable.setncattr(attribute_name, source_variable.getncattr(attribute_name))\r\n\r\n    return target_variable\r\n\r\n\r\ndef get_ncattrs(obj):\r\n    """"""\r\n    Returns ncattrs of a netcdf object as a dictionary\r\n    :param obj: Object to collect ncattrs from\r\n    :return: dictionary representation of those ncattrs\r\n    """"""\r\n\r\n    out = {}\r\n    for key in obj.ncattrs():\r\n        value = obj.getncattr(key)\r\n        if hasattr(value, \'tolist\'):\r\n            # Convert numpy dtypes to native python types\r\n            value = value.tolist()\r\n\r\n        out[key] = value\r\n\r\n    return out\r\n\r\n\r\ndef set_ncattrs(obj, atts):\r\n    """"""\r\n    Sets attribute dictionary as ncattrs\r\n    :param obj: object against which to set ncattrs\r\n    :param atts: attributes dictionary\r\n    """"""\r\n\r\n    for key in atts:\r\n        obj.setncattr(key, atts[key])\r\n\r\n\r\ndef collect_statistics(filenames, variables, mask=None):\r\n    """"""\r\n    Collects basic statistics for each variable across all files\r\n    :param filenames: files to collect statistics from\r\n    :param variable: variables to collect statistics from\r\n    :param mask: numpy mask: True = areas to mask out.  Must be broadcastable to data variable arrays\r\n    :return: dictionary of {""<variable>"": {""min"": <min> ...} }\r\n    """"""\r\n\r\n    statistics = {v: {s: [] for s in (\'min\', \'mean\', \'max\')} for v in variables}\r\n\r\n    for filename in filenames:\r\n        with Dataset(filename) as ds:\r\n            for variable in variables:\r\n                if not variable in ds.variables:\r\n                    raise ValueError(\'Variable {0} is not present in dataset {1}\'.format(variable, filename))\r\n\r\n                data = numpy.ma.masked_array(ds.variables[variable][:], mask=mask)\r\n                stats = statistics[variable]\r\n                stats[\'min\'].append(data.min())\r\n                stats[\'mean\'].append(data.mean())\r\n                stats[\'max\'].append(data.max())\r\n\r\n    for variable in variables:\r\n        stats = statistics[variable]\r\n        stats[\'min\'] = numpy.min(stats[\'min\']).item()\r\n        stats[\'mean\'] = numpy.mean(stats[\'mean\']).item()\r\n        stats[\'max\'] = numpy.max(stats[\'max\']).item()\r\n\r\n    return statistics\r\n\r\n\r\ndef data_variables(ds):\r\n    """"""\r\n    Returns subset of ds.variables that are data rather than dimension variables\r\n    """"""\r\n\r\n    exclude_vars = set()\r\n    exclude_atts = (\'bounds\', \'grid_mapping\')  # exclude variables referenced from these attributes\r\n    for v in ds.variables.values():\r\n        atts = get_ncattrs(v)\r\n        for att in exclude_atts:\r\n            if att in atts:\r\n                exclude_vars.add(atts[att])\r\n\r\n    return OrderedDict([(k, v) for k, v in six.iteritems(ds.variables)\r\n                        if k not in ds.dimensions and k not in exclude_vars])\r\n\r\n\r\ndef get_pack_atts(dtype, min_value, max_value):\r\n    """"""\r\n    Get attributes based on the data type and value range.  Assumes that default fill values are used (top of data type\r\n    range for unsigned integer types).\r\n\r\n    scale_factor and add_offset attributes will be set on variable according to formula described here:\r\n    http://nco.sourceforge.net/nco.html#Packed-data\r\n\r\n    scale_factor = (max_value - min_value) / (2**bits - 2)        where bits are the number of bits from data type\r\n    add_offset = min_value\r\n\r\n    Parameters\r\n    ----------\r\n    dtype: a numpy dtype object or string (must be one of: int8, int16, int32, uint8, uint16, uint32)\r\n    min_value: number, minimum value of data to pack\r\n    max_value: number, maximum value of data to pack\r\n\r\n    Returns\r\n    -------\r\n    (scale_factor, add_offset)\r\n    """"""\r\n\r\n    if hasattr(dtype, \'name\'):\r\n        dtype = dtype.name\r\n\r\n    if not dtype in (\'uint8\', \'uint16\', \'uint32\'):\r\n        raise ValueError(\'data type for variable must be one of: uint8, uint16, uint32\')\r\n\r\n    nbits = int(NUMBER_REGEXP.search(dtype).group())\r\n\r\n    scale = (max_value - min_value) / (2**nbits - 2)\r\n    return scale, min_value\r\n\r\n\r\ndef resolve_dataset_variable(path):\r\n    """"""\r\n    Resolves a dataset plus variable path into a dataset path and\r\n    variable name.\r\n\r\n    example: ""/tmp/foo.nc:bar"" -> (""/tmp/foo.nc"", ""bar"")\r\n\r\n    Parameters\r\n    ----------\r\n    path: string, a compound path of dataset:variable\r\n\r\n    Returns\r\n    -------\r\n    tuple of dataset and variable\r\n    """"""\r\n\r\n    path, dataset = os.path.split(path)\r\n    variable = None\r\n    if \':\' in dataset:\r\n        dataset, variable = dataset.split(\':\')\r\n\r\n    return os.path.join(path, dataset), variable\r\n'"
trefoil/netcdf/variable.py,0,"b'from bisect import bisect_left, bisect_right\n\nimport numpy\nfrom datetime import date, datetime\nimport pytz\nfrom affine import Affine\nfrom netCDF4 import num2date, date2num, Variable\nfrom pyproj import Proj\nimport six\n\nfrom trefoil.geometry.bbox import BBox\nfrom trefoil.utilities.window import Window\nfrom trefoil.netcdf.utilities import get_ncattrs\nfrom trefoil.netcdf.crs import PROJ4_GEOGRAPHIC\n\n\nclass CoordinateVariable(object):\n    """"""\n    Wraps a one-dimensional variable with the same name as a dimension\n    (http://www.unidata.ucar.edu/software/netcdf/docs/BestPractices.html).\n    """"""\n\n    def __init__(self, input):\n        """"""\n        A Coordinate Variable can be created from a netCDF dataset variable or a numpy array.\n\n        :param input: variable in a netCDF dataset or a numpy array\n        """"""\n\n        self._ncattrs = dict()\n\n        if isinstance(input, Variable):\n            self.values = input[:]\n            for attr in input.ncattrs():\n                if not attr == \'_FillValue\':\n                    self._ncattrs[attr] = input.getncattr(attr)\n        else:\n            self.values = input[:].copy()\n\n    def __len__(self):\n        return self.values.shape[0]\n\n    def is_ascending_order(self):\n        return self.values[0] < self.values[1]\n\n    def indices_for_range(self, start, stop):\n        """"""\n        Returns the indices in this variable for the start and stop values\n        :param start: start value\n        :param stop: stop value\n        :return: start and stop indices\n        """"""\n\n        assert stop > start\n\n        if start > self.values.max():\n            return self.values.size - 1, self.values.size - 1\n        elif stop < self.values.min():\n            return 0, 0\n\n        if self.is_ascending_order():\n            start_index = min(self.values.searchsorted(start), self.values.size - 1)\n\n            # Need to move 1 index to the left unless we matched an index closely (allowing for precision errors)\n            if start_index > 0 and not numpy.isclose(start, self.values[start_index]):\n                start_index -= 1\n\n            stop_index = min(self.values.searchsorted(stop), self.values.size - 1)\n            if not numpy.isclose(stop, self.values[stop_index]) and stop < self.values[stop_index]:\n                stop_index -= 1\n\n            return start_index, stop_index\n        else:\n            # If values are not ascending, they need to be reversed\n            temp = self.values[::-1]\n            start_index = min(temp.searchsorted(start), temp.size - 1)\n\n            if start_index > 0 and not numpy.isclose(start, temp[start_index]):\n                start_index -= 1\n\n            stop_index = min(temp.searchsorted(stop), temp.size - 1)\n            if not numpy.isclose(stop, temp[stop_index]) and stop < temp[stop_index]:\n                stop_index -= 1\n\n            size = self.values.size - 1\n            return max(size - stop_index, 0), max(size - start_index, 0)\n\n    def slice_by_range(self, start, stop):\n        """"""\n        Slices a subset of values between start and stop values.\n\n        :param start: start value\n        :param stop: stop value\n        :return: sliced view of self.values.  Make sure to copy this before altering it!\n        """"""\n        assert stop > start\n        if start >= self.values.max() or stop <= self.values.min():\n            return numpy.array([])\n\n        start_index, stop_index = self.indices_for_range(start, stop)\n        return self.values[start_index:stop_index+1]\n\n    def add_to_dataset(self, dataset, name, is_unlimited=False, **kwargs):\n        """"""\n        :param dataset: name of the dataset to add the dimension and variable to\n        :param name: name of the dimension and variable\n        :param is_unlimited: set the dimension as unlimited\n        :param kwargs: creation options for output variable.  Should be limited to compression info.\n        :return: the newly created variable\n        """"""\n\n        if name in dataset.variables:\n            raise Exception(""Variable already exists in dataset"")\n\n        if name in dataset.dimensions:\n            dimension = dataset.dimensions[name]\n            if is_unlimited != dimension.isunlimited() or len(self) != len(dimension):\n                raise Exception(""Dimension already exists in dataset, but has different size"")\n        else:\n            dimension_length = None if is_unlimited else len(self)\n            dataset.createDimension(name, dimension_length)\n\n        if \'fill_value\' not in kwargs:\n            fill_value = getattr(self.values, \'fill_value\', None)\n            if fill_value is not None:\n                kwargs[\'fill_value\'] = fill_value\n\n        if self.values.dtype.char == \'S\':\n            variable = dataset.createVariable(name, \'string\', (name,), **kwargs)\n            # Have to write each index at a time, and cast to string.  Not optimal but seems to be the only way allowed by netCDF4.\n            for index, value in enumerate(self.values):\n                variable[index] = str(value)\n        else:\n            variable = dataset.createVariable(name, self.values.dtype, (name,), **kwargs)\n            variable[:] = self.values[:]\n\n        for att, value in six.iteritems(self._ncattrs):\n            variable.setncattr(att, value)\n\n        return variable\n\n\nclass BoundsCoordinateVariable(CoordinateVariable):\n    """"""\n    Wraps a two-dimensional variable representing bounds.  Shape is always (N, 2).\n\n    Useful for representing time ranges, etc.\n\n    Example: http://www.cgd.ucar.edu/cms/eaton/netcdf/CF-20010629.htm#grid_ex4\n    """"""\n\n    def is_ascending_order(self):\n        return self.values[0][0] < self.values[1][0]\n\n    def indices_for_range(self, start, stop):\n        raise NotImplementedError(""Not yet implemented"")\n\n    def add_to_dataset(self, dataset, name, is_unlimited=False, **kwargs):\n        """"""\n        :param dataset: name of the dataset to add the dimension and variable to\n        :param name: name of the dimension and variable.  Note: a new dimension for the bounds \'_bnds\' will be created.\n        :param is_unlimited: set the dimension as unlimited\n        :param kwargs: creation options for output variable.  Should be limited to compression info.\n        :return: the newly created variable\n        """"""\n\n        if name in dataset.variables:\n            raise Exception(""Variable already exists in dataset"")\n\n        bounds_dimension_name = \'_bnds\'\n        if bounds_dimension_name in dataset.dimensions:\n            if len(dataset.dimensions[bounds_dimension_name]) != 2:\n                raise ValueError(\'Bounds dimension _bnds is already present in dataset and not of size 2\')\n        else:\n            dataset.createDimension(bounds_dimension_name, 2)\n\n        if name in dataset.dimensions:\n            dimension = dataset.dimensions[name]\n            if is_unlimited != dimension.isunlimited() or len(self) != len(dimension):\n                raise Exception(""Dimension already exists in dataset, but has different size"")\n        else:\n            dimension_length = None if is_unlimited else len(self)\n            dataset.createDimension(name, dimension_length)\n\n        fill_value = getattr(self.values, \'fill_value\', None)\n        if fill_value is not None:\n            kwargs[\'fill_value\'] = fill_value\n\n        variable = dataset.createVariable(name, self.values.dtype, (name,bounds_dimension_name), **kwargs)\n        variable[:] = self.values[:]\n\n        for att, value in six.iteritems(self._ncattrs):\n            variable.setncattr(att, value)\n\n        return variable\n\n\nclass SpatialCoordinateVariable(CoordinateVariable):\n    """"""\n    Abstracts properties for a given spatial dimension (e.g., longitude).\n    Assumes that pixels follow a regular grid, and that dimension values represent centroids\n    """"""\n\n    @property\n    def min(self):\n        return self.values.min()\n\n    @property\n    def max(self):\n        return self.values.max()\n\n    @property\n    def pixel_size(self):\n        return float(abs(self.values[1] - self.values[0]))\n\n    @property\n    def edges(self):\n        """"""\n        Return coordinates of pixel edges from the min to the max\n        """"""\n\n        pixel_size = self.pixel_size\n\n        if self.is_ascending_order():\n            temp = numpy.append(self.values, self.values[-1] + pixel_size)\n        else:\n            temp = numpy.append(self.values[0] + pixel_size, self.values)\n        return temp - (pixel_size / 2.0)\n\n    def get_offset_for_subset(self, coordinate_variable):\n        """"""\n        Find the offset index of coordinate_variable within this coordinate variable.\n        This assumes that coordinate_variable is a subset of this one, and that coordinates and projections match.\n        """"""\n\n        assert len(coordinate_variable) <= self.values.shape[0]\n        #TODO: make this a fuzzy match within a certain decimal precision\n        return list(self.values).index(coordinate_variable.values[0])\n\n\nclass SpatialCoordinateVariables(object):\n    """"""\n    Encapsulates x and y coordinates with projection information\n    """"""\n\n    def __init__(self, x, y, projection):\n        assert isinstance(x, SpatialCoordinateVariable)\n        assert isinstance(y, SpatialCoordinateVariable)\n        if projection is not None:\n            assert isinstance(projection, Proj)\n\n        self.x = x\n        self.y = y\n        self.projection = projection\n\n    @property\n    def shape(self):\n        return (len(self.y), len(self.x))\n\n    @property\n    def bbox(self):\n\n        half_x_pixel_size = self.x.pixel_size / 2.0\n        half_y_pixel_size = self.y.pixel_size / 2.0\n\n        return BBox(\n            (\n                self.x.min - half_x_pixel_size,\n                self.y.min - half_y_pixel_size,\n                self.x.max + half_x_pixel_size,\n                self.y.max + half_y_pixel_size\n            ),\n            self.projection\n        )\n\n    @property\n    def affine(self):\n        bbox = self.bbox\n\n        return Affine(\n            self.x.pixel_size,\n            0,  # Not used\n            bbox.xmin,\n            0,  # Not used\n            self.y.values[1] - self.y.values[0],  # Negative if y is descending\n            bbox.ymin if self.y.is_ascending_order() else bbox.ymax\n        )\n\n    @classmethod\n    def from_dataset(cls, dataset, x_name=\'longitude\', y_name=\'latitude\', projection=None):\n        """"""\n        Return a SpatialCoordinateVariables object for a dataset\n\n        :param dataset: netCDF dataset\n        :param x_varname: name of the x dimension\n        :param y_varname: name of the y dimension\n        :param projection: pyproj Proj object\n        :return: CoordinateVariables instance\n        """"""\n\n        #TODO: detect x and y names, and projection\n        if projection is None and x_name == \'longitude\':\n            projection = Proj(PROJ4_GEOGRAPHIC)\n\n\n        return cls(\n            SpatialCoordinateVariable(dataset.variables[x_name]),\n            SpatialCoordinateVariable(dataset.variables[y_name]),\n            projection\n        )\n\n    @staticmethod\n    def from_bbox(bbox, x_size, y_size, dtype=\'float32\', y_ascending=False):\n        """"""\n        Return a SpatialCoordinateVariables object from BBox and dimensions\n\n        :param bbox: instance of a BBox, must have a projection\n        :param x_size: number of pixels in x dimension (width or number of columns)\n        :param y_size: number of pixels in y dimension (height or number of rows)\n        :param dtype: data type (string or numpy dtype object) of values\n        :param y_ascending: by default, y values are anchored from top left and are descending; if True, this inverts that order\n        :return: CoordinateVariables instance, assuming that rows are ordered in decreasing value\n        """"""\n\n        assert isinstance(bbox, BBox)\n        if not bbox.projection:\n            raise ValueError(\'bbox projection must be defined\')\n\n        x_pixel_size = (bbox.xmax - bbox.xmin) / float(x_size)\n        y_pixel_size = (bbox.ymax - bbox.ymin) / float(y_size)\n\n        x_arr = numpy.arange(x_size, dtype=dtype)\n        x_arr *= x_pixel_size\n        x_arr += (bbox.xmin + x_pixel_size / 2.0)\n\n        if y_ascending:\n            y_arr = numpy.arange(y_size, dtype=dtype)\n            y_arr *= y_pixel_size\n            y_arr += (bbox.ymin + y_pixel_size / 2.0)\n\n        else:\n            y_arr = numpy.arange(0, -y_size, -1, dtype=dtype)\n            y_arr *= y_pixel_size\n            y_arr += (bbox.ymax - y_pixel_size / 2.0)\n\n        x = SpatialCoordinateVariable(x_arr)\n        y = SpatialCoordinateVariable(y_arr)\n\n        return SpatialCoordinateVariables(x, y, bbox.projection)\n\n    def add_to_dataset(self, dataset, x_name, y_name, **kwargs):\n        x_var = self.x.add_to_dataset(dataset, x_name, **kwargs)\n        y_var = self.y.add_to_dataset(dataset, y_name, **kwargs)\n\n        x_var.setncattr(\'axis\', \'X\')\n        y_var.setncattr(\'axis\', \'Y\')\n\n        if self.projection:\n            if self.projection.is_latlong():\n                x_var.setncattr(\'standard_name\', \'longitude\')\n                x_var.setncattr(\'long_name\', \'longitude\')\n                x_var.setncattr(\'units\', \'degrees_east\')\n                y_var.setncattr(\'standard_name\', \'latitude\')\n                y_var.setncattr(\'long_name\', \'latitude\')\n                y_var.setncattr(\'units\', \'degrees_north\')\n\n            else:\n                x_var.setncattr(\'standard_name\', \'projection_x_coordinate\')\n                x_var.setncattr(\'long_name\', \'x coordinate of projection\')\n                y_var.setncattr(\'standard_name\', \'projection_y_coordinate\')\n                y_var.setncattr(\'long_name\', \'y coordinate of projection\')\n\n\n    def slice_by_bbox(self, bbox):\n        assert isinstance(bbox, BBox)\n\n        x_half_pixel_size = float(self.x.pixel_size)/2\n        y_half_pixel_size = float(self.y.pixel_size)/2\n\n        # Note: this is very sensitive to decimal precision.\n        x = SpatialCoordinateVariable(\n            self.x.slice_by_range(bbox.xmin + x_half_pixel_size, bbox.xmax - x_half_pixel_size)\n        )\n        y = SpatialCoordinateVariable(\n            self.y.slice_by_range(bbox.ymin + y_half_pixel_size, bbox.ymax - y_half_pixel_size)\n        )\n        return SpatialCoordinateVariables(x, y, self.projection)\n\n    def slice_by_window(self, window):\n        assert isinstance(window, Window)\n\n        x = SpatialCoordinateVariable(self.x.values[window.x_slice])\n        y = SpatialCoordinateVariable(self.y.values[window.y_slice])\n        return SpatialCoordinateVariables(x, y, self.projection)\n\n    def get_window_for_subset(self, subset_coordinates):\n        """"""\n        return a Window representing offsets of subset_coordinates self within subset_coordinates.\n        Assumed to be in same projection, etc.\n\n        :param subset_coordinates: the coordinates of the subset within self\n        """"""\n\n        assert isinstance(subset_coordinates, SpatialCoordinateVariables)\n\n        y_offset = self.y.get_offset_for_subset(subset_coordinates.y)\n        x_offset = self.x.get_offset_for_subset(subset_coordinates.x)\n        return Window((y_offset, len(subset_coordinates.y) + y_offset),\n                      (x_offset, len(subset_coordinates.x) + x_offset))\n\n    def get_window_for_bbox(self, bbox):\n        """"""\n        return a Window representing offsets of bbox within self\n        :param bbox: instance of bounding box representing coordinates to use for Window\n        :return: Window instance to extract data from within coordinate range of self\n        """"""\n\n        assert isinstance(bbox, BBox)\n\n        y_half_pixel_size = float(self.y.pixel_size)/2\n        x_half_pixel_size = float(self.x.pixel_size)/2\n\n        y_offset, y_max = self.y.indices_for_range(bbox.ymin + y_half_pixel_size, bbox.ymax - y_half_pixel_size)\n        x_offset, x_max =  self.x.indices_for_range(bbox.xmin + x_half_pixel_size, bbox.xmax - x_half_pixel_size)\n        return Window((y_offset, y_max + 1), (x_offset, x_max + 1))\n\n\nclass DateVariable(CoordinateVariable):\n    """"""\n    Provides utility wrapper of a date variable, especially when stored according to CF convention.\n    If variable conforms to CF convention pattern (has units with \'since\' in label and calendar) then\n    dates are extracted and converted to python date objects.\n\n    Dates are assumed to be sorted in ascending order.\n    """"""\n\n    def __init__(self, input, units_start_date=date(2000, 1, 1), calendar=\'360_day\'):\n        """"""\n        Create from a variable with CF Convention units and calendar, or\n        an array of years.\n\n        If created from years, values are recorded on the first day of the month\n        for each year, and are exported using units of days (years not allowed\n        by CF convention.  Lame).\n        """"""\n\n        assert calendar in (\'360_day\', \'gregorian\', \'standard\', \'julian\', \'360\', \'noleap\')\n\n        super(DateVariable, self).__init__(input)\n\n        if isinstance(input, Variable):\n            attributes = get_ncattrs(input)\n            self.units = attributes.get(\'units\', \'\').lower()\n            self.calendar = attributes.get(\'calendar\', \'\').lower()\n            if self.units and self.calendar and \'since\' in self.units.lower():\n                self.dates = num2date(self.values, self.units, self.calendar)\n            elif (self.units and \'year\' in self.units) or \'year\' in input._name.lower():\n                self.dates = numpy.array([datetime(y, 1, 1, tzinfo=pytz.UTC) for y in self.values.astype(\'int\')])\n            else:\n                raise ValueError(\'Variable is missing required attributes: units, calendar\')\n        else:\n            self.units = \'year\' if self.unit == \'year\' else \'{0}s since {1}\'.format(self.unit, str(units_start_date))\n            self.calendar = calendar\n\n            if self.values.dtype.kind in (\'i\', \'u\', \'f\'):\n                self.dates = numpy.array([datetime(y, 1, 1) for y in self.values])\n            elif isinstance(self.values[0], datetime):\n                self.dates = self.values.copy()\n\n            if self.unit == \'year\':\n                self.values = numpy.array([x.year for x in self.values], dtype=\'int32\')\n            else:\n                self.values = numpy.array(\n                    date2num(self.dates, units=self.units, calendar=self.calendar), dtype=numpy.int32\n                )\n\n    @property\n    def datetimes(self):\n        """"""\n        Convert to python datetimes if not done automatically (calendar not compatible with python datetimes).\n        Use with caution\n        """"""\n\n        if isinstance(self.dates[0], datetime):\n            return self.dates\n        else:\n            return numpy.array([datetime(*d.timetuple()[:6], tzinfo=pytz.UTC) for d in self.dates])\n\n    @property\n    def unit(self):\n        def varies_by_year(x, y):\n            if y.year == x.year or (y - x).seconds != 0 or x.month != y.month or x.day != y.day:\n                return False\n\n            return True\n\n        def varies_by_month(x, y):\n            if x.month == y.month or (y - x).seconds != 0 or x.day != y.day:\n                return False\n\n            return True\n\n        datetimes = self.datetimes if not self.values.dtype == datetime else self.values\n\n        if all(varies_by_year(datetimes[i], datetimes[i-1]) for i in range(1, len(datetimes))):\n            return \'year\'\n        elif all(varies_by_month(datetimes[i], datetimes[i-1]) for i in range(1, len(datetimes))):\n            return \'month\'\n\n        deltas = datetimes[1:] - datetimes[:-1]\n\n        for unit, seconds in ((\'day\', 86400), (\'hour\', 3600), (\'minute\', 60), (\'second\', 1)):\n            if any(x.seconds % seconds != 0 for x in deltas):\n                continue\n            break\n\n        return unit\n\n    def add_to_dataset(self, dataset, name, **kwargs):\n        variable = super(DateVariable, self).add_to_dataset(dataset, name, **kwargs)\n        for att in (\'units\', \'calendar\'):\n            variable.setncattr(att, getattr(self, att))\n\n    def indices_for_range(self, start, stop):\n        """"""\n        Returns the indices in this variable for the start and stop values.  Data must be in ascending order\n        :param start: start value.  Can be a date object or a year.\n        :param stop: stop value.  Can be a date object or a year.\n        :return: start and stop indices\n        """"""\n\n        if not self.is_ascending_order():\n            raise ValueError(""Dates must be in ascending order"")\n\n        if not isinstance(start, date):\n            start = date(start, 1, 1)\n\n        if not isinstance(stop, date):\n            stop = date(stop, 12, 31)\n\n        return numpy.searchsorted(self.dates, start), numpy.searchsorted(self.dates, stop)\n'"
trefoil/netcdf/warp.py,0,"b'import numpy\nfrom pyproj import Proj\nimport click\nimport rasterio\nfrom rasterio.crs import CRS\nfrom rasterio.warp import reproject\nfrom rasterio.enums import Resampling\n\nfrom trefoil.netcdf.crs import get_crs\nfrom trefoil.netcdf.utilities import copy_variable, copy_dimension, get_fill_value\nfrom trefoil.netcdf.variable import SpatialCoordinateVariables\n\n\ndef warp_array(\n    arr,\n    src_crs,\n    src_transform,\n    dst_crs,\n    dst_transform,\n    dst_shape,\n    resampling=Resampling.nearest):\n\n    """"""\n    Warp a 2D array using rasterio, always returning a masked array.\n\n    A fill_value will be chosen from the array\'s data type if the input is not a masked array (beware conflicts with\n    valid values!)\n\n    All nodata values are filled in prior to warping, and masked back out later if necessary.\n\n    :param dst_shape: shape of destination array\n\n    All other parameters are the same as for rasterio.warp.reproject\n    """"""\n\n    with rasterio.Env():\n        orig_dtype = arr.dtype\n        if arr.dtype == numpy.int8:\n            # Have to upcast for rasterio\n            arr = arr.astype(\'int16\')\n\n        out = numpy.empty(shape=dst_shape, dtype=arr.dtype)\n\n        fill = get_fill_value(arr.dtype)\n        if hasattr(arr, \'fill_value\'):\n            fill = arr.fill_value\n            arr = numpy.ma.filled(arr, arr.fill_value)\n\n        reproject(\n            arr,\n            out,\n            src_crs=src_crs,\n            src_transform=src_transform,\n            dst_crs=dst_crs,\n            dst_transform=dst_transform,\n            resampling=resampling,\n            src_nodata=fill,\n            dst_nodata=fill\n        )\n\n        if out.dtype != orig_dtype:\n            out = out.astype(orig_dtype)\n\n        out = numpy.ma.masked_array(out, mask=out == fill)\n\n        return out\n\n\n\ndef warp_like(ds, ds_projection, variables, out_ds, template_ds, template_varname, resampling=Resampling.nearest):\n    """"""\n    Warp one or more variables in a NetCDF file based on the coordinate reference system and\n    spatial domain of a template NetCDF file.\n    :param ds: source dataset\n    :param ds_projection: source dataset coordiante reference system, proj4 string or EPSG:NNNN code\n    :param variables: list of variable names in source dataset to warp\n    :param out_ds: output dataset.  Must be opened in write or append mode.\n    :param template_ds: template dataset\n    :param template_varname: variable name for template data variable in template dataset\n    :param resampling: resampling method.  See rasterio.enums.Resampling for options\n    """"""\n\n    template_variable = template_ds.variables[template_varname]\n    template_prj = Proj(get_crs(template_ds, template_varname))\n    template_mask = template_variable[:].mask\n\n    template_y_name, template_x_name = template_variable.dimensions[-2:]\n    template_coords = SpatialCoordinateVariables.from_dataset(\n        template_ds,\n        x_name=template_x_name,\n        y_name=template_y_name,\n        projection=template_prj\n    )\n    # template_geo_bbox = template_coords.bbox.project(ds_prj, edge_points=21)  # TODO: add when needing to subset\n\n    ds_y_name, ds_x_name = ds.variables[variables[0]].dimensions[-2:]\n    proj = Proj(init=ds_projection) if \'EPSG:\' in ds_projection.upper() else Proj(str(ds_projection))\n    ds_coords = SpatialCoordinateVariables.from_dataset(ds, x_name=ds_x_name, y_name=ds_y_name, projection=proj)\n\n    with rasterio.Env():\n        # Copy dimensions for variable across to output\n        for dim_name in template_variable.dimensions:\n            if not dim_name in out_ds.dimensions:\n                if dim_name in template_ds.variables and not dim_name in out_ds.variables:\n                    copy_variable(template_ds, out_ds, dim_name)\n                else:\n                    copy_dimension(template_ds, out_ds, dim_name)\n\n        for variable_name in variables:\n            click.echo(\'Processing: {0}\'.format(variable_name))\n\n            variable = ds.variables[variable_name]\n            fill_value = getattr(variable, \'_FillValue\', variable[0, 0].fill_value)\n\n            for dim_name in variable.dimensions[:-2]:\n                if not dim_name in out_ds.dimensions:\n                    if dim_name in ds.variables:\n                        copy_variable(ds, out_ds, dim_name)\n                    else:\n                        copy_dimension(ds, out_ds, dim_name)\n\n            out_var = out_ds.createVariable(\n                variable_name,\n                variable.dtype,\n                dimensions=variable.dimensions[:-2] + template_variable.dimensions,\n                fill_value=fill_value\n            )\n\n            reproject_kwargs = {\n                \'src_transform\': ds_coords.affine,\n                \'src_crs\': CRS.from_string(ds_projection),\n                \'dst_transform\': template_coords.affine,\n                \'dst_crs\': template_prj.srs,\n                \'resampling\': resampling,\n                \'src_nodata\': fill_value,\n                \'dst_nodata\': fill_value,\n                \'threads\': 4\n            }\n\n            # TODO: may only need to select out what is in window\n\n            if len(variable.shape) == 3:\n                idxs = range(variable.shape[0])\n                with click.progressbar(idxs) as bar:\n                    for i in bar:\n                        # print(\'processing slice: {0}\'.format(i))\n\n                        data = variable[i, :]\n                        out = numpy.ma.empty(template_coords.shape, dtype=data.dtype)\n                        out.mask = template_mask\n                        out.fill(fill_value)\n                        reproject(data, out, **reproject_kwargs)\n                        out_var[i, :] = out\n\n            else:\n                data = variable[:]\n                out = numpy.ma.empty(template_coords.shape, dtype=data.dtype)\n                out.mask = template_mask\n                out.fill(fill_value)\n                reproject(data, out, **reproject_kwargs)\n                out_var[:] = out\n'"
trefoil/render/__init__.py,0,b''
trefoil/utilities/__init__.py,0,b''
trefoil/utilities/color.py,0,"b'import bisect\r\nimport colorsys\r\nimport numpy\r\nimport six\r\n\r\n#TODO: cleanup usage\r\nNUM_COLORS = 255\r\n\r\nclass Color(object):\r\n    """"""\r\n    convenience class that represents integer colors of 8, 16, 32 bits each\r\n    """"""\r\n\r\n    def __init__(self, red, green, blue, alpha=None, bits=8):\r\n        assert isinstance(red, int) and isinstance(green, int) and isinstance(blue, int)\r\n        assert bits in (8, 16, 32)\r\n        if alpha is not None:\r\n            assert isinstance(alpha, int)\r\n\r\n        self.red = red\r\n        self.green = green\r\n        self.blue = blue\r\n        self.alpha = alpha\r\n        self._has_alpha = alpha is not None\r\n        self.bits = 8\r\n\r\n    def __repr__(self):\r\n        return str(self)\r\n\r\n    def __str__(self):\r\n        return str(self.to_tuple())\r\n\r\n    def to_tuple(self):\r\n        values = [self.red, self.green, self.blue]\r\n        if self._has_alpha:\r\n            values.append(self.alpha)\r\n        return tuple(values)\r\n\r\n    def to_hex(self):\r\n        s = \'\'.join(\'{:02x}\'.format(x) for x in (self.red, self.green, self.blue))\r\n\r\n        # Use short form if possible\r\n        if all((s[i] == s[i+1] for i in (0, 2, 4))):\r\n            s = \'\'.join((s[0], s[2], s[4]))\r\n\r\n        return \'#{0}\'.format(s).upper()\r\n\r\n    def to_float(self):\r\n        factor = 1.0 / float(2**self.bits - 1)\r\n        values = [getattr(self,x) * factor for x in (\'red\', \'green\',\'blue\')]\r\n        if self._has_alpha:\r\n            values.append(self.alpha * factor)\r\n        return tuple(values)\r\n\r\n    def to_hsv(self):\r\n        """"""\r\n        Calculate and return HSV values as integer values\r\n        """"""\r\n\r\n        h, s, v = colorsys.rgb_to_hsv(*self.to_float()[:3])\r\n        values = [\r\n            int(round(h * 360)),\r\n            int(round(s * 100)),\r\n            int(round(v * 100)),\r\n        ]\r\n        if self._has_alpha:\r\n            values.append(self.alpha)\r\n        return tuple(values)\r\n\r\n    @classmethod\r\n    def from_hsv(cls, hue, saturation, value, alpha=None, bits=8):\r\n        """"""\r\n        Construct new Color instance from HSV integers\r\n        """"""\r\n\r\n        assert isinstance(hue, int) and isinstance(saturation, int) and isinstance(value, int)\r\n        assert bits in (8, 16, 32)\r\n        if alpha is not None:\r\n            assert isinstance(alpha, int)\r\n\r\n        #Convert to and from float used by colorsys functions\r\n        red, green, blue = [int(round(x * (2**bits - 1), 0)) for x\r\n                            in colorsys.hsv_to_rgb(float(hue) / 360.0, float(saturation) / 100.0, float(value) / 100.0)]\r\n        return Color(red, green, blue, alpha=alpha, bits=bits)\r\n\r\n    @classmethod\r\n    def from_hex(cls, value, alpha=None):\r\n        try:\r\n            if value[0] == \'#\':\r\n                value = value[1:]\r\n            if len(value) == 3:\r\n                value = \'\'.join([c*2 for c in value])\r\n            if len(value) == 6:\r\n                value = ""{0}{1:02X}"".format(value, alpha if alpha is not None else 255)\r\n            if len(value) != 8:\r\n                raise ValueError\r\n\r\n            color = []\r\n            for i in range(0, 8, 2):\r\n                color.append(int(value[i:i+2], 16))\r\n\r\n            return cls(*color)\r\n\r\n        except ValueError:\r\n            raise ValueError(""Invalid hex color: {}"".format(value))\r\n\r\n\r\n\r\ndef rgb_to_hsv(colors):\r\n    """"""\r\n    Convert array of 8-bit unsigned RGB colors to floating point HSV.\r\n\r\n    Expected input: [(r,g,b)...]\r\n\r\n    Derived from matplotlib.colors::rgb_to_hsv\r\n    """"""\r\n\r\n    if not isinstance(colors, numpy.ndarray):\r\n        colors = numpy.asarray(colors).astype(numpy.uint8)\r\n\r\n    assert len(colors.shape) == 2 and colors.shape[-1] == 3\r\n    assert colors.dtype == numpy.uint8\r\n\r\n    colors = colors / 255.0\r\n    hsv = numpy.zeros_like(colors)\r\n    vmax = colors.max(-1)\r\n    vrange = colors.ptp(-1)\r\n    vmax_ge_zero = vmax > 0\r\n    s = numpy.zeros_like(vmax)\r\n    s[vmax_ge_zero] = vrange[vmax_ge_zero] / vmax[vmax_ge_zero]\r\n\r\n    vrange_ge_zero = vrange > 0\r\n    # red is max\r\n    idx = (colors[..., 0] == vmax) & vrange_ge_zero\r\n    hsv[idx, 0] = (colors[idx, 1] - colors[idx, 2]) / vrange[idx]\r\n    # green is max\r\n    idx = (colors[..., 1] == vmax) & vrange_ge_zero\r\n    hsv[idx, 0] = 2. + (colors[idx, 2] - colors[idx, 0]) / vrange[idx]\r\n    # blue is max\r\n    idx = (colors[..., 2] == vmax) & vrange_ge_zero\r\n    hsv[idx, 0] = 4. + (colors[idx, 0] - colors[idx, 1]) / vrange[idx]\r\n\r\n    hsv[..., 0] = (hsv[..., 0] / 6.0) % 1.0\r\n    hsv[..., 1] = s\r\n    hsv[..., 2] = vmax\r\n    return hsv\r\n\r\n\r\ndef hsv_to_rgb(colors):\r\n    """"""\r\n    Convert array of floating point HSV to 8-bit unsigned RGB colors.\r\n\r\n    Expected input [(h,s,v)...]\r\n\r\n    Derived from matplotlib.colors::hsv_to_rgb\r\n    """"""\r\n\r\n    if not isinstance(colors, numpy.ndarray):\r\n        colors = numpy.asarray(colors)\r\n\r\n    assert len(colors.shape) == 2 and colors.shape[-1] == 3\r\n    assert colors.dtype.kind == \'f\'\r\n\r\n    h, s, v = colors.T\r\n    rgb = numpy.zeros_like(colors)\r\n    r, g, b = rgb.T\r\n\r\n    i = (h * 6.0).astype(numpy.int)\r\n    f = (h * 6.0) - i\r\n    p = v * (1.0 - s)\r\n    q = v * (1.0 - s * f)\r\n    t = v * (1.0 - s * (1.0 - f))\r\n\r\n    idx = i % 6 == 0\r\n    r[idx] = v[idx]\r\n    g[idx] = t[idx]\r\n    b[idx] = p[idx]\r\n\r\n    idx = i == 1\r\n    r[idx] = q[idx]\r\n    g[idx] = v[idx]\r\n    b[idx] = p[idx]\r\n\r\n    idx = i == 2\r\n    r[idx] = p[idx]\r\n    g[idx] = v[idx]\r\n    b[idx] = t[idx]\r\n\r\n    idx = i == 3\r\n    r[idx] = p[idx]\r\n    g[idx] = q[idx]\r\n    b[idx] = v[idx]\r\n\r\n    idx = i == 4\r\n    r[idx] = t[idx]\r\n    g[idx] = p[idx]\r\n    b[idx] = v[idx]\r\n\r\n    idx = i == 5\r\n    r[idx] = v[idx]\r\n    g[idx] = p[idx]\r\n    b[idx] = q[idx]\r\n\r\n    idx = s == 0\r\n    r[idx] = v[idx]\r\n    g[idx] = v[idx]\r\n    b[idx] = v[idx]\r\n\r\n    return (rgb * 255).astype(numpy.uint8)\r\n\r\n\r\ndef interpolate_linear(colors, values, num_colors, colorspace=""hsv""):\r\n    """"""\r\n    Interpolates colors based on the positions of values.\r\n\r\n    :param colors: the numpy array (must be uint8) of colors to interpolate between.\r\n    :param values: the values that correspond to the colors (order must match).  Used to determine the position of each color w/in interpolation.\r\n    :param num_colors: number of new colors to create.\r\n    :param colorspace: hsv or rgb, determines the colorspace of the interpolation method\r\n    """"""\r\n\r\n    if not isinstance(colors, numpy.ndarray):\r\n        colors = numpy.asarray(colors).astype(numpy.uint8)\r\n\r\n    assert len(colors.shape) == 2\r\n    assert colors.shape[0] > 1\r\n    assert len(colors) == len(values)\r\n    assert colors.dtype == numpy.uint8\r\n\r\n    min_value = min(values)\r\n    value_range = max(values) - min_value\r\n    if value_range == 0:\r\n        factor = 1.0\r\n    else:\r\n        factor = float(num_colors-1) / value_range\r\n\r\n    target_x = numpy.arange(0, num_colors)\r\n    x = []\r\n    for value in values:\r\n        x.append((value - min_value) * factor)\r\n\r\n    if colorspace == ""rgb"":\r\n        src_colors = colors.T\r\n        target_colors = numpy.zeros((src_colors.shape[0], num_colors))\r\n        for i in range(0, target_colors.shape[0]):\r\n            target_colors[i] = numpy.interp(target_x, x, src_colors[i])\r\n        return target_colors.T.astype(numpy.uint8)\r\n    else:\r\n        hsv = rgb_to_hsv(colors[..., :3]).T\r\n\r\n        target_hsv = numpy.zeros((hsv.shape[0], num_colors))\r\n\r\n        # Interpolate saturation and value\r\n        for i in range(1, target_hsv.shape[0]):\r\n            target_hsv[i] = numpy.interp(target_x, x, hsv[i])\r\n\r\n        # Interpolate hue separately, since it has some special conditions\r\n        for i in six.moves.range(1, len(hsv[0])):\r\n            lo_h = hsv[0][i-1]\r\n            hi_h = hsv[0][i]\r\n            lo_s = hsv[1][i-1]\r\n            hi_s = hsv[1][1]\r\n            lo_x = x[i-1]\r\n            hi_x = x[i]\r\n            lo_idx = bisect.bisect_left(target_x, lo_x)\r\n            hi_idx = bisect.bisect_left(target_x, hi_x)\r\n\r\n            # Make sure we interpolate through the last position in palette\r\n            if hi_idx == len(target_x) - 1:\r\n                hi_idx = len(target_x)\r\n\r\n            # Avoid moving through other colors when ramping from or to a shade of grey.\r\n            if lo_s == 0:\r\n                lo_h = hi_h\r\n            elif hi_s == 0:\r\n                hi_h = lo_h\r\n\r\n            target_hsv[0][lo_idx:hi_idx] = numpy.interp(target_x[lo_idx:hi_idx], [lo_x, hi_x], [lo_h, hi_h])\r\n\r\n        if colors.shape[1] == 4:\r\n            r, g, b = hsv_to_rgb(target_hsv.T).T\r\n            a = numpy.interp(target_x, x, colors[..., 3].T).astype(numpy.uint8)\r\n            return numpy.vstack((r, g, b, a)).T.astype(numpy.uint8)\r\n        else:\r\n            return hsv_to_rgb(target_hsv.T).astype(numpy.uint8)\r\n'"
trefoil/utilities/conversion.py,0,"b'import rasterio\nfrom affine import Affine\n\n\ndef array_to_raster(data, outfilename=None, format=\'GTiff\', affine=Affine.identity(), projection=None):\n    """"""\n    Only GTiff driver supported at present.\n\n    Will implicitly overwrite existing output.\n    prj must be a pyproj.Proj object\n    """"""\n\n    if format != \'GTiff\':\n        raise NotImplementedError(\'Formats besides GTiff not yet supported\')\n\n    meta = {\n        \'width\': data.shape[1],\n        \'height\': data.shape[0],\n        \'dtype\': data.dtype.name, # rasterio uses strings, not dtypes\n        \'transform\': affine,\n        \'count\': 1,\n        \'crs\': projection.srs\n    }\n\n    with rasterio.Env():\n        with rasterio.open(outfilename, \'w\', driver=format, **meta) as out:\n            out.write(data, 1)\n'"
trefoil/utilities/format.py,0,"b'import numpy\r\n\r\nMAX_PRECISION = 6\r\n\r\nclass PrecisionFormatter(object):\r\n    """"""\r\n    Utility class to provide cleaner handling of decimal precision for string outputs\r\n    """"""\r\n\r\n    def __init__(self, values, max_precision=6):\r\n        """"""\r\n        Extract the maximum precision required to represent the precision of values.  Must be <= 6 (python truncates\r\n        beyond this point), and less than max_precision.\r\n        If input is an instance of a numpy array, uses numpy methods instead for better efficiency.\r\n        """"""\r\n\r\n        assert max_precision <= 6\r\n\r\n        self._precision = 0\r\n        decimal_strs = set([""{:g}"".format(float(x) - int(round(x))) for x in values])\r\n        if \'0\' in decimal_strs:\r\n            decimal_strs.remove(\'0\')\r\n        if decimal_strs:\r\n            self._precision = max([len(x) for x in decimal_strs]) - 2\r\n        if max_precision is not None:\r\n            self._precision = min(self._precision, max_precision)\r\n        self._precision = min(self._precision, MAX_PRECISION)\r\n\r\n    def format(self, value):\r\n        if self._precision == 0:\r\n            return str(int(round(float(value), 0)))\r\n        else:\r\n            return (""{:.%if}"" % self._precision).format(float(value)).rstrip(\'0\').rstrip(\'.\')\r\n'"
trefoil/utilities/image.py,0,"b'from PIL import Image, ImageDraw, ImageChops\r\n\r\nfrom trefoil.utilities.color import Color\r\n\r\n\r\ndef add_border(image, width, color):\r\n    if isinstance(color, Color):\r\n        color = color.to_tuple()\r\n    color=color[:3]\r\n\r\n    image_width, image_height = image.size\r\n    canvas = ImageDraw.Draw(image)\r\n    offset = int(round(width/2.0))\r\n    canvas.line(((0, 0), (0, image_height)), fill=color, width=width)\r\n    canvas.line(((0, 0), (image_width, 0)), fill=color, width=width)\r\n    canvas.line(((0, image_height-offset), (image_width, image_height-offset)), fill=color, width=width)\r\n    canvas.line(((image_width-offset, 0), (image_width-offset, image_height)), fill=color, width=width)\r\n\r\n\r\ndef autocrop(image, background=None):\r\n    """"""\r\n    Automatically crop to the bounds of non-background pixels.\r\n    """"""\r\n\r\n    # Derived from: http://stackoverflow.com/questions/10615901/trim-whitespace-using-pil\r\n\r\n    assert image.mode == \'RGBA\'\r\n\r\n    if not background:\r\n        background = (255, 255, 255, 0)\r\n\r\n    background_img = Image.new(""RGBA"", image.size, color=background)\r\n    diff = ImageChops.difference(image, background_img)\r\n    diff = ImageChops.add(diff, diff, 2.0, -100)\r\n    bbox = diff.getbbox()\r\n    if bbox:\r\n        return image.crop(bbox)\r\n\r\n    return image'"
trefoil/utilities/window.py,0,"b'class Window(object):\r\n    """"""\r\n    Encapsulates a window representing the y_offset, y_max, x_offset, x_max of spatial coordinates\r\n    from one dataset within another.\r\n    Both are assumed to be in the same projection, and one is expected to be a subset of the other\r\n    """"""\r\n\r\n    def __init__(self, y_slice, x_slice):\r\n        """"""\r\n        :param y_slice: slice or tuple of y_offset to y_max + 1\r\n        :param x_slice: slice or tuple of x_ffset to x_max + 1\r\n        """"""\r\n\r\n        if isinstance(y_slice, tuple):\r\n            y_slice = slice(*y_slice)\r\n        if isinstance(x_slice, tuple):\r\n            x_slice = slice(*x_slice)\r\n\r\n        self.y_slice = y_slice\r\n        self.x_slice = x_slice\r\n\r\n    def __str__(self):\r\n        return \'y: %s, x: %s\' % (self.y_slice, self.x_slice)\r\n\r\n    @property\r\n    def shape(self):\r\n        return (self.y_slice.stop - self.y_slice.start,\r\n                self.x_slice.stop - self.x_slice.start)\r\n\r\n    def clip(self, values, slices=None):\r\n        """"""\r\n        Returns a subset view of values within domain represented by this instance.\r\n\r\n        :param values: values to be clipped.  Last 2 dimensions must be row, col\r\n        :param slices: list of indices\r\n        :return: subset view of values clipped by self\r\n        """"""\r\n\r\n        if slices is None:\r\n            slices = []\r\n            if len(values.shape) > 2:\r\n                for s in values.shape[:-2]:\r\n                    slices.append(slice(0, s))\r\n\r\n        elif isinstance(slices, (list, tuple)):\r\n            slices = list(slices)\r\n        else:\r\n            slices = [slices]\r\n        slices.append(self.y_slice)\r\n        slices.append(self.x_slice)\r\n\r\n        if len(slices) != len(values.shape):\r\n            raise ValueError(""Dimensions of input does not match number of slices"")\r\n\r\n        return values[slices]'"
trefoil/analysis/tests/__init__.py,0,b''
trefoil/analysis/tests/test_summary.py,0,"b""import numpy\r\nfrom trefoil.analysis.summary import statistic_by_interval, calculate_zonal_statistics\r\nfrom trefoil.analysis.summary import VALID_ZONAL_STATISTICS\r\n\r\n\r\ndef test_sum_by_interval():\r\n    monthly = numpy.random.randint(0, 4, (24, 1, 1))\r\n    annual = statistic_by_interval(monthly, 12, 'sum')\r\n    flat_monthly = monthly.flatten()\r\n    midpoint = int(flat_monthly.shape[0] / 2)\r\n    assert numpy.array_equal(annual.flatten(), (flat_monthly[:midpoint].sum(), flat_monthly[midpoint:].sum()))\r\n\r\n\r\ndef test_calculate_zonal_stats_2d():\r\n    zones = numpy.zeros((10, 10), dtype='uint8')\r\n    zones[5:] = 1\r\n    zone_values = numpy.array([0, 1])\r\n    data = numpy.arange(1, 101, dtype='uint8').reshape((10, 10))\r\n\r\n    statistics = list(VALID_ZONAL_STATISTICS)\r\n\r\n    results = calculate_zonal_statistics(zones, zone_values, data, statistics)\r\n\r\n    print(results)\r\n    assert not set(zone_values).difference(results.keys())\r\n\r\n    assert not set(statistics).difference(results[0].keys())\r\n    assert not set(statistics).difference(results[1].keys())\r\n\r\n    for zone in zone_values:\r\n        result = results[zone]\r\n        truth = numpy.arange(zone * 50 + 1, zone * 50 + 51)\r\n        for statistic in statistics:\r\n            if statistic == 'count':\r\n                assert result[statistic] == truth.size\r\n            else:\r\n                assert result[statistic] == getattr(truth, statistic)()\r\n"""
trefoil/analysis/tests/test_timeseries.py,0,"b""import numpy\r\nfrom trefoil.analysis.timeseries import linear_regression\r\n\r\n\r\ndef test_linear_regression():\r\n    timesteps = numpy.arange(0, 3, dtype=numpy.uint8)\r\n    # Randomly generated data in advance\r\n    values = numpy.array([\r\n        [[0.29669283, 0.0388028], [0.64656131, 0.37802995]],\r\n        [[0.56178477, 0.38010095], [0.67501274, 0.72232721]],\r\n        [[1.04133795, 0.56589527], [0.73599531, 0.50754688]]\r\n    ])\r\n\r\n    slopes, intercepts = linear_regression(timesteps, values)\r\n\r\n    # Calculated in advance, at a time when this was checked against results from scipy\r\n    expected_slopes = numpy.array([[0.37232256, 0.26354623], [0.044717, 0.06475847]])\r\n    expected_intercepts = numpy.array([[0.26094929, 0.06472011],[0.64113945, 0.47120955]])\r\n\r\n    assert numpy.allclose(slopes, expected_slopes)\r\n    assert numpy.allclose(intercepts, expected_intercepts)\r\n\r\n\r\ndef test_linear_regression_full():\r\n    # Requires scipy, which isn't always available.\r\n    # Not good for coverage but scipy is hard to install correctly on travis-ci\r\n    try:\r\n        from scipy.stats import linregress\r\n\r\n        timesteps = numpy.arange(0, 100, dtype=numpy.uint8)\r\n        k = numpy.random.rand(100)\r\n        b = numpy.random.rand(k.shape[0])\r\n        # shape is (timesteps.shape, k.shape)\r\n        values = numpy.outer(timesteps, k) + b + numpy.random.normal(\r\n            size=(timesteps.shape[0], k.shape[0]), scale=0.1)\r\n        values = values.reshape(\r\n            (100, 10, 10))  # divide the second dimension into two parts\r\n\r\n        slopes, intercepts, r2vals, pvals = linear_regression(timesteps, values,\r\n                                                              full=True)\r\n\r\n        s, i, r, p = linregress(timesteps, values[:, 0, 0])[:4]\r\n        assert numpy.allclose((s, i, r ** 2, p),\r\n                              (slopes[0, 0], intercepts[0, 0], r2vals[0, 0],\r\n                               pvals[0, 0]))\r\n\r\n        s, i, r, p = linregress(timesteps, values[:, 2, 0])[:4]\r\n        assert numpy.allclose((s, i, r ** 2, p),\r\n                              (slopes[2, 0], intercepts[2, 0], r2vals[2, 0],\r\n                               pvals[2, 0]))\r\n\r\n    except ImportError:\r\n        print('WARNING: scipy not available for testing')\r\n"""
trefoil/geometry/tests/__init__.py,0,b''
trefoil/geometry/tests/test_bbox.py,0,"b'import numpy\r\nfrom pyproj import Proj\r\nfrom trefoil.geometry.bbox import BBox\r\nfrom rasterio.crs import CRS\r\n\r\n\r\nTEST_COORDS = (-124.75, 48.625, -124.375, 49.0)\r\nTEST_COORDS_PRJ = Proj(init=""EPSG:4326"")\r\n\r\n\r\ndef test_bbox():\r\n    bbox = BBox(TEST_COORDS, TEST_COORDS_PRJ)\r\n    assert bbox.xmin == TEST_COORDS[0]\r\n    assert bbox.ymin == TEST_COORDS[1]\r\n    assert bbox.xmax == TEST_COORDS[2]\r\n    assert bbox.ymax == TEST_COORDS[3]\r\n    assert bbox.projection.srs == TEST_COORDS_PRJ.srs\r\n\r\n\r\ndef test_bbox_local_projection():\r\n    bbox = BBox(TEST_COORDS, TEST_COORDS_PRJ)\r\n    out = CRS.from_string(bbox.get_local_albers_projection().srs)\r\n    expected = CRS.from_string(""+lon_0=-124.5625 +ellps=WGS84 +datum=WGS84 +y_0=0 +no_defs=True +proj=aea +x_0=0 +units=m +lat_2=48.9375 +lat_1=48.6875 +lat_0=0 "")\r\n    assert expected == out\r\n\r\n\r\ndef test_projection():\r\n    bbox = BBox(TEST_COORDS, TEST_COORDS_PRJ)\r\n    proj_bbox = bbox.project(Proj(init=""EPSG:3857""))\r\n    # Calculated by running this previously under controlled conditions.  No validation against truth of projection values.\r\n    assert numpy.allclose(\r\n        proj_bbox.as_list(),\r\n        [-13887106.476460878, 6211469.632719522, -13845361.6674134, 6274861.394006577]\r\n    )\r\n'"
trefoil/netcdf/tests/__init__.py,0,b''
trefoil/netcdf/tests/test_conversion.py,0,"b""import os\nfrom netCDF4 import Dataset\nfrom pyproj import Proj\nimport numpy\nimport rasterio\nfrom trefoil.netcdf.crs import PROJ4_GEOGRAPHIC\nfrom trefoil.netcdf.conversion import netcdf_to_raster\n\n\n# TODO: need 3d test file\n# TODO: need better test fixtures to fabricate data\n\ndef test_netcdf_to_raster(tmpdir):\n    outfilename = str(tmpdir.join('test.tif'))\n\n    dataset = Dataset('trefoil/test_data/tmin.nc')\n    varname = 'tmin'\n    y_name, x_name = dataset.variables[varname].dimensions[:2]\n\n    netcdf_to_raster(\n        dataset,\n        varname,\n        outfilename,\n        projection=Proj(PROJ4_GEOGRAPHIC)\n    )\n\n    assert os.path.exists(outfilename)\n\n    variable = dataset.variables[varname]\n    height, width = variable.shape[:2]\n    data = variable[:]\n\n    with rasterio.open(outfilename, 'r') as src:\n        assert src.count == 1\n        assert src.width == width\n        assert src.height == height\n        assert src.crs.to_string() == '+init=epsg:4326'\n        assert src.dtypes[0] == data.dtype\n        assert numpy.array_equal(data, src.read(1))\n"""
trefoil/netcdf/tests/test_crs.py,0,"b'import pytest\r\nfrom netCDF4 import Dataset\r\nfrom pyproj import Proj, pj_ellps\r\nfrom trefoil.netcdf.crs import get_crs, set_crs, is_geographic\r\nfrom trefoil.netcdf.utilities import get_ncattrs, set_ncattrs\r\nfrom rasterio.crs import CRS\r\n\r\nimport logging, sys\r\nlogging.basicConfig(stream=sys.stderr, level=logging.DEBUG)\r\n\r\n\r\ndef test_get_crs(tmpdir):\r\n    """""" Test reading proj4 string from CF convention parameters """"""\r\n\r\n    ds = Dataset(str(tmpdir.join(\'test.nc\')), \'w\')\r\n    data_var = ds.createVariable(\'data\', \'S1\')\r\n    data_var.setncattr(\'grid_mapping\', \'crs_Lambert\')\r\n    crs_var = ds.createVariable(\'crs_Lambert\', \'S1\')\r\n\r\n    in_proj4 = \'+proj=lcc +units=m +lat_1=30 +lat_2=60 +lat_0=47.5 +lon_0=-97 +x_0=3825000 +y_0=3200000\'\r\n\r\n    # These parameters match the above proj4 string\r\n    ncatts = dict()\r\n    ncatts[\'grid_mapping_name\'] = \'lambert_conformal_conic\'\r\n    ncatts[\'latitude_of_projection_origin\'] = 47.5\r\n    ncatts[\'longitude_of_central_meridian\'] = -97\r\n    ncatts[\'standard_parallel\'] = [30, 60]\r\n    ncatts[\'false_northing\'] = 3200000\r\n    ncatts[\'false_easting\'] = 3825000\r\n    set_ncattrs(crs_var, ncatts)\r\n\r\n    out_proj4 = get_crs(ds, \'data\')\r\n    assert out_proj4 is not None\r\n\r\n    out_data = CRS.from_string(out_proj4).to_dict()\r\n\r\n    assert len(out_data) == 8  # There should be 8 parameters\r\n    assert CRS.from_string(in_proj4).to_dict() == out_data\r\n\r\n    # Test WGS84 lat/long\r\n    data_var = ds.createVariable(\'data2\', \'S1\')\r\n    data_var.setncattr(\'grid_mapping\', \'crs_latlong\')\r\n    crs_var = ds.createVariable(\'crs_latlong\', \'S1\')\r\n\r\n    in_proj4 = \'+proj=latlong +a={0} +rf={1}\'.format(pj_ellps[\'WGS84\'][\'a\'], pj_ellps[\'WGS84\'][\'rf\'])\r\n\r\n    # These parameters match the above proj4 string\r\n    ncatts = dict()\r\n    ncatts[\'grid_mapping_name\'] = \'latitude_longitude\'\r\n    ncatts[\'semi_major_axis\'] = 6378137.0\r\n    ncatts[\'inverse_flattening\'] = 298.257223563\r\n    set_ncattrs(crs_var, ncatts)\r\n\r\n    out_proj4 = get_crs(ds, \'data2\')\r\n    assert out_proj4 is not None\r\n\r\n    out_data = CRS.from_string(out_proj4).to_dict()\r\n\r\n    assert len(out_data) == 4  # There should be 4 parameters\r\n    # Note: pyproj adds units=m even for latlong, which is incorrect but not our problem\r\n    assert CRS.from_string(in_proj4 + \' +units=m\').to_dict() == out_data\r\n\r\n\r\ndef test_set_crs(tmpdir):\r\n    """""" Test proper encoding of projection into CF Convention parameters """"""\r\n\r\n    ds = Dataset(str(tmpdir.join(\'test.nc\')), \'w\')\r\n\r\n    # Test polar stereographic\r\n    proj4 = \'+proj=stere +datum=WGS84 +lat_ts=60 +lat_0=90 +lon_0=263 +lat_1=60 +x_0=3475000 +y_0=7475000\'\r\n    data_var = ds.createVariable(\'data\', \'S1\')\r\n    set_crs(ds, \'data\', Proj(proj4))\r\n    crs_var = ds.variables[get_ncattrs(data_var)[\'grid_mapping\']]\r\n    ncatts = get_ncattrs(crs_var)\r\n\r\n    assert ncatts[\'grid_mapping_name\'] == \'polar_stereographic\'\r\n    assert ncatts[\'inverse_flattening\'] == 298.257223563\r\n    assert ncatts[\'latitude_of_projection_origin\'] == 90\r\n    assert ncatts[\'straight_vertical_longitude_from_pole\'] == 263\r\n    assert ncatts[\'standard_parallel\'] == 60\r\n    assert ncatts[\'false_northing\'] == 7475000\r\n    assert ncatts[\'false_easting\'] == 3475000\r\n\r\n    # Test Lambert conformal conic\r\n    proj4 = \'+proj=lcc +lat_1=30 +lat_2=60 +lat_0=47.5 +lon_0=-97 +x_0=3825000 +y_0=3200000\'\r\n    data_var = ds.createVariable(\'data2\', \'S1\')\r\n    set_crs(ds, \'data2\', Proj(proj4))\r\n    crs_var = ds.variables[get_ncattrs(data_var)[\'grid_mapping\']]\r\n    ncatts = get_ncattrs(crs_var)\r\n\r\n    assert ncatts[\'grid_mapping_name\'] == \'lambert_conformal_conic\'\r\n    assert ncatts[\'latitude_of_projection_origin\'] == 47.5\r\n    assert ncatts[\'longitude_of_central_meridian\'] == -97\r\n    assert ncatts[\'standard_parallel\'] == [30, 60]\r\n    assert ncatts[\'false_northing\'] == 3200000\r\n    assert ncatts[\'false_easting\'] == 3825000\r\n\r\n    # Unsupported projection should fail\r\n    proj4 = \'+proj=merc +lat_1=30 +lat_2=60 +lat_0=47.5 +lon_0=-97 +x_0=3825000 +y_0=3200000\'\r\n    ds.createVariable(\'data3\', \'S1\')\r\n    with pytest.raises(ValueError):\r\n        set_crs(ds, \'data3\', Proj(proj4))\r\n\r\n\r\ndef test_set_crs_epsg(tmpdir):\r\n    """""" Tests for EPSG codes specifically """"""\r\n\r\n    ds = Dataset(str(tmpdir.join(\'test.nc\')), \'w\')\r\n    data_var = ds.createVariable(\'data\', \'S1\')\r\n    set_crs(ds, \'data\', Proj(init=\'EPSG:4326\'), set_proj4_att=True)\r\n    data_atts = get_ncattrs(data_var)\r\n    crs_var = ds.variables[data_atts[\'grid_mapping\']]\r\n    ncatts = get_ncattrs(crs_var)\r\n\r\n    assert data_atts[\'proj4\'] == \'+proj=latlong +datum=WGS84 +no_defs\'\r\n    assert ncatts[\'grid_mapping_name\'] == \'latitude_longitude\'\r\n    assert ncatts[\'semi_major_axis\'] == 6378137.0\r\n    assert ncatts[\'inverse_flattening\'] == 298.257223563\r\n\r\n    data_var = ds.createVariable(\'data2\', \'S1\')\r\n    set_crs(ds, \'data2\', Proj(init=\'EPSG:4269\'), set_proj4_att=True)\r\n    data_atts = get_ncattrs(data_var)\r\n    crs_var = ds.variables[data_atts[\'grid_mapping\']]\r\n    ncatts = get_ncattrs(crs_var)\r\n\r\n    assert data_atts[\'proj4\'] == \'+proj=latlong +datum=NAD83 +no_defs\'\r\n    assert ncatts[\'grid_mapping_name\'] == \'latitude_longitude\'\r\n    assert ncatts[\'semi_major_axis\'] == 6378137.0\r\n    assert ncatts[\'inverse_flattening\'] == 298.257223563\r\n\r\n\r\ndef test_symmetric_proj4(tmpdir):\r\n    """""" Test writing and reading proj4 string as attribute of variable """"""\r\n\r\n    ds = Dataset(str(tmpdir.join(\'test.nc\')), \'w\')\r\n    proj4 = \'+proj=stere +units=m +datum=WGS84 +lat_ts=60 +lat_0=90 +lon_0=263 +lat_1=60 +x_0=3475000 +y_0=7475000\'\r\n    ds.createVariable(\'data\', \'S1\')\r\n    set_crs(ds, \'data\', Proj(proj4), set_proj4_att=True)\r\n    out_proj4 = get_crs(ds, \'data\')\r\n\r\n    out_data = CRS.from_string(out_proj4).to_dict()\r\n\r\n    assert len(out_data) == 9  # There should be 9 parameters\r\n    assert CRS.from_string(proj4).to_dict() == out_data\r\n\r\n\r\ndef test_utm(tmpdir):\r\n    ds = Dataset(str(tmpdir.join(\'test.nc\')), \'w\')\r\n    proj4 = \'+init=epsg:3157\'  # UTM Zone 10\r\n    ds.createVariable(\'data\', \'S1\')\r\n    set_crs(ds, \'data\', Proj(proj4), set_proj4_att=True)\r\n    out_proj4 = get_crs(ds, \'data\')\r\n\r\n    out_data = CRS.from_string(out_proj4).to_dict()\r\n\r\n    # ESPG will have been converted to long form\r\n    assert len(out_data) == 6\r\n\r\n    expected = {\r\n        u\'zone\': 10,\r\n        u\'ellps\': u\'GRS80\',\r\n        u\'no_defs\': True,\r\n        u\'proj\': u\'utm\',\r\n        u\'units\': u\'m\',\r\n        u\'towgs84\': u\'0,0,0,0,0,0,0\'\r\n    }\r\n    assert expected == out_data\r\n\r\n\r\ndef test_is_geographic(tmpdir):\r\n    ds = Dataset(str(tmpdir.join(\'test.nc\')), \'w\')\r\n    ds.createDimension(\'lat\', 1)\r\n    ds.createDimension(\'lon\', 1)\r\n    ds.createVariable(\'data\', \'S1\', dimensions=(\'lat\', \'lon\'))\r\n\r\n    assert is_geographic(ds, \'data\') == True\r\n\r\n    ds.createDimension(\'foo\', 1)\r\n    ds.createDimension(\'bar\', 1)\r\n    ds.createVariable(\'data2\', \'S1\', dimensions=(\'foo\', \'bar\'))\r\n\r\n    assert is_geographic(ds, \'data2\') == False\r\n'"
trefoil/netcdf/tests/test_variable.py,0,"b""import os\r\nimport numpy\r\nfrom pyproj import Proj\r\nfrom netCDF4 import Dataset\r\nfrom trefoil.netcdf.variable import CoordinateVariable, BoundsCoordinateVariable\r\nfrom trefoil.netcdf.variable import SpatialCoordinateVariable, SpatialCoordinateVariables\r\nfrom trefoil.geometry.bbox import BBox\r\n\r\n\r\ndef test_coordinate_variable_length():\r\n    data = numpy.arange(10)\r\n    variable = CoordinateVariable(data)\r\n    assert len(variable) == data.shape[0]\r\n\r\n\r\ndef test_range_functions():\r\n    data = numpy.arange(10)\r\n    variable = CoordinateVariable(data)\r\n    value_range = (2, 5)\r\n    indices = variable.indices_for_range(*value_range)\r\n    assert indices == value_range\r\n    assert numpy.array_equal(variable.slice_by_range(*value_range), data[2:6])\r\n\r\n    # Test values in reverse order\r\n    data = data[::-1]\r\n    variable = CoordinateVariable(data)\r\n    indices = variable.indices_for_range(*value_range)\r\n    size = len(variable) - 1\r\n    assert indices == (size - value_range[1], size - value_range[0])\r\n    assert numpy.array_equal(variable.slice_by_range(*value_range), data[4:8])\r\n\r\n    # Test value range much larger than data\r\n    value_range = (-100, 100)\r\n    variable = CoordinateVariable(numpy.arange(1, 11))\r\n    indices = variable.indices_for_range(*value_range)\r\n    assert indices == (0, len(variable) - 1)\r\n\r\n    data = numpy.arange(20,40)\r\n    variable = CoordinateVariable(data)\r\n    # Test out of range\r\n    assert variable.indices_for_range(0, 10) == (0, 0)\r\n    assert numpy.array_equal(variable.slice_by_range(0, 10), numpy.array([]))\r\n\r\n    #Test partial overlap\r\n    assert numpy.array_equal(variable.slice_by_range(10, 30), numpy.arange(20, 31))\r\n\r\n    assert variable.indices_for_range(40, 50) == (variable.values.size-1, variable.values.size-1)\r\n    assert numpy.array_equal(variable.slice_by_range(40, 50), numpy.array([]))\r\n\r\n\r\ndef test_window_for_bbox():\r\n    coords = SpatialCoordinateVariables.from_bbox(BBox([-124, 82, -122, 90], Proj(init='epsg:4326')), 20, 20)\r\n    window = coords.get_window_for_bbox(BBox([-123.9, 82.4, -122.1, 89.6]))\r\n\r\n    assert window.x_slice == slice(1, 19)\r\n    assert window.y_slice == slice(1, 19)\r\n\r\n\r\ndef test_BoundsCoordinateVariable():\r\n    bounds = numpy.array(((0, 1), (1, 2)))\r\n    variable = BoundsCoordinateVariable(bounds)\r\n    outvarname = 'test_bounds'\r\n    outfilename = 'test.nc'\r\n    try:\r\n        with Dataset(outfilename, 'w') as target_ds:\r\n            variable.add_to_dataset(target_ds, outvarname)\r\n            assert '_bnds' in target_ds.dimensions\r\n            assert outvarname in target_ds.dimensions\r\n            assert outvarname in target_ds.variables\r\n            assert numpy.array_equal(target_ds.variables[outvarname][:], bounds)\r\n    finally:\r\n        if os.path.exists(outfilename):\r\n            os.remove(outfilename)\r\n\r\n\r\ndef test_SpatialCoordinateVariable():\r\n    # Ascending\r\n    variable = SpatialCoordinateVariable(numpy.arange(10))\r\n    assert numpy.array_equal(variable.edges, numpy.arange(11) - 0.5)\r\n\r\n    # Descending\r\n    variable = SpatialCoordinateVariable(numpy.arange(9, -1, -1))\r\n    assert numpy.array_equal(variable.edges, numpy.arange(10, -1, -1) - 0.5)\r\n\r\n    outvarname = 'lat'\r\n    outfilename = 'test.nc'\r\n\r\n    try:\r\n        with Dataset(outfilename, 'w') as target_ds:\r\n            variable.add_to_dataset(target_ds, outvarname)\r\n            assert outvarname in target_ds.dimensions\r\n            assert outvarname in target_ds.variables\r\n            assert numpy.array_equal(target_ds.variables[outvarname][:], variable.values)\r\n    finally:\r\n        if os.path.exists(outfilename):\r\n            os.remove(outfilename)\r\n\r\n\r\ndef test_SpatialCoordinateVariables_bbox():\r\n    proj = Proj(init='EPSG:4326')\r\n    bbox = BBox((10.5, 5, 110.5, 55), projection=proj)\r\n    coords = SpatialCoordinateVariables.from_bbox(bbox, 10, 5)\r\n    assert coords.bbox.as_list() == bbox.as_list()\r\n\r\n\r\ndef test_SpatialCoordinateVariables_slice_by_bbox():\r\n    lat = SpatialCoordinateVariable(numpy.arange(19, -1, -1))\r\n    lon = SpatialCoordinateVariable(numpy.arange(10))\r\n    proj = Proj(init='EPSG:4326')\r\n    coords = SpatialCoordinateVariables(lon, lat, proj)\r\n\r\n    subset = coords.slice_by_bbox(BBox((1.75, 3.7, 6.2, 16.7), proj))\r\n    assert numpy.array_equal(subset.x.values, numpy.arange(2, 6))\r\n    assert subset.x.values[0] == 2\r\n    assert subset.x.values[-1] == 5\r\n    assert subset.y.values[0] == 16\r\n    assert subset.y.values[-1] == 4\r\n\r\n\r\ndef test_SpatialCoordinateVariables_add_to_dataset():\r\n    lat = SpatialCoordinateVariable(numpy.arange(19, -1, -1))\r\n    lon = SpatialCoordinateVariable(numpy.arange(10))\r\n    coords = SpatialCoordinateVariables(lon, lat, Proj(init='EPSG:4326'))\r\n\r\n    lat_varname = 'lat'\r\n    lon_varname = 'lon'\r\n    outfilename = 'test.nc'\r\n\r\n    try:\r\n        with Dataset(outfilename, 'w') as target_ds:\r\n            coords.add_to_dataset(target_ds, lon_varname, lat_varname)\r\n\r\n            assert lat_varname in target_ds.dimensions\r\n            assert lat_varname in target_ds.variables\r\n            assert len(target_ds.dimensions[lat_varname]) == lat.values.size\r\n            assert numpy.array_equal(lat.values, target_ds.variables[lat_varname][:])\r\n\r\n            assert lon_varname in target_ds.dimensions\r\n            assert lon_varname in target_ds.variables\r\n            assert len(target_ds.dimensions[lon_varname]) == lon.values.size\r\n            assert numpy.array_equal(lon.values, target_ds.variables[lon_varname][:])\r\n    finally:\r\n        if os.path.exists(outfilename):\r\n            os.remove(outfilename)\r\n\r\n\r\n"""
trefoil/render/renderers/__init__.py,0,"b'import json\r\nfrom PIL import Image\r\nimport numpy\r\n\r\nfrom trefoil.utilities.color import Color\r\n\r\n\r\nLEGEND_ELEMENT_BORDER_COLOR = Color(150, 150, 150, 0)\r\nLEGEND_ELEMENT_BORDER_WIDTH = 1\r\n\r\nclass RasterRenderer(object):\r\n    def __init__(self, colormap, fill_value, background_color):\r\n        """"""\r\n        Construct a new renderer.\r\n\r\n        :param colormap: [(value or class break, Color object)...]\r\n        :param fill_value: value to fill with background color (if provided) or transparent\r\n        :param background_color: the background color to apply to all areas not specifically handled by colormap, including\r\n        areas with fill_value or masked out.\r\n        """"""\r\n\r\n        if background_color is not None:\r\n            assert isinstance(background_color, Color)\r\n        else:\r\n            background_color = Color(0, 0, 0, 0)\r\n\r\n        self.colormap = list(colormap)\r\n        self.fill_value = fill_value\r\n        self.background_color = background_color\r\n        self.colormap.sort(key=lambda x: x[0])\r\n        self.values = numpy.array([entry[0] for entry in self.colormap])\r\n        self._generate_palette()\r\n\r\n    @property\r\n    def name(self):\r\n        return self.__class__.__name__.lower().replace(\'renderer\', \'\').replace(\'values\', \'\')\r\n\r\n    def get_legend(self, image_width=20, image_height=20):\r\n        raise NotImplementedError(""Must be provided by child class"")\r\n\r\n    def render_image(self, data, row_major_order=True):\r\n        raise NotImplementedError(""Must be provided by child class"")\r\n\r\n    def _generate_palette(self):\r\n        """"""\r\n        Create the palette used by this renderer.  Sets self.palette to a numpy array of colors\r\n        """"""\r\n\r\n        raise NotImplementedError(""Must be provided by child class"")\r\n\r\n    def _mask_fill_value(self, data):\r\n        """"""\r\n        Mask out the fill value, if set.  Always return an instance of a masked array.\r\n        """"""\r\n\r\n        mask = False if self.fill_value is None else (data == self.fill_value)\r\n        return numpy.ma.masked_array(data, mask=mask)\r\n\r\n    def _create_image(self, image_data, size):\r\n        """"""\r\n        Creates image, setting background color into image and palette\r\n        """"""\r\n        background_index = self.palette.shape[0]\r\n        if hasattr(image_data, \'mask\'):\r\n            image_data = image_data.filled(background_index)\r\n\r\n        image = Image.frombuffer(""P"", size, image_data, ""raw"", ""P"", 0, 1)\r\n\r\n        palette = self.palette[..., :3].flatten().tolist()\r\n        # Append background color\r\n        palette.extend(self.background_color.to_tuple()[:3])\r\n        image.putpalette(palette, ""RGB"")\r\n\r\n        if self.background_color.alpha == 0:\r\n            image.info[\'transparency\'] = background_index\r\n\r\n        return image\r\n\r\n    def serialize(self):\r\n        """""" Returns self as a dictionary """"""\r\n        ret = {\r\n            ""type"": self.name,\r\n            ""colors"": [(entry[0], entry[1].to_hex()) for entry in self.colormap]\r\n        }\r\n        if self.fill_value is not None:\r\n            ret[\'options\'] = {\'fill_value\': self.fill_value}\r\n        # TODO: background color\r\n\r\n        return ret\r\n\r\n    def to_json(self, indent=4):\r\n        """""" Returns self serialized to JSON """"""\r\n\r\n        return json.dumps(self.serialize(), indent=indent)'"
trefoil/render/renderers/classified.py,0,"b'from PIL import Image\r\nimport numpy\r\n\r\nfrom trefoil.utilities.format import PrecisionFormatter\r\nfrom trefoil.render.renderers import RasterRenderer\r\nfrom trefoil.render.renderers.legend import LegendElement\r\n\r\nLEGEND_TICK_POSITION = 0.5  # center-aligned with legend image\r\n\r\n\r\nclass ClassifiedRenderer(RasterRenderer):\r\n    def __init__(self, colormap, fill_value=None, background_color=None):\r\n        """"""\r\n        Maps class breaks to colors.  Values <= break (and greater than previous break) are assigned the color for that break\r\n\r\n        :param colormap:\r\n            [\r\n                (break1, Color1),  # all values <= break1 get Color1\r\n                (break2, Color2),  # all values > break1 and <= break2 get Color2\r\n                ...\r\n                (max, ColorN),  # max can be numpy.inf to apply the renderer to multiple value ranges\r\n            ]\r\n        """"""\r\n\r\n        assert len(colormap) >= 2\r\n\r\n        super(ClassifiedRenderer, self).__init__(colormap, fill_value, background_color)\r\n\r\n    def get_legend(self, image_width=20, image_height=20, min_value=None, max_value=None):\r\n        format_values = list(self.values)\r\n        if min_value is not None:\r\n            format_values.append(min_value)\r\n        if numpy.inf in format_values:\r\n            format_values.remove(numpy.inf)\r\n        formatter = PrecisionFormatter(format_values)\r\n        legend_elements = []\r\n        num_breaks = len(self.values)\r\n\r\n        for index in range(0, num_breaks):\r\n            img = Image.new(""RGBA"", (image_width, image_height), tuple(self.palette[index]))\r\n            if index == 0:\r\n                if min_value is not None:\r\n                    label = ""%s - %s"" % (formatter.format(min_value), formatter.format(self.values[0]))\r\n                else:\r\n                    label = ""<= %s"" % formatter.format(self.values[0])\r\n            elif index == (num_breaks - 1):\r\n                if max_value is not None:\r\n                    label = ""%s - %s"" % (formatter.format(self.values[index-1]), formatter.format(max_value))\r\n                else:\r\n                    label = ""> %s"" % formatter.format(self.values[index-1])\r\n            else:\r\n                label = ""%s - %s"" % (formatter.format(self.values[index-1]), formatter.format(self.values[index]))\r\n            legend_elements.append(LegendElement(\r\n                img,\r\n                [LEGEND_TICK_POSITION],\r\n                [label]\r\n            ))\r\n\r\n        return legend_elements\r\n\r\n    def render_image(self, data, row_major_order=True):\r\n        values = self._mask_fill_value(data.ravel())\r\n        classified = numpy.digitize(values, self.values).astype(numpy.uint8)\r\n        image_data = numpy.ma.masked_array(classified, mask=values.mask)\r\n\r\n        # have to invert dimensions because PIL thinks about this backwards\r\n        size = data.shape[::-1] if row_major_order else data.shape[:2]\r\n        return self._create_image(image_data, size)\r\n\r\n    def _generate_palette(self):\r\n        self.palette = numpy.asarray([entry[1].to_tuple() for entry in self.colormap]).astype(numpy.uint8)\r\n'"
trefoil/render/renderers/legend.py,0,"b'from base64 import b64encode\r\nfrom PIL import Image, ImageDraw, ImageFont\r\nfrom six import BytesIO\r\nfrom trefoil.utilities.image import autocrop\r\n\r\n\r\nclass LegendElement(object):\r\n    def __init__(self, image, ticks, labels):\r\n        """"""\r\n        :param image: PIL image\r\n        :param ticks: the normalized offsets from the bottom (0) to the top (1) of the image (floating point)\r\n        :param labels: the labels that correspond to the ticks at the same position\r\n        """"""\r\n\r\n        assert len(ticks) == len(labels)\r\n\r\n        self.image = image\r\n        self.ticks = ticks\r\n        self.labels = labels\r\n\r\n    @property\r\n    def image_base64(self):\r\n        if self.image:\r\n            out = BytesIO()\r\n            self.image.save(out, ""PNG"")\r\n            return b64encode(out.getvalue()).decode(\'utf-8\')\r\n        return None\r\n\r\n    def to_image(self):\r\n        """"""render entire legend with labels to a new image""""""\r\n\r\n        try:\r\n            font = ImageFont.truetype(""arial.ttf"", 14)\r\n        except:\r\n            font = ImageFont.load_default()\r\n\r\n        text_width = max([font.getsize(l)[0] for l in self.labels]) + 20\r\n        text_height = font.getsize(self.labels[0])[1]\r\n        half_text_height = int(round(text_height / 2.0))\r\n        label_x_padding = 10\r\n\r\n        width = self.image.size[0] + text_width\r\n        height = self.image.size[1]\r\n\r\n        img = Image.new(""RGBA"", (width, height + 2 * text_height), color=(255,255,255,0))\r\n        img.paste(self.image, (0, text_height))\r\n\r\n        canvas = ImageDraw.Draw(img)\r\n\r\n        for index, label in enumerate(self.labels):\r\n            label_x = self.image.size[0] + label_x_padding\r\n            label_y = int(round(float(1 - self.ticks[index]) * height - half_text_height)) + text_height\r\n            line_x = self.image.size[0] + 2\r\n            line_y = label_y + half_text_height\r\n            canvas.line((line_x, line_y, line_x + label_x_padding - 6, line_y), fill=(150,150,150,255), width=1)\r\n            canvas.text((label_x, label_y), label, font=font, fill=(0,0,0,255))\r\n\r\n        return img\r\n\r\n\r\ndef composite_elements(elements, padding=0):\r\n    """"""\r\n    Return a new image created from compositing elements together, starting from\r\n    the top of the image.\r\n    """"""\r\n\r\n    images = [autocrop(e.to_image()) for e in elements]\r\n    widths = [i.size[0] for i in images]\r\n    heights = [i.size[1] for i in images]\r\n\r\n    img = Image.new(""RGBA"", (max(widths), sum(heights) + len(heights) * padding),\r\n                    color=(255, 255, 255, 0))\r\n\r\n    offset = 0\r\n    for index, element_image in enumerate(images):\r\n        img.paste(element_image, (0, offset))\r\n        offset += element_image.size[1] + padding\r\n\r\n    return img'"
trefoil/render/renderers/stretched.py,0,"b'from PIL import Image\r\nimport numpy\r\n\r\nfrom trefoil.utilities.format import PrecisionFormatter\r\nfrom trefoil.utilities.color import interpolate_linear\r\nfrom trefoil.render.renderers import RasterRenderer\r\nfrom trefoil.render.renderers.legend import LegendElement\r\n\r\n\r\ndef getRendererClass():\r\n    return StretchedRenderer\r\n\r\n\r\n# TODO: params for palette size and optimization\r\n\r\nclass StretchedRenderer(RasterRenderer):\r\n    def __init__(self,\r\n                 colormap,\r\n                 fill_value=None,\r\n                 background_color=None,\r\n                 method=""linear"",\r\n                 colorspace=""hsv"",\r\n                 palette_size=None):\r\n        """"""\r\n        Maps a stretched color palette against the data range according to a particular method\r\n        (e.g., linear from min to max value).\r\n\r\n        :param linear: apply a linear stretch between min and max values\r\n        :param colorspace: the colorspace in which to do color interpolation\r\n        :param palette_size: if provided, will be used as palette size.  Otherwise will default to 255 if > 20 colors in colormap, otherwise 90\r\n        """"""\r\n\r\n        assert len(colormap) >= 2\r\n\r\n        self.method = method\r\n        self.colorspace = colorspace\r\n\r\n        if palette_size is not None:\r\n            assert palette_size <= 255\r\n            self.palette_size = palette_size\r\n        elif len(colormap) > 20:\r\n            self.palette_size = 255\r\n        else:\r\n            self.palette_size = 90\r\n\r\n        super(StretchedRenderer, self).__init__(colormap, fill_value, background_color)\r\n\r\n    def get_legend(self, image_width=20, image_height=100, ticks=None, breaks=None, max_precision=6, discrete_images=False):\r\n        """"""\r\n        Image is always rendered high to low.\r\n\r\n        :param ticks: tick positions in dataset value range\r\n        :param breaks: number of breaks between min and max to show on legend\r\n        """"""\r\n\r\n        if self.method == ""linear"":\r\n            formatter = PrecisionFormatter(self.values, max_precision)\r\n\r\n            if ticks is not None:\r\n                if not len(ticks) >= 2:\r\n                    raise ValueError(\'Must provide at least 2 ticks\')\r\n                tick_values = ticks\r\n            elif breaks is not None:\r\n                tick_values = numpy.linspace(self.min_value, self.max_value, breaks + 2)\r\n            else:\r\n                tick_values = self.values.copy()\r\n\r\n            tick_values.sort()\r\n            value_range = self.max_value - self.min_value\r\n\r\n            tick_positions = []\r\n            labels = []\r\n\r\n            if abs(value_range) > 0:\r\n                for value in tick_values:\r\n                    tick_positions.append((value - self.min_value) / value_range)\r\n                    labels.append(formatter.format(value))\r\n            else:\r\n                tick_positions = [0, 1.0]\r\n                labels = [formatter.format(tick_values[0])] * 2\r\n\r\n            if discrete_images:\r\n                colors = numpy.asarray([entry[1].to_tuple() for entry in self.colormap]).astype(numpy.uint8)\r\n                return [\r\n                    LegendElement(\r\n                        Image.new(""RGBA"", (image_width, image_height), tuple(colors[index])),\r\n                        [0.5],\r\n                        [labels[index]]\r\n                    )\r\n                    for index, value in enumerate(tick_values)\r\n                ]\r\n\r\n            else:\r\n                legend_values = numpy.linspace(self.min_value, self.max_value, image_height)\r\n                return [LegendElement(\r\n                    self.render_image(numpy.array([legend_values,]).T[::-1]).resize((image_width, image_height), Image.ANTIALIAS),\r\n                    tick_positions,\r\n                    labels\r\n                )]\r\n\r\n        else:\r\n            raise NotImplementedError(""Legends not built for other stretched renderer methods"")\r\n\r\n    # TODO: handle cropping of stretched values properly (truncate above and below and set to background value?)\r\n    def render_image(self, data, row_major_order=True):\r\n        num_colors = self.palette.shape[0]\r\n        if self.min_value == self.max_value:\r\n            factor = 1.0\r\n        else:\r\n            factor = float(num_colors - 1) / float(self.max_value - self.min_value)\r\n\r\n        values = self._mask_fill_value(data.ravel())\r\n\r\n        # derive palette index, and clip to [0, num_colors - 1]\r\n        stretched = ((values - self.min_value) * factor).astype(numpy.int)\r\n        image_data = stretched.clip(0, num_colors - 1).astype(numpy.uint8)\r\n\r\n        # have to invert dimensions because PIL thinks about this backwards\r\n        size = data.shape[::-1] if row_major_order else data.shape[:2]\r\n        return self._create_image(image_data, size)\r\n\r\n    def _generate_palette(self):\r\n        self.min_value = self.colormap[0][0]\r\n        self.max_value = self.colormap[len(self.colormap)-1][0]\r\n        colors = numpy.asarray([c[1].to_tuple() for c in self.colormap]).astype(numpy.uint8)\r\n\r\n        if self.method == ""linear"":\r\n            self.palette = interpolate_linear(colors, self.values, self.palette_size, colorspace=self.colorspace)\r\n        else:\r\n            raise NotImplementedError(""Other stretched render methods not built!"")\r\n\r\n    def serialize(self):\r\n        ret = super(StretchedRenderer, self).serialize()\r\n\r\n        if \'options\' in ret:\r\n            ret[\'options\'][\'color_space\'] = self.colorspace\r\n        else:\r\n            ret[\'options\'] = {\'color_space\': self.colorspace}\r\n\r\n        return ret'"
trefoil/render/renderers/unique.py,0,"b'from PIL import Image\r\nimport numpy\r\n\r\nfrom trefoil.render.renderers import RasterRenderer\r\nfrom trefoil.render.renderers.legend import LegendElement\r\nfrom trefoil.utilities.format import PrecisionFormatter\r\n\r\n\r\nLEGEND_TICK_POSITION = 0.5\r\n\r\n\r\nclass UniqueValuesRenderer(RasterRenderer):\r\n    def __init__(self, colormap, fill_value=None, background_color=None, labels=None):\r\n        """"""\r\n        Maps unique values to colors.  Any color not mapped is set to transparent or background_color.\r\n\r\n        :param colormap: list of value, Color instances: [(value, Color)...]\r\n        """"""\r\n\r\n        assert len(colormap) > 0\r\n\r\n        super(UniqueValuesRenderer, self).__init__(colormap, fill_value, background_color)\r\n        if labels:\r\n            assert len(colormap) == len(labels)\r\n            self.labels = labels\r\n        else:\r\n            self.labels = []\r\n\r\n    def get_legend(self, image_width=20, image_height=20):\r\n        legend_elements = []\r\n        if self.labels:\r\n            labels = self.labels\r\n        else:\r\n            formatter = PrecisionFormatter(self.values)\r\n            labels = [formatter.format(x) for x in self.values]\r\n\r\n        for index, value in enumerate(self.values):\r\n            legend_elements.append(LegendElement(\r\n                Image.new(""RGBA"", (image_width, image_height), tuple(self.palette[index])),\r\n                [LEGEND_TICK_POSITION],\r\n                [labels[index]]\r\n            ))\r\n        return legend_elements\r\n\r\n    def render_image(self, data, row_major_order=True):\r\n        values = self._mask_fill_value(data.ravel())\r\n\r\n        max_value = max(values.max(), self.values.max())\r\n        if values.dtype.kind == \'u\' and max_value < 65536:\r\n            palette_indices = numpy.zeros(max_value + 1, dtype=numpy.uint8)\r\n            palette_indices.fill(self.values.shape[0])\r\n            for index, value in enumerate(self.values):\r\n                palette_indices.itemset(value, index)\r\n            image_data = palette_indices[values].astype(numpy.uint8)\r\n        else:\r\n            image_data = numpy.zeros(values.shape, dtype=numpy.uint8)\r\n            image_data.fill(self.values.shape[0])\r\n            for index, value in enumerate(self.values):\r\n                image_data[values == value] = index\r\n\r\n         # have to invert dimensions because PIL thinks about this backwards\r\n        size = data.shape[::-1] if row_major_order else data.shape[:2]\r\n        return self._create_image(image_data, size)\r\n\r\n    def _generate_palette(self):\r\n        self.palette = numpy.asarray([entry[1].to_tuple() for entry in self.colormap]).astype(numpy.uint8)\r\n\r\n    def serialize(self):\r\n        ret = super(UniqueValuesRenderer, self).serialize()\r\n        if self.labels:\r\n            if \'options\' in ret:\r\n                ret[\'options\'][\'labels\'] = self.labels\r\n            else:\r\n                ret[\'options\'] = {\'labels\': self.labels}\r\n\r\n        return ret'"
trefoil/render/renderers/utilities.py,0,"b'import six\r\nfrom trefoil.render.renderers.classified import ClassifiedRenderer\r\nfrom trefoil.render.renderers.stretched import StretchedRenderer\r\nfrom trefoil.render.renderers.unique import UniqueValuesRenderer\r\nfrom trefoil.utilities.color import Color\r\n\r\n\r\nAVAILABLE_RENDERERS = {\r\n    ""classified"": ClassifiedRenderer,\r\n    ""stretched"": StretchedRenderer,\r\n    ""unique"": UniqueValuesRenderer\r\n}\r\n\r\n\r\ndef get_renderer_by_name(name):\r\n    return AVAILABLE_RENDERERS[name]\r\n\r\n\r\ndef get_renderer_name(renderer):\r\n    if isinstance(renderer, ClassifiedRenderer):\r\n        return ""classified""\r\n    elif isinstance(renderer, StretchedRenderer):\r\n        return ""stretched""\r\n    elif isinstance(renderer, UniqueValuesRenderer):\r\n        return ""unique""\r\n    else:\r\n        raise ValueError(""Could not find name for renderer: %s"" % renderer)\r\n\r\n\r\ndef renderer_from_dict(renderer_dict):\r\n    """"""Returns a renderer object from a dictionary object""""""\r\n\r\n    options = renderer_dict.get(\'options\', {})\r\n\r\n    try:\r\n        renderer_type = renderer_dict[\'type\']\r\n        renderer_colors = [(float(x[0]), Color.from_hex(x[1])) for x in renderer_dict[\'colors\']]\r\n        fill_value = options.get(\'fill_value\')\r\n        if fill_value is not None:\r\n            fill_value = float(fill_value)\r\n    except KeyError:\r\n        raise ValueError(""Missing required keys from renderer renderer_dicturation"")\r\n\r\n    renderer_kwargs = {\r\n        \'colormap\': renderer_colors,\r\n        \'fill_value\': fill_value,\r\n        \'background_color\': Color(255, 255, 255, 0)\r\n    }\r\n\r\n    if renderer_type == ""stretched"":\r\n        color_space = options.get(\'color_space\', \'hsv\').lower().strip()\r\n        if not color_space in (\'rgb\', \'hsv\'):\r\n            raise ValueError(""Invalid color space: {}"".format(color_space))\r\n\r\n        renderer = StretchedRenderer(colorspace=color_space, **renderer_kwargs)\r\n    elif renderer_type == ""classified"":\r\n        renderer = ClassifiedRenderer(**renderer_kwargs)\r\n    elif renderer_type == ""unique"":\r\n        try:\r\n            labels = [six.text_type(x) for x in options.get(\'labels\', [])]\r\n        except TypeError:\r\n            raise ValueError(""Labels option must be an array"")\r\n\r\n        renderer = UniqueValuesRenderer(labels=labels, **renderer_kwargs)\r\n\r\n    return renderer'"
trefoil/utilities/tests/__init__.py,0,b''
trefoil/utilities/tests/test_color.py,0,"b'from trefoil.utilities.color import Color\r\n\r\n\r\ndef test_color():\r\n    color_tuple = (0, 0, 0)\r\n    c = Color(*color_tuple)\r\n    assert c.to_tuple() == color_tuple\r\n    assert c.to_hex() == ""#000""\r\n    c2 = Color.from_hsv(*c.to_hsv())\r\n    assert c2.to_tuple() == color_tuple\r\n\r\n    assert Color.from_hex(""#000000"", alpha=100)\r\n'"
trefoil/render/renderers/tests/__init__.py,0,b''
trefoil/render/renderers/tests/test_renderers.py,0,"b'import numpy\r\n\r\nfrom trefoil.utilities.color import Color\r\nfrom trefoil.render.renderers.stretched import StretchedRenderer\r\nfrom trefoil.render.renderers.classified import ClassifiedRenderer\r\nfrom trefoil.render.renderers.unique import UniqueValuesRenderer\r\nfrom trefoil.render.renderers.utilities import get_renderer_by_name\r\n\r\n\r\ndef test_stretched_renderer(tmpdir):\r\n    data = numpy.zeros((100, 100))\r\n    for i in range(0, 100):\r\n        data[i] = i\r\n    colors = (\r\n        (data.min(), Color(255, 0, 0, 255)),\r\n        (data.max(), Color(0, 0, 255, 255))\r\n    )\r\n    renderer = StretchedRenderer(colors)\r\n\r\n    assert renderer.name == \'stretched\'\r\n\r\n    img = renderer.render_image(data)\r\n    assert len(img.getpalette()) / 3 == 256\r\n    assert img.size == (100, 100)\r\n    img.save(str(tmpdir.join(""stretched.png"")))\r\n\r\n    legend = renderer.get_legend(20, 20)\r\n    assert len(legend) == 1\r\n    assert legend[0].image.size == (20, 20)\r\n    legend[0].image.save(str(tmpdir.join(""stretched_legend.png"")))\r\n\r\n    legend = renderer.get_legend(20, 20, discrete_images=True)\r\n    assert len(legend) == 2\r\n    assert legend[0].image.size == (20, 20)\r\n\r\n    expected = {\r\n        \'colors\': [(0.0, \'#F00\'), (99.0, \'#00F\')],\r\n        \'type\': \'stretched\',\r\n        \'options\': {\'color_space\': \'hsv\'}\r\n    }\r\n\r\n    assert renderer.serialize() == expected\r\n\r\n\r\ndef test_classified_rendererer(tmpdir):\r\n    data = numpy.zeros((100, 100))\r\n    for i in range(0, 100):\r\n        data[i] = i\r\n    colors = (\r\n        (10, Color(255, 0, 0, 255)),\r\n        (50, Color(0, 255, 0, 255)),\r\n        (data.max(), Color(0, 0, 255, 255))\r\n    )\r\n    renderer = ClassifiedRenderer(colors)\r\n\r\n    assert renderer.name == \'classified\'\r\n\r\n    img = renderer.render_image(data)\r\n    img.save(str(tmpdir.join(""classified.png"")))\r\n    assert img.palette.palette == b\'\\xff\\x00\\x00\\x00\\xff\\x00\\x00\\x00\\xff\\x00\\x00\\x00\'\r\n    assert img.size == (100, 100)\r\n\r\n    legend = renderer.get_legend(20, 20)\r\n    assert len(legend) == 3\r\n    for index, element in enumerate(legend):\r\n        element.image.save(str(tmpdir.join(""classified_legend_%i.png"" % index)))\r\n\r\n    expected = {\r\n        \'colors\': [(10, \'#F00\'), (50, \'#0F0\'), (99.0, \'#00F\')],\r\n        \'type\': \'classified\'\r\n    }\r\n    assert renderer.serialize() == expected\r\n\r\n\r\ndef test_uniquevalues_renderer(tmpdir):\r\n    data = numpy.zeros((100, 100))\r\n    data[10:25] = 10\r\n    data[35:50] = 25\r\n    data[50:75] = 50\r\n    data[85:100] = 100\r\n    colors = (\r\n        (10, Color(255, 0, 0, 255)),\r\n        (25, Color(255, 255, 255, 255)),\r\n        (50, Color(0, 255, 0, 255)),\r\n        (100, Color(0, 0, 255, 255))\r\n    )\r\n    labels = (\'A\', \'B\', \'C\', \'D\')\r\n\r\n    renderer = UniqueValuesRenderer(colors, labels=labels)\r\n\r\n    assert renderer.name == \'unique\'\r\n\r\n    img = renderer.render_image(data)\r\n    img.save(str(tmpdir.join(""unique.png"")))\r\n    assert img.palette.palette == b\'\\xff\\x00\\x00\\xff\\xff\\xff\\x00\\xff\\x00\\x00\\x00\\xff\\x00\\x00\\x00\'\r\n    assert img.size == (100, 100)\r\n    legend = renderer.get_legend(20, 20)\r\n    assert len(legend) == 4\r\n    for index, element in enumerate(legend):\r\n        element.image.save(\r\n            str(tmpdir.join(""uniquevalues_legend_%i.png"" % index)))\r\n\r\n    expected = {\r\n        \'colors\': [\r\n            (10, \'#F00\'),\r\n            (25, \'#FFF\'),\r\n            (50, \'#0F0\'),\r\n            (100, \'#00F\')],\r\n        \'type\': \'unique\',\r\n        \'options\': {\r\n            \'labels\': (\'A\', \'B\', \'C\', \'D\')\r\n        }\r\n    }\r\n    assert renderer.serialize() == expected\r\n\r\n\r\ndef test_get_renderers_by_name():\r\n    data = numpy.zeros((100, 100))\r\n    for i in range(0, 100):\r\n        data[i] = i\r\n    colors = (\r\n        (10, Color(255, 0, 0, 255)),\r\n        (50, Color(0, 255, 0, 255)),\r\n        (data.max(), Color(0, 0, 255, 255))\r\n    )\r\n    renderer = get_renderer_by_name(""classified"")(colors)\r\n    img = renderer.render_image(data)\r\n    assert img.palette.palette == b\'\\xff\\x00\\x00\\x00\\xff\\x00\\x00\\x00\\xff\\x00\\x00\\x00\'\r\n    assert img.size == (100, 100)\r\n'"
