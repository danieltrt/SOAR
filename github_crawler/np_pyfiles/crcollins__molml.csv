file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n\n# http://stackoverflow.com/questions/9810603/adding-install-requires-to-setup-py-when-making-a-python-package\ntry:\n    from setuptools import setup\nexcept ImportError:\n    from distutils.core import setup\n\ntry:\n    import pypandoc\n    try:\n        LONG_DESCRIPTION = pypandoc.convert(\'README.md\', \'rst\')\n    except:\n        # Catch all exceptions because FileNotFoundError is only in 3.x\n        from pypandoc.pandoc_download import download_pandoc\n        download_pandoc()\n        LONG_DESCRIPTION = pypandoc.convert(\'README.md\', \'rst\')\nexcept ImportError:\n    with open(\'README.md\', \'r\') as f:\n        LONG_DESCRIPTION = f.read()\n\nsetup(\n    name=\'molml\',\n    version=\'0.9.0\',\n    description=\'An interface between molecules and machine learning\',\n    long_description=LONG_DESCRIPTION,\n    author=\'Chris Collins\',\n    author_email=\'chris@crcollins.com\',\n    url=\'https://github.com/crcollins/molml/\',\n    license=\'MIT\',\n    packages=[\'molml\'],\n    test_suite=\'nose.collector\',\n    tests_require=[\'nose\'],\n    install_requires=[\n        \'pathos\',\n        \'future\',\n        \'bidict\',\n    ],\n    classifiers=[\n                    ""Intended Audience :: Developers"",\n                    ""Intended Audience :: Science/Research"",\n                    ""License :: OSI Approved :: MIT License"",\n                    ""Operating System :: POSIX"",\n                    ""Programming Language :: Python :: 2"",\n                    ""Programming Language :: Python :: 2.7"",\n                    ""Programming Language :: Python :: 3"",\n                    ""Programming Language :: Python :: 3.4"",\n                    ""Programming Language :: Python :: 3.5"",\n                    ""Programming Language :: Python :: 3.6"",\n                    ""Topic :: Scientific/Engineering"",\n                    ""Topic :: Scientific/Engineering :: Bio-Informatics"",\n                    ""Topic :: Scientific/Engineering :: Chemistry"",\n                    ""Topic :: Scientific/Engineering :: Physics"",\n    ]\n)\n'"
docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# MolML documentation build configuration file, created by\n# sphinx-quickstart on Mon Feb 20 23:03:10 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../\'))\n\nimport mock\nMOCK_MODULES = [\n                \'numpy\',\n                \'scipy\',\n                \'scipy.spatial\',\n                \'scipy.spatial.distance\',\n                \'scipy.special\',\n                \'scipy.stats\',\n                \'builtins\',\n                \'pathos\',\n                \'pathos.multiprocessing\',\n]\nfor mod_name in MOCK_MODULES:\n    sys.modules[mod_name] = mock.Mock()\n\n# Auto run apidoc to get API docstrings\nfrom sphinx.apidoc import main\nmain([\'-F\', \'../molml\', \'-o\', \'.\', \'-e\'])\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.autosummary\',\n    \'numpydoc\'\n    ]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'MolML\'\ncopyright = \'2016-2019, Chris Collins\'\nauthor = \'Chris Collins\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'0.9\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'0.9.0\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'MolMLdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'MolML.tex\', \'MolML Documentation\',\n     \'Chris Collins\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'molml\', \'MolML Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'MolML\', \'MolML Documentation\',\n     author, \'MolML\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# Fix issue with autosummary\nnumpydoc_show_class_members = False\n'"
examples/crystal.py,0,"b'from __future__ import print_function\n\nimport numpy\n\nfrom molml.features import CoulombMatrix\nfrom molml.crystal import GenerallizedCrystal, EwaldSumMatrix, SineMatrix\n\n\n# Define some base data\nH2_ELES = [\'H\', \'H\']\nH2_NUMS = [1, 1]\nH2_COORDS = numpy.array([\n    [0.0, 0.0, 0.0],\n    [1.0, 0.0, 0.0],\n])\nH2_CONNS = {\n    0: {1: \'1\'},\n    1: {0: \'1\'},\n}\nH2_UNIT = numpy.array([\n    [2., .5, 0.],\n    [.25, 1., 0.],\n    [0., .3, 1.],\n])\n\nradius = 4.1\ninput_type = (""elements"", ""coords"", ""unit_cell"")\nX = (H2_ELES, H2_COORDS, H2_UNIT)\n\nif __name__ == ""__main__"":\n    trans = EwaldSumMatrix(input_type=input_type, G_max=3.2, L_max=2.1)\n    res = trans.fit_transform([X])\n    print(res)\n\n    trans = SineMatrix(input_type=input_type)\n    res = trans.fit_transform([X])\n    print(res)\n\n    # Example of generallized crystal\n    # Any transformer can be used as it just expands the molecule using the\n    # unit cell and coordinates.\n    cm = CoulombMatrix(input_type=input_type)\n    trans = GenerallizedCrystal(transformer=cm,\n                                radius=radius)\n    res = trans.fit_transform([X])\n    print(res)\n'"
examples/load_save.py,0,"b'from __future__ import print_function\n\nimport numpy\n\nfrom molml.features import CoulombMatrix\nfrom molml.crystal import GenerallizedCrystal\nfrom molml.utils import load_json\n\n\n# Define some base data\nH2_ELES = [\'H\', \'H\']\nH2_COORDS = [\n    [0.0, 0.0, 0.0],\n    [1.0, 0.0, 0.0],\n]\nH2_UNIT = numpy.array([\n    [2., .5, 0.],\n    [.25, 1., 0.],\n    [0., .3, 1.],\n])\nH2 = (H2_ELES, H2_COORDS)\nH2_FULL = (H2_ELES, H2_COORDS, H2_UNIT)\n\nHCN_ELES = [\'H\', \'C\', \'N\']\nHCN_COORDS = [\n    [-1.0, 0.0, 0.0],\n    [0.0, 0.0, 0.0],\n    [1.0, 0.0, 0.0],\n]\nHCN = (HCN_ELES, HCN_COORDS)\n\n\nif __name__ == ""__main__"":\n    # Example of fitting the Coulomb matrix and then saving it\n    feat = CoulombMatrix()\n    feat.fit([H2, HCN])\n    print(""Saving Model"")\n    feat.save_json(""coulomb_model.json"")\n\n    print(""Loading Model"")\n    feat2 = load_json(""coulomb_model.json"")\n    print(feat2.transform([H2, HCN]))\n\n    # Example of fitting a generallized crystal with the Coulomb matrix and\n    # then saving it\n    input_type = (""elements"", ""coords"", ""unit_cell"")\n    radius = 4.1\n    feat = CoulombMatrix(input_type=input_type)\n    crystal = GenerallizedCrystal(transformer=feat, radius=radius)\n    feat.fit([H2_FULL])\n    print(""Saving Model"")\n    feat.save_json(""coulomb_crystal_model.json"")\n\n    print(""Loading Model"")\n    feat2 = load_json(""coulomb_crystal_model.json"")\n    print(feat2.transform([H2_FULL]))\n'"
examples/missing_constants.py,1,"b""import numpy as np\n\nfrom molml.features import Connectivity, Autocorrelation\nfrom molml.constants import BOND_LENGTHS, ELE_TO_NUM\n\n# Currently, there are two recommended ways to work with elements that are not\n# included in molml/constants.py. In this example, we will look at an iron\n# complex (iron is not in the constants).\n# Maybe at some point, molml will include more constants, but it seems outside\n# of the scope of this library.\n\nif __name__ == '__main__':\n    elements = ['Fe', 'H', 'H', 'H', 'H', 'H', 'H']\n    coords = np.array([\n        [0., 0., 0.],\n        [1.46, 0., 0.],\n        [0., 1.46, 0.],\n        [0., 0., 1.46],\n        [-1.46, 0., 0.],\n        [0., -1.46, 0.],\n        [0., 0., -1.46],\n    ])\n    feat = Connectivity(depth=2)\n    # Notice the warning about missing elements.\n    print(feat.fit_transform([(elements, coords)]))\n\n    # 1) Modify the values in the constants module before your script.\n    BOND_LENGTHS['Fe'] = {'1': 1.32}\n    print(feat.fit_transform([(elements, coords)]))\n    del BOND_LENGTHS['Fe']\n\n    # 2) Include connectivity information in your data. The other instances\n    # where constants are used (electronegativity, element symbols, atomic\n    # numbers).\n    connections = {\n        0: {1: '1', 2: '1', 3: '1', 4: '1', 5: '1', 6: '1'},\n        1: {0: '1'},\n        2: {0: '1'},\n        3: {0: '1'},\n        4: {0: '1'},\n        5: {0: '1'},\n        6: {0: '1'},\n    }\n    print(feat.fit_transform([(elements, coords, connections)]))\n\n    # Other potential constant additions would be ELECTRONEGATIVITY or\n    # ELE_TO_NUM.\n    # NOTE: ELE_TO_NUM is a bidict mapping element symbols to atomic numbers,\n    # meaning ELE_TO_NUM.inv can be used for atomic number to element symbols.\n    ELE_TO_NUM['Fe'] = 26\n    numbers = [ELE_TO_NUM[x] for x in elements]\n    assert elements == [ELE_TO_NUM.inv[x] for x in numbers]\n    BOND_LENGTHS['Fe'] = {'1': 1.32}\n\n    feat2 = Autocorrelation(properties=('Z', 'R'))\n    print(feat2.fit_transform([(elements, coords)]))\n"""
examples/qm7.py,0,"b'from __future__ import print_function\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error as MAE\n\nfrom molml.features import EncodedBond, BagOfBonds, Connectivity, CoulombMatrix\n\nfrom utils import load_qm7\n\n\nif __name__ == ""__main__"":\n    # This is just boiler plate code to load the data\n    Xin_train, Xin_test, y_train, y_test = load_qm7()\n\n    # Change this to make the tranformations parallel\n    # Values less than 1 will set to the number of cores the CPU has\n    N_JOBS = 1\n\n    # Just a few examples of different features\n    tfs = [\n        EncodedBond(n_jobs=N_JOBS),\n        EncodedBond(spacing=""inverse"", n_jobs=N_JOBS),\n        BagOfBonds(n_jobs=N_JOBS),\n        CoulombMatrix(n_jobs=N_JOBS),\n        Connectivity(depth=1, n_jobs=N_JOBS),\n        Connectivity(depth=2, use_bond_order=True, n_jobs=N_JOBS),\n        Connectivity(depth=3, use_coordination=True, n_jobs=N_JOBS),\n    ]\n\n    for tf in tfs:\n        print(tf)\n        X_train = tf.fit_transform(Xin_train)\n        X_test = tf.transform(Xin_test)\n\n        # We will not do a hyperparmeter search for simplicity\n        clf = Ridge()\n        clf.fit(X_train, y_train)\n        train_error = MAE(clf.predict(X_train), y_train)\n        test_error = MAE(clf.predict(X_test), y_test)\n        print(""Train MAE: %.4f Test MAE: %.4f"" % (train_error, test_error))\n        print()\n'"
examples/qm7_atom.py,0,"b'from sklearn.kernel_ridge import KernelRidge\nfrom sklearn.metrics import mean_absolute_error as MAE\n\nfrom molml.features import LocalEncodedBond\nfrom molml.kernel import AtomKernel\n\nfrom utils import load_qm7\n\n\nif __name__ == ""__main__"":\n    # This is just boiler plate code to load the data\n    Xin_train, Xin_test, y_train, y_test = load_qm7()\n\n    # Look at just a few examples to be quick\n    n_train = 200\n    n_test = 200\n    Xin_train = Xin_train[:n_train]\n    y_train = y_train[:n_train]\n    Xin_test = Xin_test[:n_test]\n    y_test = y_test[:n_test]\n\n    gamma = 1e-7\n    alpha = 1e-7\n    kern = AtomKernel(gamma=gamma, transformer=LocalEncodedBond(n_jobs=-1),\n                      n_jobs=-1)\n    K_train = kern.fit_transform(Xin_train)\n    K_test = kern.transform(Xin_test)\n\n    clf = KernelRidge(alpha=alpha, kernel=""precomputed"")\n    clf.fit(K_train, y_train)\n    train_error = MAE(clf.predict(K_train), y_train)\n    test_error = MAE(clf.predict(K_test), y_test)\n    print(""Train MAE: %.4f Test MAE: %.4f"" % (train_error, test_error))\n    print()\n'"
examples/qm7_long.py,0,"b'from sklearn.linear_model import Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.metrics import mean_absolute_error as MAE\nfrom sklearn.pipeline import FeatureUnion\n\nfrom molml.features import EncodedBond, Connectivity\n\nfrom utils import load_qm7\n\n\nif __name__ == ""__main__"":\n    # This is just boiler plate code to load the data\n    Xin_train, Xin_test, y_train, y_test = load_qm7()\n\n    feats = [\n        (""encoded_bond"", EncodedBond(n_jobs=-1, max_depth=3)),\n        (""atom_count"", Connectivity(depth=1, n_jobs=-1)),\n        (""angle_count"", Connectivity(depth=3, use_coordination=True,\n                                     n_jobs=-1)),\n    ]\n\n    full_feat = FeatureUnion(feats)\n    X_train = full_feat.fit_transform(Xin_train)\n    X_test = full_feat.transform(Xin_test)\n\n    clfs = [\n        Ridge(alpha=0.01),\n        KernelRidge(alpha=1e-9, gamma=1e-5, kernel=""rbf""),\n    ]\n    for clf in clfs:\n        print(clf)\n        clf.fit(X_train, y_train)\n        train_error = MAE(clf.predict(X_train), y_train)\n        test_error = MAE(clf.predict(X_test), y_test)\n        print(""Train MAE: %.4f Test MAE: %.4f"" % (train_error, test_error))\n        print()\n'"
examples/simple.py,0,"b'from __future__ import print_function\n\nfrom molml.features import CoulombMatrix\nfrom molml.features import LocalCoulombMatrix\nfrom molml.kernel import AtomKernel\n\nfrom molml.utils import LazyValues\n\n# Define some base data\nH2_ELES = [\'H\', \'H\']\nH2_NUMS = [1, 1]\nH2_COORDS = [\n    [0.0, 0.0, 0.0],\n    [1.0, 0.0, 0.0],\n]\nH2_CONNS = {\n    0: {1: \'1\'},\n    1: {0: \'1\'},\n}\n\nHCN_ELES = [\'H\', \'C\', \'N\']\nHCN_NUMS = [1, 6, 7]\nHCN_COORDS = [\n    [-1.0, 0.0, 0.0],\n    [0.0, 0.0, 0.0],\n    [1.0, 0.0, 0.0],\n]\nHCN_CONNS = {\n    0: {1: \'1\'},\n    1: {0: \'1\', 2: \'3\'},\n    2: {1: \'3\'},\n}\n\n\nif __name__ == ""__main__"":\n    # Example of generating the Coulomb matrix with just elements and coords\n    # for a single example molecule.\n    feat = CoulombMatrix()\n    H2 = (H2_ELES, H2_COORDS)\n    feat.fit([H2])\n    print(""Transformed H2"")\n    print(feat.transform([H2]))\n    print()\n\n    # Example of generating the Coulomb matrix with just elements and coords\n    # for multiple molecules.\n    feat = CoulombMatrix()\n    HCN = (HCN_ELES, HCN_COORDS)\n    feat.fit([H2, HCN])\n    print(""Transformed H2"")\n    print(feat.transform([H2]))\n    print(""H2 and HCN transformed"")\n    print(feat.transform([H2, HCN]))\n    print()\n\n    # Example of generating the Coulomb matrix with elements, coords, and\n    # connections.\n    feat = CoulombMatrix()\n    H2_conn = (H2_ELES, H2_COORDS, H2_CONNS)\n    HCN_conn = (HCN_ELES, HCN_COORDS, HCN_CONNS)\n    print(feat.fit_transform([H2_conn, HCN_conn]))\n    print()\n\n    # Example of generating the Coulomb matrix using a specified input_type\n    print(""User specified input_type"")\n    feat = CoulombMatrix(input_type=(""coords"", ""numbers""))\n    H2_spec = (H2_COORDS, H2_NUMS)\n    HCN_spec = (HCN_COORDS, HCN_NUMS)\n    print(feat.fit_transform([H2_spec, HCN_spec]))\n    print()\n\n    # Example of generating the Local Coulomb matrix (atom-wise\n    # representation)\n    print(""Atom feature"")\n    feat = LocalCoulombMatrix()\n    print(feat.fit_transform([H2, HCN]))\n\n    # Example of generating AtomKernel\n    print(""Atom Kernel"")\n    feat = AtomKernel(transformer=LocalCoulombMatrix())\n    print(feat.fit_transform([H2, HCN]))\n\n    # Example of using arbitrary function to load data\n    # This example is useless, but it shows the possibility\n    feat = CoulombMatrix(input_type=lambda x: LazyValues(elements=HCN_ELES,\n                                                         coords=HCN_COORDS))\n    feat.fit_transform(list(range(10)))\n'"
examples/simple_crystal.py,0,"b'from __future__ import print_function\n\nimport numpy\n\nfrom molml.utils import LazyValues\nfrom molml.io import read_cry_data\n\nfrom utils import plot_cell\n\n\n# Define some base data\nH_ELES = [\'H\']\nH_NUMS = [1]\nH_COORDS = numpy.array([\n    [0.0, 0.0, 0.0],\n])\nH_CONNS = {\n    0: {},\n}\nH_UNIT = numpy.array([\n    [1., 0., 0.],\n    [0., 1., 0.],\n    [0., 0., 1.],\n])\nradius = 3\n\n\nif __name__ == ""__main__"":\n    vals = LazyValues(coords=H_COORDS, unit_cell=H_UNIT,\n                      elements=H_ELES, numbers=H_NUMS)\n    vals.fill_in_crystal(radius=radius)\n    plot_cell(vals.coords, radius, vals.unit_cell,\n              connections=vals.connections)\n\n    vals = read_cry_data(""../tests/data/methane.cry"")\n    vals.fill_in_crystal(radius=radius)\n    plot_cell(vals.coords, radius, vals.unit_cell,\n              connections=vals.connections)\n'"
examples/utils.py,0,"b'from __future__ import print_function\nimport os\ntry:\n    from urllib.request import urlopen\nexcept ImportError:\n    from urllib2 import urlopen\nfrom builtins import range\n\nimport numpy\nimport scipy.io\nfrom scipy.constants import physical_constants, angstrom\n\nBOHR_TO_ANGSTROMS = physical_constants[\'Bohr radius\'][0] / angstrom\n\n\ndef download_data():\n    \'\'\'\n    Download the QM7 data set\n    \'\'\'\n    url = ""http://quantum-machine.org/data/qm7.mat""\n    print(""Downloading data"")\n    response = urlopen(url)\n    print(""Writing data"")\n    with open(""qm7.mat"", ""wb"") as f:\n        f.write(response.read())\n    print(""Data written"")\n\n\ndef convert_input(Xin):\n    \'\'\'\n    Convert the QM7 data to the proper format\n\n    This removes all the padding values from the elements and coords\n    \'\'\'\n    new = []\n    for z, r in Xin:\n        temp = [(int(x), y) for x, y in zip(z, r) if x]\n        zs, rs = zip(*temp)\n        new.append((numpy.array(zs), numpy.array(rs)))\n    return new\n\n\ndef get_fold_idxs(P, fold=0):\n    train_folds = [x for x in range(5) if x != fold]\n    train_idxs = numpy.ravel(P[train_folds])\n    test_idxs = numpy.ravel(P[fold])\n    return train_idxs, test_idxs\n\n\ndef get_data_train_test(data, fold=0):\n    train_idxs, test_idxs = get_fold_idxs(data[\'P\'], fold=fold)\n\n    y_train = data[\'T\'][train_idxs]\n    y_test = data[\'T\'][test_idxs]\n\n    R_train = data[\'R\'][train_idxs]\n    Z_train = data[\'Z\'][train_idxs]\n    Xin_train = list(zip(Z_train, R_train))\n\n    R_test = data[\'R\'][test_idxs]\n    Z_test = data[\'Z\'][test_idxs]\n    Xin_test = list(zip(Z_test, R_test))\n    return Xin_train, Xin_test, y_train, y_test\n\n\ndef load_qm7_data():\n    \'\'\'\n    Load the QM7 data set\n    \'\'\'\n    if not os.path.exists(""qm7.mat""):\n        download_data()\n    data = scipy.io.loadmat(""qm7.mat"")\n\n    R = BOHR_TO_ANGSTROMS * data[\'R\']\n    Xin = zip(data[\'Z\'], R)\n    Xout = convert_input(Xin)\n    Z, R = zip(*Xout)\n    new_data = {\n        \'P\': data[\'P\'],\n        \'R\': numpy.array(R),\n        \'Z\': numpy.array(Z),\n        \'T\': numpy.ravel(data[\'T\']),\n    }\n    return new_data\n\n\ndef load_qm7(fold=0):\n    data = load_qm7_data()\n    return get_data_train_test(data, fold=fold)\n\n\ndef plot_cell(coords, radius, unit, connections=None):\n    from mpl_toolkits.mplot3d import Axes3D  # NOQA\n    import matplotlib.pyplot as plt\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection=\'3d\')\n\n    if connections is not None:\n        for i, values in connections.items():\n            for j in values:\n                if i > j:\n                    continue\n                ax.plot([coords[i, 0], coords[j, 0]],\n                        [coords[i, 1], coords[j, 1]],\n                        [coords[i, 2], coords[j, 2]],\n                        \'--c\')\n\n    ax.scatter(coords[:, 0], coords[:, 1], coords[:, 2])\n    unit = numpy.array(unit)\n\n    vals = numpy.linspace(0, 2 * numpy.pi)\n    y = radius * numpy.cos(vals)\n    x = radius * numpy.sin(vals)\n    zeros = numpy.zeros(vals.shape)\n\n    ax.plot(x, y, \'-r\')\n    ax.plot(zeros, x, y, \'-g\')\n    ax.plot(x, zeros, y, \'-b\')\n\n    ax.plot([-radius, radius], [0, 0], \'r-\')\n    ax.plot([0, 0], [-radius, radius], \'g-\')\n    ax.plot([0, 0], [0, 0], [-radius, radius], \'b-\')\n\n    ax.plot([0, unit[0, 0]], [0, unit[1, 0]], [0, unit[2, 0]], \'-k\')\n    ax.plot([0, unit[0, 1]], [0, unit[1, 1]], [0, unit[2, 1]], \'-k\')\n    ax.plot([0, unit[0, 2]], [0, unit[1, 2]], [0, unit[2, 2]], \'-k\')\n\n    plt.show()\n'"
molml/__init__.py,0,"b'""""""\nMolML\n=====\nAn interface between molecules and machine learning\n\nMolML is a python module to use to map molecules into representations that\nare usable with machine learning. This is done using an API similar to\nscikit-learn to keep things simple and straightforward. For documentation,\nlook at the docstrings.\n""""""\n__version__ = ""0.9.0""\n'"
molml/atom.py,2,"b'""""""\nA module to compute atom based representations.\n\nThis module contains a variety of methods to extract features from molecules\nbased on the atoms in the molecule. This means that every molecule will\nresult in an array of values (n_atoms, n_features).\n""""""\nfrom builtins import range\nfrom itertools import product\n\nimport numpy\nfrom scipy.spatial.distance import cdist\n\nfrom .base import BaseFeature, SetMergeMixin, EncodedFeature, FormMixin\nfrom .utils import get_depth_threshold_mask_connections, get_coulomb_matrix\nfrom .utils import get_element_pairs, cosine_decay, get_angles\nfrom .constants import UNKNOWN\n\n\n__all__ = (""Shell"", ""LocalEncodedBond"", ""LocalEncodedAngle"",\n           ""LocalCoulombMatrix"", ""BehlerParrinello"")\n\n\nclass Shell(SetMergeMixin, BaseFeature):\n    """"""\n    A feature that counts the number of elements in a distance shell from the\n    starting atom. This is similar to the features developed in Qu et. al.\n    with the exception that it is atom-based rather than bond-based.\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    depth : int, default=1\n        The length of the atom chains to generate for connections\n\n    use_coordination : boolean, default=False\n        Specifies whether or not to use the coordination number of the atoms\n        (C1 vs C2 vs C3 vs C4).\n\n    add_unknown : boolean, default=False\n        Specifies whether or not to include an extra UNKNOWN count in the\n        feature vector.\n\n    Attributes\n    ----------\n    _elements : tuple\n        All the elements/types that are in the fit molecules.\n\n    References\n    ----------\n    Qu, X.; Latino, D. A.; Aires-de Sousa, J. A Big Data Approach to the\n    Ultra-fast Prediction of DFT-calculated Bond Energies. J. Cheminf. 2013,\n    5, 34.\n    """"""\n    ATTRIBUTES = (""_elements"", )\n    LABELS = ((""get_shell_labels"", ""_elements""), )\n\n    def __init__(self, input_type=\'list\', n_jobs=1, depth=1,\n                 use_coordination=False, add_unknown=False):\n        super(Shell, self).__init__(input_type=input_type, n_jobs=n_jobs)\n        self.depth = depth\n        self.use_coordination = use_coordination\n        self.add_unknown = add_unknown\n        self._elements = None\n\n    def _loop_depth(self, start, connections):\n        """"""\n        Loop over the depth number expanding chains. Only keep the elements\n        in the last shell.\n\n        Parameters\n        ----------\n        start : int\n            The index of the atom to start the search from\n\n        connections : dict, key->list of keys\n            A dictonary edge table with all the bidirectional connections\n\n        Returns\n        -------\n        limit : list\n            A list of index values that are at the given depth.\n        """"""\n        # This is just a slightly modified breadth-first search\n        visited = {start: 1}\n        frontier = [start]\n\n        limit = []\n        while len(frontier):\n            node = frontier.pop(0)\n            prev_depth = visited[node]\n            if prev_depth >= self.depth:\n                limit.append(node)\n                continue\n\n            for x in connections[node]:\n                if x in visited:\n                    continue\n                visited[x] = prev_depth + 1\n                frontier.append(x)\n        return limit\n\n    def _tally_limits(self, limits, elements, connections=None):\n        """"""\n        Tally limit values and return a dictonary with counts of the types.\n\n        Parameters\n        ----------\n        limits : list\n            All of the elements in the molecule at an end point\n\n        nodes : list\n            All of the element labels of the atoms\n\n        connections : dict, key->list of keys\n            A dictonary edge table with all the bidirectional connections\n\n        Returns\n        -------\n        counts : dict, element->int\n            Totals of the number of each type of element\n        """"""\n        counts = {}\n        for x in limits:\n            ele = elements[x]\n            if self.use_coordination:\n                ele += str(len(connections[x]))\n            if ele not in counts:\n                counts[ele] = 0\n            counts[ele] += 1\n        return counts\n\n    def _para_fit(self, X):\n        """"""\n        A single instance of the fit procedure.\n\n        This is formulated in a way that the fits can be done completely\n        parallel in a map/reduce fashion.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the fit\n\n        Returns\n        -------\n        value : set\n            All the elements in the molecule\n        """"""\n        data = self.convert_input(X)\n        # This is just a cheap way to approximate the actual value\n        elements = data.elements\n        connections = data.connections\n        if self.use_coordination:\n            elements = [ele + str(len(connections[i])) for i, ele in\n                        enumerate(elements)]\n        return set(elements)\n\n    def _para_transform(self, X):\n        """"""\n        A single instance of the transform procedure.\n\n        This is formulated in a way that the transformations can be done\n        completely parallel with map.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the transform\n\n        Returns\n        -------\n        value : list, shape=(n_atoms, len(self._elements))\n            The features extracted from the molecule\n\n        Raises\n        ------\n        ValueError\n            If the transformer has not been fit.\n        """"""\n        self.check_fit()\n        data = self.convert_input(X)\n\n        vectors = []\n        for atom in range(len(data.elements)):\n            limits = self._loop_depth(atom, data.connections)\n            tallies = self._tally_limits(limits, data.elements,\n                                         data.connections)\n            vec = [tallies.get(x, 0) for x in self._elements]\n            if self.add_unknown:\n                unknown = 0\n                for key, value in tallies.items():\n                    if key not in self._elements:\n                        unknown += value\n                vec.append(unknown)\n            vectors.append(vec)\n        return vectors\n\n    def get_shell_labels(self, elements):\n        if self.add_unknown:\n            elements += (UNKNOWN, )\n        return elements\n\n\nclass LocalEncodedBond(FormMixin, SetMergeMixin, EncodedFeature):\n    """"""\n    A smoothed histogram of atomic distances.\n\n    This is a method to generallize the idea of bond counting. Instead of\n    seeing bonds as a discrete count that is thresholded at a given length,\n    they are seen as general distance histograms. This is supplemented with\n    smoothing functions. This is a slight modification of the EncodedBond\n    to use with atoms.\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    segments : int, default=100\n        The number of bins/segments to use when generating the histogram.\n\n    smoothing : string or callable, default=\'norm\'\n        A string or callable to use to smooth the histogram values. If a\n        callable is given, it must take just a single argument that is a float.\n        For a list of supported default functions look at SMOOTHING_FUNCTIONS.\n\n    start : float, default=0.2\n        The starting point for the histgram sampling in angstroms.\n\n    end : float, default=6.0\n        The ending point for the histogram sampling in angstroms.\n\n    slope : float, default=20.\n        A parameter to tune the smoothing values. This is applied as a\n        multiplication before calling the smoothing function.\n\n    min_depth : int, default=0\n        A parameter to set the minimum geodesic distance to include in the\n        interactions. A value of np.inf signifies including only intermolecular\n        interactions.\n\n    max_depth : int, default=0\n        A parameter to set the maximum geodesic distance to include in the\n        interactions. A value of 0 signifies that all interactions are\n        included.\n\n    spacing : string or callable, default=\'linear\'\n        The histogram interval spacing type. Must be one of (""linear"",\n        ""inverse"", or ""log""). Linear spacing is normal spacing. Inverse takes\n        and evaluates the distances as 1/r and the start and end points are\n        1/x. For log spacing, the distances are evaluated as numpy.log(r)\n        and the start and end points are numpy.log(x). If the value is\n        callable, then it should take a float or vector of floats and return\n        a similar mapping like the other methods.\n\n    form : int, default=1\n        The histogram splitting style to use. This value changes the scaling\n        of this method to be O(E) or O(1) for 1 or 0 respectively (where E is\n        the number of elements).\n\n    add_unknown : boolean, default=False\n        Specifies whether or not to include an extra UNKNOWN count in the\n        feature vector.\n\n    use_comb_idxs : bool, default=False\n        Whether or not to use all combinations of indices when doing the\n        subselection. If this is false, a  middle out scheme will be used.\n\n    Attributes\n    ----------\n    _elements : tuple\n        A tuple of all the elements in the fit molecules.\n    """"""\n    ATTRIBUTES = (""_elements"", )\n    LABELS = ((""get_encoded_labels"", ""_elements""), )\n\n    def __init__(self, input_type=\'list\', n_jobs=1, segments=100,\n                 smoothing=\'norm\', start=0.2, end=6.0, slope=20., min_depth=0,\n                 max_depth=0, spacing=\'linear\', form=1, add_unknown=False,\n                 use_comb_idxs=False):\n        super(LocalEncodedBond, self).__init__(input_type=input_type,\n                                               n_jobs=n_jobs,\n                                               segments=segments,\n                                               smoothing=smoothing,\n                                               start=start,\n                                               end=end,\n                                               slope=slope,\n                                               spacing=spacing,\n                                               form=form,\n                                               add_unknown=add_unknown,\n                                               use_comb_idxs=use_comb_idxs)\n        self._elements = None\n        self.min_depth = min_depth\n        self.max_depth = max_depth\n\n    def _para_fit(self, X):\n        """"""\n        A single instance of the fit procedure.\n\n        This is formulated in a way that the fits can be done completely\n        parallel in a map/reduce fashion.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the fit\n\n        Returns\n        -------\n        value : set\n            All the element pairs in the molecule\n        """"""\n        data = self.convert_input(X)\n        # This is just a cheap way to approximate the actual value\n        return set((x, ) for x in data.elements)\n\n    def _iterator(self, data, idx_map):\n        mat = get_depth_threshold_mask_connections(data.connections,\n                                                   min_depth=self.min_depth,\n                                                   max_depth=self.max_depth)\n        distances = cdist(data.coords, data.coords)\n        for i, ele1 in enumerate(data.elements):\n            for j, ele2 in enumerate(data.elements):\n                if i == j or not mat[i, j]:\n                    continue\n                for idx in idx_map.get_idx_iter((ele2, ), other=(i, )):\n                    yield idx, distances[i, j], 1.\n\n    def _para_transform(self, X, y=None):\n        """"""\n        A single instance of the transform procedure.\n\n        This is formulated in a way that the transformations can be done\n        completely parallel with map.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the transform\n\n        Returns\n        -------\n        value : array, shape=(n_atoms, len(self._elements) * self.segments)\n            The features extracted from the molecule\n\n        Raises\n        ------\n        ValueError\n            If the transformer has not been fit.\n        """"""\n        self.check_fit()\n        data = self.convert_input(X)\n        idx_map = self.get_idx_map()\n        iterator = self._iterator(data, idx_map)\n        return self.encode_values(iterator, (len(data.elements), len(idx_map)),\n                                  saved_lengths=1)\n\n\nclass LocalEncodedAngle(FormMixin, SetMergeMixin, EncodedFeature):\n    r""""""\n    A smoothed histogram of atomic angles.\n\n    This method is similar to EncodedBond but for angles in molecules. This is\n    done by enumerating triplets of atoms and computing the angle between\n    them. The bins are thing smoothed with smoothing functions. This is a\n    slight modification of the EncodedAngle to work with single atoms at a\n    time. This sets the vertex of the angle to be the atom being examined.\n\n    Note: The angles used are 0 to :math:`\\pi`.\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    segments : int, default=100\n        The number of bins/segments to use when generating the histogram.\n\n    smoothing : string or callable, default=\'norm\'\n        A string or callable to use to smooth the histogram values. If a\n        callable is given, it must take just a single argument that is a float.\n        For a list of supported default functions look at SMOOTHING_FUNCTIONS.\n\n    slope : float, default=20.\n        A parameter to tune the smoothing values. This is applied as a\n        multiplication before calling the smoothing function.\n\n    min_depth : int, default=0\n        A parameter to set the minimum geodesic distance to include in the\n        interactions. A value of np.inf signifies including only intermolecular\n        interactions.\n\n    max_depth : int, default=0\n        A parameter to set the maximum geodesic distance to include in the\n        interactions. A value of 0 signifies that all interactions are\n        included.\n\n    form : int, default=2\n        The histogram splitting style to use. This value changes the scaling\n        of this method to be O(E^2), O(E), or O(1) for 2, 1, or 0 respectively\n        (where E is the number of elements).\n\n    add_unknown : boolean, default=False\n        Specifies whether or not to include an extra UNKNOWN count in the\n        feature vector.\n\n    use_comb_idxs : bool, default=False\n        Whether or not to use all combinations of indices when doing the\n        subselection. If this is false, a  middle out scheme will be used.\n\n    Attributes\n    ----------\n    _pairs : tuple\n        A tuple of all the element pairs in the fit molecules.\n    """"""\n    ATTRIBUTES = (""_pairs"", )\n    LABELS = ((""get_encoded_labels"", ""_pairs""), )\n\n    def __init__(self, input_type=\'list\', n_jobs=1, segments=100,\n                 smoothing=\'norm\', slope=20., min_depth=0, max_depth=0,\n                 r_cut=6., form=2, add_unknown=False):\n        super(LocalEncodedAngle, self).__init__(input_type=input_type,\n                                                n_jobs=n_jobs,\n                                                segments=segments,\n                                                smoothing=smoothing,\n                                                slope=slope,\n                                                start=0.,\n                                                end=numpy.pi,\n                                                form=form,\n                                                add_unknown=add_unknown)\n        self._pairs = None\n        self.min_depth = min_depth\n        self.max_depth = max_depth\n        self.r_cut = r_cut\n\n    def f_c(self, R):\n        return cosine_decay(R, r_cut=self.r_cut)\n\n    def _para_fit(self, X):\n        """"""\n        A single instance of the fit procedure.\n\n        This is formulated in a way that the fits can be done completely\n        parallel in a map/reduce fashion.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the fit\n\n        Returns\n        -------\n        value : set\n            All the element pairs in the molecule\n        """"""\n        data = self.convert_input(X)\n        # This is just a cheap way to approximate the actual value\n        return get_element_pairs(data.elements)\n\n    def _iterator(self, data, idx_map):\n        mat = get_depth_threshold_mask_connections(data.connections,\n                                                   min_depth=self.min_depth,\n                                                   max_depth=self.max_depth)\n\n        distances = cdist(data.coords, data.coords)\n        f_c = self.f_c(distances)\n        angles = get_angles(data.coords)\n        for i, ele1 in enumerate(data.elements):\n            for j, ele2 in enumerate(data.elements):\n                if i == j or not mat[i, j]:\n                    continue\n                if not f_c[i, j]:\n                    continue\n                for k, ele3 in enumerate(data.elements):\n                    if j == k or not mat[j, k]:\n                        continue\n                    if i > k:\n                        continue\n                    if not f_c[i, k] or not f_c[j, k]:\n                        continue\n\n                    F = f_c[i, j] * f_c[j, k] * f_c[i, k]\n                    for idx in idx_map.get_idx_iter((ele1, ele3), other=(j, )):\n                        yield idx, angles[i, j, k], F\n\n    def _para_transform(self, X, y=None):\n        """"""\n        A single instance of the transform procedure.\n\n        This is formulated in a way that the transformations can be done\n        completely parallel with map.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the transform\n\n        Returns\n        -------\n        value : array, shape=(n_atoms, len(self._pairs) * self.segments)\n            The features extracted from the molecule\n\n        Raises\n        ------\n        ValueError\n            If the transformer has not been fit.\n        """"""\n        self.check_fit()\n        data = self.convert_input(X)\n        idx_map = self.get_idx_map()\n        iterator = self._iterator(data, idx_map)\n        return self.encode_values(iterator, (len(data.elements), len(idx_map)),\n                                  saved_lengths=1)\n\n\nclass LocalCoulombMatrix(BaseFeature):\n    r""""""\n    An implementation of the Coulomb Matrix where only the local atom\n    environment is used by using a cutoff radius.\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    max_occupancy : int, default=4\n        The maximum number of atoms to be included the in local environment.\n\n    r_cut : float, default=6\n        The maximum distance allowed for atoms to be considered local to the\n        ""central atom"".\n\n    alpha : number, default=6\n        Some value to exponentiate the distance in the coulomb matrix.\n\n    use_reduced : bool, default=False\n        This setting uses only the first row of the local coulomb matrix and\n        the diagonal. This reduces the feature from scaling as\n        O(max_occupancy ** 2) to just O(max_occupancy).\n\n    use_decay : bool, default=False\n        This setting defines an extra decay for the values as they get futher\n        away from the ""central atom"". This is to alleviate issues the arise as\n        atoms enter or leave the cutoff radius.\n\n        .. math::\n\n            M_{ij} = \\begin{cases}\n            \\frac{Z_{p_i} Z_{p_j}}{(\\| R_{p_1} - R_{p_i} \\|_2\n                                  + \\| R_{p_1} - R_{p_j} \\|_2\n                                  + \\| R_{p_i} - R_{p_j} \\|_2 )^{\\alpha}},\n                                  & i \\neq j \\\\\n                             0.5 Z_{p_i}^{2.4} & i = j\n            \\end{cases}\n\n    References\n    ----------\n    Barker, J.; Bulin, J.;  Hamaekers, J.; Mathias, S. Localized Coulomb\n    Descriptors for the Gaussian Approximation Potential. arXiv 1611.05126\n    """"""\n    ATTRIBUTES = None\n    LABELS = ((\'get_local_coulomb_labels\', None), )\n\n    def __init__(self, input_type=\'list\', n_jobs=1, max_occupancy=4, r_cut=10.,\n                 alpha=6, use_reduced=False, use_decay=False):\n        super(LocalCoulombMatrix, self).__init__(input_type=input_type,\n                                                 n_jobs=n_jobs)\n        self.max_occupancy = max_occupancy\n        self.r_cut = r_cut\n        self.alpha = alpha\n        self.use_reduced = use_reduced\n        self.use_decay = use_decay\n\n    def fit(self, X, y=None):\n        """"""No fitting is required because it is defined by the parameters.""""""\n        return self\n\n    def _para_transform(self, X):\n        """"""\n        A single instance of the transform procedure.\n\n        This is formulated in a way that the transformations can be done\n        completely parallel with map.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the transform\n\n        Returns\n        -------\n        value : array\n            The features extracted from the molecule\n        """"""\n        data = self.convert_input(X)\n        dist = cdist(data.coords, data.coords)\n\n        numbers = numpy.array(data.numbers)\n        coords = numpy.array(data.coords)\n\n        vectors = []\n        for i in range(len(numbers)):\n            nearest = numpy.where(dist[i, :] < self.r_cut)\n            ordering = numpy.argsort(dist[i, :][nearest])\n            # Add 1 to offset for the start value\n            local_atoms = ordering[:self.max_occupancy + 1]\n            mat = get_coulomb_matrix(numbers[local_atoms],\n                                     coords[local_atoms],\n                                     alpha=self.alpha,\n                                     use_decay=self.use_decay)\n            # Take away 1 for the start value\n            n = self.max_occupancy - (len(local_atoms) - 1)\n            mat = numpy.pad(mat, ((0, n), (0, n)), ""constant"")\n            norm_vals = numpy.linalg.norm(mat, axis=0)\n            norm_vals[0] = numpy.inf\n            sorting = numpy.argsort(norm_vals)[::-1]\n            if self.use_reduced:\n                # skip the first value in the diag because it is already in\n                # the first row\n                diag = numpy.diag(mat)[1:].tolist()\n                vectors.append(mat[sorting[0]].tolist() + diag)\n            else:\n                vectors.append(mat[sorting].flatten())\n        return numpy.array(vectors)\n\n    def get_local_coulomb_labels(self):\n        idxs = []\n\n        vals = list(range(self.max_occupancy + 1))\n        if self.use_reduced:\n            for i in vals:\n                idxs.append((0, i))\n            for i in vals[1:]:\n                idxs.append((i, i))\n        else:\n            for pair in product(vals, vals):\n                idxs.append(pair)\n        return [\'local-coul_%d-%d\' % (i, j) for i, j in idxs]\n\n\nclass BehlerParrinello(SetMergeMixin, BaseFeature):\n    """"""\n    An implementation of the descriptors used in Behler-Parrinello Neural\n    Networks.\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    r_cut : float, default=6.\n        The maximum distance allowed for atoms to be considered local to the\n        ""central atom"".\n\n    r_s : float, default=1.0\n        An offset parameter for computing gaussian values between pairwise\n        distances.\n\n    eta : float, default=1.0\n        A decay parameter for the gaussian distances.\n\n    lambda_ : float, default=1.0\n        This value sets the orientation of the cosine function for the angles.\n        It should only take values in {-1., 1.}.\n\n    zeta : float, default=1.0\n        A decay parameter for the angular terms.\n\n    Attributes\n    ----------\n    _elements : tuple\n        A set of all the elements in the molecules.\n\n    _element_pairs : tuple\n        A set of all the element pairs in the molecules.\n\n    References\n    ----------\n    Behler, J; Parrinello, M. Generalized Neural-Network Representation of\n    High-Dimensional Potential-Energy Surfaces. Phys. Rev. Lett. 98, 146401.\n    """"""\n    ATTRIBUTES = (""_elements"", ""_element_pairs"")\n    LABELS = (""_elements"", (""get_chain_labels"", ""_element_pairs""))\n\n    def __init__(self, input_type=\'list\', n_jobs=1, r_cut=6.0, r_s=1., eta=1.,\n                 lambda_=1., zeta=1.):\n        super(BehlerParrinello, self).__init__(input_type=input_type,\n                                               n_jobs=n_jobs)\n        self.r_cut = r_cut\n        self.r_s = r_s\n        self.eta = eta\n        self.lambda_ = lambda_\n        self.zeta = zeta\n        self._elements = None\n        self._element_pairs = None\n\n    def _para_fit(self, X):\n        """"""\n        A single instance of the fit procedure.\n\n        This is formulated in a way that the fits can be done completely\n        parallel in a map/reduce fashion.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the fit\n\n        Returns\n        -------\n        unique_elements : set\n            A set of all the elements in the molecule\n\n        pairs : list\n            A list of all the element pairs in the molecule\n        """"""\n        data = self.convert_input(X)\n        # This is just a cheap way to approximate the actual value\n        unique_elements = set(data.elements)\n        pairs = get_element_pairs(data.elements)\n        return unique_elements, pairs\n\n    def f_c(self, R):\n        return cosine_decay(R, r_cut=self.r_cut)\n\n    def g_1(self, R, elements):\n        r""""""\n        A radial symmetry function.\n\n        .. math::\n\n            G^1_i = \\sum_{j \\neq i} \\exp(- \\eta (R_{ij} - R_s)^2) f_c(R_{ij})\n\n        Parameters\n        ----------\n        R : array, shape=(N_atoms, N_atoms)\n            A distance matrix for all the atoms (scipy.spatial.cdist)\n\n        Returns\n        -------\n        total : array, shape=(N_atoms, N_elements)\n            The atom-wise g_1 evaluations.\n        """"""\n        values = numpy.exp(-self.eta * (R - self.r_s) ** 2) * self.f_c(R)\n        numpy.fill_diagonal(values, 0)\n\n        elements = numpy.array(elements)\n\n        totals = []\n        for ele in self._elements:\n            idxs = numpy.where(elements == ele)[0]\n            total = values[:, idxs].sum(1)\n            totals.append(total)\n        return numpy.array(totals).T\n\n    def g_2(self, Theta, R, elements):\n        r""""""\n        An angular symmetry function.\n\n        .. math::\n\n            G^2_i = 2^{1-\\zeta} \\sum_{i,k \\neq i}\n                        (1 + \\lambda \\cos(\\Theta_{ijk}))^\\zeta\n                        \\exp(-\\eta (R_{ij}^2 + R_{ik}^2 + R_{jk}^2))\n                        f_c(R_{ij}) f_c(R_{ik}) f_c(R_{jk})\n\n\n        This function needs to be optimized.\n\n        Parameters\n        ----------\n        Theta : array, shape=(N_atoms, N_atoms, N_atoms)\n            An array of triplet angles.\n\n        R : array, shape=(N_atoms, N_atoms)\n            A distance matrix for all the atoms (scipy.spatial.cdist).\n\n        elements : list\n            A list of all the elements in the molecule.\n\n        Returns\n        -------\n        total : array, shape=(N_atoms, len(self._element_pairs))\n            The atom-wise g_2 evaluations.\n        """"""\n        F_c_R = self.f_c(R)\n\n        R2 = self.eta * R ** 2\n        new_Theta = (1 + self.lambda_ * Theta) ** self.zeta\n        mapping = {x: i for i, x in enumerate(self._element_pairs)}\n        n = R.shape[0]\n        values = numpy.zeros((n, len(mapping)))\n        for i in range(n):\n            for j in range(n):\n                if i == j or not F_c_R[i, j]:\n                    continue\n                ele1 = elements[j]\n\n                for k in range(n):\n                    if k == i or j == k:\n                        continue\n                    if not F_c_R[i, k] or not F_c_R[j, k]:\n                        continue\n                    ele2 = elements[k]\n                    eles = (ele1, ele2) if ele1 < ele2 else (ele2, ele1)\n\n                    exp_term = numpy.exp(-(R2[i, j] + R2[i, k] + R2[j, k]))\n                    angular_term = new_Theta[i, j, k]\n                    radial_cuts = F_c_R[i, j] * F_c_R[i, k] * F_c_R[j, k]\n                    temp = angular_term * exp_term * radial_cuts\n                    try:\n                        values[i, mapping[eles]] += temp\n                    except KeyError:\n                        pass\n        return 2 ** (1 - self.zeta) * values\n\n    def calculate_Theta(self, R_vecs):\n        r""""""\n        Compute the angular term for all triples of atoms.\n\n        .. math::\n\n            \\Theta_{ijk} = (R_{ij} . R_{ik}) / (|R_{ij}| |R_{ik}|)\n\n        Right now this is a fairly naive implementation so this could be\n        optimized quite a bit.\n\n        Parameters\n        ----------\n        R_vecs : array, shape=(N_atoms, 3)\n            An array of the Cartesian coordinates of all the atoms\n\n        Returns\n        -------\n        Theta : array, shape=(N_atoms, N_atoms, N_atoms)\n            The angular term for all the atoms given.\n        """"""\n        n = R_vecs.shape[0]\n        Theta = numpy.zeros((n, n, n))\n        for i, Ri in enumerate(R_vecs):\n            for j, Rj in enumerate(R_vecs):\n                if i == j:\n                    continue\n                Rij = Ri - Rj\n                normRij = numpy.linalg.norm(Rij)\n                for k, Rk in enumerate(R_vecs):\n                    if i == k or j == k:\n                        continue\n                    Rik = Ri - Rk\n                    normRik = numpy.linalg.norm(Rik)\n                    Theta[i, j, k] = numpy.dot(Rij, Rik) / (normRij * normRik)\n        return Theta\n\n    def _para_transform(self, X):\n        """"""\n        A single instance of the transform procedure.\n\n        This is formulated in a way that the transformations can be done\n        completely parallel with map.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the transform\n\n        Returns\n        -------\n        value : array\n            The features extracted from the molecule\n\n        Raises\n        ------\n        ValueError\n            If the transformer has not been fit.\n        """"""\n        self.check_fit()\n\n        data = self.convert_input(X)\n\n        coords = numpy.array(data.coords)\n        R = cdist(coords, coords)\n        Theta = self.calculate_Theta(coords)\n\n        g1 = self.g_1(R, data.elements)\n        g2 = self.g_2(Theta, R, data.elements)\n        return numpy.hstack([g1, g2])\n\n    def get_chain_labels(self, chains):\n        return [\'-\'.join(x) for x in chains]\n'"
molml/base.py,0,"b'""""""\nA collection of all the base transformer constructions.\n\nThis module is a collection of all the base classes and mixins for use with\nthe other transformers.\n""""""\nimport inspect\nimport multiprocessing\nfrom functools import reduce\nimport json\n\nimport numpy\nfrom pathos.multiprocessing import ProcessingPool as Pool\n\nfrom .utils import get_smoothing_function, get_spacing_function\nfrom .utils import LazyValues, IndexMap\nfrom .io import read_file_data\n\n\ndef _func_star(args):\n    """"""\n    A function and argument expanding helper function.\n\n    The first item in args is callable, and the remainder of the items are\n    used as expanded arguments. This is to make the function header for reduce\n    the same for the normal and parallel versions. Otherwise, the functions\n    would have to do their own unpacking of arguments.\n    """"""\n    f = args[0]\n    args = args[1:]\n    return f(*args)\n\n\nclass BaseFeature(object):\n    """"""\n    A base class for all the features.\n\n    Parameters\n    ----------\n    input_type : str, list of str, or callable, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\',\n        \'filename\', a list of strings, or a callable). If it is a list of\n        strings, the strings tell the order of (and if they are included) the\n        different molecule attributes (coords, elements, numbers,\n        connections). If a callable is given, then it is assumed to return a\n        LazyValues object.\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n    """"""\n    def __init__(self, input_type=\'list\', n_jobs=1):\n        self.input_type = input_type\n        self.n_jobs = n_jobs\n\n    def _get_param_strings(self):\n        argspec = inspect.getargspec(type(self).__init__)\n        # Delete the only non-keyword argument\n        args = [x for x in argspec.args if x != ""self""]\n        values = [getattr(self, x) for x in args]\n        return [""%s=%r"" % (x, y) for x, y in zip(args, values)]\n\n    def __repr__(self):\n        name = type(self).__name__\n        params = self._get_param_strings()\n        return ""%s(%s)"" % (name, \', \'.join(params))\n\n    def set_params(self, **kwargs):\n        """"""\n        Set the feature parameter values.\n\n        Parameters\n        ----------\n            kwargs : kwargs\n                Key value pairs to set for the feature parameters. Keys that\n                are not valid parameters will be ignored.\n        """"""\n        for key, value in kwargs.items():\n            try:\n                getattr(self, key)\n                setattr(self, key, value)\n            except AttributeError:\n                continue\n\n    def get_params(self):\n        """"""\n        Get a dictonary of all the feature parameters.\n\n        Returns\n        -------\n            params : dict\n                A dictonary of all the feature parameters.\n        """"""\n        argspec = inspect.getargspec(type(self).__init__)\n        # Delete the only non-keyword argument\n        args = [x for x in argspec.args if x != ""self""]\n        values = [getattr(self, x) for x in args]\n        return {key: value for key, value in zip(args, values)}\n\n    def slugify(self):\n        """"""\n        Convert an instance to a simple string.\n\n        Returns\n        -------\n        string : str\n            The slug string\n\n        """"""\n        name = type(self).__name__\n        # Skip the first two parameters\n        params = self._get_param_strings()[2:]\n        string = \'__\'.join([name] + params).replace(""\'"", \'\')\n        return string\n\n    def convert_input(self, X):\n        """"""\n        Convert the input (as specified in self.input_type) to a usable form.\n\n        Parameters\n        ----------\n        X : list or string (depends on the instance value of input_type)\n            An object that stores the data for a single molecule. See the\n            Notes for more details.\n\n        Returns\n        -------\n        values : Object\n            An object that allows the lazy evaluation of different properties\n\n        Raises\n        ------\n        ValueError\n            If the input_type given is not allowed.\n\n        Notes\n        -----\n        If input_type is \'list\', then it must be an iterable of (elements,\n        coodinates pairs) for each molecule. Where the elements are an\n        iterable of the form (ele1, ele2, ..., elen) and coordinates are an\n        iterable of the form [(x1, y1, z1), (x2, y2, z2), ..., (xn, yn,\n        zn)]. This allows allows for connections to be included. This is a\n        dictionary where the keys are the indices of the atoms and the\n        values are dictonaries with the key being another index and the\n        value is the bond order (one of \'1\', \'Ar\', \'2\', or \'3\').\n        Example for methane::\n\n            {\n                0: {1: ""1"", 2: ""1"", 3: ""1"", 4: ""1""},\n                1: {0: ""1""},\n                2: {0: ""1""},\n                3: {0: ""1""},\n                4: {0: ""1""},\n            }\n\n        If input_type is \'filename\', then it must be an iterable of\n        paths/filenames for each molecule. Currently, the supported formats\n        are: xyz, mol2, and a simple xyz format (.out).\n\n        If input_type is a list, then they will be treated as labels to\n        each of the arguments passed in via a tuple. For example,\n        input_type=""list"" can be reproduced with [""elements"", ""coords""]\n        or [""elements"", ""coords"", ""connections""].\n\n        If input_type is a callable, then it is assumed that the callable\n        returns a LazyValues object.\n        """"""\n        connections = None\n        if self.input_type == ""list"":\n            try:\n                first, coords = X\n            except ValueError:\n                first, coords, connections = X\n            if len(first) and isinstance(first[0], str):\n                values = LazyValues(elements=first, coords=coords,\n                                    connections=connections)\n            else:\n                values = LazyValues(numbers=first, coords=coords,\n                                    connections=connections)\n        elif self.input_type == ""filename"":\n            values = read_file_data(X)\n        elif type(self.input_type) in (list, tuple):\n            d = {x: y for x, y in zip(self.input_type, X)}\n            values = LazyValues(**d)\n        elif callable(self.input_type):\n            values = self.input_type(X)\n        else:\n            raise ValueError(""The input_type \'%s\' is not allowed."" %\n                             self.input_type)\n        return values\n\n    def get_labels(self):\n        """"""\n        Get the labels for the features in the transformer\n\n        Returns\n        -------\n        values : tuple\n            All of the labels of the resulting features.\n        """"""\n        if self.LABELS is None:\n            return tuple()\n\n        values = []\n        for x in self.LABELS:\n            try:\n                func_name, data_name = x\n                func = getattr(self, func_name)\n                if data_name is None:\n                    temp = func()\n                else:\n                    data = getattr(self, data_name)\n                    temp = func(data)\n            except (TypeError, ValueError) as e:\n                if len(x) == 2:\n                    # Error in calling the function\n                    raise e\n                temp = getattr(self, x)\n            values.append(tuple(temp))\n        return sum(values, tuple())\n\n    def check_fit(self):\n        """"""\n        Check if the transformer has been fit\n\n        Raises\n        ------\n        ValueError\n            The transformer has not been fit.\n        """"""\n        if self.ATTRIBUTES is None:\n            return\n\n        msg = ""This %s instance is not fitted yet. Call \'fit\' first.""\n        for key in self.ATTRIBUTES:\n            if getattr(self, key) is None:\n                raise ValueError(msg % type(self).__name__)\n\n    @classmethod\n    def get_citation(self):\n        try:\n            docs = self.__doc__\n            idx = docs.index(""References"")\n            values = [x.strip() for x in docs[idx:].split(\'\\n\')[2:]]\n            new_values = [[]]\n            for value in values:\n                if ""----"" in value:\n                    new_values.pop()\n                    break\n                elif not value:\n                    new_values.append([])\n                else:\n                    new_values[-1].append(value)\n            strings = [\' \'.join(x) for x in new_values if x]\n            return \'\\n\'.join(strings)\n        except ValueError:\n            return ""MolML https://github.com/crcollins/molml""\n\n    def map(self, f, seq):\n        """"""\n        Parallel implementation of map.\n\n        Parameters\n        ----------\n        f : callable\n            A function to map to all the values in \'seq\'\n\n        seq : iterable\n            An iterable of values to process with \'f\'\n\n        Returns\n        -------\n        results : list, shape=[len(seq)]\n            The evaluated values\n        """"""\n        if self.n_jobs < 1:\n            n_jobs = multiprocessing.cpu_count()\n        elif self.n_jobs == 1:\n            return list(map(f, seq))\n        else:\n            n_jobs = self.n_jobs\n\n        pool = Pool(n_jobs)\n        results = list(pool.map(f, seq))\n        # Closing/joining is not really allowed because pathos sees pools as\n        # lasting for the duration of the program.\n        return results\n\n    def reduce(self, f, seq):\n        """"""\n        Parallel implementation of reduce.\n\n        This changes the problem from being O(n) steps to O(lg n)\n\n        Parameters\n        ----------\n        f : callable\n            A function to use to reduce the values of \'seq\'\n\n        seq : iterable\n            An iterable of values to process\n\n        Returns\n        -------\n        results : object\n            A single reduced object based on \'seq\' and \'f\'\n        """"""\n        if self.n_jobs == 1:\n            return reduce(f, seq)\n\n        while len(seq) > 1:\n            pairs = [(f, x, y) for x, y in zip(seq[::2], seq[1::2])]\n            temp_seq = self.map(_func_star, pairs)\n            # If the sequence length is odd add the last element on\n            # This is because it will not get included with the zip\n            if len(seq) % 2:\n                temp_seq.append(seq[-1])\n            seq = temp_seq\n        return seq[0]\n\n    def fit(self, X, y=None):\n        """"""\n        Fit the model.\n\n        Parameters\n        ----------\n        X : list, shape=(n_samples, )\n            A list of objects to use to fit.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        """"""\n        raise NotImplementedError\n\n    def _para_transform(self, X):\n        """"""\n        A single instance of the transform procedure.\n\n        This is formulated in a way that the transformations can be done\n        completely parallel with map.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the transform\n\n        Returns\n        -------\n        value : array-like\n            The features extracted from the molecule\n        """"""\n        raise NotImplementedError\n\n    def transform(self, X, y=None):\n        """"""\n        Framework for a potentially parallel transform.\n\n        Parameters\n        ----------\n        X : list, shape=(n_samples, )\n            A list of objects to use to transform\n\n        Returns\n        -------\n        array : array, shape=(n_samples, n_features)\n            The transformed features\n        """"""\n        self.check_fit()\n        results = self.map(self._para_transform, X)\n        return numpy.array(results)\n\n    def fit_transform(self, X, y=None):\n        """"""\n        A naive default implementation of fitting and transforming.\n\n        Parameters\n        ----------\n        X : list, shape=(n_samples, )\n            A list of objects to use to fit and then transform\n\n        Returns\n        -------\n        array : array, shape=(n_samples, n_features)\n            The transformed features\n        """"""\n        self.fit(X, y)\n        return self.transform(X, y)\n\n    def to_json(self):\n        """"""\n        Return model data as a json compatible dict\n\n        This will recursively convert other transformer objects as well.\n\n        Returns\n        -------\n        data : dict\n            The json data\n        """"""\n        attributes = {}\n        if self.ATTRIBUTES is not None:\n            attributes = {key: getattr(self, key) for key in self.ATTRIBUTES}\n\n        full_name = self.__module__ + \'.\' + self.__class__.__name__\n        params = {}\n        for key, value in self.get_params().items():\n            try:\n                params[key] = value.to_json()\n            except AttributeError:\n                params[key] = value\n\n        data = {\n                ""transformer"": full_name,\n                ""parameters"": params,\n                ""attributes"": attributes,\n        }\n        return data\n\n    def save_json(self, f):\n        """"""\n        Save the model data in a json file\n\n        Parameters\n        ----------\n        f : str or file descriptor\n            The path to save the data or a file descriptor to save it to.\n        """"""\n        data = self.to_json()\n        try:\n            json.dump(data, f)\n        except AttributeError:\n            with open(f, \'w\') as out_file:\n                json.dump(data, out_file)\n\n\nclass SetMergeMixin(object):\n    """"""\n    A simple mixin that will merge sets.\n\n    This mixin replaces all the duplicate code that just merges sets when\n    doing the parallel fits. For this to work, it requires that the subclasses\n    define `ATTRIBUTES`.\n    """"""\n    def fit(self, X, y=None):\n        """"""\n        Fit the model.\n\n        Parameters\n        ----------\n        X : list, shape=(n_samples, )\n            A list of objects to use to fit.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        """"""\n        res = self.map(self._para_fit, X)\n        if len(self.ATTRIBUTES) > 1:\n            temp = self.reduce(lambda x, y: tuple(set(xx) | set(yy)\n                                                  for xx, yy in zip(x, y)),\n                               res)\n            for attr, vals in zip(self.ATTRIBUTES, temp):\n                setattr(self, attr, tuple(sorted(set(vals))))\n        else:\n            vals = self.reduce(lambda x, y: set(x) | set(y), res)\n            setattr(self, self.ATTRIBUTES[0], tuple(sorted(set(vals))))\n        return self\n\n\nclass FormMixin(object):\n    """"""\n    A simple mixin for handling form transformations\n\n    This mixin handles all how index mapping is done when going from higher\n    dimensional attributes to lower dimensional ones. By default, this mixin\n    uses the first value in ATTRIBUTES as the basis for the index mapping.\n\n    use_comb_idxs : bool, default=False\n        Whether or not to use all combinations of indices when doing the\n        subselection. If this is false, a  middle out scheme will be used.\n    """"""\n    def __init__(self, form=1, add_unknown=False, use_comb_idxs=False,\n                 *args, **kwargs):\n        super(FormMixin, self).__init__(*args, **kwargs)\n        self.form = form\n        self.add_unknown = add_unknown\n        self.use_comb_idxs = use_comb_idxs\n        self._idx_map = None\n\n    def get_idx_map(self):\n        """"""\n        Lazily load the idx_map.\n\n        Returns\n        -------\n        idx_map : IndexMap\n            The IndexMap object for this form and add_unknown setting.\n        """"""\n        values = getattr(self, self.ATTRIBUTES[0])\n        if self._idx_map is None or not self._idx_map.is_valid(values):\n            self._idx_map = IndexMap(values, self.form, self.add_unknown,\n                                     self.use_comb_idxs)\n        return self._idx_map\n\n    def get_group_order(self, groups):\n        """"""\n        Parameters\n        ----------\n        groups : list\n            A list of all the groups. This is ignored.\n\n        Returns\n        -------\n        value_order : list\n            A list of all groups in order.\n        """"""\n        return self.get_idx_map().get_value_order()\n\n    def transform(self, X, y=None):\n        """"""\n        Framework for a potentially parallel transform.\n\n        Parameters\n        ----------\n        X : list, shape=(n_samples, )\n            A list of objects to use to transform\n\n        Returns\n        -------\n        array : array, shape=(n_samples, n_features)\n            The transformed features\n        """"""\n        self.check_fit()\n        # This is to ensure that the idx_map exists before potentially\n        # duplicating the process for parallelization.\n        self.get_idx_map()\n        results = self.map(self._para_transform, X)\n        return numpy.array(results)\n\n\nclass InputTypeMixin(object):\n    """"""\n    A simple mixin to to check input_types if there are multiples.\n\n    This mixin adds a method to check if a transformer parameter does not have\n    the same input_type as the parent object.\n    """"""\n    def check_transformer(self, transformer):\n        """"""\n        Check a transformer.\n\n        Parameters\n        ----------\n        transformer : BaseFeature\n            A transformer object.\n\n        Raises\n        ------\n        ValueError\n            If the input_type pairing given is not allowed.\n        """"""\n        if self.input_type is None:\n            if transformer is not None:\n                self.input_type = transformer.input_type\n            else:\n                # Standard default\n                self.input_type = \'list\'\n        elif transformer is not None:\n            if self.input_type != transformer.input_type:\n                string = ""The input_type for transformer (%r) does not ""\n                string += ""match the input_type of this %s (%r)""\n                raise ValueError(string % (transformer.input_type,\n                                           self.__class__.__name__,\n                                           self.input_type))\n\n\nclass EncodedFeature(BaseFeature):\n    """"""\n    This is a generalized class to handle all kinds of encoding feature\n    representations. These approaches seem to be a fairly general way of\n    making lists of scalar values more effective to use in machine learning\n    models. Essentially, it can be viewed as kernel smoothed histograms over\n    the values of interest.\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    segments : int, default=100\n        The number of bins/segments to use when generating the histogram.\n        Empirically, it has been found that values beyond 50-100 have little\n        benefit.\n\n    smoothing : string or callable, default=\'norm\'\n        A string or callable to use to smooth the histogram values. If a\n        callable is given, it must take just a single argument that is a float\n        (or vector of floats). For a list of supported default functions look\n        at SMOOTHING_FUNCTIONS.\n\n    start : float, default=0.2\n        The starting point for the histgram sampling in angstroms.\n\n    end : float, default=6.0\n        The ending point for the histogram sampling in angstroms.\n\n    slope : float, default=20.\n        A parameter to tune the smoothing values. This is applied as a\n        multiplication before calling the smoothing function.\n\n    spacing : string or callable, default=\'linear\'\n        The histogram interval spacing type. Must be one of (""linear"",\n        ""inverse"", or ""log""). Linear spacing is normal spacing. Inverse takes\n        and evaluates the distances as 1/r and the start and end points are\n        1/x. For log spacing, the distances are evaluated as numpy.log(r)\n        and the start and end points are numpy.log(x). If the value is\n        callable, then it should take a float or vector of floats and return\n        a similar mapping like the other methods.\n\n    References\n    ----------\n    Collins, C.; Gordon, G.; von Lilienfeld, O. A.; Yaron, D. Constant Size\n    Molecular Descriptors For Use With Machine Learning. arXiv:1701.06649\n    """"""\n    def __init__(self, input_type=\'list\', n_jobs=1, segments=100,\n                 smoothing=\'norm\', slope=20., start=0.2, end=6.,\n                 spacing=\'linear\'):\n        super(EncodedFeature, self).__init__(input_type=input_type,\n                                             n_jobs=n_jobs)\n        self.segments = segments\n        self.smoothing = smoothing\n        self.slope = slope\n        self.start = start\n        self.end = end\n        self.spacing = spacing\n\n    def _get_theta_info(self):\n        theta_func = get_spacing_function(self.spacing)\n        theta = numpy.linspace(theta_func(self.start), theta_func(self.end),\n                               self.segments)\n        return theta, theta_func\n\n    def encode_values(self, iterator, lengths, saved_lengths=0):\n        \'\'\'\n        Encodes an iterable of values into a uniform length array. These\n        values can then be indexed to allow binning them in different sections\n        of the array. After the values are processed, the array can by\n        flattened down to a desired number of axes.\n\n        Parameters\n        ----------\n            iterator : iterable\n                The collection of values to encode. Each item in the iterable\n                must contain values for (idx, value, scaling). Where idx is a\n                tuple of integer values indicating which encoding bucket the\n                values go in, value is the value to encode, and scaling is a\n                factor that gets multiplied by the final encoded subvector\n                before getting added to the total (This is mostly used to mask\n                values and scale their influence with distance. If idx is None,\n                then the value will be skipped.\n\n            lengths : tuple of ints\n                The number of encoding axes to create. In terms of\n                EncodedBonds, this would be the number of element pairs.\n\n            saved_lengths : ints\n                The number of axis components to retain. The order that they\n                get saved is the same order that is given in lengths. For\n                example, when doing atom encodings, this should be 1 to retain\n                the atom axis.\n\n        Returns\n        -------\n            vector : array\n                The final concatenated vector of all the subvectors. This will\n                have a shape of lengths[:saved_lengths]\n                + product(lengths[saved_lengths:]) * segments).\n        \'\'\'\n        smoothing_func = get_smoothing_function(self.smoothing)\n        vector = numpy.zeros(tuple(lengths) + (self.segments, ))\n        theta, theta_func = self._get_theta_info()\n\n        for idxs, value, scaling in iterator:\n            if idxs is None:\n                continue\n            diff = theta - theta_func(value)\n            value = smoothing_func(diff, self.slope)\n            vector[tuple(idxs)] += value * scaling\n\n        reshape = tuple(lengths)[:saved_lengths] + (-1, )\n        return vector.reshape(*reshape)\n\n    def get_group_order(self, groups):\n        """"""\n        Parameters\n        ----------\n        groups : list\n            A list of all the groups.\n\n        Returns\n        -------\n        value_order : list\n            A list of all groups in order.\n        """"""\n        return groups\n\n    def get_encoded_labels(self, groups):\n        """"""\n        Parameters\n        ----------\n        groups : list\n            A list of all the groups.\n\n        Returns\n        -------\n        labels : list\n            A list of all the feature labels.\n        """"""\n        theta, theta_func = self._get_theta_info()\n        labels = []\n        for group in self.get_group_order(groups):\n            name = \'-\'.join(group)\n            for x in theta:\n                labels.append(\'%s_%s\' % (name, round(x, 5)))\n        return labels\n'"
molml/constants.py,0,"b'from bidict import bidict\n\nELE_TO_NUM = bidict({\n    \'H\': 1,\n    \'C\': 6,\n    \'N\': 7,\n    \'O\': 8,\n    \'F\': 9,\n    \'P\': 15,\n    \'S\': 16,\n    \'Cl\': 17,\n    \'Br\': 35,\n    \'I\': 53,\n    \'Se\': 34\n})\nTYPE_ORDER = [\'1\', \'Ar\', \'2\', \'3\']\nBOND_LENGTHS = {\n    ""C"": {\n        ""3"":   0.62,\n        ""2"":   0.69,\n        ""Ar"": 0.72,\n        ""1"":   0.85,\n    },\n    ""Cl"": {\n        ""1"":   1.045,\n    },\n    ""F"": {\n        ""1"":   1.23,\n    },\n    ""H"": {\n        ""1"":   0.6,\n    },\n    ""N"": {\n        ""3"":   0.565,\n        ""2"":   0.63,\n        ""Ar"": 0.655,\n        ""1"":   0.74,\n    },\n    ""O"": {\n        ""3"":   0.53,\n        ""2"":   0.59,\n        ""Ar"": 0.62,\n        ""1"":   0.695,\n    },\n    ""P"": {\n        ""2"":   0.945,\n        ""Ar"": 0.985,\n        ""1"":   1.11,\n    },\n    ""S"": {\n        ""2"":   0.905,\n        ""Ar"": 0.945,\n        ""1"":   1.07,\n    },\n}\n# From wiki\nELECTRONEGATIVITY = {\n    ""C"": 2.55,\n    ""Cl"": 3.16,\n    ""F"": 3.98,\n    ""H"": 2.20,\n    ""N"": 3.04,\n    ""O"": 3.44,\n    ""P"": 2.19,\n    ""S"": 2.58,\n}\nUNKNOWN = \'UNKNOWN\'\n'"
molml/crystal.py,0,"b'""""""\nA module to compute molecule based representations.\n\nThis module contains a variety of methods to extract features from molecules\nbased on the entire molecule. All of the methods included here will produce\none vector per molecule input.\n""""""\nimport numpy\nimport scipy\n\nfrom .base import BaseFeature, InputTypeMixin\nfrom .molecule import CoulombMatrix\nfrom .utils import _radial_iterator\n\n\n__all__ = (""GenerallizedCrystal"", ""EwaldSumMatrix"", ""SineMatrix"")\n\n\nclass GenerallizedCrystal(InputTypeMixin, BaseFeature):\n    """"""\n    A wrapper around other features to facilitate faking crystals.\n\n    This is done by a brute force expansion of atoms in the molecules based on\n    a given unit cell. This is highly inefficient, but it does set a baseline.\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    transformer : BaseFeature, default=None\n        The transformer that will be used once the atoms have been expanded\n        into the crystal.\n\n    radius : float, default=None\n        The cutoff radius for including unit cells in angstroms.\n\n    units : list or int, default=None\n        The number of unit cells to include for each axis (if this is an int,\n        then it is the same for all).\n\n    References\n    ----------\n    Faber, F.; Lindmaa, A; von Lilienfeld, O. A.; Armiento, R. Crystal\n    Structure Representations for Machine Learning Models of Formation\n    Energies. arXiv:1503.07406\n    """"""\n    ATTRIBUTES = None\n    LABELS = None\n\n    def __init__(self, input_type=None, n_jobs=1, transformer=None,\n                 radius=None, units=None):\n        super(GenerallizedCrystal, self).__init__(input_type=input_type,\n                                                  n_jobs=n_jobs)\n        self.check_transformer(transformer)\n        self.transformer = transformer\n        if radius is not None and units is not None:\n            msg = ""`radius` and `units` can not be set at the same time.""\n            raise ValueError(msg)\n        self.radius = radius\n        self.units = units\n\n        self._old_convert_input = self.transformer.convert_input\n        self.transformer.convert_input = self.convert_input\n\n    def convert_input(self, X):\n        temp = self._old_convert_input(X)\n        temp.fill_in_crystal(radius=self.radius, units=self.units)\n        return temp\n\n    def fit(self, X, y=None):\n        return self.transformer.fit(X)\n\n    def fit_transform(self, X, y=None):\n        return self.transformer.fit_transform(X)\n\n    def transform(self, X, y=None):\n        return self.transformer.transform(X)\n\n\nclass EwaldSumMatrix(CoulombMatrix):\n    r""""""\n    In this construction, we use a similar form to the Ewald sum of breaking\n    the interaction into three parts and adding them together.\n\n    The interaction between two atoms is defined as follows\n\n    .. math::\n        x_{ij} = x_{ij}^{(r)} + x_{ij}^{(m)} + x_{ij}^0.\n\n\n    The components are defined as follows\n\n    .. math::\n\n        x_{ij}^{(r)} = Z_i Z_j\n        \\sum_L \\frac{\\text{erfc}(\\alpha \\| r_i - r_j + L \\|_2)}\n                                        {\\| r_i - r_j + L \\|_2}\n\n        x_{ij}^{(m)} = \\frac{Z_i Z_j}{\\pi V}\n                            \\sum_G \\frac{e^{-\\|G\\|_2^2 / (2 \\alpha)^2}}\n                            {\\|G\\|_2^2} \\cos(G \\cdot (r_i - r_j))\n\n        x_{ij}^0 = -(Z_i^2 + Z_j^2) \\frac{\\alpha}{\\sqrt{\\pi}} -\n                    (Z_i + Z_j)^2 \\frac{\\pi}{2 V \\alpha^2}\n\n        x_{ii} = -Z_i^2 \\frac{\\alpha}{\\sqrt{\\pi}} -\n                  Z_i^2 \\frac{\\pi}{2 V \\alpha^2}\n\n        \\alpha = \\sqrt{\\pi} \\left(\\frac{0.01 M}{V}\\right)^{1/6}\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    sort : bool, default=False\n        Specifies whether or not to sort the coulomb matrix based on the\n        sum of the rows (same as L1 norm).\n\n    eigen : bool, default=False\n        Specifies whether or not to use the eigen spectrum of the coulomb\n        matrix rather than the matrix itself. This changes the scaling to be\n        linear in the number of atoms.\n\n    Attributes\n    ----------\n    _max_size : int\n        The size of the largest molecule in the fit molecules by number of\n        atoms.\n\n    References\n    ----------\n    Faber, F.; Lindmaa, A; von Lilienfeld, O. A.; Armiento, R. Crystal\n    Structure Representations for Machine Learning Models of Formation\n    Energies. arXiv:1503.07406\n    """"""\n    ATTRIBUTES = (""_max_size"", )\n    LABELS = None\n\n    def __init__(self, input_type=\'list\', n_jobs=1, L_max=10, G_max=10,\n                 sort=False, eigen=False):\n        super(EwaldSumMatrix, self).__init__(input_type=input_type,\n                                             n_jobs=n_jobs)\n        self._max_size = None\n        self.L_max = L_max\n        self.G_max = G_max\n        self.sort = sort\n        self.eigen = eigen\n\n    def _para_transform(self, X):\n        """"""\n        A single instance of the transform procedure.\n\n        This is formulated in a way that the transformations can be done\n        completely parallel with map.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the transform\n\n        Returns\n        -------\n        value : array\n            The features extracted from the molecule\n\n        Raises\n        ------\n        ValueError\n            If the transformer has not been fit.\n\n        ValueError\n            If the size of the transforming molecules are larger than the fit.\n        """"""\n        self.check_fit()\n\n        data = self.convert_input(X)\n        if len(data.numbers) > self._max_size:\n            msg = ""The fit molecules (%d) were not as large as the ones that""\n            msg += "" are being transformed (%d).""\n            raise ValueError(msg % (self._max_size, len(data.numbers)))\n\n        ZZ = numpy.outer(data.numbers, data.numbers)\n        numpy.fill_diagonal(ZZ, 0)\n        rr = data.coords[:, None] - data.coords\n        erfc = scipy.special.erfc\n        norm = numpy.linalg.norm\n        B = data.unit_cell\n        Binv = 2 * numpy.pi * numpy.linalg.inv(B)\n        V = numpy.linalg.det(B)\n\n        alpha = numpy.pi ** 0.5 * (0.01 * len(data.numbers) / V) ** (1./6)\n\n        # Short range interactions\n        xr = numpy.zeros(ZZ.shape)\n        for L in _radial_iterator(B, self.L_max):\n            # TODO: optimize symmetry\n            temp = norm(rr + L, axis=2)\n            with numpy.errstate(divide=\'ignore\'):\n                xr += erfc(alpha * temp) / temp\n        with numpy.errstate(invalid=\'ignore\'):\n            xr *= ZZ\n\n        # Long range interactions\n        xm = numpy.zeros(ZZ.shape)\n        for G in _radial_iterator(Binv, self.G_max):\n            # TODO: optimize symmetry\n            temp = norm(G) ** 2\n            if not temp:\n                continue\n            first = numpy.exp(-temp / (2*alpha) ** 2) / temp\n            second = numpy.cos(rr.dot(G))\n            xm += first * second\n        xm *= 1. / (numpy.pi * V) * ZZ\n\n        # Constant\n        num2 = data.numbers ** 2\n        factor = numpy.pi / (2 * V * alpha ** 2)\n        x0 = numpy.add.outer(num2, num2) * alpha/numpy.pi - ZZ ** 2 * factor\n\n        # Penultimate\n        values = xr + xm + x0\n\n        # Final\n        xii = alpha / numpy.sqrt(numpy.pi) + factor\n        xii *= -num2\n        numpy.fill_diagonal(values, xii)\n\n        if self.sort:\n            order = numpy.argsort(values.sum(0))[::-1]\n            values = values[order, :][:, order]\n\n        if self.eigen:\n            values = numpy.linalg.eig(values)[0]\n\n        padding_difference = self._max_size - len(data.numbers)\n        values = numpy.pad(values,\n                           (0, padding_difference),\n                           mode=""constant"")\n        return values.reshape(-1)\n\n\nclass SineMatrix(CoulombMatrix):\n    r""""""\n    A molecular descriptor based on Coulomb interactions.\n\n    This is a feature that uses a Coulomb-like interaction between all atoms\n    in the molecule to generate a matrix that is then vectorized.\n\n    .. math::\n\n        C_{ij} = \\begin{cases}\n        Z_i Z_j \\Phi(r_i, r_j) & i \\neq j \\\\\n                          0.5 Z_i^{2.4} & i = j\n        \\end{cases}\n\n    Where :math:`\\Phi(r_i, r_j)`\n\n    .. math::\n\n        \\| B \\cdot \\sum_{k={x,y,z}} \\hat e_k \\sin^2 \\left[\n                    \\pi \\hat e_k B^{-1} \\cdot (r_i - r_j) \\right] \\|_2^{-1}\n\n    and :math:`B` is a matrix of the lattice basis vectors.\n\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    sort : bool, default=False\n        Specifies whether or not to sort the coulomb matrix based on the\n        sum of the rows (same as L1 norm).\n\n    eigen : bool, default=False\n        Specifies whether or not to use the eigen spectrum of the coulomb\n        matrix rather than the matrix itself. This changes the scaling to be\n        linear in the number of atoms.\n\n    Attributes\n    ----------\n    _max_size : int\n        The size of the largest molecule in the fit molecules by number of\n        atoms.\n\n    References\n    ----------\n    Faber, F.; Lindmaa, A; von Lilienfeld, O. A.; Armiento, R. Crystal\n    Structure Representations for Machine Learning Models of Formation\n    Energies. arXiv:1503.07406\n    """"""\n    ATTRIBUTES = (""_max_size"", )\n    LABELS = None\n\n    def __init__(self, input_type=\'list\', n_jobs=1, sort=False, eigen=False):\n        super(SineMatrix, self).__init__(input_type=input_type,\n                                         n_jobs=n_jobs)\n        self._max_size = None\n        self.sort = sort\n        self.eigen = eigen\n\n    def _para_transform(self, X):\n        """"""\n        A single instance of the transform procedure.\n\n        This is formulated in a way that the transformations can be done\n        completely parallel with map.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the transform\n\n        Returns\n        -------\n        value : array\n            The features extracted from the molecule\n\n        Raises\n        ------\n        ValueError\n            If the transformer has not been fit.\n\n        ValueError\n            If the size of the transforming molecules are larger than the fit.\n        """"""\n        self.check_fit()\n\n        data = self.convert_input(X)\n        if len(data.numbers) > self._max_size:\n            msg = ""The fit molecules (%d) were not as large as the ones that""\n            msg += "" are being transformed (%d).""\n            raise ValueError(msg % (self._max_size, len(data.numbers)))\n\n        padding_difference = self._max_size - len(data.numbers)\n\n        # Standard parts\n        ZZ = numpy.outer(data.numbers, data.numbers).astype(float)\n        rr = data.coords[:, None] - data.coords\n\n        # Compute phi\n        B = data.unit_cell\n        Binv = numpy.linalg.inv(B)\n        inner = numpy.tensordot(Binv, rr, [[1], [2]])\n        inner *= numpy.pi\n        numpy.sin(inner, inner)\n        numpy.square(inner, inner)\n        full = numpy.tensordot(B, inner, [[1], [0]])\n        phi = numpy.linalg.norm(full, axis=0)\n        with numpy.errstate(divide=\'ignore\'):\n            numpy.power(phi, -1, phi)\n\n        # Final\n        ZZ *= phi\n        diag = 0.5 * data.numbers ** 2.4\n        numpy.fill_diagonal(ZZ, diag)\n        values = ZZ\n\n        if self.sort:\n            order = numpy.argsort(values.sum(0))[::-1]\n            values = values[order, :][:, order]\n\n        if self.eigen:\n            values = numpy.linalg.eig(values)[0]\n\n        values = numpy.pad(values,\n                           (0, padding_difference),\n                           mode=""constant"")\n        return values.reshape(-1)\n'"
molml/features.py,0,b'from .atom import *  # NOQA\nfrom .molecule import *  # NOQA\nfrom .crystal import *  # NOQA\nfrom .fragment import *  # NOQA\nfrom .kernel import *  # NOQA\n'
molml/fragment.py,0,"b'""""""\nA module to compute fragment based representations.\n\nThis module contains a variety of methods to extract features from molecules\nbased on defined fragments in the molecule. This means that every molecule will\nresult in an array of values (n_fragments, n_features). Note: If atom-wise\nfeatures are used, then this would extend to be (n_fragments, n_atoms,\nn_features).\n""""""\nimport os\nimport glob\nfrom functools import partial\nfrom builtins import range\n\nimport numpy\nimport six\n\nfrom .base import BaseFeature\n\n\n__all__ = (""FragmentMap"", )\n\n\ndef _glob_search(label, search_dirs):\n    for d in search_dirs:\n        string = os.path.join(d, label + \'.*\')\n        found = sorted(glob.glob(string))\n        if found:\n            return found[0]\n    else:\n        raise ValueError(\'Label (""%s"") not found in search dirs.\' % label)\n\n\nclass FragmentMap(BaseFeature):\n    """"""\n    Extract information based on features from fragments.\n\n    This is like if there were `n` features that were extracted from the\n    molecule of interest, and each of these `n` features corresponded to their\n    own feature vectors. These fragments are then used together as a single\n    representation. The output of these fragment vectors is in the same order\n    that they are given.\n\n    For example,\n\n        FragmentMap().fit_transform([[\'A\', \'B\'], [\'C, \'A\'], [\'B\', \'C\'])\n\n    would produce arrays like\n\n        [[f_A, f_B], [f_C, f_A], [f_B, f_C]]\n\n    for a final shape of (3, 2, n_features).\n\n    Parameters\n    ----------\n    input_type : str, default=\'filename\'\n        Specifies the format the input values will be (must be one of \'label\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specific amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    transformer : BaseFeature, default=None\n        Some feature extractor that takes inputs and converts them to a numpy\n        array of data. This should convert the fragment fragments into some\n        vector representation to use. Because the information given to this\n        class is at the label/filename level, the transformer must be able to\n        work with the filenames directly. Either using the standard \'filename\'\n        `input_type`, or using a user-defined function.\n\n    filename_to_label : callable or str, default=\'basename\'\n        The function to use to convert labels into filenames. The function\n        should take a single str argument and return a label to use for that\n        filename. The conversion between labels and filenames is not really\n        required, but may allow for simpler bookkeeping outside this class.\n        There are some predefined functions available in cls.LABEL_FUNCTIONS\n        (\'identity\', \'basename\') as recommendations for what to use.\n\n    label_to_filename : callable or list of str, default=(\'.\', )\n        A function to convert labels into filenames to pass to the transformer.\n        The function should take a single str argument and return a valid path.\n        If a valid path does not exist, this should raise a ValueError.\n        If this is a list, then it will be interpreted as a list of paths to\n        search for files. Specifically, these are used in globs of the form\n        os.path.join(dir_name, label + \'.*\'). Note: This will only use the\n        first file that is found matching that label. The directories will be\n        searched in the order given.\n\n    Attributes\n    ----------\n    _x_fragments : dict, str->numpy.array\n        Dictionary mapping label strings to their corresponding feature\n        vectors.\n    """"""\n    ATTRIBUTES = (\'_x_fragments\', )\n    LABELS = ((\'get_mapping_labels\', None), )\n    LABEL_FUNCTIONS = {\n        \'identity\': lambda x: x,\n        \'basename\': lambda x: os.path.splitext(os.path.basename(x))[0],\n    }\n\n    def __init__(self, input_type=\'filename\', n_jobs=1, transformer=None,\n                 filename_to_label=\'basename\', label_to_filename=(\'.\', )):\n        super(FragmentMap, self).__init__(input_type=input_type, n_jobs=n_jobs)\n        if transformer is None:\n            raise ValueError(\'transformer can not be None.\')\n        self.transformer = transformer\n        self.filename_to_label = filename_to_label\n        self.label_to_filename = label_to_filename\n        self._x_fragments = None\n\n    def fit(self, X, y=None):\n        unique_values = set(numpy.reshape(X, -1))\n        filenames, labels = self.convert_input(unique_values)\n        x_ligands = self.transformer.fit_transform(filenames)\n        self._x_fragments = {x: y for x, y in zip(labels, x_ligands)}\n        return self\n\n    def _get_filename_to_label(self):\n        if callable(self.filename_to_label):\n            return self.filename_to_label\n        else:\n            return self.LABEL_FUNCTIONS[self.filename_to_label]\n\n    def _get_label_to_filename(self):\n        if callable(self.label_to_filename):\n            return self.label_to_filename\n        else:\n            return partial(_glob_search, search_dirs=self.label_to_filename)\n\n    def convert_input(self, X):\n        if self.input_type == \'filename\':\n            func = self._get_filename_to_label()\n            labels = [func(x) for x in X]\n            filenames = X\n        elif self.input_type == \'label\':\n            filenames = []\n            func = self._get_label_to_filename()\n            for x in X:\n                try:\n                    filenames.append(func(x))\n                except ValueError:\n                    pass\n            labels = X\n        else:\n            m = \'This only accepts ""filename"" or ""label"" for input_type.\'\n            raise ValueError(m)\n        return tuple(filenames), tuple(labels)\n\n    def _lookup(self, fragment):\n        return self._x_fragments[fragment]\n\n    def _para_transform(self, X):\n        self.check_fit()\n        _, labels = self.convert_input(X)\n        return numpy.array([self._lookup(x) for x in labels])\n\n    def get_mapping_labels(self):\n        try:\n            labels = self.transformer.get_labels()\n        except AttributeError:\n            # Hack to get length of features\n            length = len(six.next(six.itervalues(self._x_fragments)))\n            labels = [str(x) for x in range(length)]\n        return labels\n'"
molml/io.py,0,"b'""""""\nA collection of functions for loading molecule data from different file types.\n\nNote: Functions in this file should be agnostic to the elements/numbers. This\nshould be deferred to the LazyValues object.\n""""""\nfrom .utils import LazyValues\n\n\ndef read_file_data(path):\n    """"""\n    Determine the file type and call the correct parser.\n\n    The accepted file types are .out and .xyz files.\n\n    Parameters\n    ----------\n    path : str\n        A path to a file to read\n\n    Returns\n    -------\n    elements : list\n        All the elements in the molecule.\n\n    numbers : list\n        All the atomic numbers in the molecule.\n\n    coords : numpy.array, shape=(n_atoms, 3)\n        The atomic coordinates of the molecule.\n    """"""\n    end = path.split(\'.\')[-1]\n    mapping = {\n        \'out\': read_out_data,\n        \'xyz\': read_xyz_data,\n        \'mol2\': read_mol2_data,\n        \'cry\': read_cry_data,\n    }\n    if end in mapping:\n        return mapping[end](path)\n    else:\n        raise ValueError(""Unknown file type"")\n\n\ndef read_out_data(path):\n    """"""\n    Read an out and extract the molecule\'s geometry.\n\n    The file should be in the format::\n\n        ele0 x0 y0 z0\n        ele1 x1 y1 z1\n        ...\n\n    Parameters\n    ----------\n    path : str\n        A path to a file to read\n\n    Returns\n    -------\n    val : LazyValues\n        An object storing all the data\n    """"""\n    elements = []\n    coords = []\n    with open(path, \'r\') as f:\n        for line in f:\n            ele, x, y, z = line.strip().split()\n            point = (float(x), float(y), float(z))\n            elements.append(ele)\n            coords.append(point)\n    return LazyValues(elements=elements, coords=coords)\n\n\ndef read_xyz_data(path):\n    """"""\n    Read an xyz file and extract the molecule\'s geometry.\n\n    The file should be in the format::\n\n        num_atoms\n        comment\n        ele0 x0 y0 z0\n        ele1 x1 y1 z1\n        ...\n\n    Parameters\n    ----------\n    path : str\n        A path to a file to read\n\n    Returns\n    -------\n    val : LazyValues\n        An object storing all the data\n    """"""\n    elements = []\n    coords = []\n    with open(path, \'r\') as f:\n        for i, line in enumerate(f):\n            if i < 2:\n                continue\n            ele, x, y, z = line.strip().split()\n            point = (float(x), float(y), float(z))\n            elements.append(ele)\n            coords.append(point)\n    return LazyValues(elements=elements, coords=coords)\n\n\ndef read_mol2_data(path):\n    """"""\n    Read a mol2 file and extract the molecule\'s geometry.\n\n    Roughly, the file format is something like::\n\n        @<TRIPOS>MOLECULE\n        ...\n        @<TRIPOS>ATOM\n         1 ele0id x0 y0 z0 ele0.type 1 MOL charge0\n         2 ele1id x1 y1 z1 ele1.type 1 MOL charge1\n        ...\n        @<TRIPOS>BOND\n        ...\n\n    Parameters\n    ----------\n    path : str\n        A path to a file to read\n\n    Returns\n    -------\n    val : LazyValues\n        An object storing all the data\n    """"""\n    elements = []\n    coords = []\n    with open(path, \'r\') as f:\n        start = False\n        for line in f:\n            if ""@<TRIPOS>ATOM"" in line:\n                start = True\n                continue\n            if ""@<TRIPOS>BOND"" in line:\n                break  # can\'t use connection info yet\n            if not start:\n                continue\n            vals = line.split()\n            ele = vals[5].split(\'.\')[0]\n            elements.append(ele)\n            coords.append([float(x) for x in vals[2:5]])\n    return LazyValues(elements=elements, coords=coords)\n\n\ndef read_cry_data(path):\n    """"""\n    Read a cry file and extract the molecule\'s geometry.\n\n    The format should be as follows::\n\n        U_xx U_xy U_xz\n        U_yx U_yy U_yz\n        U_zx U_zy U_zz\n        energy (or comment, this is ignored for now)\n        ele0 x0 y0 z0\n        ele1 x1 y1 z1\n        ...\n        elen xn yn zn\n\n    Where the U matrix is made of the unit cell basis vectors as column\n    vectors.\n\n    Parameters\n    ----------\n    path : str\n        A path to a file to read\n\n    Returns\n    -------\n    val : LazyValues\n        An object storing all the data\n    """"""\n    unit = []\n    coords = []\n    elements = []\n    with open(path, \'r\') as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) == 3:\n                unit.append([float(x) for x in parts])\n            if len(parts) == 4:\n                elements.append(parts[0])\n                coords.append([float(x) for x in parts[1:]])\n    return LazyValues(elements=elements, coords=coords, unit_cell=unit)\n'"
molml/kernel.py,0,"b'""""""\nA module to compute kernel based representations.\n\nThe methods in this module are intended to be used directly as kernels for\nkernel methods (e.g. SVMs or KRR). This results in features that are dependent\non the number of molecules used to fit the transformers. These should then\ngive single vectors that have length n_fit_molecules.\n""""""\nfrom contextlib import contextmanager\n\nimport numpy\nfrom scipy.spatial.distance import cdist\n\nfrom .base import BaseFeature, InputTypeMixin\n\n\n__all__ = (""AtomKernel"", )\n\nKERNELS = {\n    \'rbf\': \'sqeuclidean\',\n    \'laplace\': \'cityblock\',\n}\n\n\nclass AtomKernel(InputTypeMixin, BaseFeature):\n    """"""\n    Computes a kernel between molecules using atom similarity.\n\n    This kernel comes with the benefit that because it is atom-wise, it stays\n    size consistent. So, for extensive properties this should properly scale\n    with the size of the molecule compared to other kernel methods.\n\n    Parameters\n    ----------\n    input_type : string, default=None\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\'). Note: This input type depends on the value from\n        transformer. See Below for more details. If this value is None, then\n        it will take the value from transformer, or if there is no transformer\n        then it will default to \'list\'. If a value is given and it does not\n        match the value given for the transformer, then this will raise a\n        ValueError.\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    gamma : float, default=1e-7\n        The hyperparameter to use for the width of the RBF or Laplace kernels\n\n    transformer : BaseFeature, default=None\n        The transformer to use to convert molecules to atom-wise features. If\n        this is not given, then it is assumed that the features have already\n        been created and will be passed directly to fit/transform. Note: if no\n        transformer is given, then the assumed input type is going to be a\n        a list of (numbers, features) pairs. Where numbers is an iterable of\n        the atomic numbers, and features is a numpy array of the features\n        (shape=(n_atoms, n_features)).\n\n    same_element : bool, default=True\n        Require that the atom-atom similarity only be computed if the two\n        atoms are the same element.\n\n    kernel : string or callable, default=""rbf""\n        The kernel function to use when computing the atom-atom interactions.\n        There possible string options are the keys of KERNELS. If a callable\n        object is given, then it must take two arrays and return the pairwise\n        kernel metric between them.\n\n    Attributes\n    ----------\n    _features : numpy.array, shape=(n_mols, (n_atoms, n_features))\n        A numpy array of numpy arrays (that may be different lengths) that\n        stores all of the atom features for the training molecules.\n\n    _numbers : numpy.array, shape=(n_mols, (n_atoms))\n        A numpy array of numpy arrays (that may be different lengths) that\n        stores all the atomic numbers for the training atoms.\n\n    Raises\n    ------\n    ValueError\n        If the input_type of the transformer and the input_type keyword given\n        do not match.\n\n    References\n    ----------\n    Barker, J.; Bulin, J.;  Hamaekers, J. LC-GAP: Localized Coulomb Descriptors\n    for the Gaussian Approximation Potential. 2016\n    """"""\n    ATTRIBUTES = (""_features"", ""_numbers"")\n    LABELS = None\n\n    def __init__(self, input_type=None, n_jobs=1, gamma=1e-7,\n                 transformer=None, same_element=True, kernel=""rbf""):\n        super(AtomKernel, self).__init__(input_type=input_type, n_jobs=n_jobs)\n        self.gamma = gamma\n        self.check_transformer(transformer)\n        self.transformer = transformer\n        self.same_element = same_element\n        self.kernel = kernel\n        self._features = None\n        self._numbers = None\n\n        # This makes this not thread safe for multiple calls to transform\n        self._temp_other_features = None\n        self._temp_other_numbers = None\n\n    @contextmanager\n    def _temp_store(self, feats, nums):\n        """"""\n        Helper method to store features/numbers on the object\n\n        This makes the parallel kernel generation easier.\n\n        Parameters\n        ----------\n        feats : numpy.array\n            The array of features to use.\n\n        nums : numpy.array\n            The array of numbers to use.\n        """"""\n        self._temp_other_features = feats\n        self._temp_other_numbers = nums\n        yield\n        self._temp_other_features = None\n        self._temp_other_numbers = None\n\n    def _para_compute_kernel(self, data):\n        """"""\n        Inner parallel function to compute molecule-molecule the kernel value.\n\n        This is formulated in a way that it can easily be done in a map/reduce\n        fashion.\n\n        Parameters\n        ----------\n        X : tuple\n            A tuple of ints (i, j) to index the test molecule and the train\n            molecule respectively.\n\n        Returns\n        -------\n        value : float\n            The final resulting kernel value.\n\n        Raises\n        ------\n            ValueError\n                If the kernel type is not a valid input.\n        """"""\n        i, j = data\n        x = self._temp_other_features[i]\n        x_nums = self._temp_other_numbers[i]\n        y = self._features[j]\n        y_nums = self._numbers[j]\n\n        if callable(self.kernel):\n            block = self.kernel(x, y)\n        elif self.kernel in KERNELS:\n            string = KERNELS[self.kernel]\n            block = cdist(x, y, string)\n            block *= -self.gamma\n            numpy.exp(block, block)\n        else:\n            raise ValueError(""This is not a valid kernel value."")\n\n        # Mask to make sure only elements of the same type are compared\n        if self.same_element:\n            mask = numpy.equal.outer(x_nums, y_nums)\n            block *= mask\n        return block.sum()\n\n    def compute_kernel(self, b_feats, b_nums, symmetric=False):\n        """"""\n        Compute a kernel between molecules based on atom features.\n\n        Parameters\n        ----------\n            b_feats : list of numpy.array, shape=(n_molecules_b, )\n                Each array is of shape (n_atoms, n_features), where n_atoms is\n                for that particular molecule.\n\n            b_nums : list of lists, shape=(n_molecules_b, )\n                Contains all the atom elements for each molecule in group b\n\n            symmetric : bool, default=True\n                Whether or not the kernel is symmetric. This is just to cut the\n                computational cost in half. This is mainly an optimization\n                when computing the (train, train) kernel.\n\n        Returns\n        -------\n            kernel : numpy.array, shape=(n_molecules_b, n_molecules_fit)\n                The kernel matrix between the two sets of molecules\n        """"""\n        kernel = numpy.zeros((len(b_feats), len(self._features)))\n\n        with self._temp_store(b_feats, b_nums):\n            if symmetric:\n                idxs = numpy.tril_indices(kernel.shape[0])\n            else:\n                xvals = numpy.arange(len(b_feats))\n                yvals = numpy.arange(len(self._features))\n                vals = numpy.meshgrid(xvals, yvals)\n                idxs = (vals[0].reshape(-1), vals[1].reshape(-1))\n\n            values = self.map(self._para_compute_kernel, zip(*idxs))\n            kernel[idxs] = values\n            if symmetric:\n                kernel[idxs[1], idxs[0]] = values\n\n        return kernel\n\n    def _para_get_numbers(self, X):\n        """"""\n        Inner parallel function to collect the atomic numbers of a molecule.\n\n        This is formulated in a way that it can easily be done in a map/reduce\n        fashion.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the fit.\n\n        Returns\n        -------\n        numbers : list\n            A list of the atomic numbers in the molecule.\n        """"""\n        data = self.convert_input(X)\n        numbers = data.numbers\n        return numbers\n\n    def fit(self, X, y=None):\n        """"""\n        Fit the model.\n\n        If there is no self.transformer, then this assumes that the input is\n        a list of (features, numbers) pairs where features is a numpy array of\n        features (shape=(n_atoms, n_features)), and numbers is a list of\n        atomic numbers in the molecule.\n\n        Otherwise, it directly passes these values to the transformer to\n        compute the features, and extracts all the atomic numbers.\n\n        Parameters\n        ----------\n        X : list, shape=(n_samples, )\n            A list of objects to use to fit.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        """"""\n        if self.transformer is None:\n            feats, numbers = zip(*X)\n            self._features = numpy.array(feats)\n            self._numbers = numpy.array(numbers)\n        else:\n            self._features = self.transformer.fit_transform(X, y)\n            self._numbers = numpy.array(self.map(self._para_get_numbers, X))\n        return self\n\n    def transform(self, X, y=None):\n        """"""\n        Transform features/molecules into a kernel matrix.\n\n        If there is no self.transformer, then this assumes that the input is\n        a list of (features, numbers) pairs where features is a numpy array of\n        features (shape=(n_atoms, n_features)), and numbers is a list of\n        atomic numbers in the molecule.\n\n        Otherwise, it directly passes these values to the transformer to\n        compute the features, and extracts all the atomic numbers.\n\n        Parameters\n        ----------\n        X : list, shape=(n_samples, )\n            A list of objects to use to transform\n\n        Returns\n        -------\n        kernel : array, shape=(n_samples, n_samples_fit)\n            The resulting kernel matrix\n\n        Raises\n        ------\n            ValueError\n                If the transformer has not been fit.\n        """"""\n        self.check_fit()\n        if self.transformer is None:\n            features, numbers = zip(*X)\n        else:\n            features = self.transformer.transform(X, y)\n            numbers = self.map(self._para_get_numbers, X)\n        return self.compute_kernel(features, numbers)\n\n    def fit_transform(self, X, y=None):\n        """"""\n        A slightly cheaper way of fitting and then transforming.\n\n        This benefit comes from the resulting kernel matrix being symmetric.\n        Meaning, that only half of it has to be computed.\n\n        Parameters\n        ----------\n        X : list, shape=(n_samples, )\n            A list of objects to use to transform\n\n        Returns\n        -------\n        kernel : array, shape=(n_samples, n_samples)\n            The resulting kernel matrix\n        """"""\n        self.fit(X)\n        return self.compute_kernel(self._features, self._numbers,\n                                   symmetric=True)\n'"
molml/molecule.py,2,"b'""""""\nA module to compute molecule based representations.\n\nThis module contains a variety of methods to extract features from molecules\nbased on the entire molecule. All of the methods included here will produce\none vector per molecule input.\n""""""\nfrom builtins import range\nfrom collections import defaultdict, Counter\nfrom itertools import product\n\nimport numpy\nfrom scipy.spatial.distance import cdist\n\nfrom .base import BaseFeature, SetMergeMixin, EncodedFeature, FormMixin\nfrom .utils import get_depth_threshold_mask_connections, get_coulomb_matrix\nfrom .utils import get_element_pairs, cosine_decay, needs_reversal\nfrom .utils import get_angles, get_graph_distance\nfrom .constants import ELECTRONEGATIVITY, BOND_LENGTHS, UNKNOWN\n\n\n__all__ = (""Connectivity"", ""ConnectivityTree"", ""Autocorrelation"",\n           ""EncodedAngle"", ""EncodedBond"", ""CoulombMatrix"", ""BagOfBonds"")\n\n\nclass Connectivity(SetMergeMixin, BaseFeature):\n    """"""\n    A collection of feature types based on the connectivity of atoms.\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    depth : int, default=1\n        The length of the atom chains to generate for connections\n\n    use_bond_order : boolean, default=False\n        Specifies whether or not to use bond order information (C-C versus\n        C=C). Note: for depth=1, this option does nothing.\n\n    use_coordination : boolean, default=False\n        Specifies whether or not to use the coordination number of the atoms\n        (C1 vs C2 vs C3 vs C4).\n\n    add_unknown : boolean, default=False\n        Specifies whether or not to include an extra UNKNOWN count in the\n        feature vector.\n\n    do_tfidf : boolean, default=False\n        Apply weighting to counts based on their inverse document (molecule)\n        frequency.\n\n    Attributes\n    ----------\n    _base_groups : tuple, tuples\n        All the chains that are in the fit molecules.\n\n    References\n    ----------\n    Collins, C.; Gordon, G.; von Lilienfeld, O. A.; Yaron, D. Constant Size\n    Molecular Descriptors For Use With Machine Learning. arXiv:1701.06649\n    """"""\n    ATTRIBUTES = (""_base_groups"", ""_idf_values"")\n    LABELS = ((""get_chain_labels"", ""_base_groups""), )\n\n    def __init__(self, input_type=\'list\', n_jobs=1, depth=1,\n                 use_bond_order=False, use_coordination=False,\n                 add_unknown=False, do_tfidf=False):\n        super(Connectivity, self).__init__(input_type=input_type,\n                                           n_jobs=n_jobs)\n        self.depth = depth\n        self.use_bond_order = use_bond_order\n        self.use_coordination = use_coordination\n        self.add_unknown = add_unknown\n        self.do_tfidf = do_tfidf\n        self._base_groups = None\n\n        if self.do_tfidf:\n            self._idf_values = None\n        else:\n            self._idf_values = {}\n\n    def _loop_depth(self, connections):\n        """"""\n        Loop over the depth number expanding chains.\n\n        Parameters\n        ----------\n        connections : dict, key->list of keys\n            A dictonary edge table with all the bidirectional connections\n\n        Returns\n        -------\n        chains : list\n            A list of key tuples of all the chains in the molecule\n        """"""\n        chains = [(x, ) for x in connections]\n        for i in range(self.depth - 1):\n            chains = self._expand_chains(chains, connections)\n        return chains\n\n    def _expand_chains(self, initial, connections):\n        """"""\n        Use the connectivity information to add one more atom to each chain.\n\n        Parameters\n        ----------\n        initial : list\n            A list of key tuples of all the chains in the molecule\n\n        connections : dict, key->list of keys\n            A dictonary edge table with all the bidirectional connections\n\n        Returns\n        -------\n        results : list\n            A list of index chains that are one index longer than the inputs\n            in initial.\n        """"""\n        if len(initial) and len(initial[0]) > 1:\n            # All of the chains are duplicated and reversed.\n            # This is to make the loop simpler when handling both ends of the\n            # chain.\n            initial = initial + [x[::-1] for x in initial]\n\n        results = {}\n        for item in initial:\n            # We use the first item because the indexing is easier?\n            for x in connections[item[0]]:\n                if x in item:\n                    continue\n                new = (x, ) + item\n                if new[0] > new[-1]:\n                    new = new[::-1]\n                if new not in results:\n                    results[new] = 1\n        return list(results.keys())\n\n    def _convert_to_bond_order(self, chain, labelled, connections):\n        """"""\n        Convert a chain based on elements into one that includes bond order.\n\n        Parameters\n        ----------\n        chain : tuple\n            The atom index values in the chain\n\n        labelled : tuple\n            Elements corresponding to the chain indices\n\n        connections : dict, key->list of keys\n            A dictonary edge table with all the bidirectional connections\n\n        Returns\n        -------\n        labelled : list\n            The new labelled chain\n        """"""\n        temp = []\n        for i, x in enumerate(chain[:-1]):\n            idx1 = x\n            idx2 = chain[i + 1]\n            symbol1 = labelled[i]\n            symbol2 = labelled[i + 1]\n            temp.append((symbol1, symbol2, connections[idx1][idx2]))\n        return temp\n\n    def _tally_groups(self, chains, nodes, connections=None):\n        """"""\n        Tally chain types and return a dictonary with counts of the types.\n\n        Parameters\n        ----------\n        chains : list\n            All of the chains in the molecule\n\n        nodes : list\n            All of the element labels of the atoms\n\n        connections : dict, key->list of keys\n            A dictonary edge table with all the bidirectional connections\n\n        Returns\n        -------\n        results : dict, labelled_chain->int\n            Totals of the number of each type of chain\n        """"""\n        if self.use_coordination:\n            extra = tuple(str(len(v)) for k, v in sorted(connections.items()))\n            nodes = [x + y for x, y in zip(nodes, extra)]\n\n        results = {}\n        for chain in chains:\n            labelled = tuple(nodes[x] for x in chain)\n\n            if needs_reversal(labelled):\n                labelled = labelled[::-1]\n                chain = chain[::-1]\n\n            if self.use_bond_order and len(labelled) > 1:\n                labelled = self._convert_to_bond_order(chain, labelled,\n                                                       connections)\n\n            labelled = tuple(labelled)\n            if labelled not in results:\n                results[labelled] = 0\n            results[labelled] += 1\n        return results\n\n    def _para_fit(self, X):\n        """"""\n        A single instance of the fit procedure.\n\n        This is formulated in a way that the fits can be done completely\n        parallel in a map/reduce fashion.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the fit\n\n        Returns\n        -------\n        value : list\n            All the chains in the molecule\n        """"""\n        data = self.convert_input(X)\n        chains = self._loop_depth(data.connections)\n        all_counts = self._tally_groups(chains, data.elements,\n                                        data.connections)\n        return list(all_counts.keys())\n\n    def _idf(self, all_keys):\n        res = defaultdict(float)\n        for mol in all_keys:\n            for key in mol:\n                res[key] += 1\n        N = len(all_keys)\n        return {key: numpy.log(N / x) for key, x in res.items()}\n\n    def fit(self, X, y=None):\n        res = self.map(self._para_fit, X)\n        vals = self.reduce(lambda x, y: set(x) | set(y), res)\n        self._base_groups = tuple(sorted(vals))\n\n        if self.do_tfidf:\n            self._idf_values = self._idf(res)\n        return self\n\n    def _para_transform(self, X, y=None):\n        """"""\n        A single instance of the transform procedure.\n\n        This is formulated in a way that the transformations can be done\n        completely parallel with map.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the transform\n\n        Returns\n        -------\n        value : list\n            The features extracted from the molecule\n\n        Raises\n        ------\n        ValueError\n            If the transformer has not been fit.\n        """"""\n        self.check_fit()\n\n        data = self.convert_input(X)\n        groups = self._loop_depth(data.connections)\n        tallies = self._tally_groups(groups, data.elements, data.connections)\n\n        vector = []\n        for x in self._base_groups:\n            value = tallies.get(x, 0)\n            if self.do_tfidf:\n                value *= self._idf_values[x]\n            vector.append(value)\n\n        if self.add_unknown:\n            unknown = 0\n            for key, value in tallies.items():\n                if key not in self._base_groups:\n                    unknown += value\n            vector.append(unknown)\n        return vector\n\n    def get_chain_labels(self, chains):\n        unknown = [UNKNOWN] if self.add_unknown else []\n        if self.use_bond_order:\n            temp = [\'_\'.join([\'-\'.join(y) for y in x]) for x in chains]\n        else:\n            temp = [\'-\'.join(x) for x in chains]\n        return temp + unknown\n\n\nclass ConnectivityTree(Connectivity):\n    """"""\n    A collection of feature types based on a connectivity tree of atoms.\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    depth : int, default=1\n        The length of the atom trees to generate for connections\n\n    use_bond_order : boolean, default=False\n        Specifies whether or not to use bond order information (C-C versus\n        C=C). Note: for depth=1, this option does nothing.\n\n    use_coordination : boolean, default=False\n        Specifies whether or not to use the coordination number of the atoms\n        (C1 vs C2 vs C3 vs C4).\n\n    preserve_paths : boolean, default=False\n        Include the local index to the parent node in each tuple. This helps\n        to differentiate elements at the same depth, but with different\n        parents.\n        Note: for depth<3, this option does nothing.\n\n    use_parent_element : boolean, default=True\n        Include the parent nodes element type. This helps to differentiate\n        elements with different parent elements, but not to the same extreme as\n        `preserve_paths`.\n        Note: this does nothing if `use_bond_order` is set as they are\n        redundant.\n\n    add_unknown : boolean, default=False\n        Specifies whether or not to include an extra UNKNOWN count in the\n        feature vector.\n\n    do_tfidf : boolean, default=False\n        Apply weighting to counts based on their inverse document (molecule)\n        frequency.\n\n    Attributes\n    ----------\n    _base_groups : tuple, tuples\n        All the trees that are in the fit molecules.\n    """"""\n    ATTRIBUTES = (""_base_groups"", ""_idf_values"")\n    LABELS = ((""get_tree_labels"", ""_base_groups""), )\n\n    def __init__(self, input_type=\'list\', n_jobs=1, depth=1,\n                 use_bond_order=False, use_coordination=False,\n                 preserve_paths=False, use_parent_element=True,\n                 add_unknown=False, do_tfidf=False):\n        super(ConnectivityTree, self).__init__(\n            input_type=input_type,\n            n_jobs=n_jobs,\n            depth=depth,\n            use_bond_order=use_bond_order,\n            use_coordination=use_coordination,\n            add_unknown=add_unknown,\n            do_tfidf=do_tfidf)\n        self.preserve_paths = preserve_paths\n        self.use_parent_element = use_parent_element\n        self._base_groups = None\n\n    def _get_trees(self, nodes, connections):\n        trees = []\n        for start in connections:\n            trees.append(self._bfs(connections, start))\n        new_trees = [self._convert_tree(x) for x in trees]\n        labelled_trees = self._label_trees(new_trees, nodes, connections)\n        sorted_trees = [self._sort_tree(x) for x in labelled_trees]\n        return sorted_trees\n\n    def _bfs(self, connections, start):\n        seen = {start}\n        tree = dict()\n        frontier = [(start, 0, tree)]\n\n        while len(frontier):\n            node, depth, parent_tree = frontier.pop(0)\n            if depth >= self.depth:\n                continue\n            parent_tree[node] = dict()\n\n            for neighbor in connections[node]:\n                if neighbor in seen:\n                    continue\n                seen.update({neighbor})\n                frontier.append((neighbor, depth + 1, parent_tree[node]))\n\n        return tree\n\n    def _convert_tree(self, tree, parent_idx=-1, depth=0):\n        if tree == {}:\n            return None\n\n        res = []\n        for key, value in tree.items():\n            sub_tree = self._convert_tree(value, parent_idx=key, depth=depth+1)\n            res.append(((depth, parent_idx, key), sub_tree))\n        return res\n\n    def _label_trees(self, trees, nodes, connections):\n        if self.use_coordination:\n            extra = tuple(str(len(v)) for k, v in sorted(connections.items()))\n            nodes = [x + y for x, y in zip(nodes, extra)]\n\n        # Add a hack to label the parents of root node separately\n        nodes = list(nodes) + [\'Root\']\n\n        def label_tree(tree):\n            if tree is None:\n                return None\n            new = []\n            for key, subtree in tree:\n                depth, parent_idx, idx = key\n                ele = nodes[idx]\n                p_ele = nodes[parent_idx]\n                if self.use_bond_order and self.depth > 1:\n                    if parent_idx == -1:\n                        ele = None\n                    else:\n                        bond = connections[idx][parent_idx]\n                        ele = (ele, bond, p_ele)\n                new_key = depth, p_ele, ele\n                new.append((new_key, label_tree(subtree)))\n            return new\n        return [label_tree(x) for x in trees]\n\n    def _sort_tree(self, tree):\n        if tree is None:\n            return\n        new_tree = [(x, self._sort_tree(y)) for x, y in tree]\n        return sorted(new_tree)\n\n    def _linearize_tree(self, tree, idx=-1):\n        if tree is None:\n            return []\n        vals = [[x + (idx, )] + self._linearize_tree(y, idx=i)\n                for i, (x, y) in enumerate(tree)]\n        return sum(vals, [])\n\n    def _fix_bond_order(self, linear):\n        new_linear = []\n        for x in linear:\n            if x[2] is None:\n                continue\n            # The order is reversed to get parent -> child chains\n            new_ele = \'_\'.join(x[2][::-1])\n            new_linear.append(x[:2] + (new_ele, ) + x[3:])\n        return new_linear\n\n    def _tally_groups(self, trees, nodes, connections=None):\n        """"""\n        Tally tree types and return a dictonary with counts of the types.\n\n        Parameters\n        ----------\n        trees : list\n            All of the trees in the molecule\n\n        nodes : list\n            All of the element labels of the atoms\n\n        connections : dict, key->list of keys\n            A dictonary edge table with all the bidirectional connections\n\n        Returns\n        -------\n        results : dict, labelled_tree->int\n            Totals of the number of each type of tree\n        """"""\n\n        # (depth, p_ele, ele, p_rel_idx)\n        select_idxs = (0, )\n        if self.preserve_paths and self.depth > 2:\n            select_idxs += (3, )\n        if self.use_parent_element and not self.use_bond_order:\n            select_idxs += (1, )\n        select_idxs += (2, )\n\n        results = {}\n        for tree in self._get_trees(nodes, connections):\n            linear = self._linearize_tree(tree)\n            if self.use_bond_order:\n                linear = self._fix_bond_order(linear)\n                if not len(linear):\n                    continue\n\n            selected = [tuple(x[i] for i in select_idxs) for x in linear]\n\n            # selected is now one of these three states:\n            # [(depth, ele), ...],\n            # [(depth, p_ele, ele), ...],\n            # [(depth, rel_idx, p_ele, ele), ...]\n\n            items = sorted(Counter(selected).items())\n            labelled = tuple(x + (y, ) for x, y in items)\n            if labelled not in results:\n                results[labelled] = 0\n            results[labelled] += 1\n        return results\n\n    def get_tree_labels(self, trees):\n        unknown = [UNKNOWN] if self.add_unknown else []\n        return [\'__\'.join(\'-\'.join(str(z) for z in y)\n                          for y in x) for x in trees] + unknown\n\n\nclass Autocorrelation(BaseFeature):\n    r""""""\n    A molecular descriptor based on Autocorrelation functions for properties.\n\n    This is a compact (only depends on the number of properties used and the\n    number of depths) molecule representation that uses the graph distance\n    between atoms to extract information.\n\n    .. math::\n\n        V_d = \\sum_i \\sum_j P_i P_j \\delta(d_{ij}, d)\n\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    depths : list/tuple, default=(0, 1, 2, 3)\n        A list of depths to use for computing the autocorrelations functions.\n\n    properties : list/tuple, default=None\n        A list/tuple of properties to use. Each of these properties should be\n        defined for a single atom in the molecule. Each property can be either\n        a function (that takes in a LazyValues function and returns a vector\n        the with one element per atom) or it can be a one of the following\n        strings (\'Z\', \'EN\', \'CN\', \'I\', \'R\'). Each of these keys corresponds\n        to the atomic number, the electronegativity, coordination number, the\n        identity function (always returns 1), and the covalent radius.\n        If this value is None, then all the predefined properties will be\n        used.\n\n    References\n    ----------\n    Janet, J. P. and  Kulik, H. J. Resolving Transition Metal Chemical Space:\n    Feature Selection for Machine Learning and Structure-Property\n    Relationships. J. Phys. Chem. A 2017, 121, 8939-8954\n    """"""\n    ATTRIBUTES = None\n    LABELS = (\'_labels\', )\n\n    FUNCTIONS = {\n        \'Z\': lambda data: data.numbers,\n        \'EN\': lambda data: [ELECTRONEGATIVITY[x] for x in data.elements],\n        \'CN\': lambda data: [len(value) for key, value in\n                            data.connections.items()],\n        \'I\': lambda data: [1 for x in data.numbers],\n        \'R\': lambda data: [BOND_LENGTHS[x][\'1\'] for x in data.elements],\n    }\n\n    def __init__(self, input_type=\'list\', n_jobs=1, depths=(0, 1, 2, 3),\n                 properties=None):\n        super(Autocorrelation, self).__init__(input_type=input_type,\n                                              n_jobs=n_jobs)\n        self.depths = depths\n        self.properties = properties\n        self._labels = [\'%s_%s\' % pair for pair in\n                        product(self._get_properties(), self.depths)]\n\n    def _get_properties(self):\n        properties = self.properties\n        if properties is None:\n            properties = self.FUNCTIONS.keys()\n        return sorted(properties, key=lambda x: str(x))\n\n    def fit(self, X, y=None):\n        """"""No fitting is required because it is defined by the parameters.""""""\n        return self\n\n    def _para_transform(self, X):\n        self.check_fit()\n\n        data = self.convert_input(X)\n        D = get_graph_distance(data.connections)\n\n        res = []\n        for prop in self._get_properties():\n            if callable(prop):\n                p = prop(data)\n            else:\n                p = self.FUNCTIONS[prop](data)\n\n            P = numpy.outer(p, p)\n            for d in self.depths:\n                res.append(((D == d) * P).sum())\n        return res\n\n\nclass EncodedAngle(FormMixin, SetMergeMixin, EncodedFeature):\n    r""""""\n    A smoothed histogram of atomic angles.\n\n    This method is similar to EncodedBond but for angles in molecules. This is\n    done by enumerating all triplets of atoms and computing the angle between\n    them. The bins are then smoothed with smoothing functions. Note: The\n    angles used are 0 to \\pi.\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    segments : int, default=40\n        The number of bins/segments to use when generating the histogram.\n        Empirically, it has been found that there is no benefit to having more\n        than 40-50 segments.\n\n    smoothing : string or callable, default=\'norm\'\n        A string or callable to use to smooth the histogram values. If a\n        callable is given, it must take just a single argument that is a float\n        (or vector of floats). For a list of supported default functions look\n        at SMOOTHING_FUNCTIONS.\n\n    slope : float, default=20.\n        A parameter to tune the smoothing values. This is applied as a\n        multiplication before calling the smoothing function.\n\n    min_depth : int, default=0\n        A parameter to set the minimum geodesic distance to include in the\n        interactions. A value of np.inf signifies including only intermolecular\n        interactions.\n\n    max_depth : int, default=0\n        A parameter to set the maximum geodesic distance to include in the\n        interactions. A value of 0 signifies that all interactions are\n        included.\n\n    form : int, default=3\n        The histogram splitting style to use. This changes the scaling of\n        this method to be O(E^3), O(E^2), O(E), or O(1) for 3, 2, 1, or 0\n        respectively (where E is the number of elements).\n\n    r_cut : float, default=6.\n        The maximum distance allowed for atoms to be considered local to the\n        ""central atom"".\n\n    add_unknown : boolean, default=False\n        Specifies whether or not to include an extra UNKNOWN count in the\n        feature vector.\n\n    use_comb_idxs : bool, default=False\n        Whether or not to use all combinations of indices when doing the\n        subselection. If this is false, a  middle out scheme will be used.\n\n    Attributes\n    ----------\n    _groups : tuple, tuples\n        A tuple of all the groups (element chains) in the fit molecules.\n    """"""\n    ATTRIBUTES = (""_groups"", )\n    LABELS = ((""get_encoded_labels"", ""_groups""), )\n\n    def __init__(self, input_type=\'list\', n_jobs=1, segments=40,\n                 smoothing=""norm"", slope=20., min_depth=0, max_depth=0,\n                 form=3, r_cut=6., add_unknown=False,\n                 use_comb_idxs=False):\n        super(EncodedAngle, self).__init__(input_type=input_type,\n                                           n_jobs=n_jobs, segments=segments,\n                                           smoothing=smoothing, slope=slope,\n                                           start=0., end=numpy.pi,\n                                           form=form, add_unknown=add_unknown,\n                                           use_comb_idxs=use_comb_idxs)\n        self._groups = None\n        self.min_depth = min_depth\n        self.max_depth = max_depth\n        self.r_cut = r_cut\n\n    def _para_fit(self, X):\n        """"""\n        A single instance of the fit procedure.\n\n        This is formulated in a way that the fits can be done completely\n        parallel in a map/reduce fashion.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the fit\n\n        Returns\n        -------\n        value : list\n            All the elements in the molecule\n        """"""\n        # The creation of these triples could be done in several different\n        # ways. Other approaches include filtering ""impossible"" duplicates\n        # such as C-H-C in methane, or creating only the all combinations of\n        # element triples. Loosely, this is something like:\n        # combinations(sum([[ele]*min(num) for\n        #              ele, num in Counter(elements)], []), 3).\n        # In all the tests that were done, these approaches, while they might\n        # be more ""correct"", perform significantly worse than the procedure\n        # here (in terms of predictions). Similar issues arise in\n        # _para_transform if i == k is removed.\n\n        data = self.convert_input(X)\n        pairs = get_element_pairs(data.elements)\n        res = []\n        for pair1 in pairs:\n            for pair2 in pairs:\n\n                for i in (0, 1):\n                    # select the other index to use\n                    inv_i = int(not i)\n                    for j in (0, 1):\n                        if pair1[i] != pair2[j]:\n                            continue\n                        inv_j = int(not j)\n                        temp = (pair2[inv_j], pair1[i], pair1[inv_i])\n                        res.append(temp)\n        return set([x if x[0] < x[2] else x[::-1] for x in res])\n\n    def f_c(self, R):\n        return cosine_decay(R, r_cut=self.r_cut)\n\n    def _iterator(self, data, idx_map):\n        mat = get_depth_threshold_mask_connections(data.connections,\n                                                   min_depth=self.min_depth,\n                                                   max_depth=self.max_depth)\n        distances = cdist(data.coords, data.coords)\n        f_c = self.f_c(distances)\n        angles = get_angles(data.coords)\n        for i, ele1 in enumerate(data.elements):\n            for j, ele2 in enumerate(data.elements):\n                if i == j or not mat[i, j]:\n                    continue\n                if not f_c[i, j]:\n                    continue\n                for k, ele3 in enumerate(data.elements):\n                    if j == k or not mat[j, k]:\n                        continue\n                    if i > k:\n                        continue\n                    if not f_c[i, k] or not f_c[j, k]:\n                        continue\n                    F = f_c[i, j] * f_c[j, k] * f_c[i, k]\n                    for idx in idx_map.get_idx_iter((ele1, ele2, ele3)):\n                        yield idx, angles[i, j, k], F\n\n    def _para_transform(self, X, y=None):\n        """"""\n        A single instance of the transform procedure.\n\n        This is formulated in a way that the transformations can be done\n        completely parallel with map.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the transform\n\n        Returns\n        -------\n        value : list\n            The features extracted from the molecule\n\n        Raises\n        ------\n        ValueError\n            If the transformer has not been fit.\n        """"""\n        self.check_fit()\n        data = self.convert_input(X)\n        idx_map = self.get_idx_map()\n        iterator = self._iterator(data, idx_map)\n        return self.encode_values(iterator, (len(idx_map), ))\n\n\nclass EncodedBond(FormMixin, SetMergeMixin, EncodedFeature):\n    """"""\n    A smoothed histogram of atomic distances.\n\n    This is a method to generallize the idea of bond counting. Instead of\n    seeing bonds as a discrete count that is thresholded at a given length,\n    they are seen as general distance histograms. This is supplemented with\n    smoothing functions.\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    segments : int, default=100\n        The number of bins/segments to use when generating the histogram.\n        Empirically, it has been found that values beyond 50-100 have little\n        benefit.\n\n    smoothing : string or callable, default=\'norm\'\n        A string or callable to use to smooth the histogram values. If a\n        callable is given, it must take just a single argument that is a float\n        (or vector of floats). For a list of supported default functions look\n        at SMOOTHING_FUNCTIONS.\n\n    start : float, default=0.2\n        The starting point for the histgram sampling in angstroms.\n\n    end : float, default=6.0\n        The ending point for the histogram sampling in angstroms.\n\n    slope : float, default=20.\n        A parameter to tune the smoothing values. This is applied as a\n        multiplication before calling the smoothing function.\n\n    min_depth : int, default=0\n        A parameter to set the minimum geodesic distance to include in the\n        interactions. A value of np.inf signifies including only intermolecular\n        interactions.\n\n    max_depth : int, default=0\n        A parameter to set the maximum geodesic distance to include in the\n        interactions. A value of 0 signifies that all interactions are\n        included.\n\n    spacing : string or callable, default=\'linear\'\n        The histogram interval spacing type. Must be one of (""linear"",\n        ""inverse"", or ""log""). Linear spacing is normal spacing. Inverse takes\n        and evaluates the distances as 1/r and the start and end points are\n        1/x. For log spacing, the distances are evaluated as numpy.log(r)\n        and the start and end points are numpy.log(x). If the value is\n        callable, then it should take a float or vector of floats and return\n        a similar mapping like the other methods.\n\n    form : int, default=2\n        The histogram splitting style to use. This changes the scaling of this\n        method to be O(E^2), O(E), or O(1) for 2, 1, or 0 respectively (where\n        E is the number of elements).\n\n    add_unknown : boolean, default=False\n        Specifies whether or not to include an extra UNKNOWN count in the\n        feature vector.\n\n    use_comb_idxs : bool, default=False\n        Whether or not to use all combinations of indices when doing the\n        subselection. If this is false, a  middle out scheme will be used.\n\n    Attributes\n    ----------\n    _element_pairs : tuple, tuples\n        A tuple of all the element pairs in the fit molecules.\n\n    References\n    ----------\n    Collins, C.; Gordon, G.; von Lilienfeld, O. A.; Yaron, D. Constant Size\n    Molecular Descriptors For Use With Machine Learning. arXiv:1701.06649\n    """"""\n    ATTRIBUTES = (""_element_pairs"", )\n    LABELS = ((""get_encoded_labels"", ""_element_pairs""), )\n\n    def __init__(self, input_type=\'list\', n_jobs=1, segments=100,\n                 smoothing=\'norm\', start=0.2, end=6.0, slope=20.,\n                 min_depth=0, max_depth=0, spacing=\'linear\', form=2,\n                 add_unknown=False, use_comb_idxs=False):\n        super(EncodedBond, self).__init__(input_type=input_type,\n                                          n_jobs=n_jobs, segments=segments,\n                                          smoothing=smoothing, start=start,\n                                          end=end, slope=slope,\n                                          spacing=spacing, form=form,\n                                          add_unknown=add_unknown,\n                                          use_comb_idxs=use_comb_idxs)\n        self._element_pairs = None\n        self.min_depth = min_depth\n        self.max_depth = max_depth\n\n    def _para_fit(self, X):\n        """"""\n        A single instance of the fit procedure.\n\n        This is formulated in a way that the fits can be done completely\n        parallel in a map/reduce fashion.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the fit\n\n        Returns\n        -------\n        value : list\n            All the element pairs in the molecule\n        """"""\n        data = self.convert_input(X)\n        return get_element_pairs(data.elements)\n\n    def _iterator(self, data, idx_map):\n        mat = get_depth_threshold_mask_connections(data.connections,\n                                                   max_depth=self.max_depth,\n                                                   min_depth=self.min_depth)\n        distances = cdist(data.coords, data.coords)\n        for i, ele1 in enumerate(data.elements):\n            for j, ele2 in enumerate(data.elements):\n                if i >= j or not mat[i, j]:\n                    continue\n                for idx in idx_map.get_idx_iter((ele1, ele2)):\n                    yield idx, distances[i, j], 1.\n\n    def _para_transform(self, X, y=None):\n        """"""\n        A single instance of the transform procedure.\n\n        This is formulated in a way that the transformations can be done\n        completely parallel with map.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the transform\n\n        Returns\n        -------\n        value : list\n            The features extracted from the molecule\n\n        Raises\n        ------\n        ValueError\n            If the transformer has not been fit.\n        """"""\n        self.check_fit()\n        data = self.convert_input(X)\n        idx_map = self.get_idx_map()\n        iterator = self._iterator(data, idx_map)\n        return self.encode_values(iterator, (len(idx_map), ))\n\n\nclass CoulombMatrix(BaseFeature):\n    r""""""\n    A molecular descriptor based on Coulomb interactions.\n\n    This is a feature that uses a Coulomb-like interaction between all atoms\n    in the molecule to generate a matrix that is then vectorized.\n\n    .. math::\n\n        C_{ij} = \\begin{cases}\n        \\frac{Z_i Z_j}{\\| r_i - r_j \\|} & i \\neq j \\\\\n                          0.5 Z_i^{2.4} & i = j\n        \\end{cases}\n\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    sort : bool, default=False\n        Specifies whether or not to sort the coulomb matrix based on the\n        sum of the rows (same as L1 norm).\n\n    eigen : bool, default=False\n        Specifies whether or not to use the eigen spectrum of the coulomb\n        matrix rather than the matrix itself. This changes the scaling to be\n        linear in the number of atoms.\n\n    drop_values : bool, default=False\n        Specifies whether or not to drop the atoms from molecules larger than\n        the training set. If this value is set to False, and the molecule is\n        too large to transform, the transform will throw a ValueError. If it is\n        set to True, then it will truncate the molecule to only include the\n        first _max_size atoms of the molecule.\n\n    only_lower_triangle : bool, default=False\n        Specifies whether or not to only use the lower triangle of the Coulomb\n        Matrix. This cuts the dimensionality in half by removing the duplicate\n        values in the upper triangle of the matrix. This does nothing if\n        `eigen` is set.\n\n    Attributes\n    ----------\n    _max_size : int\n        The size of the largest molecule in the fit molecules by number of\n        atoms.\n\n    References\n    ----------\n    Rupp, M.; Tkatchenko, A.; Muller, K.-R.; von Lilienfeld, O. A. Fast and\n    Accurate Modeling of Molecular Atomization Energies with Machine Learning.\n    Phys. Rev. Lett. 2012, 108, 058301.\n\n    Hansen, K.; Montavon, G.; Biegler, F.; Fazli, S.; Rupp, M.; Scheffler, M.;\n    von Lilienfeld, O. A.; Tkatchenko, A.; Muller, K.-R. Assessment and\n    Validation of Machine Learning Methods for Predicting Molecular\n    Atomization Energies. J. Chem. Theory Comput. 2013, 9, 3404-3419.\n    """"""\n    ATTRIBUTES = (""_max_size"", )\n    LABELS = ((""get_coulomb_labels"", ""_max_size""), )\n\n    def __init__(self, input_type=\'list\', n_jobs=1, sort=False, eigen=False,\n                 drop_values=False, only_lower_triangle=False):\n        super(CoulombMatrix, self).__init__(input_type=input_type,\n                                            n_jobs=n_jobs)\n        self._max_size = None\n        self.sort = sort\n        self.eigen = eigen\n        self.drop_values = drop_values\n        self.only_lower_triangle = only_lower_triangle\n\n    def _para_fit(self, X):\n        """"""\n        A single instance of the fit procedure.\n\n        This is formulated in a way that the fits can be done completely\n        parallel in a map/reduce fashion.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the fit\n\n        Returns\n        -------\n        value : int\n            The number of atoms in the molecule\n        """"""\n        data = self.convert_input(X)\n        return len(data.elements)\n\n    def fit(self, X, y=None):\n        """"""\n        Fit the model.\n\n        Parameters\n        ----------\n        X : list, shape=(n_samples, )\n            A list of objects to use to fit.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        """"""\n        max_size = self.map(self._para_fit, X)\n        self._max_size = max(max_size)\n        return self\n\n    def _para_transform(self, X):\n        """"""\n        A single instance of the transform procedure.\n\n        This is formulated in a way that the transformations can be done\n        completely parallel with map.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the transform\n\n        Returns\n        -------\n        value : array\n            The features extracted from the molecule\n\n        Raises\n        ------\n        ValueError\n            If the transformer has not been fit.\n\n        ValueError\n            If the size of the transforming molecules are larger than the fit.\n        """"""\n        self.check_fit()\n\n        data = self.convert_input(X)\n        if len(data.numbers) > self._max_size:\n            if not self.drop_values:\n                msg = ""The fit molecules (%d) were not as large as the ones""\n                msg += "" that are being transformed (%d).""\n                raise ValueError(msg % (self._max_size, len(data.numbers)))\n            numbers = data.numbers[:self._max_size]\n            coords = data.coords[:self._max_size, :]\n        else:\n            numbers = data.numbers\n            coords = data.coords\n\n        padding_difference = self._max_size - len(numbers)\n        values = get_coulomb_matrix(numbers, coords)\n        if self.sort:\n            order = numpy.argsort(values.sum(0))[::-1]\n            values = values[order, :][:, order]\n\n        if self.eigen:\n            values = numpy.linalg.eig(values)[0]\n            values = numpy.sort(values)[::-1]\n\n        values = numpy.pad(values,\n                           (0, padding_difference),\n                           mode=""constant"")\n        if self.only_lower_triangle and len(values.shape) > 1:\n            idxs = numpy.tril_indices(values.shape[0])\n            values = values[idxs]\n        return values.reshape(-1)\n\n    def get_coulomb_labels(self, max_size):\n        if self.eigen:\n            return [\'coul-%d\' % i for i in range(max_size)]\n        labels = []\n        for i in range(max_size):\n            for j in range(max_size):\n                if self.only_lower_triangle and j > i:\n                    continue\n                labels.append(\'coul-%d-%d\' % (i, j))\n        return labels\n\n\nclass BagOfBonds(BaseFeature):\n    """"""\n    A molecular descriptor that groups interactions from the Coulomb Matrix.\n\n    This feature starts the same as the Coulomb Matrix, and then interaction\n    terms of the same element pair are grouped together and then sorted before\n    they are vectorized.\n\n    Parameters\n    ----------\n    input_type : string, default=\'list\'\n        Specifies the format the input values will be (must be one of \'list\'\n        or \'filename\').\n\n    n_jobs : int, default=1\n        Specifies the number of processes to create when generating the\n        features. Positive numbers specify a specifc amount, and numbers less\n        than 1 will use the number of cores the computer has.\n\n    drop_values : bool, default=False\n        Specifies whether or not to drop interactions if there are more than\n        was seen in the training set. If this value is set to False, and the\n        molecule is too large to transform, it will throw a ValueError. If it\n        is set to True, then it will truncate that particular bag to only\n        include the largest _bag_sizes[ele1, ele2] of the molecule.\n\n    add_atoms : bool, default=False\n        Adds the diagonal of the Coulomb Matrix to the bags.\n\n    Attributes\n    ----------\n    _bag_sizes : dict, element pair->int\n        A dictonary mapping element pairs to the maximum size of that element\n        pair block in all the fit molecules.\n\n    References\n    ----------\n    Hansen, K.; Biegler, F.; Ramakrishnan, R.; Pronobis, W.; von Lilienfeld,\n    O. A.; Muller, K.-R.; Tkatchenko, A. Machine Learning Predictions of\n    Molecular Properties: Accurate Many-body Potentials and Nonlocality in\n    Chemical Space. J. Phys. Chem. Lett. 2015, 6, 2326-2331.\n    """"""\n    ATTRIBUTES = (""_bag_sizes"", )\n    LABELS = ((""get_bob_labels"", ""_bag_sizes""), )\n\n    def __init__(self, input_type=\'list\', n_jobs=1, drop_values=False,\n                 add_atoms=False):\n        super(BagOfBonds, self).__init__(input_type=input_type, n_jobs=n_jobs)\n        self._bag_sizes = None\n        self.drop_values = drop_values\n        self.add_atoms = add_atoms\n\n    def _para_fit(self, X):\n        """"""\n        A single instance of the fit procedure.\n\n        This is formulated in a way that the fits can be done completely\n        parallel in a map/reduce fashion.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the fit\n\n        Returns\n        -------\n        value : list\n            All the element pairs in the molecule\n        """"""\n        data = self.convert_input(X)\n\n        local = {}\n        for element in data.elements:\n            if element not in local:\n                local[element] = 0\n            local[element] += 1\n\n        bags = {}\n        if self.add_atoms:\n            bags = {(ele, ): val for ele, val in local.items()}\n\n        for i, ele1 in enumerate(local.keys()):\n            for j, ele2 in enumerate(local.keys()):\n                if j > i:\n                    continue\n                if ele1 == ele2:\n                    # Minus 1 is to remove the diagonal\n                    num = local[ele1] - 1\n                    # Using Gauss summation trick\n                    new_value = num * (num + 1) // 2\n                else:\n                    new_value = local[ele1] * local[ele2]\n\n                sorted_ele = tuple(sorted([ele1, ele2]))\n                bags[sorted_ele] = max(new_value, bags.get(sorted_ele, 0))\n        return {key: value for key, value in bags.items() if value}\n\n    def _max_merge_dict(self, x, y):\n        """"""\n        Merge the values of two dictonaries using the max of their values.\n\n        Parameters\n        ----------\n        x : dict, key->number\n\n        y : dict, key->number\n\n        Returns\n        -------\n        dict : dict, key->number\n        """"""\n        all_keys = tuple(x.keys()) + tuple(y.keys())\n        return {key: max(x.get(key, 0), y.get(key, 0)) for key in all_keys}\n\n    def fit(self, X, y=None):\n        """"""\n        Fit the model.\n\n        Parameters\n        ----------\n        X : list, shape=(n_samples, )\n            A list of objects to use to fit.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        """"""\n        bags = self.map(self._para_fit, X)\n        d = self.reduce(self._max_merge_dict, bags)\n        self._bag_sizes = tuple(sorted(d.items()))\n        return self\n\n    def _para_transform(self, X):\n        """"""\n        A single instance of the transform procedure.\n\n        This is formulated in a way that the transformations can be done\n        completely parallel with map.\n\n        Parameters\n        ----------\n        X : object\n            An object to use for the transform\n\n        Returns\n        -------\n        value : array\n            The features extracted from the molecule\n\n        Raises\n        ------\n        ValueError\n            If the transformer has not been fit.\n\n        ValueError\n            If the size of the transforming molecules have more values in at\n            least one bag than the same bag from the are larger than the fit.\n        """"""\n        self.check_fit()\n\n        data = self.convert_input(X)\n        # Sort the elements and coords based on the element\n        temp = sorted(zip(data.elements, data.numbers, data.coords),\n                      key=lambda x: x[0])\n        elements, numbers, coords = zip(*temp)\n\n        bags = {k: [0 for i in range(v)] for k, v in self._bag_sizes}\n        coulomb_matrix = get_coulomb_matrix(numbers, coords)\n\n        ele_array = numpy.array(elements)\n        for eles in bags.keys():\n            if len(eles) == 2:\n                # Select only the rows that are of the first element\n                first = ele_array == eles[0]\n                # Select only the cols that are of the second element\n                second = ele_array == eles[1]\n\n                # Select only the rows/cols that are in the upper triangle\n                # (This could also be the lower), and are in a row, col with\n                # the first element and the second element respectively\n                outer = numpy.logical_and.outer(first, second)\n                mask = numpy.triu(outer | outer.T, k=1)\n                values = coulomb_matrix[mask]\n            else:\n                mask = ele_array == eles[0]\n                values = numpy.diag(coulomb_matrix)[mask]\n\n            # Sort order of values to be consistent\n            values = sorted(values, reverse=True)\n\n            # The molecule being used was fit to something smaller\n            if len(values) > len(bags[eles]):\n                if not self.drop_values:\n                    msg = ""The size of the %s bag is too small for this input""\n                    raise ValueError(msg % (eles, ))\n                values = values[:len(bags[eles])]\n\n            bags[eles][:len(values)] = values\n        order = [x[0] for x in self._bag_sizes]\n        return sum((bags[key] for key in order), [])\n\n    def get_bob_labels(self, bag_sizes):\n        labels = []\n        for bag, size in bag_sizes:\n            name = \'-\'.join(bag)\n            labels.extend([\'%s_%d\' % (name, i) for i in range(size)])\n        return labels\n'"
molml/utils.py,0,"b'""""""\nA collection of assorted utility functions.\n""""""\nfrom builtins import range\nfrom collections import Counter\nimport importlib\nimport json\nimport warnings\nfrom itertools import product, combinations\n\n\nimport numpy\nfrom scipy.spatial.distance import cdist\nfrom scipy.special import expit\nimport scipy.stats\n\nfrom .constants import ELE_TO_NUM, TYPE_ORDER, BOND_LENGTHS, UNKNOWN\n\n\ndef lerp_smooth(x):\n    span = x[1] - x[0]\n    return numpy.maximum(-numpy.abs(x / span) + 1, 0)\n\n\ndef multi_beta(f):\n    return lambda x, beta: f(beta * x)\n\n\nSMOOTHING_FUNCTIONS = {\n    ""norm_cdf"": multi_beta(scipy.stats.norm.cdf),\n    ""zero_one"": lambda x, beta: (beta * x > 0.).astype(float),\n    ""expit"": multi_beta(expit),\n    ""tanh"": lambda x, beta: (numpy.tanh(beta * x) + 1) / 2,\n    ""norm"": multi_beta(scipy.stats.norm.pdf),\n    ""circ"": scipy.stats.vonmises.pdf,\n    ""expit_pdf"": multi_beta(scipy.stats.logistic.pdf),\n    ""spike"": lambda x, beta: (numpy.abs(beta * x) < 1.).astype(float),\n    ""lerp"": multi_beta(lerp_smooth),\n}\nSPACING_FUNCTIONS = {\n    ""log"": lambda x: numpy.log(x),\n    ""inverse"": lambda x: 1 / x,\n    ""linear"": lambda x: x,\n}\n\n\ndef get_dict_func_getter(d, label=\'\'):\n    def func(key):\n        try:\n            if callable(key):\n                return key\n            return d[key]\n        except KeyError:\n            msg = ""The value \'%s\' is not a valid %s type.""\n            raise KeyError(msg % (label, key))\n    return func\n\n\nget_smoothing_function = get_dict_func_getter(SMOOTHING_FUNCTIONS,\n                                              label=\'smoothing\')\nget_spacing_function = get_dict_func_getter(SPACING_FUNCTIONS,\n                                            label=\'spacing\')\n\n\ndef get_bond_type(element1, element2, dist):\n    """"""\n    Get the bond type between two elements based on their distance.\n\n    If there is no bond, return None.\n\n    Parameters\n    ----------\n    element1 : str\n        The element of the first atom\n    element2 : str\n        The element of the second atom\n    dist : float\n        The distance between the two atoms\n    Returns\n    -------\n    key : str\n        The type of the bond\n    """"""\n    bad_eles = [x for x in (element1, element2) if x not in BOND_LENGTHS]\n    if len(bad_eles):\n        msg = ""The following elements are not in BOND_LENGTHS: %s"" % bad_eles\n        warnings.warn(msg)\n        return\n\n    for key in TYPE_ORDER[::-1]:\n        try:\n            cutoff = BOND_LENGTHS[element1][key] + BOND_LENGTHS[element2][key]\n            if dist < cutoff:\n                return key\n        except KeyError:\n            continue\n\n\ndef get_connections(elements1, coords1, elements2=None, coords2=None):\n    """"""\n    Return a dictionary edge list\n\n    If two sets of elements and coordinates are given, then they\n    will be treated as two disjoint sets of atoms.\n\n    Each value is is a tuple of the index of the connecting atom and the bond\n    order as a string. Where the bond order is one of [\'1\', \'Ar\', \'2\', \'3\'].\n\n    Note: If two sets are given, this returns only the connections from the\n    first set to the second. This is in contrast to returning connections from\n    both directions.\n\n    Parameters\n    ----------\n    elements1 : list\n        All the elements in set 1.\n\n    coords1 : array, shape=(n_atoms, 3)\n        The coordinates of the atoms in set 1.\n\n    elements2 : list, default=None\n        All the elements in set 2.\n\n    coords2 : array, shape=(n_atoms, 3), default=None\n        The coordinates of the atoms in set 2.\n\n    Returns\n    -------\n    connections : dict, int->dict\n        Contains all atoms that are connected to each atom and bond type.\n    """"""\n    disjoint = True\n    if elements2 is None or coords2 is None:\n        disjoint = False\n        elements2 = elements1\n        coords2 = coords1\n\n    dist_mat = cdist(coords1, coords2)\n    connections = {i: {} for i in range(len(elements1))}\n    for i, element1 in enumerate(elements1):\n        for j, element2 in enumerate(elements2):\n            if not disjoint and i >= j:\n                continue\n\n            dist = dist_mat[i, j]\n            bond_type = get_bond_type(element1, element2, dist)\n            if not bond_type:\n                continue\n\n            connections[i][j] = bond_type\n            if not disjoint:\n                connections[j][i] = bond_type\n    return connections\n\n\ndef get_graph_distance(connections):\n    """"""\n    Compute the graph distance between all pairs of atoms using Floyd-Warshall\n\n    Parameters\n    ----------\n    connections : dict, index->list of indices\n        A dictionary that contains lists of all connected atoms.\n\n    Returns\n    -------\n    dist : numpy.array, shape=(len(connections), len(connections))\n        The graph distance between all pairs of atoms\n    """"""\n    # Floyd-Warshall\n    V = len(connections)\n    dist = numpy.ones((V, V)) * numpy.inf\n    numpy.fill_diagonal(dist, numpy.zeros(V))\n    for key, values in connections.items():\n        for val in values:\n            dist[key, val] = 1\n\n    for k in range(V):\n        for i in range(V):\n            for j in range(V):\n                temp = dist[i, k] + dist[k, j]\n                if dist[i, j] > temp:\n                    dist[i, j] = temp\n    return dist\n\n\ndef get_depth_threshold_mask_connections(connections, min_depth=0,\n                                         max_depth=numpy.inf):\n    """"""\n    Get the depth threshold mask from connections.\n\n    Parameters\n    ----------\n    connections : dict, index->list of indices\n        A dictionary that contains lists of all connected atoms.\n\n\n    min_depth : int, default=0\n        The minimum depth to allow in the masking\n\n    max_depth : int, default=numpy.inf\n        The maximum depth to allow in the masking\n\n    Returns\n    -------\n    mask : numpy.array, shape=(len(connections), len(connections))\n        A mask of all the atoms that are less than or equal to `max_depth`\n        away.\n    """"""\n    if max_depth < 1:\n        max_depth = numpy.inf\n    if not min_depth and max_depth is numpy.inf:\n        V = len(connections)\n        return numpy.ones((V, V)).astype(bool)\n    dist = get_graph_distance(connections)\n    return (min_depth <= dist) & (dist <= max_depth)\n\n\nclass LazyValues(object):\n    """"""\n    An object to store molecule graph properties in a lazy fashion.\n\n    This object allows only needing to compute different molecule graph\n    properties if they are needed. The prime example of this being the\n    computation of connections.\n\n    Parameters\n    ----------\n    connections : dict, key->list of keys, default=None\n        A dictionary edge table with all the bidirectional connections.\n\n    numbers : array-like, shape=(n_atoms, ), default=None\n        The atomic numbers of all the atoms.\n\n    coords : array-like, shape=(n_atoms, 3), default=None\n        The xyz coordinates of all the atoms (in angstroms).\n\n    elements : array-like, shape=(n_atoms, ), default=None\n        The element symbols of all the atoms.\n\n    unit_cell : array-like, shape=(3, 3), default=None\n        An array of unit cell basis vectors, where the vectors are columns.\n\n\n    Attributes\n    ----------\n    connections : dict, key->list of keys\n        A dictionary edge table with all the bidirectional connections. If the\n        initialized value for this was None, then this will be computed from\n        the coords and numbers/elements.\n\n    numbers : array, shape=(n_atoms, )\n        The atomic numbers of all the atoms. If the initialized value for this\n        was None, then this will be computed from the elements.\n\n    coords : array, shape=(n_atoms, 3)\n        The xyz coordinates of all the atoms (in angstroms).\n\n    elements : array, shape=(n_atoms, )\n        The element symbols of all the atoms. If the initialized value for this\n        was None, then this will be computed from the numbers.\n\n    unit_cell : array, shape=(3, 3)\n        An array of unit cell basis vectors, where the vectors are columns.\n    """"""\n    def __init__(self, connections=None, coords=None, numbers=None,\n                 elements=None, unit_cell=None):\n        self._connections = connections\n        self._coords = self._none_check(coords)\n        self._numbers = self._none_check(numbers)\n        self._elements = self._none_check(elements)\n        self._unit_cell = self._none_check(unit_cell)\n        self.__crystal_size = None\n\n    def _none_check(self, x):\n        return numpy.array(x) if x is not None else x\n\n    def fill_in_crystal(self, radius=None, units=None):\n        """"""\n        Duplicate the atoms to form a crystal.\n\n        Parameters\n        ----------\n        radius : float, default=None\n            Specifies the radius of unit cell points to include\n\n        units : list or int, default=None\n            Specifies the number of unit cells to include on each axis.\n            These will all be equal if it is an int.\n\n        Raises\n        ------\n        ValueError\n            If radius and units are either both None, or if both are not None.\n        """"""\n        if radius is not None and units is None:\n            offsets = list(_radial_iterator(self.unit_cell, radius))\n        elif radius is None and units is not None:\n            offsets = list(_unit_iterator(self.unit_cell, units))\n        else:\n            raise ValueError(""Only one of radius and units must be set."")\n        coords = numpy.array(self.coords)\n        self.__crystal_size = len(offsets)\n\n        new_coords = []\n        for offset in offsets:\n            new_coords.append(coords + offset)\n        self._coords = numpy.concatenate(new_coords)\n\n        if self._numbers is not None:\n            self._numbers = numpy.tile(self._numbers, self.__crystal_size)\n\n        if self._elements is not None:\n            self._elements = numpy.tile(self._elements, self.__crystal_size)\n\n        if self._connections is not None:\n            self._connections = self._expand_connections(offsets)\n\n    def _expand_connections(self, offsets):\n        new_conn = {}\n\n        # Local connections\n        n = len(self._connections)\n        for key, items in self._connections.items():\n            for i in range(self.__crystal_size):\n                off = n * i\n                values = {inner_key + off: value for\n                          inner_key, value in items.items()}\n                new_conn[key + off] = values\n\n        # Connections between cells\n        a = numpy.array(offsets)\n        Inv = numpy.linalg.inv(self.unit_cell)\n        counts = Inv.dot(a.T).T\n        dists = cdist(counts, counts, \'chebyshev\')\n        for i, j in zip(*numpy.where(dists <= 1)):\n            if i == j or i > j:\n                continue\n            off1 = n * i\n            end1 = off1 + n\n            off2 = n * j\n            end2 = off2 + n\n            elements1 = self.elements[off1:end1]\n            coords1 = self.coords[off1:end1, :]\n            elements2 = self.elements[off2:end2]\n            coords2 = self.coords[off2:end2, :]\n            conn = get_connections(elements1, coords1,\n                                   elements2, coords2)\n\n            for key1, items in conn.items():\n                idx1 = key1 + off1\n                for key2, bond in items.items():\n                    idx2 = key2 + off2\n                    new_conn[idx1][idx2] = bond\n                    new_conn[idx2][idx1] = bond\n        return new_conn\n\n    @property\n    def connections(self):\n        if self._connections is None:\n            self._connections = get_connections(self.elements, self.coords)\n        return self._connections\n\n    @property\n    def unit_cell(self):\n        if self._unit_cell is None:\n            raise ValueError(""No unit cell exists."")\n        return self._unit_cell\n\n    @property\n    def coords(self):\n        if self._coords is None:\n            raise ValueError(""No coordinates exist."")\n        return self._coords\n\n    @property\n    def numbers(self):\n        if self._numbers is None:\n            if self._elements is not None:\n                temp = [ELE_TO_NUM[x] for x in self._elements]\n                self._numbers = numpy.array(temp)\n            else:\n                raise ValueError(""No elements to convert to numbers."")\n        return self._numbers\n\n    @property\n    def elements(self):\n        if self._elements is None:\n            if self._numbers is not None:\n                temp = [ELE_TO_NUM.inv[x] for x in self._numbers]\n                self._elements = numpy.array(temp)\n            else:\n                raise ValueError(""No numbers to convert to elements."")\n        return self._elements\n\n\ndef get_coulomb_matrix(numbers, coords, alpha=1, use_decay=False):\n    r""""""\n    Return the coulomb matrix for the given coords and numbers.\n\n    .. math::\n\n        C_{ij} = \\begin{cases}\n            \\frac{Z_i Z_j}{\\| r_i - r_j \\|^\\alpha} & i \\neq j\\\\\n            \\frac{1}{2} Z_i^{2.4} & i = j\n        \\end{cases}\n\n    Parameters\n    ----------\n    numbers : array-like, shape=(n_atoms, )\n        The atomic numbers of all the atoms\n\n    coords : array-like, shape=(n_atoms, 3)\n        The xyz coordinates of all the atoms (in angstroms)\n\n    alpha : number, default=6\n        Some value to exponentiate the distance in the coulomb matrix.\n\n    use_decay : bool, default=False\n        This setting defines an extra decay for the values as they get futher\n        away from the ""central atom"". This is to alleviate issues the arise as\n        atoms enter or leave the cutoff radius.\n\n    Returns\n    -------\n    top : array, shape=(n_atoms, n_atoms)\n        The coulomb matrix\n    """"""\n    top = numpy.outer(numbers, numbers).astype(numpy.float64)\n    r = cdist(coords, coords)\n    if use_decay:\n        other = cdist([coords[0]], coords).reshape(-1)\n        r += numpy.add.outer(other, other)\n\n    r **= alpha\n\n    with numpy.errstate(divide=\'ignore\', invalid=\'ignore\'):\n        numpy.divide(top, r, top)\n    numpy.fill_diagonal(top, 0.5 * numpy.array(numbers) ** 2.4)\n    top[top == numpy.Infinity] = 0\n    top[numpy.isnan(top)] = 0\n    return top\n\n\ndef get_element_pairs(elements):\n    """"""\n    Extract all the element pairs in a molecule.\n\n    Parameters\n    ----------\n    elements : list\n        All the elements in the molecule\n\n    Returns\n    -------\n    value : list\n        All the element pairs in the molecule\n    """"""\n    # This is like computing set(combinations(sorted(elements), 2))\n    # We do this because it scales with elements instead of atoms.\n    counts = Counter(elements)\n    pairs = {}\n    order = sorted(counts)\n    for i, x in enumerate(order):\n        for j, y in enumerate(order):\n            if i > j:\n                continue\n            if x == y and counts[x] < 2:\n                continue\n            pairs[x, y] = 1\n    return list(pairs.keys())\n\n\ndef deslugify(string):\n    """"""\n    Convert a string to a feature name and its parameters.\n\n    Parameters\n    ----------\n        string : str\n            The slug string to extract values from.\n\n    Returns\n    -------\n        name : str\n            The name of the class corresponding to the string.\n\n        final_params : dict\n            A dictionary of the feature parameters.\n    """"""\n    parts = string.split(\'__\')\n    name = parts[0]\n    params = parts[1:]\n    swap = {\n        \'None\': None,\n        \'True\': True,\n        \'False\': False,\n    }\n    final_params = dict()\n    for param in params:\n        arg, value = param.split(\'=\')\n        try:\n            value = int(value)\n        except ValueError:\n            try:\n                value = float(value)\n            except ValueError:\n                pass\n        if value in swap:\n            value = swap[value]\n        final_params[arg] = value\n    return name, final_params\n\n\ndef cosine_decay(R, r_cut=6.):\n    r""""""\n    Compute all the cutoff distances.\n\n    The cutoff is defined as\n\n    .. math::\n\n        f_{R_{c}}(R_{ij}) = \\begin{cases}\n            0.5 ( \\cos( \\frac{\\pi R_{ij}}{R_c} ) + 1 ), & R_{ij} \\le R_c \\\\\n            0,  & otherwise\n        \\end{cases}\n\n\n    Parameters\n    ----------\n    R : array, shape=(N_atoms, N_atoms)\n        A distance matrix for all the atoms (scipy.spatial.cdist)\n\n    r_cut : float, default=6.\n        The maximum distance allowed for atoms to be considered local to the\n        ""central atom"".\n\n    Returns\n    -------\n    values : array, shape=(N_atoms, N_atoms)\n        The new distance matrix with the cutoff function applied\n    """"""\n    values = 0.5 * (numpy.cos(numpy.pi * R / r_cut) + 1)\n    values[R > r_cut] = 0\n    return values\n\n\nclass IndexMap(object):\n    \'\'\'\n    An object to handle dynamic mapping of groups to indices.\n\n    The intention of the class is to allow for dynamic subselection from lists\n    to give new mapping groups.\n\n    For example, with the a group of values that have length three might be\n    reduced as follows:\n        (\'A\', \'B\', \'C\') -> (\'A\', \'C\')\n    using some predefined index selection. Then, this new reduced value is\n    used to find the index for this new value just like a dict that maps\n    the new shorter values to an int index.\n\n    This class also allows handling of groups that are not in the map.\n\n    Parameters\n    ----------\n    values : list of tuples\n        A collection of values overwhich the mapping will be done.\n\n    depth : int\n        The number of values that are retained in the subselection.\n\n    add_unknown : bool, default=False\n        Whether or not to allocate an UNKNOWN index.\n\n    use_comb_idxs : bool, default=False\n        Whether or not to use all combinations of indices when doing the\n        subselection. If this is False, it will use the old style of trying to\n        select indices from the center of the value outward.\n    \'\'\'\n    def __init__(self, values, depth, add_unknown=False,\n                 use_comb_idxs=False):\n        self.values = values\n        self.depth = depth\n        self.add_unknown = add_unknown\n        self.use_comb_idxs = use_comb_idxs\n        length = len(values[0])\n        if use_comb_idxs:\n            idx_values = list(range(length))\n            self.idx_groups = list(combinations(idx_values, self.depth))\n        else:\n            self.idx_groups = IndexMap._get_form_indices(length, depth)\n        self._mapping = IndexMap.get_index_mapping(values, depth,\n                                                   self.idx_groups)\n\n    def is_valid(self, values):\n        return self.values == values\n\n    def __len__(self):\n        return len(self._mapping) + int(self.add_unknown)\n\n    def __iter__(self):\n        for x in self.get_value_order():\n            yield x\n\n    def __getitem__(self, key):\n        return self._inner(key, self.idx_groups[0])\n\n    def _inner(self, key, idxs):\n        key = sort_chain(tuple(key[i] for i in idxs))\n        if key not in self._mapping and self.add_unknown:\n            return -1\n        return self._mapping[key]\n\n    def get_idx_iter(self, key, other=None):\n        if other is None:\n            other = tuple()\n        for idxs in self.idx_groups:\n            try:\n                yield other + (self._inner(key, idxs), )\n            except KeyError:\n                yield None\n\n    def get_value_order(self):\n        base = [None for x in range(len(self))]\n        for key, val in self._mapping.items():\n            base[val] = key\n        if self.add_unknown:\n            base[-1] = (UNKNOWN, )\n        return base\n\n    @staticmethod\n    def _get_form_indices(value_length, depth):\n        """"""\n        Determine the indices to select from a value for a given form.\n\n        One can view this function as a way to get the indices to extract all\n        the unique subchains from a single chain starting from the center of a\n        given size.\n\n        Ex:\n            (A, B, C) -> [0, 1, 2]  # Depth 3\n            (A, B, C) -> [0, 2]     # Depth 2\n            (A, B, C) -> [1]        # Depth 1\n            (A, B, C) -> []         # Depth 0\n\n        Parameters\n        ----------\n        value_length : int\n            The length of a value to select from.\n\n        depth : int\n            The number of elements to use from the value.\n\n        Returns\n        -------\n        final : list of tuples of ints\n            A list of list of indices to select.\n        """"""\n        if depth < 1:\n            return [tuple()]\n\n        if value_length < 1:\n            raise ValueError(""No values to use."")\n\n        if depth >= value_length:\n            return [tuple(range(value_length))]\n\n        middle_idx = value_length // 2\n        even = not (value_length % 2)\n        both = even and depth % 2\n        half_depth = depth // 2\n        start = middle_idx - half_depth - both\n        end = middle_idx + half_depth + (not even)\n        res = list(range(start, end))\n        if not even and not (depth % 2):\n            res.remove(middle_idx)\n        final = [tuple(res)]\n        if both:\n            final.append(tuple(i + 1 for i in res))\n        return final\n\n    @staticmethod\n    def get_index_mapping(values, depth, idx_groups):\n        """"""\n        Determine the ordering and mapping of feature groups.\n\n        Parameters\n        ----------\n        values : list\n            A list of possible values.\n\n        depth : int\n            The number of elements to use from each values value.\n\n        idx_groups : list of list of int\n            A list of list of indices to select.\n\n        Returns\n        -------\n        mapping : dict(key)->int\n            A dict that gives the mapping index for a given key.\n        """"""\n        if depth < 1:\n            # Just a constant value\n            return {tuple(): 0}\n        temp_values = [[tuple(x[i] for i in idxs) for x in values]\n                       for idxs in idx_groups]\n        new_values = set(sort_chain(x) for x in sum(temp_values, []))\n        mapping = {key: i for i, key in enumerate(sorted(new_values))}\n        return mapping\n\n\ndef needs_reversal(chain):\n    """"""\n    Determine if the chain needs to be reversed.\n\n    This is to set the chains such that they are in a canonical ordering\n\n    Parameters\n    ----------\n    chain : tuple\n        A tuple of elements to treat as a chain\n\n    Returns\n    -------\n    needs_flip : bool\n        Whether or not the chain needs to be reversed\n    """"""\n    x = len(chain)\n    if x == 1:\n        first = 0\n        second = 0\n    else:\n        q, r = divmod(x, 2)\n        first = q - 1\n        second = q + r\n\n    while first >= 0 and second < len(chain):\n        if chain[first] > chain[second]:\n            # Case where order reversal is needed\n            return True\n        elif chain[first] == chain[second]:\n            # Indeterminate case\n            first -= 1\n            second += 1\n        else:\n            # Case already in the correct order\n            return False\n    return False\n\n\ndef sort_chain(chain):\n    """"""\n    Sort a chain from the inside out.\n\n    Parameters\n    ----------\n    chain : tuple\n        A tuple of elements to treat as a chain\n\n    Returns\n    -------\n    chain : tuple\n        The sorted chain\n    """"""\n    if needs_reversal(chain):\n        return chain[::-1]\n    return chain\n\n\ndef get_angles(coords):\n    r""""""\n    Get the angles between all triples of coords.\n\n    The resulting values are :math:`[0, \\pi]` and all invalid values are NaNs.\n\n    Parameters\n    ----------\n    coords : numpy.array, shape=(n_atoms, n_dim)\n        An array of all the coordinates.\n\n    Returns\n    -------\n    res : numpy.array, shape=(n_atoms, n_atoms, n_atoms)\n        An array the angles of all triples.\n    """"""\n    diffs = coords - coords[:, None]\n    lengths = numpy.linalg.norm(diffs, axis=2)\n    with numpy.errstate(divide=\'ignore\', invalid=\'ignore\'):\n        unit = diffs / lengths[:, :, None]\n    res = numpy.einsum(\'ijk,jmk->ijm\', unit, unit)\n    numpy.clip(res, -1., 1., res)\n    numpy.arccos(res, res)\n    return res\n\n\ndef _load_transformer(data):\n    """"""\n    Load the transformer object\n\n    Parameters\n    ----------\n    data : dict\n        A dictionary of values to load as a transformer.\n\n    Returns\n    -------\n    obj : Transformer\n        The transformer object.\n    """"""\n    module, klass = data[""transformer""].rsplit(\'.\', 1)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(""ignore"")\n        # Throw away the warnings in Python 3.x because of Dill...\n        # https://github.com/uqfoundation/dill/issues/210\n        m = importlib.import_module(module)\n\n    cls = getattr(m, klass)\n    comp = set([""attributes"", ""parameters"", ""transformer""])\n    parameters = {}\n    for key, value in data[""parameters""].items():\n        if not isinstance(value, dict) or \\\n           len(comp & set(value.keys())) != 3:\n            parameters[key] = value\n            continue\n        parameters[key] = _load_transformer(value)\n\n    obj = cls(**parameters)\n    for key, value in data[""attributes""].items():\n        setattr(obj, key, value)\n    return obj\n\n\ndef load_json(f):\n    """"""\n    Load the model data from a json file\n\n    Parameters\n    ----------\n    f : str or file descriptor\n        The path to save the data or a file descriptor to save it to.\n\n    Returns\n    -------\n    obj : Transformer\n        The transformer object.\n    """"""\n    try:\n        data = json.load(f)\n    except AttributeError:\n        with open(f, \'r\') as in_file:\n            data = json.load(in_file)\n    return _load_transformer(data)\n\n\ndef _radial_iterator(X, r_max):\n    X = numpy.array(X)\n    norm = numpy.linalg.norm\n    lengths = norm(X, axis=0)\n    # Compute the upper bounds for each axis\n    steps = numpy.ceil(r_max / lengths).astype(int)\n    ranges = [range(-x, x + 1) for x in steps]\n\n    for group in product(*ranges):\n        group = numpy.array(group)\n        temp_z = numpy.dot(X, group)\n        if norm(temp_z) > r_max:\n            continue\n        yield temp_z\n\n\ndef _unit_iterator(X, unit_max):\n    X = numpy.array(X)\n    if isinstance(unit_max, int):\n        ranges = [range(-unit_max, unit_max + 1) for _ in range(3)]\n    else:\n        # Assumed iterable of len 3\n        if len(unit_max) != X.shape[1]:\n            raise ValueError(""Invalid unit cell size."")\n        ranges = [range(-x, x + 1) for x in unit_max]\n\n    for group in product(*ranges):\n        group = numpy.array(group)\n        temp_z = numpy.dot(X, group)\n        yield temp_z\n'"
tests/__init__.py,0,b''
tests/constants.py,0,"b'import os\n\nfrom molml.io import read_file_data\n\n\nDATA_PATH = os.path.join(os.path.dirname(__file__), ""data"")\n\nMETHANE_PATH = os.path.join(DATA_PATH, ""methane.out"")\nMETHANE_VALS = read_file_data(METHANE_PATH)\nMETHANE_ELEMENTS = METHANE_VALS.elements.tolist()\nMETHANE_NUMBERS = METHANE_VALS.numbers.tolist()\nMETHANE_COORDS = METHANE_VALS.coords\nMETHANE = (METHANE_ELEMENTS, METHANE_COORDS)\nMETHANE_CONNECTIONS = {\n    0: {1: ""1"", 2: ""1"", 3: ""1"", 4: ""1""},\n    1: {0: ""1""},\n    2: {0: ""1""},\n    3: {0: ""1""},\n    4: {0: ""1""},\n}\n\nBIG_PATH = os.path.join(DATA_PATH, ""big.out"")\nBIG_VALS = read_file_data(BIG_PATH)\nBIG_ELEMENTS = BIG_VALS.elements.tolist()\nBIG_NUMBERS = BIG_VALS.numbers.tolist()\nBIG_COORDS = BIG_VALS.coords\nBIG = (BIG_ELEMENTS, BIG_COORDS)\n\nMID_PATH = os.path.join(DATA_PATH, ""mid.out"")\nMID_VALS = read_file_data(MID_PATH)\nMID_ELEMENTS = MID_VALS.elements.tolist()\nMID_NUMBERS = MID_VALS.numbers.tolist()\nMID_COORDS = MID_VALS.coords\nMID = (MID_ELEMENTS, MID_COORDS)\n\nALL_DATA = [METHANE, MID, BIG]\n'"
tests/test_atom.py,0,"b'import unittest\n\nimport numpy\n\nfrom molml.atom import Shell, LocalEncodedBond, LocalCoulombMatrix\nfrom molml.atom import LocalEncodedAngle\nfrom molml.atom import BehlerParrinello\nfrom molml.constants import UNKNOWN\n\nfrom .constants import METHANE, BIG, MID, ALL_DATA\n\nBASE_SHELL = numpy.array([\n    [[1, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0],\n     [0, 1, 0, 0]],\n    [[1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1],\n     [0, 0, 0, 1], [0, 0, 0, 1], [0, 1, 0, 0], [0, 1, 0, 0],\n     [0, 1, 0, 0]],\n    [[1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0],\n     [1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 1, 0, 0],\n     [0, 0, 0, 1], [0, 1, 0, 0], [0, 1, 0, 0], [1, 0, 0, 0],\n     [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0],\n     [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 1, 0, 0],\n     [0, 0, 0, 1], [1, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0],\n     [0, 1, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0],\n     [1, 0, 0, 0], [0, 0, 0, 1], [1, 0, 0, 0], [1, 0, 0, 0],\n     [0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [1, 0, 0, 0],\n     [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0],\n     [0, 0, 1, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0],\n     [0, 1, 0, 0], [0, 1, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0],\n     [0, 1, 0, 0]]\n])\nBASE_LOCAL_COULOMB = numpy.array([\n    numpy.array([\n        [36.85810519942594, 3.5389458702468084, 3.5389458702468084, 0.5],\n        [0.5, 3.5387965701194934, 3.5387965701194974, 36.85810519942594],\n        [0.5, 3.5389458702468084, 3.5389458702468084, 36.85810519942594],\n        [0.5, 3.538447971815893, 3.538447971815893, 36.85810519942594],\n        [0.5, 3.52229970767669, 3.52229970767669, 36.85810519942594]\n    ]),\n    numpy.array([\n        [36.85810519942594, 4.642192257970912,\n            4.642192257970912, 36.85810519942594],\n        [36.85810519942594, 6.483079598556282,\n            6.483079598556282, 73.51669471981023],\n        [73.51669471981023, 8.650089711338763,\n            8.650089711338763, 73.51669471981023],\n        [73.51669471981023, 8.650089711338763,\n            8.650089711338763, 73.51669471981023],\n        [73.51669471981023, 10.698256448660478,\n            10.698256448660478, 73.51669471981023],\n        [73.51669471981023, 10.698256448660478,\n            10.698256448660478, 73.51669471981023],\n        [0.5, 0.044832923800298255, 0.044832923800298255, 0.5],\n        [0.5, 1.0, 1.0, 0.5],\n        [0.5, 1.0, 1.0, 0.5]\n    ]),\n])\n\n\nclass ShellTest(unittest.TestCase):\n\n    def test_fit(self):\n        a = Shell(depth=1)\n        a.fit(ALL_DATA)\n        self.assertEqual(a._elements, (\'C\', \'H\', \'N\', \'O\'))\n\n    def test_fit_use_coordination(self):\n        a = Shell(depth=1, use_coordination=True)\n        a.fit(ALL_DATA)\n        self.assertEqual(a._elements, (\'C1\', \'C2\', \'C3\', \'C4\', \'H0\', \'H1\',\n                                       \'N1\', \'N2\', \'N3\', \'O0\', \'O1\', \'O2\'))\n\n    def test_transform(self):\n        a = Shell()\n        a.fit(ALL_DATA)\n        self.assertTrue((a.transform(ALL_DATA) == BASE_SHELL).all())\n\n    def test_transform_use_coordination(self):\n        a = Shell(depth=1, use_coordination=True)\n        a.fit([MID])\n        expected_results = numpy.array([\n            [[1, 0, 0, 0, 0, 0],\n             [0, 1, 0, 0, 0, 0],\n             [0, 0, 0, 0, 0, 1],\n             [0, 0, 0, 0, 1, 0],\n             [0, 0, 0, 0, 0, 1],\n             [0, 0, 0, 0, 0, 1],\n             [0, 0, 1, 0, 0, 0],\n             [0, 0, 0, 1, 0, 0],\n             [0, 0, 0, 1, 0, 0]]\n        ])\n        self.assertTrue((a.transform([MID]) == expected_results).all())\n\n    def test_transform_depth2(self):\n        a = Shell(depth=2)\n        a.fit(ALL_DATA)\n        expected_results = numpy.array([\n            [[0, 4, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0],\n             [1, 0, 0, 0]],\n            [[1, 0, 0, 0], [1, 0, 0, 1], [1, 0, 0, 0], [0, 0, 0, 0],\n             [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 0], [0, 1, 0, 0],\n             [0, 1, 0, 0]],\n            [[1, 0, 1, 1], [2, 1, 0, 0], [2, 1, 0, 0], [2, 1, 0, 0],\n             [2, 0, 1, 0], [2, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0],\n             [1, 1, 0, 0], [0, 0, 0, 1], [1, 0, 0, 0], [1, 0, 1, 1],\n             [3, 0, 0, 0], [2, 1, 0, 0], [2, 0, 0, 1], [2, 0, 1, 0],\n             [2, 0, 0, 0], [1, 0, 0, 0], [1, 1, 0, 0], [0, 0, 0, 1],\n             [2, 0, 0, 0], [0, 3, 0, 1], [1, 0, 0, 0], [1, 0, 0, 0],\n             [1, 0, 0, 0], [2, 0, 0, 1], [3, 0, 0, 0], [3, 0, 0, 0],\n             [2, 0, 0, 1], [2, 0, 0, 0], [2, 0, 0, 0], [1, 1, 0, 0],\n             [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 1, 0], [1, 0, 2, 0],\n             [3, 0, 0, 0], [2, 1, 0, 0], [3, 0, 0, 0], [1, 1, 1, 0],\n             [2, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [1, 2, 0, 0],\n             [0, 0, 1, 0], [0, 0, 1, 0], [2, 0, 0, 0], [1, 1, 0, 0],\n             [1, 0, 0, 0]]\n        ])\n        self.assertTrue((a.transform(ALL_DATA) == expected_results).all())\n\n    def test_transform_depth3(self):\n        # This is to test loop backs in the breadth-first search\n        a = Shell(depth=3)\n        a.fit(ALL_DATA)\n        expected_results = numpy.array([\n            [[0, 0, 0, 0], [0, 3, 0, 0], [0, 3, 0, 0], [0, 3, 0, 0],\n             [0, 3, 0, 0]],\n            [[0, 0, 0, 1], [0, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 0],\n             [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0],\n             [0, 0, 0, 0]],\n            [[2, 2, 0, 0], [1, 1, 1, 1], [2, 2, 0, 0], [2, 1, 1, 0],\n             [4, 1, 0, 0], [3, 0, 0, 1], [2, 0, 0, 0], [2, 0, 0, 0],\n             [1, 0, 1, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 1, 0, 0],\n             [2, 1, 2, 1], [3, 0, 0, 1], [3, 1, 1, 0], [3, 0, 0, 2],\n             [3, 0, 0, 1], [2, 0, 0, 0], [1, 0, 1, 0], [1, 0, 0, 0],\n             [2, 3, 0, 0], [1, 0, 0, 0], [0, 2, 0, 1], [0, 2, 0, 1],\n             [0, 2, 0, 1], [4, 0, 1, 0], [4, 0, 0, 1], [3, 0, 1, 1],\n             [5, 0, 0, 0], [4, 0, 0, 0], [2, 1, 0, 0], [1, 0, 0, 0],\n             [1, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 2, 0, 0],\n             [2, 1, 2, 1], [4, 0, 0, 0], [2, 2, 1, 0], [3, 0, 0, 0],\n             [2, 1, 1, 0], [2, 0, 0, 0], [1, 0, 1, 0], [1, 0, 1, 0],\n             [1, 1, 0, 0], [1, 1, 0, 0], [2, 1, 0, 0], [1, 0, 0, 0],\n             [1, 0, 0, 0]]\n        ])\n        self.assertTrue((a.transform(ALL_DATA) == expected_results).all())\n\n    def test_small_to_large_transform(self):\n        a = Shell()\n        a.fit([METHANE])\n        expected = numpy.array([numpy.array(x)[:, :2].tolist()\n                                for x in BASE_SHELL])\n        self.assertTrue((a.transform(ALL_DATA) == expected).all())\n\n    def test_large_to_small_transform(self):\n        a = Shell()\n        a.fit([BIG])\n        self.assertTrue((a.transform(ALL_DATA) == BASE_SHELL).all())\n\n    def test_transform_before_fit(self):\n        a = Shell()\n        with self.assertRaises(ValueError):\n            a.transform(ALL_DATA)\n\n    def test_fit_transform(self):\n        a = Shell()\n        self.assertTrue((a.fit_transform(ALL_DATA) == BASE_SHELL).all())\n\n    def test_add_unknown(self):\n        a = Shell(add_unknown=True)\n        a.fit([METHANE])\n        temp = []\n        for mol in BASE_SHELL:\n            inner = []\n            for atom in mol:\n                inner.append(atom[:2] + [atom[2] + atom[3]])\n            temp.append(inner)\n        expected = numpy.array(temp)\n        self.assertTrue((a.transform(ALL_DATA) == expected).all())\n\n    def test_get_labels(self):\n        a = Shell()\n        a.fit(ALL_DATA)\n        expected = (\'C\', \'H\', \'N\', \'O\')\n        self.assertEqual(a.get_labels(), expected)\n\n    def test_get_labels_unknown(self):\n        a = Shell(add_unknown=True)\n        a.fit(ALL_DATA)\n        expected = (\'C\', \'H\', \'N\', \'O\', UNKNOWN)\n        self.assertEqual(a.get_labels(), expected)\n\n\nclass LocalEncodedBondTest(unittest.TestCase):\n\n    def test_fit(self):\n        a = LocalEncodedBond()\n        a.fit(ALL_DATA)\n        self.assertEqual(a._elements, ((\'C\', ), (\'H\', ), (\'N\', ), (\'O\', )))\n\n    def test_transform(self):\n        a = LocalEncodedBond()\n        a.fit(ALL_DATA)\n        m = a.transform(ALL_DATA)\n        expected_results = numpy.array([17.068978019300587,\n                                        54.629902544876572,\n                                        1006.4744899075993])\n        mm = numpy.array([x.sum() for x in m])\n        self.assertTrue((numpy.allclose(mm, expected_results)))\n\n    def test_small_to_large(self):\n        a = LocalEncodedBond()\n        a.fit([METHANE])\n\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.016125813269,  # mean\n            0.065471987297,  # std\n            0.,              # min\n            0.398807098298,  # max\n            29.02646388512,  # sum\n        ])\n        try:\n            m = a.transform([MID])\n            val = numpy.array([\n                m.mean(),\n                m.std(),\n                m.min(),\n                m.max(),\n                m.sum(),\n            ])\n            numpy.testing.assert_allclose(val, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_max_depth1(self):\n        a = LocalEncodedBond(max_depth=1)\n        a.fit(ALL_DATA)\n        m = a.transform(ALL_DATA)\n        expected_results = numpy.array([6.82758723,\n                                        6.82758018,\n                                        88.75860423])\n        mm = numpy.array([x.sum() for x in m])\n        try:\n            numpy.testing.assert_allclose(mm, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_before_fit(self):\n        a = LocalEncodedBond()\n        with self.assertRaises(ValueError):\n            a.transform(ALL_DATA)\n\n    def test_transform_invalid_smoothing(self):\n        a = LocalEncodedBond(smoothing=\'not real""\')\n        with self.assertRaises(KeyError):\n            a.fit_transform(ALL_DATA)\n\n    def test_transform_invalid_spacing(self):\n        a = LocalEncodedBond(spacing=\'not real""\')\n        with self.assertRaises(KeyError):\n            a.fit_transform(ALL_DATA)\n\n    def test_add_unknown(self):\n        a = LocalEncodedBond(add_unknown=True)\n        a.fit([METHANE])\n        m = a.transform([MID])\n        self.assertEqual(m.shape, (1, 9, 300))\n\n    def test_form(self):\n        a = LocalEncodedBond(form=0)\n        m = a.fit_transform([METHANE])\n        self.assertEqual(m.shape, (1, 5, 100))\n\n    def test_get_labels(self):\n        a = LocalEncodedBond(segments=2, start=0., end=1.)\n        m = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(m.shape[2], len(labels))\n        expected = (\n            \'C_0.0\', \'C_1.0\',\n            \'H_0.0\', \'H_1.0\',\n        )\n        self.assertEqual(labels, expected)\n\n\nclass LocalEncodedAngleTest(unittest.TestCase):\n\n    def test_fit(self):\n        a = LocalEncodedAngle()\n        a.fit(ALL_DATA)\n        expected = ((\'C\', \'C\'), (\'C\', \'H\'), (\'C\', \'N\'), (\'C\', \'O\'),\n                    (\'H\', \'H\'), (\'H\', \'N\'), (\'H\', \'O\'), (\'N\', \'N\'),\n                    (\'N\', \'O\'), (\'O\', \'O\'))\n        self.assertEqual(a._pairs, expected)\n\n    def test_transform(self):\n        a = LocalEncodedAngle()\n        a.fit(ALL_DATA)\n        m = a.transform([METHANE, MID])\n        expected_results = numpy.array([42.968775,\n                                        53.28433])\n        mm = numpy.array([x.sum() for x in m])\n        try:\n            numpy.testing.assert_allclose(mm, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_small_to_large(self):\n        a = LocalEncodedAngle()\n        a.fit([METHANE])\n\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.005350052647,  # mean\n            0.036552192752,  # std\n            0.,              # min\n            0.614984152986,  # max\n            9.630094765591,  # sum\n        ])\n        try:\n            m = a.transform([MID])\n            val = numpy.array([\n                m.mean(),\n                m.std(),\n                m.min(),\n                m.max(),\n                m.sum(),\n            ])\n            numpy.testing.assert_allclose(val, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_max_depth1(self):\n        a = LocalEncodedAngle(max_depth=1)\n        a.fit(ALL_DATA)\n        m = a.transform(ALL_DATA)\n        expected_results = numpy.array([13.078022,\n                                        7.028573,\n                                        146.255683])\n        mm = numpy.array([x.sum() for x in m])\n        try:\n            numpy.testing.assert_allclose(mm, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_before_fit(self):\n        a = LocalEncodedAngle()\n        with self.assertRaises(ValueError):\n            a.transform(ALL_DATA)\n\n    def test_transform_invalid_smoothing(self):\n        a = LocalEncodedAngle(smoothing=\'not real""\')\n        with self.assertRaises(KeyError):\n            a.fit_transform(ALL_DATA)\n\n    def test_add_unknown(self):\n        a = LocalEncodedAngle(add_unknown=True)\n        a.fit([METHANE])\n        m = a.transform([MID])\n        self.assertEqual(m.shape, (1, 9, 300))\n\n    def test_form1(self):\n        a = LocalEncodedAngle(form=1)\n        m = a.fit_transform([METHANE])\n        self.assertEqual(m.shape, (1, 5, 200))\n\n    def test_form0(self):\n        a = LocalEncodedAngle(form=0)\n        m = a.fit_transform([METHANE])\n        self.assertEqual(m.shape, (1, 5, 100))\n\n    def test_get_labels(self):\n        a = LocalEncodedAngle(segments=2)\n        m = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(m.shape[2], len(labels))\n        expected = (\n            \'C-H_0.0\', \'C-H_3.14159\',\n            \'H-H_0.0\', \'H-H_3.14159\',\n        )\n        self.assertEqual(labels, expected)\n\n\nclass LocalCoulombMatrixTest(unittest.TestCase):\n\n    def test_fit(self):\n        a = LocalCoulombMatrix()\n        # This should not do anything\n        a.fit(ALL_DATA)\n\n    def test_transform(self):\n        a = LocalCoulombMatrix(max_occupancy=1)\n        a.fit([METHANE, MID])\n        m = a.transform([METHANE, MID])\n        try:\n            mm = [numpy.linalg.norm(x) for x in (BASE_LOCAL_COULOMB - m)]\n            numpy.testing.assert_array_almost_equal(\n                mm,\n                [0.0, 0.0])\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_reduced(self):\n        a = LocalCoulombMatrix(max_occupancy=1, use_reduced=True)\n        a.fit([METHANE, MID])\n        m = a.transform([METHANE, MID])\n        expected_results = numpy.array([\n            [\n                [36.8581052, 3.53894587, 0.5],\n                [0.5, 3.53879657, 36.8581052],\n                [0.5, 3.53894587, 36.8581052],\n                [0.5, 3.53844797, 36.8581052],\n                [0.5, 3.52229971, 36.8581052],\n            ],\n            [\n                [36.85810519942594, 4.642192257970912, 36.85810519942594],\n                [36.85810519942594, 6.483079598556282, 73.51669471981023],\n                [73.51669471981023, 8.650089711338763, 73.51669471981023],\n                [73.51669471981023, 8.650089711338763, 73.51669471981023],\n                [73.51669471981023, 10.698256448660478, 73.51669471981023],\n                [73.51669471981023, 10.698256448660478, 73.51669471981023],\n                [0.5, 0.04483292, 0.5],\n                [0.5, 1., 0.5],\n                [0.5, 1., 0.5]\n            ]\n        ])\n        try:\n            mm = [numpy.linalg.norm(x) for x in (expected_results - m)]\n            numpy.testing.assert_array_almost_equal(\n                mm,\n                [0.0, 0.0])\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_alpha(self):\n        a = LocalCoulombMatrix(max_occupancy=1, alpha=2., use_reduced=True)\n        a.fit([METHANE, MID])\n        m = a.transform([METHANE, MID])\n        expected_results = numpy.array([\n            [\n                [36.8581052, 5.03182436, 0.5],\n                [0.5, 5.0317536, 36.8581052],\n                [0.5, 5.03182436, 36.8581052],\n                [0.5, 5.03158837, 36.8581052],\n                [0.5, 5.02392255, 36.8581052],\n            ],\n            [\n                [36.8581052, 18.18762711, 36.8581052],\n                [36.8581052, 24.62755378, 73.51669472],\n                [73.51669472, 32.84431333, 73.51669472],\n                [73.51669472, 32.84431333, 73.51669472],\n                [73.51669472, 35.25529211, 73.51669472],\n                [73.51669472, 35.25529211, 73.51669472],\n                [0.5, 0.35524858, 0.5],\n                [0.5, 1., 0.5],\n                [0.5, 1., 0.5]\n            ]\n        ])\n        try:\n            mm = [numpy.linalg.norm(x) for x in (expected_results - m)]\n            numpy.testing.assert_array_almost_equal(\n                mm,\n                [0.0, 0.0])\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_max_occupancy(self):\n        a = LocalCoulombMatrix(max_occupancy=5)\n        a.fit([METHANE, MID])\n        m = a.transform([METHANE, MID])\n        # Reduce to a sum to save space\n        expected_results = [337.53938456166259, 3019.413939202841]\n\n        try:\n            numpy.testing.assert_array_almost_equal(\n                [x.sum() for x in m],\n                expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_before_fit(self):\n        a = LocalCoulombMatrix(max_occupancy=1)\n        # This should not raise an error, becaues no fitting is needed\n        a.transform(ALL_DATA)\n\n    def test_fit_transform(self):\n        a = LocalCoulombMatrix(max_occupancy=1)\n        m = a.fit_transform([METHANE, MID])\n        try:\n            mm = [numpy.linalg.norm(x) for x in (BASE_LOCAL_COULOMB - m)]\n            numpy.testing.assert_array_almost_equal(\n                mm,\n                [0.0, 0.0])\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_labels(self):\n        a = LocalCoulombMatrix(max_occupancy=1)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[2], len(labels))\n        expected = (\n            \'local-coul_0-0\', \'local-coul_0-1\',\n            \'local-coul_1-0\', \'local-coul_1-1\'\n        )\n        self.assertEqual(labels, expected)\n\n    def test_get_labels_reduced(self):\n        a = LocalCoulombMatrix(max_occupancy=1, use_reduced=True)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[2], len(labels))\n        expected = (\'local-coul_0-0\', \'local-coul_0-1\', \'local-coul_1-1\')\n        self.assertEqual(labels, expected)\n\n\nclass BehlerParrinelloTest(unittest.TestCase):\n\n    def test_fit(self):\n        a = BehlerParrinello()\n        a.fit(ALL_DATA)\n        eles = (\'C\', \'H\', \'N\', \'O\')\n        pairs = ((\'C\', \'C\'), (\'C\', \'H\'), (\'C\', \'N\'), (\'C\', \'O\'),\n                 (\'H\', \'H\'), (\'H\', \'N\'), (\'H\', \'O\'), (\'N\', \'N\'),\n                 (\'N\', \'O\'), (\'O\', \'O\'))\n        self.assertEqual(a._elements, eles)\n        self.assertEqual(a._element_pairs, pairs)\n\n    def test_transform_before_fit(self):\n        a = BehlerParrinello()\n        with self.assertRaises(ValueError):\n            a.transform(ALL_DATA)\n\n    def test_predict_outside_fit(self):\n        a = BehlerParrinello()\n        a.fit([METHANE])\n        res = a.transform([MID])\n        expected = numpy.array([[2.812505e-01,\n                                 3.838817e-01,\n                                 1.351548e-17,\n                                 3.040356e-05]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                numpy.array([x.mean(0) for x in res]),\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform(self):\n        a = BehlerParrinello()\n        a.fit(ALL_DATA)\n        m = a.transform(ALL_DATA)\n        expected = numpy.array([\n            [7.30122351e-01, 1.76581653e+00, 0.00000000e+00, 0.00000000e+00,\n             0.00000000e+00, 2.25151270e-02, 0.00000000e+00, 0.00000000e+00,\n             4.39327069e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n             0.00000000e+00, 0.00000000e+00],\n            [2.81250479e-01, 3.83881730e-01, 0.00000000e+00, 5.18353517e-01,\n             1.21609280e-05, 1.35154847e-17, 0.00000000e+00, 3.22304785e-05,\n             3.04035608e-05, 0.00000000e+00, 2.17982869e-06, 0.00000000e+00,\n             0.00000000e+00, 1.45702791e-05],\n            [1.41699320e+00, 4.71095583e-01, 2.07937343e-01, 1.78908050e-01,\n             4.35009106e-04, 1.90163675e-03, 3.00682337e-04, 3.86290776e-04,\n             4.65644534e-04, 1.19773689e-03, 2.65217272e-04, 2.50781918e-06,\n             5.01216130e-06, 4.99291073e-12]\n        ])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                numpy.array([x.mean(0) for x in m]),\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_fit_transform(self):\n        a = BehlerParrinello()\n        m = a.fit_transform(ALL_DATA)\n        expected = numpy.array([\n            [7.30122351e-01, 1.76581653e+00, 0.00000000e+00, 0.00000000e+00,\n             0.00000000e+00, 2.25151270e-02, 0.00000000e+00, 0.00000000e+00,\n             4.39327069e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n             0.00000000e+00, 0.00000000e+00],\n            [2.81250479e-01, 3.83881730e-01, 0.00000000e+00, 5.18353517e-01,\n             1.21609280e-05, 1.35154847e-17, 0.00000000e+00, 3.22304785e-05,\n             3.04035608e-05, 0.00000000e+00, 2.17982869e-06, 0.00000000e+00,\n             0.00000000e+00, 1.45702791e-05],\n            [1.41699320e+00, 4.71095583e-01, 2.07937343e-01, 1.78908050e-01,\n             4.35009106e-04, 1.90163675e-03, 3.00682337e-04, 3.86290776e-04,\n             4.65644534e-04, 1.19773689e-03, 2.65217272e-04, 2.50781918e-06,\n             5.01216130e-06, 4.99291073e-12]\n        ])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                numpy.array([x.mean(0) for x in m]),\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_labels(self):\n        a = BehlerParrinello()\n        m = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(m[0].shape[1], len(labels))\n        expected = (\'C\', \'H\', \'C-H\', \'H-H\')\n        self.assertEqual(labels, expected)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_base.py,0,"b'import os\nimport unittest\nimport json\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from io import StringIO\n\nimport numpy\n\nfrom molml.base import BaseFeature, SetMergeMixin, InputTypeMixin, _func_star\nfrom molml.base import EncodedFeature, FormMixin\n\nfrom .constants import METHANE_ELEMENTS, METHANE_COORDS, METHANE_PATH\nfrom .constants import METHANE, METHANE_NUMBERS, METHANE_CONNECTIONS\n\n\nMETHANE_ATOMS = numpy.array([[1, 4]])\n\n\nclass TestFeature1(BaseFeature):\n    \'\'\'\n    Some example doc string.\n\n    References\n    ----------\n    Doe, J. Nature. (2016).\n    \'\'\'\n    LABELS = (\'labels\', )\n    ATTRIBUTES = (\'data\', )\n\n    def __init__(self, input_type=\'list\', n_jobs=1, data=None, value=None):\n        super(TestFeature1, self).__init__(input_type, n_jobs)\n        self.labels = (\'C\', \'B\', \'A\')\n        self.data = data\n        self.value = value\n\n    def fit(self, X, y=None):\n        self.data = [1]\n\n    def _para_transform(self, X):\n        return [1]\n\n\nclass TestFeature2(BaseFeature):\n    \'\'\'\n    Some example doc string.\n\n    References\n    ----------\n    Doe, J. Nature. (2016).\n\n    Smith, J. Science. (2010).\n    \'\'\'\n    LABELS = (\'labels1\', \'labels2\')\n    ATTRIBUTES = (\'data1\', \'data2\')\n\n    def __init__(self, value=None):\n        self.labels1 = (\'A\', \'B\', \'C\')\n        self.labels2 = (\'DD\', \'CC\')\n        self.data1 = value\n        self.data2 = value\n\n\nclass TestFeature3(BaseFeature):\n    \'\'\'\n    Some example doc string.\n\n    References\n    ----------\n    Doe, J. Nature. (2016).\n\n    Smith, J. Science. (2010).\n\n    Other\n    -----\n    Something else.\n    \'\'\'\n    LABELS = None\n    ATTRIBUTES = None\n\n\nclass TestFeature4(BaseFeature):\n    \'\'\'\n    Some example doc string.\n\n    References\n    ----------\n    Doe, J. Nature. (2016).\n\n    Smith, J. Science. (2010).\n    \'\'\'\n    LABELS = ((\'func\', \'labels\'), )\n    ATTRIBUTES = (\'data1\', \'data2\')\n\n    def __init__(self, value=None):\n        self.labels = (\'A\', \'B\', \'C\')\n        self.data1 = value\n        self.data2 = value\n\n    def func(self, labels):\n        return labels\n\n\nclass TestFeature5(BaseFeature):\n    \'\'\'\n    \'\'\'\n    LABELS = ((\'func\', None), )\n    ATTRIBUTES = (\'data1\', \'data2\')\n\n    def __init__(self, value=None):\n        self.labels = (\'A\', \'B\', \'C\')\n        self.data1 = value\n        self.data2 = value\n\n    def func(self):\n        return [\'A\', \'B\', \'C\']\n\n\nclass TestFeature6(BaseFeature):\n    \'\'\'\'\'\'\n    LABELS = ((\'func\', None), )\n\n    def __init__(self, value=None):\n        pass\n\n    def func(self):\n        raise ValueError\n\n\n#################################################\nclass OtherTest(unittest.TestCase):\n\n    def test__func_star(self):\n        res = _func_star((lambda x, y: x + y, 2, 3))\n        self.assertEqual(res, 5)\n\n\nclass BaseFeatureTest(unittest.TestCase):\n\n    def test_map_n_jobs_negative(self):\n        a = BaseFeature(n_jobs=-1)\n        res = a.map(lambda x: x ** 2, range(10))\n        self.assertEqual(res, [x ** 2 for x in range(10)])\n\n    def test_map_n_jobs_one(self):\n        a = BaseFeature(n_jobs=1)\n        res = a.map(lambda x: x ** 2, range(10))\n        self.assertEqual(res, [x ** 2 for x in range(10)])\n\n    def test_map_n_jobs_greater(self):\n        a = BaseFeature(n_jobs=2)\n        res = a.map(lambda x: x ** 2, range(10))\n        self.assertEqual(res, [x ** 2 for x in range(10)])\n\n    def test_reduce_n_jobs_negative(self):\n        a = BaseFeature(n_jobs=-1)\n        res = a.reduce(lambda x, y: x + y, range(10))\n        self.assertEqual(res, sum(range(10)))\n\n    def test_reduce_n_jobs_one(self):\n        a = BaseFeature(n_jobs=1)\n        res = a.reduce(lambda x, y: x + y, range(10))\n        self.assertEqual(res, sum(range(10)))\n\n    def test_reduce_n_jobs_greater(self):\n        a = BaseFeature(n_jobs=2)\n        res = a.reduce(lambda x, y: x + y, range(10))\n        self.assertEqual(res, sum(range(10)))\n\n    def test_convert_input_list(self):\n        a = BaseFeature(input_type=""list"")\n        data = a.convert_input(METHANE)\n        self.assertEqual(data.connections, METHANE_CONNECTIONS)\n        self.assertEqual(data.elements.tolist(), METHANE_ELEMENTS)\n        self.assertEqual(data.coords.tolist(), METHANE_COORDS.tolist())\n\n    def test_convert_input_list_numbers(self):\n        a = BaseFeature(input_type=""list"")\n        data = a.convert_input([METHANE_NUMBERS, METHANE_COORDS])\n        self.assertEqual(data.numbers.tolist(), METHANE_NUMBERS)\n        self.assertEqual(data.connections, METHANE_CONNECTIONS)\n        self.assertEqual(data.coords.tolist(), METHANE_COORDS.tolist())\n\n    def test_convert_input_list_connections(self):\n        a = BaseFeature(input_type=""list"")\n        data = a.convert_input([METHANE_ELEMENTS, METHANE_COORDS,\n                                METHANE_CONNECTIONS])\n        self.assertEqual(data.connections, METHANE_CONNECTIONS)\n        self.assertEqual(data.elements.tolist(), METHANE_ELEMENTS)\n        self.assertEqual(data.coords.tolist(), METHANE_COORDS.tolist())\n\n    def test_convert_input_filename(self):\n        a = BaseFeature(input_type=""filename"")\n        base_path = os.path.join(os.path.dirname(__file__), ""data"", ""methane"")\n        for ending in (\'.xyz\', \'.out\'):\n            path = base_path + ending\n            data = a.convert_input(path)\n            self.assertEqual(data.elements.tolist(), METHANE_ELEMENTS)\n            self.assertEqual(data.connections, METHANE_CONNECTIONS)\n            try:\n                numpy.testing.assert_array_almost_equal(\n                    data.coords, METHANE_COORDS)\n            except AssertionError as e:\n                self.fail(e)\n\n    def test_convert_input_ele_coords(self):\n        a = BaseFeature(input_type=[""elements"", ""coords""])\n        data = a.convert_input([METHANE_ELEMENTS, METHANE_COORDS])\n        self.assertEqual(data.elements.tolist(), METHANE_ELEMENTS)\n        try:\n            numpy.testing.assert_array_almost_equal(\n                data.coords, METHANE_COORDS)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_convert_input_num_ele(self):\n        a = BaseFeature(input_type=[""numbers"", ""elements""])\n        data = a.convert_input([METHANE_NUMBERS, METHANE_ELEMENTS])\n        self.assertEqual(data.elements.tolist(), METHANE_ELEMENTS)\n        self.assertEqual(data.numbers.tolist(), METHANE_NUMBERS)\n\n    def test_convert_input_invalid_list(self):\n        a = BaseFeature(input_type=[""error""])\n        with self.assertRaises(TypeError):\n            a.convert_input(""bad data"")\n\n    def test_convert_input_error(self):\n        a = BaseFeature(input_type=""error"")\n        with self.assertRaises(ValueError):\n            a.convert_input(""bad data"")\n\n    def test_convert_input_callable(self):\n        a = BaseFeature(input_type=lambda x: (x, x ** 2))\n        res = a.convert_input(10)\n        self.assertEqual(res, (10, 100))\n\n    def test_slugify(self):\n        a = TestFeature1()\n        expected = [\n                    \'TestFeature1\',\n                    \'data=None\',\n                    \'value=None\',\n                    ]\n        self.assertEqual(a.slugify(), \'__\'.join(expected))\n\n    def test_get_params(self):\n        a = BaseFeature(n_jobs=10)\n        expected = {""input_type"": ""list"", ""n_jobs"": 10}\n        self.assertEqual(a.get_params(), expected)\n\n    def test_set_params(self):\n        a = BaseFeature(n_jobs=10)\n        new = {\n                ""input_type"": ""coords"",\n                ""n_jobs"": 100,\n                ""fake"": None,\n        }\n        a.set_params(**new)\n        self.assertEqual(a.input_type, ""coords"")\n        self.assertEqual(a.n_jobs, 100)\n        with self.assertRaises(AttributeError):\n            a.fake\n\n    def test_get_labels(self):\n        a = TestFeature1()\n        self.assertEqual(a.get_labels(), (\'C\', \'B\', \'A\'))\n        b = TestFeature2()\n        self.assertEqual(b.get_labels(), (\'A\', \'B\', \'C\', \'DD\', \'CC\'))\n        c = TestFeature3()\n        self.assertEqual(c.get_labels(), tuple())\n        d = TestFeature4()\n        self.assertEqual(d.get_labels(), (\'A\', \'B\', \'C\'))\n        e = TestFeature5()\n        self.assertEqual(e.get_labels(), (\'A\', \'B\', \'C\'))\n        f = TestFeature6()\n        with self.assertRaises(ValueError):\n            f.get_labels()\n\n    def test_check_fit(self):\n        a = TestFeature1(data=1)\n        self.assertIsNone(a.check_fit())\n        b = TestFeature2(value=1)\n        self.assertIsNone(b.check_fit())\n        c = TestFeature3()\n        self.assertIsNone(c.check_fit())\n\n        with self.assertRaises(ValueError):\n            a = TestFeature1()\n            a.check_fit()\n\n        with self.assertRaises(ValueError):\n            b = TestFeature2()\n            b.check_fit()\n\n    def test_get_citation(self):\n        citation = ""MolML https://github.com/crcollins/molml""\n        self.assertEqual(citation, BaseFeature.get_citation())\n        self.assertEqual(""Doe, J. Nature. (2016)."",\n                         TestFeature1.get_citation())\n        expected = ""Doe, J. Nature. (2016).\\n""\n        expected += ""Smith, J. Science. (2010).""\n        self.assertEqual(expected, TestFeature2.get_citation())\n        self.assertEqual(expected, TestFeature3.get_citation())\n\n    def test_save_json(self):\n        a = TestFeature1()\n        f = StringIO()\n        a.save_json(f)\n        string = f.getvalue()\n        data = json.loads(string)\n        base = a.__module__\n        expected = {\'parameters\': {\'n_jobs\': 1,\n                                   \'input_type\': \'list\',\n                                   \'data\': None,\n                                   \'value\': None},\n                    \'attributes\': {\'data\': None},\n                    \'transformer\': base + \'.TestFeature1\'}\n        self.assertEqual(data, expected)\n\n        path = \'/tmp/somefile.json\'\n        a.save_json(path)\n        with open(path, \'r\') as f:\n            data = json.load(f)\n            self.assertEqual(data, expected)\n\n    def test_to_json_no_attributes(self):\n        a = TestFeature3()\n        data = a.to_json()\n        base = a.__module__\n        expected = {\'parameters\': {\'n_jobs\': 1,\n                                   \'input_type\': \'list\'},\n                    \'attributes\': {},\n                    \'transformer\': base + \'.TestFeature3\'}\n        self.assertEqual(data, expected)\n\n    def test_save_json_nested_obj(self):\n        a = TestFeature1(value=TestFeature1())\n        data = a.to_json()\n        base = a.__module__\n        expected = {\n            \'attributes\': {\'data\': None},\n            \'parameters\': {\n                \'n_jobs\': 1,\n                \'input_type\': \'list\',\n                \'value\': {\n                    \'parameters\': {\n                        \'n_jobs\': 1,\n                        \'input_type\': \'list\',\n                        \'value\': None,\n                        \'data\': None,\n                    },\n                    \'attributes\': {\'data\': None},\n                    \'transformer\': base + \'.TestFeature1\',\n                },\n                \'data\': None,\n            },\n            \'transformer\': base + \'.TestFeature1\'\n        }\n        self.assertEqual(data, expected)\n\n    def test_transform(self):\n        a = TestFeature1()\n        a.fit([1])\n        res = a.transform([1, 2, 3])\n        expected = numpy.array([[1], [1], [1]])\n        try:\n            numpy.testing.assert_array_almost_equal(res, expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_fit_transform(self):\n        a = TestFeature1()\n        res = a.fit_transform([1, 2, 3])\n        expected = numpy.array([[1], [1], [1]])\n        try:\n            numpy.testing.assert_array_almost_equal(res, expected)\n        except AssertionError as e:\n            self.fail(e)\n\n\nclass TestSetMergeMixin(unittest.TestCase):\n    def test_multiple_attributes(self):\n        class TestFeature(SetMergeMixin, BaseFeature):\n            ATTRIBUTES = (""test1"", ""test2"")\n\n            def __init__(self, *args, **kwargs):\n                super(TestFeature, self).__init__(*args, **kwargs)\n\n            def _para_fit(self, X):\n                return (set([1, 2, 3]), set([2, 3, 4]))\n\n        a = TestFeature(input_type=""filename"")\n        a.fit([METHANE_PATH, METHANE_PATH])\n        self.assertEqual((1, 2, 3), a.test1)\n        self.assertEqual((2, 3, 4), a.test2)\n\n    def test_fit(self):\n        class TestFeature(SetMergeMixin, BaseFeature):\n            ATTRIBUTES = (""test1"", )\n\n            def __init__(self, *args, **kwargs):\n                super(TestFeature, self).__init__(*args, **kwargs)\n\n            def _para_fit(self, X):\n                return set([1, 2, 3])\n\n        a = TestFeature(input_type=""filename"")\n        a.fit([METHANE_PATH, METHANE_PATH])\n        self.assertEqual((1, 2, 3), a.test1)\n\n\nclass Feature(InputTypeMixin, BaseFeature):\n\n    def __init__(self, input_type=None, transformer=None, *args, **kwargs):\n        super(Feature, self).__init__(input_type=input_type, *args, **kwargs)\n        self.check_transformer(transformer)\n        self.transformer = transformer\n\n\nclass TestInputTypeMixin(unittest.TestCase):\n\n    def test_input_type_default(self):\n        a = Feature()\n        self.assertEqual(""list"", a.input_type)\n\n    def test_input_type_mismatch(self):\n        trans = BaseFeature(input_type=""filename"")\n        with self.assertRaises(ValueError):\n            Feature(input_type=""list"", transformer=trans)\n\n    def test_input_type_match(self):\n        trans = BaseFeature(input_type=""filename"")\n        a = Feature(input_type=""filename"", transformer=trans)\n        self.assertEqual(""filename"", a.input_type)\n\n    def test_input_type_normal(self):\n        a = Feature(input_type=""filename"")\n        self.assertEqual(""filename"", a.input_type)\n\n    def test_input_type_from_param(self):\n        trans = BaseFeature(input_type=""filename"")\n        a = Feature(transformer=trans)\n        self.assertEqual(""filename"", a.input_type)\n\n\nclass FormFeature(FormMixin, BaseFeature):\n    ATTRIBUTES = (\'data\', )\n    LABELS = (\'data\', )\n\n    def __init__(self, input_type=None, form=1, add_unknown=False,\n                 *args, **kwargs):\n        super(FormFeature, self).__init__(input_type=input_type,\n                                          *args, **kwargs)\n        self.data = ((\'A\', \'B\'), (\'B\', \'C\'), (\'C\', \'D\'))\n        self.form = form\n        self.add_unknown = add_unknown\n\n    def _para_transform(self, X):\n        pass\n\n\nclass TestFormMixin(unittest.TestCase):\n    def test_get_encoded_labels_unknown(self):\n        a = FormFeature(form=1)\n        labels = a.get_labels()\n        self.assertEqual(labels, a.data)\n\n    def test_get_idx_map(self):\n        a = FormFeature(form=1)\n        b = a.get_idx_map()\n        self.assertIsNotNone(a._idx_map)\n        c = a.get_idx_map()\n        self.assertIs(b, c)\n\n    def test_rebuild_idx_map_on_change(self):\n        a = FormFeature(form=1)\n        b = a.get_idx_map()\n        self.assertIsNotNone(a._idx_map)\n        c = a.get_idx_map()\n        self.assertIs(b, c)\n        a.data = ((\'D\', \'B\'), )\n        d = a.get_idx_map()\n        self.assertIsNot(d, c)\n\n    def test_get_group_order(self):\n        a = FormFeature(form=1)\n        self.assertEqual(a.get_group_order(None),\n                         [(\'A\', ), (\'B\', ), (\'C\', ), (\'D\', )])\n\n    def test_transform(self):\n        a = FormFeature(form=1)\n        a.transform([None, None])\n        self.assertTrue(hasattr(a, \'_idx_map\'))\n\n\nclass TestEncodedFeature(unittest.TestCase):\n\n    def test_encode_values(self):\n        a = EncodedFeature(segments=5)\n        data = [((0, ), 3, 1), (None, 1, 1), ((1, ), 3, 2)]\n        res = a.encode_values(data, (2, ))\n        expected = [\n            0, 1.997889e-159, 5.399097e-2, 8.363952e-210, 0,\n            0, 3.995779e-159, 1.079819e-1, 1.672790e-209, 0]\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_encode_values_saved_length(self):\n        a = EncodedFeature(segments=5)\n        data = [((0, 1), 3, 1), ((1, 0), 3, 2), (None, 1, 1)]\n        res = a.encode_values(data, (2, 2), saved_lengths=1)\n\n        expected = numpy.zeros((2, 10))\n        expected = numpy.array([\n            [0, 0, 0, 0, 0, 0, 1.997889e-159, 5.399097e-002, 8.363952e-210, 0],\n            [0, 3.995779e-159, 1.079819e-1, 1.672790e-209, 0, 0, 0, 0, 0, 0],\n        ])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_encoded_labels(self):\n        a = EncodedFeature(segments=3, start=1., end=3.)\n        labels = a.get_encoded_labels([(\'A\', \'B\'), (\'C\', \'D\')])\n        expected = [\n            \'A-B_1.0\', \'A-B_2.0\', \'A-B_3.0\',\n            \'C-D_1.0\', \'C-D_2.0\', \'C-D_3.0\',\n        ]\n        self.assertEqual(labels, expected)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_crystal.py,0,"b'import unittest\n\nimport numpy\n\nfrom molml.molecule import Connectivity\nfrom molml.crystal import GenerallizedCrystal\nfrom molml.crystal import EwaldSumMatrix, SineMatrix\n\n\nH_ELES = [\'H\']\nH_NUMS = [1]\nH_COORDS = numpy.array([[0.0, 0.0, 0.0]])\nH_UNIT = numpy.array([\n    [2., .5, 0.],\n    [.25, 1., 0.],\n    [0., .3, 1.],\n])\nH_INPUT = (""elements"", ""coords"", ""unit_cell"")\nH = (H_ELES, H_COORDS, H_UNIT)\n\n\nH2_ELES = [\'H\', \'H\']\nH2_NUMS = [1, 1]\nH2_COORDS = numpy.array([\n    [0.0, 0.0, 0.0],\n    [1.0, 0.0, 0.0],\n])\nH2_CONNS = {\n    0: {1: \'1\'},\n    1: {0: \'1\'},\n}\nH2_UNIT = numpy.array([\n    [2., .5, 0.],\n    [.25, 1., 0.],\n    [0., .3, 1.],\n])\nH2 = (H2_ELES, H2_COORDS, H2_UNIT)\n\n\nclass GenerallizedCrystalTest(unittest.TestCase):\n    def test_fit(self):\n        t = Connectivity(input_type=H_INPUT)\n        a = GenerallizedCrystal(transformer=t, radius=2.5)\n        a.fit([H])\n        self.assertEqual(a.transformer.get_labels(), (\'H\', ))\n\n    def test_transform(self):\n        t = Connectivity(input_type=H_INPUT)\n        a = GenerallizedCrystal(transformer=t, radius=2.5)\n        a.fit([H])\n        res = a.transform([H])\n        self.assertEqual(res, numpy.array([[37]]))\n\n    def test_transform_before_fit(self):\n        t = Connectivity(input_type=H_INPUT)\n        a = GenerallizedCrystal(transformer=t, radius=2.5)\n        with self.assertRaises(ValueError):\n            a.transform([H])\n\n    def test_fit_transform(self):\n        t = Connectivity(input_type=H_INPUT)\n        a = GenerallizedCrystal(transformer=t, radius=2.5)\n        res = a.fit_transform([H])\n        self.assertEqual(res, numpy.array([[37]]))\n\n    def test_radius_and_units(self):\n        t = Connectivity(input_type=H_INPUT)\n        with self.assertRaises(ValueError):\n            GenerallizedCrystal(transformer=t, radius=2.5, units=2)\n\n\nclass EwaldSumMatrixCrystalTest(unittest.TestCase):\n    def test_fit(self):\n        a = EwaldSumMatrix()\n        a.fit([(H2_ELES, H2_COORDS)])\n        self.assertEqual(a._max_size, 2)\n\n    def test_transform(self):\n        a = EwaldSumMatrix(input_type=H_INPUT)\n        a.fit([H2])\n        res = a.transform([H2])\n        expected = numpy.array([[-1.68059225, 0.94480435,\n                                 0.94480435, -1.68059225]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_G_max(self):\n        a = EwaldSumMatrix(input_type=H_INPUT, G_max=2)\n        a.fit([H2])\n        res = a.transform([H2])\n        expected = numpy.array([[-1.68059225, 0.945167,\n                                 0.945167, -1.68059225]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_L_max(self):\n        a = EwaldSumMatrix(input_type=H_INPUT, L_max=2)\n        a.fit([H2])\n        res = a.transform([H2])\n        expected = numpy.array([[-1.68059225, 0.43748,\n                                 0.43748, -1.68059225]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_small_to_large_transform(self):\n        a = EwaldSumMatrix(input_type=H_INPUT)\n        a.fit([H])\n        with self.assertRaises(ValueError):\n            a.transform([H2])\n\n    def test_large_to_small_transform(self):\n        a = EwaldSumMatrix(input_type=H_INPUT)\n        a.fit([(H2_ELES, H2_COORDS, H2_UNIT)])\n        res = a.transform([H])\n        expected = numpy.array([[-1.944276, 0., 0., 0.]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_before_fit(self):\n        a = EwaldSumMatrix()\n        with self.assertRaises(ValueError):\n            a.transform([H])\n\n    def test_fit_transform(self):\n        a = EwaldSumMatrix(input_type=H_INPUT)\n        res = a.fit_transform([H2])\n        expected = numpy.array([[-1.68059225, 0.94480435,\n                                 0.94480435, -1.68059225]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_sort(self):\n        a = EwaldSumMatrix(input_type=H_INPUT, sort=True)\n        res = a.fit_transform([H2])\n        expected = numpy.array([[-1.68059225, 0.94480435,\n                                 0.94480435, -1.68059225]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_eigen(self):\n        a = EwaldSumMatrix(input_type=H_INPUT, eigen=True)\n        res = a.fit_transform([H2])\n        expected = numpy.array([[-0.735788, -2.625397]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n\nclass SineMatrixTest(unittest.TestCase):\n    def test_fit(self):\n        a = SineMatrix()\n        a.fit([(H2_ELES, H2_COORDS)])\n        self.assertEqual(a._max_size, 2)\n\n    def test_transform(self):\n        a = SineMatrix(input_type=H_INPUT)\n        a.fit([H2])\n        res = a.transform([H2])\n        expected = numpy.array([[0.5, 0.475557, 0.475557, 0.5]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_small_to_large_transform(self):\n        a = SineMatrix(input_type=H_INPUT)\n        a.fit([H])\n        with self.assertRaises(ValueError):\n            a.transform([H2])\n\n    def test_large_to_small_transform(self):\n        a = SineMatrix(input_type=H_INPUT)\n        a.fit([H2])\n        res = a.transform([H])\n        expected = numpy.array([[0.5, 0., 0., 0.]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_before_fit(self):\n        a = SineMatrix()\n        with self.assertRaises(ValueError):\n            a.transform([H])\n\n    def test_fit_transform(self):\n        a = SineMatrix(input_type=H_INPUT)\n        res = a.fit_transform([H2])\n        expected = numpy.array([[0.5, 0.475557, 0.475557, 0.5]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_sort(self):\n        a = SineMatrix(input_type=H_INPUT, sort=True)\n        res = a.fit_transform([H2])\n        expected = numpy.array([[0.5, 0.475557, 0.475557, 0.5]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_eigen(self):\n        a = SineMatrix(input_type=H_INPUT, eigen=True)\n        res = a.fit_transform([H2])\n        expected = numpy.array([[0.975557, 0.024443]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected)\n        except AssertionError as e:\n            self.fail(e)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_fragment.py,0,"b'import os\nimport unittest\n\nimport numpy\nfrom sklearn.pipeline import Pipeline\n\nfrom molml.fragment import FragmentMap\nfrom molml.molecule import Connectivity\n\nfrom .constants import METHANE_NUMBERS, MID_NUMBERS\nfrom .constants import DATA_PATH, METHANE_PATH, MID_PATH\n\n\nALL = (METHANE_PATH, MID_PATH)\nALL_NUMS = [METHANE_NUMBERS, MID_NUMBERS]\nALL_FEATURES = numpy.array([\n    [[1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0]],\n    [[1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1],\n     [0, 1, 0], [0, 1, 0], [0, 1, 0]]])\n\nLABELS = [\'methane\', \'mid\']\n\n\nclass FragmentMapTest(unittest.TestCase):\n    def test_error_if_no_transformer(self):\n        with self.assertRaises(ValueError):\n            FragmentMap()\n\n    def test_filename_to_label(self):\n        trans = Connectivity(input_type=""filename"")\n        a = FragmentMap(transformer=trans, filename_to_label=\'identity\',\n                        label_to_filename=(DATA_PATH, ))\n        func = a._get_filename_to_label()\n        self.assertEqual(func(METHANE_PATH), METHANE_PATH)\n\n    def test_callable_filename_to_label(self):\n        trans = Connectivity(input_type=""filename"")\n        a = FragmentMap(transformer=trans, filename_to_label=lambda x: x[-5:],\n                        label_to_filename=(DATA_PATH, ))\n        func = a._get_filename_to_label()\n        self.assertEqual(func(METHANE_PATH), METHANE_PATH[-5:])\n\n    def test_invalid_filename_to_label(self):\n        trans = Connectivity(input_type=""filename"")\n        a = FragmentMap(transformer=trans, filename_to_label=\'bad\')\n        with self.assertRaises(KeyError):\n            a.fit([ALL])\n\n    def test_label_to_filename(self):\n        trans = Connectivity(input_type=""filename"")\n        a = FragmentMap(transformer=trans, label_to_filename=(DATA_PATH, ))\n        # The paths are sorted when searching\n        path = METHANE_PATH.replace(\'.out\', \'.cry\')\n        self.assertEqual(a._get_label_to_filename()(\'methane\'), path)\n\n    def test_label_to_filename_not_found(self):\n        trans = Connectivity(input_type=""filename"")\n        a = FragmentMap(transformer=trans, label_to_filename=(DATA_PATH, ))\n        with self.assertRaises(ValueError):\n            a._get_label_to_filename()(\'not real\')\n\n    def test_callable_label_to_filename(self):\n        trans = Connectivity(input_type=""filename"")\n\n        def func(x):\n            return os.path.join(DATA_PATH, x)\n\n        a = FragmentMap(transformer=trans, label_to_filename=func)\n        self.assertEqual(a._get_label_to_filename()(\'test\'),\n                         os.path.join(DATA_PATH, \'test\'))\n\n    def test_invalid_label_to_filename(self):\n        trans = Connectivity(input_type=""filename"")\n        a = FragmentMap(transformer=trans, label_to_filename=lambda x: 1)\n        with self.assertRaises(KeyError):\n            a.fit_transform(ALL)\n\n    def test_bad_input_type(self):\n        trans = Connectivity(input_type=""filename"")\n        a = FragmentMap(input_type=\'bad\', transformer=trans)\n        with self.assertRaises(ValueError):\n            a.fit([ALL])\n\n    def test_label_input_type(self):\n        trans = Connectivity(input_type=""filename"")\n        a = FragmentMap(input_type=\'label\', transformer=trans,\n                        label_to_filename=(DATA_PATH, ))\n        a.fit([[\'methane\', \'mid\', \'bad\']])\n\n    def test_fit(self):\n        trans = Connectivity(input_type=""filename"")\n        a = FragmentMap(transformer=trans)\n        a.fit([ALL])\n        expected = {\'mid\': [2, 3, 4], \'methane\': [1, 4, 0]}\n        simplified = {x: y.tolist() for x, y in a._x_fragments.items()}\n        self.assertEqual(expected, simplified)\n\n    def test_transform_before_fit(self):\n        trans = Connectivity(input_type=""filename"")\n        a = FragmentMap(transformer=trans)\n        with self.assertRaises(ValueError):\n            a.transform(ALL)\n\n    def test_transform(self):\n        trans = Connectivity(input_type=""filename"")\n        a = FragmentMap(transformer=trans)\n        a.fit([ALL])\n        res = a.transform([ALL])\n        expected = numpy.array([[[1, 4, 0],\n                                 [2, 3, 4]]])\n        try:\n            numpy.testing.assert_array_almost_equal(expected, res)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_fit_transform(self):\n        trans = Connectivity(input_type=""filename"")\n        a = FragmentMap(transformer=trans)\n        res = a.fit_transform([ALL])\n        expected = numpy.array([[[1, 4, 0],\n                                 [2, 3, 4]]])\n        try:\n            numpy.testing.assert_array_almost_equal(expected, res)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_labels(self):\n        trans = Connectivity(input_type=""filename"")\n        a = FragmentMap(transformer=trans)\n        res = a.fit_transform([ALL])\n        labels = a.get_labels()\n        self.assertEqual(res.shape[2], len(labels))\n        expected = (\'C\', \'H\', \'O\')\n        self.assertEqual(labels, expected)\n\n    def test_get_labels_no_labels(self):\n        trans = Pipeline([(\'Con\', Connectivity(input_type=""filename""))])\n        a = FragmentMap(transformer=trans)\n        res = a.fit_transform([ALL])\n        labels = a.get_labels()\n        self.assertEqual(res.shape[2], len(labels))\n        expected = (\'0\', \'1\', \'2\')\n        self.assertEqual(labels, expected)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_io.py,0,"b'import unittest\nimport os\n\nimport numpy\n\nfrom molml.io import read_file_data\nfrom molml.io import read_out_data, read_xyz_data, read_mol2_data\nfrom molml.io import read_cry_data\n\n\nDATA_PATH = os.path.join(os.path.dirname(__file__), ""data"")\nELEMENTS = [\'C\', \'H\', \'H\', \'H\', \'H\']\nNUMBERS = [6, 1, 1, 1, 1]\nCOORDS = [\n    [0.99826008, -0.00246000, -0.00436000],\n    [2.09021016, -0.00243000, 0.00414000],\n    [0.63379005, 1.02686007, 0.00414000],\n    [0.62704006, -0.52773003, 0.87811010],\n    [0.64136006, -0.50747003, -0.90540005],\n]\nUNIT = [\n    [2.0, 0.5, 0.05],\n    [0.0, 2.0, 0.05],\n    [0.0, 0.1, 2.0],\n]\n\n\nclass IOTest(unittest.TestCase):\n\n    def test_read_file_data(self):\n        base_path = os.path.join(DATA_PATH, ""methane"")\n        data = (\n                (\'.out\', read_out_data),\n                (\'.xyz\', read_xyz_data),\n                (\'.mol2\', read_mol2_data),\n                (\'.cry\', read_cry_data),\n        )\n        for ending, func in data:\n            path = base_path + ending\n            v1 = func(path)\n            v2 = read_file_data(path)\n            self.assertEqual(v1.elements.tolist(), v2.elements.tolist())\n            self.assertEqual(v1.numbers.tolist(), v2.numbers.tolist())\n            self.assertTrue(numpy.allclose(v1.coords, v2.coords))\n\n            self.assertEqual(v1.elements.tolist(), ELEMENTS)\n            try:\n                numpy.testing.assert_array_almost_equal(\n                    v1.coords,\n                    COORDS,\n                    decimal=3)\n            except AssertionError as e:\n                self.fail(e)\n            self.assertEqual(v1.numbers.tolist(), NUMBERS)\n\n    def test_empty_file(self):\n        path = os.path.join(DATA_PATH, ""empty"")\n        read_mol2_data(path)\n\n    def test_read_cry_data_unit(self):\n        path = os.path.join(DATA_PATH, ""methane.cry"")\n        v = read_cry_data(path)\n        try:\n            numpy.testing.assert_array_almost_equal(\n                v.unit_cell,\n                UNIT,\n                decimal=3)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_read_file_data_error(self):\n        path = ""garbage.nope""\n        with self.assertRaises(ValueError):\n            read_file_data(path)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_kernel.py,0,"b'import unittest\n\nimport numpy\n\nfrom molml.kernel import AtomKernel\nfrom molml.atom import Shell\n\nfrom .constants import METHANE_NUMBERS, MID_NUMBERS\nfrom .constants import METHANE_PATH, MID_PATH\n\n\nALL = (METHANE_PATH, MID_PATH)\nALL_NUMS = [METHANE_NUMBERS, MID_NUMBERS]\nALL_FEATURES = numpy.array([\n    [[1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0]],\n    [[1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1],\n     [0, 1, 0], [0, 1, 0], [0, 1, 0]]])\nRBF_KERNEL = numpy.array([\n    [17., 14.],\n    [14., 29.],\n])\nLAPLACE_KERNEL = numpy.array([\n    [17., 2.563417],\n    [2.563417, 17.955894],\n])\nLINEAR_KERNEL = numpy.array([\n    [32., 0.],\n    [0., 14.],\n])\n\n\nclass AtomKernelTest(unittest.TestCase):\n    def test_fit_features(self):\n        trans = Shell(input_type=""filename"")\n        feats = trans.fit_transform(ALL)\n        a = AtomKernel()\n        values = list(zip(feats, ALL_NUMS))\n        a.fit(values)\n        self.assertEqual(ALL_NUMS, list(a._numbers))\n        self.assertEqual(list(ALL_FEATURES), list(a._features))\n\n    def test_fit_transformer(self):\n        trans = Shell(input_type=""filename"")\n        a = AtomKernel(transformer=trans)\n        a.fit(ALL)\n        self.assertEqual(ALL_NUMS, [x.tolist() for x in a._numbers])\n        self.assertEqual(list(ALL_FEATURES), list(a._features))\n\n    def test_transform_before_fit(self):\n        a = AtomKernel()\n        with self.assertRaises(ValueError):\n            a.transform(ALL)\n\n    def test_transform_features(self):\n        trans = Shell(input_type=""filename"")\n        feats = trans.fit_transform(ALL)\n        a = AtomKernel()\n        values = list(zip(feats, ALL_NUMS))\n        a.fit(values)\n        res = a.transform(values)\n        try:\n            numpy.testing.assert_array_almost_equal(RBF_KERNEL, res)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_transformer(self):\n        trans = Shell(input_type=""filename"")\n        a = AtomKernel(transformer=trans)\n        a.fit(ALL)\n        res = a.transform(ALL)\n        try:\n            numpy.testing.assert_array_almost_equal(RBF_KERNEL, res)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_fit_transform_features(self):\n        trans = Shell(input_type=""filename"")\n        feats = trans.fit_transform(ALL)\n        a = AtomKernel()\n        values = list(zip(feats, ALL_NUMS))\n        res = a.fit_transform(values)\n        try:\n            numpy.testing.assert_array_almost_equal(RBF_KERNEL, res)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_fit_transform_transformer(self):\n        trans = Shell(input_type=""filename"")\n        a = AtomKernel(transformer=trans)\n        res = a.fit_transform(ALL)\n        try:\n            numpy.testing.assert_array_almost_equal(RBF_KERNEL, res)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_same_element(self):\n        # Set depth=2 so the comparison is not trivial\n        trans = Shell(input_type=""filename"", depth=2)\n        # Set gamma=1 to make the differences more noticeable\n        a = AtomKernel(transformer=trans, same_element=False, gamma=1.)\n        res = a.fit_transform(ALL)\n        expected = numpy.array([[17.00000033, 14.58016505],\n                                [14.58016505, 32.76067832]])\n        try:\n            numpy.testing.assert_array_almost_equal(expected, res)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_laplace_kernel(self):\n        # Set depth=2 so the comparison is not trivial\n        trans = Shell(input_type=""filename"", depth=2)\n        a = AtomKernel(transformer=trans, kernel=""laplace"", gamma=1.)\n        a.fit(ALL)\n        res = a.transform(ALL)\n        try:\n            numpy.testing.assert_array_almost_equal(LAPLACE_KERNEL, res)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_custom_kernel(self):\n        # Set depth=2 so the comparison is not trivial\n        trans = Shell(input_type=""filename"", depth=2)\n        # Simple linear kernel\n        a = AtomKernel(transformer=trans,\n                       kernel=lambda x, y: numpy.dot(x, numpy.transpose(y)))\n        a.fit(ALL)\n        res = a.transform(ALL)\n        try:\n            numpy.testing.assert_array_almost_equal(LINEAR_KERNEL, res)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_invalid_kernel(self):\n        with self.assertRaises(ValueError):\n            trans = Shell(input_type=""filename"")\n            a = AtomKernel(kernel=1, transformer=trans)\n            a.fit_transform(ALL)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_molecule.py,0,"b'import unittest\n\nimport numpy\n\nfrom molml.molecule import BagOfBonds, Connectivity, Autocorrelation\nfrom molml.molecule import CoulombMatrix, EncodedBond, EncodedAngle\nfrom molml.molecule import ConnectivityTree\nfrom molml.constants import UNKNOWN\n\nfrom .constants import METHANE, BIG, MID, ALL_DATA\n\nMETHANE2 = (METHANE[0], 2 * METHANE[1])\nALL_ATOM = numpy.array([[1, 4, 0, 0],\n                        [2, 3, 0, 4],\n                        [25, 15, 5, 4]])\nALL_ATOM_TREE = numpy.array([\n    [0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  4,  0,  0,\n     0,  0,  0,  0,  0,  0,  0,  0,  0],\n    [1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  2,  0,\n     0,  0,  0,  0,  1,  1,  0,  0,  2],\n    [0,  2,  1,  1,  2,  1,  0,  2,  5,  2,  3,  5,  1,  0,  0, 11,  0,  2,\n     2,  1,  1,  3,  0,  0,  2,  2,  0],\n])\n\n\ndef assert_close_statistics(array, expected):\n    \'\'\'\n    Compare float arrays by comparing some statistics.\n    \'\'\'\n    value = numpy.array([\n                        array.mean(),\n                        array.std(),\n                        array.min(),\n                        array.max(),\n                        ])\n    numpy.testing.assert_array_almost_equal(value, expected)\n\n\nclass ConnectivityTest(unittest.TestCase):\n\n    def test_fit_atom(self):\n        a = Connectivity(depth=1)\n        a.fit(ALL_DATA)\n        self.assertEqual(a._base_groups, ((\'C\',), (\'H\',), (\'N\',), (\'O\',)))\n\n    def test_fit_atom_separated(self):\n        a = Connectivity(depth=1)\n        a.fit([METHANE2])\n        self.assertEqual(a._base_groups, ((\'C\',), (\'H\',)))\n        self.assertTrue(\n            (a.transform([METHANE2]) == numpy.array([[1, 4]])).all())\n\n    def test_fit_bond(self):\n        a = Connectivity(depth=2)\n        a.fit(ALL_DATA)\n        self.assertEqual(a._base_groups,\n                         ((\'C\', \'C\'), (\'C\', \'H\'), (\'C\', \'N\'), (\'C\', \'O\'),\n                          (\'H\', \'H\'), (\'H\', \'N\'), (\'H\', \'O\'), (\'O\', \'O\')))\n\n    def test_fit_angle(self):\n        a = Connectivity(depth=3)\n        a.fit(ALL_DATA)\n        self.assertEqual(a._base_groups,\n                         ((\'C\', \'C\', \'C\'), (\'C\', \'C\', \'H\'),\n                          (\'C\', \'C\', \'N\'), (\'C\', \'C\', \'O\'),\n                          (\'C\', \'N\', \'C\'), (\'C\', \'N\', \'H\'),\n                          (\'C\', \'O\', \'C\'), (\'C\', \'O\', \'H\'),\n                          (\'H\', \'C\', \'H\'), (\'H\', \'C\', \'N\'),\n                          (\'H\', \'C\', \'O\'), (\'H\', \'N\', \'H\'),\n                          (\'N\', \'C\', \'N\'), (\'N\', \'C\', \'O\')))\n\n    def test_fit_dihedral(self):\n        # This is to test the double order flipping (CCCH vs HCCC)\n        a = Connectivity(depth=4)\n        a.fit(ALL_DATA)\n        self.assertEqual(a._base_groups,\n                         ((\'C\', \'C\', \'C\', \'C\'), (\'C\', \'C\', \'C\', \'H\'),\n                          (\'C\', \'C\', \'C\', \'N\'), (\'C\', \'C\', \'C\', \'O\'),\n                          (\'C\', \'C\', \'N\', \'C\'), (\'C\', \'C\', \'N\', \'H\'),\n                          (\'C\', \'C\', \'O\', \'C\'), (\'C\', \'C\', \'O\', \'H\'),\n                          (\'H\', \'C\', \'C\', \'H\'), (\'H\', \'C\', \'C\', \'N\'),\n                          (\'H\', \'C\', \'C\', \'O\'), (\'H\', \'C\', \'N\', \'C\'),\n                          (\'H\', \'C\', \'O\', \'C\'), (\'N\', \'C\', \'C\', \'O\'),\n                          (\'N\', \'C\', \'N\', \'C\'), (\'N\', \'C\', \'N\', \'H\'),\n                          (\'N\', \'C\', \'O\', \'H\'), (\'O\', \'C\', \'N\', \'C\')))\n\n    def test_fit_atom_bond(self):\n        # This should be the exact same thing as doing it with\n        # use_bond_order=False\n        a = Connectivity(depth=1, use_bond_order=True)\n        a.fit(ALL_DATA)\n        self.assertEqual(a._base_groups, ((\'C\',), (\'H\',), (\'N\',), (\'O\',)))\n\n    def test_fit_bond_bond(self):\n        a = Connectivity(depth=2, use_bond_order=True)\n        a.fit(ALL_DATA)\n        self.assertEqual(a._base_groups,\n                         (((\'C\', \'C\', \'1\'),), ((\'C\', \'C\', \'2\'),),\n                          ((\'C\', \'C\', \'3\'),), ((\'C\', \'C\', \'Ar\'),),\n                          ((\'C\', \'H\', \'1\'),), ((\'C\', \'N\', \'2\'),),\n                          ((\'C\', \'N\', \'3\'),), ((\'C\', \'N\', \'Ar\'),),\n                          ((\'C\', \'O\', \'1\'),), ((\'C\', \'O\', \'Ar\'),),\n                          ((\'H\', \'H\', \'1\'),), ((\'H\', \'N\', \'1\'),),\n                          ((\'H\', \'O\', \'1\'),), ((\'O\', \'O\', \'1\'),)))\n\n    def test_fit_atom_coordination(self):\n        a = Connectivity(depth=1, use_coordination=True)\n        a.fit(ALL_DATA)\n        self.assertEqual(a._base_groups,\n                         ((\'C1\',), (\'C2\',), (\'C3\',), (\'C4\',), (\'H0\',),\n                          (\'H1\',), (\'N1\',), (\'N2\',), (\'N3\',), (\'O0\',),\n                          (\'O1\',), (\'O2\',)))\n\n    def test_transform(self):\n        a = Connectivity()\n        a.fit(ALL_DATA)\n        self.assertTrue((a.transform(ALL_DATA) == ALL_ATOM).all())\n\n    def test_small_to_large_transform(self):\n        a = Connectivity()\n        a.fit([METHANE])\n        self.assertTrue((a.transform(ALL_DATA) == ALL_ATOM[:, :2]).all())\n\n    def test_large_to_small_transform(self):\n        a = Connectivity()\n        a.fit([BIG])\n        self.assertTrue((a.transform(ALL_DATA) == ALL_ATOM).all())\n\n    def test_transform_before_fit(self):\n        a = Connectivity()\n        with self.assertRaises(ValueError):\n            a.transform(ALL_DATA)\n\n    def test_fit_transform(self):\n        a = Connectivity()\n        self.assertTrue((a.fit_transform(ALL_DATA) == ALL_ATOM).all())\n\n    def test_unknown(self):\n        a = Connectivity(add_unknown=True)\n        expected_results = numpy.array([[1,  4, 0],\n                                        [2,  3, 4],\n                                        [25, 15, 9]])\n        a.fit([METHANE])\n        self.assertTrue((a.transform(ALL_DATA) == expected_results).all())\n\n    def test_tfidf(self):\n        a = Connectivity(do_tfidf=True)\n        expected = numpy.array([[0., 0.,  0., 0.],\n                                [0., 0., 0., 1.62186043],\n                                [0., 0., 5.49306144, 1.62186043]])\n        a.fit(ALL_DATA)\n        try:\n            m = a.transform(ALL_DATA)\n            numpy.testing.assert_array_almost_equal(m, expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_labels(self):\n        a = Connectivity(depth=2)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\'C-H\', )\n        self.assertEqual(labels, expected)\n\n    def test_get_labels_unknown(self):\n        a = Connectivity(depth=2, add_unknown=True)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\'C-H\', UNKNOWN)\n        self.assertEqual(labels, expected)\n\n    def test_get_labels_coordination(self):\n        a = Connectivity(depth=1, use_coordination=True)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\'C4\', \'H1\')\n        self.assertEqual(labels, expected)\n\n    def test_get_labels_bond_order(self):\n        a = Connectivity(depth=3, use_bond_order=True)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\'H-C-1_C-H-1\', )\n        self.assertEqual(labels, expected)\n\n\nclass ConnectivityTreeTest(unittest.TestCase):\n\n    def test_fit_depth_1(self):\n        a = ConnectivityTree(depth=1, use_parent_element=False)\n        a.fit(ALL_DATA)\n        base = (\'C\', \'H\', \'N\', \'O\')\n        self.assertEqual(a._base_groups, tuple(((0, x, 1), ) for x in base))\n\n    def test_fit_depth_1_separated(self):\n        a = ConnectivityTree(depth=1)\n        a.fit([METHANE2])\n        self.assertTrue(\n            (a.transform([METHANE2]) == numpy.array([[1, 4]])).all())\n\n    def test_fit_depth_2(self):\n        a = ConnectivityTree(depth=2, use_parent_element=False)\n        a.fit(ALL_DATA)\n        expected = (\n            ((0, \'C\', 1), (1, \'C\', 1)),\n            ((0, \'C\', 1), (1, \'C\', 1), (1, \'H\', 1), (1, \'N\', 1)),\n            ((0, \'C\', 1), (1, \'C\', 1), (1, \'N\', 1), (1, \'O\', 1)),\n            ((0, \'C\', 1), (1, \'C\', 1), (1, \'O\', 1)),\n            ((0, \'C\', 1), (1, \'C\', 2), (1, \'H\', 1)),\n            ((0, \'C\', 1), (1, \'C\', 2), (1, \'O\', 1)),\n            ((0, \'C\', 1), (1, \'H\', 3), (1, \'O\', 1)), ((0, \'H\', 1),),\n            ((0, \'H\', 1), (1, \'H\', 1)),\n            ((0, \'H\', 1), (1, \'O\', 1)),\n            ((0, \'N\', 1), (1, \'C\', 1), (1, \'H\', 2)), ((0, \'O\', 1),),\n            ((0, \'O\', 1), (1, \'C\', 1), (1, \'H\', 1)),\n            ((0, \'O\', 1), (1, \'O\', 1)),\n        )\n        self.assertEqual(a._base_groups[::2], expected)\n\n    def test_fit_depth_2_parent_element(self):\n        a = ConnectivityTree(depth=2, use_parent_element=True)\n        a.fit(ALL_DATA)\n        expected = (\n            ((0, \'Root\', \'C\', 1), (1, \'C\', \'C\', 1)),\n            ((0, \'Root\', \'C\', 1), (1, \'C\', \'C\', 1), (1, \'C\', \'H\', 1),\n             (1, \'C\', \'N\', 1)),\n            ((0, \'Root\', \'C\', 1), (1, \'C\', \'C\', 1), (1, \'C\', \'N\', 1),\n             (1, \'C\', \'O\', 1)),\n            ((0, \'Root\', \'C\', 1), (1, \'C\', \'C\', 1), (1, \'C\', \'O\', 1)),\n            ((0, \'Root\', \'C\', 1), (1, \'C\', \'C\', 2), (1, \'C\', \'H\', 1)),\n            ((0, \'Root\', \'C\', 1), (1, \'C\', \'C\', 2), (1, \'C\', \'O\', 1)),\n            ((0, \'Root\', \'C\', 1), (1, \'C\', \'H\', 3), (1, \'C\', \'O\', 1)),\n            ((0, \'Root\', \'H\', 1),),\n            ((0, \'Root\', \'H\', 1), (1, \'H\', \'H\', 1)),\n            ((0, \'Root\', \'H\', 1), (1, \'H\', \'O\', 1)),\n            ((0, \'Root\', \'N\', 1), (1, \'N\', \'C\', 1), (1, \'N\', \'H\', 2)),\n            ((0, \'Root\', \'O\', 1),),\n            ((0, \'Root\', \'O\', 1), (1, \'O\', \'C\', 1), (1, \'O\', \'H\', 1)),\n            ((0, \'Root\', \'O\', 1), (1, \'O\', \'O\', 1)),\n        )\n        self.assertEqual(a._base_groups[::2], expected)\n\n    def test_fit_depth_2_parent_element_bond_order(self):\n        a = ConnectivityTree(depth=2, use_parent_element=True,\n                             use_bond_order=True)\n        a.fit(ALL_DATA)\n        expected = (\n            ((1, \'C_1_C\', 1), (1, \'C_2_C\', 2)),\n            ((1, \'C_1_H\', 1), (1, \'C_Ar_C\', 1), (1, \'C_Ar_N\', 1)),\n            ((1, \'C_1_H\', 3), (1, \'C_1_O\', 1)),\n            ((1, \'C_1_O\', 1), (1, \'C_2_C\', 2)),\n            ((1, \'C_2_C\', 1), (1, \'C_3_C\', 1)),\n            ((1, \'C_2_C\', 1), (1, \'C_Ar_C\', 1), (1, \'C_Ar_N\', 1)),\n            ((1, \'C_2_N\', 1), (1, \'C_Ar_C\', 1), (1, \'C_Ar_N\', 1)),\n            ((1, \'C_Ar_C\', 1), (1, \'C_Ar_N\', 1), (1, \'C_Ar_O\', 1)),\n            ((1, \'H_1_C\', 1),),\n            ((1, \'H_1_N\', 1),),\n            ((1, \'N_1_H\', 2), (1, \'N_2_C\', 1)),\n            ((1, \'N_Ar_C\', 2),),\n            ((1, \'O_1_C\', 1), (1, \'O_Ar_C\', 1)),\n            ((1, \'O_1_H\', 1), (1, \'O_Ar_C\', 1)),\n        )\n        self.assertEqual(a._base_groups[::2], expected)\n\n    def test_fit_depth_3(self):\n        a = ConnectivityTree(depth=3, use_parent_element=False,\n                             preserve_paths=False)\n        a.fit(ALL_DATA)\n        expected = (\n            ((0, \'C\', 1), (1, \'C\', 1), (1, \'H\', 1), (1, \'N\', 1), (2, \'C\', 3)),\n            ((0, \'C\', 1), (1, \'C\', 1), (1, \'N\', 2), (2, \'C\', 3), (2, \'H\', 2)),\n            ((0, \'C\', 1), (1, \'C\', 2), (1, \'H\', 1), (2, \'C\', 2), (2, \'H\', 2)),\n            ((0, \'C\', 1), (1, \'C\', 2), (1, \'O\', 1), (2, \'C\', 3), (2, \'H\', 1),\n             (2, \'N\', 1)),\n            ((0, \'C\', 1), (1, \'C\', 3), (2, \'C\', 2), (2, \'H\', 2), (2, \'N\', 1)),\n            ((0, \'H\', 1),),\n            ((0, \'H\', 1), (1, \'C\', 1), (2, \'H\', 3)),\n            ((0, \'N\', 1), (1, \'C\', 1), (2, \'C\', 1)),\n            ((0, \'O\', 1), (1, \'C\', 1), (2, \'C\', 1))\n        )\n        self.assertEqual(a._base_groups[::5], expected)\n\n    def test_fit_depth_3_preserve_paths(self):\n        a = ConnectivityTree(depth=3, use_parent_element=False,\n                             preserve_paths=True)\n        a.fit(ALL_DATA)\n        expected = (\n            ((0, -1, \'C\', 1), (1, 0, \'C\', 1), (1, 0, \'H\', 1), (1, 0, \'N\', 1),\n             (2, 0, \'C\', 2), (2, 2, \'C\', 1)),\n            ((0, -1, \'C\', 1), (1, 0, \'C\', 1), (1, 0, \'N\', 2), (2, 0, \'C\', 2),\n             (2, 1, \'C\', 1), (2, 2, \'H\', 2)),\n            ((0, -1, \'C\', 1), (1, 0, \'C\', 2), (1, 0, \'H\', 1), (2, 0, \'C\', 1),\n             (2, 0, \'H\', 1), (2, 1, \'N\', 1), (2, 1, \'O\', 1)),\n            ((0, -1, \'C\', 1), (1, 0, \'C\', 2), (1, 0, \'O\', 1), (2, 0, \'C\', 1),\n             (2, 0, \'H\', 1), (2, 1, \'C\', 1), (2, 1, \'N\', 1), (2, 2, \'C\', 1)),\n            ((0, -1, \'C\', 1), (1, 0, \'C\', 3), (2, 0, \'C\', 1), (2, 0, \'H\', 1),\n             (2, 1, \'C\', 1), (2, 1, \'O\', 1), (2, 2, \'N\', 2)),\n            ((0, -1, \'C\', 1), (1, 0, \'H\', 4)),\n            ((0, -1, \'H\', 1), (1, 0, \'C\', 1), (2, 0, \'H\', 2), (2, 0, \'O\', 1)),\n            ((0, -1, \'N\', 1), (1, 0, \'C\', 1), (1, 0, \'H\', 2), (2, 0, \'C\', 1),\n             (2, 0, \'N\', 1)),\n            ((0, -1, \'O\', 1), (1, 0, \'C\', 1), (1, 0, \'H\', 1), (2, 0, \'C\', 1),\n             (2, 0, \'N\', 1)),\n        )\n        self.assertEqual(a._base_groups[::5], expected)\n\n    def test_fit_depth_1_use_bond_order(self):\n        # This should be the exact same thing as doing it with\n        # use_bond_order=False\n        a = ConnectivityTree(depth=1, use_parent_element=False,\n                             use_bond_order=True)\n        a.fit(ALL_DATA)\n        base = (\'C\', \'H\', \'N\', \'O\')\n        self.assertEqual(a._base_groups, tuple(((0, x, 1), ) for x in base))\n\n    def test_fit_depth_2_use_bond_order(self):\n        a = ConnectivityTree(depth=2, use_parent_element=False,\n                             use_bond_order=True)\n        a.fit(ALL_DATA)\n        expected = (\n            ((1, \'C_1_C\', 1), (1, \'C_2_C\', 2)),\n            ((1, \'C_1_H\', 4),),\n            ((1, \'C_2_C\', 1), (1, \'C_Ar_C\', 1), (1, \'C_Ar_N\', 1)),\n            ((1, \'C_Ar_C\', 2), (1, \'C_Ar_O\', 1)),\n            ((1, \'N_1_H\', 2), (1, \'N_2_C\', 1)),\n            ((1, \'O_1_C\', 2),),\n        )\n        self.assertEqual(a._base_groups[::5], expected)\n\n    def test_fit_depth_1_coordination(self):\n        a = ConnectivityTree(depth=1, use_coordination=True)\n        a.fit(ALL_DATA)\n\n        bases = (\'C1\', \'C2\', \'C3\', \'C4\', \'H0\', \'H1\',\n                 \'N1\', \'N2\', \'N3\', \'O0\', \'O1\', \'O2\')\n        expected = tuple(((0, \'Root\', x, 1), ) for x in bases)\n        self.assertEqual(a._base_groups, expected)\n\n    def test_sorting_order(self):\n        eles = [\'A\', \'B\', \'B\', \'B\', \'C\', \'D\', \'B\']\n        connections = {\n            0: {1: \'1\', 2: \'1\', 3: \'1\'},\n            1: {0: \'1\', 4: \'1\'},\n            2: {0: \'1\', 5: \'1\'},\n            3: {0: \'1\', 6: \'1\'},\n            4: {1: \'1\'},\n            5: {2: \'1\'},\n            6: {3: \'1\'},\n        }\n        a = ConnectivityTree(input_type=(\'elements\', \'connections\'), depth=3,\n                             preserve_paths=True, use_parent_element=True)\n        a.fit([(eles, connections)])\n        expected = (\n            (0, -1, \'Root\', \'A\', 1),\n            (1, 0, \'A\', \'B\', 3),\n            (2, 0, \'B\', \'B\', 1),\n            (2, 1, \'B\', \'C\', 1),\n            (2, 2, \'B\', \'D\', 1),\n        )\n        a_base = [x for x in a._base_groups if x[0][-2] == \'A\']\n        self.assertEqual(len(a_base), 1)\n        self.assertEqual(a_base[0], expected)\n\n    def test_transform(self):\n        a = ConnectivityTree()\n        a.fit(ALL_DATA)\n        self.assertTrue((a.transform(ALL_DATA) == ALL_ATOM).all())\n\n    def test_small_to_large_transform(self):\n        a = ConnectivityTree(depth=2)\n        a.fit([METHANE])\n        idxs = numpy.where(ALL_ATOM_TREE[0] != 0)[0]\n        expected = ALL_ATOM_TREE[:, idxs]\n        self.assertTrue((a.transform(ALL_DATA) == expected).all())\n\n    def test_large_to_small_transform(self):\n        a = ConnectivityTree(depth=2)\n        a.fit([BIG])\n        idxs = numpy.where(ALL_ATOM_TREE[2] != 0)[0]\n        expected = ALL_ATOM_TREE[:, idxs]\n        self.assertTrue((a.transform(ALL_DATA) == expected).all())\n\n    def test_transform_before_fit(self):\n        a = ConnectivityTree()\n        with self.assertRaises(ValueError):\n            a.transform(ALL_DATA)\n\n    def test_fit_transform(self):\n        a = ConnectivityTree(depth=2)\n        self.assertTrue((a.fit_transform(ALL_DATA) == ALL_ATOM_TREE).all())\n\n    def test_unknown(self):\n        a = ConnectivityTree(add_unknown=True)\n        expected_results = numpy.array([[1,  4, 0],\n                                        [2,  3, 4],\n                                        [25, 15, 9]])\n        a.fit([METHANE])\n        self.assertTrue((a.transform(ALL_DATA) == expected_results).all())\n\n    def test_tfidf(self):\n        a = ConnectivityTree(do_tfidf=True)\n        expected = numpy.array([[0., 0.,  0., 0.],\n                                [0., 0., 0., 1.62186043],\n                                [0., 0., 5.49306144, 1.62186043]])\n        a.fit(ALL_DATA)\n        try:\n            m = a.transform(ALL_DATA)\n            numpy.testing.assert_array_almost_equal(m, expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_labels(self):\n        a = ConnectivityTree(depth=2)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\'0-Root-C-1__1-C-H-4\', \'0-Root-H-1__1-H-C-1\')\n        self.assertEqual(labels, expected)\n\n    def test_get_labels_unknown(self):\n        a = ConnectivityTree(depth=2, add_unknown=True)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\'0-Root-C-1__1-C-H-4\', \'0-Root-H-1__1-H-C-1\', UNKNOWN)\n        self.assertEqual(labels, expected)\n\n    def test_get_labels_coordination(self):\n        a = ConnectivityTree(depth=1, use_coordination=True)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\'0-Root-C4-1\', \'0-Root-H1-1\')\n        self.assertEqual(labels, expected)\n\n    def test_get_labels_bond_order(self):\n        a = ConnectivityTree(depth=3, use_bond_order=True)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\'1-C_1_H-4\', \'1-H_1_C-1__2-C_1_H-3\')\n        self.assertEqual(labels, expected)\n\n\nclass AutocorrelationTest(unittest.TestCase):\n\n    def test_depths_properties(self):\n        a = Autocorrelation(depths=[1, 2], properties=[\'I\', \'Z\'])\n        a.fit([METHANE])\n        self.assertTrue(\n            (a.transform([METHANE]) == numpy.array([[8, 12, 48, 12]])).all())\n\n    def test_fit_atom_separated(self):\n        a = Autocorrelation(depths=[0, 1], properties=[\'I\', \'Z\'])\n        a.fit([METHANE2])\n        self.assertTrue(\n            (a.transform([METHANE2]) == numpy.array([[5, 0, 40, 0]])).all())\n\n    def test_default_depths(self):\n        a = Autocorrelation(properties=[\'I\'])\n        a.fit(ALL_DATA)\n        expected = numpy.array([\n            [5, 8, 12, 0],\n            [9, 8, 2, 0],\n            [49, 104, 156, 190]\n        ])\n        self.assertTrue((a.transform(ALL_DATA) == expected).all())\n\n    def test_wide_depths(self):\n        a = Autocorrelation(depths=[-1, 1, 4, 10, 100], properties=[\'I\'])\n        a.fit(ALL_DATA)\n        expected = numpy.array([\n            [0, 8, 0, 0, 0],\n            [0, 8, 0, 0, 0],\n            [0, 104, 216, 166, 0]\n        ])\n        self.assertTrue((a.transform(ALL_DATA) == expected).all())\n\n    def test_properties(self):\n        a = Autocorrelation(depths=[0])\n        a.fit(ALL_DATA)\n        expected = numpy.array([\n            [20., 25.8625, 5., 2.1625, 40.],\n            [10., 74.8594, 9., 4.4571, 331.],\n            [260., 328.7049, 49., 28.1326, 1416.]\n        ])\n        try:\n            m = a.transform(ALL_DATA)\n            numpy.testing.assert_array_almost_equal(m, expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_property_function(self):\n        a = Autocorrelation(depths=[1],\n                            properties=[lambda data:\n                                        [2 for x in data.numbers]])\n        a.fit(ALL_DATA)\n        expected = numpy.array([\n            [32],\n            [32],\n            [416],\n        ])\n        self.assertTrue((a.transform(ALL_DATA) == expected).all())\n\n    def test_both_property(self):\n        a = Autocorrelation(depths=[1],\n                            properties=[\'I\',\n                                        lambda data:\n                                        [2 for x in data.numbers]])\n        a.fit(ALL_DATA)\n        expected = numpy.array([\n            [32, 8],\n            [32, 8],\n            [416, 104],\n        ])\n        self.assertTrue((a.transform(ALL_DATA) == expected).all())\n\n    def test_transform(self):\n        a = Autocorrelation(depths=[1], properties=[\'I\'])\n        a.fit(ALL_DATA)\n        expected = numpy.array([\n            [8],\n            [8],\n            [104],\n        ])\n        self.assertTrue((a.transform(ALL_DATA) == expected).all())\n\n    def test_fit_transform(self):\n        a = Autocorrelation(depths=[1], properties=[\'I\'])\n        expected = numpy.array([\n            [8],\n            [8],\n            [104],\n        ])\n        self.assertTrue((a.fit_transform(ALL_DATA) == expected).all())\n\n    def test_get_labels(self):\n        a = Autocorrelation(depths=[1, 2], properties=[\'I\', \'EN\'])\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\n            \'EN_1\', \'EN_2\',\n            \'I_1\', \'I_2\',\n        )\n        self.assertEqual(labels, expected)\n\n\nclass EncodedBondTest(unittest.TestCase):\n\n    def test_fit(self):\n        a = EncodedBond()\n        a.fit(ALL_DATA)\n        self.assertEqual(a._element_pairs,\n                         ((\'C\', \'C\'), (\'C\', \'H\'), (\'C\', \'N\'), (\'C\', \'O\'),\n                          (\'H\', \'H\'), (\'H\', \'N\'), (\'H\', \'O\'), (\'N\', \'N\'),\n                          (\'N\', \'O\'), (\'O\', \'O\')))\n\n    def test_transform(self):\n        a = EncodedBond()\n        a.fit([METHANE])\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.042672,  # mean\n            0.246663,  # std\n            0.,  # min\n            2.392207,  # max\n        ])\n        try:\n            m = a.transform([METHANE])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_large_to_small_transform(self):\n        a = EncodedBond()\n        a.fit([MID])\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.014224,  # mean\n            0.143824,  # std\n            0.,  # min\n            2.392207,  # max\n        ])\n        try:\n            m = a.transform([METHANE])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_small_to_large_transform(self):\n        a = EncodedBond()\n        a.fit([METHANE])\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            9.207308e-001,  # mean\n            1.062388e+000,  # std\n            0.,  # min\n            5.023670e+000,  # max\n        ])\n        try:\n            m = a.transform([BIG])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_fit_transform(self):\n        a = EncodedBond()\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.042672,  # mean\n            0.246663,  # std\n            0.,  # min\n            2.392207,  # max\n        ])\n        try:\n            m = a.fit_transform([METHANE])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_before_fit(self):\n        a = EncodedBond()\n        with self.assertRaises(ValueError):\n            a.transform(ALL_DATA)\n\n    def test_smoothing_function(self):\n        a = EncodedBond(smoothing=""norm_cdf"")\n\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            3.859534e+000,  # mean\n            2.182923e+000,  # std\n            0.,  # min\n            6.000000e+000,  # max\n        ])\n        try:\n            m = a.fit_transform([METHANE])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_smoothing_function_error(self):\n        a = EncodedBond(smoothing=""not valid"")\n\n        with self.assertRaises(KeyError):\n            a.fit_transform([METHANE])\n\n    def test_max_depth_neg(self):\n        a = EncodedBond(max_depth=-1)\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.503237244954,  # mean\n            0.857850829564,  # std\n            0.,  # min\n            7.15861023,  # max\n        ])\n        try:\n            m = a.fit_transform([BIG])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_max_depth_1(self):\n        a = EncodedBond(max_depth=1)\n\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.0443793,  # mean\n            0.33766942,  # std\n            0.,  # min\n            5.76559336,  # max\n        ])\n        try:\n            m = a.fit_transform([BIG])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_max_depth_3(self):\n        a = EncodedBond(max_depth=3)\n\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.192026,  # mean\n            0.63276,  # std\n            0.,  # min\n            7.15861023,  # max\n        ])\n        try:\n            m = a.fit_transform([BIG])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_spacing_inverse(self):\n        a = EncodedBond(spacing=""inverse"")\n\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.051207,  # mean\n            0.269248,  # std\n            0.,  # min\n            2.387995,  # max\n        ])\n        try:\n            m = a.fit_transform([METHANE])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_spacing_log(self):\n        a = EncodedBond(spacing=""log"")\n\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.072768,  # mean\n            0.318508,  # std\n            0.,  # min\n            2.339376,  # max\n        ])\n        try:\n            m = a.fit_transform([METHANE])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_spacing_invalid(self):\n        a = EncodedBond(spacing=""not valid"")\n\n        with self.assertRaises(KeyError):\n            a.fit_transform([METHANE])\n\n    def test_form_element(self):\n        a = EncodedBond(form=1)\n\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.085345,  # mean\n            0.452595,  # std\n            0.,  # min\n            4.784414,  # max\n        ])\n        try:\n            m = a.fit_transform([METHANE])\n            self.assertEqual(m.shape, (1, 200))\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_form_0(self):\n        a = EncodedBond(form=0)\n\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.085345,  # mean\n            0.343574,  # std\n            0.,  # min\n            2.392207,  # max\n        ])\n        try:\n            m = a.fit_transform([METHANE])\n            self.assertEqual(m.shape, (1, 100))\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_add_unknown(self):\n        a = EncodedBond(add_unknown=True)\n        a.fit([METHANE])\n\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.09105,  # mean\n            0.231761,  # std\n            0.,  # min\n            1.869012,  # max\n        ])\n        try:\n            m = a.transform([MID])\n            self.assertEqual(m.shape, (1, 300))\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_labels(self):\n        a = EncodedBond(segments=2, start=0., end=1.)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\n            \'C-H_0.0\', \'C-H_1.0\',\n            \'H-H_0.0\', \'H-H_1.0\',\n        )\n        self.assertEqual(labels, expected)\n\n\nclass EncodedAngleTest(unittest.TestCase):\n\n    def test_fit(self):\n        a = EncodedAngle()\n        a.fit(ALL_DATA)\n        expected = ((\'C\', \'C\', \'C\'), (\'C\', \'C\', \'H\'), (\'C\', \'C\', \'N\'),\n                    (\'C\', \'C\', \'O\'), (\'C\', \'H\', \'C\'), (\'C\', \'H\', \'H\'),\n                    (\'C\', \'H\', \'N\'), (\'C\', \'H\', \'O\'), (\'C\', \'N\', \'C\'),\n                    (\'C\', \'N\', \'H\'), (\'C\', \'N\', \'N\'), (\'C\', \'N\', \'O\'),\n                    (\'C\', \'O\', \'C\'), (\'C\', \'O\', \'H\'), (\'C\', \'O\', \'N\'),\n                    (\'C\', \'O\', \'O\'), (\'H\', \'C\', \'H\'), (\'H\', \'C\', \'N\'),\n                    (\'H\', \'C\', \'O\'), (\'H\', \'H\', \'H\'), (\'H\', \'H\', \'N\'),\n                    (\'H\', \'H\', \'O\'), (\'H\', \'N\', \'H\'), (\'H\', \'N\', \'N\'),\n                    (\'H\', \'N\', \'O\'), (\'H\', \'O\', \'H\'), (\'H\', \'O\', \'N\'),\n                    (\'H\', \'O\', \'O\'), (\'N\', \'C\', \'N\'), (\'N\', \'C\', \'O\'),\n                    (\'N\', \'H\', \'N\'), (\'N\', \'H\', \'O\'), (\'N\', \'N\', \'N\'),\n                    (\'N\', \'N\', \'O\'), (\'N\', \'O\', \'N\'), (\'N\', \'O\', \'O\'),\n                    (\'O\', \'C\', \'O\'), (\'O\', \'H\', \'O\'), (\'O\', \'N\', \'O\'),\n                    (\'O\', \'O\', \'O\'))\n        self.assertEqual(a._groups, expected)\n\n    def test_transform(self):\n        a = EncodedAngle()\n        a.fit([METHANE])\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.116708,  # mean\n            0.450738,  # std\n            0.,  # min\n            3.043729,  # max\n        ])\n        try:\n            m = a.fit_transform([METHANE])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_large_to_small_transform(self):\n        a = EncodedAngle()\n        a.fit([MID])\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.025935,  # mean\n            0.21795,  # std\n            0.,  # min\n            3.043729,  # max\n        ])\n        try:\n            m = a.transform([METHANE])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_small_to_large_transform(self):\n        a = EncodedAngle()\n        a.fit([METHANE])\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.018603,  # mean\n            0.130329,  # std\n            0.,  # min\n            1.568823,  # max\n        ])\n        try:\n            m = a.transform([MID])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_fit_transform(self):\n        a = EncodedAngle()\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.116708,  # mean\n            0.450738,  # std\n            0.,  # min\n            3.043729,  # max\n        ])\n        try:\n            m = a.fit_transform([METHANE])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_before_fit(self):\n        a = EncodedAngle()\n        with self.assertRaises(ValueError):\n            a.transform(ALL_DATA)\n\n    def test_smoothing_function(self):\n        a = EncodedAngle(smoothing=""norm_cdf"")\n\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            1.5891686,  # mean\n            2.5907034,  # std\n            0.,         # min\n            9.8982443,  # max\n        ])\n        try:\n            m = a.fit_transform([METHANE])\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_smoothing_function_error(self):\n        a = EncodedAngle(smoothing=""not valid"")\n\n        with self.assertRaises(KeyError):\n            a.fit_transform([METHANE])\n\n    def test_max_depth(self):\n        a = EncodedAngle(max_depth=3)\n        # This is a cheap test to prevent needing all the values here\n        data = (\n            #       mean          std     min      max\n            (-1, [0.0325158765862, 0.132101907024, 0.0, 2.01566683797]),\n            (1, [0.00491078348799, 0.0463273875823, 0.0, 0.694568644823]),\n            (3, [0.0063668265711, 0.0513782485995, 0.0, 0.694568644823]),\n\n        )\n        for max_depth, expected in data:\n            a = EncodedAngle(max_depth=max_depth)\n            expected_results = numpy.array(expected)\n            try:\n                m = a.fit_transform([MID])\n                assert_close_statistics(m, expected_results)\n            except AssertionError as e:\n                self.fail(e)\n\n    def test_form(self):\n        data = (\n            #    mean         std   min     max\n            (2, [0.155611, 0.581838, 0., 4.395692], 120),\n            (1, [0.233417, 0.699744, 0., 4.395692], 80),\n            (0, [4.668338e-001, 1.090704e+000, 0., 5.747656e+000], 40),\n        )\n        for form, expected, size in data:\n            a = EncodedAngle(form=form)\n            expected_results = numpy.array(expected)\n            try:\n                m = a.fit_transform([METHANE])\n                self.assertEqual(m.shape, (1, size))\n                assert_close_statistics(m, expected_results)\n            except AssertionError as e:\n                self.fail(e)\n\n    def test_add_unknown(self):\n        a = EncodedAngle(add_unknown=True)\n        a.fit([METHANE])\n\n        # This is a cheap test to prevent needing all the values here\n        expected_results = numpy.array([\n            0.117057,  # mean\n            0.510819,  # std\n            0.,  # min\n            6.343512,  # max\n        ])\n        try:\n            m = a.transform([MID])\n            self.assertEqual(m.shape, (1, 200))\n            assert_close_statistics(m, expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_labels(self):\n        a = EncodedAngle(segments=2)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\n            \'C-H-C_0.0\', \'C-H-C_3.14159\',\n            \'C-H-H_0.0\', \'C-H-H_3.14159\',\n            \'H-C-H_0.0\', \'H-C-H_3.14159\',\n            \'H-H-H_0.0\', \'H-H-H_3.14159\',\n        )\n        self.assertEqual(labels, expected)\n\n\nclass CoulombMatrixTest(unittest.TestCase):\n\n    def test_fit(self):\n        a = CoulombMatrix()\n        a.fit(ALL_DATA)\n        self.assertEqual(a._max_size, 49)\n\n    def test_transform(self):\n        a = CoulombMatrix()\n        a.fit([METHANE])\n        expected_results = numpy.array([\n            [36.8581052,   5.49459021,   5.49462885,   5.4945,\n                5.49031286,   5.49459021,   0.5,   0.56071947,\n                0.56071656,   0.56064037,   5.49462885,   0.56071947,\n                0.5,   0.56071752,   0.56064089,   5.4945,\n                0.56071656,   0.56071752,   0.5,   0.56063783,\n                5.49031286,   0.56064037,   0.56064089,   0.56063783,\n                0.5]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                a.transform([METHANE]),\n                expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_small_to_large_transform(self):\n        a = CoulombMatrix()\n        a.fit([METHANE])\n        with self.assertRaises(ValueError):\n            a.transform(ALL_DATA)\n\n    def test_small_to_large_transform_drop_values(self):\n        a = CoulombMatrix(drop_values=True)\n        a.fit([METHANE])\n        a.transform(ALL_DATA)\n        self.assertEqual(a.transform(ALL_DATA).shape, (3, 25))\n\n    def test_large_to_small_transform(self):\n        a = CoulombMatrix()\n        a.fit([MID])\n\n        expected_results = numpy.array([\n            [36.8581052,   5.49459021,   5.49462885,   5.4945,\n             5.49031286,   0.,   0.,   0.,\n             0.,   5.49459021,   0.5,   0.56071947,\n             0.56071656,   0.56064037,   0.,   0.,\n             0.,   0.,   5.49462885,   0.56071947,\n             0.5,   0.56071752,   0.56064089,   0.,\n             0.,   0.,   0.,   5.4945,\n             0.56071656,   0.56071752,   0.5,   0.56063783,\n             0.,   0.,   0.,   0.,\n             5.49031286,   0.56064037,   0.56064089,   0.56063783,\n             0.5] + [0.0] * 40\n        ])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                a.transform([METHANE]),\n                expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_before_fit(self):\n        a = CoulombMatrix()\n        with self.assertRaises(ValueError):\n            a.transform(ALL_DATA)\n\n    def test_fit_transform(self):\n        a = CoulombMatrix()\n        expected_results = numpy.array([\n            [36.8581052,   5.49459021,   5.49462885,   5.4945,\n                5.49031286,   5.49459021,   0.5,   0.56071947,\n                0.56071656,   0.56064037,   5.49462885,   0.56071947,\n                0.5,   0.56071752,   0.56064089,   5.4945,\n                0.56071656,   0.56071752,   0.5,   0.56063783,\n                5.49031286,   0.56064037,   0.56064089,   0.56063783,\n                0.5]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                a.fit_transform([METHANE]),\n                expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_sort(self):\n        a = CoulombMatrix(sort=True)\n        b = CoulombMatrix()\n\n        res_a = a.fit_transform([MID])\n        res_b = b.fit_transform([MID])\n        self.assertFalse(numpy.allclose(res_a, res_b))\n        expected_results = numpy.array([73.51669472, 45.84796673, 20.4393443,\n                                        18.51709592, 34.38200956, 19.92342035,\n                                        1.71317156, 1.39374152, 1.20676731])\n\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res_a[0, :9],\n                expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_eigen(self):\n        a = CoulombMatrix(eigen=True)\n\n        expected_results = numpy.array([\n                                        40.04619974,\n                                        -0.06059994,\n                                        -0.06071616,\n                                        -0.06071957,\n                                        -1.00605888,\n        ])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                a.fit_transform([METHANE])[0],\n                expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_only_lower_triangle(self):\n        a = CoulombMatrix(only_lower_triangle=True)\n        expected_results = numpy.array([36.858105, 5.49459, 0.5, 5.494629,\n                                        0.560719, 0.5, 5.4945, 0.560717,\n                                        0.560718, 0.5, 5.490313, 0.56064,\n                                        0.560641, 0.560638, 0.5])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                a.fit_transform([METHANE])[0],\n                expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_labels(self):\n        a = CoulombMatrix()\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\n            \'coul-0-0\', \'coul-0-1\', \'coul-0-2\', \'coul-0-3\', \'coul-0-4\',\n            \'coul-1-0\', \'coul-1-1\', \'coul-1-2\', \'coul-1-3\', \'coul-1-4\',\n            \'coul-2-0\', \'coul-2-1\', \'coul-2-2\', \'coul-2-3\', \'coul-2-4\',\n            \'coul-3-0\', \'coul-3-1\', \'coul-3-2\', \'coul-3-3\', \'coul-3-4\',\n            \'coul-4-0\', \'coul-4-1\', \'coul-4-2\', \'coul-4-3\', \'coul-4-4\',\n        )\n        self.assertEqual(labels, expected)\n\n    def test_get_labels_eigen(self):\n        a = CoulombMatrix(eigen=True)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\'coul-0\', \'coul-1\', \'coul-2\', \'coul-3\', \'coul-4\')\n        self.assertEqual(labels, expected)\n\n    def test_get_labels_lower_triangle(self):\n        a = CoulombMatrix(only_lower_triangle=True)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\n            \'coul-0-0\',\n            \'coul-1-0\', \'coul-1-1\',\n            \'coul-2-0\', \'coul-2-1\', \'coul-2-2\',\n            \'coul-3-0\', \'coul-3-1\', \'coul-3-2\', \'coul-3-3\',\n            \'coul-4-0\', \'coul-4-1\', \'coul-4-2\', \'coul-4-3\', \'coul-4-4\',\n        )\n        self.assertEqual(labels, expected)\n\n\nclass BagOfBondsTest(unittest.TestCase):\n\n    def test_fit(self):\n        a = BagOfBonds()\n        a.fit([METHANE])\n        expected_results = (\n            ((\'C\', \'H\'), 4),\n            ((\'H\', \'H\'), 6),\n        )\n        self.assertEqual(a._bag_sizes, expected_results)\n\n    def test_fit_multi_mol(self):\n        a = BagOfBonds()\n        a.fit(ALL_DATA)\n        expected_results = (\n            ((\'C\', \'C\'), 300),\n            ((\'C\', \'H\'), 375),\n            ((\'C\', \'N\'), 125),\n            ((\'C\', \'O\'), 100),\n            ((\'H\', \'H\'), 105),\n            ((\'H\', \'N\'), 75),\n            ((\'H\', \'O\'), 60),\n            ((\'N\', \'N\'), 10),\n            ((\'N\', \'O\'), 20),\n            ((\'O\', \'O\'), 6),\n        )\n        self.assertEqual(a._bag_sizes, expected_results)\n\n    def test_transform(self):\n        a = BagOfBonds()\n        a.fit([METHANE])\n        expected_results = numpy.array([\n            [5.49462885, 5.49459021, 5.4945, 5.49031286, 0.56071947,\n             0.56071752, 0.56071656, 0.56064089, 0.56064037, 0.56063783]\n        ])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                a.transform([METHANE]),\n                expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_small_to_large_transform(self):\n        a = BagOfBonds()\n        a.fit([METHANE])\n        with self.assertRaises(ValueError):\n            a.transform(ALL_DATA)\n\n    def test_small_to_large_transform_drop_values(self):\n        a = BagOfBonds(drop_values=True)\n        a.fit([METHANE])\n        self.assertEqual(a.transform(ALL_DATA).shape, (3, 10))\n\n    def test_add_atoms(self):\n        a = BagOfBonds(add_atoms=True)\n        a.fit([METHANE])\n        expected_results = numpy.array([[\n            36.8581052, 5.49462885, 5.49459021, 5.4945, 5.49031286, 0.5,\n            0.5, 0.5, 0.5, 0.56071947, 0.56071752, 0.56071656, 0.56064089,\n            0.56064037, 0.56063783]])\n        try:\n\n            numpy.testing.assert_array_almost_equal(\n                a.transform([METHANE]),\n                expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_large_to_small_transform(self):\n        a = BagOfBonds()\n        a.fit([BIG])\n\n        expected_results = numpy.array([\n            [0.0] * 300 +\n            [5.494628848219048, 5.494590213211275, 5.494499999706413,\n             5.49031286145183] +\n            [0.0] * 596 +\n            [0.5607194714171738, 0.5607175240809282, 0.5607165613824526,\n             0.5606408892793993, 0.5606403708987712, 0.560637829974531] +\n            [0.0] * 270\n        ])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                a.transform([METHANE]),\n                expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_transform_before_fit(self):\n        a = BagOfBonds()\n        with self.assertRaises(ValueError):\n            a.transform(ALL_DATA)\n\n    def test_get_labels(self):\n        a = BagOfBonds()\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\n            \'C-H_0\', \'C-H_1\', \'C-H_2\', \'C-H_3\',\n            \'H-H_0\', \'H-H_1\', \'H-H_2\',\n            \'H-H_3\', \'H-H_4\', \'H-H_5\',\n        )\n        self.assertEqual(labels, expected)\n\n    def test_get_labels_add_atoms(self):\n        a = BagOfBonds(add_atoms=True)\n        X = a.fit_transform([METHANE])\n        labels = a.get_labels()\n        self.assertEqual(X.shape[1], len(labels))\n        expected = (\n            \'C_0\',\n            \'C-H_0\', \'C-H_1\', \'C-H_2\', \'C-H_3\',\n            \'H_0\', \'H_1\', \'H_2\', \'H_3\',\n            \'H-H_0\', \'H-H_1\', \'H-H_2\',\n            \'H-H_3\', \'H-H_4\', \'H-H_5\',\n        )\n        self.assertEqual(labels, expected)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_utils.py,0,"b'import unittest\nimport os\nimport json\nimport warnings\n\nimport numpy\n\nfrom molml.utils import get_dict_func_getter\nfrom molml.utils import get_bond_type\nfrom molml.utils import get_connections, get_depth_threshold_mask_connections\nfrom molml.utils import get_graph_distance\nfrom molml.utils import LazyValues, get_smoothing_function\nfrom molml.utils import get_coulomb_matrix, get_element_pairs\nfrom molml.utils import deslugify\nfrom molml.utils import sort_chain, needs_reversal\nfrom molml.utils import load_json, IndexMap\nfrom molml.constants import UNKNOWN\n\n\nDATA_PATH = os.path.join(os.path.dirname(__file__), ""data"")\nELEMENTS = [\'C\', \'H\', \'H\', \'H\', \'H\']\nNUMBERS = [6, 1, 1, 1, 1]\nCOORDS = [\n    [0.99826008, -0.00246000, -0.00436000],\n    [2.09021016, -0.00243000, 0.00414000],\n    [0.63379005, 1.02686007, 0.00414000],\n    [0.62704006, -0.52773003, 0.87811010],\n    [0.64136006, -0.50747003, -0.90540005],\n]\nCONNECTIONS = {\n    0: {1: ""1"", 2: ""1"", 3: ""1"", 4: ""1""},\n    1: {0: ""1""},\n    2: {0: ""1""},\n    3: {0: ""1""},\n    4: {0: ""1""},\n}\nUNIT_CELL = [\n    [2., .1, 0.],\n    [0., 1., 0.],\n    [0., 0., 1.]\n]\n\n\nclass UtilsTest(unittest.TestCase):\n\n    def test_smoothing_functions(self):\n        # These are mostly sanity checks for the functions\n        expected = {\n            ""norm_cdf"": numpy.array([0., 0.158655, 0.308538, 0.5, 0.691462,\n                                     0.841345, 1.]),\n            ""zero_one"": numpy.array([0., 0., 0., 0., 1., 1., 1.]),\n            ""expit"": numpy.array([0., 0.268941, 0.377541, 0.5, 0.622459,\n                                  0.731059, 1.]),\n            ""tanh"": numpy.array([0., 0.11920292, 0.26894142, 0.5, 0.73105858,\n                                 0.88079708, 1.]),\n            ""norm"": numpy.array([0., 0.241971, 0.352065, 0.398942, 0.352065,\n                                 0.241971, 0.]),\n            ""circ"": numpy.array([0.220598, 0.215781, 0.302338, 0.34171,\n                                 0.302338, 0.215781, 0.220598]),\n            ""expit_pdf"": numpy.array([0., 0.196612, 0.235004, 0.25, 0.235004,\n                                      0.196612, 0.]),\n            ""spike"": numpy.array([0., 0., 1., 1., 1., 0., 0.]),\n        }\n\n        values = numpy.array([-1000., -1., -0.5, 0, 0.5, 1., 1000.])\n        for key, expected_value in expected.items():\n            with numpy.warnings.catch_warnings():\n                numpy.warnings.filterwarnings(\'ignore\', \'overflow\')\n                res = get_smoothing_function(key)(values, 1.)\n            try:\n                numpy.testing.assert_array_almost_equal(\n                    res,\n                    expected_value)\n            except AssertionError as e:\n                self.fail(e)\n\n    def test_smoothing_lerp(self):\n        f = get_smoothing_function(\'lerp\')\n        # Note: this is slightly different from the others because lerp\n        # depends on the spacing between the first two bins\n        theta = numpy.linspace(1, 3, 3)\n        pairs = [\n            (0., numpy.array([0., 0., 0.])),\n            (.4, numpy.array([0.4, 0., 0.])),\n            (1., numpy.array([1., 0., 0.])),\n            (1.3, numpy.array([.7, .3, 0.])),\n            (2., numpy.array([0., 1., 0.])),\n            (2.5, numpy.array([0., .5, .5])),\n            (3., numpy.array([0., 0., 1.])),\n            (3.3, numpy.array([0., 0., .7])),\n        ]\n        for off, expected in pairs:\n            try:\n                numpy.testing.assert_array_almost_equal(\n                    f(theta - off, 1.),\n                    expected)\n            except AssertionError as e:\n                self.fail(e)\n\n    def test_get_dict_func_getter(self):\n        f = get_dict_func_getter({\'norm\': lambda x: x})\n        self.assertAlmostEqual(f(\'norm\')(3), 3)\n\n    def test_get_dict_func_getter_fails(self):\n        f = get_dict_func_getter({})\n        with self.assertRaises(KeyError):\n            self.assertAlmostEqual(f(\'not_real\')(3), 0.00)\n\n    def test_get_dict_func_getter_callable(self):\n        f = get_dict_func_getter({})\n        self.assertAlmostEqual(f(lambda x: x)(3), 3)\n\n    def test_get_coulomb_matrix(self):\n        res = get_coulomb_matrix([1, 1], [[0.0, 0.0, 0.0], [0.0, 0.0, 1.0]])\n        expected_results = numpy.array([\n            [0.5, 1.0],\n            [1.0, 0.5]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_coulomb_matrix_alpha(self):\n        nums = [1, 1]\n        coords = [[0.0, 0.0, 0.0], [0.0, 0.0, .5]]\n        res = get_coulomb_matrix(nums, coords, alpha=2)\n        expected_results = numpy.array([\n            [0.5, 4.],\n            [4., 0.5]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_coulomb_matrix_use_decay(self):\n        nums = [1, 1, 1]\n        coords = [[0.0, 0.0, 0.0], [0.0, 0.0, .5], [0.0, 0.5, 0.0]]\n        res = get_coulomb_matrix(nums, coords, use_decay=True)\n        expected_results = numpy.array([\n            [0.5, 1., 1.],\n            [1., 0.5, 0.585786],\n            [1., 0.585786, 0.5]])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                res,\n                expected_results)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_element_pairs(self):\n        res = get_element_pairs(ELEMENTS)\n        self.assertEqual(set(res), set([(\'C\', \'H\'), (\'H\', \'H\')]))\n\n    def test_deslugify(self):\n        string = \'Class__int=1__float=1.__str=string\'\n        expected = (\'Class\', {\'int\': 1, \'float\': 1., \'str\': \'string\'})\n        self.assertEqual(deslugify(string), expected)\n\n        string = \'ClassM__none=None__true=True__false=False\'\n        expected = (\'ClassM\', {\'none\': None, \'true\': True, \'false\': False})\n        self.assertEqual(deslugify(string), expected)\n\n    def test_sort_chain(self):\n        needs_flip = (""O"", ""H"", ""C"")\n        expected = (""C"", ""H"", ""O"")\n        self.assertEqual(sort_chain(needs_flip), expected)\n\n        needs_flip = (""O"", ""H"", ""H"", ""C"")\n        expected = (""C"", ""H"", ""H"", ""O"")\n        self.assertEqual(sort_chain(needs_flip), expected)\n\n    def test_needs_reversal(self):\n        needs_flip = (""O"", ""H"", ""C"")\n        self.assertTrue(needs_reversal(needs_flip))\n\n        needs_flip = (""O"", ""H"", ""H"", ""C"")\n        self.assertTrue(needs_reversal(needs_flip))\n\n        no_flip = (""O"", ""H"", ""H"", ""O"")\n        self.assertFalse(needs_reversal(no_flip))\n\n        no_flip = (""O"", ""C"", ""H"", ""O"")\n        self.assertFalse(needs_reversal(no_flip))\n\n    def test_load_json(self):\n        data = {\'parameters\': {\'n_jobs\': 2,\n                               \'input_type\': \'list\'},\n                \'attributes\': {\'_base_chains\': [[\'H\']]},\n                \'transformer\': \'molml.molecule.Connectivity\'}\n        path = \'/tmp/somefile.json\'\n        with open(path, \'w\') as f:\n            json.dump(data, f)\n\n        with open(path, \'r\') as f:\n            for x in (path, f):\n                m = load_json(path)\n                self.assertEqual(m.__class__.__name__,\n                                 data[""transformer""].split(\'.\')[-1])\n                self.assertEqual(m.n_jobs, data[""parameters""][""n_jobs""])\n                self.assertEqual(m._base_chains,\n                                 data[""attributes""][""_base_chains""])\n\n    def test_load_json_nested(self):\n        # We will hack on connectivity a sub transformer on its `depth` param.\n        data = {\n            \'parameters\': {\n                \'n_jobs\': 2,\n                \'depth\': {\n                    \'parameters\': {\n                        \'n_jobs\': 3,\n                        \'input_type\': \'list\'},\n                    \'attributes\': {\'_base_chains\': [[\'H\']]},\n                    \'transformer\': \'molml.molecule.Connectivity\'},\n                \'input_type\': \'list\'},\n            \'attributes\': {\'_base_chains\': [[\'H\']]},\n            \'transformer\': \'molml.molecule.Connectivity\'\n        }\n        path = \'/tmp/somefile.json\'\n        with open(path, \'w\') as f:\n            json.dump(data, f)\n\n        m = load_json(path)\n        self.assertEqual(m.__class__.__name__,\n                         data[""transformer""].split(\'.\')[-1])\n        self.assertEqual(m.n_jobs, data[""parameters""][""n_jobs""])\n        self.assertEqual(m._base_chains, data[""attributes""][""_base_chains""])\n\n        in_data = data[""parameters""][""depth""]\n        in_m = m.depth\n        self.assertEqual(in_m.__class__.__name__,\n                         in_data[""transformer""].split(\'.\')[-1])\n        self.assertEqual(in_m.n_jobs, in_data[""parameters""][""n_jobs""])\n        self.assertEqual(in_m._base_chains,\n                         in_data[""attributes""][""_base_chains""])\n\n    def test_get_connections(self):\n        res = get_connections(ELEMENTS, COORDS)\n        self.assertEqual(res, CONNECTIONS)\n\n    def test_get_graph_distance(self):\n        conn = {\n            0: {1: \'1\'},\n            1: {0: \'1\'},\n            2: {3: \'1\'},\n            3: {2: \'1\', 4: \'1\', 5: \'1\'},\n            4: {3: \'1\', 5: \'1\'},\n            5: {3: \'1\', 4: \'1\'},\n        }\n        inf = numpy.inf\n        expected = numpy.array([\n            [0, 1, inf, inf, inf, inf],\n            [1, 0, inf, inf, inf, inf],\n            [inf, inf, 0, 1, 2, 2],\n            [inf, inf, 1, 0, 1, 1],\n            [inf, inf, 2, 1, 0, 1],\n            [inf, inf, 2, 1, 1, 0],\n        ])\n        res = get_graph_distance(conn)\n        try:\n            numpy.testing.assert_equal(res, expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_connections_disjoint(self):\n        coords2 = numpy.array(COORDS) + 1\n        res = get_connections(ELEMENTS, COORDS, ELEMENTS, coords2)\n        expected = {\n            0: {4: \'1\'},\n            1: {0: \'1\', 4: \'1\'},\n            2: {4: \'1\'},\n            3: {},\n            4: {}\n        }\n        self.assertEqual(res, expected)\n\n    def test_get_bond_type_warn(self):\n        with warnings.catch_warnings(record=True) as w:\n            get_bond_type(\'Fake\', \'H\', 1)\n            get_bond_type(\'H\', \'Not Real\', 1)\n            get_bond_type(\'Fake\', \'Not Real\', 1)\n            self.assertEqual(len(w), 3)\n\n    def test_get_depth_threshold_mask_connections_all(self):\n        conn = {\n            0: {1: \'1\'},\n            1: {0: \'1\'},\n            2: {3: \'1\'},\n            3: {2: \'1\'},\n        }\n        res = get_depth_threshold_mask_connections(conn, max_depth=0)\n        try:\n            numpy.testing.assert_equal(res,\n                                       numpy.ones((len(conn), len(conn))))\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_depth_threshold_mask_connections_disjoint(self):\n        conn = {\n            0: {1: \'1\'},\n            1: {0: \'1\'},\n            2: {3: \'1\'},\n            3: {2: \'1\'},\n        }\n        res = get_depth_threshold_mask_connections(conn, min_depth=numpy.inf)\n        expected = numpy.array([\n            [False, False, True, True],\n            [False, False, True, True],\n            [True, True, False, False],\n            [True, True, False, False],\n        ])\n        try:\n            numpy.testing.assert_equal(res, expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_depth_threshold_mask_connections_max(self):\n        conn = {\n            0: {1: \'1\'},\n            1: {0: \'1\', 2: \'1\'},\n            2: {1: \'1\', 3: \'1\'},\n            3: {2: \'1\'},\n        }\n        res = get_depth_threshold_mask_connections(conn, max_depth=2)\n        expected = numpy.array([\n            [True, True, True, False],\n            [True, True, True, True],\n            [True, True, True, True],\n            [False, True, True, True],\n        ])\n        try:\n            numpy.testing.assert_equal(res, expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_depth_threshold_mask_connections_min(self):\n        conn = {\n            0: {1: \'1\'},\n            1: {0: \'1\', 2: \'1\'},\n            2: {1: \'1\', 3: \'1\'},\n            3: {2: \'1\'},\n        }\n        res = get_depth_threshold_mask_connections(conn, min_depth=2)\n        expected = numpy.array([\n            [False, False, True, True],\n            [False, False, False, True],\n            [True, False, False, False],\n            [True, True, False, False],\n        ])\n        try:\n            numpy.testing.assert_equal(res, expected)\n        except AssertionError as e:\n            self.fail(e)\n\n    def test_get_depth_threshold_mask_connections_both(self):\n        conn = {\n            0: {1: \'1\'},\n            1: {0: \'1\', 2: \'1\'},\n            2: {1: \'1\', 3: \'1\'},\n            3: {2: \'1\', 4: \'1\'},\n            4: {3: \'1\'},\n        }\n        res = get_depth_threshold_mask_connections(conn, min_depth=2,\n                                                   max_depth=2)\n        expected = numpy.array([\n            [False, False, True, False, False],\n            [False, False, False, True, False],\n            [True, False, False, False, True],\n            [False, True, False, False, False],\n            [False, False, True, False, False],\n        ])\n        try:\n            numpy.testing.assert_equal(res, expected)\n        except AssertionError as e:\n            self.fail(e)\n\n\nclass LazyValuesTest(unittest.TestCase):\n\n    def test_all(self):\n        a = LazyValues(elements=ELEMENTS, coords=COORDS, numbers=NUMBERS,\n                       connections=CONNECTIONS, unit_cell=UNIT_CELL)\n        self.assertEqual(a.elements.tolist(), ELEMENTS)\n        self.assertEqual(a.coords.tolist(), COORDS)\n        self.assertEqual(a.numbers.tolist(), NUMBERS)\n        self.assertEqual(a.connections, CONNECTIONS)\n        self.assertEqual(a.unit_cell.tolist(), UNIT_CELL)\n\n    def test_num_from_ele(self):\n        a = LazyValues(elements=ELEMENTS)\n        self.assertEqual(a.numbers.tolist(), NUMBERS)\n\n    def test_ele_from_num(self):\n        a = LazyValues(numbers=NUMBERS)\n        self.assertEqual(a.elements.tolist(), ELEMENTS)\n\n    def test_no_coords(self):\n        a = LazyValues(elements=ELEMENTS, numbers=NUMBERS)\n        with self.assertRaises(ValueError):\n            a.coords\n\n    def test_no_unit_cell(self):\n        a = LazyValues(elements=ELEMENTS, numbers=NUMBERS)\n        with self.assertRaises(ValueError):\n            a.unit_cell\n\n    def test_no_ele_or_num(self):\n        a = LazyValues(coords=COORDS)\n        with self.assertRaises(ValueError):\n            a.elements\n        with self.assertRaises(ValueError):\n            a.numbers\n\n    def test_connections(self):\n        a = LazyValues(elements=ELEMENTS, coords=COORDS, numbers=NUMBERS)\n        self.assertEqual(a.connections, CONNECTIONS)\n\n    def test_fill_in_crystal(self):\n        a = LazyValues(elements=ELEMENTS, coords=COORDS, numbers=NUMBERS,\n                       connections=CONNECTIONS, unit_cell=UNIT_CELL)\n        a.fill_in_crystal(radius=1.)\n        length = 3\n        self.assertEqual(a.elements.tolist(), ELEMENTS * length)\n        expected_results = numpy.array([\n            [0.99826008, -0.00246000, -1.00436000],\n            [2.09021016, -0.00243000, -0.99586000],\n            [0.63379005,  1.02686007, -0.99586000],\n            [0.62704006, -0.52773003, -0.12188990],\n            [0.64136006, -0.50747003, -1.90540005],\n            [0.99826008, -0.00246000, -0.00436000],\n            [2.09021016, -0.00243000,  0.00414000],\n            [0.63379005,  1.02686007,  0.00414000],\n            [0.62704006, -0.52773003,  0.87811010],\n            [0.64136006, -0.50747003, -0.90540005],\n            [0.99826008, -0.00246000,  0.99564000],\n            [2.09021016, -0.00243000,  1.00414000],\n            [0.63379005,  1.02686007,  1.00414000],\n            [0.62704006, -0.52773003,  1.87811010],\n            [0.64136006, -0.50747003,  0.09459995],\n        ])\n        try:\n            numpy.testing.assert_array_almost_equal(\n                a.coords,\n                expected_results)\n        except AssertionError as e:\n            self.fail(e)\n        self.assertEqual(a.numbers.tolist(), NUMBERS * length)\n\n    def test_fill_in_crystal_units(self):\n        a = LazyValues(elements=ELEMENTS, coords=COORDS, numbers=NUMBERS,\n                       connections=CONNECTIONS, unit_cell=UNIT_CELL)\n        a.fill_in_crystal(units=1)\n        length = 3 * 3 * 3\n        self.assertEqual(a.elements.tolist(), ELEMENTS * length)\n        self.assertEqual(a.numbers.tolist(), NUMBERS * length)\n\n    def test_fill_in_crystal_units_list(self):\n        a = LazyValues(elements=ELEMENTS, coords=COORDS, numbers=NUMBERS,\n                       connections=CONNECTIONS, unit_cell=UNIT_CELL)\n        a.fill_in_crystal(units=[1, 2, 3])\n        length = 3 * 5 * 7\n        self.assertEqual(a.elements.tolist(), ELEMENTS * length)\n        self.assertEqual(a.numbers.tolist(), NUMBERS * length)\n\n    def test_fill_in_crystal_units_list_invalid(self):\n        a = LazyValues(elements=ELEMENTS, coords=COORDS, numbers=NUMBERS,\n                       connections=CONNECTIONS, unit_cell=UNIT_CELL)\n        with self.assertRaises(ValueError):\n            a.fill_in_crystal(units=[1, 2, 3, 4])\n\n        with self.assertRaises(ValueError):\n            a.fill_in_crystal(units=[1, 2])\n\n        with self.assertRaises(ValueError):\n            a.fill_in_crystal(units=[1, 2], radius=2)\n\n    def test_fill_in_crystal_conn(self):\n        eles = [\'H\']\n        coords = numpy.array([[0.0, 0.0, 0.0]])\n        nums = [1]\n        connections = {0: {}}\n        unit_cell = numpy.array([[1., 0, 0],\n                                 [0, 1., 0],\n                                 [0, 0, 1.]])\n        a = LazyValues(elements=eles, coords=coords, numbers=nums,\n                       connections=connections, unit_cell=unit_cell)\n        a.fill_in_crystal(radius=1.)\n        self.assertEqual(a.elements.tolist(), eles * 7)\n        expected = {\n            0: {3: \'1\'},\n            1: {3: \'1\'},\n            2: {3: \'1\'},\n            3: {0: \'1\', 1: \'1\', 2: \'1\', 4: \'1\', 5: \'1\', 6: \'1\'},\n            4: {3: \'1\'},\n            5: {3: \'1\'},\n            6: {3: \'1\'},\n        }\n        self.assertEqual(a.connections, expected)\n\n    def test_fill_in_crystal_null(self):\n        a = LazyValues()\n        with self.assertRaises(ValueError):\n            a.fill_in_crystal(radius=1.)\n\n        a = LazyValues(unit_cell=UNIT_CELL)\n        with self.assertRaises(ValueError):\n            a.fill_in_crystal(radius=1.)\n\n        length = 3\n        a = LazyValues(numbers=NUMBERS, coords=COORDS, unit_cell=UNIT_CELL)\n        a.fill_in_crystal(radius=1.)\n        self.assertEqual(a.numbers.tolist(), NUMBERS * length)\n\n        a = LazyValues(elements=ELEMENTS, coords=COORDS, unit_cell=UNIT_CELL)\n        a.fill_in_crystal(radius=1.)\n        self.assertEqual(a.elements.tolist(), ELEMENTS * length)\n\n\nclass IndexMapTest(unittest.TestCase):\n    def test_standard(self):\n        a = IndexMap([(\'C\', \'H\'), (\'H\', \'H\'), (\'C\', \'C\')], 2)\n        self.assertEqual(a[\'C\', \'C\'], 0)\n        self.assertEqual(a[\'C\', \'H\'], 1)\n        self.assertEqual(a[\'H\', \'C\'], 1)\n        with self.assertRaises(KeyError):\n            a[\'something\']\n        self.assertEqual(len(a), 3)\n        expected_order = [(\'C\', \'C\'), (\'C\', \'H\'), (\'H\', \'H\')]\n        self.assertEqual(a.get_value_order(), expected_order)\n\n        for x, y in zip(a, expected_order):\n            self.assertEqual(x, y)\n\n    def test_use_comb_idxs(self):\n        a = IndexMap([(\'C\', \'H\', \'A\'), (\'H\', \'H\', \'A\'), (\'C\', \'C\', \'B\')], 2,\n                     use_comb_idxs=True)\n        self.assertEqual(a.idx_groups, [(0, 1), (0, 2), (1, 2)])\n\n    def test_shorter(self):\n        a = IndexMap([(\'C\', \'H\'), (\'H\', \'H\'), (\'C\', \'C\')], 1)\n        self.assertEqual(a[\'C\'], 0)\n        self.assertEqual(a[\'H\'], 1)\n        with self.assertRaises(KeyError):\n            a[\'something\']\n        self.assertEqual(len(a), 2)\n        self.assertEqual(a.get_value_order(), [(\'C\', ), (\'H\', )])\n\n    def test_all_add_unknown(self):\n        a = IndexMap([(\'C\', \'H\'), (\'H\', \'H\'), (\'C\', \'C\')], 1, add_unknown=True)\n        self.assertEqual(a[\'C\'], 0)\n        self.assertEqual(a[\'H\'], 1)\n        self.assertEqual(a[\'something\'], -1)\n        self.assertEqual(len(a), 3)\n        self.assertEqual(a.get_value_order(),\n                         [(\'C\', ), (\'H\', ), (UNKNOWN, )])\n\n    def test__get_form_indices(self):\n        data = (\n            (  # 1\n                (0, [tuple()]),\n                (1, [(0, )]),\n                (2, [(0, )]),\n            ),\n            (  # 2\n                (0, [tuple()]),\n                (1, [(0, ), (1, )]),\n                (2, [(0, 1)]),\n            ),\n            (  # 3\n                (0, [tuple()]),\n                (1, [(1, )]),\n                (2, [(0, 2)]),\n                (3, [(0, 1, 2)]),\n            ),\n            (  # 4\n                (0, [tuple()]),\n                (1, [(1, ), (2, )]),\n                (2, [(1, 2)]),\n                (3, [(0, 1, 2), (1, 2, 3)]),\n                (4, [(0, 1, 2, 3)]),\n            ),\n            (  # 5\n                (0, [tuple()]),\n                (1, [(2, )]),\n                (2, [(1, 3)]),\n                (3, [(1, 2, 3)]),\n                (4, [(0, 1, 3, 4)]),\n                (5, [(0, 1, 2, 3, 4)]),\n            ),\n            (  # 6\n                (0, [tuple()]),\n                (1, [(2, ), (3, )]),\n                (2, [(2, 3)]),\n                (3, [(1, 2, 3), (2, 3, 4)]),\n                (4, [(1, 2, 3, 4)]),\n                (5, [(0, 1, 2, 3, 4), (1, 2, 3, 4, 5)]),\n                (6, [(0, 1, 2, 3, 4, 5)]),\n            )\n        )\n        for i, group in enumerate(data):\n            for depth, expected in group:\n                vals = IndexMap._get_form_indices(i + 1, depth)\n                self.assertEqual(vals, expected)\n\n    def test__get_form_indices_invalid(self):\n        with self.assertRaises(ValueError):\n            IndexMap._get_form_indices(0, 1)\n\n    def test_get_index_mapping(self):\n        values = [(\'H\', \'H\'), (\'H\', \'C\'), (\'C\', \'C\')]\n        expected = (\n            (0, [], {tuple(): 0}),\n            (1, [0], {(\'C\', ): 0, (\'H\', ): 1}),\n            (2, [0, 1], {(\'C\', \'C\'): 0, (\'C\', \'H\'): 1, (\'H\', \'H\'): 2}),\n            (3, [0, 1], {(\'C\', \'C\'): 0, (\'C\', \'H\'): 1, (\'H\', \'H\'): 2}),\n        )\n        for depth, idxs, expected_mapping in expected:\n            mapping = IndexMap.get_index_mapping(values, depth, [idxs])\n            self.assertEqual(mapping, expected_mapping)\n            for value in values:\n                new_value = sort_chain(tuple(value[i] for i in idxs))\n                self.assertIn(new_value, mapping)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
